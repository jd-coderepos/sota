\section{Experiments}
\label{sec:results}

In this section, we describe our experimental setup, compare the \textbf{driving performance} of TransFuser against several baselines, visualize the \textbf{attention maps} of TransFuser and present \textbf{ablation studies} to highlight the importance of different components of our approach.

\subsection{Task}
\label{sec:task}
We consider the task of navigation along a set of predefined routes in a variety of areas, \eg freeways, urban areas, and residential districts. The routes are defined by a sequence of sparse goal locations in GPS coordinates provided by a global planner. Each route consists of several scenarios, initialized at predefined positions, which test the ability of the agent to handle different kinds of adversarial situations, \eg obstacle avoidance, unprotected turns at intersections, vehicles running red lights, and pedestrians emerging from occluded regions to cross the road at random locations. The agent needs to complete the route within a specified time limit while following traffic regulations and coping with high densities of dynamic agents.

\subsection{Training Dataset}
\label{sec:data}
We use the CARLA~\cite{Dosovitskiy2017CORL} simulator for training and testing, specifically CARLA 0.9.10 which consists of 8 publicly available towns. We use all 8 towns for training. Our dataset is collected along a set of training routes: around 2500 routes through junctions with an average length of 100m, and around 1000 routes along curved highways with an average length of 400m. For generating training data, we roll out an expert policy designed to drive using privileged information from the simulation and store data at 2FPS. The expert is a rule-based algorithm similar to the CARLA traffic manager autopilot\footnote{\url{https://carla.readthedocs.io/en/latest/adv_traffic_manager/}}. Our training dataset contains 228k frames in total. In the following, we provide more details regarding the expert algorithm.

\begin{figure}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{gfx/autopilot_intersection_04.png}
        \caption{The expert waits before taking the turn because the trajectory forecasting predicts a collision if the expert would drive.}
        \label{fig:expert1}
    \end{subfigure}
    \hfill
    \vspace{0.15cm}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{gfx/autopilot_intersection_05.png}
        \caption{After the oncoming cars have passed, the expert crosses the intersection.}
        \label{fig:expert2}
    \end{subfigure}
    \vspace{0.0cm}
    \caption{\label{fig:expert}\red{\textbf{Expert performing an unprotected left turn.}} The black boxes on the street mark the path that the expert has to follow. The predictions of the bicycle model are colored green for the expert and blue for all other vehicles. Red bounding boxes mark predicted collisions. The white box around the car is used to detect the traffic light trigger boxes that are placed on the street (\eg bottom left \figref{fig:expert1}).}
\end{figure}

\subsection{Expert}
For generating training data, we roll out an expert policy designed to drive using privileged information from the simulator. \red{The waypoints of the expert are the ground truth labels for the imitation loss, so it can be viewed as an automatic labeling algorithm. The ground truth labels for the auxiliary tasks are provided by the simulator.} We build upon the code provided by the authors of~\cite{Chen2019CORL}. This approach is based on simple handcrafted rules. Building the expert with RL is also possible~\cite{Zhang2021ICCV, Agarwal2021ARXIV} but it is more computationally demanding and less interpretable. Our expert policy consists of an A* planner followed by 2 PID controllers (for lateral and longitudinal control). The lateral and longitudinal control tasks are treated independently.

Lateral control is done by following the path generated by the A* planner. Specifically, we minimize the angle of the car towards the next waypoint in the route, which is at least 3.5 meters away, using a PID controller. Longitudinal control is done using a version of model predictive control and differentiates between 3 target speeds. The standard target speed is 4.0 m/s. When the expert is inside an intersection, the target speed is reduced to 3.0 m/s. Lastly, in case an infraction is predicted the target speed is set to 0.0 m/s bringing the vehicle to a halt. We predict red light infractions by performing intersection tests with trigger boxes that CARLA provides. Collision infractions are predicted by forecasting the oriented bounding boxes of all traffic participants. We forecast in 50 ms discrete steps, for 4 seconds in intersections and 1 second in other areas. The forecasting is done using the pretrained \red{kinematic} bicycle model from \cite{Chen2021ICCVb}. \red{This bicycle model is a simple mathematical model that can predict the position, orientation, and speed of a car after a discrete time step, given its current position orientation speed and the applied control. We forecast all vehicles by iteratively rolling out the bicycle model using its output at time step  as the input for time step . Since we only know the control input of other traffic participants at the current time step (provided by the simulator), we assume that they will continue to apply the same control at future time steps. For the ego vehicle, we calculate its future steering and throttle by using PID controllers that try to follow the route. The ego brake control is always set to 0 because we want to answer the counterfactual question of whether there will be a collision if we do not brake. We forecast pedestrians analogously but model them as a point with velocity and acceleration. This works well because the movement patterns of pedestrians in CARLA are simple.} A collision infraction is detected if there is an intersection of the ego vehicle bounding box at future time step t with the bounding box of another traffic participant at future time step t. \red{A common failure of the action repeat forecast mechanism described above is that it does not anticipate that fast cars approaching the expert from behind will eventually slow down before colliding. To avoid false positives, we do not consider rear-end collisions by only using the front half of the vehicle as its bounding box.} For the longitudinal controller, we set  and for the lateral controller, we set . Both controllers use a buffer of size 40 to approximate the integral term as a running average. An example of the expert performing an unprotected left turn can be seen in \figref{fig:expert}.

\subsection{Longest6 Benchmark}
\label{sec:benchmark}

The CARLA simulator provides an official evaluation leaderboard consisting of 100 secret routes. However, teams using this platform are restricted to only 200 hours of evaluation time per month. A single evaluation takes over 100 hours, making the official leaderboard unsuitable for ablation studies or obtaining detailed statistics involving multiple evaluations of each model. Therefore, we propose the Longest6 Benchmark, which shares several similarities to the official leaderboard, but can be used for evaluation on local resources without computational budget restrictions.

The CARLA leaderboard repository provides a set of 76 routes as a starting point for training and evaluating agents. These routes were originally released along with the 2019 CARLA challenge. They span 6 towns and each of them is defined by a sequence of waypoints. However, there is a large imbalance in the number of routes per town, \eg Town03 and Town04 have 20 routes each, but Town02 has only 6 routes. To balance the Longest6 driving benchmark across all available towns, we choose the 6 longest routes per town from the set of 76 routes. This results in 36 routes with an average route length of 1.5km, which is similar to the average route length of the official leaderboard (1.7km). We make three other design choices for the Longest6 benchmark, motivated by the official leaderboard. (1) During evaluation, we ensure a high density of dynamic agents by spawning vehicles at every possible spawn point permitted by the CARLA simulator. (2) Following \cite{Chitta2021ICCV}, each route has a unique environmental condition obtained by combining one of 6 weather conditions (Cloudy, Wet, MidRain, WetCloudy, HardRain, SoftRain) with one of 6 daylight conditions (Night, Twilight, Dawn, Morning, Noon, Sunset). (3) We include CARLA's adversarial scenarios in the evaluation, which are spawned at predefined positions along the route. Specifically, we include CARLA's scenarios 1, 3, 4, 7, 8, 9, 10 which are generated based on the NHTSA pre-crash typology\footnote{\url{https://leaderboard.carla.org/scenarios/}}. \red{Visualizations of the routes in the Longest6 benchmark are provided in the supplementary material.}

\subsection{Metrics}
\label{sec:metrics}

We report several metrics to provide a comprehensive understanding of the driving behavior of each agent.

\noindent (1) \textbf{Route Completion (RC)}: percentage of route distance completed,  by the agent in route , averaged across  routes. However, if an agent drives outside the route lanes for a percentage of the route, then the RC is reduced by a multiplier (1- \% off route distance).


\noindent (2) \textbf{Infraction Score (IS)}: geometric series of infraction penalty coefficients,  for every instance of infraction  incurred by the agent during the route. Agents start with an ideal 1.0 base score, which is reduced by a penalty coefficient for every infraction.

The penalty coefficient for each infraction is pre-defined and set to 0.50 for collision with a pedestrian, 0.60 for collision with a vehicle, 0.65 for collision with static layout, and 0.7 for red light violations. The official CARLA leaderboard also mentions a penalty for stop sign violations. However, we observe that none of our submissions have any stop sign infractions. Hence, we omit this infraction from our analysis.
\BlankLine

\noindent (3) \textbf{Driving Score (DS)}: weighted average of the route completion with infraction multiplier 


\noindent (4) \textbf{Infractions per km}: the infractions considered are collisions with pedestrians, vehicles, and static elements, running a red light, off-road infractions, route deviations, timeouts, and vehicle blocked.
\red{We report the total number of infractions, normalized by the total number of km driven.}

\red{where  is the driven distance (in km) for route . The Off-road infraction is slightly different. Instead of the total \textit{number} of infractions the sum of \textit{km driven off-road} is used. We multiply by  because this metric is a percentage.}

\begin{table*}[t]
\small
\centering
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{c| c c c | c c c c c c c c}
        \textbf{Method} & \textbf{DS}  & \textbf{RC}  & \textbf{IS}  & \textbf{Ped}  & \textbf{Veh}  & \textbf{Stat}  & \textbf{Red}  & \textbf{OR}  & \textbf{Dev}  & \textbf{TO}  & \textbf{Block}  \\
        \hline
        WOR~\cite{Chen2021ICCVb} & \red{20.53}  \red{3.12} & \red{48.47}  \red{3.86} & \textbf{\red{0.56}}  \red{0.03} & \red{0.18} & \red{1.05} & \red{0.37} & \red{1.28} & \red{0.47} & \red{0.88} & \red{0.08} & \red{0.20} \\
        Latent TransFuser (Ours) & \red{37.31}  \red{5.35} & \textbf{\red{95.18}}  \red{0.45} & \red{0.38}  \red{0.05} & \red{\textbf{0.03}} & \red{3.66} & \red{0.18} & \red{0.13} & \red{\textbf{0.04}} & \red{\textbf{0.00}} & \red{0.12} & \red{\textbf{0.05}}\\
        \hline
        \red{LAV~\cite{Chen2022CVPR}} & \red{32.74}  \red{1.45} & \red{70.36}  \red{3.14} & \red{0.51}  \red{0.02} & \red{0.16} & \red{\textbf{0.83}} & \red{0.15} & \red{0.96} & \red{0.42} & \red{0.06} & \red{0.12} & \red{0.45} \\
        Late Fusion (LF) & \red{22.47}  \red{3.71} & \red{83.30}  \red{3.04} & \red{0.27}  \red{0.04} & \red{0.05} & \red{4.63} & \red{0.28} & \red{\textbf{0.11}} & \red{0.48} & \red{0.02} & \red{0.11} & \red{0.21} \\
        Geometric Fusion (GF) & \red{27.32}  \red{0.80} & \red{91.13}  \red{0.95} & \red{0.30}  \red{0.01} & \red{0.06} & \red{4.64} & \red{0.17} & \red{0.13} & \red{0.48} & \textbf{\red{0.00}} & \textbf{\red{0.05}} & \red{0.11} \\
        TransFuser (Ours) & \textbf{\red{47.30}}  \red{5.72} & \red{93.38}  \red{1.20} & \red{0.50}  \red{0.06} & \red{\textbf{0.03}} & \red{2.45} & \red{\textbf{0.07}} & \red{0.16} & \red{\textbf{0.04}} & \red{\textbf{0.00}} & \red{0.06} & \red{0.10} \\
        \hline
        \textit{Expert} & \textit{76.91  2.23} & \textit{88.67  0.56} & \textit{0.86  0.03} & \textit{\red{0.02}} & \textit{\red{0.28}} & \textit{\red{0.01}} & \textit{\red{0.03}} & \textit{\red{0.00}} & \textit{\red{0.00}} & \textit{\red{0.08}} & \textit{\red{0.13}}\\
        \hline
    \end{tabular}
    \caption{\textbf{Longest6 Benchmark Results.} We compare our TransFuser model with several baselines in terms of driving performance and infractions incurred. We report the metrics for 3 evaluation runs of each model on the Longest6 evaluation setting. For the primary metrics (DS: Driving Score, RC: Route Completion, IS: Infraction Score) we show the mean and std. For the remaining infractions per km metrics (Ped: Collisions with pedestrians, Veh: Collisions with vehicles, Stat: Collisions with static layout, Red: Red light violation, OR: Off-road driving, Dev: Route deviation, TO: Timeout, Block: Vehicle Blocked) we show only the mean. TransFuser obtains the best DS \red{by a large margin}.}
    \label{tab:detailed_results}
    \vspace{0.0cm}
\end{table*}

\subsection{Baselines}
\label{sec:baselines}

We compare our TransFuser model to several baselines. (1) \textbf{WOR}~\cite{Chen2021ICCVb}: this is a multi-stage training approach that supervises the driving task with a Q function obtained using dynamic programming. It is the current state-of-the-art approach on the simpler NoCrash benchmark~\cite{Codevilla2019ICCV} for CARLA version 0.9.10. We use the author-provided pretrained model for evaluating this approach. (2) \textbf{Latent TransFuser}: to investigate the importance of the LiDAR input, we implement an auto-regressive waypoint prediction network that has the same architecture as the TransFuser but uses a fixed positional encoding image as input instead of the BEV LiDAR, as described in \secref{sec:ltf}. \red{(3) \textbf{LAV}~\cite{Chen2022CVPR}: this is a concurrent approach that performs sensor fusion via PointPainting~\cite{Vora2020CVPR}, which concatenates semantic class information extracted from the RGB image to the LiDAR point cloud, to train a privileged motion planner to predict trajectories of all nearby vehicles in the scene. This privileged planner is then distilled into a policy that drives from raw sensor inputs only. We use the checkpoint publicly released by the authors\footnote{\url{https://github.com/dotchen/LAV}} for our experiments. We note that this published checkpoint is not the exact same model as the one used for LAV's leaderboard entry.} (4) \textbf{Late Fusion}: we implement a version of our architecture where the image and the LiDAR features are extracted independently using the same encoders as TransFuser but without the transformers (similar to~\cite{Sobh2018NEURIPSW}). The features from each branch are then fused through element-wise summation and passed to the waypoint prediction network. (5) \textbf{Geometric Fusion}: we implement a multi-scale geometry-based fusion method, inspired by~\cite{Liang2018ECCV, Liang2019CVPR}, involving both image-to-LiDAR and LiDAR-to-image feature fusion. We unproject each 0.125m  0.125m block in our LiDAR BEV representation into 3D space, resulting in a 3D cell. We randomly select 5 points from the LiDAR point cloud lying in this 3D cell and project them into the image space. Then, we aggregate the image features of these points via element-wise summation before passing them to a 3-layer MLP. The output of the MLP is then combined with the LiDAR BEV feature of the corresponding 0.125m  0.125m block at multiple resolutions throughout the feature extractor. Similarly, for each image pixel, we aggregate information from the LiDAR BEV features at multiple resolutions. This baseline is equivalent to replacing the transformers in our architecture with projection-based feature fusion. We also report results for the expert used for generating our training data, which defines an upper bound for the performance on the Longest6 evaluation setting. We provide additional details regarding the baselines in the supplementary material. 

\subsection{Implementation Details}
\label{sec:impl}
 
We use 2 sensor modalities, the front-facing cameras and LiDAR point cloud converted to a BEV representation (\secref{sec:io_parameterization}), \ie, . The camera inputs are concatenated to a single image and encoded using a RegNetY-3.2GF~\cite{Radosavovic2020CVPR} which is pretrained on ImageNet~\cite{Deng2009CVPR}. \red{We use pre-trained models from~\cite{Wightman2019TIMM}.} The LiDAR BEV representation is encoded using another RegNetY-3.2GF~\cite{Radosavovic2020CVPR} which is trained from scratch. Similar to \cite{Chen2019CORL}, we perform angular viewpoint augmentation for the training data, by randomly rotating the sensor inputs by 20 and adjusting the waypoint labels according to this rotation. We use 1 transformer per resolution and 4 attention heads for each transformer. We select  from  for the 4 transformers corresponding to the feature embedding dimension  at each resolution. We train the models with 4 RTX 2080Ti GPUs for 41 epochs, with an initial learning rate of  \red{and a batch size of }. We reduce the learning rate by a factor of 10 after epoch 30 and 40. We evaluate the epochs 31, 33, 35, 37, 39 and 41 closed loop on the validation routes of \cite{Chitta2021ICCV} for one seed and pick the epoch with the highest driving score. For all models, we use the AdamW optimizer~\cite{Loshchilov2019ICLR}, which is a variant of Adam. Weight decay is set to , and Adam beta values to the PyTorch defaults of  and .

\subsection{Longest6 Benchmark Results}
\label{sec:results_internal}

We begin with an analysis of driving performance on CARLA on the new Longest6 evaluation setting (\tabref{tab:detailed_results}). For each experimental result, the evaluation is repeated 3 times to account for the non-determinism of the CARLA simulator. Furthermore, imitation based methods typically show variation in performance when there is a change in the random initialization and data sampling due to the training seed~\cite{Behl2020IROS,Prakash2021CVPR}. To account for the variance observed between different training runs, we use 3 different random seeds for each method, and report the metrics for an ensemble of these 3 training runs.

\boldparagraph{Latent TransFuser as a Strong Baseline} In our first experiment, we examine the performance of image-based methods. From the results in \tabref{tab:detailed_results}, we observe that WOR performs poorly on the Longest6 evaluation setting. In particular, we observe that WOR suffers from a poor RC with a much larger number of route deviations (Dev) than the remaining methods. On the other hand, we find that Latent Transfuser obtains the best RC in \tabref{tab:detailed_results}, \red{with zero route deviations}. This is likely because Latent TransFuser uses our inverse dynamics model (PID controller) for low-level control and represents goal locations in the same BEV coordinate space in which waypoints are predicted. In contrast, WOR uses coarse navigational commands (\eg follow lane, turn left/right, change lane) to inform the model regarding driver intentions, and chooses an output action from a discrete action space. This result indicates that the TransFuser architecture involving auto-regressive waypoint prediction a strong baseline for the end-to-end driving task, even in the absence of a LiDAR sensor. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{gfx/DenseTrafficCollision1.png}
    \includegraphics[width=\textwidth]{gfx/DenseTrafficCollision2.png}
    \caption{\red{\textbf{Lane Change Failures.} TransFuser fails at lane changes in dense traffic incurring a high number of consecutive collisions in routes where these situations occur. Two examples are shown in the top and bottom rows. Time goes forward from left to right.}}
    \vspace{-0.0cm}
    \label{fig:lane_change_collision}
    \vspace{-0.0cm}
\end{figure*}

\boldparagraph{Sensor Fusion Methods} The goal of this experiment is to determine the impact of the LiDAR modality on the driving performance and compare different fusion methods. \red{For this, we compare TransFuser to three baselines, LAV, Late Fusion (LF), Geometric Fusion (GF)}. \red{LAV performs worse than TransFuser in terms of DS. The main difference arises from the 23\% lower RC. Potential reasons could be worse steering as indicated by the higher off-road infractions, or false positives in the modular components which might be the reason for the higher blocked infraction. While LAV obtains a similar IS to TransFuser, upon probing further, we notice that it is only better in terms of avoiding collisions with vehicles and TransFuser performs better with respect to all other infractions. We note that in the Longest6 benchmark, there are a few routes where the vehicle is required to drive in dense traffic on multi-lane highways. TransFuser fails at lane merging in these situations, incurring a large amount of vehicle collisions (20), strongly affecting its vehicle collision metric.} Surprisingly, we observe that LF and GF perform worse than the image-only Latent TransFuser baseline (\tabref{tab:detailed_results}). The multi-scale geometry-based fusion encoder of GF gives some improvements when compared to LF, however, both LF and GF suffer from a poor IS. We hypothesize that this occurs because they do not incorporate global contextual reasoning which is necessary to safely navigate the intersections, and focus primarily on navigation to the goal at all costs while ignoring obstacles, which leads to several infractions. In contrast, our TransFuser model obtains an absolute improvement \red{19.98\%} in terms of DS when compared to GF. It also achieves an \red{48.59\%} reduction compared to LF and \red{47.64\%} reduction compared to GF in collisions per kilometer \red{(Ped+Veh+Stat), and an absolute improvement of over 0.2 in its IS}. These results indicate that attention is effective in incorporating the global context of the 3D scene, which allows for safer driving. 

\begin{table}[t]
\small
    \setlength{\tabcolsep}{6pt}
    \centering
    \begin{tabular}{c| c c}
        \textbf{Method} & \textbf{Single Model} & \textbf{Ensemble (3)} \\
        \hline
        Late Fusion (LF) & 23.5 & 46.7 \\
        Geometric Fusion (GF) & 43.5 & 69.1 \\
        TransFuser (Ours) & 27.6 & 59.6 \\
        \hline
    \end{tabular}
    \caption{\textbf{Runtime.} We show the runtime per frame in ms for each method averaged over all timesteps in a single evaluation route. We measure runtimes for both a single model and an ensemble of three models. A single TransFuser model runs in real-time on an RTX 3090 GPU.}
    \label{tab:runtime}
    \vspace{-0.0cm}
\end{table}

\boldparagraph{Limitations} We observe that all methods \red{with a high RC} struggle with vehicle collisions (Veh). Avoiding collisions is very challenging in our evaluation setting due to the traffic density being set to the maximum allowed density in CARLA. In particular, TransFuser has around \red{9} more vehicle collisions per kilometer than the expert. We observe that these collisions primarily occur during unprotected turns and lane changes \red{as is illustrated in \figref{fig:lane_change_collision}}.

\boldparagraph{Runtime} We measure the runtime of each method on a single RTX 3090 GPU by averaging over all time-steps of one evaluation route. The runtime considered includes sensor data pre-processing, model inference and PID control. The results are shown in \tabref{tab:runtime}. We observe that the transformers in our architecture increase the runtime relative to the LF baseline by 17\% for a single model and 28\% for an ensemble of three models. However, a single TransFuser model can still be executed in real-time on this hardware. The GF baseline is slower than TransFuser despite its simpler fusion mechanism due to the extra time taken finding correspondences between the image and LiDAR tokens.

\subsection{Leaderboard Results}
\label{sec:results_leaderboard}

We submit the models from our study to the CARLA autonomous driving leaderboard which contains a secret set of 100 evaluation routes and report the results in \tabref{tab:leaderboard}. Among the models that do not use LiDAR inputs, Latent TransFuser achieves the best performance. \red{It obtains a DS of 45.20, which is nearly 10 points better than the next best image-based method, GRIAD~\cite{Chekroun2021ARXIV}}. GRIAD builds on top of the Reinforcement Learning (RL) method presented in~\cite{Toromanoff2020CVPR}. In this approach, an encoder is first trained to predict both the 2D semantics and specific affordances such as the scene traffic light state, and the relative position and orientation between the vehicle and lane. The encoder is then frozen and used to train a value function-based RL method using a replay buffer that is partially filled with expert demonstrations. GRIAD requires 45M samples from the CARLA simulator for training ~\cite{Chekroun2021ARXIV} whereas our training dataset has only 228k frames (200 less than GRIAD). 

\begin{table}[t]
\small
	\centering
	\setlength{\tabcolsep}{5pt}
	\begin{tabular}{c | c | c c c}
	    \textbf{{Method}} & \textbf{{LiDAR?}} & \textbf{DS}  & \textbf{RC}  & \textbf{IS}  \\
	    \hline
	    NEAT~\cite{Chitta2021ICCV} & - & 21.83 & 41.71 & 0.65 \\
	    MaRLn~\cite{Toromanoff2020CVPR} & - & 24.98 & 46.97 & 0.52 \\
	    WOR~\cite{Chen2021ICCVb} & - & 31.37 & 57.65 & 0.56 \\
	    GRIAD~\cite{Chekroun2021ARXIV} & - & 36.79 & 61.86 & 0.60 \\
		Latent TransFuser (Ours) & - & \red{45.20} & \red{66.31} & \textbf{\red{0.72}} \\
		\hline
 		Late Fusion (LF) &  & \red{26.07} & \red{64.67} & \red{0.47} \\
 		Geometric Fusion (GF) &   & \red{41.70} & \red{87.85} & \red{0.47} \\
		TransFuser (Ours) &   & \red{61.18} & \red{86.69} & {\red{0.71}} \\
		LAV*~\cite{Chen2022CVPR} &   & \textbf{\red{61.85}} & \textbf{\red{94.46}} & \red{0.64} \\
		\hline
	\end{tabular}
	\caption{\red{\textbf{CARLA Leaderboard Evaluation.} We report the DS, RC, and IS over the 100 secret routes of the official evaluation server. Latent TransFuser and TransFuser improve the IS by a large margin in comparison to existing methods. *The LAV leaderboard entry uses an updated model different from the public checkpoint in \tabref{tab:detailed_results}.}}
	\label{tab:leaderboard}
	\vspace{0.0cm}
\end{table}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{gfx/attn_maps_pami/image_to_lidar_1.pdf}
    \includegraphics[width=\textwidth]{gfx/attn_maps_pami/image_to_lidar_2.pdf}
    \includegraphics[width=\textwidth]{gfx/attn_maps_pami/lidar_to_image_1.pdf}
    \includegraphics[width=\textwidth]{gfx/attn_maps_pami/lidar_to_image_2.pdf}
    \caption{\textbf{Attention Maps.} For the {\color{red}{red}} query token, we show the top-5 attended tokens in {\color{darkgreen}{green}} and highlight the presence of vehicles in the LiDAR point cloud in {\color{darkyellow}{yellow}}. Top 2 rows: image to LiDAR, bottom 2 rows: LiDAR to image. TransFuser attends to areas near vehicles and traffic lights at intersections.}
    \vspace{-0.0cm}
    \label{fig:attn_map}
    \vspace{-0.0cm}
\end{figure*}

\red{For the LiDAR-based baselines, GF performs better than LF, similar to our findings on Longest6. Incorporating global attention via TransFuser leads to further improvements with a state-of-the-art IS. While LAV performs similarly to TransFuser, it is only marginally better in terms of DS (+0.67), which is likely within the evaluation variance. To obtain this marginal improvement, LAV adopts multi-stage training with several pretrained modular components and a teacher-student distillation framework. In contrast, TransFuser achieves state-of-the-art results with a straightforward single-stage IL training procedure.} For further improvements, the training procedure of TransFuser can potentially be combined with techniques used in previous work on autonomous driving such as Active Learning~\cite{Chitta2018ARXIV,Chitta2019ARXIV,Haussmann2020IV}, DAgger~\cite{Ross2011AISTATS,Prakash2020CVPR}, \red{adversarial simulation~\cite{Wang2021CVPRb,Rempe2021CVPR,Hanselmann2022ARXIV}}, RL-based fine-tuning~\cite{Ohn-Bar2020CVPR} and teacher-student distillation~\cite{Chen2019CORL, Chen2021ICCVb, Zhao2020CORL, Chen2022CVPR}.

\begin{table}[t]
\small
    \centering
    \setlength{\tabcolsep}{3pt}
    \begin{tabular}{c | c c | c c | c c | c c}
        \textbf{Head} & \multicolumn{2}{c|}{\textbf{T1}} & \multicolumn{2}{c|}{\textbf{T2}} & \multicolumn{2}{c|}{\textbf{T3}} & \multicolumn{2}{c}{\textbf{T4}} \\
        \hline
        & I & L & I & L & I & L & I & L \\
        \hline
        1 & 100.00 & 0.00 & 99.83 & 0.00 & 44.55 & 98.69 & 77.79 & 89.97 \\
        2 & 100.00 & 0.00 & 99.99 & 0.00 & 07.58 & 98.98 & 80.05 & 95.91 \\
        3 & 100.00 & 0.00 & 39.71 & 0.00 & 27.73 & 98.09 & 90.08 & 99.98 \\
        4 & 99.99 & 1.45 & 99.99 & 0.26 & 99.99 & 99.98 & 80.13 & 99.47 \\
        \hline
    \end{tabular}
    \caption{\textbf{Cross-Modal Attention Statistics.} We report the \% of tokens (I: Image tokens, L: LiDAR tokens) for which at least 1 of the top-5 attended tokens belongs to the other modality for each head of the four transformers: T1, T2, T3, T4.}
    \label{tab:attention_statistics}
\end{table}

\begin{table*}[t]
\small
	\centering
	\setlength{\tabcolsep}{6pt}
	\begin{tabular}{c | c | c | c c c | c c c }
	    \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Ensemble?}} & \multirow{2}{*}{\textbf{Safety Heuristic}} & \multicolumn{3}{c|}{\textbf{Longest6}} & \multicolumn{3}{c}{\textbf{Leaderboard}}\\
	    & & & {DS}  & {RC}  & {IS}  & {DS}  & {RC}  & {IS}  \\
	    \hline
		\multirow{3}{*}{Latent TransFuser} & - & Global & \textbf{\red{50.00}  \red{1.13}} & \red{90.38}  \red{3.32} & \textbf{\red{0.56}  \red{0.02}} & \red{47.05} & \red{71.66} & \red{0.72} \\ & - & Creep Only & \red{42.19}  \red{5.49} & \red{94.84}  \red{1.40} & \red{0.44}  \red{0.06} & \red{42.36} & \red{86.67} & \red{0.51} \\ &  & Creep Only & \red{36.18}  \red{5.36} & \textbf{\red{95.34}  \red{2.20}} & \red{0.37}  \red{0.05} & \red{45.03} & \red{75.37} & \red{0.62} \\ \hline
		\multirow{3}{*}{TransFuser} & - & Global & \red{49.49}  \red{8.63} & \red{90.67}  \red{4.78} & \red{0.55}  \red{0.09} & \red{41.93} & \red{58.55} & \red{\textbf{0.77}} \\& - & Creep Only & \red{42.51}  \red{2.49} & \red{91.01}  \red{0.83} & \red{0.46}  \red{0.02} & \red{50.57} & \red{73.84} & \red{0.68} \\ &  & Creep Only & \red{47.30}  \red{5.72} & \red{93.38}  \red{1.20} & \red{0.50}  \red{0.06} & \red{\textbf{61.18}} & \red{\textbf{86.69}} & \red{0.71} \\ \hline
	\end{tabular}
	\caption{\red{\textbf{Impact of Global Safety Heuristic.} The heuristic leads to consistent minor improvements for Latent TransFuser. For TransFuser, though the heuristic improves the Longest6 scores, it reduces the performance on the CARLA leaderboard.}} 
	\label{tab:safety}
	\vspace{0.0cm}
\end{table*}

\subsection{Attention Statistics and Visualizations}
\label{sec:visualizations}

Our architecture (\figref{fig:model}) consists of 4 transformers with 4 attention layers and 4 attention heads in each transformer. In this section, we visualize the attention maps from the final attention layer for each head for each transformer. The transformer takes in 110 image feature tokens and 64 LiDAR feature tokens as input where each token corresponds to a  patch in the input modality. We consider intersection scenarios from Town03 and Town05 and examine the top-5 attention weights for the 66 tokens in the 2, 3 and 4 rows of the image feature map and the 24 tokens in the 4, 5 and 6 rows of the LiDAR feature map. We select these tokens since they correspond to the intersection region in the input modality and contain traffic lights and vehicles.

We compute statistics on cross-modal attention for image and LiDAR feature tokens. Specifically, we report the \% of tokens for which at least one of the top-5 attended tokens belong to the other modality for each head of the 4 transformers (T1, T2, T3, T4) in~\tabref{tab:attention_statistics}. We observe that the earlier transformers have negligible LiDAR to image attention compared to later transformers in which nearly all the LiDAR tokens aggregate information from the image features. Furthermore, different heads of each transformer also show distinctive behavior, \eg head 3 of T2 has significantly less cross-attention for image tokens than other heads, head 2 of T3 has very little cross-attention whereas head 4 has significantly higher cross-attention for image tokens compared to other heads. Overall, T4 exhibits extensive cross-attention for both image and LiDAR tokens, which indicates that TransFuser is effective in aggregating information between the two modalities.

We show four cross-attention visualizations for T4 in~\figref{fig:attn_map}. We observe a common trend in attention maps: TransFuser attends to areas near vehicles and traffic lights at intersections, albeit at a slightly different location in the image and LiDAR feature maps. Additional visualizations for all the transformers are provided in the supplementary.

\subsection{Global Safety Heuristic}
\label{sec:safety}
\red{The primary motivation of the safety heuristic described in \secref{sec:control} is to prevent collisions during the applied creeping behavior. Therefore, in Tables \ref{tab:detailed_results} and \ref{tab:leaderboard}, we apply the safety heuristic during creeping only. However, rule-based fallback systems have been shown to improve the safety of IL models~\cite{Vitelli2021ARXIV}. In this experiment, we investigate the impact of applying the safety heuristic described in \secref{sec:control} globally, \ie during both creeping and regular driving. We show results on both the Longest6 benchmark and the CARLA leaderboard. To reduce the computational overhead of this analysis, we evaluate a single model instead of an ensemble of 3 different training runs, which were used in Tables \ref{tab:detailed_results} and \ref{tab:leaderboard}. However, for clarity, we also report the scores of the ensemble. Additionally, the results shown for Latent TransFuser are from preliminary experiments where we include a LiDAR sensor and use the LiDAR-based safety heuristic instead of the CenterNet based intersection check described in \secref{sec:ltf}, leading to minor differences when compared to the numbers from Tables \ref{tab:detailed_results} and \ref{tab:leaderboard}.

The results are shown in \tabref{tab:safety}. For Latent TransFuser, we observe that the global safety heuristic improves the DS by 8 points on the Longest6 benchmark and 5 points on the leaderboard compared to it being applied during creeping only. In particular, this is due to a large improvement in the IS (\eg from 0.51 to 0.72 on the leaderboard). Interestingly, we observe a different trend for TransFuser. The global safety heuristic improves the DS by 7 points on the Longest6 benchmark where we tuned the hyper-parameters (\ie, size of the rectangular safety box). However, it leads to a drop of nearly 10 points in DS on the leaderboard. The global safety heuristic leads to reduced route completion for both methods on the CARLA leaderboard. This indicates that the heuristic works well on observed maps, but does not generalize to unknown and potentially unseen road layouts. For TransFuser, which already had a higher infraction score than Latent TransFuser, the improvement in IS does not make up for the reduction in RC leading to an overall reduction in DS through the safety heuristic.

When comparing the results of a single model and the corresponding ensemble, we find that ensembling improves the DS for both Latent TransFuser and TransFuser on the leaderboard, in particular for TransFuser which improves by more than 10 points. On the Longest6 routes, the ensembling has a lower impact of 5 points for the TransFuser and even a reduction in performance for Latent TransFuser. The single models reported in \tabref{tab:safety} are the first of three training seeds (\tabref{tab:trainseedvar}). Ensembling might have a larger positive impact on TransFuser because the models had a larger training variance, which we discuss in the following.}

\begin{table}[t]
\small
    \setlength{\tabcolsep}{6pt}
    \centering
    \begin{tabular}{c| c | c c c c }
        \textbf{Training} & \textbf{Eval} & \textbf{LTF} & \textbf{LF} & \textbf{GF} & \textbf{TF} \\
        \hline
        \multirow{4}{*}{1} & 1 & 48.82 & 31.94 & \red{46.13} & 59.45 \\
        & 2 & 50.12 & 33.29 & \red{43.62} & 44.15 \\
        & 3 & 51.07 & 34.79 & \red{39.42} & 44.87 \\
        & \red{avg.} & \red{50.00} & \red{33.34} & \red{43.06} & \red{49.49} \\
        \hline
        \multirow{4}{*}{2} & 1 & \red{44.53} & 37.05 & \red{36.64} & 51.50 \\
        & 2 & \red{54.35} & 36.79 & \red{39.40} & 50.91 \\
        & 3 & \red{52.57} & 47.98 & \red{34.93} & 52.35 \\
        & \red{avg.} & \red{50.48} & \red{40.60} & \red{36.99} & \red{51.59} \\
        \hline
        \multirow{4}{*}{3} & 1 & \red{51.15} & 34.90 & \red{49.77} & 59.76 \\
        & 2 & \red{48.10} & 32.47 & \red{45.64} & 57.39 \\
        & 3 & \red{49.80} & 44.98 & \red{52.47} & 52.88 \\
        & \red{avg.} & \red{49.68} & \red{37.45} & \red{49.30} & \red{56.68} \\
        \hline
    \end{tabular}
    \caption{\textbf{Training and evaluation variance.} We show the DS of each evaluation on the Longest6 benchmark. We train each baseline 3 times, and perform 3 evaluation runs of each individual trained model. LTF: Latent TransFuser, LF: Late Fusion, GF: Geometric Fusion, TF: TransFuser. All models exhibit large variance in scores.}
    \label{tab:trainseedvar}
    \vspace{-0.0cm}
\end{table}

\boldparagraph{Training Seed Variance} We show the impact of training and evaluation seed variance in \tabref{tab:trainseedvar}. We train each baseline 3 times and evaluate each model 3 times on the Longest6 routes \red{with the global safety heuristic enabled}. We observe that the best achieved score can differ from the worst score by 10-15 points for an individual model, leading to extremely large variance. In particular, for the first seed of TransFuser, the DS ranges from 44.15 to 59.45. For Geometric Fusion, \red{the average DS differs by 12 points between the worst and best training seed.} This amount of variance is problematic when trying to analyze or compare different approaches. We would like to emphasize that the variance reported in \tabref{tab:trainseedvar} comes from two factors. The training variance between different seeds results from different network initializations, data sampling and data augmentations during optimization. The evaluation variance is a result of the variation in the traffic manager, physics and sensor noise of CARLA 0.9.10. Based on the results observed, the randomness in evaluation is the primary cause of variance, in addition to secondary training seed variance, but both factors are considerable. The existing practice for state-of-the-art methods on CARLA is to report only the evaluation variance by running multiple evaluations of a single training seed. \red{This may lead to premature conclusions (\eg when considering only the three evaluations of the first training seed, Latent TransFuser outperforms TransFuser).} We argue (given these findings) that future studies should report results by varying the training seed for both the baselines and proposed new methods, in addition to the results of the best seed or ensemble.

\subsection{Ablation Studies}
\label{sec:ablation}

We now analyze several design choices for TransFuser in a series of ablation studies on the Longest6 benchmark. \red{Since the global safety heuristic leads to consistent and significant improvements for TransFuser on the Longest6 routes (\tabref{tab:safety}), we use this setting for the ablation studies.} The evaluation is repeated 3 times for each experiment, however, we use a single training run for these results instead of an ensemble of 3 different training runs as in \tabref{tab:detailed_results}. \red{To further reduce the computational overhead, we always evaluate epoch 31, as we have observed in preliminary experiments that it is usually close to the best epoch in performance.} \red{For the default configuration, we have 3 available training runs. We report the best and worst training seed to account for the randomness due to training. Ablations lying within this interval likely do not have a large impact.}

\boldparagraph{Auxiliary Tasks} As described in \secref{sec:loss}, we consider 4 auxiliary tasks in this work. In \tabref{tab:ablation_aux} we show the performance of TransFuser when all these auxiliary losses are removed, as well as the impact of removing each loss independently. We observe that with no auxiliary tasks, there is a steep drop in RC from 92.28 to 78.17. \red{Removing only a single auxiliary task does not have a large impact. All results lie within the range between the best and worst seed of the default configuration in terms of driving score.} In \figref{fig:qual}, we visualize the predictions made by TransFuser when trained with all 4 auxiliary losses.

\begin{figure}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{gfx/rgb_305.png}
        \caption{Top to bottom: image input, predicted depth, predicted semantics (legend: {\color{color_unlabled}none}, {\color{color_road}road}, {\color{color_roadline}lane}, {\color{color_sidewalk}sidewalk}, {\color{color_vehicle}vehicle}, {\color{color_person}person}).}
        \label{fig:qual1}
    \end{subfigure}
    \hfill
    \vspace{0.15cm}
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{gfx/lidar_305.png}
        \caption{Left to right: LiDAR ground plane channel, bounding box predictions overlaid on LiDAR obstacle channel (points above ground plane), HD map prediction.}
        \label{fig:qual2}
    \end{subfigure}
    \vspace{0.0cm}
    \caption{\label{fig:qual}\textbf{Visualization of Auxiliary Tasks.} We visualize the inputs and outputs of both the image branch and LiDAR branch for the same driving scene. Further, the input target point is visualized as a red circle and predicted waypoints as blue and white circles on the HD map prediction. The first two waypoints (which are used to obtain the steering angle for our PID controller) are shown as blue circles, and the remaining two waypoints as white circles.}
\end{figure}

\begin{table}[t]
\small
    \setlength{\tabcolsep}{8pt}
    \centering
    \begin{tabular}{c | c c c }
        \textbf{Auxiliary Losses} & \textbf{DS}  & \textbf{RC}  & \textbf{IS}  \\
        \hline
        None & 44.29 & 78.17 & 0.58 \\
        \hline
        No Depth & \red{56.23} & \red{91.80} & 0.61\\
        No Semantics & 53.76 & 88.40 & 0.61\\
        No HD Map & 50.96 & 89.52&0.58 \\
        No Vehicle Detection & 53.43 & 88.49 & 0.60\\
        \hline
        \red{All Losses (Worst Seed)} & \red{49.49} & \red{90.67} & \red{0.55} \\
        All Losses \red{(Best Seed)} & \textbf{56.68} &  \textbf{92.28} &  \textbf{0.62}\\
        \hline
    \end{tabular}
    \caption{\textbf{Auxiliary Tasks.} The results shown are the mean over 3 evaluations on Longest6. \red{Training without auxiliary losses leads to a significant reduction in RC and DS.}}
    \label{tab:ablation_aux}
    \vspace{-0.0cm}
\end{table}

\begin{table}[t]
\small
    \setlength{\tabcolsep}{5pt}
    \centering
    \begin{tabular}{c| c | c c c}
        \textbf{Parameter} & \textbf{Value} & \textbf{DS}  & \textbf{RC}  & \textbf{IS}  \\
        \hline
        \multirow{2}{*}{\red{Fusion Direction}} & \red{LiDAR  Camera} & \red{46.36} & \red{87.46} & \red{0.55} \\
         & \red{Camera  LiDAR} & \red{47.99} & \red{86.24} & \red{0.57} \\
         \hline
        \multirow{3}{*}{Fusion Scales} & 1 & 49.35 & 84.47&0.57 \\
        & 2 & 53.52 &91.78 & 0.59\\
        & 3 & 48.77 & 85.33& 0.60\\
        \hline
        \multirow{3}{*}{Attention \red{L}ayers} & 2 & 53.49 & 90.65 & 0.60 \\
        & 6 & 56.24 & {92.56} & 0.61\\
        & 8 & \red{56.13} & \red{\textbf{92.61}} & \red{0.61}\\
        \hline
         \multirow{2}{*}{\red{Token Count}} & \red{113 + 88} & \red{45.63} & \red{90.32} & \red{0.51} \\
         & \red{225 + 44} & \red{49.14} & \red{87.10} & \red{0.56} \\
        \hline
         \red{Averaging} & \red{Attention Token} & \red{54.21} & \red{90.78} & \red{0.60} \\
        \hline
        \multirow{4}{*}{Backbones} & Res34-Res18 & 42.01 & 92.43 & 0.45\\
         & Reg0.8-Reg0.8 & 44.38 & 87.96& 0.50\\
         & Reg1.6-Reg1.6 & 47.80 & 91.62& 0.52\\
         & \red{NeXt-T-NeXt-T} & \red{48.27} & \red{92.30} & \red{0.53} \\ 
        \hline
        \multirow{2}{*}{\red{Default Config}} & \red{Worst Seed} & \red{49.49} & \red{90.67} & \red{0.55} \\
        & \red{Best Seed} & \textbf{56.68} & 92.28 & \textbf{0.62} \\
        \hline
    \end{tabular}
    \caption{\textbf{Architecture Ablations.} The results shown are the mean over 3 evaluations on Longest6. \red{The default configuration fuses in both directions. It uses} 4 fusion scales, 4 attention layers, \red{225 + 88 tokens, global average pooling}, and RegNetY-3.2GF backbones. \red{The encoder backbone has the highest impact on the final driving score.}}
    \label{tab:ablation_arch}
    \vspace{-0.0cm}
\end{table}

\boldparagraph{Architecture} In \tabref{tab:ablation_arch}, we analyze the impact of varying the TransFuser encoder architecture. \red{We study the importance of fusion in both directions by selectively removing the residual output connections from the fusion transformers to the convolutional backbones. Fusion for only the Camera  LiDAR or only the LiDAR  Camera direction gives a slightly lower performance that the default model with bi-directional fusion.} Removing the fusion mechanism in the early blocks of the encoder and performing feature fusion at only the deepest 1, 2 and 3 scales also leads to a small drop in performance. For the fusion transformers, we find that \red{2-8} attention layers give similar performance. \red{The default resolution of the image features is 225 and the LiDAR features is 88. We observe that using each of these 225 + 88 features as independent input tokens for the transformer leads to better results when compared to a fusing information across different image and LiDAR resolutions through a reduced image token count of 113 or LiDAR token count of 44. We also evaluate a version of TransFuser where the input to the MLP and GRU decoder from the final fusion transformer is obtained via a dedicated attention token instead of the default global average pooling. This is a standard idea for attention-based averaging of spatial features, similar to the CLS token of Vision Transformers~\cite{Dosovitskiy2021ICLR}. We find that the default architecture with global average pooling is simpler to implement and performs similarly in practice.} The most impactful architecture choice is the backbone architecture for both branches. The default configuration of RegNetY-3.2GF backbones outperforms \red{ConvNeXt-Tiny~\cite{Liu2022ARXIV}}, RegNetY-1.6GF and RegNetY-0.8GF based backbones. We also observe a large improvement over the use of a ResNet34 for the image branch and ResNet18 for the BEV branch, as in \cite{Prakash2021CVPR}, which leads to a model with lower network capacity.

\boldparagraph{Model Inputs} As shown in \tabref{tab:ablation_input}, increasing the LiDAR range or reducing the camera FOV from the default configuration leads to a reduced IS and a corresponding drop in DS. \red{Our method works with arbitrary grids as inputs. Therefore, it could benefit from orthogonal improvements in LiDAR encoding. However, we did not observe improvements by using a learned LiDAR encoder~\cite{Lang2019CVPR}, and hence stick with the simpler voxelization approach. This model was trained with batch size 10 due to its higher memory requirements.} Interestingly, despite being useful in our preliminary experiments, removing the rasterized goal location channel from the LiDAR branch, or removing the random rotation of sensor inputs by 20 used during data augmentation show only a small impact on the performance in the final configuration which is unlikely to be significant.

\boldparagraph{Inertia Problem} As we note in \secref{sec:control}, we add creeping to our controller to prevent the agent from being overly cautious. This type of behavior, called the inertia problem~\cite{Codevilla2019ICCV} is typically attributed to the spurious correlation that exists between input velocity and output acceleration in an IL dataset. Interestingly, though we do not use velocity as an input to our models, we observe that creeping in the controller increases the RC significantly while maintaining a similar IS (\tabref{tab:ablation_creeping}). This indicates that a factor besides the velocity input, such as an imbalance in training data distribution, may be a key contributing factor to the inertia problem. We also train a version of TransFuser where we provide the current velocity as input by projecting the scalar value into the same dimensions as the transformer positional embedding using a linear layer. This velocity embedding is combined with the learnable positional embedding through element-wise summation and fed into the transformer at all 4 stages of the backbone. Including the velocity input leads to a sharp drop in DS, which cannot be recovered through the creeping behavior.

\begin{table}[t]
\small
    \setlength{\tabcolsep}{5pt}
    \centering
    \begin{tabular}{c| c | c c c}
        \textbf{Parameter} & \textbf{Value} & \textbf{DS}  & \textbf{RC}  & \textbf{IS}  \\
        \hline
        \multirow{2}{*}{LiDAR Range} & 64m  32m & 49.08 & 91.10 & 0.54\\
        & 64m  64m & 47.55 & 90.72 & 0.52\\
        \hline
        \red{LiDAR Encoder} & \red{PointPillars} & \red{50.83} & \red{91.56} & \red{0.55}\\
        \hline
        \multirow{2}{*}{Camera FOV} & 120 & 49.90 & 90.05& 0.56\\
        & 90 & 42.18 & 88.49& 0.51\\
        \hline
        No Rasterized Goal & - & 54.80 & 91.63 & 0.60\\
        No Rotation Aug & - & \textbf{56.85} & \textbf{92.73} & 0.61 \\
        \hline
        \multirow{2}{*}{\red{Default Config}} & \red{Worst Seed} & \red{49.49} & \red{90.67} & \red{0.55} \\
        & \red{Best Seed} & 56.68 & 92.28 & \textbf{0.62} \\
        \hline
    \end{tabular}
    \caption{\textbf{Model Input Ablations.} The results shown are the mean over 3 evaluations on Longest6. The default configuration \red{uses} a 32m  32m LiDAR range and 132 camera FOV. \red{Camera FOV has the largest impact on the DS.}}
    \label{tab:ablation_input}
    \vspace{-0.0cm}
\end{table}

\begin{table}[t]
\small
    \setlength{\tabcolsep}{6pt}
    \centering
    \begin{tabular}{c| c | c c c }
        \textbf{Velocity Input?} & \textbf{Creeping?} & \textbf{DS}  & \textbf{RC}  & \textbf{IS}  \\
        \hline
        \multirow{2}{*}{-} & - & 46.35 & 78.28 & 0.63\\
        &  & \textbf{56.68} & \textbf{92.28} & 0.62 \\
        \hline
        \multirow{2}{*}{} & - & 37.34 & 64.27 & \textbf{0.65}\\
        &  & 45.35 & 86.22 & 0.52\\
        \hline
    \end{tabular}
    \caption{\textbf{Inertia Problem.} The results shown are the mean over 3 evaluations on Longest6. Creeping improves the RC in both the setting where we input the velocity to our encoder and our default configuration (no velocity input).}
    \label{tab:ablation_creeping}
    \vspace{-0.0cm}
\end{table}
