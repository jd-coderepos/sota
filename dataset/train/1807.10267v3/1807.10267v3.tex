\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{sampling_neighbor.jpg}
\end{center}
\caption{Sampling from the latent space of the mesh autoencoder around the mean face $j=0$ along 3 different components.}
\label{fig:latent_space}
\end{figure}

\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{kld_samplingcomp02.jpg}
\end{center}
\caption{Sampling using Gaussian noise with variational loss (bottom), and without(top). With $w_{kld}=0$, the latent representation might not have a Gaussian distribution. Hence, samples on top are not diverse.}
\label{fig:vae}
\end{figure}

\section{Experiments}
In this section, we evaluate the effectiveness of CoMA on an extreme facial expression dataset. We demonstrate that CoMA allows the synthesis of new expressive faces by sampling from the latent space in Section~\ref{sec:latent_sampling}, including the effect of adding variational loss. Following, we compare CoMA to the widely used PCA representation for reconstructing expressive 3D faces. For this, we evaluate in Section~\ref{sec:comparison_pca} the ability to reconstruct data similar to the training data (interpolation experiment), and the ability to reconstruct expressions not seen during training (extrapolation experiment). Finally, in Section~\ref{sec:deep_FLAME}, we show improved performance by replacing the expression space of state of the art face model, FLAME \cite{FLAME2017} with our autoencoder.


\subsection{Facial Expression Dataset}
Our dataset consists of 12 classes of extreme expressions from 12 different subjects. These expressions are complex and asymmetric. The expression sequences in our dataset are -- bareteeth, cheeks in, eyebrow, high smile, lips back, lips up, mouth down, mouth extreme, mouth middle, mouth side and mouth up. We show samples from our dataset and the number of frames of each captured sequence in the Supplementary Material.

The data is captured at 60fps with a multi-camera active stereo system
(3dMD LLC, Atlanta) with six stereo camera pairs, five speckle projectors, and
six color cameras.  Our dataset contains 20,466 3D Meshes, each with about 120,000 vertices. The data is pre-processed using a sequential mesh registration method~\cite{FLAME2017} to reduce the data dimensionality to 5023 vertices.

\subsection{Sampling the Latent Space}
\label{sec:latent_sampling}
Let $E$ be the encoder and $D$ be the decoder.
We first encode a face mesh from our test set in the latent space to obtain features $z=E(\mathcal{F})$. We then vary each of the components of the latent vector as $\tilde{z}_i = z_i + \epsilon$. We then use the decoder to transform the latent vector into a reconstructed mesh $\mathcal{\tilde{F}} =D(\tilde{z}) $. In Figure \ref{fig:latent_space}, we show a diversity of face meshes sampled from the latent space. Here, we extend or contract the latent vector along different dimensions by a factor of 0.3 such that $\tilde{z}_i = (1+0.3j) z_i $, where $j$ is the step. In Figure \ref{fig:latent_space}, $j \in [-4, 4]$, and the mean face $\mathcal{F}$ is shown in the middle of the row. More examples are shown in the Supplementary Material. \\

\qheading{Variational Convolutional Mesh Autoencoder.}
Although 3D faces can be sampled from our convolutional mesh autoencoder, the distribution of the latent space is not known. Therefore, sampling requires a mesh to be encoded in that space. In order to constrain the distribution of the latent space, we add a variational loss on our model. Let $E$ be the encoder, $D$ be the decoder, and $z$ be the latent representation of face $\mathcal{F}$. We minimize the loss,
\begin{equation}
	l = ||\mathcal{F} - D(z) ||_1 + w_{kld} KL(\mathcal{N}(0,1)||Q(z|\mathcal{F})),
\end{equation}
where $w_{kld}=0.001$ weights the $KL$ divergence loss. The first term minimizes the L1 reconstruction error, and the second term enforces a unit Gaussian prior $\mathcal{N}(0,1)$ with zero mean on the distribution of latent vectors $Q(z)$. This enforces the latent space to be a multivariate Gaussian. In Figure \ref{fig:vae}, we show visualizations by sampling faces from a Gaussian distribution on this space within $[-3\sigma, 3\sigma]$, where $\sigma=1$, is the variance of the Gaussian prior. We compare the visualizations by setting $w_{kld}=0$. We observe that $w_{kld}=0$ does not enforce any Gaussian prior on $P(z)$, and therefore sampling with Gaussian noise from this distribution results in limited diversity in face meshes. We show more examples in the Supplementary Material.


\subsection{Comparison with PCA Spaces}
\label{sec:comparison_pca}
Several face models use PCA space to represent identity and expression variations ~\cite{Tewari2017,FLAME2017,Amberg2008,Breidt2011,Yang2011}. We perform interpolation and extrapolation experiments to evaluate our performance. We use Scikit-learn~\cite{scikit-learn} to compute PCA coefficients. We consistently use an 8-dimensional latent space to encode the face mesh using both the PCA model and Mesh Autoencoder.

\begin{figure}[t]
  \begin{minipage}[b]{0.5\textwidth}
    \begin{center}
    \includegraphics[width=\textwidth]{interp_cum_error.png} \\
    (a)
    \end{center}
\end{minipage}
 \hfil
  \begin{minipage}[b]{0.5\textwidth}
    \begin{center}
    \includegraphics[width=\textwidth]{cum_extrap.png} \\
    (b)
	\end{center}
\end{minipage}
\caption{Cumulative Euclidean error between PCA model and Mesh Autoencoder for Interpolation (a) and Extrapolation (b) experiments}
    \label{fig:cum_error1}
\end{figure}


\begin{figure}[t]
\begin{center}
\includegraphics[width=\linewidth]{interp.jpg}
\end{center}
\vspace{-4mm}
\caption{Comparison with PCA: Qualitative results for interpolation experiment}
\label{fig:interp}
\end{figure}

\qheading{Interpolation Experiment.}
In order to evaluate the interpolation capability of the autoencoder, we split the dataset in training and test samples with a ratio of 9:1. The test samples are obtained by picking consecutive frames of length 10  uniformly at random across the sequences. We train CoMA for 300 epochs and evaluate it on the test set. We use Euclidean distance for comparison with the PCA method. The mean error with standard deviation, and median errors are shown in Table \ref{tab:interp} for comparison.

\begin{table}
\begin{center}
\caption{Comparison with PCA: Interpolation experiments. Errors are in millimeters}
\begin{tabular}{l|cc|c}
     &  Mean Error      & Median Error & \# Parameters \\ \hline
PCA  & 1.639 $\pm$ 1.638 & 1.101& 120,552 \\
Mesh Autoencoder  & \textbf{0.845 $\pm$ 0.994} & \textbf{0.496}  & \textbf{33,856}\\
\end{tabular}
\label{tab:interp}
\end{center}
\end{table}

We observe that our reconstruction error is 50\% lower than PCA. At the same time, the number of parameters in CoMA is about 75\% fewer than the PCA model as shown in Table~\ref{tab:interp}. Visual inspection of our qualitative results in Figure \ref{fig:interp} shows that our reconstructions are more realistic and are effective in capturing extreme facial expressions. We also show the histogram of cumulative errors in Figure \ref{fig:cum_error1}a. We observe that our Mesh Autoencoder (CoMA) has about 72.6\% of the vertices within a Euclidean error of 1 mm, as compared to 47.3\% for the PCA model.



\begin{figure}[h]
\begin{center}
\includegraphics[width=\linewidth]{extrap.jpg}
\end{center}
\caption{Comparison with PCA: Qualitative results for extrapolation experiment}
\label{fig:extrap}
\end{figure}

\begin{table}[t]
\begin{center}
\caption{Comparison with PCA: Extrapolation experiment. Errors are in millimeters.}
\begin{tabular}{l|cc|cc|cc}
&  \multicolumn{2}{|c}{Mesh Autoencoder} & \multicolumn{2}{|c}{PCA} & \multicolumn{2}{|c}{FLAME \cite{FLAME2017}} \\
Sequence   &  Mean Error & Median & Mean Error & Median & Mean Error & Median \\ \hline
bareteeth & \textbf{1.376$\pm$1.536} & \textbf{0.856}  &1.957$\pm$1.888 & 1.335 & 2.002$\pm$1.456 & 1.606 \\
cheeks in & \textbf{1.288$\pm$1.501} & \textbf{0.794}  &1.854$\pm$1.906 & 1.179 & 2.011$\pm$1.468 & 1.609 \\
eyebrow &  \textbf{1.053$\pm$1.088} & \textbf{0.706} &1.609$\pm$1.535 & 1.090 & 1.862$\pm$1.342 & 1.516 \\
high smile & \textbf{1.205$\pm$1.252} & \textbf{0.772} & 1.841$\pm$1.831 & 1.246 & 1.960$\pm$1.370 & 1.625 \\
lips back & \textbf{1.193$\pm$1.476} & \textbf{0.708} & 1.842$\pm$1.947 & 1.198 & 2.047$\pm$1.485 & 1.639 \\
lips up & \textbf{1.081$\pm$1.192} & \textbf{0.656} & 1.788$\pm$1.764 & 1.216 & 1.983$\pm$1.427 & 1.616 \\
mouth down & \textbf{1.050$\pm$1.183} & \textbf{0.654} & 1.618$\pm$1.594 & 1.105 & 2.029$\pm$1.454 & 1.651 \\
mouth extreme &\textbf{1.336$\pm$1.820} & \textbf{0.738}  & 2.011$\pm$2.405 & 1.224 & 2.028$\pm$1.464 & 1.613 \\
mouth middle & \textbf{1.017$\pm$1.192} & \textbf{0.610}  &1.697$\pm$1.715 & 1.133 & 2.043$\pm$1.496 & 1.620 \\
mouth open & \textbf{0.961$\pm$1.127} & \textbf{0.583}  &1.612$\pm$1.728 & 1.060 & 1.894$\pm$1.422 & 1.544 \\
mouth side &  \textbf{1.264$\pm$1.611} & \textbf{0.730}  &1.894$\pm$2.274 & 1.132& 2.090$\pm$1.510 & 1.659 \\
mouth up & \textbf{1.097$\pm$1.212} & \textbf{0.683}  &1.710$\pm$1.680 & 1.159 & 2.067$\pm$1.485 & 1.680
\end{tabular}
\vspace{-4mm}
\label{tab:extrap}
\end{center}
\end{table}

\qheading{Extrapolation Experiment.} To measure generalization of our model, we compare the performance of CoMA with the PCA model and FLAME \cite{FLAME2017}. For comparison, we train the expression model of FLAME on our dataset. The FLAME reconstructions are obtained with latent vector size of 16 with 8 components each for encoding identity and expression. The latent vectors encoded using the PCA model and Mesh autoencoder have a size of 8.

To evaluate generalization capability of our model, we reconstruct the expressions that are completely unseen by our model. We perform 12 different experiments for evaluation. For each experiment, we split our dataset by completely excluding one expression set from all the subjects of the dataset. We test our Mesh Autoencoder on the excluded expression. We compare the performance of our model with PCA and FLAME using the Euclidean distance (mean, standard deviation, median). We perform 12 fold cross validation, one for each expression as shown in Table \ref{tab:extrap}.
In Table \ref{tab:extrap}, we also show that our model performs better than PCA and FLAME \cite{FLAME2017} on all expression sequences. We show the qualitative results in Figure \ref{fig:extrap}.
We show the cumulative Euclidean error histogram in Figure \ref{fig:cum_error1}b. For a 1 mm accuracy, Mesh Autoencoder captures 63.8\% of the vertices while the PCA model captures 45\%.


\subsection{DeepFLAME}
\label{sec:deep_FLAME}
FLAME~\cite{FLAME2017} is a state of the art model for face representation that combines linear blendskinning for head and jaw motion with linear PCA spaces to represent identity and expression shape variations. To improve the reconstruction error of FLAME, we replace the PCA expression space of FLAME with our autoencoder, and refer to the new model as DeepFLAME. We compare the performance of DeepFLAME with FLAME by varying the size of the latent vector for encoding. Head rotations are factored out for comparison since they are well modeled by linear blendskinning in FLAME, and we consider only the expression space. The reconstruction accuracy is measured using Euclidean distance metric. We show the comparisons in Table \ref{tab:ablexp}. The median reconstruction of DeepFLAME is lower for all chosen latent space dimensions, while the mean reconstruction error is lower for up to 12 latent variables. This shows that DeepFLAME provides a more compact face representation; i.e., captures more shape variation with fewer latent variables.



\begin{table}[t]
\begin{center}
\caption{Comparison of FLAME and DeepFLAME. DeepFLAME is obtained by replacing expression model of FLAME with CoMA. All errors are in millimeters.}
\begin{tabular}{l|cc|cc}
&  \multicolumn{2}{|c}{DeepFLAME} & \multicolumn{2}{|c}{FLAME \cite{FLAME2017}}  \\
\#dim of $z$   &  Mean Error & Median & Mean Error & Median  \\ \hline
2 & \textbf{0.610$\pm$0.851} & \textbf{0.317} & 0.668$\pm$0.876 & 0.371 \\
4 &  \textbf{0.509$\pm$0.746} & \textbf{0.235} &0.589$\pm$0.803 & 0.305 \\
6 &  \textbf{0.464$\pm$0.711} & \textbf{0.196} &0.525$\pm$0.743 & 0.252 \\
8 &  \textbf{0.432$\pm$0.681} & \textbf{0.169} &0.477$\pm$0.691 & 0.217 \\
10 &  \textbf{0.421$\pm$0.664} & \textbf{0.162} &0.439$\pm$0.655 & 0.193 \\
12 &  \textbf{0.388$\pm$0.630} & \textbf{0.139} &0.403$\pm$0.604 & 0.172 \\
14 &  0.371$\pm$0.605 & \textbf{0.128} &\textbf{0.371$\pm$0.567} & 0.152 \\
16 &  0.372$\pm$0.611 & \textbf{0.125} &\textbf{0.351$\pm$0.543} & 0.139
\end{tabular}
\label{tab:ablexp}
\end{center}
\end{table}















\subsection{Discussion}

The focus of CoMA is to model facial shape for reconstruction applications. The Laplace-Beltrami operator (LBo) describes the intrinsic surface geometry and is invariant under isometric surface deformations. This isometry invariance of the LBo is beneficial for shape matching and registration. Since changes in facial expression are near isometric deformations~\cite[Section 13.3]{Bronstein2008}, applying LBo to expressive faces would result in a loss of most expression-related shape variations, making it infeasible to model such variations. The graph Laplacian used by CoMA in contrast to the LBo is not isometry invariant.

While we evaluate CoMA on face shapes, it is applicable to any class of objects. Similar to existing statistical models however, it requires all meshes in dense vertex correspondence; i.e. all meshes need to share the same topology. A future research direction is to directly learn a 3D face representation from raw 3D face scans or 2D images without requiring vertex correspondence.

As is also true for other deep learning based models, the performance of CoMA could further improve with more training data. The amount of existing 3D face data however is very limited. The data scarcity especially limits our expression model to outperform existing models for higher latent space dimensions ($> 12$ see Table~\ref{tab:ablexp}). We predict superior quality on larger datasets and plan to evaluate CoMA on significantly more data in the future.

As CoMA is an end-to-end trained model, it could also be combined with some existing image convolutional network to regress the 3D face shape from 2D images. We will explore this in future work. 





