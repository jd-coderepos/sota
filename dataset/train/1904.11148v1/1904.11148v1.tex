









\documentclass[journal]{IEEEtran}

















\usepackage{cite}





\usepackage{multirow}
\usepackage{fixmath}
\usepackage{amssymb,amsmath,bm}
\usepackage{cases}


\ifCLASSINFOpdf
   \usepackage[pdftex]{graphicx}
\else
\usepackage[dvips]{graphicx}
\fi
























\ifCLASSOPTIONcompsoc
  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\else
  \usepackage[caption=false,font=footnotesize]{subfig}
\fi


























\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\title{Divide and Conquer: A Deep CASA Approach to Talker-independent Monaural Speaker Separation}


\author{Yuzhou~Liu 
        and~DeLiang~Wang\thanks{This work was supported in part by two NIDCD Grants (R01 DC012048 and R01 DC015521), and in part by the Ohio Supercomputer Center.}\thanks{Y. Liu is with the Department of Computer Science and Engineering, The Ohio State University, Columbus, OH 43210-1277 USA (e-mail: liuyuz@cse.ohio-state.edu).}\thanks{
D. L. Wang is with the Department of Computer Science and Engineering and the Center for Cognitive and Brain Sciences, The Ohio State University, Columbus, OH 43210-1277 USA (e-mail: dwang@cse.ohio-state.edu).
}}





\markboth{}{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Journals}













\maketitle

\begin{abstract}


We address talker-independent monaural speaker separation from the perspectives of deep learning and computational auditory scene analysis (CASA).
Specifically, we decompose the multi-speaker separation task into the stages of simultaneous grouping and sequential grouping. 
Simultaneous grouping is first performed in each time frame by separating the spectra of different speakers with a permutation-invariantly trained neural network.
In the second stage, the frame-level separated spectra are sequentially grouped to different speakers by a clustering network.
The proposed deep CASA approach optimizes frame-level separation and speaker tracking in turn, and produces excellent results for both objectives.
Experimental results on the benchmark WSJ0-2mix database show that the new approach achieves the state-of-the-art results with a modest model size.

  \end{abstract}

\begin{IEEEkeywords}
Monaural speech separation, speaker separation, computational auditory scene analysis, deep CASA.\end{IEEEkeywords}






\IEEEpeerreviewmaketitle



\section{Introduction}

    


   
      \IEEEPARstart{S}{peech} usually occurs simultaneously with interference in real acoustic environments.
   Interference suppression is needed in a wide variety of speech applications, including automatic speech recognition, speaker identification, and hearing aids. 
   One particular kind of interference is the speech signal from competing speakers.
  Although human listeners excel at attending to a target speaker even without any spatial cues \cite{human-ss1}, speech separation remains a challenge for machines despite decades of research. In this study, we address monaural (one microphone) speaker separation, mainly in the case of two concurrent speakers, which is also known as co-channel speech separation.
   
   
A traditional approach to monaural speech separation is computational auditory scene analysis (CASA) \cite{CASA}, which is inspired by human auditory scene analysis (ASA) mechanisms \cite{ASA}. CASA addresses speech separation in two main stages: simultaneous grouping and sequential grouping. 
With an acoustic mixture decomposed into a matrix of time-frequency (T-F) units, simultaneous grouping aggregates T-F units overlapping in time to short segments, each originating from the same source. 
In sequential grouping, segments are grouped across time into auditory streams, each corresponding to a distinct source.
For example, an unsupervised speaker separation method \cite{Ke-unsupervised} first generates T-F segments based on pitch and onset/offset analysis, and then uses clustering to sequentially group T-F segments into speakers.

Recently deep learning has been employed to address speaker separation.
The general idea is to train a deep neural network (DNN) to predict T-F masks or spectra of two speakers in a mixture \cite{Du} \cite{Huang} \cite{Zhang}.
There are usually two output layers in such a DNN, one for an individual speaker. 
These studies assume that the two speakers do not change between training and testing.
It has been shown that such talker-dependent training leads to significant intelligibility improvement for hearing impaired listeners \cite{Masood}.
However, talker-dependent training does not generalize to untrained speakers. Talker-independent speaker separation has to address the permutation problem\cite{DC} \cite{PIT}, i.e., how the output layers are tied to the underlying speakers.
The details of the permutation problem are introduced in Section~\ref{sec:3.1}.



Frame-level permutation invariant training (denoted by tPIT) \cite{PIT} tackles this problem by examining all possible label permutations within each frame during training, and uses the one with the lowest frame-level loss to train the separation network.
A locally optimized output-speaker pairing can thus be reached, which leads to excellent frame-level separation performance.
However, the correct speaker assignment in tPIT's output may swap frequently across frames.
In other words, the frame-level optimized outputs cannot be readily streamed into underlying speakers without reorganization. 
To address this issue, an utterance-level PIT (uPIT) algorithm \cite{PIT} is proposed to align each speaker to a fixed output layer throughout a whole training utterance.
Recent uPIT improvements include new network structure \cite{CBLDNN} \cite{grid} and new training objectives \cite{CBLDNN}.
TasNet \cite{Tasnet} extends uPIT to the waveform domain using a convolutional encoder-decoder structure.
FurcaNeXt \cite{Furca} integrates gated activations and ensemble learning into TasNet, and reports very high performance.

Deep clustering (DC) \cite{DC} looks at the permutation problem from a different perspective.
In DC, a recurrent neural network (RNN) with bi-directional long short-term memory (BLSTM) is trained to assign one embedding vector to each T-F unit of the mixture spectrogram.
The Frobenius norm between the affinity matrix of embedding vectors and the affinity matrix of the ideal speaker assignment (or the ideal binary mask) is used as the training objective.
DC avoids the permutation problem due to the permutation-invariant property of affinity matrices.
As training unfolds, embedding vectors of T-F units dominated by the same source are drawn closer together, and embeddings of those units dominated by different sources become farther apart.
Clustering these embedding vectors using the K-means algorithm assigns each T-F unit to one of the speakers in the mixture, which can be viewed as binary masking for speech separation.
In \cite{DAN}, a concept of attractors is introduced to DC to enable ratio masking and real-time processing.
Alternative training objectives, together with a chimera network which simultaneously estimates DC embeddings and uPIT outputs, are proposed in \cite{alter}.
In \cite{end2end}, iterative phase reconstruction is integrated into the chimera network to alleviate phase distortions.
In \cite{tri}, a phase prediction network is further added to \cite{end2end} to estimate the clean phase of each speaker source.

DC and PIT represent major approaches to talker-independent speaker separation. There are, however, limitations.
As indicated in \cite{PIT} \cite{me}, uPIT sacrifices frame-level performance for better assignments at the utterance level.
The speaker tracking mechanism in uPIT works poorly for same-gender mixtures.
On the other hand, DC is better at speaker tracking, but its frame-level separation is suboptimal compared to ratio masking used in tPIT.

Inspired by CASA, PIT and DC, we proposed a deep learning based two-stage method in our preliminary study \cite{me} to perform talker-independent speaker separation.
The method consists of two stages, a simultaneous grouping stage and a sequential grouping stage.
In the first stage, a tPIT-BLSTM is trained to predict the spectra of the two speakers at each frame without speaker assignment.
This stage separates spectral components of the two speakers at the same frame, corresponding to simultaneous grouping in CASA.
In the sequential grouping stage, frame-level separated spectra and the mixture spectrogram are fed to another BLSTM to predict embedding vectors for the estimated spectra, such that the embedding vectors corresponding to the same speaker are close together, and those corresponding to different speakers are far apart.
A constrained K-means algorithm is then employed to group the two spectral estimates at the same frame across time to different speakers.
This stage corresponds to sequential grouping in CASA.

In this study, we adopt the same divide-and-conquer strategy but improve its realization in major ways, resulting in what we call a deep CASA approach. 
In the simultaneous grouping stage, we utilize a UNet \cite{UNET0} convolutional neural network (CNN) with densely-connected layers \cite{DENSE} to improve the performance of frame-level separation. 
A frequency mapping layer is added to deal with inconsistencies between different frequency bands. 
To overcome the effects of noisy phase in inverse short-time Fourier transform (STFT), we explore complex STFT objectives and time-domain objectives as the training targets.
In the sequential grouping stage, we introduce a new embedding representation and weighted objective function.
In addition, we leverage the latest development in temporal convolutional networks (TCNs) \cite{TCN1} \cite{TCN0} \cite{Tasnet} \cite{ashu}, and use a TCN for sequential grouping, which greatly improves speaker tracking.
A new dropout scheme is proposed for TCNs to overcome the overfitting problem.
The evaluation results and comparisons demonstrate the resulting system achieves better frame-level separation and speaker tracking at the same time  compared to uPIT and \cite{me}. 





	
The rest of the paper is organized as follows. 
Section ~\ref{sec:3} presents details on monaural speaker separation and permutation invariant training. 
    The proposed algorithm, including the simultaneous and sequential grouping stages, is introduced in Section~\ref{sec:4}.
    Section~\ref{sec:5} presents experimental results, comparisons and analysis. 
    Conclusion and related issues are discussed in Section ~\ref{sec:6}.   


    









\section{Monaural speaker separation and permutation invariant training}
\label{sec:3}



\subsection{Monaural Speaker Separation}
\label{sec:3.1}

The goal of monaural speaker separation is to estimate  independent speech signals , , from a single-channel recording of speech mixture , where  and  indexes time. In this work, we focus on the co-channel situation where .

Many deep learning based speaker separation systems \cite{Du} \cite{Huang} \cite{Zhang} address this problem in the T-F domain, where STFT is calculated using an analysis window  with FFT length  and frame shift :


where  and  denote the frame and frequency, respectively. 
The magnitude STFT of the mixture signal , together with other spectral features, are fed into a neural network to predict a T-F mask  for each speaker .
The masks are multiplied by the mixture to estimate the original sources:

Here  denotes element-wise multiplication, and  denotes the estimated magnitude STFT of speaker .
An estimate of complex STFT  can be obtained by coupling  with noisy phase. 
In the end, separated waveforms are resynthesized using inverse STFT (iSTFT):


Various training targets of  have been explored for masking based speech separation \cite{Targets}.
Phase-sensitive approximation (PSA) is found to be effective as it accounts for errors introduced by the noisy phase \cite{PSM} \cite{PIT}.
In PSA, the desired reconstructed signal is defined as: , where  is the phase difference between  and .
Overall, the training loss at each frame is computed as:

where  denotes the  norm.

The above formulation works well only when each output layer is tied to a training target signal with similar characteristics.
For instance, we may tie each output to a specific speaker, leading to talker-dependent training. 
We may also tie two outputs with male and female speakers respectively, leading to gender-dependent training. 
However, for talker-independent training data, how to select output-speaker pairing becomes a nontrivial problem.
Think of a training set consisting of three female speakers.
For speaker 1-2 mixtures, we can tie output1 to speaker1, and output2 to speaker2.
For speaker 1-3 mixtures, again output1 can be tied to speaker1, and output2 tied to speaker3.
However, it is hard to decide the pairing arrangement for speaker 2-3 mixtures. 
If output-speaker pairing is not arranged properly, conflicting gradients may be generated during training, preventing the neural network from converging.
This is referred as the permutation problem \cite{DC} \cite{PIT}. 

\subsection{Permutation Invariant Training }
\label{sec:3.2}

Frame-level PIT \cite{PIT} overcomes the permutation problem by providing target speakers as a set instead of an ordered list,
and output-speaker pairing , for a given frame , is defined as the pairing that minimizes the loss function over all possible speaker permutations . 
For tPIT, the frame-level training loss in Eq.~\ref{eq:loss1} is rewritten as:

We omit  in  and  for brevity.



tPIT does a good job in separating two speakers at the frame level \cite{PIT} \cite{me}.
However, due to its locally optimized training objective, an output layer may be tied to different speakers at different frames, and the correct speaker assignment may swap frequently.
If we reassign the outputs with respect to the minimum loss for each speaker, tPIT can almost perfectly reconstruct both speakers \cite{me}.

Optimal speaker assignments are not obtainable in practice as the targets are not given beforehand. 
To address this issue, uPIT fixes output-speaker pairing  for a whole utterance,
which corresponds to the pairing that provides the minimum utterance-level loss over all possible permutations.

As reported in \cite{PIT} \cite{me}, uPIT considerably improves the separation performance with a default output assignment.
But it has the following shortcomings.
First, uPIT's output-speaker pairing is fixed throughout a whole utterance, which prevents frame-level loss to be optimized as in tPIT. As a result, uPIT always underperforms tPIT if their outputs are optimally reassigned. 
Second, uPIT addresses separation and speaker tracking simultaneously and due to limited modeling capacity of a neural network, uPIT does not work well for speaker tracking, especially for same-gender mixtures. 
    

\section{Deep CASA approach to monaural speaker separation}
\label{sec:4}

We employ a divide and conquer idea to break down monaural speaker separation into two stages.
In the simultaneous grouping stage, a tPIT based neural network separates spectral components of different speakers at the frame-level.
The sequential grouping stage then streams frame-level estimates belonging to the same speaker.
Unlike uPIT, separation and tracking are optimized in turn in the deep CASA framework.
The two stages are detailed in the following subsections.

\subsection{Simultaneous Grouping Stage}
\label{sec:4.1}

\subsubsection{Baseline system}
\label{sec:4.1.1}
We adopt the tPIT framework described in \cite{me} as the baseline simultaneous grouping system.
The magnitude STFT of the mixture is used as the input.
BLSTM is employed as the learning machine.
The system is trained using the loss function in Eq.~\ref{eq:loss2}.
In the end, frame-level spectral estimates are passed to the second stage for sequential grouping.

\subsubsection{Alternative training targets for tPIT} 
\label{sec:4.1.2}

As mentioned, the PSA training target partially accounts for STFT phase, unlike the ideal binary mask (IBM) and ideal ratio mask (IRM).
However, PSA cannot completely restore the phase information in clean sources, because it uses noisy phase during iSTFT.
Recently, complex ratio masking \cite{CRM} (cRM) attempts to restore clean phase. 
The complex ideal ratio mask (cIRM) is defined in the complex STFT domain, with real and imaginary parts.
When applied to the complex STFT of the mixture, it perfectly reconstructs clean sources:

where  denotes point-wise complex multiplication. 


We propose complex ratio masking to perform monaural speaker separation.
Instead of directly using the cIRM as the training target, we first multiply the complex mixture by the estimated complex mask  to perform complex domain reconstruction:

The reconstructed sources are then compared with clean sources to form the training objective:

where the  norm is applied to both the real and imaginary parts of the loss.
We call this training objective complex approximation (CA).


We also consider a training objective based on time-domain signal-to-noise ratio (SNR).
The proposed framework consists of two steps:
First, we organize all frame-level complex estimates  with respect to the minimum frame-level loss, so that each organized output  corresponds to a single speaker. The frame-level loss for organization can be defined in three domains: the complex STFT, magnitude STFT and time domain. 
In each domain, we compare the estimates and ground-truth targets, and calculate the  norm of the difference as the loss.
We found the complex STFT loss to be slightly better.
Second, we apply iSTFT (Eq.~\ref{iSTFT}) to , and compute utterance-level SNR for the final time-domain estimates :






\subsubsection{Convolutional neural networks for simultaneous grouping}
\label{sec:4.1.3}
    \begin{figure}[]
\begin{minipage}[a]{\linewidth}
      \centering
      \centerline{\includegraphics[width=1.0\linewidth]{DenseUNet}}
    \medskip
    \end{minipage}
\caption{Diagram of the Dense-UNet used in simultaneous grouping. Gray blocks denote dense CNN layers. DS blocks denote downsampling layers and US blocks denote upsampling layers. Skip connections are added to connect layers at the same level. The inputs, masks and outputs can be defined in either magnitude or complex STFT domain.}
    \label{fig:unet}
\end{figure}
    
Motivated by the recent success of DenseNet \cite{DENSE} and UNet \cite{UNET0} in music source separation \cite{UNET1} \cite{MM}, we propose a Dense-UNet structure for simultaneous grouping.

The proposed Dense-UNet is shown in Fig.~\ref{fig:unet}, and it is based on a UNet architecture \cite{UNET0}.
It consists of a series of convolutional layers, downsampling layers and upsampling layers.
The first half of the network encodes utterance-level STFT feature maps into a higher level of abstraction.
Convolutional layers and downsampling layers are alternated in this half, allowing the network to model large T-F contexts.
Convolutional layers and upsampling layers are alternated in the second half to project the encoded features back to its original resolution.
In this study, we use strided  depthwise convolutional layers \cite{depthwise} as downsampling layers. Strided transpose convolutional layers are used as upsampling layers. 
Skip connections are added between layers at the same hierarchical level in the encoder and decoder to preserve raw information in the mixture.

Next, we replace convolutional layers in the original UNet with densely-connected CNN blocks (DenseNet) \cite{DENSE}. 
The basic idea of DenseNet is to decompose one convolutional layer with many channels into a sequence of densely connected convolutional layers with fewer channels, where each layer is connected to every other layer in a feed-forward fashion:

where  denotes the input feature map,  the output of the  layer,  concatenation, and  the  convolutional layer followed by ELU (exponential linear unit) activation \cite{elu} and layer normalization \cite{ln}.
The DenseNet structure has shown excellent performance in image classification \cite{DENSE} and music source separation \cite{MM}.
In this study, all output layers  in a dense block have the same number of channels, denoted by . 
The total number of layers in each dense block is denoted by .
As shown in Fig.~\ref{fig:unet}, we alternate 9 dense blocks with 4 downsampling layers and 4 upsampling layers.
After the last dense block, we use a  CNN layer to reorganize the feature map, and then output two masks.



In CNNs, convolutional kernels are usually applied across the entire input field. This is reasonable in the case of visual processing, where similar patterns can appear anywhere in the visual field with translation and rotation.
However, in the auditory representation of speech, patterns that occur in different frequency bands are usually different.
A generic CNN kernel may result in inconsistent outputs at different frequencies.
To address this problem, Takahashi and Mitsufuji \cite{MM} split the spectral input into several subbands, and train band-dependent CNNs, leading to a substantial rise in model size. 

We propose a frequency mapping layer which effectively alleviates this problem with a significant reduction of parameters.
The basic idea is to project inconsistent frequency outputs to an organized space using a fully-connected layer. 
We replace one CNN layer in each dense block with a frequency mapping layer.
The input to a frequency mapping layer is a concatenation of CNN layers , where  and  denote time and frequency respectively,  the number of channels in the input.
 is passed to a  convolutional layer, followed by ELU activation and layer normalization, to reduce the number of channels to . 
The resulting output is denoted by . 
We then transpose the  and  dimension of  to get .
Next,  is fed to a  convolutional layer, followed by ELU activation and layer normalization, to output . This layer can also be viewed as a frequency-wise fully connected layer, which takes all frequency estimates as the input and reorganize them in a different space.
Finally,  is transposed back, and the output of the frequency mapping layer  is generated.




\subsection{Sequential Grouping Stage}
\label{sec:4.2}
\subsubsection{Baseline system}
\label{sec:4.2.1}
In this stage, we group frame-level spectral estimates across time using a clustering network, which corresponds to sequential grouping in CASA.
In deep clustering based speaker separation, T-F level embedding vectors estimated by BLSTM are clustered into different speakers.
We extend this framework to frame-level speaker tracking.

Fig.~\ref{fig:sg} illustrates our sequential grouping. 
We first stack the mixture spectrogram and two spectral estimates (including real, imaginary and magnitude STFT) as the input to the system.
A neural network then projects frame-level inputs to a -dimensional unit-length embedding vector .
The target label is a two-dimensional indicator vector, denoted by . 
During the training of tPIT, if the minimum loss is achieved when  is paired with speaker 1, and  is paired with speaker 2, we set  to [1 0]. 
Otherwise,  is set to [0 1].
In other words,  indicates the optimal output assignment of each frame.
 and  can be reshaped into a  matrix , and a  matrix , respectively.
A permutation independent objective function \cite{DC} is:

where  is the Frobenius norm.
Optimizing  forces  corresponding to the same optimal assignment to get closer during training, and otherwise to become farther apart.


Because we care more about the speaker assignment of frames where the two outputs are substantially different, a weight  is used during training
where  represents the frame-level loss difference (LD) between the two possible speaker assignments.
  is large if two conditions are both satisfied:
 1) the frame-level energy of the mixture is high;
 2) the two frame-level outputs,  and , are quite different, so that the losses with respect to different speaker assignments are significantly different. 
 can be used to construct a diagonal matrix .
The final weighted objective function is:

This objective function emphasizes frames where the speaker assignment plays an important role.


During inference, the K-means algorithm is first applied to cluster  into two groups.
We then organize frame-level outputs according to their K-means labels. Finally, iSTFT is employed to convert complex outputs to the time domain.



\begin{figure}[]
\begin{minipage}[a]{\linewidth}
      \centering
      \centerline{\includegraphics[width=0.5\linewidth]{DPCL}}
    \medskip
    \end{minipage}
\caption{Diagram of the sequential grouping stage. We use BLSTM or TCN as the neural network in this stage.}
    \label{fig:sg}
\end{figure}
    
        
    \begin{figure*}[]
\begin{minipage}[a]{\linewidth}
      \centering
      \centerline{\includegraphics[width=0.95\linewidth]{TCN}}
    \medskip
    \end{minipage}
\caption{Diagram of the TCN used in sequential grouping. Outputs from the previous stage are fed into a series of dilated convolutional blocks to predict frame-level embedding vectors. The dilation factor of each block is marked on the right. The detailed structure of a dilated convolutional block is illustrated in the large gray box. The network within the dashed box can be also used for uPIT based speaker separation.}
    \label{fig:TCN}
\end{figure*}
    
    
\subsubsection{Temporal convolutional networks for sequential grouping}
\label{sec:4.2.1}

Temporal convolutional networks (TCNs) have been used as a replacement for RNNs, and have shown comparable or better performance in various tasks \cite{TCN1} \cite{TCN0} \cite{Tasnet} \cite{ashu}.
In TCNs, a series of dilated convolutional layers are stacked to form a deep network, which enables very long memory.
In this study, we adopt a TCN similar to TasNet \cite{Tasnet} for sequential grouping, as illustrated in Fig.~\ref{fig:TCN}.


In the proposed TCN, input features are first passed to a 2-D dense CNN block, a  convolutional layer and a layer normalization module, to perform frame-level feature preprocessing.
The  convolutional layer here refers to a 1-D CNN layer with a kernel size of 1.
The preprocessed features are then passed to a series of dilated convolutional blocks, with an exponentially increasing dilation factor (, , ...,  to exploit large temporal contexts.
Next, the  stacked dilated convolutional blocks are repeated 3 times to further increase the receptive field.
Lastly, the outputs are fed into a  convolutional layer for embedding estimation.

In each dilated convolutional block, a bottleneck input with  channels  is first passed to a  convolutional layer, followed by PReLU (parametric rectified linear unit) activation \cite{prelu} and layer normalization, to extend the number of channels to , with output denoted by .
A depthwise dilated convolutional layer \cite{depthwise} with kernel , followed by PReLU activation and layer normalization, is then employed to capture the temporal context.
The number 3 here indicates the size of the temporal filter in each channel, and there are  depthwise separable filters in the kernel.
We adopt non-causal filters to exploit both past and future information, with a dilation factor from ,... , as in \cite{Tasnet}.
The output of this part is denoted by , which is then passed to a  convolutional layer to project the number of channels back to , denoted by .
In the end, an identity residual connection combines  and  and forms the final output. 


Overfitting is a major concern in sequence models.
If not regularized properly, sequence models tend to memorize the patterns in the training data, and get trapped in local minima. 
To address this issue, various dropout techniques \cite{RDO1} \cite{RDO3} \cite{RDO2} have been proposed for RNNs. Consistent improvement has been achieved if dropout is applied to recurrent connections \cite{RDO3}. 
Meanwhile, a simple dropout scheme for TCNs is used in \cite{TCN1}, i.e., dropping  in each dilated convolutional block, but it does not yield satisfactory performance in our experience. 
Based on these findings, we design a new dropout scheme for the TCN model, denoted by dropDilation.
In dropDilation, the dilated connections in depthwise dilated convolutional layers are dropped with a probability of , where  denotes the keep rate.
To be more specific, a binary mask, , 
is multiplied with each depthwise dilated convolutional kernel  during training, with  and  drawn independently from a Bernoulli distribution .
In dropDilation, we only drop the dilated connections while keeping the direct connections to preserve local information.


 
\section{Evaluation and comparison}
\label{sec:5}
\subsection{Experimental Setup}
\label{sec:5.1}
We use the WSJ0-2mix dataset, a monaural two-talker speaker separation dataset introduced in \cite{DC}, for evaluations.
WSJ0-2mix has a 30-hour training set and a 10-hour validation set generated by selecting random speaker pairs in the Wall Street Journal (WSJ0) training set si\_tr\_s, and mixing them at various SNRs between 0 dB and 5 dB.
Evaluation is conducted on the 5-hour open-condition (OC) test set, which is similarly generated using 16 untrained speakers from the WSJ0 development set si\_dt\_05 and si\_et\_05.
All mixtures are sampled at 8 kHz.
STFT with a frame length of 32ms, a frame shift of 8 ms, and a square root hanning window is taken for the whole system. 


We report results in terms of signal-to-distortion ratio improvement (SDR) \cite{SDR}, perceptual evaluation of speech quality (PESQ) \cite{PESQ}, and extended short-time objective intelligibility (ESTOI) \cite{eSTOI}, to measure source separation performance, speech quality and speech intelligibility, respectively.
We also report the final result in terms of scale-invariant signal-to-noise ratio improvement (SI-SNR) \cite{Tasnet} for a systematical comparison with other competitive systems.


\subsection{Models}
\label{sec:5.2}
\subsubsection{Simultaneous grouping models}
\label{sec:5.2.1}
Two models are evaluated for simultaneous grouping: BLSTM and Dense-UNet.

The baseline BLSTM contains 3 BLSTM layers, with 8962 units in each layer. 
In each dense block of Dense-UNet, the number of channels  is set to 64, the total number of dense layers  is set to 5, and all CNN layers have a kernel size of  and a stride of .
The middle layer in each dense block is replaced with a frequency mapping layer. 
We use valid padding (a term in CNN literature referring to no input padding) for the last CNN layer in each dense block, and same padding (padding the input with zeros so that the output has the same dimension as the original input) for all other layers.
The input STFT is zero-padded accordingly.

For both models, when trained with , the magnitude STFT of the mixture is adopted as the input, and ELU activation is applied to output layers for phase-sensitive mask estimation.
If  or  is used for training, a stack of real and imaginary STFT is used as the input, and linear output layers are used to predict the real and imaginary parts of complex ratio masks separately.

Both networks are trained with the Adam optimization algorithm \cite{Adam} and dropout regularization \cite{Dropout}.
The initial learning rate is set to 0.0002 for BLSTM, and 0.0001 for Dense-UNet.
Learning rate adjustment and early stopping are employed based on the loss on the validation set. 


\subsubsection{Sequential grouping models}
\label{sec:5.2.2}
Two models are evaluated for sequential grouping: BLSTM and TCN.
Both models are trained on top of a well-tuned simultaneous grouping model.

The baseline BLSTM contains 4 BLSTM layers, with 3002 units in each layer. 
In TCN, the maximum dilation factor is set to , to reach a theoretical receptive field of 8.128s.
The number of bottleneck units  is selected as 256. 
The number of units in depthwise dilated convolutional layers  is set to 512.
Same padding is employed in all CNN layers.
DropDilation with  is applied during training.



A 2-D dense CNN block is used in both models for frame-level feature preprocessing, with , , a kernel size of   and a stride of .
The dimensionality of embedding vectors  is set to 40.
Both networks are trained with the Adam optimization algorithm, with an initial learning rate of 0.001 for BLSTM, and 0.00025 for TCN.
Learning rate adjustment and early stopping are again adopted.



\subsubsection{One stage uPIT models}
\label{sec:5.2.3}
To systematically evaluate the proposed methods, we train a Dense-UNet and a TCN with SNR objectives and uPIT training criterion, i.e., .
Other training recipes follow those in Section~\ref{sec:5.2.1} and~\ref{sec:5.2.2}..


\subsection{Results and Comparisons}
\label{sec:5.3}

      \begin{table}[!t]
\renewcommand{\arraystretch}{1.2}
    \caption{SDR, PESQ and ESTOI for simultaneous grouping models with optimal output assignment on WSJ0-2mix OC.}
    \label{TABLE1}
    \centering
    \resizebox{\columnwidth}{!}{{
    \begin{tabular}{l|c|c|c|c|c}
    \hline
      &Objective & \# of param. & SDR (dB) & PESQ& ESTOI (\%) \\
\hline
    Mixture & - &-& 0.0 & 2.02 & 56.1\\
   \hline
    tPIT BLSTM& PSA& 46.3M&13.0&3.13&86.7\\
tPIT Dense-UNet &PSA & 4.7M&14.7	&3.41&90.5 \\
    tPIT Dense-UNet&CA & 4.7M& 18.6 & 3.57 & 93.8\\
tPIT Dense-UNet&SNR& 4.7M& 19.1 & 3.63 & 94.3\\
    \hline

    \end{tabular}
    }
    }

    \end{table}

We first evaluate the simultaneous grouping stage.
Table~\ref{TABLE1} summarizes the performance of tPIT models with respect to different network structures and training objectives.
For all models, outputs are organized with the optimal speaker assignment before evaluation.
Scores of mixtures are presented in the first row. 
Compared to BLSTM, Dense-UNet drastically reduces the number of trainable parameters to 4.7 million, and introduces significant performance gain.
The frequency mapping layers in our Dense-UNet introduce a 0.3 dB increment in SDR, 0.1 increment in PESQ, 0.8\% increment in ESTOI and a parameter reduction of 0.9 million. 
Next, we switch from magnitude STFT to complex STFT, and change the training objective to .
This change leads to large improvement, revealing the importance of phase information for source separation.
The SNR objective further outperforms the CA objective. 
We thus adopt tPIT Dense-UNet trained with  for simultaneous grouping in the following evaluations.

 \begin{table}[!t]
\renewcommand{\arraystretch}{1.2}
    \caption{SDR, PESQ and ESTOI for tPIT and uPIT based Dense-UNet trained with SNR objectives.}
    \label{TABLE2}
    \centering
    \resizebox{\columnwidth}{!}{{
    \begin{tabular}{l|c|c|c|c}
    \hline
     &Output Assign. & SDR (dB) & PESQ& ESTOI (\%) \\
\hline
    \multirow{2}{*}{tPIT Dense-UNet} & Optimal&19.1 & 3.63 & 94.3\\
     & Default & 0.0 & 1.99&55.8 \\
     \hline
     \multirow{2}{*}{uPIT Dense-UNet} &Optimal&17.0&3.40&91.6\\
     &Default&15.2 & 3.24 & 88.9       \\
    \hline 

    \end{tabular}
    }
    }

    \end{table}

\captionsetup[subfloat]{captionskip=-0.7pt}


\begin{figure*}[htb]
    \centering
    \subfloat[]{\includegraphics[width=0.5\linewidth]{tpit_mix.pdf}}
     \-2.2ex]
    \subfloat[]{\includegraphics[width=0.5\linewidth]{tpit_o1.pdf}}
\subfloat[]{\includegraphics[width=0.5\linewidth]{tpit_o2.pdf}}
     \-2.2ex]
    \subfloat[]{\includegraphics[width=0.5\linewidth]{upit_o1.pdf}}
\subfloat[]{\includegraphics[width=0.5\linewidth]{upit_o2.pdf}}
     \\mathrm{\Delta}\mathrm{\Delta}\mathrm{\Delta}\mathrm{\Delta}-2.2ex]
    \subfloat[]{\includegraphics[width=0.5\linewidth]{casa_o1.pdf}}
    \subfloat[]{\includegraphics[width=0.5\linewidth]{casa_o2.pdf}}
             \-0.2ex]

    \subfloat[]{\includegraphics[width=0.91\linewidth]{casa_assign.pdf}}
         \-2.2ex]
    \subfloat[]{\includegraphics[width=0.5\linewidth]{casa_rr1.pdf}}
    \subfloat[]{\includegraphics[width=0.5\linewidth]{casa_rr2.pdf}}
        \\mathrm{\Delta}\mathrm{\Delta}\mathrm{\Delta}\mathrm{\Delta}J^{tPIT-SNR}\mathrm{\Delta}\mathrm{\Delta}\mathrm{\Delta}\mathrm{\Delta}O(FT)O(T) in deep CASA. 




Although the deep CASA algorithm is formulated for two speakers, it can be extended to three or more speakers. 
First, additional output layers can be added in the simultaneous grouping stage.
In sequential grouping, we can employ the setup in \cite{me} to predict one embedding vector for each frame-level spectral estimate.
A constrained K-means algorithm can then be used to assign each frame-level embedding to a different speaker.








\ifCLASSOPTIONcaptionsoff
  \newpage
\fi









\bibliographystyle{IEEEtranS}

\bibliography{REFERENCE}


















\end{document}
