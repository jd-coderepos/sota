\section{Experiments}\label{experiments}
In this section, we conduct extensive experiments to answer the following questions:
\begin{itemize}
    \item \textbf{(Q1)} How does our proposed CIN perform in high-order feature interactions learning?
    \item \textbf{(Q2)} Is it necessary to combine explicit and implicit high-order feature interactions for recommender systems?
    \item \textbf{(Q3)} How does the settings of networks influence the performance of xDeepFM?
\end{itemize}
We will answer these questions after presenting some fundamental experimental settings.
\subsection{Experiment Setup}
\subsubsection{Datasets.} We evaluate our proposed models on the following three datasets:\\
\indent \textbf{1. Criteo Dataset}. It is a famous industry benchmarking dataset for developing models predicting ad click-through rate, and is publicly accessible\footnote{http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset/}. Given a user and the page he is visiting, the goal is to predict the probability that he will clik on a given ad. \\
\indent \textbf{2. Dianping Dataset}. \textsl{Dianping.com} is the largest consumer review site in China. It provides diverse functions such as reviews, check-ins, and shops' meta information (including geographical messages and shop attributes). We collect 6 months' users check-in activities for restaurant recommendation experiments. Given a user's profile, a restaurant's attributes and the user's last three visited POIs (point of interest), we want to predict the probability that he will visit the restaurant. For each restaurant in a user's check-in instance, we sample four restaurants which are within 3 kilometers as negative instances by POI popularity.\\
\indent \textbf{3. Bing News Dataset}. Bing News\footnote{https://www.bing.com/news} is part of Microsoft's Bing search engine. In order to evaluate the performance of our model in a real commercial dataset, we collect five consecutive days' impression logs on news reading service. We use the first three days' data for training and validation, and the next two days for testing.  \\
\indent For the Criteo dataset and the Dianping dataset, we randomly split instances by 8:1:1 for training , validation and test. The characteristics of the three datasets are summarized in Table \ref{tab:datasets}.
  \begin{table}[ht]
     \centering
     \caption{Statistics of the evaluation datasets. M indicates million and K indicates thousand. }
     \begin{tabular}{|c|c|c|c|} \hline
        Datasest  &  \#instances & \#fields &\#features (sparse)  \\ \hline    
        Criteo    &  45M & 39 & 2.3M \\ \hline           
        Dianping &  1.2M & 18 & 230K \\ \hline   
        Bing News &  5M & 45 & 17K \\  \hline
     \end{tabular}
     \label{tab:datasets} 
 \end{table}
\subsubsection{Evaluation Metrics.} We use two metrics for model evaluation: \textbf{AUC} (Area Under the ROC curve) and \textbf{Logloss} (cross entropy). These two metrics evaluate the performance from two different angels: AUC measures the probability that a positive instance will be ranked higher than a randomly chosen negative one. It only takes into account the order of predicted instances and is insensitive to class imbalance problem. Logloss, in contrast, measures the distance between the predicted score and the true label for each instance. Sometimes we rely more on Logloss because we need to use the predicted probability to estimate the benefit of a ranking strategy (which is usually adjusted as CTR $\times$ bid).
\subsubsection{Baselines.} We compare our xDeepFM with LR(logistic regression), FM, DNN (plain deep neural network), PNN (choose the better one from iPNN and oPNN) \cite{qu2016product}, Wide \& Deep \cite{cheng2016wide}, DCN (Deep \& Cross Network) \cite{wang2017deep} and DeepFM \cite{guo2017deepfm}. As introduced and discussed in Section \ref{preliminaries}, these models are highly related to our xDeepFM and some of them are state-of-the-art models for recommender systems. Note that the focus of this paper is to learn feature interactions automatically, so we do not include any hand-crafted cross features.  
\subsubsection{Reproducibility} We implement our method using Tensorflow\footnote{https://www.tensorflow.org/}. Hyper-parameters of each model are tuned by grid-searching on the validation set, and the best settings for each model will be shown in corresponding sections. Learning rate is set to \textsl{0.001}. For optimization method, we use the Adam \cite{kingma2014adam} with a mini-batch size of 4096. We use a L2 regularization with $\lambda=0.0001$ for DNN, DCN, Wide\&Deep, DeepFM and xDeepFM, and use dropout 0.5 for PNN. The default setting for number of neurons per layer is: (1) 400 for DNN layers; (2) 200 for CIN layers on Criteo dataset, and 100 for CIN layers on Dianping and Bing News datasets. Since we focus on neural networks structures in this paper, we make the dimension of field embedding for all models be a fixed value of 10. We conduct experiments of different settings in parallel with 5 \textsl{Tesla K80 GPUs}.
The source code is available at \textsl{\url{https://github.com/Leavingseason/xDeepFM}}. 

  \begin{table}[ht]
     \centering
     \caption{Performance of individual models on the Criteo, Dianping, and Bing News datasets. Column \textsl{Depth} indicates the best network depth for each model.  }
     \begin{tabular}{c|cc|c} \hline\hline  
        Model name &  AUC & Logloss & Depth \\ \hline  
        \multicolumn{4}{c}{Criteo}\\ \hline  
        FM    & 0.7900   & 0.4592  &  -  \\ \hline           
        DNN & 0.7993  &  0.4491 &  2 \\ \hline   
        CrossNet &  0.7961 & 0.4508 & 3 \\  \hline
        CIN & \textbf{0.8012}  &  0.4493 &  3 \\ \hline   
         \multicolumn{4}{c}{Dianping}\\ \hline  
         FM    & 0.8165  & 0.3558  &  -  \\ \hline           
        DNN & 0.8318  &  0.3382 &  3 \\ \hline   
        CrossNet &  0.8283 & 0.3404 & 2 \\  \hline
        CIN & \textbf{0.8576}  &  \textbf{0.3225} &  2 \\ \hline    
          \multicolumn{4}{c}{Bing News}\\ \hline  
         FM    & 0.8223 & 0.2779  &  -  \\ \hline           
        DNN & 0.8366  &  0.273 &  2 \\ \hline   
        CrossNet &  0.8304 & 0.2765 & 6 \\  \hline
        CIN & \textbf{0.8377}  &  \textbf{0.2662} &  5 \\ \hline \hline  
     \end{tabular}
     \label{tab:cin} 
 \end{table}
   \begin{table*}[th]
    \centering
    \caption{Overall performance of different models on Criteo, Dianping and Bing News datasets. The column \textsl{Depth} presents the best setting for network depth with a format of (cross layers, DNN layers).}
     {\setlength{\tabcolsep}{1em}{\renewcommand{\arraystretch}{1.1}
     \begin{tabular}{c|cc|c||cc|c||cc|c} \hline\hline  
         & \multicolumn{3}{c||}{Criteo} & \multicolumn{3}{c||}{Dianping} & \multicolumn{3}{c}{Bing News}  \\ \hline  
        Model name &  AUC & Logloss & Depth&  AUC & Logloss & Depth &  AUC & Logloss & Depth  \\ \hline  
        LR  & 0.7577 & 0.4854   &    -,-  & 0.8018 & 0.3608  &    -,-  & 0.7988 & 0.2950  &    -,-   \\ \hline       
        FM    & 0.7900  & 0.4592  &  -,-  & 0.8165  & 0.3558  &  -,- & 0.8223 & 0.2779 &  -,-  \\ \hline    
        DNN & 0.7993  &  0.4491 &  -,2  & 0.8318  &  0.3382 & -,3 & 0.8366  &  0.2730 & -,2 \\ \hline   
        DCN &  0.8026 & 0.4467 & 2,2 &  0.8391 & 0.3379 & 4,3&  0.8379 & 0.2677 & 2,2  \\  \hline
        Wide\&Deep & 0.8000 & 0.4490 & -,3& 0.8361 & 0.3364 & -,2 & 0.8377 & 0.2668 & -,2 \\  \hline
        PNN &  0.8038 & 0.4927 & -,2 &  0.8445 & 0.3424 & -,3&  0.8321 & 0.2775 & -,3\\  \hline
        DeepFM &  0.8025 & 0.4468 & -,2&  0.8481 & 0.3333 & -,2 &  0.8376 & 0.2671 & -,3  \\  \hline
        xDeepFM & \textbf{0.8052}  &  \textbf{0.4418} &  3,2& \textbf{0.8639}  &  \textbf{0.3156} &  3,3 & \textbf{0.8400}  &  \textbf{0.2649} &  3,2  \\ \hline   \hline   
     \end{tabular}
     }}
     \label{tab:overallperformance} 
\end{table*}
\subsection{Performance Comparison among Individual Neural Components (Q1)}
We want to know how CIN performs individually. Note that FM measures 2-order feature interactions explicitly, DNN model high-order feature interactions implicitly, CrossNet tries to model high-order feature interactions with a small number of parameters (which is proven not effective in Section \ref{sec:explicit}), and CIN models high-order feature interactions explicitly. There is no theoretic guarantee of the superiority of one individual model over the others, due to that it really depends on the dataset. For example, if the practical dataset does not require high-order feature interactions, FM may be the best individual model. Thus we do not have any expectation for which model will perform the best in this experiment. 
 
 \indent Table \ref{tab:cin} shows the results of individual models on the three practical datasets. Surprisingly, our CIN outperform the other models consistently. On one hand, the results indicate that for practical datasets, higher-order interactions over sparse features are necessary, and this can be verified through the fact that DNN, CrossNet and CIN outperform FM significantly on all the three datasets.  On the other hand, CIN is the best individual model, which demonstrates the effectiveness of CIN on modeling explicit high-order feature interactions. Note that a $k$-layer CIN can model $k$-degree feature interactions. It is also interesting to see that it take 5 layers for CIN to yield the best result ON the Bing News dataset.
\subsection{Performance of Integrated Models (Q2)}
xDeepFM integrates CIN and DNN into an end-to-end model. While CIN and DNN covers two distinct properties in learning feature interactions, we are interested to know whether it is indeed necessary and effective to combine them together for jointly explicit and implicit learning. Here we compare several strong baselines which are not limited to individual models, and the results are shown in Table \ref{tab:overallperformance}. We observe that LR is far worse than all the rest models, which demonstrates that factorization-based models are essential for measuring sparse features. Wide\&Deep, DCN, DeepFM and xDeepFM are significantly better than DNN, which directly reflects that,  despite their simplicity, incorporating hybrid components are important for boosting the accuracy of predictive systems. Our proposed xDeepFM achieves the best performance on all datasets, which demonstrates that combining explicit and implicit high-order feature interaction is necessary, and xDeepFM is effective in learning this class of combination. Another interesting observation is that, all the neural-based models do not require a very deep network structure for the best performance. Typical settings for the depth hyper-parameter are 2 and 3, and the best depth setting for xDeepFM is 3, which indicates that the interactions we learned are at most 4-order.
 \subsection{Hyper-Parameter Study (Q3)}
 We study the impact of hyper-parameters on xDeepFM in this section, including (1) the number of hidden layers; (2) the number of neurons per layer; and (3) activation functions. We conduct experiments via holding the best settings for the DNN part while varying the settings for the CIN part.
 \begin{figure*}[htbp]
\centering
\begin{subfigure}{.32\textwidth}
  \centering
  \includegraphics[width=0.85\textwidth,height=.5\textwidth]{materials/auc_depth_network.pdf}
  \caption{Number of layers.}
  \label{fig:auc_depth}
\end{subfigure} \hfill  
\begin{subfigure}{.32\textwidth}
  \centering
  \includegraphics[width=0.85\textwidth,height=.5\textwidth]{materials/auc_neworksize.pdf}
  \caption{Number of neurons per layer.}
  \label{fig:auc_size}
\end{subfigure}    \hfill  
\begin{subfigure}{.32\textwidth}
  \centering
  \includegraphics[width=0.85\textwidth,height=.5\textwidth]{materials/auc_activation.pdf}
  \caption{Activation functions}
  \label{fig:auc_activation}
\end{subfigure} 
\caption{Impact of network hyper-parameters on AUC performance.}
\label{fig:auc_hyperparameter}
\end{figure*}
 \begin{figure*}[htbp]
\centering
\begin{subfigure}{.32\textwidth}
  \centering
  \includegraphics[width=0.85\textwidth,height=.5\textwidth]{materials/logloss_depth_network.pdf}
  \caption{Number of layers.}
  \label{fig:logloss_depth}
\end{subfigure} \hfill  
\begin{subfigure}{.32\textwidth}
  \centering
  \includegraphics[width=0.85\textwidth,height=.5\textwidth]{materials/logloss_networksize.pdf}
  \caption{Number of neurons per layer.}
  \label{fig:logloss_size}
\end{subfigure}    \hfill  
\begin{subfigure}{.32\textwidth}
  \centering
  \includegraphics[width=0.85\textwidth,height=.5\textwidth]{materials/logloss_activation.pdf}
  \caption{Activation functions}
  \label{fig:logloss_activation}
\end{subfigure} 
\caption{Impact of network hyper-parameters on Logloss performance.}
\label{fig:logloss_hyperparameter}
\end{figure*}  
 \\\indent\textbf{Depth of Network}. Figure \ref{fig:auc_depth} and \ref{fig:logloss_depth} demonstrate the impact of number of hidden layers. We can observe that the performance of xDeepFM increases with the depth of network at the beginning. However, model performance degrades when the depth of network is set greater than 3. It is caused by overfitting evidenced by that we notice that the loss of training data still keeps decreasing when we add more hidden layers.
 \\\indent\textbf{Number of Neurons per Layer}. Adding the number of neurons per layer indicates increasing the number of feature maps in CIN. As shown in Figure \ref{fig:auc_size} and \ref{fig:logloss_size}, model performance on Bing News dataset increases steadily when we increase the number of neurons from $20$ to $200$, while on Dianping dataset, $100$ is a more suitable setting for the number of neurons per layer. In this experiment we fix the depth of network at 3.
 \\\indent\textbf{Activation Function}. Note that we exploit the identity as activation function on neurons of CIN, as shown in Eq. \ref{eq:xk}. A common practice in deep learning literature is to employ non-linear activation functions on hidden neurons. We thus compare the results of different activation functions on CIN (for neurons in DNN, we keep the activation function with \textsl{relu}). As shown in Figure \ref{fig:auc_activation} and \ref{fig:logloss_activation}, identify function is indeed the most suitable one for neurons in CIN.