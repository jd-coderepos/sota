\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{wacv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[accsupp]{axessibility}  \usepackage{pifont}
\usepackage{subfig}
\usepackage{multirow}






\def\wacvPaperID{****} 

\wacvfinalcopy 

\ifwacvfinal
\pagestyle{empty}
\fi




\ifwacvfinal
\usepackage[breaklinks=true,bookmarks=false]{hyperref}
\else
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}
\fi

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\begin{document}

\title{Robust Lane Detection via Expanded Self Attention}

\author{
	Minhyeok Lee \quad
	Junhyeop Lee \quad
	Dogyoon Lee \quad
	Woojin Kim \quad
	Sangwon Hwang \\
	Sangyoun Lee \quad
\vspace{0.01cm}\\
	Yonsei University School of Electrical and Electronic Engineering \\
	{\tt\small \{hydragon516,jun.lee,nemotio,woojinkim0207,sangwon1042,syleee\}@yonsei.ac.kr}
}

\maketitle

\ifwacvfinal
\thispagestyle{empty}
\fi

\begin{abstract}
   The image-based lane detection algorithm is one of the key technologies in autonomous vehicles. Modern deep learning methods achieve high performance in lane detection, but it is still difficult to accurately detect lanes in challenging situations such as congested roads and extreme lighting conditions. To be robust on these challenging situations, it is important to extract global contextual information even from limited visual cues. In this paper, we propose a simple but powerful self-attention mechanism optimized for lane detection called the Expanded Self Attention (ESA) module. Inspired by the simple geometric structure of lanes, the proposed method predicts the confidence of a lane along the vertical and horizontal directions in an image. The prediction of the confidence enables estimating occluded locations by extracting global contextual information. ESA module can be easily implemented and applied to any encoder-decoder-based model without increasing the inference time. The performance of our method is evaluated on three popular lane detection benchmarks (TuSimple, CULane and BDD100K). We achieve state-of-the-art performance in CULane and BDD100K and distinct improvement on TuSimple dataset. The experimental results show that our approach is robust to occlusion and extreme lighting conditions. 
\end{abstract}

\section{Introduction}
Advanced Driver Assistance Systems (ADAS), which are a key technology for autonomous driving, assists drivers in a variety of driving scenarios owing to deep learning. For ADAS, lane detection is an essential technology for vehicles to stably follow lanes. However, lane detection tasks, which rely on visual cues such as cameras, remain challenging owing to severe occlusions, extreme changes in the lighting conditions, and poor pavement conditions. Even in such difficult driving scenarios, humans can sensibly determine the positions of lanes by recognizing the positional relationship between the vehicles and surrounding environment. This remains a difficult task in image-based deep learning.

\begin{figure}
	\setlength{\belowcaptionskip}{-24pt}
	\begin{center}
		\includegraphics[width=\linewidth]{./image/fig1/1.pdf}
		\
	\mathcal{L}_{seg} = \mathcal{L}_{CE}\left(S_{pred},S_{gt}\right),
	\label{eq:one}

	\mathcal{L}_{exist} = \mathcal{L}_{BCE}\left(l_{pred},l_{gt}\right),
	\label{eq:two}

	\begin{split}
		\mathcal{L}_{esa} &=\mathcal{L}_{MSE}\left(E_{pred},E_{gt}\right)\\
		&+\lambda\left|\Psi\left(E_{pred}\right)-\Upsilon\Psi\left(S_{gt}\right)\right|,
		\label{eq:three}
	\end{split}
-1.5ex]
		\caption{Comparison of low (top) and high (bottom) loss. The mean square error is determined according to the location in which the ESA matrix is active.}
		\label{fig:mse}
	\end{center}
\end{figure}

\begin{table*}[t]
	\label{culane_table}
	\centering
	\footnotesize{
		\begin{tabular}{c|c|c||c|c|c|c|c}
			\hline
			Category & \textbf{R-34-H\&VESA} & \textbf{ERFNet-H\&VESA} & SCNN~\cite{pan2017spatial} & ENet-SAD~\cite{hou2019learning} & R-34-Ultra~\cite{qin2020ultra} & ERFNet~\cite{romera2017erfnet} & ERFNet-E2E~\cite{yoo2020end} \\
			\hline \hline
			Normal & 90.5 & \textbf{92.0} & 90.6 & 90.1 & 90.7 & 91.5 & 91.0 \\
Crowded & 68.3 & \textbf{73.1} & 69.7 & 68.8 & 70.2 & 71.6 & \textbf{73.1} \\
Night & 65.7 & \textbf{69.5} & 66.1 & 66.0 & 66.7 & 67.1 & 67.9 \\
No line & 42.2 & 45.8 & 43.4 & 41.6 & 44.4 & 45.1 & \textbf{46.6} \\
Shadow & 65.1 & \textbf{75.1} & 66.9 & 65.9 & 69.3 & 71.3 & 74.1 \\
Arrow & 85.4 & \textbf{88.1} & 84.1 & 84.0 & 85.7 & 87.2 & 85.8 \\
Dazzle light & 59.8 & 63.1 & 58.5 & 60.2 & 59.5 & \textbf{66.0} & 64.5 \\
Curve & 61.5 & 68.8 & 64.4 & 65.7 & 69.5 & 66.3 & \textbf{71.9} \\
Crossroad & \textbf{1675} & 2001 & 1990 & 1998 & 2037 & 2199 & 2022 \\
Total & 70.9 & \textbf{74.2} & 71.6 & 70.8 & 72.3 & 73.1 & 74.0 \\
			\hline \hline
			Runtime (ms) &  \textbf{4.7} & 8.1 & 133.5 & 13.4 & 5.7 & 8.1 & \--  \\
			Parameter (M) & 2.44 & 2.68 & 20.72 & \textbf{0.98} & \-- & 2.68 & \--  \\
			\hline
		\end{tabular}
	}
	\caption{Comparison of F1-measures and runtimes for CULane test set. Only the FP is shown for crossroad. "R-" denotes "ResNet" and same abbreviation is used in the following sections.}
	\label{table:one}
	\vspace{-2ex}
\end{table*}

\noindent
where the ESA matrix is , the weighted probability map , the weighted ground truth map , and  is the mean square error loss. Moreover, the operator  calculates the average of all values of the feature map, and  is a regularization coefficient. The coefficient  has an important effect on the performance of the model, and it determines the proportion of the weighted lane area. The first term on the right-hand side of Equation (\ref{eq:three}) is visualized in Figure~\ref{fig:mse}. In general, the lane probability map is blurred in areas with sparse supervisory signals. As shown in Figure~\ref{fig:mse}, if a large weight is given to the accurately predicted region in the probability map, the mean square error is small. Conversely, when a large weight is given to an uncertainly predicted area, the mean square error is large. This is how to predict the confidence of the lane without additional annotations.

In fact, if only the mean square error loss is used as the ESA loss, the ESA module outputs are all zeros in the training. To solve this problem, a second term is added as a regularizer to the right-hand side of Equation (\ref{eq:three}). This regularization term keeps the average pixel value of the weighted probability map equal to a certain percentage of the average pixel value of the ground truth map. This ratio is determined by , which has a value between 0 and 1.

It should be noted that although one ESA module is an HESA or a VESA module, both modules can be simultaneously attached to the model. In that case, the ESA loss is , where  is the ESA loss of the HESA module, and  is the ESA loss of the VESA module. Finally, the above losses are combined to form the final objective function:



\noindent
The parameters ,  and  balance the segmentation loss, existence loss, and ESA loss of the final objective function.

\section{Experiments}
\noindent
\textbf{Datasets.} We use three popular lane detection datasets TuSimple \cite{tusimple}, CULane \cite{pan2017spatial}, and BDD100K \cite{yu2018bdd100k} for our experiments. TuSimple datasets consist of images of highways with constant illumination and good weather, and are relatively simple datasets because the roads are not congested. Therefore, various algorithms \cite{pan2017spatial, neven2018towards, ghafoorian2018gan, hou2019learning, jung2020towards} have been tested on TuSimple datasets since before 2018. CULane is a very challenging dataset that contains crowded environments with city roads and highways with varying lighting conditions. The CULane dataset and TuSimple dataset are officially labeled with up to four lanes and one background excluding lanes. These datasets focus on the detection of four lane markings, which are paid most attention to in real applications. The BDD100K dataset also consists of images captured under various lighting and weather conditions. In addition, the largest number of lanes among the three datasets is labeled. However, because the number of lanes is large and inconsistent, we detect lanes without distinguishing instances of lanes.

\noindent
\textbf{Evaluation metrics.} 

\noindent
\textit{1) TuSimple.} In accordance with \cite{tusimple}, the accuracy is expressed as , where  is the number of predicted correct lane points and  is the number of ground truth lane points. Furthermore, false positives (FP) and false negatives (FN) in the evaluation index.

\noindent
\textit{2) CULane.} In accordance with the evaluation metric in \cite{pan2017spatial}, each lane is considered 30 pixel thick, and the intersection-over-union (IoU) between the ground truth and prediction is calculated. Predictions with IoUs greater than 0.5 are considered true positives (TP). In addition, the F1-measure is used as an evaluation metric and is defined as follows:



\noindent
where , .

\noindent
\textit{3) BDD100K.} In general, since there are more than 8 lanes in an image, following~\cite{hou2019learning}, we determine the pixel accuracy and IoU of the lane as evaluation metrics.

We used different evaluation method for fair comparisons with previous studies. We evaluated with the same method as~\cite{ghafoorian2018gan, hou2019learning, neven2018towards, pan2017spatial} for TuSimple and~\cite{hou2019learning, pan2017spatial, qin2020ultra, yoo2020end} for CULane.

\begin{table}[!t]
	\centering
	\scriptsize{
		\begin{tabular}{c|c|c|c|c}
			\hline
			Algorithm & Accuracy & FP & FN & Runtime (ms)\\
			\hline \hline
			ResNet-18~\cite{he2016deep} & 92.69\% & 0.0948 & 0.0822 & \textbf{3.4} \\
			ResNet-34~\cite{he2016deep} & 92.84\% & 0.0918 & 0.0796 & 4.7 \\
			LaneNet~\cite{neven2018towards} & 96.38\% & 0.0780 & 0.0244 & 19.0 \\
			EL-GAN~\cite{ghafoorian2018gan} & 96.39\% & 0.0412 & 0.0336 & \-- \\
			ENet-SAD~\cite{hou2019learning} & \textbf{96.64\%} & 0.0602 & 0.0205 & 13.4 \\
			SCNN~\cite{pan2017spatial} & 96.53\% & 0.0617 & \textbf{0.0180} & 133.5 \\
			\hline \hline
			\textbf{R-18-H\&VESA} & 95.70\% & 0.0588 & 0.0622 & \textbf{3.4} \\
			\textbf{R-34-H\&VESA} & 95.83\% & 0.0587 & 0.0599 & 4.7 \\
			\textbf{ERFNet-HESA} & 96.01\% & \textbf{0.0329} & 0.0458 & 8.1 \\
			\textbf{ERFNet-VESA} & 95.94\% & 0.0340 & 0.0451 & 8.1 \\
			\textbf{ERFNet-H\&VESA} & 96.12\% & 0.0331 & 0.0450 & 8.1 \\
			\hline
\end{tabular}
	}
	\caption{Comparison of performance results of different algorithms applied to TuSimple test set.}
	\label{table:two}
\end{table}

\begin{table}[!t]
	\centering
	\footnotesize{
		\begin{tabular}{c|c|c|c}
			\hline
			Algorithm & Accuracy & IoU & Runtime (ms)\\
			\hline \hline
			ResNet-18~\cite{he2016deep} & 54.59\% & 44.63 & \textbf{2.7} \\
			ResNet-34~\cite{he2016deep} & 56.62\% & 46.00 & 4.1 \\
			ERFNet~\cite{romera2017erfnet} & 55.36\% & 47.04 & 7.3 \\
			ENet-SAD~\cite{hou2019learning} & 57.01\% & 47.72 & 12.1 \\
			SCNN~\cite{pan2017spatial} & 56.83\% & 47.34 & 123.6 \\
			\hline \hline
			\textbf{R-18-H\&VESA} & 57.03\% & 46.50 & \textbf{2.7} \\
			\textbf{R-34-H\&VESA} & 59.93\% & 49.51 & 4.1 \\
			\textbf{ERFNet-HESA} & 57.47\% & 48.97 & 7.3 \\
			\textbf{ERFNet-VESA} & 57.51\% & 48.24 & 7.3 \\
			\textbf{ERFNet-H\&VESA} & \textbf{60.24\%} & \textbf{51.77} & 7.3 \\
			\hline
		\end{tabular}
	}
	\caption{Comparison of results for BDD100K test set.}
	\label{table:three}
	\vspace{-2ex}
\end{table}

\begin{figure*}
	\setlength{\belowcaptionskip}{-10pt}
	\begin{center}
		\includegraphics[width=0.9\linewidth]{"./image/fig5/1".pdf}
		\-2ex]
		\caption{Comparison of the output probability maps of different algorithms applied to CULane test set. The third column is the result of the proposed ERFNet-HESA. The probability maps of the four lane classes are displayed in blue, green, red, and yellow, respectively.}
		\label{fig:culane}
	\end{center}
	\vspace{-2ex}
\end{figure*}

\noindent
\textbf{Implementation details.} Following \cite{pan2017spatial, hou2019learning}, we resize the images of TuSimple, CULane, and BDD100K to , , and , respectively. The original BDD100K images label one lane with two lines. Because this labeling method is difficult to learn, so we drew new 8 pixel thick ground truth labels that pass through the center of the lane. The new ground truth labels are applied equally to both train and test sets. Moreover, SGD~\cite{bottou2010large} is used as the optimizer, and the initial learning rate and batch size are set to 0.1 and 12, respectively. The loss balance coefficients , , and  in Equation (\ref{eq:four}) are set to 1, 0.1, and 50, respectively. The regularization coefficient  in Equation (\ref{eq:three}) is 1. It is experimentally verified whether the value of the coefficient  in Equation (\ref{eq:three}) has a significant effect on the performance of the model. In CULane and BDD100K, the optimal  value is set to 0.8, and TuSimple is set to 0.9. The effect of  on the performance is discussed in detail in Section~\ref{ablation}. Because the BDD100K experiment regards all lanes as one class, the output of the original segmentation branch is replaced with a binary segmentation map. In addition, the lane existence branch is removed for the evaluation. All models are trained and tested with PyTorch and the Nvidia RTX 2080Ti GPU.

\subsection{Results}
\label{result}
Tables~\ref{table:one}-\ref{table:three} compare the performance results of the proposed method and previously presented state-of-the-art algorithms for CULane, TuSimple, and BDD100K datasets. The proposed method is evaluated with the baseline models ResNet-18 \cite{he2016deep}, ResNet-34 \cite{he2016deep}, and ERFNet \cite{romera2017erfnet}, and each model is combined with either an HESA or a VESA. Moreover, the use of both HESA and VESA modules is denoted as “H\&VESA”. The effects of using both modules simultaneously are presented in Section~\ref{ablation}.

\begin{figure}
	\setlength{\belowcaptionskip}{-24pt}
	\begin{center}
		\includegraphics[width=\linewidth]{./image/fig6/1.pdf}
		\caption{Performance of different algorithms for (a) TuSimple and (b) BDD100K test sets.}
		\label{fig:bdd}
	\end{center}
\end{figure}

\begin{table*}[!t]
	\centering
	\footnotesize{
		\begin{tabular}{c|c c|c|c|c|c|c|c}
			\hline
			\multirow{2}*{Baseline} & \multirow{2}*{HESA} & \multirow{2}*{VESA} & \multicolumn{3}{|c|}{TuSimple} & CULane & \multicolumn{2}{|c}{BDD100K} \\
			\cline{4-9}
			~& & & Accuracy & FP & FN & F1 (total) & Accuracy & IoU \\
			\hline \hline
			\multirow{4}*{ResNet-18~\cite{he2016deep}} & & & 92.69\% & 0.0948 & 0.0822 & 67.8 & 54.59\% & 44.63 \\
			& \ding{51} & & \textbf{95.73\%} & 0.0590 & 0.0643 & 70.4 & 56.68\% & 46.11 \\
			& & \ding{51} & 95.70\% & 0.0598 & \textbf{0.0615} & 70.3 & 56.70\% & 46.08 \\
			& \ding{51} & \ding{51} & 95.70\% & \textbf{0.0588} & 0.0622 & \textbf{70.7} & \textbf{57.03\%} & \textbf{46.50} \\
			\hline
			\multirow{4}*{ResNet-34~\cite{he2016deep}} & & & 92.84\% & 0.0918 & 0.0796 & 68.4 & 56.62\% & 46.00 \\
			& \ding{51} & & 95.68\% & 0.0584 & 0.0634 & 70.7 & 58.36\% & 47.29 \\
			& & \ding{51} & 95.70\% & \textbf{0.0533} & 0.0681 & 70.7 & 58.11\% & 47.30 \\
			& \ding{51} & \ding{51} & \textbf{95.83\%} & 0.0587 & \textbf{0.0599} & \textbf{70.9} & \textbf{59.93\%} & \textbf{49.51} \\
			\hline
			\multirow{4}*{ERFNet~\cite{romera2017erfnet}} & & & 94.90\% & 0.0379 & 0.0563 & 73.1 & 55.36\% & 47.04 \\
			& \ding{51} & & 96.01\% & \textbf{0.0329} & 0.0458 & \textbf{74.2} & 57.47\% & 48.97 \\
			& & \ding{51} & 95.94\% & 0.0340 & 0.0451 & 74.1 & 57.51\% & 48.24 \\
			& \ding{51} & \ding{51} & \textbf{96.12\%} & 0.0331 & \textbf{0.0450} & \textbf{74.2} & \textbf{60.24\%} & \textbf{51.77} \\
			\hline
		\end{tabular}
	}
	\caption{Performance comparison of various combinations of HESA and VESA modules with TuSimple, CULane, and BDD100K test sets}
	\label{table:four}
	\vspace{-3ex}
\end{table*}

The combination of the baseline model ERFNet and ESA module outdoes the performance of the ERFNet and achieves state-of-the-art performance for CULane and BDD100K. In particular, ERFNet-H\&VESA provides significant performance gains for almost all driving scenarios in the CULane dataset compared to ERFNet. However, the runtime and number of parameters remain unchanged. In addition, ERFNet-H\&VESA surpasses the existing methods by achieving an F1-measure of 69.5 in the challenging low-light environment in the lane detection with the CULane dataset. It has a fast runtime similar to those of the previous state-of-the-art methods in Table~\ref{table:one}. Thus, the proposed method is much more efficient than the previously proposed methods. As shown in Table~\ref{table:three}, compared to ERFNet, ERFNet-HESA increases accuracy from 55.36\% to 57.47\% with the BDD100K dataset. In addition, ERFNet-H\&VESA achieves the highest accuracy of 60.24\%. These results show that the HESA and VESA modules work complementarily. The regarding details are covered in Section~\ref{ablation}. The results of the TuSimple dataset in Table~\ref{table:two} show the effect of the ESA module, but it does not achieve the highest performance. The TuSimple dataset contains images of highways with bright light, and generally less occlusion. Because the ESA module extracts global contextual information by predicting the occluded location, our method is less effective for datasets with less occlusion. Unlike our method, ENet-SAD~\cite{hou2019learning} provides additional supervision signals to lane areas and SCNN~\cite{pan2017spatial} applies a message passing mechanism between adjacent pixels in visible lanes. EL-GAN~\cite{ghafoorian2018gan} uses adversarial learning to capture sparse supervision from visible lanes or sharpen blurry lanes. Therefore, these methods are effective in datasets with less occlusion. 

We provide qualitative results of our algorithm for various driving scenarios in three benchmarks. In particular, the first and second rows of Figure~\ref{fig:culane} show that our method can detect sharp lanes even under extreme lighting conditions and in situations in which the lanes are barely visible owing to other vehicles. Figure~\ref{fig:bdd} (a) shows that the ESA module can connect the lanes occluded by vehicles without interruption. According to Figure~\ref{fig:bdd} (b), the approach achieves more accurate lane detection in low-light environments. Thus, compared to the baseline model, the ESA module can improve performance in challenging driving scenarios with extreme occlusion and lighting conditions.

\subsection{Ablation Study}
\label{ablation}

\noindent
\textbf{Combination of HESA and VESA.} Table~\ref{table:four} summarizes the performance characteristics of different combinations of HESA and VESA. The following observations can be made. (1) The performance characteristics of the HESA and VESA modules are similar. (2) In general, the performance of H\&VESA with HESA and VESA modules applied simultaneously is better. In addition, H\&VESA results in a remarkable performance improvement for BDD100K. The reason why the HESA and VESA modules lead to similar performance characteristics is that the predicted direction of the lane confidence is not important for extracting the low-level occlusion location because the lane has a simple geometric shape. Because the HESA and VESA modules complement each other to extract more abundant global contextual information, it is not surprising that H\&VESA generally achieves the highest performance. Therefore, global contextual information is more important for the BDD100K dataset, which includes many lanes.

\begin{figure}
	\setlength{\belowcaptionskip}{-24pt}
	\begin{center}
		\includegraphics[width=0.8\linewidth]{./image/figy/1.pdf}
		\vspace{-3ex}
		\caption{Comparison of performance characteristics with respect to  for the CULane validation set.}
		\label{fig:graph}
	\end{center}
\end{figure}

\noindent
\textbf{Value of .} Figure~\ref{fig:graph} compares the total F1-score of the CULane validation set with respect to  in Equation~(\ref{eq:three}). As shown in Figure~\ref{fig:graph}, the model shows the best performance at  in ERFNet-HESA. It is important to find a suitable  value because it determines the ratio of occluded and normal areas. When the  is small (\ie, when the predicted occlusion area is wide), the sensitivity to occlusion decreases, which makes it difficult to determine the occluded location accurately. Conversely, when  is large, the detected occlusion area becomes narrow, which makes it difficult for the network to reinforce learning for the entire occluded area.

\section{Conclusion}
This paper proposes ESA module, a novel self-attention module for robust lane detection in occluded and low-light environments. The ESA module extracts global contextual information by predicting the confidence of the lane. The performance of the model is evaluated on the datasets containing a variety of challenging driving scenarios. According to the results, our method outperforms previous methods. We confirm the effectiveness of the ESA module in various comparative experiments and demonstrate that our method is robust in challenging driving scenarios.

\noindent\footnotesize\textbf{Acknowledgement.} This research was supported by Multi-Ministry Collaborative R\&D Program(R\&D program for complex cognitive technology) through the National Research Foundation of Korea(NRF) funded by MSIT, MOTIE, KNPA(NRF-2018M3E3A1057289).


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
