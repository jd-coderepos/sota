\documentclass[12pt]{article}

\usepackage[textwidth=19cm, textheight=24cm]{geometry}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm, amssymb}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{multirow}
\usepackage{booktabs} 

\usepackage{xcolor}
\def\rd{\color{red}}
\def\bk{\color{black}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\usepackage{subcaption}
\usepackage{array}
\usepackage{wrapfig}
\usepackage{enumerate}

\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{}
\chead{\small\textit{Mondal et al./Morphological Network: How Far Can We Go with Morphological Neurons?}}
\rhead{}
\cfoot{\small\thepage}
\renewcommand{\headrulewidth}{0pt}


\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


\newcommand\Myperm[2][^n]{\prescript{#1\mkern-2.5mu}{}P_{#2}}
\newcommand\Mycomb[2][^n]{\prescript{#1\mkern-0.5mu}{}C_{#2}}


\begin{document}

\title{\bf Morphological Network: How Far Can We Go with Morphological Neurons?}
\author{
\normalsize{\textbf{Ranjan Mondal}}\\
\normalsize{Electronics and Communication Sciences Unit}\\
\normalsize{Indian Statistical Institute, Kolkata}\\
{\tt\small ranjan15\_r@isical.ac.in}\\
\and
\normalsize{\textbf{Soumendu Sundar Mukherjee}\thanks{Supported by an INSPIRE Faculty Fellowship, Department of Science and Technology, Government of India.}}\\
\normalsize{Interdisciplinary Statistical Research Unit}\\
\normalsize{Indian Statistical Institute, Kolkata}\\
{\tt\small soumendu041@gmail.com}\\
\and
\normalsize{\textbf{Sanchayan Santra}}\\
\normalsize{Electronics and Communication Sciences Unit}\\
\normalsize{Indian Statistical Institute, Kolkata}\\
{\tt\small sanchayan\_r@isical.ac.in}\\
\and
\normalsize{\textbf{Bhabatosh Chanda}}\\
\normalsize{Electronics and Communication Sciences Unit}\\
\normalsize{Indian Statistical Institute, Kolkata}\\
{\tt\small chanda@isical.ac.in}
}



\maketitle

\begin{abstract}
In recent years, the idea of using morphological operations as networks has received much attention. Mathematical morphology provides very efficient and useful image processing and image analysis tools based on basic operators like dilation and erosion, defined in terms of kernels. Many other morphological operations are built up using the dilation and erosion operations. Although the learning of structuring elements such as dilation or erosion using the backpropagation algorithm is not new, the order and the way these morphological operations are used is not standard. In this paper, we have theoretically analyzed the use of morphological operations for processing 1D feature vectors and shown that this gets extended to the 2D case in a simple manner. Our theoretical results show that a morphological block represents a sum of hinge functions. Hinge functions are used in many places for classification and regression tasks (Breiman (1993) \cite{breiman1993hinging}). 
We have also proved a universal approximation theorem---a stack of two morphological blocks can approximate any continuous function over arbitrary compact sets.
To experimentally validate the efficacy of this network in real-life applications, we have evaluated its performance on satellite image classification datasets since morphological operations are very sensitive to geometrical shapes and structures. We have also shown results on a few tasks 
like segmentation of blood vessels from fundus images, segmentation of lungs from chest x-ray and image dehazing. The results are encouraging and further establishes the potential of morphological networks. 
\end{abstract}


\section{Introduction}
In recent years, there has been a renewed interest in learning structuring elements of morphological operations. Mathematical morphology is a set and lattice theoretic technique for the analysis of geometrical structures. It originated from the study of the geometry of porous materials, and later on formalized for binary images. Afterwards, it has been extended to grayscale \cite{serra1983image} and color images \cite{angulo2003morphological} as well. Currently, mathematical morphology is employed in the analysis of graphs, meshes and many other spatial structures, although digital images remain its common application domain. It serves as a non-linear tool for processing digital images based on shapes present in them. Morphological operators decompose objects or shapes into meaningful parts which helps in understanding them in terms of the elements. Since identification of objects and their features are directly correlated with their shapes and arrangement, morphological methods are quite suited for visual tasks~\cite{Haralick1992computer}. Current approaches to image analysis mainly utilize tools of linear systems, which may not always be suitable or efficient for the task at hand. Being a powerful nonlinear methodology, mathematical morphology can potentially be a viable alternative to the existing image analysis tools.

The most basic morphological operation dilation and its dual, erosion, have quite a bit of similarity with the convolution operation. Similar to convolution, these morphological operators work by operating a structuring element (kernel in case of convolution) locally over the whole input. But instead of sum of products, they compute  (or, ) over sums (resp. differences). More complex morphological operators are constructed by the composition of these two basic operators. It is known in the community that, for a given problem, a single image transformation seldom gives the desired output. In fact, most real-world problems are complex in nature and potentially require a combination of elementary transformations. So, a sequence of operations are often employed with unique structuring elements. But the choice of the operations and the associated structuring elements requires expert knowledge along with trial-and-error. If we draw an analogy with convolutional neural networks (CNN), there also a sequence of convolution operators are employed to extract hierarchy of features. The network starts from simple ones, like edges and corners, and progressively composes them to extract complex features. Learning the hierarchy of features automatically from data is one of the reasons of the success of CNNs. Similar things may be achieved if we arrange the dilation and erosion operations in a network form and try to learn the structuring elements automatically from data. However, to achieve acceptable performance, this network should be generic enough. We have shown that the networks with morphological operations, when built in a certain way, are also generic like neural networks. 
Our contributions can be summarized as follows. We start with the most basic version of a 1D morphological network, where the morphological operators work over the whole input, not locally, to facilitate theoretical analysis. This single layer version of the proposed morphological network, called a Morphological Block, contains a layer of both dilation and erosion operations, followed by a layer computing linear combination of the outputs of these operators. 
We show that this Morphological block is a sum of hinge functions, and it can produce an exponential number of hyperplanes in comparison with a neural network with a single hidden layer. Then we turn our attention to the deeper architectures and provide the required analysis during the journey. We prove that two morphological blocks, when used together, can approximate any continuous function over arbitrary compact sets. This simple version of the network is then extended to the 2D version where the dilation and erosion are performed locally withholding all the properties of the 1D network.  

The rest of the paper is organized as follows. Existing works that tried to combine morphological operations and neural networks are briefly outlined in Section~\ref{sec:rel_work}. Some other related works are also mentioned in this section. In Section~\ref{sec:morph_net}, we describe our morphological network along with a theoretical analysis of its capabilities. Section~\ref{sec:results} provides a few applications of the proposed networks. Finally, concluding remarks are presented in Section~\ref{sec:conclustion}. 


\section{Related works}
\label{sec:rel_work}
The use of morphological operations in the form of a network was first introduced by Davidson and Hummer~\cite{davidson_morphology_1993} in their effort to learn the structuring element of dilation operation on images. Similar effort has been made to learn the structuring elements in a more recent work by Masci et al.~\cite{masci2013learning}. The use of morphological neurons in a more general setting was first proposed by Ritter and Sussner~\cite{ritter_introduction_1996}. They restricted the network to a single-layer architecture and focused only on the binary classification task. To classify the data, these networks used two axis-parallel hyperplanes as the decision boundary. This single-layer architecture has been extended to two-layer architecture by Sussner~\cite{sussner_morphological_1998}. This two-layer architecture can learn multiple axis-parallel hyperplanes, and therefore is able to solve arbitrary binary classification tasks. But, in general, the decision boundaries may not be axis-parallel, and so, large number of hyperplanes need to be learned by the network for such decision boundaries. So, 
Barmpoutis and Ritter~\cite{barmpoutis_orthonormal_2006} proposed to learn an additional rotational matrix that rotates the input before trying to classify data using axis-parallel hyperplanes. 
In a separate work by Ritter et al.~\cite{ritter_two_2014} the use of  and  norm has been proposed as a replacement of the \emph{max/min} operation of dilation and erosion in order to smooth the decision boundaries. 
Ritter and Urcid~\cite{ritter_lattice_2003} first introduced the dendritic structure of biological neurons to the morphological networks. This new structure creates hyperbox based decision boundaries instead of hyperplanes. The authors have proved that hyperboxes can estimate any compact region and, thus, any two-class classification problem can be solved. A generalization of this structure to the multiclass case has also been done by Ritter et al.~\cite{ritter_learning_2007}. Sussner and Esmi~\cite{sussner_morphological_2011} proposed a new type of structure called morphological perceptrons with competitive neurons, where the output followed winner-take-all strategy. This was modelled using the \emph{argmax} operator that allows the network to learn more complex decision boundaries.
For morphological neurons with dendritic structure Zamora and Sossa~\cite{zamora_dendrite_2017} replaced the \emph{argmax} operator with a softmax function. This overcomes the problem of gradient computation and, therefore, gradient descent optimizer was employed. 

Due to the non-differentiability of the max/min operations, researchers had to use optimizer other than gradient descent based methods to train their networks. A separate line of research has been attempted to overcome this. 
Ara\'{u}jo~\cite{de_a_araujo_morphological_2012} utilized network architecture similar to morphological perceptrons with competitive learning to forecast stock markets. The \emph{argmax} operator was replaced with a linear activation function so that the network is able to regress forecasts and enables the use of gradient descent for training. Recently, a few works~\cite{aminul2019deep,franchi2020deep,nogueira2019introduction} has appeared in the field. Franchi et al.\cite{franchi2020deep} replace standard max pooling in CNN with a learned morphological pooling. They build a network which is mixed with CNNs for image image denoising. Nogueira et al.\cite{nogueira2019introduction}  introduces morphological networks with multiple morphological operations for image classification tasks. Islam et al.~\cite{aminul2019deep} use morphological hit-or-miss transform to build the network. Mondal et al.~\cite{mondal2020image} introduce opening closing network for image de-raining and de-hazing with morphological opening and closing operation. The authors of \cite{limonova2020bipolar} use morphological operations to build Bipolar morphological neural networks.
Although morphological operations has been utilized in a network form in various ways for specific applications, its generic theoretical justification is scarce till date. It is still an open question how morphological networks should be designed so that they become a generic tool that can solve any learning problem.
In the following subsections, we have tried to answer these questions. 

\section{Morphological networks}
\label{sec:morph_net} 
In this section we introduce the morphological network, or `Morph-Net' in short. It is a simple feed forward network that consists of dilation and erosion neurons. 
We begin with defining dilation and erosion neurons, and then describe the simplest version of the network and its capabilities. 

\subsection{Dilation and erosion neurons}
\label{sec:de_neurons} 
Dilation and Erosion neurons are the building blocks of our proposed network. 
Given an input  and a structuring element , the operation of \newterm{dilation} () and \newterm{erosion} () neurons are defined, respectively, as follows.

 denotes  element of augmented input vector . 
Note that the erosion operation may also be written as . In these neurons the structuring element () is learned in the training phase.

The  and  operators used in the dilation and erosion neurons are only piece-wise differentiable. As a result,  only a single element of structuring element is updated at each iteration of backpropagation algorithm. To overcome this problem we propose to use the soft version of  and ~\cite{cook2011basic} to define soft dilation and erosion operation as follows.

where  and  denote the soft dilation and soft erosion, respectively, and  is the "hardness" of the soft operations.The soft version can be made close to their ``hard'' counterpart by making  large enough \cite{cook2011basic}. 

\begin{figure}
    \centering
    \includegraphics[width=0.64\linewidth]{./images/morph_net_nb.pdf}   
    \caption{Architecture of single layer morphological block. It contains an input layer, a dilation-erosion layer with  dilation and  erosion neuron and a linear combination layer with  neurons producing the output.}
    \label{fig:single_layer_network}
\end{figure}

\subsection{The morphological block}
\label{sec:SL_morph}
The simplest form of Morph-Net is the morphological block. It consists of a layer with dilation and erosion neurons followed by a linear combination (\Figref{fig:single_layer_network}) of their outputs. We call the layer of dilation and erosion neurons as the \newterm{dilation-erosion layer} and the following layer as the \newterm{linear combination layer}. Let us consider a network with  dilation neurons and  erosion neurons in the dilation-erosion layer followed by  neurons in the linear combination layer. Let  be the input to the network and  and  be the output of the  dilation neuron and the  erosion neuron respectively: 

where  and  are the structuring elements of the respective neurons. Note that  and . The final output of a node in the linear combination layer is computed as 

where  and  are the weights of the combination layer. When the network is trained, it learns , ,  and . 

\subsection{Morphological block as sum of hinge functions}
Hinging hyperplanes \cite{breiman1993hinging} have many applications in regression and classification tasks. In this subsection, we show that the simple morphological block can be represented as a sum of hinge functions. Further, we try to develop the intuition behind a single morphological block with a toy example. 

\begin{figure*}
    \centering
    \includegraphics[width=0.3\linewidth]{images/1D/legend.pdf}
    
    \smallskip
    \begin{subfigure}[t]{0.247\linewidth}
        \includegraphics[width=\linewidth]{./images/1D/circle_tanh.pdf}
        \caption{NN-ReLU}
        \label{fig:decision_nn}
    \end{subfigure}
    \begin{subfigure}[t]{0.24\linewidth}
        \includegraphics[width=\linewidth]{images/1D/circle_maxout.pdf}
        \caption{Maxout network}
        \label{fig:decision_maxout}
    \end{subfigure}
    \begin{subfigure}[t]{0.216\linewidth}
        \includegraphics[width=\linewidth]{./images/1D/circle_morph.pdf}
        \caption{Morph-Net}
        \label{fig:decision_morph}
    \end{subfigure}
    \begin{subfigure}[t]{0.2175\linewidth}
        \includegraphics[width=\linewidth]{./images/1D/circle_Smorph.png}
        \caption{Soft Morph-Net}
        \label{fig:decision_morph_soft}
    \end{subfigure}
\caption{Decision boundaries learned by different networks with two hidden neurons. (a)~Baseline neural network is able to learn only two planes
(b)~Maxout network 
is able to learn two more planes with the help of additional parameters. 
(c)~Morph-Net is able to learn more planes with same number of parameters as NN-ReLU. 
(d)~Using soft version of Morph-Net, smooths the learned decision boundary. 
This further enhances the discrimination capability of the network while retaining the same number of parameters.}
    \label{fig:pmap_circle}
\end{figure*}


\begin{definition}[-order Hinge Function \cite{wang2005generalization}]
    A -order hinge function  consists of  hyperplanes continuously joined together. It may be defined as 
    
\end{definition}

\begin{proposition}
\label{th:gx_sum_hinge}
The function computed by a single layer Morph-Net (denoted by ) with  dilation and  erosion neurons followed by their linear combination, is a sum of multi-order hinge functions.
\end{proposition}
\noindent In fact, we can show that 

where ,  and , are -order hinge functions. The proof is given in the appendix.

The function  learned by a single layer Morphological network may also be expressed in the following form:

where , , . 
The derivation can be found in the appendix. 
We see that  is sum of  functions, each of which computes  over the linearly transformed elements of . Since the  is computed over the (transformed) elements of , each  operation selects only one element of . So, the computed  may not contain all the elements of  and the index () of selected element varies depending on the input and the structuring element. However, if ,  may contain all the elements of . So \eqref{eq:num_d_plane} can be rewritten as 

where  represents any one of the  elements of  selected by -th neuron by  operation depending on structuring element . So each  is chosen from  elements. Therefore, depending on which element of  gets selected by each neuron,  forms one of the  hyperplanes. 
In case of added bias to the neurons i.e appended 0 to input,  forms one of the  hyperplanes. The  occurs in number of hyperpalnes because in one occasion  all neurons select the augmented . 

Note that some of these hyperplanes must be parallel to some axes. 
For  to form a hyperplane that is not parallel to any of the axes, all elements of  must get selected by some  functions or other. This occurs in  ways. The remaining  number of elements 's are repeat selection by some functions. So, there can be almost  hinging hyperplanes that are not parallel to any of the axes. If we consider bias in morphological neurons the number will be . Therefore, number of hinging hyperplanes increases exponentially with the number of neurons in the dilation-erosion layer. These hyperplanes can act as decision boundaries if the network is employed as a classifier. This is demonstrated experimentally using a toy dataset representing two-class problem. 

The toy dataset contains samples that are distributed along two concentric circles, one circle for each class. Suppose, the circles are centered at the origin.
We compare the results of various networks with two neurons in the hidden layer. It is observed that baseline neural network fails to classify this data as with two hidden neurons it learns only two hyperplanes, one for each neuron. The boundaries learned by the network with ReLU activation function (NN-ReLU) is shown in \Figref{fig:decision_nn}. The result of maxout network is better,  
because, in this case, the network learns  hyperplanes as shown in \Figref{fig:decision_maxout}. 
Note that with two morphological neurons in dilation-erosion layer, our Morph-Net has learned 6 hyperplanes to form the decision boundary (\Figref{fig:decision_morph}).  \eqref{eq:num_d_plane} suggests that we should get at most 8 distinct lines. However, out of these only two decision boundaries are placed in any arbitrary orientation in the 2D space, while others are parallel to either of the axes. 

\subsection{Multilayer Morph-Net: universal approximation}
In this subsection, we discuss universal approximation properties of multilayer morphological networks. We show that two layer morphological networks can approximate any continuous function.

We begin with a lemma that shows that two layer Morph-nets can represent any linear combination of hinge functions. Its proof is given in the appendix.
\begin{lemma}
Any linear combination of hinge functions  can be represented over an arbitrary compact set  as a two-layer Morph-Net consisting of dilation neurons only.
\end{lemma} 

\begin{theorem}[Universal approximation]
Two-layer Morph-nets can approximate continuous functions over arbitrary compact sets.
\end{theorem}

\begin{proof}
Continuous functions can be approximated over compact sets by sums of hinge functions (see Theorem 3.1 of \cite{breiman1993hinging}). Therefore, by Lemma 1, it follows that any continuous function can be approximated over arbitrary compact sets by two-layer Morph-Nets.
\end{proof}

Universal approximation properties of single layer Morph-Nets are not known. However, we have a few suggestive empirical results regarding this in the appendix.

\subsection{2D morphological network }

Let gray-scale image  is of size . The Dilation~() and Erosion~() operations on   are defined as~\cite{sternberg1986grayscale} 
{\small 
 
}
where   is the 2D structuring element of size  defined on domain . Since  and , at each location  of image  we consider a box of pixels of size  which is operated on by  to produce the output at  independent of output at any other location. We can flatten the box of pixels to generate  (assuming ). Similarly we can flatten  and append an element to obtain . So we can employ \eqref{eq:dilation} and \eqref{eq:erosion} to get similar result as obtained by \eqref{eq:dilation2D} and \eqref{eq:erosion2D} respectively. As a result, all the properties stated above for 1D are also true for 2D. Thus same morphological network is used for 2D operations too. Similarly, it can be defined for colour images. Figure~\ref{fig:2dd1e1_lc} shows a morphological block containing a layer of 6 dilation and 6 erosion , followed by 4 different linear combinations of feature maps. The structuring element size is . This entire block is denoted by .


\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{images/2D_morph_block.pdf}
    \caption{A single 2D morphological  block consisting of a Dilation-Erosion layer  followed by linear combination of dilated and eroded featuremap. The block is denoted by  .}
    \label{fig:2dd1e1_lc}
\end{figure} 

\section{Results}
\label{sec:results}
The applicability of the proposed Morphological Network in real-world problems is shown with the help of a few experiments. The Morphological operators are known to be effective at extracting geometrical features. This has been illustrated with the help of two satellite image dataset. Morphological Network is not only good at extracting geometrical features, it also adept at tasks involving image to image transformation. We have shown the results on a few tasks like, segmentation of medical images and image dehazing. They are given in the following subsections.

\begin{table*}
    \centering
    \caption{Accuracy achieved on the UC Merced and WHU-RS19 datasets}
    \label{UCMerced}
    \begin{tabular}{lcccc}
    \toprule
    \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{Parameters (Millions)} &
    \multicolumn{2}{c}{Accuracy} \\
    \cmidrule(lr){3-4}
    & & UCMerced & WHU-RS19 \\
    \cmidrule(r){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(l){5-5}
    LeNet\cite{lecun1998gradient}                          & 4.42          &           & \\
Depth-LeNet                                     & 5.04          &           &   \\
DeepMorphLeNet                                  & 6.04          &           &  \\
AlexNet-based\cite{krizhevsky_imagenet_2012}               & 6.5           &           &  \\
Depth-AlexNet-based                             & 7.47          &           &  \\
DeepMorphAlexNet\cite{nogueira2019introduction} & 10.5          &  &   \\
Morph-Net (ours)                     & \textbf{2.23} &          &    \\
    \bottomrule
    \end{tabular}
    \label{quant_UCRS}
\end{table*}
\begin{figure*}
    \centering 
    \includegraphics[width=0.9\linewidth]{images/2D/eye/eye_network.pdf}
    \caption{Architecture of the Morph-Net utilized for blood vessel segmentation in retinal images.}
    \label{fig:eye_network}
\end{figure*}

\subsection{Satellite image classification}
We have evaluated morphological network for image classification task on two publicly available dataset: UCMerced Land-use~\cite{yang2010bag} and WHU-RS19 Dataset~\cite{xia2010structural}. For training and testing, we have utilized the protocol mentioned in~\cite{nogueira2019introduction}. Instead of convolution we have employed 2D morphological network to build up the network. We have employed  Alex-net like architecture using for this purpose. The network is trained in each fold for upto 300 epochs for each dataset. All the images are resized to  before they are fed into the network. From the results reported in the Table~\ref{quant_UCRS}, it can be seen that our network works as good as the method of~\cite{nogueira2019introduction} while requiring only a fraction of the parameters. It is also seen not just ours, all morphological networks are performing better than CNNs.  

\subsection{Morphological network for image  transformation}
In order to demonstrate the applicability of morphological networks in generic image transformation tasks, we have compared its performance in three different tasks: segmentation of retina from fundus images, segmentation of lung from from X-Ray images and image dehazing. We have shown the performance on two different type of segmentation, because segmenting fine structures from images is different from segmenting large structures. The problem of image dehazing is utilized to show how this network may perform on natural images. We have also shown image de-hazing with morphological network. Please refer to the appendix for a more detailed comparison.

\subsubsection{Retinal segmentation}
\begin{figure}
    \centering
    \begin{subfigure}[t]{0.325\linewidth}
        \includegraphics[width=\linewidth]{./images/2D/eye/stare/input/im0077.jpg}
        \includegraphics[width=\linewidth]{./images/2D/eye/stare/input/im0255.jpg}
        \caption{Input}
    \end{subfigure}
    \begin{subfigure}[t]{0.325\linewidth}
        \includegraphics[width=\linewidth]{./images/2D/eye/stare/ours/im0077.jpg}
        \includegraphics[width=\linewidth]{./images/2D/eye/stare/ours/im0255.jpg}
        \caption{Morph-Net}
    \end{subfigure}
    \begin{subfigure}[t]{0.325\linewidth}
        \includegraphics[width=\linewidth]{./images/2D/eye/stare/ground_truth/im0077.jpg}
        \includegraphics[width=\linewidth]{./images/2D/eye/stare/ground_truth/im0255.jpg}
        \caption{Ground truth}
    \end{subfigure}
\caption{Results obtained by 2D Morph-Net on two samples of the STARE dataset.}
\label{qualstare}
\end{figure}

For performance study, we have used two publicly available retinal image datasets: DRIVE\footnote{https://drive.grand-challenge.org/} and STARE\footnote{http://cecas.clemson.edu/~ahoover/stare/}~\cite{hoover2000locating}. For this task, we have employed a 2D Morph-Net containing seven sequential morphological block to produce output.
A sigmoid activation function is utilized only in the last layer to restrict the output to lie between 0 and 1. Other layers do not make use of any activation function. A schematic block diagram of the network architecture is shown in Figure~\ref{fig:eye_network}. 
In retinal images  very small blood vessels small in size and randomly distributed.  In Figure~\ref{qualstare} we have shown few qualitative results and compared with ground truth.  

\subsubsection{Lungs segmentation}
Lung segmentation is an important biomedical image segmentation problem. Unlike the previous task of blood vessel segmentation, here we have to segment a large blob from the image. In this experiment 
we have used the publicly available Shenzhen chest X-Ray imageset~\cite{jaeger2014two}. This problem requires both global and local information, so that a large blob with intricate detail of boundary region (region of interest) can be extracted. This suggests us to employ a U-net~\cite{ronneberger2015u} type of network architecture. Batch normalization taken to increase the stability of the network. In Figure~\ref{Shenzhen_qual}, we have shown a few results before thresholdig.


\begin{figure}[h!]
    \centering
    \begin{subfigure}[t]{0.325\linewidth}
        \includegraphics[width=\linewidth]{./images/2D/lungs/senzen/input/CHNCXR_0608_1.jpg}
        \includegraphics[width=\linewidth]{./images/2D/lungs/senzen/input/CHNCXR_0609_1.jpg}
        \caption{Input}
    \end{subfigure}
    \begin{subfigure}[t]{0.325\linewidth}
        \includegraphics[width=\linewidth]{./images/2D/lungs/senzen/output/CHNCXR_0608_1.jpg}
        \includegraphics[width=\linewidth]{./images/2D/lungs/senzen/output/CHNCXR_0609_1.jpg}
        \caption{Morph-Net}
    \end{subfigure}
    \begin{subfigure}[t]{0.325\linewidth}
        \includegraphics[width=\linewidth]{./images/2D/lungs/senzen/gt/CHNCXR_0608_1_mask.jpg}
        \includegraphics[width=\linewidth]{./images/2D/lungs/senzen/gt/CHNCXR_0609_1_mask.jpg}
        \caption{Ground truth}
    \end{subfigure}
\caption{Results obtained by 2D Morph-Net on the Shenzhen dataset\cite{jaeger2014two} before thresholding.}
\label{Shenzhen_qual}
\end{figure}

\subsubsection{Image dehazing}
\begin{figure*}
\begin{subfigure}[t]{0.19\textwidth}
\includegraphics[width=\textwidth]{./images/2D/dehaze/hazy/36.jpg}
\includegraphics[width=\textwidth]{./images/2D/dehaze/hazy/37.jpg}
\caption{Input image}
\end{subfigure}
\begin{subfigure}[t]{0.19\textwidth}
\includegraphics[width=\textwidth]{./images/2D/dehaze/T/T_36.jpg}
\includegraphics[width=\textwidth]{./images/2D/dehaze/T/T_37.jpg}
\caption{Transmittance}
\end{subfigure}
\begin{subfigure}[t]{0.19\textwidth}
\includegraphics[width=\textwidth]{./images/2D/dehaze/K/K_36.jpg}
\includegraphics[width=\textwidth]{./images/2D/dehaze/K/K_37.jpg}
\caption{Airlight}
\end{subfigure}
\begin{subfigure}[t]{0.19\textwidth}
\includegraphics[width=\textwidth]{./images/2D/dehaze/output/36.jpg}
\includegraphics[width=\textwidth]{./images/2D/dehaze/output/37.jpg}
\caption{Our output}
\end{subfigure}
\begin{subfigure}[t]{0.19\textwidth}
\includegraphics[width=\textwidth]{./images/2D/dehaze/gt/36.jpg}
\includegraphics[width=\textwidth]{./images/2D/dehaze/gt/37.jpg}
\caption{Ground truth}
\end{subfigure}
\caption{Results of our 2D Morph-Net on three validation image of the O-HAZE dataset. Transmittance and airlight map is shown along with ground truth for comparison.}
\label{qual_ntire}
\end{figure*}


Image De-hazing is one of the trending problems of image to image transformation. Haze occurs due to scattering of the light by the particles present in the atmosphere. Here, the hazy image formation process is modelled as follows~\cite{mondal2018image} 

where  and  are the haze-free and the observed hazy image, respectively.  is the scene transmittance that denotes the amount of haze in pixel , and  is called air light map. 
To dehaze an image, we propose a 2D Morph-Net that takes hazy image () as input and produces estimated the airlight map  and the transmittance map  as output. We minimize a loss function inspired from the bi-directional consistency loss \cite{mondal2018image} based on Structural Similarity Index (SSIM) between two images~\cite{wang2004image}. The network is trained on the training images of O-HAZE dataset~\cite{ancuti2018haze} until convergence of the training error. Some results are shown in \Figref{qual_ntire} for qualitative evaluation and comparison. It can seen that Morphological network is able to extract consistent transmittance map and airlight from image hazy images. 


\section{Conclusion}
\label{sec:conclustion}  
In this paper, we have proposed a novel trainable morphological network (Morph-Net) using dilation and erosion operators as neurons. It is shown that these operators, in conjunction with linear combinations, represent a sum of hinge functions. As for multi-layer morphological networks, we have proved that taking two sequential morphological blocks can approximate any continuous function. The 1D Morph-Net is readily extended to 2D, forming a CNN like network with all the characteristics of the 1D morphological network. Efficacy of the 2D Morph-Net is evaluated by applying it on various computer vision problems. The proposed network, both in 1D and 2D, has great potential to be applied to many more problems, especially in image-processing. However, there is also scope for future work regarding architecture design and optimization. 


{
\small
\bibliographystyle{plain}
\bibliography{egbib}
}

\appendix


\section{Soft maximum}
\begin{lemma}
The soft dilation operation converges to the dilation operation as . This is true for the soft erosion operation also.
\end{lemma}
\begin{proof}
The proof if standard. We include it here for completeness. Let . We have

This completes the proof.
\end{proof}


\section{Single morphological block as a sum of hinge functions}
\label{sec:proof_lemma1}
\begin{proposition}[Proposition 1 of the main paper]
\label{th:gx_sum_hinge}
The function computed by a single layer Morph-Net with  dilation and  erosion neurons followed by a linear combination layer computes , which is a sum of multi-order hinge functions.
\end{proposition}
\begin{proof}
As defined in the main paper the computed  has the following form.

where  and  are the output of  dilation neuron and  erosion neuron, respectively and  and  are the weights of the the linear combination layer. Replacing the  and  with their expression, the equation becomes the following.

where  and  denote the  component of the  structuring element of dilation and erosion neurons, respectively. The above equation can be further expressed in the following form:

where , ,  and  are defined in the following way:


Now, without any loss of generality, we can write \eqref{eq:g_phi} as follows:

where 

Clearly, Equation~\ref{eq:finaleq} can be rewritten as 

where ,  and
 
with

In \eqref{eqstd1},  is affine and  is a -order hinge function. Hence , i.e. , represents a sum of multi-order hinge functions. 
\end{proof}

\subsection{A single morphological block and universal approximation}
A single morphological block represents a sum of hinge functions. However, it is not clear if all hinge functions can be represented by a single morphological block. 
In a numerical study, we have tried to approximate the hinge function  using a single morphological block by varying the number of dilation/erosion neurons. We have generated values of the function in the square , and trained the network with mean squared error (MSE) loss. In  Figure~\ref{fig:morph-block}, we have plotted the MSE loss (after convergence) against the number of morphological neurons used. It is seen that a single morphological block is unable to reduce the error unless we use additional bias terms in the morphological neurons. However, we do not know theoretically if having additional bias terms in morphological operations help in universal approximation.

We have also tried to perform classification task in CIFAR-10 and SVHN using a single morphological block with bias in morphological neurons. The results are shown in Figures~\ref{cifar10_lc} and~\ref{svhn_lc}. It can be seen that a single layer morphological block (with bias) works better than neural networks of similar architecture in CIFAR-10.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/Loss-without.pdf}
    \caption{Graph of approximation loss with varying  morphological neurons in a single morphological block.}
    \label{fig:morph-block}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}[t]{0.49\linewidth}
        \includegraphics[width=\linewidth]{./images/1D/graphs_bar_error/error_cifar10_400.pdf}
        \caption{CIFAR-10}
        \label{cifar10_lc}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\linewidth}
        \includegraphics[width=\linewidth]{./images/1D/graphs_bar_error/error_svhn_400.pdf}
                \caption{SVHN}
        \label{svhn_lc}
            \end{subfigure}
\caption{Test accuracy attained over epochs by different methods with the same number of neurons (400) as in the hidden layer of a single morphological block.}
\end{figure}

\subsection{Two morphological blocks and universal approximation}

\begin{lemma}\label{lem:hyp_rep}
Let  be a compact subset of . Then, over , any hyperplane  can be represented as an affine combination of  dilation neurons which only depend on .
\end{lemma}
\begin{proof}
Since we are in a compact set, there exists  such that  for any . Take 

where  is the vector of all ones and  is the -th unit vector in . Then all but the -th coordinate of  are , while the -th coordinate is . Then note that, for any , and ,

for any . It follows that for any , and ,


Now given any hyperplane , we can express it exactly as a linear combination of dilation neurons over :

This completes the proof.
\end{proof}

\begin{lemma}[Lemma 1 of the main paper]
Any linear combination of hinge functions  can be represented over any compact set  as a two-layer Morph-Net consisting of dilation neurons only.
\end{lemma}
\begin{proof}
Let . We now give the architecture of the desired Morph-Net.
\begin{enumerate}
    \item The first dilation-erosion layer has exactly  dilation neurons given by .
    \item The first linear combination layer has  neurons, with the -th block of  neurons outputting the constituent hyperplanes of . This can be done by Lemma~\ref{lem:hyp_rep}.
    \item The second dilation-erosion layer just has  dilation neurons, each outputting a hinge function. The -th neuron is constructed as follows: Write any  as  where .  We want the output of the -th neuron to be . So we take , where  for , and . Then, for any , , and , we have
    
    It follows that . With this construction, the outputs of the second dilation-erosion layer are the  numbers .
    \item The second linear combination layer just has a single neuron that combines the outputs of the previous layer in the desired way:
    
\end{enumerate}
This completes the proof.
\end{proof}

\section{Properties of multilayer morphological networks}
In this section, we record a few properties of multilayer morphological networks.
\begin{lemma}
Suppose  and  are real-valued functions on . Then  if and only if, for all , one has equality of the sub-level sets:

\label{lemma_feqg}
\end{lemma}
\begin{proof}
The ``only if'' part is trivial. As for the ``if'' part, note that we have

Same goes for , and so, by our hypothesis,

Therefore, for any , we have , or, in other words, .
\end{proof}

\begin{theorem}
The following are true for morphological  network architectures.
\begin{itemize}
    \item[(i)] The architecture  consisting only of dilation layers is equivalent to the architecture  with a single dilation layer. A similar statement is true if one considers architectures with only purely erosion layers.
    
    Which indicates taking multiple sequential dilation or erosion layer is equivalent  of taking a single dilation or erosion layer.  
    \item[(ii)] The architecture  is not equivalent to . Similarly, it is not equivalent to , and, consequently, the architectures  and  are inequivalent. 
    
    \item[(iii)] The architecture  is not equivalent to .
     \item[(iv)] The architecture  is not equivalent to .
\end{itemize}
\end{theorem}

\begin{proof}
(i) Let  be the input to the network . Let there be two networks  and . Let there be  and  dilated neurons in, respectively, the first and the second layers of Network . Let the parameters of the network  in the first layer and 2nd layer are   and   respectively. Whereas let  there are only  single layer with   number of  dilated neurons in network  and the parameters are denoted as  .  Let    and  are the output from the last layer of network  and  respectively.

 
For Network 


For network 


Let



For Network 


From Equation~\ref{eqs0} and Equation~\ref{eqs4} we get 




Which means 



For network  


To hold  the  set   is equal to  to    


Hence, from Lemma \ref{lemma_feqg}, given a parameter   and  of and 2 layer network , there exist a equivalent single layer network  with dilated neurons  which can represent the same  function. From the Equation \ref{eqlongpath} we can see the parameters of the single layer network can be constructed considering the longest path from input to output. Recursively we can say it holds for multiple layers. Similar argument can be given in case of erosion layers.

\vskip10pt
\noindent
(ii)
\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{images/d1e1_d1.pdf}
    \caption{A network of architecture }
    \label{fig:d1e1_d1}
\end{figure}
\begin{figure*}[th!]
\centering
\begin{subfigure}[t]{0.03\linewidth}
\begin{picture}(1,25)
  \put(0,10){\rotatebox{90}{~~~~~[ Input ]}}
\end{picture} \\ ~\\ ~\\ ~\\  ~\\ ~\\  ~\\ 
\begin{picture}(1,25)
  \put(0,5){\rotatebox{90}{~~~~~[ Output ]}}
\end{picture} \\ ~\\ ~\\ ~\\  ~\\ ~\\  ~\\ ~\\ ~\\  
\begin{picture}(1,25)
  \put(0,0){\rotatebox{90}{~~~~~[ Ground Truth ]}}
\end{picture} 
\end{subfigure}
\begin{subfigure}[t]{0.22\linewidth}
\includegraphics[width=\linewidth]{./images/2D/eye/stare/input/im0005.jpg}
\includegraphics[width=\linewidth]{./images/2D/eye/stare/ours/im0005.jpg}
\includegraphics[width=\linewidth]{./images/2D/eye/stare/ground_truth/im0005.jpg}
\caption{}
\end{subfigure}
\begin{subfigure}[t]{0.22\linewidth}
\includegraphics[width=\linewidth]{./images/2D/eye/stare/input/im0236.jpg}
\includegraphics[width=\linewidth]{./images/2D/eye/stare/ours/im0236.jpg}
\includegraphics[width=\linewidth]{./images/2D/eye/stare/ground_truth/im0236.jpg}
\caption{}
\end{subfigure}
\begin{subfigure}[t]{0.22\linewidth}
\includegraphics[width=\linewidth]{./images/2D/eye/stare/input/im0239.jpg}
\includegraphics[width=\linewidth]{./images/2D/eye/stare/ours/im0239.jpg}
\includegraphics[width=\linewidth]{./images/2D/eye/stare/ground_truth/im0239.jpg}
\caption{}
\end{subfigure}
\begin{subfigure}[t]{0.22\linewidth}
\includegraphics[width=\linewidth]{./images/2D/eye/stare/input/im0240.jpg}
\includegraphics[width=\linewidth]{./images/2D/eye/stare/ours/im0240.jpg}
\includegraphics[width=\linewidth]{./images/2D/eye/stare/ground_truth/im0240.jpg}
\caption{}
\end{subfigure}
\caption{Results on the STARE dataset.}
\label{fig:failure_cases_stare}
\end{figure*}
For simplicity, we will assume -dimensional input. Suppose that the outputs from the first layer are  and  where  is the output of a dilation neurone and  is the output of an erosion neurone (see Figure~\ref{fig:d1e1_d1}). We write

After the second layer consisting of a single dilation neurone, we get the output

Note that

Note that  and .
Therefore, if  and , then

Thus in this case  can be realized in the architecture .

If, however,  and , then

which is not realizable as the sublevel set of a function of  architecture.
\vskip10pt
\noindent
(iii) 
The proof is a simple modification of the proof of (ii). For ,

Note that  and .

Therefore, if  and , then

Thus, in this case,  can be realized in the architecture . More explicitly, for ,

Equating , , we can see that one can take  to realize the function  in the  architecture.

If, however,  and , then

which is not realizable as the sublevel set of a function of  architecture.
\vskip10pt
\noindent
(iv) It can be proved in the same way as (ii)
\end{proof}

\section{2D morphological network experiments}

\begin{table}
    \centering
    \caption{Accuracy and AUC achieved on the test set by different networks for the DRIVE and STARE datasets.}
    \label{cifar10_table}
    \scalebox{0.8}{
    \begin{tabular}{ccccc}
    \toprule
    \multirow{2}{*}{\textbf{Methods}} & \multicolumn{2}{c}{DRIVE} & \multicolumn{2}{c}{STARE} \\
    \cmidrule(lr){2-3}\cmidrule(l){4-5}
    & Accuracy &AUC & Accuracy &AUC \\
    \cmidrule(r){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-4}\cmidrule(l){5-5}
    Marin et al.~\cite{marin2010new}&94.50&0.958&95.20&\textbf{0.976}\\
    Staal et al.~\cite{staal2004ridge}&94.40&0.952&-&- \\
    Niemeijer et al.~\cite{niemeijer2004comparative}&94.10&0.929&-&-\\
    Soares et al.~\cite{soares2006retinal}&94.60&0.961&94.80&0.967\\
    Ricci et al.~\cite{ricci2007retinal}&95.90&0.963&\textbf{96.40}&0.968\\
    2D Morph-Net&\textbf{96.50}&\textbf{0.977}&{93.40} & {0.937}\\
    \bottomrule
    \end{tabular}
    }
    \label{quant_eye}
\end{table}


\begin{figure}
    \centering
    \begin{subfigure}[t]{0.325\linewidth}
        \includegraphics[width=\linewidth]{./images/2D/eye/drive/input/01_test.jpg}
        \includegraphics[width=\linewidth]{./images/2D/eye/drive/input/02_test.jpg}
        \caption{input}
    \end{subfigure}
    \begin{subfigure}[t]{0.325\linewidth}
    \includegraphics[width=\linewidth]{./images/2D/eye/drive/ours/01_test.jpg}       
    \includegraphics[width=\linewidth]{./images/2D/eye/drive/ours/02_test.jpg}
    \caption{Morph-Net}
    \end{subfigure}
    \begin{subfigure}[t]{0.325\linewidth}
        \includegraphics[width=\linewidth]{./images/2D/eye/drive/ground_truth/01_manual1.jpg}
        \includegraphics[width=\linewidth]{./images/2D/eye/drive/ground_truth/02_manual1.jpg}
        \caption{Ground Truth}
    \end{subfigure}
\caption{Results obtained by 2D Morph-Net on two samples of the DRIVE dataset.}
\label{qualdrive}
\end{figure}

\subsection{Retinal segmentation}
\begin{figure}
    \centering 
    \includegraphics[width=0.95\linewidth]{images/2D/eye/eye_roc.pdf}
    \caption{ROC curve on the STARE and DRIVE datasets.}
    \label{fig:eye_roc}
\end{figure}

\begin{figure*}[h]
\centering
\begin{subfigure}[t]{0.03\linewidth}
\begin{picture}(1,25)
  \put(0,10){\rotatebox{90}{~~~~~[ Input ]}}
\end{picture} \\ ~\\ ~\\ ~\\  ~\\ ~\\  ~\\ 
\begin{picture}(1,25)
  \put(0,5){\rotatebox{90}{~~~~~[ Output ]}}
\end{picture} \\ ~\\ ~\\ ~\\  ~\\ ~\\  ~\\ ~\\ ~\\  
\begin{picture}(1,25)
  \put(0,0){\rotatebox{90}{~~~~~[ Ground Truth ]}}
\end{picture} 
\end{subfigure}
\begin{subfigure}[t]{0.22\linewidth}
\includegraphics[width=\linewidth]{./images/2D/eye/drive/input/07_test.jpg}
\includegraphics[width=\linewidth]{./images/2D/eye/drive/ours/07_test.jpg}
\includegraphics[width=\linewidth]{./images/2D/eye/drive/ground_truth/07_manual1.jpg}
\caption{}
\end{subfigure}
\begin{subfigure}[t]{0.22\linewidth}
\includegraphics[width=\linewidth]{./images/2D/eye/drive/input/04_test.jpg}
\includegraphics[width=\linewidth]{./images/2D/eye/drive/ours/04_test.jpg}
\includegraphics[width=\linewidth]{./images/2D/eye/drive/ground_truth/04_manual1.jpg}
\caption{}
\end{subfigure}
\begin{subfigure}[t]{0.22\linewidth}
\includegraphics[width=\linewidth]{./images/2D/eye/drive/input/05_test.jpg}
\includegraphics[width=\linewidth]{./images/2D/eye/drive/ours/05_test.jpg}
\includegraphics[width=\linewidth]{./images/2D/eye/drive/ground_truth/05_manual1.jpg}
\caption{}
\end{subfigure}
\begin{subfigure}[t]{0.22\linewidth}
\includegraphics[width=\linewidth]{./images/2D/eye/drive/input/06_test.jpg}
\includegraphics[width=\linewidth]{./images/2D/eye/drive/ours/06_test.jpg}
\includegraphics[width=\linewidth]{./images/2D/eye/drive/ground_truth/06_manual1.jpg}
\caption{}
\end{subfigure}
\caption{Results on the DRIVE dataset.}
\label{fig:failure_cases_drive}
\end{figure*}

The DRIVE dataset has 40 eye-fundus color images captured with Canon CR5 non-mydriatic 3CCD camera with a  field-of-view (FOV). Each image is of size . The dataset is divided into two groups: 20 training and 20 test images. The STARE dataset consists of 20 eye-fundus color images captured with a TopCon TRV-50 fundus camera at  FOV. Each image of STARE datasets has a size of . 

For this task we have employed a 2D Morph-Net containing 7  blocks. 
A sigmoid activation function is utilized only in the last layer to restrict the output to lie between 0 and 1. 
The network is designed to take an input of size  and produces a output of size . So, database images are resized to  using bilinear interpolation, before they are fed to the network. The network is trained using the binary cross-entropy loss and Adam optimizer \cite{kingma_adam:_2014} with batch size 6. The images obtained from the network is thresholded at 0.5 to get the final result. Since, unlike DRIVE dataset, STARE dataset is not divided in training and testing sets, we decided to train the network using the training set of the DRIVE dataset till convergence, and test the network using test set of the DRIVE data as well as all the images of STARE dataset.
Table~\ref{quant_eye} presents the results in terms of accuracy and area under the curve (AUC). We measure the accuracy as 

The Table~\ref{quant_eye} shows that the network performs well also on the STARE dataset even being trained on DRIVE dataset. For both the datasets, Morph-Net performs comparably, if not better. The ROC curves for both the datasets are given in Figure~\ref{fig:eye_roc}. For qualitative comparison, we have presented samples from DRIVE dataset in Figure~\ref{qualdrive}. Note that extracted blood vessels are little thicker in a few places in the images of DRIVE (\Figref{qualdrive}). 
This occurs due to use of fixed threshold 0.5 in the final step.

We have shown sensitivity (Se), specificity (Sp), positive predictive value (Ppv), negative predictive value (Npv) in Tables~\ref{retinal_table2_drive},~\ref{retinal_table2_stare} for the DRIVE and STARE datasets, where





\begin{table}[h]
    \centering
    \caption{Individual results on the DRIVE dataset.}
    \label{retinal_table2_drive}
    \begin{tabular}{c|c|c|c|c|c}
     \toprule
    Image& Se &Sp &Ppv &Npv &Acc \\
    \toprule
1 & 0.64 &  0.99  & 0.90 & 0.97 & 0.96  \\
2 & 0.61 &  0.99  & 0.91 & 0.97 & 0.96  \\
3 & 0.47 &  1.00  & 0.95 & 0.94 & 0.94  \\
4 & 0.79 &  0.99  & 0.89 & 0.98 & 0.97  \\
5 & 0.63 &  0.99  & 0.92 & 0.96 & 0.96  \\
6 & 0.69 &  0.99  & 0.88 & 0.97 & 0.96  \\
7 & 0.80 &  0.99  & 0.82 & 0.98 & 0.97  \\
8 & 0.87 &  0.98  & 0.82 & 0.99 & 0.97  \\
9 & 0.69 &  0.99  & 0.91 & 0.97 & 0.96  \\
10 & 0.80 &  0.98  & 0.83 & 0.98 & 0.97  \\
11 & 0.73 &  0.99  & 0.87 & 0.97 & 0.97  \\
12 & 0.74 &  0.99  & 0.89 & 0.98 & 0.97  \\
13 & 0.75 &  0.99  & 0.85 & 0.98 & 0.97  \\
14 & 0.77 &  0.99  & 0.82 & 0.98 & 0.97  \\
15 & 0.77 &  0.99  & 0.83 & 0.98 & 0.97  \\
16 & 0.82 &  0.98  & 0.75 & 0.99 & 0.97  \\
17 & 0.75 &  0.99  & 0.88 & 0.98 & 0.97  \\
18 & 0.69 &  0.99  & 0.90 & 0.97 & 0.96  \\
19 & 0.75 &  0.99  & 0.84 & 0.98 & 0.97  \\
20 & 0.61 &  0.99  & 0.88 & 0.96 & 0.96  \\
\midrule
Average & 0.718& 0.989& 0.867 & 0.973 &0.96 \\
    \bottomrule
    \end{tabular}
\end{table}

\begin{table}
    \centering
    \caption{Results on the STARE dataset}
    \label{retinal_table2_stare}
    \begin{tabular}{c|c|c|c|c|c}
     \toprule
    Image& Se &Sp &Ppv &Npv&Acc \\
    \toprule
        1 & 0.25 &  0.99  & 0.72 & 0.96 & 0.95 \\
        2 & 0.69 &  0.97  & 0.74 & 0.96 & 0.94 \\
        3 & 0.23 &  1.00  & 0.87 & 0.93 & 0.93 \\
        4 & 0.40 &  0.99  & 0.91 & 0.93 & 0.93 \\
        5 & 0.15 &  1.00  & 0.94 & 0.95 & 0.95 \\
        6 & 0.57 &  1.00  & 0.97 & 0.94 & 0.94 \\
        7 & 0.54 &  1.00  & 0.98 & 0.93 & 0.93 \\
        8 & 0.10 &  1.00  & 0.94 & 0.91 & 0.91 \\
        9 & 0.69 &  0.99  & 0.94 & 0.95 & 0.95 \\
        10 & 0.12 &  1.00  & 0.93 & 0.94 & 0.94 \\
        11 & 0.30 &  1.00  & 0.97 & 0.92 & 0.92 \\
        12 & 0.40 &  0.99  & 0.82 & 0.97 & 0.96 \\
        13 & 0.46 &  1.00  & 0.97 & 0.92 & 0.92 \\
        14 & 0.35 &  1.00  & 0.98 & 0.92 & 0.92 \\
        15 & 0.62 &  1.00  & 0.95 & 0.95 & 0.95 \\
        16 & 0.36 &  1.00  & 0.97 & 0.92 & 0.93 \\
        17 & 0.42 &  0.99  & 0.93 & 0.90 & 0.91 \\
        18 & 0.54 &  1.00  & 0.98 & 0.93 & 0.94 \\
        19 & 0.55 &  1.00  & 0.94 & 0.97 & 0.97 \\
        20 & 0.47 &  1.00  & 0.96 & 0.91 & 0.92 \\
   \midrule
    Average & 0.41& 0.995& 0.92 & 0.94& 0.93 \\
    \bottomrule
    \end{tabular}
\end{table}

\subsection{Lungs segmentation}

 In this experiment, we have used the publicly available Shenzhen chest X-Ray imageset~\cite{jaeger2014two}. This dataset is collected by Shenzhen No.3 Hospital in Shenzhen, Guangdong Province, China. The dataset contains 326 normal and 336 abnormal X-Ray images showing various manifestations of tuberculosis. Each image is of size approximately . We have randomly selected 90\% of the data for training and remaining 10\% as test set. All the images are resized to  before before into the network. 
This problem requires both global and local information, so that a large blob with intricate detail of boundary region (region of interest) can be extracted. This suggests us to employ a U-net\cite{ronneberger2015u} type of network architecture (Figure~\ref{fig:unet}). 
We have used max-pool to down sample the feature maps and 2D Up Sampling to get back to same size as input. 2D Up Sampling scales up the image by using nearest neighbour. Batch normalization taken to increase the stability of the network.
The network is trained using binary cross entropy loss and Adam optimizer. The results have been generated with a network trained for 400 epochs with a batch size 2. The output of the network is binarized with a threshold of 0.5 to get the final output. For quantitative evaluation, we have reported the DICE coefficient scores obtained on the test set in Table~\ref{Shenzhen_quant}. DICE coefficient computes the overlap between the ground-truth and the predicted segmentation mask. So, higher the value, better is the result.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/2D/lungs/unet.pdf}
    \caption{U-Net like architecture of the 2D Morph-Net utilized for lungs segmentation.}
    \label{fig:unet}
\end{figure}
\begin{table}
    \centering
    \caption{Attained DICE coefficient on the Shenzhen dataset by different networks.}
    \label{Shenzhen_quant}
    \begin{tabular}{m{0.2\linewidth}m{0.15\linewidth}m{0.15\linewidth}m{0.1\linewidth}m{0.15\linewidth}}
    \toprule
    Method & Candemir et al.\cite{candemir2013lung} & ED-CNN\cite{kalinovsky2016lung} & FCN\cite{rashid2018fully}  & 2D Morph-Net\\
    \midrule
    Dice Coefficient &94.1 &97.4&97.7 & 95.9 \\
    \bottomrule
    \end{tabular}
\end{table} 


\begin{figure*}[h]
\centering
\begin{subfigure}[b]{0.220\linewidth}
\includegraphics[width=\linewidth]{./images/2D/lungs/senzen/input/CHNCXR_0629_1.jpg}
\includegraphics[width=\linewidth]{./images/2D/lungs/senzen/output/CHNCXR_0629_1.jpg}
\includegraphics[width=\linewidth]{./images/2D/lungs/senzen/gt/CHNCXR_0629_1_mask.jpg}
\caption{}
\end{subfigure}
\begin{subfigure}[b]{0.2265\linewidth}
\includegraphics[width=\linewidth]{./images/2D/lungs/senzen/input/CHNCXR_0649_1.jpg}
\includegraphics[width=\linewidth]{./images/2D/lungs/senzen/output/CHNCXR_0649_1.jpg}
\includegraphics[width=\linewidth]{./images/2D/lungs/senzen/gt/CHNCXR_0649_1_mask.jpg}
\caption{}
\end{subfigure}
\begin{subfigure}[b]{0.2005\linewidth}
\includegraphics[width=\linewidth]{./images/2D/lungs/senzen/input/CHNCXR_0650_1.jpg}
\includegraphics[width=\linewidth]{./images/2D/lungs/senzen/output/CHNCXR_0650_1.jpg}
\includegraphics[width=\linewidth]{./images/2D/lungs/senzen/gt/CHNCXR_0650_1_mask.jpg}
\caption{}
\end{subfigure}
\begin{subfigure}[b]{0.2029\linewidth}
\includegraphics[width=\linewidth]{./images/2D/lungs/senzen/input/CHNCXR_0627_1.jpg}
\includegraphics[width=\linewidth]{./images/2D/lungs/senzen/output/CHNCXR_0627_1.jpg}
\includegraphics[width=\linewidth]{./images/2D/lungs/senzen/gt/CHNCXR_0627_1_mask.jpg}
\caption{}
\end{subfigure}
\caption{Results on the Shenzhen  dataset.}
\label{fig:failure_cases_shenzen}
\end{figure*}



\subsection{Image dehazing}


\begin{table*}
    \centering 
    \caption{Quantitative evaluation on validation dataset of O-HAZE images in terms of the SSIM and PSNR metrics.} 
    \begin{tabular}{l|cccccc}
    \toprule 
    Method & 36.png & 37.png & 38.png & 39.png & 40.png & Average\\
    \midrule
    CVPR09\cite{he2010single} & 18.18/0.45 & 16.09/0.49 & 14.12/0.08 & 12.88/0.36 & 14.21/0.39 & 15.10/0.35\\
    TIP15\cite{zhu2015fast} & 17.47/0.50 & 16.17/0.45 & 15.14/0.18 & 14.80/0.41 & 16.37/0.57 & 15.99/0.42\\
    TIP16\cite{ren2016single} & 16.59/0.49 & 15.76/0.43 & 13.25/0.19 & 12.79/0.40 & 16.53/0.56 & 14.99/0.41\\
    CVPR16\cite{cai2016dehazenet} & 16.92/0.43 & 14.98/0.48 & 15.54/0.34 & 17.65/0.48 & 17.04/0.54 & 16.43/0.45 \\
    ICCV17\cite{li2017aod} & 17.10/0.45 & 16.47/0.39 & 16.12/0.12 & 15.04/0.34 & 15.95/0.50 & 16.13/0.36\\
    CVPR18\cite{zhang2018densely} & 17.14/0.44 & 15.29/0.42 & 14.66/0.11 & 15.24/0.36 & \textcolor{blue}{17.78}/0.52 & 16.02/0.36\\
    CVPRW18\cite{zhang2018multi} & \textbf{24.67}/\textcolor{blue}{0.73} & \textbf{22.41}/\textcolor{blue}{0.66} & \textbf{23.75}/\textcolor{blue}{0.72} & \textbf{21.91}/\textcolor{blue}{0.63} & \textbf{22.29}/\textbf{0.68} & \textbf{23.00}/\textcolor{blue}{0.68}\\
    Ours &\textcolor{blue}{20.22}/\textbf{0.75} & \textcolor{blue}{21.11}/\textbf{0.75} & \textcolor{blue}{19.45}/\textbf{0.75}&  \textcolor{blue}{19.63}/\textbf{0.76}  & 14.57/\textcolor{blue}{0.64} & \textcolor{blue}{19.00}/\textbf{0.73}\\
    \bottomrule 
\end{tabular}
    \label{quant_haze_ntire}
\end{table*}


\begin{figure*}
    \centering
    \includegraphics[width=0.97\linewidth]{images/2D/dehaze/dehaze_net.pdf}
    \caption{Architecture of the 2D Morph-Net utilized for image dehazing. The network outputs both transmittance and airlight maps which are later utilized to obtain the dehazed image. }
    \label{fig:dehaze_net}
\end{figure*}

The hazy image formation process is modelled as~\cite{kosch}  

where  and  are the haze-free and the observed hazy image, respectively.  is the scene transmittance that controls the haze in a pixel , and  denotes the uniform environmental illumination. We modify the model by introducing space-variant environmental illumination  to accommodate more general situations as 
 
Note that  varies much slowly than . 
Image dehazing methods try to estimate the  having only the  and this usually requires the estimation of  and . This makes dehazing an ill-posed problem. Before presenting our method let us re-write \eqref{haze_eq_gen} as 

Here , called \textit{airlight map}, is expressed as . To dehaze an image, we propose a 2D Morph-Net (\Figref{fig:dehaze_net}) that takes hazy image () as input and produces estimated the airlight map  and the transmittance map  as output. 
Since,  and , we have used sigmoid activation function in the last layer. From  the estimated  and  the haze-free image may be obtained as follows.

To train the network, given the hazy image  and corresponding haze-free ground-truth image , we minimize a loss function inspired from the bi-directional consistency loss \cite{mondal2018image} based on Structural Similarity Index (SSIM) between two images~\cite{wang2004image}. Specifically, the loss function is given by 

 and  are obtained from , ,  and  as

where 
 
and 
 
 and  are the  patches of the  and ,  respectively.  is the total number of patches.  

The network is trained on the training images of the O-HAZE dataset~\cite{ancuti2018haze} until convergence of the training error. In Table~\ref{quant_haze_ntire}, we have reported the PSNR and SSIM of the results obtained on the validation images of the dataset. Some results are shown in \Figref{qual_ntire} for qualitative evaluation and comparison. 


We have trained the Morph-Net for dehazing using the NYU portion of the D-Hazy dataset and tested them with  the Middlebury portion of the D-Hazy dataset. We have reported the results of comparison in Table~\ref{tab:comp:middlebury}. We have also shown on real dehazed images in Figure~\ref{qual_real} which is produced by Morph-Net trained on the NYU portion of the D-Hazy dataset.

\begin{table*}[h]
  \centering
  \caption{Quantitative results obtained on the Middlebury portion of the D-Hazy dataset.}
  \label{tab:comp:middlebury}
  \begin{tabular}{l|ccccc}
    \toprule
     Image & He et al.~\cite{he_single_2011} & Ren et al.~\cite{ren_single_2016} & Berman~\cite{berman_non-local_2016} & AOD-Net~\cite{li_aod-net:_2017} &Morph-Net\\
    & PSNR/SSIM                    & PSNR/SSIM                & PSNR/SSIM                    & PSNR/SSIM            &PSNR/SSIM  \\ \midrule
Adirondack  & 16.02/0.82                   & 14.39/\textbf{0.89}               & \textbf{16.74}/\textcolor{blue}{0.88}             & 14.18/\textbf{0.89}           &\textcolor{blue}{16.61}/0.87       \\
Backpack    & 14.4/0.85                    & \textbf{16.21}/{\textcolor{blue}{0.87}} & 12.24/0.82                   & {  \textcolor{blue}{16.1}}/\textbf{0.91}      &14.81/0.85  \\
Bicycle1    & 12.39/0.81                   & {\textcolor{blue}{20.66}}/\textcolor{blue}{0.93}         & 12.61/0.82              & \textbf{23.21}/\textbf{0.96}      &16.06/{0.89}     \\
Cable       & \textbf{12.95}/\textbf{0.7}  & 7.65/0.64                & { 9.93}/0.63              & 6.95/0.64                           &\textcolor{blue}{11.46}/\textcolor{blue}{0.66} \\
Classroom1  & {\textcolor{blue}{20.17}}/{\textcolor{blue}{0.87}}       & 10.91/0.74               & \textbf{20.95}/\textbf{0.89} & 10.02/0.72                       &12.62/0.77   \\
Couch       & \textbf{18.68}/\textbf{0.81} & 10.13/0.61               & \textcolor{blue}{13.76}/\textcolor{blue}{0.7}           & 10.56/0.63                         &12.13/0.65       \\
Flowers     & \textbf{17.73}/{\textcolor{blue}{0.89}}    & 10.47/0.78               & {\textcolor{blue}{17.45}}/\textbf{0.9}     & 9.25/0.76                        &14.19/0.83  \\
Jadeplant   & \textbf{13.48}/\textbf{0.69} & 7.78/0.6                 & 7.06/\textcolor{blue}{0.65}                    & 7.65/0.59                    &\textcolor{blue}{12.15}/0.63  \\
Mask        & \textbf{15.88}/\textcolor{blue}{0.89}                   & 14.15/0.85               & {14.18}/0.84                   & 14.3/\textbf{0.91}                  &\textcolor{blue}{15.74}/0.86          \\
Motorcycle  & \textcolor{blue}{13.81}/0.79                   & 13.2/\textcolor{blue}{0.81}                & 11.6/0.62                    & 12.25/{\textbf{0.82}}        &\textbf{14.38}/\textcolor{blue}{0.81}      \\
Piano       & \textbf{18.66}/\textbf{0.86}          & 12.4/0.71                & 15.08/0.78                   & 13.89/0.75             &\textcolor{blue}{17.06}/\textcolor{blue}{0.80}       \\
Pipes       & \textbf{15.52}/\textbf{0.79} & 10.9/0.68                & 13.81/0.74                   & 10.34/0.69                      &\textcolor{blue}{14.92}/\textcolor{blue}{0.77}  \\
Playroom    & \textbf{17.7}/\textbf{0.85}           & 13.42/0.77               & \textcolor{blue}{17.64}/{  \textcolor{blue}{0.83}}             & 13.24/0.78                &15.71/0.80     \\
Playtable   & {\textbf{18.58}}/{\textbf{0.9}}        & 15.09/0.86               & \textcolor{blue}{16.63}/\textcolor{blue}{0.88}                   & 14.73/0.86            &15.67/0.86    \\
Recycle     & 12.5/0.82                    & \textbf{18.3}/{\textbf{0.95}}    & 13.43/0.88                   & \textcolor{blue}{16.62}/\textcolor{blue}{0.9}                      &13.25/0.88  \\
Shelves     & 15.47/0.83                   & \textbf{20.43}/\textbf{0.94}        & {16.9}/0.88                   & 16.52 {\textcolor{blue}{0.92}}               &\textcolor{blue}{17.03}/0.89          \\
Shopvac     & \textbf{13.87}/\textbf{0.8}  & 7.62/0.66                & \textcolor{blue}{11.58}/{  \textcolor{blue}{0.78}}             & 6.89/0.64                      &10.47/0.72    \\
Sticks      & 16.96/0.9                    &\textcolor{blue}{20.5}/\textbf{0.96} & {20.41}/0.93           & 19.13/\textbf{0.96}                         &\textbf{22.10}/\textbf{0.96}    \\
Storage     & \textbf{17.38}/\textbf{0.88} & 11.23/0.82               & {  16.36}/\textbf{0.88}    & 10.24/0.79                   &\textcolor{blue}{17.33}/0.87   \\
Sword1      & {\textcolor{blue}{15.06}}/\textcolor{blue}{0.87}            &\textbf{15.48}/\textbf{0.91}& 12.57/0.83                  & 14.29/\textbf{0.91}               &14.74/0.87 \\
Sword2      & \textcolor{blue}{15.66}/\textcolor{blue}{0.89}          & 12.89/0.88               & {  14.89}/0.88             & 12.8/{  \textbf{0.9}}                   &\textbf{16.06}/\textcolor{blue}{0.89}   \\
Umbrella    & 10.4/0.8                    &\textbf{14.92}/{\textcolor{blue}{0.9}}   & 9.63/0.72                    & {  \textcolor{blue}{14.58}}/\textbf{0.91}        &11.61/0.84  \\
Vintage     & 14.63/0.86                  &\textbf{19.27}/\textbf{0.96} & 14.09/0.83                 & \textcolor{blue}{16.82}/{\textcolor{blue}{0.94}}                &15.01/0.88   \\ \midrule
Average                & {\textbf{15.56}}/\textbf{0.83}            & 13.82/0.81                & 14.33/0.81                   & 13.35/\textcolor{blue}{0.82}         &\textcolor{blue}{14.83}/\textcolor{blue}{0.82}           \\ \bottomrule
 \end{tabular}
  \label{quant_haze_middlebury}
\end{table*}

\begin{figure*}
\begin{subfigure}[t]{0.19\linewidth}
\includegraphics[width=\linewidth]{./images/2D/dehaze/hazy/Bicycle1_Hazy.jpg}
\includegraphics[width=\linewidth]{./images/2D/dehaze/hazy/Motorcycle_Hazy.jpg}
\includegraphics[width=\linewidth]{./images/2D/dehaze/hazy/Playroom_Hazy.jpg}
\includegraphics[width=\linewidth]{./images/2D/dehaze/hazy/Piano_Hazy.jpg}
\caption{Input image}
\end{subfigure}
\begin{subfigure}[t]{0.19\linewidth}
\includegraphics[width=\linewidth]{./images/2D/dehaze/T/T_Bicycle1_Hazy.jpg}
\includegraphics[width=\linewidth]{./images/2D/dehaze/T/T_Motorcycle_Hazy.jpg}
\includegraphics[width=\linewidth]{./images/2D/dehaze/T/T_Playroom_Hazy.jpg}
\includegraphics[width=\linewidth]{./images/2D/dehaze/T/T_Piano_Hazy.jpg}
\caption{Transmittance}
\end{subfigure}
\begin{subfigure}[t]{0.19\linewidth}
\includegraphics[width=\linewidth]{./images/2D/dehaze/K/K_Bicycle1_Hazy.jpg}
\includegraphics[width=\linewidth]{./images/2D/dehaze/K/K_Motorcycle_Hazy.jpg}
\includegraphics[width=\linewidth]{./images/2D/dehaze/K/K_Playroom_Hazy.jpg}
\includegraphics[width=\linewidth]{./images/2D/dehaze/K/K_Piano_Hazy.jpg}
\caption{Airlight}
\end{subfigure}
\begin{subfigure}[t]{0.19\linewidth}
\includegraphics[width=\linewidth]{./images/2D/dehaze/output/Bicycle1_Hazy.jpg}
\includegraphics[width=\linewidth]{./images/2D/dehaze/output/Motorcycle_Hazy.jpg}
\includegraphics[width=\linewidth]{./images/2D/dehaze/output/Playroom_Hazy.jpg}
\includegraphics[width=\linewidth]{./images/2D/dehaze/output/Piano_Hazy.jpg}
\caption{Our output}
\end{subfigure}
\begin{subfigure}[t]{0.19\linewidth}
\includegraphics[width=\linewidth]{./images/2D/dehaze/gt/Bicycle1_im0.jpg}
\includegraphics[width=\linewidth]{./images/2D/dehaze/gt/Motorcycle_im0.jpg}
\includegraphics[width=\linewidth]{./images/2D/dehaze/gt/Playroom_im0.jpg}
\includegraphics[width=\linewidth]{./images/2D/dehaze/gt/Piano_im0.jpg}
\caption{Ground truth}
\end{subfigure}
\caption{Results of our 2D Morph-Net on the Middlebury part of the D-Hazy dataset. Transmittance and airlight maps are shown along with the ground truth for comparison. The network is trained with the NYU part of the D-Hazy dataset.}
\label{qual_ntire}
\end{figure*}

\begin{figure*}[h]
\begin{subfigure}[t]{0.24\linewidth}
\includegraphics[width=\linewidth]{./images/2D/dehaze/hazy/in_flags.jpg}
\includegraphics[width=\linewidth]{./images/2D/dehaze/hazy/in_cones.png}
\includegraphics[width=\linewidth]{./images/2D/dehaze/hazy/in_mountain.png}
\includegraphics[width=\linewidth]{./images/2D/dehaze/hazy/in_florence_input.png}
\includegraphics[width=\linewidth]{./images/2D/dehaze/hazy/in_cityscape1.png}
\caption{Input image}
\end{subfigure}
\begin{subfigure}[t]{0.24\linewidth}
\includegraphics[width=\linewidth]{./images/2D/dehaze/T/T_flags.jpg}
\includegraphics[width=\linewidth]{./images/2D/dehaze/T/T_cones.png}
\includegraphics[width=\linewidth]{./images/2D/dehaze/T/T_mountain.png}
\includegraphics[width=\linewidth]{./images/2D/dehaze/T/T_florence_input.png}
\includegraphics[width=\linewidth]{./images/2D/dehaze/T/T_cityscape1.png}
\caption{Transmittance}
\end{subfigure}
\begin{subfigure}[t]{0.24\linewidth}
\includegraphics[width=\linewidth]{./images/2D/dehaze/K/K_flags.jpg}
\includegraphics[width=\linewidth]{./images/2D/dehaze/K/K_cones.png}
\includegraphics[width=\linewidth]{./images/2D/dehaze/K/K_mountain.png}
\includegraphics[width=\linewidth]{./images/2D/dehaze/K/K_florence_input.png}
\includegraphics[width=\linewidth]{./images/2D/dehaze/K/K_cityscape1.png}
\caption{Airlight}
\end{subfigure}
\begin{subfigure}[t]{0.24\linewidth}
\includegraphics[width=\linewidth]{./images/2D/dehaze/output/out_flags.jpg}
\includegraphics[width=\linewidth]{./images/2D/dehaze/output/out_cones.png}
\includegraphics[width=\linewidth]{./images/2D/dehaze/output/out_mountain.png}
\includegraphics[width=\linewidth]{./images/2D/dehaze/output/out_florence_input.png}
\includegraphics[width=\linewidth]{./images/2D/dehaze/output/out_cityscape1.png}
\caption{Our output}
\end{subfigure}
\caption{Performance of Morph-Net on real outdoor images. Transmittance and airlight are also shown.}
\label{qual_real}
\end{figure*}

\end{document}
