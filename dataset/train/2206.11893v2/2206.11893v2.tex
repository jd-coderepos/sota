\documentclass{article} 


\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage{amsmath,amsfonts,amsthm}       \usepackage{subcaption}
\usepackage{bm}
\usepackage{bbm}
\usepackage{multirow}
\usepackage[inline]{enumitem}
\usepackage{diagbox}
\usepackage[capitalise]{cleveref}  \usepackage{comment}
\usepackage{etoolbox}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage[font=small]{caption}
\usepackage{algorithm,algorithmicx,algpseudocode}
\usepackage{subcaption}
\usepackage{sidecap}

\usepackage[frozencache,cachedir=.]{minted} 


\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}



\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{observation}[lemma]{Observation}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{remark}[lemma]{Remark}
\newtheorem{claim}{Claim}

\newcommand{\dt}{\Delta}
\newcommand{\paren}[1]{\left (#1 \right)}
\newcommand{\dd}{\mathop{}\!d}
\DeclareMathOperator{\hippo}{\mathsf{hippo}}
\DeclareMathOperator*{\diag}{diag}
\newcommand\norm[1]{\left\lVert#1\right\rVert} \newcommand\angles[1]{\left\langle#1\right\rangle} \newcommand\abs[1]{\left\lvert#1\right\rvert} 

\newcommand{\discont}{t_0}
\newcommand{\twpa}{a}
\newcommand{\twpb}{b}
\newcommand{\twpc}{c}
\newcommand{\phid}{d}


\newcommand{\para}[1]{\paragraph{#1}}

\usepackage[square,numbers,sort]{natbib}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\topmargin}{-0.5in}
\newlength{\defbaselineskip}
\setlength{\defbaselineskip}{\baselineskip}
\setlength{\marginparwidth}{0.8in}
\setlength{\parskip}{5pt}\setlength{\parindent}{0pt}


\bibliographystyle{plainnat}
\title{On the Parameterization and Initialization of \\ Diagonal State Space Models}
\usepackage{authblk}
\author[]{Albert Gu}
\author[]{Ankit Gupta}
\author[]{Karan Goel}
\author[]{Christopher R{\'e}}
\affil[]{Department of Computer Science, Stanford University}
\affil[]{IBM Research}
\affil[]{{\texttt{\{albertgu,krng\}@stanford.edu}, \texttt{chrismre@cs.stanford.edu}}}
\date{}


\begin{document}


\maketitle


\begin{abstract}
  State space models (SSM) have recently been shown to be very effective as a deep learning layer as a promising alternative to sequence models such as RNNs, CNNs, or Transformers.
  The first version to show this potential was the S4 model, which is particularly effective on tasks involving long-range dependencies by using a prescribed state matrix called the HiPPO matrix.
  While this has an interpretable mathematical mechanism for modeling long dependencies,
  it introduces a custom representation and algorithm that can be difficult to implement.
  On the other hand, a recent variant of S4 called DSS showed that restricting the state matrix to be fully diagonal can still preserve the performance of the original model when using a specific initialization based on approximating S4's matrix.
  This work seeks to systematically understand how to parameterize and initialize such diagonal state space models.
  While it follows from classical results that almost all SSMs have an equivalent diagonal form, we show that the initialization is critical for performance.
  We explain why DSS works mathematically, by showing that the diagonal restriction of S4's matrix surprisingly recovers the same kernel in the limit of infinite state dimension.
  We also systematically describe various design choices in parameterizing and computing diagonal SSMs, and perform a controlled empirical study ablating the effects of these choices.
  Our final model S4D is a simple diagonal version of S4 whose kernel computation requires just 2 lines of code and performs comparably to S4 in almost all settings, with state-of-the-art results for image, audio, and medical time-series domains, and averaging 85\% on the Long Range Arena benchmark.
\end{abstract}
 
\section{Introduction}
\label{sec:intro}

A core class of models in modern deep learning are sequence models, which are parameterized mappings operating on arbitrary sequences of inputs. Recent approaches based on state space models (SSMs) have outperformed traditional deep sequence models such as recurrent neural networks (RNNs), convolutional neural networks (CNNs), and Transformers, in both computational efficiency and modeling ability.
In particular, the S4 model displayed strong results on a range of sequence modeling tasks, especially on long sequences \citep{gu2022efficiently}.
Its ability to model long-range dependencies arises from being defined with a particular state matrix called the ``HiPPO matrix''~\citep{gu2020hippo},
which allows S4 to be viewed as a convolutional model that decomposes an input onto an orthogonal system of smooth basis functions\citep{gu2022hippo}. 

However, beyond its theoretical interpretation,
actually computing S4 as a deep learning model requires a sophisticated algorithm with many linear algebraic techniques that are difficult to understand and implement.
These techniques were necessitated by parameterizing its state matrix as a \textbf{diagonal plus low-rank} (DPLR) matrix, which is necessary to capture HiPPO matrices.
A natural question is whether simplifications of this parameterization and algorithm are possible.
In particular, removing the low-rank term would result in a \textbf{diagonal state space model} (diagonal SSM) that is dramatically simpler to implement and understand. 

Although it is known that almost all SSMs have an equivalent diagonal form---and therefore (complex) diagonal SSMs are fully expressive algebraically---they may not represent all SSMs numerically, and finding a good initialization is critical.
\citet{gu2022efficiently} showed that it is difficult to find a performant diagonal SSM, and that many alternative parameterizations of the state matrix -- including by random diagonal matrices -- are much less effective empirically, which motivated the necessity of the more complicated HiPPO matrix.
However, recently \citet{gupta2022diagonal} made the empirical observation that a variant of S4 using a \emph{particular diagonal matrix} is nearly as effective as the original S4 method.
This matrix is based on the original HiPPO matrix and is defined by simply chopping off the low-rank term in the DPLR representation.

The discovery of performant diagonal state matrices opens up new possibilities for simplifying deep state space models,
and consolidating models such as S4 and DSS to understand and improve them.
First, the strongest version of DSS \emph{computes} the SSM with a complex-valued softmax that complicates the algorithm, and is actually less efficient than S4.
Additionally, DSS and S4 differ in several auxiliary aspects of \emph{parameterizing} SSMs that can conflate performance effects, making it more difficult to isolate the core effects of diagonal versus DPLR state matrices.
Most importantly, DSS relies on \emph{initializing} the state matrix to a particular approximation of S4's HiPPO matrix.
While S4's matrix has a mathematical interpretation for addressing long-range dependencies, the efficacy of the diagonal approximation to it remains theoretically unexplained.

\begin{figure}[!t]
\begin{minipage}{\linewidth}\centering
    \includegraphics[width=\linewidth]{figs/s4d.png}
\end{minipage}
\caption{S4D is a diagonal SSM which inherits the strengths of S4 while being much simpler. (\emph{Left}) The diagonal structure allows it to be viewed as a collection of 1-dimensional SSMs. (\emph{Right}) As a convolutional model, S4D has a simple interpretable convolution kernel which can be implemented in two lines of code. Colors denote independent 1-D SSMs; purple denotes trainable parameters.}
\label{fig:1}
\end{figure}

In this work, we seek to systematically understand how to train diagonal SSMs.
We introduce the \textbf{S4D} method, a diagonal SSM which combines the best of S4's \emph{computation} and \emph{parameterization} and DSS's \emph{initialization}, resulting in a method that is extremely simple, theoretically princpled, and empirically effective.
\begin{itemize}[leftmargin=*,itemsep=0pt]\item First, we describe S4D, a simple method outlined by S4 for computing diagonal instead of DPLR matrices, which is based on \textbf{Vandermonde matrix multiplication} and is even simpler and more efficient than the DSS.
    Outside of the core state matrix, we categorize different representations of the other components of SSMs, introducing flexible design choices that capture both S4 and DSS and allow different SSM parameterizations to be systematically compared (\cref{sec:method}).
  \item We provide a new mathematical analysis of DSS's initialization, showing that the diagonal approximation of the original HiPPO matrix surprisingly produces the same dynamics as S4 when the state size goes to infinity.
    We propose even simpler variants of diagonal SSMs using different initializations of the state matrix (\cref{sec:initialization}).
  \item We perform a controlled study of these various design choices across many domains, tasks, and sequence lengths, and additionally compare diagonal (S4D) versus DPLR (S4) variants.
    Our best S4D methods are competitive with S4 on almost all settings, with near state-of-the-art results on image, audio, and medical time series benchmarks, and achieving \textbf{85\%} on the Long Range Arena benchmark (\cref{sec:experiments}).
\end{itemize}







 

\section{Background}

\paragraph{Continuous State Spaces Models}

S4 investigated state space models \eqref{eq:ssm} that are parameterized maps on signals .
These SSMs are linear time-invariant systems that can be represented either as a linear ODE (equation \eqref{eq:ssm})
or convolution (equation \eqref{eq:ssm-convolution}).



\begin{minipage}{.5\linewidth}

\end{minipage}
\begin{minipage}{.5\linewidth}

\end{minipage}

Here the parameters are the state matrix  and other matrices .
In the case of diagonal SSMs,  is diagonal and we will overload notation so that  denotes the entries of the parameters.



An intuitive way to view the convolution kernel \eqref{eq:ssm-convolution} is to interpret it as a linear combination (controlled by  of \textbf{basis kernels}  (controlled by 

We denote this basis as  if necessary to disambiguate; note that it is a vector of  functions.
In the case of diagonal SSMs, each function  is just .

\paragraph{S4: Structured State Spaces}

\looseness=-1
As a deep learning model, SSMs have many elegant properties with concrete empirical and computational benefits \citep{gu2021lssl}.
For example, the convolutional form \eqref{eq:ssm-convolution} can be converted into a temporal recurrence that is substantially faster for autoregressive applications \citep{goel2022sashimi}.

However, making SSMs effective required overcoming two key challenges: choosing appropriate values for the matrices, and computing the kernel \eqref{eq:ssm-convolution} efficiently.

First, \citet{gu2021lssl} showed that naive instantiations of the SSM do not perform well,
and instead relied on a particular (real-valued) matrix  called the HiPPO-LegS matrix \eqref{eq:hippo-legs}.\footnote{HiPPO also specifies formulas for , but the state matrix  is more important. There are many other HiPPO instantiations besides LegS, but HiPPO-LegS is the main one that S4 uses and the term ``HiPPO matrix'' without the suffix refers to this one.}
These matrices were derived so that the basis kernels  have closed-form formulas , where  are normalized Legendre polynomials.
Consequently, the SSM has a mathematical interpretation of decomposing the input signal  onto a set of infinitely-long basis functions that are orthogonal respect to an exponentially-decaying measure,
giving it long-range modeling abilities~\citep{gu2022hippo}.

\looseness=-1
Second, S4 introduced a particular parameterization that decomposed this  matrix into the sum of a normal and rank-1 matrix \eqref{eq:hippo-legsd}, which can be unitarily conjugated into a (complex) diagonal plus rank-1 matrix.
Leveraging this structured form, they then introduced a sophisticated algorithm for efficiently computing the convolution kernel \eqref{eq:ssm-convolution} for state matrices that are \textbf{diagonal plus low-rank (DPLR)}.

\small
\begin{minipage}{.5\linewidth}
\end{minipage}
\begin{minipage}{.5\linewidth}
\end{minipage}
\normalsize



\paragraph{DSS: Diagonal State Spaces}

S4 was originally motivated by searching for a \emph{diagonal state matrix},
which would be even more structured and result in very simple computation of the SSM.
However, the HiPPO-LegS matrix cannot be stably transformed into diagonal form \citep[Lemma 3.2]{gu2022efficiently},
and they were unable to find any diagonal matrices that performed well, resulting in the DPLR formulation.

\looseness=-1
\citet{gupta2022diagonal} made the surprising empirical observation that simply removing the low-rank portion of the DPLR form of the HiPPO-LegS matrix results in a diagonal matrix that performs comparably to the original S4 method.
More precisely, their initialization is the diagonal matrix , or the diagonalization of  in \eqref{eq:hippo-legsd}.
They termed  the \emph{skew-HiPPO} matrix,
which we will also call the \emph{normal-HiPPO} matrix.
To be more specific and disambiguate these variants, we may also call  the HiPPO-LegS-N or HiPPO-N matrix and  the HiPPO-LegS-D or HiPPO-D matrix.

In addition to this initialization, they proposed a method for computing a diagonal SSM kernel.
Beyond these two core differences, several other aspects of their parameterization differ from S4's.

In \cref{sec:method,sec:initialization}, we systematically study the components of DSS:
we categorize different ways to parameterize and compute the diagonal state space,
and explain the theoretical interpretion of this particular diagonal  matrix.

Because there are several different concrete matrices with different naming conventions, this table summarizes these special matrices and ways to refer to them.
\begin{center}
  \small
  \centering
  \begin{tabular}{@{}lll@{}}
    \toprule
    Matrix & Full Name & Alternate Names \\
    \midrule
     & HiPPO-LegS & HiPPO matrix, LegS matrix \\
     & HiPPO-LegS-N & HiPPO-N, skew-HiPPO, normal-HiPPO \\
     & HiPPO-LegS-D & HiPPO-D, diagonal-HiPPO \\
    \bottomrule
  \end{tabular}
\end{center}


 

\section{Parameterizing Diagonal State Spaces}
\label{sec:method}


We describe various choices for the computation and parameterization of diagonal state spaces.
Our categorization of these choices leads to simple variants of the core method.
Both DSS and our proposed S4D can be described using a combination of these factors (\cref{sec:s4d}).

\subsection{Discretization}
\label{sec:method:disc}

The true continuous-time SSM can be represented as a continuous convolution


In discrete time, we view an input sequence  as uniformly-spaced samples from an underlying function  and must approximate this integral.
Standard methods for doing so that preserve the convolutional structure of the model exist.
The first step is to discretize the parameters.
Two simple choices that have been used in prior work include 

With these methods, the discrete-time SSM output is just


These integration rules have both been used in prior works (e.g. LMU and DSS use ZOH \citep{voelker2019legendre,gupta2022diagonal} while S4 and its predecessors use bilinear \citep{gu2020hippo,gu2021lssl,gu2022efficiently}).

\looseness=-1
In \cref{sec:experiments}, we show that there is little empirical difference between them.
However, we note that there is a curious phenomenon where the bilinear transform actually perfectly smooths out the kernel used in DSS to match the S4 kernel (\cref{sec:initialization} \cref{fig:basis-legsd-1024-bilinear}).
We additionally note that numerical integration is a rich and well-studied topic and more stable methods of approximating the convolutional integral may exist.
For example, it is well-known that simple rules like the Trapezoid rule \citep{ralston2001first} can dramatically reduce numerical integration error when the function has bounded second derivative.


\subsection{Convolution Kernel}

The main computational difficulty of the original S4 model is computing the convolution kernel .
This is extremely slow for general state matrices ,
and S4 introduced a complicated algorithm for DPLR state matrices.
When  is diagonal, the computation is nearly trivial.
By \eqref{eq:krylov},

where  is Hadamard product,  is matrix multiplication, and  is known as a \textbf{Vandermonde matrix}.
Unpacking this a little more, we can write  as the following Vandermonde matrix-vector multiplication.


\paragraph{Time and Space Complexity}

The naive way to compute \eqref{eq:vandermonde} is by materializing the Vandermonde matrix  and performing a matrix multiplication,
which requires  time and space.

However, Vandermonde matrices are well-studied and theoretically the multiplication can be computed in
 operations and  space.
In fact, Vandermonde matrices are closely related to Cauchy matrices, which are the computational core of S4's DPLR algorithm,
and have identical complexity \citep{pan2001structured}.

\begin{proposition}The time and space complexity of computing the kernel of diagonal SSMs is equal to that of computing DPLR SSMs.
\end{proposition}

We note that on modern parallelizable hardware such as GPUs, a simple fast algorithm is to compute \eqref{eq:vandermonde} with naive summation (using  operations),
but without materializing the Vandermonde matrix (using  space).
Just as with S4, this may require implementing a custom kernel in some modern deep learning frameworks such as PyTorch to achieve the space savings.


\subsection{Parameterization}
\label{sec:method:param}

The next question is how to represent the parameters .

\para{Parameterization of .}
\looseness=-1
Note that the kernel  blows up to  as  if  has any eigenvalues with positive real part.
\citet{goel2022sashimi} found that this is a serious constraint that affects the stability of the model, especially when using the SSM as an autoregressive generative model.
They propose to force the real part of  to be negative,
also known as the left-half plane condition in classical controls,
by parameterizing the real part inside an exponential function


We note that instead of , any activation function can be used as long as its range is bounded on one side, such as ReLU, softplus, etc. The original DSS does not constrain the real part of ,
which is sufficient for simple tasks involving fixed-length sequences,
but could become unstable in other settings.


\para{Parameterization of .}
\looseness=-1
Another choice in the parameterization is how to represent  and .
Note that the computation of the final discrete convolution kernel  depends only on the elementwise product  (equation \eqref{eq:vandermonde}).
Therefore DSS chose to parameterize this product directly, which they call , instead of  and  individually.

However, we observe that this is equivalent to keeping independent  and , and simply freezing  while training .
Therefore, just as S4 has separate parameters , , and  and uses a fixed initialization for  and ,
S4D also proposes separate , and  and uses fixed initializations for  (discussed in \cref{sec:initialization}) and  (set to .
Then the difference between S4D and DSS is simply that DSS does not train .
In our ablations, we show that training  gives a minor but consistent improvement in performance.


As described in \citep{gu2022hippo}, S4 initializes  randomly with standard deviation  (in contrast to standard deep learning initializations, which scale with the dimension e.g. , which is variance-preserving for S4's  as a consequence of the HiPPO theory.
Because it turns out that the diagonal approximation to HiPPO has similar theoretical properties, we retain this initialization in the diagonal case.

\para{Conjugate Symmetry.}
Finally, we make note of a minor parameterization detail originally used in S4.
Note that we ultimately care about sequence transformations over \emph{real} numbers.
For example, HiPPO defines real  matrices, and the base definition of S4 is a real SSM that is a map on sequences of real numbers.
In this case, note that the state  at any time is a vector , and similarly  and  would consist of  real parameters.

However, if using complex numbers, this effectively doubles the state dimension and the number of parameters in .
Furthermore, when using a complex SSM, the output of the SSM is not guaranteed to be real even if the input is real, and similarly the convolution kernel \eqref{eq:vandermonde} will in general be complex.

To resolve this discrepency, note that when diagonalizing a real SSM into a complex SSM (see \cref{prop:diagonalize}),
the resulting parameters always occur in \emph{conjugate pairs}.
Therefore we can throw out half of the parameters.

In other words, to parameterize a real SSM of state size , we can instead parameterize a complex SSM of state size , and implicitly add back the conjugate pairs of the parameters. This ensures that the total state size and parameter count is actually the equivalent of  real numbers, and also guarantees that the output of the kernel is real.
The implementation of this is very simple; the sum in \eqref{eq:vandermonde} will implicitly include the conjugate pairs of  and therefore resolve to twice the real part of the original sum.







\subsection{S4D: the Diagonal Version of S4}
\label{sec:s4d}

A key component of our exposition is disentangling the various choices possible in representing and computing state space models.
With this categorization, different choices can be mixed and matched to define variants of the core method.
\cref{tab:parameterization} compares S4, DSS, and S4D, which have a core structure and kernel computation, but have various choices of other aspects of the parameterization.


\begin{table}[!ht]
  \small
  \caption{(\textbf{Parameterization choices for Structured SSMs}.) Aside from the core structure of  and the computation of its convolution kernel, SSMs have several design choices which are consolidated in S4D.}
  \begin{tabular}{@{}lllllll@{}}
    \toprule
    Method       & Structure & Kernel Computation & Discretization & Constraint    & Trainable  & Initialization of  \\
    \midrule
    \textbf{S4}  & DPLR      & Cauchy    & Bilinear       &                      & Yes                    & HiPPO                          \\
    \textbf{DSS} & diagonal  & softmax          & ZOH            & id (none)                      & No                     & HiPPO-D                        \\
    \textbf{S4D} & diagonal  & Vandermonde      & either        &  /  & optional                 & various                        \\
    \bottomrule
  \end{tabular}
  \label{tab:parameterization}
\end{table}


\para{Comparison to S4 and DSS.}


We will define the base version of S4D to match the parameterization of S4 (i.e. bilinear discretization,  parameterized with , trainable , and HiPPO-D initialization), but many other variants are possible.
Note that unlike DSS, the output of S4D would be \emph{exactly the same as masking out the low-rank component of S4's DPLR representation}.
Thus comparing S4D vs. S4 is a comparison of diagonal vs. DPLR representations of  while controlling all other factors.
In our empirical study in \cref{sec:experiments},
we systematically ablate the effects of each of these components.

We elaborate more on the comparisons between S4, DSS, and S4D below.


\para{Kernel computation.}

The original S4 work briefly considered the diagonal case as motivation \citep[Section 3.1]{gu2022efficiently}, and explicitly mentioned the connection to Vandermonde products and the computational complexity of diagonal SSMs.
However, their focus was the more complex DPLR representation because it is difficult to find a performant diagonal state matrix.
Compared to S4, we fleshed out details of the Vandermonde connection and its computational complexity, which matches that of S4.

On the other hand, DSS empirically found an effective diagonal state matrix, but introduced a more complicated method based on a \textbf{complex softmax} for computing it.
Compared to S4D, this softmax essentially \emph{normalizes} by the row-sums of the Vandermonde matrix, so we may sometimes refer to this distinction as ``softmax normalization''.
This makes the kernel more complicated than necessary, and has a few concrete drawbacks.
First, the row-normalization effectively makes the model dependent on a particular sequence length , and special logic is required to handle different sequence lengths.
Second, it does not expose the optimal computational complexity of the method, and the original version of DSS in fact uses  more memory in the kernel construction than S4(D).\footnote{An early version of DSS claimed that it did not require a custom kernel while S4 does, but this is because of its extra memory usage. The PyTorch implementation of S4 has an optional custom CUDA kernel primarily to save this factor of  in space.}



\para{Discretization.}

S4D disentangles the discretization method from the kernel computation (equation \eqref{eq:vandermonde}), so that any discretization can be used,
whereas previous methods required a specific discretization.
For example, DSS requires the zero-order hold (ZOH) discretization because the  term in the ZOH formula lends itself to be computed with a softmax.
On the other hand, when  is not diagonal, ZOH involves a matrix exponential which can be slower to compute, so S4 uses the bilinear discretization which can be computed efficiently for DPLR matrices.

\para{Eigenvalue constraint.}

All methods can enforce any constraint on the eigenvalues of . While DSS found that letting them be unconstrained has slightly better performance, our experiments find that the difference is negligible and we recommend contraining negative real part of  as is standard practice in control systems.
This ensures stability even in unbounded autoregressive settings.

\para{The full model.}

The entire S4D method is very straightforward to implement, requiring just a few lines of code each for the parameterization and initialization, kernel computation, and full forward pass (\cref{listing}).
This minimal model maps an input sequence of length  to an output of the same length; given multiple input channels, independent S4D layers are broadcast over them.
Other details such as the initialization of  and other components of the overall neural network architecture are the same as in S4 and DSS.


\begin{listing}[ht]
\begin{minted}[fontsize=\footnotesize]{python}
def parameters(N, dt_min=1e-3, dt_max=1e-1):
  # Initialization
  log_dt = np.random.rand() * (np.log(dt_max)-np.log(dt_min)) + np.log(dt_min) # Geometrically uniform timescale
  A = -0.5 + 1j * np.pi * np.arange(N//2) # S4D-Lin initialization
  B = np.ones(N//2) + 0j
  C = np.random.randn(N//2) + 1j * np.random.randn(N) # Variance preserving initialization
  return log_dt, np.log(-A.real), A.imag, B, C

def kernel(L, log_dt, log_A_real, A_imag, B, C):
  # Discretization (e.g. bilinear transform)
  dt, A = np.exp(log_dt), -np.exp(log_A_real) + 1j * A_imag
  dA, dB = (1+dt*A/2) / (1-dt*A/2), dt*B / (1-dt*A/2)

  # Computation (Vandermonde matrix multiplication - can be optimized)
  # Return twice the real part - same as adding conjugate pairs
  return 2 * ((B*C) @ (dA[:, None] ** np.arange(L))).real

def forward(u, parameters):
    L = u.shape[-1]
    K = kernel(L, *parameters)
    # Convolve y = u * K using FFT
    K_f, u_f = np.fft.fft(K, n=2*L), np.fft.fft(u, n=2*L)
    return np.fft.ifft(K_f*u_f, n=2*L)[..., :L]

\end{minted}
\caption{Full Numpy example of the parameterization and computation of a 1-dimensional S4D-Lin model}
\label{listing}
\end{listing}

Finally, note that different combinations of parameterization choices can lead to slightly different implementations of the kernel.
\cref{fig:1} illustrates the S4D kernel with ZOH discretization which can be simplified even further to just \emph{2 lines of code}.
 \section{Initialization of Diagonal State Matrices}
\label{sec:initialization}

The critical question remains: which diagonal state matrices  are actually effective?
We comment on the limitations of diagonal SSMs,
and then provide three instantiations of S4D that perform well empirically.

\para{Expressivity and Limitations of Diagonal SSMs.}
We first present a simplified view on the expressivity of diagonal SSMs mentioned by \citep{gupta2022diagonal}.
First, it is well-known that almost all matrices diagonalize over the complex plane.
Therefore it is critical to use complex-valued matrices in order to use diagonal SSMs.
\begin{proposition}\label{prop:diagonalize}
  The set  of diagonalizable matrices is dense in , and has full measure (i.e. its complement has measure .
\end{proposition}
It is also well known that the state space  is exactly equivalent to (i.e. expresses the same map  the state space , known in the SSM literature as a state space transformation.
Therefore \cref{prop:diagonalize} says that \emph{(almost) all SSMs are equivalent to a diagonal SSM}.

However, we emphasize that \cref{prop:diagonalize} is about \emph{expressivity} which does not guarantee strong performance of a trained model after optimization.
For example, \citet{gu2022efficiently} and \citet{gupta2022diagonal} show that parameterizing  as a dense real matrix or diagonal complex matrix, which are both fully expressive classes, performs poorly if randomly initialized.

Second, \cref{prop:diagonalize} does not take into account numerical representations of data,
which was the original reason S4 required a low-rank correction term instead of a pure diagonalization \citep[Lemma 3.2]{gu2022efficiently}.
In \cref{sec:experiments:init}, we also show that two different initializations with the \emph{same spectrum} (i.e., are equivalent to the same diagonal  can have very different performance.

\para{S4D-LegS.}
\looseness=-1
The HiPPO-LegS matrix has DPLR representation ,
and \citet{gupta2022diagonal} showed that simply approximating it with  works quite well~\eqref{eq:hippo-legsd}.
Our first result is providing a clean mathematical interpretation of this method.
\cref{thm:legsd} shows a surprising fact that does not hold in general for DPLR matrices (\cref{sec:theory-details:proofs}), and arises out of the special structure of this particular matrix.
\begin{theorem}\label{thm:legsd}
  Let  and  be the HiPPO-LegS matrices, and  be its basis.
  As the state size , the SSM basis  limits to  (\cref{fig:basis-legsd}).
\end{theorem}
Note that  is then \emph{unitarily} equivalent to , which preserves the stability and timescale~\citep{gu2022hippo} of the system.

We define \textbf{S4D-LegS} to be the S4D method for this choice of diagonal .
\cref{thm:legsd} explains the empirical results in \citep{gupta2022diagonal} whereby this system performed quite close to S4, but was usually slightly worse. This is because DSS is a variant of S4D-LegS, which by \cref{thm:legsd} is a noisy approximation to S4-LegS. \cref{fig:basis-legsd} illustrates this result, and also shows a curious phenomenon involving different discretization rules that is open for future work.

\para{S4D-Inv.}
To further simplify S4D-LegS, we analyze the structure of  in more detail.
The real part is easy to understand, which follows from the analysis in \citep{gu2022efficiently}:
\begin{proposition}
\end{proposition}
Let the imaginary part be sorted, i.e.  is the -th largest (positive) imaginary component.
We empirically deduced the following conjecture for the asymptotics of the imaginary part.
\begin{conjecture}\label{conj:s4d-inv}
  As ,  where  is a constant.
  For a fixed , the other eigenvalues satisfy an inverse scaling in : .
\end{conjecture}
\cref{fig:s4d-scaling} empirically supports this conjecture. Based on \cref{conj:s4d-inv}, we propose the initialization S4D-Inv to use the following inverse-law diagonal matrix which closely approximates S4D-LegS.

\begin{minipage}{.6\linewidth}\noindent
  
\end{minipage}\begin{minipage}{.4\linewidth}
\end{minipage}

\para{S4D-Lin.}
While S4D-Inv can be seen as an approximation to the original S4-LegS, we propose an even simpler scaling law for the imaginary parts that can be seen as an approximation of S4-FouT (\citep{gu2022hippo}),
where the imaginary parts are simply the Fourier series frequencies (i.e. matches the diagonal part of the DPLR form of S4-FouT).
\cref{fig:1} (\emph{Right}) illustrates the S4D-Lin basis , which are simply damped Fourier basis functions.

\para{General Diagonal SSM Basis Functions.}
The empirical study in \cref{sec:experiments} performs many ablations of different diagonal initializations, showing that many natural variants of the proposed methods do not perform as well.
The overall guiding principles for the diagonal state matrix  are twofold, which can be seen from the closed form of the basis functions  (\cref{eq:ssm-basis}).

First, the real part of  controls the decay rate of the function.
 is a good default that bounds the basis functions by the envelope , giving a constant timescale (\cref{fig:1} (\emph{Right})).

Second, the imaginary part of  controls the oscillating frequencies of the basis function.
Critically, these should be spread out, which explains why random initializations of  do not perform well.
S4D-Inv and S4D-Lin use simple asymptotics for these imaginary components that provide interpretable bases.
We believe that alternative initializations that have different mathematical interpretations may exist, which is an interesting question for future work.



\begin{figure}[!t]
  \centering
\begin{subfigure}{.48\linewidth}\centering
  \includegraphics[width=\linewidth]{figs/basis_legs_clean.png}
  \caption{S4-LegS (original S4 kernel)}
  \label{fig:basis-legs}
\end{subfigure}
\begin{subfigure}{.48\linewidth}\centering
  \includegraphics[width=\linewidth]{figs/basis_legsd_256_zoh.png}
  \caption{S4D-LegS ()}
  \label{fig:basis-legsd-256}
\end{subfigure}
\\
\begin{subfigure}{.48\linewidth}\centering
  \includegraphics[width=\linewidth]{figs/basis_legsd_1024_zoh.png}
  \caption{S4D-LegS (, ZOH)}
  \label{fig:basis-legsd-1024-zoh}
\end{subfigure}
\begin{subfigure}{.48\linewidth}\centering
  \includegraphics[width=\linewidth]{figs/basis_legsd_1024_bilinear.png}
  \caption{S4D-LegS (, Bilinear)}
  \label{fig:basis-legsd-1024-bilinear}
\end{subfigure}
\caption{
  \textbf{(Visualization of \cref{thm:legsd}).}
  (a) The particular  matrix chosen in S4 results in smooth basis functions  with a closed form formula in terms of Legendre polynomials. By the HiPPO theory, convolving against these functions has a mathematical interpretation as orthogonalizing against an exponentially-decaying measure.
  (b, c) By special properties of this state matrix, removing the low-rank term of its NPLR representation produces the same basis functions as , explaining the empirical effectiveness of DSS.
  (c) Curiously, the bilinear transform instead of ZOH smooths out the kernel to exactly match S4-LegS as  grows.
}
\label{fig:basis-legsd}
\end{figure}

\begin{SCfigure}[50][t]
  \includegraphics[width=0.40\linewidth]{figs/s4d_scaling.png}
  \caption{
    (\textbf{S4D eigenvalues.}) All S4D methods have eigenvalues . S4D-LegS theoretically approximates dynamics of the original (non-diagonal) S4 (Blue), and has eigenvalues following an inverse law  (Orange). The precise law is important: other scaling laws with the same range, including an inverse law with different constant (Purple) and a quadratic law (Red), perform empirically worse (\cref{sec:experiments:init}). A very different linear law based on Fourier frequencies also performs well (Green).
  }
  \label{fig:s4d-scaling}
  \vspace*{-1em}
\end{SCfigure}


 
\section{Experiments}
\label{sec:experiments}


Our experimental study shows that S4D has strong performance in a wide variety of domains and tasks,
including the well-studied Long Range Arena (LRA) benchmark where the best S4D variant is competitive with S4 on all tasks and significantly outperforms all non-SSM baselines.

\looseness=-1
We begin with controlled ablations of the various representations of diagonal state space models.
\begin{itemize}\item In \cref{sec:experiments:diag}, we compare the different methods of parameterizing and computing a diagonal state space model (\cref{sec:method}).
  \item In \cref{sec:experiments:init}, we compare our proposed initializations of the critical  matrix and perform several ablations showing that simple variants can substantially degrade performance, underscoring the importance of choosing  carefully (\cref{sec:initialization}).
  \item In \cref{sec:experiments:full}, we compare our proposed S4D methods against the original S4 method (and the variants proposed in~\citep{gu2022hippo}).
\end{itemize}


\para{Methodology and Datasets.}
In order to study the effects of different S4 and S4D variants in a controlled setting, we propose the following protocol.
We focus on three datasets covering a varied range of
data modalities (image pixels, biosignal time series, audio waveforms), sequence lengths (1K, 4K, 16K), and tasks (classification and regression with bidirectional and causal models).
\begin{itemize}[leftmargin=*,itemsep=0pt]\item \textbf{Sequential CIFAR (sCIFAR).} CIFAR-10 images are flattened into a sequence of length , and a bidirectional sequence model is used to perform 10-way classification.
  \item \textbf{BIDMC Vital Signs.} EKG and PPG signals of length  are used to predict respiratory rate (RR), heart rate (HR), and blood oxygen saturation (SpO2). We focus on SpO2 in this study.
  \item \textbf{Speech Commands (SC).}\footnote{We note that a line of prior work including S4 \citep{kidger2020neural,romero2021ckconv,gu2022efficiently} all used a smaller 10-class subset of SC, so our results on the full dataset are not directly comparable.}
    A 1-second raw audio waveform comprising  samples is used for 35-way spoken word classification. We use an autoregressive (AR) model to vary the setting; this causal setting more closely imitates autoregressive speech generation, where SSMs have shown recent promise \citep{goel2022sashimi}.
\end{itemize}

We fix a simple architecture and training protocol that works generically.
The architecture has  layers and hidden dimension , resulting in  parameters.
All results are averaged over multiple seeds (full protocol and results including std. reported in \cref{sec:experiment-details}).


\subsection{Parameterization, Computation, Discretization}
\label{sec:experiments:diag}





Given the same diagonal SSM matrices , there are many variants of how to parameterize the matrices and compute the SSM kernel described in \cref{sec:method}.
We ablate the different choices described in \cref{tab:parameterization}.
Results are in \cref{tab:ablations-param}, and show that:
\begin{enumerate}[label=(\roman*),leftmargin=*]\item Computing the model with a softmax instead of Vandermonde product does not make much difference
  \item Training  is consistently slightly better
  \item Different discretizations (\cref{sec:method:disc}) do not make a noticeable difference
  \item Unrestricting the real part of  (\cref{sec:method:param}) may be slightly better
\end{enumerate}


\begin{table}[!t]
  \begin{minipage}{.5\linewidth}\centering
    \includegraphics[width=\linewidth]{tables/diag_compute.pdf}
  \end{minipage}
  \begin{minipage}{.5\linewidth}\centering
    \includegraphics[width=\linewidth]{tables/diag_real.pdf}
  \end{minipage}
  \caption{
    Ablations of different parameterizations of diagonal SSMs using S4D-Inv.
    (\emph{Left}) trainability and computation;
    (\emph{Right}) discretization and parameterization.
  }
  \label{tab:ablations-param}
\end{table}

These ablations show that for a fixed initialization , different aspects of parameterizing SSMs make little difference overall.
This justifies the parameterization and algorithm S4D uses (\cref{sec:s4d}), which preserves the choices of the original S4 model and is simpler than DSS.
For the remaining of the experiments in \cref{sec:experiments:init} and \cref{sec:experiments:full},
we fix the S4D parameterization and algorithm described in \cref{sec:method}.
Note that this computes exactly the same kernel as the original S4 algorithm when the low-rank portion is set to ,
allowing controlled comparisons of the critical state matrix  for the remainder of this section.

\subsection{S4D Initialization Ablations}
\label{sec:experiments:init}

The original S4 model proposed a specific formula for the  matrix,
and the first diagonal version \citep{gupta2022diagonal} used a specific matrix based on it.
Our new proposed variants S4D-Inv and S4D-Lin also define precise formulas for the initialization of the  matrix \eqref{eq:s4d-inv}.
This raises the question of whether the initialization of the  still needs to be so precise, despite the large simplifications from the original version.
We perform several natural ablations on these initializations, showing that even simple variations of the precise formula can degrade performance.

\para{Imaginary part scaling factor.}
The scaling rules for the imaginary parts of S4D-Inv and S4D-Lin are simple polynomial laws, but how is the constant factor chosen and how important is it?
These constants are based on approximations to HiPPO methods (e.g. \cref{conj:s4d-inv}).
Note that the range of imaginary components for S4D-Inv and S4D-Lin are quite different (\cref{fig:s4d-scaling}); the largest imaginary part is  for S4D-Inv and  for S4D-Lin.

We consider scaling all imaginary parts by a constant factor of  or  to investigate whether the constant matters.
Note that this preserves the overall shape of the basis functions (\cref{fig:1}, dashed lines) and simply changes the frequencies,
and it is not obvious that this should degrade performance.
However, both changes substantially reduce the performance of S4D in all settings.


\para{Randomly initialized imaginary part.}
\looseness=-1
Next, we consider choosing the imaginary parts randomly.
For S4D-Inv, we keep the real parts equal to  and set each imaginary component to

Note that when  is equally spaced in  instead of uniformly random, this exactly recovers S4D-Inv \eqref{eq:s4d-inv}, so this is a sensible random approximation to it.

Similarly, we consider a variant of S4D-Lin

that is equal to equation \eqref{eq:s4d-lin} when  is equally spaced instead of random.

\cref{tab:ablations-init} (\emph{Random Imag}) shows that this small change causes minor degradation in performance.
We additionally note that the randomly initialized imaginary ablation can be interpreted as follows.
\cref{fig:s4d-scaling} shows the asymptotics of the imaginary parts of SSM matrices, where the imaginary parts of the eigenvalues correspond to y-values corresponding to uniformly spaced nodes on the x-axis.
This ablation then replaces the uniform spacing on the x-axis with uniformly random x values.

\para{Randomly initialized real part.}

We considering initializing the real part of each eigenvalue as  instead of fixing them to .
\cref{tab:ablations-init}(Left, \emph{Random Real}) shows that this also causes minor but consistent degradation in performance on the ablation datasets.
Finally, we also consider randomizing both real and imaginary parts, which degrades performance even further.


\begin{table}[!t]
\caption{(\textit{Initialization and Trainability ablations})}
\begin{subtable}{.40\linewidth}\centering
  \includegraphics[width=\linewidth]{tables/diag_init.pdf}
  \caption{
    Ablations of the initialization of the diagonal  matrix in S4D.
    Very simple changes that largely preserve the structure of the diagonal eigenvalues all degrade performance.
  }
  \label{tab:ablations-init}
\end{subtable}
\begin{subtable}{.60\linewidth}\centering
  \includegraphics[width=\linewidth]{tables/ablations_trainable.pdf}
  \caption{
    Results for all S4 and S4D methods on the ablation datasets, when the  and  matrices are either frozen (\emph{Top}) or trained (\emph{Bottom}).
    Diagonal state matrices are highly competitive with full DPLR versions, achieving strong results on all datasets.
  }
  \label{tab:ablations-trainable}
\end{subtable}
\vspace*{-1.5em}
\end{table}


\para{Ablation: Other S4D matrices.}

Other simple variants of initializations show that it is not just the range of the eigenvalues but the actual distribution that is important (\cref{fig:s4d-scaling}).
Both S4D-Inv2 and S4D-Quad have real part  and imaginary part satisfying the same maximum value as \cref{conj:s4d-inv}.
The S4D-Inv2 initialization uses the same formula as S4D-Inv, but replaces a  in the denominator with .
The S4D-Quad initialization uses a polynomial law with power  instead of  (S4D-Inv) or  (S4D-Lin).
\begin{minipage}{.5\linewidth}\noindent
  
\end{minipage}\begin{minipage}{.5\linewidth}\vspace*{8pt}
  
\end{minipage}


We include two additional methods here that are not based on the proposed S4D-Inv or S4D-Lin methods.
First, S4D-Rand uses a randomly initialized diagonal , and validates that it performs poorly, in line with earlier findings \citep{gu2022efficiently,gupta2022diagonal}.
Second, S4D-Real uses a particular real initialization with .
This is the exact same spectrum as the original S4(-LegS) method,
which validates that it is not just the diagonalization that matters,
highlighting the limitations of \cref{prop:diagonalize}.




\subsection{Full Comparisons of S4D and S4 Methods}
\label{sec:experiments:full}


\para{Trainable  matrices.}

\cref{tab:ablations-trainable} shows the performance of all S4D and S4 variants \citep{gu2022hippo} on the ablations datasets.
We observe several interesting phenomena:
\begin{enumerate}[label=(\roman*)]
  \item Freezing the matrices performs comparably to training them on sCIFAR and BIDMC, but is substantially worse on SC. We hypothesize that this results from  being poorly initialized for \textsc{SC}, so that at initialization models do not have context over the entire sequence, and training  and  helps adjust for this.
    As further evidence, the \emph{finite window methods} S4-LegT and S4-FouT (defined in \citep{gu2022hippo}) have the most limited context and suffer the most when  is frozen.
  \item The full DPLR versions are often slightly better than the diagonal version throughout the entire training curve. We report the validation accuracy after 1 epoch of training on sCIFAR and SC to illustrate this phenomenon. Note that this is not a consequence of having more parameters (\cref{sec:experiment-details}).
\end{enumerate}

\para{Large models on ablation datasets.}

Finally, we relax the strict requirements on model size and regularization for the ablation datasets,
and show the performance of S4 and S4D variants on the test sets with a larger model (architecture and training details in \cref{sec:experiment-details}) when the model size and regularization is simply increased (\cref{tab:full}).
We note that results for each dataset are better than the original S4 model, which was already state-of-the-art on these datasets~\citep{gu2021lssl,gu2022efficiently}.


\begin{table}[t!]
  \small
  \caption{
    (\textbf{Ablation datasets:} Full results with larger models.) For Speech Commands, we show both an autoregressive model as in the ablations, and an unconstrained bidirectional model.
  }
    \centering
    \begin{tabular}{@{}llllllll@{}}
        \toprule
        \textsc{Model}         & \textsc{sCIFAR}          & \multicolumn{2}{c}{\textsc{SC}} & \multicolumn{3}{c}{\textsc{BIDMC}} \\
        \cmidrule(lr){2-2} \cmidrule(lr){3-4} \cmidrule(lr){5-7}
                               & \textsc{Test}            & \textsc{AR}                     & \textsc{Bi.}                        & \textsc{HR}               & \textsc{RR}               & \textsc{SpO2}          \\
                               \midrule
                       S4-LegS & \textbf{91.80} (0.43)    & \textbf{93.60} (0.13)           & 96.08 (0.15)                        & \textbf{0.332} (0.013)    & \underline{0.247} (0.062) & 0.090 (0.006)          \\
    S4-FouT                    & \underline{91.22} (0.25) & 91.78 (0.10)                    & 95.27 (0.20)                        & \underline{0.339} (0.020) & 0.301 (0.030)             & \textbf{0.068} (0.003) \\
    \midrule
    S4D-LegS                   & 89.92 (1.69)             & \underline{93.57} (0.09)        & 95.83 (0.14)                        & 0.367 (0.001)             & 0.248 (0.036)             & 0.102 (0.001)          \\
    S4D-Inv                    & 90.69 (0.06)             & 93.40 (0.67)                    & \underline{96.18} (0.27)            & 0.373 (0.024)             & 0.254 (0.022)             & 0.110 (0.001)          \\
    S4D-Lin                    & 90.42 (0.03)             & 93.37 (0.05)                    & \textbf{96.25} (0.03)               & 0.379 (0.006)             & \textbf{0.226} (0.008)    & 0.114 (0.003)          \\
        \bottomrule
    \end{tabular}
    \label{tab:full}
\end{table}








\para{Long Range Arena.}

We use the same hyperparameter setting for the state-of-the-art S4 model in~\citep{gu2022hippo} on the Long Range Arena benchmark for testing long dependencies in sequence models.
S4D variants are highly competitive on all datasets except Path-X, and outperform the S4 variants on several of them.
On Path-X using this hyperparameter setting with bidirectional models,
only S4D-Inv, our simpler approximation to the original S4-LegS model, achieves above random chance,
and has an average of 85\% on the full LRA suite, more than 30 points better than the original Transformer \citep{tay2021long}.

\begin{table}[!t]
  \small
  \caption{
    (\textbf{Long Range Arena}) Accuracy on full suite of LRA tasks. Hyperparameters in \cref{sec:experiment-details}.
  }
    \centering
    \begin{tabular}{@{}llllllll@{}}
        \toprule
        \textsc{Model} & \textsc{ListOps}         & \textsc{Text}            & \textsc{Retrieval}       & \textsc{Image}           & \textsc{Pathfinder}      & \textsc{Path-X}   & \textsc{Avg}      \\ \midrule
        S4-LegS        & 59.60 (0.07)             & 86.82 (0.13)             & 90.90 (0.15)             & \underline{88.65} (0.23) & \underline{94.20} (0.25) & \textbf{96.35}    & \textbf{86.09}    \\
        S4-FouT        & 57.88 (1.90)             & 86.34 (0.31)             & 89.66 (0.88)             & \textbf{89.07} (0.19)    & \textbf{94.46} (0.24)    & \xmark            & 77.90             \\
        \midrule
        S4D-LegS       & \underline{60.47} (0.34) & 86.18 (0.43)             & 89.46 (0.14)             & 88.19 (0.26)             & 93.06 (1.24)             & 91.95             & 84.89             \\
        S4D-Inv        & 60.18 (0.35)             & \textbf{87.34} (0.20)    & \textbf{91.09} (0.01)    & 87.83 (0.37)             & 93.78 (0.25)             & \underline{92.80} & \underline{85.50} \\
        S4D-Lin        & \textbf{60.52} (0.51)    & \underline{86.97} (0.23) & \underline{90.96} (0.09) & 87.93 (0.34)             & 93.96 (0.60)             & \xmark            & 78.39             \\
        \midrule
        S4 (original)  & 58.35                    & 76.02                    & 87.09                    & 87.26                    & 86.05                    & 88.10             & 80.48             \\ Transformer    & 36.37                    & 64.27                    & 57.46                    & 42.44                    & 71.40                    & \xmark            & 53.66             \\ \bottomrule
    \end{tabular}
    \label{tab:lra}
\end{table}


\para{Final parameterization ablations on Path-X.}
Finally, we return to the parameterization choices presented in \cref{sec:method} and ablated in \cref{sec:experiments:diag},
and ablate them once more on the difficult Path-X dataset.
We use small models of between 150K and 200K parameters (differing only depending on whether  is trained).
We fix the S4D-LegS initialization (i.e., the diagonal HiPPO initialization \eqref{eq:hippo-legsd}).

We start from the base S4D parameterization based on S4: bilinear discretization,  , trainable , and no softmax (\cref{tab:parameterization}).
We ablate each of these choices one at a time for the discretization, constraint on , trainability of , and normalization.
We also consider the combination that defines DSS: ZOH discretization, identity , frozen , softmax normalization.

\begin{table}[!t]
  \caption{(\textbf{S4D Path-X Ablations}.) Ablating parameterization choices for models with less than 200K parameters.}
    \centering
    \begin{tabular}{@{}llllllll@{}}
        \toprule
        \textbf{S4D}    & Identity  &   & ZOH disc.    & Frozen  & ZOH + softmax & \textbf{DSS} \\
        \midrule
          92.12 (0.34) & 92.32 (0.16)                     & 92.29 (0.20)                              & 92.09 (0.08) & 91.66 (0.62)        & 90.92 (0.34)  & 89.72 (0.33) \\
        \bottomrule
    \end{tabular}
    \label{tab:pathx-ablation}
\end{table}

\cref{tab:pathx-ablation} shows that the default S4 parameterization choices are a strong baseline.
As in \cref{sec:experiments:diag}, we find that most of the other choices do not make much difference:
\begin{enumerate}[label=(\roman*),itemsep=0pt]\item letting  be unconstrained has little benefit, and can theoretically cause instabilities, so we do not recommend it,
  \item the bilinear vs. ZOH discretizations make no difference,
  \item training  helps slightly, for a minor increase in parameter count and no change in speed.
\end{enumerate}
Finally, on this task -- unlike the easier ablation datasets in \cref{sec:experiments:diag} --
the softmax normalization of DSS actually hurts performance, and we do not recommend it in general.



 
\section{Conclusion}

State space models based on S4 are a promising family of models for modeling many types of sequential data, with particular strengths for continuous signals and long-range interactions.
These models are a large departure from conventional sequence models such as RNNs, CNNs, and Transformers, with many new ideas and moving parts.
This work provides a more in-depth exposition for all aspects of working with S4-style models, from their core structures and kernel computation algorithms,
to miscellaneous choices in their parameterizations, to new theory and methods for their initialization.
We systematically analyzed and ablated each of these components, and provide recommendations for building a state space model that is as simple as possible, while as theoretically principled and empirically effective as S4.
We believe that S4D can be a strong generic sequence model for a variety of domains, that opens new directions for state space models theoretically, and is much more practical to understand and implement for practitioners.
 

\subsubsection*{Acknowledgments}
We gratefully acknowledge the support of DARPA under Nos. FA86501827865 (SDH) and FA86501827882 (ASED); NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ONR under No. N000141712266 (Unifying Weak Supervision); the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, American Family Insurance, Google Cloud, Swiss Re, Brown Institute for Media Innovation, Department of Defense (DoD) through the National Defense Science and Engineering Graduate Fellowship (NDSEG) Program,  Fannie and John Hertz Foundation, National Science Foundation Graduate Research Fellowship Program, Texas Instruments, and members of the Stanford DAWN project: Teradata, Facebook, Google, Ant Financial, NEC, VMWare, and Infosys. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of DARPA, NIH, ONR, or the U.S. Government.

\newpage
\bibliography{biblio}

\newpage

\appendix

 \section{Method Details}
\label{sec:theory-details}










\subsection{Proofs}
\label{sec:theory-details:proofs}

We prove \cref{thm:legsd}, and then show why this it is a surprising result that is not true in general to low-rank perturbations of SSMs.

We start with the interpretation of the S4-LegS matrix shown in~\citep{gu2022hippo},
which corresponds to \cref{fig:1} (Left).

\begin{theorem}\label{thm:legs}
  Let  be the matrices defined in equation \eqref{eq:hippo-legs}.
  The SSM kernels  have the closed form formula
  
  where  are the Legendre polynomials shifted and scaled to be orthonormal on the interval .
\end{theorem}



\begin{lemma}\label{lmm:legs-orthonormal}
  The functions  are a complete orthonormal basis with respect to the measure .
\end{lemma}
\begin{proof}The polynomials are defined to be orthonormal on , i.e.
  
  By the change of variables  with ,
  
  which shows the orthonormality.

  Completeness follows from the fact that polynomials are complete.
\end{proof}

\begin{proof}[Proof of \cref{thm:legsd}]We start with the standard interpretation of SSMs as convolutional systems.
  The SSM  is equivalent to the convolution
  
  for the SSM kernels  (equation \eqref{eq:ssm-basis}).

  Defining , we can write this as
  
  where  and
  
  is the inner product in the Hilbert space of L2 functions with respect to measure .

  By \cref{thm:legs}, the  are a complete orthonormal basis in this Hilbert space.
  There  represents a decomposition of the function  with respect to this basis, and can be recovered as a linear combination of these projections
  
  Pointwise over the inner times ,
  
  This implies that
  
  Intuitively, due to the function reconstruction interpretation of HiPPO~\citep{gu2022hippo},
  we can approximate  using knowledge in the current state .
  There in the limit ,
  the original SSM is equivalent to
  
\end{proof}

\paragraph{General low-rank perturbations.}

Finally, we remark that this phenomenon where removing the low-rank correction to a DPLR matrix approximates the original dynamics, is unique to this HiPPO-LegS matrix.
We note that if instead of , a \emph{random} rank-1 correction is added to the HiPPO-LegS matrix in \cref{thm:legsd},
the resulting SSM kernels look completely different and in fact diverge rapidly as the magnitude of  increases (\cref{fig:basis-noise}).

\begin{figure}[!ht]
\begin{subfigure}{.32\linewidth}\centering
  \includegraphics[width=\linewidth]{figs/basis_legs_std_03.png}
  \caption{}
\end{subfigure}
\begin{subfigure}{.32\linewidth}\centering
  \includegraphics[width=\linewidth]{figs/basis_legs_std_04.png}
  \caption{}
\end{subfigure}
\begin{subfigure}{.32\linewidth}\centering
  \includegraphics[width=\linewidth]{figs/basis_legs_std_05.png}
  \caption{}
\end{subfigure}
\caption{
  Basis kernels for  for HiPPO-LegS  and random i.i.d. Gaussian  with varying std ,
  illustrating that the SSM basis is very sensitive to low-rank perturbations.
  Note that the normal-HiPPO matrix  for  with entries of magnitude  which is far larger,
  highlighting how unexpected the theoretical result \cref{thm:legsd} is.
}
\label{fig:basis-noise}
\end{figure}

Similarly, \cref{fig:basis-fout} shows a new S4 variant called S4-FouT that is also DPLR~\citep{gu2022hippo}, but removing the low-rank component dramatically changes the SSM kernels.

\begin{figure}[!ht]
\begin{subfigure}{.5\linewidth}\centering
  \includegraphics[width=\linewidth]{figs/basis_fout_1024.png}
  \caption{S4-FouT}
  \label{fig:basis-fout}
\end{subfigure}
\begin{subfigure}{.5\linewidth}\centering
  \includegraphics[width=\linewidth]{figs/basis_fout_norank.png}
  \caption{Diagonal approximation to S4-FouT}
  \label{fig:basis-fourier-norank}
\end{subfigure}
\caption{
  (a) S4-FouT is a version of S4 that produces \emph{truncated Fourier} basis functions choosing a particular . This captures sliding Fourier transforms as a state space model.
  (b) Removing the low-rank term from the FouT matrix does \emph{not} approximate S4-FouT. This diagonal state matrix has real part  that produces infinite oscillations and does not perform well empirically.
}
\label{fig:basis-fourier}
\end{figure}
  \section{Experiment Details}
\label{sec:experiment-details}

\paragraph{Ablation datasets training protocol.}

The architecture has  layers and hidden dimension , resulting in around 100K trainable parameters.
The  and  parameters were tied across the  SSM copies; therefore the S4 models have only  more parameters than S4D models, arising from the  tensor in the DPLR representation .
This choice was made because it generally does not affect performance much, while reducing parameter count and ensuring that S4 vs. S4D models have very similar numbers of parameters.

All results are averaged over 2 or 3 seeds.

All models use learning rate ,  weight decay, and no other regularization or data augmentation.
For the classification tasks (sCIFAR and SC).
we use a cosine scheduler with 1 epoch warmup and decaying to .
For the regression task (BIDMC), we use a multistep scheduler following \citep{rusch2021unicornn,gu2021lssl}.

Reported results are all best validation accuracy, except for the large models in \cref{tab:full}.

\paragraph{Full results for parameterization ablations.}

\cref{tab:ablations-disc-full} and \cref{tab:ablations-real-full} contain the raw results for \cref{tab:ablations-param} including standard deviations.



\begin{table}
  \caption{Full results for \cref{tab:ablations-param} (Left) including standard deviations.}
    \centering
    \begin{tabular}{@{}lllll@{}}
        \toprule
        Trainable  & Method      & sCIFAR       & SC (AR)      & BIDMC (SpO2)    \\
        \midrule
        No                     & Softmax     & 85.04 (0.22) & 89.80 (0.21) & 0.1299 (0.0048) \\
        No                     & Vandermonde & 84.78 (0.16) & 89.62 (0.03) & 0.1355 (0.0039) \\
        Yes                    & Softmax     & 85.37 (0.43) & 90.06 (0.11) & 0.1170 (0.0039) \\
        Yes                    & Vandermonde & 85.37 (0.43) & 90.34 (0.18) & 0.1274 (0.0020) \\
        \bottomrule
    \end{tabular}
    \label{tab:ablations-disc-full}
\end{table}

\begin{table}
  \caption{Full results for \cref{tab:ablations-param} (Right) including standard deviations.}
    \centering
    \begin{tabular}{@{}lllll@{}}
        \toprule
        Discretization & Real part & sCIFAR       & SC (AR)      & BIDMC (SpO2)    \\
        \midrule
        Bilinear       & Exp       & 85.20 (0.18) & 89.52 (0.01) & 0.1193 (0.0069) \\
        Bilinear       & -         & 85.35 (0.27) & 90.58 (0.37) & 0.1102 (0.0075) \\
        Bilinear       & ReLU      & 85.06 (0.06) & 90.22 (0.25) & 0.1172 (0.0063) \\
        ZOH            & Exp       & 85.02 (0.24) & 89.93 (0.07) & 0.1303 (0.0014) \\
        ZOH            & -         & 85.15 (0.13) & 90.19 (0.58) & 0.1289 (0.0035) \\
        ZOH            & ReLU      & 84.98 (0.72) & 90.03 (0.13) & 0.1232 (0.0065) \\
        \bottomrule
    \end{tabular}
    \label{tab:ablations-real-full}
\end{table}


\paragraph{Full results for large models on ablations datasets.}


\cref{tab:scifar-full,tab:bidmc-full,tab:sc-full} show full results comparing our proposed methods against the best models from the literature; citations indicate numbers from prior work.

\begin{figure}[!t]
  \small
\begin{minipage}[t]{0.35\linewidth}
  \small
  \centering
  \captionsetup{type=table}
  \caption{
    (\textbf{Sequential CIFAR image classification.} Test accuracy (Std. dev.)
  }
  \begin{tabular}{@{}ll@{}}
    \toprule
    Model                                      & \textsc{sCIFAR}          \\
    \midrule
    S4-LegS                                    & \textbf{91.80} (0.43)    \\
    S4-FouT                                    & 91.22 (0.25)             \\
    S4-(LegS/FouT)                             & \underline{91.58} (0.17) \\
    \midrule
    S4D-LegS                                   & 89.92 (1.69)             \\
    S4D-Inv                                    & 90.69 (0.06)             \\
    S4D-Lin                                    & 90.42 (0.03)             \\
    \midrule
    Transformer~\citep{trinh2018learning}      & 62.2                     \\
    FlexConv~\citep{romero2022flexconv}        & 80.82                    \\
    TrellisNet~\citep{trellisnet}              & 73.42                    \\
    LSTM~\citep{lstm,gu2020improving}          & 63.01                    \\
    r-LSTM~\citep{trinh2018learning}          & 72.2                     \\
    UR-GRU~\citep{gu2020improving}             & \underline{74.4}         \\
    HiPPO-RNN~\citep{gu2020hippo}              & 61.1                     \\
    LipschitzRNN~\citep{erichson2021lipschitz} & 64.2                     \\
    \bottomrule
  \end{tabular}
    \label{tab:scifar-full}
\end{minipage}
\begin{minipage}[t]{0.65\linewidth}
  \small
  \centering
  \captionsetup{type=table}
  \caption{
    (\textbf{BIDMC Vital signs prediction.})
    RMSE for predicting
    respiratory rate (RR), heart rate (HR), and blood oxygen (SpO2).
  }
    \begin{tabular}{@{}llll@{}}
      \toprule
      Model                              & HR                        & RR                        & SpO2                      \\
      \midrule
      S4-LegS                            & \textbf{0.332} (0.013)    & 0.247 (0.062)             & 0.090 (0.006)             \\
      S4-FouT                            & \underline{0.339} (0.020) & 0.301 (0.030)             & \textbf{0.068} (0.003)    \\
      S4-(LegS/FouT)                     & 0.344 (0.032)             & \textbf{0.163} (0.008)    & \underline{0.080} (0.007) \\
      \midrule
      S4D-LegS                           & 0.367 (0.001)             & 0.248 (0.036)             & 0.102 (0.001)             \\
      S4D-Inv                            & 0.373 (0.024)             & 0.254 (0.022)             & 0.110 (0.001)             \\
      S4D-Lin                            & 0.379 (0.006)             & \underline{0.226} (0.008) & 0.114 (0.003)             \\
      \midrule
      UnICORNN~\citep{rusch2021unicornn} & 1.39                      & 1.06                      & 0.869                     \\
      coRNN~\citep{rusch2021unicornn}    & 1.81                      & 1.45                      & -                         \\
      CKConv                             & 2.05                      & 1.214                     & 1.051                     \\
      NRDE~\citep{morrill2021neural}     & 2.97                      & 1.49                      & 1.29                      \\
      LSTM                               & 10.7                      & 2.28                      & -                         \\
      Transformer                        & 12.2                      & 2.61                      & 3.02                      \\

      XGBoost~\citep{Tan2020TSER}        & 4.72                      & 1.67                      & 1.52                      \\
      Random Forest~\citep{Tan2020TSER}  & 5.69                      & 1.85                      & 1.74                      \\
      Ridge Regress.~\citep{Tan2020TSER} & 17.3                      & 3.86                      & 4.16                      \\
      \bottomrule
    \end{tabular}
    \label{tab:bidmc-full}
\end{minipage}
\end{figure}

Note that earlier works on the Speech Commands dataset typically use pre-processing such as MFCC features, or a 10-class subset of the full 35-class dataset \citep{kidger2020neural,romero2021ckconv,gu2022efficiently}.
As we are not aware of a collection of strong baselines for raw waveform classification using the full dataset, we trained several baselines from scratch for \cref{tab:sc-full}.
The InceptionNet, ResNet-18, and XResNet-50 models are 1D adaptations from \citet{nonaka2021depth} of popular CNN architectures for vision.
The ConvNet architecture is a generic convolutional neural network that we tuned for strong performance, comprising:
\begin{itemize}\item Four stages, each composed of three identical residual blocks.
  \item The first stage has model dimension (i.e. channels, in CNN nomenclature) . Each stage doubles the dimension of the previous stage (with a position-wise linear layer) and ends in an average pooling layer of width . Thus, the first stage operates on inputs of length , dimension  (the input is zero-padded from 16000 to 16384) and the last on length , dimension .
  \item Each residual block has a (pre-norm) BatchNorm layer followed by a convolution layer and GeLU activation.
  \item Convolution layers have a kernel size of .
\end{itemize}


\begin{table}[!t]
  \small
  \centering
  \caption{
    (\textbf{Speech Commands classification.})
    Test accuracy on 35-way keyword spotting. Training examples are 1-second audio waveforms sampled at 16000Hz, or a 1-D sequence of length 16000.
    Last column indicates 0-shot testing at 8000Hz where examples are constructed by naive decimation.
  }
    \begin{tabular}{@{}llll@{}}
      \toprule
      Model          & Parameters & 16000Hz                  & 8000Hz                   \\
      \midrule
      S4-LegS        & 307K       & 96.08 (0.15)             & 91.32 (0.17)             \\
      S4-FouT        & 307K       & 95.27 (0.20)             & 91.59 (0.23)             \\
      S4-(LegS/FouT) & 307K       & 95.32 (0.10)             & 90.72 (0.68)             \\
      \midrule
      S4D-LegS       & 306K       & 95.83 (0.14)             & 91.08 (0.16)             \\
      S4D-Inv        & 306K       & \underline{96.18} (0.27) & \textbf{91.80} (0.24)    \\
      S4D-Lin        & 306K       & \textbf{96.25} (0.03)    & \underline{91.58} (0.33) \\
      \midrule
      InceptionNet   & 481K       & 61.24 (0.69)             & 05.18 (0.07)             \\
      ResNet-18      & 216K       & 77.86 (0.24)             & 08.74 (0.57)             \\
      XResNet-50     & 904K       & 83.01 (0.48)             & 07.72 (0.39)             \\
      ConvNet        & 26.2M      & 95.51 (0.18)             & 07.26 (0.79)             \\
      \bottomrule
    \end{tabular}
    \label{tab:sc-full}
\end{table}

\paragraph{Long Range Arena.}


Our Long Range Arena experiments follow the same setup as the original S4 paper with some differences in model architecture and hyperparameters.
The main global differences are as follows:
\begin{description}\item [\textbf{Bidirectional}] The original S4 layer is unidirectional or causal, which is an unnecessary constraint for the classification tasks appearing in LRA. \citet{goel2022sashimi} propose a bidirectional version of S4 that simply concatenates two S4 convolution kernels back-to-back. We use this for all tasks.
  \item [\textbf{GLU feedforward}] S4 consists of  independent 1-dimensional SSMs, each of which are processed by an independent S4 SSM mapping . These outputs are then mixed with a position-wise linear layer, i.e.  for a learned matrix .
    Instead of this linear mapping, we use a GLU activation  for  \citep{dauphin2017language}.
    These have been empirically found to improve linear layers of DNNs in general \citep{shazeer2020glu}.
  \item [\textbf{Cosine scheduler}] Instead of the plateau scheduler used in \citep{gu2022efficiently}, we use a cosine annealing learning rate scheduler for all tasks.
  \item [\textbf{Regularization}] Almost all tasks used no dropout and  weight decay.
  \item [\textbf{Architecture}] Almost all tasks used an architecture with  layers,  hidden units, BatchNorm, pre-norm placement of the normalization layer.
\end{description}

Exceptions to the above rules are described below.
Full hyperparameters are in \cref{tab:lra-hyperparameters}.

\paragraph{sCIFAR / LRA Image.}
This dataset is grayscale sequential CIFAR-10, and the settings for this task were taken from S4's hyperparameters on the normal sequential CIFAR-10 task.
In particular, this used LayerNorm~\citep{ba2016layer} instead of BatchNorm~\citep{ioffe2015batch}, a larger number of hidden features , post-norm instead of pre-norm, and minor dropout.
We note that the choice of normalization and increased  do not make a significant difference on final performance, still attaining classification accuracy in the high 80's.
Dropout does seem to make a difference.

\paragraph{BIDMC.}
We used a larger state size of , since we hypothesized that picking up higher frequency features on this dataset would help.
We also used a step scheduler that decayed the LR by  every  epochs, following prior work~\citep{rusch2021unicornn,gu2021lssl}.

\paragraph{ListOps.}
We hypothesized that this task benefits from deeper models, because of the explicit hierarchical nature of the task,
so the architecture used here had 8 layers and  hidden features.
However, results are very close with much smaller models.
We also found that post-norm generalized better than pre-norm, but results are again close (less than  difference).

\paragraph{PathX.}
As described in \citep{gu2022hippo}, the initialization range for PathX is decreased from  to .

\begin{table*}[!t]
  \caption{
    The values of the best hyperparameters found for all datasets; full models on ablation datasets (Top) and LRA (Bottom).
    LR is learning rate and WD is weight decay. BN and LN refer to Batch Normalization and Layer Normalization.
  }
  \centering
  \resizebox{\textwidth}{!}{\begin{tabular}{@{}llllllllllll@{}}
      \toprule
                          & \textbf{Depth} & \textbf{Features } & \textbf{State Size } & \textbf{Norm} & \textbf{Pre-norm} & {\bf Dropout} & {\bf LR} & {\bf Batch Size} & {\bf Epochs} & \textbf{WD} &  \\
      \midrule
      \textbf{sCIFAR}     & 6              & 512                       & 64                          & LN            & False             & 0.1           & 0.01     & 50               & 200          & 0.05        &            \\
      \textbf{SC}         & 6              & 128                       & 64                          & BN            & True              & 0             & 0.01     & 16               & 40           & 0.05        &            \\
      \textbf{BIDMC}      & 6              & 128                       & 256                         & LN            & True              & 0             & 0.01     & 32               & 500          & 0.05        &            \\
      \midrule
      \textbf{ListOps}    & 8              & 128                       & 64                          & BN            & False             & 0             & 0.01     & 50               & 40           & 0.05        &            \\
      \textbf{Text}       & 6              & 256                       & 64                          & BN            & True              & 0             & 0.01     & 16               & 32           & 0.05        &            \\
      \textbf{Retrieval}  & 6              & 256                       & 64                          & BN            & True              & 0             & 0.01     & 64               & 20           & 0.05        &            \\
      \textbf{Image}      & 6              & 512                       & 64                          & LN            & False             & 0.1           & 0.01     & 50               & 200          & 0.05        &            \\
      \textbf{Pathfinder} & 6              & 256                       & 64                          & BN            & True              & 0             & 0.004    & 64               & 200          & 0.03        &            \\
      \textbf{Path-X}     & 6              & 256                       & 64                          & BN            & True              & 0             & 0.0005   & 32               & 50           & 0.05        &          \\
      \bottomrule
    \end{tabular}}
  \label{tab:lra-hyperparameters}
\end{table*}
 

\end{document}
