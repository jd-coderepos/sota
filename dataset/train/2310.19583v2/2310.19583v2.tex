

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{wacv}              

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tabularx,multirow}
\usepackage{times}
\usepackage{epsfig}
\usepackage{nicefrac}
\usepackage[dvipsnames]{xcolor}
\usepackage{soul}
\usepackage[accsupp]{axessibility}  


\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage{etoolbox}


\usepackage[capitalise]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\wacvPaperID{713} \def\confName{WACV}
\def\confYear{2024}

\newcommand{\djc}[1]{{\color{blue}{{\textbf{djc says: #1}}}}}
\newcommand{\vkv}[1]{{\color{red}{{\textbf{vkv says: #1}}}}}

\begin{document}


\title{GC-MVSNet: Multi-View, Multi-Scale, Geometrically-Consistent \\
  Multi-View Stereo}

\author{Vibhas K. Vats, Sripad Joshi, David J. Crandall\\
Indiana University Bloomington\\
{\tt\small \{vkvats, joshisri, djcran\}@iu.edu}
\and
Md. Alimoor Reza\\
Drake University\\
{\tt\small md.reza@drake.edu}
\and 
Soon-heung Jung\\
ETRI\\
{\tt\small zeroone@etri.re.kr}
}

\newcommand\myfigure{\vspace{-25pt}
  \includegraphics[width=1\textwidth,height=12\baselineskip]{figures/idea_digram.pdf}\\\refstepcounter{figure}\normalfont Figure~\thefigure: Our
  multi-view, multi-scale geometric consistency checking
  process. During training, the geometric consistency of the estimated
  depth map is explicitly modeled across multiple source views. This allows the model to more quickly and accurately learn about geometric consistency,
  allowing the trained model to produce better reconstructions during inference.
  \label{fig:gc-module-idea}\\
}

\makeatletter
\let\@oldmaketitle\@maketitle \renewcommand{\@maketitle}{\@oldmaketitle
    \myfigure}
\makeatother


\maketitle






\begin{abstract}
\vspace{-10pt}
Traditional multi-view stereo (MVS) methods rely heavily on
photometric and geometric consistency constraints, but
newer machine learning-based MVS methods check geometric
consistency across multiple source views only as a
post-processing step. In this paper, we present a novel 
approach that explicitly encourages
geometric consistency of reference view depth maps across
multiple source views at different scales during learning (see \cref{fig:gc-module-idea}).
We find that adding this geometric consistency loss
significantly accelerates learning by explicitly
penalizing geometrically inconsistent pixels,  
reducing the training iteration requirements to nearly half
that of other MVS methods.
Our extensive experiments show that our approach 
achieves a new state-of-the-art on the DTU and
BlendedMVS datasets, and competitive results on the Tanks and
Temples benchmark. To the best of our knowledge,
GC-MVSNet is the first attempt to enforce multi-view, multi-scale 
geometric consistency during learning.
\end{abstract}



\vspace{-18pt}
\section{Introduction}\label{sec:intro}

Traditional multi-view stereo (MVS) methods such as Gipuma
\cite{Galliani2015fusibile}, Furu \cite{Furukawa2010AccurateDA},
COLMAP \cite{Johannes2016pixelwise}, and Tola
\cite{tola2011largescale} rely on solving for photometric and
geometric consistency constraints across multiple views.  Recent
machine learning-based MVS methods \cite{ding2022transmvsnet,
  peng2022rethinkingMVS, Luo2019Pmvsnet, yao2019recurrent,
  chen2019pointbased, gu2019casmvsnet, yang2019CVPMVS,
  Cheng2019USCNet, wei2021aa, xu2019multiscale, yao2018mvsnet,
  YU2021AAcostvolume} use deep networks to extract feature maps and
then construct 3D cost volumes to measure similarity between feature
maps \cite{gu2019casmvsnet}.
Each paper in this stream of machine learning-based approaches    has introduced innovations that have significantly improved
the quality of depth estimates and point cloud
reconstructions,
like multi-level feature 
extraction, attention-based feature matching, similarity-based 
cost-volume creation, and improved loss formulations.
These modern methods use plane sweep
volumes to implicitly encode geometric constraints,
and perform multi-view geometric
consistency checks as a postprocess after inference to filter the depth maps.
However,
they do not
explicitly 
model
multi-view geometric constraints during learning. Instead,
learning about multi-view geometric
information thus must happen only implicitly.



In this paper, we show, for the first time, that providing the model with explicit multi-view geometric
cues using geometric consistency checks across multiple source
views during training  (see Fig. \ref{fig:gc-module-idea}) significantly improves
accuracy while significantly lowering the training iteration requirements.
We formulate a multi-stage model called GC-MVSNet which learns
geometric cues at three scales.
At each scale,
we introduce a novel multi-view geometric consistency module that performs geometric
consistency checks of reference view depth estimates across multiple
source views and generates a per-pixel penalty. This
penalty is then combined with per-pixel depth error
(estimated using cross-entropy loss at each stage) to generate the final loss.

This formulation of loss function provides abundant geometric cues to
accelerate learning of the model. Our extensive experiments show that
GC-MVSNet requires nearly
half the training iterations needed by other recent models \cite{yao2018mvsnet,
  gu2019casmvsnet, peng2022rethinkingMVS, wei2021aa,
  ding2022transmvsnet}. Our approach also achieves a new state-of-the-art accuracy 
on DTU \cite{jensen2014dtu} and BlendedMVS \cite{yao2019blended}
datasets, and competitive results on
Tanks and Temples
\cite{Knapitsch2017tnt}. To the best
of our knowledge, GC-MVSNet is the first attempt to leverage
multi-view, multi-scale geometric consistency checks during the
training process. We also perform extensive ablation experiments to
demonstrate the effectiveness of the proposed approach.

In summary, in this paper:
\begin{itemize}
\setlength\itemsep{-0.1em}
    \item[--] We propose a novel multi-view, multi-scale geometric consistency (GC) module during learning that encourages geometric consistency of reference view depth maps across multiple source views.
    \item[--] We show that this technique reduces the training iteration requirements to
      nearly half that of other  models,
        by explicitly providing multi-view geometric cues during learning.
    \item[--] We show that the module is highly general and can be plugged into different MVS pipelines to enhance geometric cues during training.
\end{itemize}

\section{Related Work}\label{sec:related works}

The taxonomy proposed by Furukawa and Ponce
\cite{Furukawa2010AccurateDA} classifies MVS methods into four primary
scene representations: \textit{volumetric fields}
\cite{kutulakos1999spacecarving, seitz1997photorealistic,
  faugeras1998, sinha2007graphcut}, \textit{point clouds}
\cite{lhuillier2005quasidense, chen2019pointbased}, \textit{3D meshes}
\cite{fua1995object}, and \textit{depth maps}
\cite{Campbell2008UsingMH, Galliani2015fusibile,
  Johannes2016pixelwise, xu2019multiscale, gu2019casmvsnet,
  yao2018mvsnet, YU2021AAcostvolume, ding2022transmvsnet,
  peng2022rethinkingMVS, Cheng2019USCNet}. Depth map-based methods can
 further be categorized into either traditional techniques based on
feature detection and solving for geometric constraints
\cite{Campbell2008UsingMH, Galliani2015fusibile,
  Furukawa2010AccurateDA, Johannes2016pixelwise}, or learning-based
methods \cite{xu2019multiscale, gu2019casmvsnet, yao2018mvsnet,
  YU2021AAcostvolume, ding2022transmvsnet, peng2022rethinkingMVS,
  Cheng2019USCNet}. The latter have become very popular in
the last few years.

Among the learning-based techniques, MVSNet \cite{yao2018mvsnet}
formulates a single-stage MVS pipeline by encoding camera parameters
via differential homography to build 3D cost volumes. It requires a huge amount of
memory and computation as it uses 3D U-Nets \cite{olaf2015unet}
to regularize the cost volume. Subsequent work has taken two main
approaches to alleviate this problem: RNNs
\cite{yao2019recurrent, wei2021aa, yan2020dynamicfusion, xu2021nonlocalrecurrent} and 
coarse-to-fine multi-stage methods \cite{xu2019multiscale,
  gu2019casmvsnet, YU2021AAcostvolume, ding2022transmvsnet,
  peng2022rethinkingMVS, Cheng2019USCNet}. 

Among the RNN-based methods, R-MVSNet \cite{yao2019recurrent}
sequentially regularizes the 2D cost maps along the depth direction
via gated recurrent units. AA-RMVSNet \cite{wei2021aa} slices the cost
volume along  depth hypotheses and regularizes the horizontal and
vertical components using CNN and ConvLSTMCells, respectively. Xu
et al. \cite{xu2021nonlocalrecurrent} use RNNs to model global
dependencies with non-local depth interactions. Yan et
al. \cite{yan2020dynamicfusion} couple LSTM and U-Net architectures to
regularize multi-scale information.

Coarse-to-fine multi-stage methods \cite{ding2022transmvsnet,
  peng2022rethinkingMVS, Luo2019Pmvsnet, yao2019recurrent,
  chen2019pointbased, gu2019casmvsnet, yang2019CVPMVS,
  Cheng2019USCNet, wei2021aa} have  significantly improved the
quality of depth estimates and point cloud reconstructions. They  initially predict a low-resolution
(coarse) depth map and then progressively refine it.
For example,
inspired by
other coarse-to-fine methods \cite{tonioni2018realtime,
  wang2018anytimestereo, yin2018hierarchical}, CasMVSNet
\cite{gu2019casmvsnet} presents a multi-stage formulation of
single-stage MVSNet \cite{yao2018mvsnet},
TransMVSNet \cite{ding2022transmvsnet} focuses on feature matching to
improve performance over CasMVSNet, UniMVSNet \cite{peng2022rethinkingMVS} uses 
unified loss formulation to further improve over CasMVSNet. 
CVP-MVSNet \cite{yang2019CVPMVS} builds a cost volume pyramid in a
coarse-to-fine manner.
UCS-Net \cite{gu2019casmvsnet} uses an adaptive thin volume
module that uses a smaller number of hypothesis planes to efficiently
partition the local depth range within learned small
intervals. TransMVSnet \cite{ding2022transmvsnet} uses transformer
based \cite{vaswani2017attention, angelos2020linearattention} feature
matching to promote similarity in the extracted features. UniMVSNet
\cite{peng2022rethinkingMVS} unifies the advantages of regression and
classification methods by designing a unified focal loss in a
multi-stage framework.

While all these methods improve the performance
of the multi-stage MVS pipeline by improving specific portions, none
of them explicitly models multi-view geometric cues during the learning
process. Consequently, during training these models depend  on the limited
geometric cues available from multiple source views and the cost function
formulation.
Xu and Tao \cite{xu2019multiscale} present a multi-scale geometric
consistency-guided MVS method that uses multi-hypothesis joint view
selection to leverage structured region information to sample better
candidate hypotheses. They hypothesize that the upsampled depth maps
of source images can geometrically constrain these estimates, and use reprojection error \cite{Johannes2016pixelwise,
  zhang2008consistentvideo} to indicate this consistency. In this
paper, we use forward-backward reprojection with multiple source
views to check the geometric consistency of depth estimates and to generate
per-pixel penalties for geometrically inconsistent pixels. 


\begin{figure*}[t]
\vspace{-5pt}
\begin{center}
    \includegraphics[width=1\textwidth, height=14\baselineskip]{figures/gcmvsnet.pdf}
    \vspace{-14pt}
    \caption{The GC-MVSNet architecture. The GC module is applied at the end of each stage. It takes the estimated reference view depth,  source view ground truths and their camera parameters to perform a multi-view geometric consistency check. It generates a per-pixel penalty () for reference view, which is element-wise multiplied with per-pixel depth error () to generate stage loss .  is calculated using cross-entropy loss. All stage losses are added to produce the final loss.}
    \label{fig:gc-mvsnet architecture}
    \vspace{-25pt}
\end{center}
\end{figure*}

\section{Methodology}\label{sec:methodology}

Our goal is to take
 views as input, including a
reference image \textbf{} and
its paired -1 source view images \{\textbf{}\},
along with the corresponding camera parameters , 
and then   to estimate the reference view depth
map () as the output.


\subsection{Network Overview}\label{sec:network-overview}

\cref{fig:gc-mvsnet architecture} shows the architecture of our approach, which we call
Geometric Consistency MVSNet (or GC-MVSNet). We use a deformable convolution-based
\cite{Dai2017deformableCNN}  feature pyramid network (FPN) \cite{Lin2016featurepyramid}
architecture (Sec. \ref{sec:other-modification}) to extract features
from input images in a coarse-to-fine manner in three stages.
 At each stage, we build a correlation-based cost volume
of shape  using feature
maps of shape ,
where , , and
 denote the height, width, and number of channels of a given stage, and
 is the number of depth
hypotheses at the corresponding stage.
The cost volume is
regularized with a cost regularization network.
We use a
winner-takes-all strategy to estimate the depth
map  at each stage. At only the coarse stage, we apply feature matching
\cite{ding2022transmvsnet} with linear attention
\cite{angelos2020linearattention, ding2022transmvsnet} to leverage
global context information within and between reference and source
view features. 

We employ the GC module at each stage. The GC module
checks the geometric consistency of each pixel in  across 
source views and generates 
(Sec. \ref{sec:multi-source-GC-module}), a pixel-wise 
factor that is
multiplied with the per-pixel depth error (), calculated using a
cross-entropy function. It penalizes each pixel in  for its
inconsistency across  source views to accelerate geometric cues
learning during training. TransMVSNet \cite{ding2022transmvsnet} trained with cross-entropy loss (TransMVSNet-B) is our baseline; see Table \ref{table:performance-comparison-with-GC-module} for different stages of GC-MVSNet.

\vspace{-3pt}
\subsection{Multi-View Geometric Consistency Module}\label{sec:multi-source-GC-module}

GC-MVSNet estimates reference depth maps at three stages with
different resolutions. At each stage, the GC module takes , 
source view ground truths , and their camera
parameters  as input (see Alg. \ref{alg:gc-algorithm}). The GC module is then initialized
with a \textit{geometric inconsistency mask sum} (or ) of
zero at each stage. This mask sum accumulates the inconsistency of
each pixel across the  source views. For each source view, the GC
module performs \textit{forward-backward reprojection}  of 
to generate the penalty and then adds it to the mask sum.

Forward-backward reprojection (FBR), as shown in Alg. \ref{alg:forward-backward-reprojection}, is a crucial three-step process. First,
we project each pixel  of  to
its  neighboring source view using intrinsic () and extrinsic () camera 
parameters to obtain corresponding pixel , and denote the corresponding depth map as . Second, we 
similarly remap  
to obtain .
Finally, 
we back project  to the reference view using intrinsic and extrinsic camera parameters 
to obtain  (see Alg. \ref{alg:forward-backward-reprojection}).  and  represent
the depth values of pixels  and
 \cite{Hartley2012Multi-view-geometry}. With 
and , we calculate the pixel displacement error (PDE)
and relative depth difference (RDD). PDE is the  norm between
 and  and RDD is the absolute value difference between
 and  relative to  as shown in Alg. \ref{alg:gc-algorithm}.


\begin{algorithm}[t]
\footnotesize
\begin{algorithmic}
    \State \textbf{Inputs:} 
    \State \textbf{Output:} 
    \State \textbf{Require} 
    \State 
    \State 
    \State 
    \For{ in } 
    \State  \Comment{Alg. \ref{alg:forward-backward-reprojection}}
    \State 
    \State 
    \State 
    \State 
    \State 
    \If{}
        \State  \Else
        \State 
    \EndIf
    \State 
    \EndFor \\

\end{algorithmic}
\caption{Geometric Consistency Check Algorithm}
\label{alg:gc-algorithm}
\end{algorithm}




For each stage, we generate two binary masks of inconsistent pixels,  and ,
by applying thresholds
 and , and then take a logical-OR of the two to preduce a single mask
of inconsistent pixels.
These inconsistent pixels are assigned a value  and all
other pixels, including the consistent and the out-of-scope pixels,
are assigned  to form a penalty mask. This penalty mask
is then added to the mask sum (Alg. \ref{alg:gc-algorithm}),
which accumulates the penalty mask for each of the  source views to generate a final
mask sum with values .
Each pixel value indicates the number of inconsistencies of the pixel
across the  source views.

From this mask sum, we then generate the inconsistency penalty 
for each pixel. Our initial approach generated  by dividing the mask sum by 
 to normalize within the .
However, we found that using  itself for element-wise
multiplication reduces the contribution of perfectly consistent (zero
inconsistency) pixels to zero, preventing further improvement of such
pixels.  To avoid this, we add  so that elements of  are in
. A reference view binary mask is applied on initial  to
generate the final , as shown in Fig. \ref{fig:geo-weight-masking}.


\begin{algorithm}[t]
\footnotesize
\begin{algorithmic}
    \State \textbf{Inputs:} 
    \State \textbf{Output:} 
    \State ;  
    \State  \Comment{Project}
    \State  
    \State   \Comment{Remap}
    \State          \Comment{Back project}
    \State 
\end{algorithmic}
\caption{Forward Backward Reprojection (FBR)}
\label{alg:forward-backward-reprojection}
\end{algorithm}

\vspace{-10pt}
\paragraph{Occlusion and its impact.}
Occluded pixels naturally arise in multi-view stereo, since 3D points
are often not visible in all views. These occluded pixels have a
major impact on geometric constraints, since the 
reference view pixels whose corresponding 3D points
are occluded are penalized as inconsistent.
It is thus important to prevent occluded pixels from dominating the
geometric consistency losses.  While occlusion is sometimes modeled
explicitly \cite{kang2001occlusion, nakamura1996stereoocclusion}, we
found that our approach is naturally robust to occlusion because of 
the following three considerations. First, we select the closest  source views as defined in
MVSNet \cite{yao2018mvsnet} to minimize the number of occluded pixels
in different source views. Then, during FBR, we remap  to 
obtain  and back project it as shown in Alg. \ref{alg:forward-backward-reprojection}. Remapping and back
projection largely handles extreme cases of occlusion (see Appendix A
in Supplemental Material). Finally, we apply
reference view binary mask on ,
Fig. \ref{fig:geo-weight-masking}, to restrict penalties only to valid
reference view pixels. The combination of these steps helps us deal
with occluded pixels and loss explosion.







\vspace{-5pt}
\subsection{Cost Function}\label{sec:cost-function}

\begin{figure}[t]
\begin{center}
    \vspace{-7pt}
   \includegraphics[width=0.9\linewidth]{figures/geo_weight_masking.pdf}
    \vspace{-10pt}
    \caption{The final  is the outcome of elementwise multiplication () of initial  and reference view mask. It restricts the penalties within the reference view mask.}
    \label{fig:geo-weight-masking}
\end{center}
\vspace{-25pt}
\end{figure}


Most learning-based MVS methods \cite{gu2019casmvsnet, yang2019CVPMVS,Zhang2020visibility} 
treat depth estimation as a regression problem and use an  loss between prediction and ground truth. Following AA-RMVSNet \cite{wei2021aa} and UniMVSNet \cite{peng2022rethinkingMVS}, we treat depth 
estimation as a classification problem and adopt a cross-entropy loss formulation 
from AA-RMVSNet \cite{wei2021aa} (see \cite{peng2022rethinkingMVS} for 
relative advantages of regression and classification approaches.) The pixelwise 
depth error  is calculated at each stage,

\vspace{-8pt}

\vspace{-5pt}

\noindent where  is the subset of pixels with valid ground
truth,  denotes the estimated probability of the
depth hypothesis  at pixel , and  denotes the depth
value closest to the ground truth among all hypotheses \cite{ding2022transmvsnet}. We further
enhance the one-hot supervision by penalizing each pixel for its
inconsistency across different source views. This is implemented using
element-wise multiplication () between  and  at
each stage. The mean stage loss, , is calculated as,


\vspace{-14pt}

\vspace{-10pt}

\noindent where  is the mean stage loss and  is the total loss. 
 and  are the stage-wise weights.
This formulation of cost function
with pixel-level inconsistency penalty explicitly forces the model to learn to produce multi-view geometrically-consistent depth maps.




\subsection{Other Modifications}\label{sec:other-modification}

Besides the geometric consistency module, we made two other major modifications to the MVS pipeline.
First, while keeping the feature
extraction network architecture as FPN, we
replaced its regular convolutional layers with deformable layers
\cite{Dai2017deformableCNN, zhu2018deformableV2}. Deformable layers
are known to adjust their sampling locations based on model
requirements \cite{Dai2017deformableCNN, zhu2018deformableV2}. This
helps extract better features for accelerated learning.

Second, most MVS methods \cite{gu2019casmvsnet, ding2022transmvsnet,
  peng2022rethinkingMVS, wei2021aa, Zhang2020visibility,
  weilharter2021highresMVS} use batch normalization 
\cite{ioffe2015batchnorm} and batch synchronisation during training.
As observed in \cite{ioffe2015batchnorm}, batch normalization provides more consistent
and stable training with large batch sizes, but it is inconsistent and
has a degrading effect on training with smaller batches. MVS
methods are restricted to very small batch sizes, often , due to
large memory requirements. Thus, we replaced batch normalization with 
group normalization  layers
\cite{he2018groupnorm} of group size  across the network. Group normalization performs normalization across a 
number of channels that is independent of the
number of examples in a batch \cite{he2018groupnorm}. We also implement weight standardization
\cite{qiao2019weightstandardization} for all layers in the
network. With these modifications, we achieve stable and reproducible
training (see Appendix D in Supplemental Material).



\vspace{-5pt}
\section{Experiments}\label{sec:experiments}

We evaluate on three datasets with different
complexities. \textbf{DTU} \cite{jensen2014dtu} is an indoor dataset
that contains  scenes with  or  views under  lighting
conditions and predefined camera trajectories. We follow MVSNet
\cite{yao2018mvsnet} for training, validation, and test
splits. \textbf{BlendedMVS}\cite{yao2019blended} is a
large-scale synthetic dataset with  indoor and outdoor scenes. It
has 106 training scenes and  validation scenes. \textbf{Tanks and
  Temples} \cite{Knapitsch2017tnt} is collected from a more
complicated and realistic scene, and contains  intermediate and 
advanced scenes. DTU and TnT evaluate using point clouds while BLD
evaluates on depth maps.





\begin{table}[t]
  \begin{center}
    {\footnotesize{
\begin{tabular}{clccc}
\toprule
& Method  & Acc  & Comp  & Overall  \\
\cmidrule{2-5}
\parbox[t]{2mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}{Traditional}}} & Furu \cite{Furukawa2010AccurateDA} & 0.613 & 0.941 & 0.777 \\
& Tola \cite{tola2011largescale} & 0.342 & 1.190 & 0.766 \\
& Gipuma \cite{Galliani2015fusibile}  & \textbf{0.283} & 0.873 & 0.578 \\
& COLMAP \cite{Johannes2016pixelwise}  &  0.400 & 0.664 & 0.532 \\ 
\cmidrule{2-5}
\parbox[t]{2mm}{\multirow{16}{*}{\rotatebox[origin=c]{90}{Learning-based}}}& SurfaceNet \cite{Ji2017surfacenet} & 0.450 & 1.040 & 0.745 \\
& MVSNet \cite{yao2018mvsnet}  & 0.396 & 0.527 & 0.462 \\
& P-MVSNet \cite{Luo2019Pmvsnet} &  0.406 & 0.434 & 0.420 \\
& R-MVSNet \cite{yao2019recurrent}  &  0.383 & 0.452 & 0.417 \\
& Point-MVSNet \cite{chen2019pointbased}  & 0.342 & 0.411 & 0.376 \\
& CasMVSNet \cite{gu2019casmvsnet}   & 0.325 & 0.385 & 0.355 \\
& CVP-MVSNet \cite{yang2019CVPMVS}  & \underline{0.296} & 0.406 & 0.351 \\
& UCS-Net \cite{Cheng2019USCNet}  & 0.338 & 0.349 & 0.344 \\
& AA-RMVSNet \cite{wei2021aa}  & 0.376 & 0.339 & 0.357 \\
& UniMVSNet \cite{peng2022rethinkingMVS}  & 0.352 & 0.278 & 0.315 \\
& TransMVSNet \cite{ding2022transmvsnet}  & 0.321 & 0.289 & 0.305\\
& GBi-Net \cite{mi2022gbi} & 0.312 & 0.293 & \underline{0.303} \\
& MVSTER \cite{wang2022mvster} & 0.350 & \underline{0.276} & 0.313 \\
& \textbf{GC-MVSNet} (ours) & 0.330 & \textbf{0.260} & \textbf{0.295}\\
\cmidrule{2-5}
& \textcolor{Gray}{GBi-Net \cite{mi2022gbi}} & \textcolor{Gray}{0.315} & \textcolor{Gray}{0.262} & \textcolor{Gray}{0.289} \\
& \textcolor{Gray}{\textbf{GC-MVSNet} (ours)} & \textcolor{Gray}{0.323} & \textcolor{Gray}{\textbf{0.255}} & \textcolor{Gray}{0.289}\\
\bottomrule
\end{tabular}
}}
\vspace{-6pt}
\caption{Quantitative results on DTU evaluation set at 864  1152 resolution. Accuracy (Acc), completeness (comp) and overall are in .  means that GBiNet is re-tested with the same post-processing threshold to all scans for fair comparison with other methods. Gray font shows the methods that use scan-specific thresholds for evaluation. \textbf{Bold} and \underline{underline} represents first and second place, respectively.}
\label{Table:DTU-model-comparison}
\vspace{-22pt}
\end{center}
\end{table}

\begin{table}[t]
  \begin{center}
    {\footnotesize{
\begin{tabular}{lccc}
    \toprule
    Method  & EPE  &  & \\
    \midrule
    MVSNet \cite{yao2018mvsnet} & 1.49  &  21.98 &  8.32 \\
    CasMVSNet \cite{gu2019casmvsnet} &  1.43 & 19.01  &  9.77 \\
    CVP-MVSNet \cite{yang2019CVPMVS} &  1.90 & 19.73  & 10.24  \\
    Vis-MVSNet \cite{Zhang2020visibility} & 1.47  & 15.14  &  5.13 \\
    EPP-MVSNet \cite{ma2021eppMVS} & 1.17  & 12.66 &  6.20 \\
    TransMVSNet \cite{ding2022transmvsnet} & \underline{0.73} & \underline{8.32} & \underline{3.62} \\
    \midrule
\textbf{GC-MVSNet} (ours) & \textbf{0.48}  & \textbf{0.89}  &  \textbf{0.97} \\
    \bottomrule
\end{tabular}
}}
\vspace{-6pt}
\caption{Quantitative comparison on BlendedMVS evaluation set. We follow evaluation steps described in \cite{darmon2021wildMVS}. \textbf{Bold} and \underline{underline} represents first and second place, respectively.}
\label{table:quantitative-comparison-on-blended}
\vspace{-25pt}
\end{center}
\end{table}

\vspace{-3pt}
\subsection{Implementation Details}\label{sec:implementation-details}

Following the general practice \cite{ding2022transmvsnet}, we first train
and evaluate our model on DTU. Then, we finetune on BlendedMVS to
evaluate on Tanks and Temples. For training on DTU, we set the number
of input images  and image resolution as . The
depth hypotheses are sampled from  to  for
coarse-to-fine regularization with the number of plane sweeping depth
hypotheses for the three stages set to 48, 32, and 8. The corresponding depth
interval ratio (DIR) is set as 2.0, 0.8, and 0.4. The model is trained
with Adam \cite{Kingma2014AdamAM} for 9 epochs with an initial
learning rate () of 0.001, which decays by a factor of 0.5
once after  epoch. For the Geometric Consistency (GC) module, we use =8 and set
the stage-wise thresholds  as 1, 0.5, 0.25 and  as
0.01, 0.005, 0.0025. We use ==1 and  for all experiments. We train our model with a batch size of 3 on 8
NVIDIA RTX A6000 GPUs for about 9 hours.





\begin{figure*}[t]
\begin{center}
    \vspace{-10pt}
    \includegraphics[width=0.95\textwidth,height=16\baselineskip]{figures/piont-cloud-comparison.pdf}
    \vspace{-15pt}
    \caption{Visual comparison of reconstructed point clouds of GC-MVSNet with CasMVSNet \cite{gu2019casmvsnet}, R-MVSNet \cite{yao2019recurrent} and Ground truths. Our method obtains a more complete point cloud. See Appendix H in Supplemental Material for all point clouds.}
    \label{fig:point-cloud-comparison}
    \vspace{-17pt}
\end{center}
\end{figure*}




\begingroup
\setlength{\tabcolsep}{3pt} \begin{table*}[ht]
\footnotesize
  \begin{center}
    \begin{tabular}{lccccccccc|ccccccc}
    \toprule
    & \multicolumn{9}{c}{\textbf{Intermediate set}} & \multicolumn{7}{c}{\textbf{Advanced set}} \\
    \cmidrule{2-17}
    Method & \textbf{Mean } & Fam. & Fra. & Hor. & Lig. & M60 & Pan. & Pla. & Tra. & \textbf{Mean } & Aud. & Bal. & Cour. & Mus. & Pal. & Tem.\\
    \midrule
    COLMAP \cite{Johannes2016pixelwise} & 42.14 & 50.41 & 22.25 & 26.63 & 56.53 & 44.83 & 46.97 & 48.53 & 42.04 & 27.24 & 16.02 & 25.23 & 34.70 & 41.51 & 18.05 & 27.94\\
    P-MVSNet \cite{Luo2019Pmvsnet}& 55.62 & 70.04 & 44.64 & 40.22 & \textbf{65.20} & 55.08 & 55.17 & 60.37 & 54.29  & - & - & - & - & - & - & - \\
    R-MVSNet \cite{yao2019recurrent} & 50.55 & 73.01 & 54.56 & 43.42 & 43.88 & 46.80 & 46.69 & 50.87 & 45.25  & 29.55 & 19.49 & 31.45 & 29.99 & 42.31 & 22.94 & 31.10\\
    Point-MVSNet \cite{chen2019pointbased}& 48.27 & 61.79 & 41.15 & 34.20 & 50.79 & 51.97 & 50.85 & 52.38 & 43.06 & - & - & - & - & - & - & -\\
    CasMVSNet \cite{gu2019casmvsnet} & 56.84 & 76.37 & 58.45 & 46.26 & 55.81 & 56.11 & 54.06 & 58.18 &  49.51 & 31.12 & 19.81 & 38.46 & 29.10 & 43.87 & 27.36 & 28.11\\
    CVP-MVSNet \cite{yang2019CVPMVS}& 54.03 & 76.50 & 47.74 & 36.34 & 55.12 & 57.28 & 54.28 & 57.43 & 47.54  & - & - & - & - & - & - & - \\
    UCS-Net \cite{Cheng2019USCNet}& 54.83 & 76.09 & 53.16 & 43.03 & 54.00 & 55.60 & 51.49 & 57.38 & 47.89  & - & - & - & - & - & - & - \\
    AA-RMVSNet \cite{wei2021aa} & 61.51 & 77.77 & 59.53 & 51.53 & \underline{64.02} & \underline{64.05}& 59.47 & \underline{60.85} & 54.90  & 33.53 & 20.96 & 40.15 & 32.05 & 46.01 & 29.28 & 32.71\\
    UniMVSNet \cite{peng2022rethinkingMVS} & \textbf{64.36} & \textbf{81.20} & \underline{\underline{66.43}} & \underline{\underline{53.11}} & \underline{\underline{63.46}} & \textbf{66.09} & \textbf{64.84} & \textbf{62.23} & \underline{\underline{57.53}}  & \textbf{38.96} & \underline{28.33} & \underline{\underline{44.36}} & \textbf{39.74} & \textbf{52.89} & \underline{\underline{33.80}} & 34.63 \\
    TransMVSNet \cite{ding2022transmvsnet} & \underline{63.52} & \underline{80.92} & 65.83 & \textbf{56.94} & 62.54 & \underline{\underline{63.06}} & \underline{60.00} & 60.20 & \textbf{58.67}  & 37.00 & 24.84 & \underline{44.59} & 34.77 & 46.49 & \underline{34.69} & \underline{\underline{36.62 }}\\
    GBi-Net \cite{mi2022gbi}  & 61.42 & 79.77 & \textbf{67.69} & 51.81 & 61.25 & 60.37 & 55.87 & \underline{\underline{60.67}} & 53.89 & 37.32 & \textbf{29.77} & 42.12 & \underline{\underline{36.30}} & 47.69 & 31.11 & 36.39  \\
    MVSTER \cite{wang2022mvster}  & 60.92 & 80.21 & 63.51 & 52.30 & 61.38 & 61.47 & 58.16 & 58.98 & 51.38 & \underline{\underline{37.53}} & \underline{\underline{26.68}} & 42.14 & 35.65 & \underline{\underline{49.37}} & 32.16 & \textbf{39.19}  \\
    \midrule
    GC-MVSNet(ours) & \underline{\underline{62.74}}  & \underline{\underline{80.87}}  & \underline{67.13}  &  \underline{53.82} & 61.05  & 62.60  & \underline{\underline{59.64}}  &  58.68 &  \underline{58.48} & \underline{38.74} & 25.37  &  \textbf{46.50} & \underline{36.65} & \underline{49.97} & \textbf{35.81} & \underline{38.11}\\ 
    \bottomrule
    \end{tabular}
    \vspace{-5pt}
    \caption{Quantitative results on intermediate and advanced sets of Tanks and Temples \cite{Knapitsch2017tnt}. \textbf{Bold}, \underline{single-underline}, \underline{\underline{double-underline}} represent first, second and third places, respectively.}
    \label{table:quantitative-comparison-tanks-and-temples}
    \vspace{-25pt}
\end{center}
\end{table*}
\endgroup



\subsection{Experimental Performance}\label{sec:experimental-performance}

\vspace{-3pt}
\paragraph{Evaluation on DTU.}
On DTU, we generate depth maps with =5 at an input
resolution of . We slightly adjust the depth interval ratio (DIR) to  to accommodate the resolution change (more on DIR in
Appendix C in Supplemental Material) and use the Fusibile algorithm \cite{Galliani2015fusibile} for
depth fusion.
Table \ref{Table:DTU-model-comparison} shows quantitative
evaluations, where accuracy is the mean
absolute distance in  from the reconstructed point cloud to the
ground truth point cloud, completeness measures the opposite
(see Appendix F in Supplemental Material), and overall is the average of
these metrics, indicating the overall performance of the
models. We find that GC-MVSNet achieves the best overall score as well as the best
completeness score, when compared to nearly two dozen previous and state-of-the-art techniques.
A qualitative evaluation is presented in Fig. \ref{fig:point-cloud-comparison} on a few
sample MVS problems.
We find that our model
generates denser and more complete point clouds.


\vspace{-10pt}
\paragraph{Evaluation on BlendedMVS.}
Unlike DTU and Tanks and Temples, evaluation on Blended MVS is usually measured
as the quality of depth maps, not the quality of point clouds.
We set =5, =8, image
resolution as , and number of depth planes =128, and
finetune for 10 epochs with one-tenth the learning rate we used for DTU ().
We follow \cite{darmon2021wildMVS} for evaluation process.

Table \ref{table:quantitative-comparison-on-blended} presents the results of 
our quantitative evaluation, using three metrics:
Endpoint error (EPE) is the average 
distance between the estimated and the ground truth depth values, and
 and  are the ratio of number of pixels with  error
larger than 1\textit{mm} and 3\textit{mm}, respectively.
The significant improvement in depth map estimates corroborates that providing explicit geometric cues during training
helps the model learn about multi-view geometric consistency while requiring much less training iteration. See Appendix H of the Supplemental Material for point clouds.


\begin{figure*}[t]
\begin{center}
    \vspace{-12pt}
    \includegraphics[width=0.95\textwidth,height=13\baselineskip]{figures/tnt_precision_error_plot.pdf}
    \vspace{-13pt}
    \caption{Precision and recall comparison with other methods \cite{gu2019casmvsnet, yao2019recurrent, ma2021epp} for Train on Tanks and Temples benchmark.  is the scene-relevant distance threshold. Darker regions indicate larger error encountered with regard to . GC-MVSNet shows visual improvements with brighter regions for precision as well as recall metric.}
    \label{fig:tnt-point-cloud-main-paper}
    \vspace{-25pt}
\end{center}
\end{figure*}


\vspace{-12.5pt}
\paragraph{Evaluation on Tanks and Temples.}
We also test the
performance of our model on an outdoor dataset with the Tanks and Temples benchmark. To
adapt to this change, we first finetune our model on BlendedMVS and then
evaluate on the intermediate and advanced test sets of Tanks and Temples. We use an image
resolution of , =7, =10, one-tenth the learning rate of DTU (),
and =192 for finetuning. We finetune the model for 12 epochs.
The camera parameters and neighboring view
selection are used as in R-MVSNet \cite{yao2019recurrent} 
and follow
evaluation steps described in CDS-MVSNet
\cite{giang2022curvatureguided}.

Table
\ref{table:quantitative-comparison-tanks-and-temples} presents our
quantitative comparison of different methods. GC-MVSNet achieves the 
third highest spot on the intermediate set and the second highest spot on advanced set
evaluation. Fig. \ref{fig:tnt-point-cloud-main-paper} shows point clouds visualizing precision
and recall comparisons with other MVS methods. See Appendix H in Supplemental Material for point clouds.


\subsection{Ablation Study}\label{sec:ablation-study}

Having demonstrated the efficacy of our proposed approach relative to
the state of the art, we now conduct ablation studies 
to evaluate the importance of the various components of our model.

\vspace{-10pt}
\paragraph{Range of .}
 is generated using the mask sum ( in Alg. \ref{alg:gc-algorithm}). 
It is the sum of penalties accumulated across the  source views during multi-view geometric consistency check. At this stage, its elements take a discrete value between  and . Using mask sum as it is leads to very high penalty
per-pixel and consequently, very high loss value. Such a high loss value 
destabilizes the learning process. We control the magnitude of penalty by controlling the range of per-pixel penalty. 

We explore two different ranges to control the magnitude of ,
 and . To generate , we divide the mask
sum by  and then add . To
generate , we divide the mask by 
and then add . Table
\ref{table:range-of-per-pixel-penalty} shows the impact of these two
ranges of penalty for =8. Since  produces best results,
we use it for all other experiments.


\vspace{-14pt}
\paragraph{Hyperparameters of GC module.}
The GC module has two types of hyperparameters, global and local. In this section,
we investigate the effect of these hyperparameters on our results.

The
global hyperparameter  is the number of source views across which
the geometric consistency is checked, and is the same for all three
stages (coarse, intermediate, and refinement stages).  For training on
DTU, we vary the value of  while keeping =5, i.e. while the MVS
method uses only  source views to estimate , the GC module
checks the geometrical consistency of  across  source
views. It is important to note that the first -1 out of  source
views are exactly the same used by GC-MVSNet to estimate . We
always keep .

Table \ref{table:src views gc check ablation} presents a quantitative
comparison for different values of  and the amount of training
iterations required for optimal performance of the model. At =-1= 4,
i.e. checking geometric consistency across the same number of source
views as used by GC-MVSNet to estimate , the model performance
significantly improves with sharp decrease in training iteration requirements, 
as compared to our baseline TransMVSNet-B (Table
\ref{table:performance-comparison-with-GC-module}). As we increase the
value of  from  to , the training iteration required by our model
further decreases. We find that at , which is twice the number of
source views used by GC-MVSNet, it achieves its best performance.



The two local hyperparameters,  and , are the
stage-wise thresholds applied to generate  and
 in Alg. \ref{alg:gc-algorithm}. These values are set to smaller values in the later (finer) stages,
providing a stricter
penalty to geometrically inconsistent pixels at finer
resolutions. Table \ref{table:gc-module-hyperparameter} shows the
overall performance of GC-MVSNet with a range of different 
and  thresholds. GC-MVSNet performance remains fairly consistent and it achieves its best performance
with = 1, 0.5, 0.25 and = 0.01, 0.005, 0.0025. We
use these threshold values for all datasets throughout the paper.










\begin{table}[t]
  \begin{center}
    {\footnotesize{
\begin{tabular}{cccc}
\toprule
 Range  & Acc & Comp & Overall \\
\midrule
 & 0.331 & 0.270 & 0.3005 \\ 
 & \textbf{0.330} & \textbf{0.260} & \textbf{0.295} \\
\bottomrule
\end{tabular}}}
\vspace{-6pt}
\caption{Impact of range of  during training on DTU with =8, =5. Numbers are generated on DTU evaluation set.}
\label{table:range-of-per-pixel-penalty}
\vspace{-10pt}
\end{center}


\begin{center}
    {\footnotesize{
\begin{tabular}{ccccc}
\toprule
M  & Acc & Comp & Overall & Opt. Epoch\\
\midrule
4 & 0.343 & 0.264  &  0.3035  & 12\\
5 & 0.342 & 0.271  & 0.3065   &  13\\
6 & \textbf{0.326} & 0.271  &  0.298  & 9 \\
7 & 0.332 &  0.270 &  0.301  & 10 \\
\textbf{8} & 0.330 &  \textbf{0.260} &  \textbf{0.295}  & 9 \\
9 & 0.328 &  0.280 & 0.304  & 9 \\
10 & 0.329 & 0.268  &  0.298 & 10 \\
\bottomrule
\end{tabular}}}
\vspace{-6pt}
\caption{Quantitative results on DTU evaluation set \cite{jensen2014dtu}. M is the number of source views used by the GC module for checking geometric consistency of reference view depth map. Training iteration requirement of the model decreases as M increases.}
\label{table:src views gc check ablation}
\end{center}
\vspace{-30pt}
\end{table}


\begin{table}[t]
  \begin{center}
    {\footnotesize{
\begin{tabular}{ccccccc}
    \toprule
    \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multirow{2}{*}{Overall} \\
    \cmidrule{1-6}
    C & I & R & C & I & R &  \\
    \midrule
    0.04 & 0.03 & 0.02 & 4 & 3 & 2 & 0.302\\
    0.03 & 0.0225 & 0.015 & 3 & 2.25 & 1.5 & 0.302 \\
    0.02 & 0.015 & 0.01 & 2 & 1.5 & 1.0 & 0.298 \\
    0.01 & 0.008 & 0.006 & 1 & 0.8 & 0.6 & 0.303 \\
    \textbf{0.01} & \textbf{0.005} & \textbf{0.0025} & \textbf{1} & \textbf{0.5} & \textbf{0.25} & \textbf{0.295} \\
0.008 & 0.003 & 0.002 & 0.8 & 0.3 & 0.2 & 0.303 \\
    0.005 & 0.002 & 0.001 & 0.5 & 0.2 & 0.1 & 0.3015 \\
    \bottomrule
\end{tabular}
}}
\vspace{-6pt}
\caption{Overall score on the evaluation set of DTU \cite{jensen2014dtu} for different values of  and .  is fixed at . C, I, and R means Coarse, Intermediate and Refine stages.}
\label{table:gc-module-hyperparameter}
\vspace{-8pt}
\end{center}


\vspace{-18pt}
  \begin{center}
    {\footnotesize{
\begin{tabular}{lllcccc}
\toprule
  & Methods & Loss & Other & GC & Overall & Epoch\\
\cmidrule{2-7}
\parbox[t]{1mm}{\multirow{6}{*}{\rotatebox[origin=c]{90}{GC as a plug-in}}} &\multirow{3}{*}{CasMVSNet} &  &  &  & 0.355 & 16\\
 & &  &  &  & 0.357 & 16\\
 & &  &  &  & \textbf{0.335} & \textbf{11} \\
\cmidrule{2-7}
& \multirow{3}{*}{TransMVSNet} & FL &  &  & 0.305 & 16 \\
& & FL &  &  & 0.322 & 16 \\
& & FL &  &  & \textbf{0.303} & \textbf{8} \\
\midrule
\parbox[t]{2mm}{\multirow{3}{*}{\rotatebox[origin=c]{90}{Stages}}}& TransMVSNet-B & CE &  &  & 0.332 & 16 \\
& TransMVSNet-B & CE &  &  & 0.328 & 16 \\
& TransMVSNet-B & CE &  &  & 0.298 &  \textbf{8} \\
& GC-MVSNet & CE &  &  & \textbf{0.295} & 9 \\
\bottomrule
\end{tabular}
}}
\vspace{-6pt}
\caption{Performance comparison of different MVS methods with different modifications on DTU \cite{jensen2014dtu}. , FL, CE and Others indicate  loss, Focal loss \cite{Lin2017FocalLoss}, Cross-entropy loss \cite{wei2021aa} and other modifications from Sec. \ref{sec:other-modification}, respectively.}
\label{table:performance-comparison-with-GC-module}
\vspace{-30pt}
\end{center}
\end{table}



\vspace{-10pt}
\paragraph{GC module as a plug-in.}
Our Geometric Consistency module is generic can be integrated into
many different MVS pipelines.  To demonstrate this, we tested it with
two very different MVS pipeline, CasMVSNet \cite{gu2019casmvsnet} and
TransMVSNet \cite{ding2022transmvsnet}.  CasMVSNet treats depth
estimation as a regression problem, while TransMVSNet treats it as a
classification problem and uses winner-take-all to estimate the final
depth map. We purposefully choose different methods to show that the
GC module can perform well for both types of formulation. We 
compare the architectures of GC-MVSNet with TransMVSNet and CasMVSNet
in Sec. \ref{sec:model_comparison}.

Table \ref{table:performance-comparison-with-GC-module} presents the results,
showing the impact
of adding the GC module as well as the \textit{other} modifications (deformable convolution-based FPN with group-norm and weight-standardization)
in the
original pipeline. To observe the absolute impact of adding these
modifications, we do not change anything else in the original
pipelines. We observe in the table that applying only the \textit{other}
modification leads to degradation in  performance. It
indicates that the \textit{other} modification helps in
stabilizing the training process and promoting reproducibility, but
has no significant impact on the performance of the model on its
own. We also observe a sharp increase in model
performance and decrease in training iteration requirements after integrating
our GC module into the original pipeline. With GC, training the
CasMVSNet pipeline requires only  epochs 
instead of  epochs, while TransMVSNet (with GC module) requires only  epochs
instead of  epochs. This corroborates our
hypothesis that multi-view geometric consistency significantly reduces training computation
because it accelerates learning of geometric cues.

Table \ref{table:performance-comparison-with-GC-module} also shows
different stages of development of GC-MVSNet.  TransMVSNet-B uses TransMVSNet pipeline with cross-entropy loss, performs much worse than original TransMVSNet
\cite{ding2022transmvsnet} which uses focal loss. With only \textit{other} 
modifications, it slightly improves the overall performance of the model but does not
  impact the training iteration requirements. Only after applying the GC module,
  independently and with \textit{other} modifications, we see significant reduction
  in training iteration requirements as well as a significant improvement in the overall
  accuracy metric. This clearly shows the significance of multi-view multi-scale geometric consistency check in the GC-MVSNet pipeline.

\vspace{-8pt}
\section{Discussion}
\subsection{Comparison to Related Work}\label{sec:model_comparison}

\paragraph{GC-MVSNet vs.~TransMVSNet.}
TransMVSNet \cite{ding2022transmvsnet} uses regular 2D
convolution-based FPN (with batch-norm) for feature extraction and
employs adaptive receptive field (ARF) modules with deformable layers
after feature extraction. It trains using focal loss
\cite{Lin2017FocalLoss}. GC-MVSNet replaces the combination of regular
FPN and ARF modules with deformable FPN (with group-norm and
weight-standardization) for feature extraction. It trains with cross-entropy  
loss
and GC module for accelerated learning.

\vspace{-10pt}
\paragraph{GC-MVSNet vs.~CasMVSNet:}
CasMVSNet \cite{gu2019casmvsnet} proposes a coarse-to-fine regularization
technique. It uses regular 2D convolutions-based FPN for feature
extraction, generates variance-based cost volume and employ depth
regression to estimate . The only similarity with our model is
that we also use coarse-to-fine regularization.

\vspace{-5pt}
\subsection{Limitations}
Like any other MVS method, GC-MVSNet require hyper-parameter tuning during learning. Hyperparameters like, depth interval ratio, number of stage-wise depth hypothesis, number of initial depth hypothesis, depth interval decay factor, etc. impacts model performance. The GC module hyperparameters,  and , also require tuning to achieve its best performance. Along with the GC module hyperparameters, the quality of ground truth has also a direct impact on its performance as it uses source view ground truth depth maps for multi-view geometric consistency check.




\vspace{-8pt}
\section{Conclusion}

In this paper, we present a novel learning-based MVS pipeline,
GC-MVSNet, which explicitly models geometric consistency of reference
depth maps across multiple source views during training. To the best
of our knowledge, this is the first attempt to leverage multi-view
multi-scale geometric consistency check during the training
process. We show that the GC module is generic and can be plugged into
other MVS methods to accelerate their learning as well.  We perform
extensive experiments and ablation study to show the advantages of
GC-MVSNet.  
We hope that our work will bring some insights about including explicit geometric reasoning during learning.

\textbf{Acknowledgement:}
This work was supported by Electronics and Telecommunications Research Institute (ETRI) 
grant funded by the Korean government. 
[23ZH1200, The research of the fundamental media·contents technologies for hyper-realistic media space].




















\newpage

{\small
\bibliographystyle{ieee_fullname}
\bibliography{paper}
}

\end{document}
