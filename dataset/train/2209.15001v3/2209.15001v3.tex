\appendix
\renewcommand{\thetable}{\Roman{table}}
\renewcommand{\thefigure}{\Roman{figure}}
\setcounter{table}{0}
\setcounter{figure}{0}

\begin{figure*}[t]
    \centering
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[trim={6mm, 6mm, 5mm, 6mm},clip,width=\textwidth]{figures/cudatime.pdf}
        \caption{Speed.}
        \label{appfig:cudatime}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.595\textwidth}
        \centering
        \includegraphics[trim={6mm, 6mm, 5mm, 5mm},clip,width=\textwidth]{figures/cudamemory.pdf}
        \caption{Memory usage.}
        \label{appfig:cudamemory}
    \end{subfigure}
    \caption{
    \textbf{Layer-wise relative speed and memory comparison between NAT and DiNAT, with respect to Swin.} 
    NAT layers, which are only two consecutive NA layers with kernel size 7\sq{}, are already up to 40\% faster than Swin layers with the same kernel size.
    DiNAT layers, comprised of an NA layer followed by a DiNA layer, are slightly slower in practice due to the break in memory access pattern, but are still faster than Swin layers.
    }
    \label{appfig:cudatimeandmemory}
\end{figure*}

\section*{\Large{Appendix}}


\section{Implementation notes}
\label{appsec:implementation}

As discussed in \cref{subsec:implementation}, we extend the existing \natten{} package to support dilated neighborhoods.
\natten{} has a two-stage attention computation, similar to many other implementations: QK, and AV. The former computes the dot product of queries and keys, and produces attention weights, and the latter applies attention weights to the values. Scaling, softmax, and dropout are not included, as to prevent re-implementation.
One of the advantages of this two-stage structure over manual implementations is that, like implementations of convolutions, sliding windows are taken directly from the source tensor, and not cached into an intermediary tensor, thus using significantly less memory. We refer readers to \natten{} documentation, and NAT~\cite{hassani2022neighborhood} for further details.

\paragraph{Dilation support.} Adding dilation to \natten{}'s naive kernels is mostly simple: instead of incrementing neighbors across each axis by , we simply instruct the kernels to increment by a variable . NA however has a special way to handle edge/corner pixels, which requires additional changes to support dilation.
The greater challenge in adding dilation to \natten{} was adding it to the ``tiled'' kernels that utilize shared memory.
Tiled NA kernels are a more recent addition to \natten{}, and boost NA's throughput significantly. 
Tiled implementations of matrix multiplication and convolutions are essential in parallelizing these operations efficiently, while minimizing DRAM accesses.
As the name suggests, tiled implementations divide the operation into tiles and cache tiles o inputs from the global memory into the shared memory within each threadblock.
Accessing values from shared memory is typically much faster compared to directly accessing global memory, but also comes with challenges such as bank conflicts.
Tiled implementations also operate with the assumption that access patterns are not broken.
Introducing dilation values would break those access patterns and require a re-implementation that ensures dilated neighbors are cached instead of local neighbors.
We present a layer-wise relative speed and memory usage comparison between NAT and DiNAT with respect to Swin in \cref{appfig:cudatimeandmemory}.

\paragraph{Scaling and brain float support.} In order to train our larger models and avoid overflowing activation values in later layers of the model, we've had to switch from automatic mixed-precision training with the default half precision data type, float16, which has 5 exponent bits and 10 mantissa bits, to bfloat16, which has the advantage of having 8 exponent bits while having only 7 mantissa bits. Utilizing bfloat16 has often been recommended for cases which lead to large activations, which includes ours as we scale our model. However, switching to bfloat16 required a re-implementation of \natten{}'s half precision kernels to support and utilize bfloat16 correctly.

\section{Training settings}
\label{appsec:hyperparameters}
We provide additional details on training DiNAT in \cref{apptab:variants}.
We also provide details on DiNAT, which utilizes non-overlapping patch embedding and downsampling, similar to Swin~\cite{liu2021swin} and ConvNeXt~\cite{liu2022convnet}.
DiNAT serves as an alternative DiNA-based model, which has an architecture identical to Swin.
DiNAT can also serve as an ablation model, since it is identical to Swin in architecture, with WSA replaced with NA, and SWSA replaced with DiNA.

\setlength{\tabcolsep}{3pt}
\begin{table}[t]
    \centering
    \resizebox{0.475\textwidth}{!}{
    \begin{tabular}{lcccccc}
        \toprule
        \textbf{Variant}                        & \textbf{Downsampling} & \textbf{Layers}   & \textbf{Dim \texttimes{}}         & \textbf{MLP}          & \textbf{\# of}        & \textbf{FLOPs}\\
                                                &                       & \textbf{per level}& \textbf{Heads}                    & \textbf{ratio}        & \textbf{Params}       &               \\
        \midrule
        \db\textbf{DiNAT-T}         & Patched               & 2, 2,  6, 2       & 32 \texttimes{} 3                 & 4                     &  28 M                 &  4.5 G \\
        \db\textbf{DiNAT-S}         & Patched               & 2, 2, 18, 2       & 32 \texttimes{} 3                 & 4                     &  50 M                 &  8.7 G \\
        \db\textbf{DiNAT-B}         & Patched               & 2, 2, 18, 2       & 32 \texttimes{} 4                 & 4                     &  88 M                 & 15.4 G \\
        \db\textbf{DiNAT-L}         & Patched               & 2, 2, 18, 2       & 32 \texttimes{} 6                 & 4                     & 197 M                 & 34.5 G \\
        \midrule
        \db\textbf{DiNAT-M}             & Conv                  & 3, 4,  6, 5       & 32 \texttimes{} 2                 & 3                     &  20 M                 &  2.7 G \\
        \db\textbf{DiNAT-T}             & Conv                  & 3, 4, 18, 5       & 32 \texttimes{} 2                 & 3                     &  28 M                 &  4.3 G \\
        \db\textbf{DiNAT-S}             & Conv                  & 3, 4, 18, 5       & 32 \texttimes{} 3                 & 2                     &  51 M                 &  7.8 G \\
        \db\textbf{DiNAT-B}             & Conv                  & 3, 4, 18, 5       & 32 \texttimes{} 4                 & 2                     &  90 M                 & 13.7 G \\
        \db\textbf{DiNAT-L}             & Conv                  & 3, 4, 18, 5       & 32 \texttimes{} 6                 & 2                     & 200 M                 & 30.6 G \\
        \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Summary of DiNAT and DiNAT configurations.} 
    Channels (heads and dim) double after every level until the final one.
    Default dilation values for the four levels are 8, 4, 2, and 1. Kernel size is 7\texttimes{}7 in all variants.
    }
    \label{apptab:variants}
\end{table}

\setlength{\tabcolsep}{4pt}
\begin{table}[t]
    \centering
    \resizebox{0.475\textwidth}{!}{
    \begin{tabular}{lc|cccc}
        \toprule
        \textbf{Variant}                  & \textbf{Resolution} & \textbf{Level 1} & \textbf{Level 2}       & \textbf{Level 3}          & \textbf{Level 4}         \\
        \midrule
        \multicolumn{6}{c}{\textit{ImageNet classification.}}\\
        \midrule
        \db\textbf{DiNAT-T}     & 224\sq  & 1, 8             & 1, 4                   & 1, 2, 1, 2, 1, 2 & 1, 1   \\
        \db\textbf{DiNAT-S/B/L} & 224\sq  & 1, 8             & 1, 4                   & 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2  & 1, 1   \\
        \db\textbf{DiNAT-L}     & 384\sq  & 1, 13            & 1, 6                   & 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3  & 1, 1   \\
        \midrule
        \db\textbf{DiNAT-M}         & 224\sq  & 1, 8, 1          & 1, 4, 1, 4 & 1, 2, 1, 2, 1, 2 & 1, 1, 1, 1, 1   \\
        \db\textbf{DiNAT-T/S/B/L}   & 224\sq  & 1, 8, 1          & 1, 4, 1, 4 & 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2  & 1, 1, 1, 1, 1   \\
        \db\textbf{DiNAT-L}         & 384\sq  & 1, 13, 1         & 1, 6, 1, 6 & 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3  & 1, 1, 1, 1, 1   \\
        \midrule
        \multicolumn{6}{c}{\textit{MS-COCO detection and instance segmentation.}}\\
        \midrule
        \db\textbf{DiNAT-T}     & 800\sq  & 1, 28             & 1, 14                  & 1, 3, 1, 5, 1, 7  & 1, 3   \\
        \db\textbf{DiNAT-S/B/L} & 800\sq  & 1, 28             & 1, 14                  & 1, 3, 1, 5, 1, 7, 1, 3, 1, 5, 1, 7, 1, 3, 1, 5, 1, 7  & 1, 3   \\
        \midrule
        \db\textbf{DiNAT-M}         & 800\sq  & 1, 28, 1          & 1, 7, 1, 14            & 1, 3, 1, 5, 1, 7  & 1, 3, 1, 3, 1 \\
        \db\textbf{DiNAT-T/S/B/L}   & 800\sq  & 1, 28, 1          & 1, 7, 1, 14            & 1, 3, 1, 5, 1, 7, 1, 3, 1, 5, 1, 7, 1, 3, 1, 5, 1, 7  & 1, 3, 1, 3, 1 \\
        \midrule
        \multicolumn{6}{c}{\textit{ADE20K semantic segmentation.}}\\
        \midrule
        \db\textbf{DiNAT-T}     & 512\sq  & 1, 16             & 1, 8                   & 1, 2, 1, 3, 1, 4  & 1, 2   \\
        \db\textbf{DiNAT-S/B}   & 512\sq  & 1, 16             & 1, 8                   & 1, 2, 1, 3, 1, 4, 1, 2, 1, 3, 1, 4, 1, 2, 1, 3, 1, 4 & 1, 2   \\
        \db\textbf{DiNAT-L}     & 640\sq  & 1, 20             & 1, 10                  & 1, 2, 1, 3, 1, 4, 1, 5, 1, 2, 1, 3, 1, 4, 1, 5, 1, 5 & 1, 2   \\
        \midrule
        \db\textbf{DiNAT-M}         & 512\sq  & 1, 16, 1          & 1, 4, 1, 8             & 1, 2, 1, 3, 1, 4  & 1, 2, 1, 2, 1 \\
        \db\textbf{DiNAT-T/S/B}     & 512\sq  & 1, 16, 1          & 1, 4, 1, 8             & 1, 2, 1, 3, 1, 4, 1, 2, 1, 3, 1, 4, 1, 2, 1, 3, 1, 4  & 1, 2, 1, 2, 1 \\
        \db\textbf{DiNAT-L}         & 640\sq  & 1, 20, 1          & 1, 5, 1, 10            & 1, 2, 1, 3, 1, 4, 1, 5, 1, 2, 1, 3, 1, 4, 1, 5, 1, 5  & 1, 2, 1, 2, 1 \\
        \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Dilation values.} 
    Due to ImageNet's relatively small input resolution, level 4 layers cannot go beyond a dilation value of 1, which is equivalent to NA. 
    Also note that at 224\texttimes{}224 resolution, level 4 inputs will be exactly 7\texttimes{}7, therefore NA will be equivalent to self attention. 
    This is not true in downstream tasks where resolutions are noticeably higher where levels 2 and 3 have \textit{gradually} increasing dilation values, which are repeated in deeper models. 
    This corresponds to the highlighted rows in \cref{tab:dilationperformance} labeled ``Gradual''. 
    These configurations apply to all downstream experiments (excluding those in \cref{subsec:miscexps}).
    }
    \label{apptab:dinatsettings}
\end{table}

One of the most important architecture-related hyperparameters in DiNA-based models is dilation values. 
Both DiNAT and DiNAT use a combination of NA and DiNA layers.
We typically set dilation values in DiNA layers to be the maximum possible value with respect to input resolutions, if known.
For example, ImageNet classification at 224\texttimes{}224 is downsampled to a quarter of the original size initially, therefore Level 1 layers take feature maps of resolution 56\texttimes{}56 as input.
With a kernel size of 7\texttimes{}7, the maximum possible dilation value is . Level 2 will take feature maps of resolution 28\texttimes{}28 as input, leading to a maximum possible dilation value of .
Because of this, we change dilation values depending on the task and resolution.
We present the final dilation values we used in classification, detection, and segmentation in \cref{apptab:dinatsettings}.
Note that we only change dilation values for DiNA layers, since we found that fine-tuning NA layers to DiNA layers may result in a slight decrease in initial performance (see \cref{subsec:miscexps}, \cref{tab:testtimedilationchange}).

\section{Experiments with alternative architecture}

\label{appsec:altarchresults}
We conducted all primary experiments with both our main model, DiNAT, as well as DiNAT.
We found that DiNAT could serve as alternatives in certain cases, as they still provide noticeable improvements over Swin in terms of speed, accuracy, and memory usage.
Classification results are provided in \cref{apptab:imagenet_comparison}, object detection and instance segmentation results are provided in \cref{apptab:objectdetection}, and semantic segmentation results are provided in \cref{apptab:semseg}.

\setlength{\tabcolsep}{3pt}
\begin{table}[t]
    \centering
    \resizebox{0.475\textwidth}{!}{
    \begin{tabular}{lcccccc}
        \toprule
        \textbf{Model} & \textbf{Res.} & \textbf{\# of}     & \textbf{FLOPs} & \textbf{Thru.} & \textbf{Memory} & \textbf{Top-1}\\
                       &               & \textbf{Params}    &                & (imgs/sec)     & (GB)            & (\%) \\
        \midrule
        \multicolumn{7}{c}{\textit{ImageNet-1K trained models}}\\
        \midrule
        \nb\textbf{NAT-M}                           & 224\sq &  20 M &   2.7 G & 2132 &  2.4 & \textbf{81.8} \\
        \ours \textbf{DiNAT-M}                      & 224\sq &  20 M &   2.7 G & 2080 &  2.4 & \textbf{81.8} \\
        \midrule
        \wb\textbf{Swin-T}                          & 224\sq &  28 M &   4.5 G & 1724 &  4.8 & 81.3 \\
        \ours \textbf{DiNAT-T}                  & 224\sq &  28 M &   4.5 G & 1954 &  4.0 & 81.8 \\
        \cb\textbf{ConvNeXt-T}                      & 224\sq &  28 M &   4.5 G & 2491 &  3.4 & 82.1 \\
        \nb\textbf{NAT-T}                           & 224\sq &  28 M &   4.3 G & 1537 &  2.5 & \textbf{83.2} \\
        \ours \textbf{DiNAT-T}                      & 224\sq &  28 M &   4.3 G & 1500 &  2.5 & 82.7 \\
        \midrule
        \wb\textbf{Swin-S}                          & 224\sq &  50 M &   8.7 G & 1056 &  5.0 & 83.0 \\
        \ours \textbf{DiNAT-S}                  & 224\sq &  50 M &   8.7 G & 1203 &  4.1 & 83.5 \\
        \cb\textbf{ConvNeXt-S}                      & 224\sq &  50 M &   8.7 G & 1549 &  3.5 & 83.1 \\
        \nb\textbf{NAT-S}                           & 224\sq &  51 M &   7.8 G & 1049 &  3.7 & 83.7 \\
        \ours \textbf{DiNAT-S}                      & 224\sq &  51 M &   7.8 G & 1058 &  3.7 & \textbf{83.8} \\
        \midrule
        \wb\textbf{Swin-B}                          & 224\sq &  88 M &  15.4 G &  774 &  6.7 & 83.5 \\
        \ours \textbf{DiNAT-B}                  & 224\sq &  88 M &  15.4 G &  877 &  5.5 & 83.8 \\
        \cb\textbf{ConvNeXt-B}                      & 224\sq &  89 M &  15.4 G & 1107 &  4.8 & 83.8 \\
        \nb\textbf{NAT-B}                           & 224\sq &  90 M &  13.7 G &  781 &  5.0 & 84.3 \\
        \ours \textbf{DiNAT-B}                      & 224\sq &  90 M &  13.7 G &  764 &  5.0 & \textbf{84.4} \\
        \midrule
        \multicolumn{7}{c}{\textit{ImageNet-22K pre-trained models}}\\
        \midrule
        \wb\textbf{Swin-L}                          & 224\sq & 197 M &  34.5 G &  478 & 10.4 & 86.3 \\
        \ours \textbf{DiNAT-L}                  & 224\sq & 197 M &  34.5 G &  528 &  8.6 & 86.5 \\
        \cb\textbf{ConvNeXt-L}                      & 224\sq & 198 M &  34.4 G &  643 &  7.5 & \textbf{86.6} \\
\ours \textbf{DiNAT-L}                      & 224\sq & 200 M &  30.6 G &  474 &  7.8 & \textbf{86.6} \\
        \midrule
        \wb\textbf{Swin-L\dgr}                      & 384\sq & 197 M & 104.0 G &  169 & 32.7 & 87.3 \\
        \ours \textbf{DiNAT-L}                  & 384\sq & 197 M & 101.5 G &  181 & 22.6 & 87.4 \\
        \cb\textbf{ConvNeXt-L}                      & 384\sq & 198 M & 101.1 G &  221 & 19.2 & \textbf{87.5} \\
\ours \textbf{DiNAT-L}                      & 384\sq & 200 M &  89.7 G &  161 & 20.1 & 87.4 \\
        \ours \textbf{DiNAT-L\dgr}                  & 384\sq & 200 M &  92.4 G &  110 & 26.9 & \textbf{87.5} \\
        \bottomrule
    \end{tabular}
    }
    \caption{
    \textbf{ImageNet-1K image classification performance.} 
    \dgr indicates increased window size from 7\sq{} to 11\sq{} (DiNAT) and 12\sq{} (Swin). 
    Throughput and peak memory usage are measured from forward passes with a batch size of 256 on a single A100 GPU.
    Note that DiNAT is identical in architecture to Swin, and only different in attention modules (WSA/SWSA replaced with NA/DiNA).
    }
    \label{apptab:imagenet_comparison}
\end{table}

\setlength{\tabcolsep}{3pt}
\begin{table}[t]
    \centering
    \resizebox{0.475\textwidth}{!}{
    \begin{tabular}{lccc|ccc|ccc}
        \toprule
        \textbf{Backbone} & \textbf{\# of} & \textbf{FLOPs} & \textbf{Thru.} & \textbf{AP\textsuperscript{b}} & \textbf{AP\textsuperscript{b}\textsubscript{50}} & \textbf{AP\textsuperscript{b}\textsubscript{75}} & \textbf{AP\textsuperscript{m}} & \textbf{AP\textsuperscript{m}\textsubscript{50}} & \textbf{AP\textsuperscript{m}\textsubscript{75}} \\ 
        & \textbf{Params} &&(FPS)\\
        \midrule
        \multicolumn{10}{c}{\textit{Mask R-CNN - 3x schedule}} \\
        \midrule
        \nb\textbf{NAT-M}                                       &  40 M &  225 G & 54.1 & 46.5 & 68.1 & 51.3 & 41.7 & 65.2 & 44.7 \\
        \ours \textbf{DiNAT-M}                                  &  40 M &  225 G & 53.8 & \textbf{47.2} & \textbf{69.1} & \textbf{51.9} & \textbf{42.5} & \textbf{66.0} & \textbf{45.9} \\
        \midrule
        \wb\textbf{Swin-T}                                      &  48 M &  267 G & 45.1 & 46.0 & 68.1 & 50.3 & 41.6 & 65.1 & 44.9 \\
        \ours \textbf{DiNAT-T}                              &  48 M &  263 G & 52.5 & 46.6 & 68.8 & 51.3 & 42.1 & 65.7 & 45.4 \\
        \cb\textbf{ConvNeXt-T}                                  &  48 M &  262 G & 52.0 & 46.2 & 67.0 & 50.8 & 41.7 & 65.0 & 44.9 \\
        \nb\textbf{NAT-T}                                       &  48 M &  258 G & 44.5 & 47.7 & 69.0 & 52.6 & 42.6 & 66.1 & 45.9 \\
        \ours \textbf{DiNAT-T}                                  &  48 M &  258 G & 43.3 & \textbf{48.6} & \textbf{70.2} & \textbf{53.4} & \textbf{43.5} & \textbf{67.3} & \textbf{46.8} \\
        \midrule
        \wb\textbf{Swin-S}                                      &  69 M &  359 G & 31.7 & 48.5 & 70.2 & 53.5 & 43.3 & 67.3 & 46.6 \\
        \ours \textbf{DiNAT-S}                              &  69 M &  350 G & 38.7 & 48.6 & 70.4 & 53.2 & 43.5 & 67.6 & 46.9 \\
        \nb\textbf{NAT-S}                                       &  70 M &  330 G & 34.8 & 48.4 & 69.8 & 53.2 & 43.2 & 66.9 & 46.5 \\
        \ours \textbf{DiNAT-S}                                  &  70 M &  330 G & 35.3 & \textbf{49.3} & \textbf{70.8} & \textbf{54.2} & \textbf{44.0} & \textbf{68.0} & \textbf{47.4} \\
        \midrule
        \multicolumn{10}{c}{\textit{Cascade Mask R-CNN - 3x schedule}} \\
        \midrule
        \nb\textbf{NAT-M}                                       &  77 M &  704 G & 27.8 & 50.3 & 68.9 & 54.9 & 43.6 & 66.4 & 47.2 \\
        \ours \textbf{DiNAT-M}                                  &  77 M &  704 G & 27.6 & \textbf{51.2} & \textbf{69.8} & \textbf{55.7} & \textbf{44.4} & \textbf{67.3} & \textbf{47.8} \\
        \midrule
        \wb\textbf{Swin-T}                                      &  86 M &  745 G & 25.1 & 50.4 & 69.2 & 54.7 & 43.7 & 66.6 & 47.3 \\
        \ours \textbf{DiNAT-T}                              &  86 M &  742 G & 27.4 & 51.0 & 69.9 & 55.4 & 44.1 & 67.3 & 47.6 \\
        \cb\textbf{ConvNeXt-T}                                  &  86 M &  741 G & 27.3 & 50.4 & 69.1 & 54.8 & 43.7 & 66.5 & 47.3 \\
        \nb\textbf{NAT-T}                                       &  85 M &  737 G & 24.9 & 51.4 & 70.0 & 55.9 & 44.5 & 67.6 & 47.9 \\
        \ours \textbf{DiNAT-T}                                  &  85 M &  737 G & 25.0 & \textbf{52.2} & \textbf{71.0} & \textbf{56.8} & \textbf{45.1} & \textbf{68.3} & \textbf{48.8} \\
        \midrule
        \wb\textbf{Swin-S}                                      & 107 M &  838 G & 20.3 & 51.8 & 70.4 & 56.3 & 44.7 & 67.9 & 48.5 \\
        \ours \textbf{DiNAT-S}                              & 107 M &  829 G & 23.1 & 52.3 & 71.2 & 56.7 & 45.2 & 68.6 & 49.1 \\
        \cb\textbf{ConvNeXt-S}                                  & 108 M &  827 G & 23.0 & 51.9 & 70.8 & 56.5 & 45.0 & 68.4 & 49.1 \\
        \nb\textbf{NAT-S}                                       & 108 M &  809 G & 21.7 & 52.0 & 70.4 & 56.3 & 44.9 & 68.1 & 48.6 \\
        \ours \textbf{DiNAT-S}                                  & 108 M &  809 G & 21.8 & \textbf{52.9} & \textbf{71.8} & \textbf{57.6} & \textbf{45.8} & \textbf{69.3} & \textbf{49.9} \\
        \midrule
        \wb\textbf{Swin-B}                                      & 145 M &  982 G & 17.3 & 51.9 & 70.9 & 56.5 & 45.0 & 68.4 & 48.7 \\
        \ours \textbf{DiNAT-B}                              & 145 M &  966 G & 19.7 & 52.6 & 71.5 & 57.2 & 45.3 & 68.8 & 49.1 \\
        \cb\textbf{ConvNeXt-B}                                  & 146 M &  964 G & 19.5 & 52.7 & 71.3 & 57.2 & 45.6 & 68.9 & 49.5 \\
        \nb\textbf{NAT-B}                                       & 147 M &  931 G & 18.6 & 52.3 & 70.9 & 56.9 & 45.1 & 68.3 & 49.1 \\
        \ours \textbf{DiNAT-B}                                  & 147 M &  931 G & 18.5 & \textbf{53.4} & \textbf{72.1} & \textbf{58.2} & \textbf{46.2} & \textbf{69.7} & \textbf{50.2} \\
        \midrule
        \wb\textbf{Swin-L\strr\dgg}                             & 253 M & 1393 G & 12.9 & 53.7 & 72.2 & 58.7 & 46.4 & 69.9 & 50.7 \\
        \ours \textbf{DiNAT-L\dgg}                          & 253 M & 1357 G & 15.0 & 54.8 & 74.2 & 59.8 & 47.2 & 71.3 & 51.2 \\
        \cb\textbf{ConvNeXt-L\dgg}                              & 253 M & 1354 G & 14.8 & 54.8 & 73.8 & 59.8 & 47.6 & 71.3 & 51.7 \\
\ours \textbf{DiNAT-L\dgg}                              & 258 M & 1276 G & 14.0 & \textbf{55.3} & \textbf{74.3} & \textbf{60.2} & \textbf{47.8} & \textbf{71.8} & \textbf{52.0} \\
        \bottomrule
    \end{tabular}
    }
    \caption{
    \textbf{COCO object detection and instance segmentation performance.} 
    \dgg indicates that the model was pre-trained on ImageNet-22K. 
    \strr Swin-L was not reported with Cascade Mask R-CNN, therefore we trained it with their official checkpoint. 
    Throughput is measured on a single A100 GPU.
    Note that DiNAT is identical in architecture to Swin, and only different in attention modules (WSA/SWSA replaced with NA/DiNA).
    }
    \label{apptab:objectdetection}
\end{table}

\setlength{\tabcolsep}{5pt}
\begin{table}[t]
    \centering
    \resizebox{0.475\textwidth}{!}{
    \begin{tabular}{lc|c|cc|c}
        \toprule
        \textbf{Model}                              & \textbf{Dilation}             & \textbf{ImageNet} & \multicolumn{2}{c}{\textbf{MSCOCO}}                               & \textbf{ADE20K}   \\
                                                    & \textbf{per level}            & \textbf{Top-1 (\%)}& \textbf{AP\textsuperscript{b}} & \textbf{AP\textsuperscript{m}}   & \textbf{mIoU}     \\
        \midrule
        \wb\textbf{Swin-Tiny}                       & Not Applicable                & 81.3    & 46.0                           & 41.6                             & 45.8              \\
        \nb\textbf{NAT-Tiny}                    & 1, 1, 1, 1                    & 81.8    & 46.1                           & 41.5                             & 46.2              \\
        \db\textbf{DiNAT-Tiny}                  & 8, 4, 2, 1                    & 81.8    & 46.3                           & 41.6                             & 46.7              \\
        \db\textbf{DiNAT-Tiny}                  & 16, 8, 4, 2                   & -       & \textbf{46.4}                  & \textbf{41.8}                    & 47.1              \\
        \db\textbf{DiNAT-Tiny}                  & Maximum                       & 81.8    & \textbf{46.4}                  & \textbf{41.9}                    & 47.0              \\
        \ours \textbf{DiNAT-Tiny}               & Gradual                       & -       & \textbf{46.6}                  & \textbf{42.1}                    & \textbf{47.4}     \\
        \midrule
        \nb\textbf{NAT-Tiny}                        & 1, 1, 1, 1                    & 83.2    & 47.7                           & 42.6                             & 48.4              \\
        \db\textbf{DiNAT-Tiny}                      & 8, 4, 2, 1                    & 82.7    & 48.0                           & 42.9                             & 48.5              \\
        \db\textbf{DiNAT-Tiny}                      & 16, 8, 4, 2                   & -       & 48.3                           & 43.4                             & 48.5              \\
        \db\textbf{DiNAT-Tiny}                      & Maximum                       & 82.7    & \textbf{48.6}                  & \textbf{43.5}                    & 48.7              \\
        \ours \textbf{DiNAT-Tiny}                   & Gradual                       & -       & \textbf{48.6}                  & \textbf{43.5}                    & \textbf{48.8}     \\
        \bottomrule
    \end{tabular}
    }
    \caption{
    \textbf{Dilation impact on performance.} 
    Models listed within the same section have identical architectures and are different only in attention patterns (NAT is identical to Swin with both WSA and SWSA replaced with NA, DiNAT replaces SWSA with DiNA).
    }
    \label{apptab:dilationperformance}
\end{table}


\setlength{\tabcolsep}{3pt}
\begin{table}[t]
    \centering
    \resizebox{0.475\textwidth}{!}{
    \begin{tabular}{l|cccc|cc}
        \toprule
        \textbf{Backbone} & \textbf{Res.} & \textbf{\# of}     & \textbf{FLOPs} & \textbf{Thru.} & \multicolumn{2}{c}{\textbf{mIoU}}\\ 
                          &               & \textbf{Params}    &                & (FPS)          & single scale & multi scale \\ 
        \midrule
        \nb\textbf{NAT-M}                           & 2048 \texttimes{} 512 &  50 M &  900 G & 24.5 & 45.1 & 46.4 \\
        \ours \textbf{DiNAT-M}                      & 2048 \texttimes{} 512 &  50 M &  900 G & 24.2 & \textbf{45.8} & \textbf{47.2} \\
        \midrule
        \wb\textbf{Swin-T}                          & 2048 \texttimes{} 512 &  60 M &  946 G & 21.3 & 44.5 & 45.8 \\
        \ours \textbf{DiNAT-T}                  & 2048 \texttimes{} 512 &  60 M &  941 G & 23.5 & 46.0 & 47.4 \\
        \cb\textbf{ConvNeXt-T}                      & 2048 \texttimes{} 512 &  60 M &  939 G & 23.3 & 46.0 & 46.7 \\
        \nb\textbf{NAT-T}                           & 2048 \texttimes{} 512 &  58 M &  934 G & 21.4 & 47.1 & 48.4 \\
        \ours \textbf{DiNAT-T}                      & 2048 \texttimes{} 512 &  58 M &  934 G & 21.3 & \textbf{47.8} & \textbf{48.8} \\
        \midrule
        \wb\textbf{Swin-S}                          & 2048 \texttimes{} 512 &  81 M & 1040 G & 17.0 & 47.6 & 49.5 \\
        \ours \textbf{DiNAT-S}                  & 2048 \texttimes{} 512 &  81 M & 1030 G & 19.1 & 48.6 & \textbf{49.9} \\
        \cb\textbf{ConvNeXt-S}                      & 2048 \texttimes{} 512 &  82 M & 1027 G & 19.1 & 48.7 & 49.6 \\
        \nb\textbf{NAT-S}                           & 2048 \texttimes{} 512 &  82 M & 1010 G & 17.9 & 48.0 & 49.5 \\
        \ours \textbf{DiNAT-S}                      & 2048 \texttimes{} 512 &  82 M & 1010 G & 18.1 & \textbf{48.9} & \textbf{49.9} \\
        \midrule
        \wb\textbf{Swin-B}                          & 2048 \texttimes{} 512 & 121 M & 1188 G & 14.6 & 48.1 & 49.7 \\
        \ours \textbf{DiNAT-B}                  & 2048 \texttimes{} 512 & 121 M & 1173 G & 16.5 & 49.4 & 50.2 \\
        \cb\textbf{ConvNeXt-B}                      & 2048 \texttimes{} 512 & 122 M & 1170 G & 16.4 & 49.1 & 49.9 \\
        \nb\textbf{NAT-B}                           & 2048 \texttimes{} 512 & 123 M & 1137 G & 15.6 & 48.5 & 49.7 \\
        \ours \textbf{DiNAT-B}                      & 2048 \texttimes{} 512 & 123 M & 1137 G & 15.4 & \textbf{49.6} & \textbf{50.4} \\
        \midrule
        \wb\textbf{Swin-L\dgr\dgg}                  & 2560 \texttimes{} 640 & 234 M & 2585 G &  8.5 &   -  & 53.5 \\
        \ours \textbf{DiNAT-L\dgg}              & 2560 \texttimes{} 640 & 234 M & 2466 G &  9.7 & 53.4 & 54.6 \\
        \cb\textbf{ConvNeXt-L\dgg}                  & 2560 \texttimes{} 640 & 235 M & 2458 G &  9.6 & 53.2 & 53.7 \\
\ours \textbf{DiNAT-L\dgg}                  & 2560 \texttimes{} 640 & 238 M & 2335 G &  9.0 & \textbf{54.0} & \textbf{54.9} \\
        \bottomrule
    \end{tabular}
    }
    \caption{
    \textbf{ADE20K semantic segmentation performance.} 
    \dgg indicates that the model was pre-trained on ImageNet-22K. 
    \dgr indicates increased window size from 7\sq{} to 12\sq{}. 
    Throughput is measured on a single A100 GPU.
    Note that DiNAT is identical in architecture to Swin, and only different in attention modules (WSA/SWSA replaced with NA/DiNA).
    }
    \label{apptab:semseg}
\end{table}

In \cref{subsec:miscexps} we experimented with architecture-related hyperparameters that are introduced by DiNA: dilation values, and the ordering of NA and DiNA layers.
We also complete those dilation experiments by adding DiNAT and Swin, and presente the results in \cref{apptab:dilationperformance,apptab:nadinaorderperformance}.
\setlength{\tabcolsep}{5pt}
\begin{table}[t]
    \centering
    \resizebox{0.475\textwidth}{!}{
    \begin{tabular}{lc|c|cc|c}
        \toprule
        \textbf{Variant}                                & \textbf{Layer}            & \textbf{ImageNet} & \multicolumn{2}{c|}{\textbf{MSCOCO}}                              & \textbf{ADE20K}   \\
                                                        & \textbf{structure}        & \textbf{Top-1 (\%)}& \textbf{AP\textsuperscript{b}} & \textbf{AP\textsuperscript{m}}   & \textbf{mIoU}     \\
        \midrule
        \wb\wb\textbf{Swin-Tiny}                        & WSA-SWSA                  & 81.3            & 46.0                           & 41.6                             & 45.8              \\
        \nb\nb\textbf{NAT-Tiny}                     & NA-NA                     & \textbf{81.8}   & 46.1                           & 41.5                             & 46.2              \\
        \grayrow\nb\db \textbf{DiNAT-Tiny}          & NA-DiNA                   & \textbf{81.8}   & 46.4                           & \textbf{41.8}                    & \textbf{47.1}     \\
        \db\nb                                          & DiNA-NA                   & 81.5            & \textbf{46.5}                  & \textbf{41.8}                    & 46.9              \\
        \db\db                                          & DiNA-DiNA                 & 79.7            & 39.8                           & 36.8                             & 40.7              \\
        \midrule
        \nb\nb\textbf{NAT-Tiny}                         & NA-NA                     & \textbf{83.2}   & 47.7                           & 42.6                             & 48.4              \\
        \grayrow\nb\db\textbf{DiNAT-Tiny}               & NA-DiNA                   & 82.7            & 48.3                           & 43.4                             & \textbf{48.5}     \\
        \db\nb                                          & DiNA-NA                   & 82.6            & \textbf{48.5}                  & \textbf{43.5}                    & 47.9              \\
        \db\db                                          & DiNA-DiNA                 & 82.2            & 44.9                           & 40.5                             & 45.8              \\
        \bottomrule
    \end{tabular}
    }
    \caption{
    \textbf{Layer structure impact on performance.} 
}
    \label{apptab:nadinaorderperformance}
\end{table}
