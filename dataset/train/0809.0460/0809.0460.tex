\documentclass[11pt,onecolumn]{article}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{epsfig}
\usepackage{natbib}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{assumption}[theorem]{Assumption}
\usepackage{url}
\usepackage{color}
\newcommand{\newtext}[1]{{#1}}
\newcommand{\comment}[1]{}

\usepackage{fullpage}




\setlength{\columnsep}{20pt}




\begin{document}
\title{Stochastic Combinatorial Optimization under Probabilistic Constraints}
\author{
Shipra Agrawal \thanks{Department of Computer Science, Stanford University. 
Email: shipra@stanford.edu. Research partially supported by Boeing.}
    \and 
Amin Saberi \thanks{Department of Management Science and Engineering, Stanford University. Email: saberi@stanford.edu}
\and
Yinyu Ye \thanks{Department of Management Science and Engineering,
Stanford University. 
Email: yinyu-ye@stanford.edu. Research partially supported by Boeing.}
} 
\maketitle
\date{ }
\begin{abstract}
{\small
In this paper, we present approximation algorithms for combinatorial optimization problems under probabilistic constraints. Specifically, we focus on stochastic variants of two important combinatorial optimization problems: the k-center problem and the set cover problem, with uncertainty characterized by a probability distribution over set of points or elements to be covered. 
We consider these problems under adaptive and non-adaptive settings, and present efficient approximation algorithms for the case when underlying distribution is a product distribution. 
\newtext{In contrast to the {\it expected cost} model prevalent in stochastic optimization literature, our problem definitions support restrictions on the {\it probability distributions} of the total costs, via incorporating constraints that bound the probability with which the incurred costs may exceed a given threshold}. 
}
\end{abstract}
\section{Introduction}
\label{Intro}
\newtext{A prevalent model to deal with uncertain data in optimization problems is to minimize expected cost over an input probability distribution. However, the expected cost model does not adequately capture the following two aspects of the problem. Firstly, in many applications, constraint violations cannot be modeled by costs or penalties in any reasonable way (e.g., safety relevant restrictions like levels of a water reservoir). Thus, if the problem constraints involve an uncertain parameter, one would rather insist on bounding the probability that a decision is infeasible.}
This leads to {\it probabilistic or chance constraints} of type:
  
	where  and  are decision and random vectors, respectively, ``" refers to a finite set of constraints,  is a probability measure, and  is a small input constant.

Another criticism of expected value measure is that it fails to capture the {\it risk} associated with the decisions: two decisions are valued equally if they have same expected cost. However, it can be the case that while one decision incurs moderate cost under all scenarios, the other incurs a huge cost for a disaster scenario with non-negligible probability. A risk averse user will naturally prefer the former decision. 
Various measures have been proposed in finance and stochastic optimization literature to capture this notion of risk averseness. A popular measure is the `value-at-risk (VaR)' measure, which is widely used in financial models, and has even been written into some industry regulations \cite{VaR1, VaR2}. For a given risk aversion level , value-at-risk is given by the smallest value  such that probability that objective cost exceeds  is less than . This leads to the probabilistic constraint:

where  is the objective value for decision  in scenario .

\newtext{In this paper, we develop approximation algorithms for such probabilistically constrained optimization problems.
Specifically, we look at stochastic variants of two important combinatorial optimization problems: the k-center problem and the set cover problem, with uncertainty characterized by a probability distribution over subset of points or elements to be covered. We study the problems under ``non-adaptive" and ``adaptive" settings.
In non-adaptive
setting, the entire set cover (k-center) must be chosen before the random element set is known. The goal is to minimize the covering cost (clustering distance) while satisfying a constraint that probability of covering a random subset of elements is higher than a given input threshold. In adaptive setting, the set cover (k-center) can be chosen adaptively for each scenario {\it after} observing the random element set. The goal is to determine the quality of optimal adaptive solution using value-at-risk (VaR) measure, that is, determine the minimum value  such that probability that the covering cost (clustering distance) exceeds  is less than . Note that these two settings capture the two problem aspects mentioned in the previous paragraph.
}

Below we give formal definitions of our optimization problems and assumptions made on the statistical information available; followed by a summary of results and related previous work.
\paragraph{Non-adaptive stochastic k-center:} Consider a set  of  vertices. 
Assume that distance  between two vertices  and  in  is given by a
graph metric . The deterministic k-center problem is to find a subset , , which minimizes the distance  such that 
 
In the stochastic k-center problem, the subset of  that actually needs to be served is given by a random variable , where each vertex  appears in  independently with probability . 
The problem is to choose a set , , which minimizes the distance  such that 
 
for a small input constant .

\paragraph{Adaptive stochastic k-center}
In adaptive setting, the  centers will be chosen after the random subset  becomes known. Thus, the -center solution  is itself a random variable, and depends on the random subset . The problem is to compute the value-at-risk, that is, the distance  such that  

Here  denotes optimal -center solution for subset .

\paragraph{Non-adaptive stochastic set cover}
Given a universe of  elements , and a family  of  subsets of .
The deterministic set cover problem is to find the minimum cost subcollection  such that every element in  is covered by some set in . In the stochastic set cover problem, the elements to be covered are a random subset  of , where each element  appears independently in  with probability . 
The problem is to find minimum cost sub-collection  such that the probability that every element in  is covered is by some set in  higher than an input threshold .

\paragraph{Adaptive stochastic set cover}
In adaptive setting, the set cover will be chosen after the random subset of elements  becomes known. 
The problem is to compute the value-at-risk , that is the minimum value  such that 

Here  denotes optimal set cover for random subset .

\subsection{Summary of our results}
For the k-center problems (non-adaptive and adaptive), we present polynomial-time dynamic programming algorithms that give {\it optimal solutions} for {\it tree metrics}. Moreover, we show that the algorithms for tree metrics can be extended to give efficient PTAS for {\it planar graph metrics}, and more generally a class of graphs called `bounded genus' graphs. Here, the approximation is only in the number of centers; the probabilistic constraint holds exactly. For set cover problem, we give an -approximation algorithm for the non-adaptive case. We also show that for the adaptive case of this problem, verifying the probability threshold is atleast as hard as the problem of counting maximum independent sets of a graph, and hence is likely to be very hard to approximate. 

\newtext{We use combinatorial optimization techniques like dynamic programming to obtain fast and accurate algorithms for stochastic optimization problems. A common limitation of previous work \cite{goel99stochastic, swamy-risk-averse} on approximation algorithms for probabilistically constrained optimization problems is that the probabilistic constraint cannot be satisfied accurately. That is, an approximation of type  is obtained. We overcome this limitation by taking advantage of special structure of the problems in case of product distributions, and obtain approximation algorithms where probabilistic constraints hold exactly.}

\subsection{Related Work}
There has been significant recent interest in studying stochastic optimization models from approximation algorithms perspective. A variety of approximation results have been obtained for the expected value models where a compensation or recourse is available for failed scenarios (the two-stage recourse models), see, e.g., \cite{swamy-survey} and references therein. 
But, results on probabilistically constrained and risk-averse models are relatively limited. In general, probabilistic constraints are difficult to handle: reasons being the inherent nonconvexity of the feasible set of a probabilistic constraint, as well as computational difficulty of estimating the probability term itself. However, some success has been achieved in obtaining approximation algorithms for specific combinatorial optimization problems by taking advantage of structure of the problem. In this regard, closest to our work are \cite{goel99stochastic} and \cite{cormode-pods07}.  
Goel et al \cite{goel99stochastic} focus on stochastic load balancing problems,
where item sizes are independent random variables following Bernoulli, exponential, or Poisson distributions specified in the input. They obtain approximation algorithms for stochastic bin packing and knapsack problems under probabilistic constraints that limit the overflow probability of a bin or knapsack.  These problems fall into the non-adaptive framework described above. Cormode et al \cite{cormode-pods07} emphasize the application of ``uncertain k-center" and similar clustering problems in probabilistic databases, and present various bi-criteria approximation (both in number of centers and maximum distance to a center) algorithms for an expected cost model. Their results include a  approximation in number of centers for our non-adaptive k-center problem, but only under an assumption that the individual probabilities  are polynomial. As mentioned in the previous subsection, our work improves upon this result for tree and planar graph metrics. We give exact algorithm for tree metric case, and efficient PTAS for planar graphs, where the approximation factor and running time does not depend on the probabilities . The adaptive k-center problem was not considered in the referred work.

A recent unpublished work by Swamy \cite{swamy-risk-averse} considers two stage risk-averse models for stochastic set cover and related combinatorial optimization problems. In the two stage recourse model, some sets can be chosen in the first stage at a low cost, and then if a scenario is not covered, more sets can be bought in the second stage as a recourse action. The risk averse problem is to minimize the sum of first stage cost and value-at-risk for the second stage. It was observed that if the value-at-risk for second stage is fixed to be , the problem reduces to chance-constrained set cover without recourse - same as our non-adaptive set cover problem. Although the algorithms in \cite{swamy-risk-averse} can be used under more general assumptions of ``black box distributions", we present faster algorithms that achieve better approximation factors for the special case of product distributions. Specifically, {\it in contrast} to the results in \cite{swamy-risk-averse}, we do not incur any approximation in the probabilistic constraint, and the running time of our algorithms is independent of the input threshold .  

An alternate approach widely used for dealing with probabilistic constraints focuses on replacing such constraints by more tractable constraints \cite{calafiore-chance06, neimro-chance06, iyengar-chance07} so that any solution satisfying the new constraints also satisfies the original probabilistic constraints with high probability. Observe that this type of relaxation is {\it opposite} to what one aims for in the design of approximation algorithms. 
Although some approximation results have been obtained \cite{calafiore-chance06, neimro-chance06, iyengar-chance07}, they apply only to problems involving continuous random variables whose distribution satisfies a certain concentration-of-measure property. These conditions are not fulfilled by problems with  random vectors considered in this paper.

\section{Non-adaptive stochastic k-center problem}
In this section, we look at the non-adaptive k-center problem. We present a dynamic programming algorithm for choosing a set  of  centers that maximizes the `success probability'  for a given distance . The final solution can then be found by doing binary search for optimal  over a sorted list of  distances. 
Below, we first describe an exact algorithm for tree metrics. The algorithm is similar in spirit to the dynamic programming algorithm given in \cite{p-median} for (deterministic) -median problem under tree metrics. 
In the sequel, we extend this algorithm to obtain approximation algorithms for more general graph metrics.



\subsection{Exact algorithm for tree metrics}
Our algorithm for tree metrics is based on a key property of our model, that is, ``for any subtree in a tree, once the number of centers in the subtree and the center closest to its root are fixed, the probability of success for the subtree is independent of the rest of the tree". The reason this property holds lie with the structural properties of the problem on a tree graph, and our independence assumption on probability of vertices. The hierarchical structure of tree ensures that the closest center to any vertex in a subtree either lies inside the subtree or is the center closest to the root of the subtree. 
The independence assumption on vertices implies that inter-dependencies between disjoint subtrees are caused only due to the common centers used to cover them. Once the closest center to root and number of centers in the subtrees are fixed, the joint probability of success for a tree can be expressed as product of success probabilities for its subtrees.  This observation will give us the optimal substructure property required for a dynamic programming approach.

We make these ideas more precise in the following.
 
\paragraph{Dynamic programming algorithm}
Given a rooted tree  with root .  denotes the subtree of  under vertex  (including ),  denotes the  child edge of vertex , and  denotes the subtree of  on the left of the edge  (including  and edge ). 
Also,  denotes the total number of child edges of a vertex .



Now, for any subtree  of , define function  as maximum probability (i.e., the probability under optimal choice of centers) that random subsets  of  can be {\it covered} by -centers. Given clustering distance , we say that a set of vertices is {\it covered} by a set of centers iff for every vertex there is some center within distance .

Note that  gives the desired optimal value. We now define function  which will prove to be an essential tool for computing values of . Suppose it is given that  is a closest center to the root of the subtree , then  is defined as maximum probability that a set of  centers in , along with the center , can cover a random subset of . That is,


We employ a dynamic programming type procedure that proceeds bottom up in the tree and computes all values of  and  (and finally  for the whole tree ). 
\paragraph{The initial values:} For any leaf , ,

Also, for any vertex , . So, for any pair of vertices :


\begin{figure}[htbp]
\begin{center}
\includegraphics[totalheight=0.17\textheight]{dp1.ps}
\caption{Tree  and its subtrees}
\label{fig1}
\end{center}
\end{figure}

\noindent {\it \bf Computation of : }
Let  be the optimal set of -centers for tree , and  be the closest center to  in . Then, by the definition: 

Therefore we can compute  using the following relation:


\paragraph{Computation of :}
By definition,  is closest vertex to the root  of the subtree; and , the number of child edges of . If  is a leaf, then , and  is given by the initial values. 
Assume that  is not a leaf and . Let  be the vertex on the other end of edge  (refer Figure \ref{fig1}). The value of  is given by the following recursion:



The reason this equation holds is as follows. 
Since  was the closest center to the root of subtree , it remains closest center to the root of subtree . However for subtree  (same as ), there are two possible choices: either  remains the closest center, or a center in  is the closest center.  
The two terms on the right represent these two choices. The product expression follows from the independence property discussed in the beginning of this section.
 
We order the vertices of the tree from bottom to top and left to right. 
At stage , we compute values  for  vertex  picked in this order. For a given vertex ,  is computed for increasing values of  and , and all choices of  in . Then, we compute values , and go on to the next stage.  
Thus, at any stage, all the terms in above expression are already known from computations in the previous stages. 

\paragraph{Computing the optimal solution}
Assume that we have calculated (and recorded) all values of  and .  gives the optimal probability. The corresponding optimal set of -centers can be generated by carrying out another pass over this table of values. This is a standard component of any dynamic programming procedure, we omit the details here. 

\paragraph{Running time complexity} For each edge  and each vertex , we compute  for all  values of . 
Also, each computation of  requires taking  over atmost  terms. Therefore total complexity of computing
the terms  is .
For each vertex , there are atmost  values of  for which  need to be computed. 
And each of these computations takes  steps. Hence, total complexity of computing terms  is . 

Also, as a pre-procedure for the algorithm we compute the distance-matrix of the tree (this requires  steps). And, the algorithm needs to be repeated for  possible values of . Thus, total complexity of the procedure is .

\subsection{Extensions} 
\paragraph{Extensions to more general graph metrics}
In this section, we extend our algorithm to obtain efficient PTAS
for planar graphs and a more general class of graphs called ``bounded genus graphs". The heart of this approach lies in the adaptability of the structure of {\it c-outerplanar graphs} to dynamic programming. A c-outerplanar graph has the property that it can easily be decomposed into two subgraphs with just 2c common boundary nodes \cite{baker94}. 
Now, a dynamic programming algorithm similar to our algorithm for tree case can be used. For a given , let  is a -outerplanar graph. Then, using techniques in \cite{baker94},  can be recursively decomposed into -outerplanar subgraphs  and  with atmost  common boundary nodes. The dynamic programming recursion is now defined as:


Since there are  vertices, there are atmost  values of  and  for which  has to be computed, and each computation requires taking max over  values. So complexity of computing terms  is . Similarly, there are  values for which  has to be computed. Each computation requires taking max over  terms. Hence, total running time complexity of above procedure is . 

To extend this approach to general planar graphs, we can use graph decomposition concepts from \cite{baker94}. Here, we give an outline of the method. 
The idea is to decompose the planar graph into disjoint -outerplanar components by copying the nodes in every  `level' \cite{baker94}. Then, use the above algorithm for resulting -outerplanar graph. Note that we are potentially duplicating the centers in the copied levels. However, by pigeonhole principle, there exists  such that if we copy levels congruent to , , then number of centers increase by a factor of atmost . This gives a -approximation in number of centers, with running time .
A result by Eppstein \cite{eppstein} shows that similar decompositions can be achieved in polynomial-time for a more general class of graphs called ``bounded genus" graphs. Thus, our approximation algorithms extend in a natural way to this class of graphs. 

\paragraph{Extensions to other covering problems}
Our algorithm can be directly applied to other stochastic covering problems on planar graphs, like vertex cover, edge cover and dominating set. The basic idea remains the same: once we fix the number of centers (covering nodes or edges) in a subgraph and the closet center(s) to its boundary node(s), the probability of covering the subgraph is independent of the rest of the graph. Note however, that for problems with non-uniform cost of centers, our dynamic programming algorithm will be pseudo-polynomial (polynomial in `total cost').
\section{Adaptive stochastic k-center problem}
In adaptive setting, the goal is to find the minimum distance  such that the {\it failure probability}  is less than . Again, the desired value  could be found by doing a binary search over  values of , and testing for each  whether the failure probability is less than . 
However, evaluating this probability term is not straightforward. Here, a key difference from the non-adaptive setting is that a different set of centers  is chosen for each random scenario , optimized for the subset of vertices in that scenario. A brute force approach to find the failed scenarios would require solving a deterministic -center problem for each of the  subsets of .


In this section, we propose a dynamic programming algorithm to compute this failure probability in polynomial-time for a given value of . 
First, we present an exact algorithm for tree metrics, and then extend it to more general graph metrics.

\subsection{Exact algorithm for tree metrics}
The basic idea in our algorithm is to characterize each random subset of a subtree via a profile  that completely captures its covering properties. Specifically, given a subtree , a random subset  belongs to a profile  if and only if
\begin{itemize}
\item the {\it minimum} number of centers sufficient to cover  within distance  is ,
\item among the covers of size , minimum distance of a center to the root of  is , and
\item  is the maximum distance such that if a vertex  outside the subtree  and at distance  from its root is a center, then the subtree can be covered using only  centers. 
If no such vertex  exists, then . 
\end{itemize}
Note that each subset of vertices belongs to exactly one profile . This is because there is a unique minimum
number of centers  required for any subset, and that corresponds to a unique minimum distance  of closest center
to the root. Also note that using any {\it help}  from 
outside the tree atmost  center can be removed out of the  centers -- otherwise we could place a center at the root and reduce the minimum  number of centers to . Taking maximum of the distances of all such s from root, we get our unique . 

Above argument shows that the profile  define a disjoint partition over the subsets of any subtree . 
Now, define function  as the probability of random subsets in  under profile . Then, by definition, the probability of failure is given by:

Here,  can take atmost  possible values -- corresponding to possible distances of vertices from the root.
Now, we are ready to present our dynamic programming algorithm. We use the same notations as in the previous section. The algorithm will compute all values  in a bottom to top, left to right order, finally computing the values  that appear in the above expression for failure probability.

\paragraph{Initial values: } For , ,


\paragraph{Computation of : }
Now, assume that  is not a leaf, and . 
To compute  for some , we reduce it to an expression consisting of function  on subtrees  and , where  is the vertex on the other end of edge . We use the observation that a random subset  
of this tree has a profile  if and only if  
and   have profiles  and , respectively, satisfying {\it either} of the following conditions:
\begin{itemize}
\item : In this case, we must ensure that the centers in  do not help  and vice-versa so that total 
minimum number of centers is . Let  denote the distance , then we require . To get , the least of  and  must be equal to 
, and to get , the max of  and  must be equal to .
\item : In this case we must ensure that the centers in  help  {\it or} vice-versa, so that total 
minimum number of centers is , that is  or . To get , the least of 
 and  must be equal to . To get ,  must be equal to  if  is helped by , and  
must be equal to  if  is helped by .
\item : In this case we must ensure that the centers in  help  {\it and} vice-versa, so that total
minimum number of centers is , that is . To get , the least of 
 and  must be equal to . Only negative values of  have this case.
\end{itemize}
It is easy to see that in each of the above cases, the conditions on  and  are necessary and sufficient to get the joint profile . Let  denotes the collection of profiles ,  satisfying either of the above conditions. 
Then, using the fact that the profiles are disjoint, and independence assumptions on the probability model,   can be expressed as

Observe that due to the specific order in which we compute the values of , all terms in the above expression were already computed in a previous stage. 

\paragraph{Running time complexity}
For each edge, we compute atmost  values of  (possible values of  and , ). For each of these 
terms we sum over at most  terms. Therefore, total complexity is . The preprocessing time is  for computing distance pairs, and  for assigning initial values. Including the  iterations for binary search on , the effective complexity is . 


\subsection{Extensions}
The algorithm can be extended to more general graph classes and other covering problems on graphs, using ideas similar to those discussed at the end of previous section. We omit the details here.
\section{Non-adaptive stochastic set cover problem}
We give an approximation method for non-adaptive stochastic set cover problem by reformulating it as a partial set cover problem. The problem (refer Section \ref{Intro}) can be restated as:

 Here,  denotes the set . The value of 0-1 variable  indicates whether set  is chosen or not. 

For any element , let  denote the collection of sets that cover the element . Then, indicator function  takes value  if  is NOT covered by solution  and  otherwise. 
Using the assumptions on our probability model:


Let , and . Then, the probabilistic constraint is equivalent to:

Therefore, we can reformulate our problem as:

which is equivalent to the following integer program:


The above problem can be interpreted as a `partial set cover problem', where penalty for not covering an element  is given by . The partial set cover problem is to minimize the cost of sets () such that the total penalty () for uncovered elements is less than a given limit (). A -approximation algorithm for the partial set cover problem is appears in \cite{partialCover}. The algorithm can be directly used for the above problem. 

\section{Adaptive stochastic set-cover problem}
In the adaptive setting, our goal is to compute the minimum value  so that probability that cost of optimal set cover for a random subset of elements in  exceeds , is less than . Given a fixed value , we call the subsets of  with adaptive set cover cost  as {\it failed subsets}, and probability of these subsets as {\it failure probability}. We show that even for the uniform cost edge cover case, the problem of {\it approximating} this failure probability is harder than the problem of approximately counting maximum independent sets in a graph. 
An inapproximability result for the latter problem appears in \cite{luby-counting}, which states that this problem cannot be approximated within a polynomial factor unless RP=NP (refer Theorem  in \cite{luby-counting}). 
Thus, a reduction from this problem will suggest that our problem is hard to approximate as well.

Given graph  and a parameter , we denote the edge cover failure probability by . It is the probability of random subsets  of  such that the number of edges in the edge cover of  is greater than . We call such subsets of , the ``failed subsets". Let each vertex 
appears independently in the random subset  with probability  (that is,  for all ). 
Denote by  the number of failed subsets containing  vertices. Then,

Denote the count of maximum independent sets of graph  by . 
 Let  be the size of a maximum independent set in .
We show that computing  with a good approximation factor is harder 
than approximating the number of independent sets . Note that  denotes the number of subsets of  that have  vertices and need  or 
more edges to cover them. The edge cover needs  or more edges to cover 
vertices if and only if the  vertices form an independent set. Hence, 
. Therefore,

Observe that . Also, assume 
. Then,

From inequalities (\ref{lower-bound}) and (\ref{upper-bound}), we can conclude that 

Thus, if we have a  approximation of , then we could
get a  approximation for . These completes the reduction.




\bibliographystyle{plain}
\bibliography{paper}

\end{document}
