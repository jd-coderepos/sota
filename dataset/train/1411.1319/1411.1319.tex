\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{url}
\usepackage{xspace}

\usepackage{graphics}
\usepackage[dvips]{epsfig}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{algorithm, algorithmic}
\usepackage{comment}







\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{claim}{Claim}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{fact}{Fact}[section]
\newtheorem{example}{Example}[section]

\newcommand{\qed}{\hfill  \bigbreak}
\newenvironment{proof}{\noindent {\bf Proof.}}{\qed}
\newcommand{\noproof}{\hfill }
\newcommand{\reals}{I\!\!R}
\newcommand{\np}{\mbox{{\sc NP}}}
\newcommand{\sing}{\mbox{{\sc Sing}}}
\newcommand{\con}{\mbox{{\sc Con}}}
\newcommand{\hop}{\mbox{{\sc Hops}}}
\newcommand{\atm}{\mbox{{\sc ATM}}}
\newcommand{\hopn}{\hop_{\cN}}
\newcommand{\atmn}{\atm_{\cN}}
\newcommand{\cC}{{\cal C}}
\newcommand{\cN}{{\cal N}}
\newcommand{\cP}{{\cal P}}
\newcommand{\cD}{{\cal D}}
\newcommand{\cH}{{\cal H}}
\newcommand{\cS}{{\cal S}}
\newcommand{\D}{\Delta}
\newcommand{\cA}{{\cal A}}
\newcommand{\cR}{{\cal R}}

\newcommand{\A}{{\cal{R}}}
\newcommand{\E}{{\cal{E}}}
\newcommand{\Si}{\Sigma}

\newcommand{\classletter}{\ensuremath{\mathcal{G}}}
\newcommand{\ourclass}{\ensuremath{\classletter (k,\ell)}}
\newcommand{\treasure}{\ensuremath{g_{\ell+1}}}



\newcommand{\succeed}{{\sc success}}
\newcommand{\explore}{{\sc explore}}
\newcommand{\fail}{{\sc failure}}
\newcommand{\ceil}[1]{\lceil #1 \rceil}
\newcommand{\lcm}{\mbox{lcm}}
\newcommand{\dist}{\mbox{dist}}
\newcommand{\caL}{{\cal{L}}}

\newcommand{\remove}[1]{}

\newcommand{\qq}{\hfill  \smallbreak}

\newcommand{\cO}{{\cal O}}
\newcommand{\cZ}{{\mathbb Z}}
\newcommand{\bt}{{\bar t}}
\newcommand{\GC}{{\sc Graph\-Cover}}
\newcommand{\CW}{{\sc Cover\-Walk}}

\newcommand{\F}{\vspace*{\smallskipamount}}
\newcommand{\FF}{\vspace*{\medskipamount}}
\newcommand{\FFF}{\vspace*{\bigskipamount}}
\newcommand{\B}{\vspace*{-\smallskipamount}}
\newcommand{\BB}{\vspace*{-\medskipamount}}
\newcommand{\BBB}{\vspace*{-\bigskipamount}}
\newcommand{\BBBB}{\vspace*{-2.5\bigskipamount}}
\newcommand{\T}{\hspace*{1em}}
\newcommand{\TT}{\hspace*{2em}}
\newcommand{\TTT}{\hspace*{4em}}
\newcommand{\TTTT}{\hspace*{6em}}
\newcommand{\TTTTT}{\hspace*{10em}}


\newcommand{\caA}{{\cal{A}}}
\newcommand{\BW}{\mbox{\sc bw}}
\newcommand{\CBW}{\mbox{\sc cbw}}
\newcommand{\reco}{\mbox{\tt recognize}}
\newcommand{\recobis}{\mbox{\tt recognize-bis}}
\newcommand{\prem}{\mbox{\tt prime}}
\renewcommand{\wedge}{\,|\,}


\newcommand{\algname}{\ensuremath{{\cal A}}}
\newcommand{\diam}[1]{\ensuremath{{diam}(#1)}}
\newcommand{\chop}[1]{\ensuremath{\mathrm{chop}(#1)}}
\newcommand{\selected}[1]{\ensuremath{\mathrm{max}_{\cal A}(#1)}}
\newcommand{\advice}[1]{\ensuremath{\mathrm{adv}_{\algname}(#1)}}
\newcommand{\glue}[2]{\ensuremath{\mathrm{glue}(#1,#2)}}

\newcommand{\OurSelect}[1]{\ensuremath{\mathtt{Max-Select(#1)}}}
\newcommand{\candidates}[1]{\ensuremath{C_{#1}}}
\newcommand{\maxcandidates}{\ensuremath{\beta}}
\newcommand{\maxcandidatesval}{\ensuremath{\lceil8/\alpha\rceil}}
\newcommand{\smaller}[2]{\ensuremath{sml(#1,#2)}}
\newcommand{\bigger}[2]{\ensuremath{big(#1,#2)}}

\newcommand{\givensize}{\ensuremath{\tilde{n}}}
\newcommand{\givendiameter}{\ensuremath{D}}




\begin{document}

\baselineskip  0.18in \parskip     0.0in \parindent   0.2in 

\title{{\bf Election vs. Selection:\\ Two Ways of Finding the Largest Node in a Graph}}
\date{}
\newcommand{\inst}[1]{}

\author{
Avery Miller\inst{1},
Andrzej Pelc\inst{1}\footnote{Partially supported by NSERC discovery grant and by the Research Chair in Distributed Computing at the Universit\'e du Qu\'{e}bec en Outaouais.}\\
\inst{1} Universit\'{e} du Qu\'{e}bec en Outaouais, Gatineau, Canada.\\
E-mails: \url{avery@averymiller.ca}, \url{pelc@uqo.ca}\\
}

\date{ }
\maketitle

\begin{abstract}

Finding the node with the largest label in a labeled network, modeled as an undirected connected graph, is one of the fundamental problems in distributed computing.
This is the way in which {\em leader election} is usually solved. We consider two distinct tasks in which the largest-labeled node is found deterministically.
In {\em selection}, this node has to output 1 and all other nodes have to output 0. In {\em election}, the other nodes must additionally learn the largest
label (everybody has to know who is the elected leader). Our aim is to compare the difficulty of these two seemingly similar tasks executed under stringent running time constraints. The measure of difficulty is the amount of information
 that nodes of the network must initially possess, in order to solve the given task in an imposed amount of time. Following the standard framework of {\em algorithms
with advice}, this information (a single binary string) is provided to all nodes at the start by an oracle knowing the entire graph. The length of this string is called the {\em size of advice}. The paradigm of algorithms with advice has a far-reaching importance in the realm of network algorithms. Lower bounds on the size of advice
give us impossibility results based strictly on the \emph{amount} of initial knowledge outlined in a model's description.
This more general approach should be contrasted with
traditional results that focus on specific \emph{kinds} of information available to nodes, such as the size, diameter, or maximum node degree. 

Consider the class of -node graphs with any diameter , for some integer .
If time is larger than , then both tasks can be solved without advice.
For the task of {\em election}, we show that if time is smaller than , then the optimal size of advice is ,
and if time is exactly , then the optimal size of advice is .  For the task of {\em selection}, the situation changes dramatically,
even within the class of rings. Indeed, for the class of rings, we show that, if time is
, for any , then the optimal size of advice is , and, if time is  (and at most ) then
 this optimal size is . Thus there is an {\em exponential} increase of difficulty (measured by the size of advice) between selection in time , for any ,
and selection in time . As for the comparison between election and selection, our results show that, perhaps surprisingly, while  for small time, the difficulty of these two tasks 
 on rings is similar, for time   the difficulty of election (measured by the size of advice) is exponentially larger than that of selection.
\vspace{2ex}

\noindent {\bf Keywords:} election, selection, maximum finding, advice, deterministic distributed algorithm, time. 
\end{abstract}

\vfill



\vfill

\thispagestyle{empty}
\setcounter{page}{0}
\pagebreak


\section{Introduction}



{\bf Background.} Finding the node with the largest label in a labeled network is one of the fundamental problems in distributed computing.
This is the way in which {\em leader election} is usually solved. (In leader election, one node of a network has to become a {\em leader}
and all other nodes have to become {\em non-leaders}). In fact, to the best of our knowledge, all existing leader election algorithms
performed in labeled networks choose, as leader, the node with the largest label or the node with the smallest label \cite{Ly}. The classic problem
of leader election first appeared in the study of local area token ring networks \cite{LL}, where, at all times, exactly one node (the owner of a circulating token) has the right to initiate
communication. When the token is accidentally lost, a leader is elected as the initial owner of the token.



\noindent
{\bf Model and Problem Description.} The network is modeled as an undirected connected graph with  labeled nodes and with diameter  at most .
We denote by  the diameter of graph .
Labels are drawn from the set of integers , where  is polynomial in . Each node has a distinct label.
Initially each node knows its label and its degree. The node with the largest label in a graph will be called its {\em largest node}.

We use the extensively studied  communication model \cite{Pe}. In this model, communication proceeds in synchronous
rounds and all nodes start simultaneously. In each round, each node
can exchange arbitrary messages with all of its neighbours and perform arbitrary local computations. For any  and any node , we use
 to denote     
the knowledge acquired by  within  rounds. Thus,   consists of the subgraph induced by all nodes at distance at most 
from , except for the edges joining nodes at distance exactly  from , and of degrees (in the entire graph) of all nodes at distance exactly  from . Hence,
if no additional knowledge is provided {\em a priori} to the nodes, the decisions of  in round  in any deterministic algorithm are a function of . We denote by  the set of labels of nodes in the subgraph induced by all nodes at distance at most 
from .
The {\em time} of a task is the minimum number of rounds sufficient to complete it by all nodes. 

It is well known that the synchronous process of the   model can be simulated in an asynchronous network. This can be achieved 
by defining for each node separately its asynchronous round ;
in this round, a node performs local computations, then sends messages stamped  to all neighbours, and  waits until it gets messages stamped  from all neighbours.
To make this work, every node is required to send at least one (possibly empty) message with each stamp, until termination.
All of our results can be translated for asynchronous networks by replacing ``time of completing a task''  by ``the maximum number of asynchronous rounds  to complete it, taken over all nodes''.

We consider two distinct tasks in which the largest node is found deterministically.
In {\em selection}, this node has to output 1 and all other nodes have to output 0. In {\em election}, all nodes must output the largest label.
Note that in election all nodes perform selection and additionally learn the identity of the largest node.
Both variations are useful in different applications. In the aforementioned application of recovering a lost
token, selection is enough, as the chosen node will be the only one to get a single token and then the token will be passed from node to node. In this case,
other nodes do not need to know the identity of the chosen leader. The situation is different if all nodes must agree on a label of one of the nodes, e.g., to use it later
as a common parameter for further computations. Then the full strength of election is needed. Likewise, learning the largest label by all nodes is important when
labels carry some additional information apart from the identities of nodes, e.g., some values obtained by sensors located in nodes. Our results also remain valid
in such situations, as long as the  ``informative label'' can be represented as an integer polynomial in .  

Our aim is to compare the difficulty of the two seemingly similar tasks of selection and election executed under stringent running time constraints. 
The measure of difficulty is the amount of information
 that nodes of the network must initially possess in order to solve the given task in an imposed amount of time. Following the standard framework of {\em algorithms
with advice}, see, e.g.,   \cite{DP,EFKR,FGIP,FKL,FP,IKP,SN}, this information (a single binary string) is provided to all nodes at the start by an oracle knowing the entire graph. The length of this string is called the {\em size of advice}. 

The paradigm of algorithms with advice has a far-reaching importance in the realm of network algorithms. Establishing a tight bound on the minimum size of advice sufficient to accomplish a given task permits to rule out
entire classes of algorithms and thus focus only on possible candidates. For example, if we prove that  bits of advice are needed to perform a certain task in -node graphs, this rules out all 
potential algorithms that can work using only some linear upper bound on the size of the network, as such an upper bound could be given
to the nodes using  bits by providing them with
. Lower bounds on the size of advice
give us impossibility results based strictly on the \emph{amount} of initial knowledge outlined in a model's description.
This more general approach should be contrasted with
traditional results that focus on specific \emph{kinds} of information available to nodes, such as the size, diameter, or maximum node degree. 






\noindent
{\bf Our results.} Consider the class of -node graphs with any diameter , for some integer .
First observe that if time is larger than , then both tasks can be solved without advice,
as nodes learn the entire network and {\em learn that they have learned it}. Thus they can just choose the maximum of all labels seen.



For the task of {\em election}, we show that if time is smaller than , then the optimal size of advice is ,
and if time is exactly , then the optimal size of advice is . 
Here our contribution consists in proving two lower bounds on the size of advice. We prove one lower bound by exhibiting, for any positive integers , networks of size  and diameter , for which
 bits of advice are needed for election in time below . To prove the other lower bound, we present, for any positive integer , networks of diameter 
 for which  bits of advice are needed for election in time exactly . 
These lower bounds are clearly tight, as even for time 0,  bits of advice are enough to provide the largest label in the network, and, for time  --
when nodes know the entire network, but they {\em do not know that they know it} --  bits of advice are enough to give the diameter  to all nodes and thus reassure them that they have seen everything. In this case,  they can safely choose the maximum of all labels seen.

Hence, a high-level statement of our results for election is the following. If time is too small for all nodes to see everything, then no more efficient help in election is possible
than just giving the largest label. If time is sufficient for all nodes to see everything, but too small for them to realize that they do, i.e., the time is exactly , then no more efficient help in election is possible than providing , which supplies nodes with the missing certainty that they have seen everything.

It should be noted that  could be exponential
in , as in hypercubes, or  can even be constant with respect to arbitrarily large  . Thus, our results for election show that, for some networks, there is an exponential (or even
larger) gap of difficulty of election (measured by the size of advice) between time smaller than  and time exactly . Another such huge gap is between time
, when advice of size  is optimal, and time larger than , when 0 advice is enough. These gaps could be called {\em intra-task}
jumps in difficulty for election with respect to time.  

 
 


For the task of {\em selection}, the situation changes dramatically,
even within the class of rings. Indeed, for the class of rings, we show that, if time is
, for any , then the optimal size of advice is , and, if time is  (and at most ) then
 this optimal size is . Here our contribution is three-fold. For selection in time , for any ,  we exhibit, for any
 positive integer , a class of rings with diameter at most  which requires advice of size . As before, this lower bound is tight, even for time 0.
Further, for selection in time at most , for , we construct a class of rings with diameter  which requires advice of size . At first glance, it might seem that this lower bound is too weak. Indeed, providing  either the diameter or the largest label, which are both natural choices of advice, would not give a tight upper bound, as this may require  bits. However, we use a more sophisticated idea that permits us to construct a very compact advice 
(of matching size ) and we design a selection algorithm,
working for all rings of diameter  in time at most , for which this advice is enough.   
 
 
 
 
 Thus there is an {\em exponential} increase of difficulty (measured by the size of advice) between selection in time , for any ,
and selection in time . As in the case of election, another huge increase of difficulty occurs between time  and time larger than .  These gaps could be called {\em intra-task} jumps of difficulty for selection with respect to time. 

 As for the comparison between election and selection, our results show that, perhaps surprisingly, while  for small time, the difficulty of these two tasks 
 on rings is similar, for time   the difficulty of election (measured by the size of advice) is exponentially larger than that of selection,
even for the class of rings.
 While both in selection and in election the unique leader having the maximum label is chosen, these tasks differ in how widely this label is known. It follows from our results that,
if linear time (not larger than the diameter) is available, then the increase of difficulty (in terms of advice) of making this knowledge widely known is exponential.
This could be called the {\em inter-task} jump of difficulty between election and selection.  Figure \ref{summarytable} provides a summary of our results.


\begin{figure}[!ht]
\begin{center}
\includegraphics[scale=1]{summarytable}
\end{center}
\caption{Tight bounds on the size of advice for election and selection in arbitrary graphs and rings (respectively) with diameter diam }
\label{summarytable}
\end{figure}












\noindent
{\bf Related work.}
The leader election problem was introduced in \cite{LL}. This problem  has been extensively studied in the scenario adopted in the present paper, i.e.,
where all nodes have distinct labels. As far as we know, this task was always solved by finding either the node with the largest or that with the smallest label.
Leader election was first studied for rings.
A synchronous algorithm based on label comparisons and using
 messages was given in \cite{HS}. It was proved in \cite{FL} that
this complexity is optimal for comparison-based algorithms. On the other hand, the authors showed
an algorithm using a linear number of messages but requiring very large running time.
An asynchronous algorithm using  messages was given, e.g., in \cite{P}, and
the optimality of this message complexity was shown in \cite{B}. Deterministic leader election in radio networks has been studied, e.g., 
in \cite{JKZ,KP,NO}, as well as randomized leader election, e.g., in \cite{Wil}. In \cite{HKMMJ}, the leader election problem was
approached in a model based on mobile agents for networks with labeled nodes.

Many authors \cite{An,AtSn,ASW,BV,YK2,YK3} studied leader election
in anonymous networks. In particular, \cite{BSVCGS,YK3} characterize message-passing networks in which
leader election can be achieved when nodes are anonymous.
Characterizations of feasible instances for leader election were provided in~\cite{C,CM}.
Memory needed for leader election in unlabeled networks was studied in \cite{FP}. 







Providing nodes or agents with arbitrary kinds of information that can be used to perform network tasks more efficiently has previously been
proposed in \cite{AKM01,CFP,DP,EFKR,FGIP,FIP1,FIP2,FKL,FP,FPR,GPPR02,IKP,KKKP02,KKP05,SN,TZ05}. This approach was referred to as
{\em algorithms with advice}.  
The advice is given either to nodes of the network or to mobile agents performing some network task.
In the first case, instead of advice, the term {\em informative labeling schemes} is sometimes used, if (unlike in our scenario) different nodes can get different information.






Several authors studied the minimum size of advice required to solve
network problems in an efficient way. 
 In \cite{KKP05}, given a distributed representation of a solution for a problem,
the authors investigated the number of bits of communication needed to verify the legality of the represented solution.
In \cite{FIP1}, the authors compared the minimum size of advice required to
solve two information dissemination problems using a linear number of messages. 
In \cite{FKL}, it was shown that advice of constant size given to the nodes enables the distributed construction of a minimum
spanning tree in logarithmic time. 
In \cite{EFKR}, the advice paradigm was used for online problems.
In \cite{FGIP}, the authors established lower bounds on the size of advice 
needed to beat time 
for 3-coloring cycles and to achieve time  for 3-coloring unoriented trees.  
In the case of \cite{SN}, the issue was not efficiency but feasibility: it
was shown that  is the minimum size of advice
required to perform monotone connected graph clearing.
In \cite{IKP}, the authors studied radio networks for
which it is possible to perform centralized broadcasting in constant time. They proved that constant time is achievable with
 bits of advice in such networks, while
 bits are not enough. In \cite{FPR}, the authors studied the problem of topology recognition with advice given to nodes. 
To the best of our knowledge, the problems of leader election or maximum finding with advice have never been studied before.


\section{Election}


Notice that in order to perform election in a graph  in time larger than its diameter, no advice is needed. Indeed, after time , all nodes
know the labels of all other nodes, and  they are aware that they have this knowledge. This is because, in round , no messages containing new labels are received by any node. So, it suffices for all nodes to output the largest of the labels that they have seen up until round . 

Our first result shows that, if election time is no more than the diameter of the graph, then the size of advice must be at least logarithmic in the diameter. 
This demonstrates a dramatic difference between the difficulty of election in time  and election in time , measured by the minimum size of advice.



\begin{theorem}\label{diam}
Consider any algorithm  such that, for every graph , algorithm  solves election within  rounds. For every integer , there exists a ring of diameter at most  for which algorithm  requires advice of size .
\end{theorem}
\begin{proof}
Fix any integer . We will show a stronger statement: at least  different advice strings are needed in order to solve election within  rounds for 
rings  with diameter at most . The high-level idea of the proof is to first construct a particular sequence of  rings of increasing sizes, each with a different largest label.
With few advice strings, two such rings get the same advice. We show that there is a node in these two rings which acquires the same knowledge when executing algorithm , and hence has to elect the same node in both rings, which is a contradiction. 

To obtain a contradiction, assume that  different advice strings suffice. Consider a ring  of diameter  whose node labels form the sequence  (see Figure \ref{rings}). For each , define  to be the ring obtained from  by taking the subgraph induced by the nodes at distance at most  from node  and adding an edge between nodes  and  (see Figure \ref{rings}). First, note that, for each , the diameter of  is , and, the largest node in  has label . The correctness of  implies that, when  is executed at a node in , it must halt within  rounds and output . Next, by the Pigeonhole Principle, there exist  with  such that the same advice string is provided to nodes of both  and  when they execute . When executed at node 1 in , algorithm  halts in some round  and outputs . We show that, when executed at node 1 in , algorithm  also halts in round  and outputs . Indeed, the algorithm is provided with the same advice string for both  and , and,  in  is equal to   in . This contradicts the correctness of  since, for the ring , there is a node with label . To conclude, notice that, 
since at least  different advice strings are needed for the class of rings of diameter at most , the size of advice must be  for at least one
of these rings. 
\end{proof}

\begin{figure}[!ht]
\begin{center}
\includegraphics[scale=1]{rings}
\end{center}
\caption{Rings  and , as constructed in the proof of Theorem \ref{diam}}
\label{rings}
\end{figure}



Note that the lower bound established in Theorem \ref{diam} is tight. Indeed, to achieve election in time  for any graph , it is enough to provide
the value of  to the nodes of the graph and have each node elect the node with the largest label it has seen up until round  .
Hence we have the following corollary for the class of graphs of diameter at most .

\begin{corollary}\label{cor1}
The optimal size of advice to complete election in any graph in time at most equal to its diameter is .
\end{corollary}

We next consider accomplishing election in time less than the diameter of the graph. One way to do it is to provide the maximum label to all nodes as advice. This yields 
election in time 0 and uses advice of size , where  is the size of the graph,  since the space of labels is of size  polynomial in .
The following result shows that this size of advice cannot be improved for election in {\em any} time below the diameter. This result, when compared to Corollary
\ref{cor1}, again shows the dramatic difference in the difficulty of election (measured by the minimum size of advice) but now between times  and . 




\begin{theorem}
Consider any positive integers . There exists  such that, for any election algorithm  in which every execution halts within  rounds, there exists an -node graph of diameter  for which the size of advice needed by  is .
\end{theorem}
\begin{proof}
The high-level idea of the proof is the following. We first construct a family of ``ring-like'' graphs.
For a given number of advice strings, we obtain a lower bound on the number of such graphs for which the same advice is given.
On the other hand, an upper bound on this number is obtained by exploiting the fact that no node can see the entire graph within  rounds. Comparing these bounds gives the desired bound on the size of advice.  


Let  be the smallest integer greater than  that is divisible by 2D (and note that ).


Consider a family  of  pairwise disjoint sets, each of size . In particular, let 
, where
.

We construct a family  of -node graphs. Each graph in  is obtained by first choosing an arbitrary sequence of  sets from , say . The nodes of the graph are the elements of these sets (which are integers),
and this induces a natural labeling of the nodes. Next,
for each , add all edges between pairs of elements of the set , as well as all edges between every element in  and 
every element in  (where the indices are taken modulo ). 
In other words, each graph in  is a ``fat ring'', as illustrated in Figure \ref{fatring}. 
We uniquely identify each graph in  by its sequence of sets , where the node  with the smallest label belongs to the set ,
 and  contains the smallest neighbour of  outside of . The size of  is calculated in the following claim.
 
\begin{figure}[!ht]
\begin{center}
\includegraphics[scale=1]{fatring}
\end{center}
\caption{A ``fat ring'' with  and .}
\label{fatring}
\end{figure}

\vspace{3mm}\noindent{\bf Claim 1 } 
 .
\vspace{3mm}

To prove the claim, first note that the number of sequences  consisting of  distinct sets from  is . To count the number of such sequences belonging to , we first divide this integer by  to eliminate those sequences in which  does not contain the smallest label. Then, we divide the result by 2 to eliminate those sequences in which the labels in  are smaller than those in . This completes the proof of the claim.


Next, let  be the maximum number of advice bits provided to , taken over all graphs in . By the Pigeonhole Principle, there exists a family  of at least  graphs in  such that the algorithm receives the same advice string when executed on each graph in .
The following claim will be used to find an upper bound on the size of . 

\vspace{3mm}\noindent{\bf Claim 2 } 
\textit{Consider two graphs from , say  and . Suppose that, for some , algorithm  elects a node from set  when executed on  and elects a node from set  when executed on . If  for each , then .}
\vspace{3mm}

To prove the claim, let ).
Since  for all , it follows that each node  is also an element of , and, moreover, for each such ,
knowledge  in  is equal to knowledge  in .
The nodes of  output some label  at the end of the execution of  on .
Since we assumed that every execution of  halts within  rounds, and the same advice is given for  and , 
the nodes of  also output label  at the end of execution of  on . Since  elects a node from  when executed on  and elects a node from  when executed on , it follows that  is a label that appears in both  and . As the sets in  are pairwise disjoint, it follows that . This concludes the proof of the claim.

Using Claim 2, we now obtain an upper bound on the size of . In particular, for each , consider the subfamily of graphs  in  such that algorithm  elects a node from . By the claim, for each choice of the  sets , there is exactly one set  such that  belongs to . The number of such choices is bounded above by . Since this is true for all  possible values for , we get that . Comparing this upper bound to our lower bound on , it follows from Claim 1 that . Re-arranging this inequality, we get that , and hence .
\end{proof}

Hence we have the following corollary.

\begin{corollary}
The optimal size of advice to complete election in any graph in time less than its diameter is .
\end{corollary}

 
\section{Selection}

In this section, we study the selection problem for the class of rings. It turns out that significant differences between election and selection can be exhibited
already for this class. As in the case of election, and for the same reasons, selection in time larger than the diameter can be accomplished without any advice. Hence, in the rest of the section, we consider selection in time at most equal to the diameter. 
For any ring  and any selection algorithm , denote by  the advice string provided to all nodes in  when they execute algorithm . Denote by  the node that outputs 1 in the execution of algorithm  on ring .
We first look at selection algorithms working in time at most , for any ring  and any constant .

\subsection{Selection in time linear in the diameter}

We start with the lower bound on the size of advice needed by any selection algorithm working in time equal to the diameter of the ring.
This lower bound shows that, for any positive integer , there exists a ring with diameter at most  for which such an algorithm requires 
 bits of advice. Of course, this implies the same lower bound on the size of advice for selection in any smaller time.


The following theorem provides our first lower bound on the size of advice for selection. 

\begin{theorem}\label{lb1}
Consider any selection algorithm  such that, for any ring , algorithm  halts within  rounds. For every positive integer , there exists a ring  of diameter at most  for which algorithm  requires advice of size .
\end{theorem}
\begin{proof}
At a high level we consider a ``rings-into-bins'' problem, in which each bin represents a distinct advice string. We recursively construct sets of rings, such that 
the rings constructed at a given recursion level cannot be put into the same bin as previously-constructed rings. We continue the construction long enough to run out of bins. With few bins, the number of levels of recursion is sufficiently small to keep the diameters of the constructed rings bounded by .

The following claim will be used to show that a particular ring that we construct will cause algorithm  to fail.
We will use the following {\em chopping} operation in our constructions.
For any selection algorithm  and any ring  of odd size, we define the \emph{chop} of , denoted by , to be the path obtained from  by removing the edge between the two nodes at distance  from  (see Figure \ref{chopandglue}(a)).



\vspace{3mm}\noindent{\bf Claim 1 } 
\textit{Consider any selection algorithm  such that, for any ring , algorithm  halts within  rounds. Consider two disjoint rings  of odd size such that . For any ring  that contains  and  as subgraphs such that , two distinct nodes in  output 1 when executing .}
\vspace{3mm}

In order to prove the claim, 
consider the execution of  by the nodes of . Using the definition of , it can be shown that knowledge  in  is equal to knowledge  in . Therefore, the execution of  at node  in  halts in round , and  outputs 1. Similarly, the execution of  at node  in  halts in round , and  outputs 1. Since  and  are disjoint, we have that . Hence, two distinct nodes in  output 1 when executing .
This proves the claim.



It is enough to prove the theorem for sufficiently large .
Fix any integer  and set the label space to be . To obtain a contradiction, assume that  different advice strings suffice. 

Form a family  of  disjoint sequences of integers, each of size 3. More specifically, let . Let . Note that  is a set of integer labels, each of which is larger than all labels that belong to sequences in . To verify that we have enough labels to define these sets, note that the largest label . Using the inequality , we get that . When , one can verify that , and , so . It follows that . 



Next, we construct a special family  of rings of diameter at most . We will add rings to  by following a procedure that we will describe shortly. Each new ring that we add to  will be the result of at most one `gluing' operation, denoted by , that takes two disjoint odd-sized rings  and forms a new odd-sized ring . More specifically, the  gluing operation takes two disjoint odd-sized rings  and  and forms the new ring
defined as follows: construct paths  and ,  add a new edge between the leaf of  with smaller label and the leaf of  with smaller label, add a new node
with label , and add edges from this new node to the two remaining leaves.  
The gluing operation is illustrated in Figure \ref{chopandglue}(b). The additional node is introduced so that the resulting ring has an odd number of nodes. Note that this additional node's label comes from , which ensures that the new ring formed by a gluing operation does not contain duplicate labels. Further, note that, due to the dependence of the additional node's label on , no two gluing operations introduce additional nodes with the same label.

\begin{figure}[!ht]
\begin{center}
\includegraphics[scale=0.8]{chopandglue}
\end{center}
\caption{(a) The chop operation on a ring of size 3: the edge between the two nodes at distance  from node  is removed. (b) The glue operation on two rings of size 3: concatenate paths ,  and add an additional node  with a label from .}
\label{chopandglue}
\end{figure}


We now describe the procedure for adding rings to . In stage 1, we consider the set  of 3-cliques obtained from each sequence in  by identifying each integer with a node and adding all edges between them. We take a subset  of size at least  such that the same advice string is provided to the algorithm for each ring in . The rings in  are added to , which concludes stage 1. In each stage , we consider the set  of rings that were added to  in stage . The elements of  are partitioned into  pairs of rings in an arbitrary way. For each such pair , we perform . Define  to be the set of all of the resulting rings.   We take a subset  of size at least  such that the same advice string is provided to the algorithm for each ring in . The rings in  are added to , which concludes stage . This concludes the construction of .

It is not immediately clear that this construction can always be carried out. In particular, in order to define  in each stage ,
there must exist  rings in  such that the same advice string is provided to the algorithm for each. To prove this fact, and to obtain the desired contradiction to prove the theorem, we will use the following two claims. 

\vspace{3mm}\noindent{\bf Claim 2 } 
\textit{Consider any  and any ring . For every , there exists a ring  such that  contains  as a subgraph.}
\vspace{3mm}

To prove the claim, we proceed by induction on . The case where  is trivial. Next, assume that, for some  and any ring , for every , there exists a ring  such that  contains  as a subgraph. We now prove that, for any , there exists a ring  such that  contains  as a subgraph. By construction,  for some disjoint rings . By the definition of the gluing operation,  contains both  and  as disjoint subgraphs. Since  has one fewer edge than , at least one of   and  is a subgraph of . By induction, this proves Claim 2.

\vspace{3mm}\noindent{\bf Claim 3 } 
\textit{Consider any  with . For any ring , there exist disjoint  such that  contains  and  as subgraphs.}
\vspace{3mm}

To prove the claim, note that, by our construction,   for some disjoint . It follows that  and  are disjoint subgraphs of . If , setting  and  satisfies the statement of the claim. If , then, by Claim 2, there exists a ring  such that  contains  as a subgraph. Similarly, there exists a ring  such that  contains  as a subgraph. Note that, since  and  are disjoint, it follows that  and  are disjoint, so  and  are disjoint. Thus,  and  satisfy the statement of the claim, which concludes its proof.

We now show that, in any fixed stage of the above construction of , the advice string that is provided for the rings added to  in this stage is different than the advice strings provided for the rings added to  in all previous stages.

\vspace{3mm}\noindent{\bf Claim 4 } 
\textit{Consider any  with . For any ring  and any ring , we have .}
\vspace{3mm}

To prove the claim, notice that, by Claim 3, there exist disjoint  such that  contains  and  as subgraphs. Recall that the algorithm is provided the same advice string for all graphs in , so . By Claim 1 and the correctness of , it follows that . Hence, , which completes the proof of Claim 4.


We show that the construction of  can always be carried out. 

\vspace{3mm}\noindent{\bf Claim 5 } 
\textit{For all , in stage  of the construction, there exist at least  rings in  such that the same advice string is provided to the algorithm for each of them.}
\vspace{3mm}

To prove the claim, first note that, for , there are  different strings that could be used as advice for rings in . So, by the Pigeonhole Principle, there are at least  rings in  such that the same advice string is provided to the algorithm for each of them. Next, for any , Claim 4 implies that there are  strings that are not provided to the algorithm as advice for rings in . Namely, there are at most  different strings used as advice for rings in . By the Pigeonhole Principle, there are at least  rings in  such that the same advice string is provided to the algorithm for each of them, which proves the claim.

The following claim implies that , for all . Later, we will use two rings from some  to obtain the desired contradiction needed to complete the proof of the theorem. 

\vspace{3mm}\noindent{\bf Claim 6 } 
\textit{For all , at the end of stage , .}
\vspace{3mm} 

We prove the claim by induction on . When , we have . By Claim 5, there exist at least  rings in  such that the same advice string is provided to the algorithm for each of them. This implies that , as required. As induction hypothesis, assume that, at the end of some stage , . In stage , the set  is partitioned into pairs and  consists of one (glued) ring for each such pair. Thus, . By Claim 5, there exist at least  rings in  such that the same advice string is provided to the algorithm for each of them. This implies that , as required. This concludes the proof of Claim 6.

Finally, we construct a ring  on which algorithm  fails. Note that the rings in  all have node labels from the sets  and , and we proved that the largest integer in these sets is less than . Thus, the rings in  all have node labels from  the range . To construct , we take any two (disjoint) rings  (which exist by Claim 6) and form a ring  consisting of the concatenation of paths , , and the path of  nodes with labels . Recall that, in the construction, all rings in , for any fixed , get the same advice string.   
By Claim 4, each of the  distinct advice strings is used for rings in some . Therefore, there exists a stage  such that the advice provided for all graphs in  is the string . By Claim 2, there exists a ring  such that  contains  as a subgraph, and, there exists a ring  such that  contains . It follows that  contains  and  as subgraphs, and . By Claim 1, when algorithm  is executed by the nodes of ring , two distinct nodes output 1, which contradicts the correctness of . Note that the size of  is at least  and at most , which implies that the size of the label space is linear in the size of . The obtained contradiction was due to the assumption that . Hence the number  of different advice strings is at least , which implies that the size of advice is
 for some ring of diameter at most .
\end{proof}

Since imposing less time cannot make the selection task easier, we have the following corollary.

\begin{corollary}\label{cor}
For any constant ,
consider any selection algorithm  such that, for any ring , algorithm  halts within  rounds.  For every positive integer , there exists a ring  of diameter at most  for which algorithm  requires advice of size .
\end{corollary}


\begin{comment}
\subsection{ advice for selection in time }


Note that the lower bound established in Theorem \ref{lb1} uses the fact that the selection algorithm does not know the diameter of the ring. Indeed, if an algorithm
that works in time  knew , no extra advice would be needed, as nodes would know that they have seen the entire ring and hence could safely select (and even elect) the largest node. However, if selection time is strictly smaller than , it turns out that even within the class of rings of fixed and known
diameter ,  bits of advice are needed. We now prove this additional result.   



Let  be any positive integer, let , let , and suppose that  is a power of 2.  We consider algorithms that solve selection on the class of rings with diameter exactly  and labels from . Recall that we assume that  is polynomial in the size of the ring, and hence also in .

\begin{theorem}
For the class of rings with diameter exactly , any algorithm that always halts within  rounds requires advice of size . 
\end{theorem}
\begin{proof}
We first construct a special class  of rings with diameter exactly . Every ring in , for each integer , contains a node with label , along with two nodes with labels from . More precisely, the class  consists of rings ,   
for each ordered pair . Each ring   consists of nodes with the labels  in circular order (see Figure \ref{blah}). 
Since node with label  can be uniquely identified in the ring  as the node with neighbours labeled  and , there is a natural bijection between 
 and . Thus in the sequel we will identify the pair  with the ring  .  

Suppose that  different strings are provided as advice for rings in . Let  be any bijection from the set of advice strings to the set .

We construct a complete directed graph  on  nodes, each with a label in . We colour each edge  of   using . 
Let  be the resulting colouring.
We now define what it means for a colouring of  to be \emph{permissible}, and prove that  is a permissible colouring of . A colouring of  is \emph{permissible} if, for each colour  the following properties hold:
\begin{enumerate}
\item if  and  are coloured , then  is either smallest or largest in , 
\item if  and  are coloured , then  is either smallest or largest in .
\end{enumerate}

\begin{claim}
 is a permissible colouring.
\end{claim}


\begin{definition}
Consider any colouring  of . For any  and any , let , and let . Let , and let .
\end{definition}

\begin{lemma}\label{atmost}
In any legal colouring of a -clique, for any colour :
\begin{enumerate}
\item At most  nodes  have  or at most  nodes  have , and,
\item At most  nodes  have  or at most  nodes  have ,
\end{enumerate}
\end{lemma}









\end{proof}

\end{comment}




We now establish an upper bound on the size of advice that matches the lower bound from Theorem \ref{lb1}.
Let  be any positive integer and let   be a power of 2.  We consider algorithms that solve selection on the class of rings with diameter at most  and labels from . Recall that we assume that  is polynomial in the size of the ring, and hence also in .



In order to prove the upper bound, we propose a family of selection algorithms such that, for each fixed , there is an algorithm in the family that takes  bits of advice, and, for each ring , halts within  rounds. 

We start with an informal description of the algorithm and the advice for any fixed . The algorithm consists of two stages, and the advice consists of two substrings  and .

For any ring , the substring  of the advice is the binary representation of the integer . The size of this advice is . Note that .

In stage 1 of the algorithm the nodes perform  communication rounds, after which each node  has acquired knowledge . Next, each node  checks if its own label is the largest of the labels it has seen within  communication rounds, i.e., the largest in the set . If not, then  outputs 0 and halts immediately. 
Let  be the set of {\em candidate nodes}, i.e., nodes  whose label is the largest in . Clearly, the largest node in  is in , and every node knows if it belongs to
 . Nodes in  proceed to the next stage of the algorithm. 

In stage 2 of the algorithm, each node in  determines whether or not it is the largest node in , without using any further communication rounds. This is achieved using , the second substring of the advice, which we now describe.

Let  be the family of sets  of labels which contain all labels in  and no larger labels.
At a high level, we construct an integer colouring  of the family  such that, for any ,  
when the colour  is given as advice to candidate nodes, each of them can determine, without any communication, whether or not it is the candidate node with the largest label.
Call such a colouring {\em discriminatory}. 
Substring  of the advice will be  for some  and some discriminatory colouring  of . (We cannot simply use  because our colouring  will be defined on sets of fixed size, and sets of candidate nodes for different rings do not have to be of equal size.) 
Using , the candidate nodes solve selection among themselves.
This concludes the high-level description of the algorithm.

The main difficulty of the algorithm is finding  a discriminatory colouring . 
For example, bijections are trivially discriminatory, as nodes could deduce the set .
However, we cannot use such a colouring. Indeed, the colouring must use few colours, otherwise the advice would be too large.
We will be able to construct a discriminatory colouring with few colours  using the fact that the number of candidate nodes is bounded by a constant that depends only on , as given in the following lemma. 



\begin{lemma}\label{boundcandidates}
.
\end{lemma}
\begin{proof}
First note that  since the largest node  in  also has the largest label in . 
If , then  is at most the number of nodes in , i.e., at most , which is at most . Hence we may assume that .
Next, note that there cannot be two candidate nodes  such that the distance between them is at most . Indeed, if  and , then the node in  with smaller label will not be a candidate. Since the number of nodes in  is , it follows that the number of candidate nodes is less than . 
Since  and , it can be shown that  is at most . This completes the proof of the lemma.
\end{proof}



We define what it means for a colouring to be \emph{legal}. 
It will be shown that a legal colouring known by all nodes is discriminatory. Let . 


\begin{definition}
Let  denote the set of all -tuples of the form  such that  and . (We identify the tuple 
with the set .)

A colouring  of elements of  by integers is \emph{legal} if, for each colour  and each integer , either
\begin{enumerate}
\item every  that contains  and is coloured  has , or,
\item every  that contains  and is coloured  has .
\end{enumerate}
\end{definition}

Informally, a colouring of sets of labels is legal if,  for all sets of a given colour in which a label  appears,
 is either always the largest label or never the largest label. 

Assume that we have a legal colouring  of  that uses  colours, and that each node knows . Using , we provide a complete description of our algorithm with advice of size  and prove that it is correct. We will then describe such a legal colouring . 







\begin{center}
\fbox{
\begin{minipage}{0.8\columnwidth}\small

{\bf Advice Construction}, for fixed \\

Input: Ring  with diameter at most \\

1: \\
2: \\
3: v\lambda(\lfloor\alpha 2^{a_1}\rfloor,v)\\
4: \candidates{R}\\
5: \\
6: \ell_0 = \gamma_0\\ \hspace*{3.05cm}\textrm{and }\\
7: \\
8: Advice := 
\end{minipage}
}
\end{center}


\begin{center}
\fbox{
\begin{minipage}{0.8\columnwidth}\small

{\bf Algorithm} {\tt Select} at node  with label , for fixed \\

Input: Advice \\

1:\phantom{0} A_1\\
2:\phantom{0}  Acquire knowledge  using  communication rounds\\
3:\phantom{0}  \textbf{if }  is not the largest label in  \textbf{ then}\\
4:\phantom{0}  \hspace*{0.7cm} Output 0 and halt.\\
5:\phantom{0}  A_2\\
6:\phantom{0}  \\
7:\phantom{0}  \maxcandidatesXF(X) = a_2\\
8:\phantom{0}  \textbf{if } there is a tuple in  with first entry equal to  \textbf{ then}\\
9:\phantom{0}  \hspace*{0.7cm} Output 1 and halt.\\
10: \textbf{else}\\
11: \hspace*{0.7cm} Output 0 and halt.
\end{minipage}
}
\end{center}

In order to complete the description, it remains to construct a legal colouring 
that uses  colours. Note that, since  is polynomial in , that the number of colours is indeed .

Consider the following mapping  that maps each  to a -tuple . For each , set  to be the largest integer  such that there exists an integer in the range  that is divisible by . Next, take any bijection  between  and  the set . 
Define the colouring  of   as the composition of   and . Note that the colouring  uses at most  colours. 

\begin{lemma}\label{isLegal}
 is a legal colouring of the elements of .
\end{lemma}
\begin{proof}
Consider an arbitrary colour  and any integer . To obtain a contradiction, assume that: 
\begin{itemize}
\item there exists an element of , say , that contains , is coloured , and has , and,
\item there exists an element of , say , that contains , is coloured , and has .
\end{itemize}
Let .
Since  contains  and , it follows that  for some . By the definition of , we know that the range  contains an integer, say , that is divisible by , and we know that no integer in this range is divisible by . Moreover, since , we know that the range  contains an integer, say , that is divisible by , and we know that no integer in this range is divisible by . It follows that there are two distinct integers  in the range  that are divisible by  and that no integer in the range  is divisible by . Since  is the smallest integer greater than  that is divisible by , it follows that , so  is not divisible by . Note that, for some positive integer , we can write  and . Since neither  nor  is divisible by , it follows that both  and  must be odd, a contradiction.
\end{proof}


\begin{theorem}\label{ub}
Consider any fixed  and any positive integer . For any ring  with diameter at most ,
Algorithm {\tt Select} solves selection in the ring  
in time  and with advice of size .
\end{theorem}

\begin{proof}
Notice that Algorithm {\tt Select} halts within   communication rounds. Indeed, 
by line 2 of the algorithm, every node uses exactly  communication rounds, 
and . Next, note that since  is the binary representation of , the length of  is . Further, recall that  is the binary representation of a colour assigned by , which uses  colours where . Thus, the length of  is , and hence the size of advice is  .



Finally, we prove the correctness of the algorithm for an arbitrary ring . First, note that the construction of the advice string can indeed be carried out. In particular, at line 6 in the Advice Construction, the tuple  exists since, by Lemma \ref{boundcandidates}, the number of candidates (and, hence, the value of ) is bounded above by . Next, recall that  is the family of sets  of labels which contain all labels in  and no larger labels. The following claim shows that the colouring  is discriminatory.
 








\vspace{3mm}\noindent{\bf Claim 1 } 
\textit{For any , let . When lines 6 -- 11 of Algorithm {\tt Select} are executed, the largest node in  outputs 1, and all other nodes in  output 0. This proves that colouring  is discriminatory.}
\vspace{3mm}

To prove the claim, consider the largest node . We first show that  is equal to 's label. Indeed, since  has the largest label in , it follows that . Since  does not contain labels larger than those in ,  is equal to 's label.

Next, note that, by line 7, . Since  is equal to 's label, the condition of the \textbf{if} statement at line 8 evaluates to true, so  outputs 1. Next, consider any node  that is not the largest.
Since  is not equal to 's label and  is a legal colouring, it follows that no tuple that is coloured  has 's label as its first entry. In other words, no tuple in  has 's label as its first entry, so the \textbf{if} statement at line 8 evaluates to false, and  outputs 0. This completes the proof of the claim.

We have shown that, if the nodes in  are provided as advice the value of  for any , then lines 6 -- 11 of Algorithm {\tt Select} solve selection among the nodes in . It remains to show that the advice substring  created in the Advice Construction is indeed the binary representation of such an .

\vspace{3mm}\noindent{\bf Claim 2 } 
\textit{In Advice Construction, .}
\vspace{3mm}

To prove the claim, recall that, by line 4 of the Advice Construction,  is the decreasing sequence of labels of nodes in . Consider line 6 of the Advice Construction, and note that the largest element  of the tuple  is equal to , which proves that the labels  are no larger than those of nodes in . Further, since  and , it follows that  contains all of the labels of nodes in . Therefore, , as claimed.

We can now conclude that Algorithm {\tt Select} solves selection in the entire ring. Indeed, in lines 1 -- 4, each node that learns about the existence of a node with a larger label than itself outputs 0 and halts. The remaining nodes, namely those in  (which necessarily includes the largest node), proceed to lines 5 -- 11. From line 7 of the Advice Construction and by Claim 2, line 5 of Algorithm {\tt Select} assigns to  the value of  for some . Finally, Claim 1 shows that the largest node outputs 1 and all other nodes output 0.
\end{proof}

Corollary \ref{cor} and Theorem \ref{ub} imply the following tight bound on the size of advice.

\begin{corollary}
The optimal size of advice to complete selection in any ring in time linear in its diameter (and not exceeding it) is  .
\end{corollary} 




\subsection{Selection in time  where }

We now turn attention to very fast selection for rings of diameter . Since every such ring has size  linear in , and since  is polynomial in , selection (and even election) can be 
accomplished using  bits of advice without any communication by providing the largest label as advice.
It turns out that, even when the available time is , for any constant , this size of advice is necessary.  Compared to Theorem \ref{ub},
this shows that
selection in time  where , requires exponentially more advice
than selection in time .


\begin{theorem}
For any constant ,  any selection algorithm 
for rings of diameter  that works in time at most  requires  bits of advice. 
\end{theorem}
\begin{proof}
It is enough to consider sufficiently large values of .
Let , and let . We start by defining a special class of rings, and then proceed to show that algorithm  requires  bits of advice for this class. For the sake of clarity, we will use the notation  and .

First, we define a set of paths  as follows. Each path consists of  nodes with labels from , with the middle node of the path having the largest label. More specifically, path  is obtained by considering the path of nodes with labels , respectively, and reversing the order of the last  labels. Formally, for each , let  be the path of nodes whose labels form the sequence .

Next, we define a set  of rings . Each ring  will consist of the paths  along with enough nodes with labels from  to ensure that  has size . In order to define the ring , we first construct a path  by taking the paths , and, for each , connecting the last node of path  with the first node of path . More specifically,  is obtained by taking the union of the paths , and, for each , adding an edge between the nodes with labels  and . 
Next, we construct a path  whose labels form the sequence . 
Since the number of nodes in  is at most , which, for sufficiently large , is strictly less than , it follows that path  has at least one node.
Finally, the ring  is obtained by joining the paths  and . More specifically,  is obtained by taking the union of the paths  and  and adding the edges  and . The paths and rings constructed above are illustrated in Figure \ref{pathsrings}.

\begin{figure}[!ht]
\begin{center}
\includegraphics[scale=1]{pathsrings}
\end{center}
\caption{Graphs used in the construction of }
\label{pathsrings}
\end{figure}

The following claim asserts that, in any two rings in  that contain path  as a subgraph, the middle node of this path in both rings acquires the same knowledge 
when executing algorithm .


\vspace{3mm}\noindent{\bf Claim 1 } 
\textit{Consider any  and any . Let  be the node in  with label  and let  be the node in  with label . Then, knowledge  in  is equal to knowledge  in .}
\vspace{3mm}

To prove the claim, note that, in both  and , the node with label  is the middle node of path . Since the path  has length , it follows that, within  communication rounds, node  does not learn about any nodes in  outside of . Similarly, node  does not learn about any nodes in  outside of . This implies that knowledge  in  is equal to knowledge  in , which completes the proof of the claim.

We proceed to prove the theorem by way of contradiction. Assume that the number of bits of advice needed by algorithm  for the rings in  is less than . It follows that the number of distinct advice strings that are provided to algorithm  for the rings in  is strictly less than . However, since the class  consists of  rings (where  for sufficiently large ) this means that there exist two rings in , say  with , such that .

First, consider the execution of  by the nodes of ring . Let  be the node in  with label , and note that  is the largest node in . It follows that, in this execution, node  outputs 1. Next, consider the execution of  by the nodes of ring . Let  be the node in  with the label . By Claim 1,  in  is equal to  in . Moreover, 
algorithm  halts within  rounds and . It follows that the execution of  by  in  is identical to the execution of  by  in . Hence,  outputs 1 in the execution of  in . However, node  is not the largest in . Indeed, 's label is , whereas path  (and, hence, ring ) contains a node labeled . This contradicts the correctness of~.
\end{proof}

\begin{corollary}
For any constant ,
the optimal size of advice to complete selection in any ring of diameter at most  in time   is .
\end{corollary} 








\section{Conclusion}

We established tradeoffs between the time of choosing the largest node in a network and the amount of {\em a priori} information (advice) needed
to accomplish two variations of this task: election and selection. For the election problem, the tradeoff is complete and tight up to multiplicative constants
in the advice size.
Moreover, it holds for the class of arbitrary connected graphs.
For selection, our results are for the class of rings and a small gap remains in the picture. 
We proved that in rings with diameter  at most , 
the optimal size of advice is  if time is
 for any , and that it is  if time is at most .
Hence, the first open problem is to establish the optimal size of advice to perform selection for rings when the time is in the small remaining gap, for example, in time .  Another problem is to extend the tradeoff obtained for selection in rings to the class
of arbitrary connected graphs. In particular, it would be interesting to investigate whether the optimal advice needed to perform fast selection in graphs of size much larger
than the diameter depends on their size (like in the case of election) or on their diameter.

As noted in the introduction, all known leader election algorithms in labeled networks choose as leader either the node with the largest label or that with 
the smallest label. It is worth noting that the situation is not always symmetric here. For example, the Time Slice algorithm for leader election \cite{Ly} (which elects the
leader using exactly  messages in -node rings, at the expense of possibly huge time) finds the node
with the smallest label, and it does not seem to be possible to convert it directly to finding the node with the largest label. (Of course it is possible to first
find the node with the smallest label and then to use this leader to find the node with the largest label, but this takes additional time and communication.)
In our case, however, the situation is completely symmetric with respect to the order of labels: our results hold without change, if finding the largest node is replaced by finding the smallest. It is an open question whether they also remain valid if electing the largest node is replaced by general leader election (i.e., the task
in which a single {\em arbitrary} node becomes the leader, and all other nodes become non-leaders and also learn the identity of the leader) and if selecting the largest
node is replaced by general  leader selection  (i.e., the task
in which a single {\em arbitrary} node becomes the leader, and all other nodes become non-leaders). 
Our lower bound for election in time at most the diameter and our algorithms for selection use the ordering in an essential way, and it is not clear if smaller advice would be sufficient to elect or select some arbitrary node in a given time.


\pagebreak



\bibliographystyle{plain}
\begin{thebibliography}{12}
\bibitem{AKM01}
S.~Abiteboul, H.~Kaplan, T.~Milo, Compact labeling schemes for ancestor
queries, Proc. 12th Annual ACM-SIAM Symposium on Discrete
Algorithms (SODA 2001), 547--556.

\bibitem{An}
D.~Angluin, Local and global properties in networks of processors. 
Proc. 12th Annual ACM Symposium on Theory of Computing (STOC 1980), 82--93.


\bibitem{AtSn}
H. Attiya and M. Snir,
Better Computing on the Anonymous Ring,
{\em Journal of Algorithms} 12, (1991), 204-238.



\bibitem{ASW}
H. Attiya, M. Snir, and M. Warmuth,
Computing on an Anonymous Ring,
{\em Journal of the ACM} 35, (1988), 845-875.

\bibitem{BSVCGS}
P. Boldi, S. Shammah, S. Vigna, B. Codenotti, P. Gemmell, and J. Simon,
Symmetry Breaking in Anonymous Networks: Characterizations. 
{\em Proc. 4th Israel Symposium on Theory of Computing and Systems,
(ISTCS 1996}), 16-26.




\bibitem{BV}
P. Boldi and S. Vigna,
Computing Anonymously with Arbitrary Knowledge,
{\em Proc. 18th ACM Symp. on Principles of Distributed Computing (PODC 1999)}, 181-188.

\bibitem{B}
J.E. Burns, A Formal Model for Message Passing Systems,
{\em Tech. Report TR-91}, Computer Science Department,
Indiana University, Bloomington, September 1980.



\bibitem{CFP}
S. Caminiti, I. Finocchi, R. Petreschi,
Engineering tree labeling schemes: a case study on least common ancestor,
Proc. 16th Annual European Symposium on Algorithms (ESA 2008), 234--245.

\bibitem{C}
J. Chalopin,
Local Computations on Closed Unlabelled Edges: The Election Problem and the Naming Problem 
{\em Proc. 31st Conference on Current Trends in Theory and Practice of Computer Science (SOFSEM 2005)}, 82-91.

\bibitem{CM}
J. Chalopin and Y. M\'etivier,
Election and Local Computations on Edges.
{\em Proc. Foundations of Software Science and Computation Structures (FoSSaCS 2004)}, 90-104.



\bibitem{DP}
D. Dereniowski, A. Pelc, Drawing maps with advice,  Journal of Parallel and Distributed Computing 72 (2012), 132--143. 


\bibitem{EFKR}
Y. Emek, P. Fraigniaud, A. Korman, A. Rosen, Online computation with advice, Theoretical Computer Science 412 (2011), 2642--2656.




\bibitem{FGIP}
P. Fraigniaud, C. Gavoille, D. Ilcinkas, A. Pelc, 
Distributed computing with advice: Information sensitivity of graph coloring, 
Distributed Computing 21 (2009), 395--403.

\bibitem{FIP1}
P. Fraigniaud, D. Ilcinkas, A. Pelc, 
Communication algorithms with advice, Journal of  Computer and System Sciences 76 (2010), 222--232.


\bibitem{FIP2}
P. Fraigniaud, D. Ilcinkas, A. Pelc, 
Tree exploration with advice, Information and Computation 206 (2008), 1276--1287.


\bibitem{FKL}
P. Fraigniaud, A. Korman, E. Lebhar,
Local MST computation with short advice,
Theory of Computing Systems 47 (2010), 920--933.

\bibitem{FL}
G.N. Fredrickson and N.A. Lynch,
Electing a Leader in a Synchronous Ring,
{\em Journal of the ACM} 34 (1987), 98-115.

\bibitem{FP2}
E. Fusco, A. Pelc, How Much Memory is Needed for Leader Election, {\em Distributed Computing} 24 (2011), 65-78. 








\bibitem{FP}
E. Fusco, A. Pelc, Trade-offs between the size of advice and broadcasting time in trees, Algorithmica 60 (2011), 719--734. 


\bibitem{FPR}
E. Fusco, A. Pelc, R. Petreschi, Use knowledge to learn faster: Topology recognition with advice, Proc. 27th International Symposium on Distributed Computing (DISC 2013), 31-45.

\bibitem{GPPR02}
C.~Gavoille, D.~Peleg, S.~P\'{e}rennes, R.~Raz.
Distance labeling in graphs, 
Journal of Algorithms 53 (2004), 85-112.

   \bibitem{HKMMJ}
     M.A. Haddar, A.H. Kacem, Y. M\'{e}tivier, M. Mosbah, and M. Jmaiel,  Electing a Leader in the Local Computation Model using Mobile Agents.
{\em Proc.  6th ACS/IEEE International Conference on Computer Systems and Applications (AICCSA 2008}), 473-480.

\bibitem{HS}
D.S. Hirschberg, and J.B. Sinclair,
Decentralized Extrema-Finding in Circular Configurations of Processes,
{\em Communications of the ACM} 23 (1980), 627-628.





\bibitem{IKP}
D. Ilcinkas, D. Kowalski, A. Pelc, 
Fast radio broadcasting with advice, 
 Theoretical Computer Science, 411 (2012),  1544--1557.
 
 \bibitem{JKZ}
T. Jurdzinski, M. Kutylowski, and J. Zatopianski, 
Efficient Algorithms for Leader Election in~Radio Networks.
 {\em Proc., 21st ACM Symp. on Principles of Distributed Computing
(PODC 2002)}, 51-57.





\bibitem{KKKP02}
M.~Katz, N.~Katz, A.~Korman, D.~Peleg, Labeling schemes for flow and
connectivity, 
SIAM Journal of  Computing 34 (2004), 23--40.



\bibitem{KKP05}
A. Korman, S. Kutten, D. Peleg, Proof labeling schemes,
Distributed Computing 22 (2010), 215--233.  

\bibitem{KP}
D. Kowalski, and A. Pelc, Leader Election in Ad Hoc Radio Networks: A Keen Ear Helps, 
{\em Proc. 36th International Colloquium on Automata, Languages and Programming (ICALP 2009)}, 
{\em LNCS} 5556, 521-533. 



\bibitem{LL}
G. Le Lann,
Distributed Systems - Towards a Formal Approach,
{\em Proc. IFIP Congress}, 1977, 155--160, North Holland.




\bibitem{Ly}
N.L. Lynch, Distributed algorithms, Morgan Kaufmann Publ. Inc.,
San Francisco, USA, 1996.

\bibitem{NO}
K. Nakano and S. Olariu, Uniform Leader Election Protocols for Radio Networks,
{\em IEEE Transactions on Parallel and Distributed Systems} 13
(2002), 516-526.

\bibitem{SN}
N. Nisse, D. Soguet, Graph searching with advice,
Theoretical Computer Science 410 (2009), 1307--1318.



 \bibitem{Pe}D. Peleg,
  Distributed Computing, A Locality-Sensitive Approach,
  SIAM Monographs on Discrete Mathematics and Applications, Philadelphia 2000.
  
  \bibitem{P}
G.L. Peterson, An  Unidirectional Distributed Algorithm
for the Circular Extrema Problem,
{\em ACM Transactions on Programming Languages and Systems} 4 (1982), 758-762.








\bibitem{TZ05}
M.~Thorup, U.~Zwick, Approximate distance oracles,
Journal of the ACM, 52 (2005), 1--24.

\bibitem{Wil}
D.E. Willard, 
Log-logarithmic Selection Resolution Protocols in a Multiple Access Channel,
{\em SIAM J. on Computing} 15 (1986), 468-477. 


\bibitem{YK2}
M. Yamashita and T. Kameda,
Electing a Leader when Procesor Identity Numbers are not Distinct,
{\em Proc. 3rd Workshop on Distributed Algorithms (WDAG 1989)},
{\em LNCS} 392, 303-314.

\bibitem{YK3}
M. Yamashita and T. Kameda,
Computing on Anonymous Networks: Part I - Characterizing the Solvable Cases,
{\em IEEE Trans. Parallel and Distributed Systems} 7 (1996), 69-89. 









\end{thebibliography}


\end{document}
