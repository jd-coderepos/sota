\section{Experiments}
\label{sec:experiment}




\begin{table}[tb]
    \centering
        \caption{Collection statistics. (There are 43 test queries in DL'19 and 45 test  queries in DL'20.)}
        \resizebox{.45\textwidth}{!}{
    \begin{tabular}{cccc} \toprule
    Collection & \# Queries & \# Documents & \# tokens / doc \\ \hline
      Robust04 & 249 & 0.5M & 0.7K \\
      GOV2     & 149 & 25M  & 3.8K \\ 
      Genomics & 64  & 162K    & 6.5K    \\ 
      MSMARCO    & 43/45  & 3.2M & 1.3K \\
     ClueWeb12-B13 & 80 & 52M   & 1.9K \\ \bottomrule
    \end{tabular}}

    \label{tab.stats}
\end{table}



\begin{table*}[tb] 
\centering
    \caption{Ranking effectiveness of PARADE on the {\it Robust04} and {\it GOV2} collection. 
    Best performance is in bold. 
    Significant difference between \PARADE{Transformer} and the corresponding method is marked with $\dagger$ ($p < 0.05$, two-tailed paired $t$-test). We also report the current best-performing model on Robust04 (T5-3B from~\cite{DBLP:journals/corr/abs-2003-06713}).}
    \resizebox{\textwidth}{!}{
\begin{tabular}{l|lll|lll|lll|lll} \toprule
& \multicolumn{3}{c}{Robust04 Title} & \multicolumn{3}{c}{Robust04 Description} & \multicolumn{3}{c}{GOV2 Title} & \multicolumn{3}{c}{GOV2 Description}\\ 
& MAP & P@20 & nDCG@20 & MAP & P@20 & nDCG@20 & MAP & P@20 & nDCG@20 & MAP & P@20 & nDCG@20 \\ \midrule 
BM25               & 0.2531$^{\dagger}$ &	0.3631$^{\dagger}$ &	0.4240$^{\dagger}$ &	0.2249$^{\dagger}$ &	0.3345$^{\dagger}$ &	0.4058$^{\dagger}$ & 0.3056$^{\dagger}$ & 0.5362$^{\dagger}$ & 	0.4774$^{\dagger}$ & 	0.2407$^{\dagger}$ & 	0.4705$^{\dagger}$ & 	0.4264$^{\dagger}$  \\ 
BM25+RM3           & 0.3033$^{\dagger}$ &	0.3974$^{\dagger}$ &	0.4514$^{\dagger}$ &	0.2875$^{\dagger}$ &	0.3659$^{\dagger}$ &	0.4307$^{\dagger}$ & 0.3350$^{\dagger}$ & 0.5634$^{\dagger}$ & 0.4851$^{\dagger}$ & 0.2702$^{\dagger}$ & 0.4993$^{\dagger}$ & 0.4219$^{\dagger}$   \\ \midrule
Birch              & 0.3763 & 0.4749$^{\dagger}$ & 0.5454$^{\dagger}$ & 0.4009$^{\dagger}$ & 0.5120$^{\dagger}$ & 0.5931$^{\dagger}$ & 0.3406$^{\dagger}$ & 0.6154$^{\dagger}$ & 0.5520$^{\dagger}$ & 0.3270 & 0.6312$^{\dagger}$ & 0.5763$^{\dagger}$\\ 
ELECTRA-MaxP       & 0.3183$^{\dagger}$ & 0.4337$^{\dagger}$ & 0.4959$^{\dagger}$ & 0.3464$^{\dagger}$ & 0.4731$^{\dagger}$ & 0.5540$^{\dagger}$ & 0.3193$^{\dagger}$ & 0.5802$^{\dagger}$ & 0.5265$^{\dagger}$ & 0.2857$^{\dagger}$ & 0.5872$^{\dagger}$ & 0.5319$^{\dagger}$\\
T5-3B (from~\cite{DBLP:journals/corr/abs-2003-06713})&-&-&-&0.4062&-&0.6122&-&-&-&-&-&-\\
ELECTRA-KNRM       & 0.3673$^{\dagger}$ & 0.4755$^{\dagger}$ & 0.5470$^{\dagger}$ & 0.4066 & {\bf 0.5255} & 0.6113 & 0.3469$^{\dagger}$ & 0.6342$^{\dagger}$ & 0.5750$^{\dagger}$ & 0.3269 & 0.6466 & 0.5864$^{\dagger}$\\
CEDR-KNRM (Max)    & 0.3701$^{\dagger}$ & 0.4769$^{\dagger}$ & 0.5475$^{\dagger}$ & 0.3975$^{\dagger}$ & 0.5219 & 0.6044$^{\dagger}$ & 0.3481$^{\dagger}$ & 0.6332$^{\dagger}$ & 0.5773$^{\dagger}$ & {\bf 0.3354}$^{\dagger}$ & 0.6648 & 0.6086 \\
\midrule
PARADE-Avg         & 0.3352$^{\dagger}$ & 0.4464$^{\dagger}$ & 0.5124$^{\dagger}$ & 0.3640$^{\dagger}$ & 0.4896$^{\dagger}$ & 0.5642$^{\dagger}$ & 0.3174$^{\dagger}$ & 0.6225$^{\dagger}$ & 0.5741$^{\dagger}$ & 0.2924$^{\dagger}$ & 0.6228$^{\dagger}$ & 0.5710$^{\dagger}$\\
PARADE-Sum         & 0.3526$^{\dagger}$ & 0.4711$^{\dagger}$ & 0.5385$^{\dagger}$ & 0.3789$^{\dagger}$ & 0.5100$^{\dagger}$ & 0.5878$^{\dagger}$ & 0.3268$^{\dagger}$ & 0.6218$^{\dagger}$ & 0.5747$^{\dagger}$ & 0.3075$^{\dagger}$ & 0.6436$^{\dagger}$ & 0.5879$^{\dagger}$ \\
PARADE-Max         & 0.3711$^{\dagger}$ & 0.4723$^{\dagger}$ & 0.5442$^{\dagger}$ & 0.3992$^{\dagger}$ & 0.5217 & 0.6022 & 0.3352$^{\dagger}$ & 0.6228$^{\dagger}$ & 0.5636$^{\dagger}$ & 0.3160$^{\dagger}$ & 0.6275$^{\dagger}$ & 0.5732$^{\dagger}$\\
PARADE-Attn        & 0.3462$^{\dagger}$ & 0.4576$^{\dagger}$ & 0.5266$^{\dagger}$ & 0.3797$^{\dagger}$ & 0.5068$^{\dagger}$ & 0.5871$^{\dagger}$ & 0.3306$^{\dagger}$ & 0.6359$^{\dagger}$ & 0.5864$^{\dagger}$ & 0.3116$^{\dagger}$ & 0.6584 & 0.5990\\
PARADE-CNN         & {\bf 0.3807 } & 0.4821$^{\dagger}$ & 0.5625 & 0.4005$^{\dagger}$ & 0.5249 & 0.6102 & 0.3555$^{\dagger}$ & 0.6530 & 0.6045 & 0.3308 & {\bf 0.6688} & {\bf 0.6169}\\
PARADE-Transformer & 0.3803 & {\bf 0.4920} & {\bf 0.5659} & {\bf 0.4084} & {\bf 0.5255} & {\bf 0.6127} & {\bf 0.3628} & {\bf 0.6651} & {\bf 0.6093} & 0.3269 & 0.6621 & 0.6069 \\ \bottomrule
\end{tabular}}
\label{tb.main_result}
\end{table*}











\subsection{Datasets} \label{sec:PARADE_dataset}

We experiment with several ad-hoc ranking collections.
Robust04\footnote{\url{https://trec.nist.gov/data/qa/T8_QAdata/disks4_5.html}} is a newswire collection used by the TREC 2004 Robust track.
GOV2\footnote{\url{http://ir.dcs.gla.ac.uk/test_collections/gov2-summary.htm}} is a web collection crawled from US government websites used in the TREC Terabyte 2004--06 tracks.
For Robust04 and GOV2, we consider both keyword (title) queries and description queries in our experiments.
The Genomics dataset~\cite{Hersh2006TrecGenomics,Hersh2007TrecGenomics} consists of scientific articles from the Highwire Press\footnote{\url{https://www.highwirepress.com/}} with natural-language queries about specific genes, and was used in the TREC Genomics 2006--07 track.
The MSMARCO document ranking dataset\footnote{\url{https://microsoft.github.io/TREC-2019-Deep-Learning}} is a large-scale collection and is used in TREC 2019--20 Deep Learning Tracks~\cite{DBLP:conf/trec/CraswellMYCV19,DBLP:conf/trec/CraswellMYC20}.
To create document labels for the development and training sets, passage-level labels from the MSMARCO passage dataset are transferred to the corresponding source document that contained the passage.
In other words, a document is considered relevant as long as it contains a relevant passage, and each query can be satisfied by a single passage.
The ClueWeb12-B13 dataset\footnote{\url{http://lemurproject.org/clueweb12/}} is a large-scale collection crawled from the web between February 10, 2012 and May 10, 2012. 
It is used for the NTCIR We Want Web 3 (WWW-3) Track~\cite{WWW-3}.
The statistics of these datasets are shown in Table~\ref{tab.stats}.
Note that the average document length is obtained only from the documents returned by BM25. 
Documents in GOV2 and Genomics are much longer than Robust04, making it more challenging to train an end-to-end ranker.







\subsection{Baselines}
We compare PARADE against the following traditional and neural baselines, including those that employ other passage aggregation techniques.

{\bf BM25} 
is  an  unsupervised  ranking  model  based  on  IDF-weighted counting~\cite{DBLP:conf/trec/RobertsonWHGP95}.
The documents retrieved by BM25 also serve as the candidate documents used with reranking methods.

{\bf BM25+RM3} is a query expansion model based on RM3~\cite{DBLP:conf/sigir/LavrenkoC01}.
We used Anserini's~\cite{DBLP:journals/jdiq/YangFL18} implementations of BM25 and BM25+RM3.
Documents are indexed and retrieved with the default settings for keywords queries.
For description queries, we set $b=0.6$ and changed the number of expansion terms to 20.


{\bf Birch} 
aggregates sentence-level evidence provided by BERT to rank documents~\cite{DBLP:conf/emnlp/YilmazWYZL19}.
Rather than using the original Birch model provided by the authors, we train an improved ``Birch-Passage'' variant.
Unlike the original model, Birch-Passage uses passages rather than sentences as input, it is trained end-to-end, it is fine-tuned on the target corpus rather than being applied zero-shot, and it does not interpolate retrieval scores with the first-stage retrieval method.
These changes bring our Birch variant into line with the other models and baselines (e.g., using passages inputs and no interpolating), and they additionally improved effectiveness over the original Birch model in our pilot experiments.

{\bf ELECTRA-MaxP}
adopts the maximum score of passages within a document as an overall relevance score~\cite{DBLP:conf/sigir/DaiC19}.
However, rather than fine-tuning BERT-base on a Bing search log, we improve performance by fine-tuning on the MSMARCO passage ranking dataset.
We also use the more recent and efficient pre-trained ELECTRA model rather than BERT.

{\bf ELECTRA-KNRM} 
is a kernel-pooling neural ranking model based on query-document similarity matrix~\cite{DBLP:conf/sigir/XiongDCLP17}.
We set the kernel size as 11.
Different from the original work, we use the embeddings from the pre-trained ELECTRA model for model initialization.

{\bf CEDR-KNRM (Max)}
combines the advantages from both KNRM and pre-trained model~\cite{DBLP:conf/sigir/MacAvaneyYCG19}.
It digests the kernel features learned from KNRM and the \texttt{[CLS]} representation as ranking feature. We again replace the BERT model with the more effective ELECTRA.
We also use a more effective variant that performs max-pooling on the passages' \texttt{[CLS]} representations, rather than averaging.

{\bf T5-3B}
defines text ranking in a sequence-to-sequence generation context using the pre-trained T5 model~\cite{DBLP:journals/corr/abs-2003-06713}.
For document reranking task, it utilizes the same score max-pooling technique as in BERT-MaxP~\cite{DBLP:conf/sigir/DaiC19}. Due to its large size and expensive training, we present the values reported by~\cite{DBLP:journals/corr/abs-2003-06713} in their zero-shot setting, rather than training it ourselves.

\subsection{Training} \label{sec:training}
To prepare the ELECTRA model for the ranking task, we first fine-tune ELECTRA on the MSMARCO passage ranking dataset~\cite{msmarco}.
The fine-tuned ELECTRA model is then used to initialize PARADE's PLM component.
For \PARADE{Transformer} we use two randomly initialized transformer encoder layers with the same hyperparemeters  (e.g., number of attention heads, hidden size, etc.) used by BERT-base.
Training of PARADE and the baselines was performed on a single Google TPU v3-8 using a pairwise hinge loss.
We use the Tensorflow implementation of PARADE available in the Capreolus toolkit~\cite{yates2020capreolus}, and a standalone imiplementation is also available\footnote{\url{https://github.com/canjiali/PARADE/}}.
We train on the top 1,000 documents returned by a first-stage retrieval method; documents that are labeled relevant in the ground-truth are taken as positive samples and all other documents serve as negative samples.
We use BM25+RM3 for first-stage retrieval on Robust04 and BM25 on the other datasets with parameters tuned on the dev sets via grid search.
We train for 36 ``epochs'' consisting of 4,096 pairs of training examples with a learning rate of 3e-6, warm-up over the first ten epochs, and a linear decay rate of 0.1 after the warm-up.
Due to its larger memory requirements, we use a batch size of 16 with CEDR and a batch size of 24 with all other methods.
Each instance comprises a query and all split passages in a document.
We use a learning rate of 3e-6 with warm-up over the first 10 proportions of training steps.


Documents are split into a maximum of 16 passages. 
As we split the documents using a sliding window of 225 tokens with a stride of 200 tokens, a maximum number of 3,250 tokens in each document are retained.
The maximum passage sequence length is set as 256.
Documents with fewer than the maximum number of passages are padded and later masked out by passage level masks.
For documents longer than required, the first and last passages are always kept while the remaining are uniformly sampled as in~\cite{DBLP:conf/sigir/DaiC19}.

\begin{table}[tb]
\centering
    \caption{Ranking effectiveness on the {\it Genomics} collection. Significant difference between \PARADE{Transformer} and the corresponding method is marked with $\dagger$ ($p < 0.05$, two-tailed paired $t$-test). The top neural results are listed in bold, and the top overall scores are underlined.}
    \label{tab:genomics}
\begin{tabular}{l|lll} \toprule
& MAP & P@20 & nDCG@20  \\ \midrule
BM25                & 0.3108 & 0.3867 & 0.4740 \\
TREC Best           & \underline{0.3770} & \underline{0.4461} & \underline{0.5810} \\ \midrule
Birch               & 0.2832 & 0.3711 & 0.4601 \\
BioBERT-MaxP        & 0.2577 & 0.3469 & 0.4195$^{\dagger}$ \\
BioBERT-KNRM        & 0.2724 & 0.3859 & 0.4605 \\
CEDR-KNRM (Max)     & 0.2486 & 0.3516$^{\dagger}$ & 0.4290 \\
\midrule
PARADE-Avg          & 0.2514$^{\dagger}$ & 0.3602 & 0.4381 \\
PARADE-Sum          & 0.2579$^{\dagger}$ & 0.3680 & 0.4483 \\
PARADE-Max          &\bf0.2972 &\bf0.4062$^{\dagger}$ & \bf 0.4902 \\
PARADE-Attn         & 0.2536$^{\dagger}$ & 0.3703 & 0.4468 \\
PARADE-CNN          & 0.2803 & 0.3820 & 0.4625 \\
PARADE-Transformer  & 0.2855 & 0.3734 & 0.4652 \\ \bottomrule
\end{tabular}
\end{table}


\subsection{Evaluation}
Following prior work~\cite{DBLP:conf/sigir/DaiC19, DBLP:conf/sigir/MacAvaneyYCG19}, we use 5-fold cross-validation.
We set the reranking threshold to 1000 on the test fold as trade-off between latency and effectiveness.
The reported results are based on the average of all test folds.
Performance is measured in terms of the MAP, Precision, ERR and nDCG ranking metrics using \texttt{trec\_eval}\footnote{\url{https://trec.nist.gov/trec_eval}} with different cutoff.
For NTCIR WWW-3, the results are reported using \texttt{NTCIREVAL} \footnote{\url{http://research.nii.ac.jp/ntcir/tools/ntcireval-en.html}}.

\subsection{Main Results}
\label{sec:results}

The reranking effectiveness of PARADE on the two commonly-used Robust04 and GOV2 collections is shown in Table~\ref{tb.main_result}.
Considering the three approaches that do not introduce any new weights, \PARADE{Max} is usually more effective than \PARADE{Avg} and \PARADE{Sum}, though the results are mixed on GOV2.
\PARADE{Max} is consistently better than \PARADE{Attn} on Robust04, but \PARADE{Attn} sometimes outperforms \PARADE{Max} on GOV2.
The two variants that consume passage representations in a hierarchical manner, \PARADE{CNN} and \PARADE{Transformer}, consistently outperforms the four other variants.
This confirms the effectiveness of our proposed passage representation aggregation approaches.

Considering the baseline methods, \PARADE{Transformer} significantly outperforms the Birch and ELECTRA-MaxP score aggregation approaches for most metrics on both collections.
\PARADE{Transformer}'s ranking effectiveness is comparable with T5-3B on the Robust04 collection while using only $4\%$ of the parameters, though it is worth noting that T5-3B is being used in a zero-shot setting.
CEDR-KNRM and ELECTRA-KNRM, which both use some form of representation aggregation, are significantly worse than \PARADE{Transformer} on title queries and have comparable effectiveness on description queries.
Overall, \PARADE{CNN} and \PARADE{Transformer} are consistently among the most effective approaches, which suggests the importance of performing complex representation aggregation on these datasets.






Results on the Genomics dataset are shown in Table~\ref{tab:genomics}. We first observe that this is a surprisingly challenging task for neural models. Unlike Robust04 and GOV2, where transformer-based models are clearly state-of-the-art, we observe that all of the methods we consider almost always underperform a simple BM25 baseline, and they perform well below the best-performing TREC submission.
It is unclear whether this is due to the specialized domain, the smaller amount of training data, or some other factor. 
Nevertheless, we observe some interesting trends.
First, we see that PARADE approaches can outperform score aggregation baselines. However, we note that statistical significance can be difficult to achieve on this dataset, given the small sample size (64 queries). Next, we notice that \PARADE{Max} performs the best among neural methods. This is in contrast with what we observed on Robust04 and GOV2, and suggests that hierarchically aggregating evidence from different passages is not required on the Genomics dataset.

\subsection{Results on the TREC DL Track and NTCIR WWW-3 Track}\label{sec:dlwww}

We additionally study the effectiveness of PARADE on the TREC DL Track and NTCIR WWW-3 Track.
We report results in this section and refer the readers to the TREC and NTCIR task papers for details on the specific hyperparameters used~\cite{li2020ntcir,li2020trec}.


\begin{table}[tb]
    \centering
        \caption{Ranking effectiveness on TREC DL Track document ranking task.
        PARADE's best result is in bold. 
        The top overall result of of each track is underlined.}
\begin{tabular}{cllcc}\toprule
Year & Group  & Runid                & MAP   & nDCG@10 \\ \hline
\multirow{6}{*}{2019} 
&\multirow{4}{*}{TREC} 
&BM25                 & 0.237 & 0.517   \\
&&ucas\_runid1~\cite{DBLP:conf/trec/ChenLHS19}         & 0.264 & 0.644   \\
&&TUW19-d3-re~\cite{DBLP:conf/trec/HofstatterZH19}          & 0.271 & 0.644   \\
&&idst\_bert\_r1~\cite{DBLP:conf/trec/YanLWBWXS19}       & \underline{0.291} & \underline{0.719}  \\ \cmidrule{2-5}
&\multirow{2}{*}{Ours}
&\PARADE{Max}        & {\bf 0.287} & {\bf 0.679}   \\
&&\PARADE{Transformer} & 0.274 & 0.650   \\ \bottomrule
\multirow{7}{*}{2020} 
&\multirow{5}{*}{TREC} 
&    BM25           & 0.379 & 0.527  \\
&&bcai\_bertb\_docv  & 0.430 & 0.627\\ 
&&fr\_doc\_roberta   & 0.442 & 0.640 \\
&&ICIP\_run1         & 0.433 & 0.662  \\
&&    d\_d2q\_duo    & \underline{0.542} & \underline{0.693} \\\cmidrule{2-5}
&\multirow{2}{*}{Ours}    
& \PARADE{Max}      & \textbf{0.420}   & \textbf{0.613}   \\
&& \PARADE{Transformer}  & 0.403   & 0.601    \\ \bottomrule

\end{tabular}
    \label{tab.DL_Doc}
\end{table}

Results from the TREC Deep Learning Track are shown in Table~\ref{tab.DL_Doc}.
In TREC DL'19, we include comparisons with competitive runs from TREC:
{\tt ucas\_runid1}~\citep{DBLP:conf/trec/ChenLHS19} used BERT-MaxP~\citep{DBLP:conf/sigir/DaiC19} as the reranking method,
{\tt TUW19-d3-re}~\citep{DBLP:conf/trec/HofstatterZH19} is a Transformer-based non-BERT method, and
{\tt idst\_bert\_r1}~\citep{DBLP:conf/trec/YanLWBWXS19} utilizes structBERT~\cite{DBLP:conf/iclr/0225BYWXBPS20}, which is intended to strengthen the modeling of sentence relationships.
All PARADE variants outperform {\tt ucas\_runid1} and {\tt TUW19-d3-re} in terms of nDCG@10, but cannot outperform {\tt idst\_bert\_r1}.
Since this run's pre-trained structBERT model is not publicly available, we are not able to embed it into PARADE and make a fair comparison.
In TREC DL'20, the best TREC run {\tt d\_d2q\_duo} is a T5-3B model.
Moreover, \PARADE{Max} again outperforms \PARADE{Transformer}, which is in line to the Genomics results and in contrast to results on Robust04 and GOV2.
contrast to the previous result in Table~\ref{tb.main_result}.
We explore this further in Section~\ref{sec.bias}.

Results from the NTCIR WWW-3 Track are shown in Table~\ref{tab:run_ntcir}.
{\tt KASYS-E-CO-NEW-1} is a Birch-based method~\cite{DBLP:conf/emnlp/YilmazWYZL19} that uses BERT-Large and {\tt Technion-E-CO-NEW-1} is a cluster-based method.
As shown in Table~\ref{tab:run_ntcir}, \PARADE{Transformer}'s effectiveness is comparable with {\tt KASYS-E-CO-NEW-1} across metrics.
On this benchmark, \PARADE{Transformer} outperforms \PARADE{Max} by a large margin.




\begin{table}[tb]
    \centering
        \caption{Ranking effectiveness of PARADE on NTCIR WWW-3 task. 
        PARADE's best result is in bold. 
        The best result of the Track is underlined.}
    \begin{tabular}{lccc} \toprule
     Model       & nDCG@10  & Q@10   & ERR@10  \\ \hline
     BM25        & 0.5748.  & 0.5850 & 0.6757   \\ 
     Technion-E-CO-NEW-1 & 0.6581 &0.6815 & 0.7791   \\
KASYS-E-CO-NEW-1 & \underline{0.6935}  & \underline{0.7123} & 0.7959  \\
\hline
 \PARADE{Max}    & 0.6337   & 0.6556 & 0.7395   \\
 \PARADE{Transformer}          & {\bf 0.6897}   & {\bf 0.7016} & \underline{{\bf 0.8090}}   \\ \bottomrule
    \end{tabular}
    \label{tab:run_ntcir}
\end{table}
