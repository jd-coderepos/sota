




\documentclass[journal]{IEEEtran}

\usepackage{amssymb}
\usepackage{cite}
\usepackage{color}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{epstopdf}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algorithmic}
\ifCLASSINFOpdf
\else
\fi
\usepackage{amsmath}
\usepackage{algorithmic}
\usepackage{array}
\usepackage{url}
\usepackage{enumitem}



\DeclareMathOperator*{\argmin}{arg\,min}
\graphicspath{ {./Figures/} }

\newtheorem{theorem}{Theorem}
\newtheorem{condition}{Condition}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{assumption}{Assumption}
\newtheorem{scheme}{Scheme}
\newtheorem{example}{Example}


\newcommand{\ba}{\begin{array}}
\newcommand{\ea}{\end{array}}
\newcommand{\be}{}
\newcommand{\upcite}[1]{\textsuperscript{\textsuperscript{\cite{#1}}}}
\newcommand{\RNum}[1]{\lowercase\expandafter{\romannumeral #1\relax}}
\newcommand{\RNumU}[1]{\uppercase\expandafter{\romannumeral #1\relax}}
\newcommand{\citealtt}[1]{\citeauthor{#1},\citeyear{#1}}
\newcommand{\myycite}[1]{\citep{#1}}
\newcommand{\captionfonts}{\normalsize}


\def\R{\mathbb{R}}
\def\Rm{\mathbb{R}^{m\times n}}
\def\A{{\bf A}}
\def\B{{\bf B}}
\def\C{{\bf C}}
\def\D{{\bf D}}
\def\E{{\bf E}}
\def\H{{\bf H}}
\def\I{{\bf I}}
\def\M{{\bf M}}

\def\P{{\bf P}}
\def\Q{{\bf Q}}
\def\T{{\mathbb T}}
\def\U{{\bf U}}
\def\V{{\bf V}}
\def\W{{\bf W}}
\def\X{{\bf X}}
\def\Y{{\bf Y}}
\def\Z{{\bf Z}}

\def\CU{{\boldsymbol{\mathcal U}}}
\def\CV{{\boldsymbol{\mathcal V}}}
\def\CW{{\boldsymbol{\mathcal W}}}

\def\a{{\bf a}}
\def\am{{\phi_{\bf a}}}
\def\aim{{\phi_{\bf A_{:i}}}}
\def\b{{\bf b}}
\def\e{{\bf e}}
\def\hd{{\rm hdmx}}
\def\t{{\bf t}}
\def\u{{\bf u}}
\def\um{ {\Gamma_{\bf u}}}
\def\v{{\bf v}}
\def\w{{\bf w}}
\def\x{{\bf x}}
\def\y{{\bf y}}
\def\ytm{ {\Gamma_{\bf \y_t}}}
\def\z{{\bf z}}

\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\title{\huge 0/1 Deep Neural Networks via Block Coordinate Descent
}





\author{Hui Zhang, Shenglong Zhou, Geoffrey Ye Li, Naihua Xiu\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem This work is supported by the National Natural Science Foundation of China (No.12131004, No.11971052).
\IEEEcompsocthanksitem  H. Zhang is with School of Management Science, Qufu Normal University, Rizhao Shandong,
China.  S.L. Zhou and N.H Xiu are with the School of Mathematics and Statistics, Beijing Jiaotong University, China. G.Y. Li is with ITP Lab, Department of EEE, Imperial College London, United Kingdom (e-mail: 18118011@bjtu.edu.cn; slzhou2021@163.com; geoffrey.li@imperial.ac.uk; nhxiu@bjtu.edu.cn;).}}









\markboth{Journal of \LaTeX\ Class Files,~Vol.~ , No.~ ,  ~ }{Shell \MakeLowercase{{\em et al.}}: 0/1 Deep Neural Networks via Block Coordinate Descent}














\maketitle

\begin{abstract}
The step function is one of the simplest and most natural activation functions for deep neural networks (DNNs). As it counts 1 for positive variables and 0 for others, its intrinsic characteristics (e.g., discontinuity and no viable information of subgradients) impede its development for several decades. Even if there is an impressive body of work on designing DNNs with continuous activation functions that can be deemed as surrogates of  the step function, it is still in the possession of some advantageous properties, such as robustness to outliers. Hence,  in this paper, we aim to train DNNs with the step function used as an activation function (dubbed as 0/1 DNNs). We first reformulate 0/1 DNNs as an unconstrained optimization problem and then solve it by a  block coordinate descend (BCD) method. Moreover, we acquire closed-form solutions for sub-problems of BCD as well as its convergence properties. Furthermore, we also integrate -regularization into 0/1 DNN to accelerate the training process and compress the network scale.As a result, the proposed algorithm has a desirable performance on classifying MNIST, FashionMNIST, Cifar10, and Cifar100 datasets.
\end{abstract}
\begin{IEEEkeywords}
0/1 DNNs,  -regularization,  BCD,  convergence analysis,  numerical experiment
\end{IEEEkeywords}




\IEEEpeerreviewmaketitle


\section{Introduction}
\IEEEPARstart{D}{NN}s can be traced back to the Mcculloch-Pitts (M-P) Neuron \cite{Mcculloch1943}, where the response of a nerve cell follows an ``all-or-none" law \cite{Britannica2019}, that is, the strength of a nerve cell's response is independent of the strength of the stimulus. If a stimulus is above a certain threshold, a nerve will fire. Essentially, there will be either a full response or no response at all for an individual neuron. Such a phenomenon can be characterized by the step function, which returns  if the variable is positive and  otherwise. Therefore, in this paper, we aim to investigate DNNs where the step function is used as an activation function (dubbed as 0/1 activation) on neurons.
\subsection{0/1 DNNs with -regularization}
Now we introduce -layer neural networks with  hidden layers and  the 0/1  activation. Specifically, let  be the number of hidden units of the -th hidden layer for , where  for a positive integer . Let  and  represent the number of input and output units. Denote  with  being the weight matrix between the th layer and the th layer. Now we are given a set of input and output data , where ,  is the batch size of samples and  is a one-hot vector consisting of 0s in all cells with the exception of a single 1 in a cell. The optimization model of 0/1  DNNs for classification problems can be built as follows,
\be\label{OP}
 \arraycolsep=1.4pt\def\arraystretch{1.25}
\ba{cl}
\min\limits_{\CW} &\frac{1}{2N}\sum_{s=1}^N \| \y_s-{\widetilde \y}_s\|^{2} +   g(\CW) \\
{\rm s.t.}& {\widetilde \y}_s=(\W_h(\cdots(\W_2(\W_1\x_s)_{0/1})_{0/1}\cdots)_{0/1})_{\hd},\\
&s\in[N],
\ea
\ee
where  represents the Euclidean norm,  is a sparsity-induced regularized function. In this paper, we focus on the -regularization in  for the purpose of getting neural networks with an adaptive width, namely,  

Here,  and  are positive constants and  counts the number of nonzero columns of matrix . A column is said to be nonzero if it has at least one nonzero entry. For step function  and hardmax  ,  their th entries are respectively defined by

In the sequel,  the step function is also called the 0/1 function. 
\subsection{Prior Arts} \label{Section-relatedlectures}
To the best of our knowledge, there is no study dedicated exclusively to the optimization analysis of 0/1 DNNs with an -regularization. Currently, an impressive body of work has developed various surrogates of the step function as  activation functions. Based on them, a number of algorithms have been cast to train different structured neural networks. In this section, we conduct a brief review of  popular activation functions and relevant algorithms.

{\bf a) Activation functions} in DNNs play a key role in neural networks so it becomes fundamental to understand their advantages and disadvantages to achieve better performance. One of the simplest and most natural activation functions for DNNs is the step function, which is also regarded as the ideal one in \cite{zhouzhihua}. However, this function is discontinuous and its subgradients vanish almost everywhere. As a consequence, the traditional gradient-driven backpropagation algorithms \cite{Rumelhart1986} cannot be used to train the multilayer networks with the 0/1 activation. Therefore, a plenty of
continuous surrogates have been introduced to approximate it.  We show five popular ones in addition to the step function in Fig. \ref{figureactivation}. One of the widely used surrogates is the Sigmoid function \cite{Han1995}, which was the default activation function used on neural networks for a long time. Another activation function is hyperbolic tangent (Tanh) with a similar shape to Sigmoid.  However, both Sigmoid and Tanh saturate across most of their domain \cite{Goodfellow2016}.
 Since introduced in DNNs \cite{Nair2012}, the Rectifier Linear Unit (ReLU) has quickly gained its popularity due to its desirable features, such as being piecewise linear and avoiding easy saturation. We note that ReLU DNNs are usually learned by the gradient-based algorithms, the family of backpropagation algorithms. However, similar to 0/1 DNNs, ReLU DNNs also involve the problem of ``vanishing gradients" that may prevent backpropagation algorithms from training desired networks. Then SELU (scaled exponential linear unit \cite{Klambauer2017}) and Swish \cite{Ramachandran2017} have been designed to enable very deep neural networks as the issue of vanishing gradients can be  eliminated. Some other popular activation functions include the leaky ReLU \cite{Maas2013},  parametric ReLU \cite{He2015},  ELUs \cite{Clevert2015}, and those in \cite{Goodfellow2016}.

\begin{figure*}[!th]
\centering
\includegraphics[width=.85\textwidth]{activations.pdf}
\caption{Six activation functions.}\vspace{-5mm}
\label{figureactivation}
\end{figure*}

{  A separate line of research aims at exploiting the 1-bit
activation functions, such as the sign and binarization  functions,  resulting in various binary neural networks (BNNs). These BNNs also use 1-bit weights. Hence,  they are capable of compressing networks for extreme computational and storage efficiency. However, the discontinuity of 1-bit activation functions makes it enormous difficult to train the BNNs. To overcome this drawback, the popular approach is to approximate these functions or their (sub)gradient. For example,   the authors in \cite{Hinton2012} first proposed a straight-through empirical (STE) gradient estimator approach to train BNNs. The main idea of this method is to approximate the sub-gradient of the sign function by the sub-gradient of the hard tanh function, which enables gradient-driven algorithms to train the networks.  Similar ideas were then extensively adopted by \cite{zhong2012sensitivity,courbariaux2015,Tang2017,Alizadeh2018,Xu-Cheung2019,Gu-Zhang2019,Martinez-Yang2020, Martinez2020, Bulat2021,Ye-wang2021,Zhang-wang2021,jiang-wang2022,Li-Wang2022,Wang-He2022}. For example, methods like compact convolutional neural networks (CCNN \cite{Xu-Cheung2019}) use a derivative estimator to approximate the (sub)gradient of the binarization function, and the Bi-Real-Net \cite{Martinez-Yang2020} employs a polynomial step function to approximate the sign function. We refer to a nice survey for more details \cite{Yuan2021}. Once an algorithm obtains trained parameters based on the approximation, these parameters are then reintroduced into BNNs using 1-bit activations for prediction.}

 

{We point out that some 1-bit activation functions and the 0/1 function share similar structures, such as being discontinuous at the origin and having zero gradients everywhere except for the origin. Hence the aforementioned approximation methods can be employed to train 0/1 DNNs.   However, some 1-bit activation functions like the sign function have different binary values in comparison with the 0/1 function,  and the sign function is upper semi-continuous while the 0/1 function is lower semi-continuous. Most importantly, differing from the approximation methods, the aim of this paper is to develop a method directly processing the 0/1 function.}

 {\bf b) Gradient descent-based algorithms} have been extensively developed to train neural networks with different continuous activation functions. Popular candidates include  AdaGrad \cite{Duchi2011}, RMSProp \cite{Tieleman2012}, Adadelta \cite{Zeiler2012}, Adam \cite{Kingma2014}, AdaMax \cite{Kingma2014}, Nadam \cite{Dozat2016}, AMSGrad \cite{Reddi2018}, AdamW \cite{Loshchilov2019}, QHAdam \cite{Ma2019},  and AggMo \cite{Lucas2019}. More information  can be found in \cite{Ruder2016} and the references therein. We shall highlight that these algorithms belong to the family of backpropagation algorithms that need to calculate the (sub)gradients of involved functions through a chain rule. As a consequence, the problem of vanishing gradients restricts them from training some DNNs, such as 0/1 DNNs and BNNs. Therefore,  a natural question is if there is an algorithm for 0/1 DNNs without relying on the (sub)gradients heavily like the one proposed in \cite{Taylor2016} so as to get rid of the problem of vanishing gradients.


\subsection{Motivation and contributions}
{
The motivation behind conducting research on 0/1 DNNs is threefold.  Firstly,  similar to BNNs, 0/1 DNNs exploit 1-bit activation functions (i.e., 0/1 functions) and thus enable  network compression for extreme computational and storage efficiency.  Secondly,  the step function offers several advantageous properties. For instance, it can enhance the stability of noises from the training data in DNNs \cite{Courbariaux2015, Courbariaux2016, Yuan2021}. Lastly, despite its drawbacks (such as discontinuity and vanishing gradients) hampering its implementation for backpropagation algorithms for a long time, recent advances have emerged to directly address the 0/1 function, which has led to success in processing various applications, such as the binary classifications \cite{Wang2021, Zhou2021} and one-bit compressive sensing \cite{Zhou12022}. }

{
In addition to employing the step function for both feedforward and backpropagation, we harness the -regularization to train the network. It allows us to reduce redundant neurons to get suitable networks to accelerate the computation and mitigate over-fitting issues \cite{lin2019, Yang2019, Dinh2020, Zhang2021, Hoefler2021}. Therefore, inspired by the advantages in utilizing the step function and -regularization, our aim is to explore problem \eqref{OP} and make three main contributions.}
 
\begin{itemize}[leftmargin=20pt]
\item[C1)] This is the first paper that investigates  0/1  DNNs with an -regularization. To tackle model \eqref{OP}, we first reformulate it as an unconstrained optimization and then develop a block coordinate descend (BCD) method  to solve it from the outer to the inner layer. As the involved sub-problems in BCD either admit closed-form solutions or are easy to be addressed, the proposed method does not belong to the family of backpropagation algorithms. Therefore, there is no problem with the vanishing gradients stemmed from the 0/1 activation. Most importantly, the fulfilment of BCD provides a viable method for 0/1 DNNs, one of the toughest models in deep learning.

\item[C2)] It is noted that problem \eqref{OP} involves three hard functions: the step function, hardmax, and -regularization. Hence, it is non-trivial to establish the convergence property for BCD. Nevertheless, we show that any accumulating point of the sequence generated by BCD is a stationary point of the unconstrained optimization problem, see Theorem \ref{the-convergence}. It is worth mentioning that the global convergence of BCD has been established in \cite{Zhang2017, Lau2018, Zeng2019} under some assumptions which, however, are violated by all three hard functions in  \eqref{OP}.

\item[C3)] We conduct some primary numerical experiments for BCD on classifying four datasets. The promising results have demonstrated that 0/1 DNNs not only deliver desirable classification accuracy but also are more robust to adversarial perturbations \cite{Zheng2018} than DNNs with other popular activation functions.
\end{itemize}

\subsection{Organization and notation}
We organize the paper as follows. In the next section, we reformulate problem \eqref{OP} as an unconstrained optimization and describe four types of induced sub-problems. In Section \ref{Section-Technical}, we provide technical analysis for solving the induced sub-problems. It is shown that three of them admit closed-form solutions and one can be solved efficiently. In Section \ref{Section-algorithm}, BCD is developed. Then its implementation and the convergence analysis are also provided. Numerical experiments and conclusions are given in the last two sections.

We end this section by summarizing the notation to be employed throughout the paper.
We use ``'' to mean ``define''. For an index set , let  be cardinality of  and  be its complementary set. Let  represent  and . Given two vectors , we use   to denote the indices of maximal entries of , namely,
\be
 \arraycolsep=1.4pt\def\arraystretch{1.25}
\ba{lll}
\am:=\{i\in[m]: a_i = \a_{\max}\},~~~\a_{\max}:= \max_{r\in[m]} a_r.
\ea
\ee
Hence,    means the th element of . {It is worth mentioning that  may have multiple maximal entries and thus  is a vector. However, it is a scalar if  admits a single maximal value, e.g. one-hot vectors  that we are interested in this paper.} For matrix , we write , , and  as the th row,  the th column,  and the th entry of .
Let  denote the Frobenius norm for matrices and the Euclidean norm for vectors, and  denote the Spectral norm.  The identity matrix is denoted by   and  its  th column is denoted by . The inner product of  two matrices  and  is defined by . Finally, for a set of matrices , denote
\be
\ba{lll}
\CW_{\leq i}:=\{\W_1,\cdots,\W_{i}\},~
 \CW_{\geq i}:=\{\W_{i},\cdots,\W_h\}.
\ea
\ee
\section{Model Reformulation}
We first reformulate original model \eqref{OP} into an unconstrained optimization problem and then briefly describe four types of induced sub-problems.
\subsection{Model reformulation}
We aim to divide multi-composite problem \eqref{OP} into a series of easy-to-analyze sub-problems. To proceed with that, we introduce some intermediate variables as follows. Let  and . Similar to the definition of , we denote  and . Moreover, denote  and  for easier reference. Then problem \eqref{OP} can be rewritten as
\be\label{OP010}
 \arraycolsep=1.4pt\def\arraystretch{1.25}
\ba{cll}
\min\limits_{\CW, \CU, \CV} &&\frac{1}{2N} \|\Y- (\U_h)_{\hd}\|^{2} + g(\CW)\\
{\rm s.t.} &&\U_i=\W_i\V_{i-1},~i\in[h],\\
&&\V_{i}=(\U_i)_{0/1},~~~i\in[h-1],
\ea
\ee
where .

One can discern that it is considerably hard to solve problem \eqref{OP010} (or problem \eqref{OP}). Therefore,  we relax the constraints and shift them to the objective function, resulting in an unconstrained optimization problem,
\be\label{penalf1}
 \arraycolsep=0.0pt\def\arraystretch{1.75}
\begin{array}{lcl}
\min\limits_{\CW,\CU,\CV} F(\CW,\CU,\CV)&:=&  \frac{1}{2N} \|\Y- (\U_h)_{\hd}\|^{2}+ g(\CW)\\
&+&\frac{\tau}{2} \sum_{i=1}^h \|\U_i-\W_{i}\V_{i-1}\|^2 \\
&+& \frac{\pi }{2}\sum_{i=1}^{h-1}\|\V_i-(\U_{i})_{0/1}\|^2,
\end{array}
\ee
where  and  are positive penalty parameters.

\subsection{Induced sub-problems}\label{sec:induced}
To solving problem \eqref{penalf1}, we will adopt the BCD algorithm, which solves one block while fixing the others. This will induce the following four types of sub-problems.\\
a) The outermost layer sub-problem with respect to :
\be\label{U-outproblem}
\arraycolsep=1.0pt\def\arraystretch{1.25}
\ba{l}
\min\limits_{\U_h} \frac{1}{2N}\|\Y- (\U_h)_{\hd}\|^{2}+\frac{\tau}{2} \|\U_h-\W_h\V_{h-1}\|^2.
\ea\ee
b) Inner layer sub-problems with respect to  :
\be\label{U-inproblem}
\ba{cl}
\min\limits_{\U_i}\frac{\tau}{2} \|\U_i-\W_{i}\V_{i-1}\|^2+ \frac{\pi }{2} \|\V_i-(\U_{i})_{0/1}\|^2.
\ea\ee
c) Sub-problems with respect to  :
\be\label{W-sub-problem}
\ba{cl}
\min\limits_{\W_i} \frac{\tau}{2}\|\U_i-\W_i\V_{i-1}\|^2+\frac{\gamma}{2} \|\W_i\|^2+\lambda \|\W_i\|_{2,0}.
\ea\ee
d) Sub-problems with respect to  :
\be\label{V-sub-problem}
 \arraycolsep=0.4pt\def\arraystretch{1.25}
\ba{crl}
\min\limits_{\V_i}\frac{\tau}{2}\|\U_{i+1}-\W_{i+1}\V_{i}\|^2+ \frac{\pi }{2} \|\V_i-(\U_{i})_{0/1}\|^2.
\ea\ee


\section{Solving Sub-problems} \label{Section-Technical}
In this section,  we aim to solve problems \eqref{U-outproblem}, \eqref{U-inproblem} ,  \eqref{W-sub-problem}, and \eqref{V-sub-problem}. We note that the first two problems  have separable objective functions and thus can be reduced to simpler problems with closed-form solutions, while problem \eqref{W-sub-problem} is slightly hard to tackle but can be solved efficiently.
\subsection{Solutions to sub-problem (\ref{U-outproblem})}
Since  , sub-problem \eqref{U-outproblem} can be solved column-wisely. In addition, as  and  are one-hot vectors, to address sub-problem \eqref{U-outproblem},  we can focus on the following problem in a vector form,

where , ,  and  are given parameters.  The following lemma states that the above problem admits a closed-form solution.
\begin{lemma}\label{u-outlemma} For any given ,  solution  to (\ref{u-outproblem}) takes the form of

where   and .
\end{lemma}
 In the numerical experiments, we use  to approximate . The above result enables us  to show closed-form solution  to \eqref{U-outproblem} as follows. For each , each column of  can be solved by \eqref{soultion-Uh} with setting
\be\ba{l}\label{Uh-problem-*}
(\U^*_{h})_{:s}=\u^*, \y = \y_s,  \b=(\W_h\V_{h-1})_{:s},\mu=\tau N.
\ea\ee
\subsection{Solutions to sub-problem  (\ref{U-inproblem})}
Since the objective function in sub-problem \eqref{U-inproblem} is separable, it can be addressed element-wisely. Therefore, we aim to solve the following one-dimensional problem,
\be\ba{cl}\label{u-inproblem}
u^*\in {\rm arg}\min\limits_{u\in \R}~\varphi (u):= (a- (u)_{0/1})^2+ \rho  (u-b)^2,
\ea\ee
where , , and  are given scalars. Our next result shows the above problem admits a closed-form solution.
\begin{lemma} \label{u_insolution}Denote  and .  Then the solution to  (\ref{u-inproblem}) is solved as follows.\\
i) If , then

ii) If , then


\end{lemma}

\begin{remark}The solutions to (\ref{u-inproblem}) are illustrated by Fig. \ref{01-solutions}. It is worth mentioning that when , condition    means  and , resulting in . However,   is hard to handle numerically. In our numerical experiments, we set 
\end{remark}
\begin{figure}[!th]
\centering
\includegraphics[width=0.24\textwidth]{01-positive.pdf}
\includegraphics[width=0.24\textwidth]{01-negative.pdf}
  \caption{Illustrations for \eqref{a0-Prox-+} and \eqref{a0-Prox--} with .}\label{01-solutions}
\end{figure}
The above lemma allows us to derive solution  to sub-problem \eqref{U-inproblem}  by setting
\be \label{U-inproblem-*}
\ba{lllllll}
a&=&{(\V_i)}_{jr},~ &b&=&(\W_{i}\V_{i-1})_{jr},\\
t&=&2(\V_i)_{jr}-1,~ &\rho&=&\pi/\tau.
\ea\ee


\subsection{Solving  sub-problem (\ref{W-sub-problem}) approximately}

Instead of addressing sub-problem \eqref{W-sub-problem} directly, we focus on the following optimization,
\be\label{W-sub-problem31}
\ba{cl}
\W^*\in{\rm argmin}_{\W}~  \Psi(\W)  +\lambda \|\W\|_{0,2},
\ea\ee
where
\be
\ba{cl}
 \Psi(\W):=\frac{\tau}{2}\|\U-\V\W\|^2+\frac{\gamma}{2} \|\W\|^2.\nonumber
\ea\ee
Here, , and  correspond to , and  in \eqref{W-sub-problem}, and   stands for the row sparsity.



To solve sub-problem \eqref{W-sub-problem31}, we introduce the concept of a proximal stationary point (P-stationary point), which is associated with the proximal operator in variational analysis \cite{Rockafellar1998}.  Let  be a proper lower semi-continuous function. The proximal operator of , associated with  parameter , is defined by

Hu and Beck independently presented the proximal operator of  in \cite[Proposition 18]{Hu2017} and \cite[Theorem 3.2]{Beck2019}. Any point   takes the form of

Based on the proximal operator, the P-stationary point of  \eqref{W-sub-problem31} can be defined as follows.
\begin{definition}\label{def-S-prox2}
A matrix  is a P-stationary point of  problem (\ref{W-sub-problem31}) if there is a constant  such that
\be\label{P-CW}
    \W^* \in {\rm Prox}_{\beta \lambda\|\cdot\|_{0,2}}(\W^*-\beta \nabla \Psi(\W^*)).\ee
\end{definition}
It is easy to show that  is a P-stationary point if and only if it satisfies that for any ,
 \be \label{P-equivalent}
  \arraycolsep=1.4pt\def\arraystretch{1.25}
\left\{ \ba{lll}
\|(\nabla \Psi(\W^*))_{s:}\|=0,~ \|\W^*_{s:}\|\geq
\sqrt{2\beta\lambda}, & \text{if} ~ \|\W^*_{s:}\|\neq 0, \\
\|(\nabla \Psi(\W^*))_{s:}\|\leq \sqrt{2\lambda/\beta}, & \text{if} ~ \|\W^*_{s:}\|= 0.
 \ea \right.
 \ee
In the subsequent part, we first show that a P-stationary point is a unique locally optimal solution  to problem \eqref{W-sub-problem31} and satisfies a quadratic growth property.
 \begin{theorem} \label{P-first-order-local}
Let  be a P-stationary point for some , then it is a unique locally optimal solution to problem (\ref{W-sub-problem31}) and satisfies the following quadratic growth property,
 \be \label{quadratic-growth-property}
  \arraycolsep=1.4pt\def\arraystretch{1.25}
\ba{lll}
&&\Psi(\W) + \lambda\|\W\|_{0,2} -( \Psi(\W^*) + \lambda\|\W^*\|_{0,2})\\
&  \geq & \frac{\gamma}{2} \|\W -\W^*\|^2,~~\forall ~\W\in{\mathbb N}(\W^*, \sqrt{\beta\lambda/(2m^2)})
 \ea
 \ee
where .
\end{theorem}
In addition to the relation to the locally optimal solution,
 a P-stationary point also has a close relationship with the globally optimal solution to   \eqref{W-sub-problem31}, stated as follows.
\begin{theorem} \label{P-first-order}
If  is a globally optimal solution to problem (\ref{W-sub-problem31}), then it is a P-stationary point for any . On the contrary, a P-stationary point with  is also a globally optimal solution.
\end{theorem}
The above theorem shows that instead of finding an optimal solution to \eqref{W-sub-problem31}, it is meaningful to find a P-stationary point as the later can be achieved by the proximal gradient method (PGM) \cite{Beck2019} presented in Algorithm \ref{ses}.
 \begin{algorithm}[H] \caption{PGM()}\label{ses}
	\begin{algorithmic}[1]
\STATE \textbf{Initialize}    and  .
	\FOR{}	
	 \STATE  Update   by
	  \be\label{P-CW-l}
    \W^{\ell+1} \in {\rm Prox}_{\beta \lambda\|\cdot\|_{0,2}}(\W^{\ell}-\beta \nabla \Psi(\W^{\ell})).\ee
\ENDFOR
\RETURN 
	\end{algorithmic}
\end{algorithm}
Algorithm \ref{ses}  is associated with , and   since it aims to deal with \eqref{W-sub-problem31}. Here,   is an initial point that can be set by the previous point during the training.  The following theorem shows that the whole sequence generated by Algorithm \ref{ses}  converges to a  P-stationary point of  \eqref{W-sub-problem31}.
\begin{theorem} \label{PGM-converge}
Let  be the sequence generated by Algorithm \ref{ses} with , then the whole sequence converges to a P-stationary point of problem (\ref{W-sub-problem31}).
\end{theorem}

\subsection{Solving  sub-problem (\ref{V-sub-problem}) }
{Problem (\ref{V-sub-problem}) can be solved by finding  solution  to the following linear equation,
\be\label{V-sub-problem-sol}
(\tau\W_{i+1}^\top\W_{i+1}+\pi \I)\V_i^*=\tau\W_{i+1}^\top\U_{i+1} +\pi(\U_{i})_{0/1}.
\ee
One way is to compute the inverse of  with computational complexity , which is impractical when  is large.  Alternatively, we can adopt the conjugate gradient method, which requires computational complexity , where  is the iteration number and usually small. In our numerical experiments, we adopt this method to solve sub-problem (\ref{V-sub-problem}).}
\section{Block Coordinate Descent} \label{Section-algorithm}
In this section, we adopt the BCD  algorithm  to solve problem \eqref{penalf1} and then establish its convergence property.

\subsection{Algorithmic design}

Suppose we have computed  at step . Then next point    is updated as \eqref{Int2}.

\begin{table*}[!htb]
\centering
\begin{tabular}{lcccr}
\parbox{1\textwidth}{ } \\\hline
\end{tabular}
\end{table*}
\indent Based on the order of updating each variable in Fig. \ref{scheldual}, the updating starts from the outermost layer to the first layer. By dropping constant terms, the sub-problems in \eqref{Int2} are equivalent to following ones,

Although there are  sub-problems to be solved in \eqref{Int2}, we only need to solve 4 types of optimization problems essentially, namely,  4 types of the induced sub-problems in the above scheme, also see Section \ref{sec:induced}.

Recalling the relationship between a globally optimal solution and a P-stationary point in Theorem \ref{P-first-order}, instead of solving  \eqref{Int3-Wi}, we aim to find a P-stationary point by solving

for , where   is the gradient of   defined by

\begin{figure}[!th]
\centering
\includegraphics[width=1\linewidth]{flow-update.png}
  \caption{{Update order of parameters.}}\label{scheldual}
\end{figure}
We point out that \eqref{Int3-Wi-k} can be solved by Algorithm \ref{ses} due to Theorem \ref{PGM-converge}. Now,   all steps for updating  are summarized in Algorithm \ref{BCD-0}.
 \begin{algorithm}[!th] \caption{BCD for 0/1 DNNs\label{BCD-0}}
	\begin{algorithmic}[1]
	\STATE \textbf{Require} training data  and network parameters .
	\STATE \textbf{Initialize} , , and maximum number of iteration .
	\FOR{}
	\STATE Update  by \eqref{Int3-Uh}.
	\STATE Update  by \eqref{Int3-Wi-k}, namely, by Algorithm \ref{ses}.
	 \FOR{ }
		\STATE Update  by \eqref{Int3-Vi}.
		\STATE Update  by \eqref{Int3-Ui}.
		\STATE Update  by \eqref{Int3-Wi-k}, namely, by Algorithm \ref{ses}.
		\ENDFOR
\ENDFOR
\RETURN 
	\end{algorithmic}
\end{algorithm}


\subsection{Convergence analysis}







To establish the convergence property, we introduce the stationary point of problem \eqref{penalf1}.
\begin{definition}\label{def-Gstationary}
A point  {()} is a stationary point of  problem (\ref{penalf1}) if there exists a constant  satisfying (\ref{G-In3}).
\end{definition}
The next theorem shows that the objective function values of the sequence  generated by Algorithm \ref{BCD-0} converge and any its accumulating point is a stationary point.
\begin{theorem}\label{the-convergence}
Let  be the sequence generated by Algorithm \ref{BCD-0}. Then  the following statements are valid.
\begin{itemize}[leftmargin=15pt]
\item[a)] For any ,
\be\label{decreasing-pro}
  \arraycolsep=1.4pt\def\arraystretch{1.75}
\ba{lcl}
&& F(\CW^{k+1},\CU^{k+1},\CV^{k+1})- F(\CW^k,\CU^k,\CV^k)\\
&\leq&- \frac{\gamma-1/\beta}{2}\sum_{i=1}^n \|  \W_{i}^{k+1}-\W_{i}^{k}\|^2\\ &&-\frac{\pi}{2}\sum_{i=1}^{h-1} \|\V_{i}^{k+1}-\V_{i}^{k} \|^2.
\ea\ee
\item[b)] For any , sequence  is non-increasing and converges. Hence,

\item[c)] Let . Then any accumulating point     is a  stationary point of problem (\ref{penalf1}) if every column of  only has one maximal entry.
\end{itemize}
\end{theorem}




\subsection{Implementation of BCD}
In the sequel, we present the details for each sub-problem in Algorithm \ref{BCD-0}  so as to make the algorithm tractable.

\begin{itemize}[leftmargin=10pt]
\item \underline{ sub-problem} can be solved by

for  due to \eqref{soultion-Uh} and \eqref{Uh-problem-*}, where

\item \underline{ sub-problem} can be solved by Algorithm \ref{ses}, i.e.,

for . As we mentioned before, we use  as the initial point for Algorithm \ref{ses}.
\item \underline{ sub-problem} can be solved by finding solution  to the following equation for  ,

\item \underline{ sub-problem} can be solved as follows for  and  . \\
If  then

If  then

due to \eqref{a0-Prox-+}, \eqref{a0-Prox--}, and \eqref{U-inproblem-*}, where

\end{itemize}
Overall, the detailed computations for  are summarized in Algorithm \ref{BCD}.

 \begin{algorithm}[!th] \caption{BCD for 0/1 DNNs\label{BCD}}
	\begin{algorithmic}[1]
	\STATE \textbf{Require} training data  and network parameters .
	\STATE \textbf{Initialize} , , and maximum number of iteration .
	\FOR{}
	\STATE Update  by \eqref{soultion-Uh-k1}.
	\STATE Update  by \eqref{soultion-Wi-k1}.
	 \FOR{ }
		\STATE Update  by solving \eqref{soultion-Vi-k1}.
		\STATE Update  by \eqref{soultion-Ui-k1} and \eqref{soultion-Ui-k2}.
		\STATE Update  by \eqref{soultion-Wi-k1}.
		\ENDFOR
\ENDFOR
\RETURN 
	\end{algorithmic}
\end{algorithm}
 
\subsection{Some extensions}\label{extension-BCD}
We extend BCD to train networks in some other scenarios. 
\begin{itemize}[leftmargin=12pt]
\item[I.] If the popular softmax and cross entropy (CE) are used in the network as the outermost activation functions and the  loss function, we can amend model (\ref{OP010}) as 

where  is the cross entropy between  and , and   is the softmax. Accordingly,  in Algorithm \ref{BCD} we can remove the subproblem of  and replace two subproblems  of  and  by

Both problems can be solved by some popular optimization algorithms, such as the gradient descent (GD) method.
\item[II.]  BCD can be adjusted to train a  convolutional neural network (CNN). Given a CNN with  convolutional layers and  fully connected layers where the 0/1 function is used as the activation functions, let  be the input and 

Then to train CNN, we can adjust model (\ref{OP010}) as follows

where  corresponds to the kernel in the th convolutional layer,  represents the output after the kernel operation on input ,   and  can be deemed as the pooling operation. Therefore, in BCD two additional subproblems of  and  are induced as follows:

where the first problem may have closed form solutions for some particular pooling functions   (e.g., max pooling) and the second one can be addressed similarly to  (\ref{V-sub-problem}).  

Moreover, combining model \eqref{OP010-s} and \eqref{OP010-s-c}, BCD enables training CNN with softmax used as the outermost activation functions   and cross entropy used for the loss function.
\end{itemize}
 
\section{Numerical Experiments} \label{Section-Numerical}
In this section, we will conduct some numerical experiments to demonstrate the performance of BCD in Algorithm \ref{BCD} using python on Collab provided by Google (CPU version). To show the efficiency of the 0/1 activation, we will compare 0/1 DNNs (solved by BCD) with DNNs with different popular activation functions (solved by Adam \cite{Kingma2014})  for classifying four datasets.
\subsection{Datasets and implementations}
We aim to implement our algorithm to classify four popular datasets: MNIST \cite{LeCun2010}, FashionMNIST \cite{Xiao2017}, { Cifar10 \cite{Krizhevsky-Nair2014}, and Cifar100 \cite{Krizhevsky-Nair2014}}. For each dataset we build the following architecture of 0/1 DNNs.

\begin{itemize}[leftmargin=15pt]
\item[i)] MNIST dataset with 10 classes has 60,000 and 10,000 images used for training and testing. Each piece of data is a 2828 grey-scale handwritten digit image. For such a dataset, we train our 0/1 DNNs with two hidden layers and one  output layer. {All layers are fully-connected. We denote this network by F3NN.} The number of each hidden layer node is set as 2000. Overall, we have .

\item[ii)] FashionMNIST dataset with 10 classes consists of 28  28 grayscale images curled from 70,000 fashion products \cite{Xiao2017}. Again, it is divided into two parts: 60,000 images for training and 10,000 images for testing. The architecture of our 0/1 DNNs is similar to the one for the MNIST dataset. Therefore,  .

\item[iii)] { Both Cifar10  and Cifar100 datasets comprise 50,000 images for training and 10,000 images for testing. Each image is a full-color  picture of real objects such as airplanes, cars, and birds. Although the sample size is slightly smaller than that of MNIST and FashionMNIST,  the images have much more features, as .

To classify these two datasets, we leverage the architecture of Resnet18, a popular type of CNN.  The descriptions of BCD to train this type of networks were given in Section \ref{extension-BCD}.  Finally, to classify the Cifar10, we use hardmax at the the output layer along with the mean squared error (MSE) loss in \eqref{OP010-s-c}. However, to classify the Cifar100, we use the softmax and CE as shown in model \eqref{OP010-s}. This choice highlights the flexibility of our BCD approach in handling DNNs with diverse activation and loss functions.}
\end{itemize}
\begin{figure*}[!th]
\centering
\includegraphics[width=1\textwidth]{MNIST_result.pdf}\\  \vspace{1mm}
\includegraphics[width=1\textwidth]{MNIST_result_batchsize256.pdf}\\  
\caption{Performance on classifying MNIST: The top two are trained with full batches, the bottom two are trained with small batches.}
\label{fig:MNIST} 
\end{figure*}


 




The implementations for BCD are as follows: We initialize the weights in  as small random Gaussian values without any pretreatments. Parameters are set as , , , and . The maximum number of iterations is  We shall emphasize that the selections of these parameters would not impact BCD significantly.  For Algorithm \ref{ses}, we fix  and choose  for the purpose of accelerating the computation.




\subsection{Benchmark methods}
As mentioned above, we will compare 0/1 DNNs with DNNs with seven different activation functions. They are the Sign function, Binarization function, ReLU,   Sigmoid, Swish (with  used in this paper), SELU, and Tanh. The first two are 1-bit activation functions and the last five are continuous activation functions as shown in Fig. \ref{figureactivation}. Their analytical expressions are given as follows.

{As mentioned in Section \ref{Section-relatedlectures}, BNNs take 1-bit activation functions and are challenging to train directly. Popular training approaches are based on approximations. We select four state-of-the-art methods: STE \cite{courbariaux2015}, Bi-Real Net \cite{Martinez-Yang2020}, CCNN \cite{Xu-Cheung2019}, and PCNN \cite{Gu-Zhang2019}. Overall, we may have 9 types of DNNs  and one our 0/1 NDDs. To ensure a fair comparison, we endeavor to maintain similar setups across them. Hence,   we continue to use floating point values  instead of using 1-bit values as weights for BNNs.  The detailed architectures of the networks and the optimizers are presented in Table \ref{setupdiff}.}
\begin{table}[!th]
	\renewcommand{\arraystretch}{1.25}\addtolength{\tabcolsep}{-3.5pt}
\centering
\caption{Setup for different solvers. \label{setupdiff}}
\begin{tabular}{c|c|c|c}\hline
Activation &&Continuous& 1-bit\\\hline 
\multirow{2}{*}{Networks}&\multirow{2}{*}{0/1(BCD)}&ReLU, Sigmoid,& STE, CCNN, PCNN \\
&&Swish, SELU, Tanh& Bi-Real-Net\\\hline
MNIST&{hdmax+MSE+}&\multicolumn{2}{c}{sfmax+CE+}\\
FashionMNIST&F3NN+BCD&\multicolumn{2}{c}{F3NN+Adam}\\\hline
\multirow{2}{*}{Cifar10} &hdmax+MSE+&\multicolumn{2}{c}{\multirow{3}{*}{sfmax+CE+}}\\
&Resnet18+BCD&\multicolumn{2}{c}{\multirow{3}{*}{Resnet18+Adam}}\\\cline{1-2}
\multirow{2}{*}{Cifar100} &sfmax+CE+& \multicolumn{2}{c}{}\\
&Resnet18+BCD& \multicolumn{2}{c}{}\\\hline
\end{tabular}
\end{table}

{For instance, in cases of ReLU and Cifar10, `sfmax+CE+Resnet18+Adam' means that the network  adopts the   Resnet18 architecture, uses ReLU activation functions in the hidden layers and softmax in the output layer, and is trained by the famous optimizer Adam.  We  opt for  Adam because it not only is one of the most popular gradient-driven algorithms at present but also sets fixed parameters to avoid poor performance from improper parameter adjustments. Moreover, for more efficient training, we preprocess the data consistently across all networks: pre-train data by a small batch size at the beginning of several epochs (one epoch for MNIST, two epochs for FashionMNIST). }



We will report the following five metrics
\begin{center}
(Tr. error,~~Te. error,~~Fil.~num.,~~Par.~num., FLOPs.)
\end{center}
to illustrate the performance of all methods. The first four metrics represent the error on the training dataset, the error on the testing dataset, the number of nodes/filters in hidden layers, and the number of network parameters, respectively. The FLOPs stands for the floating-point operations per second. { We point out that Tr. error and Te error are  Top-1 errors for all datasets. However, we also report the Top-5 test errors for Cifar100. To reduce the influence of the initial point on the network training, we test each network five times with five randomly initialized points. Training and test errors of 10 methods are shown in  Figs. \ref{fig:MNIST}-\ref{fig:Cifar100Top5}, where solid curves and shaded regions respectively represent the mean and standard deviation of five trials.}


{
\subsection{Numerical results}
\subsubsection{Training and testing accuracy} According to Tables \ref{com-9-algs1} and  \ref{com-9-algs2}, it is evident that ReLU yields the lowest training errors when classifying MNIST (with a full batch size), MNIST (with Batchsize=256), Cifar10, and Cifar100, while Swish obtains  the best one for classifying FashionMNIST. In contrast, our algorithm 0/1(BCD) generates relatively competitive training errors and the best testing errors for most of the datasets. 
  
  \begin{figure*}[!t]
\centering
\includegraphics[width=1\textwidth]{FashionMNIST86.pdf}
\caption{Performance on classifying FashionMNIST.}
\label{fig:Fashion}\vspace{-3mm}
\end{figure*}



\begin{table*}[!tb] \renewcommand{\arraystretch}{1.0}\addtolength{\tabcolsep}{-4pt}
\centering
\caption{{Comparison of nine methods.}}\label{com-9-algs1}
\begin{tabular}{l |ccccc| ccccc |ccccc |ccccc}\hline
{~} & Tr.   & Te.  &  Fil. &  Par. &  FLOPs & Tr.   & Te.  &  Fil. &  Par. &  FLOPs& Tr.   & Te.  &  Fil. &  Par. &  FLOPs& Tr.   & Te.  &  Fil. &  Par. &  FLOPs\\ 
{~} & error &  error &  num. &  num. &  & error &  error &  num. &  num. &  & error &  error &  num. &  num. &  & error &  error &  num. &  num. &  \\
{~} & () &  () &   &    &  (M) & () &  () &    &    &  (M)& () &  () &    &    &  (M)& () &  () &   &    &  (M)\\\hline 
&\multicolumn{5}{c|}{MNIST}&\multicolumn{5}{c|}{MNIST (Batchsize=256)}
&\multicolumn{5}{c|}{FashionMNIST}
&\multicolumn{5}{c}{Cifar10}\\
\hline
ReLU   &{\bf 0.166}  & 0.657   &  4000 & 5172k & 0.558&{\bf 0.146}  & 0.690   &  4000 & 5172k & 0.558&7.831     & 9.480     & 4000       & 5172k  & 0.558& 2.478 & 18.432   &  3000 & 14.21M & 223.4\\
Sigmoid   & 0.353  & 0.662&  4000 & 5172k & 0.558& 0.545  & 0.800&  4000 & 5172k & 0.558& 8.760  & 10.04   & 4000     & 5172k & 0.558 &3.050   & 18.638&  3000 & 14.21M & 223.4\\
Swish      & 0.175  & 0.712& 4000 & 5172k & 0.558 & {\bf 0.146}  & 0.678& 4000 & 5172k & 0.558&8.285  & 9.580    & 4000     & 5172k & 0.558& 2.486  &{\bf 18.384}& 3000 & 14.21M & 223.4\\
SELU          &0.226  & 0.678& 4000 & 5172k& 0.558 &0.154  & 0.664& 4000 & 5172k& 0.558& 8.325  & 9.960   & 4000   & 5172k & 0.558& 2.430 & 18.328 & 3000 & 14.21M& 223.4\\
Tanh         &0.475  & 0.784 & 4000 & 5172k&0.558 &0.892  & 0.970 & 4000 & 5172k&0.558& 8.165  & 9.720     & 4000   & 5172k & 0.558&2.488   & 18.430 & 3000 & 14.21M&223.4\\
STE         &0.475  & 0.788 & 4000 & 5172k& 0.382&1.161  & 1.211 & 4000 & 5172k& 0.382&7.797  & 9.764      & 4000   & 5172k& 0.382&2.465  & 18.480 & 3000 & 14.21M& 221.7\\
CCNN         &1.852  & 1.891 & 4000 & 5172k& 0.382&1.135  & 1.171 & 4000 & 5172k& 0.382&8.344  & 9.785 & 4000 & 5172k& 0.382&3.626  & 19.178& 3000 & 14.21M& 221.7\\
PCNN        &1.031  & 1.203 & 4000 & 5172k& 0.382&0.538  & 0.656 & 4000 & 5172k& 0.382&7.812  & 9.496 & 4000 & 5172k& 0.382&3.007  & 18.681& 3000 & 14.21M& 221.7\\  
Bi-Real-Net     &0.514  & 0.879 & 4000 & 5172k& 0.382 &1.192  & 1.306 & 4000 & 5172k& 0.382&{\bf 7.632}  & 9.462 & 4000 & 5172k& 0.382&2.497 & 18.456 & 3000 & 14.21M& 221.7\\
0/1(BCD) & 0.395  & {\bf 0.652}   &  {\bf 3800}  & {\bf 4294k}  & {\bf 0.363}& 0.306  & {\bf 0.634}   &  {\bf 3800}  & {\bf 4294k}  & {\bf 0.363}&7.780   & {\bf 9.451}   & {\bf 3800}        & {\bf 4294k}& {\bf 0.363}&{\bf 2.179}    & 18.470  &  {\bf 2340}  & {\bf 14.20M}  & {\bf 221.4}\\   
\hline
\end{tabular}
\end{table*}
 \subsubsection{FLOPs and Network scale}  It can be clearly seen from Tables \ref{com-9-algs1} and  \ref{com-9-algs2} that 0/1(BCD) enables a more efficient network with lowest FLOPs followed by STE, CCNN, PCNN, and Bi-Real-Net. This is because the binary operation is capable of accelerating the computation and the -regularization can sparsify the network. Consequently, the number of active nodes in the trained network is lower compared to other networks, e.g., Fil. num. v.s. Fil. num..}


  \subsubsection{Effect of the initial process} As presented in Fig. \ref{fig:MNIST}, the initial process impacts on all methods to some extend. If there is no such an initial process (corresponding to  0 epochs), then all methods obtain relatively high training error. However, with the increasing number of epochs, the training and testing errors decline dramatically at first and then tend to be steady. 
  
   \begin{table}[H] \renewcommand{\arraystretch}{1}\addtolength{\tabcolsep}{0.3pt}
\centering
\caption{{Comparison of eight methods for Cifar100.}}\label{com-9-algs2}
\begin{tabular}{lccccc}\hline
{~} & Top-1 & Top-5 & {Fil.  num.} & {Par. num.}& {FLOPs}\\ \hline
\multicolumn{6}{c}{Cifar100}\\\hline
ReLU   &{\bf 52.934}  & {\bf 26.321}   &  4200 & 16.54M & 225.6M\\
Sigmoid   & 54.019  & 27.544&  4200 & 16.54M  & 225.6M\\
Swish      & 52.982  & 26.572& 4200 & 16.54M  & 225.6M\\
SELU          &52.996  & 26.635& 4200 & 16.54M & 225.6M\\
Tanh         &52.993  & 26.523 & 4200 & 16.54M &225.6M\\
STE         &54.131  & 26.689 & 4200 & 16.54M & 183.0M\\
CCNN         &55.432  & 29.624 & 4200 & 16.54M & 183.0M\\
Bi-Real-Net     &55.167  & 29.239 & 4200 & 16.54M & 183.0M\\
0/1 & 53.189  & 26.691   &  {\bf 3990}  & {\bf 16.53M} & {\bf 182.9M}\\\hline
\end{tabular}
\end{table}

\begin{figure*}[!th]
\centering
\includegraphics[width=1\textwidth]{Cifar10_hardmax.pdf}\vspace{-2mm}
\caption{Performance on classifying  Cifar10.}
\label{fig:Cifar10}
\end{figure*}


\begin{figure*}[!th]
\centering
\includegraphics[width=1\textwidth]{final_result_CIfar100Top1Top5.pdf}
\vspace{-5mm}
\caption{Performance on classifying Cifar100.}
\label{fig:Cifar100Top5}\vspace{-2mm}
\end{figure*}




 \subsubsection{Robustness to the noise} Based on our experiment results,  0/1 DNNs are quite robust to adversarial perturbations to the noise. We add Gaussian noise to the original testing images of FashionMNIST, as shown in Fig.  \ref{rubfashion1}, and gradually increase the noise level. Then we use these noisy images to verify the stability of networks generated by 0/1 DNNs and ReLU DNNs. As present in Fig. \ref{rubfashion2}, for Shirt (in the top-left sub-figure) and Pullover (in the bottom-right sub-figure),  the noise levels do not impact the classifications by 0/1 DNNs. In contrast, ReLU DNNs frequently classify Shirts and Pullover as Coat for some higher noisy intensities.  Fig. \ref{fig:Robust} illustrates that the error of all test dataset with different noise level. From the figure, as the noise intensity increases, the test error of  0/1 DNNs is more stable than that for the other networks.

\begin{figure}[H]
\centering
\includegraphics[scale=.3]{Robustresult606.pdf}\vspace{-2mm}
\caption{Test error in different noise level.}
\label{fig:Robust}
\end{figure}

\begin{figure*}[!th]
\centering
\includegraphics[width=0.24\textwidth]{Robustoriginsneaker101.pdf}
\includegraphics[width=0.24\textwidth]{Robustoriginshirt102.pdf}
\includegraphics[width=0.24\textwidth]{Robustorigin6616coat.pdf}
\includegraphics[width=0.24\textwidth]{Robustorigin20Pullover.pdf}\\
\includegraphics[width=0.24\textwidth]{Robustaftercoat101.pdf}
\includegraphics[width=0.24\textwidth]{RobustafterSandal102.pdf}
\includegraphics[width=0.24\textwidth]{Robustafter6616bag.pdf}
\includegraphics[width=0.24\textwidth]{Robustafter20Coat.pdf}
\caption{Images in FashionMNIST. The first and second rows show  the original and noisy images.  }
\label{rubfashion1}
\end{figure*}


\begin{figure*}[!th]
\centering
\includegraphics[width=0.475\textwidth]{Robustall101.pdf}~~
\includegraphics[width=0.475\textwidth]{Robustall102.pdf}\\
\includegraphics[width=0.475\textwidth]{Robustall6616.pdf}~~
\includegraphics[width=0.475\textwidth]{Robustall20.pdf}\vspace{-5mm}
\caption{Robustness to the noise.}
\label{rubfashion2}
\end{figure*}



\section{Conclusion} \label{Section-conclusion}
The story of the 0/1 DNNs is historic, but there has been limited progress of research on developing algorithms specifically designed for 0/1 DNNs over the years. This is mainly due to the challenging of vanishing gradients of the 0/1 activation function that impedes the effectiveness of all backpropagation algorithms. Therefore, to train the 0/1 DNNs, we  cast a BCD algorithm that does not depend on the (sub)gradients heavily, thereby enabling training DNNs with other activation functions that may incur the issue of vanishing  gradients. {
Moreover, when using  as weights, 0/1 DNNs can be deemed as a special case of BNNs. Exploring how to apply the framework of BCD to this scenario presents a promising topic of research.  Furthermore, in addition to the  image classification, we believe that the proposed 0/1 DNNs  could find utility in some other applications, such as medical diagnosis and automatic speech recognition. We leave this as the future research.}

\section*{Acknowledgements}
We acknowledge the support provided by the China Scholarship Council (CSC) during Dr. Hui Zhang's visit to Imperial College London. The authors extend their gratitude to Prof. Baochang Zhang and Dr. Jiaxin Gu from Beihang University for generously sharing the PCNN code and providing valuable insights regarding the numerical experiment.


\bibliographystyle{IEEEtran}
\begin{thebibliography}{99}
\bibitem{Mcculloch1943}
W. S. Mcculloch and W. H. Pitts,
\newblock ``A logical calculus of the ideas immanent in nervous activity,"
\newblock {\em Bull. Math. Biol.},
\newblock vol. 5, pp. 115--133, 1943.

\bibitem{Britannica2019}
Britannica,
\newblock The editors of encyclopaedia, ``all-or-none law,"
\newblock {\em Encyclopedia Britannica},
\newblock  {\em https://www.britannica.com/science/all-or-none-law}, 2019.


\bibitem{zhouzhihua}
Z. H. Zhou,
\newblock ``Machine learning"
\newblock {\em Springer Singapore},
\newblock  {\em https://doi.org/10.1}\\ {\em 007/978-981-15-1967-3}, 2021.


 \bibitem{Rumelhart1986}
 D. E. Rumelhart, G. E. Hinton, and R. J. Williams,
 \newblock  ``Learning representations by back-propagating errors,"
\newblock {\em Nature}, vol. 323, no. 6088, pp. 533-536, 1986.

\bibitem{Han1995}
J. Han and C. Moraga,
\newblock ``The influence of the sigmoid function parameters on the speed of backpropagation learning,"
\newblock {\em ICANN}, pp. 195-201, 1995.


\bibitem{Goodfellow2016}
I. Goodfellow, Y. Bengio, and A. Courville,
\newblock  ``Deep learning,"
\newblock {\em MIT press}, 2016.


\bibitem{Nair2012}
V. Nair and G. E. Hinton,
\newblock ``Rectified linear units improve restricted boltzmann machines,"
\newblock  {\em ICML}, 2010.


\bibitem{Klambauer2017}
G. Klambauer, T. Unterthiner,  A. Mayr, and S. Hochreiter,
\newblock  ``Self-normalizing neural networks,"
\newblock {\em NeurIPS}, 30, 2017.


\bibitem{Ramachandran2017}
P. Ramachandran, B. Zoph, and Q. V. Le,
\newblock ``Swish: a self-gated activation function,"
\newblock {\em arXiv:1710.05941}, 2017.


\bibitem{Maas2013}
A. L. Maas, A. Y. Hannun, and A. Y. Ng,
\newblock  ``Rectifier nonlinearities improve neural network acoustic models,"
\newblock {\em ICML}, 2013.


\bibitem{He2015}
K. He,  X. Zhang,  S. Ren, and J. Sun,
  \newblock  ``Delving deep into rectifiers: Surpassing human-level performance on imagenet classification,"
  \newblock {\em ICCV}, pp. 1026-1034,  2015.


\bibitem{Clevert2015}
D. A. Clevert,  T.  Unterthiner, and S. Hochreiter,
 \newblock  ``Fast and accurate deep network learning by exponential linear units (ELUs),"
 \newblock {\em arXiv:1511.07289}, 2015.

\bibitem{Hinton2012}
G. Hinton, N. Srivastava and K. Swersky
\newblock ``Neural networks for machine learning lecture 6a overview of mini-batch gradient descent,"
\newblock {\em Cited on}, 14(8):2, 2012.



\bibitem{courbariaux2015}
 M. Courbariaux,  Y. Bengio and J-P. David,
\newblock ``Binaryconnect: Training deep neural networks with binary weights during propagations",
\newblock {\em Advances in neural information processing systems}, 28, 2015.

\bibitem{Tang2017}
W. Tang, G. Hua, L. Wang,
\newblock ``How to train a compact binary neural network with high accuracy",
\newblock {\em AAAI}, 2017.


\bibitem{Alizadeh2018}
M. Alizadeh,  J. Fern andez-Marques, N. Lane and Y. Gal,
\newblock ``An empirical study of binary neural networks' optimisation",
\newblock {\em ICLR}, 2018.

\bibitem{Martinez2020}
 B. Martinez, J. Yang, A. Bulat, G. Tzimiropoulos,
\newblock ``Training binary neural networks with real-to-binary convolutions",
\newblock {\em ICLR}, 2020.

\bibitem{Bulat2021}
A. Bulat, B. Martinez and G. Tzimiropoulos,
\newblock ``High-capacity expert binary networks",
\newblock {\em ICLR}, 2021.

{
\bibitem{zhong2012sensitivity}
S. M. Zhong, X. Q. Zeng, S. L. Wu and L. X. Han
\newblock  ``Sensitivity-based adaptive learning rules for binary feedforward neural networks,''
\newblock {\em IEEE Trans. Neural Netw. Learn. Syst.}, vol. 23, no. 3, pp. 480--491, 2012.

\bibitem{Gu-Zhang2019}
J. X. Gu,  C. Li , B. C. Zhang, et al.
\newblock  ``Projection convolutional neural networks for 1-bit CNNs via discrete back propagation."
\newblock {\em AAAI.}
\newblock 2019, 33(01): 8344-8351.

\bibitem{Martinez-Yang2020}
B. Martinez,  J. Yang, A. Bulat, and G. Tzimiropoulos.
\newblock  ``Training binary neural networks with real-to-binary convolutions. "
\newblock {\em ICML}, 2020.

\bibitem{Ye-wang2021}
J. M. Ye, J. D. Wang, and S. L. Zhang.
\newblock  ``Distillation-guided residual learning for binary convolutional neural networks."
\newblock {\em IEEE Trans. Neural Netw. Learn. Syst.}, 2021.

\bibitem{Zhang-wang2021}
B. C. Zhang, R. Q. Wang, X.D. Wang, J.G. Han, and R. R. Ji.
\newblock  ``Modulated convolutional networks."
\newblock {\em IEEE Trans. Neural Netw. Learn. Syst.}, 2021.

\bibitem{jiang-wang2022}
X. R. Jiang,  N. Wang, J. Xin, K. Li, X. Yang, J. Li, X. Gao.
\newblock  ``Toward pixel-level precision for binary super-resolution with mixed binary representation."
\newblock {\em IEEE Trans. Neural Netw. Learn. Syst.}, 2022.

\bibitem{Li-Wang2022}
K. Li, N. N. Wang,  J. W. Xin, X. R. Jiang, J. Li, X. B. Gao, K. Han, and Y. H. Wang.
\newblock  ``Local means binary networks for image super-resolution."
\newblock {\em IEEE Trans. Neural Netw. Learn. Syst.}, 2022.

\bibitem{Wang-He2022}
P. S. Wang, X. Y. He, and J. Cheng.
\newblock  ``Toward accurate binarized neural networks with sparsity for mobile application."
\newblock {\em IEEE Trans. Neural Netw. Learn. Syst.}, 2022.






}

\bibitem{Xu-Cheung2019}
Z. Xu, R. C. Cheung.
\newblock ``Accurate and compact convolutional neural networks with trained binarization."
\newblock {\em BMVC},  2019.

\bibitem{Yuan2021}
C Yuan and S. Agaian,
\newblock  ``A comprehensive review of binary neural network,"
\newblock {\em Artif. Intell. Rev.}, 2023.

\bibitem{Duchi2011}
J. Duchi, E. Hazan, and Y. Singer,
\newblock  ``Adaptive subgradient methods for online learning
and stochastic optimization,"
\newblock   {\em J. Mach. Learn. Res.}, vol. 12, pp. 2121--2159, 2011.


 \bibitem{Tieleman2012}
T. Tieleman  and G. Hinton,
\newblock  ``Lecture 6.5 - RMSProp, COURSERA: Neural networks for machine learning,"
\newblock Technical report, 2012.


\bibitem{Zeiler2012}
M. D. Zeiler,
\newblock  ``Adadelta: an adaptive learning rate method,"
\newblock {\em arXiv:1212.5701}, 2012.


\bibitem{Kingma2014}
 D. P. Kingma and  J. Ba,
\newblock  ``Adam: A method for stochastic optimization,"
\newblock {\em arXiv:1412.6980}, 2014.


\bibitem{Dozat2016}
T. Dozat,
\newblock  ``Incorporating Nesterov momentum into Adam,"
\newblock   {\em ICLR Workshop}, vol. 1, pp. 2013--2016,
2016.


\bibitem{Reddi2018}
S. J. Reddi, S.  Kale,  and S. Kumar,
\newblock  ``On the convergence of Adam and beyond,"
\newblock {\em ICLR}, 2018.


\bibitem{Loshchilov2019}
I. Loshchilov and  F. Hutter,
\newblock  ``Decoupled weight decay regularization,"
\newblock  {\em ICLR}, 2019.


\bibitem{Ma2019}
 J. Ma and D. Yarats,
  \newblock  ``Quasi-hyperbolic momentum and Adam for deep learning,"
 \newblock  {\em ICLR}, 2019.


 \bibitem{Lucas2019}
J. Lucas,  S. Sun,   R. Zemel, and R. Grosse,
 \newblock  ``Aggregated momentum: stability through passive damping,"
\newblock  {\em ICLR}, 2019.


 \bibitem{Ruder2016}
S. Ruder,
\newblock  ``An overview of gradient descent optimization algorithms,"
\newblock {\em arXiv:1609.04747}, 2016.


\bibitem{Taylor2016}
G. Taylor, R. Burmeister, Z. Xu, B. Singh, A. Patel, and T. Goldstein,
\newblock  ``Training neural networks without gradients: A scalable ADMM approach,"
\newblock {\em ICML},
 pp. 2722--2731, 2016.


 \bibitem{Courbariaux2015}
M. Courbariaux and Y. S. Bengio,
\newblock  ``BinaryConnect: Training deep neural networks with binary weights during propagations,"
\newblock {\em NeurIPS}, 2015.


\bibitem{Courbariaux2016}
M. Courbariaux, I. Hubara, D. Soudry and Y. S. Bengio,
\newblock  ``Binarised neural networks: Training deep neural networks with weights and activations
constrained to +1 or -1,"
\newblock  {\em  NeurIPS}, 2016.





\bibitem{Wang2021}
H. J. Wang, Y. H. Shao, S. L. Zhou, C. Zhang, and N. H. Xiu,
\newblock  ``Support vector machine classifier via  soft-margin loss,"
\newblock {\em IEEE Trans. Pattern Anal. Mach. Intell.},
\newblock 2021.


\bibitem{Zhou2021}
S. L. Zhou, L. L. Pan, N. H. Xiu, and H. D. Qi,
\newblock  ``Quadratic convergence of smoothing Newton's method for 0/1 loss optimization,"
\newblock {\em SIAM J. Optim.},
\newblock vol. 34, no. 4, pp. 3184--3211, 2021.


\bibitem{Zhou12022}
S. L. Zhou, Z. Y. Luo, N H. Xiu, and G. Y. Li,
\newblock  ``Computing one-bit compressive sensing via double-sparsity constrained optimization,"
\newblock {\em IEEE Trans. Signal Process.},
\newblock vol. 70, pp. 1593--1680, 2022.


\bibitem{lin2019}
S. H. Lin, R. R. Ji, Y. C. Li, C. Deng, and X. L. Li,
\newblock  ``Toward compact ConvNets via structure-sparsity regularized filter pruning,"
\newblock {\em IEEE Trans. Neural Netw. Learn. Syst.},
\newblock vol. 31, no. 2, pp. 574--588, 2019.


\bibitem{Yang2019}
B. Yang, J. Lyu, S. Zhang, Y. Qi, and J. Xin,
\newblock  ``Channel pruning for deep neural networks via a relaxed group-wise splitting method,"
\newblock   {\em AI4I}, 2019.


\bibitem{Dinh2020}
T. Dinh, B. Wang, A. L. Bertozzi, and S. J. Osher,
\newblock  ``Sparsity meets robustness: Channel pruning for the Feynman-Kac formalism principled robust deep neural nets,"
\newblock {\em arXiv:2003.00631}, 2020.


\bibitem{Zhang2021}
H. Zhang, Z. P. Yuan, and N. H. Xiu,
\newblock  ``Recursion Newton-like algorithm for -ReLU deep neural networks,"
\newblock {\em IEEE Trans. Neural Netw. Learn. Syst.}, 2021.



\bibitem{Hoefler2021}
T. Hoefler, D. Alistarh, T. Ben-Nun, N. Dryden, and A. Peste,
\newblock  ``Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks,"
\newblock {\em arXiv:2102.00554}, 2021.


\bibitem{Zhang2017}
Z. Zhang and M. Brand,
\newblock  ``Convergent block coordinate descent for training Tikhonov regularized  deep neural networks,"
\newblock {\em NeurIPS }, pp. 1721--1730, 2017.


\bibitem{Lau2018}
T. T. Lau, J. Zeng, B. Wu, and Y. Yao,
\newblock  ``A proximal block coordinate descent algorithm for deep neural network  training,"
\newblock {\em arXiv:1803.09082}, 2018.


\bibitem{Zeng2019}
J. S. Zeng, T. T. Lau, S. B. Lin, and Y. Yao,
\newblock  ``Global convergence of block coordinate descent in deep learning,"
\newblock   {\em  ICML}, 2019.


\bibitem{Zheng2018}
Z. H. Zheng and P. Y. Hong,
\newblock  ``Robust detection of adversarial attacks by modeling the intrinsic properties of deep neural networks,"
\newblock {\em  NeurIPS }, pp. 7924--7933, 2018.


\bibitem{Rockafellar1998}
R. T. Rockafellar and R. J. Wets,
\newblock  ``Variational analysis,"
\newblock {\em Springer Science and Business Media}, 2009.


\bibitem{Hu2017}
Y. Hu, C. Li, K. Meng, J. Qin, and X. Yang,
\newblock  ``Group sparse optimization via  regularization,"
\newblock {\em J. Mach. Learn. Res.},
\newblock vol. 18, no. 1, pp. 960--1011, 2017.


\bibitem{Beck2019}
A. Beck and N. Hallak,
\newblock  ``Optimization problems involving group sparsity terms,"
\newblock {\em Math. Program.},
\newblock vol. 178, no. 1-2, pp. 39--67, 2019.







\bibitem{LeCun2010}
Y. LeCun, and C. Cortes.
\newblock  ``MNIST handwritten digit database," 2010.


\bibitem{Xiao2017}
H. Xiao, K. Rasul, and R. Vollgraf,
\newblock  ``Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms,"
\newblock  {\em arXiv:1708.07747}, 2017.

\bibitem{Krizhevsky-Nair2014}
 A. Krizhevsky, V. Nair, and G. Hinton.
\newblock  ``The CIFAR-10 dataset,"
\newblock {\em https://www.cs.toronto.edu/~kriz/cifar.html.}, 2014.

\bibitem{Zhou32021}
S. L. Zhou, L. L. Pan, and N. H. Xiu,
\newblock  ``Newton method for -regularized optimization,"
\newblock {\em Numer. Algorithms},
\newblock vol. 88, no. 4, pp. 1541--1570, 2021.


\bibitem{More1983}
J. J. More and D. C. Sorensen,
\newblock  ``Computing a trust region step,"
\newblock {\em SIAM J. Sci. Statist. Comput.}, vol. 4, no. 3, pp. 553--572, 1983.

\end{thebibliography}

\newpage
\onecolumn
\appendices
 \section{Proofs of all theorems}
\subsection{Proof Lemma \ref{u-outlemma}}
\begin{proof}
Since   and   , two possible optimal solutions to problem \eqref{u-outproblem} are  or , where . It is easy to show that  and

If , then the above formulas suffice to

Similarly, we can derive the result for the other two cases.
\end{proof}


\subsection{Proof Lemma \ref{u_insolution}}
\begin{proof} One can find the possible solutions to \eqref{u-inproblem} are , and , where  is a positive constant. Their objective function  values are summarized in Table \ref{obj-values}.

\begin{table}[H]
	\renewcommand{\arraystretch}{1.25}\addtolength{\tabcolsep}{10pt}
\centering
\caption{Four objective function  values, where `' stands for the possible minimal value. \label{obj-values}}
\begin{tabular}{c llll}\hline
 && & & \\\hline
\multirow{2}{*}{ } &  &  &  &    \\
 &&&& \\\hline
\multirow{2}{*}{   } &  &  &  &   \\
 &&&& \\\hline
\multirow{2}{*}{    } &  &  &  &    \\
 &&&& \\\hline
\end{tabular}
\end{table}\vspace{-3mm}
\noindent We only prove the result when  as the other two cases are similar. If  , then  . However,  is increasing with the rising of  due to . Hence, we have  and .
\end{proof}
\subsection{Proof Theorem \ref{P-first-order-local}}
\begin{proof} For any , we denote two index sets as
  
 We can prove that . In fact, if there is  an index  but  , then
 we obtain
  
 where   the last inequality holds due to  \eqref{P-equivalent}. The above   contradiction implies that . Therefore, . If , then , which by the strong convexity of  allows us to derive that
  
 where the penultimate equality is due to \eqref{P-equivalent} and ,  showing \eqref{quadratic-growth-property}. If , then , which together with the strong convexity of   derives that
   
where the first equality and the last inequality are from  \eqref{P-equivalent} and  , displaying \eqref{quadratic-growth-property} as well.
\end{proof}
\subsection{Proof Theorem \ref{P-first-order} }
\begin{proof} We note that  is a strongly smooth with a constant  and a strongly convex with a constant . Then the rest proof is similar to \cite[Theorem 1]{Zhou32021} and thus is omitted here.
\end{proof}

\subsection{Proof Theorem \ref{PGM-converge} }
\begin{proof} As   is  strongly smooth with  , it follows from \cite[Theorem 5.1]{Beck2019} that
\be\label{W-descent}
\ba{cl}
 \Psi(\W^{\ell+1}) + \lambda \|\W^{\ell+1}\|_{0,2} -(\Psi(\W^{\ell}) + \lambda \|\W^{\ell}\|_{0,2} ) \leq -\frac{1/\beta-L_{\Psi}}{2}\|\W^{\ell+1}-\W^{\ell}\|^2,
\ea\ee
which by  means that sequence  converges. Then taking the limit of both sides of \eqref{W-descent} yields  . Again by \cite[Theorem 5.1]{Beck2019},
 any accumulating point (say ) of sequence  is a P-stationary point of problem \eqref{W-sub-problem31}. Moreover, we note that the  quadratic growth property in  Theorem \ref{P-first-order-local} states that any P-stationary point is a unique local minimizer and thus it is isolated. Finally,  three facts: \cite[Theorem 4.10]{More1983}, P-stationary point being isolated, and  yield that the whole sequence converges to a P-stationary point of \eqref{W-sub-problem31}.
\end{proof}


\subsection{Proof Theorem \ref{the-convergence}}
 \begin{proof}
a)   For , it is the solution to  \eqref{Int3-Uh} (or the first sub-problem of \eqref{Int2}), leading to
\be
\arraycolsep=1.4pt\def\arraystretch{1.25}
\ba{lcl} F(\CW^{k}, \{\CU_{\leq h-1}^{k},\U_h^{k+1}\}, \CV^{k})\leq F(\CW^{k}, \CU^{k}, \CV^{k}).\ea\ee
For ,  it is a solution to \eqref{Int3-Wi-k}.  Then it follow from \eqref{def-prox} that

where  and  is defined by \eqref{Int3-Wi-obj}. This indicates that

which after simple manipulation yields
 \be\label{lemma-decreasing-W-eq}
\arraycolsep=1.4pt\def\arraystretch{1.25}
\ba{lcl}
 \langle \Theta_h^{k+1}, \W_{h}^{k+1}-\W_{h}^{k} \rangle + \lambda (\|\W_{h}^{k+1}\|_{2,0} - \|\W_{h}^{k}\|_{2,0}) \leq    \frac{1}{2\beta} \|  \W_{h}^{k+1}-\W_{h}^{k}\|^2.\ea\ee
Using the above fact enables to derive that
\be
\arraycolsep=1.4pt\def\arraystretch{1.25}
\ba{lcl}
&&F(\{\CW_{\leq h-1}^{k},\W_h^{k+1}\}, \{\CU_{\leq h-1}^{k},\U_h^{k+1}\}, \CV^{k})\\
&-& F(\{\CW_{\leq h-1}^{k},\W_h^{k}\}, \{\CU_{\leq h-1}^{k},\U_h^{k+1}\}, \CV^{k})\\
&=& \frac{\tau}{2} \|\U_{h}^{k+1}-\W_{h}^{k+1}\V_{h-1}^k\|^2+\frac{\gamma }{2}\|\W_{h}^{k+1}\|^2+ \lambda \|\W_{h}^{k+1}\|_{2,0}\\
&-&\frac{\tau}{2} \|\U_{h}^{k+1}-\W_{h}^k\V_{h-1}^k\|^2-\frac{\gamma }{2}\|\W_{h}^{k}\|^2- \lambda \|\W_{h}^{k}\|_{2,0}\\
&=& \langle \Theta_h^{k+1}, \W_{h}^{k+1}-\W_{h}^{k} \rangle+ \lambda (\|\W_{h}^{k+1}\|_{2,0} - \|\W_{h}^{k}\|_{2,0})\\
&-& \frac{\tau}{2} \| (\W_{h}^{k+1}-\W_{h}^{k})\V_{h-1}^k\|^2 - \frac{\gamma}{2} \|  \W_{h}^{k+1}-\W_{h}^{k}\|^2 \\
&\leq&  - \frac{\tau}{2} \| (\W_{h}^{k+1}-\W_{h}^{k})\V_{h-1}^k\|^2 - \frac{\gamma-1/\beta}{2} \|  \W_{h}^{k+1}-\W_{h}^{k}\|^2 \qquad (\text{by \eqref{lemma-decreasing-W-eq}})\\
&\leq&   - \frac{\gamma-1/\beta}{2} \|  \W_{h}^{k+1}-\W_{h}^{k}\|^2.\ea\ee
For , since it is a solution to \eqref{Int3-Vi}, the optimality condition is
 \be\label{opt-con-V}
\arraycolsep=1.4pt\def\arraystretch{1.25}
\ba{lcl}
\tau (\W_{h}^{k+1})^\top(\W_{h}^{k+1}\V_{h-1}^k-\U_{h}^{k+1})  + \pi (\V_{h-1}^{k+1}-(\U_{h-1}^{k})_{0/1}) = 0. \ea\ee
As a result,
\be\arraycolsep=1.4pt\def\arraystretch{1.25}
\ba{lcl}
&&F(\{\CW_{\leq h-1}^{k},\W_h^{k+1}\}, \{\CU_{\leq h-1}^{k},\U_h^{k+1}\}, \{\CV_{\leq h-2}^{k+1},\V_{h-1}^{k+1}\})-F(\{\CW_{\leq h-1}^{k},\W_h^{k+1}\}, \{\CU_{\leq h-1}^{k},\U_h^{k+1}\}, \{\CV_{\leq h-2}^{k},\V_{h-1}^{k}\})\\
&=&\frac{\tau}{2}\|\U_{h}^{k+1}-\W_{h}^{k+1}\V_{h-1}^{k+1}\|^2+\frac{\pi}{2}\|\V_{h-1}^{k+1}-(\U_{h-1}^{k})_{0/1}\|^2-\frac{\tau}{2}\|\U_{h}^{k+1}-\W_{h}^{k+1}\V_{h-1}^k\|^2+\frac{\pi}{2}\|\V_{h-1}^{k}-(\U_{h-1}^{k})_{0/1}\|^2\\
&=& \langle \tau (\W_{h}^{k+1})^\top(\W_{h}^{k+1}\V_{h-1}^k-\U_{h}^{k+1})  + \pi (\V_{h-1}^{k+1}-(\U_{h-1}^{k})_{0/1}), \V_{h-1}^{k+1}-\V_{h-1}^{k} \rangle\\
&-&\frac{\tau}{2}\| \W_{h}^{k+1}(\V_{h-1}^{k+1}-\V_{h-1}^{k} )\|^2-\frac{\pi}{2}\|\V_{h-1}^{k+1}-\V_{h-1}^{k} \|^2\\
&=&-\frac{\tau}{2}\| \W_{h}^{k+1}(\V_{h-1}^{k+1}-\V_{h-1}^{k} )\|^2-\frac{\pi}{2}\|\V_{h-1}^{k+1}-\V_{h-1}^{k} \|^2~~(\text{by \eqref{opt-con-V}})\\
&\leq& -\frac{\pi}{2}\|\V_{h-1}^{k+1}-\V_{h-1}^{k} \|^2.
\ea\ee
For , it is a solution to the fourth sub-problem of \eqref{Int2}, leading to
\be \arraycolsep=1.4pt\def\arraystretch{1.25}
\ba{lcl}
&&F(\{\CW_{\leq h-1}^{k},\W_h^{k+1}\}, \{\CU_{\leq h-2}^{k},\U_{h-1}^{k+1},\U_h^{k+1}\}, \{\CV_{\leq h-2}^{k},\V_{h-1}^{k}\})\\
&\leq&F(\{\CW_{\leq h-1}^{k},\W_h^{k+1}\}, \{\CU_{\leq h-2}^{k},\U_{h-1}^k,\U_h^{k+1}\}, \{\CV_{\leq h-2}^{k},\V_{h-1}^{k}\}).
\ea\ee
Same analysis can be applied to . Overall,  the above conditions allow us to conclude that
\be\label{decreasing-pro-0}
  \arraycolsep=1.4pt\def\arraystretch{1.25}
\ba{lcl}
&& F(\CW^{k+1},\CU^{k+1},\CV^{k+1})- F(\CW^k,\CU^k,\CV^k)\\
&=&F(\CW^{k+1},\CU^{k+1},\CV^{k+1})- F(\{\W_{1}^k,\CW_{\geq 2}^{k+1}\}, \CU^{k+1}, \CV^{k+1})\\
&+&F(\{\W_{1}^k,\CW_{\geq 2}^{k+1}\}, \CU^{k+1}, \CV^{k+1}) - F(\{\W_{1}^{k},\CW_{\geq 2}^{k+1}\}, \{\U_1^k,\CU_{\geq 2}^{k+1}\}, \CV^{k+1})\\
&+& \cdots\\
&+& F(\{\CW_{\leq h-1}^{k},\W_h^{k+1}\}, \{\CU_{\leq h-1}^{k},\U_h^{k+1}\}, \CV^k)-F(\CW^{k}, \{\CU_{\leq h-1}^{k},\U_h^{k+1}\}, \CV^k)\\
&+&F(\CW^{k}, \{\CU_{\leq h-1}^{k},\U_h^{k+1}\}, \CV^k) - F(\CW^{k},\CU^{k}, \CV^{k})\\
&\leq& - \frac{\gamma-1/\beta}{2}\sum_{i\in[h]} \|  \W_{i}^{k+1}-\W_{i}^{k}\|^2   -\frac{\pi}{2}\sum_{i\in[h-1]} \|\V_{i}^{k+1}-\V_{i}^{k} \|^2.
\ea\ee

b) It follows from \eqref{decreasing-pro-0}, , and  that sequence   is non-increasing and convergent.    Taking the limit of  both sides of \eqref{decreasing-pro} immediately display \eqref{gap-0}.

c) Let   be any accumulating point of sequence , that is there exist a subsequence  such that

For   , by letting   , and  , we define  three index sets as

 Similarly, we also define  for . Now we claim that
 \be\label{>-<-=}
\arraycolsep=1.4pt\def\arraystretch{1.25}
 \ba{lclll}
\Upsilon^*_> \subseteq \Upsilon^k_>, \qquad \Upsilon^*_< \subseteq \Upsilon^k_<, \qquad \Upsilon^*_= \supseteq \Upsilon^k_=,
\ea
\ee
for sufficiently large . To show that, denote . Recall the assumption that every column of  only has one maximal entry, namely,  only has one maximal entry. This together with \eqref{limit-k-*} means that   also has one maximal entry and  for sufficiently large , thereby leading to

 Moreover, condition \eqref{limit-k-*} also suffices to 
delivering   for sufficiently large . Now, for any , it follows

which means  , resulting in . Similar reasoning also allows us to show that , and hence  due to . Overall, we prove \eqref{>-<-=}. Since  contains finitely many entries, for sufficiently large , there must exist a subset  such that
\be\label{G-In2-W-J}
\arraycolsep=1.4pt\def\arraystretch{1.25}
 \ba{lclll}
&& \Upsilon^*_> \subseteq \Upsilon^{k_1}_> \equiv \Upsilon^{k_2}_> \equiv \cdots  =: \Upsilon_>, \Upsilon^*_< \subseteq \Upsilon^{k_1}_< \equiv \Upsilon^{k_2}_< \equiv \cdots=: \Upsilon_<,\Upsilon^*_= \supseteq \Upsilon^{k_1}_= \equiv \Upsilon^{k_2}_= \equiv \cdots=: \Upsilon_=,
\ea
\ee
where the inclusions are due to \eqref{>-<-=}. Then from \eqref{soultion-Uh-k1}, we have that, for any ,

where . It follows from   that

Taking the limit of both sides of \eqref{uk+1} along with  yields

This together with \eqref{G-In2-W-J} enable us to obtain

which  recalling \eqref{soultion-Uh} and \eqref {Uh-problem-*} indicates
\be
\arraycolsep=1.4pt\def\arraystretch{1.25}
  \ba{lclll}
\U_{h}^*&\in&\operatorname{arg} \min\limits_{\U_h}& \frac{\tau}{2} \|\U_h-\W_{h}^*\V_{h-1}^*\|^2+\frac{1}{2N} \|\Y- (\U_h)_{\hd}\|^{2}.
\ea \nonumber
\ee
Similarly, we can also prove that
\be
\arraycolsep=1.4pt\def\arraystretch{1.25}
 \left\{\ba{lclll}
\V_{i}^*&=&\operatorname{arg} \min\limits_{\V_{i}}& \frac{\tau}{2}\|\U_{i+1}^*-\W_{i+1}^*\V_{i}\|^2+\frac{\pi}{2}\|\V_{i}-(\U_{i}^*)_{0/1}\|^2, &\forall i\in[h-1],\\
\U_i^*&\in&\operatorname{arg} \min\limits_{\U_i}& \frac{\tau}{2}\|\U_i-\W_i^*\V_{i-1}^*\|^2+ \frac{\pi}{2}\|\V_i^*-(\U_i)_{0/1}\|^2, &\forall i\in[h-1].\\
\ea\right.\nonumber
\ee
Finally, we prove that
\be\label{G-In3-W*}
\arraycolsep=1.4pt\def\arraystretch{1.25}
 \ba{lclll}
\W_i^*&\in& {\rm Prox}_{\beta \lambda\|\cdot\|_{2,0}}&\left(\W_i^*-\beta \nabla_\W \Phi ( \W_{i}^*; \U_{i}^*,\V_{i-1}^*)  \right),& \forall i\in[h].
\ea
\ee
Let  and .  Since  contains finitely many entries, for sufficiently large , there must exist a subset  such that
\be\label{G-In3-W-J}
\arraycolsep=1.4pt\def\arraystretch{1.25}
 \ba{lclll}
\Xi_i^{k+1}\equiv \Xi_i^*,~~\forall k\in\cal J.
\ea
\ee
It follows from \eqref{Int3-Wi-k}   that

where ,  which by \eqref{P-equivalent} derives
 \be
  \arraycolsep=1.4pt\def\arraystretch{1.25}
\left\{ \ba{lll}
\|(\Theta_i^{k+1})_{s:}\||=0~\text{and}~ \|(\W_i^{k+1})_{s:}\|\geq
\sqrt{2\beta\lambda},~~ & \text{if} ~ \|(\W_i^{k+1})_{s:}\|\neq 0, \\
\|(\Theta_i^{k+1})_{s:}\|\leq \sqrt{2\lambda/\beta}, & \text{if} ~ \|(\W_i^{k+1})_{s:}\|= 0. \nonumber
 \ea \right.
 \ee
 This together with \eqref{G-In3-W-J} leads to, for any ,
  \be
  \arraycolsep=1.4pt\def\arraystretch{1.25}
\left\{ \ba{lll}
\|(\Theta_i^{k+1})_{s:}\|=0~\text{and}~ \|(\W_i^{k+1})_{s:}\|\geq
\sqrt{2\beta\lambda},~~ & \text{if} ~ s\in \Xi_i^*, \\
\|(\Theta_i^{k+1})_{s:}\|\leq \sqrt{2\lambda/\beta}, & \text{if} ~  s\notin \Xi_i^*.
 \ea \right. \nonumber
 \ee
 Now taking the limit of all items in the above formula along with   enables us to obtain
   \be
  \arraycolsep=1.4pt\def\arraystretch{1.25}
\left\{ \ba{lll}
\|(\nabla_\W \Phi ( \W_{i}^*; \U_{i}^*,\V_{i-1}^*))_{s:}\|=0~\text{and}~ \|(\W_i^*)_{s:}\|\geq
\sqrt{2\beta\lambda},~~ & \text{if} ~ s\in \Xi_i^*, \\
\|(\nabla_\W \Phi ( \W_{i}^*; \U_{i}^*,\V_{i-1}^*))_{s:}\|\leq \sqrt{2\lambda/\beta}, & \text{if} ~  s\notin \Xi_i^*.
 \ea \right.
 \ee
 This recalling  \eqref{P-equivalent} proves that \eqref{G-In3-W*}. Overall, we show that  satisfies \eqref{G-In3}.
\end{proof}


\end{document}
