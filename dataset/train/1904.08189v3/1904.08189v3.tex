\documentclass[10pt,twocolumn,letterpaper]{article}


\usepackage{bm}
\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{algorithmic}
\usepackage{verbatim}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{caption}
\usepackage{authblk}
\usepackage{url}
\usepackage[linesnumbered,boxed]{algorithm2e}

\renewcommand\thefootnote{\textcolor{red}{\arabic{footnote}}}
\interfootnotelinepenalty=10000
\def\mycolor{\cellcolor[rgb]{0.8275,0.8275,0.8275}}


\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{****} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\setcounter{page}{1}
\begin{document}

\title{CenterNet: Keypoint Triplets for Object Detection}
\author{Kaiwen Duan\thanks{This work was done when the first author was interning at Huawei Noah's Ark Lab.}\qquad Song Bai\qquad Lingxi Xie\qquad Honggang Qi\qquad Qingming Huang\qquad Qi Tian\\
University of Chinese Academy of Sciences\qquad University of Oxford\qquadHuawei Noah's Ark Lab\\
{\tt\small kaiwen.duan@vipl.ict.ac.cn\qquad songbai.site@gmail.com\qquad 198808xc@gmail.com\qquad hgqi@ucas.ac.cn\qquad qmhuang@ucas.ac.cn \qquad tian.qi1@huawei.com}}
\maketitle



\begin{abstract}
In object detection, keypoint-based approaches often suffer a large number of incorrect object bounding boxes, arguably due to the lack of an additional look into the cropped regions. This paper presents an efficient solution which explores the visual patterns within each cropped region with minimal costs. We build our framework upon a representative one-stage keypoint-based detector named CornerNet. Our approach, named CenterNet, detects each object as a triplet, rather than a pair, of keypoints, which improves both precision and recall. Accordingly, we design two customized modules named cascade corner pooling and center pooling, which play the roles of enriching information collected by both top-left and bottom-right corners and providing more recognizable information at the central regions, respectively. On the MS-COCO dataset, CenterNet achieves an AP of \textbf{47.0\%}, which outperforms all existing one-stage detectors by at least \textbf{4.9\%}. Meanwhile, with a faster inference speed, CenterNet demonstrates quite comparable performance to the top-ranked two-stage detectors. Code is available at \url{https://github.com/Duankaiwen/CenterNet}.
\end{abstract}\section{Introduction}
Object detection has been significantly improved and advanced with the help of deep learning, especially convolutional neural networks~\cite{girshick2014rich} (CNNs). In the current era, one of the most popular flowcharts is anchor-based~\cite{girshick2015fast,he2017mask,liu2016ssd,redmon2016you, ren2015faster}, which placed a set of rectangles with pre-defined sizes, and regressed them to the desired place with the help of ground-truth objects. These approaches often need a large number of anchors to ensure a sufficiently high IoU (intersection over union) rate with the ground-truth objects, and the size and aspect ratio of each anchor box need to be manually designed. In addition, anchors are usually not aligned with the ground-truth boxes, which is not conducive to the bounding box classification task.

\begin{figure}[tb]
\centering 
\subfigure{
\includegraphics[height=0.165\textwidth,width=0.165\textheight]{1_a.pdf}
\hspace{0.05in}
\includegraphics[height=0.165\textwidth,width=0.165\textheight]{1_b.pdf}
\label{fig1a}
}
\subfigure{
\includegraphics[height=0.155\textwidth,width=0.345\textheight]{1_cc.pdf}
\label{fig1b}
}
\vspace{-2ex}
\caption{In the first row, we visualize the top 100 bounding boxes (according to the MS-COCO dataset standard) of CornerNet. Ground-truth and predicted objects are marked in blue and red, respectively. In the second row, we show that correct predictions can be determined by checking the central parts.}
\label{fig1}
\end{figure}
To overcome the drawbacks of anchor-based approaches, a keypoint-based object detection pipeline named CornerNet~\cite{law2018cornernet} was proposed. It represented each object by a pair of corner keypoints, which bypassed the need of anchor boxes and achieved the state-of-the-art one-stage object detection accuracy. Nevertheless, the performance of CornerNet is still restricted by its relatively weak ability of referring to the global information of an object. That is to say, since each object is constructed by a pair of corners, the algorithm is sensitive to detect the boundary of objects, meanwhile not being aware of which pairs of keypoints should be grouped into objects. Consequently, as shown in Figure~\ref{fig1}, it often generates some incorrect bounding boxes, most of which could be easily filtered out with complementary information, {\em e.g.}, the aspect ratio.


To address this issue, we equip CornerNet with an ability of perceiving the visual patterns within each proposed region, so that it can identify the correctness of each bounding box by itself. In this paper, we present a low-cost yet effective solution named {\bf CenterNet}, which explores the central part of a proposal, {\em i.e.}, the region that is close to the geometric center, with one extra keypoint. Our intuition is that, if a predicted bounding box has a high IoU with the ground-truth box, then the probability that the center keypoint in its central region is predicted as the same class is high, and vice versa. Thus, during inference, after a proposal is generated as a pair of corner keypoints, we determine if the proposal is indeed an object by checking if there is a center keypoint of the same class falling within its central region. The idea, as shown in Figure~\ref{fig1}, is to use a triplet, instead of a pair, of keypoints to represent each object. 

Accordingly, for better detecting center keypoints and corners, we propose two strategies to enrich center and corner information, respectively.
The first strategy is named {\bf center pooling}, which is used in the branch for predicting center keypoints. Center pooling helps the center keypoints obtain more recognizable visual patterns within objects, which makes it easier to perceive the central part of a proposal. We achieve this by getting out the max summed response in both horizontal and vertical directions of the center keypoint on a feature map for predicting center keypoints. 
The second strategy is named {\bf cascade corner pooling}, which equips the original corner pooling module~\cite{law2018cornernet} with the ability of perceiving internal information. We achieve this by getting out the max summed response in both boundary and internal directions of objects on a feature map for predicting corners. Empirically, we verify that such a two-directional pooling method is more stable, {\em i.e.}, being more robust to feature-level noises, which contributes to the improvement of both precision and recall. 

We evaluate the proposed CenterNet on the MS-COCO dataset~\cite{lin2014microsoft}, one of the most popular benchmarks for large-scale object detection. CenterNet, with both center pooling and cascade corner pooling incorporated, reports an AP of  on the test-dev set, which outperforms all existing one-stage detectors by a large margin. With an average inference time of  using a 52-layer hourglass backbone~\cite{newell2016stacked} and  using a 104-layer hourglass backbone~\cite{newell2016stacked} per image, CenterNet is quite efficient yet closely matches the state-of-the-art performance of the other two-stage detectors.

The remainder of this paper is organized as follows. Section~\ref{RelatedWork} briefly reviews related work, and Section~\ref{Approach} details the proposed CenterNet. Experimental results are given in Section~\ref{Experiments}, followed by the conclusion in Section~\ref{Conclusions}.

\section{Related Work}
\label{RelatedWork}

Object detection involves locating and classifying the objects. In the deep learning era, powered by deep convolutional neural networks, object detection approaches can be roughly categorized into two main types of pipelines, namely, two-stage approaches and one-stage approaches.

\vspace{1ex}\noindent \textbf{Two-stage approaches}~divide the object detection task into two stages: extract RoIs, then classify and regress the RoIs.

R-CNN~\cite{girshick2014rich} uses a selective search method~\cite{uijlings2013selective} to locate RoIs in the input images and uses a DCN-based regionwise classifier to classify the RoIs independently. SPP-Net~\cite{he2015spatial} and Fast-RCNN~\cite{girshick2015fast} improve R-CNN by extracting the RoIs from the feature maps. Faster-RCNN~\cite{ren2015faster} is allowed to be trained end to end by introducing RPN (region proposal network). RPN can generate RoIs by regressing the anchor boxes. Later, the anchor boxes are widely used in the object detection task. Mask-RCNN~\cite{he2017mask} adds a mask prediction branch on the Faster-RCNN, which can detect objects and predict their masks at the same time. R-FCN~\cite{dai2016r} replaces the fully connected layers with the position-sensitive score maps for better detecting objects. Cascade R-CNN~\cite{cai2018cascade} addresses the problem of overfitting at training and quality mismatch at inference by training a sequence of detectors with increasing IoU thresholds. The keypoint-based object detection approaches~\cite{tychsen2017denet,Lu2018Grid} are proposed to avoid the disadvantages of using anchor boxes and bounding boxes regression. Other meaningful works are proposed for different problems in object detection, \eg,~\cite{zhu2017couplenet,lee2017me} focus on the architecture design,~\cite{bell2016inside,gidaris2015object,shrivastava2016contextual,zeng2016gated} focus on the contextual relationship,~\cite{li2019scale,cai2016unified} focus on the multi-scale unification.
\begin{figure*}[!tb]
  \centering 
  \includegraphics[width=0.98\textwidth]{Network_Structure.pdf}
  \vspace{-2ex}
  \caption{Architecture of CenterNet. A convolutional backbone network applies cascade corner pooling and center pooling to output two corner heatmaps and a center keypoint heatmap, respectively. Similar to CornerNet, a pair of detected corners and the similar embeddings are used to detect a potential bounding box. Then the detected center keypoints are used to determine the final bounding boxes.} 
  \label{structure} \end{figure*}

\vspace{1ex}\noindent \textbf{One-stage approaches}~remove the RoI extraction process and directly classify and regress the candidate anchor boxes.

YOLO~\cite{redmon2016you} uses fewer anchor boxes (divide the input image into an  grid) to do regression and classification. YOLOv2~\cite{redmon2017yolo9000} improves the performance by using more anchor boxes and a new bounding box regression method. SSD~\cite{liu2016ssd} places anchor boxes densely over an input image and use features from different convolutional layers to regress and classify the anchor boxes. DSSD~\cite{fu2017dssd} introduces a deconvolution module into SSD to combine low-level and high-level features. While R-SSD~\cite{jeong2017enhancement} uses pooling and deconvolution operations in different feature layers to combine low-level and high-level features. RON~\cite{kong2017ron} proposes a reverse connection and an objectness prior to extract multiscale features effectively. RefineDet~\cite{zhang2018single} refines the locations and sizes of the anchor boxes for two times, which inherits the merits of both one-stage and two-stage approaches. CornerNet~\cite{law2018cornernet} is another keypoint-based approach, which directly detects an object using a pair of corners. Although CornerNet achieves high performance, it still has more room to improve.

\section{Our Approach}
\label{Approach}

\subsection{Baseline and Motivation}\label{baseline}
This paper uses CornerNet~\cite{law2018cornernet} as the baseline. For detecting corners, CornerNet produces two heatmaps: a heatmap of top-left corners and a heatmap of bottom-right corners. The heatmaps represent the locations of keypoints of different categories and assigns a confidence score for each keypoint. Besides, it also predicts an embedding and a group of offsets for each corner. The embeddings are used to identify if two corners are from the same object. The offsets learn to remap the corners from the heatmaps to the input image. For generating object bounding boxes, top- left-top corners and bottom-right corners are selected from the heatmaps according to their scores, respectively. Then, the distance of the embedding vectors of a pair of corners is calculated to determine if the paired corners belong to the same object. An object bounding box is generated if the distance is less than a threshold. The bounding box is assigned a confidence score, which equals to the average scores of the corner pair. 
\begin{table}[tb]
\centering
\resizebox{.48\textwidth}{!}{
\begin{tabular}{|l|ccccccc|}
\hline
Method & FD & FD & FD & FD & FD & FD & FD\\
\hline
CornerNet & 37.8 & 32.7 & 36.8 & 43.8 & 60.3 & 33.2 & 25.1 \\
\hline
\end{tabular}}
\vspace{-2ex}
\caption{False discovery rates () of CornerNet. The false discovery rate reflects the distribution of incorrect bounding boxes. The results suggest the incorrect bounding boxes account for a large proportion.}
\label{FDR1}
\end{table}

In Table~\ref{FDR1}, we provide a deeper analysis of CornerNet. We count the FD\footnote{, where AP denotes the average precision at  on the MS-COCO dataset. Also, , where  denotes the average precision at , , where , denotes the scale of object.} (false discovery) rate of CornerNet on the MS-COCO validation dataset, defined as the proportion of the incorrect bounding boxes. The quantitative results demonstrate the incorrect bounding boxes account for a large proportion even at low IoU thresholds, \eg, CornerNet obtains  FD rate at . This means in average,   out of every  object bounding boxes have IoU lower than  with the ground-truth. The small incorrect bounding boxes are even more, which achieves  FD rate. One of the possible reasons lies in that CornerNet cannot look into the regions inside the bounding boxes. To make CornerNet~\cite{law2018cornernet} perceive the visual patterns in bounding boxes, one potential solution is to adapt CornerNet into a two-stage detector, which uses the RoI pooling~\cite{girshick2015fast} to look into the visual patterns in bounding boxes. However, it is known that such a paradigm is computationally expensive.

In this paper, we propose a highly efficient alternative called \textbf{CenterNet} to explore the visual patterns within each bounding box.
For detecting an object, our approach uses a triplet, rather than a pair, of keypoints. By doing so, our approach is still a one-stage detector, but partially inherits the functionality of RoI pooling. Our approach only pays attention to the center information, the cost of our approach is minimal. Meanwhile, we further introduce the visual patterns within objects into the keypoint detection process by using center pooling and cascade corner pooling.

\subsection{Object Detection as Keypoint Triplets}\label{Triplets}
The overall network architecture is shown in Figure~\ref{structure}. We represent each object by a center keypoint and a pair of corners. Specifically, we embed a heatmap for the center keypoints on the basis of CornerNet and predict the offsets of the center keypoints. Then, we use the method proposed in CornerNet~\cite{law2018cornernet} to generate top- bounding boxes. However, to effectively filter out the incorrect bounding boxes, we leverage the detected center keypoints and resort to the following procedure: (1) select top- center keypoints according to their scores; (2) use the corresponding offsets to remap these center keypoints to the input image; (3) define a central region for each bounding box and check if the central region contains center keypoints. Note that the class labels of the checked center keypoints should be same as that of the bounding box; (4) if a center keypoint is detected in the central region, we will preserve the bounding box. The score of the bounding box will be replaced by the average scores of the three points,~\ie,~the top-left corner, the bottom-right corner and the center keypoint. If there are no center keypoints detected in its central region, the bounding box will be removed.

\begin{figure}[tb]
  \centering 
  \subfigure[]{ 
    \includegraphics[width = 0.225\textwidth]{NCS1.pdf}
    \label{fig_ncs1} 
  } 
  \subfigure[]{ 
    \includegraphics[width = 0.225\textwidth]{NCS2.pdf}
    \label{fig_ncs2} 
  }
  \vspace{-2ex}
  \caption{(a) The central region when . (b) The central region when . The solid rectangles denote the predicted bounding boxes and the shaded regions denote the central regions.}
  \label{fig_ncs}
\end{figure}
The size of the central region in the bounding box affects the detection results. For example, smaller central regions lead to a low recall rate for small bounding boxes, while larger central regions lead to a low precision for large bounding boxes. Therefore, we propose a scale-aware central region to adaptively fit the size of bounding boxes. The scale-aware central region tends to generate a relatively large central region for a small bounding box, while a relatively small central region for a large bounding box. Suppose we want to determine if a bounding box  needs to be preserved. Let  and  denote the coordinates of the top-left corner of  and  and  denote the coordinates of the bottom-right corner of . Define a central region . Let  and  denote the coordinates of the top-left corner of  and  and  denote the coordinates of the bottom-right corner of . Then , , , , , ,  and  should satisfy the following relationship:

where  is odd that determines the scale of the central region . In this paper,  is set to be  and  for the scales of bounding boxes less and greater than , respectively. Figure~\ref{fig_ncs} shows two central regions when  and , respectively. According to Equation~(\ref{eq_ncs1}), we can determine a scale-aware central region, then we check if the central region contains center keypoints.

\subsection{Enriching Center and Corner Information}\label{Enriching}

\begin{figure}[tb]
  \centering 
  \subfigure[]{ 
    \includegraphics[width=0.11\textheight]{CenterPooling.pdf}
    \label{fig_ct}
  } 
  \subfigure[]{ 
    \includegraphics[width=0.11\textheight]{CornerPooling.pdf}
    \label{fig_cp} 
  } 
  \subfigure[]{ 
    \includegraphics[width=0.11\textheight]{CascadeCornerPooling.pdf}
    \label{fig_ccp} 
  } 
  \vspace{-2ex}
  \caption{(a) Center pooling takes the maximum values in both horizontal and vertical directions. (b) Corner pooling only takes the maximum values in boundary directions. (c) Cascade corner pooling takes the maximum values in both boundary directions and internal directions of objects.} 
  \label{fig_cornerpooling}
  \vspace{-3ex}
\end{figure}


\noindent\textbf{Center pooling.}~The geometric centers of objects do not necessarily convey very recognizable visual patterns (\eg, the human head contains strong visual patterns, but the center keypoint is often in the middle of the human body). To address this issue, we propose center pooling to capture richer and more recognizable visual patterns. Figure~\ref{fig_ct} shows the principle of center pooling. The detailed process of center pooling is as follows: the backbone outputs a feature map, and to determine if a pixel in the feature map is a center keypoint, we need to find the maximum value in its both horizontal and vertical directions and add them together. By doing this, center pooling helps the better detection of center keypoints.

\vspace{1ex}\noindent\textbf{Cascade corner pooling.}~Corners are often outside the objects, which lacks local appearance features. CornerNet~\cite{law2018cornernet} uses corner pooling to address this issue. The principle of corner pooling is shown in Figure~\ref{fig_cp}. Corner pooling aims to find the maximum values on the boundary directions so as to determine corners. However, it makes corners sensitive to the edges. To address this problem, we need to let corners ``see" the visual patterns of objects. The principle of cascade corner pooling is presented in Figure~\ref{fig_ccp}. It first looks along a boundary to find a boundary maximum value, then looks inside along the location of the boundary maximum value\footnote{For the topmost, leftmost, bottommost and rightmost boundary, look vertically towards the bottom, horizontally towards the right, vertically towards the top and horizontally towards the left, respectively.} to find an internal maximum value, and finally, add the two maximum values together. By doing this, the corners obtain both the the boundary information and the visual patterns of objects.

Both the center pooling and the cascade corner pooling can be easily achieved by combining the corner pooling~\cite{law2018cornernet} at different directions. Figure~\ref{PoolingStructure}{\color{red}(a)} shows the structure of the center pooling module. To take a maximum value in a direction, \eg, the horizontal direction, we only need to connect the left pooling and the right pooling in series. Figure~\ref{PoolingStructure}{\color{red}(b)} shows the structure of a cascade top corner pooling module. Compared with the top corner pooling in CornerNet~\cite{law2018cornernet}, we add a left corner pooling before the top corner pooling.
\begin{figure}[tb]
  \centering 
  \includegraphics[width=0.48\textwidth]{PoolingStructure.pdf}
  \vspace{-4ex}
  \caption{The structures of the center pooling module (a) and the cascade top corner pooling module (b). We achieve center pooling and the cascade corner pooling by combining the corner pooling at different directions.} 
  \label{PoolingStructure} 
\end{figure}

\subsection{Training and Inference }\label{Training}
\noindent\textbf{Training.} Our method is implemented in Pytorch~\cite{paszke2017automatic} and the network is trained from scratch. The resolution of the input image is , leading to heatmaps of size . We use the data augmentation strategy presented in~\cite{law2018cornernet} to train a robust model. Adam~\cite{kingma2014adam} is used to optimize the training loss:

where  and  denote the focal losses, which are used to train the network to detect corners and center keypoints, respectively.  is a ``pull'' loss for corners, which is used to minimize the distance of the embedding vectors that belongs to the same objects.  is a  ``push'' loss for corners, which is used to maximize the distance of the embedding vectors that belongs to different objects.  and  are -losses~\cite{girshick2015fast}, which are used to train the network to predict the offsets of corners and center keypoints, respectively. ,  and  denote the weights for corresponding losses, which are set to ,  and , respectively. , ,  and  are all defined in the CornerNet, we suggest to refer to~\cite{law2018cornernet} for details. We train the CenterNet on  Tesla V100 (32GB) GPUs and use a batch size of . The maximum number of iterations is . We use a learning rate of  for the first  iterations and then continue training  iterations with a rate of . 

\vspace{1ex}\noindent\textbf{Inference.} Following~\cite{law2018cornernet}, for the single-scale testing, we input both the original and horizontally flipped images with the original resolutions into the network. While for the multi-scale testing, we input both the original and horizontally flipped images with the resolutions of  and . We select top  center keypoints, top  top-left corners and top  bottom-right corners from the heatmaps to detect the bounding boxes. We flip the bounding boxes detected in the horizontally flipped images and mix them into the original bounding boxes. Soft-nms~\cite{bodla2017soft} is used to remove the redundant bounding boxes. We finally select top  bounding boxes according to their scores as the final detection results.
\section{Experiments}
\label{Experiments}

\subsection{Dataset, Metrics and Baseline}
\label{sec:setting}

We evaluate our method on the MS-COCO dataset~\cite{lin2014microsoft}. It contains  categories and more than  million object instances. The large number of small objects makes it a very challenging dataset. We use the `trainval35k' set~\cite{hoiem2012diagnosing} (\ie,  training images and  validation images) for training and test the results on the test-dev set. We use another  images in the validation set to perform ablation studies and visualization experiments.

MS-COCO dataset~\cite{lin2014microsoft} uses AP and AR metrics to characterize the performance of a detector. AP represents the average precision rate, which is computed over ten different IoU thresholds (\ie, ) and all categories. It is considered the single most important metric on the MS-COCO dataset. AR represents the maximum recall rate, which is computed over a fixed number of detections (\ie, ,  and  ) per image and averaged over all categories and the ten different IoU thresholds. Additionally, AP and AR can be used to evaluate the performance under different object scales, including small objects (), medium objects () and large objects ().

Our direct baseline is CornerNet~\cite{law2018cornernet}. Following it, we use the stacked hourglass network~\cite{newell2016stacked} with  and  layers as the backbone -- the latter has two hourglass modules while the former has only one. All modifications on the hourglass architecture, made by~\cite{law2018cornernet}, are preserved.

\begin{table*}[tb]
\small
\centering
\resizebox{1.0\textwidth}{!}{
\begin{tabular}{|l|l|cc|cccccc|cccccc|}
\hline
Method & Backbone & Train input & Test input& AP & AP & AP & AP & AP & AP & AR & AR & AR & AR & AR & AR\\
\hline
\hline
\textbf{Two-stage:} & & & & & & & & & & & & & & &\\
DeNet~\cite{tychsen2017denet} & ResNet-101~\cite{he2016deep} & 512512 & 512512 & 33.8 & 53.4 & 36.1 & 12.3 & 36.1 & 50.8 & 29.6 & 42.6 & 43.5 & 19.2 & 46.9 & 64.3 \\
CoupleNet~\cite{zhu2017couplenet} & ResNet-101 & ori.& ori. & 34.4 & 54.8 & 37.2 & 13.4 & 38.1 & 50.8 & 30.0 & 45.0 & 46.4 & 20.7 & 53.1 & 68.5 \\
Faster R-CNN by G-RMI~\cite{huang2017speed} & Inception-ResNet-v2~\cite{szegedy2017inception} &  1000600 &  1000600 & 34.7 & 55.5 & 36.7 & 13.5 & 38.1 & 52.0 & - & - & - & - & - & - \\
Faster R-CNN +++~\cite{he2016deep} & ResNet-101 &  1000600 &  1000600 & 34.9 & 55.7 & 37.4 & 15.6 & 38.7 & 50.9 & - & - & - & - & - & - \\
Faster R-CNN w/ FPN~\cite{lin2017feature} & ResNet-101 &  1000600 &  1000600 & 36.2 & 59.1 & 39.0 & 18.2 & 39.0 & 48.2 & - & - & - & - & - & - \\
Faster R-CNN w/ TDM~\cite{shrivastava2016beyond} & Inception-ResNet-v2 &  - & - & 36.8 & 57.7 & 39.2 & 16.2 & 39.8 & 52.1 & \textbf{31.6} & \textbf{49.3} & \textbf{51.9} & \textbf{28.1} & \textbf{56.6} & \textbf{71.1} \\
D-FCN~\cite{dai2017deformable} & Aligned-Inception-ResNet &  1000600 &  1000600 & 37.5 & 58.0 & - & 19.4 & 40.1 & 52.5 & - & - & - & - & - & - \\
Regionlets~\cite{xu2018deep} & ResNet-101 &  1000600 &  1000600 & 39.3 & 59.8 & - & 21.7 & 43.7 & 50.9 & - & - & - & - & - & - \\
Mask R-CNN~\cite{he2017mask} & ResNeXt-101 &  1300800 &  1300800 & 39.8 & 62.3 & 43.4 & 22.1 & 43.2 & 51.2 & - & - & - & - & - & - \\
Soft-NMS~\cite{bodla2017soft} & Aligned-Inception-ResNet &  1300800 &  1300800 & 40.9 & 62.8 & - & 23.3 & 43.6 & 53.3 & - & - & - & - & - & - \\
Fitness R-CNN~\cite{tychsen2018improving} & ResNet-101 & 512512 & 10241024 & 41.8 & 60.9 & 44.9 & 21.5 & 45.0 & 57.5 & - & - & - & - & - & - \\
Cascade R-CNN~\cite{cai2018cascade} & ResNet-101 & - & - & 42.8 & 62.1 & 46.3 & 23.7 & 45.5 & 55.2 & - & - & - & - & - & - \\
Grid R-CNN w/ FPN~\cite{Lu2018Grid} & ResNeXt-101 &  1300800 &  1300800 & 43.2 & 63.0 & 46.6 & 25.1 & 46.5 & 55.2 & - & - & - & - & - & - \\
D-RFCN + SNIP (multi-scale)~\cite{singh2018analysis} & DPN-98~\cite{chen2017dual} &  20001200 &  20001200 & 45.7 & \textbf{67.3} & 51.1 & 29.3 & 48.8 & 57.1 & - & - & - & - & - & - \\
PANet (multi-scale)~\cite{liu2018path} & ResNeXt-101 &  1400840 &  1400840 & \textbf{47.4} & 67.2 & \textbf{51.8} & \textbf{30.1} & \textbf{51.7} & \textbf{60.0} & - & - & - & - & - & - \\
\hline
\hline
\textbf{One-stage:} & & & & & & & & & & & & & & &\\
YOLOv2~\cite{redmon2017yolo9000} & DarkNet-19 & 544544 & 544544 & 21.6 & 44.0 & 19.2 & 5.0 & 22.4 & 35.5 & 20.7 & 31.6 & 33.3 & 9.8 & 36.5 & 54.4 \\
DSOD300~\cite{shen2017dsod} & DS/64-192-48-1 & 300300 & 300300 & 29.3 & 47.3 & 30.6 & 9.4 & 31.5 & 47.0 & 27.3 & 40.7 & 43.0 & 16.7 & 47.1 & 65.0 \\
GRP-DSOD320~\cite{shen2017learning} & DS/64-192-48-1 & 320320 & 320320 & 30.0 & 47.9 & 31.8 & 10.9 & 33.6 & 46.3 & 28.0 & 42.1 & 44.5 & 18.8 & 49.1 & 65.0 \\
SSD513~\cite{liu2016ssd} & ResNet-101 & 513513 & 513513 & 31.2 & 50.4 & 33.3 & 10.2 & 34.5 & 49.8 & 28.3 & 42.1 & 44.4 & 17.6 & 49.2 & 65.8 \\
DSSD513~\cite{fu2017dssd} & ResNet-101 & 513513 & 513513 & 33.2 & 53.3 & 35.2 & 13.0 & 35.4 & 51.1 & 28.9 & 43.5 & 46.2 & 21.8 & 49.1 & 66.4 \\
RefineDet512 (single-scale)~\cite{zhang2018single} & ResNet-101 & 512512 & 512512 & 36.4 & 57.5 & 39.5 & 16.6 & 39.9 & 51.4 & - & - & - & - & - & - \\
CornerNet511 (single-scale)~\cite{law2018cornernet} & Hourglass-52 & 511511 & ori. & 37.8 & 53.7 & 40.1 & 17.0 & 39.0 & 50.5 & 33.9 & 52.3 & 57.0 & 35.0 & 59.3 & 74.7 \\
RetinaNet800~\cite{lin2017focal} & ResNet-101 & 800800 & 800800 & 39.1 & 59.1 & 42.3 & 21.8 & 42.7 & 50.2 & - & - & - & - & - & - \\
CornerNet511 (multi-scale)~\cite{law2018cornernet} & Hourglass-52 & 511511 & 1.5 & 39.4 & 54.9 & 42.3 & 18.9 & 41.2 & 52.7 & 35.0 & 53.5 & 57.7 & 36.1 & 60.1 & 75.1 \\
CornerNet511 (single-scale)~\cite{law2018cornernet} & Hourglass-104 & 511511 & ori. & 40.5 & 56.5 & 43.1 & 19.4 & 42.7 & 53.9 & 35.3 & 54.3 & 59.1 & 37.4 & 61.9 & 76.9 \\
RefineDet512 (multi-scale)~\cite{zhang2018single} & ResNet-101 & 512512 & 2.25 & 41.8 & 62.9 & 45.7 & 25.6 & 45.1 & 54.1 &  &  &  &  &  &  \\
CornerNet511 (multi-scale)~\cite{law2018cornernet} & Hourglass-104 & 511511 & 1.5 & 42.1 & 57.8 & 45.3 & 20.8 & 44.8 & 56.7 & 36.4 & 55.7 & 60.0 & 38.5 & 62.7 & 77.4 \\
\hline
\textbf{CenterNet511} (single-scale) & Hourglass-52 & 511511 & ori. & \mycolor{41.6} & \mycolor{59.4} & \mycolor{44.2} & \mycolor{22.5} & \mycolor{43.1} & \mycolor{54.1} & \mycolor{34.8} & \mycolor{55.7} & \mycolor{60.1} & \mycolor{38.6} & \mycolor{63.3} & \mycolor{76.9} \\
\textbf{CenterNet511} (single-scale) & Hourglass-104 & 511511 & ori. & \mycolor{44.9} & \mycolor{62.4} & \mycolor{48.1} & \mycolor{25.6} & \mycolor{47.4} & \mycolor{57.4} & \mycolor{36.1} & \mycolor{58.4} & \mycolor{63.3} & \mycolor{41.3} & \mycolor{67.1} & \mycolor{80.2} \\
\textbf{CenterNet511} (multi-scale) & Hourglass-52 & 511511 & 1.8 & \mycolor{43.5} & \mycolor{61.3} & \mycolor{46.7} & \mycolor{25.3} & \mycolor{45.3} & \mycolor{55.0} & \mycolor{36.0} & \mycolor{57.2} & \mycolor{61.3} & \mycolor{41.4} & \mycolor{64.0} & \mycolor{76.3} \\
\textbf{CenterNet511} (multi-scale) & Hourglass-104 &  511511 & 1.8 & \mycolor{\textbf{47.0}} & \mycolor{\textbf{64.5}} & \mycolor{\textbf{50.7}} & \mycolor{\textbf{28.9}} & \mycolor{\textbf{49.9}} & \mycolor{\textbf{58.9}} & \mycolor{\textbf{37.5}} & \mycolor{\textbf{60.3}} & \mycolor{\textbf{64.8}} & \mycolor{\textbf{45.1}} & \mycolor{\textbf{68.3}} & \mycolor{\textbf{79.7}} \\
\hline
\end{tabular}}
\vspace{-2ex}
\caption{Performance comparison () with the state-of-the-art methods on the MS-COCO test-dev dataset. CenterNet outperforms all existing one-stage detectors by a large margin and ranks among the top of state-of-the-art two-stage detectors.}
\label{tab1}
\vspace{-2ex}
\end{table*}


\subsection{Comparisons with State-of-the-art Detectors}

Table~\ref{tab1} shows the comparison with the state-of-the-art detectors on the MS-COCO test-dev set.

Compared with the baseline CornerNet~\cite{law2018cornernet}, the proposed CenterNet achieves a remarkable improvement. For example, CenterNet511-52 (means that the resolution of input images is  and the backbone is Hourglass-52) reports a single-scale testing AP of , an improvement of  over , and a multi-scale testing AP of , an improvement of  over , achieved by CornerNet under the same setting. When using the deeper backbone (\ie, Hourglass-104), the AP improvement over CornerNet are  (from  to ) and  (from  to ) under the single-scale and multi-scale testing, respectively. These results firmly demonstrate the effectiveness of CenterNet.

Meanwhile, it can be seen that the most contribution comes from the small objects. For instance, CenterNet511-52 improves the AP for small objects by  (single-scale) and by 
 (multi-scale). As for the backbone Hourglass-104, the improvements are  (single-scale) and by  (multi-scale), respectively. The benefit stems from the center information modeled by the center keypoints: the smaller the scale of an incorrect bounding box is, the lower probability a center keypoint can be detected in its central region. Figure~\ref{fig7_12} and Figure~\ref{fig7_22} show some qualitative comparisons, which demonstrate the effectiveness of CenterNet in reducing small incorrect bounding boxes.

CenterNet also leads to a large improvement for reducing medium and large incorrect bounding boxes. As Table~\ref{tab1} shows, CenterNet511-104 improves the single-scale testing AP by  (from  to ) and  (from  to ), respectively. Figure~\ref{fig7_32} and Figure~\ref{fig7_42} show some qualitative comparisons for reducing medium and large incorrect bounding boxes. It is worth noting that the AR is also significantly improved, with the best performance achieved with multi-scale testing. This is because our approach removes lots of incorrect bounding boxes, which is equivalent to improving the confidence of those bounding boxes with accurate locations but lower scores.

When comparing other one-stage approaches, CenterNet511-52 reports  single-scale testing AP. This achievement is already better than those using deeper models (\eg, RetinaNet800~\cite{lin2017focal} and RefineDet~\cite{zhang2018single}). The best performance of CenterNet is AP , dramatically surpassing all the published one-stage approaches to our best knowledge.

At last, one can observe that the performance of CenterNet is also competitive with the two-stage approaches,~\eg,~the single-scale testing AP of CenterNet511-52 is comparable to the two-stage approach Fitness R-CNN~\cite{tychsen2018improving} (~\emph{vs.}~) and that of CenterNet511-104 is comparable to D-RFCN + SNIP~\cite{singh2018analysis} (~\emph{vs.}~), respectively. Nevertheless, it should be mentioned that two-stage approaches usually use larger resolution input images (\eg, ), which significantly improves the detection accuracy especially for small objects. 
The multi-scale testing AP  achieved by CenterNet511-104 closely matches the state-of-the-art AP , achieved by the two-stage detector PANet~\cite{liu2018path}. We present some qualitative detection results in Figure~\ref{qualitative_detection}.
\begin{figure*}[t]
\subfigure{ 
    \includegraphics[height=0.122\textwidth,width=0.07\textheight]{fig7_3.pdf}
    \label{fig7_11}
  } 
  \hspace{-0.1in}
  \subfigure{ 
    \includegraphics[height=0.12\textwidth,width=0.13\textheight]{fig7_4.pdf}
    \label{fig7_21}
  } 
  \hspace{-0.11in}
  \subfigure{  
    \includegraphics[height=0.125\textwidth,width=0.14\textheight]{fig7_5.pdf}
    \label{fig7_31}
  } 
  \hspace{-0.11in}
  \subfigure{ 
    \includegraphics[height=0.12\textwidth,width=0.13\textheight]{fig7_6.pdf}
    \label{fig7_41}
  } 
  \hspace{-0.11in}
  \subfigure{ 
    \includegraphics[height=0.12\textwidth,width=0.13\textheight]{fig7_1.pdf}
    \label{fig7_51}
  }
  \vspace{-0.12in}
  \hspace{-0.11in}
  \subfigure{ 
    \includegraphics[height=0.12\textwidth,width=0.14\textheight]{fig7_2.pdf}
    \label{fig7_61}
  }
  \renewcommand\thesubfigure{(a)}
  \hspace{-0.1in}
  \subfigure[]{ 
    \includegraphics[height=0.122\textwidth,width=0.071\textheight]{fig7_9.pdf}
    \label{fig7_12}
  } 
  \renewcommand\thesubfigure{(b)}
  \hspace{-0.11in}
  \subfigure[]{ 
    \includegraphics[height=0.12\textwidth,width=0.122\textheight]{fig7_10.pdf}
    \label{fig7_22}
  } 
  \renewcommand\thesubfigure{(c)}
  \hspace{-0.04in}
  \subfigure[]{ 
    \includegraphics[height=0.12\textwidth,width=0.13\textheight]{fig7_11.pdf}
    \label{fig7_32}
  } 
  \renewcommand\thesubfigure{(d)}
  \hspace{-0.02in}
  \subfigure[]{ 
    \includegraphics[height=0.12\textwidth,width=0.13\textheight]{fig7_12.pdf}
    \label{fig7_42}
  }
  \renewcommand\thesubfigure{(e)}
  \hspace{-0.1in}
  \subfigure[]{ 
    \includegraphics[height=0.12\textwidth,width=0.13\textheight]{fig7_7.pdf}
    \label{fig7_52}
  } 
  \renewcommand\thesubfigure{(f)}
  \hspace{-0.1in}
  \subfigure[]{ 
    \includegraphics[height=0.12\textwidth,width=0.14\textheight]{fig7_8.pdf}
    \label{fig7_62}
  }
  \vspace{-3ex}
  \caption{(a) and (b) show the small incorrect bounding boxes are significantly reduced by modeling center information. (c) and (d) show that the center information works for reducing medium and large incorrect bounding boxes. (e) shows the results of detecting the center keypoints without/with the center pooling. (f) shows the results of detecting the corners with corner pooling and cascade corner pooling, respectively. The blue boxes above denote the ground-truth. The red boxes and dots denote the predicted bounding boxes and keypoints, respectively.} 
  \label{qualitative}
  \vspace{-2ex}
\end{figure*}


\begin{figure*}[t]
  \centering 
  \subfigure{ 
    \includegraphics[height=0.12\textwidth,width=0.13\textheight]{fig8_1.pdf}
    \label{fig8_1}
  }
  \hspace{-0.05in}
  \subfigure{ 
    \includegraphics[height=0.12\textwidth,width=0.08\textheight]{fig8_2.pdf}
    \label{fig8_2}
  }
  \hspace{-0.05in}
  \subfigure{ 
    \includegraphics[height=0.12\textwidth,width=0.14\textheight]{fig8_3.pdf}
    \label{fig8_3}
  } 
  \hspace{-0.05in}
  \subfigure{ 
    \includegraphics[height=0.12\textwidth,width=0.14\textheight]{fig8_4.pdf}
    \label{fig8_4}
  } 
  \hspace{-0.05in}
  \subfigure{ 
    \includegraphics[height=0.12\textwidth,width=0.09\textheight]{fig8_5.pdf}
    \label{fig8_5}
  } 
  \hspace{-0.05in}
  \subfigure{ 
    \includegraphics[height=0.125\textwidth,width=0.135\textheight]{fig8_6.pdf}
    \label{fig8_6}
  }
  \vspace{-2ex}
  \caption{Some qualitative detection results on the MS-COCO validation dataset. Only detections with scores higher than  are shown.} 
  \label{qualitative_detection}
  \vspace{-2ex}
\end{figure*}


\subsection{Incorrect Bounding Box Reduction}
The AP~\cite{lin2014microsoft} metric reflects how many high quality object bounding boxes (usually ) a network can predict, but cannot directly reflect how many incorrect object bounding boxes (usually ) a network generates. The FD rate is a suitable metric, which reflects the proportion of the incorrect bounding boxes. Table~\ref{FDR2} shows the FD rates for CornerNet and CenterNet. CornerNet generates many incorrect bounding boxes even at  threshold, \ie, CornerNet511-52 and CornerNet511-104 obtain  and  FD rate, respectively. On the other hand, CornerNet generates more small incorrect bounding boxes than medium and large incorrect bounding boxes, which reports  for CornerNet511-52 and  for CornerNet511-104, respectively. Our CenterNet decreases the FD rates at all criteria via exploring central regions. For instance, CenterNet511-52 and CenterNet511-104 decrease  by both . In addition, the FD rates for small bounding boxes decrease the most, which are  by CenterNet511-52 and  by CenterNet511-104, respectively. This is also the reason why the AP improvement for small objects is more prominent.
\begin{table}[tb]
\centering
\resizebox{.48\textwidth}{!}{
\begin{tabular}{|l|ccccccc|}
\hline
 Method & FD & FD & FD & FD & FD & FD & FD\\
\hline
\hline
 CornerNet511-52 & 40.4 & 35.2 & 39.4 & 46.7 & 62.5 & 36.9 & 28.0 \\
 CenterNet511-52 & \textbf{35.1} & \textbf{30.7} & \textbf{34.2} & \textbf{40.8} & \textbf{53.0} & \textbf{31.3} & \textbf{24.4} \\
\hline
\hline
 CornerNet511-104 & 37.8 & 32.7 & 36.8 & 43.8 & 60.3 & 33.2 & 25.1 \\
 CenterNet511-104 & \textbf{32.4} & \textbf{28.2} & \textbf{31.6} & \textbf{37.5} & \textbf{50.7} & \textbf{27.1} & \textbf{23.0} \\
\hline
\end{tabular}}
\vspace{-2ex}
\caption{Comparison of false discovery rates () of CornerNet and CenterNet on the MS-COCO validation dataset. The results suggest CenterNet avoids a large number of incorrect bounding boxes, especially for small incorrect bounding boxes.}
\label{FDR2}
\vspace{-4ex}
\end{table}

\subsection{Inference Speed}
The proposed CenterNet explores the visual patterns within each proposed region with minimal costs. To ensure a fair comparison, we test the inference speed of both CornerNet~\cite{law2018cornernet} and CenterNet on a NVIDIA Tesla P100 GPU. We obtain that the average inference time of CornerNet511-104 is  per image and that of CenterNet511-104 is . Meanwhile, using the Hourglass-52 backbone can speed up the inference speed. Our CenterNet511-52 takes an average of  to process per image, which is faster and more accurate than CornerNet511-104.

\subsection{Ablation Study}
Our work has contributed three components, including central region exploration, center pooling and cascade corner pooling. To analyze the contribution of each individual component, an ablation study is given here. The baseline is CornerNet511-52~\cite{law2018cornernet}. We add the three components to the baseline one by one and follow the default parameter setting detailed in Section~\ref{sec:setting}. The results are given in Table~\ref{ablation}. 
\begin{table*}[tb]
\small
\centering
\begin{tabular}{|*{3}{p{0.785cm}<{\centering}}|*{6}{p{0.71cm}<{\centering}}|*{6}{p{0.71cm}<{\centering}}|}
\hline
CRE & CTP & CCP & AP & AP & AP & AP & AP & AP & AR & AR & AR & AR & AR & AR\\
\hline
\hline
 &  &  & 37.6 & 53.3 & 40.0 & 18.5 & 39.6 & 52.2 & 33.7 & 52.2 & 56.7 & 37.2 & 60.0 & 74.0 \\
 \hline
 &  & \checkmark & 38.3 & 54.2 & 40.5 & 18.6 & 40.5 & 52.2 & 34.0 & 53.0 & 57.9 & 36.6 & 60.8 & 75.8 \\
 \hline
\checkmark &  &  & 39.9 & 57.7 & 42.3 & 23.1 & 42.3 & 52.3 & 33.8 & 54.2 & 58.5 & 38.7 & 62.4 & 74.4 \\
\hline
\checkmark & \checkmark &  & 40.8 & 58.6 & 43.6 & 23.6 & 43.6 & 53.6 & 33.9 & 54.5 & 59.0 & 39.0 & 63.2 & 74.7 \\
\hline
\checkmark & \checkmark & \checkmark & \textbf{41.3} & \textbf{59.2} & \textbf{43.9} & \textbf{23.6} & \textbf{43.8} & \textbf{55.8} & \textbf{34.5} & \textbf{55.0} & \textbf{59.2} & \textbf{39.1} & \textbf{63.5} & \textbf{75.1} \\
\hline
\end{tabular}
\vspace{-2ex}
\caption{Ablation study on the major components of CenterNet511-52 on the MS-COCO validation dataset. The CRE denotes central region exploration, the CTP denotes center pooling, and the CCP denotes cascade corner pooling.}
\label{ablation}
\vspace{-2ex}
\end{table*}

\vspace{1ex}\noindent\textbf{Central region exploration.} To understand the importance of the central region exploration (see CRE in the table), we add a center heatmap branch to the baseline and use a triplet of keypoints to detect bounding boxes. For the center keypoint detection, we only use conventional convolutions. As presented in the third row in Table~\ref{ablation}, we improve the AP by  (from  to ). However, we find that the improvement for the small objects (that is \%) is more significant than that for other object scales. The improvement for large objects is almost negligible (from  to ). This is not surprising because, from a probabilistic point of view, the center keypoint for a small object is easier to be located than that of a large object. 

\vspace{1ex}\noindent\textbf{Center pooling.} To demonstrate the effectiveness of proposed center pooling, we then add the center pooling module to the network (see CTP in the table). The fourth row in Table~\ref{ablation} shows that center pooling improves the AP by  (from  to ). Notably, with the help of center pooling, we improve the AP for large objects by  (from  to ), which is much higher than the improvement using conventional convolutions (\ie,~~\emph{vs.}~). It demonstrates that our center pooling is effective in detecting center keypoints of objects, especially for large objects. Our explanation is that center pooling can extract richer internal visual patterns, and larger objects contain more accessible internal visual patterns. Figure~\ref{fig7_52} shows the results of detecting center keypoints without/with center pooling. We can see the conventional convolution fails to locate the center keypoint for the cow, but with center pooling, it successfully locates the center keypoint.
\begin{table}[!tb]
\small
\centering
\resizebox{.48\textwidth}{!}{
\begin{tabular}{|l|cccccc|}
\hline
Method & AP & AP & AP & AP & AP & AP\\
\hline
\hline
CenterNet511-52 w/o GT & 41.3 & 59.2 & 43.9 & 23.6 & 43.8 & 55.8 \\
CenterNet511-52 w/ GT & \textbf{56.5} & \textbf{78.3} & \textbf{61.4} & \textbf{39.1} & \textbf{60.3} & \textbf{70.3} \\
\hline
\hline
CenterNet511-104 w/o GT & 44.8 & 62.4 & 48.2 & 25.9 & 48.9 & 58.8\\
CenterNet511-104 w/ GT & \textbf{58.1} & \textbf{78.4} & \textbf{63.9} & \textbf{40.4} & \textbf{63.0} & \textbf{72.1} \\
\hline
\end{tabular}
}
\vspace{-2ex}
\caption{Error analysis of center keypoints via using ground-truth. we replace the predicted center keypoints with the ground-truth values, the results suggest there is still room for improvement in detecting center keypoints.}
\label{Error}
\vspace{-2ex}
\end{table}

\vspace{1ex}\noindent\textbf{Cascade corner pooling.} We replace corner pooling~\cite{law2018cornernet} with cascade corner pooling to detect corners (see CCP in the table). The second row in Table~\ref{ablation} shows the results that we test on the basis of CornerNet511-52. We find that cascade corner pooling improves the AP by  (from  to ). The last row shows the results that we test on the basis of CenterNet511-52, which improves the AP by  (from  to ). The results of the second row show there is almost no change in the AP for large objects (\ie,~~\emph{vs.}~), but the AR is improved by  (from  to ). This suggests that cascade corner pooling can ``see" more objects due to the rich internal visual patterns in large objects, but too rich visual patterns may interfere with its perception for the boundary information, leading to many inaccurate bounding boxes. After equipping with our CenterNet, the inaccurate bounding boxes are effectively suppressed, which improves the AP for large objects by  (from  to ). Figure~\ref{fig7_62} shows the result of detecting corners with corner pooling or cascade corner pooling. We can see that cascade corner pooling can successfully locate a pair of corners for the cat on the left while corner pooling cannot.

\subsection{Error Analysis} The exploration of visual patterns within each bounding box depends on the center keypoints. In other words, once a center keypoint is missed, the proposed CenterNet would miss the visual patterns within the bounding box. To understand the importance of center keypoints, we replace the predicted center keypoints with the ground-truth values and evaluate performance on the MS-COCO validation dataset. Table~\ref{Error} shows that using the ground-truth center keypoints improves the AP from  to  for CenterNet511-52 and from  to  for CenterNet511-104, respectively. APs for small, medium and large objects are improved by , , and  for CenterNet511-52 and , , and  for CenterNet511-104, respectively. This demonstrates that the detection of center keypoints is far from the bottleneck.

\section{Conclusions}
\label{Conclusions}
In this paper, we propose CenterNet, which detects objects using a triplet, including one center keypoint and two corners. Our approach addresses the problem that CornerNet lacks an additional look into the cropped regions by exploring the visual patterns within each proposed region with minimal costs. In fact, this is a common defect for all one-stage approaches. As one-stage approaches remove the RoI extraction process, they cannot pay attention to internal information within cropped regions. 

{\bf An intuitive explanation of our contribution lies in that we equip a one-stage detector with the ability of two-stage approaches, with an efficient discriminator being added.} We believe that our idea of adding an extra branch for the center keypoint can be potentially generalized to other existing one-stage approaches (\eg,~SSD~\cite{liu2016ssd}). Meanwhile, some advanced training strategies~\cite{zhuscratchdet} can be used for better performance. We leave as our future work.

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}
\end{document}
