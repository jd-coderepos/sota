\documentclass{article}











\usepackage[final]{neurips_2022}

\usepackage{courier}
\usepackage[T1]{fontenc}
\pdfminorversion=7
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{graphicx}
\usepackage{microtype}      \usepackage{xcolor}         \usepackage{color}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{bbding}
\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{array}
\newcommand\cz[1]{\textcolor{red}{\small\textbf{ [Chenguang: #1]}}}
\newcommand\yujia[1]{\textcolor{black}{{ [Yujia: #1]}}}
\newcommand\yichong[1]{\textcolor{cyan}{ [Yichong: #1]}}
\newcommand\ddc[1]{\textcolor{red}{#1}}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argtopk}{arg\,TopK}
\DeclareMathOperator*{\argtopp}{arg\,TopP}
\definecolor{newblue}{HTML}{1770b3}
\definecolor{hyperref-red}{RGB}{200,0,0}
\usepackage{hyperref}
\definecolor{deeppink}{rgb}{1.0, 0.08, 0.58}
\hypersetup{colorlinks=true,citecolor={newblue},urlcolor={deeppink}}

\title{REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering}


\author{
  Yuanze Lin\thanks{Work done during an internship at Microsoft Redmond.} \quad\quad Yujia Xie \quad\quad Dongdong Chen \quad\\ \textbf{Yichong Xu} \quad\quad \textbf{Chenguang Zhu} \quad\quad \textbf{Lu Yuan} \\
   University of Washington \quad\quad Microsoft \\
{\tt\small{yuanze@uw.edu} \quad\quad\quad  \tt\small{\{yujiaxie, dochen, yicxu\}@microsoft.com}}
}

\begin{document}

\maketitle

\begin{abstract}
This paper revisits visual representation in knowledge-based visual question answering (VQA) and demonstrates that using regional information in a better way can significantly improve the performance. While visual representation is extensively studied in  traditional VQA, it is under-explored in knowledge-based VQA even though these two tasks share the common spirit, i.e., rely on visual input to answer the question. Specifically, we observe that in most state-of-the-art knowledge-based VQA methods: 1) visual features are  extracted either from the whole image or in a sliding window manner for retrieving knowledge, and the important relationship within/among object regions is neglected; 2) visual features are not well utilized in the final answering model, which is counter-intuitive to some extent. Based on these observations, we propose a new knowledge-based VQA method \textbf{\textit{REVIVE}}, which tries to utilize the explicit information of object regions not only in the knowledge retrieval stage but also in the answering model. The key motivation is that object regions and inherent relationship are important for knowledge-based VQA. We perform extensive experiments on the standard OK-VQA dataset and achieve new state-of-the-art performance, \textit{i.e.}, \textbf{58.0\%} accuracy, surpassing previous state-of-the-art method by a large margin (\textbf{+3.6\%}). We also conduct detailed analysis and show the necessity of regional information in different framework components for knowledge-based VQA. Code is publicly available at \url{https://github.com/yzleroy/REVIVE}.

\end{abstract}


\section{Introduction}
Many vision-based decision making processes in our daily life go beyond perception and recognition. For example, if we see a salad bowl in the deli bar, our decision on whether to buy it does not only depend on what is in the bowl, but also the calories in each of the item. This motivates the knowledge-based Visual Question Answering (VQA) task \cite{marino2019ok}, which extends traditional VQA task \cite{antol2015vqa}  to solve more complex problems, \textit{i.e.}, where commonsense knowledge is required to answer the open-domain questions. 

By definition, knowledge-based VQA takes three different information sources to predict the answer: input visual information (image), input question, and the external knowledge. While existing research on knowledge-based VQA mainly focuses on improving the incorporation of external knowledge, this paper focuses on improving the object-centric visual representation and presents a comprehensive empirical study to demonstrate that visual features matter in this task.

Intuitively, visual information should be well used for both knowledge retrieval and final answering. However, we find existing state-of-the-art (SOTA) methods \cite{yang2021empirical,gui2021kat} \textcolor{black}{in such domain} have not fully utilized it. On the one hand, they simply use either the whole image or a sliding window on the image to retrieve the external knowledge. On the other hand, \textcolor{black}{they ignore the essential visual information (\textit{i.e.}, object-centric representations) in the final answering model}. In other words, they fuse only the retrieved knowledge and the question as a pure natural language processing (NLP) model to obtain the answer, a typical method \cite{gui2021kat} is illustrated in Figure \ref{fig:comparison} (b). 

In this paper, we revisit visual representation in knowledge-based VQA, and argue that the information of object regions and their relationship should be considered and used in a dedicated way. The underlying motivation is shown in Figure \ref{fig:comparison} (a), which demonstrates that understanding the objects and their relationship is necessary.  To this end, we propose \textbf{REVIVE} to better utilize \textbf{RE}gional \textbf{VI}sual Representation for knowledge-based \textbf{V}isual qu\textbf{E}stion answering. It not only exploits the detailed regional information for better knowledge retrieval, but also fuses the regional visual representation into the final answering model. Specifically, we first use the object detector GLIP \cite{li2021grounded} to locate the objects, and then use the cropped region proposals to retrieve different types of external knowledge. Finally, we integrate the knowledge together with the regional visual features into a unified transformer based answering model for final answer generation. 

We perform extensive experiments on the OK-VQA dataset \cite{marino2019ok}, and the proposed \emph{REVIVE} achieves the SOTA performance of \textbf{58.0\%} accuracy, a \textbf{3.6\%} absolute improvement from the results of previous SOTA method \cite{gui2021kat}. 

We summarize our contribution as follows:

\begin{enumerate}[label=(\alph*)]
\item We systematically explore how to better exploit the visual feature to retrieve knowledge. The empirical results suggest the region-based approach performs the best, compared to whole image-based and sliding window-based approaches.
\item We integrate the regional visual representation, retrieved external and implicit knowledge into a transformer-based question answering model, which can effectively leverage the three information sources for solving knowledge-based VQA.  

\item Our proposed REVIVE achieves the state-of-the-art performance on OK-VQA dataset, \textit{i.e.}, \textbf{58.0\%} accuracy, surpassing the previous methods by a large margin.


\end{enumerate}

\section{Related Work}

\begin{figure*}
\begin{center}

\includegraphics[scale=0.238]{1.pdf}
\end{center}
\vspace{-3mm}
   \caption{(a) An example from OK-VQA dataset, our method utilizes the retrieved knowledge and object-centric regions to solve the question.  (b) The pipeline of previous state-of-the-art method KAT \cite{gui2021kat}. (c) The pipeline of our proposed \textit{REVIVE}. }
\label{fig:comparison}
\vspace{-2mm}
\end{figure*}

\textbf{Knowledge-Based VQA. } Knowledge-based VQA \cite{marino2019ok} aims to predict answers for general questions by leveraging external knowledge beyond image content. Early works \cite{wang2015explicit,wang2017fvqa} introduce external knowledge to solve visual question answering (VQA) tasks. OK-VQA dataset \cite{marino2019ok} is the first large-scale dataset with questions that need be answered using external knowledge instead of a provided fixed knowledge base \cite{wang2017fvqa}. Recent studies \cite{wang2015explicit,wang2017fvqa,narasimhan2018straight,narasimhan2018out,zhu2020mucko,wu2021multi,marino2021krisp,yang2021empirical,gui2021kat} integrate different knowledge from various external knowledge resources, \textit{e.g.}, ConceptNet \cite{speer2017conceptnet}, Wikipedia \cite{vrandevcic2014wikidata}, \textit{etc}, for solving knowledge-based VQA. Later, PICa \cite{yang2021empirical} regards large language models, e.g., GPT-3 \cite{brown2020language} as an implicit knowledge source and employs it \cite{brown2020language} to get answer prediction based on textual prompts. Inspired by the recent success of knowledge-retrieved methods \cite{izacard2020leveraging,izacard2020distilling} that leverage external knowledge retrieval with language generative models for open-domain question answering, KAT \cite{gui2021kat} exploits the FiD reader \cite{izacard2020leveraging} to perform knowledge reasoning over retrieved implicit and explicit knowledge. Our work instead emphasizes revisiting the visual representation for knowledge retrieval, \textit{i.e.}, resorting to regional visual representation. In addition, we propose to incorporate object-centric regional visual representation together with retrieved knowledge in the answer generative model. Several works \cite{narasimhan2018out, garcia2020knowit,garcia2020knowledge, shah2019kvqa, narasimhan2018straight} have incorporated visual embeddings or captions in predicting the final answers. However, these works target at different settings and they haven't fully explored how to better use regional representations to retrieve knowledge.

\textbf{Vision-Language Models. } Recent years have witnessed the rapid development of vision-language models \cite{vaswani2017attention,jiang2022pseudo,yuan2021florence,su2019vl,li2020oscar,zhang2021vinvl,wang2021ufo, wang2021simvlm}. Those works usually first pre-train a neural network on a large-scale image-text dataset and then finetune the models for solving specific  vision-language tasks. Among them, VinVL \cite{zhang2021vinvl} aims to learn the object-centric representation. CLIP \cite{radford2021learning} pre-trains the models with large-scale text-image pairs by contrastive learning. GLIP \cite{yuan2021florence} reformulates the pre-training process by unifying object detection and phrase grounding. Our method uses the three models as sub-modules to identify object-centric regions and retrieve knowledge for knowledge-based VQA task.


\section{Proposed Method \label{sec:method}}

Knowledge-based VQA task \cite{marino2019ok} seeks to answer questions based on external knowledge beyond images. Specifically, let us denote a knowledge-based VQA dataset as , where ,  and  denote the input image, question and answer of the -th sample respectively, and  is the number of total samples. Given the dataset, the goal is to train a model with parameter  to generate the answer  with input  and . 

In this section, we introduce our method \emph{REVIVE}. Figure \ref{fig:framework} shows an overview of the method. We leverage the detected regions of the input image to obtain the object-centric region features and retrieve explicit knowledge. Meanwhile, we prompt GPT-3 \cite{brown2020language} by regional tags, question and context to retrieve implicit knowledge. After that, the regional visual features, retrieved  knowledge, and the text prompt consists of regional tags, question and context will then be fused into a encoder-decoder module to generate the answer. We explain more details in Section \ref{region_module}, \ref{knowledge_module} and \ref{fusion_module}.

\begin{figure*}
\begin{center}
\includegraphics[scale=0.472]{2.pdf}
\end{center}
    \vspace{-2mm}
   \caption{\textbf{The illustration of \textit{REVIVE.}} It exploits regional information (\textit{i.e.}, features, positions and tags), question and context to retrieve different types of knowledge. In addition, it also incorporates learned object-centric region features with retrieved knowledge for answer generation.}
    \vspace{-2mm}
\label{fig:framework}
\end{figure*}

\subsection{Regional Feature Extraction Module}
\label{region_module}
Given an image , we first adopt a object detector to give us the positions of region proposals,

where  is the set of bouding boxes,  is the number of detected boxes, and  is the object detector.

Here, we adopt  as the visual grounding model GLIP \cite{li2021grounded}. We use the text prompt ``\texttt{Detect:} \texttt{person}, \texttt{bicycle}, \texttt{car}, \texttt{...}, \texttt{toothbrush}", which contains all object categories of MSCOCO dataset \cite{lin2014microsoft}. In this way, the model can provide us with all bounding boxes associated with those categories. 

After we get the bounding boxes  of interested objects from GLIP, 
We crop the image  according to  to obtain region proposals . We then extract the object-centric visual features from the proposals:  where  is the visual embedding of the -th proposal,  is the embedding dimension and  represents the image encoder. Inspired by the strong transferring capability of recent contrastively trained vision-language models, we adopt the visual encoder of CLIP \cite{radford2021learning} as our image encoder . We use the encoding of \texttt{[CLS]} token as the final embedding. 

To understand the relationship between/among the objects, we find it also important to introduce the position information  along with its regional visual features.

In addition to the embeddings, explicitly obtaining the description of each region proposal in the textual format is also helpful for knowledge retrieval. For the contrastively trained vision-language models, the training loss explicitly encourages inner product between the image embedding and the text embedding to be larger if the image and the text are well-aligned. Therefore, such a model is capable of selecting the tags that describe the image from a set of customized tags  by computing the inner product. Denote the language encoder of CLIP as . Given a set of tags ,  is the number of total tags, we compute the inner product between the region proposals and all tags, and adopt the tags with the top- similarities as the description of the region proposals,


where  is the inner product,  denotes the number of the obtained regional tags and  means the retrieved regional tags.

In complement to the localized textual description , we adopt a caption model to explicitly describe the relationships between the major objects and provide more context,

where  is the caption model. For example, in Figure \ref{fig:framework}, the context ``\texttt{Two brown dogs fighting over a red frisbee}'' provides us with the essential relationships between the objects, e.g., fighting \textit{over} a red frisbee. Here, we adopt Vinvl \cite{zhang2021vinvl} as the caption model .

In summary, we extract regional visual and positional information as  and , and textual descriptions for the objects and the relationship between the objects as  and . In the next section, we will elaborate on how we use these regional information sources to retrieve external knowledge.

\subsection{Object-Centric Knowledge Retrieval Module}
\label{knowledge_module}

Inspired by KAT \cite{gui2021kat}, we consider both the explicit knowledge and implicit knowledge. But different from it, we utilize regional visual information to help boost the final performance.

\subsubsection{Explicit Regional Knowledge}
Since the questions from knowledge-based VQA \cite{marino2019ok} are general and open-ended, introducing external knowledge is important for model to generate accurate answers by providing extra and complementary knowledge beyond visual contents of input images.

\textbf{External Knowledge Base. } We construct an external knowledge base  by constructing a subset from Wikidata \cite{vrandevcic2014wikidata} following KAT \cite{gui2021kat}. Specifically, we extract 8 commonly appeared categories, \textit{i.e.},  \texttt{Role}, \texttt{Point of interest}, \texttt{Tool}, \texttt{Vehicle}, \texttt{Animal}, \texttt{Clothing}, \texttt{Company}, \texttt{Sport}, to form the subset . Each item in  consists of an entity and a corresponding description, \textit{e.g.}, one entity and its description can be ``\texttt{pegboard}'' and ``\texttt{board wall covering with regularly-spaced holes for insertion of pegs or hooks}" respectively. 

\textbf{Regional Knowledge Retrieval. } 
As mentioned earlier, vision-language models like CLIP are capable of selecting the most relevant text from a set of texts. We reformat the entries in knowledge base  as ``\{entity\} \texttt{is} \texttt{a} \{description\}", and denote the reformatted text set as . 
We retrieve the top- most relevant knowledge entries among \emph{all the regional proposals} as explicit knowledge ,  

where  denotes the number of retrieved explicit knowledge samples. In our implementation, we use FAISS \cite{johnson2019billion} to speed up the computation of Equation (\ref{retrieve_tag}) and (\ref{retrieve_exp}).

\subsubsection{Implicit Knowledge with Regional Descriptions}
Large language models, \textit{e.g.}, GPT-3 \cite{brown2020language}, not only excel in many language tasks, but also memorize lots of commonsense knowledge from its training corpus \cite{yang2021empirical}. Therefore, we exploit GPT-3 \cite{brown2020language} as our implicit knowledge base by reformulating the task as open-domain question answering.

\textbf{Context-Aware Prompt with Regional Descriptions. }  We design the textual prompt based on question , caption , and \emph{tags} . Different from PICa \cite{yang2021empirical} and KAT \cite{gui2021kat} that use whole-image feature to get the tags, we utilize fine-grained regional features to extract regional tags. Specifically, we adopt the prompt  to be ``\texttt{context:} \{caption\} + \{tags\}. \texttt{question:} \{question\}''. In this way, the language model is also supplemented with regional visual information.


\textbf{Implicit Knowledge Retrieval. } Finally, we query GPT-3 model \cite{brown2020language} which takes the reformulated prompt  as input, and obtain predictive answer. Since some of the questions may have ambiguity, we follow the prompt tuning procedure of PICa \cite{yang2021empirical} and get answer candidates . In addition to answer prediction, we also aim for acquiring corresponding explanation  from GPT-3 model to obtain more context information. To be more specific, the corresponding explanation is acquired by feeding the text prompt ``\{question\} \{answer candidate\}. \texttt{This} \texttt{is} \texttt{because}" into GPT-3. Note that ``\{question\}" and ``\{answer candidate\} are input question  and GPT-3's  answer  for image  respectively. The final retrieved implicit knowledge can be denoted as . 

\subsection{Encoder-Decoder Module with Object-Centric Visual Features}
\label{fusion_module}

Once we've retrieved the explicit and implicit knowledge and the regional information, we utilize the FiD network structure \cite{izacard2020leveraging} to encode and decode retrieved knowledge and regional information.

\textbf{Knowledge Encoder. } For the explicit knowledge, we reformat the input text as ``\texttt{entity:} \{entity\} \texttt{description:} \{description\}'', where the entity and the description is from the entries in the retrieved explicit knowledge . We denote this text as , where . 

For implicit knowledge, we adopt input format as ``\texttt{candidate:} \{answer\} \texttt{evidence:} \{explanation\}'', where answer is the retrieved answer  and explanation is . Here, , where  is the number of answers provided by GPT-3. We denote the input text as .

We then encode the knowledge in textual format by the FiD's encoder \cite{vaswani2017attention}, which is denoted as ,

in which ,  and  means the embedding dimension.

\textbf{Regional Visual Encoder. } We introduce a visual encoder for the regional visual embeddings  and positional coordinates . We feed  and  into two different fully connected layers, stack the outputs into a sequence of vectors, and then feed them into a transformer encoder ,
	
where ,  and  are two different fully-connected layers,  is the concatenation operation along a new dimension.

\textbf{Context-aware Question Encoder. } To better leverage the context information, we replace the input question  by the context-aware prompt , we then encode it by the same transformer encoder ,

where  and  means encoded context-aware question.

\textbf{Generative Decoder. } We have obtained the knowledge encoding  and , visual encoding , and context-aware question encoding . Note that as the outputs of the encoder , they are all sequences of vectors. We then concatenate these vectors along the first dimension, and feed them into the FiD's decoder ,

where  means the generated answer. The cross entropy loss function is adopted to train the model,
	
in which  is the length of the ground truth answer text,  is ground truth text at the position  and  is the model parameters.

\textbf{Model Ensemble. } To generate more accurate answers, one promising method is to leverage multiple trained models, \textit{i.e.}, model ensemble. In our experiments, we just train three models whose initialized seeds are different, and then the most frequent result among the generated results from these three models is selected as final answer prediction for each sample.

\subsection{Relationship to Existing Works}
Inspired by KAT \cite{gui2021kat}, \textit{REVIVE} also retrieves two types of knowledge, \textit{i.e.}, implicit and explicit knowledge. \textcolor{black}{Different from KAT, we explore how to better use visual features to retrieve knowledge. Motivated by the fact that the retrieved knowledge should also corresponds to individual concepts in the images in addition to the global theme, we use extracted regional features to retrieve external knowledge, and use regional descriptions to obtain the implicit knowledge. Moreover, we integrate the visual representation of object regions with retrieved knowledge in the answer generative model}. The pipeline differences between KAT \cite{gui2021kat} and our method can be explained in Figure \ref{fig:comparison}.

There're two works \cite{wu2021multi, marino2021krisp} that leverage visual regions for knowledge-based VQA as well. However, MAVEx \cite{wu2021multi} considers object regions as a kind of knowledge without using their visual representation to retrieve other knowledge, KRISP \cite{marino2021krisp} utilizes object regions to learn implicit knowledge by a transformer-based model and retrieve external knowledge by the text symbols of these regions, while our proposed \textit{REVIVE} explores how to better leverage visual representation to retrieve knowledge and integrate their visual features with retrieved knowledge into the answering model.

\section{Experiments}

\subsection{Experimental Setup}
\label{setup}

\textbf{Dataset. } OK-VQA dataset \cite{marino2019ok} is selected for evaluation, which is currently the largest knowledge-based VQA dataset. OK-VQA dataset includes 14055 questions associated with 14031 images from MSCOCO dataset \cite{lin2014microsoft}. Its questions cover a variety of knowledge categories, and are annotated by Amazon Mechanical Turkers. The training and testing split consist of 9009 and 5046 samples respectively. Each data sample is made up of one question, one corresponding image and 10 ground-truth answers. To construct the general domain tag set , we collect the most frequently searched 400K queries in Bing Search as the tags. 

\textbf{Pre-processing. } We utilize the pre-trained visual grounding model GLIP-T \cite{li2021grounded} to detect object-centric region proposals by using its default prompt ``\texttt{Detect:} \texttt{person}, \texttt{bicycle}, \texttt{car}, \texttt{...}, \texttt{toothbrush}", which contains all object categories of MS-COCO dataset \cite{lin2014microsoft}. The captions of images are obtained by the pre-trained Vinvl-Large model \cite{zhang2021vinvl}. For explicit knowledge and regional tag retrieval, we choose CLIP model (ViT-B/16 variant) \cite{radford2021learning}. In our experiments, we adopt , ,  and  as 5, 40, 36 and 30 respectively. Note that the models of CLIP, GLIP, Vinvl and GPT-3 are all frozen during usage.

\textbf{Implementation Details. } We use 4  NVIDIA V100 32Gb to train models for 10K steps, with a batch size of 8. The learning rate is  and AdamW \cite{loshchilov2017decoupled} is chosen as optimizer. The warm-up steps are 1K and the trained models are evaluated every 500 steps. We initialize our model with the pre-trained T5 model \cite{raffel2019exploring}, \textit{i.e.}, T5-large, following KAT \cite{gui2021kat}. The encoder  in Equation (\ref{vis_v}) consists of 9 transformer layers \cite{vaswani2017attention}. Note that we evaluate the prediction results after normalization, and the normalization process mainly includes removing articles, punctuation and duplicated whitespace and lowercasing \cite{chen2017reading, lee2019latent}. 

\textbf{Evaluation Metric. } In our experiments, we choose the soft accuracy of VQAv2 \cite{antol2015vqa} as evaluation metric for comparison.

\begin{table}
  \caption{Results comparison with existing methods on OK-VQA dataset \cite{marino2019ok}, the evaluation metric (\textit{i.e.}, accuracy) is in \%.}
  \label{sota}
  \centering
  \begin{tabular}{p{4.6cm}<{\centering} | p{6cm}<{\centering} | p{2cm}<{\centering}}
    \toprule
    Method & Knowledge Resources & Accuracy (\%) \\
    \midrule
    Q only \cite{marino2019ok} & - & 14.9 \\
    MLP \cite{marino2019ok} & - & 20.7 \\
    BAN \cite{marino2019ok} & - & 25.1 \\
    BAN+AN \cite{marino2019ok} & Wikipedia & 25.6 \\
    MUTAN \cite{marino2019ok} & - & 26.4 \\
    BAN+KG-AUG \cite{li2020boosting} & Wikipedia+ConceptNet & 26.7 \\
    MUTAN+AN \cite{marino2019ok} & Wikipedia & 27.8 \\
    ConceptBERT \cite{garderes2020conceptbert} & ConceptNet & 33.7 \\
    KRISP \cite{marino2021krisp} & Wikipedia + ConceptNet & 38.4 \\
    Visual Retriever-Reader \cite{luo2021weakly} & Google Search & 39.2 \\
    MAVEx \cite{wu2021multi} & Wikipedia+ConceptNet+Google Images & 39.4 \\
    PICa-Base \cite{yang2021empirical} & Frozen GPT-3 (175B) & 43.3 \\
    PICa-Full \cite{yang2021empirical} & Frozen GPT-3 (175B) & 48.0 \\
    KAT (Single) \cite{gui2021kat} & Wikidata+Frozen GPT-3 (175B) & 53.1 \\ 
    KAT (Ensemble) \cite{gui2021kat} & Wikidata+Frozen GPT-3 (175B) & 54.4 \\ 
    \midrule
    REVIVE (Single) & Wikidata+Frozen GPT-3 (175B) & \textbf{56.6} \\
    REVIVE (Ensemble) & Wikidata+Frozen GPT-3 (175B) & \textbf{58.0} \\
    \bottomrule
  \end{tabular}
  \vspace{-4.5mm}
\end{table}

\subsection{Comparison with State-of-the-art Methods}
\label{comparison}
As shown in Table \ref{sota}, we can see that previous works (\textit{e.g.}, KRISP \cite{marino2021krisp}, Visual Retriever-Reader \cite{luo2021weakly} and MAVEx \cite{wu2021multi}) achieve similar performances, about 38.4\% to 39.4\% accuracy. Until recently, PICa \cite{yang2021empirical} is the first one that exploits the pre-trained language model GPT-3 \cite{brown2020language} as knowledge base for knowledge-based VQA task and KAT \cite{gui2021kat} further introduces Wikidata \cite{vrandevcic2014wikidata} as an external knowledge resource, these two works obtain significant performances compared with previous ones.

The proposed \textit{REVIVE} can outperform all existing methods by large margins. Specifically, even using the same knowledge resources (\textit{i.e.}, Wikidata \cite{vrandevcic2014wikidata} and GPT-3 \cite{brown2020language}), our single model can achieve \textbf{56.6\%} accuracy versus previous state-of-the-art method KAT's \textbf{53.1\%} accuracy, when using model ensemble, our method can achieve \textbf{58.0\%} accuracy compared with KAT's \textbf{54.4\%} accuracy. These results demonstrate the effectiveness of the proposed approach.

\subsection{Ablation Study}
\label{ablation}
Next, we conduct extensive ablation studies on the single model to figure out the influence of each component of \textit{REVIVE}. 

\textbf{Effect of Region Proposal Number. } 
We perform the ablation study to figure out the effect of using different region proposal numbers. The results are displayed in Table \ref{region_proposal}. It can be observed that when the region proposal number is 36, the model achieves optimal performance. We conjecture that when the number of region proposals is too large, there are some meaningless and noisy region proposals, while if the number of region proposals is too small, many essential object-centric regions are ignored, which both hurt the model's performance.

\textbf{Different Knowledge Retrieval Methods. } The way of utilizing visual representation for retrieving knowledge plays an important role in knowledge-based VQA. We show the results of using three kinds of knowledge retrieval methods, \textit{i.e.}, image-based, sliding window-based and region-based, in Table \ref{retrieve_method}. Note that \textit{sliding window-based} approach follows KAT \cite{gui2021kat}. Specifically, we first resizes input images to 384  384 and then crop the images with a sliding window whose size is  and stride size is 128. We can observe that the proposed \textit{region-based} approach achieves best performance and surpasses \textit{sliding window-based} method by \textbf{1.8\%} points, which can validate the effectiveness of exploiting region-based visual representation for retrieving knowledge.

\textbf{Effect of Regional Tag Number. } In order to introduce more semantics into contexts, we propose to add region-aware descriptions (\textit{i.e.}, regional tags) behind given contexts. We report the results of using different regional tag number for text prompt  in Table \ref{region_tag}. The results show that when the number of regional tags is 30, it achieves optimal performances. In fact, when the number of regional tags is too large, we'll retrieve relatively irrelevant object tags, sacrificing the model's performance.

\begin{table}[t]
    \begin{minipage}{.45\linewidth}
      \caption{Ablation study on using different region proposal number.}
      \centering
          \label{region_proposal}
\begin{tabular}{p{3.25cm}<{\centering} p{2.25cm}<{\centering}}
            \toprule
            \# of region proposals & Accuracy (\%) \\
            \midrule
            5 &  54.7 \\
            18 &  55.8 \\
            36 & \textbf{56.6} \\
            50 &  56.2 \\
            \bottomrule
        \end{tabular}
          \label{table2}
    \end{minipage}\hspace{5mm}
    \begin{minipage}{.508\linewidth}
      \centering
        \caption{Ablation study on using different regional tag number.}
              \label{region_tag}
        \begin{tabular}{p{3.75cm}<{\centering} p{2.4cm}<{\centering}}
            \toprule
            \# of regional tags & Accuracy (\%) \\
            \midrule
            8 &  56.2 \\
            24 & 56.4  \\
            30 & \textbf{56.6} \\
            50 & 56.3 \\
            \bottomrule
        \end{tabular}
    \end{minipage} 
\vspace{-4mm}
\end{table}

\begin{table}[t]
\begin{minipage}{.45\linewidth}
      \caption{Ablation study on adopting bounding box coordinates.}
      \centering
            \label{coordinate}
        \begin{tabular}{p{3.25cm}<{\centering} p{2.25cm}<{\centering}}
            \toprule
            Positional coordinates & Accuracy (\%) \\
            \midrule
            \xmark &  55.8 \\
            \cmark & \textbf{56.6} \\
            \bottomrule
        \end{tabular}
    \end{minipage}\hspace{5mm}
    \begin{minipage}{.502\linewidth}
      \centering
        \caption{Ablation study on adopting different methods for retrieving knowledge.}
          \label{retrieve_method}
        \begin{tabular}{p{3.65cm}<{\centering} p{2.55cm}<{\centering}}
            \toprule
             Method & Accuracy (\%) \\
            \midrule
            Image-based & 53.2 \\
            Sliding window-based  & 54.8 \\
            Region-based & \textbf{56.6} \\
            \bottomrule
        \end{tabular}
    \end{minipage}
\vspace{-3mm}
\end{table}

\textbf{Effect of Positional Coordinates. } In addition to incorporating visual representation of object-centric region proposals into the model, we also adopt the position information (\textit{i.e.}, positional coordinates). The results of whether using positional coordinates are reported in Table \ref{coordinate}. Introducing regional coordinates can improve the performance by \textbf{0.8\%} points.

\textbf{Effect of Each Component.}  \textcolor{black}{Finally, we showcase the results of using different components of \textit{REVIVE} in Table \ref{component}. We can observe that the introduced components can consistently improve the model's performance. Especially for knowledge retrieval, using the regional descriptions can improve the performance of implicit knowledge by \textbf{1.2\%}, while adopting the regional features can boost the performance of explicit knowledge retrieval by \textbf{1.1\%}.}

\textcolor{black}{The object-centric region features can achieve \textbf{1.4\%} points improvement, feeding context-aware questions, which can be denoted as prompt ``\texttt{context:} \{caption\}. \texttt{question:} \{question\}'', into the answer generative model attain \textbf{0.5\%} points gain, further introducing regional descriptions (\textit{i.e.}., regional tags) into contexts, \textit{i.e.}, prompt , has \textbf{0.7\%} points improvement. These results can validate the efficiencies of our proposed components.}

\begin{table}[H]
    \vspace{-3mm}
      \caption{\textcolor{black}{Ablation study on different components of \textit{REVIVE}. Note that ``\textit{Imp.}" and ``\textit{R-Imp.}" mean implicit knowledge retrieved without and with the proposed regional descriptions, ``\textit{Exp.}" and ``\textit{R-Exp.}" mean explicit knowledge retrieved without and with regional features, ``\textit{Visual}" represents object-centric region features, ``\textit{Context}" and  ``\textit{Tag}" mean introducing the contexts and regional descriptions (\textit{i.e.} tags) into the final answering model respectively. \textit{``Acc."} means accuracy.}}
      \centering
            \label{component}
        \begin{tabular}{p{1.3cm}<{\centering} p{1.3cm}<{\centering} p{1.3cm}<{\centering} p{1.3cm}<{\centering} p{1.3cm}<{\centering} p{1.3cm}<{\centering} p{1.3cm}<{\centering}  p{1.4cm}<{\centering}}
            \toprule
            Imp. & R-Imp. & Exp. & R-Exp. & Visual & Context & Tag & Acc. (\%)\\
            \midrule
             \cmark & & & & &  & & 51.2 \\
                   \cmark & \cmark & & & & & & 52.4 \\
                 \cmark & \cmark & \cmark & & &  & & 52.9 \\
                \cmark & \cmark & \cmark & \cmark & &  & & 54.0 \\
            \cmark  & \cmark & \cmark & \cmark & \cmark & & & 55.4 \\
             \cmark & \cmark & \cmark & \cmark & \cmark & \cmark& & 55.9 \\
            \cmark  & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark & 56.6 \\
            \bottomrule
        \end{tabular}
        \vspace{-1mm}
\end{table}

\subsection{Quantitative Result Analysis}
\label{error}
Finally, we present the quantitative results and provide analysis for error cases, so that we can have a clear insight into the proposed approach.

\begin{figure*}[t]
\begin{center}
\includegraphics[scale=0.473]{3.pdf}
\end{center}
    \vspace{-3mm}
   \caption{Representative success cases of the proposed \textit{REVIVE} on OK-VQA dataset \cite{marino2019ok}. \textit{``Q"}, \textit{``C"}, \textit{``A"} and \textit{``GT"} denote question, context, predictive answer, ground truth answers respectively. Note that the underlined text represents regional tags and five tags are selected for illustration. We rescale all the object regions to the same size for a clearer view. \textit{``Acc."} means accuracy.}
\vspace{-4mm}
\label{fig:visualuzation}
\end{figure*}

\begin{figure*}[t]
\begin{center}
\includegraphics[scale=0.47]{4.pdf}
\end{center}
    \vspace{-3mm}
   \caption{Representative failure cases of the proposed \textit{REVIVE} on OK-VQA dataset \cite{marino2019ok}.}
\vspace{-4mm}
\label{fig:error}
\end{figure*}

\textbf{Visualizing Results. } The success cases of our approach are shown in Figure \ref{fig:visualuzation}. We can observe that our approach can accurately retrieve implicit and explicit knowledge, which corresponds to the detected object regions, and deal with the relationship among these object areas. For example, in the left example of Figure \ref{fig:visualuzation}, our method can recognize the potential referred objects and retrieve useful knowledge (\textit{e.g.}, cheeseburger and cheese), thus generating the correct answer, while in the right example, our method can also retrieve important knowledge (\textit{e.g.}, brazilian terrier) to answer the breed of the referred dog.

\textbf{Failure Cases Analysis. } We showcase the failure examples in Figure \ref{fig:error}. As shown in the left example, even though the prediction result \textit{Cabin} doesn't appear in the ground truth answers, the generated answer of our approach is still reasonable for such scenario. For the right example, our predicted result is wrong due to the difficulty of answering such a general question. From figure \ref{fig:error}, we can also observe that our method can generate useful object-centric regions and accurately retrieve corresponding knowledge, especially explicit knowledge, which can demonstrate the potential of the proposed method.

\section{Limitations and Broader Impact}
The quality of constructed Wikidata subset and designed textual prompt can influence final retrieved knowledge. In addition, the detector for obtaining region proposals also affect retrieved knowledge and visual features, all these factors affect the models' performances.

This paper proposes a novel approach \textit{REVIVE} for knowledge-based VQA. \textit{REVIVE} can help models to efficiently use visual and language information sources to answer open-domain questions. It can also generalize to real-life products, \textit{e.g.}, dialogue robot. However, the failure cases of \textit{REVIVE} will be negative to the society when using it as the educational technique. \textcolor{black}{There may also exist certain forms of bias,  \textit{i.e.}, the model may predict biased answers if the training data of knowledge-based VQA contain certain bias. For example, \cite{agrawal2018don} suggests the model may be driven by superficial correlations in the training data, and \cite{hirota2022gender} shows the VQA datasets may contain gender and racial bias that may cause the
models to learn harmful stereotypes.}

\bibliographystyle{plain} 
\bibliography{revive_paper}

\newpage
\appendix 

\section{Overview}
In the supplementary materials, we provide the following sections:

\begin{enumerate}[label=(\alph*)]
\item Implementation details of implicit knowledge retrieval in Section \ref{implementation}.
 
\item Ablation study experiments in Section \ref{sup_ablation}.
   
\item Visualization results in Section \ref{visualization}.

\end{enumerate}

\section{Implementation Details of Implicit Knowledge Retrieval}
\label{implementation}
We first describe more implementation details of implicit knowledge retrieval of the proposed \textit{REVIVE}. Specifically, we explain how we extract multiple answer candidates.

\textbf{Multiple Candidates. } We retrieve multiple implicit knowledge candidates for each sample during training and inference stages to improve the robustness of answer generation. Specifically, we follow PICa \cite{yang2021empirical}, which proposes to use multi-query ensemble, \textit{i.e.}, they prompt the GPT-3 \cite{brown2020language} for  times and choose the one with the highest probability as final answer prediction. Compared with PICa's multi-query ensemble approach, we take all these  predictions from GPT-3 \cite{brown2020language} as implicit knowledge candidates. Note that for each candidate, we also prompt the GPT-3 model to obtain its corresponding explanation. In our experiments, we just retrieve 5 (\textit{i.e.}, ) implicit knowledge candidates and corresponding explanations.

\section{Ablation Study}
\label{sup_ablation}

Next, we conduct more ablation study experiments to provide deeper insight into the components of our proposed \textit{REVIVE}.

\textbf{The effect of multiple implicit knowledge candidates. } To validate the influence of the number of retrieved implicit knowledge candidate on the model's performance, we report the results in Table \ref{implicit}. When using only one implicit knowledge candidate, the model can achieve \textbf{55.8\%} accuracy, after taking 5 implicit knowledge candidates, the performance can be improved to \textbf{56.6\%} accuracy. However, when the retrieved candidate number is 8, we can see that the performance isn't the best, we conjecture that it's enough to include essential candidates when  5. Due to certain incorrect answer predictions by GPT-3, larger  may introduce incorrect and unnecessary candidates, thus hurting the model's performance by using too much noisy and misleading knowledge.

\textbf{The effect of explicit knowledge number. } Since the number of retrieved explicit knowledge samples can have an effect on the model's performance, we conduct the experiments and show the results in Table \ref{explicit}. We find the model can achieve optimal performance when  40. It's reasonable to see that a too large  (\textit{i.e.},  50) cannot let the model achieve optimal performance, since when  increases, there will exist certain retrieved explicit knowledge samples which have relatively low confidences, thus introducing unreliable knowledge and hurting the model's performance.

\textbf{The effect of using different detectors. } To figure out the effect of choosing different object detectors on the final performances, we show the results of using Faster R-CNN \cite{ren2015faster} and GLIP \cite{li2021grounded} in Table \ref{detector}. We can see that Faster R-CNN with ResNet-50 and ResNet-101 as the backbone can achieve \textbf{55.3\%} and \textbf{55.6\%} accuracy respectively, and using the GLIP as the object detector can achieve the optimal performance (\textit{i.e.}, \textbf{56.6\%}). These results demonstrate the accuracy of detecting object regions play an important role in the final performances.

\section{Visualization Results}
\label{visualization}

\begin{table}[!]
    \begin{minipage}{.45\linewidth}
      \caption{Ablation study on using different implicit knowledge candidates.  represents the number of retrieved implicit knowledge candidates.}
      \centering
          \label{implicit}
        \begin{tabular}{p{3.25cm}<{\centering} p{2.25cm}<{\centering}}
            \toprule
             & Accuracy (\%) \\
            \midrule
            1 &  55.8 \\
            3 &  56.3 \\
            5 & \textbf{56.6} \\
            8 &  56.4 \\
            \bottomrule
        \end{tabular}
    \end{minipage}\hspace{5mm}
    \begin{minipage}{.508\linewidth}
      \centering
        \caption{Ablation study on using different explicit knowledge numbers.  represents the number of retrieved explicit knowledge samples.}
          \label{explicit}
        \begin{tabular}{p{3.75cm}<{\centering} p{2.4cm}<{\centering}}
            \toprule
             & Accuracy (\%) \\
            \midrule
            10 & 55.6  \\
            20 & 55.9 \\
            30 & 56.2 \\
            40 & \textbf{56.6} \\
            50 &  56.3\\
            \bottomrule
        \end{tabular}
    \end{minipage} 
\end{table}

\begin{table}[!]
    \begin{minipage}{\linewidth}
      \caption{\textcolor{black}{Ablation study on using different object detectors. Note that Faster R-CNN (R50) and Faster R-CNN (R101) mean using ResNet-50 \cite{he2016deep} and ResNet-101 \cite{he2016deep} as backbones.}}
      \centering
          \label{detector}
        \begin{tabular}{p{4.7cm}<{\centering} p{3.2cm}<{\centering}}
            \toprule
            Detector & Accuracy (\%) \\
            \midrule
            Faster R-CNN (R50) & 55.3  \\
            Faster R-CNN (R101) & 55.6  \\
            GLIP &  \textbf{56.6} \\
            \bottomrule
        \end{tabular}
    \end{minipage}\end{table}

\textcolor{black}{Finally, we showcase more visualization cases in Figure \ref{fig:sup_imp}, \ref{fig:sup_vis}, \ref{fig:sup_vis2}, \ref{fig:sup_vis3} and \ref{fig:sup_vis4}. In Figure \ref{fig:sup_imp}, using the proposed regional descriptions/tags, we can retrieve more accurate implicit knowledge. Taking the top example of Figure \ref{fig:sup_imp} for explanation, without introducing the informative regional descriptions (e.g., ``sunlight" and ``sun"), we cannot generate the correct implicit knowledge candidate ``Sun", since the ``Lamp" is also reasonable when given the question and context, which can demonstrate the effectiveness of using the regional descriptions for implicit knowledge retrieval.}

In Figure \ref{fig:sup_vis}, \ref{fig:sup_vis2}, \ref{fig:sup_vis3} and \ref{fig:sup_vis4}, we can see that our proposed method can focus on important object-centric areas, and then retrieve relevant knowledge for corresponding regional areas, which can be used to generate accurate answers. These visualization results can demonstrate the effectiveness and potential of the proposed \textit{REVIVE}.


\begin{figure*}[!]
\begin{center}
\includegraphics[scale=0.473]{sup_imp.pdf}
\end{center}
    \vspace{-2mm}
   \caption{The implicit knowledge retrieval visualization results without and with the proposed regional descriptions/tags. Note that \textit{``Imp."} and \textit{``R-Imp."} mean the implicit knowledge retrieved without and with the regional descriptions/tags. ``Regional Tags" represents the proposed regional descriptions. ``Context" means the caption. We only use 10 regional tags for illustration.}
\label{fig:sup_imp}
\end{figure*}

\newpage 

\begin{figure*}[!ht]
\begin{center}
\includegraphics[scale=0.472]{sup1.pdf}
\end{center}
    \vspace{-3mm}
   \caption{Representative visualization cases of the proposed \textit{REVIVE} on OK-VQA dataset \cite{marino2019ok}. \textit{``Q"}, \textit{``C"}, \textit{``A"} and \textit{``GT"} denote question, context, predictive answer, ground truth answers respectively. Note that the underlined text represents regional tags and five tags are selected for illustration. We rescale all the object regions to the same size for a clearer view. \textit{``Acc."} means accuracy.}
\vspace{-3mm}
\label{fig:sup_vis}
\end{figure*}

\vspace{20mm}

\begin{figure*}[!ht]
\begin{center}
\includegraphics[scale=0.472]{sup2.pdf}
\end{center}
    \vspace{-3mm}
   \caption{Representative visualization cases of the proposed \textit{REVIVE} on OK-VQA dataset \cite{marino2019ok}. \textit{``Q"}, \textit{``C"}, \textit{``A"} and \textit{``GT"} denote question, context, predictive answer, ground truth answers respectively. Note that the underlined text represents regional tags and five tags are selected for illustration. We rescale all the object regions to the same size for a clearer view. \textit{``Acc."} means accuracy.}
\vspace{-3mm}
\label{fig:sup_vis2}
\end{figure*}

\newpage

\begin{figure*}[!ht]
\begin{center}
\includegraphics[scale=0.472]{sup3.pdf}
\end{center}
    \vspace{-3mm}
   \caption{\textcolor{black}{Representative visualization cases of the proposed \textit{REVIVE} on OK-VQA dataset \cite{marino2019ok}. \textit{``Q"}, \textit{``C"}, \textit{``A"} and \textit{``GT"} denote question, context, predictive answer, ground truth answers respectively. Note that the underlined text represents regional tags and five tags are selected for illustration. We rescale all the object regions to the same size for a clearer view. \textit{``Acc."} means accuracy.}}
\vspace{-3mm}
\label{fig:sup_vis3}
\end{figure*}

\vspace{20mm}

\begin{figure*}[!ht]
\begin{center}
\includegraphics[scale=0.472]{sup4.pdf}
\end{center}
    \vspace{-3mm}
   \caption{\textcolor{black}{Representative visualization cases of the proposed \textit{REVIVE} on OK-VQA dataset \cite{marino2019ok}. \textit{``Q"}, \textit{``C"}, \textit{``A"} and \textit{``GT"} denote question, context, predictive answer, ground truth answers respectively. Note that the underlined text represents regional tags and five tags are selected for illustration. We rescale all the object regions to the same size for a clearer view. \textit{``Acc."} means accuracy.}}
\vspace{-3mm}
\label{fig:sup_vis4}
\end{figure*}

\end{document}