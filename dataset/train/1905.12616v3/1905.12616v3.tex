\documentclass{article}





\usepackage[final]{neurips_2019}



\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{enumerate}      


\usepackage{savesym}
\savesymbol{iint} \savesymbol{iiint} \savesymbol{iiiint} \savesymbol{idotsint} 

\usepackage{txfonts}        \restoresymbol{TXF}{iint}
\restoresymbol{TXF}{iiint}
\restoresymbol{TXF}{iiiint}
\restoresymbol{TXF}{idotsint}


\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{mathtools}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}


\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{caption}
\usepackage{csquotes}
\usepackage{mathabx}
\usepackage[normalem]{ulem}
\usepackage{xcolor,colortbl, makecell}
\usepackage{multicol}
\usepackage{soul}
\usepackage{framed}

\newcommand{\draftcomment}[3]{\textcolor{#3}{[#1 #2]}}
\newcommand{\rowan}[1]{\draftcomment{RZ}{#1}{purple}}
\newcommand{\yejin}[1]{\draftcomment{YC}{#1}{cyan}}
\newcommand{\hannah}[1]{\draftcomment{HR}{#1}{blue}}
\newcommand{\ari}[1]{\draftcomment{AH}{#1}{green}}
\newcommand{\franzi}[1]{\draftcomment{FR}{#1}{violet}}



\setcitestyle{authoryear,round,citesep={;},aysep={,},yysep={;}}












\newcommand{\modelname}{{\textsc{Grover}}}
\newcommand{\modelnamefordisc}{{\textsc{Grover}}}
\newcommand{\modelnamelong}{{\textbf{G}enerating a\textbf{R}ticles by \textbf{O}nly \textbf{V}iewing m\textbf{E}tadata \textbf{R}ecords}}
\newcommand{\datasetname}{{\textsc{RealNews}}}

\newcommand{\commentoutforneurips}[1]{}



\title{Defending Against Neural Fake News}





\author{Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk \\ \textbf{Ali Farhadi, Franziska Roesner, Yejin Choi}\\
  Paul G. Allen School of Computer Science \& Engineering, University of Washington \\
  Allen Institute for Artificial Intelligence \\
  \url{https://rowanzellers.com/grover}
}
\begin{document}


\maketitle

\begin{abstract}
 \vspace{-2mm}
Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate \emph{neural fake news}: targeted propaganda that closely mimics the style of real news.

Modern computer security relies on careful \emph{threat modeling}: identifying potential threats and vulnerabilities from an adversary's point of view, and exploring potential mitigations to these threats.
Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called \modelname. Given a headline like `Link Found Between Vaccines and Autism,' \modelname~can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation.

Developing robust verification techniques against generators like \modelname~is critical.
We find that best current discriminators can classify neural fake news from real, human-written, news with 73\% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against \modelname~turns out to be \modelname~itself, with 92\% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias -- and sampling strategies that alleviate its effects -- both leave artifacts that similar discriminators can pick up on.
We conclude by discussing ethical issues regarding the technology, and plan to release \modelname~publicly, helping pave the way for better detection of neural fake news.  \vspace{-2mm}
\end{abstract}

\section{Introduction}
\begin{figure}[b!]
\vspace{-3mm}
  \centering\small
    \includegraphics[width=1\columnwidth]{figures/fakenewsteaseranonymous.pdf}
    \vspace{-4mm}
\caption{In this paper, we explore \modelname, a model which can detect \emph{and generate} neural fake news. Humans find the articles difficult to distinguish from ``real news'' without high levels of scrutiny.}
\vspace{-5mm}
  \label{fig:teaser}
\end{figure}

Online fake news -- news designed to intentionally deceive -- has recently emerged as a major societal problem. Malicious actors spread fallacious viral stories in order to gain advertising revenue, influence opinions, and even tip elections \citep{faris2017partisanship, wardle2017information}. As such, countering the spread of disinformation online presents an urgent technical and political issue.

To the best of our knowledge, most disinformation online today is manually written \citep{vargo2018agenda}. However, as progress continues in natural language generation, malicious actors will increasingly be able to controllably generate realistic-looking propaganda at scale. Thus, while we are excited about recent progress in text generation \citep{Jzefowicz2016ExploringTL,radford2018improving,radford2019gpttwo}, we are also concerned with the inevitability of AI-generated `neural' fake news.\footnote{
We thank past work, such as \href{https://openai.com/blog/better-language-models/}{OpenAI's Staged Release Policy for GPT2} for drawing attention to neural disinformation, alongside other dual-use implications.
}

With this paper, we seek to understand and respond to neural fake news \emph{before} it manifests at scale. We draw on the field of computer security, which relies on \emph{threat modeling}: analyzing the space of potential threats and vulnerabilities in a system to develop robust defenses. To scientifically study the risks of neural disinformation, we present a new generative model called \modelname.\footnote{Short for \modelnamelong.} Our model allows for controllable yet efficient generation of an entire news article -- not just the body, but also the title, news source, publication date, and author list. This lets us study an adversary with controllable generations 
(e.g. Figure~\ref{fig:teaser}, an example anti-vaccine article written in the style of the New York Times).

Humans rate the disinformation generated by \modelname~as trustworthy, even more so than human-written disinformation. Thus, developing robust verification techniques against generators such as \modelname~is an important research area.
We consider a setting in which a discriminator has access to 5000 \modelname~generations, but unlimited access to real news. In this setting, the best existing fake news discriminators 
are, themselves, deep pretrained language models (73\% accuracy) \citep{peters2018deep,radford2018improving,radford2019gpttwo,devlin2018bert}. However, we find that \modelname,~when used in a discriminative setting, performs even better at 92\% accuracy. This finding represents an exciting opportunity for defense against neural fake news: the best models for generating neural disinformation are also the best models at detecting it. 

Next, we investigate how deep pretrained language models distinguish between real and machine-generated text. We find that key artifacts are introduced during generation as a result of exposure bias: the generator is not perfect, so randomly sampling from its distribution results in generations that fall increasingly out-of-distribution as length increases. However, sampling strategies that alleviate these effects also introduce artifacts that strong discriminators can pick up on.

We conclude with a sketch of the ethical territory that must be mapped out in order to understand our responsibilities as researchers when studying fake news, and the potential negative implications of releasing models \citep{hecht2018s, zellers2019whywereleasedgrover, solaiman2019release}.
Accordingly, we suggest a provisional policy of how such models should be released and why we believe it to be safe  -- and perhaps even imperative -- to do so.  We believe our proposed framework and accompanying models provide a concrete initial proposal for an evolving conversation about ML-based disinformation threats and how they can be countered. 
\section{Fake News in a Neural and Adversarial Setting}
\label{sec:overview}
We present a framework -- motivated by today's dynamics of manually created fake news -- for understanding what \emph{adversaries} will attempt with deep models, and how \emph{verifiers} should respond.

\paragraph{Scope of fake news.}
There are many types of \emph{false} news, ranging from satire to propaganda \citep{wardle2017fake}. In this paper, we focus on text-only documents formatted as news articles: stories and their corresponding metadata that contain purposefully false information.
Existing fake news is predominantly human-written, for two broad goals: monetization (ad revenue through clicks) and propaganda (communicating targeted information)  \citep{bradshaw2017troops, melford2019disinfo}. Achieving either goal requires the adversary to be selective about the news that they make, whether by producing only viral content, or content that advances a given agenda.

\paragraph{Fact checking and verification: related work.}
There is considerable interest in fighting online disinformation. Major platforms such as Facebook prioritize trustworthy sources and shut down accounts linked to disinformation \citep{mosseri2018news,dwoskin2018facebook}. Some users of these platforms avoid fake news with tools such as NewsGuard and Hoaxy \citep{shao2016hoaxy} and websites like Snopes and PolitiFact. These services rely on manual fact-checking efforts: verifying the accuracy of claims, articles, and entire websites. 
Efforts to automate fake news detection generally point out stylistic biases that exist in the text \citep{rashkin2017truth, wang2017liar,perez2018automatic}. These efforts can help moderators on social media platforms shut down suspicious accounts. However, fact checking is not a panacea -- cognitive biases such as the backfire effect and confirmation bias make humans liable to believe fake news that fits their worldview \citep{swire2017role}.

\paragraph{Framework.}\hspace{-1mm} We cast fake news generation and detection as an adversarial game, with two players:

\begin{itemize}[wide, leftmargin=10pt, labelwidth=!,labelindent=0pt,noitemsep,topsep=0pt]
    \item \textbf{Adversary}. Their goal is to generate fake stories that match specified attributes: generally, being viral or persuasive. The stories must read realistically to both human users as well as the verifier. 
    \item \textbf{Verifier}. Their goal is to classify news stories as real or fake. The verifier has access to unlimited real news stories, but few fake news stories from a specific adversary. This setup matches the existing landscape: when a platform blocks an account or website,
    their disinformative stories provide training for the verifier; but 
    it is difficult to collect fake news from newly-created accounts.
\end{itemize}

The dual objectives of these two players suggest an escalating ``arms race'' between attackers and defenders. As verification systems get better, so too will adversaries. We must therefore be prepared to deal with ever-stronger adversarial attacks, which is the focus of the next section.
















%
 
\section{\modelname: Modeling Conditional Generation of Neural Fake News}
\definecolor{domain}{HTML}{FFC7BF}
\definecolor{date}{HTML}{FFE9BF}
\definecolor{authors}{HTML}{CFFFCC}
\definecolor{headline}{HTML}{C0DEFF}
\definecolor{body}{HTML}{E3C0FF}
\definecolor{boringcolor}{rgb}{0.95, 0.95, 0.95}

\makeatletter
 \def\SOUL@hlpreamble{\setul{}{2.4ex}\let\SOUL@stcolor\SOUL@hlcolor
 \SOUL@stpreamble
 }
\makeatother


\newcommand{\hlc}[2][yellow]{{\colorlet{foo}{#1}\sethlcolor{foo}\hl{#2}}}


\newcommand{\metadata}{metadata}
\newcommand{\Metadata}{Metadata}

\newcommand{\tokenstart}{{\tt\small <start>}}
\newcommand{\tokenend}{{\tt\small <end>}}
\newcommand{\taustart}{{\tt\small <start>}}
\newcommand{\tauend}{{\tt\small <end>}}

\newcommand{\bodyfield}{\hlc[body]{body}}
\newcommand{\domainfield}{\hlc[domain]{domain}}
\newcommand{\datefield}{\hlc[date]{date}}
\newcommand{\authorsfield}{\hlc[authors]{authors}}
\newcommand{\authorfield}{\hlc[authors]{author}}
\newcommand{\headlinefield}{\hlc[headline]{headline}}
Given existing online disinformation, we have reason to believe adversaries will try to generate targeted content (e.g. clickbait and propaganda). Recently introduced large-scale generative models produce realistic-looking text \citep{radford2019gpttwo}, but they do not lend themselves to producing controllable generations \citep{hu2017toward}.\footnote{A common workaround is to have a human seed the text to provide context. However, this \textbf{a)} is a heavy handed technique for biasing which may not capture the desired attributes, and \textbf{b)} leaves in place a human-written beginning (as tokens are only generated left-to-right), which may create distributional artifacts.} 
Therefore, to probe the feasibility of realistic-looking neural fake news, we introduce \modelname, which produces both realistic \emph{and} controlled generations.

The current state-of-the-art in unconditional text generation views it as a language modeling problem \citep{bengio2003neural}, in which the probability of a document  is the product of the conditional probability of generating each token  given previous tokens:
{
\setlength{\abovedisplayskip}{1pt}
\setlength{\belowdisplayskip}{1pt}
\setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}

}The document is typically treated as a single unstructured \emph{text field}, beginning with a \tokenstart~token and ending with an \tokenend~token. The latter, \tokenend, is particularly important because it indicates the end of the field, and when to should stop generating.
However, a news article has necessary structure beyond the running text, or \bodyfield~field. \Metadata~fields include the \domainfield~where the article is published (indirectly marking the style), the \datefield~of publication, the names of the \authorsfield, and the \headlinefield~of the article itself. Not only does generating a news article require producing all of these components, these fields also allow significant control over the generations (e.g. specifying a \headlinefield~helps control the generated \bodyfield). An article can be modeled by the joint distribution:
{
\setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}

}However, it is not immediately obvious how to sample from Equation~\ref{eq:jointdist}. One option is to define a \emph{canonical order} among the article's fields : (), and model the article left-to-right in that order using Equation~\ref{eq:lmodeling}: . However, this ordering would forbid sampling certain fields without prohibitively expensive marginalization. Alternatively, one could generate fields in any order, but this requires the model to learn to handle  potential orderings during inference time. 

Our solution is \modelname, a new approach for efficient learning and generation of multi-field documents. We adopt the language modeling framework of Equation~\ref{eq:lmodeling} in a way that allows for flexible decomposition of Equation~\ref{eq:jointdist}. During inference time, we start with a set of fields  as context, with each field  containing field-specific start and end tokens. We sort the fields using a standard order\footnote{Our ordering is the following field types in order:  \domainfield, \datefield, \authorsfield, \headlinefield, and then the \bodyfield.} and combine the resulting tokens together. To generate a target field , we append the field-specific start token \taustart~to the context tokens; then, we sample from the model until we hit \tauend.

Figure \ref{fig:setup} shows an example of using \modelname~to generate an anti-vaccine article. Here, the adversary specifies a \domainfield, \datefield, and \headlinefield. After \modelname~generates the \bodyfield, it can be used to generate a fake \authorfield, before finally generating a new and more appropriate \headlinefield.

During training, we simulate inference by randomly partitioning an article's fields into two disjoint sets  and . We also randomly drop out individual fields with probability 10\%, and drop out all but the \bodyfield~with probability 35\%. This allows the model to learn how to perform unconditional generation. We sort the metadata fields in each set using our standard order, and concatenate the underlying tokens. The model is then trained to minimize the cross-entropy of predicting the tokens in  followed by the tokens in .\footnote{All tokens use the same vocabulary. By using a standard order, but partitioning the fields into two sets, the model can generate any field conditioned on others while only needing to learn  orderings, versus .}

\begin{figure}[t!]
  \centering\small
    \includegraphics[width=\textwidth]{figures/dataspec2.pdf}
    \vspace{-4mm}
\caption{A diagram of three \modelname~examples for article generation. In row {\tt\small a)}, the \hlc[body]{body} is generated from partial context (the \hlc[authors]{authors} field is missing). In {\tt\small b)}, the model generates the \hlc[authors]{authors}. In {\tt\small c)}, the model uses the new generations to regenerate the provided \hlc[headline]{headline} to one that is more realistic.}\vspace{-3mm}
  \label{fig:setup}
\end{figure}

\paragraph{Architecture.}
We draw on recent progress in training large Transformers for language modeling \citep{vaswani2017attention}, building \modelname~using the same architecture as for GPT2 \citep{radford2019gpttwo}. We consider three model sizes. Our smallest model, \modelname-Base, has 12 layers and 124 million parameters, on par with GPT and BERT-Base \citep{radford2018improving,devlin2018bert}. Our next model, \modelname-Large, has 24 layers and 355 million parameters, on par with BERT-Large. Our largest model, \modelname-Mega, has 48 layers and 1.5 billion parameters, on par with GPT2.

\paragraph{Dataset.} We present \datasetname, a large corpus of news articles from Common Crawl. Training \modelname~requires a large corpus of news articles with metadata, but none currently exists. Thus, we construct one by scraping dumps from Common Crawl, limiting ourselves to the ~5000 news domains indexed by Google News. We used the Newspaper Python library to extract the \bodyfield~and~\metadata~from each article. News from Common Crawl dumps from December 2016 through March 2019 were used as training data; articles published in April 2019 from the April 2019 dump were used for evaluation. After deduplication, \datasetname~is 120 gigabytes without compression.


\paragraph{Learning.} We trained each \modelname~model on randomly-sampled sequences from \datasetname~with length 1024. Other optimization hyperparameters are in Appendix~\ref{sec:optimizationhyperparameters}. We trained \modelname-Mega for 800k iterations, using a batch size of 512 and 256 TPU v3 cores.
Training time was two weeks.

\subsection{Language Modeling results: measuring the importance of data, context, and size}
We validate \modelname, versus standard unconditional language models, on the  April 2019 test set. We consider two evaluation modes: \emph{unconditional}, where no context is provided and the model must generate the article \bodyfield; and \emph{conditional}, in which the full metadata is provided as context. In both cases, we calculate the perplexity only over the article \bodyfield.

Our results, shown in Figure~\ref{fig:ppl}, show several conclusions. First, \modelname~noticeably improves (between .6 to .9 perplexity points) when conditioned on metadata. Second, perplexity decreases with size, with \modelname-Mega obtaining 8.7 perplexity in the conditional setting. Third, the data distribution is still important: though the GPT2 models with 124M parameters and 355M parameters respectively match our \modelname-Base and \modelname-Large architectures, our model is over 5 perplexity points lower in both cases, possibly because the OpenAI WebText corpus also contains non-news articles.

\subsection{Carefully restricting the variance of generations with Nucleus Sampling}
Sampling from \modelname~is straightforward as it behaves like a left-to-right language model during decoding. However, the choice of decoding algorithm is important.
While likelihood-maximization strategies such as beam search work well for \emph{closed-ended} generation tasks where the output contains the same information as the context (like machine translation), these approaches have been shown to produce degenerate text during \emph{open-ended} generation \citep{hashimoto2019unifying, holtzman2019curious}. However, as we will show in Section~\ref{sec:analysis}, restricting the variance of generations is also crucial. 


In this paper, we primarily use Nucleus Sampling (top-): for a given threshold , at each timestep we sample from the most probable words whose cumulative probability comprises the top-\% of the entire vocabulary  \citep{holtzman2019curious}.\footnote{In early experiments, we found Nucleus Sampling produced better and less-detectable generations than alternatives like top- sampling, wherein the most probable  tokens are used at each timestep \citep{fan2018hierarchical}.} 
\section{Humans are Easily Fooled by \modelname-written Propaganda}
\label{sec:genexps}
\begin{figure}[t!]
\centering\small
\begin{minipage}{.48\textwidth}
  \centering\small
  \includegraphics[width=\linewidth]{figures/ppl.pdf}\vspace*{-1mm}
  \captionof{figure}{Language Modeling results on the \bodyfield~field of April 2019 articles. We evaluate in the \emph{Unconditional} setting (without provided metadata) as well as in the \emph{Conditional} setting (with all metadata). \modelname~sees over a 0.6 point drop in perplexity when given metadata.}
  \label{fig:ppl}
\end{minipage}\hspace{.039\textwidth}\begin{minipage}{.48\textwidth}\vspace*{-3mm}
  \centering\small
  \includegraphics[width=\linewidth]{figures/humaneval-clean.pdf}\vspace*{-2mm}
  \captionof{figure}{Human evaluation. For each article, three annotators evaluated style, content, and the overall trustworthiness; 100 articles of each category were used. The results show that propaganda generated by \modelname~is rated more plausible than the original human-written propaganda.}\vspace{-1mm}
  \label{fig:humaneval}
\end{minipage}\vspace{-4mm}\end{figure}

We evaluate the quality of disinformation generated by our largest model, \modelname-Mega, using . We consider four classes of articles: 
human-written articles from reputable news websites ({\tt\small Human News}), 
\modelname-written articles conditioned on the same metadata ({\tt\small Machine News}),
human-written articles from known \emph{propaganda} websites ({\tt\small Human Propaganda}),
and \modelname-written articles conditioned on the propaganda metadata ({\tt\small Machine Propaganda}).\footnote{We use the technique described in Figure~\ref{fig:setup} to rewrite the propaganda: given the metadata, generate the article first, and then rewrite the headline.} The domains used are in Appendix~\ref{sec:newssites}; examples are in Appendix~\ref{sec:suppexamples}. We asked a pool of qualified workers on Amazon Mechanical Turk to rate each article on three dimensions: stylistic consistency, content sensibility, and overall trustworthiness.\footnote{With these guidelines, we tried to separate style versus content. Overall trustworthiness asks `Does the article read like it comes from a trustworthy source?' which emphasizes style, while content sensibility asks whether the content is believable on a semantic level.}

Results (Figure~\ref{fig:humaneval}) show a striking trend: though the quality of \modelname-written news is not as high as human-written news, it is adept at rewriting propaganda. The overall trustworthiness score of propaganda increases from 2.19 to 2.42 (out of 3) when rewritten by \modelname.\footnote{This difference is statistically significant at . One possible hypothesis for this effect is that \modelname~ignores the provided context. To test this hypothesis, we did a human evaluation of the consistency of the article body with the headline, date, and author. We found that human-written propaganda articles are consistent with the headline with an average score of 2.85 of 3 on the same 1-3 scale, while machine-written propaganda is consistent with 2.64 of 3.}
 
\section{Neural Fake News Detection}
\label{sec:detection}
The high quality of neural fake news written by \modelname, as judged by humans, makes automatic neural fake news detection an important research area. Using models (below) for the role of the \emph{Verifier} can mitigate the harm of neural fake news by classifying articles as {\tt\small Human} or {\tt\small Machine} written. These decisions can assist content moderators and end users in identifying likely (neural) disinformation. 








\begin{enumerate}[wide, leftmargin=10pt, labelwidth=!,labelindent=-2pt,itemsep=1pt,topsep=0pt,label=\textbf{\alph*}.]
    \item \modelnamefordisc. We consider a version of our model adapted for discrimination. Similar to GPT \citep{radford2018improving}, we place a special {\small\tt [CLS]} token at the end of each article, and extract the final hidden state at that point. The hidden state is fed to a linear layer to predict the label {\tt\small Human} or {\tt\small Machine}. 
    
    To simulate real conditions, and ensure minimal overlap between the generator and discriminator parameters, we initialize \modelnamefordisc~for discrimination using the checkpoint at iteration 700k, whereas the generator uses the checkpoint at iteration 800k.
    \item GPT2, a 124M or 355M parameter pretrained Transformer language model. Similar to \modelnamefordisc, we follow the GPT approach and extract the hidden state from a newly-added  {\small\tt [CLS]} token.
    \item BERT, a 110M parameter (BERT-Base) or 340M parameter (BERT-Large) bidirectional Transformer encoder commonly used for discriminative tasks. We perform domain adaptation to adapt BERT to the news domain, as well as to account for long articles; details in Appendix~\ref{sec:bertda}.
    \item FastText, an off-the-shelf library for bag-of-ngram text classification \citep{joulin2017bag}. Though not pretrained, similar models do well at detecting human-written fake news.
\end{enumerate}
All models are trained to minimize the cross-entropy loss of predicting the right label. Hyperparameters used during discrimination are in Appendix~\ref{sec:dischyperparams}.


\subsection{A semi-supervised setting for neural fake news detection}
While there are many human-written articles online, most are from the distant past, whereas articles to be detected will likely be set in the present. Likewise, there might be relatively few neural fake news articles from a given adversary.\footnote{Moreover, since disinformation can be shared on a heterogeneous mix of platforms, it might be challenging to pin down a single generated model.} We thus frame neural fake news detection as a semi-supervised problem. A neural verifier (or \emph{discriminator}) has access to many human-written news articles from March 2019 and before -- the entire \datasetname~training set. However, it has limited access to generations, and more recent news articles. Using 10k news articles from April 2019, we generate article body text; another 10k articles are used as a set of human-written news articles. We split the articles in a balanced way, with 10k for training (5k per label), 2k for validation, and 8k for testing.

We consider two evaluation modes. In the \textbf{unpaired} setting, a discriminator is provided single news articles, and must classify each independently as {\tt\small Human} or {\tt\small Machine}. In the \textbf{paired} setting, a model is given two news articles with the same metadata, one real and one machine-generated. The discriminator must assign the machine-written article a higher {\tt\small Machine} probability than the human-written article. We evaluate both modes in terms of accuracy.


\subsection{Discrimination results: \modelname~performs best at detecting \modelname's fake news}
\definecolor{lightgray}{rgb}{0.95, 0.95, 0.95}
\newcolumntype{g}{>{\columncolor{lightgray}}c}

\newcommand{\best}[1]{\textbf{#1}}
\newcommand{\scnd}[1]{#1}
\newcommand{\resultswidth}{1.35cm}

\begin{figure}[t!]
\centering\small
\begin{minipage}{.48\textwidth}
  \centering\scriptsize
    \captionof{table}{Results of discriminators versus generators, in both the paired and unpaired settings and across architecture sizes. We also vary the generation hyperparameters for each generator-discriminator pair, reporting the discrimination test accuracy for the hyperparameters with the \emph{lowest} validation accuracy. Compared with other models such as BERT, \modelname~is the best at detecting its own generations as neural fake news.}
\begin{tabular}{@{}l@{\hspace{0.25em}}r@{\hspace{0.5em}}l@{\hspace{0.25em}}c@{\hspace{0.5em}}c@{\hspace{-2em}}c@{\hspace{-2em}}||@{\hspace{0.4em}}c@{\hspace{0.25em}}c@{\hspace{0.3em}}c@{}}
& & & \multicolumn{3}{g}{Unpaired Accuracy} & \multicolumn{3}{g}{Paired Accuracy} \\
& & & \multicolumn{3}{c}{Generator size} & \multicolumn{3}{c}{Generator size} \\
& & & 1.5B & 355M & 124M & 1.5B & 355M & 124M \\
\cmidrule{3-9}
& & Chance & & 50.0 & & & 50.0 & \\ \cmidrule{3-9}
\multirow{8}{*}{\rotatebox[origin=c]{90}{Discriminator size}} & 1.5B & \modelname-Mega&\textbf{91.6}&\textbf{98.7}&\textbf{99.8}&\textbf{98.8}&\textbf{100.0}&\textbf{100.0}\\ \cmidrule{3-9}
& \multirow{3}{*}{355M} & \modelname-Large&\textbf{79.5}&\textbf{91.0}&\textbf{98.7}&\textbf{88.7}&\textbf{98.4}&\textbf{99.9}\\
& & BERT-Large&68.0&78.9&93.7&75.3&90.4&99.5\\
& & GPT2&70.1&77.2&88.0&79.1&86.8&95.0\\ \cmidrule{3-9}
& \multirow{3}{*}{124M} & \modelname-Base&\textbf{71.3}&\textbf{79.4}&\textbf{90.0}&80.8&88.5&\textbf{97.0}\\
& & BERT-Base&67.2&75.0&82.0&\textbf{84.7}&\textbf{90.9}&96.6\\
& & GPT2&67.7&73.2&81.8&72.9&80.6&87.1\\ \cmidrule{3-9}
& 11M & FastText&63.8&65.4&70.0&73.0&73.0&79.0 \\ \cmidrule{3-9}
\end{tabular}
  \label{tab:results}
\end{minipage}\hspace{.039\textwidth}\begin{minipage}{.48\textwidth}
  \centering\small
  \includegraphics[width=.97\linewidth]{figures/learningcurve.pdf} \vspace{-1mm}
  \captionof{figure}{Exploring weak supervision for discriminating \modelname-Mega generations. 
  With no weak supervision, the discriminator sees  machine-written articles (from \modelname~Mega). For \modelname-Base and \modelname-Mega, the discriminator sees  machine-written articles given by the weaker generator in question. Seeing weaker generations improves performance when few in-domain samples are given. }
  \label{fig:weaksupervision}
\end{minipage}\vspace{-5mm}
\end{figure} We present experimental results in Table~\ref{tab:results} for all generator and discriminator combinations. For each pair, we show the test results using the most adversarial generation hyperparameters (top-) as judged on the validation set.\footnote{For each discriminator/generator pair, we search over .} The results show several trends. First, the paired setting appears much easier than the unpaired setting, suggesting that it is difficult for the model to calibrate its predictions. Second, model size is highly important in the arms race between generators and discriminators. Using \modelname~to discriminate \modelname's generations results in roughly 90\% accuracy across the range of sizes. If a larger generator is used, accuracy slips below 81\%; conversely, if the discriminator is larger, accuracy is above 98\%. Third, other discriminators perform worse than \modelname~overall, even when controlling for architecture size and (for both BERT models) the domain. 

That \modelnamefordisc~is the best discriminator is possibly surprising: being unidirectional, it is less expressive than deep bidirectional models such as BERT.\footnote{Indeed, bidirectional approaches perform best on leaderboards like GLUE \citep{wang2018glue}.} That the more expressive model here is \textbf{not} the best at discriminating between real and generated news articles suggests that neural fake news discrimination requires having a similar \emph{inductive bias} as the generator.\footnote{This matches findings on the HellaSwag dataset \citep{zellers2018hellaswag}. Given human text and machine text written by a finetuned GPT model, a GPT discriminator outperforms BERT-Base at picking out human text.}

\subsection{Weak supervision: what happens if we don't have access to \modelname-Mega?}
These results suggest that \modelnamefordisc~is an effective discriminator when we have a medium number of fake news examples from the exact adversary that we will encounter at test time. What happens if we relax this assumption? Here, we consider the problem of detecting an adversary who is generating news with \modelname-Mega and an unknown top- threshold.\footnote{The top- threshold used was , but we are not supposed to know this!} In this setup, during training, we have access to a weaker model (\modelname-Base or \modelname-Large). We consider the effect of having only  examples from \modelname-Mega, and sampling the missing  articles from one of the weaker models, where the top-p threshold is uniformly chosen for each article in the range of . 

We show the results of this experiment in Figure~\ref{fig:weaksupervision}. The results suggest that observing additional generations greatly helps discrimination performance when few examples of \modelname-Mega are available: weak supervision with between 16 and 256 examples from \modelname-Large yields around 78\% accuracy, while accuracy remains around 50\% without weak supervision. As the portion of examples that come from \modelname-Mega increases, however, accuracy converges to around 92\%.\footnote{In additional experiments we show that accuracy increases even more -- up to 98\% -- when the number of examples is increased \citep{zellers2019blogpost}. We also find that \modelname~when trained to discriminate between real and fake \modelname-generated news can detect GPT2-Mega generated news as fake with 96\% accuracy.}

 
\section{How does a model distinguish between human and machine text?}
\begin{figure}[t!]
\centering\small
\begin{minipage}{.54\textwidth}
  \centering\small
  \includegraphics[width=\linewidth]{figures/ppl_by_pos.pdf}\vspace{-2mm}
  \captionof{figure}{Perplexities of \modelname-Mega, averaged over each position in the \bodyfield~(after conditioning on metadata). We consider human-written with \modelname-Mega generated text at  (random sampling) and . The perplexity of randomly sampled text is higher than human-written text, and the gap increases with position. This suggests that sampling without variance reduction increasingly falls out-of-distribution.}
  \label{fig:pplbypos}
  \vspace{-3mm}
\end{minipage}\hspace{.039\textwidth}\begin{minipage}{.42\textwidth}
\vspace{-3mm}
  \centering\small
  \includegraphics[width=\linewidth]{figures/grovermega_and_bertmedium_nok.pdf}\vspace{-2mm}
  \captionof{figure}{Unpaired validation accuracy, telling apart generated news articles (from \modelname~Mega) from real articles, at different variance reduction thresholds  (for Nucleus Sampling). Results varying  show a sweet spot ( -- ) wherein discrimination is hardest.}
  \label{fig:varyingp}
    \vspace{-3mm}
\end{minipage}\vspace{-2mm}\end{figure}
\label{sec:analysis}
In this section, we explore why \modelnamefordisc~performs best at detecting fake news generated by other \modelname~models. We find that there is a double-bind between \textbf{exposure bias} and \textbf{variance-reduction} algorithms that alleviate these biases while at the same time creating other artifacts. 

\paragraph{Exposure Bias.} Models maximizing Equation~\ref{eq:lmodeling} are trained only conditioned on human-written text, never on its own generations, creating a problem known as exposure bias \citep{Ranzato2016SequenceLT}. 

We investigate the importance of exposure bias towards creating artifacts. In Figure~\ref{fig:pplbypos} we plot the perplexities given by \modelname-Mega over each position for \bodyfield~text at top- thresholds of  and , as well as over human text. Generating the first token after {\tt\small <start\bodyfield>} results in high perplexity. However, the rest of the positions show a curious pattern: the perplexity of human-written text is lower than randomly sampled text, and this gap increases with sequence length, suggesting that random sampling causes \modelname~to fall increasingly out of the distribution of human language. However, limiting the variance () lowers the resulting perplexity and limits its growth.

\paragraph{Limiting the variance of a model also creates artifacts}
On the other hand, clipping the model's variance also leaves an artifact, as prior work has observed for top- sampling \citep{strobelt2019gltr}. A similar phenomenon holds for Nucleus (top-) sampling. The probability of observing a human-written article where all tokens are drawn from the top-\% of the distribution is , where  is the document's length. This probability goes to zero as  increases. However, for Nucleus Sampled text -- in which the final  is cut off -- all tokens come from the top-. 

The visibility of the artifacts depends on the choice of discriminator.
The top- at each timestep is calculated under the generator's worldview, meaning that if the discriminator models text in a different way, it might have a harder time pinpointing the empty  tail. This could explain BERT's lower performance during discrimination.

\paragraph{A sweet spot of careful variance reduction}
Not reducing the variance, as well as significantly reducing the variance, both cause problems. Might there be a \emph{sweet spot} for how much to truncate the variance, to make discrimination maximally hard? In Figure~\ref{fig:varyingp}, we show results varying the top- threshold for the discrimination task applied to \modelname-Mega's generations. The results indeed show a sweet spot, roughly between  and  depending on the discriminator, wherein discrimination is hardest. Interestingly, we note that the most adversarial top- threshold for BERT-Large is considerably lower than the corresponding top- for \modelnamefordisc-Large of the same size. This supports our hypothesis that BERT's view of language differs markedly from \modelname; using a lower top- threshold does not seem to give it much more information about the missing tail. 

\textbf{Overall}, our analysis suggests that \modelname~might be the best at catching \modelname~because it is the best at knowing where the tail is, and thus whether it was truncated.


















 


\section{Conclusion: a Release Strategy for \modelname}
This paper investigates the threats posed by adversaries seeking to spread disinformation. Our sketch of what these threats might look like -- a controllable language model named \modelname~-- suggests that these threats are real and dangerous. \modelname~can rewrite propaganda articles, with humans rating the rewritten versions as more trustworthy. At the same time, there are defenses to these models -- notably, in the form of \modelname~itself. We conclude with a discussion of next steps and ethical considerations. 


\paragraph{The Era of Neural Disinformation.} Though training \modelname~was challenging, it is easily achievable by real-world adversaries today. Obtaining the data required through Common Crawl cost \0.30 per TPU v3 core-hour and two weeks of training, the total cost is \\beta_1=0.9990.012e-52e-50.5pp{=}0.9517 million in a Series A round of funding led by Plug and Play Tech Center, with participation from Open Ocean Capital and previous investors, including the Cahill Foundation.

So what does DogSpotter offer, beyond a fairly basic service that could theoretically be used by anyone, which is at the base of a human-side algorithm that determines the best potential location for a dog owner’s pet — a problem that dogs are notoriously impatient for in cities where dog walking is often unprofitable or a lost art for many?

Well, DogSpotter leverages its core recommendation engine to process the thousands of now readily available reviews of dog care providers via its app that users can find by searching for their own city or a curated selection of recommendations. That way, it’s much quicker than traditional alternatives, where you have to wade through a fair number of reviews to find the best-rated providers in your city, and does it with far less hassle.

From there, the DogSpotter app makes use of the data it provides to recommend relevant walking services, among them a variety of preferred types of walks for your dog, based on everything from their preferences and sorts of paths to the weather, geographic differences, and degree of safety.

DogSpotter’s founders also contend that their software can — and in some cases already has been — used to make sure that the actual human responsible for dog walking is nearby, by using the location-based data to facilitate long-distance walkings or picking out a dog for a veterinarian who can be contacted.

Speaking of which, they’re not just operating on the basis that it’s an effective way to walk dogs without suffering losses. Although I won’t deny that I’ve seen some big cities suffer from too many small businesses advertising purely online and with little regard for actual human location. As it happens, DogSpotter tells me it has its own network of paying businesses in places like U.S. cities, but it claims to be seeing a company like DogBuddy.com.au as a market that it is disrupting.

My hope is that the company ends up developing a number of other application-specific verticals such as veterinary procedures — again, I have not seen this yet but that would be a great and easy way to monetize.
The business model is to offer DogSpotter services for a set fee per user, per location per month. It includes complimentary services for canine community or adoption groups, dog run assessment and mapping, dog boarding facilities, dog walking packages, and a myriad of other services.

“DogSpotter is different from other dog walking platforms because we have the leading mobile app which is the most effective tool for location-based visual targeting for relevant content and experiences,” said Rowan Zellers, founder and CEO of DogSpotter, in a statement. “This means that DogSpotter is able to provide more relevant products and services and, for early users, we can allow companies to target only the users they want to engage with.”

The problem, of course, is that not all dog walkers have the best apps. DogSpotter says it will compete on performance of its offering, not with other dog walking apps but rather with making it easier for dog owners to find the people they are looking for and, importantly, for those individual dog walkers to save a small bit of money on their operating costs, by saving time and effort with a website. And so on.

}
Generated \headlinefield: DogSpotter raises \$17M to improve your search for walking services

\hrule
\caption{An example of using \modelname-Mega to generate fake business news. This generates an article about a fake startup for `Uber for Dogs', ostensibly created by the first author of this paper.}
\label{fig:uberfordogs}
\end{figure}
  \end{document}
