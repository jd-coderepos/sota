









\documentclass[10pt,journal]{IEEEtran}
















\ifCLASSOPTIONcompsoc
\else
\fi






\ifCLASSINFOpdf
\else
\fi































































\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage[ruled,lined]{algorithm2e}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{tikz}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{caption}
\captionsetup{font={small, sf}}




\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}



\begin{document}
\title{A	 Joint Optimization of Operational Cost and Performance Interference in Cloud Data Centers}


\author{\IEEEauthorblockN{Xibo Jin, Fa Zhang, Lin Wang, Songlin Hu, Biyu Zhou and Zhiyong Liu}\\
\IEEEauthorblockA{Institute of Computing Technology, Chinese Academy of Sciences\\
University of Chinese Academy of Sciences, Beijing, China\\
Email: \{jinxibo, zhangfa, wangling, husonglin, zhoubiyu, zyliu\}@ict.ac.cn}
}











\maketitle

\begin{abstract}
Virtual machine (VM) scheduling is an important technique to efficiently operate the computing resources in a data center. Previous work has mainly focused on consolidating VMs to improve resource utilization and thus to optimize energy consumption. However, the interference between collocated VMs is usually ignored, which can result in very worse performance degradation to the applications running in those VMs due to the contention of the shared resources. Based on this observation, we aim at designing efficient VM assignment and scheduling strategies where we consider optimizing both the operational cost of the data center and the performance degradation of running applications and then, we propose a general model which captures the inherent tradeoff between the two contradictory objectives. We present offline and online solutions for this problem by exploiting the spatial and temporal information of VMs where VM scheduling is done by jointly consider the combinations and the life-cycle overlapping of the VMs. Evaluation results show that the proposed methods can generate efficient schedules for VMs, achieving low operational cost while significantly reducing the performance degradation of applications in cloud data centers.
\end{abstract}










\IEEEpeerreviewmaketitle



\section{Introduction}




Cloud computing has become a promising choice for modern computing platforms and will most likely continue to be the dominant service model in the future. The foundation of cloud computing is founded by taking advantage of virtualization technologies such as VMware \cite{VMware} and Xen \cite{Xen} to encapsulate applications into virtual machines (VMs) and allow independent applications to execute on the same physical server simultaneously. Furthermore, cloud computing affords users to obtain, configure, and deploy cloud services themselves using cloud service catalogues, without requiring the assistance of IT (Infrastructure Technology) \cite{Perera12}. The feasibility of VM consolidation and on-demand resource allocation offers an opportunity for cloud operators to multiplex resources among users and thus improve the operational cost, e.g., reducing the energy consumption.

However, although it brings better utilization to the cloud system, such kind of resource multiplexing is not always beneficial. When VMs are consolidated together, the performance interference between the VMs brought by the contention of shared resources such as last-level-cache, memory bus, network and disk bandwidth can not be ignored \cite{Govindan11, Chiang11, Mars11, Roytman13, Kim13, Verboven13}. As compared to running in a dedicated server, a VM has to compete on the shared resources with other VMs that collocated with it and thus the performance will be degraded even with the same resource reservation. While previous work is focused on analysis on application-level interference in single servers, we aim to study the assignment and scheduling of VMs to physical servers, mitigating the performance interference while optimizing the operational cost. We tackle this combinatorial problem of joint optimization by leveraging the specific structures of VM collocation.

\subsection{Performance Interference inside a Cloud Data Center}

It is necessary to provide efficient management of performance interference in order to guarantee the quality of service for tenants in a cloud data center. In general, the performance interference between VMs can be affected by the following two factors.

\textbf{VM combination.} 
Recent researches have analysed the resource contention for possible VM combinations and suggested to collocate those VMs that have less competition between shared resources \cite{Govindan11, Chiang11, Mars11, Roytman13, Kim13, Verboven13}. In order to quantify the overall performance interference between collocated VMs, we evaluated the performance degradation of VMs using SPECcpu 2006 benchmark \cite{SPEC06} where we assume each application executes in a virtual machine and runs on a physical core. We define the \textit{Performance Degradation Ratio} (PDR) of a VM as the increment of running time divided by the time used for the VM to be executed in a dedicated server. The statistical results are demonstrated in Table~\ref{tb:spatial}. As can be seen from the table, the PDR of  when being collocated with  is  while it is  when being collocated with . This reveals that different VM combinations lead to variable level of performance interference. As a result, VM placement can be done in an intelligent way such that the performance interference between VMs is minimized. Another observation is that the PDRs of VMs become larger with the increase of number of collocated VMs. For example, in Table~\ref{tb:spatial}, the PDRs of  and  are  and  respectively, while these values increase up to  and  when a third VM for  is launched simultaneously on the same physical server.

\renewcommand{\arraystretch}{1.3}
\begin{table*}[htbp]
\centering
\caption{\label{tb:spatial}Running times (and stretches in percentile) of applications (VMs) collocated in the same physical server. (For example, the 1st row means each application runs on a dedicated server and, the 3rd row means  and  collocate in a server, et al.)}
\begin{tabular}{c|c|c|c|c|c}\hline\hline
&&&&&\\\hline
////&498&265&269&186&318\\\hline
 + &642 (28.92)&--&--&--&358 (12.58)\\
\textbf{\textit{403.gcc + 429.mcf}}&--&{\bf 299 (12.83)}&{\bf 301 (11.90)}&--&--\\
\textbf{\textit{429.mcf + 470.lbm}}&--&--&{\bf 436 (62.08)}&--&{\bf 366 (15.09)}\\
 + &--&270 (1.89)&--&193 (3.76)&--\\
 + &--&--&--&201 (8.06)&326 (2.52)\\
\textbf{\textit{403.gcc + 470.lbm}}&--&{\bf 383 (44.53)}&--&--&{\bf 350 (10.06)}\\\hline
\textbf{\textit{403.gcc + 429.mcf + 470.lbm}}&--&{\bf 444 (67.55)}&{\bf 487 (81.04)}&--&{\bf 407 (16.29)}\\
 +  + &725 (45.58)&--&482 (79.18)&--&404 (27.04)\\
 +  +  + &778 (56.22)&495 (86.79)&538 (100.00)&--&466 (46.54)\\\hline\hline
\end{tabular}
\end{table*}


\textbf{Life-cycle overlapping.} 
It is a challenging problem to take into account the life cycles of VMs. On the one hand, overlapping the execution of VMs can improve the resource utilization of the system and thus reduce the marginal cost.\footnote{This refers to the static cost irrespective of the load of server incurred by always-on components such as idle-energy.} On the other hand, due to performance interference, reducing the overlap of the executions of VMs can mitigate performance degradation thus shortening the completion times of VMs. This can be verified by the results shown in Table~\ref{tb:temporal}. For example, when collocated with , , and ,  receives a considerable reduction on PDR from  to  with the lessening of the execution overlaps. As a consequence of performance interference, the neglect of life-cycle overlapping can result in more serious problems such as resource-reservation violation brought by the stretch on the execution duration of VMs. Moreover, the performance of some VMs will become unacceptably worse when the execution is always overlapped with other mutual-interference VMs and therefore, their performance is degraded all the time by collocated VMs.

\begin{table}[!t]
\centering
\caption{\label{tb:temporal}Running times (and stretches in percentile) of applications (VMs) collocated in the same physical server with different overlap times. (For example, the 1st grid means  and  collocate in a physical server with different overlap times. I.e.,  means that  starts after  has run 60 unit times.)}
\begin{tabular}{p{1.0cm}p{1.32cm}p{1.32cm}p{1.32cm}p{1.32cm}}\hline\hline {\bf Apps}	&	0&	+60&		+120	&	+180\\\hline
&	301 (11.90)&	{\bf 	294 (9.29)}&	283 (5.20)&	277 (2.97)\\
&	299 (12.83)&	{\bf 	292 (10.19)}&286 (7.92)&	278 (4.91)\\\hline
&	404 (27.04)&		377 (18.55)&	354 (11.32)&	336 (5.66)\\
&	482 (79.18)&		440 (63.57)&	401 (49.07)&	360 (33.83)\\
& 725 (45.58)&		643 (29.12)&	578 (16.06)&	523 (5.02)\\\hline
&	466 (46.54)&		407 (27.99)&	357 (12.26)&	337 (5.97)\\
&	538 (100.0)&		476 (76.95)&	418 (55.39)&	365 (35.69)\\
\textbf{\textit{403.gcc}}&	{\bf 495 (86.79)}&		{\bf 424 (60.00)}&	{\bf 353 (33.21)}&	{\bf 293 (10.57)}\\
&	778 (56.22)&		658 (32.13)&	550 (10.44)&	510 (2.41)\\\hline\hline
\end{tabular}
\end{table}

\subsection{Tradeoff between Operational Cost and Performance Interference}

In general, operational cost refers to the daily expenditure caused by the operation of a cloud computing system, including electricity cost and system maintenance expenses. Among them, the electricity cost takes a dominant proportion \cite{Hamilton09, Amokrane2013}. As a consequence, achieving energy efficiency on servers can result in significant reduction on the operational cost of a data center. For this reason, we will use the term \emph{energy consumption} to refer to operational cost. Throughout the paper, we use both terms interchangeably. There has been a large body of work focused on improving the energy efficiency of single servers, such as Dynamic Voltage Frequency Scaling (DVFS) and powering down \cite{Albers10}. Based on the two fundamental mechanisms, researches have investigated to reduce the energy consumption of a cloud system using virtualization techniques such as VM consolidation to improve hardware utilization. However, while these methods can help reach the goal of energy conservation elegantly, very little attention has been paid on the accompanying side-effect, i.e., performance interference. Moreover, to the best of our knowledge, a quantitative analysis on the tradeoff between energy consumption and performance interference is almost completely missing in the literature, which is highly desired by cloud operators.

We study the VM assignment and scheduling problem for arbitrating between energy consumption and performance interference, i.e., reducing energy consumption while maintaining low performance degradation for VMs. On the one hand, ideally, the energy consumption is minimized when a minimum number of servers is used. This can be done by consolidating VMs and then turning idle servers into some power-saving mode (sleeping or power-off). The set of active servers is managed dynamically according to the workload. Consequently, the energy consumed by underutilized servers can be saved, as well as the corresponding cost incurred by power delivery and cooling infrastructure. 

On the other hand, VM consolidation can result in undesirable performance interference between VMs because of the contention in shared resources. This performance interference can stretch the execution durations of VMs to a large extent, which may bring unacceptable performance loss to user applications (and further result in Service-Level-Agreement violation). A simple example is illustrated in Fig.~\ref{fig:example_allocation}. It can be observed that the assignment shown in the right-side figure is better than the one shown on the left in terms of two aspects:  the performance of most VMs such as  is less degraded and  the real-time accommodation of  becomes possible. (As shown in the part of the ellipses.) This also reveals that the two factors, VM combination and life-cycle overlapping, are coupled and mutually affected. Therefore, in order to arbitrate between energy consumption and performance interference, it is necessary to provide a careful design of VM consolidation where VMs are allocated with appropriate combinations and collocated VMs are scheduled with the the most favourable life-cycle overlapping.

\begin{figure}
\begin{tikzpicture}
    \draw[->, >=latex,line width=1pt] (0.0,0)--(3.75+0.25,0) node[pos=1,below] {};
    \draw[->, >=latex,line width=1pt] (0,0.0)--(0,2);
    \draw[dotted] (0.0,1.75)--(0.0,0.0) node[below, scale=0.8] {};
    \draw[dotted] (0.25,1.75)--(0.25,0.0);
    \draw[dotted] (0.5,1.75)--(0.5,0.0) node[below, scale=0.8] {};
    \draw[dotted] (0.75,1.75)--(0.75,0.0);
    \draw[dotted] (1,1.75)--(1,0.0) node[below, scale=0.8] {};
    \draw[dotted] (1.25,1.75)--(1.25,0.0);
    \draw[dotted] (1.5,1.75)--(1.5,0.0) node[below, scale=0.8] {};
    \draw[dotted] (1.75,1.75)--(1.75,0.0) node[below=6pt, scale=0.8] {(a)};
    \draw[dotted] (2,1.75)--(2,0.0) node[below, scale=0.8] {};
    \draw[dotted] (2.25,1.75)--(2.25,0.0);
    \draw[dotted] (2.5,1.75)--(2.5,0.0) node[below, scale=0.8] {};
    \draw[dotted] (2.75,1.75)--(2.75,0.0);
    \draw[dotted] (3,1.75)--(3,0.0) node[below, scale=0.8] {};
    \draw[dotted] (3.25,1.75)--(3.25,0.0);
    \draw[dotted] (3.5,1.75)--(3.5,0) node[below, scale=0.8] {}; 
    
    \draw(0.0,1.5) rectangle (1.75,1.3) node[pos=0.5,text width=0.25em, scale=0.8] {};
    \draw[fill=brown](1.75,1.5) rectangle (2.25,1.3);
    
    \draw(2.0,1.7) rectangle (3,1.5) node[pos=0.5, scale=0.8] {};
    
    \draw[color=blue] (2.12,1.55) ellipse(5pt and 8pt);
    
    \draw(0.25,1.3) rectangle (2.75,1.1) node[pos=0.5,text width=0.25em, scale=0.8]{} ;
    \draw[fill=brown](2.75,1.3) rectangle (3.25,1.1);
    
    \draw(0.25,1.1) rectangle (2.25,0.9) node[pos=0.5,text width=0.25em, scale=0.8]{};
    \draw[fill=brown](2.25,1.1) rectangle (3,0.9);
        
    \draw(0.5,0.75) rectangle (2,0.35) node[pos=0.5,text width=0.25em, scale=0.8]{};
    \draw[fill=brown] (2,0.75) rectangle (2.75,0.35);
    
    \draw(1,0.35) rectangle (2.75,0.15) node[pos=0.5,text width=0.25em, scale=0.8]{};
    \draw[fill=brown] (2.75,0.35) rectangle (3.25,0.15);   
    
    \draw[dotted,line width=0.9pt](0.0,1.5) rectangle (3.25,0.9) node[right=0pt,pos=0.98,scale=0.8] {};    
    \draw[dotted,line width=0.9pt](0.0,0.75) rectangle (3.25,0.15) node[right=0pt,pos=0.98,scale=0.8] {}; 
    
    \draw[->, >=latex,line width=1pt] (4.0+0.25,0)--(7.75+0.25+0.25,0) node[pos=1,below] {};
    \draw[->, >=latex,line width=1pt] (4+0.25,0.0)--(4+0.25,2);
    \draw[dotted] (4.0+0.25,1.75)--(4.0+0.25,0.0) node[below, scale=0.8] {};
    \draw[dotted] (4.25+0.25,1.75)--(4.25+0.25,0.0);
    \draw[dotted] (4.5+0.25,1.75)--(4.5+0.25,0.0) node[below, scale=0.8] {};
    \draw[dotted] (4.75+0.25,1.75)--(4.75+0.25,0.0);
    \draw[dotted] (5+0.25,1.75)--(5+0.25,0.0) node[below, scale=0.8] {};
    \draw[dotted] (5.25+0.25,1.75)--(5.25+0.25,0.0);
    \draw[dotted] (5.5+0.25,1.75)--(5.5+0.25,0.0) node[below, scale=0.8] {};
    \draw[dotted] (5.75+0.25,1.75)--(5.75+0.25,0.0) node[below=6pt, scale=0.8] {(b)};
    \draw[dotted] (6+0.25,1.75)--(6+0.25,0.0) node[below, scale=0.8] {};
    \draw[dotted] (6.25+0.25,1.75)--(6.25+0.25,0.0);
    \draw[dotted] (6.5+0.25,1.75)--(6.5+0.25,0.0) node[below, scale=0.8] {};
    \draw[dotted] (6.75+0.25,1.75)--(6.75+0.25,0.0);
    \draw[dotted] (7+0.25,1.75)--(7+0.25,0.0) node[below, scale=0.8] {};
    \draw[dotted] (7.25+0.25,1.75)--(7.25+0.25,0.0);
    \draw[dotted] (7.5+0.25,1.75)--(7.5+0.25,0.0) node[below, scale=0.8] {};  
    
    \draw(4.0+0.25,1.5) rectangle (5.75+0.25,1.3) node[pos=0.5,text width=0.25em, scale=0.8] {};
    \draw[fill=brown](5.75+0.25,1.5) rectangle (6.0+0.25,1.3);
    
    \draw(4.5+0.25,1.3) rectangle (6+0.25,0.9) node[pos=0.5,text width=0.25em, scale=0.8]{};
    \draw[fill=brown] (6+0.25,1.3) rectangle (6.5+0.25,0.9);
    
    \draw(6.0+0.25,1.7) rectangle (7+0.25,1.5) node[pos=0.5, scale=0.8] {};
    
    \draw[color=blue] (6.0+0.25+0.12,1.55) ellipse(5pt and 8pt);
    
    \draw(4.25+0.25,0.75) rectangle (6.75+0.25,0.55) node[pos=0.5,text width=0.25em, scale=0.8]{} ;
    \draw[fill=brown](6.75+0.25,0.75) rectangle (7.25+0.25,0.55);
    \draw(4.25+0.25,0.55) rectangle (6.25+0.25,0.35) node[pos=0.5,text width=0.25em, scale=0.8]{};
    \draw[fill=brown](6.25+0.25,0.55) rectangle (6.75+0.25,0.35);
        
    \draw(5+0.25,0.35) rectangle (6.75+0.25,0.15) node[pos=0.5,text width=0.25em, scale=0.8]{};
    \draw[fill=brown] (6.75+0.25,0.35) rectangle (7.25+0.25,0.15);
    
    \draw[dotted,line width=0.9pt](4.0+0.25,1.5) rectangle (4.0+0.25+3.25,0.9) node[right=0pt,pos=0.98,scale=0.8] {};    
    \draw[dotted,line width=0.9pt](4.0+0.25,0.75) rectangle (4.0+0.25+3.25,0.15) node[right=0pt,pos=0.98,scale=0.8] {}; 
 
\end{tikzpicture}
\caption{\label{fig:example_allocation}Two ways of VM allocation. Assume the two servers have the same resource capacity of \{\} unit and each VM from  demands \{\} unit while  requires \{\} unit. The rectangles in color represent the stretch of execution time due to resource contention, which represent the same mean in the following figures. (a): An inappropriate scheduling, (b): A better scheduling.}
\end{figure}

\subsection{Overview of the Paper}

In this work, we seek to find out efficient solutions for reducing the energy consumption while minimizing the performance interference among VMs. Our main contributions are summarized in the following three aspects:
\begin{enumerate}
\item We characterize the energy consumption and the performance interference in a unified model and  formally formulate the challenge of VM assignment and scheduling into an optimization problem. We also prove the NP-Completeness of the problem;
\item We propose efficient algorithms for offline VM assignment and scheduling, assuming all information is known a priori;
\item We extend the offline algorithms to the case with dynamic VM arrival. Using information such as resource reservation, these algorithms can further be improved. We also provide a distributed implementation of the algorithms for large-scale data centers; and
\item We evaluate the efficiency of the proposed algorithms through comprehensive simulations, showing that the proposed solution can achieve desirable arbitration between energy consumption and performance interference.
\end{enumerate}

The rest of this paper is organized as follows. In Section~\ref{sec:related}, we summarize the related works relevant to ours. Section~\ref{sec:model} shows the modelling of the problem. Section~\ref{sec:algorithm} provides our algorithms for VM assignment and scheduling where both offline and online cases are considered, while distributed solution is also provided. Section~\ref{sec:exp} validates the performance of the algorithms by extensive simulations. We finally conclude the paper in Section~\ref{sec:conclusion}.



\section{Relate Work}
\label{sec:related}

With the cloud computing being used more and more widely, researchers have conducted studies on executing traditional applications (e.g., HPC and scientific computing) in cloud environments. This section summarizes the research efforts on VM assignment and scheduling that is relevant to our work in terms of operational energy management and application performance interference in data centers.

\textbf{Energy consumption management.} 
It is known that the most efficient way to reduce the energy consumption is consolidating the applications (VMs) into a set of active servers, such that the utilization of the data center is kept at a high level. An early research \cite{Nathuji07} extended virtualization solutions to support rich and effective policies for active power management which had not been done before. They integrated ``hard" and ``soft" power states to provide high power savings, and showed that substantial benefits could been derived from coordination of online methods for server consolidation with their proposed management techniques. Kusic \emph{et al.} \cite{Kusic09} considered the problem of consolidating services onto a smaller number of computing resources. They implemented a dynamic resource provisioning framework for virtualized server environments, which was tackled as one of sequential optimization and solved using a lookahead control scheme. Beloglazov \emph{et al.} \cite{Beloglazov12} investigated scheduling algorithms that consolidate VMs onto the minimum number of servers. They proposed a policy as known as Modified Best Fit Decreasing (MBFD), for energy-efficient management of cloud computing environments. There is another representative work \cite{Lin11}, in which the authors investigated the energy-saving problem by dynamically ``right-sizing" the data center in both offline and online cases. Liu \emph{et al.} \cite{Liu13} studied the problem of arbitrating the power-performance tradeoff in clouds. They provided a probabilistic framework where online decisions are made on request admission control, routing, and VM allocation.

Therefore, these works are totally different from our work as they only focus on optimizing energy consumption while guaranteeing some other metrics, such as throughput. In our approach, the performance interference is an important objective for scheduling.

\textbf{Performance interference optimization.} 
Several works  \cite{Govindan11, Chiang11, Mars11, Roytman13, Kim13, Verboven13, Xu14} have take into account the performance interference when exploiting the virtual machine consolidation to improve resource utilization. Govindan \emph{et al.} \cite{Govindan11} presented a technique for predicting performance interference due to processor cache sharing. They showed that their technique can be used to achieve the most efficient consolidation as the prediction of the performance degradation for any possible application placement only use a linear number of measurements. Chiang \emph{et al.} \cite{Chiang11} considered the problem of interference-aware scheduling for data-intensive applications in virtualized environment. They presented a task and resource allocation control framework, which can mitigate the interference effects from concurrent data-intensive applications and improve the overall system performance. Mars \emph{et al.} \cite{Mars11} presented a characterization methodology, named ``Bubble-Up", which enables the accurate prediction of the performance degradation that results from contention for shared resources in memory subsystem. They showed their methodology could predict the performance interference between collocated applications with an accuracy within  to  of the actual performance degradation. Roytman \emph{et al.} \cite{Roytman13} proposed a system that consolidates virtual machines to minimize the unused resources, and guarantees that the performance degradation is within a tunable bound. Their system employed a method for suitable VM combinations which was proved to perform closely to the optimal, and the system included another technique that maximizes performance while not leaving any resource unused. Kim \emph{et al.} \cite{Kim13} suggested a performance model that considers interferences in the shared last-level cache and memory bus. They claimed that the model could be used to estimate the performance degradation among applications. Based on the interference model they also presented a virtual machine consolidation method. Verboven \emph{et al.} \cite{Verboven13} addressed the performance degradation prediction models and proposed a novel approach using both the classification and regression capabilities of support vector machines. A latest survey \cite{Xu14} gave the state of the art of some of these solutions for managing the performance overhead in different cloud scenarios.

Compared with these previous works considering performance interference optimization, our model provides a unified characterization of both the energy consumption and the performance interference and our solution for VM assignment and scheduling considers both VM combination and life-cycle overlapping. We explore the tradeoff between the performance degradation overhead of VMs and resource provision of cloud data centers on a high level, which is raised as an open research issue in \cite{Esch14}. Moreover, our work can be regarded as a complement to previous works in terms of that the solutions provided by them can be integrated into our optimization framework to reduce the overall cost of a cloud system.



\section{Model and Problem Description}
\label{sec:model}
In this section we describe the model and formulate an optimization problem of VMs scheduling that aims at arbitrating between energy consumption cost and performance degradation penalty.

\subsection{Resource Allocation and Energy Cost}
We model cloud data center as an undirected graph and denote it by , where   is the set of physical servers and  is the set of physical links between servers. Each server  is associated with  type of resources, e.g., CPU, memory, and storage space et al. The resources of  are available in   units, respectively.

Recent studies \cite{Kusic09, Fan07} have shown that the power consumption  and the CPU utilization  of a server has a linear relationship

The  and  represent the power consumption by a server at the CPU utilization of 0\% and 100\%, respectively. Obviously, the energy consumption of a server is its power integrated over duration time, i.e., .

\subsection{Virtual Machine Request and Interference}
Cloud computing provides users with scalable, elastic and on-demand resources. Users submit their VM requests to cloud data center scheduler. Each VM request  is specified by an instance vector , where  is the arrival time, and  is the work of processing time when  runs alone. Note that the VMs should start at the arrival time. The capacity vector   represents the resources that  requires for processing its work. For example, an instance type of VM in Amazon EC2 \cite{Amazon} specifics its resource capacity \{CPU:2 vcpu/8 EC2 units, memory:7GB, storage:1680GB\}.

For each pair of  and , it defines the degradation factor  as the percentage increase in the execution time of  when they run concurrently on the same server. It is assumed that the performance degradation factor  between each pair of VMs, when allocating together, is known from existing methods \cite{Govindan11, Mars11, Roytman13, Verboven13, Koh07}, and we focus on the virtual machine scheduling given these factors. Note that  may not equal to  as two VMs will experience different degradation suffering from each other. It is also noted that adding VMs to the server to concurrently run with exist VMs will not reduce the degradation of previous VMs \cite{Roytman13}. It defines the degradation factor  of  when it concurrently runs with a set  of VMs. Without loss of generality, it defines the  as

This model is used to instead of  as it is reasonable to give more severe penalty for performance degradation additive. Then the degradation factor is used to transform the processing time work. I.e., when  concurrently runs with a set  of VMs for duration time , it finishes  work of processing time. To illustrate the behaviour of this interference model between VMs consider the example of Fig.~\ref{fig:example_degradationFactor}. During the first 2 unit time,  is collocated with . Each of them processes 1 unit of work because . In the next 2 unit time, as the  joins in, all of them process 0.5 unit of work because . From the time 4 to 6, it is the same as time 2 to 4, they process 0.5 unit of work.  leaves the server at time 6 when it finishes its processing work. From time 6 to 8,  and  process 1 unit of work.  leaves at time 8 as it finishes its processing work. At last,  will process 1 more unit time to finish its work if it runs alone or the server is assigned VMs that do not cause performance degradation to .

\begin{figure}
\begin{tikzpicture}
    \draw[->, >=latex,line width=1pt] (0.0,0)--(3.0,0) node[pos=1,below] {};
    \draw[->, >=latex,line width=1pt] (0,0.0)--(0,1.25);
    \draw[dotted] (0.0,1.00)--(0.0,0.0) node[below, scale=0.8] {};
    \draw[dotted] (0.5,1.00)--(0.5,0.0) node[below, scale=0.8] {};
    \draw[dotted] (1,1.00)--(1,0.0) node[below, scale=0.8] {};
    \draw[dotted] (1.5,1.00)--(1.5,0.0) node[below, scale=0.8] {};
    \draw (1.5,0.0) node[below=8pt, scale=0.8] {(a)};
    \draw[dotted] (2,1.00)--(2,0.0) node[below, scale=0.8] {};
    \draw[dotted] (2.5,1.00)--(2.5,0.0) node[below, scale=0.8] {};
        
    \draw(0.0,0.75) rectangle (2,0.55) node[pos=0.5,text width=0.25em, scale=0.8]{};

    \draw(0,0.55) rectangle (1.0,0.35) node[pos=0.5,text width=0.25em, scale=0.8]{};
    
    \draw(1.0,0.35) rectangle (2.0,0.15) node[pos=0.5,text width=0.25em, scale=0.8]{};
    
    \draw[->, >=latex,line width=1pt] (3.25,0)--(8.0+0.25,0) node[pos=1,below] {};
    \draw[->, >=latex,line width=1pt] (3.25,0.0)--(3.25,1.25);
    \draw[dotted] (3.25,1.00)--(3.25,0.0) node[below, scale=0.8] {};

    \draw[dotted] (3.75,1.00)--(3.75,0.0) node[below, scale=0.8] {};

    \draw[dotted] (4.25,1.00)--(4.25,0.0) node[below, scale=0.8] {};

    \draw[dotted] (4.75,1.00)--(4.75,0.0) node[below, scale=0.8] {};

    \draw[dotted] (5.25,1.00)--(5.25,0.0) node[below, scale=0.8] {};

    \draw[dotted] (5.75,1.00)--(5.75,0.0) node[below, scale=0.8] {};

    \draw (5.75,0.0) node[below=8pt, scale=0.8] {(b)};

    \draw[dotted] (6.25,1.00)--(6.25,0.0) node[below, scale=0.8] {};

    \draw[dotted] (6.75,1.00)--(6.75,0.0) node[below, scale=0.8] {};

    \draw[dotted] (7.25,1.00)--(7.25,0.0) node[below, scale=0.8] {};

    \draw[dotted] (7.75,1.00)--(7.75,0.0) node[below, scale=0.8] {};
    
    
    \draw(3.25,0.75) rectangle (5.25,0.55) node[pos=0.5,text width=0.25em, scale=0.8]{} ;
    \draw[fill=brown](5.25,0.75) rectangle (7.75,0.55);
    
    \draw(3.25,0.55) rectangle (4.25,0.35) node[pos=0.5,text width=0.25em, scale=0.8]{};
    \draw[fill=brown](4.25,0.55) rectangle (6.25,0.35);
        
    \draw(4.25,0.35) rectangle (5.25,0.15) node[pos=0.5,text width=0.25em, scale=0.8]{};
    \draw[fill=brown] (5.25,0.35) rectangle (7.25,0.15);
 
    \draw[dotted, line width=0.9pt](3.25,0.75) rectangle (7.75,0.15) node[right=0pt,pos=0.98,scale=0.8] {}; 
\end{tikzpicture}
\caption{\label{fig:example_degradationFactor}The execution of VM Instances collocation in a server. Assume the server has resource capacity of \{\} unit. Each VM from \{\} demands \{\} unit and the processing times for the three VMs are given by , , and . The performance degradation factors among them are all . (a): VM Instance configuration. (b): Running in a server.}
\end{figure}

\subsection{Scheduling Problem Description}

There are two issues need to be concerned about the allocation of virtual machines. Cloud infrastructure providers offer some specific kinds of VMs, which tend to reserve resources, such as CPU, memory and storage space. They pursue to reduce the operational cost, i.e., minimize energy consumption or capacity cost. On the other side, cloud users seek to reduce the running time of their requests. In this way, they can save bills for the rented resources.

In our scheduling, time is divided into discrete periods, . For example, the interval  can be one or five minute(s). The binary decision variable  indicates whether  is allocated to  at the time slot . It defines  as the energy consumption cost of  running during time slot , i.e., . Thus, the total operational cost during time slot  is the sum of all running servers, which is calculated as


Let  denote the set of VMs that run at time slot . Define  as the set of VMs that complete their execution and leave at the time slot . Hence, the execution time  of   is


The performance degradation penalty is model by a convex function . One natural model for it is  , which penalizes the delay cost from the processing time . Therefore, the VMs scheduling problem is defined as the following optimization:


The objective function  minimizes the total operational server costs and performance degradation penalties, and  is some constant and represents the relative importance between two objectives. Constraint  ensures that the aggregated resource demand of multiple VMs does not exceed a server's capacity for all resource types and at all time slot. Constraint  relates to that each VM is allocated to one of the servers at any point in time. Constraint  refers to that if a VM has assigned on a server it will not be assigned to other servers. Constraint  follows that  is set to one if  is allocated to  at time slot . We first give the computational complexity of this problem as following:
\newtheorem{theorem}{Theorem}
\begin{theorem}
Find an optimal VMs schedule for arbitrating between operational cost and performance degradation penalty is NP-Complete. \label{theorem:hardness}
\end{theorem}
\begin{proof}
First, we transform the optimization problem to an associated decision problem: given the instance vectors of VMs, the performance degradation factors, and a bound on the sum of energy consumption and performance degradation penalty, is there a schedule such that the bound on sum of cost and penalty is satisfied? Clearly, it belongs to NP, since we can computing and verify in polynomial time that a proposed schedule satisfies the given bound on the sum of operational cost and performance degradation penalty. We next prove that finding an optimal VMs schedule for arbitrating between energy consumption and performance degradation penalty is NP-Complete via the reduction to the 3-Dimensional Matching problem \cite{Garey79, Leung04}.

Consider an instance of 3-Dimensional Matching: Let , , and  be three disjoint sets of  elements each. Let  be a set of triples such that each  consists of one element from , one element from , and one element from . Is there a subset  such that every element in , , and  appears in exactly one triple in ? We construct an instance of VM scheduling problem as follows. Let there be  VMs and  servers. The VMs correspond to the elements in ,  and . For each ,  has resource vector  . For each ,  has instance vector . VMs have no interference with each other in the triples ; otherwise, they have performance degradation factor 1 between each other. The sum of cost and penalty is . The energy consumption cost of a server is  per unit of time slot when it runs at full utilization (Suppose it be  at idle). The sum of cost and penalty is equal to  if and only if the  VMs are scheduled on  servers and do not cause performance degradation. I.e., . Thus, there is an optimal VMs schedule if and only if there is a 3-Dimensional matching. It is clear that the above reduction is a pseudo-polynomial reduction. So we can conclude that the problem is NP-Complete by this pseudo-polynomial time reduction to the 3-Dimensional Matching problem which has been proved to be NP-Complete.
\end{proof}















\section{Virtual Machine Scheduling Design}
\label{sec:algorithm}

As it is a NP-hard combinatorial optimization problem and there is no computationally-efficient solution, we exploit the unique problem structure of VM scheduling in cloud data centers to develop the solutions. We first study a static problem (offline). After then, we develop the solution to the dynamic version of the problem (online).

\subsection{Offline scheduling Design}
In this condition, the informations of VMs that will be scheduled are known at the outset. We propose offline algorithms for virtual machine scheduling and analyse the performance.

\textbf{Bin Packing Variant Algorithm (BPV).} From the perspective of single energy consumption criterion optimization, various packing algorithms are become the reserve choices. It is an obvious advantage to reduce the energy consumption when decreasing the number of active servers. So an algorithm derived from First-Fit bin packing is considered. The algorithm keeps the VMs in a list sorted in increasing order of the arrival time. Each VM is allocated to the first possible accommodated server according to the list order. It invokes a server when capacity violation happens. The difference from First Fit algorithm is VMs will depart from the servers when they finish their work and the relevant resources will be recovered.

\textbf{Minimum Increasing Cost Algorithm (MIC).} Another natural algorithm is greedy differential of increasing costs of energy consumption and performance degradation penalty. The VMs are also kept in the increasing order of their arrival time. It would assign the next VM to the server that minimizes the increment of total cost. There would be two choices for the allocation of next VM. The increment of total cost is the sum of energy consumption and performance degradation penalty when the VM is allocated to an active server running with exist VMs. The other choice is a currently unused server with paying for more static energy consumption that supposing the VM process alone.
\newtheorem{lemma}{Lemma}
\begin{theorem}
Let  denote the maximum number of VMs that can be simultaneously accommodated by a server. The approximation ratio of MIC algorithm is . \label{theorem:approximation}
\end{theorem}
\begin{proof}
Note that for the minimization problem, an algorithm achieves a -approximation factor if for all instances it returns a solution at most  times the optimal value.

We decompose the power of a  at time  into the VMs according to the proportion of their CPU resources. For example, at time , there are  VMs with CPU resource of   (here we set the resource type  as the CPU resource) in  which has CPU resource . Then  consumes  power respectively, where the former part corresponds to the proportion of the static power and the later part is the dynamic power this VM consumes. Without loss of generality, We consider the VM . The power of this VM during its execution time is at least  because . Again, we decompose the total cost of energy consumption and performance degradation penalty to the cost of each VM when it is allocated. According to the MIC algorithm, the cost of inserted  is no more than . So the approximation ratio is

where the second inequality follows from  as , and a mathematical inequality  as . The third inequality results from , i.e., . This concludes the Theorem.
\end{proof}
Remark: BPV algorithm only considers to accept the next VM, and does not take into account the performance degradation. Both of the above algorithms sort the VMs by their arrival time and depend only on the information that is available to the algorithms at the scheduling time. So they are also online algorithms. Note that when a VM is allocated to a server, there is a need to update duration time of itself and other VMs that are interfered by it.

An observation is that these algorithms do not consider the life cycle overlapping of VMs. For example, there are three VMs to be scheduled, which are configured as Fig.~\ref{fig:example_overlapping}. In Fig.~\ref{fig:example_overlapping}(a), both of above algorithms cause 2 servers to be active from time 0 to 11 and 1 to 11. A better scheduling (Fig.~\ref{fig:example_overlapping}(b)) is that it assigns   and  in  and  in . Then we can put  into power-saving mode or turn-off from time 2 to 11.
\begin{figure}
\begin{tikzpicture}
    \draw[->, >=latex,line width=1pt] (0.0,0)--(3.75,0) node[pos=1,below] {};
    \draw[->, >=latex,line width=1pt] (0,0.0)--(0,2);
    \draw[dotted] (0.0,1.75)--(0.0,0.0) node[below, scale=0.8] {};
    \draw[dotted] (0.25,1.75)--(0.25,0.0);
    \draw[dotted] (0.5,1.75)--(0.5,0.0) node[below, scale=0.8] {};
    \draw[dotted] (0.75,1.75)--(0.75,0.0);
    \draw[dotted] (1,1.75)--(1,0.0) node[below, scale=0.8] {};
    \draw[dotted] (1.25,1.75)--(1.25,0.0);
    \draw[dotted] (1.5,1.75)--(1.5,0.0) node[below, scale=0.8] {};
    \draw[dotted] (1.75,1.75)--(1.75,0.0) node[below=6pt, scale=0.8] {(a)};
    \draw[dotted] (2,1.75)--(2,0.0) node[below, scale=0.8] {};
    \draw[dotted] (2.25,1.75)--(2.25,0.0);
    \draw[dotted] (2.5,1.75)--(2.5,0.0) node[below, scale=0.8] {};
    \draw[dotted] (2.75,1.75)--(2.75,0.0);
    \draw[dotted] (3,1.75)--(3,0.0) node[below, scale=0.8] {};
    \draw[dotted] (3.25,1.75)--(3.25,0.0);

    
    \draw(0.0,1.5) rectangle (2.5,1.3) node[pos=0.55,text width=0.25em, scale=0.8] {};
    \draw[fill=brown](2.5,1.5) rectangle (2.75,1.3);
    
    
    \draw(0.0,1.3) rectangle (0.75,0.9) node[pos=0.55, scale=0.8]{};
        
    \draw(0.25,0.75) rectangle (2.75,0.55) node[pos=0.55,text width=0.25em, scale=0.8]{};
    
    \draw[dotted,line width=0.9pt](0.0,1.5) rectangle (2.75,0.9) node[right=0pt,pos=0.98,scale=0.8] {};    
    \draw[dotted,line width=0.9pt](0.0,0.75) rectangle (2.75,0.15) node[right=0pt,pos=0.98,scale=0.8] {}; 
        
    \draw[->, >=latex,line width=1pt] (4.0,0)--(7.75,0) node[pos=1,below] {};
    \draw[->, >=latex,line width=1pt] (4,0.0)--(4,2);
    \draw[dotted] (4.0,1.75)--(4.0,0.0) node[below, scale=0.8] {};
    \draw[dotted] (4.25,1.75)--(4.25,0.0);
    \draw[dotted] (4.5,1.75)--(4.5,0.0) node[below, scale=0.8] {};
    \draw[dotted] (4.75,1.75)--(4.75,0.0);
    \draw[dotted] (5,1.75)--(5,0.0) node[below, scale=0.8] {};
    \draw[dotted] (5.25,1.75)--(5.25,0.0);
    \draw[dotted] (5.5,1.75)--(5.5,0.0) node[below, scale=0.8] {};
    \draw[dotted] (5.75,1.75)--(5.75,0.0) node[below=6pt, scale=0.8] {(b)};
    \draw[dotted] (6,1.75)--(6,0.0) node[below, scale=0.8] {};
    \draw[dotted] (6.25,1.75)--(6.25,0.0);
    \draw[dotted] (6.5,1.75)--(6.5,0.0) node[below, scale=0.8] {};
    \draw[dotted] (6.75,1.75)--(6.75,0.0);
    \draw[dotted] (7,1.75)--(7,0.0) node[below, scale=0.8] {};
    \draw[dotted] (7.25,1.75)--(7.25,0.0);  
    
    \draw(4.0,1.5) rectangle (6.5,1.3) node[pos=0.55,text width=0.25em, scale=0.8] {};
    \draw[fill=brown](6.5,1.5) rectangle (6.75,1.3);
    
    \draw(4.25,1.3) rectangle (6.75,1.1) node[pos=0.55,text width=0.25em, scale=0.8]{};
        
    \draw(4.0,0.75) rectangle (4.75,0.35) node[pos=0.55, scale=0.8]{};

    \draw[dotted,line width=0.9pt](4.0,1.5) rectangle (6.75,0.9) node[right=0pt,pos=0.98,scale=0.8] {};    
    \draw[dotted,line width=0.9pt](4.0,0.75) rectangle (6.75,0.15) node[right=0pt,pos=0.98,scale=0.8] {}; 
 
\end{tikzpicture}
\caption{\label{fig:example_overlapping}BPV+MIC and an improved schedule for three VMs. Assume both physical servers have resource capacity of \{\} unit and ,  and  have resource capacity of \{\}, \{\}, and \{\} unit respectively. The performance degradation factors among them are \{, , \} while the processing times for the three VMs are given by , , and . (a): BPV or MIC scheduling. (b): A better scheduling.}
\end{figure}

\textbf{Maximum Decreasing Cost Algorithm (MDC)}. Instead of sorting the VMs by the arrival time, MDC algorithm considers the information of all VMs and works like the clustering algorithm. We pursue the minimum cost of energy consumption and performance degradation penalty iteratively. Initially, each VM is allocated to a dedicated server. Next, we decide to repeatedly merge servers together by the form of pairs. The process of merging is to collocate the VMs on one server. There is also a need to update duration time of the VMs which cause interference among them. We define the gain function of merging two server,  and  as the following:

where  denotes the total cost of the server according to the cost model defined in Section~\ref{sec:model}-C. With regard to the merger which causes the violation of server capacity, we define the gain as negative number. At each step we choose the merger of two servers that results in the maximum decrease in the total cost. The algorithm ends when the merger of any two servers will produce an negative gain. The pseudo-code for MDC algorithm is summarized in \textbf{\textit{Algorithm.~\ref{algo:MDC_algorithm}}}.
\begin{algorithm}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{the set of VMs }
\Output{the scheduling result of VMs}
\Begin{
Initial Servers =\; \;
\While{}{
MergeServers()\;
Set , \;
\;
}
Return the set of servers  and their accommodated VMs, correspondingly.
}
\caption{\label{algo:MDC_algorithm}Maximum Decreasing Cost Algorithm}
\end{algorithm}

To illustrate the different behaviour of these three scheduling strategies we present an example in Fig.~\ref{fig:behaviour_algo}. BPV aggregates the VMs in parts of servers and leaves some servers to be low utilization. MIC is more likely to assign the subsequent VMs to be included by the anterior ones duration its execution when their performance degradation factor is low. As a result, it considers to balance the VMs between the servers. MDC prefers to collocate the VMs that share long life cycle and have low performance degradation factors between them. In summary, these scheduling algorithms have different performances and we will evaluate them in Section~\ref{sec:exp}.
\begin{figure}
\begin{tikzpicture}
    \draw[->, >=latex,line width=1pt] (0.0,0)--(3.75+0.05,0) node[pos=1,below] {};
    \draw[->, >=latex,line width=1pt] (0,0.0)--(0,2);
    \draw[dotted] (0.0,1.75)--(0.0,0.0) node[below, scale=0.8] {};
    \draw[dotted] (0.25,1.75)--(0.25,0.0);
    \draw[dotted] (0.5,1.75)--(0.5,0.0) node[below, scale=0.8] {};
    \draw[dotted] (0.75,1.75)--(0.75,0.0);
    \draw[dotted] (1,1.75)--(1,0.0) node[below, scale=0.8] {};
    \draw[dotted] (1.25,1.75)--(1.25,0.0);
    \draw[dotted] (1.5,1.75)--(1.5,0.0) node[below, scale=0.8] {};

    \draw[dotted] (1.75,1.75)--(1.75,0.0) node[below=6pt, scale=0.8] {(a)};
    \draw[dotted] (2,1.75)--(2,0.0) node[below, scale=0.8] {};
    \draw[dotted] (2.25,1.75)--(2.25,0.0);
    \draw[dotted] (2.5,1.75)--(2.5,0.0) node[below, scale=0.8] {};
    \draw[dotted] (2.75,1.75)--(2.75,0.0);
    \draw[dotted] (3,1.75)--(3,0.0) node[below, scale=0.8] {};
    \draw[dotted] (3.25,1.75)--(3.25,0.0);
    \draw[dotted] (3.5,1.75)--(3.5,0) node[below, scale=0.8] {}; 
    
    \draw(0.0,1.5) rectangle (2.0,1.3) node[pos=0.55,text width=0.25em, scale=0.8] {};   
    
    \draw(0.25,1.3) rectangle (0.875,0.9) node[pos=0.55, scale=0.8]{};
        
    \draw(0.375,0.75) rectangle (2.75,0.55) node[pos=0.55,text width=0.25em, scale=0.8]{};

    \draw(1.0,0.55) rectangle (2.5,0.30) node[pos=0.45,scale=0.8]{};
     
        
    \draw[->, >=latex,line width=1pt] (4.0,0)--(7.75+0.25,0) node[pos=1,below] {};
    \draw[->, >=latex,line width=1pt] (4,0.0)--(4,2);
    \draw[dotted] (4.0,1.75)--(4.0,0.0) node[below, scale=0.8] {};
    \draw[dotted] (4.25,1.75)--(4.25,0.0);
    \draw[dotted] (4.5,1.75)--(4.5,0.0) node[below, scale=0.8] {};
    \draw[dotted] (4.75,1.75)--(4.75,0.0);
    \draw[dotted] (5,1.75)--(5,0.0) node[below, scale=0.8] {};
    \draw[dotted] (5.25,1.75)--(5.25,0.0);
    \draw[dotted] (5.5,1.75)--(5.5,0.0) node[below, scale=0.8] {};

    \draw[dotted] (5.75,1.75)--(5.75,0.0) node[below=6pt, scale=0.8] {(b)};
    \draw[dotted] (6,1.75)--(6,0.0) node[below, scale=0.8] {};
    \draw[dotted] (6.25,1.75)--(6.25,0.0);
    \draw[dotted] (6.5,1.75)--(6.5,0.0) node[below, scale=0.8] {};
    \draw[dotted] (6.75,1.75)--(6.75,0.0);
    \draw[dotted] (7,1.75)--(7,0.0) node[below, scale=0.8] {};
    \draw[dotted] (7.25,1.75)--(7.25,0.0);
    \draw[dotted] (7.5,1.75)--(7.5,0.0) node[below, scale=0.8] {};  
    
    \draw(4.0,1.5) rectangle (6,1.3) node[pos=0.55,text width=0.25em, scale=0.8] {};
    \draw[fill=brown](6,1.5) rectangle (6.25,1.3);
    
    \draw(4.25,1.3) rectangle (4.875,0.9) node[pos=0.55, scale=0.8]{};
    
    \draw(5.0,1.3) rectangle (6.5,1.1) node[pos=0.45,scale=0.8]{};
    
    \draw(4.375,0.75) rectangle (6.75,0.55) node[pos=0.55,text width=0.25em, scale=0.8]{};
           
    \draw[dotted,line width=0.9pt](4.0,1.5) rectangle (6.75,0.9) node[right=0pt,pos=0.98,scale=0.8] {};    
    \draw[dotted,line width=0.9pt](4.0,0.75) rectangle (6.75,0.15) node[right=0pt,pos=0.98,scale=0.8] {}; 
    
    
  
    \draw[->, >=latex,line width=1pt] (0.0,-2.5)--(3.75+0.05,-2.5) node[pos=1,below] {};
    \draw[->, >=latex,line width=1pt] (0,-2.5)--(0,-0.5);
    
    \draw[dotted] (0.0,1.75-2.5)--(0.0,0.0-2.5) node[below, scale=0.8] {};
    \draw[dotted] (0.25,1.75-2.5)--(0.25,0.0-2.5);
    \draw[dotted] (0.5,1.75-2.5)--(0.5,0.0-2.5) node[below, scale=0.8] {};
    \draw[dotted] (0.75,1.75-2.5)--(0.75,0.0-2.5);
    \draw[dotted] (1,1.75-2.5)--(1,0.0-2.5) node[below, scale=0.8] {};
    \draw[dotted] (1.25,1.75-2.5)--(1.25,0.0-2.5);
    \draw[dotted] (1.5,1.75-2.5)--(1.5,0.0-2.5) node[below, scale=0.8] {};

    \draw[dotted] (1.75,1.75-2.5)--(1.75,0.0-2.5) node[below=6pt, scale=0.8] {(c)};
    \draw[dotted] (2,1.75-2.5)--(2,0.0-2.5) node[below, scale=0.8] {};
    \draw[dotted] (2.25,1.75-2.5)--(2.25,0.0-2.5);
    \draw[dotted] (2.5,1.75-2.5)--(2.5,0.0-2.5) node[below, scale=0.8] {};
    \draw[dotted] (2.75,1.75-2.5)--(2.75,0.0-2.5);
    \draw[dotted] (3,1.75-2.5)--(3,0.0-2.5) node[below, scale=0.8] {};
    \draw[dotted] (3.25,1.75-2.5)--(3.25,0.0-2.5);
    \draw[dotted] (3.5,1.75-2.5)--(3.5,0-2.5) node[below, scale=0.8] {}; 
    
    \draw(0.0,1.5-2.5) rectangle (2,1.3-2.5) node[pos=0.55,text width=0.25em, scale=0.8] {};
    \draw[fill=brown](2,1.5-2.5) rectangle (2.25,1.3-2.5);
    
    \draw(0.25,1.3-2.5) rectangle (0.875,0.9-2.5) node[pos=0.55, scale=0.8]{};
        
    \draw(0.375,0.75-2.5) rectangle (2.75,0.55-2.5) node[pos=0.55,text width=0.25em, scale=0.8]{};
    
    \draw(1.0,0.55-2.5) rectangle (2.5,0.30-2.5) node[pos=0.45,scale=0.8]{};
    
    \draw[dotted,line width=0.9pt](0.0,1.5-2.5) rectangle (2.75,0.9-2.5) node[right=0pt,pos=0.98,scale=0.8] {};    
    \draw[dotted,line width=0.9pt](0.0,0.75-2.5) rectangle (2.75,0.15-2.5) node[right=0pt,pos=0.98,scale=0.8] {}; 
    
    
    \draw[->, >=latex,line width=1pt] (4.0,0-2.5)--(7.75+0.25,0-2.5) node[pos=1,below] {};
    \draw[->, >=latex,line width=1pt] (4,0.0-2.5)--(4,2-2.5);
    \draw[dotted] (4.0,1.75-2.5)--(4.0,0.0-2.5) node[below, scale=0.8] {};
    \draw[dotted] (4.25,1.75-2.5)--(4.25,0.0-2.5);
    \draw[dotted] (4.5,1.75-2.5)--(4.5,0.0-2.5) node[below, scale=0.8] {};
    \draw[dotted] (4.75,1.75-2.5)--(4.75,0.0-2.5);
    \draw[dotted] (5,1.75-2.5)--(5,0.0-2.5) node[below, scale=0.8] {};
    \draw[dotted] (5.25,1.75-2.5)--(5.25,0.0-2.5);
    \draw[dotted] (5.5,1.75-2.5)--(5.5,0.0-2.5) node[below, scale=0.8] {};
 
    \draw[dotted] (5.75,1.75-2.5)--(5.75,0.0-2.5) node[below=6pt, scale=0.8] {(d)};
    \draw[dotted] (6,1.75-2.5)--(6,0.0-2.5) node[below, scale=0.8] {};
    \draw[dotted] (6.25,1.75-2.5)--(6.25,0.0-2.5);
    \draw[dotted] (6.5,1.75-2.5)--(6.5,0.0-2.5) node[below, scale=0.8] {};
    \draw[dotted] (6.75,1.75-2.5)--(6.75,0.0-2.5);
    \draw[dotted] (7,1.75-2.5)--(7,0.0-2.5) node[below, scale=0.8] {};
    \draw[dotted] (7.25,1.75-2.5)--(7.25,0.0-2.5);
    \draw[dotted] (7.5,1.75-2.5)--(7.5,0.0-2.5) node[below, scale=0.8] {};  
    
    \draw(4.0,1.5-2.5) rectangle (6,1.3-2.5) node[pos=0.55,text width=0.25em, scale=0.8] {};
    
    \draw(4.375,1.3-2.5) rectangle (6.75,1.1-2.5) node[pos=0.55,text width=0.25em, scale=0.8]{};
    
    \draw(5.0,1.1-2.5) rectangle (6.5,0.9-2.5) node[pos=0.45,scale=0.8]{};
    
    \draw(4.25,0.75-2.5) rectangle (4.875,0.35-2.5) node[pos=0.55, scale=0.8]{};
    
    \draw[dotted,line width=0.9pt](4.0,1.5-2.5) rectangle (6.75,0.9-2.5) node[right=0pt,pos=0.98,scale=0.8] {};    
    \draw[dotted,line width=0.9pt](4.0,0.75-2.5) rectangle (6.75,0.15-2.5) node[right=0pt,pos=0.98,scale=0.8] {}; 
    
\end{tikzpicture}
\caption{\label{fig:behaviour_algo}Four VMs scheduling. Assume each physical server has resource capacity of \{\} unit. Each VM from \{\} demands \{\} unit while  requires \{\} unit. The processing times for them are given by , , , and . The performance degradation factor  is , and other factors among them are . (a): VM instance. (b): BPV scheduling. (c): MIC scheduling. (d): MDC scheduling.}
\end{figure}

\subsection{Online Algorithm for Dynamic Problem}
In this section, we introduce the online version of the VM scheduling, in which a sequence  of VMs arrive over time, where . Each VM  must be assigned upon its arrival, without information about future VMs . We explore algorithm that schedules each incoming VM by dispatching them to current active servers or a new server that be activated. Recall that BPV and MIC algorithms depend only on the informations that are available to the algorithms when we schedule upon the arrival of a VM  at time  instead of sorting the VMs by their arrival time. They can be modified to support the scheduling in online version, and they are denoted by OBPV and OMIC, respectively. Specially, we derive a competitive ratio for the OMIC algorithm. We say that for the minimization problem, an algorithm is - if for all the problem instances, it returns the cost at most  times the cost of the optimal offline solution. From the Theorem~\ref{theorem:approximation}, we have the following theorem:
\begin{theorem}
The competitive ratio of OMIC algorithm for VM scheduling which aims at arbitrating between operational cost and performance degradation is at most , where  has the same mean of Theorem~\ref{theorem:approximation}. \label{theorem:competitive}
\end{theorem}

\subsection{Incorporating VM batch arrival and VM reservation}
In the previous section, the VMs are consider to arrive one by one and there are no information about the future arriving VMs. In order to match the cloud data centers, we incorporate the following two properties to the scheduling design:
\begin{itemize}
\item There are a set  of VMs to be scheduled at time  due to the many users submit their VMs to the cloud data center at the same time.
\item There are a set  of reserved VMs at time  due to users reserve for lower costs and reserving capacity in the cloud data center.  
\end{itemize}
It defines the time  as scheduling time only when there are some VMs need to be started at this time. According to the definition of the reserved VMs, We have the arrival time relationship . In this situation, the problem is transformed to schedule a set of VMs, i.e., , to be allocated on the cloud data center. The difference from the offline scheduling is that at the begin of the scheduling there are some VMs had been allocated on the cloud data center. Consider an example scheduling time  in previous Fig.~\ref{fig:behaviour_algo}, incoming VM  and reserved VMs  need to be scheduled at this time. We prefer the scheduling of Fig.~\ref{fig:behaviour_algo}(d) to Fig.~\ref{fig:behaviour_algo}(c) as it is known that a server can be put into power-saving mode or shut down only if there are no VMs active on it. Then it causes a problem of which VM first to be scheduled if the OMIC algorithm is used to schedule VM one by one. It defines the alignment ratio of VM to server as following: the ratio of VMs{'} completion time to server's completion time. An obvious intuition is to allocate the VMs to maximize the alignment ratio if these VMs have weak performance interference, i.e., to align the VMs and their server. Then the points is to allocate the VMs to their best candidate server. Based on the above analysis, we present an algorithm from the servers' perspective. First, it supposes that each VM  is allocated on a dedicate virtual server. Each server proposes the profits to other VMs that are allocated on virtual servers. To be specific, it defines the profit metric as the following:

where  is the total cost of a server to run  alone, and  is the increment total cost of run  on . When  cannot be allocated on the server , the  is simply set to negative value. The algorithm allocates the VMs iteratively. In each round, it picks the maximum profit, i.e., . Then the  is allocated on . The algorithm stops when . In this situation, it says that the total cost cannot be improved. The VMs are allocated to their current servers. It is summarized in \textbf{\textit{Algorithm.~\ref{algo:IVP_algorithm}}} (). 
\begin{algorithm}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{the set of VMs , current active servers  at time }
\Output{the scheduling result of VMs}
\Begin{
Set virtual servers ==\;
Set all servers \;
Set , \;
\While{}{
The configuration \;
Update  and Delete  from \;
Set \;
}
Return the plan of VMs.
}
\caption{\label{algo:IVP_algorithm}Incorporating VM Plan Online Algorithm}
\end{algorithm}

We use an example to explain this plan algorithm, as illustrated in Fig.~\ref{fig:behaviour_IVP}. Assume that at time , there is one active server  with  running on it, and there are one VM  arriving and two reserved VMs . As  is the scheduling time, the plan algorithm is triggered. First each VM  is allocated on a virtual server , respectively. Then the profit  is calculated according to Equation.~\ref{eq:profitCalc}. As  is the maximum profit, the algorithm allocates the  to  in the first round. This procedure repeats in the second round, and  is allocated on . At last, it generates an allocation showed in Fig.~\ref{fig:behaviour_IVP}(b). Assume at time , there is one VM  arriving. So the plan algorithm is triggered, and generates an allocation showed in Fig.~\ref{fig:behaviour_IVP}(d). Note that the allocation of reserved VM  is changed from  to .
\begin{figure}
\begin{tikzpicture}
    \draw[->, >=latex,line width=1pt] (0.0,0)--(3.75+0.05,0) node[pos=1,below] {};
    \draw[->, >=latex,line width=1pt] (0,0.0)--(0,2);
    \draw[dotted] (0.0,1.75)--(0.0,0.0) node[below, scale=0.8] {};
    \draw[dotted] (0.25,1.75)--(0.25,0.0);
    \draw[dotted] (0.5,1.75)--(0.5,0.0) node[below, scale=0.8] {};
    \draw[dotted] (0.75,1.75)--(0.75,0.0);
    \draw[dotted] (1,1.75)--(1,0.0) node[below, scale=0.8] {};
    \draw[dotted] (1.25,1.75)--(1.25,0.0);
    \draw[dotted] (1.5,1.75)--(1.5,0.0) node[below, scale=0.8] {};

    \draw[dotted] (1.75,1.75)--(1.75,0.0) node[below=6pt, scale=0.8] {(a)};
    \draw[dotted] (2,1.75)--(2,0.0) node[below, scale=0.8] {};
    \draw[dotted] (2.25,1.75)--(2.25,0.0);
    \draw[dotted] (2.5,1.75)--(2.5,0.0) node[below, scale=0.8] {};
    \draw[dotted] (2.75,1.75)--(2.75,0.0);
    \draw[dotted] (3,1.75)--(3,0.0) node[below, scale=0.8] {};
    \draw[dotted] (3.25,1.75)--(3.25,0.0);
    \draw[dotted] (3.5,1.75)--(3.5,0) node[below, scale=0.8] {}; 
    
    \draw(0.0,1.5) rectangle (2.0,1.3) node[pos=0.55,text width=0.25em, scale=0.8] {};
        
    \draw(0.375,0.75) rectangle (2.75,0.55) node[pos=0.55,text width=0.25em, scale=0.8]{};
    
    \draw(0.25,0.55) rectangle (0.875,0.15) node[pos=0.55, scale=0.8]{};
    
    \draw(1.0,0.55) rectangle (2.5,0.30) node[pos=0.45,scale=0.8]{};
    
    \draw[dotted,line width=0.9pt](0.0,1.5) rectangle (2.75,0.9) node[right=0pt,pos=0.98,scale=0.8] {};   

    \draw[->, >=latex,line width=1pt] (4.0,0)--(7.75+0.25,0) node[pos=1,below] {};
    \draw[->, >=latex,line width=1pt] (4,0.0)--(4,2);
    \draw[dotted] (4.0,1.75)--(4.0,0.0) node[below, scale=0.8] {};
    \draw[dotted] (4.25,1.75)--(4.25,0.0);
    \draw[dotted] (4.5,1.75)--(4.5,0.0) node[below, scale=0.8] {};
    \draw[dotted] (4.75,1.75)--(4.75,0.0);
    \draw[dotted] (5,1.75)--(5,0.0) node[below, scale=0.8] {};
    \draw[dotted] (5.25,1.75)--(5.25,0.0);
    \draw[dotted] (5.5,1.75)--(5.5,0.0) node[below, scale=0.8] {};

    \draw[dotted] (5.75,1.75)--(5.75,0.0) node[below=6pt, scale=0.8] {(b)};
    \draw[dotted] (6,1.75)--(6,0.0) node[below, scale=0.8] {};
    \draw[dotted] (6.25,1.75)--(6.25,0.0);
    \draw[dotted] (6.5,1.75)--(6.5,0.0) node[below, scale=0.8] {};
    \draw[dotted] (6.75,1.75)--(6.75,0.0);
    \draw[dotted] (7,1.75)--(7,0.0) node[below, scale=0.8] {};
    \draw[dotted] (7.25,1.75)--(7.25,0.0);
    \draw[dotted] (7.5,1.75)--(7.5,0.0) node[below, scale=0.8] {};  
    
    \draw(4.0,1.5) rectangle (6,1.3) node[pos=0.55,text width=0.25em, scale=0.8] {};
    
    \draw(4.375,1.3) rectangle (6.75,1.1) node[pos=0.55,text width=0.25em, scale=0.8]{};
    
    \draw(5.0,1.1) rectangle (6.5,0.9) node[pos=0.45,scale=0.8]{};
    
    \draw(4.25,0.75) rectangle (4.875,0.35) node[pos=0.55, scale=0.8]{};
    
    \draw[dotted,line width=0.9pt](4.0,1.5) rectangle (6.75,0.9) node[right=0pt,pos=0.98,scale=0.8] {};    
    \draw[dotted,line width=0.9pt](4.0,0.75) rectangle (6.75,0.15) node[right=0pt,pos=0.98,scale=0.8] {};    
  
    \draw[->, >=latex,line width=1pt] (0.0,-2.75)--(3.75+0.05,-2.75) node[pos=1,below] {};
    \draw[->, >=latex,line width=1pt] (0,-2.75)--(0,-0.5);
    
    \draw[dotted] (0.0,1.75-2.5)--(0.0,0.0-2.75) node[below, scale=0.8] {};
    \draw[dotted] (0.25,1.75-2.5)--(0.25,0.0-2.75);
    \draw[dotted] (0.5,1.75-2.5)--(0.5,0.0-2.75) node[below, scale=0.8] {};
    \draw[dotted] (0.75,1.75-2.5)--(0.75,0.0-2.75);
    \draw[dotted] (1,1.75-2.5)--(1,0.0-2.75) node[below, scale=0.8] {};
    \draw[dotted] (1.25,1.75-2.5)--(1.25,0.0-2.75);
    \draw[dotted] (1.5,1.75-2.5)--(1.5,0.0-2.75) node[below, scale=0.8] {};

    \draw[dotted] (1.75,1.75-2.5)--(1.75,0-2.75) node[below=6pt, scale=0.8] {(c)};
    \draw[dotted] (2,1.75-2.5)--(2,0.0-2.75) node[below, scale=0.8] {};
    \draw[dotted] (2.25,1.75-2.5)--(2.25,0.0-2.75);
    \draw[dotted] (2.5,1.75-2.5)--(2.5,0.0-2.75) node[below, scale=0.8] {};
    \draw[dotted] (2.75,1.75-2.5)--(2.75,0.0-2.75);
    \draw[dotted] (3,1.75-2.5)--(3,0.0-2.75) node[below, scale=0.8] {};
    \draw[dotted] (3.25,1.75-2.5)--(3.25,0.0-2.75);
    \draw[dotted] (3.5,1.75-2.5)--(3.5,0-2.75) node[below, scale=0.8] {}; 
    
    
    \draw(0.0,1.5-2.75) rectangle (2,1.3-2.75) node[pos=0.55,text width=0.25em, scale=0.8] {};
    
    \draw(0.375,1.3-2.75) rectangle (2.75,1.1-2.75) node[pos=0.55,text width=0.25em, scale=0.8]{};
    
    \draw(1.0,1.1-2.75) rectangle (2.5,0.9-2.75) node[pos=0.45,scale=0.8]{};
    
    \draw(0.25,0.75-2.75) rectangle (0.875,0.35-2.75) node[pos=0.55, scale=0.8]{};
    
    \draw[dotted,line width=0.9pt](0.0,1.5-2.75) rectangle (2.75,0.9-2.75) node[right=0pt,pos=0.98,scale=0.8] {};    
    \draw[dotted,line width=0.9pt](0.0,0.75-2.75) rectangle (2.75,0.15-2.75) node[right=0pt,pos=0.98,scale=0.8] {};
    
    \draw(0.875,1.5+0.15+0.2-2.75) rectangle (2.75,1.5+0.15-2.75) node[pos=0.50,scale=0.8]{};
    
    
    \draw[->, >=latex,line width=1pt] (4.0,0-2.75)--(7.75+0.25,0-2.75) node[pos=1,below] {};
    \draw[->, >=latex,line width=1pt] (4,0.0-2.75)--(4,2-2.5);
    \draw[dotted] (4.0,1.75-2.5)--(4.0,0.0-2.75) node[below, scale=0.8] {};
    \draw[dotted] (4.25,1.75-2.5)--(4.25,0.0-2.75);
    \draw[dotted] (4.5,1.75-2.5)--(4.5,0.0-2.75) node[below, scale=0.8] {};
    \draw[dotted] (4.75,1.75-2.5)--(4.75,0.0-2.75);
    \draw[dotted] (5,1.75-2.5)--(5,0.0-2.75) node[below, scale=0.8] {};
    \draw[dotted] (5.25,1.75-2.5)--(5.25,0.0-2.75);
    \draw[dotted] (5.5,1.75-2.5)--(5.5,0.0-2.75) node[below, scale=0.8] {};

    \draw[dotted] (5.75,1.75-2.5)--(5.75,0.0-2.75) node[below=6pt, scale=0.8] {(d)};
    \draw[dotted] (6,1.75-2.5)--(6,0.0-2.75) node[below, scale=0.8] {};
    \draw[dotted] (6.25,1.75-2.5)--(6.25,0.0-2.75);
    \draw[dotted] (6.5,1.75-2.5)--(6.5,0.0-2.75) node[below, scale=0.8] {};
    \draw[dotted] (6.75,1.75-2.5)--(6.75,0.0-2.75);
    \draw[dotted] (7,1.75-2.5)--(7,0.0-2.75) node[below, scale=0.8] {};
    \draw[dotted] (7.25,1.75-2.5)--(7.25,0.0-2.75);
    \draw[dotted] (7.5,1.75-2.5)--(7.5,0.0-2.75) node[below, scale=0.8] {};  
    
    \draw(4.0,1.5-2.75) rectangle (6,1.3-2.75) node[pos=0.55,text width=0.25em, scale=0.8] {};
    
    \draw(4.375,1.3-2.75) rectangle (6.75,1.1-2.75) node[pos=0.55,text width=0.25em, scale=0.8]{};
    
    \draw(4.875,1.1-2.75) rectangle (6.75,0.9-2.75) node[pos=0.50,scale=0.8]{};
    
    \draw(5.0,0.75-2.75) rectangle (6.5,0.55-2.75) node[pos=0.45,scale=0.8]{};
    
    \draw(4.25,0.75-2.75) rectangle (4.875,0.35-2.75) node[pos=0.55, scale=0.8]{};
    
    \draw[dotted,line width=0.9pt](4.0,1.5-2.75) rectangle (6.75,0.9-2.75) node[right=0pt,pos=0.98,scale=0.8] {};    
    \draw[dotted,line width=0.9pt](4.0,0.75-2.75) rectangle (6.75,0.15-2.75) node[right=0pt,pos=0.98,scale=0.8] {}; 
    
\end{tikzpicture}
\caption{\label{fig:behaviour_IVP}Incorporating VMs batch arrival and VMs reservation scheduling. Assume each physical server has resource capacity of \{\} unit.  have resource capacity \{\}, \{\}, \{\}, \{\}, and \{\} unit respectively. The processing times for them are given by , , , , and . The performance degradation factor  is , and other factors among them are . (a):  Initial servers and VMs. (b):  Allocation result. (c):  Initial servers and VMs. (d):  Allocation result.}
\end{figure}

The time complexity of this algorithm is  where  is the number of servers () and  is the number of VMs (). This is followed from that each server  proposes a profit to VM , and in each round we fix a VM .

\subsection{Distributed Design towards Data Center Scale}
In a large data center, it is time-consuming to gather the detailed information about each server and run the algorithm on a single server. We now propose a distribution scheme, which opposes to the algorithm introduced in the previous section that centralizes the information and picks up the best candidate server. Upon each new VM arriving at the data center, the information of VMs, which are waiting to be allocated, are passed to each active server. Each server  maintains the information of VMs. Next, the algorithm proceeds in stages, and synchronizes using a common clock. In the first stage, a single client server  proposes profits to the VMs and sends the maximum profit  of  to the distribution server (dispatcher). The distribution server collects the maximum profits from all client servers and chooses the -, i.e., the current maximum benefit from the allocation of  on . Then the allocation decision of stage  is broadcast to client servers. In the subsequent stages, the client server  receives the decision message  from the distribution server, and proceeds the following two chooses: 1) If the profit of its  stage is chosen, it fixes the  on it and removes the  from the unscheduled VMs. 2) If the profit of its  stage is not chosen, it justly removes the  from the unscheduled VMs. The procedure of profit proposing is the same as the previous stage. Assume there are some VMs unscheduled, i.e., they are not benefit from being allocated to the client servers or cannot be allocated to current active servers. We run \textit{Algorithm.~\ref{algo:IVP_algorithm}} to schedule them with the input of these VMs and some current inactive servers. The pseudo-code of this algorithm is summarized in \textbf{\textit{Algorithm.~\ref{algo:TDPP_algorithm}}}.
\begin{algorithm}
\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
\Input{the set of VMs  at time }
\Output{the allocation results of VMs}
\\
\Begin{
Initialized round: Broadcast \;
\ForEach{round }{
Receive message \;
\If{}
{
Pick =\;
Broadcast \;
Update \;
}
}
\If{}
{
Run \textbf{\textit{Algorithm.~\ref{algo:IVP_algorithm}}}, i.e., ;
}
Return the allocation of VMs.
}
\\
\Begin{
\ForEach{server  in parallel}{
Receive message  from \;
\uIf{}{
Save \;
Set ==\;
Set \;
Set \;
Send  to the \;
}
\uElseIf{}{
\If{}{
Fix  on  and Update \;
}
Set ==\;
Set \;
Set \;
Send  to the \;
}
}
}
\caption{\label{algo:TDPP_algorithm}VM Profit Plan Algorithm}
\end{algorithm}



\section{Performance Evaluation}
\label{sec:exp}
In this section, we study the performance of proposed algorithms on several comprehensive VM scheduling problems.

\subsection{Evaluation Setup}
\textit{Simulation Settings}: The simulations are run in a data center that equips physical servers with computing resource of  Cores (E.g., \textit{HP ProLiant DL385 G6}). For simplicity, we assume that the servers are homogeneous in the data center and the VMs take up the total resource of their request demand. The configuration of VMs refers to the types of instances available in Amazon EC2 \cite{Amazon}. For example, a type VM, so called \textit{m1.small}, with  Core computing units,  GB memory and 160 GB storage space. As we focus on studying the arbitrating between energy consumption and performance interference degradation, we omit other resource bounds, such as memory, in the simulations. Four different types of VMs are available to be chosen with computing resource of  Core,  Cores,  Cores,  Cores, respectively. The simulations are conducted on two different scenarios corresponding to offline and online VMs scheduling. In the offline problem, there is a list of VMs that are waiting to be processed. In the online scenario, the VMs arrive randomly over time.

\textit{Compared Baseline Algorithms}: To provide benchmarks for our evaluations, we introduce other three algorithms:
\begin{itemize}
\item Random Strategy: it is a naive algorithm which randomly schedules the next VM on a physical server as long as the server has enough resource to host the VM.
\item Round Robin: it allocates the next VM on physical servers in turn, which is a used scheduling algorithm in Amazon EC2 \cite{Amazon}.
\item Minimum Increase Energy: it assigns the next VM to the server which minimizes the increment of energy consumption. Note that this algorithm is different from MIC, as MIC considers the increment of total cost. 
\end{itemize}

In all of the algorithms, it assumes that a new server will open if the next VM cannot be allocated on current active servers.

\textit{Parameters}: The power of a server is characterized by the three parameters of Equation.~\ref{eq:powerCalc}. We use HP ProLiant DL385 G6 with a 2 Chips/12 Cores processor. According to the server power consumption parameters, we set  and . The performance degradation cost is characterized by parameters:  and . The parameter  represents the intensity of performance degradation penalty, and without loss of generality it is set to . The tuning parameter  is used for adjusting the energy consumption cost and performance degradation penalty, and also used for representing the weight between two costs.

\textit{Performance Metrics}: To evaluate the performance of proposed algorithms, we use the following four metrics:
\begin{itemize}
\item Normalized Total Energy Consumption: It shows the quality of the solution produced by the proposed algorithms in terms of total energy consumption.
\item Normalized Total Performance Degradation Penalty: This metric represents the penalties of the solution produced by the proposed algorithms in terms of performance degradation cost.
\item Normalized Total Cost: It is defined as the sum of energy consumption cost and performance degradation penalty.
\item Normalized Worst Degradation Factor: This indicates the worst performance degradation factor of VMs caused by the scheduling algorithms.
\end{itemize}

In the offline problem, we also record the total number of used physical servers and the makespan of the VMs.

\subsection{Evaluation of real workload}
We first conduct a small-scale experiment to evaluate the performance of the proposed algorithms. The performance degradation ratio is obtained from the statistics of SPECcpu 2006 benchmarks \cite{Kim13}. The properties (such as the arrival time, et.al) of the applications are drawn from a real OpenCloud Hadoop cluster trace \cite{Ren13}. The result is shown on Fig.~\ref{fig:eva_real_workload}. From the figure it can be seen that the overall cost are reduced apparently. More precisely, the MDC algorithm saves the total cost about 41\% compared with the BPV algorithm. This demonstrate the competitive advantages of the proposed algorithms against the methods which do not provide a unified consideration of both the energy consumption and the performance interference.
\begin{figure}[htbp]
\centering
\includegraphics[width=3.6in]{Figure0}
\centering
\caption{\label{fig:eva_real_workload} Performance of algorithms on a real workload. (The result is normalized against BPV.)}
\end{figure}

\subsection{Evaluation of offline Problem Solution}
We now go to present our large-scale simulation results on algorithms proposed for offline problem. In this scenario, the computing resource required for a VM is uniformly chosen from four given types at random. We set each time slot as  minute. The duration time of VMs is randomly generated from  and the arrival time of VMs is randomly generated from . The number of VMs varies from  to  to emulate the low workload and heavy workload in the data center. It considers two kinds of degradation factors between VMs. One is generated from the normal distribution and the other is generated from the exponential distribution. For each simulation, we run proposed algorithms and compared algorithms using the same list of VMs, and the results of randomized algorithm is the average of running 10 times.


\textbf{Resource Violation:} In the first simulation of this evaluation, we examine the resource violation of algorithms when they do not take into account the performance degradation and do not update the duration time of VMs. In Fig.~\ref{fig:eva_resource_violation}, we show the resource violation of a server scheduled from the BPV algorithm. As we can see the computing resource surpasses the capacity during some periods (E.g.,  to ) due to the stretch of VM execution time.
\begin{figure}[htbp]
\centering
\includegraphics[width=3.6in]{Figure11}
\centering
\caption{\label{fig:eva_resource_violation}Resource violation of a server in BPV scheduling. The green dashed line means the CPU capacity of a server, i.e., 12 Cores. (a) CPU Resource used of a server from omitting the performance degradation; (b) The true CPU resource used when it considers the performance degradation stretching the execution time of VMs.}
\end{figure}


\textbf{Algorithm Performance Comparison:} We now discuss the performance of all above algorithms against the BPV algorithm with respect of four metrics, i.e., normalized energy consumption, performance degradation penalty, total cost and worst performance degradation factor. In this simulation, the degradation factor between VMs is generated from the normal distribution   if . The results are depicted in Fig.~\ref{fig:eva_performance_algo}. As we can see, the performance of MDC algorithm is apparently better than other algorithms on reducing the performance degradation penalty. More precisely, the performance degradation penalty of MDC is  against BPV, while MIC is . Another observation from Fig.~\ref{fig:eva_performance_algo} is that MIC and MDC performs much better on worst performance degradation factor, which is important in data center due to SLA requirement. When considering the energy consumption, we find that MIC and MDC also have a slight reduction compared with other algorithms. This is because MIC and MDC reduce the unnecessary execution time due to the performance degradation causing by interference. As a result, MIC and MDC reduce the total cost of energy consumption and performance degradation penalty up to  and , respectively. It should be noted, however, the total cost of MDC is reduced  when comparing with MIC.
\begin{figure*}[htbp]
\centering
\includegraphics[width=7.5in]{Figure22}
\centering
\caption{\label{fig:eva_performance_algo}Performance of algorithms. (a) Normalized energy consumption against BPV; (b) Normalized performance degradation penalty against BPV; (c) Normalized total cost against BPV; (d) Normalized worst performance degradation factor against BPV.}
\end{figure*}


\textbf{Impact of Performance Degradation Factor:} We next investigate the impact of performance degradation factor on proposed algorithms. We keep the number of VMs fixed and execute the above algorithms with five different kinds of degradation factor between VMs. They are generated from the normal distributions , , , , , respectively  if . This corresponds to the interference between VMs is more fluctuation when the variance is changed from  to . In each normal distribution, we generate 5 groups of degradation factor, and we generate 3 groups of VMs list. The result is the average of the cross simulations, i.e., 15 times. Table~\ref{tb:eva_perf_degFactorN} shows how degradation factor affects the total cost. When the performance interference between VMs becomes more intensive, the total cost of BPV, RAND, RR and MIE algorithms become much larger, because they do not take into account the performance degradation penalty when making the scheduling decisions. For MIC and MDC algorithms, we can see that they lead to a slight total cost increment due to their intelligent scheduling. This rule also holds when we generate degradation factor between VMs from exponential distributions , , , , , , respectively. (The results are listed in Table~\ref{tb:eva_perf_degFactorE}.)
\begin{table}[htbp]
\centering
\caption{\label{tb:eva_perf_degFactorN}Algorithm performance in different  degradation factor, and the result is normalized against .}
\begin{tabular}{c|c|c|c|c|c}\hline\hline
Algorithm&0.2&0.4&0.6&0.8&1.0\\\hline
BPV&1&186.83&200&200&200\\
RAND&1&1583.17&2000&2000&2000\\
RR&1&10.60&23.87&200&200\\
MIE&1&2.78&8.87&14.48&188.06\\\hline
MIC&1&1.0307&1.0333&1.0381&1.0406\\
MDC&1&1.0304&1.0480&1.0551&1.0567\\\hline\hline
\end{tabular}
\end{table}
\begin{table}[htbp]
\centering
\caption{\label{tb:eva_perf_degFactorE}Algorithm performance in different  degradation factor, and the result is normalized against .}
\begin{tabular}{c|c|c|c|c|c|c}\hline\hline
Algo.&100&50&20&10&5&2\\\hline
BPV&1&1.0549&1.2728&2.1169&200&200\\
RAND&1&1.0447&1.2130&1.7677&230&300\\
RR&1&1.0415&1.1938&1.6070&13.5063&200\\
MIE&1&1.0437&1.2089&1.6300&3.6698&263.4512\\\hline
MIC&1&1.0471&1.1683&1.3006&1.4120&1.5360\\
MDC&1&1.0383&1.1179&1.2003&1.3072&1.4544\\\hline\hline
\end{tabular}
\end{table}

\textbf{Impact of Workload Density:} We compare the proposed algorithms on five data sets: , , ,  and  VMs, and in each of them the arrival time of VMs is generated from the same range . I.e., the number of data set from  to  represents the increment of workload density. The degradation factor between VMs is also generated from the normal distribution   if . Fig.~\ref{fig:eva_perf_density} presents the results. In all simulations with different intensity, the minimum total cost is achieved by MDC due to its more global view. Moreover, the improvement margin is stable with the increment of workload density. MIC and MDC perform better in more intensive load. This is attributed to the fact that the other four algorithms would lead to more performance degradation when they do not take into account the performance interference in heavy load.
\begin{figure}[htbp]
\centering
\includegraphics[width=3.6in]{Figure31}
\centering
\caption{\label{fig:eva_perf_density}Impact of different workload density.}\label{fig:graph}
\end{figure}
\begin{figure*}[htbp]
\centering
\includegraphics[width=7.5in]{Figure42}
\centering
\caption{\label{fig:eva_perf_weight}Impact of different weight  between the operational cost and performance degradation penalty.}\label{fig:graph}
\end{figure*}

\textbf{Impact of Weight of Performance Degradation Penalty:} As mentioned in the optimization model of Section~\ref{sec:model}-C, the weight  is some constant incorporating the normalization and the relative importance of the performance degradation penalty. Our virtual machine scheduling exploits this weight, and now we study the impact of this weight on the performance of the proposed algorithms. In this simulation, we focus on four metrics: total cost, worst performance degradation factor, the number of servers to be used and makespan. The results are depicted in Fig.~\ref{fig:eva_perf_weight}. The first observation from the Fig.~\ref{fig:eva_perf_weight} is that as the weight increases, the results of worst performance degradation factor, the number of servers to be used and makespan mainly stay the same in BPV, RAND, RR and MIE. This is because the scheduling of these four algorithms is not influenced by the weight. The results of their total cost increase only because the weight  grows. Another remark is that the worst performance degradation factor reduces quickly in MIC and MDC when the weight surpasses a certain value (In our simulation, e.g., ). However, the number of servers to be used increases a little when we give more weight to performance degradation penalty, which leads to makespan metric reduction. Obviously, the improvement of total cost in MIC and MDC getting more with an increment of the weight.

\subsection{Evaluation of Online Algorithm}
We finally present the simulation on our online algorithms in the dynamic environment. It should be noted that the aforementioned algorithms except MDC can be transformed to the online versions, so the corresponding results are also held. We focus on evaluating the performance of \textit{Algorithm.~\ref{algo:IVP_algorithm}} which incorporates the VM batch arrival and VM reservation. In this simulation, we fix the total number of VMs and their performance degradation factors. We adopt different sizes of VMs to be revealed at each scheduling time to represent the VMs with batch arrival and reservation. There are  VMs revealed at each scheduling time if the size is . For example, it corresponds to one by one scheduling when the size is 1 (like OMIC), and there are 10 VMs revealed at each scheduling time when the size is 10. We run the algorithm with five randomly generated instances (1\#---5\#) and one sequentially generated instance (6\#) on each size. The result is depicted in Fig.~\ref{fig:eva_perf_online}. As we can see the performance is better with the the number of size increases, i.e., more information about VMs are revealed due to batch arrival and reservation. In addition, the total cost have a much improvement from the number of size 1 to 2. In summary, the \textit{Algorithm.~\ref{algo:IVP_algorithm}} exploits the properties of VMs scheduling in cloud data centers and obtains a better improvement of the total cost than OMIC algorithm which do not consider these properties.
\begin{figure}[htbp]
\centering
\includegraphics[width=3.6in]{Figure51}
\centering
\caption{\label{fig:eva_perf_online}Total cost of different size  of VMs to be revealed at each scheduling time.}\label{fig:graph}
\end{figure}



\section{Conclusion}
\label{sec:conclusion}
In this paper, we present virtual machine scheduling for arbitrating between operational cost and performance interference in cloud data centers. While previous works only provide energy consumption management or performance interference optimization separately, we are among the first to build a joint model to capture the inherent tradeoff between the two contradictory objectives. We also develop efficient scheduling algorithms for both offline and online cases and improve them by exploiting some properties in clouds such as resource reservation. We evaluate the performance of the proposed algorithms by a comprehensive set of simulations. Our results confirm that a joint optimization that takes into account both VM combination and life-cycle overlapping can significantly reduce the operational cost, as well as the performance interference in cloud data centers.
















\begin{thebibliography}{99}
\small
\bibitem{VMware}
VMware. http://www.vmware.com.
\bibitem{Xen}
Xen. http://xen.org.
\bibitem{Perera12}
D. Perera. The real obstacle to federal cloud computing. FiereceGovernmentIT. 2012. 
\bibitem{Govindan11}
S. Govindan, J. Liu, A. Kansal and A. Sivasubramaniam. Cuanta:Quantifying effects of shared on-chip resource interference for consolidated virtual machines. In \emph{Proceedings of the 2nd ACM Symposium on Cloud Computing (SOCC{'}11)}, No.22, 2011.
\bibitem{Chiang11}
R. C. Chiang, and H. H. Huang. TRACON:Interference-aware scheduling of data-intensive applications in virtualized environments. In \emph{Proceedings of the 2011 International Conference for High Performance Computing, Networking, Storage and Analysis (SC{'}11)}, No.47, 2011.
\bibitem{Mars11}
J. Mars, L. Tang, R. Hundt, K. Skadron and M. L. Soffa. Bubble-Up:increasing utilization in modern warehouse scale computers via sensible co-locations. In \emph{Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO{'}11)}, pages 248-259, 2011.
\bibitem{Roytman13}
A. Roytman, A. Kansal, S. Govindan, J. Liu and S. Nath. PACMan:Performance aware virtual machine consolidation. In \emph{Proceedings of the 10th International Conference on Autonomic Computing (ICAC{'}13)}, 2013.
\bibitem{Kim13}
S. Kim, H. Eom and H. Y. Yeom. Virtual machine consolidation based on interference modelling. In \emph{Journal of Supercomputing, 66(3)}, pages 1489-1506, 2013.
\bibitem{Verboven13}
S. Verboven, K. Vanmechelen and J. Broeckhove. Black box scheduling for resource intensive virtual machine workloads with interference models. In \emph{Journal of Future Generation Computer Systems, 29(8)}, pages 1871-1884, 2013.
\bibitem{SPEC06}
SPECcpu2006 Benchmark. http://www.spec.org/cpu2006/Docs.
\bibitem{Hamilton09}
J. Hamilton. Cooperative expendable micro-slice servers (CEMS): low cost, low power servers for internet-scale services. In \emph{Proceedings of the 4th Biennial Conference on Innovative Data Systems Research (CIDR{'}09)}, 2009.
\bibitem{Amokrane2013}
A. Amokrane, M. F. Zhani, R. Langar, R. Boutaba and G. Pujolle. Greenhead: Virtual Data Center Embedding across Distributed Infrastructures. In \emph{IEEE Transactions on Cloud Computing, 1(1)}, pages 36-49, 2013.
\bibitem{Albers10}
S. Albers. Energy-efficient algorithms. In \emph{Communications of the ACM, 53(5)}, pages 86-96, 2010.
\bibitem{Nathuji07}
R. Nathuji, K. Schwan. VirtualPower:coordinated power management in virtualized enterprise systems. In \emph{Proceedings of twenty-first ACM SIGOPS symposium on Operating systems principles (SOSP{'}07)}, pages 265-278, 2007.
\bibitem{Kusic09}
D. Kusic, J. O. kephart, J. E. Hanson, N. Kandasamy and G. Jiang. Power and performance management of virtualized computing environments via lookahead control. \emph{Journal of Cluster Computing, 12(1)}, pages 1-15, 2009.
\bibitem{Beloglazov12}
A. Beloglazov, J. Abawajy, R. Buyya. Energy-aware resource allocation heuristics for efficient management of data centers for cloud computing. In \emph{Journal Future Generation Computing Systems, 28(5)}, pages 755-768, 2012.
\bibitem{Lin11}
M. Lin, A. Wierman, L. Andrew and E. Thereska. Dynamic right-sizing for power-proportional data centers. In \emph{Proceedings of the 30nd Annual IEEE International Conference on Computer Communications (INFOCOM{'}11)}, pages 1098-1106, 2011.
\bibitem{Liu13}
F. Liu, Z. Zhou, H. Jin, B. Li, B. Li and H. Jiang. On Arbitrating the Power-Performance Tradeoff in SaaS Clouds. In \emph{IEEE Transactions on Parallel and Distributed Systems, Vol.99}, 2013.
\bibitem{Xu14}
F. Xu, F, Liu, H. Jin and A. V. Vasilakos. Managing performance overhead of virtual machines in cloud computing: a survey, state of the art, and future directions. In \emph{Proceedings of the IEEE, 102(1)}, pages 11-31, 2014.
\bibitem{Esch14}
J. Esch. Prolog to "Managing performance overhead of virtual machines in cloud computing: a survey, state of the art, and future directions". In \emph{Proceedings of the IEEE, 102(1)}, pages 7-10, 2014.
\bibitem{Fan07}
X. Fan,W. Weber and L.A. Barroso.Power provisioning for a warehouse-size computer. In \emph{Proceedings of the 34th Annual International Symposium on Computer Architecture (ISCA{'}07)}, pages 13-23, 2007.
\bibitem{Amazon}
Amazon EC2. http://aws.amazon.com/ec2.
\bibitem{Koh07}
Y. Koh, R. Knauerhase, P. Brett, M. Bowman, Z. Wen and C. Pu. An analysis of performance interference effects in virtual environments. In \emph{Proceedings of the IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS{'}07)}, pages 200-209, 2007.
\bibitem{Garey79}
M. R. Garey and D. S. Johnson. \emph{Computers and intractability: A guide to the theory of NP-completeness.} W.H. Freeman, New York, 1979.
\bibitem{Leung04}
J. Leung. \emph{Handbook of scheduling.} CRC Press, Inc., Boca Raton, FL, USA, 2004.
\bibitem{Ren13}
K. Ren, Y. Kwon, M. Balazinska and B. Howe. Hadoop{'}s adolescence: an analysis of Hadoop usage in scientific workloads. \emph{Journal of VLDB Endowment, 6(10)}, pages 853-864, 2013.
\end{thebibliography}

\end{document}
