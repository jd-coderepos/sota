\documentclass[letterpaper, 10 pt, conference]{amsart}
\renewcommand{\thefootnote}{\roman{footnote}}

 \usepackage[foot]{amsaddr}
\usepackage{graphicx,dblfloatfix,subcaption}
\usepackage{wrapfig}
\usepackage[innercaption]{sidecap}
\usepackage{tikz,epstopdf,pgfplots,pdfpages}
\usepackage{framed}  
\usepackage{blkarray}
\usepackage{longtable}
\usepackage{enumerate}\usepackage[author={Sofie},draft]{pdfcomment}
\usepackage{amsmath,amssymb,amsfonts}
 	\usepackage{algorithm}
	\usepackage{algorithmic}
 	\interdisplaylinepenalty=2500
\usepackage{mathtools}
\usetikzlibrary{decorations.shapes,shapes.geometric,shadows,arrows,automata,positioning,calendar,mindmap,backgrounds,scopes,chains,er,patterns,pgfplots.groupplots}
  \usetikzlibrary{calc} 

\newcommand{\err}{\varepsilon}
 \newcommand{\mathscr}[1]{\mathcal{#1}}
  \newcommand{\mc}[1]{\mathcal{#1}}
	\newtheorem{theorem}{Theorem}
	\newtheorem{thm}[theorem]{Theorem}
	\newtheorem{prop}[theorem]{Proposition}
	\newtheorem{cor}[theorem]{Corollary}
	\theoremstyle{definition}
	\newtheorem{definition}{Definition}
	\newtheorem{defn}[definition]{Definition}
\theoremstyle{example}
	\newtheorem{example}{Example}
			\newtheorem{condition}{Condition}
			\newtheorem{claim}{Claim}
			\newtheorem{prob}{Problem}
			\theoremstyle{remark}
			\newtheorem{remark}{Remark}


 
 \newcommand{\newsymb}[4]{
 \newcommand{#2}{#3} 
  }

 
\newsymb{Hilbert}	{\Hilbert}	{\mathcal{H}_2}
{The Hilbert space} 
\newsymb{Reals}		{\Real}		{\mathbb R}
{The set of reals}        
\newsymb{Naturals}	{\Nat}		{\mathbf{N}}
{The set of natural numbers}             
\newsymb{Complex}	{\Complex}	{\mathbb C}  
{The set of complex numbers}


\newcommand{\N}[2]{\mathcal{N}\left(#1,#2\right)}{} 

\newcommand{\po}{\mathbb{P}}     \newcommand{\p}[1]{\po\left(#1\right)}     \newcommand{\pd}[1]{p\left(#1\right)}     \newcommand{\borel}[1]{\mathcal{B}\left(#1\right)}
 
\newcommand{\ind}[2]{\mathbf{1}_{#1} \left(#2\right)}
\newcommand{\norm}[1]{\left\Vert#1\right\Vert}
\newsymb{norm}{\normempty}{\norm{\cdot}}  
{The Euclidean norm }

\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\supp}{\operatorname{supp}}

\newcommand{\eps}{\epsilon}  
 
\newsymb{system}{\s}{\mathbf{S}}{The true `(cyber-)physical' system}
\newsymb{model}{\M}{\mathbf{M}}{A mathematical model}
\newsymb{detmodel}{\Md}{\bar{\mathbf{M}}}{The minimal realisation of  when the noise input is neglected.}



\newsymb{statespace}{\X}{\mathbb{X}}{The state space, i.e. the set of states}     \newsymb{state}{\xv}{\mathbf{x}}{An element of the state space }   
    \newcommand{\x}[1]{\mathbf{x}(#1)} \newsymb{actionspace}{\A}{\mathbb{U}}{The set of controllable inputs}    
   	\newcommand{\ac}[1]{u(#1)}         \newsymb{actions}{\acv}{u}{An element of the set of controllable input }   
\newsymb{outputspace}{\Y}{\mathbb{Y}}{The set of output values}     \newcommand{\y}[1]{\hat{y}(#1)}          \newsymb{output}{\yv}{y}{An element of the set of outputs}
\newsymb{performanceoutput}{\Z}{\mathbb{Z}}{The set of performance outputs}     

   
\newsymb{observer}{\Obs}{\mathbf{O}}{An observer}

   
   
   
\newsymb{TS}{\TS}{\Sigma}{A transition system}
	\newcommand{\I}[1]{{{#1}_a}} \newcommand{\II}[1]{{{#1}_b}} 

	\newsymb{XTS}{\XTS}{\mathscr{X}}{The set of states of a transition system}
	\newsymb{xTS}{\xTS}{x}{The set of states of a transition system}
	\newsymb{UTS}{\UTS}{\mathscr{A}}{The set of inputs to a transition system} 
	\newsymb{uTS}{\uTS}{u}{The set of inputs to a transition system}
	\newsymb{ZTS}{\ZTS}{\mathscr{Z}}{The set of possible infinite observations}
	 \newsymb{HTS}{\HTS}{\mathscr{H}}{The labelling map}
	
\newsymb{initialstates}{\init}{\mathbb{I}}{The set of initial states}



\newsymb{Markov}{\Markov}{\mathcal{M}}{A Markov process}
\newsymb{Relation}{\rel}{\mathcal{R}}{A relation}
\newsymb{EquivRelation}{\erel}{\mathcal{R}}{An equivalence  relation}



\newsymb{similated}{\simu}{\preceq_{\mathcal{S}}}{Simulated by}
\newsymb{bisimilar}{\bsimu}{\sim_{\mathcal{B}}}{Bisimulation equivalence}
\newsymb{simulationequiv}{\esimu}{\simeq_{\mathcal{S}}}{Simulation equivalence}
\newsymb{altsimilated_app}{\asimu}{\preceq^\eps_{\mathcal{AS}}}{-approximate alternating simulation}
\newsymb{similated_app}{\simuapp}{\preceq^\eps_{\mathcal{S}}}{-approximate simulated by}



 


\newsymb{until}{\un}{\mbox{\textbf{U}}}{until operator}
\newcommand{\alwaysi}[1]{\Box^{#1}}   \newcommand{\eventuallyi}[1]{\diamondsuit^{#1}}   \newcommand{\always}{\Box}
\newcommand{\eventually}{\Diamond}
\newcommand{\until}{\mathbin{\sf U}}
\newcommand{\release}{\mathbin{\sf R}}
\newcommand{\nex}{\mathord{\bigcirc}}

\newcommand{\alphabeth}{\Sigma}


\newcommand{\Lab}{\textmd{L}}   \newcommand{\Labex}{L}      \newcommand{\Lang}[1]{\mathcal{L}(#1)}
\newcommand{\BLang}{\mathcal{L}_\psi}
\newcommand{\Sexec}{\Omega}     \newcommand{\exec}{\omega}  

\newcommand{\maxLang}{\Sigma^{\mathbb{N}}}
\newcommand{\BLTL}{\psi}        \newcommand{\BLTLtext}{BLTL }   \newcommand{\AP}{AP}            \newcommand{\safe}{\mathcal{I}}  \newcommand{\pol}{\mathcal{P}}
\newcommand{\threshold}{\delta} \newcommand{\Coloneqq}{::=}      



\newcommand{\Fint}{\mathcal{F}}    

\newcommand{\Fg}{\mathcal{F}_g} \newcommand{\F}{\mathcal{F}} 


\newcommand{\horizon}{{N_t} }   

\newcommand{\Mset}{\mathcal{G}}\newcommand{\Var}{R}
      \newcommand{\data}{
    \underline{\tilde{\mathbf{y}}}_{N_s}}
	\newcommand{\inputdata}{\underline{\mathbf{u}}_{N_s}}
	\newcommand{\ExpSet}{\mathcal{E}}
	


\newcommand{\Nsample}{{N_s}}    \newcommand{\Nparam}{n}         \newcommand{\pa}{\theta}        \newcommand{\parEst}{\hat{\pa}_{\Nsample}} \newcommand{\parTrue}{\pa^0}               

\newcommand{\w}{w}



\newcommand{\B}{\mathcal{B}} 		

\newcommand{\InF}{\mathcal{U}_{v}}
\newcommand{\XX}{L}
\newcommand{\C}{{\mathbf{C}}}




\makeatletter
\newcommand\tabfill[1]{\dimen@\linewidth \advance\dimen@\@totalleftmargin \advance\dimen@-\dimen\@curtab \parbox[t]\dimen@{#1\ifhmode\strut\fi}}
\newcommand{\Wt}{\mathbb{W}_{\mathbb{T}}}
\newcommand{\pcm}[2]{\po_{\scalebox{0.5}[.5]{}}}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Initialise:}}
\renewcommand{\algorithmiccomment}[1]{\hfill \{#1\}}

 



\title[approximate similarity relations and policy refinement]{Verification of general Markov decision processes by approximate similarity relations and policy refinement} 



\author[S. Haesaert]{S. Haesaert\textsuperscript{1}}
\address{\textsuperscript{1}Department of Electrical Engineering, 
Eindhoven University of Technology}
\author[S. Esmaeil Zadeh Soudjani ]{S. Esmaeil Zadeh Soudjani\textsuperscript{2}}
\author[A. Abate]{A. Abate\textsuperscript{2}}
\address{\textsuperscript{2}Department of Computer Science, 
University of Oxford }
 \begin{document}
\begin{abstract}
In this work we introduce new approximate similarity relations that are shown to be key for policy (or control) synthesis over general Markov decision processes.  
The models of interest are discrete-time Markov decision processes, 
endowed with uncountably-infinite state spaces and metric output (or observation) spaces. 
The new relations, underpinned by the use of metrics, allow in particular for a useful trade-off between   
deviations over probability distributions on states, 
and distances between model outputs.  
We show that the new probabilistic similarity relations, 
inspired by a notion of simulation developed for finite-state models,   
can be effectively employed  over general Markov decision processes for verification purposes, 
and specifically for control refinement from abstract models.    
\end{abstract}



\maketitle         
\section{Introduction}
  \setcounter{footnote}{1}
The formal verification of computer systems allows for the quantification of their properties and for their correct functioning.  
Whilst verification has classically focused on finite-state models, 
with the ever more ubiquitous embedding of digital components into physical systems  
richer models are needed and correct functioning can only be expressed over the combined behaviour of both the digital computer and the surrounding physical system.  
It is in particular of interest to synthesise the part of the computer software that controls or interacts with he physical system automatically, 
with low likelihood of malfunctioning. Furthermore, 
when computers interact with physical systems such as biological processes, power networks, and smart-grids, 
stochastic models are key. 
Consider, as an example, a power network for which we would like to quantify the likelihood of blackouts and to synthesise strategies to minimise this. 


Systems with uncertainty and non-determinism can be naturally modelled as Markov decision processes (MDP).  
In this work, we focus on general Markov decision processes (gMDP) that have uncountable state spaces as well as metric output spaces. 
The characterisation of properties over such processes cannot in general be attained analytically \cite{Abate1}, 
so an alternative is to approximate these models by simpler processes that are prone to be mathematically analysed or algorithmically verified \cite{SAID}, such as finite-state MDP \cite{FAUST13}.   
Clearly, it is then key to provide formal guarantees on this approximation step,  such that solutions of the verification or synthesis problem for a property on the simpler process can be extended to the original model. Our verification problems include the synthesis of a policy (or a control strategy) that maximises the likelihood of the specification of interest. 


In this work we develop a new notion of approximate similarity relation, 
aimed to attain a computationally efficient controller synthesis over Markov decision processes with metric output spaces. 
We show that it is possible to obtain a control strategy for a gMDP as a refinement of a strategy synthesised for an abstract model, 
at the expense of accuracy defined on a similarity relation between them, 
which quantifies bounded deviations in transition probabilities and output distances. 
In summary, 
we provide results allowing us to quantitatively relate the outcome of verification problems performed over the simpler (abstract) model to the original (concrete) model, 
and further to refine control strategies synthesised over the abstract model to strategies for the original model.

\smallskip 

The use of similarity relations on \emph{finite-state} probabilistic models has been broadly investigated, 
either via exact notions of probabilistic simulation and bisimulation relations  
\cite{larsen1991bisimulation,Segala1995,Segala1995a},   
or (more recently) via approximate notions \cite{Desharnais2008,cDAK12}. 
On the other hand, 
similar notions over \emph{general, uncountable-state spaces} have been only recently studied:  
available relations either hinge on stability requirements on model outputs \cite{Julius2009a,ZMMAL14} (established via martingale theory or contractivity analysis), 
or alternatively enforce structural abstractions of a model \cite{desharnais2004metrics} by exploiting continuity conditions on its probability laws \cite{Abate2011,bcAKNP14}.  

In this work,   
we want to quantify properties with a certified precision \emph{both} in the deviation of the probability laws for finite-time events (as in the classical notion of probabilistic bisimulation) and of the output trajectories (as studied for dynamical models).  
Additionally, we impose no strict requirements on the dynamics of the given gMDP and its abstraction. 
To these ends, 
we first extend the exact probabilistic simulation and bisimulation relations based on lifting for finite-state probabilistic automata and stochastic games \cite{Segala1995,Segala1995a,Zhang2010a} to gMDP (Section \ref{sec:exact}).  
We then generalise these notions to allow for errors on the probability laws \emph{and} deviations over the output space (Section \ref{sec:epsdelta}). 
Two case studies in the area of smart buildings (Section \ref{sec:case}) are used to evaluate these new approximate probabilistic simulation relations. 
Unlike cognate recent work \cite{Abate2011,Julius2009a}, 
we are interested in similarity relations that allow refining over the concrete model a control strategy synthesised on the abstract one. 
We zoom in on relations that, quite like the alternating notions in \cite{Alur1998,Tabuada2009b} for non-probabilistic models and in \cite{Zhang2010a} for stochastic ones, 
quantitatively bound the difference in the controllable behaviour of pairs of models (namely a gMDP and its abstraction).  
In Appendix \ref{sec:lit} we show how over a class 
of Markov processes (without controls), 
this newly developed approximate similarity relation practically generalises notions of probabilistic \mbox{(bi-)simulations} of Labeled Markov processes \cite[based on zigzag-morphisms]{Desharnais2002},\cite[based on equivalence relations]{Desharnais2003}, and  
 their  approximate versions \cite[based on binary relations]{desharnais2004metrics,Desharnais2008,cDAK12}. 
 

\section{Verification of general Markov decision processes: problem setup}  
\subsection{Preliminaries and notations} 
Given two sets  and ,  the Cartesian product of  and  is given as . 
The disjoint union of  and  is denoted as  and consists of the combination of the members of  and , where the original set membership is the distinguishing characteristic that forces the union to be disjoint,i.e.,  As usual for  we denote .
For the sets  and  a relation  is a subset of their Cartesian product that relates elements  with elements , denoted as .
We use the following notation for the mappings  and   for  and .
A relation over a set defines a preorder if it is reflexive, ; and transitive,  if  and  then . A relation  is an equivalence relation if it is reflexive, transitive and symmetric,  if  then .
 
A measurable space is a pair  with sample space  and -algebra  defined over , 
which is equipped with a topology.    
As a specific instance of  consider the Borel measurable space . 
In this work, we restrict our attention to Polish spaces and generally consider the Borel -field \cite{bogachev2007measure}. 
Recall that a Polish space is a separable completely metrisable topological space. 
In other words, 
the space admits a topological isomorphism to a complete metric space which is dense with respect to a countable subset. 
A simple example of such a space is the real line.  

A probability measure  for  is a non-negative map, 
 such that  and such that for all countable collections  of pairwise disjoint sets in , 
it holds that  
.   
Together with the measurable space, such a probability measure  defines the probability space, which is denoted as  and has realisations  .   
Let us further denote the set of all probability measures for a given measurable pair  as .  
\setcounter{footnote}{0} 
For a probability space\footnote{The index  in  distinguishes the given -algebra on  from that on , which is denoted as . 
Whenever possible this index will be dropped.}   and a measurable space , a - valued \emph{random variable}  is a function  that is -measurable, 
 and which induces the probability measure  in .  
For a given set  a metric or distance function  is a function . 

\subsection{gMDP models - syntax and semantics}
General Markov decision processes are related to control Markov processes \cite{Abate2011} and Markov decision processes \cite{bible,mt1993,hll1996}, 
and formalised as follows.  
\begin{defn}[Markov decision process (MDP)] \label{def:MDP}
The tuple  defines a discrete-time MDP over an uncountable state space , and is characterised by , a conditional stochastic kernel that assigns to each point  and control  a probability measure  over . For any set , , where  denotes the conditional probability . The initial probability distribution is . 
\end{defn}

At every state the state transition depends non-deterministically on the choice of .  
When chosen according to 
a distribution  , we refer to the stochastic control input as . Moreover   
the transition kernel is denoted as . 
Given a string of inputs ,  
over a finite time horizon , 
and an initial condition   (sampled from distribution ), 
the state at the -st time instant, ,
is obtained as a realisation of the controlled Borel-measurable stochastic kernel  -- 
these semantics induce paths (or executions) of the MDP.   
\begin{defn}[General Markov decision process (gMDP)] 
 is a discrete-time gMDP consisting of an MDP combined with output space  and a measurable output mapping .   
A metric  decorates the output space .  \end{defn}
The gMDP semantics are directly inherited from those of the MDP. 
Further, output traces of gMDP are obtained as mappings of MDP paths, namely 
, 
where . Denote the 
class of all gMDP with the metric output space  as . Note that gMDP 
can be regarded as a super-class of the known labelled Markov processes (LMP) \cite{desharnais2004metrics} as elucidated in \cite{bcAKNP14}. 
\begin{example}\label{ex11}
Consider the stochastic process 
with variables , 
taking values in , 
representing the state, control input\footnote{ In other domains one also refers to the control variables as actions (Machine Learning, Stochastic Games) or as external non-determinism (Computer science).}, and noise
terms respectively. 
The process is initialised as , 
and driven by , 
a white noise sequence with zero-mean normal distributions and covariance matrix . This stochastic process, defined as a dynamical model, 
is a gMDP characterised by a tuple , 
where the conditional transition kernel is defined as , 
a normal probability distribution with mean  and covariance matrix .\qed
\end{example} 

A policy is a selection of control inputs based on the past history of states and controls. 
We allow controls to be selected via universally measurable maps \cite{bible} from the state to the control space, 
so that time-bounded properties such as safety can be maximised \cite{Abate1}. 
When the selected controls are only dependent on the current states, 
and thus conditionally independent of history (or memoryless), 
the policy is referred to as Markov. 
\begin{defn}[Markov policy]\label{def:markovpolicy}
For a gMDP , a Markov policy  is a sequence  of universally measurable maps  , from the state space  to the set of controls.
\end{defn}
Recall that a function  is universally measurable  if the inverse image of every Borel set is measurable with respect to every complete probability measure on  that measures all Borel subsets of . 

The execution
 initialised by  and controlled with Markov policy  is a stochastic process defined on the canonical sample space  endowed with its product topology . 
This stochastic process has a probability measure  uniquely defined by the transition kernel , policy , 
and initial distribution  \cite[Prop. 7.45]{bible}.  \\
Of interest are time-dependent properties such as those expressed as specifications in a temporal logic of choice.   
This leads to problems where one maximises the probability that a sequence of labelled sets is reached within a time limit and in the right order. 
One can intuitively realise that in general the optimal policy leading to the maximal probability is not a Markov (memoryless) policy, as introduced in Def. \ref{def:markovpolicy}. 
We introduce the notion of a control strategy, and define it as a broader, memory-dependent version of the Markov policy above. 
This strategy is formulated as a Markov process that takes as an input the state of the to-be-controlled gMDP. 
\begin{defn}[Control strategy]\label{def:CS}
A control strategy  for a gMDP  with state space  and control space  over the time horizon  is an \emph{inhomogenous Markov process} with 
state space ;  
an initial state ;  inputs  ; time-dependent, universally measurable kernels , ;  
and with universally measurable output maps , ,
with elements . \qed  
\end{defn}
Unlike a Markov policy, the control strategy is in general dependent on the history, as it has an internal state that can be used to remember relevant past events.  
As elucidated in Algorithm \ref{alg:CM}, 
note that the first control  is selected by drawing  according to , 
where  and selecting  from measure .\footnote{Note that the stochastic transitions for the control strategy and the gMDP are selected in an alternating fashion. The output map of the strategy is indexed based on the time instant at which the resulting policy will be applied to the gMDP. }
The control strategy applied to  can be both stochastic (as a realisation of \mbox{\,}), 
a function of the initial state , 
and of time.  

The execution  of a gMDP  controlled with strategy  is defined on the canonical sample space  endowed with its product topology . 
This stochastic process is associated to a unique probability measure , 
since the stochastic kernels  for  and  are Borel measurable and composed via universally measurable policies \cite[Prop. 7.45]{bible}. 
\begin{algorithm}[htp]
\caption{Execution of the controlled model }\label{alg:CM}
\begin{algorithmic}
\STATE{set  and  }
\STATE{draw  }\COMMENT{from }
\WHILE{}
\STATE{draw   }\COMMENT{from }
\STATE{set , draw  from }
\STATE{draw  }\COMMENT{from }
\STATE{set }
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\subsection{gMDP verification and strategy refinement: problem statement} 
We qualitatively introduce the main problem that we want to solve in this work: 
How can one provide a general framework to synthesise control policies over a formal abstraction  of a concrete complex model ,  
with the understanding that  is much simpler to be manipulated (analytically or computationally) than  is? 
We approach this problem by defining a simulation relation under which a policy  
for the abstract Markov process  implies the existence of a policy   for , 
so that we can quantify differences in the stochastic transition kernels and in the output trajectories for the two controlled models.  
This allows us to derive bounds on the probability of satisfaction of a specification for  from 
the satisfaction probability of modified specifications for . We will show that with this setup we can deal with finite-horizon temporal properties, including safety verification as a relevant instance.   


The  results in this paper are to be used in parallel with optimisation, 
 both for selecting the control refinement and for synthesising a policy on the abstract model. It has been shown in \cite{bible} that stochastic optimal control  even for a system on a ``basic"  space can lead to measurability issues: 
in order to avoid these issues we follow \cite{bible,Desharnais2008} and the developed theory for Polish spaces and Borel (or universally) measurable notions.  Throughout the paper we will give as clarifying examples Markov processes evolving, as in Example \ref{ex11}, over Euclidean spaces which are a special instances of Polish spaces. 
 This allows us to elucidate the theory. 
 
\section{Exact (bi-)simulation relations based on lifting}\label{sec:exact}
\subsection{Introduction}
In this section we define probabilistic simulation and bisimulation relations that are, respectively, a preorder and an equivalence relation on . 
Before introducing these relations, 
we first extend Segala's notion \cite{Segala1995,Segala1995a} of \emph{lifting} to uncountable state spaces, which allows us to equate the transition kernels of two given gMDPs.  
Thereafter, we leverage liftings to define (bi-)simulation relations over , which characterise the similarity in the controllable behaviours of the two gMDPs.  
Subsequently we show that these similarity relations also imply controller refinement, 
i.e., within the similarity relation a control strategy for a given gMDP can be refined to a controller for another gMDP. 
In the next section, we show that this exact notion of similarity allows a more general notion of approximate probabilistic simulation. 
The new notions of similarity relations extend the known exact notions in \cite{larsen1991bisimulation}, and the approximate notions of \cite{Desharnais2008,cDAK12}. 
Additionally,  we will show that these results can be naturally extended to allow for both differences in probability and deviations in the outputs of the two gMDPs. 


We work with pairs of gMDP put in a relationship, denoting them with numerical indices (), 
with the intention to apply the developed notions to an abstraction  of a concrete model , 
respectively.  

\subsection{Lifting for general Markov decision processes}
 
Consider two gMDP  mapping to a common output space  with metric .
For  and  at given state-action pairs  and , respectively, 
we want to relate the corresponding transition kernels, 
namely the probability measures   and . 

Similar to the coupling of measures in  \cite{art2014,lindvall2002lectures}, 
consider the \emph{coupling} of two arbitrary probability spaces  and  (cf. \cite{skala1993,strassen1965}).  
A probability measure  
defined on  \emph{couples} the two spaces if the projections , ,  
with  and , 
define respectively an - and an -valued random variables, 
such that  and .   
For \emph{finite- or countable-state} stochastic processes a related concept has been introduced in \cite{Segala1995,Segala1995a}  
and referred to as \emph{lifting}:  
the transition probabilities are coupled using a weight function in a way that respects a given relation over the combined state spaces. Rather than using weight functions over a countable or finite domain \cite{Segala1995}, 
we introduce lifting as a coupling of measures over Polish space and their corresponding Borel measurable -fields. 


Since we assume that the state spaces are Polish and have a corresponding Borel -field for the given probability spaces  and  with 
    and , the natural choice for the -algebra becomes   \footnote{  denotes the product -algebra of  and .}  and the question of finding a coupling can be reduced to finding a probability measure in  .  
 
\begin{defn}[Lifting for general state spaces] \label{def:lifting}
Let  be two sets with associated measurable spaces  and  and let the Borel measureable set  be a relation. 
We denote by 
 the corresponding lifted relation, 
so that  holds if there exists a probability space  
(equivalently, a lifting ) satisfying 
{ \setlength{\parskip}{-1pt}\setlength{\parsep}{0pt} \begin{enumerate}
\item for all : ;
\item  for all :  ;
\item for the probability space   it holds that 
 with probability , or equivalently that .
\end{enumerate}}\noindent
\end{defn} 
\smallskip 
With reference to the connection with the notion of coupling, 
an equivalent definition of lifting is obtained be replacing  and  by the condition that for   the projections , , with  and , we can define  and -valued random variables  and . 
An example is portrayed in Fig. \ref{fig:lifting1} containing two models  and a relation (denoted by equally labelled/coloured pairs of states), where the transition kernels for a pair of states is lifted with respect to the relation. 
  \begin{figure}[htp] 
     \centering
     \includegraphics[width=3in]{FSMarkov.eps} 
     \includegraphics[width=1.75in]{FSMarkovW.eps} 
     \caption{ Finite-state Markov processes  and  (left  middle) with   and  the respective state spaces.  
   The states are labelled with three different colours.
   Lifting probabilities of the transition kernels for  are given on the edges of the rightmost figure.}\label{fig:lifting1}\end{figure}


\begin{remark}\label{rem:measdiag}
Notice that the extension of the notion of lifting to general spaces has required the use of \emph{measures}, 
rather than weight functions over a countable or finite domain, as in \cite{Segala1995}. 
We have required that the -algebra  contains not only sets of the form  and , 
but also specifically the sets that characterise the relation . 
Since the spaces  and  have been assumed to be Polish, 
it holds that every open (closed) set in  belongs to  \cite[Lemma 6.4.2]{bogachev2007measure}. 
As an instance consider the diagonal relation  over , 
of importance for examples introduced later. This is a Borel measurable set \cite[Theorem 6.5.7]{bogachev2007measure}. 
\qed
\end{remark}

\subsection{Exact probabilistic (bi-)simulation relations via lifting} 
  
Similar to the alternating notions for probabilistic game structures in \cite{Zhang2010a}, 
we provide a simulation that relates any input chosen for the first process with one for the second process. 
As such, we allow for more elaborate handling of the inputs than in the probabilistic simulation relations discussed in \cite{Desharnais2008,cDAK12}, 
and further pave the way towards the inclusion of output maps.  
We extend the notions in \cite{Segala1995,Zhang2010a} by allowing for more general Polish spaces. 
Further, we introduce the notion of \emph{interface function} in order 
to connect the controllable behaviour of two gMDP:     

where we require that  is a Borel measurable function. 
This means that  induces a Borel measurable stochastic kernel, 
again denoted by , 
over  given .
The notion of interface function is known in the context of correct-by-design controller synthesis and of hierarchical controller refinement \cite{Girard2009,Tabuada2009b}.   
For the objective of hierarchical controller refinement, 
an interface function implements (or refines) any control action synthesised over the abstract model to an action for the concrete model. 
In order to establish an exact simulation relation between abstract and concrete models,  
we can attempt to refine the control actions from one model to the other by choosing an interface function that matches their stochastic behaviours.  
On the other hand in the next section, the interface function will be used to establish approximate simulation relations: 
for this goal, the optimal selection of the interface function is the one that optimises the accuracy of the relation.  
This is topic of ongoing research. 

In this work we extend standard interface functions for deterministic systems by allowing randomised actions . 
The lifting of the transition kernels for the chosen interface generates a stochastic kernel  conditional on the values of signals in  and in .  
Let us trivially extend the interface function to  
\begin{defn}[Probabilistic simulation]\label{def:pbsim}
Consider two gMDP , .    
The gMDP  is stochastically simulated by  if there exists an interface function  and
a relation , for which there exists a Borel measurable stochastic kernel  on  given ,
such that { \setlength{\parskip}{-1pt}\setlength{\parsep}{0pt}
\begin{enumerate}
\item , ;  
\item ,   
 with lifted probability measure ;  
\item .
\end{enumerate} }\noindent The relationship between the two models is denoted as .  
\end{defn}  
The Borel measurability for both  (see above) and  (as in this definition), 
which is technically needed for the well posedness of the controller refinement, 
can be relaxed to universal measurability, as will be discussed in the Appendix.  
\begin{defn}[Probabilistic bisimulation]\label{def:pbbi}
Under the same conditions as above, 
 is a probabilistic bisimulation of  if there exists a   relation  such that  w.r.t.  and  w.r.t. the inverse relation .
 and  are said to be probabilistically bisimilar, 
which is denoted .   
\end{defn}
For every gMDP : 
.
This can be seen by considering the diagonal relation  and selecting equal inputs  for the associated interfaces. 
The resulting equal
 transition kernels  
  are lifted by the measure   where  denotes the Dirac distribution located at .  
\begin{example}[Lifting for diagonal relations]\label{ex:1} \mbox{ }\\ 
\noindent{}~Consider the gMDP  introduced in Ex. \ref{ex11} and a slight variation of it , given as stochastic dynamic processes, 

with variables  taking values in , 
and with dynamics initialised with the same probability distribution at  and driven by white noise sequences , 
both with zero mean normal distributions and with variance , respectively.  Notice that if  is positive definite then .  To see this, 
select the control input pair  as , 
and  according to the zero-mean normal distribution with variance , 
then the associated \emph{interface }is .  
For this interface the stochastic dynamics of the two processes are equal, 
and can be lifted with .\\*\noindent{}~
Similar as above, consider 
two gMDP modelled as Gaussian processes 
 
with variables  taking values in  and , matrices , , .  Then  , since in 
for every action  chosen for , the choice of \emph{interface}   for  results in the same transition kernel for the second model.   
 

\end{example}
\begin{remark} 
Over , 
the class of gMDP with a shared output space, 
the relation  is a preorder, 
since it is reflexive (see Example \ref{ex:1}) and transitive (see later Cor.~\ref{cor:prop}). 
Moreover the relation  is an equivalence relation as it is also symmetric (see Cor.~\ref{def:pbbi}). 
\end{remark} 
\subsection{Controller refinement via probabilistic simulation relations}\label{sec:refine_exact}

The ideas underlying the controller refinement are first discussed,  
after which it is shown that the refined controller induces a strategy as per Def. \ref{def:CS}. 
Finally the equivalence of properties defined over the controlled gMDPs is shown. 

Consider two gMDP   with .
Given the entities  and  associated to , 
the distribution of the next state  of  is given as , 
and is equivalently defined via the lifted measure as the marginal of  on . 
Therefore, the distribution of the combined next state , 
defined as , 
can be expressed as

where  is referred to as the 
conditional probability given  (c.f. \cite[Corollary 3.1.2]{borkar2012probability}).\footnote{ Beyond Borel measurability, this also holds when the kernels are universally measurable, as corresponding universally measurable regular conditional probability measures are obtained \cite{Edalat1999a}.} Similarly, the conditional measure for the initialisation   
 is denoted as 
. 

Now suppose that we have a control strategy for , referred to as , and 
we want to construct the refined control strategy  for ,  
which is such that  
events defined over the output space have equal probability. 
This refinement procedure follows directly from the interface and the conditional probability distributions, 
and is described in Algorithm \ref{alg:refinement}. 
This execution algorithm is separated into the refined control strategy  and its gMDP . 
 is composed of ,  the stochastic kernel , and the interface , 
and it remembers the previous state of  (cf. line \ref{alg2:remx2} in Algorithm \ref{alg:refinement}). \smallskip

\begin{algorithm}[htp]\caption{Refinement of control strategy  as }\label{alg:refinement}\begin{algorithmic}[1]
\STATE{set \\
\STATE draw   from , \\
\STATE draw  from .  
}
\LOOP
\STATE{given , select  according , }
\STATE  set , \\*
\STATE draw  from ,
 \STATE  draw  from ,  \label{alg2:remx2}
\STATE  set . 
\ENDLOOP
\end{algorithmic} \end{algorithm}\begin{thm}[Refined control strategy]\label{thm:Cs1}
Let gMDP  and  be related as , 
and consider the control strategy  for  as given. 
Then there exists at least one refined control strategy , 
as defined in Def. \ref{def:CS}, with \begin{itemize}
\item state space , 
with elements ; 
\item initial state ;
\item input variable , namely the state variable of ;  
\item time-dependent stochastic kernels , defined as
\item measurable output maps . 
\qed  
\end{itemize}
\end{thm}
Both the time-dependent stochastic kernels  and the output maps , for , are universally measurable, 
since Borel measurable maps are universally measurable and the latter are closed under composition \cite[Ch.7]{bible}.  
 

Since, by the above construction of , 
the output spaces of the controlled systems  and  have equal distribution, 
it follows that measurable events have equal probability, as stated next and proved in the Appendix. 

\begin{theorem}\label{thm:events} 
If , then for all control strategies  there exists a control strategy  such that, 
for all measurable events ,  

\end{theorem}

\section{{New -approximate (bi-)simulation relations via lifting}}\label{sec:epsdelta}  
\subsection{Motivation and -lifting}
The requirement on an exact simulation relation between two models is evidently restrictive. 
Consider the following example, where two Markov processes have a bounded output deviation.  

\begin{example}[Models with a shared noise source] \label{Ex:delISS} 
Consider an output space , 
with a metric  (the Euclidean norm), 
and two gMDP expressed as noisy dynamic processes: \&\M_1:   
x_1(t+1)=f(x_1(t),u_1(t))+ e_1(t)
,&\ y_1(t)&=h(x_1(t)),&\\ &\M_2:   
x_2(t+1)=f(x_2(t),u_2(t))+ e_2(t)
, &\ 
y_2(t)&=h(x_2(t)) , &
\textstyle\|x_1(t+1)-x_2(t+1)\|\leq L \|x_1(t)-x_2(t)\|+c\|y_1(t)-y_2(t)\|\leq \frac{cH}{1-L}\mathbb T_1(\cdot| x_1, u_1)\ \bar \rel_\delta \  \mathbb T_2(\cdot| x_2, \InF(u_1,x_1,x_2)), \pcm{\C_1}{\M_1}\!\left(\{ y_1 (t)\}_{_{0:N}}\!\!\in\! A_{_{\scalebox{0.7}[.7]{\mbox{}}}}\right)-\gamma \leq \pcm{\C_2}{\M_2}\!\left(\{y_2 (t)\}_{_{0:N}}\!\!\in\! A\right)\leq\pcm{\C_1}{\M_1}\!\left(\{y_1 (t)\}_{_{0:N}}\!\!\in\! A_{\eps}\right)+\gamma, 
 \textstyle&A_\eps:=\!\big\{\{y_\eps(t)\}_{0:N}| \exists \{y(t)\}_{0:N}\in A:  \textstyle\max_{{t\in [0,N]}}  \mathbf d_\Y(y_\eps(t),y(t))\leq \eps\big\}A_{-\eps}:= \{\{y(t)\}_{0:N}|\{\{y(t)\}_{0:N}\}_{\eps}\subset A \}
\Wt(dx_1'\mid x_2', u_1,x_2,x_1), 
\textstyle \Wt (dx_1'\times dx_2'| u_1,x_1,x_2):= \int_{\omega}\delta_{f(x_1,u_1)+g_1(\omega)} (dx_1')  \delta_{f(x_2,u)+g_2(\omega)} (dx_2') \mathbb P_\omega(d\omega),\M_1:  
x(t+1)=f(x(t),u(t))+ e(t) y(t)=h(x(t)),\M_2:  
x(t+1)=f(x(t),u(t))+ \tilde e(t),y(t)=h(x(t)).& 
\textstyle\Wt(dx_1'\times dx_2'|u_1,x_1,x_2):=
 \int_{e\in D} \delta_{x_1'} (dx_2')  \delta_{t_1(e)} (dx_1') \po_e(de)\label{eq:ref truncnoise}\\&\textstyle\hspace{4cm}
+ \int_{e\in\mathbb R^n\setminus D} \delta_{t_1(e)} (dx_1')\po_e(de)
\int_{\tilde e}\delta_{t_2(\tilde e)}  (dx_2') \po_e(d\tilde e|D)  \notag 
 \M_3: 
x(t+1)=f(x(t),u(t)),\,
y(t)=h(x(t))\M_1\preceq^{\delta_a}_{\epsilon_a}\M_2 \textmd{ and }\M_2\preceq^{\delta_b}_{\epsilon_b}\M_3, \textmd{ then }\M_1\preceq^{ \cramped{\delta_a+\delta_b}}_{ \cramped{\epsilon_a+\epsilon_b}}\M_3.&\textstyle\Wt(dx_1'\times dx_3'|u_1,x_1,x_2)=\\&\qquad \textstyle\int_{\X_2} \mathbb W_{23}(dx_3'|x_2',{\InF}_{12}(u_1,x_1,x_2),x_2,x_3)\mathbb W_{12}(dx_1'\times d x_2'|u_1,x_1,x_2).\M_1\approx_{\eps}^\delta \M_3. &\M:x(t+1)=A x(t)+ B  u(t) + Fe(t),\label{eq:casedyn}\hspace{2cm}\ y(t)=\begin{bsmallmatrix}1&0&0\\0&1&0\end{bsmallmatrix}x(t),\shortintertext{with stable dynamics characterised by matrices}
A&=\begin{bsmallmatrix}
    0.8725  &  0.0625 &   0.0375\\
    0.0625  &  0.8775 &   0.0250\\
         0  &       0 &   0.9900\end{bsmallmatrix}\notag ,\quad B =\begin{bsmallmatrix*}[l]
    0.0650& 0\\
         0&0.60\\
         0&0
\end{bsmallmatrix*} ,\quad F=\begin{bsmallmatrix*}[r] 0.05&    -0.02  &       0\\
   -0.02   &  0.05    &      0\\
         0    &     0 &   0.1 \end{bsmallmatrix*}, 

\tilde\M:\left\{\begin{array}{lll}
 \tilde x(t+1)&= \tilde A\tilde x(t)+ \tilde B  \tilde u(t)\in \mathbb R^2, \mbox{ with }&\tilde A:=\begin{bsmallmatrix}
    0.8725  &  0.0625 \\
    0.0625  &  0.8775 \\
\end{bsmallmatrix},\label{eq:casedyn2}\\
\tilde y(t)&=\begin{bsmallmatrix}1&0\\0&1\end{bsmallmatrix}\tilde x(t),  \quad&\tilde B:=\begin{bsmallmatrix}
    0.0650& 0\\
         0&0.60
\end{bsmallmatrix}.
\end{array}\right.
\InF(\tilde u,\tilde x,x)=\tilde u+\tilde B^{-1}(\tilde A \tilde x - \bar Ax )
\Wt(d\tilde x'\times d x'\mid \tilde u,\tilde x,x ) = \int_{e}
 \delta_{\tilde f }(d\tilde x')\delta_{f(e)}(d  x') \mathcal N(de\,{\mid}\,0,I), .45em]
\begin{subfigure}{\textwidth}
\resizebox{\textwidth}{!}{\definecolor{mycolor1}{rgb}{0.00000,0.44700,0.74100}\begin{tikzpicture}

\begin{axis}[width=\textwidth,
height=.3\textwidth,
scale only axis,
x label style={at={(axis description cs:0.5,0.1)},anchor=north},
    y label style={at={(axis description cs:0.1,.6)},anchor=south},
ylabel={},
xlabel={},
separate axis lines,
every outer x axis line/.append style={white!15!black},
every x tick label/.append style={font=\color{white!15!black}},
xmin=0,
xmax=200,
every outer y axis line/.append style={white!15!black},
every y tick label/.append style={font=\color{white!15!black}},
ymin=-5,
ymax=1
]
\addplot [color=mycolor1,dotted,mark=x,mark options={solid,scale=.4,opacity=.3},forget plot]
  table{ambienttemp-1.tsv};
\end{axis}
\end{tikzpicture}}
\end{subfigure}\&\mathbf M_{\mbox{office}}:\quad\left\{\begin{array}{ll}
x_b(t+1)&=\Xi x_b(t)+ \Gamma q(t)+B_{p} w_p(t)+ B_s\Phi_s(t)+ B_a T_a(t)\\
y(t)&=\begin{bsmallmatrix}0&1&0&0\end{bsmallmatrix}x_b(t),\end{array}\right.\\
& \begin{bsmallmatrix*}\Xi\mid \Gamma\mid B_p  \mid    B_s\mid B_{a} \end{bsmallmatrix*}= \left[\begin{smallmatrix}  
    0.4487 &    0.216   & 0.2164&    0.1186\\
     0.216 &   0.1778  &  0.3719 &   0.2334\\
   0.09639 &   0.1657 &   0.6569  & 0.08082\\
 0.005234   & 0.0103 & 0.008007    &0.9708\end{smallmatrix}\right|\left.  
 \begin{smallmatrix}    2.65\mbox{e-}5\\
    7.45\mbox{e-}5\\
   2.06\mbox{e-}4\\
    0.07\mbox{e-}5\\
 \end{smallmatrix}\right|\left. 
 \begin{smallmatrix}     1.0939\mbox{e-}4\\
   2.16\mbox{e-}4\\
   7.45\mbox{e-}5\\
   3.92\mbox{e-}6   \end{smallmatrix}\right| \left.\begin{smallmatrix}    6.60\mbox{e-}4\\
   1.31\mbox{e-}3\\
   4.49\mbox{e-}4\\
   2.36\mbox{e-}5
   \end{smallmatrix}\right|\left.
\begin{smallmatrix}
   2.96\mbox{e-}4\\
   8.79\mbox{e-}4\\
   1.93\mbox{e-}4\\
   5.67\mbox{e-}3
 \end{smallmatrix}\right].T_a(t+1)=0.7788 T_a+0.6273 w_{w}(t)
&\mathbf M=(A,B,B_w,C):\quad\left\{\begin{array}{ll}
x(t+1)&=Ax(t)+B_w w(t)+ Bu(t)\\
y(t)&=\begin{bsmallmatrix}0&1&0&0&0\end{bsmallmatrix}x(t)\end{array}\right.\\
&\begin{bmatrix}A\mid\! B\mid \!B_w\end{bmatrix}\!=\! \left[\begin{smallmatrix*}[l]  
    0.4487 &   0.216 &  0.2164 &  0.1186 & 2.96\mbox{e-}4 \\ 
   0.216 &  0.1778 &  0.3719 &  0.2334 & 8.789\mbox{e-}4 \\ 
 0.09639 &  0.1657 &  0.6569 & 0.08082 & 1.928\mbox{e-}4 \\ 
0.005234 &  0.0103 &8.007\mbox{e-}3 &  0.9708 &0.005667 \\ 
       0 &       0 &       0 &       0 &  0.7788\end{smallmatrix*}\right|\left.\begin{smallmatrix*}[l]  0.1326 \\ 
  0.3725 \\ 
   1.029 \\ 
4.309\mbox{e-}3 \\ 
       0  \end{smallmatrix*}\right|\left.
 \begin{smallmatrix*}[l]0.006918 & 0.06596 &       0 \\ 
 0.01372 &  0.1308 &       0 \\ 
0.004712 & 0.04492 &       0 \\ 
2.485\mbox{e-}4 &0.002369 &       0 \\ 
       0 &       0 &  0.6273
   \end{smallmatrix*}\right]\!.
\mathbb{P}_{\geq p}\left(  \always^6[ |y|<0.5]\right), 
\
F=\begin{bmatrix}
  0.48456 &     0.39865  &    0.85352   &   0.56387  &  0.0024252
\end{bmatrix}.\mathbf M_{i} :\quad\left\{\begin{array}{ll}
x_{s}(t+1)&=A_{i} x_{s}(t)+ B_{wi}w(t)+B_{i} u_{s}(t)\\
y_{s}(t)&= C_{i} x_{s}(t),\end{array}\right.

 \rel :=\left\{ (x,x_s)\mid 
 (x-Px_s)^T M (x-Px_s)\leq \epsilon^2
 \right\}, 
 u=R u_s+Qx_s+K(x-Px_s),\mathbb T_s\left(\cdot\mid x_s,u_s \right)\bar \rel_\delta \mathbb T\left(\cdot\mid x,\InF(u_s,x_s,x) \right),\mathbb T\left(\cdot\mid x,u\right) = \mathcal{N}(\cdot| Ax+B\InF(u_s,x_s,x),B_wB_w^T), 
 \mathbb T_s\left(\cdot\mid x_s,u_s\right) = \mathcal{N}(\cdot| A_sx_s+B_su_s,B_{wi}B_{wi}^T). \label{eq:diffRelation}
x'- P x_s'&=(A +BK)(x- P x_s) + (B_w -PB_{wi}) w  +( BR   -PB_s) u_s.
  
\psi:=\mathbb{P}_{\geq p}\left( \always^6[ |y|<0.5]\right)

\psi_{\epsilon,\delta}:=\mathbb{P}_{\geq p+\gamma}\left( \always^6[ |y|<0.5-\eps]\right). 

 \mathbf M_{i} :\quad\left\{\begin{array}{ll}
x_{s}(t+1)&=A_{i} x_{s}(t)+ B_{wi}w(t)+B_{i} u_{s}(t)\\
y_{s}(t)&= C_{i} x_{s}(t),\end{array}\right.

&\mathbf M_{1}: A_{1}=\begin{bsmallmatrix*}[r]            
      0 & -0.8572 \\ 
       1 &   1.857
   \end{bsmallmatrix*}, B_{1}=\begin{bsmallmatrix*}[r]     -0.5343 \\ 
  0.5523 \end{bsmallmatrix*}, B_{w1}=\begin{bsmallmatrix*}[r]-5.916\mbox{e-}3 & -0.0564 &  8.62\mbox{e-}3 \\ 
 6.138\mbox{e-}3 & 0.05852 &-6.739\mbox{e-}3 
\end{bsmallmatrix*},C_{1}= \begin{bsmallmatrix*}[l]      0 &       1 \end{bsmallmatrix*}; \!\!\!\!\\
&\mathbf M_{2}:A_{2} =\begin{bsmallmatrix*}[l]             
      0 &-0.05267 \\ 
   0.125 & -0.1081 
   \end{bsmallmatrix*},\,B_{2}=\begin{bsmallmatrix*}[l]
    0.8917 \\ 
  0.3725\end{bsmallmatrix*},\,B_{w2}=\begin{bsmallmatrix*}[l] 0.01925 &  0.1835 &0.002356 \\ 
 0.01372 &  0.1308 &3.229\mbox{e-}5 
\end{bsmallmatrix*},\,C_{2}= \begin{bsmallmatrix*}[l]    0 &       1 \end{bsmallmatrix*}; \\
&\mathbf M_{3}:\,A_{3} =\begin{bsmallmatrix}              
  0.9951  \end{bsmallmatrix},\,B_{3}=\begin{bsmallmatrix}        0.1194 \end{bsmallmatrix},\,B_{w3}=\begin{bsmallmatrix*}[l]
0.001497 & 0.01427 & 0.01467\end{bsmallmatrix*},C_{3}= \begin{bsmallmatrix}       1\end{bsmallmatrix}; \\
&\mathbf M_{4}:\,A_{4}=\begin{bsmallmatrix}              
0.1203 
   \end{bsmallmatrix},\,B_{4}=\begin{bsmallmatrix}
       0.3829 \end{bsmallmatrix},\,B_{w4}=\begin{bsmallmatrix*}[l]
     0.01257 &  0.1198 &0.0002907
\end{bsmallmatrix*},\,C_{4}= \begin{bsmallmatrix}            1\end{bsmallmatrix}. 
Q& :=\begin{bsmallmatrix}-0.08954 &-0.07712\end{bsmallmatrix} ,& \quad K:=\begin{bsmallmatrix}  -0.5717 & -0.4705 & -0.9859 & -0.6213 &-0.002364 \end{bsmallmatrix}, \\
P&:= \begin{bsmallmatrix}   -1.061 & 0.09045 \\ 
       0 &       1 \\ 
  -2.295 & -0.9696 \\ 
   9.064 &   8.775 \\ 
       0 &       0  \end{bsmallmatrix}, &\quad
M:=\begin{bsmallmatrix}   0.4797 &  0.1476 &  0.3298 &  0.1397 &-0.001306 \\ 
  0.1476 &   1.104 &  0.1592 & 0.06704 &-0.00359 \\ 
  0.3298 &  0.1592 &  0.2862 &  0.1207 &-0.001327 \\ 
  0.1397 & 0.06704 &  0.1207 &  0.1744 &0.003174 \\ 
-0.001306 &-0.00359 &-0.001327 &0.003174 &0.003676\end{bsmallmatrix}.

Q& :=\begin{bsmallmatrix}  -1.857 &   1.406 \end{bsmallmatrix},
&\quad  K:=\begin{bsmallmatrix} -0.3553 & -0.2931 &   -0.65 & -0.4739 &-0.002547 \end{bsmallmatrix}, \\
P&:= \begin{bsmallmatrix}  -0.6186 &  0.2348 \\ 
       0 &       1 \\ 
   2.562 &  -2.314 \\ 
-0.009378 &0.001329 \\ 
       0 &       0 \end{bsmallmatrix},& \
M:=\begin{bsmallmatrix}       0.2416 & 0.06342 &  0.3159 &  0.1299 & 0.00106 \\ 
 0.06342 &   1.772 & 0.07267 & 0.02663 &0.0007664 \\ 
  0.3159 & 0.07267 &  0.4191 &  0.1728 &0.001395 \\ 
  0.1299 & 0.02663 &  0.1728 & 0.08168 &0.000351 \\ 
 0.00106 &0.0007664 &0.001395 &0.000351 &0.0001456 \end{bsmallmatrix}.

Q& :=-0.0008755,&\quad 
K:=\begin{bsmallmatrix} -0.5796 &  -0.477 & -0.9978 & -0.6265 &-0.00236\end{bsmallmatrix}, \\
P&:= \begin{bsmallmatrix}     1.004 \\ 
       1 \\ 
   1.006 \\ 
  0.9713 \\ 
       0\end{bsmallmatrix}, &\quad 
M:=\begin{bsmallmatrix}    8.584 &  -4.974 &   4.929 &   2.078 &  0.1158 \\ 
  -4.974 &   3.944 &  -3.106 &   -1.31 &-0.05919 \\ 
   4.929 &  -3.106 &   3.917 &   1.653 & 0.06135 \\ 
   2.078 &   -1.31 &   1.653 &  0.7024 & 0.02595 \\ 
  0.1158 &-0.05919 & 0.06135 & 0.02595 & 0.01179  \end{bsmallmatrix}.

Q& := -0.6961,&\quad K:=\begin{bsmallmatrix}   -0.5307 & -0.4366 & -0.9241 & -0.5946 &-0.002391\end{bsmallmatrix},\\
P&:= \begin{bsmallmatrix}    -1.191 \\ 
       1 \\ 
   1.242 \\ 
-0.01296 \\ 
       0 \end{bsmallmatrix},&\quad 
M:=\begin{bsmallmatrix}  0.03949 &-0.01465 & 0.06076 & 0.02542 &1.999e-05 \\ 
-0.01465 &   1.788 &  0.1162 & 0.05143 &-0.0005164 \\ 
 0.06076 &  0.1162 &   0.128 & 0.05469 &-2.765e-05 \\ 
 0.02542 & 0.05143 & 0.05469 & 0.04108 &-0.0004062 \\ 
1.999e-05 &-0.0005164 &-2.765e-05 &-0.0004062 &0.0003725 \end{bsmallmatrix}.
t_x(\bar x\mid x,u)\sim \mc N(\cdot;A_ix+B_i u,\Sigma )V_0(x)=\mathbb P\left[ \always^6(|y(t)|\leq 0.5-\err)\right].\mc A:=\mathbb R \times [-0.5+\err,0.5-\err]\subset\X=\mathbb R^2,V_0(x)=\mathbb P\left[ \always^6 \mc A\right].\label{FAUSTeq1}\int_{\mathbb R^2}|t_x(\bar x\mid x,u)-t_x(\bar x\mid x',u)|d\bar x\leq H_1|x_1'-x_1|+H_2|x_2'-x_2|.N(H_1\Delta_1+H_2\Delta_2)t_x(\bar x\mid x,u)=\frac{1}{\sqrt{(2\pi)^2\det (\sigma)}}\exp\left[-\frac{1}{2}\left(\bar x-A_ix-B_iu\right)^T\Sigma^{-1}\left(\bar x-A_ix-B_iu\right)\right],m=\begin{bmatrix}m_1\\m_2\end{bmatrix}=A_ix+B_i ut_x(\bar x\mid x,u)=\frac{1}{\sqrt{(2\pi)^2\det (\sigma)}}\exp\left[-\|L\bar x-Lm\|^2\right].
&\int_{\mathbb R^2}\left|
\frac{1}{\sqrt{(2\pi)^2\det (\Sigma)}}
\left(
\exp\left[-\frac{1}{2}\|v-Lm\|^2
\right]-\exp\left[-\frac{1}{2}\|v-Lm'\|^2
\right]
\right)
\right|\frac{dv}{\det(L)}. 
\shortintertext{Note that , hence  and consequently }
&=\int_{\mathbb R^2}\frac{1}{ 2\pi}\left|
\left(
\exp\left[-\frac{1}{2}\|v-Lm\|^2
\right]-\exp\left[-\frac{1}{2}\|v-Lm'\|^2
\right]
\right)
\right| dv.  
\shortintertext{Now we can transform a two-dimensional integral into two one-dimensional integrals:}
&\leq \int_{\mathbb R} \frac{1}{\sqrt{2\pi}}
\left|
\left(
\exp\left[-\frac{1}{2}\|v_1-L_1m_1\|^2
\right]-\exp\left[-\frac{1}{2}\|v_1-L_1m'_1\|^2
\right]
\right)
\right|dv_1\\&+
 \int_{\mathbb R} \frac{1}{\sqrt{2\pi}}
\left|
\left(
\exp\left[-\frac{1}{2}\|v_2-L_2m_2\|^2
\right]-\exp\left[-\frac{1}{2}\|v_2-L_2m'_2\|^2
\right]
\right)
\right|dv_2\\
&\leq \frac{2|L_1 m-L_1m'|}{\sqrt{2\pi}}+\frac{2|L_2 m-L_2m'|}{\sqrt{2\pi}}
\leq \frac{2 }{\sqrt{2\pi}} \left(|L_1 A_i(x-x')|+|L_2 A_i(x-x')|\right).
\begin{bmatrix}\bar{a}_{11}&\bar{a}_{12}\\
\bar{a}_{21}&\bar{a}_{22}\end{bmatrix}=LA_i.\textstyle\forall l\in L:\forall A\in \mathbb S/\equiv_p, \sum_{s'\in A}\mathbb T_l(s|s')=\sum_{s'\in A}\mathbb T_l(t|s').\mathbb T_{l}(B|x_1)=\mathbb T_{l}(B|x_2),\  \forall l\in L.\mathbb T_{l,1}(B\cap \X_1|x_1)=\mathbb T_{l,2}(B\cap \X_2 |x_2),\  \forall l\in L.\mathbb P_1(B\cap \X_1)=\mathbb P_2 (B\cap \X_2),
\Delta\bar \rel \Theta \, \textmd{ if and only if } \, \Delta\equiv_{\rel_{eq}} \Theta. 
\mathbb T(f^{-1}(B)|s)=\mathbb T'(B|f(s)),
\mathbb T (B|t)= \mathbb T_1 (f_1^{-1}(B)|s) \quad \mbox{with } s\in f_1^{-1}(t)
 
\mathbb W\left(dx_1'\times dx_2'\mid (x_1,x_2)\right)=\int_{q'\in Q} \mathbb T_1(dx_1'\mid x_1,q') \mathbb T_2(dx_2'\mid x_2,q')  \mathbb T(dq'\mid f_1(x_1)), 

\mathbb T_i(X_i \cap f_{1}^{-1}(Q) \mid x_i)=\int_Q \mathbb T_i(dx_i'\mid x_i,q') \mathbb T (dq'\mid f_1(x_1)).\mathbb{T}_l(\mathcal{R}(X)|t)\geq \mathbb T_l (X|s)-\epsilon.|\mathbb P_1(B\cap \X_1)-\mathbb P_2 (B\cap \X_2)|\leq \delta,\Delta\equiv^\delta_{\rel_{eq}} \Theta\textmd{ if and only if } \Delta\bar \rel_{\delta} \Theta, \mathbb
W(\tilde S\times (\X_2\setminus \tilde T))\leq \delta 
\mathbb W((\X_1\setminus \tilde S)\times \tilde T)\leq \delta \Delta(\tilde S)\leq \Delta(\tilde S)+\mathbb W((\X_1\setminus \tilde S)\times\tilde T)=\Theta(\tilde T)+\mathbb W(\tilde S\times (\X_2\setminus \tilde T))\leq \Theta(\tilde T)+\delta 
\gamma(\nu, \tilde\nu):=\Psi_Z(\nu\wedge \tilde\nu)+\mathbf{1}_{[0,1)} (\|\nu\wedge \tilde\nu\|).\frac{(\nu- \tilde\nu)^+\otimes(\nu- \tilde\nu)^-}{1-\|\nu-\tilde\nu\|}
\mathbb W:= \int_{Q\times Q} \Delta(dx_1\mid q_1) \Theta(dx_2\mid q_2)  \mathbb W_{\mathcal Q} (dq_1\times d q_2).
\mathbb T^0_{\C_2\times \M_2}(d x_{\C_2}\times dx_2) = 
\mathbb T^0_{\C_1} (dx_{\C_1}{\mid}x_{\C_10},x_1)\mathbb W_\pi(dx_1 {\mid} x_2 )\delta_{x_2(0)}(dx_2)\pi(dx_2(0)). 
\mathbb T^0_{\C_2\times \M_2}(d x_{\C_1}\times dx_1\times dx_2)&= 
\mathbb T^0_{\C_1} (dx_{\C_1}{\mid}x_{\C_10},x_1)\mathbb W_\pi(dx_1 {\mid} x_2 )\pi(dx_2)\\
&=\mathbb T^0_{\C_1} (dx_{\C_1}{\mid}x_{\C_10},x_1)\mathbb W_\pi(dx_2 {\mid} x_1 )\pi(dx_1). 

\mathbb T^0_{\C_2\times \M_2}(d x_{\C_1}\times dx_1)= 
\mathbb T^0_{\C_1} (dx_{\C_1}{\mid}x_{\C_10},x_1)\pi(dx_1)= \mathbb T^0_{\C_1\times \M_1}(d x_{\C_1}\times dx_1).
&\mathbb T^t_{\C_2\times \M_2}(d x_{\C_1}'\times dx_1'\times dx'_2)= \mathbb T^t_{\C_2} (dx_{\C_1}'{\mid}x_{\C_1},x_1')\\ &\hspace{3cm} \Wt(d x_1' {\mid} x_2',h_{\C_1}^t(x_{\C_1}),x_2, x_1)\mathbb T_2(dx_2'{\mid}x_2,h^t_{\C_2}(x_{\C_2}) )\\
&\hspace{3cm}= \mathbb T^t_{\C_1} (dx_{\C_1}'{\mid}x_{\C_1},x_1')\Wt(d x_1'\times d x_2' {\mid} h_{\C_1}^t(x_{\C_1}),x_2, x_1) \po_{\C_1\times\M_1}\left(\{y_1 (t)\}_{0:N}\in A\right)= \po_{\C_2\times\M_2}\left(\{y_2 (t)\}_{0:N}\in A\right).
\mathbb T^0_{\C_2} (dx_{\C_2}^{\operatorname{refine}}{\mid}x_{\C_20},x_2(0) ):=&\mathbb T_{\C_1}^0 (dx_{\C_1}{\mid}x_{\C_10},x_1)\mathbf 1_{\rel}\left(x_1,x_2\right)\\&\qquad\times \mathbb W_\pi(dx_1 {\mid} x_2 )\delta_{x_2(0)}(dx_2)\\
\mathbb T^0_{\C_2} (dx_{\C_2}^{\operatorname{recover}}{\mid}x_{\C_20},x_2(0) ):=&\mathbb T^0_{init,rec} (dx_{\C_{rec}}{\mid}x_2)\mathbf 1_{(\X_1\times \X_2)\setminus\rel}\left(x_1,x_2\right)\\&\qquad\times \mathbb W_\pi(dx_1 {\mid} x_2 )\delta_{x_2(0)}(dx_2)

\mathbb T^t_{\C_2} (dx_{\C_2}^{\operatorname{refine}'}{\mid}x_{\C_2}^{\operatorname{refine}}(t),x_2(t) )&:=\mathbb T^t_{\C_1} (dx_{\C_1}'{\mid}x_{\C_1},x_1')\mathbf 1_{\rel}(x_1',x_2')\\ &\qquad\times
\Wt(d x_1' {\mid} x_2',h_{\C_1}^t(x_{\C_1}),x_2, x_1)\delta_{x_2(t)}(dx'_2);\\
\mathbb T^t_{\C_2} (dx_{\C_2}^{\operatorname{recover}'}{\mid}x_{\C_2}^{\operatorname{refine}}(t),x_2(t) )&:=\mathbb T^t_{init,rec}(dx_{\C_{rec}}'{\mid}x_2')\mathbf 1_{(\X_1\times \X_2)\setminus\rel}(x_1',x_2')\\ &\qquad\times
\Wt(d x_1' {\mid} x_2',h_{\C_1}^t(x_{\C_1}),x_2, x_1)\delta_{x_2(t)}(dx'_2);

\mathbb T^t_{\C_2} (dx_{\C_2}^{\operatorname{recover}'}{\mid}x_{\C_2}^{\operatorname{recover}}(t),x_2(t) )&:=\mathbb T^t_{\C_{rec}} (dx_{\C_{rec}}'{\mid}x_{\C_{rec}}(t),x_2(t) );
h^t_{\C_2}(x_{\C_2}):= \left\{\begin{array}{ll}\InF(h^t_{\C_1}(x_{\C_1}),x_1, x_2) &\quad \mbox{ for \,,}\\
h^t_{\C_{rec}}(x_{\C_{rec}}) &\quad \mbox{ for \,.}\end{array}\right.\, \mathbb T^t_{\C_{rec}^\ast} (dx_{\C_{rec}^\ast}'{\mid}x_{\C_{rec}^\ast}(t),x_2(t) )&=\mathbb T^t_{\C_{rec}} (dx_{\C_{rec}}'{\mid}x_{\C_{rec}}(t),x_2(t) )\\&\hspace{2cm }\mathbb T^t_{\C_{1}\times \M_1} (dx_{\C_{1}\times \M_1}'{\mid}x_{\C_{1}\times \M_1}(t)) 
\mathbb P_{\C_1\times \M_1}(\{h_1(x_1(t))\}_{0:N}\in L)=\mathbb P_{\C_2^\ast\times \M_2}(\{h_1(x_1(t))\}_{0:N}\in L).
\mathbb P_{\C_2^\ast\times \M_2}\left((x_1(t),x_2(t))\in \rel \mbox{ for } t\in[0,N]\right)\geq (1-\delta)^{N+1}.

 &\mathbb P_{\C_2^\ast\times \M_2}(\{h_1(x_1(t))\}_{0:N}\in A_{-\eps}\wedge (x_1(t),x(t))\in \rel \mbox{ for } t\in [0,N] )\\&\qquad\leq  \mathbb P_{\C_2^\ast\times \M_2}(\{h_2(x_2(t))\}_{0:N}\in A)= \mathbb P_{\C_2\times \M_2}(\{h_2(x_2(t))\}_{0:N}\in A). 

&\mathbb P_{\C_2^\ast\times \M_2}(\{h_1(x_1(t))\}_{0:N}\in A_{-\eps})-(1-\delta)^{N+1}\\
&\quad\leq
\mathbb P_{\C_2^\ast\times \M_2}(\{h_1(x_1(t))\}_{0:N}\in A_{-\eps}\wedge (x_1(t),x(t))\in \rel \mbox{ for } t\in [0,N] )
&1-\mathbb P_{\C_2^\ast\times \M_2}(\{h_1(x_1(t))\}_{0:N}\in A_{-\eps}\wedge (x_1(t),x(t))\in \rel \mbox{ for } t\in [0,N] )\\
 &\leq (1-\mathbb P_{\C_2^\ast\times \M_2}(\{h_1(x_1(t))\}_{0:N}\in A_{-\eps} ))\\ &\hspace{2cm}
 + (1-\mathbb P_{\C_2^\ast\times \M_2} \left(((x_1(t),x(t))\in \rel \mbox{ for } t\in [0,N] )\right)\\
  &\leq (1-\mathbb P_{\C_2^\ast\times \M_2}(\{h_1(x_1(t))\}_{0:N}\in A_{-\eps} )) + (1-(1-\delta)^{N+1}). 

\mathbb P_{\C_1\times \M_1}(\{h_1(x_1(t))\}_{0:N}\in A_{-\eps} )-(1-(1-\delta)^{N+1}) \leq \mathbb P_{\C_2\times \M_2}(\{h_2(x_2(t))\}_{0:N}\in A). 

\mathbb P_{\C_2\times \M_2}(\{h_2(x_2(t))\}_{0:N}\in A)
\leq 
\mathbb P_{\C_1\times \M_1}(\{h_1(x_1(t))\}_{0:N}\in A_{\eps} )+(1-(1-\delta)^{N+1}). 
\cramped{\forall u_1\in\A_1:
\mathbb T_1(\cdot| x_1, u_1)\ \bar \rel_{12,\delta_a} \  \mathbb T_2(\cdot| x_2, {\InF}_{12}(u_1,x_1,x_2))}\mathbb T_2(\cdot| x_2, u_2)\ \bar \rel_{23,\delta_b} \  \mathbb T_3(\cdot| x_3, {\InF}_{23}(u_2,x_2,x_3)){\Wt}_{13}(dx_1'\times dx_3'|u_1,x_1,x_2,x_3)&= \int_{\X_2}{\Wt}_{23}(dx_3'\mid x_2',\InF(u_1,x_1,x_2),x_2,x_3)\\&\hspace{3cm}\times  {\Wt}_{12}(dx_1'\times dx_2'|u_1,x_1,x_2). &{\Wt}_{13}(\X_1\times\X_3\setminus \rel_{13})=\int_{\X_1} \int_{\X_2}\int_{\X_3\setminus \rel_{13}(x_1)}{\Wt}_{23}(dx_3\mid x_2) {\Wt}_{12}(dx_1\times dx_2)\\
&=\int_{\rel_{12}}\int_{\X_3\setminus \rel_{13}(x_1)}{\Wt}_{23}(dx_3\mid x_2) {\Wt}_{12}(dx_1\times dx_2)\\
&+\int_{\X_1} \int_{\X_2\setminus \rel_{12}(x_1)}\int_{\X_3\setminus \rel_{13}(x_1)}{\Wt}_{23}(dx_3\mid x_2) {\Wt}_{12}(dx_1\times dx_2)\\
\shortintertext{for all }
&\leq\int_{\X_2}\int_{\X_3\setminus \rel_{23}(x_2)}   \int_{\rel_{12}^{-1}(x_2)}{\Wt}_{12}(dx_1\mid x_2) {\Wt}_{23}(dx_2\times d x_3) \\
&+\int_{\X_1} \int_{\X_2\setminus \rel_{12}(x_1)}\int_{\X_3\setminus \rel_{13}(x_1)}{\Wt}_{23}(dx_3\mid x_2) {\Wt}_{12}(dx_1\times dx_2)\\
&\leq\int_{\X_2}\int_{\X_3\setminus \rel_{23}(x_2)}   \int_{\X_1}{\Wt}_{12}(dx_1\mid x_2) {\Wt}_{23}(dx_2\times d x_3) \\
&+\int_{\X_1} \int_{\X_2\setminus \rel_{12}(x_1)}\int_{\X_3}{\Wt}_{23}(dx_3\mid x_2) {\Wt}_{12}(dx_1\times dx_2)\\
&=\int_{\X_2}\int_{\X_3\setminus \rel_{23}(x_2)}     {\Wt}_{23}(dx_2\times d x_3)  +\int_{\X_1} \int_{\X_2\setminus \rel_{12}(x_1)} {\Wt}_{12}(dx_1\times dx_2)\\
&\leq \delta_a+\delta_b. 

\mathbb {\Wt}_{13}(X_1\times \X_3)&=\int_{X_1} \int_{\X_3}\int_{\X_2}{\Wt}_{23}(dx_3\mid x_2) {\Wt}_{12}(dx_1\times dx_2)\\
&=\int_{X_1}\int_{\X_2} \int_{\X_3}{\Wt}_{23}(dx_3\mid x_2) {\Wt}_{12}(dx_1\times dx_2)\\&= {\Wt}_{12}(X_1\times \X_2)= \mathbb T_{1}(\cdot|x_1,\mu_{u,1}). 

The condition  can be proven via similar arguments. In conclusion .
To complete the proof we can show, using the same arguments as before,  that if  and if  then .
\qed
\end{proof}

 
\end{document}
