








\documentclass[10pt, conference, compsocconf]{IEEEtran}























\ifCLASSINFOpdf
\else
\fi




























































\hyphenation{op-tical net-works semi-conduc-tor}


\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{url}
\usepackage{cite}
\usepackage[pdftex]{graphicx}
\begin{document}
\title{PCS: Predictive Component-level Scheduling for Reducing Tail Latency in Cloud Online Services}



\author{
\IEEEauthorblockN{Rui Han, Junwei Wang, Siguang Huang, Chenrong Shao, Shulin Zhan, Jianfeng Zhan}
\IEEEauthorblockA{Institute Of Computing Technology,\\
Chinese Academy of Sciences\\
Beijing, China\\
{hanrui,wangjunwei,huangsiguang,shaochenrong,zhanshulin,zhanjianfeng}@ict.ac.cn}
\and
\IEEEauthorblockN{Jose Luis Vazquez-Poletti}
\IEEEauthorblockA{Facultad de Informatica,\\
Universidad Complutense de Madrid\\
Madrid, Spain\\
jlvazquez@fdi.ucm.es}
}

\maketitle



\begin{abstract}
Modern latency-critical online services often rely on composing results from a large number of server components. Hence the tail latency (e.g. the 99th percentile of response time), rather than the average, of these components determines the overall service performance. When hosted on a cloud environment, the components of a service typically co-locate with short batch jobs to increase machine utilizations, and share and contend resources such as caches and I/O bandwidths with them. The highly dynamic nature of batch jobs in terms of their workload types and input sizes causes continuously changing performance interference to individual components, hence leading to their latency variability and high tail latency.
However, existing techniques either ignore such fine-grained component latency variability when managing service performance, or rely on executing redundant requests to reduce the tail latency, which adversely deteriorate the service performance when load gets heavier.
In this paper, we propose PCS, a predictive and component-level scheduling framework to reduce tail latency for large-scale, parallel online services. It uses an analytical performance model to simultaneously predict the component latency and the overall service performance on different nodes. Based on the predicted performance, the scheduler identifies straggling components and conducts near-optimal component-node allocations to adapt to the changing performance interferences from batch jobs. We demonstrate that, using realistic workloads, the proposed scheduler reduces the component tail latency by an average of 67.05\% and the average overall service latency by 64.16\% compared with the state-of-the-art techniques on reducing tail latency.
\end{abstract}

\begin{IEEEkeywords}
cloud online services; component latency variability; tail latency; predictive scheduler;

\end{IEEEkeywords}


\IEEEpeerreviewmaketitle



\section{Introduction}

Providing fluid responsiveness to user requests is essential for online services: their potential profits are proportional to service latency (i.e. request response time including both the request queueing delay and the time of being processed) \cite{jalaparti2013speeding,tailatScale}. In large online services such as search engines, e-commerce sites, and social networks, the processing of incoming requests consists of several sequential stages, where each stage composes responses parallelized across hundreds or thousands of server \emph{components}. Hence the tail (e.g. the 99th percentile) of these components' latencies, rather than the average, determines the overall service performance \cite{tailatScale,kapoor2012chronos}.
For example, Figure \ref{Fig: SearchEngine} shows an example Nutch search engine \cite{nutchsearch} with three stages. Suppose that at stage 2, the request processing is parallelized into 100 components, in which 99 components can respond in 10ms but only one component gets a slow response of 1 second, the overall service performance is deteriorated by this straggling component and hence providing slow responsiveness of 1 second.




\begin{figure}
\centering
  \includegraphics[scale=0.44]{SearchEngine.pdf}\\
  \caption{An example of Nutch search engine}
  \label{Fig: SearchEngine}
\end{figure}


In modern cloud data centers and warehouse-scale computers, it is critical to improve machine utilizations by co-locating long-running online services and offline batch jobs (e.g. Hadoop \cite{HadoopWebsite} and Spark \cite{SparkWebsite} analytics jobs) on the same node (physical machine), while still keeping the overall latency of online services at a satisfactory level \cite{reiss2012heterogeneity,yang2013bubble}.
Although the components of a service are typically hosted on dedicated environments such as Xen virtual machines (VMs) or LinuX Containers (LXCs), these components still share and contend resources such as processing units, caches and I/O bandwidths with their co-running batch jobs on the same node, hence inevitably suffer from performance interference. Workload traces from Google \cite{reiss2012heterogeneity} and Facebook \cite{chen2012interactive} show that small batch jobs form a majority (over 90\%) of all jobs in their data center workloads. For example, approximately 50\% of Google jobs complete in 10 minutes and 94\% of them complete within 3 hours.
These short-term batch jobs have various workload types (e.g. CPU and I/O intensive workloads) and input data sizes (e.g. ranging from KB to GB), thus causing continuously changing performance interferences to their co-located components. This results in the \textbf{component latency variability}, which can be explained from two aspects: (a) each component's latency (performance) varies over time, and (b) components hosted on different nodes have different changes in their latencies, hence causing high tail latency in individual components of the service.














Many existing techniques have been developed to guarantee the performance of latency-critical services by mitigating the performance interference due to resource sharing and contention \cite{kasture2014ubik,xu2013bobtail,ahn2012dynamic,xu2010mitigating}. However, these techniques only manage service performance at the coarse granularity of the entire application, ignoring fine-grained component latency variability that may come to dominate service performance at large scale. Moreover, state-of-the-art techniques reduce tail latency via request redundancy. They either create replicas for all the requests \cite{vulimiri2012more,ananthanarayanan2013effective,stewart2013zoolander} or reissue slow requests' replicas to a different component \cite{jalaparti2013speeding, tailatScale}, and then use the quickest replica. Although these techniques work well under light load, they adversely deteriorate the service performance when load gets heavier \cite{shah2013redundant}.








In this paper, we propose a new component-level service scheduler that dynamically schedules the components of a service to appropriate nodes with the assistance of cost-effective online monitors. Compared to existing latency reduction techniques, the proposed scheduler applies an analytic performance model to predict the latencies of all components and their impact on the overall service performance, and then formulates the scheduling decisions based on the predicted performance. The performance model also dynamically updates the prediction results at each scheduling interval by collecting the latest resource contention information during the service execution, thus allowing the scheduler to adapt to changes in performance interference. The concrete contributions of this work are as follows:
\begin{itemize}
\item  We build a flexible analytic performance model to accurately predict the performance of an online service. The basic model comprehensively covers some of the most representative shared resources that are likely to incur contentions and predicts each component's service time on different nodes by taking the resource contention and performance interference into consideration. The extended model further considers the request queueing delay and estimates the latency of the whole service based on its implementation topology. We show that the proposed model can predict the latency with an average error of 2.68\%.
\item  Based on the performance model, we present a framework for component-level scheduling. At each scheduling interval, our approach efficiently identifies the straggling components of a service such that the migration of these components brings the maximum reduction in the overall service latency. The effectiveness of the proposed approach is evaluated using comparative experiments on a variety of realistic workloads publicly available from the BigDataBench suite \cite{opensourceBigDataBench}. The experiment results in a 100-machine cluster demonstrate that compared with the state-of-the-art techniques on mitigating tail latency, our approach reduces components' 99th percentile latency by an average of 67.05\% and the average overall service latency by 64.16\%.
\end{itemize}
The remainder of this paper is organized as follows: Section \ref{Section: Background} introduces the background information. Section \ref{Section: Overview of the framework} gives an overview of the proposed scheduling framework. Section \ref{Section: Performance predictor} presents the performance model and Section \ref{Section: Algorithm} explains the scheduling algorithm. Section \ref{Section: Evaluation} evaluates the proposed approach. Section \ref{Section:Related Work} presents the related work, and finally, Sections \ref{Section: Conclusion} summarizes the work.



\section{Background} \label{Section: Background}
\subsection{Sources of component latency variability} \label{Section:Sources of tail latency}

\emph{Resource sharing and contention}. When deploying an online service on a cloud platform, the performance interference due to the co-located batch jobs' resource contention is often regarded as a major cause of a component's service time variability \cite{tailatScale,leverich2014reconciling}. Some system activities including hardware activities (such as garbage collections of storage devices and energy management behaviors) and software activities (such as kernel daemons and system maintenance) also influence the component's service time.

\emph{Queueing delay}. The component's service time variability is significantly amplified in the request queueing delay when considering different request arrival rates. Hence the variability of service time and queueing delay work together to cause large latency variability in individual components.

























\subsection{Dynamic performance interference of batch jobs} \label{Section:Managing data center applications}




The dynamic performance interference of batch jobs are caused by their short running periods and continually changing workload characteristics, which can be explained in two aspects.



\emph{Workload type}. It has twofold meanings: (i) \emph{Computation semantics}. Batch jobs with different computation semantics (i.e. source codes) may have different resource demands. For example, Sort is an I/O-intensive workload, Bayes classification is a CPU-intensive workload with dominated floating point operations, and Page Index has similar demands for CPU and I/O resources. (ii) \emph{Software stacks}. Model software stacks such as Hadoop and Spark usually provide rich libraries to facilitate development of new applications, and allow a programmer focus on writing a few lines of codes to implement an application. Hence a batch job of the same computation semantic may have considerably different resource demands when implemented with different software stacks \cite{jia2014characterizing}. For example, Hadoop Bayes is a CPU-intensive workload but Spark Bayes is an I/O-intensive workload.

\emph{Input data size}. The resource demand of a job varies when it processes different input data sizes. For example, when running on a 12-core Xeon E5635 processor, the CPU utilizations of the WordCount workload are 31\%, 61\%, and 79\% when its input data sizes are 500MB, 2GB, and 8GB, respectively.




















\section{Overview of the framework} \label{Section: Overview of the framework}

As shown in Figure \ref{Fig: Overview}, the proposed framework for predictive component-level scheduling consists of three modules: the on-line monitors, the performance predictor and the scheduling heuristic.

The \emph{on-line monitor} continuously detects two types of information in a running service, whose components are distributed on  nodes of a data center. The first type of information represents the service's workload status, i.e. its request arrival rate. The second type of information reflects the resource contention information of each component due to its co-located programs on the same node. Specifically, the monitor obtains the request arrival rate by profiling service's running logs, collects system-level contention information (e.g. core usage and I/O bandwidths) by accessing the proc file system, and profiles micro-architectural contention information (e.g. shared cache misses) using hardware performance counters for Linux 2.6+ based systems. In our monitor, Perf \cite{PerfWebsite} is used to profile physical machines and Oprofile \cite{OprofileWebsite} is used to profile VMs.

At the end of each scheduling interval, the \emph{performance predictor} collects the monitored information and predicts the component's latency on all  nodes. This predictor also estimates the impact of individual component latencies on the performance of the whole service based on its implementation topology, and organizes the predicted values as a performance matrix. Using this matrix, the \emph{data center scheduler} applies the scheduling algorithm to identify the straggling components and enforces the appropriate node assignment of the components for the next interval. Consequently, the framework is able to dynamically and efficiently adapt to component latency variability.

Note that the proposed scheduling algorithm is not intended to replace, but rather complement the existing scaling or resource provisioning techniques (e.g. reactive scaling up \cite{han2012lightweight} or prediction-based resource provisioning \cite{calheiros2011virtual,han2014enabling} approaches) for multi-stage online services. Specifically, the component-level scheduling is enforced only after the machines have been allocated to the service. At each scheduling interval, the component-node allocation can be conducted by calling the deployment APIs offered by existing distributed realtime computation systems such as Storm \cite{StormWebsite} and Drill \cite{DrillWebsite} to migrate the components to the available machines (e.g. VMs or LXCs) on the scheduled nodes. Note also that although this component-node allocation can be enforced by directly migrating the machines to the nodes, we prefer the former solution as it produces lower overheads on scheduling.



\begin{figure}
\centering
  \includegraphics[scale=0.45]{Overview.pdf}\\
  \caption{The overview of the framework}
  \label{Fig: Overview}
\end{figure}

\section{Performance predictor} \label{Section: Performance predictor}



Predicting a component's latency when running on different nodes is the key step to detect straggling components in a service. This requires the performance predictor to consider all causes of latency variability discussed in Section \ref{Section:Sources of tail latency}. In the presence of fine-grained heterogeneity of resource contentions on each component, the basic performance predictor is responsible for collecting the information of resource sharing and contention, and predicting the impact on individual component's performance (Section \ref{Section: Basic performance model}). The extended performance model further estimates the component's latency by taking the current request arrival rate into account, and calculates the overall service latency based on the service implementation topology (Section \ref{Section: Extended Performance Model}). With these two models, the performance predictor finally exposes the component latency variability to the scheduler as a performance matrix of reduced overall service latencies (Section \ref{Section: Matrix Construction}). Table \ref{table: Table of notations} lists all notations.

\begin{table}[h!]
  \caption{Table of notations}
  \centering
  \begin{tabular}{|l|l|}
    \hline
    \textbf{Symbol} & \textbf{Meaning} \\
    \hline
     & A node \\
    \hline
     & A component belonging to a service\\
    \hline
     & 's service time \\
    \hline
     & One type of shared resources \\
    \hline
     & 's resource contention information \\
    \hline
     & The contention vector consists of contention \\
    &  information of all shared resources\\
    \hline
     & A basic regression model \\
    \hline
     & A combined regression model representing\\
    &the predicted service time\\
    \hline
     & 's latency \\
\hline
     & A stage's latency \\
    \hline
     & The overall service latency \\
    \hline
     & The matrix of the reduced overall service latency \\
    \hline
     & An entry of \textbf{L}, which represents the reduced \\
    & overall latency when component 's is migrated\\
    & from its current node to node  \\
    \hline
  \end{tabular}
  \label{table: Table of notations}
\end{table}



\subsection{Basic performance model} \label{Section: Basic performance model}

Given a component  hosted on a node , the basic performance model is developed to capture the impact of resource sharing and contention on 's performance and estimate its service time .

Table \ref{table: Usage information of shared resources} lists the contention information of shared resources. The model comprehensively considers both on-chip resources (e.g. shared processing units and caches) and off-chip resources (disk and network bandwidths) contended by different programs on node . In Table \ref{table: Usage information of shared resources}, core usage represents the ratio of time running instructions on the cores (including private cache hits); MPKI represents the number of instruction Misses Per Kilo Instructions of shared caches including last level cache (LLC), instruction Translation Lookaside Buffer (TLB/ITLB), and data TLB (DTLB). MPKI thus indicates the stalled cycles due to cache contention. Note that the \emph{contention} of these resources comes from 's co-running programs within the same service or across other applications, and node 's hardware/software activities.















\begin{table}[h!]
  \caption{Contention information of shared resources}
  \centering
  \begin{tabular}{|l|l|}
    \hline
    \textbf{Shared resources} & \textbf{Contention information} \\
    \hline
    Floating point and vector  & =core usage\\
    processing units, pipelines,& \\
    and data prefetchers & \\
    \hline
    LLC, ITLB, DTLB & =MPKI\\
    \hline
    Disk bandwidth & =the amount of \\
    & read/write data per second \\
    \hline
    Network bandwidth & =the amount \\
    & of send/receive data per second  \\
    \hline
  \end{tabular}
  \label{table: Usage information of shared resources}
\end{table}


Based on the contention information, the basic performance model predicts 's service time  using two steps. The first step employs a regression model to describe the relationship between one contention information and 's service time.
The training of the regression model takes a set of  samples \{({,}),...,({,})\} as input and outputs a model , where  and ,,, (= 1,...,). Hence 's service time  is predicted as  when the contention information is . The training samples are obtained from profiling runs or historical running logs.













During the training of regression model, the first step also calculates the relevance (i.e. weight ) between the contention information of shared resource  and 's service time.
Suppose four regression models (, ,  and ) and their weights (,, and ) are obtained, the second step predicts 's service time  by producing the final regression model  that takes a weighted combination of all the four models:


where the resource contention vector .

















\subsection{Extended Performance Model} \label{Section: Extended Performance Model}





The extended performance model further employs the queueing system to estimate individual component latency under different request arrival rates.
Typically, a queueing system can be described as an , where  represents the distribution of interarrival time of requests;  denotes the distribution of service time; and  is the number of servers.
The choice of M/G/1 queueing system is based on the assumption that the distribution of interarrival time of incoming requests are determined by a Poisson process (M for Markov); a component is modeled as a server in the queueing system and the distribution of its service time can follow arbitrary distributions (G for General). Let  be the monitored request arrival rate and  be the service rate. Let  be the mean service time (=), and  be the variance of service time. Component 's expected latency  is calculated as:



where  is the squared coefficient of variation of service time  and  is the server utilization. In many service components, when the service time follows the exponential distribution, that is, the squared coefficient of variation , the M/G/1 queueing system equals the M/M/1 queueing system and the expected latency .
At each scheduling interval, a set of resource contention vectors can be collected for each component. By substituting them into Equation \ref{Equation: regressionModel}, the component's corresponding service time  can be estimated, so its mean and variance can be calculated.



Furthermore, the model computes the overall latency of a service based on its implementation topology. In the online services studied in this work, the processing of a request includes several sequential stages, and each stage parallelizes requests across one or multiple components to aggregate their responses. Hence the calculation of an overall service latency consists of two steps. The first step computes the latency of each stage. Suppose a stage consists of  parallel components, its latency is the maximum value of these component latencies:

where  is the latency of component  (= 1,...,).

Suppose the service consists of  sequential stages, the second steps calculates its overall latency:

where  is the latency of the th stage (= 1,...,).
 
\subsection{Performance Matrix} \label{Section: Matrix Construction}

Suppose  components of a service are deployed in  nodes, the  performance matrix  is constructed using components as rows and nodes as columns. An entry  denotes the changes in the \emph{overall service latency}  when a component  is migrated from its current node  to node  ( and ). This migration may influence all  components' contention vectors. For any component  of the service, let its original resource contention vector be  and the updated contention vector after the migration be . Let the resource contention from  itself be  and the resource consumption from all programs on node  be .  Four situations needs to be considered when calculating the updated contention vector , as listed in Table \ref{table: Updated usage vectors in migration}.

\begin{table}[h!]
  \caption{Calculation of the updated contention vector }
  \centering
  \begin{tabular}{|l|l|}
    \hline
    \textbf{Type of component } & \textbf{Updated contention vector } \\
    \hline
    & \\
    \hline
    Any component on  & -\\
    \hline
    Any component on  & +\\
    \hline
    Any other component & \\
    \hline
  \end{tabular}
  \label{table: Updated usage vectors in migration}
\end{table}

By substituting  into Equations \ref{Equation: regressionModel} and \ref{Equation: a server's response time}, 's updated latency  can be calculated. We have: (i) 's latency  decreases if  has lighter resource contention than ; otherwise  increases. (ii) All the components on node  have decreased latencies because the removal of  alleviates the resource contention on . (iii) All the components on node  have increased latencies because the addition of  aggravates the resource contention on . (iv) the latencies of other components keep unchanged. Furthermore, by substituting the updated latencies of all  components into Equations \ref{Equation: a stage's response time} and \ref{Equation: the overall response time}, the updated overall service latency  can be calculated. Let the overall latency be  before the migration. The entry  can be calculated as:




Figure \ref{Fig: EntryInMatrix} shows an example service with three stages, where stage 2 is parallelized into two components  and . After  is migrated from node  to , 's latency  increases and 's latency  decreases. By considering all the updated latencies, the overall service latency before and after the migration can be calculated: =57ms and =39ms. Hence the reduced latency =18ms.

\begin{figure}
\centering
  \includegraphics[scale=0.40]{EntryInMatrix.pdf}\\
  \caption{An example of entry \textbf{L}[2][4] in the performance matrix}
  \label{Fig: EntryInMatrix}
\end{figure}



\section{The Component-level Scheduling Algorithm} \label{Section: Algorithm}

Based on the performance matrix of a service, the scheduler can conduct component-node allocations to minimize the overall service latency. Let the  components \{,...,\} be deployed in  nodes \{,...,\}, a na\"{i}ve approach needs a time complexity  to identify the optimal component-node allocation and such exhaustive search is not scalable in practical scenarios. However, performance interference is changing overtime, hence optimizing component-node allocation for a particular dynamic scheduling is not worthwhile.
The proposed approach, therefore, applies a greedy algorithm with polynomial computation complexity and the algorithm has several iterations. At each iteration, the algorithm aims to minimize the overall service latency by evaluating all possible component-node migrations and selecting the migration that would reduce the latency the most.

The pseudocode of the algorithm is presented in Algorithm \ref{SchedulingAlgorithm}. At each scheduling interval, the algorithm first constructs the performance matrix  using the performance mode and the monitoring information (line 2). The initial candidate array  takes all  components as its elements (line 3). The scheduling process then iteratively executes under two conditions: (a)  is not empty; (b) at least one component in  can be migrated (line 5). The second condition indicates that a migration is enforced only when the predicted maximum reduced overall latency  is larger than a specified threshold . This threshold prevents inefficient migrations such that the reduced latency cannot compensate the migration cost. In each loop (line 5 to 15), the algorithm first traverses the matrix  to identify a set  of entries with the largest value (line 6). Any of these entries denotes the migration of a component that brings the maximal reduction in the overall service latency. If set  contains multiple entries,
the algorithm further searches  to find the entry  representing the migration that brings the largest reduction to the latency of the migrated component itself (line 7). Component  is regarded as the straggling component and it is allocated to node  (line 11). The migrated component  is then removed from the candidate array  and the matrix  is updated after this migration.




\begin{algorithm}[htbp]
\caption{Predictive Component-level Scheduling}
\label{SchedulingAlgorithm}
\algsetup{
linenosize=\small,
linenodelimiter=.
}
\begin{algorithmic}[1]
\REQUIRE : the number of components;\\
: the number of nodes;\\
: the number of candidate components to be migrated;\\
: the index array of candidate components; \\
: the migration threshold; \\
: the component-node allocation array, where  represents the index of the th component's hosting node; \\
: the index of the straggling component ;\\
: the index of 's original node;\\
: the index of 's destination node;\\
: A component 's reduced latency when migrating from note  to .\\

\STATE Obtain the monitoring information once every scheduling interval;
\STATE Construct the performance matrix ;
\STATE =\{,...,\};
\STATE =+1;
\WHILE{( is not empty and )}
\STATE Find a set of entries  in the performance matrix L with the largest value;
    \STATE Find the entry  in  with the largest value ;
\STATE ;
    \IF{()}
        \STATE  = ;
        \STATE ; \STATE Remove  from ;
        \STATE UpdateMatrix(, , , , );
    \ENDIF
\ENDWHILE
\STATE Enforce component-node allocation based on .
\end{algorithmic}
\end{algorithm}

The detailed matrix updating function is given in Algorithm \ref{UpdatingFunction}. The migration of component  from node  to  alleviates the resource contention on  but aggravates the resource contention on , hence has a twofold impact on the predicted reduction of the overall latency for the following migrations. First of all, components to \emph{migrate to}  () have \emph{increased} (\emph{decreased}) reductions in the overall latency. Hence the entries in the nOriginth and nDestinationth columns should be updated according to Equations \ref{Equation: regressionModel} to \ref{Equation: changes in the overall response time} (line 1 to 5). Secondly, components to \emph{migrate out} of  () have \emph{decreased} (\emph{increased}) reductions in the overall latency. Each of such components is hosted on either  or  (line 3) and the component corresponds to one row in the matrix, hence the entries in this row should be updated (line 7 to 10). Note that component  is removed from the candidate array , so all the entries related to  are not updated.

\begin{algorithm}[htbp]
\caption{UpdateMatrix(, , , , )}
\label{UpdatingFunction}
\algsetup{
linenosize=\small,
linenodelimiter=.
}
\begin{algorithmic}[1]
\REQUIRE : the number of rows to be updated;\\
: the index array of rows; \\
\FOR{(=0; ; ++)}
    \STATE Update  and ;
    \IF{((== or ==) and )}
        \STATE Add the th row  to ;
    \ENDIF
\ENDFOR
\FOR {each row  in }
    \FOR{(=0; ; ++)}
        \STATE Update ;
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

Figure \ref{Fig: ServerMigration} illustrates an example loop, in which migrating component  to either node  or  can result in the maximal reduction in the overall service latency. At the same time, the reduction in 's latency is 20ms when it is migrated to  and 30ms when it is migrated to , which indicates  suffers from less performance interference when it is hosted on . Hence the scheduling algorithm allocates  to , after which the entries in the second and fourth columns (representing components to migrate \emph{to} nodes  and ), and the fourth row (representing components to migrate \emph{out of}  and ) are updated. All the entries in the second row are not updated because  is not considered in the following scheduling. Let the migration threshold be =5ms, we can see that after this loop, the scheduling process is completed because no further effective migration can be conducted: all the values of entries in the updated matrix are smaller than 5ms.

\begin{figure}
\centering
  \includegraphics[scale=0.45]{ServerMigration.pdf}\\
  \caption{An example loop of migrating component  to node }
  \label{Fig: ServerMigration}
\end{figure}

The complexity of each scheduling interval is . Specifically, the performance matrix can be constructed in  time. The scheduling process can be completed within  loops, where each loops takes  to find the optimal migration and  to update the matrix.

\section{Evaluation}  \label{Section: Evaluation}
\subsection{Experiment methodology} \label{Section: Experiment methodology}
\textbf{Experiment platform}. The experiments were conducted in a set of 30 nodes connected with a 1Gb ethernet network. Each node has two 6-core Xeon E5645 processors and hosts multiple VMs using Xen Virtual Machine Monitor (VMM). The operating system of both physical machines and VMs is SUSE Linux Enterprise Server (SLES)-11-SP1. The Xen, JDK versions are 4.0, 1.7.0, respectively. In addition, the versions of Nutch (search engine), Hadoop, and Spark are 1.1, 1.0.2, and 0.8.0, and the versions of Storm, Python, and Zookeeper are 0.9.2, 2.6, and 3.4.6, respectively.



\textbf{Workloads}. We use representative workloads from the open-source BigDataBench workload suite \cite{opensourceBigDataBench}. The Nutch web search engine \cite{nutchsearch} represents the latency-critical online service and its online web search performance was tested. As shown in Figure \ref{Fig: SearchEngine}, this service has three stages and we call the components at Stage 1, 2, and 3 \emph{segmenting components}, \emph{searching components}, and \emph{aggregating components}, respectively.
The batch jobs involve a variety of Hadoop MapReduce and Spark jobs.
Hadoop jobs include the two typical CPU-intensive workloads with float point and integer calculations (Na\"{i}ve Bayes classification and WordCount) and one workload having similar demands for CPU and I/O resources (Page Index).
Spark jobs are mostly I/O-intensive workloads including Na\"{i}ve Bayes, WordCount and Sort.
These short-running batch jobs whose execution time ranges from a few seconds to several minutes represent a large fraction of jobs in today's data center workloads \cite{reiss2012heterogeneity,chen2012interactive}.




















\textbf{Compared techniques}.
Two classes of state-of-the-art latency reduction techniques are compared. (i) \emph{Request redundancy} \cite{vulimiri2012more, ananthanarayanan2013effective,stewart2013zoolander}. For each request, multiple replicas are created for parallel execution and only the quickest replica is used. Two different redundancy policies, which generate three or five replicas were tested. (ii) \emph{Request reissue} \cite{jalaparti2013speeding,tailatScale}. A request is first sent to the most approximate component for execution, and a replica of this request is sent if the first one is not completed after a brief delay. The quickest replica is then used. Two reissue policies, which send a secondary request after the first has been executed for more than the 90th percentile or the 99th percentile of the expected latency for this class of requests, were tested.

For simplicity, we will call the four compared techniques, \emph{RED-3}, \emph{RED-5}, \emph{RI-90}, \emph{RI-99}. We also call the basic technique without any redundancy or reissue \emph{Basic}, and our predictive component-level scheduling approach \emph{PCS}.

\textbf{Metrics}. Two metrics are used to evaluate the performance of the search engine service. The \emph{first} metric is the 99th percentile latency of individual components of all requests. In the case of the request redundancy and reissue techniques, this metric denotes latencies of components belonging to the quickest replica. The \emph{second} metric is the average overall service latency of all requests.



\textbf{Measurement method}. In the experiments, the monitor dynamically inspects the running service, including its request arrival rate and resource contention information listed in Table \ref{table: Usage information of shared resources}. The monitor obtains the request arrival rate and the system-level contention information once every second and the micro-architectural contention information once every minute. This measurement method guarantees low overheads in monitoring and does not affect the application performance.





\subsection{Prediction accuracy} \label{Section: Prediction accuracy}
The effectiveness of the proposed scheduling approach is considerably impacted by the performance model's accuracy. To evaluate this accuracy, we ran each searching component of the service on a VM with 1 core and 1GB memory, and used another VM with 4 core and 4GB memory co-located on the same node to run a Hadoop or Spark job of different input sizes. In each test, we trained the regression models based on the historical running information and predicted the component's service using the constructed models.















As listed in Figure \ref{Fig: Prediction errors of the performance model under different performance interferences}, in our evaluation, the Hadoop workloads have 20 different input sizes ranging from 50MB to 4GB, and the Spark workloads have 10 different input sizes ranging from 200MB to 7GB, thus having distinct performance interferences to the component's latency. As shown in Figure \ref{Fig: Prediction errors of the performance model under different performance interferences}, the prediction errors are \emph{smaller} than 3\%, 5\%, and 8\% in 63.33\%, 82.22\%, and 96.67\% of the evaluation cases, respectively. When considering all the input sizes, the average prediction error is 2.68\%, indicating the performance model keeps a good track of the observed latency and it is sufficient for our scheduling heuristic to achieve a near-optimal performance.



\begin{figure*}
\centering
  \includegraphics[scale=0.51]{performanceModel.pdf}\\
  \caption{Prediction errors of the performance model under different performance interferences}
  \label{Fig: Prediction errors of the performance model under different performance interferences}
\end{figure*}














\subsection{Service Performance} \label{Section: Service Performance}
\textbf{Evaluation setting}. Following the deployment settings of the previous section, we tested the performance of the Nutch search engine service whose searching components are deployed in 100 VMs. Each component co-locates with a mixed of batch jobs running on VMs of the same node. The Hadoop workloads were tested with continuously changing input data sizes ranging from 1MB to 10GB. Six request arrival rates, namely 10, 20, 50, 100, 200, 500 requests/second, were tested to compare the latency reduction techniques under online services' diurnal variation in load.



\textbf{Migration threshold}. As explained in Section \ref{Section: Algorithm}, the proposed scheduling algorithm employs a migration threshold to control latency reduction and throttle non-beneficial component migration. This threshold should be reasonably high to filter out most of the detrimental migrations whose overheads are larger than the possible latency reduction. On the other hand, the threshold cannot be too high to miss the opportunities for latency reduction.
The major overhead of migrating a component is caused by the movement from its current VM to the destination VM. The component runs on a VM installed Storm and its migration is enforced by calling Storm's deployment APIs. Specifically, Storm first uploads the source codes (e.g. codes for looking up indexes for documents) and the configuration information of the component to ZooKeeper \cite{ZooKeeperWebsite}, a widely used distributed coordination system to manage application deployment. ZooKeeper then allocates them to a new component on the destination VM. At each scheduling interval, the migration of components (e.g. 10 to 20 components) can be completed within 3 seconds without interrupting the running services and only causes small consumptions of memory and I/O resources.
Considering the migration cost, we find out that 5\% of the accepted overall service latency (100ms) is a reasonable threshold value for the studied online services and thus the threshold in scheduling is set as 5ms. Applying an adaptive threshold to improve the service performance is possible, but it is beyond the scope of this paper.











\begin{figure*}
\centering
  \includegraphics[scale=0.58]{CompareLatencyArrivalRate.pdf}\\
  \caption{Comparison of overall service latency and the tail latency under different request arrival rates}
  \label{Fig: Overall service time and tail latency under different request arrival rates}
\end{figure*}

\textbf{Evaluation results}.
Figure \ref{Fig: Overall service time and tail latency under different request arrival rates} shows the comparison of service performance for six different techniques. The results show that PCS achieves the smallest tail latencies and the overall service latencies in all cases. This is because during the execution of the service, PCS dynamically enforces different component-node allocations along with the latest performance interference changes on different nodes and reduces the component latency variability by migrating the straggling components to nodes with less resource contentions.

By contrast, the \emph{request redundancy} technique just collects responses from the quickest component based on the current service deployment, missing the opportunity to migrate the components to the idlest nodes with the least performance interference. Figure \ref{Fig: Overall service time and tail latency under different request arrival rates}(a) and (b) show that this technique achieves some latency reduction under light workloads. However, when the arrival rate gradually increases to 500, Figure \ref{Fig: Overall service time and tail latency under different request arrival rates}(c) to (f) show that this technique adversely causes longer latencies compared to those of Basic. In particular, RED-5 causes the longest latencies because it produces the largest workloads, namely incurring the longest queueing delay, among all techniques.
Although the redundancy technique employs the cancelation mechanism that sends messages to cancel other queuing replicas when one replica begins execution, the components still execute replicas of the same request unnecessarily. This phenomenon mainly comes from two sources: (i) all replicas of a request are sent to multiple components simultaneously, hence two components having similar performances may start executing the requests in similar time; (ii) there is a network message delay for different components to communicate each other's status, hence two components may start executing the same request and the cancelation messages are both in the flight to each other.
Moreover, the \emph{request reissue} technique applies a conservative redundancy mechanism that only creates replicas for requests judged as outliers (i.e. requests whose execution time is larger than an expected latency). Results in Figure \ref{Fig: Overall service time and tail latency under different request arrival rates} show that compared to the request redundancy technique, this conservative reissue technique causes less performance deterioration when load becomes heavier.















\textbf{Results}. \emph{Considering all the six request arrival rates, PCS achieves 67.05\% reduction in the 99th component latency and 64.16\% reduction in the overall service latency when comparing to the request redundancy and reissue techniques.}







































\subsection{Scalability of scheduling} \label{Section: Scalability of scheduling}



In proposed scheduling heuristic, the used performance mode is constructed based on profiling of each component. That is, only one out of all homogeneous components needs to be profiled and thereby avoiding the scalability issue associated with the service profiling. For example, in the tested search engine service, only three components (segmenting, searching and aggregating) need to be profiled. Meanwhile, the proposed scheduling algorithm estimates the service performance by analyzing the resource contention information obtained from each component, and hence the \emph{analysis time} scales only linearly with the number of components. Another important aspect of the scalability of scheduling is to \emph{search} the appropriate component-code allocation, and the time complexity of this is  when allocating  components to  nodes.

To evaluate the scheduling algorithm scalability, we measured both the analysis and searching time under different numbers of components and nodes. Figure \ref{Fig: Scalability of the searching algorithm} shows that even if the number of components reaches 640 (and the number of nodes reaches 128), the algorithm takes only 551ms to complete. This time is less than 0.1\% of the 600 seconds scheduling interval and hence can be ignored. For services with more components, the scheduler could apply a hierarchical strategy that divides the components into small groups of 640 components or less and finds the appropriate component-node allocation between groups and then within groups. The scheduling overhead therefore can remain low even with a large number of components.

\begin{figure}
\centering
  \includegraphics[scale=0.50]{Scalability.pdf}\\
  \caption{Scalability of the schduling algorithm}
  \label{Fig: Scalability of the searching algorithm}
\end{figure}


\section{Related Work} \label{Section:Related Work}

\subsection{Application-level management of service performance}

At present, two categories of techniques have been proposed to meet the performance requirement of latency-critical services by alleviate the performance degradation due to resource sharing and contention. The \emph{first category} of techniques disallow the co-location of services with applications incurring large contention of resources such as caches \cite{kasture2014ubik} and CPU resources \cite{xu2013bobtail}.
The \emph{second category} of techniques dynamically manage applications to meet their performance requirement at run-time according to the monitored interference metrics, such as the LLC miss rate reflecting cache contentions \cite{ahn2012dynamic} and the bandwidths reflecting I/O resource contentions \cite{xu2010mitigating}.
These techniques focus on addressing performance variability of applications by viewing the application as a whole, ignoring issues relating to fine-grained latency variability of its individual components. However, these components' tail latency dominates performance of large-scale, parallel services.

\subsection{Tail latency reduction techniques}

We now review four categories of reduction techniques.

\textbf{Modifying hardware/software systems}. These techniques aim at solving the tail latency caused by system design issues. Those include architecture-level design that disables the power saving model to promote system performance \cite{wang2013impact}; OS-level design that changes the default kernel scheduler to a better scheduler (e.g. Borrowed Virtual Time (BVT)) with better support for time-sensitive requests \cite{leverich2014reconciling}.

\textbf{Adding additional resources}. These techniques require additional resources to handle slow requests, either by increasing the parallelism degree of the request processing \cite{jeon2014predictive} or adding new server components \cite{stewart2013zoolander}.

\textbf{Partially processing request}. These techniques reduce tail latency by only using a portion (e.g. 90\%) of the quickest sub-requests \cite{jalaparti2013speeding} or a synopsis representing the entire input data at a high level of approximation \cite{han2015sarp}, thus scarifying result correctness such as query accuracy for reducing service latency.

The approach proposed in this work can work together with the above techniques to reduce tail latency, thus forming a complement to these techniques.
Both this work and the fourth category of techniques, namely \textbf{request redundancy} \cite{vulimiri2012more, ananthanarayanan2013effective,stewart2013zoolander} \textbf{and reissue} \cite{jalaparti2013speeding,tailatScale} explained in Section \ref{Section: Experiment methodology}, reduce tail latency by addressing component latency variability. The key idea of the request redundancy technique is to execute the same request on multiple components so as to reduce its latency by using the quickest one. Although these techniques work well when workloads underutilize system resources \cite{stewart2013zoolander}, they start hurting the service performance and adversely worsen the latency when load gets heavier \cite{shah2013redundant} .







\section{Conclusion} \label{Section: Conclusion}
This paper presents a component-level scheduling framework that can dynamically schedule components of a service across hundreds of machines in a cloud data center. To adapt to the changing performance interferences and workloads, this framework leverages cost-efficient online monitors and an analytic performance model to simultaneously predict the components' latency when running on different nodes. Using the predicted performance, the scheduler identifies straggling components and enforces near optimum component-node allocations. By comparing to the best well-known techniques on reducing tail latency, we demonstrate that our approach achieves significant reductions in both component tail latency and overall service latency.











\section{Acknowledgements}
We sincerely thank Moustafa M. Ghanem and Li Guo and for their useful comments, and the anonymous reviewers for their feedback on earlier versions of this manuscript. This work is supported by Chinese 973 projects under Grants No. 2014CB340402.

\bibliographystyle{plain}
\bibliography{references}
\end{document}
