\subsection{3D face reconstruction}
\label{subsec:3d-face-recon}

\begin{figure}[t]
    \begin{minipage}{6.95cm}
    \includegraphics[width=\textwidth]{figures/now_curves.pdf}\end{minipage}\hfill \begin{minipage}{4.95cm}
        \small
        \begin{tabularx}{\textwidth}{Xccc}
            Method & \multicolumn{3}{c}{Error (mm)}\\
            Single view & Median & Mean & Std\\
            \hline
            \scriptsize{Deng et al. \cite{deng2019accurate}} & 1.11 & 1.41 & 1.21\\ 
            RingNet \cite{RingNet:CVPR:2019} & 1.21 & 1.53 & 1.31\\ 
            3DFAv2 \cite{guo2020towards} & 1.23 & 1.57& 1.39\\
            DECA \cite{Feng:SIGGRAPH:2021} & 1.09 & 1.38 & 1.18\\
            Dib et al. \cite{dib2021towards} & 1.26 & 1.57 & 1.31\\ 
            \textbf{ours} & \textbf{1.02} & \textbf{1.28} & \textbf{1.08}\0.1em]
    \includegraphics[width=0.25\textwidth]{figures/now_ablation_0.jpg}\includegraphics[width=0.25\textwidth]{figures/now_ablation_1.jpg}\includegraphics[width=0.25\textwidth]{figures/now_ablation_2.jpg}\includegraphics[width=0.25\textwidth]{figures/now_ablation_3.jpg}
    \end{minipage}\hfill \begin{minipage}{4.95cm}
        \small
        \begin{tabularx}{\textwidth}{Xccc}
            Number of & \multicolumn{3}{c}{Error (mm)}\\
            Landmarks & Median & Mean & Std\\
            \hline
            68 & 1.10 & 1.38 & 1.16\\ 
            320 & 1.00 & 1.24 & 1.02\\ 
            \textbf{703} & \textbf{0.95} & \textbf{1.17} & \textbf{0.97}\\\hline
            703 {\scriptsize(without )} & 1.02 & 1.26 & 1.03\\
        \end{tabularx}
    \end{minipage}
    \caption{Ablation studies on the NoW \cite{RingNet:CVPR:2019} validation set confirm that denser is better: model fitting with more landmarks leads to more accurate results.
    In addition, we see that fitting without using  leads to worse results.}
    \label{fig:experiments-now-ablation}
\end{figure}

\subsection{Facial performance capture}

\subsubsection{Multi-view}


Good synthetic training data requires a database of facial expression parameters from which to sample.
We acquired such a database by conducting markerless facial performance capture for 108 subjects.
We recorded each subject in our 17-camera studio, and processed each recording with our offline multi-view model fitter.
For a 520 frame sequence it takes 3 minutes to predict dense landmarks for all images, and a further 9 minutes to optimize face model parameters.
See \autoref{fig:eval-multi-view-perf-cap} for some of the 125,000 frames of expression data captured with our system.
As the system which is used to create the database is then subsequently re-trained with it, we
produced several databases in this manner until no further improvement was seen.
We do not reconstruct faces in fine detail like previous multi-view stereo approaches \cite{globallyConsistentReconstructionPopa, passiveFacialPerfCapture, anchorFramesPaper}.
However, while previous work can track a detailed 3D mesh over a performance, our approach reconstructs the performance with richer semantics: identity and expression parameters for our generative model.
In many cases it is sufficient to reconstruct the low-frequency shape of the face accurately, without fine details.

\begin{figure}[t]
    \includegraphics[width=\textwidth]{figures/camcap_grid_small.jpg}\caption{We demonstrate the robustness and reliability of our method by using it to collect a massive database of 125,000 facial expressions, fully automatically.}
\label{fig:eval-multi-view-perf-cap}
\end{figure}

\subsubsection{Real-time monocular}

See the last two columns of \autoref{fig:experiments-3d-recon-qaul} for a comparison between our offline and real-time systems for monocular 3D model-fitting.
While our offline system produces the best possible results by using a large CNN and optimizing over all frames simultaneously,
our real-time system can still produce accurate and expressive results fitting frame-to-frame.
Please refer to the supplementary material for more results. Running on a single CPU thread (i5-11600K), our real-time system spends 6.5ms processing a frame (150FPS), of which 4.1ms is spent predicting dense landmarks and 2.3ms is spent fitting our face model.

\begin{figure}[t]
    \scriptsize
    \begin{tabularx}{\textwidth}{YYYYYYYY}
        & RingNet
        & Deng et al.
        & 3DFAv2
        & MGCNet 
        & DECA 
        & ours
        & ours\\
        & \cite{RingNet:CVPR:2019}
        & \cite{deng2019accurate}
        & \cite{guo2020towards}
        & \cite{shang2020self}
        & \cite{Feng:SIGGRAPH:2021}
        & (offline)
        & (real-time)
    \end{tabularx}
    \includegraphics[width=\linewidth]{figures/3d_face_recon_qual_comp.jpg}
    \caption{
Compared to previous recent monocular 3D face reconstruction methods,
ours better captures gaze, expressions like winks and sneers, and subtleties of facial identity.
In addition, our method can run in real time with only a minor loss of fidelity.}
    \label{fig:experiments-3d-recon-qaul}
\end{figure}

