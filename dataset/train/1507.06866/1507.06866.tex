\documentclass[11pt]{article}\usepackage{fullpage}
\usepackage{amssymb,amsmath}
\usepackage{graphicx, epsfig}




\def\id#1{\ensuremath{\mathit{#1}}}
\let\idit=\id
\def\idbf#1{\ensuremath{\mathbf{#1}}}
\def\idrm#1{\ensuremath{\mathrm{#1}}}
\def\idtt#1{\ensuremath{\mathtt{#1}}}
\def\idsf#1{\ensuremath{\mathsf{#1}}}
\def\idcal#1{\ensuremath{\mathcal{#1}}}  
\def\floor#1{\lfloor #1 \rfloor}
\def\ceil#1{\lceil #1 \rceil}
\def\etal{\emph{et~al.}}

\newcommand{\no}[1]{}
\newcommand{\myparagraph}[1]{{\bf #1}}
\newcommand{\todo}[1]{} \newcommand{\settwo}[2]{\ensuremath{\{\,#1\,|\,#2\,\} }}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newenvironment{proof}{\trivlist\item[]\emph{Proof}:}{\unskip\nobreak\hskip 1em plus 1fil\nobreak
\parfillskip=0pt\endtrivlist}

\newenvironment{itemize*}{\begin{itemize}\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}\setlength{\parsep}{0pt}\setlength{\topsep}{0pt}\setlength{\partopsep}{0pt}}{\end{itemize}}


\newcommand{\cT}{{\cal T}}
\newcommand{\tL}{{\tilde L}}
\newcommand{\tS}{{\tilde S}}
\newcommand{\tB}{{\tilde B}}
\newcommand{\tE}{{\tilde E}}
\newcommand{\tW}{{\tilde W}}
\newcommand{\cTb}{{T}^{bwt}}
\newcommand{\cD}{{\cal D}}
\newcommand{\cB}{{\cal B}}
\newcommand{\cK}{{\cal K}}
\newcommand{\cC}{{\cal C}}
\newcommand{\cI}{{\cal I}}
\newcommand{\cL}{{\cal L}}
\newcommand{\cN}{{\cal N}}
\newcommand{\cM}{{\cal M}}
\newcommand{\Temp}{{Temp}}
\newcommand{\bT}{\mathbb{T}}
\newcommand{\tR}{{\widetilde R}}
\newcommand{\oX}{{\overline X}}
\newcommand{\oY}{{\overline Y}}
\newcommand{\oZ}{{\overline Z}}
\newcommand{\oS}{{\overline S}}
\newcommand{\on}{{\overline n}}
\newcommand{\tpi}{{\widetilde \pi}}
\newcommand{\eps}{\varepsilon}
\newcommand{\tpred}{\mathrm{tpred}}
\newcommand{\Col}{\mathrm{Col}}
\newcommand{\occ}{\mathrm{occ}}

\newcommand{\tcount}{t_{\mathrm{count}}}
\newcommand{\trange}{t_{\mathrm{range}}}
\newcommand{\tlocate}{t_{\mathrm{locate}}}
\newcommand{\textract}{t_{\mathrm{extract}}}
\newcommand{\tsa}{t_{\mathrm{SA}}}

\newcommand{\ra}{\idrm{rank}}
\newcommand{\sel}{\idrm{select}}
\newcommand{\acc}{\idrm{access}}
\newcommand{\init}{\idrm{init}}
\newcommand{\Pref}{\mathit{Pref}}
\pagestyle{plain}


\newcommand{\shortver}[1]{}
\newcommand{\longver}[1]{#1}
\newcommand{\shlongver}[2]{#2}




\begin{document}
\title{Compressed Data Structures  for  Dynamic Sequences}

\author{
 J. Ian Munro\thanks{Cheriton School of Computer Science, University of Waterloo. Email {\tt imunro@uwaterloo.ca}.}
 \and
 Yakov Nekrich\thanks{Cheriton School of Computer Science, University of Waterloo.
 Email: {\tt yakov.nekrich@googlemail.com}.}
}
\date{}
\maketitle

\begin{abstract}
We consider the problem of storing a dynamic string  over an alphabet  in compressed form. Our representation supports insertions and deletions of symbols and answers three 
fundamental queries:  returns the -th symbol in , 
counts how many times a symbol  occurs among the first  positions in , 
and  finds the position where a symbol  occurs for the -th time. We present the first fully-dynamic data structure for arbitrarily large alphabets that achieves optimal query times for all three operations and supports updates with worst-case time guarantees. Ours is also the first fully-dynamic data structure that needs only  bits, where  is the -th order entropy and  is the string length. 
Moreover our representation supports extraction of a substring  in optimal  time.
\end{abstract}
 

\section{Introduction}
In this paper we consider the problem of storing a sequence  of length  over an alphabet  so that the following operations are supported:\\
-  returns the -th symbol, , in \\
-  counts how many times  occurs among the first  symbols 
in , \\
- finds the position in  where  occurs for the -th time,
 where  is such that   and . \\
This problem, also known as the rank-select problem, is one of the most fundamental problems in compressed data structures. 
There are many  data structures that store a string in compressed form and support three above defined operations efficiently. There are static data structures that use  bits 
or even  bits for any   and a positive constant \footnote{Henceforth , where  is the number of times  occurs in , 
is the -th order entropy and   for  is the -th order empirical entropy.  can be defined as 
, where  is the subsequence of  generated by symbols that follow the -tuple ;  is the lower bound on the average space
usage of any statistical compression method that encodes each symbol using the context of  previous symbols~\cite{Manzini01}.}. Efficient static rank-select data structures are described in \cite{GGV03,GMR06,FMMN07,LP07,LP09,BGNN10,HM10,NS14,BN12}. We refer to~\cite{BN12} for most recent results and a discussion of previous static solutions.

In many situations we must work with dynamic sequences. We must be able to insert a new symbol at an arbitrary 
position  in the sequence or delete an arbitrary symbol .  The design of dynamic solutions, that support insertions and deletions of symbols, is an important problem.  Fully-dynamic data structures for rank-select problem were considered in~\cite{HSS03,CHL04,BB04,MN06,CHLS07,GHSV07,MN08,HSS11}. 
Recently Navarro and Nekrich~\cite{NavarroN13,NavarroN13a} obtained a fully-dynamic solution with  times for , , and  operations. By the lower bound of Fredman and Saks~\cite{FS89}, these query times are optimal.  The data structure described in~\cite{NavarroN13} uses  bits and supports updates in  amortized time. It is also possible to support updates in  worst-case time, but then the time for answering a  query grows to ~\cite{NavarroN13a}. All previously known fully-dynamic data structures need at least  bits. Two only exceptions are data structures of Jansson et al.~\cite{JanssonSS12} and Grossi et al.~\cite{GrossiRRV13} that keep  in  bits, but do not support  and  queries. 
A more restrictive dynamic scenario was considered by Grossi et al.~\cite{GrossiRRV13} and Jansson et al.~\cite{JanssonSS12}: an update operation \emph{replaces} a symbol  with another symbol so that the total length of  does not change, but insertions of new symbols or deletions of symbols of  are not supported.  Their data structures need  bits and answer access queries in  time; the data structure of Grossi et al.~\cite{GrossiRRV13} also supports rank and select queries in  time.

In this paper we describe the first fully-dynamic data structure 
that keeps the input sequence in  bits; our representation supports , , and  queries 
in optimal  time. 
Symbol insertions and deletions at any position in  are supported in  worst-case time.
We list our and previous results for fully-dynamic sequences in Table~\ref{tab:ranksel}. 
Our representation of dynamic sequences also supports the operation of extracting a substring. Previous dynamic data structures 
require  calls of  operation in order to extract the substring of length . Thus the previous best fully-dynamic 
representation, described in~\cite{NavarroN13} needs  time to extract a substring  of . Data structures described in~\cite{GrossiRRV13} and~\cite{JanssonSS12} support substring extraction in  time but they either do not support  and  queries or they support only updates that replace a symbol with another symbol. Our dynamic data structure can extract a substring in optimal  time without any restrictions on updates or queries. 



\begin{table}[tb]
  \centering
\resizebox{\textwidth}{!}{
  \begin{tabular}{|l|c|c|c|c|c|c|} \hline
    Ref. & Space & Rank & Select & Access & Insert/  &  \\
         &       &      &        &        & Delete   &  \\ \hline \hline
\cite{HM10} &  & \multicolumn{3}{|c| }{} &  & W \\ \hline
\cite{NS14} &  & \multicolumn{3}{|c|}{} &  & W \\
\hline
\cite{NavarroN13}   &  &  &  &  &  & A \\ \hline
\cite{NavarroN13}   &  &  &  &  &  & W \\   
\hline
\cite{JanssonSS12} & & - & - &  &  & W \\ \hline
\cite{GrossiRRV13} &  & - & -  &   &   & W \\ \hline\hline
New                &  &  &  &  &   & W \\ \hline \hline
  \end{tabular}
}
  \caption{Previous and New Results for Fully-Dynamic Sequences. The rightmost column indicates whether updates are amortized (A) or worst-case (W). We use notation  in this table.}
  \label{tab:ranksel}
\end{table}


In Section~\ref{sec:ranklog} we describe a data structure that uses  bits per symbol and supports , , and  in optimal  time. This data structure essentially maintains a linked list  containing 
all symbols of ; using some auxiliary data structures on , we can answer  , , and  queries on . 
In Section~\ref{sec:ranklogsigma} we show how the space usage can be reduced to  bits per symbol. A compressed data structure that needs  bits per symbol is presented in Section~\ref{sec:compr}.  The approach of Section~\ref{sec:compr} is based on dividing  into a number of subsequences. We store a fully-dynamic data structure for only one such subsequence of appropriately small size.  Updates on other subsequences are supported by periodic re-building.
 In Section~\ref{sec:compr2} we show that the space usage can be reduced to . 

\section{-Bit Data Structure}
\label{sec:ranklog}
We start by describing a data structure that uses  bits per symbol.
\begin{lemma}
  \label{lemma:logsigma0}
A dynamic string  for  over alphabet  
can be stored in a data structure that needs
 bits, and answers queries
,  and  in time .
Insertions and deletions of symbols are supported in  time. The data structure uses a universal look-up table of size  for an arbitrarily small .
\end{lemma}
\begin{proof}
  We keep elements of  in a list . Each  entry of  contains a symbol . 
For every , we also maintain the list . Entries of  correspond to those entries of  that contain the symbol .
We maintain data structures  and  that enable us to find the number 
of entries in  (or in some list ) that precede 
an entry  (resp.\ ); we can also find the -th entry  in  or  using .
We will prove in Lemma~\ref{lemma:dl} that  needs  bits and supports queries and updates on  in  time. 


We can answer a query  by finding the -th entry  in , following the pointer from  to the corresponding entry , and counting the number   of entries preceding  in . Clearly\footnote{To simplify the description, we assume that a list entry precedes itself.}, . To answer a query , we first find the -th entry  in . Then we find  the last entry   that precedes  and contains . Such queries can be answered in  time as will be shown in \shlongver{the full version of this paper, attached at the end of this submission}{Lemma~\ref{lemma:colpred} in Section~\ref{sec:markpred}}. If  is the entry that corresponds to  in , then , where  is the number of entries that precede  in . 
\end{proof}



\section{-Bit Data Structure}
\label{sec:ranklogsigma}
\begin{lemma}
  \label{lemma:logsigma}
A dynamic string  over alphabet  
can be stored in a data structure using 
 bits, and supporting queries
,  and  in time .
Insertions and deletions of symbols 
are supported in  time. 
\end{lemma}
\begin{proof}
If , then the data structures described in~\cite{NS14} and~\cite{HM10} provide desired query and update times. The case  is considered below. 

\tolerance=1000
We show how the problem on a sequence of size  can be reduced to the same problem on a sequence of size . 
The sequence  is divided into chunks.
We can maintain the size  of each chunk , so that  and the total number of chunks is bounded by . We will show how to maintain chunks in \shlongver{the full version of this paper}{Section~\ref{sec:logsigmaupd}}. 
For each
, we keep a global bit sequence . 
 where  is the number 
of times  occurs in the chunk . 
We also keep a bit sequence .
We can compute 
 where , , ,
 and .     
To answer a query , we first find the index 
of the chunk  that contains the -th occurrence of ,
. Then we find  for ;
 identifies the position of the -th occurrence of  in the chunk , where  denotes the number of 's in the first  chunks. Finally we compute  where  
is the total number of symbols in the first  chunks. 
We can support queries and updates on  and on each  
in  time~\cite{NS14}. 
By Lemma~\ref{lemma:logsigma0}, queries and updates on  
are supported in  time. 
Hence, the query and update times of our data structure are 
.

  can be kept in  bits~\cite{NS14}. The array 
 uses  bits, where  is the number of times  occurs in .  \no{ bits. Hence all  need . By Jensen's inequality, the second term of this expression can be bounded by .} Hence,  all  and  use  bits.
By Lemma~\ref{lemma:logsigma0}, we can also keep the data structure for each chunk in  bits per symbol.
\end{proof}



\section{Compressed Data Structure}
\label{sec:compr}
In this Section we describe a data structure that uses  bits per symbol. We start by considering the case when the alphabet size is not too large, .
The sequence  is split into subsequences , ,   for . 
The subsequence  is stored in  bits per element as described in Lemma~\ref{lemma:logsigma}. 
Subsequences  are substrings of .  are stored in compressed static data structures. New elements are always inserted into the subsequence .  Deletions from , , are implemented as lazy deletions: an element in  is marked as deleted. We guarantee that the number of elements that are marked as deleted  is bounded by 
. If a subsequence  contains many elements marked as deleted, it is re-built: we create a new instance of  
that does not contain deleted symbols. If a symbol sequence  contains too many elements, we insert the elements of  into 
 and re-build  for .
 Processes of constructing a new subsequence and re-building a subsequence with too many obsolete elements are run 
in the background.



\no{As follows from the above high-level description,  are subsequences (but not substrings!) of : if an element  precedes , then  precedes  in . But the index of the subsequence  that contains an element  
depends only on the time when it was inserted and does not depend on its position in . Thus  subsequences can be interspersed in an arbitrary way. }


The bit sequence  identifies elements in  that are marked as deleted:  if and only if   is marked as deleted. 
The bit sequence  distinguishes between the elements of  and elements of , :  if the -th element of  is kept in  and  otherwise. 

We further need  auxiliary data structures for answering 
select queries. We start by defining an auxiliary subsequence  that contains copies of elements already stored in other subsequences.  
Consider a subsequence  obtained by merging subsequences , ,  (in other words,  is obtained from  by removing elements of ). 
Let  be the subsequence obtained by selecting (roughly) every -th occurrence of a symbol  in . 
The subsequence  is obtained by merging  subsequences  for all .  
Finally  is obtained  by merging  and .  
 We support queries  on , defined as follows:  such that (i) a copy of  is stored in  and (ii) if , then  and copies of elements , , ,  are not stored in . That is,  returns the largest index  , such that   precedes  and  is also stored in . The data structure for  delivers approximate 
answers for  queries; we will show later how the answer to a query  can be found quickly if the answer to  is known. Queries  can be implemented using standard operations on a bit sequence of size   bits; for completeness, we provide a description in Section~\ref{sec:appselprime}.  
We remark that  and  are introduced to define ; these two subsequences are not stored in our data structure. 
The bit sequence  indicates what symbols of  are also stored in :
 if a copy of  is stored in  and  otherwise. 
The bit sequence  indicates what symbols in  are actually from :  iff  is stored in the subsequence . 
Besides, we keep bit sequences  for each . Bits of  correspond to occurrences of  in . 
If the -th occurrence of  in  is marked as deleted, 
then . All other bits in  are set to .

We provide the list of subsequences in Table~\ref{tab:auxdata}. Each subsequence is augmented with a data structure that supports rank and select queries.  
For simplicity we will not distinguish between a subsequence and a data structure on its elements. 
If a subsequence supports updates, then either (i) this is a subsequence over 
a small alphabet or (ii) this subsequence contains a small number of elements. 
In case (i), the subsequence is over an alphabet of constant size; by ~\cite{NS14,HM10} queries on such subsequences are answered in  time. In case (ii) the subsequence contains  elements; data structures on such subsequences are implemented as in Lemma~\ref{lemma:logsigma}. 
All auxiliary subsequences, except for , are of type (i). Subsequence  and an auxiliary subsequence  are of type (ii). Subsequences  for  are static, i.e. they are stored in data structures that do not support updates. We re-build these subsequences when they contain too many obsolete elements.  Thus dynamic subsequences support , , , and updates in  time. It is known that we can implement all basic operations on a static sequence in  time\footnote{Static data structures also achieve significantly faster query times, but this is not necessary for our implementation.}. Our data structures on static subsequences are based on the approach of Barbay et al.~\cite{BHMR07}; however, our data structure can be constructed faster when the alphabet size is small and supports a substring extraction operation. A full description will be given in \shlongver{the full version of this paper}{Section~\ref{sec:construct}}. We will show below that queries on  are answered by  queries on dynamic subsequences and  queries on static subsequences.

We also maintain arrays  and  for every . For any ,  is the number of symbols in  and  specifies how many times  occurs in . We keep a data structure that computes  the sum of the first  entries in  and find the largest  such that  for any integer . The same kinds of queries are also supported on .  Arrays  and 
 use  bits. 


\begin{table}[tb]
  \centering
  \begin{tabular}{|l|c|c|c|} \hline
    Name & Purpose & Alph. & Dynamic/ \\
         &          & Size  & Static\\ \hline
                    & Subsequence of  & - & Dynamic \\
    ,  & Subsequence of  & - & Static \\
       & Positions of symbols in , , that are marked as deleted & const & Dynamic \\
                      & Positions of symbols from  in  \no{Relative order of symbols in  and  for } & const & Dynamic\\
                & Delivers an approximate answer to select queries          & -  & Dynamic \\
    ,  & Auxiliary sequences for                             & - & Dynamic \\
                    & Positions of symbols from  in                     & const & Dynamic \\
                    & Positions of symbols from  in   & const & Dynamic \\
                    & Positions of symbols marked as deleted among all 's    & const & Dynamic \\
\hline
  \end{tabular}
  \caption{Auxiliary subsequences for answering rank and select queries. A subsequence is dynamic if both insertions and deletions are supported. If a subsequence is static, then updates are not supported. Static subsequences are re-built when they contain too many obsolete elements.}
  \label{tab:auxdata}
\end{table}


\paragraph{Queries.}
To answer a query , we start by computing ;  is the position of the -th element that is not marked as deleted. Then we find  and . By definition of ,  is the number of elements of  that are stored in the subsequence . 
The number of 's in   is computed as .
The number of 's in  before the position  is 
found as follows. We identify the index , such that . 
Then we compute how many times  occurred in , , 
and in the relevant prefix of , . 
Let . Thus  is the number of symbols '' that are not marked as deleted among the first  occurrences of  in .
Hence .


To answer a query , we first obtain an approximate answer by asking a query . 
Let  be the rank of the -th symbol  that is not marked as deleted. Let . We find  and . 
Let  and  be the positions of   and  in .
By definition of ,  and .  If , then obviously .
Otherwise the answer to  is an integer between  and .
By definition of , the substring , , ,  contains at most  occurrences of .
All these occurrences are stored in  subsequences  for  .
We compute  and . We find the index  
such that .
Then  is the position of  in . We find its index in  by computing   and . Finally  .




Answering an  query is straightforward. We determine whether  is stored in  or in some  for  using . Let . If  and  is stored in , then . If , we compute   and find the index  such that 
. The answer to  is  for .


\paragraph{Space Usage.}
The redundancy of our data structure can be estimated as follows. The space needed to keep the symbols that are marked as deleted in subsequences  is bounded by : Let  denote the number of symbols  that are marked as deleted and let . Then all symbols that are marked as deleted use  bits. Since , . If , . If , then  .   also takes  bits.
The bit sequences  and  need  bits; ,  also use  bits. 
 Each bit sequence  can be maintained in  bits where  is the total number of symbols  in  and  is the number of symbols  that are marked as deleted. All  take . To estimate the last expression, we divide the alphabet  into  and ;   contains all symbols  such that  and  contains all symbols , such that . Then . Hence all  need 
 bits.  
The subsequence  can be stored in 
 bits. Thus all auxiliary subsequences use 
 bits. 
Data structures for subsequences , , use   bits for any , 
where  is the number of symbols in .  
Since  for , all subsequences  are stored in  bits. 


\paragraph{Updates.}
When a new symbol is inserted, we insert it into the subsequence  and update the sequence . 
The data structure for  is also updated accordingly. We also insert a -bit at the appropriate position of bit sequences  and  where  is the inserted symbol. Deletions from  are symmetric.  
When an element is deleted from , , we replace the -bit corresponding to this element in   with a -bit. We also  change the appropriate bit in  to , where  is the symbol that was deleted from . 


We must guarantee that the number of elements in  is bounded by ; the number of elements marked as deleted must be also bounded by . Hence we must re-build the data structure when the number of symbols in  or the number of deleted symbols is too big. Since we aim for updates with worst-case bounds, the cost of re-building is distributed among  updates. We run two processes in the background. The first background process moves elements of  into subsequences . The second process purges sequences , ,  and removes all symbols marked as deleted from these sequences. Details are given in Section~\ref{sec:updatesbackground}. 


We assumed in the description of updates that  is fixed. In the general case we need additional background processes that increase or decrease sizes of subsequences when  becomes too large or too small. These processes are organized in a standard way. \no{and will be described in the full version of this paper.}
Thus we obtain the following result
\begin{lemma}
  \label{lemma:midsigma}
A dynamic string   over alphabet  for 
can be stored in a data structure that uses  bits and answers queries
,  and  in time .
Insertions and deletions of symbols are supported in  time. 
\end{lemma}
\shortver{In the full version of this paper we show that the space usage of the above described data structure can be reduced to  bits. We also show how the result of Lemma~\ref{lemma:midsigma} can be extended to the case when .  The full version also contains the description of the static data structure and presents the procedure for extracting a substring  of  in 
 time.}

\subsection{Compressed Data Structure for }
\label{sec:bigsigma}
If the alphabet size  is almost linear, we cannot afford storing the arrays 
. 
Instead, we keep a bit sequence  for each alphabet symbol .
Let  denote the number of 's occurrences in the subsequence  and . Then  . 
If ,we can keep  in  bits.
If , we can keep  in  bits.
Using , we can find for any  the subsequence , such that  in  time.

We also keep an effective alphabet\footnote{An alphabet for  is effective if it contains only symbols that actually occurred in .} for each . We keep 
a bit vector  of size , such that  if and only if  occurs in . Using , we can map a symbol  to a symbol  
so that  for any  that occurs in . Let . For every  we can find the corresponding symbol  
using a  query on . We keep a static data structure for each sequence  over .
Queries and updates are supported in the same way as in Lemma~\ref{lemma:midsigma}. Combining the result of this sub-section and Lemma~\ref{lemma:midsigma}, we obtain 
the data structure for an arbitrary alphabet size.
\begin{theorem}
  \label{theor:anysigma}
A dynamic string   over alphabet  
can be stored in a data structure that uses  bits and answers queries ,  and  in time .
Insertions and deletions of symbols are supported in  time. 
\end{theorem}


\section{Compressed Data Structure II}
\label{sec:compr2}
By slightly modifying the data structure of Theorem~\ref{theor:anysigma} we can reduce the space usage to essentially  bit per symbol for any   simultaneously. First, we observe that any sub-sequence  for  is kept in a data structures that consumes  bits of space. Thus all  use  bits.  It can be shown that 
 bits; we prove this bound in Section~\ref{sec:anal}.  Since , the data structure of Theorem~\ref{theor:anysigma} uses  bits. 

In order to get rid of the  additive term, we use a different static data structure;  our static data structure is described in Section~\ref{sec:construct}. As before, the data structure for a sequence  uses   bits.   But we also show in Section~\ref{sec:construct} that our static data structure can be constructed in  time if the alphabet size  is sufficiently small, .   The space usage  can be achieved by appropriate change of the parameter .  
If , we use the data structure of Theorem~\ref{theor:anysigma}.  As explained above, the space usage is .
If  we  also use the data structure of Theorem~\ref{theor:anysigma}, but we set  and implement static data structures as in  Section~\ref{sec:construct}. The data structure needs 
 bits. Since we can re-build a static data structure for a sequence  in  time, background processes incur an additional cost of . Hence the cost of updates does not increase.
 \begin{theorem}
  \label{theor:anysigma2}
A dynamic string   over alphabet  
can be stored in a data structure that uses  bits and answers queries ,  and  in time .
Insertions and deletions of symbols are supported in  time. 
\end{theorem}

\section{Substring Extraction}
\label{sec:substr0}
Our representation of compressed sequences also enables us to retrieve a substring  of .
The static data structure, described in Section~\ref{sec:construct} supports substring extraction in  time. Hence we can quickly retrieve a substring of any . We can also augment  with an  additional bits, so that a substring of  is extracted in the same time.
We can retrieve a substring of  by extracting a substring of  and a substring of some  for  and merging the result. A detailed description is provided in Section~\ref{sec:substr}.  Our  result can be summed up as follows.
\begin{theorem}
  \label{theor:substr}
We can augment  data structures described in Theorem~\ref{theor:anysigma} and Theorem~\ref{theor:anysigma2} with 
 additional bits, so that a substring of length  can be extracted in  time. The parameter  is defined in the same way as in Theorems~\ref{theor:anysigma} and~\ref{theor:anysigma2}. 
\end{theorem}




\bibliographystyle{abbrv}
\bibliography{dynrank}

\newpage
\appendix
\renewcommand\thesection{A.\arabic{section}}

\section{Colored Predecessor Queries}
\label{sec:markpred}
In this section we consider predecessor queries on a linked list, called  colored predecessor queries.  The result of this section  is used in the proof of Lemma~\ref{lemma:logsigma0}.
Suppose that each entry in an ordered list  is \emph{colored} with a symbol  from an alphabet . We will also sometimes say that 
an entry  contains a symbol . 
A colored predecessor query  for an entry  and a symbol  asks for the rightmost entry  
that is   colored with  and precedes . We consider the problem of answering colored predecessor queries on a dynamic list . 
This problem was previously considered by Kopelowitz~\cite{Kopelowitz12} who described  a randomized -time solution.
Mortensen~\cite{Mor03} described an  time solution for the case  and a  constant . 
We present here a deterministic solution for an arbitrarily large alphabet. This result is also of independent interest. 


We start by describing a data structure that uses  more than linear space. Then we will show how the space usage can be reduced to linear and how the update time can be decreased. 
\begin{lemma}
  \label{lemma:colpred0}
Let  be a list with  entries. There exists an -bit data structure that answers colored predecessor queries on  in  time and supports insertions and deletions in  time.
\end{lemma}
\begin{proof}
For a symbol , let  denote the sublist of  that consists of entries containing . Each entry that contains  
is augmented with a pointer to the next and the previous entries in .  We also store an order maintenance data structure on . This data structure can determine in  time whether  precedes  in  for two arbitrary  entries  and  in a dynamic list . We refer to~\cite{BenderCDFZ02,Kopelowitz12} for a description of such a data structure. 

We keep a balanced tree  on . For a node , the set  consists of all symbols  such that at least one leaf descendant of  contains . In every leaf of , we keep pointers to all its ancestors.  
For every , we also keep 
  and ;  (resp.\ ) points to the leftmost (rightmost) element of  in the subtree of  colored with . 

Suppose that we want to find the rightmost entry  that contains a symbol  and precedes an entry . 
We look for the lowest ancestor  of (the leaf that contains)  such that . Using binary search on  ancestors of , we can find  in  time.  If  is in the right subtree of , then 
 where  is the left child of . If  is in the left subtree of , then we find  where  is the right child of . The entry  is the leftmost entry that follows . Hence the entry  is the first occurrence of  in  before . In other words,  precedes  in . 

When a new element  is inserted into , we insert it into some leaf  of  and a new entry into the corresponding list . Insertion into  requires that we find the rightmost entry  that is colored with  and precedes  in the list. This takes  time as described above. Then we visit all ancestors of  in . If necessary, we add  to   in each visited node .
 We keep the tree balanced, using the algorithms of weight-balanced B-tree~\cite{ArgeV03}.
The cost of maintaining  so that its height remains  is  per insertion.  Deletions are symmetric. When an element  is deleted, we remove it from the list  and update  in at most one ancestor of . Then we remove the leaf that contains . The weight-balanced B-tree is 
not modified after the deletion of a leaf. But when a fraction of leaves is deleted, we construct a new tree  and discard the old instance of . The process of re-building  can be run in the background so that the total worst-case cost of deleting  is  .
\end{proof}

\begin{lemma}
\label{lemma:colpred}
Let  be a list with  entries. There exists an -bit data structure that answers colored predecessor queries on  in  time and supports insertions and deletions in  time.
\end{lemma}
\begin{proof}
We divide every  into  blocks so that every block contains  consecutive entries of  .  If  consists of more than one block, then we maintain the list  that contains the first entry from every block of .   The list  contains all elements of  for all symbols . We keep  in the data structure of Lemma~\ref{lemma:colpred0}. 
For any symbol , all elements of  are also stored in a data structure  that supports finger searches~\cite{GuibasMPR77}:  For any element  and a \emph{finger} ,  can return the rightmost entry 
 that is colored with  and precedes  in  
comparisons, where  is the number of entries between  and  in . Finally we also keep the list  in the union-split-find data structure~ of Mehlhorn~\cite{MehlhornN90}. Using this data structure, we can find the first  that precedes any  in  time. The data structure of Mehlhorn et al.~\cite{MehlhornN90} uses  words and supports updates in  time. 


In order to find  colored with symbol  that precedes , we find the first entry  that precedes . Then we identify the first entry  colored with  that precedes .  There are  entries of  between  and . When  is known, we can find  in  time using finger search on . The total query time is dominated by the search in  and equals . 

When a new entry  of color  is inserted, we update . 
Then we find the position of  in  and update  and . We can maintain the sizes of blocks in lists  so that each block consists of  entries and  there is one insertion into  for  insertions into ; details will be given in the full version.  Thus the total cost of an insertion is . Deletions are symmetric. 
\end{proof}
  

\section{Prefix Sum  Queries on a List}
\label{sec:dl}
In this section we describe a data structure on a list  that is used in the proof of Lemma~\ref{lemma:logsigma0} in Section~\ref{sec:ranklog}.
\begin{lemma}
  \label{lemma:dl}
We can keep  a dynamic list  in an -bit data structure , where  is the number of entries in .  can find the -th entry in  for  in 
 time.  can also compute the number of entries before a given element  in  time. Insertions and deletions are also supported in  time.  
\end{lemma}
\begin{proof}
 is implemented as a balanced tree with node degree . In every internal node we keep a data structure ;  contains the total number  of elements stored below every child  of .
 supports prefix sum queries (i.e., computes  for any ) and finds the largest 
, such that  for any integer . 
We implement   as in Lemma 2.2 in~\cite{PatrascuD04}
so that both types of queries are supported in  time.    uses linear space (in the number of its elements) and can be updated in  time.  needs a look-up table of size . To find the -th entry  in a list, we traverse the root-to-leaf path; in each visited node  we find the child that contains the -th entry using . 
To find the number of entries preceding a given  entry  in a list, we traverse  the leaf-to-root path  that starts in the leaf containing . In each visited node  we answer a query to : if the -th child  of  is on , then we  compute  using . The total number of entries to the left of  is the sum of  for all  nodes  on . 
Since we spend  time in each visited node, both types of queries are answered in  time. 
An update operation leads to  updates of
data structures . The tree can be re-balanced using the weight-balanced B-tree~\cite{ArgeV03}, so that its height is always bounded by .
\end{proof}





\section{Updating Data Structure in Lemma~\ref{lemma:logsigma}}
\label{sec:logsigmaupd}
When the size of a chunk  equals  we start the procedure of re-building this chunk. During the next  updates of  we retrieve all elements of  and insert them  into data structures for new chunks,  and . If an update is a deletion of some element  and  was already copied into  or , then we remove the copy of 
from  or . When all elements of  are copied into  and , we say that a chunk  is a copied
chunk. We keep ids of all copied chunks in a data structure . Whenever a copied chunk  is updated we also execute  the same update of  or .

We also run the following iterative procedure that replaces copied chunks with two chunks. Each iteration starts by finding a chunk  with the largest number of elements. Then all arrays  are updated in increasing order of . We insert a -bit at an appropriate position of  so that  is changed to  where ,  and  denote the number of 's that occur in ,  and  respectively. We keep a variable  that equals the largest symbol , such that  is already updated. When all  are modified in the above manner, we also update  and change it from  to  where ,  and  denote the total number of symbols in ,  and  respectively. Finally we delete the id of  from 
 set  and start the next iteration.  Every iteration takes  time.  When a chunk is added to , its size does not exceed . Using Theorem 5 in~\cite{DietzS87}, we can show that the size of each chunk in  grows by at most by  where  denotes the -th harmonic number. 

We slightly modify the method for answering a  query. 
Let  denote the index of the last chunk that was retrieved from . That is, the above described iterative procedure is currently changing bit vectors  and  changing 
 to  and 
 to .
To answer a query , we first find the index 
of the chunk  that contains the -th occurrence of ,
. 
If  or , we proceed as described in the proof of Lemma~\ref{lemma:logsigma}.  
If  and , we decrement  by ,  and also proceed as in Lemma~\ref{lemma:logsigma}. 


We also keep track of the number of chunks that contain no more than   elements. If there are at least  chunks containing at most  symbols, then we start a global re-building procedure. We retrieve all elements of  and insert them into a new data structure. In the new data structure all elements are distributed among chunks, so that each chunk contains  elements. The global re-building process is executed during  updates. 



\section{Re-Building  Compressed Data Structure in the Background}
\label{sec:updatesbackground}
As shown in Section~\ref{sec:compr}, we must bound the total number of symbols in  by  for a parameter . 
We must also bound the number of symbols in  for 
that are marked as deleted by . 
We run two alternating processes in the background to satisfy these requirements.
 In order to bound the workspace we process sub-sequences  one-by-one. For every , , we produce a new version  of  containing  all relevant  elements of  (i.e., all elements of  that precede the first element of 
 and follow the last element of  in ). 
In order to navigate in the new version of , we must modify parts of auxiliary sequences (such as , , , and ). Therefore our background process also produces new versions for the relevant portions of auxiliary sequences. When the new version of  is created, we discard the old version; we also replace the parts of auxiliary sequences with their new versions.  The second background process removes elements marked as deleted and updates  in the same manner. A more detailed description follows. 

We conceptually divide  into  substrings  for . An element  is in  for  iff  precedes the first element of  in   and follows the last element of  in . An element  is in  if  precedes the first element of ;  is in  if  follows the last element of . 
Likewise the sequence  is conceptually divided into  substrings .
An element  is in  for some  if  is a copy of some  or  is a copy of some  and . 
We conceptually divide the binary sequence   using the same principle:  
 is in  if the -th element of  is from  or the -th element of  is some  such that  . Other binary sequences are divided in the same way.  The procedure for moving elements of  into  for some , , is as follows.
\begin{description}
\item[Step 1]
We start by creating a new instance  of  and a new instance  of ; we also create new instances of 
 and the -th parts of other binary sequences; namely ,  for all  such that  occurs in ,  and  are copies of , ,  and  respectively. The cost of creating new instances for parts of auxiliary sequences can be distributed 
among the following updates of , as will be explained below. At the end of Step 1,  is a copy of ; likewise  ,  and  are copies of  ,  and  respectively. These newly created sequences will be called copy sequences.\\
\item[Step 2]
Then we insert the elements of  at appropriate positions of .  
We  modify the sequence   accordingly. 
Changes in  and  also lead to changes in copy sequences ,  ,  and . We distribute the cost of Step 2 among  updates of .  We will say that all  elements that are kept in  (resp. in ) upon completion of Step 1  are \emph{old} elements.
 When a sequence  is updated, we spend  time on the following actions: (i) we find the next unprocessed element  in  (symbols in  are processed in the left-to-right order);  we set the bit corresponding to  in  to  (ii) we insert  at appropriate position of     (iii) if necessary, we update ; copy sequences  and  are updated accordingly.
We may also need to  update copy sequences after an update of . 
If the update of  is an insertion, and a new element  is inserted into , then we also insert  into . If an element  is deleted and , then we remove the copy of  from ; changes in  can also lead to changes in .  If a symbol  is deleted from , then we update   accordingly. 
\item[Step 3]
When  is completed, we discard old , set , and start using the new  from now on. Simultaneously we replace the relevant section of 
 with . We also replace the relevant parts of ,  ,   and  with , ,   and .
\end{description}
In order to execute the above background process, we must implement binary sequences, so that two additional procedures are supported: A binary sequence of length  is divided into  sectors (substrings) of length  each. We can produce a copy of each sector. The cost of producing a copy is distributed among  updates; when the procedure is finished, the sector and its copy are equal. We can perform updates on the original sequence and on a sector copy. We can also replace a sector with its copy and discard the original sector. 
Same procedures are also supported for the non-binary sequence .
We can implement these procedures in such way that the cost of , , , and updates is not increased. Implementation of auxiliary procedures is explained in \shlongver{the full version of this paper, attached at the end of this submission}{Section~\ref{sec:seqaddition}}.

Step 1 of the above process takes  time. Step 2 (insertion of new elements into ) takes  time, where  is the number of elements inserted into . Step 3 takes  time. Thus old elements of  are moved to  for  in 
 time. This process can be distributed among  updates.   


The process of purging the sequences , ,  is based on the same approach. 
For each , we create a new instance of  without  deleted elements; then we discard the old instance 
and start using the new version of . Relevant parts of  and binary sequences are also updated.
The re-building of  is implemented in the same way as in the procedure of moving elements from  to  for .  The cost of purging  is distributed among  following updates. 
Two above described background processes are run alternatingly; the first process starts when the either the number of elements in   or the number of elements marked as deleted is equal to .  In this way we guarantee that the number of elements in  and the number of deleted elements does not exceed .


\section{Auxiliary Procedures for Binary and Non-Binary Sequences}
\label{sec:seqaddition}
In this section we show how a sequence  can be stored  in such a way that additional processes that create a copy for a part 
of  are supported. Furthermore we can update the copied part and later replace the original part with its modified copy. 
We start by describing a binary sequence that supports an additional operation ;  initializes an empty sequence of length  that consists of  -bits.  Recall that .
\begin{lemma}
\label{lemma:ints}
  A binary sequence  that supports , , , insertions, deletions and  for any   can be stored in  bits, where  is the number of -bits and  is the length of the sequence. All operations, except for  take  time;  can be executed in  time.
\end{lemma}
\begin{proof}
We divide the sequence   into blocks  such that each  consists of  bits.  Each block is further divided into sub-blocks of  bits. We will say that a block or a sub-block is non-empty if it contains at least one -bit. A doubly-linked list  contains one entry for each non-empty block.  We also keep a list  for every block  that contains -bits;  contains one entry for each non-empty sub-block.  For each entry  of  we keep the number of 's in the corresponding block ; we also keep the total  number of bits in blocks , , , , where  is the rightmost  non-empty block that precedes . We maintain a data structure that enables us to find the block that contains the -th bit in the sequence. We also maintain a data structure that can find the block containing the -th -bit (or -bit) and the number of -bits (-bits) that precede a specified block. We maintain the same data structure for each sub-block. All these data structures are implemented as balanced trees with node degree  for a small constant . Each node is augmented with additional information about the number of -bits (resp.\ the total number of bits) in the subtrees of its children. Implementation is the same as for data structures  and  in Lemma~\ref{lemma:logsigma0}.

 Positions of -bits in the same sub-block are difference coded: for every -bit we store the difference 
between its position and the position of the preceding -bit 
in the same block; for the first -bit in the block, we store 
its position in the block.  
The list  and its data structures can be kept in  bits. All lists  and their data structures are kept in  bits. Difference coding of -bits in all blocks consumes  bits. 


To answer a query  we find the block  and its sub-block  containing the -th bit. 
Then we find the number of -bits that precede  in  and the number of -bits that precede  in . 
We can find the number of -bits that precede the bit with global position  in  using a look-up table.  
Summing three above values, we obtain .  Queries , , and  are computed in a similar way. Thus all queries are answered in  time.

Since we only keep non-empty blocks and sub-blocks, operation  takes constant time.
Insertions and deletions are implemented as in previously known  data structures supporting rank and select on binary sequences. When an element is inserted, we find its block   and its sub-block; we insert the new element into its sub-block and  update lists  and  if necessary. We maintain sizes of blocks and sub-blocks using standard techniques. Deletions are symmetric. 
Hence insertions and deletions are supported in  time. 
\end{proof}

Now we describe how a copy of a binary sequence  can be created. Let .  
\begin{lemma}
\label{lemma:bincopy}
  Let  be a binary sequence of length . Procedure , that produces a copy of , can be implemented as a background process that runs during  consecutive updates.
We can support updates on the original sequence and its copy in  time. Operations , , and  are 
executed in  time.  The underlying data structure uses  bits. 
\end{lemma}
\begin{proof}
The procedure for creating a copy  of  consists of two stages. During the first stage we produce a copy of .  is represented in the same way as in~\cite{NS14}. As described in~\cite{NS14},  is split into chunks 
and we maintain data structures that  support counting the number of  -bits (resp.\ -bits) among the chunks and searching for the chunk that contains the -th -bit (or -bit). We can create a copy by copying the original sequence of chunks. The data structure that supports counting and searching among chunks is essentially a tree with  nodes; we can create this tree in  time, where  is the number of chunks. 

Thus the background process that creates a copy of  takes  time. We can distribute its cost among  updates where . We keep information about these updates in four data structures. The data structure  keeps information about positions of updates: the -th -bit in a sequence
 is the position of the -th update (insertion or deletion) in .
Thus  contains one bit 
for every element in  and one bit for every  element that was deleted from . Updates are counted in the left-to-right order and  is implemented as in Lemma~\ref{lemma:ints}. We also keep a bit sequence  which indicates
the type of updates on :  if the -th update stored in  is 
a deletion and  if the -th update is an insertion. The sequence  
contains the values of elements inserted into . A sequence  helps us navigate between  and ;  contains one bit 
for every element in  and one bit for every  element that was deleted from . If , then the corresponding element was already deleted from ; if , then the corresponding element is in .
During each update, we perform the following operations: 
\begin{itemize*}
\item 
a new element is inserted into or deleted from  at position \\
\item Let  be the position of the -th -bit in . If the update of  is an insertion, we insert a -bit into  at position . If the update is a deletion, we replace the -th bit in  with a -bit (replacement is implemented by deleting a -bit and inserting a -bit at the same position). \\
\item If the update of  is an insertion, we insert a -bit at position  into ; if the update is a deletion we replace the -th bit of  with a -bit. We also insert a  bit indicating the type of update into the sequence . If an update is an insertion, we add the value of a new bit  into a bit sequence .  \\
\item  we spend  time on constructing a copy sequence.
\end{itemize*}

The first stage is finished after  updates of . When the first stage is completed,  and its copy sequence  differ because  most recent updates changed the original sequence but were not performed on its copy. During the second stage we synchronize  and . The synchronisation procedure is also distributed among  updates. During every update operation, we proceed as follows:\\
- a new element is inserted into  or deleted from . We also change the copy sequence  accordingly. If the position  of an element in  is known, then we can find its position  in  using sequences ,  and .
Using , we find the position  corresponding to  in .
Using , we find the number  of updates that precede ; using  we can find the number of insertions and deletions among the first  updates.\\ 
- we also execute updates stored in sequences , , and . We retrieve 
the position  of the first -bit stored in  and find the position  in  that corresponds to the position  in . Then we either insert a new element at the position  or remove the -th element from  according to the data stored in  and . Finally we delete the -th bit from  and . We also delete the corresponding bit from  and remove the corresponding symbol from  (if the processed update is an insertion). \\
At the end of the second stage  and  are equivalent.
\end{proof}


Now we consider the sequence  of length  that is divided into  contiguous parts for . Each part, called a sector of , consists of  elements. The procedure  creates a copy of an arbitrary sector. The procedure  can be executed in the background during a sequence of  updates. Furthermore we can split a sector into two sectors and merge two adjacent sectors in the same time. Last, we can also replace a sector with its copy in  time. Update operations are supported on both  itself and on the copy of a sector (we assume that at any time a copy of only one sector is created or used). 
\begin{lemma}
  \label{lemma:bincopysec}
Let  be a binary sequence of length   and let  be divided into  sectors of  symbols. Procedure , that produces a copy of a sector, can be implemented as a background process that runs during  consecutive updates. Procedures  and  can be executed in the same way.
Operation  can be executed in  time.  The underlying data structure uses  bits. 
\end{lemma}
\begin{proof}
  Every sector is maintained in the data structure of Lemma~\ref{lemma:bincopy}. Furthermore we maintain a sequence  that keeps the numbers of elements in every sector. Sequences  and  maintain the number of 's and 's in every sector. Using  and data structures for individual sectors, we can answer rank, select, and access queries on . Procedure  is implemented as  on a sector of . When a copy of a sector is ready, we can support updates on this copy. Besides we can also replace a sector with its copy and update the data structure on the sequence  accordingly; this operation takes  time. Splitting and merging of sectors is implemented in a similar way. Suppose that we want to split a sector  into  and . We employ the same two-stage procedure that was used to create a copy of a sector. During the first stage we assign elements of  to  and . Then we create the data structures for  and . Updates that are relevant for new versions are deposited in data structures , ,  and , ,  respectively. During the second stage we execute updates stored in , , and  for . Auxiliary data structures are realized in the same way as in the procedure . When new sectors  and  are ready, we replace  with  and  and update .   
\end{proof}

We can implement similar procedures for a sequence over a general alphabet. We assume, however, that copies of sectors are produced consecutively: first copy of the first sector is created; when the first sector is replaced with a (possibly modified) copy sector, we create the copy of the next sector, etc. In this scenario, it is easy to maintain the dynamic sequence that supports the copying, splitting and merging of sectors.
\begin{lemma}
\label{lemma:copygen}
  Let  be a sequence of length  over an alphabet  and let  be divided into  sectors. Procedure , that produces a copy of a sector, can be implemented as a background process that runs during  consecutive updates. Procedures  and  can be executed in the same way.
Operation  can be executed in  time. The data structure for  uses  bits.  
\end{lemma}
\begin{proof}
   Elements of  are distributed among two  dynamic data structure,  and . Both of them are implemented as in Lemma~\ref{lemma:logsigma}. Originally  is empty and all elements of  are in . Procedure  traverses elements of the next sector and appends them at the end of the new sequence . When  is executed for the last (rightmost) sector, we set  and . Let  denote the number of elements in all sectors of  for which operation  was executed. Let  denote the total number of elements currently kept in . We can answer  by retrieving  if  or retrieving  if . We can answer a query  as follows. If , . If , . 
To answer a query  we check whether . If this condition is satisfied, then ; otherwise  where .
\end{proof}


\section{Analysis}
\label{sec:anal}
We show that deleting  symbols from a sequence  does not increase too much the -th order entropy. This result is needed in Section~\ref{sec:compr2} to prove the  space bound 
of . 
Let . Let  denote the subsequence of symbols that are deleted from  and let . Let  for a parameter . We want to estimate   for some parameter  and .

A context   is an arbitrary sequence of length  over an alphabet ; let  denote the number of times 
a symbol  is preceded by context  in  and . The -th order empirical entropy is defined as .


For a context , let  be the number of times it occurs in  and let  be the number of times it occurs in .
Suppose that a symbol  is deleted. It changes the context for the next  symbols . We will say that one deletion spoils  symbols and moves them to a different context. 
If a symbol  is spoiled and the context of  in  is , then 
 is encoded with at most  bits. 
Let  be the number of new symbols in the context . Let  be the frequency of a new symbol  in  (that is, the number of times a spoiled symbol 
 appears in the context  in ). Then the total 
encoding length of spoiled symbols in the context  does not exceed   where 
.
By Jensen's inequality, . Summing over all contexts , the total encoding length of spoiled symbols can be bounded by .
 


The total number  of symbols that are spoiled is between  and  because each deletion spoils between  and  following symbols. The number of spoiled symbols does not exceed  independently of  and .
Hence . Besides 
. 
Therefore . To prove the latter fact, we divide all contexts  into classes , , , . 
 contains all context indices , such that , where  and  for .
For any , . Hence . Hence  because 
.
Thus the total encoding length of all spoiled symbols is bounded by . 


Another factor that may increase the encoding length  is that spoiled symbols are moved to new contexts and thus the encoding length of all other symbols in these new contexts slightly increases. Consider a context   that occurred  times in  and  times in  for some . 
We say that a symbol  that follows  in  is an old occurrence if this occurrence of  is also preceded by  in . 
The  encoding length for all old occurrences in  is . The total encoding length for the same occurrences in  is  .
The difference between encoding lengths of old occurrences in  and  
is . If , then . Summing up the differences over all  contexts  such that 
, we obtain . 
If , then 
Summing up over all contexts  such that , we get  . Since , 
 and .
Hence, .  

Thus .  
We must also account for elements that are marked as deleted, but are still stored in sequences  for . The number of elements that are marked as deleted is bounded by . These elements  need  bits. Every deleted element spoils up to  symbols of . Using the same analysis as above, the extra encoding length due to spoiled symbols can be estimated to be . Thus all static sequences  for  are stored in  bits.




\section{Static Data Structure}
\label{sec:construct}
\tolerance=1000
In this section we describe a  static data structure supporting 
access, rank and select queries. In comparison to previous static data structures, we obtain two additional results. Our data structure can be constructed quickly if the alphabet size  is small.  At the same time we show that our data structure supports extraction of a  substring of length  in optimal  time. 
As before, let  denote a sequence of length  over an  alphabet . 

Our static representation keeps the sequence  in compressed form following the approach of\cite{FerraginaV07}.  is represented as a sequence of meta-symbols
over an alphabet  for . That is, each meta-symbol encodes  symbols of the original sequence. 
It is shown in~\cite{FerraginaV07} that  simultaneously for all . We can keep  in  bits using e.g. Huffman coding. \no{For any , this representation needs  bits.}
\paragraph{Data Structure for Rank and Select Queries.}
 We split  into blocks of size . For every , we keep a binary sequence 
where  denotes the number of 's occurrences in the -th block. 
It was shown in 
\cite{BHMR07} how query  or   can be reduced to  rank and select queries on a block  and  queries on .
The data structure for a block   is as follows. We keep a bit vector  where  is the number of times  occurs in . Let  denote the 
position of  in the stable sorted ordering. That is,  is the permutation of  
obtained by stably sorting the symbols of . Let  denote the inverse of . Then  where . We can find  for any  by answering one rank and one select query on . 

Let .
For every symbol , , the set   contains every -th occurrence of  in ; that is  contains all  such that  and  for some integer . We keep a y-trie data structure on , so that for any  we can find the largest  satisfying . Furthermore we store  values of  for all . For each symbol , we also keep . We need  bits to store the array  and  bits to store . Hence the total space usage is .

Let  denote the partial rank query: if , ; otherwise  is undefined. If ,  where  is the largest position in  such that . Since  can be found in  time,  can be computed in  time.
We can compute  as follows. If , then . Since  can be computed in  time, we can find  for any  in  time. 
Using the data structure of~\cite{MunroRRR12}, we can compute  in  time using  additional bits, where  is the time needed to compute . This data structure works as follows:
We decompose the permutation  into cycles. A cycle is the shortest subsequence  of  such that  for  and . For every cycle of length , we select every -th element and mark it. We keep the value of  for the marked elements where  denotes the inverse of  iterated  times. In order to find , we compute , , ,  until we reach a marked position  or  for some . If , then . If we reached a marked position , we compute . Then we identify , ,  until . Clearly  in this case. It is easy to check that we must compute  at most  times; details can be found in~\cite{MunroRRR12}.  
Thus  is computed in  time. We already showed how to answer  query using . Hence  is also answered in  time. To answer a rank query , we first find the largest  such that . If , then . We can find the exact value of  by answering   queries as described in\cite{GMR06,BHMR07}. Hence  is computed in  time.  We set . Hence a query  is answered in  time. 






\paragraph{Linear Construction Time.}
The data structure described above can be constructed in  time.  We can split  into blocks in linear time. Then we stably sort each block and compute the number of times  the symbol  occurs in a block. We can implement stable sorting by replacing each  with  and applying radix sort to the resulting sequence. 
Using sorted array , we can: (i) compute  for each position  within a block; (ii) find values of  for each symbol  and construct the sequence ; (iii) generate sets  and the array . All these auxiliary structures can be created in linear time. 
We can construct a -trie for  in  time: 
each element of a y-trie is kept in  dictionary data structures; using the deterministic method described in~\cite{Ruzic08}, we can construct a dictionary with  elements in  time. 
Hence the total time needed to construct a y-trie is . Since all  contain  elements,
y-tries for all  are created in  time.
Since we can compute  for each  in  time using , we can produce a data structure for computing  in linear time. Thus the data structure for answering rank and select queries in a block can 
be created in  time. When values of  are known for all blocks we can  construct global bit sequences  for each  . 

\paragraph{Data Structure  for .}
In this case the data structure can be constructed in less than linear time. We assume that the symbols of  are initially packed into words of  bits so that each word contains  symbols. 
We split the sequence  into blocks of size . We keep exactly the same 
data structures for each block as in the case of   and bit sequences  defined in the same way as above. 
We start by splitting  into blocks and producing an array  for each block  so that 
. This step takes  time. 
 can be sorted in  time, using the ideas of sorting algorithms for small integers described in~\cite{AnderssonHNR95} and~\cite{AlbersH97} . Then we can traverse sorted array  and generate sets that must be stored in data structures  in  time. 
All  contain  elements and can be constructed in  time. 
We traverse  again and obtain  for each . Given , we can construct  by ``reverse sorting''. Let . That is, the first  most significant  bits of  contain a symbol  of the sequence , the next  bits contain its position  in , the next  bits contain the value of . We sort  according to 
the value of bits at positions , ,  (bits that correspond to the positions  of symbols in the original sequence) and then discard the first  bits. 
The resulting array is the array . 

We can also use  to construct the bit sequence : we traverse  and compute  for all , . When all  are known, we can produce  in  time; a data structure supporting rank and select on  can be also produced in  time. 

Finally we need to create the data structure for computing . Recall that we have to find all cycles of length at least  and select every -th element in a cycle. 
Let  for . During the first stage we create  tuples so that each tuple is of the form  for some  and each integer ,  occurs in at most one tuple. 
First we obtain values  for all  and keep tuples 
 in the array . Using , we can obtain  in  time. We traverse  and remove all tuples 
 such that . Then we obtain the sequence  
that contains tuples  for all  such that  
is still in . We create a new instance  of  and sort all tuples by their second components. Elements of  are tuples  sorted by . Elements of  are  tuples  sorted by . Both  and  are traversed simultaneously. 
If the -th tuple in  is  and , then the
-th tuple in  is . When we read the  and , we create the new tuple  and keep it in a sequence . When  is constructed, we discard ;
then we traverse  and remove all  such that 
. This procedure is iterated  times. During the -th iteration, we sort tuples in  by their last components and obtain . Then we merge  with  and obtain . 
We traverse  and remove tuples  satisfying 
. Each iteration takes  time. Hence  
is obtained in  time. 

 

At the end of the first stage we obtain the sequence . Every value 
 that is not in a cycle of length  is stored in exactly one tuple of . Hence  consists of  tuples. We can easily process all tuples in  time and find all values , , that 
must be marked. We can find  for all marked positions  in  time. Thus the structure for computing  is constructed in  time. The total time needed to produce the static data structure for a sequence  is thus .


\paragraph{Data Structure for }
In the case when  is very small, we use a different data structure. 
We implement rank and select operations on  using the result of Theorem 13 in~\cite{BN12}. Their data structure splits  into chunks of size . Each chunk is kept as in~\cite{RRR07}. We can traverse  and obtain compressed representation of each chunk in  time. We maintain certain bit sequences for chunks that are described in~\cite{BN12} and can be constructed in  time. Since , . This representation of  also supports fast substring extraction: since  is kept in chunks, we can decode all symbols from a chunk in  time and retrieve a string of length  in  time. 

\begin{theorem}
  \label{theor:static}
There exists a data structure   that that stores a sequence  in   bits, where  is the alphabet size,  and supports queries , , and  in  time.   can be constructed in  time. \\
Suppose that  and  is initially stored in  words, so that every word contains  consecutive symbols; then  can be constructed in  time.  
\end{theorem}
Finally we remark about re-building static sequences that is needed by background processes described in Section~\ref{sec:updatesbackground}. When a subsequence  is re-built, we retrieve  using the algorithm for substring extraction in  time. The decoded sequence  
is then kept in uncompressed form; we keep  in a sequence of words, so that each word contains  symbols. We can apply the construction algorithms described in this section to uncompressed sequence . The workspace needed to store  in plain form is  bits. 



\section{Operation  on  and Reporting a Substring of a Binary Sequence}
\label{sec:appselprime}
\paragraph{Operation .}
Let  be the subsequence of  that consists of all occurrences of a symbol . 
We maintain a bit sequence  for each sequence . 
For every element of , we keep one or two consecutive bits 
in . If the -th occurrence of  is not stored in , then we represent it by a ; if the -th occurrence 
is stored in  (i.e., it is stored in either  or ), then we represent it by 
a two-bit sequence . Let  denote the number of symbols in  among the first  symbols of . Then  is represented by the -th bit in  or by the -th and  -st bits in : if  is stored 
in , then  and ; otherwise 
 and  represents the next symbol in . We can answer rank and select queries on  and support updates on  in  time. 
Let  and . Then 
.  


\paragraph{Reporting a Substring  in a Binary Sequence.}
Let  be a binary sequence. 
We prove the following Lemma:
\begin{lemma}
  \label{lemma:substr}
Let  be a binary  sequence of length  with  
 -bits. We can store  in  bits, 
so that any substring  can be obtained in  time.
Insertions and deletions are supported in  time. 
\end{lemma}
\begin{proof}
  We store  using a variant of run-length encoding: 
each substring that consists of  -bits followed by a -bit, where , is encoded as an integer . For instance, a sequence  will be encoded as 
. We divide the run-length encoded sequence into blocks, such that each block consists of at least  and at most  run-lengths and the length of each block is at most  bits. Run-lengths are delta-encoded so that a run of length  uses  bits. Thus  each block contains  bits. 

We  also maintain an additional data structure  that finds for each position  in , the run-length  that encodes  and the block that contains the run-length .   encodes every run-length in unary. Thus a run of length  is represented by 
. Since  contains  -bits,  consists of  runs of 's followed by a . Hence  consists of  's and  -bits. The sequence  encodes in unary the number of runs in every block of .  Using standard methods, we can keep  and  in  bits and support queries and updates in  time. 
Using rank and select queries on  and , we can find the block that encodes  and the position of  in its block
for any , , in  time. 
We also keep a look-up table  that enables us to  retrieve all  elements stored in a 
block in  time; for every block,  contains the sequence of bits encoded by this block.  Since there are  different blocks and each block encodes a poly-logarithmic number of elements,  uses  bits. 

 Each block contains either at least  -bits or at least 
 -bits. Hence the total number of blocks 
is . 
Each block needs  bits. Hence all blocks 
use  bits.  

To extract a substring , we start by finding the block  that contains  and the position of  in .  Then we simply decode the remaining part of the block  and the following blocks until  symbols are decoded.
\end{proof}

\section{Substring Extraction}
\label{sec:substr}
Now we show how the fully-dynamic data structure described in Section~\ref{sec:compr2} supports the operation of retrieving a substring of length .  
Suppose that we want to extract the substring .
We keep a copy  of subsequence  implemented as follows. 
 is split into words, such that each word contains between  and  symbols of . Let  be the number of symbols in the -th word; 
we maintain a prefix-sum data structure on . Using this data structure, we can find the word  that contains the -th symbol of  in  time. We can find the position  of  in that word in  time. Using table look-up, we can extract the remaining symbols of  in  time. If , we extract the following symbols from words , ,  until  symbols are 
reported. 

The static data structure on  can be used to extract  symbols in  time. Some of these symbols can, however, be marked as deleted. We use the following additional structures in order to extract  undeleted symbols in  time.
Recall that each sequence  is stored as a sequence of meta-symbols  and every meta-symbol represents  symbols. We say that a meta-symbol  is \emph{spoiled} if at least  symbols represented by  are marked as deleted. A symbol is spoiled if it is stored in a spoiled meta-symbol. Positions of spoiled symbols are indicated by a binary sequence . That is,  iff the symbol  is not spoiled. Symbols stored in spoiled meta-symbols are also kept in a sequence . Representation of   is similar to representation of , but it contains only undeleted symbols stored in spoiled meta-symbols.  is divided into words and each word  contains up to  
symbols.  If a word  contains less than  symbols, than the last symbol in this word is followed by a non-spoiled symbol. Each word is augmented with a field . 
Let  denote the symbol that follows the last symbol in .
  if  is spoiled; otherwise  points to the position of  in . 
A sequence   indicates boundaries of words in :  contains a -bit for every symbol in  that is not the last symbol in 
its word ;  contains a two-bit substring  for every symbol that is the last symbol in its word. Thus each symbol is encoded by a -bit and the end of every word in  is encoded by a -bit. If a symbol  is not marked as deleted and kept in a spoiled meta-symbol, then we can find the position of  in  by answering one rank query on  and one rank and one select query on . 

The total number of symbols that are marked as deleted in all  is bounded by . Hence the number of spoiled symbols in all  is also . 
Non-deleted symbols kept in a spoiled meta-symbol are stored in at most three words of . Hence the total number of words in all  is bounded by . Since every word uses  bits of space, all  need   bits.  All bit sequences  and  use  and  bits respectively. Hence we need 
 additional bits in order to support substring extraction.

 


Suppose that a string  must be extracted. We find the meta-symbol  that contains  and decode meta-symbols , ,  and output the appropriate symbols until  symbols are reported or a spoiled meta-symbol is encountered. If the symbol  
is spoiled, we find the position of  in  and output  symbols from . If we enumerated symbols of  and , then we switch back to , where  is the meta-symbol that is pointed to by , and decode symbols from , ,  until a spoiled symbol is encountered. We output symbols from , ,  until .  
We proceed in the same way until  symbols are decoded. 
Each meta-symbol of  and each word of  is processed in  time. It is easy to check that the total number of words and meta-symbols  is bounded by . Every retrieved non-spoiled symbol in , except for the first one and the last one, contains  symbols. Every processed word in , except for the last one, either contains  symbols or is followed by a non-spoiled meta-symbol.  The position of the first accessed spoiled symbol in  is computed in  time. The position of the first accessed meta-symbol in  is also computed in  time.  Thus the total query time is . 


 The extraction of  symbols  from the global sequence  is implemented as follows. We find  and . We compute   such that  and extract substring  for . If the end of  is reached, we extract remaining symbols from , , . 
We also extract . Let  be the substring extracted from the static subsequence (or subsequences) and let  be the string extracted from . We can merge the prefix of  with the prefix of  using . 
At each step we consider the next  symbols of  and  symbols of  that are not processed yet. Suppose that these symbols are stored in words  and  respectively.  We read the next  bits of  and keep them in a bit sequence . Using a look-up table, we can obtain the sequence  that consists of  following symbols in  time: if , then the -th symbol of  is the -th symbol of  where  is the number of 's among the first  bits of ; otherwise the -th symbol of  is the -th symbol of  where  is the number of 's among the first  bits of . The sequence  contains the next  symbols of . Proceeding in the same way, we can obtain the substring  in  time. 



\end{document}
