\documentclass{llncs}
\usepackage{times}
\usepackage{abbrev}
\usepackage{trackchanges}

\usepackage{listings}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{a4,a4wide}
\pagestyle{empty}

\author{Alexandre Miguel Pinto \and Lu\'is Moniz Pereira \\
		\{amplmp\}@di.fct.unl.pt}
\title{Each \nlp\ has a \twov\\ \MHs}
\institute{Centro de Intelig\^encia Artificial (CENTRIA), Departamento de Inform\'atica\\
		Faculdade de Ci\^encias e Tecnologia, Universidade Nova de Lisboa\\
		2829-516 Caparica, Portugal}

\begin{document}
	\maketitle
	\begin{abstract}
		In this paper we explore a unifying approach --- that of hypotheses assumption --- as a means to provide a semantics for all \NLPs\ 
		(NLPs), the \MH\ (MH) semantics
		\footnote{	This paper is a very condensed summary of some of the main contributions of the PhD Thesis \cite{PHDAMP} of the first author, supported by FCT-MCTES grant SFRH / BD / 28761 / 2006, and supervised by the second author.}.
		This semantics takes a positive hypotheses assumption approach as a means to guarantee the desirable properties of \m\ existence,
		relevance and cumulativity, and of generalizing the \SMss\ in the process.
		To do so we first introduce the fundamental semantic concept of minimality of assumed positive hypotheses, define the MH semantics, and
		analyze the semantics' properties and applicability. 
		Indeed, abductive Logic Programming can be conceptually captured by a strategy centered on the assumption of abducibles (or hypotheses).
		Likewise, the Argumentation perspective of \LPs\ (e.g. \cite{dung95acceptability}) also lends itself to an arguments (or hypotheses)
		assumption approach.
		Previous works on Abduction (e.g. \cite{DBLP:journals/logcom/KakasKT92}) have depicted the atoms of default
		negated literals in NLPs as abducibles, i.e., assumable hypotheses.
		We take a complementary and more general view than these works to NLP semantics by employing positive hypotheses instead.		
		
		{\bf Keywords:} Hypotheses, Semantics, NLPs, Abduction, Argumentation.
	\end{abstract}
	
	\section{Background}
		\LPs\ have long been used in \KRR.
		\mydef{\NLP}{nlp}{
			By an alphabet  of a language  we mean (finite or countably infinite) disjoint sets of constants, predicate 
			symbols, and function symbols, with at least one constant.
			In addition, any alphabet is assumed to contain a countably infinite set of distinguished variable symbols.
			A term over  is defined recursively as either a variable, a constant or an expression of the form  where 
			 is a function symbol of ,  its arity, and the  are terms.
			An atom over  is an expression of the form  where  is a predicate symbol of , and
			the  are terms.
			A literal is either an atom  or its default negation .
			We dub default literals (or default negated literals --- DNLs, for short) those of the form .
A term (resp. atom, literal) is said ground if it does not contain variables.
			The set of all ground terms (resp. atoms) of  is called the Herbrand universe (resp. base) of .
			For short we use  to denote the Herbrand base of .				
			A \NLP\ (NLP) is a possibly infinite set of rules (with no infinite descending chains of syntactical dependency) of the form
			
			where , the  and the  are atoms, and each rule stands for all its ground instances.
			In conformity with the standard convention, we write rules of the form  also simply as  (known as ``facts'').
			An NLP  is called definite if none of its rules contain default literals.
			 is the head of the rule , denoted by , and  denotes the set
			 of all the literals in the body of .
		}
		When doing problem modelling with \lps, rules of the form
		
		with a non-empty body are known as a type of \ICs\ (ICs), specifically \emph{denials}, and they are normally used to prune out
		unwanted candidate solutions.
			We abuse the `' default negation notation applying it to non-empty sets of literals too: we write  to denote 
		, and confound .
		When  is an arbitrary, non-empty set of literals  we use 
		\begin{itemize}
			\item  denotes the set  of positive literals in 
			\item  denotes the set  of negative literals in 
			\item  denotes the set  of atoms of 
		\end{itemize}
		As expected, we say a set of literals  is consistent iff .
We also write  to denote the set of heads of non-IC rules of a (possibly constrained) program , i.e., 
		, and  to denote the set of facts of  --- 
		.
		\mydef{Part of body of a rule not in loop}{overlineBody}{
			Let  be an NLP and  a rule of .
			We write  to denote the subset of  whose atoms do not depend on .
			Formally,  is the largest set of literals such that
			
			where  means rule  depends on rule , i.e., either  or
			there is some other rule  such that  and .
		}

		\mydef{\LSed\ and Classically supported interpretations}{lsupport}{
			We say an interpretation  of an NLP  is layer (classically) supported iff every atom  of  is layer (classically) supported in .
 is layer (classically) supported in  iff there is some rule  in  with  such that  ().
Likewise, we say the rule  is layer (classically) supported in  iff  ().
		}
		
		Literals in  are, by definition, not in loop with .
		The notion of \ls\ requires that all such literals be \true\ under  in order for  to be \lsed\ in .
		Hence, if  is empty,  is \emph{ipso facto} \lsed.
\myproposition{Classical Support implies Layered Support}{cs=>ls}{
			Given a NLP , an interpretation , and an atom  such that , if  is classically supported in  then  is also \lsed\
			in .
		}{
Knowing that, by definition,  for every rule , it follows trivially that  is \lsed\ in  if  is
			classically supported in .
		}		
	\section{Motivation}
		``Why the need for another 2-valued semantics for NLPs since we already have the \SMs\ one?''
		The question has its merit since the \SMs\ (SMs) semantics \cite{SM-GL} is exactly what is necessary for so many problem solving issues, but the answer to it is best 
		understood when we ask it the other way around:
		``Is there any situation where the SMs semantics does not provide all the intended \ms?'' and
		``Is there any \twov\ generalization of SMs that keeps the intended \ms\ it does provide, adds the missing intended ones, and also enjoys
		the useful properties of guarantee of \m\ existence, relevance, and cumulativity?''
	
			\myex{A Joint Vacation Problem --- Merging \LPs}{vacation}{
			Three friends are planning a joint vacation.
			First friend says ``If we don't go to the mountains, then we should go to the beach''.
			The second friend says ``If we don't go to travelling, then we should go to the mountains''.
			The third friend says ``If we don't go to the beach, then we should go travelling''.
			We code this information as the following NLP:
			\myprog{
				beach & \< & \dnot mountain\\
				mountain & \< & \dnot travel\\
				travel & \< & \dnot beach}
			Each of these individual consistent rules come from a different friend.
			According to the SMs, each friend had a ``solution'' (a SM) for his own rule, but when we put the three rules together, because they
			form an \olon\ (OLON), the resulting merged \lp\ has no SM.
			If we assume  is \true\ then we cannot conclude  and therefore we conclude  is also \true\ ---
			this gives rise to the  joint and multi-place vacation solution.
			The other (symmetric) two are  and .
			This example too shows the importance of having a \twovs\ guaranteeing \m\ existence, in this case for the sake of arbitrary merging of
			\lps\ (and for the sake of existence of a joint vacation for these three friends).
		}
		\paragraph{{\bf Increased Declarativity.}}\label{subsec:increasedDeclarativity}
			An IC is a rule whose head is , and although such syntactical definition of IC is generally accepted as standard, the
			SM semantics can employ \olons, such as the  to act as ICs, thereby mixing and unnecessarily confounding two distinct
			\KR\ issues: the one of IC use, and the one of assigning semantics to loops.
			For the sake of declarativity, rules with  head should be the only way to write ICs in a LP: no rule, 
			or combination of rules, with head different from  should possibly act as IC(s) under any given semantics.
			\amp{
			It is commonly argued that answer sets (or \sms) of a program correspond to the solutions of the corresponding problem, so no answer
			set means no solution.
			We argue against this position: ``normal'' logic rules (i.e., non-ICs) should be used to shape the candidate-solution space, whereas
			ICs, and ICs alone, should be allowed to play the role of cutting down the undesired candidate-solutions.
			In this regard, an IC-free NLP should always have a \m; if some problem modelled as an NLP with ICs has no solution (i.e., no \m) that
			should be due only to the ICs, not to the ``normal'' rules.
			}
		\paragraph{{\bf Argumentation}}\label{subsec:modelingArgumentation}
			From an argumentation perspective, the author of \cite{dung95acceptability}, states:
			\begin{quotation}
				{\em ``Stable extensions do not capture the intuitive semantics of every meaningful argumentation system.''}
			\end{quotation}
			where the ``stable extensions'' have a 1-to-1 correspondence to the SMs (\cite{dung95acceptability}), and also
			\begin{quotation}
				{\em ``Let  be a knowledge base represented either as a logic program, or as a nonmonotonic theory or as an argumentation 
						framework. 
						Then there is not necessarily a ``bug'' in  if  has no stable semantics.
						
						This theorem defeats an often held opinion in the logic programming and nonmonotonic reasoning community that if a \lp\ 
						or a nonmonotonic theory has no stable semantics then there is something ``wrong'' in it.''}
			\end{quotation}
			Thus, a criterion different from the \emph{stability} one must be used in order to effectively \m\ every argumentation framework adequately.
		\paragraph{{\bf Arbitrary Updates and/or Merges}}\label{subsec:allowingArbitraryUpdatesMerges}
			One of the main goals behind the conception of non-monotonic logics was the ability to deal with the changing, evolving, updating of 
			knowledge.
			There are scenarios where it is possible and useful to combine several Knowledge Bases (possibly from different authors or sources) into a 
			single one, and/or to update a given KB with new knowledge. Assuming the KBs are coded as IC-free NLPs, as well as the updates, the resulting KB is also an IC-free NLP.
			In such a case, the resulting (merged and/or updated) KB should always have a semantics.
			This should be true particularly in the case of NLPs where no negations are allowed in the heads of rules.
			In this case no contradictions can arise because there are no conflicting rule heads.
			The lack of such guarantee when the underlying semantics used is the \SMs, for example, compromises the possibility of arbitrarily
			updating and/or merging KBs (coded as IC-free NLPs).
			In the case of self-updating programs, the desirable ``liveness'' property is put into question, even without outside intervention.
	
			These motivational issues raise the questions
``Which should be the \twov\ \ms\ of an NLP when it has no SMs?'',
			``How do these relate to SMs?'',
			``Is there a uniform approach to characterize both such \ms\ and the SMs?'', and 
			``Is there any \twov\ generalization of the SMs that encompasses the intuitive semantics of \emph{every} \lp?''.
			Answering such questions is a paramount motivation and thrust in this paper.
		\subsection{Intuitively Desired Semantics}\label{subsec:intuitivelydesired}
			It is commonly accepted that the non-stratification of the default  is the fundamental ingredient which allows for the possibility of 
			existence of several \ms\ for a program. The non-stratified DNLs (i.e., in a loop) of a program can thus be seen as non-deterministically assumable choices.
			The rules in the program, as well as the particular semantics we wish to assign them, is what constrains which sets of those choices
			we take as acceptable.
			Programs with OLONs (ex. \ref{ex:vacation}) are said to be
			``contradictory'' by the SMs community because the latter takes a negative hypotheses assumption approach,
			consistently maximizing them,
			i.e., DNLs are seen as assumable/abducible hypotheses.
			In ex.\ref{ex:vacation} though, assuming whichever maximal negative hypotheses leads to a positive contradictory
			conclusion via the rules.
			On the other hand, if we take a consistent minimal \emph{positive} hypotheses assumption (where the assumed
			hypotheses are the \emph{atoms} of the DNLs), then it is impossible to achieve a
			contradiction since no negative conclusions can be drawn from NLP rules.
			Minimizing positive assumptions implies the maximizing of negative ones but gaining an extra degree of freedom.

		\subsection{Desirable Formal Properties}\label{subsec:desirableproperties}
			Only ICs (rules with  head) should ``endanger'' \m\ existence in a \lp.
			Therefore, a semantics for NLPs with no ICs should guarantee \m\ existence (which, e.g., does not occur with SMs).
			Relevance is also a useful property since it allows the development of top-down query-driven proof-procedures that allow for the
			sound and complete search for answers to a user's query.
			This is useful in the sense that in order to find an answer to a query only the relevant part of the program must be considered, whereas
			with a non-relevant semantics 
the whole program must be considered, with corresponding performance disadvantage compared
			to a relevant semantics.
			
			\mydef{Relevant part of  for atom }{Rel_P(a)}{
				The relevant part of NLP  for atom  is\\
				
			}
			
			\mydef{Relevance (adapted from \cite{dix93aclassificationtheoryii})}{relevance}{
				A semantics  for \lps\ is said Relevant iff for every program 
				
			}			
			Moreover, cumulativity also plays a role in performance enhancement in the sense that only a semantics enjoying this property can take 
			advantage of storing intermediate lemmas to speed up future computations.
			\mydef{Cumulativity (adapted from \cite{Dix95aclassification})}{cumulativity}{
				Let  be an NLP, and  two atoms of .
				A semantics  is Cumulative iff the semantics of  remains unchanged when any atom \true\ in the semantics is 
				added to  as a fact:
				
				
			}
			Finally, each individual SM of a program, by being minimal and classically supported, should be accepted as a \m\ according to
			every \twovs, and hence every \twovs\ should be a \m\ conservative extension of \SMs.
	\section{Syntactic Transformations}
		It is commonly accepted that definite LPs (i.e., without default negation) have only one \twov\ \m\ --- its \emph{least \m} which coincides
		with the \WFM\ (WFM \cite{WFS}).
		This is also the case for locally stratified LPs.
		In such cases we can use a syntactic transformation on a program to obtain that \m.
		In \cite{DBLP:journals/tplp/BrassDFZ01} the author defined the program Remainder (denoted by ) for calculating the
		WFM, which coincides with the unique perfect \m\ for locally stratified LPs.
		The Remainder can thus be seen as a generalization for NLPs of the , the latter obtainable only from the subclass of definite LPs.
		We recap here the definitions necessary for the Remainder because we will use it in the definition of our \MHs.
		The intuitive gist of MH semantics (formally defined in section \ref{sec:MHs}) is as follows: an interpretation  is a MH \m\ of program  iff 
		there is some minimal set of hypotheses  such that the truth-values of all atoms of  become determined assuming the atoms in 
		as \true.
		We resort to the program Remainder as a deterministic (and efficient, i.e., computable in polynomial time) means to find out if
		the truth-values of all literals became determined or not --- we will see below how the Remainder can be used to find this out.
		
		\subsection{Program Remainder}
			For self-containment, we include here the definitions of \cite{DBLP:journals/tplp/BrassDFZ01} upon which the Remainder 
			relies, and adapt them where convenient to better match the syntactic conventions used throughout this paper.
			\mydef{Program transformation (def. 4.2 of \cite{DBLP:journals/tplp/BrassDFZ01})}{progTransf}{
				A \emph{program transformation} is a relation  between ground \lps.
				A semantics  allows a transformation  iff  for all  and  with 
				.
				We write  to denote the fixed point of the  operation, i.e.,  where .
				It follows that .
			}
			\mydef{Positive reduction (def. 4.6 of \cite{DBLP:journals/tplp/BrassDFZ01})}{posReduct}{
				Let  and  be ground programs.
				Program  results from  by \emph{positive reduction}  iff there is a rule  and a 
				negative literal  such that , i.e., there is no rule for  in , and 
				.
			}			
			\mydef{Negative reduction (def. 4.7 of \cite{DBLP:journals/tplp/BrassDFZ01})}{negReduct}{
				Let  and  be ground programs.
				Program  results from  by \emph{negative reduction}  iff there is a rule  and a 
				negative literal  such that , i.e.,  appears as a fact in , and 
				.
			}				
			Negative reduction is consistent with classical support, but not with the layered one.
			Therefore, we introduce now a layered version of the negative reduction operation.
			\mydef{Layered negative reduction}{layeredNegReduct}{
				Let  and  be ground programs.
				Program  results from  by \emph{layered negative reduction}  iff there is a rule 
				 and a negative literal  such that , i.e.,  appears as a fact in , 
				and .
			}	
			The \SCCs\ (SCCs) of rules of a program can be calculated in polynomial time \cite{citeulike:2204443}.
			Once the SCCs of rules have been identified, the  subset of , for each rule , is
			identifiable in linear time --- one needs to check just once for each literal in  if it is also in .
			Therefore, these polynomial time complexity operations are all the added complexity Layered negative reduction adds over regular
			Negative reduction.
			\mydef{Success (def. 5.2 of \cite{DBLP:journals/tplp/BrassDFZ01})}{success}{
				Let  and  be ground programs.
				Program  results from  by \emph{success}  iff there are a rule  and a fact
				 such that , and .
			}
			\mydef{Failure (def. 5.3 of \cite{DBLP:journals/tplp/BrassDFZ01})}{failure}{
				Let  and  be ground programs.
				Program  results from  by \emph{failure}  iff there are a rule  and a positive 
				literal  such that , i.e., there are no rules for  in , and .
			}
			\mydef{Loop detection (def. 5.10 of \cite{DBLP:journals/tplp/BrassDFZ01})}{loopDetection}{
				Let  and  be ground programs.
				Program  results from  by \emph{loop detection}  iff there is a set  of ground 
				atoms such that
				\begin{enumerate}
					\item for each rule , if , then ,
					\item ,
					\item .
				\end{enumerate}		
			}
We are not entering here into the details of the \emph{loop detection} step, but just taking note that
			1) such a set  corresponds to an unfounded set (cf. \cite{WFS}); 
			2) loop detection is computationally equivalent to finding the SCCs \cite{citeulike:2204443}, and is known to be of polynomial time
			complexity; and
			3) the atoms in the unfounded set  have all their corresponding rules involved in SCCs where all heads of rules in loop
			appear positive in the bodies of the rules in loop.
			\mydef{Reduction (def. 5.15 of \cite{DBLP:journals/tplp/BrassDFZ01})}{reduction}{\\
				Let  denote the rewriting system:\hspace{5mm}
				.
			}
			\mydef{Layered reduction}{layeredReduction}{\\
				Let  denote the rewriting system:
				.
			}				
			\mydef{Remainder (def. 5.17 of \cite{DBLP:journals/tplp/BrassDFZ01})}{remainder}{
				Let  be a program.
				Let  satisfy\\
.
Then  is called the \emph{remainder} of , and is guaranteed to exist and to be unique to .
				Moreover, the calculus of  is known to be of polynomial time complexity
				\cite{DBLP:journals/tplp/BrassDFZ01}.
				When convenient, we write  instead of .
			}
			An important result from \cite{DBLP:journals/tplp/BrassDFZ01} is that the WFM of  is such that
			, , and , 
			where 
			 denotes the set of atoms of  \true\ in the WFM,
			 denotes the set of atoms of  \true\ or \undef\ in the WFM, and
			 denotes the set of atoms of  \false\ in the WFM.
			\mydef{Layered Remainder}{layeredRemainder}{
				Let  be a program.
				Let the program  satisfy
.
Then  is called a \emph{layered remainder} of .
				Since  is equivalent to , apart from the difference between  and ,
				it is trivial that  is also guaranteed to exist and to be unique for .
				Moreover, the calculus of  is likewise of polynomial time complexity because  is also of
				polynomial time complexity.
}
			The remainder's rewrite rules are provably confluent, ie. independent of application order.
			The layered remainder's rules differ only in the negative reduction rule and the confluence proof of the former is readily adapted to the latter.
			\myex{ versus }{LRem(P)vsRem(P)}{
				Recall the program from example \ref{ex:vacation} but now with an additional fourth stubborn friend who insists on going to the 
				beach no matter what.
				
				\myprog{
					beach & \< & \dnot mountain\\
					mountain & \< & \dnot travel\\
					travel & \< & \dnot beach\\
					beach &&
				}
				We can clearly see that the single fact rule does not depend on any other, and that the remaining three rules forming the loop all
				depend on each other and on the fact rule .
				 is the fixed point of , i.e., the fixed point of 
				.
				Since  is a fact, the  transformation deletes the
				 rule;
				i.e.,  is such that
				
				\ \ \ \ \ \ \ \ \ \ \ \ \ \ 
				
				Now in  there are no rules for  and hence we can apply the  transformation which deletes the  from the body of 's rule; i.e, 
				where 
				\ \ \ \ \ \ \ \ \ \ \ \ \ \ 

				Finally, in   is a fact and hence we can again apply the  obtaining  where 
				\ \ \ 
upon which no more transformations can be applied, so .
				Instead,  is the fixed point of , i.e., the fixed point of 
				.
			}
	\section{\MHS}\label{sec:MHs}
		\subsection{Choosing Hypotheses}
			The abductive perspective of \cite{DBLP:journals/logcom/KakasKT92} depicts the atoms of DNLs as abducibles,
			i.e., assumable hypotheses.	
			Atoms of DNLs can be considered as abducibles, i.e., assumable hypotheses, but not all of them.
			When we have a locally stratified program we cannot really say there is any degree of freedom in
			assuming 	truth values for the atoms of the program's DNLs.
			So, we realize that only the atoms of DNLs involved in SCCs\footnote{\SCCs, as in Examples
			\ref{ex:vacation} and \ref{ex:LRem(P)vsRem(P)}}			
are eligible to be considered further assumable
			hypotheses.
	
			Both the SMs and the approach of \cite{DBLP:journals/logcom/KakasKT92}, when taking the abductive perspective, adopt negative
			hypotheses only.
This approach works fine for some instances of non-\wf\ negation such as loops (in particular, for \elons\ like this one), but not for
			\olons\ like, e.g. :
			assuming  would lead to the conclusion that  is \true\ which contradicts the initial assumption.
			To overcome this problem, we generalized the hypotheses assumption perspective to allow the adoption, not only of negative hypotheses,
			but also of positive ones.
			Having taken this generalization step we realized that positive hypotheses assumption alone is sufficient to address all situations, i.e.,
			there is no need for both positive and negative hypotheses assumption.
			Indeed, because we minimize the positive hypotheses we are with one stroke maximizing the negative ones, which has been the traditional
			way of dealing with the CWA, and also with \sms\ because the latter's requirement of classical support minimizes \ms.


			In example \ref{ex:vacation} we saw three solutions, each assuming as \true\ one of the DNLs in the loop.
			Adding a fourth stubborn friend insisting on going to the beach, as in example \ref{ex:LRem(P)vsRem(P)}, should still permit the two
			solutions  and .
			The only way to permit both these solutions is by resorting to the Layered Remainder, and not to the Remainder, as a means to identify
			the set of assumable hypotheses.
			
Thus, all the literals of  that are not determined \false\ in

are candidates for the role of hypotheses we may consider to assume as \true.
Merging this perspective with the abductive perspective of \cite{DBLP:journals/logcom/KakasKT92} (where the DNLs are the abducibles) we 
			come to the following definition of the Hypotheses set of a program.
			
\mydef{Hypotheses set of a program}{hyps}{
				Let  be an NLP.
				We write  to denote the set of assumable hypotheses of : the atoms that appear as DNLs in the bodies of 
				rules of .
				Formally, .
			}
			One can define a classical support compatible version of the Hypotheses set of a program, only using to that effect the Remainder 
			instead of the Layered Remainder. I.e.,	
			\mydef{Classical Hypotheses set of a program}{cHyps}{
				Let  be an NLP.
				We write  to denote the set of assumable hypotheses of  consistent with the classical notion of support: the atoms that
				appear as DNLs in the bodies of rules of .
				Formally, .
			}
			Here we take the layered support compatible approach and, therefore, we will use the Hypotheses set as in definition
			\ref{def:hyps}.
			Since  for every NLP , there is no generality loss in using  instead of , while
			using  allows for some useful semantics properties examined in the sequel.		
		\subsection{Definition}
Intuitively, a \MH\ \m\ of a program is obtained from a minimal set of hypotheses which is sufficiently large to determine the truth-value of all literals via Remainder.
			\mydef{\MH\ \m}{MHm}{
				Let  be an NLP. Let  be the set of assumable hypotheses of  (cf. definition \ref{def:hyps}), and  some subset of .
				
				A \twov\ \m\  of  is a \MH\ \m\ of  iff 
				where 
				 or 
				 is non-empty set-inclusion minimal (the set-inclusion minimality is considered only for non-empty s).
				I.e., the hypotheses set  is minimal but sufficient to determine (via Remainder) the truth-value of all literals in the program.
			}
			We already know that  and that .
			Thus, whenever  we have  which means .
			Moreover, whenever  we know, by Corollary 5.6 of \cite{WFS}, that the \twov\ \m\  such that 
			 is the unique \sm\ of .
			Thus, we conclude that, as an alternative equivalent definition,  is a \MH\ \m\ of  iff  is a \sm\ of  where  is 
			empty or a non-empty set-inclusion minimal subset of .
			Moreover, it follows immediately that every SM of  is a \MH\ \m\ of .			


				In example \ref{ex:LRem(P)vsRem(P)} we can thus see that we have the two \ms\  and 
			.
			This is the case because the addition of the fourth stubborn friend does not change the set of  which is based upon the Layered
			Remainder, and not on the Remainder.
			
		\amp{
		\myex{\MH\ \ms\ for the vacation with passport variation}{MHVacationPassport}{
			Consider again the vacation problem from example \ref{ex:vacation} with a variation including the need for valid passports for travelling 
			
			\myprog{
				beach & \< & \dnot mountain\\
				mountain & \< & \dnot travel\\
				travel & \< & \dnot beach, \dnot expired\_passport\\
				&&\\
				passport\_ok & \< & \dnot expired\_passport\\
				expired\_passport & \< & \dnot passport\_ok
			}
			We have  and thus .
			Let us see which are the MH \ms\ for this program.\\  does not yield a MH \m.\\
			Assuming  we have 
			\myprog{
				beach & \< & \dnot mountain\\
				mountain & \< & \dnot travel\\
				travel & \< & \dnot beach, \dnot expired\_passport\\
				beach&&\\
				passport\_ok & \< & \dnot expired\_passport\\
				expired\_passport & \< & \dnot passport\_ok
			}
			and 
			\myprog{
				mountain&&\\
				beach&&\\
				passport\_ok & \< & \dnot expired\_passport\\
				expired\_passport & \< & \dnot passport\_ok
			}
			which means  is not sufficient to determine the truth values of all literals of .
			One can easily see that the same happens for  and for : in either case the literals  and
			 remain non-determined.\\			
			If we assume  then  is
			\myprog{
				beach & \< & \dnot mountain\\
				mountain & \< & \dnot travel\\
				travel & \< & \dnot beach, \dnot expired\_passport\\
				&&\\
				passport\_ok & \< & \dnot expired\_passport\\
				expired\_passport & \< & \dnot passport\_ok\\
				expired\_passport &&
			}
			and 
			\myprog{
				mountain &&\\
				expired\_passport &&
			}
			which means , i.e., \\
			, is a MH \m\ of .
Since assuming  alone is sufficient to determine all literals, there is no other set of hypotheses  of 
			such that  (notice the \emph{strict} , not ), yielding a MH \m\ of .
			E.g.,  does not lead to a MH \m\ of  simply because  is not minimal w.r.t.
			.\\			
			If we assume  then  is
			\myprog{
				beach & \< & \dnot mountain\\
				mountain & \< & \dnot travel\\
				travel & \< & \dnot beach, \dnot expired\_passport\\
				&&\\
				passport\_ok & \< & \dnot expired\_passport\\
				expired\_passport & \< & \dnot passport\_ok\\
				passport\_ok &&
			}
			and 
			\myprog{
				beach & \< & \dnot mountain\\
				mountain & \< & \dnot travel\\
				travel & \< & \dnot beach\\
				passport\_ok &&
			}
			which, apart from the fact , corresponds to the original version of this example and still leaves literals with non-determined
			truth-values.
			I.e., assuming the passports are OK allows for the three possibilities of example \ref{ex:vacation} but it is not enough to entirely ``solve''
			the vacation problem: we need some hypotheses set containing one of , , or  if (in this case, 
			\emph{and only if}) it also contains .
		}
		}
			
		\amp{
		\myex{Minimality of Hypotheses does not guarantee minimality of \m}{MHnotGuaranteesMM}{
			Let , with no SMs, be
			\myprog{
				a &\<& \dnot b, c\\
				b &\<& \dnot c, \dnot a\\
				c &\<& \dnot a, b
			}
			In this case , which makes .\\			
			 does not determine all literals of  because
			 and
			.\\
 does determine all literals of  because
			 and 
			,
thus yielding the MH \m\  such that , i.e., .\\			
			 is also a minimal set of hypotheses determining all literals because
			 and 
			,
thus yielding the MH \m\  of  such that , i.e., .
			However,  is not a \mm\ of  because  is a strict superset of .
			 is indeed an MH \m\ of , but just not a minimal \m\ thereby being a clear example of how minimality of hypotheses does not 
			entail minimality of consequences.
			Just to make this example complete, we show that  also determines all literals of  because
			 and
			, 
thus yielding the MH \m\  such that , i.e., .
			Any other hypotheses set is necessarily a strict superset of either , , or  and, therefore, not set-inclusion minimal; i.e., there are no more MH \ms\ of .
		}
		Also, not all \mms\ of a program are MH \ms, 
as the following example shows.
		\myex{Some \mms\ are not Minimal Hypotheses \ms}{LDm=/=>MHm}{
			Let  (with no SMs) be
			\myprog{
				a & \< & k\\
				k & \< & \dnot t\\
				t & \< & a, b\\
				a & \< & \dnot b\\
				b & \< & \dnot a
			}
			In this case  and therefore .
			Since , the hypotheses set  does not yield a MH \m.		
			Assuming  we have 
so,  is the set of facts  and, therefore,  such that 
			, is a MH \m\ of .
			Assuming  we have 
			\myprog{
				a &\<& k\\
				k &\<& \dnot t\\
				t &\<& a\\
				b &\<& \dnot a\\
				b&&
			}
			thus , which means the set of hypotheses  does
			not yield a MH \m\ of .
			Assuming  we have 
			\myprog{
				t &\<& a,b\\
				b &\<& \dnot a\\
				a &\<& \dnot b\\
				t&&
			}
			thus , which means the set of hypotheses  does not
			yield a MH \m\ of .\\			
			Since we already know that  yields an MH \m\  with , there is no point in trying out any subset  
			of  such that  because any such subset would not be minimal w.r.t. .
			Let us, therefore, move on to the unique subset left: .
			Assuming  we have 
thus , which means  such that 
			, is a MH \m\ of .\\			
			It is important to remark that this program has other \cms, e.g, , and , but only the first two are \MH\ \ms\ ---  is obtainable only via the set of hypotheses  which is non-minimal w.r.t.
			 that yields the MH \m\ .
		}
		}
			
			
		\subsection{Properties}
			The minimality of  is not sufficient to ensure minimality of  making its checking explicitly necessary
			if that is so desired.
			Minimality of hypotheses is indeed the common practice is science, not the minimality of their inevitable consequences.
			To the contrary, the more of these the better because it signifies a greater predictive power.
	
			In Logic Programming \m\ minimality is a consequence of definitions: the  operator in definite programs is conducive to
			defining a
			least fixed point, a unique \mm\ semantics; in SM, though there may be more than one \m, minimality turns out to be a property because
			of the stability (and its attendant classical support) requirement; in the WFS, again the existence of a least fixed point operator affords a
			minimal (information) \m.
			In abduction too, minimality of consequences is not a caveat, but rather minimality of hypotheses is, if that even.
			Hence our approach to LP semantics via MHS is novel indeed, and insisting instead on positive hypotheses establishes an improved and
			more general link to abduction and argumentation \cite{Pereira:2007fv,lmp:arg07}.
			\mytheo{At least one Minimal Hypotheses \m\ of  complies with the \WFM}{ExistsMH>=WFM}{
				Let  be an NLP.
				Then, there is at least one Minimal Hypotheses \m\  of  such that 
				 and .
			}{
				If  or equivalently, , then  is a MH \m\
				of  given that  because
				.
				On the other hand, if , then there is at least one non-empty set-inclusion minimal set of hypotheses 
				such that .
				The corresponding  is, by definition, a MH \m\ of  which is guaranteed to comply with 
				and .
}
			\mytheo{Minimal Hypotheses semantics guarantees \m\ existence}{MHModelExistence}{
				Let  be an NLP.
				There is always, at least, one Minimal Hypotheses \m\ of .
			}{
				It is trivial to see that one can always find a set  such that
				 --- in the extreme case, .
				From such  one can always select a minimal subset  such that
				 still holds.
			}
			\subsection{Relevance}
				\mytheo{Minimal Hypotheses semantics enjoys Relevance}{MHRelevance}{
					Let  be an NLP.
					Then, by definition \ref{def:relevance}, it holds that
					
				}{
					:
					Assume .
					Now we need to prove \\.
					Assume some ; now we show that assuming  leads to an absurdity.
					Since  is a \twov\ complete \m\ of  we know that  hence, if 
					, then necessarily .
					Since , by theorem \ref{theo:MHModelExistence} we know that there is some \m\  of 
					such that , and thus  which contradicts the initial assumption that 
					.
We conclude  cannot hold, i.e.,  must hold.
					Since  hold for every \m\  of , then  must hold for every \m\  of .
					
					:
					Assume .
					Now we need to prove \\.
					Let us write  as an abbreviation of .
					We have therefore .
					Let us now take .
					We know that every NLP as an MH \m, hence every MH \m\  of  is such that .
					Let  denote the Hypotheses set of  --- i.e.,
					, with  or 
					non-empty set-inclusion minimal, as per definition \ref{def:MHm}.
					If  then 
					 is an MH \m\ of  with  and, necessarily, 
					.
		
					If  then, knowing that every program has a MH \m, we can
					always find an MH \m\  of , with , where 
					.
					Such  is thus  where
					, which means  is a MH \m\ of  with .
					Since every \m\  of  is such that , then every \m\  of  must also be such that .
				}
			\subsection{Cumulativity}
				MH semantics enjoys Cumulativity thus allowing for lemma storing techniques to be used during
				computation of answers to queries.
				\mytheo{Minimal Hypotheses semantics enjoys Cumulativity}{MHCumulativity}{
					Let  be an NLP.
					Then
					\vspace{-3mm}
					\begin{center}
						\\
						
					\end{center}
				}{
					Assume .
					
					:
					Assume .
					Since every MH \m\  contains  it follows that all such  are also MH \ms\ of .
					Since we assumed  as well, and we know that  is a MH \m\ of  we conclude  is also in those MH \ms\
					 of .
					By adding  as a fact we have necessarily  which means that there cannot be more MH \ms\
					for  than for .
					Since we already know that for every MH \m\  of ,  is also a MH \m\ of  we must conclude that
					 such that .
					Since  we necessarily conclude
					.
					
					:
					Assume .
					Since the MH semantics is relevant (theorem \ref{theo:MHRelevance}) if  does not depend on  then adding  as a fact to 
					 or not has no impact on 's truth-value, and if  then  as well.
					If  does depend on , which is true in every MH \m\  of , then either 
					1)  depends positively on , and in this case since  then  as well; or
					2)  depends negatively on , and in this case the lack of  as a fact in  can only contribute, if at all, to make  true in
						 as well.	
					Then we conclude .		
				}	
			\subsection{Complexity}
				The complexity issues usually relate to a particular set of tasks, namely: 
				1) knowing if the program has a \m; 
				2) if it has any \m\ entailing some set of ground literals (a query); 
				3) if all \ms\ entail a set of literals.
				In the case of MH semantics, the answer to the first question is an immediate ``yes'' because MH semantics guarantees \m\ existence
				for NLPs; the second and third questions correspond (respectively) to Brave and Cautious Reasoning, which we now analyse.
				\subsubsection{Brave Reasoning}
					The complexity of the Brave Reasoning task with MH semantics, i.e., finding an MH \m\ satisfying some particular set of literals is -complete.

					\mytheo{Brave Reasoning with MH semantics is -complete}{MHBraveReasoningIsSigma2P}{
						Let  be an NLP, and  a set of literals, or \emph{query}.
						Finding an MH \m\ such that  is a -complete task.
					}{
						To show that finding a MH \m\  is -complete, note that a nondeterministic Turing machine with access to an NP-complete oracle can solve the problem as follows: nondeterministically guess a set  of hypotheses (i.e., a subset of ). It remains to check if  is empty or non-empty minimal such that  and . Checking that  can be done in polynomial time (because computing  can be done in polynomial time \cite{DBLP:journals/tplp/BrassDFZ01} for whichever ), and checking  is empty or non-empty minimal requires a nondeterministic guess of a strict subset  of  and then a polynomial check if .
}
										
				\subsubsection{Cautious Reasoning}
					Conversely, the Cautious Reasoning, i.e., guaranteeing that every MH \m\ satisfies some particular set of literals, is -complete.
\mytheo{Cautious Reasoning with MH semantics is -complete}{MHCautiousReasoningIsPi2P}{
						Let  be an NLP, and  a set of literals, or \emph{query}.
						Guaranteeing that all MH \ms\ are such that  is a -complete task.
					}{
						Cautious Reasoning is the complement of Brave Reasoning, and since the latter is -complete
						(theorem \ref{theo:MHBraveReasoningIsSigma2P}), the former must necessarily be -complete.
					}					

		The set of hypotheses  is obtained from  which identifies rules that depend on themselves.
		The hypotheses are the atoms of DNLs of , i.e., the ``atoms of \emph{not}s in loop''.
		A \MH\ \m\ is then obtained from a minimal set of these hypotheses sufficient to determine the \twov\ truth-value of every literal in the program.
		The MH semantics imposes no ordering or preference between hypotheses --- only their set-inclusion minimality.
		For this reason, we can think of the choosing of a set of hypotheses yielding a MH \m\ as finding a minimal solution to a disjunction problem, where the disjuncts are the hypotheses.
		In this sense, it is therefore understandable that the complexity of the reasoning tasks with MH semantics is in line with that of, e.g., reasoning tasks with SM semantics with Disjunctive \LPs, i.e, -complete and -complete.

					\amp{
					In abductive reasoning (as well as in Belief Revision) one does not always require minimal solutions.
					Likewise, taking a hypotheses assumption based semantic approach, like the one of MH, one may not require minimality of assumed
					hypotheses.
					In such case, we would be under a non-Minimal Hypotheses semantics, and the complexity classes of the corresponding reasoning
					task would be one level down in the Polynomial Hierarchy relatively to the MH semantics, i.e., Brave Reasoning with a non-Minimal
					Hypotheses semantics would be NP-complete, and Cautious Reasoning would be coNP-complete.
					We leave the exploration of such possibilities for future work.
					}


		
		\subsection{Comparisons}
			As we have seen all \sms\ are MH \ms.
			Since MH \ms\ are always guaranteed to exist for every NLP (cf. theorem \ref{theo:MHModelExistence}) and SMs are not, it follows
			immediately that the Minimal Hypotheses semantics is a strict \m\ conservative generalization of the \SMss.
			The MH \ms\ that are \sms\ are exactly those in which all rules are classically supported.
			With this criterion one can conclude whether some program does not have any \sms.
			For \NLPs, the \SMss\ coincides with the Answer-Set semantics (which is a generalization of SMs to \ELPs), where the latter is known 
			(cf. \cite{LPsWithCNeg}) to correspond to Reiter's default logic.
			Hence, all Reiter's default extensions have a corresponding Minimal Hypotheses \m.
			Also, since Moore's expansions of an autoepistemic theory \cite{2781} are known to have a one-to-one correspondence with the \sms\ of
			the NLP version of the theory, we conclude that for every such expansion there is a matching Minimal Hypotheses \m\ for the same NLP.
			
			\amp{
			Disjunctive Logic Programs (DisjLPs --- allowing for disjunctions in the heads of rules) can be syntactically transformed into NLPs by
			applying the Shifting Rule presented in \cite{Dix96reducingdisjunctive} in all possible ways.
			By non-deterministically applying such transformation in all possible ways, several SCCs of rules may appear in the resulting NLP that
			were not present in the original DisjLP --- assigning a meaning to every such SCC is a distinctive feature of MH semantics, unlike other
			semantics such as the SMs.
			This way, the MH semantics can be defined for DisjLPs as well: the MH \ms\ of a DisjLP are the MH \ms\ of the NLP resulting from the
			transformation via Shifting Rule.
			}
			
			\amp{
			There are other kinds of disjunction, like the one in logic programs with ordered disjunction (LPOD) \cite{Brewka02logicprogramming}.
			These employ ``\emph{a new connective called ordered disjunction.
			The new connective allows to represent alternative, ranked options for problem solutions in the heads of rules}''.
			As the author of \cite{Brewka02logicprogramming} says ``the semantics of logic programs with ordered disjunction is based on a
			preference relation on answer sets.''
			This is different from the semantics assigned by MH since the latter includes no ordering, nor preferences, in the assumed minimal
			sets of hypotheses.
			E.g., in example \ref{ex:vacation} there is no notion of preference or ordering amongst candidate \ms\ --- LPODs would not be the 
			appropriate formalism for such cases.
			We leave for future work a thorough comparison of these approaches, namely comparing the semantics of LPODs against the MH \ms\ of
			LPODs transformed into NLPs (via the Shifting Rule).
			}
			
			\amp{
			The motivation for \cite{Witteveen:1995} is similar to our own --- to assign a semantics to \emph{every} NLP --- however their approach
			is different from ours in the sense that the methods in \cite{Witteveen:1995} resort to contrapositive rules allowing any positive literal in
			the head to be shifted (by negating it) to the body or any negative literal in the body to be shifted to the head (by making it positive).
			This approach considers each rule as a disjunction making no distinction between such literals occurring in the rule, whether or not they
			are in loop with the head of the rule.
			This permits the shifting operations in \cite{Witteveen:1995} to create support for atoms that have no rules in the original program.
			E.g.
			\myex{Nearly-\SMs\ vs MH \ms}{nearlySMsVsMHms}{
				Take the program
				
				\myprog{
					a&\<&\dnot b\\
					b&\<&\dnot c\\
					c&\<&\dnot a,\dnot x
				}
				According to the shifting operations in \cite{Witteveen:1995} this program could be transformed into
				
				\myprog{
					b&\<&\dnot a\\
					b&\<&\dnot c\\
					x&\<&\dnot a,\dnot c
				}
				by
				shifting  and  in the first rule, and 
				shifting the  to the head (becoming positive ) and  to the body (becoming negative ) of the third rule
				thus allowing for  (which is a \sm\ of ) to be a \emph{nearly \sm} of .
				In this sense the approach of \cite{Witteveen:1995} allows for the violation of the Closed-World Assumption.
				This does not happen with our approach: 
 is not a \MHm\ simply because since  has no rules in  it cannot be true in any MH \m\ ---
				 is not a member of  (cf. def. \ref{def:hyps}).
			}			
			}			
			
			As shown in theorem \ref{theo:ExistsMH>=WFM}, at least one MH \m\ of a program complies with its \wfm, although not necessarily all
			MH \ms\ do.
			E.g., the program in Ex. \ref{ex:LRem(P)vsRem(P)} has the two MH \ms\  and 
			, whereas the imposes , , and
			.
			This is due to the set of Hypotheses  of  being taken from  (based on the layered
			support notion) instead of being taken from  (based on the classical notion of support).
			
			Not all Minimal Hypotheses \ms\ are \MMs\ of a program.
			The rationale behind MH semantics is minimality of hypotheses, but not necessarily minimality of consequences, the latter being
			enforceable, if so desired, as an additional requirement, although at the expense of increased complexity.
						
			The relation between \lps\ and argumentation systems has been considered for a long time now
			(\cite{dung95acceptability}	
amongst many others)
			and we have also taken steps to understand and further that relationship
			\cite{Pereira:2007fv,lmp:arg07,lmp_amp_oppositional_concepts_chapter}.
			Dung's Preferred Extensions \cite{dung95acceptability} are maximal sets of negative hypotheses yielding consistent \ms.
			Preferred Extensions, however, these are not guaranteed to always yield \twov\ complete \ms.
			Our previous approaches \cite{Pereira:2007fv,lmp:arg07} to argumentation have already addressed the issue of \twov\ \m\ existence
			guarantee, and the MH semantics also solves that problem by virtue of positive, instead of negative, hypotheses assumption.
					


	\section{Conclusions and Future Work}
		Taking a positive hypotheses assumption approach we defined the \twov\ \MHs\ for NLPs that guarantees \m\ existence, enjoys relevance
		and cumulativity, and is also a \m\ conservative generalization of the SM semantics.
		Also, by adopting positive hypotheses, we not only generalized the argumentation based approach of \cite{dung95acceptability}, but the
		resulting MH semantics lends itself naturally to abductive reasoning, it being understood as hypothesizing plausible reasons sufficient for
		justifying given observations or supporting desired goals.
		We also defined the layered support notion which generalizes the classical one by recognizing the special role of loops.
		
		For query answering, the MH semantics provides mainly three advantages over the SMs:
		1) by enjoying Relevance top-down query-solving is possible, thereby circumventing whole \m\ computation (and grounding) which is
		unavoidable with SMs;
		2) by considering only the relevant sub-part of the program when answering a query it is possible to enact grounding of only those rules, if
		grounding is really desired, whereas with SM semantics whole program grounding is, once again, inevitable --- grounding is known to be a
		major source of computational time consumption; MH semantics, by enjoying Relevance, permits curbing this task to the minimum sufficient
		to answer a query;
		3) by enjoying Cumulativity, as soon as the truth-value of a literal is determined in a branch for the top query it can be stored in a table and
		its value used to speed up the computations of other branches within the same top query.
		
		Goal-driven abductive reasoning is elegantly modelled by top-down abductive-query-solving. 
		By taking a hypotheses assumption approach, enjoying Relevance, MH semantics caters well for this convenient problem representation and
		reasoning category.
	
		Many applications have been developed using the \SM/Answer-set semantics as the underlying platform.
		These generally tend to be focused on solving problems that require complete knowledge, such as search problems where all the knowledge
		represented is relevant to the solutions.
		However, as \KBs\ increase in size and complexity, and as merging and updating of KBs becomes more and more common, e.g. for
		Semantic Web applications, \cite{MKNFpadl10},
		partial knowledge problem solving importance grows, as the need to ensure overall
		consistency of the merged/updated KBs.
		
		The \MHs\ is intended to, and can be used in
		\emph{all} the applications where the \SMs/\ASs\ semantics are themselves used to model KRR and search problems,
		\emph{plus} all applications where query answering (both under a credulous mode of reasoning and under a skeptical one) is intented, 
		\emph{plus} all applications where abductive reasoning is needed.
		The MH semantics aims to be a sound theoretical platform for \twov\ (possibly abductive) reasoning with \lps.
	
		Much work still remains to be done that can be rooted in this platform contribution.
		The general topics of using non-normal \lps\ (allowing for negation, default and/or explicit, in the heads of rules) for Belief Revision, Updates, Preferences, etc., are \emph{per se} orthogonal to the semantics issue, and therefore, all these subjects can now be addressed with \MHs\ as the
		underlying platform.
Importantly, MH can guarantee the liveness of updated and self-updating LP programs such as those of EVOLP \cite{EVOLP} and related
		applications.
The \MHs\ still has to be thoroughly compared with \RSMs\ \cite{PP05}, P\SMs\ \cite{DBLP:conf/micai/OsorioN07}, and other related
		semantics.
								
		In summary, we have provided a fresh platform on which to re-examine ever present issues in Logic Programming and its uses, which
		purports to provide a natural continuation and improvement of LP development.

	\bibliographystyle{plain}
	
	\bibliography{bibliography}
\end{document}
