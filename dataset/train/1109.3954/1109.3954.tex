\documentclass[review]{elsarticle}
\usepackage{graphicx}
\pdfoutput=1

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\newcommand{\Oh}[1]
    {\ensuremath{\mathcal{O}\!\left( {#1} \right)}}
\newcommand{\occ}
    {\ensuremath{\mathrm{occ}}}

\begin{document}

\begin{frontmatter}

\title{A Faster Grammar-Based Self-Index}

\author[aalto]{Travis Gagie}
\author[mpi]{Pawe\l\ Gawrychowski}
\author[helsinki]{Juha K\"arkk\"ainen}
\author[chile]{\\Yakov Nekrich}
\author[helsinki]{Simon J. Puglisi}
\address[aalto]{Aalto University}
\address[mpi]{Max Planck Institute}
\address[helsinki]{University of Helsinki}
\address[chile]{University of Chile}

\begin{abstract}
To store and search genomic databases efficiently, researchers have recently started building compressed self-indexes based on grammars.  In this paper we show how, given a straight-line program with  rules for a string  whose LZ77 parse consists of  phrases, we can store a self-index for  in  space such that, given a pattern , we can list the  occurrences of  in  in  time.  If the straight-line program is balanced and we accept a small probability of building a faulty index, then we can reduce the  term to .  All previous self-indexes are larger or slower in the worst case.
\end{abstract}

\begin{keyword}
compressed self-indexes \sep grammar-based compression \sep Lempel-Ziv compression
\end{keyword}

\end{frontmatter}

\section{Introduction} \label{sec:intro}

With the advance of DNA-sequencing technologies comes the problem of how to store many individuals' genomes compactly but such that we can search them quickly.  Any two human genomes are 99.9\% the same, but compressed self-indexes based on compressed suffix arrays, the Burrows-Wheeler Transform or LZ78 (see~\cite{NM07} for a survey) do not take full advantage of this similarity.  Researchers have recently started building self-indexes based on context-free grammars (CFGs) and LZ77~\cite{ZL77}, which better compress highly repetitive strings.  A compressed self-index stores a string  in compressed form such that, first, given a position  and a length , we can quickly extract  and, second, given a pattern , we can quickly list the  occurrences of  in .

Claude and Navarro~\cite{CN11a} gave the first compressed self-index based on grammars or, more precisely, straight-line programs (SLPs).  An SLP is a context-free grammar (CFG) in Chomsky normal form that generates only one string.  Figure~\ref{fig:slp} shows an example.  They showed how, given an SLP with  rules for a string , we can build a self-index that takes  space and supports extraction in  time and pattern matching in  time, respectively, where  is the height of the parse tree.  Our model is the word RAM with -bit words; except where stated otherwise, by  we mean  and we measure space in words.  The same authors~\cite{CN11b} recently gave a self-index that has better time bounds and can be based on any CFG generating  and only .  Specifically, they showed how, given such a CFG with  distinct terminal and non-terminal symbols and  symbols on the righthand sides of the rules, we can build a self-index that takes  space and supports extraction in  time and pattern matching in  time.

\begin{figure}[t]
\begin{center}
\resizebox{60ex}{!}
{\begin{tabular}{l@{\hspace{5ex}}r}
\parbox{20ex}
{\vspace{-25ex}
} &
\includegraphics[width=50ex]{fibonacci_slp.pdf}
\end{tabular}}
\caption{A balanced SLP for  (left) and the corresponding parse tree (right).}
\label{fig:slp}
\end{center}
\end{figure}

If we are not concerned about the constant coefficient in the space bound, we can improve Claude and Navarro's time bound for extraction.  Calculation shows that .  Given a CFG generating  and only  with  symbols on the righthand sides of the rules, we can turn it into an SLP with  rules (although the number of distinct symbols and the height of the parse tree can each increase by a factor of ).  Bille et al.~\cite{BLRSSW11} showed how we can store such an SLP in  space and support extraction in  time.  Combining their result with Claude and Navarro's improved one, we obtain an index that still takes  space and  time for pattern matching but only  time for extraction.

In this paper we show how, given an SLP for  with  rules, we can build a self-index that takes  space, where  is the number of phrases in the LZ77 parse of , and supports extraction in  time and pattern matching in  time.  Therefore, by the observations above, given a CFG generating  and only  with  symbols on the righthand sides of the rules, we can build an index with the same time bounds that takes  space.

If we are given a balanced SLP for  --- i.e., one for which the parse tree is height- or weight-balanced~\cite{CLRS01} --- and we accept a small probability of building a faulty index, then we do not need Bille et al.'s result to extract in  time and we can reduce the time bound for pattern matching to .  Rytter~\cite{Ryt03} showed how we can build such an SLP with  rules, and proved that no SLP for  has fewer than  rules.  His algorithm still has the best known approximation ratio even when the SLP need not be balanced, but performs badly in practice.  Recently, however, Maruyama, Sakamoto and Takeda~\cite{MST12} gave a practical online algorithm that produces a balanced SLP with  rules.  In other words, requiring the SLP to be balanced is a reasonable restriction both in theory and in practice.

Table~\ref{tab:bounds} summarizes Claude and Navarro's bounds and our own.  Since all the self-indexes mentioned can be made to support extraction in  time without increasing their space usage by more than a constant factor, we do not include this bound in the table.  As noted above, given a CFG generating  and only  with  symbols on the righthand sides of the rules, we can turn it into an SLP with  rules, so our first result is as general as Claude and Navarro's; the  in the second row of the table can be replaced by .  By Rytter's result, we can assume .

\begin{table}[t]
\begin{center}
\caption{Claude and Navarro's bounds and our own.  In the first row,  is the number of symbols on the righthand sides of the rules in a given CFG generating  and only , and  is the number of distinct terminal and non-terminal symbols in that CFG.  In the second and third rows,  is the number of rules in a given SLP for  --- which must be balanced in the third row --- and  is the number of phrases in the LZ77 parse of .}
\label{tab:bounds}
\vspace{2ex}
\begin{tabular}{c@{\hspace{2ex}}|@{\hspace{2ex}}c@{\hspace{4ex}}c}
source & space & search time\\
\hline &&\2ex]
Theorem~\ref{thm:unbalanced} &
 &
 \S [1..i - 1]S [i..j - 1]S [i..m]S [i..j]j = 1S [i..j - 1]abaababaabaabS [i..j]S [i'..i' + m - 1]i, j)i', i' + m - 1)\Oh{z \log n} + o (n)d \leq zz \geq \log nS = abaababaabaab lexicographically less than any in the alphabet, and each reversed phrase as ending with another special character .  Figure~\ref{fig:Patricia} shows the Patricia trees for this example.
\begin{center}
\begin{tabular}{rl@{\hspace{3ex}}rl@{\hspace{8ex}}rl@{\hspace{3ex}}rl}
1) &  & 6) &  & 1) & \epsilonaabaab\ & 2) & aabaa\#b\#b\\\
6) &  & 4) &  & 6) &  & 3) & ziSj\Oh{z}\Oh{(p + 1) \log z}p\Oh{p + 1}p\Oh{m^2}\Oh{m^2 d}\Oh{(m + \occ) \log z}PSPP\Oh{\occ}\Oh{m^2 d + (m + \occ) \log z}\Oh{z \log \log z}\Oh{m^2 d + m \log \log z + occ \log \log n}\Oh{m \log \log z}\Oh{m^2}\Oh{m \log \log z}\Oh{z \log \log z}\Oh{m^2 d + occ \log \log n}Sr\Oh{r + z \log \log z}\Oh{\ell + \log n}\Oh{m^2 + m \log n + \occ \log \log n}\Oh{m \log n}Srt\Oh{\ell + \log n}\ellt\Oh{t}SS'\Oh{z}SSS'SS'S'S'S'\Oh{z}S'\Oh{\log z + \log \log n}S'\Oh{r + t + z \log \log n}S\Oh{\ell + \log n}t\Oh{\ell + \log z}S\Oh{r + z \log \log n}\Oh{\ell + \log n}\Oh{m^2 + m \log z + \occ \log \log n}\Oh{m \log z}S'iS'L\Oh{1}S'\Oh{\ell + \log L}\Oh{\ell + \log L}uiTS'vuviTwuwS'vw\Oh{\log L}uvivw\Oh{\log r'}\Oh{\log L}vi\Oh{1}TviTTvwTvi\Oh{\log L}\Oh{\log \ell}ivwvi\Oh{\ell + \log L}\Oh{\ell + \log L}\elliS'\Oh{\ell + \log L}\Oh{\ell + \log n'}\Oh{1}i\Oh{\ell + \log \log n'}\Oh{1}\Oh{\ell + \log \log \log n'}\Oh{\log^* n'}\Oh{\ell}S'\Oh{\log^* n'}\ell\Oh{\ell}S'SSSrtS\Oh{r + t + z \log \log n}\ell\Oh{\ell}St\Oh{r + z \log \log n}\Oh{\ell + \log n}\Oh{\ell}rzS\Oh{r + z \log \log n}\ell\Oh{\ell + \log n}\occPS\Oh{m^2 + \occ \log \log n}PSPPvwvwmvwP\Oh{m^2 \log \log z}PSPiPSPiPSi\Oh{m \log \log z}imiPS\Oh{m^2}\Oh{m^2 \log \log z}\Oh{m^2 \log \log z}PSS\sigmaqqn^c\Oh{c \log \sigma / n^{c - 1}}\Oh{1}\Oh{1}\Oh{1}uu\Oh{z}Pvvvv\Oh{\log m}\Oh{z}\Oh{\log z}vvv\Oh{1}vwvwvwwwrSiSL\Oh{1}\Oh{\log L}\Oh{\log \ell}\Oh{\log L}\Oh{1}\Oh{\log L}Li\Oh{1}\Oh{\log L}LiSL\ellL\Oh{\log \log n}S\Oh{\log \log n}\ell\Oh{\log \ell}SSrS\Oh{r + z \log \log n}\ell\Oh{\log \ell}\Oh{r + z \log \log n}\Oh{\log m}PP\Oh{m}\Oh{m \log m}i\Oh{\log n + \ell}\Oh{\ell}\Oh{m \log m + m \log \log z + \occ \log \log n}\Oh{m \log m + \occ \log \log n}rzS\Oh{r + z \log \log n}\ell\Oh{\ell + \log n}\occPS\Oh{m \log m + \occ \log \log n}c\Oh{m \log \log z}\Oh{r + z \log \log n}\Oh{m \log m + \occ \log \log n}\Oh{m \log m}\Oh{m \log \log z}\Oh{z \log z}S\Oh{\log \log z}\Oh{z \log \log z}P\Oh{m}vPSPPPSi\Oh{1}PS\Oh{m}PS\Oh{\occ}vP\Oh{m \log m + \occ \log \log z}PS\Oh{r}\occ_1\Oh{(m + \occ_1) \log z}$ time, with reasonable coefficients.  As we have explained, finding all secondary occurrences is relatively easy once we have found all the primary occurrences.

Approximate pattern matching is often more useful than exact pattern matching, especially in bioinformatics.  Fortunately, Russo, Navarro, Oliveira and Morales~\cite{RNOM09} showed how to support practical approximate pattern matching using indexes for exact pattern matching, and we believe most of their techniques are applicable to our self-index.  One potential problem is how to perform backtracking using Patricia trees augmented with Karp-Rabin hashes, without storing or extracting edge labels.  This is because comparing hashes tells us (with high probability) when strings differ, but it does not tell us by how much they differ.  We are currently investigating a new variant of Karp-Rabin hashes by Policriti, Tomescu and Vezzi~\cite{PTV??} that roughly preserves Hamming distance.

Finally, we have shown elsewhere~\cite{GGP11} that supporting extraction from specified positions has applications to, e.g., sequential approximate pattern matching.  In that paper we developed a different data structure to support such extraction, which we have now implemented and found to be faster and more space-efficient than Kreft and Navarro's solutions.  Nevertheless, we expect the solutions we have given here to be even better.

\section*{Acknowledgments}

Many thanks to Djamal Belazzougui, Francisco Claude, Veli M\"akinen, Gonzalo Navarro and Jorma Tarhio, for helpful discussions.

\bibliographystyle{model1-num-names}
\bibliography{faster}

\end{document} 