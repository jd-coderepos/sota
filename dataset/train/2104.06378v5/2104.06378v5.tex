\section{Experiments}\label{sec:experiment}
\subsection{Datasets}
We evaluate \methodname on three question answering datasets: \textit{CommonsenseQA} \cite{talmor2018commonsenseqa}, \textit{OpenBookQA} \cite{obqa}, and \textit{MedQA-USMLE} \cite{jin2021disease}.

\heading{CommonsenseQA} is a 5-way multiple choice QA task that requires reasoning with commonsense knowledge, containing 12,102 questions.
The test set of CommonsenseQA is not publicly available, and model predictions can only be evaluated once every two weeks via the official leaderboard. Hence, we perform main experiments on the {in-house (IH) data splits} used in \citet{lin2019kagnet}, and also report the score of our final system on the {official test} set.

\heading{OpenBookQA} is a 4-way multiple choice QA task that requires reasoning with elementary science knowledge, containing 5,957 questions. We use the official data splits from \citet{mihaylov2018knowledgeable}. 

\heading{MedQA-USMLE} is a 4-way multiple choice QA task that requires biomedical and clinical knowledge. The questions are originally from practice tests for the United States Medical License Exams (USMLE). The dataset contains 12,723 questions. We use the original data splits from \citet{jin2021disease}.


\subsection{Knowledge graphs}
\label{sec:experiment-kg}
For CommonsenseQA and OpenBookQA, we use \textit{ConceptNet} \cite{speer2016conceptnet}, a general-domain knowledge graph, as our structured knowledge source . It has 799,273
nodes and 2,487,810 edges in total. Node embeddings are initialized using the entity embeddings prepared by \citet{feng2020scalable}, which applies pre-trained LMs to all triples in ConceptNet and then obtains a pooled representation for each entity.

For MedQA-USMLE, we use a self-constructed knowledge graph that integrates the Disease Database portion of the Unified Medical Language System (UMLS; \citealp{bodenreider2004unified}) and DrugBank \cite{wishart2018drugbank}. The knowledge graph contains 9,958 nodes and 44,561 edges. Node embeddings are initialized using the pooled representations of the entity name from SapBERT (\citealp{liu2020self}).

Given each QA context (question and answer choice), we retrieve the subgraph  from  following the pre-processing step described in \citet{feng2020scalable}, with hop size . We then prune  to keep the top 200 nodes according to the node relevance score computed in \S \ref{sec:method-contextualization}.
Henceforth, in this section (\S \ref{sec:experiment}) we use the term ``KG'' to refer to .




\subsection{Implementation \& training details}
We set the dimension () and number of layers () of our GNN module, with dropout rate
0.2 applied to each layer \cite{srivastava2014dropout}. 
We train the model with the RAdam \cite{liu2019variance} optimizer using two GPUs (GeForce RTX 2080 Ti), which takes 20 hours.
We set the batch size from \{32, 64, 128, 256\}, learning rate for the LM module from \{5e-6, 1e-5, 2e-5, 3e-5, 5e-5\}, and learning rate for the GNN module from \{2e-4, 5e-4, 1e-3, 2e-3\}. The above hyperparameters are tuned on the development set.


\subsection{Baselines}
\label{sec:experiment-baseline}
\paragraph{Fine-tuned LM.}
To study the role of KGs, we compare with a vanilla fine-tuned LM, which does not use the KG. We use RoBERTa-large \cite{liu2019roberta} for \textit{CommonsenseQA}, and RoBERTa-large and AristoRoBERTa\footnote{OpenBookQA provides an extra corpus of scientific {facts} in a textual form. AristoRoBERTa uses the facts corresponding to each question, prepared by \citet{clark2019f}, as an additional input to the QA context.\vspace{-0mm}} \cite{clark2019f} for \textit{OpenBookQA}.
For \textit{MedQA-USMLE}, we use a state-of-the-art biomedical LM, SapBERT \cite{liu2020self}.


\paragraph{Existing LM+KG models.}
We compare with existing LM+KG methods, which share the same high-level framework as ours but use different modules to reason on the KG in place of \methodname (``yellow box'' in Figure\! \ref{fig:overview}):
(1) Relation Network (RN) \cite{santoro2017simple}, (2) RGCN \cite{schlichtkrull2018modeling}, (3) GconAttn \cite{wang2019improving}, (4) KagNet \cite{lin2019kagnet}, and (5) MHGRN \cite{feng2020scalable}.
(1),(2),(3) are relation-aware GNNs for KGs, and (4),(5) further model paths in KGs. MHGRN is the existing top performance model under this LM+KG framework.
For fair comparison, we use the same LM in all the baselines and our model.
The key differences between \methodname and these are that they do not perform {relevance scoring} or {joint updates} with the QA context (\S \ref{sec:method}).


\subsection{Main results}
\begin{table}[t]
\centering
\scalebox{0.7}{
\begin{tabular}{lcc}
    \toprule  
    {\textbf{Methods}}& \textbf{IHdev-Acc.} (\%) & \textbf{IHtest-Acc.} (\%) \\
    \midrule  
    RoBERTa-large (w/o KG)  & 73.07~(0.45) & 68.69 (0.56) \\
    \midrule
    + {RGCN \scalebox{0.7}{\cite{schlichtkrull2018modeling}}} & 72.69~(0.19) & 68.41~(0.66) \\
    + {GconAttn \scalebox{0.7}{\cite{wang2019improving}}}  & 72.61(0.39) &
    68.59~(0.96)\\
    + {KagNet \scalebox{0.7}{\cite{lin2019kagnet}}}  & 73.47~(0.22) & 69.01~(0.76) \\
    + {RN} \scalebox{0.7}{\cite{santoro2017simple}}  & 74.57~(0.91) & 69.08~(0.21) \\
    + {MHGRN} \scalebox{0.7}{\cite{feng2020scalable}} & 74.45~(0.10)   & {71.11}~(0.81)  \\
    \midrule
    + \methodname (\textbf{Ours}) &  \textbf{76.54}~(0.21)   & \textbf{73.41}~(0.92)  \\
    \bottomrule 
\end{tabular}
}
\vspace{-2mm}
\caption{\textbf{Performance comparison on \textit{Commonsense \!QA} in-house split} (controlled experiments). 
As the official test is hidden, here we report the in-house Dev (IHdev) and Test (IHtest) accuracy, following the data split of \citet{lin2019kagnet}.
}
\label{tab:csqa_main}
\end{table}

 \begin{table}[tb]
\centering
\small
\scalebox{0.9}{
\begin{tabular}{lc}
\toprule
\textbf{Methods}& \textbf{Test}  \\
\midrule
RoBERTa~\cite{liu2019roberta} & 72.1 \\

RoBERTa+FreeLB~\cite{zhu2019freelb} (ensemble) &73.1\\
RoBERTa+HyKAS~\cite{ma2019towards}&73.2\\
RoBERTa+KE (ensemble) & 73.3\\
RoBERTa+KEDGN (ensemble) & 74.4\\
XLNet+GraphReason~\cite{lv2020graph} & 75.3\\
RoBERTa+MHGRN \cite{feng2020scalable} & 75.4  \\
Albert+PG \cite{wang2020connecting} & 75.6  \\
Albert~\cite{lan2019albert} (ensemble) &76.5          \\
UnifiedQA\textsuperscript{*}~\cite{khashabi2020unifiedqa} & \textbf{79.1} \\
\midrule
RoBERTa + \methodname (\textbf{Ours}) & 76.1 \\
\bottomrule
\end{tabular}
}\vspace{-2mm}
\caption{\textbf{Test accuracy on \textit{CommonsenseQA}'s official leaderboard}. The top system, UnifiedQA (11B parameters) is 30x larger than our model.
}
\label{tab:csqa_leaderboard}
\end{table}

 \begin{table}[tb]
\centering
\scalebox{0.7}{
\begin{tabular}{lcc}
\toprule
\textbf{Methods}          & \textbf{RoBERTa-large}     & \textbf{AristoRoBERTa}     \\
\midrule
Fine-tuned LMs (w/o KG)          &  64.80~(2.37)  & 78.40~(1.64)       \\
\midrule
+ RGCN           & 62.45~(1.57)   & 74.60~(2.53)
       \\
+ GconAtten      & 64.75~(1.48)   & 71.80~(1.21)
       \\
+ RN             & 65.20~(1.18)    & 75.35~(1.39)
       \\
+ MHGRN & 66.85~(1.19) & 80.6        \\
\midrule
+ \methodname (\textbf{Ours})       &  \textbf{67.80}~(2.75)   &  \textbf{82.77}~(1.56)    \\

\bottomrule
\end{tabular}
}\vspace{-2mm}
\caption{\textbf{Test accuracy comparison on \textit{OpenBook \!\!QA}} (controlled experiments). Methods with AristoRoBERTa use the textual evidence by \citet{clark2019f} as an additional input to the QA context.}
\label{tab:obqa_main}
\end{table}
 \begin{table}[h]
\centering
\small
\scalebox{0.9}{
\begin{tabular}{lc}
\toprule
\textbf{Methods}         & \textbf{Test}           \\
\midrule
Careful Selection~\cite{banerjee2019careful} & 72.0\\
AristoRoBERTa & 77.8\\
KF + SIR~\cite{banerjee2020knowledge} & 80.0\\
AristoRoBERTa + PG \cite{wang2020connecting}      & 80.2 \\
AristoRoBERTa + MHGRN \cite{feng2020scalable}     & 80.6 \\
Albert + KB & 81.0\\
T5\textsuperscript{*}~\cite{t5} & 83.2\\
UnifiedQA\textsuperscript{*}~\cite{khashabi2020unifiedqa} &          \\
\midrule
AristoRoBERTa + \methodname (\textbf{Ours}) & 82.8\\
\bottomrule
\end{tabular}
}\vspace{-2mm}
\caption{\textbf{Test accuracy on \textit{OpenBookQA} leaderboard}. All listed methods use the provided science facts as an additional input to the language context. The top 2 systems, UnifiedQA (11B params) and T5 (3B params) are  30x and 8x larger than our model.}
\label{tab:obqa_leaderboard}
\end{table} \begin{table}
\centering
\small
\scalebox{0.9}{
\begin{tabular}{lc}
\toprule
\textbf{Methods}          & \textbf{Test}     \\
\midrule
{BERT-base}~ \scalebox{0.9}{\cite{devlin2018bert}}       & 34.3 \\ 
{BioBERT-base}~ \scalebox{0.9}{\cite{lee2020biobert}}     & 34.1 \\ 
{RoBERTa-large}~ \scalebox{0.9}{\cite{liu2019roberta}}    & 35.0 \\ 
{BioBERT-large}~ \scalebox{0.9}{\cite{lee2020biobert}}       & 36.7 \\
{SapBERT}~ \scalebox{0.9}{\cite{liu2020self}}     & 37.2 \\
\midrule
{SapBERT + \methodname}  (\textbf{Ours})  &  \textbf{38.0}  \\
\bottomrule
\end{tabular}
}\vspace{-2mm}
\caption{
\textbf{Test accuracy on \textit{MedQA-USMLE}}.
}
\label{tab:medqa_main}
\end{table} 

Table \ref{tab:csqa_main} and Table \ref{tab:obqa_main} show the results on CommonsenseQA and OpenBookQA, respectively. On both datasets, we observe consistent improvements over fine-tuned LMs and existing LM+KG models, \eg, on CommonsenseQA, +4.7\% over RoBERTa, and +2.3\% over the prior best LM+KG system, MHGRN. 
The boost over MHGRN suggests that \methodname makes a better use of KGs to perform joint reasoning than existing LM+KG methods.

We also achieve competitive results to other systems on the official leaderboards (Table  \ref{tab:csqa_leaderboard} and \ref{tab:obqa_leaderboard}). Notably, the top two systems, T5 \cite{t5} and UnifiedQA \cite{khashabi2020unifiedqa}, are trained with more data and use 8x to 30x more parameters than our model (ours has 360M parameters).
Excluding these and ensemble systems, our model is comparable in size and amount of data to other systems, and achieves the top performance on the two datasets.


Table \ref{tab:medqa_main} shows the result on MedQA-USMLE. QA-GNN outperforms state-of-the-art fine-tuned LMs (\eg, SapBERT). This result suggests that our method is an effective augmentation of LMs and KGs across different domains (\ie, the biomedical domain besides the commonsense domain).



\subsection{Analysis}
\subsubsection{Ablation studies}
\label{sec:experiment-ablation}
\begin{table}[!t]
\hspace{-2mm}
\includegraphics[width=0.51\textwidth]{tbl_ablation_v5.pdf}
    \vspace{-7mm}
    \caption{\textbf{Ablation study} of our model components, using the CommonsenseQA IHdev set.}
\label{tbl:ablation}
\end{table}

Table \ref{tbl:ablation} summarizes the ablation study conducted on each of our model components (\S \ref{sec:method-joint-graph}, \S \ref{sec:method-contextualization}, \S \ref{sec:method-gnn}), using the CommonsenseQA IHdev set.

\paragraph{Graph connection}\!\!\!\!(top left table):~~ The first key component of \methodname is the joint graph that connects the  node (QA context) to QA entity nodes  in the KG (\S \ref{sec:method-joint-graph}). Without these edges, the QA context and KG cannot mutually update their representations, hurting the performance: 76.5\% \!\! 74.8\%, which is close to the previous LM+KG system, MHGRN. 
If we connected  to all the nodes in the KG (not just QA entities), the performance is comparable or drops slightly (-0.16\%).


\paragraph{KG node relevance scoring}\!\!\!\!(top right table):~~
We find the relevance scoring of KG nodes (\S \ref{sec:method-contextualization}) provides a boost: 75.56\%  76.54\%.
As a variant of the relevance scoring in Eq.\! \ref{eq:relevance}, we also experimented with obtaining a \emph{contextual embedding}  for each node  and adding to the node features:
.
However, we find that it does not perform as well (76.31\%), and
using both the relevance score and contextual embedding performs on par with using the score alone, suggesting that the score has a sufficient information in our tasks; hence, our final system simply uses the relevance score.



\paragraph{GNN architecture}\!\!\!\!(bottom tables):~~ We ablate the information of node type, relation, and relevance score from the attention and message computation in the GNN (\S \ref{sec:method-gnn}). The results suggest that all these features improve the model performance. 
For the number of GNN layers, we find  works the best on the dev set. Our intuition is that 5 layers allow various message passing or reasoning patterns between the QA context () and KG, such as ``  3 hops on KG nodes  ''.


\subsubsection{Model interpretability}
\begin{figure}[!t]
\hspace{-1mm}
    \centering \includegraphics[width=0.45\textwidth]{fig_interpret_v3.pdf}
\caption{\textbf{Interpreting \methodname's reasoning process} by analyzing the node-to-node attention weights induced by the GNN. Darker and thicker edges indicate higher attention weights.
    }
\label{fig:interpret}
\end{figure}

We aim to interpret \methodname's reasoning process by analyzing the node-to-node attention weights induced by the GNN. 
Figure \ref{fig:interpret} shows two examples. In (a), we perform Best First Search (BFS) on the working graph to trace high attention weights from the QA context node (\textbf{Z}; purple) to \textbf{Q}uestion entity nodes (blue) to \textbf{O}ther (gray) or \textbf{A}nswer choice entity nodes (orange), which reveals that the QA context  attends to ``elevator'' and ``basement'' in the KG, ``elevator'' and ``basement'' both attend strongly to ``building'', and ``building'' attends to ``office building'', which is our final answer. 
In (b), we use BFS to trace attention weights from two directions: 
\textbf{Z}  \textbf{Q}  \textbf{O} and \textbf{Z}  \textbf{A}  \textbf{O}, which reveals concepts (``sea'' and ``ocean'') in the KG that are not necessarily mentioned in the QA context but bridge the reasoning between the question entity (``crab'') and answer choice entity (``salt water'').
While prior KG reasoning models \cite{lin2019kagnet,feng2020scalable} enumerate individual paths in the KG for model interpretation, \methodname is not specific to paths, and helps to find more general reasoning structures (\eg, a KG subgraph with multiple anchor nodes as in example (a)).




\begin{figure*}[!th]
    \vspace{-5mm}
    \hspace{-1mm}
    \centering \includegraphics[width=0.92\textwidth]{fig_structure_reason_v3.pdf}
\vspace{-1mm}
    \caption{\textbf{Analysis of \methodname's behavior for structured reasoning}. Given an original question (left), we modify its negation (middle) or topic entity (right): we find that \methodname adapts attention weights and final predictions accordingly, suggesting its capability to handle structured reasoning.
    }
    \vspace{-2mm}
\label{fig:structure_reason}
\end{figure*}

\begin{table*}[!th]
    \vspace{-0mm}
    \hspace{-1mm}
    \centering \includegraphics[width=0.92\textwidth]{tbl_structure_reason_v2.pdf}
    \vspace{-2mm}
    \caption{\textbf{Case study of structured reasoning}, comparing predictions by RoBERTa and our model (RoBERTa + \methodname). Our model correctly handles changes in negation and topic entities.
    }
\label{tbl:structure_reason}
\end{table*}


\subsubsection{Structured reasoning}
\label{sec:experiment-structured-reasoning}
Structured reasoning, \eg, precise handling of negation or entity substitution (\eg, ``hair''  ``art'' in Figure \!\ref{fig:structure_reason}b) in question, is crucial for making robust predictions. Here we analyze \methodname's ability to perform structured reasoning and compare with baselines (fine-tuned LMs and existing LM+KG models).

\paragraph{Quantitative analysis.}
\begin{table}[tb]
\centering
\scalebox{0.7}{
\begin{tabular}{lcc}
\toprule
\multirow{2}{*}{\textbf{Methods}  }        & \textbf{IHtest-Acc.}     & \textbf{IHtest-Acc.}     \\
& (Overall) & \scalebox{0.93}[1]{(Question w/ \textbf{negation})}\! \\
\midrule
RoBERTa-large (w/o KG)          &  68.7 & 54.2     \\
\midrule
+ KagNet &  69.0 (+0.3) &
54.2 (+0.0)
        \\
+ MHGRN        & 71.1 (+2.4) & 54.8 (+0.6) \\
\midrule
+ \methodname (\textbf{Ours})       &  {73.4 (+4.7)}   &  \textbf{58.8 (+4.6)}
     \-3pt] \hspace{9pt}between Z and KG) \end{tabular}   &   71.5 (+2.8) &
55.1 (+0.9)
     \\
\bottomrule
\end{tabular}\vspace{-3mm}
}
\caption{Performance on \textbf{questions with negation} in \textit{CommonsenseQA}.  () shows the difference with RoBERTa.
Existing LM+KG methods (KagNet, MHGRN) provide limited improvements over RoBERTa (+0.6\%); \methodname exhibits a bigger boost (+4.6\%), suggesting its strength in structured reasoning. 
}
\label{tab:negation_result}
\end{table}
 Table \ref{tab:negation_result} compares model performance on questions containing negation words (\eg, no, not, nothing, unlikely), taken from the CommonsenseQA IHtest set.
We find that previous LM+KG models (KagNet, MHGRN) provide limited improvements over RoBERTa on questions with negation (+0.6\%); whereas \methodname exhibits a bigger boost (+4.6\%), suggesting its strength in structured reasoning.
We hypothesize that \methodname's joint updates of the representations of the QA context and KG (during GNN message passing) allows the model to integrate semantic nuances expressed in language.
To further study this hypothesis, we remove the connections between  and KG nodes from our \methodname (Table \ref{tab:negation_result} bottom): now the performance on negation becomes close to the prior work, MHGRN, suggesting that the joint message passing helps for performing structured reasoning.





\paragraph{Qualitative analysis.}


Figure \ref{fig:structure_reason} shows a case study to analyze our model's behavior for structured reasoning. 
The question on the left contains negation ``\textbf{not} used for hair'', and the correct answer is ``B. art supply''.
We observe that in the 1st layer of QA-GNN, the attention from  to question entities (``hair'', ``round brush'') is diffuse. After multiples rounds of message passing on the working graph,  attends strongly to ``round brush'' in the final layer of the GNN, but weakly to the negated entity ``hair''. The model correctly predicts the answer ``B. art supply''.
Next, given the original question on the left, we (a) drop the negation or (b) modify the topic entity (``hair''  ``art'').
In (a),  now attends strongly to ``hair'', which is not negated anymore. The model predicts the correct answer ``A. hair brush''.
In (b), we observe that QA-GNN recognizes the same structure as the original question (with only the entity swapped):  attends weakly to the negated entity (``art'') like before, and the model correctly predicts ``A. hair brush'' over ``B. art supply''.


Table \ref{tbl:structure_reason} shows additional examples, where we compare QA-GNN's predictions with the LM baseline (RoBERTa).
We observe that RoBERTa tends to make the same prediction despite the modifications we make to the original questions (\eg, drop/insert negation, change an entity); on the other hand, \methodname adapts predictions to the modifications correctly (except for double negation in the table bottom, which is a future work). 




\subsubsection{Effect of KG node relevance scoring}
\begin{table}[tb]
\centering
\scalebox{0.7}{
\begin{tabular}{lcc}
\toprule
\multirow{2}{*}{\vspace{-11pt}\textbf{Methods}  }        & \textbf{IHtest-Acc.}     & \textbf{IHtest-Acc.}     \\
& \begin{tabular}{@{}c@{}}(Question w/ \-2pt] \hspace{6pt}\scalebox{0.9}{}10 entities) \end{tabular}\\
\midrule
RoBERTa-large (w/o KG)          &   68.4 &
70.0\\
\midrule
+ MHGRN        &  71.5& 70.1
\\
\midrule
\begin{tabular}{@{}l@{}}+ \methodname (w/o node\5pt]
\begin{tabular}{@{}l@{}}+ \methodname (w/ node\-3pt] \hspace{9pt}relevance score; \textbf{final system})\end{tabular}\!\!   &    73.4 (+1.9) &
\textbf{73.5 (+3.4)}\\
\bottomrule
\end{tabular}\vspace{-3mm}
}
\caption{Performance on \textbf{questions with fewer \!/\! more entities} in \textit{CommonsenseQA}.  () shows the difference with MHGRN (LM+KG baseline). KG node relevance scoring (\S 3.2) boosts the performance on questions containing more entities (i.e. larger retrieved KG). 
}
\label{tab:kg_size_result}
\end{table} We find that KG node relevance scoring (\S \ref{sec:method-contextualization}) is helpful when the retrieved KG () is large.
Table \ref{tab:kg_size_result} shows model performance on questions containing fewer (10) or more (>10) entities in the CommonsenseQA IHtest set (on average, the former and latter result in 90 and 160 nodes in , respectively).
Existing LM+KG models such as MHGRN achieve limited performance on questions with more entities due to the size and noisiness of retrieved KGs: 70.1\% accuracy vs 71.5\% accuracy on questions with fewer entities. 
KG node relevance scoring mitigates this bottleneck, reducing the accuracy discrepancy: 73.5\% and 73.4\% accuracy on questions with more \!/\! fewer entities, respectively.


