\documentclass{llncs}

\usepackage{graphicx}
\usepackage{psfrag}

\pagestyle{plain}

\usepackage{times}
\usepackage{enumerate}
\usepackage{bm}


\usepackage{amssymb}

\newcommand{\OPT}{\text{{\sc opt}}}
\newcommand{\opt}{\OPT}
\newcommand{\MPS}{{\rm MPS}} 
\newcommand{\MPSO}{{\rm MPS}} 
\newcommand{\eps}{\varepsilon}

\usepackage{amsmath,amsfonts,amssymb}
\usepackage{savesym}
\savesymbol{vec}
\usepackage{MnSymbol}
\usepackage{amsfonts}


\begin{document}

\title{Online Makespan Minimization with Parallel Schedules\vspace*{-0.3cm}}
\author{Susanne Albers \and Matthias Hellwig}
\institute{{Department of Computer Science, Humboldt-Universit\"at zu Berlin}
\email{\{albers,mhellwig\}@informatik.hu-berlin.de}\vspace*{-0.5cm}}

\maketitle

\begin{abstract}

Online makespan minimization is a classical problem in which a sequence of jobs 
has to be scheduled on  identical parallel machines so as to minimize the maximum completion time of any job.
In this paper we investigate the problem with an essentially new model of resource augmentation.
More specifically, an online algorithm is allowed to build several schedules in parallel while processing . 
At the end of the scheduling process the best schedule is selected. This model can be viewed as 
providing an online algorithm with extra space, which is invested to maintain multiple solutions. The 
setting is of particular interest in parallel processing environments where each processor can maintain a 
single or a small set of solutions. 

As a main result we develop a -competitive algorithm, for any , that uses a 
constant number of schedules. The constant is . We also give 
a -competitive algorithm, 
for any , that builds a polynomial number of  
schedules. This value depends on  but is independent of the input . The performance guarantees
are nearly best possible. We show that any algorithm that achieves a competitiveness smaller than 
must construct  schedules. Our algorithms make use of novel guessing schemes that (1)~predict
the optimum makespan of a job sequence  to within a factor of  and (2)~guess the job processing 
times and their frequencies in . In~(2) we have to sparsify the universe of all guesses so
as to reduce the number of schedules to a constant.

The competitive ratios achieved using parallel schedules are considerably smaller than
those in the standard problem without resource augmentation. Furthermore they are at least as good and
in most cases better than the ratios obtained with other means of resource augmentation for makespan
minimization.

\end{abstract}

\section{Introduction}
Makespan minimization is a fundamental and extensively studied problem in scheduling theory. Consider a
sequence of jobs  that has to be scheduled on  identical parallel
machines. Each job  is specified by a processing time , . Preemption of 
jobs is not allowed. The goal is to minimize the makespan, i.\,e.\ the maximum completion time of
any job in the constructed schedule. We focus on the online version of the problem where the jobs
of  arrive one by one. Each incoming job  has to be assigned immediately 
to one of the machines without knowledge of any future jobs , .

Online algorithms for makespan minimization have been studied since the 1960s. In an early paper
Graham~\cite{G} showed that the famous {\em List\/} scheduling algorithm is -competitive.
The best online strategy currently known achieves a competitiveness of about 1.92. Makespan minimization 
has also been studied with various types of {\em resource augmentation\/}, giving an online algorithm  
additional information or power while processing . The following scenarios were considered.
(1)~An online algorithm knows the optimum makespan or the sum of the processing times of .
(2)~An online strategy has a buffer that can be used to reorder . Whenever a job arrives, it
is inserted into the buffer; then one job of the buffer is removed and placed in the current schedule. 
(3)~An online algorithm may migrate a certain number or volume of jobs. 

In this paper we investigate makespan minimization assuming that an online algorithm is allowed to
build several schedules in parallel while processing a job sequence . Each incoming job is
sequenced in each of the schedules. At the end of the scheduling process the best schedule is selected. 
We believe that this is a natural form of resource augmentation: In classical online makespan minimization, 
studied in the literature so far, an algorithm constructs a schedule while jobs arrive one by one. 
Once all jobs have arrived, the schedule may be executed. 
Hence in this standard framework there is a priori no reason why an algorithm should not be 
able to construct several solutions, the best of which is finally chosen. 

Our new proposed setting can be viewed
as providing an online algorithm with extra space, which is used to maintain several 
solutions. Very little is known about the value of 
extra space in the design of online algorithms. Makespan minimization with parallel schedules
is of particular interest in parallel processing environments where each processor can take care of
a single or a small set of schedules. We develop algorithms that require hardly any coordination 
or communication among the schedules. Last not least the proposed setting is interesting
w.\,r.\,t. to the foundations of scheduling theory, giving insight into the value of multiple
candidate solutions.

Makespan minimization with parallel schedules was also addressed by Kellerer et al.~\cite{KKST}. 
However, the paper focused on the restricted setting with   machines.
In this paper we explore the problem for a
general number  of machines. As a main result we show that a constant number of schedules
suffices to achieve a significantly improved competitiveness, compared to the standard setting
without resource augmentation. The competitive ratios obtained are at least 
as good and in most cases better than those attained in the other models of resource augmentation 
mentioned above. 

The approach to grant an online algorithm extra space, invested to maintain multiple solutions,
could be interesting in other problems as well. The approach is viable in applications where an 
online algorithm constructs a solution that is used when the entire input has arrived.  
This is the case, for instance, in basic online graph coloring and matching problems~\cite{I,KVV,LST}. 
The approach is also promising in problems that can be solved by a set of independent agents, each of 
which constructs a separate solution. Good examples are online navigation and exploration problems
in robotics~\cite{BC,BRS,DKP}. Some results are known for graph search and 
exploration, see e.\,g.~\cite{BS,FGK,LS}, but the approach has not been studied for geometric 
environments.

\vspace*{0.1cm}

{\bf Problem definition:}
We investigate the problem {\em Makespan Minimization with Parallel Schedules (MPS)\/}. As always, the
jobs of a sequence  arrive one by one and must be scheduled 
non-preemptively on  identical parallel machines. Each job  has a processing time . 
In \MPS, an online algorithm  may maintain a set  of schedules
during the scheduling process while jobs of  arrive. Each job  is sequenced 
in each schedule , . At the end of , algorithm  selects a schedule 
 having the smallest makespan and outputs this solution. The other schedules of 
are deleted. 

As we shall show \MPS\ can be reduced to the problem variant where the optimum makespan of the job sequence
to the processed is known in advance. Hence let \MPSO\ denote the variant of \MPS\ where, prior to the
arrival of the first job, an algorithm  is given the value of the optimum makespan
 for the incoming job sequence . 
An algorithm  for \MPS\ or \MPSO\ is 
-competitive if, for every job sequence , it outputs a schedule whose makespan is at most 
 times .

\vspace*{0.1cm}
{\bf Our contribution:}
We present a comprehensive study of \MPS. We develop a -competitive algorithm, for any
, using a constant number of  schedules.
Furthermore, we give a -competitive algorithm, for any , that uses a 
polynomial number of schedules. The number is , which
depends on  but is independent of the job sequence . These performance guarantees are nearly 
best possible. The algorithms are obtained via some intermediate results that may be of 
independent interest.

First, in Section~\ref{sec:redu} we show that the original problem \MPS\ can be reduced to the variant
\MPSO\ in which the optimum makespan is known. More specifically, given any
-competitive algorithm  for \MPSO\ we construct a -competitive algorithm
, for any . If  uses  schedules, then 
uses  schedules. 
The construction works for any algorithm  for \MPSO. In particular we could use a 1.6-competitive
algorithm by Chen et al.~\cite{CKK} that assumes that the optimum makespan is known and builds
a single schedule. We would obtain a -competitive algorithm that builds at most 
 schedules.

We proceed and develop algorithms for \MPSO. In Section~\ref{sec:ptas} we give a -competitive
algorithm, for any , that uses 
 schedules. In Section~\ref{sec:4/3}
we devise a -competitive algorithm, for any , that uses 
 schedules. Combining these algorithms with , we
derive the two algorithms for \MPS\ mentioned in the above paragraph; see also Section~\ref{sec:mps}.
The number of schedules used by our strategies depends on  and exponentially on 
 or . Such a dependence seems inherent if we wish to explore the full power
of parallel schedules. The trade-offs resemble those exhibited by PTASes in offline approximation.
Recall that the PTAS by Hochbaum and Shmoys~\cite{HS} for makespan minimization achieves
a -approximation with a running time of .

In Section~\ref{sec:lb} we present lower bounds. We show that any online algorithm
for \MPS\ that achieves a competitive ratio smaller than 4/3 must construct more than 
schedules. Hence the competitive ratio of 4/3 is best possible using a constant number of
schedules. We show a second lower bound that implies that the number of schedules of our 
-competitive algorithm is nearly optimal, up to a polynomial factor.

Our algorithms make use of novel guessing schemes.  works with guesses on the 
optimum makespan. Guessing and {\em doubling\/} the value of the optimal solution is a technique that
has been applied in other load balancing problems, see e.\,g.~\cite{Azar}. However here we 
design a refined scheme that carefully sets and readjusts guesses so that the resulting competitive
ratio increases by a factor of  only, for any . Moreover, the readjustment
and job assignment rules have to ensure that scheduling errors, made when guesses were to
small, are not critical. Our -competitive algorithm works with guesses on the job processing
times and their frequencies in . In order to achieve a constant number of schedules, we have
to sparsify the set of all possible guesses. As far as we know such an approach has not been used 
in the literature before.

All our algorithms have the property that the parallel schedules are constructed basically independently.
The algorithms for \MPSO\ require no coordination at all among the schedules. In  
a schedule only has to report when it fails, i.\,e.\ when a guess on the optimum makespan is
too small.

The competitive ratios achieved with parallel schedules are considerably smaller than
the best ratios of about 1.92 known for the scenario without resource augmentation. Our ratio of , 
for small , is lower than the competitiveness of about 1.46 obtained in the settings where a 
reordering buffer of size  is available or  jobs may be reassigned. 
Skutella et al.~\cite{SSS} gave an online algorithm that is -competitive if,
before the assignment of any job , jobs of processing volume 
may be migrated. Hence the total amount of extra resources used while scheduling 
depends on the input sequence.



{\bf Related work:} Makespan minimization with parallel schedules was first studied by 
Kellerer et al.~\cite{KKST}. They assume that  machines are available and two schedules may
be constructed. They show that in this case the optimal competitive ratio is 4/3.

We summarize results known for online makespan minimization without resource augmentation. As mentioned before, 
{\em List\/} is -competitive. Deterministic online algorithms with a smaller competitive ratio were
presented in~\cite{A,BFKV,FW,GW,KPT}. The best algorithm currently known is 1.9201-competitive~\cite{FW}.
Lower bounds on the performance of deterministic strategies were given in~\cite{A,BKR,FKT,GRTW,R,RC}.
The best bound currently known is 1.88, see~\cite{R}. No randomized online algorithm whose competitive 
ratio is provably below the deterministic lower bound is currently known for general . 

We next review the results for the various models of resource augmentation.
Articles~\cite{ANST,AST,AST2,AR,CKK,KKST} study makespan minimization assuming that an online algorithm knows
the optimum makespan or the sum of the processing times of . Chen et al.~\cite{CKK} developed a
1.6-competitive algorithm. Azar and Regev~\cite{AR} showed that no online algorithm can attain a competitive
ratio smaller than 4/3. The setting in which an online algorithm is given a reordering buffer
was explored in~\cite{EOW,KKST}. Englert et al.~\cite{EOW} presented an algorithm that, using a buffer
of size , achieves a competitive ratio of ,
where  is the Lambert  function. No algorithm using a buffer of size  can beat 
this ratio. 

Makespan minimization with job migration was addressed in~\cite{AH,SSS}. An algorithm that
achieves again a competitiveness of  and uses  job
reassignments was devised in~\cite{AH}. No algorithm using  reassignments can obtain
a smaller competitiveness. Sanders et al.~\cite{SSS} study a  scenario in which before the assignment 
of each job , jobs up to a total processing volume of  may be migrated, for some constant
. For , they present a 1.5-competitive algorithm. They also show a -competitive
algorithm, for any , where . 

As for memory in online algorithms, Sleator and Tarjan~\cite{ST} studied the paging problem assuming that an
online algorithm has a larger fast memory than an offline strategy. Raghavan and Snir~\cite{RS} traded memory for randomness 
in online caching. 

{\bf Notation:} Throughout this paper it will be convenient to associate schedules with algorithms, i.\,e.\ a 
schedule  is maintained by an algorithm  that specifies how to assign jobs to machines in . Thus an
algorithm  for \MPS\ or \MPSO\ can be viewed as a family  of algorithms 
that maintain the various schedules. We will write . If  is an 
algorithm for \MPSO, then the value  is of course given to all algorithms of . 
Furthermore, the {\em load\/} of a machine always denotes the sum of the processing
times of the jobs already assigned to that machine.

\section{Reducing \MPS\ to \MPSO}\label{sec:redu}

In this section we will show that any -competitive algorithm  for \MPSO\ can 
be used to construct a -competitive algorithm  for \MPS, for any . 
The main idea is to repeatedly execute  for a set of guesses on the optimum makespan.
The initial guesses are small and are increased whenever a guess turns out to be smaller than . 
The increments are done in small steps so that, among the final guesses, there exists one that is upper bounded
by approximately . In the analysis of this scheme we will have to bound machine
loads caused by scheduling ``errors'' made when guesses were too small.
Unfortunately the execution of , given a guess , can lead to undefined
algorithmic behavior. As we shall show,
guesses  are not critical. However, guesses  have to be handled 
carefully. 

So let  be a -competitive algorithm for \MPSO\ that, given guess 
, is executed on a job sequence . Upon the arrival of a job , an algorithm
 may {\em fail\/}  because the scheduling rules of  do not specify a machine where 
to place  in the current schedule . We define two further conditions when an algorithm 
fails. The first one identifies situations where a makespan of  is not preserved and hence
-competitiveness may not be guaranteed. More precisely,  would assign  to a machine  such 
that , where  denotes 's machine load before the assignment.
The second condition identifies situations where  is not consistent with lower bounds on the
optimum makespan, i.\,e.\  is smaller than the average machine load or the processing time
of . Formally, an algorithm  {\em fails\/} if a job , , has to be
scheduled and one of the following conditions holds.
\begin{enumerate}[(i)] 
 \item  does not specify a machine where to place  in the current schedule .
 \item There holds , for the machine  to which  would assign  
      in .
 \item There holds  or .
\end{enumerate}

We first show that guesses  are not problematic. If a -competitive 
algorithm  for MPS is given a guess , 
then there exists an algorithm  that does not fail during 
the processing of  and generates a schedule whose makespan is at most . This is shown by the next lemma.


\begin{lemma}\label{lem:guess1}
Let  be a -competitive algorithm for MPS that, given guess
, is executed on a job sequence  with . Then there exists an
algorithm  that does not fail during the processing of  and generates a schedule
whose makespan is at most .
\end{lemma}
\begin{proof}
Let  be an optimal schedule for the job sequence \linebreak . Moreover, let  
denote the load of machine  in , . For any  with , define a
job  of processing time . Let  be the job sequence consisting
of  followed by the new jobs . These up to  jobs may be appended to  in any order.
Obviously . Hence when  using guess  is executed on ,
there must exist an algorithm  that generates a schedule with a makespan of at most 
. Since  is a prefix of , this algorithm  does not fail and generates
a schedule with a makespan of at most , when  given guess  is executed on
. \hspace*{\fill}{}
\end{proof}



\vspace*{0.1cm}

{\bf Algorithm for MPS:} We describe our algorithm  for \MPS, where 
 and  may be
chosen arbitrarily. The construction takes as input any algorithm 
for \MPSO. For a proper choice of ,  will be -competitive, provided
that  is -competitive. 

At any time  works with  guesses  on the optimum
makespan for the incoming job sequence . These guesses may be adjusted during the processing
of ; the update procedure will be described in detail below. For each guess , ,
 executes . Hence  maintains a total of  schedules,
which can be partitioned into subsets . Subset  contains
those schedules generated by  using , . Let 
denote the schedule generated by  using . 

A job sequence  is processed as follows. Initially, upon the arrival of the first job , the
guesses are initialized as  and , for . 
Each job , , is handled in the following way. Of course each such job is sequenced
in every schedule ,  and . Algorithm  
checks if  using  fails when having to sequence  in . We remark that this
check can be performed easily by just verifying if one of the conditions (i--iii) holds. If  using 
 does not fail and has not failed since the last adjustment of , then in 
job  is assigned to the machine specified by  using . The initialization of a
guess is also regarded as an adjustment.  If  using  does fail, then
 and all future jobs are always assigned to a least loaded machine in 
until  is adjusted the next time. 

Suppose that after the sequencing of  all algorithms of  using
a particular guess  have failed since the last adjustment of this guess. Let  be the
largest index  with this property. Then the guesses  are adjusted.
Set  and 
, for . For any readjusted  guess , 
, algorithm  using  ignores all jobs  with  when
processing future jobs of . Specifically, when making scheduling decisions and determining
machine loads, algorithm  using  ignores all job  with  in its schedule
. These jobs are also ignored when  checks if  using guess  fails
on the arrival of a job. Furthermore, after the assignment of , machines in  machines are renumbered
so that  is located on a machine it would occupy if it were the first job of an input sequence. 

When guesses have been adjusted, they are renumbered, together with the corresponding schedule sets , 
such that again . Hence at any time  and
, for . We also observe that whenever a guess is adjusted, its 
value increases by a factor of at least . A summary of  is given in Figure~\ref{fig:1}.


\begin{figure}[h]
\fbox{
\begin{minipage}{11.7cm}
{\bf Algorithm } \-8pt]
\item[2.] At time  execute the following steps.\-8pt]
\item[(b)] If all algorithms  for some  have failed since the last readjustment
  of , then let  be the largest index with this property. Set ,  for . Renumber the guesses such that .\\label{eq:b1}
 \ell(j) =  \ell_{t_s}(j) +\sum_{i=0}^{s-1} \left ( \ell_{t_{i}}(j) - \ell_{t_{i+1}}(j) \right ).

\rho \gamma_1 + \sum_{i=0}^{s-1}  \max\{\rho,2\}\gamma(i).  \label{eq:b2}

\lefteqn{\rho(1+\frac{\eps}{3\rho})\opt(\sigma) + \sum_{i=0}^{s-1}  \frac{\max\{\rho,2\}\gamma(s)}{(1+\eps/(3\rho))^{h\cdot (s-i)}} \notag }\\
& \leq & \rho (1+\frac{\eps}{3\rho})\opt(\sigma) +  \rho(1+\frac{\eps}{3\rho})\opt(\sigma)\sum_{i=0}^{s-1}  \frac{2}{(1+\eps/(3\rho))^{h\cdot (s-i)}} \label{eq:xb2}\\
& \leq & \rho (1+\frac{\eps}{3\rho})\opt(\sigma) \left ( 1 +  \sum_{i=1}^{\infty}  \frac{2}{(1+\eps/(3\rho))^{h\cdot i}} \right ) \notag  \\
& =  & \rho (1+\frac{\eps}{3\rho})\opt(\sigma) \left ( 1 +    \frac{2}{(1+\eps/(3\rho))^h-1} \right )   \label{eq:b3} \\
&\leq  & \rho (1+\frac{\eps}{3\rho})^2\opt(\sigma) \ \leq \ \rho (1+\frac{\eps}{\rho})\opt(\sigma) = (\rho+\eps)\opt(\sigma).    \label{eq:b4} 
-15pt]
\begin{itemize}
\item[1.] , where 
 \\
 with  and .\-8pt]
\begin{itemize}
\item[(a)] Compute optimal schedule  for input consisting of  jobs of processing time
, .\-8pt]
\end{itemize}
\end{itemize}
\end{minipage} 
}
\caption{The algorithm }\label{fig:2}
\end{figure}
 


\begin{theorem}\label{th:guess2}
For any ,  is -competitive and uses at most  schedules.
\end{theorem}
\begin{proof}
The bound on the number of schedules simply follows from the fact that  maintains
 schedules where  and 
.

Let  be an arbitrary job sequence and let  be the number of jobs with a processing time in 
, for . Since any  is upper bounded by , the resulting
vector  is in . For this vector , consider the associated algorithm .
We prove that when  has finished processing , the resulting schedule  has a makespan
of at most . Recall again that we assume without loss of generality
that .

We analyze the steps in which  assigns jobs , , to machines in . If  is
large with , , then there must exist a machine  in the current schedule
 such that . Algorithm  will assign  to such a machine. Hence after
the processing of , for any  in , the total load caused by large jobs is upper bounded
by . We next argue that this value is at most . Consider an optimal
schedule  for . Modify this schedule by (a)~deleting all small jobs and (b)~rounding
each job processing time in  to , for . The resulting schedule 
schedule  has a makespan of at most . Furthermore  is
a schedule for an input sequence consisting of  jobs of processing time . Since
 is an optimal schedule for this input, each machine load  is upper bounded by 
.

We finally show that when  has to sequence a small job , then there is a machine  
such that  is upper bounded by . This implies that the
assignment of  causes a machine load of at most  in the final schedule . 

So suppose that upon the arrival of a small job  there holds 
for all machines , . Recall that  is the load on machine  caused by small
jobs in the current schedule . Note that  is the total processing time of large jobs
in  if processing times in  are rounded up to , for . 
Hence  is a lower bound on the total processing time of large jobs in .
It follows that the total processing time of all jobs in  is at least
. The assumption that 
 holds for all machines  implies that
the total processing time of jobs in  is at least , 
which contradicts the fact that  is the optimum makespan.  \hspace*{\fill}{}
\end{proof}



\section{A {}-competitive algorithm for \MPSO}\label{sec:4/3}

We develop an algorithm   for \MPSO\ that is -competitive, for any , if
the number  of machines is not too small. We then combine  with ,
presented in the last section, and derive a strategy  that is -competitive,
for arbitrary . The number of required schedules is , which is a constant independent of
 and . We firstly present a description of the algorithm; the corresponding analysis is given thereafter.

Before describing  in detail, we explain the main ideas of the algorithm. One concept
is identical to that used by : Partition the range of possible job processing times into
intervals or {\em job classes\/} and consider distributions of jobs over these classes. However, in order
to achieve a constant number of schedules we have to refine this scheme and incorporate new
ideas. First, the job classes have to be chosen properly so as to allow a compact packing of jobs on the machines.
An important, new aspect in the construction of  is that we will not consider 
the entire set  of tuples specifying how large jobs of an input sequence~ are distributed over 
the job classes. Instead we will define a suitable sparsification  of . Each  represents 
an estimate or guess on the number of large jobs arising in . More specifically, if 
, then it is assumed that  contains at least  jobs with a processing
time of job class . 

Obviously, the job sequence  may contain more large jobs, the exact number of which is unknown. 
Furthermore, it is unknown which portion of the total processing time of  will arrive as small 
jobs. In order to cope with these uncertainties  has to construct robust schedules. 
To this end the number of machines is partitioned into two sets  and
. For the machines of , the algorithm initially determines a good assignment or
{\em configuration\/} assuming that  jobs of job class  will arrive. The machines of 
are reserve machines and will be assigned additional large jobs as they arise in . Small jobs
will always be placed on machines in . The initial configuration determined for these 
machines has the property that, no matter how many small jobs arrive, a machine load never exceeds 
times the optimum makespan.

We proceed to describe  in detail. Let . Moreover, set .
Again we assume without loss of generality that, for an incoming job sequence, there holds . 
Hence the processing time of any job is upper bounded by~1. 

{\bf Job classes:} A job , , is {\em small\/}
if ; otherwise  is {\em large\/}. We divide the range of possible job processing times
into job classes. Let  be the interval containing the processing times of small jobs.
Let  and , where the logarithm is
taken to base~2. For , let 

It is easy to verify that  and , for . Furthermore
 and . For  define
. There holds . Moreover, for 
, let . Intuitively,  contains the processing times that
are twice as large as those in , . There holds 
. Hence . In the following  represents {\em job class\/} , for . We say that
 is a {\em class- job\/} if , where .

{\bf Definition of target configurations:} As mentioned above, for any incoming job sequence , 
 works with estimates on the number of class- jobs arising in , . 
For each estimate, the algorithm initially determines a virtual schedule or {\em target configuration\/} on a 
subset of the machines, assuming that the estimated set of large jobs will indeed arrive. Hence we partition the 
 machines into two sets  and . Let . Moreover, 
let  and . Set  
contains the machines for which a target configuration will be computed;  contains the 
reserve machines. The proportion of   to   is roughly .

A target configuration has the important property that any machine  contains large jobs
of only one job class , . Therefore, a target configuration is properly defined
by a vector . If , then  does not contain 
any large jobs in the target configuration, . If  , where 
, then  contains class- jobs, . The vector  
implicitly also specifies how many large jobs reside on a machine. If  with , then
 contains two class- jobs. Note that, for general , a third job cannot be 
placed on the machine without exceeding a load bound of . If  with , then
 contains one class- job. Again, the assignment of a second job is not feasible in general. 
Given a configuration ,  is referred to as a {\em class- machine} if , where
 and . 

With the above interpretation of target configurations, each vector   encodes
inputs containing  class- jobs, for , as
well as   class- jobs, for .  Hence,
for an incoming job sequence, instead of considering estimates on the number of class- jobs, for any
, we can equivalently consider target configurations. Unfortunately, it
will not be possible to work with all target configurations  since the
resulting number of schedules to be constructed would be .
Therefore, we will work with a suitable sparsification of the set of all configurations.

{\bf Sparsification of the set of target configurations:} Let  and
. We will show that 
 if  is not too small (see Lemma~\ref{lem:kappa}).
This property in turn will ensure that any job sequence  can be mapped to a .
For any vector , we define a target configuration
 that contains  class- machines, for , 
provided that  does not exceed . More specifically,
for any , let  and ,
be the partial sums of the first  entries of , multiplied by , for
. Let . First construct a vector 
of length  that contains exactly  class- machines.
That is, for , let  for . We now truncate
or extend  to obtain a vector of length . If , then  is the vector consisting
of the first  entries of . If , then , i.\,e.\
the last  entries are set to~0. Let  be the set of all target configurations
constructed from vectors .

{\bf The algorithm family:} Let . For any , algorithm
 works as follows. Initially, prior to the arrival of any job of ,  determines
the target configuration specified by  and uses this virtual schedule for
the machines of  to make scheduling decisions. Consider a machine  and
suppose , i.\,e.\  is a class- machine  for some . Let  and 
be the targeted minimal and maximal loads caused by large jobs on , according to the target
configuration. More precisely, if , then   and  . 
Recall that in a target configuration a class- machine contains two class- jobs if .
If  and hence  for some , then   
and  . If  is 
a machine with , then . While the job sequence  is processed,
a machine  may or may not be {\em admissible\/}. Again assume that  is a class-
machine with . If , then at any time during the scheduling process  is
admissible if it has received less than two class- jobs so far. Analogously, if  ,
then  is admissible if it has received no class- job so far. Finally, at any time during the
scheduling process, let  be the current load of machine  and let  be the
load due to small jobs, .

Algorithm  schedules each incoming job , , in the following way. First assume
that  is a large job and, in particular, a class- job, . The algorithm checks
if there is a class- machine in  that is admissible. If so,  is assigned to such a machine.
If there is no admissible class- machine available, then  is placed on a machine in .
There jobs are scheduled according to the {\em Best-Fit\/} policy. More specifically,  checks if there exists
a machine  such that . If this is the case, then  is
assigned to such a machine with the largest current load . If no such machine exists, 
is assigned to an arbitrary machine in . Next assume that  is small. The job is a assigned
to a machine in , where preference is given to machines that have already received small jobs. 
Algorithm  checks if there is an  with  such that 
. If this is the case, then  is assigned to any such machine.
Otherwise  considers the machines of  which have not yet received any small jobs. If there exists
an  with  such that , then among these
machines  is assigned to one having the smallest targeted load . 
If again no such machine exists,   is assigned to an arbitrary machine in . A summary of  
, which focuses on the job assignment rules, is given in Figure~\ref{fig:3}. We obtain the following result.

\begin{figure}[h]
\fbox{
\begin{minipage}{11.7cm}
{\bf Algorithm } \-8pt]
\item[2.]  works as follows.\-8pt]
\item[(b)] Each  is sequenced as follows.\\
{\em  is large:\/} Let  be a class- job. If there is an admissible class- machine
in , assign  to it. Otherwise check if   such that 
. If so, assign  to such an  with the highest ; otherwise place 
 on an arbitrary .\\
{\em  is small:\/} If   with  such that  , assign  to it. Otherwise check if   with  such 
that  . If so, assign  to such an  with the lowest ; otherwise place 
 on an arbitrary .\
\kappa \lfloor (m-\mu)/(2l-1)\rfloor &\geq 
 & 2 \textstyle{(2+\frac{1}{\eps'})} (2l-1)   \cdot    \lfloor    (m-\mu)/(2l-1) \rfloor   \\
 & \geq & 2 \textstyle{(2+\frac{1}{\eps'})}  (2l-1)  \cdot   (  (m-\mu)/(2l-1)  -1  )   \notag \\
& = & 2 \textstyle{(2+\frac{1}{\eps'})}  (2l-1)   \cdot \left (  \dfrac {m-\lceil \frac{1+\eps'}{1+2\eps'}m  \rceil }{ 2l-1 } -1  \right)     \notag  \\
&  \geq & 2 \textstyle{(2+\frac{1}{\eps'})}  (2l-1) \cdot \left (  \dfrac {  \frac{\eps'}{1+2\eps'}m  -1 }{ 2l-1 } -1  \right)    \notag  \\ 
& = & 2 \textstyle{(2+\frac{1}{\eps'})} (2l-1)  \cdot \left ( \frac { \eps'm  - (1+2\eps')2l }{(1+2\eps')(2l-1)} \right) \notag \\
&\geq & m + m - (2/\eps')(1+2\eps') 2l \notag \\
& \geq & m, \notag
\textstyle \lceil \sum_{i=1}^l n_i(\sigma) / 2 \rceil -\mu_1 + \sum_{i=l+1}^{2l-1} n_i(\sigma) -\mu_2 
\leq m-\mu.\textstyle \sum_{i=l+1}^{2l-1} n_i(\sigma) < \sum_{i=l+1}^{2l-1} (u_i+1)m_0 = \sum_{i=l+1}^{2l-1} u_im_0 + (l-1)m_0.\label{eq:n1}
\textstyle \sum_{i=l+1}^{2l-1} n_i(\sigma) \leq \mu_2 + (l-1)m_0.
\label{eq:n2}
\textstyle \lceil \sum_{i=1}^l n_i(\sigma)/2\rceil \leq \sum_{i=1}^l n_i(\sigma)/2 +1 \leq 
\sum_{i=1}^l (u_i+1)m_0 = \mu_1 +lm_0.
\textstyle \lceil \sum_{i=1}^l n_i(\sigma)/2\rceil -\mu_1 + \sum_{i=l+1}^{2l-1} n_i(\sigma) - \mu_2 \leq
(2l-1) \lfloor (m-\mu)/(2l-1)\rfloor \leq m-\mu.
\ell^-(j') +p_t &\geq& 1/3 + 7\eps' + \ell^-(j) - \ell^+(j) + \ell^-(j)\\
& \geq & {\textstyle 1/3 + 7\eps' + 2({1\over 12} + {3\over 2}\eps')({1\over 2^{\lambda+1-i}} - {1\over 2^{\lambda-i}})}\\
& & {\textstyle +2/3 - 4\eps' +  2({1\over 12} + {3\over 2}\eps'){1\over 2^{\lambda+1-i}}}\\
& = & 1 + 3\eps' > 1+\eps',

{\textstyle \sum_{j=1}^{\mu} (\ell^-(j) + \ell_s(j))} & \geq & 
\textstyle{(1+\eps')(\mu-1) \geq (1+\eps') ({1+\eps' \over 1+2\eps'} m -1)}\\
& = & {\textstyle m + {(\eps')^2\over1+2\eps'} m - (1+\eps') \geq m.}

\lefteqn{\textstyle{M = \{(m_0, \ldots, m_{2h}) \in \mathbb{N}_0^{2h+1}\mid  \sum_{i=0}^{2h} m_i = m \ \ \mbox{and} }}\\
& & \hspace*{4.6cm}\textstyle{(1/\eps')m_{2h} + \sum_{i=1}^{2h-1} i m_i = mh\}.}

\lefteqn{\textstyle{M = \{(m_0, \ldots, m_{2h}) \in \mathbb{N}_0^{2h-1} \mid  \sum_{i=0}^{2h} m_i = m-1 \ \ \mbox{and}}}\\
& & \hspace{4.6cm} \textstyle{(1/\eps')m_{2h} + \sum_{i=1}^{2h-1} i m_i = (m-1)h\}.}
\textstyle{M' = \{(0,\ldots, 0,m'_{h+1}, \ldots, m'_{2h-1},0) \in \mathbb{N}_0^{2h+1} \mid \sum_{i=1}^{h-1} m'_{h+i} \leq m/2\}.}
m_{2h}/\eps' + \sum_{i=0}^{2h-1} im_i & = & \sum_{i=1}^{2h-1} im_i = \sum_{i=1}^{h-1} (i+2h-i) m_i +hm_h \\
& = & 2h\sum_{i=1}^{h-1}m_i + h(m-2\sum_{i=1}^{h-1} m_i) = mh.
\textstyle{M' = \{(0,\ldots, 0,m'_{h+1}, \ldots, m'_{2h-1},0) \mid  m'_i\in \mathbb{N}_0 \ \ \mbox{and} \ \
\sum_{i=1}^{h-1} m'_{h+i} \leq \lfloor m/2\rfloor\}.}
{m'+h-1\choose h-1} &=& {(m'+h-1)!\over m'!(h-1)!} \  \geq {(m'+h-1)^{m'+h-1/2}\over 4\sqrt{2\pi} (m')^{m'+1/2} (h-1)^{h-1/2}}\\
&=& {1\over 4\sqrt{2\pi m'}} \left(1+{h-1\over m'}\right)^{m'} \left(1+{m'\over h-1}\right)^{h-1/2} \\
& > & {1\over 4\sqrt{2\pi m'}} \left(1+{m/2-1/2 \over 1/(4\eps)}\right)^{h-1/2}.

The last expression is .\hspace*{\fill}{}
\end{proof}



\begin{thebibliography}{10}
\setlength{\itemsep}{0pt plus .3pt}
\setlength{\parsep}{0pt plus .3pt}
\setlength{\parskip}{0pt plus .3pt}

\bibitem{A}
S.~Albers. Better bounds for online scheduling. {\em SIAM J.\ Comput.},
29:459-473, 1999. 

\bibitem{AH}
S.\ Albers and M.\ Hellwig. On the value of job migration in online makespan minimization.
{\em Proc.\ 20th Annual European Symposium on Algorithms}, Springer LNCS 7501, 84--95, 2012.

\bibitem{ANST}
E.\ Angelelli, A.B.\ Nagy, M.G.\ Speranza and Z.\ Tuza. The on-line multiprocessor scheduling
problem with known sum of the tasks. {\em Journal of Scheduling\/},  7:421--428, 2004.

\bibitem{AST}
E.\ Angelelli, M.G.\ Speranza and Z.\ Tuza. Semi-on-line scheduling on two parallel processors 
with an upper bound on the items. {\em Algorithmica\/}, 37:243--262, 2003.

\bibitem{AST2}
E.\ Angelelli, M.G.\ Speranza and Z.\ Tuza. New bounds and algorithms for on-line scheduling: two 
identical processors, known sum and upper bound on the tasks. {\em Discrete Mathematics \& Theoretical 
Computer Science\/},  8:1--16, 2006.

\bibitem{Azar}
Y.\ Azar. On-line load balancing. In {\em Online Algorithms: The State of the Art\/} (A.\ Fiat and
G.\ Woeginger, eds). Springer LNCS 1441, 178--195, 1998.

\bibitem{AR}
Y.\ Azar and O.\ Regev. On-line bin-stretching. {\em Theor.\ Comput.\ Sci.\/},
268:17--41, 2001.

\bibitem{BKR}
Y.\ Bartal, H.\ Karloff and Y.\ Rabani. A better lower bound for on-line
scheduling. {\em Infomation Processing Letters}, 50:113--116, 1994.

\bibitem{BFKV}
Y.~Bartal, A.~Fiat, H.~Karloff and R.~Vohra. New algorithms for an 
ancient scheduling problem. {\em Journal of Computer and System
Sciences}, 51:359--366, 1995.

\bibitem{BS}
M.A.\ Bender and D.K.\ Slonim. The Power of team exploration: Two robots can learn unlabeled directed graphs. 
{\em 35th Annual Symp.\ on Foundations of Comput.\ Sci.\/}, 75--85, 1994.

\bibitem{BC}
A.\ Blum and P.\ Chalasani. An online algorithm for improving performance in navigation. 
{\em SIAM Journal on Computing\/}, 29:1907--1938, 2000.

\bibitem{BRS}
A.\ Blum, P.\ Raghavan and B.\ Schieber. Navigating in unfamiliar geometric terrain. 
{\em SIAM Journal on Computing\/}, 26:110--137, 1997.

\bibitem{CKK}
T.C.E.\ Cheng, H.\ Kellerer and V.\ Kotov. Semi-on-line multiprocessor scheduling with given 
total processing time. {\em Theor.\ Comput.\ Sci.\/}, 337:134--146, 2005.

\bibitem{DKP}
X.\ Deng, T.\ Kameda and C.H.\ Papadimitriou. How to learn an unknown environment I: the rectilinear 
case. {\em Journal of the ACM\/}, 45:215--245, 1998.

\bibitem{EOW}
M.\ Englert, D.\ \"Ozmen and M.\ Westermann. The power of reordering for online minimum makespan scheduling. 
{\em Proc.\ 49th Annual IEEE Symposium on Foundations of Computer Science\/}, 603--612, 2008.

\bibitem{FKT}
U.\ Faigle, W.\ Kern and G.\ Turan. On the performance of on-line
algorithms for partition problems. {\em  Acta Cybernetica}, 9:107--119,
1989.

\bibitem{F}
W.\ Feller. An Introduction to Prob.\ Theory and its Applications. John Wiley\& Sons, 1968.

\bibitem{FW}
R.~Fleischer and M.~Wahl. Online scheduling revisited. {\em J.\ of
Scheduling}, 3:343--353, 2000.

\bibitem{FGK}
P.\ Fraigniaud, L.\ Gasieniec, D.R.\ Kowalski and A.\ Pelc. Collective tree exploration. 
{\em Networks\/}, 48:166--177, 2006.

\bibitem{GW}
G.\ Galambos and G.\ Woeginger. An on-line scheduling heuristic with 
better worst case ratio than Graham's list scheduling. {\em SIAM
Journal on Computing}, 22:349--355, 1993.

\bibitem{G}
R.L.\ Graham. Bounds for certain multi-processing anomalies.
{\em Bell System Technical Journal}, 45:1563--1581, 1966.

\bibitem{GRTW}
T.~Gormley, N.~Reingold, E.~Torng and J.~Westbrook. Generating
adversaries for request-answer games. {\em Proc.\ 11th ACM-SIAM
Symposium on Discrete Algorithms\/}, 564--565, 2000.

\bibitem{HS}
D.S.\ Hochbaum and D.B.\ Shmoys. Using dual approximation algorithms for scheduling problems:
Theoretical and practical results. {\em Journal of the ACM\/}, 34:144--162, 1987.

\bibitem{I}
S.\ Irani. Coloring inductive graphs on-line. {\em Algorithmica\/},  11:53--72, 1994.

\bibitem{KPT}
D.R.~Karger, S.J.~Phillips and E.~Torng. A better algorithm for an
ancient scheduling problem. {\em Journal of Algorithms}, 20:400--430, 1996.

\bibitem{KVV}
R.M.\ Karp, U.V.\ Vazirani and V.V.\ Vazirani. An optimal algorithm for on-line bipartite matching. 
{\em Proc.\ 22nd Annual ACM Symposium on Theory of Computing\/}, 352--358, 1990.

\bibitem{KKST}
H.\ Kellerer, V.\ Kotov, M.G.\ Speranza and Z.\ Tuza. Semi on-line algorithms for the 
partition problem. {\em  Operations Research Letters\/}, 21:235--242, 1997.

\bibitem{LS}
A.\ L\'opez-Ortiz, S.\ Schuierer. On-line parallel heuristics, processor scheduling and robot searching 
under the competitive framework. {\em Theor.\ Comput.\ Sci.\/}, 310:527--537, 2004.

\bibitem{LST}
L.\ Lov\'asz, M.E.\ Saks and W.A.\ Trotter. An on-line graph coloring algorithm with sublinear 
performance ratio. {\em Discrete Mathematics\/}, 75:319--325, 1989. 

\bibitem{RS}
P.\ Raghavan and M.\ Snir. Memory versus randomization in on-line algorithms. {\em IBM Journal of Research 
and Development\/}, 38:683--708, 1994.

\bibitem{R}
J.F.\ Rudin III. Improved bounds for the on-line scheduling problem.
Ph.D.\ Thesis. The University of Texas at Dallas, May 2001.

\bibitem{RC}
J.F.\ Rudin III and R.\ Chandrasekaran. Improved bounds for the online scheduling problem. 
{\em SIAM Journal on Computing\/}, 32:717--735, 2003.

\bibitem{SSS}
P.\ Sanders, N.\ Sivadasan and M.\ Skutella. Online scheduling with bounded migration. {\em Mathematics
of Operations Reseach\/}, 34(2):481--498, 2009.

\bibitem{ST} 
D.D.~Sleator and R.E.~Tarjan. Amortized efficiency of list update and
paging rules. {\em Communications of the ACM}, 28:202--208, 1985.

\end{thebibliography}



\end{document}
