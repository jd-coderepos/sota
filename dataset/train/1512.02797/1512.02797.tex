

Finite automata play a central role in the field of formal language
theory and are intensively used to address algorithmic problems from
model-checking to text processing. Many automata-based algorithms have been
developed and are still  being developed. They propose new approaches and heuristics,
even for basic problems like the inclusion
problem\footnote{see \url{http://www.languageinclusion.org/}}.
Evaluating new algorithms is a challenging problem that cannot be addressed
only by the theoretical computation of the worst case complexity. Several
other complementary techniques can be used to measure the efficiency of an
algorithm: average complexity, generic case complexity, benchmarking,
evaluation on hard instances, evaluations on random instances. The first two
approaches are hard theoretical problems, particularly for algorithms
using heuristics and optmizations. Benchmarks, as well as known hard instances, are not always available. 
Nevertheless, in practice, random generation
of inputs is a good way to estimate the efficiency of an algorithm.
Designing uniform random generator for classes of finite automata is a
challenging problem that has been addressed mostly for deterministic
automata~\cite{DBLP:journals/tcs/ChamparnaudP05,DBLP:journals/tcs/BassinoN07,DBLP:journals/tcs/AlmeidaMR07,DBLP:conf/stacs/CarayolN12}
 -the interested reader is referred to~\cite{DBLP:conf/mfcs/Nicaud14} for a
recent survey. However, the problem of uniform random generation of non
deterministic automata (NFAs) is more complex, particularly for a random generation
up to isomorphism: the size of the automorphism group of a $n$-state non
deterministic automata may vary from $1$ to $n!$. 
For most applications, the complexity of the algorithm is related to the
structure of the automata, not to the names of the states:
randomly generated NFAs, regardless of the number of isomorphic automata, may therefore 
lead to an over representation of some isomorphism classes of automata.
Moreover, as discussed in the conclusion of~\cite{DBLP:conf/mfcs/Nicaud14},
the random generation of non deterministic automata has to be done on particular
subclasses of automata in order to obtain a better sampler for the
evaluation of algorithm (since most of the NFAs, for the uniform distribution,
will accept all words).  

The random generation of non deterministic automata is explored
in~\cite{DBLP:conf/lpar/TabakovV05} using random graph techniques (without
considering the obtained distribution relative to automata or to the
isomorphism classes). In~\cite{DBLP:conf/dcfs/ChamparnaudHPZ02}, the random
generation of NFAs is performed using bitstream generation.
In~\cite{DBLP:conf/lata/Nicaud09,DBLP:conf/fsttcs/NicaudPR10} NFAs are
obtained by the random generation of a regular expression and by
transforming it into an equivalent automaton using Glushkov Algorithm. The
use of Markov chains based techniques to randomly generate finite automata
was introduced
in~\cite{DBLP:conf/wia/CarninoF11,DBLP:journals/tcs/CarninoF12} for acyclic
automata.

\subsection{Contributions}
In this paper we address the problem of the uniform generation of elements
in several classes of non deterministic automata (up to isomorphism) by
using Monte-Carlo techniques. We propose this approach for the class of
$n$-state non deterministic automata as well as for (a priori) more
interesting sub-classes. Determining the most interesting subclasses of NFAs
for testing practical applications is not the purpose of this paper. We
would like to point out that Monte Carlo approaches are very flexible and
that the results of this paper can be applied-adapted quite easily for many
classes of NFAs. More precisely:
\begin{enumerate}
\item We propose in Section~\ref{sec:random} several ergodic Markov Chains
whose stationary distributions are respectively uniform on the set of
$n$-state NFAs, $n$-state NFAs with a fixed maximal output degree and
$n$-state NFAs with a fixed maximal output degree for each letter. In
addition, these chains can be adapted for these three classes restricted to
automata with a fixed single initial state. Moving into these Markov chains
can be done in time polynomial in $n$.
\item The main idea of this paper is exposed in
Section~\ref{sec:algo-metro}, where we show how to modify these Markov
Chains using the Metropolis-Hastings Algorithm in order to obtain stationary
distributions that are uniform for the given classes of automata up to
isomorphism. Moving into these new Markov chains requires to compute the sizes
of the automorphism groups of the occurring NFAs, as explained in Section~\ref{sec:randomup}. 

\item The main contributions of this paper are given in
Section~\ref{sec:poly} and in Section~\ref{sec:label}, which can be
red independently. In Section~\ref{sec:poly}, we show a theoretical result for
the classes with a bounded output degree: moving into the modified Markov chains
(for a generation up to isomorphism) can be done in polynomial time. 

\item 
In Section~\ref{sec:label}, we explain how to use {\it labelings}, a classical
graph technique, to compute in practice the sizes of the automorphism
groups. The efficiency of the approach is illustrated with promising
experiments in Section~\ref{sec:xp}.
\end{enumerate}







\subsection{Theoretical Background on NFA}\label{sec:theoryNFA}


For a general reference on finite automata see~\cite{Hopcroft}.  In
this paper $\Sigma$ is a fixed finite alphabet of cardinal
$|\Sigma|\geq 2$, and $m$ is an integer satisfying $m\geq 2$.

A {\it non-deterministic automaton} (NFA) on $\Sigma$ is a tuple $(Q,
\Delta,I,F)$ where $Q$ is a finite set of {\it states}, $\Sigma$ is a finite
alphabet, $\Delta\subseteq Q\times \Sigma \times Q$ is the set of
transitions, $F\subseteq Q$ is the set of final states and $I\subseteq
Q$ is the set of initial states.  For any state $p$ and any letter
$a$, we denote by $p\cdot a$ the set of states $q$ such that
$(p,a,q)\in\Delta$.  The set of transitions $\Delta$ is {\it
deterministic} if for every pair $(p,a)$ in $Q\times\Sigma$ there is
at most one $q\in Q$ such that $(p,a,q)\in \Delta$. Two NFAs are
depicted on Fig.~\ref{fig:isom}.  A NFA is {\it complete} if for every
pair $(p,a)$ in $Q\times\Sigma$ there is at least one $q\in Q$ such
that $(p,a,q)\in \Delta$.  A {\it path} in a NFA is a sequence of
transitions $(p_0,a_0,q_0)(p_1,a_1,q_1)\ldots(p_k,a_k,q_k)$ such that
$q_i=p_{i+1}$. The word $a_0\ldots a_k$ is the {\it label} of the path
and $k$ its {\it length}. If $p_0\in I$ and $q_k\in F$ the path is
successful. A word is {\it accepted} by a NFA if it's the label of a
successful path.  A NFA is {\it accessible} (resp.  {\it
co-accessible}) if for every state $q$ there exists a path from an
initial state to $q$ (resp. if for every state $q$ there exists a path
from $q$ to a final state).  A NFA is {\it trim} if it is both
accessible and co-accessible A {\it deterministic automaton} is a NFA
where $|I|=1$ and whose set of transitions is deterministic.

Let $\NFA(n)$ be the class of  finite automata whose set of states is
$\{0,\ldots,n-1\}$. We are interesting in several subclasses of $\NFA(n)$.
\begin{itemize}
\item $\N(n)$ is the subclass of $\NFA(n)$ of trim finite automata.
\item  $\N_m(n)$ be the class of
finite automata in  $\N(n)$ such that, for each state $p$, there is at most $m$
pairs $(a,q)$ such that $(p,a,q)$ is a transition: there are at most $m$
transitions outgoing each state. 
\item $\N^\prime_m(n)$ be the class of
finite automata in  $\N_m(n)$ such that, for each state $p$ and each
letter $a$, there is at most $m$
states $q$ such that $(p,a,q)$ is a transition: for each letter, there are
at most $m$ transitions labeled by this letter  outgoing each state.
\item Finally, for any class $\X$ of finite automata, we denote by $\X^{\bullet}$ the
subclass of $\X$ of automata whose set of initial states is reduced to
$\{1\}$.  
\end{itemize}

Examples of automata in One
has $$\N_m(n)\subseteq \N_m^\prime(n)\subseteq \N(n)\subseteq\NFA(n).$$
these classes are depicted on Fig.~\ref{fig:AutomatesClasses}.

\begin{figure}[!ht]
\centering
\begin{tikzpicture}
\node (1) [state,draw,initial, initial text=,fill=black!20] at (0,0) {$1$} ;
\node (2) [state,draw,fill=black!20] at (3,0) {$2$} ;
\node (3) [state,accepting,fill=black!20] at (6,0) {$3$} ;

\path[->,>=latex] (1) edge[above] node {$a$} (2);
\path[->,>=latex] (1) edge[above,bend left] node {$a$} (3);
\path[->,>=latex] (1) edge[loop above] node {$b$} (1);
\path[->,>=latex] (2) edge[above] node {$a,b$} (3);
\path[->,>=latex] (2) edge[below, bend left] node {$a,b$} (1);


\draw (3,-2) node {
\begin{tabular}{c}
Automaton  in $\N(3)^\bullet$, $\N_4(3)^\bullet$, $\N_2'(3)^\bullet$\\
and in   $\N(3)$, $\N_4(3)$, $\N_2'(3)$.
\end{tabular}};













\begin{scope}[xshift=0cm,yshift=-5cm]
\node (1) [state,draw,initial, initial text=,fill=black!20] at (0,0) {$1$} ;
\node (2) [state,draw,fill=black!20] at (3,0) {$2$} ;
\node (3) [state,accepting,fill=black!20,initial, initial
text=,fill=black!20] at (6,0) {$3$} ;

\path[->,>=latex] (1) edge[above] node {$a$} (2);
\path[->,>=latex] (1) edge[loop above] node {$a$} (1);
\path[->,>=latex] (2) edge[above,bend left] node {$b$} (3);
\path[->,>=latex] (2) edge[below, bend left] node {$b$} (1);

\draw (3,-2) node {
\begin{tabular}{c}
Automaton  in $\N(3)$, $\N_2(3)$ and $\N_2'(3)$\\
but not in any doted class.
\end{tabular} };
\end{scope}
\end{tikzpicture}
\caption{Several Classes of Automata.}\label{fig:AutomatesClasses}
\end{figure}





Two NFAs are {\it isomorphic} if there exists a 
bijection between their sets of
states preserving the sets initial states, final states and transitions. More precisely,
let $\A=(Q,\Sigma,\Delta,I,F)$ and let $\varphi$ be a bijection from $Q$
into a finite set $\varphi(Q)$. We denote by $\varphi(\A)$ the automaton
$(\varphi(Q),\Sigma,\Delta^\prime,\varphi(I),\varphi(F))$, with 
$\Delta^\prime=\{(\varphi(p),a,\varphi(q))\mid (p,a,q)\in \Delta\}$. 
Two automata $\A_1$ and $\A_2$ are isomorphic if there exists a bijection
$\varphi$ such that $\varphi(\A_1)=\A_2$. 

\begin{figure}
\begin{center}
\begin{tikzpicture}[scale=1.2]
\node [circle,draw,initial,initial text=,fill=black!20](1) at (-2.5,0) {$1$};
\node [circle,draw,fill=black!20](2) at (0,0) {$2$};
\node [circle,draw,fill=black!20](3) at (0,-2) {$3$};
\node [circle,draw,fill=black!20,accepting](4) at (-2.5,-2) {$4$};

\path[->,>=latex] (1) edge[above] node {$a$} (2);
\path[->,>=latex] (2) edge[bend left,left] node {$b$} (3);
\path[->,>=latex] (3) edge[bend left,left] node {$b$} (2);
\path[->,>=latex] (3) edge[left,above] node {$a$} (4);
\path[->,>=latex] (4) edge[loop below] node {$b$} (4);
\path[->,>=latex] (4) edge[left] node {$a$} (1);

\node [circle,draw,initial,initial text=,fill=black!20](1b) at (2.5,0) {$2$};
\node [circle,draw,fill=black!20](2b) at (5,-2) {$3$};
\node [circle,draw,fill=black!20](3b) at (2.5,-2) {$4$};
\node [circle,draw,fill=black!20,accepting](4b) at (5,-0) {$1$};

\path[->,>=latex] (1b) edge[right] node {$a$} (2b);
\path[->,>=latex] (2b) edge[bend left, below] node {$b$} (3b);
\path[->,>=latex] (3b) edge[bend left, below] node {$b$} (2b);
\path[->,>=latex] (3b) edge[left] node {$a$} (4b);
\path[->,>=latex] (4b) edge[loop below] node {$b$} (4b);
\path[->,>=latex] (4b) edge[above] node {$a$} (1b);

\path[->,>=latex,dashed] (1) edge[above,color=black,bend left] node {} (1b);
\path[->,>=latex,dashed] (2) edge[above,color=black] node {} (2b);
\path[->,>=latex,dashed] (3) edge[above,color=black,bend left] node {} (3b);
\path[->,>=latex,dashed] (4) edge[above,color=black] node {} (4b);


\draw (-1,-3.5) node {$\A_1$};
\draw (3.5,-3.5) node {$\A_2$};

\draw (1,-4.5) node {$\varphi(\A_1)=\A_2,\; \textcolor{black}{\varphi(1)=2,\, \varphi(2)=3,\, \varphi(3)=4,\, \varphi(4)=1}$.};
\end{tikzpicture}








\end{center}
\caption{Two Isomorphic Automata}\label{fig:isom}
\end{figure}

Two isomorphic NFAs have the same number of states and are equal, up to
 the states names. The relation {\it is isomorphic to} is an equivalence relation.
For instance, the two automata depicted on Fig.~\ref{fig:isom} are
isomorphic.An {\it automorphism} for a NFA is an isomorphism between this NFA and
itself. 
Given a NFA $\A=(Q,\Sigma,\Delta,I,F)$, the set of automorphisms of $\A$ is
a finite group denoted $\Aut(\A)$. For $Q^\prime\subseteq Q$, 
$\Aut_{Q^\prime}(\A)$ denotes the subset of  $\Aut(\A)$ of automorphisms $\phi$
fixing each element of $Q^\prime$:  for each $q\in Q^\prime$, $\phi(q)=q$.
Particularly $\Aut_{\emptyset}(\A)=\Aut(\A)$, and  
$\Aut_{Q}(\A)$ is reduce to the identity.
For instance, the  automorphism group of the 
automaton depicted on Fig.~\ref{fig:isom}(a) has two elements, the identity
and the isomorphism switching $2$ and $3$. 


The size of the automorphism group of a non deterministic $n$-state
automaton may vary from $1$ to $n!$. For instance, any deterministic trim
automaton whose states are all final has an  automorphism group reduce to
the identity. The non deterministic $n$-state automaton with no transition
and where all states are both initial and final has for automorphism group
the symmetric group. 

The isomorphism problem consists in deciding whether two finite automata are
isomorphic. It is investigated for deterministic automata on different alphabet
in~\cite{DBLP:journals/siamcomp/Booth78} (with a different definition of
isomorphism). It is naturally closed to the same problem for directed graph
and the following result~\cite{DBLP:journals/jcss/Luks82} will be useful in
this paper.
\begin{theorem}\label{thm:luks}
Let $m$ be a fixed positive integer. The isomorphism problem for directed
graphs with degree bounded by $m$ is polynomial.
\end{theorem}

\subsection{Theoretical Background on Markov Chains}\label{sec:theoryMC}

For a general reference on Markov Chains see~\cite{mixing}. Basic
probability notions will not be defined in this paper. The reader is
referred for instance to~\cite{proba-and-computing}.

Let $\Omega$ be a finite set. A {\it Markov chain} on $\Omega$ is a sequence
$X_0,\ldots,X_t,\ldots$ of random variables on $\Omega$ such that
$\P(X_{t+1}=x_{t+1}\mid X_t=x_t)=\P(X_{t+1}=x_{t+1}\mid
X_t=x_t,\ldots,X_i=x_i,\ldots,X_0=x_0),$ for all $x_i\in\Omega$. A Markov
chain is defined by its {\it transition matrix} $M$, which is a function
from $\Omega\times\Omega$ into $[0,1]$ satisfying $M(x,y)=\P(X_{t+1}=y \mid
X_t=x)$. The underlying graph of a Markov chain is the graph whose set of
vertices is $\Omega$ and there is an edge from $x$ to $y$ if $M(x,y)\neq 0$.
A Markov chain is {\it irreducible} if its underlying graph is strongly
connected. It is {\it aperiodic} if for all node $x$, the gcd of the lengths
of all cycles visiting $x$ is $1$. Particularly, if for each $x$, $M(x,x)\neq
0$, the Markov chain is aperiodic. A Markov chain is {\it ergodic} if it is
irreducible and aperiodic. A Markov chain is symmetric if $M(x,y)=M(y,x)$
for all $x,y\in\Omega$. A distribution $\pi$ on $\Omega$ is a stationary
distribution for the Markov Chain if $\pi M =\pi$. It is known that an
ergodic Markov chain has a unique stationary
distribution~\cite[Chapter~1]{mixing}. Moreover, if the chain is symmetric,
this distribution is the uniform distribution on $\Omega$.

Given an ergodic Markov chain $X_0,\ldots,X_t,\ldots$ with stationary
distribution $\pi$, it is known that, whatever is the value of $X_0$, the
distribution of $X_t$ converges to $\pi$ when $t\to +\infty$:
$\max \tv{M^t(x,\cdot)-\pi}\underset{t\to +\infty}{\to} 0,$ where $\tv{}$
designates the total variation distance between two
distributions~\cite[Chapter~4]{mixing}. This leads to the Monte-Carlo
technique to randomly generate elements of $\Omega$ according to the
distribution $\pi$ by choosing arbitrarily $X_0$, computing $X_1,X_2,\ldots$, and
returning $X_t$ for $t$ large enough. The convergence rate is known to be
exponential, but computing the constants is a very difficult problem: choosing
the step $t$ to stop is a challenging question depending both on how close to
$\pi$ we want to be and on the convergence rate of $M^t(x,\cdot)$ to $\pi$.
For this purpose, the $\varepsilon$-{\it mixing time} of an ergodic Markov
chain of matrix $M$ and stationary distribution $\pi$ is defined by $t_{\rm
mix}(\varepsilon)= \min \{t\mid \max_{x\in \Omega}\tv{P_t(x,\cdot)-\pi}\leq \varepsilon\}.$
Computing mixing time bounds is a central question on Markov Chains.

The Metropolis-Hasting Algorithm is based on the Monte-Carlo technique and
aims at modifying the transition matrix of the Markov chain in order to
obtain a particular stationary distribution~\cite[Chapter 3]{mixing}.
Suppose that $M$ is an ergodic symmetric transition matrix of a symmetric
Markov chain on $\Omega$ and $\nu$ is a distribution on $\Omega$. The
transition matrix $P_\nu$ for $\nu$ is defined by:

$$
P_\nu(x,y)=
\left\{
\begin{array}{ll}
\min\left\{1,\frac{\nu(y)}{\nu(x)}\right\} M(x,y)& \text{if } x\neq y,\\
1-\sum_{z\neq x}\min\left\{1,\frac{\nu(z)}{\nu(x)}\right\} M(x,z)& \text{if } x= y.
\end{array}\right. 
$$

The chain defined by $P_\nu$ is called {\it the Metropolis Chain for $\nu$}.
It is known~\cite[Chapter 3]{mixing} that it is an ergodic Markov chain
whose stationary distribution is~$\nu$.


