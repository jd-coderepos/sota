

Finite automata play a central role in the field of formal language
theory and are intensively used to address algorithmic problems from
model-checking to text processing. Many automata-based algorithms have been
developed and are still  being developed. They propose new approaches and heuristics,
even for basic problems like the inclusion
problem\footnote{see \url{http://www.languageinclusion.org/}}.
Evaluating new algorithms is a challenging problem that cannot be addressed
only by the theoretical computation of the worst case complexity. Several
other complementary techniques can be used to measure the efficiency of an
algorithm: average complexity, generic case complexity, benchmarking,
evaluation on hard instances, evaluations on random instances. The first two
approaches are hard theoretical problems, particularly for algorithms
using heuristics and optmizations. Benchmarks, as well as known hard instances, are not always available. 
Nevertheless, in practice, random generation
of inputs is a good way to estimate the efficiency of an algorithm.
Designing uniform random generator for classes of finite automata is a
challenging problem that has been addressed mostly for deterministic
automata~\cite{DBLP:journals/tcs/ChamparnaudP05,DBLP:journals/tcs/BassinoN07,DBLP:journals/tcs/AlmeidaMR07,DBLP:conf/stacs/CarayolN12}
 -the interested reader is referred to~\cite{DBLP:conf/mfcs/Nicaud14} for a
recent survey. However, the problem of uniform random generation of non
deterministic automata (NFAs) is more complex, particularly for a random generation
up to isomorphism: the size of the automorphism group of a -state non
deterministic automata may vary from  to . 
For most applications, the complexity of the algorithm is related to the
structure of the automata, not to the names of the states:
randomly generated NFAs, regardless of the number of isomorphic automata, may therefore 
lead to an over representation of some isomorphism classes of automata.
Moreover, as discussed in the conclusion of~\cite{DBLP:conf/mfcs/Nicaud14},
the random generation of non deterministic automata has to be done on particular
subclasses of automata in order to obtain a better sampler for the
evaluation of algorithm (since most of the NFAs, for the uniform distribution,
will accept all words).  

The random generation of non deterministic automata is explored
in~\cite{DBLP:conf/lpar/TabakovV05} using random graph techniques (without
considering the obtained distribution relative to automata or to the
isomorphism classes). In~\cite{DBLP:conf/dcfs/ChamparnaudHPZ02}, the random
generation of NFAs is performed using bitstream generation.
In~\cite{DBLP:conf/lata/Nicaud09,DBLP:conf/fsttcs/NicaudPR10} NFAs are
obtained by the random generation of a regular expression and by
transforming it into an equivalent automaton using Glushkov Algorithm. The
use of Markov chains based techniques to randomly generate finite automata
was introduced
in~\cite{DBLP:conf/wia/CarninoF11,DBLP:journals/tcs/CarninoF12} for acyclic
automata.

\subsection{Contributions}
In this paper we address the problem of the uniform generation of elements
in several classes of non deterministic automata (up to isomorphism) by
using Monte-Carlo techniques. We propose this approach for the class of
-state non deterministic automata as well as for (a priori) more
interesting sub-classes. Determining the most interesting subclasses of NFAs
for testing practical applications is not the purpose of this paper. We
would like to point out that Monte Carlo approaches are very flexible and
that the results of this paper can be applied-adapted quite easily for many
classes of NFAs. More precisely:
\begin{enumerate}
\item We propose in Section~\ref{sec:random} several ergodic Markov Chains
whose stationary distributions are respectively uniform on the set of
-state NFAs, -state NFAs with a fixed maximal output degree and
-state NFAs with a fixed maximal output degree for each letter. In
addition, these chains can be adapted for these three classes restricted to
automata with a fixed single initial state. Moving into these Markov chains
can be done in time polynomial in .
\item The main idea of this paper is exposed in
Section~\ref{sec:algo-metro}, where we show how to modify these Markov
Chains using the Metropolis-Hastings Algorithm in order to obtain stationary
distributions that are uniform for the given classes of automata up to
isomorphism. Moving into these new Markov chains requires to compute the sizes
of the automorphism groups of the occurring NFAs, as explained in Section~\ref{sec:randomup}. 

\item The main contributions of this paper are given in
Section~\ref{sec:poly} and in Section~\ref{sec:label}, which can be
red independently. In Section~\ref{sec:poly}, we show a theoretical result for
the classes with a bounded output degree: moving into the modified Markov chains
(for a generation up to isomorphism) can be done in polynomial time. 

\item 
In Section~\ref{sec:label}, we explain how to use {\it labelings}, a classical
graph technique, to compute in practice the sizes of the automorphism
groups. The efficiency of the approach is illustrated with promising
experiments in Section~\ref{sec:xp}.
\end{enumerate}







\subsection{Theoretical Background on NFA}\label{sec:theoryNFA}


For a general reference on finite automata see~\cite{Hopcroft}.  In
this paper  is a fixed finite alphabet of cardinal
, and  is an integer satisfying .

A {\it non-deterministic automaton} (NFA) on  is a tuple  where  is a finite set of {\it states},  is a finite
alphabet,  is the set of
transitions,  is the set of final states and  is the set of initial states.  For any state  and any letter
, we denote by  the set of states  such that
.  The set of transitions  is {\it
deterministic} if for every pair  in  there is
at most one  such that . Two NFAs are
depicted on Fig.~\ref{fig:isom}.  A NFA is {\it complete} if for every
pair  in  there is at least one  such
that .  A {\it path} in a NFA is a sequence of
transitions  such that
. The word  is the {\it label} of the path
and  its {\it length}. If  and  the path is
successful. A word is {\it accepted} by a NFA if it's the label of a
successful path.  A NFA is {\it accessible} (resp.  {\it
co-accessible}) if for every state  there exists a path from an
initial state to  (resp. if for every state  there exists a path
from  to a final state).  A NFA is {\it trim} if it is both
accessible and co-accessible A {\it deterministic automaton} is a NFA
where  and whose set of transitions is deterministic.

Let  be the class of  finite automata whose set of states is
. We are interesting in several subclasses of .
\begin{itemize}
\item  is the subclass of  of trim finite automata.
\item   be the class of
finite automata in   such that, for each state , there is at most 
pairs  such that  is a transition: there are at most 
transitions outgoing each state. 
\item  be the class of
finite automata in   such that, for each state  and each
letter , there is at most 
states  such that  is a transition: for each letter, there are
at most  transitions labeled by this letter  outgoing each state.
\item Finally, for any class  of finite automata, we denote by  the
subclass of  of automata whose set of initial states is reduced to
.  
\end{itemize}

Examples of automata in One
has 
these classes are depicted on Fig.~\ref{fig:AutomatesClasses}.

\begin{figure}[!ht]
\centering
\begin{tikzpicture}
\node (1) [state,draw,initial, initial text=,fill=black!20] at (0,0) {} ;
\node (2) [state,draw,fill=black!20] at (3,0) {} ;
\node (3) [state,accepting,fill=black!20] at (6,0) {} ;

\path[->,>=latex] (1) edge[above] node {} (2);
\path[->,>=latex] (1) edge[above,bend left] node {} (3);
\path[->,>=latex] (1) edge[loop above] node {} (1);
\path[->,>=latex] (2) edge[above] node {} (3);
\path[->,>=latex] (2) edge[below, bend left] node {} (1);


\draw (3,-2) node {
\begin{tabular}{c}
Automaton  in , , \\
and in   , , .
\end{tabular}};













\begin{scope}[xshift=0cm,yshift=-5cm]
\node (1) [state,draw,initial, initial text=,fill=black!20] at (0,0) {} ;
\node (2) [state,draw,fill=black!20] at (3,0) {} ;
\node (3) [state,accepting,fill=black!20,initial, initial
text=,fill=black!20] at (6,0) {} ;

\path[->,>=latex] (1) edge[above] node {} (2);
\path[->,>=latex] (1) edge[loop above] node {} (1);
\path[->,>=latex] (2) edge[above,bend left] node {} (3);
\path[->,>=latex] (2) edge[below, bend left] node {} (1);

\draw (3,-2) node {
\begin{tabular}{c}
Automaton  in ,  and \\
but not in any doted class.
\end{tabular} };
\end{scope}
\end{tikzpicture}
\caption{Several Classes of Automata.}\label{fig:AutomatesClasses}
\end{figure}





Two NFAs are {\it isomorphic} if there exists a 
bijection between their sets of
states preserving the sets initial states, final states and transitions. More precisely,
let  and let  be a bijection from 
into a finite set . We denote by  the automaton
, with 
. 
Two automata  and  are isomorphic if there exists a bijection
 such that . 

\begin{figure}
\begin{center}
\begin{tikzpicture}[scale=1.2]
\node [circle,draw,initial,initial text=,fill=black!20](1) at (-2.5,0) {};
\node [circle,draw,fill=black!20](2) at (0,0) {};
\node [circle,draw,fill=black!20](3) at (0,-2) {};
\node [circle,draw,fill=black!20,accepting](4) at (-2.5,-2) {};

\path[->,>=latex] (1) edge[above] node {} (2);
\path[->,>=latex] (2) edge[bend left,left] node {} (3);
\path[->,>=latex] (3) edge[bend left,left] node {} (2);
\path[->,>=latex] (3) edge[left,above] node {} (4);
\path[->,>=latex] (4) edge[loop below] node {} (4);
\path[->,>=latex] (4) edge[left] node {} (1);

\node [circle,draw,initial,initial text=,fill=black!20](1b) at (2.5,0) {};
\node [circle,draw,fill=black!20](2b) at (5,-2) {};
\node [circle,draw,fill=black!20](3b) at (2.5,-2) {};
\node [circle,draw,fill=black!20,accepting](4b) at (5,-0) {};

\path[->,>=latex] (1b) edge[right] node {} (2b);
\path[->,>=latex] (2b) edge[bend left, below] node {} (3b);
\path[->,>=latex] (3b) edge[bend left, below] node {} (2b);
\path[->,>=latex] (3b) edge[left] node {} (4b);
\path[->,>=latex] (4b) edge[loop below] node {} (4b);
\path[->,>=latex] (4b) edge[above] node {} (1b);

\path[->,>=latex,dashed] (1) edge[above,color=black,bend left] node {} (1b);
\path[->,>=latex,dashed] (2) edge[above,color=black] node {} (2b);
\path[->,>=latex,dashed] (3) edge[above,color=black,bend left] node {} (3b);
\path[->,>=latex,dashed] (4) edge[above,color=black] node {} (4b);


\draw (-1,-3.5) node {};
\draw (3.5,-3.5) node {};

\draw (1,-4.5) node {.};
\end{tikzpicture}








\end{center}
\caption{Two Isomorphic Automata}\label{fig:isom}
\end{figure}

Two isomorphic NFAs have the same number of states and are equal, up to
 the states names. The relation {\it is isomorphic to} is an equivalence relation.
For instance, the two automata depicted on Fig.~\ref{fig:isom} are
isomorphic.An {\it automorphism} for a NFA is an isomorphism between this NFA and
itself. 
Given a NFA , the set of automorphisms of  is
a finite group denoted . For , 
 denotes the subset of   of automorphisms 
fixing each element of :  for each , .
Particularly , and  
 is reduce to the identity.
For instance, the  automorphism group of the 
automaton depicted on Fig.~\ref{fig:isom}(a) has two elements, the identity
and the isomorphism switching  and . 


The size of the automorphism group of a non deterministic -state
automaton may vary from  to . For instance, any deterministic trim
automaton whose states are all final has an  automorphism group reduce to
the identity. The non deterministic -state automaton with no transition
and where all states are both initial and final has for automorphism group
the symmetric group. 

The isomorphism problem consists in deciding whether two finite automata are
isomorphic. It is investigated for deterministic automata on different alphabet
in~\cite{DBLP:journals/siamcomp/Booth78} (with a different definition of
isomorphism). It is naturally closed to the same problem for directed graph
and the following result~\cite{DBLP:journals/jcss/Luks82} will be useful in
this paper.
\begin{theorem}\label{thm:luks}
Let  be a fixed positive integer. The isomorphism problem for directed
graphs with degree bounded by  is polynomial.
\end{theorem}

\subsection{Theoretical Background on Markov Chains}\label{sec:theoryMC}

For a general reference on Markov Chains see~\cite{mixing}. Basic
probability notions will not be defined in this paper. The reader is
referred for instance to~\cite{proba-and-computing}.

Let  be a finite set. A {\it Markov chain} on  is a sequence
 of random variables on  such that
 for all . A Markov
chain is defined by its {\it transition matrix} , which is a function
from  into  satisfying . The underlying graph of a Markov chain is the graph whose set of
vertices is  and there is an edge from  to  if .
A Markov chain is {\it irreducible} if its underlying graph is strongly
connected. It is {\it aperiodic} if for all node , the gcd of the lengths
of all cycles visiting  is . Particularly, if for each , , the Markov chain is aperiodic. A Markov chain is {\it ergodic} if it is
irreducible and aperiodic. A Markov chain is symmetric if 
for all . A distribution  on  is a stationary
distribution for the Markov Chain if . It is known that an
ergodic Markov chain has a unique stationary
distribution~\cite[Chapter~1]{mixing}. Moreover, if the chain is symmetric,
this distribution is the uniform distribution on .

Given an ergodic Markov chain  with stationary
distribution , it is known that, whatever is the value of , the
distribution of  converges to  when :
 where 
designates the total variation distance between two
distributions~\cite[Chapter~4]{mixing}. This leads to the Monte-Carlo
technique to randomly generate elements of  according to the
distribution  by choosing arbitrarily , computing , and
returning  for  large enough. The convergence rate is known to be
exponential, but computing the constants is a very difficult problem: choosing
the step  to stop is a challenging question depending both on how close to
 we want to be and on the convergence rate of  to .
For this purpose, the -{\it mixing time} of an ergodic Markov
chain of matrix  and stationary distribution  is defined by 
Computing mixing time bounds is a central question on Markov Chains.

The Metropolis-Hasting Algorithm is based on the Monte-Carlo technique and
aims at modifying the transition matrix of the Markov chain in order to
obtain a particular stationary distribution~\cite[Chapter 3]{mixing}.
Suppose that  is an ergodic symmetric transition matrix of a symmetric
Markov chain on  and  is a distribution on . The
transition matrix  for  is defined by:



The chain defined by  is called {\it the Metropolis Chain for }.
It is known~\cite[Chapter 3]{mixing} that it is an ergodic Markov chain
whose stationary distribution is~.


