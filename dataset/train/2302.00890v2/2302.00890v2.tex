

\documentclass{article}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} 

\usepackage{hyperref}
\usepackage{enumitem}
\usepackage[caption=false,farskip=0pt,labelfont={bf}]{subfig}





\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[preprint]{icml2023}
 


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{multirow}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\usepackage[textsize=tiny]{todonotes}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}

\def\dd{\mathrm{d}}
\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\icmltitlerunning{Neural Common Neighbor with Completion for Link Prediction}

\begin{document}

\twocolumn[
\icmltitle{Neural Common Neighbor with Completion for Link Prediction}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Xiyuan Wang}{pkuiai,pkusai}
\icmlauthor{Haotong Yang}{pkuiai,pkusai}
\icmlauthor{Muhan Zhang}{pkuiai,bigai}
\end{icmlauthorlist}

\icmlaffiliation{pkuiai}{Institute for Artificial Intelligence, Peking University}
\icmlaffiliation{pkusai}{School of Intelligence Science and Technology, Peking University}
\icmlaffiliation{bigai}{Beijing Institute for General Artificial Intelligence}

\icmlcorrespondingauthor{Muhan Zhang}{muhan@pku.edu.cn}

\icmlkeywords{Graph Neural Network, Link Prediction}

\vskip 0.3in
]





\printAffiliationsAndNotice{}

\begin{abstract}
Despite its outstanding performance in various graph tasks, vanilla Message Passing Neural Network (MPNN) usually fails in link prediction tasks, as it only uses representations of two individual target nodes and ignores the pairwise relation between them. To capture the pairwise relations, some models add manual features to the input graph and use the output of MPNN to produce pairwise representations. In contrast, others directly use manual features as pairwise representations. Though this simplification avoids applying a GNN to each link individually and thus improves scalability, these models still have much room for performance improvement due to the hand-crafted and unlearnable pairwise features. To upgrade performance while maintaining scalability, we propose Neural Common Neighbor (NCN), which uses learnable pairwise representations. To further boost NCN, we study the unobserved link problem. The incompleteness of the graph is ubiquitous and leads to distribution shifts between the training and test set, loss of common neighbor information, and performance degradation of models. Therefore, we propose two intervention methods: common neighbor completion and target link removal. Combining the two methods with NCN, we propose Neural Common Neighbor with Completion (NCNC). NCN and NCNC outperform recent strong baselines by large margins. NCNC achieves state-of-the-art performance in link prediction tasks. Our code is available at \url{https://github.com/GraphPKU/NeuralCommonNeighbor}.
\end{abstract}

\section{Introduction}
\label{sec::introduction}
Link prediction is a crucial task in graph machine learning. It has various real-world applications, such as recommender systems~\citep{Zhang2020}, knowledge graph completion~\citep{NBFNet}, and drug interaction prediction~\citep{DrugInteraction}. Graph Neural Networks have been used in link prediction. Among these GNNs, Graph Autoencoder (GAE)~\citep{GAE} is one representative method, which uses the representations of two target nodes produced by Message Passing Neural Network (MPNN)~\citep{MPNN} to predict the existence of the link. GAE achieves good performance on some datasets. However, traditional link prediction heuristics, including Common Neighbor (CN)~\citep{CommonNeighbor}, Resource Allocation (RA)~\citep{RA}, and Adamic-Adar (AA)~\citep{AA}, sometimes can outperform GAE/MPNN by large margins~\citep{zhang2021labeling}. Therefore, recent works have been trying to boost GNNs for link prediction~\citep{SEAL,NBFNet,Neo-GNN,Gsketch}. 

\citet{zhang2021labeling} notice that GAE only uses node representations and ignores pairwise relations between target nodes. For example, in Figure~\ref{fig:GAEfailure}, MPNN will produce exactly equal representations for node  as they are symmetric in the graph. So GAE will produce the same prediction for two links  and . However, their pairwise relations are different. For example,  and  have a common neighbor , while  and  do not have any. This suggests the key to boost MPNNs is capturing pairwise relations.
\begin{figure}
\vskip 0.1in
    \centering
    \includegraphics[width=0.25\textwidth]{GAEfailexample.pdf}
    \caption{The failure of MPNN in link prediction task.  and  have equal MPNN node representations due to symmetry. However, with different pairwise relations,  and  should have nonequal representations.}
    \label{fig:GAEfailure}
\vskip -0.2in
\end{figure}

Existing works boosting MPNNs vary in how they capture pairwise representations. SEAL~\citep{SEAL} adds target-link-specific hand-crafted features to the node features and modifies the input graph of MPNN, whose output node representations are then pooled to produce pairwise representations. Though it outperforms GAE significantly, SEAL has to rerun MPNN on a different graph for each target link, leading to high computation overhead. To accelerate it,  Neo-GNN~\citep{Neo-GNN} and BUDDY~\citep{Gsketch} decouple the pairwise representations from node representation learning. They directly use manual features as pairwise representations and only run MPNN on the original graph. Therefore, these models need to run MPNN only once for all target links and scale much better. However, their pairwise representations are oversimplified and still have much room for improvement. 

To upgrade performance, we propose Neural Common Neighbor (NCN). Similar to Neo-GNN and BUDDY, NCN runs MPNN on the original graph for node representations. However, instead of using hand-crafted features, NCN uses learnable and flexible pairwise representations, which sum common neighbors' node representations produced by MPNN. In experiments, NCN maintains scalability and outperforms existing models. 

As our second contribution, we analyze how the incompleteness of the input graph affects link prediction. Incompleteness of input graph is ubiquitous for link prediction, as the task itself is to predict unobserved edges which do not exist in the input graph. In this work, we empirically find that incompleteness leads to distribution shift between the training and test set and loss of common neighbor information. We intervene in the incompleteness to solve this problem. Specifically, we focus on two graph structural properties crucial to NCN, namely common neighbor and the existence of target link, and propose two intervention methods: Common Neighbor Completion (CNC) and Target Link Removal (TLR). CNC iteratively completes unobserved links with a link prediction model. TLR removes the target links from the input graph. In experiments, our intervention methods further improve the performance of NCN.

In conclusion, our contributions are as follows:
\begin{itemize}[itemsep=0pt,topsep=0pt,parsep=0pt]
    \item We propose Neural Common Neighbor (NCN) to boost link prediction with learnable pairwise representations. NCN outperforms existing models and maintains the scalability.
    \item We analyze how graph incompleteness hampers link prediction models. 
    To alleviate the unobserved link problem, we intervene in the incompleteness and propose two methods: target link removal and common neighbor completion. 
    \item With these methods, we further improve NCN and propose Neural Common Neighbor with Completion (NCNC). NCNC achieves state-of-the-art performance in link prediction tasks. 
\end{itemize}
\section{Preliminaries}
We consider an undirected graph , where  is the set of  nodes,  is the set of edges,  is node feature matrix whose -th row  is of node , and adjacency matrix  is a symmetric matrix and defined as follows

where  can also be replaced with edge weight. The \textit{degree} of node  is . Node 's neighbors are nodes connected to , . For simplicity of notations, we use  to denote  when  is fixed. \textit{Common neighbor} means nodes connected to both  and : .

\paragraph{High Order Neighbor of Graph.} We define  as the adjacency matrix of the \textit{high-order graph}, where  is a neighbor of  if there is a walk of length  between them in the original graph.  denotes the set of 's neighbors in the high-order graph.  denotes the set of nodes whose shortest path distance to  in graph  is . Existing works define \textit{high-order neighbors} as either  or . Most generally, the neighborhood of  can be expressed as , returning all nodes with shortest path distance  to  in the high-order graph . For simplicity of notations, we use  to denote  when  is fixed, and let , . Given a target link , their neighborbood overlap is given by , and their neighborhood difference is given by .

\paragraph{Message Passing Neural Network.}
Message passing neural network (MPNN)~\citep{MPNN}, composed of message passing layers, is a common GNN framework. The  layer () can be formulated as follows.

where  is the representation of node  at the  layer,   are some functions like multi-layer perceptron (MLP), and  denotes an aggregation function like sum and max. For each node, a message passing layer aggregates information from neighbors to update the representation of the node. The initial node representation  is node feature . Node representations produced by MPNN are the output of the last message passing layer, . 

\section{Related Work}
\subsection{Link Prediction Heuristics}\label{sec:heuristic}
Research on developing hand-crafted graph structure features (heuristics) for link prediction has been longstanding before GNN methods~\citep{liben2003link}. The most relevant heuristics to our paper are Common Neighbor (CN)~\citep{CommonNeighbor}, Resource Allocation (RA)~\citep{RA}, and Adamic Adar (AA)~\citep{AA}. Given an graph and a link  in it, they produce a score  by pooling representations of the common neighbors of  and . Links with higher scores are more likely to exist. Equations~\ref{equ:heuristics} show the score function used in these different heuristics.

    S_{\text{CN}}(i,j)&=\sum_{u\in N(i)\bigcap N(j)} 1,\\
    S_{\text{RA}}(i,j)&=\sum_{u\in N(i)\bigcap N(j)} \frac{1}{d(u)},\\
    S_{\text{AA}}(i,j)&=\sum_{u\in N(i)\bigcap N(j)} \frac{1}{\log d(u)}.

Specifically, CN simply uses  as the node representation, while RA and AA further use node degree information and usually outperform CN.

\subsection{GNNs for Link Prediction}\label{sec:relatedwork:LPmodel}
Another research direction is using graph neural networks for link prediction, where 
Graph Autoencoder (GAE)~\citep{GAE} is the representative one. It predicts an edge with node embeddings produced by MPNN as follows,

where  is the existence probability of link  predicted by GAE. Note that GAE ignores pairwise relations completely as we have shown in Section~\ref{sec::introduction} and Figure~\ref{fig:GAEfailure}, so CN, RA, and AA outperform GAE in many datasets.

Recently, many methods have been proposed to help GNNs to capture pairwise relations. SEAL~\citep{SEAL} adds target-link-specific distance features to the input graph of MPNN and outperforms GAE, CN, RA, and AA significantly. However, SEAL suffers from high computation overhead. Unlike GAE, which only needs to run MPNN once to predict all target links, SEAL must rerun MPNN for each target link. 

To improve the scalability, \citet{Neo-GNN} and \citet{Gsketch} directly add manual pairwise features to the node representations produced by MPNN instead of modifying the input graph of MPNN. 
Inspired by traditional heuristics, Neo-GNN~\citep{Neo-GNN} further utilizes high-order neighborhood overlaps and, moreover, uses a learnable node degree function instead of the fixed functions in RA and AA, which leads to significant outperformance over the heuristics. Another recent method is called BUDDY~\citep{Gsketch}, 
which further distinguishes the pairwise feature as the overlap feature and difference feature, but ignores node degree information as CN.

Both Neo-GNN and BUDDY only run MPNN once and achieve similar computation overhead to GAE. However, one key limitation is that their pairwise features are hardly learnable and therefore inflexible. Additionally, they are completely decoupled from the node features/representations produced by the MPNN. These problems restrict their performance.


\subsection{Incompleteness of Graph}

As the link prediction task is to predict unobserved edges, the observed graph is deemed to be incomplete. However, existing methods relying heavily on graph structures such as CN might behave very differently under different graph completeness levels. Some existing works also notice this problem. \citet{OpenworldKG} analyze how unobserved links distort the evaluation score. They focus on metrics and benchmark design. In contrast, we for the first time study the performance degradation caused by incompleteness as well as how to alleviate it. Different from our settings, \citet{entityopenworld} study unobserved nodes and propose an inductive learning method. Their method uses no intervention method and is completely different from ours.  



\section{Neural Common Neighbor}\label{sec:NCN}


Recent link prediction models use manual and inflexible pairwise features, which restrict their performance and generalization. In this section, we first propose a more general framework, which can include the heuristics introduced in Section~\ref{sec:heuristic} as well as Neo-GNN and BUDDY in Section~\ref{sec:relatedwork:LPmodel}.
Then, based on the framework, we propose a realization called
Neural Common Neighbor (NCN) by replacing the manual features with an MPNN to achieve better capacity.


\iffalse
In this section, we first design some toy models to illustrate the necessity of combining node representations and pairwise features and then propose our Neural Common Neighbor (NCN) model. 
\begin{figure}[t]
    \vskip 0.2in
    \centering    \includegraphics[width=0.45\textwidth]{FeatCN.pdf}
    \caption{A comparison of four heuristics on ogbl-collab dataset~\citep{OGB}. The prediction is produced by logistic regression with heuristics as input.}
    \label{fig:visheuristic}
    \vskip -0.2in
\end{figure}
\paragraph{Motivation from toy models.} All heuristics in Section~\ref{sec:heuristic} ignore node features. For featured graph, we design four heuristics. Given the adjacency matrix  and the node feature , for a target link , the heuristics are as follows,
\begin{itemize}[itemsep=0pt,topsep=0pt,parsep=0pt]
    \item Feat: . It only use target node features.
    \item CN: . It only use pairwise features.
    \item Feat + CN: . Node and pairwise features are used but separated.
    \item FeatCN: . It combines the pairwise features and node features.
\end{itemize}
These heuristics are compared in Figure~\ref{fig:visheuristic}. Feat+CN outperforms CN and Feat significantly, and FeatCN achieves an even higher score. The success of FeatCN validates the necessity of combining node features and pairwise features.
\fi

\begin{table*}[th]
    \centering
    \caption{How the general framework shown in Equation~(\ref{equ:framework}) include several models.}\label{tab:framework}
\vskip 0.15in
\begin{small}
\begin{tabular}{cccccc}
\toprule
 & Model & Neighbor selection & Set operator  &  &  \\ \midrule
\multirow{3}{*}{Heuristics} & CN & First-order & Intersection &  &  \\
 & RA & First-order & Intersection &  &  \\
 & AA & First-order & Intersection &  &  \\
 \midrule
\multirow{3}{*}{GNN} & Neo-GNN & High-order & Intersection &  & MLP \\
 & BUDDY & High-order & Intersection \& Difference &  &  \\
 & \textbf{NCN} & First-order & Intersection &  & \textbf{MPNN}\\
 \bottomrule
\end{tabular}

\end{small}
\end{table*}


\subsection{A General Framework of Pairwise Feature}
Neo-GNN uses the higher-order pairwise feature, which is a weighted summation in different high-order neighborhood as shown in Equation~(\ref{equ:neo-gnn-weightedsum}).

where  and  are hyperparameters, and  is the feature from high-order neighbors  and  defined as follows.

where  is a learnable function of node degree .
The pairwise feature  pools degree features of nodes within high-order neighborhood overlap. 

BUDDY further uses the high-order difference feature. The pair-wise feature is composed of  overlap features and  difference features as follows.

where  is a hyperparameter, and functions  and  measure high-order neighborhood overlaps and differences, respectively, which are defined as follows.

a_{l_1,l_2}(i,j)&= \sum_{u\in N_{l_1}(i)\bigcap  N_{l_2}(j)} 1,\\
b_l(i,j) &= \sum_{u\in N_{l}(i)-  \bigcup_{l'=1}^k N_{l'}(j)} 1.

How to use the pairwise feature is the key to link prediction models. Let's focus on the form of the pairwise feature: score function  in heuristics, the high-order overlap feature  in Neo-GNN, and high-order overlap feature  and difference feature  in BUDDY.
All these features can be summarized into the following general framework

where  and  denote the general neighborhood of  and  respectively,  is a set operator, and  are node (degree) and weight functions, respectively. Table~\ref{tab:framework} shows how the general framework contains the three heurisitics CN, RA and AA as well as the two GNN models, Neo-GNN and BUDDY.  Models using high-order neighbors are called \textit{high-order models} and \textit{first-order models} use first-order neighbors only.

\subsection{Neural Common Neighbor}
Now we introduce Neural Common Neighbor (NCN). We first notice that one of the main differences between existing models is the node function . However, existing models all use inflexible constant or degree functions, which fail to capture more refined node features such as multi-hop structure and attribute information. Therefore, we propose to use MPNN to replace , which leads to the following pairwise feature

MPNN is a more powerful and flexible feature extractor than manually-designed degree-based feature functions. Theoretically, because MPNN can fit arbitrary degree functions ~\citep{HowPowerfulAreGNNs}, this model is \textit{strictly more powerful} than the heuristics, Neo-GNN and BUDDY.


The next question is the selection of the neighborhood and the set operator . Surprisingly, we find the improvement given by explicit higher-order neighbors is marginal after we import the MPNN (see Section~\ref{sec::abl}). We believe that it is because the higher-order information could be implicitly learned by the MPNN. Considering the scalability, we choose to only keep the first-order neighbor and set the operator only to intersection, which leads to our NCN model:

where  and  are constants and ignored. 

As our first contribution, NCN is a simple but powerful model to capture pairwise features. It is an implicit high-order model by aggregating first-order common neighbors each has its higher-order information implicitly learned by an MPNN. This trick effectively controls the model's time complexity, making it close to Neo-GNN and BUDDY but having a more powerful and flexible feature extractor. More analysis on time complexity can be found in Appendix~\ref{app:complexity}. 

\begin{figure}[h]
\centering
    \subfloat{\includegraphics[width=0.46\textwidth]{mergecollab2.pdf}}
    \quad
   \subfloat{ \includegraphics[width=0.46\textwidth]{mergeCora2.pdf}}
    \vskip -0.1in
    \caption{
    Visualization of incompleteness on datasets. The incomplete graph only contains edges in the training set, and the complete graph further contains edges in the validation and test set.
    (a) and (b) visualize the ogbl-collab dataset. (c) and (d) visualize the Cora dataset. (a) and (c) are for distributions of the number of common neighbors of the training edges and test edges. (b) and (d) show performance of CN on the training set and test set. } 
    \label{fig:cndist}
    \vskip -0.2in
\end{figure}

\section{Neural Common Neighbor with Completion}\label{sec::intervene}
Incompleteness of graph is ubiquitous in link prediction tasks, as the task is to predict unobserved edges. However, few works have studied this problem. This section first shows that the incompleteness may lead to distribution shifts between the training and test sets, loss of common neighbor information, and therefore performance degradation of models. Then, we intervene in the incompleteness and propose two methods, namely \textbf{common neighbor completion} and \textbf{target link removal}. With these intervention methods, we further improve NCN and propose Neural Common Neighbor with Completion (NCNC).

\subsection{Incompleteness Visualization}
\label{sec:incompletenessvis}
Assume the graph only containing edges in the training set is the input \textit{incomplete} graph, while the graph containing edges in the training, validation and test sets is the ground-truth \textit{complete} graph. We focus on the following questions:
\begin{center}
    \textit{(1) What is different between the complete and incomplete graphs, and (2) whether the difference leads to performance degradation?}
\end{center}
To answer these questions, we investigate two common datasets: ogbl-collab~\citep{OGB} and Cora~\citep{Cora}. Because the common neighbor information is crucial for link prediction as well as our NCN model, we visualize the distribution of the number of common neighbors of training/test edges in the complete/incomplete graphs separately. Firstly, by comparing the blue and green lines in Figure~\ref{fig:cndist}(a), we find there is a significant \textit{distribution shift} between the training and test sets in the incomplete graph of the ogbl-collab dataset, and the shift disappears when the graph is complete (the red and orange lines), which suggests this shift is due to the incompleteness. Such a significant distribution shift between training and test links in the input graph might cause \textbf{difficulty in model generalization}.

In contrast, there is no such distribution shift between training and test edges in Cora (Figure~\ref{fig:cndist}(c)). The reason could be due to the different dataset split methods. Obgl-collab splits the training and test edges according to the timestamp of edges, and the edges of the test set are all in the same year. Thus, edges in the test set will have stronger correlations with other test edges than with edges in the training set. Therefore, the test edges may have fewer common neighbors in the incomplete graph than the training edges. On the contrary, the Cora dataset randomly chooses the test set and thus avoids the distribution shift. 

Another phenomenon is the decrease of test edges' common neighbors in the incomplete graph, which can be see by the blue and green lines in Figure~\ref{fig:cndist}(c). Comparing the incomplete and complete graphs for the same training/test set, there are fewer common neighbors in the incomplete graph, which indicates \textit{loss of common neighbor information} due to the incompleteness. 

In conclusion, for the first question, the incompleteness could lead to at least the following two problems:
\begin{itemize}
[itemsep=0pt,topsep=0pt,parsep=0pt]
    \item \textbf{Distribution shift}: with certain dataset splits, there could be a distribution shift between the training and test sets due to the incompleteness.
    \item \textbf{Loss of common neighbor information}: there could be fewer common neighbors in the incomplete graph. 
\end{itemize}

\textbf{Remark.}~~  On the one hand, we acknowledge that there is no guarantee that these two phenomena fully account for the difference between complete and incomplete graphs. On the other hand, these two problems do not necessarily appear in every dataset, and the significance of these problems could also vary in different datasets according to the data type, generation method, data split, and so on. 


To verify whether the incompletness causes \textbf{performance degradation} of link prediction models, we evaluate CN by ranking the training/test edges against the negative test links, under both complete and incomplete graphs, on both datasets. We also use Hits@K metrics as in ~\citep{Gsketch}.
Though CN is not learnable and does not involve generalization, it has no data leakage problem, as
CN computation for a target edge does not involve the edge itself. Moreover, CN can reveal how incompleteness changes the input graph structure of other learnable models, so it is a good reference here. The performance is shown in Figure~\ref{fig:cndist}(b) and (d). On both datasets, CN's performance for test edges degrades a lot between complete and incomplete graphs (green and orange bars), which verifies the performance degradation. This phenomenon indicates that we could have gotten a much better link prediction model if the input graph were more complete.


Comparing the blue and green bars in Figure~\ref{fig:cndist}(b) (ogbl-collab) and Figure~\ref{fig:cndist}(d) (Cora), we see that there is a huge performance difference between training and test sets in the incomplete ogbl-collab, which disappears in the incomplete Cora. This aligns well with the \textit{distribution shift} problem discussed above, which is less significant under random data split. 

In conclusion, the incompleteness problem leads to distribution shift and loss of common neighbor information and causes performance degradation of link prediction models. In practice, we can never really evaluate CN in the complete graph but only in the incomplete input graph. Thus, the above phenomena need other alleviation methods, which will be discussed in later subsections.






\subsection{Common Neighbor Completion}\label{sec:CNC} Motivated by the above analysis, our strategy is to first complement the input graph softly using a link prediction model (intervention on the incompleteness), and then apply the model again on the more complete graph to give final predictions. We use NCN as the link prediction model. To illustrate our model, we use the causal graphs~\citep{CausalGraph} in Figure~\ref{fig:causalgraph}(a) and \ref{fig:causalgraph}(b). In Figure~\ref{fig:causalgraph}(a),  denotes the ground-truth complete graph,  is a random variable determining link 's incompleteness ( means link  is missing from the complete graph), and  together determine the link existence  in the input graph. In Figure~\ref{fig:causalgraph}(b),  is a random variable indicating whether node  is a common neighbor of  in the input graph, which is determined by  together. 

To alleviate the incompleteness, we intervene in  by setting , so that , i.e., we want to use the edges  and  in the complete graph to compute the common neighbor . However, since  and  are unknown, we choose to predict them using NCN first. Instead of predicting the existence, we let NCN output a probability instead, and the final probability of  being a common neighbor of  is modeled as follows:

where  is the sigmoid function. If both edges  and  are observed,  must be a common neighbor of . If one of  and  is unobserved, we use NCN to predict its link existence probability, which is also used as the probability of  being a common neighbor. The fourth case where both  and  are unobserved has a much less probability, so we just set the probability to .

\begin{figure}[t]
\vskip 0.0in
    \centering
\includegraphics[width=0.45\textwidth]{Intervention.pdf}
\vskip -0.1in
    \caption{
    (a) Causal graph of the link existence and incompleteness.
    (b) Causal graph of common neighbor and incompleteness. : Edge incompleteness variable. : edge is observed. : edge is unobserved.  varies with the target link. : the complete graph. Edge incompleteness and the complete graph jointly determine the observed graph. : the existence of link  in the observed graph. : whether  is a common neighbor of  in the observed graph.  iff both links  and  exist.
    (c) An example of our two intervention methods.  is the target link. Solid lines mean observed edges. Dotted lines mean links in the full graph affected by incompleteness.  and  are observed in the training set while unobserved in the test set. TLR (target link removal) removes the target link . CNC (common neighbor completion) completes the observed graph and thus transforms the dotted line  into a solid line. With the two methods, dotted lines are eliminated, and the incompleteness problem is alleviated.}
    \label{fig:causalgraph}
\vskip -0.2in
\end{figure}

We call the above intervention trick Common Neighbor Completion (CNC). In fact, it can be combined with any link prediction method. After CNC, we apply NCN again to the ``completed'' graph where soft common neighbor weight  is used, leading to the final model which we call\textbf{ Neural Common Neighbor with Completion (NCNC)}:

Furthermore, this strategy can be extended to a iterative algorithm. Starting from the original graph , we \textit{iteratively} complete the graph to get the  from  until the final iteration . We call this model NCNC with maximum number of iterations  (NCNC-). For the first iteration , NCNC- is the same as NCN. At iteration , NCNC- of the target link , denoted by , has the following form

where  is the link existence probability produced by NCNC at iteration , and  is the node representation of  produced by MPNN. 

The time complexity of NCNC- is loosely bounded by  times of NCN, where  is the maximum node degree. More analysis can be found in Appendix~\ref{app:complexity}. In practice,  is enough for most datasets.


\iffalse

where the completion probability  is 

where the link existence probability is produced by an NCNC model with  maximum number of iterations.
\fi

\subsection{Target Link Removal.}\label{sec:TLR}
In Section~\ref{sec:CNC}, we pointed out that the incompleteness  affects the common neighbor and degrades the performance of link prediction models. In this section, we will point out that the incompleteness will also affect the target link directly and cause bias between training and test sets. 
As shown in Figure~\ref{fig:causalgraph}(a), the complete graph and the incompleteness jointly determine the target link existence. Links in the training set are all observed (), while other links to predict are all unobserved (), leading to another distribution shift, which is different from the one we have shown in Section~\ref{sec:incompletenessvis}. However, in the test set, we cannot know whether the absence of the target link is due to the incompleteness  or the actual ground-truth . Thus, we cannot solve the problem by completing the graph. Therefore, we always intervene in  of the target link and set it to  in both the training and test sets, which is equivalent to always \textit{removing the target link}  from the input graph  to get . We call this intervention trick Target Link Removal (TLR). Note that TLR is also used in previous works such as \citet{SEAL} but never studied from a causal perspective. The total intervention strategy combining CNC and TLR is shown in Figure~\ref{fig:causalgraph}(c). We use the TLR trick in both NCN and NCNC models by just replacing  with .
In implementation, ideally we need to perform TLR for each link, but for scalability we simultaneously remove all edges in a batch from the input graph during the training stage. Experiments show that this method achieves up to  performance gain in real-world tasks (see Section~\ref{sec::abl}).



\section{Experiment}
\begin{table*}[th]
    \centering
    \caption{Results on link prediction benchmarks. The format is average score  standard deviation. OOM means out of GPU memory.}\label{tab:main_results}
\vskip 0.15in
\small{
    \begin{tabular}{lccccccc}
    \toprule
         &
         \textbf{Cora} &  
         \textbf{Citeseer} & 
         \textbf{Pubmed} &
         \textbf{Collab} &
         \textbf{PPA} &
         \textbf{Citation2} 
         &\textbf{DDI} 
         \\
\midrule
          Metric &

          HR@100 &
          HR@100 & 
          HR@100 &
          HR@50 &
          HR@100 &
          MRR 
          &HR@20
         \\ 
         
         \midrule
          
         \textbf{CN} & 
         & 
         & 
         &
         &
         &
         
         &
         \\

        \textbf{AA} & 
        &
        &
        &
        &
        &
        
        &
        \\
        
        
        \textbf{RA} &
         &
        &
        & 
        &
         & 
        
        &
        \\ \midrule
        
\iffalse        
        \textbf{transE} &
        & 
        &
        & 
        & 
         &
         
        & 
        \\
          
        \textbf{complEx} & 
        & 
        & 
        &
        & 
         &
         
        &
        \\
        
        \textbf{DistMult} & 
        & 
        & 
        &
        & 
        &
        
        &
         \\
        \midrule
\fi 
        
        \textbf{GCN} & 
        & 
        &
        & 
        &
        &
        
        &
         \\
        \textbf{SAGE} & 
        & 
        & 
        & 
        & 
        &
        
        & 
        \\ 
        \midrule

        \textbf{SEAL} & 
        & 
         & 
        &
        & 
        & 
        
        &
        \\ 
        
         \textbf{NBFnet} & 
         &
         &
         &
         OOM&
         OOM&
         OOM
         &
         \\  
        \midrule
        \textbf{Neo-GNN} & 
         &
         &
         &
        & 
        & 
        
        & 
        \\ 
        

\iffalse
         \textbf{ELPH} & 
         &
         &
          &
         &
         OOM&
         OOM
         &
         \\ 
\fi         
         \textbf{BUDDY} & 
         &
         &
         &
         &
         & 
         
         & 
         \\
         \midrule
         \textbf{NCN} &
         &
         &
         &
         &
         & 
         
         & 
         \\
         \textbf{NCNC} &
         &
         &
         &
         &
         & 
         
         &
         \\ \bottomrule
\end{tabular}
}
\end{table*}
\begin{table*}[th]
    \centering
    \vskip -0.15in
    \caption{Ablation analysis on link prediction benchmarks. The format is average score  standard deviation. }\label{tab:abl}
\vskip 0.15in
\small{    \begin{tabular}{lccccccc}
    \toprule
         &
         \textbf{Cora} &  
         \textbf{Citeseer} & 
         \textbf{Pubmed} &
         \textbf{Collab} &
         \textbf{PPA} &
         \textbf{Citation2} 
         &\textbf{DDI} 
         \\
\midrule
          Metric &

          HR@100 &
          HR@100 & 
          HR@100 &
          HR@50 &
          HR@100 &
          MRR 
          &HR@20
         \\ 
         \midrule
         \iffalse
         \textbf{NoTarMaskGAE}& 
         & 
         & 
         & TODO
         &TODO&
         TODO\\
         \fi
         \textbf{CN} & 
         & 
         & 
         &
         &
         &
         
         &
         \\
\iffalse
         \textbf{FeatCN} &
         & 
         & 
         & 
         &&
         \\
\fi
         \textbf{GAE}& 
         & 
         & 
         & 
         & &
         
         &\\
         \textbf{GAE+CN} &
         & 
         & 
         & 
         &&
         
         & \\
\iffalse    \textbf{GAE+CNC} & 
         & 
         & 
         & 
         & &
         
         & \\
\fi
\midrule
         \textbf{NCN2} &
         &
         &
         &
         &
         OOM& 
         OOM&
         OOM\\
         \textbf{NCN-diff} &&
         &
         &
         &
         & 
         &
         
         \\
\midrule
         \textbf{NCN} &
         &
         &
         &
         &
         & 
         &
         \\
         \textbf{NoTLR}& 
         & 
         & 
         & 
         & 
         & 
         &  
         \\
         \iffalse
         \textbf{GAE}& 
         & 
         & 
         & 
         & &
         
         &\\
         \textbf{GAE-TLR}& 
         & 
         & 
         & 
         & 
         & 
         & 
         \\
         \fi
         \midrule
         \textbf{NCNC} &
         &
         &
         &
         &
         & 
          &
         
         \\
         \textbf{NCNC-2} & 
         & 
         & 
         & 
         &&
         &
         
         \\ 
         \bottomrule
\end{tabular}}
\vskip -0.1in
\end{table*}

In this section, we evaluate the performance of NCN and NCNC. Our code is available in the supplementary material. Detailed experimental settings are included in Appendix~\ref{app::experimentsetting}. 

We use seven link prediction benchmarks. Three are Planetoid citation networks: Cora, Citeseer, and Pubmed~\citep{Cora}. Others are from Open Graph Benchmark~\citep{OGB}: ogbl-collab, ogbl-ppa, ogbl-citation2, and ogbl-ddi. Their statistics and splits are shown in Appendix~\ref{app:data}.

\subsection{Evaluation on Real-World Datasets}

 Our baselines include traditional heuristics: CN~\citep{CommonNeighbor}, RA~\citep{RA}, AA~\citep{AA}; vanilla GNN with no pairwise feature: GCN~\citep{GCN}, SAGE~\citep{GraphSage}; GNNs modifying the input graph of MPNN: SEAL~\citep{SEAL}, NBFNet~\citep{NBFNet}; and GNNs with manual features as pairwise representations: Neo-GNN~\citep{Neo-GNN}, BUDDY~\citep{Gsketch}. Their results are from ~\citep{Gsketch}. Our models are NCN and NCNC. We set the maximum number of iterations of NCNC to . Their architectures are detailed in Appendix~\ref{app:arch}.

Results are shown in Table~\ref{tab:main_results}. NCN beats all baselines on 5/7 datasets and achieves  score increase on average compared with BUDDY, the most competitive baseline. On the two datasets left, NCN can still outperform all baselines other than BUDDY. NCN's outstanding performance validates the advantage of learnable pairwise features over manual ones. NCNC further leads to  performance gain, \textbf{outperforming all baselines on all datasets}. Notably, NCNC achives  HR@100 on ogbl-ppa, outperforming the strongest baseline BUDDY by over  absolute score.



\begin{figure}[t]
\vskip 0.05in
    \centering
    \includegraphics[width=0.499\textwidth]{collabtime.pdf}
    \vskip -0.05in
    \caption{Inference time and GPU memory on ogbl-collab. The process we measure includes preprocessing, MPNN, and predicting one batch of test links. As SEAL has out-of-memory problem with large batch sizes, we only use small batch sizes for SEAL.}\label{fig:time}
\vskip -0.25in
\end{figure}
\subsection{Scalability}\label{sec:scalability}
We compare the inference time and GPU memory on ogbl-collab in Figure~\ref{fig:time}. Our strongest baseline BUDDY~\citep{Gsketch}'s code is not released so is not compared. NCN and NCNC have a \textbf{similar computation overhead to GAE}, as they both need to run MPNN only once. In contrast, SEAL, which reruns MPNN for each target link, takes  times more time compared with NCN with a small batch size , and the disadvantage will be more significant with a larger batch size. To our surprise, Neo-GNN is even slower than SEAL, even if it only needs to run MPNN once. The reason is that it uses pairwise features that are much more complex and time-consuming than CN. We also conduct scalability comparison on other datasets and observe the same results (see Appendix~\ref{app:time}).

\subsection{Ablation Analysis}\label{sec::abl}
To validate the design of Neural Common Neighbor, we conduct a thorough ablation analysis (see Table~\ref{tab:abl}). 

GAE uses node representations only. GAE+CN further utilizes CN as pairwise features. On OGB datasets, GAE+CN outperforms GAE by , and NCN further achieves  higher score than GAE+CN, which implies that the learnable pairwise representations we propose are effective. On Planetoid datasets though, the improvement is minimal, which indicates that these datasets require less common neighbor information. 

NCN-diff is NCN plus neighborhood difference information, and NCN2 is NCN with high-order neighborhood overlap. For a target link , compared with NCN, NCN-diff further sums representations of nodes in  and , and NCN2 further uses  and . As we can see, NCN, NCN-diff and NCN2 have similar performance on most datasets, verifying that first-order neighborhood overlap may be enough. The low score of NCN-diff on DDI might be because the DDI's high node degree makes neighborhood difference noisy and uninformative.

NoTLR is NCN without TLR. On average, TLR leads to  performance gain. NCNC-2 is NCNC with maximum number of iterations . It achieves similar performance to NCNC. Therefore, setting maximum number of iterations to  is enough for our datasets. Both intervention methods boost NCN significantly.

\section{Conclusion}
We propose Neural Common Neighbor (NCN), a scalable and powerful model for link prediction leveraging learnable pairwise features. Furthermore, we study the graph incompleteness problem. By visualizing the  distribution shift and performance degradation caused by incompleteness, we propose two tricks, target link removal and common neighbor completion. Combining NCN with the two tricks, our final model NCNC outperforms state-of-the-art baselines on all the datasets in both speed and performance. 

\newpage
\bibliography{example_paper}
\bibliographystyle{icml2023}


\newpage
\appendix
\onecolumn
\section{Dataset Statistics}\label{app:data}
The statistics of each dataset are shown in Table~\ref{tab:datasets}.

\begin{table*}[h]
    \centering
    \caption{Statistics of dataset.}\label{tab:datasets}
    \vskip 0.15in
    \begin{tabular}{l ccccccc}
    \toprule 
         &
         \textbf{Cora} &  
         \textbf{Citeseer} & 
         \textbf{Pubmed} &
         \textbf{Collab} &
         \textbf{PPA} &
         \textbf{DDI} &
         \textbf{Citation2}
        \\
\midrule
         
                  \#Nodes &

         2,708 & 
         3,327 &
         18,717 &
         235,868 &
         576,289 &
         4,267 &
         2,927,963
          \\
         
         \#Edges &
         5,278 & 
         4,676 &
         44,327 &
         1,285,465 &
         30,326,273 &
         1,334,889 &
         30,561,187
          \\

         splits &

         random &
         random & 
         random &
         fixed &
         fixed &
         fixed &
         fixed \\
          
         average degree &
         3.9 &
         2.74 & 
         4.5 &
         5.45 &
         52.62 &
         312.84 &
         10.44
        \\
         
         \bottomrule
\end{tabular}

\label{tab:subgraph properties}
\end{table*}
Random splits use  edges for training/validation/test set respectively. Different from others, the collab dataset allows using validation edges as input on test set. 

\section{Model Architecture}\label{app:arch}

This section concludes our methods in Section~\ref{sec:NCN} and Section~\ref{sec::intervene}.

Given an input graph , a node feature matrix , and target links , our models consist of three steps: target link removal, MPNN, and predictor. NCN and NCNC only differ in the last step.

\paragraph{Target link removal.} We make no changes to the input graph in the validation and test set where the target links are unobserved. In the training set, we remove target links from . Let  denote the processed graph. This trick is detailed in Section~\ref{sec::intervene}. Note that target link removal is not used on ogbl-citation2 dataset.

\paragraph{MPNN.} We use MPNN to produce node representations . For each node ,

For all target links, MPNN needs to run only once.

\paragraph{Predictor.}
Predictors use the node representations and graph structure to produce link prediction. Link representations of NCN are as follows,

where  means concatenation,  is the  representation of link .  composed of two components: two nodes' presentation  and representations of nodes within the common neighbor set. The former component is often used in link prediction models~\citep{GAE, Neo-GNN, Gsketch}, while we propose the latter one for the first time. Link representations are then used to produce link existence probability.

where  is the probability that link  exists. 

NCNC has a similar form. The only difference is that  in Equation~(\ref{equ:ncn}) is replaced with the follow form:

where  is the link existence probability produced by NCNC. NCNC- will predict unobserved common neighbor with NCNC-. In the end, NCNC- is NCN. NCNC complete common neighbor during both the training and test stage. 

\section{Time Complexity}\label{app:complexity}
\begin{table}[t]
    \centering
    \caption{Scalability comparison. : the complexity of hash function in BUDDY, . : the dimension of node representations. When predicting the  target links, time complexity of existing models can be expressed as .}\label{tab:complexity}
    \vskip 0.15in
\small{
    \begin{tabular}{lcc}
    \toprule
    Method& B & C\\
    \midrule
    GAE & &  \\ 
    SEAL & &  \\
    Neo-GNN &  &\\
    BUDDY & &  \\
    NCN & &  \\ 
    NCNC- & &  \\
    NCNC- &  & \\
    \bottomrule
\end{tabular}
}
\vskip -0.1in
\end{table}
\begin{figure}[h]
\centering
    \subfloat[Cora]{\includegraphics[width=0.46\textwidth]{coratime.pdf}}
   \subfloat[Citeseer]{ \includegraphics[width=0.46\textwidth]{citeseertime.pdf}}\quad
   \subfloat[Pubmed]{ \includegraphics[width=0.46\textwidth]{pubmedtime.pdf}}
   \subfloat[Ogbl-ppa]{ \includegraphics[width=0.46\textwidth]{ppatime.pdf}}\quad
    \subfloat[Ogbl-ddi]{ \includegraphics[width=0.46\textwidth]{dditime.pdf}}
    \subfloat[Ogbl-citation2]{ \includegraphics[width=0.46\textwidth]{citation2time.pdf}}
    \vskip -0.1in
    \caption{Inference time and GPU memory on datasets. The process we measure includes preprocessing, MPNN, and predicting one batch of test links. } 
    \label{fig:apptime}
    \vskip -0.2in
\end{figure}
Let  denote the number of target links,  denote the number of nodes in the graph, and  denote the maximum node degree. Existing models' time complexity can be expressed in , where  are irrelevant to .  and  of models are summarized in Table~\ref{tab:complexity}. The derivation of the time complexity is as follows. As NCN, GAE, and GNN with separated structural features run MPNN on the original graph, they share similar . Specifically, BUDDY~\citep{Gsketch} uses a simplified MPNN with . In contrast,  of SEAL is  as it does not run MPNN on the original graph. For each target link, vanilla GNN only needs to feed the feature vector to MLP for each link, so . Besides GAE's operation, BUDDY further needs to hash the structure for structural features, whose complexity is complex but higher than , and Neo-GNN computes pairwise feature with  complexity, where  is the number of hop Neo-GNN consider. NCN needs to compute common neighbor: , pool node embeddings: , and feed to MLP: . NCNC- runs NCN for each potential common neighbor: . Similarly, NCNC- runs  times NCNC-, so its time complexity is . For each target link, SEAL segregates a subgraph of size  and runs MPNN on it, so , where  is the number of hops of the subgraph. 

\section{Experimental Settings}\label{app::experimentsetting}

{\bf Computing infrastructure.}~~We leverage Pytorch Geometric and Pytorch for model development. All experiments are conducted on an Nvidia 4090 GPU on a Linux server. 

{\bf Baselines.}~~We directly use the results reported in \citep{Gsketch}.

{\bf Model hyperparameter.}~~We use optuna to perform random searches. Hyperparameters were selected to maximize validation score. The best hyperparameters selected for each model can be found in our code. 


{\bf Training process.} We utilize Adam optimizer to optimize models and set an epoch upper bound . 



\section{Scalability Comparison on datasets}\label{app:time}
The time and memory consumption of models on different datasets are shown in Figure~\ref{fig:apptime}. On these datasets, we observe results similar to those on the ogbl-collab dataset in Section~\ref{sec:scalability}: NCN achieves similar computation overhead to GAE; NCNC usually scales better than Neo-GNN; SEAL's scalabilty is the worst. However, on the ogbl-citation2 dataset, SEAL has the lowest GPU memory consumption with small batch sizes, because the whole graph in ogbl-citation2 is large, on which MPNN is expensive, while SEAL only runs MPNN on small subgraphs sampled from the whole graph, leading to lower overhead.
\end{document}
