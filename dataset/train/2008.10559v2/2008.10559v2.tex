
\begin{table*}
	\scriptsize
	\setlength{\tabcolsep}{0.0035\linewidth}
	\newcommand{\classfreq}[1]{{~\tiny(\semkitfreq{#1}\%)}}  \centering
	\begin{tabular}{l|c c c|c c c c c c c c c c c c c c c c c c c|c}
		\toprule
		& \multicolumn{3}{c|}{scene completion} & \multicolumn{20}{c}{semantic scene completion} \\
		Approach 
		& \rotatebox{90}{precision}
		& \rotatebox{90}{recall}
		& \rotatebox{90}{IoU}
		& \rotatebox{90}{\textcolor{road}{} road\classfreq{road}} 
		& \rotatebox{90}{\textcolor{sidewalk}{} sidewalk\classfreq{sidewalk}}
		& \rotatebox{90}{\textcolor{parking}{} parking\classfreq{parking}} 
		& \rotatebox{90}{\textcolor{other-ground}{} other-ground\classfreq{otherground}} 
		& \rotatebox{90}{\textcolor{building}{} building\classfreq{building}} 
		& \rotatebox{90}{\textcolor{car}{} car\classfreq{car}} 
		& \rotatebox{90}{\textcolor{truck}{} truck\classfreq{truck}} 
		& \rotatebox{90}{\textcolor{bicycle}{} bicycle\classfreq{bicycle}} 
		& \rotatebox{90}{\textcolor{motorcycle}{} motorcycle\classfreq{motorcycle}} 
		& \rotatebox{90}{\textcolor{other-vehicle}{} other-vehicle\classfreq{othervehicle}} 
		& \rotatebox{90}{\textcolor{vegetation}{} vegetation\classfreq{vegetation}} 
		& \rotatebox{90}{\textcolor{trunk}{} trunk\classfreq{trunk}} 
		& \rotatebox{90}{\textcolor{terrain}{} terrain\classfreq{terrain}} 
		& \rotatebox{90}{\textcolor{person}{} person\classfreq{person}} 
		& \rotatebox{90}{\textcolor{bicyclist}{} bicyclist\classfreq{bicyclist}} 
		& \rotatebox{90}{\textcolor{motorcyclist}{} motorcyclist\classfreq{motorcyclist}} 
		& \rotatebox{90}{\textcolor{fence}{} fence\classfreq{fence}} 
		& \rotatebox{90}{\textcolor{pole}{} pole\classfreq{pole}} 
		& \rotatebox{90}{\textcolor{traffic-sign}{} traffic-sign\classfreq{trafficsign}} 
		& \rotatebox{90}{mIoU}  \\
		\midrule
		SSCNet~\cite{Song2017SemanticSC} & 31.71 & 83.40 & 29.83 & 27.55 & 16.99 & 15.60 & 6.04 & 20.88 & 10.35 & 1.79 & 0 & 0 & 0.11 & 25.77 & 11.88 & 18.16 & 0 & 0 & 0 & 14.40 & 7.90 & 3.67 & 9.53  \\ *SSCNet-full~\cite{Song2017SemanticSC} & 59.64 & 75.52 & 49.98 & 51.15 & 30.76 & 27.12 & 6.44 & 34.53 & 24.26 & 1.18 & \textbf{0.54} & \textbf{0.78} & \textbf{4.34} & 35.25 & 18.17 & 29.01 & 0.25 & 0.25 & \textbf{0.03} & 19.87 & 13.10 & 6.73 & 16.14  \\ TS3D~\cite{Garbade2019TwoS3} & 31.58 & 84.18 & 29.81 & 28.00 & 16.98 & 15.65 & 4.86 & 23.19 & 10.72 & 2.39 & 0 & 0 & 0.19 & 24.73 & 12.46 & 18.32 & 0.03 & 0.05 & 0 & 13.23 & 6.98 & 3.52 & 9.54 \\ TS3D+DNet~\cite{Behley2019SemanticKITTIAD} & 25.85 & \textbf{88.25} & 24.99 & 27.53 & 18.51 & 18.89 & \textbf{6.58} & 22.05 & 8.04 & 2.19 & 0.08 & 0.02 & 3.96 & 19.48 & 12.85 & 20.22 & \textbf{2.33} & \textbf{0.61} & 0.01 & 15.79 & 7.57 & \textbf{6.99} & 10.19 \\ TS3D+DNet+SATNet~\cite{Behley2019SemanticKITTIAD} & 80.52 & 57.65 & 50.60 & 62.20 & 31.57 & 23.29 & 6.46 & 34.12 & 30.70 & \textbf{4.85} & 0 & 0 & 0.07 & 40.12 & \textbf{21.88} & \textbf{33.09} & 0 & 0 & 0 & \textbf{24.05} & \textbf{16.89} & 6.94 & \textbf{17.70} \\ \midrule
		LMSCNet (ours) & 77.11 & 66.19 & 55.32 & 64.04& 33.12 & 24.91& 3.22 & \textbf{38.67} & 29.48 & 2.53 & 0 & 0 & 0.11 & 40.53 & 18.97 & 30.77 & 0 & 0 & 0 & 20.52 & 15.72 & 0.54 & 17.01 \\
		LMSCNet-singlescale (ours) &  \textbf{81.55} & 65.07 & \textbf{56.72} & \textbf{64.80} & \textbf{34.68} & \textbf{29.02} & 4.62 & 38.08 & \textbf{30.89} & 1.47 & 0 & 0 & 0.81 & \textbf{41.31} & 19.89 & 32.05 & 0 & 0 & 0 & 21.32 & 15.01 & 0.84 & 17.62 \\
		\bottomrule
	\end{tabular}\\
	{\scriptsize * Own implementation.}
	\caption{Comparison of published methods on the official SemanticKITTI~\cite{Behley2019SemanticKITTIAD} benchmark. Despite light mixed 2D/3D reasoning, our network performs 2nd on the semantic metrics (mIoU), outdistanced by the more complex TS3D+DNet+SATNet also twice slower than us. On the completion metrics (IoU), we perform 1st with a comfortable margin. The last two rows show that LMSCNet is better in its single scale version (\textit{LMSCNet-singlescale}), though this comes at the cost of loosing multiscale capacity. Except for SSCNet-full, all results originate from~\cite{Behley2019SemanticKITTIAD}.} 
	\label{table:net_reults}
\end{table*}


\begin{figure*}
	\centering
	\scriptsize
	\setlength{\tabcolsep}{0.017\linewidth}
	\renewcommand{\arraystretch}{0.8}
	\begin{tabular}{cccc}
		Input & SSCNet-full~\cite{Song2017SemanticSC} & LMSCNet (ours) & Ground Truth\\
		
		\includegraphics[width=0.4\columnwidth]{Figures/method_qualitative/000065/000065_01_inours.png} & 
		\includegraphics[width=0.4\columnwidth]{Figures/method_qualitative/000065/000065_01_prSSCNet_D.png} &
		\includegraphics[width=0.4\columnwidth]{Figures/method_qualitative/000065/000065_01_prours.png} &
		\includegraphics[width=0.4\columnwidth]{Figures/method_qualitative/000065/000065_01_gtours.png} \\
		
		\includegraphics[width=0.4\columnwidth]{Figures/method_qualitative/000065/000065_04_inours.png} & 
		\includegraphics[width=0.4\columnwidth]{Figures/method_qualitative/000065/000065_04_prSSCNet_D.png} &
		\includegraphics[width=0.4\columnwidth]{Figures/method_qualitative/000065/000065_04_prours.png} &
		{\includegraphics[width=0.4\columnwidth]{Figures/method_qualitative/000065/000065_04_gtours.png}} \vspace{0.5cm}\\
		
		
		
		\includegraphics[width=0.4\columnwidth]{Figures/method_qualitative/000295/000295_01_inours.png} & 
		\includegraphics[width=0.4\columnwidth]{Figures/method_qualitative/000295/000295_01_prSSCNet_D.png} &
		\includegraphics[width=0.4\columnwidth]{Figures/method_qualitative/000295/000295_01_prours.png} &
		\includegraphics[width=0.4\columnwidth]{Figures/method_qualitative/000295/000295_01_gtours.png} \\
		
		\includegraphics[width=0.4\columnwidth]{Figures/method_qualitative/000295/000295_04_inours.png} & 
		\includegraphics[width=0.4\columnwidth]{Figures/method_qualitative/000295/000295_04_prSSCNet_D.png} &
		\includegraphics[width=0.4\columnwidth]{Figures/method_qualitative/000295/000295_04_prours.png} &
		{\includegraphics[width=0.4\columnwidth]{Figures/method_qualitative/000295/000295_04_gtours.png}} \vspace{0.5cm}\\
		
		
		\includegraphics[width=0.4\columnwidth]{Figures/method_qualitative/000100/000100_01_inours.png} & 
		\includegraphics[width=0.4\columnwidth]{Figures/method_qualitative/000100/000100_01_prSSCNet_D.png} &
		\includegraphics[width=0.4\columnwidth]{Figures/method_qualitative/000100/000100_01_prours.png} &
		\includegraphics[width=0.4\columnwidth]{Figures/method_qualitative/000100/000100_01_gtours.png} \\
		
		\includegraphics[width=0.4\columnwidth]{Figures/method_qualitative/000100/000100_04_inours.png} & 
		\includegraphics[width=0.4\columnwidth]{Figures/method_qualitative/000100/000100_04_prSSCNet_D.png} &
		\includegraphics[width=0.4\columnwidth]{Figures/method_qualitative/000100/000100_04_prours.png} &
		{\includegraphics[width=0.4\columnwidth]{Figures/method_qualitative/000100/000100_04_gtours.png}} \vspace{0.5cm}\\
		
		
		\includegraphics[width=0.4\columnwidth]{Figures/method_qualitative/001470/001470_01_inours.png} & 
		\includegraphics[width=0.4\columnwidth]{Figures/method_qualitative/001470/001470_01_prSSCNet_D.png} &
		\includegraphics[width=0.4\columnwidth]{Figures/method_qualitative/001470/001470_01_prours.png} &
		\includegraphics[width=0.4\columnwidth]{Figures/method_qualitative/001470/001470_01_gtours.png} \\
		
		\includegraphics[width=0.4\columnwidth]{Figures/method_qualitative/001470/001470_04_inours.png} & 
		\includegraphics[width=0.4\columnwidth]{Figures/method_qualitative/001470/001470_04_prSSCNet_D.png} &
		\includegraphics[width=0.4\columnwidth]{Figures/method_qualitative/001470/001470_04_prours.png} &
		{\includegraphics[width=0.4\columnwidth]{Figures/method_qualitative/001470/001470_04_gtours.png}} \\
		
		\multicolumn{4}{c}{\includegraphics[width=1.7\columnwidth]{Figures/method_qualitative/legend_method_qualitative.pdf}}
		
	\end{tabular}
	\caption{Qualitative 3D semantic completion at full size on the SemanticKITTI~\cite{Behley2019SemanticKITTIAD} validation set. Each pairs of rows show a single scene with different viewpoints. Compared to SSCNet-full~\cite{Song2017SemanticSC}, our LMSCNet provides smoother semantics labels and is capable of retrieving finer details. This is evident when looking at the cars (rows 7-8) or the trees (rows 5-6).}
	\label{fig:qualitative_comparison}
\end{figure*} 
\begin{figure}
	\centering
	\scriptsize
	\setlength{\tabcolsep}{0.005\linewidth}
	\renewcommand{\arraystretch}{0.7}
	\begin{tabular}{ccc}
	    Input & SSCNet-full~\cite{Song2017SemanticSC} & LMSCNet (ours) \\ 
		\includegraphics[width=0.32\linewidth]{Figures/qualitative_nuscenes/input_grid/02_in_ours.png}&\includegraphics[width=0.32\columnwidth]{Figures/qualitative_nuscenes/SSCNet_full/02_pr_SSCNet_D.png}&\includegraphics[width=0.32\columnwidth]{Figures/qualitative_nuscenes/ours/02_pr_ours.png} \\
	\end{tabular}
	\caption{Inference results on nuScenes~\cite{Caesar2019nuScenesAM} with 32-layers LiDAR, while being trained on 64-layers SemanticKITTI. Our method performs well with sharp scene labeling, despite the change of input density.}
	\label{fig:nuScenes_qualitative}
\end{figure} 


\section{Experiments} \label{sec:experiments}
\label{sec:exp_metrics}
We evaluate our LMSCNet method by training on the recent semantic scene completion benchmark SemanticKITTI~\cite{Behley2019SemanticKITTIAD} providing 3D voxel grids from semantically labeled scans of HDL-64E rotating LiDAR in outdoor urban scenes~\cite{geiger2012we}. 
In~\cite{Behley2019SemanticKITTIAD}, inputs are voxelized single scans, while the ground truth was obtained from the voxelized aggregation of successive registered scans. Grids are 256x256x32 with 0.2m voxel size, and it is important to note that input \textit{and} ground truth are sparse, with average density of 6.7\% and 65.8\%, respectively. 
We use standard mIoU as a semantic completion metric, measuring the intersection over union averaged over all classes (20 semantic classes + \textit{free}). Additionally, we consider completion metrics IoU, Precision, and Recall to provide a sense of the scene completion quality, regardless of the assigned semantic labels (i.e. considering the binary \textit{free}~/~\textit{occupied} setting). Note that completion is crucial for obstacle avoidance in mobile robotics.








\paragraph{Implementation details.} 
We train using the original train/val splits with 3834/815 grids~\cite{Behley2019SemanticKITTIAD}, adding x-y flipping augmentation for generalization.
Adam optimizer is used (, ) with learning rate of 0.001 scaled by . Training fits in a single 11GB GPU with batch size 4, taking around 48 hours to converge (~80 epochs).



\subsection{Performance}\label{sec:exp-perf}
In the following we report performance against four state-of-the-art methods: SSCNet~\cite{Song2017SemanticSC}, TS3D~\cite{Garbade2019TwoS3}, TS3D+DNet~\cite{Behley2019SemanticKITTIAD}, TS3D+DNet+SATNet~\cite{Behley2019SemanticKITTIAD}. Because SSCNet output is 4x downsampled, we also report performance using deconvolution to reach full input resolution, hereafter denoted SSCNet-full. We refer to the supplementary for details on the required architectures adjustments.\\
\noindent{}Hereafter, we denote our multiscale architecture as \mbox{LMSCNet}. We detail semantic completion performance and then demonstrate the speed and lightness of our architecture. 

\subsubsection{Semantic Scene Completion}\label{sec:exp-perf-ssc}

Performance on the SemanticKITTI benchmark~\cite{Behley2019SemanticKITTIAD} is reported in Tab.~\ref{table:net_reults} against all published methods and SSCNet-full. 
The evaluation was conducted on the official server (i.e. hidden test set) hence, with the full size ground truth.

Overall, we perform on par with the best methods, though 2nd on the semantic completion metric (mIoU). On the latter, TS3D+DNet+SATNet~\cite{Behley2019SemanticKITTIAD} is slightly better despite their significantly heavier and slower network. Note also that TS3D uses additional RGB input, and all TS3D+DNet use also LiDAR refraction intensity. Conversely, LMSCNet is more versatile as it uses only occupancy grid input.
Notice that the highly imbalanced class frequencies (shown in parenthesis in Tab.~\ref{table:net_reults}) also illustrate the task complexity. Specifically, we outperform others on the largest four classes but performs on par or lower on the others, which advocates for some improvement in our balancing strategy. 
On the completion metrics (IoU) our method outperforms all others by a comfortable margin. Again, completion is of high importance for practical mobile robotics applications. 

In addition to the multiscale proposal (LMSCNet), we also report LMSCNet-singlescale -- a variation of LMSCNet where we train with  --, which logically performs a little better at full size though at the cost of loosing crucial multiscale capacity.








\paragraph{Qualitative performance.}


We compare qualitatively full size outputs of our LMSCNet and SSCNet-full in Fig.~\ref{fig:qualitative_comparison}, with views pairs from 4 scenes of the SemanticKITTI validation set\footnote{Note that SemanticKITTI benchmark (i.e. test set) does not provide any visual results. Hence, we omit TS3D baselines due to retraining complexities and their use of additional modalities (RGB or LiDAR intensity).}. 
At the rightmost, ground truth visualization also illustrates the sparse supervision complexity since holes are still visible.
Our method produces visually smoother semantic labels, easily noticeable in rows 5-8, and is able to reconstruct thin structures, like trees or cars (rows 6 or 7). 
For comprehensive analysis, we further test the same model (trained on 64-layer LiDAR) on the popular nuScenes dataset~\cite{Caesar2019nuScenesAM}, which has been registered using a 32-layers lidar sensor. Fig.~\ref{fig:nuScenes_qualitative} shows that our network better adjusts to the change of density and maintains the smoothness in the reconstruction. \vspace{-0.3 cm}

\paragraph{Multiscale performance.}
\begin{table}
		\setlength{\tabcolsep}{0.0038\linewidth}
		\centering
		\footnotesize
		\begin{tabular}{lccc}
			\toprule
			LMSCNet scale & IoU & mIoU  \\
			\midrule
			1:1 (full size) & 54.22 & 16.78 \\
			1:2 & 56.27 & 16.78 \\
			1:4 & 59.36 & 17.19 \\
			1:8 & 65.45 & 17.37 \\
			\bottomrule
	\end{tabular}
	\caption{LMSCNet multiscale semantic completion performance on SemanticKITTI validation set. We reach good performance at all levels, even better at the coarsest levels .\vspace{-0.5 cm}}
	\label{table:multiscale}
\end{table}
Tab.~\ref{table:multiscale} shows multiscale performance of our method on the SemanticKITTI validation set, where the scale is relative to the full size resolution~(level~0). From Sec.~\ref{sec:method-mscale}, scale at level  is . Ground truths at lower resolution were obtained from majority vote pooling of the full size ground truth.
From the above table, our architecture maintains a good performance in all resolutions, with best performance logically reached at the lowest resolution (highest level). 
Qualitative multiscale completion is visible in Fig.~\ref{fig:mscale_pipeline}.
We argue that our architecture reaches multiscale capacity thanks to the disentanglement of segmentation features with our custom head. Additionally, at coarser resolution our network reaches very fast inference, which will be described in details in the following section.


\subsubsection{Architectures comparison.}\label{sec:exp-perf-arch}


\begin{table}[!h]
	\footnotesize
	\centering
	\setlength{\tabcolsep}{0.01\linewidth}
	\begin{tabular}{cccc}
		\toprule
		Method & Params (M) &  FLOPs (G) & FPS\\
		\midrule
		*SSCNet~\cite{Song2017SemanticSC}                    & 0.93 & 82.5 & 56.90 \\  
		*SSCNet-full~\cite{Song2017SemanticSC}             & 1.09 & 769.6 & 45.94 \\
		*TS3D~\cite{Garbade2019TwoS3}                        & 43.77 & 2016.7 & 9.79 \\  *TS3D+DNet~\cite{Behley2019SemanticKITTIAD}          & 51.31 & 847.1 & 8.72\\ *TS3D+DNet+SATNet~\cite{Behley2019SemanticKITTIAD}   & 50.57 & 905.2 & 1.27\\  \midrule
		LMSCNet			                                 & 0.35 & 72.6 & 21.28\\
		LMSCNet (1:2)                                 & 0.32 & 13.7  & 126.38\\
		LMSCNet (1:4)                                 & 0.28 & 5.7  & 323.46\\
		LMSCNet (1:8)                                 & 0.24 & 4.4  & 372.24\\
		\bottomrule
	\end{tabular}\\
	{\scriptsize * Own implementation to compute network statistics.}
	\caption{Network statistics. Even at full resolution \mbox{LMSCNet} (ours) has significantly less parameters with lower FLOPs. On a speed basis, we are twice slower than SSCNet-full~\cite{Song2017SemanticSC} which performs worse than us (see Tab.~\ref{table:net_reults}). Still, our multiscale versions -- denoted \mbox{\textit{LMSCNet (1:x)}} -- enable very fast inference.}
	\label{table:net_stats}
\end{table}

\begin{figure}
	\centering
	\scriptsize
	\setlength{\tabcolsep}{0.005\linewidth}
	\renewcommand{\arraystretch}{0.7}
	\begin{tabular}{cc}
		\includegraphics[width=0.5\linewidth]{Figures/fps_method_plots/rebuttal/mIoU.pdf}&\includegraphics[width=0.5\columnwidth]{Figures/fps_method_plots/rebuttal/IoU.pdf}\\
	\end{tabular}\vspace{-1em}
	\caption{Architectures performance versus speed (markers are scaled with \# of parameters). Notice that \mbox{TS3D+DNet+SATNet} is the only better method on semantics (+0.69 mIoU) though less time performant (x17 slower) and worse on completion (-4.72 IoU).}
	\label{fig:scatter-perf-speed-params}
\end{figure} 

Tab.~\ref{table:net_stats} reports networks statistics for our architecture and all above mentioned baselines. From the latter, even at full size LMSCNet has significantly less parameters (0.35M) and lower computational cost for inference (72.6G FLOPs). Compared to any TS3D baselines it is at least an order of magnitude faster. However, SSCNet (original or full) is twice faster than LMSCNet, though with more parameters and worse performance (cf. Tab.~\ref{table:net_reults}). Since lighter models does not \textit{always} run faster due to the sequentiality of some operations on GPU, we conjecture the higher speed of SSCNet is caused by the lower number of convolutional operations compared to LMSCNet full scale (16 vs. 25).

In last rows of Tab.~\ref{table:net_stats}, we report statistics for coarser completion, removing \textit{unnecessary} parts of our network at inference. 
Lower resolution inference allows significant speedups in the processing, reaching 372 FPS at the highest scale -- 6x faster than SSCNet and ~300x faster than \mbox{TS3D+DNet+SATNet} --.
Fig. \ref{fig:scatter-perf-speed-params} illustrates the architectures performance versus speed. Notice that \textit{even at full scale} we provide a better speed-performance balance.
Because semantic completion is an application of high interest for mobile robotics, like autonomous driving, our lighter architecture is beneficial for embedded GPUs and enables coarse scene analysis at high speed. 



\begin{figure}
	\centering
	\scriptsize
	\setlength{\tabcolsep}{0.01\linewidth}
	\renewcommand{\arraystretch}{0.7}
	\begin{tabular}{ccc}
		\includegraphics[height=1.75cm]{Figures/quantitative_lidar_multiresolution/plot_res_lidar_tinys_2.pdf}&\includegraphics[width=0.33\columnwidth]{Figures/qualitative_lidar_multiresolution/Lidar08.png}&\includegraphics[width=0.33\columnwidth]{Figures/qualitative_lidar_multiresolution/Lidar32.png}\\
		& 8 layers & 32 layers
	\end{tabular}
	\caption{Semantic scene completion results from simulated lower resolution LiDAR sensors (downsampled from 64 layers input). Even with only 8 layers input our LMSCNet correctly predicts the scene outline.}
	\label{fig:lidar_multires_qualitative}\label{fig:lidar_res_plot}
\end{figure} 











\subsection{Ablation studies}
\label{sec:exp-ablation}

To study the benefit of our design choices, we conduct a series of ablation studies on SemanticKITTI validation set. This is done by modifying important blocks of our architecture and evaluating its performance. 

\paragraph{Influence of input resolution.}
We evaluate our robustness, by retrieving the original 64-layers KITTI scans used in SemanticKITTI and simulating 8/16/32 layers LiDARs with layers subsampling\footnote{Every 2nd, 4th and 8th layer are subsampled to simulate 32, 16 and 8 layer LiDARs, respectively. Unlike~\cite{Jaritz2018SparseAD}, note that data SemanticKITTI uses KITTI odometry set in which data is already untwisted.}, as in~\cite{Jaritz2018SparseAD}.


Fig.~\ref{fig:lidar_res_plot} shows quantitative and qualitative performance using simulated and original LiDAR. As expected, lower layers input deteriorate the performance, especially in areas far from the sensor location, but our network still performs reasonably well on semantics (mIoU) and completion (IoU). This is visible in the middle image, as 8 layers input (2.10\% density) is sufficient to retrieve the general outline of the scene.








\begin{table}
\centering
	\setlength{\tabcolsep}{0.01\linewidth}
		\centering
		\footnotesize
		\begin{tabular}{lccc}
			\toprule
			Method & IoU & mIoU  \\
			\midrule
			LMSCNet (ours) & 54.22 & 16.78 \\
			w/o Deconv & 52.79 &	15.64 \\
			w/o ASPP & 53.81 &	16.21 \\
			w/o Multiscale UNet & 53.54 &	16.22 \\
			\bottomrule
	\end{tabular}
\caption{Ablation study of our model design choices on the SemanticKITTI~\cite{Behley2019SemanticKITTIAD} validation set. } 
\label{table:ablation_table}
\end{table}
\paragraph{Deconv versus Upsampling.}

As we aimed to preserve a lightweight architecture, we tried to remove the parameters-greedy deconv layers from our network (cf. Sec.~\ref{fig:architecture}), replacing them with up-sampling layers. From Tab.~\ref{table:ablation_table}, performance \textit{without deconv} introduces a 1.43\% and 1.14\% performance drop for completion and semantic completion respectively, with only 3\% less parameters.


\paragraph{Dilated convolutions.}
We evaluate the benefit of dilated convolutions in the decoder by ablating ASPP blocks from the segmentation head (see Fig.~\ref{fig:architecture}). Tab.~\ref{table:ablation_table} indicates that mIoU drops by 0.41\% without ASPP. We conjecture that the boost of ASSP results come from the increasing receptive fields of the inner dilated convolutions, providing richer features.

\paragraph{Multiscale UNet decoder.}
\begin{figure}
\centering
\subfloat[Vanilla UNet decoder]{\includegraphics[width=0.455\columnwidth]{Figures/all_features_scheme/non_all_features_scheme.pdf}\label{fig:scheme_non_allf_concat}}
\hspace{0.02\columnwidth}
\subfloat[Multiscale UNet decoder]{\includegraphics[width=0.49\columnwidth]{Figures/all_features_scheme/all_features_scheme.pdf}\label{fig:scheme_allf_concat}}
\caption{Decoders comparison. While Vanilla UNet decoder only considers features from the previous level \protect\subref{fig:scheme_non_allf_concat}, we instead use Multiscale UNet where all coarser levels enhance spatial contextual information \protect\subref{fig:scheme_allf_concat}. Circles show intermediary operations to reach required feature maps size.}
\label{fig:schemes_decoder_vanilla_vs_mscale}
\end{figure}


As illustrated in Fig.~\ref{fig:scheme_non_allf_concat}, unlike vanilla UNet decoder we concatenate the features at the end of each decoder level to all other levels. This is intended to aggregate multiscale features and should intuitively help considering coarser semantic features for fine resolutions.

We assess the benefit of our \textit{multiscale UNet} by evaluating \textit{Vanilla UNet} in the last row of Tab.~\ref{table:ablation_table}, which shows that our proposal boosts completion by 0.68\% and semantic completion by 0.56\%.

