\begin{table*} 
\centering 
\caption{The general statistics of the four datasets used in our experiments.} \vspace{-0.3cm}\begin{tabular}{l | c c c c } \toprule \textbf{Property} &\textbf{UCF-HMDB$_{small}$} &\textbf{UCF-HMDB$_{full}$} &\textbf{UCF-Olympic} &\textbf{Kinetics-Gameplay} \\ \midrule Video Length & $\sim$21 Seconds & $\sim$33 Seconds &$\sim$39 Seconds &$\sim$ 10 Seconds\\ Classes &5 &12 &6 &30\\ Training Videos &UCF: 482 / HMDB: 350 &UCF:1,438 / HMDB: 840 &UCF: 601 / Olympic: 250 & Kinetics: 43,378 / Gameplay: 2,625 \\ Validation Videos &UCF: 189 / HMDB: 571 &UCF: 360 / HMDB: 350 &UCF: 240 / Olympic: 54 & Kinetics: 3,246 / Gameplay: 749 \\ \bottomrule \end{tabular}
\label{tab:datasets}\vspace{-0.3cm}
\end{table*}
\vspace{-0.2cm}
\section{Experiments}
\subsection{Datasets}
We compare and contrast our proposed approach with the existing domain adaptation approaches on four benchmark datasets, \textit{i.e.}, the \textbf{UCF-HMDB$_{small}$}, \textbf{UCF-HMDB$_{full}$}, \textbf{UCF-Olympic} and \textbf{Kinetics-Gameplay}. For fair comparison, we follow the dataset partition and feature extraction strategies from \cite{TAN}, that utilizes the ResNet101 model pre-trained on ImageNet as the frame-level feature extractor. The statistics of the four datasets are summarized in Table \ref{tab:datasets}. The \textbf{UCF-HMDB$_{small}$} and \textbf{UCF-HMDB$_{full}$} are the overlapped subsets of two large-scale action recognition datasets, \textit{i.e.}, the UCF101~\cite{ucf} and HMDB51~\cite{hmdb}, covering 5 and 12 highly relevant categories respectively. The \textbf{UCF-Olympic} selects the shared 6 classes from the UCF101 and Olympic Sports Datasets~\cite{olympic}, including \textit{Basketball, Clearn and Jerk, Diving, Pole Vault, Tennis and Discus Throw}. The \textbf{Kinetics-Gameplay} is the most challenging cross-domain dataset, with a large domain gap between the synthetic videos and real-world videos. The dataset is build by selecting 30 shared categories between Gameplay~\cite{TAN} and one of the largest public video datasets Kinetics-600~\cite{kinetics}. Each category may also correspond to multiple categories in both dataset, which poses another challenge of class imbalance.
\vspace{-0.3cm}

\subsection{Baselines}
We compare our approach with several state-of-the-art video domain adaptation methods, image domain adaptation approaches, single-domain action recognition models, and a basic ResNet-101 classification model pre-trained on the ImageNet dataset. Single-domain action recognition models include the 3D ConvNets (\textbf{C3D}) \cite{C3D} and Temporal Segment Networks (\textbf{TSN})~\cite{TSN}, which are pre-trained on the source domain and tested on the target domain. Four classical image-level domain adaptation methods, \textit{i.e.}, Domain-Adversarial Neural Network (\textbf{DANN}) \cite{DANN}, Joint Adaptation Network (\textbf{JAN}) \cite{JAN}, Adaptive Batch Normalization (\textbf{AdaBN}) \cite{AdaBN} and Maximum Classifier Discrepancy (\textbf{MCD}) \cite{MCD} are adjusted to align the distributions of video features with the frame aggregation module. As for non-deep video domain adaptation, we compare the proposed \textbf{HABG} method with \textbf{Many-to-One}~\cite{DBLP:journals/ivc/XuZWF16} Encoder, two variants of Action Modeling on Latent Subspace (\textbf{AMLS})~\cite{AMLS}, \textit{i.e.}, the Subspace Alignment (\textbf{AMLS-SA}) and Geodesic Flow Kernel (\textbf{AMLS-GFK}). For deep video domain adaptation methods, we adopt the Deep Adversarial Action Adaptation (\textbf{DAAA})~\cite{AMLS}, Temporal Adversarial Adaptation Network (\textbf{TA$^{2}$N})~\cite{TAN}, Temporal Attentive Adversarial Adaptation Network (\textbf{TA$^{3}$N})~\cite{TAN} and Temporal Co-attention Network (\textbf{TCoN})~\cite{TcoN} for comparison.



\subsection{Implementation Details} Our source code is based on PyTorch \cite{pytorch}, which is available in a Githu repository\footnote{https://github.com/Luoyadan/MM2020\_ABG} for reference. All experiments are conducted on two servers with two GeForce GTX 2080 Ti GPUs.

\subsubsection{Video Pre-processing}~\label{dp}
Following the standard protocol used in~\cite{TAN}, we sample a fixed-number $K$ of frames with an equal spacing from each video for training, and encode each frame with the Resnet-101 \cite{resnet} pre-trained on ImageNet into a 2048-D vector, \textit{i.e.}, $D=2048$. For fair comparison, we set $K$ to 5 in our experiments.


\begin{figure}[t]
    \centering
    \subfloat[][HMDB$\rightarrow$UCF$_{small}$]{\includegraphics[width=0.49\linewidth]{images/cf/hmdb_ucf_small-ucf101_train-False-avgpool-LSTM-5seg.pdf}}
    \subfloat[][UCF$\rightarrow$HMDB$_{small}$]{\includegraphics[width=0.49\linewidth]{images/cf/hmdb_ucf_small-hmdb51_train-False-avgpool-LSTM-5seg.pdf}}\\
    \subfloat[][HMDB$\rightarrow$UCF$_{full}$]{\includegraphics[width=0.49\linewidth]{images/cf/hmdb_ucf-ucf101_train-avgpool-LSTM-5seg.pdf}}
    \subfloat[][UCF$\rightarrow$Olympic]{\includegraphics[width=0.49\linewidth]{images/cf/ucf_olympic-olympic_train-False-avgpool-LSTM-5seg.pdf}}
    \caption{The confusion matrices of the proposed ABG method performed on three benchmark datasets.}
    \label{fig:cf}\vspace{-0.7cm}
\end{figure}


\subsubsection{Module Architecture.}
The edge update networks $\mathcal{F}_{fe}$ and $\mathcal{F}_{ve}$ project the affinity map from $D$ and $D_v$ dimensions to 1-dim scores, which are composed of the two convolutional layers, batch normalization, LeakyReLU and edge dropout. The node update networks $\mathcal{F}_{fv}$ and $\mathcal{F}_{vn}$ map the $2D$-dim and $2D_v$-dim concatenation of node features and the neighbors' features to the $D_v$-dim and $D_n$-dim vectors, respectively. $\mathcal{F}_{fv}$ and $\mathcal{F}_{vn}$ include two convolutional layers, batch normalization, LeakyReLU and node dropout.


\subsubsection{Parameter Settings.}
 The hidden size, the feature dimension of frame nodes (\textit{i.e.} $D_v$) and the feature dimension of video nodes (\textit{i.e.} $D_n$) are fixed to 512. The total number of training epochs $M$ is 60 for Kinetics-Gameplay dataset, and 30 for the rest of datasets. The batch size $B_s$, $B_t$ for the source data and target data are set to 128. The stochastic gradient optimizer (SGD) is used as the optimizer with a momentum of 0.9 and weight decay of $1\times10^{-4}$. The learning rate $\mu$ is initiated as $4\times10^{-2}$ then decayed as the number of epoch increases, which follows the rule used in \cite{DANN,TAN}. The loss coefficients $\alpha$ and $\lambda$ are empirically fixed at 0.1 and 1 for semi-supervised experiments. The dropout rate is set to 0.2.



\subsection{Comparisons with State-of-The-Art}
 Under the unsupervised domain adaptation protocol, we compare the proposed ABG method with multiple baseline approaches on \textbf{UCF-HMDB$_{small}$}, \textbf{UCF-Olympic} and \textbf{UCF-HMDB$_{full}$} datasets. With different backbone networks, the comparison results achieved from the relatively small datasets are reported in Table \ref{tab:toy}. Table \ref{tab:UCF-HMDB} presents the results on the full UCF-HMDB dataset using various frame aggregation strategies. It is observed that the proposed \textbf{ABG} framework is superior to all the compared image- and video-level domain adaptation methods in most cases, especially achieving a significant performance boost on the large-scale testbed. Notably, \textbf{Source Only} indicates the backbone model pretrained on the source domain and tested on the target domain. \textbf{Target Only} denotes the backbone model trained and tested on the target domain. From Table \ref{tab:toy}, it is demonstrated that the deep video DA methods (line 6-10) generally outperforms the non-deep video DA approaches (line 3-5) and the classification models without DA (line 1-2). Among the deep \textbf{TCoN} and \textbf{TA$^3$N} leverage the attention mechanism and then suppress the variance caused by the outlier frames, improving the recognition accuracy by up to $6.3\%$ and $11.1\%$ over \textbf{DAAA} on the UCF-Olympic dataset. With the same backbone of TA$^3$N, our \textbf{ABG} model performs comparably without relying on the frame attention or complex frame aggregation strategies. This phenomenon is also observed in the large \textbf{UCF-HMDB$_{full}$} dataset, in which the proposed \textbf{ABG} with the average pooling boosts the performance by up to $10.1\%$ and $11.5\%$ over the state-of-the-art \textbf{TA$^3$N} on the UCF$\rightarrow$HMDB and HMDB$\rightarrow$UCF tasks, respectively. As shown in Table \ref{tab:UCF-HMDB}, average pooling (\textbf{AvgPool}) and \textbf{LSTM} suits the proposed model better among other frame aggregation functions. We infer the reasons behind is the frame bipartite graph has already fused similar frames regardless the order, which weakens the power of multi-scaled \textbf{TRN} aggregation. Notably, it is observed that \textbf{AdaBN} surpasses the most of image domain adaptation methods. As it separates the batch normalization layer for source and target data, AdaBN minimizes the risk of being overfitting to the source domain, which provides a strong support to our statement discussed in Section \ref{sec:intro}. To further investigate the detailed performance of the proposed ABG with respect to specific classes, four confusion matrices are provided in Figure \ref{fig:cf}.



\begin{table}[t]
	\caption{Recognition accuracies (\%) on the UCF-HMDB$_{small}$ and UCF-Olympic datasets. U: UCF, H: HMDB, O: Olympic.}\vspace{-0.2cm}
	\label{tab:toy}
	\centering
	\resizebox{1\linewidth}{!}{\begin{tabular}{l ccccc}
			\toprule
			\textbf{Method} &\textbf{Backbone} &\textbf{U$\rightarrow$H$_{small}$}	&\textbf{H$\rightarrow$U$_{small}$}	& \textbf{U$\rightarrow$O} & \textbf{O$\rightarrow$U}\\
			\midrule
			\multirow{2}{*}{Source Only} 
			&TSN &- &82.10 &80.00 &76.67\\
			&C3D &- &- &82.13 &83.16\\
			\midrule
			Many-to-One \cite{DBLP:journals/ivc/XuZWF16} &Action Bank &82.00 &82.00 &87.00 &75.00\\
			AMLS-SA \cite{AMLS} &C3D &90.25 &94.40 &83.92 &86.07\\
			AMLS-GFK \cite{AMLS} &C3D &89.53 &95.36 &84.65 & 86.44\\
			\midrule
			DAAA \cite{AMLS} &TSN &- &88.36 &88.37 &86.25\\
			DAAA \cite{AMLS} &C3D &- &- &91.60 &89.96\\
			TCoN \cite{TcoN} &TSN & - &93.01  &93.91 &91.65\\
			TA$^{3}$N \cite{TAN} &ResNet-101 &99.33 &\textbf{99.47} &\textbf{98.15} &\textbf{92.92}\\
			\midrule
			\textbf{ABG-AvgPool}  &ResNet-101 &\textbf{99.33} &98.41 &\textbf{98.15} &92.50\\
			\bottomrule
		\end{tabular}
	}\vspace{-0.3cm}
\end{table}

\begin{table}[t] \centering \caption{Recognition accuracies (\%) of the domain adaptation methods and the proposed ABG model with respect to various frame aggregation strategies on the full UCF-HMDB dataset.}\vspace{-0.2cm}
	\resizebox{1\linewidth}{!}{\begin{tabular}{l c c c c c c c c}
			\toprule & \multicolumn{4}{c}{\textbf{UCF$\rightarrow$HMDB}} & \multicolumn{4}{c}{\textbf{HMDB$\rightarrow$UCF}}\\ 
			\cmidrule(l){2-5}\cmidrule(l){6-9} 
			\textbf{Method} &AvgPool &LSTM &GRU &TRN &AvgPool &LSTM &GRU &TRN\\ 
			\midrule Source Only &70.28 &69.17 &70.83 &71.67 &74.96 &70.05 &76.36 &73.91\\ 
			\midrule
			DANN~\cite{DANN} &71.11 &70.00 &70.83 &75.28 &75.13 &75.83 &75.13 &76.36\\ 
			JAN~\cite{JAN} &71.39 &70.56  &72.50  &74.72 &77.58 &77.58 &77.75 &79.36\\
			AdaBN~\cite{AdaBN} &75.56 &74.17  &74.72  &72.22 &76.36 &77.41 &74.96 &77.41\\
			MCD~\cite{MCD} &71.67 &70.00 &74.44 &73.89 &76.18 &68.30 &78.81 &79.34\\ 
			\midrule TA$^{2}$N~\cite{TAN}  &71.11 &70.00  &70.83 &77.22 &76.36 &70.75 &76.89 &80.56\\
			TA$^{3}$N~\cite{TAN}  &71.94 &70.00  &69.72 &\textbf{78.33} &76.36 &70.75 &77.23 &81.79\\
			\midrule \textbf{ABG} &\textbf{79.17} &\textbf{75.56}  &\textbf{75.56}  &76.67  &\textbf{85.11} &\textbf{84.24} &\textbf{83.36} &\textbf{81.79}\\ 
			\midrule
			Target Only &80.56 &- &- &82.78 &92.12 &- &-  &94.92\\
			\bottomrule \vspace{-1cm}
		\end{tabular}
	}
	\label{tab:UCF-HMDB} 
\end{table}   

 
\begin{figure*}[!htb]
    \centering\vspace{-2.2cm}
    \subfloat[][DANN]{\includegraphics[width=0.23\linewidth, height=3.5cm]{images/tsne/DANN-hmdb_ucf-ucf101_train-rnn-LSTM-5seg.pdf}}
    \subfloat[][JAN]{\includegraphics[width=0.23\linewidth,height=3.5cm]{images/tsne/JAN-hmdb_ucf-ucf101_train-avgpool-LSTM-5seg.pdf}}
    \subfloat[][MCD]{\includegraphics[width=0.23\linewidth,height=3.5cm]{images/tsne/MCD-hmdb_ucf-ucf101_train-rnn-GRU-5seg.pdf}}
    \subfloat[][TA$^{3}$N]{\includegraphics[width=0.23\linewidth,height=3.5cm]{images/tsne/baseline-hmdb_ucf-ucf101_train-trn-m-LSTM-5seg.pdf}}\\\vspace{-0.3cm}
    \subfloat[][ABG-AvgPool]{\includegraphics[width=0.23\linewidth,height=3.5cm]{images/tsne/hmdb_ucf-ucf101_train-avgpool-LSTM-5seg.pdf}}
    \subfloat[][ABG-LSTM]{ \includegraphics[width=0.23\linewidth,height=3.5cm]{images/tsne/hmdb_ucf-ucf101_train-rnn-LSTM-5seg.pdf}}
     \subfloat[][ABG-GRU]{\includegraphics[width=0.23\linewidth,height=3.5cm]{images/tsne/hmdb_ucf-ucf101_train-rnn-GRU-5seg.pdf}}
     \subfloat[][ABG-TRN]{\includegraphics[width=0.23\linewidth,height=3.5cm]{images/tsne/hmdb_ucf-ucf101_train-trn-m-LSTM-5seg.pdf}}
    \vspace{-0.2cm}\caption{The t-SNE visualization of the learned source and target video representations on the HMDB$\rightarrow$UCF task.}
    \label{fig:tsne}
\end{figure*}

\begin{table}[t]
\caption{Experimental results under the semi-supervised setting on the Kinetics-Gameplay dataset.}\vspace{-0.2cm}
\resizebox{1\linewidth}{!}{\begin{tabular}{l c c c c c c c c} 
\toprule & \multicolumn{4}{c}{\textbf{Kinetics$\rightarrow$Gameplay}} & \multicolumn{4}{c}{\textbf{Gameplay$\rightarrow$Kinetics}}\\ 
\cmidrule(l){2-5}\cmidrule(l){6-9} \textbf{Method} &30\% &50\% &70\% &90\% &30\% &50\% &70\% &90\%\\ \midrule Source Only &16.29 &16.29 &16.29 &16.29 &14.82 &14.82 &14.82 &14.82\\ \midrule
DANN~\cite{DANN} &49.67 &56.74 &60.48 &65.02 &33.24 &40.05 &40.44 &43.38\\ JAN~\cite{JAN} &47.40 &53.94 &60.35 &61.28 &32.69 &22.95 &41.31 &32.75\\ AdaBN~\cite{AdaBN} &55.54 &59.95 &64.62 &67.56 &44.82 &47.67 &47.73 &48.00\\ MCD~\cite{MCD} &47.53 &52.60 &57.68 &59.95 &36.29 &40.08 &41.13 &42.05 \\ \midrule TA$^{2}$N~\cite{TAN}  &57.14 &60.08 &64.09 &65.02 &42.39 &44.82 &44.39 &45.66\\
TA$^{3}$N~\cite{TAN}  &56.61 &62.35 &63.02 &63.95 &43.25 &44.27 &44.02 &42.05\\
\midrule \textbf{Semi-ABG} &57.28 &64.35 &65.42 &68.36 &59.80 &62.19 &62.46 &63.67\\
\textbf{Semi-HABG} &\textbf{61.15} &\textbf{65.29} &\textbf{67.29}&\textbf{70.36} &\textbf{60.39} &\textbf{62.54} &\textbf{63.36} &\textbf{63.98}\\ \midrule
Target Only &54.21 &58.88 &61.55 &66.62 &39.96 &42.54 &44.58 &44.36\\
\bottomrule \vspace{-1cm}
\end{tabular}
}
\label{tab:Kinetics-Gameplay} \end{table}  


\subsection{Semi-supervised Learning}
To study the robustness of the proposed algorithm, we extend the unsupervised domain adaptation to a semi-supervised setting, where a part of target labels are available for training. Extensive experiments are conducted on the most challenging ``Synthetic-to-Real'' testbed, \textit{i.e.}, the \textbf{Kinetics-Gameplay} dataset, on which the results with varying ratios of target labels are reported in Table \ref{tab:Kinetics-Gameplay}. The seen ratio of target labels ranges from $0.3$ to $0.9$. Similarly, \textbf{Source Only} / \textbf{Target only} represents the backbone model trained with source / target data only. All image-level domain adaptation methods (line 2-5), basic classification model (line 1,10) and our models (line 8-9) utilize \textbf{AvgPool} as the frame aggregation function. \textbf{TA$^2$N} and \textbf{TA$^3$N} use the \textbf{TRN} aggregator, since TRN is the major part of their works. It can bee seen that the overall performance is lower on the \textbf{Gameplay$\rightarrow$Kinetics} compared to \textbf{Kinetics$\rightarrow$Gameplay}, due to the insufficient samples in the source domain. The proposed \textbf{Semi-ABG} and its hierarchical variant \textbf{Semi-HABG} achieve higher recognition accuracy on both two transfer tasks, since the integrated graphs help to propagate the label information and take a full advantage of the supervision. The respective recognition accuracies of the proposed HABG on two transfer takss are improved by up to $8.0\%$ and $39.6\%$ over the state-of-the-art \textbf{TA$^3$N} with only $30\%$ of target labels available.


\vspace{-0.3cm}
\subsection{Ablation Study}
To investigate the validity of the derived modules and objective functions, we compare the four variants of \textbf{ABG} model on the full UCF-HMDB dataset. The comparison results are summarized in Table \ref{tab:ablation}. Removing the bipartite graphs, the \textbf{ABG w/o Graph} suffers a drop dramatically compared with the full model. The \textbf{ABG w/o $\mathcal{L}_d$} is the variant without the conditional adversarial learning, decreasing the recognition accuracy by $8.4\%$ and $2.95\%$ on average for the \textbf{UCF$\rightarrow$HMDB} and\textbf{ HMDB$\rightarrow$UCF} tasks, respectively. The \textbf{ABG w/o $\mathcal{L}_y^t$} refers to the variant without the entropy loss for target data, which triggers a slight decrease on the model performance. \textbf{HABG} is the hierarchical variant of the plain ABG model, performing better on the challenging UCF$\rightarrow$HMDB transfer task, which is consistent with the findings in semi-supervised experiments. The \textbf{ABG} model is more versatile and suitable for small datasets and easier transfer tasks such as HMDB$\rightarrow$UCF.

\begin{table}[t]
\caption{The ablation performance of the proposed ABG and HABG models on the full UCF-HMDB dataset.}\vspace{-0.4cm}
\resizebox{1\linewidth}{!}{\begin{tabular}{l c c c c} 
\toprule & \multicolumn{2}{c}{\textbf{UCF$\rightarrow$HMDB}$_{full}$} & \multicolumn{2}{c}{\textbf{HMDB$\rightarrow$UCF}$_{full}$}\\ 
\cmidrule(l){2-3}\cmidrule(l){4-5}
\textbf{Method} &AvgPool &TRN &AvgPool &TRN\\ \midrule ABG w/o Graph &71.39 &73.89 &74.96  &74.61\\
ABG w/o $\mathcal{L}_d$ &72.78 &70.00 &82.14 &79.86\\
ABG w/o $\mathcal{L}_y^t$  &78.33 &75.83 &82.67 &81.79\\
HABG &\textbf{80.00} &\textbf{76.94} &82.49 &80.21\\
ABG &79.17 &76.67 &\textbf{85.11} &\textbf{81.79}\\
\bottomrule \vspace{-0.5cm}
\end{tabular}
}
\label{tab:ablation}
\end{table}

\vspace{-0.3cm}
\subsection{Parameter Sensitivity}
To study the effect of the loss coefficients, we conduct the experiments on the UCF-HMDB$_{full}$ dataset with the varying values of $\beta$ and $\gamma$. The $\beta$ and $\gamma$ are utilized to reconcile the adversarial loss and the entropy loss, respectively. We compare the proposed ABG model and its variant HABG integrated with the identical AvgPool frame aggregation function. As plotted in Figure \ref{fig:params}, the average accuracies for both \textbf{ABG} and \textbf{HABG} models become quite stable when reaching sufficiently large loss coefficients. This indicates that our framework is robust with respect to loss coefficients.

\vspace{-0.2cm}
\subsection{Visualization}
To shed a qualitative light on evaluating the proposed model with various aggregation functions, we conduct the experiments on the HMDB$\rightarrow$UCF task, and visualize the features with t-SNE in Figure \ref{fig:tsne}. The features are extracted from the last layer of \textbf{ABG} and the baseline models, including \textbf{DANN}, \textbf{JAN}, \textbf{MCD} and \textbf{TA$^3$N}. Different colors indicate different classes. Circles represent the source videos and triangles represent the target videos. It is clearly shown that the features from ABG achieve the tighter clusters compared to the baselines.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{images/Params_uh.pdf}\\
    \includegraphics[width=1\linewidth]{images/Params_hu.pdf}
    \caption{Performance comparisons of the proposed ABG and HABG with respect to the varying loss coefficients on the UCF$\rightarrow$HMDB (shown in the upper row) and HMDB$\rightarrow$UCF (shown in the bottom row) tasks.}
    \label{fig:params}
\end{figure}
