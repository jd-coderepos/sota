\section{Intra-session throughput prediction}
\label{sec:premodel}

The previous section reveals  significant throughput variation during a session
and the need for  good video bitrate adaptation schemes.  Ideally, we can
accurately predict the TCP throughput to select bitrates for the next few
chunks to optimize user perceived quality of
experience~\cite{yin2015controlvideo, tian2012towards}. However, this is
challenging and the limitations of existing prediction mechanisms
(\Section\ref{sec:premodel:strawman}) have even motivated efforts that avoid
throughput-based adaptation~\cite{huang2014dash}.  In this section, we describe
a simple but effective prediction motivated by the temporal structure in the
throughput. Before we do so, we describe strawman solutions considered in the
literature and their limitations in light of our observations.


\subsection{Strawman solutions}
\label{sec:premodel:strawman}

Our goal here is not to exhaustively enumerate all possible prediction
algorithms. As such, the models  we consider are representative of classical
time series models used in adaptive streaming
proposals~\cite{yin2015controlvideo, tcppredictability}.\footnote{We also
 tried  ``forecast'' models that extrapolated trends but these
performed worse and are not shown.}  At a high level, a
throughput prediction model can be viewed as a function of the observed
throughputs over the previous  $p$ epochs.  Let $W_t$ denote the  observed
throughput at epoch $t$ and $\hat{W}_t,\ldots,\hat{W}_{t+\Delta}$ denote the
estimate for the next $\Delta$ epochs.

\begin{packeditemize}

\item \textbf{{Last Sample} (LS)}: In the simplest case, we simply use the previous
observation; i.e., $\forall i \in [t,t+\Delta]:  \hat{W}_i=W_{t-1}$. The
problem with this approach is that a single sample will be a very noisy
estimator and thus may cause significant bitrate oscillations~\cite{yin2015controlvideo,jiang2014improving}.

\item \textbf{Arithmetic Mean (AM)}: To address the noise, we can consider
``smoothing'' using  $p$ measurements from  history; i.e.,   $ \forall i \in
[t,t+\Delta]: \hat{W}_i=\frac{\sum_{q=1}^p W_{t-q}}{p}$.  However, there are
still two fundamental problems.  First, if we use a small $p$,  outliers can
still cause significant under- or overestimation. Second, if we use a large
$p$,  measurements made too far back in history may induce serious biases as we saw in
Figure~\ref{fig:example_session}.

\item \textbf{Harmonic Mean (HM)}: One way to minimize the impact of outliers
in AM is using a harmonic mean~\cite{jiang2014improving}: $\forall i \in
[t,t+\Delta]:\hat{W}_i = \frac{1}{\sum_{q=1}^p \frac{1}{W_{t-q}}}$. While this
addresses the outlier problem, uncorrelated  measurements
too far in  history can still bias the predictions.

\item \textbf{Auto-regressive models (ARMA,AR):} Auto-regressive moving average
(ARMA)  is a classical timeseries modeling technique~\cite{tcppredictability}.
The ARMA model assumes  $W_t$ has the following form: $ W_{t} = a_0 +
\sum_{j=1}^p a_j W_{t-j} + \sum_{j=1}^q b_j S_{t-j}$, where $S_t \sim
N(0,\sigma^2)$ is i.i.d. Gaussian noise,  independent of $W_t$.  $p,q$ are the
sizes of the sliding windows for auto-regression and moving average,
respectively, and $\theta_{ARMA} = \{\{a_i\}_{i=0}^p, \{b_i\}_{i=1}^q\}$ are
the parameters that can be learned from training data (e.g., historical
sessions).  The auto-regression (AR) model is  a simplified version of ARMA
that  assumes  $W_{t} = a_0 + \sum_{j=1}^p a_j W_{t-j} + e_t$, where $a_0$ is a
constant and $e_t$ is 
i.i.d. zero-mean Gaussian noise independent of $W_t$.
$\theta_{AR} = \{\{a_i\}_{i=0}^p\}$.  Given training data and
$p,q$, Yule-Walker equations can be adopted to learn the parameters
$\theta_{ARMA}$, or $\theta_{AR}$.  The key problem with these models is that
they have implicit independence and stationary assumptions.   However,
Figures~\ref{fig:autocorrelation} and~\ref{fig:example_session} suggest that
there is some inherent ``stateful'' and ``evolving'' temporal structure in the
throughput, which contradicts these assumptions.


\end{packeditemize}


\subsection{Using a Hidden Markov Model}
\label{sec:premodel:hmm}

Hidden Markov models (HMM) are widely used in many applications, ranging from
speech recognition to event detection \cite{bishop2006pattern}. From a
networking perspective, the intuition behind the use of HMM in our context is
that the throughput depends on the hidden state---the number of flows
sharing the bottleneck link.  The visualization in
Figure~\ref{fig:example_session} confirms this intuition that the throughput
has some stateful evolving behaviors.  By capturing these state transitions and
the dependency between the throughput vs.\ the hidden state, using HMM can
yield more robust throughput predictions.

\mypara{Model specification} Suppose the throughput depends on some hidden
state variables $X_t\in{\cal X}$, where ${\cal X} = \{x_1,\cdots, x_M\}$ is the
set of possible states and $M = |{\cal X}|$ is the number of states.  The state
evolves as a Markov process where the likelihood of the current state only
depends on the last state, i.e., $\mathbb{P}(X_t|X_{t-1},X_{t-2},\cdots, X_1) =
\mathbb{P}(X_t|X_{t-1})$. We denote the transition probability matrix by
$P=\{P_{ij}\}$, where $P_{ij} = \mathbb{P}(X_t = x_i|X_{t-1}=x_j)$.  We let the
probability distribution vector $\pi_t = (\mathbb{P}(X_t = x_1), \cdots,
\mathbb{P}(X_t = x_M))$.  Then $\pi_{t+\tau} = \pi_{t}P^{\tau}$.  Each state ``emits''
the throughput expected within that state. Within each  hidden state
$X_t$, we model  the  throughput $W_t$ by a  Gaussian distribution; i.e., $W_t|X_t=x \
\sim N(\mu_x, \sigma_x^2)$.

To see this concretely, let us revisit Figure~\ref{fig:example_session}.  Here,
we can conceptually think of splitting the timeseries into roughly 11
 segments each corresponding to a hidden  state.  Within each segment, the
throughput is largely Gaussian;  e.g., between timeslots 20--75 the throughput
has mean 2900, and in slots 10-20 and 125--135 the mean is  2500.

\mypara{Model learning} Given number of states $M$, we can  use training data
to  learn the parameters of HMM, $\theta_{HMM} = \{\pi_0, P, \{(\mu_x,
\sigma_x^2), x\in{\cal X}\}\}$ via the expectation-maximization (EM) algorithm
\cite{bishop2006pattern}.  Note that the number of states $M$ needs to be
specified. There is a tradeoff here in choosing suitable $M$.  Smaller $M$
yields simpler models, but may be inadequate to represent the space of possible
behaviors.  On the other hand, a large $M$ leads to more complex model with
more parameters, but may in turn lead to overfitting issues.   We find
empirically that $M=6$ is a ``sweet spot'' in the tradeoff (Figure~\ref{fig:hmmstate}).

\mypara{Online throughput prediction} At time $t$, given past throughput $W_{1:t-1}=\{W_1,\cdots,W_{t-1} \}$, we first use
forward-backward algorithm \cite{bishop2006pattern} to determine
$\pi_{t-1|1:t-1} = (\mathbb{P}(X_{t-1} = x_1|W_{1:t-1}), \cdots,
\mathbb{P}(X_{t-1} = x_M|W_{1:t-1}))$. Then the distribution of $X_{t+\tau}$ can be
obtained by: $\pi_{t+\tau|1:t-1} = \pi_{t-1|1:t-1}P^{\tau+1}$. Finally, we compute
 the maximum likelihood estimate of $W_{t+\tau}, \tau > 0$  as
$\hat{W}_{t+\tau} = \mu_x$, where $x = arg\max_{x\in{\cal X}}
\mathbb{P}(X_{t+\tau}=x|W_{1:t-1})$.




