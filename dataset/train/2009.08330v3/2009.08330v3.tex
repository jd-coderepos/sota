\pdfoutput=1








\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp2020}
\usepackage{times}
\usepackage{latexsym}
\usepackage{soul}
\usepackage{url}
\renewcommand{\UrlFont}{\ttfamily\small}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{pifont}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{xparse}
\urlstyle{same}
\usepackage{boldline}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz-dependency}
\usepackage{wrapfig}
\usepackage{pgfplots}
\usepackage{filecontents}
\usepackage[T1]{fontenc}
\usepackage{subfiles}
\usepackage{readarray}
\usepackage{xcolor}
\usepackage{import}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{microtype}


\aclfinalcopy \def\aclpaperid{2971} 




\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\softmax}{\operatornamewithlimits{softmax}}
\newcommand{\xvec}{\mathbf{x}}
\newcommand{\yvec}{\mathbf{y}}
\newcommand{\bvec}{\mathbf{b}}
\newcommand{\cvec}{\mathbf{c}}
\newcommand{\evec}{\mathbf{e}}
\newcommand{\hvec}{\mathbf{h}}
\newcommand{\wvec}{\mathbf{w}}
\newcommand{\Wvec}{\mathbf{W}}
\newcommand{\Rvec}{\mathbf{R}}
\newcommand{\zvec}{\mathbf{z}}
\newcommand{\svec}{\mathbf{s}}
\newcommand{\tvec}{\mathbf{t}}
\newcommand{\rvec}{\mathbf{r}}
\newcommand{\dvec}{\mathbf{d}}
\newcommand{\mcL}{\mathcal{L}}
\newcommand{\mcR}{\mathcal{R}}
\newcommand{\mcT}{\mathcal{T}}
\newcommand{\mcY}{\mathcal{Y}}
\newcommand{\mcZ}{\mathcal{Z}}
\newcommand{\mcV}{\mathcal{V}}
\newcommand{\mcC}{\mathcal{C}}
\newcommand{\mcA}{\mathcal{A}}
\newcommand{\cmark}{\textcolor{blue}{\ding{51}}}
\newcommand{\xmark}{\textcolor{red}{\ding{55}}}



\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{More Embeddings, Better Sequence Labelers?}



\author{\parbox{\linewidth}{\centering Xinyu Wang$^{\diamond\ddagger}$, Yong Jiang$^{\dagger}$\textsuperscript{$\ast$}, Nguyen Bach$^{\dagger}$, \\ Tao Wang$^{\dagger}$, Zhongqiang Huang$^{\dagger}$, Fei Huang$^{\dagger}$,  Kewei Tu$^{\diamond}$\thanks{\hspace{1mm} Yong Jiang and Kewei Tu are the corresponding authors. $^{\ddagger}$: This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy.}} \\
 $^\diamond$School of Information Science and Technology, ShanghaiTech University \\
 $^{\diamond}$Shanghai Engineering Research Center of Intelligent Vision and Imaging \\
$^{\diamond}$University of Chinese Academy of Sciences \\
 $^\dagger$DAMO Academy, Alibaba Group \\
  {\tt \{wangxy1,tukw\}@shanghaitech.edu.cn} \\
  {\tt \{yongjiang.jy,nguyen.bach\}@alibaba-inc.com} \\
  {\tt \{leeo.wangt,z.huang,f.huang\}@alibaba-inc.com} \\
 }


\date{}

\begin{document}
\maketitle
\begin{abstract}
Recent work proposes a family of contextual embeddings that significantly improves the accuracy of sequence labelers over non-contextual embeddings. However, there is no definite conclusion on whether we can build better sequence labelers by combining different kinds of embeddings in various settings. 
In this paper, we conduct extensive experiments on 3 tasks over 18 datasets and 8 languages to study the accuracy of sequence labeling with various embedding concatenations and make three observations: (1) concatenating more embedding variants leads to better accuracy in rich-resource and cross-domain settings and some conditions of low-resource settings; (2) concatenating contextual sub-word embeddings with contextual character embeddings hurts the accuracy in extremely low-resource settings; (3) based on the conclusion of (1), concatenating additional similar contextual embeddings cannot lead to further improvements. We hope these conclusions can help people build stronger sequence labelers in various settings.


\end{abstract}


\section{Introduction}
\label{sec:introduction}
In recent years, sequence labelers equipped with contextual embeddings have achieved significant accuracy improvement \cite{peters-etal-2018-deep,akbik-etal-2018-contextual,devlin-etal-2019-bert,martin2019camembert} over approaches that use static non-contextual word embeddings \cite{mikolov2013distributed} and character embeddings \cite{santos2014learning}.
Different types of embeddings have different inductive biases to guide the learning process. However, little work has been done to study how to concatenate these contextual embeddings and non-contextual embeddings to build better sequence labelers in multilingual, low-resource, or cross-domain settings over various sequence labeling tasks. In this paper, we empirically investigate the effectiveness of concatenating various kinds of embeddings for multilingual sequence labeling and try to answer the following questions:
\begin{enumerate}
    \item In rich-resources settings, does combining different kinds of contextual embeddings result in a better sequence labeler? Are non-contextual embeddings helpful when the models are equipped with contextual embeddings?
    \item When we train models in low-resource and cross-domain settings, do the conclusions from the rich-resource settings still hold? 
    \item Can sequence labelers automatically learn the importance of each kind of embeddings when they are concatenated?
\end{enumerate}




\section{Model Architecture}
\subsection{Sequence Labeling}
We use the BiLSTM structure for all the sequence labeling tasks, which is one of the most popular approaches to sequence labeling \cite{huang2015bidirectional,ma-hovy-2016-end}. Given a $n$ word sentence $\xvec = \{x_1, \cdots, x_n\}$ and $L$ kinds of embeddings, we feed the sentence to generate the $l$-th kind of word embeddings $\{\evec^l_1, \cdots, \evec^l_n\}$:
\begin{align}
     \evec_i^l &= \text{embed}^l (\xvec) \nonumber
\end{align}
We concatenate these embeddings to generate the word representations $\{\rvec_1, \cdots, \rvec_n\}$ as the input of the BiLSTM layer:
\begin{align}
     \rvec_i &= \evec_i^1 \oplus \dots \oplus \evec_i^L \nonumber
\end{align}
where $\oplus$ represents the vector concatenation operation. We feed the word representations into a single-layer BiLSTM to generate the contextual hidden layer of each word. Then we use either a Softmax layer (the MaxEnt approach) or a Conditional Random Field layer (the CRF approach) \cite{10.5555/645530.655813,lample-etal-2016-neural,ma-hovy-2016-end} fed with the hidden layers to generate the conditional probability $p(\yvec|\xvec)$. Given the corresponding sequence of gold labels $\yvec^* = \{y_1^*, \cdots, y_n^*\}$ for the input sentence, the loss function for a model with parameters $\theta$ is:
\begin{displaymath}
\mcL_{\theta} = - \log p(\yvec^*|\xvec;\theta)
\end{displaymath}

\subsection{Embeddings}
There are mainly four kinds of embeddings that have been proved effective on the sequence labeling task: contextual sub-word embeddings, contextual character embeddings, non-contextual word embeddings and non-contextual character embeddings\footnote{We do not use contextual word embeddings such as ELMo \cite{peters-etal-2018-deep} since \citet{akbik-etal-2018-contextual} showed that concatenating Flair embeddings with ELMo embeddings cannot further improve the accuracy.}.
As we conduct our experiments in multilingual settings, we need to select suitable embeddings from each category for the concatenation. 

\paragraph{Contextual Sub-word Embeddings (CSEs)} 
\textbf{CSEs} such as OpenAI GPT \cite{radford2018improving} and BERT \cite{devlin-etal-2019-bert} are based on transformer \cite{vaswani2017attention} and use WordPiece embeddings \cite{sennrich-etal-2016-neural,wu2016google} as input.
Much research has focused on improving BERT model's performance such as better masking strategy \cite{liu2019roberta} and cross-lingual training \cite{conneau2019cross}. 
Since we focus on the multilingual settings of sequence labeling tasks, we use multilingual BERT (M-BERT), as recent researches shows its strong generalizability over various languages and tasks \cite{pires-etal-2019-multilingual,karthikeyan2020cross}.

\paragraph{Contextual Character Embeddings (CCEs)} 
\citet{liu2018empower} proposed a character language model by applying the BiLSTM over the sentence and trained jointly with the sequence labeling task. (Pooled) Contextual string embeddings (Flair) \cite{akbik-etal-2018-contextual,akbik-etal-2019-pooled} are pretrained on a large amount of unlabeled data and result in significant improvements for sequence labeling tasks. We use the Flair embeddings due to their high accuracy for sequence labeling task\footnote{We do not use the pooled version of Flair due to its slower speed in training.}.


\paragraph{Non-contextual Word Embeddings (NWEs)} The most common approach to the \textbf{NWEs} is Word2vec \cite{mikolov2013distributed}, which is a skip-gram model learning word representations by predicting neighboring words. Based on this approach, GloVe \cite{pennington2014glove} creates a co-occurrence matrix for global information and fastText \cite{bojanowski2017enriching} represents each word as an n-gram of characters. We use fastText in our experiments as there are pretrained embeddings for 294 languages.

\paragraph{Non-contextual Character Embeddings (NCEs)} 
Using character information to represent the embeddings of word is proposed by \citet{santos2014learning} with a lot of following work using a CNN structure to encode character representation \cite{dos-santos-guimaraes-2015-boosting,chiu-nichols-2016-named,ma-hovy-2016-end}. \citet{lample-etal-2016-neural} utilized BiLSTM on the character sequence of each word. We follow this approach as it usually results in better accuracy \cite{yang-etal-2018-design}.


\section{Experiments and Results}
For simplicity, we use \textbf{M} to represent M-BERT embeddings, \textbf{F} to represent Flair embeddings, \textbf{W} to represent fastText embeddings, \textbf{C} to represent non-contextual character embeddings, \textbf{All} to represents the concatenation of all types of embeddings and the operator ``+'' to represent the concatenation operation. We use the MaxEnt approach for all experiments\footnote{We find that the observations from the MaxEnt experiments do not change in all experiments with the CRF approach.}. Due to the space limit, some detailed experiment settings, extra experiments and discussions are included in the appendix. 

\subsection{Settings}
\paragraph{Datasets}
We use datasets from three multilingual sequence labeling tasks over 8 languages in our experiments: WikiAnn NER datasets \cite{pan-etal-2017-cross}, UD Part-Of-Speech (POS) tagging datasets \cite{nivre-etal-2016-universal}, and CoNLL 2003 chunking datasets \cite{tjong-kim-sang-de-meulder-2003-introduction}. We use language-specific fastText and Flair embeddings depending on the dataset. 





\begin{filecontents}{wikiner.dat}
sentences word+flair flair mbert+word+flair mbert+word+flair+char mbert+word+char mbert+word
1 10.26945833 9.136458333 4.88171875 5.121927083 0.94484375 0.665677083
2 7.923037202 6.876287202 6.725907738 6.611547619 2.475520833 2.098251488
3 6.0245 3.322 6.559416667 6.545916667 2.7955 2.66675
4 1.107083333 -0.844333333 3.845083333 3.8825 2.684666667 2.593041667
5 0.834416667 -1.530458333 3.249041667 3.162166667 2.048916667 2.114041667
6 0.869147727 -1.596778846 3.05891369 3.09421875 1.762291667 1.707864583
\end{filecontents}

\begin{filecontents}{pos.dat}
sentences word+flair flair mbert+word+flair mbert+word+flair+char mbert+word+char mbert+word
1 9.527447917 9.277197917 8.140416667 8.2290625 2.587760417 2.157552083
2 7.573160714 7.206994048 6.845714286 6.906309524 2.642380952 2.318422619
3 5.83075 5.446895833 5.562333333 5.552083333 2.365958333 2.079416667
4 3.0793125 2.913666667 3.161 3.1615 1.749125 1.597
5 2.59275 2.387 2.8015 2.7515 1.6120625 1.496375
6 1.262072917 1.026125 1.443296131 1.43875 0.950520833 0.867447917
\end{filecontents}


\begin{filecontents}{atis.dat}
sentences word+flair flair mbert+word+flair mbert+word+flair+char mbert+word+char mbert+word
1 7.701805556 1.466527778 6.759305556 6.700833333 4.392777778 4.485
2 7.299166667 2.746666667 6.209305556 6.395694444 2.347777778 1.731944444
3 4.760694444 0.999722222 4.611289683 4.696111111 2.582638889 2.208194444
4 0.955555556 -1.113333333 1.716777778 1.586333333 0.420333333 0.559444444
5 1.087222222 -0.98875 1.887361111 1.92 0.800833333 0.8475
6 1.326527778 -0.764166667 1.829166667 1.868333333 0.879305556 0.857361111
\end{filecontents}

\begin{filecontents}{chunk.dat}
sentences word+flair flair mbert+word+flair mbert+word+flair+char mbert+word+char mbert+word
1 13.204375 12.575625 10.91020833 11.04875 1.703541667 1.963125
2 7.593333333 6.98375 5.947916667 5.871875 1.480625 1.427916667
3 6.342291667 5.839375 5.359375 5.476875 2.231041667 2.058958333
4 2.812 2.511333333 2.729 2.811 1.5465 1.4475
5 2.315625 2.091875 2.310625 2.338125 1.415 1.28125
6 1.7325 1.103958333 1.691875 1.684583333 1.4125 1.245416667
\end{filecontents}

\begin{filecontents}{bert.dat}
sentences mbert
1 0
2 0 
3 0 
4 0 
5 0 
6 0
\end{filecontents}

\begin{figure*}[ht]
\centering
\begin{tikzpicture}
    \node at (7.0,-1.0) {\large \# of Sentences};
    \node [rotate=90] at (-0.6,1.2) {Relative Scores};
    \begin{axis}[
name=ner,
width=0.37\textwidth,
        height=0.25\textwidth,
xlabel=\small NER,
legend columns=6, 
        legend pos=north west,
        legend style={font=\small,at={(0.5,-0.5)}},
        tick label style={font=\small},
        xticklabels={10,50,100,500,1000,All},
        xtick={1,2,3,4,5,6},
        xlabel style={yshift=0.2cm},
        ]
        \addplot[black,mark=o] table[x=sentences,y=flair] {wikiner.dat};
        \addplot[black,mark=*] table[x=sentences,y=word+flair] {wikiner.dat};
        \addplot[red,mark=square*] table[x=sentences,y=mbert+word] {wikiner.dat};
        \addplot[blue,mark=square*] table[x=sentences,y=mbert+word+char] {wikiner.dat};
        \addplot[red,mark=triangle*] table[x=sentences,y=mbert+word+flair] {wikiner.dat};
        \addplot[blue,mark=triangle*] table[x=sentences,y=mbert+word+flair+char] {wikiner.dat};
        \addplot[black,dashed] table[x=sentences,y=mbert] {bert.dat};
        \legend{F,F+W,M+W,M+W+C,M+F+W,All}
    \end{axis}
    \begin{axis}[
at={(ner.south east)},
        xshift=0.5cm,
        name=pos,
        width=0.37\textwidth,
        height=0.25\textwidth,
xlabel=\small POS,
legend columns=4, 
        legend pos=north west,
        legend style={font=\tiny,at={(0.5,-0.5)}},
        tick label style={font=\small},
        xticklabels={10,50,100,500,1000,All},
        xtick={1,2,3,4,5,6},
        xlabel style={yshift=0.2cm},
]
\addplot[black,mark=o] table[x=sentences,y=flair] {pos.dat};
        \addplot[black,mark=*] table[x=sentences,y=word+flair] {pos.dat};
        \addplot[red,mark=square*] table[x=sentences,y=mbert+word] {pos.dat};
        \addplot[blue,mark=square*] table[x=sentences,y=mbert+word+char] {pos.dat};
        \addplot[red,mark=triangle*] table[x=sentences,y=mbert+word+flair] {pos.dat};
        \addplot[blue,mark=triangle*] table[x=sentences,y=mbert+word+flair+char] {pos.dat};
        table[x=sentences,y=mbert] {bert.dat};
\end{axis}
    \begin{axis}[
at={(pos.south east)},
        xshift=0.5cm,
name=chunk,
        width=0.37\textwidth,
        height=0.25\textwidth,
xlabel=\small Chunking,
legend columns=4, 
        legend pos=north west,
        legend style={font=\small,at={(0.15,-0.5)}},
        xticklabels={10,50,100,500,1000,All},
        tick label style={font=\small},
        xtick={1,2,3,4,5,6},
        xlabel style={yshift=0.2cm},
]
\addplot[black,mark=o] table[x=sentences,y=flair] {chunk.dat};
        \addplot[black,mark=*] table[x=sentences,y=word+flair] {chunk.dat};
        \addplot[red,mark=square*] table[x=sentences,y=mbert+word] {chunk.dat};
        \addplot[blue,mark=square*] table[x=sentences,y=mbert+word+char] {chunk.dat};
        \addplot[red,mark=triangle*] table[x=sentences,y=mbert+word+flair] {chunk.dat};
        \addplot[blue,mark=triangle*] table[x=sentences,y=mbert+word+flair+char] {chunk.dat};
        table[x=sentences,y=mbert] {bert.dat};
\end{axis}
\end{tikzpicture}
\caption{Relative score improvements against models with M-BERT embeddings for three tasks.}
\label{fig:lowres}
\end{figure*}



\paragraph{Embedding Concatenation} Since experimenting on all 15 concatenation combinations of the four embeddings is not essential for evaluating the effectiveness of each kind of embeddings, we experiment on the following 7 concatenations: \textbf{F}, \textbf{F+W}, \textbf{M}, \textbf{M+W}, \textbf{M+W+C}, \textbf{M+F+W}, \textbf{All}. Through these concatenations, we can answer the following questions: (1) whether \textbf{NWEs} are still helpful (\textbf{F} vs. \textbf{F+W} and \textbf{M} vs. \textbf{M+W}); (2) whether \textbf{NCEs} are still helpful (\textbf{M+W} vs. \textbf{M+W+C} and \textbf{M+F+W} vs. \textbf{All}); (3) whether concatenating different contextual embeddings results in a better sequence labeler (\textbf{F+W} vs. \textbf{M+F+W} and \textbf{M+W} vs. \textbf{M+F+W}); (4) which one is the best concatenation.








\begin{table}[t!]
\setlength\tabcolsep{4pt}
\centering
\small
\begin{tabular}{l|cccc||ccc|c}
\hlineB{4}
& \multicolumn{4}{c||}{\bf \textsc{Embeddings}} & \multicolumn{4}{c}{\bf \textsc{Tasks}} \\ \hhline{~|----||----}
& \textbf{M} & \textbf{F} & \textbf{W} & \textbf{C} &  {\bf\textsc{NER}}  & {\bf\textsc{POS}}  & {\bf\textsc{Chunk}} & {\bf\textsc{Avg.}} \\ 
\hline\hline
1. & \xmark & \cmark & \xmark & \xmark & 82.1 & 96.3 & 92.3 & 90.2 \\
2. & \xmark & \cmark & \cmark & \xmark & 84.6 & 96.5 & \textbf{92.9} & 91.4 \\
3. & \cmark & \xmark & \xmark & \xmark & 83.8 & 95.3 & 91.3 & 90.1 \\
4. & \cmark & \xmark & \cmark & \xmark & 85.5 & 96.1 & 92.5 & 91.4 \\
5. & \cmark & \xmark & \cmark & \cmark & 85.5 & 96.2 & 92.6 & 91.5 \\
6. & \cmark & \cmark & \cmark & \xmark & \textbf{86.8} & \textbf{96.7} & \textbf{92.9} & \textbf{92.1} \\
7. & \cmark & \cmark & \cmark & \cmark & \textbf{86.8} & \textbf{96.7} & \textbf{92.9} & \textbf{92.1} \\
\hlineB{4}
\end{tabular}
\caption{Averaged F1 scores over languages for each task with different embedding concatenations.}
\label{tab:main_results}
\end{table}





\begin{filecontents}{zs_ner.dat}
Type val
F+W 21.39350877
F 23.02319079
M+F+W 41.91631579
All 41.97504386
M+W+C 45.90416667
M+W 47.26492481
M 54.27210526
\end{filecontents}

\begin{filecontents}{zs_pos.dat}
Type val
F+W 23.99609375
F 21.100875
M+F+W 31.74304688
All 31.52898438
M+W+C 43.76242188
M+W 45.938125
M 69.35196429
\end{filecontents}

\begin{filecontents}{zs_mixed.dat}
Task F+W M+F+W All M+W+C M+W M F
NER 21.39350877 41.91631579 41.97504386 45.90416667 47.26492481 54.27210526 23.02319079
POS 23.99609375 31.74304688 31.52898438 43.76242188 45.938125 69.35196429 21.100875
\end{filecontents}










\begin{filecontents}{cross_domain.dat}
model avg avg_std en en_std nl nl_std es es_std de de_std
F+W 48.56416667 0.558231334 48.05 1.031729939 52.9 0.340978983 50.14 0.277968823 43.16666667 0.645049524
M+F+W 50.85583333 0.460169051 51.73333333 0.64339378 54.75666667 0.488830805 50.87666667 2.218773435 46.05666667 0.116714276
All 50.36083333 1.443706649 52.29666667 0.493783578 54.70333333 0.291013554 47.88333333 0.288020061 46.56 0.37094474
M+W+C 49.14416667 0.347594911 51.2 1.706536453 51.86666667 0.321904057 46.92666667 2.507925216 46.58333333 0.393728615
M+W 49.55666667 0.446119067 52.45666667 1.652883003 52.68333333 0.424604391 46.4 0.911957601 46.68666667 0.637198731
M 47.39352922 0.719517745 49.246 0.730659976 49.92454545 0.687014141 45.245 2.758853929 45.15857143 0.868930847
\end{filecontents}

\begin{filecontents}{cross_domain2.dat}
model F+W F+W_std M+F+W M+F+W_std All All_std M+W+C M+W+C_std M+W M+W_std M M_std
de 43.16666667 0.645049524 46.05666667 0.116714276 46.56 0.37094474 46.58333333 0.393728615 46.68666667 0.637198731 45.15857143 0.868930847
en 48.05 1.031729939 51.73333333 0.64339378 52.29666667 0.493783578 51.2 1.706536453 52.45666667 1.652883003 49.246 0.730659976
es 50.14 0.277968823 50.87666667 2.218773435 47.88333333 0.288020061 46.92666667 2.507925216 46.4 0.911957601 45.245 2.758853929
nl 52.9 0.340978983 54.75666667 0.488830805 54.70333333 0.291013554 51.86666667 0.321904057 52.68333333 0.424604391 49.92454545 0.687014141
avg 48.56416667 0.558231334 50.85583333 0.460169051 50.36083333 1.443706649 49.14416667 0.347594911 49.55666667 0.446119067 47.39352922 0.719517745
\end{filecontents}


\subsection{Rich-resource and Low-resource Settings}
How to build better sequence labelers through embedding concatenations in both rich-resource and low-resource settings is the most important concern for users. We report the results of various concatenations of embeddings for the tasks in Table \ref{tab:main_results} for rich-resource settings and in Figure \ref{fig:lowres} for low-resource settings. From the results, we have the following observations.

\noindent\textbf{Observation \#1. Concatenating more embedding variants results in better sequence labelers:} In rich-resource settings, concatenating more embedding variants (\textbf{M+F+W} and \textbf{All}) results in best scores in most of the cases, which indicates that the inductive biases in various kind of embeddings are helpful to train a better sequence labeler.
In low-resource settings, \textbf{M+F+W} and \textbf{All} performs inferior to the \textbf{F+W} when the number of sentences are lower than 100. However, when the training set gets larger, the gap between these concatenations becomes smaller and reverses when the training set becomes larger than 100 for NER and POS tagging and the gap also disappears for Chunking. A possible reason is that using \textbf{CSEs} makes the model sample inefficient so that \textbf{CSEs} requires more training samples to improve accuracy than \textbf{CCEs}. The observation suggests that concatenating more embedding variants performs better if the training set is not extremely small.

\noindent\textbf{Observation \#2. \textbf{NCEs} become less effective when concatenated with \textbf{CSEs} and \textbf{CCEs}:} Concatenating \textbf{NCEs} with \textbf{CSEs} only marginally improves the accuracy. There is almost no improvement when concatenated with both \textbf{CSEs} and \textbf{CCEs} but the \textbf{NCEs} does not hurt the accuracy as well. A possible reason is that the \textbf{CSEs} and \textbf{CCEs} largely contain the information in \textbf{NCEs}\footnote{The observation is consistent with the observation of \citet{akbik-etal-2018-contextual}, but we experimented on more languages and tasks with the M-BERT embeddings.}.


\noindent\textbf{Observation \#3. \textbf{NWEs} are significantly helpful on top of contextual embeddings:} Although models based on contextual embeddings have proved to be stronger than models based on \textbf{NWEs} for sequence labeling, concatenating \textbf{NWEs} with contextual embeddings can still improve the accuracy significantly. The results imply that the contextual embeddings contain more contextual information over the input but lack static word information.

From these observations, we find that in most of rich-resource and low-resource settings, concatenating all embeddings variants or all embeddings variants except \textbf{NCEs} is the simplest choice for a better sequence labeler.



\begin{table}[t]
\small
\centering
\setlength\tabcolsep{2.5pt}
\begin{tabular}{l||ccccccc}
\hlineB{4}
 & \textbf{F} & \textbf{F+W} & \textbf{M} & \textbf{M+W} & \textbf{M+W+C} & \textbf{M+F+W} & \textbf{All} \\
 \hline
\textsc{\textbf{ Avg.}} & 46.3 & 48.6 & 47.4 & 48.4 & 48.7 & 49.9 & \textbf{50.4}\\
\hlineB{4}
\end{tabular}
\caption{Cross-domain transfer from the Wikipedia domain to the news domain on the NER task.}
\label{tab:cross_domain}
\end{table}

\begin{filecontents}{mask_embed.dat}
type Flair char Word MBERT
NER 0.612576424 0 0.443612346 0.75839615
POS 0.903590049 0.259074475 0.728139403 0.862333208
Chunk 0.885280289 0.258842676 0.670596745 0.612188065
\end{filecontents}


\begin{figure}[t]
\centering
\begin{tikzpicture}

\begin{axis}[
    ybar=0pt,
bar width = {1em},
    width=0.49\textwidth,
    height=0.25\textwidth,
    enlarge x limits={abs=1cm},
    symbolic x coords={NER,POS,Chunk},
xticklabel style={
align=center, font=\small, },
    yticklabel style={font=\small},
    ylabel = {Score Percentage},
    ylabel style = {yshift=-0.5cm},
    legend style={font=\small,at={(0.85,-0.3)}},
legend columns=4, 
    xtick={NER,POS,Chunk},
    legend image code/.code={\draw[#1] (0cm,-0.1cm) rectangle (0.15cm,0.2cm);
    },
    ytick={0,0.5,1},
    ymin=0,
    ymax=1,
    ]
    \addplot[ybar,fill=blue] table [x=type, y=MBERT] {mask_embed.dat};
    \addplot[ybar,fill=red] table [x=type, y=Flair] {mask_embed.dat};
    \addplot[ybar,fill=green] table [x=type, y=Word] {mask_embed.dat};
    \addplot[ybar,fill=yellow] table [x=type, y=char] {mask_embed.dat};
    \legend{M-BERT,Flair,Word,Char};
\end{axis}
\node at (1.6,0.2) {\tiny 0.0};
\end{tikzpicture}
\caption{Importance of each embedding over the concatenation of \textbf{All} embeddings. The score percentage represents the average score preserving only one kind of embeddings divided by the score without masking.}
\label{fig:masking}
\end{figure}


\subsection{Cross-domain Settings}
Another concern for users is that we want to build better sequence labelers not only in in-domain settings but in out-of-domain settings as well. We conduct experiments in cross-domain settings to show how the embedding concatenations impact the accuracy when the distribution of training data and test data are different. We evaluate our Wikipedia NER models on CoNLL 2002/2003 NER \cite{tjong-kim-sang-2002-introduction,tjong-kim-sang-de-meulder-2003-introduction} datasets from the news domain. The results (Table \ref{tab:cross_domain}) are almost consistent with rich-resource settings, suggesting that concatenating more embedding variants results in better sequence labelers. 













\subsection{Importance of Embeddings}
\label{sec:importance}
To study the effectiveness of concatenating embeddings from another perspective, we preserve only one kind of embedding in \textbf{All} and mask out the other embeddings as 0 to study how the models rely upon each kind of embeddings. To avoid the impact of embedding dimensions, we train the model by linearly projecting each kind of embeddings into the same dimension of 4096. The results (Figure \ref{fig:masking}) show that the accuracy of preserved embeddings has a positive correlation with the results in Table \ref{tab:main_results}. For example, \textbf{M} gets higher accuracy than other embeddings in NER and Table \ref{tab:main_results} also shows that the model with \textbf{F} performs inferior to the model with \textbf{M} only. The models with concatenated embeddings almost do not rely on \textbf{NCEs} and relies mostly on \textbf{CSEs} or \textbf{CCEs} depending on the task. These results show that models with concatenated embeddings can extract helpful information from each kind of embeddings to improve accuracy.


\begin{table}[t!]
\small
\centering
\setlength\tabcolsep{5pt}
\begin{tabular}{cccccc||ccc}
\hlineB{4}
\multicolumn{6}{c||}{\bf \textsc{Embeddings}} & \multicolumn{3}{c}{\bf \textsc{Tasks}} \\ 
\hline
\textbf{M} & \textbf{F} & \textbf{W} & \textbf{C} & \textbf{B} &  \textbf{MF} & {\bf\textsc{NER}}  & {\bf\textsc{POS}}  & {\bf\textsc{Chunk}}\\ 
\hline
\multicolumn{9}{c}{\bf +En-BERT (English)}\\
\hline
\cmark & \cmark & \cmark & \cmark & \xmark & \xmark & 81.8 & 97.0 & 91.6 \\
\xmark & \cmark & \cmark & \cmark & \cmark & \xmark & 80.5 & \textbf{97.2} & \textbf{91.8} \\
\cmark & \cmark & \cmark & \cmark & \cmark & \xmark & \textbf{82.1} & \textbf{97.2} & 91.6 \\
\hline
\multicolumn{9}{c}{\bf +M-Flair (All languages)}\\
\hline
\cmark & \cmark & \cmark & \cmark & \xmark & \xmark & \textbf{86.8} & \textbf{96.7} & \textbf{92.9} \\
\cmark & \xmark & \cmark & \cmark & \xmark & \cmark & 86.1 & 96.5 & 92.8 \\
\cmark & \cmark & \cmark & \cmark & \xmark & \cmark & \textbf{86.8} & \textbf{96.7} & \textbf{92.9} \\
\hlineB{4}
\end{tabular}
\caption{Comparisons of the effectiveness for additionally concatenating the same category of embeddings. \textbf{B} represents the En-BERT embeddings and \textbf{MF} represents the M-Flair embeddings.}
\label{tab:same_embed}
\end{table}

\subsection{On Concatenating Similar Embeddings}
Since concatenating more embeddings variants results in better sequence labelers, we additionally concatenate multilingual Flair embeddings (M-Flair) or English BERT embeddings (En-BERT) with \textbf{All} embeddings to show whether concatenating the same category of embeddings can further improve the accuracy. We evaluate the addition of En-BERT on English and M-Flair on all languages in each task. The results are shown in Table \ref{tab:same_embed}. It can be seen that additionally concatenating the same category of embeddings does not further improve the accuracy in most cases except for concatenating En-BERT on English WikiAnn NER. A possible reason is that the BERT models are trained on the same domain as WikiAnn and hence the inductive biases of BERT embeddings help improve the accuracy.

We also find that concatenating En-BERT with \textbf{All} only improves the accuracy of WikiAnn English NER. We think the possible reason for the improvement is that the BERT and the training data have the same domain of Wikipedia. We conduct the same concatenation on the CoNLL English NER dataset for comparison. The results in Table \ref{tab:en_m_bert} show that concatenating En-BERT with \textbf{All} does not further improve the accuracy on CoNLL English NER.


\begin{table}[t!]
\setlength\tabcolsep{4pt}
\small
\centering
\begin{tabular}{l|ccc}
\hlineB{4}
{\bf \textsc{Embeddings}} & \multicolumn{3}{c}{\bf \textsc{Tasks}} \\  
 \hline
 & {\bf\textsc{NER}}  & {\bf\textsc{POS}}  & {\bf\textsc{Chunk}} \\
 \hline
\textbf{F+W} & 32.7 & 81.7 & 78.2 \\
\textbf{F+W}+Proj. & \textbf{33.2} & \textbf{82.3} & \textbf{79.0} \\
\textbf{All} & 27.5 & 80.4 & 76.1 \\
\hlineB{4}
\end{tabular}
\caption{Comparisons of \textbf{F+W}, \textbf{All}, and \textbf{F+W+proj} (\textbf{F+W} with linearly projecting the hidden size into the hidden size of \textbf{All}) in three tasks with 10-sentence low-resource settings. The accuracy is averaged over tasks.}
\label{tab:proj}
\end{table}




\begin{table}[t!]
\setlength\tabcolsep{4pt}
\small
\centering
\begin{tabular}{l|ccccc||ccc}
\hlineB{4}
& \multicolumn{5}{c||}{\bf \textsc{Embeddings}} & \multicolumn{3}{c}{\bf \textsc{Tasks}} \\  
\hhline{~|-----||---}
& \textbf{M} & \textbf{F} & \textbf{W} & \textbf{C} & \textbf{B}  & {\bf\textsc{NER}}  & {\bf\textsc{POS}}  & {\bf\textsc{Chunk}} \\
\hline\hline
\multicolumn{9}{c}{\bf \textsc{Low-Resource: 10 Sentences}}\\
\hline
1. & \xmark & \cmark & \cmark & \xmark & \xmark & \textbf{35.5}$\pm1.4$ & \textbf{80.2}$\pm0.1$ & \textbf{73.3}$\pm0.6$ \\
2. & \cmark & \cmark & \cmark & \cmark & \xmark & 25.4$\pm0.8$ & 77.9$\pm0.2$ & 70.8$\pm0.5$ \\
3. & \xmark & \cmark & \cmark & \cmark & \cmark & 29.3$\pm0.8$ & 79.6$\pm0.2$ & 67.9$\pm0.5$ \\
\hline\hline
\multicolumn{9}{c}{\bf \textsc{Low-Resource: 50 Sentences}}\\
\hline
1. & \xmark & \cmark & \cmark & \xmark & \xmark & \textbf{48.6}$\pm0.3$ & 88.8$\pm0.0$ & \textbf{82.2}$\pm0.0$ \\
2. & \cmark & \cmark & \cmark & \cmark & \xmark & 48.5$\pm0.4$ & 87.5$\pm0.1$ & 80.3$\pm0.3$ \\
3. & \xmark & \cmark & \cmark & \cmark & \cmark & 43.4$\pm0.9$ & \textbf{88.9}$\pm0.0$ & 78.8$\pm0.1$ \\
\hline\hline
\multicolumn{9}{c}{\bf \textsc{Low-Resource: 100 Sentences}}\\
\hline
1. & \xmark & \cmark & \cmark & \xmark & \xmark & 54.8$\pm0.5$ & 90.6$\pm0.1$ & \textbf{83.7}$\pm0.0$ \\
2. & \cmark & \cmark & \cmark & \cmark & \xmark & \textbf{56.8}$\pm0.1$ & 90.3$\pm0.0$ & 82.4$\pm0.0$ \\
3. & \xmark & \cmark & \cmark & \cmark & \cmark & 50.2$\pm0.5$ & \textbf{91.4}$\pm0.1$ & 82.9$\pm0.1$ \\
\hline\hline
\multicolumn{9}{c}{\bf \textsc{Low-Resource: 500 Sentences}}\\
\hline
1. & \xmark & \cmark & \cmark & \xmark & \xmark & 68.3$\pm0.2$ & 92.8$\pm0.0$ & 86.8$\pm0.0$ \\
2. & \cmark & \cmark & \cmark & \cmark & \xmark & \textbf{69.1}$\pm0.2$ & 93.0$\pm0.1$ & 86.7$\pm0.1$ \\
3. & \xmark & \cmark & \cmark & \cmark & \cmark & 67.3$\pm0.1$ & \textbf{93.9}$\pm0.1$ & \textbf{86.9}$\pm0.0$ \\
\hline\hline
\multicolumn{9}{c}{\bf \textsc{Low-Resource: 1000 Sentences}}\\
\hline
1. & \xmark & \cmark & \cmark & \xmark & \xmark & 72.0$\pm0.1$ & 94.0$\pm0.1$ & 87.1$\pm0.1$ \\
2. & \cmark & \cmark & \cmark & \cmark & \xmark & \textbf{75.2}$\pm0.3$ & 94.4$\pm0.1$ & 87.1$\pm0.2$ \\
3. & \xmark & \cmark & \cmark & \cmark & \cmark & 70.8$\pm0.1$ & \textbf{95.0}$\pm0.0$ & \textbf{87.6}$\pm0.1$ \\
\hline\hline
\multicolumn{9}{c}{\bf \textsc{Rich-Resource}}\\
\hline
1. & \xmark & \cmark & \cmark & \xmark & \xmark & 79.9$\pm0.3$ & 96.7$\pm0.0$ & 91.7$\pm0.1$ \\
2. & \cmark & \cmark & \cmark & \cmark & \xmark & \textbf{81.7}$\pm0.2$ & 97.0$\pm0.1$ & 91.6$\pm0.1$ \\
3. & \xmark & \cmark & \cmark & \cmark & \cmark & 80.5$\pm0.2$ & \textbf{97.2}$\pm0.0$ & \textbf{91.8}$\pm0.1$\\
\hlineB{4}
\end{tabular}
\caption{Comparisons of using English BERT instead of M-BERT in English datasets. \textbf{B} represents the En-BERT embeddings. We also provide the concatenation of Flair and pretrained word embeddings for reference.}
\label{tab:bert}
\end{table}

\begin{table}[t!]
\setlength\tabcolsep{4pt}
\small
\centering
\begin{tabular}{l|ccc}
\hlineB{4}
{\bf \textsc{Embeddings}} & \multicolumn{3}{c}{\bf \textsc{Tasks}} \\  
 \hline
 & {\bf\textsc{NER}}  & {\bf\textsc{POS}}  & {\bf\textsc{Chunk}} \\
 \hline
\textbf{All} & \textbf{86.8} & \textbf{96.7} & \textbf{92.9} \\
\textbf{All}+50d Proj. & 83.8 & 96.3 & 92.0 \\
\textbf{All}+1024d Proj. & 84.8 & 96.5 & 92.2 \\
\textbf{All}+4096d Proj. & 85.1 & 96.5 & 92.2 \\
\hlineB{4}
\end{tabular}
\caption{Comparisons of \textbf{All} with different linear projection size in three tasks with rich-resource settings. The accuracy is averaged over tasks.}
\label{tab:eachproj}
\end{table}

\subsection{English BERT vs. M-BERT}
We use English BERT embeddings instead of M-BERT embeddings to see whether the language-specific \textbf{CSEs} impact the observations. The results (Table \ref{tab:bert}) show that our observations do not change in both rich-resource and low-resource settings. Using a language-specific BERT embedding can even get better sequence labelers for the POS tagging and chunking tasks in rich-resource settings.

\subsection{Hidden Sizes and Accuracy}
In low-resource settings with 10 sentences, we find that models with \textbf{All} perform inferior to the models with \textbf{F+W}. One possible concern is that whether the larger hidden size of \textbf{All} introduces more parameters in the model and makes the model over-fits the training set. We linearly project the hidden size of \textbf{F+W} (4396) to the same hidden size as \textbf{All} (5214). Table \ref{tab:proj} shows that with linear projection, \textbf{F+W} performs even better. Therefore, the cause for over-fitting is not the inferior accuracy of \textbf{All} but possibly the sample inefficiency for \textbf{CSEs}. 

Another concern is whether we can project each embedding to a larger hidden size to improve the accuracy. Since we try a projection to 4096 for each embedding in \textbf{F+W+proj} (Section \ref{sec:importance}), we further project each embedding variants to see how the projection affect the accuracy in rich-resource settings. The results (Table \ref{tab:eachproj}) show that the linear projection for each embedding significantly decreases the accuracy of the models.

From the two experiments, we find that the hidden sizes of concatenated embeddings do not impact the observations.




\begin{table}[t!]
\setlength\tabcolsep{4pt}
\small
\centering
\begin{tabular}{ccccc||c}
\hlineB{4}
\multicolumn{5}{c||}{\bf \textsc{Embeddings}} & \multicolumn{1}{c}{\bf \textsc{Task}} \\  
\hhline{-----||-}
\textbf{M} & \textbf{F} & \textbf{W} & \textbf{C} & \textbf{B}  & {\bf\textsc{English NER}} \\
\hline
\cmark & \cmark & \cmark & \cmark & \xmark & \textbf{92.1}$\pm0.1$\\
\xmark & \cmark & \cmark & \cmark & \cmark & 92.0$\pm0.1$\\
\cmark & \cmark & \cmark & \cmark & \cmark & \textbf{92.1}$\pm0.1$\\
\hlineB{4}
\end{tabular}
\caption{Comparisons of concatenating En-BERT with \textbf{All} on CoNLL NER. \textbf{B} represents the En-BERT.}
\label{tab:en_m_bert}
\end{table}












\section{Conclusion}
In this paper, we analyze how to get a better sequence labeler by concatenating various kinds of embeddings. We make several empirical observations that we hope can guide future work to build better sequence labelers: 
(1) in most settings, concatenating more embedding variants leads to better results, while in extremely low-resource settings, only using \textbf{CSEs} and \textbf{NWEs} performs better; (2) \textbf{NCEs} become less effective when concatenated with contextual embeddings, while \textbf{NWEs} are still beneficial; (3) neural models can automatically learn which embeddings are beneficial to the task; (4) additionally concatenating similar contextual embeddings with the best concatenations from (1) cannot further improve the accuracy in most cases.

\section*{Acknowledgements}
This work was supported by the National Natural Science Foundation of China (61976139). This work also was supported by Alibaba Group through Alibaba Innovative Research Program. 
The authors wish to thank Chao Lou for his helpful comments and suggestions. 


\bibliographystyle{acl_natbib}
\bibliography{anthology,emnlp2020}




\appendix

\section{Appendix}
In this appendix, we use ISO 639-1 codes\footnote{\url{https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes}} to represent each language for simplification.
\subsection{Settings}
\paragraph{Datasets}
We use the following datasets for experiments:
\begin{itemize}
\item \textbf{Named Entity Recognition (NER)}: We use \textbf{{\bf WikiAnn}}\footnote{\url{https://elisa-ie.github.io/wikiann/}} \cite{pan-etal-2017-cross} datasets and CoNLL 2002/2003 NER\footnote{\url{https://www.clips.uantwerpen.be/conll2003/ner/}} \cite{tjong-kim-sang-2002-introduction,tjong-kim-sang-de-meulder-2003-introduction} datasets  for experiments. The {\bf WikiAnn} datasets contain silver standard NER tags over 282 languages. We select 8 languages from the dataset. We randomly choose 5000 sentences from the dataset for each language except English with 12000 sentences. We split the dataset by 3:1:1 for training/development/test. We use the standard training/development/test split for the CoNLL NER experiments.
    \item \textbf{Part-Of-Speech (POS) tagging}: We use universal POS tagging annotations in the {\bf Universal Dependencies} (UD) \cite{nivre-etal-2016-universal} datasets\footnote{\url{https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-2837}}. We choose one treebank for each language from the same 8 languages that are used in the WikiAnn experiments. The list of treebank are shown in Table \ref{tab:treebank}. We use the official train/development/test split for experiments.
    \item \textbf{Chunking}: We use the chunking datasets from the CoNLL 2003 shared task \cite{tjong-kim-sang-de-meulder-2003-introduction}, which contain two languages for chunking. We use the official train/development/test split for experiments.
\end{itemize}

\paragraph{Model Configuration and Running} For the embeddings, the hidden sizes for fastText and Flair embeddings\footnote{Details of Flair embeddings \url{https://github.com/flairNLP/flair/blob/master/resources/docs/embeddings/FLAIR_EMBEDDINGS.md}} are 300 and 4096, respectively. The dimension of character embeddings is set to 50\footnote{We did not observe further gains when increasing the dimension size.} following previous work \cite{lample-etal-2016-neural}. For M-BERT embeddings, we use the cased version that is trained on 104 languages for all datasets. We use the official release of \textit{bert-base-cased} model in the experiments for English BERT. The word embeddings are fine-tuned and character embeddings are trained for tasks while the Flair and BERT embeddings are fixed. Our codes are mainly based on the official release of Flair\footnote{\url{https://github.com/flairNLP/flair}} which is based on PyTorch v1.1.0 \cite{NEURIPS2019_9015}. We run our experiments on a GPU server with NVIDIA Tesla V100 GPU. For model training, we set the mini-batch size to 2,000 tokens for better GPU utilization. Following the official release of Flair, we use an SGD optimizer with a learning rate of 0.1 for training all models and set the hidden size of BiLSTM to 256. We anneal the learning rate by 0.5 if there is no improvement on the development sets for 10 and 100 epochs when training rich-resource and low-resource datasets respectively. We fix these hyper-parameters for all experiments because we find that tuning these hyper-parameters does not impact the observation and usually results in lower accuracy. We average over 5 runs for each experiment and report the macro-average score over all languages for each task. 

\paragraph{Pre-processing and Evaluation} We evaluate the NER and chunking by the F1 score and POS tagging by the accuracy. We use the evaluation script in the official release of Flair. We convert the BIO format into BIOES format for all NER and chunking datasets.




\subsection{Detailed Results}
For the models using the CRF layer, similar to the main paper, we plot our results in the rich-resource and low-resource settings in Figure \ref{fig:crf_lowres}. The figures have similar trends as the MaxEnt models, showing that output structures do not impact the observations.

Table \ref{tab:importance} shows the importance of each kind of embeddings for each language and task (Section 3.4 in the main paper). Table \ref{tab:ner}, \ref{tab:pos} and \ref{tab:chunk} show average scores over each language for each task in the rich-resource and low-resource settings (Section 3.2). Table \ref{tab:crf_ner}, \ref{tab:crf_pos} and \ref{tab:crf_chunk} show average scores over each language for each task in the rich-resource and low-resource settings. Table \ref{tab:app_cross_domain} shows the average scores for each language in our cross-domain experiments (Section 3.3). Table \ref{tab:mflair} show the detailed comparison for additionally concatenating M-Flair embeddings with \textbf{All} for all datasets (Section 3.5).


\begin{table}[t!]

\centering
\begin{tabular}{lr}
\hlineB{4}
Language & Treebank\\
\hline
ar & PADT\\
cs & FicTree\\
de & GSD\\
en & EWT\\
es & GSD\\
fr & Sequoia\\
nl & LassySmall\\
ta & TTB\\
\hlineB{4}
\end{tabular}
\caption{The list of treebank that we used in UD POS tagging.}
\label{tab:treebank}
\end{table}


\begin{filecontents}{app_wikiner.dat}
sentences word+flair flair mbert+word+flair mbert+word+flair+char mbert+word+char mbert+word
1 8.927375 9.0748125 3.053875 3.235 0.4729375 -1.123541667
2 7.0545 6.331875 4.237125 3.8386875 0.272416667 0.532583333
3 5.632666667 3.98475 5.017291667 3.799291667 1.6055625 1.966666667
4 0.979708333 -0.841083333 3.421145833 3.156958333 2.084708333 2.088583333
5 1.015416667 -0.984583333 3.203291667 2.863916667 2.436041667 2.313020833
6 0.909666667 -1.4071875 2.623125 2.6059375 1.5454375 1.5494375
\end{filecontents}

\begin{filecontents}{app_pos.dat}
sentences word+flair flair mbert+word+flair mbert+word+flair+char mbert+word+char mbert+word
1 9.023583333 8.505 7.38625 7.42775 2.0016875 1.815
2 7.27725 6.751854167 6.504666667 6.502916667 2.158833333 1.976291667
3 5.91675 5.593 5.647375 5.69 2.434291667 2.2818125
4 3.3431875 3.1216875 3.340791667 3.421729167 1.951145833 1.751395833
5 2.65875 2.52825 2.883458333 2.873208333 1.733958333 1.5776875
6 1.2896875 1.0916875 1.500416667 1.4959375 0.9599375 0.9091875

\end{filecontents}


\begin{filecontents}{app_chunk.dat}
sentences word+flair flair mbert+word+flair mbert+word+flair+char mbert+word+char mbert+word
1 11.089 11.0875 7.5745 8.371166667 0.6305 1.1955
2 9.657666667 9.226166667 5.157 5.594666667 2.484166667 2.168166667
3 6.055333333 6.147 4.545 3.238 0.597 0.893
4 2.629 2.1135 2.1465 2.0845 1.156 1.1255
5 2.242 2.024 2.099833333 2.116 1.238833333 1.033333333
6 1.502 1.019 1.3645 1.418 1.143 1.097

\end{filecontents}


\begin{figure*}[t!]
\centering
\begin{tikzpicture}
    \node at (7.0,-1.0) {\large \# of Sentences};
    \node [rotate=90] at (-0.6,1.5) {Relative Score};
    \begin{axis}[
name=ner,
width=0.37\textwidth,
        height=0.3\textwidth,
xlabel=\small NER,
legend columns=6, 
        legend pos=north west,
        legend style={font=\small,at={(0.5,-0.4)}},
        tick label style={font=\small},
        xticklabels={10,50,100,500,1000,All},
        xtick={1,2,3,4,5,6},
        xlabel style={yshift=0.2cm},
]
\addplot[black,mark=o] table[x=sentences,y=flair] {app_wikiner.dat};
        \addplot[black,mark=*] table[x=sentences,y=word+flair] {app_wikiner.dat};
        \addplot[red,mark=square*] table[x=sentences,y=mbert+word] {app_wikiner.dat};
        \addplot[blue,mark=square*] table[x=sentences,y=mbert+word+char] {app_wikiner.dat};
        \addplot[red,mark=triangle*] table[x=sentences,y=mbert+word+flair] {app_wikiner.dat};
        \addplot[blue,mark=triangle*] table[x=sentences,y=mbert+word+flair+char] {app_wikiner.dat};
        \legend{F,F+W,M+W,M+W+C,M+F+W,All}
    \end{axis}
    \begin{axis}[
at={(ner.south east)},
        xshift=0.5cm,
        name=pos,
        width=0.37\textwidth,
        height=0.3\textwidth,
xlabel=\small POS,
legend columns=4, 
        legend pos=north west,
        legend style={font=\tiny,at={(0.5,-0.5)}},
        tick label style={font=\small},
        xticklabels={10,50,100,500,1000,All},
        xtick={1,2,3,4,5,6},
        xlabel style={yshift=0.2cm},
]
        \addplot[black,mark=o] table[x=sentences,y=flair] {app_pos.dat};
        \addplot[black,mark=*] table[x=sentences,y=word+flair] {app_pos.dat};
        \addplot[red,mark=square*] table[x=sentences,y=mbert+word] {app_pos.dat};
        \addplot[blue,mark=square*] table[x=sentences,y=mbert+word+char] {app_pos.dat};
        \addplot[red,mark=triangle*] table[x=sentences,y=mbert+word+flair] {app_pos.dat};
        \addplot[blue,mark=triangle*] table[x=sentences,y=mbert+word+flair+char] {app_pos.dat};
    \end{axis}
    \begin{axis}[
at={(pos.south east)},
        xshift=0.5cm,
name=chunk,
        width=0.37\textwidth,
        height=0.3\textwidth,
xlabel=\small Chunking,
legend columns=4, 
        legend pos=north west,
        legend style={font=\small,at={(0.15,-0.5)}},
        xticklabels={10,50,100,500,1000,All},
        tick label style={font=\small},
        xtick={1,2,3,4,5,6},
        xlabel style={yshift=0.2cm},
]
        \addplot[black,mark=o] table[x=sentences,y=flair] {app_chunk.dat};
        \addplot[black,mark=*] table[x=sentences,y=word+flair] {app_chunk.dat};
        \addplot[red,mark=square*] table[x=sentences,y=mbert+word] {app_chunk.dat};
        \addplot[blue,mark=square*] table[x=sentences,y=mbert+word+char] {app_chunk.dat};
        \addplot[red,mark=triangle*] table[x=sentences,y=mbert+word+flair] {app_chunk.dat};
        \addplot[blue,mark=triangle*] table[x=sentences,y=mbert+word+flair+char] {app_chunk.dat};
    \end{axis}
\end{tikzpicture}
\caption{Relative score improvements against models with M-BERT embeddings for three tasks. Models are equipped with the CRF layer.}
\label{fig:crf_lowres}
\end{figure*}










\begin{filecontents}{zs_mixed.dat}
Task F F+W M+W+F All M+W+C M+W M
NER 24.2 22.2  45.0  44.7  48.2  47.5  56.5 
POS 21.1 23.6  31.5  30.7  41.1  40.9  69.4 
\end{filecontents}






\begin{table*}[ht]
\small
\centering
\begin{tabular}{l|cccc||cccc|c}
\hlineB{4}
& \multicolumn{4}{c||}{\bf \textsc{Embeddings}} & \multicolumn{5}{c}{\bf MaxEnt models on WikiAnn NER} \\ 
\hhline{~|----||-----}
 &  \textbf{M} & \textbf{F} & \textbf{W} & \textbf{C} & de & en & es & nl & avg \\
 \hline
1. & \xmark & \cmark & \xmark & \xmark & 41.0$\pm0.7$ & 46.9$\pm1.4$ & 47.4$\pm1.7$ & 49.8$\pm0.6$ & 46.3 \\
2. & \xmark & \cmark & \cmark & \xmark & 43.2$\pm0.6$ & 48.1$\pm1.0$ & \textbf{50.1}$\pm0.3$ & 52.9$\pm0.3$ & 48.6 \\
3. & \cmark & \xmark & \xmark & \xmark & 45.2$\pm0.9$ & 49.2$\pm0.7$ & 45.2$\pm2.8$ & 49.9$\pm0.7$ & 47.4 \\
4. & \cmark & \xmark & \cmark & \xmark & 43.5$\pm0.1$ & 50.9$\pm0.7$ & 46.9$\pm1.3$ & 52.5$\pm0.6$ & 48.4 \\
5. & \cmark & \xmark & \cmark & \cmark & 44.4$\pm1.2$ & 50.6$\pm0.6$ & 47.1$\pm0.1$ & 52.9$\pm0.6$ & 48.7 \\
6. & \cmark & \cmark & \cmark & \xmark & \textbf{46.7}$\pm0.4$ & 51.1$\pm0.7$ & 48.0$\pm2.1$ & 53.9$\pm0.9$ & 49.9 \\
7. & \cmark & \cmark & \cmark & \cmark & 46.6$\pm0.4$ & \textbf{52.3}$\pm0.5$ & 47.9$\pm0.3$ & \textbf{54.7}$\pm0.3$ & \textbf{50.4} \\
\hlineB{4}
\end{tabular}
\caption{Detailed results of cross-domain transfer from the Wikipedia domain to the news domain on the NER task. We use the ISO 639 language code to represent each language.}
\label{tab:app_cross_domain}
\end{table*}




\begin{table}[t!]
\setlength\tabcolsep{2.5pt}
\small
\centering
\begin{tabular}{l||cccc|c}
\hlineB{4}
 & M-BERT & Flair & Word & Char & \textbf{All} \\
\hline\hline
\multicolumn{6}{c}{WikiAnn NER}\\
\hline
ar & 53.0$\pm1.4$ & 43.7$\pm1.1$ & 44.9$\pm2.6$ & 0.0$\pm0.0$ & 82.6$\pm0.1$ \\
cs & 71.6$\pm0.4$ & 54.5$\pm2.4$ & 45.9$\pm2.1$ & 0.0$\pm0.0$ & 88.0$\pm0.1$ \\
de & 67.0$\pm0.8$ & 58.1$\pm2.5$ & 33.6$\pm1.9$ & 0.0$\pm0.0$ & 85.6$\pm0.2$ \\
en & 67.7$\pm0.9$ & 46.5$\pm0.9$ & 22.8$\pm0.5$ & 0.0$\pm0.0$ & 88.4$\pm0.7$ \\
es & 74.9$\pm1.0$ & 54.6$\pm2.5$ & 35.0$\pm2.0$ & 0.0$\pm0.0$ & 79.9$\pm0.6$ \\
fr & 75.9$\pm1.5$ & 48.0$\pm1.3$ & 35.2$\pm3.9$ & 0.0$\pm0.0$ & 84.2$\pm0.1$ \\
nl & 63.4$\pm3.7$ & 59.0$\pm1.6$ & 40.3$\pm0.9$ & 0.0$\pm0.0$ & 86.7$\pm0.4$ \\
ta & 41.3$\pm0.9$ & 51.5$\pm1.5$ & 43.5$\pm1.4$ & 0.0$\pm0.0$ & 83.4$\pm0.2$ \\
\hline
Avg. & 64.3 & 52.0 & 37.6 & 0.0 & 84.8 \\
\hline\hline
\multicolumn{6}{c}{UD POS tagging}\\
\hline
ar & 84.5$\pm1.7$ & 88.3$\pm0.7$ & 79.3$\pm0.6$ & 27.4$\pm0.9$ & 92.0$\pm0.6$ \\
cs & 84.0$\pm0.6$ & 90.8$\pm0.1$ & 68.5$\pm0.6$ & 25.6$\pm3.2$ & 96.5$\pm0.1$ \\
de & 78.3$\pm2.9$ & 88.3$\pm0.2$ & 71.3$\pm0.8$ & 26.3$\pm3.6$ & 98.8$\pm0.0$ \\
en & 85.1$\pm0.9$ & 85.8$\pm0.2$ & 65.3$\pm1.8$ & 32.5$\pm1.1$ & 97.2$\pm0.0$ \\
es & 83.2$\pm1.6$ & 92.4$\pm0.5$ & 80.9$\pm1.6$ & 20.4$\pm5.9$ & 96.6$\pm0.0$ \\
fr & 92.6$\pm0.7$ & 85.2$\pm0.6$ & 63.7$\pm0.2$ & 15.8$\pm3.0$ & 95.2$\pm0.0$ \\
nl & 80.9$\pm0.3$ & 89.9$\pm0.1$ & 70.9$\pm1.9$ & 18.9$\pm1.7$ & 98.7$\pm0.0$ \\
ta & 76.8$\pm1.8$ & 76.6$\pm0.3$ & 62.0$\pm0.0$ & 33.1$\pm4.8$ & 96.7$\pm0.0$ \\
\hline
Avg. & 83.2 & 87.2 & 70.2 & 25.0 & 96.5 \\
\hline\hline
\multicolumn{6}{c}{Chunking}\\
\hline
de & 66.3$\pm4.0$ & 90.2$\pm0.3$ & 52.9$\pm2.9$ & 28.0$\pm0.5$ & 93.5$\pm0.1$ \\
en & 46.6$\pm2.1$ & 73.0$\pm0.8$ & 70.7$\pm0.6$ & 19.8$\pm0.3$ & 90.8$\pm0.1$ \\
\hline
Avg. & 56.4 & 81.6 & 61.8 & 23.9 & 92.2 \\
\hlineB{4}
\end{tabular}
\caption{Detailed results on importance of embeddings for each language.}
\label{tab:importance}
\end{table}








\begin{table*}[t]
\setlength\tabcolsep{4pt}
\small
\centering
\begin{tabular}{l|cccc||cccccccc|c}
\hlineB{4}
& \multicolumn{4}{c||}{\bf \textsc{Embeddings}} & \multicolumn{9}{c}{\bf MaxEnt models on WikiAnn NER} \\  
\hhline{~|----||---------}
 &  \textbf{M} & \textbf{F} & \textbf{W} & \textbf{C}  & ar & cs & de & en & es & fr & nl & ta & Avg. \\
\hline\hline
\multicolumn{13}{c}{\bf \textsc{Low-Resource: 10 Sentences}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & 33.6$\pm1.3$ & 36.9$\pm1.5$ & 25.5$\pm0.8$ & \textbf{35.8}$\pm1.2$ & \textbf{34.4}$\pm2.0$ & 39.4$\pm2.0$ & 26.2$\pm1.7$ & 20.6$\pm0.7$ & 31.5 \\
2. & \xmark & \cmark & \cmark & \xmark & \textbf{34.4}$\pm1.4$ & \textbf{40.5}$\pm1.4$ & \textbf{26.0}$\pm0.4$ & 35.5$\pm1.4$ & 33.9$\pm3.1$ & \textbf{41.6}$\pm0.2$ & 28.6$\pm1.9$ & \textbf{20.8}$\pm1.1$ & \textbf{32.7} \\
3. & \cmark & \xmark & \xmark & \xmark & 29.6$\pm2.4$ & 31.0$\pm0.9$ & 13.8$\pm1.9$ & 23.2$\pm1.7$ & 23.2$\pm1.5$ & 24.6$\pm2.3$ & 16.2$\pm1.5$ & 17.5$\pm0.8$ & 22.4 \\
4. & \cmark & \xmark & \cmark & \xmark & 28.8$\pm1.9$ & 32.2$\pm0.9$ & 15.7$\pm1.6$ & 22.1$\pm0.6$ & 23.1$\pm1.0$ & 24.2$\pm2.5$ & 20.0$\pm2.9$ & 18.6$\pm0.7$ & 23.1 \\
5. & \cmark & \xmark & \cmark & \cmark & 27.8$\pm0.6$ & 32.1$\pm1.5$ & 15.8$\pm2.2$ & 23.1$\pm0.5$ & 23.9$\pm1.3$ & 26.3$\pm2.3$ & 19.2$\pm1.8$ & 18.6$\pm0.4$ & 23.4 \\
6. & \cmark & \cmark & \cmark & \xmark & 29.6$\pm1.9$ & 34.8$\pm0.5$ & 22.3$\pm0.9$ & 26.4$\pm0.9$ & 25.2$\pm1.0$ & 29.8$\pm1.5$ & 29.5$\pm2.1$ & 20.7$\pm0.6$ & 27.3 \\
7. & \cmark & \cmark & \cmark & \cmark & 30.3$\pm3.0$ & 34.5$\pm0.6$ & 23.0$\pm1.4$ & 25.4$\pm0.8$ & 25.3$\pm1.3$ & 30.8$\pm1.3$ & \textbf{30.6}$\pm3.2$ & 20.5$\pm0.5$ & 27.5 \\
\hline\hline
\multicolumn{13}{c}{\bf \textsc{Low-Resource: 50 Sentences}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & 47.6$\pm1.0$ & 54.6$\pm1.9$ & 52.7$\pm1.4$ & 47.3$\pm0.3$ & \textbf{54.8}$\pm0.8$ & \textbf{54.5}$\pm0.5$ & 49.2$\pm0.6$ & \textbf{45.3}$\pm0.6$ & 50.8 \\
2. & \xmark & \cmark & \cmark & \xmark & \textbf{49.8}$\pm0.8$ & \textbf{58.0}$\pm0.4$ & \textbf{55.7}$\pm1.0$ & \textbf{48.6}$\pm0.3$ & 54.2$\pm1.6$ & 54.0$\pm1.1$ & 49.7$\pm0.9$ & 44.5$\pm0.6$ & \textbf{51.8} \\
3. & \cmark & \xmark & \xmark & \xmark & 40.7$\pm1.3$ & 52.8$\pm2.1$ & 42.1$\pm1.6$ & 45.4$\pm0.6$ & 42.7$\pm2.6$ & 49.0$\pm1.3$ & 46.4$\pm2.0$ & 31.9$\pm1.4$ & 43.9 \\
4. & \cmark & \xmark & \cmark & \xmark & 44.3$\pm1.2$ & 54.6$\pm0.4$ & 46.6$\pm2.3$ & 46.9$\pm0.5$ & 45.8$\pm1.2$ & 49.2$\pm1.2$ & 46.7$\pm1.3$ & 33.6$\pm1.1$ & 46.0 \\
5. & \cmark & \xmark & \cmark & \cmark & 44.1$\pm0.6$ & 55.7$\pm0.9$ & 47.0$\pm4.1$ & 47.1$\pm0.6$ & 44.9$\pm2.0$ & 49.1$\pm1.2$ & 47.6$\pm1.5$ & 35.4$\pm1.1$ & 46.4 \\
6. & \cmark & \cmark & \cmark & \xmark & 49.4$\pm1.7$ & \textbf{58.0}$\pm1.3$ & 50.1$\pm1.1$ & 48.5$\pm0.1$ & 50.4$\pm1.0$ & 53.4$\pm1.3$ & \textbf{52.5}$\pm0.9$ & 42.5$\pm2.6$ & 50.6 \\
7. & \cmark & \cmark & \cmark & \cmark & 48.3$\pm1.4$ & \textbf{58.0}$\pm1.2$ & 49.6$\pm1.3$ & 48.5$\pm0.4$ & 51.0$\pm1.5$ & 52.4$\pm0.9$ & 51.9$\pm0.5$ & 44.3$\pm2.3$ & 50.5 \\
\hline\hline
\multicolumn{13}{c}{\bf \textsc{Low-Resource: 100 Sentences}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & 56.5$\pm0.7$ & 57.8$\pm0.8$ & 55.0$\pm1.9$ & 52.3$\pm0.3$ & 66.1$\pm0.9$ & 56.8$\pm2.8$ & 55.4$\pm1.0$ & 50.6$\pm0.8$ & 56.3 \\
2. & \xmark & \cmark & \cmark & \xmark & \textbf{58.7}$\pm1.2$ & 61.9$\pm1.0$ & \textbf{56.9}$\pm1.1$ & 54.8$\pm0.5$ & \textbf{67.0}$\pm1.0$ & 60.5$\pm0.7$ & 57.9$\pm1.3$ & \textbf{54.6}$\pm1.3$ & 59.0 \\
3. & \cmark & \xmark & \xmark & \xmark & 47.6$\pm3.0$ & 57.7$\pm3.8$ & 49.7$\pm3.7$ & 54.9$\pm0.5$ & 59.2$\pm1.2$ & 57.5$\pm0.9$ & 54.9$\pm1.0$ & 42.3$\pm3.9$ & 53.0 \\
4. & \cmark & \xmark & \cmark & \xmark & 51.0$\pm3.2$ & 59.8$\pm0.8$ & 52.3$\pm1.3$ & 56.0$\pm0.3$ & 60.0$\pm1.6$ & 58.4$\pm0.4$ & 59.0$\pm3.1$ & 48.8$\pm4.0$ & 55.7 \\
5. & \cmark & \xmark & \cmark & \cmark & 51.2$\pm2.9$ & 61.1$\pm0.9$ & 52.8$\pm1.7$ & 55.9$\pm0.7$ & 61.0$\pm0.9$ & 60.4$\pm2.2$ & 57.3$\pm2.2$ & 46.6$\pm1.6$ & 55.8 \\
6. & \cmark & \cmark & \cmark & \xmark & 57.4$\pm1.5$ & \textbf{64.3}$\pm1.7$ & 55.3$\pm1.0$ & \textbf{57.0}$\pm0.4$ & 65.7$\pm2.3$ & 62.2$\pm0.7$ & 61.3$\pm0.7$ & 53.2$\pm1.3$ & \textbf{59.6} \\
7. & \cmark & \cmark & \cmark & \cmark & 58.2$\pm1.1$ & 62.8$\pm1.0$ & 54.4$\pm1.5$ & 56.8$\pm0.1$ & 66.0$\pm0.3$ & \textbf{62.7}$\pm0.7$ & \textbf{61.8}$\pm1.3$ & 53.7$\pm0.6$ & 59.5 \\
\hline\hline
\multicolumn{13}{c}{\bf \textsc{Low-Resource: 500 Sentences}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & 69.2$\pm0.8$ & 77.2$\pm1.1$ & 72.1$\pm0.7$ & 65.6$\pm0.3$ & 77.6$\pm0.6$ & 73.6$\pm0.4$ & 74.6$\pm1.5$ & 61.9$\pm1.1$ & 71.5 \\
2. & \xmark & \cmark & \cmark & \xmark & \textbf{73.2}$\pm0.7$ & 78.6$\pm1.3$ & 72.9$\pm1.0$ & 68.3$\pm0.2$ & 78.2$\pm1.3$ & 75.1$\pm1.1$ & 75.0$\pm1.3$ & 66.1$\pm1.2$ & 73.4 \\
3. & \cmark & \xmark & \xmark & \xmark & 67.3$\pm1.0$ & 77.0$\pm0.5$ & 71.8$\pm1.0$ & 67.7$\pm0.2$ & 77.6$\pm0.7$ & 76.7$\pm0.9$ & 75.5$\pm1.7$ & 64.9$\pm1.3$ & 72.3 \\
4. & \cmark & \xmark & \cmark & \xmark & 72.0$\pm0.3$ & 78.6$\pm0.8$ & 75.2$\pm1.7$ & 68.3$\pm0.0$ & 78.6$\pm1.5$ & 77.7$\pm1.1$ & 78.0$\pm0.9$ & 70.8$\pm1.0$ & 74.9 \\
5. & \cmark & \xmark & \cmark & \cmark & 72.1$\pm0.9$ & 78.5$\pm0.8$ & 74.7$\pm0.6$ & 68.1$\pm0.2$ & 78.5$\pm1.3$ & 79.4$\pm0.1$ & \textbf{78.2}$\pm1.4$ & 70.4$\pm1.3$ & 75.0 \\
6. & \cmark & \cmark & \cmark & \xmark & 72.9$\pm0.7$ & 79.0$\pm0.4$ & 76.2$\pm0.4$ & \textbf{69.7}$\pm0.2$ & \textbf{82.0}$\pm0.4$ & \textbf{80.5}$\pm0.4$ & 78.0$\pm0.6$ & \textbf{71.0}$\pm0.8$ & \textbf{76.2} \\
7. & \cmark & \cmark & \cmark & \cmark & 72.9$\pm0.7$ & \textbf{79.9}$\pm1.0$ & \textbf{76.3}$\pm0.5$ & 69.1$\pm0.2$ & \textbf{82.0}$\pm0.5$ & \textbf{80.5}$\pm0.6$ & 78.0$\pm1.8$ & 70.8$\pm0.5$ & \textbf{76.2} \\
\hline\hline
\multicolumn{13}{c}{\bf \textsc{Low-Resource: 1000 Sentences}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & 74.7$\pm0.6$ & 80.2$\pm0.4$ & 75.6$\pm0.6$ & 68.9$\pm0.0$ & 82.9$\pm0.4$ & 75.9$\pm0.8$ & 80.1$\pm0.7$ & 73.8$\pm0.4$ & 76.5 \\
2. & \xmark & \cmark & \cmark & \xmark & 77.8$\pm0.2$ & 82.6$\pm0.5$ & 77.2$\pm0.4$ & 72.0$\pm0.1$ & 84.0$\pm0.3$ & 78.4$\pm1.0$ & 83.3$\pm0.5$ & 75.7$\pm0.7$ & 78.9 \\
3. & \cmark & \xmark & \xmark & \xmark & 73.6$\pm0.4$ & 81.7$\pm0.4$ & 77.3$\pm0.2$ & 72.2$\pm0.1$ & 84.8$\pm0.6$ & 82.2$\pm0.9$ & 81.4$\pm0.4$ & 71.2$\pm0.8$ & 78.0 \\
4. & \cmark & \xmark & \cmark & \xmark & 77.0$\pm0.2$ & 83.3$\pm0.4$ & 78.7$\pm0.6$ & 73.4$\pm0.5$ & 85.2$\pm0.8$ & 83.1$\pm0.4$ & 84.2$\pm0.6$ & 76.3$\pm0.7$ & 80.2 \\
5. & \cmark & \xmark & \cmark & \cmark & 77.9$\pm0.6$ & 83.3$\pm0.7$ & 78.9$\pm0.3$ & 73.6$\pm0.0$ & 84.6$\pm0.8$ & 82.3$\pm0.9$ & 84.2$\pm0.7$ & 76.0$\pm0.1$ & 80.1 \\
6. & \cmark & \cmark & \cmark & \xmark & 78.3$\pm0.3$ & \textbf{84.0}$\pm0.4$ & \textbf{79.8}$\pm0.5$ & \textbf{75.3}$\pm0.3$ & 86.3$\pm0.4$ & 83.3$\pm0.6$ & \textbf{85.0}$\pm1.0$ & \textbf{78.2}$\pm0.5$ & \textbf{81.3} \\
7. & \cmark & \cmark & \cmark & \cmark & \textbf{78.4}$\pm0.6$ & 83.8$\pm0.3$ & \textbf{79.8}$\pm0.3$ & 75.2$\pm0.3$ & \textbf{86.4}$\pm0.2$ & \textbf{83.6}$\pm0.3$ & 84.5$\pm0.6$ & 77.9$\pm0.5$ & 81.2\\
\hline\hline
\multicolumn{13}{c}{\bf \textsc{Rich-Resource}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & 81.0$\pm0.5$ & 83.0$\pm2.5$ & 80.4$\pm0.5$ & 76.9$\pm0.1$ & 86.2$\pm0.4$ & 81.8$\pm0.6$ & 85.7$\pm0.5$ & 82.3$\pm0.5$ & 82.2 \\
2. & \xmark & \cmark & \cmark & \xmark & 84.5$\pm0.3$ & 86.9$\pm0.6$ & 81.8$\pm0.4$ & 79.9$\pm0.3$ & 88.4$\pm0.5$ & 83.8$\pm0.3$ & 88.2$\pm0.5$ & 83.5$\pm0.6$ & 84.6 \\
3. & \cmark & \xmark & \xmark & \xmark & 79.4$\pm0.4$ & 86.2$\pm0.4$ & 83.1$\pm0.3$ & 79.6$\pm0.4$ & 87.5$\pm0.8$ & 86.9$\pm0.4$ & 87.8$\pm0.6$ & 79.5$\pm0.4$ & 83.7 \\
4. & \cmark & \xmark & \cmark & \xmark & 83.2$\pm0.4$ & 87.1$\pm0.6$ & 84.2$\pm0.3$ & 80.3$\pm0.6$ & 88.4$\pm0.5$ & 87.4$\pm0.3$ & 89.3$\pm0.4$ & 83.8$\pm0.4$ & 85.5 \\
5. & \cmark & \xmark & \cmark & \cmark & 83.2$\pm0.8$ & 87.4$\pm0.3$ & 83.9$\pm0.2$ & 81.0$\pm0.1$ & 88.2$\pm0.4$ & 87.5$\pm0.3$ & 89.2$\pm0.4$ & 83.7$\pm0.6$ & 85.5 \\
6. & \cmark & \cmark & \cmark & \xmark & 84.2$\pm0.5$ & \textbf{88.8}$\pm0.5$ & 85.4$\pm0.5$ & \textbf{81.8}$\pm0.1$ & 90.4$\pm0.4$ & 88.1$\pm0.5$ & \textbf{90.4}$\pm0.3$ & 85.4$\pm0.3$ & \textbf{86.8} \\
7. & \cmark & \cmark & \cmark & \cmark & \textbf{84.6}$\pm0.2$ & 88.4$\pm0.2$ & \textbf{85.5}$\pm0.3$ & 81.8$\pm0.2$ & \textbf{90.6}$\pm0.2$ & \textbf{88.4}$\pm0.5$ & 90.1$\pm0.3$ & \textbf{85.5}$\pm0.3$ & \textbf{86.8} \\
\hlineB{4}
\end{tabular}
\caption{Averaged F1 scores over 8 languages for WikiAnn NER.}
\label{tab:ner}
\end{table*}




\begin{table*}[t]
\setlength\tabcolsep{4pt}
\small
\centering
\begin{tabular}{l|cccc||cccccccc|c}
\hlineB{4}
& \multicolumn{4}{c||}{\bf \textsc{Embeddings}} & \multicolumn{9}{c}{\bf CRF models on WikiAnn NER} \\  
\hhline{~|----||---------}
 &  \textbf{M} & \textbf{F} & \textbf{W} & \textbf{C}  & ar & cs & de & en & es & fr & nl & ta & Avg. \\
\hline\hline
\multicolumn{13}{c}{\bf \textsc{Low-Resource: 10 Sentences}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & 31.0$\pm5.2$ & 38.6$\pm1.1$ & 23.5$\pm0.7$ & \textbf{35.1}$\pm2.2$ & \textbf{37.6}$\pm3.6$ & 41.2$\pm2.2$ & 27.4$\pm3.4$ & 19.6$\pm1.1$ & \textbf{31.8} \\
2. & \xmark & \cmark & \cmark & \xmark & \textbf{32.0}$\pm3.3$ & \textbf{38.9}$\pm4.6$ & \textbf{24.1}$\pm1.3$ & 33.3$\pm0.5$ & 31.7$\pm2.0$ & \textbf{43.1}$\pm4.5$ & \textbf{30.4}$\pm3.9$ & 19.6$\pm3.7$ & 31.6 \\
3. & \cmark & \xmark & \xmark & \xmark & 31.5$\pm4.7$ & 30.0$\pm2.8$ & 12.5$\pm4.9$ & 23.1$\pm1.0$ & 23.5$\pm5.2$ & 24.7$\pm3.2$ & 18.3$\pm2.7$ & 17.8$\pm1.0$ & 22.7 \\
4. & \cmark & \xmark & \cmark & \xmark & 28.4$\pm3.2$ & 30.9$\pm1.9$ & 14.2$\pm1.3$ & 21.3$\pm2.9$ & 21.5$\pm1.9$ & 25.4$\pm3.4$ & 12.5$\pm8.0$ & 18.5$\pm1.0$ & 21.6 \\
5. & \cmark & \xmark & \cmark & \cmark & 30.9$\pm1.8$ & 32.8$\pm2.2$ & 11.0$\pm5.7$ & 19.7$\pm1.4$ & 24.7$\pm1.4$ & 28.1$\pm0.3$ & 19.3$\pm3.8$ & 18.8$\pm1.3$ & 23.2 \\
6. & \cmark & \cmark & \cmark & \xmark & 29.3$\pm1.4$ & 33.0$\pm2.5$ & 20.9$\pm1.5$ & 22.4$\pm0.8$ & 23.8$\pm2.0$ & 28.5$\pm1.9$ & 26.5$\pm4.6$ & \textbf{21.7}$\pm1.7$ & 25.7 \\
7. & \cmark & \cmark & \cmark & \cmark & 30.0$\pm2.4$ & 32.2$\pm1.6$ & 22.2$\pm2.2$ & 22.1$\pm0.0$ & 23.1$\pm1.9$ & 30.0$\pm3.6$ & 28.5$\pm2.1$ & 19.2$\pm2.0$ & 25.9 \\
\hline\hline
\multicolumn{13}{c}{\bf \textsc{Low-Resource: 50 Sentences}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & 49.6$\pm2.0$ & 55.0$\pm1.3$ & \textbf{54.3}$\pm0.6$ & 43.8$\pm1.7$ & 55.3$\pm2.7$ & \textbf{57.6}$\pm1.0$ & 53.9$\pm1.3$ & 45.4$\pm2.7$ & 51.9 \\
2. & \xmark & \cmark & \cmark & \xmark & \textbf{51.3}$\pm4.6$ & \textbf{58.6}$\pm1.8$ & 52.8$\pm1.0$ & 45.1$\pm0.3$ & \textbf{55.9}$\pm1.7$ & 55.6$\pm2.1$ & \textbf{55.2}$\pm1.3$ & \textbf{46.1}$\pm2.2$ & \textbf{52.6} \\
3. & \cmark & \xmark & \xmark & \xmark & 45.5$\pm2.7$ & 55.3$\pm3.3$ & 42.2$\pm3.0$ & 44.4$\pm0.4$ & 44.7$\pm2.7$ & 52.3$\pm1.7$ & 47.0$\pm3.7$ & 32.8$\pm5.2$ & 45.5 \\
4. & \cmark & \xmark & \cmark & \xmark & 45.9$\pm1.2$ & 54.6$\pm1.3$ & 45.3$\pm2.6$ & 40.6$\pm2.6$ & 45.5$\pm2.1$ & 51.4$\pm2.2$ & 47.7$\pm1.2$ & 37.5$\pm3.4$ & 46.1 \\
5. & \cmark & \xmark & \cmark & \cmark & 44.1$\pm4.4$ & 54.8$\pm1.7$ & 47.0$\pm3.5$ & 40.9$\pm1.3$ & 47.1$\pm2.4$ & 50.8$\pm2.7$ & 46.8$\pm3.0$ & 34.9$\pm7.3$ & 45.8 \\
6. & \cmark & \cmark & \cmark & \xmark & 48.2$\pm4.7$ & 57.9$\pm2.3$ & 51.6$\pm1.8$ & 41.7$\pm0.5$ & 51.0$\pm2.7$ & 53.7$\pm3.1$ & 51.3$\pm2.0$ & 42.6$\pm2.8$ & 49.8 \\
7. & \cmark & \cmark & \cmark & \cmark & 45.0$\pm6.2$ & 57.1$\pm1.2$ & 50.0$\pm3.3$ & \textbf{45.8}$\pm0.6$ & 51.5$\pm1.4$ & 53.0$\pm2.6$ & 51.1$\pm4.2$ & 41.3$\pm3.2$ & 49.4 \\
\hline\hline
\multicolumn{13}{c}{\bf \textsc{Low-Resource: 100 Sentences}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & 55.6$\pm3.0$ & 62.8$\pm0.9$ & \textbf{59.3}$\pm1.3$ & 52.5$\pm1.7$ & 66.2$\pm1.6$ & 62.7$\pm2.1$ & 58.9$\pm2.5$ & 52.4$\pm1.2$ & 58.8 \\
2. & \xmark & \cmark & \cmark & \xmark & \textbf{60.7}$\pm1.9$ & 63.1$\pm0.6$ & 58.2$\pm1.9$ & 50.5$\pm1.1$ & \textbf{66.7}$\pm1.7$ & \textbf{66.1}$\pm0.8$ & 61.6$\pm0.8$ & \textbf{56.6}$\pm1.3$ & \textbf{60.4} \\
3. & \cmark & \xmark & \xmark & \xmark & 55.1$\pm3.1$ & 59.5$\pm2.1$ & 47.9$\pm3.3$ & 52.6$\pm3.4$ & 61.2$\pm1.4$ & 59.9$\pm3.0$ & 56.4$\pm4.4$ & 45.9$\pm0.9$ & 54.8 \\
4. & \cmark & \xmark & \cmark & \xmark & 53.9$\pm1.5$ & 61.7$\pm2.4$ & 51.1$\pm2.5$ & 51.4$\pm0.9$ & 62.9$\pm1.8$ & 62.5$\pm0.5$ & 60.9$\pm1.2$ & 49.8$\pm2.3$ & 56.8 \\
5. & \cmark & \xmark & \cmark & \cmark & 53.5$\pm2.8$ & 62.8$\pm2.3$ & 52.2$\pm2.1$ & \textbf{52.8}$\pm1.3$ & 61.9$\pm2.6$ & 61.6$\pm1.8$ & 57.9$\pm4.2$ & 48.7$\pm4.0$ & 56.4 \\
6. & \cmark & \cmark & \cmark & \xmark & 58.4$\pm2.2$ & \textbf{65.0}$\pm1.0$ & 55.5$\pm3.0$ & 52.6$\pm2.5$ & 66.3$\pm1.6$ & 62.6$\pm1.4$ & \textbf{64.7}$\pm1.2$ & 53.4$\pm1.3$ & 59.8 \\
7. & \cmark & \cmark & \cmark & \cmark & 59.1$\pm2.1$ & 63.0$\pm2.4$ & 54.5$\pm2.4$ & 52.0$\pm2.9$ & 63.9$\pm1.5$ & 64.1$\pm1.7$ & 61.2$\pm1.3$ & 51.0$\pm2.1$ & 58.6 \\
\hline\hline
\multicolumn{13}{c}{\bf \textsc{Low-Resource: 500 Sentences}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & 69.3$\pm0.7$ & 78.0$\pm0.8$ & 73.0$\pm1.9$ & 65.3$\pm0.7$ & 80.4$\pm0.7$ & 76.0$\pm0.5$ & 76.2$\pm0.8$ & 66.8$\pm0.8$ & 73.1 \\
2. & \xmark & \cmark & \cmark & \xmark & 73.0$\pm2.0$ & 79.5$\pm0.8$ & 74.2$\pm0.6$ & 67.4$\pm0.6$ & 81.1$\pm0.9$ & 77.8$\pm0.8$ & 77.4$\pm0.4$ & 69.2$\pm1.1$ & 75.0 \\
3. & \cmark & \xmark & \xmark & \xmark & 70.2$\pm1.0$ & 77.7$\pm0.9$ & 73.1$\pm1.2$ & 67.5$\pm0.9$ & 80.8$\pm1.1$ & 79.0$\pm1.2$ & 76.1$\pm0.7$ & 67.3$\pm0.7$ & 74.0 \\
4. & \cmark & \xmark & \cmark & \xmark & 74.1$\pm1.0$ & 79.1$\pm0.3$ & 76.3$\pm0.6$ & 67.5$\pm0.3$ & 80.7$\pm0.6$ & 80.0$\pm0.3$ & 79.3$\pm1.5$ & 71.6$\pm0.6$ & 76.1 \\
5. & \cmark & \xmark & \cmark & \cmark & 73.6$\pm0.5$ & 78.5$\pm1.2$ & 75.5$\pm1.0$ & 68.1$\pm0.5$ & 81.5$\pm1.2$ & 80.3$\pm0.8$ & 78.9$\pm1.5$ & 72.1$\pm0.4$ & 76.1 \\
6. & \cmark & \cmark & \cmark & \xmark & \textbf{74.5}$\pm0.4$ & \textbf{80.3}$\pm0.6$ & \textbf{76.7}$\pm0.6$ & \textbf{70.3}$\pm1.2$ & 82.6$\pm0.4$ & 81.5$\pm0.9$ & \textbf{80.5}$\pm1.0$ & \textbf{72.9}$\pm0.9$ & \textbf{77.4} \\
7. & \cmark & \cmark & \cmark & \cmark & 74.2$\pm0.7$ & 80.2$\pm1.0$ & 75.8$\pm1.0$ & 69.4$\pm0.1$ & \textbf{83.9}$\pm1.0$ & \textbf{81.7}$\pm0.8$ & 79.4$\pm0.7$ & 72.4$\pm1.3$ & 77.1 \\
\hline\hline
\multicolumn{13}{c}{\bf \textsc{Low-Resource: 1000 Sentences}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & 76.9$\pm0.6$ & 80.7$\pm0.7$ & 77.0$\pm0.5$ & 69.4$\pm0.1$ & 84.2$\pm0.7$ & 78.0$\pm0.5$ & 81.4$\pm0.4$ & 75.5$\pm0.6$ & 77.9 \\
2. & \xmark & \cmark & \cmark & \xmark & 79.4$\pm0.3$ & 83.6$\pm0.7$ & 77.8$\pm0.5$ & 70.9$\pm0.4$ & 85.7$\pm0.4$ & 81.0$\pm0.4$ & 83.4$\pm0.3$ & 77.3$\pm0.8$ & 79.9 \\
3. & \cmark & \xmark & \xmark & \xmark & 75.3$\pm0.7$ & 82.7$\pm0.5$ & 76.9$\pm0.5$ & 72.5$\pm1.4$ & 85.9$\pm0.4$ & 82.4$\pm0.6$ & 82.0$\pm0.4$ & 73.4$\pm0.4$ & 78.9 \\
4. & \cmark & \xmark & \cmark & \xmark & 79.2$\pm0.5$ & 84.4$\pm0.8$ & 79.7$\pm0.8$ & 73.3$\pm0.5$ & 86.7$\pm0.4$ & 83.8$\pm0.5$ & 84.0$\pm0.6$ & 78.5$\pm0.6$ & 81.2 \\
5. & \cmark & \xmark & \cmark & \cmark & 78.7$\pm0.8$ & 84.7$\pm1.1$ & 79.4$\pm0.4$ & 73.7$\pm0.1$ & 86.8$\pm0.2$ & 84.1$\pm0.1$ & 84.8$\pm0.4$ & 78.3$\pm0.4$ & 81.3 \\
6. & \cmark & \cmark & \cmark & \xmark & \textbf{79.6}$\pm0.7$ & \textbf{85.0}$\pm0.7$ & \textbf{80.8}$\pm0.4$ & \textbf{74.4}$\pm0.7$ & \textbf{87.4}$\pm0.7$ & \textbf{84.5}$\pm0.6$ & \textbf{85.6}$\pm0.8$ & \textbf{79.3}$\pm0.7$ & \textbf{82.1} \\
7. & \cmark & \cmark & \cmark & \cmark & 79.4$\pm0.6$ & 84.8$\pm0.4$ & 80.2$\pm0.6$ & 74.3$\pm0.2$ & 87.1$\pm0.8$ & 84.3$\pm0.5$ & 84.6$\pm0.5$ & 79.2$\pm0.5$ & 81.7 \\
\hline\hline
\multicolumn{13}{c}{\bf \textsc{Rich-Resource}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & 82.8$\pm0.3$ & 85.8$\pm0.5$ & 81.0$\pm0.6$ & 78.4$\pm0.1$ & 87.1$\pm0.5$ & 82.9$\pm0.7$ & 86.2$\pm0.4$ & 82.8$\pm0.5$ & 83.4 \\
2. & \xmark & \cmark & \cmark & \xmark & \textbf{85.2}$\pm0.5$ & 87.9$\pm0.2$ & 83.0$\pm0.1$ & 81.1$\pm0.2$ & 89.0$\pm0.4$ & 85.8$\pm0.4$ & 88.8$\pm0.4$ & 84.6$\pm0.3$ & 85.7 \\
3. & \cmark & \xmark & \xmark & \xmark & 80.3$\pm0.4$ & 87.1$\pm0.4$ & 84.2$\pm0.4$ & 81.2$\pm0.1$ & 88.8$\pm0.2$ & 87.8$\pm0.4$ & 87.6$\pm0.6$ & 81.3$\pm0.5$ & 84.8 \\
4. & \cmark & \xmark & \cmark & \xmark & 84.2$\pm0.3$ & 88.0$\pm0.3$ & 84.7$\pm0.3$ & 82.3$\pm0.3$ & 89.1$\pm0.4$ & 87.9$\pm0.4$ & 89.7$\pm0.6$ & 84.9$\pm0.2$ & 86.3 \\
5. & \cmark & \xmark & \cmark & \cmark & 84.0$\pm0.4$ & 87.9$\pm0.4$ & 85.0$\pm0.4$ & 82.2$\pm0.2$ & 89.3$\pm0.5$ & 87.6$\pm0.4$ & 89.6$\pm0.5$ & 85.0$\pm0.3$ & 86.3 \\
6. & \cmark & \cmark & \cmark & \xmark & 85.1$\pm0.4$ & \textbf{89.6}$\pm0.0$ & 85.5$\pm0.6$ & \textbf{82.9}$\pm0.1$ & 90.6$\pm0.3$ & 88.6$\pm0.4$ & \textbf{90.8}$\pm0.1$ & \textbf{86.1}$\pm0.4$ & \textbf{87.4} \\
7. & \cmark & \cmark & \cmark & \cmark & 85.0$\pm0.4$ & 89.3$\pm0.2$ & \textbf{85.8}$\pm0.1$ & 82.8$\pm0.2$ & \textbf{91.0}$\pm0.3$ & \textbf{88.7}$\pm0.3$ & 90.4$\pm0.3$ & 86.0$\pm0.3$ & \textbf{87.4} \\
\hlineB{4}
\end{tabular}
\caption{Averaged F1 scores over 8 languages for WikiAnn NER with the CRF layer.}
\label{tab:crf_ner}
\end{table*}




\begin{table*}[ht]
\setlength\tabcolsep{4pt}
\small
\centering
\begin{tabular}{l|cccc||cccccccc|c}
\hlineB{4}
& \multicolumn{4}{c||}{\bf \textsc{Embeddings}} & \multicolumn{9}{c}{\bf MaxEnt models on UD POS tagging} \\  
\hhline{~|----||---------}
 &  \textbf{M} & \textbf{F} & \textbf{W} & \textbf{C}  & ar & cs & de & en & es & fr & nl & ta & Avg. \\
\hline\hline
\multicolumn{13}{c}{\bf \textsc{Low-Resource: 10 Sentences}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & 86.4$\pm0.3$ & \textbf{83.0}$\pm0.4$ & \textbf{83.4}$\pm0.6$ & 79.5$\pm0.1$ & 88.7$\pm0.1$ & 85.5$\pm0.2$ & \textbf{72.1}$\pm0.5$ & 72.5$\pm0.4$ & 81.4 \\
2. & \xmark & \cmark & \cmark & \xmark & \textbf{87.1}$\pm0.1$ & 82.5$\pm0.2$ & 83.1$\pm0.3$ & \textbf{80.2}$\pm0.1$ & \textbf{88.8}$\pm0.1$ & \textbf{86.2}$\pm0.2$ & \textbf{72.1}$\pm0.6$ & 73.2$\pm0.2$ & \textbf{81.7} \\
3. & \cmark & \xmark & \xmark & \xmark & 80.7$\pm0.7$ & 71.0$\pm1.3$ & 73.4$\pm1.1$ & 72.5$\pm0.1$ & 78.5$\pm1.2$ & 76.4$\pm0.7$ & 62.8$\pm1.3$ & 61.7$\pm1.6$ & 72.1 \\
4. & \cmark & \xmark & \cmark & \xmark & 82.4$\pm0.9$ & 74.4$\pm0.8$ & 75.1$\pm0.9$ & 73.7$\pm0.1$ & 80.2$\pm0.9$ & 78.5$\pm0.5$ & 65.0$\pm0.9$ & 65.0$\pm1.4$ & 74.3 \\
5. & \cmark & \xmark & \cmark & \cmark & 82.6$\pm0.6$ & 74.7$\pm0.3$ & 75.9$\pm0.4$ & 73.9$\pm0.4$ & 81.4$\pm0.8$ & 78.3$\pm0.9$ & 64.5$\pm0.9$ & 66.4$\pm1.4$ & 74.7 \\
6. & \cmark & \cmark & \cmark & \xmark & 86.6$\pm0.2$ & 80.8$\pm0.2$ & 81.8$\pm0.2$ & 77.9$\pm0.0$ & 86.9$\pm0.4$ & 82.6$\pm0.7$ & \textbf{72.1}$\pm0.8$ & \textbf{73.5}$\pm0.7$ & 80.3 \\
7. & \cmark & \cmark & \cmark & \cmark & 86.8$\pm0.2$ & 81.1$\pm0.2$ & 81.9$\pm0.2$ & 77.9$\pm0.2$ & 86.9$\pm0.3$ & 82.6$\pm0.5$ & \textbf{72.1}$\pm0.9$ & \textbf{73.5}$\pm0.4$ & 80.4 \\
\hline\hline
\multicolumn{13}{c}{\bf \textsc{Low-Resource: 50 Sentences}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & 91.9$\pm0.1$ & 91.1$\pm0.2$ & \textbf{91.4}$\pm0.1$ & 88.6$\pm0.1$ & 93.3$\pm0.0$ & 92.2$\pm0.1$ & 83.3$\pm0.1$ & 85.9$\pm0.3$ & 89.7 \\
2. & \xmark & \cmark & \cmark & \xmark & \textbf{92.3}$\pm0.1$ & \textbf{91.6}$\pm0.1$ & 91.3$\pm0.2$ & \textbf{88.8}$\pm0.0$ & \textbf{93.6}$\pm0.1$ & \textbf{92.3}$\pm0.1$ & \textbf{84.0}$\pm0.2$ & 86.7$\pm0.3$ & \textbf{90.1} \\
3. & \cmark & \xmark & \xmark & \xmark & 87.7$\pm0.3$ & 83.9$\pm1.3$ & 83.5$\pm0.5$ & 82.2$\pm0.2$ & 87.9$\pm0.3$ & 86.0$\pm0.5$ & 71.9$\pm0.8$ & 76.9$\pm0.4$ & 82.5 \\
4. & \cmark & \xmark & \cmark & \xmark & 89.3$\pm0.3$ & 86.3$\pm0.4$ & 85.5$\pm0.8$ & 83.9$\pm0.1$ & 89.5$\pm0.7$ & 88.1$\pm0.5$ & 75.1$\pm1.0$ & 81.0$\pm0.7$ & 84.8 \\
5. & \cmark & \xmark & \cmark & \cmark & 89.6$\pm0.1$ & 86.5$\pm1.1$ & 86.0$\pm0.7$ & 84.1$\pm0.2$ & 90.0$\pm0.5$ & 88.4$\pm0.3$ & 75.1$\pm0.7$ & 81.4$\pm0.5$ & 85.1 \\
6. & \cmark & \cmark & \cmark & \xmark & 91.6$\pm0.1$ & 91.1$\pm0.3$ & 90.8$\pm0.2$ & 87.5$\pm0.2$ & 92.5$\pm0.3$ & 91.7$\pm0.1$ & 82.7$\pm0.1$ & \textbf{87.0}$\pm0.2$ & 89.3 \\
7. & \cmark & \cmark & \cmark & \cmark & 91.6$\pm0.1$ & 91.1$\pm0.3$ & 91.0$\pm0.2$ & 87.5$\pm0.1$ & 92.5$\pm0.2$ & 91.8$\pm0.2$ & 82.8$\pm0.5$ & 86.9$\pm0.2$ & 89.4 \\
\hline\hline
\multicolumn{13}{c}{\bf \textsc{Low-Resource: 100 Sentences}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & 93.5$\pm0.1$ & 93.6$\pm0.1$ & \textbf{92.2}$\pm0.1$ & 90.2$\pm0.1$ & 94.2$\pm0.0$ & 94.4$\pm0.1$ & 88.2$\pm0.2$ & 88.5$\pm0.7$ & 91.8 \\
2. & \xmark & \cmark & \cmark & \xmark & \textbf{93.7}$\pm0.1$ & \textbf{93.9}$\pm0.1$ & \textbf{92.2}$\pm0.0$ & \textbf{90.6}$\pm0.1$ & \textbf{94.6}$\pm0.1$ & \textbf{94.5}$\pm0.2$ & \textbf{89.1}$\pm0.1$ & 89.3$\pm0.1$ & \textbf{92.2} \\
3. & \cmark & \xmark & \xmark & \xmark & 90.4$\pm0.0$ & 88.8$\pm0.2$ & 85.9$\pm0.5$ & 85.7$\pm0.1$ & 90.4$\pm0.2$ & 90.2$\pm0.4$ & 77.9$\pm0.8$ & 81.9$\pm0.4$ & 86.4 \\
4. & \cmark & \xmark & \cmark & \xmark & 91.8$\pm0.1$ & 90.6$\pm0.2$ & 87.5$\pm0.3$ & 87.4$\pm0.1$ & 92.1$\pm0.3$ & 91.7$\pm0.3$ & 81.0$\pm1.7$ & 85.7$\pm0.4$ & 88.5 \\
5. & \cmark & \xmark & \cmark & \cmark & 91.9$\pm0.1$ & 90.9$\pm0.1$ & 87.8$\pm0.2$ & 87.7$\pm0.1$ & 92.1$\pm0.3$ & 91.8$\pm0.2$ & 81.9$\pm1.7$ & 85.9$\pm0.5$ & 88.8 \\
6. & \cmark & \cmark & \cmark & \xmark & 93.6$\pm0.1$ & 93.6$\pm0.1$ & 92.0$\pm0.3$ & 90.4$\pm0.1$ & 94.3$\pm0.2$ & 94.3$\pm0.3$ & 87.8$\pm0.2$ & 89.7$\pm0.2$ & 92.0 \\
7. & \cmark & \cmark & \cmark & \cmark & 93.6$\pm0.1$ & 93.6$\pm0.1$ & 91.9$\pm0.1$ & 90.3$\pm0.0$ & 94.4$\pm0.1$ & 94.3$\pm0.1$ & 87.8$\pm0.5$ & \textbf{89.8}$\pm0.3$ & 91.9 \\
\hline\hline
\multicolumn{13}{c}{\bf \textsc{Low-Resource: 500 Sentences}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & 95.2$\pm0.0$ & 96.0$\pm0.1$ & 94.3$\pm0.1$ & 92.7$\pm0.1$ & 95.9$\pm0.1$ & 97.0$\pm0.1$ & 93.0$\pm0.2$ & 92.2$\pm0.3$ & 94.5 \\
2. & \xmark & \cmark & \cmark & \xmark & 95.3$\pm0.0$ & \textbf{96.3}$\pm0.1$ & \textbf{94.4}$\pm0.0$ & 92.8$\pm0.0$ & 96.0$\pm0.0$ & 97.4$\pm0.1$ & 93.2$\pm0.2$ & 92.3$\pm0.6$ & 94.7 \\
3. & \cmark & \xmark & \xmark & \xmark & 93.5$\pm0.1$ & 92.7$\pm0.1$ & 90.2$\pm0.1$ & 90.0$\pm0.1$ & 93.9$\pm0.1$ & 95.6$\pm0.1$ & 90.3$\pm0.2$ & 86.9$\pm0.1$ & 91.6 \\
4. & \cmark & \xmark & \cmark & \xmark & 94.5$\pm0.1$ & 94.7$\pm0.1$ & 91.5$\pm0.1$ & 91.8$\pm0.1$ & 95.1$\pm0.1$ & 96.7$\pm0.1$ & 91.6$\pm0.1$ & 89.9$\pm0.2$ & 93.2 \\
5. & \cmark & \xmark & \cmark & \cmark & 94.7$\pm0.1$ & 94.9$\pm0.1$ & 91.6$\pm0.1$ & 92.0$\pm0.0$ & 95.2$\pm0.1$ & 96.9$\pm0.1$ & 91.8$\pm0.1$ & 89.7$\pm0.6$ & 93.4 \\
6. & \cmark & \cmark & \cmark & \xmark & \textbf{95.5}$\pm0.1$ & 96.1$\pm0.1$ & 94.0$\pm0.1$ & \textbf{93.0}$\pm0.0$ & \textbf{96.1}$\pm0.1$ & \textbf{97.5}$\pm0.0$ & \textbf{93.4}$\pm0.1$ & \textbf{92.6}$\pm0.4$ & \textbf{94.8} \\
7. & \cmark & \cmark & \cmark & \cmark & \textbf{95.5}$\pm0.0$ & 96.2$\pm0.1$ & 94.0$\pm0.1$ & \textbf{93.0}$\pm0.1$ & 96.0$\pm0.2$ & \textbf{97.5}$\pm0.0$ & \textbf{93.4}$\pm0.1$ & \textbf{92.6}$\pm0.3$ & \textbf{94.8} \\
\hline\hline
\multicolumn{13}{c}{\bf \textsc{Low-Resource: 1000 Sentences}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & 95.7$\pm0.0$ & 96.8$\pm0.1$ & \textbf{94.7}$\pm0.1$ & 93.6$\pm0.1$ & 96.2$\pm0.0$ & 97.5$\pm0.1$ & 94.5$\pm0.2$ & 92.3$\pm0.2$ & 95.2 \\
2. & \xmark & \cmark & \cmark & \xmark & 95.8$\pm0.0$ & \textbf{97.0}$\pm0.1$ & 94.6$\pm0.1$ & 94.0$\pm0.1$ & 96.3$\pm0.1$ & 98.0$\pm0.0$ & 94.8$\pm0.0$ & 92.4$\pm0.2$ & 95.4 \\
3. & \cmark & \xmark & \xmark & \xmark & 94.3$\pm0.0$ & 93.9$\pm0.1$ & 91.2$\pm0.1$ & 91.7$\pm0.1$ & 94.7$\pm0.0$ & 96.8$\pm0.1$ & 92.5$\pm0.1$ & 87.1$\pm0.2$ & 92.8 \\
4. & \cmark & \xmark & \cmark & \xmark & 95.1$\pm0.1$ & 95.8$\pm0.0$ & 92.5$\pm0.1$ & 93.4$\pm0.0$ & 96.0$\pm0.1$ & 97.6$\pm0.0$ & 94.0$\pm0.1$ & 89.7$\pm0.3$ & 94.3 \\
5. & \cmark & \xmark & \cmark & \cmark & 95.2$\pm0.0$ & 96.1$\pm0.1$ & 92.6$\pm0.1$ & 93.5$\pm0.1$ & 96.0$\pm0.1$ & 97.6$\pm0.1$ & 94.2$\pm0.1$ & 89.8$\pm0.5$ & 94.4 \\
6. & \cmark & \cmark & \cmark & \xmark & \textbf{95.9}$\pm0.0$ & \textbf{97.0}$\pm0.1$ & 94.6$\pm0.1$ & \textbf{94.4}$\pm0.1$ & \textbf{96.6}$\pm0.1$ & \textbf{98.2}$\pm0.1$ & \textbf{95.0}$\pm0.0$ & \textbf{92.9}$\pm0.2$ & \textbf{95.6} \\
7. & \cmark & \cmark & \cmark & \cmark & \textbf{95.9}$\pm0.0$ & \textbf{97.0}$\pm0.0$ & \textbf{94.7}$\pm0.1$ & \textbf{94.4}$\pm0.1$ & \textbf{96.6}$\pm0.1$ & 98.1$\pm0.1$ & \textbf{95.0}$\pm0.0$ & 92.6$\pm0.5$ & 95.5 \\
\hline\hline
\multicolumn{13}{c}{\bf \textsc{Rich-Resource}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & 96.7$\pm0.1$ & 98.6$\pm0.1$ & 94.9$\pm0.1$ & 96.3$\pm0.0$ & 97.0$\pm0.1$ & 98.6$\pm0.1$ & 96.3$\pm0.0$ & 91.9$\pm0.4$ & 96.3 \\
2. & \xmark & \cmark & \cmark & \xmark & 96.9$\pm0.0$ & 98.6$\pm0.0$ & 95.0$\pm0.1$ & 96.7$\pm0.0$ & 97.1$\pm0.1$ & 98.9$\pm0.0$ & \textbf{96.7}$\pm0.0$ & 92.4$\pm0.5$ & 96.5 \\
3. & \cmark & \xmark & \xmark & \xmark & 96.3$\pm0.0$ & 97.8$\pm0.0$ & 94.9$\pm0.1$ & 95.8$\pm0.0$ & 96.6$\pm0.1$ & 98.4$\pm0.1$ & 95.5$\pm0.1$ & 86.9$\pm0.2$ & 95.3 \\
4. & \cmark & \xmark & \cmark & \xmark & 96.7$\pm0.0$ & 98.6$\pm0.1$ & 95.2$\pm0.1$ & 96.6$\pm0.0$ & 97.0$\pm0.1$ & 98.9$\pm0.0$ & 96.2$\pm0.1$ & 89.9$\pm0.3$ & 96.1 \\
5. & \cmark & \xmark & \cmark & \cmark & 96.8$\pm0.0$ & 98.6$\pm0.0$ & 95.2$\pm0.1$ & 96.7$\pm0.0$ & 97.1$\pm0.1$ & 99.0$\pm0.1$ & 96.3$\pm0.1$ & 90.1$\pm0.2$ & 96.2 \\
6. & \cmark & \cmark & \cmark & \xmark & 96.9$\pm0.0$ & \textbf{98.8}$\pm0.0$ & 95.3$\pm0.1$ & \textbf{97.0}$\pm0.1$ & \textbf{97.3}$\pm0.0$ & \textbf{99.1}$\pm0.1$ & \textbf{96.7}$\pm0.1$ & \textbf{92.6}$\pm0.4$ & \textbf{96.7} \\
7. & \cmark & \cmark & \cmark & \cmark & \textbf{97.0}$\pm0.1$ & \textbf{98.8}$\pm0.0$ & \textbf{95.4}$\pm0.1$ & \textbf{97.0}$\pm0.1$ & \textbf{97.3}$\pm0.1$ & \textbf{99.1}$\pm0.1$ & \textbf{96.7}$\pm0.1$ & 92.5$\pm0.4$ & \textbf{96.7} \\
\hlineB{4}
\end{tabular}
\caption{Averaged accuracy scores over 8 languages for UD POS tagging.}
\label{tab:pos}
\end{table*}


\begin{table*}[ht]
\setlength\tabcolsep{4pt}
\small
\centering
\begin{tabular}{l|cccc||cccccccc|c}
\hlineB{4}
& \multicolumn{4}{c||}{\bf \textsc{Embeddings}} & \multicolumn{9}{c}{\bf BiLSTM-CRF models on UD POS tagging} \\  
\hhline{~|----||---------}
 &  \textbf{M} & \textbf{F} & \textbf{W} & \textbf{C}  & ar & cs & de & en & es & fr & nl & ta & Avg. \\
\hline\hline
\multicolumn{13}{c}{\bf \textsc{Low-Resource: 10 Sentences}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & 85.5$\pm0.3$ & 81.6$\pm0.8$ & \textbf{82.9}$\pm0.2$ & 77.1$\pm0.0$ & 87.9$\pm0.3$ & 84.9$\pm0.7$ & \textbf{70.7}$\pm0.7$ & 71.5$\pm0.8$ & 80.2 \\
2. & \xmark & \cmark & \cmark & \xmark & \textbf{86.3}$\pm0.5$ & \textbf{82.0}$\pm0.2$ & 82.5$\pm0.4$ & \textbf{78.9}$\pm0.4$ & \textbf{88.2}$\pm0.3$ & \textbf{85.2}$\pm0.4$ & 70.5$\pm1.1$ & 72.6$\pm0.5$ & \textbf{80.8} \\
3. & \cmark & \xmark & \xmark & \xmark & 79.7$\pm1.2$ & 72.3$\pm0.4$ & 72.6$\pm1.0$ & 69.0$\pm0.3$ & 78.8$\pm0.6$ & 76.7$\pm0.5$ & 62.5$\pm1.4$ & 62.3$\pm0.8$ & 71.7 \\
4. & \cmark & \xmark & \cmark & \xmark & 81.7$\pm0.4$ & 74.2$\pm0.8$ & 74.2$\pm0.8$ & 70.9$\pm0.4$ & 80.7$\pm0.3$ & 77.8$\pm0.7$ & 62.7$\pm0.7$ & 66.4$\pm1.2$ & 73.6 \\
5. & \cmark & \xmark & \cmark & \cmark & 82.0$\pm0.5$ & 74.0$\pm0.8$ & 73.9$\pm0.8$ & 70.6$\pm1.9$ & 81.0$\pm0.4$ & 78.5$\pm0.5$ & 63.6$\pm1.2$ & 66.4$\pm1.2$ & 73.7 \\
6. & \cmark & \cmark & \cmark & \xmark & 85.8$\pm0.5$ & 79.5$\pm0.4$ & 80.8$\pm0.5$ & 75.3$\pm0.4$ & 86.6$\pm0.2$ & 82.2$\pm0.4$ & 70.3$\pm0.8$ & 72.5$\pm0.4$ & 79.1 \\
7. & \cmark & \cmark & \cmark & \cmark & 85.9$\pm0.4$ & 80.0$\pm0.1$ & 80.5$\pm0.7$ & 74.7$\pm0.1$ & 86.4$\pm0.4$ & 82.4$\pm0.9$ & \textbf{70.7}$\pm0.3$ & \textbf{72.8}$\pm0.5$ & 79.2 \\
\hline\hline
\multicolumn{13}{c}{\bf \textsc{Low-Resource: 50 Sentences}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & 91.7$\pm0.2$ & 90.5$\pm0.1$ & \textbf{90.9}$\pm0.3$ & 87.6$\pm0.2$ & 93.0$\pm0.1$ & 91.6$\pm0.3$ & 81.8$\pm0.7$ & 85.6$\pm0.5$ & 89.1 \\
2. & \xmark & \cmark & \cmark & \xmark & \textbf{91.9}$\pm0.2$ & \textbf{91.2}$\pm0.2$ & 90.8$\pm0.1$ & \textbf{87.9}$\pm0.3$ & \textbf{93.5}$\pm0.1$ & 91.8$\pm0.2$ & \textbf{83.2}$\pm0.4$ & \textbf{86.5}$\pm0.3$ & \textbf{89.6} \\
3. & \cmark & \xmark & \xmark & \xmark & 87.4$\pm0.4$ & 84.2$\pm0.7$ & 83.6$\pm0.2$ & 80.1$\pm0.4$ & 88.3$\pm0.5$ & 86.4$\pm0.4$ & 72.6$\pm0.3$ & 76.1$\pm0.7$ & 82.3 \\
4. & \cmark & \xmark & \cmark & \xmark & 89.4$\pm0.4$ & 86.3$\pm0.3$ & 85.3$\pm0.9$ & 81.0$\pm0.1$ & 89.7$\pm0.4$ & 88.4$\pm1.0$ & 74.2$\pm1.6$ & 80.3$\pm0.3$ & 84.3 \\
5. & \cmark & \xmark & \cmark & \cmark & 89.4$\pm0.4$ & 86.6$\pm0.6$ & 85.0$\pm0.6$ & 81.5$\pm0.6$ & 89.6$\pm0.3$ & 88.1$\pm0.5$ & 75.1$\pm0.6$ & 80.7$\pm0.6$ & 84.5 \\
6. & \cmark & \cmark & \cmark & \xmark & 91.5$\pm0.2$ & 90.7$\pm0.1$ & 90.2$\pm0.3$ & 85.7$\pm0.1$ & 92.8$\pm0.5$ & 91.2$\pm0.5$ & 82.1$\pm0.6$ & \textbf{86.5}$\pm0.3$ & 88.8 \\
7. & \cmark & \cmark & \cmark & \cmark & 91.5$\pm0.1$ & 90.7$\pm0.5$ & 90.3$\pm0.3$ & 85.3$\pm0.1$ & 92.4$\pm0.2$ & \textbf{91.9}$\pm0.6$ & 82.5$\pm0.2$ & 86.0$\pm0.7$ & 88.8 \\
\hline\hline
\multicolumn{13}{c}{\bf \textsc{Low-Resource: 100 Sentences}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & 93.3$\pm0.1$ & 93.5$\pm0.2$ & \textbf{92.0}$\pm0.3$ & 89.8$\pm0.1$ & 94.0$\pm0.1$ & 94.2$\pm0.2$ & 87.4$\pm0.3$ & 88.3$\pm0.4$ & 91.6 \\
2. & \xmark & \cmark & \cmark & \xmark & \textbf{93.6}$\pm0.1$ & 93.6$\pm0.1$ & 91.8$\pm0.3$ & \textbf{90.0}$\pm0.1$ & \textbf{94.5}$\pm0.1$ & \textbf{94.5}$\pm0.1$ & \textbf{88.5}$\pm0.1$ & 88.7$\pm0.3$ & \textbf{91.9} \\
3. & \cmark & \xmark & \xmark & \xmark & 90.2$\pm0.2$ & 88.4$\pm0.1$ & 85.2$\pm0.3$ & 84.6$\pm0.2$ & 90.5$\pm0.2$ & 90.0$\pm0.3$ & 78.0$\pm0.7$ & 80.8$\pm0.3$ & 86.0 \\
4. & \cmark & \xmark & \cmark & \xmark & 91.6$\pm0.1$ & 90.6$\pm0.1$ & 87.4$\pm0.3$ & 85.5$\pm0.1$ & 92.5$\pm0.5$ & 91.7$\pm0.2$ & 81.7$\pm0.6$ & 85.2$\pm0.4$ & 88.3 \\
5. & \cmark & \xmark & \cmark & \cmark & 91.8$\pm0.1$ & 91.1$\pm0.5$ & 87.4$\pm0.4$ & 86.1$\pm0.1$ & 92.2$\pm0.1$ & 92.1$\pm0.8$ & 81.2$\pm1.1$ & 85.5$\pm0.2$ & 88.4 \\
6. & \cmark & \cmark & \cmark & \xmark & 93.5$\pm0.2$ & \textbf{93.7}$\pm0.3$ & 91.4$\pm0.2$ & 89.4$\pm0.6$ & 94.3$\pm0.2$ & 94.2$\pm0.2$ & 86.9$\pm0.3$ & \textbf{89.6}$\pm0.4$ & 91.6 \\
7. & \cmark & \cmark & \cmark & \cmark & \textbf{93.6}$\pm0.1$ & \textbf{93.7}$\pm0.3$ & 91.7$\pm0.2$ & 89.3$\pm0.6$ & 94.4$\pm0.1$ & 94.3$\pm0.3$ & 87.0$\pm0.8$ & 89.4$\pm0.2$ & 91.7 \\
\hline\hline
\multicolumn{13}{c}{\bf \textsc{Low-Resource: 500 Sentences}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & 95.2$\pm0.1$ & 96.1$\pm0.1$ & 94.1$\pm0.1$ & 92.6$\pm0.1$ & 95.9$\pm0.0$ & 97.0$\pm0.1$ & 92.8$\pm0.2$ & 91.9$\pm0.4$ & 94.4 \\
2. & \xmark & \cmark & \cmark & \xmark & 95.3$\pm0.1$ & \textbf{96.3}$\pm0.1$ & \textbf{94.3}$\pm0.1$ & 92.6$\pm0.1$ & 95.9$\pm0.1$ & 97.2$\pm0.1$ & \textbf{93.3}$\pm0.1$ & 92.5$\pm0.2$ & \textbf{94.7} \\
3. & \cmark & \xmark & \xmark & \xmark & 93.4$\pm0.1$ & 92.6$\pm0.0$ & 89.6$\pm0.2$ & 89.7$\pm0.0$ & 93.8$\pm0.2$ & 95.5$\pm0.1$ & 89.5$\pm0.2$ & 86.5$\pm0.2$ & 91.3 \\
4. & \cmark & \xmark & \cmark & \xmark & 94.5$\pm0.2$ & 94.7$\pm0.2$ & 91.4$\pm0.1$ & 91.0$\pm0.0$ & 95.1$\pm0.1$ & 96.7$\pm0.0$ & 91.3$\pm0.2$ & 89.9$\pm0.3$ & 93.1 \\
5. & \cmark & \xmark & \cmark & \cmark & 94.8$\pm0.1$ & 95.0$\pm0.2$ & 91.5$\pm0.1$ & 91.1$\pm0.1$ & 95.2$\pm0.1$ & 97.0$\pm0.1$ & 91.6$\pm0.1$ & 90.0$\pm0.4$ & 93.3 \\
6. & \cmark & \cmark & \cmark & \xmark & \textbf{95.5}$\pm0.0$ & 96.2$\pm0.1$ & 93.8$\pm0.1$ & 92.7$\pm0.1$ & 95.9$\pm0.2$ & \textbf{97.5}$\pm0.1$ & \textbf{93.3}$\pm0.1$ & 92.3$\pm0.2$ & \textbf{94.7} \\
7. & \cmark & \cmark & \cmark & \cmark & \textbf{95.5}$\pm0.1$ & 96.1$\pm0.1$ & 93.9$\pm0.1$ & \textbf{92.8}$\pm0.1$ & \textbf{96.2}$\pm0.1$ & 97.4$\pm0.1$ & \textbf{93.3}$\pm0.2$ & \textbf{92.7}$\pm0.2$ & \textbf{94.7} \\
\hline\hline
\multicolumn{13}{c}{\bf \textsc{Low-Resource: 1000 Sentences}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & 95.6$\pm0.0$ & 96.8$\pm0.0$ & \textbf{94.6}$\pm0.1$ & 93.5$\pm0.1$ & 96.3$\pm0.1$ & 97.5$\pm0.1$ & 94.5$\pm0.1$ & 92.0$\pm0.5$ & 95.1 \\
2. & \xmark & \cmark & \cmark & \xmark & 95.8$\pm0.1$ & 96.9$\pm0.0$ & 94.5$\pm0.0$ & 93.8$\pm0.1$ & 96.3$\pm0.1$ & 97.9$\pm0.1$ & 94.8$\pm0.1$ & 91.9$\pm0.4$ & 95.2 \\
3. & \cmark & \xmark & \xmark & \xmark & 94.2$\pm0.1$ & 93.9$\pm0.1$ & 90.9$\pm0.2$ & 91.4$\pm0.0$ & 94.7$\pm0.1$ & 96.7$\pm0.2$ & 92.5$\pm0.1$ & 86.4$\pm0.3$ & 92.6 \\
4. & \cmark & \xmark & \cmark & \xmark & 95.1$\pm0.0$ & 95.9$\pm0.1$ & 92.3$\pm0.1$ & 92.8$\pm0.0$ & 95.9$\pm0.2$ & 97.6$\pm0.2$ & 93.9$\pm0.1$ & 89.7$\pm0.4$ & 94.2 \\
5. & \cmark & \xmark & \cmark & \cmark & 95.2$\pm0.1$ & 96.1$\pm0.1$ & 92.5$\pm0.2$ & 93.3$\pm0.1$ & 96.1$\pm0.1$ & 97.5$\pm0.1$ & 94.0$\pm0.1$ & 89.8$\pm0.4$ & 94.3 \\
6. & \cmark & \cmark & \cmark & \xmark & \textbf{95.9}$\pm0.0$ & 96.9$\pm0.1$ & 94.4$\pm0.2$ & \textbf{94.2}$\pm0.2$ & \textbf{96.5}$\pm0.1$ & \textbf{98.1}$\pm0.1$ & \textbf{95.0}$\pm0.1$ & \textbf{92.7}$\pm0.2$ & \textbf{95.5} \\
7. & \cmark & \cmark & \cmark & \cmark & \textbf{95.9}$\pm0.1$ & \textbf{97.0}$\pm0.0$ & 94.5$\pm0.1$ & 94.1$\pm0.1$ & \textbf{96.5}$\pm0.0$ & \textbf{98.1}$\pm0.1$ & \textbf{95.0}$\pm0.1$ & 92.5$\pm0.3$ & \textbf{95.5}\\
\hline\hline
\multicolumn{13}{c}{\bf \textsc{Rich-Resource}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & 96.7$\pm0.0$ & 98.6$\pm0.0$ & 95.0$\pm0.1$ & 96.4$\pm0.1$ & 97.0$\pm0.0$ & 98.5$\pm0.1$ & 96.3$\pm0.0$ & 92.1$\pm0.5$ & 96.3 \\
2. & \xmark & \cmark & \cmark & \xmark & 96.9$\pm0.0$ & 98.7$\pm0.0$ & 95.0$\pm0.1$ & 96.7$\pm0.1$ & 97.1$\pm0.0$ & 98.9$\pm0.0$ & 96.6$\pm0.0$ & 92.4$\pm0.4$ & 96.5 \\
3. & \cmark & \xmark & \xmark & \xmark & 96.3$\pm0.0$ & 97.8$\pm0.0$ & 94.9$\pm0.0$ & 95.8$\pm0.1$ & 96.6$\pm0.1$ & 98.4$\pm0.1$ & 95.4$\pm0.1$ & 86.7$\pm0.3$ & 95.2 \\
4. & \cmark & \xmark & \cmark & \xmark & 96.7$\pm0.0$ & 98.6$\pm0.0$ & 95.3$\pm0.1$ & 96.6$\pm0.1$ & 97.0$\pm0.1$ & 99.0$\pm0.0$ & 96.2$\pm0.1$ & 89.8$\pm0.3$ & 96.1 \\
5. & \cmark & \xmark & \cmark & \cmark & 96.8$\pm0.1$ & 98.6$\pm0.0$ & 95.1$\pm0.0$ & 96.7$\pm0.0$ & 97.0$\pm0.1$ & 99.0$\pm0.1$ & 96.3$\pm0.1$ & 90.0$\pm0.6$ & 96.2 \\
6. & \cmark & \cmark & \cmark & \xmark & \textbf{97.0}$\pm0.0$ & \textbf{98.8}$\pm0.0$ & \textbf{95.4}$\pm0.1$ & 97.0$\pm0.0$ & \textbf{97.3}$\pm0.1$ & \textbf{99.1}$\pm0.1$ & \textbf{96.7}$\pm0.1$ & \textbf{92.7}$\pm0.3$ & \textbf{96.7} \\
7. & \cmark & \cmark & \cmark & \cmark & \textbf{97.0}$\pm0.0$ & \textbf{98.8}$\pm0.0$ & \textbf{95.4}$\pm0.1$ & \textbf{97.1}$\pm0.0$ & \textbf{97.3}$\pm0.1$ & \textbf{99.1}$\pm0.0$ & \textbf{96.7}$\pm0.1$ & 92.6$\pm0.1$ & \textbf{96.7} \\
\hlineB{4}
\end{tabular}
\caption{Averaged accuracy scores over 8 languages for UD POS tagging with the CRF layer.}
\label{tab:crf_pos}
\end{table*}


\begin{table}[]
\small
\centering
\begin{tabular}{l|cccc||cc|c}
\hlineB{4}
& \multicolumn{4}{c||}{\bf \textsc{Embeddings}} & \multicolumn{3}{c}{\bf MaxEnt models on chunking} \\  
\hhline{~|----||---}
 &  \textbf{M} & \textbf{F} & \textbf{W} & \textbf{C}  & de & en & Avg. \\
\hline\hline
\multicolumn{8}{c}{\bf \textsc{Low-Resource: 10 Sentences}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & \textbf{83.3}$\pm0.6$ & 71.8$\pm0.3$ & 77.6 \\
2. & \xmark & \cmark & \cmark & \xmark & 83.2$\pm0.6$ & \textbf{73.3}$\pm0.6$ & \textbf{78.2} \\
3. & \cmark & \xmark & \xmark & \xmark & 64.9$\pm1.1$ & 65.1$\pm1.0$ & 65.0 \\
4. & \cmark & \xmark & \cmark & \xmark & 66.3$\pm3.0$ & 67.7$\pm0.6$ & 67.0 \\
5. & \cmark & \xmark & \cmark & \cmark & 66.5$\pm1.8$ & 67.0$\pm0.3$ & 66.7 \\
6. & \cmark & \cmark & \cmark & \xmark & 81.3$\pm1.6$ & 70.6$\pm0.1$ & 75.9 \\
7. & \cmark & \cmark & \cmark & \cmark & 81.4$\pm0.9$ & 70.8$\pm0.5$ & 76.1 \\
\hline\hline
\multicolumn{8}{c}{\bf \textsc{Low-Resource: 50 Sentences}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & \textbf{87.9}$\pm0.3$ & 80.6$\pm0.7$ & 84.3 \\
2. & \xmark & \cmark & \cmark & \xmark & 87.6$\pm0.4$ & \textbf{82.2}$\pm0.0$ & \textbf{84.9} \\
3. & \cmark & \xmark & \xmark & \xmark & 80.5$\pm1.3$ & 74.1$\pm0.2$ & 77.3 \\
4. & \cmark & \xmark & \cmark & \xmark & 80.6$\pm0.8$ & 76.8$\pm0.2$ & 78.7 \\
5. & \cmark & \xmark & \cmark & \cmark & 81.3$\pm0.5$ & 76.2$\pm0.3$ & 78.7 \\
6. & \cmark & \cmark & \cmark & \xmark & 86.2$\pm0.7$ & 80.3$\pm0.3$ & 83.2 \\
7. & \cmark & \cmark & \cmark & \cmark & 86.0$\pm1.0$ & 80.3$\pm0.3$ & 83.1 \\
\hline\hline
\multicolumn{8}{c}{\bf \textsc{Low-Resource: 100 Sentences}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & \textbf{88.8}$\pm0.3$ & 82.7$\pm0.0$ & 85.8 \\
2. & \xmark & \cmark & \cmark & \xmark & \textbf{88.8}$\pm0.4$ & \textbf{83.7}$\pm0.0$ & \textbf{86.3} \\
3. & \cmark & \xmark & \xmark & \xmark & 84.2$\pm0.4$ & 75.7$\pm0.4$ & 79.9 \\
4. & \cmark & \xmark & \cmark & \xmark & 84.9$\pm0.5$ & 79.1$\pm0.4$ & 82.0 \\
5. & \cmark & \xmark & \cmark & \cmark & 85.0$\pm0.4$ & 79.3$\pm0.2$ & 82.2 \\
6. & \cmark & \cmark & \cmark & \xmark & 88.3$\pm0.4$ & 82.3$\pm0.2$ & 85.3 \\
7. & \cmark & \cmark & \cmark & \cmark & 88.4$\pm0.3$ & 82.4$\pm0.0$ & 85.4 \\
\hline\hline
\multicolumn{8}{c}{\bf \textsc{Low-Resource: 500 Sentences}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & 91.3$\pm0.1$ & 86.2$\pm0.2$ & 88.7 \\
2. & \xmark & \cmark & \cmark & \xmark & 91.2$\pm0.1$ & \textbf{86.8}$\pm0.0$ & \textbf{89.0} \\
3. & \cmark & \xmark & \xmark & \xmark & 89.7$\pm0.1$ & 82.7$\pm0.1$ & 86.2 \\
4. & \cmark & \xmark & \cmark & \xmark & 90.5$\pm0.1$ & 84.9$\pm0.2$ & 87.7 \\
5. & \cmark & \xmark & \cmark & \cmark & 90.3$\pm0.3$ & 85.2$\pm0.0$ & 87.8 \\
6. & \cmark & \cmark & \cmark & \xmark & \textbf{91.4}$\pm0.1$ & 86.6$\pm0.0$ & \textbf{89.0} \\
7. & \cmark & \cmark & \cmark & \cmark & \textbf{91.4}$\pm0.1$ & 86.7$\pm0.1$ & \textbf{89.0} \\
\hline\hline
\multicolumn{8}{c}{\bf \textsc{Low-Resource: 1000 Sentences}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & \textbf{92.2}$\pm0.1$ & 86.6$\pm0.1$ & 89.4 \\
2. & \xmark & \cmark & \cmark & \xmark & 92.1$\pm0.1$ & \textbf{87.1}$\pm0.1$ & \textbf{89.6} \\
3. & \cmark & \xmark & \xmark & \xmark & 90.6$\pm0.1$ & 84.0$\pm0.1$ & 87.3 \\
4. & \cmark & \xmark & \cmark & \xmark & 91.4$\pm0.1$ & 85.7$\pm0.1$ & 88.6 \\
5. & \cmark & \xmark & \cmark & \cmark & 91.5$\pm0.2$ & 86.0$\pm0.1$ & 88.7 \\
6. & \cmark & \cmark & \cmark & \xmark & \textbf{92.2}$\pm0.1$ & \textbf{87.1}$\pm0.1$ & \textbf{89.6} \\
7. & \cmark & \cmark & \cmark & \cmark & \textbf{92.2}$\pm0.1$ & \textbf{87.1}$\pm0.2$ & \textbf{89.6} \\
\hline\hline
\multicolumn{8}{c}{\bf \textsc{Rich-Resource}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & 94.1$\pm0.1$ & 90.5$\pm0.0$ & 92.3 \\
2. & \xmark & \cmark & \cmark & \xmark & \textbf{94.2}$\pm0.1$ & \textbf{91.7}$\pm0.1$ & \textbf{92.9} \\
3. & \cmark & \xmark & \xmark & \xmark & 93.0$\pm0.1$ & 89.4$\pm0.0$ & 91.2 \\
4. & \cmark & \xmark & \cmark & \xmark & 93.6$\pm0.1$ & 91.3$\pm0.1$ & 92.5 \\
5. & \cmark & \xmark & \cmark & \cmark & 93.8$\pm0.1$ & 91.4$\pm0.0$ & 92.6 \\
6. & \cmark & \cmark & \cmark & \xmark & 94.0$\pm0.1$ & \textbf{91.7}$\pm0.0$ & \textbf{92.9} \\
7. & \cmark & \cmark & \cmark & \cmark & 94.1$\pm0.1$ & 91.6$\pm0.1$ & \textbf{92.9} \\
\hlineB{4}
\end{tabular}
\caption{Averaged F1 scores over 2 languages for chunking.}
\label{tab:chunk}
\end{table}

\begin{table}[]
\small
\centering
\begin{tabular}{l|cccc||cc|c}
\hlineB{4}
& \multicolumn{4}{c||}{\bf \textsc{Embeddings}} & \multicolumn{3}{c}{\bf CRF models on chunking} \\  
\hhline{~|----||---}
 &  \textbf{M} & \textbf{F} & \textbf{W} & \textbf{C}  & de & en & Avg. \\
\hline\hline
\multicolumn{8}{c}{\bf \textsc{Low-Resource: 10 Sentences}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & \textbf{83.0}$\pm0.8$ & 70.1$\pm0.1$ & \textbf{76.5} \\
2. & \xmark & \cmark & \cmark & \xmark & 82.6$\pm1.1$ & \textbf{70.5}$\pm0.1$ & \textbf{76.5} \\
3. & \cmark & \xmark & \xmark & \xmark & 68.0$\pm2.1$ & 62.9$\pm1.2$ & 65.4 \\
4. & \cmark & \xmark & \cmark & \xmark & 71.2$\pm1.8$ & 62.1$\pm0.6$ & 66.6 \\
5. & \cmark & \xmark & \cmark & \cmark & 71.0$\pm1.0$ & 61.2$\pm0.8$ & 66.1 \\
6. & \cmark & \cmark & \cmark & \xmark & 81.2$\pm0.9$ & 64.9$\pm3.2$ & 73.0 \\
7. & \cmark & \cmark & \cmark & \cmark & 81.3$\pm0.8$ & 66.3$\pm2.9$ & 73.8 \\
\hline\hline
\multicolumn{8}{c}{\bf \textsc{Low-Resource: 50 Sentences}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & \textbf{88.0}$\pm0.4$ & 79.9$\pm0.2$ & 84.0 \\
2. & \xmark & \cmark & \cmark & \xmark & 87.9$\pm0.3$ & \textbf{80.9}$\pm0.4$ & \textbf{84.4} \\
3. & \cmark & \xmark & \xmark & \xmark & 80.8$\pm0.8$ & 68.7$\pm0.7$ & 74.8 \\
4. & \cmark & \xmark & \cmark & \xmark & 82.8$\pm1.2$ & 71.1$\pm1.1$ & 76.9 \\
5. & \cmark & \xmark & \cmark & \cmark & 83.0$\pm1.5$ & 71.4$\pm1.1$ & 77.2 \\
6. & \cmark & \cmark & \cmark & \xmark & 86.2$\pm1.3$ & 73.6$\pm0.6$ & 79.9 \\
7. & \cmark & \cmark & \cmark & \cmark & 86.5$\pm0.5$ & 74.2$\pm1.4$ & 80.3 \\
\hline\hline
\multicolumn{8}{c}{\bf \textsc{Low-Resource: 100 Sentences}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & \textbf{89.0}$\pm0.5$ & \textbf{82.9}$\pm0.5$ & \textbf{85.9} \\
2. & \xmark & \cmark & \cmark & \xmark & \textbf{89.0}$\pm0.4$ & 82.6$\pm0.2$ & 85.8 \\
3. & \cmark & \xmark & \xmark & \xmark & 84.7$\pm0.5$ & 74.8$\pm0.6$ & 79.8 \\
4. & \cmark & \xmark & \cmark & \xmark & 86.0$\pm0.4$ & 75.4$\pm1.1$ & 80.7 \\
5. & \cmark & \xmark & \cmark & \cmark & 86.1$\pm0.4$ & 74.6$\pm1.6$ & 80.4 \\
6. & \cmark & \cmark & \cmark & \xmark & \textbf{89.0}$\pm0.6$ & 79.7$\pm0.5$ & 84.3 \\
7. & \cmark & \cmark & \cmark & \cmark & 88.3$\pm0.7$ & 77.7$\pm0.6$ & 83.0 \\
\hline\hline
\multicolumn{8}{c}{\bf \textsc{Low-Resource: 500 Sentences}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & 91.5$\pm0.2$ & 86.1$\pm0.2$ & 88.8 \\
2. & \xmark & \cmark & \cmark & \xmark & 91.4$\pm0.1$ & \textbf{87.1}$\pm0.1$ & \textbf{89.3} \\
3. & \cmark & \xmark & \xmark & \xmark & 90.1$\pm0.2$ & 83.3$\pm0.3$ & 86.7 \\
4. & \cmark & \xmark & \cmark & \xmark & 90.8$\pm0.1$ & 84.8$\pm0.2$ & 87.8 \\
5. & \cmark & \xmark & \cmark & \cmark & 90.8$\pm0.1$ & 84.8$\pm0.2$ & 87.8 \\
6. & \cmark & \cmark & \cmark & \xmark & 91.5$\pm0.1$ & 86.1$\pm0.1$ & 88.8 \\
7. & \cmark & \cmark & \cmark & \cmark & \textbf{91.6}$\pm0.1$ & 85.9$\pm0.3$ & 88.7 \\
\hline\hline
\multicolumn{8}{c}{\bf \textsc{Low-Resource: 1000 Sentences}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & 92.4$\pm0.1$ & 86.7$\pm0.2$ & 89.6 \\
2. & \xmark & \cmark & \cmark & \xmark & 92.4$\pm0.1$ & \textbf{87.2}$\pm0.2$ & \textbf{89.8} \\
3. & \cmark & \xmark & \xmark & \xmark & 91.0$\pm0.0$ & 84.0$\pm0.0$ & 87.5 \\
4. & \cmark & \xmark & \cmark & \xmark & 91.5$\pm0.1$ & 85.6$\pm0.2$ & 88.6 \\
5. & \cmark & \xmark & \cmark & \cmark & 91.7$\pm0.2$ & 85.9$\pm0.1$ & 88.8 \\
6. & \cmark & \cmark & \cmark & \xmark & 92.4$\pm0.1$ & 86.9$\pm0.1$ & 89.6 \\
7. & \cmark & \cmark & \cmark & \cmark & \textbf{92.5}$\pm0.1$ & 86.8$\pm0.1$ & 89.7 \\
\hline\hline
\multicolumn{8}{c}{\bf \textsc{Rich-Resource}}\\
\hline
1. & \xmark & \cmark & \xmark & \xmark & \textbf{94.4}$\pm0.0$ & 91.0$\pm0.1$ & 92.7 \\
2. & \xmark & \cmark & \cmark & \xmark & \textbf{94.4}$\pm0.1$ & \textbf{92.0}$\pm0.0$ & \textbf{93.2} \\
3. & \cmark & \xmark & \xmark & \xmark & 93.2$\pm0.1$ & 90.2$\pm0.1$ & 91.7 \\
4. & \cmark & \xmark & \cmark & \xmark & 93.8$\pm0.1$ & 91.7$\pm0.0$ & 92.8 \\
5. & \cmark & \xmark & \cmark & \cmark & 94.0$\pm0.1$ & 91.7$\pm0.0$ & 92.8 \\
6. & \cmark & \cmark & \cmark & \xmark & 94.2$\pm0.1$ & 91.8$\pm0.0$ & 93.0 \\
7. & \cmark & \cmark & \cmark & \cmark & 94.3$\pm0.1$ & 91.9$\pm0.1$ & 93.1 \\

\hlineB{4}
\end{tabular}
\caption{Averaged F1 scores over 2 languages for chunking with the CRF layer.}
\label{tab:crf_chunk}
\end{table}



\begin{table*}[]
\setlength\tabcolsep{4pt}
\small
\centering
\begin{tabular}{ccccc||cccccccc|c}
\hlineB{4}
\multicolumn{5}{c||}{\bf \textsc{Embeddings}} & \multicolumn{9}{c}{\bf Languages} \\  
\hline\hline
\multirow{2}{*}{\textbf{M}} & \multirow{2}{*}{\textbf{F}} & \multirow{2}{*}{\textbf{W}} & \multirow{2}{*}{\textbf{C}} & \multirow{2}{*}{\textbf{MF}} & \multicolumn{9}{c}{\bf \textsc{NER}}\\
\hhline{~~~~~||--------|-}
 &  &  &  &  & ar & cs & de & en & es & fr & nl & ta & Avg. \\
 \hline
\cmark & \cmark & \cmark & \cmark & \xmark & 84.6$\pm0.2$ & 88.4$\pm0.2$ & \textbf{85.5}$\pm0.3$ & 81.7$\pm0.2$ & \textbf{90.6}$\pm0.2$ & \textbf{88.4}$\pm0.5$ & \textbf{90.1}$\pm0.3$ & \textbf{85.5}$\pm0.3$ & \textbf{86.8} \\
\cmark & \xmark & \cmark & \cmark & \cmark & 83.6$\pm0.8$ & 87.7$\pm0.3$ & 84.2$\pm0.3$ & 81.4$\pm0.1$ & 90.0$\pm0.1$ & 87.9$\pm0.1$ & 89.9$\pm0.4$ & 84.2$\pm0.2$ & 86.1 \\
\cmark & \cmark & \cmark & \cmark & \cmark & \textbf{84.8}$\pm0.5$ & \textbf{88.6}$\pm0.2$ & 85.1$\pm0.3$ & \textbf{82.0}$\pm0.2$ & 90.3$\pm0.4$ & 88.1$\pm0.3$ & \textbf{90.1}$\pm0.2$ & 85.3$\pm0.3$ & \textbf{86.8} \\
\hline\hline
& & & & & \multicolumn{9}{c}{\bf \textsc{POS Tagging}}\\
\hhline{~~~~~||--------|-}
 &  &  &  &  &  ar & cs & de & en & es & fr & nl & ta & Avg. \\
 \hline
\cmark & \cmark & \cmark & \cmark & \xmark & \textbf{97.0}$\pm0.1$ & \textbf{98.8}$\pm0.0$ & \textbf{95.4}$\pm0.1$ & \textbf{97.0}$\pm0.1$ & \textbf{97.3}$\pm0.1$ & \textbf{99.1}$\pm0.1$ & \textbf{96.7}$\pm0.1$ & 92.5$\pm0.4$ & \textbf{96.7} \\
\cmark & \xmark & \cmark & \cmark & \cmark & 96.8$\pm0.0$ & 98.7$\pm0.0$ & 95.2$\pm0.1$ & 96.6$\pm0.1$ & 97.2$\pm0.0$ & 99.0$\pm0.0$ & 96.5$\pm0.1$ & 91.2$\pm0.3$ & 96.4 \\
\cmark & \cmark & \cmark & \cmark & \cmark & \textbf{97.0}$\pm0.0$ & \textbf{98.8}$\pm0.0$ & 95.3$\pm0.0$ & 96.9$\pm0.1$ & \textbf{97.3}$\pm0.1$ & \textbf{99.1}$\pm0.0$ & \textbf{96.7}$\pm0.1$ & \textbf{92.7}$\pm0.5$ & \textbf{96.7} \\
\hline\hline
& & & & & \multicolumn{9}{c}{\bf \textsc{Chunking}}\\
\hhline{~~~~~||--------|-}
 &  &  &  &  & de & en & &  &  &  &  &  &  Avg.\\
 \hline
\cmark & \cmark & \cmark & \cmark & \xmark & 94.0$\pm0.0$ & 91.5$\pm0.0$ &  &  &  &  &  &  & 92.8 \\
\cmark & \xmark & \cmark & \cmark & \cmark & 94.1$\pm0.1$ & 91.6$\pm0.1$  & &  &  &  &  & & \textbf{92.9} \\
\cmark & \cmark & \cmark & \cmark & \cmark & \textbf{94.2}$\pm0.1$ & \textbf{91.7}$\pm0.1$ & &  &  &  &  &  & \textbf{92.9}\\ 
\hlineB{4}
\end{tabular}
\caption{Detailed comparison for additionally concatenating \textbf{MF} with \textbf{All}. \textbf{MF} represents the M-Flair embeddings.}
\label{tab:mflair}
\end{table*}



\end{document}
