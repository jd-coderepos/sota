\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{arxiv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{array}
\usepackage{booktabs}
 \usepackage{enumitem}
 \usepackage{float}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\usepackage[numbers]{natbib}


\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}




\newcommand{\secref}[1]{Section~\ref{sec:#1}}
\newcommand{\figref}[1]{Figure~\ref{fig:#1}}
\newcommand{\tabref}[1]{Table~\ref{tab:#1}}
\newcommand{\eqnref}[1]{\eqref{eq:#1}}
\newcommand{\thmref}[1]{Theorem~\ref{#1}}
\newcommand{\prgref}[1]{Program~\ref{#1}}
\newcommand{\clmref}[1]{Claim~\ref{#1}}
\newcommand{\lemref}[1]{Lemma~\ref{#1}}
\newcommand{\ptyref}[1]{Property~\ref{#1}}

\def\ie{{\it{i.e.,~}}}
\def\eg{{\it{e.g.,~}}}
\def\etc{{etc}}
\def\cf{{\it{cf.~}}}
\def\viz{{\it{viz.~}}}
\def\vs{{\it{vs.~}}}
\def\etal{{\it{et~al.\xspace}}}
\def\Naive{Na\"{\i}ve~}
\def\naive{na\"{\i}ve~}


\newcommand {\cg}[1]{{\color{blue}\textbf{Changxin: }#1}\normalfont}

\arxivfinalcopy 
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\begin{document}

\title{PLIP: Language-Image Pre-training for Person Representation Learning}

\author{Jialong Zuo\quad
		 Changqian Yu\quad
		Nong Sang\quad
		Changxin Gao\quad \\
Key Laboratory of Image Processing and Intelligent Control, School of Artificial Intelligence\\ and Automation, Huazhong University of Science and Technology 
        \\ Meituan  \\
		{\tt\small \{jlongzuo, cgao\}@hust.edu.cn} \quad 
	}

\maketitle
\ifarxivfinal\thispagestyle{empty}\fi


\begin{abstract}
  Pre-training has emerged as an effective technique for learning powerful person representations. Most existing methods have shown that pre-training on pure-vision large-scale datasets like ImageNet and LUPerson has achieved remarkable performance. However, solely relying on visual information, the absence of robust explicit indicators poses a challenge for these methods to learn discriminative person representations. Drawing inspiration from the intrinsic fine-grained attribute indicators of person descriptions, we explore introducing the language modality into person representation learning. To this end, we propose a novel language-image pre-training framework for person representation learning, termed PLIP. To explicitly build fine-grained cross-modal associations, we specifically design three pretext tasks, \ie semantic-fused image colorization, visual-fused attributes prediction, and vision-language matching. In addition, due to the lack of an appropriate dataset, we present a large-scale person dataset named SYNTH-PEDES, where the Stylish Pedestrian Attributes-union Captioning method is proposed to synthesize diverse textual descriptions. We pre-train PLIP on SYNTH-PEDES and evaluate our model by spanning downstream tasks such as text-based Re-ID, image-based Re-ID, and person attribute recognition. Extensive experiments demonstrate that our model not only significantly improves existing methods on all these tasks, but also shows great ability in the few-shot and domain generalization settings. The code, dataset and weights will be released at~\url{https://github.com/Zplusdragon/PLIP} .
\end{abstract}

\section{Introduction}
\begin{figure} \label{fig1}
    \centering \includegraphics[width=\linewidth]{Figure1.pdf}
    \caption{Illumination of our PLIP framework. We construct a large-scale synthetic image-text person dataset using the Stylish Pedestrian Attributes-union Captioning (SPAC) method. We then pre-train a language-image model by three pretext tasks. Finally, we transfer the model to some downstream person tasks.
    }
\end{figure}

Learning powerful person representations is a fundamental problem for person understanding tasks, such as image/text-based person re-identification (Re-ID) and person attribute recognition. Existing works~\cite{MGN,ABDNet,LGUR,tipcb} have shown that pre-training on ImageNet~\cite{imagenet} is able to learn rich representations, resulting in a remarkable performance on person understanding. However, due to the big domain gap between generic images and person images, it is suboptimal to learn person specific representations and results in a limited improvement. To address this problem, some studies~\cite{LUP,LUPnl} attempt to learn the person representations directly from person images at a large scale. Although these methods have proven to be effective in image-based person Re-ID, two issues arise when it comes to person understanding.
\textit{i)} Learning person representations without description indicators is prone to ignore the explicit fine-grained attributes, such as blue hat, white shirt, \etc. These fine-grained attributes naturally provide distinctive information for person Re-ID. \textit{ii)} These methods are specifically designed for image-based person Re-ID and cannot be directly transferred to text-based person Re-ID or person attribute recognition.

Some works~\cite{CLIP,BLIP,lightningdot} have demonstrated that introducing the language modality helps to learn better visual concepts and mine discriminative features. The reason is that the language enjoys a higher information density and inherently contains fine-grained characteristics. With this motivation in mind, we explore introducing the language modality as a supervision signal for learning discriminative person representations, to help the downstream tasks, \ie image-based person Re-ID, text-based person Re-ID, and person attribute recognition, as illustrated in Figure~\ref{fig1}. To this end, we propose a new language-image pre-training framework for person representation learning, termed PLIP. It aims to enhance the interaction and fusion of information between vision and language, and establish a shared language-image feature space with discriminative learned representations. Specifically, to explicitly learn fine-grained cross-modal associations, we design three pretext tasks in PLIP: (1) \textit{Semantic-fused Image Colorization}, given a textual description, aims to restore the color information of a grayscale person image; (2) \textit{Visual-fused Attributes Prediction}, by exploiting the paired colorful images, aims to predict the masked attributes phrases in textual descriptions; (3) \textit{Vision-language Matching} aims to associate representations between vision and language. 

As is well known, the scale of a dataset is essential for learning rich representations. However, the scale of the existing datasets~\cite{SSAN,textreid} with manual textual descriptions is limited due to expensive hand-labeled annotations. Meanwhile, Xiang~\etal~\cite{VTBR} construct a large-scale dataset with textual descriptions, but the game-style pictures and coarse-grained attributes are not in line with reality. Therefore, we investigate an automatic annotation method to build a large-scale image-text person dataset effectively. Inspired by the image captioning technology~\cite{imcaption,imcaption2,clipcap}, we propose a new method named Stylish Pedestrian Attributes-union Captioning (SPAC), to synthesize diverse textual descriptions at a large scale. Using SPAC as an image captioner, we construct a new dataset named SYNTH-PEDES based on the LUPerson-NL dataset~\cite{LUPnl} and the LPW dataset~\cite{LPW}. It contains 312,321 identities, 4,791,711 images and 12,138,157 textual descriptions.

We utilize PLIP to pre-train a model on our SYNTH-PEDES dataset, and then evaluate the model on downstream person understanding tasks. The experiments show that our proposed model pushes the state-of-the-art results to a higher level on a wide range of person understanding tasks. For example, for text-based Re-ID, by applying our model on LGUR~\cite{LGUR}, we improve the Rank-1 metric on CUHK-PEDES and ICFG-PEDES by 3.98\% and 5.23\%, respectively. The key contributions of this paper can be summarized as follows:

\begin{itemize}[leftmargin=*,itemsep=2pt,topsep=0pt,parsep=0pt]
\item
 We propose a novel language-image pre-training framework for person representation learning to facilitate the cross-modal representation association. It incorporates semantic-fused image colorization, visual-fused attributes prediction and vision-language matching to a unified framework. 
 \item
 We construct a large-scale person dataset with stylish generated textual descriptions. To the best of our knowledge, it is by far the largest person dataset with image-text pairs.
 \item
 Our pre-trained model performs remarkable ability in various person understanding tasks. It is demonstrated as generic to initialize the backbones of various baselines and bring significant improvements in this community.
\end{itemize}
\section{Related Work}
\noindent
\textbf{Pre-training Methods.}
Pre-training has advanced many vision tasks, which can be categorized into two typical groups, \ie pure vision pre-traning and language-vision pre-training. The former pre-trains a model on a large-scale image dataset like ImageNet~\cite{imagenet} on a fully supervised~\cite{resnet} or self-supervised~\cite{UFLNI,mocov1,mocov2} manner.
However, pre-training on vision datasets is still limited, since only one modality is available. Accordingly, some recent works study the language-vision pre-training method. For example, CLIP~\cite{CLIP} shows great success by learning from large-scale image-text pairs.
BLIP~\cite{BLIP} improves the performance by proposing CapFilt. 

In the person-related field, it is common to leverage the backbones pre-trained on ImageNet~\cite{SSG,UMD,HC}, which ignores the domain gap. To address this, LUP~\cite{LUP} constructs a large-scale unlabeled person dataset and attempts to perform unsupervised pre-training for learning person representations. Also, LUPnl~\cite{LUPnl} attempts to learn with noisy labels, which demonstrates the effectiveness of learning from raw videos. VTBR~\cite{VTBR} proposes a semantic-based pre-training approach, which uses coarse-grained captions to learn visual representations with game-style images.
 
\noindent
\textbf{Person Understanding Tasks.}
Person understanding mainly includes three tasks. 1) \textit{Text-based Re-ID} aims to search for person images of a specific identity by textual descriptions. Existing works are divided into attention-based and attention-free methods. The former~\cite{SMAT,textreid,MG,SSAN, LGUR} attempts to establish region-text correspondences, which usually ignore the efficiency. To better align the multimodal features into a shared space, the latter usually focus on designing various objective functions~\cite{CMPM,vse++,ARL} and model structures~\cite{vitaa,dualpath}. 2) \textit{Image-based Re-ID} aims to search for person images with the same identity by given person images. Most studies of this task are based on supervised learning. The hard triplet loss~\cite{triplet1,triplet2,triplet3} and classification loss~\cite{SGGNN,DCNN} are introduced to learn a global feature. Also, some works~\cite{PABR,BPM} focus on learning a part-based feature instead. For example, Sun ~\cite{BPM} proposed to represent features as horizontal stripes and learn with separate classification losses. 3) \textit{Person attribute recognition} aims to identify the person’s attributes. Many methods~\cite{a1,a2,a3} treat this task as a multi-label classification problem, while some others~\cite{a4,a5,a6} adopt recurrent neural networks for exploring the attribute context. Also, some works~\cite{VTB,Label2Label} introduce the language modality to get better performance. 
 


\section{SYNTH-PEDES: A Large-scale Image-text Person Dataset}
Generating a description of an input image is called image captioning, which needs to generate syntactically and semantically meaningful sentences. However, in the person-related field, there is no specific work for person image captioning as far as we know. Therefore, we first propose a new method for person image captioning. Given a person image, it can generate attribute-descriptions and stylish textual-captions, which simulate the diverse perspectives that different people may have on the same person picture. Additionally, we employ prompt engineering technology to synthesize prompt-captions and propose the seed filter strategy to eliminate noises in LUPerson-NL~\cite{LUPnl}.

Based on the above methods, we build SYNTH-PEDES, the largest real person dataset with image-text pairs by far, which contains 312,321 identities, 4,791,711 images, and 12,138,157 textual descriptions. 

\subsection{Stylish Pedestrian Attributes-union Captioning}
As illustrated in Figure~\ref{fig:SPAC}, Stylish Pedestrian Attributes-union Captioning (SPAC) mainly comprises two modules, \ie a prefix encoder and a shared generator. Specifically, we use ResNet101-FPN~\cite{resnet,fpn} as the encoder and GPT2~\cite{gpt2} as the generator to capture rich person image details and generate high-quality texts.

\begin{figure}
\centering
	\includegraphics[width=\linewidth]{Figure2SPAC_model.pdf}
\caption{Architecture of Stylish Pedestrian Attributes-union Captioning. It mainly consists of a prefix encoder and a shared generator. We use it to generate descriptions from a pedestrian image.}
\label{fig:SPAC}
\end{figure}

In the existing image-text person datasets~\cite{textreid,SSAN}, many unique workers were involved in the labeling tasks. The same images usually have inconsistent-style language descriptions. Constructing image-text pairs for an image with multiple descriptions will lead to unstable training and affect the model performance. Thus, in order to replicate the stylized variations that arise from multiple workers' labels and mitigate the issue of redundant real labels, we have incorporated style encoders into our pipeline. We let the concatenated prefixes pass through different style encoders to get the prefixes of their own style and then send them to the generator for the subsequent generation.

The entire training process can be seen as an autoregressive problem. Given a dataset of paired images, attributes and captions , where  is an attribute set of image  containing six attribute descriptions, the learning goal is to generate meaningful attribute descriptions and captions from an unseen person image. The attributes and captions can be referred as a sequence of padded tokens , with maximal lengths  accordingly. 

Following recent works~\cite{clipcap,zhou2019vlp}, our key solution is to jointly train a prefix encoder and a generator. The former is to capture the semantic embeddings as the prefixes from the image, and the latter, as an autoregressive language model, is to use the prefixes to predict the next token one by one. As shown in Figure~\ref{fig:SPAC}, we first feed the input image  into the prefix encoder  and different branches  to get the  attribute-and-relation prefix embeddings:



And then the concatenated embeddings are fed into different style encoders  to get the stylized caption embeddings:

each embedding has the same dimension as a token embedding. We then concatenate the obtained embeddings to the atrribute and caption token embeddings, where  is selected from the stylized caption embeddings in turn:


Finally, we feed the embeddings  into the shared generator  to predict the attribute and caption tokens in an autoregressive fashion, using the cross-entropy loss:





Define  as a balance factor, then the overall loss  is computed as:


\subsection{Dataset Construction}
\begin{table*}[htb]
\tiny
\centering
\caption{Statistics comparison on existing popular datasets. SYNTH-PEDES is by far the largest person dataset with textual descriptions without any human annotation effort. FineGPR-C is based on game-style pictures and coarse-grained captions.}
\label{table1}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|c|c|c|c|c|c|c}
\hline 
Datasets & year &\#images &\#identities&\#descriptions&view &label type & label method  &crop size   \\
\hline 
CUHK03~\cite{CUHK03} &2014& 14,097& 1,467&-	&fix camera &identity&hand+DPM~\cite{DPM}&vary\\
Market1501~\cite{market} &2015& 32,668& 1,501&-	&fix camara &identity &hand+DPM~\cite{DPM}&128×64\\
PRW~\cite{PRW}&2016& 34,304& 932&-&fix camera &identity&hand&vary\\
DukeMTMC~\cite{dukemtmc}&2017&36,411& 1,852&-&fix camera &identity&hand&vary\\
CUHK-PEDES~\cite{textreid}&2017&40,206&13,003&80,412&fix camera &identity+description&hand&vary \\
LPW~\cite{LPW}&2018&592,438&2,731&-&fix camera &identity &hand+Detector+NN& vary\\
MSMT17~\cite{MSMT17}&2018&126,441&4,101&-&fix camera&identity&FasterRCNN~\cite{rcnn}&vary\\
SYSU30K~\cite{SYSU30K}&2020&29,606,918&30,508&-&dymamic&identity&YOLOv2~\cite{yolov2}&vary\\
ICFG-PEDES~\cite{SSAN}&2021&54,522&4,102&54,522&fix camera&identity+description&hand&vary\\
FineGPR-C~\cite{VTBR}&2021&2,028,600&1,150&2,028,600&fix camera&identity+description&attribute embed&vary\\
LUPerson-NL~\cite{LUPnl}&2022&10,683,716&433,997&-&dymamic&identity&FairMOT~\cite{fairmot}&vary\\
\hline
\textbf{SYNTH-PEDES}&2023&4,791,771&312,321&12,138,157&dymamic&identity+description&SPAC&vary\\
\hline 
\end{tabular}}
\end{table*}
To build the dataset, for the image part, we first collected person Re-ID datasets LUPerson-NL~\cite{LUPnl} and LPW~\cite{LPW} to form a large-scale and diversified dataset. However, LUPerson-NL contains a multitude of noise types that result from labeling errors. To filter them for better training, we propose a seed filter strategy, which can be seen in supplementary materials for details.

For the text part, it is noteworthy that diverse lexicons and sentence structures can be employed to convey equivalent and intricate semantics. It has been proved that in the vision-language task, the diversity of language styles is conducive to improving the model performance. Therefore, for a person image, we have generated three different styles of textual descriptions, two of which are directly generated by SPAC, and the other is generated by Attributes Prompt Engineering based on the attribute descriptions generated by SPAC. The Engineering process is shown in Figure~\ref{fig:Attributes_eng}.

\begin{figure}[tbp]
\centering \includegraphics[width=\linewidth]{Figure3_Attributes_Prompt_Engineering.pdf}
\caption{Visual example of Attributes Prompt Engineering. Given an image, we first generate the attribute descriptions based on SPAC and then embed them into the masked sentence randomly chosen from the sentence library to form a complete caption.}
\label{fig:Attributes_eng}
\end{figure}
In Table~\ref{table1}, we compare the statistics of SYNTH-PEDES with other popular person datasets. As we can see, SYNTH-PEDES is the only large real person dataset with stylish textual descriptions. It consists of over 4.7\textit{M} images, 312\textit{K} de-noised labeled identities, and 12\textit{M} textual descriptions. FineGPR-C~\cite{VTBR} is the second largest with descriptions among the listed. However, it is full of game-style pictures and coarse-grained textual descriptions. This introduces an additional domain gap and leads into limited performance for person understanding in real applications.


 \section{PLIP: Person Representation Learning }
 Based on the SYNTH-PEDES dataset, this section presents the proposed language-image pre-training framework PLIP for person representation learning via three pretext tasks, \ie semantic-fused image colorization (SIC), visual-fused attributes prediction (VAP), and vision-language matching (VLM). The whole architecture is illustrated in Figure~\ref{fig:main}.
\begin{figure*}[htb]
\centering
	\includegraphics[width=\linewidth]{Figure4Main_model.pdf}
\caption{Overview of our proposed framework incorporating a semantic-fused image colorization task, a visual-fused attributes prediction task and a vision-language matching task.}
\label{fig:main}
\end{figure*}


\subsection{Semantic-fused Image Colorization}
The SIC task aims to restore the original color information of gray images by exploiting textual descriptions. The recent work on text-based Re-ID~\cite{Lapscore} has shown that such cross-modal colorization promotes the construction of image-text association. Similar to it, the overall task can be converted into a pixel-wise regression problem.

 For a pair of a gray image and a complete textual description, we denote  as the parameters of a trainable regression model. It maps textual global embedding  and gray image extracted feature  to the output recovered color image  as a target. SIC is supervised by:

where  can be any differentiable distance function such as  distance and  represents the total number of samples in the dataset.

As illustrated in Figure~\ref{fig:main}, in the encoding stage, the input gray image is firstly fed into a feature-pyramid backbone ResNet50~\cite{CLIP,fpn}, which is as the visual encoder. Secondly, the pyramid features are up-sampled to the same scale and concatenated to produce feature . Then, we input the complete textual description to the textual encoder~\cite{bert} and adopts the pooler-out as the textual global embedding . 

In the decoding stage, the visual features and textual global embedding should be fused for colorization. Specifically, we adopt the multi-modal SE-blocks~\cite{Lapscore} as the crossmodal feature fusing module, so that text information can play a role in the image feature channels. The decode is also made up of several deconvolution layers, which are employed to restore the original channel information and predict the target.

 \subsection{Visual-fused Attributes Prediction}
The VAP task requires using original colorful images to predict the masked words in textual descriptions. For each sentence, the attribute phrases are masked to create a masked caption. In this way, the correlation between images and texts can be constructed more in depth.

Firstly, for a pair of a colorful image and a masked caption with masked words , we feed them to respective encoders to extract visual global embedding  and masked hidden states . Then, VAP can be optimized by minimizing the negative log-likelihood:

where  is the addition parameters introduced to map concatenated embeddings  to word probabilities,  and  are the total numbers of masked words and samples in the dataset, respectively.

\subsection{Vision-language Matching}
In order to further reduce the feature gap between vision and language modalities, the Cross-Modal Projection Matching (CMPM) loss~\cite{CMPM} is adopted, which can promote the representation association between multiple modalities by incorporating the cross-projection into KL divergence.

For pairs of a colorful image and a complete caption, in the image branch, the colorful images are firstly fed into the visual encoder. Then the last-stage features are all pooled to get visual global embeddings . In the text branch, the complete captions are directly fed into the textual encoder to get the pooler outs as the textual global embeddings . Finally, the VLM task can be optimized by:

then the overall multi-task loss  is computed as:

where  are hyper-parameters to control the importance of each pretext task.

\section{Experiments}
\subsection{Implementation Details}
 \noindent\textbf{Training Settings.} For pre-training, the visual encoder and textual encoder are ResNet50 with a few modifications~\cite{CLIP} and BERT~\cite{bert}, respectively. We train the model with  Geforce RTX 3090 GPUs for 70 epochs with batch size 512. The initial learning rate is 0.001 and decreased at the epoch of 30 and 50. More details can be found in our supplementary materials.
 
\noindent\textbf{Datasets.} We evaluate our method on different datasets for different downstream tasks. For text-based person Re-ID, we conduct extensive experiments on two popular pubic datasets, \ie CUHK-PEDES~\cite{textreid} and ICFG-PEDES~\cite{SSAN}. For these datasets, each image is described by one or two natural language sentences. For image-based person Re-ID, we use two popular public datasets, \ie Market1501~\cite{market} and DukeMTMC~\cite{dukemtmc}. For person attribute recognition, three large-scale datasets PETA~\cite{peta}, PA-100K~\cite{a2} and RAP~\cite{rap} are used, and also the Market1501-attributes and FineGPR dataset~\cite{VTBR} are used for the zero-shot study.

\noindent\textbf{Evaluation protocols.} For text-based person Re-ID, we adopt the standard metrics Rank-\textit{k} (\textit{k}=1,5,10) to evaluate the model performance. For image-based person Re-ID, we follow the popular evaluation metrics: the mean Average Precision (mAP) and the Cumulated Matching Characteristics top-1 (cmc1). For person attribute recognition, we adopt three evaluation metrics including accuracy (Acc), mean accuracy (mA), and F1 value (F1).

\subsection{Evaluation on Text-based Person Re-ID}
\noindent\textbf{Transfer capability.} To evaluate the transfer capability of our pre-trained model on SYNTH-PEDES, we conduct three different experiments. Firstly, to perform zero-shot retrieval, we directly evaluate the model's performance even without any extra fine-tune training. Secondly, we perform linear probing by adding a trainable linear embedding layer to each modal encoder and keeping the backbone frozen. Finally, we unfreeze all encoders and do fine-tune training. We use a simple CMPM loss~\cite{CMPM} for training without bells and whistles. We choose the state-of-the-art method LGUR~\cite{LGUR} as the baseline. From Table~\ref{tab1} we can see, the model is not only competitive with the fully supervised methods even without any specific training, but also greatly exceeds them by a large margin with just a simple fine-tune training. Specifically, the improvements are at least 3.9\% and 5.3\% in terms of Rank-1 on CUHK-PEDES and ICFG-PEDES, respectively. It shows that our pre-trained model has excellent transfer capability.

\begin{table}[t]
\centering
\caption{The results of our transfer experiments. We show the best score in bold and the second score underlined. \textit{z-s}: zero-shot setting; \textit{l-p}: linear-probing setting; \textit{f-t}: fine-tune setting.  stands for the results reproduced with public checkpoints released by the authors.}
\label{tab1}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|ccc|ccc}
\hline
\multicolumn{1}{c|}{\multirow{2}{*}{Method}} & \multicolumn{3}{c|}{CUHK-PEDES}&\multicolumn{3}{c}{ICFG-PEDES}\\
\cline{2-7}
 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10  \\
\hline
\multicolumn{1}{c|}{ViTAA~\cite{vitaa}} &55.97 &75.84 &83.52&50.98 &68.79 &75.78\\

\multicolumn{1}{c|}{SSAN~\cite{SSAN}} &61.37& 80.15 &86.73	&54.23 &72.63 &79.53\\

\multicolumn{1}{c|}{LapsCore~\cite{Lapscore}} &63.40& -& 87.80	&- &- &-\\

\multicolumn{1}{c|}{TIPCB~\cite{tipcb}} &63.63& \underline{82.82}& \underline{89.01}&54.96 &74.72 &81.89\\ 

\multicolumn{1}{c|}{LGUR~\cite{LGUR}} &\underline{64.21}& 81.94& 87.93&\underline{57.42} &74.97 &81.45\\ 
\hline
\multicolumn{1}{c|}{ours+\textit{z-s}} &57.73& 78.75& 85.01	&55.42 &74.77 &81.61\\

\multicolumn{1}{c|}{ours+\textit{l-p}} &61.65& 81.84& 88.50	&56.22 &\underline{76.51}&\underline{83.57}\\

\multicolumn{1}{c|}{ours+\textit{f-t}} &\textbf{68.16}&\textbf{85.56}& \textbf{91.21}	&\textbf{62.76} &\textbf{80.36} &\textbf{86.05}\\
\hline
\end{tabular}}
\end{table}

\noindent\textbf{Domain generalization.} Due to the three cross-modal pretext tasks, it could be assumed that our model is supposed to show excellent domain generalization capability. To verify this, we carry out experiments on CUHK-PEDES and ICFG-PEDES with cross-domain settings, which means the model trained on the source domain is directly deployed to the target domain. We adopt the CMPM loss between the different modal embeddings as the training target. As illustrated in Table~\ref{tab2}, our model achieves improvements by large margins when compared with all other methods. Specifically, our model outperforms LGUR by 19.4\% and 30.54\% in terms of Rank-1 metric on the  and  settings, respectively. These results demonstrate that our pre-trained model has great capability in domain generalization.
\begin{table}[t]
\centering
\caption{Comparison on domain generalization. ``C" and ``I" denote CUHK-PEDES and ICFG-PEDES, respectively.}
\label{tab2}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|ccc|ccc}
\hline
\multicolumn{1}{c|}{\multirow{2}{*}{Method}} & \multicolumn{3}{c|}{}&\multicolumn{3}{c}{}\\
\cline{2-7}
 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10  \\
\hline
\multicolumn{1}{c|}{Dual Path~\cite{dualpath}} &15.41 &29.80 &38.19&7.63 &17.14 &23.52\\

\multicolumn{1}{c|}{MIA~\cite{MG}} &19.35& 36.78 &46.42&10.93 &23.77 &32.39\\

\multicolumn{1}{c|}{SCAN~\cite{scan}} &21.27& 39.26&48.83&13.63 &28.61 &37.05\\ 

\multicolumn{1}{c|}{SSAN~\cite{SSAN}} &24.72& 43.43& 53.01	&16.68 &33.84&43.00\\

\multicolumn{1}{c|}{SSAN(adv)~\cite{SSAN}} &29.24& 49.00& 58.53	&21.07 &38.94&48.54\\

\multicolumn{1}{c|}{LGUR~\cite{LGUR}} &34.25& 52.58& 60.85	&25.44 &44.48&54.39\\
\hline
\multicolumn{1}{c|}{ours} &\textbf{53.60}&\textbf{73.26}& \textbf{80.19}&\textbf{54.08} &\textbf{76.43} &\textbf{83.76}\\
\hline
\end{tabular}}
\end{table}

\noindent\textbf{Improvement over existing methods.} Our pre-trained model can bring significant advantages to existing text-based person Re-ID methods. We conduct experiments on three baseline methods~\cite{CMPM,SSAN,LGUR} by replacing the visual encoder with different ResNet50 pre-trained models. We compare our model with supervised~\cite{resnet}/unsupervised~\cite{mocov2} ImageNet pre-trained models, contrastive language-image pre-trained model CLIP~\cite{CLIP} and LUPerson~\cite{LUP}/LUPerson-nl~\cite{LUPnl} pre-trained models. From Table~\ref{tab3}, we can see that equipped with our pre-trained model, all the baseline methods achieve higher accuracy on each dataset. Note that even though the latter three pre-train models are either task-specifically designed for person Re-ID tasks or use much larger image-text pairs training datasets, our model still outperforms them by a considerable margin. It demonstrates that our language-image pre-training framework and image-text person dataset enjoy superiority.

\begin{table}[t]
\centering
\caption{Comparison on three baseline methods by using different pre-trained models. All results are shown in Rank-1/Rank-10. : to ensure fairness, we replace the MobileNet and LSTM in CMPM/C with a ResNet50 and BERT. }
\label{tab3}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|ccc}
\hline
\multicolumn{1}{c|}{}&\multicolumn{1}{c|}{Pre-train} & CMPM/C~\cite{CMPM} & SSAN~\cite{SSAN} & LGUR~\cite{LGUR}\\
\hline
\multicolumn{1}{c|}{\multirow{6}{*}{\rotatebox{90}{CUHK-PEDES}}}
&{IN sup.} &54.81/83.22 &61.37/86.73& 64.21/87.93\\
&{IN unsup.} &55.34/83.76& 61.97/86.63& 65.33/88.47\\
&{CLIP} &55.67/83.82& 62.09/86.89& 64.70/88.76\\
&{LUP} &57.21/84.68 &63.91/88.36 &65.42/89.36\\
&{LUPnl} &57.35/84.77& 63.71/87.46 &64.68/88.69 \\
&{ours} &\textbf{69.23/91.16}& \textbf{64.91/88.39}& \textbf{67.22/89.49}\\ 
\hline
\multicolumn{1}{c|}{\multirow{6}{*}{\rotatebox{90}{ICFG-PEDES}}}
&{IN sup.} &47.61/75.48 &54.23/79.53&57.42/81.45 \\
&{IN unsup.} &48.34/75.66& 55.27/79.64& 59.90/82.94 \\
&{CLIP} &48.12/75.51& 53.58/78.96& 58.35/82.02\\
&{LUP} &50.12/76.23 &56.51/80.41& 60.33/83.06\\
&{LUPnl} &49.64/76.15& 55.59/79.78 &60.25/82.84\\
&{ours} &\textbf{64.25/86.32}& \textbf{60.12/82.84}&\textbf{62.27/83.96}\\ 
\hline
\end{tabular}}
\end{table}

\noindent\textbf{Comparison with state-of-the-art methods.}
In Table~\ref{tab4}, we compare our results with existing state-of-the-art text-based person Re-ID methods on each dataset. The results verify the remarkable improvements brought by our pre-trained model. By simply applying our model on LGUR~\cite{LGUR} and CMPM/C~\cite{CMPM}, we achieve state-of-the-art performance on all datasets and leave all existing methods far behind. Specifically, we outperform LGUR by 3.98\% and 5.23\% in terms of Rank-1 on each dataset respectively, even though LGUR adopts a stronger backbone DeiT. The results demonstrate that model initialization plays a key role in text-based person Re-ID training.

\begin{table}[t]
\centering
\caption{Comparison with the state-of-the-art methods on text-based person Re-ID. We show the best score in bold and the second score underlined.}
\label{tab4}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|ccc|ccc}
\hline
\multicolumn{1}{c|}{\multirow{2}{*}{Method}} & \multicolumn{3}{c|}{CUHK-PEDES}&\multicolumn{3}{c}{ICFG-PEDES}\\
\cline{2-7}
 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10  \\
\hline
\multicolumn{1}{c|}{GNA-RNN~\cite{textreid}} &19.05 &- &53.64&- &- &-\\

\multicolumn{1}{c|}{Dual Path~\cite{dualpath}} &44.40& 66.26 &75.07&38.99&59.44&68.41\\

\multicolumn{1}{c|}{CMPM/C~\cite{CMPM}} &49.37& -& 79.27	&43.51 &65.44 &74.26\\
 
\multicolumn{1}{c|}{MIA~\cite{MG}} &53.10 &75.00 &82.90&46.49&67.14&75.18\\ 

\multicolumn{1}{c|}{CMPM/C~\cite{CMPM}} &54.81& 75.57& 83.22	&47.61 &68.25 &75.48\\

\multicolumn{1}{c|}{TDE~\cite{TDE}} &55.25&77.46 &84.56&-&-&-\\ 

\multicolumn{1}{c|}{SCAN~\cite{scan}} &55.86 &75.97 &83.69&50.05&69.65&77.21\\

\multicolumn{1}{c|}{ViTAA~\cite{vitaa}} &55.97&75.84 &83.52&50.98&68.79&75.78\\

\multicolumn{1}{c|}{CMAAM~\cite{CMAAM}} &56.68 &77.18& 84.86&-&-&-\\

\multicolumn{1}{c|}{HGAN~\cite{HGAN}} &59.00& 79.49& 86.62&-&-&-\\

\multicolumn{1}{c|}{NAFS~\cite{NAFS}} &59.94& 79.86& 86.70&-&-&-\\

\multicolumn{1}{c|}{MGEL~\cite{MGEL}} &60.27 &80.01 &86.74&-&-&-\\

\multicolumn{1}{c|}{SSAN~\cite{SSAN}} &61.37 &80.15 &86.73&54.23&72.63&79.53\\

\multicolumn{1}{c|}{LapsCore~\cite{Lapscore}} &63.40 &- &87.80&-&-&-\\

\multicolumn{1}{c|}{TIPCB~\cite{tipcb}} &63.63& 82.82& 89.01&54.96 &74.72 &81.89\\ 

\multicolumn{1}{c|}{LGUR~\cite{LGUR}} &64.21 &81.94 &87.93&57.42 &74.97 &81.45\\ 

\multicolumn{1}{c|}{LGUR(DeiT)~\cite{LGUR}} &65.25 &83.12 &89.00&59.02 &75.32 &81.56\\ 
\hline

\multicolumn{1}{c|}{ours+LGUR} & \underline{67.22}  &\underline{84.47}  &\underline{89.49}	&\underline{62.27} & \underline{78.28}  &\underline{83.96}\\

\multicolumn{1}{c|}{ours+CMPM/C} &\textbf{69.23}  &\textbf{85.84}  &\textbf{91.16}	&\textbf{64.25} &\textbf{80.88}&  \textbf{86.32}\\
\hline
\end{tabular}}
\end{table}

\subsection{Evaluation on Image-based Person Re-ID}

\noindent\textbf{Comparison on few-shot settings.} In real applications, it is expensive to collect a large labeled Re-ID dataset. For this reason, we further study how our pre-trained model improves the performance when the target dataset is of a few labels. Particularly, as the few-shot settings, we follow the same protocols proposed by \cite{LUP} and compare different pre-trained models~\cite{resnet,mocov2,LUP,LUPnl} with MGN~\cite{MGN} as the baseline method on the two target datasets: Market1501 and DukeMTMC. From Table~\ref{tab5}, we can see that, by varying the percentage from 0\% to 90\%, our pre-trained model significantly improves the performance of the baseline method in most cases, note that the metrics of 'IN sup' and 'IN unsup' are directly cited from \cite{LUPnl}. Most importantly, even without any labeled data from the datasets, our pre-trained model achieves much higher accuracy than the supervised and unsupervised pre-trained model on ImageNet with 10\% labeled data. This eye-catching result demonstrates that our pre-trained model has an exciting prospect in real-world applications. Even if LUP and LUPnl adopt more specific image-based person Re-ID pre-training strategy and use larger-scale training data, our model still shows strong competitiveness compared to it, especially in the small percentage few-shot settings.

\begin{table}[t]
\centering
\huge
\caption{Comparison for few-shot settings on Market1501 and DukeMTMC in terms of mAP/cmc1, respectively. ``IN sup" and ``IN unsup" refer to supervised and unsupervised pre-trained model on ImageNet, respectively.}
\label{tab5}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|cccccc}
\hline
\multicolumn{1}{c|}{\multirow{2}{*}{}}&\multicolumn{1}{c|}{\multirow{2}{*}{Pre-train}} & \multicolumn{6}{c}{few-shot}\\
\cline{3-8}

 \multicolumn{1}{c|}{}&\multicolumn{1}{c|}{} & 0\% & 10\%  & 30\%  & 50\%  &70\% &90\%\\
\hline
\multicolumn{1}{c|}{\multirow{5}{*}{\rotatebox{90}{Market1501}}}
&{IN sup.} &- &21.1/41.8& 68.1/87.6& 80.2/92.8& 84.2/94.0& 86.7/94.6\\
&{IN unsup.} &-& 18.6/36.1& 69.3/87.8& 78.3/90.9& 84.4/94.1 &87.1/95.2\\
&{LUP} &- &26.4/47.5 &78.3/92.1& 84.2/93.9& 88.4/95.5& 90.4/96.3\\
&{LUPnl} &-& 42.0/61.6 &83.7/94.0 &88.1/95.2 &90.5/96.3 &91.6/96.4\\
&{ours} &59.7/80.4& 73.2/89.5&82.6/93.4 & 86.4/94.7  &88.1/95.4 &90.3/96.2\\ 
\hline
\multicolumn{1}{c|}{\multirow{5}{*}{\rotatebox{90}{DukeMTMC}}}
&{IN sup.} &- &31.5/47.1 &65.4/79.8 &73.9/85.7 &77.2/87.8 &79.1/88.8\\
&{IN unsup.} &-& 32.4/48.0& 65.3/80.2 &73.7/85.1& 77.7/87.8& 79.4/89.0\\
&{LUP} &- &35.8/50.2& 72.3/83.8& 77.7/87.4& 80.8/89.2 &82.0/90.6\\
&{LUPnl} &-& 52.2/64.1 &77.7/87.9& 81.1/89.6& 83.2/91.1& 84.1/91.3\\
&{ours} &41.2/61.9& 56.3/67.1& 74.7/85.1  & 77.3/87.0&79.6/88.3&81.2/89.8\\ 
\hline
\end{tabular}}
\end{table}
\noindent\textbf{Improvement over existing methods.} 
We compare our results with existing popular methods on Market1501 and DukeMTMC by replacing the visual encoder. Any post-processing techniques like Re-rank~\cite{rerank} or methods relying on stronger backbones are excluded for a fair comparison. To verify the effectiveness of pre-training from person images, We only compare the methods adopting the backbone pre-trained on ImageNet. As illustrated in Table~\ref{tab6}, for all the four baselines~\cite{BOT,MGN,ABDNet,BDB}, our model improves their performance on each dataset by a large margin. Particularly, by applying on ABD-Net~\cite{ABDNet}, it beats all other methods, which shows that large-scale cross-modal pre-training helps to learn better visual person representations.

\begin{table}[t]
\tiny
\centering
\caption{Comparison with state-of-the-art methods on image-based person Re-ID. The results of MGN are from a re-implementation based on FastReID. We show the best score in bold and the second score underlined.}
\label{tab6}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|cc|cc}
\hline
\multicolumn{1}{c|}{\multirow{2}{*}{Method}} & \multicolumn{2}{c|}{Market1501}&\multicolumn{2}{c}{DukeMTMC}\\
\cline{2-5}
 & mAP & cmc1 & mAP & cmc1  \\
\hline
\multicolumn{1}{c|}{MGN~\cite{MGN}(2018)} &87.5 &95.1 &79.4&89.0 \\

\multicolumn{1}{c|}{SONA~\cite{SONA}(2019)} &88.8& 95.6 &78.3&89.4\\

\multicolumn{1}{c|}{BDB~\cite{BDB}(2019)} &86.7 &95.3 &76.0&89.0 \\ 

\multicolumn{1}{c|}{BOT~\cite{BOT}(2019)} &85.9 &94.5 &76.4&86.4 \\ 

\multicolumn{1}{c|}{MHN~\cite{MHN}(2019)} &85.0 &95.1 &77.2&89.1\\ 

\multicolumn{1}{c|}{OSNet~\cite{OSNet}(2019)} &84.9 &94.8 &73.5&88.6\\

\multicolumn{1}{c|}{ABDNet~\cite{ABDNet}(2019)} &88.3 &95.6 &78.6&89.0\\

\multicolumn{1}{c|}{GCP~\cite{GCP}(2020)} &88.9 &95.2 &78.6&87.9\\

\multicolumn{1}{c|}{SAN~\cite{SAN}(2020)} &88.0 &96.1 &75.5&87.9\\

\multicolumn{1}{c|}{ISP~\cite{ISP}(2020)} &88.6 &95.3 &80.0&89.6\\

\multicolumn{1}{c|}{CAL~\cite{CAL}(2021)} &87.0 &94.5 &76.4&87.2\\
\multicolumn{1}{c|}{BPB~\cite{BPB}(2023)} &87.0&95.1 &78.3 &89.6 \\
\hline
\multicolumn{1}{c|}{ours+BOT} & 88.0  &95.1  &77.0	&86.5  \\

\multicolumn{1}{c|}{ours+MGN} & \underline{90.6}  &\underline{96.3}  &\textbf{81.7}	&\underline{90.3}\\

\multicolumn{1}{c|}{ours+ABD-Net} &\textbf{91.2}  &\textbf{96.7}  &\underline{81.6} &\textbf{90.9} \\

\multicolumn{1}{c|}{ours+BDB} &88.4  &95.7  &78.2	&89.8 \\
\hline
\end{tabular}}
\end{table}

\subsection{Evaluation on Person Attribute Recognition}
\noindent\textbf{Improvement over existing methods.}
Our model can also bring considerable improvement to person attribute recognition methods. We conduct experiments using four representative baseline methods~\cite{a1,rethinking,VTB,Label2Label} by comparing the performance gain between the default settings and using our pre-trained model. We report the results in Table~\ref{tab7}, where the former two methods are based on traditional CNN structure and multi-label classification loss, and the latter two introduce powerful language models to get better performance. As we can see, our model improves their performance significantly on the three popular datasets. Particularly, in terms of \textit{mA}, the average improvements of these methods are 2.0\%, 2.1\%, and 2.2\% on PETA, PA-100K, and RAP respectively. Even though Label2Label is a state-of-the-art method with extremely high performance, our model still benefits it with a considerable margin.
\begin{table}[t]
\centering
\caption{Improving four person attribute recognition baseline methods, with results shown in \textit{mA/F1}. The results of DeepMAR are from a re-implementation by replacing the backbone with ResNet50, which are much better than the original. The underlined results are not shown in the original paper, which are re-produced by the official code.}
\label{tab7}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c|ccc}
\hline
\multicolumn{1}{c|}{}&\multicolumn{1}{c|}{Method} & PETA & PA-100K&RAP\\
\hline
\multicolumn{1}{c|}{\multirow{4}{*}{\rotatebox{90}{DEFAULT}}}
&{DeepMAR~\cite{a1}} & 80.14/83.56& 78.28/84.32& 76.81/78.94 \\
&{Rethinking~\cite{rethinking}}  & 83.96/86.35& 80.21/87.40& 79.27/79.95\\
&{VTB~\cite{VTB}} & \underline{84.12/86.63}& 81.02/87.31& 81.43/80.63\\
&{Label2Label~\cite{Label2Label}}  & \underline{84.08/86.57}& 82.24/87.08& \underline{81.82/80.93}\\
\hline
\multicolumn{1}{c|}{\multirow{4}{*}{\rotatebox{90}{+OURS}}}
&{DeepMAR~\cite{a1}}  & 82.46/85.87& 80.33/87.24& 78.96/80.12 \\
&{Rethinking~\cite{rethinking}} & 85.56/87.63& 82.09/88.12& 81.87/81.53\\
&{VTB~\cite{VTB}}  & 86.03/\textbf{88.14}& 83.24/88.57& 83.64/\textbf{81.78}\\
&{Label2Label~\cite{Label2Label}} & \textbf{86.12}/88.08& \textbf{84.36/88.63}& \textbf{83.77}/81.49\\
\hline
\end{tabular}}
\end{table}

\noindent\textbf{Zero-shot and Natural distribution shift.}
PLIP is pre-trained to map the cross-modal features into a shared feature space. To perform zero-shot person attribute recognition, we preprocess the attribute labels to produce a textual sentence, as shown in Figure~\ref{fig:label2sentence} below. Then, we use the sentences of different attributes as the set of potential predictions and predict the most probable one for an image. This can be accomplished by computing the feature embeddings' distance of the image and the potentials. We conduct experiments on Market1501 and FineGPR with a popular open source as the baseline. The latter used for natural distribution shift is full of game-style images and we process it to maintain the consistency of dataset size. Without any training, we directly make zero-shot attributes prediction via upon approaches. Then, to further prove the transferability of our model, we fine-tune it with just a litter training, which quickly achieves the performance matching with the baseline. These results are very appealing because it can be developed for open-vocabulary attribute recognition.

\begin{figure}[htb]
\centering \includegraphics[width=\linewidth]{Fig5.pdf}
\caption{Results of the zero-shot experiments and the illustration of processing the labels to sentences. To meet the practical application, the metrics of upcolor and downcolor are tested by multi-classification problem rather than two-classification problem. }
\label{fig:label2sentence}
\end{figure}

\subsection{Ablation Study}
 We have investigated the efficacy of different pretext tasks in PLIP through ablation experiments. To save training expenses, we divide the sub-dataset from SYNTH-PEDES for pre-training, which has 10,000 identities and 353,617 image-text pairs. After pre-train, we directly perform zero-shot retrieval on CUHK-PEDES and Market1501 to investigate the generalizability of models through different task settings. As shown by Table~\ref{tab9}, each pretext task facilitates the model’s generalizability and combining all the tasks leads to the best performance.
\begin{table}[t]
\Large
\centering
\caption{Ablation study of PLIP by zero-shot retrieval.}
\label{tab9}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|ccc|ccc|ccc}
\hline 
\multicolumn{1}{c|}{\multirow{2}{*}{\#}} & \multicolumn{3}{c|}{Components}&\multicolumn{3}{c|}{CUHK-PEDES}&\multicolumn{3}{c}{Market1501}\\
\cline{2-10}
& VLM &SIC &VAP&R@1&R@5&R@10&cmc1&cmc5&cmc10      \\
\hline 
1 &\checkmark& & &21.1	&43.1&52.8	&48.9&68.3	&76.8 \\
2 &\checkmark& \checkmark& &22.7	&44.6&54.9	&51.7&70.3	&78.4 \\
3&\checkmark& &\checkmark&22.4	&44.2&54.3	&49.0&69.2	&77.5 \\
4&&\checkmark&\checkmark&-	&-&-	&38.5&59.6	&68.6 \\
5&\checkmark&\checkmark&\checkmark&\textbf{23.4}	&\textbf{45.4}&\textbf{56.1}	&\textbf{52.6}&\textbf{71.3}	&\textbf{78.9} \\
\hline 
\end{tabular}}
\end{table}

\section{Conclusion}
In this paper, we propose a novel language-image pre-training framework for learning discriminative person representations and build a large-scale image-text person dataset SYNTH-PEDES by image captioning technology. Equipped with our pre-trained models without bells and whistles, we push many existing methods to a higher level. Also, without any specific dataset training, our model can be competitive with many fully-supervised methods. All the experiments show the great versatility and transferability of our proposed approach in a wide range of person understanding tasks.




\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{CMAAM}
Surbhi Aggarwal, Venkatesh~Babu Radhakrishnan, and Anirban Chakraborty.
\newblock Text-based person search via attribute-aided matching.
\newblock In {\em Proceedings of the IEEE/CVF winter conference on applications
  of computer vision}, pages 2617--2625, 2020.

\bibitem{UMD}
Zechen Bai, Zhigang Wang, Jian Wang, Di Hu, and Errui Ding.
\newblock Unsupervised multi-source domain adaptation for person
  re-identification.
\newblock In {\em IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, pages 12914--12923, 2021.

\bibitem{MHN}
Binghui Chen, Weihong Deng, and Jiani Hu.
\newblock Mixed high-order attention network for person re-identification.
\newblock In {\em IEEE International Conference on Computer Vision}, pages
  371--381, 2019.

\bibitem{ABDNet}
Tianlong Chen, Shaojin Ding, Jingyi Xie, Ye Yuan, Wuyang Chen, Yang Yang, Zhou
  Ren, and Zhangyang Wang.
\newblock Abd-net: Attentive but diverse person re-identification.
\newblock In {\em IEEE International Conference on Computer Vision}, pages
  8351--8361, 2019.

\bibitem{SMAT}
Tianlang Chen, Chenliang Xu, and Jiebo Luo.
\newblock Improving text-based person search by spatial matching and adaptive
  threshold.
\newblock In {\em IEEE Winter Conference on Applications of Computer Vision},
  pages 1879--1887. IEEE, 2018.

\bibitem{triplet1}
Weihua Chen, Xiaotang Chen, Jianguo Zhang, and Kaiqi Huang.
\newblock Beyond triplet loss: a deep quadruplet network for person
  re-identification.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition},
  pages 403--412, 2017.

\bibitem{mocov2}
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.
\newblock Improved baselines with momentum contrastive learning.
\newblock {\em arXiv preprint arXiv:2003.04297}, 2020.

\bibitem{tipcb}
Yuhao Chen, Guoqing Zhang, Yujiang Lu, Zhenxing Wang, and Yuhui Zheng.
\newblock Tipcb: A simple but effective part-based convolutional baseline for
  text-based person search.
\newblock {\em Neurocomputing}, 494:171--181, 2022.

\bibitem{VTB}
Xinhua Cheng, Mengxi Jia, Qian Wang, and Jian Zhang.
\newblock A simple visual-textual baseline for pedestrian attribute
  recognition.
\newblock {\em IEEE Transactions on Circuits and Systems for Video Technology},
  32(10):6994--7004, 2022.

\bibitem{BDB}
Zuozhuo Dai, Mingqiang Chen, Xiaodong Gu, Siyu Zhu, and Ping Tan.
\newblock Batch dropblock network for person re-identification and beyond.
\newblock In {\em IEEE International Conference on Computer Vision}, pages
  3691--3701, 2019.

\bibitem{imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition},
  pages 248--255. Ieee, 2009.

\bibitem{peta}
Yubin Deng, Ping Luo, Chen~Change Loy, and Xiaoou Tang.
\newblock Pedestrian attribute recognition at far distance.
\newblock In {\em ACM International Conference on Multimedia}, pages 789--792,
  2014.

\bibitem{bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186,
  Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

\bibitem{SSAN}
Zefeng Ding, Changxing Ding, Zhiyin Shao, and Dacheng Tao.
\newblock Semantically self-aligned network for text-to-image part-aware person
  re-identification.
\newblock {\em arXiv preprint arXiv:2107.12666}, 2021.

\bibitem{vse++}
Fartash Faghri, David~J Fleet, Jamie~Ryan Kiros, and Sanja Fidler.
\newblock Vse++: Improving visual-semantic embeddings with hard negatives.
\newblock {\em arXiv preprint arXiv:1707.05612}, 2017.

\bibitem{DPM}
Pedro~F Felzenszwalb, Ross~B Girshick, David McAllester, and Deva Ramanan.
\newblock Object detection with discriminatively trained part-based models.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  32(9):1627--1645, 2009.

\bibitem{LUP}
Dengpan Fu, Dongdong Chen, Jianmin Bao, Hao Yang, Lu Yuan, Lei Zhang, Houqiang
  Li, and Dong Chen.
\newblock Unsupervised pre-training for person re-identification.
\newblock In {\em IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, pages 14750--14759, 2021.

\bibitem{LUPnl}
Dengpan Fu, Dongdong Chen, Hao Yang, Jianmin Bao, Lu Yuan, Lei Zhang, Houqiang
  Li, Fang Wen, and Dong Chen.
\newblock Large-scale pre-training for person re-identification with noisy
  labels.
\newblock In {\em IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, pages 2476--2486, 2022.

\bibitem{SSG}
Yang Fu, Yunchao Wei, Guanshuo Wang, Yuqian Zhou, Honghui Shi, and Thomas~S
  Huang.
\newblock Self-similarity grouping: A simple unsupervised cross domain
  adaptation approach for person re-identification.
\newblock In {\em IEEE International Conference on Computer Vision}, pages
  6112--6121, 2019.

\bibitem{NAFS}
Chenyang Gao, Guanyu Cai, Xinyang Jiang, Feng Zheng, Jun Zhang, Yifei Gong, Pai
  Peng, Xiaowei Guo, and Xing Sun.
\newblock Contextual non-local alignment over full-scale representation for
  text-based person search.
\newblock {\em arXiv preprint arXiv:2101.03036}, 2021.

\bibitem{mocov1}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In {\em IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, pages 9729--9738, 2020.

\bibitem{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition},
  pages 770--778, 2016.

\bibitem{triplet2}
Alexander Hermans, Lucas Beyer, and Bastian Leibe.
\newblock In defense of the triplet loss for person re-identification.
\newblock {\em arXiv preprint arXiv:1703.07737}, 2017.

\bibitem{rethinking}
Jian Jia, Houjing Huang, Xiaotang Chen, and Kaiqi Huang.
\newblock Rethinking of pedestrian attribute recognition: A reliable evaluation
  under zero-shot pedestrian identity setting.
\newblock {\em arXiv preprint arXiv:2107.03576}, 2021.

\bibitem{SAN}
Xin Jin, Cuiling Lan, Wenjun Zeng, Guoqiang Wei, and Zhibo Chen.
\newblock Semantics-aligned representation learning for person
  re-identification.
\newblock In {\em AAAI Conference on Artificial Intelligence}, volume~34, pages
  11173--11180, 2020.

\bibitem{scan}
Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, and Xiaodong He.
\newblock Stacked cross attention for image-text matching.
\newblock In {\em European Conference on Computer Vision}, pages 201--216,
  2018.

\bibitem{a1}
Dangwei Li, Xiaotang Chen, and Kaiqi Huang.
\newblock Multi-attribute learning for pedestrian attribute recognition in
  surveillance scenarios.
\newblock In {\em 2015 3rd IAPR Asian Conference on Pattern Recognition
  (ACPR)}, pages 111--115. IEEE, 2015.

\bibitem{rap}
Dangwei Li, Zhang Zhang, Xiaotang Chen, and Kaiqi Huang.
\newblock A richly annotated pedestrian dataset for person retrieval in real
  surveillance scenarios.
\newblock {\em IEEE transactions on image processing}, 28(4):1575--1590, 2018.

\bibitem{BLIP}
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
\newblock Blip: Bootstrapping language-image pre-training for unified
  vision-language understanding and generation.
\newblock In {\em International Conference on Machine Learning}, pages
  12888--12900. PMLR, 2022.

\bibitem{a5}
Qiaozhe Li, Xin Zhao, Ran He, and Kaiqi Huang.
\newblock Visual-semantic graph reasoning for pedestrian attribute recognition.
\newblock In {\em AAAI Conference on Artificial Intelligence}, volume~33, pages
  8634--8641, 2019.

\bibitem{textreid}
Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu Yue, and Xiaogang Wang.
\newblock Person search with natural language description.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition},
  pages 1970--1979, 2017.

\bibitem{Label2Label}
Wanhua Li, Zhexuan Cao, Jianjiang Feng, Jie Zhou, and Jiwen Lu.
\newblock Label2label: A language modeling framework for multi-attribute
  learning.
\newblock In {\em European Conference on Computer Vision}, pages 562--579.
  Springer, 2022.

\bibitem{CUHK03}
Wei Li, Rui Zhao, Tong Xiao, and Xiaogang Wang.
\newblock Deepreid: Deep filter pairing neural network for person
  re-identification.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition},
  pages 152--159, 2014.

\bibitem{fpn}
Tsung-Yi Lin, Piotr Doll{\'a}r, Ross Girshick, Kaiming He, Bharath Hariharan,
  and Serge Belongie.
\newblock Feature pyramid networks for object detection.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2117--2125, 2017.

\bibitem{a3}
Pengze Liu, Xihui Liu, Junjie Yan, and Jing Shao.
\newblock Localization guided learning for pedestrian attribute recognition.
\newblock {\em arXiv preprint arXiv:1808.09102}, 2018.

\bibitem{a2}
Xihui Liu, Haiyu Zhao, Maoqing Tian, Lu Sheng, Jing Shao, Junjie Yan, and
  Xiaogang Wang.
\newblock Hydraplus-net: Attentive deep features for pedestrian analysis.
\newblock In {\em IEEE International Conference on Computer Vision}, pages
  1--9, 2017.

\bibitem{BOT}
Hao Luo, Wei Jiang, Youzhi Gu, Fuxu Liu, Xingyu Liao, Shenqi Lai, and Jianyang
  Gu.
\newblock A strong baseline and batch normalization neck for deep person
  re-identification.
\newblock {\em IEEE Transactions on Multimedia}, 22(10):2597--2609, 2019.

\bibitem{zhou2019vlp}
Zhou Luowei, Palangi Hamid, Zhang Lei, Hu Houdong, Jason~J. Corso, and Gao
  Jianfeng.
\newblock Unified vision-language pre-training for image captioning and vqa.
\newblock {\em arXiv preprint arXiv:1909.11059}, 2019.

\bibitem{clipcap}
Ron Mokady, Amir Hertz, and Amit~H Bermano.
\newblock Clipcap: Clip prefix for image captioning.
\newblock {\em arXiv preprint arXiv:2111.09734}, 2021.

\bibitem{MG}
Kai Niu, Yan Huang, Wanli Ouyang, and Liang Wang.
\newblock Improving description-based person re-identification by
  multi-granularity image-text alignments.
\newblock {\em IEEE Transactions on Image Processing}, 29:5542--5556, 2020.

\bibitem{TDE}
Kai Niu, Yan Huang, and Liang Wang.
\newblock Textual dependency embedding for person search by language.
\newblock In {\em ACM International Conference on Multimedia}, pages
  4032--4040, 2020.

\bibitem{GCP}
Hyunjong Park and Bumsub Ham.
\newblock Relation network for person re-identification.
\newblock In {\em AAAI Conference on Artificial Intelligence}, volume~34, pages
  11839--11847, 2020.

\bibitem{CLIP}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In {\em International Conference on Machine Learning}, pages
  8748--8763, 2021.

\bibitem{gpt2}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 1(8):9, 2019.

\bibitem{CAL}
Yongming Rao, Guangyi Chen, Jiwen Lu, and Jie Zhou.
\newblock Counterfactual attention learning for fine-grained visual
  categorization and re-identification.
\newblock In {\em IEEE International Conference on Computer Vision}, pages
  1025--1034, 2021.

\bibitem{yolov2}
Joseph Redmon and Ali Farhadi.
\newblock Yolo9000: better, faster, stronger.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition},
  pages 7263--7271, 2017.

\bibitem{rcnn}
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
\newblock Faster r-cnn: Towards real-time object detection with region proposal
  networks.
\newblock {\em Advances in neural information processing systems}, 28, 2015.

\bibitem{ARL}
Nikolaos Sarafianos, Xiang Xu, and Ioannis~A Kakadiaris.
\newblock Adversarial representation learning for text-to-image matching.
\newblock In {\em IEEE International Conference on Computer Vision}, pages
  5814--5824, 2019.

\bibitem{LGUR}
Zhiyin Shao, Xinyu Zhang, Meng Fang, Zhifeng Lin, Jian Wang, and Changxing
  Ding.
\newblock Learning granularity-unified representations for text-to-image person
  re-identification.
\newblock In {\em ACM International Conference on Multimedia}, pages
  5566--5574, 2022.

\bibitem{imcaption}
Himanshu Sharma, Manmohan Agrahari, Sujeet~Kumar Singh, Mohd Firoj, and
  Ravi~Kumar Mishra.
\newblock Image captioning: a comprehensive survey.
\newblock In {\em 2020 International Conference on Power Electronics \& IoT
  Applications in Renewable Energy and its Control (PARC)}, pages 325--328.
  IEEE, 2020.

\bibitem{SGGNN}
Yantao Shen, Hongsheng Li, Shuai Yi, Dapeng Chen, and Xiaogang Wang.
\newblock Person re-identification with deep similarity-guided graph neural
  network.
\newblock In {\em European Conference on Computer Vision}, pages 486--504,
  2018.

\bibitem{BPB}
Vladimir Somers, Christophe De~Vleeschouwer, and Alexandre Alahi.
\newblock Body part-based representation learning for occluded person
  re-identification.
\newblock In {\em Proceedings of the IEEE/CVF Winter Conference on Applications
  of Computer Vision}, pages 1613--1623, 2023.

\bibitem{LPW}
Guanglu Song, Biao Leng, Yu Liu, Congrui Hetang, and Shaofan Cai.
\newblock Region-based quality estimation network for large-scale person
  re-identification.
\newblock In {\em AAAI Conference on Artificial Intelligence}, volume~32, 2018.

\bibitem{imcaption2}
Matteo Stefanini, Marcella Cornia, Lorenzo Baraldi, Silvia Cascianelli,
  Giuseppe Fiameni, and Rita Cucchiara.
\newblock From show to tell: A survey on deep learning-based image captioning.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  45(1):539--559, 2023.

\bibitem{PABR}
Yumin Suh, Jingdong Wang, Siyu Tang, Tao Mei, and Kyoung~Mu Lee.
\newblock Part-aligned bilinear representations for person re-identification.
\newblock In {\em European Conference on Computer Vision}, pages 402--419,
  2018.

\bibitem{lightningdot}
Siqi Sun, Yen-Chun Chen, Linjie Li, Shuohang Wang, Yuwei Fang, and Jingjing
  Liu.
\newblock Lightningdot: Pre-training visual-semantic embeddings for real-time
  image-text retrieval.
\newblock In {\em Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 982--997, 2021.

\bibitem{BPM}
Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, and Shengjin Wang.
\newblock Beyond part models: Person retrieval with refined part pooling (and a
  strong convolutional baseline).
\newblock In {\em European Conference on Computer Vision}, pages 480--496,
  2018.

\bibitem{MGEL}
Chengji Wang, Zhiming Luo, Yaojin Lin, and Shaozi Li.
\newblock Text-based person search via multi-granularity embedding learning.
\newblock In {\em IJCAI}, pages 1068--1074, 2021.

\bibitem{SYSU30K}
Guangrun Wang, Guangcong Wang, Xujie Zhang, Jianhuang Lai, Zhengtao Yu, and
  Liang Lin.
\newblock Weakly supervised person re-id: Differentiable graphical learning and
  a new benchmark.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  32(5):2142--2156, 2020.

\bibitem{MGN}
Guanshuo Wang, Yufeng Yuan, Xiong Chen, Jiwei Li, and Xi Zhou.
\newblock Learning discriminative features with multiple granularities for
  person re-identification.
\newblock In {\em ACM International Conference on Multimedia}, pages 274--282,
  2018.

\bibitem{a4}
Jingya Wang, Xiatian Zhu, Shaogang Gong, and Wei Li.
\newblock Attribute recognition by joint recurrent learning of context and
  correlation.
\newblock In {\em IEEE International Conference on Computer Vision}, pages
  531--540, 2017.

\bibitem{vitaa}
Zhe Wang, Zhiyuan Fang, Jun Wang, and Yezhou Yang.
\newblock Vitaa: Visual-textual attributes alignment in person search by
  natural language.
\newblock In {\em Computer Vision--ECCV 2020: 16th European Conference,
  Glasgow, UK, August 23--28, 2020, Proceedings, Part XII 16}, pages 402--420.
  Springer, 2020.

\bibitem{MSMT17}
Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.
\newblock Person transfer gan to bridge domain gap for person
  re-identification.
\newblock In {\em IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, pages 79--88, 2018.

\bibitem{Lapscore}
Yushuang Wu, Zizheng Yan, Xiaoguang Han, Guanbin Li, Changqing Zou, and
  Shuguang Cui.
\newblock Lapscore: language-guided person search via color reasoning.
\newblock In {\em IEEE International Conference on Computer Vision}, pages
  1624--1633, 2021.

\bibitem{UFLNI}
Zhirong Wu, Yuanjun Xiong, Stella~X Yu, and Dahua Lin.
\newblock Unsupervised feature learning via non-parametric instance
  discrimination.
\newblock In {\em IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, pages 3733--3742, 2018.

\bibitem{SONA}
Bryan~Ning Xia, Yuan Gong, Yizhe Zhang, and Christian Poellabauer.
\newblock Second-order non-local attention networks for person
  re-identification.
\newblock In {\em IEEE International Conference on Computer Vision}, pages
  3760--3769, 2019.

\bibitem{VTBR}
Suncheng Xiang, Zirui Zhang, Mengyuan Guan, Hao Chen, Binjie Yan, Ting Liu, and
  Yuzhuo Fu.
\newblock Vtbr: Semantic-based pretraining for person re-identification.
\newblock {\em arXiv}, pages arXiv--2110, 2021.

\bibitem{triplet3}
Ye Yuan, Wuyang Chen, Yang Yang, and Zhangyang Wang.
\newblock In defense of the triplet loss again: Learning robust person
  re-identification with fast approximated triplet loss and label distillation.
\newblock In {\em IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, pages 354--355, 2020.

\bibitem{HC}
Kaiwei Zeng, Munan Ning, Yaohua Wang, and Yang Guo.
\newblock Hierarchical clustering with hard-batch triplet loss for person
  re-identification.
\newblock In {\em IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, pages 13657--13665, 2020.

\bibitem{CMPM}
Ying Zhang and Huchuan Lu.
\newblock Deep cross-modal projection learning for image-text matching.
\newblock In {\em European Conference on Computer Vision}, pages 686--701,
  2018.

\bibitem{fairmot}
Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, and Wenyu Liu.
\newblock Fairmot: On the fairness of detection and re-identification in
  multiple object tracking.
\newblock {\em International Journal of Computer Vision}, 129:3069--3087, 2021.

\bibitem{a6}
Xin Zhao, Liufang Sang, Guiguang Ding, Jungong Han, Na Di, and Chenggang Yan.
\newblock Recurrent attention model for pedestrian attribute recognition.
\newblock In {\em AAAI Conference on Artificial Intelligence}, volume~33, pages
  9275--9282, 2019.

\bibitem{HGAN}
Kecheng Zheng, Wu Liu, Jiawei Liu, Zheng-Jun Zha, and Tao Mei.
\newblock Hierarchical gumbel attention network for text-based person search.
\newblock In {\em ACM International Conference on Multimedia}, pages
  3441--3449, 2020.

\bibitem{market}
Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, and Qi Tian.
\newblock Scalable person re-identification: A benchmark.
\newblock In {\em IEEE International Conference on Computer Vision}, pages
  1116--1124, 2015.

\bibitem{PRW}
Liang Zheng, Hengheng Zhang, Shaoyan Sun, Manmohan Chandraker, Yi Yang, and Qi
  Tian.
\newblock Person re-identification in the wild.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition},
  pages 1367--1376, 2017.

\bibitem{dualpath}
Zhedong Zheng, Liang Zheng, Michael Garrett, Yi Yang, Mingliang Xu, and Yi-Dong
  Shen.
\newblock Dual-path convolutional image-text embeddings with instance loss.
\newblock {\em ACM Transactions on Multimedia Computing, Communications, and
  Applications (TOMM)}, 16(2):1--23, 2020.

\bibitem{DCNN}
Zhedong Zheng, Liang Zheng, and Yi Yang.
\newblock A discriminatively learned cnn embedding for person reidentification.
\newblock {\em ACM transactions on multimedia computing, communications, and
  applications (TOMM)}, 14(1):1--20, 2017.

\bibitem{dukemtmc}
Zhedong Zheng, Liang Zheng, and Yi Yang.
\newblock Unlabeled samples generated by gan improve the person
  re-identification baseline in vitro.
\newblock In {\em IEEE International Conference on Computer Vision}, pages
  3774--3782, 2017.

\bibitem{rerank}
Zhun Zhong, Liang Zheng, Donglin Cao, and Shaozi Li.
\newblock Re-ranking person re-identification with k-reciprocal encoding.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition},
  pages 1318--1327, 2017.

\bibitem{OSNet}
Kaiyang Zhou, Yongxin Yang, Andrea Cavallaro, and Tao Xiang.
\newblock Omni-scale feature learning for person re-identification.
\newblock In {\em IEEE International Conference on Computer Vision}, pages
  3702--3712, 2019.

\bibitem{ISP}
Kuan Zhu, Haiyun Guo, Zhiwei Liu, Ming Tang, and Jinqiao Wang.
\newblock Identity-guided human semantic parsing for person re-identification.
\newblock In {\em European Conference on Computer Vision}. Springer, 2020.

\end{thebibliography}
\end{document}