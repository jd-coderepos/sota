\documentclass[final]{siamltex}
\usepackage{epsfig, amsmath, amssymb}\usepackage[colorlinks]{hyperref}
\usepackage[lined,boxed]{algorithm2e} \usepackage{multirow}
\newcommand{\mtp}{minimum -sum problem}
\newcommand{\keyw}[1]{{\bf #1}}
\newcommand{\GETS}{\leftarrow}

\newtheorem{defn}{Definition}[section]


\newcommand{\scotch}{\textsc{Scotch}}

\title{\Large Relaxation-based coarsening and multiscale graph organization}
\author{
Dorit Ron\thanks{Faculty of Mathematics and Computer Science, The Weizmann Institute of Science, \href{mailto:dorit.ron@weizmann.ac.il}{\tt dorit.ron@weizmann.ac.il}}
\and Ilya Safro\thanks{Mathematics and Computer Science Division, Argonne National Laboratory, \href{mailto:safro@mcs.anl.gov}{\tt safro@mcs.anl.gov}}
\and Achi Brandt\thanks{Faculty of Mathematics and Computer Science, The Weizmann Institute of Science, \href{mailto:achi.brandt@weizmann.ac.il}{\tt achi.brandt@weizmann.ac.il}}
}

\begin{document}

\maketitle
\begin{abstract}
In this paper we generalize and improve the multiscale organization of graphs by introducing a new measure that quantifies the ``closeness'' between two nodes. The calculation of the measure is linear in the number of edges in the graph and involves just a small number of relaxation sweeps. A similar notion of distance is then calculated and used at each coarser level. We demonstrate the use of this measure in multiscale methods for several important combinatorial optimization problems and discuss the multiscale graph organization.
\end{abstract}

\section{Introduction}

A general approach for solving many large-scale graph problems, as
well as most other classes of large-scale computational science problems, is
through multilevel (multiscale, multiresolution, etc.)
algorithms. This approach generally involves {\it coarsening}
the problem, producing from it a sequence of progressively coarser
levels (smaller, hence simpler, related problems), then
recursively using the (approximate) solution of each coarse
problem to provide an initial approximation to the solution at the
next-finer level. At each level, this initial approximation is first
improved by what we generally call ``local processing" (LP). This
is an inexpensive sequence of short steps, each involving only a few
unknowns, together covering all unknowns of that level several
times over. The usual examples of LP are few sweeps of classical
(e.g., Gauss-Seidel or Jacobi) relaxation in the case of solving a
system of equations, or a few Monte Carlo passes in
statistical-physics simulations. Following the LP, the resulting approximation
may be further improved by one or several cycles, each
using again a coarser-level approximation followed by LP, applying  them at each time to
the {\it residual} problem (the problem of calculating the {\it
error} in the current approximation). See, for example, references
\cite{Brandt:1977:MLAa,bamg1,bmr82,bmr84,brandt-renormalization,vlsicad,rs87,safro2005}.

At each level of coarsening one needs to define the set of  coarse
unknown variables and the equations (or the stochastic relations)
that they should satisfy (or the energy that they should
minimize). Each coarse unknown is defined in terms of the
next-finer-level unknowns ({\it defined}, not {\it calculated}: they  are all unknowns until the coarse level is approximately solved and the fine level is interpolated from
that solution). The following are examples:
\begin{itemize}
    \item The set of coarse unknowns can simply represent a chosen
    subset of the fine-level set.
    \item If the fine-level variables are real numbers or vectors,
    each coarse variable can represent a weighted average of
    several of them.
    \item If the fine-level variables are Ising spins (having only
    values of  or ), each coarse variable can again be an
    Ising spin, representing the {\it sign} of the sum of  several
    fine spins.
    \item A coarse variable can be defined from several fine
    variables by a stochastic process (\cite{blote96}, for example).
    \item In the case of graph problems, each node of the coarse
    graph can represent an {\it aggregate} of several fine-level
    nodes or a {\it weighted aggregate} of such nodes, that is,
    allowing each fine-level node to be split between several
    aggregates.
\end{itemize}

The choice of an adequate local processing at a fine level
and the choice of an adequate set of variables at the next-coarser level are
strongly coupled. The general guiding rule \cite{SU} is
that this pair of choices is good if (and to the extent that) a
fine-level solution can always be recovered from the corresponding
set of coarse variables by a short iterative use of a suitably
modified version of the LP. That version is called {\it compatible LP} (CLP).
Examples are compatible Monte Carlo (CMC), introduced in
\cite{brandt-renormalization}, and compatible relaxation (CR),
introduced in \cite{crs}.

The CLP, needed in several important upscaling procedures
(such as the selection of the coarse variables, the acceleration of the fine-level simulations,
and the processing of fine-level windows within coarse simulations; see
\cite{SU}) can also be used for performing the interpolation
from the coarse solution to obtain the first approximation at the
fine level. When possible, however, the construction of a more
explicit interpolation is desired in order to apply it for the direct formulation of
equations (or an energy functional) that should govern the
coarse level, as in Galerkin coarsening.

In the process of defining the set of coarse variables and
in constructing an explicit interpolation, it is important to know
how ``close"  two given fine-level variables are to each other at
the stage of switching to the coarse level. We need to know, in other words, to what extent
the value after the LP of one variable implies the value of the other.
If they are sufficiently close, they can, for example, be
aggregated to form a coarse variable.

The central issue addressed in the present article is how to
measure this ``closeness" between two variables in a system of
equations or between two nodes in a given graph. (We consider the
latter to be a special case of the former, by associating the
graph with the system formed by its Laplacian.) More generally, we
want to define the distance of one variable  from a small subset
 of {\it several} variables, in order to measure how well 
can be interpolated from  following the LP.

In classical Algebraic Multigrid (AMG), aimed at solving the
linear system

the closeness of two unknowns  and  is measured simply
by the relative size of their coupling , for example, by the quantity

(or similarly by the relative size of their coupling in some {\it
power} of ). Although this definition has worked well for
the coarsening procedures of discretized scalar elliptic
differential equations, it is not really effective, and sometimes
meaningless, for systems lacking sufficient diagonal
dominance (including many discretized {\it nonscalar} elliptic
systems). Moreover, even for systems with a fully dominant diagonal
(such as the Laplacian of a graph), the classical AMG definition
may result in wrong coarsening, for example, in graphs
with nonlocal edges (see example in Sec. \ref{algorithm}).

Instead, we propose to define the ``closeness" between two
variables exactly, by measuring how well their values are
correlated at the coarsening stage, namely, following the LP
relaxation sweeps. Since the coarse level is actually applied to
the {\it residual} system, the two variables will be considered
close if their {\it errors} have nearly the same ratio in all
relaxed vectors. We will thus create a sequence of  {\it
normalized relaxed error vectors} , each
obtained by relaxing the homogeneous system  from some
(e.g., random) start and then normalizing the result. We will then
define the {\it algebraic distance} (reciprocal of ``closeness")
between any two variables  and  as

where  in order to attach larger weights to larger differences (using usually either  or the maximum norm ()). This use of  gives a symmetric measure of how well
 can be interpolated from  or vice versa.
For the graph Laplacian (and other zero-sum ) this can
be simplified to a distance defined as


More generally, the distance of a node  from a subset  of
several nodes can similarly be defined as the deviation of the
best-fitted interpolation from  to , where the deviation
is the  norm of the vector of  errors obtained upon
applying the interpolation to our  normalized relaxed error
vectors, and the best-fitted interpolation is the one having the
minimal deviation. (This least-square interpolation is the one
introduced in bootstrap AMG (BAMG) \cite{amg} for the coarse-to-fine explicit
interpolation.)

An essential aspect of the ``algebraic distance" defined here is
that it is a crude {\it local distance}. It measures
meaningful closeness only between neighboring nodes; the closer
they are the less fuzzy is their measured distance. For nodes that
should not be considered as neighbors, their algebraic distance
just detects the fact that they are far apart; its exact value carries
no further meaning. The important point is that this crude local
definition of distance is fast to calculate and is all
that is required for the coarsening purposes. A similar notion of
distance is then similarly calculated at each coarser level.

Indeed, we argue that meaningful distances in a general graph
should, in principle, be {\it defined} (not just {\it calculated})
only in such a multiscale fashion. This essential viewpoint, and
relations to diffusion distances and spectral clustering are
discussed in Section \ref{multiscale}. In particular, we
advocate the replacement of spectral methods by AMG-like
multilevel algorithms, which are both faster and more tunable to
define better solutions to many fuzzy graph problems
(see, for example, \cite{safro2004,safro2005}).

The paper is organized as follows. The graph problems we use to
demonstrate our approach are introduced in Sec \ref{prob-def}. The
calculation of the ``algebraic distance" and its use within the
multiscale algorithm is described in Section \ref{algorithm}. Results of tests
are summarized in Section \ref{results}. Finally, the
relations of our approach to diffusion distances and spectral
clustering are discussed in Section \ref{multiscale}.





\section{Notation and problem definitions}\label{prob-def}
\par Given a weighted graph , where 
is the set of nodes (vertices) and  is the set of edges.
Denote by  the non-negative weight (coupling)
of the undirected edge  between nodes  and ; if
, then . We consider as our examples the following two optimization problems.
\subsection{Linear ordering}
Let  be a bijection

The purpose of linear ordering problems is to minimize some functional over all possible permutations . The following functional should be minimized for the {\mtp}\footnote{We use this definition for simplicity. The usual definition of the functional is , which yields the same minimization problem.} (MSP):

 In the generalized form of the problem that emerges during the multilevel solver, each vertex  is assigned with a  (or ), denoted . Given the vector of all volumes, , the task now is to minimize the cost
 
where \mbox{}; that is each vertex is positioned at its center of mass, capturing a segment on the real axis that equals its length. The original form of the problem is the special case where all the volumes are equal. In particular, we would like to concentrate on the minimum linear arrangement (where ) and the minimum 2-sum problem (M2SP) that were proven to be NP-complete in \cite{gjs,georgepothen} and whose solution can serve as an approximation for many different linear ordering problems replacing the spectral approaches \cite{safro2004,safro2005}.
\subsection{Partitioning}
\par The goal of the 2-partitioning problem is to find a partition of  into two disjoint nonempty subsets  and  such that

where  is a given {\it imbalance factor}.
\par Graph partitioning is an NP-hard problem \cite{Garey79} used in many fields of
computer science and engineering. Applications include VLSI
design, minimizing the cost of data distribution in parallel
computing and optimal tasks scheduling. Because of its practical
importance, many different heuristics (spectral~\cite{posili90}, combinatorial~\cite{keli70,fima82},
evolutionist~\cite{buiMoon96}, etc.) have been developed to
provide an approximation in a reasonable (and, one hopes, linear)
computational time. However, only the introduction of multilevel
methods for partitioning
\cite{metis,webscotch,alpert97multilevel,MeyerhenkeMonienSauerwald08new,Walshaw-AoOR-04,evo03proc,doritpart,barnard94fast,hendrickson95multilevel,kaku95a,Abou-RjeiliK06}
has really provided a breakthrough in efficiency and quality.


\section{The coarsening algorithm}\label{algorithm}
\par In the multilevel framework a hierarchy of decreasing size graphs
 is constructed. Starting from the given graph,
, we create by recursive {\it coarsening} the sequence ,
then solve the coarsest level directly, and uncoarsen the
solution back to .
\par In general, the AMG-based coarsening is interpreted as a process
of weighted aggregation of the graph nodes to define the nodes of
the next coarser graph. In weighted aggregation each node can be
divided into fractions, and different fractions belong to
different aggregates. The construction of a coarse graph from a
given one is divided into three stages. First a subset of the fine
nodes is chosen to serve as the {\it seeds} of the aggregates (the
nodes of the coarse graph). Then the rules for aggregation are
determined, thereby establishing the fraction of each nonseed
node belonging to each aggregate. Finally, the graph couplings
(or edges) between the coarse nodes are calculated. The entire
coarsening scheme is shown in Algorithm \ref{alg-gen-coarsening}.

\par The AMG-based multilevel framework for graph optimization problems is discussed, for example, in \cite{safro2005}.
In the present work we generalize the coarsening part of the AMG-based
framework. The problem-dependent solution of the coarsest level
and the uncoarsening are not changed here. They are
fully described in \cite{safro2005} and references therein.
\par The principal difference between the previous AMG-based coarsening approaches
\cite{safro2005,Hu01amultilevel,cheval-mlpartcompar}
and the new {\it relaxation-based} approach is the improved
measure, the {\it algebraic coupling},
assigned to each edge, or, more generally, between any two nodes, in the graph. The algebraic
coupling is the reciprocal of the calculated {\it algebraic distance}
introduced below.
\par {\bf Algebraic distance and coupling.} The need for an improved
measure for the graph couplings can be explained by observing the
graph depicted in Figure \ref{fig:meshexample}: one additional edge
 (connecting nodes  and ) is added to a regular twodimensional mesh. While coarsening, nodes 
and  clearly should not belong to the same aggregate unless their coupling
is much stronger than other graph couplings. However, if the
weight of  is just somewhat larger than all other graph edges, and if the
black dots are some of the seeds of the coarse aggregates (chosen
by some AMG-based criterion; see, for example, Algorithm \ref{alg-coarse-nodes}), node  will
tend to be aggregated with node , rather than with any of its neighbors.
Such a decision will create bad coarse-level approximations
in many optimization problems (e.g., linear ordering and
partitioning). Moreover, at the next-coarser levels the approximation
may further deteriorate by making similar wrong decisions, making the entire neighborhood of  close to , thereby promoting linear arrangements in which {\it many} local couplings would unnecessarily become long-range ones. To
prevent this situation we would like to have a measure that not only
evaluates the coupling between  and  according to
the {\it direct} coupling between them but also takes into
account the contribution of connections between the {\it
neighborhoods} of  and . That is, if the immediate (graph)
neighbors of  are connected to those of ,
the coupling between  and  should be enhanced, while if
's neighbors are not connected to those of , as in Figure
\ref{fig:meshexample}, a significant weakening of the
 coupling is due. This
measure will prevent possible errors while coarsening.
\begin{figure}[h]
\centering
\includegraphics[width=6cm]{algdistctrex2.pdf}
\caption{Mesh graph with an additional edge between nodes 
and . The black dots mark some of the nodes selected to serve
as the seeds of the coarse aggregates; see Algorithm \ref{alg-coarse-nodes}.}\label{fig:meshexample}
\end{figure}



\par We introduce the notion of {\it algebraic distance}, which is
based on the same set of {\it test vectors} (TVs) being used in the
bootstrap AMG (BAMG) \cite{amg}. The key new ingredient of the
adaptive BAMG setup is the use of several TVs, collectively
representing algebraically smooth error, to define the
interpolation weights. When a priori knowledge of the nature of
this error is not available, slightly relaxed random vectors are
used for this task. A set of some  low-residual TVs
 can first be obtained by relaxation.
Namely, each  is a result of  fine-level
relaxation sweeps on the homogeneous equation , starting
from a {\it random} approximation, where  is the Laplacian of
the graph. In particular, we have used a small number (usually r=10)
of Jacobi under relaxation sweeps with . That is, the
new value for each ,  (in our tests ) is

where

 being the diagonal of . The
algebraic distance from node  to node  is defined over the
 relaxed TVs by

Other definitions, such as

are also possible.
Hence, {\it only} if  is small may nodes  and  be
aggregated into the same coarse node. The algebraic coupling
between  and , , is defined as the reciprocal of
:

\begin{algorithm}
\SetLine \KwData{ , } \KwResult{coarse graph}

For every edge  derive its algebraic distance 
(\ref{alg-dis}) or (\ref{other-alg-dis}) and algebraic coupling 
(\ref{alg-coupling})\; SelectCoarseNodes( , )\; Define the
coarse graph using the matrix  in Equation (\ref{interp-mat})\;
\caption{Coarsening scheme}\label{alg-gen-coarsening}
\end{algorithm}

\par We return to the example in Figure \ref{fig:meshexample} and demonstrate the
outcome of Definition (\ref{alg-dis}) by comparing  with  min a nearest neighbor of .
We show that  will not tend to be connected to  unless  equals the sum of 's other couplings. Furthermore, we show that even if  {\it is} connected to  as a result of strong , 's other neighbors will not tend to be connected to  as well but will prefer other neighbors; hence the neighborhoods of  and  will not tend to be connected to each other.
Consider Table
\ref{tab:meshexampleDiag}. The number  of TVs is given in the leftmost column.
The number  of Jacobi relaxation sweeps
varies from  to  as shown in the second to the left column. Each of
the four columns to the right presents the (natural) logarithm of ,
averaged over 100 independent runs, for graph
couplings  when  and  are nearest neighbors, and
 or 4 as shown. The numbers in parentheses are the corresponding standard deviations.
Clearly the strength of the coupling between 
and  is relatively decreased when measured by the algebraic distance. For
instance, if the graph coupling between  and  is 1 (as are
all other couplings in the graph), then after 20 relaxation sweeps (with )
 is three times bigger than the minimum of the
(algebraic distance of the) edges to 's four nearest neighbors. Thus,
 the algebraic coupling between  and  is
{\it not} the strongest coupling of  (not even close to it),
and hence it is guaranteed that  and  will not belong to the same coarse node.
The importance of using more than just 1 TV
can be seen from the values of the standard deviations: The use of 1 TV results in standard deviations similar to the average, which means that  has
a significant chance to become negative, so  has a significant chance to be the strongest
coupling of . With 10 TVs this chance becomes much smaller, at least for .
Even with 10 TVs, however, the chance grows with
, becoming more than  roughly when .
Thus, the aggregation of  with   becomes likely. This by itself is fine and justified.
What we really need to avoid is that entire neighborhoods of  and  will, as a result, be aggregated at some coarser level.
Hence, it is important to see whether the neighbors of  will tend to be aggregated with  (and thus also with ) or will prefer their other neighbors. To see that, we calculate the (natural) logarithm of , where
 min a nearest neighbor of  other than .
As shown in Table \ref{tab:meshexampleNonDiag},  would rather be aggregated
with one of its other-than- neighbors. For example, for ,  and  out of the 100 runs, in 95  would have been connected with .
The main conclusion is that nodes  and  do not tend to be connected as long as  is smaller than the sum of all other couplings of  or of . When the coupling is of the same strength, they will be connected about half the time, but then, not less important, the neighbors of  (and similarly of ) will {\it not} tend to join them but will prefer to be connected to other nearest neighbors nodes.
Similar results are obtained when using (\ref{other-alg-dis}) to calculate .
\par With the notion of the algebraic coupling in mind, the
coarse nodes selection and the calculation of the aggregation
weights are modified as follows.


\begin{table}[h]
\begin{center}







{
\begin{tabular}{|c|c|cccc|}
\hline
& & \multicolumn{4}{c|}{ for  nearest neighbors} \\ \cline{3-6}
 &  &  &  &  &  \\
\hline
\multicolumn{1}{|c|}{\multirow{4}{*}{1}} &
\multicolumn{1}{|c|}{10} & 2.47(1.51) & 1.88(1.74) & 1.38(1.85) & 1.14(1.69)\\
\multicolumn{1}{|c|}{}                        &
\multicolumn{1}{|c|}{20} & 2.74(1.74) & 2.1(1.59) & 1.4(1.26) & 1.44(1.59) \\
\multicolumn{1}{|c|}{}                        &
\multicolumn{1}{|c|}{50} & 2.65(1.36) & 1.98(1.76) & 1.92(1.41) & 1.5(1.59) \\
\multicolumn{1}{|c|}{}                        &
\multicolumn{1}{|c|}{100}& 3.03(1.72) & 2.14(1.32) & 1.51(1.42) & 1.16(1.78) \\
\hline
\multicolumn{1}{|c|}{\multirow{4}{*}{5}} &
\multicolumn{1}{|c|}{10} & 1(0.502) & 0.628(0.416) & 0.24(0.417) & -0.0484(0.397)\\
\multicolumn{1}{|c|}{}                        &
\multicolumn{1}{|c|}{20} & 1.34(0.442) & 0.825(0.415) & 0.435(0.358) & 0.208(0.332) \\
\multicolumn{1}{|c|}{}                        &
\multicolumn{1}{|c|}{50} & 1.68(0.342) & 1.04(0.338) & 0.643(0.306) & 0.362(0.296) \\
\multicolumn{1}{|c|}{}                        &
\multicolumn{1}{|c|}{100} &1.78(0.467) & 1.06(0.392) & 0.743(0.369) & 0.396(0.359) \\
\hline
\multicolumn{1}{|c|}{\multirow{4}{*}{10}} &
\multicolumn{1}{|c|}{10} & 0.821(0.281) & 0.443(0.294) & 0.022(0.293) & -0.244(0.313) \\
\multicolumn{1}{|c|}{}                        &
\multicolumn{1}{|c|}{20} &1.09(0.268) & 0.624(0.239) & 0.298(0.235) & 0.0126(0.253) \\
\multicolumn{1}{|c|}{}                        &
\multicolumn{1}{|c|}{50} & 1.49(0.263) & 0.86(0.235) & 0.504(0.226) & 0.2(0.204) \\
\multicolumn{1}{|c|}{}                        &
\multicolumn{1}{|c|}{100} & 1.69(0.315) & 1.01(0.275) & 0.572(0.264) & 0.285(0.271) \\
\hline
\end{tabular}
}

\caption{Statistical results (over 100 runs) for the average (and, in parentheses,
the standard deviation) of , calculated with
 TVs and  Jacobi relaxation sweeps for different relative strengths of .}
\label{tab:meshexampleDiag}
\end{center}
\end{table}

\begin{table}[h]
\begin{center}
{
\begin{tabular}{|c|c|cccc|}
\hline
& & \multicolumn{4}{c|}{ for  nearest neighbors} \\ \cline{3-6}
 &  &  &  &  & \\
\hline
\multicolumn{1}{|c|}{\multirow{4}{*}{1}} &
\multicolumn{1}{|c|}{10} & 0.975(1.67) & 0.939(1.7) & 1.14(1.63) & 1.07(1.89)\\
\multicolumn{1}{|c|}{}                        &
\multicolumn{1}{|c|}{20} & 0.911(1.62) & 1.09(1.31) & 1.02(1.46) & 0.931(1.64) \\
\multicolumn{1}{|c|}{}                        &
\multicolumn{1}{|c|}{50} & 1.37(1.77) & 1.14(1.79) & 1.28(1.49) & 1.24(1.45) \\
\multicolumn{1}{|c|}{}                        &
\multicolumn{1}{|c|}{100} & 0.897(1.55) & 1.23(1.45) & 1.29(1.53) & 1.31(1.44) \\
\hline
\multicolumn{1}{|c|}{\multirow{4}{*}{5}} &
\multicolumn{1}{|c|}{10} & 0.382(0.534) & 0.482(0.428) & 0.416(0.52) & 0.587(0.487) \\
\multicolumn{1}{|c|}{}                        &
\multicolumn{1}{|c|}{20} & 0.434(0.444) & 0.472(0.366) & 0.592(0.486) & 0.663(0.458) \\
\multicolumn{1}{|c|}{}                        &
\multicolumn{1}{|c|}{50} &   0.498(0.436) & 0.755(0.53) & 0.784(0.526) & 0.813(0.455) \\
\multicolumn{1}{|c|}{}                        &
\multicolumn{1}{|c|}{100} & 0.501(0.522) & 0.746(0.544) & 0.812(0.549) & 0.816(0.535) \\
\hline
\multicolumn{1}{|c|}{\multirow{4}{*}{10}} &
\multicolumn{1}{|c|}{10} & 0.283(0.312) & 0.299(0.316) & 0.376(0.307) & 0.401(0.357) \\
\multicolumn{1}{|c|}{}                        &
\multicolumn{1}{|c|}{20} & 0.362(0.281) & 0.419(0.295) & 0.449(0.288) & 0.441(0.327) \\
\multicolumn{1}{|c|}{}                        &
\multicolumn{1}{|c|}{50} & 0.448(0.311) & 0.531(0.35) & 0.672(0.351) & 0.604(0.333) \\
\multicolumn{1}{|c|}{}                        &
\multicolumn{1}{|c|}{100} & 0.464(0.377) & 0.682(0.348) & 0.839(0.374) & 0.749(0.39) \\
\hline
\end{tabular}
}
\caption{Statistical results (over 100 runs) for the average (and, in parentheses,
the standard deviation) of  (see Figure
\ref{fig:meshexample}), calculated with
 TVs and  Jacobi relaxation sweeps for different relative strengths of .}
\label{tab:meshexampleNonDiag}
\end{center}
\end{table}

\par {\bf Seed selection.} The construction of the set of seeds  and its complement
 is guided by the principle that each -node should be
``strongly coupled" to . We will include in  nodes with
exceptionally large volume or nodes expected (if used as seeds)
to aggregate around them an exceptionally large total volume of -nodes.
We start with , hence , and then sequentially
transfer nodes from  to , as follows. As a measure of how
large an aggregate seeded by  might grow, we define its
{\it future volume} 
by

Nodes with future volume larger than  times the
average of the 's are first transferred to  as
most ``representative" (in our tests ). The insertion of
additional -nodes to  depends on a ``strength of coupling to " threshold  (in our
tests ),
as specified in Algorithm
\ref{alg-coarse-nodes}.
\begin{algorithm}
\SetLine
\KwData{, }
\KwResult{set of seeds }

\; Calculate 
(\ref{future-vol}) for each , and their average
\; nodes  with \; \;
\SetLine \ForAll { in descending order of } {
\SetNoline \lIf{ or }{ move  from  to }\; }
\caption{SelectCoarseNodes(, )}\label{alg-coarse-nodes}
\end{algorithm}
\par {\bf Coarse nodes.} Each node in the chosen set  becomes
the seed of an aggregate that will constitute one coarse-level
node. Next it is necessary to determine for each  a list
of  to which  will belong. Define {\it caliber}, ,
to be the maximal number of -points allowed in that list. The
selection we propose here is based on both measures: the graph
couplings 's and the algebraic couplings 's.
Define for each  a coarse neighborhood
. Set  to be the
maximal  in . Construct a possibly
smaller coarse neighborhood by including only nodes with strong
algebraic coupling , we use . If , then the final coarse
neighborhood  will include the first  largest
's in . If , then
. This construction of the coarse
neighborhood  of  is summarized in Algorithm
\ref{alg-interpsort2}. (In the results below we have used only  and .)
The classical AMG interpolation matrix
 (of size ) is then defined by

 represents the fraction of  that will belong to the
th aggregate.

\begin{algorithm}
\SetLine \KwData{, , } \; \; \;
\SetVline
\If{}{
 the  largest 's in
\; } \If{}{ \; } \caption{The coarse neighborhood
}\label{alg-interpsort2}
\end{algorithm}


\par {\bf Coarse graph couplings.} The coarse couplings are constructed as follows. Let  be
the ordinal number in the coarse graph of the node that represents
the aggregate around a seed whose ordinal number at the fine level
is . Following the weighted aggregation scheme used in
\cite{sharon}, the edge connecting two coarse aggregates,  and , is assigned with the weight
. The volume
of the th coarse aggregate is .



\section{Computational results}\label{results}
\par We demonstrate the power of our new relaxation-based coarsening scheme by comparing its experimental
results with those of the
classical AMG-based coarsening
for three important NP-hard optimization problems: the minimum 2-sum
((\ref{minpsumdef}) with ), the minimum linear arrangement ((\ref{minpsumdef}) with ), and the minimum 2-partitioning (\ref{part-def}) problems. In all cases
the results are obtained by taking the lightest possible
uncoarsening schemes, so that differences due to the different coarsening schemes are least blurred.
\par We have implemented and tested the new coarsening scheme by
using the linear ordering packages developed in \cite{safro2004} and in \cite{safro2003} and the \scotch{} package \cite{webscotch} on a Linux
machine. The implementation is nonparallel and has not been
optimized. The results should be considered only qualitatively and
can certainly be improved by more advanced uncoarsening.
Thus, no intensive attempt to achieve the best-known
results for the particular test sets was done. The details
regarding the uncoarsening schemes for the above problems are
given in \cite{safro2004,safro2003,cheval-mlpartcompar}.
\subsection{The minimum -sum problem}


\par We present the numerical comparison for two minimum -sum problems: the minimum 2-sum problem and the minimum linear arrangement. For these problems we have designed a full relaxation-based
coarsening solver and evaluated it on a test set of 150 graphs of
different nature, size ( and )
and properties. The test graphs are taken from
\cite{davis} and from real-life network data such as social networks, power grids, and peer-to-peer connections. Our solvers are free and can be downloaded with
detailed solutions for every graph from \cite{safroproj}.
To emphasize the difference in the minimization results
between the two coarsening schemes (the relaxation-based and the classical AMG-based schemes), we measure the results
obtained at the end of the multilevel cycle {\it before} the final local
optimization postprocessing (Gauss-Seidel relaxation and the local processing in \cite{safro2004,safro2003}),
as well as after its application. Moreover, we use small calibers, , since these demonstrate
more sharply the quality of matching between the -points and
the -points. For higher calibers it is also important to use
the adaptive BAMG scheme \cite{amg} for
calculating the interpolation weights, which is beyond the scope
of this work. Small calibers are important for maintaining the
low complexity of the multilevel framework, which is vital, for example, for hypergraphs and expanders.

\par {\bf The minimum 2-sum problem (M2SP).} A comparison of the relaxation-based and AMG-based
coarsenings with calibers 1 and 2 is presented in Figures
\ref{fig:2sum}(a) and \ref{fig:2sum}(b), respectively. Each x-axis
scale division corresponds to one graph from the test set. The
y-axis corresponds to the ratio between the average cost obtained by 100
runs of the AMG-based coarsening and the one obtained by 100 runs of
the relaxation-based coarsening. Each figure contains two curves:
the dashed curves with cost measurements before applying the postprocessing of local optimization (e.g., Gauss-Seidel
relaxation, Window Minimization \cite{safro2004}) and the regular curves with cost measurements
after adding such optimization steps.
Clearly most graphs benefit from
the relaxation-based coarsening, showing a ratio greater than 1. The ratio
decreases when more optimization is used, especially since the
Gauss-Seidel relaxation is powerful algorithmic component for this problem and thus
brings the results of the two coarsening schemes closer to each
other as is indicated by the regular curves. All the these results
were obtained with  TVs. When we lowered  to , we observed no significant change in the results. Our number of
Jacobi overrelaxation sweeps  cannot be reduced by more than twice
since this relaxation scheme is expected to evolve slowly. The detailed analysis of the convergence properties are presented in \cite{chen-safro-algdist-full}.


\begin{figure}[h]
\begin{minipage}[b]{0.5\linewidth} \centering
\includegraphics[width=7cm]{min2sum_io1_points.pdf}\\
(a) 
\end{minipage}
\hspace{0.5cm} \begin{minipage}[b]{0.5\linewidth}
\centering
\includegraphics[width=7cm]{min2sum_io2_points.pdf}\\
(b) 
\end{minipage}
\caption{Results for the minimum 2-sum problem.}\label{fig:2sum}
\end{figure}


\par {\bf The minimum linear arrangement problem.} Similarly to the previous problem, we designed a relaxation-based solver and established a series of experiments for the minimum linear arrangement problem. The experimental setup was identical to that of the M2SP. It was based on the solver designed in \cite{safro2003}. In this case we can observe even more significant improvement when employing the relaxation-based coarsening than for the M2SP.
\begin{figure}[h]
\begin{minipage}[b]{0.5\linewidth} \centering
\includegraphics[width=7cm]{min1sum-points-io1.pdf}\\
(a) 
\end{minipage}
\hspace{0.5cm} \begin{minipage}[b]{0.5\linewidth}
\centering
\includegraphics[width=7cm]{min1sum-points-io2.pdf}\\
(b) 
\end{minipage}
\caption{Results for the minimum 1-sum (linear arrangement) problem.}\label{fig:1sum}
\end{figure}
\par {\bf Which graphs are most beneficial?} It is remarkable that the most beneficial graphs in our test set come from VLSI design and general optimization problems. We know that these graphs are very irregular (compared, for example,   with finite-element graphs and with those that pose 2D/3D geometry). Thus, we may conclude that the algebraic couplings help to identify the weakness of nonlocal connections and prevent them from being aggregated. In several examples, we achieved the best known results with caliber 1, while using classical AMG-based approaches they can be achieved with bigger calibers only.


\par {\bf An algebraic coupling-based algorithm.} We have also tried a straightforward algorithm in which, during coarsening, the weights of the graph are simply replaced by their algebraic couplings. That is, in the {\bf if} statement at the end of Algorithm \ref{alg-coarse-nodes}, only the first term is taken into account (namely,  ). Similarly, in Algorithm \ref{alg-interpsort2},
 (in the first {\bf if}) is replaced by .
We present the comparison of the obtained simple algebraic couplings based coarsening scheme with the mixed scheme described in Algorithms \ref{alg-coarse-nodes} and \ref{alg-interpsort2} in Figure \ref{fig:2sum-algdist-only}. The comparison was done for the M2SP including  postprocessing (of local optimization) using the same experimental setup. The bold curve corresponds to the ratios between the classical AMG-based results and the simple algebraic coupling-based coarsening scheme. To see the difference between this algorithm and the more elaborate one, we add a copy of its results, that is, the bold curve from Figure \ref{fig:2sum}(a). The mixed version is clearly better: in about 25 more graphs the results are improved. The average improvement was .
\begin{figure}[h]
\centering
\includegraphics[width=7cm]{min2sum_algdist_only_ver2.pdf}\\
\caption{Results for the minimum 2-sum problem. Comparison of the algebraic distance based only and mixed full relaxation based algorithms.}\label{fig:2sum-algdist-only}
\end{figure}



\subsection{The minimum 2-partitioning problem}
\par We compared the relaxation-based coarsening and the classical AMG-based
 by combining two packages. The coarsening part was
 the same as in the minimum -sum problems. The
uncoarsening was based on the \scotch{} package; details of
its fastest version can be taken from \cite{cheval-mlpartcompar}.
\par The comparison of the relaxation-based and the AMG-based coarsenings with caliber
1 is presented in Figure \ref{fig:partitioning}. The
interpretation of x- and y-axes is similar to Figure
\ref{fig:2sum}.
Included are 15 graphs of different nature and size. The details
regarding the numerical results can be obtained from
\cite{safroproj}. The four best ratios are obtained for graphs with
power-law degree distributions.
\begin{figure}[h]
\centering
\includegraphics[width=7cm]{partitioning-points.pdf}
\caption{Results for the minimum 2-partitioning problem.}\label{fig:partitioning}
\end{figure}
More results for the graph and hypergraph partitioning problems are reported in \cite{chen-safro-algdist-full}. Even though the algorithm used there only substitutes the original given couplings by their algebraic couplings, it is already clear that better results are observed for most tested instances of both graphs and hypergraphs.
\subsection{Running time}
\par The implementation of stationary iterative processes and their running time are well studied issues. These topics are beyond the scope of this paper; we refer the reader to two books in which one can find discussions about sequential and parallel matrix-vector multiplications and general relaxations \cite{Grama,heath}. Typical running time of an AMG-based framework for linear ordering problems on graphs can be found in \cite{safro2003,safro2004,safro2005}. The introduction of the algebraic disctance did not increase significantly those running time estimations.

\section{Multiscale distance definition and hierarchical organization}\label{multiscale}
As mentioned in the introduction, the algebraic distance defined
above is only a crude {\it local distance}, measuring meaningful
relative distances only between neighboring nodes while also
detecting which nodes should not be considered as close neighbors.
This fuzzy local distance, which can be calculated rapidly, is
all we need for coarsening. A similar distance is then
calculated at each coarser level, thus yielding a multiscale
definition of distances through the entire graph, where at large
distances one defines the distances only between (usually large)
aggregates of nodes, not between any individual pair of distant
nodes. Such multiscale distances are not only far less expensive
to calculate: we next list several reasons why, {\it in principle},
distances in a general large graph should be {\it defined} better
in such a multiscale fashion.
\begin{itemize}
    \item At large distances the detailed individual distances (the exact travelling time from each house in Baltimore to each house in Boston, say)
    are usually not of interest.
    \item The distance in a general graph is a fuzzy notion, whose
    definition is to a certain  extent arbitrary. The difference
    between the two distances of two neighboring nodes from a
    third, far one is much less than the difference between various,
    equally legitimate distance definitions, and also far less than the
    accuracy of the graph data (e.g., its edge weights) and far
    less than the accuracy of solving the equations that define these
    distances.
    \item The most important reason: At different scales different
    factors should in principle enter into the distance
    definition. In particular, at increasingly larger distances,
    intrinsic properties of increasingly larger aggregates should
    play a progressively more important role. For example, in image
    segmentation, while at the finest level the ``closeness"  of
    two neighboring pixels (i.e., their chance to belong to the same
    segment) can be defined by their color similarity, at larger
    scales the closeness of two neighboring patches should be
    defined in terms of the similarity in their {\it average}
    color (which is different from the direct color similarity of
    neighboring pixels along the boundary between the patches) and also in
    terms of similarity in various texture measure (color variances,
    shape moments of subaggregates, average orientation of fine embedded
    edges, etc.) and other aggregative properties \cite{segm}, \cite{Nature}. Another example: in the
    problem of identifying clusters in a large set of points in
    , at the finest level the distance between data points
    can simply be their Euclidean distance, while at coarser
    levels the distance between two aggregates of points should
    also take into account similarity in terms of aggregative
    properties, such as density, orientation and  dimensionality     \cite{Dan1}.
    \item The multiscale definition of distance also brings much
    needed flexibility into the way distances at one level are
    converted into distances at coarser levels. For example, in a
    graph whose finest level consists of face images and their
    similarity scores, if at some coarse level node  is the
    union of  two fine-level nodes  and , and node
     is the union of  and , then the coarse weight
     of the edge  can be defined either as some
    {\it average} of , , , and
    , or alternatively as the {\it maximum} (or 
    average with large ) of those four weights. The former
    choice (average)  is more suitable if one wants to cluster
     faces having a {\it similar pose}, while the latter
    choice (max or ) is more suitable if we need clusters of
    images each belonging to the {\it same person} (or, generally,
    when the clustering should be based on transitive similarity).
\end{itemize}

An ingenious rigorous definition of distances in a general graph,
introduced in \cite{diffusion-maps}, is called {\it diffusion distance}. Denoting
by  the probability of a random walk on the graph
starting at  to reach  after  steps, the diffusion
distance between two nodes  and  is defined by

with some suitable choice of the node weights . This is, in fact,
a multiscale definition of distance, with the diffusion time 
serving as the scaling parameter. And indeed the definition is
used for hierarchical organizations of graphs (even though
large-scale distances are still defined in detail for any pair of
nodes). The calculation of our ``algebraic distance" can be viewed
as just a fast way to compute a crude approximation to diffusion
distances at some small .

The essential practical point is that this crude and inexpensive ``algebraic
distance" is all one needs for solving graph problems by repeated
coarsening. The calculation of the {\it diffusion map} (the diffusion
distances at various scales ) for a large graph is, on the other hand, quite expensive,
requiring computing (possibly many) eigenvectors of the graph Laplacian.
The fast way to calculate them should involve using a multiscale algorithm
such as AMG (which is likely to work well in those cases
where hierarchical organizations of the graph is meaningful; the
AMG solver can, by the way, calculate {\it many} eigenvectors for
nearly the same work of calculating only one \cite{Dan2}). However,
instead of calculating the diffusion map and then use it for
organizing the graph, {\it the AMG structure can itself be used directly,
and more efficiently for any such organization.}

Indeed, as pointed out in \cite{amg}, the same coarsening procedures used
by the AMG solver can directly be used for efficient
hierarchical organizations (such as multiscale clustering) of a graph
(as in \cite{Dan1}) or for multiscale segmentation of an image (as in \cite{segm}, \cite{Nature}). As
exemplified in this article (and also in \cite{safro2004}, \cite{safro2005}), this kind
of procedures can also be used for many other types of graph
problems, in particular, it can also be used for detecting small hidden cliques in random graphs \cite{feige-ron-2010}.

Thus, for discrete graphs, and analogously also for related
continuum field problems, although the diffusion map is a useful theoretical concept, it is often not the most practical
tool. We believe this to be true for most if not all spectral
graph methods (using eigenvectors of the graph Laplacian): The
same AMG structure that would rapidly calculate the eigenvectors can be
better used to directly address the problem at hand. As
pointed out in the discussion of multiscale distances, this can yield not just
faster solutions but also, and more important, better
definitions and more tunable treatments for many practical
problems.


\section{Conclusions}
We have proposed a new measure that quantifies the ``closeness" between two nodes in a given graph. The calculation of the measure is linear in the number of edges in the graph and involves just a small number of relaxation sweeps. The calculated measure is all that is required for coarsening purposes. A similar notion of distance is then calculated and used at each coarser level. We demonstrate the use of this new measure for the minimum (1,2)-sum
linear ordering problem and for the minimum 2-partitioning problem. The improvement in the results shows that this measure indeed detects the most important couplings in the graph and helps in producing a better coarsening, while at the same time preventing nonlocal vertices from belonging to the same coarse aggregate.


\section*{Acknowledgments}
This work was partially funded by the CSCAPES institute, a DOE project, and in part by DOE Contract DE-AC02-06CH11357. We express our gratitude to Dr. C\'edric Chevalier for providing us with a modification of Scotch software and helping to design the minimum 2-partitioning problem experiments.
\bibliographystyle{plain}
\bibliography{algdist}
\vspace*{1cm}
\hspace*{1.5in}{\scriptsize\framebox{\parbox{2.4in}{
The submitted manuscript has been created in part by UChicago Argonne, LLC, Operator of Argonne National Laboratory (``Argonne'').  Argonne, a U.S. Department of Energy Office of Science laboratory, is operated under Contract No. DE-AC02-06CH11357.  The U.S. Government retains for itself, and others acting on its behalf, a paid-up nonexclusive, irrevocable worldwide license in said article to reproduce, prepare derivative works, distribute copies to the public, and perform publicly and display publicly, by or on behalf of the Government.
}}}

\end{document} 