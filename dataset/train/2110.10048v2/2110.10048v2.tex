
\begin{table*}[!t]
	
	\centering
	\begin{tabular}	{c|c c c c}
	\hline	 	 	
			Method & Overall & Many & Medium & Few \\
			
		\hline	 
			OLTR~\cite{oltr} & 41.9  & 51.0 & 40.8 & 20.8 \\
			Focal Loss~\cite{focal}  & 43.7  & 64.3 & 37.1 & 8.2 \\

			NCM~\cite{decouple-longtail}  & 47.3 & 56.6 & 45.3 & 28.1 \\
			-norm~\cite{decouple-longtail}  & 49.4 & 59.1 & 46.9 & 30.7 \\ 
			cRT~\cite{decouple-longtail}  & 49.6 & 61.8 & 46.2 & 27.4 \\ 
			LWS~\cite{decouple-longtail} & 49.9 & 60.2 & 47.2 & 30.3 \\
			De-confound-TDE~\cite{tang2020longtailed}  & 51.8 & 62.7 & 48.8 & 31.6 \\ 
	\hline	 
			cRT~\cite{decouple-longtail}& 52.4 & 64.3  & 49.1 & 30.7 \\  
			LWS~\cite{decouple-longtail}  & 52.5 & 63.0 & 49.6 & 32.8 \\ 
			De-confound-TDE~\cite{tang2020longtailed}  & 52.4 &  63.5 &  49.2 & 32.2 \\

			ICCL (ours) & \textbf{54.0}  & 60.7 & \textbf{52.9}  & \textbf{39.0} \\
	\hline	 
	\end{tabular}

    \vspace{-1ex}
	\caption
		{\small	
		Top-1 accuracy on ImageNet-LT using ResNeXt-50.  denotes results copied from Tang \etal ~\cite{tang2020longtailed}.  denotes our reproduced results using improved settings. 
		The trade-off between head class (i.e. many) and tail class (i.e. medium and few) accuracy is adjustable without affecting the overall accuracy (see Fig.~\ref{fig:distill_tau}).
		}
	\label{tbl:imagenetlt_resnext}
\vspace{-2ex}
\end{table*}		

 
\section{Experiments}
\label{sec:experiment}
\subsection{Dataset}

We evaluate our method on three standard benchmark datasets for long-tail recognition as follows:

\textbf{CIFAR-LT.} 
CIFAR10-LT and CIFAR100-LT contain samples from the CIFAR10 and CIFAR100~\cite{cifar} dataset, respectively.
The class sampling frequency follows an exponential distribution.
Following ~\cite{bbn, ldam}, we construct LT datasets with different imbalance ratios of , , and .
Imbalance ratio is defined as the ratio of maximum to the minimum class sampling frequency. The number of training images for CIFAR10-LT with an imbalance ratio of ,  and  is k, k and k, respectively. Similarly, CIFAR100-LT has a training set size of k, k and k. Both test sets are balanced with the original size of k.

\textbf{ImageNet-LT.} The training set consists of  classes with k images sampled from the ImageNet~\cite{ImageNet} dataset.
The class sampling frequency follows a Pareto distribution with a shape parameter of ~\cite{oltr}. 
The imbalance ratio is 256.
Despite a smaller training size, it retains ImageNet~\cite{ImageNet} original test set size of k. 

\textbf{iNaturalist 2018.} It is a real-world long-tailed dataset for fine-grained image classification of  species~\cite{inat18}. We utilise the official training and test datasets composing of k training and k test images.  

\subsection{Evaluation}
For all datasets, we evaluate our models on the test sets and report the overall top-1 accuracy across all classes. 
To further access the model's accuracy on different classes, we group the classes into splits according to their number of images~\cite{oltr, decouple-longtail}: many ( images), medium ( images) and few ( images). 

\subsection{Implementation details}
For fair comparison, we follow the same training setup of previous works using SGD optimiser with a momentum of . 
For all experiments, we fix class centroid momentum coefficient , class-aware sampler adjustment parameter , , distillation weight  and distillation temperature . 
Unless otherwise specified, for the hyperparameters, we set temperature , uniform branch weight , interpolative branch weight , and MLP projection head embedding size  in the representation learning stage. 
In the classifier balancing stage, we freeze the CNN and fine-tune the classifier using the original learning rate with cosine scheduling~\cite{loshchilov2016sgdr} for  epochs.



We also design a warm-up training curriculum.
Specifically,
In the first  epochs, we train only the uniform branch using the cross-entropy loss  and a (non-interpolative) centroid contrastive loss .
After  epochs, we activate the interpolative branch and optimise  in Equation ~\ref{eq:total_loss}.
The warm-up provides good initialisation for the representations and the centroids.
 is scheduled to be approximately halfway through the total number of epochs. 

\textbf{CIFAR-LT.} We use a ResNet-32~\cite{resnet} as the CNN encoder and follow the training strategies in ~\cite{bbn}. 
We train the model for  epochs with a batch size of . 
The projected embedding size is . 
We use standard data augmentation which consists of random horizontal flip and cropping with a padding size of . 
The learning rate warms up to  within the first  epochs and decays at epoch  and  with a step size of . 
We use a weight decay of . 
We set  and  as  and  epochs for CIFAR100-LT and CIFAR10-LT, respectively.  
 is set as 0 after warm-up.
In the classifier balancing stage, we fine-tune the CNN encoder using cosine scheduling with an initial learning rate of .

\textbf{ImageNet-LT.} We train a ResNeXt-50~\cite{resnext} model for  epochs using a batch size of , a weight decay of ,
and a base learning rate of  with cosine scheduling. 
Similar to ~\cite{decouple-longtail}, we augment the data using random horizontal flip, cropping and colour jittering. 
We set .

\textbf{iNaturalist 2018.} Following ~\cite{decouple-longtail}, we train a ResNet-50 model for  epochs and  epochs using  learning rate with cosine decay,  batch size and  weight decay. The data augmentation comprises of only horizontal flip and cropping.  is set as  and  epochs for training epochs of  and , respectively.

\subsection{Results}
\begin{table*}[!t]
	
	\centering
	
	\begin{tabular}	{c| c c c  c| c c c c }
		\hline	 	 	
			\multirow{2}{*}{Method} & \multicolumn{4}{c|}{90 Epochs}  & \multicolumn{4}{c}{200 Epochs} \\
			&  Overall & Many & Medium & Few  &  Overall & Many & Medium & Few \\
			\hline	 
			CB-Focal~\cite{cbloss}                                                & 61.1 &  -    &   -   &   -   & - &  -    &   -   &   - \\
			CE-DRS~\cite{ldam}                                         & 63.6 &  -    &  -    &   -   & - &  -    &   -   &   - \\
			CE-DRW~\cite{ldam}                                    & 63.7 &   -   &  -    &  -    & - &  -    &   -   &   -  \\
			LDAM-DRW~\cite{ldam}                                                & 68.0 &   -   &    -  &  -    & - &  -    &   -   &   - \\
			LDAM-DRW~\cite{ldam}                               & 64.6 &   -   &    -  &  -    & 66.1 &   -   &    -  &  -  \\
			NCM~\cite{decouple-longtail}                                          & 58.2 & 55.5 & 57.9 & 59.3  & 63.1 & 61.0 & 63.5 & 63.3 \\
			cRT~\cite{decouple-longtail}                                                  & 65.2 & \textbf{69.0} & 66.0 & 63.2 & 68.2 & \textbf{73.2} & 68.8 & 66.1\\
			-norm~\cite{decouple-longtail}                          & 65.6 & 65.6 & 65.3 & 65.9 & 69.3 & 71.1 & 68.9 & 69.3\\
			LWS~\cite{decouple-longtail}                                              & 65.9 & 65.0 & 66.3 & 65.5 & 69.5 & 71.0 & 69.8 & 68.8\\
			BBN~\cite{bbn}                                               & 66.4 & 49.4 &  \textbf{70.8} & 65.3 & 69.7 & 61.7 & \textbf{73.6} & 66.9 \\

			ICCL (ours) &  \textbf{70.5} & 67.6 & 70.2 &  \textbf{71.6} & \textbf{72.5} & 72.1 & 72.3 & \textbf{72.9} \\

			
		\hline	 
	\end{tabular}
    \vspace{-1ex}
	\caption
		{\small	
		Top-1 accuracy on iNaturalist 2018 using ResNet-50 for 90 epochs and 200 epochs.  denotes results copied from Zhou \etal ~\cite{bbn}
		which uses 90 and 180 epochs.
        The trade-off between head class (i.e. many) and tail class (i.e. medium and few) is adjustable without affecting the overall accuracy (see Fig.~\ref{fig:distill_tau}).
		}
	\label{tbl:inat18_new}
\vspace{-2ex}
\end{table*}		
 Next we present the results,
where the proposed ICCL achieves significant improved performance on all benchmarks.

\textbf{CIFAR-LT.} 
Table~\ref{tbl:cifarlt} demonstrates that ICCL surpasses existing methods across different imbalance ratios for both CIFAR100-LT and CIFAR10-LT. 
Notably, after the representation learning stage, our approach generally achieves competitive performance compared to existing methods apart from De-confound-TDE~\cite{tang2020longtailed}.  
By balancing the classifier, the performance of ICCL further improves and outperforms De-confound-TDE by \% on the more challenging CIFAR100-LT with imbalance ratio of . 

\textbf{ImageNet-LT.} 
Table~\ref{tbl:imagenetlt_resnext} presents the ImageNet-LT results,
where ICCL outperforms the existing methods. 
For ImageNet-LT, we also propose an improved set of hyperparameters which increases the accuracy for existing methods.
Specifically, different from the original hyperparameters used in~\cite{decouple-longtail},
we use a smaller batch size of  and a learning rate of .
Furthermore, we find it is better to use original learning rate    for classifier balancing.
For fair comparison, we re-implement Decouple methods~\cite{decouple-longtail} and De-confound-TDE~\cite{tang2020longtailed} using 
our settings and obtain better accuracy than those reported in the original papers. 
However, ICCL still achieves the best overall accuracy of \% with noticeable 
accuracy gains on medium and few classes.

\textbf{iNaturalist 2018.} 
On the real-world large-scale iNaturalist 2018 dataset, ICCL achieves substantial improvements compared with existing methods as shown in Table~\ref{tbl:inat18_new}.
For  and  epochs, our method surpasses BBN~\cite{bbn} by \% and \% respectively.
We obtain the split accuracy of BBN based on the checkpoint released by the authors. 
We observe that BBN suffers from a large discrepancy of \% between the many and medium class accuracy for  epochs, 
whereas our method has more consistent accuracy across all splits.
Additionally, ICCL obtains a best overall accuracy of \% at  epochs which is better than BBN (\%) at  epochs. 


\subsection{Ablation study}

Here we perform extensive ablation study to examine the effect of each component and hyperparameters of ICCL, and provide analysis on what makes ICCL successful.
\begin{table}[!t]
\small

	\centering

	\begin{tabular}	{ c c c c |c c c }
	\hline	 	 
		     &  & &     Warm-up   &   Overall  & Head & Tail \\
	\hline	 
		 	\checkmark		& 				& 						& 		 	& 51.3       & 60.6 & 45.5\\
		 									& 		\checkmark			& 					&	& 51.6 & 58.3	 & 47.4 \\
								& 		\checkmark			& 					&	\checkmark		& 	51.7 & 57.9 &  47.9\\
			\checkmark	& 		 	& 		\checkmark			&\checkmark   	&   52.4  	       & 59.9 &  47.7\\
			\checkmark	& 		\checkmark		& 				& 	\checkmark	&  53.4             & 61.1 & 48.6\\
			\checkmark	& 		\checkmark		& 		\checkmark		&	  	&  53.6 & 61.2 & 48.8\\
			 \checkmark	& 		\checkmark		& 		\checkmark		&	\checkmark   	& 54.0    & 60.7 & 49.8\\
	\hline	 
	\end{tabular}

	
	\vspace{-1ex}
	\caption
		{\small	
		Ablation study on different components of ICCL on ImageNet-LT.
		Head denotes the many split, whereas tail includes the medium and few splits.
		The proposed , , and warm-up all contribute to accuracy improvement.
		Using only  is equivalent to Mixup~\cite{mixup}. 
		}
	\label{tbl:abt_component_new}
\vspace{-2ex}
\end{table}		
		
 \begin{table}[!t]
	\begin{minipage}[b]{0.5\linewidth}
		\centering
			\begin{tabular}	{c |c }
			\hline	 	 
			  &    CIFAR100-LT \\
		\hline	 
			(0.2, 1.0)			& 43.6 \\
			(0.2, 0.2)			& 43.8\\
			(0.6,0.6) 		&   45.4\\
			(1.0,1.0)		&     46.6\\
			(2.0, 2.0) 		&   46.8\\
		\hline	 
			\end{tabular}

	\end{minipage}\hfill
	\begin{minipage}[!b]{0.44\linewidth}
		\centering
		\includegraphics[width=38mm]{beta_dist.pdf}

	\end{minipage}
	\vspace{-2.3ex}
	\caption
		{\small	
		Effect of sampling  from different  on CIFAR100-LT.
		 determines the weighting of the two samples for a given interpolative sample.  
		}
    \label{tbl:lambda}
\vspace{-2.5ex}
\end{table}
 
\textbf{Loss components.}
For representation learning, ICCL introduces the interpolative centroid contrastive loss  and 
the interpolative cross-entropy loss  as shown in Equation ~\ref{eq:total_loss}. 
In Table~\ref{tbl:abt_component_new},
we evaluate the contribution of each loss components using ImageNet-LT dataset.
We consider many split as the head classes ( images per class), medium and few splits as the tail classes ( images per class).
We employ the same classifier balancing technique as described in Section 3.4.
We observe that both  and  improve the overall accuracy individually and collectively. 
By comparing with  only which is equivalent to Mixup~\cite{mixup}, we demonstrate that our loss formulation achieves superior performance.
Additionally, having a warm-up before incorporating interpolative losses provides an extra accuracy boost, especially for the tail classes. 
This aligns with the observation 
in ~\cite{ldam} which suggests that adopting a deferred schedule before re-sampling is better for representation learning.


\begin{table}[!t]
		\setlength\tabcolsep{4pt}
	\centering
	\resizebox{1\columnwidth}{!}{  
	\begin{tabular}	{c |c |c |c |c }
	\hline	 	 
		  Sampler &     CIFAR100-LT    & CIFAR10-LT & ImageNet-LT & iNaturalist \\
\hline	 
			Uniform & 44.7 & 79.9 & 52.8& 69.4 \\

			 & 46.6 & 82.1 & 54.0 & 70.5 \\
			 & 46.2 & 81.6 & 54.1 & 70.2\\
			 & 46.2 & 81.1 & 53.1 & 70.1 \\
	\hline	 
	\end{tabular}
}
    \vspace{-1ex}
	\caption
		{\small	
		Adjustment parameter  of the interpolative branch class-aware sampler.
		Focus excessively on head-class (uniform) or tail-class samples ()
		leads to worse performance.
		}
	\label{tbl:second_sampler_choice}
\vspace{-2.5ex}
\end{table}		


 \begin{figure*}[!t]

	\centering
	\includegraphics[width=0.8\textwidth]{weight_norm.pdf}
    \vspace{-2ex}
  \caption
  	{ \small
	Visualisation of classifer's weight norm and centroid's norm of ICCL after the representation learning (left) and classifier balancing stage (right) on ImageNet-LT. Comparing with cRT~\cite{decouple-longtail}, the weight norm of our ICCL classifier is more balanced. Additionally, the class centroids have intrinsically balanced norm.
	  } 
  \label{fig:weight_norm1}
   \vspace{-2.5ex}
 \end{figure*} 
 \textbf{Interpolation weight .}
In Equation~\ref{eq:x_fused},
We sample the interpolation weight  from a uniform distribution,
which is equivalent to .
We vary the beta distribution and study its effect on CIFAR100-LT with an imbalance ratio of . 
The resulting accuracy and the corresponding beta distribution are shown in Table~\ref{tbl:lambda}.
Sampling from  is more likely to return a small , thus the interpolative samples 
contain more information about images from the class-aware sampler.
As we fix  and increase them from  to ,
the accuracy increases. 
Good performance can be achieved with  and ,
where the sampled  is less likely to be an extreme value.

\textbf{Class-aware sampler adjustment parameter }. 
We further investigate the influence of  on representation learning. 
When  and , the class-aware sampler is equivalent to class-balanced sampler~\cite{shen2016relay} 
and reverse sampler~\cite{bbn} respectively.
We include a class-agnostic uniform sampler as the baseline.
Table~\ref{tbl:second_sampler_choice} shows that the interpolative branch sampler should neither focus excessively on the 
tail classes () nor on the head classes (uniform). 
When using either of these two samplers,
the resulting interpolative image might be less informative due to 
excessive repetition of tail-class samples or redundant head-class samples.  

\begin{table}[!t]
	
	\centering
	\resizebox{1\columnwidth}{!}{
	\begin{tabular}	{c|c| c c c c}
		\hline	 	 	
			Method & Rebalancing & Overall & Many & Medium & Few \\
		\hline	 
			CE~\cite{decouple-longtail} & - &  46.7 & 68.1 & 40.2 & 9.0 \\
			ICCL (ours)  & -	& \textbf{50.5} & \textbf{68.5} & \textbf{44.4} & \textbf{20.8}  \\
		\hline	 
			
			CE~\cite{decouple-longtail}& \checkmark& 52.4 & \textbf{64.3}  & 49.1 & 30.7 \\ 
			ICCL (ours) & \checkmark	& \textbf{54.0}  & 60.7 & \textbf{52.9}  & \textbf{39.0} \\
	\hline	 
	\end{tabular}
}
    \vspace{-1ex}
	\caption
		{\small	
		Effect of classifier rebalancing on ImageNet-LT. ICCL learns better tail-class representation which leads to higher tail-class (i.e. medium and few) accuracy after classifier rebalancing.
		}
	\label{tbl:rebalance_cls_imgnet}
\vspace{-2ex}
\end{table}	 \begin{figure}[!t]

	\centering
	\includegraphics[width=0.45\textwidth]{distill_tau.pdf}
	\vspace{-1ex}

	\caption
	{ \small
		Effect of distillation temperature  on ImageNet-LT. ICCL's overall accuracy is not sensitive to  variation.
	} 
	\label{fig:distill_tau}
	\vspace{-3ex}
\end{figure}  
\textbf{Rebalancing classifier.} 
In Table~\ref{tbl:rebalance_cls_imgnet}, 
we show the effect of classifier rebalancing,
which improves both ICCL and the baseline CE method~\cite{decouple-longtail}.
By learning better tail-class representation, 
ICCL achieves higher overall accuracy compared to~\cite{decouple-longtail} both before and after classifier rebalancing. 




\textbf{Classifier balancing parameters.} 
In the classifier balancing stage,
we fix the sampler adjustment parameter , 
and the distillation weight .
We study their effects in Table~\ref{tbl:cls_balancing}.
For our ICCL approach, 
using a reverse sampler () is better 
than a balanced sampler ().
Furthermore, the distillation loss tends to benefit more complex ImageNet-LT and iNaturalist than 
CIFAR-LT datasets.
For the baseline cRT~\cite{decouple-longtail},
applying the reverse sampler and distillation does not give accuracy improvement compared to the default setting (52.4).

\begin{table}[!t]
		\setlength\tabcolsep{4pt}
	\centering
	\resizebox{1\columnwidth}{!}{  
	\begin{tabular}	{ c c |c |c |c |c c}
		\hline	 	 
		    \multirow{2}{*}{} &  \multirow{2}{*}{} &    CIFAR100-LT  & CIFAR10-LT  & iNaturalist  & \multicolumn{2}{c}{ImageNet-LT} \\
			\cmidrule{3-7}
		   &  & ICCL & ICCL & ICCL & ICCL & cRT~\cite{decouple-longtail} \\
			\midrule
			0 & 0			& 45.3 & 77.5 & 69.5 & 53.7 & 52.4\\
			0 & 0.5  & 45.0 &  77.6 &  69.5  & 53.2 &  52.2 \\
			1 & 0		&  47.1 &  82.3 &  70.2 & 53.6 & 49.6 \\
			1 & 0.5  & 46.6 &82.1  & 70.5 & 54.0 & 51.3 \\
	\hline	 
	\end{tabular}
}
\vspace{-1ex}
	\caption
		{\small	
		Ablation study for classifier balancing methods.
		ICCL benefits from using a reverse sampler () 
		and knowledge distillation (), especially for 
		the more complex ImageNet-LT and iNaturalist datasets.
		}
	\label{tbl:cls_balancing}
\vspace{-2.5ex}
\end{table}		


 
\textbf{Weight norm visualisation.}
The  norms of the weights for the linear classification layer suggest how balanced the classifier is.
Having a high weight norm for a particular class indicates that the classifier is more likely to generate a high logit score for that class.
Figure~\ref{fig:weight_norm1} depicts the weight norm of ICCL and cRT~\cite{decouple-longtail} after the representation learning and classifier balancing stage. 
In both stages, our ICCL classifier has a more balanced weight norm compared with cRT. 
Furthermore, we also plot the norm of our class centroids ,
which shows that the centroids are intrinsically balanced across different classes.



\textbf{Distillation temperature .}
In Figure~\ref{fig:distill_tau}, we study how  affects the accuracy of ICCL on ImageNet-LT. 
We find that the overall accuracy is not sensitive to changes in .
As  increases, the teacher's logit distribution becomes more flattened.
Therefore, the accuracy for medium and few class improves,
whereas the accuracy for many class decreases. 







