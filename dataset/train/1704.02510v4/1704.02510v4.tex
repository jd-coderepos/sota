\section{Experimental results and evaluation}
\label{sect:exp}

To assess the capability of DualGAN in general-purpose image-to-image translation, we 
conduct experiments on a variety of tasks, including photo-sketch conversion, 
label-image translation, and artistic stylization.

\begin{figure}
\begin{center}
\includegraphics[width=0.19\linewidth]{figures/day1_o.jpg}
\includegraphics[width=0.19\linewidth]{figures/day1_gt.jpg}
\includegraphics[width=0.19\linewidth]{figures/day1_dual.jpg}
\includegraphics[width=0.19\linewidth]{figures/day1_gan.jpg}
\includegraphics[width=0.19\linewidth]{figures/day1_cgan.jpg}

\includegraphics[width=0.19\linewidth]{figures/day2_o.jpg}
\includegraphics[width=0.19\linewidth]{figures/day2_gt.jpg}
\includegraphics[width=0.19\linewidth]{figures/day2_dual.jpg}
\includegraphics[width=0.19\linewidth]{figures/day2_gan.jpg}
\includegraphics[width=0.19\linewidth]{figures/day2_cgan.jpg}

\includegraphics[width=0.19\linewidth]{figures/day3_o.jpg}
\includegraphics[width=0.19\linewidth]{figures/day3_gt.jpg}
\includegraphics[width=0.19\linewidth]{figures/day3_dual.jpg}
\includegraphics[width=0.19\linewidth]{figures/day3_gan.jpg}
\includegraphics[width=0.19\linewidth]{figures/day3_cgan.jpg}

\includegraphics[width=0.19\linewidth]{figures/day4_o.jpg}
\includegraphics[width=0.19\linewidth]{figures/day4_gt.jpg}
\includegraphics[width=0.19\linewidth]{figures/day4_dual.jpg}
\includegraphics[width=0.19\linewidth]{figures/day4_gan.jpg}
\includegraphics[width=0.19\linewidth]{figures/day4_cgan.jpg}

\begin{subfigure}[]{0.19\linewidth}\caption*{Input}\end{subfigure}
\begin{subfigure}[]{0.19\linewidth}\caption*{GT}\end{subfigure}
\begin{subfigure}[]{0.19\linewidth}\caption*{\textbf{DualGAN}}\end{subfigure}
\begin{subfigure}[]{0.19\linewidth}\caption*{GAN}\end{subfigure}
\begin{subfigure}[]{0.19\linewidth}\caption*{cGAN~\cite{isola2016image}}\end{subfigure}
\caption{Results of day$\rightarrow$night translation. cGAN~\cite{isola2016image} is trained with labeled data, whereas DualGAN and GAN are trained in an unsupervised manner. DualGAN successfully emulates the night scenes while preserving textures in the inputs, e.g., see differences over the cloud regions between our results and the ground truth (GT). In comparison, results of cGAN and GAN contain much less details.}
\label{fig:day}
\end{center}
\end{figure}



\begin{figure}
\begin{center}
\includegraphics[width=0.19\linewidth]{figures/facades1_o.jpg}
\includegraphics[width=0.19\linewidth]{figures/facades1_gt.jpg}
\includegraphics[width=0.19\linewidth]{figures/facades1_dual.jpg}
\includegraphics[width=0.19\linewidth]{figures/facades1_gan.jpg}
\includegraphics[width=0.19\linewidth]{figures/facades1_cgan.jpg}

\includegraphics[width=0.19\linewidth]{figures/facades2_o.jpg}
\includegraphics[width=0.19\linewidth]{figures/facades2_gt.jpg}
\includegraphics[width=0.19\linewidth]{figures/facades2_dual.jpg}
\includegraphics[width=0.19\linewidth]{figures/facades2_gan.jpg}
\includegraphics[width=0.19\linewidth]{figures/facades2_cgan.jpg}

\includegraphics[width=0.19\linewidth]{figures/facades3_o.jpg}
\includegraphics[width=0.19\linewidth]{figures/facades3_gt.jpg}
\includegraphics[width=0.19\linewidth]{figures/facades3_dual.jpg}
\includegraphics[width=0.19\linewidth]{figures/facades3_gan.jpg}
\includegraphics[width=0.19\linewidth]{figures/facades3_cgan.jpg}

\includegraphics[width=0.19\linewidth]{figures/facades4_o.jpg}
\includegraphics[width=0.19\linewidth]{figures/facades4_gt.jpg}
\includegraphics[width=0.19\linewidth]{figures/facades4_dual.jpg}
\includegraphics[width=0.19\linewidth]{figures/facades4_gan.jpg}
\includegraphics[width=0.19\linewidth]{figures/facades4_cgan.jpg}

\begin{subfigure}[]{0.19\linewidth}\caption*{Input}\end{subfigure}
\begin{subfigure}[]{0.19\linewidth}\caption*{GT}\end{subfigure}
\begin{subfigure}[]{0.19\linewidth}\caption*{\textbf{DualGAN}}\end{subfigure}
\begin{subfigure}[]{0.19\linewidth}\caption*{GAN}\end{subfigure}
\begin{subfigure}[]{0.19\linewidth}\caption*{cGAN~\cite{isola2016image}}\end{subfigure}
\caption{Results of label$\rightarrow$facade translation. DualGAN faithfully preserves the structures 
in the label images, even though some labels do not match well with the corresponding photos
in finer details. In contrast, results from GAN and cGAN contain many artifacts. Over regions with
label-photo misalignment, cGAN often yields blurry output (e.g., the roof in second row and 
the entrance in third row).}
\label{fig:facades}
\end{center}
\end{figure}

To compare DualGAN with GAN and cGAN~\cite{isola2016image}, four labeled datasets are used: 
PHOTO-SKETCH~\cite{wang2009face,zhang2011coupled}, DAY-NIGHT~\cite{laffont2014transient}, 
LABEL-FACADES~\cite{tylevcek2013spatial}, and AERIAL-MAPS, which was directly captured from 
Google Map~\cite{isola2016image}. These datasets consist of corresponding images between two 
domains; they serve as ground truth (GT) and can also be used for supervised learning. However, none of these datasets could guarantee accurate feature alignment at the pixel level. For example, 
the sketches in SKETCH-PHOTO dataset were drawn by artists and do not accurately align with the 
corresponding photos, moving objects and cloud pattern changes often show up in the DAY-NIGHT dataset, 
and the labels in LABEL-FACADES dataset are not always precise. This highlights, in part, the difficulty in 
obtaining high quality matching image pairs.

DualGAN enables us to utilize abundant unlabeled image sources from the Web. Two unlabeled and 
unpaired datasets are also tested in our experiments. The MATERIAL dataset includes images of objects made of 
different materials, e.g., stone, metal, plastic, fabric, and wood. These images were manually selected from 
Flickr and cover a variety of illumination conditions, compositions, color, texture, and material 
sub-types~\cite{sharan2009material}. This dataset was initially used for material recognition, but is applied 
here for material transfer. The OIL-CHINESE painting dataset includes artistic paintings of two disparate styles: 
oil and Chinese. All images were crawled from search engines and they contain images with varying 
quality, format, and size. We reformat, crop, and resize the images for training and evaluation.
In both of these datasets, no correspondence is available between images from different domains.

\section{Qualitative evaluation}

Using the four labeled datasets, we first compare DualGAN with GAN and cGAN~\cite{isola2016image} on the 
following translation tasks: day$\rightarrow$night (Figure~\ref{fig:day}), labels$\leftrightarrow$facade 
(Figures~\ref{fig:facades} and~\ref{fig:facades2label}), face photo$\leftrightarrow$sketch (Figures~\ref{fig:photo} 
and \ref{fig:sketch}), and map$\leftrightarrow$aerial photo (Figures~\ref{fig:maps} and \ref{fig:aerial}). 
In all these tasks, cGAN was trained with labeled (i.e., paired) data, where we ran the model and code provided 
in~\cite{isola2016image} and chose the optimal loss function for each task: $L_1$ loss for facade$\rightarrow$label 
and $L_1+cGAN$ loss for the other tasks (see~\cite{isola2016image} for more details). In contrast, DualGAN and 
GAN were trained in an unsupervised way, i.e., we decouple the image pairs and then reshuffle the data. The 
results of GAN were generated using our approach by setting $\lambda_U=\lambda_V = 0.0$ in eq.~(\ref{eq:loss_g}), 
noting that this GAN is different from the original GAN model~\cite{goodfellow2014generative} as it employs a conditional 
generator.

All three models were trained on the same training datasets and tested on novel data that does not overlap those for training.
All the training were carried out on a single GeForce GTX Titan X GPU. At test time, all models ran in well under a second 
on this GPU. 

Compared to GAN, in almost all cases, DualGAN produces results that are less blurry, contain fewer artifacts, and better
preserve content structures in the inputs and capture features (e.g., texture, color, and/or style) of the target domain. 
We attribute the improvements to the reconstruction loss, which forces the inputs to be reconstructable from 
outputs through the dual generator and strengthens feedback signals that encodes the targeted distribution.  

In many cases, DualGAN also compares favorably over the supervised cGAN in terms of sharpness of the outputs and 
faithfulness to the input images; see Figures~\ref{fig:day},~\ref{fig:facades},~\ref{fig:photo},~\ref{fig:sketch}, and~\ref{fig:maps}.
This is encouraging since the supervision in cGAN does utilize additional image and pixel correspondences. On the other hand, 
when translating between photos and semantic-based labels, such as map$\leftrightarrow$aerial and label$\leftrightarrow$facades, 
it is often impossible to infer the correspondences between pixel colors and labels based on targeted distribution alone. As a result, 
DualGAN may map pixels to wrong labels (see Figures~\ref{fig:aerial} and~\ref{fig:facades2label}) or labels to wrong colors/textures 
(see Figures~\ref{fig:facades} and~\ref{fig:maps}).

Figures~\ref{fig:chinese} and \ref{fig:material} show image translation results obtained using the two unlabeled datasets, including 
oil$\leftrightarrow$Chinese, plastic$\rightarrow$metal, metal$\rightarrow$stone, leather$\rightarrow$fabric, as well as wood$\leftrightarrow$plastic. 
The results demonstrate that visually convincing images can be generated by DualGAN when no corresponding images can be found in 
the target domains. As well, the DualGAN results generally contain less artifacts than those from GAN.




\begin{figure}
\begin{center}
\includegraphics[width=0.19\linewidth]{figures/photo1_o.jpg}
\includegraphics[width=0.19\linewidth]{figures/photo1_gt.jpg}
\includegraphics[width=0.19\linewidth]{figures/photo1_dual.jpg}
\includegraphics[width=0.19\linewidth]{figures/photo1_gan.jpg}
\includegraphics[width=0.19\linewidth]{figures/photo1_cgan.jpg}

\includegraphics[width=0.19\linewidth]{figures/photo2_o.jpg}
\includegraphics[width=0.19\linewidth]{figures/photo2_gt.jpg}
\includegraphics[width=0.19\linewidth]{figures/photo2_dual.jpg}
\includegraphics[width=0.19\linewidth]{figures/photo2_gan.jpg}
\includegraphics[width=0.19\linewidth]{figures/photo2_cgan.jpg}

\includegraphics[width=0.19\linewidth]{figures/photo3_o.jpg}
\includegraphics[width=0.19\linewidth]{figures/photo3_gt.jpg}
\includegraphics[width=0.19\linewidth]{figures/photo3_dual.jpg}
\includegraphics[width=0.19\linewidth]{figures/photo3_gan.jpg}
\includegraphics[width=0.19\linewidth]{figures/photo3_cgan.jpg}

\includegraphics[width=0.19\linewidth]{figures/photo4_o.jpg}
\includegraphics[width=0.19\linewidth]{figures/photo4_gt.jpg}
\includegraphics[width=0.19\linewidth]{figures/photo4_dual.jpg}
\includegraphics[width=0.19\linewidth]{figures/photo4_gan.jpg}
\includegraphics[width=0.19\linewidth]{figures/photo4_cgan.jpg}

\begin{subfigure}[]{0.19\linewidth}\caption*{Input}\end{subfigure}
\begin{subfigure}[]{0.19\linewidth}\caption*{GT}\end{subfigure}
\begin{subfigure}[]{0.19\linewidth}\caption*{\textbf{DualGAN}}\end{subfigure}
\begin{subfigure}[]{0.19\linewidth}\caption*{GAN}\end{subfigure}
\begin{subfigure}[]{0.19\linewidth}\caption*{cGAN~\cite{isola2016image}}\end{subfigure}
\caption{Photo$\rightarrow$sketch translation for faces. Results of DualGAN are 
generally sharper than those from cGAN, even though the former was trained using 
unpaired data, whereas the latter makes use of image correspondence.}
\label{fig:photo}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=0.19\linewidth]{figures/sketch1_o.jpg}
\includegraphics[width=0.19\linewidth]{figures/sketch1_gt.jpg}
\includegraphics[width=0.19\linewidth]{figures/sketch1_dual.jpg}
\includegraphics[width=0.19\linewidth]{figures/sketch1_gan.jpg}
\includegraphics[width=0.19\linewidth]{figures/sketch1_cgan.jpg}

\includegraphics[width=0.19\linewidth]{figures/sketch2_o.jpg}
\includegraphics[width=0.19\linewidth]{figures/sketch2_gt.jpg}
\includegraphics[width=0.19\linewidth]{figures/sketch2_dual.jpg}
\includegraphics[width=0.19\linewidth]{figures/sketch2_gan.jpg}
\includegraphics[width=0.19\linewidth]{figures/sketch2_cgan.jpg}

\includegraphics[width=0.19\linewidth]{figures/sketch3_o.jpg}
\includegraphics[width=0.19\linewidth]{figures/sketch3_gt.jpg}
\includegraphics[width=0.19\linewidth]{figures/sketch3_dual.jpg}
\includegraphics[width=0.19\linewidth]{figures/sketch3_gan.jpg}
\includegraphics[width=0.19\linewidth]{figures/sketch3_cgan.jpg}

\includegraphics[width=0.19\linewidth]{figures/sketch4_o.jpg}
\includegraphics[width=0.19\linewidth]{figures/sketch4_gt.jpg}
\includegraphics[width=0.19\linewidth]{figures/sketch4_dual.jpg}
\includegraphics[width=0.19\linewidth]{figures/sketch4_gan.jpg}
\includegraphics[width=0.19\linewidth]{figures/sketch4_cgan.jpg}

\begin{subfigure}[]{0.19\linewidth}\caption*{Input}\end{subfigure}
\begin{subfigure}[]{0.19\linewidth}\caption*{GT}\end{subfigure}
\begin{subfigure}[]{0.19\linewidth}\caption*{\textbf{DualGAN}}\end{subfigure}
\begin{subfigure}[]{0.19\linewidth}\caption*{GAN}\end{subfigure}
\begin{subfigure}[]{0.19\linewidth}\caption*{cGAN~\cite{isola2016image}}\end{subfigure}
\caption{Results for sketch$\rightarrow$photo translation of faces. More 
artifacts and blurriness are showing up in results generated by GAN and cGAN than 
DualGAN.} \label{fig:sketch}
\end{center}
\end{figure}






\begin{figure}
\begin{center}
\includegraphics[width=0.32\linewidth]{figures/chinese1_o.jpg}
\includegraphics[width=0.32\linewidth]{figures/chinese1_dual.jpg}
\includegraphics[width=0.32\linewidth]{figures/chinese1_gan.jpg}

\includegraphics[width=0.32\linewidth]{figures/chinese2_o.jpg}
\includegraphics[width=0.32\linewidth]{figures/chinese2_dual.jpg}
\includegraphics[width=0.32\linewidth]{figures/chinese2_gan.jpg}

\includegraphics[width=0.32\linewidth]{figures/chinese3_o.jpg}
\includegraphics[width=0.32\linewidth]{figures/chinese3_dual.jpg}
\includegraphics[width=0.32\linewidth]{figures/chinese3_gan.jpg}

\includegraphics[width=0.32\linewidth]{figures/chinese4_o.jpg}
\includegraphics[width=0.32\linewidth]{figures/chinese4_dual.jpg}
\includegraphics[width=0.32\linewidth]{figures/chinese4_gan.jpg}

\begin{subfigure}[]{0.32\linewidth}\caption*{Input}\end{subfigure}
\begin{subfigure}[]{0.32\linewidth}\caption*{\textbf{DualGAN}}\end{subfigure}
\begin{subfigure}[]{0.32\linewidth}\caption*{GAN}\end{subfigure}
\caption{Experimental results for translating Chinese paintings to oil paintings
(without GT available). The background grids shown in the GAN results imply that the outputs of GAN 
are not as stable as those of DualGAN.} \label{fig:chinese}
\end{center}
\end{figure}


\begin{figure*}
\begin{center}

\includegraphics[width=0.13\linewidth]{figures/material10_o.jpg}
\includegraphics[width=0.13\linewidth]{figures/material10_dual.jpg}
\includegraphics[width=0.13\linewidth]{figures/material10_gan.jpg}
\includegraphics[width=0.13\linewidth]{figures/material11_o.jpg}
\includegraphics[width=0.13\linewidth]{figures/material11_dual.jpg}
\includegraphics[width=0.13\linewidth]{figures/material11_gan.jpg}

\begin{subfigure}[]{0.13\linewidth}\caption*{plastic (input)}\end{subfigure}
\begin{subfigure}[]{0.14\linewidth}\caption*{metal (DualGAN)}\end{subfigure}
\begin{subfigure}[]{0.13\linewidth}\caption*{metal (GAN)}\end{subfigure}
\begin{subfigure}[]{0.13\linewidth}\caption*{plastic (input)}\end{subfigure}
\begin{subfigure}[]{0.14\linewidth}\caption*{metal (DualGAN)}\end{subfigure}
\begin{subfigure}[]{0.13\linewidth}\caption*{metal (GAN)}\end{subfigure}

\includegraphics[width=0.13\linewidth]{figures/material6_o.jpg}
\includegraphics[width=0.13\linewidth]{figures/material6_dual.jpg}
\includegraphics[width=0.13\linewidth]{figures/material6_gan.jpg}
\includegraphics[width=0.13\linewidth]{figures/material7_o.jpg}
\includegraphics[width=0.13\linewidth]{figures/material7_dual.jpg}
\includegraphics[width=0.13\linewidth]{figures/material7_gan.jpg}

\begin{subfigure}[]{0.13\linewidth}\caption*{metal (input)}\end{subfigure}
\begin{subfigure}[]{0.13\linewidth}\caption*{stone (DualGAN)}\end{subfigure}
\begin{subfigure}[]{0.13\linewidth}\caption*{stone (GAN)}\end{subfigure}
\begin{subfigure}[]{0.13\linewidth}\caption*{metal (input)}\end{subfigure}
\begin{subfigure}[]{0.13\linewidth}\caption*{stone (DualGAN)}\end{subfigure}
\begin{subfigure}[]{0.13\linewidth}\caption*{stone (GAN)}\end{subfigure}

\includegraphics[width=0.13\linewidth]{figures/material9_o.jpg}
\includegraphics[width=0.13\linewidth]{figures/material9_dual.jpg}
\includegraphics[width=0.13\linewidth]{figures/material9_gan.jpg}
\includegraphics[width=0.13\linewidth]{figures/material8_o.jpg}
\includegraphics[width=0.13\linewidth]{figures/material8_dual.jpg}
\includegraphics[width=0.13\linewidth]{figures/material8_gan.jpg}

\begin{subfigure}[]{0.13\linewidth}\caption*{leather (input)}\end{subfigure}
\begin{subfigure}[]{0.14\linewidth}\caption*{fabric (DualGAN)}\end{subfigure}
\begin{subfigure}[]{0.13\linewidth}\caption*{fabric (GAN)}\end{subfigure}
\begin{subfigure}[]{0.13\linewidth}\caption*{leather (input)}\end{subfigure}
\begin{subfigure}[]{0.14\linewidth}\caption*{fabric (DualGAN)}\end{subfigure}
\begin{subfigure}[]{0.13\linewidth}\caption*{fabric (GAN)}\end{subfigure}

\includegraphics[width=0.13\linewidth]{figures/material13_o.jpg}
\includegraphics[width=0.13\linewidth]{figures/material13_dual.jpg}
\includegraphics[width=0.13\linewidth]{figures/material13_gan.jpg}
\includegraphics[width=0.13\linewidth]{figures/material14_o.jpg}
\includegraphics[width=0.13\linewidth]{figures/material14_dual.jpg}
\includegraphics[width=0.13\linewidth]{figures/material14_gan.jpg}

\begin{subfigure}[]{0.13\linewidth}\caption*{wood (input)}\end{subfigure}
\begin{subfigure}[]{0.15\linewidth}\caption*{plastic (DualGAN)}\end{subfigure}
\begin{subfigure}[]{0.13\linewidth}\caption*{plastic (GAN)}\end{subfigure}
\begin{subfigure}[]{0.13\linewidth}\caption*{plastic (input)}\end{subfigure}
\begin{subfigure}[]{0.14\linewidth}\caption*{wood (DualGAN)}\end{subfigure}
\begin{subfigure}[]{0.13\linewidth}\caption*{wood (GAN)}\end{subfigure}
\caption{Experimental results for various material transfer tasks. From top to bottom, 
plastic$\rightarrow$metal, metal$\rightarrow$stone, leather$\rightarrow$fabric, and 
plastic$\leftrightarrow$wood.} \label{fig:material}
\end{center}
\end{figure*}


\begin{figure}
\begin{center}
\includegraphics[width=0.19\linewidth]{figures/maps1_o.jpg}
\includegraphics[width=0.19\linewidth]{figures/maps1_gt.jpg}
\includegraphics[width=0.19\linewidth]{figures/maps1_dual.jpg}
\includegraphics[width=0.19\linewidth]{figures/maps1_gan.jpg}
\includegraphics[width=0.19\linewidth]{figures/maps1_cgan.jpg}



\includegraphics[width=0.19\linewidth]{figures/maps3_o.jpg}
\includegraphics[width=0.19\linewidth]{figures/maps3_gt.jpg}
\includegraphics[width=0.19\linewidth]{figures/maps3_dual.jpg}
\includegraphics[width=0.19\linewidth]{figures/maps3_gan.jpg}
\includegraphics[width=0.19\linewidth]{figures/maps3_cgan.jpg}

\includegraphics[width=0.19\linewidth]{figures/maps4_o.jpg}
\includegraphics[width=0.19\linewidth]{figures/maps4_gt.jpg}
\includegraphics[width=0.19\linewidth]{figures/maps4_dual.jpg}
\includegraphics[width=0.19\linewidth]{figures/maps4_gan.jpg}
\includegraphics[width=0.19\linewidth]{figures/maps4_cgan.jpg}

\begin{subfigure}[]{0.19\linewidth}\caption*{Input}\end{subfigure}
\begin{subfigure}[]{0.19\linewidth}\caption*{GT}\end{subfigure}
\begin{subfigure}[]{0.19\linewidth}\caption*{\textbf{DualGAN}}\end{subfigure}
\begin{subfigure}[]{0.19\linewidth}\caption*{GAN}\end{subfigure}
\begin{subfigure}[]{0.19\linewidth}\caption*{cGAN~\cite{isola2016image}}\end{subfigure}
\caption{Map$\rightarrow$aerial photo translation. Without image correspondences for training, 
DualGAN may map the orange-colored interstate highways to building roofs with bright colors. 
Nevertheless, the DualGAN results are sharper than those from GAN and cGAN.}
\label{fig:maps}
\end{center}
\end{figure}


\begin{figure}
\begin{center}


\includegraphics[width=0.19\linewidth]{figures/arial3_o.jpg}
\includegraphics[width=0.19\linewidth]{figures/arial3_gt.jpg}
\includegraphics[width=0.19\linewidth]{figures/arial3_dual.jpg}
\includegraphics[width=0.19\linewidth]{figures/arial3_gan.jpg}
\includegraphics[width=0.19\linewidth]{figures/arial3_cgan.jpg}



\includegraphics[width=0.19\linewidth]{figures/arial5_o.jpg}
\includegraphics[width=0.19\linewidth]{figures/arial5_gt.jpg}
\includegraphics[width=0.19\linewidth]{figures/arial5_dual.jpg}
\includegraphics[width=0.19\linewidth]{figures/arial5_gan.jpg}
\includegraphics[width=0.19\linewidth]{figures/arial5_cgan.jpg}

\includegraphics[width=0.19\linewidth]{figures/arial6_o.jpg}
\includegraphics[width=0.19\linewidth]{figures/arial6_gt.jpg}
\includegraphics[width=0.19\linewidth]{figures/arial6_dual.jpg}
\includegraphics[width=0.19\linewidth]{figures/arial6_gan.jpg}
\includegraphics[width=0.19\linewidth]{figures/arial6_cgan.jpg}

\begin{subfigure}[]{0.19\linewidth}\caption*{Input}\end{subfigure}
\begin{subfigure}[]{0.19\linewidth}\caption*{GT}\end{subfigure}
\begin{subfigure}[]{0.19\linewidth}\caption*{\textbf{DualGAN}}\end{subfigure}
\begin{subfigure}[]{0.19\linewidth}\caption*{GAN)}\end{subfigure}
\begin{subfigure}[]{0.19\linewidth}\caption*{cGAN~\cite{isola2016image}}\end{subfigure}
\caption{Results for aerial photo$\rightarrow$map translation. DualGAN performs better than 
GAN, but not as good as cGAN. With additional pixel correspondence information, cGAN performs 
well in terms of labeling local roads, but still cannot detect interstate highways.}
\label{fig:aerial}
\end{center}
\end{figure}


\begin{figure}
\begin{center}
\includegraphics[width=0.19\linewidth]{figures/facadeslabel1_o.jpg}
\includegraphics[width=0.19\linewidth]{figures/facadeslabel1_gt.jpg}
\includegraphics[width=0.19\linewidth]{figures/facadeslabel1_dual.jpg}
\includegraphics[width=0.19\linewidth]{figures/facadeslabel1_gan.jpg}
\includegraphics[width=0.19\linewidth]{figures/facadeslabel1_cgan.jpg}

\includegraphics[width=0.19\linewidth]{figures/facadeslabel2_o.jpg}
\includegraphics[width=0.19\linewidth]{figures/facadeslabel2_gt.jpg}
\includegraphics[width=0.19\linewidth]{figures/facadeslabel2_dual.jpg}
\includegraphics[width=0.19\linewidth]{figures/facadeslabel2_gan.jpg}
\includegraphics[width=0.19\linewidth]{figures/facadeslabel2_cgan.jpg}

\includegraphics[width=0.19\linewidth]{figures/facadeslabel3_o.jpg}
\includegraphics[width=0.19\linewidth]{figures/facadeslabel3_gt.jpg}
\includegraphics[width=0.19\linewidth]{figures/facadeslabel3_dual.jpg}
\includegraphics[width=0.19\linewidth]{figures/facadeslabel3_gan.jpg}
\includegraphics[width=0.19\linewidth]{figures/facadeslabel3_cgan.jpg}



\begin{subfigure}[]{0.19\linewidth}\caption*{Input}\end{subfigure}
\begin{subfigure}[]{0.19\linewidth}\caption*{GT}\end{subfigure}
\begin{subfigure}[]{0.19\linewidth}\caption*{\textbf{DualGAN}}\end{subfigure}
\begin{subfigure}[]{0.19\linewidth}\caption*{GAN)}\end{subfigure}
\begin{subfigure}[]{0.19\linewidth}\caption*{cGAN~\cite{isola2016image}}\end{subfigure}
\caption{Facades$\rightarrow$label translation. While cGAN correctly labels various 
bulding components such as windows, doors, and balconies, the overall label images 
are not as detailed and structured as DualGAN's outputs.} 
\label{fig:facades2label}
\end{center}
\end{figure}

\subsection{Quantitative evaluation}

To quantitatively evaluate DualGAN, we set up two user studies through Amazon Mechanical Turk (AMT). 
The ``material perceptual'' test evaluates the material transfer results, in which we mix the outputs 
from all material transfer tasks and let the Turkers choose the best match based on which material they 
believe the objects in the image are made of. For a total of 176 output images, each was evaluated by 
ten Turkers. An output image is rated as a success if at least three Turkers selected the target material type. 
Success rates of various material transfer results using different approaches are summarized in 
Table~\ref{table:material}, showing that DualGAN outperforms GAN by a large margin.

In addition, we run the AMT ``realness score'' evaluation for sketch$\rightarrow$photo, label map$\rightarrow$facades, 
maps$\rightarrow$aerial photo, and day$\rightarrow$night translations. To eliminate potential bias, for each of the four 
evaluations, we randomly shuffle real photos and outputs from all three approaches before showing them to Turkers.  
Each image is shown to 20 Turkers, who were asked to score the image based on to what extent the synthesized photo 
looks real. The ``realness'' score ranges from 0 (totally missing), 1 (bad), 2 (acceptable), 3 (good), to 4 (compelling). The 
average score of different approaches on various tasks are then computed and shown in Table.~\ref{table:score}. The 
AMT study results show that DualGAN outperforms GAN on all tasks and outperforms cGAN on two tasks as well. This 
indicates that cGAN has little tolerance to misalignment and inconsistency between image pairs, but the additional 
pixel-level correspondence does help cGAN correctly map labels to colors and textures.



Finally, we compute the segmentation accuracies for facades$\rightarrow$label and aerial$\rightarrow$map tasks, as reported
in Tables~\ref{table:acc} and \ref{table:acc_maps}. The comparison shows that DualGAN is outperformed by cGAN, which is 
expected as it is difficult to infer proper labeling without image correspondence information from the training data.

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Task  & DualGAN  &  GAN \\
\hline\hline
plastic$\rightarrow$wood &  \textbf{2}/11 & 0/11 \\
\hline
wood$\rightarrow$plastic &  \textbf{1}/11 & 0/11 \\
\hline
metal$\rightarrow$stone&   \textbf{2}/11 & 0/11 \\
\hline
stone$\rightarrow$metal &   \textbf{2}/11 & 0/11 \\
\hline
leather$\rightarrow$fabric &  \textbf{3}/11 & 2/11 \\
\hline
fabric$\rightarrow$leather &  \textbf{2}/11 & 1/11 \\
\hline
plastic$\rightarrow$metal &   \textbf{7}/11 & 3/11 \\
\hline
metal$\rightarrow$plastic &   \textbf{1}/11 & 0/11 \\
\hline
\end{tabular}
\caption{Success rates of various material transfer tasks based on the AMT ``material perceptual'' test. 
There are 11 images in each set of transfer result, with noticeable improvements of DualGAN over GAN.} 
\label{table:material}
\end{center}
\end{table}



\begin{table}
\tabcolsep=0.11cm
\begin{center}
\begin{tabular}{c|cccc}
\hline
	   &   \multicolumn{4}{|c}{Avg. ``realness'' score }\\
		  Task & DualGAN  &  cGAN\cite{isola2016image} & GAN & GT\\
\hline\hline

sketch$\rightarrow$photo & \textbf{1.87 }&  1.69   & 1.04  & 3.56 \\
\hline
day$\rightarrow$night &  \textbf{2.42 } & 1.89   &  0.13  & 3.05  \\
\hline
label$\rightarrow$facades & 1.89  &   \textbf{2.59 } & 1.43  & 3.33 \\
\hline
map$\rightarrow$aerial & 2.52 &  \textbf{2.92 } & 1.88  &  3.21  \\
\hline
\end{tabular}
\caption{Average AMT ``realness'' scores of outputs from various tasks. The results show that 
DualGAN outperforms GAN in all tasks. It also outperforms cGAN for sketch$\rightarrow$photo 
and day$\rightarrow$night tasks, but still lag behind for label$\rightarrow$facade and 
map$\rightarrow$aerial tasks. In the latter two tasks, the additional image correspondence in 
training data would help cGAN map labels to the proper colors/textures.}
\label{table:score}
\end{center}
\end{table}

\tabcolsep=0.11cm
\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
  & Per-pixel acc.  & Per-class acc. & Class IOU\\
\hline\hline
DualGAN &  0.27 &  0.13 & 0.06\\      
\hline
cGAN~\cite{isola2016image}  &  \textbf{0.54}&  \textbf{0.33} &\textbf{0.19}\\  
\hline
GAN &   0.22&   0.10 & 0.05 \\
\hline
\end{tabular}
\caption{Segmentation accuracy for the facades$\rightarrow$label task. DualGAN outperforms GAN, but is not as accurate as cGAN. 
Without image correspondence (for cGAN), even if DualGAN segments a region properly, it may not assign the region with a 
correct label.} \label{table:acc}
\end{center}
\end{table}


\tabcolsep=0.11cm
\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
  & Per-pixel acc.  & Per-class acc. & Class IOU\\
\hline\hline
DualGAN &  0.42 &  0.22 & 0.09\\      
\hline
cGAN~\cite{isola2016image}  &  \textbf{0.70}&  \textbf{0.46} &\textbf{0.26}\\  
\hline
GAN &   0.41&   0.23 & 0.09 \\
\hline
\end{tabular}
\caption{Segmentation accuracy for the aerial$\rightarrow$map task, for which DualGAN performs less than 
satisfactorily.} \label{table:acc_maps}
\end{center}
\end{table}


