\documentclass{article}

\usepackage[final,nonatbib]{neurips_2022}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         

\usepackage{tikz}
\usepackage{amsmath,amssymb} \usepackage{xcolor}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\newcommand{\etal}{\textit{et al}. }

\usepackage{bbm}
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\usepackage{graphicx}
\usepackage{makecell} 
\usepackage[misc]{ifsym}

\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow, overpic, textpos, array}
\usepackage{footnote}

\title{PointTAD: Multi-Label Temporal Action Detection with Learnable Query Points}

\author{Jing Tan \thanks{Work is done during internship at Tencent PCG. ~~\textsuperscript{\dag}Corresponding author. } \quad
Xiaotong Zhao  \quad
Xintian Shi  \quad
Bin Kang  \quad
Limin Wang \vspace{0.2em}\\
State Key Laboratory for Novel Software Technology, Nanjing University \\ Platform and Content Group (PCG), Tencent \quad \quad Shanghai AI Lab \vspace{.2em}\\
{\tt\small{jtan@smail.nju.edu.cn,  \{davidxtzhao,tinaxtshi,binkang\}@tencent.com, lmwang@nju.edu.cn }}
}

\usepackage{caption}
\usepackage{subcaption}
\usepackage{pifont} 
\usepackage{multirow, overpic, textpos, array}
\usepackage{makecell}
\usepackage{footnote}
\usepackage[flushleft]{threeparttable}

\usepackage{wrapfig,lipsum,booktabs}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}
\renewcommand{\paragraph}[1]{\vspace{1.25mm}\noindent\textbf{#1}}
\newcommand\blfootnote[1]{\begingroup\renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup}

\newcolumntype{x}[1]{>{\centering\arraybackslash}p{#1pt}}
\newcolumntype{y}[1]{>{\raggedright\arraybackslash}p{#1pt}}
\newcolumntype{z}[1]{>{\raggedleft\arraybackslash}p{#1pt}}


\newcommand{\km}[1]{{\color{citecolor}[km: #1]}}
\newcommand{\xl}[1]{{\color{orange}[xl: #1]}}
\newcommand{\rbg}[1]{{\color{red}[rbg: #1]}}
\newcommand{\yh}[1]{{\color{cyan}[yh: #1]}}


\newcommand{\headrule}{\toprule}
\newcommand{\normrule}{\midrule}
\usepackage{color, colortbl}
\definecolor{Gray}{gray}{0.9}
\definecolor{baselinecolor}{gray}{0.9}

\newcommand{\TableSOTA}{
\begin{table*}[t]
\begin{threeparttable}
\centering
\small
    \renewcommand\arraystretch{0.9}
    \setlength{\tabcolsep}{13pt}
    \caption{\textbf{Comparison with the state of the art} on the MultiTHUMOS test set and Charades test set, under detection-mAP (\%) and segmentation-mAP(\%).}
\begin{tabular}{cccccc}
\headrule
\multirow{2}{*}{{Methods}}& \multirow{2}{*}{Modality}  & \multicolumn{2}{c}{MultiTHUMOS}               & \multicolumn{2}{c}{Charades} \\ \cmidrule(lr{0.5em}){3-4} \cmidrule(l{0.5em}r){5-6} 
\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & Det-mAP & \multicolumn{1}{c}{Seg-mAP} & Det-mAP   & Seg-mAP  \\ 
\midrule
R-C3D~\cite{DBLP:conf/iccv/XuDS17}  & RGB & -       & -         & -        & {17.6}      \\
Super-event~\cite{DBLP:conf/cvpr/PiergiovanniR18}& RGB & -   & 36.4           & -        & 18.6       \\
TGM~\cite{DBLP:conf/icml/PiergiovanniR19}   & RGB & -         & 37.2         & -        & 20.6       \\
{\color{gray}TGM~\cite{DBLP:conf/icml/PiergiovanniR19} }  & {\color{gray}RGB+OF} & {\color{gray}-} & {\color{gray}44.3}  & {\color{gray}-} & {\color{gray}21.5}    \\
PDAN~\cite{DBLP:conf/wacv/DaiDMGFB21}& RGB &{17.3/17.1\footnotemark[3]}  & 40.2 & {8.5} & 23.7 \\
Coarse-Fine~\cite{DBLP:conf/cvpr/KahatapitiyaR21}  & RGB  & -  & -  & {6.1}   & 25.1   \\
MLAD~\cite{DBLP:conf/cvpr/TirupatturDRS21}    & RGB & {14.2/13.9\footnotemark[3]}  & 42.2  & - & 18.4 \\
{\color{gray}MLAD~\cite{DBLP:conf/cvpr/TirupatturDRS21}} & {\color{gray}RGB+OF} & \textcolor{gray}{-}& \textcolor{gray}{51.5} & \textcolor{gray}{-} & \textcolor{gray}{23.7} \\
CTRN~\cite{DBLP:conf/bmvc/DaiDB21}  & RGB & -  & {\bf 44.0}    & -       & {25.3}       \\
{\color{gray}CTRN~\cite{DBLP:conf/bmvc/DaiDB21}}  & {\color{gray}RGB+OF} & {\color{gray}-}  & {\color{gray}51.2}    & {\color{gray}-} & {\color{gray}27.8}       \\
{\color{gray}AGT~\cite{DBLP:journals/corr/abs-2101-08540}} & {\color{gray}RGB+OF}& {\color{gray}-} & {\color{gray}-} &{\color{gray}-}& {\color{gray}28.6} \\
MS-TCT~\cite{DBLP:conf/cvpr/DaiDKRB22}  & RGB & {16.2/16.0\footnotemark[3]}        & {43.1}     & {7.9}            & {\bf 25.4}       \\
\midrule
Ours      & RGB & {{ 21.5/21.4\footnotemark[3]}}         & 39.8                           & { 11.1}           & 21.0      \\
Ours\footnotemark[4]    & RGB & {{\bf 23.5/23.4\footnotemark[3]}}         & 41.2                         & {\bf 12.1}           & 22.1     \\
\bottomrule
\end{tabular}
\label{tab:sota}
\begin{tablenotes}
\item \small{\em \footnotemark[3] indicates detection results excluding NoHuman class. \footnotemark[4]indicates results trained with stronger image augmentation as in \cite{liu2022an}.}
\vspace{-4mm}
\end{tablenotes}
\end{threeparttable}
\end{table*}
}

\newcommand{\TableAblationSegvsPoint}{
         & 0.2 & 0.5 & 0.7 & Avg \\
        \headrule
        Segment & 33.1  & 20.1  & 9.8  & 19.4  \\
        \rowcolor{Gray}
        Point& {\bf 36.6} & {\bf 22.8} & {\bf 10.6} & {\bf 21.5} \\
}


\newcommand{\TableAblationInitialization}{
         Init. & 0.2 & 0.5 & 0.7 & Avg \\
        \headrule
        Uniform(0,1) & 36.9  & 22.6  & 10.0   &  21.3 \\
        Norm(0.5,0.3) &  {\bf 37.0} & 21.9  & 9.4  & 21.2 \\
        \rowcolor{Gray}
        Const.(0.5) & 36.6 & {\bf 22.8} & {\bf 10.6}& {\bf 21.5} \\
}

\newcommand{\TableAblationNumPoints}{
          & 0.2 & 0.5 & 0.7 & Avg \\
        \headrule
        9 & 36.2  & 22.3  & 10.3  & 21.0 \\
        15 & {\bf 36.6}  & {\bf 22.8} & 10.4  & 21.4 \\
        \rowcolor{Gray}
        21 & {\bf 36.6} & {\bf 22.8} & {\bf 10.6}& {\bf 21.5}  \\
        27 & {\bf 36.6}  & 22.6 & {\bf 10.6} & 21.4 \\
}


\newcommand{\TableAblationOnMixing}{
        Mixing & 0.2 & 0.5 & 0.7 & Avg \\
        \headrule
        Frame Only  & 35.7 & 22.3 & 10.2  & 20.9   \\
        Channel Only & 34.2  & 21.4 & 9.8  & 20.1 \\
        FrameChannel & 34.3 & 21.4 & 9.2 & 19.9 \\
        ChannelFrame & 30.7 & 17.6 & 6.7 & 17.1 \\
        \rowcolor{Gray}
        Parallel Mixing & {\bf 36.6} & {\bf 22.8} & {\bf 10.6} & {\bf 21.5}  \\
}

\newcommand{\TableAblationOnDeform}{
         Deform. & \multirow{2}{*}{0.2}& \multirow{2}{*}{0.5} & \multirow{2}{*}{0.7} & \multirow{2}{*}{Avg} \\
         Conv. &  &  &  &  \\
        \headrule
        \rowcolor{Gray}
        \checkmark &{\bf 36.6} & {\bf 22.8} & {\bf 10.6} & {\bf 21.5}  \\
          & 35.7 & 22.1 & 9.9 & 20.8 \\
}

\newcommand{\TableAblationOnConvert}{
         & 0.2 & 0.5 & 0.7 & Avg \\
        \headrule
        Min-max & 36.5  & {\bf 22.9}  & {\bf 10.9}& {\bf 21.6 }\\
        \rowcolor{Gray}
        Partial &  &  &  & \\
        \rowcolor{Gray}
        Min-max & \multirow{-2}{*}{\bf 36.6}& \multirow{-2}{*}{ 22.8}& \multirow{-2}{*}{ 10.6} & \multirow{-2}{*}{21.5}\\ 
}

\newcommand{\TableAblationStudiesIntegrated}{
    \begin{table*}[t]
        \vspace{-.4em}
        \caption{\textbf{PointTAD Ablation experiments} on MultiTHUMOS. Default setting is colored \colorbox{Gray}{gray}.
        }
        \centering
\subfloat[\small
        \textbf{Segments} vs. \textbf{Query Points} in query-based action detectors
        \label{tab:segmentvspoint}
        ]{
        \centering
        \begin{minipage}{0.3\linewidth}{\begin{center}
        \tablestyle{1pt}{1.1}
        \small
        {
        \begin{tabular}{ccccc}
            \TableAblationSegvsPoint
        \end{tabular}
        }
        \end{center}}\end{minipage}
        }
        \hspace{1em}
\subfloat[\small
        \textbf{Initialization} of query points at training.
        \label{tab:initialization}
        ]{
        \begin{minipage}{0.3\linewidth}{\begin{center}
        \tablestyle{1pt}{1.1}
        \small
        \begin{tabular}{ccccc}
            \TableAblationInitialization
        \end{tabular}
        \end{center}}\end{minipage}
        }
        \hspace{1em}
\subfloat[\small
        \textbf{Point2Segment Transformation } .
        \label{tab:convert}
        ]{
        \begin{minipage}{0.3\linewidth}{\begin{center}
        \tablestyle{1pt}{1.1}
        \small
        \begin{tabular}{ccccc}
            \TableAblationOnConvert
        \end{tabular}
        \end{center}}\end{minipage}
        }\\\vspace{0.5em}
\subfloat[\small
        \textbf{Number of Query Points}  per query.
        \label{tab:n_query_points}
        ]{
        \centering
        \begin{minipage}{0.3\linewidth}{\begin{center}
            \tablestyle{1pt}{1.1}
        \small
        \begin{tabular}{x{32}x{18}x{18}x{18}x{18}}
            \TableAblationNumPoints
        \end{tabular}
        \end{center}}\end{minipage}
        }
        \hspace{1em}
\subfloat[\small
        \textbf{Point-Level}: Deformable Convolution.
        \label{tab:deform_conv}
        ]{
        \centering
        \begin{minipage}{0.3\linewidth}{\begin{center}
        \tablestyle{3pt}{1.1}
        \small
        \begin{tabular}{ccccc}
            \TableAblationOnDeform
        \end{tabular}
        \end{center}}\end{minipage}
        }
        \hspace{1em}
\subfloat[\small
        \textbf{Instance-Level}: different variants of mixing strategy.
        \label{tab:mixing}
        ]{
        \centering
        \begin{minipage}{0.3\linewidth}{\begin{center}
        \tablestyle{1pt}{1.2}
\scalebox{0.9}{
        \begin{tabular}{ccccc}
            \TableAblationOnMixing
        \end{tabular}}
        \end{center}}\end{minipage}
        }
        \\
        \vspace{0.5em}
        
        \label{tab:ablations} \vspace{-2.0em}
        \end{table*}
}

\newcommand{\TableAblationBeta}{
      & 0    & 0.2           & 0.4  & 0.6  & 0.8 & 0.96 & 1    \\ \headrule
MultiTHUMOS  & 33.0 & \cellcolor{Gray}{\bf 39.8} & 39.2 & 38.1 & 37.3 & 36.8 & 35.9 \\ 
Charades     & 13.8 & 14.3 & 15.1 & 16.6 & 19.2 & \cellcolor{Gray}{\bf 21.0} & 18.7 \\ 
}

\newcommand{\TableAblationOnS}{
 & 0.2 & 0.5 & 0.7 & Avg \\ \headrule
scale to clip duration      & 36.4 & 21.9 & 9.8 & 21.0 \\ 
\rowcolor{Gray}
scale to action duration & {\bf 36.6} & {\bf 22.8} & {\bf 10.6} & {\bf 21.5} \\ 
}

\newcommand{\TableExtendAblate}{
    \begin{table*}[t]
        \vspace{-.2em}
        \caption{{Ablation study w.r.t. \textbf{fusion parameter } and {\bf scaling parameter } on MultiTHUMOS.}
        }
\subfloat[\small
        {\textbf{Result fusion parameter }:  indicates full sparse detection and  indicates full dense results. The results are reported under segmentation-mAP.}
        \label{tab:beta}
        ]{
        \begin{minipage}{0.5\linewidth}{\begin{center}
        \tablestyle{1pt}{1.1}
        \setlength{\tabcolsep}{2.2pt}
        \small
        \begin{tabular}{cccccccc}
            \TableAblationBeta
        \end{tabular}
        \end{center}}\end{minipage}
        }
        \hspace{1em}
\subfloat[\small
        {\textbf{Offset scaling parameter }: scale to window size vs. scale to action duration. The results are reported under detection-mAP@tIoU.\label{tab:s}}
        ]{
        \begin{minipage}{0.4\linewidth}{\begin{center}
        \tablestyle{1pt}{1.1}
        \setlength{\tabcolsep}{3pt}
        \small
        \begin{tabular}{ccccc}
            \TableAblationOnS
        \end{tabular}
        \end{center}}\end{minipage}
        }
        \\
        \label{tab:extendablation}  \vspace{-3em}
        \end{table*}
}




\newcommand{\TableTAD}{
\begin{table}[ht]
    \centering
    \small
    \renewcommand\arraystretch{0.9}
    \setlength{\tabcolsep}{15pt}
    \caption{\textbf{Comparison with the state-of-the-art single-label TAD models} on MultiTHUMOS test set with under detection-mAP (\%). The single-label TAD methods are reproduced with RGB input only.}
    \vspace{1em}
    \begin{tabular}{@{}ccccccc@{}}
\headrule
Methods       & 0.1  & 0.2  & 0.3  & 0.4  & 0.5  & Average \\ \midrule
BSN~\cite{DBLP:conf/eccv/LinZSWY18}+P-GCN~\cite{DBLP:conf/iccv/ZengHGTRZH19}     
&  22.2    &  20.0    &  16.7    &  12.5    & 8.5    &  10.0      \\ 
BSN~\cite{DBLP:conf/eccv/LinZSWY18}+ContextLoc~\cite{DBLP:conf/iccv/ZhuT00021}         
& 22.9 &  21.0  &  18.0  & 14.6  & 10.8  &   11.0      \\
AFSD~\cite{DBLP:conf/cvpr/Lin0LWTWLHF21}          
& 30.5 & 27.4 & 23.7 & 19.0 & 14.0 & 14.7 \\ \midrule
Ours          
& {\bf 42.3} & {\bf 39.7} & {\bf 35.8} & {\bf 30.9} & {\bf 24.9} & {\bf 23.5} \\ \bottomrule
\end{tabular}
    \label{tab:sota_tad}
\end{table}
}

\newcommand{\TableErrorBar}{
\begin{table}[ht]
\centering
    \small
    \renewcommand\arraystretch{1}
    \setlength{\tabcolsep}{2.5pt}
    \caption{\textbf{Error Bar} on MultiTHUMOS test set and Charades test set under detection-mAP (\%).}
    \vspace{1em}
    \begin{tabular}{@{}cccccccc@{}}
\headrule
 & Dataset       & 0.1  & 0.2  & 0.3  & 0.4  & 0.5  & Average \\ \midrule
\multirow{2}{*}{PointTAD} & MultiTHUMOS  
& {41.161.08} & {38.571.04} & {34.960.80} & {30.340.61} & { 24.850.30} & { 23.100.34}   \\
& Charades
& {17.710.41} & {17.050.38} & {15.980.39} & {14.740.28} & {13.200.25} & {11.920.23} \\ \midrule
\multirow{2}{*}{PointTAD} &MultiTHUMOS  
& {39.200.10} & {36.660.04} & {33.320.03} & {28.510.07} & {22.900.36} & {21.510.06}  \\
& Charades
& {16.720.26} & {16.110.28} & {15.170.26} & {14.000.26} & {12.680.29} & {11.360.20} \\ 
\bottomrule
\end{tabular}
\begin{tablenotes}
\item \small{\em  indicates results trained with stronger image augmentation as in \cite{liu2022an}.}
\end{tablenotes}
    \label{tab:error_bar}
\end{table}
}


\newcommand{\TableTHUMOS}{
\begin{table}[ht]
    \centering
    \small
    \renewcommand\arraystretch{1}
    \setlength{\tabcolsep}{15pt}
    \caption{\textbf{Evaluation on standard TAD benchmark THUMOS14} under detection-mAP (\%).}
    \vspace{1em}
    \begin{tabular}{@{}ccccccc@{}}
\headrule
Methods       & 0.3  & 0.4  & 0.5  & 0.6  & 0.7  & Average \\ \midrule
RTD + UNet & 58.5 & 53.1 & 45.1 & 36.4 & 25.0 & 43.6 \\ 
PointTAD  & {62.6} & {55.9} & {46.2} & 35.3 & 22.8 & {44.6} \\ 
 \bottomrule
\end{tabular}
    \label{tab:thumos14}
\end{table}
}

\newcommand{\TableBaseline}{
\begin{table}[ht]
    \centering
    \small
    \renewcommand\arraystretch{0.9}
    \setlength{\tabcolsep}{18pt}
    \caption{{\textbf{Comparison with query-based baseline} under detection-mAP (\%).}}
    \vspace{1em}
    \begin{tabular}{@{}ccccc@{}}
\headrule
Methods                     & 0.2  & 0.5  & 0.7  & Avg  \\ \midrule
DETR-alike baseline         & 26.1 & 16.9 & 7.7 & 15.5 \\ 
Sparse R-CNN alike baseline & 33.1 &  20.1 & 9.8 & 19.4 \\ 
PointTAD                    & {\bf 36.6} & {\bf 22.8} & {\bf 10.6} & {\bf 21.5} \\ 
 \bottomrule
\end{tabular}
    \label{tab:baseline}
\end{table}
}


\newcommand{\TableAblationSpatialResolution}{
          & 0.2 & 0.5 & 0.7 & Avg \\
        \headrule
         & 28.1 & 16.4 & 7.5  & 16.0 \\
         & 33.8 & 21.3 & 10.1  & 19.9 \\
         & 35.8 & 22.4 & 10.2 & 20.9  \\
        \rowcolor{Gray}
         & {\bf 36.6} & {\bf 22.8} & {\bf 10.6} & {\bf 21.5} \\
}

\newcommand{\TableAblationOnL}{
          & 0.2 & 0.5 & 0.7 & Avg \\
        \headrule
        3 & 35.1 & 21.7 & 9.0 & 20.2 \\
        \rowcolor{Gray}
        4 & {\bf 36.6} & {\bf 22.8} & {\bf 10.6} &{\bf 21.5} \\
        5 & 35.9 & 22.6 & 10.2 & 21.1  \\
        6 & \multicolumn{4}{c}{Out of Memory}\\
}

\newcommand{\TableEtoE}{
    \begin{table*}[ht]
        \vspace{-.2em}
        \caption{Ablation experiments w.r.t. \textbf{E2E training} on MultiTHUMOS. Default setting is colored \colorbox{Gray}{gray}.
        }
        \centering
\subfloat[\small
        \textbf{Input spatial resolution}: from  to .
        \label{tab:resolution}
        ]{
        \centering
        \begin{minipage}{0.5\linewidth}{\begin{center}
        \tablestyle{1pt}{1.1}
        \small
        \begin{tabular}{x{48}x{24}x{24}x{24}x{24}}
            \TableAblationSpatialResolution
        \end{tabular}
        \end{center}}\end{minipage}
        }
        \hspace{-2em}
\subfloat[\small
        \textbf{Number of decoder layers} .
        \label{tab:n_decoder_layer}
        ]{
        \centering
        \begin{minipage}{0.5\linewidth}{\begin{center}
        \tablestyle{1pt}{1.1}
        \small
        \begin{tabular}{x{48}x{24}x{24}x{24}x{24}}
            \TableAblationOnL
        \end{tabular}
        \end{center}}\end{minipage}
        }
        \\
        \vspace{0.5em}
        
        \label{tab:e2e} \vspace{-1.0em}
        \end{table*}
} \begin{document}

\maketitle


\begin{abstract}
  Traditional temporal action detection (TAD) usually handles untrimmed videos with small number of action instances from a single label (e.g., ActivityNet, THUMOS). However, this setting might be unrealistic as different classes of actions often co-occur in practice. 
  In this paper, we focus on the task of multi-label temporal action detection that aims to localize all action instances from a multi-label untrimmed video. 
  Multi-label TAD is more challenging as it requires for fine-grained class discrimination within a single video and precise localization of the co-occurring instances. 
  To mitigate this issue, we extend the sparse query-based detection paradigm from the traditional TAD and propose the multi-label TAD framework of PointTAD. 
  Specifically, our PointTAD introduces a small set of learnable query points to represent the important frames of each action instance. This point-based representation provides a flexible mechanism to localize the discriminative frames at boundaries and as well the important frames inside the action.
  Moreover, we perform the action decoding process with the Multi-level Interactive Module to capture both point-level and instance-level action semantics. Finally, our PointTAD employs an end-to-end trainable framework simply based on RGB input for easy deployment. We evaluate our proposed method on two popular benchmarks and introduce the new metric of detection-mAP for multi-label TAD. Our model outperforms all previous methods by a large margin under the detection-mAP metric, and also achieves promising results under the segmentation-mAP metric.
  Code is available at \href{https://github.com/MCG-NJU/PointTAD}{https://github.com/MCG-NJU/PointTAD}.
\end{abstract}


\section{Introduction}
With the increasing amount of video resources on the Internet, video understanding is becoming one of the most important topics in computer vision. Temporal action detection (TAD) ~\cite{DBLP:journals/ijcv/ZhaoXWWTL20,DBLP:conf/eccv/LinZSWY18, DBLP:conf/iccv/LinLLDW19, DBLP:conf/cvpr/ChaoVSRDS18, DBLP:conf/aaai/GaoSWLYGZ20, DBLP:conf/cvpr/Lin0LWTWLHF21, DBLP:conf/iccv/TanT0W21} has been formally studied on traditional benchmarks such as THUMOS~\cite{DBLP:journals/cviu/IdreesZJGLSS17}, ActivityNet~\cite{DBLP:conf/cvpr/HeilbronEGN15}, and HACS~\cite{DBLP:conf/iccv/Zhao0TY19}. 
However, the task seems impractical because their videos almost contain non-overlapping actions from a single category:  videos in THUMOS are annotated with single action category. As a result, most TAD methods~\cite{DBLP:conf/eccv/LinZSWY18,DBLP:conf/iccv/LinLLDW19,DBLP:conf/cvpr/XuZRTG20,DBLP:journals/corr/abs-2112-03612,DBLP:conf/aaai/SuGWQY21} simply cast this TAD problem into sub-problems of action proposal generation and global video classification~\cite{DBLP:conf/cvpr/WangXLG17}.
In this paper, we shift our playground to the more complex setup of multi-label temporal action detection, which aims to detect all action instances from multi-labeled untrimmed videos. Existing works~\cite{DBLP:conf/wacv/DaiDMGFB21,DBLP:conf/cvpr/KahatapitiyaR21,DBLP:conf/cvpr/TirupatturDRS21,DBLP:conf/cvpr/DaiDKRB22} in this field formulate the problem as a dense prediction task and perform multi-label classification in a frame-wise manner. 
Consequently, these methods are weak in localization and fail to provide the instance-level detection results (i.e., the starting time and ending time of each instance).
In analogy to image instance segmentation~\cite{DBLP:conf/eccv/LinMBHPRDZ14}, we argue that it is necessary to redefine multi-label TAD as a instance-level detection problem rather than a frame-wise segmentation task.
In this sense, multi-label TAD results not only provide the action labels, but also the exact temporal extent of each instance.


Direct adaptation of action detectors is insufficient to deal with the challenges of concurrent instances and complex action relations in multi-label TAD. 
The convention of extracting action features from {\bf action segments}~\cite{DBLP:conf/cvpr/ChaoVSRDS18,DBLP:conf/cvpr/QingSGW0W0YGS21,DBLP:conf/iccv/ZengHGTRZH19,DBLP:journals/corr/abs-2109-08847} lacks the flexibility of handling both important semantic frames inside the instance as well as discriminative boundary frames. Consider the groundtruth action of {\em ``Jump''} in \cref{fig:segment}, segment-based action detectors mainly produce two kinds of error predictions, as in type  and type .  successfully predicts the correct action category with an {\bf incomplete} segment of action highlights, whereas  does a better job in locating the boundaries yet get {\bf misclassified} as {\em ``Sit''} due to the inclusion of confusing frames.  
In addition, most action detectors~\cite{DBLP:conf/iccv/ZengHGTRZH19,DBLP:conf/cvpr/QingSGW0W0YGS21, DBLP:journals/corr/abs-2106-10271} are inadequate in processing sampled frames and classifying fine-grained action classes. They often exploit temporal context modeling at a single level and ignore the exploration of channel semantics. 

To address the above issues, we present PointTAD, a sparse query-based action detector that leverages learnable query points to flexibly attend important frames for action instances. 
Inspired by RepPoints~\cite{DBLP:conf/iccv/YangLHWL19}, the query points are directly supervised by regression loss. Given specific regression targets, the query points learn to locate at discriminative frames at action boundaries as well as semantic key frames within actions. Hence, concurrent actions of different categories can yield distinctive features through the specific arrangement of query points. Moreover, we improve the action localization and semantics decoding by proposing the Multi-level Interactive Module with dynamic kernels based on query vector. Direct interpolation or pooling at each query point lacks temporal reasoning over consecutive frames. Following deformable DETR~\cite{DBLP:conf/iclr/ZhuSLLWD21}, we extract point-level features with deformable convolution~\cite{DBLP:conf/iccv/DaiQXLZHW17,DBLP:conf/cvpr/ZhuHLD19} from a local snippet to capture the temporal cues of action change or important movement. At instance-level, both temporal context and channel semantics are captured with frame-wise and channel-wise dynamic mixing~\cite{DBLP:conf/cvpr/SunZJKXZTLYW021,DBLP:journals/corr/abs-2203-16507} to further decode the distinctive features of simultaneous actions. 

PointTAD streamlines end-to-end TAD with joint optimization of the backbone network and action decoder without any post-processing technique. We validate our model on two challenging multi-label TAD benchmarks. Our model achieves the state-of-the-art detection-mAP performance and competitive segmentation-mAP performance to previous methods with RGB input.

\begin{figure}
  \centering
  \vspace{-2mm}
  \includegraphics[scale=0.5]{figs/seg_detectors.pdf}
  \caption{Illustration of action predictions by segment-based action detectors in multi-label TAD.} 
  \label{fig:segment}
  \vspace{-3mm}
\end{figure}

\section{Related Work}
\paragraph{Multi-label temporal action detection.}  
Multi-label temporal action detection has been studied as a multi-label frame-wise classification problem in the previous literature. Early methods~\cite{DBLP:conf/cvpr/PiergiovanniR18, DBLP:conf/icml/PiergiovanniR19} paid a lot of attention on modeling the temporal relations between frames with the help of Gaussian filters in temporal dimension. Other works integrated features at different temporal scales with dilated temporal kernels~\cite{DBLP:conf/wacv/DaiDMGFB21} or iterative convolution-attention pairs~\cite{DBLP:conf/cvpr/DaiDKRB22}. Recently, attention has shifted beyond temporal modeling. Coarse-Fine~\cite{DBLP:conf/cvpr/KahatapitiyaR21} handled different temporal resolutions in the slow-fast fashion and performed spatial-temporal attention during fusion. 
MLAD~\cite{DBLP:conf/cvpr/TirupatturDRS21} used multi-head self-attention blocks at both spatial and class dimension to model class relations at each timestamp. In our proposed method, we view the task as a instance-level detection problem and employ query-based framework with sparse temporal points for accurate action detection. In addition, we study the temporal context at different semantic levels, including inter-proposal, intra-proposal and point-level of modeling.

\paragraph{Segment-based representation.} 
Following the prevailing practice of bounding boxes~\cite{DBLP:conf/cvpr/LinDGHHB17, DBLP:journals/pami/HeGDG20, DBLP:conf/nips/RenHGS15, DBLP:conf/cvpr/CaiV18} in object detection, existing temporal action detectors incorporated action segments heavily with three kinds of usage: as anchors, as intermediate proposals, and as final predictions. 
Segments as anchors are explored mainly in anchor-based frameworks. These methods~\cite{DBLP:conf/iccv/OneataVS13, DBLP:conf/mir/MettesGCMS15,DBLP:conf/iccv/ZengHGTRZH19, DBLP:conf/cvpr/QingSGW0W0YGS21} used sliding windows or pre-computed proposals as anchors. 
Most TAD methods \cite{DBLP:conf/iccv/ZengHGTRZH19,DBLP:conf/cvpr/Lin0LWTWLHF21,DBLP:conf/cvpr/XuZRTG20,DBLP:conf/cvpr/ChaoVSRDS18,DBLP:journals/ijcv/ZhaoXWWTL20,DBLP:conf/eccv/ZhaoXJZW020} use segments as intermediate proposals. Uniform sampling or pooling are commonly used to extract features from these segments. 
P-GCN~\cite{DBLP:conf/iccv/ZengHGTRZH19} applied max-pooling within local segments for proposal features. G-TAD~\cite{DBLP:conf/cvpr/XuZRTG20} uniformly divided segments into bins and average-pooled each bin to obtain proposal features. 
AFSD~\cite{DBLP:conf/cvpr/Lin0LWTWLHF21} proposed boundary pooling in boundary region to refine action feature.  
Segments as final predictions are employed among all TAD frameworks, because segments generally facilitate the computation of action overlaps and loss functions. 
Instead, in this paper, we do not need segments as anchors and directly employ learnable query points as intermediate proposals with iterative refinement. The learnable query points represent the important frames within action and action feature is extracted only from these keyframes rather than using RoI pooling. 

\paragraph{Point-based representation.}
Several existing works have used point representations to describe keyframes~\cite{DBLP:journals/tcsv/GuanWLDF13,DBLP:journals/ijon/TangLXS19}, objects~\cite{DBLP:conf/iccv/YangLHWL19,DBLP:journals/corr/abs-2203-16507}, tracks~\cite{DBLP:conf/eccv/ZhouKK20}, and actions~\cite{DBLP:conf/eccv/LiW0W20}. \cite{DBLP:journals/tcsv/GuanWLDF13,DBLP:journals/ijon/TangLXS19} tackled keyframe selection by operating greedy algorithm on spatial SIFT keypoints~\cite{DBLP:journals/tcsv/GuanWLDF13} or clustering on local extremes of image color/intensity~\cite{DBLP:journals/ijon/TangLXS19}. These methods followed a bottom-up strategy to choose keyframes based on local cues. In contrast, PointTAD represents action as a set of temporal points (keyframes). We follow RepPoints~\cite{DBLP:conf/iccv/YangLHWL19} to handle the important frames of actions with point representations and refine these points by action feature iteratively. Our method directly regresses keyframes from query vectors in a top-down manner for more flexible temporal action detection. Note that PointTAD tackles different tasks from RepPoints~\cite{DBLP:conf/iccv/YangLHWL19}. We also built PointTAD upon a query-based detector, where a small set of action queries is employed to sparsely attend the frame sequence for potential actions, resulting in an efficient detection framework.

\paragraph{Temporal context in videos.}
Context aggregation at different levels of semantics is crucial for temporal action modeling~\cite{DBLP:conf/cvpr/0002TJW21} and has been discussed in previous TAD methods. G-TAD~\cite{DBLP:conf/cvpr/XuZRTG20} treated each snippet input as graph node and applied graph convolution networks to enhance snippet-level features with global context. ContextLoc~\cite{DBLP:conf/iccv/ZhuT00021} handled action semantics in hierarchy: it updated snippet features with global context, obtained proposal features with frame-wise dynamic modeling within each proposal and modeled the inter-proposal relations with GCNs. Although we considered the same levels of semantic modeling, our method is different from ContextLoc. PointTAD focuses on aggregating temporal cues at multiple levels, with deformable convolution at point-level as well as {\em frame} and {\em channel} attentions at intra-proposal level. We also apply multi-head self-attention for inter-proposal relation modeling.

\section{PointTAD}
\label{sec:method}

We formulate the task of multi-label temporal action detection (TAD) as a set prediction problem. Formally, given a video clip with  consecutive frames, we predict a set of action instances ,  is the number of learnable queries,  are the starting and ending timestamp of the -th detected instance,  is its action category. The groundtruth action set to detect are denoted , where  are the starting and ending timestamp of the -th action,  is the groundtruth action category,  is the number of groundtruth actions. 

The overall architecture of PointTAD is depicted in \cref{fig:overview}. PointTAD consists of a {\bf video encoder} and an {\bf action decoder}. The model takes three inputs for each sample: RGB frame sequence of length , a set of learnable query points , and query vectors . Learnable query points explicitly describe the action locations by positioning themselves around action boundaries and semantic key frames, and the query vectors decode action semantics and locations from the sampled features. In the model, the video encoder extracts video features  from RGB frames. The action decoder contains  stacked decoder layers and takes query points , query vectors  and video features  as input.
Each decoder layer contains two parts: 1) the {multi-head self-attention} block models the pair-wise relationship of query vectors and establishes inter-proposal modeling for action detection; 2) the {\bf Multi-level Interactive Module} models the point-level and instance-level semantics with dynamic weights based on query vector. 
Overall, the action decoder aggregates the temporal context at {\bf point-level}, {\bf intra-proposal} level and {\bf inter-proposal} level. 
Finally, we use two linear projection heads to decode action labels from query vectors, and transform query points to detection outputs.

\begin{figure}
  \centering
  \includegraphics[scale=0.5]{figs/main_arch.pdf}
  \caption{{\bf Pipeline of PointTAD}. It consists of a backbone network that extracts video features from consecutive RGB frames and an action decoder of  layers that directly decodes actions from video features. 
  PointTAD enables end-to-end training of backbone and action decoder without any post-processing of predictions. }
  \label{fig:overview}
  \vspace{-4mm}
\end{figure}

\subsection{Video Encoder}
We use the I3D backbone~\cite{DBLP:conf/cvpr/CarreiraZ17} as the video encoder in our framework. The video encoder is trained end-to-end with the action decoder and optimized by action detection loss to bridge the domain gap between action recognition and detection. 
For easy deployment of our framework in practice, we avoid the usage of optical flow due to its cumbersome preparation procedure. 
In order to achieve good performance on par with two-stream features by only using RGB input, we follow \cite{DBLP:journals/corr/abs-2205-02717} to remove the temporal pooling at  and fuse the features from  with features from  as in  \cite{liu2022an}. As a result, the temporal stride of encoded video features is 4. Spatial average pooling is performed to squeeze the spatiotemporal representations from backbone to temporal features.

\subsection{Learnable Query Points}
Segment-based action representation (i.e., representing each action instance simply with a starting and ending time) is limited in describing its boundary and content at the same time. To increase the representation flexibility, we present a novel point-based representation to automatically learn the positions of action boundary as well as its semantic key frames inside the instance. Specifically, the point-based representation is denoted by  for each query, where  is the temporal location of  query point, and the point quantity per query is  and set to 21 empirically. 
We explain the updating strategy and the learning of query points below.

\paragraph{Iterative point refinement.}
During training, the query points are initially placed at the midpoint of the input video clip. Then, they are refined by query vectors  through iterations of decoder layers to reach final positions. To be specific, at each decoder layer, the query point offsets are predicted from updated query vector (see \cref{sec:mim}) by linear projection. We design a self-paced updating strategy with adaptive scaling for each query at each layer to stabilize the training process. At decoder layer , the query points for one query are represented by .  The  offsets are denoted . The refinement can be summarized as:

where  is the scaling parameter and describes the span of query points at layer . As a result, the updated step size gets smaller for shorter action, which helps with the localization of short actions. Updated query points from previous layer are inputs to the next layer. 

\paragraph{Learning query points.} 
The training of query points is directly supervised by regression loss at both intermediate and final stages. We follow \cite{DBLP:conf/iccv/YangLHWL19} to transform query points to pseudo segments for regression loss calculation. The resulted pseudo segments participate in the calculation of L1-loss and tIoU loss with groundtruth action segments in both label assignment and loss computation. 

The transformation function is denoted by . We experiment with two kinds of functions: Min-max  and Partial min-max .  {\em Min-max} is to take the minimum and maximum location from the set of query points as the starting and ending timestamp of the pseudo segment, . With Min-max transformation, the query points are strictly bounded within the local segment of the target action instance. {\em Partial min-max} function is to select a subset of query points  and perform the min-max function on them to determine a pseudo segment, .
It allows several query points to aggregate information from outside the action. Empirically, we choose partial min-max by default. For each query, we randomly take  points from the query point set to form . 

\begin{figure}[t]
  \centering
  \includegraphics[scale=0.7]{figs/mim.pdf}
  \caption{{\bf Multi-level Interactive Module} aggregates action semantics at point-level and instance-level with dynamic parameters. }
  \vspace{-2mm}
  \label{fig:mim}
\end{figure}

\subsection{Multi-level Interactive Module}
\label{sec:mim}
Apart from the limitation in the segment-based representation, previous temporal action detectors are also insufficient in decoding the sampled frames. 
These methods seldom consider semantic aggregation from different levels and at multiple aspects. Accordingly, we present a multi-level interactive module that consider both {\bf local temporal cues at point-level} and {\bf intra-proposal relation modeling at instance-level}, depicted in \cref{fig:mim}. To achieve distinct representations for each query, these parameters at both levels are dynamically generated based on the query vector.

\paragraph{Point-level local deformation.} Paired with the refined point representation, we employ deformable convolutions to extract point features within the local neighborhood. For the  query point, we predict four temporal offsets  from the point location and corresponding weights . The query point at frame  acts as the center point and is added with temporal offsets to achieve four deformable sub-points. These sub-points characterize the local neighborhood of center points. The features at sub-points are extracted by bilinear interpolation, then multiplied with weights and combined together to get point-level feature . This process can be expressed as:

Both the offsets and weights are generated from query vectors  by linear projection,

where the weights are additionally normalized by softmax for each query.

\paragraph{Instance-level adaptive mixing.}
Faced with the challenge of simultaneous actions, with only temporal modeling, actions with large overlap may result in similar representation and harms the classification. To tackle this problem, we propose adaptive mixing at both frames and channels by using dynamic convolutions. 
Specifically, the stacked features of query points is denoted by . Given the query vector {}, we generate dynamic parameters for frame mixing and channel mixing: 

Frame mixing is carried out with dynamic projection followed by LayerNorm and ReLU activation on  points to explore intra-proposal frame relations:

Similar to frame mixing, channel mixing uses two bottle-necked layers of dynamic projection on the channel dimension to enhance action semantics:

These two mixed features are concatenated along channel and squeezed by linear operations to the size of query vector. The query vector  at the  layer is then updated with residual connection:

Finally, the query point offsets and the action labels are decoded from the updated vector by two linear projection heads to produce detection results at each layer.

\subsection{Training and Inference}
\paragraph{Label assignment.} Similar to all query-based detectors~\cite{DBLP:conf/eccv/CarionMSUKZ20,DBLP:conf/iclr/ZhuSLLWD21,DBLP:journals/corr/abs-2203-16507,DBLP:conf/iccv/TanT0W21}, we apply hungarian matcher on (detected) pseudo segments to search for the optimal permutation  for label assignment. The groundtruth set  on each video clip is extended with no action  to the size of . The matching cost is formulated as:

By minimizing the matching cost, the bipartite matching algorithm finds the optimal permutation  that assigns each prediction with a target.  is set to  respectively.

\paragraph{Loss functions.} PointTAD is jointly optimized by localization loss and classification loss. We use  loss and iou loss as localization loss:

The cross-entropy loss between query labels and target labels is used as classification loss. In addition, to improve the performance under segmentation-mAP, we generate dense classification scores  by linear projection from video features . Cross-entropy loss is enforced on  with dense groundtruth . Therefore, the classification loss is composed of the set prediction loss and the dense per-frame loss:

The overall loss function is formulated as follows:

,  and  are hyper-parameters that are set to  respectively.

\paragraph{Inference.}
During inference, our PointTAD uses a single linear projection layer followed by LayerNorm and ReLU activation to predict class labels from query vectors. As for localization, we use pseudo segments transformed from query points as the final predictions. These sparse predictions are then evaluated under the detection-mAP metric. Additional dense scores  could be generated at video features for the segmentation-mAP calculation. 
The sparse predictions are filtered with threshold  and processed with Gaussian kernels to approximate dense scores at each frame. Then, the approximated scores are added with weight  to the predicted dense scores for segmentation-mAP calculation. The final dense scores are generated as:

Note that the sparse predictions are adjusted by dense scores only for segmentation-mAP.

\section{Experiments}
\vspace{-0.3em}
\subsection{Datasets and Setup}
\vspace{-0.3em}
\paragraph{Datasets.}
We conduct experiments on two popular multi-label action detection benchmarks: MultiTHUMOS~\cite{DBLP:journals/ijcv/YeungRJAMF18} and Charades~\cite{DBLP:conf/eccv/SigurdssonVWFLG16}. {\bf MultiTHUMOS} is a densely labeled dataset extended from THUMOS14. It includes 413 sports videos of 65 classes. The average number of distinctive action categories per video in MultiTHUMOS is 10.5, compared with 1.1 in THUMOS14. {\bf Charades} is a large Multi-label TAD dataset that contains 9848 videos of daily in-door activities. The annotations are spread over 157 action classes, with an average of 6.8 instances per video.

\paragraph{Implementation details.}
\label{parag:implement_details}
With I3D backbone network, we extract frames at 10 fps for MultiTHUMOS and 12 fps for Charades. The spatial resolution is set to  for both datasets. {We report ablations with only \texttt{Center\_Crop} for training, and report comparison in \cref{tab:sota} with stronger image augmentation following \cite{liu2022an}.} The video sequence is pre-processed with sliding window mechanism. To accommodate most of the actions, the window size is set to 256 frames for MultiTHUMOS (99.1\% actions included), and 400 frames for Charades (97.3\% actions included). The overlap ratio is 0.75 at training, and 0 at inference.  is set to 48 for both benchmarks. The number of query points per query  is 21. The number of deformable sub-points is set to 4 according to the number of sampling points in TadTR~\cite{DBLP:journals/corr/abs-2106-10271}.  {The optimal  is 0.01 for both datasets.}

Appropriate {\bf initialization} is required for backbone, query points and point-level deformable convolutions for stable training. Following common practice, the I3D backbone are initialized with Kinetics400~\cite{DBLP:journals/corr/KayCSZHVVGBNSZ17} pre-trained weights for MultiTHUMOS and Charades pre-trained weights for Charades. Query points are initialized with constant 0.5 in training and with learned weights in inference. Other possible initializations are explored in ablations. The linear layer to produce deformable offsets are initialized as follows: zeroing for weights and  for biases. The weights and biases to generate deformable weights are initialized as zero. 

We adopt AdamW as optimizer with 1e-4 weight decay. The network is trained on a server with 8 V100 GPUs. {The batch size is 3 per GPU for MultiTHUMOS and 2 per GPU for Charades. The learning rate is set to 2e-4 and drops by half at every 10 epochs.} Backbone learning rate is additionally multiplied with 0.1 for stable training.

\TableSOTA

\paragraph{Evaluation metrics.}
The default evaluation metric for multi-label TAD is segmentation-mAP, which is the frame-wise mAP. In addition, we extend the detection-mAP metric from traditional TAD to further evaluate the completeness of predicted action instances. The detection-mAP is the instance-wise mAP of action predictions under different tIoU thresholds. We report the average mAP as well as mAPs at tIoU threshold set  for both datasets. The average detection-mAP is calculated with tIoU thresholds set . We argue that detection-mAP is more reasonable for an instance detection task. 



\subsection{Comparison with the State-of-the-Art Methods}

In \cref{tab:sota}, we compare the performance of PointTAD with previous multi-label TAD methods under both segmentation-mAP and detection-mAP. The sparse prediction tuples of PointTAD are converted to dense segmentation scores with \cref{eq:sparse2dense} for segmentation-mAP. In order to compare the detection-mAP, we reproduce several previous multi-label temporal action localization methods and convert their dense segmentation results to sparse predictions following \cite{DBLP:journals/ijcv/YeungRJAMF18}. 
The prediction confidence score of an action with  consecutive frames is calculated as: 

where  is the probability for class  at frame ,  and  is the mean and standard derivation of action duration of class  in the training set.

Our PointTAD surpasses all previous multi-label TAD methods by a large margin under detection-mAP, indicating our ability to predict complete actions is beyond previous dense segmentation models. As for segmentation-mAP, we achieve encouraging and comparable results to the previous methods on both benchmarks with a sparse detection framework.\\
\begin{wrapfigure}{r}{0.4\textwidth}
    \centering
        \vspace{-8mm}
         \includegraphics[scale=0.32]{figs/qualitative.pdf}
         \vspace{-4mm}
         \caption{\small{\bf Qualitative results} on the MultiTHUMOS test set.}
         \label{fig:qualitative}
        \vspace{-8mm}
\end{wrapfigure}
In \cref{fig:qualitative}, we show the qualitative results of PointTAD on the MultiTHUMOS test set compared to the segment-based baseline, PDAN~\cite{DBLP:conf/wacv/DaiDMGFB21} and MS-TCT~\cite{DBLP:conf/cvpr/DaiDKRB22}. {PointTAD detects more instances at harder categories, such as ``Fall'' and ``SoccerPenalty''.}  MS-TCT and PDAN perform better at ``NoHuman'' category. We argue that this is because ``NoHuman'' class is not a well-defined action category with precise paired action boundaries, whereas PointTAD have to leverage pair relations of boundaries for localization. 

\TableAblationStudiesIntegrated

\subsection{Ablation Study}
\label{sec:ablations}
\paragraph{Segment-based representation vs point-based representation.} 
In \cref{tab:segmentvspoint}, we compare the performance between segment-based representation and point-based representation. In the segment-based baseline, actions are represented by segments of paired start-end timestamps. We apply temporal RoI align to the segments as in \cite{DBLP:journals/corr/abs-2109-08847,DBLP:conf/eccv/ZhaoXJZW020} to retrieve action features. Temporal RoI align divides segments into uniform bins and apply average pooling on each bin to transform frame features for query vector mixing. For fair comparison, the number of bins in RoI align is set to  and the parallel dynamic mixing is applied to both implementations. Results show that point-based representation significantly outperforms segment-based representation, demonstrating the advantage of adaptive sampling based on temporal points over grid alignment based on segments.

\paragraph{Study on query point initialization.}
We consider three different initialization for query points in training: uniform distribution in , normal distribution with mean= and std = , and initialization with constant . Results in \cref{tab:initialization} indicates that all three initialization methods are beneficial for the training, with little performance gap in avg-mAP. In addition, normal distribution and uniform distribution achieve higher detection-mAP with lower tIoU threshold, which shows these two initialization methods are weaker at accurate localization under higher tIoU threshold. Empirically, we set constant initialization by default.

\paragraph{Study on pseudo segment transformation functions.}
\cref{tab:convert} shows two alternatives of transformation function . Min-max and Partial min-max achieve similar performance under average detection-mAP. We choose Partial min-max as the default function because it offers a more relaxed constraint for query points with higher flexibility.

\paragraph{Study on the number of query points.}
We ablate with different numbers of query points  in \cref{tab:n_query_points}. In general, the performance is in proportion to the point quantity, although the increase of  only benefits the performance a little.  reaches the sweet spot at , where the performance no longer increases with larger . To balance the model complexity and performance, we set  to 21.

\paragraph{Study on point-level deformation.} 
In \cref{tab:deform_conv}, we compare point-level deformable convolution and direct interpolation for point-level feature extraction. Using deformable operator achieves higher performance than direct interpolation at query point location, demonstrating that it is beneficial to consider local context with dynamic modeling in action detection.

\paragraph{Study on instance-level mixing.}
The effect of frame-wise and channel-wise mixing is studied in \cref{tab:mixing}. We first ablate with the single application of each mixing, i.e. frame-only or channel-only. Results show that the performance degrades more without frame mixing, demonstrating the importance of frame mixing over channel mixing in multi-label TAD. Furthermore, we explore different combinations of frame and channel mixing with two cascaded alternatives, i.e. frame mixing  channel mixing and channel mixing  frame mixing. \cref{tab:mixing} shows that compared with frame-only and channel-only performances respectively, cascaded designs backfire at the performance with a decrease of 1\% avg-mAP at frame  channel and 3\% avg-mAP at channel  frame. To tackle action detection in video domain, intra-proposal channel mixing does not help with subsequent frame mixing and vice versa.

\TableExtendAblate

\paragraph{{Study on the result fusion parameter .}}
{Combining sparse detection results with dense segmentation scores provides smoother frame-level scores for segmentation-mAP. We ablate with choices of  on both datasets in \cref{tab:beta}.  is set to 0.2 for MultiTHUMOS and 0.96 for Charades based on empirical results.  
}

\paragraph{{Study on the offset scaling parameter .}}
{This scaling parameter is conventional in box-based object detectors~\cite{DBLP:conf/nips/RenHGS15,DBLP:conf/cvpr/CaiV18,DBLP:conf/cvpr/SunZJKXZTLYW021}, which is to scale regression offsets with respect to the box size instead of the image size. We extend this design to our PointTAD. In \cref{tab:s}, we compare the regression offsets predicted with respect to action duration (a.k.a offset scaled by duration) and with respect to clip duration (offset without scaling) on MultiTHUMOS. The result demonstrates the effectiveness of this scaling strategy on point-based detectors.}

\vspace{-0.2em}
\subsection{Query Points Visualization}
\vspace{-0.5em}
\cref{fig:query_points} illustrates the learned query points and the corresponding action target on a sample from MultiTHUMOS. Partial mix-max transformation divides the query points into confined local points (\textcolor{blue}{blue}) and open-ended global points (\textcolor{orange}{orange}). Local points are learned to attend action boundaries with two extreme points and semantic key frames with interior points. Global points sparsely distribute over the entire video for global context. Specifically, we observe that the interior local points effectively capture the most distinguished characteristic of target action {\em ``Run''}, by capturing rapid forward movement of {\em both} legs and neglecting frames of similar yet disruptive movements: hopping and scoring. Moreover, the iterative refining process of query points are depicted from initial positions to the final positions in the last decoder layer. Query points are learned to kick off from the median position in the window. After the first decoder layer, the query points are already able to coarsely locate the action target. Then, these query points automatically converge to finer localization along the decoder layers.

\begin{figure}
  \centering
  \vspace{-4mm}
  \includegraphics[scale=0.43]{figs/query_points.pdf}
  \caption{Visualization of {\bf learned query points} and the corresponding action groundtruth on a sample from MultiTHUMOS. }
  \label{fig:query_points}
  \vspace{-5mm}
\end{figure}

\vspace{-0.5em}
\section{Limitations and Future Work}
\label{sec:limitation}
\vspace{-0.5em}
PointTAD is proposed to solve the complex problem of multi-label TAD, by leveraging learnable query points for flexible and distinct action representation. Currently, we validate our model on two popular multi-label TAD benchmarks which include sport events and daily indoor activities. PointTAD achieves superior performance to all previous multi-label TAD methods as well as the state-of-the-art single-label TAD methods under detection-mAP metric. We have not demonstrated our model’s ability in general action detection in more diverse scenarios, including important tasks such as action spotting, sentence grounding, and so on. In the future, we would continue to explore the advantages of point-based action representation in broader scope for video understanding. Meanwhile, our PointTAD training still relies on the intermediate supervision and we hope to design more effective training paradigm for the query-based detection pipeline.

\vspace{-0.5em}
\section{Conclusion}
\vspace{-0.5em}
In this paper, we have studied the complex multi-label TAD that aims to detect all actions from a multi-label untrimmed video. We formulate this problem as a sparse detection task and extend the traditional query-based action detection framework from single-label TAD. Faced with the challenge of concurrent instances and complex action relations in multi-label TAD, we introduce a set of learnable query points to effectively capture action boundaries and characterize action semantics for fine-grained action modeling Moreover, to facilitate the decoding process, we propose the Multi-level Interactive Module that integrates action semantics at both point level and instance level, by using dynamic kernels based on query vector. Finally, PointTAD yields an end-to-end trainable architecture by using only RGB inputs for easy deployment in practice. Our PointTAD surpasses all previous methods by a large margin under the detection-mAP and achieves promising results under the segmentation-mAP on two popular multi-label TAD benchmarks. 

\begin{ack}
{This work is supported by National Natural Science Foundation of China (No. 62076119, No. 61921006), the Fundamental Research Funds for the Central Universities (No. 020214380091), and Collaborative Innovation Center of Novel Software Technology and Industrialization.}
\end{ack}


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage

\newpage

\appendix

\section{Appendix}


\subsection{Comparison with the State-of-the-Art Single-label TAD Methods}
In this sub-section, we report the detection-mAP results of some state-of-the-art single-label TAD methods (i.e. P-GCN~\cite{DBLP:conf/iccv/ZengHGTRZH19}, AFSD~\cite{DBLP:conf/cvpr/Lin0LWTWLHF21}, ContextLoc~\cite{DBLP:conf/iccv/ZhuT00021}) reproduced on multi-label TAD task. 
Since MultiTHUMOS share similar data preparation with THUMOS14, the reproductions are conducted on MultiTHUMOS to get the best performance out of these models without additional hyper-parameter tuning. 
P-GCN~\cite{DBLP:conf/iccv/ZengHGTRZH19} and ContextLoc~\cite{DBLP:conf/iccv/ZhuT00021} are both two-staged action detector that take in proposal generation results (e.g. BSN~\cite{DBLP:conf/eccv/LinZSWY18} proposals) and perform relation modeling based on coarse proposals to refine and classify each action candidate.
AFSD~\cite{DBLP:conf/cvpr/Lin0LWTWLHF21} follows FCOS~\cite{DBLP:conf/iccv/TianSCH19} to employ anchor-free architecture for TAD task. It is able to generate action detection results directly from the network, without pre-defined anchors or external video-level class labels.

The results in \cref{tab:sota_tad} show that direct application of single-label TAD models on multi-label TAD is deficient to achieve good detection performance, hence it is non-trivial to extend action detectors to multi-label TAD. Our PointTAD with strong image augmentation surpasses all of these state-of-the-art single-label TAD methods by a large margin, demonstrating the advance of our model to deal with concurrent instances and complex action relations as an action detector. 

\TableTAD

\subsection{{Error Bar}}
We follow the practice in \cite{riad2022diffstride} to compute the {error bar} for our model over 3 runs, and report the mean and standard derivation under detection-mAP in \cref{tab:error_bar}. The implementation with stronger image augmentation appears to have larger derivation on MultiTHUMOS than the \texttt{Center\_Crop} implementation.

\TableErrorBar

\subsection{Evaluation on THUMOS14}
Following RTD (query-based TAD method), we use the same feature representation and place our PointTAD head on top to build a direct TAD detector. Note that our TAD detector does not reply on the video-level classifier for action recognition and directly produce the action labels with our own PointTAD head. The result on the THUMOS14 dataset is reported in the \cref{tab:thumos14}. We obtain better performance on this single-label TAD dataset, demonstrating the generalization ability of PointTAD to various TAD datasets. 

\TableTHUMOS

\subsection{Comparison with Query-based Baselines}
In the ablation study of the main paper, we have shown the comparison between PointTAD and a Sparse-RCNN based baseline (segment-based variant), which proves the effectiveness of point representation. We have implemented another DETR based baseline on the MultiTHUMOS dataset. The performance comparison is reported in \cref{tab:baseline}, and our PointTAD obtains better results thanks to our more flexible point-based representation.

\TableBaseline

\TableEtoE

\subsection{Other Training Details}
In this sub-section, we share some of our observations in building an end-to-end trainable architecture for multi-label TAD via ablations on input image resolution and the number of decoder layers .

\paragraph{Study on input image resolution.}
In \cref{tab:resolution}, we show the detection performance with different input image resolution. According to \cite{liu2022an}, cropping images to  is adequate to tackle single-label TAD. However, we observe from the experiments that handling multi-label TAD requires more spatial information to distinguish concurrent instances from different categories and  image resolution is far less sufficient to solve the task. Our detection performance improves greatly by the increase of image resolution: from  to  the detection-mAP improves by absolute 3.9\% at average det-mAP, from  to  the performance improves by absolute 1\% at average det-mAP. The performance gain slows down at larger image resolution: 0.6\% gain at average det-mAP from  to . We settled at  to balance the trade-off between memory consumption and model performance.

\begin{figure*}[h]
\centering
\subfloat[]{\includegraphics[scale=0.5]{figs/multithumos_visualization.pdf}
\label{fig:multithumos_concurrent}}
\hspace{2em}

\subfloat[]{\includegraphics[scale=0.5]{figs/charades_visualization.pdf}
\label{fig:charades_concurrent}}
\caption{ Visualizations of the learnable query points of PointTAD on (a) MultiTHUMOS and (b) Charades.}
\label{fig:concurrent}
\end{figure*}

\paragraph{Study on the number of decoder layers.}
The choice of  is influenced by end-to-end training due to memory consumption of video encoder. We carefully decrease  from the common setting of , as shown in \cref{tab:n_decoder_layer}. Results indicate that 4-layer and 5-layer designs are quite similar in performance, yet from 4-layer design down, the decrease of  leads to obvious performance degrade. Hence,  is set to 4 empirically.


\subsection{Visualization}
We show detailed visualizations of learnable query points of PointTAD in \cref{fig:concurrent}. The visualizations are conducted on the samples with concurrent actions and multiple action categories from the test set of MultiTHUMOS (\cref{fig:multithumos_concurrent}) and Charades (\cref{fig:charades_concurrent}), covering different video scenarios such as daily events and sports matches. The first row briefly visualizes RGB frames of the video. The second row plots the temporal locations of groundtruth actions and the query points from the query that best predicts the action. In the rest of the figure, the left column shows action frames inside the groundtruth, where the semantic keyframes decided by the local query points are highlighted in blue. In the right column are selected frames corresponding to global query points. From the figure, we can see that the local query points not only learn different sets of representations for concurrent actions, but also capture important frames that indicate the action semantics. Global query points tend to distribute uniformly in the video clip and capture mostly close-up or background frames for sport events, possibly in the purpose of providing supporting background information for temporal action detection. 

\subsection{Societal Impacts}
\label{sec:societal}
This paper proposes PointTAD, a solution with learnable query points to tackle multi-label TAD. PointTAD is the first to introduce points/keyframes for segment-level video representation. Such practice addresses the non-uniform temporal structure of videos well and could potentially drive the development of general video understanding systems for finer point-based representation. 
The potential applications include video editing, anomaly event detection, etc. PointTAD enables end-to-end inference with raw video input, which benefits the deployment of automated online services for batch video processing, saving lots of human effort from offline, video-per-video handling.
As the model is data-driven, any bias in training data could be captured in the algorithm. Apart from this aspect, there are no known ethical issues in the real-world applications of this technology.

\subsection{Code and License}
\label{sec:license}
Our codebase is mainly built upon RTD-Net\footnote[5]{\url{https://github.com/MCG-NJU/RTD-Action}}~\cite{DBLP:conf/iccv/TanT0W21} protected by Apache-2.0 License and Sparse R-CNN\footnote[6]{\url{https://github.com/PeizeSun/SparseR-CNN}}~\cite{DBLP:conf/cvpr/SunZJKXZTLYW021} protected by MIT license. MultiTHUMOS~\cite{DBLP:journals/ijcv/YeungRJAMF18} dataset and Charades~\cite{DBLP:conf/eccv/SigurdssonVWFLG16} dataset are restricted to non-commercial use only.


\end{document}