\label{sec:method}
In this section, we analyse the correlation of popular metrics -- FLOPs and parameter count  -- with latency on a mobile device. We also evaluate how different design choices in architectures effect the latency on the phone. Based on the evaluation, we describe our architecture and training algorithm.


\subsection{Metric Correlations}
The most commonly used cost indicators for comparing the size of two or more models are parameter count and FLOPs~\cite{dehghani2021efficiency}. However, they may not be well correlated with latency in real-world mobile applications.
Therefore, we study the correlation of latency with FLOPS and parameter count for benchmarking efficient neural networks. We consider recent models and use their Pytorch implementation to convert them into ONNX format~\cite{bai2019onnx}. We convert each of these models to coreml packages using Core ML Tools~\cite{coremltools}. 
We then develop an iOS application to measure the latency of the models on an iPhone12.

We plot latency vs. FLOPs and latency vs. parameter count as shown in Figure~\ref{fig:latency_graph}.
We observe that many models with higher parameter count can have lower latency. We observe a similar plot between FLOPs and latency. Furthermore, we note the convolutional models such as MobileNets~\cite{sandler2018mobilenetv2, Ma_2018_shufflenetv2, efficientnet_v2_quoc} have lower latency for similar FLOPs and parameter count than their transformer counterparts~\cite{Touvron_2021_ICCV, mobileformer_cvpr2022, mobilevit_iclr2022}.
We also estimate the Spearman rank correlation~\cite{zar2005spearman} in Table~\ref{tab:correlation_coeff}a. 
We find that latency is moderately correlated with FLOPs and weakly correlated with parameter counts for efficient architectures on a mobile device. This correlation is even lower on a desktop CPU.

\begin{figure}
    \centering
    \includegraphics[width=0.94\linewidth]{images/flops_v_latency.png} \\ 
    \includegraphics[width=0.94\linewidth]{images/param_v_latency.png} 
    
    \scalebox{0.7}{
    \begin{tabular}{cccc}
    \\
        \hline 
         1 & 2 & 3 & 4 \\
         MobileNetV1 & EfficientNet-B0 & ShuffleNetV2-2.0 &
        ShuffleNetV2-1.0 \\ \hline
        5 & 6 & 7 & 8  \\
        MobileNext-1.4 & MobileNetV2  & MobileNetV3-S & MobileNetV3-L \\
        \hline
        9 & 10 & 11 & 12 \\
        MixNet-S &
       MNASNet-A1 & MobileOne-S1 & MobileOne-S0 \\
       \hline
    \end{tabular}}
    \caption{Top: FLOPs vs Latency on iPhone12. Bottom: Parameter Count vs Latency on iPhone 12. We indicate some networks using numbers as shown in the table above.}
    \label{fig:latency_graph}
\end{figure}

\begin{table}
    \centering
    \footnotesize 
\scalebox{0.85}{
    \begin{tabular}{lcccc}
    \toprule
        & \multicolumn{2}{c}{\textbf{FLOPs}} & \multicolumn{2}{c}{\textbf{Parameters}} \\
        \textbf{Type} & \textbf{corr.} & \textbf{p-value} & \textbf{corr.} & \textbf{p-value} \\
    \midrule
         \textbf{Mobile Latency} & 0.47 & 0.03 & 0.30 & 0.18\\
         \textbf{CPU Latency} & 0.06 & 0.80 & 0.07 & 0.77\\
    \bottomrule
    \end{tabular}
    }
    \caption{Spearman rank correlation coeff. between latency-flops.}
    \label{tab:correlation_coeff}
\end{table}

\subsection{Key Bottlenecks}

\paragraph{Activation Functions}
To analyze the effect of activation functions on latency, we construct a 30 layer convolutional neural network and benchmark it on iPhone12 using different activation functions, commonly used in efficient CNN backbones. All models in Table~\ref{tab:activation_latency} have the same architecture except for activations, but their latencies are drastically different. This can be attributed to synchronization costs mostly incurred by recently introduced activation functions like SE-ReLU~\cite{hu2018senet}, Dynamic Shift-Max~\cite{Li_2021_ICCV_micronet} and DynamicReLUs~\cite{chen2020dynamic}.
DynamicReLU and Dynamic Shift-Max have shown significant accuracy improvement in extremely low FLOP models like MicroNet~\cite{Li_2021_ICCV_micronet}, but, the latency cost of using these activations can be significant. 
Therefore we use only ReLU activations in MobileOne.

\begin{table}[]
 \vspace{-0.2cm}
 \centering
 \scalebox{0.9}{
 \small
      \begin{tabular}{lc}
            \toprule
            \textbf{Activation Function} & \textbf{Latency (ms)}  \\
            \midrule
            ReLU~\cite{agarap2018deep_relu} & 1.53  \\
            GELU~\cite{hendrycks2016gelu} & 1.63  \\
            SE-ReLU~\cite{hu2018senet} & 2.10 \\
            SiLU~\cite{silu_2017_elfwing} & 2.54  \\
            Dynamic Shift-Max~\cite{Li_2021_ICCV_micronet} & 57.04  \\
            DynamicReLU-A~\cite{chen2020dynamic} & 273.49  \\
            DynamicReLU-B~\cite{chen2020dynamic} & 242.14  \\
            \bottomrule
        \end{tabular}
        }
        \vspace{-0.2cm}
      \captionof{table}{Comparison of latency on mobile device of different activation functions in a 30-layer convolutional neural network.}\label{tab:activation_latency} 
      \vspace{-0.2cm}
\end{table}

\paragraph{Architectural Blocks}
Two of the key factors that affect runtime performance are memory access cost and degree of parallelism~\cite{Ma_2018_shufflenetv2}. 
Memory access cost increases significantly in multi-branch architectures as activations from each branch have to be stored to compute the next tensor in the graph. 
Such memory bottlenecks can be avoided if the network has smaller number of branches.
Architectural blocks that force synchronization like global pooling operations used in Squeeze-Excite block~\cite{hu2018senet} also affect overall run-time due to synchronization costs.
To demonstrate the hidden costs like memory access cost and synchronization cost, we ablate over using skip connections and squeeze-excite blocks in a 30 layer convolutional neural network. In Table~\ref{table:arch_blocks_vs_latency}b, we show how each of these choices contribute towards latency. Therefore we adopt an architecture with no branches at inference, which results in smaller memory access cost. In addition, we limit the use of Squeeze-Excite blocks to our biggest variant in order to improve accuracy. 

\begin{table}
\centering
\scalebox{0.8}{
    \begin{tabular}{lccc}
        \toprule
        \textbf{Architectural} & \multirow{2}{*}{Baseline}  & {+ Squeeze} & {+ Skip}  \\
        \textbf{Blocks }& & {Excite~\cite{hu2018senet}
        } & { Connections~\cite{He2015}} \\
        \midrule
        \textbf{Latency (ms)} & 1.53 & 2.10 & 2.62 \\
        \bottomrule
    \end{tabular}}
    \caption{Ablation on latency of different architectural blocks in a 30-layer convolutional neural network.}\label{table:arch_blocks_vs_latency}
\end{table}

\subsection{MobileOne Architecture}\label{section:arch}
Based on the our evaluations of different design choices, we develop the architecture of MobileOne. Like prior works on structural re-parameterization~\cite{NEURIPS2020_expandnets,Ding_2019_ICCV,Ding_2021_repvgg,ding2021diverse}, the train-time and inference time architecture of MobileOne is different. In this section, we introduce the basic block of MobileOne and the model scaling strategy used to build the network.

\paragraph{MobileOne Block}
MobileOne blocks are similar to blocks introduced in~\cite{NEURIPS2020_expandnets,Ding_2019_ICCV,Ding_2021_repvgg,ding2021diverse}, except that our blocks are designed for convolutional layers that are factorized into depthwise and pointwise layers. Furthermore, we introduce trivial over-parameterization branches which provide further accuracy gains. Our basic block builds on the MobileNet-V1~\cite{Howard2017MobileNets} block of 3x3 depthwise convolution followed by 1x1 pointwise convolutions. We then introduce re-parameterizable skip connection~\cite{Ding_2021_repvgg} with batchnorm along with branches that replicate the structure as shown in Figure~\ref{fig:mobileone_blocks}. The trivial over-parameterization factor  is a hyperparameter which is varied from 1 to 5. We ablate over the choice for  in Table~\ref{tab:compare_traintime_overparam}. At inference, MobileOne model does not have any branches. They are removed using the re-parameterization process described in ~\cite{Ding_2021_repvgg, ding2021diverse}.

 \begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{images/mobileone_block.pdf}
    \captionof{figure}{MobileOne block has two different structures at train time and test time. Left: Train time MobileOne block with reparameterizable branches. Right: MobileOne block at inference where the branches are reparameterized. Either ReLU or SE-ReLU is used as activation. The trivial over-parameterization factor  is a hyperparameter which is tuned for every variant.}\label{fig:mobileone_blocks}
 \end{figure}
 
 \begin{table}[]
\centering
 \small
 \scalebox{0.9}{
        \begin{tabular}{lccc}
            \toprule
            \textbf{Model} & \textbf{\# Params.} & \textbf{Top-1} \\
            \midrule
            ExpandNet-CL MobileNetV1~\cite{NEURIPS2020_expandnets}        & 4.2 & 69.4 & \\
            RepVGG-A0~\cite{Ding_2021_repvgg}     & 8.3  & 72.4 \\
            RepVGG-A1~\cite{Ding_2021_repvgg}     & 12.8 & 74.5 \\
            RepVGG-B0~\cite{Ding_2021_repvgg}     & 14.3 & 75.1 \\
            ACNet MobileNetV1~\cite{Ding_2019_ICCV}   & 4.2  & 72.1 \\
            ACNet ResNet18~\cite{Ding_2019_ICCV}      & 11.7 & 71.1 \\
            DBBNet MobileNetV1~\cite{ding2021diverse}  & 4.2  & 72.9 \\
            DBBNet ResNet18~\cite{ding2021diverse}     & 11.7 & 71.0 \\
            \midrule
            \textbf{MobileOne-S0}   & 2.1 & 71.4 \\
            \textbf{MobileOne-S1}   & 4.8 & 75.9 \\
            \textbf{MobileOne-S2}   & 7.8 & 77.4 \\
            \textbf{MobileOne-S3}   & 10.1 & 78.1 \\
            \textbf{MobileOne-S4}   & 14.8 & 79.4 \\
            \bottomrule
        \end{tabular}
      }
      \vspace{-0.2cm}
      \captionof{table}{Comparison of Top-1 Accuracy on ImageNet against recent train time over-parameterization works. Number of parameters listed above is at inference.}\label{tab:compare_traintime_overparam}
      \vspace{-0.2cm}
 \end{table} 
\begin{table}
\vspace{-0.2cm}
\centering
\footnotesize
\scalebox{0.9}{
        \begin{tabular}{lccc}
        \\ 
            \toprule
            \textbf{Re-param.} & \textbf{MobileOne-S0} & \textbf{MobileOne-S1} & \textbf{MobileOne-S3} \\ 
            \midrule
            \textbf{with}    &   71.4         & 75.9 & 78.1 \\
            \textbf{without} &  69.6 &    74.6        & 77.2 \\
            \bottomrule
        \end{tabular} 
        }
        \vspace{-0.2cm}
      \captionof{table}{Effect re-parametrizable branches on Top-1 ImageNet accuracy.}~\label{table:ablate_rep_branches}
      \vspace{-0.4cm}
\end{table}

\begin{table}
    \centering
    \small
    \scalebox{0.9}{
          \begin{tabular}{cccccc}
            \toprule
            \multirow{2}{*}{\textbf{Model}} & \multicolumn{5}{c}{\textbf{Top-1}} \\
            \cmidrule{2-6}
            & k=1 & k=2 & k=3 & k=4 & k-5 \\
            \midrule
            MobileOne-S0    & 70.9 & 70.7 & 71.3 & 71.4 & 71.1 \\
            MobileOne-S1    & 75.9 & 75.7 & 75.6 & 75.6 & 75.2 \\
            \bottomrule
        \end{tabular}
        }
        \vspace{-0.2cm}
      \captionof{table}{Comparison of Top-1 on ImageNet for various values of trivial over-parameterization factor .}~\label{table:ablate_k_rep_branches}
      \vspace{-0.2cm}
\end{table}

For a convolutional layer of kernel size , input channel dimension  and output channel dimension , the weight matrix is denoted as  and bias is denoted as . A batchnorm layer contains accumulated mean , accumulated standard deviation , scale  and bias . Since convolution and batchnorm at inference are linear operations, they can be folded into a single convolution layer with weights  and bias . Batchnorm is folded into preceding convolutional layer in all the branches. For skip connection the batchnorm is folded to a convolutional layer with identity 1x1 kernel, which is then padded by  zeros as described in~\cite{Ding_2021_repvgg}. After obtaining the batchnorm folded weights in each branch, the weights  and bias  for convolution layer at inference is obtained, where  is the number of branches.  

To better understand the improvements from using train time re-parameterizable branches, we ablate over versions of MobileOne models by removing train-time re-parameterizable branches (see Table~\ref{table:ablate_rep_branches}), while keeping all other training parameters the same as described in Section~\ref{section:experiment_details_imagenet}. Using re-parameterizable branches significantly improves performance. To understand the importance of trivial over-parameterization branches, we ablate over the choice of over-parameterization factor  in Table~\ref{table:ablate_k_rep_branches}. For larger variants of MobileOne, the improvements from trivial over-parameterization starts diminishing. For smaller variant like MobileOne-S0, we see improvements of 0.5\% by using trivial over-parameterization branches. In Figure~\ref{fig:trainval_loss}, we see that adding re-parameterizable branches improves optimization as both train and validation losses are further lowered. 

\begin{table*}
\footnotesize
\begin{tabular}{ccccccccccc}
          \toprule
          \multirow{2}{*}{\textbf{Stage}} & \multirow{2}{*}{\textbf{Input}} & \multirow{2}{*}{\textbf{\# Blocks}} & \multirow{2}{*}{\textbf{Stride}} & \multirow{2}{*}{\textbf{Block Type}} &
          \multirow{2}{*}{\textbf{\# Channels}} & \multicolumn{5}{c}{\textbf{MobileOne Block Parameters (, , act=ReLU)}} \\
          \cmidrule{7-11}
          & & & & & & \textbf{S0} & \textbf{S1} & \textbf{S2} & \textbf{S3} & \textbf{S4} \\
          \midrule
          1  &  & 1 & 2 & MobileOne-Block & 64  & (0.75, 4) & (1.5, 1) & (1.5, 1) & (2.0, 1) & (3.0, 1) \\
          2  &  & 2 & 2 & MobileOne-Block & 64  & (0.75, 4) & (1.5, 1) & (1.5, 1) & (2.0, 1) & (3.0, 1)\\
          3  &    & 8 & 2 & MobileOne-Block & 128 & (1.0, 4)  & (1.5, 1) & (2.0, 1) & (2.5, 1) & (3.5, 1) \\
          4  &    & 5 & 2 & MobileOne-Block & 256 & (1.0, 4)  & (2.0, 1) & (2.5, 1) & (3.0, 1) & (3.5, 1) \\
          5  &    & 5 & 1 & MobileOne-Block & 256 & (1.0, 4)  & (2.0, 1) & (2.5, 1) & (3.0, 1) & (3.5, 1, SE-ReLU) \\
          6  &    & 1 & 2 & MobileOne-Block & 512 & (2.0, 4)  & (2.5, 1) & (4.0, 1) & (4.0, 1) & (4.0, 1, SE-ReLU) \\
          7  &      & 1 & 1 & AvgPool & - & - & - & - & - & - \\
          8  &      & 1 & 1 & Linear & 512 & 2.0 & 2.5 & 4.0 & 4.0 & 4.0 \\
          \midrule
        \end{tabular}
\vspace{-0.2cm}
    \caption{MobileOne Network Specifications}
    \label{table:mobileone_network_specification}
    \vspace{-0.2cm}
\end{table*}


 
  



\paragraph{Model Scaling} 
Recent works scale model dimensions like width, depth, and resolution to improve performance \cite{efficientnet_v1_plmr,tinynet_neurips}. MobileOne has similar depth scaling as MobileNet-V2, i.e. using shallower early stages where input resolution is larger as these layers are significantly slower compared to later stages which operate on smaller input resolution. We introduce 5 different width scales as seen in Table~\ref{table:mobileone_network_specification}. Furthermore, we do not explore scaling up of input resolution as both FLOPs and memory consumption increase, which is detrimental to runtime performance on a mobile device. As our model does not have a multi-branched architecture at inference, it does not incur data movement costs as discussed in previous sections. This enables us to aggressively scale model parameters compared to competing multi-branched architectures like MobileNet-V2, EfficientNets, etc. without incurring significant latency cost. The increased parameter count enables our models to generalize well to other computer vision tasks like object detection and semantic segmentation (see Section~\ref{section:object_detection_mscoco}). In Table~\ref{tab:compare_traintime_overparam}, we compare against recent train time over-parameterization works ~\cite{Ding_2021_repvgg,ding2021diverse,Ding_2019_ICCV,NEURIPS2020_expandnets} and show that MobileOne-S1 variant outperforms RepVGG-B0 which is 3 bigger.

\subsection{Training}\label{sec:progressive_training}
As opposed to large models, small models need less regularization to combat overfitting. It is important to have weight decay in early stages of training as demonstrated empirically by~\cite{time_matters_weightdecay}. Instead of completely removing weight decay regularization as studied in~\cite{time_matters_weightdecay}, we find that annealing the loss incurred by weight decay regularization over the course of training is more effective. In all our experiments, we use cosine schedule~\cite{cosinelr_iclr2017} for learning rate. Further, we use the same schedule to anneal weight decay coefficient. We also use the progressive learning curriculum introduced in~\cite{efficientnet_v2_quoc}. 
In Table~\ref{table:pl_ablation}, we ablate over the various train settings keeping all other parameters fixed. We see that annealing the weight decay coefficient gives a 0.5\% improvement.
\begin{table}
  \footnotesize
  \centering
  \scalebox{0.9}{
  \begin{tabular}{lcccc}
        \toprule
        & \multirow{2}{*}{\bf Baseline} & {\bf + Progressive }  & {\bf + Annealing} & \multirow{2}{*}{\bf + EMA}\\
        & \textbf{} & {\bf Learning}  & {\bf Weight Decay} & \\
        \midrule
         \textbf{Top-1} & 76.4 & 76.8 & 77.3  & 77.4 \\
        \bottomrule
    \end{tabular}
    }
  \caption{Ablation on various train settings for MobileOne-S2 showing Top-1 accuracy on ImageNet.} \label{table:pl_ablation}
  \vspace{-0.2cm}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{images/trainval_loss_plots.pdf}
    \captionof{figure}{Plot of train and validation losses of MobileOne-S0 model. From no branches to adding re-parameterizable branches with =1, leads to 3.4\% lower train loss. Adding more branches (=4) lowers train loss by an additional 1\%. From no branches to the variant with re-parameterizable branches (=4), validation loss improves by 3.1\%}\label{fig:trainval_loss}
\end{figure}


\subsection{Benchmarking}\label{sec:benchmarking}
Getting accurate latency measurements on a mobile device can be difficult. On the iPhone 12, there is no command line access or functionality to reserve all of a compute fabric for just the model execution. We also do not have access to the breakdown of the round-trip-latency into categories like the network initialization, data movement, and network execution. To measure latency, we developed an iOS application using swift \cite{swift_language}.
The application runs the models using Core ML \cite{coremltools}. To eliminate  startup  inconsistencies, the model graph is loaded, the input tensor is preallocated, and the model is run once before benchmarking begins. During benchmarking, the app runs the model many times (default is 1000) and statistic are accumulated. To achieve lowest latency and highest consistency, all other applications on the phone are closed. 
For the models latency seen in Table \ref{tab:compare_imagenet}, we report the full round-trip latency. A large fraction of this time may be from platform processes that are not model execution, but in a real application these delays may be unavoidable. Therefore we chose to include them in the reported latency.  In order to filter out interrupts from other processes, we report the minimum latency for all the models.
For CPU latency, we run the models on an Ubuntu desktop with a 2.3 GHz -- Intel Xeon Gold 5118 processor. For GPU latency, we compile the models using NVIDIA TensorRT library (v8.0.1.6) and run on a single RTX-2080Ti GPU with batch size set to 1. We report the median latency value out of 100 runs.
