\documentclass{article}
\usepackage{log_2023}						

\usepackage{booktabs}						\usepackage{multirow}						\usepackage{amsfonts}						\usepackage{graphicx}						\usepackage{duckuments}						

\usepackage[numbers,compress,sort]{natbib}	

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{graphicx}       \usepackage{multirow}       \usepackage{arydshln}       \usepackage{enumitem}      \usepackage[textsize=tiny]{todonotes} \usepackage{caption}
\usepackage{subcaption}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}








\newcommand{\inp}{\leftarrow}
\newcommand{\out}{\rightarrow}
\newcommand\ours{Directed Graph Neural Network}
\newcommand\oursacro{Dir-GNN}
\newcommand{\R}{\mathbb{R}}

\newcommand\m{m}
\newcommand\nnodes{n}
\newcommand\nedges{m}
\newcommand\nfeatures{d}
\newcommand\iclass{c}
\newcommand\nclass{C}
\newcommand\ilayer{k}
\newcommand\nlayers{K}
\newcommand\effhom{h^{(\text{eff})}}

\newcommand*{\ldblbrace}{\{\mskip-6mu\{}
\newcommand*{\rdblbrace}{\}\mskip-6mu\}}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark} 

\title{Edge Directionality Improves Learning on Heterophilic Graphs}





\author[E. Rossi et al.]{Emanuele Rossi \\
  Imperial College London \\
\And
  Bertrand Charpentier \\
  Technical University of Munich \\
\And
  Francesco Di Giovanni \\
  University of Cambridge \\
\And
  Fabrizio Frasca \\
  Imperial College London \\
\And
  Stephan GÃ¼nnemann \\
  Technical University of Munich \\
\And
  Michael Bronstein \\
  University of Oxford \\
} 

\begin{document}


\maketitle


\begin{abstract}

Graph Neural Networks (GNNs) have become the de-facto standard tool for modeling relational data. However, while many real-world graphs are \emph{directed}, the majority of today's GNN models discard this information altogether by simply making the graph undirected.
The reasons for this are historical: 1) many early variants of spectral GNNs explicitly required undirected graphs, and 2) the first benchmarks on homophilic graphs did not find significant gain from using direction. 
In this paper, we show that in \emph{heterophilic} settings, treating the graph as directed increases the {\em effective homophily} of the graph, suggesting a potential gain from the correct use of directionality information.  
To this end, we introduce \ours{} (\oursacro{}), a novel general framework for deep learning on directed graphs. \oursacro{} can be used to extend \emph{any} Message Passing Neural Network (MPNN) to account for edge directionality information by performing separate aggregations of the \emph{incoming} and \emph{outgoing} edges. We prove that \oursacro{} matches the expressivity of the Directed Weisfeiler-Lehman test, exceeding that of conventional MPNNs. In extensive experiments, we validate that while our framework leaves performance unchanged on homophilic datasets, it leads to large gains over base models such as GCN, GAT and GraphSage on heterophilic benchmarks, outperforming much more complex methods and achieving new state-of-the-art results. 
The code for the paper can be found at \url{https://github.com/emalgorithm/directed-graph-neural-network}.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

Graph Neural Networks (GNNs) have demonstrated remarkable success across a wide range of problems and fields~\cite{zhou2018gnn}. Most GNN models, however, assume that the input graph is undirected~\cite{kipf2016semi,velivckovic2017graph,hamilton2017inductive}, despite the fact that many real-world networks, such as citation and social networks, are inherently directed. Applying GNNs to directed graphs often involves either converting them to undirected graphs or only propagating information over incoming (or outgoing) edges, both of which may discard valuable information crucial for downstream tasks.

\begin{figure}[t!]
\centering
\vspace{-7mm}
\begin{subfigure}[b]{0.4\textwidth}
     \centering
     \includegraphics[width=\linewidth]{images/average_results_heterophilic.pdf}
     \caption{Heterophilic Graphs}
     \label{fig:aggregate_results_heterophilic}
\end{subfigure}
\begin{subfigure}[b]{0.375\textwidth}
     \centering
     \includegraphics[width=\linewidth,trim={1cm 0 0 0},clip]{images/average_results_homophilic.pdf}
     \caption{Homophilic Graphs}
     \label{fig:aggregate_results_homophilic}
\end{subfigure}
\vspace{-1mm}
\caption{Extending popular GNN architectures with our \oursacro{} framework to incorporate edge-directionality information brings large gains ( to ) on heterophilic datasets (left), while leaving performance mostly unchanged on homophilic datasets (right). The plots illustrate the average performance over all datasets, while the full results are presented in \cref{tab:direction_ablation}.}
\label{fig:aggregate_results}
\vspace{-4mm}
\end{figure}


We believe the dominance of undirected graphs in the field is rooted in two ``original sins'' of GNNs.   
First, undirected graphs have symmetric Laplacians which admit orthogonal eigendecomposition. Orthogonal Laplacian eigenvectors act as a natural generalization of the Fourier transform and allow to express graph convolution operations in the Fourier domain. 
Since some of the early graph neural networks originated from the field of graph signal processing \cite{shuman2013emerging,sandryhaila2013discrete}, the undirected graph assumption was necessary for spectral GNNs \cite{bruna2013spectral,defferrard2016convolutional,kipf2016semi} to be properly defined. 
With the emergence of spatial GNNs, unified with the message-passing framework (MPNNs~\cite{gilmer2017neural}), 
this assumption was not strictly required anymore, as MPNNs can easily be applied to directed graphs by propagating over the directed adjacency, resulting however in information being propagated only in a single direction, at the risk of discarding useful information from the opposite one. However, early works empirically observed that making the graph undirected consistently leads to better performance on established node-classification benchmarks, which historically have been mainly \textit{homophilic} graphs such as Cora and Pubmed~\cite{sen:aimag08}, where neighbors tend to share the same label. Consequently, converting input graphs to undirected ones has become a standard part of the dataset preprocessing pipeline, to the extent that the popular GNN library PyTorch-Geometric~\cite{fey2019graph} includes a general utility function that automatically makes graphs undirected when loading datasets\footnote{ \href{https://github.com/pyg-team/pytorch_geometric/blob/66b17806b1f4a2008e8be766064d9ef9a883ff03/torch_geometric/io/npz.py\#L26}{This Pytorch-Geometric routine} is used to load datasets stored in an npz format. It makes some directed datasets, such as \href{https://github.com/pyg-team/pytorch_geometric/blob/6fa2ee7bfef32311df73ca78266c18c4449a7382/torch_geometric/datasets/citation_full.py\#L99}{Cora-ML} and \href{https://github.com/pyg-team/pytorch_geometric/blob/6fa2ee7bfef32311df73ca78266c18c4449a7382/torch_geometric/datasets/citation_full.py\#L99}{Citeseer-Full}, automatically undirected without any option to get the directed version instead.}.

The key observation in this paper is that while accounting for edge directionality indeed does not help in homophilic graphs, it can bring extensive gains in \textit{heterophilic} settings (Fig.~\ref{fig:aggregate_results}), where neighbors tend to have different labels.
In the rest of the paper, we study \emph{why} and \emph{how} to use directionality to improve learning on heterophilic graphs. 

\textbf{Contributions.} 
Our contributions are the following:
\begin{itemize}[leftmargin=*]
    \setlength\itemsep{-0em}
    \item We show that considering the directionality of a graph substantially increases its \textit{effective homophily} in heterophilic settings, with negligible or no impact on homophilic settings (see~\cref{sec:directed_heterophily}). 
    \item We propose a novel and generic \ours{} framework (\oursacro{}) which extends \emph{any} MPNN to work on directed graphs by performing separate aggregations of the \emph{incoming} and \emph{outgoing} edges. Moreover, we show that \oursacro{} leads to more homophilic aggregations compared to their undirected counterparts (see~\cref{sec:method}). 
    \item Our theoretical analysis establishes that \oursacro{} is as expressive as the Directed Weisfeiler-Lehman test, while being \emph{strictly more expressive than MPNNs} (see ~\cref{sec:expressivity}).
    \item We empirically validate that augmenting popular GNN architectures with the \oursacro{} framework yields \emph{large improvements on heterophilic benchmarks}, achieving \emph{state-of-the-art results} and outperforming even more complex methods specifically designed for such settings. Moreover, this enhancement does not negatively impact the performance on homophilic benchmarks (see~\cref{sec:experiments}).
\end{itemize} \section{Background}
\label{sec:background}

We consider a directed graph  with a node set  of  nodes, and an edge set  of  edges. We define its respective directed adjacency matrix  where  if  and zero otherwise, its respective undirected adjacency matrix  where  if  or  and zero otherwise.
In this paper, we focus on the task of (semi-supervised) node classification on an attributed graph  with node features arranged into the  matrix  and node labels .

\subsection{Homophily and Heterophily in Undirected Graphs} \label{sec:heterophily_in_gnns}

In this Section, we first review the heterophily metrics for undirected graphs, while in Sec.~\ref{sec:directed_heterophily} we propose heterophily metrics adapted to directed graphs. Most GNNs are based on the \textit{homophily assumption} for undirected graphs, i.e., that neighboring nodes tend to share the same labels. 
While a reasonable assumption in some settings, it turns out not to be true in many important applications such as gender classification on social networks or fraudster detection on e-commerce networks. 

\textbf{Homophily metrics.} Several metrics have been proposed to measure homophily on undirected graphs. {\em Node homophily} is defined as

where  is the indicator function with value 1 if  or zero otherwise. Intuitively, node homophily measures the fraction of neighbors with the same label as the node itself, averaged across all nodes.
However, since heterophily is a complex phenomenon which is hard to capture with only a single scalar, a better representation of a graph's homophily is the   \textit{class compatibility matrix} ~\cite{zhu2020beyond},  capturing the fraction of edges from nodes with label  to nodes with label : 


\emph{Homophilic} datasets are expected to have most of the mass of their compatibility matrices concentrated in the diagonal, as most of the edges are between nodes of the same class (e.g. see Citeseer-Full in~\cref{fig:citeseer_chameleon_compat}). Conversely, \emph{heterophilic} datasets will have most of the mass away from the diagonal of the compatibility matrix (e.g. see Chameleon in~\cref{fig:citeseer_chameleon_compat}).

\subsection{Message Passing Neural Network}
In this Section, we first review the Message Passing Neural Network (MPNN) paradigm for undirected graphs, while in Sec.~\ref{sec:method} we extend this formalism to directed graphs. An MPNN is a parametric model which \emph{iteratively} applies  aggregation maps  and combination maps  to compute embeddings  for node  based on messages  containing information on its neighbors. Namely, the -th layer of an MPNN is given by

\noindent where  is a multi-set.
The aggregation maps  and the combination maps  are {\em learnable} (usually a small neural network) and their different implementations  result in specific architectures (e.g.  graph convolutional neural networks (GCN) use linear aggregation, graph attention networks (GAT) use attentional layers, etc.). After the last layer , the node representation  is mapped into the -probability simplex via a (learnable) decoding step, often given by an MLP. Independent of the choice of  and , all MPNNs only send messages along the edges of the graph, which make them particularly suitable for tasks where edges do encode a notion of similarity -- as it is the case when adjacent nodes in the graph often share the same label (homophilic). Conversely, MPNNs tend to struggle in the scenario where they need to separate 
a node embedding from that of its neighbours \cite{nt2019revisiting}, often a challenging problem that has gained attention in the community and which we discuss in detail next.

 \section{Heterophily in Directed Graphs}
\label{sec:directed_heterophily}

In this Section, we discuss how accounting for directionality can be particularly helpful for dealing with heterophilic graphs. By leveraging the directionality information, we argue that even standard MPNNs that are traditionally thought to struggle in the heterophilic regime, can in fact perform extremely well. 

\textbf{Weighted homophily metrics.} First, we extend the homophily metrics introduced in Section \ref{sec:heterophily_in_gnns} to account for  directed edges and higher-order neighborhoods. 
Given a possibly directed and weighted  message-passing matrix , we define the \textit{weighted node homophily} as

\noindent Accordingly, by taking  and  respectively, we can compute the node homophily based on outgoing or incoming edges. Similarly, we can also take  to be any {\em weighted} 2-hop matrix associated with a directed graph (see details below) and compute its node homophily. 

We can also extend the construction to edge-computations by defining the  \textit{weighted compatibility matrix}  of a message-passing matrix  as 

\noindent As above, one can take  or  to derive the compatibility matrix associated with the out and in-edges, respectively.

\textbf{Effective homophily.} Stacking multiple layers of a GNN effectively corresponds to taking powers of diffusion matrices, resulting in message propagation over higher-order hops. \citet{zhu2020beyond} noted that for heterophilic graphs, the 2-hop tends to be more homophilic than the 1-hop. This phenomenon of similarity within ``friends-of-friends'' has been widely observed and is commonly referred to as monophily~\cite{monophily}. If higher-order hops exhibit increased homophily, exploring the graph through layers can prove beneficial for the task. Consequently, we introduce the concept of \textit{effective homophily} as the maximum weighted node homophily observable at any hop of the graph.

For directed graphs, there exists an exponential number of -hops. For instance, four 2-hop matrices can be considered: the squared operators  and , which correspond to following the same forward or backward edge direction twice, as well as the {\em non}-squared operators  and , representing the forward/backward and backward/forward edge directions. Given a graph , we define its effective homophily as follows:


where  denotes the set of all -hop matrices for a graph. If  is undirected,  contains only . In our empirical analysis, we will focus on the 2-hop matrices, as computing higher-order -hop matrices becomes intractable for all but the smallest graphs~\footnote{This is attributed to the fact that while  is typically quite sparse,  grows increasingly dense as  increases, quickly approaching  non-zero entries.}. 

\begin{table*}[t]
\begin{center}
\begin{small}
\resizebox{.8\textwidth}{!}{\begin{tabular}{lcccc:ccccc:r}
\toprule
           {} & {} &   &   &   &     &   &   &   &   &  \\
\midrule
\multirow{3}{5em}{Homophilic} & \textsc{citeseer-full} &  0.958 &    0.951 &  0.958 &  0.954 &  0.959 &   0.971 &   0.951 &  0.971 &         1.36\% \\
& \textsc{cora-ml}       &  0.810 &    0.767 &  0.810 &  0.808 &  0.833 &   0.803 &   0.779 &  0.833 &         2.84\% \\
& \textsc{ogbn-arxiv}    &  0.635 &    0.548 &  0.635 &  0.632 &  0.675 &   0.658 &   0.556 &  0.675 &          6.3\% \\
\hdashline
\multirow{7}{5em}{Heterophilic} 
& \textsc{chameleon}     &  0.248 &    0.331 &  0.331 &  0.249 &  0.274 &   0.383 &   0.335 &  0.383 &        15.71\% \\
& \textsc{squirrel}      &  0.218 &    0.252 &  0.252 &  0.219 &  0.210 &   0.257 &   0.258 &  0.258 &         2.38\% \\
& \textsc{arxiv-year}    &  0.289 &    0.397 &  0.397 &  0.310 &  0.403 &   0.487 &   0.431 &  0.487 &        22.67\% \\
& \textsc{snap-patents}  &  0.221 &    0.372 &  0.372 &  0.266 &  0.271 &   0.478 &   0.522 &  0.522 &        40.32\% \\
& \textsc{roman-empire}  &  0.046 &    0.365 &  0.365 &  0.045 &  0.042 &   0.535 &   0.609 &  0.609 &        66.85\% \\
\bottomrule
\end{tabular}}
\end{small}
\end{center}
\caption{Weighted node homophily for different diffusion matrices, and effective homophily for both undirected () and directed graph (). The last column reports the gain in effective homophily obtained by using the directed graph as opposed to the undirected graph.}
\label{tab:node_homophilies}
\end{table*}

 
\begin{figure}[t!]
\centering
\vspace{-7mm}
\begin{subfigure}[b]{0.44\textwidth}
     \centering
     \includegraphics[width=\linewidth]{images/synthetic_effective_homophily/comparison.pdf}
     \caption{}
     \label{fig:synthetic_effective_homophily}
\end{subfigure}
\hspace{5mm}
\begin{subfigure}[b]{0.44\textwidth}
     \centering
     \includegraphics[width=\linewidth]{images/synthetic_experiment/synthetic_experiment_Sage.pdf}
     \caption{}
    \label{fig:synthetic_task}
\end{subfigure}

\caption{In our synthetic experiments, we observe the following: (a) the effective homophily of directed graphs is consistently higher compared to their undirected counterparts. Interestingly, this gap widens for graphs that are less homophilic. (b) When examining the performance of GraphSage and its \oursacro{} extensions on a synthetic task requiring directionality information, only Dir-Sage(=0.5), which utilizes information from both directions, is capable of solving the task.}
\label{fig:test1}
\end{figure}

\looseness=-1
\textbf{Leveraging directionality to enhance effective homophily.}
We observe that converting a graph from directed to undirected results in lower effective homophily for heterophilic graphs, while the impact on homophilic graphs is negligible (refer to the last column of~\cref{tab:node_homophilies}). Specifically,  and  emerge as the most homophilic diffusion matrices for heterophilic graphs. In fact, the average relative gain of effective homophily, , when using directed graphs compared to undirected ones is only around  for homophilic datasets, while it is almost  for heterophilic datasets. We further validate this observation on synthetic directed graphs exhibiting various levels of homophily, generated through a modified preferential attachment process (see \cref{sec:effective_homophily_synthetic} for more details). \cref{fig:synthetic_effective_homophily} displays the results: the directed graph consistently demonstrates higher effective homophily compared to its undirected counterpart, with the gap being particularly prominent for lower node homophily levels. The minimal effective homophily gain on homophilic datasets further substantiates the traditional practice of using undirected graphs for benchmarking GNNs, as the datasets were predominantly homophilic until recently. 






\textbf{Real-world example.} We illustrate the concept of effective homophily in heterophilic directed graphs by taking the concrete task of predicting the publication year of papers based on a directed citation network such as Arxiv-Year~\cite{lim2021large}. 
In this case, the different 2-hop neighbourhoods have very different semantics: the diffusion operator  represents papers that are cited by those papers that paper  cites. As these 2-hop neighboring papers were published further in the past relative to paper , they do not offer much information about the publication year of .
On the other hand,  represents papers that share citations with paper . Papers cited by the same sources are more likely to have been published in the same period, so the diffusion operator  is expected to be more homophilic.
The undirected 2-hop operator  is the average of the four directed 2-hops. Therefore, the highly homophilic matrix  is diluted by the inclusion of , leading to a less homophilic operator overall.




 \section{\ours{}}
\label{sec:method}

In this Section, we extend the class of MPNNs to directed graphs, and refer to such generalization as \ours{} (\oursacro{}). We follow the scheme in \Cref{eq:mpnn}, meaning that the update of a node feature is the result of a {\em combination} of its previous state and an {\em aggregation} over its neighbours. Crucially though, the characterization of neighbours now has to account for the edge-directionality. Accordingly, given a node , we perform separate aggregations over the in-neighbours () and the out-neighbours () respectively:

\noindent The idea behind our framework is that given {\em any} MPNN, we can readily adapt it to the directed case by choosing how to aggregate over both directions. For this purpose, we replace the neighbour aggregator  with two separate in- and out-aggregators  and , which can have {\em independent} sets of parameters for the two aggregation phases and, possibly, different normalizations as discussed below -- however the functional expression in both cases stays the same. As we will show, accounting for both directions separately is fundamental for both the expressivity of the model (see \cref{sec:expressivity}) and its empirical performance (see \Cref{sec:synthetic_experiments}).

\textbf{Extension of common architectures.}
To make our discussion more explicit, we describe extensions of popular MPNNs where the aggregation map is computed by \smash{}, where  is a message-passing matrix. In GCN~\cite{kipf2016semi}, \smash{}, where  is the degree matrix of the undirected graph. In the case of directed graphs, two message-passing matrices  and  are required for in- and out-neighbours respectively. Additionally, the normalization is slightly more subtle, since we now have two different diagonal degree matrices  and  containing the in-degrees and out-degrees respectively. Accordingly, we propose a normalization of the form  i.e. . To motivate this choice, note that the normalization modulates the aggregation based on the out-degree of  and the in-degree of  as one would expect given that we are computing a message going from  to . We can then take  and write the update at layer  of Dir-GCN as 

\noindent for learnable channel-mixing matrices  and with  a pointwise activation map. Finally, we note that in our implementation of \oursacro{} we use an additional learnable or tunable parameter  allowing the framework to weight one direction more than the other (a convex combination), depending on the dataset. \oursacro{} extensions of GAT~\cite{velivckovic2017graph} and GraphSAGE~\cite{hamilton2017inductive} can be found in~\cref{app:extensions}. 

\textbf{\oursacro{} leads to more homophilic aggregations.} Since our main application amounts to relying on the graph-directionality to mitigate heterophily, here we comment on how information is iteratively propagated in a \oursacro{}, generally leading to an aggregation scheme beneficial on most real-world directed heterophilic graphs. We focus on the Dir-GCN formulation, however the following applies to any other \oursacro{} up to changing the message-passing matrices. Consider a 2-layer Dir-GCN as in \Cref{eq:dir-gcn}, and let us remove the pointwise activation ~\footnote{Note that this does not affect our discussion, in fact any observation can be extended to the non-linear case by computing the Jacobian of node features as in \citet{topping2021understanding}.}. Then, the node representation can be written as 

\noindent We observe that when we aggregate information over multiple layers, the final node representation is derived by also computing convolutions over 2-hop matrices  and . From the discussion in \Cref{sec:directed_heterophily}, we deduce that this framework may be more suited to handle heterophilic graphs since generally such 2-hop matrices are more likely to encode similarity than ,  or  -- this is validated empirically on real-world datasets in \Cref{sec:experiments}.

\textbf{Advantages of two-directional updates.} We discuss the benefits of incorporating both directions in the layer update, as opposed to using a single direction. Although spatial MPNNs can be adapted to directed graphs by simply utilizing  instead of âresulting in message propagation only along out-edgesârelying on a single direction presents three primary drawbacks. 
First, if the layer update only considers one direction, the exploration of the multi-hop neighbourhoods through powers of diffusion operators would not include the mixed terms  and , which have been shown to be particularly beneficial for heterophilic graphs in \Cref{sec:directed_heterophily}.
Second, by using only one direction we disregard the graph entirely for nodes where the out-degree is zero~\footnote{Or in-degree, depending on which direction is selected.}. This phenomenon frequently occurs in real-world graphs, as reported in~\cref{tab:zero_degrees}. Incorporating both directions in the layer update helps mitigate this problem, as it is far less common for a node to have both in- and out-degree to be zero, as also illustrated in~\cref{tab:zero_degrees}.
Third, limiting the update to a single direction reduces expressivity, as we discuss in~\cref{sec:expressivity}.

\textbf{Complexity.}
The complexity of \oursacro{} depends on the specific instantiation of the framework. Dir-GCN, Dir-Sage, and Dir-GAT maintain the same per-layer computational complexity as their undirected counterparts ( for GCN and GraphSage, and  for GAT). However, they have twice as many parameters, owing to their separate weight matrices for in- and out-neighbors.

\subsection{Expressive power of \oursacro{}} \label{sec:expressivity}
It is a well known result that MPNNs are bound in expressivity by the 1-WL test, and that it is possible to construct MPNN models which are as expressive as the 1-WL test~\cite{DBLP:conf/iclr/XuHLJ19}. In this section, we show that \oursacro{} is the optimal way to extend MPNNs to directed graphs. We do so by proving that \oursacro{} models can be constructed to be as expressive as an extension of the 1-WL test to directed graphs~\cite{Grohe2021ColorRA}, referred to as \textit{D-WL} (for a formal definition, see~\cref{sec:appendix_wl}). Additionally, we illustrate its greater expressivity over more straightforward approaches, such as converting the graph to its undirected form and utilizing a standard MPNN (\textit{MPNN-U}) or applying an MPNN that propagates solely along edge direction (\textit{MPNN-D})\footnote{The same results apply to a model which sends messages only along in-edges.}. Formal statements for the theorems in this section along with their proofs can be found in~\cref{app:expressivity_analysis}. 


\begin{theorem}[Informal]\label{thm:dirgnn-as-expressive-as-d-wl}
    \oursacro{} is as expressive as D-WL if , , and  are injective for all . 
\end{theorem}

A discussion of how a \oursacro{} can be parametrized to meet these conditions (similarly to what is done in~\citet{DBLP:conf/iclr/XuHLJ19}) can be found in~\cref{app:d-wl-proof}.

\begin{theorem}[Informal]\label{thm:dirgnn-strictly-more-expressive-than-mpnn}
\oursacro{} is strictly more expressive than both MPNN-U and MPNN-D.
\end{theorem}

Intuitively, the theorem states that while all directed graphs distinguished by MPNNs are also separated by \oursacro{}s, there also exist directed graphs separated by the latter but not by the former. This holds true for  MPNNs applied both on the directed and undirected graph.
We observe these theoretical findings to be in line with the empirical results detailed in~\cref{fig:synthetic_gcn_gat} and~\cref{tab:full_direction_ablation}, where \oursacro{} performs comparably or better (typically in the case of heterophily) than MPNNs. \section{Related Work}
\label{sec:related_work}



\textbf{GNNs for directed graphs.}
While several classical papers have alluded to the extension of their spatial models to directed graphs, empirical validation has not been conducted~\cite{Scarselli:2009ku,li2015gated,gilmer2017neural}. GatedGCN~\cite{DBLP:journals/corr/LiTBZ15} deals with directed graphs, however it aggregates information only from out-neighbors, neglecting potentially valuable information from in-neighbors. More recently,~\citet{vrvcek2022learning} tackle the genome assembly problem by employing a GatedGCN with separate aggregations for in- and out-neighbors.
Various approaches have been developed to generalize spectral convolutions for directed graphs~\cite{spectral-dgcn,motifnet}. Of particular interest are DGCN~\cite{dgcn}, which leverages  and  for its convolution (see~\cref{sec:comparison_with_directed_graph_methods} for a more detailed comparison with Dir-GNN), DiGCN~\cite{digraph}, which uses Personalized Page Rank matrix as a generalized Laplacian and incorporates -hop diffusion matrices, and MagNet~\cite{zhang2021magnet}, which adopts a complex matrix for graph diffusion where the real and imaginary parts represent the undirected adjacency and edge direction respectively. 
The above spectral methods share the following limitations: 1) in-neighbors and out-neighbors share the same weight matrix, which restricts expressivity; 2) they are specialized models, often inspired by GCN, as opposed to broader frameworks; 3) their scalability is severely limited due to their spectral nature. Concurrently to our work, \citet{pmlr-v202-geisler23a} extend transformers to directed graph for the task of graph classification, while~\citet{maskey2023fractional} generalize the concept of oversmoothing to directed graphs. 

\textbf{GNNs for relational graphs.} While counter intuitive at first, a directed graph cannot be equivalently represented by an \textit{undirected} relational graph (see~\cref{sec:alternative_representations} for more details). However, our Dir-GCN model can be considered as a Relational Graph Convolutional Network (R-GCN)~\cite{10.1007/978-3-319-93417-4_38} applied to an augmented \textit{directed} relational graph that incorporates two relation types: one for the original edges and another for the inverse edges added to the graph. Several papers handle multi-relational directed graphs by adding inverse relations~\cite{10.1007/978-3-319-93417-4_38,marcheggiani-titov-2017-encoding,DBLP:journals/corr/abs-1904-08745,Vashishth2020Composition-based}. 
Similarly to the above, directionality can be addressed using an MPNN~\cite{gilmer2017neural} combined with binary edge features, although at the cost of increased memory usage (see~\cref{sec:mpnn_binary_features} for more details).
In our work, however, we are the first to perform an in-depth investigation of the role of directionality in graph learning and its relation with homophily of the graph.

\textbf{Heterophilic GNNs.}
Several GNN architectures have been proposed to handle heterophily. One way amounts to effectively allow the model to enhance the high-frequency components by learning `generalized' negative weights on the graph \cite{chien2020adaptive,bo2021beyond,luan2022revisiting,bodnar2022neural,di2022graph}.  
A different approach tries to enlarge the neighbourhood aggregation to take advantage of the fact that on heterophilic graphs, the likelihood of finding similar nodes increases beyond the 1-hop \cite{abu-el-haijaMixHopHigherOrderGraph2019,zhu2020beyond,lim2021large,maurya2021improving,li2022finding}.
 \section{Experiments} \label{sec:experiments}

\subsection{Synthetic Task} \label{sec:synthetic_experiments}

\textbf{Setup.} In order to show the limits of current MPNNs, we design a synthetic task where the label of a node depends on both its in- and out-neighbors: it is one if the mean of the scalar features of their in-neighbors is greater than the mean of the features of their out-neighbors, or zero otherwise (more details in~\cref{sec:appendix_synthetic_experiment}). We report the results using GraphSage as base MPNN, but similar results were obtained with GCN and GAT and reported in~\cref{fig:synthetic-gcn-gat} of the Appendix. We compare GraphSage on the undirected version of the graph (Sage), with three \oursacro{} extensions of GraphSage using different convex combination coefficients : Dir-Sage() (only considering in-edges), Dir-Sage() (only considering out-edges) and Dir-Sage() (considering both in- and out-edges equally).

\textbf{Results.} The results in~\cref{fig:synthetic_task} show that only \textit{Dir-Sage(=0.5)}, which accounts for both directions, is able to almost perfectly solve the task. Using only in- or out-edges results in around 75\% accuracy, whereas GraphSage on the undirected graph is no better than a random classifier. 

\begin{table*}[t]
\begin{center}
\begin{small}
\begin{sc}
\resizebox{1.\textwidth}{!}{\begin{tabular}{lccc:ccccc}
\toprule
{} & \multicolumn{3}{c:}{Homophilic} & \multicolumn{5}{c}{Heterophilic} \\
{} & citeseer\_full & cora\_ml & ogbn-arxiv & chameleon & squirrel & arxiv-year & snap-patents & roman-empire \\ 
hom.      & 0.949  &  0.792  &  0.655 &  0.235  &  0.223 &  0.221   & 0.218   & 0.05    \\
hom. gain & 1.36\% &  2.84\% & 6.30\% & 15.71\% & 2.38\% &  22.67\% & 40.32\% & 66.85\% \\
\midrule
gcn      &           93.37  0.22 &           84.37  1.52 &  \textbf{68.39  0.01} &           71.12  2.28 &           62.71  2.27 &           46.28  0.39 &           51.02  0.07 &           56.23  0.37 \\
dir-gcn  &  \textbf{93.44  0.59} &  \textbf{84.45  1.69} &           66.66  0.02 &  \textbf{78.77  1.72} &  \textbf{74.43  0.74} &  \textbf{59.56  0.16} &  \textbf{71.32  0.06} &  \textbf{74.54  0.71} \\
\hdashline
sage     &  \textbf{94.15  0.61} &  \textbf{86.01  1.56} &  \textbf{67.78  0.07} &           61.14  2.00 &           42.64  1.72 &           44.05  0.02 &           52.55  0.10 &           72.05  0.41 \\
dir-sage &           94.14  0.65 &           85.84  2.09 &           65.14  0.03 &  \textbf{64.47  2.27} &  \textbf{46.05  1.16} &  \textbf{55.76  0.10} &  \textbf{70.26  0.14} &  \textbf{79.10  0.19} \\
\hdashline
gat      &  \textbf{94.53  0.48} &  \textbf{86.44  1.45} &  \textbf{69.60  0.01} &           66.82  2.56 &           56.49  1.73 &           45.30  0.23 &                   OOM &           49.18  1.35 \\
dir-gat  &           94.48  0.52 &           86.21  1.40 &           66.50  0.16 &  \textbf{71.40  1.63} &  \textbf{67.53  1.04} &  \textbf{54.47  0.14} &                   OOM &  \textbf{72.25  0.04} \\
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\caption{Ablation study comparing base MPNNs on the undirected graphs versus their \oursacro{} extension on the directed graphs. Homophilic datasets, located to the left of the dashed line, show little to no improvement when incorporating directionality, sometimes even experiencing a minor decrease in performance. Conversely, heterophilic datasets, found to the right of the dashed line, demonstrate large accuracy improvements when directionality is incorporated into the model.}
\label{tab:direction_ablation}
\end{table*}
 
\subsection{Extending Popular GNNs with \oursacro{}}
\label{sec:experiments_extending_gnn}

\textbf{Datasets.} We evaluate on the task of node classification on several directed benchmark datasets with varying levels of homophily: Citesee-Full, Cora-ML~\cite{bojchevski2018deep}, OGBN-Arxiv~\cite{hu2020ogb}, Chameleon, Squirrel~\cite{pei2020geom}, Arxiv-Year, Snap-Patents~\cite{lim2021large} and Roman-Empire~\cite{platonov2023a} (refer to~\cref{tab:datasets-statistics} for dataset statistics). While the first three are mainly homophilic (edge homophily greater than 0.65), the last five are highly heterophilic (edge homophily smaller than 0.24). Refer to~\cref{app:experimental_setup} for more details on the experimental setup and on dataset splits.

\textbf{Setup.} We evaluate the gain of extending popular undirected GNN architectures (GCN~\cite{kipf2016semi}, GraphSage~\cite{hamilton2017inductive} and GAT~\cite{velivckovic2017graph}) with our framework. For this ablation, we use the same hyperparameters (provided in~\cref{sec:appendix_ablation_hyperparams}) for all models and datasets. The aggregated results are plotted in~\cref{fig:aggregate_results}, while the raw numbers are reported in~\cref{tab:direction_ablation}. For \oursacro{}, we take the best results out of  (see~\cref{tab:full_direction_ablation} for the full results).

\textbf{Results.} We report aggregated results in~\cref{fig:aggregate_results}, while~\cref{tab:direction_ablation} shows the results for each dataset. On \textbf{heterophilic datasets}, using directionality brings exceptionally large gains (10\% to 20\% absolute) in accuracy across all three base GNN models. On the other hand, on \textbf{homophilic datasets} using directionality leaves the performance unchanged or slightly hurts. This is in line with the findings of Tab.~\ref{tab:node_homophilies}, which shows that using directionality as in our framework generally increases the effective homophily of heterophilic datasets, while leaving it almost unchanged for homophilic datasets. 
The inductive bias of undirected GNNs to propagate information in the \emph{same} way in both directions is beneficial on homophilic datasets where edges encode a notion of class similarity. Moreover, averaging information across {\em all} your neighbors, independent of direction, leads to a low-pass filtering effect that is indeed beneficial on homophilic graphs \cite{nt2019revisiting}. In contrast, \oursacro{} has to learn to align in- and out-convolutions since they have independent weights.



\subsection{Comparison with State-of-the-Art Models}
\label{sec:experiments_sota}

\begin{table*}[t]
\begin{center}
\begin{small}
\begin{sc}
\resizebox{.7\textwidth}{!}{\begin{tabular}{lcccccr}
\toprule
             &  Squirrel                   &  Chameleon                  &  Arxiv-year                 &  Snap-patents               &  Roman-Empire\\
\midrule
MLP          &  28.77  1.56           &  46.21  2.99           &  36.70  0.21           &  31.34  0.05           &  64.94  0.62          \\
GCN          &  53.43  2.01           &  64.82  2.24           &  46.02  0.26           &  51.02  0.06           &  73.69  0.74          \\
\hdashline
HGCN     &  37.90  2.02           &  59.39  1.98           &  49.09  0.10           &  OOM                        &  60.11  0.52          \\
GPR-GNN      &  54.35  0.87           &  62.85  2.90           &  45.07  0.21           &  40.19  0.03           &  64.85  0.27          \\
LINKX        &  61.81  1.80           &  68.42  1.38           &  56.00  0.17           &  61.95  0.12           &  37.55  0.36          \\
FSGNN        &  74.10  1.89           &  78.27  1.28           &  50.47  0.21           &  65.07  0.03           &  79.92  0.56          \\
ACM-GCN      &  67.40  2.21           &  74.76  2.20           &  47.37  0.59           &  55.14  0.16           &  69.66  0.62          \\
GloGNN       &  57.88  1.76           &  71.21  1.84           &  54.79  0.25           &  62.09  0.27           &  59.63  0.69          \\
Grad. Gating &  64.26  2.38           &  71.40  2.38           &  63.30  1.84           &  69.50  0.39           &  82.16  0.78          \\
\hdashline
DiGCN        &  37.74  1.54           &  52.24  3.65           &  OOM                        &  OOM                        &  52.71  0.32          \\ 
MagNet       &  39.01  1.93           &  58.22  2.87           &  60.29  0.27           &  OOM                        &  88.07  0.27          \\
\hdashline
\oursacro{}  &  \textbf{75.31  1.92}  &  \textbf{79.71  1.26}  &  \textbf{64.08  0.26}  &  \textbf{73.95  0.05}  &  \textbf{91.23  0.32} \\
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\caption{Results on real-world directed heterophilic datasets. OOM indicates out of memory. }
\label{tab:heterophilic_results}
\end{table*}
 
\textbf{Setup.} Given the importance of directionality on heterophilic tasks, we compare Dir-GNN with state-of-the-art models on heterophilic benchmarks Chameleon, Squirrel~\cite{pei2020geom}, Arxiv-Year, Snap-Patents~\cite{lim2021large} and Roman-Empire~\cite{platonov2023a}. In particular, we compare to \textbf{simple baselines}: MLP and GCN~\cite{kipf2016semi}, \textbf{heterophilic state-of-the-art models}: HGCN~\cite{zhu2020beyond}, GPR-GNN~\cite{chien2020adaptive}, LINKX~\cite{lim2021large}, FSGNN~\cite{maurya2021improving}, ACM-GCN~\cite{luan2022revisiting}, GloGNN~\cite{li2022finding}, Gradient Gating~\cite{rusch2022gradient}, and \textbf{state-of-the-art models for directed graphs}: DiGCN~\cite{digraph} and MagNet~\cite{zhang2021magnet}.~\cref{sec:appendix_baseline_results} contains more details on how baseline results were obtained. Differently from the results in~\cref{tab:direction_ablation}, we now tune the hyperparameters of our model using a grid search (see~\cref{sec:appendix_grid_search} for the exact ranges).


\textbf{Results.} In~\cref{tab:heterophilic_results} we observe that \oursacro{} obtains {\bf new state-of-the-art} results on all five heterophilic datasets, outperforming complex methods which were specifically designed to tackle heterophily. These results suggest that, when present, {\em using the edge direction can significantly improve learning on heterophilic graphs}, justifying the title of the paper. In contrast, discarding it is so harmful that not even complex architectures can make up for this loss of information. We further note that DiGCN and MagNet, despite being specifically designed for directed graphs, struggle on Squirrel and Chameleon. This is due to their inability to selectively aggregate from one direction while disregarding the other, a strategy that proves particularly advantageous for these two datasets (see~\cref{tab:full_direction_ablation}). Our proposed \oursacro{} framework overcomes this limitation thanks to its distinct weight matrices and the flexibility provided by the  parameter, enabling selective directional aggregation. \section{Conclusion}
\label{sec:conclusion}

\looseness=-1
We introduced \oursacro, a generic framework to extend any spatial graph neural network to directed graphs, which we prove to be strictly more expressive than MPNNs. We showed that treating the graph as directed improves the effective homophily of heterophilic datasets, and validated empirically that augmenting popular GNN architectures with our framework results in large improvements on heterophilic benchmarks, while leaving performance almost unchanged on homophilic benchmarks. Surprisingly, we found simple instantiations of our framework to obtain state-of-the-art results on the five directed heterophilic benchmarks we experimented on, outperforming recent architectures developed specifically for heterophilic settings as well as previously proposed methods for directed graphs.

\textbf{Limitations.} Our research has several areas that could be further refined and explored. First, the theoretical exploration of the conditions that lead to a higher effective homophily in directed graphs compared to their undirected counterparts is still largely unexplored. Furthermore, we have yet to investigate the expressivity advantage of \oursacro{} in the specific context of heterophilic graphs, where empirical gains were most pronounced.  Finally, we haven't empirically investigated different functional forms for aggregating in- and out-edges. These aspects mark potential areas for future enhancements and investigations.  
\section*{Acknowledgements}
Emanuele Rossi, Fabrizio Frasca and Michael Bronstein are supported in part by ERC Consolidator Grant No. 274228 (LEMAN).

\bibliographystyle{unsrtnat}
\bibliography{references}

\appendix
\crefalias{section}{appsec}
\crefalias{subsection}{appsec}
\crefalias{subsubsection}{appsec}
\newpage



\begin{figure}
    \centering
    \includegraphics[width=0.65\textwidth]{images/compatibility_matrices/citeseer-chameleon-compatibility-matrices.pdf}
    \caption{Compatibility matrices for the undirected version of Citeseer-Full (homophilic, left) and Chameleon (heterophilic, right).}
    \label{fig:citeseer_chameleon_compat}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=.8\textwidth]{images/compatibility_matrices/arxiv-year-compatibility-matrices-1-hop.pdf}
    \caption{Weighted compatibility matrices of the undirected diffusion operator  and the two directed diffusion operators  and  for Arxiv-Year. The last two have rows (classes) which are much more distinguishable then the first, despite still being heterophilic.}
    \label{fig:arxiv-year-compat}
\end{figure*}


\section{Compatibility Matrices}
\cref{fig:citeseer_chameleon_compat} shows the compatibility matrices for both Citeseer-Full (homophilic) and Chameleon (heterophilic). Additionally, ~\cref{fig:arxiv-year-compat} presents the weighted compatibility matrices of the undirected diffusion operator  and the two directed diffusion operators  and  for Arxiv-Year. The last two have rows (classes) which are much more distinguishable then the first, despite still being heterophilic. This phenomen, called harmless heterophily, is discussed in~\cref{sec:directed_heterophily}.

\section{Harmless Heterophily Through Directions}
It has been recently shown that heterophily is not necessarily harmful for GNNs, as long as nodes with the same label share similar neighborhood patterns, and different classes have distinguishable patterns~\cite{ma2022is, luan2023graph} . We find that some directed datasets, such as Arxiv-Year and Snap-Patents, show this form of \textit{harmless heterophily} when treated as directed, and instead manifest \textit{harmful heterophily} when made undirected (see~\cref{fig:arxiv-year-compat} in the Appendix). This suggests that using directionality can be beneficial also when using only one layer, as we confirm empirically (see~\cref{fig:arxiv1-layer} in the Appendix).

\textbf{Toy example.} We further illustrate the concepts presented in this Section with the toy example in Fig.~\ref{fig:synthetic_example}, which shows a directed graph with three classes (blue, orange, green). Despite the graph being maximally heterophilic, it presents harmless heterophily, since different classes have very different neighborhood patterns that can be observed clearly from the compatibility matrix of  (b). When the graph is made undirected (c), we are corrupting this information, and the classes become less distinguishable, making the task harder. We also note that both  and  presents perfect homophily (d), while  does not, in line with the discussion in previous paragraphs.

\begin{figure}[t!]
\vspace{-4mm}
\centering
\begin{subfigure}[b]{0.19\textwidth}
     \centering
     \includegraphics[width=\linewidth]{images/synthetic_example/harmless_heterophily.pdf}
     \caption{}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.19\textwidth}
     \centering
     \includegraphics[width=\linewidth]{images/synthetic_example/synthetic-compatibility-matrix-a.pdf}
     \caption{}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.19\textwidth}
     \centering
     \includegraphics[width=\linewidth]{images/synthetic_example/synthetic-compatibility-matrix-u.pdf}
     \caption{}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.19\textwidth}
     \centering
     \includegraphics[width=\linewidth]{images/synthetic_example/synthetic-compatibility-matrix-ata.pdf}
     \caption{}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.19\textwidth}
     \centering
     \includegraphics[width=\linewidth]{images/synthetic_example/synthetic-compatibility-matrix-uu.pdf}
     \caption{}
\end{subfigure}

\caption{(a) A toy directed graph with three classes showcasing harmless heterophily.
(b) Compatibility matrix of  showing that classes (blue, orange and green) have very different neighborhoods and can be easily distinguished. (c) Making the graph undirected makes the classes harder to distinguish, making the task harder to solve. 
(d) The mixed directed 2-hops ( and ) have perfect homophily, while (e) this is not the case for the undirected 2-hop.}
\label{fig:synthetic_example}
\vspace{-5mm}
\end{figure}

\section{Extension of Popular GNNs} \label{app:extensions}
We first consider an extension of GraphSAGE \cite{hamilton2017inductive} using our \oursacro{} framework. The main choice reduces to that of normalization. In the spirit of GraphSAGE, we require the message-passing matrices  and  to both be row-stochastic. This is done by taking  and , respectively. In this case, the directed version of GraphSAGE becomes

\noindent Finally, we consider the generalization of GAT \cite{velivckovic2017graph} to the directed case. Here we simply compute attention coefficients over the in- and out-neighbours separately. If we denote the attention coefficient over the edge  by , then the update of node  at layer  can be computed as 

\noindent where  are both row-stochastic matrices with support given by  and , respectively.



\section{Analysis of Expressivity} \label{app:expressivity_analysis}

In this appendix we prove the expressivity results reported in \cref{sec:expressivity}, after restating them more formally. It is important to note that we cannot build on the expressivity results from \citet{barcelo2022weisfeiler}, since their scope is limited to undirected relational graphs, and (perhaps surprisingly) it is not possible to equivalently represent a directed graph with an undirected relational graphs, as we show in~\cref{sec:alternative_representations}. 


We start by introducing useful concepts which will be instrumental to our discussion. As commonly done, we will assume in our analysis that all nodes have constant scalar node features . 

\subsection{(Directed) Weisfeiler-Lehman Test} \label{sec:appendix_wl}
The 1-dimensional Weisfeiler-Lehman algorithm (1-WL), or color refinement, is a heuristic approach to the graph isomorphism problem, initially proposed by~\citet{weisfeiler1968reduction}. This algorithm is essentially an iterative process of vertex labeling or coloring, aimed at identifying whether two graphs are non-isomorphic.

Starting with an identical coloring of the vertices in both graphs, the algorithm proceeds through multiple iterations. In each round, vertices with identical colors are assigned different colors if their respective sets of equally-colored neighbors are unequal in number. The algorithm continues this process until it either reaches a point where the distribution of vertices colors is different in the two graphs or converges to the same distribution. In the former case, the algorithm concludes that the graphs are not isomorphic and halts. Alternatively, the algorithm terminates with an inconclusive result: the two graphs are `possibly isomorphic'. It has been shown that this algorithm cannot distinguish all non-isomorphic graphs~\cite{cai1992anoptimal}. 

Formally, given an undirected graph , the 1-WL algorithm calculates a node coloring  for each iteration , as follows:



where  is a function that injectively assigns a unique color, not used in previous iterations, to the pair of arguments. The function  represents the set of neighbours of .

Since we deal with directed graphs, it is necessary to extend the 1-WL test to accommodate directed graphs. We note that a few variants have been proposed in the literature~\cite{Grohe2021ColorRA,DBLP:journals/corr/abs-1904-08745,Kollias2022DirectedGA}. Here, we focus on a variant whereby in- and out-neighbours are treated separately, as discussed in~\citep{Grohe2021ColorRA}. This variant, which we refer to as \textit{D-WL}, refines colours as follows:



where  and  are the set of out- and in-neighbors of , respectively. Our first objective is to demonstrate that \oursacro{} is as expressive as D-WL. Establishing this will enable us to further show that \oursacro{} is strictly more expressive than an MPNN operating on either the directed or undirected version of a graph. Let us start by introducing some further auxiliary tools that will be used in our analysis.

\subsection{Expressiveness and color refinements}

A way to compare graph models (or algorithms) in their expressiveness is by contrasting their discriminative power, that is the ability they have to disambiguate between non-isomorphic graphs.

Two graphs are called \emph{isomorphic} whenever there exists a graph isomorphism between the two:
\begin{definition}[Graph isomorphism]\label{def:isomorphism}
    Let ,  be two (directed) graphs. An \emph{isomorphism} between  is a bijective map  which preserves adjacencies, that is: . 
\end{definition}
\noindent On the contrary, they are deemed non-isomorphic when such a bijection does not exist. A model that is able to discriminate between two non-isomorphic graphs assigns them distinct representations. This concept is extended to families of models as follows:
\begin{definition}[Graph discrimination]
    Let  be any (directed) graph and  a model belonging to some family . 
    Let  and  be two graphs. We say  discriminates ,  iff . 
    We write . If there exists such a model , then family  distinguishes between the two graphs and we write . 
\end{definition}

Families of models can be compared in their expressive power in terms of graph disambiguation:
\begin{definition}[At least as expressive]\label{def:at_least_as_expressive}
    Let  be two model families. We say  is at least as expressive as . We write .
\end{definition}

Intuitively,  is at least as expressive as  if when  discriminates a pair of graphs, also  does. Additionally, a family can be \emph{strictly} more expressive than another:

\begin{definition}[Strictly more expressive] \label{def:strictly_more_expressive}
    Let  be two model families. We say  is strictly more expressive than . Equivalently, .
\end{definition}

Intuitively,  is strictly more expressive than  if  is at least as expressive as  and there exist pairs of graphs that  distinguishes but  does not. 

Many graph algorithms and models operate by generating \emph{node colorings} or representations. These can be gathered into multisets of colors that are compared to assess whether two graphs are non-isomorphic. Other than convenient, in these cases it is interesting to characterise the discriminative power at the level of nodes by means of the concept of {color refinement}~\cite{morris2019weisfeiler,bodnar2022neural,bevilacqua2022equivariant}. 
\begin{definition}[Color refinement]
    Let  be a graph and  two coloring functions. Coloring  refines colouring  when .
\end{definition}
\noindent Essentially, when  refines , if any two nodes are assigned the same color by , the same holds for . Equivalently, if two nodes are distinguished by  (because they are assigned different colors), then they are also distinguished by . When, for any graph,  refines , then we write  and, when also the opposite holds, (), we then write . As an example, for any  it can be shown that, on any graph, the coloring generated by the 1-WL algorithm at round  refines that at round , that is ; this being essentially due to the injectivity property of the  function.

Importantly, as we were anticipating above, this concept can be directly translated into graph discrimination as long as graphs are represented by the multiset of their vertices' colors, or an \emph{injection} thereof. This link, which explains the use of the same symbol to refer to the concepts of color refinement and discriminative power, is explicitly shown, for example, in~\citet{pmlr-v139-bodnar21a,bevilacqua2022equivariant}. More concretely, it can be shown that, if coloring  refines coloring , then the algorithm which generates  is at least as expressive as the one generating , as long as multisets of node colours are directly compared to discriminate between graphs, or they are first encoded by a multiset injection before the comparison is carried out. In the following we will resort to the concept of color refinement to prove some of our theoretical results. This approach is not only practically convenient for the required derivations, but it also informs us on the discriminative power models have at the level of nodes, something which is of relevance to us given our focus on node-classification tasks.

Furthermore, even though \oursacro{} outputs node-wise embeddings, it can be augmented with a global readout function to generate a single graph-wise embeddings . We will assume that all models discussed in this section are augmented with a global readout function.

\subsection{MPNNs on Directed Graphs}

Before moving forward to prove our expressiveness results, let us introduce the families of architectures we compare with. These embody straightforward approaches to adapt MPNNs to directed graphs.

Let MPNN-D be a model that performs message-passing by only propagating messages in accordance with the directionality of edges. Its layers can be defined as follows:



Instead, let MPNN-U be a model which propagates messages equally along any incident edge, independent of their directionality. Its layers can be defined as follows:


Note that if there are no bi-directional edges, MPNN-U is equivalent to first converting the graph to its undirected form (where the edge set is redefined as ) and then running an undirected MPNN(~\cref{eq:mpnn}). In practice, we observe that the number of bi-directional edges is generally small on average, while \emph{extremely} small on specific datasets (see~\cref{tab:datasets-statistics}). In these cases, we expect the empirical performance of the two approaches to be close to each other. We remark that, in our experiments, we opt for the latter strategy as it is easier and more efficient to implement.

We can now formally define families for the models we will be comparing.

\begin{definition}[Model families] \label{def:model_families}
    Let  be the family of Message Passing Neural Networks on the directed graph (\cref{eq:mpnn-d}),  that of Message Passing Neural Networks on the undirected form of the graph (\cref{eq:mpnn-u}), and  that of \oursacro{} models (\cref{eq:directed-mpnn}).
\end{definition}

\subsection{Comparison with D-WL} \label{app:d-wl-proof}

We start by restating~\cref{thm:dirgnn-as-expressive-as-d-wl} more formally:

\begin{theorem}\label{thm:dirgnn-as-expressive-as-d-wl-formal}
     is as expressive as D-WL if , , and  are injective for all  and node representations are aggregated via an injective  function. 
\end{theorem}

We now prove the theorem by showing that D-WL and \oursacro{} (under the hypotheses of the theorem) are equivalent in their expressive power. We will show this in terms of color refinement and, in particular, by showing that, not only the D-WL coloring at any round  refines that induced by any \oursacro{} at the same time step, but also that, when \oursacro{}'s components are injective, the opposite holds.

\begin{proof}[Proof of \Cref{thm:dirgnn-as-expressive-as-d-wl-formal}]

Let us begin by showing that \oursacro{} is upper-bounded in expressive power by the D-WL test. We do this by showing that, at any , the D-WL coloring  refines the coloring induced by the representations of any \oursacro{}, that is, on any graph , , where  refers to the representation of node  in output from any \oursacro{} at layer . For  nodes are populated with a constant color: , or an appropriate encoding thereof in the case of the Dir-GNN .

We proceed by induction. The base step trivially holds for  given how nodes are initialised. As for the recursion step, let us assume the thesis hold for ; we seek to prove it also hold for , showing that .  implies the equality of the inputs of the  function given it is injective. That is: , , and . By the induction hypothesis, we immediately get . Also, the induction hypothesis, along with \citep[Lemma 2]{bevilacqua2022equivariant}, gives us: , and . Given that , we also have , and : it would be sufficient, for example, to construct the well-defined function  and invoke~\citep[Lemma 3]{bevilacqua2022equivariant}. These all represents the only inputs to a \oursacro{} layer -- the two  and the  function in particular. Being well defined functions, they must return equal outputs for equal inputs, so that  .

In a similar way, we show that, when  and  functions are injective, the opposite hold, that is, . The base step holds for  for the same motivations above. Let us assume the thesis holds for  and seek to show that for . If , then . As  is injective, it must also hold , which, by the induction hypothesis, gives . Furthermore, by the same argument, we must also have , and . At this point we recall that, for any node ,  and , where, by our assumption, ,  are injective. This implies the equality between the multisets in input, i.e. , and . From these equalities it clearly follows , and  -- one can invoke~\citep[Lemma 3]{bevilacqua2022equivariant} with the well defined function . Again, by the induction hypothesis, and \citep[Lemma 2]{bevilacqua2022equivariant}, we have  and . Finally, as all and only inputs to the  function are equal, . The proof then terminates: as the  function is assumed to be injective, having proved the refinement holds at the level of nodes, this is enough to also state that, if two graphs are distinguished by D-WL they are also distinguished by a \oursacro{} satisfying the injectivity assumptions above.
\end{proof}

As for the existence and implementation of these injective components, constructions can be found in \citet{DBLP:conf/iclr/XuHLJ19} and \citet{corso2020principal}.
In particular, in \citep[Lemma 5]{DBLP:conf/iclr/XuHLJ19}, the authors show that, for a countable , there exist maps  such that function  is injective for subsets  of bounded cardinality, and any multiset function  can be decomposed as  for some function . As  is countable, there always exists an injection , and function  can be constructed, for example, as , with  being the maximum (bounded) cardinality of subsets . These constructions are used to build multiset aggregators in the GIN architecture~\citep{DBLP:conf/iclr/XuHLJ19} when operating on features from a countable set and neighbourhoods of bounded size. Under the same assumptions, the same constructions can be readily adapted to express the aggregators  as well as  in our \oursacro{}. 
Similarly to the above, under the same assumptions, injective maps for elements  can be constructed as  for infinitely many choice of , including all irrational numbers, and any function  on couples  can be decomposed as ~\citep[Corollary 6]{DBLP:conf/iclr/XuHLJ19}. The same approach can be extended to our use-case. In fact, for irrationals , an injection on triple  (with  of bounded size and  countable) can be built as , where  is an injection on a countable set and  realise injections over couples  as described above. This construction can be used to express the  components of \oursacro{}. 
In practice, in view of the Universal Approximation Theorem (UAT)~\citep{HORNIK1989359}, \citet{DBLP:conf/iclr/XuHLJ19} propose to use Multi-Layer Perceptrons (MLPs) to learn the required components described above, functions  and  in particular. We note that, in order to resort to the original statement of the UAT, this approach additionally requires boundedness of set  itself. Similar practical parameterizations can be used to build our desired \oursacro{} layers. Last, we refer readers to~\citep{corso2020principal} for constructions which can be adopted in the case where initial node features have a continuous, uncountable support.

\subsection{Comparison with MPNNs}

In this subsection we prove \cref{thm:dirgnn-strictly-more-expressive-than-mpnn}, which we restate more formally and split into two separate parts, one regarding MPNN-D and the other regarding MPNN-U. We start by proving that \oursacro{} is stricly more expressive than MPNN-D, i.e an MPNN which operates on directed graphs by only propagating messages according to the directionality of edges:

\begin{theorem}\label{thm:dirgnn-strictly-more-expressive-than-mpnn-d}
     is strictly more powerful than .
\end{theorem}

We begin by first proving the following lemmas:

\begin{lemma}\label{lemma:dirgnn-at-least-as-expressive-as-mpnn-d}
     is at least as expressive as  (). 
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lemma:dirgnn-at-least-as-expressive-as-mpnn-d}]
    We prove this Lemma by noting that the \oursacro{} architecture generalizes that of an MPNN-D, so that a \oursacro{} model can (learn to) simulate a standard MPNN-D by adopting particular weights. Specifically, \oursacro{} defaults to MPNN-D (which only sends messages along the out edges) if , i.e.  ignores in-messages, and the two readout modules coincide. Importantly, the direct implication of the above is that whenever an MPNN-D model distinguishes two graphs, then there exists a \oursacro{} which can implement such a model and then discriminate the two graphs as well.
\end{proof}

\begin{figure*}
    \centering
    \includegraphics[width=.7\textwidth]{images/expressiveness/mpnn-d_fails.pdf}
    \vspace{3mm}
    \caption{Two non-isomorphic directed graphs that cannot be distinguished by any MPNN-D model but can be distinguished by \oursacro{}.}
    \label{fig:mpnn-d_fails}
\end{figure*}

\begin{lemma}\label{lemma:exists-graphs-distinguished-by-dirgnn-but-not-by-mpnn-d}
    There exist graph pairs discriminated by a \oursacro{} model which are not discriminated by any MPNN-D model.
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lemma:exists-graphs-distinguished-by-dirgnn-but-not-by-mpnn-d}]
    Let  and  be the non-isomorphic graphs illustrated in~\cref{fig:mpnn-d_fails}. To confirm that they are not isomorphic, simply note that node  in  has an in-degree of two, while no node in  has an in-degree of two. 
    
    To prove that no MPNN-D model can distinguish between the two graphs, we will show that any MPNN-D induce the same coloring for the two graphs. In particular, we will show that, if  refers to the representation a MPNN-D computes for node  at time step , then  and  for any .

    We proceed by induction. The base step trivially holds for  given that nodes are all initialised with the same color. As for the inductive step, let us assume that the statement holds for  and prove that it also holds for .
    Assume  and  (induction hypothesis).
    Then we have:
    
    The induction hypothesis then gives us that .
    As for the other nodes, we have:
    
    The induction hypothesis then gives us that . Importantly, the above holds for any parameters of the  and  functions. As \emph{any} MPNN-D will always compute the same set of node representations for the two graphs, it follows that no MPNN-D can disambiguate between the two graphs, no matter the way they are aggregated. To conclude our proof, we show that there exists \oursacro{} models that can discriminate the two graphs. In view of~\cref{thm:dirgnn-as-expressive-as-d-wl}, it is enough to show that the two graphs are disambiguated by D-WL.
    Applying D-WL to the two graphs leads to different colorings after two iterations (see~\cref{tab:wl-d-1}), so the D-WL algorithm terminates deeming the two graphs non-isomorphic. Then, by~\cref{thm:dirgnn-as-expressive-as-d-wl}, there exist \oursacro{}s which distinguish them. In fact, it is easy to even construct simple 1-layer architecture that can assign the two graphs distinct representations, an exercise which we leave to the reader. Importantly, note how \oursacro{} can distinguish between the two graphs hinging on the discrimination of non-isomorphic nodes such as 1, 2, something no MPNN-D is capable of doing.
\end{proof}

\begin{table*}[t]
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l||cccc|cccc}
\toprule
Iteration     & Node 1  & Node 2 & Node 3 & Node 4 & Node 5  & Node 6 & Node 7 & Node 8 \\
\midrule
1             &        &       &       &       &        &       &       &       \\
2             &        &       &       &       &        &       &       &       \\
\bottomrule
\end{tabular}

\begin{tabular}{lcccr}
\toprule
                                               &    \\
\midrule
initialize                                           &                           \\
             &                           \\
                     &                           \\
                           &                           \\
                    &                           \\
\bottomrule
\end{tabular}

\end{sc}
\end{small}
\end{center}
\caption{Node colorings at different iterations, as well as the  hash function, when applying D-WL to the two graphs in~\cref{fig:mpnn-d_fails}.}
\label{tab:wl-d-1}
\end{table*}




 
With the two results above prove \cref{thm:dirgnn-strictly-more-expressive-than-mpnn-d}.

\begin{proof}[Proof of Theorem~\ref{thm:dirgnn-strictly-more-expressive-than-mpnn-d}]
    The theorem follows directly from Lemmas~\ref{lemma:dirgnn-at-least-as-expressive-as-mpnn-d} and~\ref{lemma:exists-graphs-distinguished-by-dirgnn-but-not-by-mpnn-d}.
\end{proof}

\begin{figure*}
    \centering
    \includegraphics[width=.55\textwidth]{images/expressiveness/mpnn-u_fails.pdf}
    \caption{Two non-isomorphic directed graphs that cannot be distinguished by any MPNN-U model but can be distinguished by \oursacro{}.}
    \label{fig:mpnn-u_fails}
\end{figure*}

\begin{table*}[t]
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l||ccc|ccc}
\toprule
Iteration     & Node 1  & Node 2 & Node 3 & Node 4 & Node 5  & Node 6 \\
\midrule
1             &        &       &       &       &        &       \\
2             &        &       &       &       &        &       \\
\bottomrule
\end{tabular}

\begin{tabular}{lcccr}
\toprule
                                               &    \\
\midrule
initialize                                           &                           \\
              &                           \\
             &                           \\
             &                           \\
\bottomrule
\end{tabular}

\end{sc}
\end{small}
\end{center}
\caption{Node colorings at different iterations, , as well as the  hash function, when applying D-WL to the two graphs in~\cref{fig:mpnn-u_fails}.}
\label{tab:wl-d-2}
\end{table*}




 
Next, we focus on the comparison with MPNN-U, i.e. an MPNN on the undirected form of the graph: 

\begin{theorem}\label{thm:dirgnn-strictly-more-expressive-than-mpnn-u}
     is strictly more expressive than .
\end{theorem}

Instrumental to us is to consider a variant of the 1-WL test MPNN-U can be regarded as the neural counterpart of. In the following we will show that such a variant, which we call U-WL, generates colorings which refine the ones induced by \emph{any} MPNN-U and that, in turn, are refined by the D-WL test. In view of \Cref{thm:dirgnn-as-expressive-as-d-wl}, this will be enough to show that there exists Dir-GNNs refining any MPNN-U instantiation, so that, ultimately, .

\begin{lemma}\label{lemma:dirgnn-at-least-as-expressive-as-mpnn-u}
     is at least as expressive as  (). 
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lemma:dirgnn-at-least-as-expressive-as-mpnn-u}]
    Let us start by introducing the U-WL test, which, on an undirected graph, refines node colors as:
    
    that is, by the gathering neighbouring colors from each incident edge, independent of its direction. It is easy to show that U-WL generates a coloring that, at any round  is refined by the coloring generated by D-WL, i.e., for any graph  it holds that , where  refers to the coloring of D-WL. Again, proceeding by induction, we have the following. First, the base step hold trivially for . We assume the thesis holds true for  and seek to show it also holds for . If  then, by the injectivity of  we must have , which implies  via the induction hypothesis. Additionally, we have , and  which, by the induction hypothesis and~\citep[Lemma 2]{bevilacqua2022equivariant}, gives , and . From these equalities we then derive . Indeed, let us suppose that, instead,  and that, w.l.o.g., this is due by the existence of a color  such that its number of appearances in  is larger than that in . We write . Then, as these are all multisets, we can rewrite . Since , we must have that , which leads to necessarily having . However, this entails a contradiction, because by hypothesis we had that . Last, given that , and , being the only inputs to the  function in U-WL, then we also have that , concluding the proof of the refinement.

    Now, in view of \Cref{thm:dirgnn-as-expressive-as-d-wl}, it is sufficient to show that U-WL refines the coloring induced by any MPNN-U; the lemma will then follow by transitivity. We want to show that , the U-WL coloring  refines the coloring induced by the representations of any MPNN-U, that is, on any graph , , with  referring to the representation an MPNN-U assigns to node  at time step . The thesis is easily proved. It clearly holds for  if initial node representations are produced by a well-defined function . Then, if we assume the thesis holds for , we can show it also holds for . Indeed, if , from the injectivity of , it follows that  and . By the induction hypothesis, we have  and, jointly due to~\citep[Lemma 2]{bevilacqua2022equivariant}, . Given that , it also clearly holds that  -- it is sufficient to construct the well-defined function  and invoke~\citep[Lemma 3]{bevilacqua2022equivariant}. These are the inputs to the well-defined functions constituting the update equations of an MPNN-U architecture, eventually entailing .
\end{proof}

Now, with the following lemma we show that, not only  is at least as expressive as , there actually exist pairs of graphs distinguished by former family but not by the latter.

\begin{lemma}\label{lemma:exists-graphs-distinguished-by-dirgnn-but-not-by-mpnn-u}
    There exist graph pairs distinguished by a \oursacro{} model which are not distinguished by any MPNN-U model.
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lemma:exists-graphs-distinguished-by-dirgnn-but-not-by-mpnn-u}]
    Let  and  be the non-isomorphic graphs illustrated in \cref{fig:mpnn-u_fails}. From~\cref{tab:wl-u} we observe that U-WL is not able to distinguish between the two graphs, as after the first iteration all nodes still have the same color: the U-WL is at convergence and terminates concluding that the two graphs are possibly isomorphic. From \cref{lemma:dirgnn-at-least-as-expressive-as-mpnn-u}, we conclude that no MPNN-U can distinguish between the two graphs. On the other hand, applying D-WL to the two graphs leads to different colorings after two iterations (see~\cref{tab:wl-d-2}, so the D-WL algorithm terminates deeming the two graphs non-isomorphic. Then, by~\cref{thm:dirgnn-as-expressive-as-d-wl}, there exists \oursacro{}s which distinguish them. In fact, simple \oursacro{} architectures which distinguish the two graphs are easy to construct. Importantly, we note, again, how these architectures distinguish between the two graphs by disambiguating non-isomorphic nodes such as 4, 6, something no MPNN-U is capable of doing.
\end{proof}

Last, the two results above are sufficient to prove~\cref{thm:dirgnn-strictly-more-expressive-than-mpnn-u}.

\begin{proof}[Proof of Theorem~\ref{thm:dirgnn-strictly-more-expressive-than-mpnn-u}]
    The theorem follows directly from Lemmas~\ref{lemma:dirgnn-at-least-as-expressive-as-mpnn-u} and~\ref{lemma:exists-graphs-distinguished-by-dirgnn-but-not-by-mpnn-u}.
\end{proof}


\begin{table*}[t]
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l||cccc|cccc}
\toprule
Iteration     & Node 1  & Node 2 & Node 3 & Node 4 & Node 5  & Node 6 & Node 7 & Node 8 \\
\midrule
1             &        &       &       &       &        &       &       &       \\
2             &        &       &       &       &        &       &       &       \\
\bottomrule
\end{tabular}

\begin{tabular}{lcccr}
\toprule
                                                      &    \\
\midrule
initialize                                                  &                           \\
             &                           \\
\bottomrule
\end{tabular}

\end{sc}
\end{small}
\end{center}
\caption{Node colorings at different iterations, as well as the  hash function, when applying U-WL to the two graphs in~\cref{fig:mpnn-u_fails}.}
\label{tab:wl-u}
\end{table*}




  
\begin{figure}
    \centering
    \includegraphics[width=0.65\textwidth]{images/representations/relational_graph_fail.pdf}
    \caption{The two non-isomorphic directed graphs  and  become isomorphic when converted to an undirected relational graph. This shows that is not possible to represent directed graphs with undirected relational graphs without losing information.}
    \label{fig:relational_graph_fail}
\end{figure}

\section{Exploring Alternative Representations for Directed Graphs} \label{sec:alternative_representations}
One might intuitively consider the possibility of representing a directed graph equivalently through an undirected relational graph, having two relations for \textit{original} and \textit{inverse} edges. However, this assumption proves to be erroneous, as illustrated by the following counterexample. Take the two non-isomorphic directed graphs illustrated in~\cref{fig:relational_graph_fail}. Surprisingly, these two directed graphs share an identical representation as an undirected relational graph, as depicted in~\cref{fig:relational_graph_fail}. 

\citet{Kollias2022DirectedGA} showed that it is however possible to equivalently represent a directed graph with an undirected graph having two nodes for each node in the original graph, representing the source and destination role of the node respectively.

\section{MPNN with Binary Edge Features} \label{sec:mpnn_binary_features}
A possible alternative approach to address directed graphs involves using an MPNN~\cite{gilmer2017neural} combined with binary edge features. In this case, the original directed graph is augmented with inverse edges, which are assigned a distinct binary feature compared to the original ones. Given the original directed graph , we define the augmented graph as  with .

An MPNN with edge features can be defined as:



However, since messages do not depend only on the source nodes but also on the destination node (through the edge features), this approach necessitates the materialization of explicit edge messages~\cite{tailor2022adaptive}. Given that there are  such messages (one per edge) with dimension , the memory complexity amounts to . This is significantly higher than the  memory complexity achieved by instantiations of \oursacro{} such as Dir-GCN or Dir-SAGE, and could lead to out-of-memory issues for even moderately sized datasets. Therefore, while having the same expressivity, our model has better memory complexity. 

\section{Relation with Existing Methods for Directed Graphs} \label{sec:comparison_with_directed_graph_methods}
DGCN~\cite{dgcn} directly employs  and  for spectral convolution, alongside . They observe that these 2-hop matrices enhance feature and label smoothness, a finding that aligns with our observations in~\cref{sec:directed_heterophily}. However, DGCN limits its experiments to homophilic datasets, thereby limiting the applicability of their insights. Furthermore, the model encounters several limitations: it does not provide access to directed 1-hop edges ( and ); it is constrained to the 2-hop, without the capability to access higher hopsâcontrastingly, our model consistently benefits from utilizing 5 or 6 layers (see~\cref{tab:best_hyperparameters}); it is limited to a specific, GCN-like architecture, rather than a more general framework; and it lacks scalability due to the explicit computation of  and .


\section{Experimental Details} \label{sec:appendix_experimental_details}
\begin{table*}[t]
\begin{center}
\begin{small}
\begin{sc}
    \resizebox{1.\textwidth}{!}{\begin{tabular}{lcccccr}
    \toprule
    Dataset       & \# Nodes   & \# Edges    & \# Feat. & \# C & Unidirectional Edges & Edge hom. \\
    \midrule
    citeseer-full &     4,230  &      5,358  &     602  &   6  & 99.61\%     & 0.949 \\
    cora-ml       &     2,995  &      8,416  &    2,879 &   7  & 96.84\%     & 0.792 \\
    ogbn-arxiv    &   169,343  &   1,166,243 &     128  &  40  & 99.27\%     & 0.655 \\
    chameleon     &     2,277  &     36,101  &    2,325 &   5  & 85.01\%     & 0.235 \\
    squirrel      &     5,201  &    217,073  &    2,089 &   5  & 90.60\%     & 0.223 \\
    arxiv-year    &   169,343  &   1,166,243 &     128  &  40  & 99.27\%     & 0.221 \\
    snap-patents  &  2,923,922 &  13,975,791 &     269  &   5  & 99.98\%     & 0.218 \\
    roman-empire  &     22,662 &      44,363 &     300  &  18  & 65.24\%     & 0.050 \\
    \bottomrule
    \end{tabular}
    }
\end{sc}
\end{small}
\end{center}
\caption{Statistics of the datasets used in this paper.}
\label{tab:datasets-statistics}
\end{table*} \begin{table*}[t]
\begin{center}
\begin{small}
\begin{sc}
    \resizebox{1.\textwidth}{!}{
    \begin{tabular}{lcccccccr}
        \toprule
        Dataset       & model\_type &   lr   & \# hidden\_dim & \# num\_layers & jk     & norm  & dropout &  \\
        \midrule
        chameleon     &  Dir-GCN    & 0.005  &     128         &    5           &  max  & True  &  0      & 1. \\
        squirrel      &  Dir-GCN    & 0.01   &     128         &    4           &  max  & True  &  0      & 1. \\
        arxiv-year    &  Dir-GCN    & 0.005  &     256         &     6          &  cat  & False &  0      & 0.5 \\
        snap-patents  &  Dir-GCN    & 0.01   &     32          &     5          &   max & True  &  0      & 0.5 \\
        roman-empire  &  Dir-SAGE   & 0.01   &     256         &     5          &   cat & False &  0.2    & 0.5 \\
        \bottomrule
    \end{tabular}
    }
\end{sc}
\end{small}
\end{center}
\caption{Best hyperparameters for each dataset, determined through grid search, for our model.}
\label{tab:best_hyperparameters}
\end{table*} 
\subsection{Effective Homophily of Synthetic Graphs}  \label{sec:effective_homophily_synthetic}
For the results in~\cref{fig:synthetic_effective_homophily}, we generate directed synthetic graphs with various homophily levels using a modified preferential attachment process~\cite{Barabasi99emergenceScaling}, inspired by~\citet{zhu2020beyond}. New nodes are incrementally added to the graphs until the desired number of nodes is achieved. Each node is assigned a class label, chosen uniformly at random among  classes, and forms out-edges with exactly  pre-existing nodes, where  is a parameter of the process. The  out-neighbors are sampled without replacement from a distribution that is proportional to both their in-degree and the class compatibility of the two nodes. Consequently, nodes with higher in-degree are more likely to receive new edges, leading to a "rich get richer" effect where a small number of highly connected "hub" nodes emerge. This results in the in-degree distribution of the generated graphs following a power-law, with heterophily controlled by the class compatibility matrix . In our experiments, we generate graphs comprising 1000 nodes and set , . Note that by construction, the generated graphs will not have any bidirectional edge.

\subsection{Synthetic Experiment}  \label{sec:appendix_synthetic_experiment}
For the results in~\cref{fig:synthetic_task}, we construct an Erdos-Renyi graph with  nodes and edge probability of , where each node has a scalar feature sampled uniformly at random from . The label of a node is set to 1 if the mean of the features of their in-neighbors is greater than the mean of the features of their out-neighbors, or zero otherwise. 

\subsection{Experimental Setup} \label{app:experimental_setup}
Real-World datasets statistics are reported in table \ref{tab:datasets-statistics}.
All experiments are conducted on a GCP machine with 1 NVIDIA V100 GPU with 16GB of memory, apart from experiments on snap-patents which have been performed on a machine with 1 NVIDIA A100 GPU with 40GB of memory. The total GPU time required to conduct all the experiments presented in this paper is approximately two weeks.
In all experiments, we use the Adam optimizer and train the model for 10000 epochs, using early stopping on the validation accuracy with a patience of 200 for all datasets apart from Chameleon and Squirrel, for which we use a patience of 400. We do not use regularization as it did not help on heterophilic datasets. For Citeseer-Full and Cora-ML we use random 50/25/25 splits, for OGBN-Arxiv we use the fixed split provided by OGB~\cite{hu2020ogb}, for Chameleon and Squirrel we use the fixed GEOM-GCN splits~\cite{pei2020geom}, for Arxiv-Year and Snap-Patents we use the splits provided in \citeauthor{lim2021large}, while for Roman-Empire we use the splits from~\citet{platonov2023a}. We report the mean and standard deviation of the test accuracy, computed over 10 runs in all experiments. 

\subsection{Directionality Ablation Hyperparameters} \label{sec:appendix_ablation_hyperparams}
For the ablation study in~\cref{sec:experiments_extending_gnn}, we use the same hyperparameters for all models and datasets: , , , , .  refers to applying L2 normalization after each convolutional layer, which we found to be generally useful, while  refers to the type of jumping knowledge~\cite{pmlr-v80-xu18c} used.

\subsection{Comparison with State-of-the-Art Results} \label{sec:appendix_grid_search}

To obtain the results for Dir-GNN in~\cref{tab:heterophilic_results}, we perform a grid search over the following hyperparameters: Dir-GCN, Dir-SAGE, , , , , ,  and . The best hyperparameters for each dataset are reported in table \ref{tab:best_hyperparameters}.

\subsection{Baseline Results} \label{sec:appendix_baseline_results}
\paragraph{GNNs for Heterophily}
Results for HGCN, GPR-GNN and LINKX were taken from \citeauthor{lim2021large}. Results for Gradient Gating are taken from their paper~\cite{rusch2022gradient}. Results for FSGNN are taken from their paper~\cite{maurya2021improving} for Actor, Squirrel and Chameleon, whereas we re-implement it to generate results on Arxiv-year and Snap-Patents, performing the same gridsearch outlined in~\cref{sec:appendix_grid_search}. Results for GloGNN as well as MLP and GCN are taken from~\citeauthor{li2022finding}. Results on Roman-Empire are taken from~\citet{platonov2023a} for GCN, HGCN, GPR-GNN, FSGNN and GloGNN whereas we re-implement and generate results for MLP, LINKX, ACM-GCN and Gradient Gating performing the same gridsearch outlined in~\cref{sec:appendix_grid_search}.

\paragraph{Directed GNNs}
For DiGCN and MagNet, we used the classes provided by PyTorch Geometric Signed Directed library~\cite{he2022pytorch}. For MagNet, we tuned the , the , the , the  parameter for its chebyshev convolution to , and its  hyperparameter . For DiGCN, we tune the , the , the , and the  .



\begin{table*}[t]
\begin{center}
\begin{small}
\begin{sc}
\resizebox{0.6\textwidth}{!}{\begin{tabular}{lrrr}
\toprule
{} &  in\_degree &  out\_degree &  total\_degree \\
\midrule
cora\_ml               &      41.70\% &       11.65\% &          0.00\% \\
citeseer\_full         &      63.45\% &       21.35\% &          0.00\% \\
ogbn-arxiv            &      36.62\% &       10.30\% &          0.00\% \\
chameleon             &      62.06\% &        0.00\% &          0.00\% \\
squirrel              &      57.60\% &        0.00\% &          0.00\% \\
arxiv-year            &      36.62\% &       10.30\% &          0.00\% \\
snap-patents          &      23.38\% &       30.16\% &          6.09\% \\
directed-roman-empire &       0.00\% &        0.00\% &          0.00\% \\
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\caption{Percentage of nodes with either in-, out- or total-degree equal to zero.}
\label{tab:zero_degrees}
\end{table*}
 \begin{table*}[t]
\begin{center}
\begin{small}
\begin{sc}
\resizebox{1.\textwidth}{!}{\begin{tabular}{lccc:ccccr}
\toprule
& citeseer\_full & cora\_ml & ogbn-arxiv & chameleon & squirrel & arxiv-year & snap-patents & roman-empire \\ 
hom. & 0.949 &  0.792 &  0.655 &  0.235 &  0.223 &  0.221 & 0.218 & 0.050 \\ 
\midrule
gcn             &   93.370.22 &  84.371.52 &  68.390.01 &  71.122.28 &  62.712.27 &  46.280.39 &  51.020.07 &           56.230.37 \\
dir-gcn(=0.0)  &   93.210.41 &  84.451.69 &  23.700.20 &  29.781.27 &  33.030.78 &  50.510.45 &  51.710.06 &           42.690.41 \\
dir-gcn(=1.0)  &   93.440.59 &  83.811.44 &  62.930.21 &  78.771.72 &  74.430.74 &  50.520.09 &  62.240.04 &           45.520.14 \\
dir-gcn(=0.5)  &   92.970.31 &  84.212.48 &  66.660.02 &  72.371.50 &  67.821.73 &  59.560.16 &  71.320.06 &           74.540.71 \\
\hdashline
sage            &   94.150.61 &  86.011.56 &  67.780.07 &  61.142.00 &  42.641.72 &  44.050.02 &  52.550.10 &           72.050.41 \\
dir-sage(=0.0) &   94.050.25 &  85.842.09 &  52.080.17 &  48.332.40 &  35.310.52 &  47.450.32 &  52.530.03 &           76.470.14 \\
dir-sage(=1.0) &   93.970.67 &  85.730.35 &  65.140.03 &  64.472.27 &  46.051.16 &  50.370.09 &  61.590.05 &           68.810.48 \\
dir-sage(=0.5) &   94.140.65 &  85.811.18 &  65.060.28 &  60.221.16 &  43.291.04 &  55.760.10 &  70.260.14 &           79.100.19 \\
\hdashline
gat             &   94.530.48 &  86.441.45 &  69.600.01 &  66.822.56 &  56.491.73 &  45.300.23 &          OOM &           49.181.35 \\
dir-gat(=0.0)  &   94.480.52 &  86.131.58 &  52.570.05 &  40.443.11 &  28.281.02 &  46.010.06 &          OOM &           53.582.51 \\
dir-gat(=1.0)  &   94.080.69 &  86.211.40 &  66.500.16 &  71.401.63 &  67.531.04 &  51.580.19 &          OOM &           56.240.41 \\
dir-gat(=0.5)  &   94.120.49 &  86.051.71 &  66.440.41 &  55.571.02 &  37.751.24 &  54.470.14 &          OOM &           72.250.04 \\

\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\caption{Ablation study comparing base MPNNs on the undirected graph versus their \oursacro{} extensions on the directed graph. We conducted experiments with  (only in-edges),  (only out-edges), and  (both in- and out-edges, but with different weight matrices). For homophilic datasets (to the left of the dashed line), incorporating directionality does not significantly enhance or may slightly impair performance. However, for heterophilic datasets (to the right of the dashed line), the inclusion of directionality substantially improves accuracy.}
\label{tab:full_direction_ablation}
\end{table*}
 \section{Additional Results}

\begin{figure}[t!]
\centering
\label{fig:synthetic_gcn_gat}
\begin{minipage}[t]{0.5\linewidth}
  \centering
  \includegraphics[width=1\linewidth]{images/synthetic_experiment/synthetic_experiment_GCN.pdf}
\end{minipage}\begin{minipage}[t]{.5\linewidth}
  \centering
  \includegraphics[width=1\linewidth]{images/synthetic_experiment/synthetic_experiment_GAT.pdf}
\end{minipage}
\caption{Validation accuracy (solid lines) and training accuracy (dashed lines) of GCN (left) and GAT (right), as well as their respective extensions using our \oursacro{} framework, on a synthetic task which requires directionality information in order to be solved.}
  \label{fig:synthetic-gcn-gat}
\end{figure}

\subsection{Synthetic Experiment}
We also evaluate GCN and GAT (and their \oursacro{} extensions) on the synthetic task outlined in~\cref{sec:appendix_synthetic_experiment}. Similarly to what observed for GraphSage (\cref{fig:synthetic_task}), the \oursacro{} variant using both directions () significantly outperforms the other configurations, despite not reaching 100\% accuracy. The undirected models are akin to a random classifier, whereas the models using only one directions obtain between 70\% and 75\% of accuracy.


\subsection{Ablation Study on Using Directionality}
Table \ref{tab:full_direction_ablation} compares using the undirected graph vs using the directed graph with our framework with different . We observe that only on Chameleon and Squirrel, using only one direction of the edges, in particular the out direction, performs better than using both direction. Moreover, for these two datasets, the gap between the two directions ( vs ) is extremely large (more than 40\% absolute accuracy). We find that this is likely due to the high number of nodes with zero in neighbors, as reported in Table \ref{tab:zero_degrees}. Chameleon and Squirrel have respectively about 62\% and 57\% of nodes with no in-edges: when propagating only over in edges, these nodes would get zero features. 
We observe a similar trend for other datasets, where  performs generally better than , in line with the fact that all these datasets have more nodes with zero in edges than out edges (Table \ref{tab:zero_degrees}). In general, using both in- and out- edges is the preferred solution.


\begin{figure*}
    \centering
    \includegraphics[width=.5\textwidth]{images/arxiv-year-1-layer.pdf}
    \caption{Performance of GCN (on the undirected version of the graph) and Dir-GCN on Arxiv-Year when using only one layer. Remarkably, directionality yields significant benefits, even in the absence of access to the homophilic directed 2-hop. This is largely attributable to the harmless heterophily exhibited by the directed graph.}
    \label{fig:arxiv1-layer}
\end{figure*}

\subsection{Ablation Study on Using a Single Layer}
In~\cref{sec:directed_heterophily} we discuss how Arxiv-Year and Snap-Patents exhibit harmless heterophily when treating as directed. This suggest that even a 1-layer Dir-GNN model should be able to perform much better of its undirected counterpart, despite not being able to access the much more homophilic 2-hop. We verify this empirically by comparing a 1-layer GCN (on the undirected version of the graph) with a 1-layer Dir-GCN on Arxiv-Year. \cref{fig:arxiv1-layer} presents the results, showing that Dir-GCN does indeed significantly outperform GCN. 

\begin{figure*}
    \centering
    \includegraphics[width=0.5\textwidth]{images/alpha_ablation.pdf}
    \caption{\oursacro{} test accuracy on Arxiv-Year for different values of the hyperparameter .}
    \label{fig:alpha_ablation}
\end{figure*}

\subsection{Ablation Study on Different Values of \texorpdfstring{}{alpha}}
We train \oursacro{} models on Arxiv-Year with varying values of , using the hyperparameters outlined in~\cref{sec:appendix_ablation_hyperparams}. \cref{fig:alpha_ablation} presents the results: while a large drop is observed for  and , i.e. propagating messages only along one direction, the results for other values of  are largely similar. 

\subsection{Runtime Analysis}
We assess the runtime of Dir-GNN by comparing it with its undirected counterpart. Specifically, in \cref{fig:wall_time}, we illustrate a comparison between the validation accuracies of Dir-Sage and Sage over time, measured in seconds. Both models were executed on a single NVIDIA V100 GPU  with 16GB of memory. Notably, despite employing two separate weight matricesâwhich enhances its performanceâDir-Sage exhibits only a marginal increase in runtime. The respective runtimes per epoch are  seconds for Dir-Sage and  seconds for Sage.

\begin{figure*}
    \centering
    \includegraphics[width=0.5\textwidth]{images/wall_time.pdf}
    \caption{Validation Accuracy of Dir-Sage and Sage, displayed as a function of time (seconds).}
    \label{fig:wall_time}
\end{figure*}













 


\end{document}
