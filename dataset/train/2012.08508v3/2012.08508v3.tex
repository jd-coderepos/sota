\documentclass{article}
\usepackage{times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \PassOptionsToPackage{numbers,square,sort,compress}{natbib}

\usepackage[final]{neurips_2021}


\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}
\usepackage{amsmath}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{listings}

\usepackage{tikz}
\usepackage{enumitem}
\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;}
\newcommand{\instr}[1]{{\fontfamily{cmtt}\selectfont#1}}

\newcommand{\ignore}[1]{}
\newcommand{\pb}[1]{\textcolor{red}{\bf\small [#1 --PB]}}
\newcommand{\at}[1]{\textcolor{blue}{\bf\small [#1 --AT]}}
\newcommand{\Model}{\emph{Aloe}}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\felix}[1]{\textit{\color{magenta}[Felix: #1]}}
\newcommand{\david}[1]{\textit{\color{blue}[David: #1]}}

\definecolor{darkgreen}{RGB}{0, 153, 46}
\definecolor{darkbrown}{RGB}{153, 76, 0}
\definecolor{darkyellow}{RGB}{204, 204, 0}

\title{Attention over learned object embeddings enables complex visual reasoning}

\author{David Ding 
\qquad
Felix Hill
\qquad
Adam Santoro
\qquad
Malcolm Reynolds
\qquad
Matt Botvinick\\
{DeepMind} \\
{London, United Kingdom }\\
\texttt{\{fding, felixhill, adamsantoro, mareynolds, botvinick\}@google.com}
}

\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}

\begin{document}
\maketitle

\begin{abstract}

Neural networks have achieved success in a wide array of perceptual tasks
but often fail at tasks involving both perception and higher-level reasoning.
On these more challenging tasks,
bespoke approaches (such as modular symbolic components, independent dynamics models or semantic parsers) targeted towards that specific type of task
 have typically performed better.
The downside to these targeted approaches, however, is that they can be more brittle than general-purpose neural networks, requiring significant modification or even redesign according to the particular task at hand.
Here, we propose a more general neural-network-based approach to dynamic visual reasoning problems that obtains state-of-the-art performance on three different domains, in each case outperforming bespoke modular approaches tailored specifically to the task. Our method relies on learned object-centric representations, self-attention and self-supervised dynamics learning, and all three elements together are required for strong performance to emerge. The success of this combination suggests that there may be no need to trade off flexibility for performance on problems involving spatio-temporal or causal-style reasoning. With the right soft biases and learning objectives in a neural network we may be able to attain the best of both worlds. \blfootnote{Model Code: \url{https://github.com/deepmind/deepmind-research/tree/master/object_attention_for_reasoning}.}

\end{abstract}



\section{Introduction}


Despite the popularity of artificial neural networks, a body of recent work has focused on their limitations as models of cognition and reasoning. Experiments with dynamical reasoning datasets such as CLEVRER~\citep{clevrer}, CATER~\citep{cater}, and ACRE~\citep{acre} show that neural networks can fail to adequately reason about the spatio-temporal, compositional or causal structure of visual scenes.
On CLEVRER, where models must answer questions about the dynamics of colliding objects, previous experiments show that neural networks can adequately \emph{describe} the video, but fail when asked to \emph{predict}, \emph{explain}, or consider \emph{counterfactual} possibilities. Similarly, on CATER, an object-tracking task, models have trouble tracking the movement of objects when they are hidden in a container. Finally, on ACRE, a dataset testing for  causal inference, popular models only learned correlations between visual scenes and not the deeper causal logic.

Failures such as these on reasoning (rather than perception) problems have motivated the adoption of pipeline-style approaches
that combine a general purpose neural network (such as a convolutional block) with a task-specific module that builds in the core logic of the task. For example, on CLEVRER the NS-DR method~\citep{clevrer} applies a hand-coded symbolic logic engine (that has the core logic of CLEVRER built-in) to the outputs of a ``perceptual'' neural front-end, achieving better results than neural network baselines, particularly on counterfactual and explanatory problems. One limitation of these pipeline approaches, however, is that they are typically created with a single problem or problem domain in mind, and may not apply out-of-the-box to other related problems.
For example, to apply NS-DR to CATER,
the entire symbolic module needs to be rewritten to handle the new interactions and task logic of CATER:
the custom logic to handle collisions and object removal must be replaced with
new custom logic to handle occlusions and grid-resolution,
and these changes require further modifications to the perceptual front-end
to output data in a new format.
This brittleness is not exclusive to symbolic approaches. While Hungarian-matching between object embeddings may be well-suited for object-tracking tasks \citep{zhou2021hopper}, it is not obvious how it would help for
causal inference tasks.

Here, we describe a more general neural-network-based approach to visual spatio-temporal reasoning problems, which does not rely on task-specific integration of modular components. In place of these components, our model relies on three key aspects: 
\begin{itemize}[leftmargin=0.8cm]
    \item Self-attention to effectively integrate information over time
    \item Soft-discretization of the input at the most informative level of abstraction -- above pixels and local features, and below entire frames---corresponding approximately to `objects'
    \item Self-supervised learning, i.e. requiring the model to infer masked out objects, to extract more information about dynamics from each sample.
\end{itemize}
While many past models have applied each individual ingredient separately (including on the tasks we study),
we show that it is the \emph{combination of all three ingredients in the right way}
that allows our model to succeed.




The resulting model, which we call \Model{} (Attention over Learned Object Embeddings), outperforms both pipeline and neural-network-based approaches on three different task domains designed to test  physical and dynamical reasoning from pixel inputs. We highlight our key results here:
\begin{itemize}[leftmargin=0.8cm]
    \item \textbf{CLEVRER} (explanatory, predictive, and counterfactual reasoning): \Model{} achieves significantly higher accuracy than both more task-specific, modular approaches, and previous neural network methods on all question types. On counterfactual questions, thought to be most challenging for neural-only architectures, we achieve \textbf{75\%} vs \textbf{46\%} accuracy for more specialised methods.

    \item \textbf{CATER} (object-permanence): \Model{} achieves accuracy exceeding or matching other current models.
    Notably, the strongest alternative models were expressly designed for object-tracking,
    whereas our architecture is applicable without modification to other reasoning tasks as well.

    \item \textbf{ACRE} (causal-inference ``beyond the simple strategy of inducing causal
relationships by covariation'' \citep{acre}):  Overall, \Model{} achieves \textbf{94\%} vs the \textbf{67\%} accuracy achieved by the top neuro-symbolic model.
On the most challenging tasks,
we achieve, for ``backward-blocking'' inference, \textbf{94.48\%} (vs \textbf{16.06\%} by the best modular, neuro-symbolic systems),
and, for ``screen-off'' inference, \textbf{98.97\%}~(vs \textbf{0.00\%} by a CNN-BERT baseline). 
\end{itemize}

As we have emphasized, the previous best performing models for each task all contain task-specific design elements, whereas \Model{} can be applied \emph{to all the tasks without modification}.
On CLEVRER, we also show that \Model{} matches the performance of the previous best models with 40\% less training data, which demonstrates that our approach is data-efficient as well as performant.


\section{Methods}
\label{section:methods}
A guiding motivation for the design of \Model{} is the converging evidence for the value of self-attention mechanisms operating on a finite sequences of discrete entities. Written language is inherently discrete and hence is well-suited to self-attention-based approaches. In other domains, such as raw audio or vision, it is less clear how to leverage self-attention. We hypothesize that the application of self-attention-based models to visual tasks could benefit from an approximate `discretization' process, and determining the right level of discretization is an important choice that can significantly affect model performance.

At the finest level, data could simply be discretized into pixels (as is already the case for most machine-processed visual data).
Pixels are too fine-grained for many applications, however---for one, the memory required to support self-attention
across all pixels is prohibitive.
Partly for this reason,
coarser representations, such as the downsampled ``hyper-pixel'' outputs of a convolutional network,
are often used instead  (e.g. \citep{zambaldi2018deep,lu2019vilbert}).
In the case of videos, previous work considered even coarser discretization schemes, such as frame or subclip level representations \citep{sun-videobert}.

The neuroscience literature, however, suggests that biological visual systems infer and exploit the existence of \emph{objects}, rather than  spatial or temporal blocks with artificial boundaries \citep{roelfsema1998, spelke2000, chen2012}.
Because objects are the atomic units of physical interactions,
it makes sense to discretize on the level of objects.
Numerous object segmentation algorithms have been proposed \citep{ren2015faster, he2017maskrcnn, greff2019-iodine}. We chose to use MONet, an unsupervised object segmentation algorithm \citep{monet}. Because MONet is unsupervised, we can train it directly in our domain of interest without the need for object segmentation labels. We emphasize that our choice of MONet is an implementation
detail, and in Appendix~\ref{appendix:other-models}, we show that our framework of attention over learned object embeddings also works with other object-segmentation schemes.
We also do not need to place strong demands on the object segmentation algorithm,
e.g. for it to produce aligned output or to have a built-in dynamics model.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\textwidth,keepaspectratio]{clevrer_model.pdf}
    \caption{\label{figure:architecture} A schematic of the model architecture. See the main text for details.}
\end{figure*}


To segment each frame into object representations,~\textbf{MONet}  uses a recurrent attention network to obtain a set of  ``object attention masks'' ( is a fixed parameter). Each attention mask represents the probability that any given pixel belongs to that mask's object.
The pixels assigned to the mask are encoded into latent variables with means 
, where  indexes the object slot and  the frame.
These means are used as the object embeddings in \Model{}.
More details are provided in Appendix~\ref{appendix:monet}.




The \textbf{self-attention component} is a transformer model \citep{vaswani2017attention}
operating on a sequence of vectors in :
the object representations  for all  and ,
a trainable vector  used to generate classification results
(analogous to the CLS token in BERT \citep{devlin2018bert}),
and (for CLEVRER) the embedded words 
    from the question (and choice for multiple choice questions).
For the object representations  and word embeddings ,
we append a two-dimensional one-hot vector to  and 
to indicate whether the input is a word or an object.
Because the transformer is shared between the modalities,
information can flow between objects and words to solve the task,
as we show in Section~\ref{section:clevrer}.

We pass this sequence of vectors
through a transformer with  layers.
All inputs are first projected (via a linear layer and ReLU activation) to , where  is the number of self-attention heads.
We add a relative sinusoidal positional encoding at each layer of the transformer
to give the model knowledge of the word and frame order \citep{dai2019transformer}.
The transformed value of  is passed through an MLP (with one hidden layer of size ) to generate the final answer.
A schema of our architecture is shown in Figure \ref{figure:architecture}.



Note that in the model presented above (which we call \emph{global attention}), the transformer sees no distinction between objects of different frames (other than through the position encoding).
Another intuitive choice, which we call \emph{hierarchical attention},
is to have one transformer acting on the objects of each frame independently,
and another transformer acting on the concatenated outputs of the first transformer (this temporal division of input data is commonly used, e.g. in \citep{sun-videobert}).
In pseudo-code, global attention can be expressed as

{
\ttfamily
\small
\ \ \ \   out = transformer(reshape(objects, [B, F * N, D])
}

and hiearchical attention as

{
\ttfamily
\small
\ \ \ \  out = transformer1(reshape(objects, [B * F, N, D]))

\ \ \ \  out = transformer2(reshape(out, [B, F, N * D]))
}.

We study the importance of global attention (objects as the atomic entities) vs hierarchical attention (objects, and subsequently frames as the atomic entities).
The comparison is shown in Table~\ref{table:clevrer-baseline-comparisons}.

\subsection{Self-supervised learning}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\textwidth,keepaspectratio]{self_supervised.pdf}
    \caption{\label{figure:self_supervised_schema} Different masking schemes for self-supervised learning applied to \Model{}.
    }
\end{figure*}


We explored whether self-supervised learning could improve the performance of \Model{} beyond the benefits conveyed by object-level representation,
i.e. in ways that support the model's interpretation of scene dynamics rather than just via improved perception of static observations. Our approach is inspired by the loss used in BERT \citep{devlin2018bert}, where a transformer model is trained to predict certain words that are masked from the input. In our case, we mask \emph{object embeddings}, and train the model to infer the content of the masked object representations using its knowledge of unmasked objects.

Concretely, during training, we multiply each MONet latent  by
a masking indicator, .
Let  be the transformed value of  after passing through the transformer.
We expect the transformer to understand the underlying dynamics of the video,
so that the masked out slot  could be predicted from .
To guide the transformer in learning effective representations
capable of this type of dynamics prediction, we add an auxiliary loss:

where  is a learned linear mapping to ,  a loss function, and  are one-hot indicator variables
identifying the prediction targets (not necessarily just the masked out entries, since the prediction targets could be a subset
of the masked out entries).
We propagate gradients only to 
the parameters of  and the transformer
and not to the learned word and  embeddings.
This auxiliary loss is added to the main classification loss with weighting ,
and both losses are minimized simultaneously by the optimizer.
We do not pretrain the model with only the auxiliary loss.

We tested two different loss functions for , an L2 loss
and a contrastive loss (formulas given in Appendix~\ref{appendix:self-supervised-formulas}),
and six different masking schemes (settings of  and ), as illustrated in Figure \ref{figure:self_supervised_schema}.
This exploration was motivated by the observation that video inputs at adjacent timesteps are highly correlated in a way that adjacent words are not.
We thus hypothesized that BERT-style prediction of adjacent words might not be optimal.
A different masking strategy, in which prediction targets are separated from the context by more than a single timestep,
may stimulate capacity in the network to acquire knowledge that permits context-based unrolls and better long-horizon predictions. 

The simplest approach would be to set  uniformly at random across  and , fixing the expected proportion of the  set to 1
(schema \emph{b} in Figure \ref{figure:self_supervised_schema}).
The targets would simply be the unmasked slots, .
One potential problem with this approach is 
that multiple objects could be masked out in a single frame.
MONet can unpredictably switch object-to-slot assignments multiple times in a single video.
If multiple slots are masked out,
the transformer cannot determine with certainty which missing object to assign to each slot.
Thus, the auxiliary loss could penalize the model even if it predicted all the objects correctly.
To avoid this problem, we also try constraining the mask such that exactly one slot
is masked out per frame (schema \emph{a}).

To pose harder prediction challenges,
we can add a buffer between the context (where ) and the infilling targets (where ).
For  in this buffer zone, both  and  (schemas \emph{c--f}).
We choose a single cutoff   randomly,
and we set  for  and  for .
In the presence of this buffer, we compared prediction (where the context is strictly before the targets; schema \emph{c}, \emph{d}) versus infilling (where the context surrounds the targets; schema \emph{e}, \emph{f}).
We also compared setting the targets as individual objects (schema \emph{c}, \emph{e}) versus targets as all objects in the scene (schema \emph{d}, \emph{f}).
We visually inspect the efficacy of this self-supervised loss in encouraging better representations (beyond improvements of scores on tasks)
in Appendix~\ref{section:qualitative-analysis}.



\section{Experiments}
We tested \Model{} on three datasets, CLEVRER \citep{clevrer}, CATER \citep{cater}, and ACRE \citep{acre}.
For each dataset, we pretrained a MONet model on individual frames.
 More training details and a table of hyperparameters
are given in Appendix~\ref{appendix:hyperparameters}; these hyperparameters were obtained through a hyperparameter sweep.
All error bars are standard deviations computed over at least 5 random seeds.

\subsection{CLEVRER}
\label{section:clevrer}

CLEVRER features videos of CLEVR objects \citep{Johnson2016clevr}
that move and collide with each other.
For each video, several questions are posed to test the model's understanding of the scene.
Unlike most other visual question answering datasets, which test for only descriptive understanding (``what happened?''),
CLEVRER poses other more complex questions, including 
explanatory questions (``why did something happen?''), predictive questions (``what will happen next?''),
and counterfactual questions (``what would happen in a unseen circumstance?'') \citep{clevrer}.


We compare \Model{} to 
state-of-the-art models reported in the literature:
MAC (V+) and \mbox{NS-DR} \citep{clevrer},
as well as the DCL model \citep{chen2021grounding} (simultaneous to our work).
MAC (V+) (based on the MAC network \citep{hudson2018compositional}) is an end-to-end network
augmented with object information and trained using ground truth labels
for object segmentation masks and features (e.g. color, shape).
NS-DR and DCL are hybrid models that apply a symbolic logic engine to outputs of various neural networks. The neural networks are used to detect objects, predict dynamics, and parse the question into a program,
and the symbolic executor runs the parsed program to obtain the final output.
NS-DR is trained using ground truth labels
and ground truth parsed programs,
while DCL requires only the ground truth parsed programs.


\begin{table*}[t]
    \centering
    \begin{tabular}{c|c|c|c|c}
         Model & Descriptive & Explanatory & Predictive & Counterfactual \\
         \hline
         MAC (V+) & 86.4 & 22.3 & 42.9 & 25.1\\
         NS-DR & 88.1 & 79.6 & 68.7 & 42.2  \\
         DCL & 90.7 & 82.8 & 82.0 & 46.5 \\
         \hline
         \Model{} & \textbf{94.0}  0.4 & \textbf{96.0}  0.6 & \textbf{87.5}  3.0 & \textbf{75.6}  3.8 \\
         \hline
         \Model{}  self-attention + MLP &  45.4 & 16.0 & 27.7 & 9.9 \\
         \Model{}  object-repr. + ResNet & 74.9	     & 66.1         & 58.3         & 32.4 \\
         \Model{}  global + hierarchical attn. & 80.6 & 87.4  & 73.5         & 55.1 \\
         \Model{}  self-supervised loss & 91.0 & 92.8 & 82.8 & 68.7
    \end{tabular}
    \caption{Performance (per question accuracy) on CLEVRER of \Model{} compared to results from literature
    and to ablations: 1) MLP instead of self-attention; 2) ResNet superpixels instead of MONet objects; 3) hierarchical frame-level and intra-frame attention instead of global cross-frame object attention; 4) no auxiliary loss.
    }
    \label{table:clevrer-baseline-comparisons}
\end{table*}


Table \ref{table:clevrer-baseline-comparisons} shows the result
of \Model{} compared to these models.
Across all categories, \Model{} significantly outperforms the previous best models.
Moreover, compared to the other models, \Model{} does not use any labeled
data other than the correct answer for the questions, nor does it require pretraining on any other dataset.
\Model{} also was not specifically designed for this task, and it straightforwardly generalizes to other tasks as well, such as CATER \citep{cater} and ACRE \citep{acre}.
We provide a few sample model classifications
on a randomly selected set of videos and questions
in Appendix \ref{appendix:clevrer_examples}
and detailed analysis of counterfactual questions in Appendix~\ref{appendix:clevrer_flaw}.
These examples suggest qualitatively that, for most instances where the model was incorrect, humans would plausibly furnish the same answer.


\paragraph{Attention analysis} 
(More analyses are given in Appendix~\ref{section:qualitative-analysis})
We analyzed the cross-modal attention between question-words and the MONet objects.
For each word, we determined the object that attended to that word with highest weight (for one head in the last layer).
In the visualization below,
the bounding boxes show the objects found by MONet,
and each word is colored according to the object that attended to it with highest weight
(black represents a MONet slot without any objects).
We observe that generally, objects attend heavily to the words that describe them.

\noindent\begin{minipage}{0.4\textwidth}
 \includegraphics[width=0.8\textwidth]{qualitative_analysis/frame10.png}
 \end{minipage}\begin{minipage}{0.5\textwidth}
\textbf{Q:} {\color{darkgreen} If}
{\color{cyan} the}
{\color{cyan} cylinder}
{\color{darkgreen} is}
{\color{cyan} removed,}
{\color{darkgreen} which}
{\color{cyan} event}
{\color{cyan} will}
{\color{darkgreen} not}
{\color{cyan} happen?}

\begin{enumerate}[leftmargin=*]
\item {\color{darkgreen} The}
{\color{darkbrown} brown}
{\color{darkbrown} object}
{\color{darkgreen} collides}
{\color{darkgreen} with}
{\color{darkgreen} the}
{\color{darkgreen} green}
{\color{darkgreen} object.}

\item {\color{darkgreen} The}
yellow
{\color{darkyellow} object}
{\color{darkbrown} and}
{\color{darkbrown} the}
{\color{darkbrown} metal}
{\color{darkbrown} cube}
{\color{darkbrown} collide.}

\item {\color{darkgreen} The}
{\color{darkyellow} yellow}
{\color{darkyellow} cube}
{\color{darkgreen} collides}
{\color{darkgreen} with}
{\color{darkgreen} the}
{\color{darkgreen} green}
{\color{darkgreen} object.}
\end{enumerate}
\end{minipage}

We also looked at the objects that were most heavily attended upon in determining the final answer.
The image below illustrates the attention weights for the  token
attending on each object (for one head in the last layer),
when the model is tasked with assessing the first choice of the  question above.
The bounding boxes show the two most heavily attended upon objects for one transformer head.
We observe that this head focuses on the green and brown objects (asked about in choice 1), but switches its focus to the cyan cylinder 
when it looks like the cylinder
might collide with the cubes and change the outcome.

\noindent\includegraphics[width=1\textwidth]{qualitative_analysis/mvp_objects.png}




\paragraph{Model ablation} Table \ref{table:clevrer-baseline-comparisons} shows the contributions
of various components of \Model{}.
First, self-attention is necessary for solving this problem.
For comparison, we replace \Model{}'s transformer
with four fully connected layers with 2048 units per layer\footnote{We also tried a bidirectional LSTM, which achieved even lower performance. This may be because the structure of our inputs requires the learning of long-range dependencies.}.
We find that an MLP is unable to answer non-descriptive questions effectively,
despite using more parameters (20M vs 15M parameters).

Second, we verify that an object-based discretization scheme is essential to the performance of \Model{}.
We compare with a version of the architecture where the MONet object representations 
are replaced with ResNet hyperpixels as in  \citet{zambaldi2018deep}.
Concretely, we flatten the output of the final convolutional layer of the ResNet to obtain
a sequence of
feature vectors that is fed into the transformer
as the discrete entities.
To match MONet's pretraining regimen,
we pretrain the ResNet on CLEVR \citep{Johnson2016clevr} by training an \Model{} model (using a ResNet instead of MONet) on the CLEVR task and initializing the ResNet used in the CLEVRER task with these pre-trained weights.
We find that
an object level representation, such as one output by MONet,
greatly outperforms the locality-aware but object-agnostic ResNet representation.



We also observe the importance of global attention
between all objects across all frames,
compared to a hierarchical attention model where objects within
a frame could attend to each other but frames could only attend
to each other as an atomic entity.
We hypothesize that global attention may be important because with hierarchical attention, objects in different frames can only attend to each other at the ``frame'' granularity.
A cube attending to a cube in a different frame would then gather information about the other non-cube objects, muddling the resulting representation.


Finally, we see that an auxiliary self-supervised  loss improves the performance of the model by between  and  percentage points, with the greatest improvement on the counterfactual questions.





\paragraph{Self-supervision strategies}
We compared the various masking schemes and loss functions for our auxiliary loss;
a detailed figure is provided in Appendix~\ref{appendix:methods} (Figure~\ref{fig:auxiliary-comparison}).
 We find that for all question types in CLEVRER, an L2 loss performs better than a contrastive loss, and among the masking schemes, masking one object per frame is the most effective.
 This particular result runs counter to
 our hypothesis that predictions or infilling in which the target is temporally removed from the context could encourage the model to learn more about scene dynamics and object interactions than (BERT-style) local predictions of adjacent targets.
 Of course, there may be other settings or loss functions that reveal the benefits of non-local prediction or constrastive losses; we leave this investigation to future work. 

\paragraph{Data efficiency}
We investigated how model performance varies as a function of the number of labelled (question-answer) pairs it learns from. To do so, we train models on  of the videos
and their associated labeled data.
We evaluate the effect of including the auxiliary self-supervised loss
(applied to the entire dataset, not just the labelled portion) in this low data regime.
This scenario, where unlabeled data is plentiful while labeled data is scarce,
occurs frequently in practice,  since collecting labeled data is much more expensive than collecting unlabeled data.

Figure \ref{fig:lowdata} shows that our best model reaches the approximate level of the previous state-of-the-art approaches using only 50\%-60\% of the data.
The self-supervised auxiliary loss makes a particular improvement to performance in low-data regimes.
For instance, when trained on only 50\% of the available labelled data, self-supervised learning enables the model to reach a performance of 37\% on counterfactual questions (compared to 25\% by MAC (V+) and 42\% by NS-DR on the full dataset),
while without self-supervision,
the model only reaches a performance of 13\% (compared to the 10\% achieved by answering randomly \citep{clevrer}).



\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{clevrer_scaling_law.png}
    \includegraphics[width=\textwidth]{cater_scaling_law.png}
    \caption{Accuracy with/without auxiliary loss for different proportions of CLEVRER (row 1) and CATER (row 2) training data.
    We also show comparisons with previous and concurrent work.
    For CLEVRER, the lighter yellow bar represents the best neurosymbolic model DCL,
    and the darker yellow bar represents the previous best distributed model, MAC (V+).
    For CATER, the lighter yellow bar represents Hopper
    and the darker yellow bar represents R3D+NL,
    the best published results for the moving camera dataset.
    }
    \label{fig:lowdata}
\end{figure*}

\subsection{CATER}
In a second experiment, we tested \Model{} on CATER, a widely-used object-tracking dataset \citep{cater,shamsian2020learning,zhou2021hopper,goyal2021}.
In CATER, objects from the CLEVR dataset~\citep{Johnson2016clevr} move and potentially occlude other objects, and the goal is to predict the location of a target object (called the \emph{snitch})
in the final frame.
Because the snitch could be occluded by multiple objects that could move in the meantime,
a successful model must be sensitive to notions of object permanence.
CATER also includes a moving
camera variant,
which introduces additional complexities
for the model.

Concretely, CATER is setup as a classification challenge. Objects are located in an  coordinate system, where x and y range from -3 to 3. The  plane is divided into a 6 by 6 grid,
and the task is to predict the grid index of the snitch in the final frame. For \Model{}, we use a classification loss (cross entropy over the 36 possible grid indices) and an L1 loss (L1 distance between predicted grid cell and the true grid cell). 

Table \ref{table:cater-baseline-comparisons}
shows \Model{} compared to state-of-the-art models in  the literature
on both static and moving camera videos.
R3D and R3D  NL are the strongest two models evaluated by \citet{cater}.
OPNet, or the Object Permanence Network \citep{shamsian2020learning}, is an architecture with inductive biases designed for object tracking tasks;
it was trained with extra supervised labels, namely the bounding boxes for all objects (including occluded ones).
Hopper is a multi-hop transformer model developed simultaneously with this work \citep{zhou2021hopper}.
One key component of Hopper is Hungarian matching between objects of different frames,
a strong inductive bias for object tracking.

We train \Model{} simultaneously on both static
and moving camera videos.
\Model{} outperforms the R3D models for both static and moving cameras.
We also ran \Model{} with an additional auxiliary loss consisting
of the L1 distance between the predicted cell and the actual cell.
With this additional loss, we get comparable results in the \emph{moving} camera case
as the R3D models for the \emph{static} camera case.
Moreover, we achieve comparable accuracy as OPNet for accuracy and L1 distance, despite
requiring less supervision to train.
Appendix \ref{appendix:cater_examples} gives a few sample outputs from \Model{};
in particular we note that it is able to find the target object in several cases where the object was occluded,
demonstrating that \Model{} is able to do some level of object tracking.
Finally,we find that an auxiliary self-supervised loss helps the model perform well in the low data regime for
CATER as well, as shown in  Figure \ref{fig:lowdata}.


\begin{table*}[t]
    \centering
    \begin{adjustbox}{center}
    \begin{tabular}{c||c|c|c||c|c|c}
         Model & Top 1 (S) & Top 5 (S) & L1 (S)
         &  Top 1 (M) & Top 5 (M) & L1 (M)
         \\
         \hline
         R3D LSTM & 60.2 & 81.8 & 1.2 &  28.6 & 63.3 & 1.7 \\
         R3D + NL LSTM & 46.2 & 69.9 & 1.5 & 38.6 & 70.2 & 1.5 \\
         OPNet & \textbf{74.8} & - & 0.54 & - & - & - \\
         Hopper & 73.2 & 93.8 & 0.85 & - & - & -\\ 
         \hline
         \Model{} (no auxiliary)     & 60.5 & 84.5 & 0.90 & 46.8 & 75.1 & 1.3 \\
         \Model{}                 & 70.6 & 93.0 & 0.53 &  56.6 & 87.0 & 0.82 \\ 
         \Model{} (with L1 loss)  & 74.0  0.3 & \textbf{94.0}  0.4 & \textbf{0.44}  0.01 & \textbf{59.7}  0.5 & \textbf{90.1}  0.6 & \textbf{0.69}  0.01\\ 
    \end{tabular}
    \end{adjustbox}
    \caption{Performance on CATER of \Model{} compared to the best results from literature.
    We report top 1 accuracy, top 5 accuracy,
    and L1 distance between the predicted grid cell and true grid cell.
    The labels (S) and (M) refer to static and moving cameras.
    }
    \label{table:cater-baseline-comparisons}
\end{table*}

\subsection{ACRE}

Finally, we measured \Model{}'s performance on ACRE, a causal induction dataset inspired by the Blicket task from developmental psychology
\citep{acre, blicket}. 
ACRE is divided into a set of problems. In each problem, certain  objects are chosen to be ``Blickets'',
and this assignment changes across problems.
Each problem presents a context of six images to the model,
where different objects are placed on a Blicket machine that lights up if one of those objects is a Blicket.
The model is asked whether an unseen combination of objects will light up the Blicket machine.
Besides ``yes'' and ``no'', a third possible answer is ``undetermined'',
which is the case if it is impossible to determine for certain if the objects will light up the machine.
Correct inference goes beyond mere correlation:
even if 
every context scene involving object A has a lit-up machine,
A's Blicketness is still uncertain if each of those scenes can potentially be explained by another object (deduction of A's Blicketness is \emph{backward-blocked}).

Inference problems in ACRE are categorized by reasoning type: reasoning from \emph{direct} evidence (one of the context frames show the query objects on a machine), reasoning from \emph{indirect evidence} (Blicketness must be deduced by combining evidence from several frames), \emph{screened-off} reasoning (presence of non-Blickets do not matter if a single Blicket is present), and \emph{backward-blocked} reasoning (Blicketness cannot be deduced due to confounding variables). Please see \citet{acre} for a more detailed discussion of these reasoning types.


Table \ref{table:acre-baseline-comparisons} show \Model{} performance compared to a CNN-BERT baseline and to NS-OPT, a neuro-symbolic model
introduced in \citet{acre}.
\Model{} outperforms all extant models for almost all reasoning types and train-test splits.
We did not need to do any tuning to apply our model to ACRE---settings from CATER yielded the reported results on the first attempt. 
Contrary to widely-held opinions that neural networks cannot generalize,
\Model{} generalizes in scenarios where the training and test sets contain different visual features (compositional split)
or different numbers of activated  machines in the context (systematic split).
Moreover, \Model{} achieved by far the best performance on the backward-blocking task,
which requires the model to ``go beyond the simple
covariation strategy to discover the hidden causal relations'' \citep{acre},
dispelling the notion that neural networks can only find correlation.
Comparison with NS-OPT (which uses object representations) and CNN-BERT (which uses attention)
shows that neither object representations nor attention alone is sufficient for the task;
combining these two ideas, as done in \Model{} for instance, is essential for this
complex reasoning task as well.

\begin{table*}[t]
    \centering
    \begin{adjustbox}{center}
    \begin{tabular}{c||c|c|c|c|c||c|c|c|c|c }
         Model & All (C) & D.R. & I.D. & S.O.  & B.B.  & All (S) & D.R.  & I.D. & S.O. & B.B 
         \\
         \hline
         CNN-BERT & 43.79 & 54.07 & 46.88 & 40.57 & 28.79 &      39.93 & 55.97 & 68.25 & 0.00 &  45.59  \\
         NS-OPT & 69.04 & 92.5 & 76.05 & 88.33 & 13.48 &         67.44 & 94.73 & \textbf{88.38} & 82.76 & 16.06 \\
         \hline
         \Model{}   & \textbf{91.76} & \textbf{97.14} & \textbf{90.8} & \textbf{96.8} & \textbf{78.81} &         \textbf{93.90} & \textbf{97.18} & 71.24 & \textbf{98.97} & \textbf{94.48}
    \end{tabular}
    \end{adjustbox}
    \caption{Performance on ACRE of \Model{} compared to the best results from \citet{acre},
    split across inference type (D.R=Direct, I.D=Indirect, S.O=Screen-Off, B.B=Backwards Blocking)
    and generalization type (C=Compositional, S=Systematic).
    }
    \label{table:acre-baseline-comparisons}
\end{table*}


\section{Related work}


\paragraph{Self-attention for reasoning}
Various studies have shown that transformers \citep{vaswani2017attention}
 can manipulate symbolic data in a manner traditionally associated with symbolic computation.
For example, in \citet{Lample2020Deep}, a transformer model learned to do symbolic integration and solve
ordinary differential equations symbolically, tasks traditionally reserved for symbolic computer algebra systems. Similarly, in \citet{hahn2020transformers}, a transformer model learned to solve formulas in propositional logic
and demonstrated some degree of generalization to out of distribution formulas.
Finally, \citet{brown2020language} showed that a transformer trained for language modeling can also do simple analogical reasoning tasks without explicit training.
Although these models do not necessarily beat carefully tuned symbolic algorithms in all cases (especially on out of distribution data),
they are an important motivation for our proposed recipe for attaining strong reasoning capabilities from self-attention-based models on visually grounded tasks.

 \paragraph{Object representations}
A wide body of research points to the importance of object segmentation and representation learning (see e.g.~\citet{garnelo2019reconciling} for a discussion).
Various methods have been proposed for object detection and feature extraction
\citep{ren2015faster, he2017maskrcnn, monet, greff2019-iodine, Lin2020SPACE, du2020unsupervised, slot_attention}.
Past research have also investigated using object based representations in downstream tasks
\citep{raposo2017discovering, desta2018object}.



\paragraph{Self-supervised learning}
 Another line of research concerns learning good representations through self-supervised learning,
 with an unsupervised auxiliary loss to encourage the discovery of better representations.
 These better representations could lead to improved performance on supervised tasks,
 especially when labeled data is scarce.
 In \citet{devlin2018bert}, for instance, an auxiliary infill loss allows the BERT model to benefit from  pretraining on
 a large corpus of unlabeled data.
 Our approach to object-centric self-supervised learning is heavily inspired by the BERT infilling loss.
 Other studies have shown similar benefits to auxiliary learning in vision as well \citep{gregor2019Shaping, Han19dpc, chen2020simple}. These works apply various forms of contrastive losses to predict scene dynamics,
 and
 the better representations that result carry
 downstream benefits to supervised and reinforcement learning tasks.
 
 \paragraph{Vision and language in self-attention models}
 


Recently, many works have emerged on applying transformer models to visual and multimodal data,
for static images \citep{li2019visualbert, lu2019vilbert, tan2019lxmert, Su2020VL-BERT} and videos \citep{zambaldi2018deep, sun-videobert, sun2019contrastive}.
These approaches combine the output of convolutional networks with language in various ways using self-attention.
While these previous works focused on popular visual question answering tasks,
which typically consist of descriptive questions only \citep{clevrer},
we focus on understanding deeper causal dynamics of videos.
Together with these works, we provide more evidence that self-attention
between visual and language elements enables good performance on a diverse set of tasks.

In addition, while the use of object representations for discretization
in tasks involving static images is becoming more popular,
the right way to discretize videos is less clear.
We provide strong evidence in the form of ablation studies
for architectural decisions that we claim are essential for higher reasoning for this type of data:
visual elements should correspond to physical objects in the videos and
inter-frame attention between sub-frame entities (as opposed to inter-frame attention of entire frames) is crucial.
We also demonstrate the success of using
unsupervised object segmentation methods as opposed to the supervised methods used in past work.



\section{Conclusion}

We have presented \Model{}, a model that obtains state-of-the-art performance on three different task domains involving spatiotemporal reasoning about objects.
In each of these tasks, previous state-of-the-art results were established by models with modular, task-specific components. \Model{}, by contrast, is a unified solution to all three domains. Its flexibility comes from a reliance on only soft biases and learning objectives: self-attention over learned object embeddings and self-supervised learning of dynamics. We believe the simplicity of this approach is its strength, and hope that this fact, together with the provided code, makes it easy for others to adopt and apply to arbitrary spatio-temporal reasoning problems. 

On many of these spatio-temporal reasoning problems, previous state-of-the-art was achieved by neuro-symbolic models \citep{clevrer, acre, yi2018neural,garnelo2019reconciling,chen2021grounding}. Compared to neuro-symbolic models, \Model{} can more easily be adapted to other tasks. Indeed, the symbolic components of neuro-symbolic models are often task-specific and not straightforwardly applicable to other tasks. Neuro-symbolic models do have a few advantages, however. First, they are often easier to interpret. Despite the insights that can be gleaned from \Model{}'s attention weights, these soft computations are harder to interpret than the explicit symbolic computation found in neuro-symbolic models. Moreover, neuro-symbolic models can be structured in a more modular fashion, which can enable effective generalization to sub-tasks of the task on which the model was trained \citep{chen2021grounding}.


\Model{} also has some important limitations. First, it has only been applied to synthetic datasets. This limitation is mainly due to the lack of real-world datasets that test for higher-order spatiotemporal reasoning, although we are excited that new datasets such as Traffic QA will be released soon \citep{xu2021sutd}. Second, while the domains where \Model{} is applied have been widely adopted and well-received by the research community, it remains possible that they do not evaluate the capacities that they aim to evaluate because of hidden biases or other factors. Regardless, we hope that this work stimulates the design and development of more challenging tasks that more closely approximate the ultimate goal of human or super-human-level visual, spatiotemporal and causal reasoning. Finally, from an ethical point of view, our model may share the common drawback of deep-learning models in perpetuating biases found in the training data, especially when applied to real world data. Development of causal reasoning models could also invite problematic applications involving automated assignment of blame.


\clearpage

\bibliographystyle{plainnat}
\bibliography{paper}

\clearpage
\appendix

\section{Methods details}
\label{appendix:methods}
\subsection{MONet}
\label{appendix:monet}
To segment each  frame  into  object representations,
MONet uses a recurrent attention network
to obtain  attention masks  for 
that represent the probability of each pixel in  belonging to the -th object,
with . 
This attention network is coupled with a component VAE with latents  for 
that reconstructs , the -th object in the image.
The latent posterior distribution  is a diagonal Gaussian with mean ,
and we use  as the representation of the -th object.

When these representations are fed into the transformer,
we use a linear projection to map the raw object/word embeddings, which lie in ,
to a vector in ,
where  is the number of self-attention heads.
This step is necessary as generally the latent dimensionality of MONet, ,
is less than 
whereas a transformer expects the embedding size to be divisible by .

\subsection{Self-supervised training}
\label{appendix:self-supervised-formulas}
Recall in the main text that we wrote the auxiliary self-supervised loss as 

We tested an L2 loss and a contrastive loss (inspired by the loss used in \citep{Han19dpc}),
and the formulas for the two losses are respectively:

A comparison of these losses and the masking schemes is given in Figure~\ref{fig:auxiliary-comparison}.

We also tested a few variations of the contrastive loss inspired by literature and tested all combinations of variations.
The first variation is where the negative examples all come from the same frame:

The second variation is adding a temperature  to the softmax \citep{chen2020simple}:

The final variation we tested is using cosine similarity instead of dot product:

where .
We found that these variations did not significantly change the performance of the model (and the optimal temperature setting was close to ),
and leave to future work more careful analysis of these contrastive losses and the representations they encourage.


\begin{figure}[]
    \centering
    \includegraphics[width=0.9\textwidth]{mask_comparisons.png}
    \caption{Comparison of different mask types and loss functions for auxiliary loss computation. Models were trained on 50\% of the CLEVRER dataset to magnify the effects of the self-supervised loss.
    }
    \label{fig:auxiliary-comparison}
\end{figure}

\subsection{Training details}
\label{appendix:hyperparameters}
We generally follow similar training procedures as for the models described
in
\citep{clevrer} and \citep{cater}.
We train on 16 TPU v2 chips.

For CLEVRER, we resize videos to 64 by 64 resolution and sample 25 random frames, as in \citep{clevrer}.
We use two different MLP heads on top of the transformed value of the  token
to extract the final answer, one head for descriptive questions and one head for multiple choice questions.
For descriptive questions, the MLP head outputs a categorical distribution over possible output tokens,
whereas for multiple choice questions, the MLP outputs the probability that the choice is true.
For each training step, we sample a supervised batch of 256 videos with their accompanying questions and answers along with an unsupervised batch of 256 videos, which do not include the answers.
These batches are sampled independently from the dataset.
The supervised batch is used to calculate the classification loss,
and the unsupervised sub-batch is used to calculate the unsupervised auxiliary loss.
This division was made so that we can use a subset of available data for the supervised batch while using all data for the unsupervised batch.
The supervised batch is further subdivided into two sub-batches of size 128, for descriptive and multiple choice questions (this division was made since the output format is different for the two types of questions).
\Model{} converges within 200,000 training steps.

For CATER, we also resize videos to 64 by 64 resolution and sample 80 random frames.
We use an MLP head on top of the transformed  token.
This head outputs a categorical distribution over the grid index of the final snitch location.
We train on static and moving camera data simultaneously, with the batch of 256 videos divided equally between the two.
\Model{} converges within 50,000 training steps.

On ACRE, we resize each image to 64 by 64 resolution and concatenate the context images along with one query image
to form a ``video''.
The MLP head on top of the transformed  token outputs a categorical distribution over the three possible answers:
``yes'', ``no'', and ``undetermined''.
\Model{} converges within 60,000 steps.

For the CLEVRER and CATER datasets, we pretrain a MONet model on frames extracted from the respective dataset.
The training of the MONet models follow the procedures described in \cite{monet}.
For ACRE, we reuse the MONet model we trained for CATER.

Motivated by findings from language modeling,
we trained the main transformer model using the LAMB optimizer \citep{You-LAMB} and found that it offered 
a significant performance boost over
the ADAM optimizer \citep{kingma2014adam} for the CLEVRER dataset (data not shown).
We use learning rate warmup over 4000 steps and a linear learning rate decay.
We also used a weight decay of 0.01.
All error bars are computed over at least 5 seeds.
We swept over hyperparameters, and the below table lists the values used in our model.
The hyperparameters we used for ACRE
were the same as those we used for CATER,
except that the prediction-head hidden layer size is reduced to 36 (from 144),
because ACRE has only 3 possible outputs compared to the 36 for CATER.
We did not do any hyperparameter tuning for ACRE.

\begin{table}[h]
    \centering
     \subfloat[Hyperparameters for CLEVRER.]{
    \begin{tabular}{c|c}
         Parameter & Value \\
         \hline
         Batch-size & 512 \\
         Transformer heads & 10 \\
         Transformer layers & 28 \\
         Embedding size  & 16 \\
         Number of objects  & 8 \\
         Prediction head hidden layer size & 128 \\
         Maximum learning rate & 0.002 \\
         Learning rate warmup steps & 4000 \\
         Final learning rate &  \\
         Learning rate decay steps &  \\
         Weight decay rate & 0.01 \\
         Infill cost  & 0.01
         \end{tabular}
    }
     \subfloat[Hyperparameters for CATER.]{
     \begin{tabular}{c|c}
         Parameter & Value \\
         \hline
         Batch-size & 256 \\
         Transformer heads & 8 \\
         Transformer layers & 16 \\
         Embedding size  & 36 \\
         Number of objects  & 8  \\
         Prediction head hidden layer size & 144 \\
         Maximum learning rate & 0.002 \\
         Learning rate warmup steps & 4000 \\
         Final learning rate &  \\
         Learning rate decay steps &  \\
         Weight decay rate & 0.01 \\
         Infill cost  & 2.0 \\
    \end{tabular}
    }
\end{table}

\section{Using other object-segmentation algorithms}
\label{appendix:other-models}

In the main text, we  use MONet to obtain object representations,
because MONet's unsupervised nature allows us to establish our state-of-the-art results using only data from the datasets.
Our method of attention over learned object embeddings, however,
does not rely on MONet representations in particular.
In this section, we show how to apply our method to object detection models that output an object segmentation mask
but not necessarily a feature vector for each object.
This includes, for example,
often-used models such as Mask R-CNN and DETR \citep{he2017maskrcnn, detr}.

Let  be the segmentation
masks, either produced by an object segmentation algorithm or
ground-truth masks.
For any function 
mapping from the image space to a latent space of dimension ,
we can construct object feature vectors
.
That is, we apply  to the image with the segmentation masks applied, once for each object.
In our experiments, we choose to represent  with a
ResNet consisting of 3 blocks, with 2 convolutional layers per block.
The weights of the ResNet are learned with the rest of the network,
but the weights of the object segmentation model are fixed.

We provide a proof-of-concept using ground-truth segmentation masks
to show the performance of our model
in the ideal setting, independent of the quality of the segmentation model.
We apply our model to the original CLEVR dataset \citep{Johnson2016clevr},
for which we have ground-truth segmentation masks.
CLEVR is a widely used benchmark testing for understanding
of spatial relationships between objects in a still image.
We obtain an accuracy of \textbf{99.5\%},
which is inline with state-of-the-art results (99.8\%, \citep{nsvqa}).


\section{Analysis of CLEVRER dataset}
\label{appendix:clevrer_flaw}


During analysis of our results, we noticed that some counterfactual questions in the CLEVRER dataset can be solved without using counterfactual reasoning.
In particular, about 47\% of the counterfactual questions
ask about the effect of removing an object that did not collide
with any other object, hence having no effect on object dynamics;
an example is given in Figure~\ref{fig:example-counterfactual}.
Moreover, even for the questions
where the removed object is causally connected
to the other objects,
about 45\% can be answered perfectly by an algorithm
answering the question as if it were a descriptive question.
To quantify this, we wrote a symbolic executor that uses the provided ground-truth video annotations and parsed questions to determine causal connectivity and whether each choice happened in the non-counterfactual scenario.

Although determining whether or not a given counterfactual question
can be answered this way still requires counterfactual reasoning,
we want to eliminate the possibility that
our model achieved its 75\% accuracy on counterfactual questions
without learning counterfactual reasoning;
instead
it might have reached that score simply by answering all counterfactual questions as descriptive questions.
To verify this is not the case, we evaluated \Model{} on
only the harder category of counterfactual questions
where the removed object does collide with other objects
and which cannot be answered by a descriptive algorithm.
We find that \Model{}
achieves a performance of 59.8\%
on this harder category. This is significantly above chance,
suggesting that \Model{} is indeed able to do some amount
of true counterfactual reasoning.

\begin{figure}
    \centering
    \includegraphics[width=1.1\textwidth]{video1176.png}
    \caption{The video for an example counterfactual question that can be answered as if it were a descriptive question. The question is: if the brown rubber sphere is removed, what will not happen?
  }
    \label{fig:example-counterfactual}
\end{figure}


\section{Qualitative analysis}
\label{section:qualitative-analysis}
We provide more qualitative analysis of attention weights in order to shed light on how \Model{} arrives at its predictions.
These examples illustrate broad patterns evident from informal observation of the model's attention weights.
We focus on the following video from CLEVRER:

\includegraphics[width=1.1\textwidth]{clevrer_videos/video2164.png}

In this video, a yellow rubber cube collides with a cyan rubber cylinder. The yellow cube then collides with a brown metallic cube, while the cyan cylinder and a green rubber cube approach each other but do not collide. Finally, the green cube approaches but does not collide with the brown cube.

\paragraph{Most important objects}
In the main text, we looked at the most heavily attended-upon objects
in determining the answer to a counterfactual question about this video. 
By looking at the attention patterns when answering a different question about the same video
(a predictive question, whether or not the cylinder and the green cube will collide),
we see that the relative importance of the various objects depends on the question the model is answering.
Here, we observe one head of the transformer focusing on collisions:
first the collision of the cylinder and the yellow cube,
then on the cylinder and the green cube when they move towards each other.

\noindent\includegraphics[width=1.1\textwidth]{qualitative_analysis/mvp_predictive.png}

\paragraph{Object alignment}
Recall that MONet does not assign objects to slots in a well-determined manner---tiny changes in an image can cause MONet to unpredictably assign objects to slots in a different permutation.
This is a general flaw for object segmentation algorithms without built-in alignment.
Nevertheless, \Model{} can still effectively utilize
these representations,
because
\Model{} is able to maintain object identity even when the objects appear in different order in different frames.
The image below,
where we again show the two most attended-upon objects for each frame,
illustrate instances where MONet changes the permutation of objects.
In this image,
we plot time on the x-axis and MONet slot index on the y-axis;
the slots containing the two most important objects are grayed out.
We observe that the transformer is able to align objects across time,
maintaining consistent attention to the green and brown objects.

\begin{center}
\includegraphics[width=0.9\textwidth]{qualitative_analysis/alignment.png}
\end{center}

\paragraph{Effectiveness of the auxiliary loss}
Finally, we visually inspect our hypothesis that our self-supervised loss
encourages the transformer in learning better representations.
For clarity of the subsequent illustration, we use the scene prediction masking scheme, as described in Figure \ref{figure:self_supervised_schema}.
In this scheme, the transformer has to predict the contents of the last few frames (the \emph{target frames}) given the beginning of the video.
To pose harder predictive challenges, we mask out the three frames preceding the target frames in addition to the target frames themselves.
The two images below compare the predicted frames (second image) to the true frames (first image).
In the second image, the black frames are the three masked out frames preceding the target frames.
The frames following the black frames are the target frames;
they contain the MONet-reconstructed images obtained from latents predicted by the transformer.
The frames preceding the black frames are MONet-reconstructed images obtained from the original latents (the latents input into the transformer).

We observe that with the self-supervised loss, we get coherent images from the transformer-predicted latents with all the right objects
(in the absence of the auxiliary loss, the transformed latents generate incoherent rainbow blobs).
We also observe the rudiments of prediction, as seen in the movement of the yellow object in the predicted image.
Nevertheless, it is also clear that the transformer's predictions are not perfect, and we leave improvements of this predictive infilling
to future work.

\noindent\includegraphics[width=\textwidth,trim=720 0 135 0,clip]{qualitative_analysis/input_sequence.png}

\noindent\includegraphics[width=\textwidth,trim=720 0 135 0,clip]{qualitative_analysis/infilled_sequence.png}

\section{Example model predictions}
In this section, we provide a few sample classifications produced by \Model{}.
All examples are produced at random from the validation set;
in particular we did not cherry-pick any examples to highlight the performance of \Model{}.

\subsection{CLEVRER}
\label{appendix:clevrer_examples}
We provide four videos and up to two questions per question type for the video
(many videos in the dataset come with only one explanatory or predictive question).
For each question type with more than one question,
we try to choose
one correct classification and one misclassification if available
to provide for greater diversity.
Besides this editorial choice, all classifications are sampled randomly.




  \includegraphics[width=1.1\textwidth]{clevrer_videos/video2752.png}
  \begin{tabular}{c|c|c|c}
  
  \begin{minipage}[t]{0.245\textwidth}
  \textbf{Q:} How many metal objects are moving? \\
  \textbf{Model:} 1 \\
  \textbf{Label:} 1\\
  \end{minipage}
  & 

  \begin{minipage}[t]{0.245\textwidth}
  \textbf{Q:} Which of the following is not responsible for the collision between the metal cube and the yellow cube?
  \begin{enumerate}[leftmargin=*]
  \item the presence of the gray cube
\item the gray object's entrance
\item the presence of the red rubber cube
\item the collision between the gray cube and the metal cube
  \end{enumerate}
  \textbf{Model:} 3 \\
  \textbf{Label:} 3\\
  \end{minipage}
  & 

  \begin{minipage}[t]{0.245\textwidth}
  \textbf{Q:} Which event will happen next?
  \begin{enumerate}[leftmargin=*]
  \item The gray object collides with the red object
\item The gray object and the cylinder collide
  \end{enumerate}
  \textbf{Model:} 1 \\
  \textbf{Label:} 1\\
  \end{minipage}
  & 

  \begin{minipage}[t]{0.245\textwidth}
  \textbf{Q:} Which event will happen if the red object is removed?
  \begin{enumerate}[leftmargin=*]
  \item The gray object and the brown object collide
\item The gray object collides with the cylinder
\item The gray cube collides with the yellow object
\item The brown cube and the yellow object collide
  \end{enumerate}
  \textbf{Model:} 1, 4 \\
  \textbf{Label:} 1, 4\\
  \end{minipage}
  \\ 

  \begin{minipage}[t]{0.245\textwidth}
  \textbf{Q:} What is the shape of the stationary metal object when the red cube enters the scene? \\
  \textbf{Model:} cylinder \\
  \textbf{Label:} cylinder\\
  \end{minipage}
  & 


  & 


  & 

  \begin{minipage}[t]{0.245\textwidth}
  \textbf{Q:} What will happen if the cylinder is removed?
  \begin{enumerate}[leftmargin=*]
  \item The brown cube collides with the red cube
\item The red object and the yellow object collide
\item The gray cube collides with the red cube
\item The gray object collides with the brown object
  \end{enumerate}
  \textbf{Model:} 3, 4 \\
  \textbf{Label:} 3, 4\\
  \end{minipage}
  
  \end{tabular}
  



  \includegraphics[width=1.1\textwidth]{clevrer_videos/video2227.png}
  \begin{tabular}{c|c|c|c}
  
  \begin{minipage}[t]{0.245\textwidth}
  \textbf{Q:} What color is the metal object that is stationary when the metal cube enters the scene? \\
  \textbf{Model:} blue \\
  \textbf{Label:} blue\\
  \end{minipage}
  & 

  \begin{minipage}[t]{0.245\textwidth}
  \textbf{Q:} Which of the following is not responsible for the collision between the cyan object and the sphere?
  \begin{enumerate}[leftmargin=*]
  \item the presence of the red rubber object
\item the red object's entering the scene
\item the collision between the sphere and the blue cube
  \end{enumerate}
  \textbf{Model:} 1, 2, 3 \\
  \textbf{Label:} 1, 2, 3\\
  \end{minipage}
  & 

  \begin{minipage}[t]{0.245\textwidth}
  \textbf{Q:} What will happen next?
  \begin{enumerate}[leftmargin=*]
  \item The metal cube and the red cube collide
\item The sphere collides with the metal cube
  \end{enumerate}
  \textbf{Model:} 1 \\
  \textbf{Label:} 1\\
  \end{minipage}
  & 

  \begin{minipage}[t]{0.245\textwidth}
  \textbf{Q:} Without the red cube, which event will happen?
  \begin{enumerate}[leftmargin=*]
  \item The sphere collides with the blue cube
\item The cyan object and the blue cube collide
  \end{enumerate}
  \textbf{Model:} 1 \\
  \textbf{Label:} 1\\
  \end{minipage}
  \\ 

  \begin{minipage}[t]{0.245\textwidth}
  \textbf{Q:} What material is the last object that enters the scene? \\
  \textbf{Model:} metal \\
  \textbf{Label:} rubber\\
  \end{minipage}
  & 


  & 

  & 

  \begin{minipage}[t]{0.245\textwidth}
  \textbf{Q:} What will not happen without the sphere?
  \begin{enumerate}[leftmargin=*]
  \item The cyan object collides with the red cube
\item The cyan object collides with the metal cube
\item The metal cube and the red cube collide
  \end{enumerate}
  \textbf{Model:} 1, 3 \\
  \textbf{Label:} 3\\
  \end{minipage}
  
  \end{tabular}
  



  \includegraphics[width=1.1\textwidth]{clevrer_videos/video853.png}
  \begin{tabular}{c|c|c|c}
  
  \begin{minipage}[t]{0.245\textwidth}
  \textbf{Q:} Are there any moving brown objects when the red object enters the scene? \\
  \textbf{Model:} no \\
  \textbf{Label:} no\\
  \end{minipage}
  & 

  \begin{minipage}[t]{0.245\textwidth}
  \textbf{Q:} Which of the following is not responsible for the collision between the red object and the gray sphere?
  \begin{enumerate}[leftmargin=*]
  \item the presence of the gray cube
\item the collision between the red object and the cyan object
\item the rubber cube's entering the scene
\item the presence of the cyan object
  \end{enumerate}
  \textbf{Model:} 1, 3 \\
  \textbf{Label:} 1, 3\\
  \end{minipage}
  & 

  \begin{minipage}[t]{0.245\textwidth}
  \textbf{Q:} What will happen next?
  \begin{enumerate}[leftmargin=*]
  \item The gray cube and the brown object collide
\item The red object collides with the rubber cube
  \end{enumerate}
  \textbf{Model:} 2 \\
  \textbf{Label:} 2\\
  \end{minipage}
  & 

  \begin{minipage}[t]{0.245\textwidth}
  \textbf{Q:} If the cylinder is removed, which of the following will not happen?
  \begin{enumerate}[leftmargin=*]
  \item The gray cube and the brown cube collide
\item The red object and the cyan object collide
\item The red sphere and the rubber cube collide
\item The cyan object and the brown cube collide
  \end{enumerate}
  \textbf{Model:} 1, 4 \\
  \textbf{Label:} 1, 4\\
  \end{minipage}
  \\ 

  \begin{minipage}[t]{0.245\textwidth}
  \textbf{Q:} How many rubber objects are moving? \\
  \textbf{Model:} 3 \\
  \textbf{Label:} 3\\
  \end{minipage}
  & 

  & 

  & 

  
  \end{tabular}
  



  \includegraphics[width=1.1\textwidth]{clevrer_videos/video4559.png}
  \begin{tabular}{c|c|c|c}
  
  \begin{minipage}[t]{0.245\textwidth}
  \textbf{Q:} How many objects are stationary when the sphere enters the scene? \\
  \textbf{Model:} 1 \\
  \textbf{Label:} 1\\
  \end{minipage}
  & 

  \begin{minipage}[t]{0.245\textwidth}
  \textbf{Q:} Which of the following is not responsible for the yellow object's colliding with the green object?
  \begin{enumerate}[leftmargin=*]
  \item the presence of the purple sphere
\item the blue object's entrance
\item the collision between the blue object and the rubber cube
\item the sphere's entering the scene
  \end{enumerate}
  \textbf{Model:} 2, 3 \\
  \textbf{Label:} 2, 3\\
  \end{minipage}
  & 

  \begin{minipage}[t]{0.245\textwidth}
  \textbf{Q:} What will happen next?
  \begin{enumerate}[leftmargin=*]
  \item The sphere collides with the rubber cube
\item The yellow cube and the green object collide
  \end{enumerate}
  \textbf{Model:} 1 \\
  \textbf{Label:} 1\\
  \end{minipage}
  & 

  \begin{minipage}[t]{0.245\textwidth}
  \textbf{Q:} Which event will not happen if the green cube is removed?
  \begin{enumerate}[leftmargin=*]
  \item The yellow object and the blue object collide
\item The sphere collides with the blue cube
\item The sphere and the yellow object collide
\item The sphere collides with the yellow cube
  \end{enumerate}
  \textbf{Model:} 2 \\
  \textbf{Label:} 2\\
  \end{minipage}
  \\ 

  \begin{minipage}[t]{0.245\textwidth}
  \textbf{Q:} What is the shape of the last object that enters the scene? \\
  \textbf{Model:} cube \\
  \textbf{Label:} cube\\
  \end{minipage}
  & 


  & 


  & 

  \begin{minipage}[t]{0.245\textwidth}
  \textbf{Q:} Which of the following will happen if the yellow object is removed?
  \begin{enumerate}[leftmargin=*]
  \item The blue cube and the green cube collide
\item The sphere collides with the blue cube
\item The sphere collides with the green cube
  \end{enumerate}
  \textbf{Model:} 1, 3 \\
  \textbf{Label:} 1\\
  \end{minipage}
  
  \end{tabular}
   

\subsection{CATER}
\label{appendix:cater_examples}
We include ten random videos from the validation subset of the static camera CATER dataset.
In the final frame of the video, the correct grid cell of the target snitch is drawn in blue, and the model's prediction is drawn in red.
We note that the model is able to find the snitch in scenarios where the snitch is hidden under a cone that later moves (along with the still hidden snitch);
in the sixth example, the model also handled a case where the snitch was hidden under two cones at some point in time.

\includegraphics[width=1.1\textwidth]{cater_videos/CATER_new_000139.png}
\includegraphics[width=1.1\textwidth]{cater_videos/CATER_new_000345.png}
\includegraphics[width=1.1\textwidth]{cater_videos/CATER_new_000919.png}
\includegraphics[width=1.1\textwidth]{cater_videos/CATER_new_001350.png}
\includegraphics[width=1.1\textwidth]{cater_videos/CATER_new_001417.png}
\includegraphics[width=1.1\textwidth]{cater_videos/CATER_new_002644.png}
\includegraphics[width=1.1\textwidth]{cater_videos/CATER_new_002909.png}
\includegraphics[width=1.1\textwidth]{cater_videos/CATER_new_003765.png}
\includegraphics[width=1.1\textwidth]{cater_videos/CATER_new_004542.png}
\includegraphics[width=1.1\textwidth]{cater_videos/CATER_new_005460.png}

\section{Dataset Licenses}
The CATER generation code is available under the Apache License, and the ACRE generation code is available under the GPL license.

\end{document}
