\newcommand{\mc}{\multicolumn{2}{|c|}}

\begin{figure}[t]
\hspace{-8pt}
\vspace{0pt}
\setlength{\tabcolsep}{5pt} \begin{tabular}{c c}
\begin{minipage}{3.6cm}
    \setlength{\tabcolsep}{-0.3em} \begin{tabular}{c c c}
     \includegraphics[width=1.4cm]{3Dexp3std01_test.pdf} &
\includegraphics[width=1.4cm]{3Dexp3std12_test.pdf} &
\includegraphics[width=1.4cm]{3Dexp3std20_test.pdf} \\
\includegraphics[width=1.4cm]{3Dexp3std31_test.pdf} &     
     \includegraphics[width=1.4cm]{3Dexp3std48_test.pdf} &
\includegraphics[width=1.4cm]{3Dexp3std75_test.pdf} \\\vspace{-7pt}
\end{tabular}\\
{\fontsize{7}{7} \selectfont
(\textbf{i}) 10 Random cluster centers on the unit sphere, perturbed with increasing noise STD .}
\end{minipage}
&
\begin{minipage}{8.2cm}
~\\\vspace{-3pt}~\\
 \includegraphics[width=8.3cm]{multi_plots.png} \\ \vspace{-15pt} ~\\
 \hspace{61pt}
{\fontsize{7}{7} \selectfont
(\textbf{ii}) Clustering accuracy across dimensions  (left) and noise levels  (right). For each configuration, k-means accuracy is reported when applied with original (\textit{solid}) and SOT (\textit{dashed}) features.}
\end{minipage}
\end{tabular}
\\ \vspace{-16pt} \\
   \vspace{0pt}\caption{
   {\fontsize{8.5}{8.5} \selectfont
\textbf{Clustering on the \textit{d}-dimensional sphere}. \textbf{Left} (\textbf{i}): the data generation process (illustrated for the 3D case). \textbf{Right} (\textbf{ii}): detailed k-means accuracy results. The SOT (dashed) features give superior results throughout a majority of the space of settings.
}}
    \label{fig.synthetic_exp}\vspace{-11pt}
\end{figure}


\subsection{Clustering on the Sphere}
We first demonstrate the effectiveness of SOT using a controlled synthetically generated clustering experiment, with  cluster centers that are distributed uniformly at random on a -dimensional unit-sphere, and 20  points per cluster (200 in total) that are perturbed around the cluster centers by Gaussian noise of increasing standard deviation, of up to 0.75, followed by a re-projection back to the sphere by dividing each vector by its  magnitude. We also apply dimensionality reduction with PCA to , for dimensions above 50.

We performed the experiment over a logarithmic 2D grid of combinations of data dimensionalities  in the range  and Gaussian in-cluster noise STD in the range . Refer to Fig.~\ref{fig.synthetic_exp} (i) for a visualization of the data generation process. 

Each point is represented by its -dimensional euclidean coordinates vector, where the baseline clustering is obtained by running k-means on these location features. In addition, we run k-means on the set of features that has undergone SOT. Hence, the benefits of the transform (embedding) are measured indirectly through the accuracy\footnote{Accuracy is measured by comparison with the optimal permutation of the predicted labels, found by the Hungarian Algorithm~\cite{kuhn1955hungarian}.} achieved by running k-means on the embedded vs. original vectors.
Evaluation results are reported in Fig.~\ref{fig.synthetic_exp} (ii) as averages over 10 runs, by plotting accuracy vs. dimensionality (for different noise STDs) and accuracy vs noise STDs (for different dimensionalities). The results show (i) general accuracy gains and robustness to wide ranges of data dimensionality (ii) the ability of SOT to find meaningful representations that enable clustering quality to degrade gracefully with the increase in cluster noise level. Note that the levels of noise are rather high, as they are relative to a unit radius sphere (a 3-dimensional example is shown at the top of the figure). We provide further details on this experiment in the supplementaries.

\newcommand{\bbb}[1]{({\footnotesize \color{blue} #1})}
\newcommand{\rrr}[1]{({\footnotesize \color{red} #1})}
\newcommand{\cmark}{\hspace{-10pt}\ding{51}\hspace{-6pt}}\newcommand{\xmark}{\hspace{-10pt}\ding{55}\hspace{-6pt}}\begin{table}[t]
    \centering \small
    \setlength{\tabcolsep}{0.75em} \begin{tabular}{lcccc}\hline    
    \textit{method}    &  \hspace{-22pt}\textit{transductive}\hspace{-5pt}    &  \footnotesize{\textit{backbone}}  &  \footnotesize{5way-1shot}  &  \footnotesize{5way-5shot}   \\\hline 
    MAML(*)        \cite{finn2017model}& \xmark  & conv-4    &  46.47  &  62.71  \\ 
    RelationNet(*) \cite{sung2018learning}    & \xmark  & conv-4    &  49.31  &  66.60  \\ 
ProtoNet(\#) \cite{snell2017prototypical} & \xmark  & conv-4    &  49.10  &  66.79  \\ 
    FEAT(\\blue{_\textit{p}}\red{_\textit{t}}) \cite{DeepEMD}    & \xmark  & resnet-12 &  65.91  &  82.41  \\
    FEAT( \cite{zhang2021sill}  & \cmark  & wrn-28-10 &  82.99  &  89.14  \\ 
    PTMAP-SF(\\blue{_\textit{p}}\red{_\textit{t}}\blue{_\textit{p}}\blue{_\textit{p}}) = from the method's paper itself ; 
        (\#) = our implementation ;
        }
        }
        ~\\ \vspace{-35pt} ~\\
    \label{tab:results_fsc_MiniImagenet}
\end{table} 



\subsection{Few-Shot Classification (FSC)}
Our main experiment is a comprehensive evaluation on the standard few-shot classification benchmarks \emph{MiniImagenet} \cite{vinyals2016matching}, \emph{CIFAR-FS} \cite{CIFAR}, and \emph{CUB} \cite{CUB}, with detailed results in Tables \ref{tab:results_fsc_MiniImagenet} and \ref{tab:results_fsc_CIFAR_CUB}. 
For \emph{MiniImagenet} (Table \ref{tab:results_fsc_MiniImagenet}) we report on both versions ``SOT" and ``SOT" over a range of backbone architectures, while for the smaller datasets \emph{CIFAR-FS}  and \emph{CUB} (Table \ref{tab:results_fsc_CIFAR_CUB}) we focus on the `drop-in' version ``SOT" and only the strongest wrn-28-10 architecture.

One goal here is to show that we can achieve new state-of-the-art FSC results, when we build on current state-of-the-art. But more importantly, we demonstrate the flexibility and simplicity of applying SOT in this setup, with improvements in the entire range of testing, including: (i) when building on different `hosting' methods; (ii) when working above different feature embeddings of different complexity backbones; and (iii) whether retraining the hosting network or just dropping-in SOT and performing standard inference.

To evaluate the performance of the proposed SOT, we applied it to previous FSC methods including the very recent state-of-the-art (PT-MAP \cite{hu2020leveraging}, Sill-NET \cite{zhang2021sill} and PT-MAP-SF \cite{chen2021few}) as well as a to more conventional methods like the popular ProtoNet \cite{snell2017prototypical}. The detailed results are presented in Tables \ref{tab:results_fsc_MiniImagenet} and \ref{tab:results_fsc_CIFAR_CUB}) for the different datasets. Note that SOT is by nature a transductive method\footnote{SOT is transductive in the sense that it needs to jointly process the data, but importantly, unlike other methods it does not gain its benefit in being so from making limiting assumptions about the structure of the instance, like knowing the number of classes, or the number of items per class.}, hence we marked its results as so, regardless of whether the hosting network is transductive or not. In the following, we discuss the two modes in which our transform can be used in existing FSC methods.





\vspace{3pt}\noindent\textbf{SOT insertion \textit{without} network retraining}
(notated by SOT in Tables \ref{tab:results_fsc_MiniImagenet} and \ref{tab:results_fsc_CIFAR_CUB}).
Recall that the proposed transform is \textit{non-parametric}. As such, we can simply apply it to a trained network at inference, without the need to re-train. This basic `drop-in' use of SOT consistently, and in many cases also significantly, improved the performance of the tested methods, including state-of-the-art, across all benchmarks and backbones. 
SOT gave improvements of around  and  on 1 and 5 shot \emph{MiniImagenet} tasks. 
This improvement without re-training the embedding backbone network shows SOT's effectiveness in capturing meaningful relationships between features in a very general sense. 



\newcommand{\bbbbb}[1]{{\footnotesize ({\color{blue} #1})}}
\newcommand{\rrrrr}[1]{({\footnotesize \color{red} #1})}
\newcommand{\foot}[1]{{\footnotesize #1}}

\begin{table}[t]
    \centering \small
    \setlength{\tabcolsep}{0.3em} \begin{tabular}{l:cc:cc}\hline  
    \textit{FSC benchmark}
    & \multicolumn{2}{c:}{\textit{CIFAR-FS}~\cite{CIFAR}} 
    & \multicolumn{2}{c}{\textit{CUB}~\cite{CUB}} \\
    \hline    
    \textit{method}     &  \footnotesize{5way-1shot}  &  \footnotesize{5way-5shot}  &  \footnotesize{5way-1shot}  &  \footnotesize{5way-5shot}   \\\hline 
    \footnotesize{PTMAP( \cite{zhang2021sill}}   &  87.73  &  91.09  &  94.73  &  96.28 \\ 
    \footnotesize{PTMAP-SF(\\blue{_\textit{p}}\blue{_\textit{p}}\blue{_\textit{p}}\red{_\textit{t}}\red{_\textit{t}}5\%3\%\mathcal{S}\mathcal{W}\lambda+6.8\%m\blue{_\textit{p}}\blue{_\textit{p}}$     &   \foot{\textbf{87.9}} \bbbbb{+0.9\%}    &         \foot{\textbf{{88.0}}} \bbbbb{+1.0\%}  &    \foot{\textbf{94.0}} \bbbbb{0.0\%} &     \foot{\underline{{95.0}}} \bbbbb{-0.3\%} \\
\hline           
    \end{tabular} \vspace{2pt}
    \caption{
        {\fontsize{8.5}{8.5} \selectfont
        \textbf{Re-ID} results on \emph{CUHK03} \cite{CUHK03} and  \emph{Market-1501} \cite{Market}} } 
                ~\\ \vspace{-35pt} ~\\
    \label{tab:results_on_DukeMTMC_and_market}
\end{table}








    
    
%
