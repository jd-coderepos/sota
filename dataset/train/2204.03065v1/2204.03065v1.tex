\newcommand{\mc}{\multicolumn{2}{|c|}}

\begin{figure}[t]
\hspace{-8pt}
\vspace{0pt}
\setlength{\tabcolsep}{5pt} \begin{tabular}{c c}
\begin{minipage}{3.6cm}
    \setlength{\tabcolsep}{-0.3em} \begin{tabular}{c c c}
     \includegraphics[width=1.4cm]{3Dexp3std01_test.pdf} &
\includegraphics[width=1.4cm]{3Dexp3std12_test.pdf} &
\includegraphics[width=1.4cm]{3Dexp3std20_test.pdf} \\
\includegraphics[width=1.4cm]{3Dexp3std31_test.pdf} &     
     \includegraphics[width=1.4cm]{3Dexp3std48_test.pdf} &
\includegraphics[width=1.4cm]{3Dexp3std75_test.pdf} \\\vspace{-7pt}
\end{tabular}\\
{\fontsize{7}{7} \selectfont
(\textbf{i}) 10 Random cluster centers on the unit sphere, perturbed with increasing noise STD $\sigma$.}
\end{minipage}
&
\begin{minipage}{8.2cm}
~\\\vspace{-3pt}~\\
 \includegraphics[width=8.3cm]{multi_plots.png} \\ \vspace{-15pt} ~\\
 \hspace{61pt}
{\fontsize{7}{7} \selectfont
(\textbf{ii}) Clustering accuracy across dimensions $d$ (left) and noise levels $\sigma$ (right). For each configuration, k-means accuracy is reported when applied with original (\textit{solid}) and SOT (\textit{dashed}) features.}
\end{minipage}
\end{tabular}
\\ \vspace{-16pt} \\
   \vspace{0pt}\caption{
   {\fontsize{8.5}{8.5} \selectfont
\textbf{Clustering on the \textit{d}-dimensional sphere}. \textbf{Left} (\textbf{i}): the data generation process (illustrated for the 3D case). \textbf{Right} (\textbf{ii}): detailed k-means accuracy results. The SOT (dashed) features give superior results throughout a majority of the space of settings.
}}
    \label{fig.synthetic_exp}\vspace{-11pt}
\end{figure}


\subsection{Clustering on the Sphere}
We first demonstrate the effectiveness of SOT using a controlled synthetically generated clustering experiment, with $k=10$ cluster centers that are distributed uniformly at random on a $d$-dimensional unit-sphere, and 20  points per cluster (200 in total) that are perturbed around the cluster centers by Gaussian noise of increasing standard deviation, of up to 0.75, followed by a re-projection back to the sphere by dividing each vector by its $L_2$ magnitude. We also apply dimensionality reduction with PCA to $d=50$, for dimensions above 50.

We performed the experiment over a logarithmic 2D grid of combinations of data dimensionalities $d$ in the range $[10, 1234]$ and Gaussian in-cluster noise STD in the range $[0.1,0.75]$. Refer to Fig.~\ref{fig.synthetic_exp} (i) for a visualization of the data generation process. 

Each point is represented by its $d$-dimensional euclidean coordinates vector, where the baseline clustering is obtained by running k-means on these location features. In addition, we run k-means on the set of features that has undergone SOT. Hence, the benefits of the transform (embedding) are measured indirectly through the accuracy\footnote{Accuracy is measured by comparison with the optimal permutation of the predicted labels, found by the Hungarian Algorithm~\cite{kuhn1955hungarian}.} achieved by running k-means on the embedded vs. original vectors.
Evaluation results are reported in Fig.~\ref{fig.synthetic_exp} (ii) as averages over 10 runs, by plotting accuracy vs. dimensionality (for different noise STDs) and accuracy vs noise STDs (for different dimensionalities). The results show (i) general accuracy gains and robustness to wide ranges of data dimensionality (ii) the ability of SOT to find meaningful representations that enable clustering quality to degrade gracefully with the increase in cluster noise level. Note that the levels of noise are rather high, as they are relative to a unit radius sphere (a 3-dimensional example is shown at the top of the figure). We provide further details on this experiment in the supplementaries.

\newcommand{\bbb}[1]{({\footnotesize \color{blue} #1})}
\newcommand{\rrr}[1]{({\footnotesize \color{red} #1})}
\newcommand{\cmark}{\hspace{-10pt}\ding{51}\hspace{-6pt}}\newcommand{\xmark}{\hspace{-10pt}\ding{55}\hspace{-6pt}}\begin{table}[t]
    \centering \small
    \setlength{\tabcolsep}{0.75em} \begin{tabular}{lcccc}\hline    
    \textit{method}    &  \hspace{-22pt}\textit{transductive}\hspace{-5pt}    &  \footnotesize{\textit{backbone}}  &  \footnotesize{5way-1shot}  &  \footnotesize{5way-5shot}   \\\hline 
    MAML(*)        \cite{finn2017model}& \xmark  & conv-4    &  46.47  &  62.71  \\ 
    RelationNet(*) \cite{sung2018learning}    & \xmark  & conv-4    &  49.31  &  66.60  \\ 
ProtoNet(\#) \cite{snell2017prototypical} & \xmark  & conv-4    &  49.10  &  66.79  \\ 
    FEAT(\$) \cite{ye2020few}   & \xmark  & conv-4 &  \textbf{55.15}  &  \textbf{71.61}  \\ 
\hdashline
    ProtoNet-SOT$\blue{_\textit{p}}$  & \cmark  & conv-4    &  \underline{{54.01}} \bbb{ +10.2\%}  &  69.39 \bbb{+3.9\%}  \\   
    ProtoNet-SOT$\red{_\textit{t}}$  & \cmark  & conv-4    &  53.70 \rrr{+9.3\%}  &  \underline{{70.40}} \rrr{+5.4\%}  \\    
\hline
    ProtoNet(\#) \cite{snell2017prototypical} & \xmark  & resnet-12 &  62.39  &  80.33  \\ 
    DeepEMD(\$) \cite{DeepEMD}    & \xmark  & resnet-12 &  65.91  &  82.41  \\
    FEAT(\$)    \cite{ye2020few}  & \xmark  & resnet-12 &  66.78  &  82.05  \\
    RENet(\$)   \cite{ReNet}      & \xmark  & resnet-12 &  67.60  &  82.58  \\
    PTMAP(\#) \cite{hu2020leveraging}  & \cmark  & resnet-12 &  76.90  &  85.20  \\ 
\hdashline
    ProtoNet-SOT$\blue{_\textit{p}}$  & \cmark  & resnet-12 &  67.34  \bbb{+7.9\%}  & 81.84 \bbb{+1.6\%} \\   
    ProtoNet-SOT$\red{_\textit{t}}$  & \cmark  & resnet-12 &  67.90 \rrr{+8.8\%}  &  83.09 \rrr{+3.2\%}  \\
    PTMAP-SOT$\blue{_\textit{p}}$     & \cmark  & resnet-12 &  \textbf{78.35} \bbb{+1.9\%}  &  \textbf{86.01} \bbb{+1.0\%}  \\   
    PTMAP-SOT$\red{_\textit{t}}$     & \cmark  & resnet-12 &  \underline{{77.30}} \rrr{+0.5\%}  &  \underline{{85.49}} \rrr{+0.3\%}  \\    
    \hline

ProtoNet(\&) \cite{snell2017prototypical} & \xmark  & wrn-28-10 &  62.60  &  79.97  \\ 
    PTMAP(\$) \cite{hu2020leveraging}  & \cmark  & wrn-28-10 &  82.92  &  88.80  \\ 
    Sill-Net(\$) \cite{zhang2021sill}  & \cmark  & wrn-28-10 &  82.99  &  89.14  \\ 
    PTMAP-SF(\$) \cite{chen2021few}    & \cmark  & wrn-28-10 &  \underline{{84.81}}  &  \underline{{90.62}}  \\ 
\hdashline
    PTMAP-COSINE     & \cmark  & wrn-28-10 &  74.60 \bbb{-10.0\%} &  84.68 \bbb{-4.6\%} \\   
    PTMAP-SOFTMAX    & \cmark  & wrn-28-10 &  80.08 \bbb{-3.4\%} &  83.83 \bbb{-5.6\%} \\
\hdashline



    PTMAP-SOT$\blue{_\textit{p}}$     & \cmark  & wrn-28-10 &  83.19 \bbb{+0.3\%} &  89.56 \bbb{+0.9\%} \\   
    PTMAP-SOT$\red{_\textit{t}}$     & \cmark  & wrn-28-10 &  84.18 \rrr{+1.5\%} &  90.51 \rrr{+1.9\%} \\
    Sill-Net-SOT$\blue{_\textit{p}}$  & \cmark  & wrn-28-10 &  83.35 \bbb{+0.4\%} &  89.65 \bbb{+0.6\%} \\
    PTMAP-SF-SOT$\blue{_\textit{p}}$  & \cmark  & wrn-28-10 &  \textbf{85.59} \bbb{+0.9\%}  &  \textbf{91.34}  \bbb{+0.8\%} \\
\hline           
    \end{tabular} \vspace{3pt}
    \caption{
    {\fontsize{8.5}{8.5} \selectfont
    \textbf{Few-Shot Classification (FSC)} accuracy on \textbf{MiniImagenet} \cite{vinyals2016matching}. The improvements introduced by the variants of SOT (percentages in brackets) are in comparison with each respective baseline hosting method. \textbf{Bold} and \underline{underline} notations highlight best and second best results per backbone.
         (*) = from \cite{chen2018closer} ;
        (\&) = from \cite{ziko2020laplacian} ; 
        (\$) = from the method's paper itself ; 
        (\#) = our implementation ;
        }
        }
        ~\\ \vspace{-35pt} ~\\
    \label{tab:results_fsc_MiniImagenet}
\end{table} 



\subsection{Few-Shot Classification (FSC)}
Our main experiment is a comprehensive evaluation on the standard few-shot classification benchmarks \emph{MiniImagenet} \cite{vinyals2016matching}, \emph{CIFAR-FS} \cite{CIFAR}, and \emph{CUB} \cite{CUB}, with detailed results in Tables \ref{tab:results_fsc_MiniImagenet} and \ref{tab:results_fsc_CIFAR_CUB}. 
For \emph{MiniImagenet} (Table \ref{tab:results_fsc_MiniImagenet}) we report on both versions ``SOT$\blue{_\textit{p}}$" and ``SOT$\red{_\textit{t}}$" over a range of backbone architectures, while for the smaller datasets \emph{CIFAR-FS}  and \emph{CUB} (Table \ref{tab:results_fsc_CIFAR_CUB}) we focus on the `drop-in' version ``SOT$\blue{_\textit{p}}$" and only the strongest wrn-28-10 architecture.

One goal here is to show that we can achieve new state-of-the-art FSC results, when we build on current state-of-the-art. But more importantly, we demonstrate the flexibility and simplicity of applying SOT in this setup, with improvements in the entire range of testing, including: (i) when building on different `hosting' methods; (ii) when working above different feature embeddings of different complexity backbones; and (iii) whether retraining the hosting network or just dropping-in SOT and performing standard inference.

To evaluate the performance of the proposed SOT, we applied it to previous FSC methods including the very recent state-of-the-art (PT-MAP \cite{hu2020leveraging}, Sill-NET \cite{zhang2021sill} and PT-MAP-SF \cite{chen2021few}) as well as a to more conventional methods like the popular ProtoNet \cite{snell2017prototypical}. The detailed results are presented in Tables \ref{tab:results_fsc_MiniImagenet} and \ref{tab:results_fsc_CIFAR_CUB}) for the different datasets. Note that SOT is by nature a transductive method\footnote{SOT is transductive in the sense that it needs to jointly process the data, but importantly, unlike other methods it does not gain its benefit in being so from making limiting assumptions about the structure of the instance, like knowing the number of classes, or the number of items per class.}, hence we marked its results as so, regardless of whether the hosting network is transductive or not. In the following, we discuss the two modes in which our transform can be used in existing FSC methods.





\vspace{3pt}\noindent\textbf{SOT insertion \textit{without} network retraining}
(notated by SOT$\blue{_\textit{p}}$ in Tables \ref{tab:results_fsc_MiniImagenet} and \ref{tab:results_fsc_CIFAR_CUB}).
Recall that the proposed transform is \textit{non-parametric}. As such, we can simply apply it to a trained network at inference, without the need to re-train. This basic `drop-in' use of SOT consistently, and in many cases also significantly, improved the performance of the tested methods, including state-of-the-art, across all benchmarks and backbones. 
SOT$\blue{_\textit{p}}$ gave improvements of around $3.5\%$ and $1.5\%$ on 1 and 5 shot \emph{MiniImagenet} tasks. 
This improvement without re-training the embedding backbone network shows SOT's effectiveness in capturing meaningful relationships between features in a very general sense. 



\newcommand{\bbbbb}[1]{{\footnotesize ({\color{blue} #1})}}
\newcommand{\rrrrr}[1]{({\footnotesize \color{red} #1})}
\newcommand{\foot}[1]{{\footnotesize #1}}

\begin{table}[t]
    \centering \small
    \setlength{\tabcolsep}{0.3em} \begin{tabular}{l:cc:cc}\hline  
    \textit{FSC benchmark}
    & \multicolumn{2}{c:}{\textit{CIFAR-FS}~\cite{CIFAR}} 
    & \multicolumn{2}{c}{\textit{CUB}~\cite{CUB}} \\
    \hline    
    \textit{method}     &  \footnotesize{5way-1shot}  &  \footnotesize{5way-5shot}  &  \footnotesize{5way-1shot}  &  \footnotesize{5way-5shot}   \\\hline 
    \footnotesize{PTMAP(\$) \cite{hu2020leveraging}}   &  87.69  &  90.68  &  91.55  &  93.99\\ 
    \footnotesize{Sill-Net(\$) \cite{zhang2021sill}}   &  87.73  &  91.09  &  94.73  &  96.28 \\ 
    \footnotesize{PTMAP-SF(\$) \cite{chen2021few}}     &  \underline{{89.39}}  &  \underline{{92.08}}  &  \underline{{95.45}}  &  \underline{{96.70}} \\ 
\hdashline
    PTMAP-SOT$\blue{_\textit{p}}$      &  87.37  \bbb{-0.4\%} &  91.12  \bbb{+0.5\%} & 91.90 \bbb{+0.4\%} &  94.63 \bbb{+0.7\%}  \\   
    Sill-Net-SOT$\blue{_\textit{p}}$   &  87.30  \bbb{-0.5\%} &  91.40 \bbb{+0.3\%} & 94.86 \bbb{+0.1\%} &  96.61 \bbb{+0.3\%} \\
    PTMAP-SF-SOT$\blue{_\textit{p}}$   &  \textbf{89.94} \bbb{+0.6\%}  &  \textbf{92.83} \bbb{+0.8\%}  &  \textbf{95.80} \bbb{+0.4\%} &  \textbf{97.12} \bbb{+0.4\%} \\
\hline           
    \end{tabular} \vspace{2pt}
    \caption{
    {\fontsize{8.5}{8.5} \selectfont
    \textbf{Few-Shot Classification (FSC)} accuracy on \textit{CIFAR-FS}~\cite{CIFAR} and \textit{CUB}~\cite{CUB}.
}}         ~\\ \vspace{-35pt} ~\\

    \label{tab:results_fsc_CIFAR_CUB}
\end{table}



\vspace{3pt}\noindent\textbf{SOT insertion \textit{with} network retraining}
(notated by SOT$\red{_\textit{t}}$ in Table \ref{tab:results_fsc_MiniImagenet}).
Due to its \textit{differentiability} property, the proposed method can be applied while training and hence we expect an adaptation of the hosting network's parameters to the presence of the transform with a potential for improvement. 
To evaluate this mode, we focused on the \emph{MiniImagenet} benchmark \cite{vinyals2016matching}, specifically on the same configurations that we used without  re-training, to enable a direct comparison. The results in Table \ref{tab:results_fsc_MiniImagenet} show additional improvements in almost every method. SOT$\red{_\textit{t}}$ gave improvements of around $5\%$ and $3\%$ on 1 and 5 shot \emph{MiniImagenet} tasks, further improving on the pre-trained counterpart. This result indicates the effectiveness of training with SOT in an end-to-end fashion. 

\vspace{3pt}\noindent\textbf{Ablations} Within the context of few-shot learning on \emph{MiniImagenet}, we performed several ablation studies. In Table~\ref{tab:results_fsc_MiniImagenet}, the networks `PTMAP-COSINE' and `PTMAP-SOFTMAX' stand for the obvious baseline attempts (found to be unsuccessful) that work in the line of our approach, without the specialized OT-based transform. In the former, we take the output features to be the rows of the (un-normalized) matrix  $\mathcal{S}$ (rather than those of  $\mathcal{W}$) and in the latter we also normalize its rows using soft-max.
In the supplementaries we ablate on SOT's two parameters - the number of Sinkhorn iterations and the entropy term $\lambda$. 


\subsection{Person re-Identification (Re-ID)}
In this section, we explore the possibility of using SOT on large-scale datasets by considering the Person re-Identification task. 
Given a set of \textit{query} images and a large set of \textit{gallery} images, the task is to rank the similarities of each single query against the gallery. This is done by computing specialized image features among which similarities are based on Euclidean distances. SOT is applied to such pre-computed image features, refining them with the strong relative information that it is able to capture by applying it on the union of all query and gallery features. We adapted a pre-trained standard resnet-50 architecture \cite{torchreid} and the popular TopDBNet \cite{Top-DB-Net}, which we tested on the large-scale ReID benchmarks \emph{CUHK03} \cite{CUHK03} (on the 'detected' version and similar results on the `labeled' version in the supplementaries) and \textit{Market-1501} \cite{Market}, with and without the re-ranking \cite{Re-ranking} procedure. For evaluation, we followed their conventions and compare results using the mAP (mean Average Precision) and Rank-1 metrics.

The results in Table \ref{tab:results_on_DukeMTMC_and_market} show a consistent benefit in using SOT within the different networks. For \emph{CUHK03}, the results improved by a large margin of $+6.8\%$ in $m$AP for the best configuration. These results demonstrate that the proposed SOT scales well to large-scale problems (with number of features in the  thousands) and is attractive for a variety of applications. 
ReID is not the main focus of this work, hence, we did not re-train the hosting networks with SOT included. Further research is required to measure the possible effects of doing so.

\begin{table}[t]
    \centering \small
    \setlength{\tabcolsep}{0.25em} \begin{tabular}{l | cc | cc}\hline
    \textit{ReID benchmark} & \multicolumn{2}{c}{\emph{CUHK03-detected} \cite{CUHK03}} & \multicolumn{2}{|c}{\emph{Market-1501} \cite{Market}}  \\
    \hline    
    \textit{network}    &  \foot{mAP}  &  \foot{Rank-1} &  \foot{mAP}  &  \foot{Rank-1}\\\hline
    TopDBNet \cite{Top-DB-Net}  & \foot{72.9}   &  \foot{75.7}    & \foot{85.7}   &  \foot{94.3}        \\
    TopDBNet-rerank \cite{Top-DB-Net} & \foot{\underline{87.1}}   &     \foot{\underline{87.1}}      &       \foot{\textbf{94.0}}   &    \foot{\textbf{95.3}}    \\

\hdashline

    TopDBNet-SOT$\blue{_\textit{p}}$  &       \foot{{{77.9}}} \bbbbb{+6.9\%}  &  \foot{80.4} \bbbbb{+6.2\%}    &     \foot{\underline{{88.1}}} \bbbbb{+2.8\%}     &         \foot{{94.4}} \bbbbb{+0.1\%}       \\
    TopDBNet-rerank-SOT$\blue{_\textit{p}}$     &   \foot{\textbf{87.9}} \bbbbb{+0.9\%}    &         \foot{\textbf{{88.0}}} \bbbbb{+1.0\%}  &    \foot{\textbf{94.0}} \bbbbb{0.0\%} &     \foot{\underline{{95.0}}} \bbbbb{-0.3\%} \\
\hline           
    \end{tabular} \vspace{2pt}
    \caption{
        {\fontsize{8.5}{8.5} \selectfont
        \textbf{Re-ID} results on \emph{CUHK03} \cite{CUHK03} and  \emph{Market-1501} \cite{Market}} } 
                ~\\ \vspace{-35pt} ~\\
    \label{tab:results_on_DukeMTMC_and_market}
\end{table}








    
    
%
