\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{preamble}
\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
\pdfinfo{
/Title (The Weighted Tsetlin Machine: Compressed Representations with Weighted Clauses)
/Author (Adrian Phoulady, Ole-Christoffer Granmo, Saeed Rahimi Gorji, Hady Ahmady Phoulady)}
\setcounter{secnumdepth}{2}  

\begin{document}
\title{The Weighted Tsetlin Machine:\\Compressed Representations with Weighted Clauses}
\author{Adrian Phoulady, Ole-Christoffer Granmo, Saeed R. Gorji, Hady Ahmady Phoulady\\
        \small \href{mailto:adrian.phoulady@gmail.com}{adrian.phoulady@gmail.com}\\
        \small Centre for Artificial Intelligence Research, University of Agder, Norway\\
        \small Department of Computer Science, California State University, Sacramento, CA, USA}
\nocopyright
\maketitle
\begin{abstract}
The Tsetlin Machine (TM) is an interpretable mechanism for pattern recognition that constructs conjunctive clauses from data. The clauses capture frequent patterns with high discriminating power, providing increasing expression power with each additional clause. However, the resulting accuracy gain comes at the cost of linear growth in computation time and memory usage. In this paper, we present the Weighted Tsetlin Machine (WTM), which reduces computation time and memory usage by \emph{weighting} the clauses. Real-valued weighting allows one clause to replace multiple, and supports fine-tuning the impact of each clause. Our novel scheme simultaneously learns both the composition of the clauses and their weights. Furthermore, we increase training efficiency by replacing  Bernoulli trials of success probability  with a uniform sample of average size , the size drawn from a binomial distribution. In our empirical evaluation, the WTM achieved the same accuracy as the TM on MNIST, IMDb, and Connect-4, requiring only , , and  of the clauses, respectively. With the same number of clauses, the WTM outperformed the TM, obtaining peak test accuracies of respectively , , and . Finally, our novel sampling scheme reduced sample generation time by a factor of .
\end{abstract}

\maketitle
\keywords{Tsetlin Machines, Pattern Recognition, Propositional Logic, Learning Automata, Frequent Pattern Mining.}
\section{Introduction}
The Tsetlin Machine (TM) is a novel learning mechanism that was introduced in 2018 \cite{granmo2018tsetlin}. Like artificial neural networks, the TM can map an exponential number of feature value combinations to an appropriate response. However, neural networks are based on multiplication, accumulation, and non-linear activation across multiple layers of neurons, whereas the TM is based on propositional logic formulas, formed by learning automata. 

For  binary features, there can be  propositional formulas. Equipped with an adequate number of conjunctive clauses, a TM can learn any of them in a relatively simple form. In addition to being computationally simple, the models built by the TM have the advantage of being interpretable by human experts, which may be of vital importance in sensitive applications such as medical decision making~\cite{berge2019,Rudin2019}.

\paragraph{Learning Automata.}
Learning automata have attracted considerable interest because they can learn the optimal action when operating in unknown stochastic environments \cite{thathachar1987learning}. As such, they have been used for pattern classification over more than four decades. Early work includes stochastic learning automata based classifiers and games of learning automata \cite{barto1985pattern,thathachar1987learning}. These approaches can learn the optimal classifier for specific forms of discriminant functions (e.g., linear classifiers) also when feedback is noisy. Along the same lines, Sastry and Thathachar have provided several algorithms based on cooperating systems of learning automata \cite{sastry1999learning}. 

More recently, Zahiri proposed hyperplane- and rule-based classifiers, which performed comparably to other well-known methods in the literature \cite{zahiri2008learning,zahiri2012classification}. Recent research also includes a noise-tolerant learning algorithm built upon a team of continuous-action learning automata \cite{sastry2009team}. Further, Goodwin, Yazidi, and Jonassen proposed a learning automata-guided random walk on a grid to construct robust discriminant functions \cite{goodwin2016distributed}. Finally, Motieghader et al. introduced a hybrid scheme that combines a genetic algorithm with learning automata to address the gene selection problem in cancer classification \cite{motieghader2017hybrid}. 

In general, previous learning automata schemes have mainly addressed relatively small-scale pattern recognition tasks. Lately, however, several TM-based approaches have demonstrated promising scalability properties. This includes the natural language understanding approach by Berge et al., which uses the conjunctive clauses of the TM to capture textual patterns \cite{berge2019}. Also, TM-based convolution for image analysis was introduced by Granmo et al., employing the clauses as interpretable filters \cite{granmo2019convtsetlin}. Further, Abeyrathna et al. designed a novel TM for regression \cite{abeyrathna2019regressiontsetlin} that supports continuous input \cite{abeyrathna2019scheme}. All of these schemes compare favourably with state-of-the-art pattern classification and regression techniques.



\paragraph{Paper Contributions.} In this paper, we introduce the Weighted Tsetlin Machine (WTM), which employs \emph{weighted conjunctive clauses}. Our intent is to significantly reduce memory usage and computation time by using the clause weights to compress the pattern representation of the TM. We start with giving a brief overview of the TM in Section~\ref{sec:tsetlin_machine}. Then, in Section~\ref{sec:weighted_tsetlin_machine}, we introduce the WTM and the concept of weighted clauses, that is, clauses that output continuous rather than binary values. Real-valued weighting allows one clause to replace multiple, and supports fine-tuning the impact of each clause. Our novel scheme simultaneously learns both the composition of each clause as well as their weights. Further, we simplify sample generation during learning by replacing Bernoulli process sampling with sampling from a binomial distribution followed by uniform sampling. We evaluate our approach empirically on MNIST, IMDb, and Connect-4 in Section \ref{sec:results}, demonstrating that the WTM outperforms the TM in all the experiments. We conclude in Section~\ref{sec:conclusion}, and present paths ahead for further research.

\section{The Tsetlin Machine}
\label{sec:tsetlin_machine}
TM learning is based on forming formulas in propositional logic to capture frequent patterns of high discriminating power. We here describe how the TM operates multiple teams of so-called Tsetlin automata \cite{tsetlin1961behaviour} to build discriminative conjunctive clauses, followed by majority voting to decide the final classification from the clause outputs.
\subsection{The Tsetlin Automaton}
A two-action {\em Tsetlin automaton} \cite{tsetlin1961behaviour,narendra2012learning} is a learning automaton with  states , , \ldots,  and two actions ,  (Figure~\ref{fig:tsetlin_automaton}). It performs its actions sequentially in an environment. That is, in the states , \ldots, , the Tsetlin automaton performs action , and in the states , \ldots, , it performs action . The environment responds with two types of input to the automaton: penalties  and rewards . A state transition function governs learning. With the penalty input , the transition function changes the state  to  for  and to  for . Conversely, the reward input  makes the state  change to  for  and to  for , while leaving it unchanged for . In effect,  makes the Tsetlin automaton change state toward the centre, whereas  moves the state away from the centre, towards the extreme ends  or .

\begin{figure*}[ht]
\centering
\colorlet{rcolor}{darkgreen}
\colorlet{pcolor}{darkerred}
\colorlet{scolor}{darkergray}
\begin{tikzpicture}[>=stealth, shorten >=0pt,node distance=1.5cm]
  \node [state, scolor] (S1)                {};
  \node [state, scolor] (S2)  [right of=S1] {};
  \node [scolor]        (D1)  [right of=S2] {};
  \node [state, scolor] (SN)  [right of=D1] {};
  \node [state, scolor] (SN1) [right of=SN,inner sep=0] {};
  \node [state, scolor] (SN2) [right of=SN1,inner sep=0] {};
  \node [scolor]        (D2)  [right of=SN2] {};
  \node [state, scolor] (S2N) [right of=D2] {};
  
	\path[->] (S1) edge [loop left, looseness=8, rcolor]    node               {} (S1);
	\path[->] (S2) edge [bend right, rcolor] node [above]  {} (S1);
	\path[->] (D1) edge [tail=.6,bend right, rcolor] (S2);
	\path[-] (SN) edge [head=.6,bend right, rcolor] (D1);
	\path[->] (S1) edge [densely dashed, pcolor, bend right] node [below]  {} (S2);
	\path[-] (S2) edge [head=.6, densely dashed, pcolor, bend right] (D1);
	\path[->] (D1) edge [tail=.6, densely dashed, pcolor, bend right] (SN);

	\path[->] (S2N) edge [loop right, looseness=8, rcolor]    node               {} (S2N);
	\path[<-] (SN2) edge [bend right, rcolor] node [above]  {} (SN1);
	\path[-] (SN2) edge [head=.6,bend left, rcolor] (D2);
	\path[->] (D2) edge [tail=.6,bend left, rcolor] (S2N);
	\path[<-] (SN1) edge [densely dashed, pcolor, bend right] node [below]  {} (SN2);
	\path[->] (D2) edge [tail=.6, densely dashed, pcolor, bend left] (SN2);
	\path[-] (D2) edge [tail=.6, densely dashed, pcolor, bend right] (S2N);
	\path[->] ([yshift=+1mm] SN.east) edge [densely dashed, pcolor] node [above]  {} ([yshift=+1mm] SN1.west);
	\path[<-] ([yshift=-1mm] SN.east) edge [densely dashed, pcolor] node [below]  {} ([yshift=-1mm] SN1.west);

	\path[-, thick, gray] ([yshift=-11mm] S1.west) edge node [fill=white] {} ([yshift=-11mm] SN.east);
	\path[-, thick, darkergray] ([yshift=-11mm] SN1.west) edge node [fill=white] {} ([yshift=-11mm] S2N.east);
\end{tikzpicture}
\caption{A two-action Tsetlin automaton with  internal states}\label{fig:tsetlin_automaton}
\end{figure*}

\subsection{Tsetlin Machine Structure and Inference}
A binary classifier over  binary features can be regarded as a Boolean function --- each of the  possible combinations of the  binary inputs belongs to one of the classes  or .

A Boolean function can always be represented as a disjunction of conjunctive clauses, referred to as disjunctive normal form. For binary input features , each conjunctive clause  is represented by a subset  of the literal set . Knowing , we have

The Boolean function  in disjunctive normal form with clauses , \ldots,  then becomes


Similar to Karnaugh maps \cite{karnaugh1953map}, conjunctive clauses form the core of function representation in the TM. In brief, a TM learns the composition of each conjunctive clause by specifying its literals using a team of  Tsetlin automata, one Tsetlin automaton per literal. Each Tsetlin automaton decides between two actions. Action  means excluding the associated literal, whereas action  means including it in the conjunctive clause. To exemplify, Figure~\ref{fig:tsetlin_automata_team} shows the configuration of a team of eight Tsetlin automata that form a conjunctive clause from the three binary features , , and .

\begin{figure}[ht]
\centering
\definecolor{acolor}{rgb}{0.0, 0.1, 0.2}
\colorlet{icolor}{darkgreen}
\colorlet{ecolor}{darkerred}
\colorlet{acolor}{darkergray}
\tikzset{automaton/.style = {rectangle, draw, acolor, minimum width=.9cm, minimum height=.7cm},
exclude/.style = {densely dashed, ecolor, opacity=0.8},
include/.style = {icolor}}
\begin{tikzpicture}[>=stealth, shorten >=0pt,node distance=1.2cm]
  \node [automaton]                      (TA1) {};
  \node [automaton, right of=TA1] (TA2) {};
  \node [automaton, right of=TA2] (TA3) {};
  \node [automaton, right of=TA3] (TA4) {};
  \node [automaton, right of=TA4] (TA5) {};
  \node [automaton, right of=TA5] (TA6) {};
  \path[->] (TA1) edge [include] node [right]  {}
        node [above=.3cm, black] {}
        +(0, .8) node [below=.5cm] {};
  \path[->] (TA2) edge [exclude] node [right]  {}
        +(0, .8) node [below=.5cm] {};
  \path[->] (TA3) edge [include] node [right]  {}
        node [above=.3cm, black] {}
        +(0, .8) node [below=.5cm] {};
  \path[->] (TA4) edge [exclude] node [right]  {}
        +(0, .8) node [below=.5cm] {};
  \path[->] (TA5) edge [include] node [right]  {}
        node [above=.3cm, black] {}
        +(0, .8) node [below=.5cm] {};
  \path[->] (TA6) edge [exclude] node [right]  {}
        +(0, .8) node [below=.5cm] {};
\end{tikzpicture}
\caption{A team of six two-action Tsetlin automata forming the clause }
\label{fig:tsetlin_automata_team}
\end{figure}

Instead of forming a disjunction of the conjunctive clauses, a TM sums up the clause outputs to produce an ensemble effect. While any propositional formula can be represented in disjunctive normal form, it turns out that the introduced ensemble effect helps dealing with noisy data \cite{granmo2018tsetlin}. Further, the TM groups the clauses into positive ones , , \ldots,  and negative ones , , \ldots, . In effect, for the input , the TM computes the signed sum

to perform classification. If the signed sum is negative, the TM classifies the input to class , and otherwise, it classifies it to class . In other words, the classification is performed by applying the unit step function on :


\subsection{Tsetlin Machine Learning}
The TM builds upon reinforcement learning, with feedback given directly to the conjunctive clauses. Each clause, in turn, passes the feedback onward to its individual Tsetlin automata. There are two types of feedback: Feedback Type~I and Feedback Type~II. When a clause receives Type~I feedback, its automata are modified so that the clause eventually evaluates to  for the current input. If it already evaluates to , Type~I feedback will instead refine the clause by inserting more literals. Type~II feedback, on the other hand, makes the affected clauses eventually evaluate to . In combination, these two types of feedback gradually modify which literals are present in each clause in a way that over time improves the classification accuracy.

\subsubsection{Overall Feedback Loop}
A TM is initialized by randomly setting the states of the individual Tsetlin automata, thus producing clauses with randomly selected literals. From there, training data is fed to the TM, one example  at a time. In other words, the learning can be performed on-line. For an input  of class , the goal is to make the signed sum  of the clauses become negative (see Equation~\ref{eqn:y_hat}). If this is not the case, we mitigate by giving some of the negative clauses Type~I feedback, and some of the positive clauses Type~II feedback. For input  of class , on the other hand, the learning goal is to make the signed sum  become non-negative (again see Equation~\ref{eqn:y_hat}). Accordingly, if the signed sum of the clauses is negative, we increase it by giving some of the positive clauses Type~I feedback and some of the negative clauses Type~II feedback.

To introduce an ensemble effect, feedback is given to a random selection of the clauses based on a hyperparameter  -- the target of summation. In brief, the TM strives to make the signed sum  reach  for an input of class . Conversely, it seeks to make  reach  for an input of class . To achieve this, we first clamp  between  and : . Second, each clause receives feedback with probability  proportional to the difference between the clamped sum and the summation target:
1.5ex]
		\displaystyle\frac{T-c(\mathbf x)}{2T},&\quad\text{if } y=1.
	\end{cases}
1ex]
\bfseries Literal value&\hfil\bfseries0&\hfil\bfseries1&\hfil\bfseries0&\hfil\bfseries1\\
\midrule[\heavyrulewidth]
\bfseries Inclusion response&\hfil-penalty&\hfil-penalty&\hfil NA&\hfil reward\1ex]
\bfseries Literal value&\hfil\bfseries0&\hfil\bfseries1&\hfil\bfseries0&\hfil1\\
\midrule[\heavyrulewidth]
\bfseries Inclusion response&\hfil--&\hfil--&\hfil NA&\hfil--\
P(S_u=k)=\binom u k p_s^k(1-p_s)^{u-k}.

s'(\mathbf x)=\sum_{j=1}^{c_P}w^+_j C^+_j(\mathbf x)-\sum_{j=1}^{c_N}w^-_jC^-_j(\mathbf x).

\hat y=u\bigl(s'(\mathbf x)\bigr)=u\biggl(
\sum_{j=1}^{c_P}w^+_j C^+_j(\mathbf x)-\sum_{j=1}^{c_N}w^-_jC^-_j(\mathbf x)\biggr).

\hat y=\mathop{\mathrm{argmax}}_i\{s'_i(\mathbf x)\}.

w^+_j&\gets 1.0,\\
w^-_j&\gets 1.0.

w^+_j&\gets w^+_j\cdot(1+\gamma),\quad\text{if }C^+_j(\mathbf x)=1,\\
w^-_j&\gets w^-_j\cdot(1+\gamma),\quad\text{if }C^-_j(\mathbf x)=1.

w^+_j&\gets w^+_j\,/\,(1+\gamma),\quad\text{if }C^+_j(\mathbf x)=1,\\
w^-_j&\gets w^-_j\,/\,(1+\gamma),\quad\text{if }C^-_j(\mathbf x)=1.

w^+_j&\gets w^+_j,\quad\text{if }C^+_j(\mathbf x)=0,\\
w^-_j&\gets w^-_j,\quad\text{if }C^-_j(\mathbf x)=0.

\nu=\frac{c_P+c_N}{\sum_{i=1}^{c_P}f^+_i+\sum_{j=1}^{c_N}f^-_j}

f_i^{\mathit{new}}=\nu f_i.
.5ex]
\bfseries Minimum
\setcounter{col}{0}\while{\thecol<10}{\expandafter\and
    \pgfplotstablegetelem{0}{\thecol}\of{\dt}\pgfmathprintnumberto[fixed, precision=2]{\pgfplotsretval}{\theret}\theret
    \stepcounter{col}}\\\midrule
\bfseries Mean
\setcounter{col}{0}\while{\thecol<10}{\expandafter\and
    \pgfplotstablegetelem{2}{\thecol}\of{\dt}\pgfmathprintnumberto[fixed, precision=2]{\pgfplotsretval}{\theret}\theret
    \stepcounter{col}}\.5ex]
\bfseries Ratio
\setcounter{col}{0}\while{\thecol<10}{\expandafter\and
    \pgfplotstablegetelem{1}{\thecol}\of{\dt}\pgfmathsetmacro\mx\pgfplotsretval
    \pgfplotstablegetelem{0}{\thecol}\of{\dt}\pgfmathsetmacro\mn\pgfplotsretval
    \pgfmathsetmacro\theratio{\mx/\mn}\pgfmathprintnumberto[fixed, precision=0]{\theratio}{\theret}\theret
    \stepcounter{col}}\\
\bottomrule
\end{tabular}
\caption{Clause weight statistics per class for a WTM with  clauses, trained on MNIST}
\label{tab:coefficients}
\end{table*}



\subsubsection{Comparison with Other Methods}
Table~\ref{tab:comp} contrasts the WTM mean test accuracy on MNIST against vanilla versions of selected well-known classification methods, without applying performance enhancing techniques (such as boosting, ensembles, and convolution), and without data augmentation (such as deskewing, blurring, shifting, and resizing).\footnote{The results are obtained from \url{http://yann.lecun.com/exdb/mnist/} 
and the original TM paper \cite{granmo2018tsetlin}.} Note that the TM and WTM operate on the threshold-based binary encoding of the images (compression factor 8), rather than the original greyscale images. As seen, the WTM performs favourably compared to the other techniques, under comparable conditions.



\begin{table}[!ht]
\centering
\begin{tabular}{rl}
\toprule
\bfseries Method&\bfseries Accuracy\,(\%)\!\!\!\\
\midrule[\heavyrulewidth]
2-layer NN, 800 Hidden Units&98.6\\
\bfseries Weighted Tsetlin Machine&\boldmath\\
K-nearest Neighbors, L3&97.2\\
3-layer NN, 500+150 Hidden Units&97.1\\
40 PCA + Quadratic Classifier&96.7\\
1,000 RBF + Linear Classifier&96.4\\
Logistic Regression&91.5\\
Linear Classifier (1-layer NN)&88.0\\
Decision Tree&87.8\\
Multinomial Naive Bayes&83.2\\
\bottomrule
\end{tabular}
\caption{Comparison of vanilla methods on MNIST}
\label{tab:comp}
\end{table}

\subsection{IMDb Sentiment Analysis}
The IMDb sentiment analysis dataset consists of  highly polar movie reviews, organized into two categories: positive and negative reviews. The dataset is split evenly into  reviews for training and  reviews for testing. In this experiment, we use the unigrams and bigrams of each review as feature candidates, selecting the best  ones based on Chi-square feature selection. Each review is thus represented by a binary feature vector of size , marking the presence and absence of unigrams/bigrams.

\begin{table}[!ht]
\centering
\tableit
{89.86}{89.73}{89.80}{0.05}{89.73}{89.86}
{90.37}{90.00}{90.25}{0.16}{90.00}{90.37}
\caption{IMDb test accuracy statistics for a WTM and a TM with  clauses per class}
\label{tab:imdb}
\end{table}

We first compare a WTM with  class-clauses with a TM with  class-clauses. After 30 epochs of training, the WTM reached a peak testing accuracy of \%, compared to \% for the TM.
With an equal number of clauses,  per class, the WTM obtained an accuracy of , significantly higher than what is achieved with the TM (Table~\ref{tab:imdb}). The average processing time per epoch was 16 minutes for the TM and 14 minutes for the WTM when both use binomial distribution based feedback.







\subsection{Connect-4 Winner Prediction}
\label{section:connect-4}
Connect-4 is a two-player board game similar to Tic-Tac-Toe, played on a  grid. Each player tries to connect four of her pieces in a row, either horizontally, vertically, or diagonally. The Connect-4 dataset \cite{Dua:2019} contains all  legal positions of the board after eight moves, in which neither player has won yet and the next move is not forced. Each position is labelled as either a win, a loss, or a draw --- the outcome of the game after optimal play from the respective position. We assign  of the data to testing, keeping  for training. We further represent the board state as a binary feature vector of size . The first  bits capture the placement of player one pieces, and the last  bits capture the placements of player two.

Using a WTM with  clauses, we achieved a similar mean test accuracy to a TM with  clauses over the last  epochs of  epochs of training; that is, 
the WTM provided similar accuracy with  times fewer clauses! Table~\ref{tab:connect4} contains the resulting test accuracy statistics of a WTM and a TM with the same number of  clauses per class, again, demonstrating superior performance of the WTM compared to the TM. Here, the TM spent on average 105 seconds per epoch, while the WTM used 76 seconds, which again is slightly faster despite the addition of weights.



\begin{table}[!ht]
\centering
\tableit
{82.93}{81.98}{82.49}{0.34}{82.19}{82.80}
{87.91}{86.75}{87.32}{0.20}{86.97}{87.66}
\caption{Connect-4 test accuracy statistics for a WTM and a TM with  clauses per class
}
\label{tab:connect4}
\end{table}





\section{Conclusion and Further Work}\label{sec:conclusion}
In this paper, we introduced the Weighted Tsetlin Machine (WTM), generalizing the TM by adding weighted clauses. Instead of repeating a clause multiple times to increase its impact, we assign a weight to each clause. The weights control the impact of the clauses, and are learnt using a novel variant of Type~I and Type~II feedback. In this manner, the WTM can obtain a more compact representation of patterns. In addition, since the weights can take fractional values, the clauses can more easily be fine-tuned to represent complex patterns. We further proposed a new sampling approach for Type~I feedback generation, replacing sampling from a Bernoulli process with sampling from a binomial distribution followed by uniform sampling.

Our empirical results showed that the WTM performs comparably to the WTM when equipped with much fewer clauses (from  to  times less clauses). Furthermore, with the same amount of clauses, the WTM outperformed the TM accuracy-wise on MNIST, IMDb, and Connect-4.

In our further work, we intend to investigate whether the WTM approach also can improve the Regression Tsetlin Machine \cite{abeyrathna2019regressiontsetlin} for logic regression~\cite{ruczinski2003logic}, being capable of more fine-grained tuning of clauses. Another improvement, also investigated in \cite{abeyrathna2019scheme} for the regular TM, would be to introduce real-valued instead of binary inputs. This, for instance, would enable the WTM to operate directly on greyscale images, eliminating the need for binarization.

\bibliographystyle{aaai}
\bibliography{references-StarAI20}
\end{document}
