

\section{Discussion and Comparison}
In the section, we try to verify the necessity and rationality of PCL through both qualitative and quantitative analyses.
In particular, we will focus on answering the following three questions:

1. One of the core contributions of this paper is the discovery that "reducing the deviation between features and class weights is the key to the problem of whether contrastive learning can be successfully applied in DA tasks". If it is not based on this motivation, can we achieve the performance of PCL or directly induce PCL through some technologies similar to PCL, such as the projection head, consistency of prediction space?

2. Could simply approximating the probabilities to the one-hot form substantially improve the performance of FCL without having to keep the InfoNCE form?

3. Many works~\cite{dwibedi2021little,khosla2020supervised,xu2020hierarchical} have shown that alleviating the false negative problem can effectively improve the performance of contrastive learning. Therefore, can we also solve the false negative problem to effectively improve the poor performance of FCL in domain adaptation tasks without using PCL?


4. Does PCL really effectively reduce the deviation between features and class weights?

For the quantitative analysis, the typical semi-supervised domain adaptation (SSDA) setting on DomainNet~\cite{peng2019moment} with 3-shot and ResNet34 is adopted as the benchmark.
We believe these analyses can also provide some useful insights for other visual tasks~\cite{hou2019learning,mohri2019agnostic,li2021learning}.
\subsection{PCL v.s. FCL}
Current approaches based on FCL~\cite{singh2021clda,singh2021semi,zhou2022domain} generally regard contrastive learning as a general technique to improve feature consistency and often focus on the false negative sample problem.
Few works consider the shortcomings of contrastive learning in domain adaptation tasks from the perspective of the deviation of features and class weights.
Therefore, we believe that analyzing and finding this shortcoming is not a trivial result.
Based on the above insights, we deduce a concise PCL through in-depth analysis.
We surprisingly found that only two simple operations (using probabilities and removing  normalization) are needed to force features close to class weights.


Experimentally, we compare contrastive learning based on features (FCL) and probabilities (PCL), and present the results in Table~\ref{tab:different_feature_position}.
First, the traditional FCL can improve the performance of the baseline but the gain is limited.
Second, the average gain of our probabilistic contrastive learning over FCL is more than 5, which indicates the valuable effect of probabilistic contrastive learning.




\begin{table}
\centering

\resizebox{\linewidth}{!}{
\begin{tabular}{c|ccccccc|c}
\specialrule{.1em}{.05em}{.05em}
 Method & RC & RP & PC & CS & SP & RS & PR &Mean  \\
\hline
   Baseline & 71.4 & 70.0 & 72.6 & 62.7 & 68.2 & 64.3 & 77.9 & 69.5 \\ 
\hline
   + FCL & 72.5 & 71.6 & 73.1 & 66.4 & 70.2 & 64.5 & 80.8 & 71.3 \\ 
   + NTCL & 72.9 & 71.3 & 73.3 & 66.3 & 71.3 & 67.1 & 80.5 & 71.7 \\
   + LCL & 72.8 & 70.6 & 72.5 & 66.4 & 70.5 & 64.5 & 81.3 & 71.2 \\
   + Our PCL- & 75.1 & 74.4 & 76.2 & 70.3 & 73.5 & 69.9 & 82.5 & 74.6 \\ 
   + \textbf{Our PCL} & \textbf{78.1} & \textbf{76.5} & \textbf{78.6} & \textbf{72.5} & \textbf{75.6} & \textbf{72.5} & \textbf{84.6}  & \textbf{76.9} \\ 
\hline
\specialrule{.1em}{.05em}{.05em}
\end{tabular}}
\vspace{0.3 mm}
\caption{Ablation study on effect of different features on DomainNet under the setting of -shot and Resnet34.}
\label{tab:different_feature_position}

\end{table} 
\subsection{PCL v.s. FCL with Projection Head}
\label{app:LCL}
In SimCLR~\cite{chen2020simple}, the authors have proven that projection head is a very useful technique. It changes the feature of calculating the contrastive loss from feature+ normalization to projection head+ normalization. Inspired by this, CLDA~\cite{singh2021clda} uses the classifier as the projection head to design two contrastive learning losses and achieve better performance. In this section, we mainly verify whether the performance gain of our PCL comes from the application of projection head. To this end, we designed the following three types of projection heads:

(1) Following the SimCLR, we introduce an additional nonlinear transformation (NT) on the features, and then normalize the features after the NT to calculate the contrastive loss. We call it NT-Based Contrastive Learning (NTCL).

(2) We directly use the classifier as the projection head and named it Logits Contrastive Learning (LCL for short). Since LCL uses the output after the classifier to calculate the contrastive loss, it is one of the most direct methods to introduce class weight information. 

(3) We further generalize the projection head to classifier+softmax. For this setting, the feature of calculating the contrastive loss becomes classifier+softmax with  normalization. Essentially, it has only one more  normalization than our PCL, and thus we denote it PCL-. Like LCL, PCL- is also a way to introduce class weight information. However, it is worth noting that PCL- is not a natural extension of the projection head technique, since many current contrastive learning methods ~\cite{chen2020simple,dwibedi2021little,chen2021empirical,chen2021exploring} do not include softmax in the projection head. In this section, the projection head is extended to the form of classifier + softmax in order to verify such a question: when probability is used as a special feature, can the traditional contrastive learning paradigm (feature +  normalization) be as effective as PCL? 


Table~\ref{tab:different_feature_position} gives the experimental results and we obtain the following observations.
\textbf{First}, the above three projection heads all get lower performance than PCL, which indicates that the key reason for the gain of PCL is not from the use of the projection head.
\textbf{Second}, both LCL and PCL- are inferior to PCL, which shows that simply introducing class weight information cannot effectively enforce features to locate around class weights. Thus, it is necessary to deliberately design a contrastive learning loss to explicitly constrain the distance of feature and class weights.
\textbf{Third}, the results experimentally verify the motivation of PCL. The current contrastive learning methods~\cite{dwibedi2021little, chen2021empirical, chen2021exploring} all follow the standard paradigm of feature+ normalization. In particular, neither the perspective of the projection head nor the introduction of class weights can lead to PCL since there is no reason for them to adopt probability nor is there a reason to discard the  normalization widely used in contrastive learning.
In this work, however, our motivation is exactly that contrastive learning needs to reduce the deviation between the features and class weights for domain adaptation. Therefore, we can confidently point out that the features need to be replaced by probabilities and the  normalization should be removed  (in section~\ref{method_PCL}). 







\subsection{Cosine distance v.s. MSE distance}

In domain adaptation, there has been a work~\cite{french2017self} that exploits the similarity of the prediction space for consistency constraints, although it does not use the form of contrastive learning. Intuitively, PCL seems to just apply this idea to the contrastive learning loss (transfer feature space to prediction space). Then we need to answer an important question: \textit{Is PCL effective only because of the consistency constraint in the prediction space?}

From the previous analysis, it can be seen that the goal of PCL is to make the features close to the class weights by enforcing the probabilities to appear in the one-hot form, rather than to achieve the consistency constraint of the output space. It is just that we found that contrastive learning in the form of probability inner product (cosine distance) can achieve the goal.
To verify it, we replace the inner product in PCL with the MSE used in ~\cite{french2017self} to get PCL-MSE. Table~\ref{tab:SFCL_BCE} gives the results. It can be seen that PCL is better than PCL-MSE. This is because mse can only make the probability similar but not make the probability appear the one-hot form. This again proves the importance of the motivation of PCL, because this motivation ensures that PCL must use the cosine distance but not the MSE distance.


\subsection{The Importance of InfoNCE Loss}

\begin{table}

\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{c|ccccccc|c}
\specialrule{.1em}{.05em}{.05em}
 Method & RC & RP & PC & CS & SP & RS & PR &Mean  \\
\hline
   Baseline & 71.4 & 70.0 & 72.6 & 62.7 & 68.2 & 64.3 & 77.9 & 69.5 \\
\hline
+ FCL & 72.5 & 71.6 & 73.1 & 66.4 & 70.2 & 64.5 & 80.8 & 71.3 \\
   + SFCL & 72.7 & 72.4 & 73.2 & 66.5 & 70.8 & 65.5 & 81.2 & 71.8 \\
   + BCE & 73.3 & 73.0 & 74.7 & 66.7 & 71.9 & 67.5 & 80.4 & 72.5 \\
   + FCL+BCE & 73.9 & 73.1 & 74.1 & 66.5 & 71.5 & 67.3 & 81.4 & 72.5 \\
   + Our PCL-MSE & 76.6 & 75.3 & 76.2 & 69.7 & 74.2 & 70.5 & 83.8 & 75.1 \\
   + \textbf{PCL} & \textbf{78.1} & \textbf{76.5} & \textbf{78.6} & \textbf{72.5} & \textbf{75.6} & \textbf{72.5} & \textbf{84.6}  & \textbf{76.9} \\
\hline
\specialrule{.1em}{.05em}{.05em}
\end{tabular}}
\vspace{0.3 mm}
\caption{Comparison of different FCL improvement methods on DomainNet under the setting of -shot and Resnet34.}
\label{tab:SFCL_BCE}
\end{table}

In this section, we explore whether it is necessary to use the function form based on InfoNCE loss to approximate the probability to the one-hot form. In particular, we consider binary cross entropy loss (BCE) used in~\cite{zhong2021neighborhood}. For convenience, we redefine the probabilities  and  as  and .
Then the new loss function is defined as
\begin{small}

\end{small}where ,  , and  or . We use  for the quantitative analyses.

\begin{figure}
    \centering

        \centering
     \includegraphics[width=1.0\columnwidth]{images/tsne_RS_ECCV.pdf}

     \caption{The t-SNE visualization of learned features. We focus on the relationship between features and class weights on the RS task of DomainNet dataset with Resnet34 under the setting of -shot. Best viewed in color.}
     \label{fig:tsne}

\end{figure}
Observing Eq~(\ref{tab:BCE_eq}), BCE loss will maximize  when . Note that  and  are the probability. So  will take maximum if and only if  and  are equal and both of them have a one-hot form. Therefore, BCE loss can force the probability to present the one-hot form.
Based on the above analysis, we can use BCE to replace PCL. In addition, we also consider combining FCL and BCE to optimize the model.  Table~\ref{tab:SFCL_BCE} gives the results. It can be seen that although the BCE is better than FCL, it is still inferior to PCL. This indicates that the form of InfoNCE is better than the pair-wise form of BCE, and the conclusion is consistent with~\cite{wang2021understanding}.

\subsection{PCL v.s. SFCL}
\label{sec:False Negative}
In contrastive learning, there may be some negative samples that indeed belong to the same category as the query sample, which are called false negative samples.
Many methods~\cite{dwibedi2021little,khosla2020supervised,xu2020hierarchical} point out that false negative samples are harmful to contrastive learning.
In this section, we try to answer another important question: \textit{whether can we make feature contrastive learning work well by reducing the false negative samples without probabilistic contrastive learning? }

An appropriate way to address the false negative problem is to use supervised feature contrastive learning (SFCL)~\cite{khosla2020supervised}. Since the target domain data has no labels, we regard samples with a similarity greater than 0.95 to the query sample as false negative samples and remove them from the denominator. 


Table~\ref{tab:SFCL_BCE} shows the experimental comparison.
It can be seen that SFCL can indeed improve the performance of FCL.
However, compared with PCL, SFCL has a very limited improvement over FCL. Specifically, SFCL can learn better feature representations by alleviating the false negative problem, but it cannot solve the problem of the deviation between the features and class weights. The experimental results reveal that the deviation problem is more critical than the negative samples for domain adaptation. 

\subsection{Visualization Analysis}
Figure~\ref{fig:tsne} shows the relationship between the unlabeled features and the class weights for the three methods, including MME, MME+FCL, and MME+PCL.
Firstly, compared with MME, MME+FCL produces more compact feature clusters for the same category and more separate feature distributions for different categories. However, the learned class weights are deviated from the feature centers for both MME+FCL and MME.
Secondly, the class weights of MME+PCL are much closer to the feature centers than MME+FCL. It demonstrates that PCL is significantly effective in enforcing the features close to the class weights.

\section{Experiments}
In this section, we will verify the validity of the PCL on five different tasks. Four of these are classification, detection, and segmentation tasks in domain adaptation, and one is a semi-supervised learning task under conditions with little labeled data. For more experimental details and results, please refer to Section ~\ref{app:IMPME} and Section ~\ref{app:MoreResult}.

\subsection{UDA Semantic Segmentation}
\label{app:rudass}
\begin{table}

	\centering
	\renewcommand{\arraystretch}{1.2}
	\setlength{\tabcolsep}{8pt}
	\renewcommand{\arraystretch}{1.2}
	\resizebox{0.47\textwidth}{!}{
	\begin{tabular}{cccc}
		
		\toprule
		\multirow{2}*{Methods}  				& \multicolumn{1}{c}{GTA5}	&\multicolumn{2}{c}{SYNTHIA} 		\\
		\cline{2-4}
		\multicolumn{1}{c}{ } &mIoU (\%) &mIoU-13 (\%) &mIoU-16 (\%) \\
		\hline
		

		ProDA ( CVPR'21 )~\cite{zhang2021prototypical}& 53.7& -& -				\\
            DSP ( ACM MM'21 )~\cite{gao2021dsp}	& 55.0& 59.9& 51.0\\
CPSL( CVPR'22 )~\cite{li2022class}& 55.7& 61.7& 54.4				\\
            BAPA ( ICCV'21 )~\cite{liu2021bapa}	& 57.4& 61.2& 53.3\\
  		\hline
            ProDA-D ( CVPR'21 )~\cite{zhang2021prototypical} & 57.5& 62.0& 55.5				\\
            MFA-D (BMVC'21)~\cite{zhang2021multiple} & 58.2& 62.5& - \\
ProDA-D+CaCo ( CVPR'22 )~\cite{huang2022category}& 58.0& -& -				\\
            CPSL-D ( CVPR'22 )~\cite{li2022class}& \textbf{60.8}& 65.3& 57.9				\\
    		\hline
		BAPA					& 57.7& 60.1& 53.3				\\
		+ \textbf{Our PCL}					& 60.7& \textbf{68.2}& \textbf{60.3}	\\
		\bottomrule	
	\end{tabular}}
 \vspace{0.6 mm}
 	\caption{Result on UDA Semantic Segmentation. -D means to use an additional two-step distillation technique.  means our reimplementation. For more detailed experimental results, please refer to the supplementary material.}
\label{table:BAPA_ours}
\end{table}

\noindent\textbf{\textit{Setup}} We evaluate the performance of our methods on two standard UDA semantic segmentation tasks: GTA5~\cite{richter2016playing}Cityscapes~\cite{cordts2016cityscapes} and SYNTHIA~\cite{ros2016synthia}Cityscapes.
We take BAPA~\cite{liu2021bapa} with ResNet-101~\cite{ResNet1} as our baseline.

The current SOTA method of UDA semantic segmentation generally adopts the distillation technique for post-processing. The distillation makes the training process very complicated and requires some special training strategies, such as initialization with a more transferable SimCLRV2~\cite{chen2020big} model. Therefore, here we divide UDA semantic segmentation methods into simple non-distilled methods and complex distillation methods.

Table~\ref{table:BAPA_ours} gives the results. First, our method can achieve very significant gains on the baseline and outperforms all non-distilled methods by a large margin on SYNTHIA (6.5 for mIoU-13 and 5.9 of mIoU-16). Second, compared with the methods using distillation techniques, our method has only slightly lower performance than CPSL-D on GTA5 and outperforms CPSL-D by more than 2 on SYNTHIA. \textbf{Notably, the training cost of the distillation-based methods is particularly expensive. For example, CPSL-D requires 4*V100 and a training period of 11 days, while our method only needs 1*RTX3090 and a training period of about 5 days.} 












\subsection{UDA Classification}
\label{app:ruda}

\noindent\textbf{\textit{Setup}} We evaluate our PCL in the following two standard benchmarks: \textbf{Office-Home}~\cite{Office-HOME} and \textbf{VisDA-2017}~\cite{VisDA2017}. We take GVB-GD~\cite{cui2020gradually} with ResNet50~\cite{ResNet1} as our baseline. \begin{table}

\centering
\renewcommand{\arraystretch}{1.1}
\resizebox{\linewidth}{!}{
\begin{tabular}{cc|cc}
\hline
					Method & Acc & Method & Acc \\
					\hline
RADA ( ICCV'21 )~\cite{jin2021re} & 71.4   & ToAlign ( NeurIPS'21 ) ~\cite{wei2021toalign} & 72.0 \\
					SCDA ( ICCV'21 )~\cite{li2021semantic}  & 73.1 & FixBi  ( CVPR'21 )~\cite{na2021fixbi} & 72.7 \\
     SDAT  ( ICML'22 )~\cite{rangwani2022closer}  & 72.2 & BNM (CVPR'20)~\cite{cui2020towardsbnm} & 69.4 \\
     NWD ( CVPR'22 )~\cite{chen2022reusing} & 72.6 & CTS ( NeurIPS'21 )~\cite{liu2021cycle} & 73.0 \\
				    \hline
					 GVB* ( CVPR'20 )~\cite{cui2020gradually} & 70.3 &  GVB &  73.5 \\
      			 + MetaAlign ( CVPR'21 )~\cite{wei2021metaalign} & 71.3 &  - &  -\\
					+ \textbf{Our PCL} &  \textbf{72.3}  &   + \textbf{Our PCL}  & \textbf{74.5} \\
					\hline
					\specialrule{.1em}{.05em}{.05em}
				\end{tabular}}
    \vspace{0.3 mm}
    \caption{Average performance of 12 UDA tasks on Office-Home. * means our reimplementation. For more detailed experimental results, please refer to the supplementary material.}  
\label{tab:uda_officehome} 
\end{table} 

\begin{table}

\centering
\renewcommand{\arraystretch}{1.1}
\resizebox{\linewidth}{!}{
\begin{tabular}{cc|cc}
\specialrule{.1em}{.05em}{.05em}
					\hline
					Method & Acc & Method & Acc \\
					\hline
CST ( NeurIPS'21 )~\cite{liu2021cycle}  & 80.6 & SENTRY ( ICCV'21 )~\cite{prabhu2021sentry} & 76.7 \\
\hline
					 GVB* ( CVPR'21 )~\cite{cui2020gradually} & 75.0 &  GVB  &  80.4 \\
					+ \textbf{Our PCL} &  80.8 &   + \textbf{Our PCL}  & \textbf{82.5} \\
					\hline
					\specialrule{.1em}{.05em}{.05em}
				\end{tabular}}
    \vspace{0.3 mm}
    \caption{Accuracies(\%) of SyntheticReal on VisDA-2017 for UDAs. * means our reimplementation.}  
\label{tab:uda_visda}
\end{table} 

Table~\ref{tab:uda_officehome} and Table~\ref{tab:uda_visda} give the results.
In particular, inspired by some previous semi-supervised domain adaptation methods~\cite{li2021cdac,li2021ecacl}, we add FixMatch~\cite{fixmatch} to GVB and get the stronger GVB. We evaluate our proposed method by applying PCL to GVB and GVB.
From the results, it can be seen that our method can bring considerable gains both on GVB and GVB. In particular, with GVB as the baseline, PCL outperforms MetaAlign by , which  is a meta-optimization-based technique and involves more complicated operations than PCL. This further demonstrates the superiority of PCL.

\subsection{Semi-Supervised Domain Adaptation}
\label{app:rssda}


\begin{table}

	\centering
	\renewcommand{\arraystretch}{1.2}
	\setlength{\tabcolsep}{8pt}
	\renewcommand{\arraystretch}{1.2}
	\resizebox{0.47\textwidth}{!}{
	\begin{tabular}{cccc}
		
		\toprule
		\multirow{2}*{Methods}  				& \multicolumn{1}{c}{Office-Home}	&\multicolumn{2}{c}{DomainNet} 		\\
		\cline{2-4}
		\multicolumn{1}{c}{ } &3-shot  &1-shot &3-shot \\
		\hline
		

		CLDA ( NeurIPS'21 ) ~\cite{singh2021clda}& \textbf{75.5} & 71.9 & 75.3				\\
            MME ( ICCV'19 )~\cite{saito2019semi}	& 73.5 & 67.9 & 69.5\\
+ \textbf{Our PCL} & \textbf{75.5} & \textbf{73.5} & \textbf{76.9}				\\
  		\hline
            CDAC ( CVPR'21 )~\cite{li2021cdac}	& 74.8 & 73.6& 76.0\\
            ECACL-P  ( ICCV'21 )~\cite{li2021ecacl} & -& 72.8 & 76.4 \\
            MCL ( IJCAI'22 )~\cite{yan2022multi} & 77.1& 74.4 & 76.5
            \\
MME& 76.9& 72.9&76.1				\\
		+ \textbf{Our PCL}					& \textbf{78.1}& \textbf{75.1}& \textbf{78.2}	\\
		\bottomrule	
	\end{tabular}}
 \vspace{0.3 mm}
	\caption{Average performance on  \textit{DomainNet} (7 SSDA tasks) and \textit{Office-Home} (12 SSDA tasks).  means our reimplementation. For more detailed experimental results, please refer to the supplementary material.}
	\label{tab:ssda_domainnet_officehome}
\end{table}
\noindent\textbf{\textit{Setup}} We evaluate the effectiveness of our proposed approach on two SSDA image classification benchmarks, \textit{i.e.},  \textit{DomainNet}~\cite{peng2019moment} and \textit{Office-Home}.
We choose MME~\cite{saito2019semi} with ResNet34~\cite{ResNet1} as our baseline. In particular, inspired by CDAC~\cite{li2021cdac} and ECACL-P~\cite{li2021ecacl}, we add FixMatch~\cite{fixmatch} to MME to build a stronger MME.











Table~\ref{tab:ssda_domainnet_officehome} gives the results and we have the following observations.

\noindent1) PCL outperforms the methods without Fixmatch for most of settings. In particular, CLDA uses a classifier as the projection head with instance contrastive learning and class-wise contrastive learning. But our PCL can defeat CLDA, although PCL is only equipped with instance contrastive learning. For example, under the DomainNet, PCL achieves 73.5 (1-shot) and 76.9 (3-shot) while CLDA achieves 71.9 (1-shot) and 75.3 (3-shot). The results indicate the superiority of PCL over the projection head.

\noindent2) Both CDAC and ECACL-P use Fixmatch. In addition, CDAC proposes the AAC loss to improve MME loss, while ECACL-P designs the prototypical loss and triplet loss to further improve MME performance. It can be seen that PCL is close to CDAC and ECACL-P in performance even without Fixmatch, and once equipped with Fixmatch, our method achieves state-of-the-art for almost all settings. It proves that PCL outperforms the AAC loss and prototypical loss+triplet loss.


\subsection{UDA  Detection}
\label{app:DAOD}
\noindent\textbf{\textit{Setup}} We conduct an experiment on SIM10k~\cite{SIM10K}  Cityscapes~\cite{cordts2016cityscapes} scenes to verify whether our proposed method is effective for the object detection task.
In particular, we choose the RPA~\cite{zhang2021rpn} with Vgg16~\cite{simonyan2014very} as the baseline. In the experiment, we add PCL to the classification head to improve the classification results of the RPA model. 

Table~\ref{table:sim10k_city} gives the results. It can be seen that our method can still improve the performance of the baseline model on UDA detection.

\begin{table}

\centering
\renewcommand{\arraystretch}{1.1}
\resizebox{\linewidth}{!}{
\begin{tabular}{cc|cc}
\specialrule{.1em}{.05em}{.05em}
					\hline
					Method & AP & Method & AP \\
					\hline
MeGA-CDA  ( CVPR'21 )~\cite{VS_2021_CVPR}  & 44.8 & RPA ( CVPR'21 )~\cite{zhang2021rpn} & 45.3 \\
UMT ( CVPR'21 )~\cite{deng2021unbiased} & 43.1    &  + \textbf{Our PCL} &  \textbf{47.8} \\
\hline
					\specialrule{.1em}{.05em}{.05em}
				\end{tabular}}
    \vspace{0.3 mm}
    \caption{Detection performance (\%) on UDA detection task.  means our reimplementation.}

\label{table:sim10k_city}
\end{table}










\subsection{Semi-Supervised Learning}
\label{app:rssl}



\begin{table}
 
\centering
\renewcommand{\arraystretch}{1.1}
\resizebox{\linewidth}{!}{
\begin{tabular}{cc|cc}
\specialrule{.1em}{.05em}{.05em}
					\hline
					Method & 4-shot Acc & Method & 4-shot Acc \\
					\hline
Dash ( ICML'21 )~\cite{xu2021dash} & 55.240.96 & 	
SimMatch ( CVPR'22 )~\cite{zheng2022simmatch}  & 62.19  \\
					DP-SSL  ( NeurIPS'21 )~\cite{xu2021dp}  & 56.831.29 & NP-Match ( ICML'21 )~\cite{wang2022np} & 61.090.99 \\
					Freematch  ( ICLR'23 )~\cite{wang2022freematch}  & 62.020.42 & Softmatch ( ICLR'23 )~\cite{chen2023softmatch} & 62.900.77 \\					
				    \hline
					 FixMatch* ( NeurIPS'20 )~\cite{fixmatch} & 53.58 2.09 &  + \textbf{Our PCL} &  \textbf{57.622.52} \\
					CCSSL* ( CVPR'22 )~\cite{yang2022class} &  60.490.57  &   + \textbf{Our PCL}  & \textbf{62.95}1.39 \\
					 FlexMatch* ( NeurIPS'21 )~\cite{zhang2021flexmatch} & 61.78 1.17 &  + \textbf{Our PCL} &  \textbf{64.150.53} \\     
					\hline
					\specialrule{.1em}{.05em}{.05em}
				\end{tabular}}
    \vspace{0.3 mm}
    \caption{Accuracy of SSL for CIFAR-100 (400 labels). * means our reimplementation.} 
\label{table:ssl_cifar}
\end{table} 
In fact, the applicable scenarios of PCL are not limited to the domain adaptation.
As long as the features of unlabeled data cannot be clustered around the class weights, our method has good potential to improve the performance.
In this section, we consider the case where the source domain and target domain come from the same distribution, \textit{i.e.}, semi-supervised learning.
In particular, for the semi-supervised tasks, the unlabeled features will deviate from the class weight when the labeled data is very scarce.
Therefore, we consider the case where there are only 4 labeled samples per class. 

\noindent\textbf{\textit{Setup}} We conduct the experiments on CIFAR-100 \cite{krizhevsky2009learning}.We take FixMatch~\cite{fixmatch}, CCSSL~\cite{yang2022class} and Flexmatch~\cite{zhang2021flexmatch} as our baseline and use WRN-28-8~\cite{wrn} as backbone. 


The quantitative evaluation results on CIFAR-100 are reported in Table~\ref{table:ssl_cifar}.
Our method gets a great gain (+4.04) for FixMatch. On the stronger baseline model CCSSL and Flexmatch, our PCL still get a significant gain. The results well demonstrate the effectiveness of our method.














