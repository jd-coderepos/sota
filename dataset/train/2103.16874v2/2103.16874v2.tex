\begin{figure*}[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/model-overview.pdf}
    \vspace{-0.5cm}
    \caption{Overview of a \model. (a) First, given a reference image $I$ containing a target person, we predict the segmentation map $S$ and the pose map $P$, and utilize them to pre-process $I$ and $S$ as a clothing-agnostic person image $I_a$ and segmentation $S_a$. (b) Segmentation generator produces the synthetic segmentation $\hat{S}$ from $(S_a, P, c)$. (c) Geometric matching module deforms the clothing image $c$ according to the predicted clothing segmentation $\hat{S_c}$ extracted from $\hat{S}$. (d) Finally, \norm generator synthesizes the final output image $\hat{I}$ based on the outputs from the previous stages via our \norm normalization.}
    \vspace{-0.4cm}
    \label{fig:overview}
\end{figure*}

\section{Proposed Method}
\textbf{Model Overview.}
As described in Fig.~\ref{fig:overview}, given a reference image $I \in \mathbb{R}^{3 \times H \times W}$ of a person and a clothing image $c \in \mathbb{R}^{3 \times H \times W}$ ($H$ and $W$ denote the image height and width, respectively), the goal of \model is to generate a synthetic image $\hat{I} \in \mathbb{R}^{3 \times H \times W}$ of the same person wearing the target clothes $c$, where the pose and body shape of $I$ and the details of $c$ are preserved.
While training the model with $(I, c, \hat{I})$ triplets is straightforward, construction of such dataset is costly.
Instead, we use $(I, c, I)$ where the person in the reference image $I$ is already wearing $c$.

Since directly training on $(I, c, I)$ can harm the model's generalization ability at test time, we first compose a clothing-agnostic person representation that leaves out the clothing information in $I$ and use it as an input.
Our new clothing-agnostic person representation uses both the pose map and the segmentation map of the person to eliminate the clothing information in $I$ (Section~\ref{sec:person representation}).
The model generates the segmentation map from the clothing-agnostic person representation to help the generation of $\hat{I}$ (Section~\ref{sec:segmentation}).
We then deform $c$ to roughly align it to the human body (Section~\ref{sec:deformation}).
Lastly, we propose the ALIgnment-Aware Segment (ALIAS) normalization that removes the misleading information in the misaligned area after deforming $c$.
\norm generator fills the misaligned area with the clothing texture and maintains the clothing details (Section~\ref{sec:try-on synthesis}).

\subsection{Clothing-Agnostic Person Representation}\label{sec:person representation}
To train the model with pairs of $c$ and $I$ already wearing $c$, a person representation without the clothing information in $I$ has been utilized in the virtual try-on task.
Such representations have to satisfy the following conditions:
(1) the original clothing item to be replaced should be deleted;
(2) sufficient information to predict the pose and the body shape of the person should be maintained;
(3) the regions to be preserved (\eg, face and hands) should be kept to maintain the person's identity.

\textbf{Problems of Existing Person Representations.}
In order to maintain the person's shape, several approaches~\cite{han2018viton, wang2018toward, yu2019vtnfp} provide a coarse body shape mask as a cue to synthesize the image, but fail to reproduce the body parts elaborately (\eg, hands).
To tackle this issue, ACGPN~\cite{yang2020towards} employs the detailed body shape mask as the input, and the neural network attempts to discard the clothing information to be replaced.
However, since the body shape mask includes the shape of the clothing item, neither the coarse body shape mask nor the neural network could perfectly eliminate the clothing information.
As a result, the original clothing item that is not completely removed causes problems in the test phase.

\textbf{Clothing-Agnostic Person Representation.}
We propose a clothing-agnostic image $I_a$ and a clothing-agnostic segmentation map $S_a$ as inputs of each stage, which truly eliminate the shape of clothing item and preserve the body parts that need to be reproduced.
We first predict the segmentation map $S \in \mathbb{L}^{H \times W}$ and the pose map $P \in \mathbb{R}^{3 \times H \times W}$ of the image $I$ by utilizing the pre-trained networks~\cite{gong2018instance, cao2019openpose}, where $\mathbb{L}$ is a set of integers indicating the semantic labels.
The segmentation map $S$ is used to remove the clothing region to be replaced and preserve the rest of the image.
The pose map $P$ is utilized to remove the arms, but not the hands, as they are difficult to reproduce.
Based on $S$ and $P$, we generate the clothing-agnostic image $I_a$ and the clothing-agnostic segmentation map $S_a$, which allow the model to remove the original clothing information thoroughly, and preserve the rest of the image.
In addition, unlike other previous work, which adopts the pose heatmap with each channel corresponded to one keypoint, we concatenate $I_a$ or $S_a$ to the RGB pose map $P$ representing a skeletal structure that improves generation quality.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/model-generator.pdf}
    \vspace{-0.5cm}
    \caption{\norm generator.
    (a) The \norm generator is composed of a series of \norm residual blocks, along with upsampling layers.
    The input $(I_a, P, \mathcal{W}(c, \theta))$ is resized and injected into each layer of the generator.
    (b) A detailed view of a \norm residual block.
    Resized $(I_a, P, \mathcal{W}(c,\theta))$ is concatenated to $h^{i}$ after passing through a convolution layer.
    Each \norm normalization layer leverages resized $\hat{S}$ and $M_{misalign}$ to normalize the activation.
    }
    \vspace{-0.4cm}
    \label{fig:try-on synthesis}
\end{figure*}

\subsection{Segmentation Generation}\label{sec:segmentation}
Given the clothing-agnostic person representation $(S_a, P)$, and the target clothing item $c$, the segmentation generator $G_S$ predicts the segmentation map $\hat{S} \in \mathbb{L}^{H \times W}$ of the person in the reference image wearing $c$.
We train $G_S$ to learn the mapping between $S$ and $(S_a, P, c)$, in which the original clothing item information is completely removed.
As the architecture of $G_S$, we adopt U-Net~\cite{ronneberger2015u}, and the total loss $\mathcal{L}_S$ of the segmentation generator are written as 
\begin{equation}
    \mathcal{L}_{S} = \mathcal{L}_{cGAN} + \lambda_{CE} \mathcal{L}_{CE},
    \label{eq:segloss}
\end{equation}
where $\mathcal{L}_{CE}$ and $\mathcal{L}_{cGAN}$ denote the pixel-wise cross-entropy loss and conditional adversarial loss between $\hat{S}$ and $S$, respectively. $\lambda_{CE}$ is the hyperparameter corresponding to the relative importance between two losses.

\subsection{Clothing Image Deformation}\label{sec:deformation}
In this stage, we deform the target clothing item $c$ to align it with $\hat{S}_c$, which is the clothing area of $\hat{S}$.
We employ the geometric matching module proposed in CP-VTON~\cite{wang2018toward} with the clothing-agnostic person representation $(I_a, P)$ and $\hat{S}_c$ as inputs.
A correlation matrix between the features extracted from $(I_a, P)$ and $c$ is first calculated .
With the correlation matrix as an input, the regression network predicts the TPS transformation parameters $\theta \in \mathbb{R}^{2 \times 5 \times 5}$, and then $c$ is warped by $\theta$.
In the training phase, the model takes $S_c$ extracted from $S$ instead of $\hat{S}_c$.
The module is trained with the L1 loss between the warped clothes and the clothes $I_c$ that is extracted from $I$.
In addition, the second-order difference constraint~\cite{yang2020towards} is adopted to reduce obvious distortions in the warped clothing images from deformation.
The overall objective function to warp the clothes to fit the human body is written as
\begin{equation}
    \mathcal{L}_{warp} = ||I_{c} - \mathcal{W}(c, \theta)||_{1,1}
    + \lambda_{const} \mathcal{L}_{const},
\end{equation}
where $\mathcal{W}$ is the function that deforms $c$ using $\theta$, $\mathcal{L}_{const}$ is a second-order difference constraint, and $\lambda_{const}$ is the hyperparameter for $\mathcal{L}_{const}$.

\subsection{Try-On Synthesis via \norm Normalization}\label{sec:try-on synthesis}
We aim to generate the final synthetic image $\hat{I}$ based on the outputs from the previous stages.
Overall, we fuse the clothing-agnostic person representation $(I_a, P)$ and the warped clothing image $\mathcal{W}(c, \theta)$, guided by $\hat{S}$.
$(I_a, P, \mathcal{W}(c, \theta))$ is injected into each layer of the generator.
For $\hat{S}$, we propose a new conditional normalization method called the ALIgnment-Aware Segment (\norm) normalization.
\norm normalization enables the preservation of semantic information, and the removal of misleading information from the misaligned regions by leveraging $\hat{S}$ and the mask of these regions.

\begin{figure}[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/model-alias.pdf}
    \vspace{-0.5cm}
    \caption{\norm normalization.
    First, the activation is separately standardized according to the regions divided by $M_{misalign}$, which can be obtained from the difference between $\hat{S}_c$ and $M_{align}$.
    Next, $\hat{S}_{div}$ is convolved to create the modulation parameters $\gamma$ and $\beta$, and then the standardized activation is modulated with the parameters $\gamma$ and $\beta$.}
    \vspace{-0.4cm}
    \label{fig:alias norm}
\end{figure}

\begin{table*}[t!]
\centering
\small
\begin{tabular}{@{}p{0.10\textwidth} p{0.04\textwidth}<{\centering} p{0.04\textwidth}<{\centering} p{0.04\textwidth}<{\centering} | p{0.04\textwidth}<{\centering} p{0.04\textwidth}<{\centering} p{0.04\textwidth}<{\centering} | p{0.04\textwidth}<{\centering} p{0.04\textwidth}<{\centering} p{0.04\textwidth}<{\centering}@{}}
    \toprule
    & \multicolumn{3}{c}{256 $\times$ 192} & \multicolumn{3}{c}{512 $\times$ 384} & \multicolumn{3}{c}{1024 $\times$ 768} \\
    & SSIM$_{\uparrow}$ & LPIPS$_{\downarrow}$ & FID$_{\downarrow}$ & SSIM$_{\uparrow}$ & LPIPS$_{\downarrow}$ & FID$_{\downarrow}$ & SSIM$_{\uparrow}$ & LPIPS$_{\downarrow}$ & FID$_{\downarrow}$ \\
    \midrule
    CP-VTON & 0.739 & 0.159 & 56.23 & 0.791 & 0.141 & 31.96 & 0.786 & 0.158 & 43.28 \\
    ACGPN & 0.842 & 0.064 & \textbf{26.45} & 0.863 & 0.067 & 15.22 & 0.856 & 0.102 & 43.39 \\
    \midrule
    VITON-HD*  & - & - & - & - & - & - & 0.893 & 0.054 & 12.47 \\
    VITON-HD & \textbf{0.844} & \textbf{0.062} & 27.83 & \textbf{0.870} & \textbf{0.052} & \textbf{14.05} & \textbf{0.895} & \textbf{0.053} & \textbf{11.74} \\
    \bottomrule
    \end{tabular}
    \vspace{-0.2cm}
    \caption{Quantitative comparison with baselines across different resolutions. 
    VITON-HD* is a \model variant where the standardization in \norm normalization is replaced by channel-wise standardization as in the original instance normalization.
    For the SSIM, higher is better. For the LPIPS and the FID, lower is better.}
    \label{Table:resolution comparison}
    \vspace{-0.4cm}
\end{table*}

\textbf{Alignment-Aware Segment Normalization.}
Let us denote $h^i \in \mathbb{R}^{N \times C^i \times H^i \times W^i}$ as the activation of the $i$-th layer of a network for a batch of $N$ samples, where $H^i$, $W^i$, and $C^i$ indicate the height, width, and the number of channels of $h^i$, respectively. 
\norm normalization has two inputs:
(1) the synthetic segmentation map $\hat{S}$;
(2) the misalignment binary mask $M_{misalign} \in \mathbb{L}^{H \times W}$, which excludes the warped mask of the target clothing image $\mathcal{W}(M_c, \theta)$ from $\hat{S}_c$ ($M_c$ denotes the target clothing mask), \ie,
\begin{equation}
    M_{align} = \hat{S}_c \cap \mathcal{W}(M_c, \theta)
    \label{eq:alignment mask}
\end{equation}
\begin{equation}
    M_{misalign} = \hat{S}_c - M_{align}.
    \label{eq:misalignment mask}
\end{equation}

Fig.~\ref{fig:alias norm} illustrates the workflow of the \norm normalization.
We first obtain $M_{align}$ and $M_{misalign}$ from Eq.~\eqref{eq:alignment mask} and Eq.~\eqref{eq:misalignment mask}.
We define the modified version of $\hat{S}$ as $\hat{S}_{div}$, where $\hat{S}_c$ in $\hat{S}$ separates into $M_{align}$ and $M_{misalign}$.
\norm normalization standardizes the regions of $M_{misalign}$ and the other regions in $h^i$ separately, and then modulates the standardized activation using affine transformation parameters inferred from $\hat{S}_{div}$.
The activation value at site ($n \in N, k \in C^i, y \in H^i, x \in W^i$) is calculated by
\begin{equation}
    \gamma^i_{k,y,x}(\hat{S}_{div})
    {{h^i_{n,k,y,x} - \mu^{i,m}_{n,k}} \over \sigma^{i,m}_{n,k}}
    + \beta^i_{k,y,x}(\hat{S}_{div}), 
\end{equation}
where $h^i_{n,k,y,x}$ is the activation at the site before normalization and $\gamma^i_{k,y,x}$ and $\beta^i_{k,y,x}$ are the functions that convert $\hat{S}_{div}$ to modulation parameters of the normalization layer.
$\mu^{i,m}_{n,k}$ and $\sigma^{i,m}_{n,k}$ are the mean and standard deviation of the activation in sample $n$ and channel $k$.
$\mu^{i,m}_{n,k}$ and $\sigma^{i,m}_{n,k}$ are calculated by
\begin{equation}
    \mu^{i,m}_{n,k} = {1 \over |\Omega^{i,m}_n|}
    {\sum \limits_{(y,x) \in \Omega^{i,m}_n}} h^i_{n,k,y,x}
\end{equation}
\begin{equation}
    \sigma^{i,m}_{n,k} = \sqrt{
        {1 \over |\Omega^{i,m}_n|}
        {\sum \limits_{(y,x) \in \Omega^{i,m}_n}} (h^i_{n,k,y,x} - \mu^{i,m}_{n,k})^2
    }, 
\end{equation}
where $\Omega^{i,m}_n$ denotes the set of pixels in region $m$, which is $M_{misalign}$ or the other region, and $|\Omega^{i,m}_n|$ is the number of pixels in $\Omega^{i,m}_n$.
Similar to instance normalization~\cite{ulyanov2016instance}, the activation is standardized per channel.
However, \norm normalization divides the activation in channel $k$ into the activation in the misaligned region and the other region.

The rationale behind this strategy is to remove the misleading information in the misaligned regions.
Specifically, the misaligned regions in the warped clothing image match the background that is irrelevant to the clothing texture.
Performing a standardization separately on these regions leads to a removal of the background information that causes the artifacts in the final results.
In modulation, affine parameters inferred from the segmentation map modulate the standardized activation.
Due to injecting semantic information at each \norm normalization layer, the layout of the human-parsing map in the final result is preserved.

\textbf{\norm Generator.}
Fig.~\ref{fig:try-on synthesis} describes the overview of the \norm generator, which adopts the simplified architecture that discards the encoder part of an encoder-decoder network.
The generator employs a series of residual blocks (ResBlk) with upsampling layers.
Each \norm ResBlk consists of three convolutional layers and three \norm normalization layers. 
Due to the different resolutions that ResBlks operate at, we resize the inputs of the normalization layers, $\hat{S}$ and $M_{misalign}$, before injecting them into each layer.
Similarly, the input of the generator, $(I_a, P, \mathcal{W}(c, \theta))$, is resized to different resolutions.
Before each ResBlk, the resized inputs $(I_a, P, \mathcal{W}(c, \theta))$ are concatenated to the activation of the previous layer after passing through a convolution layer, and each ResBlk utilizes the concatenated inputs to refine the activation.
In this manner, the network performs the multi-scale refinement at a feature level that better preserves the clothing details than a single refinement at the pixel level.
We train the \norm generator with the conditional adversarial loss, the feature matching loss, and the perceptual loss following SPADE~\cite{park2019semantic} and pix2pixHD~\cite{wang2018high}.
Details of the model architecture, hyperparameters, and the loss function are described in the supplementary.
