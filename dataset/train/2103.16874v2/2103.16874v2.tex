\begin{figure*}[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/model-overview.pdf}
    \vspace{-0.5cm}
    \caption{Overview of a \model. (a) First, given a reference image  containing a target person, we predict the segmentation map  and the pose map , and utilize them to pre-process  and  as a clothing-agnostic person image  and segmentation . (b) Segmentation generator produces the synthetic segmentation  from . (c) Geometric matching module deforms the clothing image  according to the predicted clothing segmentation  extracted from . (d) Finally, \norm generator synthesizes the final output image  based on the outputs from the previous stages via our \norm normalization.}
    \vspace{-0.4cm}
    \label{fig:overview}
\end{figure*}

\section{Proposed Method}
\textbf{Model Overview.}
As described in Fig.~\ref{fig:overview}, given a reference image  of a person and a clothing image  ( and  denote the image height and width, respectively), the goal of \model is to generate a synthetic image  of the same person wearing the target clothes , where the pose and body shape of  and the details of  are preserved.
While training the model with  triplets is straightforward, construction of such dataset is costly.
Instead, we use  where the person in the reference image  is already wearing .

Since directly training on  can harm the model's generalization ability at test time, we first compose a clothing-agnostic person representation that leaves out the clothing information in  and use it as an input.
Our new clothing-agnostic person representation uses both the pose map and the segmentation map of the person to eliminate the clothing information in  (Section~\ref{sec:person representation}).
The model generates the segmentation map from the clothing-agnostic person representation to help the generation of  (Section~\ref{sec:segmentation}).
We then deform  to roughly align it to the human body (Section~\ref{sec:deformation}).
Lastly, we propose the ALIgnment-Aware Segment (ALIAS) normalization that removes the misleading information in the misaligned area after deforming .
\norm generator fills the misaligned area with the clothing texture and maintains the clothing details (Section~\ref{sec:try-on synthesis}).

\subsection{Clothing-Agnostic Person Representation}\label{sec:person representation}
To train the model with pairs of  and  already wearing , a person representation without the clothing information in  has been utilized in the virtual try-on task.
Such representations have to satisfy the following conditions:
(1) the original clothing item to be replaced should be deleted;
(2) sufficient information to predict the pose and the body shape of the person should be maintained;
(3) the regions to be preserved (\eg, face and hands) should be kept to maintain the person's identity.

\textbf{Problems of Existing Person Representations.}
In order to maintain the person's shape, several approaches~\cite{han2018viton, wang2018toward, yu2019vtnfp} provide a coarse body shape mask as a cue to synthesize the image, but fail to reproduce the body parts elaborately (\eg, hands).
To tackle this issue, ACGPN~\cite{yang2020towards} employs the detailed body shape mask as the input, and the neural network attempts to discard the clothing information to be replaced.
However, since the body shape mask includes the shape of the clothing item, neither the coarse body shape mask nor the neural network could perfectly eliminate the clothing information.
As a result, the original clothing item that is not completely removed causes problems in the test phase.

\textbf{Clothing-Agnostic Person Representation.}
We propose a clothing-agnostic image  and a clothing-agnostic segmentation map  as inputs of each stage, which truly eliminate the shape of clothing item and preserve the body parts that need to be reproduced.
We first predict the segmentation map  and the pose map  of the image  by utilizing the pre-trained networks~\cite{gong2018instance, cao2019openpose}, where  is a set of integers indicating the semantic labels.
The segmentation map  is used to remove the clothing region to be replaced and preserve the rest of the image.
The pose map  is utilized to remove the arms, but not the hands, as they are difficult to reproduce.
Based on  and , we generate the clothing-agnostic image  and the clothing-agnostic segmentation map , which allow the model to remove the original clothing information thoroughly, and preserve the rest of the image.
In addition, unlike other previous work, which adopts the pose heatmap with each channel corresponded to one keypoint, we concatenate  or  to the RGB pose map  representing a skeletal structure that improves generation quality.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/model-generator.pdf}
    \vspace{-0.5cm}
    \caption{\norm generator.
    (a) The \norm generator is composed of a series of \norm residual blocks, along with upsampling layers.
    The input  is resized and injected into each layer of the generator.
    (b) A detailed view of a \norm residual block.
    Resized  is concatenated to  after passing through a convolution layer.
    Each \norm normalization layer leverages resized  and  to normalize the activation.
    }
    \vspace{-0.4cm}
    \label{fig:try-on synthesis}
\end{figure*}

\subsection{Segmentation Generation}\label{sec:segmentation}
Given the clothing-agnostic person representation , and the target clothing item , the segmentation generator  predicts the segmentation map  of the person in the reference image wearing .
We train  to learn the mapping between  and , in which the original clothing item information is completely removed.
As the architecture of , we adopt U-Net~\cite{ronneberger2015u}, and the total loss  of the segmentation generator are written as 

where  and  denote the pixel-wise cross-entropy loss and conditional adversarial loss between  and , respectively.  is the hyperparameter corresponding to the relative importance between two losses.

\subsection{Clothing Image Deformation}\label{sec:deformation}
In this stage, we deform the target clothing item  to align it with , which is the clothing area of .
We employ the geometric matching module proposed in CP-VTON~\cite{wang2018toward} with the clothing-agnostic person representation  and  as inputs.
A correlation matrix between the features extracted from  and  is first calculated .
With the correlation matrix as an input, the regression network predicts the TPS transformation parameters , and then  is warped by .
In the training phase, the model takes  extracted from  instead of .
The module is trained with the L1 loss between the warped clothes and the clothes  that is extracted from .
In addition, the second-order difference constraint~\cite{yang2020towards} is adopted to reduce obvious distortions in the warped clothing images from deformation.
The overall objective function to warp the clothes to fit the human body is written as

where  is the function that deforms  using ,  is a second-order difference constraint, and  is the hyperparameter for .

\subsection{Try-On Synthesis via \norm Normalization}\label{sec:try-on synthesis}
We aim to generate the final synthetic image  based on the outputs from the previous stages.
Overall, we fuse the clothing-agnostic person representation  and the warped clothing image , guided by .
 is injected into each layer of the generator.
For , we propose a new conditional normalization method called the ALIgnment-Aware Segment (\norm) normalization.
\norm normalization enables the preservation of semantic information, and the removal of misleading information from the misaligned regions by leveraging  and the mask of these regions.

\begin{figure}[t!]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/model-alias.pdf}
    \vspace{-0.5cm}
    \caption{\norm normalization.
    First, the activation is separately standardized according to the regions divided by , which can be obtained from the difference between  and .
    Next,  is convolved to create the modulation parameters  and , and then the standardized activation is modulated with the parameters  and .}
    \vspace{-0.4cm}
    \label{fig:alias norm}
\end{figure}

\begin{table*}[t!]
\centering
\small
\begin{tabular}{@{}p{0.10\textwidth} p{0.04\textwidth}<{\centering} p{0.04\textwidth}<{\centering} p{0.04\textwidth}<{\centering} | p{0.04\textwidth}<{\centering} p{0.04\textwidth}<{\centering} p{0.04\textwidth}<{\centering} | p{0.04\textwidth}<{\centering} p{0.04\textwidth}<{\centering} p{0.04\textwidth}<{\centering}@{}}
    \toprule
    & \multicolumn{3}{c}{256  192} & \multicolumn{3}{c}{512  384} & \multicolumn{3}{c}{1024  768} \\
    & SSIM & LPIPS & FID & SSIM & LPIPS & FID & SSIM & LPIPS & FID \\
    \midrule
    CP-VTON & 0.739 & 0.159 & 56.23 & 0.791 & 0.141 & 31.96 & 0.786 & 0.158 & 43.28 \\
    ACGPN & 0.842 & 0.064 & \textbf{26.45} & 0.863 & 0.067 & 15.22 & 0.856 & 0.102 & 43.39 \\
    \midrule
    VITON-HD*  & - & - & - & - & - & - & 0.893 & 0.054 & 12.47 \\
    VITON-HD & \textbf{0.844} & \textbf{0.062} & 27.83 & \textbf{0.870} & \textbf{0.052} & \textbf{14.05} & \textbf{0.895} & \textbf{0.053} & \textbf{11.74} \\
    \bottomrule
    \end{tabular}
    \vspace{-0.2cm}
    \caption{Quantitative comparison with baselines across different resolutions. 
    VITON-HD* is a \model variant where the standardization in \norm normalization is replaced by channel-wise standardization as in the original instance normalization.
    For the SSIM, higher is better. For the LPIPS and the FID, lower is better.}
    \label{Table:resolution comparison}
    \vspace{-0.4cm}
\end{table*}

\textbf{Alignment-Aware Segment Normalization.}
Let us denote  as the activation of the -th layer of a network for a batch of  samples, where , , and  indicate the height, width, and the number of channels of , respectively. 
\norm normalization has two inputs:
(1) the synthetic segmentation map ;
(2) the misalignment binary mask , which excludes the warped mask of the target clothing image  from  ( denotes the target clothing mask), \ie,



Fig.~\ref{fig:alias norm} illustrates the workflow of the \norm normalization.
We first obtain  and  from Eq.~\eqref{eq:alignment mask} and Eq.~\eqref{eq:misalignment mask}.
We define the modified version of  as , where  in  separates into  and .
\norm normalization standardizes the regions of  and the other regions in  separately, and then modulates the standardized activation using affine transformation parameters inferred from .
The activation value at site () is calculated by

where  is the activation at the site before normalization and  and  are the functions that convert  to modulation parameters of the normalization layer.
 and  are the mean and standard deviation of the activation in sample  and channel .
 and  are calculated by


where  denotes the set of pixels in region , which is  or the other region, and  is the number of pixels in .
Similar to instance normalization~\cite{ulyanov2016instance}, the activation is standardized per channel.
However, \norm normalization divides the activation in channel  into the activation in the misaligned region and the other region.

The rationale behind this strategy is to remove the misleading information in the misaligned regions.
Specifically, the misaligned regions in the warped clothing image match the background that is irrelevant to the clothing texture.
Performing a standardization separately on these regions leads to a removal of the background information that causes the artifacts in the final results.
In modulation, affine parameters inferred from the segmentation map modulate the standardized activation.
Due to injecting semantic information at each \norm normalization layer, the layout of the human-parsing map in the final result is preserved.

\textbf{\norm Generator.}
Fig.~\ref{fig:try-on synthesis} describes the overview of the \norm generator, which adopts the simplified architecture that discards the encoder part of an encoder-decoder network.
The generator employs a series of residual blocks (ResBlk) with upsampling layers.
Each \norm ResBlk consists of three convolutional layers and three \norm normalization layers. 
Due to the different resolutions that ResBlks operate at, we resize the inputs of the normalization layers,  and , before injecting them into each layer.
Similarly, the input of the generator, , is resized to different resolutions.
Before each ResBlk, the resized inputs  are concatenated to the activation of the previous layer after passing through a convolution layer, and each ResBlk utilizes the concatenated inputs to refine the activation.
In this manner, the network performs the multi-scale refinement at a feature level that better preserves the clothing details than a single refinement at the pixel level.
We train the \norm generator with the conditional adversarial loss, the feature matching loss, and the perceptual loss following SPADE~\cite{park2019semantic} and pix2pixHD~\cite{wang2018high}.
Details of the model architecture, hyperparameters, and the loss function are described in the supplementary.
