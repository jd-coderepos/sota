\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv} 
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{algorithmic}
\usepackage{breqn}
\usepackage{booktabs}
\usepackage{fixltx2e} \usepackage{multirow}
\usepackage{siunitx}
  
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy
\begin{document}

\title{Semi-Supervised Semantic Segmentation with \\ Pixel-Level Contrastive Learning from a Class-wise  Memory Bank}


\author{Inigo Alonso\\
University of Zaragoza\\
{\tt\small inigo@unizar.es}

\and
Alberto Sabater\\
University of Zaragoza\\
{\tt\small asabater@unizar.es}

\and
David Ferstl\\
Magic Leap\\
{\tt\small dferstl@magicleap.com}

\and
Luis Montesano\\
University of Zaragoza\\
{\tt\small montesano@unizar.es}

\and
Ana C. Murillo\\
University of Zaragoza\\
{\tt\small acm@unizar.es}

}




\maketitle
\ificcvfinal\thispagestyle{empty}\fi



\newif\ifdraft
\draftfalse





\ificcvfinal
 \newcommand{\INIGO}[1]{}
 \newcommand{\inigo}[1]{}
 \newcommand{\DF}[1]{}
 \newcommand{\df}[1]{}
 \newcommand{\ANA}[1]{}
 \newcommand{\ana}[1]{}
 \newcommand{\LUIS}[1]{}
 \newcommand{\luis}[1]{}
\else
 \newcommand{\INIGO}[1]{{\color{orange}{\bf Inigo: #1}}}
 \newcommand{\inigo}[1]{{\color{orange} #1}}
 \newcommand{\DF}[1]{{\color{cyan}{\bf David: #1}}}
 \newcommand{\df}[1]{{\color{cyan} #1}}
 \newcommand{\ANA}[1]{{\color{purple}{\bf Ana: #1}}}
 \newcommand{\ana}[1]{{\color{purple} #1}}
 \newcommand{\LUIS}[1]{{\color{blue}{\bf Luis: #1}}}
 \newcommand{\luis}[1]{{\color{blue} #1}}
\fi

\newcommand{\comment}[1]{}
\newcommand{\parag}[1]{\paragraph{#1}}
\renewcommand{\floatpagefraction}{.99}

\newcommand{\cF}{\mathcal{F}}

\newcommand{\bA}{\mathbf{A}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bM}{\mathbf{M}}
\newcommand{\bO}{\mathbf{O}}

\newcommand{\real}{\mathbb{R}}



\newcommand{\radius}{\mathbf{r}}


\newcommand{\OURS}[0]{\textbf{OURS}}

\newcommand\norm[1]{\left\lVert#1\right\rVert}

\newcommand{\colvecTwo}[2]{\ensuremath{
		\begin{bmatrix}{#1}	\\	{#2}	\end{bmatrix}
}}
\newcommand{\colvec}[3]{\ensuremath{
		\begin{bmatrix}{#1}	\\	{#2}	\\	{#3} \end{bmatrix}
}}
\newcommand{\colvecFour}[4]{\ensuremath{
		\begin{bmatrix}{#1}	\\	{#2}	\\	{#3} \\	{#4}	\end{bmatrix}
}}

\newcommand{\rowvecTwo}[2]{\ensuremath{
		\begin{bmatrix}{#1}	&	{#2}	\end{bmatrix}
}}
\newcommand{\rowvec}[3]{\ensuremath{
		\begin{bmatrix}{#1} &	{#2}	&	{#3} \end{bmatrix}
}}
\newcommand{\rowvecFour}[4]{\ensuremath{
		\begin{bmatrix}{#1}	&	{#2}	&	{#3} &	{#4}	\end{bmatrix}
}}

\newcommand{\fig}[1]{figure~\ref{#1}}
\newcommand{\tab}[1]{table~\ref{#1}}
\newcommand{\Fig}[1]{Figure~\ref{#1}}
\newcommand{\Sec}[1]{Section~\ref{#1}}
\newcommand{\Tab}[1]{Table~\ref{#1}}
\newcommand{\equ}[1]{(Equation~\ref{#1})}


 
\begin{abstract} 

This work presents a novel approach for semi-supervised semantic segmentation, i.e., per-pixel classification problem assuming that only a small set of the available data is labeled. 
We propose a novel representation learning module based on contrastive learning. This module enforces the segmentation network to yield similar pixel-level feature representations for same-class samples across the whole dataset. 
To achieve this, we maintain a memory bank continuously updated with feature vectors from labeled data. These features are selected based on their quality and relevance for the contrastive learning.
In an end-to-end training, the features from both labeled and unlabeled data are optimized to be similar to same-class samples from the memory bank. 
Our approach outperforms the current state-of-the-art for semi-supervised semantic segmentation and semi-supervised domain adaptation on well-known public benchmarks, with larger improvements on the most challenging scenarios, i.e., less available labeled data.
\end{abstract}


\section{Introduction}

The goal of semantic segmentation consists in assigning a semantic class label to each pixel in an image. It is an essential computer vision task for semantic scene understanding that plays a relevant role in many applications such as medical imaging~\cite{ronneberger2015u} or autonomous driving~\cite{badrinarayanan2017segnet}. 

As for many other computer vision tasks, deep convolutional neural networks have shown significant improvements in semantic segmentation~\cite{badrinarayanan2017segnet, jegou2017one, alonso2020MininetV2}. All these examples follow supervised learning approaches, which require a large set of annotated data to be able to generalize well. However, the availability of labeled data is a common bottleneck in supervised learning, especially for tasks such as semantic segmentation, which require tedious and expensive per-pixel annotations. 

\begin{figure}[!tb]
\centering
\includegraphics[width=1\linewidth]{introduction7.png}
\caption{
\textbf{Proposed contrastive learning module overview.} At each training iteration, the teacher network  updates the feature memory bank with a subset of selected features from labeled samples.
Then, the student network  extracts features  from both labeled and unlabeled samples, 
which are optimized to be similar to same-class features from the memory bank 
\scriptsize \textbigcircle \small. 
}

\label{fig:introduction}
\end{figure} 
 

Semi-supervised learning assumes that only a small subset of the available data is labeled. It tackles this limited labeled data issue by extracting knowledge from unlabeled samples. 
Semi-supervised learning has been applied to a wide range of applications~\cite{van2020survey}, including semantic segmentation~\cite{french2019semi, hung2018adversarial, mittal2019semi}.
Previous semi-supervised segmentation works are mostly based on per-sample entropy minimization~\cite{hung2018adversarial, lee2013pseudo, olsson2021classmix} and per-sample consistency regularization~\cite{french2019semi, tarvainen2017mean, olsson2021classmix}. These segmentation methods do not enforce any type of structure on the learned features to increase inter-class separability across the whole dataset.


Our hypothesis is that overcoming this limitation can lead to better feature learning. In particular, we expect to learn better from the unlabeled data, which is critical when the amount of available labeled data is low.


 
This work presents a novel approach for semi-supervised semantic segmentation.
Our approach follows a teacher-student scheme whose main component is a novel representation learning module (see \Fig{fig:introduction}). This module is based on contrastive learning~\cite{hadsell2006dimensionality} and enforces the class-separability of pixel-level features. To achieve this, the teacher network produces feature candidates, only from labeled data, to be stored in a memory bank. Meanwhile, the student network learns to produce similar class-wise features from both labeled and unlabeled data.
The features introduced in the memory bank are selected based on their quality and on their learned relevance for the contrastive optimization. In addition to increased inter-class separability, the module enforces the alignment of unlabeled and labeled data (memory bank) in the feature space, which is another unexploited idea in semi-supervised semantic segmentation. 


The effectiveness of the proposed approach is demonstrated on well-known benchmarks for semi-supervised semantic segmentation, reaching the state-of-the-art on different set-ups.
Additionally, our approach can naturally tackle the semi-supervised domain adaptation task, also obtaining state-of-the-art results. 
In all cases, the improvements upon comparable methods increase with the percentage of unlabeled data. 
The detailed ablation study performed shows the significance of the different components of the proposed approach. Code is available as supplementary material and will be made publicly available upon acceptance.





 

\section{Related Work}
This section summarizes relevant related work for contrastive learning and semi-supervised learning, with particular emphasis on work related to semantic segmentation.


\subsection{Contrastive Learning.}
The core idea of contrastive learning~\cite{hadsell2006dimensionality} is to create positive and negative pairs from data, to attract the positive and repulse the negative pairs in the feature space. However, recent works~\cite{chen2020exploring, grill2020bootstrap, huang2021self} have shown similar level of performance using positive pairs only. The main difference in current methods is how to obtain these pairs: by using a memory bank~\cite{wu2018unsupervised}, by using a momentum model~\cite{chen2020improved} or directly from the same batch~\cite{chen2020simple}.
Contrastive learning has been recently popularized for self-supervised representation learning~\cite{chen2020improved, grill2020bootstrap, wu2018unsupervised, ye2019unsupervised}.
As for semantic segmentation, contrastive learning has been mainly used as pre-training~\cite{wang2020dense, xie2020pointcontrast, xie2020propagate}.
Very recently, Wang \etal~\cite{wang2021exploring} have shown improvements in supervised scenarios applying contrastive learning in a pixel and region level for same-class supervised samples. Van \etal~\cite{van2021unsupervised} have shown the advantages of contrastive learning in unsupervised set-ups, applying it between features from different saliency masks.

In this work, we propose to use contrastive learning to boost the performance in semi-supervised semantic segmentation tasks.
Differently from previous works, our contrastive module aligns features from both labeled and unlabeled data to high-quality features from all over the labeled set that are stored in a memory bank. We follow the positive-only research branch for computational efficiency.

\subsection{Semi-Supervised Learning}
This section discusses the two most common strategies for semi-supervised learning, pseudo-labeling and consistency regularization, as well as the application of semi-supervised learning to semantic segmentation. 

\paragraph{Pseudo-Labeling.}
Pseudo-labeling leverages the idea of creating artificial labels for unlabeled data~\cite{mclachlan1975iterative, scudder1965probability} by keeping the most likely predicted class by an existing model~\cite{lee2013pseudo}.
The use of pseudo-labels is motivated by entropy minimization~\cite{grandvalet2004semi}, encouraging the network to output highly confident probabilities on unlabeled data.
Both pseudo-labeling and direct entropy minimization methods are commonly used in semi-supervised scenarios~\cite{feng2020semiCBC, kalluri2019universal, sohn2020fixmatch, olsson2021classmix} showing great performance.
Our approach makes use of both pseudo-labels and direct entropy minimization.

\paragraph{Consistency Regularization.}
Consistency regularization relies on the assumption that the model should be invariant to perturbations, \eg data augmentation, made to the same image. This regularization is commonly applied by using two different methods: distribution alignment~\cite{berthelot2019mixmatch, sajjadi2016regularization, tarvainenweight}, or augmentation anchoring~\cite{sohn2020fixmatch}.
While the distribution alignment enforces the predicted class distributions of perturbed images to be the same as the non-perturbed image class distribution, the augmentation anchoring forces the perturbed images to be classified as the non-perturbed image.
While distribution alignment enforces the perturbed and non-perturbed to have the same class distribution, augmentation anchoring enforces them to have the same semantic label. 
To produce high-quality non-perturbed class distribution or prediction on unlabeled data, the Mean Teacher method~\cite{tarvainen2017mean}, proposes a teacher-student scheme where the teacher network is an exponential moving average (EMA) of model parameters, producing more robust predictions.

In this work, we apply the anchoring augmentation strategy and use an EMA model for computing the pseudo-labels.

\subsection{Semi-Supervised Semantic Segmentation.}
Semi-supervised learning addresses the problem of the high annotation cost by assuming that only a small subset of the available data is labeled, while the rest remains unlabeled. One common approach for this task is to make use of Generative Adversarial Networks (GANs)~\cite{goodfellow2014generative}. Hung \etal~\cite{hung2018adversarial} propose to train the discriminator to distinguish between confidence maps from labeled and unlabeled data predictions. Mittal \etal~\cite{mittal2019semi} make use of a two-branch approach, one branch enforcing low entropy predictions using a GAN approach and another branch for removing false-positive predictions using a Mean Teacher method~\cite{tarvainenweight}.
A similar idea was proposed by Feng \etal~\cite{feng2020semi}, a recent work that introduces Dynamic Mutual Training (DMT). DMT uses two segmentation models and the model's disagreement is used to re-weight the loss. DMT method also followed the multi-stage training protocol from CBC~\cite{feng2020semiCBC}, where pseudo-labels are generated in an offline curriculum fashion.
Other works are based on data augmentation methods for consistency regularization. French \etal~\cite{french2019semi} focus on applying CutOut~\cite{devries2017cutout} and CutMix~\cite{yun2019cutmix}, while Olsson \etal~\cite{olsson2021classmix} propose a data augmentation technique specific for semantic segmentation.

Differently from previous work, we propose a novel feature learning module that shows the benefits of incorporating contrastive learning in a semi-supervised scenario. 







\section{Method}
Semi-supervised semantic segmentation consists in a per-pixel classification task where two different sources of data are available: a few fully-labeled samples , where  are the training images and  their corresponding per-pixel annotations, and a large set of unlabeled samples .

\begin{figure}[!tb]
\centering
\includegraphics[width=1\linewidth]{self-supervised5.png}
\caption{
\textbf{Supervised and self-supervised optimization.} The student network  is optimized by the supervised loss () for labeled data (). For unlabeled data , the teacher network  computes the pseudo-labels  that are later used for optimizing the  pseudo-labels loss () for pairs of augmented samples and pseudo-labels (, ). Direct entropy minimization () is also applied on predictions from .} 
\label{fig:self-training}
\end{figure}



To tackle this task, we propose to use a teacher-student scheme. The teacher network  creates robust pseudo-labels from unlabeled samples and memory bank entries from labeled samples to teach the student network  to improve its segmentation performance.

\paragraph{Teacher-student scheme.} The learned weights  of the student network  are optimized using the following loss function:

The  is the loss for supervised learning on labeled samples (\Sec{sec:supervised}).  and  tackle pseudo-labels (\Sec{sec:pseudolabels}) and entropy minimization (\Sec{sec:entropy}) techniques, respectively, where the pseudo-labels are generated by the teacher segmentation network . Finally,  is our proposed contrastive learning loss (\Sec{sec:contrastive}).

Weights  of the teacher network  are an exponential moving average of weights  of the student network  with a decay rate . The teacher model provides more accurate and robust predictions \cite{tarvainen2017mean}.
Thus, at every training step, the teacher network  is not optimized by a gradient descent but updated as follows:

 
\begin{figure*}[!tb]
\centering
\includegraphics[width=1\linewidth]{contrastive_detail_new.png}
\caption{
\textbf{Contrastive learning optimization.} At every iteration, features are extracted by  from labeled samples (see right part). These features are projected, filtered by their quality, and then, ranked to finally only store the highest-quality features into the memory bank.
Concurrently, feature vectors from input samples extracted by  are fed to the projection and prediction heads (see left part). Then, feature vectors are passed to a self-attention module in a class-wise fashion, getting a per-sample weight. Finally, input feature vectors are enforced to be similar to same-class features from the memory bank.}
\label{fig:contrastive}
\end{figure*}

\subsection{Supervised Segmentation: }
\label{sec:supervised}
Our supervised semantic segmentation optimization, applied to the labeled data , follows the standard optimization with the weighted cross-entropy loss. Let  be the weighted cross-entropy loss between two lists of  per-pixel class probability distributions :


\noindent where  is the number of classes to classify,  is the number of elements, \ie pixels in ,  is a per-class weight, and,  is a per-pixel weight. Specific values of  and  are detailed in \Sec{sec:implementation}. The supervised loss (see top part of \Fig{fig:self-training}) is defined as follows:


\noindent where  is a weak augmentation of  (see \Sec{sec:implementation} for augmentation details). 

\subsection{Learning from Pseudo-labels: }
\label{sec:pseudolabels}

The key to the success of semi-supervised learning is to learn from unlabeled data. One idea our approach exploits is to learn from pseudo-labels.
In our case, pseudo-labels are generated by the teacher network  (see \Fig{fig:self-training}). 
For every unlabeled sample , the pseudo-labels  are computed following this equation:
 
\noindent where  predicts a class probability distribution. 
Note that pseudo-label generation is performed in an online fashion at each training iteration.

Consistency regularization is introduced by using augmentation anchoring, \ie computing different data augmentation for each sample  on the same batch, which helps the model to converge to a better solution~\cite{sohn2020fixmatch}. 
The pseudo-labels loss for unlabeled data  is calculated by the cross-entropy:

where  is a strong augmentation of  and  is the number of augmentations we apply to sample  (see \Sec{sec:implementation} for augmentation details). 

\subsection{Direct Entropy Minimization: }
\label{sec:entropy}
Direct entropy minimization is applied on the class distributions predicted by the student network from unlabeled samples  as a regularization loss: 

where  is the number of classes to classify,  is the number of pixels and  is the number of augmentations.

\subsection{Contrastive Learning: }
\label{sec:contrastive}

\Fig{fig:contrastive} illustrates our proposed contrastive optimization. A memory bank is filled with high-quality feature vectors from the teacher network  (right part of \Fig{fig:contrastive}). Concurrently, the student network  extracts feature vectors from either  or . In a per-class fashion, every feature is passed through a simple self-attention module that serves as per-feature weighting in the contrastive loss. Finally, the contrastive loss enforces the weighted feature vectors from the student to be similar to feature vectors from the memory bank. 
As the memory bank contains high-quality features from all labeled samples, the contrastive loss helps to create a better class separation in the feature space across the whole dataset as well as aligning the unlabeled data distribution with the labeled data distribution. 

\paragraph{Contrastive Learning Optimization.}
Let  be the student network without the classification layer and  a training sample that is either from the labeled  or unlabeled set . The first step is to extract all feature vectors: . The feature vectors  are then fed to a projection head, , and a prediction head, , following~\cite{grill2020bootstrap}, where  and  are two different Multi-Layer Perceptrons (MLPs).
Next,  is grouped by the different semantic classes in . 

Let  be the set of prediction vectors from  of a specific class . Let  be the set of projection vectors of class  obtained by the teacher network,  from the labeled examples stored in the memory bank, \ie memory entries. 
 

Next, we learn which feature vectors ( and ) are beneficial for the contrastive learning task, by assigning per-feature learned weights \equ{eq:re-weighting} that will serve as a weighting factor \equ{eq:weighted-distance} for the contrastive learning loss function \equ{eq:contrastive-loss}.
These per-feature weights are computed using class-specific attention modules  (see \Sec{sec:implementation} for further details) that generate a single value () for every  and  feature. 
Following~\cite{sun2020learning} we L1 normalize these weights to prevent converging to the trivial all-zeros solution. For the prediction vectors  case, the weights  are then computed as follows: 

Equation~\ref{eq:re-weighting} 
is also used to compute , but using  and  instead of  and . 

The contrastive loss is computed to attract prediction vectors  to be similar to projection vectors from the memory bank . For that, we use the cosine similarity as the similarity measure : 

where, the weighted distance between predictions and memory bank entry is computed by: 

and finally, our contrastive loss is computed as follows: 





\paragraph{Memory Bank.}
The memory bank is the data structure that maintains the target feature vectors ,  for each class , used in the contrastive loss. In our case, it contains only high-quality pixel-level feature vectors from labeled data.
 
As shown in \Fig{fig:contrastive}, the memory bank is updated on every training iteration with a subset of  generated by the teacher network. To select what subset of  is included in the memory bank, we first perform a Feature Quality Filter (FQF), where we only keep features that lead to an accurate prediction when the classification layer is applied, , having confidence higher than a threshold, . 
The remaining  are grouped by classes . Finally, instead of picking randomly a subset of every  to update the memory bank, we make use of the class-specific attention modules . We get ranking scores  to sort  and we update the memory bank only with the top-K highest-scoring vectors. The memory bank is a First In First Out (FIFO) queue per class for computation and time efficiency. This way it maintains recent high-quality feature vectors in a very efficient fashion computation-wise and time-wise. Detailed information about the hyper-parameters is included in \Sec{sec:implementation}.



 
\section{Experiments}
\label{sec:experiments}
This section describes the datasets and implementation details used in the evaluation of the presented work. It also contains the comparison of our method with the state-of-the-art on different benchmarks for semi-supervised semantic segmentation, including a semi-supervised domain adaptation set-up, and a detailed ablation study. 

\subsection{Datasets}
\begin{itemize}
    \item Cityscapes~\cite{cordts2016cityscapes}. It is a real urban scene dataset composed of  training and  validation samples, with  semantic classes. \item PASCAL VOC 2012~\cite{everingham2010pascal}. It is a natural scenes dataset with  semantic classes. The dataset has  and  images for training and validation respectively.
    
    \item GTA5~\cite{richter2016playing}. It is a synthetic dataset captured from a video game with realistic urban-like scenarios with  images in total. The original dataset provides  different categories but, following~\cite{wang2020alleviating}, we only use the  classes that are shared with Cityscapes.
\end{itemize}
 



\subsection{Implementation details}
\label{sec:implementation}
\paragraph{Architecture.} We use DeepLab networks~\cite{chen2017deeplab} in our experiments. For the ablation study and most benchmarking experiments, DeepLabv2 with a ResNet-101 backbone is used for a fair comparison (\ie similar settings) to previous works~\cite{olsson2021classmix, feng2020semiCBC, hung2018adversarial, mittal2019semi}. DeepLabv3+ with Resnet50 backbone is also used to equal comparison with~\cite{mendel2020semi}. For the teacher network , we set  in \equ{eq:ema}.

The prediction and projection heads follow~\cite{grill2020bootstrap}: Linear  BatchNorm~\cite{ioffe2015batch}  Relu~\cite{nair2010rectified}  Linear, with a hidden and output dimension of .
The proposed class-specific attention modules follow a similar architecture: Linear  BatchNorm  LeakyRelu~\cite{maas2013rectifier}  Linear  Sigmoid, with a hidden and output dimension of 256 and 1 respectively. We use  attention modules since they are used in a class-wise fashion. In particular, two modules per class are used because we have different modules for projection or prediction feature vectors.

Following previous works \cite{olsson2021classmix, tarvainen2017mean}, the segmentation is performed with the student network  in the experimental validation.  

\vspace{5mm}
\paragraph{Optimization.}
For all experiments, we train for 80K iterations using the SGD optimizer with a momentum of . The learning rate is set to  for DeepLabv2 and  for DeepLabv3+ with a poly learning rate schedule. For the Cityscapes and GTA5 datasets, we use a crop size of  and batch sizes of 5 and 7 for Deeplabv2 and Deeplabv3+, respectively. For Pascal VOC, we use a crop size of  and batch sizes of 14 and 20 for Deeplabv2 and Deeplabv3+, respectively.
Cityscapes images are downsampled to  before cropping when Deeplabv2 is used for a fair comparison with previous works~\cite{olsson2021classmix, feng2020semiCBC, hung2018adversarial, mittal2019semi}.
The different loss weights in \equ{eq:approach} are set as follows for all experiments: , , , .
An exception is made for the first 2K training iterations where  and  to make sure predictions have some quality before being taken into account.
Regarding the per-pixel weights () from  in \equ{eq:crossentropy}, we set it to 1 for . For , we follow~\cite{feng2020semiCBC} weighting each pixel with its corresponding pseudo-label confidence with a sharpening operation, , where we set .
As for the per-class weights  in \equ{eq:crossentropy}, we perform a class balancing for the Cityscapes and GTA5 datasets by setting  with  being the frequency of class  and  the median of all class frequencies. 
In semi-supervised settings the amount of labels, , is usually small. 
For the Pascal VOC we set  as the class balancing does not have a beneficial effect.


\vspace{5mm}

\paragraph{Other details.} 
DeepLab's output resolution is  lower than the input resolution. For feature comparison during training, we keep the output resolution and downsample the labels reducing memory requirements and computation.

The memory bank size is fixed to  vectors per class (see \Sec{sec:ablation} for more details). The confidence threshold  for accepting features is set to . The number of vectors added to the memory bank at each iteration, for each image, and for each class is set as , where  is the number of labeled samples.
 
A single NVIDIA Tesla V100 GPU is used for all experiments. All our reported results are the mean of three different runs with different labeled/unlabeled data splits.
\vspace{5mm}

\paragraph{Data augmentation.}
In our work, we use two different augmentation set-ups, a weak augmentation for labeled samples and a strong augmentation set-up for unlabeled samples, see \Tab{tab:transformation_distributions} for configuration details.
For the augmentation anchoring \equ{eq:self-supervised}, we set  as the number of augmentations for the same sample. 

\begin{table}[ht]
    \caption{Strong and weak data augmentation set-ups}
    \centering
    \begin{tabular}{l c l} \toprule
        Parameter & Weak & Strong \\ \midrule
        Flip probability &  &  \\
        Resize ] probability  &  & \\
        Color jittering probability &  &  \\
        Brightness adjustment max intensity  &  &  \\
        Contrast adjustment max intensity &  & \\
        Saturation adjustment max intensity  &   &  \\
        Hue adjustment max intensity  &  & \\
        Gaussian blurring probability  &  &  \\
        ClassMix~\cite{olsson2021classmix} probability  &  &  \\
\bottomrule
    \end{tabular} 
    \label{tab:transformation_distributions}\vspace{5mm}

\end{table}

\subsection{Benchmark Experiments}

The following experiments compare the proposed method with state-of-the-art methods in different semi-supervised semantic segmentation set-ups, including the semi-supervised domain adaptation scenario. 

\subsubsection{Semi-supervised Semantic Segmentation} 




\begin{figure*}[!tb]
\centering
\includegraphics[width=1\linewidth]{visual.png}
\caption{Qualitative results on Cityscapes. Models are trained with  of the labeled data using Deeplabv2 with ResNet-101. From left to right: Image, manual annotations, ClassMix~\cite{olsson2021classmix}, DMT~\cite{feng2020semi}, our approach.
}
\label{fig:cityscapes-results}
\end{figure*} 
 


 




\begin{table}[!tb]

\caption{Performance (Mean IoU) for the Cityscapes \textit{val} set for different labeled-unlabeled ratios and, in parentheses, the difference w.r.t. the corresponding fully supervised (FS) result. }
\label{table:city}
\centering 
\resizebox{0.49\textwidth}{!}{

\begin{tabular}{l|ccc|c} \toprule
method & 1/30 & 1/8 & 1/4 & FS  \\

\midrule
\multicolumn{5}{@{}p{6.8cm}}{\small{\textit{Architecture}: Deeplabv2 with ResNet-101 backbone}
}\\

\midrule

Adversarial~\cite{hung2018adversarial}+ &---  &  (-) &  (-)&  \\ s4GAN~\cite{mittal2019semi}* & ---  &  (-)&  -()& \\ French \etal~\cite{french2019semi}* &   (-)   &    (-)   &   (-) & \\CBC~\cite{feng2020semiCBC}+ &   (-) &  (-)  &   (-)&   \\ClassMix~\cite{olsson2021classmix}+ &  (-)  &    (-)&  (-) &   \\DMT~\cite{feng2020semi}*+ &   (-)  &    (-)  &  ---&     \\\hline
\textbf{Ours}* &  (-) &   (-) & (-)&     \\\textbf{Ours}+ &   & &   &    \\\midrule
\multicolumn{5}{@{}p{7cm}}{\small{\textit{Architecture}: Deeplabv3+ with ResNet-50 backbone
}}\\
 \midrule
 Error-corr~\cite{mendel2020semi}* & --- &   (-)  &  (-) & \\\textbf{Ours}* &    (-)  & &   &   \\\bottomrule
\end{tabular}

}
\small{\textit{* ImageNet pre-training, + COCO pre-training} }
\vspace{5mm}

\end{table}

 
 

 

 
\begin{table}[!tb]

\caption{Performance (Mean IoU) for the Pascal VOC \textit{val} set for different labeled-unlabeled ratios and, in parentheses, the difference w.r.t. the corresponding fully supervised (FS) result.  
}
\label{table:pascal}
\centering 
\resizebox{0.49\textwidth}{!}{
\begin{tabular}{l|ccc|c} \toprule
method  & 1/50 & 1/20 & 1/8 & FS \\

\midrule
\multicolumn{5}{@{}p{7cm}}{\small{\textit{Architecture}: Deeplabv2 with ResNet-101 backbone}}\\
 \midrule

Adversarial~\cite{hung2018adversarial}+  &  (-) &  (-)  &   (-) &  \\s4GAN~\cite{mittal2019semi}+  &  (-)  &  (-)  &  (-)  &     \\French \etal~\cite{french2019semi}*  &  (-)  &   (-)  &  (-)  &   \\CBC~\cite{feng2020semiCBC}+  &  (-)  &  (-)  &  (-)  &  \\ClassMix~\cite{olsson2021classmix}+ &  (-) &  (-)  & (-)   &  \\DMT~\cite{feng2020semi}*+ &  (-)  &  (-)  &   &   \\  \hline
\textbf{Ours}* &  (-)  & (-) &   (-)  &  \\

\textbf{Ours}+ &     &    &  (-)  &        \\ 

  \midrule
\multicolumn{5}{@{}p{7cm}}{\small{\textit{Architecture}: Deeplabv3+ with ResNet-50 backbone}}\\
 \midrule
 Error-corr~\cite{mendel2020semi}* & --- &  --- &   (-)  &  \\

\textbf{Ours}* &  (-)   &  (-) &   &  \\  
\bottomrule
\end{tabular}
}
\small{\textit{* ImageNet pre-training, + COCO pre-training} }
      \vspace{5mm}

\end{table}



\paragraph{Cityscapes.}
\Tab{table:city} compares different methods on the Cityscapes benchmark for different labeled-unlabeled rates: ,  and, . Fully Supervised (FS) scenario, where all images are labeled, is also shown as a reference. 
As shown in the table, our approach outperforms the current state-of-the-art by a significant margin in all settings. The performance difference is increasing as less labeled data is available, demonstrating the effectiveness of our approach. 
This is particularly important since the goal of semi-supervised learning is to learn with as little supervision as possible. Note that the upper bound for each method is shown in the fully supervised setting (FS).  
 
 
\Fig{fig:cityscapes-results} shows a visual comparison of the top-performing methods on different relevant samples from Cityscapes. Note in these examples how our approach improves on fine-grained examples (\eg poles, traffic lights, signs, or people).


\paragraph{Pascal VOC.}
\Tab{table:pascal} shows the comparison of different methods on the Pascal VOC benchmark, using different labeled-unlabeled rates: ,  and, . Our proposed method outperforms previous methods for most of the configurations. Like in the previous benchmark, our method presents larger benefits for the more challenging cases, \ie only a small fraction of data is labeled (). This demonstrates that the proposed approach is especially effective to learn from unlabeled data. 


\begin{table}[!b]

\caption{Mean IoU in Cityscapes \textit{val} set. Central columns evaluate the semi-supervised domain adaptation task (GTA5  Cityscapes). The last column evaluates a semi-supervised setting in Cityscapes (no adaptation).
Different labeled-unlabeled ratios for Cityscapes are compared.
All methods use ImageNet pre-trained Deeplabv2 with ResNet-101 backbone.
}
\label{table:domainadaptation}
\centering 
\small
\begin{tabular}{c|cc|c} \toprule
 City & ASS~\cite{wang2020alleviating} & \textbf{Ours}   & \textbf{Ours} \\
 Labels & \multicolumn{2}{c|}{with domain adaptation} &  no adaptation \\

\midrule
 &   &     &          \\ 
&        &   &          \\ 
&    &     &       \\ 
 &     &      &      \\ 
\bottomrule
\end{tabular}

\end{table}
 
\subsubsection{Semi-supervised Domain Adaptation}
Semi-supervised domain adaptation for semantic segmentation differs from the semi-supervised set-up in the availability of labeled data from another domain. That is, apart from having  and  from the target domain, a large set of labeled data from another domain is also available: .
 
Our method can naturally tackle this task by evenly sampling from both  and  as our labeled data when optimizing  and . However, the memory bank only stores features from the target domain . 
In this way, both the features from unlabeled data , and the features from the other domain  are aligned with those from .


Following ASS~\cite{wang2020alleviating}, we take the GTA5 dataset as , where all elements are labeled, and the Cityscapes is the target domain consisting of a small set of labeled data  and a large set of unlabeled samples . \Tab{table:domainadaptation} compares the results of our method with ASS, state-of-the-art on this task, both using ImageNet pre-training for a fair comparison. 
For reference, we also show the results of our approach with no adaptation, \ie only training on the target domain Cityscapes, as we do for the semi-supervised set up from the previous
experiment (\Tab{table:city}). We can see that our approach benefits from the use of the other domain data (GTA5), especially where there is little labeled data available (), as it could be expected. 
Our method outperforms ASS by a large margin in all the different set-ups. As in previous experiments, our improvement is more significant when the amount of available labeled data is smaller. 


\subsection{Ablation Experiments}
 \label{sec:ablation}
The following experiments study the impact of the different components of the proposed approach. The evaluation is done on the Cityscapes data, since it provides more complex scenes compared to Pascal VOC. We select the challenging labeled data ratio of . 
 

\paragraph{Losses impact.} \Tab{tab:ablation} shows the impact of every loss used by the proposed method.
We can observe that the four losses are complementary, getting a  mIoU increase over our baseline model, using only the supervised training when  of the Cityscapes labeled data is available. Note that our proposed contrastive module  is able to get  mIoU even without any other complementary loss, which is the previous state-of-the-art for this set-up (see \Tab{table:city}). Adding the  significantly improves the performance and then, adding  regularization loss gives a little extra performance gain. 



\begin{table}[!tb]
\caption{Ablation study on the different losses included \equ{eq:approach}. Mean IoU obtained on Cityscapes benchmark ( available labels, Deeplabv2-ResNet101 COCO pre-trained).
}\label{tab:ablation}
\begin{center}
\setlength{\tabcolsep}{5pt}
\begin{tabular}{cccc|c}
\hline 
 &  &  &  &   mIoU  \\ \hline
\checkmark &  &  &  &        \\
\checkmark & \checkmark &  &  &        \\
\checkmark &  &  \checkmark &  &        \\
\checkmark &  &  &  \checkmark &        \\

\checkmark & \checkmark & \checkmark  &  &      \\
\checkmark &  \checkmark &  &   \checkmark &        \\
\checkmark &  &  \checkmark  &  \checkmark &     \\

\hline
\checkmark & \checkmark & \checkmark  & \checkmark  &  \\
\hline 

\end{tabular}

\end{center}
\end{table}




\paragraph{Contrastive module.} 
\Tab{tab:lr} shows an ablation on the influence of the contrastive module for different values of  \equ{eq:approach}.
As expected, if this value is too low, the effect gets diluted, with similar performance as if the proposed loss is not used at all (see \Tab{tab:ablation}). High values are also detrimental, probably because it acts as increasing the learning rate vastly, which hinders the optimization. 
The best performance is achieved when this contrastive loss acts as auxiliary (), \ie its weight is a little lower than the segmentation losses ( and ).


The effect of the size (per-class) of our memory bank is studied in \Tab{tab:memory-size}. As expected, higher values lead to stronger performances, although from 256 up they tend to maintain similarly.
Because all the elements from the memory bank are used during the contrastive optimization \equ{eq:contrastive-loss} the larger the memory bank is, the more computation and memory it requires. Therefore, we select  as our default value.

\Tab{tab:contrastive} studies the effect of the main components used in the proposed contrastive learning module. 
The base configuration of the module still presents a performance gain compared to not using the contrastive module. This shows that our simplest implementation of the per-pixel contrastive learning using the memory bank already helps to improve the segmentation task in a semi-supervised scenario. 
Generating and selecting good quality prototypes is the most important factor. This is done both by the Feature Quality Filter (FQF), \ie checking that the feature leads to an accurate and confident prediction, and extracting them with the teacher network . Then, using the class-specific attention  to weight every sample (both from the memory bank and input sample) is also beneficial, acting as a learned sampling method.
 

\begin{table}[!tb]
\caption{Effect of different values for the factor  \equ{eq:approach} that weights the effect of the contrastive loss . 
Results on Cityscapes benchmark ( available labels, Deeplabv2-ResNet101 COCO pre-trained).  
}\label{tab:lr}
\begin{center}
\resizebox{0.49\textwidth}{!}{
\setlength{\tabcolsep}{5pt}
\begin{tabular}{l|cccccccc}
\hline 
  &  &   &   &  &   & &   \\ 
 \hline
mIoU   &  &     &     &   &  & &    \\  

 \hline
\end{tabular}
 }
\end{center}
\end{table}





\begin{table}[t]
\caption{Effect of our contrastive learning memory bank size (features per-class), . 
Results on Cityscapes benchmark ( available labels, Deeplabv2-ResNet101 COCO pre-trained).}
\label{tab:memory-size}
\begin{center}

\setlength{\tabcolsep}{5pt}
\begin{tabular}{l|cccccc}
\hline 
   &   &  &  &   &    \\ 
 \hline
 mIoU  &    &  &   &      &    \\
\hline
\end{tabular}

\end{center}
\end{table}



 

\begin{table}[t]
\caption{Ablation study of our contrastive module main components. Results on Cityscapes benchmark ( available labels, using Deeplabv2-ResNet101 COCO pre-trained).
}\label{tab:contrastive}
\begin{center}
\setlength{\tabcolsep}{5pt}
\begin{tabular}{c|ccc|c}
\hline 
Base &  &  &  FQF &  mIoU \\
\hline
\checkmark & &     &   &    \\
\checkmark & \checkmark &    &   &   \\ 
\checkmark & &  \checkmark   &   &   \\
\checkmark & &  &   \checkmark    &   \\ 

\hline
\checkmark & \checkmark & \checkmark  & \checkmark  &   \\
\hline

\end{tabular}

\small{\textit{: Use teacher model  to extract features instead of } } \\  
\small{\textit{: Use class-specific attention   to weight every feature  } } \\  
\small{\textit{FQF: Feature Quality Filter for Memory Bank update}  } \\
\end{center}

\end{table}







 
 
\section{Conclusion}

This paper presents a novel approach for semi-supervised semantic segmentation. 
Our work shows the benefits of incorporating contrastive learning techniques to solve this semi-supervised task. The proposed contrastive learning module boosts the performance of semantic segmentation in these settings. 
Our new module contains a memory bank that is continuously updated with selected features from those produced by a teacher network from labeled data. These features are selected based on their quality and relevance for the contrastive learning.
Our student network is optimized for both labeled and unlabeled data to learn similar class-wise features to those in the memory bank. 
The use of contrastive learning at a pixel-level has been barely exploited and this work demonstrates the potential and benefits it brings to semi-supervised semantic segmentation and semi-supervised domain adaptation. Our results outperform state-of-the-art on several public benchmarks, with particularly significant improvements on the more challenging set-ups, \ie when the amount of available labeled data is low.

 




{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}


 

\end{document}
