\documentclass[journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\pdfoutput=1
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{cite}
\usepackage{float}
\usepackage[export]{adjustbox} 
\usepackage{siunitx}
\usepackage{multirow}
\usepackage{threeparttable}
\usepackage{color}
\usepackage{amsmath, bm}
\usepackage{bbding}
\usepackage[backref]{hyperref}
\usepackage{pifont}
\usepackage[all]{hypcap}


\begin{document}
\title{Hi-ResNet: A High-Resolution Remote Sensing Network for Semantic Segmentation}

\author{\IEEEauthorblockN{Yuxia Chen\IEEEauthorrefmark{1},
                        Pengcheng Fang\IEEEauthorrefmark{1},
                        Jianhui Yu,
                        Xiaoling Zhong,
                        Xiaoming Zhang,
                        Tianrui Li\IEEEauthorrefmark{2},~\IEEEmembership{Senior Member,~IEEE}}


\thanks{Y. Chen and X. Zhong are with the School of Mechanical and Electrical Engineering, Chengdu University of Technology, Chengdu 610000 (email:  chenyuxia@stu.cdut.edu.cn, zhongxl@cdut.edu.cn).
P. Fang is a Research Engineer at Adaspace (email: fangpengcheng0823@gmail.com).
Y. Jianhui is with the School of Computer Science, University of Sydney, Sydney, Australia (e-mail: jianhui.yu@sydney.edu.au).
Z. Xiaoming and L. Tianrui are with the School of Computing and Artificial Intelligence, Southwest Jiaotong University, Chengdu 611756, China (e-mail:  zxmswjtu@163.com; trli@swjtu.edu.cn)
\\
\IEEEauthorrefmark{1} Equal Contribution.
\\
\IEEEauthorrefmark{2} Corresponding author.
}}

\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

\maketitle


\begin{abstract}
High-resolution remote sensing (HRS) semantic segmentation extracts key objects from high-resolution coverage areas. However, objects of the same category within HRS images generally show significant differences in scale and shape across diverse geographical environments, making it difficult to fit the data distribution. Additionally, a complex background environment causes similar appearances of objects of different categories, which precipitates a substantial number of objects into misclassification as background. These issues make existing learning algorithms sub-optimal.
In this work, we solve the above-mentioned problems by proposing a High-resolution remote sensing network (Hi-ResNet) with efficient network structure designs, which consists of a funnel module, a multi-branch module with stacks of information aggregation (IA) blocks, and a feature refinement module, sequentially, and Class-agnostic Edge Aware (CEA) loss.
Specifically, we propose a funnel module to downsample, which reduces the computational cost, and extract high-resolution semantic information from the initial input image. Secondly, we downsample the processed feature images into multi-resolution branches incrementally to capture image features at different scales and apply IA blocks, which capture key latent information by leveraging attention mechanisms, for effective feature aggregation, distinguishing image features of the same class with variant scales and shapes. Finally, our feature refinement module integrate the CEA loss function, which disambiguates inter-class objects with similar shapes and increases the data distribution distance for correct predictions. With effective pre-training strategies, we demonstrated the superiority of Hi-ResNet over state-of-the-art methods on three HRS segmentation benchmarks.

\end{abstract}

\begin{IEEEkeywords}
Remote sensing, Semantic segmentation, Attention, Pre-training
\end{IEEEkeywords}

\section{Introduction}
\IEEEPARstart{I}{n} the geomatics community, high-resolution remote sensing (HRS) images have become increasingly important due to advancements in imaging technologies and the growing demand for detailed and accurate data. These images are characterized by the exceptional spatial resolution, which presents finer details and features within the observed scenes. The advent of high-resolution satellite and aerial imagery has revolutionized remote sensing, urban planning~\cite{alshehhi2017simultaneous, gao2018building}, environmental monitoring~\cite{qin2021multilayer}, and disaster management~\cite{cooner2016detection},~\cite{xiong2020automated}, among other applications.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{features1}
    \caption{Comparisons of our model behavior by heatmaps with different images illustrate the feature information obtained by upsampling and merging at the end of each layer for Hi-ResNet base and Hi-ResNet. The three rows (a)(b)(c) show the original image, and the features of Hi-ResNet base and Hi-ResNet separately. It is evident from the results that compared to the Hi-ResNet base, the Hi-ResNet extracts richer and superior feature information.}
    \label{fig:features}
\end{figure}

Long-distance shooting by satellites and aircraft brings distinctive characteristics to HRS images~\cite{demir2018challenge, marcos2018land}.
Unlike conventional street-level images, objects of the same category in HRS images are typically situated in distinct geographical landscapes, which leads to more scale differences~\cite{kemker2018algorithms, volpi2015semantic}.
These disparities are not only evident between rural and urban land features but also between cities, where the differences are equally significant (e.g., differences among British architectural style, North American architectural style, modern architectural style, and Chinese architectural style)~\cite{xia2018dota}. Therefore, the ability to obtain multi-scale image features is crucial for HRS image segmentation networks.
Moreover, objects from the same category normally present different shapes layouts, and class distribution at different locations of the HRS image~\cite{boguszewski2021landcover}. For example, Rural scenes typically contain more natural elements such as forests and barren, while urban scenes contain buildings and vehicles. In contrast, urban areas with dense populations often have orderly and diverse shapes, and contain more small object categories such as cars and airports. Whereas, rural building structures present disordered and simpler building structures, and roads and rivers are relatively narrow compared to those in urban areas~\cite{wang2021loveda}. These introduce the emergence of intra-class variability. Another problem related to HRS is that due to the complex background environment, objects belonging to different categories can have similar appearances. Although the high-resolution imagery and diverse complex scenes contribute to a richer level of detail, inter-class similarities severely impact the performance of semantic segmentation networks~\cite{zheng2020foreground, kampffmeyer2016semantic, bai2021object}.
These factors present unique challenges in effectively handling and analyzing the diverse landscape elements within the segmentation process.








Traditional segmentation methods typically use edge-based segmentation~\cite{wanto2021combination, tian2021sobel, li2021bi}, threshold-based segmentation~\cite{rogerson2002change, lei2021remote, yang2017region}, and region-based segmentation~\cite{wang2010automatic}~\cite{zhang2015segmentation} to extract key information from HRS images. However, with the increasing resolution of HRS images in recent years, traditional methods have gradually become insufficient for complex and diverse image segmentation tasks. To achieve high-precision HRS image segmentation results, currently, the commonly used methods are neural convolutional~\cite{kampffmeyer2016semantic, ding2020lanet, fu2019dual, li2020scattnet, niu2021hybrid} and transformer~\cite{strudel2021segmenter, xie2021segformer, wang2022advancing}. 
Since the introduction of the full convolutional network (FCN)~\cite{long2015fully} in 2015, semantic segmentation models have mushroomed, promoting the development of~\cite{fu2019dual}~\cite{li2020scattnet} in the field of HRS image. Classic semantic segmentation networks like U-Net restore spatial details of features through a symmetrical decoder-encoder structure~\cite{ronneberger2015u}. On the other hand, DeepLabv3+~\cite{chen2018encoder} introduces the Atrous Spatial Pyramid Pooling module to extract context information at different scales. Unlike the above work, this work introduces a novel convolutional neural network (CNN) backbone designed to address all the questions mentioned before.

Owing to the increased complexity of diverse spatial scales in high-resolution segmentation tasks, some work~\cite{yin2020optimised,li2020novel,wang2023adaptive} increases the size of the receptive field by introducing the adaptive spatial pooling module to capture features at different scales. Unfortunately, performing a one-to-two feature aggregation at the end of each block often loses spatial information about the features.~\cite{zhang2019multi} obtains the feature maps of the low, medium and high scales in the first convolution of the network, and features the dense connection modules along the diagonal. However, this approach of consecutive downsampling may result in feature loss, and the process of dense connection also has the possibility of network structure redundancy and information blocking.
In contrast to the aforementioned methods, this paper proposes the funnel module and multi-branch module. The original image passes through the inverted bottleneck block in the funnel module to obtain reliable high-resolution information. In the multi-branch module, new scale information is obtained through downsampling in a progressive manner, and features at different scales are extracted in parallel, forming an efficient and direct feature extraction convolutional stream. At the end of each feature extraction, the feature information from the previous branch is fused with the newly generated branch's information to achieve multi-scale information interaction, allowing the entire network to maintain high resolution while obtaining sufficiently complete and reliable low-resolution information.

It is also essential to recognize that the shape distribution variations of object of the same class across diverse geographical regions.
To alleviate the issue of class distribution disparities, a common approach is to incorporate attention mechanisms into the network.
For instance,~\cite{woo2018cbam,chen2021remote,chen2022gcsanet} utilize spatial attention to optimize class weights and address class imbalance problems. Additionally,~\cite{bi2021local} and~\cite{zhao2020residual} employ parallel channel attention and spatial attention to enhance local features simultaneously.
This paper argues that introducing attention mechanisms in individual modules of the network may lead to network focus bias. Therefore, we have proposed the Information Aggregation (IA) block. This block aggregates the features and extracts the location, channel and spatial information of the features through the Coordinate Attention mechanism (CA)~\cite{hou2021coordinate}. To prevent neuronal death, the GELU~\cite{hendrycks2016gaussian} is used as an activation function in the module, ensuring that the information of the attention mechanism flows smoothly through the network.
We applied IA block to the multi-branch module, thereby aggregating different shapes of the same class to reduce the intra-class distribution distances. To evaluate the validity of the IA block, we visualized the feature maps of Hi-ResNet base and Hi-ResNet, and the results are shown in Figure \ref{fig:features}. Hi-ResNet base stacks basic block to construct HRS module while Hi-ResNet stacks IA block. Obviously, the image features extracted and fused by IA block are far beyond the image features extracted by using the traditional convolutional network basic module.

As previously mentioned, due to the diversification of high-resolution satellite images and the increased background of false detection, we propose a feature refine module. This module upsamples the three feature maps with different resolutions obtained by muti-branch module to the same size, and concat into a feature map. By convolution and Object-Contextual Representations (OCR)~\cite{yuan2019segmentation}, we obtained the results of coarse and fine segmentation of Hi-ResNet, respectively, and calculated the loss according to a ratio of 0.4:1. Some work~\cite{zhou2018d,lin2020road} applied dice loss in road extraction by increasing the weights of the key road regions, FactsegNet~\cite{ma2021factseg} utilizes collaborative probability loss to merge the outputs of the dual-branch decoders at the probability level, aiming to enhance the utilization of information.
However, the proposed class-agnostic edge aware (CEA) loss in this paper focuses more on the edge information of class objects. CEA loss randomly selects one class from the segmentation results and treats the other classes as background. It computes the Hausdorff distance matrix between the background and the selected class, and then performs a Hadamard product between the matrix and the ground truth. This correction at the edge level helps improve the model's perception of boundaries and shapes, enhancing its ability to capture accurate object edges.
Finally, we evaluate the proposed method on widely used datasets. This study contributes three main points:

\begin{itemize}
\item[(1)] We propose a funnel module to reduce computing costs, efficiently extract high-resolution information and avoid feature loss from the input image.
\item[(2)] We propose a multi-branch module with stacks of information aggregation blocks with a balance of both attention and convolution mechanisms. 
\item[(3)] We develop a class-agnostic edge aware (CEA) loss, which emphasizes edge information while taking into account multiple classes.
\item[(4)] Our Hi-ResNet is validated on several benchmarks with performance better than existing state-of-the-art methods.
\end{itemize}

The paper is organized as follows: Section II provides an overview of related work, including Semantic Segmentation in Remote Sensing, Attention, and Model Pre-training. In Section III, we describe the proposed method, which includes the Hi-ResNet model, the design of loss functions, a series of training strategies, and the use of unsupervised and supervised pre-training in RS tasks. Section IV presents a series of ablation experiments and experimental results and analyses on different datasets. Finally, in Section V, we conclude the paper and provide a summary.

\section{RELATED WORK}
\subsection{Semantic Segmentation in Remote Sensing}
Parallel multi-resolution architectures primarily focus on high-level semantic information, resulting in a semantically richer and spatially more accurate representation, providing an advanced technical reference for location-sensitive vision problems. Among them, HRNet~\cite{wang2020deep} was well known as a parallel semantic segmentation model that could maintain high resolution. It passed through four stages of gradually decreasing resolution and performs multi-scale fusion to enhance high-resolution representations. Subsequently, researchers attempted to combine HRNet with OCR~\cite{yuan2019segmentation} which distinguishes contextual information for the same target category from different target categories and optimizes feature pixels. This architecture was widely applied in the field of HRS segmentation~\cite{yin2020optimised,niu2021hybrid,cheng2020remote,zhang2020multi,chen2023lite,wang2022detection}. However, HRNet primarily focused on high-resolution semantic features of images, while OCR was more concerned with the relationships between image objects and their pixels, both of which ignore semantic information about the target location. Unfortunately, reliable and comprehensive high-level semantic information is undoubtedly crucial, as HRS images often contain a large amount of complex and unrelated background, and small target objects usually occupy only a few pixels. To obtain richer high-level semantic information, some studies~\cite{hamaguchi2018effective,bi2021local,li2020dmnet,liu2020dense} applied different dilated convolutions to multiple features of traditional CNN networks to construct distinctive local semantic representation modules by expanding the receptive field of the convolution kernel, thereby effectively utilizing multi-scale features. Furthermore, some researchers~\cite{cai2020remote,chaudhuri2019siamese,zhou2021split,xu2021deep} applied graph convolution on multi-layer features, where each pixel treated as a node, and then the extracted graph features were connected with the final global visual features. Despite this, locally aggregating features in the spatial direction might overlook channel and positional information of high-level semantics. An effective solution is to establish an information connection between space and channels in convolutional networks. Recently, MBFANet~\cite{shi2023remote} combined the pooling channel attention module and convolutional coordinate attention module to complement each other, which helped the models focus on more complex background categories. 
SAPNet~\cite{zheng2022sapnet} joint models both spatial and channel affinity, which allows for preserving spatial details and extracting accurate channel information. 
Inspired by the parallel architecture of HRNet, we propose Hi-ResNet in this work. The Hi-ResNet proposed in this article utilizes a funnel module to obtain rich high-resolution semantic information. Inspired by HRNet, we parallel extract multiresolution features in a muti-branch module and use a feature refine module to obtain high-level semantic information in HRS images.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{Hi-ResNet}
    \caption{The comprehensive architecture of Hi-ResNet is partitioned into three components. (a) The funnel module, composed of a downsample part and a funnel stem, is purposed for downsampling input imagery and facilitating feature extraction. (b) The multi-branch module further hones these features via the amalgamation of a multi-resolution convolutions stream. (c) Coarse features are computed directly via a convolution layer, with fine-grained features managed through the utilization of OCR~\cite{yuan2019segmentation}. (d) Multiple loss functions are employed, including LSCE loss~\cite{muller2019does} and GD loss\cite{sudre2017generalised}, which are computed in direct relation to the ground truth and predictions. Concurrently, the CEA randomly elects a category, designating all others as background, before computing the loss between the two categories.}
    \label{fig:Hi-ResNet}
\end{figure*}

\subsection{Attention Mechanisms}
Attention mechanisms could help the network to locate the information of interest and inhibit useless information, which has been widely used in convolutional neural networks~\cite{hu2018squeeze,cao2019gcnet,fu2019dual,liu2020improving,woo2018cbam,chen2021remote}. For HRS tasks, some studies utilized the popular attention mechanism Squeeze-and-Excitation to automatically process the features of various scenes and extract more effective features~\cite{tian2021semsdnet,zhang2022mrse,zhang2022transformer,ding2020lanet,lin2020road}. However, this attention mechanism only focused on inter-channel information while neglecting spatial and positional information about the features. In order to simultaneously capture channel and positional information, researchers explored the use of Convolutional Block Attention Module or Bottleneck Attention Module in network architectures~\cite{wang2022cbam,zhu2021improving,shi2021deeply}. These modules used spatial attention to obtain the location information and reduce the input channel dimension to save the calculation cost. Due to the limited receptive field of the sliding window in convolutional operations, only local relationships were captured, it could not maintain long-range dependencies between different positions in the image. Currently, some works~\cite{wang2022unetformer,strudel2021segmenter,xie2021segformer} built spatial attention in self-attention networks to overcome the local nature of convolutions and capture diverse spatial information. Nevertheless, the substantial computational complexity introduced by self-attention made the training cost of the network expensive, which posed challenges for its application in lightweight convolutional networks. Recently, Hou et al.~\cite{hou2021coordinate} proposed an efficient attention mechanism called Coordinate Attention (CA) to capture both channel and positional information. This attention mechanism decomposed channel attention into one-dimensional features in two spatial directions through pooling operations, maintaining reliable positional information while establishing long-range dependencies. This paper suggests that the complementary characteristics of the CA mechanism in space and location can effectively alleviate the difference in the class distribution of HRS images in remote sensing segmentation tasks, so we apply the CA mechanism in the IA block.


\subsection{Model Pre-training}
In addition to the design of the network itself, excellent pre-training programs are also indispensable. Numerous studies showed that applying pre-training can make models more stable and extract more commonalities~\cite{simonyan2014very,he2016deep,wang2022advancing,wang2022empirical,ayush2021geography}. Therefore, we pre-train the Hi-ResNet to enhance the fine-tuning ability for HRS segmentation tasks. Recently, for HRS tasks, some studies~\cite{workman2023handling,kong2020enhanced} used labeled semantic segmentation datasets such as Mapillary~\cite{neuhold2017mapillary} for pre-training to improve model performance. However, these large-scale labeled datasets mostly come from natural images, and pre-training on them for HRS tasks often yields unsatisfactory results. It is worth noting that recent works on unsupervised pre-training~\cite{khan2022transformers,chen2020simple,chen2020improved,he2020momentum,xu2021regioncl} showed that unsupervised pre-training outperforms the supervised way in downstream tasks such as segmentation.
MoCo~\cite{he2020momentum}, as a mechanism for building dynamic dictionaries for contrastive learning, surpassed its supervised counterpart in seven downstream tasks.~\cite{zhao2020makes} illustrated that MoCo mainly transferred low-level and middle-level semantic features, and when performing image reconstruction, the reconstructed images without supervision were closer to the original data distribution. Based on the previous work, we argue that employing supervised pre-training will provide richer and more comprehensive prior information for HRS tasks. At the same time, using the pre-training mechanism of MoCo can effectively compensate for the loss of precise localization information in the network and reduce the emphasis on local object information. Therefore, in this study, we apply both fully supervised and unsupervised pre-training strategies on Hi-ResNet and evaluate the performance of the two methods.


\section{PROPOSED METHODS}
In this section, we present the framework of Hi-ResNet, including the funnel module, the multi-branch module with information aggregation blocks, and feature refinement module. Then we introduce the class-agnostic edge aware loss for HRS image feature extraction. Finally, we present how to transfer the various pre-training strategies to the HRS segmentation task.

\subsection{Hi-ResNet Framework}
The Hi-ResNet proposed in this paper is shown in Figure \ref{fig:Hi-ResNet}. In the following sections, we will present the funnel module, multi-branch module, and feature refinement module, and the implementation details of each module in turn.

\subsubsection{Funnel Module}
In the funnel module, we start by passing the input image through two stride-2, 3$\times$3 convolutions, which reduce the image resolution to 1/4 of its original size. During the network downsampling, the BN layer was placed before the convolution operation. It could improve the generalization and stability of the model by applying the BN layer where the spatial resolution changes, which makes the pre- and post- samples to different Gaussian distributions. Then, the image goes through a funnel stem with four inverted bottleneck (IB) blocks to obtain high-resolution semantic features. The traditional bottleneck layer structure uses a structure with long heads and a short middle. With consideration of the distribution characteristics of HRS data, to prevent the collapse of activation space and loss of channel information caused by non-linear activation functions in network layers~\cite{sandler2018mobilenetv2}, our work adapts the IB block with thin heads and a thick middle. This block is used to extract richer semantic features by performing high-dimensional upsampling on HRS images, followed by downsampling and linear activation functions to avoid information loss, thereby preserving more complete information of HRS images. The design of the funnel module is illustrated in Figure \ref{fig:funnel}. We use a 3$\times$3 convolution for downsampling in the first layer of the bottleneck, and a 1$\times$1 convolution for both fourfold downsampling and upsampling in the middle layer, thus obtaining richer high-resolution semantic information.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{funnel}
    \caption{The structure of the funnel module where IB refers to inverted bottleneck. The number in each block refers to the kernel size, stride, and channel numbers respectively.}
    \label{fig:funnel}
\end{figure}

\subsubsection{Multi-branch Module}
Multi-branch module consists of multi-resolution convolutions streams and repeated fusions. First of all, to address the issue of unstable segmentation accuracy caused by differences in image scales, our network maintains the high-resolution representation of the input image throughout the entire process, while generating a new low-resolution branch at each layer, which together with the existing branches forms the input for the next layer. We adopt the parallel approach to connect the convolutions of multi-resolution branches, forming the multi-resolution convolutions stream. Notably, the minimum resolution of the image in the parallel branch of the second layer is only 1/16 of the original image, indicating that this layer focuses more on the high-level semantic information of the image. Due to the significant layout differences among buildings and objects in different areas of HRS images, the shape and contour features in high-level semantic information are crucial. This paper argues that it cannot extract rich high-level semantic features that contain target locations if merely stacking the same number of blocks as in other layers and using the same sliding window sampling. Therefore, we stack 4 IA blocks in the first layer as high-resolution module\_4 (HRM\_4), and 12 IA blocks as high-resolution module\_12 (HRM\_12). Figure \ref{fig:features2} illustrates the semantic information extracted from the multi-branch module of the original Hi-ResNet and the extended multi-branch module. More abundant and reliable high-level semantic information can be obtained through the second layer after elongation, which not only effectively alleviates the problem of class distribution inconsistency and reduces intra-class variance but also avoids the loss of positional information of small target objects that occupy only a few pixels in the image, thereby enhancing the weak features of small target objects.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{features2}
    \caption{(a) the output features of the multi-branch module of Hi-ResNet base. (b) the output features of the multi-branch module of Hi-ResNet.}
    \label{fig:features2}
\end{figure}

Numerous studies propose methods for multi-scale feature fusion~\cite{long2015fully,cheng2020higherhrnet,zhang2019multi}. Classic semantic segmentation networks like UNET~\cite{ronneberger2015u} and SegNet~\cite{badrinarayanan2017segnet} extract low-resolution feature maps during downsampling and combine them with feature maps of the same resolution, and upsampling them to prevent feature loss during the upsampling process. In contrast, our approach performs cross-layer fusion between parallel branches with different resolutions, capturing features of different sizes by repeatedly exchanging information on different scales at each layer. Figure \ref{fig:fusion} illustrates the fusion process for layer2, where the input consists of three images with different resolutions. Different sampling methods are used depending on the resolution of the input and output. The upsampling stage includes bilinear upsampling, batch normalization, and a 1$\times$1 convolution, while the downsampling stage includes batch normalization and a stride-2, 3$\times$3 convolution. The multi-branch module  process ultimately outputs three feature maps with different resolutions.


\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{fusion}
    \caption{This figure illustrates the process of feature information aggregation across various resolutions in the transition layer of the network. Furthermore, the figure indicates a deviation from the original process of normalization followed by sampling to sampling followed by normalization using BN, as employed in this study.}
    \label{fig:fusion}
\end{figure}

\subsubsection{Information Aggregation Block}
HRS images provide rich details and features but also bring more irrelevant background objects. To suppress the impacts brought by the irrelevant background information and to enhance the spatial and positional feature representations, we propose an IA block with CA~\cite{hou2021coordinate}. The residual connection of this block consists of three parts: two downsampling convolutions and one CA attention module, both of the downsampling convolutions use 3$\times$3 convolutional kernels. The fundamental principle of the CA mechanism is to exploit the spatial coordinate information of feature maps. This is achieved by passing the $x$ and $y$ coordinates of each spatial position through separate neural network branches, allowing attention weights to be computed for each position. This approach captures spatial correlations between different positions within the feature map, which enhances the representational power of the feature map. Furthermore, CA is a lightweight and efficient attention mechanism since it can selectively attend to specific positions within the feature map, rather than computing attention weights across all positions. We hypothesize that more abstract and refined feature information is better for attention modules to extract contextual semantic information in HRS. Therefore, the CA module is placed at the end of the entire block. In the IA block, GELU is used instead of RELU, along with batchnorm (BN). The proposed block is considered lightweight and efficient compared to other attention mechanisms and can enhance the representational power of the feature map. The IA block with CA is illustrated in Figure \ref{fig:IA}.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\linewidth]{IA.pdf}
    \caption{The IA block of Hi-ResNet comprises two convolutions with a kernel size of 3x3 and a stride of 1, coordinate attention (CA)~\cite{hou2021coordinate} module, and a residual branch. CA module is an attention mechanism that enhances the model's ability to model relationships among channels.}  
    \label{fig:IA}
\end{figure}

\subsubsection{Feature Refinement Module}
We use the three different resolution feature maps output by the multi-branch module as inputs to the feature refinement module. In the feature refinement module, we combine the three output images to the same size using bilinear upsampling, which serves as the coarse segmentation of the network. By leveraging OCR~\cite{yuan2019segmentation}, we first treat a category in the coarse segmentation result as a region and estimate the comprehensive feature representation within that region by aggregating the representations of each pixel. Then, we compute the pixel-region relationships to obtain corresponding weights, which are used to enhance the representation of each pixel by weighting all the regions. The weighted feature representation serves as the refined segmentation result of the model. Lastly, Hi-ResNet outputs both coarse segmentation and refined segmentation. We calculate the losses for both segmentation results separately and weigh them with a ratio of 0.4:1.0.


\begin{table}[!ht]
  \begin{center}
  \begin{threeparttable}
  \scriptsize
    \caption{structure configuration table.}
    \begin{tabular}{c|c c c c c}
    \hline
      \textbf{Module Part} & \textbf{Multi-scale} & \textbf{Branches} & \textbf{Module} & \textbf{ModuleNums}\\
      \hline
      funnel & 1/4 & 1 & Conv, IB block & 2, 4\\
      $^{\scriptscriptstyle1}$Mb layer1 & 1/4, 1/8 & 2 & HRM\_4 & 1\\
      Mb layer2 & 1/4, 1/8, 1/16 & 3 & HRM\_12 & 4\\
      \hline
    \end{tabular}
        \begin{tablenotes}
        \footnotesize  
        \item[1] Mb layer: Multi-branch layer
      \end{tablenotes} 
    \end{threeparttable}
  \end{center}
  \label{table:1}
\end{table}
The final network block numbers and module repeated times in each stage are shown in Table \ref{table:1}.


\subsection{Loss Design}
In HRS tasks, semantic segmentation typically involves more than two labels, with significant differences in the number and pixel range of objects for different categories, leading to sample imbalance and sub-optimal performance. Therefore, an appropriate loss function is crucial. In this work, we propose a new class-agnostic edge aware (CEA) loss, which is combined with the Generalised Dice loss (GD)~\cite{sudre2017generalised}and Label Smoothing Cross-Entropy loss (LSCE)~\cite{muller2019does} as the training loss.

\subsubsection{Generalised Dice Loss}
Weighted cross-entropy and Sensitivity-Specificity approaches are designed to address imbalanced problems only in binary classification tasks. In contrast, the GD loss method can weight various pixel classes, allowing for a more comprehensive approach to imbalanced sample issues. The loss calculation for GD loss can be expressed as:
\begin{equation}
GD = 1 - 2\frac{\sum_{l=1}^2w_l\sum_{n}r_{ln}p_{ln}}{\sum_{l=1}^2w_l\sum_{n}r_{ln} + p_{ln}}
\end{equation}


The equation for GD loss involves using $r_{l}$ to represent the label of each pixel in the reference foreground segmentation for class $l$, and $p_{ln}$ to denote the predicted probabilistic map for the foreground label of class $l$ over N image elements $p_{n}$. The weighting factor $w_{l}$ is used to provide invariance to different label set properties. Its calculation is expressed as:
\begin{equation}
w_l = \frac{1}{\sum_{i=1}^Nr_{ln}^2}
\end{equation}

During the calculation process, overlapping $r_{n}$ and $p_{n}$ are added according to their weights and then divided by the weighted sum of the union part. This effectively suppresses the interference of complex background classes, enhances the features of small targets, and alleviates the problem of imbalanced image samples.

\subsubsection{Label Smoothing Cross-Entropy Loss}
The label smooth technique proposed in~\cite{muller2019does} as a training strategy can adjust the extreme values of the loss and improve the model's generalization ability when combined with Cross-Entropy loss. The formulation of Label Smoothing Cross-Entropy loss is as follows:
\begin{equation}
\mathcal{L}_{ce} = -\frac{1}{N}\sum_{n=1}^N\sum_{k=1}^Ky_k^{(n)}\log\hat{y}_k^{(n)}
\end{equation}
\begin{equation}
q_i=
\begin{cases}
1 - \varepsilon& \text{if } i = y,\\
\varepsilon/(K - 1)& \text{otherwise}
\end{cases}
\end{equation}

In the above formula, $y_{k}$ represents the sample label following the label smoothing operation, where $\epsilon$ is the smoothing factor, and $y_{k}$ is the corresponding softmax output of the network. Considering that HRS image datasets usually have a small amount of data, we argue that using this loss can prevent overfitting of the network and provide the correct optimization direction for the model.

\subsubsection{Class-agnostic Edge Aware Loss}
The Hausdorff loss~\cite{karimi2019reducing} was originally developed for boundary calculation in medical images. However, for HRS image segmentation, we modified the original HD loss calculation method to make it suitable for multi-class boundary calculation. It is shown below:
\begin{equation}
\small
    \mathcal{L}_{MHD} = \frac{1}{N}\sum_{i=0}^{N}\int_{\Omega}(g(p_{i}) - s_{\theta}(p_{i}))^2(D_{G}(p_{i})^\beta + D_{S}(p_{i})^\beta)\mathrm{d}p_{i},
\label{eq:l_hd}
\end{equation}
where $p_{i}$ refers to a distinct category of the input image, and $\Omega$ denotes the spatial domain of the training images. The distance function from the predicted boundary $S$, after applying thresholds $s_{\theta}$, is represented by $D_{S}$. The hyper-parameter $\beta$, which is set to 2 by the authors of~\cite{karimi2019reducing} through a grid search, is also a part of this process. Additionally, we use $N$ to represent the number of classes.

We have observed that the computation time for HD loss is quite high in the case of multi-class segmentation and it increases proportionally with the number of classes. In light of the specific characteristics of HRS imagery, such as the prevalence of background and the low incidence of class intersections, we have proposed a new class-agnostic edge aware (CEA) loss function in Equation~\ref{eq:l_ae} as an improved alternative. The CEA loss function randomly picks one of the categories and treats the rest of the classes as background, then computes the loss with respect to the two classes. It has a fixed computational cost and is tailored to address the challenges of HRS segmentation tasks. The CEA loss function improves the segmentation edge while mitigating the negative impact of complex backgrounds.

\begin{equation}
\small
    \mathcal{L}_{CEA} = \int_{\Omega}(g(p) - s_{\theta}(p))^2(D_{G}(p\vee0_{p})^\beta + D_{S}(p\sim0_{p})^\beta)\mathrm{d}p
\label{eq:l_ae}
\end{equation}

Here, $p_{i}$ refers to the input image, while the symbol $\Omega$ denotes the spatial domain of the training images. The distance function from the predicted boundary $S$, after applying thresholds $s_{\theta}$, is represented by $D_{S}$. The hyper-parameter $\beta$, which is set to 2 through a grid search, is also a part of this process. $0_{p}$ represents a zero matrix with the same shape as $P$, while $\vee$ and $\sim$ represent the $XOR$ operation and the $NOT$ operation, respectively.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{MoCoV2}
    \caption{The figure outlines the entire process of MoCoV2 pre-training. $\bm q$ and $\bm k^{+}$ are the positive sample and the negative respectively, augmented from the same image.  while $\bm k^{-}$ refers to the past negative features stored in the queue.}
    \label{fig:moco}
\end{figure*}


\subsection{Remote Sensing Pre-training}
\subsubsection{Dataset}
\begin{itemize}
\item[$\bullet$] The Mapillary dataset~\cite{neuhold2017mapillary} is presently the largest publicly available dataset at the street level with specific instance annotations and a high degree of diversity. This dataset encompasses 25,000 high-resolution RGB images, captured by a variety of imaging devices, and includes fine-grained labels for 66 categories.
\end{itemize}

\begin{itemize}
\item[$\bullet$] Million-AID~\cite{long2021creating} is a comprehensive benchmark dataset designed for remote sensing scene classification. This dataset obtains images with resolutions ranging from 0.5m to 153m from multiple satellites of Google Earth. The scene labels are obtained through the geographical coordinate information, resulting in over one million images labeled with 51 semantic scene categories.
\end{itemize}


\subsubsection{Pre-training Details}
 The third section of this paper introduces two distinct methods for pre-training models. For the supervised training, the Mapillary dataset is selected as it shares the same downstream task as this paper, and it is also used by HRNet~\cite{zhang2019multi} for pre-training. After clipping the Mapillary images, 2 million 256$\times$256 images are obtained. To address the issue of imbalanced data, a filter strategy is employed, considering the ratio of pixel classes. This results in a training set of 400,000 images. The supervised pre-training process is conducted similarly to that of our own model, and the hyper-parameters are specified in \ref{table:2}.

The unsupervised training utilizes the Million-AID dataset. Since the images in Million-AID have varying resolutions, they are partitioned into 400$\times$400, while images size less than 400 are dropped from the dataset. We use contrast learning MoCoV2~\cite{he2020momentum} as the unsupervised pre-training method. The process of MoCoV2 is shown in Figure \ref{fig:moco}, and the primary training settings for both pre-training approaches are presented below.

\begin{table}[!ht]
  \begin{center}
  \scriptsize
    \caption{HYPER-PARAMETER SETTINGS OF TRAINING PRE-TRAINED MODELS}
    \begin{tabular}{l|c c c c c c}
        \hline
      \textbf{Method} & \textbf{dataset} & \textbf{lr} & 
      \textbf{image size} & \textbf{batch size} & \textbf{quantity}\\
      \hline
      supervised & Mapillary & 5e-5 & 256$\times$256 & 80 & 400,000\\
      MoCoV2 & MillionAid & 0.015 & 256$\times$256 & 64 & 1,000,000\\
      \hline
    \end{tabular}
  \end{center}
  \label{table:2}
\end{table}

MoCoV2 pre-training commences with the augmentation of a batch of images twice, to generate the positive samples, denoted as $q$, and the batch negative samples, denoted as $k^{+}$. Following this, the logits of $q$ and $k^{+}$ are procured by introducing $q$ and $k^{+}$ into the standard encoder and the momentum encoder, respectively. In addition, the input negative samples are amalgamated with $k^{+}$ and $k^{-}$, which are retrieved from the queue. Subsequently, the logits of the positive and negative samples are concatenated, following which the InfoNCE loss function is computed to update the standard encoder:

\begin{equation}
\small
    \mathcal{L}_{q, k^{+}, {k^{-}}} = -\lg \frac{\exp{(q\cdot k^{+} / \tau)}}{\exp{(k\cdot k^{+} / \tau)} + \sum\limits_{k^{-}}\exp{(q\cdot k^{-} / \tau)}}  
\label{eq:moco1}
\end{equation}
where \textbf{q} is a query representation, $\bm k^{+}$ is a representation of the positive (similar) key sample, and $\bm k^{-}$ are representations of the negative (dissimilar) key samples. $\bm \tau$ is a temperature hyper-parameter. During training, only the normal encoder updates while the momentum encoder is updated with the function below:
\begin{equation}
\small
    \mathcal{\theta}_{k} = m\theta_{k} + (1 - m)\theta_{q}
\label{eq:moco2}
\end{equation}
Here \textbf{m} $\in [0, 1)$ is a momentum coefficient. Only the parameters $\theta_{q}$ are updated by back-propagation. The momentum update in \ref{eq:moco2} makes $\theta_{k}$ evolve more smoothly than $\theta_{q}$. Finally, $k^{+}$ will be added to the queue and features earlier in the queue will be dequeued.

\section{EXPERIMENTAL RESULTS AND ANALYSIS}
In this section, we evaluate the performance of our proposed model on multiple remote sensing datasets, including LoveDA, Potsdam, and Vaihingen. We first conduct a series of ablation studies on the Vaihingen dataset to analyze and identify a suitable framework for our proposed model. Next, we compare our Hi-ResNet with current state-of-the-art (SOTA) methods on public benchmarks, utilizing existing popular frameworks. Additionally, we demonstrate the superiority of our proposed model in terms of computational complexity, such as inference speed and memory footprint, as well as data transfer efficiency.

\subsection{Datasets}

\subsubsection{LoveDA}
The LoveDA dataset~\cite{wang2021loveda} comprises 5987 HSR images (GSD 0.3 m) from three different cities, each containing 166768 annotated objects. Each image is 1024$\times$1024 pixels and includes 7 land cover categories, namely building, road, water, barren, forest, agriculture, and background. The dataset provides 2522 images for training, 1669 images for validation, and 1796 official images for testing. The dataset consists of two scenes, urban and rural, from three Chinese cities, namely Nanjing, Changzhou, and Wuhan. Consequently, the dataset presents a significant research challenge due to the presence of multi-scale objects, complex backgrounds, and inconsistent class distributions.

\subsubsection{Potsdam}
Potsdam is an example of a historic city with significant building complexes, narrow streets, and dense settlement structures. The Potsdam dataset is composed of 38 patches, each measuring 6000$\times$6000 pixels, and containing a true orthophoto (TOP) extracted from a larger TOP mosaic. The dataset has been manually classified into the six most common land cover categories, and the ground sampling distance of both the TOP and the DSM is 5 cm. In this paper, we follow the approach used in~\cite{wang2022unetformer} and use 23 images (excluding image $7\_10$ with error annotations) for training and 14 images for testing.

\subsubsection{Vaihingen}
The village of Vaihingen comprises many individual buildings and small multi-story buildings, and like the Potsdam dataset, it has been classified into six common land cover categories. The dataset includes 3-band remote sensing TIFF files (near-infrared, red, green) and single-band DSM, with 33 HRS images of varying sizes. For the experiment, we have followed~\cite{wang2022unetformer} to select the remote sensing images with ID 2, 4, 6, 8, 10, 12, 14, 16, 20, 22, 24, 27, 29, 31, 33, 35, and 38 for testing, while the remaining 16 images are used for training. Table \ref{table:3} provides detailed information about each dataset.
\begin{table}[!ht]
  \begin{center}
  \scriptsize
    \caption{THE DETAILS OF DIFFERENT SEMANTIC SEGMENTATION DATASETS.}
    \begin{tabular}{c|c c c c c}
    \hline
      \text{Datasets} & \text{Training} & \text{Validation} & \text{Testing} & \text{Category} & \text{Input Size}\\
      \hline
      LoveDA & 2,522 & 1,669 & 1,796 & 7 &1,024$\times$1,024\\
      Potsdam & 24 & - & 14 & 6 &3,000$\times$6,000\\
      Vaihingen & 16 & - & 17 & 6 & 2,494$\times$2,064 \\
      \hline
    \end{tabular}
  \end{center}
  \label{table:3}
\end{table}


\subsection{Implementation Details and Experimental Settings}
For the LoveDA dataset, the training and validation sets are both used for training. Images are cropped into patches with 512$\times$512 resolution for input. During training, various enhancement techniques such as random vertical flip, random horizontal flip, and random scaling with ratios of [0.5, 0.75, 1.0, 1.25, 1.5] were employed. The training process lasted for 200 epochs with a batch size of 16. During the testing phase, 1796 images provided by the official were used, and multi-scale and random flip enhancements were applied for prediction.

As for the Potsdam and Vaihingen datasets, this paper utilized techniques including color transformation, random vertical flip, and random horizontal flip for data augmentation, and cropped them into 512$\times$512 patches for model input.
The training epoch was set to 200 with a batch size of 16.

In the experiment, learning rate warmup combined with cosine annealing was used to adjust the learning rate, where warmup was set to 3 epochs. Moreover, AdamW~\cite{loshchilov2017decoupled} optimizer was selected to accelerate model convergence, with the learning rate and weight decay set to \num{1e-4} and \num{1e-8} respectively. The training was conducted on 4 NVIDIA GTX 3090 GPUs and implemented based on the PyTorch framework.

\subsection{Ablation Study and Comparison Experiments}
\subsubsection{Determining the Layer Size}
This paper conducts ablation experiments on the Vaihingen dataset to prevent information loss in Hi-ResNet and reduce the number of model parameters. Table \ref{table:4} compares the accuracy and training cost of the model under different decisions, including the number of parameters (Params), calculation amount (FLOPs), GPU memory usage, and training time.

\begin{table}[!ht]
  \begin{center}
  \footnotesize
  \begin{threeparttable}
    \caption{RESULTS OF THE MODULE ANALYSIS EXPERIMENTS ON VAIHINGEN DATASET}
    \begin{tabular}{c|c | c c c c}
    \hline
        \multirow{2}{*}{Model} & \multirow{2}{*}{IA} & \multicolumn{3}{c}{Multi-branch layers}\\
        \cline{3-6}
          & & Layer1 & $^{\scriptscriptstyle1}$Layer2 (ext.) & Layer3 (ext.) \\
      \hline
       Hi-ResNet base &  &  &  &  & \\
       Hi-ResNet v1 & \ding{52} & \ding{52} &  & \\
       Hi-ResNet v2 & \ding{52} & \ding{52}  & $^{\scriptscriptstyle2}$\ding{52} & \\
       Hi-ResNet v3 & \ding{52} & \ding{52} &  & \ding{52}\\
       ours         & \ding{52} & \ding{52} & \ding{52} & $^{\scriptscriptstyle3}$\ding{56} \\
      \hline
    \end{tabular}
    \begin{tablenotes}
        \footnotesize  
        \item[1] extend the current layer
        \item[2] black check mark means the current layer is extended compared to the base
        \item[3] black cross mark means the current layers are deleted
      \end{tablenotes} 
    \end{threeparttable}
  \end{center}
  \label{table:4}
\end{table}

As shown in Table \ref{table:4}, we configured the multi-branch module of the Hi-ResNet base to consist of three layers. Each layer is stacked with 4, 16, and 12 basic blocks, where each basic block consists of three 3$\times$3 convolutions. Next, we define Hi-ResNet V1 by stacking 1, 4, and 3 HRM\_4, where each HRM\_4 is composed of four IA blocks. For Hi-ResNet V2 and Hi-ResNet V3, we increased the number of IA blocks in the HRM\_4 of the second and third layers to 12 for ablative experiments. Table \ref{table:4_2} displays the results of the ablative experiments. Surprisingly, despite the decision to lengthen layer3 leading to more than double the number of network parameters, the improvement in accuracy is smaller than that achieved by lengthening layer2. Through sampling analysis of the feature output from the two layers, we argue that there is some feature loss when extracting information in layer2, resulting in a reduction of the semantic features of medium and low resolution that layer3 can obtain. Lengthening layer2 effectively solves this problem, allowing for the extraction of richer and more accurate spatial information and better fitting of the features of HRS images in tasks. Therefore, we choose to lengthen layer2 by three times. After determining the module size, we attempt to remove redundant modules in the framework. We speculate that lengthening layer2 means that the information extracted by the module can fully encompass the information extracted by layer3, so we attempt to remove layer3. This decision significantly reduces the number of model parameters, speeds up the training process, and further improves the efficiency of the model, while having little impact on the final performance of the model. Ultimately, we set Hi-ResNet) to include two layers, and named HRM\_4 stacked 12 IA blocks HRM\_12. Compared to the initial assumption, the final model achieves a 10\% increase in mIoU on the Vaihingen dataset and reduces the number of parameters by 30\%.

\begin{table}[!ht]
  \begin{center}
  \footnotesize
  \begin{threeparttable}
    \caption{RESULTS OF THE MODULE ANALYSIS EXPERIMENTS ON VAIHINGEN DATASET}
    \begin{tabular}{c|c c c c c}
    \hline
      \text{Method} & \text{Params (M)} & \text{FLOPs (G)} & \text{Memory (MB)} & \text{mIoU}\\
      \hline
       Hi-ResNet base & 63.4& \textbf{35.60} & \textbf{340.20} & 63.8\\
       Hi-ResNet v1 & 74.31& 44.20 & 367.80 & 65.4\\
       Hi-ResNet v2 & 106.39 & 67.66 & 537.45 & 71.5\\
       Hi-ResNet v3 & 106.42 & 67.67 & 562.82 & 71.1 \\
       ours & \textbf{54.33} & 49.70 & 490.90 & \textbf{74.8}\\
      \hline
    \end{tabular}
    \end{threeparttable}
  \end{center}
  \label{table:4_2}
\end{table}

\subsubsection{Stability} 
To validate the stability of the proposed model, this study conducted experiments on the Vaihingen dataset using various input sizes during training, including square sizes of 256$\times$256, 512$\times$512, and 1024$\times$ 024, as well as rectangular sizes of 256$\times$512 and 512$\times$1024. As shown in Table \ref{table:5}, the Hi-ResNet presented in this study exhibited a mIoU deviation of less than 0.8\% for inputs of different sizes, with the best performance observed for input sizes of 512$\times$512. When training with HRS images, the proposed network yielded improved segmentation results for buildings and no significant loss in accuracy for the ``car'' class, indicating its effectiveness in segmenting small objects in HRS images. In addition, Moreover, the small difference between the mIoU achieved for large objects with an input image size of 256$\times$256 and the best mIoU demonstrates that the proposed model has a larger receptive field.


\begin{table}[!ht]
  \begin{center}
  \begin{threeparttable}
   \setlength{\tabcolsep}{2mm}{
    \caption{RESULTS OF DIFFERENT INPUT SIZE ON THE VAIHINGEN DATASET}
    \begin{tabular}{c|c c c c c c c}
    \hline
     $^{\scriptscriptstyle1}$\text{Method} & $^{\scriptscriptstyle2}$\text{Imp.surf} & \text{Building} & \text{Lowveg} & \text{Tree} & \text{Car} & \text{mIoU}\\
      \hline
      256x256 & 85.3 & 90.1 & 72.1 & 80.2 & 70.1 & 79.5 \\
      256x512 & 84.1 & 90.1 & 71.1 & 80.2 & 69.9 & 79.1\\
      512x512 & 85.5 & 90.6 & 72.5 & 80.3 & 70.2 & \textbf{79.8}\\
      512x1024 & 84.8 & 90.2 & 71.8 & 80.1 & 69.8 & 79.3\\
      1024x1024 & 85.5 & 90.2 & 72.1 & 80.1 & 69.9 & 79.6\\
      \hline
    \end{tabular}}
        \begin{tablenotes}
        \footnotesize  
        \item[1] All number here refers to IoU.
        \item[2] Imp.surf: impervious surfaces, Lowveg: low vegetation.
        
      \end{tablenotes} 
      \end{threeparttable}
  \end{center}
  \label{table:5}
\end{table}

\subsubsection{Pre-training Comparison}
To evaluate the impact of pre-training strategies on downstream HRS tasks, this study conducts an ablation study on Hi-ResNet using different pre-training schemes. For supervised pre-training, the original Mapillary dataset~\cite{neuhold2017mapillary} is randomly cropped into 256$\times$256 pixels, and 2 million category-balanced HRS images are selected as the pre-training dataset. The batch size is set to 80, and the base learning rate is set to \num{5e-5}. The maximum iteration number of this pre-training is 3,125,000, achieving 51.8 on the Mapillary validation set. For unsupervised pre-training, the MillionAID dataset~\cite{long2021creating} with randomly cropped 512$\times$512 pixels is used. The learning rate is set to 0.015, the batch size is 64, and the maximum iteration number is 3,125,000. The final top-1 accuracy on MillionAID is 78.9, and the top-5 accuracy is 94.1. We conduct a comprehensive evaluation of HRS pre-training models on LoveDA, Potsdam, and Vaihingen datasets, and the detailed information is presented in Table \ref{table:6}.


\begin{table}[!ht]
  \begin{center}
  \begin{threeparttable}
   \setlength{\tabcolsep}{3mm}{
    \caption{RESULTS OF THE PRETRAIN ANALYSIS EXPERIMENTS}
    \begin{tabular}{c|c | c c c c c}
      \hline
        $^{\scriptscriptstyle1}$\multirow{2}{*}{Method} & \multirow{2}{*}{Max iteration} & \multicolumn{3}{c}{IoU per class(\%)}\\
        \cline{3-5}
          & & LoveDA & Potsdam & Vaihingen\\
        \hline
      $^{\scriptscriptstyle2}$NP & - & 46.4 & 76.8 & 67.4\\
      SP & 3,125,000 & 50.8 & 82.2 & 76.1\\
      MoCo & 3,125,000 & \textbf{52.5} & \textbf{86.1} & \textbf{79.8}\\
      \hline
    \end{tabular}}
    \begin{tablenotes}
        \footnotesize  
        \item[1] All number here refers to IoU. 
        \item[2] NP: No pre-training, SP: supervised pre-training, MoCo: unsupervised pre-training using MoCoV2
        
      \end{tablenotes} 
    \end{threeparttable}
  \end{center}
  \label{table:6}
\end{table}

Two pre-trained models are loaded onto the original network, and the results showed that unsupervised pre-training of HRS images using MoCov2~\cite{he2020momentum}  can provide a 15\% increase in mIoU, while supervised pre-training only increases mIoU by 8\% under the same number of iterations. The experiment demonstrates that unsupervised remote sensing pre-training can significantly improve the performance of the model on a small data set and make the model converge faster. Furthermore, using MoCov2 provides a deeper feature representation for HRS downstream tasks. In this sense, pre-trained models using contrastive learning methods can offer competitive backbones for future research in the field of HRS.

\subsubsection{Model Size}
In addition to accuracy and precision, the complexity and speed of a model are equally important for HRS tasks. A lightweight network architecture is undoubtedly advantageous for predicting large-scale HRS images. Therefore, we use the test dataset of the LoveDA dataset to compare the proposed network with the advanced model in terms of parameter amount, GPU memory occupation and complexity, and the comparison results are shown in Table \ref{table:7}. Compared to the less complex DeepLabV3+ network~\cite{chen2018encoder}, our network performed 11\% percent higher on the mIoU, and achieves a 2\% improvement on mIoU while using only 1/50th of the memory compared to the advanced model using the vision transformer backbone. It’s also worth noting that the proposed model achieves a performance improvement of 6\% on mIoU while having fewer parameters than some CNN-based models such as DeepLabV3+~\cite{chen2018encoder} and HRNet~\cite{wang2020deep}. These results suggest that the proposed model strikes a good balance between accuracy and efficiency, making it a promising approach for practical remote sensing applications.
\begin{table}[!ht]
  \scriptsize
  \begin{center}
   \setlength{\tabcolsep}{1mm}{
    \caption{EFFICIENCY OF THE REFERENCE NETWORKS ON THE VAIHINGEN DATASET}
    \begin{tabular}{l|l c c c c}
    \hline
      \text{Method} & \text{backbone} & \text{Params(M)} & \text{Memory(M)} & \text{Complexity(G)} & \text{mIoU}\\
      \hline
      TransUNet~\cite{chen2021transunet} & ViT-R50 & 90.7 & 1245.7 & 233.7 & 48.9\\
      UperNet~\cite{xiao2018unified} & ViT-B+VSA & 113.9 & 25343 & 413.26 & 51.2\\
      SwinUNet~\cite{cao2023swin} & Swin-Tiny & 43.2 & 2001.5 & 233.7 & 47.1\\
      DeepLabV3+~\cite{chen2018encoder} & ResNet50 & 80.0 & \textbf{290.8} & \textbf{20.1} & 49.6 \\
      HRNet~\cite{wang2020deep} & HRNet-W48 & 74.3 & 367.8 & 44.2 & 49.7 \\
      \hline
      Hi-ResNet & Hi-ResNet & \textbf{54.3} & 490.9 & 49.7 & \textbf{52.5} \\
      \hline
    \end{tabular}}
  \end{center}
  \label{table:7}
\end{table}
\begin{table*}[!t]
  \scriptsize
  \begin{center}
  \begin{threeparttable}
   \setlength{\tabcolsep}{3.5mm}{
    \caption{PERFORMANCE OF THE REFERENCE METHODS AND THE PROPOSED
Hi-ResNet METHOD ON THE LOVEDA DATASET}
    \begin{tabular}{c|c c c c c c c c c c c}
    \hline
      \text{Method} & \text{Backbone} & \text{Background} & \text{Building} & \text{Road} & \text{Water} & \text{Barren} & \text{Forest} & \text{Agriculture} & \text{mIoU} & \text{Complexity}\\
      \hline
      PSPNet~\cite{zhao2017pyramid} & ResNet50 & 44.4 & 52.1 & 53.5 & 76.5 & 9.7 & 44.1 & 57.9 & 48.3 & 105.7 \\
      DeepLabV3+~\cite{chen2018encoder}& ResNet50 & 43.0 & 50.9 & 52.0 & 74.4 & 10.4 & 44.2 & 58.5 & 47.6 & 95.8 \\
      SemanticFPN~\cite{kirillov2019panoptic} & ResNet50 & 42.9 & 51.5 & 53.4 & 74.7 & 11.2 & 44.6 & 58.7 & 48.2 & 103.3 \\
      FarSeg~\cite{zheng2020foreground}& ResNet50 & 43.1 & 51.5 & 53.9 & 76.6 & 9.8 & 43.3 & 58.9 & 48.2 & - \\
      RSSFormer~\cite{xu2023rssformer}& RSS-B & 52.3 & 60.7 & 55.2 & 76.2 & 18.7 & 45.3 & 58.3 & $^{\scriptscriptstyle1}$\textcolor{green}{52.3} & - \\
      FactSeg~\cite{ma2021factseg} & ResNet50 & 42.6 & 53.6 & 52.8 & 76.9 & 16.2 & 42.9 & 57.5 & 48.9 & - \\
      BANet~\cite{wang2021transformer} & ResT-Lite & 43.7 & 51.5 & 51.1 & 76.9 & 16.6 & 44.9 & 62.5 & 49.6 & 52.6 \\
      TransUNet~\cite{chen2021transunet} & ViT-R50 & 43.0 & 56.1 & 53.7 & 78.0 & 9.3 & 44.9 & 56.9 & 48.9 & 803.4 \\
      Segmenter~\cite{strudel2021segmenter}& ViT-Tiny & 38.0 & 50.7 & 48.7 & 77.4 & 13.3 & 43.5 & 58.2 & 47.1 & \textcolor{red}{26.8} \\
      SegFormer~\cite{xie2021segformer}& MIT-B5 & 43.1 & 52.3 & 55.0 & 70.7 & 10.7 & 43.2 & 56.8 & 47.4 & - \\
      SwinUperNet~\cite{liu2021swin} & Swin-Tiny & 43.3 & 54.3 & 54.3 & 78.7 & 14.9 & 45.3 & 59.6 & 50.0 & 349.1 \\
      DC-Swin~\cite{wang2022novel} & Swin-Tiny & 41.3 & 54.5 & 56.2 & 78.1 & 14.5 & 47.2 & 62.4 & 50.6 & 183.8 \\
      UNetFormer~\cite{wang2022unetformer} & ResNet18 & 44.7 & 58.8 & 54.9 & 79.5 & 20.1 & 46.0 & 62.5 & \textcolor{blue}{52.4} & \textcolor{blue}{46.9} \\
      UNet~\cite{xiao2018unified}& ResNet50 & 43.1 & 52.7 & 52.8 & 73.0 & 10.3 & 43.1 & 59.9 & 47.8 & - \\
      UNet++~\cite{zhou2018unet++} & ResNet50 & 42.9 & 52.6 & 52.8 & 74.5 & 11.4 & 44.4 & 58.8 & 48.2 & - \\
      HRNet~\cite{wang2020deep} & W32 & 44.6 & 55.3 & 57.4 & 78.0 & 11.0 & 45.3 & 60.9 & 49.8 & - \\
      \hline
      Hi-ResNet & Hi-ResNet & 46.7 & 58.3 & 55.9 & 80.1 & 17.0 & 46.7 & 62.7 & \textcolor{red}{52.5} & \textcolor{green}{49.7} \\
      \hline
    \end{tabular}}
        \begin{tablenotes}
        \footnotesize  
        \item[1] The top three scores in each metric are marked by \textcolor{red}{red} , \textcolor{blue}{blue} and \textcolor{green}{green}. 
      \end{tablenotes} 
    \end{threeparttable}
  \end{center}
  \label{table:8}
\end{table*}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{LoveDA}
    \caption{Visual results of different methods on the LoveDA dataset. From left to right: original image, ground truth, results of SegFormer~\cite{xie2021segformer}, results of HRNet~\cite{wang2020deep}, results of FarSeg~\cite{zheng2020foreground}, results of FactSeg~\cite{ma2021factseg}, results of RSSFormer~\cite{xu2023rssformer}, and results of our Hi-ResNet.}
    \label{fig:loveda}
\end{figure*}

\begin{table*}[!t]
  \scriptsize
  \begin{center}
  \begin{threeparttable}
   \setlength{\tabcolsep}{5mm}{
    \caption{PERFORMANCE OF THE REFERENCE METHODS AND THE PROPOSED
Hi-ResNet METHOD ON THE POTSDAM DATASET}
    \begin{tabular}{c|c c c c c c c c c}
    \hline
      \text{Method} & \text{Backbone} & $^{\scriptscriptstyle1}$\text{Imp.surf} & \text{Building} & \text{Lowveg} & \text{Tree} & \text{Car} & \text{MeanF1} & \text{OA} & \text{mIoU}\\
      \hline
      ERFNet~\cite{romera2017erfnet} & - & 88.7 & 93.0 & 81.1 & 75.8 & 90.5 & 85.8 & 84.5 & 76.2\\
      BiSeNet~\cite{yu2018bisenet} & ResNet18 & 90.2 & 94.6 & 85.5 & 86.2 & 92.7 & 89.8 & 88.2 & 81.7\\
      DANet~\cite{fu2019dual} & - & 89.9 & 93.2 & 83.6 & 82.3 & 92.6 & 88.3 & 86.7 & 79.6\\
      ShelfNet~\cite{zhuang2019shelfnet} & ResNet18 & 92.5 & 95.8 & 86.6 & 87.1 & 94.6 & 91.3 & 89.9 & 84.4\\
      FANet~\cite{hu2020real} & ResNet18 & 92.0 & 96.1 & 86.0 & 87.8 & 94.5 & 91.3 & 89.8 & 84.2\\
      EaNet~\cite{zheng2020parsing} & ResNet18 & 92.0 & 95.7 & 84.3 & 85.7 & 95.1 & 90.6 & 88.7 & 83.4\\
      BANet~\cite{wang2021transformer} & ResT-Lite & 93.3 & 96.7 & 87.4 & 89.1 & 96.0 & \textcolor{blue}{92.5} & \textcolor{green}{91.0} & \textcolor{blue}{86.3}\\
      Segmenter~\cite{strudel2021segmenter}& ViT-Tiny & 91.5 & 95.3 & 85.4 & 85.0 & 88.5 & 89.2 & 88.7 & 80.7\\
      BotNet~\cite{srinivas2021bottleneck} & ResNet50 & 92.3 & 96.3 & 87.3 & 88.7 & 94.1 & 91.7 & 90.4 & -\\
      SwiftNet~\cite{orvsic2021efficient} & ResNet18 & 91.8 & 95.9 & 85.7 & 86.8 & 94.5 & 91.0 & 89.3 & 83.8\\
      MAResU-Net~\cite{li2021multistage} & ResNet18 & 91.4 & 95.6 & 85.8 & 86.6 & 93.3 & 90.5 & 89.0 & 83.9\\
      LANet~\cite{ding2020lanet} & ResNet50 & 93.0 & 97.1 & 87.3 & 88.0 & 94.1 & 91.9 & 90.8 & -\\
      HRNet~\cite{wang2020deep}& HRNet-W48 & 88.7 & 93.4 & 83.0 & 81.5 & 91.1 & 87.5 & 86.1 & 78.1\\
      UNetFormer~\cite{wang2022unetformer} & ResNet18 & 93.6 & 97.7 & 87.7 & 88.9 & 96.5 & \textcolor{red}{92.8} & \textcolor{red}{91.3} & \textcolor{red}{86.8}\\
      SwinUperNet~\cite{liu2021swin} & Swin-Tiny & 93.2 & 96.4 & 87.6 & 88.6 & 95.4 & 92.2 & 90.9 & 85.8\\
      FCN~\cite{long2015fully} & ResNet50 & 91.4 & 96.6 & 85.9 & 86.9 & 82.2 & 88.6 & 85.6 & -\\
      FCN~\cite{long2015fully} & VGG16 & 88.6 & 93.3 & 83.3 & 79.8 & 93.0 & 87.61 & 85.59 & -\\
      \hline
      Hi-ResNet & Hi-ResNet & 93.2 & 96.5 & 87.9 & 88.6 & 96.1 & \textcolor{green}{92.4} & \textcolor{blue}{91.1} & \textcolor{green}{86.1}\\
      \hline
    \end{tabular}}
    \begin{tablenotes}
        \footnotesize  
        \item[1] Imp.surf: impervious surfaces. Lowveg: low vegetation. 
      \end{tablenotes} 
    \end{threeparttable}
  \end{center}
  \label{table:9}
\end{table*}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{potsdam}
    \caption{Visualization results for the Potsdam validation set. From left to right: original image, ground truth, results of HRNet~\cite{wang2020deep}, results of ERFNet~\cite{romera2017erfnet}, results of DANet~\cite{fu2019dual}, results of Segmenter~\cite{strudel2021segmenter}, results of FCN~\cite{long2015fully}, and results of our Hi-ResNet.}
    \label{fig:potsdam}
\end{figure*}
\subsection{Results on The Dataset}
\subsubsection{LoveDA}
The LoveDA dataset is recognized as a challenging HSR dataset for Land Cover Domain Adaptive Semantic Segmentation. This dataset presents three significant challenges for large-scale remote sensing mapping, namely multi-scale targets, complex background samples, and inconsistent class distributions. As a result, achieving high scores on this dataset is quite difficult.

Table \ref{table:8} demonstrates the excellent performance of the network proposed in this paper on the LoveDA dataset. Thanks to the precise loss strategy, we can handle complex samples of different backgrounds well on LoveDA and achieve a mIoU of 52.5 on the official test set. Our network outperforms the HRNet~\cite{wang2020deep}, loaded with officially provided pre-trained weights, by 5\% on mIoU and FactSegNet~\cite{ma2021factseg}, an excellent small-object semantic segmentation network, by 6\%. It is worth noting that for the ``barren'' class, where most networks underperform, our mIoU is 3\% higher than other methods. Whether in urban or rural scenarios, sparse or dense distribution, our network can accurately segment objects with high confidence. We also provide visual comparison results with other methods in Figure \ref{fig:loveda}.

To be more specific, due to limited computing resources, we perform only 3-4 complete pre-training processes (150 epochs), which makes pre-training the depth provided by Hi-ResNet compared with the pre-training weights provided by other officials, there is a certain gap between the hierarchical representation information. However, the ablation experiments in Table \ref{table:4} have fully demonstrated the excellent performance of Hi-ResNet in terms of performance and accuracy.


\subsubsection{Potsdam}
As a widely-used dataset for segmentation tasks, Potsdam can comprehensively demonstrate the improvement of the accuracy of HRS images by the model proposed in this paper. Table \ref{table:9} shows the scores achieved on the Potsdam dataset. The network proposed in this paper achieves an F1 of 92.4 and a mIoU of 86.1 on the Potsdam dataset. Hi-ResNet outperforms the lightweight convolutional network FANet~\cite{hu2020real} and the lightweight transformer-based network Segmenter~\cite{strudel2021segmenter}. Notably, Hi-ResNet performs best among all methods for the ``lowveg''
 class, achieving a score of 87.9. The car class also obtains a higher score second only to UnetFormer~\cite{wang2022unetformer}, with a mean F1 of 96.1. This result fully demonstrates that the Hi-ResNet has better performance for small target segmentation in HRS images.

We present Potsdam segmentation results to showcase the effectiveness of Hi-ResNet for small object segmentation. As displayed in Figure \ref{fig:potsdam}, most networks perform poorly in the segmentation of object edges. To overcome this limitation, Hi-ResNet employs CEA loss in the loss calculation to maximize the distance between the two boundaries, ensuring good connectivity of the extracted edge features during loss calculation, thus avoiding category boundary blur. At the same time, small cars covered by shadows or branches on the ground are difficult to identify, while vehicles on narrow roads are easily misidentified as categories close to vehicles. In Hi-ResNet, the design of the IB block can extract richer low-level semantic information such as contours and shapes in the funnel module, thus increasing inter-class differences in HRS images. Additionally, Hi-ResNet extracts more accurate small target location information through the muti-branch module while expanding the effective global receptive field through the CA module to capture global context information effectively. This enhances the network's ability to identify and segment occluded and covered cars more clearly.
\subsubsection{Vaihingen}
\begin{table*}[!t]
  \scriptsize
  \begin{center}
   \setlength{\tabcolsep}{5mm}{
    \caption{PERFORMANCE OF THE REFERENCE METHODS AND THE PROPOSED
Hi-ResNet METHOD ON THE VAIHINGEN DATASET}
    \begin{tabular}{c|c c c c c c c c c}
    \hline
      \text{Method} & \text{Backbone} & \text{Imp.surf} & \text{Building} & \text{Lowveg} & \text{Tree} & \text{Car} & \text{MeanF1} & \text{OA} & \text{mIoU}\\
      \hline
      ERFNet~\cite{romera2017erfnet} & - & 88.5 & 90.2 & 76.4 & 85.8 & 53.6 & 78.9 & 85.8 & 69.1\\
      PSPNet~\cite{zhao2017pyramid} & - & 89.0 & 93.2 & 81.5 & 87.7 & 43.9 & 79.0 & 87.7 & 68.6\\
      HRNet~\cite{wang2020deep} & HRNet-W48 & 89.8 & 92.8 & 81.0 & 86.8 & 79.5 & 86.0 & 87.6 & 75.8\\
      BiSeNet~\cite{yu2018bisenet} & ResNet18 & 89.1 & 91.3 & 80.9 & 86.9 & 73.1 & 84.3 & 87.1 & 75.8\\
      DABNet~\cite{li2019dabnet} & - & 90.0 & 88.8 & 74.3 & 84.9 & 60.2 & 79.2 & 84.3 & 70.2\\
      DANet~\cite{fu2019dual} & ResNet18 & 90.0 & 93.9 & 82.2 & 87.3 & 44.5 & 79.6 & 88.2 & 69.4\\
      DeppLabV3+\cite{chen2018encoder}& ResNet18 & 89.9 & 93.9 & 80.6 & 89.4 & 83.3 & 87.4 & 89.0 & -\\
      ABCNet~\cite{li2021abcnet} & ResNet18 & 92.7 & 95.2 & 84.5 & 89.7 & 85.3 & \textcolor{blue}{89.5} & \textcolor{blue}{90.7} & \textcolor{blue}{81.3}\\
      FANe~\cite{hu2020real} & ViT-Tiny & 90.7 & 93.8 & 82.6 & 88.6 & 71.6 & 85.4 & 88.9 & 75.6\\
      EaNet~\cite{zheng2020parsing} & ResNet50 & 91.7 & 94.5 & 83.1 & 89.2 & 80.0 & 87.7 & 89.7 & 78.7\\
      BoTNet~\cite{srinivas2021bottleneck} & ResNet18 & 89.9 & 92.1 & 81.8 & 88.7 & 71.3 & 84.8 & 88.0 & 74.3\\
      MAResU-Net~\cite{li2021multistage}& ResNet18 & 92.0 & 95.0 & 83.7 & 89.3 & 78.3 & 87.7 & 90.1 & 78.6\\
      UNetFormer~\cite{wang2022unetformer}& ResNet18 & 92.7 & 95.3 & 84.9 & 90.6 & 88.5 & \textcolor{red}{90.4} & \textcolor{red}{91.0} & \textcolor{red}{82.7}\\
      ShelfNet~\cite{zhuang2019shelfnet} & ResT-Lite & 91.8 & 94.6 & 83.8 & 89.3 & 77.9 & 87.5 & 89.8 & 78.3\\
      Segmenter~\cite{strudel2021segmenter} & ViT-Tiny & 89.8 & 93.0 & 81.2 & 88.9 & 67.6 & 84.1 & 88.1 & 73.6\\
      SwiftNet~\cite{orvsic2021efficient}& ResNet18 & 92.2 & 94.8 & 84.1 & 89.3 & 81.2 & 88.3 & 90.2 & 79.6\\
      \hline
      Hi-ResNet & Hi-ResNet & 92.3 & 95.1 & 84.9 & 88.5 & 83.5 & \textcolor{green}{89.1} & \textcolor{blue}{90.7} & \textcolor{green}{79.8}\\
      \hline
    \end{tabular}}
  \end{center}
  \label{table:10}
\end{table*}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{vaihingen}
    \caption{Visualization results for the Vaihingen validation set. From left to right: original image, ground truth, results of HRNet~\cite{wang2020deep}, results of PSPNet~\cite{zhao2017pyramid}, results of DANet~\cite{li2019dabnet}, results of Segmenter~\cite{strudel2021segmenter}, results of DeepLabv3+~\cite{chen2018encoder}, and results of our Hi-ResNet.}
    \label{fig:vaihingen}
\end{figure*}

The Vaihingen dataset has a large number of houses obscured by tree branches and multi-story small villages, so the dataset requires the network to identify and segment small targets more accurately. To test the network's accuracy, we selected 17 images from the Vaihingen dataset and present the prediction results of the Hi-ResNet in Table \ref{table:10}. Our proposed network achieved an OA of 90.7 and a mIoU of 79.8 on the Vaihingen dataset. In the low vegetation category, Hi-ResNet secured first place with the same performance on the Potsdam dataset, with a six percent improvement over the results obtained by the HRNet network for both the ``building'' and ``car'' classes. This is because Hi-ResNet effectively solves the sample imbalance problem caused by small targets occupying small pixels in HRS images by using the CEA loss and GD loss to weigh each category. 
We show some typical segmentation results in vaihingen in Figure \ref{fig:vaihingen}. In Figure \ref{fig:vaihingen}, the ``Tree'' class and the ``low vegetable'' class have serious misclassification, and at the same time, the within-class distance segmentation redundancy of the dense small target object ``car'' class, and the edge segmentation of the small cars is not clear. Hi-ResNet can obtain the position and edge information of small cars more accurately when the global receptive field is increased, thereby avoiding misclassification in complex scenes. The accurate segmentation of opaque ground in Figure \ref{fig:vaihingen} also demonstrates the effectiveness of the network proposed in this paper.

\section{Conclusion}
Our study centers on the semantic segmentation of HRS, specifically focusing on addressing the inherent challenges of object scale and shape variance, and complex background environments. These issues often lead to object misclassification and sub-optimal outcomes with current learning algorithms. We respond by developing Hi-ResNet, which stands out due to an efficient network structure that includes a funnel module, a multi-branch module embedded with IA blocks, and a feature refinement module. Additionally, we introduce the CEA loss function. In our approach, the funnel module functions to downsample and extract high-resolution semantic information from the input image. The process then moves to the multi-branch module with stacks of IA blocks, enabling the capture of image features at different scales and distinguishing variant scales and shapes within the same class. Our study concludes with the integration of the CEA loss function within our feature refinement module. This innovative step effectively disambiguates inter-class objects with similar shapes and increases the data distribution distance for accurate predictions. The superiority of Hi-ResNet is proven through a comparative evaluation with leading methodologies across LoveDA benchmarks. The results underscore the value of our contributions to advancing HRS semantic segmentation.

\bibliographystyle{IEEEtran}
\bibliography{reference}
\end{document}
