\section{Experiments}
We present both conditional and unconditional samples generated on the PTB and IMDB data sets at word-level.  MaskGAN refers to our GAN-trained variant and MaskMLE refers to our maximum-likelihood trained variant.  Additional samples are supplied in Appendix \ref{samples_app}.

\subsection{The Penn Treebank (PTB)}
The Penn Treebank dataset \citep{marcus1993building} has a vocabulary of 10,000 unique words. The training set contains 930,000 words, the validation set contains 74,000 words and the test set contains 82,000 words.  For our experiments, we train on the training partition.

We first pretrain the commonly-used variational LSTM language model with parameter dimensions common to MaskGAN following \cite{gal2016} to a validation perplexity of 78.  After then loading the weights from the language model into the MaskGAN generator we further pretrain with a masking rate of  (half the text blanked) to a validation perplexity of 55.3.  Finally, we then pretrain the discriminator on the samples produced from the current generator and real training text.

\subsubsection{Conditional Samples} 
We produce samples conditioned on surrounding text in Table \ref{tab:ptb_cond}.  Underlined sections of text are missing and have been filled in via either the MaskGAN or MaskMLE algorithm.  
\begin{table} [ht]
  \begin{tabular}{LL} \toprule
    \textbf{Ground Truth} & \multicolumn{1}{m{10cm}}{\textbf{the next day 's show eos interactive telephone technology has taken a new leap in unk and television programmers are}} \\ \midrule
    
    MaskGAN & \multicolumn{1}{m{10cm}}{the next day 's show eos interactive telephone technology has taken a new leap \uline{in its retail business eos a}}  \\\midrule

    MaskMLE & \multicolumn{1}{m{10cm}}{the next day 's show eos interactive telephone technology has taken a new leap \uline{in the complicate case of the}}  \\ \bottomrule
  \end{tabular}
  \caption{Conditional samples from PTB for both MaskGAN and MaskMLE models.}
  \label{tab:ptb_cond}
\end{table}

\subsubsection{Language Model (Unconditional) Samples}
We may also run MaskGAN in an unconditional mode, where the entire context is blanked out, thus making it equivalent to a language model.  We present a length-20 language model sample in Table \ref{tab:ptb_lm} and additional samples are included in the Appendix. 

\begin{table}[ht]
  \begin{tabular}{LL} \toprule
    MaskGAN & \multicolumn{1}{m{10cm}}{oct. N as the end of the year the resignations were approved eos the march N N unk was down} \\\bottomrule

  \end{tabular}
  \caption{Language model (unconditional) sample from PTB for MaskGAN.}
  \label{tab:ptb_lm}
\end{table}



\subsection{IMDB Movie Dataset}
The IMDB dataset \cite{maas2011learning} consists of 100,000 movie reviews taken from IMDB.  Each review may contain several sentences.  The dataset is divided into 25,000 labeled training instances, 25,000 labeled test instances and 50,000 unlabeled training instances.  The label indicates the sentiment of the review and may be either positive or negative.  We use the first 40 words of each review in the training set to train our models, which leads to a dataset of 3 million words.

Identical to the training process in PTB, we pretrain a language model to a validation perplexity of 105.6.  After then loading the weights from the language model into the MaskGAN generator we further pretrain with masking rate of  (half the text blanked) to a validation perplexity of 87.1.  Finally, we then pretrain the discriminator on the samples produced from the current generator and real training text.

\subsubsection{Conditional Samples}
Here we compare MaskGAN and MaskMLE conditional language generation ability for the IMDB dataset. 

\begin{table} [ht]
  \begin{tabular}{LL} \toprule
    \textbf{Ground Truth} & \multicolumn{1}{m{10cm}}{\textbf{Pitch Black was a complete shock to me when I first saw it back in 2000 In the previous years I}} \\ \midrule
    
    MaskGAN & \multicolumn{1}{m{10cm}}{Pitch Black was a complete shock to me when I first saw it back in \uline{1979 I was really looking forward} }  \\ \midrule

    MaskMLE & \multicolumn{1}{m{10cm}}{Black was a complete shock to me when I first saw it back in \uline{1969 I live in New Zealand}}  \\ \bottomrule
  \end{tabular}
  \caption{Conditional samples from IMDB for both MaskGAN and MaskMLE models.}
\end{table}

\subsubsection{Language Model (Unconditional) Samples}
As in the case with PTB, we generate IMDB samples unconditionally, equivalent to a language model.  We present a length-40 sample in Table \ref{tab:imdb_lm} and additional samples are included in the Appendix.

\begin{table} [ht]
\begin{tabular}{LL} \toprule
  MaskGAN & \multicolumn{1}{m{10cm}}{\textbf{Positive}: Follow the Good Earth movie linked Vacation is a comedy that credited against the modern day era yarns which has helpful something to the modern day s best It is an interesting drama based on a story of the famed} \\ \bottomrule
\end{tabular}
\caption{Language model (unconditional) sample from IMDB for MaskGAN.}
\label{tab:imdb_lm}
\end{table}







\subsection{Perplexity of generated samples}
As of this date, GAN training has not achieved state-of-the-art word level validation perplexity on the Penn Treebank dataset.  Rather, the top performing models are still maximum-likelihood trained models, such as the recent architectures found via neural architecture search in \cite{zoph2016neural}.  An extensive hyperparameter search with MaskGAN further supported that GAN training does not improve the validation perplexity results set via state-of-the-art models.  However, we instead seek to understand the quality of the \textit{sample generation}.  As highlighted earlier, a fundamental problem of generating in free-running mode potentially leads to `off-manifold` sequences which can result in poor sample quality for teacher-forced models.  We seek to quantitatively evaluate this dynamic present only during sampling. This is commonly done with BLEU but as shown by \cite{gnmt}, BLEU is not necessarily correlated with sample quality. We believe the correlation may be even less in the in-filling task since there are many potential valid in-fillings and BLEU would penalize valid ones.

Instead, we calculate the perplexity of the generated samples by MaskGAN and MaskMLE by using the language model that was used to initialize MaskGAN and MaskMLE.  Both MaskGAN and MaskMLE produce samples autoregressively (free-running mode), building upon the previously sampled tokens to produce the distribution over the next.    

\begin{table}
  \centering
  \begin{tabular}{lc}
    \toprule
    Model & Perplexity of IMDB samples under a pretrained LM \\
    \midrule
    MaskMLE & 273.1  3.5 \\
MaskGAN & 108.3  3.5 \\
\bottomrule
  \end{tabular}
  \caption{The perplexity is calculated using a pre-trained language model that is equivalent to the decoder (in terms of architecture and size) used in the MaskMLE and MaskGAN models. This language model was used to initialize both models.}
\end{table}

The MaskGAN model produces samples which are more likely under the initial model than the MaskMLE model.  The MaskMLE model generates improbable sentences, as assessed by the initial language model, during inference as compounding sampling errors result in a recurrent hidden states that are never seen during teacher forcing \citep{lamb2016professor}.  Conversely, the MaskGAN model operates in a free-running mode while training and this supports that it is more robust to these sampling perturbations.


\subsection{Mode collapse}
In contrast to image generation, mode collapse can be measured by directly calculating certain n-gram statistics. In this instance, we measure mode collapse by the percentage of unique n-grams in a set of 10,000 generated IMDB movie reviews. We unconditionally generate each sample (consisting of 40 words). This results in almost 400K total bi/tri/quad-grams.

\begin{table}[h]
  \centering
  \begin{tabular}{lccc}
    \toprule
    Model & \% Unique bigrams & \% Unique trigrams & \% Unique quadgrams \\
    \midrule
    LM & 40.6 & 75.2 & 91.9 \\
    MaskMLE & 43.6 & 77.4 & 92.6 \\
MaskGAN & 38.2 & 70.7 & 88.2 \\
\bottomrule
  \end{tabular}

  \caption{Diversity statistics within 1000 unconditional samples of PTB news snippets (20 words each).  \label{table:diversity}}
\end{table}

The results in Table~\ref{table:diversity} show that MaskGAN does show some mode collapse, evidenced by the reduced number of unique quadgrams. However, all complete samples (taken as a sequence) for all the models are still unique. We also observed during RL training an initial small drop in perplexity on the ground-truth validation set but then a steady increase in perplexity as training progressed. Despite this, sample quality remained relatively consistent. The final samples were generated from a model that had a perplexity on the ground-truth of 400. We hypothesize that mode dropping is occurring near the tail end of sequences since generated samples are unlikely to generate all the previous words correctly in order to properly model the distribution over words at the tail. \cite{theis2015note} also shows how validation perplexity does not necessarily correlate with sample quality. 



\subsection{Human evaluation}

Ultimately, the evaluation of generative models is still best measured by unbiased human evaluation.  Therefore, we evaluate the quality of the generated samples of our initial language model (LM), the MaskMLE model and the MaskGAN model in a blind heads-up comparison using Amazon Mechanical Turk. Note that these models have the same number of parameters at inference time. We pay raters to compare the quality of two extracts along 3 axes (grammaticality, topicality and overall quality). They are asked if the first extract, second extract or neither is higher quality.

\begin{table}[h]
  \centering
  \begin{tabular}{lccc}
    \toprule
    Preferred Model & Grammaticality \% & Topicality \% & Overall \% \\
    \midrule
    LM & 15.3 & 19.7 & 15.7 \\
    \textbf{MaskGAN} & 59.7 & 58.3 & 58.0 \\
    \midrule
    LM & 20.0 & 28.3 & 21.7 \\
    \textbf{MaskMLE} & 42.7 & 43.7 & 40.3 \\
    \midrule
    \textbf{MaskGAN} & 49.7 & 43.7 & 44.3 \\
    MaskMLE & 18.7 & 20.3 & 18.3 \\
    \midrule
    Real samples & 78.3 & 72.0 & 73.3 \\
    LM & 6.7 & 7.0 & 6.3 \\
    \midrule
    Real samples & 65.7 & 59.3 & 62.3 \\
    MaskGAN & 18.0 & 20.0 & 16.7 \\
    \bottomrule
  \end{tabular}
  \caption{A Mechanical Turk blind heads-up evaluation between pairs of models trained on IMDB reviews. 100 reviews (each 40 words long) from each model are unconditionally sampled and randomized. Raters are asked which sample is preferred between each pair. 300 ratings were obtained for each model pair comparison.}
\end{table}
\begin{table}[h]
  \centering
  \begin{tabular}{lccc}
    \toprule
    Preferred model & Grammaticality \% & Topicality \% & Overall \% \\
    \midrule
    LM & 32.0 & 30.7 & 27.3 \\
    \textbf{MaskGAN} & 41.0 & 39.0 & 35.3 \\
    \midrule
    \textbf{LM} & 32.7 & 34.7 & 32.0 \\
    MaskMLE &  37.3 & 33.3 & 31.3 \\
    \midrule
    \textbf{MaskGAN} & 44.7 & 33.3 & 35.0 \\
    MaskMLE & 28.0 & 28.3 & 26.3 \\
    \midrule
    \textbf{SeqGAN} & 38.7 & 34.0 & 30.7 \\
    MaskMLE & 33.3 & 28.3 & 27.3 \\
    \midrule
    SeqGAN & 31.7 & 34.7 & 32.0 \\
    \textbf{MaskGAN} & 43.3 & 37.3 & 37.0 \\
    \bottomrule
  \end{tabular}
  \caption{A Mechanical Turk blind heads-up evaluation between pairs of models trained on PTB. 100 news snippets (each 20 words long) from each model are unconditionally sampled and randomized. Raters are asked which sample is preferred between each pair. 300 ratings were obtained for each model pair comparison.}
\end{table}

The Mechanical Turk results show that MaskGAN generates superior human-looking samples to MaskMLE on the IMDB dataset. However, on the smaller PTB dataset (with 20 word instead of 40 word samples), the results are closer. We also show results with SeqGAN (trained with the same network size and vocabulary size) as MaskGAN, which show that MaskGAN produces superior samples to SeqGAN.
