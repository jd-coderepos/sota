
\section{Experiments}
\label{sec:experiments}



We show that \detr achieves competitive results compared to Faster R-CNN
in quantitative evaluation on COCO.
Then, we provide a detailed ablation study of the architecture and loss,
with insights and qualitative results.
Finally, to show that \detr is a versatile and extensible model,
we present results on panoptic segmentation, training only a small
extension on a fixed \detr model.
We provide code and pretrained models to reproduce our experiments at
\url{https://github.com/facebookresearch/detr}.
\subsubsection{Dataset.}
We perform experiments on COCO 2017 detection and panoptic segmentation datasets~\cite{Lin2014coco,panoptic_kirillov2019fpn},
containing \oldnew{115k}{118k} training images and 5k validation images.
Each image is annotated with bounding boxes and panoptic segmentation.
There are 7 instances per image on average, up to 63 instances in a single image
in training set, ranging from small to large on the same images. 
If not specified, we report AP as bbox AP, the integral metric over multiple thresholds.
\oldnew{}{For comparison with Faster R-CNN we report validation AP at the last training epoch, for ablations we report median over validation results from the last 10 epochs.}
\subsubsection{Technical details.}
\oldnew{We define a specific instantiation of our model with \detr[-DC5] notation, where  is the dimension of the transformer. Other hyper-parameters are the same for all models and are described in detail in the supplementary.}{}
\oldnew{We train \detr with AdamW~\cite{Loshchilov2017DecoupledWD}, with a weight decay of  and initial learning rates of  and  for the tranformer and the backbone respectively}{We train \detr with AdamW~\cite{Loshchilov2017DecoupledWD}
setting the initial transformer's learning rate to , the
backbone's to , and weight decay to .}
All transformer weights are initialized with Xavier \oldnew{initialization}{init}~\cite{Glorot2010UnderstandingTD},
and the backbone is \oldnew{an}{with} ImageNet-pretrained ResNet model~\cite{He2015DeepRL} from {\sc torchvision} with frozen batchnorm layers.
\oldnew{}{We report results with two different backbones: a ResNet-50 and a ResNet-101. The corresponding models are called respectively \detr and \detr-R101. Following ~\cite{li2017fully}, we also increase the feature resolution by adding a dilation to the last stage of the backbone and removing a stride from the first convolution of this stage. The corresponding models are called respectively \detr-DC5 and \detr-DC5-R101 (dilated C5 stage). This modification increases the resolution by a factor of two, thus improving performance for small objects, at the cost of a 16x higher cost in the self-attentions of the encoder, leading to an overall 2x increase in computational cost. A full comparison of FLOPs of these models and Faster R-CNN is given in Table \ref{table:frcnn}.}


We use scale augmentation, resizing the input images such that the shortest side is at least 480 and at most 800 pixels while the longest at most 1333~\cite{wu2019detectron2}.
To help learning global relationships through the self-attention of the encoder, we also apply random crop augmentations during training, improving the performance by approximately 1 AP.
Specifically, a train image is cropped with probability 0.5 to a
random rectangular patch which is then resized again to 800-1333.
The transformer is trained with default dropout of 0.1.
\oldnew{}{At inference time, some slots predict empty class. To optimize for AP, we override the prediction of these slots with the second highest scoring class, using the corresponding confidence. This improves AP by 2 points compared to filtering out empty slots.}
\oldnew{}{Other training hyperparameters can be found in section~\ref{sec:hyperparameters}.}
For our ablation experiments we use training schedule 
of 300 epochs with a learning rate drop by a factor of 10 after 200 epochs,
where a single epoch is a pass over all training images once.
\oldnew{Training the baseline model for 300 epochs on 128 V100 GPUs takes about 24 hours, with one image per GPU (hence a total batch size of 128).}
{Training the baseline model for 300 epochs on 16 V100 GPUs takes 3 days,
with 4 images per GPU (hence a total batch size of 64).}
For the longer schedule used to compare with Faster R-CNN we train
\oldnew{for 900 epochs with learning rate drops by 2 every 50 epochs starting from epoch 600. This schedule adds 3 AP compared to the short 300 epoch schedule.}
{for 500 epochs with learning rate drop after 400 epochs.
This schedule adds 1.5 AP compared to the shorter schedule.}


\subsection{Comparison with Faster R-CNN}

\begin{table}[t]
    \centering\small
    \caption{Comparison with Faster R-CNN with a ResNet-50 and ResNet-101 backbones on the COCO validation set.
    The top section shows results for Faster R-CNN models in Detectron2~\cite{wu2019detectron2},
    the middle section shows results for Faster R-CNN models
    with GIoU~\cite{Rezatofighi_2018_CVPR}, random crops train-time augmentation, and the long \texttt{9x} training schedule.
    \detr models achieve comparable results
    to heavily tuned Faster R-CNN baselines, having lower \AP{S} but greatly improved \AP{L}.
    \oldnew{}{We use torchscript Faster R-CNN and \detr models to measure FLOPS and FPS. Results without R101 in the name correspond to ResNet-50.}
    }
    \begin{tabular}{lcccccccc}
        \toprule
        Model & GFLOPS/FPS & \#params & AP & \AP{50} & \AP{75} & \AP{S} & \AP{M} & \AP{L} \\
        \midrule
        Faster RCNN-DC5 & 320/16 & 166M  & 39.0 & 60.5 & 42.3 & 21.4 & 43.5 & 52.5 \\
        Faster RCNN-FPN & 180/26 & 42M & 40.2 & 61.0 & 43.8 & 24.2 & 43.5 & 52.0 \\
        Faster RCNN-R101-FPN & 246/20 & 60M & 42.0 & 62.5 & 45.9 & 25.2 & 45.6 & 54.6 \\
        \midrule
        Faster RCNN-DC5+ & 320/16 & 166M & 41.1 & 61.4 & 44.3 & 22.9 & 45.9 & 55.0\\
        Faster RCNN-FPN+ & 180/26 & 42M & 42.0 & 62.1 & 45.5 & 26.6 & 45.4 & 53.4\\
        Faster RCNN-R101-FPN+ & 246/20 & 60M & 44.0 & 63.9 & \textbf{47.8} & \textbf{27.2} & 48.1 & 56.0\\
        \midrule
        {\detr} & 86/28 & 41M & 42.0  & 62.4  & 44.2  & 20.5  & 45.8  & 61.1 \\
        {\detr}-DC5 & 187/12 & 41M & 43.3  & 63.1  & 45.9  & 22.5  & 47.3  & 61.1 \\
        {\detr}-R101 & 152/20 & 60M & 43.5  & 63.8  & 46.4  & 21.9  & 48.0  & 61.8 \\
        \detr-DC5-R101 & 253/10 & 60M & \textbf{44.9}  & \textbf{64.7}  & 47.7  & 23.7  & \textbf{49.5}  & \textbf{62.3} \\
        \bottomrule
    \end{tabular}
    \label{table:frcnn}
\end{table}
 
Transformers are typically trained with Adam or Adagrad optimizers with
very long training schedules and dropout, and this is true for \detr as well.
Faster R-CNN, however, is trained with SGD with minimal data augmentation
and we are not aware of successful applications of Adam or dropout.
Despite these differences we attempt to make a Faster R-CNN baseline stronger.
To align it with \detr,
we add generalized IoU~\cite{Rezatofighi_2018_CVPR} to the box loss, the same random crop augmentation and long training known to
improve results~\cite{He2018RethinkingIP}.
Results are presented in Table~\ref{table:frcnn}.
In the top section we show Faster R-CNN results from Detectron2 Model Zoo~\cite{wu2019detectron2}
for models trained with the \texttt{3x} schedule.
In the middle section we show results (with a ``+'') for the same models
but trained with the \texttt{9x} schedule (109 epochs) and the described enhancements,
which in total adds 1-2 AP. In the last section of Table~\ref{table:frcnn}
we show the results for multiple \detr models.
To be comparable in the number of parameters we choose a model
with 6 transformer and 6 decoder layers of width 256\oldnew{}{ with 8 attention heads}.
Like Faster R-CNN with FPN this model has 41.3M parameters,
out of which 23.5M are in ResNet-50, and 17.8M are in the transformer.
Even though both Faster R-CNN and \detr are still likely to
further improve with longer training,
we can conclude that \oldnew{\detr-DC5}{\detr} can be competitive with Faster R-CNN
with the same number of parameters, achieving 42 AP on the COCO val subset.
\oldnew{The way \detr achieves this is by improving \AP{L}, however note that the model is still lagging behind in \AP{S}.}
{The way \detr achieves this is by improving \AP{L} (+7.8),
however note that the model is still lagging behind in \AP{S} (-5.5).
\detr-DC5 with the same number of parameters and similar FLOP count has higher AP,
but is still significantly behind in \AP{S} too.}
\oldnew{}{Faster R-CNN and \detr with ResNet-101 backbone show comparable results as well.}



\subsection{Ablations}

Attention mechanisms in the transformer decoder are the key components which model relations between feature representations of different detections. In our ablation analysis, we explore how other components of our architecture and loss influence the final performance. For the study we choose ResNet-50-based \detr model with 6 encoder, 6 decoder layers and width 256.
The model has 41.3M parameters,
achieves \oldnew{37.3 and 40.3 AP}{40.6 and 42.0 AP} on short and long schedules respectively,
and runs at \oldnew{23}{28} FPS, similarly to Faster R-CNN-FPN with the same backbone.
\subsubsection{Number of encoder layers.}
We evaluate the importance of global image-level self-attention by changing the number of encoder layers (Table~\ref{table:enc_layers}).
Without encoder layers, overall AP drops by \oldnew{3.4}{3.9} points, with a more significant drop of \oldnew{5.5}{6.0 AP} on large objects.
We hypothesize that, by using global scene reasoning, the encoder is important for disentangling objects.
In Figure~\ref{fig:encoder_self_attention}, we visualize the attention maps of the last encoder layer of a trained model, focusing on a few points in the image.
The encoder seems to separate instances already, which likely simplifies object extraction and localization for the decoder.
\setlength{\tabcolsep}{0.78em}
\begin{table}[t]
    \centering\small
    \caption{Effect of encoder size. Each row corresponds to a model with varied number of encoder layers and fixed number of decoder layers.
    Performance gradually improves with more encoder layers.\oldnew{, mostly due to large and medium objects}{}}
    \begin{tabular}{lccccccc}
        \toprule
        \#layers & GFLOPS/FPS & \#params & AP & \AP{50} & \AP{S} & \AP{M} & \AP{L} \\
        \midrule
        0 & 76/28 & 33.4M & 36.7 & 57.4 & 16.8 & 39.6 & 54.2 \\
        3 & 81/25 & 37.4M & 40.1 & 60.6 & 18.5 & 43.8 & 58.6 \\
        6 & 86/23 & 41.3M & 40.6 & 61.6 & 19.9 & 44.3 & 60.2 \\
        12 & 95/20 & 49.2M & 41.6 & 62.1 & 19.8 & 44.9 & 61.9 \\
        \bottomrule
    \end{tabular}
    \label{table:enc_layers}
\end{table}
\begin{figure}[t]
    \centering\small
    \includegraphics[width=1.0\columnwidth]{figures/cows_attn.pdf}
    \caption{Encoder self-attention for a set of reference points. The encoder is able to separate individual instances.
    Predictions are made with baseline \detr model on a validation set image.
    }
    \label{fig:encoder_self_attention}
\end{figure}
\subsubsection{Number of decoder layers.}
\begin{figure}[t]
    \centering\small
    \begin{minipage}[t!]{0.57\textwidth}
        \centering\small
        \includegraphics[width=0.85\columnwidth]{figures/nms_fig4.pdf}
        \caption{AP and \AP{50} performance after each decoder layer.
        A single long schedule baseline model is evaluated.
        \detr does not need NMS by design, which is validated by this figure.
        NMS lowers AP in the final layers, removing TP predictions,
        but improves AP in the first decoder layers, removing double predictions,
        as there is no communication in the first layer\oldnew{}{, and slightly improves \AP{50}.}
        }
        \label{fig:nms}
    \end{minipage}
    \hskip1em
    \begin{minipage}[t!]{0.35\textwidth}
        \centering\small
        \vskip0.8em
        \includegraphics[width=\columnwidth]{figures/giraffe_collage2.jpg}
        \vskip0.8em
        \caption{Out of distribution generalization for rare classes.
        Even though no image in the training set has more than 13 giraffes, 
        \detr has no difficulty generalizing to 24 and more instances of the same class.
        }
        \label{fig:giraffe}
    \end{minipage}
\end{figure}
We apply auxiliary losses after each decoding layer
(see Section \ref{sec:deep_supervision}), hence, the prediction FFNs are trained by design to predict objects out of the outputs of every decoder layer. 
We analyze the importance of each decoder layer by evaluating the objects that would be predicted at each stage of the decoding (Fig.~\ref{fig:nms}).
Both AP and \AP{50} improve after every layer, totalling into a  very significant \oldnew{+5.4/6.3}{+8.2/9.5} AP improvement between the first and the last layer.\oldnew{even though we observe some
stagnation after layer 5.}{}
With its set-based loss, \detr does not need NMS by design. To verify this we run a standard NMS procedure with default parameters~\cite{wu2019detectron2} for the outputs after each decoder.
NMS  improves performance for the predictions from the first decoder. This can be explained by the fact that a single decoding layer of the transformer is not able to compute any cross-correlations between the output elements, and thus it is prone to making multiple predictions for the same object.
In the second and subsequent layers, the self-attention mechanism over the activations allows the model to inhibit duplicate predictions. We observe that the improvement brought by NMS diminishes as depth increases. At the last layers, we observe a small loss in AP as NMS incorrectly removes true positive predictions.

Similarly to visualizing encoder attention,
we visualize decoder attentions in Fig.~\ref{fig:attention_maps},
coloring attention maps for each predicted object in different colors.
We observe that decoder attention is fairly local, meaning that it mostly attends to object extremities such as heads or legs.
We hypothesise that after the encoder has separated instances via
global attention, the decoder only needs to attend to the extremities to extract the class and object boundaries.

\begin{figure}[t]
    \centering\small
    \includegraphics[height=0.43\columnwidth]{figures/elephants.png}
    \includegraphics[height=0.43\columnwidth]{figures/zebras.png}
    \caption{Visualizing decoder attention for every predicted object
    (images from COCO \texttt{val} set).
    Predictions are made with \detr-DC5 model.
    Attention scores are coded with different colors for different objects.
    Decoder typically attends to object extremities,
    such as legs and heads.
    Best viewed in color.
    }
    \label{fig:attention_maps}
\end{figure}



\subsubsection{Importance of FFN.}
FFN inside tranformers can be seen as  convolutional layers,
making encoder similar to
attention augmented convolutional networks~\cite{Bello2019AttentionAC}.
We attempt to remove it completely leaving only attention in the transformer layers.
By reducing the number of network parameters from 41.3M to 28.7M,
leaving only 10.8M in the transformer, performance drops by \oldnew{almost 2 AP}{2.3 AP}, we thus conclude that FFN are important for achieving good results.

\subsubsection{Importance of positional encodings.}
There are two kinds of positional encodings in our model: spatial 
positional encodings and output positional encodings (object queries).
We experiment with various combinations of fixed and learned encodings,
results can be found in table~\ref{table:pos_enc}.
Output positional encodings are required and cannot be removed,
so we experiment with either passing them once at decoder input
or adding to queries at every decoder attention layer.
In the first experiment we completely remove spatial positional encodings
and pass output positional encodings at input
and, interestingly, the model still achieves 
more than \oldnew{31}{32} AP, losing \oldnew{6}{7.8} AP to the baseline.
Then, we pass fixed sine spatial positional encodings and the output encodings
at input once, as in the original transformer~\cite{Vaswani2017AttentionIA},
and find that this leads to \oldnew{1.1}{1.4} AP drop compared to passing the positional encodings
directly in attention.
Learned spatial encodings passed to the attentions give
similar results.
Surprisingly, we find that not passing any spatial encodings in the encoder
only leads to a minor AP drop of 1.3 AP.
When we pass the encodings to the attentions, they are shared across all layers,
and the output encodings (object queries) are always learned.
\setlength{\tabcolsep}{6pt}
\begin{table}[h!]
    \centering\small
    \caption{Results for different positional encodings compared
    to the baseline (last row), which has fixed sine pos. encodings passed
    at every attention layer in both the encoder and the decoder.
    Learned embeddings are shared between all layers.
    Not using spatial positional encodings leads to a significant
    drop in AP. Interestingly, passing them in decoder only
    leads to a minor AP drop.
    All these models use learned output positional encodings.
    \label{table:pos_enc}}
    \begin{tabular}{lll|cc|cc}
        \toprule
        \multicolumn{2}{c}{spatial pos. enc.} & output pos. enc.\\
        \multicolumn{1}{c}{encoder} & \multicolumn{1}{c}{decoder} & \multicolumn{1}{c|}{decoder} & AP &  & \AP{50} &  \\
        \midrule
        none                & none & learned at input
        & 32.8 & -7.8 & 55.2 & -6.5 \\

        sine at input        & sine at input & learned at input
        & 39.2 & -1.4 & 60.0 & -1.6 \\

        learned at attn. & learned at attn. & learned at attn.
        & 39.6 & -1.0 & 60.7 & -0.9 \\

        none & sine at attn. & learned at attn.
        & 39.3 & -1.3 & 60.3 & -1.4 \\
        
        sine at attn. & sine at attn. & learned at attn.
        & \textbf{40.6} & - & \textbf{61.6} & - \\
        \bottomrule
    \end{tabular}
\end{table}


Given these ablations, we conclude that transformer components:
the global self-attention in encoder, FFN, multiple decoder layers, and positional encodings,
all significantly contribute to the final object detection performance.

\subsubsection{\oldnew{Cost}{Loss} ablations.} 
To evaluate the importance of different components of the matching cost and the loss,
we train several models turning them on and off.
There are three components to the loss: \oldnew{classification cost, bounding box distance cost}
{classification loss,  bounding box distance loss,} and GIoU~\cite{Rezatofighi_2018_CVPR} loss.
The classification \oldnew{cost}{loss} is essential for training and cannot be turned off,
so we train a model without bounding box distance loss,
and a model without the GIoU loss,
and compare with baseline, trained with all three losses.
Results are presented in table~\ref{table:cost}.
\setlength{\tabcolsep}{0.78em}
\begin{table}[h!]
    \centering\small
    \caption{Effect of \oldnew{cost}{loss} components on AP.
    \oldnew{We train two models turning off box cost and GIoU cost,
    and observe that box cost gives better results on large objects,
    whereas GIoU cost is better on small.}
    {We train two models turning off  loss, and GIoU loss,
    and observe that  gives poor results on its own,
    but when combined with GIoU improves \AP{M} and \AP{L}.
    Our baseline (last row) combines both losses.}
    }
    \begin{tabular}{ccc|cc|cc|ccc}
        \toprule
        class &  & GIoU & AP &  & \AP{50} &  & \AP{S} & \AP{M} & \AP{L} \\
        \midrule
        \chk & \chk & &
        35.8 & -4.8 & 57.3 & -4.4 & 13.7 &	39.8 &	57.9 \\
        \chk & & \chk &
        39.9 & -0.7 & \textbf{61.6} & 0 &	\textbf{19.9} &	43.2 &  57.9 \\
        \chk & \chk & \chk &
        \textbf{40.6} & - & \textbf{61.6} & - & \textbf{19.9} &	\textbf{44.3} &	\textbf{60.2} \\
        \bottomrule
    \end{tabular}
    \label{table:cost}
\end{table}
\oldnew{The GIoU cost shows better results on small objects (+2.9 AP difference),
whereas the bounding box cost improves results on large objects (+5.5 AP difference),
higher than the baseline.}
{GIoU loss on its own accounts for most of the model performance,
losing only 0.7 AP to the baseline with combined losses.
Using  without GIoU shows poor results.}
We only studied simple ablations of different \oldnew{costs}{losses} (using the same weighting every time),
but other means of combining them may achieve different results.



\subsection{Analysis}
\subsubsection{Decoder output slot analysis}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/query_distr.png}
    \caption{
    Visualization of all box predictions on all images from COCO 2017 val set for 20 out of total  prediction slots in \detr decoder. Each box prediction is represented as a point with the coordinates of its center in the 1-by-1 square normalized by each image size. The points are color-coded so that green color corresponds to small boxes, red to large horizontal boxes and blue to large vertical boxes. We observe that each slot learns to specialize on certain areas and box sizes with several operating modes. We note that almost all slots have a mode of predicting large image-wide boxes that are common in COCO dataset.
    }
    \label{fig:queries}
\end{figure}

In Fig.~\ref{fig:queries} we visualize the boxes predicted by different slots for
all images in COCO 2017 val set. \detr learns different specialization for each
query slot. We observe that each slot has several modes of operation focusing on
different areas and box sizes. In particular, all slots have the mode for
predicting image-wide boxes (visible as the red dots aligned in the middle of
the plot). We hypothesize that this is related to the distribution of objects in COCO.
\subsubsection{Generalization to unseen numbers of instances.}
Some classes in COCO are not well represented with many instances of the same class in the same image. For example, there is no image with more than 13 giraffes in the training set. We create a synthetic image\footnote{Base picture credit: https://www.piqsels.com/en/public-domain-photo-jzlwu} to verify the generalization ability of \detr (see Figure~\ref{fig:giraffe}). Our model is able to find all 24 giraffes on the image which is clearly out of distribution. This experiment confirms that there is no strong class-specialization in each object query.



\subsection{\detr for panoptic segmentation}


\begin{figure}[h]
    \centering\small
    \includegraphics[width=1.0\columnwidth]{figures/panoptic2.pdf}
    \caption{
    Illustration of the panoptic head. A binary mask is generated in parallel for each detected object, then the masks are merged using  pixel-wise argmax. }
    \label{fig:pano}
\end{figure}
\begin{figure}[h]
    \centering\small
    \includegraphics[height=0.27\columnwidth]{figures/Panoptic/0784_detr_R101.jpg}
    \includegraphics[height=0.27\columnwidth]{figures/Panoptic/3499_detr_R101.jpg}
    \includegraphics[height=0.27\columnwidth]{figures/Panoptic/0673_detr_R101.jpg}
    \caption{
    Qualitative results for panoptic segmentation generated by DETR-R101. \detr produces aligned mask predictions in a unified manner for things and stuff.
    }
    \label{fig:pano_quali}
\end{figure}

Panoptic segmentation~\cite{Kirillov2019panoptic} has recently attracted a lot of attention from the computer vision community. Similarly to the extension of Faster R-CNN~\cite{Ren2015FasterRT} to Mask R-CNN~\cite{He2017MaskR}, \detr can be naturally extended by adding a mask head on top of the decoder outputs. In this section we demonstrate that such a head can be used to produce panoptic segmentation~\cite{Kirillov2019panoptic} by treating stuff and thing classes in a unified way. We perform our experiments on the panoptic annotations of the COCO dataset that has 53 stuff categories in addition to 80 things categories.

We train \detr to predict boxes around both \emph{stuff} and \emph{things}
classes on COCO, using the same recipe. Predicting boxes is required for the
training to be possible, since the Hungarian matching is computed using
distances between boxes. We also add a mask head which predicts a binary mask
for each of the predicted boxes, see Figure~\ref{fig:pano}. It takes as input
the output of transformer decoder for each object and computes multi-head (with
 heads) attention scores of this embedding over the output of the encoder,
generating  attention heatmaps per object in a small resolution. To make the
final prediction and increase the resolution, an FPN-like architecture is used.
We describe the architecture in more details in the supplement. The final
resolution of the masks has stride 4 and each mask is supervised independently
using the DICE/F-1 loss~\cite{milletari2016v} \oldnew{}{and Focal loss~\cite{Lin2017FocalLF}}.

The mask head can be trained either jointly, or in a two steps \oldnew{}{process}, where we
train \detr for boxes only, then freeze all the weights and train only the mask
head for 25 epochs. Experimentally, these two approaches give similar results,
we report results using the latter method since it results in a shorter total
wall-clock time training.


To predict the final panoptic segmentation we simply use an argmax over the mask scores at each pixel, and assign the corresponding categories to the resulting masks. This procedure guarantees that the final masks have no overlaps and, therefore, \detr does not require a heuristic~\cite{Kirillov2019panoptic} that is often used to align different masks.

\subsubsection{Training details.} We train \detr, \detr-DC5 and \detr-R101 models
following the recipe for bounding box detection to predict boxes around stuff
and things classes in COCO dataset.  
\oldnew{During inference we collapse different mask predictions of the same stuff
category in one. The new mask head is trained for 25 epochs (see supplementary
for details). Similarly to~\cite{Kirillov2019panoptic}, we remove small stuff
(resp. things) predictions that have area smaller than 256 pixels (resp 4
pixels) as they are likely to be spurious segments, and keep only the detections
with a confidence higher than 75\%.}{The new mask head is trained for 25 epochs (see supplementary
for details). During inference we first filter out the detection with a confidence below 85\%, then compute the per-pixel argmax to determine in which mask each pixel belongs. We then collapse different mask predictions of the same stuff
category in one, and filter the empty ones (less than 4 pixels).}

\setlength{\tabcolsep}{0.3em}
\begin{table}[t]
  \centering\small
  \caption{Comparison with the state-of-the-art methods
    UPSNet~\cite{panoptic_xiong2019upsnet} and Panoptic
    FPN~\cite{panoptic_kirillov2019fpn} on the COCO \texttt{val} dataset
    We retrained PanopticFPN with the same data-augmentation as DETR,
    on a 18x schedule for fair comparison. UPSNet uses the \texttt{1x} schedule,
    UPSNet-M is the version with multiscale test-time augmentations.}
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{lc|ccc|ccc|ccc|c}
    \toprule
    Model & Backbone & PQ & SQ & RQ &  &   &   &  &  &  & AP \\
    
    \midrule
    PanopticFPN++  & R50 & 42.4 & 79.3 & 51.6 & 49.2 & 82.4 & 58.8 & 32.3 & 74.8 & 40.6 & 37.7\\
    UPSnet & R50  & 42.5 & 78.0 & 52.5 & 48.6 & 79.4 & 59.6 & 33.4 & 75.9 & 41.7 & 34.3 \\
    UPSnet-M & R50  & 
                     43.0  & 79.1 &  52.8 & 48.9 & 79.7 & 59.7 & 34.1  & 78.2 & 42.3 & 34.3 \\
    PanopticFPN++ & R101 & 44.1 & 79.5 & 53.3 & \textbf{51.0} & \textbf{83.2} & 60.6 & 33.6 & 74.0 & 42.1 & \textbf{39.7} \\

    DETR & R50 & 43.4  & 79.3 &  53.8 &  48.2 &  79.8  & 59.5 & 36.3  & 78.5  & 45.3 & 31.1 \\
    DETR-DC5 & R50& 44.6 & 79.8 & 55.0 & 49.4 & 80.5 & 60.6 & \textbf{37.3} & \textbf{78.7} & \textbf{46.5} & 31.9\\
    DETR-R101 & R101& \textbf{45.1} & \textbf{79.9} & \textbf{55.5} & 50.5 & 80.9 & \textbf{61.7} & 37.0 & 78.5 & 46.0 & 33.0\\
    \bottomrule
  \end{tabular}
}
  \label{table:panoptic_numbers}
\end{table}

\subsubsection{Main results.} Qualitative results are shown in
Figure~\ref{fig:pano_quali}. In table~\ref{table:panoptic_numbers}  we compare
our unified panoptic segmenation approach with several established methods that
treat things and stuff differently. We report the Panoptic Quality (PQ) and the
break-down on things () and stuff (). We also report the mask AP (computed on the
things classes), before any panoptic post-treatment (in our case, before taking
the pixel-wise argmax).
We show that \detr outperforms published results on COCO-val 2017, as well as
our strong PanopticFPN baseline (trained with same data-augmentation 
as \detr, for fair comparison). The result break-down shows that \detr is
especially dominant on stuff classes, and we hypothesize that the global
reasoning allowed by the encoder attention is the key element to this result.
For things class, despite a severe deficit of up to 8 mAP compared to the
baselines on the mask AP computation, \detr obtains competitive
.
\oldnew{}{We also evaluated our method on the test set of the COCO dataset, and obtained 46 PQ.}
We hope that our approach will inspire the exploration of fully unified models for panoptic segmentation in future work.
