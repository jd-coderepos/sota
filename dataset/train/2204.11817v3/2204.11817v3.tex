
\section{Experiments and Results}

\subsection{Data}


\paragraph{Pretraining Data} As described in Section \ref{sec:molt5}, the pretraining stage of MolT5 requires two monolingual corpora: one consisting of natural language text and the other consisting of molecule representations. We use the ``Colossal Clean Crawled Corpus'' (C4) \cite{raffel2020exploring} as the pretraining dataset for the textual modality. For the molecular modality, we directly utilize the 100 million SMILES strings used in Chemformer \cite{irwindimitriadishebjerrum2021}. As these strings were selected from the ZINC-15 dataset \cite{sterling2015zinc}, we refer to this pretraining dataset as ZINC from this point.

\paragraph{Finetuning and Evaluation Data} We use ChEBI-20 \cite{edwards2021text2mol} as our gold standard dataset for finetuning and evaluation. It consists of 33,010 molecule-description pairs, which are separated into 80/10/10\% train/validation/test splits.
We use ChEBI-20 to finetune MolT5-based models and to train baseline models. Many captions in ChEBI-20 contain a name for the molecule at the start of the string (e.g., ``Rostratin D is an organic disulfide isolated from ...''). To force the models to focus on the semantics of the description, we replace the molecule's name with "The molecule is [...]" (e.g., ``The molecule is an organic disulfide isolated from ...''). 

\subsection{Baselines}\label{sec:baselines}
Any sequence-to-sequence model is applicable to our new tasks (i.e., molecule captioning and generation). We implement the following baselines:
\begin{enumerate}
    \item \textbf{RNN-GRU} \cite{cho2014learning}. We implement a 4-layer GRU recurrent neural network. The encoder is bidirectional.
    \item \textbf{Transformer} \cite{vaswani2017attention}. We train a vanilla Transformer model consisting of six encoder and decoder layers. \item \textbf{T5} \cite{raffel2020exploring}. We experiment with three public T5.1.1 checkpoints\footnote{\url{https://tinyurl.com/t511-ckpts}}: small, base, and large. We finetune each checkpoint for molecule captioning or molecule generation using the t5x framework \cite{roberts2022t5x}.\end{enumerate}

We train the baseline models on ChEBI-20 using SMILES representations for the molecules. Molecule captioning and generation are trained with molecules as input/output and text as output/input. More information about the baselines and the hyperparameters is in the appendix. 


\subsection{Pretraining Process}
We first initialize an encoder-decoder Transformer model using a public checkpoint of T5.1.1 (either \textit{t5.1.1.small}, \textit{t5.1.1.base}, or \textit{t5.1.1.large}). We then pretrain the model on the combined dataset of C4 and ZINC (i.e., C4+ZINC) for 1 million steps. Each step uses a batch size of 256 evenly split between text and molecule sequences. After this, we finetune the pretrained model on ChEBI-20 for either molecule captioning or generation. The number of finetuning steps is 50,000.


\begin{table*}[ht!]
\resizebox{\textwidth}{!}{
\centering
\tiny
\begin{tabular}{ c|c|c|c|c|c|c|c }

\multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{BLEU-2} & \multicolumn{1}{c}{BLEU-4} & \multicolumn{1}{c}{ROUGE-1} & \multicolumn{1}{c}{ROUGE-2} & \multicolumn{1}{c}{ROUGE-L} & \multicolumn{1}{c}{METEOR} & \multicolumn{1}{c}{Text2Mol} \\
\thickhline
Ground Truth& &  &  &  &  &  & 0.609 \\\hline
RNN & 0.251 & 0.176 & 0.450 & 0.278 & 0.394 & 0.363 & 0.426 \\
Transformer & 0.061 & 0.027 & 0.204 & 0.087 & 0.186 & 0.114 & 0.057 \\\hline
T5-Small & 0.501 & 0.415 & 0.602 & 0.446 & 0.545 & 0.532 & 0.526 \\
MolT5-Small & 0.519 & 0.436 & 0.620 & 0.469 & 0.563 & 0.551 & 0.540 \\\hline
T5-Base & 0.511 & 0.423 & 0.607 & 0.451 & 0.550 & 0.539 & 0.523 \\
MolT5-Base & 0.540 & 0.457 & 0.634 & 0.485 & 0.578 & 0.569 & 0.547 \\\hline
T5-Large & 0.558 & 0.467 & 0.630 & 0.478 & 0.569 & 0.586 & 0.563 \\
MolT5-Large & \textbf{0.594} & \textbf{0.508} & \textbf{0.654} & \textbf{0.510} & \textbf{0.594} & \textbf{0.614} & \textbf{0.582} \\
\end{tabular}
}
\caption{Molecule captioning results on the test split of CheBI-20. Rouge scores are F1 values.}
\label{tab:results_captioning}
\end{table*}
 \begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{figures/main_fig_caption_cropped.pdf}
\caption{Example captions generated by different models.}
\label{fig:qual_caption}
\end{figure*}


\subsection{Molecule Captioning}


Table \ref{tab:results_captioning} shows the overall molecule captioning results. The pretrained models, either T5 or MolT5, are considerably better at generating realistic language to describe a molecule than the RNN and Transformer baselines. The RNN is more capable of extracting relevant properties from molecules than the Transformer, but it generally produces ungrammatical outputs. On the other hand, the Transformer produces grammatical outputs, but they tend to repeat the same properties, such as carcinogenic, regardless of whether they apply. For this reason, the Text2Mol scores are much lower for the Transformer model, since its outputs match the given molecule much less frequently. We speculate that the ChEBI-20 dataset is too small to effectively train a Transformer without large-scale pretraining. We find that our additional pretraining of MolT5 results in a reasonable increase over T5 in captioning performance on both the traditional NLG metrics and our Text2Mol metric for each model size. Finally, we refer the reader to Section \ref{appendix:stats_sign} in the appendix for information about the statistical significance of our results.

Several examples of different models' outputs are shown in Figure \ref{fig:qual_caption} and Appendix Figure \ref{fig:qual1_cap_appendix}. In (1), MolT5's description matches best, identifying the molecule as a ``GDP-L-galactose''. MolT5 is usually able to recognize what general class of molecule it is looking at (e.g. cyclohexanone, maleate salt, etc.). In general, all models often look for the closest compound they know and base their caption on that. The argon atom, example (2) with SMILES `[39Ar]', is not present in the training dataset bonded to any other atoms (likely because it is an inert noble gas). All models recognize that (2) is a single atom, but they are unable to describe it.
In (3), the models try to caption a histological dye. MolT5 captions the molecule as an azure histological dye, which is very close to the ground truth ``brilliant cresyl blue'', while T5 does not.



\begin{table*}[ht!]
\resizebox{\textwidth}{!}{
\centering
\begin{tabular}{ c|c|c|c|c|c|c|c|c|c }

\multicolumn{1}{c}{\textbf{Model}} & \multicolumn{1}{c}{BLEU} & \multicolumn{1}{c}{Exact} & \multicolumn{1}{c}{Levenshtein} & \multicolumn{1}{c}{MACCS FTS} & \multicolumn{1}{c}{RDK FTS} & \multicolumn{1}{c}{Morgan FTS} & \multicolumn{1}{c}{FCD} & \multicolumn{1}{c}{Text2Mol} & \multicolumn{1}{c}{Validity} \\
\thickhline
Ground Truth & 1.000 & 1.000 & 0.0 & 1.000 & 1.000 & 1.000 & 0.0 & 0.609 & 1.0 \\\hline
RNN & 0.652 & 0.005 & 38.09 & 0.591 & 0.400 & 0.362 & 4.55 & 0.409 & 0.542 \\
Transformer & 0.499 & 0.000 & 57.66 & 0.480 & 0.320 & 0.217 & 11.32 & 0.277 & \textbf{0.906} \\\hline
T5-Small & 0.741 & 0.064 & 27.703 & 0.704 & 0.578  & 0.525  & 2.89  & 0.479 & 0.608 \\
MolT5-Small & 0.755 & 0.079 & 25.988 & 0.703 & 0.568 & 0.517 & 2.49 & 0.482 & 0.721 \\\hline
T5-Base & 0.762 & 0.069 & 24.950 & 0.731 &  0.605 & 0.545 & 2.48 & 0.499 & 0.660  \\
MolT5-Base & 0.769 & 0.081 & 24.458 & 0.721 &  0.588 & 0.529 & 2.18 & 0.496 & 0.772 \\\hline
T5-Large & 0.854 & 0.279 & 16.721 & 0.823 &  0.731 & 0.670 & 1.22 & 0.552 & 0.902 \\
MolT5-Large & \textbf{0.854} & \textbf{0.311} & \textbf{16.071} & \textbf{0.834} & \textbf{0.746} & \textbf{0.684} & \textbf{1.20} & \textbf{0.554} & 0.905 \\
\end{tabular}
}
\caption{Molecule generation results on the test split of CheBI-20. Except for BLEU, Exact, Levenshtein, and Validity, other metrics are computed using only syntactically valid molecules, as in \cite{campos2021img2smi}.
}
\label{tab:results_generation}
\end{table*}
 
\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{figures/main_fig_gen_cropped.pdf}
\caption{Examples of molecules generated by different models.}
\label{fig:qual}
\end{figure*}

\subsection{Text-Based de novo Molecule Generation}

In the molecule generation task, the pretrained models also perform much better than the RNN and Transformer (Table \ref{tab:results_generation}). Although it is well known that scaling model size and pretraining data leads to significant performance increases \cite{kaplan2020scaling}, it was still surprising to see the results. For example, a default T5 model, which was only pretrained on text data, is capable of generating molecules which are much closer to the ground truth than the RNN and which are often valid. This trend also persists as language model size scales, since T5-large with 770M parameters outperforms the specifically pretrained MolT5-small with 60M parameters. Still, the pretraining in MolT5 slightly improves some molecule generation results, with especially large gains in validity. Finally, Section \ref{appendix:stats_sign} in the appendix has information about the statistical significance of our results.


We show results for the models in Figure \ref{fig:qual} and also in Figures \ref{fig:qual2}, \ref{fig:qual3}, and \ref{fig:qual4} in Appendix \ref{appendix:examples}, which we number by input description. Compared to T5, MolT5 is better able to understand instructions for manipulating molecules, as shown in examples (3, 4, 6, 7, 16, 18, 21). In many cases, MolT5 obtains exact matches with the ground truth (2, 3, 4, 6, 7, 8, 10, 12, 17, 20, 21). (3) is an interesting case, since it shows that MolT5 can understand crystalline solids like hydrates. (2) is another interesting example; it is the longest SMILES string, at 474 characters, which MolT5 is able to generate an exact match for. MolT5 understands peptides and can produce them from descriptions (2,15,17). It also shows this ability for saccharides (6, 21) and enzymes (8,20). MolT5 is able to understand rare atoms such as Ruthenium (5). However, in this case it still misses the atom's charge. Some example descriptions, such as (1), lack details so the molecules generated by MolT5 may be interesting to investigate. 





\subsection{Probing the Model}

We conduct probing tests on the model for certain input properties, which are shown in Appendix \ref{appendix:probing}. Often, the model will generate molecules that it knows matches the input description from the finetuning data. It also creates solutions from these as well by adding various ions (e.g. ".[Na+]"). In some cases, it generates molecules not appearing in finetuning data (sometimes successfully sometimes not). For example, given the input ``The molecule is a corticosteroid.'', the first molecule generated is a well known corticosteroid called corticosterone. The fifth molecule generated is not present in the PubChem database. Based on a structure similarity search, it is most closely related to the androgenic steroid Fluoxymesterone and the corticosteroid Hydrocortisone.
