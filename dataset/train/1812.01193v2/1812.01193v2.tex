\section{Experiments}
We first present an experiment which demonstrates that a model which can easily rely on artifacts in SNLI to provide correct labels would not be able to provide correct explanations as easily. We refer to it as \expzero.

We then present a series of experiments to elucidate whether models trained on e-SNLI are able to: (i)~predict a label and generate an explanation for the predicted label (referred to as \expone), (ii)~generate an explanation then predict the label given only the generated explanation (\exptwo), (iii)~learn better universal sentence representations (\expthree), and  (iv)~transfer to out-of-domain NLI datasets (\expfour).

Throughout our experiments, our models follow the architecture presented in \citet{infersent}, as we build directly on top of their code\footnote{https://github.com/facebookresearch/InferSent. We fixed the issue raised in https://github.com/ facebookresearch/InferSent/issues/51 that the max-pooling was taken over paddings.}. Therefore, our encoders are 2048-bidirectional-LSTMs \cite{lstm} with max-pooling, resulting in a sentence representation dimension of 4096. Our label classifiers are 3-layers MLPs with 512 internal size and without non-linearities. For our explanation decoders, we used a simple one-layer LSTM, for which we tried internal sizes of $512$, $1024$, $2048$, and $4096$.
In order to reduce the vocabulary size for explanation generation, we replaced words that appeared less than $15$ times\footnote{Counted among premises, hypothesis, and explanations.} with {\tt <UNK>.}
We obtain an output vocabulary of approximately $12K$ words. 
The preprocessing and optimization were kept the same as in \cite{infersent}. 

Whenever appropriate, we run our models with five seeds and provide the average performance with the standard deviation in parenthesis. If no standard deviation is reported, the results are from one experiment with seed 1234. 

\subsection{\expzero: Generate an explanation given only the hypothesis}
\label{expzero}

\citet{artifacts} show that a neural network that only has access to the hypothesis can predict the correct label $67\%$ of the times. We are therefore interested in evaluating how well our explanations can be predicted from hypotheses alone.


\textbf{Model   } We train a 2048-bidirectional-LSTM with max-pooling for encoding the hypothesis, followed by a one-layer LSTM for decoding the explanation. The initial state of the decoder is the vector embedding of the hypothesis, which is also concatenated at every timestep of the decoder, to avoid forgetting. 


\textbf{Selection   } We consider internal sizes of the decoder of $512$, $1024$, $2048$ and $4096$. We pick the model that gives the best perplexity on the validation set. We notice that the perplexity strictly decreases when we increase the decoder size. However, for practical reasons, we do not increase the decoder size beyond $4096$.


\textbf{Results   } We then manually look at the first $100$ test examples and obtain that only $6.83$\footnote{Partial scoring as explained in Section \ref{partial-score}.} were correct. We also separately train the same hypothesis-only encoder for label prediction alone and obtain $66$ correct labels in the same first $100$ test examples. This validates our intuition that it is much more difficult (approx.\ 10x for this architecture) to rely on spurious correlations to predict correct explanations than to predict correct labels. 



\subsection{\expone: Jointly predict a label and generate an explanation for the predicted label}

In this experiment, we investigate how the typical architecture employed on SNLI can be enhanced with a module that aims to justify the decisions of the entire network. 


\textbf{Model   } We employ the InferSent \cite{infersent} architecture, where a bidirectional-LSTM with max-pooling separately encodes the premise, $\vec{u}$, and hypothesis, $\vec{v}$. The vector of features $\vec{f} = [\vec{u}, \vec{v}, |\vec{u} - \vec{v}|, \vec{u} \odot \vec{v}]$ is then passed to the MLP classifier that outputs a distribution over the 3 labels.  We add a one-layer LSTM decoder for explanations, which takes the feature vector $\vec{f}$ both as an initial state and concatenated to the word embedding at each time step.


\begin{wrapfigure}{R}{0.5\textwidth}
    \tikzstyle{premise}=[draw, ultra thick, color=nice-yellow, text height=1.5cm, text width=0.4cm, fill=nice-yellow!20]
    \tikzstyle{hypothesis}=[draw, ultra thick, color=nice-green, text height=1.5cm, text width=0.4cm, fill=nice-green!20]
    \tikzstyle{label}=[draw, ultra thick, color=nice-red, text height=1.5cm, text width=0.4cm, fill=nice-red!20]
    \tikzstyle{explanation}=[draw, ultra thick, color=nice-blue, text height=1.5cm, text width=0.4cm, fill=nice-blue!20]
    \tikzstyle{concat}=[draw, ultra thick, color=nice-purple, text height=2cm, text width=0.4cm, fill=nice-purple!20]
    \tikzstyle{connect}=[ultra thick, -Latex]
    
    \centering
    \resizebox{1\linewidth}{!}{
    \begin{tikzpicture}
\foreach \i in {0,...,3} {
            \node[premise] at (\i, 0) {};
            \draw[connect, nice-yellow] (\i-0.7, 0) -- (\i-0.25, 0);
            \draw[connect, nice-yellow] (\i, -1.1) -- (\i, -0.8);
        }
        \node[anchor=north] at (1.5, -1.2) {Premise};


        \begin{scope}[shift={(0,-3)}]
        \foreach \i in {1,...,3} {
            \node[hypothesis] at (\i, 0) {};
            \draw[connect, nice-green] (\i-0.7, 0) -- (\i-0.25, 0);
            \draw[connect, nice-green] (\i, -1.1) -- (\i, -0.8);
        }
        \node[anchor=north] at (2, -1.2) {Hypothesis};
        \end{scope}
        
        \begin{scope}[shift={(4.5,-1.5)}]
        \foreach \i in {0} {
            \node[concat] at (0,\i) {};
        }
        \draw[connect, nice-yellow] (-1.2, 1.5) -- (-0.3, 0.5);
        \draw[connect, nice-green] (-1.2, -1.5) -- (-0.3, -0.5);
        \draw[connect, nice-purple] (0.3, 0) -- (1.2, 1.5);
        \end{scope}
        
        \node[anchor=south, text width=2cm, text centered] at (4.5, -0.3) {$\vec{f}$};
        
        \begin{scope}[shift={(7.5,-1.5)}]
        \foreach \i in {0,...,2} {
            \node[explanation] at (\i, 0) {};
            \draw[connect, nice-blue] (\i-0.7, 0) -- (\i-0.25, 0);
            \draw[connect, nice-blue] (\i, 0.9) -- (\i, 1.2);
            \path[connect, nice-purple] (-3,-1.1) edge[bend right=50] (\i, -0.9);
        }
        \foreach \i in {1,2} {
            \draw[connect, nice-blue] (\i+0.05, -1.1) -- (\i+0.05, -0.8);
        }
        \node[anchor=south] at (1, 1.2) {Explanation};
        \node[label] (label) at (-1.5, 1.5) {}; 
        \path[connect, nice-red] (-1.5,0.6) edge[bend right=60] (0, -0.9);
        \node[above = 0cm of label] {Label};
        \end{scope}
    \end{tikzpicture}
    }
    \caption{Overview of the \eInferSent{} architecture.}
    \label{fig:model}
\end{wrapfigure}

In order to condition the explanation also on the label, we prepend the label as a word (\textit{entailment, contradiction, neutral}) at the beginning of the explanation. At training time, the gold label is provided, while at test time, we use the label predicted by the classifier. 
This architecture is depicted in \Cref{fig:model}. 


\textbf{Loss   } We use negative log-likelihood for both classification and explanation losses. The explanation loss is much larger in magnitude than the classification loss, due to the summation of negative log-likelihoods over the words in the explanations. To account for this difference during training, we use a weighting coefficient $\alpha \in [0,1]$. Hence, our overall loss is: 
\begin{equation} 
  \mathcal{L}_\text{total} = \alpha \mathcal{L}_\text{label} + (1 - \alpha) \mathcal{L}_\text{explanation}
\end{equation}

\textbf{Selection   } We consider $\alpha$ values from $0.1$ to $0.9$ with a step of $0.1$ and decoder internal sizes of $512$, $1024$, $2048$, and $4096$. For this experiment, we choose as model selection criterion the accuracy on the SNLI validation set, because we want to investigate how well a model can generate justifications without sacrificing accuracy. As future work, one can inspect different trade-offs between accuracy and explanation generation. We found $\alpha=0.6$ and the decoder size of $512$ to produce the best validation accuracy, of $84.37\%$, while InferSent with no explanations produced $84.30\%$ validation accuracy. We call our model \eInferSent{}, since it freezes the InferSent architecture and training procedure, and only adds the explanations decoder.

\textbf{Results   }
The average test accuracy that we obtain when training InferSent\cite{infersent} on SNLI with five seeds is $84.01\%$  $(0.25)$. Our \eInferSent{} model obtains essentially the same test accuracy, of $83.96\%$ $(0.26)$, which shows that one can get additional justifications without sacrificing label accuracy. For the generated explanations, we obtain a perplexity of $10.58 (0.4)$ and a BLEU-score of $22.40 (0.7)$. Since we collected 3 explanations for each example in the validation and test sets, we compute the inter-annotator BLEU-score of the third explanation with respect to the first two, and obtain $22.51$. For consistency, we used the same two explanations as the only references when computing the BLEU-score for the predicted explanations. Given the low inter-annotator score and the fact that generated explanations almost match the inter-annotator BLEU-score, we conclude that this measure is not reliable for our task, and we further rely on human evaluation. Therefore, we manually annotated the first 100 datapoints in the test set (we used the same partial scoring as in Section \ref{partial-score}). Since the explanation is conditioned on the predicted label, for incorrect labels, the model would not produce a correct explanation. Therefore, we provide as correctness score the percentage of correct explanations in the subset of the first 100 examples where the predicted label was correct (80 in this experiment). We obtain a percentage of $34.68\%$ correct explanations. While this percentage is low, we keep in mind that the selection criteria was only the accuracy of the label classifier and not the perplexity of the explanation. In the next experiment, we show how training (and selecting) only for generating explanations results in higher quality explanations.

\subsection{\exptwo{}: Generate an explanation then predict a label}

In \expone, we conditioned the explanation on the label predicted by the MLP, because we wanted to see how the typical architecture used on SNLI can be adapted to justify its decisions in natural language. However, a more natural approach for solving inference is to think of the explanation first and based on the explanation to decide a label. Therefore, in this experiment, we first train a network to generate an explanation given a pair of (premise, hypothesis), and, separately, we train a network to provide a label given an explanation. This is a sensible decomposition for our dataset, due to the following key observation: In our dataset, in the large majority of the cases, one can easily detect for which label an explanation has been provided. We highlight that this is not the case in general, as the same explanation can be correctly arguing for different labels, depending on the premise and hypothesis. For example, the explanation \textit{"A woman is a person"} would be a correct explanation for the entailment pair \textit{("A woman is in the park", "A person is in the park")} as well for the contradiction pair \textit{("A woman is in the park", "There is no person in the park")}. However, there are multiple ways of formulating an explanation. In our example, for the contradiction pair, one could also explain that \textit{"There cannot be no person in the park if a woman is in the park"}, which read alone would allow one to infer that the pair was a contradiction. To support our observation, we train a neural network that given only an explanation predicts a label. We use the same bidirectional encoder and MLP-classifier as above. We obtain an accuracy of $96.83\%$ on the test set of SNLI.  


\textbf{Models   } For predicting an explanation given a pair of (premise, hypothesis), we first train a simple seq2seq model that we call \exptwoseqtoseq. Essentially, we keep the architecture in \eInferSent{}, where we eliminate the classifier by setting $\alpha=0$, and we decode the explanation without prepending the label. Secondly, we train an attention model, which we refer to as \exptwoattention. Attention mechanisms in neural networks brought consistent improvements over the non-attention counter-parts in various areas, such as computer vision \cite{showattend}, speech \cite{listenattend}, or natural language processing \cite{best-attention, BahdanauCB14}. We use the same encoder and decoder as in \exptwoseqtoseq, and we add two identical but separate attention modules, over the tokens in the premise and hypothesis. For details of the attention modules, see Appendix \ref{attention}.


\textbf{Selection   } Our only hyper-parameter is internal sizes for the decoder of $512$, $1024$, $2048$, and $4096$. Our model selection criterion is the perplexity on the validation set of SNLI. We obtain the best configuration for both \exptwoseqtoseq{} and \exptwoattention{} to have an internal size of $1024$. 


\textbf{Results   } With the described setup, the SNLI test accuracy drops from $83.96\%$ $(0.26)$ in \expone{} to $81.59\%$ $(0.45)$ in \exptwoseqtoseq{} and $81.71\% (0.36)$ in \exptwoattention. 
However, when we again manually annotate the first 100 generated explanations in the test set, we obtain significantly higher percentages of correct explanations: $49.8\%$ for \exptwoseqtoseq{} and $64.27\%$ for \exptwoattention. We note that the attention mechanism indeed significantly increases the quality of the explanations. The perplexity and BLEU-score are $8.95 (0.03)$ and $24.14 (0.58)$ for \exptwoseqtoseq{}, and $6.1 (0)$ and $27.58 (0.47)$  for \exptwoattention. Our experiment shows that, while sacrificing a bit of performance, we get a better trust that when \exptwo{} predicts a correct label, it does so for the right reasons.


\textbf{Qualitative analysis of explanations   } In \Cref{generated-expls}, we provide examples of generated explanations from the test set from (a) \expone, (b) \exptwoseqtoseq{}, and (c) \exptwoattention. At the end of each explanation, we give in brackets the score that we manually allocated as explained in section \ref{partial-score}. We notice that the explanations are mainly on topic for all the three models, with minor exceptions, such as the mention of "\textit{camouflage}" in (1c). We also notice that even when incorrect, they are sometimes frustratingly close to being correct, for example, explanation (2b) is only one word (out of its 20 words) away from being correct. It is also interesting to inspect the explanations provided when the predicted label is incorrect. For example, in (1a), we see that the network omitted the information of "\textit{facing the camera}" in the premise and therefore classified the pair as neutral, which is backed up by an otherwise correct explanation in itself. We also see that model \exptwoseqtoseq{} correctly classifies this pair as entailment, however, it only motivates 1 out the 3 reasons why it is so, and it also picks arguably the easiest reason. Interestingly, the attention model (1c) points to the correct evidence but argues that "\textit{standing}" and "\textit{facing a camera}" is not enough to conclude "\textit{posing for a picture}". 



\begin{table}[]
\caption{Examples of predicted labels and generated explanations from (a)~\expone{},\\ (b)~\exptwoseqtoseq{}, and (c)~\exptwoattention. In brackets our scores of correctness. }

\smallskip 
\label{generated-expls}
\resizebox{\textwidth}{!}{\begin{tabular}{lll}
\toprule
\iffalse
\multicolumn{3}{c}{\begin{tabular}[c]{@{}c@{}} (1) Premise: A couple walk hand in hand down a street. \\ Hypothesis: A couple is walking together. \\ Gold label: entailment\end{tabular}} \\  

\\

\multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}(a) Predicted label: entailment \\ Explanation: A couple walking \\ down a street is a rephrasing \\ of walking together. [0] \end{tabular}}                                & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}(b) Predicted label: entailment \\ Explanation: A couple walking \\ down a street is a couple. [0] \end{tabular}}                                                                       & \begin{tabular}[c]{@{}l@{}}(c) Predicted label: entailment \\ Generated explanation: A hand \\ walk hand in hand is a rephrasing \\ of walking together. [1] \end{tabular}  \\

\fi

\multicolumn{3}{c}{\begin{tabular}[c]{@{}c@{}} (1) Premise: 3 young man in hoods standing in the middle of a quiet street facing the camera. \\ Hypothesis: Three hood wearing people pose for a picture. \\ Gold label: entailment\end{tabular}} \\  

\\

\multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}(a) Predicted label: neutral \\ Explanation: Just because the men are in the \\ middle of a street doesn't mean \\ they are posing for a picture. [0] \end{tabular}}                                & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}(b) Predicted label: entailment \\ Explanation: three young men \\ are people. [0.33] \end{tabular}}                                                                       & \begin{tabular}[c]{@{}l@{}}(c) Predicted label: neutral \\ Explanation: Just because three young man \\ in camouflage standing in the middle of \\ a quiet street facing the camera \\ does not mean they pose for a picture. [0] \end{tabular}  \\

\\ \midrule

\multicolumn{3}{c}{\begin{tabular}[c]{@{}c@{}}(2) Premise: Three firefighter come out of subway station.\\ Hypothesis: Three firefighters putting out a fire inside of a subway station. \\ Gold label: neutral\end{tabular}} \\                             

\\

\multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}(a) Predicted label: contradiction \\ Explanation: The firefighters \\ can not be putting out a fire station and \\ putting out a fire at the same time. [0] \end{tabular}} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}(b) Predicted label: neutral\\ Explanation: The fact that three\\ firemen are putting out of a subway\\ station doesn't imply that they\\  are putting out a fire. [0]\end{tabular}} & \begin{tabular}[c]{@{}l@{}}(c) Predicted label: neutral\\ Explanation: The firefighters \\ may not be putting out a fire \\ inside of the subway station. [1]\end{tabular} \\


\\  \midrule

\multicolumn{3}{c}{\begin{tabular}[c]{@{}c@{}} (3) Premise: A blond-haired doctor and her African American assistant looking threw new medical manuals.\\ Hypothesis: A man is eating pb and j.\\ Gold label: contradiction\end{tabular}}   \\                                                                                \\

\multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}(a) Predicted label: contradiction\\ Explanation: A man is not a woman. [1] \end{tabular}}                                                                                   & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}(b) Predicted label: contradiction\\ Explanation: One can not be looking \\ and eating simultaneously. [0] \end{tabular}}                                                               & \begin{tabular}[c]{@{}l@{}}(c) Predicted label: contradiction\\ Explanation: A person can not be looking \\ at a medical and a book at the same time. [0]
\end{tabular} 
\\
\bottomrule
\end{tabular}}
\end{table}



\subsection{\expthree{}: Universal sentence representations}

Obtaining universal sentence representations is the task of learning an encoder to provide semantically meaningful fixed-length representations of phrases/sentences. 
These representations are further used as features in other downstream tasks, particularly for tasks where labeled training data are scarce. 
In computer vision, pretrained ImageNet-based encoders \cite{imagenet} provide standard image feature extractors for other downstream tasks. 
However, in natural language processing, there is still no consensus on general-purpose sentence encoders. 
It remains an open question on which task and dataset should such an encoder be trained. Traditional approaches make use of very large unsupervised datasets \cite{skip, fastsent}, taking weeks to train. 
\citet{infersent} showed that training only on NLI is both more accurate and more time-efficient than training on orders of magnitude larger but unsupervised datasets. Their results constitute a previous state-of-the-art for universal sentence representations and encourage the idea that supervision can be more beneficial than larger but unsupervised datasets. We hypothesize that an additional layer of supervision in the form of natural language explanations should further improve learning of universal sentence representations. 


\textbf{Model   } We use our \eInferSent{} model already trained in \expone. While we compare our model with InferSent that has not been trained on explanations, we want to ensure that eventual improvements are not purely due to the addition of a language model in the decoder network. 
We therefore introduce a second baseline, \inferSentAutoencoder, where instead of decoding explanations, we decode the premise and hypothesis separately from each sentence representation using one shared decoder.


\textbf{Evaluation metrics   }
Typically, sentence representations are evaluated by using them as fixed features on top of which shallow classifiers are trained for a series of downstream tasks. \citet{infersent} provide an excellent tool, called SentEval, for evaluating sentence representations on 10 diverse tasks: movie reviews ({\bf MR}), product reviews ({\bf CR}), subjectivity/objectivity ({\bf SUBJ}), opinion polarity ({\bf MPQA}), question-type ({\bf TREC}), sentiment analysis ({\bf SST}), semantic textual similarity ({\bf STS}), paraphrase detection ({\bf MRPC}), entailment ({\bf SICK-E}), and semantic relatedness ({\bf SICK-R}). 
We refer to their work for a more detailed description of each of these tasks and of SentEval, which we use for comparing the quality of the sentence embeddings obtained by additionally providing our explanations on top of the label supervision. 


\textbf{Results   } In Table \ref{senteval}, we report the average results and standard deviations of \eInferSent, our retrained InferSent model, and the additional \inferSentAutoencoder{} baseline on the downstream tasks mentioned above. To test if the differences in performance of \inferSentAutoencoder{} and \eInferSent{} relative to the InferSent baseline are significant, we performed  Welchâ€™s t-test.\footnote{Using the implementation in  \texttt{scipy.stats.ttest\_ind} with \texttt{equal\_var=False}.} We mark with * the results that appeared significant under the significance level of $0.05$.


\begin{table}
  \caption{Transfer results on downstream tasks. For MRPC we report accuracy/F1 score, for STS14 we report the Person/Spearman correlations, for SICK-R the Person correlation, and for all the rest their accuracies. Results are the average of 5 runs with different seeds. The standard deviations is shown in brackets, and the best result for every task is indicated in bold. * indicates significant difference at level 0.05 with respect to the InferSent baseline.}
  
  \smallskip 
  \label{senteval}
  \centering
  \begin{adjustbox}{width=1\textwidth}
  \begin{tabular}{lllllllllll}
    \toprule
Model     & MR & CR & SUBJ & MPQA & SST2 & TREC & MRPC & SICK-E & SICK-R & STS14 \\
    \midrule
    InferSent-SNLI-ours & \textbf{78.18}  &	81.28 &	\textbf{92.46} &	88.46 &	\textbf{82.12} &	89.32 &	74.82 /	82.74 &	\textbf{85.96} &	0.887 &	0.65 / 0.63 \\
& (0.25) & (0.15) &	(0.15) &	(0.21) &	(0.22) &	(0.5) &	(0.66 /	0.27) &	(0.32) &	(0.002) &	(0 / 0) \\
    \inferSentAutoencoder &  75.94* &	79.26* &	91.72* &	88.16 &	80.9* &	\textbf{90.52*} &	\textbf{76.2*} /	82.48 &	85.58 &	0.88* &	0.5* / 0.5* \\
& (0.18) &	(0.36) &	(0.28) &	(0.26) &	(0.48) &	(0.52) &	(0.93 /	1.23) &	(0.33) &	(0) &	(0.02 / 0.02)\\
    \eInferSent & 77.76	&  \textbf{81.3} &	92.14* &	\textbf{88.78*} &	81.84 &	90 &	75.56 /	\textbf{83.24*} &	85.92 &	\textbf{0.89*} &	\textbf{0.68} / \textbf{0.65*} \\
& (0.44) &	(0.16) &	(0.21) &	(0.22) &	(0.4) &	(0.51) &	(0.62 / 0.24) &	(0.52) &	(0) &	(0.01 / 0.01)\\
    \bottomrule
  \end{tabular}
  \end{adjustbox}
\end{table}

We notice that \inferSentAutoencoder{} is performing significantly worse than InferSent on 6 tasks and significantly outperforms this baseline on only 2 tasks. This indicates that just adding a language generator can harm performance. Instead, \eInferSent{} significantly outperforms InferSent on 4 tasks, while it is significantly outperformed only on 1 task. Therefore, we conclude that training with explanations helps the model to learn overall better sentence representations.

\subsection{\expfour{}: Transfer without fine-tuning to out-of-domain NLI}
\label{exp4}

Transfer without fine-tuning to out-of-domain entailment datasets is known to exhibit poor performance. For example, \citet{snli} obtained an accuracy of only $46.7\%$ when training on SNLI and evaluating on SICK-E \cite{sick}. We test how our explanations affect the direct transfer in both label prediction and explanation generation by looking at SICK-E~\cite{sick} and MultiNLI~\cite{multinli}. The latter includes a diverse range of genres of written and spoken English, as well as test sets for cross-genre transfer.

\textbf{Model   } We again use our already trained \eInferSent{} model from \expone.

\textbf{Results   } In Table \ref{artifacts}, we present the performance of \eInferSent{} and our 2 baselines when evaluated without fine-tuning on SICK-E and MultiNLI. We notice that the accuracy improvements obtained with \eInferSent{} are very small. However, \eInferSent{} additionally provides explanations, which could bring insight into the inner workings of the model. We manually annotated the first 100 explanations of the test sets. The percentage of correct explanations in the subset where the label was predicted correctly was $30.64\%$ for SICK-E and only $1.92\%$ for MultiNLI.
We also noticed that the explanations in SICK-E, even when wrong, were generally on-topic and valid statements, while the ones in MultiNLI were generally nonsense or off-topic. Therefore, transfer learning for generating explanations in out-of-domain NLI would constitute challenging future work.

\iffalse
\begin{table}
  \caption{Accuracies of direct transfer to SICK-E, MultiNLI, and artifacts datasets (standard deviations in parenthesis). For BehaviourNLI sets, in brackets, we show the percentage of deterioration of the accuracy on transformed set with respect to its control set. Best result for every dataset in bold.}
  
  \smallskip 
  \label{artifacts}
  \centering
  \resizebox{1\linewidth}{!}{
  \begin{tabular}{lllllllllllllll}
    \toprule
    Model  & SICK-E & MultiNLI & HardNLI & BreakingNLI & I_A	& I_{TA1} &	I_{TA2} &	I_{TA3} &	E_A &	E_{TA} &	I_H &	I_{TH} &	E_H &	E_{TH} \\
    \midrule
    InferSent-SNLI-ours & 53.27	 &	57  &  \textbf{68.12} & 53.74 & 93.68 &	89.71 &	69.95 &	70 &	\textbf{88.32} &	\textbf{85.03} &	86.7 &	65.89 &	\textbf{80.0}3 &	61.27 \\
    & (1.65) & (0.41) & (0.6) & (4.8) & (2.68) &	(1.38) &	(2.19) &	(1.47) &	(3.99) &	(3.69) &	(2.61) &	(2.72) &	(1.71) &	(6.07) \\ 
    & & & & & & [\textbf{4.23}] &	[25.33] &	[\textbf{25.27}] & &		[\textbf{3.72}] & &		[24] & &		[23.44] \\
    \inferSentAutoencoder &  52.9 & 55.38 & 66.1 & 56.9 & 93.84 &	84.23 &	68.17 &	68.42 &	86.42 &	83.74 &	87.22 &	64.45 &	77.7 &	56.68 \\
    & (1.77) &  (0.9) &  (0.59) & (2.58) &  (2.33) &	(2.26) &	(0.5) &	(1.08) &	(3.36) &	(2.57) &	(3.31) &	(1.6) &	(1.67) &	(3.87) \\
    \eInferSent & \textbf{53.54}	 & \textbf{57.16} & 68.07 & \textbf{56.92} & 95.42 &	\textbf{90.58} &	\textbf{71.4} &	\textbf{70.53} &	87.32 &	83.19 &	88.83 &	\textbf{67.66} &	78.54 &	\textbf{62.39} \\
    & (1.43) & (0.51) & (0.4) & (4.16) & (1.84) &	(1.65) &	(0.55) &	(1.28)	& (2.67) &	(3.68) &	(2.5)	 & (2.76) &	(2.06) &	(5.46) \\
     & & & & & & [5.07] &	[\textbf{25.17}] &	[26.08] & &		[4.72] & &		[\textbf{23.83}] & &		[\textbf{20.56}] \\
    \bottomrule
  \end{tabular}
  }
\end{table}

Contrary to our hypothesis, we notice that our model does not exhibit any difference with respect to the baseline InferSent on SICK-E, MultiNLI, and HardNLI. For BreakingNLI, while we obtain an increase in average accuracy of $3.22\%$, we can see that this number is within one standard deviation and that the \inferSentAutoencoder{} obtained the same level of improvement. For the BehaviourNLI subsets on a large majority of the transformed test sets, we obtain better absolute results with respect to the InferSent baseline. However, on the percentage deterioration of accuracy of the transformed datasets relative to the control datasets, we obtain a tie on the number of sets where our model produces less deterioration relative to the InferSent baseline. 
\fi

\begin{wraptable}{R}{0.5\textwidth}
\caption{The average performance over 5 seeds of \eInferSent{} and the 2 baselines on SICK-E and MultiNLI with no fine-tuning. Standard deviations are in parenthesis. }
  \label{artifacts}
  \centering
  \resizebox{1\linewidth}{!}{
  \begin{tabular}{lll}
    \toprule
    Model     & SICK-E & MultiNLI \\
    \midrule
    InferSent-SNLI-ours & 53.27	(1.65) &	57 (0.41) \\
    
    \inferSentAutoencoder &  52.9 (1.77) & 55.38 (0.9) \\

    \eInferSent & \textbf{53.54} (1.43)	 & \textbf{57.16} (0.51)  \\
    
    \bottomrule
  \end{tabular}
  }
\end{wraptable}

