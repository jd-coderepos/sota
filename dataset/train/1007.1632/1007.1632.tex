\documentclass{article}[11pt]

\usepackage{amsmath,amsfonts,amsthm,tikz,fullpage,hyperref,algorithm,algorithmic,multirow,tabularx}

\renewcommand\rmdefault{cmr}
\renewcommand\sfdefault{cmss}
\renewcommand\ttdefault{cmtt}
\newcommand{\INPUT}{\item[{\bf Input:}]}
\newcommand{\OUTPUT}{\item[{\bf Output:}]}




\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conj}[theorem]{Conjecture}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{propos}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newcommand{\partdiff}[2]{\frac{\partial {#1}}{\partial {#2}}}
\newcommand{\secdiff}[2]{\frac{\partial^2 {#1}}{\partial {#2}^2}}
\newcommand{\mixdiff}[3]{\frac{\partial^2 {#1}}{{\partial {#2}}{\partial {#3}}}}
\newcommand{\E}[1]{{\bf{E}}\left[#1\right]}
\newcommand{\EE}[2]{{\bf{E}}_{#1}\left[#2\right]}
\renewcommand{\P}[2]{{\bf{Pr}}_{#1}\left[#2\right]}
\renewenvironment{proof}{\noindent{\bf Proof}:~}{\\}

\newcommand{\T}[4] {\begin{tabular}{|c|c|}
\hline
#1 & #2 \\
\hline
#3 & #4 \\
\hline
\end{tabular}}

\newcommand{\TD}[4] {\begin{tabular}{r|c|c|}
\multicolumn{1}{r}{}
 & \multicolumn{1}{c}{}
 & \multicolumn{1}{c}{} \\
\cline{2-3}
 & #1 & #2 \\
\cline{2-3}
 & #3 & #4 \\
\cline{2-3}
\end{tabular}}

\newcommand{\TTR}[5]{\begin{tabular}{|c|c|l}
\cline{1-2}
#1 & #2 \\
\cline{1-2}
\multirow{2}{*}{#3} & {\small{#4}} \\
\cline{2-2}
& {\small{#5}} \\
\cline{1-2}
\end{tabular}}

\newcommand{\TTL}[5]{\begin{tabular}{|c|c|}
\cline{1-2}
 #1 & #2 \\
\cline{1-2}
{\small{#3}} & \multirow{2}{*}{#5}  \\
\cline{1-1}
 {\small{#4}} &  \\
\cline{1-2}
\end{tabular}}

\def\b1{{\bf 1}}
\def\be{{\bf e}}
\def\bx{{\bf x}}
\def\by{{\bf y}}
\def\bz{{\bf z}}
\def\bv{{\bf v}}
\def\RR{{\mathbb R}}
\def\ZZ{{\mathbb Z}}
\def\cM{{\cal M}}
\def\cI{{\cal I}}
\def\cB{{\cal B}}
\def\cF{{\cal F}}
\def\cG{{\mathcal{G}}}
\def\eps {\epsilon}
\def\max{{\rm{max}}}
\def\sup{{\rm{sup}}}
\def\G{{G}}

\begin{document}

\title{Submodular Maximization by Simulated Annealing}

\author{
Shayan Oveis Gharan\thanks{Stanford University, Stanford, CA; {\tt shayan@stanford.edu};
this work was done partly while the author was at IBM Almaden Research Center, San Jose, CA.}
\and Jan Vondr\'ak\thanks{IBM Almaden Research Center, San Jose, CA; {\tt jvondrak@us.ibm.com}}
}

\maketitle

\begin{abstract}
We consider the problem of maximizing a nonnegative (possibly non-monotone)
submodular set function with or without constraints. Feige et al. \cite{FMV07}
showed a -approximation for the unconstrained problem and also proved that
no approximation better than  is possible in the value oracle model.
Constant-factor approximation was also given for submodular maximization subject
to a matroid independence constraint  (a factor of  \cite{Vondrak09})
and for submodular maximization subject to a matroid base constraint,
provided that the fractional base packing number is at least 
(a -approximation \cite{Vondrak09}).

In this paper, we propose a new algorithm for submodular maximization which is based
on the idea of {\em simulated annealing}. We prove that this algorithm achieves
improved approximation for two problems:
a -approximation for unconstrained submodular maximization,
and a -approximation for submodular maximization subject to a matroid
independence constraint.

On the hardness side, we show that in the value oracle model it is impossible
to achieve a -approximation for submodular maximization
subject to a matroid independence constraint, or a -approximation
subject to a matroid base constraint in matroids with two disjoint bases.
Even for the special case of cardinality constraint, we prove it is impossible to
achieve a -approximation.
(Previously it was conceivable that a -approximation exists for these problems.)
It is still an open question whether a -approximation is possible
for unconstrained submodular maximization.
\end{abstract}


\thispagestyle{empty}
\newpage
\setcounter{page}{1}


\section{Introduction}


A function  is called {\em submodular} if for any ,
 In this paper,
we consider the problem of {\em maximizing a nonnegative submodular function}.
This means, given a submodular function , find
a set  (possibly under some constraints) maximizing .
We assume a {\em value oracle} access to the submodular function; i.e., for a given set ,
the algorithm can query an oracle to find its value .

\paragraph{Background.}
Submodular functions have been studied for a long time in the context of combinatorial
optimization.
Lov\'asz in his seminal paper \cite{Lovasz83} discussed various properties
of submodular functions and noted that they exhibit certain properties
reminiscent of convex functions - namely the fact that a naturally defined extension
of a submodular function to a continuous function (the "Lov\'asz extension") is convex. 
This point of view explains why submodular functions can be {\em minimized}
efficiently \cite{GLS81,FFI00,Schrijver00}.

On the other hand, submodular functions also exhibit properties closer to concavity,
for example a function  is submodular if and only if  is concave.
However, the problem of {\em maximizing} a submodular function captures
problems such as Max Cut \cite{GW95} and Max -cover \cite{Feige98} which are NP-hard.
Hence, we cannot expect to maximize a submodular function exactly;
still, the structure of a submodular functions (in particular, the ``concave aspect''
of submodularity) makes it possible to achieve non-trivial results for maximization problems.
Instead of the Lov\'asz extension, the construct which turns out to be useful for maximization
problems is the {\em multilinear extension}, introduced in \cite{CCPV07}. 
This extension has been used to design an optimal -approximation for the
problem of maximizing a monotone submodular function subject to a matroid independence
constraint \cite{Vondrak08,CCPV09}, improving the greedy -approximation of
Fisher, Nemhauser and Wolsey \cite{NWF78II}.
In contrast to the Lov\'asz extension,
the multilinear extension captures the concave as well as convex aspects of submodularity.
A number of improved results followed for maximizing monotone submodular functions subject
to various constraints \cite{KST09,LMNS09,LSV09,CVZ10}.

This paper is concerned with submodular functions which are not necessarily monotone.
We only assume that the function is nonnegative.\footnote{For submodular
functions without any restrictions, verifying whether the maximum
of the function is greater than zero or not requires exponentially many queries.
Thus, no approximation algorithm can be found for this problem.}
\iffalse 
In this setting, the maximization problem is computationally hard even without any constraint,
since it captures for example the Max Cut problem.
Typical examples of such a problem are Max Cut and Max Directed Cut.
Here, the best approximation factors have been achieved using
semidefinite programming:  for Max Cut~\cite{GW95} and
 for Max Di-Cut~\cite{FG95,LLZ02}. The approximation factor for
Max Cut has been proved optimal, assuming the Unique Games
Conjecture~\cite{KKMO04,MOO05}. An algorithm for Max Cut based on eigenvalue computations
rather than SDP is also known to beat the factor of  \cite{Trevisan08}.
However, purely combinatorial algorithms for Max Cut and Max Di-Cut are known to
achieve only a -approximation \cite{HZ01}.
\fi
The problem of maximizing a nonnegative submodular function has been studied
in the operations research community, with many heuristic solutions proposed:
data-correcting search methods~\cite{GSTT99,GTT99,K89}, accelatered greedy algorithms~\cite{RS89},
and polyhedral algorithms~\cite{LNW96}.
The first algorithms with provable performace guarantees for this problem were given 
by Feige, Mirrokni and Vondr\'ak \cite{FMV07}. They presented several algorithms achieving
constant-factor approximation, the best approximation factor being 
 (by a randomized local search algorithm).
They also proved that a better than  approximation for submodular maximization
would require exponentially many queries in the value oracle model.
This is true even for symmetric submodular functions, in which case a
-approximation is easy to achieve \cite{FMV07}.

Recently, approximation algorithms have been designed for nonnegative
submodular maximization subject to various constraints \cite{LMNS09,LSV09,Vondrak09,GRST10}.
(Submodular minimization subject to additional constraints has been
also studied \cite{SF08,GKTW09,IN09}.)
The results most relevant to this work are that a nonnegative submodular functions can be
maximized subject to a matroid independence constraint within a factor of ,
while a better than -approximation is impossible \cite{Vondrak09},
and there is -approximation subject to
a matroid base constraint for matroids of fractional
base packing number at least , while a better
than -approximation in this setting is impossible \cite{Vondrak09}.
For explicitly represented instances of unconstrained submodular maximization,
Austrin \cite{Austrin10} recently proved that assuming the Unique Games Conjecture,
the problem is NP-hard to approximate within a factor of . 



\paragraph{Our results.}
In this paper, we propose a new algorithm for submodular maximization, using
the concept of {\em simulated annealing}. The main idea is to perform a local search
under a certain amount of random noise which gradually decreases to zero.
This helps avoid bad local optima at the beginning,
and provides gradually more and more refined local search towards the end.
Algorithms of this type have been widely employed for difficult optimization problems,
but notoriously difficult to analyze.

We prove that the simulated annealing algorithm achieves a -approximation for the maximization
of any nonnegative submodular function without constraints, improving upon the previously known
-approximation \cite{FMV07}. (Although our initial hope was that this algorithm
might achieve a -approximation, we found an example where it
achieves only a factor of ; see Appendix~\ref{app:tightexample}.)
We also prove that a similar algorithm achieves a -approximation for the
maximization of a nonnegative submodular function subject to a matroid independence
constraint (improving the previously known factor of  \cite{Vondrak09}).

On the hardness side, we show the following results in the value oracle model:
For submodular maximization under a matroid base constraint, 
it is impossible to achieve a -approximation even in the special case
when the matroid contains two disjoint bases.
For maximizing a nonnegative submodular function subject to a matroid independence constraint,
we prove it is impossible to achieve a -approximation.
For the special case of a cardinality constraint (
or ), we prove a hardness threshold of .
We remark that only a hardness
of -approximation was known for all these problems prior to this work.
For matroids of fractional base packing number ,
we show that submodular maximization subject to a matroid base constraint
does not admit a -approximation for any ,
improving the previously known threshold of  \cite{Vondrak09}. 
These results rely on the notion of a {\em symmetry gap} and the hardness construction of
\cite{Vondrak09}.



\begin{figure}[here]

\caption{Summary of results:  is nonnegative submodular,
 denotes independent sets in a matroid, and
 bases in a matroid.  - in this line we assume the case
where the matroid contains two disjoint bases.
The hardness results hold in the value oracle model.}
\end{figure}


The rest of the paper is organized as follows. In Section~\ref{sec:prelims},
we discuss the notions of multilinear relaxation and simulated annealing,
which form the basis of our algorithms. In Section~\ref{sec:unconstrained},
we describe and analyze our -approximation for unconstrained submodular
maximization. In Section~\ref{sec:matroid-constraint}, we describe
our -approximation for submodular maximization subject to a matroid
independence constraint. In Section~\ref{sec:hardness},
we present our hardness results.
Many details are deferred to the appendix.


\section{Preliminaries}
\label{sec:prelims}

Our algorithm combines the following two concepts. The first one is
{\em multilinear relaxation}, which has recently proved to be
very useful for optimization problems involving submodular functions
(see \cite{CCPV07,Vondrak08,CCPV09,KST09,LMNS09,Vondrak09}).
The second concept is {\em simulated annealing},
which has been used successfully by practitioners dealing with difficult optimization problems.
Simulated annealing provides good results in many practical scenarios, but typically
eludes rigorous analysis (with several exceptions in the literature:
see e.g. \cite{BT93} for general convergence results, 
\cite{LV03,KV06} for applications to volumes estimation and optimization over convex bodies,
and \cite{SVV07,BSVV08} for applications to counting problems).



\medskip
\noindent{\bf Multilinear relaxation.}
Consider a submodular function . We define a continuous function
 as follows: For , let  be a random
set which contains each element  independently with probability . Then we define

This is the unique multilinear polynomial in  which coincides with 
on the points  (we identify such points with subsets  in a natural way).
Instead of the discrete optimization problem

where  is the family of feasible sets,
we consider a continuous optimization problem

where  is the polytope associated with .
It is known due to \cite{CCPV07,CCPV09,Vondrak09} that any fractional solution 
 where  are either all subsets, or independent sets in a matroid,
or matroid bases, can be rounded to an integral solution  such that
 . 
Our algorithm can be seen as a new way of approximately solving the relaxed
problem .

\iffalse
\begin{lemma}
\label{lem:pipage-rounding}
For any matroid , let  be either the matroid polytope (convex hull of independent sets)
or the matroid base polytope (convex hull of bases). Then there is an efficient randomized
rounding technique which for any point  finds a random vertex  of  such that
, and for any submodular function  and its multilinear extension ,

\end{lemma}
\fi

\medskip
\noindent{\bf Simulated annealing.}
The idea of simulated annealing comes from physical processes such as gradual cooling of molten metals,
whose goal is to achieve the state of lowest possible energy. The process starts at a high temperature
and gradually cools down to a "frozen state". The main idea behind gradual cooling is that
while it is natural for a physical system to seek a state of minimum energy,
this is true only in a local sense - the system does not have any knowledge of the global structure
of the search space. Thus a low-temperature system would simply find a local optimum and get stuck there,
which might be suboptimal. Starting the process at a high temperature means that there is more randomness
in the behavior of the system. This gives the system more freedom to explore the search space,
escape from bad local optima, and converge faster to a better solution.
We pursue a similar strategy here.

We should remark that our algorithm is somewhat different from a direct interpretation of
simulated annealing. In simulated annealing, the system would typically evolve as a random walk,
with sensitivity to the objective function depending on the current temperature. Here,
we adopt a simplistic interpretation of temperature as follows.
Given a set  and , we define a probability distribution
 by starting from  and adding/removing each element independently with probability .
Instead of the objective function evaluated on , we consider the expectation over
the distribution .
This corresponds to the {\em noise operator} used in the analysis of boolean functions,
which was implicitly also used in the -approximation algorithm of \cite{FMV07}.
Observe that ,
where  is the multilinear extension of .
The new idea here is that the parameter  plays a role similar to temperature
- e.g.,  means that  is uniformly random regardless of 
("infinite temperature" in physics), while  means that there are no fluctuations
present at all ("absolute zero").

We use this interpretation to design an algorithm inspired by simulated annealing:
Starting from ,
we perform local search on  in order to maximize .
Note that for  this function does not depend on  at all,
and hence any solution is a local optimum.
Then we start gradually decreasing ,
while simultaneously running a local search with respect to .
Eventually, we reach  where
the algorithm degenerates to a traditional local search and returns an
(approximate) local optimum.

We emphasize that we maintain the solution generated by previous stages
of the algorithm, as opposed to running a separate local search for each value of . 
This is also used in the analysis, whose main point is to estimate
how the solution improves as a function of .
It is not a coincidence that the approximation provided by our algorithm
is a (slight) improvement over previous algorithms. Our algorithm can be viewed
as a dynamic process which at each fixed temperature  corresponds to a certain
variant of a previous algorithm. We prove that the performance of the simulated annealing
process is described by a differential equation, whose initial condition can be
related to the performance of a previously known algorithm. Hence the fact that an improvement
can be achieved follows from the fact that the differential equation yields
a positive drift at the initial point. The exact quantitative improvement depends
on the solution of the differential equation, which we also present in this work.

\medskip
\noindent{\bf Notation.}
In this paper, we denote vectors consistently in boldface: for example .
The coordinates of  are denoted by . Subscripts next to a boldface
symbol, such as , denote different vectors. In particular, we use the notation
 to denote a vector with coordinates  for  and 
for . 
In addition, we use the following notation to denote the value of
certain fractional solutions:
\vspace{-5pt}

For example, if  and , the diagram would represent .
Typically,  will be our current solution, and  an optimal solution.
Later we omit the symbols  from the diagram.



\section{Unconstrained Submodular Maximization}
\label{sec:unconstrained}

Let us describe our algorithm for unconstrained submodular maximization.
We use a parameter , which is related to the ``temperature''
discussed above by . We also use a fixed discretization parameter .

\begin{algorithm}
\caption{Simulated Annealing Algorithm For Submodular Maximization}
\label{alg:simulated_unconstrained}
\begin{algorithmic}[1]
\INPUT A submodular function .
\OUTPUT A subset  satisfying .
\STATE {\bf{Define}} .
\STATE .
\FOR {}
\WHILE {there exists  such that }
\STATE   
\ENDWHILE
\ENDFOR
\RETURN the best solution among all sets  and  encountered by the algorithm.
\end{algorithmic}
\end{algorithm}


We remark that this algorithm would not run in polynomial time, due to the complexity of finding
a local optimum in Step 4-6. This can be fixed by standard techniques (as in
 \cite{FMV07,LMNS09,LSV09,Vondrak09}), by stopping when the conditions
of local optimality are satisfied with sufficient accuracy. 
We also assume that we can evaluate the multilinear extension , which can be
done within a certain desired accuracy by random sampling.
Since the analysis of the algorithm is already quite technical, we ignore these issues
in this extended abstract and assume instead that a true local optimum is found in Step 4-6.

\begin{theorem}
\label{thm:0.41-approx}
For any submodular function , Algorithm \ref{alg:simulated_unconstrained} returns with
high probability a solution of value
at least  where .
\end{theorem}
In Theorem \ref{thm:tightexample} we also show that Algorithm \ref{alg:simulated_unconstrained}
does not achieve any factor better than .
First, let us give an overview of our approach
and compare it to the analysis of the -approximation in \cite{FMV07}.
The algorithm of \cite{FMV07}
can be viewed in our framework as follows: for a fixed value of , it performs local search
over points of the form , with respect to element swaps in ,
and returns a locally optimal solution. Using the conditions of local optimality,
 can be compared to the global optimum.
Here, we observe the following additional property of a local optimum.
If  is a local optimum
with respect to element swaps in , then slightly increasing  cannot decrease the value
of . 
During the local search stage, the value cannot decrease either,
so in fact the value of  is non-decreasing throughout the algorithm.
Moreover, we can derive bounds on  depending
on the value of the current solution. Consequently,
unless the current solution is already valuable enough, we can conclude that an improvement
can be achieved by increasing .
This leads to a differential equation whose solution implies Theorem~\ref{thm:0.41-approx}.

We proceed slowly and first prove the basic fact that if  is
a local optimum for a fixed , we cannot lose by increasing  slightly.
This is intuitive, because the gradient  at 
must be pointing away from the center of the cube , or else we could gain
by a local step.

\begin{lemma}
\label{lem:positive-drift}
Let  and suppose  is a local optimum in the sense that
 for all . Then
\begin{itemize}
\item  if ,
and  if ,
\item .
\end{itemize}
\end{lemma}

\begin{proof}
We assume that flipping the membership of element  in  can only decrease the value
of . The effect of this local step on  is that the value of the -th coordinate
changes from  to  or vice versa (depending on whether  is in  or not).
Since  is linear when only one coordinate is being changed, this implies
 if , and  if .
By the chain rule, we have

Since  if  and  otherwise, we get

using the conditions above.
\end{proof}

In the next lemma, we prove a stronger bound on the derivative 
which will be our main tool in proving Theorem \ref{thm:0.41-approx}.
This can be combined with the analysis of \cite{FMV07} to achieve a certain improvement.
For instance, \cite{FMV07} implies that if  is a local optimum for ,
we have either , or . 
Suppose we start our analysis from the point . (The algorithm does not need
to be modified, since at  it finds a local optimum in any case, and this is
sufficient for the analysis.)
We have either  or ,
or else by the following lemma,  is a constant fraction of :

Therefore, in some -sized interval, the value of 
will increase at a slope proportional to . Thus the approximation factor of
 Algorithm \ref{alg:simulated_unconstrained} is strictly greater than .
We remark that we use a different starting point to achieve the factor of ,
and we defer the precise analysis to Appendix~\ref{app:unconstrained}.



\begin{lemma}
\label{lem:drift-bound}
Let ,  and suppose  is
a local optimum in the sense that 
 for all . Then

\end{lemma}

\begin{proof}
Let  denote an optimal solution, i.e. .
Let  denote a local optimum with respect to , and  its complement.
In our notation using diagrams,

The top row is the current solution , the bottom row is its complement ,
and the left-hand column is the optimum .
We proceed in two steps. Define 

to denote the derivative of  when moving from  towards the actual optimum .
By Lemma~\ref{lem:positive-drift}, we have

using the definition of  and the fact that
 for  and
 for .


Next, we use Lemma~\ref{lem:submod-change} to estimate  as follows.
To simplify notation, we denote  simply by . 
If we start from  and increase the coordinates in  by 
and those in  by , Lemma~\ref{lem:submod-change} says
the value of  will change by 

Similarly, if we decrease the coordinates in  by  and
those in  by , the value will change by

Adding inequalities\eqref{eq:unconstrained_one}, \eqref{eq:unconstrained_zero}
and noting the expression for  above, we obtain:

It remains to  relate the LHS of equation \eqref{eq:prior_differentialeq} to the value of . We use the "threshold lemma" (see Lemma~\ref{lem:threshold}, and the accompanying example with equation \eqref{eq:threshold_easy}):


Combining these inequalities with (\ref{eq:prior_differentialeq}), we get

Recall that . Finally, we add  to this inequality,
so that we can use submodularity to take advantage
of the last two terms: 
\end{proof}

\iffalse
\begin{proof}
Let  denote an optimal solution, i.e. .
Let  denote a local optimum with respect to , and  its complement.
We denote by  the fractional solution ; in our notation using diagrams,
its value is denoted by

The top row is the current solution , the bottom row is its complement ,
and the left-hand column is the optimum .
By Lemma~\ref{lem:positive-drift}, we know that  is non-negative
for  and non-positive for .
By Lemma~\ref{lem:submod-change},
if we increase the coordinates of  by  and the coordinates of 
by , the value increases by at most  for each element
, and at most by  for each element
:

where the last inequality holds because  for all .
Similarly, if we decrease the coordinates of  by  and
the coordinates of  by , we obtain

where the last inequality holds by the fact that 
 for all . 
By subtracting inequality \eqref{eq:unconstrained_zero} from \eqref{eq:unconstrained_one},
we obtain:

where we used Lemma~\ref{lem:positive-drift}, noting that all of the elements in 
have non-negative partial derivatives, while the elements in  have non-positive partial derivatives.

It remains to  relate the LHS of equation \eqref{eq:prior_differentialeq} to the value of . 
We use Lemma~\ref{lem:threshold},
which says that ,
where  and 
 is uniformly random.
We apply this to the following two diagrams in the role of :


Combining these inequalities with (\ref{eq:prior_differentialeq}), we get

We recall that . Finally, we need to add  to this inequality,
so that we can use submodularity to take advantage
of the last two terms: 
\end{proof}
\fi

We have proved that unless the current solution is already very valuable, there is a certain
improvement that can be achieved by increasing . The next lemma transforms this statement
into an inequality describing the evolution of the simulated-annealing algorithm.

\begin{lemma}
\label{lem:annealing-dynamics}
Let  denote the local optimum found by the simulated annealing algorithm 
at temperature ,
and let  denote its value. Assume also
that for all , we have . Then

\end{lemma}

\begin{proof}
Here we combine the positive drift obtained from decreasing the temperature
(described by Lemma~\ref{lem:drift-bound})
and from local search (which is certainly nonnegative).
Consider the local optimum  obtained at temperature .
Its value is .
By decreasing temperature by , we obtain a solution ,
whose value can be estimated in the first order by the derivative at 
(see Lemma~\ref{lem:Taylor} for a precise argument):

This is followed by another local-search stage, in which we obtain a new local optimum .
In this stage, the value of the objective function cannot decrease, so we have
.
We have .
We also estimate  using Lemma~\ref{lem:positive-drift}, to obtain

Finally, we use  and 
to derive the statement of the lemma.
\end{proof}

We only sketch the remainder of the analysis. By taking ,
the statement of
Lemma~\ref{lem:annealing-dynamics} leads naturally to the following differential equation:

This equation can be solved analytically. Starting from initial condition ,
we get for any :
 
Choosing the starting point is a non-trivial issue; for example  and 
(the uniformly random approximation of \cite{FMV07}) does not give any improvement over .
It turns out that the best choice is ,
even though the corresponding value  is less than .  We prove that we can pick
a value  such that the solution of the differential equation starting at 
 reaches a point  such that .
Details can be found in Appendix~\ref{app:unconstrained}.




\section{Matroid Independence Constraint}
\label{sec:matroid-constraint}


Let  be a matroid.
We design an algorithm for the case of submodular maximization subject
to a matroid independence constraint, , as follows. The algorithm uses
fractional local search to solve the optimization problem ,
where  is a matroid polytope intersected with a box.
This technique, which has been used already in \cite{Vondrak09},
is combined with a simulated annealing procedure, where the parameter
 is gradually being increased from  to . (The analogy with simulated annealing
is less explicit here; in some sense the system exhibits the most randomness in the middle
of the process, when .)  Finally, the fractional solution is rounded
using pipage rounding \cite{CCPV07,Vondrak09}; we omit this stage from the description
of the algorithm.

The main difficulty in designing the algorithm is how to handle the temperature-increasing
step. Contrary to the unconstrained problem, we cannot just increment all variables
which were previously saturated at , because this might violate the matroid constraint.
Instead, we find a subset of variables that can be increased, by reduction to a bipartite matching
problem. We need the following definitions.

\begin{definition}
\label{def:best-match}
Let  be an extra element not occurring in the ground set , and define formally
. 
For  and ,
we define 

\end{definition}

In other words,  is the least valuable element which can be exchanged for 
in the independent set . Note that such an element must exist due to matroid axioms.
We also consider  as an option in case  itself is independent.
In the following,  can be thought of as a special ``empty'' element,
and the partial derivative  is considered identically equal to zero.
By definition, we get the following statement.

\begin{lemma}
\label{lem:best-match}
For  defined as above, we have

\end{lemma}

The following definition is important for the description of our algorithm.

\begin{definition}
\label{def:ex-graph}
For , let .
We define a bipartite ``fractional exchange graph''  on  as follows:
We have an edge , whenever .
We define its weight as 

\end{definition}

We remark that the vertices of the bipartite exchange graph are not elements of  on both sides, but elements on 
one side and independent sets on the other side.
Now we can describe our algorithm.


\begin{algorithm}[htb]
\caption{Simulated Annealing Algorithm for a Matroid Independence Constraint}
\label{alg:simulated_matroidindependence}
\begin{algorithmic}[1]
\INPUT A submodular function  and a matroid .
\OUTPUT An independent set  such that .
\STATE Let ,   and .
\STATE {\bf{Define}} 
\STATE Maintain a representation of  where . \FOR {}
\WHILE {there is 
such that  and } 
\STATE   \COMMENT{{\bf{Local search}}}
\ENDWHILE
\FOR[{{\bf Complementary solution check}}]{each of the  possible sets }
\STATE Find a local optimum  trying to maximize .
\STATE Remember the largest  as  a possible candidate for the output of the algorithm\ENDFOR
\STATE Form the fractional exchange graph (see Definition~\ref{def:ex-graph}) 
and find a max-weight matching . 
\STATE Replace  by  for each edge , and
update the  point .
\COMMENT{{\bf Temperature relaxation}: each coordinate increases by at most
 and hence .}
\ENDFOR
\RETURN  the best encountered solution.
\end{algorithmic}
\end{algorithm}


\begin{theorem}
\label{thm:0.325-approx}
For any submodular function  and matroid ,
Algorithm \ref{alg:simulated_matroidindependence} returns with high probability
a solution of value at least  where .
\end{theorem}

Let us point out some differences between the analysis of this algorithm
and the one for unconstrained maximization (Algorithm~\ref{alg:simulated_unconstrained}).
The basic idea is the same: we obtain certain conditions for partial derivatives
at the point of a local optimum. These conditions help us either to conclude
that the local optimum already has a good value, or to prove that by relaxing
the temperature parameter we gain a certain improvement. 
We will prove the following lemma which is analogous to Lemma \ref{lem:annealing-dynamics}.

\begin{lemma}
\label{lem:diffeq}
Let  denote the local optimum found by Algorithm \ref{alg:simulated_matroidindependence}
at temperature  right after the ``Local search'' phase,
and let  denote the value of this local optimum.
Also assume that the solution found in ``Complementary solution check'' phase
of the algorithm (Steps 8-10) is always at most .
Then the function  satisfies

\end{lemma}

We proceed in two steps, again using as an intermediate bound
the notion of derivative of  on the line towards the optimum:
.
The plan is to relate the actual gain of the algorithm in the ``Temperature relaxation''
 phase (Steps 12-13) to , and then to argue that  can be compared
to the RHS of (\ref{eq:localgains}).
The second part relies on the submodularity of the objective function and is quite similar
to the second part of Lemma~\ref{lem:drift-bound} (although slightly more involved).

The heart of the proof is to show that by relaxing the temperature
we gain an improvement at least . As the algorithm suggests,
the improvement in this step is related to the weight of the matching obtained in Step 12
of the algorithm. Thus the main goal is to prove that there exists a matching of weight
at least . We prove this by a combinatorial argument using
the local optimality of the current fractional solution, and 
an application of K\"{o}nig's theorem on edge colorings of bipartite graphs.
We defer all details of the proof to Appendix~\ref{app:matroid}.

Finally, we arrive at a differential equation of the following form:

This differential equation is very similar to the one we obtained in 
Section~\ref{sec:unconstrained} and can be solved analytically as well.
We start from initial conditions corresponding to the -approximation
of \cite{Vondrak09}, which implies that 
a fractional local optimum at  has value
. We prove that there is a value
 such that
for some value of  (which turns out to be roughly ),
we get . We defer details to Appendix~\ref{app:matroid}.





\section{Hardness of approximation} 
\label{sec:hardness}

In this section, we improve the hardness of approximating several submodular maximization
problems subject to additional constraints (i.e. ),
assuming the value oracle model. We use the method of {\em symmetry gap}
\cite{Vondrak09} to derive these new results. 
This method can be summarized as follows.
We start with a fixed instance  which is symmetric
under a certain group of permutations of the ground set . We consider
the multilinear relaxation of this instance, .
We compute the {\em symmetry gap} ,
where  is the optimum of the relaxed
problem and  is the optimum
over all {\em symmetric} fractional solutions, i.e. satisfying 
for any . Due to \cite[Theorem 1.6]{Vondrak09}, we obtain hardness
of -approximation for a class of related instances, as follows.

\begin{theorem}[\cite{Vondrak09}]
\label{thm:symmetrygap}
Let  be an instance of a nonnegative submodular
maximization problem with symmetry gap .
Let  be the class of instances 
where  is nonnegative submodular and  is a ``refinement``
of . Then for every , any -approximation algorithm
for the class of instances  would require exponentially many value queries
to . 
\end{theorem}

For a formal definition of ''refinement``, we refer to \cite[Definition 1.5]{Vondrak09}.
Intuitively, these are ''blown-up`` copies of the original family of feasible sets,
such that the constraint is of the same type as the original instance (e.g. cardinality,
matroid independence and matroid base constraints are preserved).

\paragraph{Directed hypergraph cuts.}
Our main tool in deriving these new results is a construction using a variant of the Max Di-cut
problem in {\em directed hypergraphs}. We consider the following variant of directed
hypergraphs.


\begin{definition}
A directed hypergraph is a pair , where  is a set of directed hyperedges
, where  is a non-empty subset of vertices and  is
a vertex in . 

For a set , we say that a hyperedge  is cut by , or ,
if  and .
\end{definition}

Note that a directed hyperedge should have exactly one head. 
An example of a directed hypergraph is shown in Figure \ref{fig:hardness_matroidbase}.
We will construct our hard examples as Max Di-cut instances on directed hypergraphs.
It is easy to see that the number (or weight) of  hyperedges cut by a set   is indeed submodular
 as a function of .
Other types of directed hypergraphs have been considered, in particular
with hyperedges of multiple heads and tails, but a natural extension of the cut
function to such hypergraphs is no longer submodular.

In the rest of this section, 
we present our hardness result 
for maximizing submodular functions subject to a matroid base constraint.
We defer the remaining results to Appendix~\ref{app:hardness}.


\iffalse
\subsection{Submodular maximization over matroid bases}
\label{subsec:matroidbase}
First we consider the problem of maximizing a submodular function subject
to a matroid base constraint. Vondrak in \cite{Vondrak09} proved a hardness
factor of  when the base packing number \cite[Definition 1.2]{Vondrak09}
of the matroid is . Roughly speaking, the base packing number is the
maximum number of disjoint bases of the matroid that can be embedded in an instance of the problem.
On the other hand, when there are two disjoint bases (i.e. ),
the best known hardness result is 0.5 which is the hardness of approximating any symmetric submodular function \cite{FMV07}. We show that submodular maximizing problem when the base packing number is at least 2 is  strictly harder than the unconstrained case by showing that it can not be approximated better than 0.393.
\fi

\begin{theorem}
\label{thm:hard_matroidbase}
There exist instances of the problem ,
where  is a nonnegative submodular function,  is a collection of matroid bases
of packing number at least , and any -approximation
for this problem would require exponentially many value queries for any .
\end{theorem}

We remark that , and only hardness of -approximation
was previously known in this setting. 

\medskip
\noindent{\bf Instance 1.}
Consider the hypergraph in Figure \ref{fig:hardness_matroidbase},
with the set of vertices  and two hyperedges  and
. Let  be the cut function on this graph,
and let  be a partition matroid whose independent sets contain at most
one vertex from each of the sets  and . Let  be the bases of 
(i.e. ).
Note that there exist two disjoint bases in this matroid
and the base packing number of  is equal to 2.  
An optimum solution is for example  with . 

\begin{figure}
\centering


\def\sl {3}
\def \ll {6.5}
\def\hi{3.5}
\def\rl{.6}





\def \Pointsize {1pt}
\begin{tikzpicture}[inner sep=1.2pt,scale=.85,pre/.style={<-,shorten <=2pt,>=stealth,thick}, post/.style={->,shorten >=1pt,>=stealth,thick}]
\tikzstyle{every node}=[draw,circle];
\path [line width=1.5,red,inner sep=2.6pt] (0,\hi) node (a) {};
\path [inner sep=2.3pt] (\ll,\hi) node (b) {};
\path [outer sep=0pt] (a) +(-3,-\hi)  node (a1) {};
\path (a) +(-2,-\hi)  node (a2) {};
\path (a) +(-1,-\hi)  node (a3) {};
\path (a) +(2,-\hi)  node (ak) {};
\path [line width=1.5,color=red] (b) +(-3,-\hi)  node (b1) {};
\path (b) +(-2,-\hi)  node (b2) {};
\path (b) +(-1,-\hi)  node (b3) {};
\path (b) +(2,-\hi)  node (bk) {};




\tikzstyle{every node}=[];
\path (a) +(0,-1.5) node (am) {} ;
\path (am) +(.03,.25) node (apm) {} ;
\path (b) +(0,-1.5 ) node (bm) {};
\path (bm) +(0.03,.25) node (bpm) {} ;

\draw [loosely dotted, line width=2pt] (a3) +(.6,0) -- +(2.4,0);
\draw [loosely dotted, line width=2pt] (b3) +(.6,0) -- +(2.4,0);

\path[->,line width=1.5pt] (am) edge (a)
	    (bm) edge  (b);
\path  [-,line width=1pt]    (a1) edge [bend right=20] node [above=15] {} (apm)
	     (a2) edge [bend right=10] (apm)
	     (a3) edge [bend right=5] (apm)
	    (ak) edge [bend left] (apm)
	   (b1) edge [bend right=20] node [above=15] {} (bpm)
	     (b2) edge [bend right=10] (bpm)
	     (b3) edge [bend right=5] (bpm)
	    (bk) edge [bend left]  (bpm);
\draw (a) +(-\rl,\rl) node (al){};
\draw (b) +(\rl,-\rl) node (br){};
\draw (bk) + (\rl,-\rl) node (bkr){};
\draw (a1) +(-\rl,\rl) node (a1l){};
\draw [color=gray,dashed] (al) rectangle (br)  (a1l)  rectangle (bkr);

\draw (al) +(-.5,-.5) node {{\Large{A}}};
\draw (a1l) +(-.5,-.5) node {{\Large{B}}};

\end{tikzpicture} 


 \caption{Example for maximizing a submodular function subject to a matroid base constraint;
the objective function is a directed hypergraph cut function, and the constraint is
that we should pick exactly 1 element of  and 1 element of .}
\label{fig:hardness_matroidbase}
\end{figure}

In order to apply Theorem \ref{thm:symmetrygap} we need to compute the symmetry gap
of this instance .
We remark in the blown-up instances,  corresponds to the maximum value
that any algorithm can obtain, while  is the actual optimum.
The definition of  depends on the symmetries of our instance,
which we describe in the following lemma.



\begin{lemma}
\label{lem:symmetricgroups}
There exists a group  of permutations such that Instance 1
is symmetric under , in the sense that 

Moreover, for any two vertices  (or ), the probability that
 for a uniformly random  is equal to 
(or  respectively).
\end{lemma}

\begin{proof}
Let  be the set of the following two basic permutations

where  swaps the vertices of the two hyperedges and 
only rotates the tail vertices of  one of the hyperedges.
It is easy to see that both of these permutations satisfy equation
 \eqref{eq:invariant_permuation}. Therefore, our instance is {\em{invariant}}
under each of the basic permutations and also under any permutation generated by them.
Now let  be the set of all the permutations that are generated by .
 is a {\em group} and under this group of symmetries all the elements
in  (and ) are equivalent. In other words, for any three vertices ,
the number of permutations  such that  is equal to the
number of permutations such that .
\end{proof}

Using the above lemma we may compute the {\em{symmetrization}} of a vector 
which will be useful in computing  \cite{Vondrak09}.
For any vector , the ``symmetrization of ''  is:

where  denotes  with coordinates permuted by .
Now we are ready to prove Theorem \ref{thm:hard_matroidbase}.\\


\begin{proof}[Theorem \ref{thm:hard_matroidbase}]
We need to compute the value of symmetry gap , where  is the multilinear relaxation of 
and  is the convex hull of the bases in .
For any vector , we have 

By equation \eqref{eq:symmetrization} we know that the vertices in each of the sets , 
have the same value in . Using equation \eqref{eq:baseproperty},
we obtain  and 
for all ,
which yields a unique symmetrized solution . 

Now we can simply compute . 
Note that by definition a hyperedge will be cut by a random set 
if and only if at least one of its tails are included in  while its head is not included.
Therefore 

for sufficiently large . By applying Theorem~\ref{thm:symmetrygap}, it can be seen
that the refined instances are instances of submodular maximization over the bases
of a matroid where the ground set is partitioned into  and we have to take half
of the elements of  and  fraction of the elements in .
Thus the base packing number of the matroid in the refined instances is also 2
which implies the theorem.
\end{proof}


\paragraph{Acknowledgment.}
We would like to thank Tim Roughgarden for stimulating discussions.


\begin{thebibliography}{99}

\bibitem{Austrin10} P. Austrin.
Improved inapproximability for submodular maximization,
to appear in {\em APPROX 2010}.

\bibitem{BT93} D. Bertsimas and J. Tsitsiklis.
Simulated annealing,
{\em Statistical Science} 8:1 (1993), 10--15.

\bibitem{BSVV08} I. Bez\'akov\'a, D. \v{S}tefankovi\v{c}, V. Vazirani and E. Vigoda.
Accelerating simulated annealing for the permanent and combinatorial counting problems.
{\em SIAM Journal of Computing} 37:5 (2008), 1429--1454.

\bibitem{CCPV07} G. Calinescu, C. Chekuri, M. P\'al and J. Vondr\'ak.
Maximizing a submodular set function subject to a matroid constraint,
{\em Proc. of 12th IPCO} (2007), 182--196.

\bibitem{CCPV09} G. Calinescu, C. Chekuri, M. P\'al, J. Vondr\'ak.
Maximizing a submodular set function subject to a matroid constraint,
to appear in \emph{SIAM J. on Computing}.

\bibitem{CVZ10} C. Chekuri, J. Vondr\'ak and R. Zenklusen.
Dependent randomized rounding via exchange properties of combinatorial structures,
to appear in {\em Proc. of  FOCS} (2010).

\bibitem{Feige98} U. Feige.
A threshold of  for approximating Set Cover,
{\em Journal of the ACM} 45 (1998), 634--652.

\bibitem{FG95} U. Feige and M. X. Goemans.
Approximating the value of two-prover systems, with applications to MAX-2SAT and MAX-DICUT,
{\em Proc. of the 3rd Israel Symposium on Theory and Computing Systems}, Tel Aviv (1995), 182--189.

\bibitem{FMV07} U. Feige, V. Mirrokni and J. Vondr\'ak.
Maximizing non-monotone submodular functions,
\emph{Proc. of 48th IEEE FOCS} (2007), 461--471.

\bibitem{NWF78II} M. L. Fisher, G. L. Nemhauser and L. A. Wolsey.
An analysis of approximations for maximizing submodular set functions II,
{\em Mathematical Programming Study} 8 (1978), 73--87.

\bibitem{FFI00}  L. Fleischer, S. Fujishige and S. Iwata.
A combinatorial, strongly polynomial-time algorithm for minimizing submodular functions,
{\em Journal of the ACM} 48:4 (2001), 761--777.

\bibitem{Frank97} A. Frank.
Matroids and submodular functions,
{\em Annotated Biblographies in Combinatorial Optimization} (1997), 65--80.

\bibitem{GKTW09} G. Goel, C. Karande, P. Tripathi and L. Wang.
Approximability of combinatorial problems with multi-agent submodular cost functions ,
In {\em Proc. of  FOCS} (2009), 755--764.

\bibitem{GW95} M. X. Goemans and D. P. Williamson.
Improved approximation algorithms for maximum cut and satisfiability problems
using semidefinite programming, {\em Journal of the ACM} 42 (1995), 1115--1145.

\bibitem{GSTT99} B. Goldengorin, G. Sierksma, G. Tijsssen and M. Tso.
The data correcting algorithm for the minimization of supermodular functions,
{\em Management Science}, 45:11 (1999), 1539--1551.

\bibitem{GTT99} B. Goldengorin, G. Tijsssen and M. Tso.
The maximization of submodular functions: Old and new proofs for the
correctness of the dichotomy algorithm, {\em SOM Report},
University of Groningen (1999).

\bibitem{GLS81} M. Gr\"{o}tschel, L. Lov\'asz and A. Schrijver.
The ellipsoid method and its consequences in combinatorial optimization ,
{\em Combinatorica} 1:2 (1981), 169--197.

\bibitem{GRST10} A. Gupta, A. Roth, G. Schoenebeck and K. Talwar.
Constrained non-monotone submodular maximization: offline and secretary algorithms,
manuscript, 2010.

\bibitem{IN09} S. Iwata and K. Nagano.
Submodular function minimization under covering constraints,
In {\em Proc. of  FOCS} (2009), 671--680.

\bibitem{KV06} A. T. Kalai and S Vempala.
Simulated annealing for convex optimization,
{\em Math. of Operations Research}, 31:2 (2006), 253--266.

\bibitem{K89} V. R. Khachaturov.
Mathematical methods of regional programming (in Russian),
{\em Nauka, Moscow}, 1989.

\bibitem{KST09} A. Kulik, H. Shachnai and T. Tamir.
Maximizing submodular functions subject to multiple linear constraints,
\emph{Proc. of 20th ACM-SIAM SODA} (2009).

\bibitem{LMNS09} J. Lee, V. Mirrokni, V. Nagarajan and M. Sviridenko.
Non-monotone submodular maximization under matroid and knapsack constraints,
 \emph{Proc. of 41th ACM STOC} (2009), 323-332.

\bibitem{LSV09} J. Lee, M. Sviridenko and J. Vondr\'ak.
Submodular maximization over multiple matroids via generalized exchange properties,
{\em Proc. of APPROX 2009}, 244--257.

\bibitem{LNW96} H. Lee, G. Nemhauser and Y. Wang.
Maximizing a submodular function by integer programming:
Polyhedral results for the quadratic case,
{\em European Journal of Operational Research} 94 (1996), 154--166.

\bibitem{Lovasz83} L. Lov\'asz.
Submodular functions and convexity.
A. Bachem et al., editors, {\em Mathematical Programmming: The State of the Art}, 235--257.

\bibitem{LV03} L. Lov\'asz and S. Vempala.
Simulated annealing in convex bodies and an  volume algorithm,
In {\em Proc.  IEEE FOCS} (2003), 650--659.

\bibitem{RS89} T. Robertazzi and S. Schwartz.
An accelated sequential algorithm for producing D-optimal designs,
{\em SIAM Journal on Scientific and Statistical Computing} 10 (1989), 341--359.

\bibitem{Schrijver00} A. Schrijver.
A combinatorial algorithm minimizing submodular functions in strongly polynomial time,
{\em Journal of Combinatorial Theory, Series B} 80 (2000), 346--355.

\bibitem{Schrijver} A. Schrijver.
{\em Combinatorial optimization - polyhedra and efficiency.}
Springer, 2003.

\bibitem{SF08}
Z. Svitkina and L. Fleischer.
Submodular approximation: Sampling-based algorithms and lower bounds,
\emph{Proc. of 49th IEEE FOCS} (2008), 697--706.

\bibitem{SVV07} D. \v{S}tefankovi\v{c}, S. Vempala and E. Vigoda.
Adaptive simulated annealing: a near-optimal connection between sampling and counting,
{\em Journal of the ACM} 56:3 (2009), 1--36. 

\bibitem{Vondrak08} J. Vondr\'ak.
Optimal approximation for the submodular welfare problem in the value oracle model,
{\em Proc. of 40th ACM STOC} (2008), 67--74.

\bibitem{Vondrak09} J. Vondr\'ak.
Symmetry and approximability of submodular maximization problems,
{\em Proc. of 50th IEEE FOCS} (2009), 651--670.


\end{thebibliography}






\appendix

\section{Miscellaneous Lemmas}
\label{app:misc}

Let  be the multilinear extension of a submodular function.
The first lemma says that if we increase coordinates simultaneously, then the increase
in  is {\em at most} that given by partial derivatives at the lower point,
and {\em at least} that given by partial derivatives at the upper point.

\begin{lemma}
\label{lem:submod-change}
If  is the multilinear extension of a submodular function,
and  where , then

Similarly,

\end{lemma}

\begin{proof}
Since  is the multilinear extension of a submodular function,
we know that  for all  \cite{CCPV07}.
This means that whenever , the partial derivatives
at  cannot be larger than at :

Therefore, between  and , the highest partial derivatives
are attained at , and the lowest at .
By integrating along the line segment between  and , we obtain

If we evaluate the partial derivatives at  instead, we get

If we evaluate the partial derivatives at , we get

\end{proof}

For a small increase in each coordinate, the partial derivatives give
a good approximation of the change in ; this is a standard analytic argument,
which we formalize in the next lemma.

\begin{lemma}
\label{lem:Taylor}
Let  be twice differentiable,
 and . Then

 where the supremum is taken over all  and all points in .
\end{lemma}

\begin{proof}
Let .
Since  is twice differentiable, any partial derivative can change by at most
 when a coordinate changes by at most . Hence,

for any . By the fundamental theorem of calculus,

Similarly we get 
\end{proof}


The following ``threshold lemma`` appears as Lemma~A.4 in \cite{Vondrak09}.
We remark that the expression  defined below
is an alternative definition of the Lov\'asz extension of .

\begin{lemma}[Threshold Lemma]
\label{lem:threshold}
For  and , define . If  is the multilinear extension of a submodular
function , then for  uniformly random

\end{lemma}

Since we apply this lemma in various places of the paper let us describe
some applications of it in detail.

\begin{example}
In this example we apply the threshold lemma to the vector .
Here  represents the optimum set,   and .
If  is chosen uniformly at random  we know 
with probability ,  with probability 
and  with probability . Therefore by Lemma~\ref{lem:threshold} we have:

or equivalently we can write

\end{example}

In the next example we consider a more complicated application of the threshold lemma.
\begin{example}
Consider the vector   where  for  ,  for  and  for . In this case, we denote

Again  is the optimal set and . In this case if we apply the threshold lemma,
we get a random set which can contain a part of the block .
In particular, observe that if , then  contains all the
elements in , and depending on the value of ,  elements in  that are greater than . We denote the value of such a set by 

where the right-hand lower block is divided into two parts depending on the threshold .
Therefore 

can be written equivalently as

\end{example}


A further generalization of the threshold lemma is the following, which is also useful
in our analysis. (See \cite[Lemma A.5]{Vondrak09}.)

\begin{lemma}
\label{lem:threshold2}
For any partition ,

where  are independent and uniformly random in .
\end{lemma}



\section{Analysis of the -approximation}
\label{app:unconstrained}

Here we finish the analysis of the simulated annealing algorithm
for unconstrained submodular maximization (Theorem~\ref{thm:0.41-approx}).
Consider Lemma~\ref{lem:annealing-dynamics} in the limit when .
It gives the following differential inequality:


We assume here that  is so small that the difference
between the solution of this differential inequality and the actual
behavior of our algorithm is negligible. 
(We could replace  by , carry out the analysis
and then let ; however, we shall spare the reader of this annoyance.)
Our next step is to solve this differential equation, given certain initial conditions.
Without loss of generality, we assume that .

\begin{lemma}
\label{lem:diff-solution}
Assume that .
Let  denote the value of the solution at temperature .
Assume that  for some , and 
for all . Then for any ,
 
\end{lemma}

\begin{proof}
We rewrite Equation~(\ref{eq:diff-eq}) using the following trick:

Therefore, Lemma~\ref{lem:annealing-dynamics} states that

which is equivalent to

For any , the fundamental theorem of calculus implies that

Multiplying by , we obtain

\end{proof}

In order to use this lemma,
recall that the parameter  is an upper bound on the values of  throughout
the algorithm. This means that we can choose  to be our "target value":
if  achieves value more than  at some point, we are done.
If  is always upper-bounded by , we can use Lemma~\ref{lem:diff-solution},
hopefully concluding that for some  we must have .

In addition, we need to choose a suitable initial condition.
As a first attempt, we can try to plug in  and  as a starting point
(the uniformly random -approximation provided by \cite{FMV07}). We would obtain

However, this is not good enough. For example, if we choose  as our target value,
we obtain 
In can be verified that this function stays strictly below  for all .
So this does not even match the performance of the -approximation of \cite{FMV07}.

As a second attempt, we can use the -approximation itself as a starting point.
The analysis of \cite{FMV07} implies that if  is a local optimum for ,
we have either , or . This means that we can use
the starting point  with a target value of 
(effectively ignoring the behavior of the algorithm for ).
Lemma~\ref{lem:diff-solution} gives

The maximum of this function is attained at  which gives .
This is a good sign - however, it does not imply that the algorithm actually achieves
a -approximation, because we have used  as our target value.
(Also, note that , so this is not the way we achieve our main result.)

In order to get an approximation guarantee better than , we need to revisit the analysis of \cite{FMV07}
and compute the approximation factor of a local optimum as a function of the temperature 
and the complementary solution .

\begin{lemma}
\label{lem:starting-point}
Assume .
Let ,  and
let  be a local optimum with respect to .
Let . Then 

\end{lemma}

\begin{proof}
 is a local optimum with respect to the objective function .
We denote  simply by . Let  be a global optimum and .
As we argued in the proof of Lemma~\ref{lem:drift-bound}, we have

and also

We apply Lemma~\ref{lem:threshold2} which states that
,
where  are independent and uniformly random in .
This yields the following (after dropping some terms which are nonnegative):

The first term in each bound is .
However, to make use of the remaining terms, we must add some terms on both sides.
The terms we add are ;
it can be verified that both coefficients are nonnegative for .
Also, the coefficients are chosen so that they sum up to ,
the coefficient in front of the last term in each equation. Using submodularity, we get

Similarly, we get

Putting equations \eqref{eq:unconst_intial_localopt1}, \eqref{eq:unconst_intial_localopt2} \eqref{eq:unconst_intial_localopt3} and \eqref{eq:unconst_intial_localopt4} all together, we get

where the simplification came about by using the elementary relations
  and .
Submodularity implies 

and 

so we get, replacing the respective diagrams by ,  and ,

again using .
Finally, we assume that  and , which means

\end{proof}

\iffalse
\begin{proof}
 is a local optimum with respect to the objective function .
We denote  simply by . Let  be a global optimum and .
As we argued in the proof of Lemma~\ref{lem:drift-bound}, we have

and also

The fractional solutions here have the property that the fractional variables are constant
on each of 4 blocks: , ,  and .
More precisely, we have
\begin{itemize}
\item ,
\item .
\end{itemize}
We apply Lemma~\ref{lem:threshold2} which states that
,
where  are independent and uniformly random in .
This yields the following (after dropping some terms which are nonnegative):
\begin{itemize}
\item 
\item 
\end{itemize}
where we use . The first term in each bound is .
However, to make use of the remaining terms, we must add some terms on the left-hand side.
The terms we add are ;
it can be verified that both coefficients are nonnegative for .
Also, the coefficients are chosen so that they sum up to ,
the coefficient in front of  and . This implies the following, using submodularity:

Similarly, we get

Putting it all together, we get

where the simplification came about by using the elementary relations
  and .
Finally, submodularity implies 
and , so we get

again using .
Finally, we assume that  and , which means

\end{proof}
\fi

Now we can finally prove Theorem~\ref{thm:0.41-approx}. Consider Lemma~\ref{lem:diff-solution}.
Starting from , we obtain the following bound for any :

By optimizing this quadratic function, we obtain that the maximum is attained at
 and the corresponding bound is

Lemma~\ref{lem:starting-point} implies that a local optimum at temperature  has value
 
Therefore, we obtain
 
We choose  and solve for a value of  such that
. This value can be found as a solution of a quadratic equation
and is equal to

It can be verified that .
This completes the proof of Theorem~\ref{thm:0.41-approx}.



\section{Upper Bounding the Performance of the Simulated Annealing Algorithm}
\label{app:tightexample}


In this section we show that the simulated annealing algorithm
\ref{alg:simulated_unconstrained} for unconstrained submodular maximization,
does not give a half approximation even on instances of the directed 
maximum cut problem. We provide a directed graph  (found by an LP-solver)
and a set of local optimums for all values of , such the value
of  on each of them or their complement is at most  of . 

\begin{theorem}
\label{thm:tightexample}
There exists an instance of the unconstrained submodular maximization problem,
such that the approximation factor of Algorithm~\ref{alg:simulated_unconstrained}
is .
\end{theorem}



\begin{proof}
Let  be the cut function of the directed graph  in Figure \ref{fig:tightexample}.
We show that the set  is a local optimum for all  and the set  is a local optimum for all .
Moreover, since we have , it is possible
that in a run of the simulated annealing algorithm \ref{alg:simulated_unconstrained},
the set  is chosen and remains as a local optimum fort  to .
Then the local optimum changes to  and remains until the end of the algorithm. 
If the algorithm follows this path then its approximation ratio
is . This is because the value of the optimum set
, while .
We remark that even sampling from  (or from )
with probabilities  does not give value more than 17.

It remains to show that the set  is in fact a local optimum for all
. We just need to show that all the elements
in  have a  non-negative partial derivative and the elements in 
have a non-positive partial derivative. Let 
and , then:

\begin{center}
\begin{tabular}{lcl}
 & &  \\
 & & \\
  & &  \\
   & &  
\end{tabular}
\end{center}

Therefore,  is a local optimum for .
Similarly, it can be shown that  is a local optimum for 
which concludes the proof.
\end{proof}

\begin{figure}
\centering






\def \Pointsize {1.4pt}
\def\sl {3}
\def \ll {7}




\def \Pointsize {1pt}
\begin{tikzpicture}[scale=.8,pre/.style={<-,shorten <=2pt,>=stealth,thick}, post/.style={->,shorten >=1pt,>=stealth,thick}]
\tikzstyle{every node}=[draw,circle];
\path[line width=1.5pt,color=red] (45:\sl cm) node (v7) {};
\path[line width=1.5pt,color=red] (135:\sl cm) node (v6) {};
\path[line width=1.5pt,color=red] (225:\sl cm) node (v4) {};
\path[line width=1.5pt,color=red] (315:\sl cm) node (v5) {};
\path (45:\ll cm) node (v3) {};
\path (135:\ll cm) node (v2) {};
\path (225:\ll cm) node (v0) {};
\path (315:\ll cm) node (v1) {};



\tikzstyle{every node}=[];

\path[->] (v7) edge [post,bend right=20] node [above] {8} (v3)
	      (v3) edge [post,bend right=20] node [above] {4} (v7)
	      (v4) edge [post,bend left=30] node [above] {1} (v3)
	      (v3) edge [post,bend left=30] node [below] {11} (v4)
	      (v5) edge [post,bend right=20] node [below=10] {3} (v3)
	      (v3) edge [post, bend left=40] node [below=10] {1} (v5)
	      (v6) edge [post, bend left] node [above] {4} (v3) 
	      (v0) edge [post] node [above=10] {4} (v6) 
	      (v4) edge [post] node [below] {12} (v0) 
	      (v4) edge [post, bend right=10] node [above=10] {3} (v2) 
	      (v2) edge [post, bend right=10] node [above=10] {1} (v4) 
	      (v7) edge [post, bend left=10] node [above=-20] {4} (v1) 
	      (v1) edge [post] node [below] {4} (v4) ;

\draw [color=gray,dashed] (135:8cm) rectangle (10:6cm)  (80:6.5cm) rectangle (-45:8cm);
\draw (135:8cm) +(-.5,-.5) node {{\Large{B}}};
\draw (-45:8cm) +(.5,.5) node {{\Large{A}}};

\end{tikzpicture}


 \caption{Hard instance of the unconstrained submodular maximization problem,
 where Algorithm \ref{alg:simulated_unconstrained} may get value no more than .
 The bold vertices  represents the optimum set with value .}
\label{fig:tightexample}
\end{figure}




\section{Analysis of the -approximation}
\label{app:matroid}

Our first goal is to prove Lemma~\ref{lem:diffeq}. As we discussed,
the key step is to compare the gain in the temperature relaxation step
to the value of the derivative on the line towards the optimum,
.
We prove the following.

\begin{lemma}
\label{lem:temp-gain}
Let  be the the local optimum at time . Then

\end{lemma}

This lemma can be compared to the first part of the proof of Lemma \ref{lem:drift-bound},
which is not very complicated in the unconstrained case. As we said, the main difficulty here
is that relaxing the temperature does not automatically allow us to increase
all the coordinates with a positive partial derivative.
The reason is that the new fractional solution might not belong to .
Instead, the algorithm modifies coordinates according to a certain maximum-weight matching
found in Step 12. The next lemma shows that the weight of this matching
is comparable to .

\begin{lemma}
\label{lem:matching}
Let 
be a fractional local optimum, and  a global optimum.
Assume that . Let  be the fractional exchange graph
defined in Def.~\ref{def:ex-graph}.
Then  has a matching  of weight

\end{lemma}

\begin{proof}
We use a basic property of matroids (see \cite{Schrijver}) which says
that for any two independent sets , there is a
mapping  such that
for each ,  is independent,
and each element of  appears at most once as .
I.e.,  is a matching, except for the special element  which can be used
as  whenever .
Let us fix such a mapping for each pair , and denote the respective
mapping by .

Denote by  the sum of all positive edge weights in . We estimate
 as follows. For each  and each edge , we have
 and by Lemma~\ref{lem:best-match}

Observe that for , we get

because otherwise we could replace  by ,
which would increase the objective function (and for elements outside of ,
we have , so  can be increased).
Let us add up the first inequality over all elements 
and the second inequality over all elements :

where we used the fact that each element of 
appears at most once as , and 
for any element  (otherwise we could remove it
and improve the objective value).
Now it remains to add up these inequalities over all :

using .
The left-hand side is a sum of weights over a subset of edges.
Hence, the sum of all positive edge weights also satisfies

Finally, we apply K\"{o}nig's theorem on edge colorings of bipartite graphs:
Every bipartite graph of maximum degree  has an edge coloring using
at most  colors.
The degree of each node  is the number of sets  not containing ,
which is , and the degree of each node  is at most the number
of elements , by assumption . By K\"{o}nig's theorem,
there is an edge coloring using  colors. Each color class is a matching,
and by averaging, the positive edge weights in some color class have total weight

\end{proof}


The weight of the matching found by the algorithm corresponds to how
much we gain by increasing the parameter . Now we can prove Lemma \ref{lem:temp-gain}.

\medskip


\begin{proof}[Lemma \ref{lem:temp-gain}]
Assume the algorithm finds a matching . By Lemma~\ref{lem:matching},
its weight is

If we denote by  the fractional solution right after the ``Temperature relaxation'' phase, we have

Note that  is obtained by applying fractional local search to .
This cannot decrease the value of , and hence

Observe that up to first-order approximation, this increment is given by the
partial derivatives evaluated at . 
By Lemma~\ref{lem:Taylor}, the second-order term is proportional to :

and from above,

\end{proof}

It remains to relate  to the optimum (recall that ),
using the complementary solutions found in Step 9.
In the next lemma, we show that  is lower bounded by the RHS of equation
\eqref{eq:localgains}.

\begin{lemma}
\label{lem:OPT-bound}
Assume , ,
, and the value
of a local optimum on any of the subsets  is at most .
Then

\end{lemma}

\begin{proof}
Submodularity means that partial derivatives can only decrease when coordinates
increase. Therefore by Lemma \ref{lem:submod-change},

and similarly

Combining these inequalities, we obtain

Let  (and recall that  for all ). 
By applying the treshold lemma (see Lemma~\ref{lem:threshold} and the accompanying
example with equation \eqref{eq:threshold_hard}), we have:



By another application of Lemma~\ref{lem:threshold},

(We discarded the term conditioned on , where .)
It remains to combine this with a suitable set in the complement of .
Let  be a local optimum found inside .
By Lemma 2.2 in \cite{LMNS09},  can be compared to any feasible subset
of , e.g. , as follows:

We assume that  for any .
Let us take expectation over  uniformly random:

Now we can combine this with (\ref{eq:vee-exp}) and (\ref{eq:wedge-exp}):

where the last two inequalities follow from submodularity. 
Together with (\ref{eq:vee-wedge}), this finishes the proof.
\end{proof}




\begin{proof}[Lemma \ref{lem:diffeq}]
By Lemma~\ref{lem:temp-gain} and \ref{lem:OPT-bound}, we get

We have ,
which implies the lemma.
\end{proof}

Now by taking ,
the statement of Lemma~\ref{lem:diffeq} leads naturally to the following differential equation:

This differential equation is very similar to the one we obtained in 
Section~\ref{sec:unconstrained}. Let us assume that .
Starting from an initial point , the solution turns out to be

We start from initial conditions corresponding to the -approximation
of \cite{Vondrak09}. It is proved in \cite{Vondrak09} that a fractional local optimum
at  has value .
Therefore, we obtain the following solution for :

We solve for  such that the maximum of the right-hand side equals .
The solution is 

Then, for some value of  (which turns out to be roughly ),
we have . It can be verified that ;
this proves Theorem~\ref{thm:0.325-approx}.






\section{Additional Hardness Results}
\label{app:hardness}

\subsection{Matroid base constraints}

It is shown in \cite{Vondrak09} that it is hard to approximate submodular maximization
subject to a matroid base constraint with fractional base packing number ,
, better than .
We showed in Theorem~\ref{thm:hard_matroidbase} that for ,
the threshold of  can be improved to .
More generally, we show the following. 

\begin{theorem}
There exist instances of the problem ,
such that a  approximation for any  
would require exponentially many value queries.
Here  is a nonnegative submodular function,
and  is a collection of bases in a matroid with fractional base packing
number . 
\end{theorem}

\begin{proof}
Let .
Consider the hypergraph  in Figure~\ref{fig:hardness_matroidbase},
with  instead of  hyperedges. Similarly let   be the set of head (tail) vertices
respectively, and let the feasible sets be those that contain  vertices of  and
one vertex of . (i.e. ).
The optimum can simply select the heads of the first  hyperedges
and one of the tails of the last one, thus the value of  remains unchanged.
On the other hand,  will decrease since the number of symmetric elements
has increased and there is a greater chance to miss a hyperedge.
Similar to the proof of Lemma~\ref{lem:symmetricgroups} and
Theorem~\ref{thm:hard_matroidbase} we obtain a unique symmetrized
vector . Therefore,

for sufficiently large . Also it is easy to see that the feasible sets
of the refined instances, which are indeed the bases of a matroid,
are those that contain a  fraction of the vertices in 
and  fraction of vertices in . Therefore the fractional base packing number
of the refined instances is equal to .
\end{proof}


\subsection{Matroid independence constraint}
\label{subsec:matroidindependence}

In this subsection we focus on the problem of maximizing a submodular
function subject to a matroid independence constraint.
Similar to Section~\ref{sec:hardness},
we construct our hard instances using directed hypergraphs.

\begin{theorem}
\label{thm:hard_matroidindependence}
There exist instances of the problem 
where  is nonnegative submodular and  are independent sets in a matroid
such that a -approximation would require exponentially many value queries.
\end{theorem}

It is worth noting that the example we considered in Theorem \ref{thm:hard_matroidbase}
does not imply any hardness factor better than  for the matroid independence problem.
The reason is that for the vector , which is contained
in the matroid polytope , the value of the multilinear relaxation is .
In other words, it is better for an algorithm not to select any vertex in the heads set ,
and try to select as much as possible from . 


\medskip
\noindent{\bf Instance 2.} To resolve this issue,
we perturb the instance by adding an undirected edge  of weight 
and we decrease the weight of the hyperedges to , where the value of 
will be optimized later (see Figure~\ref{fig:hardness_matroidindependence}).
The objective function is again the (directed) cut function, where the edge 
contributes  if we pick exactly one of vertices .
Therefore the value of the optimum remains unchanged, .
On the other hand the optimal symmetrized vector  should have a non-zero
value for the head vertices, otherwise the edge  would not have any
contribution to .\\

\begin{figure}
\centering


\def\sl {3}
\def \ll {6.5}
\def\hi{3.5}
\def\rl{.6}





\def \Pointsize {1pt}

\begin{tikzpicture}[inner sep=1.2pt,scale=.85,pre/.style={<-,shorten <=2pt,>=stealth,thick}, post/.style={->,shorten >=1pt,>=stealth,thick}]
\tikzstyle{every node}=[draw,circle];
\path [line width=1.5pt,color=red,inner sep=2.6pt] (0,\hi) node (a) {};
\path [inner sep=2.3pt] (\ll,\hi) node (b) {};
\path  (a) +(-3,-\hi)  node (a1) {};
\path (a) +(-2,-\hi)  node (a2) {};
\path (a) +(-1,-\hi)  node (a3) {};
\path (a) +(2,-\hi)  node (ak) {};
\path [line width=1.5pt,color=red] (b) +(-3,-\hi)  node (b1) {};
\path (b) +(-2,-\hi)  node (b2) {};
\path (b) +(-1,-\hi)  node (b3) {};
\path (b) +(2,-\hi)  node (bk) {};




\tikzstyle{every node}=[];
\path (a) +(0,-1.5) node (am) {} ;
\path (am) +(.03,.25) node (apm) {} ;
\path (b) +(0,-1.5 ) node (bm) {};
\path (bm) +(0.03,.25) node (bpm) {} ;

\draw [loosely dotted, line width=2pt] (a3) +(.6,0) -- +(2.4,0);
\draw [loosely dotted, line width=2pt] (b3) +(.6,0) -- +(2.4,0);

\path[->,line width=1.5pt] (am) edge (a)
	    (bm) edge  (b);
\path  [-,line width=1pt]    (a1) edge [bend right=15] node [above=20] {} (apm)
	     (a2) edge [bend right=10] (apm)
	     (a3) edge [bend right=5] (apm)
	    (ak) edge [bend left] (apm)
	   (b1) edge [bend right=20] node [above=15] {} (bpm)
	     (b2) edge [bend right=10] (bpm)
	     (b3) edge [bend right=5] (bpm)
	    (bk) edge [bend left]  (bpm);
\path [-, line width=1pt]   (a) edge [] node [above] {} (b);
\draw (a) +(-\rl,\rl) node (al){};
\draw (b) +(\rl,-\rl) node (br){};
\draw (bk) + (\rl,-\rl) node (bkr){};
\draw (a1) +(-\rl,\rl) node (a1l){};
\draw [color=gray,dashed] (al) rectangle (br)  (a1l)  rectangle (bkr);

\draw (al) +(-0.5,-.5) node {{\Large{A}}};
\draw (a1l) +(-0.5,-.5) node {{\Large{B}}};

\end{tikzpicture}


 \caption{Example for maximizing a submodular function subject to a matroid independence
constraint;  the hypergraph contains two directed hyperedges of weight 
and the edge  of weight ; the constraint is that we pick
at most one vertex from each of  and .}
\label{fig:hardness_matroidindependence}
\end{figure}


\begin{proof}[Theorem \ref{thm:hard_matroidindependence}]
Let  be the hypergraph of Figure~\ref{fig:hardness_matroidindependence},
and consider the problem , where  is the cut function of 
and  is the set of independent sets of the matroid  defined in subsection
\ref{subsec:matroidindependence}. 
Observe that Lemma \ref{lem:symmetricgroups} can be applied to our instance  as well,
thus we may use equation \eqref{eq:symmetrization} to obtain the symmetrized
vectors .
Moreover, the matroid polytope can be described by the following equations:

Since the vertices of the set  only contribute as tails of hyperedges,
the value of  can only increase if we increase the value of  on
the vertices in . Therefore, we can assume (using equations \eqref{eq:symmetrization} and
\eqref{eq:indproperty}) that

Let ; we may compute the value of  as follows:

where .
By optimizing numerically over , we find that the smallest value of 
is obtained when . In this case we have . Also, similarly to Theorem \ref{thm:hard_matroidbase},
the refined instances are in fact instances of a submodular maximization 
problem over independent sets of a matroid (a partition matroid whose ground set
is partitioned into  and we have to take at most half of the elements of 
and  fraction of elements in ).
\end{proof}

\subsection{Cardinality constraint}

Although we do not know how to improve the hardness of approximating general submodular
functions without any additional constraint to a value smaller than , we can show
that adding a simple cardinality constraint makes the problem harder. In particular,
we show that it is hard to approximate a submodular function subject to a cardinality
constraint within a factor of . 

\begin{corollary}
There exist instances of the problem  with  nonnegative submodular
such that a -approximation would require exponentially many value queries.
\end{corollary} 

We remark that a related problem, , is at least as difficult
to approximate: we can reduce  to it by trying
all possible values .

\medskip

\begin{proof}
Let , and let  be the hypergraph we considered in previous theorem
and  be the cut function of . Similar to the proof of 
Theorem~\ref{thm:hard_matroidindependence}, we have  and we may use
equation \eqref{eq:symmetrization} to obtain the value of .
In this case the feasibility polytope will be

however, we may assume that we have equality for the maximum value of ,
otherwise we can simply increase the  value of the tail vertices in 
and this can only increase . Let  and  and .
Using equations \eqref{eq:symmetrization} and \eqref{eq:cardinalproperty} we have

Finally, we can compute the value of :

Again by optimizing over , the smallest value of 
is obtained when . In this case we have .
The refined instances are instances of submodular maximization
subject to a cardinality constraint, where the constraint is to choose at most
 fraction of the all the elements in the ground set.
\end{proof}

\end{document}
