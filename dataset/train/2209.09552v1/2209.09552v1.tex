\documentclass{article}




\PassOptionsToPackage{square,numbers, sort}{natbib}


\usepackage[preprint]{neurips_2022}












\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\usepackage{float}

\usepackage{wrapfig}
\usepackage{mathabx}

\newcommand{\yb}{\mathbf{y}}
\newcommand{\wb}{\mathbf{w}}
\newcommand{\zb}{\mathbf{z}}
\newcommand{\Mb}{\mathbf{M}}
\newcommand{\Hb}{\mathbf{H}}
\newcommand{\Qb}{\mathbf{Q}}
\newcommand{\Kb}{\mathbf{K}}
\newcommand{\Vb}{\mathbf{V}}
\newcommand{\Wb}{\mathbf{W}}
\newcommand{\Xb}{\mathbf{X}}
\newcommand{\Yb}{\mathbf{Y}}
\newcommand{\Zb}{\mathbf{Z}}
\newcommand{\Ib}{\mathbf{I}}
\newcommand{\Yh}{\mathbf{\hat{Y}}}

\setcitestyle{}

\title{Cross-modal Learning for Image-Guided Point Cloud Shape Completion}




\author{Emanuele Aiello\thanks{Code of the project: \url{https://github.com/diegovalsesia/XMFnet} } \\
  Politecnico di Torino, Italy\\
  \texttt{emanuele.aiello@polito.it} \\
\And
  Diego Valsesia\\
  Politecnico di Torino, Italy\\
  \texttt{diego.valsesia@polito.it} \\
  \AND
  Enrico Magli\\
  Politecnico di Torino, Italy\\
  \texttt{enrico.magli@polito.it} \\
}


\begin{document}


\maketitle


\begin{abstract}
In this paper we explore the recent topic of point cloud completion, guided by an auxiliary image. We show how it is possible to effectively combine the information from the two modalities in a localized latent space, thus avoiding the need for complex point cloud reconstruction methods from single views used by the state-of-the-art. We also investigate a novel weakly-supervised setting where the auxiliary image provides a supervisory signal to the training process by using a differentiable renderer on the completed point cloud to measure fidelity in the image space. Experiments show significant improvements over state-of-the-art supervised methods for both unimodal and multimodal completion. We also show the effectiveness of the weakly-supervised approach which outperforms a number of supervised methods and is competitive with the latest supervised models only exploiting point cloud information.
\end{abstract}


\section{Introduction}

The rise in popularity of 3D sensing technologies such as depth cameras, laser scanners, LiDARs, etc. is making the processing of point cloud data ever more important. The acquisitions produced by those instruments are often incomplete due to occlusions by objects in the environment, reflections, and viewing angles. This limits the exploitability of those data in tasks like scene understanding \cite{hou20193d}, robotic vision \cite{varley2017shape}, autonomous driving \cite{li2020deep} and many more. Completing a point cloud from partial observations is a challenging ill-posed inverse problem that requires strong prior knowledge about shapes to be effectively regularized. 

At the same time, we know that humans are very proficient at mapping the visual concepts learnt from 2D images to understand the 3D world, and are able to successfully infer the shape of partial 3D objects from their 2D experiences. It is thus sensible to expect that point cloud completion techniques can benefit from 2D images to better characterize the 3D shape to be completed. Indeed, several applications of interest in robotic vision can take advantage of multimodal data where the 3D acquisitions of a depth-sensing instrument are paired with images from an RGB camera. It is also worth noting that the two modalities may be acquired from different vantage points, either thanks to disparities in the acquisition geometry or because the vantage point has changed with the passing of time. This makes it clear that the two modalities may carry complementary information and effectively fusing it is key to unlock better completion performance.
Nevertheless, the literature on the topic of point cloud completion \cite{pcn,topnet,atlas,ecg,vrc,pointtr,crn,selfsup} has largely focused on the single-modality problem, where only priors about 3D shapes are exploited. Only recently, image-guided completion has started to receive attention \cite{vipc}.

In this paper, we study how the side information offered by a single image can be used in addition to shape priors to complete a partial point cloud. While following the setting of ViPC \cite{vipc}, we extend the multimodal completion methodology in several different ways. First, ViPC \cite{vipc} is bottlenecked by the need to estimate a coarse point cloud from the image via single-view reconstruction techniques to fuse the information. We avoid this task by proposing a novel architecture that performs fusion in a latent domain via cross-attention operations on fine-grained, localized representations of the two modalities, coupled with a flexible decoder that allows to complete areas of varied size. Moreover, the multimodal setting is uniquely poised for weakly-supervised learning. In fact, the input image, especially when captured from a different vantage point, may offer a supervisory signal to guide the completion of those areas occluded in the partial point cloud but visible in the image. This is especially interesting for practical applications where it could be difficult to have access to complete shapes, but significantly easier to have images from a different viewing angle. Therefore, we propose to augment the known 3D self-reconstruction losses with the exploitation of a differentiable renderer to measure the fidelity of the completed point cloud in the image space.

Our experiments show that the proposed model significantly outperforms the state-of-the-art on both the supervised and weakly-supervised settings. In particular, the addition of the rendering loss allows the weakly-supervised image-guided model to outperform several supervised baselines and to be competitive with the latest supervised models only exploiting point cloud information.


\vspace{-5pt}
\section{Related work} 
\vspace{-5pt}

\paragraph{Point cloud completion} 3D shape completion is a long-standing problem in computer vision. Early works devised explicit geometric descriptors or relied on shape retrieval from large datasets \cite{data-driven} \cite{symmetry} \cite{geometry} \cite{primitive}. Since the advent of neural networks operating on raw point cloud data, several models for the completion problem have been studied \cite{review}. They are mostly based on the encoder-decoder architecture, pioneered by PCN \cite{pcn}, which was the first model that did not require any assumption of structure or annotation information about the underlying shape.  TopNet \cite{topnet} presents a hierarchical rooted tree structure that generates structured point clouds as a collection of its subsets. AtlasNet \cite{atlas} and MSN \cite{msn}, on the other hand, recreate the point cloud by assessing a set of parametric surface elements. Convolutional-based approaches (\cite{voxel1, voxel2}) use a voxelixed representation of shapes as input to 3D CNNs; nevertheless, this representation introduces undesirable approximations in the shape due to coordinate quantization effects. GRNet \cite{grnet} uses techniques to represent point cloud onto a 3D grid, so that CNNs can be exploited, without losing structural information. Recently, VRCNet \cite{vrc} has proposed a dual path architecture and a VAE-based relation enhancement module for probabilistic modeling. Architectures based on transformers have also been proposed. PointTr \cite{pointtr} changes the transformer block to take advantage of the inductive bias of 3D geometries, creating a geometry-aware block that models local geometry relations. Moreover, SnowflakeNet \cite{snowflake} generates child points by gradually splitting parent points by means of a Skip-Transformer that learns the appropriate splitting modes for particular regions. As a result, the network is able to predict highly detailed shape geometries. Finally, it is worth mentioning that the point cloud completion literature is split between two settings: one, as in the aforementioned works, where the partial input has the same number of points as the completed point cloud and another where the completed point cloud has more points than the input such as in \cite{huang2020pf,alliegro2021denoise}. In this paper, we will consider the former setting.


\paragraph{View-guided completion} Recently, the usage of auxiliary data to complement point cloud completion has been introduced by ViPC \cite{vipc}. The idea is to help the reconstruction objective using side information available as a different imaging modality. In particular, ViPC assumes that an image corresponding to a view of the same object is also available for the completion task, and it exploits the image to retrieve the global shape information that is lacking in the incomplete point cloud. The image is processed by a pre-trained single-view reconstruction model, which estimates a coarse point cloud from the image, representing the entire shape. The key challenge in this setting is how to effectively combine features extracted from the two modalities. Unlike ViPC, our approach leverages direct fusion at a feature level, avoiding the need to explicitly reconstruct a coarse point cloud from a single image, a generally hard inverse problem in itself and full of pitfalls.


\paragraph{Self-supervised strategies for completion}
All the previously mentioned approaches rely on complete ground truth as a supervisory training signal. This may be difficult to obtain in real-word scenarios. Self-supervised training strategies avoid the need for retrieving such expensive ground truths. However, the amount of work on this topic is rather limited. 
Wang et al. \cite{crn} use resampling that removes further points from an already partial point cloud and mixup among shapes to train their completion network in a self-supervised manner. Similarly, Mittal et al. \cite{selfsup} also propose an inpainting procedure that leverages further partializations of partial point clouds.
To the best of our knowledge, there is no work exploring the availability of a different modality, namely an image to provide a weak supervisory signal to the point cloud completion task. 




\section{Proposed Methods}

\begin{figure}
  \centering
    \includegraphics[width=\textwidth]{figures/mmpc_arch.pdf}\\ \vspace*{15pt}
    \includegraphics[width=\textwidth]{figures/mmpc_arch_details.pdf}
    \vspace{-10pt}
  \caption{Architecture overview. Localized features from the partial point cloud and the input image are jointly processed via cross- and self-attentions. A decoder reconstructs the target number of points from the feature space with attention-based upsampling. The input partial point cloud is downsampled with farthest point sampling (FPS) and concatenated. Supervised training only uses the point cloud reconstruction loss with respect to the complete point cloud. Weakly-supervised training has a point cloud reconstruction loss with respect to a less partial point cloud and a rendering loss.  is the number of points of the input point cloud;  is the number of points generated by each branch of the decoder, while  represents the feature dimension.}
  \label{fig:arch}
  \vspace{-10pt}
\end{figure}

We address the setting in which a partial point cloud needs to be completed with the assistance of an image of the object taken from a certain viewpoint. Our goal is to study how to leverage this side information in the most effective manner. To this purpose we study i) a supervised learning setting, for which we show that the image features can be effectively fused with those of the partial point cloud in a latent space; ii) a weakly-supervised setting based on the idea that the image may contain clues about the missing part and can thus serve as a supervisory signal. Fig. \ref{fig:arch} shows an overview of the proposed method, named XMFnet (Cross-Modal Fusion network) which will be detailed in the next sections.



\subsection{Architecture and Supervised Setting} 
\label{sec:arch}

At a high level, the architecture of XMFnet is composed by two modality-specific feature extractors that capture localized features of the input point cloud and image, summarized at a small number of points/pixels, followed by a sequence of cross-attention and self-attention operations that progressively merge the two feature spaces. Finally, a decoder upsamples this localized information to estimate a predefined number of points of the missing component. More formally, we denote the partial point cloud as , the input view as an image  and the complete point cloud as . The task of our model is to predict a complete shape  given  and  as inputs. Notice we follow the conventional setting where the partial and complete point clouds have the same number of points, meaning that there is a resampling of the known part.


\subsubsection{Point Cloud and Image Encoder}
The point cloud encoder should extract localized features from the partial shape . It is important to keep a degree of locality, i.e., associating features to a small number of points  rather than a single global embedding because the information about the missing part that needs to be estimated by the entire model is also mostly localized. However, it is also important to have a sufficiently large receptive field to infer some global information about the object. For this reason, we adopt a graph-convolutional architecture with graph pooling operations. The architecture is a sequence of graph-convolutional layers (EdgeConv \cite{dgcnn}) interleaved by pooling operations (Self-Attention Graph Pooling \cite{sag-pool}) to reduce the cardinality of the point cloud. Pooling has the double purpose of expanding the receptive field to also include more global information and reduce the complexity of the subsequent cross-attention operations fusing the two modalities.  


Any network that extracts features from an image can be utilized as encoder for view . The design principles follow those of the point-cloud encoder, i.e., features localized at a subset  of the image pixels obtained from a sufficiently large receptive field are produced as output.

We will refer to the features produced by the point cloud encoder as  and by the image encoder as .


\subsubsection{Modality Fusion}
Once we have collected localized information from the two modalities, we need to combine them effectively to capture their complementary information, despite the obvious domain gap. The attention mechanism is particularly suited to find correspondences between the features of a region of the point cloud and a region of the image. The cross-attention layer in our architecture uses the Transformer's multihead attention mechanism \cite{attention}. The point cloud features are projected to form the query tensor, while the image features are projected to form the key and value tensors, and then attention mechanism aggregates the features from different image regions according to the weights determined by the cross-correlation between the two modalities. More formally:


being ,  the projection weights. The fused features  produced by the cross-attention mechanism can be regarded as the original point cloud features enriched by the image features.  

The XMFnet architecture depicted in Fig. \ref{fig:arch} shows a self-attention layer after the cross-attention fusion. The goal of this operation is to have a permutation-invariant transformation of the features with a global receptive field so that any information from the image not properly integrated can be rectified. Self-attention works exactly like Eq. \eqref{eq:attn} except for the fact that  are all different projections of the same features. Furthermore, a sequence of multiple cross- and self-attention layers can be used to more effectively integrate the information from the two modalities via a ``slow'' fusion. We remark that at the end of this sequence we use a special cross-attention layer that merges information from the end and the beginning of the sequence allowing better flexibility in the decision of the desired abstraction level (higher-level features cross-attend lower-level features). 



\subsubsection{Decoder and Supervised Loss}
The decoder is a crucial component of our architecture as it should take the joint feature embedding and learn to reconstruct a complete point cloud preserving both global and local structure. To be precise, the decoder seeks to estimate the positions of a number of points that upper bounds the size of the missing part, so that they can be concatenated to a version of the input partial point cloud subsampled by means of farthest point sampling (FPS). This mechanism is reminiscent of what is done in the setting with a variable number of points where only the missing part is estimated \cite{huang2020pf,alliegro2021denoise}. For example, in our experiments, we upper bounded the size of the missing part to  of the total number of points, thus having  points estimated by the decoder concatenated to  points from the subsampled input. However, our method allows to be flexible and handle more incomplete inputs by simply tuning the desired ratio of points to be estimated and points provided from the partial input.
Typically, the latent space where we perform feature fusion is much more localized to constrain complexity and allow higher-level features so that , thus requiring the decoder to upsample the feature field. We perform this operation by using a number of attention-based operations, inspired by the work in \cite{axform}, that convert features to points in parallel with the idea that each branch specializes on the reconstruction of a sub-region of the missing part. The structure is depicted in Fig.\ref{fig:arch}. More formally, calling  the features provided to the decoder, the output  of each of the  branches is computed as:

where MLP, MLP are multilayer perceptrons with different weights for each branch, projecting features to  subspaces and generating attention weights for the resampling process, respectively.  is projection matrix to 3D space.
Finally, the completed point cloud is generated by concatenation of the outputs of all decoder branches and the partial input subsampled by FPS as:



Supervised training is performed using the L1 Chamfer Distance (CD) between the generated shapes and the ground truth shapes, defined as follows:




\subsection{Weakly-supervised Setting}

The multimodal completion problem addressed in this paper is uniquely poised for weakly-supervised learning. In fact, the image available as input may contain complementary information with respect to the point cloud and, crucially, cues about the missing part. This is especially true if the image is collected from a different viewpoint or at a different time with respect to the point cloud, resulting in different kinds of occlusions. 
\begin{wrapfigure}[12]{r}{0.4\textwidth}
\centering
 \includegraphics[width=0.39\textwidth]{figures/selfsup.pdf}
    \caption{Weakly-supervised training.}
  \label{fig:selfsup}
\end{wrapfigure}

Existing architectures for self-supervised completion \cite{crn} \cite{selfsup}  rely solely on point cloud supervision, due to their unimodal nature. The key insight of the proposed method (Fig. \ref{fig:selfsup}) is to supplement completion losses on points with a loss measuring a reprojection error in the image space. In particular, we measure whether the reconstructed point cloud produced by the architecture described in Sec. \ref{sec:arch} leads to an image similar to the input one, when captured from the correct viewpoint.
 
In order to measure this information and use it in the training process, we include a differentiable rendering module based on alpha compositing \cite{wiles2020synsin} which generates a rasterized version of the object, employing provided camera parameters. In order to ensure consistency with the input image, intrinsic and extrinsic camera parameters may be estimated with a number of well-known methods \cite{insafutdinov18pointclouds} \cite{grabner2020geometric} \cite{palazzi2018end}. In order to minimize the domain shift between the input image and the result of the rendering process, we work on silhouettes, i.e., binary masks of objects. The differentiable renderer produces a soft silhouette with continuous values, while the input image is directly binarized. Inevitable inaccuracies in the camera parameters and rendering process will typically yield unreliable borders of the silhouette. Therefore, we also compute a border mask with a simple edge detector (Laplacian of Gaussian) and discount the loss function by a factor  for the pixels in the mask. In summary, our rendering loss is defined as:

being  the differentiable silhouette renderer and  the silhouette binarizer. 
We remark that \cite{xie2021style} proposes to use rendering to improve performance in point cloud completion, aiding the learning process through an image domain supervision. However, their approach is supervised and is based on rendering the ground truth and generated point cloud in depth-maps with different view-points.

In addition to the rendering loss, our weakly-supervised framework also adopts self-supervision in the point cloud domain. In particular, we use a combination of the resampling and mixup approaches proposed in \cite{selfsup,crn}. Resampling consists in removing random portions of the original partial input point cloud, yielding even more partial shapes. As a result, the original partial input is employed as a pseudo-ground truth. Mixup combines a pair of partial shapes weighed according to Beta distribution in an attempt at increasing the complexity of the shapes processed by the network. Differently from \cite{crn}, we also have images associated with a partial shape in our setting. Hence, we also mix the images in such a way that the mixup technique is carried out symmetrically for the two modalities.

We remark that the rendering loss is comparatively weaker than the point cloud loss and, by itself, has a number of ambiguities due to the lack of depth information and the use of silhouettes. For this reason, it is important to combine it with the point cloud loss. We found that the density-aware Chamfer Distance (DCD) \cite{balancedcd}, a version of CD that is more sensitive to non-uniform point distributions is superior in this weakly-supervised scenario to regularize the ambiguities of the rendering loss. Our overall weakly-supervised training procedure is therefore as follows. We alternate between a step that optimizes the point cloud loss consisting in a weighted CD:

and a step optimizing a combination of DCD and rendering loss:

where  and . Notice that  and the DCD part of  use resampling and mixup for point clouds, while  does not ( is computed with full minibatch for the point-cloud part and half minibatch with the original partials for rendering).







\section{Experimental results}\label{sec:exp}

\subsection{Experimental Settings and Implementation Details}

All the experiments are conducted on the ShapeNet-ViPC\cite{vipc} dataset.
The dataset contains 38,328 objects from 13 categories; for each object it comprises 24 partial point clouds with occlusions generated under 24 viewpoints, using the same settings as ShapeNetRendering \cite{chang2015shapenet}. The input and ground truth point clouds contains  points each.
Each 3D shape is rotated to the pose corresponding to a certain view point after being normalized within the bounding sphere with radius of 1. Images are generated from the 24 view points of ShapeNetRendering and have a resolution of  pixels. For all the experiments in this paper, we employ the same selection used in \cite{vipc}: we used 31,650 objects from eight categories, with  of them for training and  for testing.

The partial point cloud is downsampled by farthest point sampling to  points and concatenated to the output of the decoder that produces  points leading to a completed point cloud with 2048 points. The decoder has  branches, each of them producing  points. 
The point cloud encoder employs EdgeConv and SAGPooling layers; the EdgeConv layers selects  nearest neighbors, while the two pooling layers use  and  nearest neighbors, respectively. The original point cloud is overall downsampled by a factor of 16, resulting in  points with  features.
The image encoder is built with a ResNet18 \cite{resnet} as backbone, it extracts  pixels with  features. The multihead attention has 4 attention heads, with embedding size . In the  loss we use . The mask factor for the edge detector has been set to .

The differentiable renderer has been implemented with PyTorch3D\cite{Ravi2020Accelerating3D}. The rendered silhouettes  has size  that is the same size of input views in our experiments. We adopt radius  in point rasterization. The proposed framework is implemented in PyTorch and trained on an Nvidia V100 GPU. Class-specific training is performed for all models, using the Adam optimizer \cite{Kingma2015AdamAM} for roughly 200 epochs with a batch size of 128. The learning rate is initialized to 0.001 and reduced by a factor of 10 at epoch 25 and 125. 


\begin{table}
  \caption{Mean Chamfer Distance per point (). ShapeNet-ViPC dataset, supervised.}
  \setlength\tabcolsep{4.4pt}
  \label{sup-res}
  \centering
  \begin{tabular}{lccccccccc}
\cmidrule(r){1-10}
    Methods & Avg & Airplane & Cabinet & Car & Chair & Lamp & Sofa & Table & Watercraft \\
    \midrule
    \midrule
    AtlasNet \cite{atlas} & 6.062 & 5.032 & 6.414 & 4.868 & 8.161 & 7.182 & 6.023 & 6.561 & 4.261      \\
    FoldingNet \cite{folding}  &  6.271  & 5.242 & 6.958 & 5.307 & 8.823 & 6.504 & 6.368 & 7.080 & 3.882   \\
    PCN \cite{pcn}& 5.619 & 4.246 & 6.409 & 4.840 & 7.441 & 6.331 & 5.668 & 6.508 & 3.510 \\  
    TopNet \cite{topnet}  & 4.976 & 3.710 & 5.629 & 4.530 & 6.391 & 5.547 & 5.281 & 5.381 & 3.350\\
    ECG \cite{ecg} & 4.957 & 2.952 & 6.721 & 5.243 & 5.867 & 4.602 & 6.813 & 4.332 & 3.127\\ 
    VRC-Net \cite{vrc} & 4.598 & 2.813 & 6.108 & 4.932 & 5.342 &  4.103 & 6.614 & 3.953 & 2.925\\
    ViPC \cite{vipc} & 3.308 & 1.760 & 4.558 & 3.183 & 2.476 & 2.867 & 4.481 & 4.990 & 2.197    \\
    \textbf{XMFnet} &\textbf{1.443} &  \textbf{0.572} & \textbf{1.980} & \textbf{1.754} & \textbf{1.403} & \textbf{1.810} & \textbf{1.702} & \textbf{1.386} & \textbf{0.945} \\
    \bottomrule
  \end{tabular}
  
\end{table}

\begin{table}
  \caption{Mean F-Score @ 0.001. ShapeNet-ViPC dataset, supervised}
  \setlength\tabcolsep{4.4pt}
  \label{f-score}
  \centering
  \begin{tabular}{lccccccccc}
\cmidrule(r){1-10}
    Methods & Avg & Airplane & Cabinet & Car & Chair & Lamp & Sofa & Table & Watercraft \\
    \midrule
    \midrule
    AtlasNet \cite{atlas} & 0.410 & 0.509 & 0.304 & 0.379 & 0.326 & 0.426 & 0.318 & 0469 & 0.551      \\
    FoldingNet \cite{folding}  &  0.331  & 0.432 & 0.237 & 0.300 & 0.204 & 0.360 & 0.249 & 0.351 & 0.518 \\
    PCN \cite{pcn}& 0.407 & 0.578 & 0.270 & 0.331 & 0.323 & 0.456 & 0.293 & 0.431 & 0.577\\  
    TopNet \cite{topnet}  & 0.467 & 0.593 & 0.358 & 0.405 & 0.388 & 0.491 & 0.361 & 0.528 & 0.615\\
    ECG \cite{ecg} & 0.704 & 0.880 & 0.542 & 0.713 & 0.671 & 0.689 & 0.534 & 0.792 & 0.810\\ 
    VRC-Net \cite{vrc} & 0.764 & 0.902 & 0.621 & 0.753 & 0.722 & \textbf{0.823} & 0.654 & 0.810 & 0.832\\
    ViPC \cite{vipc} & 0.591 & 0.803 & 0.451 & 0.512 & 0.529 & 0.706 & 0.434 & 0.594 & 0.730\\
    \textbf{XMFnet} & \textbf{0.796}  & \textbf{0.961} & \textbf{0.662} & \textbf{0.691} & \textbf{0.809} & 0.792 &\textbf{0.723} & \textbf{0.830} & \textbf{0.901}  \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Main Results}


\subsubsection{Supervised Learning}

We first compare XMFnet against several baselines for the point cloud completion under supervised learning. Since the new multimodal setting with an auxiliary image has been introduced only recently, ViPC \cite{vipc} represents the only method fully comparable to ours. However, we also report the results of a number of state-of-the-art architectures for completion with only point cloud input, when retrained on the ViPC dataset. AtlasNet\cite{atlas} reconstructs a point cloud by estimating parametric surface elements. FoldingNet\cite{folding} is a 2-D grid based auto-encoder. PCN\cite{pcn} is an encoder-decoder framework that reconstructs the point cloud in a coarse-to-fine manner. TopNet\cite{topnet} has a rooted tree structure in the decoder. ECG\cite{ecg} is an edge-aware completion method based on Graph Convolutions. VRC-Net\cite{vrc} is the most recent method adopting a VAE-based model with a dual path architecture and probabilistic modeling. In line with the previous evaluation protocols, we use CD and F-score \cite{fscore} as metrics for the reconstruction quality. Before evaluating the CD, we normalize the output of the models to fit into the unit sphere. Table \ref{sup-res} and Table \ref{f-score} report the experimental results and show XMFnet outperforming the other techniques by a significant margin. While part of this gain with respect to state-of-the-art models for point cloud completion can be attributed to the use of the input image, it is worth noting that we also report significant improvements over the multimodal ViPC. This highlights the sub-optimality of performing modality fusion by resorting to estimating a coarse point cloud from the single input image, as in ViPC, rather than working in a latent feature space.
Qualitative comparisons\footnote{Visualizations for ViPC \cite{vipc} have been kindly provided by the original authors.} are shown in Fig. \ref{fig:qualitative}. Our method is capable of producing cleaner completions than the other baselines, with fewer outliers and a more uniform point distribution.


\begin{figure}
\centering
\setlength\tabcolsep{1.5pt}
\begin{tabular}{ccccccc}
Partial & PCN & ECG & VRCnet & ViPC & XMFnet & GT \\ 

\includegraphics[width = 0.135\textwidth]{figures/plane_partial.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/plane_pcn.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/plane_ecg.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/plane_vrc.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/plane_vipc.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/plane_ours.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/plane_gt.png}

\\
\includegraphics[width = 0.135\textwidth]{figures/chair_partial.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/chair_pcn.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/chair_ecg.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/chair_vrc.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/chair_vipc.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/chair_ours.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/chair_gt.png}

\\
\includegraphics[width = 0.135\textwidth]{figures/lamp_partial.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/lamp_pcn.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/lamp_ecg.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/lamp_vrc.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/lamp_vipc.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/lamp_ours.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/lamp_gt.png}

\\
\includegraphics[width = 0.135\textwidth]{figures/couch_partial.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/couch_pcn.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/couch_ecg.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/couch_vrc.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/couch_vipc.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/couch_ours.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/couch_gt.png}

\\
\includegraphics[width = 0.135\textwidth]{figures/table_partial.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/table_pcn.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/table_ecg.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/table_vrc.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/table_vipc.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/table_ours.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/table_gt.png}

\\
\includegraphics[width = 0.135\textwidth]{figures/water_partial.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/water_pcn.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/water_ecg.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/water_vrc.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/water_vipc.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/water_ours.png}
   
&
\includegraphics[width = 0.135\textwidth]{figures/watercraft_gt.png}

\\

\end{tabular}
\caption{Qualitative comparison of completed point clouds for different classes.}
\label{fig:qualitative}
\end{figure}

\subsubsection{Weakly-supervised}

\begin{table}
\begin{minipage}{0.5\textwidth}
  \caption{Mean Chamfer Distance per point (). ShapeNet-ViPC dataset.}
  \setlength\tabcolsep{1.3pt}
  \label{self-res}
\begin{tabular}{lccc}
\cmidrule(r){1-4}
    Methods & Airplane & Lamp & Watercraft \\
    \midrule
    \midrule
    AtlasNet \cite{atlas} & 5.032 & 7.182 & 4.261     \\
    FoldingNet \cite{folding} & 6.504  & 6.368 & 3.882  \\
    PCN \cite{pcn} &  4.246 & 6.331 & 3.510 \\  
    TopNet \cite{topnet}  & 3.710 & 5.547 & 3.350  \\
    ECG \cite{ecg} & 2.952 & 4.602 & 3.127 \\ 
    VRC-Net \cite{vrc} & 2.813 & 4.103 & 2.925 \\
    VRC-Net(self-sup.) & 4.315 & 8.023 & 7.259 \\
    ViPC  \cite{vipc} &  1.760 & 2.867 & 2.197 \\
    XMFnet (sup.) & 0.572 & 1.810 & 0.945 \\
    \midrule
   \textbf{XMFnet (weakly-sup.)} & 2.426 & 6.269 & 3.423\\
    \bottomrule
  \end{tabular}
\vspace{0.3cm}
  \caption{Mean F-Score @ 0.001. ShapeNet-ViPC dataset.}
  \setlength\tabcolsep{1.3pt}
  \label{self-res-f}
\begin{tabular}{lccc}
\cmidrule(r){1-4}
    Methods & Airplane & Lamp & Watercraft \\
    \midrule
    \midrule
    AtlasNet \cite{atlas} & 0.509 & 0.426 & 0.551     \\
    FoldingNet \cite{folding} & 0.432  & 0.360 & 0.518  \\
    PCN \cite{pcn} &  0.578 & 0.456 & 0.577 \\  
    TopNet \cite{topnet}  & 0.593 & 0.491 & 0.615  \\
    ECG \cite{ecg} & 0.880 & 0.689 & 0.810 \\ 
    VRC-Net \cite{vrc} & 0.902 & 0.823 & 0.832 \\
    VRC-Net(self-sup.) & 0.689 & 0.710 & 0.673 \\
    ViPC  \cite{vipc} &  0.803 & 0.706 & 0.730 \\
    XMFnet (sup.) & 0.961 & 0.792 & 0.901 \\
    \midrule
   \textbf{XMFnet (weakly-sup.)} & 0.742 & 0.542 & 0.704\\
    \bottomrule
  \end{tabular}
\end{minipage}
\hspace{14pt}
\begin{minipage}{0.45\textwidth}
\vspace{10pt}
  \caption{Unimodal vs. Multimodal completion (supervised)}
  \setlength\tabcolsep{1.4pt}
  \label{sup-abl}
\begin{tabular}{lcccc}
    \toprule
    Method & Avg & Airplane & Cabinet & Lamp \\
    \midrule
    \midrule
    Unimodal & 1.570 & 0.626 & 2.114 & 1.980\\
    \textbf{Multimodal} & \textbf{1.470}  &  \textbf{0.572} & \textbf{1.973} & \textbf{1.810}\\
     \textbf{best view} & 1.223 &  0.545 & 1.426 & 1.697 \\
     \textbf{worst view} & 1.819 &  0.722 & 2.621  & 2.115\\
    \bottomrule
   
  \end{tabular}
\vspace{0.7cm}
  \caption{Ablation study for the Weakly-Supervised method (\textit{airplane})}
  \vspace{-0.1cm}
  \setlength\tabcolsep{2pt} 
  \label{abl-self}
  \centering
  \begin{tabular}{cccc}
    \toprule
    Resampling & Mixup & Rendering & CD()\\
    \midrule
    \midrule
    \cmark & \xmark & \xmark & 4.568 \\ 
    \cmark & \cmark & \xmark & 4.239 \\ 
    \cmark & \cmark & \cmark & \textbf{2.426} \\
    \bottomrule
  \end{tabular}
  \vspace{0.7cm}
  \caption{Ablation study for the Weakly-Supervised method - DCD (\textit{cabinet})}
  \label{abl-dcd}
  \centering
  \begin{tabular}{cc}
    \toprule
    DCD & CD()\\
    \midrule
    \midrule
    \xmark & 3.012  \\
    \cmark & 2.426
     \\
    \bottomrule
  \end{tabular}
\end{minipage}
\end{table}

We remark that we are the first to propose a weakly-supervised training strategy for multimodal completion, so the setting in which supervisory information can be gathered from an input image is unexplored. For this reason, we compare to a number of unimodal and multimodal supervised methods as well as a self-supervised version using resampling and mixup of a unimodal state-of-the-art model\footnote{We remark the difficulty in reproducing several published methods in the self-supervised setting. The code for \cite{selfsup,crn} was not available. The code to reproduce ViPC \cite{vipc} is also incomplete so we cannot retrain the most sensible self-supervised baseline of ViPC + Resampling + Mixup.}. The results are reported in Table \ref{self-res} for the CD and Table \ref{self-res-f} for the F-Score. We notice that our weakly-supervised method outperforms a number of supervised baselines and it is close to the performance of the most recent unimodal supervised models.



\subsection{Ablation Studies}

In order to verify the effectiveness of the proposed design, we study the impact of the auxiliary image input on the completion performance. To ensure a comparison as fair as possible, the version of XMFnet that uses only point cloud information has the image encoder removed and the cross-attention blocks replaced with self-attention ones.  

The unimodal architecture is then trained with the same settings as the multimodal one, and the results are reported in Table \ref{sup-abl}. The results show that the addition of the image input provides a significant improvement in performance. Notice that besides the result averaged over all the possible 24 views, we also report the performance with the worst view and the best view. Indeed, we are interested in investigating how the viewpoint of the image affects completion. \begin{wrapfigure}[20]{r}{0.55\textwidth}
\centering
\vspace{-10pt}
\includegraphics[width = 0.50\textwidth]{figures/ablview.pdf}
\vspace{-10pt}
\caption{Impact of image contribution as function of point of view, sorted by reconstruction CD (from worst to best) averaged over cabinet category, supervised setting.}
\label{fig:abl_view}
\end{wrapfigure}
Fig. \ref{fig:abl_view} reports the average CD for different views, ordered from worst to best, for the cabinet category. It is clear that some views provide complementary information due to their different vantage point and allow to substantially improve over the average result. A small number of ``bad'' views leads to results comparable to the unimodal case.


Furthermore, it is interesting to study the impact of our novel rendering loss in the weakly-supervised setting. We noticed that it allows the training process to have a faster and smoother convergence and that the overall completion performance is increased from both a qualitative and quantitative point of view. A qualitative comparison between the weakly-supervised strategy with and without the rendering module can be visualized in Fig. \ref{fig:rendabl} and quantitative results are reported in Table \ref{abl-self}.
The mixup loss provides only a small improvement from the perspective of the CD. However, it substantially improves the completed shape from a qualitative point of view, helping the network generate more complete shapes. Moreover, the computational overhead due to creating the mixed input shape is very small, so we decided to keep the method as it offered a very favorable cost-performance trade-off.
We also include the ablation for the DCD component of the weakly-supervised loss, we found it helpful from both a qualitative and quantitative point of view. Table \ref{abl-dcd} reports the impact of the DCD on the \textit{cabinet} category, while Fig. \ref{fig:dcd_abl} provides a qualitative example, where the shape completed with the DCD presents a more uniform distribution of points and a better overall quality.




\begin{figure}
\centering
\begin{tabular}{cccc}
Partial & Without Rendering & With Rendering & GT \\

\includegraphics[width = 0.22\textwidth]{figures/partial_vis.png}
   
&
\includegraphics[width = 0.22\textwidth]{figures/vis_occo_1.png}
   
&
\includegraphics[width = 0.22\textwidth]{figures/vis_render.png}
   
&
\includegraphics[width = 0.22\textwidth]{figures/gt_vis.png}

\end{tabular}
\caption{Qualitative visualization of the effect of the proposed weakly-supervised rendering loss. The sample without rendering has CD , the one with rendering has CD. }
\label{fig:rendabl}

\end{figure}
\vspace{10pt}

\begin{figure}[t]
\centering
\begin{tabular}{cccc}
Partial & Normal CD & DCD & GT \\

\includegraphics[width = 0.23\textwidth]{figures/part_plane_2.png}
   
&
\includegraphics[width = 0.23\textwidth]{figures/testcomp_plane_no2.png}
   
&
\includegraphics[width = 0.23\textwidth]{figures/testcomp_plane_dcd_2.png}
   
&
\includegraphics[width = 0.23\textwidth]{figures/gt_plane_2.png}
   

\\
\end{tabular}
\caption{Qualitative visualization of the effect of the DCD for the weakly-supervised setting.}
\label{fig:dcd_abl}

\end{figure}




\section{Conclusions}
In this paper, we explored the topic of point cloud completion guided by an auxiliary image, discovering that effective fusion can be achieved in a latent space via cross-attention. Our method achieves state-of-the-art results on the ShapeNetViPC-Dataset. Moreover, we showed how this setting lends itself to weakly-supervised learning where the image can be used for supervision via a differentiable rendering approach. The major limitation of our work is the lack of study of a real-world scenario for the proposed framework. In future work, we will focus on extending the work to real acquisitions, thus dealing with complex effects like acquisition noise, background or additional occlusions in the auxiliary images, and many more. This paper has provided a proof of concept that effective multimodal completion is possible but a more in-depth study of such issues on real scenes is needed, along with suitable improvements to our design towards increased robustness.


\begin{ack}
Computational resources were provided by HPC@POLITO, a project of Academic Computing within the Department of Control and Computer Engineering at the Politecnico di Torino (http://www.hpc.polito.it).
\end{ack}



\clearpage
\bibliographystyle{IEEEtran}
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}\typeout{** loaded for the language `#1'. Using the pattern for}\typeout{** the default language instead.}\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{hou20193d}
J.~Hou, A.~Dai, and M.~Nie{\ss}ner, ``{3D-SIS: 3D semantic instance
  segmentation of RGB-D scans},'' in \emph{Proceedings of the IEEE/CVF
  conference on computer vision and pattern recognition}, 2019, pp. 4421--4430.

\bibitem{varley2017shape}
J.~Varley, C.~DeChant, A.~Richardson, J.~Ruales, and P.~Allen, ``Shape
  completion enabled robotic grasping,'' in \emph{2017 IEEE/RSJ international
  conference on intelligent robots and systems (IROS)}.\hskip 1em plus 0.5em
  minus 0.4em\relax IEEE, 2017, pp. 2442--2447.

\bibitem{li2020deep}
Y.~Li, L.~Ma, Z.~Zhong, F.~Liu, M.~A. Chapman, D.~Cao, and J.~Li, ``Deep
  learning for lidar point clouds in autonomous driving: A review,'' \emph{IEEE
  Transactions on Neural Networks and Learning Systems}, vol.~32, no.~8, pp.
  3412--3432, 2020.

\bibitem{pcn}
W.~Yuan, T.~Khot, D.~Held, C.~Mertz, and M.~Hebert, ``{PCN: Point completion
  network},'' in \emph{2018 International Conference on 3D Vision (3DV)}.\hskip
  1em plus 0.5em minus 0.4em\relax IEEE, 2018, pp. 728--737.

\bibitem{topnet}
L.~P. Tchapmi, V.~Kosaraju, H.~Rezatofighi, I.~Reid, and S.~Savarese, ``Topnet:
  Structural point cloud decoder,'' in \emph{Proceedings of the IEEE/CVF
  Conference on Computer Vision and Pattern Recognition}, 2019, pp. 383--392.

\bibitem{atlas}
T.~Groueix, M.~Fisher, V.~G. Kim, B.~C. Russell, and M.~Aubry, ``A
  papier-m{\^a}ch{\'e} approach to learning 3d surface generation,'' in
  \emph{Proceedings of the IEEE conference on computer vision and pattern
  recognition}, 2018, pp. 216--224.

\bibitem{ecg}
L.~Pan, ``{ECG: Edge-aware Point Cloud Completion with Graph Convolution},''
  \emph{IEEE Robotics and Automation Letters}, vol.~5, no.~3, pp. 4392--4398,
  2020.

\bibitem{vrc}
L.~Pan, X.~Chen, Z.~Cai, J.~Zhang, H.~Zhao, S.~Yi, and Z.~Liu, ``Variational
  relational point completion network,'' in \emph{Proceedings of the IEEE/CVF
  Conference on Computer Vision and Pattern Recognition}, 2021, pp. 8524--8533.

\bibitem{pointtr}
X.~Yu, Y.~Rao, Z.~Wang, Z.~Liu, J.~Lu, and J.~Zhou, ``{PoinTr: Diverse point
  cloud completion with geometry-aware transformers},'' in \emph{Proceedings of
  the IEEE/CVF International Conference on Computer Vision}, 2021, pp.
  12\,498--12\,507.

\bibitem{crn}
X.~Wang, M.~H. Ang, and G.~Lee, ``Cascaded refinement network for point cloud
  completion with self-supervision,'' \emph{IEEE Transactions on Pattern
  Analysis and Machine Intelligence}, 2021.

\bibitem{selfsup}
H.~Mittal, B.~Okorn, A.~Jangid, and D.~Held, ``{Self-Supervised Point Cloud
  Completion via Inpainting},'' in \emph{British Machine Vision Conference},
  2021.

\bibitem{vipc}
X.~Zhang, Y.~Feng, S.~Li, C.~Zou, H.~Wan, X.~Zhao, Y.~Guo, and Y.~Gao,
  ``View-guided point cloud completion,'' in \emph{Proceedings of the IEEE/CVF
  Conference on Computer Vision and Pattern Recognition}, 2021, pp.
  15\,890--15\,899.

\bibitem{data-driven}
M.~Sung, V.~G. Kim, R.~Angst, and L.~Guibas, ``Data-driven structural priors
  for shape completion,'' \emph{ACM Transactions on Graphics (TOG)}, vol.~34,
  no.~6, pp. 1--11, 2015.

\bibitem{symmetry}
N.~J. Mitra, L.~J. Guibas, and M.~Pauly, ``Partial and approximate symmetry
  detection for 3d geometry,'' \emph{ACM Transactions on Graphics (TOG)},
  vol.~25, no.~3, pp. 560--568, 2006.

\bibitem{geometry}
M.~Pauly, N.~J. Mitra, J.~Wallner, H.~Pottmann, and L.~J. Guibas, ``Discovering
  structural regularity in 3d geometry,'' \emph{ACM Transactions on Graphics
  (TOG)}, vol.~27, no.~3, pp. 1--11, 2008.

\bibitem{primitive}
R.~Schnabel, P.~Degener, and R.~Klein, ``Completion and reconstruction with
  primitive shapes,'' in \emph{Computer Graphics Forum}, vol.~28, no.~2.\hskip
  1em plus 0.5em minus 0.4em\relax Wiley Online Library, 2009, pp. 503--512.

\bibitem{review}
B.~Fei, W.~Yang, W.~Chen, Z.~Li, Y.~Li, T.~Ma, X.~Hu, and L.~Ma,
  ``Comprehensive review of deep learning-based 3d point clouds completion
  processing and analysis,'' \emph{arXiv preprint arXiv:2203.03311}, 2022.

\bibitem{msn}
M.~Liu, L.~Sheng, S.~Yang, J.~Shao, and S.-M. Hu, ``Morphing and sampling
  network for dense point cloud completion,'' in \emph{Proceedings of the AAAI
  conference on artificial intelligence}, vol.~34, no.~07, 2020, pp.
  11\,596--11\,603.

\bibitem{voxel1}
A.~Dai, C.~Ruizhongtai~Qi, and M.~Nie{\ss}ner, ``{Shape completion using
  3D-encoder-predictor CNNs and shape synthesis},'' in \emph{Proceedings of the
  IEEE conference on computer vision and pattern recognition}, 2017, pp.
  5868--5877.

\bibitem{voxel2}
X.~Han, Z.~Li, H.~Huang, E.~Kalogerakis, and Y.~Yu, ``High-resolution shape
  completion using deep neural networks for global structure and local geometry
  inference,'' in \emph{Proceedings of the IEEE international conference on
  computer vision}, 2017, pp. 85--93.

\bibitem{grnet}
H.~Xie, H.~Yao, S.~Zhou, J.~Mao, S.~Zhang, and W.~Sun, ``{GRNet: Gridding
  residual network for dense point cloud completion},'' in \emph{European
  Conference on Computer Vision}.\hskip 1em plus 0.5em minus 0.4em\relax
  Springer, 2020, pp. 365--381.

\bibitem{snowflake}
P.~Xiang, X.~Wen, Y.-S. Liu, Y.-P. Cao, P.~Wan, W.~Zheng, and Z.~Han,
  ``Snowflakenet: Point cloud completion by snowflake point deconvolution with
  skip-transformer,'' in \emph{Proceedings of the IEEE/CVF International
  Conference on Computer Vision}, 2021, pp. 5499--5509.

\bibitem{huang2020pf}
Z.~Huang, Y.~Yu, J.~Xu, F.~Ni, and X.~Le, ``Pf-net: Point fractal network for
  3d point cloud completion,'' in \emph{Proceedings of the IEEE/CVF Conference
  on Computer Vision and Pattern Recognition}, 2020, pp. 7662--7670.

\bibitem{alliegro2021denoise}
A.~Alliegro, D.~Valsesia, G.~Fracastoro, E.~Magli, and T.~Tommasi, ``Denoise
  and contrast for category agnostic shape completion,'' in \emph{Proceedings
  of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2021,
  pp. 4629--4638.

\bibitem{dgcnn}
Y.~Wang, Y.~Sun, Z.~Liu, S.~E. Sarma, M.~M. Bronstein, and J.~M. Solomon,
  ``Dynamic graph cnn for learning on point clouds,'' \emph{ACM Transactions on
  Graphics (TOG)}, 2019.

\bibitem{sag-pool}
J.~Lee, I.~Lee, and J.~Kang, ``Self-attention graph pooling,'' in
  \emph{International conference on machine learning}.\hskip 1em plus 0.5em
  minus 0.4em\relax PMLR, 2019, pp. 3734--3743.

\bibitem{attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,''
  \emph{Advances in neural information processing systems}, vol.~30, 2017.

\bibitem{axform}
K.~Zhang, X.~Yang, Y.~Wu, and C.~Jin, ``Attention-based transformation from
  latent features to point clouds,'' \emph{arXiv preprint arXiv:2112.05324},
  2021.

\bibitem{wiles2020synsin}
O.~Wiles, G.~Gkioxari, R.~Szeliski, and J.~Johnson, ``Synsin: End-to-end view
  synthesis from a single image,'' in \emph{Proceedings of the IEEE/CVF
  Conference on Computer Vision and Pattern Recognition}, 2020, pp. 7467--7477.

\bibitem{insafutdinov18pointclouds}
E.~Insafutdinov and A.~Dosovitskiy, ``Unsupervised learning of shape and pose
  with differentiable point clouds,'' in \emph{Advances in Neural Information
  Processing Systems (NeurIPS)}, 2018.

\bibitem{grabner2020geometric}
A.~Grabner, Y.~Wang, P.~Zhang, P.~Guo, T.~Xiao, P.~Vajda, P.~M. Roth, and
  V.~Lepetit, ``Geometric correspondence fields: Learned differentiable
  rendering for 3d pose refinement in the wild,'' in \emph{European Conference
  on Computer Vision}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2020,
  pp. 102--119.

\bibitem{palazzi2018end}
A.~Palazzi, L.~Bergamini, S.~Calderara, and R.~Cucchiara, ``End-to-end 6-dof
  object pose estimation through differentiable rasterization,'' in
  \emph{Proceedings of the European Conference on Computer Vision (ECCV)
  Workshops}, 2018, pp. 0--0.

\bibitem{xie2021style}
C.~Xie, C.~Wang, B.~Zhang, H.~Yang, D.~Chen, and F.~Wen, ``Style-based point
  generator with adversarial rendering for point cloud completion,'' in
  \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, 2021, pp. 4619--4628.

\bibitem{balancedcd}
T.~Wu, L.~Pan, J.~Zhang, T.~Wang, Z.~Liu, and D.~Lin, ``{Balanced Chamfer
  Distance as a Comprehensive Metric for Point Cloud Completion},''
  \emph{Advances in Neural Information Processing Systems}, vol.~34, 2021.

\bibitem{chang2015shapenet}
A.~X. Chang, T.~Funkhouser, L.~Guibas, P.~Hanrahan, Q.~Huang, Z.~Li,
  S.~Savarese, M.~Savva, S.~Song, H.~Su, J.~Xiao, L.~Yi, and F.~Yu,
  ``{ShapeNet: An Information-Rich 3D Model Repository},'' Stanford University
  --- Princeton University --- Toyota Technological Institute at Chicago, Tech.
  Rep. arXiv:1512.03012 [cs.GR], 2015.

\bibitem{resnet}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image
  recognition,'' in \emph{Proceedings of the IEEE conference on computer vision
  and pattern recognition}, 2016, pp. 770--778.

\bibitem{Ravi2020Accelerating3D}
N.~Ravi, J.~Reizenstein, D.~Novotn{\'y}, T.~Gordon, W.-Y. Lo, J.~Johnson, and
  G.~Gkioxari, ``Accelerating 3d deep learning with pytorch3d,'' \emph{SIGGRAPH
  Asia 2020 Courses}, 2020.

\bibitem{Kingma2015AdamAM}
D.~P. Kingma and J.~Ba, ``Adam: A method for stochastic optimization,'' in
  \emph{Internation Conference on Learning Representations}, 2015.

\bibitem{folding}
Y.~Yang, C.~Feng, Y.~Shen, and D.~Tian, ``{FoldingNet: Point Cloud Auto-Encoder
  via Deep Grid Deformation},'' in \emph{Proceedings of the IEEE conference on
  computer vision and pattern recognition}, 2018, pp. 206--215.

\bibitem{fscore}
A.~Knapitsch, J.~Park, Q.-Y. Zhou, and V.~Koltun, ``Tanks and temples:
  Benchmarking large-scale scene reconstruction,'' \emph{ACM Transactions on
  Graphics (TOG)}, vol.~36, pp. 1 -- 13, 2017.

\end{thebibliography}






\end{document}