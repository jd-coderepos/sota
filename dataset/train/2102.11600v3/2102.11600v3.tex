\newif\ificml
\icmltrue


\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} \usepackage{amsmath}
\usepackage{mathtools}
\usepackage{color}
\usepackage{amsthm, amssymb, bm}
\usepackage[justification=justified,singlelinecheck=false]{caption}
\usepackage[flushleft]{threeparttable}
\usepackage{hyperref}
\usepackage{multirow, makecell}
\usepackage{footnote}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\newcommand{\theHalgorithm}{\arabic{algorithm}}
\DeclareMathOperator*{\argmax}{\arg\hspace{-0.07em}\max}
\DeclareMathOperator*{\argmin}{\arg\hspace{-0.07em}\min}

\ificml
    \usepackage[accepted]{icml2021}
\else 
    \usepackage{natbib}
    \usepackage{algorithm,algorithmic}
\fi

\newcommand{\cmt}[2]{\textcolor{#1}{#2}}
\newcommand{\mf}[1]{\mathbf{#1}}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{defi}{Definition}


\ificml
    \usepackage{xr}
    \externaldocument{supply}
\fi

\ificml
    \icmltitlerunning{ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Networks}
\fi

\begin{document}

\ificml
    \twocolumn[
    \icmltitle{ASAM: Adaptive Sharpness-Aware Minimization\\ for Scale-Invariant Learning of Deep Neural Networks}


    \icmlsetsymbol{equal}{*}

    \begin{icmlauthorlist}
    \icmlauthor{Jungmin Kwon}{sr}
    \icmlauthor{Jeongseop Kim}{sr}
    \icmlauthor{Hyunseo Park}{sr}
    \icmlauthor{In Kwon Choi}{sr}
    \end{icmlauthorlist}


    \icmlaffiliation{sr}{Samsung Research, Seoul, Republic of Korea}

    \icmlcorrespondingauthor{Jeongseop Kim}{jisean.kim@samsung.com}

    \icmlkeywords{Machine Learning, ICML}

    \vskip 0.3in
    ]
    \printAffiliationsAndNotice{}  
\else 
    \title{ASAM: Adaptive Sharpness-Aware Minimization\\ for Scale-Invariant Learning of Deep Neural Networks}
    \author{
      Jungmin Kwon
\and
      Jeongseop Kim\thanks{Samsung Research, Seoul, Republic of Korea, Correspondence to: Jeongseop Kim .} \\
      \and
      Hyunseo Park
\and
      In Kwon Choi
}
    \date{}
    \maketitle
\fi


\ificml
    \newcommand\figureratio{0.85}
    \newcommand\beginfigure{\begin{figure}[!t]}
    \newcommand\jmkendfigure{\end{figure}}
    \newcommand\begintable{\begin{table}[!h]}
    \newcommand\jmkendtable{\end{table}}
\else
    \newcommand\figureratio{0.75}
    \newcommand\beginfigure{\begin{figure}}
    \newcommand\jmkendfigure{\end{figure}}
    \newcommand\begintable{\begin{table}}
    \newcommand\jmkendtable{\end{table}}
\fi 



\begin{abstract}

Recently, learning algorithms motivated from sharpness of loss surface as an effective measure of generalization gap have shown state-of-the-art performances. Nevertheless, sharpness defined in a rigid region with a fixed radius, has a drawback in sensitivity to parameter re-scaling which leaves the loss unaffected, leading to weakening of the connection between sharpness and generalization gap. In this paper, we introduce the concept of adaptive sharpness which is scale-invariant and propose the corresponding generalization bound. We suggest a novel learning method, adaptive sharpness-aware minimization (ASAM), utilizing the proposed generalization bound. Experimental results in various benchmark datasets show that ASAM contributes to significant improvement of model generalization performance.


\end{abstract}

\section{Introduction}
\label{introduction}

Generalization of deep neural networks has recently been studied with great importance to address the shortfalls of pure optimization, yielding models with no guarantee on generalization ability. 
To understand the generalization phenomenon of neural networks, many studies have attempted to clarify the relationship between the geometry of the loss surface and the generalization performance \citep{hochreiter1995simplifying, mcallester1999pac, keskar2017large, neyshabur2017exploring, jiang2019fantastic}. 
Among many proposed measures used to derive generalization bounds, loss surface sharpness and 
minimization of the derived generalization bound have proven to be effective in attaining state-of-the-art performances in various tasks \citep{hochreiter1997flat, mobahi2016training, chaudhari2019entropy, DBLP:journals/corr/abs-2006-05620, yue2020salr}.


Especially, Sharpness-Aware Minimization (SAM) \citep{foret2021sharpnessaware} as a learning algorithm based on PAC-Bayesian generalization bound, achieves a state-of-the-art generalization performance for various image classification tasks benefiting from minimizing sharpness of loss landscape, which is correlated with generalization gap. Also, they suggest a new sharpness calculation strategy, which is computationally efficient, since it requires only a single gradient ascent step in contrast to other complex generalization measures such as sample-based or Hessian-based approach. 

However, even sharpness-based learning methods including SAM and some of sharpness measures suffer from sensitivity to model parameter re-scaling. \citet{dinh2017sharp} point out that parameter re-scaling which does not change loss functions can cause a difference in sharpness values so this property may weaken correlation between sharpness and generalization gap. We call this phenomenon scale-dependency problem. 

To remedy the scale-dependency problem of sharpness, many studies have been conducted recently \citep{liang2019fisher, yi2019positively, karakida2019normalization, tsuzuku2020normalized}. However, those previous works are limited to proposing only generalization measures which do not suffer from the scale-dependency problem and do not provide sufficient investigation on combining learning algorithm with the measures. 

To this end, we introduce the concept of normalization operator which is not affected by any scaling operator that does not change the loss function.
The operator varies depending on the way of normalizing, e.g., element-wise and filter-wise.
We then define \textit{adaptive sharpness} of the loss function, sharpness whose maximization region is determined by the normalization operator.
We prove that adaptive sharpness remains the same under parameter re-scaling, i.e., \textit{scale-invariant}.
Due to the scale-invariant property, adaptive sharpness shows stronger correlation with generalization gap than sharpness does.

Motivated by the connection between generalization metrics and loss minimization, we propose a novel learning method, adaptive sharpness-aware minimization (ASAM), which adaptively adjusts maximization regions thus acting uniformly under parameter re-scaling. ASAM minimizes the corresponding generalization bound using adaptive sharpness to generalize on unseen data, avoiding the scale-dependency issue SAM suffers from.

The main contributions of this paper are summarized as follows:

\begin{itemize}
    \item We introduce adaptive sharpness of loss surface which is invariant to parameter re-scaling. 
    In terms of rank statistics, adaptive sharpness shows stronger correlation with generalization than sharpness does, which means that adaptive sharpness is more effective measure of generalization gap.
    \item We propose a new learning algorithm using adaptive sharpness which helps alleviate the side-effect in training procedure caused by scale-dependency by adjusting their maximization region with respect to weight scale. 
    \item We empirically show its consistent improvement of generalization performance on image classification and machine translation tasks using various neural network architectures.
\end{itemize}

The rest of this paper is organized as follows. Section~\ref{sec:p} briefly describes previous sharpness-based learning algorithm. In Section~\ref{sec:as}, we introduce adaptive sharpness which is a scale-invariant measure of generalization gap after scale-dependent property of sharpness is explained. In Section~\ref{sec:a}, ASAM algorithm is introduced in detail using the definition of adaptive sharpness.
In Section~\ref{sec:e}, we evaluate the generalization performance of ASAM for various models and datasets.
We provide the conclusion and future work in Section~\ref{sec:c}.




\section{Preliminary} \label{sec:p}
Let us consider a model  parametrized by a weight vector  and a loss function .
Given a sample  drawn from data distribution  with i.i.d condition, the training loss can be defined as .
Then, the generalization gap between the expected loss  and the training loss  represents the ability of the model to generalize on unseen data.

Sharpness-Aware Minimization (SAM) \citep{foret2021sharpnessaware} aims to minimize the following PAC-Bayesian generalization error upper bound


for some strictly increasing function .
The domain of max operator, called maximization region, is an  ball with radius  for .
Here, sharpness of the loss function  is defined as

Because of the monotonicity of  in Equation~\ref{eq:p1}, it can be substituted by  weight decaying regularizer, so the sharpness-aware minimization problem can be defined as the following minimax optimization

\noindent where  is a weight decay coefficient.

SAM solves the minimax problem by iteratively applying the following two-step procedure for  as

where  is an appropriately scheduled learning rate. This procedure can be obtained by a first order approximation of  and dual norm formulation as

and

where  and  denotes element-wise absolute value function, and  also denotes element-wise signum function.
It is experimentally confirmed that the above two-step procedure produces the best performance when , which results in Equation~\ref{sam}.

\noindent As can be seen from Equation~\ref{sam}, SAM estimates the point  at which the loss is approximately maximized around  in a rigid region with a fixed radius by performing gradient ascent, and performs gradient descent at  using the gradient at the maximum point .


\section{Adaptive Sharpness: Scale-Invariant Measure of Generalization Gap} \label{sec:as}

In \citet{foret2021sharpnessaware}, it is experimentally confirmed that the sharpness defined in Equation~\ref{eq:s1} is strongly correlated with the generalization gap. Also they show that SAM helps to find minima which show lower sharpness than other learning strategies and contributes to effectively lowering generalization error.

However, \citet{dinh2017sharp} show that sharpness defined in the rigid spherical region with a fixed radius can have a weak correlation with the generalization gap due to non-identifiability of rectifier neural networks, whose parameters can be freely re-scaled without affecting its output. 

If we assume that  is a scaling operator on the weight space that does not change the loss function, as shown in Figure~\ref{fig:sphere}, the interval of the loss contours around  becomes narrower than that around  but the size of the region remains the same, i.e.,


Thus, neural networks with  and  can have arbitrarily different values of sharpness defined in Equation~\ref{eq:s1}, although they have the same generalization gaps. This property of sharpness is a main cause of weak correlation between generalization gap and sharpness and we call this scale-dependency in this paper.

To solve the scale-dependency of sharpness, we introduce the concept of adaptive sharpness. Prior to explaining adaptive sharpness, we first define normalization operator. The normalization operator that cancels out the effect of  can be defined as follows.


\begin{defi}[Normalization operator]\label{norm_op}
	Let  be a family of invertible linear operators on .
	Given a weight , if  for any invertible scaling operator  on  which does not change the loss function, we say  is a normalization operator of .
\end{defi}

Using the normalization operator, we define adaptive sharpness as follows.
\begin{defi}[Adaptive sharpness]
If  is the normalization operator of  in Definition \ref{norm_op}, adaptive sharpness of  is defined by

where .
\end{defi}

Adaptive sharpness in Equation~\ref{eq:s4} has the following properties.

\beginfigure
\centering    
\subfigure[ and  \citep{foret2021sharpnessaware} \label{fig:sphere}]{\includegraphics[width=\figureratio\linewidth]{sphere.eps}}
\subfigure[ and  \citep{keskar2017large}\label{fig:cuboid}]{\includegraphics[width=\figureratio\linewidth]{cuboid.eps}}
\subfigure[ and  (In this paper)\label{fig:ellipsoid}]{\includegraphics[width=\figureratio\linewidth]{ellipsoid.eps}}
\caption{Loss contours and three types of maximization regions: (a) sphere, (b) cuboid and (c) ellipsoid.  and  are parameter points before and after multiplying a scaling operator  and are expressed as dots and triangles, respectively. The blue contour line has the same loss at , and the red contour line has a loss equal to the maximum value of the loss in each type of region centered on .  and  are the  and  which maximize the loss perturbed from  and , respectively.}
\label{fig1}
\jmkendfigure

\begin{thm} \label{thm1}
	For any invertible scaling operator  which does not change the loss function, values of adaptive sharpness at  and  are the same as
	
	where  and  are the normalization operators of  and  in Definition \ref{norm_op}, respectively. 
\end{thm}
\begin{proof}
	From the assumption, it suffices to show that the first terms of both sides are equal.
	By the definition of the normalization operator, we have .
	Therefore,
	
	where .
\end{proof}


By Theorem~\ref{thm1}, adaptive sharpness defined in Equation \ref{eq:s4} is \textit{scale-invariant} as with training loss and generalization loss. 
This property makes the correlation of adaptive sharpness with the generalization gap stronger than that of sharpness in Equation~\ref{eq:s1}.

Figure~\ref{fig:cuboid} and \ref{fig:ellipsoid} show how a re-scaled weight vector can have the same adaptive sharpness value as that of the original weight vector. It can be observed that the boundary line of each region centered on  is in contact with the red line. This implies that the maximum loss within each region centered on  is maintained when  is used for the maximization region. Thus, in this example, it can be seen that adaptive sharpness in \ref{fig:cuboid} and \ref{fig:ellipsoid} has scale-invariant property in contrast to sharpness of the spherical region shown in Figure~\ref{fig:sphere}.

The question that can be asked here is what kind of operators  can be considered as normalization operators which satisfy  for any  which does not change the loss function. One of the conditions for the scaling operator  that does not change the loss function is that it should be node-wise scaling, which corresponds to row-wise or column-wise scaling in fully-connected layers and channel-wise scaling in convolutional layers. The effect of such node-wise scaling can be canceled using the inverses of the following operators:
\begin{itemize}
	\item element-wise
	
	where
    
\item filter-wise
	
	where
    
\end{itemize}

Here,  is the -th flattened weight vector of a convolution filter and  is the -th weight parameter which is not included in any filters. And  is the number of filters and  is the number of other weight parameters in the model. If there is no convolutional layer in a model (i.e., ), then  and both normalization operators are identical to each other.
Note that we use  rather than  for sufficiently small  for stability.
 is a hyper-parameter controlling trade-off between adaptivity and stability. 

\beginfigure
\centering
\ificml
\subfigure[Sharpness (), \newline \hspace{1cm}  .]{\includegraphics[width=0.48\linewidth]{scatter_1.eps}}
\subfigure[Adaptive sharpness (), \newline .]{\includegraphics[width=0.48\linewidth]{scatter_2.eps}}
\subfigure[Sharpness (), \newline .]{\includegraphics[width=0.48\linewidth]{scatter_3.eps}}
\subfigure[Adaptive sharpness \newline(), .]{\includegraphics[width=0.48\linewidth]{scatter_4.eps}}
\else
\subfigure[Sharpness (), .]{\includegraphics[width=0.48\linewidth]{scatter_1.eps}}
\subfigure[Adaptive sharpness (), .]{\includegraphics[width=0.48\linewidth]{scatter_2.eps}}
\subfigure[Sharpness (), .]{\includegraphics[width=0.48\linewidth]{scatter_3.eps}}
\subfigure[Adaptive sharpness (), .]{\includegraphics[width=0.48\linewidth]{scatter_4.eps}}
\fi
\caption{Scatter plots which show correlation of sharpness and adaptive sharpness with respect to generalization gap and their rank correlation coefficients .}\label{scatter}
\jmkendfigure

\begin{table}[t]
\ificml
\else
    \centering
    \captionsetup{justification=centering}
\fi
\setlength\tabcolsep{3.5pt}
\caption{Rank statistics for sharpness and adaptive sharpness. \label{rank}}
\begin{center}
\begin{small}
\begin{tabular}{lcccc}
\toprule
\multirow{2}{*}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{}\\
\cmidrule{2-5}
& \multirow{1}{*}{sharpness} & \makecell{adaptive\\sharpness} & sharpness & \makecell{adaptive\\sharpness}\\
\midrule
 (rank corr.) &  &  &  & \\
mini-batch size &  &  &  & \\
learning rate &  &  &  & \\
weight decay &  &  &  & \\
dropout rate &  &  &  & \\
 (avg.) &  &  &  & \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\end{table}

To confirm that adaptive sharpness actually has a stronger correlation with generalization gap than sharpness, we compare \textit{rank statistics} which demonstrate the change of adaptive sharpness and sharpness with respect to generalization gap. For correlation analysis, we use  hyper-parameters: mini-batch size, initial learning rate, weight decay coefficient and dropout rate. As can be seen in Table~\ref{rank}, Kendall rank correlation coefficient \citep{kendall} of adaptive sharpness is greater than that of sharpness regardless of the value of . Furthermore, we compare granulated coefficients \citep{jiang2019fantastic} with respect to different hyper-parameters to measure the effect of each hyper-parameter separately. In Table~\ref{rank}, the coefficients of adaptive sharpness are higher in most hyper-parameters and the average as well. Scatter plots illustrated in Figure~\ref{scatter} also show stronger correlation of adaptive sharpness. The difference in correlation behaviors of adaptive sharpness and sharpness provides an evidence that scale-invariant property helps strengthen the correlation with generalization gap. The experimental details are described in Appendix~\ref{corrdetail}.

Although there are various normalization methods other than the normalization operators introduced above, this paper covers only element-wise and filter-wise normalization operators. Node-wise normalization can also be viewed as a normalization operator. \citet{tsuzuku2020normalized} suggest node-wise normalization method for obtaining normalized flatness. However, the method requires that the parameter should be at a critical point. Also, in the case of node-wise normalization using unit-invariant SVD \citep{uhlmann2018generalized}, there is a concern that the speed of the optimizer can be degraded due to the significant additional cost for scale-direction decomposition of weight tensors. Therefore the node-wise normalization is not covered in this paper. In the case of layer-wise normalization using spectral norm or Frobenius norm of weight matrices \citep{neyshabur2017exploring}, the condition  is not satisfied. Therefore, it cannot be used for adaptive sharpness so we do not cover it in this paper. 

Meanwhile, even though all weight parameters including biases can have scale-dependency, there remains more to consider when applying normalization to the biases. In terms of bias parameters of rectifier neural networks, there also exists translation-dependency in sharpness, which weakens the correlation with the generalization gap as well. Using the similar arguments as in the proof of Theorem~\ref{thm1}, it can be derived that diagonal elements of  corresponding to biases must be replaced by constants to guarantee translation-invariance, which induces adaptive sharpness that corresponds to the case of not applying bias normalization. We compare the generalization performance based on adaptive sharpness with and without bias normalization, in Section~\ref{sec:e}.



There are several previous works which are closely related to adaptive sharpness.
\citet{li2018visualizing}, which suggest a methodology for visualizing loss landscape, is related to adaptive sharpness. In that study, filter-wise normalization which is equivalent to the definition in Equation~\ref{filterwise} is used to remove scale-dependency from loss landscape and make comparisons between loss functions meaningful. In spite of their empirical success, \citet{li2018visualizing} do not provide a theoretical evidence for explaining how filter-wise scaling contributes the scale-invariance and correlation with generalization. In this paper, we clarify how the filter-wise normalization relates to generalization by proving the scale-invariant property of adaptive sharpness in Theorem \ref{thm1}.

Also, sharpness suggested in \citet{keskar2017large} can be regarded as a special case of adaptive sharpness which uses  and the element-wise normalization operator.
\citet{jiang2019fantastic} confirm experimentally that the adaptive sharpness suggested in \citet{keskar2017large} shows a higher correlation with the generalization gap than sharpness which does not use element-wise normalization operator.
This experimental result implies that Theorem~\ref{thm1} is also practically validated.

Therefore, it seems that sharpness with  suggested by \citet{keskar2017large} also can be used directly for learning as it is, but a problem arises in terms of generalization performance in learning. \citet{foret2021sharpnessaware} confirm experimentally that the generalization performance with sharpness defined in square region  result is worse than when SAM is performed with sharpness defined in spherical region .

We conduct performance comparison tests for  and , and experimentally reveal that  is more suitable for learning as in \citet{foret2021sharpnessaware}. The experimental results are shown in Section~\ref{sec:e}.


\section{Adaptive Sharpness-Aware Minimization} \label{sec:a}

In the previous section, we introduce a scale-invariant measure called adaptive sharpness to overcome the limitation of sharpness.
As in sharpness, we can obtain a generalization bound using adaptive sharpness, which is presented in the following theorem.
\begin{thm}\label{thm2}
Let  be the normalization operator on .
If  for some ,
then with probability , 

where  is a strictly increasing function,  and .
\end{thm}


\begin{algorithm}[tb]
   \caption{ASAM algorithm ()}
   \label{alg1}
\begin{algorithmic}
   \STATE {\bfseries Input:} Loss function , training dataset , mini-batch size , radius of maximization region , weight decay coefficient , scheduled learning rate , initial weight .
   \STATE {\bfseries Output:} Trained weight 
   \STATE Initialize weight 
   \STATE {\bfseries while} not converged  {\bfseries do} 
   \STATE   \hspace{0.05\linewidth} Sample a mini-batch  of size  from 
   \STATE   \hspace{0.05\linewidth} 
   \STATE   \hspace{0.05\linewidth} 
   \STATE {\bfseries end while}
   \STATE {\bfseries return} 
\end{algorithmic}
\end{algorithm}


Note that Theorem \ref{thm2} still holds for  due to the monotonicity of -norm, i.e.,
if ,  for any .
If  is an identity operator, Equation~\ref{eq:a1} is reduced equivalently to Equation~\ref{eq:p1}.
The proof of Equation~\ref{eq:a1} is described in detail in Appendix~\ref{app1}. 

The right hand side of Equation \ref{eq:a1}, i.e., generalization bound, can be expressed using adaptive sharpness as
\ificml
    
\else
    
\fi

Since  is a strictly increasing function with respect to , it can be substituted with  weight decaying regularizer. Therefore, we can define adaptive sharpness-aware minimization problem as

To solve the minimax problem in Equation \ref{eq:a4}, it is necessary to find optimal  first.
Analogous to SAM, we can approximate the optimal  to maximize  using a first-order approximation as

\noindent where 
Then, the two-step procedure for adaptive sharpness-aware minimization (ASAM) is expressed as

for .
Especially, if ,

and if ,


In this study, experiments are conducted on ASAM in cases of  and . 
The ASAM algorithm with  is described in detail on Algorithm~\ref{alg1}. Note that the SGD \citep{nesterov1983method} update marked with  in Algorithm~\ref{alg1} can be combined with momentum or be replaced by update of another optimization scheme such as Adam~\citep{kingma2015adam}.




\section{Experimental Results} \label{sec:e}
In this section, we evaluate the performance of ASAM.
We first show how SAM and ASAM operate differently in a toy example.
We then compare the generalization performance of ASAM with other learning algorithms for various model architectures and various datasets: CIFAR-10, CIFAR-100 \citep{krizhevsky2009cifar}, ImageNet \citep{imagenet} and IWSLT'14 DE-EN~\citep{cettolo2014report}. Finally, we show how robust to label noise ASAM is.

\subsection{Toy Example} \label{toy}


\begin{figure}
\centering
\captionsetup{justification=centering}
\includegraphics[width=\linewidth]{trajectory.eps}
\caption{Trajectories of SAM and ASAM.}\label{contour}
\end{figure}

As mentioned in Section \ref{sec:a}, sharpness varies by parameter re-scaling even if its loss function remains the same, while adaptive sharpness does not.
To elaborate this, we consider a simple loss function  where .
Figure \ref{contour} presents the trajectories of SAM and ASAM with two different initial weights  and .
The red line represents the set of minimizers of the loss function , i.e., .
As seen in Figure \ref{fig:sphere}, sharpness is maximized when  within the same loss contour line, and therefore SAM tries to converge to .
Here, we use  as in \citet{foret2021sharpnessaware}.
On the other hand, adaptive sharpness remains the same along the same contour line which implies that ASAM converges to the point in the red line near the initial point as can be seen in Figure \ref{contour}.

Since SAM uses a fixed radius in a spherical region for minimizing sharpness, it may cause undesirable results depending on the loss surface and the current weight.
If , while SAM even fails to converge to the valley with , ASAM converges no matter which  is used if . In other words, appropriate  for SAM is dependent on the scales of  on the training trajectory, whereas  of ASAM is not.

\begin{figure}[!t]
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{acc2.eps}}
\caption{
Test accuracy curves obtained from ASAM algorithm using a range of  with different factors: element-wise normalization with , element-wise normalization with  with and without bias normalization (BN) and filter-wise normalization with .
}
\label{normalization}
\end{center}
\end{figure}


\begin{table}[t]
\ificml
\else
    \centering
\fi
\begin{threeparttable}
\caption{Maximum test accuracies for SGD, SAM and ASAM on CIFAR-10 dataset. \label{table1}}
\begin{center}
\begin{small}
\begin{tabular}{lcccr}
\toprule
Model & SGD & SAM & ASAM \\
\midrule
DenseNet-121 &  &  &  \\
ResNet-20 &  &  &  \\
ResNet-56 &  &  &  \\
VGG19-BN &  &  &  \\
{\fontsize{8}{9.6}\selectfont ResNeXt29-32x4d} &  &  &  \\
WRN-28-2 &  &  &  \\
WRN-28-10 &  &  &  \\
\midrule
{\fontsize{8.5}{9.6}\selectfont PyramidNet-272} &  &  &  \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item[*] Some runs completely failed, thus giving 10\% of accuracy (success rate: SGD: 3/5, SAM 1/5, ASAM 3/5)
\item[] PyramidNet-272 architecture is tested 3 times for each learning algorithm.
\end{tablenotes}
\end{small}
\end{center}
\end{threeparttable}
\end{table}


\begin{table}[t]
\ificml
\else
    \centering
\fi
\begin{threeparttable}
\caption{Maximum test accuracies for SGD, SAM and ASAM on CIFAR-100 dataset. \label{table2}}
\begin{center}
\begin{small}
\begin{tabular}{lcccr}
\toprule
Model & SGD & SAM & ASAM \\
\midrule
DenseNet-121 &  &  &  \\
ResNet-20 &  &  &  \\
ResNet-56 &  &  &  \\
VGG19-BN &  &  &  \\
{\fontsize{8}{9.6}\selectfont ResNeXt29-32x4d} &  &  &  \\
WRN-28-2 &  &  &  \\
WRN-28-10 &  &  &  \\
\midrule
{\fontsize{8.5}{9.6}\selectfont PyramidNet-272} &  &  &  \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item[*] Some runs completely failed, thus giving 10\% of accuracy (success rate: SGD: 5/5, SAM 4/5, ASAM 4/5)
\item[] PyramidNet-272 architecture is tested 3 times for each learning algorithm.
\end{tablenotes}
\end{small}
\end{center}
\end{threeparttable}
\end{table}


\subsection{Image Classification: CIFAR-10/100 and ImageNet} \label{cifar}
To confirm the effectiveness of ASAM, we conduct comparison experiments with SAM using CIFAR-10 and CIFAR-100 datasets.
We use the same data split as the original paper \citep{krizhevsky2009cifar}. The hyper-parameters used in this test is described in Table~\ref{table2}. Before the comparison tests with SAM, there are three factors to be chosen in ASAM algorithm:
\begin{itemize}
\item normalization schemes: element-wise vs. filter-wise
\item -norm:  vs. 
\item bias normalization: with vs. without
\end{itemize}

First, we perform the comparison test for filter-wise and element-wise normalization using WideResNet-16-8 model \citep{zagoruyko2016wide} and illustrate the results in Figure~\ref{normalization}. As can be seen, both test accuracies are comparable across , and element-wise normalization provides a slightly better accuracy at . 

Similarly, Figure~\ref{normalization} shows how much test accuracy varies with the maximization region for adaptive sharpness. It can be seen that  shows better test accuracies than , which is consistent with \citet{foret2021sharpnessaware}. We could also observe that bias normalization does not contribute to the improvement of test accuracy in Figure~\ref{normalization}. Therefore, we decide to use element-wise normalization operator and , and not to employ bias normalization in the remaining tests.

As ASAM has a hyper-parameter  to be tuned, we first conduct a grid search over \{, , , \ldots, , , \} for finding appropriate values of . We use  for CIFAR-10 and  for CIFAR-100, because it gives moderately good performance across various models. We set  for SAM as  for CIFAR-10 and  for CIFAR-100 as in \citet{foret2021sharpnessaware}.  for ASAM is set to . 
We set mini-batch size to , and -sharpness suggested by \citet{foret2021sharpnessaware} is not employed. The number of epochs is set to  for SAM and ASAM and  for SGD. Momentum and weight decay coefficient are set to  and , respectively. 
Cosine learning rate decay \citep{loshchilov2016sgdr} is adopted with an initial learning rate . Also, random resize, padding by four pixels, normalization and random horizontal flip are applied for data augmentation and label smoothing \citep{NEURIPS2019_f1748d6b} is adopted with its factor of .

Using the hyper-parameters, we compare the best test accuracies obtained by SGD, SAM and ASAM for various rectifier neural network models: VGG \citep{simonyan2014very}, ResNet \citep{he2016deep}, DenseNet \citep{huang2017densely}, WideResNet \citep{zagoruyko2016wide}, and ResNeXt \citep{xie2017aggregated}. 



For PyramidNet-272~\citep{han2017deep}, we additionally apply some latest techniques: AutoAugment~\citep{cubuk2019autoaugment}, CutMix~\citep{yun2019cutmix} and ShakeDrop~\citep{yamada2019shakedrop}. We employ the -sharpness strategy with . Initial learning rate and mini-batch size are set to  and , respectively. The number of epochs is set to  for SAM and ASAM and  for SGD. We choose  for SAM as , as in \citet{foret2021sharpnessaware}, and  for ASAM as  for both CIFAR-10 and CIFAR-100. Every entry in the tables represents mean and standard deviation of 5 independent runs. In both CIFAR-10 and CIFAR-100 cases, ASAM generally surpasses SGD and SAM, as can be seen in Table~\ref{table1} and Table~\ref{table2}.

For the sake of evaluations at larger scale, we compare the performance of SGD, SAM and ASAM on ImageNet. We apply each method with ResNet-50 and use  for SAM and  for ASAM. The number of training epochs is  for SGD and  for SAM and ASAM. We use mini-batch size , initial learning rate , and SGD optimizer with weight decay coefficient . Other hyper-parameters are the same as those of CIFAR-10/100 tests. We also employ -sharpness with  for both SAM and ASAM.

Table~\ref{imagenet} shows mean and standard deviation of maximum test accuracies over  independent runs for each method. As can be seen in the table, ASAM achieves higher accuracies than SGD and SAM. These results imply that ASAM can enhance generalization performance of rectifier neural network architectures in image classification task beyond CIFAR.


\begin{table}[h]
\setlength\tabcolsep{4.5pt}
\caption{Top1 and Top5 maximum test accuracies for SGD, SAM and ASAM on ImageNet dataset using ResNet-50. \label{imagenet}}
\begin{center}
\begin{small}
\begin{tabular}{lcccc}
\toprule
& SGD & SAM & ASAM\\
\midrule
Top1 &  &  & \\
Top5 &  &  & \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\end{table}

\subsection{Machine Translation: IWSLT'14 DE-EN}
To validate effectiveness of ASAM in tasks other than image classification, we apply SAM and ASAM to IWSLT'14 DE-EN, a dataset on machine translation task. 

In this test, we adopt Transformer architecture~\citep{vaswani2017attention} and Adam optimizer as a base optimizer of SAM and ASAM instead of SGD. Learning rate,  and  for Adam are set to ,  and , respectively. Dropout rate and weight decay coefficient are set to 0.3 and 0.0001, respectively. Label smoothing is adopted with its factor . We choose  for SAM and  for ASAM as a result of a grid search over \{, , , \ldots, , , \} using validation dataset. The results of the experiments are obtained from 3 independent runs.

As can be seen in Table~\ref{iwslt}, we could observe improvement even on IWSLT'14 in BLEU score when using Adam+ASAM instead of Adam or Adam+SAM.


\begin{table}[h]
\setlength\tabcolsep{4.5pt}
\caption{BLEU scores for Adam, Adam+SAM and Adam+ASAM on IWSLT'14 DE-EN dataset using Transformer. \label{iwslt}}
\begin{center}
\begin{small}
\begin{tabular}{lccc}
\toprule
BLEU score & Adam & Adam+SAM & Adam+ASAM\\
\midrule
Validation &  &  & \\
Test &  &  & \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\end{table}






\subsection{Robustness to Label Noise} \label{robust}

As shown in \citet{foret2021sharpnessaware}, SAM is as robust to label noise in the training data as MentorMix \citep{jiang2020beyond}, which is a state-of-the-art method.  
We expect that ASAM would share the robustness to label noise. To confirm this, we compare the test accuracies of SGD, SAM and ASAM for ResNet-32 model and CIFAR-10 dataset whose labels in the training data are corrupted by symmetric label noise \citep{van2015learning} with noise levels of 20\%, 40\%, 60\% and 80\%, and the test data is not touched. Hyper-parameter settings are the same as that of previous CIFAR experiments.
Table~\ref{noise32} shows test accuracies for SGD, SAM and ASAM obtained from 3 independent runs with respect to label noise levels. Compared to SGD and SAM, ASAM generally enhances the test accuracy across various noise level by retaining the robustness to label noise.

\begintable
\caption{Maximum test accuracies of ResNet-32 models trained on CIFAR-10 with label noise.\label{noise32}}
\begin{center}
\begin{small}
\begin{tabular}{lccccr}
\toprule
Noise rate & SGD & SAM & ASAM \\
\midrule
0\% &  &  &  \\
20\% &  &  &  \\
40\% &  &  &  \\
60\% &  &  &  \\
80\% &  &  &  \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\jmkendtable

\ificml
\else
    \clearpage
\fi

\section{Conclusions}\label{sec:c}
In this paper, we have introduced adaptive sharpness with scale-invariant property that improves training path in weight space by adjusting their maximization region with respect to weight scale. Also, we have confirmed that this property, which ASAM shares, contributes in improvement of generalization performance. The superior performance of ASAM is notable from the comparison tests conducted against SAM, which is currently state-of-the-art learning algorithm in many image classification benchmarks. In addition to the contribution as a learning algorithm, adaptive sharpness can serve as a generalization measure with stronger correlation with generalization gap benefiting from their scale-invariant property. Therefore adaptive sharpness has a potential to be a metric for assessment of neural networks. We have also suggested the condition of normalization operator for adaptive sharpness but we did not cover all the normalization schemes which satisfy the condition. So this area could be further investigated for better generalization performance in future works. 

\section{Acknowledgements}
We would like to thank Kangwook Lee, Jaedeok Kim and Yonghyun Ryu for supports on our machine translation experiments. We also thank our other colleagues at Samsung Research - Joohyung Lee, Chiyoun Park and Hyun-Joo Jung - for their insightful discussions and feedback.

\ificml
\else
    \clearpage
\fi


\bibliography{example_paper}
\bibliographystyle{icml2021}


\appendix
\onecolumn
\section{Proofs}

\subsection{Proof of Theorem \ref{thm2}} \label{app1}

We first introduce the following concentration inequality.

\begin{lem}\label{concent}
Let  be independent normal variables with mean  and variance .
Then,

where .
\end{lem}
\begin{proof}
From Lemma 1 in \citet{laurent}, for any ,

Since

plugging  proves the lemma.

\end{proof}

\begin{thm}
Let  be a normalization operator of  on .
If  for some ,
then with probability , 
\ificml
    
\else
    
\fi
where  and .
\end{thm}
\begin{proof}
The idea of the proof is given in \citet{foret2021sharpnessaware}. From the assumption, adding Gaussian perturbation on the weight space does not improve the test error.
Moreover, from Theorem 3.2 in \citet{chatterji2019intriguing}, the following generalization bound holds under the perturbation:
\ificml
    
\else
    
\fi
Therefore, the left hand side of the statement can be bounded as
\ificml
    
\else
    
\fi
where the second inequality follows from Lemma \ref{concent} and  .

\end{proof}


\section{Correlation Analysis} \label{corrdetail}

To capture the correlation between generalization measures, i.e., sharpness and adaptive sharpness, and actual generalization gap, we utilize Kendall rank correlation coefficient \cite{kendall}. Formally, given the set of pairs of a measure and generalization gap observed , Kendall rank correlation coefficient  is given by

Since  represents the difference between the proportion of concordant pairs, i.e., either both  and  or both  and  among the whole  point pairs, and the proportion of discordant pairs, i.e., not concordant, the value of  is in the range of .

While the rank correlation coefficient aggregates the effects of all the hyper-parameters, granulated coefficient \cite{jiang2019fantastic} can consider the correlation with respect to the each hyper-parameter separately. If  is the Cartesian product of each hyper-parameter space , granulated coefficient with respect to  is given by

where . Then the average  of  indicates whether the correlation exists across all hyper-parameters.

We vary  hyper-parameters, mini-batch size, initial learning rate, weight decay coefficient and dropout rate, to produce different models. It is worth mentioning that changing one or two hyper-parameters for correlation analysis may cause spurious correlation \cite{jiang2019fantastic}. For each hyper-parameter, we use  different values in Table~\ref{hyper} which implies that  configurations in total.

\begin{table}[h]
\centering
\captionsetup{justification=centering}
\caption{Hyper-parameter configurations. \label{hyper}}
\begin{center}
\begin{small}
\begin{tabular}{lc}
\toprule
mini-batch size & \\
learning rate & \\
weight decay & \\
dropout rate & \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\end{table}

By using the above hyper-parameter configurations, we train WideResNet-28-2 model on CIFAR-10 dataset. We use SGD as an optimizer and set momentum to . We set the number of epochs to  and cosine learning rate decay \citep{loshchilov2016sgdr} is adopted. Also, random resize, padding by four pixels, normalization and random horizontal flip are applied for data augmentation and label smoothing \citep{NEURIPS2019_f1748d6b} is adopted with its factor of . 
Using model parameters with training accuracy higher than  among the generated models, we calculate sharpness and adaptive sharpness with respect to generalization gap. 

To calculate adaptive sharpness, we fix normalization scheme to element-wise normalization. We calculate adaptive sharpness and sharpness with both  and . We conduct a grid search over \{, , , \ldots, , \} to obtain each  for sharpness and adaptive sharpness which maximizes correlation with generalization gap. As results of the grid search, we select  and  as s for sharpness of  and , respectively, and select  and  as s for adaptive sharpness of  and , respectively. To calculate maximizers of each loss function for calculation of sharpness and adaptive sharpness, we follow -sharpness strategy suggested by \citet{foret2021sharpnessaware} and  is set to .

\end{document}