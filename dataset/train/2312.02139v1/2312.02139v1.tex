\section{Results}
\label{sec:results}

\subsection{Image Space}
We have trained the proposed DiffiT model on CIFAR-10, FFHQ-64 datasets respectively. In Table.~\ref{tab:cifar10_main}, we compare the performance of our model against a variety of different generative models including other score-based diffusion models as well as GANs, and VAEs. DiffiT achieves a state-of-the-art image generation FID score of 1.95 on the CIFAR-10 dataset, outperforming state-of-the-art diffusion models such as EDM~\cite{karras2022elucidating} and LSGM~\cite{vahdat2021score}. In comparison to two recent ViT-based diffusion models, our proposed DiffiT significantly outperforms U-ViT~\cite{bao2022all} and GenViT~\cite{yang2022your} models in terms of FID score in CIFAR-10 dataset. Additionally, DiffiT significantly outperforms EDM~\cite{karras2022elucidating} and DDPM++~\cite{song2020score} models, both on VP and VE training configurations, in terms of FID score. In Fig.~\ref{fig:ffhq}, we illustrate the generated images on FFHQ-64 dataset. Please see supplementary materials for CIFAR-10 generated images.

\begin{table}[t]
\caption{FID performance comparison against various generative approaches on the CIFAR10, FFHQ-64 datasets. VP and VE denote Variance Preserving and Variance Exploding respectively. DiffiT outperforms competing approaches, sometimes by large margins.} \label{tab:cifar10_main}
\centering
\resizebox{1\linewidth}{!}{
\begin{tabular}{lcccc}
        \toprule
        {\bf Method} & {\bf Class}& {\bf Space Type}   & {\bf CIFAR-10} & {\bf FFHQ}   \\
                     & && 3232 & 6464    \\
        \midrule
        NVAE \cite{vahdat2020nvae}      & VAE & - & 23.50 & -    \\
        GenViT~\cite{yang2022your}      & Diffusion & Image & 20.20 & -   \\
        AutoGAN \cite{gong2019autogan}      & GAN & - & 12.40 & -   \\
        TransGAN \cite{jiang2021transgan}      & GAN & - & 9.26 & -    \\
        INDM~\cite{kim2022maximum}      & Diffusion & Latent & 3.09 & -   \\
        DDPM++ (VE)~\cite{song2020score}      & Diffusion & Image & 3.77 & 25.95    \\
        U-ViT~\cite{bao2022all}      & Diffusion & Image & 3.11 & -  \\
        DDPM++ (VP)~\cite{song2020score}      & Diffusion & Image & 3.01 & 3.39    \\
        StyleGAN2 w/ ADA \cite{karras2020training}      & GAN & - & 2.92 & -   \\
        LSGM~\cite{vahdat2021score}      & Diffusion & Latent & 2.01 & -   \\
        EDM (VE)~\cite{karras2022elucidating}      & Diffusion & Image & 2.01 & 2.53    \\
        EDM (VP)~\cite{karras2022elucidating}      & Diffusion & Image & 1.99 & 2.39   \\
        \hline
        \rowcolor{Gray}
        \textbf{DiffiT} (Ours)      & Diffusion & Image & \textbf{1.95} & \textbf{2.22}    \\
        \midrule
        
        
    \end{tabular}}
\end{table}



\subsection{Latent Space}
We have also trained the latent DiffiT model on ImageNet-512 and ImageNet-256 dataset respectively. In Table.~\ref{tab:imagenet}, we present a comparison against other approaches using various image quality metrics. For this comparison, we select the best performance metrics from each model which may include techniques such classifier-free guidance. In ImageNet-256 dataset, the latent DiffiT model outperforms competing approaches, such as MDT-G~\cite{gao2023masked}, DiT-XL/2-G~\cite{peebles2022scalable} and StyleGAN-XL~\cite{sauer2022stylegan}, in terms of FID score and sets a new SOTA FID score of \textbf{1.73}. In terms of other metrics such as IS and sFID, the latent DiffiT model shows a competitive performance, hence indicating the effectiveness of the proposed time-dependant self-attention. In ImageNet-512 dataset, the latent DiffiT model significantly outperforms DiT-XL/2-G in terms of both FID and Inception Score (IS). Although StyleGAN-XL~\cite{sauer2022stylegan} shows better performance in terms of FID and IS, GAN-based models are known to suffer from issues such as low diversity that are not captured by the FID score. These issues are reflected in sub-optimal performance of StyleGAN-XL in terms of both Precision and Recall. In addition, in Fig.~\ref{fig:imagenet-256}, we show a visualization of uncurated images that are generated on ImageNet-256 and ImageNet-512 dataset. We observe that latent DiffiT model is capable of generating diverse high quality images across different classes.   

\begin{table*}[h]
\centering
\resizebox{.95\linewidth}{!}{
\begin{tabular}{lcccccccccc}
\toprule
\multirow{2}{*}{\textbf{Model}} &
\multirow{2}{*}{\textbf{Class}} &
\multicolumn{4}{c}{\textbf{ImageNet-256}} & \multicolumn{5}{c}{\textbf{ImageNet-512}}\\
\cmidrule{3-6}\cmidrule{8-11}
&& FID  & IS   & Precision  & Recall &  & FID  & IS   & Precision  & Recall \\
\midrule
LDM-4~\cite{rombach2022high} & Diffusion &10.56 & 103.49 & 0.71&\textbf{0.62} & &- & - &  - &  - \\
BigGAN-Deep~\cite{brock2018large} & GAN &6.95 & 171.40 & \textbf{0.87}& 0.28 & &8.43 & 177.90 &  \textbf{0.88} &  0.29 \\
MaskGIT~\cite{chang2022maskgit} & Masked Modeling &4.02 & \textbf{355.60} & \underline{0.83} & 0.44 & &4.46 & \textbf{342.00} &  0.83 &  0.50 \\
RQ-Transformer~\cite{lee2022autoregressive} & Autoregressive &3.80 & \underline{323.70} & - & - & &- & - &  - &  - \\
ADM-G-U~\cite{dhariwal2021diffusion} & Diffusion &3.94 & 215.84 & \underline{0.83} & 0.53 & &3.85 & 221.72 &  \underline{0.84} &  \underline{0.53} \\
LDM-4-G~\cite{rombach2022high} & Diffusion &3.60 & 247.67 & \textbf{0.87} & 0.48 & &- & - &  - &  -  \\
Simple Diffusion~\cite{hoogeboom2023simple} & Diffusion &2.77 & 211.80 & - & - & &3.54 & 205.30 &  - &  -  \\
DiT-XL/2-G~\cite{peebles2022scalable} & Diffusion &2.27 & \underline{278.24} & \underline{0.83} & 0.57 & &3.04 & 240.82 &  \underline{0.84} &  0.54  \\
StyleGAN-XL~\cite{sauer2022stylegan} & GAN &2.30 & 265.12 & 0.78 & 0.53 & &\textbf{2.41} & \underline{267.75} &  0.77 & 0.52  \\
MDT-G~\cite{gao2023masked} & Diffusion &\underline{1.79} & 283.01 & 0.81 & \underline{0.61} & &- & - &  - &  -  \\
\hline
\rowcolor{Gray}
\textbf{DiffiT} & Diffusion & \textbf{1.73} & 276.49 & 0.80 & \textbf{0.62} & &\underline{2.67} & 252.12 &  0.83 & \textbf{0.55}  \\

\bottomrule
\end{tabular}}
\caption{Comparison of image generation performance against state-of-the-art models on ImageNet-256 and ImageNet-512 dataset. The latent DiffiT model achieves SOTA performance in terms of FID score on ImageNet-256 dataset.
}
\label{tab:imagenet}
\end{table*}




\begin{figure*}
\centering\includegraphics[width=\textwidth]{Images/imagenet/imagenet_comb.png}
\caption{Visualization of uncurated generated images on ImageNet-256 and ImageNet-512 datasets by latent DiffiT model.}\label{fig:imagenet-256
}
\label{fig:imagenet-256}
\end{figure*}

\section{Ablation}
\label{sec:ablation}
In this section, we provide additional ablation studies to provide insights into DiffiT. We address four main questions: (1) What strikes the right balance between time and feature token dimensions ? (2) How do different components of DiffiT contribute to the final generative performance, (3) What is the optimal way of introducing time dependency in our Transformer block? and (4) How does our time-dependent attention behave as a function of time?

\subsection{Time and Feature Token Dimensions}
\label{sec:abl-effect-time}
We conduct experiments to study the effect of the size of time and feature token dimensions on the overall performance. As shown below, we observe degradation of performance when the token dimension is increased from 256 to 512. Furthermore, decreasing the time embedding dimension from 512 to 256 impacts the performance negatively. 

\begin{table}
\centering
\resizebox{.78\linewidth}{!}{
\setlength{\tabcolsep}{2.5pt}
  \begin{tabular}{lccc}
    \toprule
    Time Dimension  & Dimension   & CIFAR10 & FFHQ64 \\
    \midrule	 
    512&512  & 1.99 &2.27 \\
    256&256  & 2.13 &2.41 \\\rowcolor{Gray}
    512&512  & 1.95 &2.22 \\
    \bottomrule
    
  \end{tabular} 
  }
    \caption{Ablation study on the effectiveness of time and feature dimensions.}
    \label{tab:abl-1-time}
\end{table}

\subsection{Effect of Architecture Design}
\label{sec:abl-effect}
As presented in Table~\ref{tab:abl-1}, we study the effect of various components of both encoder and decoder in the architecture design on the image generation performance in terms of FID score on CIFAR-10. For these experiments, the projected temporal component is adaptively scaled and simply added to the spatial component in each stage. We start from the original ViT~\cite{dosovitskiy2020image} base model with 12 layers and employ it as the encoder (config A). For the decoder, we use the Multi-Level Feature Aggregation variant of SETR~\cite{zheng2021rethinking} (SETR-MLA) to generate images in the input resolution. Our experiments show this architecture is sub-optimal as it yields a final FID score of 5.34. We hypothesize this could be due to the isotropic architecture of ViT which does not allow learning representations at multiple scales. 

\begin{table}
\centering
\resizebox{.9\linewidth}{!}{
\setlength{\tabcolsep}{2.5pt}
  \begin{tabular}{lccc}
    \toprule
    Config & Encoder   & Decoder & FID Score \\
    \midrule	 
    A&ViT~\citep{dosovitskiy2020image}  & SETR-MLA~\citep{zheng2021rethinking} &5.34 \\
   B& + Multi-Resolution   & SETR-MLA~\citep{zheng2021rethinking} &4.64 \\
   C& Multi-Resolution   & + Multi-Resolution  &3.71 \\
    D&+ DiffiT Encoder & Multi-Resolution   & 2.27 \\ \rowcolor{Gray}
    E&+ DiffiT Encoder & + DiffiT Decoder & \textbf{1.95}\\
    \bottomrule
    
  \end{tabular} 
  }
    \caption{Ablation study on the effectiveness of encoder and decoder architecture.}
    \label{tab:abl-1}
\end{table}


We then extend the encoder ViT into 4 different multi-resolution stages with a convolutional layer in between each stage for downsampling (config B). We denote this setup as Multi-Resolution and observe that these changes and learning multi-scale feature representations in the encoder substantially improve the FID score to 4.64.

\begin{table}
\centering
\resizebox{.63\linewidth}{!}{
\setlength{\tabcolsep}{2.5pt}
  \begin{tabular}{lcc}
    \toprule
    Model & TMSA   & FID Score \\
    \midrule	 
    DDPM++(VE)~\cite{song2020score}&No  & 3.77 \\\rowcolor{Gray}
    DDPM++(VE)~\cite{song2020score}&Yes  & 3.49 \\
    DDPM++(VP)~\cite{song2020score}&No  & 3.01 \\\rowcolor{Gray}
    DDPM++(VP)~\cite{song2020score}&Yes  & 2.76 \\

    \bottomrule
    
  \end{tabular} 
  }
    \caption{Impact of TMSA as a standalone module in other denoising networks.}
    \label{table:attn_impact}
\end{table}



In addition, instead of SETR-MLA~\cite{zheng2021rethinking} decoder, we construct a symmetric U-like architecture by using the same Multi-Resolution setup except for using convolutional layer between stage for upsampling (config C). These changes further improve the FID score to 3.71. Furthermore, we first add the DiffiT Transformer blocks and construct a DiffiT Encoder and observe that FID scores substantially improve to 2.27 (config D). As a result, this validates the effectiveness of the proposed TMSA in which the self-attention models both spatial and temporal dependencies. Using the DiffiT decoder further improves the FID score to 1.95 (config E), hence demonstrating the importance of DiffiT Transformer blocks for decoding.


\subsection{Time-Dependent Self-Attention}
\label{sec:abl-temporal-effect}
We evaluate the effectiveness of our proposed TMSA layers in a generic denoising network. Specifically, using the DDPM++~\cite{song2020score} model, we replace the original self-attention layers with TMSA layers for both VE and VP settings for image generation on the CIFAR10 dataset. Note that we did not change the 
original hyper-parameters for this study. As shown in Table~\ref{table:attn_impact} employing TMSA decreases the FID scores by 0.28 and 0.25 for VE and VP settings respectively. These results demonstrate the effectiveness of the proposed TMSA to dynamically adapt to different sampling steps and capture temporal information.   









\begin{table}
\centering
\resizebox{.7\linewidth}{!}{
\setlength{\tabcolsep}{2.5pt}
  \begin{tabular}{lcc}
    \toprule
    Config & Component   & FID Score \\
    \midrule	 
    F&Relative Position Bias  & 3.97 \\
    G&MLP  & 3.81 \\\rowcolor{Gray}
   H& TMSA  & \textbf{1.95} \\

    \bottomrule
    
  \end{tabular} 
  }
    \caption{The impact of TMSA design choices on FID score.}
    \label{tab:abl-2}
\end{table}



\subsection{Impact of Self-Attention Components}
\label{sec:abl-temporal}
In Table~\ref{tab:abl-2}, we study different design choices for introducing time-dependency in self-attention layers. In the first baseline, we remove the temporal component from our proposed TMSA and we only add the temporal tokens to relative positional bias (config F). We observe a significant increase in the FID score to 3.97 from 1.95. In the second baseline, instead of using relative positional bias, we add temporal tokens to the MLP layer of DiffiT Transformer block (config G). We observe that the FID score slightly improves to 3.81, but it is still sub-optimal compared to our proposed TMSA (config H). Hence, this experiment validates the effectiveness of our proposed TMSA that integrates time tokens directly with spatial tokens when forming queries, keys, and values in self-attention layers.   


\begin{figure}
\centering\includegraphics[width=\linewidth]{Images/diffit_comparison.pdf}
\caption{Side-by-side qualitative comparison of attention maps during the denoising process for models with and without TMSA. The denoising process starts at the top row. }\label{fig:self-attn-evol}
\end{figure}










\subsection{Visualization of Self-Attention Maps}
\label{sec:abl-temporal-attn}
One of our key motivations in proposing TMSA is to allow the self-attention module to adapt its behavior dynamically for different stages of the denoising process. In Fig.~\ref{fig:self-attn-evol}, we demonstrate a qualitative comparison of self-attention maps. Although the attention maps without TMSA change in accordance to noise information, they lack fine-grained object details that are perfectly captured by TMSA.











\begin{figure}\centering
\includegraphics[width=0.9\linewidth]{Images/cls.pdf}
\caption{Effect of classifier-free guidance scale on FID score for ImageNet-256 and ImageNet-512 experiments with the latent DiffiT model.}\vspace{-4mm}
\label{fig:self-class-guide}
\end{figure}

\subsection{Effect of Classifier-Free Guidance}
\label{sec:abl-class-guidance}
We investigate the effect of classifier-free guidance scale on the quality of generated samples in terms of FID score. For ImageNet-256 experiment, we used the improved classifier-free guidance~\cite{gao2023masked} which uses a power-cosine schedule to increase the diversity of generated images in early sampling stages. This scheme was not used for ImageNet-512 experiment, since it did not result in any significant improvements. As shown in Fig.~\ref{sec:abl-class-guidance}, the guidance scales of 4.6 and 1.49 correspond to best FID scores of 1.73 and 2.67 for ImageNet-256 and ImageNet-512 experiments, respectively. Increasing the guidance scale beyond these values result in degradation of FID score. 
