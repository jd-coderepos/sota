\begin{figure}[t]
    \vspace{5mm}
    \centering
    \includegraphics[width=\columnwidth]{figures/fig1}
    \caption{Compared with previous multi-stride 3D detectors, our model is single-stride and operates sparsely on the non-empty voxels. We paint the vehicle bounding boxes on the input point cloud to show the tiny object size compared to the  input scene size.}
    \vspace{-4mm}
    \label{fig:fig1}
\end{figure}

\label{sec:intro}
LiDAR-based 3D object detection for autonomous driving has been benefiting from the progress of image-based object detection. The mainstream 3D detectors quantize the 3D space into a stack of pseudo-images from Bird Eye's View (BEV), which makes it convenient to borrow advanced techniques from the 2D counterparts. Many works~\cite{pointpillar, second, centerpoint, rangedet} are proposed under this paradigm and achieve competitive performance.
However, 3D and 2D spaces have intrinsic distinction in their relative object scales, where the objects in 3D spaces have much smaller relative sizes (See Fig.~\ref{fig:hist}). For example, in Waymo Open Dataset~\cite{wod}, the perception range is usually , while a vehicle is only about  long, even a pedestrian occupies as little as  in length. Such a tiny pedestrian equivalently translates to an object of size  pixels in a  image, suggesting that object detection on such a tiny scale is one of the challenges in 3D object detection. 

\par
Different from the above challenge of small scales in the 3D space, 2D detectors have to consider the handling of the objects with varied scales.
It is observed in Fig.~\ref{fig:hist} that the scales of objects in 2D images exhibit a long-tail distribution, while in 3D space they are quite concentrated due to the non-projective transformation used in voxelization.
To handle the varied scales, 2D detectors~\cite{fpn, snip, sniper, tridentnet} usually build multi-scale features with a series of downsampling and upsampling operations.
Such multi-scale architecture is also widely inherited in 3D detectors (See Fig.~\ref{fig:fig1})~\cite{pointpillar, second, voxelnet, centerpoint, rangedet}.
Since the object size in 3D object detectors is usually tiny while no large objects exist, a question naturally arises: \textit{do we really need downsampling in 3D object detectors ?}
\par
With this question in mind, we make an exploratory attempt on the single-stride architecture with \textbf{no downsampling operators}. The single-stride network maintains the original resolution throughout the network. However, it is challenging to make such a design feasible.
The discard of downsampling operators leads to two issues: 1) the increase of computation cost; 2) the decrease of receptive field. 
The former constrains the applicability to the real-time system and the latter hinders the capability of object recognition.
For the issue of computation, sparse convolution seems to be a solution, but the sparse connectivity between voxels\footnote{We provide a clear illustration for this in our supplementary materials.} makes the decrease of receptive field even more severe (See Table~\ref{tab:comp_with_ss_pp}).
For the issue of receptive field, we experimentally show that some commonly adopted techniques do not meet our needs (See Table~\ref{tab:pilot}):
the dilated convolution~\cite{deeplab,dilation} is not friendly to small objects, and the larger kernel leads to unaffordable computational overhead in the single stride architecture.
Therefore, we are getting into a dilemma, where it is difficult to design a convolutional network simultaneously satisfying the three aspects: \textit{single stride architecture}, \textit{sufficient receptive field}, and \textit{acceptable computation cost}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\columnwidth]{figures/size_hist_with_label}
    \caption{\textbf{Distribution of the relative object size  in COCO dataset~\cite{coco} and Waymo Open Dataset (WOD).}  is defined as , where  denotes the area of 2D objects (COCO) and the BEV area of 3D objects (WOD).  is the image area in COCO, and  in WOD. In COCO 73.03\% objects in COCO have a  larger than 0.04, while only 0.54\% objects in WOD have a  larger than 0.04.}
    \label{fig:hist}
\vspace{-5mm}
\end{figure}

\par
These difficulties naturally lead us to think out of the paradigm of CNN, and the attention mechanism emerges as a better option because of the following two reasons: 
1) The attention-based model is better at capturing large context and build sufficient receptive field.
2) Due to the capability of modeling dynamic data, the attention-based model fits well into the sparse voxelized representation of point clouds, where only a small portion of voxels are occupied. This property guarantees the efficiency of our single stride network.
Although the attention mechanism is efficient on sparse data, computing attentions on a global scale is still unaffordable and undesirable. So we partition the voxelized 3D space into many local regions and apply self-attention inside each of them. Eventually, this local attention mechanism, named as \textbf{Sparse Regional Attention (SRA)}, enjoys the best of two worlds. By stacking SRA layers, we make the single-stride network feasible and obtain a transformer-style network, called \textbf{Single-stride Sparse Transformer (SST)}. 
Extensive experiments are conducted on the large-scale Waymo Open Dataset~\cite{wod}.
We summarize our contributions as follows:
\begin{itemize}
\item 
We rethink the architecture of current mainstream LiDAR-based 3D detectors. With pilot experiments, we point out that the network stride is an overlooked design factor for LiDAR-based 3D detectors.  
\item 
We propose the {\bf S}ingle-stride {\bf S}parse {\bf T}ransformer (\textbf{SST}). With its local attention mechanism and capability of handling sparse data, we overcome receptive field shrinkage in the single-stride setting and avoid heavy computational overhead.
\item
Our method achieves state-of-the-art performance on the large-scale Waymo Open Dataset. Thanks to the characteristic of single stride, our method obtains exciting results on tiny objects like pedestrians (83.8 LEVEL\_1 AP on the validation split).
\end{itemize}
