\documentclass{llncs}

\usepackage{xspace,float,amsmath,url}


\newcommand{\ceil}[1]{\lceil #1 \rceil}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}

\newcommand{\exclude}[1]{}

\def \bxor{\textsf{xor}\xspace}
\def \band{\textsf{and}\xspace}

\newcommand{\uxor}{\mathrel{^\wedge}}
\newcommand{\uand}{\mathrel{\&}}
\newcommand{\uor}{\mathrel{|}}
\newcommand{\unot}{\mathop{\sim}}
\newcommand{\fsize}{f}
\newcommand{\bsize}{b}
\newcommand{\len}{r}
\newcommand{\word}[1]{#1}
\newcommand{\fword}[1]{-word}





\begin{document}

\title{A note on the longest common Abelian factor problem}

\author{Szymon Grabowski}

\institute{
	  Lodz University of Technology, Institute of Applied Computer Science, \\
	  Al.\ Politechniki 11, 90--924 {\L}\'od\'z, Poland \\
	  \email{sgrabow@kis.p.lodz.pl}
}

\maketitle

\begin{abstract}
Abelian string matching problems are becoming an object of 
considerable interest in last years.
Very recently, Alatabbi et al.~\cite{AILR2015} presented the first solution 
for the longest common Abelian factor problem for a pair of strings, 
reaching  time with  bits of space, 
where  is the length of the strings and  is the alphabet size.
In this note we show how the time complexity can be preserved while 
the space is reduced by a factor of , and then how the time 
complexity can be improved, if the alphabet is not too small, 
when superlinear space is allowed.
\end{abstract}

\section{Introduction}
\noindent 
The longest common Abelian factor (LCAF) problem, posed at the 
String Masters 2013 meeting by Thierry Lecroq and Arnaud Lefebvre, 
can be stated like that:
Given two strings  and , both of length , over the alphabet 
, 
compute the maximal length of a factor in  such that there exists 
a factor in  being its permutation (i.e., being an Abelian match).
Moreover, it is desirable to return some (or all) occurrences of 
such factors in  and . 

To our knowledge, the only work on this problem was presented 
very recently by Alatabbi et al.~\cite{AILR2015}, in which 
they obtained  worst-case time 
with  bits of space, 
where  is the length of the strings and  is the alphabet size.
Further on, we will express the space in words, and the cited space 
becomes  words.


While the Alatabbi et al. algorithm is simple, 
let us note that the same result can be
immediately obtained by a reduction from a well-known problem, 
the (standard) longest common factor (LCF)\footnote{Also known 
as the longest common substring (LCS) problem. 
We prefer the word ``factor'' in the problem name, to avoid confusion 
with the abbreviation for the longest common subsequence.}.
Hui~\cite{H1992} showed that using a generalized suffix tree it is possible 
to find the LCF for a pair of strings of length  in  time.
We use this algorithm  times, for each factor length , 
replacing each  symbol long factor by its Parikh vector 
followed with a unique terminator (e.g., for the factors taken from  
the subsequent terminators can be 
, , ..., 
while 
for the factors taken from  they can be 
, , ...).
The terminators disallow to have 
matches 
longer than .
If the found LCF is of length exactly , 
it must correspond to a pair of factors, one from  and one from , 
of length .
This is obtained in  time for one value of , 
using  space, hence the total time, 
for all possible factor lengths, becomes  with 
 space 
(we build and discard the generalized suffix trees one by one).
In this way, we obtained the same time and space as Alatabbi et al. did.


\section{Preliminaries}
Let  be a string of length  over an alphabet  of 
size .
It can also be written as , where , , 
denotes its -th symbol.
An analogous notation will be used for arrays.

Throughout the note we assume that  and 
.
(If this is not the case, we can remap the alphabet for both 
input strings at the start with standard means, 
in  time and  extra space.)

The Parikh vector for string , 
denoted as , 
is defined as a vector (array) of size  storing the number 
of occurrences of each alphabet symbol in .
Formally,  iff , for any alphabet 
symbol .
For two strings  and  of equal length and over a common alphabet,
we say that the Parikh vector  is (lexicographically) smaller 
than the Parikh vector , denoted as , 
iff there exists an alphabet symbol , , such that 
 for all  and .
The two Parikh vectors are equal, i.e., , 
when  for all symbols .


\section{Reducing the space}
\noindent
First, let us note that recently Kociumaka et al.~\cite{KSV2014}
showed that for any tradeoff parameter , 
the LCF problem can be solved in  space and  time.
Applying this to the LCAF problem, we obtain 
 time using  space, 
for any .

Yet, the specifics of LCAF allow for a better result.
We consider each factor length  separately.
For a given , we sort all  factors of  
according to their Parikh vectors, using the LSD radix sort.
Each factor is represented as its start position in .
There are  passes of the radix sort and accessing the keys' 
``digits'' seems to be the soft spot of this variant.
Yet, before each pass of the radix sort 
we scan  and for each -sized window collect the count 
of the corresponding symbol in it.
More precisely, just before the -th pass of the radix sort, 
in which the keys will be distributed according to , 
we compute and store  for each factor , 
using  time and  extra space.
Thanks to it, we can access a digit in the radix sort in constant time.
After the -th pass, the  statistics are discarded.
In this way, sorting of the -long factors of  takes  
time and its output (and working area) requires  words of space.

We sort the factors of  in the same way.
Additionally, for every -th evenly sampled -long factor 
of  and , we store explicitly its Parikh vector using  space.
More precisely, we compute and store the Parikh vectors for 
the factors , 
and similarly for 
.
As we scan the strings from left to right and compute the successive 
Parikh vectors incrementally (first making a copy of the previous vector), 
this phase takes  time and  space. 

The computed Parikh vectors serve to speed up factor comparisons 
during the last phase, which is to intersect the lists of factors 
from  and , similarly as in a binary merge operation.
Thanks to the Parikh vectors kept in regular intervals of  and , 
each factor comparison takes  time, therefore the 
intersection takes  time.

The total cost of the described procedure, over all relevant factor lengths, 
becomes  and the required space is .
This matches the time complexity of the Alatabbi et al. solution, 
yet the space usage is decreased by a factor of .


\section{Reducing the time}

\subsection{The general variant}
\noindent 
In this section we present a variant which achieves  time 
for the price of superlinear space.
The key idea is to sort together factors of varying (yet close) lengths.

The whole sorting phase runs in  steps, , 
where in the -th step the factors of both  and  
of all lengths from  to  
are considered (yet, each group of factors, defined by their length, 
is sorted separately).
The required space grows to .
To improve the time complexity, it is crucial to perform one step 
in  time.
To this end, we make use of a data-oblivious sorting algorithm. 
An algorithm is called data-oblivious if its sequence of 
possible memory accesses is independent of its input values.
There exist such sort algorithms working in  worst-case 
time (assuming that keys can be accessed in constant time), 
see~\cite{G2014} and references therein.

In our scenario, we compare the Parikh vectors of two factors 
of length  in  time 
and also collect all the positions , , 
at which the respective Parikh vectors have different values.
These positions are inserted in bulk into a balanced binary search 
tree , in  time.
Let the two factors be  and .
The next comparison concerns the factors of length : 
 and .
Their Parikh vectors can be obtained with updating only one counter 
in the previous vectors, which can also affect , 
as up to two elements should now be added to  
and up to two elements should be removed . 
The operations on , including finding its minimum 
(or finding out that  is empty), 
which immediately serves to resolve the factor comparison, 
take  time.
Similarly we handle the next pairs of factors, up to length .
Each time when equal (in the Abelian sense) factors are found 
and one of them is from  and the other from , 
we record their starting positions (in  or ) and length.
In this way, we cannot miss the longest Abelian matching factors.
Note that in a comparison based sort, 
and in particular in a deterministic data-oblivious sort,
it is impossible not to compare 
equal items at some moment, if such exist.
To see this, imagine that we associate a real number with 
each item according to the sorted order; that is, the smallest item 
will have the smallest number and the largest item the largest number, 
and equal items will have equal associated numbers.
Now, if two items,  and , are equal and no other item in the collection 
is equal to , not comparing  to  in the sorting process 
would mean that  and  are indistinguishable.
If, say, after the sorting  stands (just) before  
and imagine  is modified in such a way that its associated value gets 
greater by , where 
 is the minimum absolute difference between the associated values 
for any non-equal items in the collection, the hypothetical
sort algorithm not comparing  to 
would produce the same output as before, which of course means 
that the algorithm is incorrect.

One step of the presented sort algorithm takes  time, 
which sums up to  time over all steps, 
and the space usage is .
Note that a space-time tradeoff is obtained with  between 
 and .
For example, we can set , which gives 
 time and  space.
This time complexity is  when .



\subsection{Faster, sometimes}
\noindent 
In the algorithm above, the Parikh vectors of factors 
of length  were compared in  time.
Let us try to reduce this time, trying to obtain a better 
overall space-time tradeoff.

To this end, for each length  
we compute and store the Parikh vectors for 
factors of  and  sampled every -th position, 
where  will be chosen later.
Additionally, we compute the positions of the differences between each of the 
 pairs of Parikh vectors, storing them in a 
balanced binary search tree, as described in the previous subsection.
This requires overall  extra time 
and  extra space.
However, the ``main'' time component gets reduced to 
.
As we are interested in improving the space-time tradeoff, 
we need to check if  can be set to such value that 
the space complexity is not compromised, yet the time complexity 
improves, at least for some  and .
Clearly, it requires that , i.e., 
.
As only  may improve the time complexity, 
we need to have  (and of course ).
An extra requirement is .
Finally, improving the time complexity means that 
, which does not introduce 
an extra constaint since , given 
the aforementioned lower bound on .

We set .
This implies  and thus also 
, which eventually gives 
.

To sum up, if  and 
 but also ,
by choosing 
we preserve the  space and improve the time 
to .
In most cases the improvement is not large: 
for example, if  and , 
the time complexity is slashed by a factor of .
On the other hand, if e.g.  
and , then the time complexity 
becomes , 
an improvement by a factor of .


\section{Conclusions}
\noindent 
Finding the longest common Abelian factor is a recently posed problem, 
with a solution given in~\cite{AILR2015}, 
achieving  worst-case time and 
needing  words of space.
A significant weakness of that result is its space requirement, 
which may be unacceptable with a larger alphabet.
In this work we improve this result in two ways.

One algorithm keeps the time complexity of the previous result, 
while it reduces its space to .
This is obtained with very simple means (the key component is 
the LSD radix sort).
The other algorithm of ours increases the space to 
and achieves the time complexity of , 
where  is a freely chosen parameter.
When  it is always possible to 
choose such  that this algorithm beats the result from~\cite{AILR2015} 
in both time and space complexity.
This variant is also simple conceptually, yet it makes use 
of a deterministic data-oblivious sort algorithm of optimal complexity 
in the comparison based model.
There are several such algorithms known, but none of them is really simple. 
A more practical choice could be the textbook Shell sort algorithm 
with the sequence of gaps of the form , proposed 
by Pratt in 1972~\cite{P1972}.
Applying this Shell sort variant would deteriorate our time complexity 
by a factor of .
The latter of the two algorithms is also improved slightly 
for convenient values of  and .

We are convinced that better algorithms for the LCAF problem are possible.
One obvious line of attack is using word-level parallelism
(in the word-RAM model) for Parikh vector comparisons.
The anticipated speed-up factor is however only about , 
where  is the machine word size.
A more interesting question is whether sharing computations 
for different factor lengths could be exploited with a stronger effect 
than presented here.


\bibliographystyle{abbrv}
\bibliography{lcaf}

\end{document}
