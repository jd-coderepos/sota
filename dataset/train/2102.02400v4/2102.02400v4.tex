

\documentclass{article}

\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\usepackage{wrapfig}
\usepackage[utf8x]{inputenc} \DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\def\bs#1{\boldsymbol{#1}}

\def\nopo{P(\tilde{\bs{Y}}|X = \bs{x})}
\def\clpo{P(\bs{Y}|X = \bs{x})}

\def\nopos{P(\tilde{\bs{Y}}|X)}
\def\clpos{P(\bs{Y}|X)}
\def\nets{h_{\bs{\theta}}}

\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} 

\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{icml2021}

\icmltitlerunning{Provably End-to-end Label-noise Learning without Anchor Points}

\begin{document}

\twocolumn[
\icmltitle{Provably End-to-end Label-noise Learning without Anchor Points}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Xuefeng Li}{1,2}
\icmlauthor{Tongliang Liu}{2}
\icmlauthor{Bo Han}{3}
\icmlauthor{Gang Niu}{4}
\icmlauthor{Masashi Sugiyama}{4,5}
\end{icmlauthorlist}

\icmlaffiliation{1}{University of New South Wales}
\icmlaffiliation{2}{Trustworthy Machine Learning Lab, University of Sydney}
\icmlaffiliation{3}{Hong Kong Baptist University}
\icmlaffiliation{4}{RIKEN AIP}
\icmlaffiliation{5}{University of Tokyo}

\icmlcorrespondingauthor{Tongliang Liu}{tongliang.liu@sydney.edu.au}


\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]





\printAffiliationsAndNotice{} 


\begin{abstract}
In label-noise learning, the \emph{transition matrix} plays a key role in building \emph{statistically consistent} classifiers. Existing consistent estimators for the transition matrix have been developed by exploiting \textit{anchor points}. However, the anchor-point assumption is not always satisfied in real scenarios. In this paper, we propose an end-to-end framework for solving label-noise learning without anchor points, in which we simultaneously optimize two objectives: the cross entropy loss between the noisy label and the predicted probability by the neural network, and the volume of the simplex formed by the columns of the transition matrix.~Our proposed framework can identify the transition matrix if the clean class-posterior probabilities are \emph{sufficiently scattered}. This is by far the mildest assumption under which the transition matrix is provably \textit{identifiable} and the learned classifier is statistically consistent. Experimental results on benchmark datasets demonstrate the effectiveness and robustness of the proposed method.

\end{abstract}

\section{Introduction} \label{sec1}


The success of modern deep learning algorithms heavily relies on large-scale accurately annotated data \cite{daniely2019generalization, han2020survey, xia2020part, berthon2020confidence}. However, it is often expensive or even infeasible to annotate large datasets. Therefore, cheap but less accurate annotating methods have been widely used \cite{xiao2015learning, li2017webvision, han2020sigua, yu2020label, zhu2020second}. As a consequence, these alternatives inevitably introduce label noise. Training deep learning models on noisy data can significantly degenerate the test performance due to overfitting to the noisy labels \cite{arpit2017closer, zhang2016understanding, xia2021robust, wu2020class2simi}.




To mitigate the negative impacts of label noise, many methods have been developed and some of them are based on a loss correction procedure. In general, these methods are \textit{statistically consistent}, i.e., these methods guarantee that the classifier learned from the noisy data approaches to the optimal classifier defined on the clean risk as the size of the noisy training set increases \cite{liu2016classification,scott2015rate,natarajan2013learning,goldberger2016training,patrini2017making,thekumparampil2018robustness}. The idea is that the clean class-posterior  can be inferred by utilizing the noisy class-posterior  and the transition
matrix  where , i.e., . While those methods  theoretically guarantee the statistical consistency, they all heavily rely on the success of estimating transition matrices.






Generally, the transition matrix is unidentifiable without additional assumptions \cite{xia2019anchor}. In the literature, methods have been developed to estimate the transition matrices under the so-called \emph{anchor-point} assumption: it assumes the existence of anchor points, i.e., instances belonging to a specific class with probability one \cite{liu2016classification}. The assumption is reasonable in certain applications \cite{liu2016classification,patrini2017making}. However, the violation of the assumption in some cases could lead to a poorly learned transition matrix and a degenerated classifier \cite{xia2019anchor}. This motivates the development of algorithms without exploiting anchor points \cite{xia2019anchor, liu2020peer, xu2019l_dmi, zhu2021clusterability}. However, the performance is not theoretically guaranteed in these works. 



{\bf{Motivation}}. In this work, our interest lies in designing a consistent algorithm without anchor points, subject to class-dependent label noise, i.e.,  for any  in the feature space. Our algorithm is based on a geometric property of the label corruption process. Given an instance , the noisy class-posterior probability   can be thought of as a point in the -dimensional space where  is the number of classes. Since we have  and ,  is then a convex combination of the columns of . This means that the simplex  formed by the columns of  encloses  for any  \cite{boyd2004convex}. Thus, the problem of identifying the transition matrix can be treated as the problem of recovering . However, when no assumption has been made, the problem is ill-defined as  is not identifiable, i.e., there exists an infinite number of simplexes enclosing , and any of them can be regarded as the true simplex . It is apparent that under the anchor-point assumption,  can be uniquely determined by exploiting anchor points whose noisy class-posterior probabilities are the vertices of . The goal is thus to identify the points which have the largest noisy class-posterior probabilities for each class \cite{liu2016classification, patrini2017making}. However, if there are no anchor points, the identified points would not be the vertices of . In this case, existing methods cannot consistently estimate the transition matrices. To recover  without anchor points, a key observation is that, among all simplexes enclosing ,  is the one with \emph{minimum volume}. See Figure \ref{volmin_intuition} for a geometric illustration. This observation motivates the development of our method which incorporates the minimum volume constraint of  into label-noise learning.


To this end, we propose \textit{Volume Minimization Network} (VolMinNet) to consistently estimate the transition matrix and build a statistically consistent classifier. Specifically, VolMinNet consists of a classification network  and a trainable transition matrix .  We simultaneously optimize  and  with two objectives: i) the discrepancy between  and the noisy class-posterior distribution , ii) The volume of the simplex formed by the columns of . The proposed framework is end-to-end, and there is no need for identifying anchor points or \emph{pseudo anchor points} (i.e., instances belonging to a specific class with probability close to one) \cite{xia2019anchor}. Since our proposed method does not rely on any specific data points, it yields better noise robustness compared with existing methods. With a so-called \emph{sufficiently scattered assumption} where the clean class-posterior distribution is far from uniform, we theoretically prove that  will converge to the true transition matrix  while  converges to the clean class-posterior . We also prove that the anchor-point assumption is a special case of the sufficiently scattered assumption. 

The rest of this paper is organized as follows. In Section 2, we set up the notations and review the background of label-noise learning with anchor points. In Section 3, we introduce our proposed VolMinNet. In Section 4, we present the main theoretical results. In Section 5, we briefly introduce the related works in the literature. Experimental results on both synthetic and real-world datasets are provided in Section 6. Finally, we conclude the paper in Section 7.

\begin{figure}
\centering
\includegraphics[width=0.4\textwidth]{images/int2}
\caption{Geometric illustration of the problem of estimating the transition matrix without anchor points. The red triangle is the simplex of  with vertices denoted by . When there are no anchor points, the simplex found with existing methods (blue triangle) by using extreme-valued noisy class-posterior probabilities (see Eq. (\ref{anchor})) is not the true simplex (red triangle). It is obvious that among possible enclosing simplexes (black and red triangles), the true simplex  has the minimum volume.}
\label{volmin_intuition}
\vspace{-14pt}
\end{figure}




\section{Label-Noise Learning with Anchor Points}




In this section, we review the background of label-noise learning. We follow common notational conventions in the literature of label-noise learning.  and  denote a real-valued -dimensional vector and a real-valued  matrix, respectively. Elements of a vector are denoted by a subscript (e.g., ), while rows and columns of a matrix are denoted by  and  respectively. The th standard basis vector in  is denoted by . We denote the all-ones vector by , and  is the -dimensional simplex. In this work, we also make extensive use of convex analysis. Let a set  and the convex cone of  is denoted by . Similarly, the convex hull of  is defined as  . Specially, when  are affinely independent,  is also called a simplex which we denote it as .



Let   be the underlying distribution generating a pair of random variables  where  is the feature space,  is the label space and  is the number of classes. In many real-world applications, samples drawn from  are unavailable. Before being observed, labels of these samples are contaminated with noise and we obtain a set of corrupted data  where  is the noisy label and we denote by  the distribution of the noisy random pair . 




Given an instance  sampled from ,  is  derived from the random variable  through a noise transition matrix :

where  and  are the clean class-posterior probability and the noisy class-posterior probability, respectively. The -th entry of the transition matrix, i.e., , represents the probability that the instance  with the clean label  will have a noisy label . Generally, the transition matrix is \textit{non-identifiable} without any additional assumption \cite{xia2019anchor}. For example, we can decompose the transition matrix with . If we define , then  are both valid. Therefore, in this paper, we study the \textit{class-dependent} and \textit{instance-independent} transition matrix on which the majority of existing methods focus \cite{han2018co,han2018masking,patrini2017making,northcuttlearning,natarajan2013learning}. Formally, we have:

where the transition matrix  is now independent of the instance . In this work, we also assume that the true transition matrix  is \textit{diagonally dominant}\footnote{The definition of being diagonally dominant is different from the one in matrix analysis, but it has been commonly used in label-noise learning \cite{xu2019l_dmi}}. Specifically, the transition matrix  is diagonally dominant if for every column of , the
magnitude of the diagonal entry is larger than any non-diagonal entry, i.e.,   for any . This assumption has been commonly used in the literature of label-noise learning \cite{patrini2017making, xia2019anchor, yao2020dual}.

As in Eq.~(\ref{trans2}), the clean class-posterior probability  can be inferred by using the noisy class-posterior probability  and the transition matrix  as . For this reason, the transition matrix has been widely exploited to build statistically consistent classifiers, i.e., the learned classifier will converge to the optimal classifier defined with clean risk. Specifically, the transition matrix has been used to modify loss functions to build risk-consistent estimators \cite{goldberger2016training, patrini2017making,yu2018learning,xia2019anchor}, and has been used to correct hypotheses to build classifier-consistent algorithms \cite{natarajan2013learning, scott2015rate, patrini2017making}. Thus, the successes of these consistent algorithms rely on an accurately learned transition matrix. 

In recent years, considerable efforts have been invested in designing algorithms for estimating the transition matrix. These algorithms rely on a so-called \textit{anchor-point} assumption which requires that there exist anchor points for each class \cite{liu2016classification,xia2019anchor}. 



\begin{definition}[anchor-point assumption]
	For each class , there exists an instance  such that .  \end{definition}

 Under the anchor-point assumption, the task of estimating the transition matrix boils down to finding anchor points for each class. For example, given anchor points , we have 


Namely, the transition matrix can be obtained with the noisy class-posterior probabilities of anchor points. Assuming that we can accurately model the noisy class-posterior  given a sufficient number of noisy data, anchor points can be easily found as follows \cite{liu2016classification, patrini2017making}: 

However, when the anchor-point assumption is not satisfied, points found with Eq. (\ref{anchor}) are no longer anchor points. Hence, the above-mentioned method can not consistently estimate the transition matrix with Eq. (\ref{est}), which will lead to a statistically inconsistent classifier. This motivates us to design a statistically classifier-consistent algorithm which can consistently estimate the transition matrix without anchor points. 
 


\section{Volume Minimization Network}

\begin{figure*}[ht]
\begin{center}
	\includegraphics[width=150mm,height=45mm]{images/overview}
\end{center}
\caption{Overview of the proposed VolMinNet. The training in the proposed framework is carried out in an end-to-end manner with two objectives (red blocks) optimized simultaneously.}
\label{overview}
\end{figure*}

In this section, we propose a novel framework for label-noise learning which we call the Volume Minimization Network (VolMinNet). The proposed framework is end-to-end, and there is no need for identifying anchor points or a second stage for loss correction, resulting in better noise robustness than existing methods.

To learn the clean class-posterior , we define a transformation  where  is a differentiable function represented by a neural network with parameters . To estimate the transition matrix, we construct a trainable \textit{diagonally dominant column stochastic} matrix , i.e., ,   and  for any . To learn the noisy class posterior distribution from the noisy data, with some abuse of notation, we define the composition of  and  as .





Intuitively, as explained in Section \ref{sec1}, if  models  perfectly while the simplex of  has the minimum volume,  will converge to the true transition matrix  and  will converge to . This motivates us to propose the following criterion which corresponds to a constraint optimization problem:




where  is the set of diagonally dominant column stochastic matrices.  denotes a measure that is related or proportional to the volume of the simplex formed by the columns of  .

To solve criterion (\ref{cons_p}), we first note that the constraint  can be solved with expected risk minimization \cite{patrini2017making}. The risk is defined as , where  is a loss function and we use the cross-entropy loss throughout this paper. We can then re-write criterion (\ref{cons_p}) as a Lagrangian under the KKT condition \cite{karush1939minima, kuhn2014nonlinear} to obtain:

where  is the KKT multiplier. In the literature, various functions for measuring the volume of the simplex have been investigated \cite{fu2015blind, li2008minimum, miao2007endmember}. Given  is a square matrix, a common choice is , where  denotes the determinant. However, this function is numerically unstable for optimization and computationally hard to deal with. Hence, we adopt another popular alternative . This function has been widely used in low-rank matrix recovery and non-negative matrix decomposition \cite{fazel2003log, liu2012robust, fu2016robust}. Besides, since we only have access to a set of noisy training examples  instead of the distribution , we employ the empirical risk for training. Formally, we propose the following objective function: 

where  is a regularization coefficient that balances distribution fidelity versus volume minimization. 

The problem remains how to design  so that it is differentiable, diagonally dominant and column stochastic. Specifically, we first create a matrix  so that diagonal elements  for all , and all other elements  for all  where  is the sigmoid function  and each  is a real-valued variable which will be updated throughout training. Then we do the normalization  so that the sum of each column of  is equal to one. Since the sigmoid function returns a value  in the range 0 to 1, we have  for all . With this specially designed , we ensure that , i.e.,  is a diagonally dominant and column stochastic matrix. In addition,  is differentiable because the sigmoid function and the normalization operation are differentiable.

With both  and  being differentiable, the objective in Eq. (\ref{obj}) can be easily optimized with any standard gradient-based learning rule. This allows us to replace the two-stage loss correction procedure in existing works with an end-to-end learning framework. See Figure \ref{overview} for a less formal, more pedagogical explanation of our proposed learning framework.
 


\section{Theoretical Results}



In this section, we show that criterion (\ref{cons_p}) guarantees the consistency of the estimated transition matrix and the learned classifier under the sufficiently scattered assumption. We also show that the anchor-point assumption is a special case of the sufficiently assumption. To explain, we give a formal definition of the sufficiently scattered assumption:


\begin{figure*}[ht]
\begin{center}
	\includegraphics[width=150mm,height=41mm]{images/sc}
\end{center} 
	\caption{Illustration of the anchor-point assumption and the sufficiently scattered assumption in the case of  by assuming that the viewer are facing the hyperplane  from the positive orthant. The dots are class-posterior probabilities ; the triangle is the non-negative orthant; the inner circle is ; the region encompassed by red lines is . Clearly, the anchor-point assumption (a) is a special case of the sufficiently scattered assumption (b).}
	\label{sc}
\end{figure*}
\begin{definition}[Sufficiently Scattered] The clean class-posterior  is said to be sufficiently scattered if there exists a set  such that the matrix  satisfies (1)  \; where  and  denotes the convex cone formed by columns of . (2) , \;for any unitary matrix  that is not a permutation matrix. 
	\label{def:sc}
\end{definition}



This assumption is evolved from previous works in non-negative matrix decomposition \cite{fu2015blind, fu2018identifiability} with necessary modification. Intuitively, in the case of ,  corresponds to a ``ball'' tangential to the triangle formed by a permutation matrix, e.g., .  is the polytope inside this triangle. Columns of  also form triangles which are rotated versions of the triangle defined by permutation matrices; facets of those triangles are also tangential to . Condition (1) of the sufficiently scattered assumption requires that  is enclosed by , i.e.,  is a subset of . Condition (2) ensures that given condition (1),  is enclosed by the triangle formed by a permutation matrix and not any other unitary matrix. 

To understand the sufficiently scattered assumption and its relationship with the anchor-point assumption, we provide several examples in Figure \ref{sc}.  In Figure \ref{sc}.(a), we show a situation where both the anchor-point assumption and sufficiently scattered assumption are satisfied. Figure \ref{sc}.(b) shows a situation where the sufficiently scattered assumption is satisfied while the anchor-point assumption is violated. In Figure \ref{sc}.(c) and \ref{sc}.(d), both assumptions are violated. However, in Figure \ref{sc}.(d), only condition (2) of the sufficiently scattered assumption is violated while both conditions of the sufficiently scattered assumption are violated in \ref{sc}.(c).

The first observation is that if the anchor-point assumption is satisfied, then the sufficiently scattered assumption must hold, but not vice versa. Intuitively, if the anchor-point assumption is satisfied, then there exists a matrix  where  are anchor points for different classes and  is the identity matrix. From Figure \ref{sc}.(a) , it is clear that  and  can only be enclosed by the convex cone of permutation matrices. This shows that the sufficiently scattered assumption is satisfied. However, from Figure \ref{sc}.(b), it is clear that the sufficiently scattered assumption is satisfied but not the anchor-point assumption.  Formally, we show that:

\begin{proposition}
	The anchor-point assumption is a sufficient but not necessary condition for the sufficiently scattered assumption when .
	\label{prop}
\end{proposition}





The proof of Proposition \ref{prop} is included in the supplementary
material. Proposition \ref{prop} implies that the anchor-point assumption is a special case of the sufficiently scattered assumption. This means that the proposed framework can also deal with the case where the anchor-point assumption holds. Under the sufficiently scattered assumption, we get our main result:
\begin{theorem}\label{t2}
Given sufficiently many noisy data, if  is sufficiently scattered, then  and  must hold, where  are optimal solutions of Eq. (\ref{cons_p}).
\label{theorem1}
\end{theorem} 

The proof of Theorem \ref{theorem1} can be found in the supplementary
material. Intuitively, if  is sufficiently scattered, the noisy class-posterior  will be sufficiently spread in the simplex formed by the columns of . 
Then, finding the minimum-volume data-enclosing convex hull of  recovers the ground-truth  and . 



\section{Related Works}
In this section, we review existing methods in label-noise learning. Based on the statistical consistency of the learned classifier, we divided
exsisting methods for label-noise learning into two categories: heuristic algorithms and statistically consistent algorithms. 

Methods in the first category focus on employing heuristics to reduce the side-effect of noisy labels. For example, many methods use a specially designed strategy to select reliable samples \cite{yu2019does,han2018co,malach2017decoupling,ren2018learning,jiang2018mentornet, yao2020searching} or correct labels \cite{ma2018dimensionality,kremer2018robust,tanaka2018joint,reed2014training}. Although those methods empirically work well, there is not any theoretical guarantee on the consistency of the learned classifiers from all these methods. 




Statistically consistent algorithms are primarily developed based on a loss correction procedure \cite{ liu2016classification, patrini2017making, zhang2018generalized}. For these methods, the noise transition matrix plays a key role in building consistent classifiers. For example, Patrini et al.\yrcite{patrini2017making} leveraged a two-stage training procedure of first estimating the noise transition matrix and then use it to modify the loss to ensure risk consistency. These works rely on anchor points or instances belonging to a specific class with probability one or approximately one. When there are no anchor points in datasets or data distributions, all the aforementioned methods cannot guarantee the statistical consistency. Another approach is to jointly learn the noise transition matrix and classifier. For instance, on top of the softmax layer of the classification network \cite{goldberger2016training}, a constrained linear layer or a nonlinear softmax layer is added to model the noise transition matrix \cite{sukhbaatar2015training}. Zhang et al. \yrcite{zhang2021confidence} concurrently propose a one-step method for the label-noise learning problem and a derivative-free method for estimating the transition matrix. Specifically, their method uses a total variation regularization term to prevent the overconfidence problem of the neural network, which leads to a more accurate noisy class-posterior. However, the anchor-point assumption is still needed for their method. Based on different motivations, assumptions and learning objectives, their method achieves different theoretical results compared with our proposed method. Learning with noisy labels are also closely related to learning with complementary labels where instead of noisy labels, only compelementary labels are given for training \cite{yu2018learning, chou2020unbiased, feng2020learning}. 

Recently, some methods exploiting semi-supervised learning techniques have been proposed to solve the label-noise learning problem like SELF \cite{nguyen2019self} and DivideMix \cite{li2019dividemix}. These methods are aggregations of multiple techniques such as augmentations and multiple networks. Noise robustness is significantly improved with these methods. However, these methods are sensitive to the choice of hyperparameters and changes in data and noise types would generate degenerated classifiers. In addition, the computational cost of these methods increases significantly compared with previous methods. 



\section{Experiments}


   \begin{figure*}
     \centering
     
     \includegraphics[width=0.24\textwidth]{images/figs/mnist_symmetric_0.2.png}
     \includegraphics[width=0.24\textwidth]{images/figs/mnist_symmetric_0.5.png}   
     \includegraphics[width=0.24\textwidth]{images/figs/mnist_flip_0.2.png}
     \includegraphics[width=0.24\textwidth]{images/figs/mnist_flip_0.45.png}
\vspace{5mm}

     \includegraphics[width=0.24\textwidth]{images/figs/cifar10_symmetric_0.2.png}
     \includegraphics[width=0.24\textwidth]{images/figs/cifar10_symmetric_0.5.png}
     \includegraphics[width=0.24\textwidth]{images/figs/cifar10_flip_0.2.png}
     \includegraphics[width=0.24\textwidth]{images/figs/cifar10_flip_0.45.png}
     
   \vspace{5mm}
  
     \includegraphics[width=0.24\textwidth]{images/figs/cifar100_symmetric_0.2.png}
     \includegraphics[width=0.24\textwidth]{images/figs/cifar100_symmetric_0.5.png}
     \includegraphics[width=0.24\textwidth]{images/figs/cifar100_flip_0.2.png}
     \includegraphics[width=0.24\textwidth]{images/figs/cifar100_flip_0.45.png}

\vspace{5mm}

    
     \caption{Transition matrix estimation error on MNIST, CIFAR-10 and CIFAR-100. The error bar for the standard deviation in each figure has been shaded. The lower the better.}
     \label{fig:est_error}
 \end{figure*}
 
In this section, we verify the robustness of the proposed volume minimization network (VolMinNet) from two folds: the estimation error of the transition matrix and the classification accuracy.

\textbf{Datasets} We evaluate the proposed method on three synthetic noisy datasets, i.e.,
MNIST, CIFAR-10 and CIFAR-100 and one real-world noisy dataset, i.e., clothing1M. We leave out 10\% of the training examples as the validation set. The three synthetic datasets contain clean data. We corrupted the training and validation sets manually according to transition matrices. Specifically, we conduct experiments with two commonly used types of noise: (1) Symmetry flipping \cite{patrini2017making}; (2) Pair flipping  \cite{han2018co}. We report both the classification accuracy on the test set and the estimation error between the estimated transition matrix  and the true transition matrix . All experiments are repeated five times on all datasets. Following T-Revision \cite{xia2019anchor}, we also conducted experiments on datasets where possible anchor points are removed from the datasets. The details and more experimental results can be found in the Supplementary Material.



Clothing1M is a real-world noisy dataset which consists of  images with real-world noisy labels. Existing methods like Forward \cite{patrini2017making} and T-revision \cite{xia2019anchor} use the additional 50k clean training data to help initialize the transition matrix and validate on 14k clean validation data. Here we use another setting which is also commonly used in the literature \cite{xia2020part}. We only exploit the  data for both transition matrix estimation and classification training. Specifically, we leave out  of the noisy training examples as a noisy validation set for model selection. We think this setting is more natural considering that it does not require any clean data. All results of baseline methods are quoted from PTD \cite{xia2020part} as we have the same setting. 

\textbf{Network structure and optimization} For a fair comparison, we implement all methods with default parameters by PyTorch on Tesla V100-SXM2. For MNIST, we use a LeNet-5 network. SGD is used to train the classification network  with batch size , momentum , weight decay  and a learning rate . Adam with default parameters is used to train the transition matrix . The algorithm is run for  epoch. For CIFAR10, we use a ResNet-18 network. SGD is used to train both the classification network  and the transition matrix  with batch size , momentum , weight decay  and an initial learning rate . The algorithm is run for  epoch and the learning rate is divided by  after the th and th epoch. For CIFAR100, we use a ResNet-32 network. SGD is used to train the classification network  with batch size , momentum , weight decay  and an initial learning rate . Adam with default parameters is used to train the transition matrix . The algorithm is run for  epoch and the learning rate is divided by  after the th and th epoch. For CIFAR-10 and CIFAR-100, we perform data augmentation by horizontal random flips and  random crops after padding 4 pixels on each side. For clothing1M, we use a ResNet-50 pre-trained on ImageNet. We only use the 1M noisy data to train and validate the network. For the optimization, SGD is used train both the classification network  and the transition matrix  with momentum 0.9, weight decay , batch size 32, and run with learning rates  and  for 5 epochs each. For each epoch, we ensure the noisy labels for each class are balanced with undersampling. Throughout all experiments, we fixed  and the trainable weights  of  are initialized with  (roughly -2 for MNIST and CIFAR10, -4.5 for CIFAR100 and -2.5 for clothing1M).



\begin{table*}
    \centering
    \scalebox{1}{
\begin{tabular}{ccccccc}

\hline  
& \multicolumn{2}{c} {MNIST} & \multicolumn{2}{c} {CIFAR-10} & \multicolumn{2}{c} {CIFAR-100} \\
& Sym-20\% & Sym-50\% & Sym-20\% & Sym-50\% & Sym-20\% & Sym-50\% \\
\hline
Decoupling &  &  &  &  &  &  \\
MentorNet &  &  &  &  &  &  \\
Co-teaching &  &  &  &  &  &  \\
Forward &  &  &  &  &  &  \\
T-Revision &  &  &  &  &  &  \\
DMI  &  &  &  &  &  & \\
Dual T &  &  &  &  &  & \\
VolMinNet &  &  &  &  & &  \\

\hline
\end{tabular}
}
    \centering
    \scalebox{1}{
\begin{tabular}{ccccccc}


& \multicolumn{2}{c} {MNIST} & \multicolumn{2}{c} {CIFAR-10} & \multicolumn{2}{c} {CIFAR-100} \\
& Pair-20\% & Pair-45\% &Pair-20\% & Pair-45\% & Pair-20\%& Pair-45\% \\
\hline
Decoupling &  &  &  &  &  &  \\
MentorNet &  &  &  &  &  &  \\
Co-teaching &  &  &  &  &  &  \\
Forward &  &  &  &  &  &  \\
T-Revision &  &  &  &  &  & \\

DMI  &  &  &  &  &  & \\
Dual T &  &  &  &  &  & \\
VolMinNet &   &  &  &  &  &  \\
\hline
\end{tabular}
}


	\caption{Classification accuracy (percentage) on MNIST, CIFAR-10 and CIFAR-100.}
	\label{table:accs}
\end{table*}


\subsection{Transition Matrix Estimation} \label{exp:est}
For evaluating the effectiveness of estimating the transition matrix, we compare the proposed method with the following methods: (1) T-estimator max \cite{patrini2017making}, which identify the extreme-valued noisy class-posterior probabilities from given samples to estimate the transition matrix. (2) T-estimator 3\% \cite{patrini2017making}, which takes a -percentile in place of the argmax of Equation \ref{anchor}.
(3) T-Revision \cite{xia2019anchor}, which introduces a slack variable to revise the noise transition matrix after initializing the transition matrix with T-estimator. (4) Dual T-estimator \cite{yao2020dual}, which introduces an intermediate class to avoid directly estimating the noisy class-posterior and factorizes the transition matrix into the product of two easy-to-estimate transition matrices.



To show that the proposed method is more robust in estimating the transition matrix, we plot the estimation error for the transition matrix, i.e., . Figure~\ref{fig:est_error} depicts estimation errors of transition matrices estimated by the proposed VolMinNet and other baseline methods. For all different settings of noise on three different datasets (original intact datasets), VolMinNet consistently gives better results compared to the baselines, which shows its superior robustness against label noise. For example, on CIFAR100 (Flip-0.45), our method achieves estimation error around 0.25, while baseline methods can only reach at around 0.75. These results show that our method establishes the new state of the art in estimating transition matrices. 





\subsection{Classification accuracy Evaluation}


We compare the classification accuracy of the proposed method with the following methods: (1) Decoupling \cite{malach2017decoupling}. (2) MentorNet \cite{jiang2018mentornet}. (3) Co-teaching \cite{han2018co}. (4) Forward \cite{patrini2017making}. (5) T-Revision \cite{xia2019anchor}. (7) DMI \cite{xu2019l_dmi}. (8) Dual T \cite{yao2020dual}. Note that we did not compare the proposed method with some methods like SELF \cite{nguyen2019self} and DivideMix \cite{li2019dividemix}. This is because these methods are aggregations of semi-supervised learning techniques which have high computational complexity and are sensitive to the choice of hyperparameters. In this work, we are more focusing on solving the label noise learning without anchor points theoretically.


In Table~\ref{table:accs}, we present the classification accuracy by the proposed VolMinNet and baseline methods on synthetic noisy datasets. VolMinNet outperforms baseline methods on almost all settings of noise. This result is natural after we have shown that VolMinNet leads to smaller estimation error of the transition matrix compared with baseline methods. While the differences of accuracy among different methods are marginal for symmetric noise,  
VolMinNet outperforms baselines by over  with Pair- noise and has much smaller standard deviations. These results show the clear advantage of the proposed VolMinNet. It has better robustness to different settings of noise and datasets compared to baseline methods.


\begin{table}
\centering

\caption{Classification accuracy (percentage) on Clothing1M. Only noisy data are exploited for training and validation.}
\label{clothing}
\end{table}

Finally, we show the results on Clothing1M in Table \ref{clothing}. As explained in the previous section,  Forward and T-Revision exploited the 50k clean data and their noisy versions in 1M noisy data to help initialize the noise transition matrix, which is not practical in real-world settings. For a fair comparison, we report results by only using the  noisy data to train and validate the network. As shown in Table \ref{clothing}, our method outperforms previous transition matrix based methods and heuristic methods on the Clothing1M dataset. In addition, the performance on the Clothing1M dataset shows that the proposed method has certain robustness against instance-dependent noise as well.


\section{Discussion and Conclusion}

In this paper, we considered the problem of label-noise learning without anchor points. We relax the anchor-point assumption with our proposed VolMinNet. The consistency of the estimated transition matrix and the learned classifier are theoretically proved under the sufficiently scattered assumption. Experimental results have demonstrated the robustness of the proposed VolMinNet. Future work should focus on improving the estimation of the noisy class posterior which we believe is the bottleneck of our method. 

\section*{Acknowledgements}
XL was supported by an Australian Government RTP Scholarship. TL was supported by Australian Research Council Project DE-190101473. BH was supported by the RGC Early Career Scheme No. 22200720, NSFC Young Scientists Fund No. 62006202 and HKBU CSD Departmental Incentive Grant. GN and MS were supported by JST AIP Acceleration Research Grant Number JPMJCR20U3, Japan. MS was also supported by Institute for AI and Beyond, UTokyo. Authors also thank for the help from Dr. Alan Blair, Kevin Lam, Yivan Zhang and members of the Trustworthy Machine Learning Lab at the University of Sydney. 





\bibliography{icml}
\bibliographystyle{icml2021}

\clearpage

\appendix
\section{Appendix}
\subsection{Proof of Proposition 1}
To prove proposition 1, we first show that the anchor-point assumption is a sufficient condition for the sufficiently scattered assumption. In other words, we need to show that if the anchor-point assumption is satisfied, then two conditions of the sufficiently scattered assumption must hold.

We start with condition (2) of the sufficiently scattered assumption. We need to show that if the anchor-point assumption is hold, then there exists a set  such that the matrix  satisfies that , \;for any unitary matrix  that is not a permutation matrix. 

Since the anchor-point assumption is satisfied, then there exists a matrix   where  are anchor points for each class. From the definition of anchor points, we have . This implies that
 


where  is the identity matrix. By the definition of the identity matrix , it is clear that , \;for any unitary matrix  that is not a permutation matrix. This shows that condition (2) of the sufficiently scattered assumption is satisfied if the anchor-point assumption is hold. 



Next, we show that condition (1) will also be satisfied, i.e., the convex cone  where . By Eq. (\ref{prop_eq:1}), condition (1) of Theorem 1 is equivalent to 



This means that all elements in  must be in the non-negative orthant of , i.e., for all ,  for all . Consider  and let  be the normalized vector of , by definition of  we have the following chain:


	\boldsymbol{v}^{\top} \boldsymbol{1} &\geq \sqrt{C-1}\|\boldsymbol{v}\|_{2},\\
	\frac{\boldsymbol{v}^{\top}}{\|\boldsymbol{v}\|} \boldsymbol{1} = \hat{\bs{v}}^{\top}\bs{1} &\geq \sqrt{C-1},\\
	\sum_{i \in \{1,2,\ldots,C\}} \hat{\bs{v}}_i &\geq \sqrt{C-1}.


To show  is non-negative is equivalent to prove that  is non-negative, i.e.,  , . Let  be the vector which has same elements with  except that the th element  is removed. Following Eq. \ref{equation:10}, we have:


	\;\hat{\bs{v}}_k &\geq \sqrt{C-1} - \sum_{i \in \{1,2,\ldots,C\} \setminus \{k\}}\hat{\bs{v}}_i,\\
	\hat{\bs{v}}_k &\geq \sqrt{C-1} - \bs{u}^{\top}\bs{1}.


By the Cauchy-Schwarz inequality, we get the following inequality:



Then by the definition of  and , we have  and . Combined this with Eq. \ref{equation:11} and Eq. \ref{equation:12}, we get:



This simply implies that  for all  and we have proved that the anchor-point assumption is a sufficient condition of the sufficiently scattered assumption. 

We now prove that the anchor-point assumption is not a necessary condition for the sufficiently scattered assumption. Suppose  has the property that  which means that the anchor-point assumption is not satisfied. We also assume that there exist a set  such that  covers the whole non-negative orthant except the area along each axis (area formed by noisy class-posterior of anchor points). Since these areas along each axis are not part of  when , it is clear that condition (1) of the sufficiently scattered assumption is satisfied. Besides, by definition of , there is no other unitary matrix which can cover  except permutation matrices. This shows that condition (2) of the sufficiently scattered assumption is also satisfied and the proof is completed. \qed


  
\subsection{Proof of Theorem 1}
The insights of our proof are from previous works in non-negative matrix factorisation \cite{fu2015blind}. To proceed, let us first introduce following classic lemmas in convex analysis:

\begin{lemma}
If  and  are convex cones and  then, . 
\label{lemma1}
\end{lemma}

\begin{lemma} 
If  is invertible, then .
\label{lemma2}
\end{lemma}

Readers are referred to Boyd et al.\yrcite{boyd2004convex} for details.
Our purpose is to show that criterion (5) has unique solutions which are the ground-truth  and . To this end, let us denote  as a feasible solution of Criterion (5), i.e.,



As defined in sufficient scattered assumption, we have the matrix  defined on the set . Let , it follows that


Note that both  and  have full rank because they are diagonally dominant square matrices by definition. In addition, since the sufficiently scattered assumption is satisfied,  also holds \cite{fu2015blind}. Therefore, there exists an invertible matrix  such that


where  and   is the pseudo-inverse of .

Since  and  by definition, we get


Let  which by definition takes the form 
for some  Using  can be expressed as  where . This implies that  also lies in , i.e. .

Recall Condition (1) of the sufficiently scattered assumption, i.e.,  where 
 It
implies


By applying Lemmas (\ref{lemma1}-\ref{lemma2}) to Eq. (\ref{eq.12}), we have

where  is the dual cone of  which can be shown to be


Then we have the following inequalities:

|det(\bs{A})| & \leq \prod_{i=1}^{C}\|\bs{A}_{:, i}\|_{2} \label{sub2}\\
& \leq \prod_{i=1}^{C} \mathbf{1}^{\top}\bs{A}_{:, i} \label{sub3}\\
& \leq(\frac{\sum_{i=1}^{C} \mathbf{1}^{\top}\bs{A}_{:, i}}{C})^{C} \label{sub4}\\
&=(\frac{\mathbf{1}^{\top} \bs{A} \mathbf{1}}{C})^{C}=1, \label{sub5}



where (\ref{sub2}) is Hadamard's inequality; (\ref{sub3}) is by Eq. (\ref{13}); (\ref{sub4}) is by the arithmetic-geometric mean inequality; and (\ref{sub5}) is by Eq. (\ref{eq.11}). 

Note that  and  from properties of the determinant, it follows from Eq.~(\ref{15}) that . We also know that  must hold from Criterion (5), hence we have 


By Hadamard's inequality, the equality in (\ref{sub2}) holds only if  is column-orthogonal, which is equivalent to that  is column-orthogonal. Considering condition (2) in the definition of sufficiently scattered and the property of  that , the only possible choices of column-orthogonal  are



where  is any permutation matrix and 
is any diagonal matrix with non-zero diagonals. By Eq. (\ref{eq.11}), we must have  I. Subsequently, we are left with , or equivalently,  Since  and  are both diagonal dominant, the only possible permutation matrix is , which means  holds. By Eq. (\ref{eq.8}), it follows that . Hence we conclude that   is the unique optimal solution to criterion (5).  \qed

  \begin{figure*}
    
     \includegraphics[width=0.24\textwidth]{images/figs/no_anchor/mnist_symmetric_0.2.png}
     \includegraphics[width=0.24\textwidth]{images/figs/no_anchor/mnist_symmetric_0.5.png}   
     \includegraphics[width=0.24\textwidth]{images/figs/no_anchor/mnist_flip_0.2.png}
     \includegraphics[width=0.24\textwidth]{images/figs/no_anchor/mnist_flip_0.45.png}
     \vspace{5mm}

     \includegraphics[width=0.24\textwidth]{images/figs/no_anchor/cifar10_symmetric_0.2.png}
     \includegraphics[width=0.24\textwidth]{images/figs/no_anchor/cifar10_symmetric_0.5.png}
     \includegraphics[width=0.24\textwidth]{images/figs/no_anchor/cifar10_flip_0.2.png}
     \includegraphics[width=0.24\textwidth]{images/figs/no_anchor/cifar10_flip_0.45.png}
     
     \vspace{5mm}

     \includegraphics[width=0.24\textwidth]{images/figs/no_anchor/cifar100_symmetric_0.2.png}
     \includegraphics[width=0.24\textwidth]{images/figs/no_anchor/cifar100_symmetric_0.5.png}
     \includegraphics[width=0.24\textwidth]{images/figs/no_anchor/cifar100_flip_0.2.png}
     \includegraphics[width=0.24\textwidth]{images/figs/no_anchor/cifar100_flip_0.45.png}
    
     \caption{Transition matrix estimation error on MNIST/NA, CIFAR-10/NA, CIFAR-100/NA. Datasets with ``/NA'' means that possible anchor points are removed. The error bar for the standard deviation in each figure has been shaded. The lower the better.}
     \label{fig:na_est_error}
 \end{figure*}


\section{Experiments on datasets where possible anchor points are manually removed.}


 \begin{table*}

    \centering
    \scalebox{1}{
\begin{tabular}{ccccccc}

\hline  
& \multicolumn{2}{c} {MNIST/NA} & \multicolumn{2}{c} {CIFAR-10/NA} & \multicolumn{2}{c} {CIFAR-100/NA} \\
& Sym-20\% & Sym-50\% & Sym-20\% & Sym-50\% & Sym-20\% & Sym-50\% \\
\hline
Decoupling &  &  &  &  &  &  \\
MentorNet &  &  &  &  &  &  \\
Co-teaching &  &  &  &  &  & \\
Forward &  &  &  &  &  &   \\
T-Revision &  &  &  &  &  &  \\
DMI  &  &  &  &  &  & \\
Dual T &  &  &  &  &  & \\
VolMinNet &  &  &  &  &  &  \\
\hline
\end{tabular}
}
    \centering
    \scalebox{1}{
\begin{tabular}{ccccccc}


&\multicolumn{2}{c} {MNIST/NA}  &\multicolumn{2}{c} {CIFAR-10/NA} & \multicolumn{2}{c} {CIFAR-100/NA} \\
& Pair-20\% & Pair-45\% &Pair-20\% & Pair-45\% & Pair-20\%& Pair-45\% \\
\hline
Decoupling &  &  &  &  &  &  \\
MentorNet &  &  &  &  &  & \\
Co-teaching &  &  &  &  &  &  \\
Forward &  &  &  &  &  &  \\
T-Revision &  &  &  &  &  & \\
DMI  &  &  &  &  &  & \\
Dual T &  &  &  &  &  & \\
VolMinNet &  &  &  &  &  &   \\
\hline
\end{tabular}
}

	\caption{Classification accuracy (percentage) on MNIST, CIFAR-10,CIFAR-100 and MNIST/NA, CIFAR-10/NA, CIFAR-100/NA. Datasets with ``/NA'' means that possible anchor points are removed.}
	\label{table:na_accs}
\end{table*}

Following Xia et al.\yrcite{xia2019anchor}, to show the importance of anchor points, we remove possible anchor points from the datasets, i.e., instances with large estimated class-posterior probability  before corrupting the training and validation sets. For MNIST we removed  of the instances with the largest estimated class posterior probabilities in each class. For CIFAR-10 and CIFAR-100, we removed  of the instances with the largest estimated class posterior probabilities in each class. We add "/NA" following the dataset's name denote those datasets which are modified by removing possible anchor points. The detailed experimental results are shown in Figure \ref{fig:na_est_error} (estimation error) and Table \ref{table:na_accs} (classification accuracy). The experimental performance shows that our proposed method outperforms the baseline methods.
 
 



\end{document}
