\documentclass{article} \usepackage{iclr2017_conference,times}

\usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{setspace}

\usepackage{qiangstyle}
\usepackage{url}
\newcommand{\dilinfig}{./figures}
\newcommand{\dilincheck}[1]{#1}



\title{Learning to Draw Samples: With Application to Amortized MLE for Generative Adversarial Learning} 

\author{Dilin Wang, ~~Qiang Liu\\
Department of Computer Science, Dartmouth College\\
\texttt{\{dilin.wang.gr, qiang.liu\}@dartmouth.edu}
}







\begin{document}

\maketitle


\begin{abstract}
We propose a simple algorithm to train stochastic 
neural networks 
to draw samples from given target distributions for probabilistic inference. 
Our method is based on iteratively adjusting the neural network parameters 
so that the output changes along a Stein variational gradient \citep{liu2016stein} that maximumly decreases 
the KL divergence with the target distribution. 
Our method works for any target distribution specified by their unnormalized density function, 
and can train any black-box architectures that are differentiable in terms of the parameters we want to adapt. 
As an application of our method, we propose an  \emph{amortized MLE} algorithm for training deep energy model, where a neural sampler is adaptively trained to 
approximate the likelihood function. Our method mimics an adversarial game 
between the deep energy model and the neural sampler, and obtains realistic-looking images competitive with the state-of-the-art results. 
\end{abstract}



\section{Introduction}


Modern machine learning increasingly relies on highly complex probabilistic models to reason about uncertainty.  
A key computational challenge is to develop efficient inference techniques to approximate, or draw samples from complex distributions. 
Currently, most inference methods, including MCMC and variational inference, are hand-designed by researchers or domain experts. 
This makes it difficult to fully optimize the choice of different methods and their parameters, and exploit the structures in the problems of interest in an automatic way. 
The hand-designed algorithm can also be inefficient when it requires to make fast inference repeatedly on a large number of different distributions with similar structures. 
This happens, for example, when we need to reason about a number of observed datasets in settings like online learning, 
or need fast inference as inner loops for other algorithms such as maximum likelihood training. 
Therefore, it is highly desirable to develop more intelligent probabilistic inference systems that can adaptively improve its own performance to fully the optimize computational efficiency, and generalize to new tasks with similar structures. 

Specifically, denote by  a probability density of interest specified up to the normalization constant, which we want to draw sample from, or marginalize to estimate its normalization constant. We want to study the following problem: 

\begin{problem}\label{pro:prob1}
Given a distribution with density  and a function  with parameter  and random input , 
for which we only have assess to draws of the random input  (without knowing its true distribution ), 
and the output values of  and its derivative  given  and .  
We want to find an optimal parameter  so that the density of the random output variable  with  closely matches the target density . 
\end{problem}

Because we have no assumption on the structure of  
and the distribution of random input,
we can not directly calculate the actual distribution of the output random variable ; 
this makes it difficult to solve Problem~\ref{pro:prob1} using the traditional variational inference (VI) methods. 
Recall that traditional VI approximates  using simple proposal distributions  indexed by parameter , 
and finds the optimal  by minimizing KL divergence , 
which requires to calculate the density  or its derivative that is not computable by our assumption 
(even when the Monte Carlo gradient estimation and the reparametrization trick \citep{kingma2013auto} are applied).  


\begin{comment}
\begin{figure}[t]
   \centering
   \scalebox{1}{
   \includegraphics[width=.6\textwidth]{figures/neuralsampler1}}
   \begin{picture}(0,0)(0,0)
   \put(-250,-10){\figcapsize \emph{Given distribution}}
      \put(-170,-10){{\figcapsize \emph{Black-box neural sampler}}}
\put(-50,-10){\figcapsize \emph{Samples}}
   \end{picture}\\label{equ:xxii}
x_i  \gets x_i +  \epsilon \ff(x_i),  ~~~~ \forall i = 1, \ldots, n,  
\label{equ:ff00}
\ff =   \argmax_{\ff \in \F}  \bigg\{  -   \frac{d}{d\epsilon} \KL(q_{[\epsilon\ff]} ~|| ~ p) \big |_{\epsilon = 0}  \bigg\}, 
\label{equ:klstein00}
&- \frac{d}{d\epsilon} \KL(q_{[\epsilon\ff]} ~|| ~ p) \big |_{\epsilon = 0}  = \E_{x\sim q}[\sumstein_p \ff(x)], \
where  is considered as a linear operator acting on function  and is called the Stein operator in connection with Stein's identity which shows that 
the RHS of \eqref{equ:klstein00} equals zero if : 

This is a result of integration by parts assuming the value of  vanishes on the boundary of the integration domain.   


Therefore, the optimization in \eqref{equ:ff00} reduces to 

where  is the kernelized Stein discrepancy defined in \citet{liu2016kernelized}, which equals zero if and only if  under mild regularity conditions. 
Importantly, the optimal solution of \eqref{equ:ksd} yields a closed form
By approximating the expectation under  with the empirical average of the current particles ,  SVGD admits a simple form of update: 

and . 
The two terms in  play two different roles: 
the term with the gradient  drives the particles toward the high probability regions of , 
while the term with  serves as a repulsive force to encourage diversity;
to see this, consider a stationary kernel , then the second term reduces to , 
which can be treated as the negative gradient for minimizing the average similarity  in terms of . 
Overall, this particle update produces diverse points for distributional approximation and uncertainty assessment, and 
also has an interesting ``momentum'' effect in which the particles move collaboratively to escape the local optima.


It is easy to see from \eqref{equ:update11} that  reduces to the typical gradient  when there is only a single particle () and  when ,  
in which case SVGD reduces to the standard gradient ascent for maximizing  (i.e., maximum \emph{a posteriori} (MAP)). 



\begin{algorithm}[t]                      \caption{Amortized SVGD for Problem~\ref{pro:prob1}}\label{alg:alg1}                           \begin{algorithmic}                    \STATE Set batch size , step-size scheme  and kernel . Initialize . 
\FOR {iteration }
\STATE  Draw random , calculate , 
and the Stein variational gradient  in \eqref{equ:update11}. 
\STATE  Update parameter  using \eqref{equ:follow1}, \eqref{equ:follow2} or \eqref{equ:follow3}. 
\ENDFOR
\end{algorithmic}
\end{algorithm}

\section{Amortized SVGD: Towards an Automatic Neural Sampler}
\label{sec:amortizedsvgd}


SVGD and other particle-based methods become inefficient when we need to repeatedly infer
a large number different target distributions for multiple tasks, including online learning or inner loops of other algorithms, 
because they can not improve based on the experience from the past tasks, and may require a large memory to restore a large number of particles. 
We propose to ``amortize SVGD'' by training a neural network  to mimic the SVGD dynamics, yielding a solution for Problem~\ref{pro:prob1}. 

One straightforward way to achieve this is to run SVGD to convergence and 
train  to fit the SVGD results. 
This, however, requires to run many epochs of fully converged SVGD and can be slow in practice. 
We instead propose an \emph{incremental approach} in which  is iteratively adjusted 
so that the network outputs  changes along the Stein variational gradient direction in \eqref{equ:update11} 
in order to decrease the KL divergence between the target and approximation distribution. 


To be specific, denote by  the estimated parameter at the -th iteration of our method; 
each iteration of our method 
draws a batch of random inputs  
and calculate their corresponding output  based on ; here  is a mini-batch size (e.g., ). 
The Stein variational gradient  in 
\eqref{equ:update11} would then ensure that  forms a better approximation of the target distribution . 
Therefore, we should adjust  to make its output matches , 
that is, we want to update  by

See Algorithm~\ref{alg:alg1} for the summary of this procedure. If we assume  is very small, then 
\eqref{equ:follow1} reduces to a least square optimization. To see this, note that 
 by Taylor expansion. 
Since , we have

As a result, \eqref{equ:follow1} reduces to the following least square optimization: 

Update \eqref{equ:follow2} can still be computationally expensive because of the matrix inversion. 
We can derive a further approximation by performing only one step of gradient descent of \eqref{equ:follow1} (or \eqref{equ:follow2}), which gives 






Although update \eqref{equ:follow3} is derived as an approximation of \eqref{equ:follow1}-\eqref{equ:follow2}, 
it is computationally faster and we find it works very effectively in practice; 
this is because when  is small, 
one step of gradient update can be sufficiently close to the optimum.  

Update \eqref{equ:follow3} also has a simple and intuitive form: \eqref{equ:follow3} can be thought as \emph{a ``chain rule'' that back-propagates the Stein variational gradient to the network parameter }. 
This can be justified by considering the special case when we use only a single particle  
in which case  in \eqref{equ:update11} reduces to the typical gradient  of , 
and update \eqref{equ:follow3} reduces to the typical gradient ascent for maximizing 
  
in which case  is trained to maximize  (that is, \emph{learning to optimize}), 
instead of \emph{learning to draw samples from } for which it is crucial to use Stein variational gradient  to diversify the network outputs. 


Update \eqref{equ:follow3} also has a close connection with the typical variational inference with the reparameterization trick \citep{kingma2013auto}. Let  be the density function of , . Using the reparameterization trick, the gradient of  w.r.t.  can be shown to be 

With  i.i.d. drawn from  and , the standard stochastic gradient descent for minimizing the KL divergence is 

This is similar with \eqref{equ:follow3}, but replaces the Stein gradient  defined in \eqref{equ:update11} with 
. The advantage of using  is that it does not require to explicitly calculate ,
and hence admits a solution to Problem 1 in which  is not computable for complex network  and unknown input distribution . 
Further insights can be obtained by noting that 

where \eqref{equ:tmp} is obtained by using Stein's identity \eqref{equ:steinid}. 
Therefore,  can be treated as a kernel smoothed version of . 





\section{Amortized MLE for Generative Adversarial Training}
Our method allows us to design efficient approximate sampling methods 
adaptively and automatically, and enables a host of novel applications. 
In this paper, we apply it in an amortized MLE method for training deep generative models.   


Maximum likelihood estimator (MLE) provides a fundamental approach for learning probabilistic models from data, 
but can be computationally prohibitive on distributions for which drawing samples or computing likelihood is intractable due to the normalization constant. 
Traditional methods such as MCMC-MLE use hand-designed methods (e.g., MCMC) to approximate the intractable likelihood function but do not work efficiently in practice. 
We propose to adaptively train a generative neural network to draw samples from the distribution during MLE training, which not only provides computational advantage, and also allows us to generate realistic-looking images competitive with, or better than the state-of-the-art generative adversarial networks (GAN) \citep{goodfellow2014generative, radford2015unsupervised} (see Figure~\ref{fig:mnist}-\ref{fig:facemore}).  


To be specific, denote by  a set of observed data. 
We consider the maximum likelihood training of energy-based models of form 

where  is an energy function for  indexed by parameter  and  is the log-normalization constant. The log-likelihood function of  is 
whose gradient is 
where  and  denote the empirical average on the observed data 
and the expectation under model , respectively.  
The key computational difficulty is to approximate the model expectation . 
To address this problem, 
we use a generative neural network  trained by Algorithm~\ref{alg:alg1} 
to approximately sample from , yielding a gradient update for  of form

where  denotes the empirical average on  where , 
. 
As  is updated by gradient ascent, 
 is successively updated via Algorithm~\ref{alg:alg1} to \emph{follow} . 
See Algorithm \ref{alg:gan}. 

We call our method \emph{SteinGAN}, because it can be intuitively
interpreted as an adversarial game between 
the generative network  and 
the energy model  which serves as a discriminator:
The MLE gradient update of  effectively decreases the energy of the training data and increases the energy of the simulated data from , while the SVGD update of  decreases the energy of the simulated data to fit better with .  
Compared with the traditional methods based on MCMC-MLE or contrastive divergence, we \emph{amortize the sampler as we train}, which gives much faster speed and simultaneously provides a high quality generative neural network that can generate realistic-looking images; see \citet{kim2016deep} for a similar idea and discussions. 


\begin{algorithm}[t]                      \caption{Amortized MLE as Generative Adversarial Learning}\label{alg:gan}                           \begin{algorithmic}                    \STATE {\bf Goal:} MLE training for energy model .
\STATE Initialize  and . 
\FOR {iteration }
\STATE {\bf Updating :} Draw , ; update  using \eqref{equ:follow1}, \eqref{equ:follow2} or \eqref{equ:follow3} with . Repeat several times when needed. 
\STATE 
 {\bf Updating :}  Draw a mini-batch of observed data , and simulated data , update  by \eqref{equ:updatetheta}. 
\ENDFOR
\end{algorithmic}
\end{algorithm}


\section{Empirical Results}  \label{sec:gan}

We evaluated our SteinGAN on four datasets,  
MNIST, CIFAR-10, CelebA \citep{liu2015faceattributes}, and Large-scale Scene Understanding (LSUN) \citep{yu2015lsun}, on which we find our method tends to generate realistic-looking images competitive with, sometimes better than DCGAN \citep{radford2015unsupervised} (see Figure~\ref{fig:cifar10} - Figure~\ref{fig:face}). 
Our code is available at \url{https://github.com/DartML/SteinGAN}.

\newcommand{\enc}{\mathrm{E}}
\newcommand{\dec}{\mathrm{D}}
\paragraph{Model Setup} In order to generate realistic-looking images, we define our energy model based on an autoencoder: 

where  denotes the image. 
This choice is motivated by Energy-based GAN \citep{zhao2016energy} in which the autoencoder loss is used as a discriminator but without a probabilistic interpretation. 
We assume  to be a neural network whose input  is a -dimensional random vector drawn by . 
The positive definite kernel in SVGD is defined by the RBF kernel on the hidden representation obtained by the autoencoder in \eqref{equ:px}, that is, 

As it is discussed in Section~\ref{sec:amortizedsvgd}, the kernel provides a repulsive force 
to produce an amount of variability required for generating samples from . 
This is similar to the heuristic repelling regularizer in \citet{zhao2016energy} and the batch normalization based regularizer in \citet{kim2016deep}, but is derived in a more principled way. 
We take the bandwidth to be , where  is the median of the pairwise distances between  on the image simulated by . 
This makes the kernel change adaptively based on both  (through ) and  (through bandwidth ). 

Some datasets include both images  and 
their associated discrete labels . In these cases, we train a joint energy model on  
to capture both the inner structure of the images and its predictive relation with the label,  allowing us to simulate images 
with a control on which category it belongs to. Our joint energy model is defined to be 

where  is the cross entropy loss function of a fully connected output layer. 
In this case, our neural sampler first draws a label  randomly according to the empirical counts in the dataset, 
and then passes  into a neural network together with a  random vector  to generate image . 
This allows us to generate images for particular categories by controlling the value of input . 


\paragraph{Stabilization}
In practice, we find it is useful to modify \eqref{equ:updatetheta} to be 

where  is a discount factor (which we take to be ). 
This is equivalent to maximizing a regularized likelihood: 

where  is the log-partition function; note that  is a conjugate prior of . 

We initialize the weights of both the generator and discriminator from Gaussian distribution , 
and train them using Adam \citep{kingma2014adam} with a learning rate of  for the generator and  for the energy model (the discriminator).  
In order to keep the generator and discriminator approximately aligned during training, 
we speed up the MLE update \eqref{equ:disc} of the discriminator (by increasing its learning rate to ) when the energy of the real data batch is larger than the energy of the simulated images, 
while slow down it (by freezing the MLE update of  in \eqref{equ:disc}) if the magnitude of the energy difference between the real images and the simulated images goes above a threshold of 0.5.
We used the bag of architecture guidelines for stable training suggested in DCGAN \citep{radford2015unsupervised}.

\begin{comment}
We tested our SteinGAN on four datasets,  
MNIST \footnote{\url{http://yann.lecun.com/exdb/mnist/}}, CIFAR-10 \footnote{\url{https://www.cs.toronto.edu/~kriz/cifar.html}}, CelebA \footnote{\url{http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html}}, and Large-scale Scene Understanding (LSUN) \footnote{\url{http://lsun.cs.princeton.edu/2016/}}, on which we find our method tends to generate realistic-looking images competitive with DCGAN \citep{radford2015unsupervised} (see Figure~\ref{fig:face}-Figure~\ref{fig:cifar10}). 
In particular, we find we generate better images than DCGAN on CelebA (Figure~\ref{fig:face}), 
and our simulated CIFAR-10 images achieves better testing classification accuracy when used as a training data (see Figure~\ref{fig:cifar10}). 
See Appendix~\ref{sec:gan} for more information. 
We will provide code to reproduce our experiments.  
\end{comment}


\paragraph{Discussion}
The MNIST dataset has a training set of  examples. 
Both DCGAN and our model produce high quality images, both visually indistinguishable from real images; see figure \ref{fig:mnist}. 

CIFAR-10 is very diverse, and with only 50,000 training examples.
Figure~\ref{fig:cifar10} shows examples of simulated images by DCGAN and SteinGAN generated conditional on each category, which look equally well visually. 
We also provide quantitively evaluation using a recently proposed inception score \citep{salimans2016improved}, as well as 
the classification accuracy when training ResNet using  simulated images as train sets, evaluated on a separate held-out testing set never seen by the GAN models.  
Besides DCGAN and SteinGAN, we also evaluate another simple baseline obtained by subsampling 500 real images from the training set and duplicating them 100 times. 
We observe that these scores capture rather different perspectives of image generation:
The inception score favors images that look realistic individually and have uniformly distributed labels; as a result, 
the inception score of the duplicated 500 images is almost as high as the real training set. 
We find that the inception score of SteinGAN is comparable, or slightly lower than that of DCGAN. 
On the other hand, the classification accuracy measures the amount information captured in the simulated image sets;  
we find that SteinGAN achieves the highest classification accuracy, suggesting that it captures more information in the training set. 

Figure~\ref{fig:face} and \ref{fig:room} visualize the results on CelebA (with more than 200k face images) and LSUN (with nearly 3M bedroom images), respectively. 
We cropped and resized both dataset images into . 


\begin{figure}[htb]
\centering
\begin{tabular}{cc}
\includegraphics[width=0.3\textwidth]{figures/mnist/dcgan_mnist.pdf} & 
\includegraphics[width=0.3\textwidth]{figures/mnist/mnist_steingan.pdf}   \\
DCGAN & SteinGAN \\
\vspace{5\baselineskip}
\end{tabular}
\begin{comment}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|l|c|c|c|}
\hline
Training Data & Real Images & DCGAN & SteinGAN \\
\hline
Testing Accuracy& 92.58 \% &  41.82 \%  & 58.69 \%\\
\hline
\end{tabular}
\end{comment}
\caption{
 MNIST images generated by DCGAN and our SteinGAN. We use the joint model in \eqref{equ:pxy} to allow us to generate images for each digit. We set . 
}
\label{fig:mnist}
\end{figure}



\begin{figure}[t]
\centering
\begin{tabular}{ccc}
\raisebox{8.3em}{
\renewcommand{\arraystretch}{1.5}
\newcommand{\tmpfnt}{\small}
\hspace{-.04\textwidth}
\begin{tabular}{r}
\tmpfnt airplane \\
\tmpfnt automobile \\
\tmpfnt bird \\
\tmpfnt cat \\
\tmpfnt deer \\
\tmpfnt dog \\
\tmpfnt frog \\
\tmpfnt horse \\
\tmpfnt ship \\
\tmpfnt truck
\end{tabular}} & 
\hspace{-.04\textwidth}\includegraphics[width=.42\textwidth]{figures/cifar10/dcgan_cifar10.pdf}  & 
\includegraphics[width=.42\textwidth]{figures/cifar10/vgd_cifar10.pdf}  \\
& DCGAN 
& SteinGAN \\
\vspace{5\baselineskip}
\end{tabular}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|l|c|c|c|c|}
\hline
\multicolumn{5}{|c|}{Inception Score} \\
\hline
 & Real Training Set & 500 Duplicate  & DCGAN  & SteinGAN \\
\hline
Model Trained on ImageNet  & 11.237 & 11.100 & 6.581 & 6.351 \\ \hline
Model Trained on CIFAR-10 & 9.848 & 9.807 & 7.368  & 7.428 \\  \hline 
\end{tabular}  \\
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|c|c|c|c|}
\hline
\multicolumn{4}{|c|}{Testing Accuracy} \\
\hline
Real Training Set & 500 Duplicate  & DCGAN  & SteinGAN \\
\hline
92.58 \% &  44.96 \% & 44.78 \%  & 63.81 \%\\  \hline
\end{tabular}
\caption{Results on CIFAR-10. ``500 Duplicate'' denotes  500 images randomly subsampled from the training set, each duplicated 100 times.
Upper: images simulated by DCGAN and SteinGAN (based on joint model \eqref{equ:pxy}) conditional on each category.  
Middle: inception scores for samples generated by various methods (all with 50,000 images) on inception models trained on ImageNet and CIFAR-10, respectively. 
Lower: testing accuracy on real testing set when using 50,000 simulated images to train ResNets for classification. SteinGAN achieves higher testing accuracy than DCGAN. We set  and .}\label{fig:cifar10}
\end{figure}


\begin{figure}[t]
\centering
\begin{tabular}{cc}
\includegraphics[width=0.45\textwidth]{figures/faces/dcgan_bright.pdf} & 
\includegraphics[width=0.45\textwidth]{figures/faces/vgd_faces_small.pdf} \\
DCGAN & SteinGAN \\
\end{tabular}
\begin{tabular}{c}
\includegraphics[width=0.85\textwidth]{figures/faces/z2.pdf} \\
\end{tabular}
\caption{Results on CelebA. Upper: images generated by DCGAN and our SteinGAN. Lower: images generated by SteinGAN when performing a random walk  on the random input ; we can see that a man with glasses and black hair gradually changes to a woman with blonde hair. 
See Figure~\ref{fig:facemore} for more examples. }
\label{fig:face}
\end{figure}


\begin{figure}[t]
\centering
\begin{tabular}{cc}
\includegraphics[width=0.45\textwidth]{figures/lsun/dcgan_5_7.pdf} & 
\includegraphics[width=0.45\textwidth]{figures/lsun/vgd_5_7.pdf} \\
DCGAN & SteinGAN\\
\end{tabular}
\caption{Images generated by DCGAN and our SteinGAN on LSUN.}
\label{fig:room}
\end{figure}


\begin{comment}
\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/faces/vgd_gan-20.pdf}  
\caption{More images generated by SteinGAN on CelebA.}
\label{fig:facemore}
\end{figure}

\end{comment}
 
\section{Conclusion}
We propose a new method to train neural samplers for given distributions, together with a new SteinGAN method for generative adversarial training. 
Future directions involve more applications and theoretical understandings for training neural samplers. 

\newpage\clearpage
\bibliographystyle{iclr2016_conference}
{\small
\bibliography{bibrkhs_stein}
}

\appendix
\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{\dilinfig/faces/vgd_gan-20.pdf}  
\caption{More images generated by SteinGAN on CelebA.}
\label{fig:facemore}
\end{figure}


\end{document}
