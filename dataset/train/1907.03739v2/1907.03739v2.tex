\documentclass{article}

\usepackage[nonatbib,final]{neurips}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{color,xcolor}
\usepackage{epsfig}
\usepackage{graphicx}

\usepackage{adjustbox}
\usepackage{array}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{float,wrapfig}
\usepackage{hhline}
\usepackage{multirow}
\usepackage{subcaption} 

\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{bm}
\usepackage{nicefrac}
\usepackage{microtype}

\usepackage{changepage}
\usepackage{extramarks}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{setspace}
\usepackage{soul}
\usepackage{xspace}

\usepackage[pagebackref=true,breaklinks=true,colorlinks,citecolor=gray]{hyperref}
\usepackage[nocompress]{cite}
\usepackage{url}

\usepackage{algorithm, algorithmic}
\usepackage{enumerate}
\usepackage{todonotes} 

\usepackage{titlesec}
\usepackage{listings} \newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\newcommand{\xpar}[1]{\noindent\textbf{#1}\ \ }
\newcommand{\vpar}[1]{\vspace{3mm}\noindent\textbf{#1}\ \ }

\newcommand{\sect}[1]{Section~\ref{#1}}
\newcommand{\sects}[1]{Sections~\ref{#1}}
\newcommand{\eqn}[1]{Equation~\ref{#1}}
\newcommand{\eqns}[1]{Equations~\ref{#1}}
\newcommand{\fig}[1]{Figure~\ref{#1}}
\newcommand{\figs}[1]{Figures~\ref{#1}}
\newcommand{\tab}[1]{Table~\ref{#1}}
\newcommand{\tabs}[1]{Tables~\ref{#1}}

\newcommand{\ignorethis}[1]{}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\fcseven}{}

\renewcommand*{\thefootnote}{\fnsymbol{footnote}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\def\naive{na\"{\i}ve\xspace}
\def\Naive{Na\"{\i}ve\xspace}

\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\iid{\emph{i.i.d}\onedot}
\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother

\definecolor{citecolor}{RGB}{34,139,34}
\definecolor{mydarkblue}{rgb}{0,0.08,1}
\definecolor{mydarkgreen}{rgb}{0.02,0.6,0.02}
\definecolor{mydarkred}{rgb}{0.8,0.02,0.02}
\definecolor{mydarkorange}{rgb}{0.40,0.2,0.02}
\definecolor{mypurple}{RGB}{111,0,255}
\definecolor{myred}{rgb}{1.0,0.0,0.0}
\definecolor{mygold}{rgb}{0.75,0.6,0.12}
\definecolor{myblue}{rgb}{0,0.2,0.8}
\definecolor{mydarkgray}{rgb}{0.66,0.66,0.66}

\newcommand{\zhijian}[1]{\textcolor{mydarkblue}{[Zhijian: #1]}}
\newcommand{\haotian}[1]{\textcolor{mypurple}{[Haotian: #1]}}
\newcommand{\yujun}[1]{\textcolor{mydarkorange}{[Yujun: #1]}}
\newcommand{\SH}[1]{\textcolor{mydarkred}{[Song: #1]}}

\newcommand{\myparagraph}[1]{\vspace{-6pt}\paragraph{#1}}
\newcommand{\myrebutpara}[1]{\vspace{-10pt}\paragraph{#1}}

\def\model{Point-Voxel CNN\xspace}
\def\modelshort{PVCNN\xspace}
\def\modelshortp{PVCNN++\xspace}

\def\conv{Point-Voxel Convolution\xspace}
\def\convshort{PVConv\xspace} 
\title{Point-Voxel CNN for Efficient 3D Deep Learning}
\author{Zhijian Liu\\
MIT
\And
Haotian Tang\\
Shanghai Jiao Tong University
\And
Yujun Lin\\
MIT
\And
Song Han\\
MIT
}

\begin{document}
\maketitle

\footnotetext{ indicates equal contributions. The first two authors are listed in the alphabetical order.}

\begin{abstract}

We present \model (\modelshort) for efficient, fast 3D deep learning. Previous work processes 3D data using either voxel-based or point-based NN models. However, both approaches are computationally inefficient. The computation cost and memory footprints of the voxel-based models grow \textit{cubically} with the input resolution, making it memory-prohibitive to scale up the resolution. As for point-based networks, up to 80\% of the time is wasted on structuring the \textit{sparse} data which have rather poor memory locality, not on the actual feature extraction. In this paper, we propose \modelshort that represents the 3D input data in \textit{points} to reduce the memory consumption, while performing the convolutions in \textit{voxels} to reduce the irregular, sparse data access and improve the locality. Our \modelshort model is both memory and computation efficient. Evaluated on semantic and part segmentation datasets, it achieves much higher accuracy than the voxel-based baseline with \textbf{10} GPU memory reduction; it also outperforms the state-of-the-art point-based models with \textbf{7} measured speedup on average. Remarkably, the narrower version of \modelshort achieves \textbf{2} speedup over PointNet (an extremely efficient model) on part and scene segmentation benchmarks with much higher accuracy. We validate the general effectiveness of \modelshort on 3D object detection: by replacing the primitives in Frustrum PointNet with \convshort, it outperforms Frustrum PointNet++ by \textbf{2.4\%} mAP on average with \textbf{1.5} measured speedup and GPU memory reduction.

\end{abstract} \section{Introduction}
\label{sec:intro}

3D deep learning has received increased attention thanks to its wide applications: \eg, AR/VR and autonomous driving. These applications need to interact with people in real time and therefore require low latency. However, edge devices (such as mobile phones and VR headsets) are tightly constrained by hardware resources and battery. Therefore, it is important to design efficient and fast 3D deep learning models for real-time applications on the edge.

Collected by the LiDAR sensors, 3D data usually comes in the format of point clouds. Conventionally, researchers rasterize the point cloud into voxel grids and process them using 3D volumetric convolutions~\cite{Choy:2016us,Riegler:2017vk}. With low resolutions, there will be information loss during voxelization: multiple points will be merged together if they lie in the same grid. Therefore, a high-resolution representation is needed to preserve the fine details in the input data. However, the computational cost and memory requirement both increase \textit{cubically} with voxel resolution. Thus, it is infeasible to train a voxel-based model with high-resolution inputs: \eg, 3D-UNet~\cite{Cicek:2016un} requires more than 10 GB of GPU memory on 646464 inputs with batch size of 16, and the large memory footprint makes it rather difficult to scale beyond this resolution.

Recently, another stream of models attempt to directly process the input point clouds~\cite{Qi:2017vq,Qi:2017tf,Klokov:2017te,Li:2018tp}. These point-based models require much lower GPU memory than voxel-based models thanks to the sparse representation. However, they neglect the fact that the \textit{random memory access} is also very inefficient. As the points are scattered over the entire 3D space in an irregular manner, processing them introduces random memory accesses. Most point-based models~\cite{Li:2018tp} mimic the 3D volumetric convolution: they extract the feature of each point by aggregating its neighboring features. However, neighbors are not stored contiguously in the point representation; therefore, indexing them requires the costly nearest neighbor search. To trade space for time, previous methods replicate the entire point cloud for each center point in the nearest neighbor search, and the memory cost will then be , where  is the number of input points. Another overhead is introduced by the dynamic kernel computation. Since the relative positions of neighbors are not fixed, these point-based models have to generate the convolution kernels dynamically based on different offsets.

Designing efficient 3D neural network models needs to take the hardware into account. Compared with arithmetic operations, memory operations are particularly expensive: they consume two orders of magnitude \textit{higher} energy, having two orders of magnitude \textit{lower} bandwidth (\fig{fig:teaser:a}). Another aspect is the memory access pattern: the random access will introduce memory bank conflicts and decrease the throughput (\fig{fig:teaser:b}). From the hardware perspective, conventional 3D models are inefficient due to large memory footprint and random memory access.


This paper provides a novel perspective to overcome these challenges. We propose \model (\modelshort) that represents the 3D input data as point clouds to take advantage of the sparsity to reduce the memory footprint, and leverages the voxel-based convolution to obtain the contiguous memory access pattern. Extensive experiments on multiple tasks demonstrate that \modelshort outperforms the voxel-based baseline with \textbf{10} lower memory consumption. It also achieves \textbf{7} measured speedup on average compared with the state-of-the-art point-based models.

\begin{figure}[t]
\centering
\begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/teaser_dram}
    \caption{Off-chip DRAM accesses take two orders of magnitude more energy than arithmetic operations (640pJ \vs 3pJ~\cite{Horowitz:2014co}), while the bandwidth is two orders of magnitude less (30GB/s \vs 668GB/s~\cite{Jouppi:2017da}). Efficient 3D deep learning should \textbf{reduce the memory footprint}, which is the bottleneck of conventional \textit{voxel-based} methods.}
    \label{fig:teaser:a}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/teaser_random}
    \caption{Random memory access is inefficient since it cannot take advantage of the DRAM burst and will cause bank conflicts~\cite{DDR}, while contiguous memory access does not suffer from the above issue. Efficient 3D deep learning should \textbf{avoid random memory accesses}, which is the bottleneck of  conventional \textit{point-based} methods.}
    \label{fig:teaser:b}
\end{subfigure}
\caption{Efficient 3D models should reduce memory footprint and avoid random memory accesses.}
\label{fig:teaser}
\vspace{-10pt}
\end{figure}
  \section{Related Work}

\paragraph{Hardware-Efficient Deep Learning.} 

Extensive attention has been paid to hardware-efficient deep learning for real-world applications. For instance, researchers have proposed to reduce the memory access cost by pruning and quantizing the models~\cite{Han:2015pn,Han:2016uf,He:2018am,Lin:2016fp,Zhou:2017it,Wang:2019ha} or directly designing the compact models~\cite{Iandola:2016sq,Howard:2017mn,Sandler:2018ir,Howard:2019sf,Zhang:2018md,Ma:2018sn}. However, all these approaches are general-purpose and are suitable for arbitrary neural networks. In this paper, we instead design our efficient primitive based on some domain-specific properties: \eg, 3D point clouds are highly sparse and spatially structured.


\myparagraph{Voxel-Based 3D Models.}

Conventionally, researchers relied on the volumetric representation to process the 3D data~\cite{Wu:2015mn}. For instance, Maturana~\etal~\cite{Maturana:2015vn} proposed the vanilla volumetric CNN; Qi~\etal~\cite{Qi:2016vm} extended 2D CNNs to 3D and systematically analyzed the relationship between 3D CNNs and multi-view CNNs; Wang~\etal~\cite{Wang:2017td} incoporated the octree into volumetric CNNs to reduce the memory consumption. Recent studies suggest that the volumetric representation can also be used in 3D shape segmentation~\cite{Tatarchenko:2017oc,Wang:2019vs, Le:2018pg} and 3D object detection~\cite{Zhou:2018vn}.

\myparagraph{Point-Based 3D Models.}

PointNet~\cite{Qi:2017vq} takes advantage of the symmetric function to process the unordered point sets in 3D. Later research~\cite{Qi:2017tf,Klokov:2017te, Wang:2018dg} proposed to stack PointNets hierarchically to model neighborhood information and increase model capacity. Instead of stacking PointNets as basic blocks, another type of methods~\cite{Li:2018tp,Lan:2019ge, Xu:2018sp} abstract away the symmetric function using dynamically generated convolution kernels or learned neighborhood permutation function. Other research, such as SPLATNet~\cite{Su:2018sp}  which naturally extends the idea of 2D image SPLAT to 3D, and SONet~\cite{Li:2018so} which uses the self-organization mechanism with the theoretical guarantee of invariance to point order, also shows great potential in general-purpose 3D modeling with point clouds as input. 

\myparagraph{Special-Purpose 3D Models.}

There are also 3D models tailored for specific tasks. For instance, SegCloud~\cite{Tchapmi:2017sc}, SGPN~\cite{Wang:2018sg}, SPGraph~\cite{Landrieu:2018sp}, ParamConv~\cite{Wang:2018pc}, SSCN~\cite{Graham:2018ss} and RSNet~\cite{Huang:2018rs} are specialized in 3D semantic/instance segmentation. As for 3D object detection, F-PointNet~\cite{Qi:2018fd} is based on the RGB detector and point-based regional proposal networks; PointRCNN~\cite{Shi:2019pr} follows the similar idea while abstracting away the RGB detector; PointPillars~\cite{Lang:2019pp} and SECOND~\cite{Yan:2018se} focus on the efficiency.



 \section{Motivation}
\label{sec:motivation}

\begin{figure}[t]
\centering
\begin{subfigure}{0.485\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/motivation_voxel}
    \caption{Voxel-based: memory grows cubically}
    \label{fig:motivation:a}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/motivation_point}
    \caption{Point-based: large memory/computation overheads}
    \label{fig:motivation:b}
\end{subfigure}
\caption{Both voxel-based and point-based NN models are inefficient. Left: the voxel-based model suffers from large information loss at acceptable GPU memory consumption (model: 3D-UNet~\cite{Cicek:2016un}; dataset: ShapeNet Part~\cite{Chang:2015sn}). Right: the point-based model suffers from large irregular memory access and dynamic kernel computation overheads.}
\vspace{-10pt}
\label{fig:motivation}
\end{figure}
 
3D data can be represented in the format of , where  is the 3D coordinate of the \textsuperscript{th} input point or voxel grid, and  is the feature corresponding to . Both voxel-based and point-based convolution can then be formulated as

During the convolution, we iterate the center  over the entire input. For each center, we first index its neighbors  in , then convolve the neighboring features  with the kernel , and finally produces the corresponding output .



\subsection{Voxel-Based Models: Large Memory Footprint}

Voxel-based representation is regular and has good memory locality. However, it requires very high resolution in order not to lose information. When the resolution is low, multiple points are bucketed into the same voxel grid, and these points will no longer be \emph{distinguishable}. A point is kept only when it exclusively occupies one voxel grid. In \fig{fig:motivation:a}, we analyze the number of distinguishable points and the memory consumption (during training with batch size of 16) with different resolutions. On a single GPU (with 12 GB of memory), the largest affordable resolution is 64, which will lead to \textbf{42\%} of information loss (\ie, non-distinguishable points). To keep more than 90\% of the information, we need to double the resolution to 128, consuming 7.2 GPU memory (\textbf{82.6 GB}), which is prohibitive for deployment. Although the GPU memory increases cubically with the resolution, the number of distinguishable points has a diminishing return. Therefore, the voxel-based solution is not scalable.

\subsection{Point-Based Models: Irregular Memory Access and Dynamic Kernel Overhead}

Point-based 3D modeling methods are memory efficient. The initial attempt, PointNet~\cite{Qi:2017vq}, is also computation efficient, but it lacks the local context modeling capability. Later research~\cite{Qi:2017tf,Li:2018tp,Wang:2018dg,Xu:2018sp} improves the expressiveness of PointNet by aggregating the neighborhood information in the point domain. However, this will lead to the irregular memory access pattern and introduce the dynamic kernel computation overhead, which becomes the efficiency bottlenecks.

\myparagraph{Irregular Memory Access.}

Unlike the voxel-based representation, neighboring points  in the point-based representation are not laid out contiguously in memory. Besides, 3D points are scattered in ; thus, we need to explicitly identify who are in the neighboring set , rather than by direct indexing. Point-based methods often define  as nearest neighbors in the coordinate space~\cite{Li:2018tp,Xu:2018sp} or feature space~\cite{Wang:2018dg}. Either requires explicit and expensive KNN computation. After KNN, gathering all neighbors  in  requires large amount of random memory accesses, which is not cache friendly. Combining the cost of neighbor indexing and data movement, we summarize in \fig{fig:motivation:b} that the point-based models spend \textbf{36}\%~\cite{Li:2018tp}, \textbf{52}\%~\cite{Wang:2018dg} and \textbf{57}\%~\cite{Xu:2018sp} of the total runtime on structuring the irregular data and random memory access.

\myparagraph{Dynamic Kernel Computation.}

For the 3D volumetric convolutions, the kernel  can be directly indexed as the relative positions of the neighbor  are fixed for different center : \eg, each axis of the coordinate offset  can only be 0, 1 for the convolution with size of 3. However, for the point-based convolution, the points are scattered over the entire 3D space irregularly; therefore, the relative positions of neighbors become unpredictable, and we will have to calculate the kernel  for each neighbor  \textit{on the fly}. For instance, SpiderCNN~\cite{Xu:2018sp} leverages the third-order Taylor expansion as a continuous approximation of the kernel ; PointCNN~\cite{Li:2018tp} permutes the neighboring points into a canonical order with the feature transformer . Both will introduce additional matrix multiplications. Empirically, we find that for PointCNN, the overhead of dynamic kernel computation can be more than \textbf{50}\% (see \fig{fig:motivation:b})!

In summary, the combined overhead of irregular memory access and dynamic kernel computation ranges from \textbf{55}\% (for DGCNN) to \textbf{88}\% (for PointCNN), which indicates that most computations are wasted on dealing with the irregularity of the point-based representation.
 \section{Point-Voxel Convolution}

Based on our analysis on the bottlenecks, we introduce a hardware-efficient primitive for 3D deep learning: \conv (\convshort), which combines the advantages of point-based methods (\ie, small memory footprint) and voxel-based methods (\ie, good data locality and regularity).

Our \convshort disentangles the \emph{fine-grained} feature transformation and the \emph{coarse-grained} neighbor aggregation so that each branch can be implemented efficiently and effectively. As illustrated in \fig{fig:overview}, the upper voxel-based branch first transforms the points into \emph{low-resolution} voxel grids, then it aggregates the neighboring points by the voxel-based convolutions, followed by devoxelization to convert them back to points. Either voxelization or devoxelization requires one scan over all points, making the memory cost low. The lower point-based branch extracts the features for each individual point. As it does not aggregate the neighbor's information, it is able to afford a very \emph{high resolution}.

\subsection{Voxel-Based Feature Aggregation}

A key component of convolution is to aggregate the neighboring information to extract local features. We choose to perform this feature aggregation in the volumetric domain due to its regularity.

\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{figures/overview}
\caption{\convshort is composed of a \emph{low-resolution} voxel-based branch and a \emph{high-resolution} point-based branch. The voxel-based branch extracts \emph{coarse-grained} neighborhood information, which is supplemented by the \emph{fine-grained} individual point features extracted from the point-based branch.}
\vspace{-10pt}
\label{fig:overview}
\end{figure} 
\myparagraph{Normalization.}

The scale of different point cloud might be significantly different. We therefore normalize the coordinates  before converting the point cloud into the volumetric domain. First, we translate all points into the local coordinate system with the gravity center as origin. After that, we normalize the points into the unit sphere by dividing all coordinates by , and we then scale and translate the points to . Note that the point features  remain unchanged during the normalization. We denote the normalized coordinates as .

\myparagraph{Voxelization.}

We transform the normalized point cloud  into the voxel grids  by averaging all features  whose coordinate  falls into the voxel grid :

where  denotes the voxel resolution,  is the binary indicator of whether the coordinate  belongs to the voxel grid ,  denotes the \textsuperscript{th} channel feature corresponding to , and  is the normalization factor (\ie, the number of points that fall in that voxel grid). As the voxel resolution  does not have to be large to be effective in our formulation (which will be justified in \sect{sec:exp}), the voxelized representation will not introduce very large memory footprint.

\myparagraph{Feature Aggregation.}

After converting the points into voxel grids, we apply a stack of 3D volumetric convolutions to aggregate the features. Similar to conventional 3D models, we apply the batch normalization~\cite{Ioffe:2015bn} and the nonlinear activation function~\cite{Maas:2013re} after each 3D convolution.

\myparagraph{Devoxelization.}

As we need to fuse the information with the point-based feature transformation branch, we then transform the voxel-based features back to the domain of point cloud. A straightforward implementation of the voxel-to-point mapping is the nearest-neighbor interpolation (\ie, assign the feature of a grid to all points that fall into the grid). However, this will make the points in the same voxel grid always share the same features. Therefore, we instead leverage the trilinear interpolation to transform the voxel grids to points to ensure that the features mapped to each point are distinct.

As our voxelization and devoxelization are both differentiable, the entire voxel-based feature aggregation branch can then be optimized in an end-to-end manner.

\subsection{Point-Based Feature Transformation}

The voxel-based feature aggregation branch fuses the neighborhood information in a coarse granularity. However, in order to model finer-grained individual point features, low-resolution voxel-based methods alone might not be enough. To this end, we directly operate on each point to extract individual point features using an MLP. Though simple, the MLP outputs distinct and discriminative features for each point. Such high-resolution individual point information is very critical to supplement the coarse-grained voxel-based information. 

\subsection{Feature Fusion}

With both individual point features and aggregated neighborhood information, we can efficiently fuse two branches with an addition as they are providing complementary information.


\subsection{Discussions}

\paragraph{Efficiency: Better Data Locality and Regularity.}

Our \convshort is more efficient than conventional point-based convolutions due to its better data locality and regularity. Our proposed voxelization and devoxelization both require  random memory accesses, where  is the number of points, since we only need to iterate over all points once to scatter them to their corresponding voxel grids. However, for conventional point-based methods, gathering the neighbors for all points requires at least  random memory accesses, where  is the number of neighbors. Therefore, our \modelshort is  more efficient from this viewpoint. As the typical value for  is 32/64 in PointNet++~\cite{Qi:2017tf} and 16 in PointCNN~\cite{Li:2018tp}, we empirically reduce the number of incontiguous memory accesses by 16 to 64 through our design and achieve better data locality. Besides, as our convolutions are done in the voxel domain, which is regular, our \convshort does not require KNN computation and dynamic kernel computation, which are usually quite expensive.

\begin{table*}[!t]
\setlength{\tabcolsep}{6.5pt}
\small\centering
\begin{tabular}{lccccc}
    \toprule
    & Input Data & Convolution & Mean IoU & Latency & GPU Memory \\
    \midrule
    PointNet~\cite{Qi:2017vq} & points (82048) & none & 83.7 & 21.7 ms & 1.5 GB\\
    3D-UNet~\cite{Cicek:2016un} & voxels (896\textsuperscript{3}) & volumetric & 84.6 & 682.1 ms & 8.8 GB\\
    RSNet~\cite{Huang:2018rs} & points (82048) & point-based & 84.9 & 74.6 ms & 0.8 GB\\
    PointNet++~\cite{Qi:2017tf} & points (82048) & point-based & 85.1 & 77.9 ms & 2.0 GB\\
    DGCNN~\cite{Wang:2018dg} & points (82048) & point-based & 85.1 & 87.8 ms & 2.4 GB \\
    \textbf{\modelshort} (Ours, 0.25C) & points (82048) & volumetric & \textbf{85.2} & \textbf{11.6 ms} & \textbf{0.8 GB} \\
    \midrule
    SpiderCNN~\cite{Xu:2018sp} & points (82048) & point-based & 85.3 & 170.7 ms & 6.5 GB \\
    \textbf{\modelshort} (Ours, 0.5C) & points (82048) & volumetric & \textbf{85.5} & \textbf{21.7 ms} & \textbf{1.0 GB} \\
    \midrule
PointCNN~\cite{Li:2018tp} &  points (82048) & point-based & 86.1 & 135.8 ms & 2.5 GB \\
    \textbf{\modelshort} (Ours, 1C) & points (82048) & volumetric & \textbf{86.2} & \textbf{50.7 ms} & \textbf{1.6 GB} \\
    \bottomrule
\end{tabular}
\caption{Results of object part segmentation on ShapeNet Part. On average, \modelshort outperforms the point-based models with \textbf{5.5} measured speedup and \textbf{3} memory reduction, and outperforms the voxel-based baseline with \textbf{59} measured speedup and \textbf{11} memory reduction.}
\label{tab:shapenet_results}
\vspace{-5pt}
\end{table*} \begin{figure*}[!t]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/shapenet_tradeoff_latency}
        \caption{Trade-off: accuracy \vs measured latency}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/shapenet_tradeoff_memory}
        \caption{Trade-off: accuracy \vs memory consumption}
    \end{subfigure}
    \caption{Comparisons between \modelshort and point/voxel-based baselines on ShapeNet Part.}
    \label{fig:shapenet_tradeoffs}
    \vspace{-10pt}
\end{figure*}
 
\myparagraph{Effectiveness: Keeping Points in High Resolution.}

As our point-based feature extraction branch is implemented as MLP, a natural advantage is that we are able to maintain the same number of points throughout the whole network while still having the capability to model neighborhood information. Let us make a comparison between our \convshort and set abstraction (SA) module in PointNet++~\cite{Qi:2017tf}. Suppose we have a batch of 2048 points with 64-channel features (with batch size of 16). We consider to aggregate information from 125 neighbors of each point and transform the aggregated feature to output the features with the same size. The SA module will require 75.2 ms of latency and 3.6 GB of memory consumption, while our \convshort will only require 25.7 ms of latency and 1.0 GB of memory consumption. The SA module will have to downsample to 685 points (\ie, around 3 downsampling) to match up with the latency of our \convshort, while the memory consumption will still be 1.5 higher. Thus, with the same latency, our \convshort is capable of modeling the full point cloud, while the SA module has to downsample the input aggressively, which will inevitably induce information loss. Therefore, our \modelshort is more effective compared to its point-based counterpart. \section{Experiments}
\label{sec:exp}

We experimented on multiple 3D tasks including object part segmentation, indoor scene segmentation and 3D object detection. Our \modelshort achieves superior performance on all these tasks with lower measured latency and GPU memory consumption. More details are provided in the appendix.

\subsection{Object Part Segmentation}

\paragraph{Setups.}

We first conduct experiments on the large-scale 3D object dataset, ShapeNet Parts~\cite{Chang:2015sn}. For a fair comparison, we follow the same evaluation protocol as in Li~\etal~\cite{Li:2018tp} and Graham~\etal~\cite{Graham:2018ss}. The evaluation metric is mean intersection-over-union (mIoU): we first calculate the part-averaged IoU for each of the 2874 test models and average the values as the final metrics. Besides, we report the measured latency and GPU memory consumption on a single GTX 1080Ti GPU to reflect the efficiency. We ensure the input data to have the same size with 2048 points and batch size of 8.

\myparagraph{Models.}

We build our \modelshort by replacing the MLP layers in PointNet~\cite{Qi:2017vq} with our \convshort layers. We adopt PointNet~\cite{Qi:2017vq}, RSNet~\cite{Huang:2018rs}, PointNet++~\cite{Qi:2017tf} (with multi-scale grouping), DGCNN~\cite{Wang:2018dg}, SpiderCNN~\cite{Xu:2018sp} and PointCNN~\cite{Li:2018tp} as our point-based baselines. We reimplement 3D-UNet~\cite{Cicek:2016un} as our voxel-based baseline. Note that most baselines make their implementation publicly available, and we therefore collect the statistics from their official implementation.



\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/shapenet_jetson.pdf}
    \caption{\modelshort runs efficiently on edge devices with low latency.}
    \label{fig:shapenet_jetson}
\vspace{-5pt}
\end{figure} \begin{table*}[!t]
\begin{minipage}[b]{0.5\linewidth}
\small\centering
\setlength{\tabcolsep}{4pt}
\begin{tabular}{lccc}
    \toprule
    & mIoU & Latency & GPU Mem.\\
    \midrule
    \textbf{\modelshort} (1R) & 86.2 & 50.7 ms & 1.59 GB \\
    \midrule
    \textbf{\modelshort} (0.75R) & 85.7 & 36.8 ms & 1.56 GB \\
    \textbf{\modelshort} (0.5R) & 85.5 & 28.9 ms & 1.55 GB \\
    \bottomrule
\end{tabular}
\caption{Results of different voxel resolutions.}
\vspace{-5pt}
\label{tab:shapenet_resolutions}
\end{minipage}
\begin{minipage}[b]{0.5\linewidth}
\setlength{\tabcolsep}{3pt}
\small\centering
\begin{tabular}{lc}
    \toprule
    & mIoU \\
    \midrule
Devoxelization w/o trilinear interpolation & -0.5 \\
\midrule
    1 voxel convolution in each \convshort & -0.6 \\
    3 voxel convolution in each \convshort & -0.1 \\
    \bottomrule
\end{tabular}
\caption{Results of more ablation studies.}
\vspace{-5pt}
\label{tab:shapenet_ablation}
\end{minipage}
\end{table*}
 \begin{figure}[!t]
\centering
\begin{subfigure}[b]{\linewidth}
    \centering
    \caption{Top row: features extracted from \emph{coarse-grained} voxel-based branch (large, continuous).}
    \includegraphics[width=0.115\linewidth]{figures/features/1a.png}
    \includegraphics[width=0.115\linewidth]{figures/features/2a.png}
    \includegraphics[width=0.115\linewidth]{figures/features/3a.png}
    \includegraphics[width=0.115\linewidth]{figures/features/4a.png}
    \includegraphics[width=0.115\linewidth]{figures/features/5a.png}
    \includegraphics[width=0.115\linewidth]{figures/features/6a.png}
    \includegraphics[width=0.115\linewidth]{figures/features/7a.png}
    \includegraphics[width=0.115\linewidth]{figures/features/8a.png}
\end{subfigure}
\begin{subfigure}[b]{\linewidth}
    \centering
    \includegraphics[width=0.115\linewidth]{figures/features/1b.png}
    \includegraphics[width=0.115\linewidth]{figures/features/2b.png}
    \includegraphics[width=0.115\linewidth]{figures/features/3b.png}
    \includegraphics[width=0.115\linewidth]{figures/features/4b.png}
    \includegraphics[width=0.115\linewidth]{figures/features/5b.png}
    \includegraphics[width=0.115\linewidth]{figures/features/6b.png}
    \includegraphics[width=0.115\linewidth]{figures/features/7b.png}
    \includegraphics[width=0.115\linewidth]{figures/features/8b.png}
    \caption{Bottom row: features extracted from \emph{fine-grained} point-based branch (isolated, discontinuous).}
\end{subfigure}
\caption{Two branches are providing complementary information: the voxel-based branch focuses on the large, continuous parts, while the point-based focuses on the isolated, discontinuous parts.}
\label{fig:shapenet_features}
\vspace{-10pt}
\end{figure} 
\myparagraph{Results.}

As in \tab{tab:shapenet_results}, our \modelshort outperforms all previous models. \modelshort directly improves the accuracy of its backbone (PointNet) by 2.5\% with even smaller overhead compared with PointNet++. We also design narrower versions of \modelshort by reducing the number of channels to 25\% (denoted as 0.25C) and 50\% (denoted as 0.5C). The resulting model requires only 53.5\% latency of PointNet, and it still outperforms several point-based methods with sophisticated neighborhood aggregation including RSNet, PointNet++ and DGCNN, which are almost an order of magnitude slower.



In \fig{fig:shapenet_tradeoffs}, \modelshort achieves a significantly better accuracy \vs latency trade-off compared with all point-based methods. With similar accuracy, our \modelshort is \textbf{15} faster than SpiderCNN and \textbf{2.7} faster than PointCNN. Our \modelshort also achieves a significantly better accuracy \vs memory trade-off compared with modern voxel-based baseline. With better accuracy, \modelshort saves the GPU memory consumption by \textbf{10} compared with 3D-UNet.

Furthermore, we also measure the latency of \modelshort on three edge devices. In \fig{fig:shapenet_jetson}, \modelshort consistently achieves a speedup of \textbf{2} over PointNet and PointCNN  on different devices. Especially, \modelshort is able to run at 19.9 objects per second on Jetson Nano with PointNet++-level accuracy and 20.2 objects per second on Jetson Xavier with PointCNN-level accuracy.

\myparagraph{Analysis.}

Conventional voxel-based methods have saturated the performance as the input resolution increases, but the memory consumption grows cubically. \modelshort is much more efficient, and the memory increases sub-linearly (\tab{tab:shapenet_resolutions}). By increasing the resolution from 16 (0.5R) to 32 (1R), the GPU memory usage is increased from 1.55 GB to 1.59 GB, only 1.03. Even if we squeeze the volumetric resolution to 16 (0.5R), our method still outperforms 3D-UNet that has much higher voxel resolution (96) by a large margin (1\%). \modelshort is very robust even with small resolution in the voxel branch, thanks to the high-resolution point-based branch maintaining the individual point's information. We also compared different implementations of devoxelization in \tab{tab:shapenet_ablation}. The trilinear interpolation performs better than the nearest neighbor, which is because the points near the voxel boundaries will introduce larger fluctuations to the gradient, making it harder to optimize.

\myparagraph{Visualization.}

We illustrate the voxel and point branch features from the final \convshort in \fig{fig:shapenet_features}, where warmer color represents larger magnitude. We can see that the voxel branch captures large, continuous parts (\eg table top, lamp head) while the point branch captures isolated, discontinuous details (\eg, table legs, lamp neck). The two branches provide complementary information and can be explained by the fact that the convolution operation extracts features with continuity and locality.

\subsection{Indoor Scene Segmentation}

\begin{table*}[t]
\setlength{\tabcolsep}{5.5pt}
\small\centering
\begin{tabular}{lcccccc}
    \toprule
    & Input Data & Convolution & mAcc & mIoU & Latency & GPU Mem. \\
    \midrule
    PointNet~\cite{Qi:2017vq} & points (84096) & none & 82.54 & 42.97 & 20.9 ms & 1.0 GB\\
    \textbf{\modelshort} (Ours, 0.125C) & points (84096) & volumetric & \textbf{82.60} & \textbf{46.94} & \textbf{8.5 ms} & \textbf{0.6 GB}\\
    \midrule
    DGCNN~\cite{Wang:2018dg} & points (84096) & point-based & 83.64 & 47.94 & 178.1 ms & 2.4 GB \\
    RSNet~\cite{Huang:2018rs} & points (84096) & point-based & -- & 51.93 & 111.5 ms & 1.1 GB \\
    \textbf{\modelshort} (Ours, 0.25C) & points (84096) & volumetric & \textbf{85.25} & \textbf{52.25} & \textbf{11.9 ms} & \textbf{0.7 GB}\\
    \midrule
    3D-UNet~\cite{Cicek:2016un} & voxels (896\textsuperscript{3}) & volumetric & 86.12 & 54.93 & 574.7 ms & 6.8 GB \\
    \textbf{\modelshort} (Ours, 1C) & points (84096) & volumetric & 86.66 & 56.12 & 47.3 ms & 1.3 GB \\
    \textbf{\modelshortp} (Ours, 0.5C) & points (48192) & volumetric & \textbf{86.87} & \textbf{57.63} & \textbf{41.1 ms} & \textbf{0.7 GB}\\
    \midrule
    PointCNN~\cite{Li:2018tp} & points (162048) & point-based & 85.91 & 57.26 & 282.3 ms & 4.6 GB \\
    \textbf{\modelshortp} (Ours, 1C) & points (48192) & volumetric & \textbf{87.12} & \textbf{58.98} & \textbf{69.5 ms} & \textbf{0.8 GB} \\
    \bottomrule
\end{tabular}
\caption{Results of indoor scene segmentation on S3DIS. On average, our \modelshort and \modelshortp outperform the point-based models with \textbf{8} measured speedup and \textbf{3} memory reduction, and outperform the voxel-based baseline with \textbf{14} measured speedup and \textbf{10} memory reduction.}
\label{tab:s3dis_results}
\vspace{-6pt}
\end{table*} \begin{figure*}[!t]
\centering
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/s3dis_tradeoff_latency}
    \caption{Trade-off: accuracy \vs measured latency}
\end{subfigure}
\hfill
\begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/s3dis_tradeoff_memory}
    \caption{Trade-off: accuracy \vs memory consumption}
\end{subfigure}
\caption{Comparisons between \modelshort and point/voxel-based baselines on S3DIS.}
\label{fig:s3dis_tradeoffs}
\vspace{-10pt}
\end{figure*}
 
\paragraph{Setups.}

We conduct experiments on the large-scale indoor scene segmentation dataset, S3DIS~\cite{Armeni:2017is,Armeni:2016sd}. We follow Tchapmi~\etal~\cite{Tchapmi:2017sc} and Li~\etal~\cite{Li:2018tp} to train the models on area 1,2,3,4,6 and test them on area 5 since it is the only area that does not overlap with any other area. Both data processing and evaluation protocol are the same as PointCNN~\cite{Li:2018tp} for fair comparison. We measure the latency and memory consumption with 32768 points per batch at test time on a single GTX 1080Ti GPU.

\myparagraph{Models.}

Apart from \modelshort (which is based on PointNet), we also extend PointNet++~\cite{Qi:2017tf} with our \convshort to build \modelshortp. We compare our two models with the state-of-the-art point-based models~\cite{Qi:2017vq,Huang:2018rs, Wang:2018dg, Li:2018tp} and the voxel-based baseline~\cite{Cicek:2016un}.

\myparagraph{Results.}

As in \tab{tab:s3dis_results}, \modelshort improves its backbone (PointNet) by more than \textbf{13\%} in mIoU, and it also outperforms DGCNN (which involves sophisticated graph convolutions) by a large margin in both accuracy and latency. Remarkably, our \modelshortp outperforms the state-of-the-art point-based model (PointCNN) by 1.7\% in mIoU with \textbf{4} lower latency, and the voxel-based baseline (3D-UNet) by 4\% in mIoU with more than \textbf{8} lower latency and GPU memory consumption.

Similar to object part segmentation, we design compact models by reducing the number of channels in \modelshort to 12.5\%, 25\% and 50\% and \modelshortp to 50\%. Remarkably, the narrower version of our \modelshort outperforms DGCNN with \textbf{15} measured speedup, and RSNet with \textbf{9} measured speedup. Furthermore, it achieves 4\% improvement in mIoU upon PointNet while still being \textbf{2.5} faster than this extremely efficient model (which does not have any neighborhood aggregation).

\subsection{3D Object Detection}

\begin{table*}[t]
\setlength{\tabcolsep}{2.5pt}
\small\centering
\begin{tabular}{lccccccccccc}
    \toprule
    & \multicolumn{2}{c}{Efficiency} & \multicolumn{3}{c}{Car} & \multicolumn{3}{c}{Pedestrian} & \multicolumn{3}{c}{Cyclist} \\
    \cmidrule(lr){2-3}\cmidrule(lr){4-6}\cmidrule(lr){7-9}\cmidrule(lr){10-12}
    & Latency & GPU Mem. & Easy & Mod. & Hard & Easy & Mod. & Hard & Easy & Mod. & Hard \\
    \midrule
    F-PointNet~\cite{Qi:2018fd} & 29.1 ms & 1.3 GB & 83.26 & 69.28 & 62.56 & 65.08 & 55.85 & 49.28 & 74.54 & 55.95 & 52.65 \\
    F-PointNet++~\cite{Qi:2018fd} & 105.2 ms & 2.0 GB & 83.76 & 70.92 & 63.65 & 70.00 & 61.32 & 53.59 & 77.15 & 56.49 & 53.37 \\
    \textbf{\modelshort} (\emph{efficient}) & 58.9 ms & 1.4 GB & \textbf{84.22} & 71.11 & 63.63 & 69.16 & 60.28 & 52.52 & 78.67 & 57.79 & 54.16 \\
    \textbf{\modelshort} (\emph{complete}) & 69.6 ms & 1.4 GB & 84.02 & \textbf{71.54} & \textbf{63.81} & \textbf{73.20} & \textbf{64.71} & \textbf{56.78} & \textbf{81.40} & \textbf{59.97} & \textbf{56.24} \\
    \bottomrule
\end{tabular}
\caption{Results of 3D object detection on the \emph{val} set of KITTI. The \emph{complete} \modelshort outperforms F-PointNet++ in all categories significantly with \textbf{1.5} measured speedup and memory reduction.}
\label{tab:kitti_results}
\vspace{-10pt}
\end{table*} 
\paragraph{Setups.}

We finally conduct experiments on the driving-oriented dataset, KITTI~\cite{Geiger:2013kt}. We follow Qi~\etal~\cite{Qi:2018fd} to construct the \emph{val} set from the training set so that no instances in the \emph{val} set belong to the same video clip of any training instance. The size of \emph{val} set is 3769, leaving the other 3711 samples for training. We evaluate all models for 20 times and report the mean 3D average precision (AP).

\myparagraph{Models.}

We build two versions of \modelshort based on F-PointNet~\cite{Qi:2018fd}: \textbf{(a)} an \emph{efficient} version where we only replace the MLP layers within the instance segmentation network, and \textbf{(b)} a \emph{complete} version where we further replace the MLP layers in the box estimation network. We compare our two models with F-PointNet (whose backbone is PointNet) and F-PointNet++ (whose backbone is PointNet++).

\myparagraph{Results.}

In \tab{tab:kitti_results}, even if our \emph{efficient} model does not aggregate neighboring features in the box estimation network while F-PointNet++ does, ours still outperform it in most classes with \textbf{1.8} lower latency. Improving the box estimation network with \convshort, our \emph{complete} model outperforms both baselines in \textbf{all} categories significantly. Compared with F-PointNet baseline, our \modelshort obtains up to \textbf{8\%} mAP improvement in pedestrians and \textbf{3.5-6.8\%} mAP improvement in cyclist, which indicates that our proposed \modelshort is both efficient and expressive. \section{Conclusion}

We propose \model (\modelshort) for fast and efficient 3D deep learning. We bring the best of both worlds together: voxels and points, reducing the memory footprint and irregular memory access. We represent the 3D input data efficiently with the sparse, irregular point representation and perform the convolutions efficiently in the dense, regular voxel representation. Extensive experiments on multiple tasks consistently demonstrate the effectiveness and efficiency of our proposed method. We believe that our research will break the stereotype that the voxel-based convolution is naturally inefficient and shed light on co-designing the voxel-based and point-based architectures for fast and efficient 3D deep learning. \myparagraph{Acknowledgements.}

We thank MIT Quest for Intelligence, MIT-IBM Watson AI Lab, Samsung, Facebook and SONY for supporting this research. We thank AWS Machine Learning Research Awards for providing the computation resource. We thank NVIDIA for donating Jetson AGX Xavier. 
{\small
\bibliographystyle{ieee}
\bibliography{reference}
}
\end{document}