\documentclass{article}
\usepackage[preprint]{log_2022}


\usepackage{booktabs}           \usepackage{multirow}           \usepackage{amsfonts}           \usepackage{graphicx}           \usepackage{duckuments}         

\usepackage[numbers,compress,sort]{natbib}






\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{url}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{amsmath}
\usepackage{natbib}
\usepackage{adjustbox}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}


\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\newcommand{\algalign}[2]


\usepackage{cellspace}
\setlength\cellspacetoplimit{5pt}
\setlength\cellspacebottomlimit{5pt}

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\def\mA{\mbox{}}
\def\mB{\mbox{}}
\def\mP{\mbox{}}
\def\mW{\mbox{}}
\def\mZ{\mbox{}}
\def\bs{\mbox{\boldmath }}
\newenvironment{proof}[1][Proof]{\noindent \textbf{#1.} }{\qedsymbol}
\newcommand{\qedsymbol}{\hspace{\fill}\rule{1.5ex}{1.5ex}}

\newcommand{\tocite}[1]{\textcolor{green}{#1}}
\newcommand{\tb}[1]{\textbf{#1}}
\newcommand{\beq}{}
\def\btau{{\mathbf \tau}}
\def\bsigma{{\mathbf \sigma}}
\def\balpha{{\mathbf \alpha}}
\def\mL{\mbox{}}
\def\mU{\mbox{}}


\newcommand{\mybox}[1]{\parbox[t]{30pt}{\raggedright #1}}


\title[Cell Attention Networks]{Cell Attention Networks}


\author[L. Giusti et al.]{Lorenzo Giusti\\
  Sapienza University or Rome\\
  \texttt{lorenzo.giusti@uniroma1.it} \\  
  \And
  Claudio Battiloro\\
  Sapienza University or Rome\\
  \texttt{claudio.battiloro@uniroma1.it} \\
  \And
  Lucia Testa \\
  Sapienza University or Rome\\
  \texttt{lucia.testa@uniroma1.it} \\
  \And
  Paolo Di Lorenzo \\
  Sapienza University or Rome\\
  \texttt{paolo.dilorenzo@uniroma1.it} \\
  \And
  Stefania Sardellitti \\
  Sapienza University or Rome\\
  \texttt{stefania.sardellitti@uniroma1.it}\\
  \And
  Sergio Barbarossa \\
  Sapienza University or Rome\\
  \texttt{sergio.barbarossa@uniroma1.it} 
}

\begin{document}

\maketitle

\begin{abstract}
Since their introduction, graph attention networks achieved outstanding results in graph representation learning tasks. However, these networks consider only pairwise relationships among nodes and then they are not able to fully exploit higher-order interactions present in many real world data-sets. In this paper, we introduce Cell Attention Networks (CANs), a neural architecture operating on data defined over the vertices of a graph, representing the graph as the 1-skeleton of a cell complex introduced to capture higher order interactions. In particular, we exploit the lower and upper neighborhoods, as encoded in the cell complex, to design two independent masked self-attention mechanisms, thus generalizing the conventional graph attention strategy. The approach used in CANs is hierarchical and it incorporates the following steps: i) a lifting algorithm that learns {\it edge features} from {\it node features}; ii) a cell attention mechanism to find the optimal combination of edge features over both  lower and upper neighbors; iii)  a hierarchical {\it edge pooling} mechanism to extract a compact meaningful set of features. The experimental results show that CAN is a low complexity strategy that compares favorably with state of the art results on graph-based learning tasks.
\end{abstract}


\section{Introduction}

Graph Neural Networks (GNNs) find applications in a plethora of fields, like computational chemistry~\cite{gilmer2017neural}, social networks~\cite{fan2019graph} and physics simulations~\cite{shlomi2020graph}. Since their introduction~\cite{gori2005new, scarselli2008graph}, GNNs have shown remarkable results in learning tasks when data are defined over a graph domain, where the flexibility of neural networks is coupled with prior knowledge about data relationships, expressed in terms of the underlying topology. The literature on GNNs is large and numerous techniques have been studied, usually categorized in spectral~\cite{Bruna19, kipf2016semi} and non-spectral methods~\cite{hamilton2017inductive, DuvenaudMABHAA15, atwood2016diffusion}. 

Generally speaking, the idea is learning representations of node attributes using local aggregation, where the neighborhood is formally represented by the graph topology. By leveraging this basic but powerful idea, outstanding performance has been achieved in many traditional tasks such as classification for nodes or entire graphs~\cite{kipf2016semi, velivckovic2017graph, hamilton2017inductive} or link prediction~\cite{zhang2018link} as well as more specialized ones such as \emph{protein folding}~\cite{jumper2021highly} and \emph{neural algorithmic reasoning}~\cite{davies2021advancing, velivckovic2021neural}. At the same time, a major performance boost to deep learning algorithms has been offered by the inclusion of attention mechanisms, introduced to handle sequence-based tasks~\cite{bahdanau2014neural}, enabling for variable sized inputs, to concentrate on their most important features. Then, pioneering works introduced  graph attention networks~\cite{velivckovic2017graph, yun2019graph, EdgeRibeiro} achieving state-of-the-art results in most of the aforementioned tasks. 

Graphs can be also seen as a simple instance of a \emph{topological space}, able to capture {\it pairwise} interactions through the presence of an edge between any pair of directly interacting nodes. However, despite their overwhelming popularity, graph-based representations are unable to consistently model higher order relations, which play a crucial role in many practical applications. Examples where multiway relations cannot be reduced to an ensemble of pairwise relations are gene regulatory networks~\cite{sever2015signal,lambiotte2019networks}, where some reactions occur only when a set of multiple (not only two) genes interact, or applications in network neuroscience~\cite{giusti2016two, sizemore2018cliques}.
To overcome this problem, many architectures defined on general hypergraphs have been proposed~\cite{chien2022you,Yadati2019, hypergraphneuarlnet, Zhang2020Hyper-SAGNN:,NEURIPS2020_217eedd1}, and some works incorporating attention mechanisms for hypergraphs neural networks have been published~\cite{HyperAttMultimodal, HyperAttNetworks}.

However, in the meanwhile, pioneering works on Topological Signal Processing~\cite{barbarossa2020topological, sardellitti2022cell} demonstrated the benefit of processing signals defined on simplicial and cell complexes, which are specific examples of hyper-graphs with a rich algebraic description, and can easily encode multi-way relationships hidden in the data in a low-complexity fashion. Consequently, there was a natural interest in the design of (deep) neural networks architectures able to learn from data defined on simplicial and cell complexes, as summarized in the following state of the art.\\

\subsection{Related Works}
Despite Topological Deep Learning is an emerging research area that has been introduced quite recently, numerous pioneering works appeared in this field. In~\cite{bodnar2021weisfeiler}, message passing neural networks (MPNNs)~\cite{gilmer2017neural} are adapted to simplicial complexes (SCs), and a Simplicial Weisfeiler-Lehman (SWL) colouring procedure for distinguishing non-isomorphic SCs is introduced. The aggregation and updating functions are able to process the data exploiting lower and upper neighbourhoods. The architectures in~\cite{bodnar2021weisfeiler}, namely Message Passing Simplicial Networks (MPSNs), are a generalization of Graph Isomorphism Network (GIN)~\cite{xu2018powerful}.  In~\cite{bodnarcwnet}, message passing neural networks able to handle data defined over regular cell complexes are introduced under the name of CW Networks (CWNs), and they are proven to be not less powerful than the 3-WL test. The work in (~\cite{sheaf2022}) introduced Neural Sheaf Diffusion Models, neural architectures grounded in the theory of cellular sheaves able to improve learning performance on graph-based tasks especially in heterophilic settings. In~\cite{giusti22}, a novel attention neural architecture that operates on data defined on simplicial complexes leveraging masked self-attention layers is introduced, taking into account lower and upper neighbourhoods and introducing a proper technique to process the harmonic component of the data based on the design of a sparse projection operator. A similiar architecture is proposed in~\cite{anonymous2022SAT}, which re-weights the interactions between neighbouring simplices through an orientation equivariant attention mechanism. However, none of these works considered masked self-attention mechanisms for architectures designed to handle data defined over cell complexes. Finally, the work in ~\cite{hajij2022} introduced a broad class of attentional architectures operating on generalized higher-order domains called Combinatorial Complexes. 

\subsection{Contribution}

The aim of this paper is to introduce \textit{cell attention networks}, i.e., a fully  attentional low-complexity architecture able to learn from data defined over the nodes of a graph by incorporating the graph within a cell complex and working at the edge-level in order to extract higher order interactions. The idea is to implement an attention mechanism that handles relations among nearby edges, where the neighborhood is formally represented by a cell complex. To this aim, we exploit a hierarchical approach that lifts up {\it node features} to derive {\it edge features}, and then it computes the attention coefficients between nearby edges to find the optimal combination of edge features. In particular, we devise an \emph{attentional lift mechanism} that learns the data over the edges of the complex leveraging a self-attention mechanism over the features of the vertices that are on their boundaries. Consequently, our architecture is also equipped with a cellular lifting map algorithm that embeds the graph domain into a regular cell complex (CW).   Other graph lifting techniques have been used in previous works, such as  clique complexes ~\cite{Ferri2018,Milo2002}, or more sophisticated structures based on incidence tensors ~\cite{albooyeh-etal-2020-sample}.Finally, please notice that the proposed  architecture presents nice features of explainability due to its fully attention-driven design: by simply inspecting the attention coefficients, it is possible to understand the contribution of the single cells for the learning task. For instance, in a computational chemistry task, the upper attention coefficients may represent the importance of the interaction between two atoms inside the rings of a molecule. Also, considering a network traffic problem, the pooling attention coefficients may tell us which links can be considered obsolete, or even detect communities in social networks. 



\section{Cell Complexes}


In this section we recall the basics of regular cell complexes,  which are topological spaces provided with a rich algebraic structure that enables an efficient representation of high-order interaction systems, and we explain their relation with usual graphs . In particular, we will first introduce the  definition of a regular cell complex, then we will describe few additional properties enabling the representation of cell complexes via boundary operators.

\textit{\textbf{Definition 1 (Regular cell complex) }~\cite{hansen2019toward, bodnarcwnet}. A {\it regular cell complex} is a topological space  together with a partition  of subspaces  of  called , where  is the indexing set of , such that}

\begin{enumerate}
    \item For each    , every sufficient small neighborhood of  intersects finitely many ;  
    \item For all , we have that      iff   , where  is the closure of the cell;
    \item Every  is homeomorphic to  for some ;
    \item For every    there is a homeomorphism  of a closed ball in  to  such that the restriction of  to the interior of the ball is a homeomorphism onto .
\end{enumerate}


Condition (2) implies that the indexing set  has a poset structure, given by    iff   . This is known as the face poset of . The regularity condition (4) implies that all topological information about  is encoded in the poset structure of . Then, a regular cell complex can be identified with its face poset. 




\textit{\textbf{Definition 2 (k-skeleton)}. A {\it k-skeleton} of a cell complex , denoted , is the subcomplex of  consisting of cells of dimension at most k.}

From Definition 1 and Definition 2, it is trivial to check the 0-skeleton of a cell complex is the set of vertices  and the 1-skeleton is the underlying graph ; we refer to 2-cells as polygons and, in general, there is little interest with dimensions above two. Regular cell complexes can be described via an incidence relation (boundary relation) with a  reflexive and transitive closure that is consistent with the partial order introduced in Definition 1. The boundary relation describes which cells are on the boundary of other cells.


\textit{\textbf{Definition 3 (Boundary relation)}.We have the boundary relation     iff    and there is no cell  such that
     .}


We can use the previous definitions to define the four types of (local) adjacencies present in cell complexes, following the approach from~\cite{bodnarcwnet}: 

\textit{\textbf{Definition 4 (Cell complex adjacencies) }~\cite{bodnarcwnet}. For a cell complex  and a cell , we define: }

\begin{itemize}
    \item The boundary adjacent cells     , are the lower-dimensional cells on the
boundary of . For instance, the boundary cells of an edge are its vertices and the boundary cells of a polygon are its edges.

    \item The co-boundary adjacent cell      , are the higher-dimensional cells with
     on their boundary. For instance, the co-boundary cells of a vertex are the edges having that vertex as an endpoint and the co-boundary of an edge are the polygons having that edge as one of its sides. 
    
    \item The lower adjacent cells      such that  and , are the cells of
the same dimension as  that share a lower dimensional cell on their boundary. The line graph
adjacencies between the edges are a classic example of this.

     \item The upper adjacent cells      such that  and . These are the cells of
the same dimension as  that are on the boundary of the same higher-dimensional cell as . 
\end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=.42\textwidth]{figures/cc.jpg}
    \caption{Illustration of a geometric cell complex . In this figure, it is possible to distinguish the topology of  by the color attached to its elements,i.e., the polygons. Two polygons share the same color if the have the same number of boundary elements, i.e., triangles are orange, squares are green, and so on.}
    \label{fig:cc}
\end{figure}

Finally, it is possible to incorporate a graph  in a higher order cell complex  by attaching polygons to closed paths of edges having no internal chords. 

\subsection{Algebraic Representation}

Let us now introduce an algebraic representation of the cell complexes based on the incidence relations among cells. We need first to introduce
the orientation of a cell complex
by generalizing the concept of orientation of a simplex~\cite{bodnar2021weisfeiler, grady2010discrete}. To define the orientation of a -cell, we may apply a simplicial decomposition~\cite{grady2010discrete}, which consists in  subdividing the cell into a set of internal -simplices, so that, by orienting a single internal simplex, the orientation propagates to the entire cell. Defining the transposition as the permutation of two elements, two orientations are equivalent if each of them can be recovered from the other through an even number of transpositions. 
Given an oriented cell complex , let  denote the number of cells in dimension , and  and  denote two cells of the complex with dimension  and  respectively. 
The  signed boundary matrix  of  is:


  \beq \label{inc_coeff}
  [\mathbf{B}_k]_{i,j}=\left\{\begin{array}{rll}
  1, & \text{if} \; \tau_i \prec_{+} \sigma_j \\
  -1,& \text{if} \; \tau_i \prec_{-} \sigma_j\\
  0,&  \; \text{otherwise}\\
  \end{array}\right. 
  \eeq
 
where we use the notation  to indicate coherent orientation between two cells and 
to indicate a opposite orientation between two cells. 

As a particular case, let us consider a cell complex of order two  , where , ,  denote the set of  ,  and -cells, i.e., vertices, edges and polygons, respectively. We denote their cardinality by ,  and . Then, the two incidence matrices describing the connectivity of the complex are  and , where  can be written as in~\cite{sardellitti2022cell}: , with  ,  and   denoting the incidences between edges and, respectively, triangles, quadrilaterals, up to polygons with  sides (where each polygon does not include any internal chord between any pair of its vertices).  
 An interesting property of the incidence matrices is that , for all . 

Finally, the structure of a -cell complex can be described through the {\it higher-order combinatorial Laplacians} defined as~\cite{eckmann1944harmonische, Lim}: 

where  and   are respectively the lower and upper Laplacians, encoding the lower and upper adjacencies  of the -order cells.
Note that    corresponds to the combinatorial Laplacian used for graph representations.








\subsection{Data over Cell Complexes}

Let  be the set of k-th order cells in a cell complex . In most of the cases the focus is on complex  of order up to two, thus a set of vertices  with , a set of edges  with  and a set of polygons  with  are considered, resulting in  (cells of order 0),  (cells of order 1) and  (cells of order 2). In Fig. \ref{fig:cc} we sketch an example of a cell complex of order .

A -cell signal is defined as a mapping from the set of all -cells contained in the complex to real numbers:

The order of the signal is one less the cardinality of the elements of . Therefore, for a complex , the -cell signals are defined as the following mappings: 

representing vertex, edge and polygon signals, respectively. 

In this work, we will consider only vertex and edge signals. In particular, we will refer to an instance of the former as  while the instance of the latter will be referred as . 


\section{Cell Attention Networks}\label{sec:can}

The aim of this work is to extend Graph Attention Networks introduced in~\cite{velivckovic2017graph} to account for multi-way relationships, i.e., performing a masked self-attention mechanisms at the edge level. The proposed hierarchical architecture, which we refer to as Cell Attention Network (CAN) (\ref{fig:tan}), starts with the embedding of the input graphs in regular cell complexes via a skeleton-preserving cellular lifting map and an attentional lift procedure enabling the derivation of edge features from node features. Then, we introduce a novel edge-level attentional message passing scheme. After each round of message passing, we perform a novel edge pooling operation and a local readout to reduce complexity; finally, after the last message-passing round, a global readout is applied. As in the previous sections, we denote the input graph(s) with  and the input node features of node  with .

\subsection{Cellular Lifting Map}

We first need to incorporate input graphs in  regular cell complexes. To address this challenge, we exploit the notion of skeleton-preserving cellular lifting map presented in~\cite{bodnarcwnet} and defined as:

\textit{\textbf{Definition 5 (Skeleton-Preserving Cellular Lifting Map) }~\cite{bodnarcwnet}. A cellular lifting map  is a skeleton preserving function that incorporates a graph  into a regular cell complex , such that, for any graph , the 1-skeleton (i.e., the underlying graph) of   and  are isomorphic .}

Informally, Definition 5 just requires that the lifting map keeps the underlying graph structure unchanged. Several cellular lifting map can be exploited, in this work we opted for a lifting map that attach cells to all the induced (or chordless) cycles, where  can be considered a hyperparameter to be chosen arbitrarily, and which controls the maximum size of the polygons (2-cells) of the complex. 






\subsection{Attentional lift}
 After the Cellular Lifting Map, we need to learn edge features, thus performing a lift operation on the node features that we refer to as \emph{attentional lift}. To this aim, we exploit a masked multi-head self-attention mechanism~\cite{velivckovic2017graph}. The procedure is based on the computation of  attention heads such that, for each pair of nodes , connected by an edge , the corresponding edge features  are given by the concatenation of the resulting attention scores.

\textit{\textbf{Definition 6 (Attentional Lift)}}. An Attentional Lift is a \emph{learnable} function ,  of the form: 

where  is the -th (shared across nodes) learnable attention function, and  is the concatenation operator. Since the order of the nodes connected by an edge should not change the corresponding lifted edge features, we assume that the functions  are symmetric. 

\subsection{Cell Attention}

\begin{figure}[t]
    \centering
    \includegraphics[width =.8\textwidth]{figures/mw-attention.jpg}
    \caption{Illustration of upper and lower attention}
    \label{fig:att}
\end{figure}


In this section we introduce Cellular Attentional Message-passing, an attentional message passing scheme operating at edges level on the learned edges features of Eq. (\ref{eq:attentional_lift}) exploiting the connectivity given by the regular cell complex  computed via the cellular lifting map of Definition 5. Before describing the proposed scheme, please notice that, as previously introduced, we will perform an edge pooling operation after the message-passing round at each layer , meaning that the architecture will produce a sequence of cell complexes  such that   (due to the fact that the corresponding edge sets are such that ); we will describe the edge pooling in details in the next section. As already introduced in Section 2, there are two types of adjacencies that can be exploited when dealing with cell complexes. In particular, since the message exchange happen at the edges level, in each layer , our message-passing scheme exploits upper and lower edge adjacencies  and , associated with the cell complex ,  At each layer , we introduce a learnable upper attention function , responsible to evaluate the reciprocal importance of two edges that are part of the same polygon, and a lower attention function , responsible to evaluate the reciprocal importance of two edges that share a common node. Therefore, edges embedding are updated in the  message passing round as:



where  is any permutation invariant (aggregation) operator (e.g., sum, mean, max, ...),  is a (possibly) learnable function,  and   are  learnable functions sharing the weights with  and , respectively (as usual in attentional settings),  ,  (thus ). Obviously, multi-head attention can be trivially injected following the usual concatenation or averaging approach~\cite{giusti22,velivckovic2017graph}. A pictorial example of how upper and lower attention work is depicted in Figure \ref{fig:att}.





\subsection{Edge Pooling} 

 In this section, we present a self-attention edge pooling technique, adopting a variation of the method used in~\cite{lee2019self}. Let  be the hidden feature vector associated to edge  obtained via attentional message-passing after the -th message-passing round. The edge attention pooling operation consists in computing a self-attention score  for each edge of the complex via a pooling learnable attention function  : 

Let  be the \emph{pooling ratio}, i.e., the fraction of the edges that will be retained over the number of edges in input to the self-attention edge pooling layer. At this point, we keep the  edges belonging to the set   where  is the set of  the highest  self-attention scores. Finally, the  feature vectors that will be kept after the pooling stage are scaled as:

After the edge pooling, we consequently need  to adjust the structure of the cell complex  to obtain a consistent updated complex  . To this aim, we apply the procedure depicted in Fig. \ref{fig:pooling}: If an edge  belongs to  but  is not contained in , the lower connectivity is updated by disconnecting the nodes that are on the boundary of , while the upper connectivity is updated by removing the polygons that have  on their boundaries.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth, height = 4cm]{figures/pooling.jpg}
    \caption{Illustration of the proposed edge pooling procedure}
    \label{fig:pooling}
\end{figure}

Finally, we considered also a hierarchical version of the aforementioned self-attention edge pooling operation as in~\cite{cangea2018towards}. To this aim we employ a (by-layer) readout operation on the hidden feature   to obtain an aggregate embedding of the whole complex  as:
 
Then, after the last hidden layer, a final (global) readout operation is performed, e.g., by aggregating all the previously computed complexes embeddings: 

Finally, the result of the final aggregation is fed to a multi-layer perceptron (MLP) if needed for the learning task.



\subsection{CAN Architecture and Symmetries}
In summary, a \emph{Cell Attention Network} with input graph(s)  and input node features  is defined as the stack of: (i) a skeleton-preserving cellular lifting map to obtain a regular cell complex  from a graph ; (ii) a multi-head attentional lift to obtain edge features  from node features  ; (iii) a stack of  cell attention layers, each of them composed by a message passing round as in Eq. (\ref{message_passing_scheme}), an edge pooling stage as in Eq. (\ref{pooling_scores}) and Eq. (\ref{pooling_scaling}), and a (by-layer) readout function as in Eq. (\ref{local_readout}); (v) a (global) readout function, e.g. as in Eq. (\ref{global_readout}). A schematic view of the whole architecture is illustrated in Figure \ref{fig:tan}. Finally, we present the following result about equivariance properties of the proposed architecture:

\textbf{Theorem 1.} \textit{Cell Attention Networks are permutation invariant.}

In literature, a GNN is permutation invariant if a permutation of  nodes produces the same output without the permutation. More formally, a GNN  taking an input graph  with adjacency matrix  and input node features matrix  is (node) permutation invariant if  for any permutation matrix . In the same way, Cell Attention Networks are permutation invariant w.r.t. permutations of nodes, edges and polygons. 

\begin{proof}
We can assert, w.l.o.g., that Attentional Lift is permutation equivariant by construction. The operation  and  are both learnable functions acting on edges, and since  is symmetric by definition, both  and   are symmetric w.r.t. to the vertices that are endpoint of the edges we are considering. This leads to have the whole function permutation equivariant. 

The scheme followed in Edge Pooling is the selecting  element of edge set referring to self-attentional coefficients .
In order to select the  elements of , the vector  must be sorted. So no matter what is the permutation on the set, after sorting we obtain always the same result. For this reason Edge Pooling is permutation invariant. 

Finally, since the proposed CAN architecture is the composition of a permutation equivariant function (i.e., the attentional lift) and a permutation invariant function (i.e., the edge pooling), it readily follows that CAN are permutation equivariant. Please notice that the proposed architecture without the pooling stage is clearly permutation equivariant.
\end{proof}




\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth, height = 4cm]{figures/can.jpg}
    \caption{Illustration of a cell attention network}
    \label{fig:tan}
\end{figure}

\section{Experimental Results}\label{sec:exp_res}




\begin{table}[t]
\begin{center}
\caption{Details of the datasets used in our experiments.}
\label{tab:dataset_details}
\begin{tabular}{lccccc}
\toprule
\multicolumn{1}{c}{Info}  & \multicolumn{1}{c}{MUTAG} & \multicolumn{1}{c}{PTC} & \multicolumn{1}{c}{PROTEINS} & \multicolumn{1}{c}{NCI1} & \multicolumn{1}{c}{NCI109} \\  \bottomrule
\textbf{\# Graphs} & 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{} \\
\textbf{\# Classes}            & 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{}& 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{} \\
\textbf{\# Node Feat.}            & 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{}& 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{} \\
\textbf{\# Edge Feat.}            & 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{}& 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{} \\
\textbf{Avg. Nodes}            & 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{}& 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{} \\
\textbf{Avg. Edges}            & 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{}& 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{} \\
\textbf{Avg. 3 Cells.}            & 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{}& 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{} \\
\textbf{Avg. 4 Cells.}            & 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{}& 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{} \\
\textbf{Avg. 5 Cells.}            & 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{}& 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{} \\
\textbf{Avg. 6 Cells.}            & 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{}& 
\multicolumn{1}{c}{} & 
\multicolumn{1}{c}{} \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

In this section we asses the performance of the proposed architecture when solving several real-world graph classification problems, focusing on well known molecular benchmarks on TUDataset~\cite{TUDataset}. In every experiment, if the dataset is equipped with edge features, we concatenate them to the result of the lift layer (Eq. (\ref{eq:attentional_lift})). We included small molecules with class labels such as \textbf{MUTAG}~\cite{kazius2005derivation} and \textbf{PTC}~\cite{helma2001predictive}. In the former dataset, the task is  to identify mutagenic molecular compounds for potentially commercial drugs, while in the latter the goal is to identify chemical compounds based on their carcinogenicity in rodents. The \textbf{PROTEINS} dataset~\cite{dobson2003distinguishing} is composed mainly by macromolecules. Here, nodes represent secondary structure elements and are annotated by their type. Nodes are connected by an edge if the two nodes are neighbours on the amino acid sequence or one of three nearest neighbors in space; the task is to understand if a protein is an enzyme or not. Using these type of data in a Cell Complex based architecture has an underlying importance since molecules have polyadic structures. Finally, \textbf{NCI1} and \textbf{NCI109} are two datasets aimed at identifying chemical compounds against the activity of non-small lung cancer and ovarian cancer cells~\cite{wale2008comparison}. Considering the aforementioned datasets, we compare CAN with other state of the art techniques in graph representation learning. Since there are no official splits for the training and test sets, to validate the proposed architecture, we followed the method used in~\cite{bodnarcwnet}: we run a 10-fold cross-validation reporting the maximum of the average validation accuracy across folds.

\begin{table}[!ht]
\centering
\caption{Experimental results on TUDatasets. The first part shows the accuracy of graph kernel methods, while the second assess graph neural networks. Cell attention networks scores top on four out of five experiments.}\label{tab:exp-res}
\begin{tabular}{@{}|llllll|@{}}
\toprule
Dataset                         & MUTAG     & PTC      & PROTEINS & NCI1     & NCI109                      \\ \midrule
\multicolumn{1}{|l}{RWK}        & 79.2±2.1  & 55.9±0.3 & 59.6±0.1 & N/A  & \multicolumn{1}{l|}{N/A}      \\
\multicolumn{1}{|l}{GK(k=3)}    & 81.4±1.7  & 55.7±0.5 & 71.4±0.3 & 62.5±0.3 &  \multicolumn{1}{l|}{62.4±0.3} \\
\multicolumn{1}{|l}{PK}         & 76.0±2.7  & 59.5±2.4 & 73.7±0.7 & 82.5±0.5 &  \multicolumn{1}{l|}{N/A}      \\
\multicolumn{1}{|l}{WLK}  & 90.4±5.7  & 59.9±4.3 & 75.0±3.1 & 86.0±1.8 &  \multicolumn{1}{l|}{N/A}      \\ \midrule
\multicolumn{1}{|l}{DCNN}       & N/A       & N/A      & 61.3±1.6 & 56.6±1.0  & \multicolumn{1}{l|}{N/A}      \\
\multicolumn{1}{|l}{DGCNN}      & 85.8±1.8  & 58.6±2.5 & 75.5±0.9 & 74.4±0.5 & \multicolumn{1}{l|}{N/A}      \\
\multicolumn{1}{|l}{IGN}        & 83.9±13.0 & 58.5±6.9 & 76.6±5.5 & 74.3±2.7 &  \multicolumn{1}{l|}{72.8±1.5} \\
\multicolumn{1}{|l}{GIN}        & 89.4±5.6  & 64.6±7.0 & 76.2±2.8 & 82.7±1.7 &  \multicolumn{1}{l|}{N/A}      \\
\multicolumn{1}{|l}{PPGNs}      & 90.6±8.7  & 66.2±6.6 & 77.2±4.7 & 83.2±1.1 &  \multicolumn{1}{l|}{82.2±1.4} \\
\multicolumn{1}{|l}{NGN} & 89.4±1.6  & 66.8±1.7 & 71.7±1.0 & 82.4±1.3 & \multicolumn{1}{l|}{N/A}      \\
\multicolumn{1}{|l}{GSN}        & 92.2±7.5  & 68.2±7.2 & 76.6±5.0 & 83.5±2.0 & \multicolumn{1}{l|}{N/A}      \\
\multicolumn{1}{|l}{SIN}        & N/A       & N/A      & 76.4±3.3 & 82.7±2.1 & \multicolumn{1}{l|}{N/A}      \\
\multicolumn{1}{|l}{CIN}        & 92.7±6.1  & 68.2±7.2 & 77.0±4.3 & 83.6±1.4  & \multicolumn{1}{l|}{} \\ \midrule
CAN                      &  ±          &  ±         &  ±        & ±    & ±                      \\ \bottomrule
\end{tabular}
\end{table}







The performance of CAN is reported in Table \ref{tab:exp-res}, along with those of graph kernel methods: Random Walk Kernel (RWK,~\cite{gartner2003graph}), 
Graph Kernel (GK,~\cite{shervashidze2009efficient}),
Propagation Kernels (PK,~\cite{neumann2016propagation}), 
Weisfeiler-Lehman graph kernels (WLK,~\cite{shervashidze2011weisfeiler}); other GNNs: Diffusion-Convolutional Neural Networks (DCNN,~\cite{atwood2016diffusion}),  Deep Graph Convolutional Neural Network (DGCNN,~\cite{zhang2018end}), Invariant and Equivariant Graph Networks (IGN,~\cite{maron2018invariant}), Graph Isomorphism Networks (GIN,~\cite{xu2018powerful}), Provably Powerful Graph Networks (PPGNs,~\cite{maron2019provably}), Natural Graph Networks (NGN,~\cite{de2020natural}), Graph Substructure Network (GSN~\cite{bouritsas2022improving}) and topological networks: Simplicial Isomorphism Network (SIN,~\cite{bodnar2021weisfeiler},
Cell Isomorphism Network (CIN,~\cite{bodnarcwnet}). As we can see from Table \ref{tab:exp-res}, CAN achieves the best performance on four out of five benchmarks, while performing very similarly to CIN in the last experiment (i.e., NCI109). Since CAN has a much lower computational complexity than CIN (cf. Appendix C), these results support the validity and the performance obtained of the proposed architecture. The tested  models have been implemented using PyTorch~\cite{pytorch19}. The datasets have been taken from the PyTorch Geometric library~\cite{pyg2019}. The operations involved during cellular lifting maps use the code provided by \cite{bodnarcwnet} under MIT license.
PyTorch, NumPy, SciPy and are made available under the BSD license, Matplotlib under the PSF license, graph-tool under the GNU LGPL v3 license. PyTorch Geometric is made available under the MIT license. All the experimental results have been made on NVIDIA® GeForce RTX 3090 GPUs  with 10,496 CUDA cores and 24GB GPU memory. The operative system used for the experiment was Ubuntu 22.04 LTS 64-bit. See Appendix \ref{sec:additional_exp} for an extensive description of the tested architectures and an ablation study \footnote{The code implementation for the proposed architecture is available at:  \url{https://github.com/lrnzgiusti/can}}. 


\section{Conclusion and Discussion}

In this work we presented \emph{Cell Attention Networks} (CANs),  novel neural architectures operating on data defined over the nodes of a graph incorporated into a
a regular cell complex, exploiting generalized masked self-attention mechanisms. It builds on skeleton-preserving cellular lifting maps, a novel attentional features lift  and a novel edge-level attentional message-passing scheme with two attention functions that operate on the upper and lower connectivities induced by the cell complex.
 The proposed architecture is also equipped with a novel hierarchical edge pooling technique that leverage a self-attention mechanism to downsample the data in the network's hidden layers while extracting significant features for the learning task. The Cell Attention Network architecture proposed and tested in the previous sections shows promising results and it is grounded in the theory of regular cell complexes; however, some directions can be explored to enrich the proposed formulation. In particular, a signal processing perspective~\cite{sardellitti2022cell} can be exploited to reinterpret
 and modify the proposed architecture following a similar approach to~\cite{giusti22}; an expressivity analysis can be carried out based on the renewed Weisfeiler-Lehman approach~\cite{xu2018powerful}, on its generalization to cell complexes~\cite{bodnarcwnet,hajij2022}, or based on spectral approaches~\cite{ribeiro2022aremore}. We leave these problems to be addressed in future works.




\bibliographystyle{unsrtnat}
\bibliography{reference}


\appendix


\section{Experimental Details}\label{sec:additional_exp}


\subsection{Model Implementation}

In our experiments, we employ cell attention networks to regular cell complexes of order two obtained by applying the structural lifting map to the original graphs, i.e. we consider nodes as 0-cells and edges as 1-cells, and the chordless cycles of size up to  as 2-cells. In our case, each node of the original graphs is always equipped with an input feature vector.


Throughout all experiments, we employ cell attention networks with the following structure. The attentional lift mechanism in Eq. (\ref{eq:attentional_lift}) is given by:




where  is the input feature vector of the edge . If not provided by the specific benchmark,  can be considered as an empty vector. Also,  is the vector of attention coefficients associated to the k-th feature of the input edge feature vector, and  is the non-linear activation function for the lift layer. Please notice that the employed functions  are not symmetric, but they give the best learning performance on the proposed tasks.

The lower and upper attentional  functions  and  in Eq. (\ref{message_passing_scheme}) are chosen as two independent masked self-attention schemes. They can be chosen following any of the known approaches from graphs~\cite{velivckovic2017graph, brody2021attentive}. In this paper we follow the approach from \cite{velivckovic2017graph}: formally, let




where  are two independent vectors of attention coefficients,  are two learnable linear transformations shared by the lower and upper neighbourhoods of the complex, respectively, and  is a pointwise non-linear activation. The coefficients  and  in Eq. (\ref{eq:low_omega},\ref{eq:up_omega}) represent the importance of the features of edge k when exchanging messages with edge e over lower and upper neighborhoods, respectively. It worth to emphasize that since the attention schemes are decoupled, these importance coefficients will be different over the upper and lower neighborhoods. 

In line with the approach of \cite{velivckovic2017graph}, we make coefficients easily comparable across different edge by normalizing them across all choices of  using the softmax function:

5pt]
    &\alpha_{e,k}^{l,u} = \frac{\exp\left( \omega_{e,k}^{l,u} \right) } 
    {\sum_{\iota \in \mathcal{N}_{u}^{l}(e)} \exp\left( \omega_{e,\iota}^{l,u}   \right)}

    &\widetilde{\mathbf{h}}_{e}^{l} = \phi\Bigg((1+\varepsilon) \, \mathbf{W}_{s}^{l} \mathbf{h}_{e}^{l} + \sum_{k \in \mathcal{N}^l_d(e)} \alpha_{e,k}^{l,d} \, \mathbf{W}_{d}^{l} \mathbf{h}_{k}^l + \sum_{k \in \mathcal{N}^l_u(e)} \alpha_{e,k}^{l,u} \, \mathbf{W}_{u}^{l} \mathbf{h}_{k}^l)\Bigg),

    &\mathbf{h}_{e}^{l+1} = \underbrace{\phi_p \left( ( \mathbf{a}_p^l )^T \tilde{\mathbf{h}}_e^l  \right)}_{\gamma_e^l}  \tilde{\mathbf{h}}_e^l, \quad \forall e \in \mathcal{E}^{l+1},

    \mathbf{h}_{\mathcal{C}}= \mathbf{h}_{\mathcal{C}^{L-1}} = \sum_{e \in \mathcal{E}^{L-1}} \mathbf{h}_{e}^{L-1}.



Once  is obtained, it is forwarded into a 2-Layer MLP with  as activation function to perform the prediction.

In all layers, we adopt a Batch Normalization technique~\cite{ioffe2015batch} and all training operations are performed with the AdamW optimization algorithm~\cite{loshchilov2017decoupled}. In Table \ref{tab:hyper-params} we report the hyper-parameters used in our experiments for each dataset.

\begin{table}[t]
\centering
\caption{Hyperparameter used for the experiments on TUDatasets.}
\label{tab:hyper-params}
\begin{tabular}{lccccc}
\toprule 
\multicolumn{1}{l}{Parameter}  & \multicolumn{1}{c}{MUTAG} & \multicolumn{1}{c}{PTC} & \multicolumn{1}{c}{PROTEINS} & \multicolumn{1}{c}{NCI1} & \multicolumn{1}{c}{NCI109} \\ \bottomrule
Lift Heads                   & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\
Lift Activation            & \multicolumn{1}{c}{\textit{ReLU}} & \multicolumn{1}{c}{\textit{ELU}} & \multicolumn{1}{c}{\textit{ELU}} & \multicolumn{1}{c}{\textit{ELU}} & \multicolumn{1}{c}{\textit{Sigmoid}} \\
Lift Dropout                    & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{}\\
Hidden Features           & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\
Attention Heads             & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\
Attention Aggregation      & \multicolumn{1}{c}{\textit{-}} & \multicolumn{1}{c}{\textit{cat}} & \multicolumn{1}{c}{\textit{-}} & \multicolumn{1}{c}{\textit{cat}} & \multicolumn{1}{c}{\textit{cat}} \\
Attention Activation       & \multicolumn{1}{c}{\textit{LReLU}} & \multicolumn{1}{c}{\textit{LReLU}} & \multicolumn{1}{c}{\textit{Tanh}} & \multicolumn{1}{c}{\textit{Tanh}} & \multicolumn{1}{c}{\textit{Tanh}} \\
Activation                & \multicolumn{1}{c}{ELU} & \multicolumn{1}{c}{ELU} & \multicolumn{1}{c}{Tanh} & \multicolumn{1}{c}{ELU} & \multicolumn{1}{c}{GELU} \\
MLP Neurons                & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\
Batch Size                 & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{}\\
Neg. Slope                & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{}\\
Pool Ratio         & 
\multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\
Pool Type               & \multicolumn{1}{c}{\textit{Hier.}} & \multicolumn{1}{c}{\textit{Glob.}} & \multicolumn{1}{c}{\textit{Hier.}} & \multicolumn{1}{c}{\textit{Glob.}} & \multicolumn{1}{c}{\textit{Glob.}} \\
Dropout                    & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{}\\
Learning Rate               & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{}    \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Complexity Analysis}
In this section, we review the computational complexity of each operation involved the proposed architecture referring to the model implementation define above:


\textbf{Cellular Lifting Map}: Although this operation can be precomputed for the entire dataset and the connectivity results stored for a later usage, it worth to elicit its complexity noticing that for some applications the storage of the upper and lower connectivity for the \emph{entire} dataset might be not possible. We considered Cellular Lifting Maps that assign 2-cells to all the chordless cycles of a graph with a maximum number of nodes in the cycles up to  as maximum cycle size. The chord-less cycles in a graph can thus be enumerated in  time~\cite{ferreira14}. Similar to~\cite{bodnarcwnet}, in our experimental setup we have that  can upper bounded by a small constant. Thus, the complexity of this operation can be approximated to be linear in the size of the complex.

\textbf{Attentional Lift}: The complexity of this operation consists of a multi-head attention message passing scheme over the entire graph~\cite{velivckovic2017graph}. For a single node pair  connected by an edge , the attentional lift defined in Eq. (\ref{eq:attentional_lift}) can be decomposed into  independent self-attention schemes. Each attention scheme requires  computations, where  is the number of input node features. Thus, for the pair , the attentional lift is performed in , where  is a parameter to be chosen as the number of input edge features. Accounting all the edges of the complex yields an amount of  operations to lift the given node features into edge ones.

\textbf{Cell Attention Layer}: This operation consists in two independent masked self-attention message passing schemes over the upper and lower neighbourhoods of the complex, namely cell attention, an inner linear transformation of the edges' features and an outer point-wise nonlinear activation (Eq.(\ref{message_passing_scheme})). For the lower neighbourhood, a single edge  receives at most  messages for each cell in its coboundary,  (cf Fig. \ref{fig:att}.1 ); thus, for a single edge the attention over the lower neighbourhood of the complex is , where  is the number of cells that are co-faces of the edge . Regarding the upper neighbourhood, if  is the maximum ring size we have that a single edge  receives  messages (cf. Fig. \ref{fig:att}.2. Recalling that  is upper bounded by a small constant~\cite{bodnarcwnet}, cell attention is an  operation for both neighbourhoods of an arbitrary edge  i.e. linear in the size of the complex. The inner linear transformation that propagates the information contained in  is upper bounded by . Extending this to all edges of the complex, we have that the complexity of a cell attention layer can be rewritten as . In the case of a multi-head cell attention, the complexity receives an overhead induced by the number of attention heads involved within the layer, i.e., a multiplication by a factor , the number of cell attention heads.
 

\textbf{Attentional Pooling}: The operations involved in the pooling layer can be decomposed in: (i) computing the self-attention scores for each edge of the complex ( in  Eq. (\ref{pooling_scores})); (ii) select the highest  values from a collection of self-attention scores (); and (iii) adjust the connectivity of the complex (see Fig. \ref{fig:pooling}). To compute the computational complexity of this layer it is convenient to see the selection operation as a combination of a sorting algorithm over a collection of self-attention scores and a selection of the first  elements from the sorted collection. Since the computations involved in (i) and (iii) are linear in the dimension of the complex, the overall complexity of this layer in can be upper bounded by the sorting algorithm, i.e., .


In practice, all the computations involved in a cell attention network are local formulations completely disjoint from each other. Thus, using an efficient GPU-based implementation, we can rewrite \emph{all} the analysis in terms of the longest sequential chain of operations in a concurrent execution over the edges of the input domain, i.e., . For an in-depth concurrency analysis we refer readers to~\cite{Besta22parallel}, where the authors report a complete taxonomy of parallelism in GNNs.   

\subsection{Learnable Parameters}

The total number of learnable parameters of a CAN can be decomposed into:

\textbf{Cellular Lifting Map}: Lifting the input graph  to a cell complex  is an operation that assign a cell  to \emph{all} the chord-less cycles of  up to a maximum cycle size . Intuitively this operation does not involve any parameter to be learned during the network's training phase. Thus, the number of learnable parameters for the lift is .

\textbf{Attentional Lift}: In the context of lifting a pair of graph node features  to a signal defined over the edge of the complex, , we have to learn a vector of attention coefficients  for each input edge feature. The vector  has a number of learnable parameters in . Accounting multiple input edge features, the overall number of parameters for the attention lift operation is , where  is the number of input edge features computed as multiple independent attention heads.

\textbf{Cell Attention}: In terms of learnable parameters, a single \emph{cell attention layer} is composed of: two independent vectors of attention coefficients  for properly weighting the lower and upper neighbourhoods, respectively. Moreover, the layer is equipped with three linear transformations,  acting respectively on: , the hidden feature vector of edge  at layer  and the hidden feature vectors  in the lower and upper neighbourhoods of the edge . Thus, the number of learnable parameters of a cell attention layer is .

\textbf{Pooling}: For this layer, learnable parameters are employed only in computing the self-attention scores (, Eq. (\ref{pooling_scores})). The shared vector of attentional scores' coefficients , similarly to the lift layer, is known to have a number of learnable parameters in .


\subsection{Ablation Study}\label{sec:ablation}

In this section we take a detailed look at the performance of each operation involved in cell attention networks by performing different ablation studies and show their individual importance and contributions. 



\begin{figure}[!htb]
    \centering
    \includegraphics[width=.8\textwidth]{figures/ablation.png}
    \caption{TUDataset: Results of the ablation of different CAN features with respect to Table \ref{tab:exp-res} (g.t.). The ablation study shows the benefits of incorporating all the proposed operations into the message passing procedure when operating on data defined over cell complexes.}
    \label{fig:ablation}
\end{figure}


In particular, we followed the same experimental setup used in section \ref{sec:exp_res} by fixing the hyper-parameters as in Table \ref{tab:hyper-params} and removing one-by-one the cell attention network operations: (i) removing the lift refers to assign a feature  to an edge  using a linear function that takes the feature vectors   from the vertices , i.e. a simple scalar product between  and  (); (ii) removing the lower attention can be intended as initializing the lower attention coefficients as:  and left them unmodified during the update step in the training phase; (iii) similarly, for the upper attention we replicated the idea of (ii) but now only the upper attention coefficients are involved, i.e.   and kept fixed for the entire optimization stage; (iv) removing the attention means to remove both the upper and lower attention simultaneously as explained in (ii) and (iii); removing the pooling means to detach the pooling layer from the network and remove eventual intermediate readout computations involved in the hierarchical pooling setup.

As shown in Figure \ref{fig:ablation} and in Table \ref{tab:ablation}, we observe a decrease in the overall performance when removing parts of the cell attention network architecture as expected. Of particular interest is the ablation study on NCI1, which shows a slightly higher accuracy in every case we kept the attention coefficients fixed and without the pooling but a drastic drop in the performance when the edge features are no longer learned. Moreover we see that there are no evident "patters" inside the ablation study with the except that for NCI109 we observe the same behaviour of NCI1 when removing the lift layer. This fact can be explained by noticing that the aforementioned datasets experience, on average, a very similar topology (Table \ref{tab:dataset_details}).

\begin{table}[t]
\centering
\caption{Analysis of the impact of the operations involved in cell attention networks.}
\label{tab:ablation}
\begin{tabular}{lccccc}
\toprule
\multicolumn{1}{l}{Feature Removed}  & \multicolumn{1}{c}{MUTAG} & \multicolumn{1}{c}{PTC} & \multicolumn{1}{c}{PROTEINS} & \multicolumn{1}{c}{NCI1} & \multicolumn{1}{c}{NCI109} \\
\bottomrule 
No Pooling                   & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\
No Attention            & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{ } & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\
No Upper Attention                    & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{}\\
No Lower Attention           & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\
No Lift             & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{
} & \multicolumn{1}{c}{} \\
\bottomrule
\end{tabular}
\end{table}


\end{document}
