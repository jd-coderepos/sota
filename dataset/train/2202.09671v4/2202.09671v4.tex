
\documentclass{article} \usepackage{iclr2023_conference,times}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\ba}[1]{}
\newcommand{\baa}[1]{}
\newcommand{\beq}[1]{}
\newcommand{\bsplt}[1]{\begin{split}#1\end{split}}
\newcommand{\abs}[1]{\lvert #1\rvert}
\newcommand{\bE}{\mathop{\mathbb{E}}}
\newcommand{\given}{\,|\,}
\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{(\ref{#1})}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{bbold}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hhline}
\usepackage{multirow}
\usepackage{stmaryrd}
\usepackage{enumitem,pifont}
\usepackage{etoolbox}
\usepackage{wrapfig}
\usepackage{color, colortbl}
\definecolor{Gray}{gray}{0.93}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{arydshln}
\usepackage{amsmath}
\usepackage{amsthm}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\newcommand{\mz}[1]{{\color{orange}{[mz: #1]}}}
\newcommand{\hz}[1]{{\color{cyan}{[hz: #1]}}}

\title{Truncated Diffusion Probabilistic Models and Diffusion-based Adversarial Auto-Encoders}
\iclrfinalcopy

\author{Huangjie Zheng \& Pengcheng He \& Weizhu Chen \& Mingyuan Zhou \\
The University of Texas at Austin, Microsoft Azure AI\\
\texttt{huangjie.zheng@utexas.edu, penhe@microsoft.com,}\\\texttt{wzchen@microsoft.com, mingyuan.zhou@mccombs.utexas.edu} \\
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}


\maketitle

\begin{abstract}
Employing a forward diffusion chain to gradually map the data to a  noise distribution, diffusion-based generative models learn how to generate the data by inferring a reverse diffusion chain.
However, this approach is slow and costly because it needs many forward and reverse steps. We propose a faster and cheaper approach that adds noise not until the data become pure random noise, but until they reach a hidden noisy-data distribution that we can confidently learn. Then, we use fewer reverse steps to generate data by starting from this hidden distribution that is made similar to the noisy data. We reveal that the proposed model can be cast as an adversarial auto-encoder empowered by both the diffusion process and a learnable implicit prior. Experimental results show even with a significantly smaller number of reverse diffusion steps, the proposed truncated diffusion probabilistic models can provide consistent improvements over the non-truncated ones in terms of performance in both unconditional and text-guided image generations. 










\end{abstract}

\section{Introduction}\label{sec:intro}

Generating photo-realistic images with  probabilistic models is a challenging and important task in machine learning and computer vision, with many potential applications in data augmentation, image editing, style transfer, \textit{etc}.
Recently, a new class of image generative models based on diffusion processes \citep{diffusion} has achieved remarkable results on various 
commonly used image generation benchmarks~\citep{scorematching,ddpm,improvedscore,song2021scorebased,Dhariwal2021DiffusionMB}, surpassing many existing deep generative models, such as autoregressive models \citep{pixelcnn}, variational auto-encoders (VAEs)~\citep{kingma2013auto,rezende2014stochastic,van2017neural,razavi2019generating}, and generative adversarial networks (GANs)~\citep{goodfellow2014generative,radford2015unsupervised,arjovsky2017wasserstein,miyato2018spectral,brock2018large,karras2019style,karras2020analyzing}. 

This new modeling class, which includes both  score-based and diffusion-based generative models, uses noise injection to gradually corrupt the data distribution into a simple noise distribution that can be easily sampled from, and then uses a denoising network to reverse the noise injection to generate photo-realistic images.  From the perspective of score matching \citep{hyvarinen2005estimation,vincent2011connection} and Langevin dynamics \citep{neal2011mcmc,welling2011bayesian}, the denoising network is trained by matching the score function, which is the gradient of the log-density of the data, of the corrupted data distribution and that of the generator distribution at different noise levels \citep{scorematching}. This training objective can also be formulated under diffusion-based generative models \citep{diffusion,ddpm}. 
These two types of models
have been further unified by \citet{song2021scorebased} under the framework of discretized stochastic differential equations.





Despite their impressive performance, diffusion-based (or score-based) generative models suffer from high computational costs, both in training and sampling. This is because they need to perform a large number of diffusion steps, typically hundreds or thousands, to ensure that the noise injection is small enough at each step to make the assumption 
that both the diffusion and denoising processes have the Gaussian form  hold in the limit of small diffusion rate~\citep{feller1949theory,diffusion}.
In other words,  when the number of diffusion steps is small or when the rate is large, the Gaussian assumption may not hold well, and the model may not be able to capture the true score function of the data. Therefore, previous works have tried to reduce the number of diffusion steps by using non-Markovian reverse processes \citep{ddim, kong2021fast}, adaptive noise scheduling \citep{san2021noise,kingma2021variational}, knowledge distillation \citep{luhman2021knowledge,salimans2022progressive}, diffusing in a lower-dimension latent space \citep{rombach2022high},  \textit{etc.}, but they still cannot achieve significant speedup without sacrificing the generation quality.


In this paper, we propose a novel way to shorten the diffusion trajectory by learning an implicit distribution to start the reverse diffusion process, instead of relying on a tractable noise distribution. We call our method truncated diffusion probabilistic modeling (TDPM), which is based on the idea of truncating the forward diffusion chain of an existing diffusion model, such as the denoising diffusion probabilistic model (DDPM) of \citet{ddpm}.  To significantly accelerate diffusion-based text-to-image generation, we also introduce the truncated latent diffusion model (TLDM), which truncates the diffusion chain of the latent diffusion model (LDM) of \citet{rombach2022high}. We note LDM is the latent text-to-image diffusion model behind \href{https://stability.ai/blog/stable-diffusion-public-release}{\texttt{Stable Diffusion}}, an open-source project that provides state-of-the-art performance in generating photo-realistic images given text input.  
By truncating the chain, we can reduce the number of diffusion steps to an arbitrary level, but at the same time, we also lose the tractability of the distribution at the end of the chain. Therefore, we need to learn an implicit generative distribution that can approximate this distribution and provide the initial samples for the reverse diffusion process. We show that this implicit generative distribution can be implemented in different ways, such as using a separate generator network or reusing the denoising network. The former option has more flexibility and can improve the generation quality, while the latter option has no additional parameters and can achieve comparable results.



\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{image/motiv2.pdf}
    \vspace{-8mm}
    \caption{{\small (\textit{Best viewed in color}) An illustrative depiction of diffusion models and our truncated diffusion models.  \textbf{\textit{Top}}: The conventional denoising diffusion models add Gaussian noise gradually with a large number of time steps, where the true posterior can be kept close to Gaussian and hence easy to fit with denoising (score-matching) loss (marked in a solid blue box). \textbf{\textit{Bottom}}: Truncated diffusion models truncate the diffusion chain to keep its first few steps and small diffusion segment (marked in the dashed blue box). This truncated diffusion chain can still be learned with previous denoising methods. Meanwhile, as the left part is truncated, the Gaussian prior  will have a large gap to the truncated point , which is bridged with an implicit generative distribution  (marked in dashed red box).}}
    \label{fig:motivation}
    \vspace{-5mm}
\end{figure*}


We reveal that DDPM and VAE have a similar relationship as TDPM and adversarial auto-encoder (AAE, \citet{makhzani2015adversarial}). Specifically, DDPM is like a VAE with a fixed encoder and a learnable decoder that use a diffusion process, and a predefined prior. TDPM is like an AAE with a fixed encoder and a learnable decoder that use a truncated diffusion process, and a learnable implicit prior. 


{Our truncation method has several advantages when we use it to modify DDPM for generating images without text guidance or LDM for generating images with text guidance. First, it can generate samples much faster by using fewer diffusion steps, without sacrificing or even enhancing the generation quality. Second, it can exploit the cooperation between the implicit model and the diffusion model, as the diffusion model helps the implicit model train by providing noisy data samples, and the implicit model helps the diffusion model reverse by providing better initial samples. Third, it can adapt the truncation level to balance the generation quality and efficiency, depending on the data complexity and the computational resources. 
For generating images with text guidance, our method can speed up the generation significantly and make it suitable for real-time processing: while LDM takes the time to generate one photo-realistic image, our TLDM can generate more than 50 such images.}








The main contributions of our paper are as follows:
\begin{itemize}
    \item We introduce TDPM, a new diffusion-based generative model that can shorten the diffusion trajectory by learning an implicit distribution to start the reverse diffusion process, and demonstrate that the learning of the implicit distribution can be achieved in various ways. We further introduce TLDM to significantly accelerate diffusion-based text-to-image generation.
    \item We show TDPM can be formulated as a diffusion-based AAE.
    \item We show that the implicit distribution can be realized by reusing the denoising network for the reverse diffusion process, which can reduce the reverse diffusion steps by orders of magnitude without adding any extra parameters and with comparable generation quality. 
    \item We reveal the synergy between the implicit model and the diffusion model, as the diffusion process can simplify the training of the implicit model like GANs, and the implicit model can speed up the reverse diffusion process of the diffusion model.
    \item We show that both TDPM and TLDM can adapt the truncation level, according to the data complexity and the computational resources, to achieve a good balance between the generation quality and the efficiency.
\end{itemize}

\section{Preliminaries on diffusion  models}\label{sec:background}
In Gaussian diffusion  models \citep{diffusion,ddpm}, starting from the data distribution ,  a pre-defined forward diffusion process   produces auxiliary variables  by gradually adding Gaussian noise, with variance  at time , as follows:
\ba{
 q(\rvx_1, ..., \rvx_T \given \rvx_0) &\coloneqq    \textstyle\prod_{t=1}^{T} q(\rvx_t \given \rvx_{t-1}) \label{eq:joint} ,~~~
    q(\rvx_t \given \rvx_{t-1}) \coloneqq \mathcal{N}(\rvx_t; \sqrt{1-\beta_t} \rvx_{t-1}, \beta_t \mI). }
With the limit of small diffusion rate (,  is kept sufficiently small), the reverse distribution   also follows a Gaussian distribution \citep{feller1949theory,diffusion} and can be approximated using a neural network parameterized Gaussian distribution  as:
\beq{
\label{eq:nn}
p_{\theta}(\rvx_{t-1}\given \rvx_t) \coloneqq \mathcal{N}(\rvx_{t-1}; \mu_{\theta}(\rvx_t, t), \Sigma_{\theta}(\rvx_t, t)).
}
Moreover, with a  sufficiently large , the outcome of the diffusion chain  will follow an isotropic Gaussian distribution. Thus, with the pre-defined forward (inference) diffusion process and the learned reverse (generative) diffusion process, we can sample from  and run the diffusion process in reverse to get a sample from the data distribution .

Under the variational inference \citep{kingma2013auto,blei2017variational} framework, viewing   in \Eqref{eq:joint} as the inference network, we can use the evidence lower bound (ELBO) as our learning objective. Following previous works \citep{diffusion,ddpm}, the negative ELBO of a diffusion probabilistic model, parameterized by , can be expressed as
\ba{
    \gL_{\text{ELBO}}(\theta) &\coloneqq \gL_0(\theta) +   \textstyle \sum_{t=2}^T\gL_{t-1}(\theta) + \gL_T \label{eq:loss},~~~\gL_0(\theta) \coloneqq \E_{q(\rvx_{0})}\E_{q(\rvx_{1}\given \rvx_0)} \left[-\log p_{\theta}(\rvx_0 \given  \rvx_1)\right] , \\
                     \gL_{t-1}(\theta)&\coloneqq 
                    \E_{q(\rvx_{0})}\E_{q(\rvx_{t}\given \rvx_0)} [ \KL\left({q(\rvx_{t-1}\given \rvx_t,\rvx_0)}||{p_{\theta}(\rvx_{t-1}\given \rvx_t)}\right) \label{eq:losst}],~~~t\in\{2,\ldots,T\} \\
                     \gL_T&\coloneqq \E_{q(\rvx_{0})} [ \KL\left({q(\rvx_T \given  \rvx_0)}||{p(\rvx_T)}\right)] \label{eq:lossT},
}
where  denotes the Kullback--Leibler (KL) divergence from distributions  to . Generally speaking, diffusion probabilistic models assume the number of diffusion steps  to be sufficiently large to satisfy two conditions: 1) the reverse distribution at each denoising step can be fitted with a Gaussian denoising generator ; 2) with a sufficiently small diffusion rate , the long forward diffusion process will successfully corrupt the data, making , and hence approximately  becomes zero and depends on neither  nor~. 

\textbf{What happens if  is insufficiently large?}
Given a non-Gaussian data distribution ,
when the number of denoising steps is reduced, the true posterior  is not Gaussian and usually intractable~\citep{feller1949theory}, resulting in new challenges to current diffusion models.
As noted in~\citet{xiao2021tackling}, when  is not sufficiently small, the diffusion step becomes larger and the denoising distribution can be multi-modal and hence too complex to be well fitted by Gaussian.
The authors propose to define  with an implicit generator and substitute the ELBO with
\ba{\label{denoising gan}
\min _{\theta}    \textstyle \sum_{t \geq 1} \mathbb{E}_{q(t)}\left[ D_{\mathrm{adv}}\!\left(q(\rvx_{t-1} \given \rvx_{t}) \| p_{\theta}(\rvx_{t-1} \given \rvx_{t})\right)\right],
}
where  represents a statistical distance that relies on an adversarial training setup. This modified objective can be minimized by leveraging the power of conditional GANs in fitting implicit multimodal distributions~\citep{arjovsky2017wasserstein, goodfellow2014generative, nowozin2016f-gan}.
While the concept of diffusion has been used,
the proposed models in \citet{xiao2021tackling} are shown to work the best only when the number of diffusion steps is limited to be as few as four, and start to exhibit deteriorated performance when further increasing that number. 


\section{Truncated diffusion and adversarial auto-encoding }\label{sec:method}
We first introduce the idea of accelerating both the training and generation of diffusion models by truncating the diffusion chains and describe the technical challenges. We then develop the objective function and training algorithm for TDPM. We further reveal TDPM can also be formulated as an AAE \citep{makhzani2015adversarial}) empowered by diffusion models. While DDPM can be considered as a hierarchical version of a variational auto-encoder (VAE) with a fixed multi-stochastic-layer encoder, our derivation shows that TDPM can be considered as a hierarchical version of an AAE with a fixed multi-stochastic-layer encoder but a learnable implicit prior.

\subsection{Motivation and technical challenges}
{


{We propose a novel method called TDPM to speed up the diffusion process and the generative model. The main idea is to shorten the forward diffusion chain that transforms the data into Gaussian noise, and use a learned implicit distribution to sample the starting point of the reverse diffusion chain that reconstructs the data. 
To be more precise, we adopt the DDPM framework that defines a variance schedule , which controls the amount of noise added at each step of the forward diffusion process. The forward process has a simple analytical form as a Gaussian distribution:

Here,  is the noisy version of the data  at step , and  is the cumulative product of the diffusion coefficients . The forward chain of length  is designed to be long enough to make the data distribution indistinguishable from Gaussian noise . However, a long forward chain also implies a high computational cost for the reverse process, which uses a learned neural network to predict the conditional distribution of the clean data given the noisy one at each step. 
}

{
The proposed TDPM cuts off the last part of the forward chain and only keeps the first  steps . We choose  to be much smaller than  so that we can save a lot of computation time in generation. The benefit of this truncation is illustrated in \Figref{fig:motivation}, where the bottom row shows the truncated diffusion chain. We can see that the data are only partially corrupted by noise and still retain some features of the original data. This means that we can recover the data more easily and accurately by applying a few Gaussian denoising steps from the corrupted data. Moreover, we do not change the diffusion rates  for the first  steps, so we do not compromise the quality of the forward and reverse processes between time   and .

However, truncating the forward chain also introduces a new challenge for the reverse process. Unlike the original chain, where the starting point of the reverse process is 
,
the truncated chain has an unknown distribution of the corrupted data at step . This makes it difficult to sample from this distribution and initiate the reverse process. To overcome this challenge, we introduce an implicit generative model that approximates the distribution of the corrupted data by minimizing a divergence measure between the implicit and the true noisy distributions at step . This way, we can use the implicit model to sample the starting point of the reverse process and then apply the learned denoising network to generate the data.} 

\subsection{Hand-crafted TDPM objective function}
Mathematically, recall that the DDPM loss in \Eqref{eq:loss} consists of three terms: , , and . The training objective of a conventional diffusion model focuses on terms  and . It assumes  does not depend on any parameter and will be close to zero by carefully pre-defining the forward noising process such that . 

When the diffusion chains are truncated 
at time , the forward diffusion ends at time , where the marginal distribution of the forward diffusion-corrupted data can be expressed as
\ba{
q(\rvx_{T_\text{trunc}}) \coloneqq  \textstyle\int q(\rvx_{T_\text{trunc}}\given \rvx_0) p(\rvx_0) d\rvx_0, 
\label{eq:aggregated_post}
}
which takes a semi-implicit form \citep{yin2018semi} whose density function is often intractable.
To reverse this truncated forward diffusion chain, we can no longer start the reverse diffusion chain from a known distribution such as . To this end, we propose TDPM that starts the reverse chain at time   from , an implicit distribution parameterized by . 
We match  to  via a loss term as
 where  is a statistical distance between distributions  and , such as the Jensen--Shannon divergence and Wasserstein distance. As we keep all the diffusion steps before time  in TDPM the same as those in DDPM, we combine  with all the loss terms of DDPM before time  in \Eqref{eq:loss} to define the TDPM loss  as \ba{
{\gL}_{\text{TDPM}} &\coloneqq \textstyle \sum_{t=1}^{T_\text{trunc}}\gL_{t-1}(\theta) + \tilde\gL_{T_\text{trunc}}(\psi), ~~ \tilde{\gL}_{T_\text{trunc}}(\psi) \coloneqq \mathcal{D}\left(q(\rvx_{T_\text{trunc}})||p_{\psi}(\rvx_{T_\text{trunc}} )\right),\label{eq:Elbo_tilde}
}


We note while in general  in TDPM is intractable, we can employ
a deep neural network-based generator  to generate a random sample in a single step via
\ba{
\rvx_{T_\text{trunc}} = G_{\psi}(\rvz),~\rvz\sim \mathcal{N}(\vzero, \mI).
}
We will discuss later that we may simply let  to avoid adding more parameters.





\subsection{TDPM as diffusion-based adversarial auto-encoder}
Following the terminology of AAE, let us define the prior as , the decoder (likelihood) as  
\ba{
\textstyle
p_{\theta}(\rvx_{0}\given \rvx_{T_\text{trunc}})  \coloneqq  \int\ldots\int  \big[\prod_{t=1}^{T_\text{trunc}}p_{\theta}(\rvx_{t-1}\given \rvx_{t})\big] d \rvx_{T_\text{trunc}-1}\ldots d \rvx_{1},
\label{eq:decoder}
} which is empowered by a reverse diffusion chain of length , and the encoder (variational posterior) as  . Thus we can view  defined in \Eqref{eq:aggregated_post} as the aggregated posterior \citep{hoffman2016elbo,tomczak2018vae}.
In addition to imposing an auto-encoding data-reconstruction loss, the key idea of 
the AAE \citep{makhzani2015adversarial} is to also match the aggregated posterior to a fixed prior. This idea differs AAE from a VAE that regularizes the auto-encoder by matching the variational posterior to a fixed prior under the KL divergence.
To this end, we introduce a diffusion-based AAE (Diffusion-AAE), whose loss function is defined as
\ba{
\mathcal{L}_{\text{Diffusion-AAE}}=-\E_{q(\rvx_0)}\E_{q(\rvx_{T_\text{trunc}}\given \rvx_0)}\log p_{\theta}(\rvx_0\given \rvx_{T_\text{trunc}}) + \mathcal{D}(q(\rvx_{T_\text{trunc}}))|| p_{\psi}(\rvx_{T_\text{trunc}})). \label{eq:DAAE}
}
Diffusion-AAE has two notable differences from a vanilla AAE: 1) its encoder is fixed and has no learnable parameters, while its prior is not fixed and is optimized to match the aggregated posterior, and 2) its decoder is a reverse diffusion chain,  with  stochastic layers all  parameterized by . 

Note in general as the likelihood in \Eqref{eq:decoder}  is intractable, the first loss term in  \Eqref{eq:DAAE} is intractable. 
However, the loss of Diffusion-AAE is upper bounded by the loss of TDPM, as described below.
\begin{theorem} 
\label{theorem:bound}
The Diffusion-AAE loss in \Eqref{eq:DAAE} is upper bounded by the TDPM loss in \Eqref{eq:Elbo_tilde}:
    
\end{theorem}




















\subsection{Matching the prior to aggregated posterior}
Via the loss term  in \Eqref{eq:Elbo_tilde}, we aim to match the prior  to the aggregated posterior  in TDPM. While we have an analytic density function for neither  nor , we can easily draw random samples from both of them. Thus, we explore the use of two different types of statistical distances that can be estimated from samples of both  and .
We empirically show that TDPM can achieve good performance regardless of which distance is used for~optimization.




One possible statistical distance is based on the idea of GANs~\citep{goodfellow2014generative,arjovsky2017wasserstein,binkowski2018demystifying}, which are widely used to learn implicit distributions from empirical data. In this setting, we use a generator  to transform samples from an isotropic Gaussian  into samples that approximate the corrupted data, and a discriminator  to distinguish between the samples from the corrupted data distribution  and the implicit generative distribution . The generator and the discriminator are trained by the following objective :
\ba{
\min_\psi \max_\phi~~~ &\E_{\rvx \sim q(\rvx_{T_\text{trunc}})}\! \left[\log D_\phi(\rvx) \right]  + \E_{\rvz \sim p(\rvz)} \left[ \log (1\!-\! D_\phi(G_\psi(\rvz))) \right]\!. \label{eq:gan_training}
}

\subsection{Training algorithm}
As the objective in Equation \ref{eq:Elbo_tilde}  is a sum of different terms, following
DDPM~\citep{ddpm} to fix the terms , we can simplify  as an expectation defined as

where   is an injected noise at a uniformly sampled timestep index ,  is a noisy image at time ,  and  is a denoising U-Net that predicts the noise in order to refine the noisy image .   Therefore the final simplified version of \Eqref{eq:Elbo_tilde} is constructed as 
\ba{
&\gL_{\text{TDPM}}^\text{GAN} = \gL_{\text{simple\_trunc}} + 
\lambda \gL_{T_\text{trunc}}^\text{GAN} 
\label{eq:diffusion_gan_obj}, 
.
}
While , the weight of , can be tuned, we fix it as one for simplicity. 
Here the TDPM objective consists of two parts: the denoising part  is focused on denoising the truncated chain, getting updated from , while the implicit part  is focused on minimizing , {getting updated from }. 


An interesting finding of this paper is that we do not necessarily need to introduce a separate set of parameters  for the generator , as we can simply reuse the same parameters  of the reverse diffusion model (, let ) without clearly hurting the empirical performance. 
This suggests that the reverse diffusion process from  to    could  be effectively approximated by a single step using the same network architecture and parameters as the reverse diffusion steps from  to .

Therefore, we provide two configurations to parameterize the implicit distributions. \textbf{1)} To save parameters, we let the implicit generator and  denoising model share the same U-Net parameters but using different time step indices. Specifically, we
first use , where , to generate a noisy image at time . \textbf{2)} We further explore employing a different model, \textit{e.g.}, StyleGAN2~\citep{karras2020training}, for the implicit generator, which provides better performance but increases the model size to get . Then for , we iteratively refine it as , where  when  and . This process is depicted in Algorithms~\ref{alg:training} and \ref{alg:sampling} in the Appendix. For the implementation details, please refer to Appendix~\ref{appendix:experiments} and our code at \url{https://github.com/JegZheng/truncated-diffusion-probabilistic-models}.








\subsection{Related work}
In our previous discussions, we have related TDPM to several existing works such as DDPM and AAE. A detailed discussion on other related works is provided in Appendix \ref{sec:relatedwork}.

\section{Experiments}\label{sec:experiments}


We aim to demonstrate that TDPM can generate good samples faster by using fewer steps of reverse diffusion. We use different image datasets to test our method and follow the same setting as other diffusion models~\citep{ddpm,nichol2021improved,Dhariwal2021DiffusionMB,rombach2022high} for our backbones. We also have two ways to set up the implicit generator that starts the reverse diffusion. One way is to reuse the denoising network, and the other way is to use a separate network. We try both ways for generating images without any labels. For generating images from text, we use the first way with the LDM backbone. We provide comprehensive details, toy examples, and additional experimental results in Appendices \ref{sec:toy experiments}-\ref{appendix:additional results_textimage}. 






\begin{table}[t]
\begin{minipage}[t]{0.48\linewidth}
\centering
\small 
\caption{\small Results of unconditional generation on CIFAR-10, with the best FID and Recall in each group marked in bold. To compare TDPM (=0) with GAN-based methods, we use DDPM backbone as generator and StyleGAN2 discriminator.}\vspace{-2mm}
\label{tab:cifar}
\centering
\begin{tabular}{llll}
\toprule[1.5pt]
Method              & \multicolumn{1}{l}{NFE} & \multicolumn{1}{l}{\!\!FID} & \multicolumn{1}{l}{\!\!\!\!Recall} \\ \hline
\multicolumn{3}{l}{\textcolor{gray}{DDPM backbone}} \\
DDPM          & 1000                    & 3.21                    & 0.57                       \\
TDPM~~ (=99) & 100                     & 3.10                     & 0.57                       \\
TDPM+ (=99) & 100                     & \textbf{2.88}                    & \textbf{0.58 }                      \\ \hdashline
DDIM          & 50                    & 4.67                    & 0.53                       \\
TDPM~~ (=49) & 50                      & 3.30                     & 0.57                       \\
TDPM+ (=49) & 50                      & 2.94                    & \textbf{0.58}                       \\ \hdashline
TDPM~~ (=4)  & 5                       & 3.34                    & 0.57                       \\
TDPM+ (=4)  & 5                       & 3.21                    & 0.57                       \\ \hline
\multicolumn{3}{l}{\textcolor{gray}{Improved DDPM backbone}} \\
Improved DDPM           & 4000                    & 2.90                     & 0.58                       \\
TDPM~~ (=99) & 100                     & 2.97                    & 0.57                       \\
TDPM+ (=99) & 100                     & \textbf{2.83}           &\textbf{0.58}                     \\ \hdashline
Improved DDPM+DDIM          & 50                    & 3.92                    & 0.55                       \\
TDPM~~ (=49) & 50                      & 3.11                    & 0.57                       \\
TDPM+ (=49) & 50                      & 2.96                    & 0.58                       \\ TDPM~~ (=4)  & 5                       & 3.51                    & 0.55                       \\
TDPM+ (=4)  & 5                       & 3.17                    & 0.57                       \\ \hline
\multicolumn{3}{l}{\textcolor{gray}{GAN-based}} \\
{DDGAN} & {4}                       & {3.75}             & {0.57}      \\
StyleGAN2     & 1                       & 8.32                    & 0.41                       \\
StyleGAN2-ADA & 1                       & \textbf{2.92}                    & \textbf{0.49}                       \\
TDPM (=0)  & 1                       & 7.34                    & 0.46                       \\
\bottomrule[1.5pt]
\end{tabular}
\end{minipage}\hfill
\begin{minipage}[t]{.5\textwidth}
    \centering
\caption{\small Results on LSUN-Church and LSUN-Bedroom (resolution ). Similar to Table~\ref{tab:cifar}, TDPM (=0) uses DDPM backbone for the generator. }
\label{tab:lsun church 256}
\small
\renewcommand{\arraystretch}{1.0}
    \setlength{\tabcolsep}{1.0mm}{ 
   \begin{tabular}{llll}
   \toprule[1.5pt]
              &  & Church & Bedroom \\
Method              & \multicolumn{1}{l}{NFE} & \multicolumn{1}{l}{FID} & \multicolumn{1}{l}{FID} \\ \hline
\multicolumn{3}{l}{\textcolor{gray}{DDPM backbone}} \\
DDPM          & 1000                    & 7.89        & 4.90           \\
TDPM~~ (=99) & 100                     & 4.33   & 3.95                 \\
TDPM+ (=99) & 100                     & \textbf{3.98}  & \textbf{3.67}                  \\ \hdashline
DDIM                        & 50                      & 10.58       & 6.62              \\
TDPM~~ (=49) & 50                      & 5.35       &   4.10            \\
TDPM+ (=49) & 50                      & 4.34     &   3.98              \\ \hdashline
TDPM~~ (=4)  & 5                       & 4.98        & 4.16          \\
TDPM+ (=4)  & 5                       & 4.89        & 4.09             \\ \hline
\multicolumn{3}{l}{\textcolor{gray}{ADM backbone}} \\
ADM           & 1000                    & \textbf{3.49}     & 1.90         \\
ADM+DDIM           & 250                    & 6.45          & 2.31          \\
TDPM~~ (=99) & 100                     & 4.41     & 2.24              \\
TDPM+ (=99) & 100                     & 3.61    & \textbf{1.88}                \\ 
TDPM~~ (=49) & 50                      & 4.57   & 2.92               \\
TDPM+ (=49) & 50                      & 3.67    & 1.89             \\  TDPM~~ (=4)  & 5                       & 5.61   & 7.92              \\
TDPM+ (=4)  & 5                       & 4.66    & 4.01            \\ \hline
\multicolumn{3}{l}{\textcolor{gray}{GAN-based}} \\
{DDGAN} & {4}                       & {5.25}             & {-}      \\
StyleGAN2     & 1                       & \textbf{3.93}     & \textbf{3.98}     \\
StyleGAN2-ADA & 1                       & 4.12              & 7.89      \\
TDPM (=0)  & 1                       & 4.77      & 5.24          \\
\bottomrule[1.5pt]
 \end{tabular}}
\vspace{-5mm}
\end{minipage}\vspace{-5mm}
\end{table}

\begin{table}[t]
\begin{minipage}[t]{0.48\linewidth}
\centering
\small 
\caption{\small {Results of ImageNet-6464, evaluated with FID and Recall. TDPM+ is built with a pre-trained ADM and an implicit model trained at  using StylGAN-XL.}}\vspace{-2mm}
\label{tab:imagenet}
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{llll}
\toprule[1.5pt]
{Method}              & \multicolumn{1}{l}{{NFE}} & \multicolumn{1}{l}{\!\!{FID}} & \multicolumn{1}{l}{\!\!\!\!{Recall}} \\ \hline
{ADM}           & {1000}                    & {2.07}                    & \textbf{{0.63}}                       \\
{TDPM+ (=99)} & {100}                     & \textbf{{1.62}}                    & \textbf{{0.63}}                      \\ 
{TDPM+ (=49)} & {50}                      & {1.77}                    & {{0.58}}                       \\ 
{TDPM+ (=4)}  & {5}                       & {1.92}                    & {0.53}                       \\ 
{StyleGAN-XL (wo PG)} & {1}                      & {3.54}                    & {0.51}                       \\ \bottomrule[1.5pt]
\end{tabular}
}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\linewidth}
\vspace{0mm}
\centering
\includegraphics[width=.8\textwidth]{image/imagenet.pdf}\vspace{-3mm}
\captionof{figure}{\small {Random generation results of TDPM+ (=4) on ImageNet-6464. }}\label{fig:imagenet}
\end{minipage}\vspace{-8mm}
\end{table}







We use FID (lower is better) and Recall (higher is better) to measure the fidelity and diversity, respectively, of the generated images. We use CIFAR-10 \citep{cifar10}, LSUN-bedroom, and LSUN-Church \citep{lsun} datasets in unconditional experiments, and CUB-200~\citep{cub} and MS-COCO~\citep{mscoco} for text-to-image experiments. The images consist of  pixels for CIFAR-10 and  pixels for the other datasets. 


\subsection{Efficiency in both training and sampling}

We first look at the results on CIFAR-10. We use DDPM~\citep{ddpm} or improved DDPM~\citep{nichol2021improved} as our backbones. We use 4, 49, or 99 steps of reverse diffusion, which correspond to 5, 50, or 100 number of function evaluations (NFE). For the implicit generator, we either reuse the denoising U-Net or use a StyleGAN2 network (respectively, we call them TDPM and TDPM+). {For comparison, we also include DDIM~\citep{ddim} and DDGAN~\citep{xiao2021tackling}. The comparison with a more diverse set of baselines can be found in Table~\ref{tab:cifar_full} in Appendix \ref{appendix:additional unconditional results}.} 

\begin{figure}[t]
\vspace{-2mm}
\begin{minipage}[t]{0.48\linewidth}
\vspace{0mm}
    \centering
    \includegraphics[width=.95\textwidth]{image/training.pdf}\vspace{-3mm}
    \captionof{figure}{\small The required iterations (measured with iterated images) to converge in the training. The iterations for  () and  () are marked in red and blue, respectively. }\label{fig:iter_time}
\end{minipage}\hfill
\begin{minipage}[t]{0.48\linewidth}
\vspace{0mm}
    \centering
    \includegraphics[width=.95\textwidth]{image/sampling.pdf}\vspace{-3mm}
    \captionof{figure}{\small Evolution of FID and corresponding GPU time (s/image) across different timesteps in the sampling stage. }\label{fig:fid_time}
\end{minipage}
\vspace{0mm}
\end{figure}




Table~\ref{tab:cifar} shows that our TDPM can get good FID with fewer NFE. TDPM+ can get even better FID, and it is the best when NFE=100. Compared with TDPM with 0 steps of reverse diffusion (a GAN with DDPM's U-Net as generator and StyleGAN2 as discriminator) and StyleGAN2, TDPM with more than 0 steps of reverse diffusion has better recall and the FID is as good as StyleGAN2-ADA (a GAN with data augmentation for better training). This means TDPM can largely avoid the mode missing problem in GANs. We show some examples of generated images on CIFAR-10 in \Figref{fig:image_cifar10_celeba}.



We also check how fast TDPM can train and sample. In training, we count how many images TDPM needs to well fit the truncated diffusion chain and the implicit prior. \Figref{fig:iter_time} shows that when we use fewer steps of reverse diffusion, the diffusion part needs less time to train. But the implicit prior needs more time to train because it has to model a harder distribution, \textit{e.g.}, fitting the implicit prior with 4 diffusion steps needs similar time to directly fit it on the data. When we use 99 steps of reverse diffusion, the diffusion chain and the implicit prior need similar time to train, and the whole model trains faster than both GAN and DDPM. 
In sampling, we compare TDPM with 0, 1, 4, 49, or 99 steps of reverse diffusion. We report both FID and the sampling time (s/image) on one NVIDIA V100 GPU in \Figref{fig:fid_time}. When we use 4 steps of reverse diffusion, the FID is much lower than 0 steps, and the sampling time is slightly longer. When we use more steps of reverse diffusion, the FID goes down slowly, but the sampling time goes up linearly. When we use 99 steps of reverse diffusion, the FID of TDPM is better than DDPM with 1000 steps. Because the FID does not change much when we use more steps of reverse diffusion, we suggest using a small number of steps, such as 4 or more, to balance the quality and speed of generation.

      


\subsection{Results on higher-resolution and more diverse image datasets}



To test the performance of the proposed truncation method on high-resolution images, we train TDPM using two different diffusion models, DDPM~\citep{ddpm} and ADM~\citep{Dhariwal2021DiffusionMB}, as backbones on two datasets of  resolution, LSUN-Church and LSUN-Bedroom \citep{lsun}. We compare the FIDs of TDPM with those of the backbone models and some state-of-the-art GANs in Tables~\ref{tab:lsun church 256}. The results show that TDPM can generate images of similar quality with much smaller truncation steps , which means that it can produce images significantly faster than the backbone models. We also visualize the samples from the implicit distribution  that TDPM generates and the corresponding  that it finishes at the end of reverse chain in \Figref{fig:church_bedroom_adm}.
\begin{table}[t]
\vspace{-3.5mm}
\begin{minipage}[t]{\linewidth}
\vspace{-1mm}
    \centering
    \includegraphics[width=.85\textwidth]{image/adm_church_bedroom.pdf}\vspace{-2mm}
    \captionof{figure}{\small Randomly generated images  of TDPM using ADM \citep{Dhariwal2021DiffusionMB} backbone on LSUN- Church and LSUN-Bedroom (), with , , and . Note  in TDPM. Each group presents generated samples from the full model  (left) and its implicit prior  (right); the full model sample is obtained by refining the implicit prior sample via the truncated reverse diffusion chain.}\label{fig:church_bedroom_adm}
\end{minipage}
\vspace{-7mm}
\end{table}



{
We further evaluate TDPM on ImageNet-1K (with resolution 6464) that exhibits high diversity.
Here we adopt the TDPM+ configuration, where we use a pre-trained ADM~\citep{Dhariwal2021DiffusionMB} checkpoint for  and train a StyleGAN-XL~\citep{sauer2022styleganxl} based implicit model at  (for simplicity, we choose to not use the progressive growing pipeline of StyleGAN-XL; See Appendix~\ref{appendix:experiments} for more details). We compare both FID and Recall with our backbone models in Table~\ref{tab:imagenet} and show example generations in \Figref{fig:imagenet}. 
 Similar to our observations in Table \ref{tab:cifar}, TDPM has 
 good generation quality with small
 truncation steps . Moreover, properly training an implicit model at  can further improve the performance of the backbone. 
}




\vspace{-2mm}\subsection{Text-to-image generation}
Besides unconditional generation tasks,  we develop for text-to-image generation the TLDM, a conditional version of TDPM that leverages  as the backbone the LDM of \citet{rombach2022high}, which is a state-of-the-art publicly released model with 1.45B parameters pre-trained on LAION-400M~\citep{laion}. LDM consists of a fixed auto-encoder for pixel generation and a latent-diffusion module to connect text and image embeddings. Here we fine-tune its latent-diffusion part on CUB-200 and MS-COCO datasets with 25K and 100K steps as the baseline. Similar to the unconditional case, we fine-tune with the LDM loss for  and GAN loss for . More details about the setting can be found in Appendix~\ref{appendix:experiments}. 

The results of LDM with different DDIM sampling steps and TLDM with different truncated steps are summarized in \Figref{fig:text_img_FID} and Table~\ref{tab:text-image}. Similar to applying diffusion directly on the original image-pixel space, when the diffusion chain is applied in the latent space, we observe 
TLDM can achieve comparable or better performance than LDM even though it has shortened the diffusion chain of LDM to have much fewer reverse diffusion steps.
For the case that NFE is as small as 5, we note although the FID of TLDM has become higher due to using fewer diffusion steps, the generated image using TLDM at NFE=5 is still visually appealing, as shown in \Figref{fig:text_img}. Compared with 50 and 250 steps using LDM,
the sampling speed of TLDM using 5 steps is 10 and 50 times faster, respectively, while largely preserving generation quality.
We provide additional text-to-image generation results of TLDM in Appendix \ref{appendix:additional results_textimage}.



\begin{table}[t]
\begin{minipage}[t]{0.44\linewidth}
\centering
\vspace{0mm}
    \includegraphics[width=.95\textwidth]{image/text-image.pdf}\vspace{-4mm}
    \captionof{figure}{\small Quantitative text-to-image results (FID and GPU time) across different NFE.}\label{fig:text_img_FID}
\end{minipage}
\begin{minipage}[t]{0.55\linewidth}
\caption{\small Numerical results of \Figref{fig:text_img_FID}. The GPU time of sampling (s/image) is measured on one NVIDIA A100.}
\label{tab:text-image}
\centering
\resizebox{.95\textwidth}{!}{\begin{tabular}{ll|cc|cc}
\toprule[1.5pt]
    \multicolumn{2}{l|}{}           & \multicolumn{2}{l|}{CUB-Bird}                                       & \multicolumn{2}{l}{MS-COCO}                                        \\ \hline
NFE  & GPU time & {LDM} & {TLDM} &{LDM} & {TLDM} \\ \hline
5    & 0.15     & 100.81                        & 10.59                              & 48.41                         & 16.7                               \\
50   & 1.57       & 30.85                         & 7.32                               & 18.25                         & 7.47                               \\
100  & 4.10       & 11.07                         & 6.79                               & 8.2                           & 7.22                               \\
250  & 11.21      & 6.82                          & 6.72                               & 6.3                           & 6.29                               \\
1000 & 41.09      & {6.68}    &  -                                       & {6.29} & -   \\            
\bottomrule[1.5pt]                           
\end{tabular}}
\end{minipage}
\\
\begin{minipage}[t]{\linewidth}
\vspace{-1mm}
    \centering
    \includegraphics[width=.75\textwidth]{image/textimg_vis.pdf}\vspace{-3mm}
    \captionof{figure}{\small Example text-to-image generation results of LDM and TLDM ( TDPM with LDM backbone) finetuned on CUB-200 (top row) or MS-COCO (bottom row), setting the number of times iterating through the reverse diffusion U-Net as 100 (left column), 50 (middle column), or 5 (right column).  }\label{fig:text_img}
\end{minipage}
\vspace{-7mm}
\end{table}

  



\section{Conclusion}In this paper, we investigate how to reduce the trajectory length of the diffusion chain to achieve efficient sampling without loss of generation quality. We propose truncated diffusion probabilistic modeling (TDPM) that truncates the length of a diffusion chain. In this way, TDPM can use a much shorter diffusion chain, while being required to start the reverse denoising process from an intractable distribution. We propose to learn such a distribution with an implicit generative model powered by the same U-Net used for denoising diffusion, and validate with multiple ways to learn the implicit distribution to ensure the robustness of the proposed TDPM. We reveal that TDPM can be cast as an adversarial auto-encoder with a learnable implicit prior. We conduct extensive experiments on both synthetic  and real image data to demonstrate the effectiveness of TDPM in terms of both sample quality and efficiency, where the diffusion chain can be shortened to have only a few steps.

\subsubsection*{Acknowledgments}
H. Zheng and M. Zhou acknowledge the support of NSF-IIS 2212418 and IFML. 



\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}

\newpage
\appendix
\section{Proof}
\begin{proof}[Proof of Theorem \ref{theorem:bound}]
As the last terms in both losses are the same, we only need to show that the first term in \Eqref{eq:DAAE} is smaller than or equal to  in \Eqref{eq:Elbo_tilde}.
    Using Jensen's inequality, we have
\ba{
&-\E_{q(\rvx_{0})}\E_{q(\rvx_{T_\text{trunc}}\given \rvx_0)}\log p_{\theta}(\rvx_0\given \rvx_{T_\text{trunc}})\notag\\
&=  -\E_{q(\rvx_{0})}\E_{q(\rvx_{T_\text{trunc}}\given \rvx_0)} \log\E_{q(\rvx_{1: {T_\text{trunc}}-1}\given  \rvx_0,\rvx_{T_\text{trunc}})}\left[\frac{ p(\rvx_{0: {T_\text{trunc}}-1}\given \rvx_{T_\text{trunc}})}{q(\rvx_{1: {T_\text{trunc}}-1}\given \rvx_0,\rvx_{T_\text{trunc}})} \right]\notag\\
&\le -\E_{q(\rvx_{0})}\E_{q(\rvx_{T_\text{trunc}}\given \rvx_0)} \E_{q(\rvx_{1: {T_\text{trunc}}-1}\given \rvx_0,\rvx_{T_\text{trunc}})} \log\frac{ p(\rvx_{0: {T_\text{trunc}}-1}\given \rvx_{T_\text{trunc}})}{q(\rvx_{1: {T_\text{trunc}}-1}\given \rvx_0,\rvx_{T_\text{trunc}})} \notag\\
&=- \E_{q(\rvx_{0})}\E_{q(\rvx_{1: {T_\text{trunc}}}\given \rvx_0)} \log\left[\frac{ p(\rvx_{0: {T_\text{trunc}}-1})}{q(\rvx_{1: {T_\text{trunc}}}\given \rvx_0)} 
\frac{q(\rvx_{{T_\text{trunc}}}\given \rvx_0)}{ p(\rvx_{T_\text{trunc}})}\right]
\notag\\
&=\left(- \E_{q(\rvx_{0})}\E_{q(\rvx_{1: {T_\text{trunc}}}\given \rvx_0)} \log\frac{ p(\rvx_{0: {T_\text{trunc}}-1})}{q(\rvx_{1: {T_\text{trunc}}}\given \rvx_0)}\right)
-
\E_{q(\rvx_{0})}\E_{q(\rvx_{{T_\text{trunc}}}\given \rvx_0)}\log
\frac{q(\rvx_{{T_\text{trunc}}}\given \rvx_0)}{ p(\rvx_{T_\text{trunc}})}\notag\\
&=(  \textstyle \sum_{t=1}^{T_\text{trunc}}\gL_{t-1}+ \gL_{T_\text{trunc}} 
) - \gL_{T_\text{trunc}} \notag\\
&=\sum_{t=1}^{T_\text{trunc}}\gL_{t-1},
}
where the second to last equality follows the same derivation of the ELBO in \citet{ddpm}.
\end{proof}

\section{Related work}\label{sec:relatedwork} Diffusion probabilistic models \citep{diffusion, ddpm} employ a forward Markov chain to diffuse the data to noise  and learn the reversal of such a diffusion process. With the idea of exploiting the Markov operations~\citep{goyal2017variational, alain2016gsns, bordes2017learning}, diffusion models achieve great success and inspire a variety of tasks including image generation and audio generation~\citep{diffwave,wavegrad,adversarialdiff,vahdat2021score}. Recently, plenty of studies have been proposed to generalize diffusion model to continuous time diffusion and improve the diffusion models in likelihood estimation~\citep{vincent2011connection, improvedscore,scorematching,nichol2021improved,song2021scorebased,song2021maximum,kingma2021variational}.  

Another mainstream is to improve the sampling efficiency of  diffusion models, which are known for their enormous number of sampling steps. \citet{luhman2021knowledge} improve diffusion processes with knowledge distillation and \citet{san2021noise} propose a learnable adaptive noise schedule.  \citet{ddim} and \citet{kong2021fast} exploit non-Markovian diffusion processes and shorten the denoising segments. \citet{jolicoeur2021gotta} and \citet{huang2021variational} use better SDE solvers for continuous-time models. Aside from these works, recently other types of generative models such as VAEs~\citep{kingma2013auto}, GANs~\citep{goodfellow2014generative}, and autoregressive models~\citep{pixelcnn} have been incorporated to diffusion models. They are shown to benefit each other~\citep{xiao2021tackling,pandey2022diffusevae,meng2021improved} and have a closer relation to our work. \citet{xiao2021tackling} consider the use of implicit models~\citep{huszar2017variational,mohamed2016learning,tran2017hierarchical,yin2018semi,li2018implicit} to boost the efficiency of diffusion models, where they deploy implicit models in each denoising step, which has higher difficulty in the training as the number of diffusion steps increases. \citet{pandey2022diffusevae} build diffusion models on top of the output of VAEs for refinement. Our work is also related if viewing TDPM as a diffusion model on top of an implicit model, where the implicit model can be parameterized {with the U-Net or a separate network.} 

\section{Discussion}\label{appendix:discussion}
\textbf{Potential societal impacts}: 
This paper proposes truncated diffusion probabilistic model as a novel type of diffusion-based generative model. The truncated part can be trained as implicit generative models such as GANs jointly or independently with the diffusion part. The capacities of truncated diffusion probabilistic models are competitive to existing diffusion-based ones and efficiency is largely improved. On the contrary of these positive effects, some negative perspectives could also be seen, depending on how the models are used. One major concern is the truncated diffusion technique proposed in this paper could potentially be a way to hack the existing diffusion models if the implicit models are maliciously used to fit the intermediate steps. For example, for some existing diffusion models, for safety concerns, the model's capacity to generate private data needs to be locked by hiding the diffusion ending point into an unknown distribution. The technique of TDPM could be used to crack these existing online diffusion models by providing intermediate noisy images or fine-tuning the first few steps with TDPM to unlock the capacity. Besides, the capacity of generating good images can also be misused to generate ill-intentioned images at a much lower cost.

\textbf{Discussions}: 
In this work, we mainly focus on reducing the length of the diffusion chain of a finite-time diffusion model. Our model has shown its effectiveness in improving finite-time diffusion models and it is non-trivial to further explore our model on continuous-time diffusion models~\citep{song2021scorebased}. Moreover, while in this paper DDPM is the primary baseline, TDPM can also be built on other recent diffusion models. While  is parameterized as an implicit distribution, it can also be formulated as a semi-implicit distribution~\citep{yin2018semi}, which allows it to be approximated with a Gaussian generator. \citet{xiao2021tackling} also present a closely related work. While we share the same spirit to reduce the length of the diffusion chain, these two strategies are not conflicting with each other. In future work we will look into the integration of these different strategies. {There also exists plenty of options in approximating . When truncating the diffusion chain to be short, the implicit distribution still faces multi-modal and needs to fit with different methods depending upon the properties that we need. For example, in order to capture all modes, a VAE would be preferred, like done in \citet{pandey2022diffusevae}. Below we provide an alternative method proposed in \citet{zheng2021exploiting} to fit the truncated distribution. Besides the training, it’s also an open question whether TDPM can be incorporated into more advanced architectures to have further improvements and we leave this exploration for future work. 
}

\section{Algorithm details and complementary Results}\label{appendix:results}
Below we provide additional algorithm details and complementary experimental results. 

\subsection{Additional analysis on the parameterization of the implicit generator}

As shown in Section~\ref{sec:method}, in general, the objective of TDPM consists of the training of the diffusion model  (a U-Net architecture~\citep{ronneberger2015u}) with simple loss of DDPM  and the training of an implicit prior model  with objective . Without loss of generality, in our main paper, we show two configurations  to parameterize the implicit part for : 1) the implicit generator shares the same U-Net architecture used for  ; 2) the implicit generator is instantiated with a separate network. Below we explain this two configurations (denoted as TDPM+ in the main paper).


\textbf{Configuration 1)}: At , the Unet generates the noisy image at the truncated step: , where  is the pure noise image whose pixels are \textit{iid} sampled from  standard normal. For , the same Unet iteratively refines the noisy images by letting , where   is the predicted noise by the Unet. 

Under this setting, the Unet-based generator plays two roles at the same time and the training will be more challenging than using two different generators here. However, we can also see as  gets larger, the distribution of  will become more similar to a noise distribution, and generating the noisy images will be more like generating noises. In this case, being able to generate both noisy images and predicting noise becomes easier for the generator. 

\textbf{Configuration 2) (TDPM+)}: 
Unlike previous configuration, where the implicit generator at step  shares the same  U-Net architecture with . Another way is to parameterize  with a separate generator. Although this configuration increases the total parameter of the generative model, it allows the model has better flexibility in the training stage. For example, these two networks can be trained in parallel or leverage a pre-trained model. In our paper, we conduct the experiments by using Stylegan2 generator architecture~\cite{karras2020analyzing} for , resulting in an increase of 19M and 28M for the generator parameters when handling  and  images. 


The process of training and sampling of these configurations are summarized in Algorithm~\ref{alg:training} and \ref{alg:sampling}.

\begin{figure}[ht]
\begin{minipage}[t]{0.495\textwidth}
\begin{algorithm}[H]
  \caption{Training} \label{alg:training}
  \small
  \begin{algorithmic}[1]
    \REPEAT
      \STATE 
      \STATE 
      \STATE 
      \STATE Update with \Eqref{eq:diffusion_gan_obj}
    \UNTIL{converged}
  \end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}[t]{0.495\textwidth}
\begin{algorithm}[H]
  \caption{Sampling} \label{alg:sampling}
  \small
  \begin{algorithmic}[1]
    \vspace{.04in}
    \STATE 
    \IF { shared with } 
      \STATE  
    \ELSE
      \STATE  
      \ENDIF
    \FOR{}
      \STATE  if , else 
      \STATE 
    \ENDFOR
    \STATE \textbf{return} 
    \vspace{.04in}
  \end{algorithmic}
\end{algorithm}
\end{minipage}
\vspace{-1em}
\end{figure}











\subsection{Alternatives of learning the implicit distribution}
Another possible statistical distance is based on conditional transport~\citep{zheng2021exploiting}, which is proposed to balance the model-seeking and mode-covering behaviors when fitting an empirical data distribution.
In this setting, we use the same generator  as before, but instead of a discriminator, we use a conditional distribution  parameterized by  to find an optimized mapping between the samples of  and , and a critic  to measure the point-to-point cost  in the feature space. The generator, the conditional distribution, and the critic are trained by the following objective :
\ba{
\min_{\psi,\eta} &\max_\phi~\! \E_{\rvx \sim q(\rvx_{T_\text{trunc}})}\! \left[ \E_{G_\psi(\rvz) \sim \pi_\eta(G_\psi(\rvz)\given \rvx_{T_\text{trunc}})} c_\phi(\rvx_{T_\text{trunc}}, G_\psi(\rvz)) \right] \notag \\
&+ \E_{\rvz \sim p(\rvz)} \left[ \E_{\rvx_{T_\text{trunc}} \sim \pi_\eta(\rvx_{T_\text{trunc}}\given G_\psi(\rvz))} c_\phi(\rvx_{T_\text{trunc}}, G_\psi(\rvz)) \right]\!. \label{eq:CT_training}
}
Similar to \Eqref{eq:diffusion_gan_obj}, we fit TDPM-CT with following loss
\ba{
\gL_{\text{TDPM}}^\text{CT} = \gL_{\text{simple\_trunc}} + \lambda \gL_{T_\text{trunc}}^\text{CT} \label{eq:diffusion_ct_obj}
.
}
\textcolor{black}{We empirically find out this objective has no significant difference than using GAN objective shown in Equation \ref{eq:diffusion_gan_obj} in performance-wise as long as the generator is well trained.}

\subsection{Conditional truncated diffusion probabilistic models}
For conditional generation, we extend \Eqref{eq:diffusion_gan_obj} and derive a conditional version of TDPM: 
\ba{
&\gL_{\text{TDPM}}^\rvc = \gL_{\text{simple\_trunc}}^\rvc + 
\lambda \gL_{T_\text{trunc}}^\rvc
\label{eq:tdpm_obj_cond},
}
where  aims to train the conditional diffusion model with  

and the truncated distribution  can be fitted with GAN or CT:
\ba{
\min_\psi \max_\phi~~~ & \E_\rvc \left[ \E_{\rvx \sim q(\rvx_{T_\text{trunc}}\given \rvc)}\! \left[\log D_\phi(\rvx\given\rvc) \right]  + \E_{\rvz \sim p(\rvz)} \left[ \log (1\!-\! D_\phi(G_\psi(\rvz, \rvc))\given \rvc) \right]\right]\!. \label{eq:gan_cond_training}
}
\ba{
\min_{\psi,\eta} \max_\phi~\! &\E_\rvc \big[ \E_{\rvx \sim q(\rvx_{T_\text{trunc}\given\rvc})}\! \left[ \E_{G_\psi(\rvz) \sim \pi_\eta(G_\psi(\rvz,\rvc)\given \rvx_{T_\text{trunc}}, \rvc)} c_\phi(\rvx_{T_\text{trunc}}, G_\psi(\rvz,\rvc)) \right] \notag \\
&+ \E_{\rvz \sim p(\rvz)} \left[ \E_{\rvx_{T_\text{trunc}} \sim \pi_\eta(\rvx_{T_\text{trunc}}\given G_\psi(\rvz,\rvc), \rvc)} c_\phi(\rvx_{T_\text{trunc}}, G_\psi(\rvz,\rvc)) \right]\big]\!. \label{eq:CT_cond_training}
}


\subsection{Analysis on toy experiments}\label{sec:toy experiments}
Although we present image experiments in the main paper, our studies were firstly justified our method on synthetic toy data as a proof of concept. We adopt representative 2D synthetic datasets used in prior works \citep{gulrajani2017improved,zheng2021exploiting}, including Swiss Roll, Double Moons, 8-modal, and 25-modal Gaussian mixtures with equal component weights. We use an empirical sample set , consisting of  samples and illustrate the generated samples after 5000 training epochs. We take 20 grids in the range  for both the  and  axes to approximate the empirical distribution of  and , and report the corresponding forward KL  as the quantitative evaluation metric. 

{\Figref{fig:toy_swissroll} shows the results on the Swiss Roll data. We present a short chain with  and a longer chain with  to show the impacts of the number of diffusion steps. The first row shows that the data distribution is diffused with accumulated noise, and with more steps the diffused distribution will be closer to an isotropic Gaussian distribution. As one can see, truncating the diffusion chain to a short length will 
result in a clear gap between  and . When DDPM (shown in the second row) samples from the isotropic Gaussian distribution, it becomes hard to recover the original data distribution from pure noise with only a few steps. Although we can see DDPM can get slightly improved with a few more steps (), as long as  is not close to Gaussian, DDPM can hardly recover the data distribution. By contrast, as shown in the third and fourth rows, TDPM successfully approximates the non-Gaussian  with its implicit generator, and we can see the remaining part of the truncated chain is gradually recovered by the denoising steps. From both visualizations and , we can see that TDPM is able to fit every step in such short chains.}

TDPM-GAN and TDPM-CT both succeed in fitting  but the latter one fits slightly better when the diffusion length is 2.
When the length increases to 5, fitting the implicit distribution with GAN becomes easier. 
This observation demonstrate a benefit of combining the diffusion models and GANs. 
If the implicit generator is sufficiently powerful to model , then the number of steps in need can be compressed to a small number. On the contrary, if the implicit generator cannot capture the distribution, we need more steps to facilitate the fitting of the data distribution. 

Shown in \Figref{fig:toy_8gaussians}-\Figref{fig:toy_moons}, we can see 8-modal Gaussian is more similar to an isotropic Gaussian after getting diffused, thus DDPM can recover a distribution similar to data with 5 steps. On 25-Gaussians, we can observe GAN does not suffer from mode-collapse and provide a better approximation than CT, which results in better data distribution recovery in the final step. 


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{toy/swiss_roll.pdf}\vspace{-2mm}
    \caption{{\small A comparison of DDPM~\citep{ddpm}, TDPM-GAN, and TDPM-CT on Swiss Roll toy data. We show the effects of a truncated diffusion chain with length  and  ( and ). The first row displays the true distribution from  to . Each row below the first one represents the corresponding denoising distribution . DDPM assumes  and we can observe a gap between the true data distribution  and its generative distribution . TDPM learns , which can be observed to well approximate the true , which helps the model successfully recover the clean data distribution .
    Below each model, we report empirical KL divergence between data and generative distributions as the quantitative metric. More results on different toy data can be found in Appendix~\ref{appendix:results}.}}
    \label{fig:toy_swissroll}
    \vspace{-3mm}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{toy/8gaussians.pdf}
    \caption{Analogous results to \Figref{fig:toy_swissroll} using 8-modal Gaussian data.}
    \label{fig:toy_8gaussians}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{toy/25gaussians.pdf}
    \caption{Analogous results to \Figref{fig:toy_swissroll} using 25-modal Gaussian data.}
    \label{fig:toy_25gaussians}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{toy/moons.pdf}
    \caption{Analogous results to \Figref{fig:toy_swissroll} using Double Moons data.}
    \label{fig:toy_moons}
\end{figure*}


\clearpage
\subsection{Additional ablation studies}

\textbf{Using Pre-trained diffusion backbones:} Different from the default setting, here we put the implicit model of TDPM+ trained at  and a pre-trained DDPM model\footnote{The pre-trained checkpoints are provided by: \url{https://github.com/pesser/pytorch\_diffusion}} in the same pipeline of sampling. In this case we do not need to spend any time on pretraining the DDPM model, 
and only need to train the implicit model for .
As shown in Table~\ref{tab:Separate-implicit-model}, when combined with a pre-trained DDPM for , the generation performance of TDPM trained under this two-step procedure is comparable to TDPM trained end-to-end. 
\begin{table}[ht]
\caption{{Results of adding a separately trained implicit generator to a pre-trained diffusion model on CIFAR-10.}}
\label{tab:Separate-implicit-model}
\centering
\renewcommand{\arraystretch}{1.}
    \setlength{\tabcolsep}{1.0mm}{ 
    \begin{tabular}{l|l|c}
   \toprule[1.5pt]
      {Model ()} & {Model ()} & {FID} \\
      \hline
      \multirow{4}{*}{{TDPM+ (T=99)}} & {TDPM+ (DDPM backbone)} & {2.88}\\
       & {pre-trained DDPM}  & {2.85}\\ 
       & {TDPM+ (improved-DDPM backbone)}  & {2.83}\\ 
       & {pre-trained improved-DDPM}  & {2.25}\\ \hline
      \multirow{4}{*}{{TDPM+ (T=49)}} & {TDPM+}  & {2.94}\\
       & {pre-trained DDPM}  & {3.05}\\ 
        & {TDPM+ (improved-DDPM backbone)}  & {2.96}\\ 
       & {pre-trained improved-DDPM}  & {2.60}\\ \hline
       \multirow{4}{*}{{TDPM+ (T=4)}} & {TDPM+ (DDPM backbone)}   & {3.21}\\
        & {pre-trained DDPM}  & {3.25}\\ 
        & {TDPM+ (improved-DDPM backbone)}  & {3.17}\\ 
       & {pre-trained improved-DDPM}   & {2.95}\\
\bottomrule[1.5pt]
 \end{tabular}}\vspace{-3mm}
\end{table}



\textbf{Sensitivity to noise schedule}:  \citet{nichol2021improved} show the noise schedule affects the training of DDPM. Here we examine if TDPM is sensitive to the choice of noise schedule. We compare the linear schedule with cosine schedule, which adds noise in a milder manner. The results on CIFAR-10 are reported in Table~\ref{tab:schedule}, which suggest that TDPM is not sensitive to the choice between these two schedules.

\begin{table}[ht]
\caption{\small Ablation study with different noise schedules on CIFAR-10. The number before and after ``/" denotes the FID using linear and cosine schedules, respectively.}
\label{tab:schedule}
\centering
\renewcommand{\arraystretch}{1.}
    \setlength{\tabcolsep}{1.0mm}{ 
    \begin{tabular}{l@{\hskip .25in}l@{\hskip .25in}c}
   \toprule[1.5pt]
      Model & Steps  & FID (linear / cosine)\\
      \midrule
      TDPM-GAN& T=99  & 3.10 / 3.47\\
      TDPM-GAN& T=49  & 3.30 / 3.16\\
      TDPM-CT& T=99  & 3.69 / 3.62\\
      TDPM-CT& T=49  & 3.97 / 3.24\\
\bottomrule[1.5pt]
 \end{tabular}}\vspace{-3mm}
\end{table}


\textbf{On the choice of truncated step}: 
{
As the diffused distribution could facilitate the learning of the implicit generator ~\citep{arjovsky2017towards}, where we could observe by increasing the number of diffusion steps, the FID of TDPM consistently gets better. A natural question is on which step should we truncate the diffusion chain. We study the signal-to-noise ratio (SNR) of different diffusion step. Based on , we calculate SNR as

We visualize the SNR evolution across time step  in Figure~\ref{fig:snr}, where we can observe the SNR rapidly decays in the first 100 steps. According to previous studies in \citet{arjovsky2017towards}, injecting noise into the data distribution could smoothen the data distribution support and facilitate the GAN training. The SNR change in this interval indicates injecting noise in the level of  could bring in more significant improvement for the GAN training. When the step is greater than 200, the SNR is change is no longer significant and close to zero, which indicates the implicit model might not be too informative, though it is easier to train. Our experimental observations in \Figref{fig:iter_time}  also justify this conclusion: when training a GAN at , the required number of iterations is similar to training it on clean data; by training the GAN model at , the training of GAN is significantly facilitated. For , we empirically examine to train a GAN and find it would converge faster than training the diffusion model for . 
}


\textbf{Comparison of model efficiency}: 
{
In complement of the results in Table~\ref{tab:cifar}-\ref{tab:lsun church 256}, we provide detailed model size and generation time on v100 GPU. The results are summarized in Table~\ref{tab:parameter_time}. We can see TDPM has an increasing in the total number of parameter, as it involves a discriminator to help train the implicit model, while its sampling efficiency is also obvious.
}

\begin{table}[ht]
\caption{{ Comparison of model size (the added parameters corresponds to the discriminator model in the training but not involved in the generation), and GPU time in generation.}}
\label{tab:parameter_time}
\centering
\renewcommand{\arraystretch}{1.}
\setlength{\tabcolsep}{1.0mm}{ 
\resizebox{.95\columnwidth}{!}{
\begin{tabular}{l|cc|cc|cc}
\toprule[1.5pt]
Resolution & \multicolumn{2}{c|}{3232}  & \multicolumn{2}{c|}{6464}  & \multicolumn{2}{c}{256256} \\ \hline
Model      & Parameter & Time (s/image) & Parameter & Time (s/image) & Parameter  & Time (s/image) \\ \hline
DDPM       & 36M       & 31.03          & 79M       & 33.01          & 114M       & 62.93          \\
TDPM, T=99 & 36M+20M       & 3.13           & 79M+21M      & 3.52           & 114M+24M       & 6.65           \\
TDPM, T=49 & 36M+20M       & 1.52           & 79M+21M      & 1.55           & 114M+24M       & 1.88           \\
TDPM, T=4  & 36M+20M       & 0.16           & 79M+21M      & 0.26           & 114M+24M       & 0.65           \\
TDPM, T=0  & 36M+20M       & 0.03           & 79M+21M      & 0.05           & 114M+24M       & 0.14          \\
\bottomrule[1.5pt]
 \end{tabular}}\vspace{-5mm}
}
\end{table}




\begin{figure*}[ht]
    \centering
    \includegraphics[width=.85\textwidth]{image/snr_evolution.pdf}
    \caption{{Signal-to-noise ratio evolution across different diffuse step . The right sub-panel shows a zoomed-in SNR evolution in the range of [30, 140] steps. }}
    \label{fig:snr}\vspace{-3mm}
\end{figure*}


\subsection{Experimental Settings}\label{appendix:experiments}

\subsubsection{Model architecture}
\textbf{Generator: }Our generator structure strictly follows the U-Net structure \citep{ronneberger2015u} used in DDPM, improved DDPM, and ADM~\citep{ddpm,nichol2021improved,Dhariwal2021DiffusionMB}, which consists of multiple ResNet blocks \citep{he2016deep} with Attention blocks \citep{vaswani2017attention} injected in the bottleneck. Please refer to these paper for more details on the architecture.

A key difference between our model and previous diffusion models is that our model also train such U-Net as an extra implicit generator  that takes a latent variable  and a fixed time index  as input. However, this does not result in a difference in the generator architecture. We parameterize  with the same U-Net architecture for simplicity and the time embedding  is specified to be trained with the implicit loss shown in \Eqref{eq:gan_training} and \Eqref{eq:CT_training}. We have also tested to use all zero time embedding for  and found no clear differences. 

For our results of TDPM+, the generator  specifically takes a StyleGAN2 architecture~\cite{karras2020analyzing} and there is no time-embedding in . An increase of generator parameter appears caused by separating the implicit model and denoising U-Net. Note that the generator is trained with GAN loss and without specially designed adaptive augmentation in~\citet{karras2020training}. For the detailed model architecture please refer to the corresponding paper or their Github repository: \url{https://github.com/NVlabs/stylegan2-ada-pytorch}.

\textbf{Discriminator:} Similar to \citet{xiao2021tackling}, we adopt the discriminator architecture used in \citet{karras2020analyzing}, but without the time step input. The discriminator discriminate  is from the diffused distribution  or implicit generative distribution . Please refer to Appendix~C of \citet{xiao2021tackling} for the detailed design.

\textbf{Navigator:} Training with  involves an extra module named navigator~\citep{zheng2021exploiting}. We strictly follow the architecture used in~\citet{zheng2021exploiting}, where the navigator is an MLP taking the pairwise feature distance as inputs. There is no time embedding used in the navigator as it is only used for the training at . The feature is extracted from the layer before the final scalar output. Please refer to their Appendix D for detailed information.

\textbf{Architecture for text-to-image experiments:} We adopt the 1.45B LDM model~\citep{rombach2022high} that is pretrained on LAION-400M dataset~\citep{laion}. The LDM model consists of a U-Net KL-regularized autoencoder with downsampling-factor 8 (resolution 256 -> 32), a U-Net in the latent space, and a BERT~\citep{devlin2018bert} text encoder transform raw text to a sequence of 1280-dimension embeddings. We only fine-tune the latent model in our experiments. 
In the training of the truncated part, the discriminator takes the first-half of the U-Net (downsampling backbone) with a linear predicting head on top of it.

\textbf{Architecture for toy experiments:} The generator uses an architecture stacked with 4 linear layers with 128 hidden units. Each intermediate layer is equipped with a time-embedding layer and follows softplus activation. The discriminator and navigator have the same architecture, without time-embedding layers, and using leakyReLU as the activation function.
 
\subsubsection{Training configurations}
\textbf{Datasets:} We use CIFAR-10 \citep{cifar10}, LSUN-bedroom, and LSUN-Church \citep{lsun} datasets for unconditional generation in the main experiments. Additionally, we apply CelebA\citep{celeba} and CelebA-HQ~\citep{CelebAMask-HQ} for complementary justification. For text-to-image experiments, we use CUB-200~\citep{cub} and MS-COCO~\citep{mscoco}. The images consist of  pixels for CIFAR-10. For the other datasets, we apply center-crop along the short edge and resize to the target resolution ( for CelebA;  for the others). 

\textbf{Diffusion schedule:} For all datasets, we strictly follow the diffusion process used in our backbone models, and instantiate the truncated diffusion schedule by obtaining the first  diffusion rates . For example, if our goal is to fit a model with NFE=50, to truncate the diffusion process used in \citet{ddpm} (, , T=1000), we first initialize , , ... , and then taking the first 49 steps to complete the truncation. 

\textbf{Optimization:} We train our models using the Adam optimizer \citep{adam}, where most of the hyperparameters match the setting in \citet{xiao2021tackling}, and we slightly modify the generator learning rate to match the setting in~\citet{ddpm}, as shown in Table \ref{tab:optimizer}. 

We train our models using V100 GPUs, with CUDA 10.1, PyTorch 1.7.1. The training takes approximately 2 days on CIFAR-10 with 4 GPUs, and a week on CelebA-HQ and LSUN-Church with 8 GPUs.   


\begin{table}[h]
\vspace{-5mm}
\centering
\caption{Optimization hyper-parameters.}\vspace{-3mm}
\label{tab:optimizer}
\resizebox{\columnwidth}{!}{
\begin{tabular}{lcccc}
\toprule
& CIFAR10 & CelebA & CelebA-HQ & LSUN  
\\ \midrule
Initial learning rate for discriminator &&  &  & \\
Initial learning rate for navigator (if applicable) &&  &  & \\
Initial learning rate for generator &&  &  & \\
Adam optimizer  &0.5&0.5& 0.5& 0.5\\
Adam optimizer  &0.9&0.9&0.9&0.9\\
EMA &0.9999&0.9999&0.9999 &0.9999\\
Batch size &128&128 &64&64\\
 of training iterations &800k &800k &0.5M&2.4M(bedroom)/1.2M(church)\\
 of GPUs &4&8&8&8\\
\bottomrule
\end{tabular}
}\vspace{-3mm}
\end{table}

For TDPM+, where we use StyleGAN2 generator as , we directly use their original training hyper-parameters and train the model in parallel with the diffusion model. For TLDM, we set the base learning rate as  and the mini-batch size is set to 64.
{For the ImageNet1K-6464 experiments, we use StyleGAN-XL generator as  and strictly follow all the default training hyper-parameters. To simplify the implementation and save computation, instead of applying the default progressive growing pipeline , we directly train the implicit model on 6464 images corrupted at . Without using the progressive growing pipeline, the result of StyleGAN-XL shown in Table~\ref{fig:imagenet} is clearly worse than the progressive one reported in their paper (FID 1.51). However, when used as the implicit model of TDPM, the final performance of TDPM becomes competitive with this result. 
}

\textbf{Evaluation:} When evaluating the sampling time, we use models trained on CIFAR-10 and generate a batch of 128 samples. When evaluating the  FID, and recall score, following the convention, we use 50k generated samples for CIFAR-10, LSUN-bedroom and LSUN-church, 30k samples for CelebA-HQ (since the CelebA HQ dataset contains only 30k samples), 30k samples for the text-to-image datasets. The recall scores are calculated with the recipe in ~\citet{kynkaanniemi2019improved}. In the sampling stage, we follow our backbone to apply the same guidance in the diffusion part () if applicable. Specifically, for LDM backbone, we use classifier-free guidance~\citep{ho2022classifier} with scale 1.5 and there are no DDIM steps for TDLM. 

\subsection{Additional results on unconditional generation}\label{appendix:additional unconditional results}
\begin{table}[ht]
\centering
\small 
\caption{\small {Full comparison of unconditional generation on CIFAR-10. Models are grouped by the orders of sampling steps, with the best FID, and Recall in each group marked in bold. The TDPM+ results are produced using ADM and StyleGAN2 backbone. TDPM with NFE=1 is equivalent to training a GAN with the DDPM architecture as the generator.}}
\label{tab:cifar_full}
\centering
\resizebox{0.7\columnwidth}{!}{
\begin{tabular}{lccc}
\toprule[1.5pt]
 Model & NFE    & FID & Recall  
\\ \midrule
Improved DDPM \citep{nichol2021improved}&4000  &2.90& - \\
UDM \citep{kim2021score}&2000  &2.33& - \\
Likelihood SDE \citep{song2021maximum}&2000 &2.87& - \\
Score SDE (VE) \citep{song2021scorebased} &2000 & \textbf{2.20} &  0.59 \\
Score SDE (VP) \citep{song2021scorebased} &2000& 2.41& 0.59 \\ \hline
NCSN \citep{scorematching} &1000 & 25.3 &- \\
Adversarial DSM \citep{adversarialdiff} & 1000 & 6.10 & -  \\ 
VDM \citep{kingma2021variational} &1000 &4.00 &  \\
D3PMs \citep{austin2021structured} &1000 &7.34&-\\
DiffuseVAE \citep{pandey2022diffusevae}, T=1000 & 1000 & 8.72 & -  \\
DDPM \citep{ddpm} &1000   & 3.21& 0.57 \\
\midrule
Recovery EBM \citep{gao2021learning}&180  & 9.58 &- \\
Gotta Go Fast \citep{jolicoeur2021gotta}&180&{2.44}&-\\
LSGM \citep{vahdat2021score} &147   & \textbf{2.10}& \textbf{0.61} \\
Probability Flow (VP) \citep{song2021scorebased} &140  & 3.08& 0.57 \\
DiffuseVAE \citep{pandey2022diffusevae}, T=100  & 100  & 11.71 & - \\
\rowcolor{Gray}
TDPM, T=99 (ours) &100   & 2.97& 0.57\\
\rowcolor{Gray}
TDPM+, T=99 (ours) &100   & 2.83& 0.58\\
\midrule
FastDDPM, T=50 \citep{kong2021fast} & 50 & 3.41 & 0.56 \\
DDIM, T=50 \citep{ddim} & 50   & 4.67& 0.53 \\
SNGAN+DGflow \citep{ansari2021refining} & 25 & 9.62& 0.48 \\
\rowcolor{Gray}
TDPM, T=49 (ours) &50  & \textbf{3.11}& {0.57} \\
\rowcolor{Gray}
TDPM+, T=49 (ours) &50  & \textbf{2.96}& \textbf{0.58} \\
\midrule
{Progressive distiallation~\citep{salimans2022progressive}} &8 & \textbf{2.57}& - \\
{Denoising Diffusion GAN~\citep{xiao2021tackling}}, T=8 &8  & 4.36& {0.56} \\ \hdashline
{Progressive distiallation~\citep{salimans2022progressive}} &4  & \textbf{3.00}& - \\
{Denoising Diffusion GAN~\citep{xiao2021tackling}}, T=4 &4  & 3.75& \textbf{0.57} \\
\rowcolor{Gray}
TDPM, T=4 (ours) &5  & {3.51}& {0.55}\\
\rowcolor{Gray}
TDPM+, T=4 (ours) &5  & {3.17}& \textbf{0.57}\\\hdashline
{Progressive distiallation~\citep{salimans2022progressive}} &2 & 4.51& - \\
{Denoising Diffusion GAN~\citep{xiao2021tackling}}, T=2 &2 & 4.08& 0.54 \\
\rowcolor{Gray}
TDPM, T=1 (ours) &2  & {4.47} & {0.53} \\
\rowcolor{Gray}
TDPM+, T=1 (ours) &2  & \textbf{3.86} & \textbf{0.56} \\
\hdashline
DDPM Distillation \citep{luhman2021knowledge} &1   & 9.36& \textbf{0.51} \\
SNGAN \citep{miyato2018spectral} &1  & 21.7 & 0.44 \\ 
AutoGAN \citep{gong2019autogan} &1   & 12.4 &0.46 \\ 
TransGAN \citep{jiang2021transgan} &1 &9.26&-\\
StyleGAN2 w/o ADA \citep{karras2020training} &1   & 8.32& 0.41\\
StyleGAN2 w/ ADA \citep{karras2020training} &1  & \textbf{2.92}& 0.46\\
StyleGAN2 w/ Diffaug \citep{zhao2020differentiable} &1 & 5.79& 0.42 \\
{Progressive distiallation~\citep{salimans2022progressive}}&1& 9.12& -  \\
{Denoising Diffusion GAN~\citep{xiao2021tackling}}, T=1 &1  & 14.6& {0.19} \\
\rowcolor{Gray}
TDPM, T=0 (ours) &1  & 7.34 & 0.46\\
\bottomrule[1.5pt]
\end{tabular}
}
\vspace{-4mm}
\end{table}



\begin{figure*}[ht]
    \centering
    \includegraphics[width=.8\textwidth]{image/cifar10_celeba_grid.pdf}\vspace{-2.5mm}
    \caption{\small Qualitative results on CIFAR-10 and CelebA ().}
    \label{fig:image_cifar10_celeba}\vspace{-2.5mm}
\end{figure*}


\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{image/lsun_church.pdf}
    \caption{\small Qualitative results of TDPM on LSUN-Church (), with , , and . Note  in TDPM. Each group presents generated samples from  (left) and  (right).}
    \label{fig:image_church}
\end{figure*}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{image/lsun_bedroom.pdf}
    \caption{\small Analogous qualitative results to \Figref{fig:image_church} on LSUN-Bedroom. Produced by TDPM.}
    \label{fig:image_bedroom}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{image/CelebA_HQ.pdf}
    \caption{Analogous qualitative results to \Figref{fig:image_church} on CelebA-HQ. Produced by TDPM.}
    \label{fig:image_celebaHQ}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{image/lsun_church_ct.pdf}
    \caption{Analogous qualitative results to \Figref{fig:image_church} on LSUN-Church. Produced by TDPM-CT.}
    \label{fig:image_church_ct}\vspace{-5mm}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{image/lsun_bedroom_ct.pdf}
    \caption{Analogous qualitative results to \Figref{fig:image_church} on LSUN-Bedroom. Produced by TDPM-CT.}
    \label{fig:image_bedroom_ct}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{image/CelebA_HQ_ct.pdf}
    \caption{Analogous qualitative results to \Figref{fig:image_church} on CelebA-HQ. Produced by TDPM-CT.}
    \label{fig:image_celebaHQ_ct}\vspace{-5mm}
\end{figure*}
\clearpage

\subsection{Additional results on text-to-image generation}\label{appendix:additional results_textimage}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{image/textimg_vis_wide_50.pdf}
    \caption{Additional text-to-image generation results with different text prompt, produced by TLDM with .}
    \label{fig:txt-img-wide-50}\vspace{-5mm}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{image/textimg_vis_wide_5.pdf}
    \caption{Additional text-to-image generation results with different text prompt, produced by TLDM with .}
    \label{fig:txt-img-wide-5}\vspace{-5mm}
\end{figure*}



\end{document}
