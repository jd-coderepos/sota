

\documentclass[runningheads]{llncs}
\usepackage{graphicx}


\usepackage{tikz}
\usepackage{comment}
\usepackage{amsmath,amssymb} \usepackage{color}
\usepackage{booktabs}
\usepackage{float}
\usepackage[normalem]{ulem}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{array}
\usepackage{adjustbox}
\newcommand*{\affaddr}[1]{#1} \newcommand*{\affmark}[1][*]{\textsuperscript{#1}}
\newcommand*{\emails}[1]{\texttt{#1}}
\usepackage[accsupp]{axessibility}  \definecolor{Teal}{RGB}{0,105,190}
\usepackage[pagebackref,breaklinks,colorlinks,citecolor=Teal]{hyperref}

\usepackage{wrapfig}
\usepackage{CJKutf8}
\usepackage{CJKutf8}

\begin{document}
\pagestyle{headings}
\mainmatter
\def\ECCVSubNumber{296}  

\title{BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis} 

\titlerunning{BEAT}

\author{Haiyang Liu\affmark[1], Zihao Zhu\affmark[2], Naoya Iwamoto\affmark[3], Yichen Peng\affmark[4], \\
Zhengqing Li\affmark[3], You Zhou\affmark[3], Elif Bozkurt\affmark[5], Bo Zheng\affmark[3]}

\institute{\affaddr{\affmark[1]The University of Tokyo. \affmark[2]Keio University.\\
\affmark[2]Digital Human Lab, Huawei Technologies Japan K.K. \\
\affmark[4]Japan Advanced Institute of Science and Technology. \affmark[5]Huawei Turkey R\&D Center.}}
\authorrunning{H. Liu et al.}

\maketitle 
\begin{figure}[h]
    \centering
    \vspace{-0.5cm}
    \includegraphics[trim=0 0 0 0, clip,width=0.9\textwidth]{fig/fig1_final.pdf}
    \vspace{-0.3cm}
   \caption{\textbf{Overview.} BEAT is a large-scale, multi-modal mo-cap human gestures dataset with semantic, emotional annotations, diverse speakers and multiple languages.}
    \label{concept}
    \vspace{-1.2cm}
\end{figure}
 \begin{abstract}
Achieving realistic, vivid, and human-like synthesized conversational gestures conditioned on multi-modal data is still an unsolved problem due to the lack of available datasets, models and standard evaluation metrics. 
To address this, we build \textbf{B}ody-\textbf{E}xpression-\textbf{A}udio-\textbf{T}ext dataset, \textbf{BEAT}, which has i) 76 hours, high-quality, multi-modal data captured from 30 speakers talking with eight different emotions and in four different languages, ii) 32 millions frame-level emotion and semantic relevance annotations.
Our statistical analysis on BEAT demonstrates the correlation of conversational gestures with \textit{facial expressions}, \textit{emotions}, and \textit{semantics}, in addition to the known correlation with \textit{audio}, \textit{text}, and \textit{speaker identity}.
Based on this observation, we propose a baseline model, \textbf{Ca}scaded \textbf{M}otion \textbf{N}etwork \textbf{(CaMN)}, which consists of above six modalities modeled in a cascaded architecture for gesture synthesis. To evaluate the semantic relevancy, we introduce a metric, Semantic Relevance Gesture Recall (\textbf{SRGR}). 
Qualitative and quantitative experiments demonstrate metrics' validness, ground truth data quality, and baseline's state-of-the-art performance. 
To the best of our knowledge, BEAT is the largest motion capture dataset for investigating human gestures, which may contribute to a number of different research fields, including controllable gesture synthesis, cross-modality analysis, and emotional gesture recognition. The data, code and model are available on \url{https://pantomatrix.github.io/BEAT/}. 
\end{abstract} \section{Introduction}
\label{sec:intro}

Synthesizing conversational gestures can be helpful for animation, entertainment, education and virtual reality applications. To accomplish this, the complex relationship between speech, facial expressions, emotions, speaker identity and semantic meaning of gestures has to be carefully considered in the design of the gesture synthesis models.  

While synthesizing conversational gestures based on audio \cite{li2021audio2gestures,yoon2020speech,ginosar2019learning} or text \cite{yoon2019robots,bhattacharya2021text2gestures,ali2020automatic,alexanderson2020generating} has been widely studied, synthesizing realistic, vivid, human-like conversational gestures is still unsolved and challenging for several reasons. 
i) \textbf{Quality and scale of the dataset}. 
Previously proposed methods \cite{yoon2020speech,li2021audio2gestures} were trained on limited mo-cap datasets \cite{takeuchi2017creating,ferstl2018investigating} or on pseudo-label \cite{ginosar2019learning,yoon2020speech,habibie2021learning} datasets (\textit{cf.} Table \ref{tab:tab1}), which results in limited generalization capability and lack of robustness.
ii) \textbf{Rich and paired multi-modal data}. Previous works adopted one or two modalities \cite{ginosar2019learning,yoon2019robots,yoon2020speech} to synthesize gestures and reported that conversational gestures are determined by multiple modalities together. However, due to the lack of paired multi-modal data, the analysis of other modalities, \textit{e.g.}, facial expression, for gesture synthesis is still missing. 
iii) \textbf{Speaker style disentanglement}. All available datasets, as shown in Table \ref{tab:tab1}, either have only a single speaker \cite{ferstl2018investigating}, or many speakers but different speakers talk about different topics \cite{habibie2021learning,yoon2020speech,ginosar2019learning}. Speaker-specific styles were not much investigated in previous studies due to the lack of data. 
iv) \textbf{Emotion annotation}. Existing work \cite{bhattacharya2021speech2affectivegestures} analyzes the emotion-conditioned gestures by extracting implicit sentiment features from texts. Due to the unlabeled, limited emotion categories in the dataset \cite{yoon2020speech}, it cannot cover enough emotion in daily conversations.  
v) \textbf{Semantic relevance}. Due to the lack of semantic relevance annotation, only a few works \cite{kucherenko2020gesticulator,yoon2020speech} analyze the correlation between generated gestures and semantics though listing subjective visualization examples. It will enable synthesizing context-related meaningful gestures if existing semantic labels of gestures.
In conclusion, the absence of a large-scale, high-quality multi-modal dataset with semantic and emotional annotation is the main obstacle to synthesizing human-like conversational gestures.
 

There are two design choices for collecting unlabeled multi-modal data, i) the pseudo-label approach \cite{ginosar2019learning,yoon2020speech,habibie2021learning}, \textit{i.e.}, extracting conversational gestures, facial landmark from in-the-wild videos using 3D pose estimation algorithms \cite{cao2019openpose} and ii) the motion capture approach \cite{ferstl2018investigating}, \textit{i.e.}, recording the data of speakers through predefined themes or texts. In contrast to the pseudo-labeling approach, which allows for low-cost, semi-automated access to large-scale training data, \textit{e.g.}, 97h \cite{yoon2020speech}, motion-captured data requires a higher cost and more manual work resulting in smaller dataset sizes, \textit{e.g.}, 4h \cite{ferstl2018investigating}. However, Due to the motion capture can be strictly controlled and designed in advance, it is able to ensure the quality and diversity of the data, e.g., eight different emotions of the same speaker, and different gestures of 30 speakers talking in the same sentences. Besides, high-quality motion capture data are indispensable to evaluate the effectiveness of pseudo-label training. 




\begin{table}
\caption{\textbf{Comparison of Datasets.} We compare with all 3D conversational gesture and face datasets. ``\#", ``LM" and ``BSW" indicate the number, landmark and blendshape weight, respectively. \colorbox[rgb]{0.574,0.813,0.687}{best} and \colorbox[rgb]{0.855,0.933,0.894}{second} are highlighted. Our dataset is the largest mocap dataset with multi-modal data and annotations}
\begin{adjustbox}{width=\columnwidth, center}
\label{tab:tab1}
\begin{tabular}{l|c|cccccc|cc|cc}
        & \textbf{Quailty}                                                       & \multicolumn{6}{c|}{\textbf{Modality}}                                                                                                                                                                                                     & \multicolumn{2}{c|}{\textbf{Annotation}}                                  & \multicolumn{2}{c}{\textbf{Scale}}                                            \\
dataset &                                                                     & \#body                               & \#hand                               & face                                 & audio                                     & text                               & \#speaker                            & \#emo                                & sem                                & \#seq                                  & dura                                 \\ 
\hline
TED \cite{yoon2020speech}     & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}pseudo\\label\end{tabular}} & 9                                    & -                                    & -                                    & En                                        &  \checkmark                                  & {\cellcolor[rgb]{0.574,0.813,0.687}} 100                                  & -                                    & -                                  & 1400                                   & {\cellcolor[rgb]{0.574,0.813,0.687}} 97h                                  \\
S2G \cite{ginosar2019learning,habibie2021learning}     &                                                                        & 14                                   & 42                                   & 2D LM                                   & En                                        & -                                  & 6                                    & -                                    & -                                  &  N/A                                      & 33h                                  \\ 
\hline
MPI \cite{volkova2014mpi}     & \multirow{5}{*}{mo-cap}                                                 & 23                                   & -                                    & -                                    & -                                         &  \checkmark                                  & 1                                   & {\cellcolor[rgb]{0.574,0.813,0.687}}11 & -                                  & 1408                                   & 1.5h                                 \\
VOCA \cite{cudeiro2019capture}    &                                                                        & -                                    & -                                    & {\cellcolor[rgb]{0.574,0.813,0.687}}3D Mesh                                   & En                                        & -                                  & 12                                   & -                                    & -                                  & 480                                    & 0.5h                                 \\
Takechi \cite{takeuchi2017creating} &                                                                        & 24                                   & 38                                   & -                                    & Jp                                        & -                                  & 2                                    & -                                    & -                                  & 1049                                   & 5h                                   \\
Trinity \cite{ferstl2018investigating} &                                                                        & 24                                   & 38                                   & -                                    & En                                        & \checkmark                                   & 1                                    & -                                    & -                                  & 23                                     & 4h                                   \\ 
\hline
BEAT (Ours)    & {\cellcolor[rgb]{0.574,0.813,0.687}}mo-cap                                & {\cellcolor[rgb]{0.574,0.813,0.687}}27 & {\cellcolor[rgb]{0.574,0.813,0.687}}48 & {\cellcolor[rgb]{0.855,0.933,0.894}}3D BSW & {\cellcolor[rgb]{0.574,0.813,0.687}}E/C/S/J & {\cellcolor[rgb]{0.574,0.813,0.687}}\checkmark & {\cellcolor[rgb]{0.855,0.933,0.894}}30 & {\cellcolor[rgb]{0.855,0.933,0.894}}8    & {\cellcolor[rgb]{0.574,0.813,0.687}}\checkmark & {\cellcolor[rgb]{0.574,0.813,0.687}}2508 & {\cellcolor[rgb]{0.855,0.933,0.894}}76h 
\end{tabular}
    \end{adjustbox}
\vspace{-0.6cm}
\end{table} 
Based on the above analysis, to address these data-related problems, we built a mo-cap dataset \textbf{BEAT} containing semantic and eight different emotional annotations (\textit{cf.} Figure \ref{concept}), from 30 speakers in four modalities of \textbf{B}ody-\textbf{E}xpression-\textbf{A}udio-\textbf{T}ext, annotated in total of 30M frames. 
The motion capture environment is strictly controlled to ensure quality and diversity, with 76 hours and more than 2500 topic-segmented sequences. Speakers with different language mastery provided data in three other languages at different durations and in pairs. The ratio of actors/actresses, range of phonemes, and variety of languages are carefully designed to cover natural language characteristics. For emotional gestures, feedback on the speakers' expressions was provided by professional instructors during the recording process and re-recorded in case of non-expressive gesturing to ensure the expressiveness and quality of the entire dataset.
After statistical analysis on BEAT, we observed the correlation of conversational gestures with \textit{facial expressions}, \textit{emotions}, and \textit{semantics}, in addition to the known correlation with \textit{audio}, \textit{text}, and \textit{speaker identity}.  

Additionally, we propose a baseline neural network architecture,  \textbf{Ca}scaded \textbf{M}otion \textbf{N}etwork (\textbf{CaMN}), which learns synthesizing body and hand gestures by inputting all six modalities mentioned above. The proposed model consists of cascaded encoders and decoders for enhancing the contribution of audio and facial modalities. Besides, in order to evaluate the semantic relevancy, we propose \textbf{S}emantic-\textbf{R}elevant \textbf{G}esture \textbf{R}ecall (\textbf{SRGR}), which weights Probability of Correct Keypoint (PCK) based on  semantic scores of the ground truth data. Overall, our contributions can be summarized as follows:
\begin{itemize}
\setlength\itemsep{-.1em}
\item We release BEAT, which is the first gesture dataset with semantic and emotional annotation, and the largest motion capture dataset in terms of duration and available modalities to the best of our knowledge.


\item We propose CaMN as a baseline model that inputs audio, text, facial blendweight, speaker identity, emotion and semantic score to synthesize conversational body and hand gestures through cascaded network architecture.

\item  We introduce SRGR to evaluate the semantic relevancy as well as the human preference for conversational gestures. 

\end{itemize}
Finally, qualitative and quantitative experiments demonstrate the data quality of BEAT, the state-of-the-art performance of CaMN and the validness of SRGR. \section{Related Work}
\label{sec:related}

\subsubsection{Conversational Gestures Dataset.} We first review mo-cap and pseudo-label conversational gestures datasets. Volkova \textit{et al.} \cite{volkova2014mpi} built a mo-cap emotional gestures dataset in 89 mins with text annotation, Takeuchi \textit{et al.} \cite{takeuchi2017speech} captured an interview-like audio-gesture dataset in total 3.5-hour with two Japanese speakers. Ferstl and Mcdonnell \cite{ferstl2018investigating} collected a 4-hour dataset, Trinity, with a single male speaker discussing hobbies, \textit{etc.}, which is the most common used mo-cap dataset for conversational gestures synthesis. On the other hand, Ginosar \textit{et al.} \cite{ginosar2019learning} used OpenPose \cite{cao2019openpose} to extract 2D poses from YouTube videos as training data for 144 hours, called S2G Dataset. Habibie \textit{et al.} \cite{habibie2021learning} extended it to a full 3D body with facial landmarks, and the last available data is 33 hours. Similarly, Yoon \textit{et al.} \cite{yoon2020speech} used VideoPose3D \cite{pavllo20193d} to build on the TED dataset, which is 97 hours with 9 joints on upper body. The limited data amount of mo-cap and noise in ground truth makes a trade-off for the trained network's generalization capability and quality. Similar to our work, several datasets are built for talking-face generation and the datasets can be divided into 3D scan face, \textit{e.g.}, VOCA \cite{takeuchi2017creating} and MeshTalk \cite{richard2021meshtalk} or RGB images \cite{alghamdi2018corpus,cao2014crema,cooke2006audio,jackson2014surrey,wang2020mead}. However, these datasets cannot be adopted to synthesize human gestures.

\vspace{-0.5cm}

\subsubsection{Semantic or Emotion-Aware Motion Synthesis.}
Semantic analysis of motion has been studied in the action recognition and the sign-language analysis/synthesis research domains. For example, in some of action recognition datasets \cite{ionescu2013human3,kay2017kinetics,carreira2017quo,chen2015utd,singh2010muhavi,bloom2012g3d,liu2016benchmarking,song2017end,perera2020multiviewpoint,wang2012robust} clips of action with the corresponding label of a single action, \textit{e.g.}, running, walking \cite{punnakkal2021babel} is used. Another example is audio-driven sign-language synthesis \cite{kapoor2021towards}, where hand gestures have specific semantics. However, these datasets do not apply to conversational gestures synthesis since gestures used in natural conversations are more complex than single actions, and their semantic meaning differs from sign-language semantics. Recently, Bhattacharya \cite{bhattacharya2021speech2affectivegestures}  extracted emotional cues from text and used them for gesture synthesis. However, the proposed method has limitations in the accuracy of the emotion classification algorithm and the diversity of emotion categories in the dataset.

\vspace{-0.5cm}

\subsubsection{Conditional Conversational Gestures Synthesis.} Early baseline models were released with datasets such as text-conditioned gesture \cite{yoon2019robots}, audio-conditioned gesture \cite{ginosar2019learning,takeuchi2017speech,ferstl2018investigating}, and audio-text-conditioned gesture \cite{yoon2020speech}. These baseline models were based on CNN and LSTM for end-to-end modelling. Several efforts try to improve the performance of the baseline model by input/output representation selection \cite{kucherenko2021moving,ferstl2021expressgesture}, adversarial training \cite{ferstl2020adversarial} and various types of generative modeling techniques \cite{wu2021probabilistic,lu2021double,wu2021modeling,ahuja2020style}, which can be summarized by "Estimating a better distribution of gestures based on the given conditions.". As an example, StyleGestures \cite{alexanderson2020style} uses Flow-based model \cite{henter2020moglow} and additional control signal to sample gesture from the distribution. Probabilistic gesture generation enables generating diversity based on noise, which is achieved by CGAN \cite{wu2021modeling}, WGAN \cite{wu2021probabilistic}. However, due to the lack of paired multi-modal data, the analysis of other modalities, \textit{e.g.}, facial expression, for gesture synthesis is still missing.  

\vspace{-0.5cm} 

\section{BEAT: Body-Expression-Audio-Text Dataset}
\label{sec:other}
In this section, we introduce the proposed Body-Expression-Audio-Text (BEAT) Dataset. First, we describe the dataset acquisition process and then introduce text, emotion, and semantic relevance information annotation. Finally, we use BEAT to analyze the correlation between conversational gestures and emotions and show the distribution of semantic relevance.

\subsection{Data Acquisition}

\begin{figure}[h]
    \centering
    \includegraphics[trim=0 0 0 0, clip, width=0.9\textwidth]{fig/fig2_v7.pdf}
    \vspace{-0.3cm}
    \caption{\textbf{Capture System and Subject Distribution of BEAT.} (a) A 16-camera motion capture system is adopted to record data in Conversation and Self-Talk sessions. (b) Gestures are divided into four categories in Conversation session. (c) Seven additional emotion categories are set in equal proportions in the self-talk session. Besides, (d) our dataset includes four languages which mainly consist of English, (e) by 30 speakers from ten countries with different recording duration.}
    \label{fig:fig2}
    \vspace{-1cm}
\end{figure} 
\subsubsection{Motion Capture System.} The motion capture system shown in Figure \ref{fig:fig2}a, is based on 16 synchronized cameras recording motion at 120 Hz. We use Vicon's suits with 77 markers (\textit{cf.} supplementary materials for the location of markers on the body). The facial capture system uses ARKit with a depth camera on iPhone 12 Pro, which extracts 52 blendshape weights at 60 Hz. The blendshape targets are designed based on Facial Action Coding System (FACS) and are widely used by industry novice users. The audio is recorded in a 48KHz stereo.


\vspace{-0.5cm}

\subsubsection{Design Criteria.} BEAT is equally divided into \textit{conversation} and \textit{self-talk} sessions, which consist of 10-min and 1-min sequences, respectively. 
The conversation is between the speaker and the instructor remotely, \textit{i.e.}, to ensure only the speaker's voice is recorded. As shown in Figure \ref{fig:fig2}b, The speaker's gestures are divided into four categories talking, instantaneous reactions to questions, the state of thinking (silence) and asking. We timed each category's duration during the recording process. Topics were selected from 20 predefined topics, which cover 33\% and 67\% debate and description topics, respectively. \textit{Conversation} sessions would record the neutral conversations without acting to ensure the diversity of the dataset.
The \textit{self-talk} sessions consist of 120 1-minute self-talk recordings, where speakers answer questions about daily conversation topics, \textit{e.g.}, personal experiences or hobbies. The answers were written and proofread by three English native speakers, and the phonetic coverage was controlled to be similar to the frequently used 3000 words \cite{hornbyoxford}. We covered 8 emotions, \textit{neutral, anger, happiness, fear, disgust, sadness, contempt} and \textit{surprise}, in the dataset referring to \cite{livingstone2018ryerson} and the ratio of each emotion is shown in Figure \ref{fig:fig2}c. Among the 120 questions, 64 were for neutral emotions, and the remaining seven had eight questions each. Different speakers were asked to talk about the same content with their personalized gestures. Details about predefined answers and pronunciation distribution are available in the supplementary materials.

\vspace{-0.5cm}

\subsubsection{Speaker Selection and Language Ratio.} We strictly control the proportion of languages as well as accents to ensure the generalization capability of the dataset. As shown in Figure \ref{fig:fig2}d, the dataset consists mainly of English data: 60h (81\%), 12h of Chinese, 2h of Spanish and Japanese. The Spanish and Japanese are also 50\% of the size of the previous mo-cap dataset \cite{ferstl2018investigating}. The English component includes 34h of 10 native English speakers, including the US, UK, and Australia, and 26h of 20 fluent English speakers from other countries. As shown in Figure \ref{fig:fig2}e, 30 speakers (including 15 females) from different ethnicities can be grouped into two depending on their total recording duration as 4-hour (10 speakers) and 1-hour (20 speakers), where the 1-hour data is proposed for few-shot learning experiments. It is recommended to check the supplementary material for details of the speakers.

\vspace{-0.5cm}

\subsubsection{Recording.} Speakers were asked to read answers in self-talk sections proficiently. However, they were not guided to perform a specific style of gesture but were encouraged to show a natural, personal, daily style of conversational gestures. Speakers would watch 2-10 minutes of emotionally stimulating videos corresponding to different emotions before talking with the particular emotion. A professional speaker would instruct them to elicit the corresponding emotion correctly. We re-record any unqualified data to ensure the data's correctness and quality.



\subsection{Data Annotation}
\subsubsection{Text Alignment.} We use an in-house-built Automatic Speech Recognizer (ASR) to obtain the initial text for the conversation session and proofread it by annotators. Then, we adopt Montreal Forced Aligner (MFA) aligner \cite{mcauliffe2017montreal} for temporal alignment of the text with audio.

\vspace{-0.5cm}
\subsubsection{Emotion and Semantic Relevance.} The 8-class emotion label of self-talk is confirmed, and the on-site supervision guarantees the correctness. For the conversation session, annotators would watch the video with corresponding audio and gestures to perform frame-level annotation. For the semantic relevance, we get the score on a scale of 0-10 from assigned 600 annotators from Amazon Mechanical Turk (AMT). The annotators were asked to annotate a small amount of test data as a qualification check, of which only 118 annotators succeeded in the qualification phase for the final data annotation. We paid  \\leq\textbf{v}^\text{T} \in \mathbb{R}^{300} E_\text{T}i2f=34\textbf{z}^\text{T} \in \mathbb{R}^{128} \textbf{v}^\text{ID} \in \mathbb{R}^{30}\textbf{v}^\text{E} \in \mathbb{R}^{8}E_\text{ID}E_\text{E}\textbf{z}^\text{ID} \in \mathbb{R}^{8}\textbf{z}^\text{E} \in \mathbb{R}^{8} \textbf{v}^\text{A} \in \mathbb{R}^{1067}E_\text{A}E_\text{A}\textbf{z}^\text{A} \in \mathbb{R}^{128}\textbf{v}^\text{F} \in \mathbb{R}^{52}E_\text{F}\textbf{z}^\text{F} \in \mathbb{R}^{32} D_\text{B}D_\text{F}\textbf{z}^\text{B} \in \mathbb{R}^{256}\textbf{z}^\text{H} \in \mathbb{R}^{256}\hat{\textbf{v}}^\text{B} \in \mathbb{R}^{27\times3}\hat{\textbf{v}}^\text{H} \in \mathbb{R}^{48\times3}\textbf{z}^\text{M} \in \mathbb{R}^{549}
\ell_{\text{Gesture Rec.}} =\mathbb{E}\left[\left\|\textbf{v}^{B}-\hat{\textbf{v}}^{B}\right\|_{1}\right] + \alpha \mathbb{E}\left[\left\|\textbf{v}^{H}-\hat{\textbf{v}}^{H}\right\|_{1}\right],\alpha\lambda\beta_{0}\beta_{1}\delta
    D_{SRGR} = \lambda \sum \frac{1}{T \times J} \sum_{t=1}^{T} \sum_{j=1}^{J} \mathbf{1}\left[\left\|p_{t}^{j}-\hat{p}_{t}^{j}\right\|_{2}<\delta\right],\mathbf{1}T, J\downarrow\uparrow\uparrow\downarrow\uparrow\uparrow
\begin{tabular}{ll}
                    & Topics                                                                                              \\ 
\hline
1                   & Do you think smart-phone has destroyed communication among friends and family?                                    \\
\multirow{2}{*}{2}  & Now people usually work from home, for you, which one you prefer?~                    \\
                    & Face-to-face communication or work from home?        \\
3                   & In general, people are living longer now. Why? Discuss the causes of this phenomenon.  \\
\multirow{2}{*}{4}  & Some people believe that the Earth is being harmed (damaged) by human activity ~                            \\
                    & Others feel that human activity makes the Earth a better place to live. What is your opinion?.                                                         \\
\multirow{2}{*}{5}  & Some people are always in a hurry to go places and get things done then take a rest.                                 \\
                    & Other people prefer to take their time and live life at a slower pace. Which do you prefer?         \\
\multirow{2}{*}{6}  & Some people say that advertising encourages us to buy things we really do not need.                 \\
                    & Others say that advertisements tell us about new products that may improve our lives.~ ~            \\
\multirow{2}{*}{7}  & Some jobs (such as Salesmen) can be done by human or by robots (AI).~                       \\
                    & Which do you prefer? get the service from human or robots (AI)?                                  \\
\multirow{2}{*}{8}  & Television, news, and other media pay too much attention to the                                     \\
                    & personal lives of famous people such as public figures and celebrities                              \\
9                   & Is it more important to be able to work with a group of people on a team or to work independently?                \\
\multirow{2}{*}{10} & Do you agree or disagree with the following statement?~                                             \\
                    & Only people who earn a lot of money are successful.                                                 \\ 
\hline
11                  & Introduce some places, cities, countries, or even planets                                                                           \\
12                  & Introduce some celebrities, artists or you friends                                                                           \\
13                  & Introduce some sports or physic knowledge.                                                                                \\
14                  & Introduce some pets or plants.                                                                            \\
15                  & Introduce some electronic Products, cars or other vehicles                                                                                  \\
16                  & Introduce some video games, musics, books, TV programs, stories, or movies.                                                                                  \\
17                  & Introduce some foods or chemical phenomenon.                                                                     \\
18                  & Introduce some historical events.                                                                     \\
19                  & Introduce some psychological phenomenon.                                                                                 \\
20                  & Introduce some military doctrine.                                                                
\end{tabular}\times\timesN
    L1 Div. =  \frac{1}{2 N (N-1)} \sum_{t=1}^{N} \sum_{j=1}^{N} \left\|p_{t}^{i}-\hat{p}_{t}^{j}\right\|_{1},
\operatorname{FGD}(\textbf{m}, \hat{\textbf{m}})=\left\|\mu_{r}-\mu_{g}\right\|^{2}+\operatorname{Tr}\left(\Sigma_{r}+\Sigma_{g}-2\left(\Sigma_{r} \Sigma_{g}\right)^{1 / 2}\right),\mu_{r}\Sigma_{r}z_{r}\mu_{g}\Sigma_{g}z_{g}\hat{\textbf{m}}
\text {BeatAlign}= \frac{1}{G} \sum_{b_{G}\in G} \exp \left(-\frac{\min _{b_{A}\in A}\left\|b_{G}-b_{A}\right\|^{2}}{2 \sigma^{2}}\right),G, A\sigma$ is adjusted based on fps and we set 0.3 in our paper.


\section{More Subjective Results and Videos.}
The subjective results of generated gestures are shown in Figure \ref{fig:fig7}. More video results and data are available on the project page. We also provide results on other languages, \textit{e.g.}, Japanese data.

\begin{figure*}[]

\begin{center}
\includegraphics[width=0.80\columnwidth]{fig/sfig_final.pdf}
\end{center}
\caption{
\textbf{Results Visualization.} Ground truth (top) and generated results with neutral (middle) and fear (down) emotions. 
}
\label{fig:fig7}
\end{figure*} 
\section{Details of baseline training}
Currently, we do not split the dataset based on speaker, \textit{i.e.}, some speakers only exist in the validation/test data, since the speaker ID is one of the inputs. For each speaker, we use the ratio 10:1:1 for the train/valid/test data splits. For baseline training, we select the best model based on the lowest validation FGD score during training for all baseline models. The final selected epoch is listed in Table \ref{tab:sptab1}. 
\begin{table}
\centering
\caption{\textbf{Best Epoch for Baselines.}}
\label{tab:sptab1}
\begin{tabular}{lccccc}
\multicolumn{1}{c}{} & Seq2Seq & S2G & A2G & MultiContext & Ours (CaMN)  \\ 
\hline
FGD                  & 261.3   &256.7     &223.8     &176.2         &123.7             \\
Learning Rate                  &1e-3         &1e-4     &1e-4     &5e-4              &2e-4              \\
Epoch                &103         &87     &271     &129             & 117            
\end{tabular}
\end{table}





















































































































































































































































































































































































































































































 

\end{document}
