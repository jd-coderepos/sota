\vspace{-0.5em}
\section{Experiments}
\label{sec_exp}


\subsection{Experimental settings}
\textbf{Implementation details.}\quad
We use ResNet as the encoder $f$ followed by two projectors and two predictor sub-networks. During the \textit{first-stage} pre-training, we follow the exact experimental setup in BYOL. For the \textit{second-stage} in \textit{dual-stage} pre-training, the model is initialized with the pre-trained weight from the \textit{first-stage} and the first two stages of the ResNet backbone are frozen. We use a learning rate of $lr\times BatchSize/256$ with $BatchSize=1024$ and a base $lr$ selected from $\{0.3, 0.6, 0.9, 1.2\}$. The embedding dimension is set to $d=256$ as BYOL. As for image augmentations, we follow the settings in MoCo.v2 for all contrastive learning methods. We replace \textit{RandomResizedCrop} (RRC) with the proposed SaliencySwap (SS) and adopt all other augmentations in MoCo.v2~\cite{2020mocov2}, detailed in Appendix~\ref{sec_empirical_study}. We grid search cropping scale ratio of SaliencySwap $\lambda \in \{0.08, 0.2, 0.5, 0.8\}$ and set $\lambda=0.5$ by default. To balance the performance and computational cost, we adopt the stage $l=4$ for the alignment. For simplicity, we use the ground truth bounding box of fine-grained datasets because most saliency detectors are trained in a supervised manner. 
An ablation study of the performance using different detection algorithms is given in Appendix~\ref{app_ablation_detection}.
We use the same pre-training setup as in Appendix~\ref{sec_settings} for other unstated setups.

\textbf{Dataset.}\quad
We assess the performance of the representation pre-trained using \textit{dual-stage}, \textit{first-stage} only and \textit{second-stage only} on four small-scale and one large-scale fine-grained benchmarks: 1) CUB-200-2011~\cite{wah2011caltech} (CUB) contains 11,788 images from 200 wild bird species, 2) Stanford-Cars~\cite{krause20133d} (Cars) contains 16,185 images of 196 car subcategories, 3) FGVC-Aircraft~\cite{maji2013fine} (Aifcrafts) contains 10,000 images of 100 classes of aircrafts, 4) NA-birds~\cite{van2015building} (NAbirds) is a large dataset with 48,562 images for over 555 bird classes and 5) iNaturalist2018 (iNat2018)~\cite{cvpr2018inaturalist} is an unbalanced long tail dataset which contains 437,513 images of 8,142 taxa coming from 14 super-classes. We follow the standard dataset partition in the original works. For the \textit{first-stage} pre-training, we adopt two popular datasets: 1) ImageNet-1k (IN-1k) contains 1.28 million of training images. 2) MS-COCO (COCO) contains 118k images with more complex scenes of many objects.



\textbf{Hyperparameters for CVSA.}\quad
We search for the optimal hyperparameters for our proposed CVSA on the validation set of fine-grained datasets using ResNet-18 as the encoder. We set the batch size to $1024$ and follow other training settings of BYOL~\cite{nips2020byol}. The base learning rate is set to $0.3$ for the Cars dataset and $0.6$ for the other three datasets. The cropping scale ratio of SaliencySwap is $\lambda=0.5$ by default. As for the stage $l$ in CVSA, we analyze the performance and the computational cost of using $l\in \{3,4\}$ since the first two stages are frozen in the \textit{dual-stage}. Furthermore, we find that using $l=4$ achieves a balance between the performance and the computation cost.

\subsection{Comparison with State-of-the-art}
We choose two hand-crafted methods (Rel-Loc and Rot-Pred), three commonly used contrastive learning methods (SimCLR, MoCo.v2, and BYOL), and three extension methods (LooC*, DiLo, and InsLoc) for comparison. DiLo and InsLoc are reproduced using official code, with other methods reproduced using OpenMixup~\cite{li2022openmixup}. LooC* denotes its rotation version reproduced by us. Since our approach extends BYOL, we choose BYOL as the baseline. Notice that \gray{\bf{grey}} indicates baseline, \green{\bf{green}} denotes improvement over baseline while \red{\bf{red}} for degradation performance, \textbf{bold} denotes the best performance.

\begin{figure*}[t]
\vspace{-1em}
\begin{minipage}{0.49\linewidth}
\centering
    \begin{table}[H]
    \centering
    \setlength{\tabcolsep}{0.9mm}
    \resizebox{1\columnwidth}{!}{
    \begin{tabular}{lcccc}
    \toprule
    Methods            & CUB                   & NAbirds               & Aircrafts             & Cars  \\
    \hline
    \gray{Random}      & 58.51 \redbf{-6.34}   & 65.78 \redbf{-6.76}   & 70.58 \redbf{-2.02}   & 70.51 \redbf{-5.36} \\
    Rel-Loc            & 65.89 \gbf{+1.04}     & 72.60 \gbf{+0.06}     & 72.24 \redbf{-0.36}   & 75.27 \redbf{-0.60} \\
    Rot-Pred           & 66.67 \gbf{+1.82}     & 73.01 \gbf{+0.47}     & 72.67 \gbf{+0.07}     & 75.48 \redbf{-0.39} \\
    SimCLR             & 63.43 \redbf{-1.42}   & 72.05 \redbf{-0.49}   & 72.42 \redbf{-0.18}   & 75.12 \redbf{-0.75} \\
    MoCo.v2            & 63.21 \redbf{-1.64}   & 71.36 \redbf{-1.18}   & 71.93 \redbf{-0.67}   & 74.89 \redbf{-0.98} \\
    LooC*              & 66.42 \gbf{+1.57}     & 72.84 \gbf{+0.30}     & 72.49 \redbf{-0.11}   & 75.69 \redbf{-0.18} \\
    InsLoc             & 64.87 \gbf{+0.02}     & 72.80 \gbf{+0.26}     & 73.43 \gbf{+0.83}     & 76.61 \gbf{+0.64} \\
    BYOL               & 64.85 \rbf{+0.00}     & 72.54 \rbf{+0.00}     & 72.60 \rbf{+0.00}     & 75.87 \rbf{+0.00} \\
    BYOL+DiLo          & 66.16 \gbf{+1.31}     & 73.12 \gbf{+0.58}     & 73.52 \gbf{+0.92}     & 76.36 \gbf{+0.49} \\
    \textbf{BYOL+CVSA} & \bf{66.88} \gbf{+2.03} & \bf{73.75} \gbf{+1.21} & \bf{74.55} \gbf{+1.95} & \bf{77.45} \gbf{+1.58} \\
    \bottomrule
    \end{tabular}
    }
    \vspace{-0.75em}
    \caption{\textbf{Comparison of \textit{second-stage only} pre-training on fine-grained benchmarks.} Top-1 accuracy (\%) under fine-tuned evaluation is reported. Random denotes random initialized models in the \textit{second-stage}. BYOL is regarded as the baseline by default.}
    \label{tab:stage1}
    \vspace{-1em}
\end{table}
 \end{minipage}
\begin{minipage}{0.51\linewidth}
\centering
    \begin{table}[H]
    \centering
    \setlength{\tabcolsep}{0.9mm}
    \resizebox{1\columnwidth}{!}{
    \begin{tabular}{lccccc}
    \toprule
    Methods            & Stage 2    & CUB                   & NAbirds               & Aircrafts             & Cars  \\
    \hline
    Rel-Loc            & \checkmark & 67.33 \redbf{-1.22}   & 73.82 \redbf{-2.47}   & 81.20 \redbf{-0.53}   & 85.80 \redbf{-0.56} \\
    Rot-Pred           & \checkmark & 67.75 \redbf{-0.80}   & 74.26 \redbf{-2.03}   & 81.58 \redbf{-0.15}   & 85.74 \redbf{-0.62} \\
    SimCLR             & $\times$   & 68.30 \redbf{-0.30}   & 73.51 \redbf{-2.78}   & 81.18 \redbf{-0.55}   & 85.93 \redbf{-0.43} \\
    MoCo.v2            & $\times$   & 68.47 \redbf{-0.08}   & 73.73 \redbf{-2.56}   & 81.78 \gbf{+0.05}     & 86.08 \redbf{-0.28} \\
    MoCo.v2            & \checkmark & 67.60 \redbf{-1.05}   & 73.18 \redbf{-3.11}   & 81.04 \redbf{-0.69}   & 85.71 \redbf{-0.65} \\
    LooC*              & \checkmark & 68.71 \gbf{+0.16}     & 74.45 \redbf{-1.84}   & 81.75 \gbf{+0.02}     & 85.90 \redbf{-0.46} \\
    InsLoc             & \checkmark & 67.94 \redbf{-0.20}   & 76.36 \gbf{+0.07}     & 81.54 \redbf{-0.23}   & 86.38 \gbf{+0.02} \\
    BYOL               & $\times$   & 68.55 \rbf{+0.00}     & 76.29 \rbf{+0.00}     & 81.73 \rbf{+0.00}     & 86.36 \rbf{+0.00} \\
    BYOL               & \checkmark & 68.01 \redbf{-0.54}   & 75.82 \redbf{-0.47}   & 80.53 \redbf{-1.20}   & 85.49 \redbf{-0.87} \\
    BYOL+DiLo          & \checkmark & 68.70 \gbf{+0.15}     & 76.94 \gbf{+0.65}     & 82.04 \gbf{+0.31}     & 86.46 \gbf{+0.10} \\
    \textbf{BYOL+CVSA} & \checkmark & \bf{69.14} \gbf{+0.59} & \bf{77.57} \gbf{+1.28} & \bf{82.77} \gbf{+1.04} & \bf{87.13} \gbf{+0.77} \\
    \bottomrule
    \end{tabular}
    }
    \vspace{-0.75em}
    \caption{\textbf{Comparison of \textit{dual-stage} pre-training on fine-grained benchmarks.} Top-1 accuracy (\%) under fine-tuned evaluation is reported. The \textit{first-stage} is pre-trained on COCO, and \checkmark denotes performing the \textit{second-stage} pre-training on fine-grained datasets.}
    \label{tab:stage2_coco}
    \vspace{-1em}
\end{table}
 \end{minipage}
\vspace{-1.25em}
\end{figure*}

\begin{wraptable}{r}{0.55\textwidth}
\setlength{\tabcolsep}{0.9mm}
    \resizebox{0.55\columnwidth}{!}{
    \begin{tabular}{lccccc}
    \toprule
    Methods            & Stage 2    & CUB                   & NAbirds               & Aircrafts             & Cars  \\
    \hline
    \gray{Supervised}  & $\times$   & \gray{81.02}          & \gray{80.09}          & \gray{87.25}          & \gray{90.61} \\
    SimCLR             & $\times$   & 73.99 \redbf{-2.64}   & 76.30 \redbf{-2.59}   & 85.96 \redbf{-1.23}   & 88.16 \redbf{-1.43} \\
    MoCo.v2            & $\times$   & 73.19 \redbf{-3.44}   & 75.64 \redbf{-3.25}   & 85.49 \redbf{-1.70}   & 87.51 \redbf{-2.08} \\
    MoCo.v2            & \checkmark & 71.77 \redbf{-4.86}   & 73.96 \redbf{-4.93}   & 83.25 \redbf{-3.94}   & 86.88 \redbf{-2.71} \\
    InsLoc             & \checkmark & 75.83 \redbf{-0.80}   & 78.86 \redbf{-0.03}   & 86.60 \redbf{-0.59}   & 88.87 \redbf{-0.72} \\
    BYOL               & $\times$   & 76.63 \rbf{+0.00}     & 78.89 \rbf{+0.00}     & 87.19 \rbf{+0.00}     & 89.59 \rbf{+0.00} \\
    BYOL               & \checkmark & 72.46 \redbf{-4.17}   & 76.12 \redbf{-2.77}   & 84.58 \redbf{-3.61}   & 87.08 \redbf{-2.51} \\
    BYOL+DiLo          & \checkmark & 76.60 \redbf{-0.03}   & 79.04 \gbf{+0.15}     & 87.03 \redbf{-0.16}   & 89.26 \redbf{-0.33} \\
    \textbf{BYOL+CVSA} & \checkmark & \bf{77.10} \gbf{+0.47} & \bf{79.64} \gbf{+0.75} & \bf{87.27} \gbf{+0.12} & \bf{89.76} \gbf{+0.18} \\
    \bottomrule
    \end{tabular}
    }
    \vspace{-0.75em}
    \caption{\textbf{Comparison of \textit{dual-stage} pre-training on fine-grained benchmarks.} Top-1 accuracy (\%) under fine-tuned evaluation is reported. The \textit{first-stage} is pre-trained on IN-1k, and \checkmark denotes performing the \textit{second-stage} pre-training on corresponding fine-grained datasets. Supervised denotes the supervised pre-training on IN-1k.}
    \label{tab:stage2_IN}
     \vspace{-1.10em}
\end{wraptable}

\textbf{Small-scale scenarios.}\quad
We perform 800 epochs pre-training with ResNet-50 encoder using different pre-training stage settings on four small-scale fine-grained datasets and report the top-1 classification accuracy under the fine-tune protocol. 
As shown in Table~\ref{tab:stage1}, when performing the \textit{second-stage only} setting, namely pre-training with the training set of fine-grained datasets from scratch, our proposed CVSA outperforms BYOL by a large margin on all benchmarks. When applying the \textit{dual-stage} setting using COCO for the \textit{first-stage}, as shown in Table~\ref{tab:stage2_coco}, CVSA shows a consistent improvement on representations learned during the \textit{first-stage} while most comparing methods yield worse performance than the \textit{first-stage} pre-trained BYOL baseline. However, when using ImageNet for the \textit{first-stage} pre-training, as shown in Table~\ref{tab:stage2_IN}, the improvement of CVSA in the \textit{dual-stage} setting is not as significant on Aircraft and Cars. We hypothesize that this is due to the iconic nature of these two datasets. We find that most images of these two datasets are of similar and straightforward backgrounds, restricting the background variation imposed by our approach. Besides, comparison of Table~\ref{tab:stage1} with Table~\ref{tab:stage2_coco} and~\ref{tab:stage2_IN} suggests that the performance gain is affected by the size of dataset used. This is consistent with our formulation in Appendix~\ref{sec_formulation} that discriminative feature extraction relies on the size of the pre-training dataset, and localization ability alone is not enough for fine-grained recognition. In a word, CVSA and \textit{dual-stage} pre-training pipeline improve learned representation for the small-scale fine-grained classification.

\begin{wraptable}{r}{0.60\textwidth}
    \vspace{-1.1em}
    \resizebox{0.60\textwidth}{!}{
    \setlength\tabcolsep{3pt} \begin{tabular}{ccccccc}
        \toprule
        Stage 1  & Stage 2  & Sup.        & MoCo.v2           & BYOL          & BYOL+DiLo & \bf{BYOL+CVSA}            \\ \hline
        $\times$ & iNat2018 & \gray{5.7}  & 41.8\redbf{-2.8} & 44.6\rbf{+0.0} & 44.3\redbf{-0.3} & \bf{45.4}\gbf{+0.8} \\
        IN-1k    & iNat2018 & \gray{46.5} & 45.0\redbf{-1.9} & 46.9\rbf{+0.0} & 47.0\gbf{+0.1}   & \bf{47.5}\gbf{+0.6} \\
        \bottomrule
    \end{tabular}}
    \vspace{-0.75em}
    \caption{\textbf{Comparison of \textit{dual-stage} pre-training on iNat2018.} Top-1 accuracy (\%) under linear evaluation is reported. \textit{Sup.} denotes the supervised pre-training on iNat2018 in the \textit{second-stage}.}
    \label{tab:inat}
     \vspace{-1.5em}
\end{wraptable}

\textbf{Large-scale scenarios.}\quad
Next, we evaluate CVSA on large-scale benchmark iNat2018 with the \textit{dual-stage} pre-training. Following MoCo~\cite{cvpr2020moco}, we adopt the linear evaluation protocol as mentioned in Appendix~\ref{sec_settings} training 100 epochs with the basic learning rate $lr=0.025$ and batch size of $256$. In Table~\ref{tab:inat}, CVSA outperforms existing methods in both settings indicating its effectiveness in the large-scale datasets, especially improving the baseline by 0.9\% with \textit{second-stage-only}. Compared with the results in Table~\ref{tab:stage2_IN}, the performance gain is more significant than \textit{second-stage} pre-training on small-scale datasets like Aircraft and Cars, which suggests that CVSA benefits from a larger data size. Meanwhile, using \textit{dual-stage} pre-training with IN-1k for the \textit{first-stage} helps contrastive methods to learn better representations than the \textit{second-stage only} setting.


\subsection{Ablation Study}
\label{subsec:ablation}
We perform 400 epochs pre-training with ResNet-18 on CUB for the first three ablation studies. As for the fourth ablation for the \textit{dual-stage} pre-training, we perform 200 epoch \textit{first-stage} pre-training on large datasets and 400 epochs \textit{second-stage} pre-training on target datasets. The top-1 accuracy under the fine-tune evaluation is reported. Appendix~\ref{app_ablation_detection} provides more results.

\begin{figure*}[t]
\begin{minipage}{0.55\linewidth}
\centering
    \begin{figure}[H]
    \vspace{-1em}
        \centering
        \includegraphics[width=\linewidth]{figs/anblation_saliencyswap_align.pdf}
        \vspace{-2em}
        \caption{\textbf{Ablation of hyperparameters}. Left: the effect of different scaling factors of SS during \textit{second-stage}. Right: the effect of freezing different ResNet stages during \textit{second-stage}. The dotted grey line indicates the performance of BYOL pre-trained on COCO during \textit{first-stage}.
        }
        \label{fig:ss_scale_ablation}
    \end{figure}
\end{minipage}
\begin{minipage}{0.44\linewidth}
\centering
    \begin{table}[H]
    \centering
\resizebox{1.0\linewidth}{!}{
\setlength\tabcolsep{3pt} \begin{tabular}{lccccc}
    \toprule
    Methods        & Stage 1  & Stage 2      & CUB       & Aircrafts & Cars      \\ \hline
    BYOL           & IN-1k    & $\times$     & \bf{76.6} & 87.2      & 89.6      \\
    \bf{BYOL+CVSA} & IN-1k    & $\times$     & 75.9      & 86.8      & 89.2      \\
    \bf{BYOL+CVSA} & IN-1k    & $\checkmark$ & 76.5      & \bf{87.3} & \bf{89.7} \\ \hline
    BYOL           & iNat2018 & $\times$     & \bf{77.2} & 80.4      & 85.8      \\
    \bf{BYOL+CVSA} & iNat2018 & $\checkmark$ & 77.0      & \bf{81.2} & \bf{86.5} \\ \hline
    BYOL           & COCO     & $\times$     & 68.6      & 81.7      & 86.4      \\
    \bf{BYOL+CVSA} & COCO     & $\checkmark$ & \bf{68.9} & \bf{82.3} & \bf{86.7} \\
    \bottomrule
    \end{tabular}
    }
    \vspace{-0.75em}
    \caption{\textbf{Ablation of \textit{dual-stage} pre-training.} The \textit{first-stage} is pre-trained 200 epochs on IN-1k and iNat2018 and 400 epochs on COCO. The \textit{second-stage} is pre-trained on downstream datasets (denoted by $\checkmark$) or not (denoted by $\times$).
    }
    \label{tab:ablation_dual}
    \vspace{-2em}
\end{table}
 \end{minipage}
\vspace{-1.5em}
\end{figure*}

\textbf{Module effectiveness ablation.}\quad
We demonstrate the effectiveness of our CVSA by adding modules one by one onto the baseline. We compare the performance of MoCo.v2 and BYOL using \textbf{SS} against \textit{RandomResizedCrop} (RRC) while keeping all other augmentations unchanged. As shown in Figure~\ref{fig:ss_scale_ablation} left, \textbf{SS} outperforms RRC (using the default scaling factor of $0.08$) both for MoCo.v2 and BYOL. We observe a further performance improvement from adding the saliency alignment loss (\textbf{Align}) onto BYOL using \textbf{SS}. Now, we have shown that both \textbf{SS} and \textbf{Align} contribute to higher performance.

\textbf{Hyperparameter ablation.}\quad
We then analyze the performance of \textbf{SS} using different crop scaling factors, specifically $\lambda \in \{0.08, 0.2, 0.3, 0.4, 0.5, 0.8\}$ in Figure~\ref{fig:ss_scale_ablation} left. From Figure~\ref{fig:analysis} left, the performance of BYOL fluctuates drastically under the different scaling factors of RRC, with the best result achieved with a scaling factor being $0.08$. However, BYOL yields similar performance under different choices of $\lambda$ of \textbf{SS}. We argue that \textbf{SS}, together with \textbf{Align}, helps the model to localize on the foreground object, and thus local texture is no longer the only clue for contrastive pre-training to learn representation. Then, we study the effect when freezing various stages of ResNet-18 in the \textit{second-stage}. We initialize the model of all methods in the \textit{second-stage} with the weights of BYOL baseline pre-trained on COCO in the \textit{first-stage}. In Figure~\ref{fig:ss_scale_ablation} right, the horizontal axis indicates freezing up to different stages of ResNet-18. The best performance is reached when freezing up to the \textit{second-stage} of ResNet. The early stages of ResNet mostly extract low-level texture information, while the later stages are for higher-level features such as discrimination and localization. Intuitively, we wish to enhance the localization ability without jeopardizing the texture extraction ability acquired during the \textit{first-stage}. This explains the reason for freezing the first two stages during the \textit{second-stage}, which is common in object detection~\cite{NIPS2015_14bfa6bb}.

\begin{wraptable}{r}{0.50\textwidth}
    \vspace{-1.0em}
    \centering
    \setlength\tabcolsep{3pt} \resizebox{0.50\textwidth}{!}{
        \begin{tabular}{lccc}
        \toprule
        Background  & BYOL+DiLo             & BYOL+SS        & BYOL+SS+Align \\
        \hline
        CUB         & 64.14 \rbf{+0.00}     & \bf{64.35} \rbf{+0.00} & 65.02 \rbf{+0.00}  \\
        COCO        & 60.07 \redbf{-4.07}   & 60.42 \redbf{-3.93}   & 61.23 \redbf{-3.79}  \\
        NAbirds     & 62.51 \redbf{-1.81}   & 63.06 \redbf{-1.29}   & 64.80 \redbf{-0.22}  \\
        CUB+COCO    & 61.26 \redbf{-2.88}   & 61.49 \redbf{-2.86}   & 62.01 \redbf{-3.01} \\
        CUB+NAbirds & \bf{64.28}\gbf{+0.14} & 64.23 \redbf{-0.12}   & \bf{65.50}\gbf{+0.48} \\
        \bottomrule
    \end{tabular}}
    \vspace{-0.75em}
    \label{tab: table4}
    \caption{\textbf{Evaluation of background datasets extension for the \textit{second-stage} pre-training.} We study the effect of fusing different background datasets on CUB.
    }
     \vspace{-1.0em}
\end{wraptable}

\textbf{Background dataset ablation.}\quad
Moreover, we compare different foreground and background fusion methods (\textbf{SS} and DiLo) based on CUB dataset using \textit{second-stage only} settings. As shown in Table~\ref{tab: table4}, replacing the background with other datasets usually results in performance degradation, while fusing with complex backgrounds might improve the performance. We hypothesize that exploring task-specific backgrounds is more critical in fine-grained scenarios. Meanwhile, it indicates that the proposed CVSA (\textbf{SS}+\textbf{Align}) can well explore the background of CUB.

\textbf{First-stage pre-training ablation.}\quad
Additionally, we verify the necessity of the \textit{first-stage} learning discrimination on iconic datasets in the \textit{dual-stage} pre-training scheme. We compare naive BYOL and CVSA for the \textit{first-stage} pre-training on IN-1k, iNat2018, and COCO. As shown in Table~\ref{tab:ablation_dual}, using IN-1k (iconic) for the \textit{first-stage} yields better performance that scenic datasets, indicating the necessity of the \textit{first-stage} on iconic datasets. Since CVSA is design for the \textit{second-stage} on target datasets, we find that using CVSA on \textit{second-stage} outperforms naive BYOL while producing degraded performance on the \textit{first-stage}.

\begin{wrapfigure}{r}{0.60\textwidth}
    \centering
    \vspace{-1.0em}
    \includegraphics[width=1.0\linewidth]{figs/fig_vis_stage.pdf}
    \vspace{-2.0em}
    \caption{Grad-CAM visualization of BYOL and BYOL+CVSA on CUB200.
    }
    \label{fig:vis_stage}
    \vspace{-1.0em}
\end{wrapfigure}

\textbf{Visualization of localization abilities.}\quad
Lastly, we compare the localization abilities by Grad-CAM~\cite{iccv2017gradcam} visualization of SS (BYOL+CVSA) and RRC (BYOL). Figure~\ref{fig:vis_stage} (a) and (b) shows that SS enables the model to better localize the fine-grained target than RRC. Comparing to Figure~\ref{fig:vis_stage} (a)(b), Figure~\ref{fig:vis_stage} (c) shows that \textit{dual-stage} training further improves localization than the \textit{first-} or \textit{second-stage only}.
