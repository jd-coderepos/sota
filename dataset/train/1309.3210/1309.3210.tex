

\documentclass[b5paper, english, oneside]{memoir}

\usepackage{asymptotic-macros}

\ifarxiv
\else
\bibliography{asymptotic}
\usepackage[final]{pdfpages}
\fi

\title{\texorpdfstring{}{O}-notation in Algorithm Analysis \\ (updated)}
\author{Kalle Rutanen}

\ifindex
\makeindex
\fi

\begin{document}

\frontmatter

\ifthesis
\includepdf[pages=-]{thesis-cover.pdf}
\else
\maketitle
\fi

\newpage
\thispagestyle{empty}
\vspace*{.35\textheight}
\begin{center}To my dear cats Kati (1996--2015) and Minari (2001--2016), \newline with whom I shared my life.
\end{center}

\chapter{Abstract}
\setcounter{page}{1}

In computer science, algorithm analysis is concerned with the correctness and complexity of algorithms. \emph{Correctness analysis} is about verifying that an algorithm solves the problem it is claimed to solve, and \emph{complexity analysis} is about counting the amount of resources an algorithm takes to run in an abstract machine. This \manuscript{} is concerned with complexity analysis. 

The amount of resources consumed by an algorithm is captured by a cost function which maps each input of the algorithm to a non-negative real number. The cost functions are ordered by an order-relation , which makes it possible to \emph{simplify}, and to \emph{compare} cost functions. A cost function can be simplified by replacing it with an equivalent, simpler cost function. The smaller the cost function is according to , the better the algorithm --- at least for the measured resource.  

The -notation  is the set of functions  which satisfy . It contains the same simplification and comparison tools in a slightly different, but equivalent form. 

How should the -notation be defined? By the above, we may ask an equivalent question: how should the order relation  be defined? \We{} provide \nprim{} intuitive properties for , and then show that there is exactly one definition of  which satisfies these properties: linear dominance. 

\We{} show that Master theorems hold under linear dominance, define the \-/mappings as a general tool for manipulating the \-/notation, and abstract the existing definitions of the -notation under local linear dominance.

\chapter{Preface}

The research presented in this \manuscript{} was carried out at the Department of Mathematics at Tampere University of Technology during years 2013--2016. The \manuscript{} was financially supported by the Finnish National Doctoral Programme in Mathematics and its Applications (2013--2014), and by the science fund of the city of Tampere (Summer 2015).

I am deeply thankful to my supervisors Sirkka-Liisa Eriksson, Karen Egiazarian, and Germán-Gómez Herrero for the support during these years, and the years before them. 

Special thanks go to office assistant Riitta Lahti, financial administrative assistant Tiina Sävilahti, and laboratory engineer Kari Suomela, at the Department of Mathematics. When it came to practical matters, they made my life easy, all the while uplifting the atmosphere.

I want to thank Raphael Reitzig for providing me with extensive feedback on an Arxiv-version of the \manuscript{}, related to the content and typographical concerns. Concerning typography, the idea to draw inside the -symbols is his; this resulted in , , , , , , and , which are both suggestive and can be drawn inline without extending line-height.\footnote{I will take the blame for the shapes.} I was led to prove the limit theorems after he asked me whether ratio-limits could still be used to reason about the -notation. He was also the first external reader to provide me with positive feedback.

Jarkko Kari suggested to study the independence of the primitive properties, and pointed out how some of those results already followed from the study of candidate definitions.

David Wilding created the original \Cref{OrderPreservingDiagram}, which I have adapted here with his permission. In the introduction, I have used \emph{svg-cards} playing cards designed by David Bellot.

I want to thank Heikki Huttunen and Pekka Ruusuvuori for introducing me to academic work as an undergraduate at the Department of Signal Processing. I am grateful to my friends and coworkers at the Department of Signal Processing, and at the Department of Mathematics for all those excellent detours from research. Namely, Tapio Manninen, Francesco Cricri, Pasi Hämäläinen, Henri Riihimäki, Petteri Laakkonen, Marko Järvenpää, Juho Lauri, Vesa Vuojamo, Elina Viro, and Markku Åkerblom.

Lastly, I want to thank my family, friends, and Alina for their invaluable support.

\bigskip
\begin{flushright}
Tampere, October 2016 \\
\vspace{1.5cm}
Kalle Rutanen
\end{flushright}

\newpage 
\tableofcontents
\newpage 

\mainmatter

\chapter{Introduction to the \manuscript{}}
\label{Introduction}

Suppose we want to sort playing cards into increasing order. The kind of cards does not matter, only that for any two cards  and , either  comes before , denoted by , or  comes before , denoted by  One way to sort the deck is by the following \define{insertion sort}, which is visualized in \fref{InsertionSortExample}.

\begin{enumbox}
\item\label{InsertionSort-Step2} If there are no cards left in the deck, we are done.
\item\label{InsertionSort-Step3} Otherwise, we pick the top-most card  from the deck.
\item\label{InsertionSort-Step4} Starting from the right end of the cards on the table, we compare  to each card  on the table, until  for the first time.
\item\label{InsertionSort-Step5} If there is no such card , we place  on the table as the right-most card, and start again from step \ref{InsertionSort-Step2}.
\item\label{InsertionSort-Step6} Otherwise, we place  on the table immediately left of , and start again from step \ref{InsertionSort-Step2}.
\end{enumbox}

\begin{figure}
\centering
\subfloat[]{\includegraphics[scale=1.4]{insertion-sort-0.pdf}} \hfill
\subfloat[]{\includegraphics[scale=1.4]{insertion-sort-1.pdf}}

\subfloat[]{\includegraphics[scale=1.4]{insertion-sort-2.pdf}} \hfill
\subfloat[]{\includegraphics[scale=1.4]{insertion-sort-3.pdf}}

\subfloat[]{\includegraphics[scale=1.4]{insertion-sort-4.pdf}} \hfill
\subfloat[]{\includegraphics[scale=1.4]{insertion-sort-5.pdf}}

\caption{Sorting 5 playing cards with insertion sort. The cards on the table are visualized on the left, while the cards in the deck are visualized on the right. During sorting, 7 comparisons are made between cards.}
\label{InsertionSortExample}
\end{figure}

How many comparisons between cards do we have to perform to sort a given deck with  cards? In the best case, the deck is already sorted in increasing order, requiring us to perform only  comparisons. In the worst case, the deck is already sorted in decreasing order, requiring us to perform  comparisons. Therefore, sorting a deck with  cards using insertion sort always requires something between  and  comparisons.

Another way to sort the deck is by the following \define{merge sort}.

\begin{enumbox}
\item We place all the cards in a row on the table. Every card  is considered to form a sequence  consisting of a single card.
\item \label{MergeSortStartOver} If there is at most one card-sequence on the table, we are done.
\item For each two successive card-sequences,  and , we combine  and  into a single sequence  by repeatedly picking the smallest of the cards at the left end of  and the left end of  (e.g.,  and  becomes ). If one of the sequences runs out before the other, we place the remaining sequence at the right end of  without performing additional comparisons.
\item We start again from step \ref{MergeSortStartOver}.
\end{enumbox}

\begin{figure}
\centering
\hfill\subfloat[]{\includegraphics[scale=1.4]{merge-sort-0.pdf}}\hfill\null

\hfill\subfloat[]{\includegraphics[scale=1.4]{merge-sort-1.pdf}}\hfill\null

\hfill\subfloat[]{\includegraphics[scale=1.4]{merge-sort-2.pdf}}\hfill\null

\hfill\subfloat[]{\includegraphics[scale=1.4]{merge-sort-3.pdf}}\hfill\null
\caption{Sorting 8 playing cards with merge sort. During sorting, 17 comparisons are made between cards; the maximum possible for 8 cards. The merging procedure is repeated  times.}
\label{MergeSortExample}
\end{figure}

How many comparisons between cards do we now have to perform to sort a given deck with  cards? Suppose  is a power of two, so that  for some integer . In the best case, the cards on the table are already sorted, and we need to perform  comparisons to combine the sequences into a single deck. In the worst case, we need to perform  comparisons. A deck which attains this bound for  is given in \fref{MergeSortExample}. \tref{PlayingCardComparisons} tabulates the number of comparisons required, in the worst case and in the best case, as a function of the number  of cards in the deck for both ways of sorting. It can be seen that merge sort scales better than insertion sort in the worst case. Note however, that there are decks --- especially those which are sorted or almost sorted --- for which insertion sort performs less comparisons than merge sort.

\begin{table}
\begin{tabular}{|l|r|r|r|r|}
\hline 
Sorting procedure /  &  &  &  & \\
\hline 
Insertion sort --- worst case & 2016 & 32640 & 523776 & 8386560 \\
Merge sort --- worst case & 321 & 1793 & 9217 & 45057 \\
Insertion sort --- best case & 63 & 255 & 1023 & 4095 \\
Merge sort --- best case & 192 & 1024 & 5120 & 24576 \\
\hline 
\end{tabular}
\centering
\caption{Number of comparisons needed to sort  cards by using either insertion sort, or merge sort. Merge sort scales better than insertion sort in the worst case.}
\label{PlayingCardComparisons}
\end{table}

\section{Algorithms and their analysis}

\begin{figure}
\centering
\includegraphics{blackbox.pdf}
\caption{A black-box view of an algorithm. The algorithm, visualized here as a gray box, transforms an input  from an input-set  to an output  from an output-set . The transform is captured by a mapping . The abstract machine executing the algorithm consumes resources at each step of the algorithm. The cost function, for given input and resource, is captured by a mapping .}
\label{AlgorithmBlackbox}
\end{figure}

Such step-wise procedures are called algorithms. A \emph{run} of an algorithm is a finite sequence of \emph{atomic operations} --- in the above, picking a card from the deck, comparing two cards, and placing a card on the table. The atomic operations are performed one by one by an \emph{abstract machine} --- in the above the person sorting the cards. The abstract machine, running the algorithm, transforms a given \emph{input} --- such as a deck of cards --- into \emph{output} --- such as the sorted input deck. This correspondence is captured by a function , as visualized in \fref{AlgorithmBlackbox}. The kind of algorithms that can be written depends on which atomic operations the abstract machine supports.

To each atomic operation, we associate a \emph{cost} in resources --- in the above one unit of resource to the task of comparing two cards. We then \emph{analyze} the algorithm for the amount of resources that it uses. The result of the analysis is a \emph{cost function}  which associates each possible input to the resource-cost. However, as for insertion sort and merge sort, the function  can be too hard to study directly --- or to get an intuition to. 

To make the analysis tractable, the input is grouped by some simpler property --- in the above by the number of cards in the deck. The value of the property shared by a group is called the \emph{label} of the group. Since a group may contain more than one input, we must come up with a way to summarize the costs inside each group. 

Such summaries include \emph{best case analysis} and \emph{worst case analysis} --- in the above providing us with the lower and upper bounds, respectively, for the number of comparisons required to sort a deck with a given number of cards. To analyze worst-case behavior under a given grouping, we pick from each group a \emph{representative} input which triggers the worst resource\-/cost. Conceptually, we then analyze a \emph{surrogate algorithm}, which takes a group-label as an input, maps it to the corresponding group-representative, and feeds the representative as an input to the actual algorithm.

Another common summary is \emph{average case analysis}, where we assume a random distribution for the input, and then analyse the mean of the resulting random cost function.

Grouping is often required to make analysis tractable. However, too rough a grouping loses detail without aiding intuition. Sometimes grouping is not necessary at all --- as when the input-set is .

\section{Simplified analysis by \texorpdfstring{}{O}-notation}

Above we analyzed two algorithms for sorting playing cards, and noticed that merge sort is superior to insertion sort, at least when the measured resource is the number of performed comparisons in the worst case, and the deck contains at least 4 cards. This difference is fundamental, in the sense that the ratio by which merge sort beats insertion sort --- in the worst case --- keeps growing. For example, while for  the ratio is , for  the ratio is already . Such \emph{algorithmic efficiency} is often much more important than the speed at which a given abstract machine executes the atomic operations.

Our analyses above were based on careful counting of the comparisons, with an aim of capturing the worst case and best case bounds exactly. Such analysis is only possible with an algorithm that is sufficiently simple. Numerical analysis is a field whose algorithms (e.g., matrix QR-decomposition) have traditionally been analyzed in this way.

With a complex algorithm, a closed form expression for the cost function may not exist, or otherwise be too complex for a human to make sense of --- not to mention the ingenuity needed to solve the combinatorial problems that occur when deriving those expressions. 

The way out of these problems is to declare that we are not interested in small differences, and to simplify them out as they occur. For example, the relative error of approximating the worst-case resource consumption  of insertion sort by  is only  for , and decreases to zero as  grows. 

In addition, we declare that we are only interested in algorithmic efficiency, and so are ready to ignore the speed of the abstract machine up to a constant. It does not matter whether some other machine is 2 times faster to perform the algorithm. What matters is how the algorithm scales --- does it scale like  or like ? 

We then say that insertion sort takes about  comparisons, because this is simple, and does not fundamentally differ from the exact count. However, here we must be careful. Since we aim to simplify expressions whenever possible, we need a guarantee that computing with the simplified expressions provides the same answer as first computing the exact cost function, and then simplifying it. We need \emph{simplification rules} for algorithmic cost functions.

The simplification rules are formalized by a tool called \emph{-notation}. Intuitively, whenever we want to simplify, we surround the expression with an , as in , and then use whatever simplification rules the -notation provides. With proper assumptions on the domain of , we can then show that
\begin{eqs}
\oh{}{(n - 1)n / 2} & = \oh{}{(n - 1)n} \\
{} & = \oh{}{n^2 - n} \\
{} & = \oh{}{n^2}.
\end{eqs}
The notation , where  is a real-valued function, stands for the set of real-valued functions which scale at least as well as . Having the same -set divides the functions into equivalence classes --- in the above, we then say that the functions  and  are \emph{equivalent}.

However, simplification is not the whole story. In addition, the -notation must induce an order, so that the cost functions can be \emph{compared}, and the order must respect the structure that is present in algorithms: looping, branching, and composition.

How do we define such an -notation, and which rules do we need? 

\section{Definitions of \texorpdfstring{}{O}-notation}

Here are some of the proposed definitions\footnote{ is the set of positive real numbers,  is the set of subsets of ,  is the restriction of a function  to a set ,  for functions  and  means , and  is the set of functions from  to . See \sref{Notation} for more notation.} for an -notation in algorithm analysis.

\newcommand{\defineasymptotic}{
\begin{definition}[Asymptotic linear dominance]
\defineexp{Asymptotic linear dominance}{linear dominance!asymptotic}  is defined by  if and only if

for all , and all , where .
\end{definition}
}

\defineasymptotic

\begin{note}[]
This definition is given in \cite{IntroAlgo} in the univariate form on , and generalized to  in a later exercise \cite[page 50]{IntroAlgo}. 
\end{note}

\newcommand{\definecoasymptotic}{
\begin{definition}[Coasymptotic linear dominance]
\defineexp{Coasymptotic linear dominance}{linear dominance!coasymptotic}  is defined by  if and only if

for all , and all , where . 
\end{definition}
}

\definecoasymptotic

\begin{note}[]
This definition is given in \cite{IntroAlgo2009} in the univariate form on , and generalized to  in a later exercise \cite[page 53]{IntroAlgo2009}.
\end{note}

\newcommand{\definecofinite}{
\begin{definition}[Cofinite linear dominance]
\defineexp{Cofinite linear dominance}{linear dominance!cofinite}  is defined by  if and only if\footnote{.}

for all , and all sets .
\end{definition}
}

\definecofinite

\begin{note}[Symbol shapes]
The drawing inside a given version of  above mimics the shape of its restriction sets in .
\end{note}

\newcommand{\definelinear}{
\begin{definition}[Full linear dominance]
\defineexp{Full linear dominance}{linear dominance!full}  is defined by  if and only if

for all , and all , where  is the class of all sets.
\end{definition}
}

\definelinear

\begin{note}[Symbol shape]
The drawing inside  mimics a line.
\end{note}

\begin{note}[Linear dominance]
We will often shorten full linear dominance to linear dominance.
\end{note}

\newcommand{\defineaffine}{
\begin{definition}[Affine dominance]
\emph{Affine dominance}  is defined by  if and only if

for all , and all , where  is the class of all sets.
\end{definition}
}

\defineaffine

\begin{note}[Symbol shape]
The drawing inside  mimics the plus operator.
\end{note}

\begin{definition}[Related notations]
Given an -notation, we define the related notations  as follows:
\begin{eqs}
g \in \omegah{X}{f} & \iff f \in \ohx{g}, \\
g \in \smallomegax{f} & \iff g \not\in \ohx{f} \land f \in \ohx{g}, \\
g \in \smallohx{f} & \iff g \in \ohx{f} \land f \not\in \ohx{g}, \\
g \in \thetahx{f} & \iff g \in \ohx{f} \land f \in \ohx{g}.
\end{eqs}
\end{definition}

\begin{note}[Equivalence from equality of -sets]
The expression  is equivalent to ; equality between -sets can be used to establish the equivalence of functions. This is also true for  and .
\end{note}

\begin{note}[Study of the -notation suffices]
It suffices to study the \-/notation, since the related notations are completely determined by the \-/notation. 
\end{note}

We will show linear dominance  to be the correct definition of -notation for algorithm analysis.

\section{Example analyses}

In the following, we provide some example analyses of algorithms, and demonstrate why most of the definitions of -notation in the previous section --- except linear dominance --- are not suitable for algorithm analysis.

\begin{note}[Function from an expression]
We will often denote a function in the parameter of  with an expression, as in , where we actually mean , with  such that . 

When the expression contains multiple symbols, as in , we interpret the symbols to be assigned to the input-tuple in alphabetical order, as in . This is to acknowledge that  and  are different functions.
\end{note}

\begin{note}[A cost model for examples]
In the following examples, each addition operation  costs one unit, while other operations cost nothing.
\end{note}

\begin{algorithm}
\caption{An algorithm which takes as input , and outputs , if , and  otherwise.}
\label{alg:ConstantComplexity}
\begin{algorithmic}[1]
\Procedure {computeOnPlane}{}
\State 
\If {}
  \For {}
    \State 
  \EndFor
\EndIf
\State \Return 
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{An algorithm which takes as input , and returns .}
\label{alg:BasicAnalysis}
\begin{algorithmic}[1]
\Procedure {mapNaturalsToPlane}{}
  \State \Return \Call{computeOnPlane}{}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{example}[Algorithm on ]
\label{MultivariateCounterExampleInNSubAlgorithm}
Consider \aref{alg:ConstantComplexity}, which takes as input . The cost function for this algorithm is  such that
\begin{eqs}
f(m, n) = 
\begin{cases}
n, & m = 0, \\
0, & m > 0, 
\end{cases}
\end{eqs}
We can show that .
\end{example}

\begin{example}[Calling algorithm on ]
\label{MultivariateCounterExampleInN}
Consider \aref{alg:BasicAnalysis}, which takes as input , and outputs the result of \aref{alg:ConstantComplexity} at . The cost function for this algorithm is  such that
\begin{eqs}
g(n) = n.
\end{eqs}
We can show that . Therefore, by calling an algorithm which is , we get an algorithm which is . This shows that  is not suitable for a definition of -notation in algorithm analysis.
\end{example}

\begin{algorithm}
\caption{An algorithm which takes as input , and returns .}
\label{alg:ConstantComplexityZ}
\begin{algorithmic}[1]
\Procedure {computeOnIntegers}{}
\State 
\If {}
  \For {}
    \State 
  \EndFor
\EndIf
\State \Return 
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{An algorithm which takes as input , and returns .}
\label{alg:BasicAnalysisZ}
\begin{algorithmic}[1]
\Procedure {mapNaturalsToIntegers}{}
  \State \Return \Call{computeOnIntegers}{}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{example}[Algorithm on ]
\label{UnivariateCounterExampleInZSubAlgorithm}
Consider \aref{alg:ConstantComplexityZ}, which takes as input  and returns . The cost function of this algorithm is  such that
\begin{eqs}
f(z) = \max(-z, 0). 
\end{eqs}
We can show that .
\end{example}

\begin{example}[Calling algorithm on ]
\label{UnivariateCounterExampleInZ}
Consider \aref{alg:BasicAnalysisZ}, which takes as input , and outputs the result of \aref{alg:ConstantComplexityZ} at . The cost function of this algorithm is  such that
\begin{eqs}
g(n) = n.
\end{eqs}
We can show that . Therefore, by calling an algorithm which is , we get an algorithm which is . This shows that  is not suitable for a definition of -notation in algorithm analysis.
\end{example}



\begin{algorithm}
\caption{An algorithm which takes as input  and evaluates a sub-algorithm  times at .}
\label{alg:ClassRoom}
\begin{algorithmic}[1]
\Procedure {G}{}
\For{}
  \State \Call{F}{}
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{example}[Course-exercise]
\label{ZeroCounterExample}
Consider \aref{alg:ClassRoom}, which takes as input  and calls a sub-algorithm  at  repeatedly  times. Denote the cost function of  by , and the cost function of  by . Suppose . What is ? 

The given information is not sufficient to solve this problem. In particular, let  be such that , and
\begin{eqs}
f_2(n) =
\begin{cases}
0, & n = 0, \\
1, & n > 0.	
\end{cases}
\end{eqs}
Then , but
\begin{eqs}
g_1 & \in \fthetah{\TN}{n}, \\
g_2 & \in \fthetah{\TN}{0}, \\
\fthetah{\TN}{n} & \cap \fthetah{\TN}{0} = \emptyset.
\end{eqs}
This shows that  is not suitable for a definition of -notation in algorithm analysis.

There is a fundamental difference between consuming resources () and not consuming resources (), which  ignores here. 

In contrast,  provided , as expected. 
\end{example}

\begin{algorithm}
\caption{An algorithm which takes as input  and returns .}
\label{alg:Real}
\begin{algorithmic}[1]
\Procedure {doublesToOne}{}
\State 
\State 
\While {}
  \State 
  \State 
\EndWhile
\State \Return 
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{An algorithm which takes as input , and returns .}
\label{alg:UnivariateBroken}
\begin{algorithmic}[1]
\Procedure {identity}{}
\State \Return \Call{doublesToOne}{}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{example}[Algorithm on ]
\label{UnivariateCounterExampleInRSubAlgorithm}
Consider \aref{alg:Real}, which takes as input  and outputs ; this is the number of times that  must be doubled to grow . The cost function of this algorithm is  such that
\begin{eqs}
f(x) = \max(\ceilb{-\lg{x}}, 0).
\end{eqs}
We can show that .
\end{example}

\begin{example}[Calling an algorithm on ]
\label{UnivariateCounterExampleInR}
Consider \aref{alg:UnivariateBroken}, which takes in , and outputs the result of \aref{alg:Real} at . The cost function of this algorithm is  such that
\begin{eqs}
g(n) = n.
\end{eqs}
We can show that . Therefore, by calling an algorithm which is , we get an algorithm which is . This shows that  is not suitable for a definition of -notation in algorithm analysis.
\end{example}

\begin{algorithm}
\caption{An algorithm which takes as input , and outputs .}
\label{alg:AlmostIdentity}
\begin{algorithmic}[1]
\Procedure {almostIdentity}{}
\State 
\If {}
\State \Return 
\EndIf
\For {}
\State 
\EndFor
\State \Return 
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{example}[Almost identity on ]
Consider \aref{alg:AlmostIdentity}, which takes as input , and outputs . The cost function of this algorithm is  such that
\begin{eqs}
f(n) = 
\begin{cases}
n, & n \neq 4, \\
0, & n = 4.	
\end{cases}
\end{eqs}
We can show that  and .
\end{example}

\begin{algorithm}
\caption{An algorithm which takes as input  and outputs .}
\label{alg:SecondComponent}
\begin{algorithmic}[1]
\Procedure {secondComponent}{}
\State \Return \Call{almostIdentity}{}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{example}[Calling an almost identity]
\label{ExtensibilityCounterExample}
Consider \aref{alg:SecondComponent}, which takes as input , and outputs . The cost function of this algorithm is  such that
\begin{eqs}
g(m, n) = 
\begin{cases}
n, & n \neq 4, \\
0, & n = 4.	
\end{cases}
\end{eqs}
We can show that . Therefore, by calling an algorithm which is , we get an algorithm which is . This shows that  is not suitable for a definition of -notation in algorithm analysis. 
\end{example}

\begin{note}[Blow-up and zeros]
There are two kinds of problems with those definitions of -notations --- such as , ,  --- which ignore points from the input-set.

If the definition ignores an infinite set --- as in , , and  --- then it is possible to construct a cost function to blow up in that ignored set, while not being reflected in the -notation. This was demonstrated in \eref{MultivariateCounterExampleInN} for , in \eref{UnivariateCounterExampleInZ} for , and in \eref{UnivariateCounterExampleInR} for . The last example demonstrates a blow-up in a bounded set.

If the definition ignores even a single point --- as in  --- then it is possible to construct a cost function with a zero at that point, while not being reflected in the -notation. This was demonstrated in \eref{ZeroCounterExample} for .

Such definitions make it impossible to treat the cost functions as black boxes through their -sets --- something which is essential for cost analysis based on -notation.
\end{note}





\section{Some history of \texorpdfstring{}{O}-notation}
\label{History}

The first reference to the -notation seems to be that of Bachmann \cite[page 401]{BachmannOh}, in 1894, who gave a rather brief and informal definition of the -notation:
\begin{quote}
... wenn wir durch das Zeichen  eine Gr\"osse ausdr\"ucken, deren Ordnung in Bezug auf  die Ordnung von  nicht \"uberschreitet; ...
\end{quote}
which we translate as: when we use the symbol  to represent some quantity, its order w.r.t.  does not exceed the order of . 

Landau \cite[page 31]{SmallOh}, in 1909, put this definition on a formal grounding by

for all . This is asymptotic linear dominance on . On page 883, Landau credits this definition to \cite{BachmannOh}. 

Both Bachmann and Landau were writing about analytic number theory, which is a branch of mathematics studying the properties of integers using tools from analysis. Correspondingly, the -notation was constructed to address the needs in this field, such as to bound the error of truncating a series. Landau's definition was adopted by computer science, and remains the most wide-spread definition of -notation on  to this day.

In \cite{ArtOfProgrammingVol1Ed1}, in 1968, Knuth defined the -notation as

for all , where , or  is an interval of . Knuth credited the definition to Bachmann \cite{BachmannOh}. However, Knuth's definition probably contained an omission, since it does not correspond to Bachmann's definition, at least by Landau's interpretation of asymptotic linear dominance. This view is supported by that in \cite{ArtOfProgrammingVol1Ed2}, in 1973, the definition was replaced with Equation \ref{LandauAsymptotic}. To the best of my knowledge, in both editions of the book Knuth uses the -notation solely to bound the error of truncating a series, and not for the analysis of algorithms. For analysis of algorithms, he deals with explicit bounds instead.



It is unclear to me when exactly the -notation was first used in the analysis of algorithms. The earliest reference \we{} can find is \cite{EarlyONotation}, from 1972. The book \cite{DesignAndAnalysisOfComputerAlgorithms}, from 1974, uses the -notation in a modern way in the analysis of algorithms, with the following definition on :
\begin{quotation}
A function  is said to be  if there exists a constant  such that  for all but some finite (possibly empty) set of nonnegative values for .
\end{quotation}
This is cofinite linear dominance on . 

On \cite[page 39]{DesignAndAnalysisOfComputerAlgorithms}, an exercise encourages to study the properties of the following definition of -notation on :

This is linear dominance on . 

On \cite[page 39]{DesignAndAnalysisOfComputerAlgorithms}, another exercise asks for the equivalence of cofinite affine dominance on  and cofinite linear dominance on  under certain assumptions, with \emph{cofinite affine dominance} on  defined by
\begin{eqs}
f \in \oh{}{g} \coloniff \exists c, d \in \posi{\TR}: f \leq c g + d,
\end{eqs}
for all but a finite number of input-arguments. 

Prior to this \manuscript{}, there were no attempts at studying the -notation systematically, in order to check whether the definition from analytic number theory was suitable for the analysis of algorithms. Indeed, it seemed as if the definition were simply a matter of taste; to quote \cite{BigOmega},

\begin{quotation}
On the basis of the issues discussed here, I propose that members of SIGACT, and editors of
computer science and mathematics journals, adopt the , , and  notations as defined above, unless a better alternative can be found reasonably soon. 
\end{quotation}

The two exercises in \cite{DesignAndAnalysisOfComputerAlgorithms} show that the possibility of using a different definition of the -notation was certainly noted, but that an argument to favor one over another was missing. 

\subsection{Related notations}

There are several other notations which resemble the way the -notation works, which perhaps force strictness, reverse the ordering, or otherwise vary the ordering. In \cite[page 61]{SmallOh}, Landau defined the -notation as
\begin{eqs}
f \in \smalloh{}{g} \coloniff \lim_{x \to \infty} \frac{f(x)}{g(x)} = 0,
\end{eqs}
for all . On page 883, Landau states that the -notation is his own.

Knuth \cite{BigOmega}, in 1976, defined the -notation as
,
the -notation as 
,
and the -notation as
,
for all . This was to remedy the occasional misuse where  was used in place of the now-defined  or .

Vit\'anyi \cite{BigOmegaVsWild} argued that  should be defined asymmetrically by


\section{Objective and contributions}

The objective of this \manuscript{} is to provide a rigorous mathematical foundation for the -notation in algorithm analysis. In the following is a list of problems this \manuscript{} solves. 

\subsection{Problem 1: What is a cost function?}

A cost function of an algorithm has invariably been explained in books --- such as \cite{DesignAndAnalysisOfComputerAlgorithms} and \cite{IntroAlgo2009} --- as a function which maps the \emph{size of the input} to . Unfortunately, the term \emph{size} has never been defined formally. Instead, the concept has been demonstrated by examples such as the length of a sequence, the number of bits in a number, or the pair , where  is the number of vertices in a graph, and  is the number of edges in a graph. 

On Turing machines, input\-/size refers to the number of consecutive bits on the input\-/tape that need to be written in order to set the initial state. If the input is not in binary form already, the input has to be encoded as such, and the encoding specified. This definition of input\-/size \emph{does not} formalize the intuitive concept of input\-/size used in books. For example, an encoding of a graph could have an input\-/size proportional to . However, there are graph algorithms whose cost\-/functions are only dependent on , and graph algorithms whose cost\-/functions\footnote{e.g. .} depend non\-/trivially on both  and . Such cost\-/functions cannot be captured as a function of .

Here are some examples of the confusion this concept creates. The prototypical examples of \emph{size} are cardinality and volume, with an implied linear order. But how does one define a linear order \emph{sensibly} on, say, the pairs ? If that does not make sense, should the concept of size be extended to a partial order, or even a preorder? When the cost function is a function of input size, what does it mean to remap the input\-/sizes by function composition from the right (i.e. )? When this \manuscript{} allows the domain of the cost function to be arbitrary, such as ,  or , what exactly does it mean for an input to have size ,  or ?

The present \manuscript{} resolves these questions as follows. The term \emph{input\-/size} does not make sense for an arbitrary model of computation. Instead, this \manuscript{} defines the cost function as a function which maps the input-set to  --- the most detailed characterization of the cost function of an algorithm. Function composition from the right is used to access the input in a different order --- as when the algorithm is called as a sub-algorithm of another algorithm. Compatibility with this operation means that merely a different access-order cannot be made to show inconsistencies in the -notation. 

The grouping of the input-set need only be done if the cost function by itself is too complex to make sense of. In such a case, we choose a grouping property --- such as the pair  for graphs --- and partition the input-set based on that property. No additional properties --- such as an ordering --- are required from a grouping property.

\subsection{Problem 2: Why should the \texorpdfstring{}{O}-notation be defined as it is?}

Introductory texts on algorithms, such as \cite{IntroAlgo}, traditionally explain the definition of the -notation by the merits it has, such as abstracting out differences in the speeds of otherwise identical machines. The asymptotic part -- ignoring parts of the input --- is explained by being essential for the notation to concentrate on the scaling behavior on large inputs, and not on some possible artifacts on small inputs. 

Unfortunately, these explanations fail to pinpoint why we should not pick any other definition with seemingly the same properties, or whether it is necessary at all to ignore part of the input to gain sensitivity to the scaling behavior. Indeed, we show that, in general, no part of the input can be ignored without the notation failing in some way. The first hint towards this fact is seen by making an exhaustive list\footnote{An exhaustive list can be found in \tref{TableOfDesirableProperties}.} of all the rules we need the -notation to possess; to express each one, none requires any restrictions --- such as an ordering or a sense of convergence --- on the input domain. 

The present \manuscript{} resolves this question as follows. First, the -notation should provide us with the simplification we wanted. Second, it should impose an order on the functions. Third, it should be compatible with all those operations --- such as addition, multiplication, and scalar multiplication --- that are needed to combine the actual cost functions during explicit analysis. Specifically, the following procedures must yield the same answer:
\begin{itemize}
\item Apply a sequence of operations to cost functions, and then simplify the result with the -notation.
\item Simplify every cost function with the -notation, and then apply the same operations as above to the -sets.
\end{itemize}

Perhaps the most important operation for algorithm analysis --- identified in this \manuscript{} --- is that of \emph{function composition} from the right, which corresponds to reordering or remapping the input. Compatibility with function composition corresponds to the requirement that the cost function of an algorithm must stay consistent when it is called with distorted input as a sub-algorithm of another algorithm. This is a strong requirement: it forces the definitions of -notations on \emph{different domains} to be consistent with each other. When combined with the other requirements, this turns the definition from a matter of taste to a unique one.

\subsection{Problem 3: How should the multivariate -notation be defined?}

When analyzing a graph algorithm, the input\-/set is often grouped by a two-dimensional property , where  is the number of vertices in the graph, and  is the number of edges in the graph. To provide the complexity in the -notation under such a grouping, the -notation must also be defined in .

Textbooks on algorithms approach the definition on  in two ways. The first way --- exemplified in \cite[page 312]{DesignAndAnalysisOfComputerAlgorithms} --- is to define the -notation only in , and then silently continue using the notation over multiple variables too. That is, with no definition. The second way --- exemplified in \cite{IntroAlgo} and \cite{IntroAlgo2009} --- is to define the -notation in , and later provide some definition for a generalization to , with the implication that the generalization does not bring anything new. Among all the books \we{} have looked at, \cite{IntroAlgo} and \cite{IntroAlgo2009} were the only ones to provide a definition for  --- although different ones. The former provided asymptotic linear dominance, while the latter provided coasymptotic linear dominance.

The first documented symptom of a non\-/trivial generalization to  was given by Howell \cite{OhImpossible}, who demonstrated a flaw\footnote{We will take a closer look at this in \sref{HowellCounter}} in asymptotic linear dominance on . Perhaps it was for this issue, or a similar issue, that \cite{IntroAlgo2009} changed the definition to coasymptotic linear dominance. 

How can one be sure that \emph{this} definition as coasymptotic linear dominance has the desirable properties? In \cite{OhImpossible}, Howell thought he had shown that a definition with desirable properties is impossible in . However, we will show in \sref{HowellCounter} that --- due to excessively strong assumptions --- Howell only showed that asymptotic linear dominance does not have the desirable properties. 

The present \manuscript{} resolves this question by showing that the only definition which has the desirable properties is linear dominance; the multivariate definition is merely an instance of the definition on . 

\subsection{Problem 4: How are the desirable properties connected to each other?}

To make the theory in this \manuscript{} robust to variation, and to self-test it, this \manuscript{} studies the connections between the desirable properties of the -notation. This study reveals that certain combinations of properties imply other properties, and that certain combinations of properties are equivalent to each other. As a side effect, this \manuscript{} does not merely study the -notation in algorithm analysis, but also other possible or historical definitions of the -notation. 

The study on the connections culminates in the discovery of the \nprim{} primitive properties, which imply all the desirable properties. Therefore, the question of whether the desirable properties are reasonable reduces to asking whether the primitive properties are reasonable.

\subsection{Problem 5: Do Master theorems hold for linear dominance?}

Master theorems are useful tools for analyzing the cost function of a recursive algorithm up to an -equivalence. Therefore, it is important to make sure that the conclusions of Master theorems still hold after adopting linear dominance as the definition. 

The present \manuscript{} provides the necessary proofs in \sref{MasterTheorems}. In summary, Master theorems hold similarly as with asymptotic linear dominance, with the simplification that there is no need for so-called regularity conditions \cite{IntroAlgo2009}.

\subsection{Problem 6: How do existing definitions compare to each other?}

The present \manuscript{} provides in \sref{CandidateDefinitions} a comparison between various definitions of the -notation, based on which primitive properties each fulfills. The study of how exactly the existing definitions fail the primitive properties is important, because a priori there is a risk that they may have produced incorrect analyses. 

Fortunately, coasymptotic linear dominance on  --- on positive functions --- is equivalent to linear dominance on . This covers most of the analyses that have been done in algorithm analysis. In practice, the saving grace is that the -notation has been manipulated by assuming that the rules on  work in general, and by implicitly assuming that \property{SubComp} holds. 

\subsection{Problem 7: How should the related notations be defined?}

There has been some confusion on how to define the related notations , and , as discussed in \sref{History}. 

The present \manuscript{} argues that these notations are merely different viewpoints of the same underlying order-relation between cost functions; they are fixed by the definition of the -notation.

\subsection{Problem 8: Can existing techniques of analysis still be used?}

The present \manuscript{} shows in \sref{LimitTheorems} that local linear dominance --- a definition which covers most of the existing definitions  --- has an equivalent definition by limits. In addition, under suitable conditions, the other existing definitions are sometimes equivalent to linear dominance. Together, this allows to continue using existing analysis tools --- such as taking limits --- as before, provided one makes sure that there is a way to transfer the result to linear dominance.

\subsection{Problem 9: Point out misuses}

The present \manuscript{} points out in \sref{Misuses} some misuses which occur in actual publications in computer science. In particular, the -notation is sometimes used as a general something-like operator to generalize statements, where it actually does not make sense. It is my hope that making such misuses explicit eventually improves communication between computer scientists.

\section{Relation to publications}

The present \manuscript{} extends a peer-reviewed publication in the Bulletin of EATCS \cite{ONotationBeatcs}, a doctoral thesis with the same name, as well as various non-peer-reviewed versions in Arxiv \cite{ONotationArxiv}. 

Most of the proofs have been deferred to appendices. This is to make the \manuscript{} readable to the largest possible audience, emphasizing the intuitive ideas. This does not reflect an ordering in importance --- to \me{}, the proofs are the most important part of this \manuscript{}.

The research and writing related to this \manuscript{}, and the above publications, were done solely by \kr; the co-authors checked.

\section{Automated checking}

This \manuscript{} was written in such a way that the dependencies between the theorems can be --- and have been --- checked by a machine. Each theorem in the \LaTeX{} source is annotated --- in a lightweight, but machine-readable manner --- with its assumed and implied properties. When the proof of Theorem A references Theorem B, it inherits the properties implied by B, from that point on, provided that the assumptions of B are satisfied at that point. For each theorem, the software checks that
\begin{itemize}
\item the assumptions of each referred theorem are satisfied, and that
\item there are no extraneous assumptions.
\end{itemize}
When a property is not proved by referring to a theorem, it is explicitly marked proved by the writer, based on the preceding non-machine-checkable proof. 

Such automatic checking was useful in the research phases to guarantee that theorems were not broken due to changing assumptions, or otherwise to point them out. The software for checking the dependencies, written in Python, can be obtained from 
\ifwe
KR's
\else
my
\fi
homepage.\footnote{http://kaba.hilvi.org}

\section{Outline of the \manuscript{}}

This \manuscript{} is organized as follows. \sref{Introduction} introduces the \manuscript{}, reviews the history of the topic, and lists the contributions of the \manuscript{}. \sref{Preliminaries} provides a more formal introduction to the concepts related to algorithms and their analysis. \sref{DesirableProperties} provides a list of desirable properties for an -notation. \sref{Characterization} proves that the primitive properties are equivalent to the definition of the -notation as linear dominance. \sref{WorkingWith} provides tools for working with the -notation, including Master theorems and -mapping rules. \sref{LocalLinearDominance} studies local linear dominance, a generalization which covers most of the existing definitions. \sref{Conclusion} concludes the \manuscript{}.

\sref{Notation} provides the notation. \sref{HowellCounter} reviews Howell's counterexample in more detail. \sref{ImpliedProperties} shows that the desirable properties reduce to a set of \nprim{} primitive properties which imply the other properties. \sref{ProofsForLocalLinearDominance} shows properties of local linear dominance. \sref{ProofsOfLimitsTheorems} shows that local linear dominance can be characterized by ratio\-/limits. \sref{MasterTheorems} shows that Master theorems work as before under linear dominance. \sref{CandidateDefinitions} compares several definitions for an -notation, and shows how each of them fail the desirable properties. \sref{ProofsOfMinimality} provides additional definitions for an -notation, with an aim to show the minimality of pre-primitive properties. \sref{PartitionedSets} provides some theory of partitioned sets. \sref{PreorderedSets} provides some theory of preordered sets.

\chapter{Preliminaries}
\label{Preliminaries}

In this chapter we provide a brief introduction to algorithms, the computational model, the cost-model, the primitive properties, and the -notation.

\section{Algorithms}
\label{Algorithms}

What is an algorithm? We adopt an extremely liberal, but completely formalized view: an \emph{algorithm} is an abstract state machine \cite{SequentialAsm, ParallelAsm, ASMBook}. 

A variable in an abstract state machine  is identified with a string, called a \emph{(function) symbol}. Each symbol has an \emph{arity} , which gives the number of arguments the symbol accepts as input. A \emph{(ground) term} is defined recursively as follows:
\begin{itemize}
\item a -ary symbol is a term, and
\item if  is an -ary symbol, and  are terms, then the string  is a term, and
\item there are no other terms.
\end{itemize}
The set of user-defined symbols, together with a small set of predefined symbols --- such as , , , , , ,  --- is called the \emph{vocabulary} of the abstract state machine .

Each -ary symbol  is associated with a function .\footnote{.} The function  is an \emph{interpretation} of . The set  is the \emph{base-set}, which is common to all interpretations. The \emph{value} of a term , denoted by , is defined recursively as follows:
\begin{itemize}
\item if  is a -ary symbol, then ,
\item if  is an -ary symbol, for , and  are terms, then

\end{itemize}
In the following, by  --- where  is a symbol, , and  --- we mean that . In addition, if  is a -ary symbol, then by  we mean that  in the previous sense.

Compared to an ordinary programming language, a -ary symbol corresponds to a variable, while an -ary symbol, for , corresponds to an -dimensional array --- however, here the index can be an arbitrary set element. 

The abstract machine specifies how the interpretations of symbols are to be modified at each step. The program driving the abstract machine is a finite sequence of conditional assignments of the form 
\begin{algorithmic}
\If{}
\State 
\State 
\State 
\EndIf
\end{algorithmic}
where , , \dots, , and , \dots,  are terms. The formula  can be thought of as copying an element from an array to another, , provided . This sequence of assignments is repeated until (possible) termination. 

All of the assignments in a single step are carried out in parallel --- not in sequence. For example,  followed by  causes  and  to swap values in the next step.

The basic definition of abstract state machines is both simple, and extremely general. The generality derives from the virtue of making the whole of set-theory available for modeling variables. Further abstraction-tools --- such as sequential composition, sub-machine calls, local variables, and return values --- are constructed over this basic definition. For example, Turbo-ASMs \cite{ASMBook} provide such features. From now on, we will assume that such abstraction tools have already been defined.

Consider \aref{alg:FindZero}, which is Newton's method for finding local zeros of differentiable functions.\footnote{This example generalizes an example from \cite{RealRam} where Newton's method is formalized as an algorithm for real rational functions under the real-RAM model.} The input-symbols to this algorithm are a -ary continuously differentiable function , a -ary initial guess , and a -ary error threshold ; the input-set is . The output --- provided the algorithm terminates --- is a -ary point  such that ; the output-set is . Other symbols are a -ary symbol  --- differentiation  --- , a -ary symbol  --- absolute value --- a -ary symbol  --- greater-than --- and -ary symbols  and  --- subtraction and division. We have used infix notation for subtraction, division, and greater-than; postfix notation for differentiation; and midfix notation for the absolute value.

\begin{algorithm}
\caption{Newton's method for finding an element , such that , for a continuously differentiable function .}
\label{alg:FindZero}
\begin{algorithmic}[1]
\Procedure {findZeroOrHang}{f, , }
\State 
\While {}
  \State 
\EndWhile
\State \Return 
\EndProcedure
\end{algorithmic}
\end{algorithm}

\aref{alg:FindZero} reads like pseudo-code, but is a completely formalized abstract state machine. With abstract state machines, the programmer is free to use the most fitting abstraction for the problem at hand. It should be clear how using such an abstract programming language enhances communication between software engineers and domain experts (e.g., physicists). 

Termination is not required for an algorithm; consider for example an operating system. Depending on the input, \aref{alg:FindZero} may terminate, or not. Suppose we require \aref{alg:FindZero} to always terminate. To satisfy this requirement, the programmer can either restrict the input-set to terminating inputs, or to modify the algorithm --- perhaps by limiting the number of iterations. From now on, we assume every analyzed algorithm to terminate.

A software development project utilizing abstract state machines starts by creating the most abstract description of the software as an abstract state machine, called the \emph{ground model} \cite{ASMBook}. This model captures the requirements, but does not provide any additional details on how the goals are to be attained. The project then proceeds to refine the model until it can be implemented in a concrete programming language. The model --- in all stages --- is used for verification and validation, and may even be used to generate code automatically.

This brief introduction to abstract state machines is to encourage the reader to look beyond the Church-Turing \manuscript{}, and to realize the usefulness of even the most abstract algorithms --- not just those computable algorithms which work with natural numbers. These are algorithms which take arbitrary sets as input, and produce arbitrary sets as output. To analyze such abstract algorithms, we need correspondingly abstract tools.

Here are some examples of input-sets that algorithms can have, or by which they can be modeled.

\begin{example}[Algorithms on integers]
An algorithm which takes integers as input could be modeled by .
\end{example}

\begin{example}[Algorithms on floating\-/point numbers]
An algorithm which takes floating\-/point numbers as input could be modeled by .
\end{example}

\begin{example}[Algorithms on complex numbers]
An algorithm which takes complex numbers as input could be modeled by .
\end{example}

\begin{example}[Algorithms on matrices]
An algorithm which takes a matrix as input could be modeled by .
\end{example}

\begin{example}[Algorithms on sequences]
An algorithm which takes a sequence in  as input could be modeled by
\begin{eqs}
X^* = \bigcup_{d \in \TN} X^d.
\end{eqs}
\end{example}

\begin{example}[Algorithms on combinations of the above]
An algorithm which takes a sequence in  and an integer as input could be modeled by .
\end{example}

\begin{example}[Algorithms on graphs]
A graph algorithm is often analyzed under the domain . This domain comes from the worst\-/case analysis (say), where the input is grouped according to the number of vertices and edges.
\end{example}

\begin{example}[Algorithms in computational geometry]
An algorithm in computational geometry \cite{CGeometry} works directly with points in , and may not mention floating\-/point numbers at all. The worst\-/case (or direct) analysis may require the domain .
\end{example}

\begin{example}[Algorithms on different dimensions]
We may be interested in how seeing how a geometric algorithm in  scales with respect to the dimension . In this case the input-set may consist of a union of data structures (e.g. range tree) for different dimensionalities. Dimensionality can then be used as a grouping property.
\end{example}

\begin{example}[Approximation schemes]
When considering an NP-hard problem, we may consider an approximation scheme \cite{IntroAlgo2009}, where  specifies the quality of the approximation;  is specified as part of the input.
\end{example}

\section{Computational model and cost-model}
\label{Cost models}

Before an algorithm can be written, the writer must decide on the model of computation. A \define{model of computation} is a mathematical structure, and a set of atomic operations which manipulate that structure. Some models of computation are the Turing machine, the random-access machine (RAM) \cite{RamModel}, the real-RAM \cite{RealRam}, and the abstract state machine.

The result of complexity analysis --- for a given model of computation, a given algorithm, and a given resource --- is a function , which provides for each \emph{input} of the algorithm a non-negative real number. This number tells how much of that resource the algorithm consumes with the given input. As discussed in \sref{Algorithms}, the input-set  can be arbitrary.

Before a complexity analysis can be carried out, the analyst must decide on the cost\-/model. A \define{cost\-/model} specifies the amount of resources that running an atomic operation takes on a given input. A given computational model can assume different cost\-/models. When the cost\-/model is unspecified --- as it often is --- the cost of each atomic operation is assumed to be one unit irrespective of input. 

\begin{example}[Constant cost\-/models]
The most common cost\-/model is the unit\-/cost model, which counts the number of performed atomic operations. Zero costs can be used to concentrate the interest to specific resources, such as order\-/comparisons. 
\end{example}

\begin{example}[Non\-/constant cost\-/models]
An example of a non\-/constant cost\-/model is to assign the addition of natural numbers\footnote{Assuming the computational model supports such an operation.} a cost which is proportional to the logarithms of the arguments, so as to be proportional to the number of bits in their binary representations. 
\end{example}

\begin{example}[Cost\-/models for abstract state machines]
An abstract state machine specifies costs for reading or writing a memory location through a given symbol. Reading an addition symbol  at  ---  --- could be assigned a logarithmic cost as described above.
\end{example}

\section{Primitive properties}

Complexity analysis aims to classify and compare algorithms based on their resource consumptions; the less an algorithm uses resources to solve a given problem, the better it is compared to other algorithms which solve the same problem. The cost functions of the algorithms to solve a problem  are elements of .\footnote{; see \sref{Notation}.} The most general way to compare them is to define a preorder\footnote{A preorder on a set  is a reflexive and transitive relation .} . This relation should capture the intuitive concept of a cost function  being either better than or equivalent to the cost function  --- in some sense. For brevity, we use the term \emph{dominated by}.

\begin{note}[Not worse]
It is tempting to use the phrase \emph{not worse than} instead of \emph{better or equivalent}. However, the former means better, equivalent, \emph{or incomparable}, which is not what we want.
\end{note}

\begin{note}[Cost function of an algorithm]
Given an algorithm , we shall denote its cost function by .
\end{note}

\begin{figure}
\center
\begin{tikzpicture}
\begin{axis}[xmin = 0, ymin = 0, xmax=50, ymax=50, samples=100]
  \addplot[dashed, thick, domain=0:50] (x, x * x / 20 + 10);
  \addplot[thick, domain=0:50] (x, x / 2 + 20);
\end{axis}
\end{tikzpicture}
\caption{Here  are such that  (solid line), and  (dashed line). Should , , , or should  and  be incomparable?}
\label{AmbiguousOrder}
\end{figure}

\begin{example}[Essentially better?]
Consider Figure \ref{AmbiguousOrder}, where  are such that  for , and , for . We would then be inclined to say that  has a better cost function, since  is small anyway on the finite interval ;  is ``essentially'' better than . How can this intuition be formalized? 
\end{example}
To decide between various definitions, we reflect on the fundamental properties that the analyst needs to complete his/her complexity analysis. An obvious \define{dominance property} is 
\begin{description}
\item[\property{Order}] \hfill \\ 
if an algorithm  never uses more resources than algorithm , then  is better than or equivalent to :
\begin{eqs}
f_A \leq f_B \implies f_A \domi f_B.
\end{eqs}
\end{description}
\begin{note}[Reflexivity]
In particular, \property{Order} implies that  is reflexive: . 
\end{note}
Since we want  to be a preorder, we also need
\begin{description}
\item[\property{Trans}] \hfill \\ 
if  is better than or equivalent to , and  is better than or equivalent to , then  is better than or equivalent to :
\begin{eqs}
\bra{f_A \domi f_B \land f_B \domi f_C} \implies f_A \domi f_C.
\end{eqs}
\end{description}
In addition to being a preorder, the dominance relation must preserve the \emph{structure} present in algorithms: conditional branching, calls with transformed input, and looping.

\begin{algorithm}
\caption{An algorithm to demonstrate \property{Local}. Improving sub-algorithm  for even integers improves the whole algorithm. We assume resources are only spent in .}
\label{alg:locality}
\begin{algorithmic}[1]
\Procedure {branching}{}
\If{}
\State \Return{}
\EndIf
\State \Return{}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{description}
\item[\property{Local}] \hfill \\ 
substituting a sub-algorithm \emph{for a subset of the input-set} with a better or equivalent sub-algorithm results in a better or equivalent algorithm overall:
\begin{eqs}
\bra{\forall i \in [1, n]_{\TN}: \restrb{f}{A_i} \preceq_{A_i} \restrb{g}{A_i}} \implies f \domi g,
\end{eqs}
for every finite cover . \aref{alg:locality} demonstrates \property{Local}.
\end{description}

\begin{algorithm}
\caption{An algorithm to demonstrate \property{SubComp}. Improving sub-algorithm  improves the whole algorithm. We assume resources are only spent in .}
\label{alg:subcomp}
\begin{algorithmic}[1]
\Procedure {calling}{}
\State \Return{\Call{F}{}}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{description}
\item[\property{SubComp}] \hfill \\ 
substituting a sub-algorithm \emph{called with transformed input} with a better or equivalent sub-algorithm results in a better or equivalent algorithm overall:
\begin{eqs}
f_A \preleq f_B \implies f_A \circ s \preleqb f_B \circ s,
\end{eqs}
for all . \aref{alg:subcomp} demonstrates \property{SubComp}.
\end{description}

\begin{algorithm}
\caption{An algorithm to demonstrate \property{NSubHom}. Improving sub-algorithm  improves the whole algorithm. We assume resources are only spent in .}
\label{alg:nsubhom}
\begin{algorithmic}[1]
\Procedure {looping}{}
\For {}
  \State \Call{F}{}
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{description}
\item[\property{NSubHom}] \hfill \\ 
substituting a sub-algorithm \emph{in a loop} with a better or equivalent sub-algorithm results in a better or equivalent algorithm overall:
\begin{eqs}
f_A \preleq f_B \implies uf_A \preleq uf_B,
\end{eqs}
for all  such that . \aref{alg:nsubhom} demonstrates \property{NSubHom}.
\end{description}

\begin{description}
\item[\property{NSubDiv}] \hfill \\ 
if an algorithm that loops sub-algorithm A is better than or equivalent to an algorithm that loops sub-algorithm B, then it is because  is better than or equivalent to :
\begin{eqs}
f_A \preleq f_B \implies uf_A \preleq uf_B,
\end{eqs}
for all  such that .
\end{description}

\begin{note}[Many preorders]
These properties reveal that the preorders in different sets cannot be defined independently of each other; they are tightly connected by \property{Local} and \property{SubComp}. Rather, the problem is to find a consistent class of preorders , where  is a given universe.
\end{note}

\begin{note}[Trivial dominance]
A problem with the listed properties thus far is that the trivial preorder , for all , fulfills them all; then the functions in  are all equivalent. Therefore, for the comparison to be useful, we need to require  to distinguish at least some functions in , at least for some . 
\end{note}
The primitive \define{non\-/triviality property} is:
\begin{description}
\item[\property{One}] \hfill \\ .
\end{description}
Note that  already holds by \property{Order}; \property{One} prevents the order from collapsing due to equivalence.

Finally, there is the question of robustness. Writing an algorithm is an iterative process, which causes the cost function of an algorithm to change constantly. If every change is reflected in the ordering, then each change invalidates an existing complexity analysis of the algorithm. Worse, the change invalidates all the analyses of the algorithms which use the changed algorithm as a sub-algorithm. Since an algorithm may face hundreds or thousands of changes before stabilising into something usable in practice, such an approach is infeasible. Therefore, the ordering needs to introduce identification, to make it robust against small changes in the algorithms. But what is a small change?

The key realization is that for a given algorithm  it is often easy, with small changes, to produce an algorithm , for which the improvement\-/ratio\footnote{Assuming .}  stays bounded.\footnote{For example, change to use Single Instruction Multiple Data (SIMD) instructions.} In contrast, obtaining an unbounded improvement\-/ratio often requires considerable insight and fundamental changes to the way an algorithm works --- a new way of structuring data and making use of it. This definition of interesting provides the desired robustness against small changes in algorithms. 

The primitive \define{abstraction property} is:
\begin{description}
\item[\property{Scale}] \hfill \\ if , then , for all .
\end{description}

\begin{note}[Dominance is linear dominance]
We will show in \sref{Characterization} that the \nprim{} primitive properties are equivalent to the definition of dominance as linear dominance:
\begin{eqs}
f \preleq g \iff \exists c \in \posi{\TR}: f \leq cg.
\end{eqs}
\end{note}

\begin{note}[Generators and propagators]
When the primitive properties are used as axioms, every proof of  must appeal to \property{Order}, and every proof of   must appeal to \property{One}. All the other primitive properties propagate these results.
\end{note}

\begin{definition}[Related relations]
A preorder  induces the following related relations in :
\begin{eqs}
f \approx_X g & \iff f \preleq g \land g \preleq f, \\
f \prec_X g & \iff f \preleq g \land f \not\approx_X g, \\
f \succ_X g & \iff g \prec_X f, \\
f \pregeq g & \iff g \preleq f.
\end{eqs}
\end{definition}

\begin{note}[Strict comparison is weaker]
\label{StrictComparisonIsWeaker}
In a preorder which is not a partial order, the  alone cannot be used to deduce whether ; it is a weaker concept than . However, providing  and  suffices. This is in contrast to a partial order, where  is known to be the set\-/equality in .
\end{note}

\section{Problem complexity}

Apart from analyzing the resource\-/cost of a specific algorithm, complexity analysts also have a greater underlying goal: that of analyzing the resource\-/cost of the underlying problem  itself --- for a given computational model, a given cost\-/model, and a given resource. This activity divides into two sub-activities; finding a lower\-/bound and an upper\-/bound for the cost function of .

\begin{definition}[Lower bound]
A cost function  is a \define{lower\-/bound} for a problem , if , for all . 
\end{definition}

\begin{definition}[Optimal cost function]
A cost function  is \defineexp{optimal}{optimal!cost function} for a problem , if  is a lower\-/bound for , and  for each  a lower\-/bound for .\footnote{That is,  is a greatest lower\-/bound.} 
\end{definition}

\begin{note}[Optimal cost function may not exist]
An optimal cost function may not exist for a problem, as shown in \thref{DominanceIsIncomplete}. When it exists, the optimal cost function for ,
\begin{eqs}
\inf_{\domi} \setb{f_A : A \in \algo{P}},
\end{eqs}
is unique up to equivalence. 
\end{note}

\begin{definition}[Optimal algorithm]
An algorithm  is \defineexp{optimal}{optimal!algorithm}, if  is optimal for .
\end{definition}

\begin{note}[Optimal algorithm may not exist]
An optimal algorithm may not exist for a given problem.
\end{note}

\begin{definition}[Upper bound]
A cost function  is an \define{upper\-/bound} for a problem , if , for some . 
\end{definition}

An upper\-/bound is found by finding an actual algorithm for solving . While there are computational problems which cannot be solved at all,\footnote{e.g., the halting problem under Turing machines.} establishing at least one upper\-/bound for a solvable problem is often easy. These are the \define{brute\-/force} algorithms, which compute or check everything without making any use of the underlying structure in the problem. 

\section{\texorpdfstring{}{O}-notation}

\begin{definition}[-notation]
An \define{-notation} over the universe  is a class of functions

where
\begin{eqs}
\ohx{f} = \setb{g \in \rc{X} : g \domi f}.
\end{eqs}
\end{definition}

\begin{note}[Different symbols]
The  is used as a generic symbol for studying how the different desirable properties of  interact with each other. We will use drawings inside the  symbol for specific versions of the -notation, such as  for asymptotic linear dominance. 
\end{note}

\begin{note}[Different viewpoints]
It is equivalent to define either the functions , or the preorders ; one can be recovered from the other. We shall give the theorems in terms of , since this is more familiar to computer scientists. However, \we{} find that intuition works better when working with . 
\end{note}

\begin{note}[Related notations]
The related notations are given in terms of the dominance relation as:
\begin{eqs}
\smallohx{f} & = \setb{g \in \rc{X} : g \prele f}, \\
\omegahx{f} & = \setb{g \in \rc{X} : g \pregeq f}, \\
\smallomegax{f} & = \setb{g \in \rc{X} : g \prege f}, \\
\thetahx{f} & = \setb{g \in \rc{X} : g \approx_X f}.
\end{eqs}
\end{note}

The -notation extends naturally to sets of functions; such generality is sometimes needed.

\begin{definition}[-notation]
The \define{-notation} on a set  is a function  such that
\begin{eqs}
\setohx{A} = \bigcup_{f \in A} \ohx{f}.
\end{eqs}
\end{definition}

\section{Implicit conventions}
\label{ImplicitConventions}

An \define{implicit convention} is an overload of notation adopted by people working in a given field. Since it is an overload, the reader is required to deduce the correct meaning of such notation from the context. Here are some implicit conventions related to the -notation --- as commonly used in computer science.

\subsection{Placeholder convention}

\begin{definition}[Placeholder convention]
The \define{placeholder convention} is to use  as a placeholder for an anonymous function . It then must be guessed from the context whether the author means by  the actual set, or the anonymous function . 
\end{definition}

\begin{example}[Cost of an algorithm]
An algorithm costs  if its cost function is an element of .
\end{example}

\begin{example}[Exponential of an -set]
Consider the statement that an algorithm costs . As a set, . However,
\begin{eqs}
0.5 \in \setoh{\TN}{2^{\oh{\TN}{n}}},
\end{eqs}
since  and . Similarly,
\begin{eqs}
2^{2.5n} \in \setoh{\TN}{2^{\oh{\TN}{n}}},
\end{eqs}
since . 
\end{example}











\subsection{Domain convention}

\begin{definition}[Domain convention]
The \define{domain convention} is to leave off the domain of the -notation, say , and then let the reader guess, for each use, the domain from the context. Sometimes the domain convention leads to a difficult interpretation. 
\end{definition}

\begin{example}[Difficult interpretation]
Consider an algorithm \cite{LocalSearch} under the unit\-/cost -bit RAM model, where , which for  finds a nearest neighbor of  in  in time
\begin{eqs}
\oh{}{\lg{\lg{\Delta + 4}}}, 
\end{eqs}
where  is the distance between  and its nearest neighbor in . Intuitively, this sounds reasonable, but what is the domain? \Our{} thinking process went as follows.

Since the expression contains only a single symbol , \we{} assumed it to be a univariate \-/notation. \Our{} first guess was  --- with  fixed. However, since  is bounded, this is equal to . The guess had to be wrong; otherwise the authors would have reported the complexity as . 

\Our{} second guess was  --- again with  fixed. However, this arbitrarily extends the complexity analysis to elements outside the input-set, since . In addition, it is not always possible to do such an extension, such as when the function is  instead. 

Finally, \we{} observed that the complexity depends both on  and  --- although  never mentions . The correct formalization is given by , where . The corresponding algorithm would then take as input
\begin{eqs}
w & \in \posi{\TN}, \\
I & \in \cup_{k \in \posi{\TN}} \power{[0, 2^k)_{\TN}}, \\
i & \in \cup_{k \in \posi{\TN}} [0, 2^k),
\end{eqs}
subject to  and .
\end{example}

\section{Worst case, best case, average case}
\label{WorstCaseAndOthers}

In this section we will formalize the concepts of worst\-/case, best\-/case, and average\-/case analyses.

\begin{definition}[Grouping]
A \define{grouping} of  is a function . 
\end{definition}

\begin{definition}[Case]
A \define{case} over a grouping  is a function  such that ; a right inverse of .
\end{definition}

\begin{definition}[Worst case]
A case  over a grouping  is called \emph{worst} of , if

for all .
\end{definition}

\begin{note}
A worst case may not exist.
\end{note}

\begin{definition}[Worst-case analysis]
A \define{worst-case analysis} of  over a grouping  is the process of finding out
\begin{eqs}
\sup \image{f}{\preimage{g}{\setb{z}}}
\end{eqs}
or some -set which contains it. 
\end{definition}

\begin{definition}[Best case]
A case  over a grouping  is called \emph{best} of  if

for all . 
\end{definition}

\begin{note}
A best case may not exist.
\end{note}

\begin{definition}[Best-case analysis]
A \define{best-case analysis} of  over a grouping  is the process of finding out
\begin{eqs}
\inf \image{f}{\preimage{g}{\setb{z}}}
\end{eqs}
or some -set which contains it.
\end{definition}

\begin{example}[Analysis of insertion sort]
Assume the RAM model, with unit cost for comparison of integers and zero cost for other atomic operations. Let  be the set of all finite sequences over . Let  be the insertion sort algorithm \cite{IntroAlgo2009}, which sorts a given input sequence  into increasing order. Let  be the number of comparisons made by . Let  be such that , the length of the sequence . Let  be the worst case of  over ; each such sequence is decreasing. Then the worst-case complexity of  over  is , and the worst-case analysis of  provides . Let  be the best case of  over ; each such sequence is increasing. Then the best-case complexity of  over  is , and the best-case analysis of  provides . For an arbitrary case  of  over , it holds that .
\end{example}

\begin{definition}[Average-case analysis]
Let  be a probability space, and  be a measurable space. Let  be a random element, and  be a random variable. An \define{average-case analysis of  over } is the process of finding out , where  is any case over , and  stands for (conditional) expectation.
\end{definition}

Since worst-case, best-case, and average-case analyses are the most common forms of complexity analysis in computer science --- with the grouping set almost always , for some  --- this has led to the often repeated claim that the result of complexity analysis is a function which maps an `input size' to the amount of used resources. For example, \cite[page 25]{IntroAlgo2009} writes as follows (emphasis theirs):
\begin{quotation}
\noindent
The best notion for \emph{input size} depends on the problem being studied. For many problems, such as sorting or computing discrete Fourier transforms, the most natural measure is the \emph{number of items in the input} - for example, the array size  for sorting. For many other problems, such as multiplying two integers, the best measure of input size is the \emph{total number of bits} needed to represent the input in ordinary binary notation. Sometimes, it is more appropriate to describe the size of the input with two numbers rather than one. For instance, if the input to an algorithm is a graph, the input size can be described by the numbers of vertices and edges in the graph. We shall indicate which input size measure is being used with each problem we study.
\end{quotation}

Complexity theorists sometimes study the cost functions of Turing machines with respect to input-tape-size. However, this is not a formalization of the input\-/size as described in the above quotation. 

The term input\-/size --- as it has been used --- is a synonym for a group label. A group label need not have any properties, such as an order. A grouping need only be done if the analysis or its interpretation otherwise seems difficult. It would seem clearer to use the term input\-/size only in those cases where the group labels form a set which is linearly ordered and contains a least element (e.g. , , or cardinal numbers).

We have shown above how input\-/size-thinking is subsumed by the more general input-set-thinking. In the input-set thinking, a set is used to provide a mathematical model for a data structure, and a cost function is a function of this data. 

\section{Misuses}
\label{Misuses}

The -notation (and related definitions) is sometimes misused even by experienced researchers in computer science. By a \emph{misuse} we mean to use the notation in a context which does not have a formal meaning --- even after applying the implicit conventions specific to computer science. 

In the following we review some misuses made by experienced researchers in computer science. Let us note that, despite the misuses, the sources we refer to here are both great reading: \cite{IntroAlgo2009} is a classic book about algorithms, data-structures, and their analysis, while \cite{ExponentialTrees} provides an ingenious scheme for converting a static dictionary to a linear-space dynamic dictionary.

\begin{example}[Recurrence equations]
Reference \cite[page 102]{IntroAlgo2009} studies the solutions to the recurrence equation

where , , , and . The intent here is to study the Master theorem over powers --- as we do in \sref{MasterTheoremOverPowersSection}. Unfortunately, since  is a set of functions, and not a non-negative real number, this equation does not have a formal meaning. In addition, the set  is left undefined. 

Applying the placeholder convention leads to

where . This still does not make sense;  is a function, not a non-negative real number.

The motivation for this recurrence equation is the analysis of divide\-/and\-/conquer algorithms, where the algorithm recursively solves  -- perhaps overlapping --- sub-problems (when  is an integer), and then combines their results to solve the original problem. The call-graph of such an algorithm is a tree, and in each leaf of this tree we would perhaps like to assign a \emph{different} constant for the amount of resources it takes. The problem is that the function  can have only one value for .

One way to fix the recurrence equation is to simplify it to

where , which is the form we study in \sref{MasterTheorems}. We will show that  is independent of the choice of . Therefore, the simplified recurrence equation provides a solution for the divide\-/and\-/conquer analysis even when the costs in the leaf nodes vary in a fixed closed interval.\footnote{Assuming the interval does not contain zero.} \We{} believe this is the idea that \cite{IntroAlgo2009} was aiming for; it just is not captured by replacing  with .
\end{example}

\begin{example}[Conditional statements]
Let , and . In \cite[page 9]{ExponentialTrees}, there is the following statement:
\begin{quote}
Then for , we have

and

For , we trivially have

\end{quote}
We decode this as follows:
\begin{quote}
Then for , we have

and

For , we trivially have

\end{quote}
The expression  is always true for , ,  and . Similarly, the expression  is always false. The authors have an intuitive concept which they want to transmit. However, the formalization of the intuition is incorrect, and so the communication fails. 
\end{example}

\section{Completeness}

In this section we study the completeness of the dominance relation .

\begin{definition}[Completeness over a family]
A preorder  is \define{complete} over , if every  which has a lower\-/bound (an upper\-/bound) in  has a greatest lower\-/bound (a least upper\-/bound) in . 
\end{definition}

\begin{table}
\centering
\begin{tabular}{|l|l|}
\hline
Concept & Complete over... \\
\hline
Complete &  \\
Directed-complete & directed subsets of  \\
Chain-complete & linearly\-/ordered subsets of  \\
Lattice &  \\
Algorithm-complete &  \\
\hline
\end{tabular}
\caption{Different types of completeness.}
\label{DifferentTypesOfCompleteness}
\end{table}

\begin{note}
Different types of completeness are listed in \tref{DifferentTypesOfCompleteness}.
\end{note}

\begin{proposition}[\uproperty{Lattice} is implied]
\label{DominanceRelationIsALattice}
 has \require{Order}, \require{Local}, and \require{ISubComp}.   has \prove{Lattice}. 
\end{proposition}

\begin{proof}
Let . Then , and by \require{Order} . Suppose  is such that . Let
\begin{eqs}
F_i = \setb{x \in X: f_i(x) = \max(f_1, \dots, f_n)}. 
\end{eqs}
By \require{ISubComp}, . Therefore . By \require{Local}, . Therefore
\begin{eqs}
\sup \setb{f_1, \dots, f_n} \approx_X \max(f_1, \dots, f_n). 
\end{eqs}
Similarly, . \sprove{Lattice}
\end{proof}

\begin{note}[Equivalence between types of completeness]
When  is a lattice, every subset of  is directed, and therefore directed\-/complete is equivalent to complete. Further, chain\-/complete is equivalent to directed\-/complete by Zorn's lemma.\footnote{Zorn's lemma is equivalent to the axiom of choice.}
\end{note}

\begin{theorem}[Incompleteness]
\label{DominanceIsIncomplete}
 has \require{Order}, \require{Trans}, \require{NSubHom}, \require{NSubDiv}, \require{Scale}, and \require{One}.   is not complete. \sprove{Incomplete}
\end{theorem}

\begin{proof}
 has \property{SubHom} by \proveby{RSubhomogenuityIsImplied}.

Let  be such that , for all . By \require{Order}, , for all . 

Suppose there exists  such that  and . By \require{SubHom}, . It can be shown that . By \require{Order} and \require{Scale}, . By \require{Trans}, , which contradicts \property{One}. Therefore, , for all .

Let .  Then  is a lower\-/bound of . Let  be a lower\-/bound of  such that . 

Suppose , for some . Then , which contradicts  being a lower\-/bound of . Therefore , for all . 

By \require{Order}, . Suppose . By \require{SubHom}, , which contradicts \require{One}. Therefore .

By \require{SubHom}, , for all . It can be shown that , for all  such that . By \require{Order} and \require{Scale}, , for all  such that . By \require{Trans}, , for all ;  is a lower\-/bound for . Since , there is no greatest lower\-/bound for . 

\sprove{Incomplete}
\end{proof}

\chapter{Desirable properties}
\label{DesirableProperties}

In this section we provide an extensive list of desirable properties for an -notation. We will show that they all hold for linear dominance in \sref{ImpliedProperties}. Each property is given for , where . A given property holds for  if it holds for , for all . The desirable properties are listed in \taref{TableOfDesirableProperties}.

\begin{table}
\begin{tabular}{|l|l|}
\hline 
Name & Property \\
\hline 
\hline 
\textbf{\uproperty{Order}} &   \\
\hline 
\uproperty{Reflex} &  \\
\hline 
\textbf{\uproperty{Trans}} &   \\
\hline 
\uproperty{Orderness} &  \\
\hline 
\hline 
\uproperty{Zero} &  \\
\hline 
\textbf{\uproperty{One}} &  \\
\hline 
\uproperty{TrivialZero} &  \\
\hline 
\hline 
\textbf{\uproperty{Scale}} &  \\
\hline 
\usproperty{Translation} &   \\
\hline 
\uproperty{PowerH} &  \\
\hline 
\uproperty{AddCons} &  \\
\hline 
\uproperty{MultiCons} &  \\
\hline 
\uproperty{MaxCons} &  \\
\hline 
\textbf{\uproperty{Local}} &   \\
\hline 
\uproperty{ScalarHom} &  \\
\hline 
\uproperty{SubHom} &  \\
\hline 
\uproperty{QSubHom} &  \\
\hline 
\textbf{\uproperty{NSubHom}} &  \\
\hline 
\textbf{\usproperty{NSubDiv}} &  \\
\hline 
\uproperty{SuperHom} &  \\
\hline 
\uproperty{SubMulti} &  \\
\hline 
\uproperty{SuperMulti} &  \\
\hline 
\uproperty{SubRestrict} &  \\
\hline 
\uproperty{SuperRestrict} &  \\
\hline 
\uproperty{Additive} &  \\
\hline 
\uproperty{Summation} &  \\
\hline 
\uproperty{Maximum} &  \\
\hline 
\uproperty{MaximumSum} &  \\
\hline 
\textbf{\uproperty{SubComp}} &  \\
\hline 
\usproperty{ISuperComp} &  \quad ( injective) \\
\hline 
\uproperty{Extend} &  \\
\hline 
\uproperty{SubsetSum} &  \\
{} &  \\
\hline 
\end{tabular}
\centering
\caption{Desirable properties for an -notation. Here , , , , , , , , , and  is a finite cover of . Primitive properties marked with a bold face.}
\label{TableOfDesirableProperties}
\end{table}

\begin{note}[A computational model for examples]
\label{ModelForExamples}
When analyzing the cost functions of the example-algorithms in this section, addition costs one unit, while all other operations cost nothing.
\end{note}

\section{Dominance properties}

The \define{dominance properties} are those which mirror the desire for  to represent a down\-/set of a preorder, where the preorder is consistent with the partial order  in . 

\begin{definition}[\uproperty{Order}]
 has \defineproperty{Order}, if 
 
for all . 
\end{definition}

\begin{example}
Let  and . By \require{Order}, .
\end{example}

\begin{example}[Powers after ]
Suppose . Let  be such that . Then 

for all . By \require{Order},

\end{example}

\begin{example}[Powers before ]
Suppose . Let  be such that . Then 

for all . By \require{Order},

\end{example}

\begin{example}[Positive power dominates a logarithm]
\label{LogGrowsSlowly}
Suppose . Let  and . It can be shown that

for all . By \require{Order},

\end{example}

\begin{example}[Positive power dominates a logarithm, generalized]
\label{LogGrowsSlowlyGeneralized}
Let , , , and  be such that . Then

By \require{Order}, 

Since this holds for all ,

\end{example}

\begin{example}[Functions on a finite set]
\label{FunctionsInFiniteSet}
Let  be finite, and . Then . By \require{Order},
\begin{eqs}
f \in \ohx{\max(\image{f}{X})}.
\end{eqs}
\end{example}

\begin{definition}[\uproperty{Reflex}]
 has \defineproperty{Reflex}, if 
 
for all . 
\end{definition}

\begin{example}
.
\end{example}

\begin{definition}[\uproperty{Trans}]
 has \defineproperty{Trans}, if

for all .
\end{definition}

\begin{example}
Let , and . By \require{Trans}, .
\end{example}

\begin{definition}[\uproperty{Orderness}]
 has \defineproperty{Orderness}, if

for all . 
\end{definition}

\begin{example}
.
\end{example}

\section{Non-triviality properties}

The \emph{non\-/triviality properties} are those which require that the -notation be detailed enough. 

\begin{example}
The class of functions , such that , for all , satisfies all of the desirable properties for an -notation except those of non\-/triviality.
\end{example}

\begin{definition}[\uproperty{Zero}]
 has \defineproperty{Zero}, if

\end{definition}

\begin{definition}[\uproperty{One}]
 has \defineproperty{One}, if

\end{definition}

\begin{definition}[\uproperty{TrivialZero}]
 has \defineproperty{TrivialZero}, if

\end{definition}

\section{Abstraction properties}

The \emph{abstraction properties} are those which define the way in which the -notation identifies functions. 

\begin{definition}[\uproperty{Scale}]
 has \defineproperty{Scale}, if

for all , and .
\end{definition}

\begin{example}
.
\end{example}

\begin{example}[Power dominates a logarithm, continued]
Let , , , and  be such that . Continuing \eref{LogGrowsSlowlyGeneralized}, by \require{Scale},

\end{example}

\begin{example}[Functions on a finite set, continued]
Let  be finite, and . By \eref{FunctionsInFiniteSet}, . By \require{Scale}, , if , and , if .
\end{example}

\begin{definition}[\uproperty{Translation}]
 has \defineproperty{Translation}, if

for all , and .
\end{definition}

\begin{example}
.
\end{example}

\begin{note}[The role of  in \property{Translation}]
Suppose  is such that that . Then  protects against transforming a zero cost to a non-zero cost.

Suppose  is such that  and  --- e.g.,  such that . Then it is possible to call the corresponding algorithm  arbitrary number of times --- with different inputs --- while spending a bounded amount of resources. The  protects against transforming a bounded cost function into an unbounded one.
\end{note}

\begin{example}
Suppose , and . Then the following do \emph{not} follow from \property{Translation}: , , and .
\end{example}

\begin{definition}[\uproperty{PowerH}]
 has \defineproperty{PowerH}, if

for all , and .
\end{definition}

\begin{example}
.
\end{example}

\begin{definition}[\uproperty{AddCons}]
 has \defineproperty{AddCons}, if

for all .
\end{definition}

\begin{example}
.
\end{example}

\begin{definition}[\uproperty{MultiCons}]
 has \defineproperty{MultiCons}, if

for all .
\end{definition}

\begin{example}
.
\end{example}

\begin{definition}[\uproperty{MaxCons}]
 has \defineproperty{MaxCons}, if

for all .
\end{definition}

\section{Structural properties}

The \emph{structural properties} are those which mirror the structure of algorithms: repetition, conditional branching, and abstraction.

\begin{definition}[\uproperty{Local}]
 has \defineproperty{Local}, if

for all , , and  a finite cover of .
\end{definition}

\begin{definition}[\uproperty{ScalarHom}]
 has \defineproperty{ScalarHom}, if

for all , and .
\end{definition}

\begin{example}
.
\end{example}

\begin{definition}[\uproperty{SubHom}]
 has \defineproperty{SubHom}, if

for all .
\end{definition}

\begin{definition}[\uproperty{SuperHom}]
 has \defineproperty{SuperHom}, if

for all .
\end{definition}

\begin{definition}[\uproperty{Hom}]
 has \defineproperty{Hom}, if it has \property{SubHom} and \property{SuperHom}.
\end{definition}

\begin{definition}[\uproperty{SubMulti}]
 has \defineproperty{SubMulti}, if

for all .
\end{definition}

\begin{definition}[\uproperty{SuperMulti}]
 has \defineproperty{SuperMulti}, if

for all .
\end{definition}

\begin{definition}[\uproperty{Multi}]
 has \defineproperty{Multi}, if it has \property{SubMulti} and \property{SuperMulti}.
\end{definition}

\begin{algorithm}
\caption{An algorithm to demonstrate \property{Multi}.}
\label{alg:multiply}
\begin{algorithmic}[1]
\Procedure {H}{}
\For{}
  \State \Call{F}{}
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{example}[Demonstration of \property{Multi}]
Consider \aref{alg:multiply}, . This algorithm runs the same sub-algorithm, , repeatedly  times, where the algorithm  has zero cost (i.e. does not perform additions). Let the cost functions of  and  be , respectively. Then 
\begin{eqs}
h = fG.
\end{eqs}
By \require{Multi},
\begin{eqs}
\ohx{h} & = \ohx{fG} \\
{} & = \ohx{f} \cdot \ohx{G}.
\end{eqs}
That is, if we have analyzed  to the have complexity , and know , then the complexity of  is given by .
\end{example}

\begin{example}
.
\end{example}

\begin{example}
.
\end{example}

\begin{example}
Suppose . Then .
\end{example}

\begin{definition}[\uproperty{SubRestrict}]
 has \defineproperty{SubRestrict}, if

for all , and .
\end{definition}

\begin{definition}[\uproperty{SuperRestrict}]
 has \defineproperty{SuperRestrict}, if

for all , and .
\end{definition}

\begin{definition}[\uproperty{Restrict}]
 has \defineproperty{Restrict}, if it has \property{SubRestrict} and \property{SuperRestrict}.
\end{definition}

\begin{algorithm}
\caption{An algorithm to demonstrate \property{Restrict}.}
\label{alg:restriction}
\begin{algorithmic}[1]
\Procedure {H}{}
\State \Return \Call{F}{}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{example}[Demonstration of \property{Restrict}]
Let , , and  be a set. Consider \aref{alg:restriction}, . This algorithm passes its argument --- as it is --- to the algorithm . Let  and  be the cost functions of algorithms  and , respectively. Then
\begin{eqs}
h = \restrb{f}{D}.
\end{eqs}
By \require{Restrict},
\begin{eqs}
\oh{D}{h} = \restrb{\ohx{f}}{D}.
\end{eqs}
That is, if we have analyzed  to have the complexity , then the complexity of  is given by .
\end{example}

\begin{example}

\end{example}

\begin{example}
Let . Then .
\end{example}

\begin{example}

\end{example}

\begin{example}
Suppose , and . By \require{SubRestrict}, . By \require{Order}, . By \require{Trans},  
\end{example}

\begin{definition}[\uproperty{Additive}]
 has \defineproperty{Additive}, if

for all .
\end{definition}

\begin{algorithm}
\caption{An algorithm to demonstrate \property{Additive}.}
\label{alg:sequence}
\begin{algorithmic}[1]
\Procedure {H}{}
\State \Return \Call{F}{}, \Call{G}{}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{example}[Demonstration of \property{Additive}]
Consider \aref{alg:sequence}, , which decomposes into two sub-algorithms , and , such that . Suppose the cost functions of , , and  are , respectively. Then 
\begin{eqs}
h = f + g.
\end{eqs}
By \require{Additive},
\begin{eqs}
\ohx{h} & = \ohx{f + g} \\
{} & = \ohx{f} + \ohx{g}.
\end{eqs}
That is, if we have analyzed  and  to have complexities  and , respectively, then the complexity of  is given by .
\end{example}

\begin{example}
.
\end{example}

\begin{example}
Let , and . By \require{Order}, and \require{Additive}, 
\begin{eqs}
f + \ohx{g} & \subset \ohx{f} + \ohx{g} \\
{} & = \ohx{f + g}.
\end{eqs}
\end{example}

\begin{definition}[\uproperty{Summation}]
 has \defineproperty{Summation}, if

for all .
\end{definition}

\begin{note}
\uproperty{Summation} states that, in a finite sequence of algorithm calls, the worst cost function over all calls determines the complexity of the call-sequence.
\end{note}

\begin{example}
.
\end{example}

\begin{example}
Suppose . Then
\begin{eqs}
\oh{\nonnb{\TR}{1}}{x + x^2} & = \oh{\nonnb{\TR}{1}}{\max(x, x^2)} \\
{} & = \oh{\nonnb{\TR}{1}}{x^2}.
\end{eqs}
\end{example}

\begin{example}
Suppose . It does not follow from \property{Summation} that .
\end{example}

\begin{definition}[\uproperty{Maximum}]
 has \defineproperty{Maximum}, if

for all .
\end{definition}

\begin{example}
.
\end{example}

\begin{definition}[\uproperty{MaximumSum}]
 has \defineproperty{MaximumSum}, if

for all .
\end{definition}

\begin{example}
.
\end{example}

\begin{definition}[\uproperty{SubComp}]
 has \defineproperty{SubComp}, if

for all  and .
\end{definition}

\begin{definition}[\uproperty{ISubComp}]
 has \defineproperty{ISubComp}, if it has \property{SubComp} for all injective .
\end{definition}

\begin{definition}[\uproperty{ISuperComp}]
 has \defineproperty{ISuperComp}, if

for all  and injective .
\end{definition}

\begin{note}[\uproperty{SuperComp} does not make sense without injectivity]
Let  be a positive constant;  for all , and some . Then , and . Let . Then . 
\end{note}

\begin{algorithm}
\caption{An algorithm to demonstrate \property{IComp}. We assume that  does not consume any resources.}
\label{alg:DependentSequence}
\begin{algorithmic}[1]
\Procedure {}{}
\State \Return \Call{G}{}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{example}[Demonstration of \property{IComp}]
\label{CompositionExample}
Consider \aref{alg:DependentSequence}, , which decomposes into two sub-algorithms  and  such that  is injective and . Denote the cost functions of ,  and  by  and , respectively. Suppose . Then
\begin{eqs}
h & = g \circ F + f \\
{} & = g \circ F.
\end{eqs}
By \require{IComp},
\begin{eqs}
\ohx{h} & = \ohx{g \circ F} \\
{} & = \oh{Y}{g} \circ F.
\end{eqs}
That is, if , and we know , then the complexity of  is given by . 
\end{example}

\begin{definition}[\uproperty{Extend}]
 has \defineproperty{Extend}, if

for all  and .
\end{definition}

\begin{note}
\uproperty{Extend} is a special case of \property{SubComp}.
\end{note}

\begin{definition}[\uproperty{SubsetSum}]
 has \defineproperty{SubsetSum}, if

for all , , , , and .
\end{definition}

\begin{algorithm}
\caption{An algorithm to demonstrate \property{SubsetSum}.}
\label{alg:SubsetSum}
\begin{algorithmic}[1]
\Procedure {}{}
\State 
\For {}
\State 
\EndFor
\State \Return 
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{example}[Demonstration of \property{SubsetSum}]
\label{SubsetSumExample}
Consider \aref{alg:SubsetSum}, , which for  sums  over . Let the algorithms  and  take  and  operations, respectively. Then
\begin{eqs}
g(x) = \sum_{y \in S_x} f(y).
\end{eqs}
Suppose we have shown that . By \require{SubsetSum},
\begin{eqs}
\ohx{g} & = \ohx{\sum_{y \in S_x} f(y)} \\
{} & \subset \ohx{\sum_{y \in S_x} \bar{f}(y)}.
\end{eqs}
That is, if  has an upper bound of , then we can construct an upper bound of  from . Suppose we have also shown that  --- i.e. . Then
\begin{eqs}
\ohx{g} = \ohx{\sum_{y \in S_x} \bar{f}(y)}.
\end{eqs}
\end{example}

\begin{note}[Factor  in \property{SubsetSum}]
\eref{SubsetSumExample} does not make use of the factor  in \property{SubsetSum}. Such general sums occur naturally in the analysis of recursive algorithms; see \sref{MasterTheorems}.
\end{note}

\chapter{Characterization of the \texorpdfstring{}{O}-notation}
\label{Characterization}

In this chapter we show that the primitive properties are equivalent to the definition of -notation as linear dominance.

\section{Linear dominance is sufficient}
\label{SufficientDefinition}

In this section we will show the following theorem:

\begin{theorem}[Linear dominance has primitive properties]
\label{LinearImpliesPrimitive}
Let  be defined by  if and only if

for all , and all , where the universe  is the class of all sets. Then  satisfies the primitive properties.
\end{theorem}

\begin{proof}
The result follows directly from the Lemmas in this section.
\end{proof}

We shall apply the following lemma repeatedly without mentioning it.

\begin{lemma}[Simplification lemma]
\label{LinearSingleConstantLemma}
Let ,  be a finite set, , , and , for all . Then there exists , such that , for all .
\end{lemma}

\begin{proof}
Since , there exists , such that , for all . Let . Then , for all .
\end{proof}

\begin{lemma}[Linear dominance has \property{Order}]
\label{LinearOrderConsistency}
Let , and . Then \sprove{Order}

\end{lemma}

\begin{proof}
Since , it holds that .
\sprove{Order}
\end{proof}

\begin{lemma}[Linear dominance has \property{Trans}]
\label{LinearTransitivity}
Let , and . Then \sprove{Trans}

\end{lemma}

\begin{proof}
Let , and . Then there exists , such that  and . It follows that . Therefore .
\sprove{Trans}
\end{proof}

\begin{lemma}[Linear dominance has \property{Local}]
\label{LinearLocality}
Let , , and  be a finite cover of . Then \sprove{Local}

\end{lemma}

\begin{proof}
Assume , for all . Then there exist  such that , for all . Since  covers , . Therefore .
\sprove{Local}
\end{proof}

\begin{lemma}[Linear dominance has \property{One}]
\label{LinearOneSeparation}
 \sprove{One}
\end{lemma}

\begin{proof}
For all , there exists  --- for example  --- such that . Therefore . \sprove{One}
\end{proof}

\begin{lemma}[Linear dominance has \property{Scale}]
\label{LinearScaleInvariance}
Let , , and . Then . \sprove{Scale}
\end{lemma}

\begin{proof}
Assume . Then there exists , such that
\begin{eqs}
\hat{f} & \lt c f \\
{} & = (c / \alpha) \bra{\alpha f}.
\end{eqs}
Therefore . 
\sprove{Scale}
\end{proof}

\begin{lemma}[Linear dominance has \property{NSubHom} and \property{NSubDiv}]
\label{LinearSubHomogenuity}
Let , and . Then \sprove{NSubHom} \sprove{NSubDiv}
 
\end{lemma}

\begin{proof}
Let . Then there exists , such that . This implies . Therefore ;  has \prove{SubHom}. Since ,  has \prove{NSubHom}. Since ,  has \prove{NSubDiv}.
\end{proof}

\begin{lemma}[Linear dominance has \property{SubComp}]
\label{LinearSubComposability}
Let , , and . Then \sprove{SubComp}

\end{lemma}

\begin{proof}
Let . Then there exists , such that . This implies . Therefore .
\sprove{SubComp}
\end{proof}

\section{Linear dominance is necessary}
\label{NecessaryDefinition}

In this section we will show the following theorem.

\begin{theorem}[Primitive properties imply linear dominance]
Suppose  has \require{Order}, \require{Trans}, \require{One}, \require{Local}, \require{Scale},  \require{NSubHom}, \require{NSubDiv}, and \require{SubComp}. Then

\end{theorem}

To prove this result, we will use some of the results from \sref{ImpliedProperties}.

\begin{lemma}[ equals the bounded functions]
\label{OhOneIsExactlyBounded}
Suppose  has \require{Order}, \require{Trans}, \require{One}, \require{Local}, \require{Scale}, and \require{ISubComp}. Then

provided .
\sprove{OhForOne}
\end{lemma}

\begin{proof}
\proofpart{Implied properties}
 has \property{ISuperComp} by \proveby{InjectiveSuperComposabilityIsImplied}, and \property{Orderness} by \proveby{OrdernessIsImplied}.

\proofpart{}
Assume  such that  is unbounded. Then for every  there exists  such that . Therefore, let  be injective such that , for all . By \require{Orderness}, . By \require{Order}, \require{Order} and \require{Trans}, \require{ISuperComp}, and \require{ISubComp},
\begin{eqs}
(n \mapsto n) & \in \oh{\posi{\TN}}{n}\\
{} & \subset \oh{\posi{\TN}}{f \circ s} \\
{} & \subset \ohx{f} \circ s \\
{} & \subset \ohx{1} \circ s \\
{} & \subset \oh{\posi{\TN}}{1 \circ s} \\
{} & = \oh{\posi{\TN}}{1}.
\end{eqs}
This contradicts  having \require{One}. Therefore  is bounded, which is equivalent to .

\proofpart{} 
Assume . By \require{Order}, . By \require{Scale}, .

\sprove{OhForOne}
\end{proof}

\begin{lemma}[-notation for positive functions]
\label{OhForPositive}
Suppose  has \require{Order}, \require{Trans}, \require{Scale}, \require{NSubHom}, and \require{NSubDiv}. Then

for all  such that .
\sprove{OhForPositive}
\end{lemma}

\begin{proof}
 has \property{SubHom} by \proveby{RSubhomogenuityIsImplied}.

\proofpart{}
By \require{SubHom},
\begin{eqs}
{} & f \in \ohx{g} \\
\impliesr & f / g \in \ohx{g} / g \\
\impliesr & f / g \in \ohx{g / g} \\
\impliesr & f / g \in \ohx{1}.
\end{eqs}
\proofpart{}
By \require{SubHom},
\begin{eqs}
{} & f / g \in \ohx{1} \\
\impliesr & f \in \ohx{1} g \\
\impliesr & f \in \ohx{g}.
\end{eqs}
\sprove{OhForPositive}
\end{proof}

\begin{theorem}[Primitive properties imply linear dominance]
\label{PrimitiveImpliesLinear}
Suppose  has \require{Order}, \require{Trans}, \require{One}, \require{Local}, \require{Scale}, \require{NSubHom}, \require{NSubDiv}, and \require{SubComp}. Then

\end{theorem}

\begin{proof}
\proofpart{Implied properties}
\srequire{SubComp} \sprove{ISubComp}
 has \property{SubRestrict} by \proveby{SubRestrictabilityIsImplied}, and \property{TrivialZero} by \proveby{ZeroTrivialityIsImplied}. 

\proofpart{Positive subset}
Let  and . 
Then
\begin{eqs}
{} & \restrb{f}{G} \in \oh{G}{\restr{g}{G}} \\
\iffr & \frac{\restrb{f}{G}}{\restrb{g}{G}} \in \oh{G}{1} \\
\iffr & \exists c \in \posi{\TR}: \frac{\restrb{f}{G}}{\restrb{g}{G}} \leq c \\
\iffr & \exists c \in \posi{\TR}: \restrb{f}{G} \leq c \restrb{g}{G} \\
\end{eqs}
where we used \proveby{OhForPositive} and \proveby{OhOneIsExactlyBounded}. \srequire{OhForPositive} \srequire{OhForOne} 

\proofpart{Zero subset}
By \require{TrivialZero},
\begin{eqs}
{} & \restrb{f}{\compl{G}} \in \oh{\compl{G}}{\restr{g}{\compl{G}}} \\
\iffr & \restrb{f}{\compl{G}} = 0.
\end{eqs}
 
\proofpart{Whole set}
By \require{Local} and \require{SubRestrict},
\begin{eqs}
{} & f \in \ohx{g} \\
\iffr & \restrb{f}{G} \in \oh{G}{\restr{g}{G}} \textrm{ and } \restrb{f}{\compl{G}} \in \oh{\compl{G}}{\restr{g}{\compl{G}}} \\
\iffr & \bra{\exists c \in \posi{\TR}: \restrb{f}{G} \leq c \restrb{g}{G}} \textrm{ and } \restrb{f}{\compl{G}} = 0 \\
\iffr & \exists c \in \posi{\TR}: f \leq c g.
\end{eqs}
\end{proof}

\section{Minimal properties}

In this section we will show that, excluding \property{Local}, a given primitive property can not be deduced from the remaining primitive properties.

\begin{definition}[Minimal set of axioms]
A set  of axioms is \define{minimal}, if no axiom in  can be proved from the other axioms in .
\end{definition}

\begin{note}[Proving minimality]
\label{ProvingMinimality}
The minimality of an axiom set  can be proved by showing that for any axiom , there is a model of  in which  holds and another model of  in which  does not hold.
\end{note}

\begin{note}[\uproperty{Local} is implied]
\uproperty{Local} is implied by the other primitive properties by \proveby{LocalityIsImplied}; primitive properties are not minimal.
\end{note}

\begin{note}[Pre-primitive properties]
The \define{pre-primitive properties} are the primitive properties, with \property{Local} excluded.
\end{note}

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|l|}
\hline 
{} &  & T &  &  &  &  &  \\
\hline 
\hline 
 & \xmark \ref{MultipleOrderFails} & \checkmark \ref{MultipleTransivity} & \checkmark \ref{MultipleOneSeparation} & \checkmark \ref{MultipleScaleInvariance} & \checkmark \ref{MultipleSubHomogeneity} & \checkmark \ref{MultipleSubHomogeneity} & \checkmark \ref{MultipleSubComposability} \\
\hline
 & \checkmark \ref{NonTransitiveOrder} & \xmark \ref{NonTransitiveTransivityFails} & \checkmark \ref{NonTransitiveOneSeparation} & \checkmark \ref{NonTransitiveScaleInvariance} & \checkmark \ref{NonTransitiveSubHomogeneity} & \checkmark \ref{NonTransitiveSubHomogeneity} & \checkmark \ref{NonTransitiveSubComposability} \\
\hline
 & \checkmark \ref{LocalOrderConsistency} & \checkmark \ref{LocalTransitivity} & \xmark \ref{TrivialOneSeparationFails} & \checkmark \ref{LocalScaleInvariance} & \checkmark \ref{LocalSubHomogenuity} & \checkmark \ref{LocalSubHomogenuity} & \checkmark \ref{TrivialSubComposability} \\
\hline
 & \checkmark \ref{ElementwiseOrder} & \checkmark \ref{ElementwiseTransivity} & \checkmark \ref{ElementwiseOneSeparation} & \xmark \ref{ElementwiseScaleInvarianceFails} & \checkmark \ref{ElementwiseSubHomogeneity} & \checkmark \ref{ElementwiseSubHomogeneity} & \checkmark \ref{ElementwiseSubComposability} \\
\hline
 & \checkmark \ref{AffineOrderConsistency} & \checkmark \ref{AffineTransitivity} & \checkmark \ref{AffineOneSeparation} & \checkmark \ref{AffinePositiveScaleInvariance} & \xmark \ref{AffineSubHomogenuityFails} & \checkmark \ref{AffineSubHomogeneityNDiv} & \checkmark \ref{AffineSubComposability} \\
\hline
 & \checkmark \ref{PowerOrder} & \checkmark \ref{PowerTransivity} & \checkmark \ref{PowerOneSeparation} & \checkmark \ref{PowerScaleInvariance} & \checkmark \ref{PowerSubHomogeneity} & \xmark \ref{PowerSubHomogeneityDivNFails} & \checkmark \ref{PowerSubComposability} \\
\hline
 & \checkmark \ref{LocalOrderConsistency} & \checkmark \ref{LocalTransitivity} & \checkmark \ref{AsymptoticOneSeparation} & \checkmark \ref{LocalScaleInvariance} & \checkmark \ref{LocalSubHomogenuity} & \checkmark \ref{LocalSubHomogenuity} & \xmark \ref{AsymptoticInjectiveSubComposabilityFails} \\
\hline
 & \checkmark \ref{LocalOrderConsistency} & \checkmark \ref{LocalTransitivity} & \checkmark \ref{LinearOneSeparation} & \checkmark \ref{LocalScaleInvariance} & \checkmark \ref{LocalSubHomogenuity} & \checkmark \ref{LocalSubHomogenuity} & \checkmark \ref{LinearSubComposability} \\
\hline 
\end{tabular}
\centering
\caption{The pre-primitive properties fulfilled by each candidate definition. The abbreviations are:  for \property{Order}, T for \property{Trans},  for \property{One},  for \property{Scale},  for \property{NSubHom},  for \property{NSubDiv}, and  for \property{SubComp}}
\label{MinimalProperties}
\end{table}

\begin{theorem}[Pre-primitive properties are minimal]
\label{PreprimitivePropertiesAreMinimal}
Pre-primitive properties form a minimal set of axioms for linear dominance. \srequire{Order} \srequire{Trans} \srequire{One} \srequire{Scale} \srequire{NSubHom} \srequire{NSubDiv} \srequire{SubComp}
\end{theorem}

\begin{proof}
That pre-primitive properties are equivalent to the definition of -notation as linear dominance follows from \proveby{LocalityIsImplied}, \proveby{PrimitiveImpliesLinear}, and \proveby{LinearImpliesPrimitive}.

That pre-primitive properties are minimal follows from \tref{MinimalProperties}, following \nref{ProvingMinimality}.
\end{proof}

\chapter{Working with the \texorpdfstring{}{O}-notation}
\label{WorkingWith}

In this section we adopt the linear dominance -notation as \emph{the} -notation and develop more refined tools for working with it. These tools are useful in a day-to-day basis for an algorithm analyst, because they provide shortcuts over tedious derivations. A cheat sheet for working with the -notation is given in \fref{CheatSheet} --- it is a simplified version of \fref{DesirableProperties}.

\begin{table}
\begin{tabular}{|l|l|}
\hline 
Name & Property \\
\hline 
\hline 
\uproperty{Order} &   \\
\hline 
\uproperty{Reflex} &  \\
\hline 
\uproperty{Trans} &   \\
\hline 
\uproperty{Orderness} &  \\
\hline 
\hline 
\uproperty{Zero} &  \\
\hline 
\uproperty{One} &  \\
\hline 
\uproperty{TrivialZero} &  \\
\hline 
\hline 
\uproperty{Scale} &  \\
\hline 
\usproperty{Translation} &   \\
\hline 
\uproperty{PowerH} &  \\
\hline 
\uproperty{AddCons} &  \\
\hline 
\uproperty{MultiCons} &  \\
\hline 
\uproperty{MaxCons} &  \\
\hline 
\uproperty{Local} &   \\
\hline 
\uproperty{Hom} &  \\
\hline 
\uproperty{Multi} &  \\
\hline 
\uproperty{Restrict} &  \\
\hline 
\uproperty{Additive} &  \\
\hline 
\uproperty{Summation} &  \\
\hline 
\uproperty{Maximum} &  \\
\hline 
\uproperty{MaximumSum} &  \\
\hline 
\uproperty{SubComp} &  \\
\hline 
\uproperty{IComp} &  \quad ( injective) \\
\hline 
\uproperty{Extend} &  \\
\hline 
\uproperty{SubsetSum} &  \\
{} &  \\
\hline 
\end{tabular}
\centering
\caption{Cheat sheet for -notation. Here , , , , , , , , , and  is a finite cover of .}
\label{CheatSheet}
\end{table}

\section{Surprising aspects}
\label{Faq}

In this section we will look at some aspects of \-/notation which at first may seem surprising.

\begin{algorithm}
\caption{A family of algorithms, parametrized by , which take as input , and output .}
\label{alg:InfiniteDescent}
\begin{algorithmic}[1]
\Procedure {}{}
\If {}
\State \Return 
\EndIf
\State \Return 
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{example}[Infinite descent]
\label{InfiniteDescent}
Consider a family of algorithms in \aref{alg:InfiniteDescent}, for which there is a separate implementation for each . Suppose we are only interested in the number of performed additions. The cost functions are given by  such that
\begin{eqs}
f_i(n) =
\begin{cases}
1, & n \geq i, \\
0, & \text{otherwise},
\end{cases}
\end{eqs}
where . Then
\begin{eqs}
f_0 \in \lthetah{\TN}{1},
\end{eqs}
and
\begin{eqs}
f_{i + 1} \in \lsmalloh{\TN}{f_i},
\end{eqs}
for all . That is, with linear dominance, the functions  form a decreasing sequence of functions. This is how it should be: it is fundamentally different to use resources --- no matter how small an amount --- than to not use resources at all. 
\end{example}

\begin{example}[Same expressions, different functions]
It may be surprising that , but . The function  seems to be the same --- why don't they belong to the same -sets?

A function is a triple, which consists of the domain, the codomain, and the rule connecting each element of the domain to exactly one element of the codomain. An expression such as  is ambiguous as a function definition, because it does not specify the domain and the codomain. The two occurrences of  here are different functions, and need not share any other property apart from being equal on .

In our example,  on  is bounded, while  on  is unbounded.
\end{example}

\section{Master theorems}

Master theorems are popular for solving recurrence equations arising in the analysis of divide-and-conquer algorithms \cite{IntroAlgo2009} --- up to -equivalence. We state the theorems here, and prove them in \sref{MasterTheorems}.

\newcommand{\ReMasterFunctionOverIntegers}{
\begin{definition}[Master function over integers]
Let , , , and . 
A \definesub{master function}{over integers} is a function  defined by the recurrence equation

The set of such functions is denoted by .
\end{definition}
}

\ReMasterFunctionOverIntegers

\newcommand{\ReMasterTheoremOverIntegers}[1][]{
\begin{theorem}[Master theorem over integers]
\label{MasterTheoremOverIntegers#1}
\srequire{LinearDominance}
Let  be a Master function over integers, and , where . Then
\begin{eqs}
\lgb{b}{a} < c & \implies T \in \loh{\nonnb{\TN}{1}}{n^c}, \\
\lgb{b}{a} = c & \implies T \in \loh{\nonnb{\TN}{1}}{n^c \lgb{b}{bn}}, \\
\lgb{b}{a} > c & \implies T \in \lthetah{\nonnb{\TN}{1}}{n^{\lgb{b}{a}}}.
\end{eqs}
If , then each  can be replaced with .
\end{theorem}
}

\ReMasterTheoremOverIntegers[Text]

\newcommand{\ReMasterFunctionOverReals}{
\begin{definition}[Master function over reals]
Let , , , and . A \definesub{master function}{over reals} is a function  defined by the recurrence equation

The set of such functions is denoted by .
\end{definition}
}

\ReMasterFunctionOverReals

\newcommand{\ReMasterTheoremOverReals}[1][]{
\begin{theorem}[Master theorem over reals]
\label{MasterTheoremOverReals#1}
\srequire{LinearDominance}
Let  be a Master function over reals, and , where . Then
\begin{eqs}
\lgb{b}{a} < c & \implies t \in \loh{\nonnb{\TR}{1}}{x^c}, \\
\lgb{b}{a} = c & \implies t \in \loh{\nonnb{\TR}{1}}{x^c \lgb{b}{bx}}, \\
\lgb{b}{a} > c & \implies t \in \lthetah{\nonnb{\TR}{1}}{x^{\lgb{b}{a}}}.
\end{eqs}
If , then each  can be replaced with .
\end{theorem}
}

\ReMasterTheoremOverReals[Text]

\section{\texorpdfstring{}{O}-mappings}
\label{OMappings}

\begin{definition}[-mapping]
A function  is an \define{-mapping}, if
\begin{eqs}
\image{T}{\lohx{f}} \subset \loh{Y}{T(f)},
\end{eqs}
for all . 
\end{definition}

\begin{theorem}[-mapping by linear dominance]
\label{LinearDominanceOMapping}
Let . Then  is an -mapping if and only if

for all .
\end{theorem}

\begin{proof}
By definition.
\end{proof}

\begin{example}[Non-negative translation is an -mapping]
Let  be such that , where . Let  be such that . Then there exists  such that , and
\begin{eqs}
T(f) & = f + \alpha \\
{} & \leq cg + \alpha \\
{} & \leq \max(c, 1) (g + \alpha) \\
{} & = \max(c, 1) T(g).
\end{eqs}
Therefore  is an -mapping. The inverse of  does not exist, since it does not always hold that .
\end{example}

\begin{example}[Composition is an -mapping]
Let  be such that , where . Let  be such that . Then there exists  such that , and
\begin{eqs}
T(f) & = f \circ s \\
{} & \leq (cg) \circ s \\
{} & = c (g \circ s) \\
{} & = c T(g).
\end{eqs}
Therefore  is an -mapping.
\end{example}

\begin{theorem}[Subset-sum is an -mapping]
\label{SubsetSumIsAnOMapping}
Let , , , , and . Let  be such that

Then  is an -mapping. \srequire{LinearDominance} \sprove{SubsetSum}
\end{theorem}

\begin{proof}
\srequire{LinearDominance}
There exists  such that , and so
\begin{eqs}
T(\hat{f}) & = \bra{x \mapsto \sum_{(y, z) \in S_x} a(z) \hat{f}(y)} \\
{} & \leq \bra{x \mapsto \sum_{(y, z) \in S_x} a(z) (cf)(y)} \\
{} & = c \bra{x \mapsto \sum_{(y, z) \in S_x} a(z) f(y)} \\
{} & = c T(f).
\end{eqs}
Therefore  is an -mapping. \sprove{SubsetSum}
\end{proof}

\section{\texorpdfstring{}{O}-equalities}



\begin{definition}[-residual]
A function  is an \defineexp{-residual}{-residual} of , if

for all , and . 
\end{definition}

\begin{definition}[-residuated function]
A function  is \defineexp{-residuated}{-residual}, if it has an -residual.
\end{definition}

\begin{note}[]
The definitions given here are special cases of the theory of partitioned sets --- given in \sref{PartitionedSets} --- and of the theory of preordered sets --- given in \sref{PreorderedSets}, formulated in terms of -sets.
\end{note}

\begin{definition}[Strong -equality]
A \define{strong -equality} is a surjective -residuated function . 
\end{definition}

\begin{theorem}[Strong -equality rule]
\label{StrongOEqualityRule}
Let  be a strong -equality. Then

for all .
\end{theorem}

\begin{proof}
This is proved in \thref{TransposeResiduatedSurjectivePreservesDownSets}.
\end{proof}

\begin{note}[-equality]
We call the -equality strong, because we do not know how to neatly characterize the preservation of the -set under a mapping ; such functions would be called -equalities.
\end{note}

\begin{theorem}[Strong -equality by linear dominance]
\label{StrongOEqualityByLinearDominance}
Suppose . Then  is a strong -equality if and only if there exists  such that
\begin{eqs}
T(\residual{T}{g}) = g,
\end{eqs}
and

for all , and .
\end{theorem}

\begin{proof}
By definition of linear dominance and \thref{PSurjectivityIsEquivalentToHavingRightInverse}, whose special case says that surjectivity is equivalent to having a right-inverse.
\end{proof}

\begin{example}[Injective composition is a strong -equality]
Let  be injective, and  be such that . Let  be such that . Then  is bijective, and . Let  be such that , where  is a domain extension of a function from  to  by mapping the new elements to zeros. Let , , and . Then , and 
\begin{eqs}
{} & g \lt c T(f) \\
\iffr & g \lt c (f \circ \residuals{s}) \\
\iffr & \left\langle g \circ \invs{\residuals{s}} \right\rangle \lt c f \\
\iffr & \residual{T}{g} \lt c f.
\end{eqs}
Therefore  is a strong -equality.
\end{example}

\begin{example}[Positive power is a strong -equality]
Let  be such that , where . Let  be such that . Let , and . Then , and
\begin{eqs}
{} & g \lt c T(f) \\
\iffr & g \lt c f^{\alpha} \\
\iffr & g^{1 / \alpha} \lt c^{1 / \alpha} f \\
\iffr & \residual{T}{g} \lt c^{1 / \alpha} f.
\end{eqs}
Therefore  is a strong -equality.
\end{example}

\begin{example}[Positive multiplication is a strong -equality]
Let  be such that , where . Let  be such that . Let , and . Then , and
\begin{eqs}
{} & g \lt c T(f) \\
\iffr & g \lt c \alpha f \\
\iffr & g / \alpha \lt c f \\
\iffr & \residual{T}{g} \lt c f.
\end{eqs}
Therefore  is a strong -equality.
\end{example}

\chapter{Local linear dominance}
\label{LocalLinearDominance}

\begin{table}
\begin{tabular}{|l|l|l|l|}
\hline 
Name & Universe &  & Reference \\
\hline 
\hline
 Trivial &
sets &
 &
\cite{ONotationBeatcs} \\
\hline 
 Cofinite & 
sets &
 & 
\cite{DesignAndAnalysisOfComputerAlgorithms}, \cite{ONotationBeatcs} \\
\hline 
 Asymptotic &
 &
 &
\cite{IntroAlgo} \\
\hline 
 Co-asymptotic &
 &
 &
\cite{IntroAlgo2009} \\
\hline 
 Full &
sets &
 & 
\cite{ONotationBeatcs} \\
\hline 
\end{tabular}
\centering
\caption{Example versions of local linear dominance. Here  is the universe, , and  is the filter basis in . The name is used to replace the word `local', as in asymptotic linear dominance.}
\label{ExamplesOfLocalLinearDominances}
\end{table}

In this section we will study a class of candidate definitions for the -notation, the \emph{local linear dominances}. These definitions work over all universes --- unless a specific version makes additional assumptions. 

\section{Definition}

In this section we provide the definition of local linear dominance. We prove its properties in \sref{ProofsForLocalLinearDominance}.

\begin{definition}[Filter basis]
A set  is a \define{filter basis} in a set  if it is
\begin{description}
\item[non-empty] \hfill \\ 
,
\item[-directed] \hfill \\
.
\end{description}
\end{definition}

\begin{note}[Filter basis may not be proper]
Some authors require a filter basis to be proper --- . We allow a filter basis to be non-proper.
\end{note}

\begin{definition}[Local linear dominance]
\defineexp{Local linear dominance}{linear dominance!local}  on  is defined by  if and only if

where  is a class of filter bases with \define{induced sub-structure}:
\begin{eqs}
\filterset{D} = \setb{A \cap D : A \in \filterset{X}},
\end{eqs}
for all .
\end{definition}

\begin{note}[Versions]
Each choice of filter bases corresponds to a version of local linear dominance. Some such versions are given in \taref{ExamplesOfLocalLinearDominances}.
\end{note}

\begin{note}[Filter basis and limits]
A filter basis in  is the minimal amount of structure needed to make sense of the limit of a function . In fact, a local linear dominance can be characterized by a ratio\-/limit.
\end{note}

\begin{note}[Motivation]
We have already shown in \sref{Characterization} that only one instance of local linear dominance works for algorithm analysis. Why study local linear dominances? There are two reasons.

First, local linear dominance is commonly used for local function approximation in various fields of mathematics. Such results may indirectly find their way into a complexity analysis, in which case we need a way transfer such results to the algorithmic side. This can be done using the tools provided in \sref{LimitTheorems}.

Second, local linear dominances help to train the intuition behind the primitive properties, because they provide successively better approximations to linear dominance. This is shown in \sref{ProofsForLocalLinearDominance}.
\end{note}

\begin{note}[Eventually non-negative]
A function  is \define{eventually non-negative}, if there exists , such that . It is possible to generalize the definition of local linear dominance to functions which are eventually non\-/negative. When , eventually non\-/negative reduces to non\-/negative.
\end{note}

\newcommand{\ReLinearDominanceFromLocalLinearDominance}[1][]{
\begin{theorem}[ is almost equal to  for cofinite filter sets]
\label{LinearDominanceFromLocalLinearDominance#1}
Suppose , for all . Then
\begin{eqs}
\rohx{g} = \lohx{g},
\end{eqs}
for all . 
\end{theorem}
}

\ReLinearDominanceFromLocalLinearDominance[Text]

\begin{example}[Linear dominance from asymptotic linear dominance]
\label{LinearDominanceFromAsymptoticLinearDominance}
Let  be such that . It holds that  by \eref{AsymptoticLinearDominanceFromALimitInN}. Since , and , for all , it holds that  by \thref{LinearDominanceFromLocalLinearDominance}.
\end{example}

\section{Limit theorems}
\label{LimitTheorems}

In this section we show how to transfer a result from a local linear dominance  to linear dominance . First, the filter bases associated with  make it possible to define the concepts of limit superior, limit inferior, and limit. These limits can then be used to characterize the -notation. Second,  can sometimes be used to deduce . The proofs are given in \sref{ProofsOfLimitsTheorems}.

\begin{note}[Related notations for local linear dominance]
We shall denote the related notations corresponding to  by , , , and .
\end{note}

\begin{definition}[Limit superior and limit inferior under a filter basis]
Let  be a filter basis in a set . Then
\begin{eqs}
\limsup_{\mathcal{F}} f & \coloneqq \inf \gset{\sup \image{f}{A}}{A \in \mathcal{F}}, \\
\liminf_{\mathcal{F}} f & \coloneqq \sup \gset{\inf \image{f}{A}}{A \in \mathcal{F}},
\end{eqs}
for all .
\end{definition}

\begin{note}[Existence of limit superior and limit inferior]
The  and  are called the \define{limit superior} and the \define{limit inferior}, respectively. By the completeness of , both of them are well-defined as a number in .
\end{note}

\begin{definition}[Limit under a filter basis]
The \define{limit} of  under a filter basis  is
\begin{eqs}
\lim_{\mathcal{F}} f = c,
\end{eqs}
whenever .
\end{definition}

\begin{note}[Division by zero and infinity]
In this section, we use the conventions that , for all , and that , for all .
\end{note}

\newcommand{\ReRelationBetweenRatioLimits}[1][]{
\begin{theorem}[Relation between ratio-limits]
\label{RelationBetweenRatioLimits#1}
\begin{eqs}
\limsup_{\filterset{F}} \frac{\restrb{f}{F}}{\restrb{g}{F}} = 1 / \liminf_{\filterset{F}} \frac{\restrb{g}{F}}{\restrb{f}{F}},
\end{eqs}
for all , where .
\end{theorem}
}

\ReRelationBetweenRatioLimits[Text]

\newcommand{\ReLocalLinearOByALimit}[1][]{
\begin{theorem}[ by a limit]
\label{LocalLinearOByALimit#1}
\begin{eqs}
\limsup_{\filterset{F}} \frac{\restrb{f}{F}}{\restrb{g}{F}} < \infty & \iff f \in \rohx{g},
\end{eqs}
for all , where .
\end{theorem}
}

\ReLocalLinearOByALimit[Text]

\begin{example}[Asymptotic linear dominance from a limit in ]
\label{AsymptoticLinearDominanceFromALimitInN}
Consider asymptotic linear dominance . Let  be such that . Since , and
\begin{eqs}
\limsup_{\filterset{\posi{\TN}}} \frac{f(n)}{n^2} = \limsup_{n \to \infty} \frac{f(n)}{n^2} = 1,
\end{eqs}
it holds that  by \thref{LocalLinearOByALimit}
\end{example}

\newcommand{\ReLocalLinearOmegaByALimit}[1][]{
\begin{theorem}[ by a limit]
\label{LocalLinearOmegaByALimit#1}
\begin{eqs}
\liminf_{\filterset{G}} \frac{\restrb{f}{G}}{\restrb{g}{G}} > 0 & \iff f \in \romegahx{g},
\end{eqs}
for all , where .
\end{theorem}
}

\ReLocalLinearOmegaByALimit[Text]

\newcommand{\ReLocalLinearSmallOhByALimit}[1][]{
\begin{theorem}[ by a limit]
\label{LocalLinearSmallOhByALimit#1}
\begin{eqs}
\bra{\limsup_{\filterset{F}} \frac{\restrb{f}{F}}{\restrb{g}{F}} < \infty \land \liminf_{\filterset{G}} \frac{\restrb{f}{G}}{\restrb{g}{G}} = 0} \iff f \in \rsmallohx{g},
\end{eqs}
for all , where  and .
\end{theorem}
}

\ReLocalLinearSmallOhByALimit[Text]

\begin{example}[Co-asymptotic linear dominance from a limit in ]
\label{CoasymptoticLinearDominanceFromALimitInPlane}
Consider co\-/asymptotic linear dominance . Let  be such that . Since , , and
\begin{eqs}
\limsup_{\filterset{\TN^2}} \frac{f(m, n)}{m + n + 1} & = 1, \\
\liminf_{\filterset{\TN^2}} \frac{f(m, n)}{m + n + 1} & = 0,
\end{eqs}
it holds that  by \thref{LocalLinearSmallOhByALimit}.
\end{example}

\begin{example}[Sufficient limit-condition for ]
In particular,
\begin{eqs}
\limsup_{\filterset{F}} \frac{\restrb{f}{F}}{\restrb{g}{F}} = 0 \implies f \in \rsmallohx{g},
\end{eqs}
but this is only a sufficient condition.
\end{example}

\newcommand{\ReTraditionalSmallOByALimit}[1][]{
\begin{theorem}[Traditional  by a limit]
\label{TraditionalSmallOByALimit#1}
\begin{eqs}
{} & \forall \epsilon \in \posi{\TR}: \exists A \in \filterset{X}: \restrb{f}{A} \leq \epsilon \restrb{g}{A} \\
\iffr & \limsup_{\filterset{F}} \frac{\restrb{f}{F}}{\restrb{g}{F}} = 0,
\end{eqs}
for all , where  .
\end{theorem}
}

\ReTraditionalSmallOByALimit[Text]

\begin{example}[Asymptotic linear dominance from a limit in ]
\label{AsymptoticLinearDominanceFromALimitInPlane}
Consider asymptotic linear dominance . Let  be such that . Since , and
\begin{eqs}
\limsup_{\filterset{\TN^2}} \frac{f}{1} = 0,
\end{eqs}
it holds that  by \thref{LocalLinearSmallOhByALimit}.
\end{example}

\newcommand{\ReLocalLinearSmallOmegaByALimit}[1][]{
\begin{theorem}[ by a limit]
\label{LocalLinearSmallOmegaByALimit#1}
\begin{eqs}
\bra{\limsup_{\filterset{F}} \frac{\restrb{f}{F}}{\restrb{g}{F}} = \infty \land \liminf_{\filterset{G}} \frac{\restrb{f}{G}}{\restrb{g}{G}} > 0} \iff f \in \romegax{g},
\end{eqs}
for all , where  and .
\end{theorem}
}

\ReLocalLinearSmallOmegaByALimit[Text]

\begin{example}[Sufficient limit-condition for ]
In particular,
\begin{eqs}
\liminf_{\filterset{G}} \frac{\restrb{f}{G}}{\restrb{g}{G}} = \infty \implies f \in \romegax{g},
\end{eqs}
but this is only a sufficient condition.
\end{example}















































\chapter{Conclusion}
\label{Conclusion}

This \manuscript{} provides a rigorous mathematical foundation for the -notation and its related notations in algorithm analysis. To find the appropriate definition, an exhaustive list of desirable properties was constructed, and their relations were studied. This revealed that the desirable properties can be reduced to \nprim{} primitive properties which imply the others. It was shown that these primitive properties are equivalent to the definition of the -notation as linear dominance. Master theorems were shown to hold for linear dominance, and -mappings were defined for easily proving new rules for the -notation. Other existing definitions were studied and compared to each other based on the primitive properties. Some misuses of the -notation from the literature were pointed out.

\We{} hope this \manuscript{} to improve the teaching of the topic, and to improve the communication between computer scientists. Computer scientists have used the -notation correctly intuitively. That intuition is now backed by a solid mathematical foundation.

\ifarxiv
\bibliography{asymptotic}
\bibliographystyle{unsrt}
\else
\printbibliography
\fi

\appendix

\chapter{Notation}
\label{Notation}

In this section we provide the definitions for the used notation. This is to avoid ambiguities arising from differing conventions, such as a differing order of composition of functions. We will assume the Zermelo-Fraenkel set-theory with the axiom of choice, abbreviated ZFC.  

\We{} refer to theorems in the form \thref{LinearImpliesPrimitive} --- a short summary followed by a number. \We{} believe this is more useful than \Cref{LinearImpliesPrimitive} when studying the proofs. The numberings in equations, definitions, theorems etc. share the same counter.

\begin{definition}[Numbers]
The set of natural numbers, integers, and real numbers are denoted by , , and , respectively. 
\end{definition}

\begin{definition}[Subsets of a set]
The \define{set of subsets} of a set  is denoted by .
\end{definition}

\begin{definition}[Finite subsets of a set]
The \define{set of finite subsets} of a set  is denoted by
\begin{eqs}
\fpower{X} \coloneqq \setb{D \in \power{X} : \card{D} < \infty}. 
\end{eqs}
\end{definition}

\begin{definition}[Cofinite subsets of a set]
The \define{set of cofinite subsets} of a set  is denoted by
\begin{eqs}
\cpower{X} \coloneqq \setb{D \in \power{X} : \card{X \setminus D} < \infty}. 
\end{eqs}
\end{definition}

\begin{definition}[Cover of a set]
A set  is a \define{cover} of a set , if
\begin{eqs}
X \subset \bigcup C. 
\end{eqs}
\end{definition}

\begin{definition}[Set of functions]
The set of functions from a set  to a set  is denoted by , or alternatively by . 
\end{definition}

\begin{definition}[Identity function]
The \define{identity function} in a set  is a function  such that
\begin{eqs}
\iden{X}(x) = x. 
\end{eqs}
\end{definition}

\begin{definition}[Composition]
The \define{composition} of  and  is  such that . 
\end{definition}

\begin{definition}[Restriction]
The \define{restriction} of  to  is  such that . 
\end{definition}

\begin{definition}[Inverse of a function]
The \define{inverse} of a function  is  such that
\begin{eqs}
f(\inv{f}{y}) = y, \\
\inv{f}{f(x)} = x,
\end{eqs}
for all , .
\end{definition}

\begin{definition}[Image of a set under a function]
The \define{image} of a set  under a function  is
\begin{eqs}
\image{f}{S} = \setb{f(x) : x \in S}.
\end{eqs}
\end{definition}

\begin{definition}[Pre-image of a set under a function]
The \define{pre-image} of a set  under a function  is
\begin{eqs}
\preimage{f}{S} = \setb{x \in X : f(x) \in S}.
\end{eqs}
\end{definition}

\begin{definition}[Increasing function]
A function  is \define{increasing}, if , for all . 
\end{definition}

\begin{example}[Non-negative powers are increasing]
Let  be such that , where . Then  is increasing.
\end{example}

\begin{definition}[Class of sets]
A \define{class of sets} is an unambiguous collection of sets, which may or may not be a set itself. 
\end{definition}

\begin{definition}[Proper class]
A \define{proper class} is a class of sets which is not a set. 
\end{definition}

\begin{example}[Classes of sets]
Every set is a class of sets. The collection of all sets in ZFC is a proper class.
\end{example}

\begin{note}[Formalization of proper classes]
The proper classes are not formalizable in the ZFC set-theory. This is not a problem for two reasons. First, everything in this \manuscript{} can be carried through without forming proper classes, by only referring to the class elements and their relationships. Alternatively, we may adopt the von Neumann-Bernays-G\"odel set theory \cite{IntroToMathematicalLogic} --- a conservative extension of ZFC which formalizes proper classes.
\end{note}

\begin{definition}[Set of relations]
The set of relations between a set  and a set  is denoted by . 
\end{definition}

\begin{definition}[Reflexive relation]
A relation  is \define{reflexive}, if , for all .
\end{definition}

\begin{definition}[Transitive relation]
A relation  is \define{transitive}, if
\begin{eqs}
x \sim y \land y \sim z \implies x \sim z,
\end{eqs}
for all .
\end{definition}

\begin{definition}[Preorder]
A \define{preorder} in a set  is a reflexive and transitive relation .
\end{definition}

\begin{definition}[Filtered set]
Let  be a set, and . Then

for all . 
\end{definition}

\begin{example}
 is the set of non-negative real numbers.
\end{example}

\begin{definition}[Indicator function]
The \define{indicator function} of  in a set  is a function  such that 

\end{definition}

\begin{definition}[Non-negative real-valued functions]
We are specifically interested in non-negative real-valued functions; we define 

where  is a set. 
\end{definition}

\begin{definition}[Extension of a unary operator to functions]
A unary operator  is extended to functions  by

for all .
\end{definition}

\begin{definition}[Extension of a binary operator to functions]
A binary operator  is extended to functions  by

for all .
\end{definition}

\begin{definition}[Extension of a relation to functions]
A relation  is extended to functions  by

\end{definition}

\begin{example}
It holds that , for all .
\end{example}

\begin{example}
Let , where . Then
\begin{eqs}
x \geq y \iff \forall i \in d: x_i \geq y_i.
\end{eqs}
\end{example}

\begin{note}[Relation-lifting pitfall]
We adopted an implicit convention for lifting relations from sets to functions. This introduces a potential for notational ambiguity. 

Consider the formula , where . If the negation refers to the original relation, then the formula is equivalent to
\begin{eqs}
\forall x \in X : f(x) \geq g(x).
\end{eqs}
However, if the negation refers to the lifted relation, then the formula is equivalent to
\begin{eqs}
\exists x \in X : f(x) \geq g(x).
\end{eqs}
We avoid this ambiguity by not using negation for lifted relations. In particular, we denote the filtered sets of co-asymptotic linear dominance by  --- not by .
\end{note}

\begin{definition}[Unary function for a set of functions]
A unary function  is extended to  by
\begin{eqs}
\ominus U = \setb{\ominus u : u \in U}.
\end{eqs}
\end{definition}

\begin{definition}[Binary function for sets of functions]
A binary function  is extended to  by
\begin{eqs}
U \oplus V = \setb{u \oplus v : (u, v) \in U \times V}. 
\end{eqs}
\end{definition}

\begin{definition}[Iteration]
The \define{:th iteration} of , where , is  such that

\end{definition}

\begin{definition}[Projection]
A \define{projection} is a function  such that
\begin{eqs}
\projection{X}{Y}{x, y} = x.
\end{eqs}
\end{definition}

\begin{definition}[Universe]
A \define{universe} is a class  of sets such that
\begin{enumerate}
\item ,
\item ,
\item ,
\item .
\end{enumerate}
\end{definition}

\begin{definition}[Sub-universe]
A \define{sub-universe} of a universe  is a subclass  which is also a universe. 
\end{definition}

\begin{example}[Examples of universes]
The smallest universe is given by
\begin{eqs}
\bigcup_{d \in \TN} \power{\TN^d} 
\end{eqs}
Every universe contains this set as a sub-universe. The class of all sets is a universe which is a proper class.
\end{example}

\begin{definition}[Computational problem]
A \define{computational problem} is a function , where . 
\end{definition}

\begin{definition}[Set of algorithms]
The set of algorithms\footnote{We will define the term \emph{algorithm} formally in \sref{Algorithms}.} which solve a problem , under a given model of computation, is denoted by . The set of all algorithms from  to  is
\begin{eqs}
\algof{X}{Y} = \bigcup_{\function{P}{X}{Y}} \algo{P}.
\end{eqs}
\end{definition}

\begin{definition}[Composition of algorithms]
The \define{composition} of algorithms  and  is the algorithm , which is obtained by using the output of  as the input of .
\end{definition}

\begin{note}[]
We will sometimes use an algorithm  as if it were its induced function instead. 
\end{note}

\begin{definition}[Cost function of an algorithm]
The cost function of an algorithm  is denoted by .
\end{definition}

\chapter{Howell's counterexample}
\label{HowellCounter}

In this section, we consider Howell's counterexample \cite{OhImpossible}, which shows that asymptotic linear dominance  does not satisfy \property{SubsetSum}.\footnote{We have fixed the error of having the sum-index  run only to .} 

\begin{example}[Howell's counterexample]
Let ,  be such that

and  be such that . Then
\begin{eqs}
\sum_{i = 0}^{m} \hat{g}(i, n) & = 2^n + m(m + 1)n / 2 \\
{} & \not\in \pohx{m(m + 1)n / 2} \\
{} & = \pohx{\sum_{i = 0}^{m} g(i, n)}.
\end{eqs}
\end{example}

\begin{note}[Howell's requirements]
To be precise, Howell required the following properties from an -notation:
\begin{description}
\item[\property{AsymptoticRefinement}] \hfill \\

\item[\property{Reflex}] \hfill \\

\item[\property{AsymptoticOrder}] \hfill \\

\item[\property{SimpleSubsetSum}] \hfill \\
\begin{eqs}
\sum_{i = 0}^{n_k} \hat{g}(n_1, \dots, n_{k - 1}, i, n_{k + 1}, \dots, n_d) \in \\
\ohx{\sum_{i = 0}^{n_k} g(n_1, \dots, n_{k - 1}, i, n_{k + 1}, \dots, n_d)},
\end{eqs}
\end{description}
where , , , and . 
\end{note}

While Howell did not do so, we claim that any sensible definition of -notation must also satisfy \property{Scale}:
\begin{eqs}
\ohx{\alpha f} = \ohx{f},
\end{eqs}
for all  and . The following theorem then shows that Howell's result only concerns the -notation.

\begin{theorem}[Howell's definition is asymptotic dominance]
\label{HowellsDefinition}
 has \require{AsymptoticOrder}, \require{Scale}, \require{Reflex}, and \require{AsymptoticRefinement}.  .
\end{theorem}

\begin{proof}
By \require{AsymptoticOrder}, \require{Scale}, and \require{Reflex},
\begin{eqs}
{} \quad & \hat{f} \in \pohx{f} \\
\impliesr & \exists c \in \posi{\TR}, \exists y \in \TN^d : \restrb{\hat{f}}{\nonnb{X}{y}} \leq c \restrb{f}{\nonnb{X}{y}} \\
\impliesr & \exists c \in \posi{\TR} : \ohx{\hat{f}} \subset \ohx{cf} \\
\impliesr & \ohx{\hat{f}} \subset \ohx{f} \\
\impliesr & \hat{f} \in \ohx{f},
\end{eqs}
for all . Therefore . It follows from \require{AsymptoticRefinement} that .
\end{proof}

\chapter{Proofs of implied properties}
\label{ImpliedProperties}

In this section we will show that the primitive properties imply the rest of the properties. 

\begin{definition}[Composite property]
A \define{composite property} is a property which can be equivalently expressed in terms of primitive properties. 
\end{definition}

\begin{table}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|l|}
\hline 
Property &  & T &  &  & L &  &  &  & Th \\
\hline 
\hline 
\uproperty{QSubHom} & {} & {} & {} & {} & {} & \checkmark & \checkmark & {} & \ref{QSubhomogenuityIsComposite} \\
\hline 
\uproperty{SubHom} & \checkmark & \checkmark & {} & \checkmark & {} & \checkmark & \checkmark & {} & \ref{RSubhomogenuityIsImplied} \\
\hline 
\uproperty{Reflex} & \checkmark & {} & {} & {} & {} & {} & {} & {} & \ref{ReflexivityIsImplied} \\
\hline 
\uproperty{Zero} & \checkmark & \checkmark & \checkmark & {} & {} & \checkmark & {} & {} & \ref{ZeroSeparationIsImplied} \\
\hline 
\uproperty{Orderness} & \checkmark & \checkmark & {} & {} & {} & {} & {} & {} & \ref{OrdernessIsImplied} \\
\hline 
\uproperty{TrivialZero} & \checkmark & \checkmark & \checkmark & {} & {} & \checkmark & \checkmark & \checkmark & \ref{ZeroTrivialityIsImplied} \\
\hline 
\usproperty{ISuperComp} & \checkmark & {} & {} & {} & \checkmark & {} & {} & \checkmark & \ref{InjectiveSuperComposabilityIsImplied} \\
\hline 
\uproperty{SubRestrict} & {} & {} & {} & {} & {} & {} & {} & \checkmark & \ref{SubRestrictabilityIsImplied} \\
\hline 
\uproperty{SuperRestrict} & \checkmark & {} & {} & {} & \checkmark & {} & {} & {} & \ref{SuperRestrictabilityIsImplied} \\
\hline 
\uproperty{ScalarHom} & \checkmark & \checkmark & {} & \checkmark & {} & {} & {} & {} & \ref{ScalarHomogenuityIsImplied} \\
\hline 
\uproperty{SuperHom} & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \ref{SuperHomogenuityIsImplied} \\
\hline 
\uproperty{SubMulti} & \checkmark & \checkmark & {} & \checkmark & {} & \checkmark & \checkmark & {} & \ref{SubMultiplicativityIsImplied} \\
\hline 
\uproperty{SuperMulti} & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \ref{SuperMultiplicativityIsImplied} \\
\hline 
\uproperty{AddCons} & \checkmark & \checkmark & {} & {} & \checkmark & {} & {} & \checkmark & \ref{AdditiveConsistencyIsImplied} \\
\hline 
\uproperty{MaxCons} & \checkmark & \checkmark & {} & {} & \checkmark & {} & {} & \checkmark & \ref{MaximumConsistencyIsImplied} \\
\hline 
\usproperty{MultiCons} & \checkmark & \checkmark & {} & {} & \checkmark & {} & {} & \checkmark & \ref{MultiplicativeConsistencyIsImplied} \\
\hline 
\uproperty{Maximum} & \checkmark & \checkmark & {} & {} & \checkmark & {} & {} & \checkmark & \ref{MaximumIsImplied} \\
\hline 
\uproperty{Summation} & \checkmark & \checkmark & {} & \checkmark & {} & {} & {} & {} & \ref{SummationIsImplied} \\
\hline 
\uproperty{MaximumSum} & \checkmark & \checkmark & {} & \checkmark & {} & {} & {} & {} & \ref{MaximumSumIsImplied} \\
\hline 
\uproperty{Additive} & \checkmark & \checkmark & {} & \checkmark & \checkmark & {} & {} & \checkmark & \ref{AdditivityIsImplied} \\
\hline 
\usproperty{Translation} & \checkmark & \checkmark & {} & \checkmark & {} & {} & {} & {} & \ref{TranslationInvarianceIsImplied} \\
\hline 
\uproperty{Extend} & {} & {} & {} & {} & {} & {} & {} & \checkmark & \ref{ExtensibilityIsImplied} \\
\hline 
\uproperty{SubsetSum} & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \ref{SubsetSumIsImplied} \\
\hline 
\end{tabular}
\centering
\caption{The primitive properties that imply a given non-primitive property. The abbreviations are:  for \property{Order}, T for \property{Trans},  for \property{One},  for \property{Scale}, L for \property{Local},  for \property{NSubHom},  for \property{NSubDiv},  for \property{SubComp}, and Th for theorem.}
\label{PrimitivePropertiesNeededToImplyOtherProperties}
\end{table}

\begin{proposition}[\uproperty{QSubHom} is a composite]
\label{QSubhomogenuityIsComposite}
 has \prove{QSubHom}.   has \require{NSubHom} and \require{NSubDiv}.
\end{proposition}

\begin{proof}
\proofpart{}
Suppose  has \property{QSubHom}. Then  has \property{NSubHom}, since , and \property{NSubDiv}, since .

\proofpart{}
Suppose  has \require{NSubHom} and \require{NSubDiv}. Let  be such that . Then there exists , such that , , and . By \property{NSubHom} and \property{NSubDiv},
\begin{eqs}
{} & f \in \ohx{g} \\
\impliesr & pf \in \ohx{pg} \\
\impliesr & \frac{p}{q} f \in \ohx{\frac{p}{q} g} \\
\impliesr & uf \in \ohx{ug}. 
\end{eqs} \sprove{QSubHom}
\end{proof}

\begin{proposition}[\uproperty{SubHom} is implied]
\label{RSubhomogenuityIsImplied}
 has \require{Order}, \require{Trans}, \require{Scale}, \require{NSubHom}, and \require{NSubDiv}.   has \prove{SubHom}.
\end{proposition}

\begin{proof}
 has \property{QSubHom} by \proveby{QSubhomogenuityIsComposite}. Let , and  be such that
\begin{eqs}
x \leq h(x) \leq 2x.
\end{eqs}
By \require{QSubHom}, \require{Order}, \require{Trans}, and \require{Scale},
\begin{eqs}
{} & f \in \ohx{g} \\
\implies & (h \circ u) f \in \ohx{(h \circ u) g} \\
\implies & uf \in \ohx{2ug} \\
\implies & uf \in \ohx{ug}. 
\end{eqs} \sprove{SubHom}
\end{proof}

\begin{proposition}[\uproperty{Reflex} is implied]
\label{ReflexivityIsImplied}
 has \require{Order}   has \prove{Reflex}.
\end{proposition}

\begin{proof}
By \require{Order}, , for all ;  has \prove{Reflex}. 
\end{proof}

\begin{proposition}[\uproperty{Zero} is implied]
\label{ZeroSeparationIsImplied}
 has \require{Order}, \require{Trans}, \require{One}, and \require{NSubHom}   has \prove{Zero}.
\end{proposition}

\begin{proof}
Suppose  does not have \property{Zero}, so that . By \require{NSubHom}, . By \require{Order}, . By \require{Trans}, . This contradicts \require{One}. \sprove{Zero}
\end{proof}

\begin{proposition}[\uproperty{Orderness} is a composite]
\label{ReflexiveTransitiveIsOrderness}
 has \require{Reflex} and \require{Trans}   has \prove{Orderness}.
\end{proposition}

\begin{proof}
\proofpart{} 
Assume . Let . By \require{Trans}, , and so . Assume . By \require{Reflex}, . Therefore , and so  has \prove{Orderness}. 

\proofpart{} 
By \require{Orderness}, . Therefore  has \prove{Reflex}. Let , and . By \require{Orderness}, . Therefore , and so  has \prove{Trans}.
\end{proof}

\begin{proposition}[\uproperty{Orderness} is implied]
\label{OrdernessIsImplied}
 has \require{Order} and \require{Trans}.   has \prove{Orderness}.
\end{proposition}

\begin{proof}
 has \property{Reflex} by \proveby{ReflexivityIsImplied}.  has \property{Orderness} by \proveby{ReflexiveTransitiveIsOrderness}.
\end{proof}

\begin{proposition}[\uproperty{TrivialZero} is implied]
\label{ZeroTrivialityIsImplied}
 has \require{Order}, \require{Trans}, \require{One}, \require{Scale}, \require{NSubHom}, \require{NSubDiv}, and \require{SubComp}.   has \prove{TrivialZero}.
\end{proposition}

\begin{proof}
 has \property{Zero} by \proveby{ZeroSeparationIsImplied} and \property{SubHom} by \proveby{RSubhomogenuityIsImplied}. Suppose  does not have \property{TrivialZero}, so that there exists  such that . Then there exists  such that , for some . Let  be such that . By \require{SubComp} and \require{SubHom},
\begin{eqs}
{} & f \in \ohx{0} \\
\impliesr & f \circ s \in \ohx{0} \circ s \\
\impliesr & c \in \oh{\posi{\TN}}{0 \circ s} \\
\impliesr & 1 \in \oh{\posi{\TN}}{0} / c \\
\impliesr & 1 \in \oh{\posi{\TN}}{0 / c} \\
\impliesr & 1 \in \oh{\posi{\TN}}{0}.
\end{eqs}
This contradicts \require{Zero}. 
\sprove{TrivialZero}
\end{proof}

\begin{proposition}[\uproperty{ISuperComp} is implied]
\label{InjectiveSuperComposabilityIsImplied}
 has \require{Order}, \require{Local}, and \require{ISubComp} for injective .   has \prove{ISuperComp} for . \sprove{IComp}
\end{proposition}

\begin{proof}
Let , and  be such that . Then  is bijective. Let  and  be such that
\begin{eqs}
\hat{f}(x) = 
\begin{cases}
\bra{\hat{g} \circ \invs{\underline{s}}}(x), & x \in \image{s}{Y}, \\
0, & x \not\in \image{s}{Y}.
\end{cases}
\end{eqs}
Then ; we show that . 
By \require{ISubComp}
\begin{eqs}
\hat{f}|\image{s}{Y} & = \hat{g} \circ \invs{\underline{s}} \\
{} & \in \oh{Y}{f \circ s} \circ \invs{\underline{s}} \\
{} & \subset \oh{\image{s}{Y}}{f \circ s \circ \invs{\underline{s}}} \\
{} & = \oh{\image{s}{Y}}{f|\image{s}{Y}}.
\end{eqs}
By \require{Order},

By \require{Local},

Therefore  has \prove{ISuperComp} for . \sprove{IComp}
\end{proof}

\begin{proposition}[\uproperty{SubRestrict} is implied]
\label{SubRestrictabilityIsImplied}
 has \require{ISubComp}.   has \prove{SubRestrict}.
\end{proposition}

\begin{proof}
Let , and  be such that . Then  is injective. By \require{ISubComp}
\begin{eqs}
\restr{\ohx{f}}{D} & = \ohx{f} \circ s \\
{} & \subset \oh{D}{f \circ s} \\
{} & = \oh{D}{\restr{f}{D}},
\end{eqs}
for all . \sprove{SubRestrict}
\end{proof}

\begin{theorem}[\uproperty{SuperRestrict} is implied]
\label{SuperRestrictabilityIsImplied}
 has \require{Order} and \require{Local}.   has \prove{SuperRestrict}.
\end{theorem}

\begin{proof}
Let , , and . Let  be such that  and . Then , and by \require{Order}, . By \require{Local}, . \sprove{SuperRestrict}
\end{proof}

\begin{theorem}[\uproperty{ScalarHom} is implied]
\label{ScalarHomogenuityIsImplied}
 has \require{Order}, \require{Trans}, and \require{Scale}.   has \prove{ScalarHom}.
\end{theorem}

\begin{proof}
 has \property{Orderness} by \proveby{OrdernessIsImplied}.

\proofpart{}
Let , and . By \require{Order}, \require{Scale}, \require{Orderness}, and \require{Scale} again,
\begin{eqs}
\alpha \hat{f} & \in \ohx{\alpha \hat{f}} \\
{} & = \ohx{\hat{f}} \\
{} & \subset \ohx{f} \\
{} & = \ohx{\alpha f}.
\end{eqs}
Therefore .

\proofpart{}
Let . By \require{Scale}, . By \require{Orderness}, . By \require{Scale}, . By \require{Orderness}, . Therefore , and so  has \prove{ScalarHom}.
\end{proof}

\begin{theorem}[\uproperty{SuperHom} is implied]
\label{SuperHomogenuityIsImplied}
 has \require{Order}, \require{Local}, \require{Trans}, \require{One}, \require{Scale}, \require{NSubHom}, \require{NSubDiv}, and . \linebreak   has \prove{SuperHom}.
\end{theorem}

\begin{proof}
\srequire{SubComp} \sprove{ISubComp}
 has \property{SubRestrict} by \proveby{SubRestrictabilityIsImplied}, \property{SubHom} by \proveby{RSubhomogenuityIsImplied}, and \property{TrivialZero} by \proveby{ZeroTrivialityIsImplied}. Let , , and . Let  be such that  and . By \require{SubRestrict}, and \require{SubHom},
\begin{eqs}
\restrb{\hat{f}}{G} & \in \frac{\restr{\ohx{fg}}{G}}{\restr{g}{G}} \\
{} & \subset \frac{\oh{G}{\restr{fg}{G}}}{\restr{g}{G}} \\
{} & \subset \oh{G}{\restr{f}{G}}.
\end{eqs}
By \require{Order}, . By \require{Local}, . It holds that . By \require{SubRestrict}, . By \require{TrivialZero}, . Therefore .
\sprove{SuperHom}
\end{proof}

\begin{theorem}[\uproperty{SubMulti} is implied]
\label{SubMultiplicativityIsImplied}
 has \require{Order}, \require{Trans}, \require{Scale}, \require{NSubHom}, and \require{NSubDiv}.   has \prove{SubMulti}.
\end{theorem}

\begin{proof}
 has \property{Orderness} by \proveby{OrdernessIsImplied}, and \property{SubHom} by \proveby{RSubhomogenuityIsImplied}. Let , and . By \require{SubHom},
\begin{eqs}
\hat{f} g & \in \ohx{f} g \\
{} & \subset \ohx{fg}.
\end{eqs}
By \require{Orderness},
\begin{eqs}
\ohx{\hat{f} g} \subset \ohx{fg}. 
\end{eqs}
By \require{SubHom},
\begin{eqs}
\hat{f} \ohx{g} & \subset \ohx{\hat{f}g} \\
{} & \subset \ohx{fg}.
\end{eqs}
Therefore . 
\sprove{SubMulti}
\end{proof}

\begin{theorem}[\uproperty{SuperMulti} is implied]
\label{SuperMultiplicativityIsImplied}
 has \require{Order}, \require{Trans}, \require{One}, \require{Scale}, \require{NSubHom}, \require{NSubDiv}, \require{Local}, and \require{SubComp}.   has \prove{SuperMulti}.
\end{theorem}

\begin{proof}
\srequire{SubComp} \sprove{ISubComp}
 has \property{SubRestrict} by \proveby{SubRestrictabilityIsImplied}, \property{SubHom} by \proveby{RSubhomogenuityIsImplied}, and \property{TrivialZero} by \proveby{ZeroTrivialityIsImplied}. Let , and . Let , and . Let  be such that  and . It holds that . By \require{SubRestrict}, . Similarly, . By \require{SubHom}
\begin{eqs}
\restrb{\hat{f}}{G} & \in \frac{\oh{G}{\restr{fg}{G}}}{\restr{g}{G}} \\
{} & \subset \oh{G}{\frac{\restr{fg}{G}}{\restr{g}{G}}} \\
{} & = \oh{G}{\restr{f}{G}}.
\end{eqs}
By \require{Order}, . By \require{Local}, . By \require{Order}, . By definition, . By \require{TrivialZero}, , and so . Therefore ;  has \prove{SuperMulti}.
\end{proof}

\begin{theorem}[\uproperty{Local} is implied]
\label{LocalityIsImplied}
 has \require{Order}, \require{Trans}, \require{One}, \require{Scale}, \require{NSubHom}, \require{NSubDiv}, and \require{SubComp}.   has \prove{Local}.
\end{theorem}

\begin{proof}
\proofpart{Implied properties}
\srequire{SubComp} \sprove{ISubComp}  has \property{TrivialZero} by \proveby{ZeroTrivialityIsImplied}, and \property{SubMulti} by \proveby{SubMultiplicativityIsImplied}.

\proofpart{Reduction from a finite cover to partition}
Let . We can refine a finite cover  of  to a finite partition  of . In the definition of \property{Local}, we assume that
\begin{eqs}
\restrb{f}{D} \in \oh{D}{\restr{g}{D}},
\end{eqs}
for all . By \require{ISubComp},
\begin{eqs}
\restrb{f}{A_i} \in \oh{A_i}{\restr{g}{A_i}},
\end{eqs}
for all . Therefore, we may assume the finite cover of  to be a finite partition of .

\proofpart{Notation}
Let  be a finite partition of . Suppose that
\begin{eqs}
\restrb{f}{A_i} \in \oh{A_i}{\restr{g}{A_i}},
\end{eqs}
for all . Let
\begin{eqs}
I = \setb{i \in \iccn{1}{m} : \image{f}{A_i} \cap \posi{\TR} \neq \emptyset}.
\end{eqs}
Let . Let  be such that , and
\begin{eqs}
f(p_i) > 0,
\end{eqs}
Let  be such that
\begin{eqs}
c_i & = f(p_i) \\
d_i & = g(p_i).
\end{eqs}
Let  be such that
\begin{eqs}
s_i(x) = 
\begin{cases}
x, & x \in A_i, \\
p_i, & \text{otherwise}.
\end{cases}
\end{eqs}

\proofpart{Positivity}
By \require{ISubComp},
\begin{eqs}
\restrb{f}{\setb{p_i}} \in \oh{\setb{p_i}}{\restr{g}{\setb{p_i}}}.
\end{eqs}
By \require{TrivialZero}, and since , it holds that . 

\proofpart{Lift}
By \require{SubComp},
\begin{eqs}
\restrb{f}{A_i} \circ s_i \in \ohx{\restrb{g}{A_i} \circ s_i}.
\end{eqs}
This is equivalent to
\begin{eqs}
\bra{f \indi{A_i}{X} + c_i \indi{X \setminus A_i}{X}} \in \ohx{g \indi{A_i}{X} + d_i \indi{X \setminus A_i}{X}}.
\end{eqs}
By \require{Order}, and \require{Scale},
\begin{eqs}
\bra{\indi{A_i}{X} + \frac{1}{c_i} \indi{X \setminus A_i}{X}} \in \ohx{\indi{A_i}{X} + \frac{1}{d_i} \indi{X \setminus A_i}{X}}.
\end{eqs}
By \require{SubMulti},
\begin{eqs}
\bra{f \indi{A_i}{X} + \indi{X \setminus A_i}{X}} \in \ohx{g \indi{A_i}{X} + \indi{X \setminus A_i}{X}}.
\end{eqs}

\proofpart{Sum by multiplying}
We have that
\begin{eqs}
\prod_{i \in I} \bra{f \indi{A_i}{X} + \indi{X \setminus A_i}{X}} & = \sum_{i \in I} f \indi{A_i}{X} = f, \\
\prod_{i \in I} \bra{g \indi{A_i}{X} + \indi{X \setminus A_i}{X}} & = \sum_{i \in I} g \indi{A_i}{X} \leq g.
\end{eqs}
By \require{SubMulti},
\begin{eqs}
f \in \ohx{\sum_{i \in I} g \indi{A_i}{X}}.
\end{eqs}
By \require{Order}, and \require{Trans},
\begin{eqs}
f \in \ohx{g}.
\end{eqs}
\sprove{Local}
\end{proof}

\begin{theorem}[\uproperty{AddCons} is implied]
\label{AdditiveConsistencyIsImplied}
 has \require{Order}, \require{Trans}, \require{Local}, and \require{ISubComp}.   has \prove{AddCons}.
\end{theorem}

\begin{proof}
 has \property{SubRestrict} by \proveby{SubRestrictabilityIsImplied}. Let , and .

\proofpart{} 
Let , and . Let  be such that

Then
\begin{eqs}
\hat{h}(x) & = \frac{u(x) \hat{f}(x) + v(x) \hat{g}(x)}{u(x) + v(x)} \\
{} & \leq \frac{u(x) \hat{f}(x) + v(x) \hat{f}(x)}{u(x) + v(x)} \\
{} & = \hat{f}(x),
\end{eqs}
for all . Also, , for all . By \require{Order}, . By \require{SubRestrict} and \require{Trans}, . Similarly, . By \require{Local}, . In addition, . Therefore .

\proofpart{}
Let . Then .

\sprove{AddCons}
\end{proof}

\begin{theorem}[\uproperty{MaxCons} is implied]
\label{MaximumConsistencyIsImplied}
 has \require{Order}, \require{Trans}, \require{Local}, and \require{ISubComp}.   has \prove{MaxCons}.
\end{theorem}

\begin{proof}
 has \property{SubRestrict} by \proveby{SubRestrictabilityIsImplied}. 

\proofpart{} 
Let , and . Let  be such that

Then
\begin{eqs}
\restrb{\hat{h}}{F} & = \restrb{\max(\hat{f}, \hat{g})}{F} \\
{} & = \restrb{\hat{f}}{F}.
\end{eqs}
By \require{Order}, . By \require{SubRestrict} and \require{Trans}, . Similarly, . By \require{Local}, . In addition, . Therefore .

\proofpart{}
Let . Then .

\sprove{MaxCons}
\end{proof}

\begin{theorem}[\uproperty{MultiCons} is implied]
\label{MultiplicativeConsistencyIsImplied}
 has \require{Order}, \require{Trans}, \require{Local}, \require{ISubComp}.   has \prove{MultiCons}.
\end{theorem}

\begin{proof}
 has \property{SubRestrict} by \proveby{SubRestrictabilityIsImplied}. Let , and .

\proofpart{} 
Let , and . Let  be such that

Then
\begin{eqs}
\hat{h}(x) & = \bra{\hat{f}(x)^{u(x)} \hat{g}(x)^{v(x)}}^{1 / (u(x) + v(x))} \\
{} & \leq \bra{\hat{f}(x)^{u(x)} \hat{f}(x)^{v(x)}}^{1 / (u(x) + v(x))} \\
{} & = \hat{f}(x).
\end{eqs}
for all . Also, , for all . By \require{Order}, . By \require{SubRestrict} and \require{Trans}, . Similarly, . By \require{Local}, . In addition,  (we assume ). Therefore .

\proofpart{} 
Let . Then .

\sprove{MultiCons}
\end{proof}

\begin{theorem}[\uproperty{Maximum} is implied]
\label{MaximumIsImplied}
 has \require{Order}, \require{Trans}, \require{Local}, and \require{ISubComp}.   has \prove{Maximum}.
\end{theorem}

\begin{proof}
 has \property{Orderness} by \proveby{OrdernessIsImplied}, \property{SubRestrict} by \proveby{SubRestrictabilityIsImplied}, and \property{MaxCons} by \proveby{MaximumConsistencyIsImplied}.

\proofpart{} 
Let . By \require{Order}, . By \require{Orderness},  and .  By \require{MaxCons},
\begin{eqs}
\max(\ohx{f}, \ohx{g}) & \subset \max(\ohx{\max(f, g)}, \ohx{\max(f, g)}) \\
{} & = \ohx{\max(f, g)}.
\end{eqs}

\proofpart{}
Let , and . Let  be such that

By \require{Order}, . By \require{Trans}, . By \require{SubRestrict}
\begin{eqs}
\restrb{\hat{f}}{F} & \in \restr{\ohx{\max(f, g)}}{F} \\
{} & \subset \oh{F}{\restr{f}{F}}.
\end{eqs}
Also, . By \require{Order}, . By \require{Local}, . Similarly, let  be such that . Then . In addition, . Therefore .

\sprove{Maximum}
\end{proof}

\begin{theorem}[\uproperty{Summation} is implied]
\label{SummationIsImplied}
 has \require{Order}, \require{Trans}, and \require{Scale}   has \prove{Summation}.
\end{theorem}

\begin{proof}
Let . 

\proofpart{} 
It holds that

By \require{Order} and \require{Trans}, . By \require{Scale}, .

\proofpart{} 
It holds that

By \require{Order} and \require{Trans}, .

\sprove{Summation}
\end{proof}

\begin{theorem}[\uproperty{MaximumSum} is implied]
\label{MaximumSumIsImplied}
 has \require{Order}, \require{Trans}, and \require{Scale}.   has \prove{MaximumSum}.
\end{theorem}

\begin{proof}
\proofpart{}
Let  and . Let . Let  be such that . By \require{Order}, . By \require{Trans}, . Similarly, let  be such that . Then . In addition, . Therefore .

\proofpart{} 
Let , , and . Let
\begin{eqs}
F = \setb{x \in X: \hat{g}(x) \leq \hat{f}(x)}. 
\end{eqs}
Let  be such that . Then . By \require{Order} and \require{Trans}, . By \require{Scale}, . By \require{Order}, . By \require{Trans} . Similarly, let  be such that

Then . In addition, . Therefore .

\sprove{MaximumSum}
\end{proof}

\begin{theorem}[\uproperty{Additive} is implied]
\label{AdditivityIsImplied}
 has \require{Order}, \require{Trans}, \require{Local}, \require{Scale}, and \require{ISubComp}.   has \prove{Additive}.
\end{theorem}

\begin{proof}
Let .  has \property{Summation} by \proveby{SummationIsImplied}. Therefore, 

 has \property{Maximum} by \proveby{MaximumIsImplied}. Therefore, 

 has \property{MaximumSum} by \proveby{MaximumSumIsImplied}. Therefore, 

Therefore .

\sprove{Additive}
\end{proof}

\begin{theorem}[\uproperty{Translation} is implied]
\label{TranslationInvarianceIsImplied}
 has \require{Order}, \require{Trans}, and \require{Scale}   has \prove{Translation}.
\end{theorem}

\begin{proof}
Let , and .

\proofpart{}
Since ,
\begin{eqs}
(f + \beta) + \alpha & \leq (f + \beta) + \alpha ((f + \beta) / \beta) \\
{} & = (1 + \alpha / \beta) (f + \beta).
\end{eqs}
By \require{Order} and \require{Trans}, . By \require{Scale} 

\proofpart{}
By \require{Order} and \require{Trans}, .

\sprove{Translation}
\end{proof}





\begin{theorem}[\uproperty{Extend} is implied]
\label{ExtensibilityIsImplied}
 has \require{SubComp}   has \prove{Extend}.
\end{theorem}

\begin{proof}
This follows from \require{SubComp} by , , and . \sprove{Extend}
\end{proof}

\begin{theorem}[\uproperty{SubsetSum} is implied]
\label{SubsetSumIsImplied}
 has primitive properties. \srequire{LinearDominance}   has \prove{SubsetSum}.
\end{theorem}

\begin{proof}
This is proved in \proveby{SubsetSumIsAnOMapping}.
\end{proof}

\begin{theorem}[\uproperty{SubsetSum} implies \property{SubComp}]
\label{SubsetSumImpliesSubComposability}
 has \require{Order}, \require{Trans} and \require{SubsetSum}. \srequire{SubsetSum}   has \prove{SubComp}.
\end{theorem}

\begin{proof}
 has \property{Reflex} by \proveby{ReflexivityIsImplied}, and \property{Orderness} by \proveby{OrdernessIsImplied}. 

Let , and . Let  be such that . Then for ,
\begin{eqs}
\ohx{\sum_{(y, z) \in S_x} a(z) f(y)} = \ohx{f \circ s}.
\end{eqs}
Similarly, for ,
\begin{eqs}
\ohx{\sum_{(y, z) \in S_x} a(z) \bar{f}(y)} = \ohx{\bar{f} \circ s}.
\end{eqs}
By \require{SubsetSum},
\begin{eqs}
\ohx{f \circ s} \subset \ohx{\bar{f} \circ s}.
\end{eqs}
By \require{Orderness},
\begin{eqs}
(f \circ s) \in \ohx{\bar{f} \circ s}.
\end{eqs}
\sprove{SubComp}
\end{proof}

\chapter{Proofs of local linear dominance properties}
\label{ProofsForLocalLinearDominance}

In this section we prove some of the properties of a local linear dominance. We shall apply the following simplification lemma without mentioning it, since it is used in almost every proof.

\begin{lemma}[Simplification lemma for ]
\label{LocalSingleConstantLemma}
Let ,  be a finite set, , , and , for all . Then there exists  and , such that

for all .
\end{lemma}

\begin{proof}
Since , there exists  and , such that

for all . By induced sub-structure, there exists  such that , for all . Since  is -directed, and  is finite, there exists  such that . Let . Since ,

for all .
\end{proof}

\begin{note}[Filter basis and simplification lemma]
A filter basis in  seems to be the minimal amount of structure needed to prove the simplification lemma. Indeed, we first provided the abstraction of a filter basis solely to prove this lemma in its most general form. It is only later that we noticed that this structure also allows us to define limits.
\end{note}

\begin{theorem}[\uproperty{Order} for ]
\label{LocalOrderConsistency}
Let , and . Then \sprove{Order}

\end{theorem}

\begin{proof}
Let . Then there exists  and  such that

Since ,

Therefore .
\sprove{Order}
\end{proof}

\begin{theorem}[\uproperty{Trans} for ]
\label{LocalTransitivity}
Let , and . Then \sprove{Trans}

\end{theorem}

\begin{proof}
Let , and . Then there exists  and , such that
\begin{eqs}
\restrb{f}{A} & \lt c \restrb{g}{A}, \\
\restrb{g}{A} & \lt c \restrb{h}{A}.
\end{eqs}
It follows that
\begin{eqs}
\restrb{f}{A} & \lt c (c \restrb{h}{A}) \\
{} & = c^2 \restrb{h}{A}.
\end{eqs}
Therefore .
\sprove{Trans}
\end{proof}

\begin{theorem}[\uproperty{Local} for ]
\label{LocalLocality}
Let , , and  be a finite cover of . Then \sprove{Local}

\end{theorem}

\begin{proof}


Assume , for all . Then there exist  and  such that
\begin{eqs}
\qquad & \restrb{\restrb{f}{D}}{(D \cap A)} \lt c \restrb{\restrb{g}{D}}{(D \cap A)} \\
\iffr & \bra{\restr{f}{\bra{D \cap A}}} \lt c \bra{\restr{g}{\bra{D \cap A}}},
\end{eqs}
for all . Since  covers ,

Therefore .
\sprove{Local}
\end{proof}

\begin{theorem}[\uproperty{One} for  characterized]
\label{LocalOneSeparationCharacterized}

\end{theorem}

\begin{proof}
\proofpart{}
Suppose every set in  is infinite. Then for all  and  there exists  such that . Therefore .

\proofpart{}
Suppose some set  is finite. Let . Then . Therefore .
\end{proof}

\begin{theorem}[\uproperty{Scale} for ]
\label{LocalScaleInvariance}
Let , , and . Then . \sprove{Scale}
\end{theorem}

\begin{proof}
\proofpart{}
Assume . Then there exists  and , such that
\begin{eqs}
\restrb{\hat{f}}{A} & \lt c \restrb{(\alpha f)}{A} \\
{} & = (c \alpha) \restrb{f}{A}.
\end{eqs}
Therefore . 

\proofpart{}
Assume . Then there exists  and , such that
\begin{eqs}
\restrb{\hat{f}}{A} & \lt c \restrb{f}{A} \\
{} & = (c / \alpha) \restrb{(\alpha f)}{A}.
\end{eqs}
Therefore .

\sprove{Scale}
\end{proof}

\begin{theorem}[\uproperty{SubHom} for ]
\label{LocalSubHomogenuity}
Let , and . Then \sprove{SubHom}

\end{theorem}

\begin{proof}
Let . Then there exists  and , such that
\begin{eqs}
\restrb{\hat{f}}{A} & \lt c \restrb{f}{A}.
\end{eqs}
This implies
\begin{eqs}
\restrb{u\hat{f}}{A} \lt c \restrb{uf}{A}.
\end{eqs}
Therefore .
\sprove{SubHom}
\end{proof}

\begin{theorem}[\uproperty{SuperHom} for  characterized]
\label{LocalSuperHomogenuityCharacterized}
 has \property{SuperHom}.  .
\end{theorem}

\begin{proof}
\proofpart{}
Suppose . Then there exists exists  such that . Let , and  be such that . Let , and  be such that . Then , and so . Let . Then ;  does not have \property{SuperHom}.

\proofpart{}
Suppose . Let , and . Then there exists  such that

Let , and . Let  be such that  and . Then , and so . In addition, . 
\end{proof}

\begin{theorem}[\uproperty{SuperMulti} for ]
\label{LocalSuperMultiplicativity}
Let , and . Then \sprove{SuperMulti}

\end{theorem}

\begin{proof}
Let . Then there exists  and , such that

Let  be such that

Then , since . Let , and let  be such that

If , then . If , then , and . Since , it follows that . Therefore , and . 
If , then , and . If , then , and . Therefore , and .
\sprove{SuperMulti}
\end{proof}

\begin{note}[\uproperty{SuperMulti} in another way?]
In proving \property{SuperMulti}, we cannot refer to \thref{SuperMultiplicativityIsImplied}, since there are local linear dominances which do not satisfy its assumptions.
\end{note}

\begin{theorem}[\uproperty{SubComp} for  for fixed ]
\label{LocalSubComposabilityForFixedS}
Let , and . Then
\begin{eqs}
{} & \forall f \in \rc{X}: \roh{X}{f} \circ s \subset \roh{Y}{f \circ s} \\
\iffr & \forall A_X \in \filterset{X}, \exists A_Y \in \filterset{Y}: \image{s}{A_Y} \subset A_X.
\end{eqs}
\end{theorem}

\begin{proof}
First notice that
\begin{eqs}
{} & \image{s}{A_Y} \subset A_X \\
\iffr & A_Y \subset \preimage{s}{A_X} \\
\iffr & \indi{A_Y}{Y} \leq \indi{\preimage{s}{A_X}}{Y} \\
\iffr & \indi{A_Y}{Y} \leq \indi{A_X}{X} \circ s,
\end{eqs}
for all , . We attempt to prove the statement in this form. 

\proofpart{}
Let , , and  be such that . Then , and so . By assumption, , and so there exists  and  such that
\begin{eqs}
\indi{A_Y}{Y} & \leq \indi{A_Y}{Y} d (f \circ s) \\
{} & = \indi{A_Y}{Y} d \bra{\indi{A_X}{X} \circ s} \\
{} & \leq d \bra{\indi{A_X}{X} \circ s}.
\end{eqs}
Since the functions are indicator functions, this is equivalent to


\proofpart{}
Let  and . Then there exists  and  such that . By assumption, there exists  such that . Then
\begin{eqs}
{} & \idf{A_X} \hat{f} \leq \idf{A_X} c f \\
\impliesr & \bra{\idf{A_X} \circ s} \bra{\hat{f} \circ s} \leq 
\bra{\idf{A_X} \circ s} c \bra{f \circ s} \\
\impliesr & \indi{A_Y}{Y} \bra{\hat{f} \circ s} \leq 
\indi{A_Y}{Y} c \bra{f \circ s} \\
\impliesr & \bra{\hat{f} \circ s} \in \roh{Y}{f \circ s}.
\end{eqs}
\end{proof}

\begin{theorem}[Characterization of \property{SubComp} for ]
\label{LocalSubComposabilityCharacterized}
 has \property{SubComp} if and only if
\begin{eqs}
\bra{\forall X \in U: \filterset{X} = \setb{X}} \lor \bra{\forall X \in U: \emptyset \in \filterset{X}}.
\end{eqs}
\end{theorem}

\begin{proof}
\proofpart{}
Suppose there exists , such that  and . Let , , and  be such that . Then , and  does not have \property{SubComp} under  by \thref{LocalSubComposabilityForFixedS}. This shows that either , or , for all . 

Suppose there exists , such that , and . Then \property{SubComp} does not hold for any , by \thref{LocalSubComposabilityForFixedS}, because .

\proofpart{}
Let , and . Either  and , or  and . In either case, \property{SubComp} holds by \thref{LocalSubComposabilityForFixedS}.
\end{proof}

\begin{theorem}[\uproperty{SubComp} for  for positive functions]
\label{LocalSubComposabilityForPositiveFunctions}
 has \property{SubComp} for positive functions .
\end{theorem}

\begin{proof}
Let , where . Then there exists  and , such that
\begin{eqs}
{} & \restrb{\hat{f}}{A} \leq c \restrb{f}{A} \\
\iffr & \frac{\restrb{\hat{f}}{A}}{\restrb{f}{A}} \leq c.
\end{eqs}
Since , let
\begin{eqs}
d = \max \setb{\frac{\restr{\hat{f}}{(X \setminus A)}}{\restr{f}{(X \setminus A)}}} \cup \setb{c}.
\end{eqs}
Then , and
\begin{eqs}
{} & \frac{\hat{f}}{f} \leq d \\
\iffr & \hat{f} \leq d f.
\end{eqs}
Let , where . Then
\begin{eqs}
(\hat{f} \circ s) \leq d (f \circ s).
\end{eqs}
Therefore .
\end{proof}

\begin{theorem}[Characterization of \uproperty{Extend} for ]
\label{LocalExtensibilityCharacterized}
 has \property{Extend} if and only if
\begin{eqs}
\forall A \in \filterset{X}: \exists B \in \filterset{X \times Y}: \image{\projections{X}{Y}}{B} \subset A.
\end{eqs}
\end{theorem}

\begin{proof}
This follows directly from \thref{LocalSubComposabilityForFixedS}.
\end{proof}

\begin{theorem}[\uproperty{PowerH} for ]
\label{LocalPowerHomogenuity}
Let , , and . Then \sprove{PowerH}
\begin{eqs}
\rohx{f}^{\alpha} = \rohx{f^{\alpha}}.
\end{eqs}
\end{theorem}

\begin{proof}
\proofpart{}
Let . Then there exists , and , such that
\begin{eqs}
\restrb{\hat{f}}{A} \leq c \restrb{f}{A}.
\end{eqs}
Then
\begin{eqs}
\restrb{\hat{f}}{A}^{\alpha} \leq c^{\alpha} \restrb{f}{A}^{\alpha}.
\end{eqs}
Therefore .

\proofpart{}
Let . Then there exists , and , such that
\begin{eqs}
\restrb{\hat{f}}{A} \leq c \restrb{f^{\alpha}}{A}.
\end{eqs}
Then
\begin{eqs}
\restrb{\hat{f}}{A}^{1 / \alpha} \leq c \restrb{f}{A}.
\end{eqs}
Let  be such that . Then , and . 
\sprove{PowerH}
\end{proof}

\begin{note}[\uproperty{PowerH} from primitive properties]
We would rather want to prove \property{PowerH} from primitive properties, as done for other non-primitive properties in \sref{ImpliedProperties}. However, \we{} \were{} unable to come up with such a proof.
\end{note}

\begin{theorem}[\uproperty{TrivialZero} for  characterized]
\label{LocalZeroTrivialityCharacterized}
 has \property{TrivialZero} if and only if .
\end{theorem}

\begin{proof}
\proofpart{}
Suppose there exists  such that . Then  and \property{TrivialZero} does not hold.

\proofpart{}
Suppose , and . Then there exists , such that
\begin{eqs}
f \leq c 0.
\end{eqs}
Therefore .
\end{proof}

\ReLinearDominanceFromLocalLinearDominance

\begin{proof}
\proofpart{}
Suppose . Then there exists  and , such that
\begin{eqs}
{} & \restrb{f}{A} \leq c \restrb{g}{A} \\
\iffr & \frac{\restrb{f}{A}}{\restrb{g}{A}} \leq c.
\end{eqs}
Since  is finite, let
\begin{eqs}
d & = \max \bra{\frac{f}{g}}\bra{X \setminus A} \cup \setb{c}.
\end{eqs}
Then
\begin{eqs}
{} & \frac{f}{g} \leq d \\
\iffr & f \leq dg.
\end{eqs}
Therefore .

\proofpart{}
Suppose . Then there exists , such that
\begin{eqs}
f \leq cg.
\end{eqs}
Let . Then
\begin{eqs}
\restrb{f}{A} \leq c \restrb{g}{A}.
\end{eqs}
Therefore .
\end{proof}

\chapter{Proofs of limit theorems}
\label{ProofsOfLimitsTheorems}

In this section we prove the limit theorems for local linear dominance.

\ReRelationBetweenRatioLimits

\begin{proof}
\proofpart{Notation}
Let
\begin{eqs}
c \coloneqq \limsup_{\filterset{F}} \frac{\restrb{f}{F}}{\restrb{g}{F}},
\end{eqs}
and
\begin{eqs}
d \coloneqq \liminf_{\filterset{F}} \frac{\restrb{g}{F}}{\restrb{f}{F}}.
\end{eqs}

\proofpart{}
\begin{eqs}
{} & c = \infty \\
\iffr & \forall A \in \filterset{F}: \sup\image{\bra{\frac{\restr{f}{A}}{\restr{g}{A}}}}{A} = \infty \\
\iffr & \forall A \in \filterset{F}: \exists x \in A: g(x) = 0 \\
\iffr & \forall A \in \filterset{F}: \inf\image{\bra{\frac{\restr{g}{A}}{\restr{f}{A}}}}{A} = 0 \\
\iffr & d = 0.
\end{eqs}
Therefore, if , or , we have that
\begin{eqs}
c = 1 / d.
\end{eqs}

\proofpart{}
Suppose . For any , there exists , such that
\begin{eqs}
{} & \sup\image{\bra{\frac{\restr{f}{A}}{\restr{g}{A}}}}{A} \leq \epsilon \\
\impliesr & \frac{\restrb{f}{A}}{\restrb{g}{A}} \leq \epsilon \\
\impliesr & \frac{\restrb{g}{A}}{\restrb{f}{A}} \geq \frac{1}{\epsilon} \\
\impliesr & \inf\image{\bra{\frac{\restr{g}{A}}{\restr{f}{A}}}}{A} \geq \frac{1}{\epsilon} \\
\end{eqs}
Therefore,
\begin{eqs}
d \geq \frac{1}{\epsilon}.
\end{eqs}
Since this holds for all ,
\begin{eqs}
d = \infty.
\end{eqs}
Therefore, if , then , and
\begin{eqs}
c = 1 / d.
\end{eqs}

\proofpart{}
Suppose . For , there exists , such that
\begin{eqs}
{} & \inf\image{\bra{\frac{\restr{g}{A}}{\restr{f}{A}}}}{A} \geq \epsilon \\
\impliesr & \frac{\restrb{g}{A}}{\restrb{f}{A}} \geq \epsilon \\
\impliesr & \frac{\restrb{f}{A}}{\restrb{g}{A}} \leq \frac{1}{\epsilon} \\
\impliesr & \sup\image{\bra{\frac{\restr{f}{A}}{\restr{g}{A}}}}{A} \leq \frac{1}{\epsilon}.
\end{eqs}
Therefore
\begin{eqs}
c \leq \frac{1}{\epsilon}.
\end{eqs}
Since this holds for all ,
\begin{eqs}
c = 0.
\end{eqs}
Therefore, if , then , and
\begin{eqs}
c = 1 / d.
\end{eqs}

\proofpart{}
Suppose . By definition, for any , there exists , such that
\begin{eqs}
{} & \sup\image{\bra{\frac{\restr{f}{A}}{\restr{g}{A}}}}{A} - c \leq \epsilon \\
\impliesr & \frac{\restrb{f}{A}}{\restrb{g}{A}} - c \leq \epsilon \\
\impliesr & \frac{\restrb{f}{A}}{\restrb{g}{A}} \leq c + \epsilon \\
\impliesr & \frac{\restrb{g}{A}}{\restrb{f}{A}} \geq \frac{1}{c + \epsilon} \\
\impliesr & \inf\image{\bra{\frac{\restr{g}{A}}{\restr{f}{A}}}}{A} \geq \frac{1}{c + \epsilon}.
\end{eqs}
Therefore
\begin{eqs}
d \geq \frac{1}{c + \epsilon}.
\end{eqs}
Since this holds for all ,
\begin{eqs}
{} & d \geq \frac{1}{c} \\
\impliesr & c \geq 1 / d.
\end{eqs}

\proofpart{}
Suppose . By definition, for any  such that , there exists , such that
\begin{eqs}
{} & d - \inf\image{\bra{\frac{\restr{g}{A}}{\restr{f}{A}}}}{A} \leq \epsilon \\
\impliesr & d - \frac{\restrb{g}{A}}{\restrb{f}{A}} \leq \epsilon \\
\impliesr & d - \epsilon \leq \frac{\restrb{g}{A}}{\restrb{f}{A}} \\
\impliesr & \frac{1}{d - \epsilon} \geq \frac{\restrb{f}{A}}{\restrb{g}{A}} \\
\impliesr & \frac{1}{d - \epsilon} \geq \sup\image{\bra{\frac{\restr{f}{A}}{\restr{g}{A}}}}{A}.
\end{eqs}
Therefore
\begin{eqs}
c \leq \frac{1}{d - \epsilon}.
\end{eqs}
Since this holds for all ,
\begin{eqs}
c \leq \frac{1}{d}.
\end{eqs}
\end{proof}

\ReLocalLinearOByALimit

\begin{proof}
\proofpart{Notation}
Let
\begin{eqs}
d \coloneqq \limsup_{\filterset{F}} \frac{\restrb{f}{F}}{\restrb{g}{F}}.
\end{eqs}

\proofpart{}
Suppose . By definition, for any , there exists , such that , and 
\begin{eqs}
{} & \sup\image{\bra{\frac{\restr{f}{A}}{\restr{g}{A}}}}{A} - d \leq \epsilon \\
\impliesr & \frac{\restrb{f}{A}}{\restrb{g}{A}} - d \leq \epsilon \\
\impliesr & \restrb{f}{A} \leq (d + \epsilon) \restrb{g}{A}.
\end{eqs}
Therefore . By \property{Order}, , where . By \property{Local}, .

\proofpart{}
Suppose . By \property{Restrict}, . Therefore, there exists  and , such that
\begin{eqs}
{} & \restrb{f}{A} \leq c \restrb{g}{A} \\
\end{eqs}
In particular, . Therefore
\begin{eqs}
{} & \frac{\restrb{f}{A}}{\restrb{g}{A}} \leq c \\
\impliesr & \sup\image{\bra{\frac{\restr{f}{A}}{\restr{g}{A}}}}{A} \leq c.
\end{eqs}
Therefore
\begin{eqs}
d \leq c < \infty.
\end{eqs}
\end{proof}

\ReLocalLinearOmegaByALimit

\begin{proof}
This follows from \thref{LocalLinearOByALimit} and \thref{RelationBetweenRatioLimits}.
\end{proof}

\ReLocalLinearSmallOhByALimit

\begin{proof}
By definition,
\begin{eqs}
{} & f \in \rsmallohx{g} \\
\iffr & f \in \rohx{g} \land \lnot (g \in \rohx{f}) \\
\iffr & f \in \rohx{g} \land \lnot (f \in \romegahx{g}).
\end{eqs}
The first term is obtained by \thref{LocalLinearOByALimit}, and the second term is obtained by \thref{LocalLinearOmegaByALimit}.
\end{proof}

\ReTraditionalSmallOByALimit

\begin{proof}
It holds that
\begin{eqs}
{} & \forall \epsilon \in \posi{\TR}: \exists A \in \filterset{X}: \restrb{f}{A} \leq \epsilon \restrb{g}{A} \\
\iffr & \forall \epsilon \in \posi{\TR}: \exists A \in \filterset{F}: \restrb{f}{A} \leq \epsilon \restrb{g}{A} \\
\iffr & \forall \epsilon \in \posi{\TR}: \exists A \in \filterset{F}: \frac{\restrb{f}{A}}{\restrb{g}{A}} \leq \epsilon \\
\iffr & \forall \epsilon \in \posi{\TR}: \exists A \in \filterset{F}: \sup \image{\frac{\restrb{f}{A}}{\restrb{g}{A}}}{A} \leq \epsilon \\
\iffr & \forall \epsilon \in \posi{\TR}: \limsup_{\filterset{F}} \frac{\restrb{f}{F}}{\restrb{g}{F}} \leq \epsilon \\
\iffr & \limsup_{\filterset{F}} \frac{\restrb{f}{F}}{\restrb{g}{F}} = 0.
\end{eqs}
\end{proof}

\ReLocalLinearSmallOmegaByALimit

\begin{proof}
Similarly to \thref{LocalLinearSmallOhByALimit}.
\end{proof}

\chapter{Proofs of Master theorems}
\label{MasterTheorems}

In this section we will show that various Master theorems hold for the -notation as defined by linear dominance. The theorems are simpler for linear dominance than for asymptotic linear dominance; there are no ``regularity'' requirements (see \cite{IntroAlgo2009}).

\section{Master theorem over powers}
\label{MasterTheoremOverPowersSection}

\begin{definition}[Master function over powers]
Let , , , , and . A \definesub{master function}{over powers} is a function  defined by the recurrence equation

The set of such functions is denoted by .
\end{definition}

\begin{theorem}[Logarithm swap]
\label{LogarithmSwap}
Let  be such that . Then

\end{theorem}

\begin{proof}
\begin{eqs}
x^{\lgb{b}{y}} & = \bra{b^{\lgb{b}{x}}}^{\lgb{b}{y}} \\
{} & = b^{\lgb{b}{x} \lgb{b}{y}} \\
{} & = \bra{b^{\lgb{b}{y}}}^{\lgb{b}{x}} \\
{} & = y^{\lgb{b}{x}}.
\end{eqs}
\end{proof}

\begin{theorem}[Explicit form for a Master function over powers]
\label{ExplicitFormForMasterFunctionOverPowers}
Let  be a Master function over powers. Then

\end{theorem}

\begin{proof}
\proofpart{Pattern}
Expanding the recurrence, we find that
\begin{eqs}
T(1) & = d, \\
T(b) & = ad + F(b), \\
T(b^2) & = a (ad + F(b)) + F(b^2) \\
{} & = a^2 d + a F(b) + F(b^2), \\
T(b^3) & = a (a^2 d + a F(b) + F(b^2)) + F(b^3) \\
{} & = a^3 d + a^2 F(b) + a F(b^2) + F(b^3).
\end{eqs}

\proofpart{Induction}
This suggests the pattern

where . We prove this by induction. For the base case, . For the induction step, if , then
\begin{eqs}
T(b^m) & = a T(b^{m - 1}) + F(b^m) \\
{} & = a \bra{a^{m - 1} d + \sum_{i = 0}^{m - 2} a^i F(b^{m - 1 - i})} + F(b^m) \\
{} & = a^m d + \sum_{i = 0}^{m - 2} a^{i + 1} F(b^{m - 1 - i}) + F(b^m) \\
{} & = a^m d + \sum_{i = 1}^{m - 1} a^i F(b^{m - i}) + F(b^m) \\
{} & = a^m d + \sum_{i = 0}^{m - 1} a^i F(b^{m - i}).
\end{eqs}
If , then , and
\begin{eqs}
T(n) & = a^{\lgb{b}{n}} d + \sum_{i = 0}^{\lgb{b}{n} - 1} a^i F(n / b^i) \\
{} & = n^{\lgb{b}{a}} d + \sum_{i = 0}^{\lgb{b}{n} - 1} a^i F(n / b^i),
\end{eqs}
where the last step is by \proveby{LogarithmSwap}.
\end{proof}

\begin{theorem}[Difference of powers]
\label{DifferenceOfPowers}
Suppose  are such that . Then
\begin{eqs}
(n^{\beta} - n^{\alpha}) \in \lthetah{B}{n^{\beta} - 1}.
\end{eqs}
\end{theorem}

\begin{proof}
Since , we have that . Then
\begin{eqs}
(n^{\beta} - 1) (1 - b^{\alpha - \beta}) & \leq (n^{\beta} - 1) (1 - n^{\alpha - \beta}) \\
{} & \leq n^{\beta} (1 - n^{\alpha - \beta}) \\
{} & = n^{\beta} - n^{\alpha} \\
{} & \leq n^{\beta} - 1.
\end{eqs}
\end{proof}

\begin{lemma}[Master summation lemma]
\label{MasterSummationLemma}
\srequire{LinearDominance}
Let , , , and . Let  be such that
\begin{eqs}
S(n) = n^c \sum_{i = 0}^{\lgb{b}{n} - 1} (a / b^c)^i.
\end{eqs}
Then
\begin{eqs}
\lgb{b}{a} < c & \implies S \in \lthetah{B}{n^c - 1}, \\
\lgb{b}{a} = c & \implies S \in \lthetah{B}{n^c \lgb{b}{n}}, \\
\lgb{b}{a} > c & \implies S \in \lthetah{B}{n^{\lgb{b}{a}} - 1}.
\end{eqs}
\end{lemma}

\begin{proof}
\sproveby{LinearDominanceImpliesEverything}

\proofpart{}
This implies . Then
\begin{eqs}
n^c \sum_{i = 0}^{\lgb{b}{n} - 1} (a / b^c)^i = n^c \lgb{b}{n}.
\end{eqs}
Therefore .

\proofpart{}
Suppose , and let . Then
\begin{eqs}
n^c \sum_{i = 0}^{\lgb{b}{n} - 1} (a / b^c)^i & = n^c \frac{(a / b^c)^{\lgb{b}{n}} - 1}{(a / b^c) - 1} \\
{} & = \gamma n^c \bra{a^{\lgb{b}{n}} / n^c - 1} \\
{} & = \gamma \bra{a^{\lgb{b}{n}} - n^c} \\
{} & = \gamma \bra{n^{\lgb{b}{a}} - n^c}.
\end{eqs}
Therefore
\begin{eqs}
S(n) = \gamma \bra{n^{\lgb{b}{a}} - n^c}.
\end{eqs}

\proofpart{}
This implies . Then  by \thref{DifferenceOfPowers} and \require{Scale}.

\proofpart{}
This implies . Then  by \thref{DifferenceOfPowers} and \require{Scale}.
\end{proof}

\begin{theorem}[Master theorem over powers]
\label{MasterTheoremOverPowers}
\srequire{LinearDominance}
Let  be a Master function over powers, and , where . Then
\begin{eqs}
\lgb{b}{a} < c & \implies T \in \loh{B}{n^c}, \\
\lgb{b}{a} = c & \implies T \in \loh{B}{n^c \lgb{b}{bn}}, \\
\lgb{b}{a} > c & \implies T \in \lthetah{B}{n^{\lgb{b}{a}}}.
\end{eqs}
If , then each  can be replaced with .
\end{theorem}

\begin{proof}
\sproveby{LinearDominanceImpliesEverything}

\proofpart{Explicit forms}
By \proveby{ExplicitFormForMasterFunctionOverPowers},

Let  be such that
\begin{eqs}
S(n) = n^c \sum_{i = 0}^{\lgb{b}{n} - 1} (a / b^c)^i.
\end{eqs}
Since , by \require{SubsetSum}
\begin{eqs}
\loh{B}{\sum_{i = 0}^{\lgb{b}{n} - 1} a^i F(n / b^i)} & \subset \loh{B}{\sum_{i = 0}^{\lgb{b}{n} - 1} a^i (n / b^i)^c} \\
{} & = \loh{B}{n^c \sum_{i = 0}^{\lgb{b}{n} - 1} (a / b^c)^i} \\
{} & = \loh{B}{S}.
\end{eqs}

\proofpart{}
By \thref{MasterSummationLemma},
\begin{eqs}
\loh{B}{S} = \loh{B}{n^c - 1}.
\end{eqs}
By \require{Additive}, \require{Scale}, and since ,
\begin{eqs}
\loh{B}{T} & = \loh{B}{n^{\lgb{b}{a}}} + \loh{B}{\sum_{i = 0}^{\lgb{b}{n} - 1} a^i F(n / b^i)} \\
{} & \subset \loh{B}{n^{\lgb{b}{a}}} + \loh{B}{n^c - 1} \\
{} & = \loh{B}{n^{\lgb{b}{a}} + n^c - 1} \\
{} & = \loh{B}{n^c}.
\end{eqs}

\proofpart{}
By \thref{MasterSummationLemma},
\begin{eqs}
\loh{B}{S} = \loh{B}{n^c \lgb{b}{n}}.
\end{eqs}
By \require{Additive} and \require{Scale},
\begin{eqs}
\loh{B}{T} & = \loh{B}{n^c} + \loh{B}{\sum_{i = 0}^{\lgb{b}{n} - 1} a^i F(n / b^i)} \\
{} & \subset \loh{B}{n^c} + \loh{B}{n^c \lgb{b}{n}} \\
{} & = \loh{B}{n^c + n^c \lgb{b}{n}} \\
{} & = \loh{B}{n^c \lgb{b}{bn}}.
\end{eqs}

\proofpart{}
By \thref{MasterSummationLemma},
\begin{eqs}
\loh{B}{S} = \loh{B}{n^{\lgb{b}{a}} - 1}.
\end{eqs}
By \require{Additive} and \require{Scale},
\begin{eqs}
\loh{B}{T} & = \loh{B}{n^{\lgb{b}{a}}} + \loh{B}{\sum_{i = 0}^{\lgb{b}{n} - 1} a^i F(n / b^i)} \\
{} & \subset \loh{B}{n^{\lgb{b}{a}}}  + \loh{B}{n^{\lgb{b}{a}} - 1} \\
{} & = \loh{B}{2n^{\lgb{b}{a}} - 1} \\
{} & = \loh{B}{n^{\lgb{b}{a}}}.
\end{eqs}
By \require{Order}, . Therefore .

\proofpart{}
Suppose we know that . Then by \property{SubsetSum}
\begin{eqs}
\loh{B}{\sum_{i = 0}^{\lgb{b}{n} - 1} a^i F(n / b^i)} = \loh{B}{S},
\end{eqs}
and we get rid of subsets in the above proofs.
\end{proof}

\section{Master theorem over reals}

\ReMasterFunctionOverReals

\begin{theorem}[Explicit form for a Master function over reals]
\label{ExplicitFormForMasterFunctionOverReals}
Let  be a Master function over reals. Then

\end{theorem}

\begin{proof}
\proofpart{Pattern}
Suppose . Then

Suppose . Then
\begin{eqs}
t(x) & = ad + f(x).
\end{eqs}
Suppose . Then
\begin{eqs}
t(x) & = a \bra{ad + f(x / b)} + f(x) \\
{} & = a^2 d + a f(x / b) + f(x).
\end{eqs}
Suppose . Then
\begin{eqs}
t(x) & = a \bra{a^2 d + a f(x / b^2) + f(x / b)} + f(x) \\
{} & = a^3 d + a^2 f(x / b^2) + a f(x / b) + f(x).
\end{eqs}

\proofpart{Induction}
This suggests the pattern

where  is such that , which is equivalent to . That is,

This can be proved by induction, as with the analogous theorem for powers. 
\end{proof}

\begin{theorem}[Master reduction from reals to powers]
\label{MasterReductionFromRealsToPowers}
\srequire{LinearDominance}
Suppose  is a Master function over powers, and  is a Master function over reals. Then
\begin{eqs}
f \in \loh{\nonnb{\TR}{1}}{F(b^{\floorb{\lgb{b}{x}}})} & \implies t \in \loh{\nonnb{\TR}{1}}{T(b^{\floorb{\lgb{b}{x}}})}, \\
f \in \lomegah{\nonnb{\TR}{1}}{F(b^{\floorb{\lgb{b}{x}}})} & \implies t \in \lomegah{\nonnb{\TR}{1}}{T(b^{\floorb{\lgb{b}{x}}})}.
\end{eqs}
\end{theorem}

\begin{proof}
\sproveby{LinearDominanceImpliesEverything}
\proofpart{Explicit forms}
We have that
\begin{eqs}
T(b^{\floorb{\lgb{b}{x}}}) & = a^{\floorb{\lgb{b}{x}}} d + \sum_{i = 0}^{\floorb{\lgb{b}{x}} - 1} a^i F(b^{\floorb{\lgb{b}{x}}} / b^i), \\
t(x) & = a^{\floorb{\lgb{b}{x}}} d + \sum_{i = 0}^{\floorb{\lgb{b}{x}} - 1} a^i f(x / b^i),
\end{eqs}
by \proveby{ExplicitFormForMasterFunctionOverPowers} and \proveby{ExplicitFormForMasterFunctionOverReals}. 

\proofpart{-sets}
Note that
\begin{eqs}
b^{\floorb{\lgb{b}{x}}} / b^i = b^{\floorb{\lgb{b}{x / b^i}}}.
\end{eqs}
By \require{Additive} and \require{SubsetSum},
\begin{eqs}
\loh{\nonnb{\TR}{1}}{t} & = \loh{\nonnb{\TR}{1}}{a^{\floorb{\lgb{b}{x}}} d} + \loh{\nonnb{\TR}{1}}{\sum_{i = 0}^{\floorb{\lgb{b}{x}} - 1} a^i f(x / b^i)} \\
{} & \subset \loh{\nonnb{\TR}{1}}{a^{\floorb{\lgb{b}{x}}} d} + \loh{\nonnb{\TR}{1}}{\sum_{i = 0}^{\floorb{\lgb{b}{x}} - 1} a^i F(b^{\floorb{\lgb{b}{x / b^i}}})} \\
{} & = \loh{\nonnb{\TR}{1}}{T(b^{\floorb{\lgb{b}{x}}})}.
\end{eqs}
The proof for -sets is similar.
\end{proof}

\begin{lemma}[Identity equivalent]
\label{IdentityEquivalent}
\begin{eqs}
x \in \lthetah{\nonnb{\TR}{1}}{b^{\floorb{\lgb{b}{x}}}}.
\end{eqs}
\end{lemma}

\begin{proof}
\begin{eqs}
b^{\floorb{\lgb{b}{x}}} & \leq b^{\lgb{b}{x}} \\
{} & = x \\
{} & \leq b^{\floorb{\lgb{b}{x}}} b,
\end{eqs}
for all .
\end{proof}

\ReMasterTheoremOverReals

\begin{proof}
\sproveby{LinearDominanceImpliesEverything}

\proofpart{Reduction to powers}
Let  be a Master function over powers, where . By \thref{IdentityEquivalent},
\begin{eqs}
\loh{\nonnb{\TR}{1}}{x} = \loh{\nonnb{\TR}{1}}{b^{\floorb{\lgb{b}{x}}}}.
\end{eqs}
By \property{PowerH},
\begin{eqs}
\loh{\nonnb{\TR}{1}}{x^c} & = \loh{\nonnb{\TR}{1}}{b^{\floorb{\lgb{b}{x}}c}} \\
{} & = \loh{\nonnb{\TR}{1}}{F(b^{\floorb{\lgb{b}{x}}})}.
\end{eqs}
Since  by assumption,
\begin{eqs}
f \in \loh{\nonnb{\TR}{1}}{F(b^{\floorb{\lgb{b}{x}}})}.
\end{eqs}
By \proveby{MasterReductionFromRealsToPowers},
\begin{eqs}
\loh{\nonnb{\TR}{1}}{t} \subset \loh{\nonnb{\TR}{1}}{T(b^{\floorb{\lgb{b}{x}}})}.
\end{eqs}

\proofpart{}
By \thref{MasterTheoremOverPowers},
\begin{eqs}
\loh{B}{T} \subset \loh{B}{n^c}.
\end{eqs}
By \require{SubsetSum},
\begin{eqs}
\loh{\nonnb{\TR}{1}}{T(b^{\floorb{\lgb{b}{x}}})} & \subset \loh{\nonnb{\TR}{1}}{\bra{b^{\floorb{\lgb{b}{x}}}}^c} \\ 
{} & = \loh{\nonnb{\TR}{1}}{x^c}.
\end{eqs}

\proofpart{}
By \thref{MasterTheoremOverPowers},
\begin{eqs}
\loh{B}{T} \subset \loh{B}{n^c \lgb{b}{bn}}.
\end{eqs}
By \require{SubsetSum},
\begin{eqs}
\loh{\nonnb{\TR}{1}}{T(b^{\floorb{\lgb{b}{x}}})} & \subset \loh{\nonnb{\TR}{1}}{\bra{b^{\floorb{\lgb{b}{x}}}}^c \lgb{b}{b b^{\floorb{\lgb{b}{x}}}}} \\
{} & = \loh{\nonnb{\TR}{1}}{x^c \lgb{b}{bx}}.
\end{eqs}

\proofpart{}
By \thref{MasterTheoremOverPowers},
\begin{eqs}
\loh{B}{T} = \loh{B}{n^{\lgb{b}{a}}}.
\end{eqs}
By \require{SubsetSum},
\begin{eqs}
\loh{\nonnb{\TR}{1}}{t} & = \loh{\nonnb{\TR}{1}}{\bra{b^{\floorb{\lgb{b}{x}}}}^{\lgb{b}{a}}} \\ 
{} & = \loh{\nonnb{\TR}{1}}{x^{\lgb{b}{a}}}.
\end{eqs}

\proofpart{}
If , then the subsets in the above proofs can be replaced with equalities by \thref{MasterTheoremOverPowers}.
\end{proof}

\section{Master theorem over integers}

\ReMasterFunctionOverIntegers

\begin{note}[Stricter requirement]
The requirement  is stricter than the requirement  for the other Master theorems; this is needed to avoid the recursion getting stuck to a fixed-point . 
\end{note}

\begin{definition}[Ceiling division]
The \define{ceiling division} is a function   such that . 
\end{definition}

\begin{definition}[Ceiling division number]
The \define{ceiling division number} is a function  such that .
\end{definition}

\begin{theorem}[Fixed points of the ceiling division]
\label{FixedPointsOfCeilingDivision}
Let  and

Then
\begin{eqs}
F = \setb{n \in \nonnb{\TN}{1} : 1 \leq n < b / (b - 1)}.
\end{eqs}
In addition, .
\end{theorem}

\begin{proof}
To characterize the fixed points,
\begin{eqs}
{} & n \in F \\
\iffr & \ceilb{n / b} = n \\
\iffr & \exists v \in \TR : -b < v \leq 0 \textrm{ and } nb + v = n \\
\iffr & \exists v \in \TR : -b < v \leq 0 \textrm{ and } v = n(1 - b) \\
\iffr & -b < n(1 - b) \leq 0 \\
\iffr & b / (b - 1) > n \geq 0 \\
\iffr & 0 \leq n < b / (b - 1) \\
\iffr & 1 \leq n < b / (b - 1),
\end{eqs}
where we use a version of Euclidean division and the last step follows because .
Based on this characterization,
\begin{eqs}
{} & F = \setb{1} \\
\iffr & b / (b - 1) \leq 2 \\
\iffr & b \leq 2 (b - 1) \\
\iffr & 0 \leq b - 2 \\
\iffr & b \geq 2.
\end{eqs}
Let us also note that
\begin{eqs}
{} & b / (b - 1) \leq b \\
\iffr & b \leq b (b - 1) \\
\iffr & 1 \leq b - 1 \\
\iffr & b \geq 2.
\end{eqs}
\end{proof}

\begin{theorem}[Explicit form for a Master function over integers]
\label{ExplicitFormForMasterFunctionOverIntegers}
Let  be a Master function over integers. Then

\end{theorem}

\begin{proof}
The function  is well-defined by \proveby{FixedPointsOfCeilingDivision}, since . Let . 

\proofpart{Pattern}
Suppose . Then

Suppose . Then
\begin{eqs}
T(n) & = a T(\ceilb{n / b}) + F(n) \\
{} & = a T(N^{(1)}(n)) + F(n) \\
{} & = a d + F(n).
\end{eqs}
Suppose . Then
\begin{eqs}
T(n) & = a T(N^{(1)}(n)) + F(n) \\
{} & = a \bra{a T(\ceilb{N^{(1)}(n) / b}) + F(N^{(1)}(n))} + F(n) \\
{} & = a^2 T(N^{(2)}(n)) + a F(N^{(1)}(n)) + F(n) \\
{} & = a^2 d + a F(N^{(1)}(n)) + F(n).
\end{eqs}
Suppose . Then
\begin{eqs}
T(n) & = a^2 T(N^{(2)}(n)) + a F(N^{(1)}(n)) + F(n) \\
{} & = a^2 \bra{a T(\ceilb{N^{(2)}(n) / b}) + F(N^{(2)}(n))} + a F(N^{(1)}(n)) + F(n) \\
{} & = a^3 T(N^{(3)}(n)) + a^2 F(N^{(2)}(n)) + a F(N^{(1)}(n)) + F(n) \\
{} & = a^3 d + a^2 F(N^{(2)}(n)) + a F(N^{(1)}(n)) + F(n).
\end{eqs}

\proofpart{Induction}
This suggests the pattern

This can be proved by induction, as in the explicit form for powers.
\end{proof}

\begin{theorem}[Bounds for ]
\label{BoundsForN}
Let , and . Then
\begin{eqs}
n / b^i & \leq N^{(i)}(n) < n / b^i + 2.
\end{eqs}
\end{theorem}

\begin{proof}
\proofpart{Pattern}
It holds that  for all . Therefore,
\begin{eqs}
N^{(1)}(n) & = \ceilb{n / b} \\
{} & < (n / b) + 1 \\
N^{(2)}(n) & = \ceilb{N^{(1)}(n) / b} \\
{} & \leq \ceilb{((n / b) + 1) / b} \\
{} & = \ceilb{n / b^2 + 1 / b} \\
{} & < n / b^2 + 1 / b + 1 \\
N^{(3)}(n) & = \ceilb{N^{(2)}(n) / b} \\
{} & \leq \ceilb{n / b^3 + 1 / b^2 + 1 / b} \\
{} & < n / b^3 + 1 / b^2 + 1 / b + 1.
\end{eqs}

\proofpart{Induction}
This suggests the pattern

This can be proved by induction. Since ,
\begin{eqs}
N^{(i)}(n) & < n / b^i + \frac{1 - (1 / b)^i}{1 - (1 / b)} \\
{} & < n / b^i + \frac{1}{1 - (1 / b)}.
\end{eqs}
Since , 

Similarly, since , for all , it can be proved that .
\end{proof}

\begin{note}[Ceiling division pitfall]
It holds that , for all  and . A potential pitfall in the proof of \proveby{BoundsForN} is to assume that this also holds when . A counterexample is given by  and .
\end{note}

\begin{theorem}[Bounds for ]
\label{BoundsForM}
Let , and . Then
\begin{eqs}
\floorb{\lgb{b}{n}} & \leq M(n) \leq \floorb{\lgb{b}{n}} + 2.
\end{eqs}
\end{theorem}

\begin{proof}
By \thref{BoundsForN},
\begin{eqs}
N^{(\floorb{\lgb{b}{n}} + 1)}(n) & < \frac{n}{b^{\floorb{\lgb{b}{n}} + 1}} + 2 \\
{} & < \frac{n}{b^{\lgb{b}{n}}} + 2 \\
{} & = \frac{n}{n} + 2 \\
{} & = 3.
\end{eqs}
This is equivalent to . Since , it follows that . Similarly, it follows that .
\end{proof}

\begin{theorem}[Multiplicative bounds for ]
\label{MultiplicativeBoundsForN}
Let , , and . Then

\end{theorem}

\begin{proof}
We would like to find  such that
\begin{eqs}
{} & n / b^i + 2 \leq \beta (n / b^i) \\
\iff \; & \beta \geq 1 + 2b^i / n,
\end{eqs}
For the given argument-sets,
\begin{eqs}
1 + 2b^i / n & \leq 1 + 2b^{\floorb{\lgb{b}{n}} + 1} / n \\
{} & \leq 1 + 2b^{\lgb{b}{n} + 1} / n \\
{} & = 1 + 2b \\
{} & \leq b + 2b \\
{} & = 3b.
\end{eqs}
Therefore  suffices. Then
\begin{eqs}
n / b^i & \leq N^{(i)}(n) \\
{} & < n / b^i + 2 \\
{} & \leq 3b (n / b^i),
\end{eqs}
by \proveby{BoundsForN}.
\end{proof}

\ReMasterTheoremOverIntegers

\begin{proof}
\sproveby{LinearDominanceImpliesEverything}

\proofpart{Explicit form}
By \thref{ExplicitFormForMasterFunctionOverIntegers},


\proofpart{First term}
By \proveby{BoundsForM},
\begin{eqs}
{} & \floorb{\lgb{b}{n}} \leq M(n) \leq \floorb{\lgb{b}{n}} + 2 \\
\impliesr & a^{\floorb{\lgb{b}{n}}} d \leq a^{M(n)} d \leq a^{\floorb{\lgb{b}{n}} + 2} d \\
\impliesr & a^{\floorb{\lgb{b}{n}}} d \leq a^{M(n)} d \leq a^{\floorb{\lgb{b}{n}}} a^2 d \\
\end{eqs}
By \require{Order} and \require{Scale},
\begin{eqs}
\loh{\nonnb{\TN}{1}}{a^{M(n)} d} = \loh{\nonnb{\TN}{1}}{a^{\floorb{\lgb{b}{n}}}}.
\end{eqs}

\proofpart{Sum term}
Since , by \require{SubsetSum}
\begin{eqs}
{} & \loh{\nonnb{\TN}{1}}{\sum_{i = 0}^{M(n) - 1} a^i F(N^{(i)}(n))} \\
\air{\subset} & \loh{\nonnb{\TN}{1}}{\sum_{i = 0}^{M(n) - 1} a^i (N^{(i)}(n))^c}.
\end{eqs}
By \proveby{MultiplicativeBoundsForN},
\begin{eqs}
(n / b^i) \leq N^{(i)}(n) < 3b (n / b^i),
\end{eqs}
for all . By \require{Order} and \require{Scale},
\begin{eqs}
{} & \loh{\nonnb{\TN}{1}}{\sum_{i = 0}^{M(n) - 1} a^i (N^{(i)}(n))^c} \\
\air{=} & \loh{\nonnb{\TN}{1}}{\sum_{i = 0}^{M(n) - 1} a^i (n/b^i)^c}.
\end{eqs}

\proofpart{Combined terms}
We combine the first term and the sum term by \require{Additive}:
\begin{eqs}
\loh{\nonnb{\TN}{1}}{T} \subset \loh{\nonnb{\TN}{1}}{a^{\floorb{\lgb{b}{n}}} + \sum_{i = 0}^{M(n) - 1} a^i (n/b^i)^c}.
\end{eqs}
By \proveby{BoundsForM},
\begin{eqs}
\floorb{\lgb{b}{n}} \leq M(n) \leq \floorb{\lgb{b}{n}} + 2.
\end{eqs}
Therefore, by \require{Order},
\begin{eqs}
{} & \loh{\nonnb{\TN}{1}}{a^{\floorb{\lgb{b}{n}}} + \sum_{i = 0}^{\floorb{\lgb{b}{n}} - 1} a^i (n/b^i)^c} \\
\air{\subset} & \loh{\nonnb{\TN}{1}}{a^{\floorb{\lgb{b}{n}}} + \sum_{i = 0}^{M(n) - 1} a^i (n/b^i)^c} \\
\air{\subset} & \loh{\nonnb{\TN}{1}}{a^{\floorb{\lgb{b}{n}}} + \sum_{i = 0}^{\floorb{\lgb{b}{n}} + 1} a^i (n/b^i)^c}.
\end{eqs}
The last two terms in the sum are given by
\begin{eqs}
{} & a^{\floorb{\lgb{b}{n}}} (n / b^{\floorb{\lgb{b}{n}}})^c + a^{\floorb{\lgb{b}{n}} + 1} (n / b^{\floorb{\lgb{b}{n}} + 1})^c \\
\air{=} & a^{\floorb{\lgb{b}{n}}} (n / b^{\floorb{\lgb{b}{n}}})^c (1 + a / b^c) \\
\air{\in} & \lthetah{\nonnb{\TN}{1}}{a^{\floorb{\lgb{b}{n}}}}.
\end{eqs}
Therefore,
\begin{eqs}
{} & \loh{\nonnb{\TN}{1}}{a^{\floorb{\lgb{b}{n}}} + \sum_{i = 0}^{M(n) - 1} a^i (n/b^i)^c} \\
\air{=} & \loh{\nonnb{\TN}{1}}{a^{\floorb{\lgb{b}{n}}} + \sum_{i = 0}^{\floorb{\lgb{b}{n}} - 1} a^i (n/b^i)^c}.
\end{eqs}

\proofpart{Reduction to reals}
Let  be a Master function over reals, where  is such that . By \thref{ExplicitFormForMasterFunctionOverReals},
\begin{eqs}
t(x) = a^{\floorb{\lgb{b}{x}}} + \sum_{i = 0}^{\floorb{\lgb{b}{x}} - 1} a^i (x/b^i)^c.
\end{eqs}
Therefore,
\begin{eqs}
\loh{\nonnb{\TN}{1}}{T} \subset \loh{\nonnb{\TN}{1}}{\restr{t}{\nonnb{\TN}{1}}}.
\end{eqs}
The results follow from \thref{MasterTheoremOverReals}. 

\proofpart{-sets}
If , then we can get rid of the subsets in the above proof.
\end{proof}

\chapter{Comparison of definitions}
\label{CandidateDefinitions}

In this section we will study various candidate definitions for the -notation. This highlights the ways in which some familiar candidate definitions fail the primitive properties. The properties of the candidate definitions, proved in this section, are summarized upfront in \taref{tabcomparison}. 

\begin{table}
\small
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline 
Property &
 &
 &
 &
 &

\\
\hline 
\hline 
\textbf{\uproperty{Order}} & 
\multicolumn{4}{l|}{\checkmark \ref{LocalOrderConsistency}} &
\checkmark \ref{AffineOrderConsistency}
\\
\hline 
\uproperty{Reflex} &
\multicolumn{5}{l|}{\checkmark \ref{ReflexivityIsImplied}}
\\
\hline 
\textbf{\uproperty{Trans}} &
\multicolumn{4}{l|}{\checkmark \ref{LocalTransitivity}} &
\checkmark \ref{AffineTransitivity}
\\
\hline 
\uproperty{Orderness} &
\multicolumn{5}{l|}{\checkmark \ref{OrdernessIsImplied}} \\
\hline 
\hline 
\uproperty{Zero} &
\multicolumn{4}{l|}{\checkmark \ref{ZeroSeparationIsImplied}} & 
\xmark \ref{AffineZeroSeparationFails}
\\
\hline 
\uproperty{TrivialZero} &
\checkmark \ref{LocalZeroTrivialityCharacterized} & 
\xmark \ref{LocalZeroTrivialityCharacterized} &
\xmark \ref{LocalZeroTrivialityCharacterized} &
\xmark \ref{LocalZeroTrivialityCharacterized} &
\xmark \ref{ZeroTrivialityFailsForAffineDominance}
\\
\hline 
\textbf{\uproperty{One}} &
\checkmark \ref{LinearOneSeparation} &
\checkmark \ref{CofiniteOneSeparation} &
\checkmark \ref{CoasymptoticOneSeparation} &
\checkmark \ref{AsymptoticOneSeparation} &
\checkmark \ref{AffineOneSeparation}
\\
\hline 
\hline 
\textbf{\uproperty{Scale}} &
\multicolumn{4}{l|}{\checkmark \ref{LocalScaleInvariance}} &
\checkmark \ref{AffinePositiveScaleInvariance}
\\
\hline 
\uproperty{ScalarHom} &
\multicolumn{5}{l|}{\checkmark \ref{ScalarHomogenuityIsImplied}} \\
\hline 
\textbf{\uproperty{NSubHom}} &
\multicolumn{4}{l|}{\checkmark \ref{LocalSubHomogenuity}} & \xmark \ref{AffineSubHomogenuityFails} \\
\hline 
\textbf{\usproperty{NSubDiv}} &
\multicolumn{4}{l|}{\checkmark \ref{LocalSubHomogenuity}} & \checkmark \ref{AffineSubHomogeneityNDiv} \\
\hline 
\uproperty{QSubHom} &
\multicolumn{4}{l|}{\checkmark \ref{QSubhomogenuityIsComposite}} & \xmark \ref{AffineSubHomogenuityFails} \\
\hline 
\uproperty{SubHom} &
\multicolumn{4}{l|}{\checkmark \ref{RSubhomogenuityIsImplied}} & \xmark \ref{AffineSubHomogenuityFails} \\
\hline 
\uproperty{SuperHom} &
\checkmark \ref{LocalSuperHomogenuityCharacterized} & \xmark \ref{LocalSuperHomogenuityCharacterized} & \xmark \ref{LocalSuperHomogenuityCharacterized} & \xmark \ref{LocalSuperHomogenuityCharacterized} & \xmark \ref{AffineSuperHomogenuityFails} \\
\hline 
\uproperty{PowerH} &
\multicolumn{4}{l|}{\checkmark \ref{LocalPowerHomogenuity}} &
\checkmark \ref{AffinePowerHomogenuity}
\\
\hline 
\uproperty{AddCons} &
\multicolumn{5}{l|}{\checkmark \ref{AdditiveConsistencyIsImplied}} \\
\hline 
\usproperty{MultiCons} &
\multicolumn{5}{l|}{\checkmark \ref{MultiplicativeConsistencyIsImplied}} \\
\hline 
\uproperty{MaxCons} &
\multicolumn{5}{l|}{\checkmark \ref{MaximumConsistencyIsImplied}} \\
\hline 
\hline 
\textbf{\uproperty{Local}} &
\multicolumn{4}{l|}{\checkmark \ref{LocalLocality}} & 
\checkmark \ref{AffineLocality}
\\
\hline 
\uproperty{SubMulti} &
\multicolumn{4}{l|}{\checkmark \ref{SubMultiplicativityIsImplied}} & 
\xmark \ref{AffineSubHomogenuityFails}
\\
\hline 
\uproperty{SuperMulti} &
\multicolumn{4}{l|}{\checkmark \ref{LocalSuperMultiplicativity}} & 
\checkmark \ref{AffineSuperMultiplicativity}
\\
\hline 
\uproperty{SubRestrict} &
\multicolumn{5}{l|}{\checkmark \ref{SubRestrictabilityIsImplied}} \\
\hline 
\uproperty{SuperRestrict} &
\multicolumn{5}{l|}{\checkmark \ref{SuperRestrictabilityIsImplied}} \\
\hline 
\uproperty{Maximum} &
\multicolumn{5}{l|}{\checkmark \ref{MaximumIsImplied}} \\
\hline 
\uproperty{Summation} &
\multicolumn{5}{l|}{\checkmark \ref{SummationIsImplied}} \\
\hline 
\uproperty{MaximumSum} &
\multicolumn{5}{l|}{\checkmark \ref{MaximumSumIsImplied}} \\
\hline 
\uproperty{Additive} &
\multicolumn{5}{l|}{\checkmark \ref{AdditivityIsImplied}} \\
\hline 
\usproperty{Translation} &
\multicolumn{5}{l|}{\checkmark \ref{TranslationInvarianceIsImplied}} \\
\hline 
\textbf{\uproperty{SubComp}} &
\checkmark \ref{LinearSubComposability} &
\xmark \ref{CofiniteSubComposabilityFails} &
\xmark \ref{CoasymptoticInjectiveSubComposabilityFails} &
\xmark \ref{AsymptoticInjectiveSubComposabilityFails} &
\checkmark \ref{AffineSubComposability}
\\
\hline 
\usproperty{ISubComp} &
\checkmark \ref{LinearSubComposability}  &
\checkmark \ref{CofiniteInjectiveComposition} &
\xmark \ref{CoasymptoticInjectiveSubComposabilityFails} &
\xmark \ref{AsymptoticInjectiveSubComposabilityFails} &
\checkmark \ref{AffineSubComposability}
\\
\hline 
\usproperty{ISuperComp} &
\checkmark \ref{InjectiveSuperComposabilityIsImplied} &
\checkmark \ref{InjectiveSuperComposabilityIsImplied} &
? &
? &
\checkmark \ref{InjectiveSuperComposabilityIsImplied}
\\
\hline 
\uproperty{Extend} &
\checkmark \ref{LinearExtensibility} &
\xmark \ref{CofiniteExtensibilityFails} &
\xmark \ref{CoasymptoticExtensibilityFails} &
\checkmark \ref{AsymptoticExtensibility} &
\checkmark \ref{AffineExtensibility}
\\
\hline 
\uproperty{SubsetSum} &
\checkmark \ref{SubsetSumIsAnOMapping} &
\xmark \ref{SubsetSumImpliesSubComposability} &
\xmark \ref{SubsetSumImpliesSubComposability} &
\xmark \ref{SubsetSumImpliesSubComposability} &
?
\\
\hline 
\hline 
Universe &
sets &
sets &
 &
 &
sets
\\
\hline 
\end{tabular}
\centering
\caption{Comparison of -notations. The \checkmark and \xmark mean that we have proved and disproved, respectively, the property. The following number refers to the corresponding theorem. Primitive properties are marked with a bold face, and .}
\label{tabcomparison}
\end{table}

\section{Trivial linear dominance}

\begin{definition}[Trivial linear dominance]
\defineexp{Trivial linear dominance}{linear dominance!trivial}  is defined by
\begin{eqs}
\trohx{f} = \rc{X},
\end{eqs}
for all , and all , where  is the class of all sets.
\end{definition}

\begin{theorem}[Trivial linear dominance is local linear dominance]
\label{TrivialLinearDominanceIsLocalLinearDominance}
The family  is a family of filter bases with induced sub-structure.
\end{theorem}

\begin{proof}
\proofpart{Directedness}
Let , where . Then , and we may choose , so that . 

\proofpart{Induced sub-structure}
Let , where . Then .  	
\end{proof}

\begin{theorem}[\uproperty{One} fails for ]
\label{TrivialOneSeparationFails}
 does not have \property{One}.
\end{theorem}

\begin{proof}
It holds that .
\end{proof}

\begin{theorem}[\uproperty{SubComp} for ]
\label{TrivialSubComposability}
 has \property{SubComp}.
\end{theorem}

\begin{proof}
The right side of the implication
\begin{eqs}
(f \circ s) \in \oh{Y}{g \circ s}
\end{eqs}
holds by the definition of trivial dominance for all  and .
\end{proof}

\begin{note}[Trivial linear dominance has lots of nice properties?]
Trivial linear dominance satisfies all of the desirable properties, except those of non\-/triviality. This underlines the importance of the non\-/triviality properties.
\end{note}

\begin{note}[Equivalent definitions]
Suppose local linear dominance is defined so that , for all . Then it is equivalent to trivial linear dominance.
\end{note}

\section{Asymptotic linear dominance}
\label{AsymptoticLinearDominance}

Recall the definition of asymptotic linear dominance from \sref{Introduction}:
\defineasymptotic

\begin{theorem}[Asymptotic linear dominance is local linear dominance]
\label{AsymptoticLinearDominanceIsLocalLinearDominance}
The family  is a family of filter bases with induced sub-structure.
\end{theorem}

\begin{proof}
\proofpart{Directedness}
Let , and . Then there exist , such that , and . Let , and . Then , and . 

\proofpart{Induced sub-structure}
Let . Then
\begin{eqs}
\filterset{D} & = \setb{\nonnb{D}{y} : y \in \TR^d} \\
{} & = \setb{\nonnb{X}{y} \cap D : y \in \TR^d}.
\end{eqs}
\end{proof}

\begin{note}[Intrinsic pitfall]
\label{AsymptoticIntrinsicPitfall}
Let us consider a variant of asymptotic linear dominance, where , when . We call this an \emph{intrinsic} definition --- in contrast to the extrinsic () definition we have given. Then  is not a filter basis, and the definition fails many properties.

An example is given by , , and . Then , but there is no  such that .
\end{note}

\begin{theorem}[\uproperty{One} for ]
\label{AsymptoticOneSeparation}
.
\end{theorem}

\begin{proof}
 has \property{One} if and only if  by \proveby{LocalOneSeparationCharacterized}. Let  and . Then there exists  such that . Let  be such that . Let . Then
\begin{eqs}
{} \quad & f(x_1) = f(x_2) \\
{} \iffr &  x_1 - y = x_2 - y \\
{} \iffr &  x_1 = x_2.
\end{eqs}
Therefore  is injective. Let . Then . Therefore  is surjective. Since  is a bijection between  and , . Therefore \prove{One} holds for .
\end{proof}

\begin{theorem}[\uproperty{SubComp} for  for fixed ]
\label{AsymptoticCompositionForFixedS}
Let ,  be such that ,  be such that , and . Then \property{SubComp} holds for  and  if and only if

\end{theorem}

\begin{proof}
Substitute the filter bases of  into \proveby{LocalSubComposabilityForFixedS}.
\end{proof}

\begin{theorem}[\uproperty{ISubComp} fails for  in ]
\label{AsymptoticInjectiveSubComposabilityFails}
 does not have \property{ISubComp} from  to .
\end{theorem}

\begin{proof}
Let , , , , and  be such that . Then . Since  is non-empty, . \uproperty{ISubComp} fails by \thref{AsymptoticCompositionForFixedS}.
\end{proof}

\begin{theorem}[\uproperty{ISubComp} fails for  in ]
\label{AsymptoticInjectiveSubComposabilityFailsInZ}
 does not have \property{ISubComp} from  to .
\end{theorem}

\begin{proof}
Let , , , and  be such that . Then , for all . Since  is non-empty, . \uproperty{ISubComp} fails by \thref{AsymptoticCompositionForFixedS}.
\end{proof}

\begin{theorem}[\uproperty{Extend} for ]
\label{AsymptoticExtensibility}
 has \prove{Extend}.
\end{theorem}

\begin{proof}
Let  and . Then , and . The result follows by \thref{LocalExtensibilityCharacterized}. \sprove{Extend}
\end{proof}

\section{Co-asymptotic linear dominance}
\label{CoasymptoticLinearDominance}

Recall the definition of co-asymptotic linear dominance from \sref{Introduction}:
\definecoasymptotic

\begin{theorem}[Co-asymptotic linear dominance is local linear dominance]
\label{CoasymptoticLinearDominanceIsLocalLinearDominance}
The family  is a family of filter bases with induced sub-structure.
\end{theorem}

\begin{proof}
\proofpart{Directedness}
Let , and . Then there exists , such that  and . Let , and . Then  and . 

\proofpart{Induced sub-structure}
Let . Then
\begin{eqs}
\filterset{D} & = \setb{\nlb{D}{y} : y \in \TR^d} \\
{} & = \setb{\nlb{X}{y} \cap D : y \in \TR^d}. 
\end{eqs}
\end{proof}

\begin{note}[Intrinsic pitfall]
The same intrinsic pitfall as described in \nref{AsymptoticIntrinsicPitfall} for asymptotic linear dominance also holds for co-asymptotic linear dominance.
\end{note}

\begin{theorem}[\uproperty{One} for ]
\label{CoasymptoticOneSeparation}
.
\end{theorem}

\begin{proof}
The proof is the same as in \proveby{AsymptoticOneSeparation}, since co-asymptotic linear dominance is equivalent to asymptotic linear dominance in .
\end{proof}

\begin{theorem}[Composability for  for fixed ]
\label{CoasymptoticCompositionForFixedS}
Let ,  be such that ,  be such that , and . Then composability holds for  and  if and only if

\end{theorem}

\begin{proof}
Substitute the filter bases of  into \proveby{LocalSubComposabilityForFixedS}.
\end{proof}

\begin{theorem}[\uproperty{ISubComp} fails for  in ]
\label{CoasymptoticInjectiveSubComposabilityFails}
 does not have \property{ISubComp}.
\end{theorem}

\begin{proof}
Let , , , and  be such that . Then , for all . Since  is non-empty, . \uproperty{ISubComp} fails by \thref{CoasymptoticCompositionForFixedS}.
\end{proof}

\begin{theorem}[\uproperty{Extend} fails for ]
\label{CoasymptoticExtensibilityFails}
 does not have \property{Extend}.
\end{theorem}

\begin{proof}
Let . For any , , but . \uproperty{Extend} fails by \thref{LocalExtensibilityCharacterized}.
\end{proof}

\begin{theorem}[ is equal to  in ]
\label{CoasymptoticIsEquivalentToCofiniteInN}
\begin{eqs}
\cohs{X} = \fohs{X},
\end{eqs}
for all .
\end{theorem}

\begin{proof}
Let , , , and . Then , and therefore . Let , and . Then , and therefore .
\end{proof}

\begin{theorem}[\uproperty{IComp} holds for  in ]
\label{CoasymptoticInjectiveComposability}
 has \property{IComp} in .
\end{theorem}

\begin{proof}
This follows from \proveby{CoasymptoticIsEquivalentToCofiniteInN} and \proveby{CofiniteInjectiveComposition}.
\end{proof}

\section{Cofinite linear dominance}
\label{CofiniteLinearDominance}

Recall the definition of cofinite linear dominance from \sref{Introduction}:
\definecofinite

\begin{theorem}[Cofinite linear dominance is local linear dominance]
\label{CofiniteLinearDominanceIsLocalLinearDominance}
The family  is a family of filter bases with induced sub-structure.
\end{theorem}

\begin{proof}
\proofpart{Directedness}
Let , where . Then we may choose , so that . 

\proofpart{Induced sub-structure}
Let , where . If , then , and . If , then . 
\end{proof}

\begin{note}[Fr\'echet filter]
The filter basis for cofinite linear dominance is also known as the Fr\'echet filter.
\end{note}

\begin{theorem}[\uproperty{One} for ]
\label{CofiniteOneSeparation}
 has \property{One}.
\end{theorem}

\begin{proof}
 has \property{One} if and only if  by \proveby{LocalOneSeparationCharacterized}. Let  and . Let  be such that . Then  is a bijection between  and , and so . Therefore \prove{One} holds for .
\end{proof}

\begin{theorem}[\uproperty{SubComp} for  for fixed ]
\label{CofiniteCompositionForFixedS}
Let , and . Then  has \prove{SubComp} for  if and only if  is finite-to-one:

\end{theorem}

\begin{proof}
The  has \property{SubComp} for  if and only if

by \proveby{LocalSubComposabilityForFixedS}. Substituting the filter bases of , this is equivalent to

We will show equivalence to this formula.

\proofpart{}
Suppose there exists  such that . Then , and therefore . Let , and . Then , and  , which is equivalent to . Therefore ;  does not have \property{SubComp} for .

\proofpart{}
Let  be such that , and . Then , and
\begin{eqs}
|Y \setminus A_Y| & = |Y \setminus \preimage{s}{A_X}| \\
{} & = |\preimage{s}{X \setminus A_X}| \\
{} & = |\preimage{s}{\setb{x_1, \dots, x_n}}| \\
{} & \leq |\preimage{s}{\setb{x_1}}| + \cdots + |\preimage{s}{\setb{x_n}}| \\
{} & \in \TN.
\end{eqs}
Therefore ;  has \prove{SubComp} for .
\end{proof}

\begin{theorem}[\uproperty{SubComp} for  for positive ]
\label{CofiniteCompositionPositive}
Let , , and . Then

\end{theorem}

\begin{proof}
It holds that , for all , . The claim follows from \thref{LocalSubComposabilityForPositiveFunctions}.
\end{proof}

\begin{theorem}[\uproperty{SubComp} fails for ]
\label{CofiniteSubComposabilityFails}
 does not have \property{SubComp}.
\end{theorem}

\begin{proof}
Let  be such that

Then , and so \property{SubComp} fails for  and  by \proveby{CofiniteCompositionForFixedS}. For example, let  be such that  and  be such that  and . Then .
\end{proof}

\begin{theorem}[\uproperty{SubsetSum} fails for ]
\label{CofiniteSubsetSumFails}
 does not have \property{SubsetSum}.
\end{theorem}

\begin{proof}
Since \property{SubsetSum} implies \property{SubComp}, and \property{SubComp} does not hold by \thref{CofiniteSubComposabilityFails}, neither does \property{SubsetSum}.
\end{proof}

\begin{theorem}[\uproperty{IComp} for ]
\label{CofiniteInjectiveComposition}
 has \prove{IComp}.
\sprove{ISubComp}
\end{theorem}

\begin{proof}
Let , and  be injective. Then , for all . Therefore  has \property{SubComp} for  by \proveby{CofiniteCompositionForFixedS}. 
\srequire{SubComp} \sprove{ISubComp} 
Since  has \property{Order} by \proveby{LocalOrderConsistency}, and \property{Local} by \proveby{LocalLocality},  has \property{ISuperComp} by \proveby{InjectiveSuperComposabilityIsImplied}.
\sprove{IComp}
\end{proof}

\begin{theorem}[\uproperty{Extend} fails for ]
\label{CofiniteExtensibilityFails}
 does not have \property{Extend}.
\end{theorem}

\begin{proof}
Let . For any , , but . \uproperty{Extend} fails by \thref{LocalExtensibilityCharacterized}.
\end{proof}

\section{Linear dominance}
\label{LinearDominance}

Recall the definition of linear dominance from \sref{Introduction}:
\definelinear

\begin{theorem}[Linear dominance is local linear dominance]
\label{LinearDominanceIsLocalLinearDominance}
The family  is a family of filter bases with induced sub-structure.
\end{theorem}

\begin{proof}
\proofpart{Directedness}
Let , where . Then , and we may choose , so that . 

\proofpart{Induced sub-structure}
Let , where . Then .
\end{proof}

\begin{note}[]
The primitive properties of linear dominance are proved in \sref{SufficientDefinition}.
\end{note}

\begin{theorem}[\uproperty{Extend} for ]
\label{LinearExtensibility}
 has \prove{Extend}.
\end{theorem}

\begin{proof}
Let  and . Then  and . Since , the result follows by \thref{LocalExtensibilityCharacterized}. \sprove{Extend}
\end{proof}

\section{Affine dominance}
\label{AffineDominance}

Recall the definition of affine dominance from \sref{Introduction}:
\defineaffine

\begin{note}[]
Affine dominance is not a local linear dominance. 
\end{note}

We shall apply the following lemma without mentioning it, since it is used in almost every proof.

\begin{theorem}[Simplification lemma for ]
\label{AffineSingleConstant}
Let  be a finite set, , , and , for all . Then there exists , such that

for all .
\end{theorem}

\begin{proof}
There exists , such that , for all .
Let . Then , for all .
\end{proof}

\begin{theorem}[\uproperty{Order} for ]
\label{AffineOrderConsistency}
Let , and . Then 

\sprove{Order}
\end{theorem}

\begin{proof}
Let . Then there exists  such that . Since , it follows that . Therefore .
\sprove{Order}
\end{proof}

\begin{theorem}[\uproperty{Trans} for ]
\label{AffineTransitivity}
Let , and . Then 

\sprove{Trans}
\end{theorem}

\begin{proof}
Let , and . Then there exists , such that  and . It follows that
\begin{eqs}
f & \lt c (c h + c) + c \\
{} & = c^2 h + (c^2 + c) \\
{} & \leq (c^2 + c) h + (c^2 + c).
\end{eqs}
Therefore .
\sprove{Trans}
\end{proof}

\begin{theorem}[\uproperty{Local} for ]
\label{AffineLocality}
Let , , and  be a finite cover of . Then

\sprove{Local}
\end{theorem}

\begin{proof}
Assume , for all . Since  is finite, there exists , such that , for all . Since  covers , . Therefore .
\sprove{Local}
\end{proof}

\begin{theorem}[\uproperty{Zero} fails for ]
\label{AffineZeroSeparationFails}
Let  be non-empty. Then

\end{theorem}

\begin{proof}
 has \property{Order} by \proveby{AffineOrderConsistency}, \property{Trans} by \proveby{AffineTransitivity}, and \property{Orderness} by \proveby{OrdernessIsImplied}. It holds that , for all . Therefore . By \require{Orderness}, . By \require{Order}, . By \require{Orderness}, .
\end{proof} 

\begin{theorem}[\uproperty{One} for ]
\label{AffineOneSeparation}

\sprove{One}
\end{theorem}

\begin{proof}
It holds that , for all , and . Therefore . 
\sprove{One}
\end{proof}

\begin{theorem}[\uproperty{Scale} for ]
\label{AffinePositiveScaleInvariance}
Let , , and . Then 

\sprove{Scale}
\end{theorem}

\begin{proof}
\proofpart{}
Assume . Then there exists , such that
\begin{eqs}
\hat{f} & \lt c (\alpha f) + c \\
{} & \lt \max(c \alpha, c) f + \max(c \alpha, c).
\end{eqs}
Therefore . 

\proofpart{}
Assume . Then there exists , such that
\begin{eqs}
\hat{f} & \lt c f + c \\
{} & = (c / \alpha) (\alpha f) + c \\
{} & \lt \max(c / \alpha, c) (\alpha f) + \max(c / \alpha, c).
\end{eqs}
Therefore .

\sprove{Scale}
\end{proof}

\begin{theorem}[Composability for ]
\label{AffineSubComposability}
Let , , and . Then

\sprove{SubComp}
\end{theorem}

\begin{proof}
Let . Then there exists  such that . This implies . Therefore .

\sprove{SubComp}
\end{proof}

\begin{theorem}[\uproperty{SubHom} for  characterized]
\label{AffineSubHomogenuityCharacterization}

for all .
\end{theorem}

\begin{proof}
 has \property{Order} by \proveby{AffineOrderConsistency}.

\proofpart{}
Suppose . Let  be such that . Then  by \require{Order}, and , since , for all . Then . Therefore .

\proofpart{}
Suppose  and . Then there exists  such that
\begin{eqs}
f & \leq cfg + c \\
\hat{g} & \leq cg + c.
\end{eqs}
Therefore
\begin{eqs}
f\hat{g} & \leq f(cg + c) \\
{} & = cfg + cf \\
{} & \leq cfg + c\bra{cfg + c} \\
{} & \leq (c^2 + c) fg + c^2 \\
{} & \leq (c^2 + c) fg + (c^2 + c).
\end{eqs}
Therefore .
\end{proof}

\begin{theorem}[\uproperty{NSubHom} fails for ]
\label{AffineSubHomogenuityFails}
There exists  such that , and

\end{theorem}

\begin{proof}
Let  be such that , and  be such that . Then
\begin{eqs}
f\bra{\ceilb{3c}} & = \ceilb{3c} \\
{} & > c + c \\
{} & = c f\bra{\ceilb{3c}} g\bra{\ceilb{3c}} + c,
\end{eqs}
for all . Therefore , and so \property{NSubHom} fails by \thref{AffineSubHomogenuityCharacterization}.
\end{proof}

\begin{theorem}[\uproperty{NSubDiv} for ]
\label{AffineSubHomogeneityNDiv}
Let , and  be such that . Then \sprove{NSubDiv}

\end{theorem}

\begin{proof}
Let . Then there exists , such that
\begin{eqs}
\hat{f} \leq c f + c.
\end{eqs}
This implies
\begin{eqs}
u \hat{f} & \leq u(cf + c) \\
{} & = c (uf) + uc \\
{} & \leq c (uf) + c.
\end{eqs}
Therefore .
\sprove{NSubDiv}
\end{proof}

\begin{theorem}[\uproperty{Extend} for ]
\label{AffineExtensibility}
 has \prove{Extend}.
\end{theorem}

\begin{proof}
Suppose . Then there exists  such that
\begin{eqs}
f \leq cg + c.
\end{eqs}
Then
\begin{eqs}
f \circ \projections{X}{Y} \leq c \bra{g \circ \projections{X}{Y}} + c.
\end{eqs}
Therefore  has \prove{Extend}.
\end{proof}

\begin{theorem}[\uproperty{SuperHom} fails for ]
\label{AffineSuperHomogenuityFails}
There exists  such that

\end{theorem}

\begin{proof}
Let , and  be such that . Then , and so . For all  it holds that . Therefore .
\end{proof}

\begin{theorem}[\uproperty{SuperMulti} for ]
\label{AffineSuperMultiplicativity}
Let . Then

\sprove{SuperMulti}
\end{theorem}

\begin{proof}
Let . Then there exists  such that

Let  be such that . Since , it holds that . Then
\begin{eqs}
\frac{\hat{h}}{\hat{g}} & \leq cf \frac{g}{\hat{g}} + \frac{c}{\hat{g}} \\
{} & \leq cf + c.
\end{eqs}
Let  be such that . By the above, . In addition, . Therefore .
\sprove{SuperMulti}
\end{proof}

\begin{theorem}[\uproperty{PowerH} for ]
\label{AffinePowerHomogenuity}
Let , , and . Then

\end{theorem}

\begin{proof}
 has \property{Local} by \proveby{AffineLocality}.

\proofpart{}
Let . Then there exists  such that

Let . Since positive powers are increasing,
\begin{eqs}
\hat{f}^{\alpha} & \leq \bra{c f + c}^{\alpha} \\
{} & = c^{\alpha} \bra{f + 1}^{\alpha}.
\end{eqs}
Let . Then for ,
\begin{eqs}
\bra{f(x) + 1}^{\alpha} & \leq \bra{f(x) + f(x)}^{\alpha} \\
{} & = 2^{\alpha} f(x)^{\alpha}.
\end{eqs}
Let . Then for ,
\begin{eqs}
\bra{f(x) + 1}^{\alpha} & \leq \bra{1 + 1}^{\alpha} \\
{} & = 2^{\alpha}.
\end{eqs}
Therefore,
\begin{eqs}
f & \leq 2^{\alpha} (f^{\alpha} + 1) \\
{} & = 2^{\alpha} f^{\alpha} + 2^{\alpha}.
\end{eqs}
It follows that
\begin{eqs}
\hat{f}^{\alpha} & \leq c^{\alpha} 2^{\alpha} (f^{\alpha} + 1),
\end{eqs}
which shows that .

\proofpart{}
Let , and . Then there exists  such that

Since positive powers are increasing,
\begin{eqs}
\hat{g}^{1 / \alpha} & \leq \bra{c f^{\alpha} + c}^{1 / \alpha} \\
{} & = c^{1 / \alpha} (f^{\alpha} + 1)^{1 / \alpha}.
\end{eqs}
Similarly to above, we can prove that . Therefore, let  be such that . Then  and . Therefore .
\end{proof}

\begin{theorem}[Subset\-/sum rule for  in some cases]
\label{AffineSubsetSummabilityInSomeCases}
Let , , , , , and . Then \prove{SubsetSum} holds for  if

\end{theorem}

\begin{proof}
Let  such that

There exists  such that . It follows that
\begin{eqs}
T(\hat{f}) & \leq c T(f) + c \sum_{(y, z) \in S_x} a(z) \\
{} & \leq c T(f) + c M \\
{} & \leq \max(c M, c) T(f) + \max(c M, c).
\end{eqs}
Therefore , and so \prove{SubsetSum} holds.
\end{proof}

\begin{note}[Subset-sum rule for ]
It is open to us to characterize when exactly \property{SubsetSum} holds for . By \proveby{AffineContainment} we would expect \property{SubsetSum} to fail, since \property{SubsetSum} does not hold for  either.
\end{note}

\begin{theorem}[\uproperty{TrivialZero} fails for ]
\label{ZeroTrivialityFailsForAffineDominance}
 does not have \property{TrivialZero}.
\end{theorem}

\begin{proof}
It holds that
\begin{eqs}
1 \leq 1 \cdot 0 + 1.
\end{eqs}
Therefore , and .
\end{proof}

\section{Containment relations}

Analysis of local linear dominance reveals that making the filter basis sets larger increases the number of fulfilled desirable properties --- provided that at least \property{One} is satisfied. 

Trivial linear dominance has the smallest filter basis sets. It satisfies all of the desirable properties except those of non\-/triviality. Its worst defect is the failure of \property{One}. In fact, every function in  is equivalent for every . 

Asymptotic linear dominance has the next smallest filter basis sets. Its worst defect is the failure of \property{SubComp} in . 

Co-asymptotic linear dominance improves upon asymptotic linear dominance by satisfying \property{ISubComp} in . Its worst defects are the failure of \property{SubComp} in , and the failure of \property{ISubComp} in . 

Cofinite linear dominance improves upon coasymptotic linear dominance by satisfying \property{ISubComp} in all universes. Its worst defect is the failure of \property{SubComp} in .

Linear dominance has the largest filter basis sets, and fulfills all of the desirable properties. We formalize the intuitive size-comparison of the filter basis sets by the following theorem.

\begin{theorem}[Containment in ]
\label{Containment}

for all  and .
\end{theorem}

\begin{proof}
Assume  and .

\proofpart{}
Suppose . Then there exists  such that . Since , it holds that . Therefore .

\proofpart{}
Suppose . Then there exists  and  such that . Let . Then , and so . Therefore .

\proofpart{}
Suppose . Then there exist  and  such that . Since , it holds that . Therefore .

\proofpart{}
Suppose . Then , since every function is equivalent under trivial linear dominance.
\end{proof}

Affine dominance has surprisingly good properties; \we{} \were{} especially surprised to be able to prove \property{PowerH} for it. Its worst defect is the failure of \property{SubHom}. In terms of containment, affine dominance has smaller filter bases than cofinite linear dominance.

\begin{theorem}[Affine containment]
\label{AffineContainment}

for all  and .
\end{theorem}

\begin{proof}
Let . Then there exists  and  such that . Since  is cofinite, let . Then . Therefore .
\end{proof}



















\chapter{Proofs of minimality}
\label{ProofsOfMinimality}

In this section we consider additional candidate definitions. These definitions are used --- together with those in \sref{CandidateDefinitions} --- to prove the minimality of pre-primitive properties in \sref{Characterization}.

\section{Elementwise dominance}
\label{ElementwiseDominance}

\begin{definition}[Elementwise dominance]
\defineexp{Elementwise dominance}{dominance!elementwise}  is defined by  if and only if
\begin{eqs}
f \leq g,
\end{eqs}
for all , and all , where  is the class of all sets.
\end{definition}

\begin{theorem}[\uproperty{Order} for ]
\label{ElementwiseOrder}
 has \prove{Order}.
\end{theorem}

\begin{proof}
By definition. \sprove{Order}
\end{proof}

\begin{theorem}[\uproperty{Trans} for ]
\label{ElementwiseTransivity}
 has \prove{Trans}.
\end{theorem}

\begin{proof}
Let , , and . Then
\begin{eqs}
{} & f \leq g, \\
{} & g \leq h.
\end{eqs}
Therefore
\begin{eqs}
f \leq h.
\end{eqs}
\sprove{Trans}
\end{proof}

\begin{theorem}[\uproperty{Scale} fails for ]
\label{ElementwiseScaleInvarianceFails}
 does not have \property{Scale}.
\end{theorem}

\begin{proof}
Let  be such that  and  be such that . Let  be such that . Then
\begin{eqs}
f(p) > \alpha f(p).
\end{eqs}
\end{proof}

\begin{theorem}[\uproperty{Local} for ]
\label{ElementwiseLocality}
 has \prove{Local}.
\end{theorem}

\begin{proof}
Let  be a finite cover of , and suppose  for all . Then
\begin{eqs}
\restrb{f}{D} \leq \restrb{g}{D},
\end{eqs}
for all . This implies
\begin{eqs}
f \leq g.
\end{eqs}
\sprove{Local}
\end{proof}

\begin{theorem}[\uproperty{One} for ]
\label{ElementwiseOneSeparation}
 has \prove{One}.
\end{theorem}

\begin{proof}
It holds that   for . \sprove{One}
\end{proof}

\begin{theorem}[\uproperty{SubHom} for ]
\label{ElementwiseSubHomogeneity}
 has \prove{SubHom}. \sprove{SubHomN} \sprove{SubDivN}
\end{theorem}

\begin{proof}
Let , and . Then
\begin{eqs}
{} & f \leq g.
\end{eqs}
This implies
\begin{eqs}
uf \leq ug.
\end{eqs}
\sprove{SubHom} \sprove{SubHomN} \sprove{SubDivN}
\end{proof}

\begin{theorem}[\uproperty{SubComp} for ]
\label{ElementwiseSubComposability}
 has \prove{SubComp}.
\end{theorem}

\begin{proof}
Let , and . Then
\begin{eqs}
{} & f \leq g.
\end{eqs}
This implies
\begin{eqs}
f \circ s \leq g \circ s.
\end{eqs}
\sprove{SubComp}
\end{proof}

\section{Multiple dominance}
\label{MultipleDominance}

\begin{definition}[Multiple dominance]
\defineexp{Multiple dominance}{dominance!multiple}  is defined by  if and only if there exists  such that
\begin{eqs}
f = cg,
\end{eqs}
for all , and all , where  is the class of all sets.
\end{definition}

\begin{theorem}[\uproperty{Order} fails for ]
\label{MultipleOrderFails}
 does not have \property{Order}.
\end{theorem}

\begin{proof}
Let  be such that  and . Then , and
\begin{eqs}
0 = f \neq cg = c,
\end{eqs}
for all .
\end{proof}

\begin{theorem}[\uproperty{Trans} for ]
\label{MultipleTransivity}
 has \prove{Trans}.
\end{theorem}

\begin{proof}
Let , , and . Then there exists  such that
\begin{eqs}
{} & f = cg, \\
{} & g = dh.
\end{eqs}
Therefore
\begin{eqs}
f = cd h.
\end{eqs}
\sprove{Trans}
\end{proof}

\begin{theorem}[\uproperty{Scale} for ]
\label{MultipleScaleInvariance}
 has \property{Scale}.
\end{theorem}

\begin{proof}
Let , and . Then there exists  such that
\begin{eqs}
f = cg.
\end{eqs}
This implies
\begin{eqs}
f = c \frac{1}{\alpha} \alpha g.
\end{eqs}
\end{proof}

\begin{theorem}[\uproperty{Local} fails for ]
\label{MultipleLocalityFails}
 does not have \property{Local}.
\end{theorem}

\begin{proof}
Let  be such that
\begin{eqs}
f(n) = 
\begin{cases}
2, & n > 0, \\
1, & n = 0.
\end{cases}
\end{eqs}
Let  be such that . Then there is no  such that .
\end{proof}

\begin{theorem}[\uproperty{One} for ]
\label{MultipleOneSeparation}
 has \prove{One}.
\end{theorem}

\begin{proof}
It holds that  for , for all . \sprove{One}
\end{proof}

\begin{theorem}[\uproperty{SubHom} for ]
\label{MultipleSubHomogeneity}
 has \prove{SubHom}. \sprove{SubHomN} \sprove{SubDivN}
\end{theorem}

\begin{proof}
Let , and . Then there exists  such that
\begin{eqs}
f = cg.
\end{eqs}
This implies
\begin{eqs}
uf = cug.
\end{eqs}
\sprove{SubHom} \sprove{SubHomN} \sprove{SubDivN}
\end{proof}

\begin{theorem}[\uproperty{SubComp} for ]
\label{MultipleSubComposability}
 has \prove{SubComp}.
\end{theorem}

\begin{proof}
Let , and . Then there exists  such that
\begin{eqs}
f = cg.
\end{eqs}
This implies
\begin{eqs}
(f \circ s) = c (g \circ s).
\end{eqs}
\sprove{SubComp}
\end{proof}

\section{Non-transitive dominance}
\label{NonTransitiveDominance}

\begin{definition}[Non-transitive dominance]
\defineexp{Non-transitive dominance}{dominance!non-transitive}  is defined by  if and only if 
\begin{eqs}
\bra{\exists c \in \posi{\TR}: f = cg} \lor \bra{f \leq 2g},
\end{eqs}
for all , and all , where  is the class of all sets.
\end{definition}

\begin{theorem}[\uproperty{Order} for ]
\label{NonTransitiveOrder}
 has \property{Order}.
\end{theorem}

\begin{proof}
Let  be such . Then
\begin{eqs}
f \leq 2g.
\end{eqs}
\end{proof}

\begin{theorem}[\uproperty{Trans} fails for ]
\label{NonTransitiveTransivityFails}
 does not have \property{Trans}.
\end{theorem}

\begin{proof}
Let , and  be such that , , and
\begin{eqs}
f(x) =
\begin{cases}
4 & x \neq p, \\
3 & \text{otherwise}.
\end{cases}
\end{eqs}
Then
\begin{eqs}
f & \leq 2g = 4 \\
g & \leq 2h = 2,
\end{eqs}
so that  and . However, , and there is no  such that . Therefore .
\end{proof}

\begin{theorem}[\uproperty{Scale} for ]
\label{NonTransitiveScaleInvariance}
 has \property{Scale}.
\end{theorem}

\begin{proof}
Let . Then
\begin{eqs}
f = \frac{1}{\alpha} \alpha f.
\end{eqs}
\end{proof}

\begin{theorem}[\uproperty{Local} fails for ]
\label{NonTransitiveLocalityFails}
 does not have \property{Local}.
\end{theorem}

\begin{proof}
Let  be such that , and
\begin{eqs}
f(n) = 
\begin{cases}
4, & n > 0, \\
3, & n = 0.
\end{cases}
\end{eqs}
Then , and . Therefore  and . However, , and there is no  such that .
\end{proof}

\begin{theorem}[\uproperty{One} for ]
\label{NonTransitiveOneSeparation}
 has \prove{One}.
\end{theorem}

\begin{proof}
\begin{eqs}
{} & n \in \noh{\posi{\TN}}{1} \\
\iffr & \bra{\exists c \in \posi{\TR}: \forall n \in \posi{\TN}: n = c} \lor \bra{\forall n \in \posi{\TN}: n \leq 2},
\end{eqs}
which is false.\sprove{One}
\end{proof}

\begin{theorem}[\uproperty{SubHom} for ]
\label{NonTransitiveSubHomogeneity}
 has \prove{SubHom}. \sprove{SubHomN} \sprove{SubDivN}
\end{theorem}

\begin{proof}
Let , and . Suppose there exists  such that
\begin{eqs}
f = cg.
\end{eqs}
Then
\begin{eqs}
uf = cug.
\end{eqs}
Suppose . Then
\begin{eqs}
uf \leq 2(ug).
\end{eqs}
\sprove{SubHom} \sprove{SubHomN} \sprove{SubDivN}
\end{proof}

\begin{theorem}[\uproperty{SubComp} for ]
\label{NonTransitiveSubComposability}
 has \prove{SubComp}.
\end{theorem}

\begin{proof}
Let , and . Suppose there exists  such that . Then
\begin{eqs}
f \circ s = c (g \circ s).
\end{eqs}
Suppose . Then
\begin{eqs}
f \circ s \leq 2(g \circ s).
\end{eqs}
\sprove{SubComp}
\end{proof}

\section{Power dominance}
\label{PowerDominance}

\begin{definition}[Clamped power]
The \define{clamped -power}, where , is a function  such that
\begin{eqs}
\clpower{x}{k} =
\begin{cases}
x^k & x \geq 1, \\
x & x < 1.
\end{cases}
\end{eqs}
\end{definition}

\begin{definition}[Power dominance]
\defineexp{Power dominance}{dominance!power}  is defined by  if and only if there exists  and  such that
\begin{eqs}
f \leq c\clpower{g}{k},
\end{eqs}
for all , and all , where  is the class of all sets.
\end{definition}

\begin{theorem}[\uproperty{Order} for ]
\label{PowerOrder}
 has \prove{Order}.
\end{theorem}

\begin{proof}
Suppose . Then
\begin{eqs}
f \leq 1 \clpower{g}{1} = g.
\end{eqs}
\sprove{Order}
\end{proof}

\begin{theorem}[\uproperty{Trans} for ]
\label{PowerTransivity}
 has \prove{Trans}.
\end{theorem}

\begin{proof}
Let , , and . Then there exists  such that
\begin{eqs}
{} & f \leq c \clpower{g}{k}, \\
{} & g \leq c \clpower{h}{k}.
\end{eqs}
We may assume . Then
\begin{eqs}
f \leq c \clpower{(c \clpower{h}{k})}{k}.
\end{eqs}

\proofpart{Small values}
Suppose  is such that . Then
\begin{eqs}
f(x) & \leq c^2 \clpower{h(x)}{k} \\
{} & \leq c^{k + 1} \clpower{h(x)}{k^2}.
\end{eqs}

\proofpart{Large values}
Suppose  is such that  and . Then
\begin{eqs}
f(x) & \leq c (c h(x)^k)^k \\
{} & = c^{k + 1} h(x)^{k^2} \\
{} & \leq c^{k + 1} \clpower{h(x)}{k^2}.
\end{eqs}
Suppose  is such that  and . Then
\begin{eqs}
f(x) & \leq c (c h(x))^k \\
{} & = c^{k + 1} h(x)^k \\
{} & \leq c^{k + 1} \clpower{h(x)}{k^2}.
\end{eqs}
\sprove{Trans}
\end{proof}

\begin{theorem}[\uproperty{Scale} for ]
\label{PowerScaleInvariance}
 has \property{Scale}.
\end{theorem}

\begin{proof}
Let  and . Then
\begin{eqs}
f & = \frac{1}{\alpha} \alpha f \\
{} & = \frac{1}{\alpha} \clpower{(\alpha f)}{1}.
\end{eqs}
\end{proof}

\begin{theorem}[\uproperty{Local} for ]
\label{PowerLocality}
 has \prove{Local}.
\end{theorem}

\begin{proof}
Let  be a finite cover of , and suppose  for all . Then there exists  and  such that
\begin{eqs}
\restrb{f}{D} \leq c_D \clpower{\restrb{g}{D}}{k_D},
\end{eqs}
for all . Let  and . Then
\begin{eqs}
f \leq c \clpower{g}{k}.
\end{eqs}
\sprove{Local}
\end{proof}

\begin{theorem}[\uproperty{One} for ]
\label{PowerOneSeparation}
 has \prove{One}.
\end{theorem}

\begin{proof}
Let . Then  for . \sprove{One}
\end{proof}

\begin{lemma}[Clamped power lemma]
\label{ClampedPowerLemma}
\begin{eqs}
u\clpower{g}{k} \leq \clpower{(ug)}{k},
\end{eqs}
for all , .
\end{lemma}

\begin{proof}
\proofpart{}
Since , it holds that . Then
\begin{eqs}
u \clpower{g}{k} & = u g \\
{} & = \clpower{(ug)}{k}.
\end{eqs}

\proofpart{ and }
Then
\begin{eqs}
u \clpower{g}{k} & = ug \\
{} & \leq (ug)^k \\
{} & = \clpower{(ug)}{k}.
\end{eqs}

\proofpart{ and }
Then
\begin{eqs}
u \clpower{g}{k} & = u g^k \\
{} & \leq u^k g^k \\
{} & = (ug)^k \\
{} & = \clpower{(ug)}{k}.
\end{eqs}
\end{proof}

\begin{theorem}[\uproperty{NSubHom} for ]
\label{PowerSubHomogeneity}
 has \prove{NSubHom}. 
\end{theorem}

\begin{proof}
Let , and  be such that . Then there exists  and  such that
\begin{eqs}
f \leq c \clpower{g}{k}.
\end{eqs}
This implies
\begin{eqs}
uf & \leq cu \clpower{g}{k} \\
{} & \leq c \clpower{(ug)}{k},
\end{eqs}
where we used \proveby{ClampedPowerLemma}.
\sprove{NSubHom} 
\end{proof}

\begin{theorem}[\uproperty{NSubDiv} fails for ]
\label{PowerSubHomogeneityDivNFails}
 does not have \property{NSubDiv}. 
\end{theorem}

\begin{proof}
Let  be such that
\begin{eqs}
f(n) & = n^2, \\
g(n) & = n, \\
u(n) & = n.
\end{eqs}
Then . Let  and . Then the inequality
\begin{eqs}
n = f / u \leq d \clpower{\bra{g / u}}{m} = d
\end{eqs}
fails for .
\end{proof}

\begin{theorem}[\uproperty{SubComp} for ]
\label{PowerSubComposability}
 has \prove{SubComp}.
\end{theorem}

\begin{proof}
Let , and . Then there exists  and  such that
\begin{eqs}
f \leq c \clpower{g}{k}.
\end{eqs}
This implies
\begin{eqs}
(f \circ s) \leq c \clpower{(g \circ s)}{k}.
\end{eqs}
\sprove{SubComp}
\end{proof}

\chapter{Partitioned sets}
\label{PartitionedSets}

In this section we develop some theory of partitioned sets. Partitioned sets occur in the theory of -notation, because the equality of -sets in a set  ---  --- is an equivalence relation in .\footnote{An equivalence relation is a reflexive, symmetric, and transitive relation.} 

\begin{definition}[Partitioned set]
A \define{partitioned set} is a set  with an associated equivalence  in .
\end{definition}

\begin{note}[Conventions]
Let  and  be partitioned sets. We will often shorten the word \emph{partition} into a single letter p.
\end{note}

\begin{definition}[Partition\-/preserving]
A function  is \define{p-preserving} if
\begin{eqs}
x_1 \preeq x_2 \implies f(x_1) \preeqb f(x_2),
\end{eqs}
for all .
\end{definition}

\begin{note}[Homomorphisms]
The p-preserving functions are the homomorphisms of partitioned sets; they preserve the partition structure. \end{note}

\begin{definition}[Partition closure]
The \define{p-closure} on  is a function  such that

\end{definition}

\newcommand{\blackdot}[1]{\filldraw (#1) circle (0.2)}
\newcommand{\whitedot}[1]{\filldraw[white, draw=black] (#1) circle (0.2)}

\begin{figure}
\centering
\begin{tikzpicture}[every node/.style={draw,rounded corners,thick}]
        \node (bot) at (0, 0) {};
        \node (sur) at (2.5, 1.5) {p-surjection};
        \node (bij) at (5, 3) {p-bijection};
        \node (inj) at (7.5, 1.5) {p-injection};
        \node (p_inj) at (7.5, 6) {p-embedding};
        \node (p_bij) at (5, 7.5) {p-preserving p-bijection};
        \node (p_sur) at (2.5, 6) {p-preserving p-surjection};
        \node (p) at (0, 4.5) {p-preserving function};
        
        \draw[semithick] (bot) -- node[right=10, draw=none] {\ref{PNothing}} (sur);
        
        \draw[semithick] (sur) -- node[right=10, draw=none] {\ref{PSurjectiveNotPPreservingOrPInjective}} (bij);
        
        \draw[semithick] (p_inj) -- node[right=10, draw=none] {\ref{PInjectiveAndPPreservingNotPSurjective}} (p_bij);

        \draw[semithick] (bot.east) -- node[right=20, draw=none] {\ref{PNothing}} (inj);
        
        \draw[semithick, dashed] (inj) -- node[right=10, draw=none] {\ref{PBijectiveNotPPreserving}} (p_inj);

        \draw[semithick, dashed] (bot) -- node[right, draw=none] {\ref{PBijectiveNotPPreserving}} (p);

        \draw[semithick] (p) -- node[left=10, draw=none] {\ref{PPreservingNotPInjectiveOrPSurjective}} (p_sur);

        \draw[semithick] (p_sur) -- node[left=10, draw=none] {\ref{PSurjectiveAndPPreservingNotPInjective}} (p_bij);

        \draw[semithick] (p.east) -- node[left=45, draw=none] {\ref{PPreservingNotPInjectiveOrPSurjective}} (p_inj);

        \draw[semithick] (inj) -- node[left=10, draw=none] {\ref{PInjectiveNotPPreservingOrPSurjective}} (bij);

        \draw[semithick, dashed] (sur) -- node[right, draw=none] {\ref{PBijectiveNotPPreserving}} (p_sur);

        \draw[semithick, dashed] (bij) -- node[right, draw=none] {\ref{PBijectiveNotPPreserving}} (p_bij);
\end{tikzpicture}
\caption{A Hasse diagram of functions between partitioned sets, ordered by the `generalizes' partial order. The edge labels --- which refer to \fref{PartitionPreservingProper} --- show that the generalizations are proper.}
\label{PartitionPreservingDiagram}
\end{figure}

\begin{figure}
\centering
\hfill
\subfloat[Partition\-/bijective, and with two left p-inverses, but not p\-/preserving and only one generalized p-inverse.]
{
\makebox[.4\textwidth]{
\begin{tikzpicture}[scale = 0.5]
\blackdot{0, 2};
\blackdot{0, 0};
\blackdot{4, 0};
\blackdot{4, 2};
\draw[dashed] (0, 2) -- (4, 2);
\draw[dashed] (0, 0) -- (4, 0);
\draw (0, 0) -- (0, 2);
\end{tikzpicture}
}
\label{PBijectiveNotPPreserving}
}
\hfill
\subfloat[Partition\-/surjective, but not partition\-/preserving or partition\-/injective.]
{
\makebox[.4\textwidth]{
\begin{tikzpicture}[scale = 0.5]
\blackdot{0, 1};
\blackdot{0, 0};
\blackdot{4, 0};
\blackdot{4, 2};
\blackdot{0, 2};
\draw[dashed] (0, 1) -- (4, 2);
\draw[dashed] (0, 0) -- (4, 0);
\draw[dashed] (0, 2) -- (4, 2);
\draw (0, 1) -- (0, 0);
\end{tikzpicture}
}
\label{PSurjectiveNotPPreservingOrPInjective}
}
\hfill\null

\hfill
\subfloat[Partition\-/injective, but not partition\-/preserving or partition\-/surjective.]
{
\makebox[.4\textwidth]{
\begin{tikzpicture}[scale = 0.5]
\blackdot{0, 2};
\blackdot{0, 0};
\blackdot{4, 0};
\blackdot{4, 1};
\blackdot{4, 2};
\draw[dashed] (0, 2) -- (4, 2);
\draw[dashed] (0, 0) -- (4, 0);
\draw (0, 0) -- (0, 2);
\end{tikzpicture}
}
\label{PInjectiveNotPPreservingOrPSurjective}
}
\hfill
\subfloat[Not partition\-/injective, not partition\-/preserving, and not partition\-/surjective.]
{
\makebox[.4\textwidth]{
\begin{tikzpicture}[scale = 0.5]
\blackdot{0, 1};
\blackdot{0, 2};
\blackdot{0, 0};
\blackdot{4, 0};
\blackdot{4, 1};
\blackdot{4, 2};
\draw[dashed] (0, 1) -- (4, 2);
\draw[dashed] (0, 0) -- (4, 0);
\draw[dashed] (0, 2) -- (4, 2);
\draw (0, 0) -- (0, 1);
\end{tikzpicture}
}
\label{PNothing}
}
\hfill\null

\hfill
\subfloat[Partition\-/surjective, partition\-/preserving, and with two generalized p-inverses, but not partition\-/injective or with left p-inverse.]
{
\makebox[.4\textwidth]{
\begin{tikzpicture}[scale = 0.5]
\blackdot{0, 0};
\blackdot{0, 1};
\blackdot{4, 0};
\blackdot{4, 2};
\blackdot{0, 2};
\draw[dashed] (0, 2) -- (4, 2);
\draw[dashed] (0, 0) -- (4, 0);
\draw[dashed] (0, 1) -- (4, 0);
\end{tikzpicture}
}
\label{PSurjectiveAndPPreservingNotPInjective}
}
\hfill
\subfloat[Partition\-/injective, and partition\-/preserving, but not partition\-/surjective.]
{
\makebox[.4\textwidth]{
\begin{tikzpicture}[scale = 0.5]
\blackdot{0, 0};
\blackdot{0, 2};
\blackdot{4, 0};
\blackdot{4, 1};
\blackdot{4, 2};
\draw[dashed] (0, 2) -- (4, 2);
\draw[dashed] (0, 0) -- (4, 0);
\end{tikzpicture}
}
\label{PInjectiveAndPPreservingNotPSurjective}
}
\hfill\null

\hfill
\subfloat[Partition\-/preserving, but not partition\-/injective or partition\-/surjective.]
{
\makebox[.4\textwidth]{
\begin{tikzpicture}[scale = 0.5]
\blackdot{0, 0};
\blackdot{0, 2};
\blackdot{0, 1};
\blackdot{4, 0};
\blackdot{4, 1};
\blackdot{4, 2};
\draw[dashed] (0, 2) -- (4, 2);
\draw[dashed] (0, 0) -- (4, 0);
\draw[dashed] (0, 1) -- (4, 0);
\end{tikzpicture}
}
\label{PPreservingNotPInjectiveOrPSurjective}
}
\hfill
\subfloat[Partition\-/surjective, with four left p-inverses, and four right p-inverses, but not partition\-/injective.]
{
\makebox[.4\textwidth]{
\begin{tikzpicture}[scale = 0.5]
\blackdot{0, 0};
\blackdot{0, 1};
\blackdot{0, 2};
\blackdot{4, 0};
\blackdot{4, 1};
\blackdot{4, 2};
\draw[dashed] (0, 2) -- (4, 2);
\draw[dashed] (0, 1) -- (4, 1);
\draw[dashed] (0, 0) -- (4, 0);
\draw (4, 0) -- (4, 1);
\draw (0, 1) -- (0, 2);
\end{tikzpicture}
}
\label{PSurjectiveWithPInversesNotPInjective}
}
\hfill\null
\caption{Diagrams to show that the various definitions of functions on partitioned sets are not equivalent. Equivalent dots are connected with a solid line.}
\label{PartitionPreservingProper}
\end{figure}

\section{Generalized partition-inverse}

\begin{definition}[Generalized partition\-/inverse]
A \define{generalized p-inverse} of  is  such that , and
\begin{eqs}
f(\ginv{f}{f(x)}) \preeqb f(x),
\end{eqs}
for all .
\end{definition}

\begin{theorem}[Construction for a generalized p-inverse]
\label{ConstructionForGeneralizedInverse}
Let \hfill \\ . Then  is a generalized p-inverse of  if and only if
\begin{eqs}
\hat{f}(y) \in \preimage{f}{\pclosure{\setb{y}}},
\end{eqs}
for all .
\end{theorem}

\begin{proof}
It holds that
\begin{eqs}
{} & \hat{f}(f(x)) \in \preimage{f}{\pclosure{\setb{f(x)}}} \\
\iffr & f(\hat{f}(f(x))) \in \pclosure{\setb{f(x)}} \\
\iffr & f(\hat{f}(f(x))) \preeqb f(x),
\end{eqs}
for all .
\end{proof}

\begin{note}[Generalized partition\-/inverse exists]
\thref{ConstructionForGeneralizedInverse} shows that a generalized p-inverse always exists.
\end{note}

\begin{note}[Generalized partition\-/inverse may not be unique]
\fref{PSurjectiveWithPInversesNotPInjective} shows that a function  can have many non-equivalent generalized p-inverses.
\end{note}

\section{Partition-injectivity and left partition-inverse}

\begin{definition}[Partition\-/injectivity]
A function  is \define{p-injective}, if 
\begin{eqs}
f(x_1) \preeqb f(x_2) \implies x_1 \preeq x_2, 
\end{eqs}
for all .
\end{definition}

\begin{definition}[Left partition\-/inverse]
A \define{left p-inverse} of  is a function  such that  and
\begin{eqs}
\linv{f}{f(x)} \preeq x,
\end{eqs}
for all .
\end{definition}

\begin{theorem}[Generalized partition\-/inverse is a left partition\-/inverse for a partition\-/injective function]
\label{GeneralizedPInverseIsLeftPInverseForPInjective}
Let  be p-injective. Then a generalized p-inverse of  is a left p-inverse of .
\end{theorem}

\begin{proof}
Let  be a generalized p-inverse of . By definition,
\begin{eqs}
f(\ginv{f}{f(x)}) \preeqb f(x),
\end{eqs}
for all . Since  is p-injective,
\begin{eqs}
\ginv{f}{f(x)} \preeq x,
\end{eqs}
for all . Therefore  is a left p-inverse of .
\end{proof}

\begin{note}[Generalized partition\-/inverse may not be a left partition\-/inverse]
\fref{PSurjectiveAndPPreservingNotPInjective} shows that a generalized p-inverse may not be a left p-inverse.
\end{note}

\begin{theorem}[Left partition\-/inverse is a generalized partition\-/inverse for a partition\-/preserving function]
\label{LeftPInverseIsGeneralizedPInverseForPPreserving}
Let  be p-preserving. Then a left p-inverse of  is a generalized p-inverse of .
\end{theorem}

\begin{proof}
Let  be a left p-inverse of . By definition,
\begin{eqs}
\linv{f}{f(x)} \preeq x,
\end{eqs}
for all . Since  is p-preserving,
\begin{eqs}
f(\linv{f}{f(x)}) \preeqb f(x),
\end{eqs}
for all . Therefore  is a generalized p-inverse of .
\end{proof}

\begin{note}[Left partition\-/inverse may not be a generalized partition\-/inverse]
\fref{PBijectiveNotPPreserving} shows a non-p-preserving function which has two left p-inverses, but only one generalized p-inverse; a left p-inverse may not be a generalized p-inverse.
\end{note}

\begin{theorem}[Partition\-/injectivity is equivalent to having a p-preserving left p-inverse]
\label{PInjectivityIsEquivalentToPPreservingLeftInverse}
Let . Then  is p-injective if and only if  has a p-preserving left p-inverse.
\end{theorem}

\begin{proof}
\proofpart{}
Let  be such that
\begin{eqs}
\hat{f}(y) \in \preimage{f}{\pclosure{\setb{y}}}.
\end{eqs}
Then  is a generalized p-inverse of  by \thref{ConstructionForGeneralizedInverse}. Since  is p-injective,  is a left p-inverse by \thref{GeneralizedPInverseIsLeftPInverseForPInjective}. It holds that
\begin{eqs}
{} & x_1, x_2 \in \preimage{f}{\pclosure{\setb{y}}} \\
\impliesr & f(x_1), f(x_2) \in \pclosure{\setb{y})} \\
\impliesr & f(x_1) \preeqb y \preeqb f(x_2) \\
\impliesr & x_1 \preeq x_2, && \why{ p-injective}
\end{eqs}
for all  and . That is, all elements in  are equivalent to each other, for all . Let  be such that . Then
\begin{eqs}
\pclosure{\setb{y_1}} = \pclosure{\setb{y_2}}.
\end{eqs}
This implies that
\begin{eqs}
{} & \hat{f}(y_1), \hat{f}(y_2) \in \preimage{f}{\pclosure{\setb{y_1}}}.
\end{eqs}
However, since all elements in  are equivalent to each other,
\begin{eqs}
\hat{f}(y_1) \preeq \hat{f}(y_2).
\end{eqs}
Therefore  is p-preserving. 

\proofpart{}
Let  be a p-preserving left p-inverse of . Then
\begin{eqs}
{} & f(x_1) \preeqb f(x_2) \\
\impliesr & \linv{f}{f(x_1)} \preeq \linv{f}{f(x_2)} && \why{ p-preserving} \\
\impliesr & x_1 \preeq x_2, && \why{ left p-inverse of }
\end{eqs}
for all . Therefore  is p-injective.
\end{proof}

\begin{note}[Non\-/partition\-/preserving left p-inverse]
\fref{PSurjectiveWithPInversesNotPInjective} shows that the existence of a non-p-preserving left p-inverse does not imply p-injectivity.
\end{note}

\begin{theorem}[Left p-inverses are equivalent on the image]
\label{LeftPInversesAreEquivalent}
The left p-inverses of  are equivalent to each other on .
\end{theorem}

\begin{proof}
Suppose  and  are left p-inverses of . For each , there exists  such that . Since  and  are left p-inverses,
\begin{eqs}
g(y) = g(f(x_y)) \preeq x_y \preeq h(f(x_y)) = h(y),
\end{eqs}
for all .
\end{proof}

\section{Partition-surjectivity and right partition-inverse}

\begin{definition}[Partition\-/surjectivity]
A function  is \define{p-surjective}, if 
\begin{eqs}
\pclosure{\image{f}{X}} = Y.
\end{eqs}
\end{definition}

\begin{note}[Right partition\-/inverse]
A \define{right p\-/inverse} of  is  such that
\begin{eqs}
f(\rinv{f}{y}) \preeqb y,
\end{eqs}
for all .
\end{note}

\begin{theorem}[Partition\-/surjectivity is equivalent to having a right p-inverse]
\label{PSurjectivityIsEquivalentToHavingRightInverse}
Let . Then  is p-surjective if and only if  has a right p-inverse.
\end{theorem}

\begin{proof}
\proofpart{}
Since  is p-surjective,
\begin{eqs}
\preimage{f}{\pclosure{\setb{y}}} \neq \emptyset,
\end{eqs}
for all . Let  be such that
\begin{eqs}
\hat{f}(y) \in \preimage{f}{\pclosure{\setb{y}}}.
\end{eqs}
Then
\begin{eqs}
{} & f(\hat{f}(y)) \in \pclosure{\setb{y}} \\
\iffr & f(\hat{f}(y)) \preeqb y,
\end{eqs}
for all . Therefore  is a right p-inverse of .

\proofpart{}
Let  be a right p-inverse of . Then
\begin{eqs}
f(\rinv{f}{y}) \preeqb y,
\end{eqs}
for all . Therefore  is p-surjective.
\end{proof}

\begin{theorem}[Right p-inverse is order-reflecting for order-preserving]
\label{RightPInverseIsOrderReflectingForOrderPreserving}
Let  be order-preserving. Then a right p-inverse of  is order-reflecting.
\end{theorem}

\begin{proof}
Let  be a right p-inverse of . Then
\begin{eqs}
{} & \rinv{f}{y_1} \preleq \rinv{f}{y_2} \\
{} \impliesr & f(\rinv{f}{y_1}) \preleqb f(\rinv{f}{y_2}) && \why{ order-preserving} \\
{} \impliesr & y_1 \preleqb y_2, && \why{ right p-inverse of }
\end{eqs}
for all . Therefore  is order-reflecting.
\end{proof}

\section{Partition-bijectivity and partition-inverse}

\begin{definition}[Partition\-/bijectivity]
A function  is \define{p-bijective}, if it is both p-injective and p-surjective.
\end{definition}

\begin{definition}[Partition\-/inverse]
A \define{p-inverse of } is a function  which is both a left p-inverse and a right p-inverse of . 
\end{definition}

\begin{theorem}[Partition\-/bijectivity is equivalent to having a p-preserving p-inverse]
\label{PBijectivityIsEquivalentToHavingPPreservingInverse}
Let . Then  is p-bijective if and only if  has a p-preserving p-inverse.
\end{theorem}

\begin{proof}
\proofpart{}
A p-preserving left p-inverse  of  exists by \thref{PInjectivityIsEquivalentToPPreservingLeftInverse}. We may extend  to a p-preserving left p-inverse of  on . Since  is p-surjective, . We then notice that the extended  satisfies the construction of the right p-inverse  in \thref{PSurjectivityIsEquivalentToHavingRightInverse}.

\proofpart{}
This follows from \thref{PInjectivityIsEquivalentToPPreservingLeftInverse} and \thref{PSurjectivityIsEquivalentToHavingRightInverse}.
\end{proof}

\begin{theorem}[Partition\-/bijectivity implies a unique p-inverse]
Let  be p-bijective. Then  has a unique p-inverse up to an equivalence.
\label{PBijectivityImpliesUniquePInverse}
\end{theorem}

\begin{proof}
Since  is p-bijective, there exists a p-inverse  of  by \thref{PBijectivityIsEquivalentToHavingPPreservingInverse}. Suppose there exists another p-inverse  of . Then
\begin{eqs}
{} & f(g(y)) \preeqb y \preeqb f(h(y)) \\
\impliesr & g(y) \preeq h(y), && \why{ p-injective}
\end{eqs}
for all .
\end{proof}

\section{Partition-embedding}

\begin{definition}[Partition-embedding]
A function  is \define{p-embedding}, if it is both p-preserving and p-injective.
\end{definition}

\begin{theorem}[Partition-embedding by preimages]
\label{PEmbeddingByPreimages}
Let . Then  is a p-embedding if and only if
\begin{eqs}
\preimage{f}{\ppclosure{f(x)}} = \ppclosure{x},
\end{eqs}
for all .
\end{theorem}

\begin{proof}
\proofpart{}
It holds that
\begin{eqs}
{} & z \in \preimage{f}{\ppclosure{f(x)}} \\
\iffr & f(z) \in \ppclosure{f(x)} \\
\iffr & f(z) \preeqb f(x) \\
\iffr & z \preeq x && \why{ p-embedding} \\
\iffr & z \in \ppclosure{x},
\end{eqs}
for all .

\proofpart{}
To show that  is p-embedding,
\begin{eqs}
{} & x_1 \preeq x_2 \\
\iffr & \ppclosure{x_1} = \ppclosure{x_2} \\
\iffr & \preimage{f}{\ppclosure{f(x_1)}} = \preimage{f}{\ppclosure{f(x_2)}} && \why{assumption} \\
\iffr & \ppclosure{f(x_1)} = \ppclosure{f(x_2)} && \why{ p-embedding} \\
\iffr & f(x_1) \preeqb f(x_2),
\end{eqs}
for all . 
\end{proof}

\chapter{Preordered sets}
\label{PreorderedSets}

In this section we develop some theory of preorders, generalizing the theory of partial orders. Preordered sets occur in the theory of -notation, because  is a principal down-set of a preorder in . 

\begin{definition}[Preordered set]
A \define{preordered set} is a set  with an associated preorder .
\end{definition}

\begin{note}[Conventions]
Let  and  be preordered sets. 
\end{note}

\begin{definition}[Order-preserving]
A function  is \define{order-preserving}, or \define{monotone}, if
\begin{eqs}
x_1 \preleq x_2 \implies f(x_1) \preleqb f(x_2),
\end{eqs}
for all .
\end{definition}

\begin{note}[Homomorphisms]
The order-preserving functions are the homomorphisms of preordered sets; they preserve the preorder structure. 
\end{note}

\begin{definition}[Order-reflecting]
A function  is \define{order-reflecting}, if
\begin{eqs}
f(x_1) \preleqb f(x_2) \implies x_1 \preleq x_2,
\end{eqs}
for all .
\end{definition}

\begin{definition}[Order-embedding]
A function  is \define{order-embedding}, if it is both order-preserving and order-reflecting.
\end{definition}

\begin{definition}[Order-isomorphism]
A function  is an \define{order-isomorphism}, if it is p-surjective and order-embedding.
\end{definition}

The relationships between such functions is given in \figref{OrderPreservingDiagram}, with \figref{OrderPreservingProper} showing that the inclusions are proper.

\section{Order and partitions}

\begin{definition}[Induced equivalence]
Given a preorder , the \define{induced equivalence} is  such that 
\begin{eqs}
x_1 \preeq x_2 \iff x_1 \preleq x_2 \textrm{ and } x_2 \preleq x_1.
\end{eqs}
\end{definition}

\begin{note}[Relation to partitioned sets]
The induced equivalence  on a preordered set  partitions ; the theory of partitioned sets interacts with the theory of preordered sets.
\end{note}

\begin{theorem}[Order-preserving is partition\-/preserving]
\label{OrderPreservingIsPartitionPreserving}
Let \hfill \\  be order-preserving. Then  is p-preserving.
\end{theorem}

\begin{proof}
Let  be such that . Then
\begin{eqs}
x_1 \preleq x_2 \implies f(x_1) \preleqb f(x_2).
\end{eqs}
Similarly,
\begin{eqs}
x_2 \preleq x_1 \implies f(x_2) \preleqb f(x_1).
\end{eqs}
It follows that
\begin{eqs}
f(x_1) \preeqb f(x_2).
\end{eqs}
Therefore  is p-preserving.
\end{proof}

\begin{theorem}[Order-reflecting implies partition\-/injective]
\label{OrderReflectingImpliesInjective}
Let  be order-reflecting. Then  is partition\-/injective.
\end{theorem}

\begin{proof}
It holds that
\begin{eqs}
{} & f(x_1) \preeqb f(x_2) \\
\impliesr & f(x_1) \preleqb f(x_2) \textrm{ and } f(x_2) \preleqb f(x_1) \\
\impliesr & x_1 \preleq x_2 \textrm{ and } x_2 \preleq x_1 \\
\impliesr & x_1 \preeq x_2,
\end{eqs}
for all . 
\end{proof}

\section{Order-preserving functions and down-sets}

\begin{definition}[Generated down\-/set]
The \define{generated down\-/set} in  is a function  such that

\end{definition}

\begin{definition}[Down\-/set]
A subset  is a \define{down\-/set} of , if .
\end{definition}

\begin{definition}[Principal down\-/set]
A subset  is a \define{principal down\-/set} of , if there exists  such that .
\end{definition}

\begin{figure}
\centering
\begin{tikzpicture}[every node/.style={draw,rounded corners,thick}]
        \node (mon) at (0, 0) {monotone function};
        \node (mon_sur) at (2.5, 1.5) {monotone p-surjection};
        \node (mon_bij) at (5, 3) {monotone p-bijection};
        \node (mon_inj) at (7.5, 1.5) {monotone p-injection};
        \node (ord_emb) at (7.5, 4.5) {order embedding};
        \node (res_inj) at (7.5, 6) {residuated p-injection};
        \node (iso) at (5, 7.5) {order isomorphism};
        \node (res_sur) at (2.5, 6) {residuated p-surjection};
        \node (res) at (0, 4.5) {residuated function};
        
        \draw[semithick] (mon) -- node[right=10, draw=none] {\ref{OrderEmbeddingNotResiduatedOrSurjective}} (mon_sur);
        
        \draw[semithick] (mon_sur) -- node[right=10, draw=none] {\ref{ResiduatedAndSurjectiveNotInjective}} (mon_bij);
        
        \draw[semithick] (mon.east) -- node[right=20, draw=none] {\ref{ResiduatedAndSurjectiveNotInjective}} (mon_inj);
        
        \draw[semithick, dashed] (mon_inj) -- node[right, draw=none] {\ref{OrderPreservingAndBijectiveNotOrderReflectingOrResiduated}} (ord_emb);

        \draw[semithick, dashed] (ord_emb) -- node[right, draw=none] {\ref{OrderEmbeddingNotResiduatedOrSurjective}} (res_inj);

        \draw[semithick] (res_inj) -- node[right=10, draw=none] {\ref{ResiduatedAndInjectiveNotSurjective}} (iso);

        \draw[semithick, dashed] (mon) -- node[right, draw=none] {\ref{OrderPreservingAndBijectiveNotOrderReflectingOrResiduated}} (res);

        \draw[semithick] (res) -- node[left=10, draw=none] {\ref{ResiduatedNotSurjectiveOrInjective}} (res_sur);

        \draw[semithick] (res_sur) -- node[left=10, draw=none] {\ref{ResiduatedAndSurjectiveNotInjective}} (iso);

        \draw[semithick] (res.east) -- node[left=45, draw=none] {\ref{ResiduatedNotSurjectiveOrInjective}} (res_inj);

        \draw[semithick] (mon_inj) -- node[left=10, draw=none] {\ref{ResiduatedAndInjectiveNotSurjective}} (mon_bij);

        \draw[semithick, dashed] (mon_sur) -- node[right, draw=none] {\ref{OrderPreservingAndBijectiveNotOrderReflectingOrResiduated}} (res_sur);

        \draw[semithick, dashed] (mon_bij) -- node[right, draw=none] {\ref{OrderPreservingAndBijectiveNotOrderReflectingOrResiduated}} (iso);
\end{tikzpicture}
\caption{A Hasse diagram of order-preserving functions between preordered sets, ordered by the `generalizes' partial order. Original figure was created by David Wilding, and is used here with his permission. The edge labels --- which refer to \fref{OrderPreservingProper} --- have been added to show that the generalizations are proper.
}
\label{OrderPreservingDiagram}
\end{figure}

\begin{figure}
\hfill
\subfloat[Residuated (and order\-/preserving), but not p\-/surjective or p\-/injective.]
{
\makebox[.4\textwidth]{
\begin{tikzpicture}[scale = 0.5]
\blackdot{0, 0};
\blackdot{0, 1};
\blackdot{5, 0};
\blackdot{5, 1};
\draw[dashed] (0, 0) -- (5, 0);
\draw[dashed] (0, 1) -- (5, 0);
\draw (0, 0) -- (0, 1);
\draw (5, 0) -- (5, 1);
\end{tikzpicture}
}
\label{ResiduatedNotSurjectiveOrInjective}
}
\hfill
\subfloat[Order\-/embedding (and p\-/injective), but not residuated or p\-/surjective.]
{
\makebox[.4\textwidth]{
\begin{tikzpicture}[scale = 0.5]
\blackdot{0, 1};
\blackdot{5, 0};
\blackdot{5, 1};
\draw[dashed] (0, 1) -- (5, 1);
\draw (5, 0) -- (5, 1);
\end{tikzpicture}
}
\label{OrderEmbeddingNotResiduatedOrSurjective}
}
\hfill\null

\hfill
\subfloat[Residuated and p-surjective (and order-preserving), but not p-injective.]
{
\makebox[.4\textwidth]{
\begin{tikzpicture}[scale = 0.5]
\blackdot{0, 0};
\blackdot{0, 1};
\blackdot{5, 0};
\draw[dashed] (0, 0) -- (5, 0);
\draw[dashed] (0, 1) -- (5, 0);
\draw (0, 0) -- (0, 1);
\end{tikzpicture}
}
\label{ResiduatedAndSurjectiveNotInjective}
}
\hfill
\subfloat[Residuated and p-injective (and order-preserving), but not p-surjective.]
{
\makebox[.4\textwidth]{
\begin{tikzpicture}[scale = 0.5]
\blackdot{0, 0};
\blackdot{5, 0};
\blackdot{5, 1};
\draw[dashed] (0, 0) -- (5, 0);
\draw (5, 0) -- (5, 1);
\end{tikzpicture}
}
\label{ResiduatedAndInjectiveNotSurjective}
}
\hfill\null

\hfill
\subfloat[Order\-/preserving and p\-/bijective, but not order\-/reflecting or residuated.]
{
\makebox[.4\textwidth]{
\begin{tikzpicture}[scale = 0.5]
\blackdot{5, 0};
\blackdot{5, 1};
\blackdot{5, 2};
\blackdot{0, 2};
\blackdot{1, 0};
\blackdot{2, 1};
\draw[dashed] (0, 2) -- (5, 2);
\draw[dashed] (1, 0) -- (5, 0);
\draw[dashed] (2, 1) -- (5, 1);
\draw (5, 0) -- (5, 1);
\draw (5, 1) -- (5, 2);
\draw (1, 0) -- (0, 2);
\draw (1, 0) -- (2, 1);
\end{tikzpicture}
}
\label{OrderPreservingAndBijectiveNotOrderReflectingOrResiduated}
}
\hfill
\subfloat[Order\-/reflecting and p\-/bijective, but not order\-/preserving.]
{
\makebox[.4\textwidth]{
\begin{tikzpicture}[scale = 0.5]
\blackdot{0, 0};
\blackdot{0, 1};
\blackdot{0, 2};
\blackdot{3, 1};
\blackdot{4, 0};
\blackdot{5, 2};
\draw[dashed] (0, 0) -- (4, 0);
\draw[dashed] (0, 1) -- (3, 1);
\draw[dashed] (0, 2) -- (5, 2);
\draw (0, 0) -- (0, 1);
\draw (0, 1) -- (0, 2);
\draw (4, 0) -- (3, 1);
\draw (4, 0) -- (5, 2);
\end{tikzpicture}
}
\label{OrderReflectingAndBijectiveNotOrderPreserving}
}
\hfill\null
\caption{Hasse diagrams to show that the various definitions of order-preserving functions are not equivalent. Since all orders here are partial, the p-prefix is redundant.}
\label{OrderPreservingProper}
\end{figure}

\begin{theorem}[Alternative definition of down\-/sets]
\label{AlternativeDownSet}
Let . Then  is a down\-/set if and only if

\end{theorem}

\begin{proof}
It always holds that . Then
\begin{eqs}
{} & D = \setb{x \in X : \exists d \in D: x \preleq d} \\
\iffr & D \supset \setb{x \in X : \exists d \in D: x \preleq d} \\
\iffr & \forall x \in X: \bra{\exists d \in D : x \preleq d} \implies x \in D \\
\iffr & \forall x \in X: \bra{\forall d \in D : x \not\preleq d} \textrm{ or } x \in D \\
\iffr & \forall x \in X, \forall d \in D : \bra{x \not\preleq d \textrm { or } x \in D} \\
\iffr & \forall x \in X, \forall d \in D : x \preleq d \implies x \in D.
\end{eqs}
\end{proof}

\begin{theorem}[Order-preservation by preimages]
\label{OrderPreservationByPreimages}
Let . Then  is order-preserving if and only if each preimage of a down\-/set of  is a down\-/set of .
\end{theorem}

\begin{proof}
\proofpart{}
Let , , and . Let , and  be such that . Since  is order-preserving, . It holds that , for all . Since , and  is a down\-/set, . This implies . Therefore  is a down\-/set.

\proofpart{}
Let . Then there exists  such that

Let . Then
\begin{eqs}
{} & x \in \preimage{f}{\pdownset{Y}{f(x_2)}} \\
\iffr & f(x) \in \pdownset{Y}{f(x_2)} \\
\iffr & f(x) \preleqb f(x_2).
\end{eqs}
In particular, . Then
\begin{eqs}
{} & x_1 \preleq x_2 \\
\impliesr & x_1 \in \downset{X}{D} \\
\impliesr & x_1 \in \preimage{f}{\pdownset{Y}{f(x_2)}} \\
\impliesr & f(x_1) \in \pdownset{Y}{f(x_2)} \\
\impliesr & f(x_1) \preleqb f(x_2).
\end{eqs}
Therefore  is order-preserving.
\end{proof}

\begin{theorem}[Order-preservation by images]
\label{OrderPreservationByImages}
Let . Then  is order-preserving if and only if
\begin{eqs}
\image{f}{\downset{X}{S}} \subset \downset{Y}{\image{f}{S}},
\end{eqs}
for all .
\end{theorem}

\begin{proof}
\proofpart{}
It holds that
\begin{eqs}
{} & x \in \downset{X}{S} \\
\impliesr & \exists s \in S: x \preleq s \\
\impliesr & \exists s \in S: f(x) \preleqb f(s) && \why{ order-preserving} \\
\impliesr & f(x) \in \downset{Y}{\image{f}{S}},
\end{eqs}
for all .

\proofpart{}
It holds that
\begin{eqs}
{} & x_1 \preleq x_2 \\
\impliesr & \pdownset{X}{x_1} \subset \pdownset{X}{x_2} \\
\impliesr & \image{f}{\pdownset{X}{x_1}} \subset \image{f}{\pdownset{X}{x_2}} \\
\impliesr & \image{f}{\pdownset{X}{x_1}} \subset \pdownset{Y}{f(x_2)} \\
\impliesr & f(x_1) \in \pdownset{Y}{f(x_2)} \\
\impliesr & f(x_2) \preleqb f(x_2),
\end{eqs}
for all . Therefore  is order-preserving.
\end{proof}

\section{Residuated functions}

\begin{definition}[Residual]
A function  is a \define{residual} of , if
\begin{eqs}
f(x) \preleqb y \iff x \preleq \hat{f}(y),
\end{eqs}
for all  and .
\end{definition}

\begin{definition}[Residuated]
A function  is \define{residuated}, if it has a residual.
\end{definition}

\begin{theorem}[Preimage of a down\-/set under a residuated function]
\label{PreimageOfDownSetUnderResiduatedFunction}
Let  be residuated, with a residual . Then

for all .
\end{theorem}

\begin{proof}
\begin{eqs}
\preimage{f}{\downset{Y}{D}} & = \setb{x \in X : f(x) \in \downset{Y}{D}} \\
{} & = \setb{x \in X : \exists d \in D : f(x) \preleqb d} \\
{} & = \setb{x \in X : \exists d \in D : x \preleq \hat{f}(d)} \\
{} & = \setb{x \in X : \exists x' \in \hat{f}(D) : x \preleq x'} \\
{} & = \downset{X}{\hat{f}(D)}.
\end{eqs}
\end{proof}

\begin{theorem}[Residuated property by preimages]
\label{ResiduatedByPreimages}
Let . Then  is residuated if and only if for each , there exists  such that
\begin{eqs}
\preimage{f}{\pdownset{Y}{y}} = \pdownset{X}{x_y}.
\end{eqs}
\end{theorem}

\begin{proof}
\proofpart{}
Let  be residuated, and . Then 

by \proveby{PreimageOfDownSetUnderResiduatedFunction}. Therefore each preimage of a principal down\-/set of  is a principal down\-/set of .

\proofpart{}
For each , there exists  such that
\begin{eqs}
{} & \preimage{f}{\pdownset{Y}{y}} = \pdownset{X}{x_y} \\
\iffr & \setb{x \in X : f(x) \in \pdownset{Y}{y}} = \setb{x \in X : x \preleq x_y} \\
\iffr & \setb{x \in X : f(x) \preleqb y} = \setb{x \in X : x \preleq x_y} \\
\iffr & \forall x \in X: \left[ f(x) \preleqb y \iff x \preleq x_y \right].
\end{eqs}
Let  be such that . By the above,  is a residual of . Therefore  is residuated.
\end{proof}

\begin{theorem}[Properties of a residuated function] 
\label{PropertiesOfResiduatedFunction}
Let . Then  is residuated if and only if  is order-preserving, and there exists order-preserving , such that
\begin{eqs}
{} & \residual{f}{f(x)} \pregeq x, \\
{} & f(\residual{f}{y}) \preleqb y,
\end{eqs}
for all , .
\end{theorem}

\begin{proof}
\proofpart{}
For the first relation,
\begin{eqs}
{} & f(x) \preleqb f(x) \\
\iffr & x \preleq \residual{f}{f(x)}, && \why{ residual of }
\end{eqs}
for all . 
Then
\begin{eqs}
{} & x_1 \preleq x_2 \\
\impliesr & x_1 \preleq \residual{f}{f(x_2)} && \why{first relation} \\
\impliesr & f(x_1) \preleqb f(x_2), && \why{ residual of }
\end{eqs}
for all . Therefore  is order-preserving.

For the second relation,
\begin{eqs}
{} & \residual{f}{y} \preleq \residual{f}{y} \\
\iffr & f(\residual{f}{y}) \preleqb y, && \why{ residual of }
\end{eqs}
for all .
Then
\begin{eqs}
{} & y_1 \preleqb y_2 \\
\impliesr & f(\residual{f}{y_1}) \preleqb y_2 && \why{second relation} \\
\impliesr & \residual{f}{y_1} \preleq \residual{f}{y_2}, && \why{ residual of }
\end{eqs}
for all . Therefore  is order-preserving.

\proofpart{}
It holds that
\begin{eqs}
{} & f(x) \preleqb y \\
\impliesr & \residual{f}{f(x)} \preleq \residual{f}{y} && \why{ order-preserving} \\
\impliesr & x \preleq \residual{f}{y} && \why{first relation} \\
\impliesr & f(x) \preleqb f(\residual{f}{y}) && \why{ order-preserving} \\
\impliesr & f(x) \preleqb y, && \why{second relation}
\end{eqs}
for all , . Therefore  is a residual of .
\end{proof}

\begin{theorem}[Residual is essentially unique]
\label{ResidualIsEssentiallyUnique}
Let  both be residuals of . Then
\begin{eqs}
g(y) \preeq h(y),
\end{eqs}
for all .
\end{theorem}

\begin{proof}
By definition,
\begin{eqs}
{} & g(y) \preleq h(y) \\
\iffr & f(g(y)) \preleqb y,
\end{eqs}
for all . The latter holds by \thref{PropertiesOfResiduatedFunction}. Similarly,
\begin{eqs}
h(y) \preleq g(y),
\end{eqs}
for all . 
\end{proof}

\section{Residuated functions and supremum}

\begin{definition}[Upper-bound]
An element  is an \define{upper\-/bound} of , if
\begin{eqs}
x \preleq s,
\end{eqs}
for all .
\end{definition}

\begin{definition}[Least upper\-/bound]
An element  is a \define{least upper\-/bound} of , if  is an upper\-/bound of , and for any upper\-/bound  of ,
\begin{eqs}
s \preleq t.
\end{eqs}
\end{definition}

\begin{definition}[Supremum]
The \define{supremum} of a set in a set  is a function , such that
\begin{eqs}
\supremum{S} = \setb{s \in X : s \text{ is a least upper\-/bound of }}.
\end{eqs}
\end{definition}

\begin{theorem}[Residuated property by suprema]
\label{ResiduatedPropertyBySupremum}
Let . Then  is residuated if and only if  is order-preserving,
\begin{eqs}
\supremum[X]{\preimage{f}{\pdownset{Y}{y}}} \neq \emptyset,
\end{eqs}
for all , and
\begin{eqs}
\image{f}{\supremum[X]{S}} \subset \supremum[Y]{\image{f}{S}},
\end{eqs}
for all .
\end{theorem}

\begin{proof}
\proofpart{ Order-preservation}
The function  is order-preserving by \thref{PropertiesOfResiduatedFunction}.

\proofpart{ Non-emptiness}
By \thref{ResiduatedByPreimages}, for each , there exists , such that
\begin{eqs}
\preimage{f}{\pdownset{Y}{y}} = \pdownset{X}{x_y}.
\end{eqs}
It follows that
\begin{eqs}
\supremum[X]{\preimage{f}{\pdownset{Y}{y}}} = \ppclosure{x_y} \neq \emptyset.
\end{eqs}

\proofpart{ Upper-bound}
Let . Since  is an upper\-/bound of ,
\begin{eqs}
x \preleq s,
\end{eqs}
for all . Since  is order-preserving,
\begin{eqs}
f(x) \preleqb f(s),
\end{eqs}
for all . Therefore  is an upper\-/bound of .

\proofpart{ Least upper\-/bound}
Let  be a residual of , and  be an upper\-/bound of . Then
\begin{eqs}
f(x) \preleqb y,
\end{eqs}
for all . Since  is residuated,
\begin{eqs}
x \preleq \residual{f}{y},
\end{eqs}
for all . That is,  is an upper\-/bound of . Since  is a \emph{least} upper\-/bound of ,
\begin{eqs}
s \preleq \residual{f}{y}.
\end{eqs}
Since  is residuated,
\begin{eqs}
f(s) \preleqb y.
\end{eqs}
Therefore  is a least upper\-/bound of ;
\begin{eqs}
\image{f}{\supremum{S}} \subset \supremum{\image{f}{S}}.
\end{eqs}

\proofpart{ Order-preservation}
Let  be such that
\begin{eqs}
\residual{f}{y} \in \supremum{\preimage{f}{\pdownset{Y}{y}}}.
\end{eqs}
The function  is well-defined, since by assumption
\begin{eqs}
\supremum{\preimage{f}{\pdownset{Y}{y}}} \neq \emptyset,
\end{eqs}
for all . Let  be such that . Since supremum is an upper\-/bound,
\begin{eqs}
{} & \forall x \in \preimage{f}{\pdownset{Y}{y_2}}: x \preleq \residual{f}{y_2} \\
\impliesr & \forall x \in X: \brac{f(x) \preleqb y_2 \implies x \preleq \residual{f}{y_2}} \\
\impliesr & \forall x \in X: \brac{f(x) \preleqb y_1 \implies x \preleq \residual{f}{y_2}} \\
\impliesr & \forall x \in \preimage{f}{\pdownset{Y}{y_1}}: x \preleq \residual{f}{y_2}.
\end{eqs}
That is,  is also an upper\-/bound of . Since  is a \emph{least} upper\-/bound of ,
\begin{eqs}
\residual{f}{y_1} \preleq \residual{f}{y_2}.
\end{eqs}
Therefore  is order-preserving. 

\proofpart{ Deflation}
Let . By assumption,
\begin{eqs}
f(\residual{f}{y}) & \in \image{f}{\supremum{\preimage{f}{\pdownset{Y}{y}}}} \\
{} & \subset \supremum[Y]{\image{f}{\preimage{f}{\pdownset{Y}{y}}}} \\
{} & \subset \supremum[Y]{\pdownset{Y}{y}} \\
{} & = \pclosure{\setb{y}}.
\end{eqs}
Therefore 
\begin{eqs}
f(\residual{f}{y}) \preleqb y.
\end{eqs}

\proofpart{ Inflation}
Since ,
\begin{eqs}
x \preleq \residual{f}{f(x)},
\end{eqs}
for all . Therefore  is residuated with residual  by \thref{PropertiesOfResiduatedFunction}.
\end{proof}

\begin{theorem}[Residual by supremum]
\label{SupremumResidual}
Let  be residuated. Then  is a residual of  if and only if 
\begin{eqs}
\residual{f}{y} \in \supremum{\preimage{f}{\pdownset{Y}{y}}},
\end{eqs}
for all .
\end{theorem}

\begin{proof}
This is shown by \thref{ResiduatedPropertyBySupremum} and \thref{ResidualIsEssentiallyUnique}.
\end{proof}

\section{Residuated functions and p-inverses}

\begin{theorem}[Residual is a generalized p-inverse]
\label{ResidualIsGeneralizedPInverse}
Let  be residuated. Then
\begin{eqs}
f(\residual{f}{f(x)}) & \preeqb f(x), \\
\residual{f}{f(\residual{f}{y})} & \preeq \residual{f}{y},
\end{eqs}
for all , .
\end{theorem}

\begin{proof}
Let . It holds that
\begin{eqs}
f(\residual{f}{f(x)}) & \preleqb f(x),
\end{eqs}
by \thref{PropertiesOfResiduatedFunction}. By the same theorem,
\begin{eqs}
\residual{f}{f(x)} \pregeq x.
\end{eqs}
Since  is order-preserving by \thref{PropertiesOfResiduatedFunction},
\begin{eqs}
f(\residual{f}{f(x)}) \pregeqb f(x).
\end{eqs}
Similarly for the second equivalence.
\end{proof}

\begin{theorem}[Residual is a right p-inverse for p-surjection]
\label{ResidualIsRightPInverseForPSurjection}
Let  be a p-surjection. Then a residual of  is a right p-inverse of .
\end{theorem}

\begin{proof}
The function  is p-preserving by \thref{PropertiesOfResiduatedFunction}. A right p-inverse  of  exists by \thref{PSurjectivityIsEquivalentToHavingRightInverse}. Let  be a residual of . Then
\begin{eqs}
{} \quad & y \preleqb y \\
{} \impliesr & f(\rinv{f}{y}) \preleqb y && \why{right p-inverse of } \\
{} \impliesr & \rinv{f}{y} \preleq \hat{f}(y) && \why{ residuated} \\
{} \impliesr & f(\rinv{f}{y}) \preleqb f(\hat{f}(y)) && \why{ order-preserving} \\
{} \impliesr & y \preleqb f(\hat{f}(y)) && \why{right p-inverse of },
\end{eqs}
for all . By \thref{PropertiesOfResiduatedFunction},
\begin{eqs}
f(\hat{f}(y)) \preleqb y,
\end{eqs}
for all . Therefore,
\begin{eqs}
f(\hat{f}(y)) \preeqb y,
\end{eqs}
for all . That is,  is a right p-inverse of .
\end{proof}

\begin{note}[]
A residuated p-surjection may have many non-equivalent right p-inverses; a residual is a specific version of a right p-inverse.
\end{note} 

\begin{theorem}[Residual is a left p-inverse for residuated p-injection]
\label{ResidualIsLeftPInverseForResiduatedPInjection}
Let  be a residuated p-injection. Then a residual of  is a left p-inverse of .
\end{theorem}

\begin{proof}
This follows from \thref{ResidualIsGeneralizedPInverse} and \thref{GeneralizedPInverseIsLeftPInverseForPInjective}.
\end{proof}

\begin{theorem}[Residual is a p-inverse for residuated p-bijection]
\label{ResidualIsPInverseForResiduatedPBijection}
Let  be a residuated p-bijection. Then a residual of  is a p-inverse of .
\end{theorem}

\begin{proof}
This follows from \thref{ResidualIsLeftPInverseForResiduatedPInjection} and \thref{ResidualIsRightPInverseForPSurjection}.
\end{proof}

\begin{theorem}[Transpose-residuated p-surjective function preserves down-sets]
\label{TransposeResiduatedPSurjectivePreservesDownSets}
Let  be -residuated and p-surjective. Then
\begin{eqs}
\pclosure{\image{f}{\pdownset{X}{S}}} = \downset{Y}{\image{f}{S}},
\end{eqs}
for all .
\end{theorem}

\begin{proof}
\proofpart{}
The function  is -order-preserving by \thref{PropertiesOfResiduatedFunction}, which is the same as -order-preserving. The result follows from \thref{OrderPreservationByImages}.

\proofpart{}
Let  be a -residual of , which is a right p-inverse of  by \thref{ResidualIsRightPInverseForPSurjection}. Then
\begin{eqs}
{} & y \in \downset{Y}{\image{f}{S}} \\
\impliesr & \exists x \in S: y \preleqb f(x) && \why{definition of down-set} \\
\impliesr & \exists x \in S: \residual{f}{y} \preleq x && \why{ is -residual of } \\
\impliesr & \residual{f}{y} \in \downset{X}{S}  && \why{definition of down-set} \\
\impliesr & f(\residual{f}{y)} \in \image{f}{\downset{X}{S}} \\
\impliesr & y \in \pclosure{\image{f}{\downset{X}{S}}}, && \why{ right p-inverse of }
\end{eqs}
for all .
\end{proof}

\begin{theorem}[Transpose-residuated surjective function preserves down-sets]
\label{TransposeResiduatedSurjectivePreservesDownSets}
Let  be -residuated and surjective. Then
\begin{eqs}
\image{f}{\pdownset{X}{S}} = \downset{Y}{\image{f}{S}},
\end{eqs}
for all .
\end{theorem}

\begin{proof}
The proof is the same as \thref{TransposeResiduatedPSurjectivePreservesDownSets}, except that now we can choose the residual to be both a right inverse and right p-inverse, and use the fact that
\begin{eqs}
y = f(\residual{f}{y}).
\end{eqs}
\end{proof}

\section{Residuated functions and order-embeddings}

\begin{theorem}[Residuated p-injection is order-embedding]
\label{ResiduatedPInjectionIsOrderEmbedding}
Let  be a residuated p-injection. Then  is order-embedding.
\end{theorem}

\begin{proof}
Let  be a residual of . Since  is p-injective,  is a left p-inverse of  by \thref{ResidualIsLeftPInverseForResiduatedPInjection}. Then
\begin{eqs}
{} & f(x_1) \preleqb f(x_2) \\
\impliesr & x_1 \preleq \residual{f}{f(x_2)} && \why{ residual of } \\
\impliesr & x_1 \preleq x_2, && \why{ left p-inverse of }
\end{eqs}
for all . Therefore  is order-reflecting. The function  is order-preserving by \proveby{PropertiesOfResiduatedFunction}. Therefore  is order-embedding. 
\end{proof}

\begin{theorem}[Residuated p-bijection is an order isomorphism]
\label{ResiduatedPBijectionIsOrderIsomorphism}
Let . Then  is an order isomorphism if and only if  is a residuated p-bijection. 
\end{theorem}

\begin{proof}
\proofpart{}
Since  is order-embedding,  is p-injective by \thref{OrderReflectingImpliesInjective}. Therefore  is p-bijective. Let  be a p-preserving p-inverse of , which exists by \thref{PBijectivityIsEquivalentToHavingPPreservingInverse}. Then
\begin{eqs}
{} & f(x) \preleqb y \\
\iffr & f(x) \preleqb f(\pinv{f}{y}) && \why{ p-inverse of } \\
\iffr & x \preleqb \pinv{f}{y}, && \why{ p-reflecting}
\end{eqs}
for all  and . Therefore  is a residuated p-bijection.

\proofpart{}
This follows from \thref{ResiduatedPInjectionIsOrderEmbedding} and because  is p-surjective.
\end{proof}

























\ifindex
\printindex
\fi

\end{document}
