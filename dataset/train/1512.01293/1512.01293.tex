\documentclass[11pt]{article}
\usepackage{times}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{enumerate}
\usepackage{authblk}
\usepackage[letterpaper, margin=1in]{geometry}
\usepackage{tikz}
\usepackage{etoolbox}

\newtoggle{conf}
\ifx\confver\undefined
	\togglefalse{conf}
\else
	\toggletrue{conf}
\fi

\begin{document}

\newtheorem{theorem}{Theorem}
\newtheorem{innercustomthm}{Theorem}
\newenvironment{customthm}[1]
  {\renewcommand\theinnercustomthm{#1}\innercustomthm}
  {\endinnercustomthm}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{innercustomcor}{Corollary}
\newenvironment{customcor}[1]
  {\renewcommand\theinnercustomcor{#1}\innercustomcor}
  {\endinnercustomcor}
\newtheorem{lemma}{Lemma}
\newtheorem{claim}{Claim}
\newtheorem{definition}{Definition}
\newtheorem*{remark}{Remark}
\newcommand{\diu}{dynamic interval union}
\newcommand{\bps}{batch partial sum}
\newcommand{\es}{evenly-spreading}
\newcommand{\mi}{multi-index}
\newcommand{\mm}{model }
\newcommand{\E}{\ensuremath{\mathop{\mathbb{E}}}}
\newcommand{\rev}{\ensuremath{\mathrm{rev}}}

\title{Cell-probe Lower Bounds for Dynamic Problems via a New Communication Model}
\author{Huacheng Yu}
\affil{Stanford University}
\date{}

\maketitle

\begin{abstract}
In this paper, we develop a new communication model to prove a data structure lower bound for the \diu{} problem. The problem is to maintain a multiset of intervals  over  with integer coordinates, supporting the following operations:
\begin{itemize}
	\item
		\verb+insert(a, b)+: add an interval  to , provided that  and  are integers in ;
	\item
		\verb+delete(a, b)+: delete a (previously inserted) interval  from ;
	\item
		\verb+query()+: return the total length of the union of all intervals in .
\end{itemize}

It is related to the two-dimensional case of Klee's measure problem. We prove that there is a distribution over sequences of operations with  insertions and deletions, and  queries, for which any data structure with any constant error probability requires  time in expectation. Interestingly, we use the sparse set disjointness protocol of H\aa{}stad and Wigderson [ToC'07] to speed up a reduction from a new kind of nondeterministic communication games, for which we prove lower bounds.

For applications, we prove lower bounds for several dynamic graph problems by reducing them from \diu{}.
\end{abstract}

\newpage

\section{Introduction}
In computational geometry, Klee's measure problem~\cite{Klee77, Bent77, Chan13} is the following: given  rectangular ranges (axis-parallel hyperrectangles) in -dimensional space, compute the volume of their union.

A classic sweep-line algorithm by Bentley~\cite{Bent77} solves the  case in  time: consider the line  with  continuously increasing from  to ; the length of the intersection of this line and the union may change only when it reaches the left or right border of a rectangle. Bentley's algorithm uses a segment tree to dynamically maintain the length of the intersection efficiently. Surprisingly, this is the best known algorithm even for an intriguing special case: all coordinates are integers within a polynomially bounded range . In this case, the segment tree in Bentley's algorithm is essentially used to solve the following dynamic problem, which we call the \emph{\diu{} problem}:

Maintain a multiset  of intervals with integer coordinates in , supporting the following operations:

\begin{itemize}
	\item
		\verb+insert(a, b)+: add an interval  to , provided that  and  are integers in ;
	\item
		\verb+delete(a, b)+: delete a (previously inserted) interval  from ;
	\item
		\verb+query()+: return the total length of the union of all intervals in .
\end{itemize}

The segment tree data structure solves this dynamic problem with  insertion and deletion time, and  query time. For the application to 2D Klee's measure problem, there are  insertions,  deletions and  queries to the data structure. A natural question to ask here is whether we can improve the insertion and deletion time. However, there is a very simple reduction from the partial sum problem showing that the slowest operation among insertion, deletion and query needs to take  time (see Appendix~\ref{sectredps}). Moreover, P\v{a}tra\c{s}cu and Demaine~\cite{PD04b, PD06} showed an optimal trade-off between update and query time for partial sum, which can be carried over via the reduction to show that if both insertion and deletion need to be done in  time, then query has to take  time. 

This seems to be the end of the story. However, in fact, there is no  time algorithm known even for .\footnote{There is a simple linear time algorithm for the  case.} When we apply the above dynamic problem to this case, there will be  insertions,  deletions, and only  queries! There will be far fewer queries than insertions and deletions. The argument above does not rule out the possibility of having a \diu{} data structure with  update time, and  query time. It is even possible to have a data structure with  insertion and deletion time, and  query time. Having such a data structure would give a linear time algorithm for the above special case of Klee's measure problem, making a breakthrough on this 40-year-old problem. 

Unfortunately, we show that such data structure does not exist, even if we allow randomization, amortization and constant error probability. 

\begin{theorem}\label{thmdiu}
	For any  and integer , there is a distribution over operation sequences to the \diu{} problem over , with  insertions and deletions, and  queries, for which any data structure that correctly answers all queries simultaneously with probability  must spend  probes in expectation, in the cell-probe model with word size . 
\end{theorem}
We define the cell-probe model in \iftoggle{conf}{Appendix~\ref{sectcp_app}}{Section~\ref{subsectcp}}. 

It is an easy exercise to show that we can use the hard distribution from the theorem to obtain a new hard distribution with  insertions and deletions and  queries, such that any data structure requires  time on it. This lower bound rules out the possibility of using the plain-vanilla sweep-line algorithm with a sophisticated data structure to solve 2D Klee's measure problem faster than the classic algorithm. As a corollary, the theorem also implies that  insertion and deletion time leads to an almost linear lower bound on query time. 

The type of running time considered by Theorem~\ref{thmdiu} is more general than the amortized time. Having amortized time  usually means that the first  operations take at most  time for every , but here, even if we fix the number of operations in advance, and the data structure is allowed to use heavy preprocessing in order to optimize the total running time, there is still no way to break the lower bound. This notion of running time is usually what we care about, when applying a data structure to solve some computational problem. The only catch is that the data structure is online: it must output an answer before seeing the next operation, which makes it different from an offline computational problem. Moreover, we claim without proof the following theorem that using the same hard distribution, the same lower bound holds for data structures that correctly answer any constant fraction of the queries in expectation. 

\begin{customthm}{1}
	For any  and integer , there is a distribution over operation sequences to the \diu{} problem over , with  insertions and deletions, and  queries, for which any data structure with expected fraction of correct answers at least  must spend  probes in expectation for any , in the cell-probe model with word size . 
\end{customthm}

We prove Theorem~\ref{thmdiu} via a reduction from a more accessible intermediate problem called \emph{\bps{}}, for which we prove a lower bound directly. The \bps{} problem asks to maintain  sequences  of length  over a finite field , supporting the following operations to the sequences:\footnote{In this paper,  stands for the set of positive integers .}

\begin{itemize}
	\item
		\verb+update(+\verb+, +\verb+)+: for all , set  to value ;
	\item
		\verb+query(+\verb+)+: return ,
\end{itemize}
provided that  and  are vectors of length , and , . Basically, we need to maintain  independent copies of the partial sum problem, except that when answering queries, instead of returning  individual prefix sums, we only need to return the sum of these  numbers. 

\begin{theorem}\label{thmbps}
For large enough integers  with , there is a distribution over operation sequences to the \bps{} problem with  updates and  queries, for which any data structure that correctly answers all queries simultaneously with probability  must spend  probes in expectation, in the cell-probe model with word size .
\end{theorem}

Note that when  are all polynomials in  and  for some , the lower bound becomes  per operation. Therefore, in this case, the best thing to do is just to use  partial sum data structures to maintain the  sequences independently. However, the techniques we use in the proof are very different from the proof of the lower bound for partial sum by P\v{a}tra\c{s}cu and Demaine~\cite{PD04a}. See Section~\ref{subsecttc} for an overview. 

Moreover, we also apply our main theorem to prove lower bounds for three dynamic graph problems: dynamic \#SCC, dynamic weighted s-t shortest path, and dynamic planar s-t min-cost flow (see Appendix~\ref{sectcata} for formal definitions). We prove that for these problems, under constant error probability, if we spend  update time, then queries must take  time. Note that a previous result by P\v{a}tra\c{s}cu and Thorup~\cite{PT11} also implies the same trade-off for the first two problems under zero error. 

\begin{customcor}{\iftoggle{conf}{2}{\ref{cordyngraph}}}
For the following three dynamic graph problems:
\begin{enumerate}[(a)]
	\item
		dynamic \#SCC,
	\item
		dynamic planar s-t min-cost flow,
	\item
		dynamic weight s-t shortest path,
\end{enumerate}
any data structure with amortized expected update time , and error probability  under polynomially many operations must have amortized expected query time . 
\end{customcor}

\def\cellprobepre{The cell-probe model of Yao~\cite{Yao81} is a strong non-uniform computational model for data structures. A data structure in the cell-probe model has access to a set of memory cells. Each cell can store  bits. The set of cells is indexed by -bit integers, i.e., the address is in .  is usually set to be , where  is the amount of information the data structure needs to handle. \footnote{ is usually a polynomial in the number of operations. }

To access the memory, the data structure can \emph{probe} a cell, which means that it can look at the content of the cell, and then optionally overwrite it with a new value. During an operation, the data structure based on the parameters of the operation decides which cell to probe the first, then based on the parameters and the information from the first probe, decides the cell to probe next, etc. Each cell-probe (including both the address and the new value) the data structure performs may be an arbitrary function of the parameters of the operation and the contents in the cells previously probed during this operation. If the operation is a query, in the end, the data structure returns an answer based on the parameters and the contents in all cells probed during this operation. The update (query resp.) time is defined to be the number of cells probed during a(n) update (query resp.) operation. 

The cell-probe model only counts the number of memory accesses during each operation, making itself a strong model, e.g., it subsumes the word-RAM model. Thus, data structure lower bounds proved in this model will hold in various other settings as well. }
\iftoggle{conf}{}{
\subsection{Cell-probe Model}\label{subsectcp}
\cellprobepre
}

\subsection{Previous Cell-probe Lower Bounds}\label{subsectplb}

In 1989, Fredman and Saks introduced the \emph{chronogram} method to prove an  lower bound for partial sum in their seminal paper~\cite{FS89}. The lower bound is tight for maintaining a sequence of s.  was also the highest lower bound proved for any explicit data structure problem for a long time.

In 2004, P\v{a}tra\c{s}cu and Demaine~\cite{PD04a} broke this  barrier using a new approach: the \emph{information transfer tree} technique. They proved an  lower bound for the partial sum problem with numbers in . Moreover, using this new technique, one can prove an update-query time trade-off of , where  is the update time and  is the query time, while earlier approaches can only prove . Later on, the information transfer tree technique has been used to prove several other data structure lower bounds~\cite{PD04b, CJS15, CJ11, CJS13}.

In 2012, there was a breakthrough by Larsen~\cite{Larsen12a} on dynamic data structure lower bounds. Larsen combined the chronogram method with the \emph{cell sampling} technique of Panigraphy, Talwar and Wieder~\cite{PTW10}, and proved an  lower bound for the 2D orthogonal range counting problem. This lower bound is also the highest lower bound proved for any explicit dynamic data structure problem so far. Similar approaches were also applied later~\cite{Larsen12b, CGL15}. 

All above techniques can only be used to prove a relatively smooth trade-off between update and query time. However, P\v{a}tra\c{s}cu and Thorup~\cite{PT11} used a new idea to prove a sharp trade-off for the dynamic connectivity problem in undirected graphs. They proved that if one insists on  insertion and deletion time, query has to take  time. Besides the sharp trade-off, they also introduced the \emph{simulation by communication games} of the data structure. They first decomposed the entire execution of the data structure on a sequence of operations into several communication games. For each communication game, they showed how to turn a ``fast'' data structure into an efficient communication protocol. Then they proved a communication lower bound for each game. Summing all these lower bounds up establishes a lower bound on the total number of probes in the entire execution. 


\subsection{Technical Contributions}\label{subsecttc}
Although the \bps{} problem looks similar to the partial sum problem, it seems hopeless to apply the information transfer technique directly to solve our problem. It is due to a critical difference between the two problems: in partial sum, the lower bound proved roughly equals to the number of bits in the answer to a query; while in batch partial sum, the lower bound we aim at is much larger than the size of an answer. The proof in \cite{PD04a} heavily relies on the fact that in partial sum problem, after fixing the values in a lot of entries in the sequence, as long as there is still one summand in the prefix sum left uniformly at random, the sum will also be uniformly at random. Therefore, we will need to learn a certain amount of information from the memory to figure out the answer. If we apply the same technique in \bps{}, as an answer still contains only  bits of information, we will again get a lower bound of  per operation, while we aim at . The cell sampling technique has a similar issue. It can only be applied when the number of bits used to describe a query is comparable with the number of bits used in an answer. 

The main idea of our proof is to use the simulation by communication games technique mentioned in Section~\ref{subsectplb}. After decomposing into communication games, there are two things to prove: a ``fast'' data structure implies an efficient communication protocol, and no efficient communication protocol exists. The choice of communication model for the game is crucial. If we use a too weak communication model, it would be hard to take advantage of the model to design an efficient protocol given fast data structure. If the communication model we use is too strong, it would be difficult or even impossible to prove a communication lower bound, especially when small chance of error is allowed. P\v{a}tra\c{s}cu and Thorup gave two different simulations: one in the deterministic setting, the other in the nondeterministic setting. The deterministic simulation itself (transforming a data structure into a communication protocol) is not efficient enough to achieve our lower bound. The nondeterministic model, in our case of allowing error, would correspond to the distributional  model. It is particularly difficult to prove a lower bound in this model. In our application, the communication problem we want to prove a lower bound for is closely related to the inner product problem. Namely, Alice and Bob get -dimensional binary vectors  and  respectively and the goal is to compute the inner product  over . There is a clever  protocol by Aaronson and Wigderson~\cite{AW08} which solves the inner product problem with only  bits of communication. It has much less cost than expected, which suggests that it might even be impossible to prove a desired lower bound for our problem in this strong model. 

To overcome this obstacle, we define a new communication model (see Section~\ref{sectdiubps}), which is weaker than , so that we are capable of proving a desired communication lower bound. Moreover, we will be able to achieve the same performance of transforming data structure into protocol as in the nondeterministic model. Interestingly, in order to have less requirement on the power of communication model, we use an elegant protocol for computing sparse set disjointness by H\aa{}stad and Wigderson~\cite{HW07} as a subroutine:

\begin{theorem}[H\aa{}stad and Wigderson]\label{thmsparse}
In the model of common randomness,  for instances of disjoint sets and  for non-disjoint sets.
\end{theorem}

 is the following problem: Alice and Bob get sets  and  of size  over a universe  respectively, their goal is to compute whether the two sets are disjoint.  stands for the minimum expected communication cost by any zero-error protocol which computes . 

As we will see later, this new communication model has the power of nondeterminism. Also it is restricted enough so that we can apply the classic techniques for proving randomized communication lower bounds. Using this model, we prove the first sharp update-query trade-off under constant probability of error. 

\subsection{Overview}
The remainder of this paper is organized as follows. In Section~\ref{sectdiubps}, we present the reduction from \bps{} to \diu{}, and define the new communication model and the new simulation. In Section~\ref{sectcomm}, we prove a communication lower bound in this new model, which completes the proof of our main result. \iftoggle{conf}{Some proof details and the applications of our main theorem to dynamic graph problems can be found in the full version. }{In Section~\ref{sectapp}, we apply the main theorem to several dynamic graph problems. }Finally, we conclude with some remarks in Section~\ref{sectremark}.

\def\sectprecomm{
In the classic deterministic communication complexity setting~\cite{Yao79}, two players Alice and Bob receive inputs  and  respectively. Their goal is to collaboratively evaluate a function  on their joint input . The players send bits to each other according to some predefined protocol. At each step, the protocol must specify which player sends the next bit based on the transcript (the bits sent so far). The sender decides to send a bit 0 or 1 based on the transcript and his/her input. It the end, the answer  can only depend on the entire transcript. In the setting with public randomness, the players have access to a common random binary string of infinite length. Besides the sender's input and the transcript, each message may also depend on these random bits. The players have infinite computational power. The cost of a protocol is the number of bits communicated, i.e., the length of the transcript.

\begin{definition}
	For function  with domain , the matrix  is  a  matrix, with rows indexed by  and columns indexed by . The entry in row  and column  is the function value .
\end{definition}

\begin{definition}
A \emph{combinatorial rectangle} or simply a \emph{rectangle} in  is a set  for  and . 
\end{definition}

\begin{definition}
A \emph{monochromatic rectangle} in  is a combinatorial rectangle in which the function value does not vary. 
\end{definition}

\begin{definition}
Let  be a distribution over . A -\emph{monochromatic rectangle} under  is a combinatorial rectangle  such that there is a function value , , i.e., a combinatorial rectangle with at least -fraction of the input pairs having the same function value. 
\end{definition}

A classic result~\cite{BookKN97} in communication complexity is that every protocol in the deterministic setting with worst-case communication cost  induces a partitioning of  into  monochromatic rectangles. Each rectangle corresponds to one possible transcript, i.e., when the players are given an input pair in this rectangle, the corresponding transcript will be transmitted. A similar result shows that every randomized protocol with low error probability induces a partitioning into rectangles, such that \emph{most} of the rectangles are \emph{nearly monochromatic} (-monochromatic with  close to ). Proving there is no large monochromatic rectangle or nearly monochromatic rectangle in  would imply communication lower bounds in deterministic or randomized setting respectively.
}

\iftoggle{conf}{}{
\section{Preliminaries on Communication Complexity}\label{sectpre}
\sectprecomm
}

\section{Lower Bounds for Dynamic Interval Union and Batch Partial Sum}\label{sectdiubps}

In this session, we will prove our main result, a lower bound for the \diu{} problem, via a reduction from the \bps{} problem. 

\begin{customthm}{\ref{thmdiu}}
For any  and integer , there is a distribution over operation sequences to the \diu{} problem over , with  insertions and deletions, and  queries, for which any data structure that correctly answers all queries simultaneously with probability  must spend  probes in expectation, in the cell-probe model with word size . 
\end{customthm}

\begin{customthm}{\ref{thmbps}}
For large enough integers  with , there is a distribution over operation sequences to the \bps{} problem with  updates and  queries, for which any data structure that correctly answers all queries simultaneously with probability  must spend  probes in expectation, in the cell-probe model with word size . 
\end{customthm}

The idea of this reduction is similar to the proof of Proposition~\ref{redps} in Appendix~\ref{sectredps}. 

\begin{proof}[Proof of Theorem~\ref{thmdiu}]

Take prime ,  with , and . We are going to show that we can solve the \bps{} problem with this setting of the parameters given a \diu{} data structure over . We first concatenate the  sequences into one long sequence of length , such that  will be -th number in the long sequence, and try to maintain the whole sequence using one \diu{} data structure. Then we associate each number in the long sequence with a segment of length  in  such that the -th number in the long sequence is associated with . We use the length of interval in the associated segment to indicate the value of the number. That is, we always maintain the invariant that for -th number in the long sequence with non-zero value , we have exactly one interval  intersecting its associated segment. 

To set -th number to a new value , if before the operation it had value , we first call  to reset the number. Then if , we call  to update its new value to . Therefore, as an \verb+update+ of the \bps{} problem is just setting  numbers to new values, it can be implemented using  insertions and deletions of the \diu{} problem, with  extra probes to determine what the old value was and to record the new value.

To answer , we first insert intervals that correspond to associated segments of the -th number to the last number in sequence  for , to set everything we are not querying to be ``in the union'', no matter how much they were covered by intervals before. That is, we insert  for each sequence . Then we do one query, which will return the sum of all numbers as if each number not in the query was set to  (or  modulo ). This number modulo  is exactly the answer we want. At last, we do  deletions to remove the temporary intervals we inserted earlier for this query, and return the answer. Therefore, every \verb+query+ of \bps{} can be implemented using  insertions and deletions, and one query of the \diu{}. 

Thus, any sequence of  updates and  queries of \bps{} can be implemented using  insertions and deletions,  queries of \diu{}, and extra  probes. However, by Theorem~\ref{thmbps}, there is a hard distribution consisting of  updates and  queries, which requires  probes in expectation in the cell-probe model with word size . By the above reduction, this hard distribution also induces a distribution over operation sequences for \diu{} with desired number of updates, queries and lower bound on the number of probes. This proves the theorem. 

\end{proof}

By setting  in Theorem~\ref{thmdiu}, we get the following corollary. 
\begin{corollary}\label{coldiu}
Any \diu{} data structure that answers all queries correctly in a sequence of  operations with probability  with expected amortized insertion and deletion time  and query time  must have 

In particular,  implies that . 
\end{corollary}

In the following, we are going to prove a lower bound for the \bps{} problem. We will first specify a hard distribution over operation sequences. Then by Yao's Minimax Principle~\cite{Yao77}, it suffices to show that any \emph{deterministic} data structure that answers all queries correctly with high probability under this input distribution must be inefficient. To show this, we will consider a collection of communication games corresponding to different parts of the operation sequence (different time periods). For each communication game, if the data structure is \emph{fast} within the corresponding time period under certain measure of efficiency, then the game can be solved with \emph{low communication cost}. On the other hand, we will prove communication lower bounds for all these games. Summing these lower bounds up, we will be able to prove that the entire execution cannot be efficient. 

\paragraph*{Hard distribution :}

Without loss of generality, assume  is a power of , and . The operation sequence will always have  updates and  queries occurring alternatively: , where the 's are updates, and the 's are queries. The operations are indexed by integers between  and , or they can be viewed as being indexed by -bit binary strings (which corresponds to the binary representation of the integer). We may use either of these two views in the rest of the paper without further mention. Let  be a binary string,  be the string with 's bits reversed. For each , we set it to \verb+update(+\verb+, +\verb+)+ with  and  independently uniformly chosen from  for every . For each , we set it to \verb+query(+\verb+)+ with  independently and uniformly chosen from . Different operations are sampled independently. Indicate this distribution by . 

\paragraph*{Communication game:}

The hard distribution  has a fixed pattern of updates and queries. Let us fix two \emph{consecutive} intervals  of operations (with  before ) in the sequence. Define the communication game  between two players Alice and Bob to be the following: sample a sequence from , Alice is given all operations except for those in , Bob is given all operations except for those in , their goal is to cooperatively compute the answers to all queries in  by sending messages to each other alternatively.

Given a deterministic \bps{} data structure, a nature way to solve this game is to let Bob first simulate the data structure up to the beginning of , then skip all the operations in  and try to continue simulating on . Every time Bob needs to probe a cell, if it was probed in , he sends a message to Alice asking for the new value, otherwise he knows exactly what is in the cell from his own simulation. The challenge for Bob is to figure out which cells were probed. Our main idea is to introduce a prover Merlin, who knows both Alice and Bob's inputs. Merlin will tell them this information in a \emph{unique} and \emph{succinct} way. Moreover, the players will be able to \emph{verify} whether the message from Merlin is exactly what they expect, and will be able to solve the task efficiently if it is. This motivates the following definition of a new communication model.

\paragraph*{Communication model :}

Draw an input pair  from a known distribution. Alice is given , Bob is given  and Merlin is given both  and . Their goal is to compute some function . As part of the communication protocol, the players must specify a \emph{unique} string  for every possible input pair, which is the message Merlin is supposed to send. A communication procedure shall consist of the following four stages:

\begin{enumerate}
	\item
		Merlin sends a message  to Alice and Bob based on his input pair ; 
	\item
		Alice and Bob communicate based on  and  as in the classic communication setting with public randomness. Merlin does not see the random bits when sending the message ;
	\item
		Alice and Bob decide to accept or reject;
	\item
		if the players accept in Stage 3, they return a value .
\end{enumerate}

In this model, we say a protocol computes function  with error  and communication cost , if 
\begin{enumerate}
	\item
		Alice and Bob accept in Stage 3 if and only if Merlin sends what he is supposed to send, i.e.,  (with probability 1),
	\item
		given , the value  they return equals to  with probability  over the input distribution and public randomness,
	\item
		given , the expected number of bits communicated between the three players in Stage 1 and 2 is no more than  over the input distribution and public randomness.
\end{enumerate}


\begin{remark}
The public randomness used in Stage 2 does not help the players in general. Nevertheless, we still keep it in the definition for the sake of neatness of our proof. 
\end{remark}

 can be viewed as a question that the players want to ask Merlin about their joint input. One can potentially design more efficient protocols in this model than in the classic communication model if verifying the answer to this question is easier than computing it. 

With respect to this communication model, on one hand, we can show that given a ``good'' \bps{} data structure, we can solve the communication game efficiently (Lemma~\ref{commupper}). On the other hand, we prove a communication lower bound for it (Lemma~\ref{commlower}). Combining these two lemmas, we conclude that no ``good'' data structure exists. 

\begin{lemma}\label{commupper}
Given a deterministic \bps{} data structure for the cell-probe model with word size  that is correct on all queries in a random operation sequence drawn from  with probability , we can solve the communication game  with error  and cost  in \mm{}, where  ( resp.) is the (random) set of cells probed in time period  ( resp.) by the data structure. 
\end{lemma}

\begin{proof}

We prove the lemma by showing the following protocol is efficient in terms of  and . 

\paragraph*{Communication protocol:}

\begin{enumerate}[Step 1:]
	\setcounter{enumi}{0}
	\item
		(Merlin sends the key information)
		
		Merlin first simulates the data structure up to the beginning of , which is also the end of , and records the set , all the cells that were probed in time period . Then Merlin continues simulating the operations in . At the meanwhile, every time he probes a memory cell, he checks if this cell has been probed in  before and checks if it was probed in  (in set ). If this is the first time probing this cell since the beginning of , Merlin will send one bit to Alice and Bob. He sends ``1'' if the cell was probed in , and sends ``0'' otherwise.
		
\item
		(Alice and Bob simulate the data structure to accomplish the task)
		
		Alice simulates the data structure up to the beginning of , and records . Since Bob does not have any information about operations in , he instead simulates up to the beginning of , then tries to skip  and simulate the operations in  directly. Of course, the memory state Bob holds might be very different from what it should look like at the beginning of . But with the help of Merlin's message, Bob will be able to figure out the difference. 
		
		As Bob simulates the data structure, every he needs to probe a cell, he first checks if this is the first time probing this cell since the beginning of . If it is not, Bob knows its content from the last probe. Otherwise, he looks at the next bit of Merlin's message. If it is a ``0'', Merlin is claiming that this cell was not probed in . Thus, its content has not been changed since the beginning of . Bob has the information in his own copy of memory. If it is a ``1'', Bob sends the address of this cell to Alice, Alice will send back its content. At the same time, Alice checks if the cell was actually probed in . If the check fails, they report ``Merlin is cheating'' (they reject), and abort the protocol. At last, Bob updates this cell in his own copy of memory, and records that it has been probed in . 
		
		If there are no more bits left in Merlin's message when Bob needs to look at the next bit, or there are still unread bits when Bob has finished the simulation, the players reject. 
		
\item
		(Players verify that Merlin is truthful)
		
		According to the simulation in Step 2, Alice takes the set . Bob generates the set of cells that were probed in  but Merlin claims that they were not in  (and thus did not ask Alice for the contents). They check if these two sets of cells are disjoint (all cells that Merlin claims not probed in  are actually not) using the \emph{zero-error} sparse set disjointness protocol in Theorem~\ref{thmsparse} of H\aa{}stad and Wigderson. If the two sets intersect, they report ``Merlin is cheating'' (reject), otherwise they report ``Merlin is truthful'' (accept) and Bob returns the answers he has computed for all queries in . 
		
\end{enumerate}

Step 1 above describes what Merlin is \emph{supposed} to do, and thus defines . 
\iftoggle{conf}{
We can show that the protocol is a valid protocol in \mm{}, and solves the communication game efficiently. The detailed proof can be found in the full version.
}{
The following shows that the above protocol is a valid protocol in \mm{}, and solves the communication game efficiently. 

\begin{enumerate}
\item
	If Merlin tells the truth (), it is not hard to see that the players will always accept. If  is a prefix of  or  is a prefix of , Bob will detect it in Step 2 and reject. Otherwise let the -th bit be the first bit that  and  differ. As the first  bits are the same in  and , Bob will correctly simulate the data structure up to that point, right before a probe that causes Bob to read the -th bit of . Thus the cell probed by the data structure corresponding to the -th bit will be the same in Bob's simulation and in the actual execution.  If , the cell is not probed in  but Merlin claims it is. The players can detect the mistake and will reject in Step 2. If , Merlin claims the cell is not probed in  but it is. In this case, the cell will belong to both Alice's and Bob's sets in Step 3. Therefore, the sparse set disjointness protocol will return ``intersect''. The players will reject. This proves that Alice and Bob accept if and only if . 
\item
	Given , Bob will successfully simulate the data structure on all operations in . As the data structure correctly answers all queries simultaneously with  probability, in particular, it answers all queries in  correctly with  probability. Thus, the error probability is no more than . 
\item
	Given , Merlin sends exactly one bit for each cell in , . In Step 2, the players send  bits for every ``1'' in , which is  in total. In Step 3, by Theorem~\ref{thmsparse}, the players send  bits in expectation to compute sparse set disjointness. in expectation over the randomness of the protocol and the input distribution , the protocol uses 
	 bits of communication as we claimed. 
\end{enumerate}

This proves the lemma.
}

\end{proof}

Let  be a binary string of length less than . Define  to be the interval consisting operations st. Let  be the communication game defined by  and , e.g., in game , Alice receives all operations in the first half of the sequence as her input, and Bob receives the second half, in game , Alice receives the first quarter and the second half, Bob receives the second quarter and the second half. 
\begin{lemma}\label{commlower}
For  large enough, the communication game  requires communication cost at least  for any protocol with error  in \mm{}, where  is the length of string . 
\end{lemma}

We will defer the proof of Lemma~\ref{commlower} to Section~\ref{sectcomm}.
\iftoggle{conf}{Using these two lemmas, we will be able to prove Theorem~\ref{thmbps}. As the proof is similar to~\cite{PD04a,PT11}, we omit it in the conference version. See the full version for more details.}
{Using these two lemmas, we are ready to prove our data structure lower bound.

\begin{proof}[Proof of Theorem~\ref{thmbps}]
Fix a (randomized) data structure for \bps{} problem, which errors with probability no more than , and in expectation, probes  cells on an operation sequence drawn from . By Markov's inequality and union bound, there is a way to fix the random bits used by the data structure, such that the error probability is no more than , and probes at most  cells in expectation. In the following, we show that for such deterministic data structure,  must be large. 


For binary string  of length no more than , define  to be the (random) set of cells probed by the data structure in . For every , Lemma~\ref{commupper} and Lemma~\ref{commlower} together implies that 

Now sum up the two sides over all binary strings  of length at most . For the left-hand-side, fix an operation sequence. In the sum , every probe will be counted at most  times, because the probes during  or  will be counted only when  is a prefix of . In the sum , for each cell in , we refer it to its first probe in . Every probed will be referred to at most once: consider a probe during  or , assume the last probe to this cell happened during  or  for some , this probe will be referred only when  is a prefix of  and  is a prefix , i.e.,  is the longest common prefix of  and . Therefore, the left-hand-side sums up to at most . The right-hand-side sums up to


This implies , which proves the theorem.
\end{proof}
}

\section{Communication Lower Bound}\label{sectcomm}

Before proving the communication lower bound for the game  itself, we first do a ``clean-up'' to make the problem more accessible. In particular, we show that the communication game we want to prove a lower bound for is essentially the \emph{\mi{}} problem. 

In the \mi{} problem, Alice is given a vector . Bob is given an -tuple of vectors , such that  for each . Moreover, if we divide the  coordinates into  blocks of  coordinates each in the most natural way (first block is the first  coordinates, second block is the next  coordinates, etc), each  will be a -vector with \emph{at most} one  in each block. Their goal is to compute the  inner products over : . In other words, Alice gets an array, Bob gets  sets of indices of the array (of some restricted form). They want to figure out together for each set, what is the sum of elements in the corresponding entries. 

\iftoggle{conf}{
We claim that the communication game  is equivalent to the \mi{} problem with the same parameters  and  as in , parameter  and input distribution . Moreover,  is ``close'' to the uniform distribution. The detailed proof of equivalence can be found in the full version.

}{
Fix a communication game  defined by  and . By the way we set up the hard distribution , every update operation will always update the set of entries, only the values change. Therefore, the only thing about the sequences Bob does not know is the values in the entries that are updated in . As Bob knows the values in all other entries right before each query, the players' actual goal is to figure out the prefix sums as if there were only those unknown entries, which can be formulated as an instance of the \mi{} problem. Moreover, input distribution  will induce an input distribution for the \mi{} problem. We just need to prove a communication lower bound under that distribution. 

More specifically, let . Define the following function  which maps a sequence of operations , which is a possible outcome of , to an instance of the \mi{} problem. Consider all updates in interval , let  be the set of entries of sequence  which are updated in . To get a \mi{} instance, we set the -th coordinate in -th block of Alice's input  to be the sum of values in  first entries in  (the  entries with smallest indices), for  and . For Bob's input , consider the -th query in , let it be , querying the sum of first  numbers in sequence . For , assume there are  entries in  with indices at most , which will be summands in the -th query. If , we set all coordinates in the -th block of  to , otherwise, we set the -th coordinate in the block to . This defines the function . It is not hard to see that the inner product  encodes the dependence of -th query in  on updates in . Moreover, it is easy to verify that under the mapping of , the distribution over operation sequences  induces a distribution over the input pairs  for \mi{}, with some probability measure , such that

\begin{enumerate}
	\item
		 and  are independent under , i.e.,  is a product distribution;
	\item
		 is the uniform distribution over ;
	\item
		 is close to being uniform: all  blocks in all  vectors in  are independent, and in each block, each one of the  possibilities will occur with probability no more than , as adjacent elements in  are spaced by exactly  numbers. In particular,  for any singleton. 
\end{enumerate}

In the following, we will only use the above three properties of  in the proofs. }
In this setting, Alice's input carries  bits of information. Bob's input carries  bits of information. The following lemma shows that the best strategy is just to let one of the players send the whole input to the other, even with Merlin's help in \mm{}.


\begin{lemma}\label{lowermi}
For large enough  and , solving the \mi{} problem in \mm{} with error  requires communication cost  under input distribution .
\end{lemma}

\iftoggle{conf}{
}{
Before proving this lemma, we first show that it implies Lemma~\ref{commlower}. 

\begin{proof}[Proof of Lemma~\ref{commlower}]

Fix a protocol  for  with error  and cost . We are going to use it to solve \mi{}. Let us first assume that there is a sequence of public random bits that all three parties can see. We will first design a protocol in this setting, then try to get rid of this extra requirement by fixing the random bits.

For an input pair  for the \mi{} problem, consider the following protocol:
\paragraph*{Preprocessing:} Use the public randomness to sample an operation sequence from  conditioned on that  maps it to . It is easy to verify that all operations outside  does not dependent on  and all operations outside  does not depend on . Therefore, with no communication, all three parties get their inputs for . 
\paragraph*{Simulate :} Alice and Bob run protocol  to compute all answers to queries in . 
\paragraph*{Postprocessing:} Bob knows all updates outside , and from the value returned by the communication game, he gets to know the answers to all queries. Therefore, Bob can compute for -th query in , the sum of all entries updated in  that are summands of the query, which is exactly , by subtracting all other summands from the answer. With no further communication, Bob can figure out the solution to the \mi{} problem.

After Preprocessing, the inputs the players get for  will be distributed as . Therefore, in Simulate , the communication cost will be  in expectation, and error probability will be  over the randomness of , input  and random bits  used in Preprocessing. By Markov's inequality and union bound, there is a way to fix , such that the communication cost is at most  in expectation and error probability is at most  over the randomness of  and . To show that the protocol can be implemented in \mm{}, we hardwire  and define for each input pair ,  to be the message Merlin is supposed to send in Simulate  when Preprocessing uses random bits . Alice and Bob accept if and only if they were to accept in . 

It is easy to verify that the above protocol solves the \mi{} problem under input distribution  with error  and cost . However, by Lemma~\ref{lowermi}, we have a lower bound of  on the communication cost. Together with , we have . This proves the lemma.
\end{proof}
}

To prove a communication lower bound in \mm{}, the main idea is to use the uniqueness of the certificate () and the perfect completeness and soundness. To start with, let us first consider the case where the protocol is deterministic, and the communication cost  is defined in worst case instead of in expectation. 

In this case, fix one Merlin's possible message , it defines a communication problem between Alice and Bob in the classic model: check whether . By definition, the players can solve this task with zero error. By the classic monochromatic rectangle argument, we can partition the matrix  (defined in \iftoggle{conf}{Appendix~\ref{sectpre_app}}{Section~\ref{sectpre}}) into exponentially in  many combinatorial rectangles, such that in each rectangle, either  for every pair or  for every pair. In particular, it partitions the set  into combinatorial rectangles. Moreover, for each rectangle with , the protocol associates it with a value, which is the value returned in Stage 4. Now we go over all possible 's, which is again exponentially in  many. Every input pair belongs to exactly one of the 's. By cutting all , along with their partitioning into rectangles, and pasting into one single matrix, it induces a partitioning of the \emph{whole} matrix  into  rectangles. The values associated with the rectangles should match the actual function values with high probability. Therefore, there must be large \emph{nearly} monochromatic rectangles in . \iftoggle{conf}{\footnote{See Appendix~\ref{sectpre_app} for definitions.}}{}

A nature final step of the proof, as in many communication complexity lower bound proofs, would be to show that all nearly monochromatic rectangles are small. However, in the \mi{} problem, there do exist large monochromatic rectangles.

Fix a set  of coordinates, such that it has  coordinates in each block. Let , .  is a monochromatic rectangle with value , and . In particular, for , we can only prove lower bounds no better than  using this approach only. 

However, these 's are not what a ``typical'' Bob's input should look like. Since the ones in  only appear in -fraction of the coordinates, while a random input with very high probability should have ones appearing in a constant fraction of the coordinates. This motivates the following definition of \emph{\es{}}:
\begin{definition}
Define Bob's input to be \emph{\es}, if for any set of coordinates of size at most , the number of ones in all  vectors in Bob's input in these coordinates is no more than . 
\end{definition}

\iftoggle{conf}{By Hoeffding's inequality and union bound, we can prove that most of the inputs are \es{}. The detailed proof can be found in the full version.
}{}

\begin{lemma}\label{lemES}
	For  large enough, with probability , Bob's input is {\es}. 
\end{lemma}

\iftoggle{conf}{}{
\begin{proof}
	Draw a random input  from , and try to upper-bound the probability that it is not {\es}. Fix a set of coordinates  of size at most , where  is a subset of coordinates in block . Let  be the random variable , which is the number of ones in  that is in . By the properties of , in -th block of , each of the  possibilities occurs with probability no more than , and is independent of all other blocks and vectors. Thus, we have that , , and 's are independent. Let , be the number of ones in all  vectors in . We have . Thus, by Hoeffding's inequality, 
	
	
	However, the number of possible set 's is no more than . By union bound, the probability that there exists an  violating the constraint is at most , for large enough .
\end{proof}
}

Instead of upper-bounding how many input pairs a nearly monochromatic rectangle can contain, we are going to upper-bound the measure of \es{} inputs in it. Define set  to be the all input pairs  that  is \emph{not} \es{}. By Lemma~\ref{lemES}, . The following lemma shows that there are no large nearly monochromatic rectangles if we ignore all elements in . 

\begin{lemma}\label{almonorect}
For large enough , every -monochromatic rectangle , which is disjoint from , must have 
\end{lemma}

\iftoggle{conf}{
Using the above lemma, we are ready to prove Lemma~\ref{lowermi}. Both proofs can be found in the full version.
}
{
\begin{proof}
Fix a combinatorial rectangle , such that there is a value  that . We want to prove that  must be small. First, without loss of generality, we can assume that for every , , i.e., every column in  is -monochromatic. Since in general, by Markov's inequality, at least  (with respect to ) of the columns in  are -monochromatic, we can just apply the following argument to the subrectangle induced by  and these columns, and only lose a factor of . 

Let , . Let  be the dimension of subspace in  spanned by vectors in first  -tuples: , and . Let  be the dimension of the subspace spanned by all vectors in . Define  to be the probability distribution over  such that .  is proportional to ``the number of new dimensions  introduces''. Thus,  is supported on no more than  elements in .

Since every column in  is 0.6-monochromatic under ,  will also be 0.6-monochromatic under . By Markov's inequality again, at least 1/5 of the rows are 0.5-monochromatic in  under . However, when  is large, there cannot be too many such rows, even in the whole matrix. For some 0.5-monochromatic row , let  be the set of columns with value  in that row. By definition, we have . By the way we set up the distribution , the linear space spanned by all vectors in  must have dimension at least . This adds at least  independent linear constraints on . There can be at most -fraction of 's satisfying all linear constraints in the whole matrix. By taking a union bound on all possible 's, we obtain an upper bound on the measure of 0.5-monochromatic rows in  under . More formally, we have



Therefore, if , we have . 

Otherwise, . In this case, we are going to show that, it is impossible to pack too many vectors the the form of Bob's inputs into any subspace of small dimension. In particular, we will upper bound , the measure of Bob's \es{} inputs, when their span has dimension . Fix a basis of the span of vectors in , consisting of  vectors in .\footnote{These vectors do not have to be from Bob's inputs.} Without loss of generality, we can assume that for each basis vector, there is a coordinate in which this vector has value 1, and all other basis vectors have value 0, because we can always run a standard Gaussian elimination to transform the basis into this form. Let  be this set of  coordinates. As each vector in the subspace is a linear combination of the basis, fixing the values in coordinates in  uniquely determines a vector in the subspace. By definition of \es{}, and , each -tuple  can have at most  1's in . For all  blocks in the  vectors, there are  choices for the set of blocks with ones. Moreover, each  can have at most one 1 in each block. If a vector has a 1 in a block, there will be at most  different choices to place the 1. Otherwise, we must set all coordinates in  in that block to be . After fixing values all coordinates in , there can be at most one such vector in the subspace with matching values. Thus, we have


Therefore, in this case, we have . Combining both cases, we conclude that . 
\end{proof}

Using the above lemma, we can prove communication lower bound for \mi{}.

\begin{proof}[Proof of Lemma~\ref{lowermi}]
Fix a protocol that solves the \mi{} problem with error 0.15 and cost  in \mm{}. As we are working with a fixed input distribution, randomness in the protocol shall not help. In particular, by Markov's inequality and union bound, there is a way to fix the internal (public) randomness of the protocol, such that the success probability is at least  and communication cost no more than . From now on, let us assume the protocol is deterministic, and success probability is at least . 

For some message  sent by Merlin, let  be set of the input pairs  such that  is message Merlin is supposed to send when the players get these input pairs. As Alice and Bob are always able to decide whether , the classical combinatorial rectangle argument induces a way to partition each  into rectangles based on the transcript between Alice and Bob. Moreover, the set  induces a partition of all possible input pairs. Therefore, provided that Merlin tells the truth, the entire transcript , which includes both Merlin's message and the transcript between Alice and Bob, induces a partition of the matrix  into combinatorial rectangles . For each  with transcript , the players will return a fixed answer  for every pair of inputs in the rectangle. 

By the definition of communication cost, we have . Thus, by Markov's inequality


By the definition of probability of computing  correctly, 

By Markov's inequality, 


Thus, we have


Let  be the (disjoint) union of all  with value  and transcript , such that 

and . By (\ref{eqn1}) and (\ref{eqn2}), we have . However, there can be only  different such 's, and thus  such rectangles. There must be some transcript  such that  and  is -monochromatic rectangle under distribution . Therefore, by Lemma~\ref{almonorect}, we have , which proves the lemma. 

\end{proof}

\section{Applications to Dynamic Graph Problems}\label{sectapp}
In this section, we present three applications of our main theorem to dynamic graph problems. See Appendix~\ref{sectcata} for formal definitions of the problems.

\begin{corollary}\label{cordyngraph}
For the following three dynamic graph problems:
\begin{enumerate}[(a)]
	\item
		dynamic \#SCC,
	\item
		dynamic planar s-t min-cost flow,
	\item
		dynamic weight s-t shortest path,
\end{enumerate}
any data structure with amortized expected update time , and error probability  under polynomially many operations must have amortized expected query time . 
\end{corollary}

\begin{proof}(sketch)
To prove the lower bounds, we are going to give three reductions from \diu{}. The corollary follows from Corollary~\ref{coldiu}. 
\begin{enumerate}[(a)]
	\item To solve \diu{}, we maintain the following graph :  has a Hamiltonian path ; for every ,  has an edge . It is not hard to see that the graph can be maintained efficiently given a dynamic \#SCC data structure, and the total length of the union of  is exactly  minus the number of strongly connected components in . See Figure~\ref{figscc}. 
	
\begin{figure}
\centering
\tikz[node distance=35pt]{
	\tikzstyle{vertex}=[circle,draw,minimum size=14pt, inner sep=0];
	\node[vertex] (v0) {};
	\node[vertex, right of=v0] (v1) {};
	\node[vertex, right of=v1] (v2) {};
	\node[vertex, right of=v2] (v3) {};
	\node[vertex, right of=v3] (v4) {};
	\draw[->,semithick] (v0) -> (v1);
	\draw[->,semithick] (v1) -> (v2);
	\draw[->,semithick] (v2) -> (v3);
	\draw[dotted, thick] (v3) -- (v4);
	\draw[->,thick] (v2) edge [bend right=40] (v0);
	\draw[->,thick] (v3) edge [bend right=40] (v1);
}
\caption{dynamic \#SCC}\label{figscc}
\end{figure}

	\item
	The underlying graph is shown as Figure~\ref{figflow}. The edges connecting vertices  and  have infinite capacities and zero cost. The edges connect  and  have capacities 1 and cost -1. The edges connecting  and  have capacities 1 and cost 0. All edges connecting to  or  have cost 0. The only values that may change are the capacities of edges connecting to  or . More specifically, we maintain the graph such that the capacity from  to vertex  always equals to the number of intervals in  with left endpoint , the capacity from  to  always equals to the number of intervals with right endpoint . It is easy to verify that given a dynamic planar s-t min-cost flow data structure, we can efficiently maintain these invariants. To query the total length of the union of , we query the min-cost flow in  with flow value . For each , the amount of flow from  to  (also counting flow going through ) is exactly the number of intervals containing . As  has a smaller cost than going to  directly from , min-cost flow will prefer to use the former path. Each  contained in any intervals in  adds a cost of  to the flow. Therefore, the negate of the cost is exactly the length of the union. 
	
\begin{figure}
\centering
\tikz[node distance=50pt]{
	\tikzstyle{vertex}=[circle,draw,minimum size=14pt, inner sep=0];
	\node[vertex] (v0) {};
	\node[vertex, right of=v0] (v1) {};
	\node[vertex, right of=v1] (v2) {};
	\node[vertex, right of=v2] (v3) {};
	\node[vertex, right of=v3] (v4) {};
	\node[vertex, below of=v0, node distance=80pt] (vs) {};
	\node[vertex, above of=v4, node distance=80pt] (vt) {};
	\node[vertex, above of=v0, xshift=25pt, node distance=25pt] (v0') {};
	\node[vertex, above of=v1, xshift=25pt, node distance=25pt] (v1') {};
	\node[vertex, above of=v2, xshift=25pt, node distance=25pt] (v2') {};
	\draw[thick] (vs) -- (v0) -- 
	node[below] {\scriptsize } (v1) -- 
	node[below] {\scriptsize } (v2) -- 
	node[below] {\scriptsize } (v3);
	\draw[thick] (v4) -- (vt);
	\draw[thick] (v0) --
	node[right,xshift=-6pt, yshift=-4pt] {\tiny } (v0') -- 
	node[right,xshift=-7pt, yshift=5pt] {\tiny } (v1) -- 
	node[right,xshift=-6pt, yshift=-4pt] {\tiny } (v1') -- 
	node[right,xshift=-7pt, yshift=5pt] {\tiny } (v2) -- 
	node[right,xshift=-6pt, yshift=-4pt] {\tiny } (v2') -- 
	node[right,xshift=-7pt, yshift=5pt] {\tiny } (v3);
	\draw[dotted, thick] (v3) -- (v4);
	\path (v0) edge [thick, out=70, in=170] (vt);
	\path (v1) edge [thick, out=75, in=180] (vt);
	\path (v2) edge [thick, out=80, in=190] (vt);
	\path (v3) edge [thick, out=85, in=200] node[right] {\tiny } (vt);
	\path (vs) edge [thick, out=40, in=-100] node[left] {\tiny } (v1);
	\path (vs) edge [thick, out=30, in=-110] (v2);
	\path (vs) edge [thick, out=20, in=-120] (v3);
	\path (vs) edge [thick, out=10, in=-130] (v4);
}
\caption{dynamic planar s-t min-cost flow}\label{figflow}
\end{figure}	
	
	\item 
	
	We maintain a graph  such that there is an edge from  to  with weight 0, an edge from  to  with weight , edges from  to  with weight 1, and edges from  to  with weight . Moreover, for each interval , the graph has an edge  with weight 0. The shortest path from  to  is exactly the length of the union, because for  contained in any interval in , we can go from  to  with zero cost: go to the left endpoint of the interval, then go to the right endpoint using one edge, and go to . See Figure~\ref{figsp}. 
	
\begin{figure}
\centering
\tikz[node distance=30pt]{
	\tikzstyle{vertex}=[circle,draw,minimum size=14pt, inner sep=0];
	\node[vertex] (v0) {};
	\node[vertex, right of=v0] (v1) {};
	\node[vertex, right of=v1] (v2) {};
	\node[vertex, right of=v2] (v3) {};
	\node[vertex, right of=v3] (v4) {};
	\node[vertex, below of=v0, node distance=50pt] (vs) {};
	\node[vertex, above of=v4, node distance=50pt] (vt) {};
	\draw [thick,->] (vs) -> node[right,xshift=-3pt] {\scriptsize 0} (v0);
	\draw [thick,->] (v0) -> node[above,yshift=-3pt] {\scriptsize 1} (v1);
	\draw [thick,->] (v1) -> node[above,yshift=-3pt] {\scriptsize 1} (v2);
	\draw [thick,->] (v2) -> node[above,yshift=-3pt] {\scriptsize 1} (v3);
	\draw [thick,dotted] (v3) -- node (v35){} (v4);
	\draw [thick,->] (v4) -> node[left,xshift=3pt] {\scriptsize 0} (vt);
	\path (v1) edge [draw, thick,->,bend left=45] node[below,yshift=3pt] {\scriptsize 0} (v0);
	\path (v2) edge [draw, thick,->,bend left=45] node[below,yshift=3pt] {\scriptsize 0} (v1);
	\path (v3) edge [draw, thick,->,bend left=45] node[below,yshift=3pt] {\scriptsize 0} (v2);
	\path (v0) edge [draw, thick,->,bend left=45] node[above,yshift=-3pt] {\scriptsize 0} (v2);
	\path (v1) edge [draw, thick,->,bend left=45] node[above,yshift=-3pt] {\scriptsize 0} (v35);
}
\caption{dynamic weight s-t shortest path}\label{figsp}
\end{figure}
\end{enumerate}
\end{proof}
}

\section{Final Remarks}\label{sectremark}
In~\cite{CGL15}, Clifford, Gr\o{}nlund and Larsen mentioned a  barrier for dynamic data structure lower bounds, where  is the number of different queries (including parameters), and  is the number operations in the sequence we analyse. In some sense, our main result can also be viewed as a ``-type'' lower bound, as the query takes only  bits to describe. The way we prove this type of lower bound is very different from~\cite{Larsen12a, Larsen12b, CGL15}. We obtain this kind of the lower bound via reduction. Assume we want to prove a lower bound for problem . We first prove a lower bound for some other problem  with large , and get a high lower bound. Then we find a way to implement updates of  using updates of , and queries of  using updates and queries of , and thus derive a lower bound for . Note that it is important that we implement queries in problem  using \emph{both} updates and queries in the original problem. Because if we only use queries to implement queries, in order to keep all the information in the query of , it has to be decomposed into many queries of . A simple calculation shows that in this case, we cannot break the barrier for problem  unless we have already broken it for problem . However, if we use both updates and queries of , it is possible to ``hide'' information in the updates. A good example is the reduction in Proposition~\ref{redps} in Appendix~\ref{sectredps}. However, using this approach, we still cannot beat , where  is the number of different \emph{updates} (including parameters). Nevertheless, it gives us a potential way to break the  barrier for problems with  if we can combine it with the previous techniques. 

P\v{a}tra\c{s}cu has used the communication lower bound for lopsided set disjointness, set disjointness problem of a special form, to prove a collection of (static) data structure lower bounds~\cite{Pat08, Pat11}. In this paper, we applied communication \emph{protocol (upper bound)} for sparse set disjointness, set disjointness of a different special form, to prove a dynamic data structure \emph{lower bound}. In some sense, this can be viewed as an analogue of the recent development in duality between algorithm and complexity~\cite{Williams13, Williams14, AWY15} in the communication complexity and data structure world. It would be interesting to see examples where both communication lower bound and upper bound for the exact same problem can be used to prove data structure lower bounds.

On Klee's measure problem, our result is an unconditional lower bound for one certain type of algorithms. From Theorem~\ref{thmdiu}, we can generate a hard input distribution for the sweep-line algorithm, such that if the algorithm only sorts the rectangles, goes through the entire area row by row and computes the number of grids in the union only based on the rectangles intersecting the current row or previous rows, then it cannot beat Bentley's algorithm. However, our hard distribution is not very robust, in the sense that if we do the sweep-line from a different direct, the distribution over inputs becomes really easy. At least, it still shows us what an  time algorithm for computing 2D Klee's measure problem on  should \emph{not} look like, if exists. 

\paragraph*{Acknowledgement.}
The author would like to thank Yuqing Ai and Jian Li for introducing Klee's measure problem to me during a discussion, and would like to thank Timothy Chan for telling me the state-of-the-art.

 The author also wishes to thank Ryan Williams for helpful discussions on applications to dynamic graph problems and in paper-writing. 


\bibliographystyle{plain}
\bibliography{dynintun}

\appendix

\section{Reduction from Partial Sum}\label{sectredps}
The partial sum problem is to maintain a sequence of  numbers  over , supporting the following operations:
\begin{itemize}
	\item
		\verb+update(i, v)+: set  to value ;
	\item
		\verb+query(l)+: return .
\end{itemize}

P\v{a}tra\c{s}cu and Demaine~\cite{PD04a} showed that at least one of the operations needs to take  time in the cell-probe model with word size . 

\begin{proposition}\label{redps}
Any data structure for the \diu{} problem with insertion time , deletion time  and query time  must have  in the cell-probe model with word size . 
\end{proposition}

\begin{proof}
Consider the partial sum problem with  numbers over . Any data structure will take  time to update or query. Fix a \diu{} data structure over , we will use it to solve the partial sum problem. First partition  into  blocks of length  each. The -th block will correspond to number . We maintain the invariant that for each number , there is an interval in the corresponding block of length equal to the value of . 

More specifically, every time we need to update  from value  to , we delete the previous interval , then insert a new interval . When we need to query the sum of first  numbers, we first insert an interval , which covers all blocks from the -th to the last, then query the length of the union, delete the interval inserted earlier. We know that the temporarily inserted interval has length . Subtracting it from the answer returned, we get the total length of intervals in the first  blocks, which is exactly the sum of first  numbers.

Every update of partial sum can be implemented using an insertion and a deletion of \diu{}, every query can be implemented using an insertion, a deletion and a query. Therefore, at least one of the operations has to take  time. 
\end{proof}

\section{Catalogue of Dynamic Graph Problems in Our Application}\label{sectcata}

The \emph{dynamic \#SCC} problem is to maintain a directed graph , supporting:
\begin{itemize}
	\item
		\verb+insert(u, v)+: insert an edge ;
	\item
		\verb+delete(u, v)+: delete an (existing) edge ;
	\item
		\verb+query()+: return the number of strongly connected components in .
\end{itemize}

The \emph{dynamic planar s-t min-cost flow} problem is to maintain an undirected planar flow network  with edge cost, supporting:
\begin{itemize}
	\item
		\verb+update(u, v, cap)+: update the capacity of (an existing) edge  to \verb+cap+;
	\item
		\verb+query(f)+: return the min-cost flow from a fixed source  to a fixed sink  with flow value . 
\end{itemize}

The \emph{dynamic weighted s-t shortest path} problem is to maintain a weighted directed graph , supporting:
\begin{itemize}
	\item
		\verb+insert(u, v, w)+: insert an edge  with weight ;
	\item
		\verb+delete(u, v)+: delete an (existing) edge ;
	\item
		\verb+query()+: return the shortest path from a fixed source  to a fixed target .
\end{itemize}

\iftoggle{conf}{
\section{Preliminaries on Cell-probe Model}\label{sectcp_app}
\cellprobepre

\section{Preliminaries on Communication Complexity}\label{sectpre_app}
\sectprecomm

\section{Full Version}
}{}

\end{document}
