

\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[dvipsnames, table, xcdraw]{xcolor}



\usepackage[pagebackref,breaklinks,colorlinks,citecolor=RoyalPurple]{hyperref}

\usepackage{amsmath}
\usepackage{amssymb}


\def\confYear{2023}
\graphicspath{{./figures/}}

\usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         

\usepackage[group-separator={,},group-minimum-digits={3}]{siunitx}
\usepackage{color}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{overpic}
\usepackage{multirow}
\newcommand{\myPara}[1]{\vspace{.08in} \noindent\textbf{#1}}


\usepackage{cancel}  
\usepackage[normalem]{ulem}
\usepackage{mathtools}

\definecolor{citecolor}{HTML}{0071bc}
\usepackage{url}

\usepackage{booktabs}       \usepackage{xcolor}         \usepackage{soul} 

\usepackage{color}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{arydshln}
\newcommand{\gsh}[1]{{\textcolor{red}{#1}}}
\def\Real{\mathbb{R}}



\newcommand{\pz}[1]{{\color{blue}{[#1]}}}




\begin{document}

\title{Masked Diffusion Transformer is a Strong Image Synthesizer}

\author{Shanghua Gao
\thanks{This work was done while Shanghua Gao was a research intern at Sea AI Lab.} \quad
Pan Zhou \thanks{Pan Zhou and Ming-Ming Cheng are joint corresponding authors.} \quad
Ming-Ming Cheng \quad
Shuicheng Yan \\
Sea AI Lab \quad Nankai University \\
\texttt{\{shanghuagao,shuicheng.yan\}@gmail.com  zhoupan@sea.com cmm@nankai.edu.cn} \\
}



\maketitle



\begin{abstract}


Despite its success in image synthesis, 	
we observe that diffusion probabilistic models (DPMs) often 
lack contextual reasoning ability to learn the relations among object parts 
in an image, leading to a slow learning process. 
To solve this issue, we propose a Masked Diffusion Transformer (MDT) 
that introduces a mask latent modeling scheme to 
explicitly enhance the DPMs' ability of contextual relation learning among object semantic parts in an image. 
During training,
MDT operates on the latent space to mask certain tokens.
Then, an asymmetric masking diffusion transformer 
is designed to predict masked tokens from unmasked ones while maintaining the diffusion generation process.
Our MDT can reconstruct the full information of an image 
from its incomplete contextual input, 
thus enabling it to learn the associated relations among image tokens. 
Experimental results show that MDT achieves superior 
image synthesis performance, \eg~a new SoTA FID score on the ImageNet
dataset, and has about 3 faster learning speed 
than the previous SoTA DiT. 
The source code is released at \url{https://github.com/sail-sg/MDT}.

\end{abstract}



\newcommand{\addImg}[2]{\includegraphics[width=0.19\linewidth]{#1/sample304_#2}}

\newcommand{\addImgs}[1]{\addImg{#1}{05} &\addImg{#1}{1} &\addImg{#1}{2} &\addImg{#1}{3} &\addImg{#1}{30}}

\newcommand{\rotT}[1]{\rotatebox{90}{\footnotesize \qquad #1}}


\begin{figure}[t]
  \footnotesize
  \centering
  \setlength{\tabcolsep}{0.2mm}
  \renewcommand{\arraystretch}{0.6}
  \begin{tabular}{lcccccccc}
    \rotT{DiT} & \addImgs{dit} \\
    \rotT{MDT} & \addImgs{mdt} \\
    & 50k & 100k & 200k & 300k & 3000k \\
    \multicolumn{6}{c}{Increasing training steps.}
  \end{tabular} \\
  \subfloat{\footnotesize \hspace{3pt}
  \begin{overpic}[width=0.475\linewidth]{figures/converge_comp.pdf}
    \put(67,79){DiT-S/2}
    \put(67,72){MDT-S/2}
    \put(-7,18){\rotT{FID-50K}}
    \put(30,-8){Training steps (k)}
    \put(34,22){\large 3.2}
  \end{overpic}\vspace{10pt}
  } \hfill
  \subfloat{\footnotesize
  \begin{overpic}[width=0.475\linewidth]{figures/converge_time_comp.pdf} 
    \put(67,79){DiT-S/2}
    \put(67,72){MDT-S/2}
    \put(30,-7){Training times (days)}
    \put(32,20){\large 3.0}
  \end{overpic}
  } \vspace{10pt}
  \caption{Top: Visualized example of MDT/DiT~\cite{peebles2022scalable} 
    along with training steps.
    Down: learning progress comparison between DiT and MDT w.r.t. training steps/times on 8 A100 GPUs.
    MDT has about 3 faster learning speed than DiT while achieving superior FID scores. 
	}\label{fig:converge_comp}
\end{figure}

\section{Introduction}
Diffusion probabilistic models (DPMs)~\cite{dhariwal2021diffusion,rombach2022high} 
have been at the forefront of recent advances in image-level
generative models,
often surpassing the previously state-of-the-art (SoTA)
generative adversarial networks (GANs)~\cite{brock2018large,wu2019logan,razavi2019generating}. 
Additionally, DPMs have demonstrated their success in numerous other applications, 
including text-to-image generation~\cite{rombach2022high}
and speech generation~\cite{jeong2021diff}.   
DPMs adopt a time-inverted Stochastic Differential Equation (SDE) to gradually map a Gaussian noise into a sample by multiple time steps,
with each step corresponding to a network evaluation.
In practice, generating a sample is time-consuming due to the thousands of time steps required for the SDE to converge.  
To address this issue, various generation sampling strategies~\cite{ho2020denoising,lu2022dpm,salimans2022progressive} 
have been proposed to accelerate the inference speed. 
Nevertheless, 
improving the training speed of DPMs is 
less explored but highly desired. 
Training of DPMs also unavoidably requires a large number of time steps to ensure the convergence of SDEs, 
making it very computationally expensive,
especially in this era where large-scale models~\cite{dhariwal2021diffusion,peebles2022scalable} and data~\cite{imagenet_cvpr09,schuhmann2022laion,gao2021luss} 
are often used to improve generation performance. 



In this work, we first observe that DPMs often struggle to learn the associated relations among object parts  in an image. 
This leads to its slow learning process during training.   
Specifically, as illustrated by~\figref{fig:converge_comp}, 
 the classical DPM, DDPM~\cite{ho2020denoising} with DiT~\cite{peebles2022scalable} as backbone,  
 has learned the shape of a dog at the 50k-th training step,
 then learns its one eye and mouth until at the 200k-th step while missing another eye.
 Also, the relative position of two ears is not very accurate even at the 300k-th step.   
 This learning process reveals that DPMs fail to 
learn the associated relations among semantic parts
and indeed independently learn each semantic.  The reason behind this phenomenon is that DPMs maximize the log probability of real data by minimizing the per-pixel prediction loss, 
which ignores the associated relations among object parts in an image, thus resulting in their slow learning progress.  
 

Inspired by the above observation, 
we propose an effective Masked Diffusion Transformer (MDT) to improve the training efficiency of DPMs.    
MDT proposes a mask latent modeling scheme  
designed for transformer-based DPMs to 
explicitly enhance contextual learning ability and improve the associated relation learning 
among semantics in an image.  
Specifically, following~\cite{rombach2022high,peebles2022scalable}, MDT operates the diffusion process in the latent space to save computational cost. 
It  masks certain image tokens, and designs an asymmetric masking diffusion transformer (AMDT) to predict  masked  tokens from unmasked  ones  in a diffusion generation manner. 
To this end, AMDT contains an encoder, a side-interpolater and a decoder.  The encoder and decoder  modify  the  transformer block  in DiT~\cite{peebles2022scalable} via  inserting  global and  local  token position information to help predict  masked tokens.  
The encoder  only  processes unmasked  tokens during training while  handling all tokens during inference as there are no masks. 
So to ensure decoder always processes all tokens for training prediction  or inference  generation, 
a side-interpolater implemented by a small network aims to predict masked tokens from  encoder output during training, and it is removed during inference. 


By this masking latent modeling scheme,  our MDT can reconstruct full information of an image from its contextual incomplete input, 
learning the associated relations among semantics in an image. 
As shown in ~\figref{fig:converge_comp},
MDT typically generates
two eyes (and two ears) of the dog at almost the same training steps,
indicating that it correctly learns the associated semantics of an image by utilizing the mask latent modeling scheme.
In contrast,  DiT~\cite{peebles2022scalable} cannot easily synthesize
a dog with the correct semantic relations among its parts.
This comparison shows the superior relation modeling and faster learning ability of MDT over DiT.  
Experimental results demonstrate that MDT achieves superior performance on the image synthesis task, and set the new SoTA on class-conditional image synthesis on the ImageNet dataset,
as shown in~\figref{fig:sample} and \tabref{tab:sota_comp}.
MDT also enjoys about 3 faster learning progress during training than the SoTA DPMs, namely DIT, as demonstrated by \figref{fig:converge_comp} and  \tabref{tab:ditvsmdt}.  
We hope our work can inspire more works on speeding up the diffusion training process with unified representation learning.  

The main contributions are summarised as follows:
\begin{itemize}
\item Our proposed masked diffusion transformer introduces an effective mask latent
modeling scheme into DPMs and also accordingly designs an asymmetric
masking diffusion transformer. Our method is the first one that aims to 
explicitly enhance contextual learning ability and improve the relation learning among image semantics for DPMs. 
\item Experiments show that our masked diffusion transformer enjoys higher performance on image synthesis and greatly improves the learning progress during training.
It achieves the new SoTA for image synthesis.
\end{itemize}
 


\newcommand{\addvis}[1]{\includegraphics[width=0.24\linewidth]{figures/vis/#1.jpg}}



\begin{figure}[t]
  \footnotesize
  \centering
  \setlength{\tabcolsep}{0.1mm}
  \renewcommand{\arraystretch}{0.1}
  \begin{tabular}{ccccccccc}
    \addvis{sample_cls13each3} & \addvis{sample_cls249each1} & \addvis{sample_cls108each1} & \addvis{sample_cls933each2}  \\
    \addvis{sample_cls484each1} & \addvis{sample_cls977each1} & \addvis{sample_cls438each0} & \addvis{sample_cls889each1}  \\
    \addvis{sample_cls356each1} & \addvis{sample_cls389each2} & \addvis{sample_cls394each1} & \addvis{sample_cls264each3}  \\
    \addvis{sample_cls292each1} & \addvis{sample_cls18each1} & \addvis{sample_cls271each0} & \addvis{sample_cls374each1}  \\
  \end{tabular} \\
  \vspace{3pt}
  \caption{Visualization of images generated by the MDT-XL/2.}
  \label{fig:sample}
\end{figure}
	
\section{Related works}
\subsection{Diffusion Probabilistic models}
Diffusion probabilistic model (DPM)~\cite{ho2020denoising,dhariwal2021diffusion}, 
also known as score-based model~\cite{song2019generative,song2020improved},
is a competitive approach for image synthesis.
DPMs begin by using an evolving Stochastic Differential Equation (SDE) to gradually add   
Gaussian noise into real data, transforming a complex data distribution into a  
Gaussian distribution.  
Then, it adopts a time-inverted SDE to gradually map a Gaussian noise into a sample by multiple steps.
At each sampling time step, a network is utilized to generate the sample along the gradient of the log probability, also known as the score function~\cite{song2020score}.
The iterative nature of diffusion models can result in high training and inference costs. 
Efficient sampling strategies~\cite{ho2020denoising,lu2022dpm,salimans2022progressive,ho2022classifier,song2020denoising}, 
latent space diffusion~\cite{rombach2022high,vahdat2021score},
and multi-resolution cascaded generation~\cite{ho2022cascaded}
have been proposed to reduce the inference cost.
Additionally,
some training schemes~\cite{dockhorn2021score,bao2022estimating} are introduced to improve the diffusion model training,
\eg approximate maximum likelihood training~\cite{song2021maximum,nichol2021improved,kingma2021variational},
training loss weighting~\cite{kim2021soft,Karras2022edm}.
In contrast to them optimizing the diffusion training process,
we identify the lack of contextual modeling ability in diffusion models.
To address this,
we propose the mask latent modeling scheme
as a complementary approach to enhancing the contextual representation of diffusion models,
which is orthogonal to existing diffusion training schemes. 


\begin{figure*}[!t]
	\centering
	\begin{overpic}[width=1\linewidth]{figures/overall.pdf}
	\end{overpic}
	\caption{The overall framework of Masked Diffusion Transformer (MDT).
		Solid/dotted line indicates the training/inference process for each time step.
		Masking and side-interpolater are 
	 only used during training and are  removed during inference. 
	}\label{fig:overall}
\end{figure*}

\subsection{Networks for Diffusion Models}
The UNet-like~\cite{ronneberger2015u} network, enhanced by 
spatial self-attention~\cite{salimans2017pixelcnn++,van2016conditional}
and group normalization~\cite{wu2018group}
is firstly used for diffusion models~\cite{ho2020denoising}.
Several design improvements, \eg adding more attention heads,
BigGAN~\cite{brock2018large} residual block, and adaptive group normalization, 
are proposed in\cite{dhariwal2021diffusion} to further enhance the generation ability of the UNet.
Recently, due to the broad applicability of transformer networks,
several works have attempted to utilize the vision transformer (ViT) structure for 
diffusion models~\cite{yang2022your,bao2022all,peebles2022scalable}.
GenViT~\cite{yang2022your} demonstrates that ViT is capable of image generation but has inferior performance
compared to UNet.
U-ViT~\cite{bao2022all} improves ViT by adding long-skip connections and convolutional layers,
achieving competitive performance with that of UNet.
DiT~\cite{peebles2022scalable} verifies the scaling ability of ViT on large model sizes and feature
resolutions.
Our MDT is orthogonal to these diffusion networks as it focuses on
contextual representation learning.
Moreover, the position-aware designs in MDT reveal that
the mask latent modeling scheme benefits from a stronger diffusion network.
We will explore further to release the potential of these networks with MDT. 


\subsection{Mask Modeling}
Mask modeling
has been proven to be effective in both recognition learning~\cite{devlin2018bert,he2022masked,gao2022towards} 
and generative modeling~\cite{radford2018improving,chang2022maskgit}.
In the natural language processing (NLP) field, 
mask modeling was first introduced to
enable representation pretraining~\cite{devlin2018bert,radford2018improving}
and language generation~\cite{brown2020language}.
Subsequently,
it also proved feasible for 
vision recognition~\cite{bao2021beit} and generation~\cite{zhang2021m6,chang2022maskgit,gu2022vector} tasks.
In vision recognition,
pretraining schemes that utilize mask modeling
enable good representation quality~\cite{zhou2021ibot}, scalability~\cite{he2022masked} 
and faster convergence~\cite{gao2022towards}.
In generative modeling,
following the bi-directional generative modeling in NLP,
MaskGIT~\cite{chang2022maskgit} and MUSE~\cite{chang2023muse} 
use the masked generative
transformer to predict randomly masked image tokens for image generation.
Similarly, VQ-Diffusion~\cite{gu2022vector} presents a mask-replace diffusion strategy
to generate images.
In contrast,
our MDT aims to enhance the contextual representation of the denoising diffusion
transformer~\cite{peebles2022scalable} with mask latent modeling.
This preserves the detail refinement ability of denoising diffusion models
by maintaining the diffusion process during inference.
To ensure that the mask latent modeling in MDT focuses on representation learning
instead of reconstruction, we propose an asymmetrical structure in mask modeling training.
As an extra benefit, it enables lower training costs than masked generative models
because it
skips the masked patches in training instead of replacing masked input patches with a mask token.




\section{Revisitation of Diffusion Probabilistic Model}
\label{sec:ddpm}
For diffusion probabilistic models~\cite{dhariwal2021diffusion,sohl2015deep}, 
such as DDPM~\cite{ho2020denoising} and DDIM~\cite{song2020denoising}, 
training involves a forward noising process and a reverse denoising process. 
In the forward noising process, Gaussian noise  is gradually added  
to the real sample  via a discrete SDE of formulation  ,
where  denotes the noise magnitude. If the time step  is large,  would be a Gaussian noise. Similarly, the reverse denoising process is a discrete SDE that gradually maps a Gaussian
noise into a sample. At each  time step, given , it predicts the next reverse step
 via a network.
The network is trained by
optimizing the variational lower-bound   of 
 ~\cite{sohl2015deep}, where 
  .
Following~\cite{nichol2021improved,peebles2022scalable},
we obtain  by optimizing ,
and we reparameterize  as a noise prediction network 
and train it with a simple mean-squared error loss, \ie ,
where  is the ground truth Gaussian noise.
During inference, one can sample  a Gaussian noise
and then gradually reverses to a sample .


Same as~\cite{nichol2021improved,peebles2022scalable},
we train the diffusion model conditioned with class label ,
\ie .
By default, we use class-conditioned image generation in our experiments.





\section{Masked Diffusion Transformer}
\subsection{Overview}
\label{sec:mdt_method}
 As shown in~\figref{fig:converge_comp}, DPMs with DiT backbone exhibit slow 
 training convergence due to the slowly
 learning of the associated relations among semantics in an image. 
 To relieve this issue,  we  propose Masked Diffusion Transformer (MDT), which introduces  a mask latent modeling scheme to explicitly enhance contextual learning ability
 and to improve the capability of establishing associated relations among different semantics in an image.  
 To this end, as depicted in   in~\figref{fig:overall}, 
 MDT consists of 1) a latent masking operation to mask the input image in the latent space,  
 and 2) an asymmetric masking diffusion transformer that performs vanilla diffusion process as DPMs, but with masked input.  
 To reduce computational costs,  
 MDT follows LatentDiffusion~\cite{rombach2022high} to perform generative learning in the latent space instead of raw pixel space.  
 
In the training phase, MDT first encodes an image into a latent space with a pre-trained VAE encoder~\cite{rombach2022high}. 
The latent masking operation in MDT then adds Gaussian noise into the image latent embedding, 
patchifies the resulting noisy latent embedding into a sequence of tokens, 
and masks certain tokens. 
The remaining unmasked tokens are fed into the asymmetric masking diffusion transformer which contains an encoder, a side-interpolater, 
and a decoder to predict the masked tokens from the unmasked ones.  
During inference, MDT replaces the side-interpolater with a position embedding adding operation.
MDT takes the latent embedding of a Gaussian noise as input to generate the denoised latent embedding, which is then passed to a pre-trained VAE decoder~\cite{rombach2022high} for image generation. 


The above masking latent modeling scheme in the training phase forces the diffusion model 
to reconstruct full information of an image from its contextual incomplete input.
Thereby, the model is encouraged to learn the relations among image latent tokens, 
particularly the associated relations among semantics in an image. 
For example, as illustrated in~\figref{fig:overall},
the model should first well understand the correct associated relations among small image parts (tokens) of the dog image.
Then, it should generate the masked ``eye" tokens by using other unmasked tokens as contextual information. 
Furthermore, \figref{fig:converge_comp} shows that MDT often learns to generate the associated semantics of an image at nearly the same pace, 
such as the generation of the two eyes (two ears) of the dog at the almost same training step.
While DiT~\cite{peebles2022scalable}  (DDPM with transformer backbone) learns to generate one eye (one ear) initially 
and then learns to generate another eye (ear) after roughly 100k training steps. 
This demonstrates the superior learning ability of MDT over DiT in terms of the associated relation learning of image semantics. 


In the following parts, we will introduce the two key components of MDT, 
1) a latent masking operation,  
and 2) an asymmetric masking diffusion transformer.  



\subsection{Latent Masking}	\label{sec:diff_train} 
Following the Latent diffusion model (LDM)~\cite{rombach2022high}, 
MDT performs generation learning in the latent space instead of raw pixel space 
to reduce computational costs. 
In the following, we briefly recall LDM
and then introduce our latent masking operation on the latent input. 



\myPara{Latent diffusion model (LDM).}
LDM employs a pre-trained VAE encoder  to encode an image 

to a latent embedding . 
It gradually adds noise to  in the forward process 
and then denoises to predict  in the reverse process.  
Finally, LDM uses a pre-trained VAE decoder  to decode 
 into a high-resolution image . 
Both VAE encoder and decoder are kept fixed
during training and inference.
Since  and  are smaller than  and , 
performing the diffusion process in the low-resolution latent space is more efficient compared to 
 the pixel space. In this work, we adopt the efficient diffusion process of LDM.  


\myPara{Latent  masking operation.} Now we present our masking scheme on latent input.  
During training, we first add Gaussian noise to the latent embedding  of an image. 
Then following~\cite{peebles2022scalable}, we divide the noisy embedding  into 
a sequence of -sized tokens, 
and concatenate them to a matrix , 
where  is the channel number and  is the number of tokens.  
Next, we randomly mask tokens with a  ratio 
and concatenate the remaining tokens as  ,
where .  
Accordingly, we can create a binary mask   in which one (zero) denotes the masked (unmasked) tokens.   
Finally, we feed the  unmasked tokens  into our diffusion model for processing.  
We only use unmasked tokens , 
because 1)
The model should focus on learning semantics
 instead of predicting the masked tokens.
As shown in~\secref{sec:abl}, it achieves better performance than
replacing the masked tokens with a learnable mask token 
and then processing all tokens like~\cite{bao2021beit,chang2022maskgit,chang2023muse} ; 
2) It saves the training cost compared to processing all  tokens.  

\begin{figure}[!t]
	\centering
	\begin{overpic}[width=1\linewidth]{figures/block.pdf}
	\end{overpic}
	\caption{The asymmetric masking diffusion transformer in MDT.
	We modify the DiT~\cite{peebles2022scalable} by adding a side-interpolater, local relative positional bias, and learnable global position embeddings.
	The conditional scheme is omitted for simplicity.
	}\label{fig:block}
\end{figure}

\subsection{Asymmetric Masking Diffusion Transformer}
We introduce our asymmetric masking diffusion transformer for performing joint training of mask latent modeling and diffusion process. 
As shown in~\figref{fig:block}, it consists of three components: an encoder, a side-interpolater and a decoder, each of which is described in detail at below. 



\myPara{Position-aware encoder and decoder.} 
In MDT, predicting the masked latent tokens  from the unmasked tokens 
requires the position relations of all tokens.
To enhance the position information in the model, we propose positional-aware encoder and decoder that 
facilitate the learning of the masked latent tokens.  
Specifically, the encoder and decoder tailor the standard DiT block via adding two kinds of token position information, and respectively contain  and  tailored blocks. 

Firstly,  as illustrated in~\figref{fig:block},  the encoder adds the conventional learnable global position embedding into the noisy latent embedding input. 
Similarly,  the decoder also introduces the learnable position embedding into its input but with different approaches in the training and inference phases. During training, the side-interpolater already uses  the learnable global position embedding as introduced below, which can deliver the global position information to the decoder. During inference, since the side interpolater is discarded (see below),  the decoder explicitly adds the position embedding into its input to enhance positional information.


Secondly,  as shown in~\figref{fig:block},   the encoder and decoder  
   add a local relative positional bias~\cite{liu2021swin} to each head in each block when computing
the attention score of the self-attention~\cite{vaswani2017attention}: 

where , , and   respectively denote the query, key and value in self-attention module,  is the  dimension of the key, 
and  is the relative positional bias
that is selected by the relative positional difference  between the -th position  and other positions, \ie . The learnable mapping 
 is updated during training. 
The  local relative positional bias helps to capture the relative relations
among tokens, facilitating the masking latent  modeling.

The encoder takes the unmasked noisy latent embedding provided by our latent masking operation, 
and feeds its output into the side-interpolater/decoder during training/inference.   
For decoder, its input is the output of side-interpolater for training or the combination of the encoder output and the learnable position embedding for inference. 
Since during training, the encoder and decoder respectively handle unmasked tokens and full tokens, 
we call our model as the  ``asymmetric" model.  

 

\myPara{Side-interpolater.} As shown in Fig.~\ref{fig:overall}, during training, for efficiency and better performance, 
the encoder only processes the  unmasked tokens .
While in the inference phase,  
the encoder handles all tokens  due to the lack of masks. 
This means that there is a big difference in encoder output (\ie decoder input) during training and inference, at least in terms of token number. 
To ensure decoder always processes all tokens for training prediction  or inference  generation, 
side-interpolater implemented by a small network aims to predict masked tokens from encoder output during training and would be removed during inference. 


In the training phase, the encoder processes the unmasked tokens to obtain its output token embedding .  
Then as shown in Fig.~\ref{fig:overall}, the side-interpolater first fills the masked positions,
indicated by the mask  defined in Sec.~\ref{sec:diff_train}, by a shared learnable mask token, 
and also adds a learnable positional embedding  to obtain embedding . 
Next, we use a basic block of the encoder to process  to predict an interpolated embedding . 
The tokens in  denote the predicted tokens. 
Finally,  we use a masked shortcut connection to combine prediction  and  as  .  
In summary, for masked tokens, we use the prediction by side-interpolater;  for unmasked tokens, we still adopt the corresponding tokens in . 
This can 
1) boost the consistency between training and inference phases, 
2) eliminates the mask-reconstruction process in the decoder.

Since there are no masks during inference, the side-interpolater is replaced by a position embedding operation which adds the learnable position embeddings  
of side-interpolater  which is learned during training. 
This ensures the decoder always processes all tokens  and uses the same learnable position embeddings for training prediction  or inference  generation,  
thus having better image generation performance. 

	
	
\subsection{Training}
During training, we feed both full latent embedding  and the masked latent embedding  to the diffusion model,
since we observe that only using masked latent embedding makes the model
focus too much on masked region reconstruction while ignoring the diffusion training.
The training objectives for full/masked latent inputs both follow the description in~\secref{sec:ddpm}.
Due to the asymmetrical masking structure, the extra costs for using masked latent embedding
is small.  This is also demonstrated by~\figref{fig:converge_comp} which shows that  MDT still achieves about  3 faster learning progress than previous SoTA DiT  in terms of total training hours. 




\begin{table}
    \centering
    \setlength{\tabcolsep}{0.19mm} \small
    \begin{tabular}{lcccccc}
        \toprule
        Method	& Cost(IterBS) &  FID & sFID & IS & Prec. & Rec. \\	\midrule
        DCTrans.\cite{nash2021generating} & -  & 36.51 & - & - & 0.36 & 0.67 \\
        VQVAE-2\cite{razavi2019generating} & - & 31.11 & - &  - & 0.36 & 0.57 \\
        VQGAN\cite{esser2021taming} & - & 15.78 & 78.3 & - & - & - \\
        BigGAN-deep\cite{brock2018large} & -  & 6.95 &  7.36 &  171.4 &  0.87 &  0.28 \\
        StyleGAN\cite{sauer2022stylegan} & -   & 2.30 & 4.02  & 265.12 &  0.78 & 0.53 \\
        Impr. DDPM\cite{nichol2021improved} & - & 12.26 & - & - & 0.70 & 0.62 \\
        MaskGIT\cite{chang2022maskgit} & 1387k256 & 6.18 &  - & 182.1 & 0.80 & 0.51 \\
        CDM\cite{ho2022cascaded}  & - & 4.88  & - & 158.71 & - & - \\
        \midrule
        ADM\cite{dhariwal2021diffusion}  &1980k256& 10.94 & 6.02 & 100.98 & 0.69 & 0.63 \\
        LDM-8\cite{rombach2022high} & 4800k64 & 15.51 & - & 79.03 & 0.65 & 0.63 \\
        LDM-4 & 178k1200 & 10.56  & -  & 103.49 & 0.71  & 0.62 \\
        \hdashline
        DiT-XL/2\cite{peebles2022scalable} & 7000k256  & 9.62  & 6.85  & 121.50  & 0.67  & 0.67 \\
        \textbf{MDT} & 2500k256  & 7.41  & 4.95 & 121.22 & 0.72 & 0.64 \\
        \textbf{MDT} & 3500k256  & 6.46  & \textbf{4.92} & 131.70 & \textbf{0.72} & 0.63 \\
        \textbf{MDT} & 6500k256  & \textbf{6.23}  & 5.23 & \textbf{143.02} & 0.71 & 0.65 \\
        \midrule
        ADM-G\cite{dhariwal2021diffusion} &1980k256 & 4.59 & 5.25 & 186.70 & 0.82 & 0.52 \\
        ADM-G, U & 1980k256 & 3.94 & 6.14 & 215.84 & 0.83 & 0.53 \\
        LDM-8-G\cite{rombach2022high} & 4800k64 & 7.76 &  - & 209.52 & 0.84 & 0.35 \\
        LDM-4-G & 178k1200 & 3.60 & - & 247.67  & 0.87  & 0.48 \\  
        U-ViT-G~\cite{bao2022all}     & 300k1024 & 3.40 & - & - & - & - \\
        \hdashline
        DiT-XL/2-G\cite{peebles2022scalable} & 7000k256 & 2.27 & 4.60 & 278.24 & 0.83 & 0.57 \\
        \textbf{MDT-G} & 2500k256  & 2.15  & 4.52 & 249.27 & 0.82 & 0.58 \\
        \textbf{MDT-G} & 3500k256  & 2.02  & \textbf{4.46} & 263.77 & 0.82 & 0.60 \\ 
        \textbf{MDT-G} & 6500k256  & \textbf{1.79}  & 4.57 & \textbf{283.01} & 0.81 & \textbf{0.61} \\         
        \bottomrule
    \end{tabular}
    \vspace{2pt}
    \caption{Comparison with existing methods on class-conditional image generation 
    with the ImageNet 256256 dataset. 
    -G denotes the results with classifier-free guidance~\cite{ho2022classifier}.
    Results of MDT-XL/2 model are given for comparison.
    Compared results are obtained from their papers.}
    \label{tab:sota_comp}
    \end{table}


\section{Experiments}\label{exp}

\subsection{Implementation}
We give the implementation details
of MDT, including model architecture, training details,
 and evaluation metrics. 


\myPara{Model architecture.} 
We follow DiT~\cite{peebles2022scalable} to set the total block number (\ie~), token number, and channel numbers of the diffusion transformer of MDT. 
As DiT reveals stronger synthesis performance when using a smaller patch size, 
we also use a patch size =2 by default,
denoted by MDT-/2. Moreover, We also follow DiT's parameters to design MDT for getting its small-, base-, and xlarge-sized model, denoted by MDT-S/B/XL.
Same as LatentDiffusion~\cite{rombach2022high} and DiT,
MDT adopts the fixed 
VAE\footnote{The model is downloaded in \url{https://huggingface.co/stabilityai/sd-vae-ft-mse}} provided by the Stable Diffusion
to encode/decode the image/latent tokens by default.
The VAE encoder has a downsampling ratio of , and a  feature channel dimension of 4, 
\eg~an image of size 2562563 is encoded into a latent embedding of size 32324.

\myPara{Training details.}
Following~\cite{peebles2022scalable},
all models are trained by AdamW~\cite{loshchilov2017decoupled} optimizer
of 3e-4 learning rate, 256 batch size, and without weight decay on ImageNet~\cite{imagenet_cvpr09} 
with an image resolution of 256256.
We set the mask ratio as 0.3 and . 
Following the training settings in DiT,
we set the maximum step in training to 1000 and use the linear
variance schedule with a range from  to .
Other settings are also aligned with DiT.




\myPara{Evaluation.}
We evaluate models with commonly used metrics,
\ie Fre'chet Inception Distance (FID)~\cite{heusel2017gans},
sFID~\cite{nash2021generating}, Inception Score (IS)~\cite{salimans2016improved}, 
Precision and Recall~\cite{kynkaanniemi2019improved}.
The FID is used as the major metric as it measures both diversity and fidelity.
sFID improves upon FID by evaluating at the spatial level.
As a complement, IS and Precision are used for measuring fidelity,
and Recall is used to measure diversity.
For fair comparisons, we follow~\cite{peebles2022scalable}
to use the TensorFlow evaluation suite from ADM~\cite{dhariwal2021diffusion}
and report FID-50K with 250 DDPM sampling steps.
Unless specified otherwise,
we report the FID scores without the classifier-free guidance~\cite{ho2022classifier}.



\begin{table}
\centering
\setlength{\tabcolsep}{2.2mm} \small
\begin{tabular}{lcccccc}
\toprule
Method & Image Res. & Training Steps (k) & FID-50K  \\	\midrule
DiT-S/2 & 256256 & 400 & 68.40   \\ 
\hdashline
MDT-S/2 & 256256 & 300 & 57.01   \\
MDT-S/2 & 256256 & 400 & 53.46   \\
MDT-S/2 & 256256 & 2000 & 44.14  \\ 
MDT-S/2 & 256256 & 3500 & \textbf{41.37}  \\ 
\midrule
DiT-B/2 & 256256 & 400 & 43.47   \\ \hdashline
MDT-B/2 & 256256 & 400 & 34.33   \\
MDT-B/2 & 256256 & 3500 & \textbf{20.45}  \\
\midrule
DiT-XL/2 & 256256 & 400 & 19.47   \\
DiT-XL/2  & 256256 & 2352 & 10.67   \\
DiT-XL/2  & 256256 & 7000 & 9.62   \\
\hdashline
MDT-XL/2 & 256256 & 400 & 16.42    \\
MDT-XL/2 & 256256 & \textbf{1300} & \textbf{9.60}    \\
MDT-XL/2 & 256256 & 3500 & \textbf{6.65}    \\
\bottomrule
\end{tabular}
\vspace{2pt}
\caption{Comparison between DiT~\cite{peebles2022scalable} and MDT under 
different model sizes and training steps on ImageNet 256256. 
DiT results are obtained from DiT reported results.
}
\label{tab:ditvsmdt}
\end{table}

\subsection{Comparison Results}

\myPara{Performance comparison.}
\tabref{tab:ditvsmdt}  compares our MDT with the SoTA DiT  
under different model sizes. 
It is evident that 
MDT achieves higher FID scores for all model scales with fewer training costs.
The parameters and inference cost of MDTs are similar to DiT, since 
  the extra modules in MDT are negligible as introduced in~\secref{sec:mdt_method}. 
For small models, MDT-S/2 trained with 300k steps outperforms the DiT-S/2
trained with 400k steps by a large margin on FID (57.01 vs. 68.40). More importantly,
MDT-S/2 trained with 2000k steps achieves similar performance with a larger model DiT-B/2 trained with a similar computational budget.
For the largest model,
MDT-XL/2 trained with 1300k steps outperforms
DiT-XL/2 trained with 7000k steps on FID (9.60 vs. 9.62),
achieving about 5 faster training progress.

We also compare the class-conditional image generation performance of MDT with existing methods
in~\tabref{tab:sota_comp}. 
To make fair comparisons with DiT,
we also use the EMA weights of VAE decoder in this table.
Under class-conditional settings, MDT with half training iterations outperforms DiT by a large margin,
\eg 6.83 vs 9.62 in FID.
Following previous works~\cite{rombach2022high,dhariwal2021diffusion,peebles2022scalable,bao2022all},
we utilize an improved classifier-free guidance~\cite{ho2022classifier}
with a power-cosine weight scaling
to trade off between precision and recall
during class-conditional sampling.
MDT achieves superior performance over  
previous SoTA DiT and other methods with the FID score of 1.81, \emph{setting a new SoTA for class-conditional image generation}.
Similar to DiT,
we never observe the model has saturated FID scores when continuing training.

\myPara{Convergence speed.}
\figref{fig:converge_comp} compares the performance of the DiT/S-2 baseline and MDT/S-2 
under different training steps and training time
  on  8A100 GPUs.
Because of the stronger contextual learning ability,
MDT achieves better performance with faster generation learning speed.
MDT enjoys about 3 faster learning speed in terms of both training steps and training time. 
For example,  MDT-S/2 trained with about 33 hours (400k steps) achieves superior  performance 
than DiT-S/2 trained with 
about 100 hours (1500k steps). 
This reveals that contextual learning is vital for
faster generation learning of diffusion models.

\begin{table}[t!]
\centering
\small
\setlength{\tabcolsep}{1.8mm} \begin{tabular}{ccccccc}
    \toprule
    Mask Ratio	 &  FID & sFID & IS & Precision & Recall \\	\midrule
    0.1 & 51.60 & 10.23 & 26.65 & 0.44 & 0.60   \\
    0.2 & 51.44 & 10.09 & 26.75 & 0.44 & 0.58    \\
    \textbf{0.3} & \textbf{50.26} & \textbf{10.08} & \textbf{27.61} & \textbf{0.45} & 0.60   \\
    0.4 & 50.88 & 10.21 & 27.44 & 0.45 & 0.60  \\
    0.5 & 51.57 & 9.92  & 27.14 & 0.44 & 0.60  \\
    0.6 & 53.20 & 10.36 & 26.55 & 0.44 & 0.61   \\
    0.7 & 52.90 & 10.03 & 26.51 & 0.44 & 0.61 \\
    0.8 & 53.73 & 10.15 & 25.55 & 0.43 & \textbf{0.61} \\
    \bottomrule
\end{tabular}
\vspace{2pt}
\caption{Effect of different masking ratios. MDT-S/2 trained with 600k iterations. 
}
\label{tab:maskratio}
\end{table}

\subsection{Ablation}
\label{sec:abl}
In this part, we conduct ablation to verify the designs in MDT.
We report the results of MDT-S/2 model and use FID-50k as the evaluation
metric unless otherwise stated.

\begin{table}[!t]
    \centering
    \small
    \setlength{\tabcolsep}{1.7mm} \begin{tabular}{ccccccc}
        \toprule
        Decoder pos.	 &  FID & sFID & IS & Precision & Recall \\	\midrule
        Last0  & 51.05 & 9.97 & 27.31 & 0.44 & 0.60 \\
        Last1  &  50.96 & \textbf{9.90} & \textbf{27.63}  & 0.45 & 0.60 \\
        \textbf{Last2} & \textbf{50.26} & 10.08 & 27.61 & \textbf{0.45} & \textbf{0.60}   \\
        Last4  & 51.67 & 10.12 & 26.91  & 0.45 & 0.60 \\
        Last6  & 52.64 & 10.36 & 26.46  & 0.44 & 0.60 \\
        \bottomrule
    \end{tabular}
    \vspace{2pt}
    \caption{Effect of position of side-interpolater. 
    MDT-S/2 models contain 12 blocks and are trained with 600k iterations.
    \label{tab:decoder_pos}
    }
    \end{table}

\begin{table*}
\centering
\small
\subfloat[Effect of asymmetric masking structure.]{
\centering
\setlength{\tabcolsep}{3.8mm}
\begin{tabular}{cc}
\toprule
    Asymmetric stru. & FID-50k \\	\midrule
     &     51.56  \\ 
 &     \textbf{50.26}  \\ 
\bottomrule
\end{tabular}
\label{tab:asymmetric_mask}
}
\subfloat[Effect of side-interpolater.]{
\centering
\setlength{\tabcolsep}{4.0mm}
\begin{tabular}{cc}
\toprule
Side-interpolater & FID-50k \\	\midrule
     &     51.60  \\ 
 &     \textbf{50.26}  \\ 
\bottomrule
\end{tabular}
\label{tab:side_interpolater}
}
\subfloat[Effect of masked shortcut.]{
\centering
\setlength{\tabcolsep}{4.0mm}
\begin{tabular}{cc}
\toprule
Masked shortcut & FID-50k \\	\midrule
     &     50.91  \\ 
 &     \textbf{50.26}  \\ 
\bottomrule
\end{tabular}
\label{tab:masking_shortcut}
} \\
\subfloat[Using full/masked latent under aligned cost.]{
\centering
\setlength{\tabcolsep}{5.0mm}
\begin{tabular}{cc}
\toprule
Latent type & FID-50k \\	\midrule
Full+Masked &     \textbf{50.26}  \\ 
Full        &      52.30 \\ 
Masked      &  76.63     \\ 
\bottomrule
\end{tabular}
\label{tab:used_latent}
}
\subfloat[Supervision on token parts.]{
\centering
\setlength{\tabcolsep}{6.0mm}
\renewcommand{\arraystretch}{1.33}
\begin{tabular}{cc}
\toprule
Sup. parts & FID-50k \\	\midrule
All &     \textbf{50.26}  \\ 
Masked &   58.35    \\ 
\bottomrule
\end{tabular}
\label{tab:mask_sup}
}
\subfloat[Number of blocks in side-interpolator.]{
\centering
\setlength{\tabcolsep}{6.5mm}
\begin{tabular}{cc}
\toprule
Number & FID-50k \\	\midrule
1 &     \textbf{50.26}  \\ 
2 &       51.77 \\ 
3 &       51.96  \\ 
\bottomrule
\end{tabular}
\label{tab:num_block_si}
} \\
\subfloat[Effect of positional embeddings in SI.]{
\centering
\setlength{\tabcolsep}{4.0mm}
\begin{tabular}{cc}
\toprule
    IS Pos. embed. & FID-50k \\	\midrule
     &     51.58  \\ 
 &     \textbf{50.26}  \\ 
\bottomrule
\end{tabular}
\label{tab:pos_embed_si}
}
\subfloat[Effect of learnable positional embeddings.]{
\centering
\setlength{\tabcolsep}{5.0mm}
\begin{tabular}{cc}
\toprule
Learnable pos. & FID-50k \\	\midrule
     &     50.80  \\ 
 &     \textbf{50.26}  \\ 
\bottomrule
\end{tabular}
\label{tab:learn_pos}
}
\subfloat[Effect of relative positional bias.]{
\centering
\setlength{\tabcolsep}{4.0mm}
\begin{tabular}{cc}
\toprule
Relative pos. bias & FID-50k \\	\midrule
     &     53.56  \\ 
 &     \textbf{50.26}  \\ 
\bottomrule
\end{tabular}
\label{tab:rel_bias}
}
\caption{Ablation study on MDT-S/2. Models are trained for 600k iterations. 
}
\end{table*}

\myPara{Masking ratio.} 
The masking ratio determines the number of input patches that
can be processed during training.
We give the comparison of using different masking ratios
in~\tabref{tab:maskratio}.
The best masking ratio for MDT-S/2 is 30\%,
which is quite different from the masking ratio used
for recognition models, e.g.   75\%
masking ratio in MAE~\cite{he2022masked}.
We assume that 
the image generation requires learning
more details from more patches for high-quality synthesis,
while recognition models only need
the most essential patches to infer semantics.




\myPara{Asymmetric vs. Symmetric architecture in masking.}
Unlike the masked generation works~\cite{chang2022maskgit,chang2023muse},
\eg MaskGIT,
that utilize the masking scheme to generate images,
MDT focuses on improving diffusion models with contextual learning ability  via
the masking latent modeling.
Therefore, we use an asymmetric architecture to only process the unmasked tokens
in the diffusion model encoder.
We compare the asymmetric architecture in MDT and the symmetrical architecture~\cite{chang2022maskgit}
that processes full input with masked tokens replaced by a learnable mask token.
As shown in~\tabref{tab:asymmetric_mask},
the asymmetric architecture in MDT has an FID of 50.26, outperforming
 the FID of 51.56 
 achieved by the symmetric architecture.
The asymmetric architecture further reduces the training cost and allows  
the diffusion model to focus on learning contextual information instead of
reconstructing masked tokens.
 
\myPara{Full and masked latent tokens.}
In MDT, both the full and masked latent embeddings
are fed into the diffusion model during training.
In comparison, we give the results trained by only using full/masked latent embeddings
as shown in~\tabref{tab:used_latent},
where the computational cost is aligned for fair comparisons.
Trained with both full and masked latent leads to clear gain
over two competitors.
While using only the masked latent embeddings results in slow convergence,
which we attribute to the training/inference inconsistency
as the inference in MDT is a diffusion process instead of the masked reconstruction process.


\myPara{Loss on all tokens.}
By default, we calculate the loss on both masked and unmasked latent embeddings.
In comparison, mask modeling for recognition models commonly 
calculates loss on masked tokens~\cite{he2022masked,bao2021beit}.
\tabref{tab:mask_sup} shows that
calculating the loss on all tokens is much better than on masked tokens.
We assume that this is because generative models require stronger consistency among patches than recognition models do,
since details are vital for high-quality image synthesis.

\myPara{Effect of side-interpolater.}
The side-interpolater in MDT predicts the masked tokens,
allowing the diffusion model to learn more semantics
and maintain consistency in decoder inputs during training and inference.
We compare the performance with/without the side-interpolater in
\tabref{tab:side_interpolater},
and observe a gain of
1.34 in FID when using the side-interpolater,
proving its effectiveness.

\myPara{Masked shortcut in side-interpolater.}
The masked shortcut ensures that  the side-interpolater only predicts the
masked tokens from unmasked ones. 
\tabref{tab:masking_shortcut} shows
that using the masked shortcut enhances the FID
from 50.91 to 50.26,
indicating that restricting side-interpolater
to only predict masked tokens helps the diffusion model achieve stronger performance.

\myPara{Side-interpolater position.}
To meet the high-quality image generation requirements of the diffusion model,
the side-interpolater is placed
in the middle of the network instead
of the end of the network in recognition models~\cite{he2022masked, bao2021beit}.
\tabref{tab:decoder_pos},
presents the comparison of placing 
the side-interpolater
at  different positions of the MDT-S model
with 12 blocks.
The results show that placing the side-interpolater before the last two blocks achieves the best FID score,
whereas placing it at the end of the network like recognition models impairs the performance.
Placing the side-interpolater at
the early stages of the network also harm the performance,
indicating the mask latent modeling 
is beneficial to most stages in the diffusion models.

\myPara{Block number in side-interpolater.}
We compare the performance of different numbers of blocks in the side-interpolater
in~\tabref{tab:num_block_si}.
The default setting of 1 block achieves the best performance,
and the FID worsens with an increase in block number.
This result is consistent with our motivation that side-interpolater
should not learn too much information other than interpolating the masked representations.

\myPara{Positional-aware enhancement.}
To further release the potential of mask latent modeling,
we enhance the DiT baseline with stronger positional awareness ability,
\ie learnable positional embeddings and the relative positional bias in basic blocks.
\tabref{tab:pos_embed_si} shows the positional embeddings in side-interpolater
improves the FID from 51.58 to 50.26, indicating the positional embedding
is vital for the side-interpolater.
Also, enables the training of positional embeddings brings
the gain in FID as revealed in~\tabref{tab:learn_pos}.
In~\tabref{tab:rel_bias},
the relative positional bias in the basic blocks significantly improves
the FID from 53.56 to 50.26, showing
the relative positional modeling ability is essential
for diffusion models to obtain the contextual representation ability
and generate high-quality images.
Therefore, the positional awareness ability in diffusion model structure
is required to accompany the masked latent modeling,
playing a key role in improving performance.




\section{Conclusion}
This work proposes a masked diffusion transformer
to enhance the contextual representation
and improve the relation learning among image semantics for DPMs.
We introduce an effective mask latent
modeling scheme into DPMs and also accordingly designs an asymmetric
masking diffusion transformer. 
Experiments show that our masked diffusion transformer enjoys higher performance on image synthesis and largely improves the learning progress during training,
achieving the new SoTA for image synthesis on the ImageNet dataset.



{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage

\appendix

\newcommand{\addvism}[1]{\includegraphics[width=0.14\linewidth]{figures/mask_vis/#1.jpg}}
\renewcommand{\rotT}[1]{\rotatebox{90}{\footnotesize \qquad #1}}
\begin{figure*}[t]
  \centering
  \setlength{\tabcolsep}{0.1mm}
  \renewcommand{\arraystretch}{0.3}
  \begin{tabular}{ccccccccc}
    \rotT{Masked} & \addvism{sample208m0.3ori} & \addvism{sample208m0.3_mask} & \addvism{sample208m0.4_mask} & \addvism{sample208m0.5_mask}  & \addvism{sample208m0.6_mask}  & \addvism{sample208m0.7_mask} &  \addvism{sample208m0.8_mask}  \\
    \rotT{Inpainted} & \addvism{sample208m0.3ori} & \addvism{sample208m0.3_fill} & \addvism{sample208m0.4_fill} & \addvism{sample208m0.5_fill}  & \addvism{sample208m0.6_fill}  & \addvism{sample208m0.7_fill} &  \addvism{sample208m0.8_fill}   \\
    \rotT{Masked} & \addvism{sample89m0.3ori} & \addvism{sample89m0.3_mask} & \addvism{sample89m0.4_mask} & \addvism{sample89m0.5_mask}  & \addvism{sample89m0.6_mask}  & \addvism{sample89m0.7_mask} &  \addvism{sample89m0.8_mask}  \\
    \rotT{Inpainted} & \addvism{sample89m0.3ori} & \addvism{sample89m0.3_fill} & \addvism{sample89m0.4_fill} & \addvism{sample89m0.5_fill}  & \addvism{sample89m0.6_fill}  & \addvism{sample89m0.7_fill} &  \addvism{sample89m0.8_fill}   \\
    \vspace{5pt}
    & Original & mask=30\% & mask=40\% & mask=50\% & mask=60\%  & mask=70\% & mask=80\% \\
  \end{tabular} \\
  \vspace{3pt}
  \caption{Inpainting under different masked ratios using MDT-XL/2.}
  \label{fig:inpainting}
\end{figure*}

\section{Model Details}

\myPara{Network configurations.}
We follow the network configurations described in DiT~\cite{peebles2022scalable} to set the total block number (\ie~), 
token number, and channel numbers for the 
masked diffusion transformer of MDT. 
The configurations of MDT models are given in~\tabref{tab:model_config}.
Following DiT, the MDT has models with different sizes, denoted by S/B/XL.



\myPara{Network parameters and costs.}
The network parameters and training costs for MDT under different model scales 
are listed in~\tabref{tab:model_config}. 
In comparison to DiT baselines, 
MDT introduces a negligible extra inference
parameters and costs.

\newcommand{\gr}[1]{\textcolor{gray}{#1}}
\begin{table}[h]
  \centering
  \small
  \setlength{\tabcolsep}{1.6mm} \begin{tabular}{lcccccc}
      \toprule
      Size	 & Layers & Dim. & Head Num. & Param. (M) & FLOSs (G) \\	\midrule
      \multicolumn{6}{l}{Network configurations of MDT models.} \\ \hline
      S  & 12  & 384 & 6 & 33.1  & 6.07  \\               
      B  & 12  & 768 & 12 & 130.8  & 23.02 \\    
      XL & 28  & 1152 & 16 & 675.8 & 118.69  \\    \midrule
      \multicolumn{6}{l}{\gr{Network configurations of DiT baselines.}} \\ \hline
      \gr{S}  & \gr{12}  & \gr{384} & \gr{6} & \gr{32.9}  & \gr{6.06}  \\               
      \gr{B}  & \gr{12}  & \gr{768} & \gr{12} & \gr{130.3}  & \gr{23.01} \\    
      \gr{XL} & \gr{28}  & \gr{1152} & \gr{16} & \gr{674.8} & \gr{118.64}  \\    
      \bottomrule
  \end{tabular}
  \vspace{2pt}
  \caption{Network configurations of MDT models.
  The configurations are following DiT networks~\cite{peebles2022scalable}.
  The layers consist of the layer numbers of encoder and decoder, and the decoder number  is set to 2 for all models.
  FLOSs are measured with the latent embedding size of 3232 and =2.
  The parameters and FLOSs are measured using the inference model.
  \label{tab:model_config}
  }
  \end{table}
\section{Comparison of VAE decoders}
To ensure fair comparisons with DiT~\cite{peebles2022scalable}, we use both the MSE and EMA versions
of pretrained VAE decoders\footnote{MSE and EMA versions of VAE models are downloaded in \url{https://huggingface.co/stabilityai/sd-vae-ft-mse}
and \url{https://huggingface.co/stabilityai/sd-vae-ft-ema}.} for image sampling.
\tabref{tab:emavsmse} shows that
the EMA version has slightly better performance than the MSE version.
Except for the results in Table 1 of the manuscript that uses the EMA VAE decoder,
we use the MSE VAE decoder by default.
\begin{table}[h]
  \centering
  \setlength{\tabcolsep}{1.7mm} \small
  \begin{tabular}{lcccccc}
      \toprule
      Method	& Decoder &  FID & sFID & IS & Prec. & Rec. \\	\midrule
      MDT & MSE  & 6.65  & 5.07 & 129.47 & 0.72 & 0.63 \\
      MDT & EMA  & 6.46  & 4.92 & 131.70 & 0.72 & 0.63 \\
      \midrule
      MDT-G & MSE  & 2.14  & 4.45 & 259.21 & 0.82 & 0.59 \\ 
      MDT-G & EMA  & 2.02  & 4.46 & 263.77 & 0.82 & 0.60 \\ 
      \bottomrule
  \end{tabular}
  \vspace{2pt}
  \caption{Comparison between the EMA and MSE version of VAE decoders.
  -G denotes the results with classifier-free guidance.}
  \label{tab:emavsmse}
  \end{table}

\section{Inpainting with MDT}
By default, MDT uses the mask latent modeling during training
and becomes the standard diffusion model during inference.
When the side-interpolater 
is kept during inference,
MDT naturally enables the image inpainting ability.
As shown in~\figref{fig:inpainting}, 
we utilize different mask ratios on the image and inpaint
the masked parts with MDT.
Although the MDT model is trained with the mask ratio of 30\%,
it can easily handle much larger masking ratios, such as 70\% mask ratio.
We attribute this ability to the combination of our proposed mask latent modeling
and the diffusion model.




\section{Improved Classifier-free Guidance}
The classifier-free guidance sampling~\cite{ho2022classifier} enables
the trade-off between sample quality and diversity.
It achieves this by combining the class-conditional and unconditional
estimation:

where  is the class-conditional estimation,
 is the unconditional estimation, 
and  is the guidance scale.
Generally, a larger  results in high sample quality by decreasing the diversity.
MUSE~\cite{chang2023muse} changes the fixed guidance scale with
a linear increasing schedule during sampling,
which makes the model samples with more diversity at early steps while samples with higher
fidelity at late steps.
Inspired by this, 
we present a power-cosine schedule for the guidance scale during the sampling procedure:

where  is the time step during sampling, 
 is the maximum sampling step,
  is the maximum guidance scale,
and  is a factor that controls the increasing speed of the guidance scale.
As revealed in~\figref{fig:powercos},
the power-cosine schedule enables a low guidance scale at early steps
while quickly increasing the guidance scale at late steps.
By increasing ,
the guidance scale has a
slow increase at early steps and a fast increase at late steps.
The improved classifier-free guidance sampling equipped with 
the power-cosine guidance scale schedule
enables the model samples with high diversity at early steps
and high quality at late steps.
In this work,  is set to 4, and the corresponding  is set to 3.8
to ensure the model generates images with high fidelity at late steps.



\begin{figure}[t]
  \footnotesize
  \centering
  \begin{overpic}[width=0.97\linewidth]{figures/powercos.pdf}
    \put(-5,43){\large \rotatebox{0}{}}
    \put(50,-3){\large }
  \end{overpic}\vspace{10pt}
  \caption{The power-cosine scaling schedule for guidance scale in classifier-free guidance with difference .
  A larger  results in a slower increase of  at early steps and a faster increase at late steps.
	}\label{fig:powercos}
\end{figure}

\section{Visualization}
We provide more visualized examples of MDT-XL/2 generated images in~\tabref{fig:samplemore}.
In~\tabref{fig:inc}, we show more visualized examples of MDT-S/2 along with training steps.

\newcommand{\addvisinc}[1]{\hspace{-15pt}\includegraphics[width=0.99\linewidth]{figures/incvis/#1.jpg}}
\begin{figure}[b]
  \footnotesize
  \centering
  \setlength{\tabcolsep}{5.5mm}
  \renewcommand{\arraystretch}{0.5}
  \begin{tabular}{ccccc}
    \multicolumn{5}{l}{\addvisinc{sample_inc_cls699} }\\
    \multicolumn{5}{l}{\addvisinc{sample_inc_cls986} }\\
    \multicolumn{5}{l}{\addvisinc{sample_inc_cls109} }\\
    \multicolumn{5}{l}{\addvisinc{sample_inc_cls548} }\\
    \multicolumn{5}{l}{\addvisinc{sample_inc_cls934} }\\
    50k & 100k & 200k & 300k & 3000k \\
  \end{tabular} \\
  \vspace{3pt}
  \caption{Visualized example of MDT-S/2
  along with training steps.}
  \label{fig:inc}
\end{figure}

\newcommand{\addvisall}[1]{\includegraphics[width=0.2\linewidth]{figures/vismore/#1.jpg}}
\begin{figure*}[t]
  \footnotesize
  \centering
  \setlength{\tabcolsep}{0.1mm}
  \renewcommand{\arraystretch}{0.1}
  \begin{tabular}{ccccccccc}
    \addvisall{sample_cls3cfg4.0each4} & \addvisall{sample_cls3cfg4.0each7} & \addvisall{sample_cls3cfg4.0each14} & \addvisall{sample_cls3cfg6.0each2} & \addvisall{sample_cls3cfg6.0each14} \\
    \addvisall{sample_cls108cfg4.0each5} & \addvisall{sample_cls108cfg4.0each7} & \addvisall{sample_cls108cfg5.0each1} & \addvisall{sample_cls108cfg5.0each9} & \addvisall{sample_cls108cfg6.0each11} \\

    \addvisall{sample_cls23cfg6.0each0} & \addvisall{sample_cls23cfg6.0each3} & \addvisall{sample_cls23cfg6.0each4} & \addvisall{sample_cls23cfg6.0each10} & \addvisall{sample_cls23cfg6.0each11} \\

    \addvisall{sample_cls19cfg4.0each13} & \addvisall{sample_cls19cfg5.0each0} & \addvisall{sample_cls19cfg5.0each3} & \addvisall{sample_cls19cfg6.0each1} & \addvisall{sample_cls19cfg6.0each14} \\

    \addvisall{sample_cls282cfg4.0each3} & \addvisall{sample_cls282cfg4.0each5} & \addvisall{sample_cls282cfg5.0each3} & \addvisall{sample_cls282cfg5.0each13} & \addvisall{sample_cls282cfg6.0each5} \\
    \addvisall{sample_cls278cfg5.0each10} & \addvisall{sample_cls278cfg5.0each12} & \addvisall{sample_cls278cfg6.0each8} & \addvisall{sample_cls279cfg4.0each1} & \addvisall{sample_cls279cfg5.0each14} \\

  \end{tabular} \\
  \vspace{3pt}
  \caption{Visualization of images generated by the MDT-XL/2.}
  \label{fig:samplemore}
\end{figure*}
	


\end{document}
