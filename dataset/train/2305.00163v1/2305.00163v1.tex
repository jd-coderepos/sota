\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{eso-pic}
\usepackage{subcaption}
\usepackage{pifont}
\usepackage{booktabs}
\usepackage{multirow, makecell}
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}
\newcommand{\rulesep}{\unskip\ \vrule \ }

\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\iccvfinalcopy 

\def\iccvPaperID{2467} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\newcommand{\tp}[1]{{ {#1}}} 
\newcommand{\tbp}[1]{{\color{purple} {\textbf{#1}}}} 
\newcommand{\tb}[1]{{{#1}}} 

\newcommand{\tpt}[1]{{\color{purple} {#1}}} 
\newcommand{\tvt}[1]{{\color{violet} {#1}}} 
\newcommand{\tct}[1]{{\color{cyan} {#1}}} 
\newcommand{\tbt}[1]{{\color{blue} {#1}}} 
\newcommand{\trt}[1]{{\color{red} {#1}}} 

\begin{document}

\title{An Implicit Alignment for Video Super-Resolution}

\author{
Kai Xu\textsuperscript{1}\qquad Ziwei Yu\textsuperscript{1} \qquad  Xin Wang\textsuperscript{2}\qquad Michael Bi Mi\textsuperscript{2}\qquad Angela Yao\textsuperscript{1}\\
\textsuperscript{1}National University of Singapore, \textsuperscript{2}Huawei International Pte Ltd, Singapore\\
{\small\texttt{\{kxu,ayao\}@comp.nus.edu.sg, yuziwei@u.nus.edu}}\\
{\small\texttt{wangxin237@huawei.com, michaelbimi@yahoo.com}}
}

\maketitle


\begin{abstract}
    Video super-resolution commonly uses a frame-wise alignment to support the propagation of information over time.  The role of alignment is well-studied for low-level enhancement in video, but existing works have overlooked one critical step -- re-sampling.
    Most works, regardless of how they compensate for motion between frames, be it flow-based warping or deformable convolution/attention, use the default choice of bilinear interpolation for re-sampling.  However, bilinear interpolation acts effectively as a low-pass filter and thus hinders the aim of recovering high-frequency content for super-resolution.  
   
   This paper studies the impact of re-sampling on alignment for video super-resolution.  Extensive experiments reveal that for alignment to be effective, the re-sampling should preserve the original sharpness of the features and prevent distortions. From these observations, we propose an implicit alignment method that re-samples through a window-based cross-attention with sampling positions encoded by sinusoidal positional encoding.  The re-sampling is implicitly computed by learned network weights. Experiments show that the proposed implicit alignment enhances the performance of state-of-the-art frameworks with minimal impact on both synthetic and real-world datasets. \footnote{\url{https://github.com/kai422/IART}}    
\end{abstract}


\section{Introduction}\label{sec:intro}
\begin{figure}
\begin{subfigure}{\linewidth}
\includegraphics[width=\linewidth]
{figs/warp_bi_nn.pdf}
\caption{Comparison with the original image and the optical flow warped image from the previous frame with bilinear interpolation and nearest interpolation. Bilinear interpolation smooths out the image pattern \tpt{(red arrow)} while nearest interpolation produces sharper results but introduces distortions \tbt{(blue arrow)}.}\label{fig:warp_bi_nn}
\end{subfigure}
\begin{subfigure}{\linewidth}
\includegraphics[width=\linewidth]{figs/intro_sintel_visualization.pdf}
\caption{Video super-resolution results with different re-sampling methods on 
the Sintel dataset~\cite{butler2012naturalistic_sintel}.  We perform first-order backward image alignments from $t\!+\!1$ to $t$ using ground-truth optical flow. }

\label{fig:intro_sintel_image_alignment}
\vspace{-.6cm}
\end{subfigure}
\end{figure}

Video super-resolution (VSR) recovers a spatially high-resolution sequence of frames from a low-resolution sequence.  While image super-resolution can be applied naively to each frame individually, the temporal correlations across the frames give an extra source of information to improve the super-resolved output.  As such, the main difference in designing architectures for video versus image super-resolution lies in the exploration of temporal dependencies. Previous works \cite{shi2022rethinking, chan2021basicvsr, wang2019edvr, haris2019recurrent} have shown that spatial alignment is an essential pre-processing step for effective information exchange across the frames.  Given the frame-to-frame motions, either of the camera or the objects in the scene, alignment provides indications for extra sub-pixel information.  


Frame-wise alignment has two steps: motion estimation and motion compensation. The standard alignment~\cite{lin2022unsupervised_S2SVR, chan2021basicvsr, xue2019video_tof} estimates motion via optical flow and compensates the motion with a backward warp.  In the warping, the default option is to use bilinear interpolation for re-sampling for any non-integer flow values. A separate line of work~\cite{liang2022recurrent_rvrt, chan2022basicvsrpp, wang2019edvr} merges motion estimation and compensation into a single step, in the form of deformable convolutions and deformable attention. These works use multiple offsets, which can be viewed as a multi-reference ensemble (versus the single-reference setting of optical flow). Nevertheless, bilinear interpolation is also used when re-sampling the pixel/feature values for convolution or attention operations.

As other studies have shown, high-frequency components are crucial for the learning of super-resolution tasks \cite{ledig2017photo}.  Yet bilinear  
interpolation smooths out pixels or feature values, directly contradicting the aim of recovering high-frequency information for super-resolution. In fact,~\cite{shi2022rethinking} recently observed that even a crude nearest-neighbour interpolation can perform better for patch alignment on a VSR transformer. 

This paper takes a deep dive into re-sampling in alignment for video super-resolution.  One key challenge to study re-sampling is the confounding effect from the estimated motions. The accuracy of motion estimation affects the reconstruction performance~\cite{chan2021basicvsr}, and different re-sampling methods have different levels of robustness to this accuracy. Therefore, it is challenging to evaluate and compare various re-sampling methods under real-world optical flow conditions.
To better understand the effect of re-sampling on video super-resolution, we design a series of experiments on a synthetic dataset with ground-truth optical flow. This allows us to isolate and remove the influence of motion estimation for alignment.



Fig.~\ref{fig:warp_bi_nn}
shows the warping results on image alignment using bilinear and nearest interpolation, where bilinear interpolation preserves the original image structure but smooths out the pattern; nearest interpolation produces sharper results but introduces distortions.
\cref{fig:intro_sintel_image_alignment} plots video super-resolution PSNR and SSIM.  On a first-order image alignment with ground-truth optical flow for synthetic dataset Sintel ~\cite{butler2012naturalistic_sintel}, the nearest interpolation performs better than bilinear interpolation and bicubic interpolation.

Based on these observations, we aim to design a balanced alignment.  Ideally, we target  interpolation that does not have any smoothness prior when reconstructing re-sampled pixels, which is the case for bi-linear interpolation.  At the same time, we wish to  avoid distortions by providing accurate sub-pixel information aggregation.

Inspired by recent image neural implicit representations \cite{chen2021learning_liif, xu2021ultrasr}, we propose a new alignment module with an \emph{implicit} re-sampling.  
The re-sampling is achieved through a local cross-attention module, applied to a feature window based on the optical flow offset.  Rather than re-sampling on the target frame explicitly, \ie, solving for the sub-pixel feature value via interpolation, the values are aggregated via an affinity matrix.  This matrix is computed by similarity comparison with feature encodings and positional encodings.  The attention-based aggregation imposes no smoothness constraints on the re-sampling process.  Furthermore, we encode 
the sub-pixel coordinate information from the flow estimate with a sinusoidal positional encoding.   
As the result, the implicit alignment largely surpasses optical flow with the nearest interpolation and exceeds the state-of-the-art alignment method for the VSR transformer model on both synthetic and real-world datasets.

The work closest related to ours in spirit is~\cite{shi2022rethinking}, where they point out that inaccurate optical flow and the bilinear re-sampling methods could corrupt the feature patterns and reconstruction results. As a result, they propose patch alignment to handle the inaccurate optical flow. Our work further investigates this effect by experimenting with re-sampling methods under different optical flow scenarios for both image and feature alignments. Moreover, our proposed implicit alignment is as robust as patch alignment when the offset estimation is inaccurate, since it refers to a window of pixel features. Our implicit alignment also provides more precise sub-pixel information when the offset estimation is accurate, as it embeds the exact offsets into the positional encoding.  In contrast, patch alignment averages the offsets indiscriminately.





We summarize our contribution as follows:
\begin{itemize}
    \item To better understand alignment for VSR, we design a series of experiments to disentangle the effects of motion estimation from re-sampling. Our findings reveal that an effective re-sampling method should impose no smoothness prior and introduce no distortions.
    \item We propose an implicit alignment method where estimated motion is encoded by a sinusoidal positional encoding and re-sampling is implicitly performed by window-based attention.
    \item The proposed implicit alignment outperforms current the state-of-the-art alignment methods on both a synthetic dataset and a real-world dataset.
\end{itemize}


\begin{figure*}[t]
\centering

\includegraphics[width=\textwidth]{figs/method.pdf}
\vspace{-.3cm}
\caption{A comparison diagram between bilinear interpolation and our implicit alignment. Bilinear interpolation fixes aggregation weight $W_{bi}$. Implicit alignment learns affinity through the cross-attention module to calculate the final result. \tpt{Red grids} denote the source frame, \tvt{purple grids} denote the target frame, and \tct{blue grids} denote the aligned frame.}
\vspace{-.3cm}
\end{figure*}

\section{Related Work}
\label{sec:related_work}

\textbf{Video Super-Resolution}:
Video super-resolution aims to recover a spatially high-resolution sequence from low-resolution frames. Its difference with image super-resolution lies in the use of temporal information. Early methods \cite{huang2017video, fuoli2019efficient, isobe2020video, isobe2020revisiting} did not consider spatial alignment from frame to frame.  Initially, VSR methods applied optical flow-based warping to the images of temporal neighbours~\cite{kim2018spatio, xue2019video_tof}.  However, inaccurate flows lead to degradation for CNN backbone.
To mitigate the impact of inaccurate optical flow, BasicVSR~\cite{chan2021basicvsr} aligns feature maps instead of the input image, while others use the flow to guide deformable convolutions~\cite{wang2019edvr, chan2022basicvsrpp, liang2022vrt_vrt} and attention schemes~\cite{liang2022recurrent_rvrt}. 


To increase the robustness toward inaccurate optical flow, ~\cite{shi2022rethinking} propose patch alignment which aligns blocks by averaging the motions within predefined grids. The design of our implicit alignment differs from them as we use a flexible window, meaning that each pixel has a different reference window computed by its optical flow. 


\textbf{Image Re-sampling}:
 Aligning the reference frame to the destination frame requires re-sampling sub-pixel values on a discrete reference image.  
 Nearest-neighbour interpolation requires no continuity.  Linear interpolation guarantees an L0-smoothness on the resulting image intensity surface \cite{dodgson1992image}; higher orders of polynomials will be required for higher smoothness. It is also well-established that bilinear interpolation will result in blurry outputs \cite{wolberg1990digital}. 

 The recent work \cite{chen2021learning_liif} studied arbitrary-scale image super-resolution.  They proposed to learn a continuous representation from the discrete image with an MLP.  The neighbouring pixels and the 2D coordinates in the continuous image domain are fed into the network to estimate the pixel value. \cite{xu2021ultrasr} further improves the representation by adding positional encoding to represent the 2D coordinates and enhance the learning of the high-frequency components.

Our alignment module is also an implicit image representation.  The key difference is that our focus is on modelling the temporal aggregation between different frames through cross-attention.
 
\section{Preliminaries}\label{sec:preliminary}

In video super-resolution, inter-frame propagation is a key component to enhance the temporal information aggregation. To facilitate effective propagation, one should first spatially align the source frame with the target frame. the aligned frame $\hat{I}$ is then concatenated with target frame $I$ and together they are fed into the subsequent network block.



Alignment can be performed both on input images and intermediate features. In the following context, we unify the image and feature alignment here as frame-wise alignment and discuss the various implementations.


Frame-wise alignment estimates the spatially matched features in a target frame $I\in \mathbb{R}^{H \times W \times C}$ from a source frame $I'\in \mathbb{R}^{H \times W \times C}$, where $C$, $H$ and $W$ denote the channels, height and width of the feature vector. For image alignment, $C$ is the image channel. 
Frame-wise alignment has two steps: motion estimation and motion compensation. 
Motion estimation predicts the motion offsets $F\in \mathbb{R}^{H \times W \times 2}$, where $F(x,y)=(dx,dy)$ represents the motion offset of each pixel from frame $I$ to frame $I'$. Given predicted motion offsets, the aligned frame $\hat{I}$ is computed by motion compensation, which is defined by:
\begin{equation}\label{eq:alignment}
    \hat{I} = \mathcal{W}(I', F)
\end{equation}
\noindent\textbf{Optical Flow Warping}:
The most common alignment method is optical flow warping, which is used in \cite{chan2021basicvsr,xue2019video_tof,lin2022unsupervised_S2SVR}. Given an optical flow $F\in \mathbb{R}^{H \times W \times 2}$, where $F(x,y)=(dx,dy)$.  The backward warping process is often chosen to propagate pixel/feature values from source frame $I'$ to target frame $I$. Specifically, the following calculation is iterated on all spatial locations:
\begin{multline}\label{eq:of_warping}
\hat{I}(x,y)=\mathcal{W}_{of}(I',dx,dy)\\
=\sum_{(i,j)}G\big((i,j), (x+dx,y+dy)\big)I'(i,j)
\end{multline}
The need for re-sampling arises as $(dx,dy)$ is continuous but $I(\cdot,\cdot)$ is defined in discrete space. Common re-sampling methods are bilinear, bicubic and nearest interpolation, where the interpolated pixels are obtained by aggregating neighbouring reference pixels. $G((i,j), \cdot)$ defines the interpolation kernel, which stores the weights to aggregate pixel $I'(i,j)$. For example, a bilinear interpolation kernel is
\begin{multline} \label{eq:bilinear_interpolation_kernel}
    G\big((i,j),(x+dx,y+dy)\big)=g(i,x+dx)\cdot g(j,y+dy), \\
    \text{where }g(a,b)=max(0,1-|a-b|).
\end{multline}
There is no prior knowledge on how the original discrete image is sampled from the real-world. Assumptions on smoothness are often imposed to construct the interpolation kernel. Specifically, nearest interpolation has no smoothness requirements.  Bilinear interpolation enforces L0 smoothness and bicubic interpolation enforces L1 smoothness. 

Existing works often adopt bilinear interpolation for re-sampling.  Yet by enforcing L0 smoothness, the interpolation is effectively applying a low-pass filter to the source frame's features. While nearest-neighbour interpolation does not have such a low-pass effect, 
it introduces distortions by shifting the sampled position to the nearest pixel grid value.














\section{Methodology}

\subsection{Implicit Alignment}

The process of bilinear interpolation or bicubic interpolation can be viewed as imposing a low-pass filter on the original data\cite{youssef1998analysis}. We aim to propose an implicit alignment function where the re-sampled value is not obtained by 
any explicit interpolation function, \eg, a bilinear or bicubic interpolation. The implicit alignment takes the offsets and original values, and outputs a warped value through a small neural network block.

For each pixel $I(x,y)$ in the target frame $t$, and $I'(x,y)$ in the source frame$t'$, given the estimated motion $f$ from $t$ to $t'$, where $f(x,y)=(dx,dy)$ is the offset of the pixel from $t$ to $t'$ frame. Then 

\begin{multline}\label{eq:iw}
\hat{I}(x,y)=\mathcal{W}_{ia}(I',x+dx,y+dy)=A(Q, K') V',
\end{multline}

where $A$ is the affinity matrix which is computed by:

\begin{equation}
    A(Q, K')=\text{softmax}\left(\frac{QK'^T}{\sqrt{C_p}}\right).
\end{equation}

$Q \in \mathbb{R}^{1 \times C_q}$ is the encoded query for $I(x,y)$; $K' \in \mathbb{R}^{h \times w \times C_q}$ and $V' \in \mathbb{R}^{h \times w \times C_v}$ are the encoded key and value for source frame $I'$. $h$ and $w$ are the height and width of the attention window.  $C_q$ is the channel number for the query and key encoding and $C_v$ is the channel number for the value encoding. The $\text{softmax}$ operation is applied along all the pixels in the windows $h \times w$. The computation of query encoding, key encoding and value encodings are defined as follows.

\vspace{0.5em}

\noindent\textbf{Query Encoding:}
The query $Q$ is computed by a query encoder $P_Q$ on the original value $I(x,y)$ and the positional encoding of sub-pixel offset of motion estimation $(dx, dy)$.  (see \cref{sec:method_pe}):  
\begin{equation}
    Q = P_Q\left(I(x,y) + PE_Q\right).
\end{equation}

\vspace{0.5em}
\noindent\textbf{Key and Value Encoding:}
A global cross-attention scheme is to choose $K' \in \mathbb{R}^{H \times W \times C_p}$ to encode all pixels in the source frame $I'$, where $H$ and $W$ are the height and width of the feature. However, this choice has quadratic computation complexity with respect to the input image resolution $H \times W$.

We apply a local attention scheme and encode only the pixels in a window around the query position $(x+dx, y+dy)$. Let $w$ be the chosen window size. The selected pixels $M'\in \mathbb{R}^{h \times w \times C}$ are obtained by iterating over $i \in \{0,1...,h\}$ and $j \in \{0,1...,w\}$:
\begin{equation}
    M'(i, j)=I'(\lfloor x+dx \rfloor - \lfloor \frac{h}{2} \rfloor + i,\lfloor y+dy \rfloor -\lfloor \frac{w}{2} \rfloor + j).
\end{equation}

The key $K'$ and value $V'$ are computed by encoding $M'$ and its positional encoding into key space with $P_K$ and $P_V$:
\begin{equation}
     K'= P_K(M + PE_M) 
\end{equation}
\begin{equation}
     V'= P_V(M + PE_M),   
\end{equation}
where $P_k$ and $P_v$ are the key and value encoders respectively. We use a single fully connected layer for $P_q$, $P_k$ and $P_v$.


\subsection{Positional Encoding}\label{sec:method_pe}
Recent works \cite{xu2021ultrasr, mildenhall2021nerf} have shown that positional encoding can enhance the network in the high-frequency domain by expanding 2D coordinates into a high-dimensional periodic positional encoding.
In order to represent the sub-pixel offset information, we use sinusoidal functions to encode the offsets into the positional encoding $PE_Q \in \mathbb{R}^{1 \times C}$ and $PE_M\in \mathbb{R}^{N \times C}$.

The positional encoding for the query takes the sub-pixel offset into the $C$ dimensional sinusoidal representation:

\begin{equation}
    PE_Q = f(\Delta x, \Delta y),
\end{equation}

\noindent where

\begin{equation}
    \Delta x = dx - \lfloor dx \rfloor, \Delta y = dy - \lfloor dy \rfloor.
\end{equation}

\noindent The positional encoding takes the grid index inside the window to the $C$ dimensional sinusoidal representation:

\begin{equation}
    PE_M(i+wj) = f(i-\lfloor \frac{w}{2} \rfloor, j-\lfloor \frac{w}{2} \rfloor).
\end{equation}

\noindent Above, $f(x,y):\mathbb{R}^2 \rightarrow \mathbb{R}^C$ is the 2D positional encoding function which projects 2D continuous coordinates into high-dimensional space as follows:

\begin{equation}
    \!\!\!\!f(x,y)^{(c)}:=
     \begin{cases} 
      \sin(\omega_k \cdot x) & \text{if } c = 2k   \text{ \hphantom{$ +1 \,$} and } c \leq C/2,\\
      \cos(\omega_k \cdot x) & \text{if } c = 2k+1 \text{ and } c\leq C/2,\\
      \sin(\omega_k \cdot y) & \text{if } c = 2k   \text{ \hphantom{$ +1 \,$} and } c>C/2,\\
      \cos(\omega_k \cdot y) & \text{if } c = 2k+1 \text{ and } c>C/2,\\
   \end{cases}
\end{equation}

\noindent where $c$ denotes the encoding index.
We first normalize $(x,y)$ into $[-\pi/2, \pi]$. The angular speed is defined as:

\begin{equation}
    \omega_k = \frac{1}{T^{4k/C}}.
\end{equation}

In natural language processing, positional encodings were first proposed to represent the discrete text index.  Suggested parameter values for the temperature value were in the range of $T=10000$.  This forms a geometric progression from $2\pi$ to $10000\pi$ on the wavelength \cite{vaswani2017attention}. The objective of the positional encoding here is to represent the continuous coordinates and preserve the high-frequency content.  As such, set $T=0.01$, forming a geometric progression from $2\pi$ to $0.01\pi$ on the wavelength, can represent more precise sub-pixel position information. 


Conceptually, the computation of $PE_Q$ and $PE_M$ take $(\lfloor x+dx \rfloor, \lfloor y+dy \rfloor)$ as the anchor and compute the relative offsets. The $PE_Q$ represents the relative sub-pixel offsets for $I'(x+dx, y+dy)$; we add this positional encoding to $I(x,y)$ under the assumption that the query point $I'(x+dx, y+dy)$ has a similar value with $I(x,y)$, given the estimated motion.




\subsection{Network Structure}
We choose a single-layer fully connected layer as the encoder for $P_q$, $P_k$ and $P_v$. The implicit alignment module can directly replace the optical flow warping operation in the existing backbone. Specifically, we conducted experiments on both CNN-based backbone following BasicVSR~\cite{chan2021basicvsr} and Transformer-based backbone following PSRT-recurrent \cite{shi2022rethinking}. Alignment is used for either first-order or second-order bidirectional propagation. All implicit alignment modules share the same parameters, which means features of different depths are aligned through one implicit alignment module. The width and height of the attention window are set to 2 for all the experiments unless explicitly stated.


\begin{table*}[]
\small
\centering
\begin{tabular}{l|c|c|c c|c c|c c}

\toprule
\multirow{2}{*}{Alignment} & Params &\multirow{2}{*}{Re-samp.} &\multicolumn{2}{c|}{GT Flow (EPE=0)} & \multicolumn{2}{c|}{RAFT Flow (EPE=1.467)}      & \multicolumn{2}{c}{SpyNet Flow (EPE=3.710)}                                \\
 & (M) &  &Img. & Feat. &Img. & Feat. &Img. & Feat.\\
\midrule
-                               &\multirow{2}{*}{1.35}  & -                     & 31.24/0.8867  & 31.24/0.8867  & 31.24/0.8867  & 31.24/0.8867  & 31.24/0.8867  & 31.24/0.8867 \\
 Duplicate                      && -                                            & 31.50/0.8902  & 31.42/0.8883  & 31.50/0.8902  & 31.42/0.8883  & 31.50/0.8902  & 31.42/0.8883 \\
 \midrule
\multirow{2}{*}{OF Warp}        &\multirow{2}{*}{1.35}& bilinear                & 31.60/0.8934  & 31.92/0.9000  & 31.60/0.8934  & 31.87/0.8991  & 31.60/0.8934  & 31.85/0.8987  \\
                                
                                && nearest                                      & 31.85/0.8975  & 31.84/0.8977  & 31.81/0.8971  & 31.87/0.8982  & 31.75/0.8958  & 31.78/0.8967  \\
                                
\midrule
FGDC \cite{wang2019edvr}        &1.60& bilinear                                  &       -       & 32.08/0.9026  &       -       & 31.99/0.9009  &       -       & 31.98/0.9005  \\
FGDA \cite{liang2022recurrent_rvrt}  &1.56& bilinear                             &       -       & 32.03/0.9017  &       -       & 31.91/0.8998  &       -       & 31.94/0.8998  \\
PA \cite{shi2022rethinking}     &1.35& nearest                                   &       -       & 31.81/0.8971  &       -       & 31.85/0.8976  &       -       & 31.82/0.8969  \\
IA (ours)                      &1.36&  -                                            &       -       & \textbf{32.14/0.9034}  &       -       & \textbf{32.03/0.9018}  &       -       & \textbf{32.05/0.9014}  \\
\bottomrule
\end{tabular}
\vspace{-0.1cm}
\caption{PSNR/SSIM Comparison of VSR transformers on Sintel datasets with optical flow of different accuracies for alignment. The best score is marked in \textbf{bold}.}\label{tab:sintel_vsr}
\vspace{-1em}
\end{table*}


\section{VSR on Synthetic Dataset}
\subsection{Sintel Dataset} 
As mentioned in \cite{chan2021basicvsr}, image alignment suffers from inaccurate optical flow. In order to focus on the study of interpolation effectiveness and exclude the impact of optical flow accuracy, we propose using a synthetic dataset where the ground-truth optical flow is provided. In order to study the robustness of different alignment methods under different optical flow accuracy, we generated two different optical flows in addition to ground truth optical flow, namely RAFT \cite{teed2020raft} and SPyNet \cite{ranjan2017optical_spynet}.The former is a precise but slow method, the latter is an efficient method.

The Sintel clean data track is composed by 23 training videos, which we split into 20 training videos and 3 testing videos \footnote{\texttt{ambush$_\text{5}$}, \texttt{market$_\text{6}$}, \texttt{mountain$_\text{1}$}} and report the testing results. We generate the training data by bicubic down-sampling the high-resolution image.
As only first-order forward backward optical flow ($t \rightarrow t+1$) ground-truth is provided, we perform image alignment from $t+1 \rightarrow t$ and concatenate with original frame before feeding into residual blocks and the reconstruction network. 

\subsection{Experimental Settings}
We chose VSR transformer as the backbone, where the network consists of a convolution module and two Multi-Freame Self-Attention Blocks \cite{shi2022rethinking} followed by an up-sampling module. For image alignment, propagation is done on image inputs. For feature alignment, propagation is done after the first SwinIR module.  All propagated features are concatenated with the current frame’s features and then input to the next network layer. All alignment methods tested have consistent network structure and training parameters, where use Adam optimizer to learn 100k iterations at a learning rate of 2e-4, with a batch size of 8.

We tested three different optical flow settings under the following different alignment methods : 1) without propagation, which is degraded to image super-resolution. 2) Duplicate: propagation without alignment. 3) optical flow warping with bilinear and nearest interpolations. 4) FGDC \cite{wang2019edvr}: flow-guided deformable convolution. 5) FGDA \cite{liang2022recurrent_rvrt}: glow-guided deformable attention. 6) PA \cite{shi2022rethinking}: patch alignment. 7) IA: our implicit alignment.

\subsection{Results Analysis} 
\cref{tab:sintel_vsr} shows PSNR/SSIM comparisons on different alignment methods. Unlike \cite{chan2021basicvsr, shi2022rethinking}, we do not find any degradation on image alignment for the synthetic dataset. Even for propagation without alignment, the accuracy also increases. This is likely due to the small displacement between frames in the dataset. Using optical flow warping for alignment further improves  the results, but different re-sampling results perform differently and we discuss these results in the following context.


\noindent\textbf{The effect of re-sampling}:
An interesting finding is that nearest-neighbour interpolation for image alignment results in better performance than bilinear interpolation, but the opposite is true for feature alignment. We think there are two reasons for this phenomenon: 1) Image alignment can provide the most discriminative comparison on the re-sampling methods because the frequency components are not yet smoothed out by convolution layers. In this scenario, the preservation of high-frequency components by nearest interpolation outweighs the distortions introduced by it. 2) Feature alignment shows fewer differences towards high-frequency components due to the bias towards
low-frequency signals of neural networks \cite{rahaman2019spectral, chen2021ssd}. In this case,  the distortions of nearest interpolation contributes to the performance difference.

Based on this observation, we conclude that an effective re-sampling method should combine the advantages of nearest interpolation and bilinear interpolation. Specifically, one should not impose any smoothness constraint on the interpolation to avoid a smoothing effect on the high-frequency components, and avoid distortions caused by coordinates quantization. 


\noindent\textbf{Comparison with SOTA alignment methods}: We compare implicit alignment with FGDC, FGDA, and PA for feature alignment. Patch Alignment (PA) is more robust than others against inaccurate optical flow, showing no accuracy drops when optical flow degrades. However, it sacrifices the overall accuracy because patch alignment cannot utilize sub-pixel or even sub-grid motions. FGDA and FGDC are essentially ensemble modules for multiple optical flow warping, which provide extra improvements but still suffer from the disadvantage of bilinear re-sampling. Implicit alignment outperforms all three SOTA alignment methods because it learns the re-sampling weights implicitly, while the others adapt bilinear and nearest interpolation which introduces smoothing prior or distortions.


\begin{table*}[t]
  \small
  \centering

  \label{tab:BI}
  \vspace{1mm}
  \resizebox{0.80\textwidth}{!}{
  \begin{tabular}{l|c|c|cc|cc|cc}
    \toprule
    \multirow{2}{*}{Method} &  Frames & Params & \multicolumn{2}{c|}{REDS4} & \multicolumn{2}{c|}{Vimeo-90K-T} & \multicolumn{2}{c}{Vid4}\\
    & REDS/Vimeo & (M) & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM \\
    \midrule
    TOFlow~\cite{xue2019video_tof}    &       5/7 & - & 27.98 & 0.7990  & 33.08 & 0.9054  & 25.89 & 0.7651\\
    EDVR~\cite{wang2019edvr}      &   5/7 & 20.6 & 31.09 & 0.8800  & 37.61 & 0.9489 & 27.35 & 0.8264 \\
    MuCAN~\cite{li2020mucan}      &       5/7  & - & 30.88 & 0.8750  & 37.32 & 0.9465 &  - & - \\

    BasicVSR~\cite{chan2021basicvsr}& 15/14 & 6.3 & 31.42 & 0.8909 & 37.18 & 0.9450 & 27.24 & 0.8251\\
    IA-CNN (ours) & 15/14 & 8.5 & 31.68 & 0.8959 & 37.34 & 0.9463 & 27.42 & 0.8315\\
    \midrule
    VSR-T~\cite{cao2021video}     &     5/7 & 32.6 & 31.19 & 0.8815 & 37.71 & 0.9494  & 27.36 & 0.8258  \\
    VRT~\cite{liang2022vrt_vrt} &  6/- & 30.7 & 31.60 & 0.8888 & - & - & - & - \\
    PSRT-sliding~\cite{shi2022rethinking} &  5/- & 14.8 & 31.32 & 0.8834 & - & - & - & -\\
    PSRT-recurrent~\cite{shi2022rethinking} &  6/- & 10.8 & 31.88 & 0.8964 & - & - & - & -\\
    IA-RT (ours) &  6/- & 13.4 & \textbf{32.15} & \textbf{0.9010} & - & - & - & -\\
    
    \midrule
    BasicVSR++~\cite{chan2022basicvsrpp}&  30/14 & 7.3  & 32.39 & 0.9069 &  37.79 & 0.9500 & 27.79 & 0.8400\\
    VRT  &  16/7 & 35.6 & 32.19 & 0.9006 & {38.20} & {0.9530} & 27.93 & 0.8425\\
    RVRT \cite{liang2022recurrent_rvrt} & 30/14 &  10.8 & {32.75} & {0.9113} & 38.15 & 0.9527 & {27.99} & {0.8462} \\
    PSRT-recurrent~\cite{shi2022rethinking}  &  16/14 & 13.4 & {32.72} & {0.9106} & {38.27} & {0.9536} & {28.07}& {0.8485}\\
    IA-RT (ours) &  16/- & 13.4 & \textbf{32.90} & \textbf{0.9138} & - & - & - & -\\
    \bottomrule
    

    
  \end{tabular}}
    \vspace{-0.5em}
    \caption{Quantitative comparison on the REDS4~\cite{nah2019ntire} dataset, Vid4~\cite{liu2013bayesian}, Vimeo-90K-T~\cite{xue2019video_tof} dataset for $4\times$ VSR task. The best score is marked in \textbf{bold}.}\label{tab:comparision_sota}
    \vspace{-0.5em}
\end{table*}

\begin{figure*}[h!]
\centering

\includegraphics[width=0.95\textwidth]{figs/qual_reds4.pdf}\label{fig:qual_reds4}

\vspace{-.3cm}
\caption{Qualitative comparison on REDS4 dataset. We highlight the detail regions with yellow boxes. Compared with BasicVSR, IA-CNN provides more details on the wall and more uniform patterns on the window.}
\vspace{-.5cm}
\end{figure*}


\begin{figure*}[t!]
\centering

\includegraphics[width=0.95\textwidth]{figs/qual_realvsr.pdf}
\caption{Qualitative comparison on VideoLQ dataset. Our proposed IA method recovers the building details and the brick textures, which ReadlBasicVSR does not recover. We highlight the detail regions with yellow boxes.}\label{fig:qual_realvsr}
\end{figure*}

\section{VSR on Real-World Dataset}
\vspace{-0.2em}
\subsection{Experimental Settings}
\vspace{-0.2em}
To show the effectiveness of implicit alignment on large-scale datasets, we incorporate it into state-of-the-art CNN-based (BasicVSR \cite{chan2021basicvsr}) and Transformer-based backbones (PSRT-recurrent \cite{shi2022rethinking}).
We follow the training configurations of BasicVSR: 300K training iterations on the REDS datasets for BI degradation, and 150K iterations on the Vimeo-90K datasets for BD and BI degradation. Results are reported on REDS4 for the REDS BI model in RGB. For the Vimeo-90K BI model, we report accuracy on Vimeo-90K-T and Vid4. For the Vimeo-90K BD model, we report accuracy on UDM10, Vimeo-90K-T and Vid4. For PSRT-recurrent, we follow the configurations from the original paper, where the model is trained with 6 input frames for 300k iterations.
\vspace{-0.2em}
\subsection{Comparison with SOTA}
\vspace{-0.2em}
\cref{tab:comparision_sota} shows a quantitative comparison with SOTA methods. For CNN-based models, Implicit alignment (IA-CNN) outperforms BasicVSR using the same network backbone. The increased parameters (2.2M) come from the computation to calculate the current features, which is required by its implicit alignment module (BasicVSR computes each frame’s features recurrently which already include information from other frames). This implicit alignment module increase only 0.01M parameters. For Transformer-based models, implicit alignment restoration transformer (IA-RT) achieves the highest accuracy for the 6 training input frames configuration. Note that implicit alignment module only increases parameters by 0.04M compared to the released model by \cite{shi2022rethinking}, which is 13.37; however, we follow the original paper and report 10.8 in the table for PSRT-recurrent.
\vspace{-0.2em}
\subsection{Ablation Studies}
\vspace{-0.2em}
We conduct ablation experiments using a smaller-scale model and training setup on REDS dataset. The supplementary material provides more details.

\noindent \textbf{Positional Encoding}:
\cref{tab:pe} shows the results of ablation studies on the components of the positional encoding. The positional encoding improves the PSNR by 0.28 compared to the naive window-based cross-attention. Adding positional encodings only to key and value slightly enhances the performance, which can be interpreted as a case where the estimated motion is quantized to integers. However, adding positional encodings to key and value encoders leads to model collapse, as the relative positional information for the neighboring pixels is missing.




\noindent \textbf{Window Size}:
The PSNR/SSIM results for different window sizes of the cross-attention operation are presented in \cref{tab:window_size}. We observe a larger window have a bigger receptive field, but they also reduce the alignment quality due to additional noise. A window size of 4 causes model divergence during training. However, we discovered that for Real-world VSR where predicting accurate optical flow is difficult, a larger window size will improve to increase the robustness of the model.

\begin{table}[h]
\centering
\small
\begin{tabular}{c|c|c|c}
\toprule
$PE_Q$      & $PE_M$    & PSNR & SSIM   \\
\midrule
\xmark      & \xmark    & 30.43 & 0.8700   \\
\cmark      & \xmark    & 28.71 & 0.8184   \\
\xmark      & \cmark    & 30.54 & 0.8730   \\
\cmark      & \cmark    & 30.71 & 0.8776   \\
\bottomrule
\end{tabular}
\caption{Ablations on positional encodings. The positional encoding improves the alignment effectiveness compared to the naive window-based cross-attention.}\label{tab:pe}
\end{table}

\begin{table}[h]
\centering
\small
\begin{tabular}{c|c|c|c}
\toprule
Window Size & 2 &3 &4   \\
\midrule
PSNR/SSIM           & 30.71/0.8776  &30.67/0.8775   &  \textit{diverge}      \\
\bottomrule
\end{tabular}
\caption{PSNR/SSIM for different window sizes.}\label{tab:window_size}
\vspace{-.5cm}

\end{table}

\subsection{Qualitative Results}
\cref{fig:qual_reds4} shows qualitative comparisons between BasicVSR and IA-CNN on the REDS4 dataset. The implicit alignment can propagate more high-frequency contents and reconstruct finer patterns than the baseline. See Supplementary for more qualitative results.
\subsection{Real-World Video Super-Resolution}

For real-world VSR with unknown degradation, existing methods tend to produce overly smoothed results due to the lack of high-frequency content. Since the degradation is unknown, there is no fixed re-sampling method that can be effective. We demonstrate that our methods can enhance the high-frequency content when applied on top of RealBasicVSR \cite{chan2022investigating}. As the optical flow computed on the degraded image is often inaccurate, we use a window size of 4 to increase the receptive field and thus the information aggregation. \cref{fig:qual_realvsr} shows qualitative comparison. When embedded in RealBasicVSR, our implicit alignment can produce more realistic and fine-grained results. Please refer to the Supplementary for more qualitative results.

\vspace{-0.3em}
\section{Conclusion}\label{sec:conclusion}
This paper studies the impact of re-sampling on alignment for video super-resolution through experiments on a synthetic dataset utilizing ground-truth optical flow. We find that a re-sampling should preserve the original sharpness of the features and prevent distortions for an effective alignment. We further propose an implicit alignment through window-based cross-attention with estimated motions encoded into positional encoding. Our method outperforms SOTA alignments on both synthetic and real-world datasets. One of the limitations of implicit alignment is that it cannot be combined with multiple offsets, making it hard to benefit from ensemble motion predictions. The possible feature work could be embedding multiple motion offsets into positional encoding for more accurate motion estimation.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{cite}
}

\end{document}