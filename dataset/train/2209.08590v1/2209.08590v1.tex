\section{Theoretical Analysis}
\label{sec:theory}

In this section, we perform some theoretical analyses on \texttt{RankFeat} to support the empirical results. We start by proving that removing the rank-1 feature with a larger  would reduce the upper bound of \texttt{RankFeat} score more. Then based on Random Matrix Theory (RMT), we show that removing the rank-1 matrix makes the statistics of OOD features closer to random matrices. Finally, the theoretical connection of \texttt{ReAct} and our \texttt{RankFeat} is analyzed and discussed: both approaches work by optimizing the score upper bound determined by . \texttt{ReAct} manually uses a pre-defined threshold to clip the term with , whereas our \texttt{RankFeat} directly optimizes the bound by subtracting this term.



\noindent \textbf{Removing the rank-1 matrix with a larger  would reduce the upper bound of RankFeat more.} For our \texttt{RankFeat} score function, we can express its upper bound in an analytical form. Moreover, the upper bound analysis explicitly indicates that removing the rank-1 matrix with a larger first singular value would reduce the upper bound more. Specifically, we have the following proposition.

\begin{prop}
The upper bound of \texttt{RankFeat} score is defined as  where  denotes the number of categories, and  and  are the weight and bias of the last layer, respectively. A larger  would reduce the upper bound more.
\end{prop}

\begin{proof}

For the feature , its SVD   can be expressed as the summation of rank-1 matrices . The feature perturbed by \texttt{RankFeat} can be computed as:

where  denotes the shorter side of the matrix (usually ). In most deep models~\cite{he2016deep,he2016identity}, usually the last feature map needs to pass a Global Average Pooling (GAP) layer to collapse the width and height dimensions. The GAP layer can be represented by a vector

The pooled feature map is calculated as . Then the output logits are computed by the matrix-vector product with the classification head as:

where  denotes the weight matrix,  represents the bias vector, and  is the output logits that correspond to the perturbed feature . Our \texttt{RankFeat} score is computed as:

where  is the input image, and  denotes the number of categories. Here we choose Energy~\cite{liu2020energy} as the base function due to its theoretical alignment with the input probability density and its strong empirical performance. Eq.~\eqref{eq:rankfeat} can be re-formulated by the \texttt{Log-Sum-Exp} trick

The above equation directly yields the tight bound as:

Since , we have

The vector norm has the property of triangular inequality, \emph{i.e.,}  holds for any vectors  and . Moreover, since both  and  are orthogonal vectors, we have the relation  and .
Relying on these two properties, injecting eq.~\eqref{eq:logit} into eq.~\eqref{eq:score_upper} leads to

Since  is a scaled all-ones vector, we have . The bound is simplified as:

As indicated above, removing a larger  would reduce the upper bound of \texttt{RankFeat} score more.
\end{proof}



\noindent\textbf{Remark:} Considering that OOD feature usually has a much larger  (see Fig.~\ref{fig:cover}(a)), \texttt{RankFeat} would reduce the upper bound of OOD samples more. 

\emph{Notice that our bound analysis strives to improve the understanding of OOD methods from new perspectives instead of giving a strict guarantee of the score.} For example, the upper bound can be used to explain the shrinkage and skew of score distributions in Fig.~\ref{fig:score_dist}. Subtracting  would largely reduce the numerical range of both ID and OOD scores, which could squeeze score distributions. Since the dominant singular value  contributes most to the score, removing  is likely to make many samples have similar scores. This would concentrate samples in a smaller region and further skew the distribution. Given that the OOD feature tends to have a much larger , this would have a greater impact on OOD data and skew the OOD score distribution more.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.99\linewidth]{imgs/mp_dist.png}
    \caption{The exemplary eigenvalue distribution of ID/OOD feature and the fitted MP distribution. After the rank-1 matrix is removed, the lowest bin of OOD feature has a larger reduction and the middle bins gain some growth, making the ODD feature statistics closer to the MP distribution. }
    \label{fig:mp_dist}
\end{figure}

\noindent \textbf{Removing the rank-1 matrix is likely to make the statistics of OOD features closer to random matrices.} Now we turn to use RMT to analyze the statistics of OOD and ID feature matrices. For a random matrix of a given shape, the density of its eigenvalue asymptotically converges to the Manchenko-Pastur (MP) distribution~\cite{marvcenko1967distribution,sengupta1999distributions}. Formally, we have:
\begin{thm}[Manchenko-Pastur Law~\cite{marvcenko1967distribution,sengupta1999distributions}]
Let  be a random matrix of shape  whose entries are random variables with  and . Then the eigenvalues of the sample covariance  converges to the probability density function:  where  and .
\end{thm}

This theorem implies the possibility to measure the statistical distance between ID/OOD features and random matrices. To this end, we randomly sample  ID and OOD feature matrices and compute the KL divergence between the actual eigenvalue distribution and the fitted MP distribution.




\begin{table}[htbp]
    \caption{The KL divergence between ID/OOD feature and the fitted MP distribution. When the rank-1 feature is removed, the statistics of OOD matrix are closer to random matrices. }
    \centering
    \resizebox{0.7\linewidth}{!}{
    \begin{tabular}{c|cc|cc}
    \toprule
        \multirow{2}*{\textbf{Matrix Type}} & \multicolumn{2}{c|}{\textbf{Block 4}} & \multicolumn{2}{c}{\textbf{Block 3}} \\
        \cmidrule{2-5}
         & ID & OOD & ID & OOD \\
         \midrule
         Original feature matrix & 18.36 & 18.24 & 11.27 & 11.18\\
         Removing rank-1 matrix & 17.07 ( 1.29)& \textbf{15.79 ( 2.45)}& 9.84 ( 1.45) & \textbf{8.71 ( 2.47)} \\
    \bottomrule
    \end{tabular}
    }
    \label{tab:mp_distance}
\end{table}

Fig.~\ref{fig:mp_dist} and Table~\ref{tab:mp_distance} present the exemplary eigenvalue distribution and the average evaluation results of Block 4 and Block 3 features, respectively. For the original feature, the OOD and ID feature matrices exhibit similar behaviors: the distances to the fitted MP distribution are roughly the same (). However, when the rank-1 matrix is removed, the OOD feature matrix has a much larger drop in the KL divergence. This indicates that removing the rank-1 matrix makes the statistics of OOD feature closer to random matrices, \emph{i.e.,} the OOD feature is very likely to become less informative than the ID feature. The result partly explains the working mechanism of \texttt{RankFeat}: \textit{by removing the feature matrix where OOD data might convey more information than ID data, the two types of distributions have a larger discrepancy and therefore can be better separated.} 





\noindent \textbf{Connection with ReAct~\cite{sun2021react}.} \texttt{ReAct} clips the activations at the penultimate layer of a model to distinguish ID and OOD samples. Given the feature  and the pooling layer , the perturbation can be defined as:

where  is a pre-defined threshold. Their method shares some similarity with \texttt{RankFeat} formulation . \textit{Both approaches subtract from the feature a portion of information that is most likely to cause the over-confidence of OOD prediction.} \texttt{ReAct} selects the manually-defined threshold  based on statistics of the whole ID set, while \texttt{RankFeat} generates the structured rank-1 matrix from the feature itself. Taking a step further, \texttt{ReAct} has the score inequality following eq.~\eqref{eq:score_upper}

Since  is non-negative (output of \texttt{ReLU}),
we have . Exploiting the vector norm inequality  leads to the relation . Relying on this property, the above inequality can be re-formulated as:

As indicated above, the upper bound of \texttt{ReAct} is also determined by the largest singular value . In contrast, the upper bound of our \texttt{RankFeat} can be expressed as:

The upper bounds of both methods resemble each other with the only different term boxed. \textit{From this point of view, both methods distinguish the ID and OOD data by eliminating the impact of the term containing  in the upper bound.} \texttt{ReAct} optimizes it by clipping the term with a manually-defined threshold, which is indirect and might be sub-optimal. Moreover, the threshold selection requires statistics of the whole ID set. In contrast, our \texttt{RankFeat} does not require any extra data and directly subtracts this underlying term which is likely to cause the over-confidence of OOD samples. 



\section{Experimental Results}

In this section, we first discuss the setup in Sec.~\ref{sec:exp_setup}, and then present the main experimental results on ImageNet-1k in Sec.~\ref{sec:exp_result}, 
followed by the extensive ablation studies in Sec.~\ref{sec:exp_ablation}.

\subsection{Setup}
\label{sec:exp_setup}

\noindent \textbf{Datasets.} In line with~\cite{huang2021mos,sun2021react,huang2021importance}, we mainly evaluate our method on the large-scale ImageNet-1k benchmark~\cite{deng2009imagenet}. The large-scale dataset is more challenging than the traditional CIFAR benchmark~\cite{krizhevsky2009learning} because the images are more realistic and diverse (\emph{i.e.,} M images of  classes). For the OOD datasets, we select four testsets from subsets of \texttt{iNaturalist}~\cite{van2018inaturalist}, \texttt{SUN}~\cite{xiao2010sun}, \texttt{Places}~\cite{zhou2017places}, and \texttt{Textures}~\cite{cimpoi2014describing}. These datasets are crafted by~\cite{huang2021mos} with non-overlapping categories from ImageNet-1k. Besides the experiment on the large-scale benchmark, we also validate the effectiveness of our approach on Species~\cite{hendrycks2019scaling} and CIFAR~\cite{krizhevsky2009learning} benchmark. (see Supplementary Material).



\noindent \textbf{Baselines.}
We compare our method with  recent \emph{post hoc} OOD detection methods, namely \texttt{MSP}~\cite{hendrycks2016baseline}, \texttt{ODIN}~\cite{liang2017enhancing}, \texttt{Energy}~\cite{liu2020energy}, \texttt{Mahalanobis}~\cite{lee2018simple}, \texttt{GradNorm}~\cite{huang2021importance}, and \texttt{ReAct}~\cite{sun2021react}. The detailed illustration and settings of these methods are kindly referred to Supplementary Material.

\noindent \textbf{Architectures.} In line with~\cite{huang2021importance}, the main evaluation is done using Google BiT-S model~\cite{kolesnikov2020big} pretrained on ImageNet-1k with ResNetv2-101~\cite{he2016identity}. We also evaluate the performance on SqueezeNet~\cite{iandola2016squeezenet}, an alternative tiny architecture suitable for mobile devices and on T2T-ViT-24~\cite{yuan2021tokens}, a tokens-to-tokens vision transformer that has impressive performance when trained from scratch. 

\textit{For the implementation details and evaluation metrics, please refer to Supplementary Material}.

\begin{table}[htbp]
    \caption{Main evaluation results on ResNetv2-101~\cite{he2016identity}. All values are reported in percentages, and these \emph{post hoc} methods are directly applied to the model pre-trained on ImageNet-1k~\cite{deng2009imagenet}. The best three results are highlighted with \textbf{\textcolor{red}{red}}, \textbf{\textcolor{blue}{blue}}, and \textbf{\textcolor{cyan}{cyan}}.}
    \centering
    \resizebox{0.99\linewidth}{!}{
    \begin{tabular}{c|cc|cc|cc|cc|cc}
    \toprule
        \multirow{3}*{\textbf{Methods}} & \multicolumn{2}{c|}{\textbf{iNaturalist}} & \multicolumn{2}{c|}{\textbf{SUN}} & \multicolumn{2}{c|}{\textbf{Places}} & \multicolumn{2}{c|}{\textbf{Textures}} & \multicolumn{2}{c}{\textbf{Average}}   \\
        \cmidrule{2-11}
         & FPR95 & AUROC  & FPR95 & AUROC  & FPR95 & AUROC  & FPR95 & AUROC & FPR95 & AUROC  \\
         & () & () & () & () & () & () & () & () & () & ()\\
    \midrule
    MSP~\cite{hendrycks2016baseline} & 63.69 & 87.59 & 79.89 & 78.34 & 81.44 & 76.76 & 82.73 & 74.45 & 76.96 & 79.29\\
    ODIN~\cite{liang2017enhancing} & 62.69 & 89.36 & 71.67 & 83.92 & 76.27 & 80.67 & 81.31 & 76.30 & 72.99 & 82.56\\
    Energy~\cite{liu2020energy} & 64.91 & 88.48 & 65.33 & 85.32 & 73.02 & 81.37 & 80.87 & 75.79 & 71.03 & 82.74\\
    Mahalanobis~\cite{lee2018simple} &96.34 &46.33 &88.43 & 65.20 &89.75 &64.46 &52.23 &72.10 &81.69 &62.02\\
    GradNorm~\cite{huang2021importance} &50.03 &90.33 &46.48 &89.03 &60.86 &84.82 &61.42 &81.07 &54.70 &86.71\\
    ReAct~\cite{sun2021react} &\textbf{\textcolor{blue}{44.52}}&\textbf{\textcolor{blue}{91.81}}&52.71&90.16&62.66&87.83&70.73&76.85&57.66&86.67\\
\midrule
    \rowcolor{gray!20}\textbf{RankFeat (Block 4)} &\textbf{\textcolor{cyan}{46.54}} &81.49 &\textbf{\textcolor{red}{27.88}} & \textbf{\textcolor{blue}{92.18}} &\textbf{\textcolor{red}{38.26}} & \textbf{\textcolor{blue}{88.34}} & \textbf{\textcolor{cyan}{46.06}} & \textbf{\textcolor{cyan}{89.33}} & \textbf{\textcolor{blue}{39.69}} &\textbf{\textcolor{cyan}{87.84}}\\
    \rowcolor{gray!20}\textbf{RankFeat (Block 3)} &{49.61} &\textbf{\textcolor{cyan}{91.42}} &\textbf{\textcolor{cyan}{39.91}} &\textbf{\textcolor{cyan}{92.01}} &\textbf{\textcolor{cyan}{51.82}} &\textbf{\textcolor{cyan}{88.32}} &\textbf{\textcolor{blue}{41.84}} &\textbf{\textcolor{blue}{91.44}} &\textbf{\textcolor{cyan}{45.80}} &\textbf{\textcolor{blue}{90.80}} \\
\rowcolor{gray!20} \textbf{RankFeat (Block 3 + 4)}
     &\textbf{\textcolor{red}{41.31}}&\textbf{\textcolor{red}{91.91}}&\textbf{\textcolor{blue}{29.27}}&\textbf{\textcolor{red}{94.07}}&\textbf{\textcolor{blue}{39.34}}&\textbf{\textcolor{red}{90.93}}&\textbf{\textcolor{red}{37.29}}&\textbf{\textcolor{red}{91.70}}&\textbf{\textcolor{red}{36.80}}&\textbf{\textcolor{red}{92.15}}\\
\bottomrule
    \end{tabular}
    }
    \label{tab:main_results_res101}
\end{table}

\subsection{Results}
\label{sec:exp_result}

\noindent \textbf{Main results.} Following~\cite{huang2021importance}, the main evaluation is conducted using Google BiT-S model~\cite{kolesnikov2020big} pretrained on ImageNet-1k with ResNetv2-101 architecture~\cite{he2016identity}. Table~\ref{tab:main_results_res101} compares the performance of all the \emph{post hoc} methods. For both Block 3 and Block 4 features, our \texttt{RankFeat} achieves the best evaluation results across datasets and metrics. More specifically, \texttt{RankFeat} based on the Block 4 feature outperforms the second-best baseline by  in the average FPR95, while the Block 3 feature-based \texttt{RankFeat} beats the second-best method by  in the average AUROC. Their combination further surpasses other methods by  in the average FPR95 and by  in the average AUROC. The superior performances at various depths demonstrate the effectiveness and general applicability of \texttt{RankFeat}. The Block 3 feature has a higher AUROC but slightly falls behind the Block 4 feature in the FPR95, which can be considered a compromise between the two metrics.





\begin{table}[htbp]
    \caption{The results on SqueezeNet~\cite{iandola2016squeezenet} and T2T-ViT-24~\cite{yuan2021tokens}. All values are reported in percentages, and these \emph{post hoc} methods are directly applied to the model pre-trained on ImageNet-1k~\cite{deng2009imagenet}. For results on SqueezeNet~\cite{iandola2016squeezenet}, the best three results are highlighted with \textbf{\textcolor{red}{red}}, \textbf{\textcolor{blue}{blue}}, and \textbf{\textcolor{cyan}{cyan}}.}
    \centering
    \resizebox{0.99\linewidth}{!}{
    \begin{tabular}{c|c|cc|cc|cc|cc|cc}
    \toprule
        \multirow{3}*{\textbf{Model}}&\multirow{3}*{\textbf{Methods}} & \multicolumn{2}{c|}{\textbf{iNaturalist}} & \multicolumn{2}{c|}{\textbf{SUN}} & \multicolumn{2}{c|}{\textbf{Places}} & \multicolumn{2}{c|}{\textbf{Textures}} & \multicolumn{2}{c}{\textbf{Average}}   \\
        \cmidrule{3-12}
         && FPR95 & AUROC  & FPR95 & AUROC  & FPR95 & AUROC  & FPR95 & AUROC & FPR95 & AUROC  \\
         && () & () & () & () & () & () & () & () & () & ()\\
    \midrule
    \multirow{9}*{\textbf{SqueezeNet~\cite{iandola2016squeezenet}}}&MSP~\cite{hendrycks2016baseline} 
    &89.83&65.41&83.03&72.25&87.27&67.00&94.61&41.84&88.84&61.63\\
    &ODIN~\cite{liang2017enhancing} &90.79&65.75&78.32&78.37&83.23&73.31&92.25&43.43&86.15&65.17\\
    &Energy~\cite{liu2020energy} &79.27&73.30&56.41&87.88&67.74&82.73&67.16&64.51&67.65&77.11\\
    &Mahalanobis~\cite{lee2018simple} &91.50&51.79&90.33&62.18&92.26&56.63&58.60&67.16&83.17&59.44\\
    &GradNorm~\cite{huang2021importance} &76.31&73.92&53.63&87.55&65.99&\textbf{\textcolor{cyan}{83.28}}&68.72&68.07&66.16&78.21\\
    &ReAct~\cite{sun2021react} &76.78&68.56&87.57&66.37&88.80&66.20&51.05&76.57&76.05&69.43\\
\cmidrule{2-12}
   \rowcolor{gray!20} \cellcolor{white}&\textbf{RankFeat (Block 4)} &\textbf{\textcolor{red}{61.67}}&\textbf{\textcolor{red}{83.09}}&\textbf{\textcolor{blue}{46.72}}&\textbf{\textcolor{cyan}{88.31}}&\textbf{\textcolor{red}{61.31}}&{80.52}&\textbf{\textcolor{red}{38.04}}&\textbf{\textcolor{red}{88.82}}&\textbf{\textcolor{red}{51.94}}&\textbf{\textcolor{blue}{85.19}}\\
    \rowcolor{gray!20} \cellcolor{white}&\textbf{RankFeat (Block 3)} &\textbf{\textcolor{cyan}{71.04}}&\textbf{\textcolor{cyan}{81.50}}&\textbf{\textcolor{cyan}{49.18}}&\textbf{\textcolor{red}{90.43}}&\textbf{\textcolor{cyan}{62.94}}&\textbf{\textcolor{red}{85.82}}&\textbf{\textcolor{cyan}{50.14}}&\textbf{\textcolor{cyan}{79.32}}&\textbf{\textcolor{cyan}{58.33}}&\textbf{\textcolor{cyan}{84.28}} \\
    \rowcolor{gray!20} \cellcolor{white}&\textbf{RankFeat (Block 3 + 4)}
    &\textbf{\textcolor{blue}{65.81}}&\textbf{\textcolor{blue}{83.06}}&\textbf{\textcolor{red}{46.64}}&\textbf{\textcolor{blue}{90.17}}&\textbf{\textcolor{blue}{61.56}}&\textbf{\textcolor{blue}{84.51}}&\textbf{\textcolor{blue}{42.54}}&\textbf{\textcolor{blue}{85.00}}&\textbf{\textcolor{blue}{54.14}}&\textbf{\textcolor{red}{85.69}}\\
    \midrule
    \multirow{7}*{\textbf{T2T-ViT-24~\cite{yuan2021tokens}}}&MSP~\cite{hendrycks2016baseline} &48.92&88.95&61.77&81.37&69.54&80.03&62.91&82.31&60.79&83.17\\ 
    &ODIN~\cite{liang2017enhancing} &\textbf{44.07}&88.17&63.83&78.46&68.19&75.33&54.27&83.63&57.59&81.40\\
    &Energy~\cite{liu2020energy} &52.95&82.93&68.55&73.06&74.24&68.17&51.05&83.25&61.70&76.85\\
    &Mahalanobis~\cite{lee2018simple} &90.50&58.13&91.71&50.52&93.32&49.60&80.67&64.06&89.05&55.58\\
    &GradNorm~\cite{huang2021importance} &99.30&25.86&98.37&28.06&99.01&25.71&92.68&38.80&97.34&29.61\\
    &ReAct~\cite{sun2021react} &52.17&\textbf{89.51}&65.23&81.03&68.93&78.20&52.54&85.46&59.72&83.55\\
    \cmidrule{2-12}
\rowcolor{gray!20} \cellcolor{white}&\textbf{RankFeat} &50.27 & 87.81 & \textbf{57.18} & \textbf{84.33} & \textbf{66.22} & \textbf{80.89} & \textbf{32.64} & \textbf{89.36} &\textbf{51.58} & \textbf{85.60} \\
    \bottomrule
    \end{tabular}
    }
    \label{tab:main_results_vit}
\end{table}


\noindent \textbf{Our RankFeat is also effective on alternative CNN architectures.}
Besides the experiment on ResNetv2~\cite{he2016identity}, we also evaluate our method on SqueezeNet~\cite{iandola2016squeezenet}, an alternative tiny network suitable for mobile devices and on-chip applications. This network is more challenging because the tiny network size makes the model prone to overfit the training data, which could increase the difficulty to distinguish between ID and OOD samples. Table~\ref{tab:main_results_vit} top presents the performance of all the methods. Collectively, the performances of \texttt{RankFeat} are very competitive at both depths, as well as the score fusion. Our \texttt{RankFeat} achieves the \emph{state-of-the-art} performances, outperforming the second-best baseline by  in FPR95 and by  in AUROC.


\noindent \textbf{Our RankFeat also suits transformer-based architectures.} To further demonstrate the applicability of our method, we evaluate \texttt{RankFeat} on Tokens-to-Tokens Vision Transformer (T2T-ViT)~\cite{yuan2021tokens}, a popular transformer architecture that can achieve competitive performance against CNNs when trained from scratch. Similar to the CNN, \texttt{RankFeat} removes the rank-1 matrix from the final token of T2T-ViT before the last normalization layer and the classification head. Table~\ref{tab:main_results_vit} bottom compares the performance on T2T-ViT-24. Our \texttt{RankFeat} outperforms the second-best method by  in FPR95 and by  in AUROC. Since the transformer models~\cite{dosovitskiy2020image,yuan2021tokens} do not have increasing receptive fields like CNNs, we do not evaluate the performance at alternative network depths. 


\noindent \textbf{Comparison against training-needed approaches.} Since our method is \emph{post hoc}, we only compare it with other \emph{post hoc} baselines. MOS~\cite{huang2021mos} and KL Matching~\cite{hendrycks2019scaling} are not taken into account because MOS needs extra training processes and KL Matching requires the labeled validation set to compute distributions for each class. Nonetheless, we note that our method can still hold an advantage against those approaches. Table~\ref{tab:mos_kl} presents the average FPR95 and AUROC on the ImageNet-1k benchmark. Our RankFeat achieves the best performance without any extra training or validation set.

\begin{table}[htbp]
    \centering
    \caption{Comparison against training-needed methods on ImageNet-1k based on ResNetv2-101~\cite{he2016identity}.}
    \resizebox{0.7\linewidth}{!}{
    \begin{tabular}{c|c|c|c|c}
    \toprule
         Method & Post hoc? & Free of Validation Set? & FPR95 () & AUROC ()\\
    \midrule
         KL Matching~\cite{hendrycks2019scaling}&  &&54.30&80.82 \\
         MOS~\cite{huang2021mos} &  &  &39.97&90.11 \\
         \rowcolor{gray!20}\textbf{RankFeat} &  &  & \textbf{36.80} &\textbf{92.15}\\
\bottomrule
    \end{tabular}
    }
    \label{tab:mos_kl}
\end{table}

\subsection{Ablation Studies}
\label{sec:exp_ablation}

In this subsection, we conduct several ablation studies based on Google-BiT-S ResNetv2-101 model. Unless explicitly specified, we apply \texttt{RankFeat} on the Block 4 feature by default. 

\begin{table}[htbp]
    \caption{Ablation studies on keeping only the rank-1 matrix and removing the rank-n matrix. }
    \centering
    \resizebox{0.99\linewidth}{!}{
    \begin{tabular}{c|cc|cc|cc|cc|cc}
    \toprule
        \multirow{3}*{\textbf{Baselines}} & \multicolumn{2}{c|}{\textbf{iNaturalist}} & \multicolumn{2}{c|}{\textbf{SUN}} & \multicolumn{2}{c|}{\textbf{Places}} & \multicolumn{2}{c|}{\textbf{Textures}} & \multicolumn{2}{c}{\textbf{Average}}   \\
        \cmidrule{2-11}
         & FPR95 & AUROC  & FPR95 & AUROC  & FPR95 & AUROC  & FPR95 & AUROC & FPR95 & AUROC  \\
         & () & () & () & () & () & () & () & () & () & ()\\
    \midrule
    GradNorm~\cite{huang2021importance} &50.03 &90.33 &46.48 &89.03 &60.86 &84.82 &61.42 &81.07 &54.70 &86.71\\
    ReAct~\cite{sun2021react} &\textbf{44.52}&{91.81}&52.71&90.16&62.66&87.83&70.73&76.85&57.66&86.67\\
    \midrule
    Keeping Only Rank-1 &48.97&\textbf{91.93}&62.63&84.62&72.42&79.79&49.42&88.86&58.49&86.30 \\
    Removing Rank-3 &55.19&90.03&48.97&91.26&56.63&\textbf{88.81}&86.95&74.57&61.94&86.17\\
    Removing Rank-2 &50.04&89.30&48.55&90.99&56.23&88.38&76.86&81.37&57.92&87.51\\
    \midrule
    \rowcolor{gray!20}\textbf{Removing Rank-1} &\textbf{46.54} &81.49 &\textbf{27.88} & \textbf{92.18} &\textbf{38.26} & {88.34} & \textbf{46.06} & \textbf{89.33} & \textbf{39.69} &\textbf{87.84}\\
    \bottomrule
    \end{tabular}
    }
    \label{tab:ablation_rank}
\end{table}

\noindent \textbf{Removing the rank-1 matrix outperforms keeping only it.} Instead of removing the rank-1 matrix, another seemingly promising approach is keeping only the rank-1 matrix and abandoning the rest of the matrix. Table~\ref{tab:ablation_rank} presents the evaluation results of keeping only the rank-1 matrix. The performance falls behind that of removing the rank-1 feature by  in FPR95, which indicates that keeping only the rank-1 feature is inferior to removing it in distinguishing the two distributions. Nonetheless, it is worth noting that even keeping only the rank-1 matrix achieves very competitive performance against previous best methods, such as \texttt{GradNorm}~\cite{huang2021importance} and \texttt{ReAct}~\cite{sun2021react}.

\noindent \textbf{Removing the rank-1 matrix outperforms removing the rank-n matrix (n>1).} We evaluate the impact of removing the matrix of a higher rank, \emph{i.e.,} performing  where  for the high-level feature . Table~\ref{tab:ablation_rank} compares the performance of removing the rank-2 matrix and rank-3 matrix. When the rank of the removed matrix is increased, the average performance degrades accordingly. This demonstrates that removing the rank-1 matrix is the most effective approach to separate ID and OOD data. This result is coherent with the finding in Fig.~\ref{fig:cover}(a): only the largest singular value of OOD data is significantly different from that of ID data. Therefore, removing the rank-1 matrix achieves the best performance.  

\begin{table}[htbp]
    \caption{The ablation study on applying \texttt{RankFeat} to features at different network depths.} \centering
    \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{c|cc|cc|cc|cc|cc}
    \toprule
        \multirow{3}*{\textbf{Layer}} & \multicolumn{2}{c|}{\textbf{iNaturalist}} & \multicolumn{2}{c|}{\textbf{SUN}} & \multicolumn{2}{c|}{\textbf{Places}} & \multicolumn{2}{c|}{\textbf{Textures}} & \multicolumn{2}{c}{\textbf{Average}}   \\
        \cmidrule{2-11}
         & FPR95 & AUROC  & FPR95 & AUROC  & FPR95 & AUROC  & FPR95 & AUROC & FPR95 & AUROC  \\
         & () & () & () & () & () & () & () & () & () & ()\\
    \midrule
    Block 1 &87.81&77.00&59.15&87.29&65.50&84.35&94.15&60.41&76.65&77.26\\
    Block 2 &71.84&85.80&61.44&86.46&71.68&81.65&87.89&72.04&73.23&81.49\\
    \rowcolor{gray!20}\textbf{Block 3} &\textbf{49.61} &\textbf{91.42} &\textbf{39.91} &\textbf{92.01} &\textbf{51.82} &\textbf{88.32} &\textbf{41.84} &\textbf{91.44} &\textbf{45.80} &\textbf{90.80} \\
    \rowcolor{gray!20}\textbf{Block 4} &\textbf{46.54} &81.49 &\textbf{27.88} & \textbf{92.18} &\textbf{38.26} & \textbf{88.34} & \textbf{46.06} & \textbf{89.33} & \textbf{39.69} &\textbf{87.84}\\
    \bottomrule
    \end{tabular}
    }
    \label{tab:ablation_block}
\end{table}

\noindent \textbf{Block 3 and Block 4 features are the most informative.} In addition to exploring the high-level features at Block 3 and Block 4, we also investigate the possibility of applying \texttt{RankFeat} to features at shallow network layers. As shown in Table~\ref{tab:ablation_block}, the performances of \texttt{RankFeat} at the Block 1 and Block 2 features are not comparable to those at deeper layers. This is mainly because the shallow low-level features do not embed as rich semantic information as the deep features. Consequently, removing the rank-1 matrix of shallow features would not help to separate the ID and OOD data. 







\begin{table}[htbp]
    \caption{The approximate solution by PI yields competitive performance and costs much less time consumption. The test batch size is set as .}
    \centering
    \resizebox{0.99\linewidth}{!}{
    \begin{tabular}{c|c|cc|cc|cc|cc|cc}
    \toprule
        \multirow{3}*{\makecell[c]{\textbf{Computation} \\ \textbf{Technique}}} &
        \multirow{3}*{\makecell[c]{\textbf{Processing Time} \\ \textbf{Per Image} \\ \textbf{(ms)}}} 
        & \multicolumn{2}{c|}{\textbf{iNaturalist}} & \multicolumn{2}{c|}{\textbf{SUN}} & \multicolumn{2}{c|}{\textbf{Places}} & \multicolumn{2}{c|}{\textbf{Textures}} & \multicolumn{2}{c}{\textbf{Average}}   \\
        \cmidrule{3-12}
         & & FPR95 & AUROC  & FPR95 & AUROC  & FPR95 & AUROC  & FPR95 & AUROC & FPR95 & AUROC  \\
         & & () & () & () & () & () & () & () & () & () & ()\\
         \midrule
          GradNorm~\cite{huang2021importance} &80.01
          &50.03 &90.33 &46.48 &89.03 &60.86 &84.82 &61.42 &81.07 &54.70 &86.71\\
          ReAct~\cite{sun2021react} &8.79 &\textbf{44.52}&\textbf{91.81}&52.71&90.16&62.66&87.83&70.73&76.85&57.66&86.67\\
          \midrule
         SVD & 18.01 &{46.54} &81.49 &\textbf{27.88} & \textbf{92.18} &{38.26} & \textbf{88.34} & \textbf{46.06} & \textbf{89.33} & \textbf{39.69} &\textbf{87.84}\\
         PI ( iter) &9.97 &46.59&81.49&27.93&92.18&38.28&88.34&46.09&89.33&39.72&\textbf{87.84}\\
         PI ( iter) &9.47 &46.58&81.49&27.93&92.17&\textbf{38.24}&88.34&46.12&89.32 &39.72&87.83\\
         PI ( iter) &9.22 &46.58 &81.48 &27.93 &92.15 &38.28 &88.31 &46.10 &89.33 &39.75 &87.82 \\
         PI ( iter) &9.03 &46.77&81.29&28.21&91.84&38.44&87.94&46.08&89.37&39.88&87.61 \\ 
         PI ( iter) &9.00&48.34&79.81&30.44&89.71&41.33&84.97&45.34&89.41&41.36&85.98 \\ 
\bottomrule
    \end{tabular}
    }
    \label{tab:ablation_approximate}
\end{table}

\noindent \textbf{The approximate solution by PI yields competitive performances.} Table~\ref{tab:ablation_approximate} compares the time consumption and performance of SVD and PI, as well as two recent \emph{state-of-the-art} OOD methods \texttt{ReAct} and \texttt{GradNorm}. The performance of PI starts to become competitive against that of SVD () from  iterations on with  time reduction. Compared with \texttt{ReAct}, the PI-based \texttt{RankFeat} only requires marginally  more time consumption. \texttt{GradNorm} is not comparable against other baselines in terms of time cost because it does not support the batch mode.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{imgs/fusion.png}
    \caption{The impact of different fusion strategies on FPR95 and AUROC.}
    \label{fig:fusion}
\end{figure}

\noindent \textbf{Fusion at the logit space achieves the best performance.} Fig.~\ref{fig:fusion} displays the performance of different fusion strategies in combining \texttt{RankFeat} at the Block 3 and Block 4 features. As can be observed, averaging logits outperforms other fusion strategies in most datasets and metrics. This indicates that the fusing the logits can best coordinate the benefit of both features.   































































