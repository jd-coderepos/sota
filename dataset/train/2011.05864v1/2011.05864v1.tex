




\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

\usepackage{microtype}

\aclfinalcopy 



\newcommand\BibTeX{B\textsc{ib}\TeX}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{amsmath}
\usepackage{graphicx}

\usepackage{xcolor}
\usepackage{wrapfig}
\usepackage{comment}
\usepackage{tabu}

\usepackage{caption}
\usepackage{subfigure}


\usepackage{multicol}

\usepackage{longtable}
\usepackage{makecell}
\usepackage{amssymb}
\usepackage{arydshln }
\usepackage{paralist}



\usepackage{bm}
\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\vphi{{\bm{\phi}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sE{{\mathbb{E}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak 

\newcommand*{\mathcolor}{}
\def\mathcolor#1#{\mathcoloraux{#1}}
\newcommand*{\mathcoloraux}[3]{\protect\leavevmode
  \begingroup
    \color#1{#2}#3\endgroup
}

\definecolor{darkblue}{rgb}{0.0, 0.0, 0.55}
\hypersetup{
    colorlinks=true,
    citecolor=darkblue
}

\newcommand \zh[1]{{\bf \color{red} zhouh:#1.}}


\definecolor{Orange}{RGB}{255, 89, 66}
\newcommand{\bl}[1]{\textcolor{Orange}{\bf\small [#1 --BL]}}
\newcommand{\blc}[2]{\textcolor{Orange}{\sout{#1} #2}}

\newcommand{\hz}[1]{\textcolor{red}{\bf\small [#1 --HZ]}}
\newcommand{\hzc}[2]{\textcolor{red}{\sout{#1} #2}}

\newcommand{\jh}[1]{\textcolor{blue}{\bf\small [#1 --JH]}}
\newcommand{\jhc}[2]{\textcolor{blue}{\sout{#1} #2}}

\newcommand\blfootnote[1]{\begingroup
  \renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}

\title{On the Sentence Embeddings from Pre-trained Language Models}
\author{Bohan Li\thanks{\ \ The work was done when BL was an intern at ByteDance.} , 
Hao Zhou, 
Junxian He, 
Mingxuan Wang, 
Yiming Yang, 
Lei Li \\
  ByteDance AI Lab \\
  Language Technologies Institute, Carnegie Mellon University \\
  {\texttt{\{zhouhao.nlp,wangmingxuan.89,lileilab\}@bytedance.com}} \\
  {\texttt{\{bohanl1,junxianh,yiming\}@cs.cmu.edu}}
  }

\date{}

\begin{document}
\maketitle

\begin{abstract}
Pre-trained contextual representations like BERT have achieved great success in natural language processing. 
However, the sentence embeddings from the pre-trained language models without fine-tuning have been found to poorly capture semantic meaning of sentences. 
In this paper, we argue that the semantic information in the BERT embeddings is not fully exploited. 
We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically. We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of semantic similarity. To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective. Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks.
The code is available at \url{https://github.com/bohanli/BERT-flow}.

\end{abstract}
 \section{Introduction}
\label{sec:intro}
Recently, pre-trained language models and its variants~\citep{radford2019language,devlin2018bert,yang2019xlnet,liu2019roberta} like BERT~\citep{devlin2018bert} have been widely used as representations of natural language. Despite their great success on many NLP tasks through fine-tuning, the sentence embeddings from BERT without fine-tuning are significantly inferior in terms of semantic textual similarity~\citep{reimers2019sentence} -- for example, they even underperform the GloVe~\citep{pennington2014glove} embeddings which are not contextualized and trained with a much simpler model. Such issues hinder applying BERT sentence embeddings directly to many real-world scenarios where collecting labeled data is highly-costing or even intractable.



In this paper, we aim to answer two major questions: (1) why do the BERT-induced sentence embeddings perform poorly to retrieve semantically similar sentences? Do they carry too little semantic information, or just because the semantic meanings in these embeddings are not exploited properly? (2) If the BERT embeddings capture enough semantic information that is hard to be directly utilized, how can we make it easier without external supervision?




Towards this end, we first study the connection between the BERT pretraining objective and the semantic similarity task. Our analysis reveals that the sentence embeddings of BERT should be able to intuitively reflect the semantic similarity between sentences, which contradicts with experimental observations. Inspired by~\citet{gao2019representation} who find that the language modeling performance can be limited by the learned anisotropic word embedding space where the word embeddings occupy a narrow cone, and \citet{ethayarajh2019contextual} who find that BERT word embeddings also suffer from anisotropy, we hypothesize that the sentence embeddings from BERT -- as average of context embeddings from last layers\footnote{In this paper, we compute average of context embeddings from last one or two layers as our sentence embeddings since they are consistently better than the [CLS] vector as shown in~\citep{reimers2019sentence}.} -- may suffer from similar issues. 
Through empirical probing over the embeddings, we further observe that the BERT sentence embedding space is semantically non-smoothing and poorly defined in some areas, which makes it hard to be used directly through simple similarity metrics such as dot product or cosine similarity. 













To address these issues, we propose to transform the BERT sentence embedding distribution into a smooth and isotropic Gaussian distribution through normalizing flows~\citep{dinh2014nice}, which is an invertible function parameterized by neural networks. 
Concretely, we learn a flow-based generative model to maximize the likelihood of generating BERT sentence embeddings from a standard Gaussian latent variable in a \emph{unsupervised} fashion. During training, only the flow network is optimized while the BERT parameters remain unchanged. The learned flow, an invertible mapping function between the BERT sentence embedding and Gaussian latent variable, is then used to transform the BERT sentence embedding to the Gaussian space. We name the proposed method as \textit{BERT-flow}.


We perform extensive experiments on 7 standard semantic textual similarity benchmarks without using any downstream supervision. Our empirical results demonstrate that the flow transformation is able to consistently improve BERT by up to 12.70 points with an average of 8.16 points in terms of Spearman correlation between cosine embedding similarity and human annotated similarity. When combined with external supervision from natural language inference tasks~\citep{snli,mnli}, our method outperforms the sentence-BERT embeddings~\citep{reimers2019sentence}, leading to new state-of-the-art performance. In addition to semantic similarity tasks, we apply sentence embeddings to a question-answer entailment task, QNLI~\citep{wang2018glue}, directly without task-specific supervision, and demonstrate the superiority of our approach. 
Moreover, our further analysis implies that BERT-induced similarity can excessively correlate with lexical similarity compared to semantic similarity, and our proposed flow-based method can effectively remedy this problem. 


 
\section{Understanding the Sentence Embedding Space of BERT}
\label{sec:2}
To encode a sentence into a fixed-length vector with BERT, it is a convention to either compute an average of context embeddings in the last few layers of BERT, or extract the BERT context embedding at the position of the [CLS] token. Note that there is no token masked when producing sentence embeddings, which is different from pretraining. 

\citet{reimers2019sentence} demonstrate that such BERT sentence embeddings lag behind the state-of-the-art sentence embeddings in terms of semantic similarity. On the STS-B dataset, BERT sentence embeddings are even less competitive to averaged GloVe~\citep{pennington2014glove} embeddings, which is a simple and non-contextualized baseline proposed several years ago. Nevertheless, this incompetence has not been well understood yet in existing literature. 

Note that as demonstrated by \citet{reimers2019sentence}, averaging context embeddings consistently outperforms the [CLS] embedding. Therefore, unless mentioned otherwise, we use average of context embeddings as BERT sentence embeddings and do not distinguish them in the rest of the paper.


\subsection{The Connection between Semantic Similarity and BERT Pre-training}
\label{sec:formalizing:semantic:similarity}


We consider a sequence of tokens . Language modeling (LM) factorizes the joint probability  in an autoregressive way, namely  where the context . To capture bidirectional context during pretraining, BERT proposes a masked language modeling (MLM) objective, which instead factorizes the probability of noisy reconstruction , where  is a corrupted sequence,  is the masked tokens,  is equal to 1 when  is masked and 0 otherwise. The context .

Note that both LM and MLM can be reduced to modeling the conditional distribution of a token  given the context , which is typically formulated with a softmax function as,



Here the context embedding  is a function of , which is usually heavily parameterized by a deep neural network (e.g., a Transformer~\citep{vaswani2017attention}); The word embedding  is a function of , which is parameterized by an embedding lookup table. 

The similarity between BERT sentence embeddings can be reduced to the similarity between BERT context embeddings \footnote{This is because we approximate BERT sentence embeddings with context embeddings, and compute their dot product (or cosine similarity) as model-predicted sentence similarity. Dot product is equivalent to cosine similarity when the embeddings are normalized to unit hyper-sphere.}.
However, as shown in Equation~\ref{eq:softmax}, the pretraining of BERT does not explicitly involve the computation of . Therefore, we can hardly derive a mathematical formulation of what  exactly represents. 


\paragraph{Co-Occurrence Statistics as the Proxy for Semantic Similarity} 
Instead of directly analyzing , we consider , the dot product between a context embedding  and a word embedding . According to \citet{yang2017breaking}, in a well-trained language model,  can be approximately decomposed as follows,


where  denotes the pointwise mutual information between  and ,  is a word-specific term, and  is a context-specific term.

PMI captures how frequently two events co-occur more than if they independently occur. Note that co-occurrence statistics is a typical tool to deal with ``semantics'' in a computational way --- specifically, PMI is a common mathematical surrogate to approximate word-level semantic similarity~\cite{levy2014neural,ethayarajh2019towards}. Therefore, roughly speaking, it is semantically meaningful to compute the dot product between a context embedding and a word embedding. 


\paragraph{Higher-Order Co-Occurrence Statistics as Context-Context Semantic Similarity.}
During pretraining, the semantic relationship between two contexts  and  could be inferred and reinforced with their connections to words. To be specific, if both the contexts  and  co-occur with the same word , the two contexts are likely to share similar semantic meaning. During the training dynamics, when  and  occur at the same time, the embeddings  and  are encouraged to be closer to each other, meanwhile the embedding  and  where  are encouraged to be away from each other due to normalization. A similar scenario applies to the context . In this way, the similarity between  and  is also promoted. With all the words in the vocabulary acting as hubs, the context embeddings should be aware of its semantic relatedness to each other. 

Higher-order context-context co-occurrence could also be inferred and propagated during pretraining. The update of a context embedding  could affect another context embedding  in the above way, and similarly  can further affect another . Therefore, the context embeddings can form an implicit interaction among themselves via higher-order co-occurrence relations.

\begin{table*}[!h]
\centering
\scalebox{0.8}{
\begin{tabular}{lrrrrrr}
\toprule
Rank of word frequency   &  &  &   &   \\
\midrule\midrule
Mean -norm & 0.95 & 1.04 & 1.22 & 1.45\\
\midrule\midrule
Mean -NN -dist. () & 0.77 & 0.93 & 1.16 & 1.30 \\
Mean -NN -dist. () & 0.83 & 0.99 & 1.22 & 1.34 \\
Mean -NN -dist. () & 0.87 & 1.04 & 1.26 & 1.37\\
\midrule
Mean -NN dot-product. () & 0.73 & 0.92 & 1.20 & 1.63 \\
Mean -NN dot-product. () & 0.73 & 0.91 & 1.19 & 1.61 \\
Mean -NN dot-product. () & 0.72 & 0.90 & 1.17 & 1.60 \\
\bottomrule
\end{tabular}
}
\caption{\label{tab:wordfreq} The mean -norm, as well as their distance to their -nearest neighbors (among all the word embeddings) of the word embeddings of BERT, segmented by ranges of word frequency rank (counted based on Wikipedia dump; the smaller the more frequent).  }


\vspace{-10pt}
\end{table*}

\subsection{Anisotropic Embedding Space Induces Poor Semantic Similarity}
\label{sec:anisotropy}

As discussed in Section~\ref{sec:formalizing:semantic:similarity}, the pretraining of BERT should have encouraged semantically meaningful context embeddings implicitly. Why BERT sentence embeddings without finetuning yield unsatisfactory performance? 



To investigate the underlying problem of the failure, we use word embeddings as a surrogate because words and contexts share the same embedding space. If the word embeddings exhibits some misleading properties, the context embeddings will also be problematic, and vice versa.



\citet{gao2019representation} and \citet{wang2020spectrum} have pointed out that, for language modeling, the maximum likelihood training with Equation~\ref{eq:softmax} usually produces an anisotropic word embedding space. ``Anisotropic'' means word embeddings occupy a narrow cone in the vector space.
This phenomenon is also observed in the pretrained Transformers like BERT, GPT-2, etc~\cite{ethayarajh2019contextual}. 

In addition, we have two empirical observations over the learned anisotropic embedding space.

\paragraph{Observation 1: Word Frequency Biases the Embedding Space} 
\label{sec:h1}


We expect the embedding-induced similarity to be consistent to semantic similarity. If embeddings are distributed in different regions according to frequency statistics, the induced similarity is not useful any more.

However, as discussed by \citet{gao2019representation}, anisotropy is highly relevant to the imbalance of word frequency. They prove that under some assumptions, the optimal embeddings of non-appeared tokens in Transformer language models can be extremely far away from the origin. They also try to roughly generalize this conclusion to rarely-appeared words. 


To verify this hypothesis in the context of BERT, we compute the mean  distance between the BERT word embeddings and the origin (i.e., the mean -norm). In the upper half of Table~\ref{tab:wordfreq}, we observe that high-frequency words are all close to the origin, while low-frequency words are far away from the origin.


This observation indicates that the word embeddings can be biased to word frequency. This coincides with the second term in Equation~\ref{eq:pmi}, the log density of words. 
Because word embeddings play a role of connecting the context embeddings during training, context embeddings might be misled by the word frequency information accordingly and its preserved semantic information can be corrupted.


\paragraph{Observation 2: Low-Frequency Words Disperse Sparsely} 
\label{sec:h2}
We observe that, in the learned anisotropic embedding space, high-frequency words concentrates densely and low-frequency words disperse sparsely. 

This observation is achieved by computing the mean  distance of word embeddings to their -nearest neighbors. In the lower half of Table~\ref{tab:wordfreq}, we observe that the embeddings of low-frequency words tends to be farther to their -NN neighbors compared to the embeddings of high-frequency words. This demonstrates that low-frequency words tends to disperse sparsely. 

Due to the sparsity, many ``holes'' could be formed around the low-frequency word embeddings in the embedding space, where the semantic meaning can be poorly defined. Note that BERT sentence embeddings are produced by averaging the context embeddings, which is a convexity-preserving operation. However, the holes violate the convexity of the embedding space. This is a common problem in the context of representation learining~\citep{rezende2018taming,li2019surprisingly,ghosh2019variational}. Therefore, the resulted sentence embeddings can locate in the poorly-defined areas, and the induced  similarity can be problematic.








 

\section{Proposed Method: BERT-flow}
To verify the hypotheses proposed in Section~\ref{sec:anisotropy}, and to circumvent the incompetence of the BERT sentence embeddings, we proposed a calibration method called BERT-flow in which we take advantage of \textit{an invertible mapping from the BERT embedding space to a standard Gaussian latent space}. The invertibility condition assures that the mutual information between the embedding space and the data examples does not change. 

\begin{figure}[!t]
	\centering
	\includegraphics[width=\columnwidth]{./res/flow.pdf}
	\vspace{-20pt}
	\caption{\label{fig:flow} An illustration of our proposed flow-based calibration over the original sentence embedding space of BERT. }
	\vspace{-10pt}
\end{figure}

\subsection{Motivation}
A standard Gaussian latent space may have favorable properties which can help with our problem. 

\paragraph{Connection to Observation 1} First, standard Gaussian satisfies isotropy. The probabilistic density in standard Gaussian distribution does not vary in terms of angle. If the  norm of samples from standard Gaussian are normalized to 1, these samples can be regarded as uniformly distributed over a unit sphere.

We can also understand the isotropy from a singular spectrum perspective. As discussed above, the anisotropy of the embedding space stems from the imbalance of word frequency. In the literature of traditional word embeddings, \citet{mu2017all} discovers that the dominating singular vectors can be highly correlated to word frequency, which misleads the embedding space. By fitting a mapping to an isotropic distribution, the singular spectrum of the embedding space can be flattened. In this way, the word frequency-related singular directions, which are the dominating ones, can be suppressed. 

\paragraph{Connection to Observation 2} Second, the probabilistic density of Gaussian is well defined over the entire real space. This means there are no ``hole'' areas, which are poorly defined in terms of probability. The helpfulness of Gaussian prior for mitigating the ``hole'' problem has been widely observed in existing literature of deep latent variable models~\citep{rezende2018taming,li2019surprisingly,ghosh2019variational}.









\subsection{Flow-based Generative Model}



We instantiate the invertible mapping with flows. A flow-based generative model~\cite{kobyzev2019normalizing} establishes an invertible transformation from the latent space  to the observed space . The generative story of the model is defined as

where  the prior distribution, and  is an invertible transformation. With the change-of-variables theorem, the probabilistic density function (PDF) of the observable  is given as, 


In our method, we learn a flow-based generative model
by maximizing the likelihood of generating BERT sentence embeddings from a standard Gaussian latent latent variable. In other words, the base distribution  is a standard Gaussian and we consider the extracted BERT sentence embeddings as the observed space . We maximize the likelihood of 's marginal via Equation~\ref{eq:flow:mle} in a fully \emph{unsupervised} way.

Here  denotes the dataset, in other words, the collection of sentences. Note that during training, only the flow parameters are optimized while the BERT parameters remain unchanged. Eventually, we learn an invertible mapping function  which can transform each BERT sentence embedding  into a latent Gaussian representation  without loss of information.

The invertible mapping  is parameterized as a neural network, and the architectures are usually carefully designed to guarantee the invertibility~\cite{dinh2014nice}. Moreover, its determinant   should also be easy to compute so as to make the maximum likelihood training tractable. In our experiments, we follows the design of Glow~\cite{kingma2018glow}. The Glow model is composed of a stack of multiple invertible transformations, namely \textit{actnorm}, \textit{invertible  convolution}, and \textit{affine coupling layer}\footnote{For concrete mathamatical formulations, please refer to Table 1 of \citet{kingma2018glow}}.
We simplify the model by replacing affine coupling with additive coupling~\cite{dinh2014nice} to reduce model complexity, and replacing the invertible  convolution with random permutation to avoid numerical errors. For the mathematical formula of the flow model with additive coupling, please refer to Appendix~\ref{sec:appendix:additive:coupling}.














 \section{Experiments}

\begin{table*}[!h]
\begin{center}
\scalebox{0.8}{
\begin{tabular}{l llllllll}
\toprule



Dataset & STS-B & SICK-R & STS-12 & STS-13 & STS-14 & STS-15 & STS-16 \\
\midrule\midrule
\multicolumn{8}{c}{\emph{Published in \cite{reimers2019sentence}}}  \\
Avg. GloVe embeddings & 58.02 &	53.76 &	55.14 &	70.66 &	59.73 &	68.25 &	63.66 \\
Avg. BERT embeddings & 46.35 &	58.40 &	38.78 &	57.98 &	57.98 &	63.15 &	61.06 \\
BERT CLS-vector & 16.50 &	42.63 &	20.16 &	30.01 &	20.09 &	36.88 &	38.03 \\
\midrule\midrule
\multicolumn{8}{c}{\emph{Our Implementation}}  \\
BERT & 47.29	& 58.21	& 49.07	& 55.92	& 54.75	& 62.75	& 65.19 \\
BERT-last2avg & 59.04	& 63.75	& 57.84	& 61.95	& 62.48	& 70.95	& 69.81 \\
BERT-flow (NLI) & 58.56 ()	& \bf 65.44 ()	& 59.54	() & 64.69 ()	& 64.66 () & 72.92	() & 71.84 () \\
BERT-flow (target) & 70.72  ()	& 63.11() 	& 63.48 ()	& 72.14	 () & 68.42	 () & 73.77	 () & 75.37  () \\
\midrule
BERT & 46.99	& 53.74	& 46.89	& 53.32	& 49.27	& 56.54	& 61.63 \\
BERT-last2avg & 59.56	& 60.22	& 57.68	& 61.37	& 61.02	& 68.04	& 70.32 \\
BERT-flow (NLI) & 68.09 ()	& 64.62	() & 61.72 ()	& 66.05 ()	& 66.34	() & 74.87	() & 74.47 () \\
BERT-flow (target) & \bf 72.26 ()	& 62.50	() & \bf 65.20	() & \bf 73.39 ()	& \bf 69.42 ()	& \bf 74.92 ()	& \bf 77.63 () \\
\bottomrule
\end{tabular}
}
\caption{
Experimental results on semantic textual similarity \textbf{without} using NLI supervision.
We report the Spearman's rank correlation between the cosine similarity of sentence embeddings and the gold labels on multiple datasets. Numbers are reported as .  denotes outperformance over its BERT baseline and  denotes underperformance. Our proposed BERT-flow method achieves the best scores. Note that our BERT-flow use \textit{-last2avg} as default setting. : Use NLI corpus for the unsupervised training of flow; supervision labels of NLI are NOT visible.  }
\label{tbl:exp:no:nli}
\end{center}
\vspace{-20pt}
\end{table*}


To verify our hypotheses and demonstrate the effectiveness of our proposed method, in this section we present our experimental results for various tasks related to semantic textual similarity under multiple configurations. For the implementation details of our siamese BERT models and flow-based models, please refer to Appendix~\ref{sec:appendix:implementation}.



\subsection{Semantic Textual Similarity}

\paragraph{Datasets.}
We evaluate our approach extensively on the semantic textual similarity (STS) tasks. We report results on 7 datasets, namely the STS benchmark (STS-B)~\citep{cer2017semeval}
the SICK-Relatedness (SICK-R) dataset~\citep{marelli2014sick} and the
STS tasks 2012 - 2016~\citep{agirre2012semeval,agirre2013sem,agirre2014semeval,agirre2015semeval,agirre2016semeval}. We obtain all these datasets via the SentEval toolkit~\citep{conneau2018senteval}. These datasets provide a fine-grained gold standard semantic similarity between 0 and 5 for each sentence pair.



\paragraph{Evaluation Procedure. }
Following the procedure in previous work like Sentence-BERT~\cite{reimers2019sentence} for the STS task, the prediction of similarity consists of two steps: (1) first, we obtain sentence embeddings for each sentence with a sentence encoder, and  
(2) then, we compute the cosine similarity between the two embeddings of the input sentence pair as our model-predicted similarity. The reported numbers are the Spearman's correlation coefficients between the predicted similarity and gold standard similarity scores, which is the same way as in \citep{reimers2019sentence}. 


\paragraph{Experimental Details. }
We consider both BERT and BERT in our experiments. Specifically, we use an average pooling over BERT context embeddings in the last one or two layers as the sentence embedding which is found to outperform the [CLS] vector. Interestingly, our preliminary exploration shows that averaging the last two layers of BERT (denoted by \textit{-last2avg}) consistently produce better results compared to only averaging the last one layer. Therefore, we choose \textit{-last2avg} as our default configuration when assessing our own approach.

For the proposed method, the flow-based objective (Equation~\ref{eq:flow:mle}) is maximized only to update the invertible mapping while the BERT parameters remains unchanged. Our flow models are by default learned over the full target dataset (train + validation + test).
We denote this configuration as \textit{flow (target)}. Note that although we use the sentences of the entire target dataset, learning flow does not use any provided labels for training, thus it is a purely \emph{unsupervised} calibration over the BERT sentence embedding space. 


We also test our flow-based model learned on a concatenation of SNLI~\citep{snli} and MNLI~\citep{mnli} for comparison (\emph{flow (NLI)}). The concatenated NLI datasets comprise of tremendously more sentence pairs (SNLI 570K + MNLI 433K). Note that ``flow (NLI)'' does not require any supervision label. When fitting flow on NLI corpora, we only use the raw sentences instead of the entailment labels. An intuition behind the flow (NLI) setting is that, compared to Wikipedia sentences (on which BERT is pretrained), the raw sentences of both NLI and STS are simpler and shorter. This means the NLI-STS discrepancy could be relatively smaller than the Wikipedia-STS discrepancy.

We run the experiments on two settings: (1) when external labeled data is unavailable. This is the natural setting where we learn flow parameters with the unsupervised objective (Equation~\ref{eq:flow:mle}), meanwhile BERT parameters are unchanged. (2) we first fine-tune BERT on the SNLI+MNLI textual entailment classification task in a siamese fashion~\citep{reimers2019sentence}. For BERT-flow, we further learn the flow parameters. This setting is to compare with the state-of-the-art results which utilize NLI supervision~\citep{reimers2019sentence}. We denote the two different models as BERT-NLI and BERT-NLI-flow respectively.

\begin{table*}[ht]
\begin{center}
\scalebox{0.8}{
\begin{tabular}{l llllllll}
\toprule



Dataset & STS-B & SICK-R & STS-12 & STS-13 & STS-14 & STS-15 & STS-16 \\
\midrule\midrule
\multicolumn{8}{c}{\emph{Published in \cite{reimers2019sentence}}}  \\

InferSent - Glove & 68.03 &	65.65 &	52.86 &	66.75 &	62.15 &	72.77 &	66.86 \\
USE & 74.92 &	76.69 &	64.49 &	67.80 &	64.61 &	76.83 &	73.18 \\
SBERT-NLI & 77.03 &	72.91 &	70.97 &	76.53 &	73.19 &	79.09 &	74.30 \\
SBERT-NLI & 79.23 &	73.75 &	72.27 &	78.46 &	74.90 &	80.99 &	76.25 \\
SRoBERTa-NLI & 77.77 &	74.46 &	71.54 &	72.49 &	70.80 &	78.74 &	73.69 \\
SRoBERTa-NLI & 79.10 &	74.29 &	\bf 74.53 &	77.00 &	73.18 &	81.85 &	76.82 \\

\midrule\midrule
\multicolumn{8}{c}{\emph{Our Implementation}}  \\
BERT-NLI  & 77.08	& 72.62	& 66.23	& 70.22	& 72.15	& 77.35	& 73.91 \\
BERT-NLI-last2avg & 78.03	& 74.07	& 68.37	& 72.44	& 73.98	& 79.15	& 75.39 \\
BERT-NLI-flow (NLI)   & 79.10 ()	& \bf 78.03 ()	& 67.75 ()	& 76.73 ()	& 75.53 ()	& 80.63 ()	& 77.58 () \\
BERT-NLI-flow (target) & 81.03 ()	& 74.97 ()	& 68.95 ()	& 78.48 ()	& 77.62 ()	& 81.95 ()	& 78.94 () \\
\midrule
BERT-NLI & 77.80	& 73.44	& 66.87	& 73.91	& 74.04	& 79.14	& 75.35 \\
BERT-NLI-last2avg & 78.45	& 74.93	& 68.69	& 75.63	& 75.55	& 80.35	& 76.81 \\
BERT-NLI-flow (NLI) & 79.89  ()	& 77.73  ()	& 69.61  ()	& 79.45  ()	& 77.56  ()	& 82.48	  () & 79.36  () \\
BERT-NLI-flow (target) & \bf 81.18  ()	& 74.52 ()	&  70.19	 () & \bf 80.27  ()	& \bf 78.85  ()	& \bf 82.97  ()	& \bf 80.57  ()\\


\bottomrule
\end{tabular}
}
\caption{
Experimental results on semantic textual similarity \textit{with} NLI supervision. Note that our flows are still learned in a \textit{unsupervised} way. 
InferSent~\citep{conneau-EtAl:2017:EMNLP2017} is a siamese LSTM train on NLI, Universal Sentence Encoder (USE)~\citep{cer2018universal} replace the LSTM with a Transformer and SBERT~\cite{reimers2019sentence} further use BERT.
We report the Spearman's rank correlation between the cosine similarity of sentence embeddings and the gold labels on multiple datasets. Numbers are reported as .  denotes outperformance over its BERT baseline and  denotes underperformance. Our proposed BERT-flow (i.e., the ``BERT-NLI-flow'' in this table) method achieves the best scores. Note that our BERT-flow use \textit{-last2avg} as default setting. : Use NLI corpus for the unsupervised training of flow; supervision labels of NLI are NOT visible.}
\label{tbl:exp:nli}
\end{center}
\vspace{-10pt}
\end{table*}

\paragraph{Results w/o NLI Supervision. } 
As shown in Table~\ref{tbl:exp:no:nli}, the original BERT sentence embeddings (with both BERT and BERT) fail to outperform the averaged GloVe embeddings. 
And averaging the last-two layers of the BERT model can consistently improve the results. For BERT and BERT, our proposed flow-based method (BERT-flow (target)) can further boost the performance by 5.88 and 8.16 points on average respectively.
For most of the datasets, learning flows on the target datasets leads to larger performance improvement than on NLI. The only exception is SICK-R where training flows on NLI is better. We think this is because SICK-R is collected for both entailment and relatedness. 
Since SNLI and MNLI are also collected for textual entailment evaluation, the distribution discrepancy between SICK-R and NLI may be relatively small. Also due to the much larger size of the NLI datasets, it is not surprising that learning flows on NLI results in stronger performance.

\paragraph{Results w/ NLI Supervision.}
Table~\ref{tbl:exp:nli} shows the results with NLI supervisions. Similar to the fully unsupervised results before, our isotropic embedding space from invertible transformation is able to consistently improve the SBERT baselines in most cases, and outperforms the state-of-the-art SBERT/SRoBERTa results by a large margin. Robustness analysis with respect to random seeds are provided in Appendix~\ref{sec:appendix:results:with:different:seeds}.







\vspace{-5pt}
\subsection{Unsupervised Question-Answer Entailment}
\vspace{-5pt}
In addition to the semantic textual similarity tasks, we  examine the effectiveness of our method on unsupervised question-answer entailment. We use Question Natural Language Inference (QNLI,~\citet{wang2018glue}), a dataset comprising 110K question-answer pairs (with 5K+ for testing). QNLI extracts the questions as well as their corresponding context sentences from SQUAD~\citep{rajpurkar2016squad}, and annotates each pair as either \textit{entailment} or \textit{no entailment}. 
In this paper, we further adapt QNLI as an unsupervised task. The similarity between a question and an answer can be predicted by computing the cosine similarity of their sentence embeddings. Then we regard \textit{entailment} as 1 and \textit{no entailment} as 0, and evaluate the performance of the methods with AUC.



As shown in Table~\ref{tbl:qnli}, our method consistently improves the AUC on the validation set of QNLI. Also, learning flow on the target dataset can produce superior results compared to learning flows on NLI. 















\begin{table}[!h]
\centering
\scalebox{0.8}{
\begin{tabular}{ll}
\toprule
\thead{Method} & \thead{AUC} \\ 
\midrule
BERT-NLI-last2avg & 70.30\\
BERT-NLI-flow (NLI) & 72.52 ()  \\
BERT-NLI-flow (target) & 76.17 ()  \\
\midrule
BERT-NLI-last2avg & 70.41\\
BERT-NLI-flow (NLI)  & 74.19 ()  \\
BERT-NLI-flow (target) & \bf 77.09 ()  \\

\bottomrule
\end{tabular}
}
\caption{AUC on QNLI evaluated on the validation set. : Use NLI corpus for the unsupervised training of flow; supervision labels of NLI are NOT visible.}
\label{tbl:qnli}
\vspace{-10pt}
\end{table}



\subsection{Comparison with Other Embedding Calibration Baselines}
\begin{table}[!h]
\centering
\scalebox{0.8}{
\begin{tabular}{ll}
\toprule
Method & Correlation \\ 
\midrule
BERT & 47.29 \\
~~ + SN &  55.46  \\
~~ + NATSV () & 51.79  \\
~~ + NATSV () & 60.40  \\
~~ + SN + NATSV () & 56.02  \\
~~ + SN + NATSV () & 63.51  \\
BERT-flow (target)  & \bf 65.62  \\

\bottomrule
\end{tabular}
}
\caption{Comparing flow-based method with baselines on STS-B.  is selected among  on the validation set. We report the Spearmanâ€™s rank correlation ().}
\label{tbl:baselines}
\vspace{-10pt}
\end{table}

In the literature of traditional word embeddings, \citet{arora2016simple} and~\citet{mu2017all} also discover the anisotropy phenomenon of the embedding space, and they provide several methods to encourage isotropy:

\paragraph{Standard Normalization (SN).}
In this idea, we conduct a simple post-processing over the embeddings by computing the mean  and standard deviation  of the sentence embeddings 's, and normalizing the embeddings by .

\paragraph{Nulling Away Top- Singular Vectors (NATSV).} 
\citet{mu2017all} find out that sentence embeddings computed by averaging traditional word embeddings  tend to have a fast-decaying singular spectrum. They claim that, by nulling away the top- singular vectors, the anisotropy of the embeddings can be circumvented and better semantic similarity performance can be achieved.

We compare with these embedding calibration methods on STS-B dataset and the results are shown in Table~\ref{tbl:baselines}.
Standard normalization (SN) helps improve the performance but it falls behind nulling away top- singular vectors (NATSV). This means standard normalization cannot fundamentally eliminate the anisotropy. By combining the two methods, and carefully tuning  over the validation set, further improvements can be achieved. Nevertheless, our method still produces much better results. We argue that NATSV can help eliminate anisotropy but it may also discard some useful information contained in the nulled vectors. On the contrary, our method directly learns an invertible mapping to isotropic latent space without discarding any information. 




\begin{figure*}[!t]
	\centering
	\includegraphics[width=0.6\textwidth]{./res/sim_ed.pdf}
	\caption{\label{fig:sim_ed} A scatterplot of sentence pairs, where the horizontal axis represents similarity (either gold standard semantic similarity or embedding-induced similarity), the vertical axis represents edit distance. The sentence pairs with edit distance  are highlighted with {\color{green} green}, meanwhile the rest of the pairs are colored with {\color{blue} blue}. We can observed that lexically similar sentence pairs tends to be predicted to be similar by BERT embeddings, especially for the green pairs. Such correlation is less evident for gold standard labels or flow-induced embeddings. }
	\vspace{-10pt}
\end{figure*}



\subsection{Dicussion: Semantic Similarity Versus Lexical Similarity }
\label{sec:lexical:similarity}



In addition to semantic similarity, we further study lexical similarity induced by different sentence embeddings. Specifically, we use edit distance as the metric for lexical similarity between a pair of sentences, and focus on the correlations between the sentence similarity and edit distance. Concretely, we compute the cosine similarity in terms of BERT sentence embeddings as well as edit distance for each sentence pair. Within a dataset consisting of many sentence pairs, we compute the Spearman's correlation coefficient  between the similarities and the edit distances, as well as between similarities from different models. We perform experiment on the STS-B dataset and include the human annotated gold similarity into this analysis.

\paragraph{BERT-Induced Similarity Excessively Correlates with Lexical Similarity.}






\begin{table}[!t]
\centering
\scalebox{0.85}{
\begin{tabular}{lrr}
\toprule
Similarity & Edit distance &  Gold similarity \\
\midrule
Gold similarity & -24.61 & 100.00 \\
BERT-induce similarity & -50.49 & 59.30\\
Flow-induce similarity & -28.01 & 74.09 \\
\bottomrule
\end{tabular}
}
\caption{\label{tab:editdistance} Spearman's correlation  between various sentence similarities on the validation set of STS-B. We can observe that BERT-induced similarity is highly correlated to edit distance, while the correlation with edit distance is less evident for gold standard or flow-induced similarity. }
\end{table}



Table~\ref{tab:editdistance} shows that the correlation between BERT-induced similarity and edit distance is very strong (), considering that gold standard labels maintain a much smaller correlation with edit distance (\textbf{}). This phenomenon can also be observed in Figure~\ref{fig:sim_ed}. 
Especially, for sentence pairs with edit distance  (highlighted with {\color{green} green}), BERT-induced similarity is extremely correlated to edit distance. However, it is not evident that gold standard semantic similarity correlates with edit distance. In other words, it is often the case where the semantics of a sentence can be dramatically changed by modifying a single word. For example, the sentences ``I like this restaurant'' and ``I dislike this restaurant'' only differ by one word, but convey opposite semantic meaning. BERT embeddings may fail in such cases. Therefore, we argue that the lexical proximity of BERT sentence embeddings is excessive, and can spoil their induced semantic similarity. 

\paragraph{Flow-Induced Similarity Exhibits Lower Correlation with Lexical Similarity.}
By transforming the original BERT sentence embeddings into the learned isotropic latent space with flow, the embedding-induced similarity not only aligned better with the gold semantic semantic similarity, but also shows a lower correlation with lexical similarity, as presented in the last row of Table~\ref{tab:editdistance}. The phenomenon is especially evident for the examples with edit distance  (highlighted with {\color{green} green} in Figure~\ref{fig:sim_ed}).
This demonstrates that our proposed flow-based method can effectively suppress the excessive influence of lexical similarity over the embedding space.











 \section{Conclusion and Future Work}
In this paper, we investigate the deficiency of the BERT sentence embeddings on semantic textual similarity, and propose a flow-based calibration which can effectively improve the performance. In the future, we are looking forward to diving in representation learning with flow-based generative models from a broader perspective.
 
\section*{Acknowledgments}
The authors would like to thank Jiangtao Feng, Wenxian Shi, Yuxuan Song, and anonymous reviewers for their helpful comments and suggestion on this paper.

\newpage
\bibliographystyle{acl_natbib}
\bibliography{references}

\newpage
\appendix
\onecolumn
\section{Mathematical Formula of the Invertible Mapping}
\label{sec:appendix:additive:coupling}
Generally, flow-based model is a stacked sequence of many invertible transformation layers: . Specifically, in our approach, each transformation  is an additive coupling layer, which can be mathematically formulated as follows.



Here  can be parameterized with a deep neural network for the sake of expressiveness. 

Its inverse function  can be explicitly written as:


\section{Implementation Details}
\label{sec:appendix:implementation}

Throughout our experiment, we adopt the official Tensorflow code of BERT~\footnote{\url{https://github.com/google-research/bert}} as our codebase. Note that we clip the maximum sequence length to 64 to reduce the costing of GPU memory. For the NLI finetuning of siamese BERT, we folllow the settings in~\citep{reimers2019sentence} (epochs = 1,  learning rate = , and batch size = 16). Our results may vary from their published one. The authors mentioned in \url{https://github.com/UKPLab/sentence-transformers/issues/50} that this is a common phenonmenon and might be related the random seed. Note that their implementation relies on the Transformers repository of Huggingface\footnote{\url{https://github.com/huggingface/transformers}}. This may also lead to discrepancy between the specific numbers.








Our implementation of flows is adapted from both the official repository of GLOW\footnote{\url{https://github.com/openai/glow}} as well as the implementation fo the Tensor2tensor library\footnote{\url{https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/research/glow.py}}. The hyperparameters of our flow models are given in Table~\ref{tab:flow:hyperparameter}. On the target datasets, we learn the flow parameters for 1 epoch with learning rate . On NLI datasets, we learn the flow parameters for 0.15 epoch with learning rate . The optimizer that we use is Adam. 

In our preliminary experiments on STS-B, we tune the hyperparameters on the dev set of STS-B. Empirically, the performance does not vary much with regard to the architectural hyperparameters compared to the learning schedule. Afterwards, we do not tune the hyperparameters any more when working on the other datasets. Empirically, we find the hyperparameters of flow are not sensitive across the datasets.

\begin{table}[!h]
\centering
\begin{tabular}{ll}
\toprule
\bf Coupling architecture in  &  3-layer CNN with residual connection \\
\bf Coupling width & 32 \\
\bf \#levels & 2 \\
\bf Depth & 3 \\
\bottomrule
\end{tabular}
\caption{\label{tab:flow:hyperparameter} Flow hyperparameters.}
\end{table}






\newpage
\section{Results with Different Random Seeds}
\label{sec:appendix:results:with:different:seeds}
We perform 5 runs with different random seeds in the NLI-supervised setting on STS-B. Results with standard deviation and median are demonstrated in Table~\ref{tbl:results:with:different:seeds}. Although the variance of NLI finetuning is not negligible, our proposed flow-based method consistently leads to improvement.

\begin{table}[!h]
\centering
\scalebox{1.0}{
\begin{tabular}{ll}
\toprule
\bf Method & \bf Spearman's  \\ 
\midrule
BERT-NLI-large & 77.26  1.76 (median: 78.19) \\
BERT-NLI-large-last2avg & 78.07  1.50 (median: 78.68) \\
BERT-NLI-large-last2avg + flow-target & 81.10  0.55 (median: 81.35) \\
\bottomrule
\end{tabular}
}
\caption{Results with different random seeds.}
\label{tbl:results:with:different:seeds}
\end{table}

 \end{document}