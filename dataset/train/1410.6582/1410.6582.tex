




\subsection{Prototype Implementation}
\vspace{-0.05in}
We implement prototype systems of both variants.
To support automatic people detection, we implement the most popular face detection \cite{viola2004robust}
 and pedestrian detection \cite{leibe2005pedestrian} algorithm based on the library of OpenCV.
To generate portrait graph,
we adopt JSeg \cite{deng2001unsupervised} for image segmentation.
Leveraging the library of MPEG-7,
a 48-byte eigenfaces vector\cite{turk1991eigenfaces} is extracted as the property of a node labeled with face,
 and a 64-byte color histogram vector and a 20-byte texture vector (edge histogram with 4 blocks and 5 orientations)
 are extracted as the property for other nodes.
\ourprotocol is compatible with any other vector-based feature descriptors.
Obtaining the matching results, invisible people are removed from the photo by blurring and inpainting \cite{criminisi2004region},
 as shown in Fig.~\ref{fig:conceal}.
Except the image processing,
 all other building blocks are realized using Java, including portrait graph matching,
 LSH, random number agreement, vector scrambling and the messaging module.
The client side are developed as an app on Android system for case study.
A user starts this app by inputting his/her portrait profile via selfieing.




\vspace{-0.05in}
\subsection{Case Study and Experiment setup}
\label{sec:case}
\vspace{-0.05in}




To test the practicality and efficiency of \ourprotocol
 the evaluation is conducted in a crowded real-life scenario:
 a networking workshop with more than 50 attendees in a $200m^2$ meeting hall.
10 volunteers (4 female and 6 male) acted as invisible users and also photographers.
Within one day, the volunteers took photos freely and our system recorded the cost and photos.
After the experiment, we got 208 photos.
1326 pedestrians are detected which belong to 42 individuals (7
 female and 35 male), but only 412 faces are detected.
It implies that
a whole body detection and description (\eg, our graph model) is necessary.
We manually labeled all captured people as the ground truth for the following evaluations.




\paragraph{Experiment setting.}
In the experiments,
 we use three types of phones as clients: HTC G10 (1024Hz CPU and 768M RAM),
 HTC G23 (1536Hz CPU and 1G RAM) and HTC New One (1741Hz CPU and 2G RAM).
One laptop is used as the cloud: ThinkPad X1 with i7 2.7GHz CPU and 4GB
RAM.
Based on our extensive evaluation, to achieve the tradeoff between matching efficiency and accuracy,
 we set the parameter of the matching methods as $\xi_s=0.5$ and the parameters of LSH as $W=3$ and $m=128$ (the hashed vector is 128 bit).
For the random number generation, $N$ is set to 512, which provides sufficient protection for the random number agreement protocol.
The analysis of parameter setting is omitted due to space limitation.
The the following evaluation,
 we denote the implementation of the baseline system as \basic
 and the implementation of the advance system as \advanced.






\vspace{-0.05in}
\subsection{Matching Accuracy}
\vspace{-0.05in}
Here we investigate the most important metric,
the portrait matching accuracy of both variants, which determines the correctness of invisible people removal.


\begin{figure*}[ht]
\begin{minipage}[t]{0.28\linewidth}
\centering
\includegraphics[width=0.9\linewidth, clip,keepaspectratio]{people_self_sim.eps}
\caption{Portrait similarity variances.}
\label{fig:self-sim}
\end{minipage}
\vspace{-0.1in}
\hfill
\begin{minipage}[t]{0.28\linewidth}
\centering
\includegraphics[width=0.9\linewidth, clip,keepaspectratio]{false_p.eps}
\caption{FP and FN  in basic scheme}
\label{fig:false-basic}
\end{minipage}
\vspace{-0.1in}
\hfill
\begin{minipage}[t]{0.28\linewidth}
\centering
\includegraphics[width=0.9\linewidth, clip,keepaspectratio]{false_h.eps}
\caption{FP and FN in advanced scheme}
\label{fig:false-advance}
\end{minipage}
\vspace{-0.1in}
\end{figure*}




We start by examining the consistency and distinguishability of user's
portrait graph
 by self-similarity (similarity between the same entity's portrait graphs)
 and cross-similarity (similarity between different entities' portrait graphs).
In this evaluation, we remove the face property since it is highly distinctive
 but cannot always be obtained.
Figure.~\ref{fig:self-sim} presents the evaluation results using the dataset.
The upper blue line stands for mean self-similarity for each entity,
 and the lower red line is mean cross-similarity between this entity
 and all other entities.
We notice that, generally portrait graph has a good consistency, \ie,
high self-similarity and small variance.
And the obvious gap between self-similarity and cross-similarity
 shows a good distinguishability.
In fact, in most cases, portrait graph can provide accurate matching
 without face features,  which implies better privacy protection.


Then, we analyze the matching correctness by analyzing all possible combinations using the dataset.
A false negative (FN) happens when a user A is invisible, but not removed from the photo
due to a match score lower than a match threshold $\theta_s$.
A false positive (FP) happens when user A is invisible, but another visible user C is removed due to
 a higher match scores than both the threshold and A's score.
In \basic matching is conducted on portrait graph with plain feature vectors.
Fig.~\ref{fig:false-basic} illustrates the  percentage of
FN and FP changing with different threshold $\theta_s$.
By selecting the threshold $\theta_s=0.5$,
 \basic achieves $0.5\%$ false negative and $2.1\%$ false
 positive without using any face property.
With face property, the false negative decreases to about $0.1\%$ and
false positive is less than $1\%$.
In \advanced feature vectors are transformed by scrambling and LSH.
While the scrambling retains the accurate distance between vectors,
LSH could cause some accuracy loss.
Will the transformation reduce the matching accuracy?
With appropriate parameters $m=128$ and $W=3$,
\advanced achieves comparable accuracy with \basic, as shown in
Fig.~\ref{fig:false-advance}.
When  $\theta_s=0.5$,
 the false negative is about $0.7\%$ and the false positive is about
 $2.9\%$ without any face property.
So, both variants support accurate matching and our vector transformation achieves good portrait feature privacy
 protection with little accuracy loss.
Based on the evaluation, in the rest experiments,
 the threshold $\theta_s$ is set to 0.5.















\vspace{-0.1in}
\subsection{Micro Benchmark}
\vspace{-0.1in}



\paragraph{Communication cost.} In \basic each face node takes 48B and each other node takes 84B.
The size of the portrait graph depends on the node number $k$.
For most applications, $k \leq 10$ is sufficient,
so the communication cost for each portrait is 0.82KB.
In \advanced after encoding, each vector is hashed to 128 bits,
 which reduces the size of a portrait to 0.15KB.
Protocol \advanced requires extra communication for random number agreement,
 which is only about 0.19KB.
\ourprotocol
costs each participant less than 1KB data transmission to
enable portrait privacy protection.
The cost for a photographer depends on the people number in the captured photo,
 but in most cases (with less than 10 people in photo), less than 10KB overhead is incurred, which is much less than a photo.
In general, \ourprotocol achieves much smaller transmitted data size and better privacy protection
 than transmitting the image itself.






\paragraph{Computation cost.}
In \basic the computation cost is composed of portrait graph
generation on the client side, and portrait matching on the cloud.
\advanced costs extra computation for random number agreement and vector transformation by scrambling and LSH.
The runtime is only about 3 ms to transform ten 64-dimension feature vectors.
Table.~\ref{table:time} presents all the decomposed computing time.
It shows that,
 the major computation delay is caused by image processing.
For a participant, it only needs to be executed once for the profile setup;
for a photographer, it needs to be executed for every captured photo.
The runtime of portrait detection and segmentation depend on the
resolution and complexity of the photo,
but the detection and segmentation results are not sensitive to scaling.
Hence, in our prototype all images are scaled to about 240,000 pixels.
For the photographer, on average it takes about 0.4s to conduct face
and pedestrian detection.
Given a portrait image/subimage,
the processing time of segmentation and feature extraction is about 2.6s.
On average, there are 28.2 regions of each portrait.


\begin{table}[hp]
\vspace{-0.08in}
\caption{Microbenchmarks of Runtime (in second)}
\label{table:time}
\centering
{\footnotesize
\begin{tabular}{|l|c|c|c|}
\hline
\multicolumn{4}{|c|}{Client}\\
\hline
Segmentation & $0.5$ & $2.4$ & $8.1$\\
\hline
Extraction & $0.02$ & $0.25$ & $1.3$\\
\hline
Random-Gen & 0.012 & 0.014 & 0.017 \\
\hline
\multicolumn{4}{|c|}{Cloud}\\
\hline
Matching (basic) & $0.015$ & $0.037$ & $ 0.079$\\
\hline
Matching (advanced) & $0.006$ & $0.01$ & $ 0.039$\\
\hline
Random-Init &  1.31 & 0.9 & 1.57 \\
\hline
\end{tabular}}
\vspace{-0.1in}
\end{table}

Compared with the image processing, the runtime of graph generation and matching is nearly negligible.
On the client side, only extra 0.014s runtime is required for random number generation in
 \advanced. On the cloud side, the time needed to match a pair of portrait graph
 is only about 0.04s in \basic and decreases to 0.01s in \advanced due to the hashed feature vector.
The cloud also needs 0.9s to generate system parameters for random number generation.
The millisecond-level portrait graph transmission delay is negligible too.
So the total computation delays for both variants are about 3s on the client
 and 1s on the cloud, which results a 4s system computation delay.


Now we've learned the magnitude of the time cost for each component,
 the overall delay also depends on the number of co-located invisible neighbors.
With more active peers sending privacy requests, the matching cost will increase,
 but compared to the image processing cost, the matching cost on the cloud side is still quite small.
Besides, the power consumption caused by our protocol (second-level computation) is much smaller than that caused by photo capturing itself.






















\vspace{-0.05in}
\subsection{Case Evaluation}
\vspace{-0.05in}
We conduct the case based evaluation as
described in Section \ref{sec:case}.
When there are invisible users in the photo,
 the false negative rate was about $1.4\%$ and the false positive rate was $0.9\%$.
But when there are no invisible users in the photo,
 the false positive rate raises to $4.9\%$ due to the absent of any true match users,
 and the threshold 0.5 was not high enough to exclude all false match.
And the average time for successful invisible people removal is about 4 seconds.

\vspace{-0.05in}
\subsection{Compare with Alternative Solution}
\vspace{-0.05in}
For comparison purpose, we also realize private Euclidean distance computation
 using a partial homomorphic encryption (Paillier encryption)
 in the SMC manner (e.g. the method used in \cite{katz2008predicate}).
Using the same computer and test images, the Paillier-based method takes about 0.5s for feature vector encryption
 and 1.8s for portrait matching between a pair of feature vectors.
But with our approach, the transformation cost is negligible and the matching cost is only 0.01s.
Besides, our method requires no interaction during the matching process.
The comparison shows the a significant efficiency of our system.
