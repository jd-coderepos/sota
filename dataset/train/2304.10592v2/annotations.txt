[{'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'InfiMM-Eval', 'Metric': 'Overall score', 'Score': '10.43'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'InfiMM-Eval', 'Metric': 'Deductive', 'Score': '11.02'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'InfiMM-Eval', 'Metric': 'Abductive', 'Score': '13.28'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'InfiMM-Eval', 'Metric': 'Analogical', 'Score': '5.69'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'InfiMM-Eval', 'Metric': 'Params', 'Score': '8B'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering', 'Dataset': 'MM-Vet', 'Metric': 'GPT-4 score', 'Score': '24.4±0.4'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering', 'Dataset': 'MM-Vet', 'Metric': 'Params', 'Score': '14B'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering', 'Dataset': 'MM-Vet', 'Metric': 'GPT-4 score', 'Score': '22.1±0.1'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering', 'Dataset': 'MM-Vet', 'Metric': 'Params', 'Score': '8B'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering', 'Dataset': 'BenchLMM', 'Metric': 'GPT-3.5 score', 'Score': '34.93'}}]
