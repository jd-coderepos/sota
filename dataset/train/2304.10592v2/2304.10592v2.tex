





In the experiment, we aim to showcase the diverse and emergent capabilities of our MiniGPT-4 model through various qualitative examples. These abilities include generating detailed image descriptions, identifying amusing aspects within memes, providing food recipes from photos, writing poems for images, etc. Additionally, we present quantitative results on the task of image captioning. 









\begin{figure}[t]
\begin{minipage}{0.5\textwidth}
\centering
 \includegraphics[width=0.9\linewidth]
    {figs/fig5.pdf}   
\caption{Detailed description}
\label{fig:detailed}
\end{minipage} \hfill
\begin{minipage}{0.5\textwidth}
\centering
 \includegraphics[width=0.9\linewidth]
    {figs/fig3.pdf}   
\caption{Advertisement promotion}
\label{fig:ad}
\end{minipage} \hfill
\vspace{-10pt}
\end{figure}














\subsection{Uncovering emergent abilities with MiniGPT-4 through qualitative examples} 

MiniGPT-4 demonstrates many advanced abilities compared to traditional vision-language models. For example, it can describe images in detail and interpret the humorous aspects of a given meme. Here, we qualitatively compared our model to one of the leading vision-language models, BLIP-2 \citep{blip2}, with eight distinct examples, each highlighting a different ability.

An example in Fig.\ref{fig:detailed} demonstrates that MiniGPT-4 effectively identifies various elements within the image, such as busy city streets, clock towers, shops, restaurants, motorcycles, people, streetlights, and clouds. In contrast, BLIP-2 can only cover city streets, people, and motorcycles in its image caption generation. Another example presented in Fig.\ref{fig:ab_meme} shows that MiniGPT-4 successfully explains why the meme is humorous. It interprets that the lying dog is feeling the same way as many people do on Monday, which is often considered to be the most dreaded day of the week. In contrast, BLIP-2 only briefly describes the image content and fails to comprehend the amusing aspects of the image. 


We also showcase MiniGPT-4's other abilities by demonstrating other distinctive abilities. These include creating advertising promotions based on a given image (Fig.\ref{fig:ad}), retrieving factual information from a movie photograph (Fig.\ref{fig:movie}), generating a food recipe from a food image (Fig.\ref{fig:cook}), diagnosing plant diseases and suggesting treatment plans (Fig.\ref{fig:plant}), creating a website from a hand-written draft (Fig.\ref{fig:ab_website}), and writing poems inspired by an image (Fig.\ref{fig:poem}). These abilities are absent in traditional vision-language models like BLIP-2 (utilizing Flan-T5 XXL~\citep{flanT5} as a language model), which use less powerful language models (LLMs). This contrast indicates that those advanced vision-language abilities only emerge when the visual features are properly aligned with an advanced LLM such as Vicuna \citep{vicuna2023}. 


\begin{figure}[t]
  \begin{subfigure}{0.55\textwidth}
    \centering
    \includegraphics[width=0.9\linewidth]{figs/rebuttal_meme.pdf}
    \caption{Meme explaining}
    \label{fig:ab_meme}
  \end{subfigure}\hfill
  \begin{subfigure}{0.45\textwidth}
    \centering
    \includegraphics[width=0.9\linewidth]{figs/rebuttal_web.pdf}
    \caption{Website Creating}
    \label{fig:ab_website}
  \end{subfigure}
  \caption{Model generations from BLIP-2, BLIP-2 finetuned our second stage data (BLIP-2 FT), MiniGPT-4 finetuned with Local Narrative data in the second stage (MiniGPT-4 LocNa), MiniGPT-4 model without Q-Former (MiniGPT-4 No Q-Former), and MiniGPT-4.}
  \label{fig:overall_label}
  \vspace{-10pt}
\end{figure}







\begin{table}[b]
\vspace{2mm}
\centering
\caption{Quantitative results on advanced vision-language tasks. MiniGPT-4 shows strong performance and successfully responses to 65\% of the requests.}
\scalebox{0.9}{
\begin{tabular}{lccccc}
\toprule
 & Meme & Recipes & Ads & Poem & Avg. \\
\midrule
BLIP-2 & 0/25 & 4/25 & 1/25 & 0/25 & 5/100 \\
MiniGPT-4 & 8/25 & 18/25 & 19/25 & 20/25 & 65/100 \\ 
\bottomrule
\label{tab: quanti_adv}
\end{tabular}
}
\end{table}




\subsection{Quantitative analysis}

\begin{table}[b]
\begin{minipage}{0.45\textwidth}
\vspace{2mm}
\centering
\caption{COCO caption evaluation. We use ChatGPT to judge if the generated caption covers all the visual objects and relations in the ground-truth caption.}
\scalebox{0.75}{
\begin{tabular}{ccc}
\toprule
 & BLIP-2 & MiniGPT-4 \\
\midrule
Correctness & 1376/5000 & 3310/5000  \\
Percentage & 27.5\% & 66.2\% \\ 
\bottomrule
\label{human_evaluation}
\end{tabular}
}
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
\centering
\caption{Failure rates of detailed caption and poem generation tasks before and after second-stage finetuning. The finetuning stage significantly  reduces generation failures.}
\scalebox{0.75}{
\begin{tabular}{ccc}
\toprule
 Failure rate &   Detailed caption   & Poem\\
\midrule
 Before stage-2 & 35\% & 32\% \\ 
 After stage-2 & 2\% & 1\%  \\
\bottomrule
\end{tabular}
\label{exp:stage2ablation}
}
\end{minipage}
\end{table}




\begin{figure}[t]
\begin{minipage}{0.485\textwidth}
\centering
 \includegraphics[width=0.9\linewidth]
    {figs/ablation.pdf}   
\caption{MiniGPT-4 before second-stage finetuning fails to output completed texts. The generation is improved after the finetuning.}
\label{fig:secondstage}
\end{minipage} \hfill
\begin{minipage}{0.485\textwidth}
\centering
 \includegraphics[width=0.9\linewidth]
    {figs/failure.pdf}  
\caption{An example of MiniGPT-4's limitations. MiniGPT-4 hallucinates unexisting tablecloths and can't locate the windows correctly.}
\label{fig:Limitation}
\end{minipage} \hfill
\vspace{-10pt}
\end{figure}



\paragraph{Advanced Abilities}
To quantify performance on advanced vision-language tasks, we compiled a small evaluation dataset comprising 4 tasks: meme interpretation with the question ``Explain why this meme is funny.'', recipe generation with the question ``How should I make something like this?'', advertisement creation with the prompt ``Help me draft a professional advertisement for this.'', and poem composition with ``Can you craft a beautiful poem about this image?''.
In total, we collect 100 diverse images, with 25 images allocated to each task.
We asked human evaluators to determine whether the model generation satisfies the request.
We compared our results with BLIP-2 \citep{blip2} and present the findings in Tab.\ref{tab: quanti_adv}. In meme interpretation, poem writing, and advertisement creation, BLIP-2 largely struggles to fulfill any requests. For recipe generation, BLIP-2 succeeds in 4 out of 25 cases. In contrast, MiniGPT-4 manages to address the requests in recipes, advertisements, and poem generation in nearly 80\% of the instances. Furthermore, MiniGPT-4 correctly comprehends the challenging humor understanding in memes in 8 out of 25 cases.






\paragraph{Image Captioning} We evaluate the performance of MiniGPT-4 on the COCO caption benchmark and compare it with BLIP-2 \citep{blip2}. 
Our model's generated captions typically contain rich visual details. As such, conventional similarity-based image-caption evaluation metrics struggle to provide an accurate evaluation of our models. 
In this regard, we evaluate the performance by checking if the generated captions cover all the ground truth captions' information with the help of ChatGPT and details can be found in Appx.\ref{appx: caption_eval}. Results in Tab.\ref{human_evaluation} shows that MiniGPT-4 outperforms BLIP-2 in generating captions that are more closely aligned with the ground-truth visual objects and relationships. With a success rate of 66.2\%, MiniGPT-4 is considerably more accurate than BLIP-2, which achieves only 27.5\%. 
Further evaluation on traditional VQA tasks can be found in Appx.\ref{appx: vqa}.















\subsection{Analysis on the second-stage finetuning}

\paragraph{Effectiveness of the second-stage finetuning}

The utilization of only the model pretrained after the first pretraining stage may result in failures, such as the occurrence of repetitive words or sentences, fragmented sentences, or irrelevant content. However, these issues have been largely mitigated through the second-stage finetuning process. This can be observed in Fig.\ref{fig:secondstage}, where MiniGPT-4 generates incomplete captions before the second-stage finetuning. However, after the second-stage finetuning, MiniGPT-4 is capable of generating complete and fluent captions. In this section, we investigate the importance and effectiveness of the second-stage finetuning approach.

To quantify the impact of second-stage finetuning, we randomly sampled 100 images from the COCO test set and investigated the model performance on two tasks: detailed description generation and poem writing. The prompts used were ``\textit{Describe the image in detail.}'' and ``\textit{Can you write a beautiful poem about this image?}''. These tasks were performed by both the models before and after second-stage finetuning. We manually counted the number of failure generations for the model in each stage. The results are presented in Tab.\ref{exp:stage2ablation}. Prior to the second-stage finetuning, approximately 1/3 of the generated outputs failed to match ground truth captions or poems. In contrast, the model after second-stage fineuning has less than two failure cases out of the 100 test images for both tasks. These experimental results demonstrate that second-stage finetuning yields a significant improvement in the quality of generated outputs. A qualitative example of the model generation before and after the second-stage finetuning is shown in Fig.\ref{fig:secondstage}.




\paragraph{Can the original BLIP-2 benefit from the second-stage data?} 
In this study, we finetune BLIP-2 \citep{blip2} with our second-stage data in the same way as MiniGPT-4, and check if it can obtain similar advanced abilities as MiniGPT-4. The finetuned BLIP-2 is denoted as BLIP-2 FT. 
Note that MiniGPT-4 uses the same visual module as BLIP-2; while BLIP-2 uses FlanT5 XXL \citep{flanT5} as the language model, which is not as strong as the Vicuna \citep{vicuna2023} model used in our MiniGPT-4 model.
We rely on the same prompts to assess the advanced capabilities of our model. 
Qualitative results are shown in Fig.\ref{fig:overall_label}, \ref{fig:ab_cook}, and \ref{fig:ab_des}. We discover that BLIP-2 FT still generates short responses and fails to generalize to advanced tasks like meme explaining and website coding (Fig.\ref{fig:overall_label}). Our finding suggests that BLIP-2's relatively weaker language model FlanT5 XXL benefits less from such a small dataset, and highlights the effectiveness of a more advanced LLM in a VLM system.


\begin{table}[b]
\begin{minipage}{0.45\textwidth}
\centering
\caption{Ablation on architecture designs}
\scalebox{0.75}{
\begin{tabular}{lcc}
    \toprule
    \textbf{Model} & \textbf{AOK-VQA} & \textbf{GQA} \\
    \midrule
    MiniGPT-4 & 58.2 & 32.2 \\
    (a) MiniGPT-4 w/o Q-Former & 56.9 & 33.4 \\
    (b) MiniGPT-4 + 3 Layers & 49.7 & 31.0 \\
    (c) MiniGPT-4 + Finetune Q-Former & 52.1 & 28.0 \\
    \bottomrule
\end{tabular}
\label{tab: ablation}
}
\end{minipage} \hfill
\begin{minipage}{0.45\textwidth}
\centering
\caption{Hallucination Evaluation}
\scalebox{0.75}{
\begin{tabular}{lcc}
\toprule
                    & $\textbf{CHAIR}_i$  & \textbf{Avg. Length} \\
\midrule
Blip-2              & 1.3         & 6.5        \\
MiniGPT-4 (short)   & 7.2         & 28.8       \\
MiniGPT-4 (long)    & 9.6         & 175        \\
\bottomrule
\end{tabular}
\label{tab:hallu}
}
\end{minipage}
\end{table}


\paragraph{Second stage with Localized Narratives }
The dataset Localized Narratives \citep{pont2020connecting} is a detailed image description dataset where 
annotators describe images while simultaneously localizing the corresponding regions.
Here, we test the performance of our model by replacing our self-collected dataset in the second-stage with the Localized Narratives dataset. The model is denoted as MiniGPT-4 LocNa. Qualitative results in Fig.\ref{fig:overall_label}, \ref{fig:ab_cook}, and \ref{fig:ab_des} show that MiniGPT-4 LocNa can generate long image descriptions (Fig.\ref{fig:ab_des}). However, the generated outputs have lower quality with monotonous expressions. Besides, MiniGPT-4 LocNa does not generalize as well as the original MiniGPT-4 in other complex tasks like explaining why the meme is funny (Fig.\ref{fig:ab_meme}). The performance gap may be due to the monotonous and repeated image descriptions in Localized Narratives.







\subsection{Ablation on the architecture designs}
To further demonstrate the effectiveness of using one single linear layer to align visual features with LLM, we conduct experiments with different architecture designs, including (a) removing the Q-Former and directly mapping the VIT’s output to Vicuna’s embedding space (i.e., without Q-former), (b) using three linear layers instead of one layer, and (c) additionally finetuning the Q-Former in the vision module. 
All the variants are trained in the same way as the original design. Results on AOK-VQA \citep{schwenk2022okvqa} and GQA \citep{hudson2019gqa} datasets in Tab.\ref{tab: ablation} show that the variant (a) \textbf{MiniGPT-4 w/o Q-Former} has a similar performance to the original design. 
Qualitative results of this variant in Fig.\ref{fig:overall_label}, \ref{fig:ab_cook}, and \ref{fig:ab_des} also show similar advanced skills.
This reveals that the Q-Former from BLIP-2 doesn't plays a critical roles for advanced skills. 
Besides, both variants (b) \textbf{MiniGPT-4+ 3 Layers} and (c) \textbf{MiniGPT-4 + finetuning Q-Former}, perform slightly worse than the original MiniGPT-4. 
This indicates a single projection layer is sufficient to align the vision encoder and the large language model in our limited training data setting.
