\documentclass{article}
\usepackage{amsmath,amsthm,amsfonts,graphicx,amssymb}
\usepackage{url,xspace}
\usepackage{algorithm,algpseudocode}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\renewcommand{\algorithmicrequire}{{\textbf{Input:}}}
\renewcommand{\algorithmicensure}{{\textbf{Output:}}}
\usepackage{rotating}


\newcommand{\dbl}{\texttt{double} }
\newcommand{\Z}{{\mathbb{Z}} }
\newcommand{\U}[1]{{\ensuremath{\bf{U_#1}}}}\newcommand{\GO}[1]{\ensuremath{\mathcal{O}\left(#1\right)}\xspace}
\newcommand{\dhss}{\cite{Douglas:1994:gemmw}\xspace}
\newcommand{\hljjtt}{\cite{Huss-Lederman:1996:mai}\xspace}
\newcommand{\ip}{\texttt{IP}\xspace}
\newcommand{\ipl}{\texttt{OvL}\xspace}
\newcommand{\ipr}{\texttt{OvR}\xspace}
\newcommand{\acco}{\texttt{AcLR}\xspace}
\newcommand{\accl}{\texttt{AccL}\xspace}
\newcommand{\accr}{\texttt{AccR}\xspace}
\newcommand{\acc}{\texttt{Acc}\xspace}
\newcommand{\accrb}{\texttt{AcR}\xspace}
\newcommand{\IP}{\texttt{IP}\xspace}
\renewcommand{\geq}{\geqslant}
\renewcommand{\leq}{\leqslant}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}
\newtheorem{prop}{Proposition}


\usepackage{paralist,array,enumerate,multirow,float}\def\eg{\mbox{{e.g.}}}   \def\ie{\mbox{{i.e.}}}         \urldef\jgdemail\url{\{Brice.Boyer,Jean-Guillaume.Dumas\}@imag.fr}
\urldef\wzemail\url{w2zhou@uwaterloo.ca}
\urldef\cpemail\url{Clement.Pernet@imag.fr}


\begin{document}
\title{Memory efficient scheduling of Strassen-Winograd's matrix
  multiplication algorithm\footnote{\copyright ACM, 2009. This is the
    author's version of the work. It is posted here by permission of
    ACM for your personal use. Not for redistribution. The definitive
    version was published in ISSAC 2009.}}
\author{Brice Boyer\thanks{Laboratoire J. Kuntzmann, Universit\'e de
  Grenoble. 51, rue des Math\'ematiques, umr CNRS 5224, bp 53X, F38041
  Grenoble, France, \jgdemail}
\and Jean-Guillaume Dumas\footnotemark[1]
\and Cl\'ement Pernet\thanks{Laboratoire LIG, Universit\'e de
  Grenoble. umr CNRS, F38330 Montbonnot, France. \cpemail}
\and Wei Zhou\thanks{School of Computer Science, University of
  Waterloo, Waterloo, ON, N2B 3G1, Canada. \wzemail}
}
\maketitle

\begin{abstract}
We propose several new schedules for Strassen-Winograd's matrix multiplication algorithm, they reduce the extra memory allocation requirements by three different
means: by introducing a few
pre-additions, by overwriting the input matrices, or by using a
first recursive
level of classical multiplication.
In particular, we show two fully in-place schedules: one having the
same number of
operations, if the input matrices can be overwritten; the other one,
slightly increasing
the constant of the leading term of the complexity, if the input matrices are
read-only. 
Many of these schedules have been found by an implementation of an exhaustive
search algorithm based on a pebble game.
\end{abstract}




\noindent
 {\bf Keywords:} Matrix multiplication, Strassen-Winograd's algorithm, Memory placement.
\section{Introduction}
Strassen's algorithm~\cite{Strassen:1969:GENO} was the
first sub-cubic algorithm for matrix multiplication.
Its improvement by Winograd~\cite{Winograd:1971:tbtmm}
led to a highly practical algorithm.
The best asymptotic complexity for this computation has been
successively improved since then, down to 
in \cite{Coppersmith:1990:MMAP} (see
\cite{Bini:1994:PMC-FA,burgisser:1997} for a review), but
Strassen-Winograd's
still remains one of the most practicable.
Former studies on how to turn this algorithm into practice can be found
in~\cite{bailey:603,Huss-Lederman:1996:ISA, Huss-Lederman:1996:mai,
  Douglas:1994:gemmw}
and references therein for numerical computation and in
\cite{Pernet:2001:Winograd, Dumas:2002:FFLAS}
for computations over a finite field.\\
In this paper, we propose new schedules of the algorithm, that
reduce the
extra memory allocation, by three different means:
by introducing a few pre-additions, by overwriting the input matrices, or by using a first recursive level of classical multiplication.
These schedules can prove useful for instance for memory
efficient computations of the rank,
determinant, nullspace basis, system resolution, matrix inversion...
Indeed, the matrix multiplication based LQUP factorization of~\cite{Ibarra:1982:LSP} can be
computed with no other temporary allocations than the ones involved in
its block matrix
multiplications~\cite{JeannerodPernet:2007:LQUP}. Therefore the
improvements on
the memory requirements of the matrix multiplication, used together
for instance with cache optimization strategies~\cite{Bader:2006:comm}, will directly
improve these higher level computations.

We only consider here the computational complexity and space
complexity, counting the number of arithmetic operations and memory
allocations.
The focus here is neither on stability issues, nor really on speed
improvements. We rather study potential memory space savings.
Further studies have thus to be made to assess for some gains for
in-core computations or to use these schedules
for numerical computations.
They are nonetheless already useful for exact computations, for instance on
integer/rational or finite field
applications~\cite{Dumas:2004:FFPACK,Laderman:1992:PAA}. 

The remainder of this paper is organized as follows: we review Strassen-Winograd's algorithm and existing memory schedules
in sections \ref{sec:algo} and \ref{ssec:constinput}. We then present in
section \ref{sec:pebble} the dynamic program we used to search for
schedules. This allows us to give several schedules overwriting their inputs in
section \ref{sec:overwrite}, and then a new schedule for  using only two extra temporaries in section \ref{sec:hyb}, all
of them preserving the leading term of the arithmetic complexity. 
Finally, in section \ref{sec:mix}, we present a generic way of transforming non in-place matrix
multiplication algorithms into in-place ones (\ie{} without any extra
temporary space), with a small constant
factor overhead. Then we recapitulate in table \ref{tab:resume} the
different available schedules and give their respective features.
\section{Strassen-Winograd Algorithm}\label{sec:algo}
We first review Strassen-Winograd's algorithm, and setup the notations that
will be used throughout the paper.\\
Let  and  be powers of .
Let  and  be two matrices of dimension  and  and
let .
Consider the natural block decomposition:

where  and  respectively have dimensions  and .
Winograd's algorithm computes the  matrix  with the
following 22 block operations:\\
\begin{inparaenum}[\xspace]
\item 8 additions:

\item 7 recursive multiplications:
\item 7 final additions:

\item The result is the matrix:
{ }.
\end{inparaenum}

Figure~\ref{fig:winotask} illustrates the dependencies between these tasks.
\begin{figure}[htb]
\begin{center}
\includegraphics[width=\columnwidth]{wino_task}
\caption{Winograd's task dependency graph}
\label{fig:winotask}
\end{center}
\end{figure}
\section{Existing memory placements}\label{ssec:constinput}
Unlike the classic multiplication algorithm,
Winograd's algorithm requires some extra temporary memory
allocations to perform its 22 block operations.\subsection{Standard product}\label{sub:exist:standard}
We first consider the basic operation . The best known
schedule for this case was given by
\cite{Douglas:1994:gemmw}. We reproduce a
similar schedule in table~\ref{tab:schedule:AB}.
\begin{table}[htb]
	\newcolumntype{s}{>{\!\!\scriptsize}l<{\!\!\!\!}}
	\newcolumntype{t}{>{\!\!\!\!\scriptsize}r<{\!\!}}
	\small
	\begin{center}
		\begin{tabular}{|slt|slt|}
			\hline
			\# & operation & loc. & \# & operation & loc.\\
			\hline
			1  & 	& 		& 12 & 	&  \\
			2  & 	& 		& 13 & 		& \\
			3  & 			& 	& 14 & 		&  \\
			4  & 	& 		& 15 & 		&  \\
			5  & 	& 		& 16 & 		&  \\
			6  & 			& 	& 17 & 		&  \\
			7  & 		& 		& 18 &  	& \\
			8  & 		& 		& 19 & 		& \\
			9  & 			& 	& 20 & 		&  \\
			10 & 		& 		& 21 & 	&   \\
			11 & 			&   & 22 & 		& \\
			\hline
		\end{tabular}
		\caption{Winograd's algorithm for operation , with two temporaries}
		\label{tab:schedule:AB}
	\end{center}
\end{table}
It requires two temporary blocks  and  whose dimensions are respectively equal to
 and .
Thus the extra memory used is:

Summing these temporary allocations over every recursive levels leads to a total amount of memory, where for brevity :

We can prove in the same manner the following lemma:
\begin{lem}\label{lem:sum}
	Let ,  and   be powers of two,  be homogeneous,  and  be a function such that 
	 
Then .
\end{lem}
In the remainder of the paper, we use  to denote the amount of extra memory used in table number~. The amount of extra memory we consider is always the sum up to the last recursion level.


Finally, assuming   gives a total extra memory requirement of

\subsection{Product with accumulation}
For the more general operation ,
a first na\"ive method would compute the product  using
the  scheduling of table~\ref{tab:schedule:AB}, into a temporary matrix 
and finally compute  . It would require  extra
memory allocations in the square case.\\
Now the schedule of table~\ref{tab:schedule:ABC} due to
~\cite[fig. 6]{Huss-Lederman:1996:mai} only  requires 3 temporary blocks for the
same number of operations ( multiplications and  additions).
\begin{table}[htb]
	\small
	\newcolumntype{s}{>{\!\!\scriptsize}l<{\!\!\!\!}}
	\newcolumntype{t}{>{\!\!\!\!\scriptsize}r<{\!\!}}
	\begin{center}
		\begin{tabular}{|slt|slt|}
			\hline
			\# & operation & loc. & \# & operation & loc.  \\
			\hline
			1 & 			& 		& 12 & 						&  \\
			2 & 			& 		& 13 & 						&  \\
			3 & 			& 		& 14 & 	&  \\
			4 & 	& 	& 15 & 					& \\
			5 & 	& 	& 16 & 	& \\
			6 & 			& 		& 17 & 					&    \\
			7 & 				& 		& 18 & 					&    \\
			8 & 	& 		& 19 & 				& \\
			9 & 	& 	& 20 & 						&  \\
			10& 	& 		& 21 & 						&  \\
			11&& 	& 22 & &\\
			\hline
		\end{tabular}
		\caption{Schedule for operation  with 3 temporaries}
		\label{tab:schedule:ABC}
	\end{center}
\end{table}
The required three temporary blocks  have dimensions ,  and . Since the two temporary blocks in schedule~\ref{tab:schedule:AB} 
are smaller than the three ones here, we have .
Hence, using lemma~\ref{lem:sum}, we get

With , this gives \\
We propose in table~\ref{tab:ABC:2tmp} a new schedule for the same operation
 only requiring two temporary blocks.\\
Our new schedule is more efficient if some inner calls overwrite their
temporary input matrices. We now present some overwriting schedules
and the dynamic program we used to find them.
\section{Exhaustive search algorithm}\label{sec:pebble}
We used a brute force search algorithm\footnote{The code is available
  at \url{http://ljk.imag.fr/CASYS/LOGICIELS/Galet}.} to get some of the new
schedules that will be presented in the following sections.
It is very similar to the pebble game of
Huss-Lederman et al.\ \cite{Huss-Lederman:1996:mai}.\\A sequence of computations is represented as a directed graph, just like figure~\ref{fig:winotask} is built from Winograd's algorithm.\\
A node represents a program variable. The nodes can be classified as initials (when they correspond to inputs), temporaries
(for intermediate computations) or finals (results or nodes that we want to
keep, such as ready-only inputs).\\
The edges represent the operations; they point from the operands to the result.\\
A pebble represents an allocated memory. We can put pebbles on any
nodes, move or remove them according to a set of simple rules shown below.\\
When a pebble arrives to a node, the computation at the associated
variable starts, and can be ``partially'' or ``fully''
executed. If not specified, it is assumed that the computation is
fully executed.\\
Edges can be removed, when the corresponding operation has been
computed.\\
The last two points are especially useful for accumulation
operations: for example, it is possible to try schedule the multiplication
separately from the addition in an otherwise recursive  call;
the edges involved in the multiplication operation would then be removed first and the accumulated part later.
They are also useful if we do not want to fix the way some additions are
performed: if  the associativity allows
different ways of computing the sum and we let the program explore
these possibilities.
At the beginning of the exploration, each initial node has a pebble
and we may have a few extra available pebbles. 
The program then tries to apply the following rules, in order, on each
node. The program stops when every final node has a pebble or when
no further moves of pebbles are possible:

\begin{inparaenum}[{\ \xspace\emph{Rule}} 1.]
\item[{\ \xspace\emph{Rule}} 0.] \emph{Computing a result/removing edges.} If a node
has a pebble and parents with pebbles, then the operation can be
performed and the corresponding edges removed. The node is
then at least partially computed.

\item \emph{Freeing some memory/removing a pebble.} If a node is
isolated and not final, its pebble is freed. This means that
we can reclaim the memory here because this node has been fully
computed (no edge pointing to it) and is no longer in use as an
operand (no edge initiating from~it).

\item \emph{Computing in place/moving a pebble.} If a node  has
a full pebble and a single empty child node  and if other
parents of  have pebbles on them, then the pebble on  may
be transferred  to  (corresponding edges are removed). This
means an operation has been made in place in the parent 's pebble.

\item \emph{Using more memory/adding a pebble.} If parents of an empty node  have
pebbles and a free pebble is available, then this pebble can be assigned to  and the
corresponding edges are removed. This means that the operation is
computed in a new memory location.

\item \emph{Copying some memory/duplicating a pebble.} 
A computed node having a pebble can be duplicated. The edges pointed
to or from the original node are then rearranged between them.
This means that a temporary result has been copied into some free
place to allow more flexibility.
\end{inparaenum}
\section{Overwriting input matrices}
\label{sec:overwrite}
We now relax some constraints on the previous problem: the input matrices 
and  can be overwritten, as proposed
by~\cite{Kreczmar:1976:Strassen}.
For the sake of simplicity, 
we first give schedules only working for square matrices (i.e. 
and any memory location is supposed to be able to receive any result
of any size).
We nevertheless give the memory requirements of
each schedule as a function of ;  and .
Therefore it is easier in the last part of this section to adapt the proposed schedules partially
for the general case. 
In the tables, the notation  (resp.  denotes the use of the algorithm from
table~\ref{tab:schedule:AB} (resp. table~\ref{tab:schedule:ABC}) as a
subroutine. Otherwise we use the notation  to
denote a recursive call or the use of one of our new schedules as a
subroutine.
\subsection{Standard product}
We propose in table~\ref{tab:AB:inplace} a new schedule
that computes the
product  without any temporary memory
allocation. The idea here is to find an ordering where the recursive
calls can be made also in place such that the operands of a
multiplication are no longer in use after the multiplication has completed because they are overwritten.
An exhaustive search showed that no schedule exists overwriting less
than four sub-blocks.
\begin{table}[htb]
	\newcolumntype{s}{>{\!\!\scriptsize}l<{\!\!\!\!}}
	\newcolumntype{t}{>{\!\!\!\!\scriptsize}r<{\!\!}}
	\small
	\begin{center}
		\begin{tabular}{|slt|slt|}
			\hline
			\# & operation & loc. & \# & operation & loc.  \\
			\hline
1  & 	&  & 12 & 			&  \\
			2  & 	&  & 13 & 			&  \\
			3  & 	&  & 14 & 				&  \\
			4  & 	&  & 15 & 	&  \\
			5  & )		&  & 16 & 				& \bf  \\
			6  & 		&  & 17 & 				&  \\ 
			7  & 	&  & 18 & 				&  \\
			8  & 		&  & 19 & 				& \bf  \\
			9  & 		&  & 20 & 				& \bf  \\
			10 & 		&  & 21 & 		&  \\
			11 & 	&  & 22 & 				& \bf  \\
			\hline
		\end{tabular}
		\caption{\IP schedule for operation  in place}
		\label{tab:AB:inplace}
	\end{center}
\end{table}
Note that this schedule uses only two blocks of  and the whole of 
but overwrites all of  and .
For instance the recursive computation of  requires overwriting parts of  and  too. 
Using another schedule as well as back-ups of overwritten parts into some available memory 
In the following, we will denote by \ip for {\texttt InPlace}, either one of
these two schedules.\\
We present in tables~\ref{tab:AB:ipleft} and
\ref{tab:AB:ipright} two new schedules overwriting only one of
the two input matrices, but requiring an extra temporary space.
These two schedules are denoted \ipl and \ipr.
The exhaustive search also showed that no schedule
exists overwriting only one of  and  and using no extra temporary.
\begin{table}[htb]
\small
	\newcolumntype{s}{>{\!\!\scriptsize}l<{\!\!\!\!}}
	\newcolumntype{t}{>{\!\!\!\!\scriptsize}r<{\!\!}}
	\begin{center}
		\begin{tabular}{|slt|slt|}
			\hline
			\# & operation & loc. & \# & operation & loc.  \\
			\hline
			1  & 		& 	& 12 & 			& 		\\
			2  & 		& 	& 13 & 				& 		\\
			3  & 			& 	& 14 & 				& 		\\
			4  & 		& 	& 15 & 				& 		\\
			5  & 	& 	& 16 & 				& 		\\
			6  & 		& 	& 17 & 				& \bf 	\\
			7  & 			& 		& 18 & 				& \bf 	\\	 
			8  & 			& 	& 19 & 		& 			\\
			9  & 			& 	& 20 & 				& \bf 	\\ 
			10 & 			& 	& 21 & 			& 		\\ 
			11 & 		& 	& 22 & 				& \bf   \\
			\hline
		\end{tabular}
		\caption{\ipl schedule for operation  using strictly two
		blocks of  and one temporary}
		\label{tab:AB:ipleft}
	\end{center}
\end{table}
\begin{table}[htb]
\small
	\newcolumntype{s}{>{\!\!\scriptsize}l<{\!\!\!\!}}
	\newcolumntype{t}{>{\!\!\!\!\scriptsize}r<{\!\!}}
	\begin{center}
		\begin{tabular}{|slt|slt|}
			\hline
			\# & operation & loc. & \# & operation & loc.  \\
			\hline
			1  & 		& 	& 12 & 		&  \\	
			2  & 		& 	& 13 & 				&  \\
			3  & 		& 	& 14 & 				&  \\
			4  & 	& 	& 15 & 				&   \\
			5  & 			& 	& 16 & 				&  \\
			6  & 		& 	& 17 & 				& \bf  \\
			7  & 			& 		& 18 & 				& \bf \\
			8  & 			& 	& 19 & 			&  \\
			9  & 			& 	& 20 & 				& \bf  \\  
			10 & 			& 	& 21 & 		&  \\
			11 & 			& 	& 22 & 				& \bf   \\  
			\hline
		\end{tabular}
		\caption{\ipr schedule for operation  using strictly two
		blocks of  and one temporary}
		\label{tab:AB:ipright}
	\end{center}
\end{table}
We note that we can overwrite only two blocks of  in \ipl when the schedule is modified as follows:
\begin{center}
	\newcolumntype{s}{>{\!\!\scriptsize}l<{\!\!\!\!}}
	\newcolumntype{t}{>{\!\!\!\!\scriptsize}r<{\!\!}}
\small
	\begin{tabular}{|slt|}
		\hline
		\# & operation & loc. \\
		\hline
		18bis	& 	& 	\\	
		\hline
		19bis	& 	& 	\\
		\hline
		21		& 			& 	\\ 
		\hline
	\end{tabular}
\end{center}
Similarly, for \ipr, we can overwrite only two blocks of  using copies on lines 20 and 21 and \ipl on line 19.\\
We now compute the extra memory needed for the schedule of table~\ref{tab:AB:ipright}.
The size of the temporary block  is , 
the extra memory required for table~\ref{tab:AB:ipright} hence satisfies:
.
\subsection{Product with accumulation}
We now consider the operation ,
where the input matrices  and  can be overwritten. We propose in table~\ref{tab:ABC:overwrite} a schedule that only requires  temporary block
matrices, instead of the  in table~\ref{tab:schedule:ABC}. This is
achieved by overwriting the inputs and by using two additional pre-additions ( and ) on the matrix .
\arraycolsep 0pt
\begin{table}[htb]
	\scriptsize
\newcolumntype{s}{>{\!\!\!\!\scriptsize}l<{\!\!\!\!\!\!}}
	\newcolumntype{t}{>{\!\!\!\!\!\!\scriptsize}r<{\!\!\!\!}}
\begin{center}
		\begin{tabular}{|slt|slt|}
			\hline
			\# & operation & loc. & \# & operation & loc.  \\
			\hline
			1  &             &   & 13 &        &  \\ 
			2  &             &        & 14 &                          &  \\  
			3  &             &        & 15 &                   &   \\
			4  &                &   & 16 &   &  \\
			5  &             &   & 17 &                             &  \\
			6  &             &   & 18 &                             &   \\
			5  &  &   & 17 &                             &  \\
			8  &                &   & 20 &                             &   \\
			9  &                &   & 21 &                             &  \\  
			10 & &   & 22 &                             &  \\
			11 &  & 		& 23 &                &  \\
			12 &                & 		& 24 &                             &  \\
			\hline
		\end{tabular}
		\caption{\acco schedule for  overwriting~~and~ with 2
		temporaries, 4 recursive calls}
		\label{tab:ABC:overwrite}
	\end{center}
\end{table}
We also propose in table~\ref{tab:ABC:overright} a schedule
similar to table~\ref{tab:ABC:overwrite}
overwriting only for instance the right input matrix. 
It also uses only two temporaries, but has to call the \ipr schedule.
The extra memory required by  and  in table~\ref{tab:ABC:overwrite} is
.
	Hence, using lemma~\ref{lem:sum}:
	
\begin{table}[htb]
\scriptsize
	\newcolumntype{s}{>{\!\!\!\!\scriptsize}l<{\!\!\!\!\!\!}}
	\newcolumntype{t}{>{\!\!\!\!\!\!\scriptsize}r<{\!\!\!\!}}
	\begin{center}
		\begin{tabular}{|slt|slt|}
			\hline
			\# & operation & loc. & \# & operation & loc.  \\
			\hline
			1  & 					& 	& 13 & 	& 	\\
			2  & 					&  		& 14 & 									& 		\\
			3  & 						& 	& 15 & 							& 	\\
			4  & 					& 	& 16 & 									& 		\\
			5  & 						&  		& 17 & 									&   \\
			6  & 	& 	& 18 & 									&   \\
			7  & 					& 		& 19 & 									& 	\\
			8  & 						& 	& 20 & 									&   \\
			9  &  & 	& 21 & 									&   \\
			10 & 						& 		& 22 & 									&   \\
			11 &  &  	& 23 & 					&   \\
			12 & 			&  		& 24 & 									&   \\
			\hline
		\end{tabular}
		\caption{\accr schedule for  overwriting  with 2
		temporaries, 4
		recursive calls}
		\label{tab:ABC:overright}
	\end{center}
\end{table}
The extra memory  required for table~\ref{tab:ABC:overright} in the top level of recursion is:

We clearly have  and:  

 Compared with the schedule of table~\ref{tab:schedule:ABC}, the
 possibility to
 overwrite the input matrices makes it possible to have further in place
 calls and replace recursive calls with accumulation by calls without
 accumulation. We show in theorem~\ref{thm:cost} that this enables us to
 almost compensate for the extra additions performed.

\subsection{The rectangular case}\label{ssec:overwrite:gen}
We now examine the sizes of the temporary locations used, when the
matrices involved do not have identical sizes. We want to make use of
table~\ref{tab:AB:inplace} for the general case.\\
Firstly, the sizes of  and  must not be bigger than that of  (\ie{} we need ). Indeed, let's play a pebble game that we start with pebbles on the inputs and   extra pebbles that are the size of a . No initial pebble can be moved since at least two edges initiate from the initial nodes. If the size of  is larger that the size of the free pebbles, then we cannot put a free pebble on the  nodes (they are too large). We cannot put either a pebble on  or  since their operands would be overwritten. So the size of  is smaller or equal than that of . The same reasoning applies for .\\ 
Then, if we consider a pebble game that was successful, we can prove in the same fashion that either the size of  or the size of  can not be smaller that of  (so one of them has the same size as ).\\
Finally, table~\ref{tab:AB:inplace} shows that this is indeed possible, with
. It is also possible  to switch the roles of  and . \\
Now in tables~4 to~7, we need that ,  and  have the same size.
Generalizing table~\ref{tab:AB:inplace} whenever we do not have a dedicated
in-place schedule can then done by cutting the larger matrices in squares of
dimension  and doing the multiplications / product with accumulations on
these smaller matrices using
algorithm~1 to~7 and free space from ,  or .Since algorithms~1 to~7 require less than  extra memory, we can use them as soon as one small matrix is free.\\
We now propose an example in algorithm \ref{alg:ipmm0} for the case :
\begin{algorithm}[htb] \begin{algorithmic}[1]
		\Require{ and  of resp. sizes  and
		} 
		\Require{ and , ,  powers of .}
		\Ensure{}
\State Let  and .
		\State Split ,  and 
		\Comment{
		\begin{minipage}{32mm}
			where  and  have dimension 
		\end{minipage}
		}
		\State  \Comment{with alg. of table~\ref{tab:schedule:AB} and memory .}
		\State Now we use   as temporary space.
		\For {}
		\State  \Comment{with alg. of table~\ref{tab:AB:ipleft}.} 
		\EndFor
		\For {}
		\For {} 
		\State  \Comment{
with alg. of table~\ref{tab:schedule:ABC}.
}
		\EndFor
		\EndFor
\end{algorithmic}
	\caption{\texttt{IP0vMM}: In-Place Overwrite Matrix Multiply}\label{alg:ipmm0}
\end{algorithm}\begin{prop}
	Algorithm~\ref{alg:ipmm0} computes the product  in place, overwriting  and .
\end{prop}Finally, we generalize the accumulation operation from
table~\ref{tab:ABC:overright} to the rectangular case. We can no
longer use dedicated square algorithms. This is done in
table~\ref{tab:ABC:overright:gen}, overwriting only one of the inputs
and using only two temporaries, but with 5 recursive accumulation calls: 
\begin{table}[htb]
	\scriptsize
\newcolumntype{s}{>{\!\!\!\!\scriptsize}l<{\!\!\!\!\!\!}}
	\newcolumntype{t}{>{\!\!\!\!\!\!\scriptsize}r<{\!\!\!\!}}
	\begin{center}
		\begin{tabular}{|slt|slt|}
			\hline
			\# & operation & loc. & \# & operation & loc.  \\
			\hline
			1  & 			& 	& 13 & 	& 	\\
			2  & 			&  		& 14 & 							&   \\
			3  & 				& 	& 15 & 							& 		\\
			4  & 			& 	& 16 & 				    & 		\\
			5  & 			&  		& 17 & 							& 		\\
			6  &  & 	& 18 & 							&   \\
			7  & 			& 		& 19 & 							&   \\
			8  & 				& 	& 20 & 							& 		\\
			9  & & 	& 21 & 							& 		\\ 
			10 & 				& 		& 22 & 					&   \\
			11 & &  	& 23 & 							&   \\ 
			12 & 		&  		& 24 & & \\
			\hline
		\end{tabular}
		\caption{\accrb schedule for  with 5
		recursive calls, 2 temporaries and overwriting }
		\label{tab:ABC:overright:gen}
	\end{center}
\end{table}

For instance, in table \ref{tab:ABC:overright:gen}, the last
multiplication (line 22,  ) could have been
made by a call to the in place algorithm, would  be large
enough. This is not always the case in a rectangular setting. 

Now, the size of the extra temporaries required in table
\ref{tab:ABC:overright:gen} is
 and  
is equal to: 

If  or , then : 

Otherwise  and:

In the square case, this simplifies into
\\

In addition, if the size of  is bigger than that of , then  one
can store , for instance within , and separate the
recursive call  into a multiplication and an addition, which
reduces the arithmetic complexity. Otherwise, a scheduling with only 4
recursive calls exists too, but we need for instance to recompute
 at step .


\section{Hybrid scheduling}\label{sec:hyb}
By combining techniques from sections~\ref{ssec:constinput} and
\ref{sec:overwrite}, we now propose in table~\ref{tab:ABC:2tmp} a hybrid algorithm that performs the
computation
 with constant input matrices  and
, with a lower extra memory requirement than the scheduling of
~\cite{Huss-Lederman:1996:mai} (table~\ref{tab:schedule:ABC}).
We have to pay a price of order  extra
operations, as we need to compute the temporary variable  twice.
\begin{table}[htb]
\newcolumntype{s}{>{\!\!\!\!\scriptsize}l<{\!\!\!\!\!\!}}
	\newcolumntype{t}{>{\!\!\!\!\!\!\scriptsize}r<{\!\!\!\!}}
\scriptsize
\begin{center}
\begin{tabular}{|slt|slt|}
\hline
\# & operation & loc. & \# & operation & loc.  \\
\hline
1  & 			& 	& 14 & 	& \\ 
2  & 			& 	& 15 & 							& \\
3  & 			& 		& 16 & 						&  \\ 
4  & 			& 		& 17 & 					& \\
5  & 	& 	& 18 & 					&  \\
6  & 				&  		& 19 & 						&  \\
7  & 				& 		&    & &   \\
8  & & 	& 20 & 						&     \\
9  & 				&  		& 21 & 					&  \\
10 & 				& 	& 22 & 						&  \\
11 & 	& 	& 23 & 						&  \\
12 & 		&    	& 24 & 						&      \\
13 & 					&   &	 &  & \\
\hline
\end{tabular}
\caption{\acc schedule for operation  with 2 temporaries}
\label{tab:ABC:2tmp}
\end{center}
\end{table}

Again, the two temporary blocks  and  have dimensions
 so that:

In all cases,  But  is not as large as
the size of the two temporaries in table~\ref{tab:ABC:overwrite}. We
therefore get:


Assuming , one gets   which is smaller than the extra memory requirement of table \ref{tab:schedule:ABC}.\\


\section{A sub-cubic in-place algorithm}\label{sec:mix}
Following the improvements of the previous section, the question was raised
whether extra memory allocation was intrinsic to sub-cubic matrix multiplication
algorithms. More precisely, is there a matrix multiplication algorithm computing
 in
 arithmetic operations without extra memory allocation and without
overwriting its input arguments? We show in this section that a combination of Winograd's algorithm and a classic block algorithm provides a positive answer.
Furthermore this algorithm also improves the extra memory requirement for the
product with accumulation .
\subsection{The algorithm}\label{sec:fully}
The key idea is to split the result matrix  into four quadrants of dimension
. The first
three quadrants  and  are computed using fast
rectangular matrix multiplication, which accounts for  standard
Winograd multiplications on blocks of dimension . The temporary
memory for these computations is stored in . Lastly, the block 
is  computed recursively up to a base case,
as shown on algorithm~\ref{alg:ipmm}. This base case, when the matrix
is too small to benefit from the fast routine, is then computed
with the classical matrix multiplication.
\begin{algorithm}[htb]
	\algblock{Do}{EndDo}
	\algrenewtext{Do}{\textbf{do}}
	\algrenewtext{EndDo}{\textbf{end do}}
	\begin{algorithmic}[1]
		\Require{ and , of dimensions resp.  and  with
		,   powers of 2 and .}
\Ensure{}
\State Split ,  and
			\Comment \begin{minipage}{3cm}
			where each  and  have dimension .
		\end{minipage}
		\Do \Comment{with alg. of table~\ref{tab:schedule:AB} using  as temp. space}
		\State 
		\State 
		\State 
		\EndDo
		\For {} \Comment{with alg. of table~\ref{tab:schedule:ABC} using  as temporary space:}
		\State 
		\State 
		\State  
		\EndFor
		\State  \Comment{recursively using \texttt{IPMM}.}
\end{algorithmic}
	\caption{\texttt{IPMM}: In-Place Matrix Multiply}\label{alg:ipmm}
\end{algorithm}
\begin{thm}
The complexity of algorithm~\ref{alg:ipmm} is:
 when .\end{thm}
\begin{proof}
Recall that  the cost of Winograd's algorithm for square matrices is   for the operation  and  for the operation .
The cost  of algorithm~\ref{alg:ipmm} is given by the relation
 the base case
being a classical dot product: .
Thus, .
\end{proof}
\begin{thm}
For any ,  and , algorithm~\ref{alg:ipmm} is in place.
\end{thm}
\begin{proof}
W.l.o.g, we assume that  (otherwise we could use the transpose).
	The exact amount of extra memory from algorithms in table~\ref{tab:schedule:AB} and~\ref{tab:schedule:ABC} is
	respectively given by eq.~(\ref{Eq:schedule:AB}) and~(\ref{Eq:schedule:ABC}).\\ 
If we cut  into  stripes at recursion level , then the sizes for the involved submatrices of  (resp. ) are  (reps. ). 
	The lower right corner submatrix of  that we would like to use as temporary space has a size .
	Thus we need to ensure that the following inequality holds:
	
	It is clear that  which simplifies the previous inequality.
	Let us now write ,  and . We need to find, for every  an integer  so that eq.~(\ref{Eq:aim}) holds. In other words, let us show that there exists some  such that, for any , the inequality  holds.
Then the fact that  provides at least one such .\\
	As the requirements in algorithm~\ref{alg:ipmm} ensure that  and , there just remains to prove that . Since  and  again , algorithm~\ref{alg:ipmm} is indeed in place.
\end{proof}
Hence a fully in-place  algorithm is obtained for matrix
multiplication.
The overhead of this approach appears in the multiplicative constant of
the leading term of the complexity, growing from  to .\\
This approach extends to the case of matrices with general
dimensions, using for instance peeling or padding techniques.\\
It is also useful if any sub-cubic algorithm is used
instead of Winograd's. For instance, in the square case, one can use the product with accumulation in table~\ref{tab:ABC:2tmp} instead of table~\ref{tab:schedule:ABC}.
\subsection{Reduced memory usage for the product with accumulation}\label{sec:red}
In the case of computing the product with accumulation, the matrix  can no longer be used as temporary storage, and
extra memory allocation cannot be avoided.
Again we can use the idea of the classical block matrix multiplication at the
higher level and call Winograd algorithm for the block multiplications.
As in the previous subsection,  can be divided into
four blocks and then the product can be made with 8 calls to Winograd algorithm for
the smaller blocks, with only one extra temporary block of  dimension .\\
More generally, for square  matrices,
 can be divided in  blocks of dimension .
Then one can compute each block with Winograd algorithm using only
one extra memory chunk of size . The complexity is changed to
 which is

for an accumulation product with
Winograd's algorithm.
Using the parameter , one can then balance the memory usage
and the extra arithmetic operations. For example, with ,
 and
with ,

Note that one can use the algorithm of table~\ref{tab:ABC:2tmp} instead of
the classical Winograd accumulation as the base case algorithm.
Then the memory overhead drops down to  and the
arithmetic complexity increases to
.
\section{Conclusion}
With constant input matrices, we reduced the number of extra memory
allocations for the operation 
from  to ,
by introducing two extra pre-additions. As shown below, the
overhead induced by these supplementary additions is amortized by
the gains in number of memory allocations.

If the input matrices can be overwritten, we proposed a fully \textit{in-place}
schedule for the operation  without any extra operations.
We also proposed variants for the operation , where only
one of the input matrices is being overwritten and one temporary is
required.
These subroutines allow us to reduce the extra memory allocations
required for the  operation without overwrite: the extra required
temporary space drops from  to only , at a negligible cost.

Some algorithms with an even more reduced memory usage, but with some increase in arithmetic
complexity, are also shown.
Table~\ref{tab:resume} gives a summary of the features of each schedule that has been
presented.
The complexities are given only for  being a power of .


\begin{sidewaystable}[!htbp]
\small
\begin{center}
	\newcolumntype{r}{>{\centering}m{1.7cm}}
	\newcolumntype{s}{>{\centering}m{30pt}}
	\newcolumntype{t}{>{\centering}m{3.3cm}}
\begin{tabular}{|c|c|c|r|s|t|c|}
\hline
& Algorithm & Input matrices    &\hspace{-10pt} \# of extra temporaries \hspace{-10pt}&total extra memory& total \# of extra allocations & arithmetic complexity\\
\hline
& & & & & &\1 ex]
\hline 
& & & & & &\1 ex]
\hline
\end{tabular}
\caption{Complexities of the schedules presented for square matrix multiplication}
\label{tab:resume}
\end{center}
\end{sidewaystable}
\begin{thm}\label{thm:cost}
The arithmetic and memory complexities of table~\ref{tab:resume} are
correct.
\end{thm}
\begin{proof}
For the operation , the arithmetic complexity of the schedule of
table~\ref{tab:schedule:AB} classically satisfies

so that .

The schedule of table~\ref{tab:schedule:AB} requires

extra memory space, which is . Its total
number of allocations satisfies

which is .

The schedule of table~\ref{tab:AB:ipleft} requires

extra memory space, which is . Its
total number of allocations satisfies

which is .

The schedule of table~\ref{tab:AB:ipright} requires the same amount of
arithmetic operations or memory.

For , the arithmetic complexity of \hljjtt satisfies

hence ; its memory overhead satisfies

which is ; its total number of allocations satisfies

which is


The arithmetic complexity of the schedule of table~\ref{tab:ABC:overwrite} satisfies
 so that
;
its number of extra memory satisfies
 which is ;
its total number of allocations satisfies
 which is
.

The arithmetic complexity of table~\ref{tab:ABC:overright} schedule satisfies
 so that
;
its number of extra memory satisfies
 which is ;
its total number of allocations satisfies
 which is
.

The arithmetic complexity of the schedule of table ~\ref{tab:ABC:2tmp} satisfies
 so that
;
its number of extra memory satisfies

which is ;
its total number of allocations satisfies

which is .
\end{proof}
For instance, by adding up allocations and arithmetic operations in table~\ref{tab:resume},
one sees that the overhead in arithmetic operations of the schedule of table~\ref{tab:ABC:2tmp}
is somehow amortized by the decrease of memory allocations. Thus it makes it
theoretically competitive with the algorithm
of~\cite{Huss-Lederman:1996:mai} as soon as .



Also, problems with dimensions that are not powers of two can be handled 
by combining the cuttings of algorithms \ref{alg:ipmm0} and
\ref{alg:ipmm} with peeling or padding techniques. Moreover,
some cut-off can be set in order to stop the recursion and switch
to the classical algorithm. The use of these cut-offs
will in general decrease both the extra memory requirements and the arithmetic
complexity overhead.

For instance we show on table \ref{tab:ipmmperf} the relative speed of
different multiplication procedures for some double floating point 
rectangular matrices. We use atlas-3.9.4 for the BLAS and a cut-off
of 1024.
We see that pour new schedules perform quite competitively with the
previous ones and that the savings in memory enable larger
computations (MT for memory thrashing).
\begin{table}[htbp]\center
\begin{tabular}{|c||r|r|r|r|}
\hline
Dims.  & Classic & \dhss{} & \texttt{IPMM} & \texttt{IP0vMM} \\
\hline
(4096,4096,4096)  & 14.03  & 11.93   & 13.59  & 11.98\\
(4096,8192,4096)  & 28.29  & 23.39   & 27.16  & 23.88  \\
(8192,8192,8192)  & 113.07 & 85.97   & 98.75  & 85.02\\
(8192,16384,8192) & 231.86 & MT      & 197.24 & 170.72\\
\hline
\end{tabular}
\caption{Rectangular matrix multiplication: computation time in seconds  on a core2 duo,
  3.00GHz, 2Gb RAM}\label{tab:ipmmperf}
\end{table}



\newcommand{\SortNoop}[1]{}


\begin{thebibliography}{10}

\bibitem{Bader:2006:comm}
M.~Bader and C.~Zenger.
\newblock Cache oblivious matrix multiplication using an element ordering based
  on a {Peano} curve.
\newblock {\em Linear Algebra and its Applications}, 417(2--3):301--313, Sept.
  2006.

\bibitem{bailey:603}
D.~H. Bailey.
\newblock Extra high speed matrix multiplication on the {C}ray-.
\newblock {\em SIAM Journal on Scientific and Statistical Computing},
  9(3):603--607, 1988.

\bibitem{Bini:1994:PMC-FA}
D.~Bini and V.~Pan.
\newblock {\em Polynomial and Matrix Computations, Volume 1: Fundamental
  Algorithms.}
\newblock Birkhauser, Boston, 1994.

\bibitem{burgisser:1997}
M.~Clausen, P.~{B\"urgisser}, and M.~A. Shokrollahi.
\newblock {\em Algebraic Complexity Theory}.
\newblock Springer, 1997.

\bibitem{Coppersmith:1990:MMAP}
D.~Coppersmith and S.~Winograd.
\newblock Matrix multiplication via arithmetic progressions.
\newblock {\em Journal of Symbolic Computation}, 9(3):251--280, 1990.

\bibitem{Douglas:1994:gemmw}
C.~C. Douglas, M.~Heroux, G.~Slishman, and R.~M. Smith.
\newblock {GEMMW}: A portable level~3 {BLAS} {Winograd} variant of {Strassen}'s
  matrix-matrix multiply algorithm.
\newblock {\em Journal of Computational Physics}, 110:1--10, 1994.

\bibitem{Dumas:2002:FFLAS}
J.-G. Dumas, T.~Gautier, and C.~Pernet.
\newblock Finite field linear algebra subroutines.
\newblock In T.~Mora, editor, {\em {ISSAC}'2002}, pages 63--74. ACM Press, New
  York, July 2002.

\bibitem{Dumas:2004:FFPACK}
J.-G. Dumas, P.~Giorgi, and C.~Pernet.
\newblock {FFPACK}: Finite field linear algebra package.
\newblock In J.~Gutierrez, editor, {\em {ISSAC}'2004}, pages 119--126. ACM
  Press, New York, July 2004.

\bibitem{Huss-Lederman:1996:ISA}
S.~Huss-Lederman, E.~M. Jacobson, J.~R. Johnson, A.~Tsao, and T.~Turnbull.
\newblock Implementation of {Strassen}'s algorithm for matrix multiplication.
\newblock In {ACM}, editor, {\em Supercomputing '96 Conference Proceedings:
  November 17--22, Pittsburgh, {PA}}. ACM Press and IEEE Computer Society
  Press, 1996.
\newblock \url{www.supercomp.org/sc96/proceedings/SC96PROC/JACOBSON/}.

\bibitem{Huss-Lederman:1996:mai}
S.~Huss-Lederman, E.~M. Jacobson, J.~R. Johnson, A.~Tsao, and T.~Turnbull.
\newblock {Strassen}'s algorithm for matrix multiplication~: Modeling analysis,
  and implementation.
\newblock Technical report, Center for Computing Sciences, Nov. 1996.
\newblock CCS-TR-96-17.

\bibitem{Ibarra:1982:LSP}
O.~H. Ibarra, S.~Moran, and R.~Hui.
\newblock A generalization of the fast {LUP} matrix decomposition algorithm and
  applications.
\newblock {\em Journal of Algorithms}, 3(1):45--56, Mar. 1982.

\bibitem{JeannerodPernet:2007:LQUP}
C.-P. Jeannerod, C.~Pernet, and A.~Storjohann.
\newblock Fast {Gaussian} elimination and the {PLUQ} decomposition.
\newblock Technical report, 2007.

\bibitem{Kreczmar:1976:Strassen}
A.~Kreczmar.
\newblock On memory requirements of {Strassen}'s algorithms.
\newblock In A.~Mazurkiewicz, editor, {\em Proceedings of the 5th Symposium on
  Mathematical Foundations of Computer Science}, volume~45 of {\em LNCS}, pages
  404--407, Gda{\'n}sk, Poland, Sept. 1976. Springer.

\bibitem{Laderman:1992:PAA}
J.~Laderman, V.~Pan, and X.-H. Sha.
\newblock On practical algorithms for accelerated matrix multiplication.
\newblock {\em Linear Algebra and its Applications}, 162--164:557--588, 1992.

\bibitem{Pernet:2001:Winograd}
C.~Pernet.
\newblock Implementation of {Winograd}'s fast matrix multiplication over finite
  fields using {ATLAS} level 3 {BLAS}.
\newblock Technical report, Laboratoire Informatique et Distribution, July
  2001.
\newblock {\small \url{ljk.imag.fr/membres/Jean-Guillaume.Dumas/FFLAS}}

\bibitem{Strassen:1969:GENO}
V.~Strassen.
\newblock {Gaussian} elimination is not optimal.
\newblock {\em Numerische Mathematik}, 13:354--356, 1969.

\bibitem{Winograd:1971:tbtmm}
S.~Winograd.
\newblock On multiplication of 2x2 matrices.
\newblock {\em Linear Algebra and Application}, 4:381--388, 1971.

\end{thebibliography}

\end{document}
