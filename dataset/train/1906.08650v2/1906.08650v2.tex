\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{diagbox}
\usepackage{cite}
\usepackage{multirow}
\usepackage{enumitem}


\usepackage{color}



\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{5052} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\newcommand{\BG}[1]{}
\newcommand{\mysection}[1]{\vspace{3pt}\noindent\textbf{#1.}}

\ificcvfinal\pagestyle{empty}\fi

\newcommand{\boldparagraph}[1]{\vspace{0.5em}\noindent{\bf #1} }

\newcommand\todo[1]{\textcolor{red}{TODO: {#1}}}
\newcommand{\red}[1]{\noindent{\color{red}{#1}}}


\newcommand{\side}[1]{ \rotatebox{90}{#1} }



\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\dist}{dist}
\DeclareMathOperator*{\prox}{prox}






\newcommand{\nR}{\mathbb{R}}                \newcommand{\nRplus}{\nR_{\geq 0}}          \newcommand{\nZ}{\mathbb{Z}}                \newcommand{\nI}{\mathbb{I}}                






\newcommand{\nLoss}{\mathcal{L}}            \newcommand{\nx}{\mathbf{x}}                \newcommand{\nv}{\mathbf{v}}                \newcommand{\nz}{\mathbf{z}}                \newcommand{\nc}{\boldsymbol{\mu}}           
\begin{document}

\title{3D Instance Segmentation via Multi-Task Metric Learning}

\author{Jean Lahoud \\
KAUST \\
\and
Bernard Ghanem\\
KAUST\\
\and
Marc Pollefeys \\
ETH Zurich\\
\and
Martin R. Oswald \\
ETH Zurich\\
}




\maketitle
\thispagestyle{empty}


\begin{abstract}
We propose a novel method for instance label segmentation of dense 3D voxel grids\footnote{\url{https://sites.google.com/view/3d-instance-mtml}}.
We target volumetric scene representations, which have been acquired with depth sensors or multi-view stereo methods and which have been processed with semantic 3D reconstruction or scene completion methods. The main task is to learn shape information about individual object instances in order to accurately separate them, including connected and incompletely scanned objects.
We solve the 3D instance-labeling problem with a multi-task learning strategy.
The first goal is to learn an abstract feature embedding, which groups voxels with the same instance label close to each other while separating clusters with different instance labels from each other.
The second goal is to learn instance information by densely estimating directional information of the instance's center of mass for each voxel.
This is particularly useful to find instance boundaries in the clustering post-processing step, as well as, for scoring the segmentation quality for the first goal.
Both synthetic and real-world experiments demonstrate the viability and merits of our approach.
In fact, it achieves state-of-the-art performance on the ScanNet 3D instance segmentation benchmark \cite{Dai-et-al-CVPR-2017}. 
\end{abstract}
 



\section{Introduction}
A central goal of computer vision research is high-level scene understanding.
Recent methodological progress for 2D images makes reliable results possible for a variety of computer vision problems, including image classification \cite{Krizhevsky-et-al-NIPS-2012,Simonyan-Zisserman-ICLR-2015,Szegedy-et-al-CVPR-2015}, image segmentation \cite{Long-et-al-CVPR-2015,Ronneberger-et-al-MICCAI-2015,Badrinarayanan-et-al-TPAMI-2017}, object detection \cite{Ren-et-al-NIPS-2015,Lin-et-al-CVPR-2017,Redmon-et-al-CVPR-2016} and instance segmentation in 2D images
\cite{Dai-et-al-CVPR-2016,He-et-al-ICCV-2017,Pinheiro-et-al-NIPS-2015}.
Furthermore, it is now possible to recover highly-detailed 3D geometry with low-cost depth sensors \cite{Zach-et-al-ICCV-2007,Izadi-et-al-SIGGRAPH-2011,Niessner-et-al-SIGGRAPH-2013,Steinbruecker-et-al-ICCV-2013} or with image-based 3D reconstruction algorithms \cite{Kolev-et-al-IJCV-2009,Furukawa-Ponce-TPAMI-2010,Schoenberger-et-al-ECCV-2016}. 
Combining both these concepts, many algorithms have been developed for 3D scene and object classification \cite{Socher-et-al-NIPS-2012,Wu-et-al-CVPR-2015,Maturana-Scherer-ICIRS-2015}, 3D object detection \cite{Yang-et-al-CVPR-2018,lahoud20172d}, and joint 3D reconstruction and semantic labeling \cite{Kundu-et-al-ECCV-2014,Tateno-et-al-CVPR-2017,Dai-et-al-CVPR-2018,Dai-Niessner-ECCV-2018,Cherabier-et-al-ECCV-2018}.


\begin{figure}[t]
  \small
\centering
\newcommand{\sz}{0.4}
  \begin{tabular}{ccc}
\includegraphics[trim={1cm 1.5cm 0 0},width=0.43\columnwidth]{scene0549_01_color.jpg} & 
    \includegraphics[trim={1cm 1.5cm 0 0},width=0.46\columnwidth]{scene0549_01_ours.jpg} \\
Input Scene & Our Instance Labels \\
\includegraphics[trim={20.8cm 9cm 2cm 1cm},clip,angle=90,width=0.45\columnwidth]{scene0549_01_gt.jpg} & 
    \includegraphics[trim={20.8cm 9cm 2cm 1cm},clip,angle=90,width=0.45\columnwidth]{scene0549_01_ours.jpg} \\
Ground truth Instance Labels  & Our Instance Labels \\
  \end{tabular}
  \vspace{0.1cm}
  \caption{\textbf{Sample results of our method.} Our proposed method takes as input a 3D point cloud, and outputs instance labels unique to each object within the scene. The labels are generated by learning a metric that groups parts of the same object instance and estimates the direction towards the instance's center of mass.}\label{fig:teaser}
\end{figure}


Advances in 2D instance segmentation were mainly fueled by the large number of datasets and challenges available in the 2D realm. When compared to the plethora of powerful methods for instance segmentation of 2D images, the 3D counterpart problem has been less explored in the literature. In addition to the lack of datasets, the majority of 2D methods are not applicable to the 3D setting or their extension is by no means straightforward.

With the emergence of labeled datasets and benchmarks for the task of 3D instance segmentation (e.g. ScanNet \cite{Dai-et-al-CVPR-2017}), numerous works have surfaced to tackle this task. In many cases, the work in 3D benefits from pioneering work in 2D, with modifications that allow processing of 3D input data. As such, this 3D processing tends to be similar to other 3D understanding techniques, mainly semantic segmentation.

In this paper, we address the problem of 3D instance segmentation. Given the 3D geometry of a scene, we want to label all the geometry that belongs to the same object with a unique label. 
Unlike previous methods that entangle instance labeling with semantic labeling, we propose a technique that mainly focuses on instance labeling through grouping/clustering of information pertaining to a single object.
Our method still benefits from semantic information as a local cue, but adds to it information related to 3D dimensions and 3D connectivity, whose usefulness is unique to the 3D setting.

In particular, we propose a learning algorithm that processes a 3D voxel grid and learns two main characteristics: (1) a feature descriptor unique to every instance, and (2) a direction that would point towards the instance center. Our method aims to provide a grouping force that is independent of the size of the scene and the number of instances within. 


\mysection{Contributions} Our contributions are two fold. \emph{(i)} We propose a multi-task neural network architecture for 3D instance segmentation of voxel-based scene representations. In addition to a metric learning task, we task our network to predict directional information to the object's center. We demonstrate that the multi-task learning improves the results for both tasks. Our approach is robust and scalable, therefore suitable for processing large amounts of 3D data. \emph{(ii)} Our experiments demonstrate state-of-the-art performance for 3D instance segmentation. At the time of submission, our method ranks first in terms of average AP50 score on the ScanNet 3D instance segmentation benchmark~\cite{Dai-et-al-CVPR-2017}.



 \section{Related Work}

This section gives a brief overview of related 2D and 3D approaches. It is worthwhile to note that a large amount of related work exists for 2D deep learning-based semantic segmentation and instance label segmentation. Recent surveys can be found in \cite{Guo-et-al-IJMIR-2017, Garcia-et-al-arXiv-2017}.

\boldparagraph{2D Instance Segmentation via Object Proposals or Detection.}
Girshick~\cite{Girshick-ICCV-2015} proposed a network architecture that creates region proposals as candidate object segments.
In a series of followup work, this idea has been extended to be faster \cite{Ren-et-al-NIPS-2015} and to additionally output pixel-accurate masks for instance segmentation \cite{He-et-al-ICCV-2017}.
The authors of YOLO~\cite{Redmon-et-al-CVPR-2016} and its follow-up work \cite{Redmon-Farhadi-CVPR-2017} apply a grid-based approach, in which each grid cell generates an object proposal. DeepMask~\cite{Pinheiro-et-al-NIPS-2015} learns to jointly estimate an object proposal and an object score. Lin~\etal\cite{Lin-et-al-CVPR-2017} propose a multi-resolution approach for object detection, which they call feature pyramid networks.
In \cite{Hayder-et-al-CVPR-2017}, the region proposals are refined with a network that predicts the distance to the boundary which is then transformed into a binary object mask.
Khoreva~\etal\cite{Khoreva-et-al-CVPR-2017} jointly perform instance and semantic segmentation.
A similar path follows \cite{Li-et-al-CVPR-2017}, which combines fully convolutional networks for semantic segmentation with instance mask proposals.
Dai~\etal\cite{Dai-et-al-CVPR-2016} use fully convolutional networks (FCNs) and split the problem into bounding box estimation, mask estimation, and object categorization and propose a multi-task cascaded network architecture.
In a follow-up work \cite{Dai-et-al-ECCV-2016}, they combine FCNs with windowed instance-sensitive score maps.

While all these approaches have been very successful in the 2D domain, many of them require large amounts of resources and their extension to the 3D domain is non-trivial and challenging.

\boldparagraph{2D Instance Segmentation via Metric Learning.}
Liang~\etal\cite{Liang-et-al-TPAMI-2017} propose a method without object proposals as they directly estimate bounding box coordinates and confidences in combination with clustering as a post-processing step. 
Fathi~\etal\cite{Fathi-et-al-arXiv-2017} compute likelihoods of pixels to belong to the same object by grouping similar pixels together within an embedding space. 
Bai and Urtasun \cite{bai2017deep} learn an energy map of the image in which object instances can be easily predicted.
Novotny~\etal\cite{Novotny-et-al-ECCV-2018} learn a position sensitive metric (semi-convolution embedding) to better distinguish between identical copies of the same object.
Kong and Fowlkes \cite{Kong-Fowlkes-CVPR-2018} train a network that assigns all pixels to a spherical embedding, in which points of the same object instance are within a close vicinity and non-instance related points are placed apart from each other.
The instances are then extracted via a variant of mean-shift clustering\cite{Fukunaga-Hostetler-TIT-1975} that is implemented as a recurrent network.
The approach by DeBrabandere~\etal\cite{DeBrabandere-et-al-arXiv-2017} follows the same idea, but the authors do not impose constraints on the shape of the embedding space.
Likewise, they compute the final segmentation via mean-shift clustering in the feature space.



None of these approaches has been applied to a 3D setting.
Our approach builds upon the work of DeBrabandere~\etal\cite{DeBrabandere-et-al-arXiv-2017}.
We extend this method with a multi-task approach for 3D instance segmentation on dense voxel grids.


\begin{figure*}[t]
	\centering
	\includegraphics[width=\textwidth]{model_overview.pdf}
\vspace{-0.7cm}
	\caption{\textbf{Overview of our network architecture.} We cast 3D instance segmentation as a multi-task learning problem. The input to our method is a voxel grid and the output are two latent spaces: 1) a feature vector embedding that groups voxels with similar instance label close in the latent space; 2) a 3D latent space that encodes directional predictions for each voxel. The inputs and outputs of our network are visualized and explained in Fig.~\ref{fig:embedding_spaces}. The parameters in the figure correspond to (number of filters, kernel size, stride, dilation).}
	\label{fig:model_overview}
\end{figure*}




\boldparagraph{3D Instance Segmentation.}
Wang~\etal\cite{Wang-et-al-CVPR-2018} propose SGPN, an instance segmentation for 3D point clouds.
In the first step, they extract features with PointNet~\cite{Qi-et-al-CVPR-2017} and subsequently build a similarity matrix, in which each element classifies whether two points belong to the same object instance.
The approach is not very scalable and limited to small point cloud sizes, since the size of the similarity matrix is squared the number of points in the point cloud.
Moreover, there is a number of recent concurrent or unpublished works that address 3D instance segmentation. The GSPN method~\cite{yi2018gspn} proposes a generative shape proposal network, which relies on object proposals to identify instances in 3D point clouds.
The 3D-SIS approach~\cite{hou20183d} combines 2D and 3D features aggregated from multiple RGB-D input views.
MASC~\cite{liu2019masc} relies on the superior performance of the SparseConvNet~\cite{3DSemanticSegmentationWithSubmanifoldSparseConvNet} architecture and combines it with 
an instance affinity score that is estimated across multiple scales. PanopticFusion \cite{narita2019panopticfusion} predicts pixel-wise labels for RGB frames and carries them over into a 3D grid, where a fully connected CRF is used for final inference. 

Apart from these recent concurrent works, there has generally been sparse research on 3D instance segmentation.









%
 \section{Method Overview} \label{sec:method}


In this work, we aim at segmenting 3D instances within a given 3D scene. 
To fully locate a 3D instance, one would require both a semantic label and an instance label. 
Rather than solving the complex task of scene completion, semantic labeling and instance segmentation at once, we model our 3D instance segmentation process as a post-processing step for semantic segmentation labeling.
We focus on the grouping and splitting of semantic labels, relying on inter-instance and intra-instance relations.
We benefit from the real distances in 3D scenes, where sizes and distances between objects are key to the final instance segmentation. 


We split our task into a label segmentation then instance segmentation problem, as we believe that features learned in each step possess task-specific information. 
Semantic segmentation on one hand can rely on local information to predict the class label.
Learning to semantically label a volumetric representation inherently encodes features from neighboring volumes but does not require knowledge of the whole environment.
On the other hand, instance segmentation requires a holistic understanding of the scene in order to join or separate semantically labeled volumes.












\boldparagraph{Problem Setting.}
Our method's input is a voxelized 3D space with each voxel encoding either a semantic label or a local feature vector learned through semantic labeling. 
In this paper, we use the semantic labeling network in \cite{3DSemanticSegmentationWithSubmanifoldSparseConvNet}.
We fix the voxel size to preserve 3D distances among all voxels within a scene.
In problem settings where point clouds or meshes are available, one could generate a 3D voxelization by grouping information from points within every voxel.
Our method then processes the voxelized 3D space and outputs instance label masks, each corresponding to a single object in the scene, along with its semantic label.
The output mask can also be reprojected back into a point cloud by assigning the voxel label to all points within it. 









\subsection{Network Architecture} \label{subsec:network}
In order to process the 3D input, we utilize a 3D convolution network, which is based on the SSCNet architecture~\cite{Song-et-al-CVPR-2017}.
We apply some changes to the original SSCNet network to better suit our task.
As shown in Figure~\ref{fig:model_overview}, the network input and output are equally sized.
Since the pooling layer scales down the scene size, we use a transpose of convolution (also referred to as deconvolution~\cite{zeiler2010deconvolutional}) to upsample back into the original size. 
We also use larger dilations for diluted 3D convolution layers to increase the receptive field. 
We make the receptive field large enough to access all the voxels of usual indoor rooms. With a voxel size of 10cm, our receptive field is as large as 14.2m. 
With larger scenes, our 3D convolution network would still be applicable to the whole scene, while preserving the filter and voxel sizes, and thus preserving the real distances.
Objects lying at distances larger than the receptive field are separated by default. 











\subsection{Multi-task Loss Function} \label{subsec:loss}
In order to group voxels of the same instance, we aim to learn two types of feature embedding.
The first type maps every voxel into a feature space, where voxels of the same instance are closer to each other than voxels belonging to different instances.
This is similar to the work of DeBrabandere~\etal\cite{DeBrabandere-et-al-arXiv-2017}, but applied in a 3D setting.
The second type of feature embedding assigns a 3D vector to every voxel, where the vector would point towards the physical center of the object it belongs to.
This enables the learning of shape containment and removes ambiguities among similar shapes.

In order to learn both feature embeddings, we introduce a multi-task loss function that is minimized during training. 
The first part of the loss encourages discrimination in the feature space among multiple instances, while the second part penalizes angular deviations of vectors from the desired direction.




\begin{figure*}[t!]
  \small
\centering  
  \includegraphics[width=0.95\textwidth]{embedding_spaces.pdf} \
  \nLoss_\text{FE} = 
    \gamma_\text{var} \nLoss_\text{var} +
    \gamma_\text{dist} \nLoss_\text{dist} +
    \gamma_\text{reg} \nLoss_\text{reg}
  \label{eq:feature_loss}

  \nLoss_\text{var} &= \frac{1}{C}\sum_{c=1}^C \frac{1}{N_c} \sum_{i=1}^{N_c}
    \left[ \|\nc_c - \nx_i\| - \delta_\text{var} \right]_{+}^2 \\
\nLoss_\text{dist} &= \frac{1}{C(C-1)}\sum_{c_A=1}^C \sum_{\substack{c_B=1\\c_B\neq c_A}}^C
    \left[ 2\delta_\text{dist} - \|\nc_{c_A} - \nc_{c_B}\| \right]_{+}^2 \\
\nLoss_\text{reg} &= \frac{1}{C} \sum_{c=1}^C \|\nc_c\|

  \nLoss_\text{dir} = -\frac{1}{C} \sum_{c=1}^C \frac{1}{N_c} 
                     \sum_{i=1}^{N_c} \nv_i ^{\top} \nv_i^{GT}
  \quad \text{with} \;\;
  \nv_i^{GT} = \frac{\nz_i-\nz_c}{\|\nz_i-\nz_c\|}

   \nLoss_\text{joint} = \alpha_\text{FE}\nLoss_\text{FE} + \alpha_\text{dir}\nLoss_\text{dir}
\lsp]
\includegraphics[width=\sz\textwidth]{scene0342_00_input.jpg} &
    \includegraphics[width=\sz\textwidth]{scene0342_00_gt_semantic.jpg} & 
    \includegraphics[width=\sz\textwidth]{scene0342_00_spc.jpg} &
    \includegraphics[width=\sz\textwidth]{scene0342_00_cc.jpg} & 
    \includegraphics[width=\sz\textwidth]{scene0342_00_sgpn.jpg} & 
    \includegraphics[width=\sz\textwidth]{scene0342_00_gt.jpg} & 
    \includegraphics[width=\sz\textwidth]{scene0342_00_ours.jpg} \\lsp]
\includegraphics[width=\sz\textwidth]{scene0496_00_input.jpg} &
    \includegraphics[width=\sz\textwidth]{scene0496_00_gt_semantic.jpg} & 
    \includegraphics[width=\sz\textwidth]{scene0496_00_spc.jpg} &
    \includegraphics[width=\sz\textwidth]{scene0496_00_cc.jpg} & 
    \includegraphics[width=\sz\textwidth]{scene0496_00_sgpn.jpg} & 
    \includegraphics[width=\sz\textwidth]{scene0496_00_gt.jpg} & 
    \includegraphics[width=\sz\textwidth]{scene0496_00_ours.jpg} \-0.2cm]
Input (RGB) & Semantic GT &  SPC~\cite{3DSemanticSegmentationWithSubmanifoldSparseConvNet} & CC & SGPN~\cite{Wang-et-al-CVPR-2018} & Instance GT & Ours \\
\end{tabular}
  \vspace{0.005cm}
  \caption{\textbf{Qualitative results of our method on the ScanNet validation dataset~\cite{Dai-et-al-CVPR-2017}.}
  This figure shows the original input scene as a textured mesh, the semantic labeling results of SparseConvNet (SPC)~\cite{3DSemanticSegmentationWithSubmanifoldSparseConvNet} which we use as input and our instance labeling results as well as the semantic groundtruth (GT). We further show multiple 3D instance segmentation baselines: connected component (CC) labeling on the SPC semantic labeling, SPGN~\cite{Wang-et-al-CVPR-2018}, and the groundtruth instance labels next to our labeling results.}
  \label{fig:qualitative_results_a}
\end{figure*}



Table~\ref{tab:results_test} provides an overview of our benchmark results on the ScanNet test dataset (with held out ground truth). One can see that our method outperforms the others in AP50 score. Other methods include those that process all RGB-D images that were used to reconstruct the scenes of ScanNet. Instance labels of single RGB-D frames in these methods are propagated throughout the whole scene and concatenated based on the location estimation. On the other hand, our method directly operates in the 3D setting, without the need to use the 2D information. This leads to much faster processing on the 3D scenes, and requires substantially less information to extract the 3D object instance segmentations.






 \section{Conclusion}
We proposed a method for 3D instance segmentation of voxel-based scenes.
Our approach is based on metric learning and the first part assigns all voxels belonging to the same object instance feature vectors that are in close vicinity. 
Conversely, voxels belonging to different object instances are assigned  features that are further apart from each other in the feature space. 
The second part estimates directional information of object centers, which is used to score the segmentation results generated by the first part.


{\small
\vspace{0.5em}
\boldparagraph{Acknowledgments.}
This research was supported by competitive funding from King Abdullah University of Science and Technology (KAUST). Further support was received by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/ Interior Business Center (DOI/IBC) contract number D17PC00280.
The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DOI/IBC, or the U.S. Government.
\par
} 


{\small
\bibliographystyle{ieee_fullname}
\bibliography{bibliography}
}

\end{document}
