\documentclass{article} \usepackage{iclr2024_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{url}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{wrapfig}
\usepackage{listings}
\usepackage{algpseudocode}
\usepackage{authblk}
\renewcommand\Authands{, }
\newcommand{\greencheck}{{\color{green}\checkmark}} 
\newcommand{\redcross}{{\color{red}}} 

\usepackage{xcolor}
\PassOptionsToPackage{hyphens}{url}
\hypersetup{breaklinks=true,colorlinks,
  citecolor=citecolor}
\definecolor{citecolor}{RGB}{34,139,34}


\iclrfinalcopy

\title{Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models}



\author[1,2]{{Andy Zhou}}
\author[1]{{Kai Yan}}
\author[1]{{Michal Shlapentokh-Rothman}}
\author[1]{{Haohan Wang}}
\author[1]{{Yu-Xiong Wang}}
\affil[1]{University of Illinois at Urbana-Champaign}
\affil[2]{AI@UIUC}
\affil[ ]{\texttt{\{andyz3,kaiyan3,michal5,haohanw,yxw\}@illinois.edu}}




\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
While large language models (LLMs) have demonstrated impressive performance on a range of decision-making tasks, they rely on simple acting processes and fall short of broad deployment as autonomous agents. We introduce LATS (Language Agent Tree Search), a general framework that synergizes the capabilities of LLMs in planning, acting, and reasoning. Drawing inspiration from Monte Carlo tree search commonly used in model-based reinforcement learning, LATS employs LLMs as agents, value functions, and optimizers, repurposing their latent strengths for enhanced decision-making. What is crucial in this method is the use of an environment for external feedback, which offers a more deliberate and adaptive problem-solving mechanism that moves beyond the limitations of existing techniques. Our experimental evaluation across diverse domains, such as programming, HotPotQA, and WebShop, illustrates the applicability of LATS for decision-making while maintaining competitive reasoning performance. In particular, LATS achieves 94.4\% for programming on HumanEval with GPT-4 and an average score of 75.9 for web browsing on WebShop with GPT-3.5, demonstrating the effectiveness and generality of our method.
\end{abstract}
\section{Introduction}

General autonomous agents capable of reasoning and decision-making in a variety of environments \citep{wooldridge1995intelligent} have been of longstanding interest in the field of artificial intelligence. While this has traditionally been studied in reinforcement learning, the recent rise of large language models (LLMs) \citep{GPT3, chowdhery2022palm, touvron2023llama, openai2023gpt4} with strong reasoning and general adaptability offers an alternative paradigm. Not only have LLMs excelled on standard NLP tasks such as text summarization \citep{nallapati2016abstractive} or natural language inference \citep{bowman2015large}, but they have been adapted to an increasingly diverse set of tasks that often require advanced common-sense reasoning or quantitative skills \citep{cobbe2021training, saparov2022language}. LLMs are also capable of performing in complex environments that involve knowledge and reasoning, such as web navigation \citep{yao2022webshop, deng2023mind2web}, tool-use \citep{schick2023toolformer}, or open-ended games \citep{fan2022minedojo}. 

\begin{wrapfigure}{r}{0.5\textwidth}
  \centering
  \vspace{-0.2in}
  \includegraphics[width=0.4\textwidth]{figures/teaser2.pdf}
  \caption{An overview of LATS. LATS uses an external environment and self-reflection to improve	reasoning and decision-making.}
  \vspace{-0.2in}
  \label{fig:teaser}
\end{wrapfigure}

Reasoning and acting abilities have also been improved by prompting techniques that augment LLMs with feedback or observations from an external environment \citep{yao2023react, gao2022pal, shinn2023reflexion}. This eliminates the need to rely entirely on the base abilities of the Language Model (LM), enhancing it through external tools or semantic feedback. Despite this strength, these methods are reflexive and fall short of humans' deliberate and thoughtful decision-making characteristics to solve problems \citep{Sloman1996TheEC,evans2010intuition}. In particular, such methods fail to consider multiple reasoning paths or to plan ahead. Recent search-guided LLM works \citep{xie2023decomposition, yao2023tree, hao2023reasoning} address this issue by searching over multiple reasoning chains. While these methods enable planning, these methods operate in isolation and do not incorporate external feedback that can improve reasoning. 

To help address these issues, we propose LATS (Language Agent Tree Search), a general framework for decision-making and reasoning with language models. LATS unifies LM planning, acting, and reasoning strategies by expanding ReAct \citep{yao2023react} into a search over a combinatorial space of possible reasoning and acting steps. We adapt Monte Carlo tree search (MCTS) from model-based reinforcement learning \citep{Silver2017MasteringTG, anthony2017thinking, jiang2018feedback} to language agents, repurposing a pretrained LLM as an agent, value function, and optimizer. Utilizing the strong natural language understanding and in-context learning ability of modern LMs, we use text as an interface between each component of the framework, allowing LATS to adapt planning to environmental conditions without additional training. To the best of our knowledge, \emph{LATS is the first framework that combines reasoning, acting, and planning to enhance LLMs}. Notably, LATS doubles the performance of GPT-3.5 on HotPotQA \citep{yang2018hotpotqa} over ReAct \citep{yao2023react} and raises the average score by  on WebShop \citep{yao2022webshop}. When used with GPT-4, LATS achieves a  Pass@1 rate for programming on HumanEval \citep{chen2021evaluating}, setting the state of the art. To summarize, our \textbf{contributions} are the following:
\vspace{-0.1in}
\begin{itemize}
    \item We introduce an LM-based Monte Carlo tree search variant to deliberately construct the best trajectory from sampled actions, enabling more flexible and adaptive problem-solving compared to reflexive prompting methods. This is guided by heuristics from the LM.
    \item By integrating external feedback and self-reflection, LATS enhances model sensibility and enables agents to learn from experience, surpassing reasoning-based search methods.
    \item Through experiments across diverse domains like programming, interactive QA, and web navigation, we demonstrate the versatility of LATS in harnessing LLMs for autonomous reasoning and decision-making.
\end{itemize}
\vspace{-0.07in}

\section{Related Work}
\vspace{-0.06in}
\begin{table}
    \centering
    \begin{tabular}{lccccc}
    \toprule
    \textbf{Approach} & 
    \textbf{Reasoning} &
    \textbf{Acting} &
    \textbf{Planning} &
    \textbf{Self} &
    \textbf{External} \\
    & & & & \textbf{Reflection} & \textbf{Memory} \\
    \midrule
    CoT \citep{wei2022chain} & \greencheck & \redcross & \redcross & \redcross & \redcross \\
    ReAct \citep{yao2023react} & \greencheck & \greencheck & \redcross & \redcross & \redcross \\
    ToT \citep{yao2023tree} & \greencheck & \redcross & \greencheck & \greencheck & \greencheck \\
    RAP \citep{hao2023reasoning} & \greencheck & \redcross & \greencheck & \redcross & \greencheck \\
    Self-Refine \citep{madaan2023selfrefine} & \greencheck & \redcross & \redcross & \greencheck & \redcross \\
    Beam Search \citep{xie2023decomposition} & \greencheck & \redcross & \redcross & \greencheck & \redcross \\
    Reflexion \citep{shinn2023reflexion} & \greencheck & \greencheck & \redcross & \greencheck & \greencheck \\
    \textbf{LATS (Ours)} & \greencheck & \greencheck & \greencheck & \greencheck & \greencheck \\
    \bottomrule
    \end{tabular}

    \caption{A summary of related work on reasoning, acting, and planning. LATS is the first work incorporating designs from all three domains, allowing use in all corresponding tasks. We refer to planning as the use of a search algorithm, self-reflection as the use of LM-generated feedback, and external memory as storaging past text context for future updates of solution.}
    \vspace{-0.1in}
    \label{table:related_work}
\end{table}



 
\begin{figure}[!htbp]
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/tot.pdf}
        \vspace{-0.05in}
        \\\textbf{a) Tree-of-Thoughts}
        \label{fig:tot}
    \end{minipage}\hfill
    \hspace{-0.04\textwidth}
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/rap.pdf}
        \vspace{-0.05in}
        \\\textbf{b) Reasoning via Planning}
        \label{fig:rap}
    \end{minipage}\hfill
    \begin{minipage}{0.34\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/lats.pdf}
        \vspace{-0.05in}
        \\\textbf{c) Language Agent Tree Search}
        \label{fig:lats}
    \end{minipage}
    \vspace{-0.15in}
    \caption{An overview of the differences between LATS and recently proposed LM search algorithms ToT \citep{yao2023tree} and RAP \citep{hao2023reasoning}. LATS leverages environmental feedback and self-reflection to further adapt search and improve performance.}
    \vspace{-0.13in}
    \label{fig:comparison}
\end{figure}

\textbf{LLMs for reasoning.} For LLMs, reasoning typically involves decomposing complex inputs into sequential intermediate steps towards a final answer \citep{cobbe2021training}, demonstrated with Chain-of-Thought (CoT) prompting \citep{wei2022chain} and its variants~\citep{wei2022chain, kojima2022large, wang2022self}. However, these methods, which create chains autoregressively in a single step, often suffer from error propagation as the number of steps increases \citep{guo2018long,chen2022program} due to compound errors. Various advancements aim to mitigate this issue; some approaches, such as Self-Consistency~\citep{wang2022self}, employ majority voting over sampled chains, while others focus on multi-step decomposition, such as least-to-most prompting~\citep{zhou2022least}, or use of external tools such as a scratchpad \citep{nye2021show} or compiler \citep{gao2022pal}. Recently, CoT has been improved with search algorithms \citep{yao2023tree,hao2023reasoning,besta2023graph} that can sample trajectories more effectively. Tree-of-thought (ToT) prompting \citep{yao2023tree} uses DFS or BFS-based search guided by an LM-generated heuristic while Reasoning via Planning (RAP) \citep{hao2023reasoning} uses MCTS with rollouts simulated by the LM. However, they rely solely on LM internal knowledge and cannot adapt to useful external feedback.

\textbf{LLMs for acting.} The strong reasoning and common-sense abilities of LLMs have also been adapted for decision-making or acting tasks as a policy model in interactive environments. In the realm of robotics LLMs have been employed as high-level controllers of control policies \citep{ahn2022do, huang2022inner, driess2023palme}. Similar work \citep{openai2022vpt, wang2023voyager, zhu2023ghost} has also adapted LLM agents to complex multimodal games such as Minecraft \citep{guss2019minerl, fan2022minedojo}. LLMs are particularly useful in text-based environments \citep{liu2018reinforcement,shridhar2020alfworld,liu2023agentbench}, where acting-based prompting techniques such as ReAct \citep{yao2023react} have seen success. Similar to CoT, ReAct is limited by its simplicity and cannot effectively adapt to environment conditions. Many extensions have been proposed to address this, including Self-refine \citep{madaan2023selfrefine} and Reflexion \citep{shinn2023reflexion, yao2023retroformer}, which uses self-reflection to enhance reasoning and decision-making, and AdaPlanner \citep{sun2023adaplanner}, which incorporates both positive and negative environmental feedback. However these methods focus on refining an individual plan or trajectory and do not consider alternative choices at each step. In addition, recent work \citep{huang2023large} has suggested LLMs cannot self-correct their internal reasoning, making it critical to use external feedback. Alternatively to pure decision-making environments, the reasoning and practical abilities of LLMs have been enhanced by access to external tools, such as APIs, search engines, calculators, or other models \citep{schick2023toolformer, shen2023hugginggpt, sur√≠s2023vipergpt}. Contrary to reasoning-based approaches, these methods have not been improved with planning, limiting their effectiveness. We summarize them in Tab.~\ref{table:related_work}.

\textbf{Tree-based search.} Tree-based search, where multiple branches of outcomes are explored during search, is widely used in many planning algorithms~\citep{swiechowski2023monte, lavalle2001rapidly} and Reinforcement Learning (RL)~\citep{hafner2019learning, du2023learning, wu2023daydreamer} algorithms for its good exploration-exploitation trade-off. Though tree-based search requires an environment model that can expand from arbitrary state~\citep{vodopivec2017monte}, which often requires extra training in RL~\citep{hafner2023mastering}, such problem does not exist for LM tasks as we can conveniently backup to any state by setting the input to be the context and corresponding previous output by the LM. Thus, we work on the tree-based framework and use MCTS~\citep{swiechowski2023monte} to fully release the potential of LMs, while avoiding the cost of training a value function over language descriptions by leveraging the in-context learning \citep{GPT3} abilities of LLMs.

\section{Preliminaries}

\subsection{Problem Setting and Prompting}

Before describing LATS, we first define our problem and outline a few established methods that leverage large language models for reasoning or decision-making. In LM reasoning or decision making, we are given an input  in natural language and a pretrained language model  parameterized by ; our goal is to generate a final output  corresponding to the answer (reasoning) or completes the task (decision-making). Both  and  are language \textit{sequences}, which are comprised of a list of \textit{tokens} (the basic elements of natural language, often words), denoted as  and . The LM decodes text autoregressively, i.e., without other inputs, the probability for an LM to generate a sequence  is given by . Usually, to improve the LM, \textit{prompts} are provided along with the input , which are specific instructions or few-shot input-output examples. We denote the generic process where an input  is transformed into an output  by LM: , where  denotes the input .

\textbf{Chain-of-thought (CoT) Prompting}~\citep{wei2022chain} was introduced to cater to scenarios where direct mapping from  to  is intricate, such as when  is from a mathematical query or challenging question. This method hinges on creating \textit{thoughts}  that act as stepping stones between  and ; each thought  is a language sequence. To employ CoT prompting, thoughts are extracted sequentially as , with the final output being . 

\textbf{Tree-of-thought (ToT) Prompting}~\citep{yao2023tree} extends CoT prompting by exploring multiple reasoning paths over thoughts. It frames problems as a search over a tree where each node  represents a partial solution state comprising the original input  and thought sequence . Thoughts  are generated by proposal or sampling with CoT . Deliberate search algorithms like breadth-first or depth-first search are used to systematically explore the tree, guided by heuristics based on language model evaluations  of each state. 

\textbf{Reasoning via Planning} (RAP)~\citep{hao2023reasoning} is similar to ToT, except that MCTS is used over DFS or BFS. Heuristics are designed from an LM, such as the likelihood or confidence of an action, and the LM is used as a world model to predict subsequent states during the simulation step.

\textbf{ReAct}~\citep{yao2023react} extends language models to tasks where the mapping from  to  is enhanced by or requires interactions with an external environment, such as a game or API. This technique constructs an action space  that adds permissible actions  to the reasoning traces  from CoT. Observations  from the environment are used to improve both reasoning and acting. To solve problems with ReAct, after each observation, actions are generated from  sequentially as , with the final output being . 


While the previously described prompting techniques improve LM performance on reasoning tasks, they falter on difficult tasks that involve multifaceted decision-making due to several shortcomings: 1) \textit{Flexibility}: Base prompting methods (CoT or ReAct) autoregressively sample from the LM, neglecting potential alternative continuations from specific states. 2) \textit{Sensibility}: Reasoning-based methods (CoT, RAP, or ToT) rely solely on the internal representations of the LM and cannot consider external observations. This dependency risks fact hallucination and error propagation while setting a performance ceiling. 3) \textit{Adaptability}: Current planning frameworks (RAP or ToT) use simple search algorithms such as BFS or cannot leverage environmental feedback to improve planning. Additionally, the agent is static and cannot reuse previous experience or learn from trial and error. While RAP also adopts MCTS, it is constrained to tasks where the LM can become a world model and accurately predict states. These shortcomings limit the ability of LMs to be deployed as general problem-solving agents and form the motivation for LATS.


\subsection{Monte-Carlo Tree Search (MCTS)}
Monte-Carlo Tree Search (MCTS) is a heuristic search algorithm that is proved successful on many decision-making environments such as Atari~\citep{ye2021mastering} and Go~\citep{silver2016mastering}. MCTS builds a decision tree where every node in the tree is a state and edge is an action. MCTS runs for  episodes; for each episode, it starts from the root (i.e., initial state) and iteratively conducts two steps to expand the tree: 1) \textit{Expansion}, where multiple children states  are explored from the current parent state  by sampling  actions, and 2) \textit{Selection}, where the children with the highest UCT \textit{(Upper Confidence bounds applied to Trees)}~\citep{kocsis2006bandit} value is selected by the next iteration. The UCT of a child state  is calculated as follows:

where  is the number of visits to a node ,  is the value function (expected return) from the subtree of ,  is the exploration weight, and  is the parent node of . The child node with the highest UCT value is selected for expansion in the next iteration. When the end of an episode is reached, a \textit{backpropagation} is carried out: the return  is used for updating every  along the path 
with the formula , where  is the old value function. Normally, the major shortcoming of MCTS is that it requires an environment model to undo previous steps and form a searching tree, which is often a strong assumption. However, such a limitation does not exist for LMs, as we can conveniently reset to any step by simply copy-pasting historical text input. Such a special property is the key motivation of our work. 





\section{Unifying Planning, Reasoning, and Acting}

\subsection{LM Agent} 

LATS supports sequential reasoning or decision-making tasks on the basis of ReAct. At time step , an agent receives an observation  from the environment and takes an action  following some policy , where  consists of the task instruction and a number of few-shot examples. We initialize the agent with  to leverage the useful language representations of an LM as a base decision-maker. We follow the ReAct instantiation in which the action space  consists of both the space of permissible actions  and language space of reasoning traces . Actions directly affect the environment and result in observation, while thoughts are used to formalize decisions by organizing information, planning future actions, or injecting internal knowledge. The exact instantiation of the action space depends on the particular environment; for decision-making tasks actions might consist of commands on a website while for reasoning tasks the action space might be limited to a few external tools or APIs.

Instead of greedily decoding one trajectory or solution, we sample  actions from  using the current state. This is based on the intuition that for complex decision-making tasks, there is likely to be a range of potential trajectories or reasoning paths that are correct \citep{evans2010intuition}. Sampling a diverse set of candidates at each step mitigates the stochastic nature of LM text generation and enables greater exploration in both the decision-making and reasoning space. We wrap  within our proposed search algorithm to deliberately construct the best trajectory from sampled actions.

\subsection{LATS}
\label{sec:LATS}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/phases.pdf}
    \vspace{-0.3in}
    \caption{An overview of the six operations of LATS. A node is \textit{selected}, \textit{expanded}, \textit{evaluated}, then \textit{simulated} until a terminal node is reached, then the resulting value is \textit{backpropagated}. If the trajectory fails, a \textit{reflection} is generated and used as additional context for future trials. These operations are performed in succession until the budget is reached or task is successful.}
    \vspace{-0.2in}
    \label{fig:phases}
\end{figure}

The main component of LATS is a search algorithm that controls the overall problem-solving process with deliberate planning. To find the most promising trajectory and systemically balance exploration with exploitation, we adopt a variant of Monte Carlo Tree Search (MCTS) that frames decision-making as a tree search, in which each node  represents a state comprising the original input , action sequence , and observation sequence . 

To adapt MCTS for language agents, LATS repurposes  as an agent, state evaluator, and feedback generator, leveraging the useful language priors of modern LMs to facilitate planning. While standard MCTS and RAP \cite{hao2023reasoning} rely on internal dynamics models to facilitate simulation, LATS is model-free and uses environment interaction. LATS consists of a series of operations, \textit{selection, expansion, evaluation, simulation, backpropagation, and reflection}, performed in succession until the task is successfully completed or a computational limit is reached. The full psuedocode of LATS can be found in Sec.~\ref{sec:pseudocode} in the Appendix.

\textbf{Selection.}
In the first operation, the algorithm identifies a segment of the current tree most suitable for subsequent expansion. Starting from the root node, denoted as the initial state , a child node is selected at each tree level until a leaf node is reached. To balance exploration and exploitation, we use the UCT algorithm as shown in Eq.~\ref{eq:uct}.



\textbf{Expansion.} 
After selecting a node, the second operation expands the tree by sampling  actions from , as described in the prior section. The environment receives each action and returns corresponding feedback as an observation. This results in  new child nodes added to the tree. This tree is stored in an external long-term memory structure.

\textbf{Evaluation.} 
The third operation assigns a scalar value to each new child node to be used for selection and backpropagation. This value effectively quantifies the agent's progress in task completion, serving as a heuristic to steer the search algorithm towards the most promising regions of the tree. Following \citet{yao2023tree} we repurpose  into a value function by prompting it to reason about a given state. To obtain a scalar value, we instruct  to end its reasoning trace with a score indicating the correctness of the trajectory. This method offers enhanced flexibility over programmed heuristics \citep{campbell2002deep} and greater efficiency than learned heuristics \citep{Silver2017MasteringTG}.

\textbf{Simulation.} The fourth operation expands the currently selected node until a terminal state is reached. At each depth level we sample and evaluate nodes with the same operations, but prioritize nodes of highest value. Reaching a terminal state provides objective feedback on the correctness of a trajectory. If the task is completed successfully, then LATS terminates the search. If the solution is partially successful or unsuccessful, then we perform two additional operations as described below.

\textbf{Backpropagation.} 
This operation updates the values of the tree based on the outcome of a trajectory. For each node  in the trajectory from root (initial state ) of the searching tree to leaf (terminal state ), its value is updated to reflect the outcome of the simulation by  and , where  is the return and  are the old number of visits and value function. These updated values are used in the UCT formula (Eq.~\ref{eq:uct}) to guide the selection of the next node for exploration.

\textbf{Reflection.}
In addition to the environmental feedback, we also leverage \textit{self-reflection} to further refine the decision-making process \citep{shinn2023reflexion, madaan2023selfrefine}. Upon encountering an unsuccessful terminal node,  is prompted with the trajectory and final reward to provide a verbal self-reflection that summarizes the errors in the reasoning or acting process and proposes superior alternatives. We store both failed trajectories and corresponding reflections in the memory. In subsequent iterations, these are integrated as additional context to the agent and value function, refining both through in-context learning. This imparts a semantic gradient signal more useful than a scalar value, enabling the agent to learn from trial and error without the cost of expensive optimization processes such as reinforcement learning.

Conceptually, LATS has the following advantages as a general framework for reasoning and decision-making with LM agents.
(1) \textit{Generality}: LATS supports both reasoning and decision-making tasks by defining a shared space of thoughts and actions. (2) \textit{Deliberate}: The use of MCTS and LM value function ensures a principled search that selects options with high value while exploring promising alternatives. (3) \textit{Adaptability}: LATS is designed around the use of external feedback through observations and self-reflection, enabling greater adaptation during problem-solving. (4) \textit{Flexibility}: LATS can accommodate different scenarios, environments, and resource stipulations by modifying state design and tree dimensions. (5) \textit{Modularity}: The base LM agent, reflection generator, and value function can be independently altered and adapted to individual LM properties.

\section{Experiments}

To demonstrate the general applicability of LATS, we evaluate our method on a variety of decision-making domains that requires both reasoning and acting ability: programming \citep{chen2021evaluating, austin2021program}, HotPotQA \citep{yang2018hotpotqa}, and WebShop \citep{yao2022webshop}.

\subsection{HotPotQA}

For a task that can be approached with both reasoning-based and acting-based strategies, we consider HotPotQA \citep{yang2018hotpotqa}, a multi-hop question-answering benchmark that requires retrieval over two or more Wikipedia passages. For the action space, in addition to LM thoughts we follow the setup from \citet{yao2023react}, which provides the agent with API calls to search and lookup information. The output of these API calls and self-generated reflections form the observation space. We use a subset of 100 questions and three few-shot examples for each method. For ToT, we use DFS as the base search algorithm and scoring with the LM as the heuristic. For all methods that involve sampling, including LATS, we sample  trajectories. More details and prompts can be found in Sec.~\ref{sec:envdetail} and Sec.~\ref{sec:hotpotqa_prompt} in the Appendix.

We evaluate internal reasoning strategies by removing actions and observations from the context, corresponding to CoT \citep{wei2022chain} and its variants, CoT-SC \citep{wang2022self}, ToT \citep{yao2023tree}, and RAP \citep{hao2023reasoning}. These methods rely solely on the agent's existing knowledge to answer the question. We also consider acting-based methods ReAct, Reflexion, and LATS, which augment the agent with the interactive API environment and primarily evaluate its information retrieval abilities. While LATS is designed for scenarios where external feedback can enhance reasoning, we also implement a reasoning-only version with CoT as the base prompt. We also combine internal and external reasoning in LATS by first prompting with a CoT-based prompt, then switching to a ReAct-based prompt upon failure. This is closer to how humans might approach this task, by using tools to lookup additional information only when the answer is not already known. 

\begin{table}[t]
    \centering
    \begin{minipage}{0.5\linewidth}
        \centering
        \begin{tabular}{l|c}
        \toprule
            \textbf{Prompt Method} & \textbf{HotpotQA (EM)} \\
        \midrule
            I/O & 0.32 \\
            CoT {\scriptsize~\citep{wei2022chain}} & 0.34 \\ 
            CoT - SC {\scriptsize~\citep{wang2022self}} & 0.38 \\
            ToT {\scriptsize~\citep{yao2023tree}} & 0.55 \\
            RAP {\scriptsize~\citep{hao2023reasoning}} & 0.60 \\
            RAP (n = 10) & 0.60 \\
            LATS (CoT) & \textbf{0.60} \\
        \bottomrule
        \end{tabular}
    \end{minipage}\begin{minipage}{0.5\linewidth}
        \centering
        \begin{tabular}{l|c}
        \toprule
            \textbf{Prompt Method} & \textbf{HotpotQA (EM)} \\
        \midrule
            ReAct {\scriptsize~\citep{yao2023react}} & 0.32 \\
            ReAct (best of k) & 0.38 \\
            Reflexion {\scriptsize~\citep{shinn2023reflexion}} & 0.51 \\
            LATS & 0.61 \\
            LATS (n = 3) & 0.56 \\
            LATS (n = 10) & 0.64 \\
            LATS (CoT + ReAct) & \textbf{0.71} \\
        \bottomrule
        \end{tabular}
    \end{minipage}
    \caption{GPT-3.5 reasoning-based prompting (left) and acting-based prompting (right) results on HotpotQA. LATS achieves the highest exact match (EM) for acting and is competitive on reasoning. Unless otherwise specified, we sample  nodes during expansion and  trajectories.}
    \label{table:hotpot}
\end{table}


 
\textbf{Results.} We observe in Tab. \ref{table:hotpot} that both internal reasoning and external retrieval strategies perform well on HotPotQA. Due to their large-scale training corpus, modern LLMs already encode factual knowledge and can often directly answer the question correctly. While CoT can slightly enhance performance on questions requiring reasoning, larger gains are observed with search methods ToT and RAP, which can sample and explore more outputs. We observe similar results for acting-based methods. LATS surpasses ReAct, even when sampling the same number of trajectories, by expanding more nodes with principled search (see Fig.~\ref{fig:qualitative} in Appendix~\ref{sec:envdetail} for a qualitative sample). This is demonstrated when modifying , the number of nodes expanded during each iteration. Increasing  can consistently improve performance, although at greater computational and inference costs. LATS is also competitive to RAP on internal reasoning but performs worse than acting. Combining internal and external reasoning in LATS results in the highest performance, indicating the importance of external feedback in augmenting reasoning even in tasks the base LM can already perform.

\subsection{Programming}

\begin{table}[t]
\begin{minipage}{.48\linewidth}  \centering
    \begin{tabular}{ll | c}
    \toprule
    \textbf{Prompt Method} & \textbf{Model} & \textbf{Pass@1} \\
    \midrule
    CoT {\scriptsize~\citep{wei2022chain}} & GPT-3.5 & 46.9 \\
    ReAct {\scriptsize~\citep{yao2023react}} & GPT-3.5 & 56.9 \\
    Reflexion {\scriptsize~\citep{shinn2023reflexion}} & GPT-3.5 & 68.1 \\
    ToT {\scriptsize~\citep{yao2023tree}} & GPT-3.5 & 54.4 \\
    RAP {\scriptsize~\citep{hao2023reasoning}} & GPT-3.5 & 63.1 \\
    LATS (Ours) & GPT-3.5 & \textbf{83.8} \\ 
    \midrule
    I/O & GPT-4 & 80.1 \\
    Reflexion & GPT-4 & 91.0 \\
    LATS & GPT-4 & \textbf{94.4} \\
    \bottomrule
    \end{tabular}
\end{minipage}\hfill  \begin{minipage}{.48\linewidth}  \centering
    \vspace{-0.53in}
    \begin{tabular}{l | c}
    \toprule
    \textbf{Prompt Method} & \textbf{Pass@1} \\
    \midrule
    CoT {\scriptsize~\citep{wei2022chain}} & 54.9 \\
    ReAct {\scriptsize~\citep{wei2022chain}} & 67.0 \\
    Reflexion {\scriptsize~\citep{shinn2023reflexion}} & 70.0 \\
    ToT {\scriptsize~\citep{yao2023tree}} & 65.8 \\
    RAP {\scriptsize~\citep{hao2023reasoning}} & 71.4 \\
    LATS (Ours) & \textbf{81.1} \\
    \bottomrule
    \end{tabular}
\end{minipage}
\caption{GPT-3.5 and GPT-4 Pass@1 accuracy on HumanEval \citep{chen2021evaluating} and MBPP \citep{austin2021program}. Prompting with LATS achieves the highest performance. We sample 5 solutions during expansion for {\color{red}8} iterations.}
\label{table:programming}
\vspace{-0.15in}
\end{table}
 
To demonstrate the importance of external observations for complex reasoning tasks, we evaluate the baselines and LATS on programming with Humaneval~\citep{chen2021evaluating} and MBPP~\citep{austin2021program}. Both datasets measure the correctness of synthesized programs in Python from natural language docstrings. We use individual solutions as the action space and test suite and compiler feedback as the external observation. We follow \citet{bei2022codet} and use an LLM to generate a synthetic test suite of syntactically valid ``assert'' statements for each question. For each step, the solution is evaluated on this test suite, and the results including successful and failed tests and compiler output, are added to the context as an observation. We use the same test suite for Reflexion. 

For this task, the reasoning and acting baselines share an action space, but acting methods are able to incorporate observations as additional context. For LATS, since each action corresponds to a complete solution, we skip the simulation step of LATS and directly use the percentage of passed tests as the backpropagated reward. We use  iterations, set the number of generated tests at , and sample  solutions during expansion. After the search is completed, we select the solution with the highest value and evaluate it on the real test suite for the pass@1 accuracy evaluation. More details and prompts can be found in Sec.~\ref{sec:envdetail} and Sec.~\ref{sec:programming_prompt} in the Appendix.

\textbf{Results.} We find in Tab \ref{table:programming} that both search and semantic feedback are crucial for better performance. Despite not using observations, ToT and RAP are competitive with Reflexion. LATS has the highest performance on both datasets. Since RAP uses a similar search algorithm as LATS, this reveals the importance of external feedback for difficult reasoning tasks such as programming. With GPT-4, using LATS sets the state of the art for HumanEval, showing LATS can be used with more advanced LLMs for higher performance.

\subsection{Webshop}

For a complex decision-making environment with practical applications, we consider WebShop \citep{yao2022webshop}, an online shopping environment composed of a website with 1.18M real-world products and 12k human instructions. Agents must navigate a website through a variety of commands to purchase an item matching a user specification. We use the preconstructed action space of search and click commands and browser feedback and reflections for the observation. The performance is gauged using two metrics: an average score, reflecting the percentage of user-specified attributes met by the selected product, and a success rate, indicating the frequency with which the chosen product fulfills all given conditions. We compare against acting-based prompting methods and RL-based approaches. We evaluate on 50 instructions, expand  children for LATS, and set  for LATS, ReAct best of , and Reflexion. More details and prompts are in Appendix~\ref{sec:envdetail} and~\ref{sec:webshop_prompt}.

\textbf{Results.} We find in Tab. \ref{table:webshop} that GPT-3.5 with ReAct is competitive to imitation learning, and can exceed reinforcement learning techniques with stronger prompting strategies. Sampling  trajectories with ReAct and Reflexion results in a similar performance, suggesting the semantic feedback is not as helpful in complex environments like WebShop. Indeed like in \citet{shinn2023reflexion}, we find that generated reflections are often generic and do not provide useful feedback, resulting in a tendency for the agent to become stuck in local minima. However, using LATS indeed results in a noticeable improvement, indicating a more effective exploration for the same number of iterations.

\subsection{Additional Observations}

\begin{table}[t]
  \centering
  \begin{minipage}{.48\linewidth}
    \begin{tabular}{c | cc}
    \toprule
    Method & Score & SR \\
    \midrule
    ReAct {\scriptsize~\citep{yao2023react}} & 53.8 & 28.0 \\
    ReAct (best of k) & 59.1 & 32.0 \\
    Reflexion {\scriptsize~\citep{shinn2023reflexion}} & 64.2 & 35.0 \\
    LATS & \textbf{75.9} & \textbf{38.0} \\
    \midrule
    IL & 59.9 & 29.1 \\
    IL+RL & 62.4 & 28.7 \\
    Fine-tuning {\scriptsize~\citep{furuta2023multimodal}} & 67.5 & 45.0 \\
    \midrule
    Expert & 82.1 & 59.6 \\
    \bottomrule
    \end{tabular}
    \caption{
      Score and success rate (SR) on Webshop. Table is separated into prompting, RL-based training, and human performance. For the same number of iterations, LATS improves both score and success rate, and surpasses RL-based training. IL/IL+RL taken from \cite{yao2022webshop}.
    }
    \label{table:webshop}
  \end{minipage}
  \hfill
  \begin{minipage}{.5\linewidth}
    \begin{tabular}{l | c}
    \toprule
    Prompt Method & HotPotQA (EM) \\
    \midrule
    ToT (ReAct) & 0.39 \\
    RAP (ReAct) & 0.54 \\ 
    LATS (No LM Heuristic) & 0.37  \\
    LATS (DFS) & 0.42  \\
    LATS (No Reflection) & 0.56  \\
    LATS & 0.61 \\
    \bottomrule
    \end{tabular}
    \caption{Ablation results on LATS and baseline variants in HotPotQA; we use ReAct as the base prompt and sample  children and  maximum trajectories. LATS requires every component and operation for optimal performance.}
    \label{table:abl}
  \end{minipage}
  \vspace{-0.2in}
\end{table}
 
We also conduct additional experiments on HotPotQA to demonstrate the effect of each component of LATS. We also design a version of ToT and RAP with ReAct prompt and can handle external observations. We use HotPotQA as our setup incorporates both reasoning (through thoughts) and acting (through API calls); the results are shown in Tab.~\ref{table:abl}. More ablations for token consumption on HotPotQA are in Tab.~\ref{tab:cost} in Appendix~\ref{sec:abl}. Note that baselines generally perform worse than the reasoning-only setting of HotPotQA, which indicates that the acting-based setting is more challenging and adaption of search algorithms to decision-making scenarios is non-trivial.

\textbf{Self-reflection.} We use self-reflection to provide additional semantic signals for the agent. We observe a  performance drop when removed from LATS, suggesting this is useful. This is a smaller gain Reflexion \citep{shinn2023reflexion} observes over ReAct \citep{yao2023react} as shown in Tab.~\ref{table:hotpot}, suggesting overlap between the types of questions where there is an improvement with self-reflection and search. This variant outperforms RAP-ReAct, reflecting our improvements to MCTS.

\textbf{Search Algorithm.} MCTS is a more principled search algorithm than variants like A* or DFS search and the basis for observed performance gains. We observe the effects of using DFS, and incorporate the LM-based heuristic used in ToT \citep{yao2023tree} in which branches with low values are pruned. This removes the selection and backpropagation operations, and we observe a  drop in performance when sampling the same number of nodes, but outperforms ToT-ReAct.

\section{Conclusion}

In this work, we introduce Language Agent Tree Search (LATS), the first framework to unify planning, acting, and reasoning for enhanced LLM problem solving. By deliberately constructing trajectories with search algorithms, incorporating external feedback, and enabling agents to learn from experience, LATS addresses key limitations of prior prompting techniques. Our evaluations demonstrate the ability of LATS to harness LLM capabilities for a variety of decision-making tasks while keeping its reasoning ability without additional training. The proposed synergies between search, interaction, and reflection offer a versatile approach to autonomous decision-making, highlighting the potential of LLMs as generalist agents. A full discussion of the limitations and broader impacts is in  Appendix~\ref{sec:discussion}.


\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}

\newpage

\section{Appendix}
\appendix

The appendix is organized as follows. First in Sec.~\ref{sec:pseudocode}, we show the pseudocode of our proposed algorithm, LATS; then in Sec.~\ref{sec:discussion}, we provide further discussion of our method and its limitations, future direction and broader impact; then in Sec.~\ref{sec:abl} we provide additional experimental results; then in Sec.~\ref{sec:envdetail}, we specify the environment details in our experiments; finally, we list our prompts used for the three environments in Sec.~\ref{sec:hotpotqa_prompt} (HotPotQA), Sec.~\ref{sec:programming_prompt} (Programming) and Sec.~\ref{sec:webshop_prompt} (Webshop) respectively.

\section{LATS Pseudocode}
\label{sec:pseudocode}
Alg.~\ref{alg:mcts} shows the pseudocode of our algorithm LATS. Nodes are stored explicitly in the memory. Unless otherwise specified, in all experiments we use  and .
\begin{algorithm}[ht]
\centering
\caption{ \label{alg:mcts}}
\begin{minipage}{0.9\linewidth} 
\small
\begin{algorithmic}
    \Require Initial state , action generator , value function {}, reflection generator , number of generated actions , depth limit , number of roll-outs , context , and exploration weight 
    \State Initialize action space , observation space 
    \State Initialize the state-action value function  and visit counter  to zero
    \For {}
        \For {}
            \If { not\ terminal} \Comment{Expansion \& Simulation}
                \For {}
                \State Sample 
                \State Get  from environment, , 
                \State Evaluate 
                \Comment Evaluation
                \State 
                \State Add  to children
                \EndFor
            \EndIf
            \If { is terminal}
                \Comment{Reflection}
                \State Get  from environment
                \If { not success}
                    \State 
                    \State 
                \EndIf
            \EndIf
            \State  \Comment{Selection}
            \State 
            \State \algorithmicif\ { is an output action}\ \algorithmicthen\ \textbf{break}
        \EndFor
        \State  the actual number of steps
        \For {} \Comment{Backpropagation}
            \State {}
        \EndFor
    \EndFor
\end{algorithmic}
\end{minipage}
\end{algorithm}

\section{Discussion}
\label{sec:discussion}
\textbf{Limitations.} Although LATS can improve reasoning and decision-making, this arrives at a higher computational cost {relative to simpler prompting methods like ReAct or Reflexion}. The search process takes more time than standard prompting or simpler techniques, and requires greater inference costs. {While such an issue is mitigated by the fact that the number of nodes  expanded at every step provides a natural trade-off between performance and efficiency (setting  makes the method as effecient as ReAct with multiple trials or CoT-SC)}, in practice we recommend using LATS for difficult tasks like programming or for situations where performance is prioritized over efficiency. We hope that continued advancements in LLMs will reduce costs and increase the practicality of LATS.

{
Additionally, the benchmarks we use in this paper are relatively simple and focused on decision-making, compared to the complexity of real-world interactive environments. In addition, some environments might not easily support rollbacks to previous states. However, the design of LATS is flexible and can be adjusted to various resource constraints. Using planning-based prompting methods like LATS in environments like Minecraft \citep{fan2022minedojo} and more reasoning benchmarks would be interesting avenues for future work. 
}

\textbf{Broader impact.} LATS is a framework that enhances LLM performance through interactions with an environment. This improvement in autonomous decision-making may facilitate harmful uses of LLMs. Alternatively, LATS enhances interpretability and the potential for greater alignment, as it generates understandable, high-level linguistic reasoning and actions through several rounds of decision-making and reflection, rather than relying on implicit, low-level token values.
{
\section{Ablations}\label{sec:abl}

\begin{figure}[t]
   
    \centering
    \begin{minipage}{.48\textwidth}
        \centering
        \begin{table}[H]
            \begin{tabular}{c|c}
                \toprule
                \textbf{Prompt Method} & \textbf{HotpotQA (EM)} \\
                \midrule
                LATS (w=0.5) & 0.55 \\
                LATS (w=2.0) & 0.61 \\
                LATS (d=4) & 0.58 \\
                LATS (CoT) & 0.60 \\
                LATS (No LM Heuristic) & 0.37 \\
                LATS & 0.61 \\ 
                \bottomrule
            \end{tabular}
    \caption{Ablation results on LATS and baseline variants in HotPotQA measured by Exact Match (EM). We test different depth , exploration factor , and versions of LATS using CoT and without the LM value function. We sample  and  trajectories.}
        \end{table}

        \label{tab:hotpotqa}
    \end{minipage}\hspace{4pt}
    \begin{minipage}{.48\textwidth}
        \centering
         \includegraphics[width=0.95\linewidth]{figures/k.png}
        \vspace{-0.1in}
        \caption{ Performance over successive iterations on HumanEval with GPT-3.5.}
        \label{fig:k}
    \end{minipage}
\end{figure}


In this section, we ablate various designs of LATS. Experiments are conducted on HotPotQA with a maximum of  trajectories and sampling size of  and HumanEval with a maximum of  trajectories and sampling size of . The result for HotPotQA is shown in Tab.~\ref{table:abl} and HumanEval in Fig.~\ref{fig:k}.

\textbf{Exploration weight.} We find that there is lower performance on HotPotQA when the exploration weight  in the selection formula is decreased to , suggesting that this reduces the effectiveness of the search. Increasing  to  does not lead to a performance improvement, but we tend to observe faster convergence. The optimal setting depends on the particular environment and complexity of the state space.

\textbf{Depth.} In our main experiments we use a maximum depth of  on HotPotQA for all methods, following previous work \citep{yao2023react}. We ablate the effect on LATS after reducing it to . This results in only a slight drop in performance. We find that most questions can be answered within four steps, and using a greater number of steps tends to force the agent into local minima and rarely improves success.

\textbf{LM value function.} The LM value function scores states based on expected future reward. Without this heuristic, the only signal to guide search would be from environment rewards for completed trajectories, which are scarce and often binary. When we remove the evaluation operation, we observe a dramatic  drop in performance.

\textbf{Performance over time.} To see the effects of increasing the number of trajectories sampled, we change  to different values. We conduct this experiment on HumanEval, which has a more noticeable difference due to sampling less trajectories. The results are shown in Fig. \ref{fig:k}, in which LATS scales better with more iterations than Reflexion.

\textbf{Sample complexity and Token cost.} One possible concern of LATS is that the tree-structured search might consume much more tokens than existing methods. To further study the computational cost of LATS compared to prior methods, we examine the sample complexity (i.e. asymptotic token cost) of all methods considered in this paper, and count the average number of nodes expanded by our method and other tree-structured methods (ToT and RAP) upon successful search on HotPotQA. We present the results in Tab.~\ref{tab:cost}; the result shows that our method has the same sample complexity as other tree-based search methods, and has less average number of nodes expanded upon success, which indicates less token cost. The token cost gap will be even larger when taking failed trajectories into account, since our method has higher success rate and reaches computational budget limit less often.

\begin{table}[h]
    
    \small
    \centering
    \begin{tabular}{c|c|c|c}
        Method & Performance () & Sample complexity () & Avg. \#nodes upon success () \\
        \hline
         ReAct (Best ) &  &  & N/A  \\
         CoT-SC () &  &  & N/A  \\
         LATS () &  &  & N/A  \\
         ToT (ReAct) &  &  &   \\
         RAP (ReAct) &  &  &   \\
         LATS () &  &  & \\
    \end{tabular}
    \caption{ The performance, sample complexity of different methods and average number of nodes expanded upon success by methods with tree-based search.  is the number of children nodes expanded at every step and  is the number of trajectories. Our method has the same sample complexity as other methods with tree-based search and expands less nodes upon success, which indicates lower token cost.}
    \label{tab:cost}
\end{table}
}

\section{Environment Details}
\label{sec:envdetail}
\subsection{HotPotQA}

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/qualitative.pdf}
    \caption{{Example trajectories on} HotPotQA for ReAct (left) and LATS (right). LATS can sample more actions and avoid failure from previous mistakes by evaluating states with an LM to guide the search toward promising areas of the tree.}
    \label{fig:qualitative}
    \vspace{-0.1in}
\end{figure}

HotPotQA \citep{yang2018hotpotqa} is a question-answering dataset that requires reasoning over multiple supporting documents to answer questions. It contains 113k Wikipedia-based question-answer pairs crafted by crowdworkers to be diverse, multi-hop, and explainable. Questions cover a range of types like entities, locations, dates, and comparison of shared properties between two entities. Crowdworkers also provide supporting facts from the documents that justify the answer. We use the HotPotQA benchmark setting with all the Wikipedia paragraphs to test retrieval. We use a randomly selected subset of 100 questions for our experiments and a maximum depth limit of 6. {Fig.~\ref{fig:qualitative} illustrates how ReAct and LATS work on an example task of HotPotQA, and gives a qualitative example on how LATS outperforms ReAct on the task. }

\textbf{Action Space.} We adopt the Wikipedia web API proposed in \citet{yao2023react}, with three types of actions to support interactive information retrieval: 

(1) \textbf{\texttt{search}}[\texttt{entity}], which returns the first 5 sentences from the corresponding \texttt{entity} wiki page if it exists, or else suggests top-5 similar entities from the Wikipedia search engine, 

(2) \textbf{\texttt{lookup}}[\texttt{string}], which returns the next sentence in the page containing \texttt{string}, 

(3) \textbf{\texttt{finish}}[\texttt{answer}], which finishes the current task with \texttt{answer}.

These API calls and free-form thoughts form the action space for this environment.

\subsection{Programming}

The HumanEval dataset \citep{chen2021evaluating} is a collection of 164 handwritten programming problems introduced to evaluate the functional correctness of models for synthesizing programs from natural language descriptions. Each problem includes a function signature, docstring description, reference implementation, and multiple unit tests, with an average of 7.7 tests per problem. The programming tasks assess comprehension of natural language, reasoning, algorithms, and basic mathematics, at a difficulty level comparable to simple software interview questions. Pass rates are evaluated with the pass@k metric, where k samples are generated per problem and a problem is considered solved if any sample passes all tests. We use all 164 problems for our experiments and a maximum depth limit of 8.

The Mostly Basic Programming Problems (MBPP) \cite{austin2021program} benchmark contains 974 short Python functions designed to evaluate program synthesis techniques. The dataset was constructed by crowdsourcing from workers with basic Python knowledge. Each data point consists of a natural language description of a programming task, a reference solution implementation, and three test cases for functional correctness. The natural language prompts are typically short, one-sentence descriptions. Solutions cover common programming constructs including mathematical operations, list processing, string manipulation, and usage of the Python standard library. On average, solutions are 6.8 lines of code. The dataset is also supplemented with an additional set of 426 problems that were manually verified for unambiguous specifications, standard function signatures, and accurate test cases. We use a randomly selected subset of 397 problems for our experiments.

\subsection{WebShop}

WebShop \citep{yao2022webshop} is an interactive web-based environment designed to evaluate agents on grounded language understanding and decision-making. It simulates an e-commerce shopping task by providing agents with over 1 million real-world products scraped from Amazon, spanning 5 categories and 113 subcategories. These products contain rich linguistic information, with an average text length of 262 words and a vocabulary size of 224k. In addition, there are over 800k unique product options available for customization. The environment renders webpages in two modes: HTML mode provides pixel-level observations with interactive elements, while simple mode converts the raw HTML into a structured text observation more amenable for training agents. The action space consists of query searches and button clicks, which transition between 4 page types: search, results, item and item-detail. Instructions are crowdsourced natural language specifying product attributes and options, with a total of 12k collected. Automatic rewards are computed by comparing the product purchased by the agent against the attributes and options specified in the instruction, using both lexical matching and semantic similarity metrics. 
\\

\begin{table}[ht]
    \centering
\begin{tabular}{lll}
\toprule
     Type &  Argument &  State  Next State \\
\midrule
    \texttt{search} & [\textit{Query}] & Search  Results\\ 
    \texttt{choose} & Back to search &   Search\\
    \texttt{choose} & Prev/Next page & Results  Results\\
    \texttt{choose} & [\textit{Product title}] & Results  Item\\
    \texttt{choose} & [\textit{Option}] & Item  Item\\
    \texttt{choose} & Desc/Overview & Item  Item-Detail\\
    \texttt{choose} & Previous & Item-Detail  Item\\
    \texttt{choose} & Buy & Item  Episode End\\
\bottomrule
\end{tabular}
\vspace{5pt}
\caption{Action space of webshop.}
\label{table:action_space_}
\end{table}

There are two evaluation metrics used in WebShop: (1) \textbf{Task Score}: defined as , which captures the average reward obtained across episodes; and (2) \textbf{Success Rate (SR)} defined as the portion of instructions where . The reward is calculated based on the number of attributes satisfied by the selected item. We use 50 environments for our experiments and a maximum depth limit of 15.

\section{HotPotQA Prompts}
\label{sec:hotpotqa_prompt}
\subsection{Base Acting Prompt}

Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation, and Action can be three types: 

(1) Search[entity], which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it will return some similar entities to search.

(2) Lookup[keyword], which returns the next sentence containing keyword in the current passage.

(3) Finish[answer], which returns the answer and finishes the task.

After each observation, provide the next Thought and next Action. 

Here are some examples:

Question: Which magazine was started first Arthur's Magazine or First for Women?

Thought 1: I need to search Arthur's Magazine and First for Women, and find which was started first.

Action 1: Search[Arthur's Magazine]

Observation 1: Arthur's Magazine (1844\u00e2\u0080\u00931846) was an American literary periodical published in Philadelphia in the 19th century. Edited by Timothy Shay Arthur, it featured work by Edgar A. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G. Spear, and others.[1][2] In May 1846 it was merged into Godey's Lady's Book.[3]

Thought 2: Arthur's Magazine was started in 1844. I need to search First for Women next.

Action 2: Search[First for Women]

Observation 2: First for Women is a woman's magazine published by Bauer Media Group in the USA.[1] The magazine was started in 1989. It is based in Englewood Cliffs, New Jersey.[2] In 2011 the circulation of the magazine was 1,310,696 copies.[3]

Thought 3: First for Women was started in 1989. 1844 (Arthur's Magazine) < 1989 (First for Women), so Arthur's Magazine was started first.

Action 3: Finish[Arthur's Magazine]

(examples)

You have attempted to answer the following question before and failed. The following reflection(s) give a plan to avoid failing to answer the question in the same way you did previously. Use them to improve your strategy of correctly answering the given question.

(trajectories)

(input)

\subsection{Base Reasoning Prompt}

Solve a question answering task by having a Thought, then Finish with your answer. Thought can reason about the current situation. Finish[answer] returns the answer and finishes the task. You will be given context that you should use to help you answer the question. Start your response with either Action or an indexed Thought

Here are some examples:

Question: What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into? 

Let's think step by step.

Thought 1: The eastern sector of Colorado orogeny extends into the High Plains. 

Thought 2: High Plains rise in elevation from around 1,800 to 7,000 ft

Thought 3: The answer is 1,800 to 7,000 ft.

Action: Finish[1,800 to 7,000 ft]

(examples)

Previous trial:
(trajectories)

(input)

\subsection{Value Function Prompt}

Analyze the trajectories of a solution to a question answering task. The trajectories are labeled by environmental observations about the situation, thoughts that can reason about the current situation and actions that can be three types: 

(1) Search[entity], which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it will return some similar entities to search.

(2) Lookup[keyword], which returns the next sentence containing keyword in the current passage.

(3) Finish[answer], which returns the answer and finishes the task.

Given a question and a trajectory, evaluate its correctness and provide your reasoning and analysis in detail. Focus on the latest thought, action, and observation. Incomplete trajectories can be correct if the thoughts and actions so far are correct, even if the answer is not found yet. Do not generate additional thoughts or actions. Then at the last line conclude "Thus the correctness score is {s}", where s is an integer from 1 to 10.

Question: Which magazine was started first Arthur's Magazine or First for Women?

Thought 1: I need to search Arthur's Magazine and First for Women, and find which was started first.

Action 1: Search[Arthur's Magazine]

Observation 1: Arthur's Magazine (1844\u00e2\u0080\u00931846) was an American literary periodical published in Philadelphia in the 19th century. Edited by Timothy Shay Arthur, it featured work by Edgar A. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G. Spear, and others.[1][2] In May 1846 it was merged into Godey's Lady's Book.[3]

This trajectory is correct as it is reasonable to search for the first magazine provided in the question. It is also better to have simple searches corresponding to a single entity, making this the best action.

Thus the correctness score is 10

(other examples)

(failed trajectories)

(context)

\subsection{Reflection Prompt}


Analyze the trajectories of a solution to a question answering task. The trajectories are labeled by environmental observations about the situation, thoughts that can reason about the current situation and actions that can be three types: 

(1) Search[entity], which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it will return some similar entities to search.

(2) Lookup[keyword], which returns the next sentence containing keyword in the current passage.

(3) Finish[answer], which returns the answer and finishes the task.

Given a question and a trajectory, evaluate its correctness and provide your reasoning and analysis in detail. Focus on the latest thought, action, and observation. Incomplete trajectories can be correct if the thoughts and actions so far are correct, even if the answer is not found yet. Do not generate additional thoughts or actions. Then at the last line conclude "Thus the correctness score is {s}", where s is an integer from 1 to 10.

Question: Which magazine was started first Arthur's Magazine or First for Women?

Thought 1: I need to search Arthur's Magazine and First for Women, and find which was started first.

Action 1: Search[Arthur's Magazine]

Observation 1: Arthur's Magazine (1844\u00e2\u0080\u00931846) was an American literary periodical published in Philadelphia in the 19th century. Edited by Timothy Shay Arthur, it featured work by Edgar A. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G. Spear, and others.[1][2] In May 1846 it was merged into Godey's Lady's Book.[3]

This trajectory is correct as it is reasonable to search for the first magazine provided in the question. It is also better to have simple searches corresponding to a single entity, making this the best action.

Thus the correctness score is 10

(other examples)

(failed trajectories)

(context)

\section{Programming Prompts}
\label{sec:programming_prompt}
\subsection{HumanEval function implementation example}
Sample function signature:\\
\begin{lstlisting}
def minSubArraySum(nums):
    Given an array of integers nums, find the minimum sum of any
    non-empty sub-array of nums.
    Example
    minSubArraySum([2, 3, 4, 1, 2, 4]) == 1
    minSubArraySum([-1, -2, -3]) == -6
\end{lstlisting}

Sample function body implementation:\\
\begin{lstlisting}
    min_sum = float('inf')
    for i in range(len(nums)):
        current_sum = 0
        for j in range(i, len(nums)):
            current_sum += nums[j]
            if current_sum < min_sum:
                min_sum = current_sum
    return min_sum
\end{lstlisting}

\subsection{Base Acting/Reasoning Prompt}

You are an AI Python assistant. You will be given your previous implementation of a function, a series of unit tests results, and your self-reflection on your previous implementation. Write your full implementation (restate the function signature).

Example 1:

[previous impl]:

\begin{lstlisting}
def add(a: int, b: int) -> int:
    """
    Given integers a and b, return the total value of a and b.
    """
    return a - b
\end{lstlisting}

[unit test results from previous impl]:

Tested passed:

Tests failed:

assert add(1, 2) == 3 \# output: -1

assert add(1, 2) == 4 \# output: -1

[reflection on previous impl]:

The implementation failed the test cases where the input integers are 1 and 2. The issue arises because the code does not add the two integers together, but instead subtracts the second integer from the first. To fix this issue, we should change the operator from `-` to `+` in the return statement. This will ensure that the function returns the correct output for the given input.

[improved impl]:

\begin{lstlisting}
def add(a: int, b: int) -> int:
    """
    Given integers a and b, return the total value of a and b.
    """
    return a + b
\end{lstlisting}

\subsection{Reflection Prompt}

You are a Python programming assistant. You will be given a function implementation and a series of unit test results. Your goal is to write a few sentences to explain why your implementation is wrong as indicated by the tests. You will need this as guidance when you try again later. Only provide the few sentence description in your answer, not the implementation. You will be given a few examples by the user.

Example 1:

[previous impl]:

\begin{lstlisting}
    def add(a: int, b: int) -> int:
        """
        Given integers a and b, return the total value of a and b.
        """
    return a - b
\end{lstlisting}

[unit test results from previous impl]:

Tested passed:

Tests failed:

assert add(1, 2) == 3 \# output: -1

assert add(1, 2) == 4 \# output: -1

[reflection on previous impl]:

The implementation failed the test cases where the input integers are 1 and 2. The issue arises because the code does not add the two integers together, but instead subtracts the second integer from the first. To fix this issue, we should change the operator from `-` to `+` in the return statement. This will ensure that the function returns the correct output for the given input.

\subsection{Test Case Generation Prompt}

You are an AI coding assistant that can write unique, diverse, and intuitive unit tests for functions given the signature and docstring.

(input)

\section{WebShop Prompts}
\label{sec:webshop_prompt}
\subsection{Acting Prompt}

Webshop 

Instruction:  

i would like a 3 ounce bottle of bright citrus deodorant for sensitive skin, and price lower than 50.00 dollars 

[Search]  
\\

Action: search[3 ounce bright citrus deodorant sensitive skin]

Observation: 

[Back to Search] 

Page 1 (Total results: 50) 

[Next
] 
[B078GWRC1J] 

Bright Citrus Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic Calendula 3-Ounce 

\10.99 

[B08KBVJ4XN] 

Barrel and Oak - Aluminum-Free Deodorant, Deodorant for Men, Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar \& Patchouli Blend, Gentle on Sensitive Skin (Mountain Sage, 2.7 oz, 2-Pack) 

\10.99 

Rating: N.A. 

[Description] 

[Features] 

[Reviews] 

[Buy Now]  
\\

Action: think[For 3 ounce bottle of bright citrus deodorant for sensitive skin, the item has options 'bright citrus' and '3 ounce (pack of 1)' and seems good to buy.]

Observation: OK.
\\

Action: click[bright citrus]

Observation: You have clicked bright citrus. 
\\

Action: click[3 ounce (pack of 1)]

Observation: You have clicked 3 ounce (pack of 1). 
\\

Action: click[Buy Now]

\subsection{Value Function Prompt}

Given an item to purchase and a trajectory that aims to buy an item that exactly matches the specification, which corresponds to the ideal score of 1.0, analyze the following trajectory, then at the last line conclude "Thus the correctness score is {s}", where s is an integer from 1 to 10.

Here are some examples
\\

Webshop

Instruction:

i am looking for dairy free and apple variety pack of chips, and price lower than 30.00 dollars

[Search]
\\

Action: search[dairy free and apple variety pack of chips]

Observation:

[Back to Search]

Page 1 (Total results: 50)

[Next]

[B07HRFJWP8]

Enjoy Life Foods Soft Baked Ovals, Breakfast Bars, Nut Free Bars, Soy Free, Dairy Free, Non GMO, Gluten Free, Vegan, Variety Pack, 4 Boxes (20 Bars Total)

\21.49

[B008D2X8C4]

Enjoy Life Lentil Chips Variety Pack, Dairy Free Chips, Soy Free, Nut Free, Non GMO, Vegan, Gluten Free, 24 Bags (0.8 oz)

\100.0

Rating: N.A.

[Description]

[Features]

[Reviews]

[Buy Now]
\\

Action: think[The item has options 'variety pack' and '0.8 ounce (pack of 24)' and seems good to buy.]

Observation: OK.
\\

Action: click[variety pack]

Observation: You have clicked variety pack.
\\

Action: click[0.8 ounce (pack of 24)]

Observation: You have clicked 0.8 ounce (pack of 24).
\\

Action: click[Buy Now]
\\

Reflection: In this attempt, I was unsuccessful. I accidentally bought a product that was \30. Either way, the initial search results were not good. Next time, I will do search["variety pack of chips"] and then check if the results meet the dairy free and the \100.0

[B01KMHY5PG]

Enjoy Life Soft Baked Chewy Bars, Variety Pack, Nut Free Bars, Soy Free, Dairy Free, Gluten Free, 6 Boxes (30 Total Bars)

\100.0
\\

Action: think[The first two items are not what I want. The third item is the dairy free and apple variety pack of chips I want.]

Observation: OK.
\\

Action: click[B008D2X8C4]

Observation:

[Back to Search]

[Prev]

flavor name [dill and sour cream][garlic \& parmesan][light sea salt][margherita pizza][thai chili lime][variety pack]
size [0.8 ounce (pack of 24)][4 ounce (pack of 12)]

Price: \100, which is more than my budget of \30 budget constraints. I will continue to refine my searches so that I can find more products.
\\

(input)
\\

Reflection:













 
\end{document}
