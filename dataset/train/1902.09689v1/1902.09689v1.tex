\subsection{Ordinary differential equations}


We first give a brief overview of the ordinary differential equations (ODEs), a special kind of dynamical systems that involves a single variable, time $t$ in this case.
Consider the first-order ODE
\begin{equation}
    \label{eq:generic_ODE}
    \vh'(t)
    = 
    f(\vh(t)),
\end{equation}
for time $t \geq 0$,
where $\vh(t) \in \sR^n$ and $f: \sR^n \to \sR^n$. 
Together with a given initial condition $\vh(0)$, the problem of solving for the function $\vh(t)$ is called the \textit{initial value problem}. For most ODEs, it is impossible to find an analytic solution. Instead, numerical methods relying on discretization are commonly used to approximate the solution.  
The \textit{forward Euler method} is probably the best known and simplest numerical method for approximation. 
One way to derive the forward Euler method is to approximate the derivative on the left-hand side of \Eqref{eq:generic_ODE} by a finite difference, and evaluate the right-hand side at $\vh_{t-1}$:
\begin{equation}
    \frac
    {\vh_{t} - \vh_{t-1}}
    {\epsilon}
    =
    f(\vh_{t-1}).
\end{equation}
Note that for the approximation to be valid,  $\epsilon>0$ should be small by the definition of the derivative. One can easily prove that the forward Euler method converges linearly w.r.t. $\epsilon$, assuming $f(\vh)$ is Lipschitz continuous on $\vh$ and that the eigenvalues of the Jacobian of $f$ have negative real parts.   
Rearranging it, we have the forward Euler method for a given initial value
\begin{equation}
    \vh_{t}
    =
    \vh_{t-1} + \epsilon f(\vh_{t-1}),
    \quad
    \vh_0 = \vh(0).
\end{equation}
Geometrically, each forward Euler step takes a small step along the tangential direction to the exact trajectory starting at $\vh_{t-1}$.
As a result, $\epsilon$ is usually referred to as the step size.

As an example, consider the ODE
$$\vh'(t) = \tanh\left(\mW\vh(t)\right).$$
The forward Euler method approximates the solution to the ODE iteratively as
$$
    \vh_{t}
    =
    \vh_{t-1} + \epsilon \tanh(\mW \vh_{t-1}),    
$$
which can be regarded as a recurrent network without input data.  Here $\vh_t$ is the hidden state at the $t$-th step, $\mW$ is a model parameter, and $\epsilon$ is a hyperparameter.
This provides a general framework of designing recurrent network architectures by discretizing ODEs.
As a result, we can design ODEs that possess desirable properties by exploiting the theoretical successes of dynamical systems, and the resulting recurrent networks will inherit these properties.
Stability is one of the important properties to consider, which we will discuss in the next section.
It is worth mentioning that the ``skip connection'' in this architecture resembles the residual RNN~\citep{yue2018residual} and the Fourier RNN~\citep{zhang2018learning}, which are proposed to mitigate the vanishing and exploding gradient issues.



\subsection{Stability of ordinary differential equations: AntisymmetricRNNs}
\label{sec:ODE_theory}



In numerical analysis, stability theory addresses the stability of solutions of ODEs under small perturbations of initial conditions. In this section, we are going to establish the connections between the stability of an ODE and the trainability of the RNNs by discretizing the ODE, and design a new RNN architecture that is stable and capable of capturing long-term dependencies.


An ODE solution is stable if the long-term behavior of the system does not depend significantly on the initial conditions. 
A formal definition is given as follows.

\begin{definition} (Stability)
A solution $\vh(t)$ of the ODE in \Eqref{eq:generic_ODE} with initial condition $\vh(0)$ is stable if
for any $\epsilon>0$, there exists a $\delta>0$ such that any other solution $\tilde{\vh}(t)$ of the ODE with initial condition $\tilde\vh(0)$ satisfying 
$|\vh(0) - \tilde{\vh}(0)| \leq \delta$ also satisfies $|\vh(t) - \tilde{\vh}(t)| \leq \epsilon$, for all $t \geq 0$. 
\end{definition}
In plain language, given a small perturbation of size $\delta$ of the initial state, the effect of the perturbation on the subsequent states is no bigger than $\epsilon$.
The eigenvalues of the Jacobian matrix play a central role in stability analysis.
Let $\mJ(t) \in \sR^{n \times n}$ be the Jacobian matrix of $f$, and $\lambda_i(\cdot)$ denotes the $i$-th eigenvalue.

\begin{proposition}
The solution of an ODE is stable if
\begin{equation}
    \max_{i = 1, 2, \ldots, n}
    Re(\lambda_i(\mJ(t))) 
    \leq
    0,
    \quad 
    \forall t \geq 0,
\end{equation}
where $Re(\cdot)$ denotes the real part of a complex number.
\end{proposition}
A more precise proposition that involves the kinematic eigenvalues of $\mJ(t)$ is given in \cite{ascher1994numerical}.
Stability alone, however, does not suffice to capture long-term dependencies. As argued in \cite{haber2017stable}, $Re(\lambda_i(\mJ(t))) \ll 0$ results in a lossy system;
the energy or signal in the initial state is dissipated over time.
Using such an ODE as the underlying dynamical system of a recurrent network will lead to catastrophic forgetting of the past inputs during the forward propagation.  
Ideally, 
\begin{equation}
    \label{eq:ODE_zero_re_cond}
    Re(\lambda_i(\mJ(t))) \approx 0, 
    \quad 
    \forall i=1, 2, \ldots, n,
\end{equation}
a condition we referred to as the \textit{critical criterion}.
Under this condition, the system preserves the long-term dependencies of the inputs while being stable. 

\textbf{Stability and Trainability.} Here we connect the stability of the ODE to the trainability of the RNN produced by discretization. 
Inherently, the stability analysis studies the sensitivity of a solution, i.e., how much a solution of the ODE would change w.r.t. changes in the initial condition. 
Differentiating \Eqref{eq:generic_ODE} with respect to the initial state $\vh(0)$ on both sides, we have the following sensitivity analysis (with chain rules):
\begin{equation}
    \frac
    {\mathrm{d}}
    {\mathrm{d} t}
    \left(
    \frac
    {\partial \vh(t)}
    {\partial \vh(0)}
    \right)
    =
    \mJ(t)
    \frac
    {\partial \vh(t)}
    {\partial \vh(0)}
    .
\end{equation}
For notational simplicity, let us define $\mA(t) = {\partial \vh(t)} / {\partial \vh(0)}$, then we have
\begin{equation}
    \frac
    {\mathrm{d} \mA(t)}
    {\mathrm{d} t}
    = 
    \mJ(t)\mA(t),
    \quad
    \mA(0) = \mI.
\end{equation}
Note that this is a linear ODE with solution $\mA(t) = e^{\mJ \cdot t} = \mP e^{\mLambda(\mJ)t} \mP^{-1}$, assuming the Jacobian $\mJ$ does not vary or vary slowly over time (We will later show this is a valid assumption). Here $\mLambda(\mJ)$ denotes the eigenvalues of $\mJ$, and the columns of $\mP$ are the corresponding eigenvectors. See Appendix~\ref{sec:appendix_stability} for a more detailed derivation.
In the language of RNNs, $\mA(t)$ is 
the Jacobian of a hidden state $\vh_t$ with respect to the initial hidden state $\vh_0$.
When the critical criterion is met, i.e., $Re(\mLambda(\mJ)) \approx 0$, the magnitude of $\mA(t)$ is approximately constant in time, thus no exploding or vanishing gradient problems.



With the connection established, we next design ODEs that satisfy the critical criterion. An antisymmetric matrix is a square matrix whose transpose equals its negative;
i.e., a matrix $\mM \in \sR^{n \times n}$ is antisymmetric if $\mM^T = -\mM$.
An interesting property of an antisymmetric matrix $\mM$ is that, the eigenvalues of $\mM$ are all imaginary: 
\[
Re(\lambda_i(\mM)) = 0, \quad \forall i=1,2,\ldots,n,
\]
making antisymmetric matrices a suitable building block of a stable recurrent architecture.


Consider the following ODE
\begin{equation}
    \label{eq:antisymm_ODE}
    \vh'(t)
    =
    \tanh\left((\mW_h - \mW_h^T) \vh(t) 
    + \mV_h \vx(t) 
    + \vb_h \right),
\end{equation}
where
$\vh(t) \in \sR^{n}$,
$\vx(t) \in \sR^{m}$,
$\mW_h \in \sR^{n \times n}$, $\mV_h \in \sR^{n \times m}$ and $\vb_h \in \sR^{n}$.
Note that $\mW_h - \mW_h^T$ is an antisymmetric matrix.
The Jacobian matrix of the right hand side is
\begin{equation}
\label{eq:jacobian_antisymm}
\mJ(t) = \diag\left[\tanh'\left((\mW_h - \mW_h^T) \vh(t) 
+ \mV_h \vx(t) 
+ \vb\right)\right] (\mW_h - \mW_h^T),
\end{equation}
whose eigenvalues are all imaginary, i.e., 
$Re(\lambda_i(\mJ(t))) = 0, \forall i=1, 2, \ldots, n$.
In other words, it satisfies the critical criterion in \Eqref{eq:ODE_zero_re_cond}.
See Appendix~\ref{sec:appendix_proof} for a proof.
The entries of the diagonal matrix in \Eqref{eq:jacobian_antisymm} are the derivatives of the activation function, which are bounded in $[0, 1]$ for sigmoid and hyperbolic tangent. 
In other words, the Jacobian matrix $\mJ(t)$ changes smoothly over time.
Furthermore, since the input and bias term only affect the bounded diagonal matrix, their effect on the stability of the ODE is insignificant compared with the antisymmetric matrix.


A naive forward Euler discretization of the ODE in \Eqref{eq:antisymm_ODE} leads to the following recurrent network
we refer to as the \textit{AntisymmetricRNN}.
\begin{equation}
\label{eq:antisymm_RNN}
\vh_{t}
=
\vh_{t-1}
+
\epsilon
\tanh\left(
(\mW_h - \mW_h^T) \vh_{t-1} + \mV_h \vx_{t} + \vb_h
\right),
\end{equation}
where
$\vh_{t} \in \sR^n$ is the hidden state at time $t$;
$\vx_{t} \in \sR^m$ is the input at time $t$; 
$\mW_h \in \sR^{n \times n}$, $\mV_h \in \sR^{n \times m}$ and $\vb_h \in \sR^{n}$ are the parameters of the network;
$\epsilon > 0$ is a hyperparameter that represents the step size.




Note that the antisymmetric matrix $\mW_h - \mW_h^T$ only has $n(n-1)/2$ degrees of freedom.
When implementing the model, 
$\mW_h$ can be parameterized as a strictly upper triangular matrix, 
i.e., an upper triangular matrix of which the diagonal entries are all zero.
This makes the proposed model more parameter efficient than an unstructured RNN model of the same size of hidden states.


\subsection{Stability of the forward Euler method: Diffusion}

Given a stable ODE, its forward Euler discretization can still be unstable, as illustrated in \Secref{sec:toy_eg}.
The stability condition of the forward Euler method has been well studied and summarized in the following proposition.
\begin{proposition}
\label{prop:forward_Euler_stability}
(Stability of the forward Euler method) 
The forward propagation in \Eqref{eq:antisymm_RNN} is stable if
\begin{equation}
    \max_{i=1, 2, \ldots, n}
    |1 + \epsilon \lambda_i (\mJ_{t})| \leq 1,
\end{equation}
where $|\cdot|$ denote the absolute value or modulus of a complex number and $\mJ_{t}$ is the Jacobian matrix evaluated at $\vh_t$.
\end{proposition}
See \cite{ascher1998computer} for a proof.
The ODE as defined in \Eqref{eq:antisymm_ODE} is however incompatible with the stability condition of the forward Euler method.
Since $\lambda_i (\mJ_{t})$, the eigenvalues of the Jacobian matrix, are all imaginary, $|1 + \epsilon \lambda_i (\mJ_{t})|$ is always greater than 1, which makes the \textit{AntisymmetricRNN} defined in \Eqref{eq:antisymm_RNN} unstable when solved using forward Euler.


One easy way to fix it is to add \textit{diffusion} to the system by subtracting a small number $\gamma > 0$ from the diagonal elements of the transition matrix.
The model thus becomes
\begin{equation}
\label{eq:antisymm_RNN_diff}
\vh_{t}
=
\vh_{t-1}
+
\epsilon
\tanh\left((\mW_h - \mW_h^T - \gamma \mI) \vh_{t-1} + \mV_h \vx_{t} + \vb_h
\right),
\end{equation}
where $\mI$ is the identity matrix of size $n$ and $\gamma > 0$ is a hyperparameter that controls the strength of diffusion.
By doing so, the eigenvalues of the Jacobian have slightly negative real parts. 
This modification improves the stability of the numerical method as demonstrated in \Secref{sec:toy_eg}.





\subsection{Gating mechanism}
\label{sec:gating_mechanism}

Gating is commonly employed in RNNs.
Each gate is often modeled as a single layer network taking the previous hidden state $\vh_{t-1}$ and data $\vx_t$ as inputs, followed by a sigmoid activation.
As an example, LSTM cells make use of three gates, a forget gate, an input gate, and an output gate.
A systematic ablation study suggests that some of the gates are crucial to the performance of LSTM~\citep{jozefowicz2015empirical}.


Gating can be incorporated into AntisymmetricRNN as well.
However, it should be done carefully so that the critical condition in \Eqref{eq:ODE_zero_re_cond} still holds.
We propose the following modification to AntisymmetricRNN, which adds an \textit{input gate} $\vz_t$ to control the flow of information into the hidden states: 
\begin{align}
    \vz_{t} &=  \sigma 
    \left(
    (\mW_h - \mW_h^T - \gamma \mI) \vh_{t-1} + \mV_{z} \vx_{t} + \vb_{z} 
    \right), 
    \nonumber
    \\
    \label{eq:antisymm_RNN_gating}
    \vh_{t}
    &=
    \vh_{t-1}
    +
    \epsilon
    \vz_{t}
    \circ
    \tanh\left((\mW_h - \mW_h^T - \gamma \mI) \vh_{t-1} + \mV_h \vx_{t} + \vb_h \right),
\end{align}
where $\sigma$ denotes the sigmoid function and $\circ$ denotes the Hadamard product.


The effect of $\vz_t$ resembles the input gate in LSTM and the update gate in GRU.
By sharing the antisymmetric weight matrix, the number of model parameters only increases slightly, instead of being doubled.
More importantly, the Jacobian matrix of this gated model has a similar form as that in \Eqref{eq:jacobian_antisymm}, that is, a diagonal matrix multiplied by an antisymmetric matrix (ignoring diffusion).
As a result, the real parts of the eigenvalues of the Jacobian matrix are still close to zero, and the critical criterion remains satisfied.

