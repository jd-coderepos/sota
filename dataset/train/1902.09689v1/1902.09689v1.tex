\subsection{Ordinary differential equations}


We first give a brief overview of the ordinary differential equations (ODEs), a special kind of dynamical systems that involves a single variable, time  in this case.
Consider the first-order ODE

for time ,
where  and . 
Together with a given initial condition , the problem of solving for the function  is called the \textit{initial value problem}. For most ODEs, it is impossible to find an analytic solution. Instead, numerical methods relying on discretization are commonly used to approximate the solution.  
The \textit{forward Euler method} is probably the best known and simplest numerical method for approximation. 
One way to derive the forward Euler method is to approximate the derivative on the left-hand side of \Eqref{eq:generic_ODE} by a finite difference, and evaluate the right-hand side at :

Note that for the approximation to be valid,   should be small by the definition of the derivative. One can easily prove that the forward Euler method converges linearly w.r.t. , assuming  is Lipschitz continuous on  and that the eigenvalues of the Jacobian of  have negative real parts.   
Rearranging it, we have the forward Euler method for a given initial value

Geometrically, each forward Euler step takes a small step along the tangential direction to the exact trajectory starting at .
As a result,  is usually referred to as the step size.

As an example, consider the ODE

The forward Euler method approximates the solution to the ODE iteratively as

which can be regarded as a recurrent network without input data.  Here  is the hidden state at the -th step,  is a model parameter, and  is a hyperparameter.
This provides a general framework of designing recurrent network architectures by discretizing ODEs.
As a result, we can design ODEs that possess desirable properties by exploiting the theoretical successes of dynamical systems, and the resulting recurrent networks will inherit these properties.
Stability is one of the important properties to consider, which we will discuss in the next section.
It is worth mentioning that the ``skip connection'' in this architecture resembles the residual RNN~\citep{yue2018residual} and the Fourier RNN~\citep{zhang2018learning}, which are proposed to mitigate the vanishing and exploding gradient issues.



\subsection{Stability of ordinary differential equations: AntisymmetricRNNs}
\label{sec:ODE_theory}



In numerical analysis, stability theory addresses the stability of solutions of ODEs under small perturbations of initial conditions. In this section, we are going to establish the connections between the stability of an ODE and the trainability of the RNNs by discretizing the ODE, and design a new RNN architecture that is stable and capable of capturing long-term dependencies.


An ODE solution is stable if the long-term behavior of the system does not depend significantly on the initial conditions. 
A formal definition is given as follows.

\begin{definition} (Stability)
A solution  of the ODE in \Eqref{eq:generic_ODE} with initial condition  is stable if
for any , there exists a  such that any other solution  of the ODE with initial condition  satisfying 
 also satisfies , for all . 
\end{definition}
In plain language, given a small perturbation of size  of the initial state, the effect of the perturbation on the subsequent states is no bigger than .
The eigenvalues of the Jacobian matrix play a central role in stability analysis.
Let  be the Jacobian matrix of , and  denotes the -th eigenvalue.

\begin{proposition}
The solution of an ODE is stable if

where  denotes the real part of a complex number.
\end{proposition}
A more precise proposition that involves the kinematic eigenvalues of  is given in \cite{ascher1994numerical}.
Stability alone, however, does not suffice to capture long-term dependencies. As argued in \cite{haber2017stable},  results in a lossy system;
the energy or signal in the initial state is dissipated over time.
Using such an ODE as the underlying dynamical system of a recurrent network will lead to catastrophic forgetting of the past inputs during the forward propagation.  
Ideally, 

a condition we referred to as the \textit{critical criterion}.
Under this condition, the system preserves the long-term dependencies of the inputs while being stable. 

\textbf{Stability and Trainability.} Here we connect the stability of the ODE to the trainability of the RNN produced by discretization. 
Inherently, the stability analysis studies the sensitivity of a solution, i.e., how much a solution of the ODE would change w.r.t. changes in the initial condition. 
Differentiating \Eqref{eq:generic_ODE} with respect to the initial state  on both sides, we have the following sensitivity analysis (with chain rules):

For notational simplicity, let us define , then we have

Note that this is a linear ODE with solution , assuming the Jacobian  does not vary or vary slowly over time (We will later show this is a valid assumption). Here  denotes the eigenvalues of , and the columns of  are the corresponding eigenvectors. See Appendix~\ref{sec:appendix_stability} for a more detailed derivation.
In the language of RNNs,  is 
the Jacobian of a hidden state  with respect to the initial hidden state .
When the critical criterion is met, i.e., , the magnitude of  is approximately constant in time, thus no exploding or vanishing gradient problems.



With the connection established, we next design ODEs that satisfy the critical criterion. An antisymmetric matrix is a square matrix whose transpose equals its negative;
i.e., a matrix  is antisymmetric if .
An interesting property of an antisymmetric matrix  is that, the eigenvalues of  are all imaginary: 

making antisymmetric matrices a suitable building block of a stable recurrent architecture.


Consider the following ODE

where
,
,
,  and .
Note that  is an antisymmetric matrix.
The Jacobian matrix of the right hand side is

whose eigenvalues are all imaginary, i.e., 
.
In other words, it satisfies the critical criterion in \Eqref{eq:ODE_zero_re_cond}.
See Appendix~\ref{sec:appendix_proof} for a proof.
The entries of the diagonal matrix in \Eqref{eq:jacobian_antisymm} are the derivatives of the activation function, which are bounded in  for sigmoid and hyperbolic tangent. 
In other words, the Jacobian matrix  changes smoothly over time.
Furthermore, since the input and bias term only affect the bounded diagonal matrix, their effect on the stability of the ODE is insignificant compared with the antisymmetric matrix.


A naive forward Euler discretization of the ODE in \Eqref{eq:antisymm_ODE} leads to the following recurrent network
we refer to as the \textit{AntisymmetricRNN}.

where
 is the hidden state at time ;
 is the input at time ; 
,  and  are the parameters of the network;
 is a hyperparameter that represents the step size.




Note that the antisymmetric matrix  only has  degrees of freedom.
When implementing the model, 
 can be parameterized as a strictly upper triangular matrix, 
i.e., an upper triangular matrix of which the diagonal entries are all zero.
This makes the proposed model more parameter efficient than an unstructured RNN model of the same size of hidden states.


\subsection{Stability of the forward Euler method: Diffusion}

Given a stable ODE, its forward Euler discretization can still be unstable, as illustrated in \Secref{sec:toy_eg}.
The stability condition of the forward Euler method has been well studied and summarized in the following proposition.
\begin{proposition}
\label{prop:forward_Euler_stability}
(Stability of the forward Euler method) 
The forward propagation in \Eqref{eq:antisymm_RNN} is stable if

where  denote the absolute value or modulus of a complex number and  is the Jacobian matrix evaluated at .
\end{proposition}
See \cite{ascher1998computer} for a proof.
The ODE as defined in \Eqref{eq:antisymm_ODE} is however incompatible with the stability condition of the forward Euler method.
Since , the eigenvalues of the Jacobian matrix, are all imaginary,  is always greater than 1, which makes the \textit{AntisymmetricRNN} defined in \Eqref{eq:antisymm_RNN} unstable when solved using forward Euler.


One easy way to fix it is to add \textit{diffusion} to the system by subtracting a small number  from the diagonal elements of the transition matrix.
The model thus becomes

where  is the identity matrix of size  and  is a hyperparameter that controls the strength of diffusion.
By doing so, the eigenvalues of the Jacobian have slightly negative real parts. 
This modification improves the stability of the numerical method as demonstrated in \Secref{sec:toy_eg}.





\subsection{Gating mechanism}
\label{sec:gating_mechanism}

Gating is commonly employed in RNNs.
Each gate is often modeled as a single layer network taking the previous hidden state  and data  as inputs, followed by a sigmoid activation.
As an example, LSTM cells make use of three gates, a forget gate, an input gate, and an output gate.
A systematic ablation study suggests that some of the gates are crucial to the performance of LSTM~\citep{jozefowicz2015empirical}.


Gating can be incorporated into AntisymmetricRNN as well.
However, it should be done carefully so that the critical condition in \Eqref{eq:ODE_zero_re_cond} still holds.
We propose the following modification to AntisymmetricRNN, which adds an \textit{input gate}  to control the flow of information into the hidden states: 

where  denotes the sigmoid function and  denotes the Hadamard product.


The effect of  resembles the input gate in LSTM and the update gate in GRU.
By sharing the antisymmetric weight matrix, the number of model parameters only increases slightly, instead of being doubled.
More importantly, the Jacobian matrix of this gated model has a similar form as that in \Eqref{eq:jacobian_antisymm}, that is, a diagonal matrix multiplied by an antisymmetric matrix (ignoring diffusion).
As a result, the real parts of the eigenvalues of the Jacobian matrix are still close to zero, and the critical criterion remains satisfied.

