\documentclass[review, sort&compress]{elsarticle}

\usepackage{lineno}
\usepackage[colorlinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{gensymb}
\usepackage{float}
\usepackage{csquotes}
\usepackage{mathtools}  \usepackage{mathrsfs}   \usepackage{tikz}
\usepackage{mdframed}
\usepackage{caption}
\usepackage{multirow}
\usepackage{placeins}
\usepackage{subcaption}

\AtBeginDocument{\hypersetup{
		urlcolor=blue,
		citecolor=green,
		linkcolor=red,
	}}


\modulolinenumbers[5]
\definecolor{bounding_box}{RGB}{77, 121, 255}
\journal{Artificial Intelligence in Medicine}

















\bibliographystyle{elsarticle-num}


\begin{document}
	
	\begin{frontmatter}
		
		\title{AG-CUResNeSt: A Novel Method for Colon Polyp Segmentation}
		
\author[dvs]{Dinh Viet Sang\corref{mycorrespondingauthor}\fnref{equal}}
		\ead{sang.dinhviet@hust.edu.vn}
		
		\author[dvs]{Tran Quang Chung\fnref{equal}}
		\ead{chung.tqcb190214@sis.hust.edu.vn}
		
		\author[dvs]{Phan Ngoc Lan}
		\ead{lan.pn202634m@sis.hust.edu.vn}
		
		\author[2,3]{Dao Viet Hang}
		\ead{daoviethang@hmu.edu.vn}
		
		\author[2,3]{Dao Van Long}
		\ead{bsdaolong@yahoo.com}
		
		\author[4]{Nguyen Thi Thuy}
		\ead{ntthuy@vnua.edu.vn}
		
		\cortext[mycorrespondingauthor]{Corresponding author}
		\fntext[equal]{These authors contributed equally to this work}
		\address[dvs]{Hanoi University of Science and Technology, Hanoi, Vietnam}
		\address[2]{Hanoi Medical University, Hanoi, Vietnam}
		\address[3]{The Institute of Gastroenterology and Hepatology, Hanoi, Vietnam}
		\address[4]{Faculty of Information Technology, Vietnam National University of Agriculture, Hanoi, Vietnam}
		




		
		\newcommand*{\DrawBoundingBox}[1][]{\draw [red, very thick, #1]
			([shift={(-0pt,-0pt)}]current bounding box.south west)
			rectangle
			([shift={(0pt,+0pt)}]current bounding box.north east);
		}
		
		
		\begin{abstract}
			Colorectal cancer is among the most common malignancies and can develop from high-risk colon polyps. Colonoscopy is an effective screening tool to detect and remove polyps, especially in the case of precancerous lesions. However, the missing rate in clinical practice is relatively high due to many factors. The procedure could benefit greatly from using AI models for automatic polyp segmentation, which provide valuable insights for improving colon polyp detection. However, precise segmentation is still challenging due to variations of polyps in size, shape, texture, and color. This paper proposes a novel neural network architecture called AG-CUResNeSt, which enhances Coupled UNets using the robust ResNeSt backbone and attention gates. The network is capable of effectively combining multi-level features to yield accurate polyp segmentation. Experimental results on five popular benchmark datasets show that our proposed method achieves state-of-the-art accuracy compared to existing methods.
		\end{abstract}
		
		\begin{keyword}
			Deep Learning, Attention Mechanism, Polyp Segmentation, Colonoscopy
\end{keyword}
		
	\end{frontmatter}
	


	\section{Introduction}
	
	Colorectal cancer (CRC) is a leading cause of cancer deaths worldwide, with roughly 694,000 fatalities each year \cite{bernal2017comparative}. Most CRC arises from colon polyps, especially the adenomas with high-grade dysplasia \cite{gschwantler2002high}. According to a longitudinal study \cite{corley2014adenoma}, each 1\% of adenoma detection rate increase is associated with a 3\% decrease in the risk of colon cancer. Therefore, the detection and removal of polyps at the early stage are of great importance to prevent CRC. Nowadays, colonoscopy is considered as the gold standard for colon screening and recommended in many guidelines of different societies \cite{issa2017colorectal}.
	Nevertheless, the overloaded healthcare systems in many countries, especially in limited-resource settings, might result in shorter endoscopy duration to guarantee the required number of procedures per day. This factor, combined with low-quality endoscopy systems and experience gaps among endoscopists at different levels of the healthcare system, seriously affects the quality of colonoscopy procedures and increases the risk of missing lesions and inaccurate diagnosis \cite{lee2008adequate, armin2015visibility}. A literature review has shown that the colon polyp missing rate in endoscopies could range from 20-47\% \cite{leufkens2012factors}. Hence, studies to develop computer-aided systems to support endoscopists in providing accurate polyp regions are much needed in both aspects of training endoscopists and application in clinical practice.
	
	Despite the growth of many advanced machine learning and computer vision techniques in recent years, automatic polyp segmentation still a challenging problem. The first challenge is that polyps are often diverse in appearance, such as shape, size, texture, and color. Secondly, as the nature of medical images, the boundary between polyps and their surrounding mucosa, especially in case of flat lesions or unclean bowel preparation, is not always clear during colonoscopy, leading to confusion for segmentation methods. 
	
	There are different approaches to polyp segmentation. Traditional machine learning methods are based on hand-crafted features for image representation  \cite{iwahori2013automatic, silva2014toward}. These methods rely on color, texture, shape, or edge information as extracted features and train classifiers to distinguish polyps from surrounding normal mucosa. However, hand-crafted features are limited in representing polyps due to their high intra-class diversity and low inter-class variation between them and hard negative mucosa regions. Recently, deep neural networks have proven to be more effective in solving medical image segmentation problems, particularly the ones related to endoscopic images of the human gastrointestinal (GI) tract. Among various deep models, encoder-decoder based networks like UNet family \cite{ronneberger2015u} have demonstrated impressive performance. In UNets, high-level semantic features in the decoder are gradually up-sampled and fused with corresponding low-level detailed information in the encoder through skip connections. Inspired by the success of UNets, UNet++ \cite{zhou2019unet++} and ResUNet++ \cite{jha2019resunet++} were proposed for polyp segmentation and yielded promising results. However, these methods heavily depend on the dense concatenation of feature maps at multiple levels, resulting in high computational resource requirements and time-consuming procedures. Recently, the attention mechanism has been widely used in various deep learning models. In \cite{oktay2018attention}, Oktay et al. introduce attention gates to UNets in order to suppress irrelevant low-level information from encoders before concatenating them with high-level feature maps in decoders. Fan et al. \cite{fan2020pranet} enhance an FCN-like model by a parallel partial decoder and reverse attention module and obtained impressive results.
	On the other hand, previous works show that stacking multiple UNets allows the networks to learn a better feature representation and considerably improves the accuracy. DoubleUNet \cite{jha2020doubleu} stacks two UNets on top of each other and was applied for polyp segmentation. However, DoubleUNet lacks skip connections between the two UNets, which limits the information flow within the network. Another weakness of DoubleUNet is the use of an old VGG-19 backbone, which can be replaced with more efficient models proposed recently, e.g., the ResNet family \cite{xie2017aggregated,he2016deep,zhang2020resnest}. In \cite{tang2019cu}, Tang et al. introduce the coupled UNets (CUNet) architecture, where coupling connections are utilized to improve the information flow across UNets. In \cite{na2020facial}, Na et al. introduce coupled attention residual UNets and use it as a generator for adversarial learning based Facial UV map completion. The model in \cite{na2020facial} is based on ResNet backbone and utilizes the fast normalized fusion \cite{tan2020efficientdet} to combined the information between the two UNets, which can result in potential information loss.
	
	This paper proposes a novel deep network, called AG-CUResNeSt, where the encoders of coupled UNets are strengthened by residual connections and split-attention blocks in ResNeSt \cite{zhang2020resnest}. Attention gates are integrated into skip connections within each UNet to suppress the redundant low-level information from the encoders. We also leverage skip connections across the two UNets \cite{tang2019cu} to reduce gradient vanishing and promote feature reuse. Extensive experiments on five popular benchmark datasets demonstrate that our method yields superior accuracy compared to other state-of-the-art approaches.
	
	The rest of the paper is structured as follows. Section \ref{sec:related} reviews the literature regarding CNN backbones and semantic segmentation in medical image analysis. In Section \ref{sec:propose}, we describe the proposed network architecture in detail. Section \ref{sec:experiments} outlines our experiment settings. The results are presented and discussed in Section \ref{sec:results}. Finally, we conclude the paper and outline future works in Section \ref{sec:conclude}.
	
	\section{Related Work}
	\label{sec:related}
	There have been many methods proposed for semantic image segmentation in general and for the purpose of segmenting the polyps in colonoscopy images in particular. In this section, we briefly review prior methods related to our work, focusing on deep neural network models for colorectal polyps segmentation.
	
	\subsection{CNN Architectures}
	Since AlexNet \cite{krizhevsky2012imagenet}, Convolutional Neural Networks (CNNs) have dominated in solving computer vision tasks. VGG \cite{simonyan2014very} proposes a simple yet effective modularized network design exploiting the efficiency of small  kernels. However, plain networks like VGG suffer from degradation when their depth increases. ResNet \cite{he2016deep} introduces identity skip connections to smooth out the objective function's landscape. Skip connections also reduce gradient vanishing and allow very deep networks to learn better feature representations. GoogleNet \cite{szegedy2015going} demonstrates the success of multi-branch networks, where each branch is carefully designed using different convolutional kernel sizes. ResNeXt \cite{xie2017aggregated} improves ResNet with a unified multi-branch design, where all branches have the same architecture. SE-Net \cite{hu2018squeeze} introduces a channel attention mechanism that adaptively recalibrates channel-wise feature responses. SK-Net \cite{li2019selective} proposes an adaptive selection mechanism to fuse two network branches to adaptively adjust receptive field sizes of neurons according to the input. Recently, ResNeSt \cite{zhang2020resnest} integrates the channel-wise attention on different network branches to exploit their success in capturing cross-feature interactions and learning diverse representations. Besides, with the growth of computing capability, some efficient CNNs such as EfficientNet \cite{tan2019efficientnet} are automatically designed by machine thanks to neural architecture search techniques.
	
	\subsection{Semantic Segmentation for Medical Image Analysis}
	Semantic image segmentation has been an area of very active research in recent years. Many network architectures and learning techniques have been proposed to improve segmentation accuracy, latency, and throughput. In \cite{long2015fully}, the authors utilize several well-known classification networks (AlexNet \cite{krizhevsky2012imagenet}, VGG \cite{simonyan2014very} and GoogLeNet \cite{szegedy2015going}) in segmentation networks, coupled with transfer learning techniques. UNet \cite{ronneberger2015u} is among the most famous network architectures for segmentation. The network consists of an encoder and a decoder, with skip connections between corresponding levels. More recently, DeepLabV3 \cite{chen2017rethinking} introduces atrous convolutions to extract denser features for better performance in segmentation tasks.
	
	Semantic segmentation specifically for medical images has also attracted much attention. UNet is among the first successful applications of neural architecture in medical images and brings several variants in the following years. In \cite{zhou2019unet++}, Zhou et al. introduce UNet++, an ensemble of nested UNets of varying depths, which partially share an encoder and jointly learn using deep supervision. Later, Jha et al. \cite{jha2019resunet++} propose ResUNet++ that takes the advantages of residual blocks, squeeze and excitation units, atrous spatial pyramidal pooling (ASPP), and the attention mechanism. 
	DoubleUNet \cite{jha2020doubleu} uses a pre-trained VGG backbone, squeeze and excitation units, and ASPP modules. The performance of this network surpasses previous methods on several different datasets. However,  DoubleUNet suffers from using the old VGG backbone and the lack of skip connections across the two UNet blocks, limiting the information flow. CUNet \cite{tang2019cu} improves the information flow by adding skip connections across UNet blocks. Attention UNet \cite{oktay2018attention} introduces attention gates, which suppress unimportant regions while highlighting salient features useful for a specific task. 	
	Non-UNet architectures are also utilized for medical segmentation. Fan et al. \cite{fan2020pranet} propose PraNet, enhancing an FCN-like model using a parallel partial decoder and reverse attention modules. PraNet achieves state-of-the-art performance on five challenging benchmark medical datasets.
	In this paper, we analyze and integrate the three aforementioned architectures  \cite{tang2019cu,jha2020doubleu,oktay2018attention} to propose a novel network that outperforms previous methods.
	


	\section{Method}
	\label{sec:propose}
	\begin{figure*}[ht!]
		\includegraphics[width=350pt]{figure/ATTENTION_Rescunet.png}
		\caption{An overview of the proposed AG-CUResNeSt. Attention gates within each UNet are used to suppress irrelevant information in the encoder's feature maps. Skip connections across the two UNets are also utilized to boost the information flow and promote feature reuse.}
		\label{fig:fig_attention_ResCUNeSt}
	\end{figure*}
	
	\subsection{Overall Architecture}
	The overall architecture of our proposed network is depicted in Fig \ref{fig:fig_attention_ResCUNeSt}. The network consists of two coupled UNets with a similar architecture. Each UNet has an encoder and a decoder with skip connections between them. The encoder takes an image input of size , then passes through five top-down blocks to produce a high-level semantic feature map of size . This feature map is gradually up-sampled through five bottom-up blocks of the decoder and fused with low-level information in the encoder via gated skip connections called attention gates. Next, we use a 1x1 conv layer followed by a sigmoid layer at the end of each UNet to yield its output. The last feature map of the first UNet is combined with the raw input image and then fed to the second UNet for further refinement. Inspired by \cite{tang2019cu}, we also use skip connections across the two UNets to enhance the information flow and promote feature reuse in the network.
	
	\subsection{Backbone: ResNet Family}
	Plain networks tend to decrease performance on both training and test datasets as their depth increases. This is a widely observed phenomenon called the degradation problem. ResNet \cite{he2016deep} addresses this problem by introducing skip connections.
	Suppose that  as an underlying mapping to be fitted by a block of few nonlinear layers, where  is the input to the first layer. Instead of directly approximating , we can let the block approximate the corresponding residual mapping . The original mapping can be obtained using a skip connection as . By this trick, if the network wants to learn identity mappings, it just simply needs to drive the weights of the nonlinear layers in the block toward zero. Hence, residual connections facilitate the optimization of the network at almost no cost.
	
	ResNeXt \cite{xie2017aggregated} introduces a homogeneous multi-branch structure that breaks channel information into  repeated smaller bottleneck branches called cardinal groups.
	
	\begin{figure*}[ht!]
		\centering
		\includegraphics[scale=0.6]{figure/split_attention.png}
		\caption{Split attention in the -th cardinal group with  splits.}
		\label{fig:spit_attention}
	\end{figure*}
	
	One of the latest members in the ResNet family is ResNeSt \cite{zhang2020resnest}, which improves the feature representation to boost the performance across multiple computer vision tasks. ResNeSt proposes to split each cardinal group into  smaller feature groups, where  is called a new radix hyperparameter. Hence, the total number of feature groups is . Each group is associated with a transformation  and outputs an intermediate result , where , and  are the sizes of a cardinal group's output. The output of -th cardinal group is an element-wise summation of all  splits: . Inspired by the ideas of SE-Net \cite{hu2018squeeze} and SK-Net \cite{li2019selective}, ResNeSt introduces the channel-wise attention for multi network splits (Fig.~\ref{fig:spit_attention}). Firstly, the global context information across spatial dimensions  is obtained by applying global average pooling to . Then a network  of two consecutive fully connected (FC) layers is added to predict the attention weights over splits in each channel  as follows:
	
	
	The attention weights corresponding to the -th split can be denoted as .The output of -th cardinal group  is calculated by a weighted fusion over splits:
	
	
	
	Next, the representation of all cardinal groups are concatenated along the channel dimension: . Finally, a standard skip connection is applied , where  is an appropriate transform to align the output shapes if needed.
	
	The experiments in \cite{zhang2020resnest} show that ResNeSt even outperforms the machine designed architecture EfficientNet \cite{tan2019efficientnet} in accuracy and latency trade-off. In this study, we conduct an ablation study on different backbones including ResNet-50, ResNet-101, ResNeSt-50 and ResNeSt-101. Besides, we also compare the ResNet family with Non-ResNet architectures such as VGG and EfficientNet.
	
	\subsection{Attention Gate}
	\begin{figure*}[ht!]
		\includegraphics[width=350pt]{figure/Attention_Gates.png}
		\caption{The Attention Gate (AG) receives two inputs: a low-level feature map  from an encoder and a coarse feature map  from a corresponding decoder. The feature map  is firstly down-sampled and fused with , then fed to some hidden layers to yield an attention coefficient map . Finally, the input features  are scaled with attention coefficients to suppress irrelevant information.}
		\label{fig:attention_gate}
	\end{figure*}
	
	Attention gates (AG) \cite{oktay2018attention} can implicitly learn to suppress the irrelevant information in an input image while strengthening salient features necessary for a specific task. In the encoder of a UNet, input data is gradually down-sampled and transformed from low-level to high-level semantic feature maps with coarser scales. In the decoder, coarse feature maps are upsampled and fused with low-level ones to produce a final segmentation result. Fig.~\ref{fig:attention_gate} describes how the attention gate works. Suppose that  is a coarse feature map in the decoder that provides information to suppress the irrelevant content in a low-level feature map  from the encoder. Each feature map  and  is fed to a  convolutional (conv) layer with  kernels to reduce its number of channels to an intermediate value . 
	The low-level feature map is then down-sampled to align with the shape of . Next, the two resulting feature maps are combined by passing them to an element-wise summation operation followed by a ReLU function. A  conv layer with only one kernel is further applied to aggregate the information across all channels. After that, a sigmoid function is used to normalize the information and produce a coarse attention map, which is then resampled to match the shape of . Finally, the fine-grained attention map  is used to scale the feature map .
	
	In the first UNet of our proposed network (Fig.~\ref{fig:fig_attention_ResCUNeSt}), the attention gate takes a coarse feature map  from decoder and a low-level feature map  from the encoder as input and produces a filtered map . The feature map  is then upsampled and concatenated with  before fitting them to two successive   conv layers followed by ReLU and Batch Norm. A similar mechanism is applied in the second UNet. However, in our design,  is concatenated with not only  but also the coarse feature map  passed through the skip connections across the two UNets.
	
	\subsection{Loss Function}
	It is known that the problem of medical image segmentation poses an issue of imbalanced data, i.e., lesions or polyps are often small regions in an image. Therefore, we propose to employ  Tversky loss \cite{salehi2017tversky} to address the issue of data imbalance and achieve a much better tradeoff between precision and recall during training the networks. Assume that  and  are the predicted map taken after the softmax layer and the binary ground-truth, respectively, the Tversky loss is defined as follows:
	
	where  is the number of pixels in the ground-truth ;  is the the probability that pixel  belongs to a polyp,  is the probability that pixel  belongs to a non-polyp region;  for a polyp pixel,  for a non-polyp pixel and vice verse for ;  and  control the magnitude of penalties for false positives and false negatives, respectively.
	
	An auxiliary Tversky loss is also applied to the first UNet to boost the gradient flow during training. Thus, the final loss function is:
	
	where  and  are the output of the first and the second UNet, respectively.
	
	\section{Experiments}
	\label{sec:experiments}
	
	\subsection{Datasets}
	Several benchmark datasets are available for evaluating polyp segmentation models. The CVC-ClinincDB \cite{bernal2015wm} and ETIS-Larib \cite{silva2014toward} datasets are provided in the 2015 MICCAI automatic polyp detection sub-challenge. These datasets consist of frames extracted from colonoscopy videos, annotated by expert video endoscopists. CVC-ClinicDB has 612 images (384x288) extracted from 29 different video studies. ETIS-Larib has a total of 196 high-resolution images (1225x966). The CVC-ColonDB \cite{bernal2012towards} dataset is contributed by the Machine Vision Group (MVG) and contains 15 different polyps in 380 images (574x500). Finally, the Kvasir-SEG dataset \cite{jha2020kvasir}, publicized by the Simula Research Laboratory, includes 1000 polyp images with varying sizes.
	
	To evaluate the effectiveness of each proposed component in our new architecture and compare the performance of our model across data sets over state-of-the-art approaches, we conduct experiments with different scenarios of using training and test data, as follows:
	\begin{itemize}
		\item \textbf{Scenario 1}: CVC-Colon and ETIS-Larib for training, CVC-Clinic for testing;
		\item \textbf{Scenario 2}: CVC-Colon for training, CVC-Clinic for testing;
		\item \textbf{Scenario 3}: CVC-ClinicDB for training, ETIS-Larib for testing. This is the combination used in the 2015 MICCAI sub-challenge;
		\item \textbf{Scenario 4}: The Kvasir-SEG and CVC-Clinic datasets are merged, then split 80/10/10 for training, validation, and testing. The test sets are kept separate for evaluation on each source dataset. This is the combination proposed by PraNet \cite{fan2020pranet};
		\item \textbf{Scenario 5}: 5-fold cross-validation on the CVC-Clinic dataset, which is split into five equal folds. Each run uses one fold for testing and four folds for training;
		\item \textbf{Scenario 6}: 5-fold cross-validation on the Kvasir-SEG dataset, which is split into five equal folds. Each run uses one fold for testing and four folds for training.
	\end{itemize}
	
	\subsection{Experiment Settings}
	We implement the proposed models using the PyTorch framework. A single training run takes approximately 12 hours using an NVIDIA GTX 2080 GPU. Weights pre-trained on ImageNet for ResNet and ResNeSt are used as initialization for the respective backbones. The training process consists of two phases. The first phase trains the first UNet to convergence, and the second phase trains the entire coupled network model. Both phases use stochastic gradient descent (SGD) with a learning rate of , and a momentum of . We use the Tversky loss function, with  and .
	
	\subsection{Data Augmentation}
	The aforementioned datasets are generally small compared to other computer vision datasets, as annotations require expert endoscopists. Thus, image augmentation is quite often used to help diversify training data. Our experiments follow the Augmentation-II strategy proposed in \cite{shin2018automatic}. Particularly, we apply the following transformations to every training image:
	\begin{itemize}
		\item Rotating the images by 90, 180, and 270 degrees, respectively;
		\item Flipping the images both horizontally and vertically;
		\item Resizing the images with four scale factors of 0.9, 1.1, 1.15, and 1.2, respectively;
		\item Blurring the images with a kernel size of ;
		\item Brightening the images by using RandomBrightness in \cite{albumentations} with alpha = 1.5;
		\item Darkening the images by using RandomContrast in \cite{albumentations} with alpha = 0.5.
	\end{itemize}
	
	\subsection{Evaluation Metrics}
	We use the performance metrics listed in the MICCAI 2015 challenge\cite{miccai_polyp} to evaluate model performance: precision, recall, IoU (Jaccard score), and Dice score (F1). These are the most well-known measures for segmentation accuracy evaluation. Metrics are measured on the macro level: measurements are made on every image, then averaged on the whole dataset across all images.	
	
	where  represents the model's prediction,  is the ground-truth, TP is true positives, FP is false positives, and FN is false negatives.
	
	\section{Results and Discussion}
	\label{sec:results}
	\subsection{Ablation Study}
	In this section, we measure the impact of each component in the proposed model. For the ablation study, we choose Scenario 1, i.e., CVC-Colon and ETIS-Larib are used for training, and CVC-Clinic is used for testing, due to two following reasons. Firstly, there are a number of options for the model's architecture. Hence, we should choose a combination with a small training dataset to speed up the evaluation process. The CVC-ColonDB dataset, including 380 images, seems to be not enough to fit large backbones. The ETIS-Larib may be a good addition to the training dataset. Secondly, the training and test datasets are taking separately from different sources with different image properties and characteristics. This cross-dataset experiment setup is useful to evaluate the generalization capability of the model. Table~\ref{tab_colon_etis_clinic} shows the overall results of ablation study. It can be seen that AG-CUResNeSt-101 architecture obtained the best results over state-of-the-art models in terms of mDice and mIoU scores, the second-best in terms of precision, and is comparable to other models in terms of recall.  
	
	\begin{table}[h!]
		\centering
		\caption{Performance metrics for model variants in Scenario 1, i.e., training on CVC-Colon and ETIS-Larib, testing on CVC-Clinic}
		\begin{tabular}{ c | c c c c}
			\hline
			Method & mDice       & mIoU    & Recall          & Precision        \\
			\hline
			\hline
			VGG16-UNet & 0.759 &	0.660 &	0.831 &	0.778 \\
			Efficientnet-B0-UNet &	0.747 &	0.650 &	0.871 &	0.737 \\
			Efficientnet-B1-UNet &  0.754 &	0.662 &	\textbf{0.877} &	0.747 \\
			Efficientnet-B2-UNet &	0.800 &	0.715 &	0.806 &	0.860 \\
			Efficientnet-B3-UNet &	0.803 &	0.716 &	0.849 &	0.824 \\
			Efficientnet-B4-UNet &	0.813 &	0.731 &	0.835 &	0.860 \\
			\hline
			\hline
			ResNet34-UNet                  & 0.783          & 0.692          & 0.827          & 0.821           \\
ResNet50-UNet                  & 0.805          & 0.719          & 0.843          & 0.827           \\
ResNet101-UNet                 & 0.811          & 0.731          & 0.849          & 0.838           \\
ResNeSt50-UNet                 & 0.814          & 0.725          & 0.829          & 0.861           \\
ResNeSt101-UNet                & 0.816          & 0.739          & 0.813          & \textbf{0.888}           \\
			\hline
			\hline
			Attention ResNet101-UNet          & 0.815          & 0.730          & 0.863          & 0.825           \\
Attention ResNeSt101-UNet       & 0.829          & 0.749          & 0.842          & 0.877           \\
			\hline
			\hline
			Attention ResCUNet-101          & 0.820          & 0.736          & 0.859          & 0.838           \\
AG-CUResNeSt-101 & \textbf{0.833} & \textbf{0.754} & 0.840 & 0.883  \\
\hline
		\end{tabular}
		\label{tab_colon_etis_clinic}
	\end{table}
	
	\subsubsection{The Effectiveness of Backbone Networks}
	We first evaluate the use of different encoder backbones. Several ResNet variants including ResNet34, ResNet50, ResNet101, ResNeSt50, and ResNeSt101 have been used. Besides, we also try other CNN architectures such as VGG16 and EfficientNet family from B0 to B4. The ResNeSt architecture uses channel-wise attention on separate branches to enrich their features. Table~\ref{tab_colon_etis_clinic} shows that ResNeSt101 gives the best overall performance. ResNet backbones generally perform better as size increases. ResNeSt101 also improves over ResNest50, but the improvement is quite marginal. ResNest101 achieves lower recall (to 0.813 from 0.829), suggesting that a larger ResNeSt such as ResNeSt152 would likely not yield significant improvements. ResNeSt101 backbone significantly outperforms VGG16 and yields slightly better results compared to EfficientNet-B4 in terms of mDice and mIoU scores.
	
	
	\subsubsection{The Effectiveness of Attention Gate}
	Next, we conduct experiments using two backbones, ResNet101 and ResNeSt101, integrated with the Attention Gate (AG) module. The integrated models are called Attention ResNet101-UNet and Attention ResUNeSt101-UNet, respectively. Table~\ref{tab_colon_etis_clinic} shows a considerable increase in mDice score when applying AG. More specifically, mDice for ResNet101-UNet increases from 0.811 to 0.815 when AG is added, while ResNeSt101-UNet increases from 0.816 to 0.829. We note that while the overall Dice score increases, adding AG causes a drop in precision score for both models. This is likely due to increased focus on potential polyp regions that had previously been ignored without AG. As attention gates bring more focus to these regions, the network can cover more polyps but also more likely to make false predictions.
	
	\subsubsection{The Effectiveness of coupled connections}
	The Attention CUNet architecture adds one additional UNet, as well as skip connections across the two UNets. We denote the variant with ResNet backbone as Attention ResCUNet, and that with the ResNeSt backbone as AG-CUResNeSt. For each backbone, the mDice score increases by roughly . AG-CUResNeSt achieves a mDice of 0.833 and a mIOU of 0.754, the best scores among models in Table~\eqref{tab_colon_etis_clinic}. Both network size and the enrichment of semantic features play a factor in this improvement.
	
	\subsection{Comparison to Existing Methods}
	This section compares our proposed AG-CUResNeSt to several state-of-the-art models for polyp segmentation. From the previous ablation study, we select the best-performing ResNeSt101 backbone as the comparison model for this section. Therefore, the model is briefly called AG-CUResNeSt-101.
	
	\subsubsection{Cross-dataset Evaluation}
	The following experiments are for evaluating the performance of AG-CUResNeSt-101 and previous state-of-the-art models when training and testing across different datasets, i.e., Scenario 2 and Scenario 3. This setting implies that models need to generalize well to have good performance, as different polyp datasets have different image properties and feature distributions.
	
	We first compare AG-CUResNeSt-101 with Mask-RCNN \cite{qadir2019polyp} for segmentation, using Scenario 2, i.e., using CVC-Colon for training, CVC-Clinic for testing. We use the implementation of Mask-RCNN in the detectron2 project and train the model from scratch using the original paper's hyperparameter configurations mentioned in \cite{qadir2019polyp}. Table~\ref{tab_colon_clinic} shows that both Attention ResNeSt101-UNet and AG-CUResNeSt-101 outperform Mask-RCNN by a large margin (over ). Fig.~\ref{fig_colon_clinic} provides additional references for the output produced by each model. Notably, AG-CUResNeSt-101 seems capable of detecting both tiny and large polyps that occupy the whole image. We also compare the final output taken from the second UNet and the auxiliary output taken from the first UNet. Fig.~\ref{fig_compare_two_UNets} shows that the second UNet can correct some regions that the first UNet fails to predict.
	


	\begin{figure*}[ht!]
		\centering
		\includegraphics[width=350pt]{figure/Colon_Clinic.png}
		\caption{Qualitative result comparison using Colon for training and Clinic for testing. From left to righ: input image, ground truth, visualization of ResNet101-MaskR-CNN's output in overlay mode, binary output of ResNet101-MaskR-CNN, visualization of ResNet50-MaskR-CNN's output in overlay mode, binary output of ResNet50-MaskR-CNN, binary output of AG-CUResNeSt-101, and attention map in the last attention gate denote by  in Fig.~\ref{fig:fig_attention_ResCUNeSt}. The red color in the attention map indicates the region where the model focus on.}
		\label{fig_colon_clinic}
	\end{figure*}
	
	\begin{figure*}[ht!]
		\centering
		\includegraphics[width=300pt]{figure/ResNeSt_Colon_Clinic.png}
		\caption{The results of AG-CUResNeSt-101 on CVC-Clinic dataset. From left to right: input image, ground truth, output of the first UNet, output of the second UNet, and attention map in the last attention gate . The red areas in the attention map are high probability where polyps appear.}
		\label{fig_compare_two_UNets}
	\end{figure*}
	
	\begin{table}[ht!]
		\centering
		\caption{Performance metrics for Mask-RCNN and AG-CUResNeSt using Scenario 2, i.e., using CVC-Colon for training, CVC-Clinic for testing}
		\begin{tabular}{ c|c c c c}
			\hline
			Method & mDice  & mIoU  & Recall  & Precision   \\
			\hline
			\hline
			ResNet50-Mask-RCNN \cite{qadir2019polyp}   & 0.639    & 0.560        & 0.648  & 0.710      \\
			
			ResNet101-Mask-RCNN \cite{qadir2019polyp}  & 0.641 & 0.565        & 0.646  & 0.725      \\
			\hline
			\hline
			Attention ResNeSt101-UNet    & 0.765    & 0.681        & 0.773  & \textbf{0.842}      \\
			Our AG-CUResNeSt-101    & \textbf{0.771}    & \textbf{0.686}        & \textbf{0.793}  & 0.830      \\
			\hline
			\multicolumn{5}{l}{ indicates a model retrained with the original reported configurations.
			}
			
		\end{tabular}
		\label{tab_colon_clinic}
	\end{table}
	


	\begin{table}
		\centering
		\caption{Performance metrics for Mask-RCNN, Double UNet and AG-CUResNeSt in Scenario 3, i.e., using CVC-ClinicDB for training, ETIS-Larib for testing}
		\begin{tabular}{c|c c c c}
			\hline
			Method & mDice  & mIoU  & Recall  & Precision   \\
			\hline
			\hline
			ResNet50-Mask-RCNN \cite{qadir2019polyp}      & 0.501   & 0.412         & 0.546  & 0.573      \\
			
			ResNet101-Mask-RCNN \cite{qadir2019polyp}     & 0.565   & 0.469         & 0.565  & 0.639      \\
			
			DoubleUNet (BCE loss) \cite{jha2020doubleu}  & 0.482   & 0.400         & 0.713  & 0.475      \\
			
			Double UNet (Dice loss) \cite{jha2020doubleu}  & 0.588   & 0.500         & 0.689  & 0.599      \\
			\hline
			\hline
			Attention ResNeSt101-UNet     & 0.688   & 0.605         & 0.740  & 0.687      \\
			
			Our AG-CUResNeSt-101       & \textbf{0.701}  & \textbf{0.613}         & \textbf{0.755}  & \textbf{0.693}      \\
			\hline
			\multicolumn{5}{l}{ indicates a model retrained with the original reported configurations.
			}
		\end{tabular}
		\label{tab_clinic_etis}
	\end{table}
	
	Next, we compare AG-CUResNeSt-101 with Mask-RCNN and DoubleUNet in Scenario 3, i.e., using CVC-ClinicDB for training, ETIS-Larib for testing. Mask-RCNN \cite{qadir2019polyp} and DoubleUNet \cite{jha2020doubleu} are trained from scratch using the original papers' configurations. Table~\ref{tab_clinic_etis} shows that AG-CUResNeSt-101 outperforms both models in terms of mDice and mIoU, both by significant margins. Specifically, AG-CUResNeSt-101 outperforms the best Mask-RCNN by  in mDice and  in mIoU. Against DoubleUNet, both of these figures are .
	
	\subsubsection{Intra-dataset Evaluation}
	The following experiments compare AG-CUResNeSt-101 with several existing models when the training and test set are drawn from the same dataset, i.e., Scenario 4, 5, and 6).
	
	Table~\ref{tab_kvasir_clinic} shows our evaluation results in Scenario 4, i.e., Kvasir-SEG and CVC-Clinic datasets are merged, then split 80/10/10 for training, validation, and testing. The proposed AG-CUResNeSt-101 is compared with PraNet \cite{fan2020pranet}, SFA \cite{fang2019selective}, ResUNet-mod \cite{zhang2018road}, ResUNet++ \cite{jha2019resunet++}, UNet \cite{ronneberger2015u} and UNet++ \cite{zhou2019unet++}. Results for the compared models are reported in their respective papers. On the Kvasir-SEG test set, AG-CUResNeSt-101 outperforms the second-best PraNet by  in mDice and  in mIoU. For the CVC-ClinicDB test set, AG-CUResNeSt-101 is also the best performing model, exceeding PraNet by   in mDice and  in mIoU, respectively.
	
	\begin{table} [ht!]
		\centering
		\caption{mDice and mIoU scores for models trained in Scenario 4 on the Kvasir-SEG and CVC-ClinicDB test sets}
		\begin{tabular}{c|c c | c c}
			\hline
			\multirow{2}{*}{Method}               & \multicolumn{2}{c|}{Kvasir-SEG}  & \multicolumn{2}{c}{CVC-ClinicDB}   \\
			\cline{2-3} \cline{4-5} 
			& mDice  & mIoU  & mDice  & mIoU   \\
			\hline
			\hline
			
			UNet  \cite{ronneberger2015u}     & 0.818  & 0.746 & 0.823  & 0.755  \\
			UNet++  \cite{zhou2019unet++}      & 0.821  & 0.743 & 0.794  & 0.729  \\
			ResUNet-mod  \cite{zhang2018road}          & 0.791  & n/a   & 0.779  & n/a    \\
			ResUNet++ \cite{jha2019resunet++}             & 0.813  & 0.793  & 0.796  & 0.796  \\
			SFA \cite{fang2019selective}       & 0.723  & 0.611   & 0.700  & 0.607 \\
			PraNet  \cite{fan2020pranet}               & 0.898  & 0.840   & 0.899  & 0.849 \\
			\hline
			\hline
			Our AG-CUResNeSt-101 & \textbf{0.902} & \textbf{0.845}  & \textbf{0.917}  & \textbf{0.867} \\
			\hline
		\end{tabular}
		\label{tab_kvasir_clinic}
	\end{table}
	
	Next, we compare AG-CUResNeSt-101 against UNet and MultiResUNet \cite{ibtehaz2020multiresunet} in Scenario 5, i.e., 5-fold cross-validation on the CVC-Clinic dataset. Results are shown in Table~\ref{tab_clinic}. Note that the authors of UNet and MultiResUNet only reported their IoU scores on this dataset. Regardless, we can see AG-CUResNeSt-101 shows significant improvement in this metric, outperforming MultiResUNet by almost , and UNet by .
	
	\begin{table}
		\centering
		\caption{Performance metrics for UNet, MultiResUNet and AG-CUResNeSt-101 in Scenario 5, i.e., 5-fold cross-validation on the CVC-Clinic dataset}
		\begin{tabular}{c|c c c c}
			\hline
			Method & mDice  & mIoU  & Recall  & Precision   \\
			\hline
			\hline
			UNet \cite{ronneberger2015u}                   & -          & 0.792              & -          & -           \\
			MultiResUNet \cite{ibtehaz2020multiresunet}& -          & 0.849              & -          & -           \\
			\hline
			\hline
			Our AG-CUResNeSt-101 & \textbf{0.9460.01} & \textbf{0.9020.015} & \textbf{0.9530.013} & \textbf{0.9440.009}  \\
			\hline
		\end{tabular}
		\label{tab_clinic}
	\end{table}
	
	Finally, AG-CUResNeSt-101 is compared with UNet, ResUNet++, and PraNet in Scenario 6, i.e., 5-fold cross-validation on the Kvasir-SEG dataset. Results for the compared models are reported in their respective papers. Table~\ref{tab_kvasir} shows that the proposed model achieves the best mDice score, mIoU, recall, and precision. Specifically, AG-CUResNeSt-101 achieves an average Dice score of , outperforming the second-place PraNet by . Besides, metric scores across different folds demonstrate the stability of AG-CUResNeSt-101, with a slightly lower standard deviation than PraNet. 
	
	\begin{table}
		\centering
		\caption{Performance metrics for UNet, ResUNet++, PraNet and AG-CUResNeSt-101 in Scenario 6, i.e., 5-fold cross-validation on the Kvasir-SEG dataset}
		\begin{tabular}{c|c c c c}
			\hline
			Method & mDice  & mIoU  & Recall  & Precision   \\
			\hline
			\hline
			UNet \cite{ronneberger2015u}                 & 0.7080.017          & 0.6020.01         & 0.8050.014  & 0.7160.02      \\
			ResUNet++ \cite{jha2019resunet++}            & 0.7800.01          & 0.6810.008         & 0.8340.01  & 0.7990.01      \\
			PraNet \cite{fan2020pranet}               & 0.8830.02          & 0.8220.02         & 0.8970.02  & 0.9060.01      \\
			\hline
			\hline
			Our AG-CUResNeSt-101 & \textbf{0.9120.01} & \textbf{0.8600.011 }       & \textbf{0.9230.009}  & \textbf{0.9270.014}      \\
			\hline
		\end{tabular}
		\label{tab_kvasir}
	\end{table}
	
	
	A qualitative comparison between different models is shown in Fig.~\ref{fig_kvasir_results}. One can see that in many cases, our model performs significantly better than other state-of-the-art methods. The lesion regions are well-segmented and delineated. 
	
	Nevertheless, in some other case shown in Fig.~\ref{fig_kvasir_failed_results}, AG-CUResNeSt-101 is confused in estimating the attention maps, which lead to poor segmentation results. Usually, these imprecise predictions may occur when the input image contains very large polyps or colon mucosa folds, with many similar appearance characteristics as polyps. These are challenging cases for all the segmentation models and even junior endoscopists in practice. 


	\begin{figure*}[!ht]
		\centering
		\includegraphics[width=\textwidth]{figure/Kvasir_1.png}
		\caption{Qualitative result comparison of different models trained in Scenario 6, i.e., 5-fold cross-validation on the Kvasir-SEG dataset.}
		\label{fig_kvasir_results}
	\end{figure*}
	
	
	\begin{figure*}[!ht]
		\centering
		\includegraphics[height=0.4\textheight]{figure/Kvasir_Fail.png}
		\caption{Some failed cases of our model on the Kvasir-SEG dataset}
		\label{fig_kvasir_failed_results}
	\end{figure*}
	
	Fig.~\ref{fig_kvasir_ROC} shows the ROC curve and PR curve for each model in this experiment. Our AG-CUResNeSt-101 again reports the best AUC value of  and the best MAP value of 0.886. 
	
	From the perspectives of endoscopists, the use of our proposed models is expected to support the training of junior staff in colon polyp detection. The improvements of our proposed model over state-of-the-art methods are especially helpful for inexperienced endoscopists in delineating lesions in challenging cases. Furthermore, it could be considered the possibility of setting up in clinical practice as a second-look tool or an assisting system during the procedure. The high accuracy of our novel models in benchmark datasets proposes a feasible solution to reduce the missing rate in real practice, which may have a significant impact on improving the quality of colorectal cancer screening strategy.
	
	
	\begin{figure} [h!]
		\centering
		\begin{subfigure}[b]{0.49\textwidth}
			\centering
			\includegraphics[width=\textwidth]{figure/Kvasir_ROC.png}
			\caption{ROC curves}
		\end{subfigure}
		\hfill
		\begin{subfigure}[b]{0.49\textwidth}
			\centering
			\includegraphics[width=\textwidth]{figure/Kvasir_PR_curve.png}
			\caption{PR curves}
		\end{subfigure}
		\caption{ROC curves and PR curves for AG-CUResNeSt-101, PraNet, ResUNet++ and UNet in Scenario 6, i.e., 5-fold cross-validation on the Kvasir-SEG dataset. All curves are averaged over five folds.}
		\label{fig_kvasir_ROC}
	\end{figure}
	




	\section{Conclusion}
	\label{sec:conclude}
	This paper has introduced a novel neural network architecture for polyp segmentation called AG-CUResNeSt. The architecture combines several components, namely ResNeSt, attention gates, and Coupled UNets, to improve performance. The proposed model is verified using extensive experiments and compared against several state-of-the-art methods on public benchmark datasets. Results show that AG-CUResNeSt consistently improves over all compared models, with a slight tradeoff in model size.
	
	We hope that our proposal will provide a strong baseline for developing deep neural networks in medical image analysis. Our future research will focus on reducing the model size without sacrificing accuracy and conducting a deep study on failed cases to further understand the characteristic of the cases for further improvement in terms of segmentation accuracy. 



	\section*{References}
	\bibliography{mybibfile}
	
\end{document}
