



\documentclass{article} \usepackage{iclr2024_doc_style,times}



\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         



\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{threeparttable} \usepackage{float}
\usepackage{multirow}
\usepackage{authblk}        \usepackage{listings}       \usepackage{hyperref}
\usepackage{url}



\renewcommand*{\thefootnote}{\fnsymbol{footnote}}



\definecolor{blueVar}{RGB}{1, 115, 178}
\definecolor{greenVar}{RGB}{2, 158, 115}
\definecolor{orangeVar}{RGB}{222, 143, 5}
\definecolor{purpleVar}{RGB}{204, 120, 188}
\definecolor{redVar}{RGB}{213, 94, 0}
\definecolor{lightblueVar}{RGB}{86, 180, 233}
\definecolor{greyVar}{RGB}{128, 128, 128}
 



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 



\title{The Expressive Leaky Memory Neuron: \\ an Efficient and Expressive Phenomenological \\ Neuron Model Can Solve Long-Horizon Tasks}



\author[1,2]{Aaron Spieler}
\author[3,2]{Nasim Rahaman}
\author[1,2]{Georg Martius}
\author[2]{Bernhard Schölkopf}
\author[1,4]{Anna Levina}

\affil[1]{University of Tübingen}
\affil[2]{Max Planck Institute for Intelligent Systems, Tübingen}
\affil[3]{Mila, Quebec AI Institute}
\affil[4]{Max Planck Institute for Biological Cybernetics, Tübingen}



\iclrfinalcopy \begin{document}

\maketitle



\begin{abstract}
Biological cortical neurons are remarkably sophisticated computational devices, temporally integrating their vast synaptic input over an intricate dendritic tree, subject to complex, nonlinearly interacting internal biological processes. 
A recent study proposed to characterize this complexity by fitting accurate surrogate models to replicate the input-output relationship of a detailed biophysical cortical pyramidal neuron model and discovered it needed temporal convolutional networks (TCN) with millions of parameters. 
Requiring these many parameters, however, could be the result of a misalignment between the inductive biases of the TCN and cortical neuron's computations.
In light of this, and with the aim to explore the computational implications of leaky memory units and nonlinear dendritic processing, we introduce the Expressive Leaky Memory (ELM) neuron model, a biologically inspired phenomenological model of a cortical neuron.
Remarkably, by exploiting a few such slowly decaying memory-like hidden states and two-layered nonlinear integration of synaptic input, our ELM neuron can accurately match the aforementioned input-output relationship with under ten-thousand trainable parameters.
To further assess the computational ramifications of our neuron design, we evaluate on various tasks with demanding temporal structures, including the Long Range Arena (LRA) datasets, as well as a novel neuromorphic dataset based on the Spiking Heidelberg Digits dataset (SHD-Adding). Leveraging a larger number of memory units with sufficiently long timescales, and correspondingly sophisticated synaptic integration, the ELM neuron proves to be competitive on both datasets, reliably outperforming the classic Transformer or Chrono-LSTM architectures on latter, even solving the Pathfinder-X task with over  accuracy (16k context length). These findings indicate the importance of inductive biases for efficient surrogate neuron models and the potential for biologically motivated models to enhance performance in challenging machine learning tasks.
\end{abstract}

\section{Introduction}
\label{Introduction}



\looseness=-1
The human brain has impressive computational capabilities, yet the precise mechanisms underpinning them remain largely undetermined. Two complementary directions are pursued in search of mechanisms for brain computations. On the one hand, many researchers investigate how these capabilities could arise from the collective activity of neurons connected into a complex network structure \cite{maass1997networks,gerstner2002spiking,gruning2014spiking}, where individual neurons might be as basic as leaky integrators or ReLU neurons. On the other hand, it has been proposed that the intrinsic computational power possessed by individual neurons \cite{koch1997computation,koch2000role,silver2010neuronal} contributes a significant part to the computations.

\looseness=-1
Even though most work focuses on the former hypothesis, an increasing amount of evidence indicates that cortical neurons are remarkably sophisticated \cite{silver2010neuronal,gidon2020dendritic,larkum2022dendrites}, even comparable to expressive multilayered artificial neural networks \cite{poirazi2003pyramidal,jadi2014augmented,beniaguev2021single,jones2021might}, and capable of discriminating between dozens to hundreds of input patterns \cite{gutig2006tempotron,hawkins2016neurons,moldwin2020perceptron}. Numerous biological mechanisms, such as complex ion channel dynamics (e.g. NMDA nonlinearity \cite{major2013active,lafourcade2022differential,tang2023diverse}), plasticity on various and especially longer timescales (e.g. slow spike frequency adaptation \cite{kobayashi2009made,bellec_solution_2020}), the intricate cell morphology (e.g. nonlinear integration by dendritic tree \cite{stuart2015dendritic,poirazi2020illuminating,larkum2022dendrites}), and their interactions, have been identified to contribute to their complexity. 



\looseness=-1
Detailed biophysical models of cortical neurons aim to capture this inherent complexity through high-fidelity simulations \cite{hay2011models,herz2006modeling,almog2016realistic}. However, they require a lot of computing resources to run and typically operate at a very fine level of granularity that does not facilitate the extraction of higher-level insights into the neuron's computational principles. A promising approach to deriving such higher-level insights from simulations is through the training of surrogate phenomenological neuron models. These models are designed to replicate the output of high-fidelity biophysical simulations but use simplified interpretable components. This approach was employed, for example, to model computation in the dendritic tree via simple two-layer ANN~\cite{poirazi2003pyramidal,tzilivaki2019challenging,ujfalussy2018global}. Building on this line of research, a recent study by \citet{beniaguev2021single} developed a temporal convolutional network to capture the spike-level input/output (I/O) relationship with millisecond precision, accounting for the complexity of integrating diverse synaptic input across the entirety of the dendritic tree of a high-fidelity biophysical neuron model. It was found that a highly expressive temporal convolutional network with millions of parameters was essential to reproduce the aforementioned I/O relationship.



\looseness=-1
In this work, we propose that a model equipped with appropriate inductive biases that align with the high-level computational principles of a cortical neuron should be capable of capturing the I/O relationship using a substantially smaller model size. To achieve this, a model would likely need to account for multiple mechanisms of neural expressivity and judiciously allocate computational resources and parameters in rough analogy to biological neurons. Should such a construction be possible, the requisite design choices may yield insights into principles of neural computation at the conceptual level. We proceed to design the Expressive Leaky Memory (ELM) neuron model (see Figure \ref{fig:elm_neuron_sketch_and_equations}), a biologically inspired phenomenological model of a cortical neuron. While biologically inspired, low-level biological processes are abstracted away for computational efficiency, and consequently individual parameters of the ELM neuron are not designed for direct biophysical interpretation. Nevertheless, model ablations can provide conceptual insights into the computational components required to emulate the cortical input/output relationship. The ELM neuron functions as a recurrent cell and can be conveniently used as a drop-in replacement for LSTMs \cite{hochreiter1997long}. 



\looseness=-1
Our experiments show that a variant of the ELM neuron is expressive enough to accurately match the spike level I/O of a detailed biophysical model of a layer 5 pyramidal neuron at a millisecond temporal resolution with merely a few thousand parameters, in stark contrast to the millions of parameters required by temporal convolutional networks. Conceptually, we find accurate surrogate models to require multiple memory-like hidden states with longer timescales and highly nonlinear synaptic integration. 
To explore the implications of neuron-internal timescales and sophisticated synaptic integration into multiple memory units, we first probe its temporal information integration capabilities on a challenging biologically inspired neuromorphic dataset requiring the addition of spike-encoded spoken digits. We find that the ELM neuron can outperform classic LSTMs leveraging a sufficient number of slowly decaying memory and highly nonlinear synaptic integration. We subsequently evaluate the ELM neuron on three well-established long sequence modeling LRA benchmarks from the machine learning literature, including the notoriously challenging Pathfinder-X task, where it achieves over  accuracy but many transformer-based models struggle to learn at all.



\textbf{Our contributions are the following.}

\begin{enumerate}
    \item We propose the phenomenological Expressive Leaky Memory (ELM) neuron model, a recurrent cell architecture inspired by biological cortical neuron. 

    \item The ELM neuron efficiently learns the input/output relationship of a sophisticated biophysical model of a cortical neuron, indicating it's inductive biases to be well aligned.

    \item The ELM neuron facilitates the formulation and validation of hypotheses regarding the underlying computations of biological neurons.

    \item Lastly, we demonstrate the impressive long-sequence processing capabilities of the ELM, implicating the utility of leaky memory units and nonlinear dendritic processing, and suggesting broader applicability of ELM-like architectures in machine learning applications.
\end{enumerate}



\section{The Expressive Leaky Memory Neuron}
\label{the_elm_neuron}

\begin{figure}[ht]
    \centering
    \hfill
    \begin{minipage}[b]{0.37\textwidth}
        \begin{subfigure}{0.9\textwidth}
            \centering
            \includegraphics[width=\linewidth]{model_figures/pyramidal_neuron_sketch.png}
            \caption{A Cortical Neuron}
            \label{fig:a_pyramidal_neuron}
        \end{subfigure}
    \end{minipage}
    \begin{minipage}[b]{0.57\textwidth}
        \begin{subfigure}{0.9\textwidth}
            \centering
            \includegraphics[width=.8\linewidth]{model_figures/elm_model_sketch.png}
            \caption{The ELM Neuron Architecture}
            \label{fig:elm_neuron_architecture_sketch}
        \end{subfigure}
        \begin{subfigure}{0.9\textwidth}
            \centering
            \begin{minipage}[c][0.5\linewidth][c]{\linewidth}
                
            \end{minipage}\vspace{-1em}
            \caption{The ELM Neuron Equations}
        \end{subfigure}
    \end{minipage}
    \caption{\textbf{The biologically motivated Expressive Leaky Memory (ELM) neuron model}. The architecture can be divided into the following components; the input \textcolor{greenVar}{current synapse dynamics}, the \textcolor{orangeVar}{ integration mechanism dynamics}, the \textcolor{blueVar}{leaky memory dynamics}, and the \textcolor{purpleVar}{output dynamics}. \textbf{a)}~Sketch of a biological cortical pyramidal neuron segmented into the analogous architectural components using the corresponding colors. \textbf{b)}~Schematics of the ELM neuron architecture, component-wise colored accordingly. \textbf{c)} The ELM neuron equations, where  is the input at time ,  the fictitious elapsed time in milliseconds between two consecutive inputs  and  ,  are memory units,  the synapse currents (traces),  and  their respective timescales in milliseconds,  are synapse weights,  the weights of a \textit{Multilayer Perceptron} () with  hidden layers of size ,  the output weights,  a scaling factor for the delta memory , and  the output.}
    \label{fig:elm_neuron_sketch_and_equations}
\end{figure}

In this section, we discuss the design of the Expressive Leaky Memory (ELM) neuron. It's architecture is engineered to efficiently capture sophisticated cortical neuron computations. Abstracting mechanistic neuronal implementation details away, we resort to an overall recurrent cell architecture with biologically motivated computational components. This design approach emphasizes conceptual over mechanistic insight into cortical neuron computations.

\looseness=-1
\paragraph{\textcolor{greenVar}{The current synapse dynamics.}}
\label{the_synapse_dynamcis}
Neurons receive inputs at their synapses in form of sparse binary events known as spikes \cite{kandel2000principles}. While the Excitatory/Inhibitory synapse identity determines the sign of the input (always given), the positive synapse weights can act as simple input gating  (learned in Branch-ELM). The synaptic trace  denotes a filtered version of the input, believed to aid coincidence detection and synaptic information integration in neuron \cite{konig1996integrator}. This implementation is known at the current-based synapse dynamic \cite{dayan2005theoretical}.

\looseness=-1
\paragraph{\textcolor{blueVar}{The memory unit dynamics.}}
\label{the_memory_unit_dynamics}
The state of a biological neuron may be characterized by diverse measurable quantities, such as their membrane voltage or various ion/molecule concentrations (e.g. Ca+, mRNA, etc.), and their rate of decay over time (slow decay <-> large timescale), endowing them with a sort of \textbf{leaky memory} \cite{kandel2000principles,dayan2005theoretical}. However, which of these quantities are computationally relevant, how and where they interact, and on what timescale, remains a topic of active debate \cite{aru2020cellular,herz2006modeling,almog2016realistic,koch1997computation,chavlis2021drawing,cavanagh2020diversity,gjorgjieva2016computational}. Therefore, to match a biological neuron's computations, the surrogate model architecture needs to be \textbf{expressive} enough to accommodate a large range of possibilities. In the ELM neuron, we achieve this by making the number of memory units  a hyper-parameter and equipping each of them with a  (always learnable), setting it apart most other computational neuroscience models.

\looseness=-1
\paragraph{\textcolor{orangeVar}{The integration mechanism dynamics.}}
\label{the_integration_mechanism} 
This dynamic refers to how dependent on the previous memory units  the synaptic input  is integrated into the memory unit updates  over the dendritic tree of cortical neuron. We choose to parameterize this transformation using a Multilayer Perceptron () (always learnable with  and ), which is motivated by the ongoing discussion regarding the complexity and functional form of synaptic information integration over the dendritic tree in cortical neurons. While earlier perspectives suggested an integration process akin to linear summation \cite{jolivet2008quantitative}, newer studies advocate for complex non-linear integration \cite{almog2016realistic,gidon2020dendritic,larkum2022dendrites}, specifically proposing multi-layered ANNs as a more suitable model \cite{poirazi2003pyramidal,jadi2014augmented,tzilivaki2019challenging,marino_predictive_2021,jones2021might,jones2022biological,hodassman2022efficient}, backed by recent evidence of neuronal plasticity beyond synapses \cite{losonczy2008compartmentalized,holtmaat2009long,abraham2019plasticity}, therefore making an  a natural generic choice. Nevertheless, the  is merely meant to capture the \textbf{neuron analogous plasticity and dendritic nonlinearity}, and cannot give a mechanistic explanation of these phenomena in neuron. The distinction to  is mainly relevant in the Branch-ELM, where unlike in the vanilla ELM, their functionality cannot be absorbed in the . Finally, incorporating previous memory units  into the integration process, the ELM can accommodate state-dependent synaptic integration and related computations \cite{hodgkin1952quantitative,gasparini2006state,bicknell2021synaptic}, and enables the relationships among memory units  to be fully learnable. Crucially, this approach sidesteps the need for expert designed and pre-determined differential equations, typical in phenomenological neuron modeling. 

\looseness=-1
\paragraph{\textcolor{purpleVar}{The output dynamics.}} 
\label{the_output_dynamics}
Spiking neurons emit their output spike at the axon hillock roughly when their membrane voltage crosses a threshold \cite{kandel2000principles}. The ELM neuron's output is similarly based on its internal state  (using a linear readout layer ), which rectified can be interpreted as the spike probability. For task compatibility, the output dimensionality is adjusted based on the respective dataset (not affecting neuron expressivity).


\section{Related Work}
\label{related_work}

\looseness=-1
\textbf{Accurately replicating the full spike-level neuron input/output (I/O)} relationship of detailed biophysical neuron models at millisecond resolution in a computationally efficient manner presents a formidable challenge. However, addressing this task could potentially yield valuable insights into neural mechanisms of expressivity, learning, and memory capabilities. 

The relative scarcity of prior work on this subject can be partially attributed to the computational complexity of cortical neurons only recently garnering increased attention \cite{tzilivaki2019challenging,beniaguev2021single,larkum2022dendrites,poirazi2020illuminating}. Additionally, traditional phenomenological neuron models have primarily aimed to replicate specific computational phenomena of neurons or networks \cite{koch1997computation,izhikevich2004model,herz2006modeling}, rather than the entire I/O relationship. Finally, the computational resources and tooling necessary for an extensive exploration of suitable designs have only recently become widely available. 

\looseness=-1
\textbf{Phenomenological neuron modeling} research on temporally or spatially less detailed I/O relationship of biophysical neurons has been primarily centered around the use of multi-layered ANN structures in analogy to the neurons dendritic tree \cite{poirazi2003pyramidal,tzilivaki2019challenging,ujfalussy2018global}. Similarly, we parametrize the synaptic integration with an , while crucially extending this modeling perspective in several ways. Drawing upon the principles of classical phenomenological modeling via differential equations \cite{izhikevich2004model,dayan2005theoretical}, our approach embraces the recurrent nature inherent to neurons. We further consider the significance of hidden states  beyond membrane voltage, as seen in prior works with predetermined variables \cite{brette2005adaptive,gerstner2014neuronal}. This addition enables us to flexibly investigate internal memory timescales , hinted at in earlier modeling studies \cite{gjorgjieva2016computational,cavanagh2020diversity}.

\looseness=-1
\textbf{Deep learning architectures for long sequence modeling} have seen a shift towards the explicit incorporation of timescales for improved temporal processing, as observed in recent advancements in RNNs, transformers, and state-space models~\cite{guefficiently, mahto2021multitimescale, smith2023simplified, ma2023mega}. Such an explicit approach can be traced back to Leaky-RNNs \cite{mozer1991induction,jaeger2002tutorial,kusupati2018fastgrnn,tallec2018can}, which use a convex combination of old memory and updates, as done in ELM using . Whereas the classic time-varying memory decay mediated by implicit timescales \cite{tallec2018can}, is known from classic gated RNNs like LSTM \cite{hochreiter1997long} and GRU \cite{cho2014learning}. In contrast to complex gating mechanisms, time-varying implicit timescales, or sophisticated large multi-staged architectures, the ELM features a much simpler recurrent cell architecture only using fixed but trainable time constants  for gating, puting the major emphesis on the input integration dynamics using a single powerful .



\vspace{-3pt} \section{Experiments}
\label{experiments}
\vspace{-3pt}

\looseness=-1
In the experimental section of this work, we address three primary research questions. \textbf{First,} are the inductive biases imparted in the ELM neuron adequate to accurately fit a high-fidelity biophysical simulation with a small number of parameters? We detail this investigation in Section~\ref{neuronio_experiments}. \textbf{Second,} how can the ELM neuron effectively integrate non-trivial temporal information? We explore this issue in Section~\ref{heidelberg_experiments}. \textbf{Third,} what are the computational limits of the ELM design? Discussed in Section~\ref{lsm_benchmark_experiments}. Each of these questions contributes to our overall understanding of the ELM neuron's capabilities and its potential applications in both biological and machine learning contexts. For exact training details and hyper-parameters, please refer to the Appendix Table \ref{tbl:default_and_tuning_elm_parameters} and Appendix Section \ref{datasets_and_training_details} respectively. 

\subsection{Fitting a complex biophysical cortical neuron model's I/O relationship}
\label{neuronio_experiments}

\looseness=-1
The NeuronIO dataset primarily consists of simulated input-output (I/O) data for a complex biophysical layer 5 cortical pyramidal neuron model \cite{hay2011models}. Input data features biologically inspired spiking patterns (1278 pre-synaptic spike channels featuring -1,1 or 0 as input), while output data comprises the model's somatic membrane voltage and output spikes (see Figure \ref{fig:neuronio_data_and_results}a and \ref{fig:neuronio_data_and_results}b). The dataset and related code is publicly available \cite{beniaguev2021single}. 
The dataset was pre-processed in accordance with \cite{beniaguev2021single}, and the models were trained using Binary Cross Entropy (BCE) for spike prediction and Mean Squared Error (MSE) for somatic voltage prediction, with equal weighting.

\begin{figure}[t]
\centering
    \centering
    \includegraphics[width=0.9\linewidth]{neuronio_plots/neuronio_results.png}
    \caption{\textbf{The ELM neuron is a computationally efficient model of cortical neuron.} \textbf{a)} detailed biophysical model of a layer 5 cortical pyramidal cell was used to generate the NeuronIO dataset consisting of input spikes and output spikes and voltage. 
    \textbf{b) and c)} Voltage and spike prediction performance of the respective surrogate models, produced using joint ablation of  with  for ELM models. Previously around 10M parameters were required to make accurate spike predictions using a \textcolor{redVar}{TCN} \cite{beniaguev2021single}, an \textcolor{orangeVar}{LSTM} baseline is able to do it with 266K, and our \textcolor{blueVar}{ELM} and \textcolor{lightblueVar}{Branch-ELM} (introduced later) neuron model require merely 53K and 8K respectively (3rd from left each), simultaneously achieving much better voltage prediction performance than the \textcolor{redVar}{TCN}. For comparison in TP/FP Rate performance or FLOPS cost see Fig. \ref{fig:neuronio_model_and_results_branch}c or \ref{fig:neuronio_results_flops} respectively.}
    \label{fig:neuronio_data_and_results}
\end{figure}

\looseness=-1
Our \textcolor{blueVar}{ELM} neuron achieves better prediction of voltage and spikes than previously used architectures for any given number of trainable parameters (and compute). In particular, it crosses the ``sufficiently good'' spike prediction performance threshold (0.991 AUC) as defined in \citep{beniaguev2021single} by using merely 50K trainable parameters, which is around 200 improvement compared to the previous attempt (\textcolor{redVar}{TCN}) that required around 10M trainable parameters, and 6 improvement over a \textcolor{orangeVar}{LSTM} baseline which requires around 266K parameters (see Figure \ref{fig:neuronio_data_and_results}c-d). Overall, this result indicates that recurrent computation is indeed an appropriate inductive bias for modeling cortical neurons.

\looseness=-1
We use the fitted model to investigate how many memory units and which timescales are needed to match the neuron closely. We find that around 20 memory units are required (Figure \ref{fig:neuronio_ablation}a) with timescales that are allowed to reach at least 25\,ms  (Figure~\ref{fig:neuronio_ablation}d). While a diversity of timescales including long ones seem to be favorable for accurate modeling (Figure~\ref{fig:neuronio_ablation}d and \ref{fig:neuronio_ablation}f), using constant memory timescales around 25\,ms performs surprisingly well (matching the typical membrane timescales used in computational modeling \cite{dayan2005theoretical}, Figure~\ref{fig:neuronio_ablation}e). Removing the hidden layer or decreasing the integration mechanism complexity deteriorates the performance significantly (Figure~\ref{fig:neuronio_ablation}b). Allowing for more rapid memory updates through larger  is crucial (Figure~\ref{fig:neuronio_ablation}c), possibly to match the fast internal dynamics of neurons around spike times.

\begin{figure}[t]
\centering
    \centering
    \includegraphics[width=0.95\linewidth]{neuronio_plots/neuronio_ablation.png}
    \caption{\textbf{The ELM neuron gives relevant neuroscientific insights.} Ablations on NeuronIO of different hyperparameters of an \textcolor{blueVar}{ELM} neuron with AUC , and a  \textcolor{lightblueVar}{Branch-ELM} with the same default hyperparameters. The number of removed divergent runs marked with . \textbf{a)} We find between 10 and 20 memory-like hidden states to be required for accurate predictions, much more than typical phenomenological models use \cite{izhikevich2004model,dayan2005theoretical}. \textbf{b)} Highly nonlinear integration of synaptic input is required, in line with recent neuroscientific findings \cite{stuart2015dendritic,jones2022biological,larkum2022dendrites}. \textbf{c)}~Allowing greater updates to the memory units is beneficial.
    \textbf{d-f)} Ablations of memory timescale (initialization and bounding) range or (constant) value, with the default range being 1ms-150ms. Timescales around 25\,ms seems to be the most useful (matching the typical membrane timescale in the cortex \cite{dayan2005theoretical}); however, a lack can be partially compensated by longer timescales. \textbf{g) and h)}~Ablating the number of branches  and number of synapses per branch  of the  \textcolor{lightblueVar}{Branch-ELM} neuron.}
    \label{fig:neuronio_ablation}
    \vspace{-10pt} \end{figure}

\looseness=-1
\paragraph{How much nonlinearity is in the dendritic tree?}
Within the \textcolor{blueVar}{ELM} architecture, we allow for nonlinear interaction between any two synaptic inputs via the MLP. This flexibility might be necessary in cases where little is \emph{a priori} known about the structure of the input. However, for matching the I/O of biological neurons, knowledge of neuronal morphology and biophysical assumptions about \textit{linear-nonlinear} computations in the dendritic tree might be exploited to reduce the dimensionality of the input to the MLP (so far very parameter-costly component with  inputs). In particular, many studies suggest individual dendritic branches compute a rectified sum of their synaptic inputs \cite{poirazi2003pyramidal,stuart2015dendritic,hawkins2016neurons,poirazi2020illuminating}, with additional nonlinearities located downstream. Consequently, we modify the ELM neuron to include virtual branches along which the synaptic input is first reduced by a simple summation before further processing (see Figure \ref{fig:neuronio_model_and_results_branch}). For NeuronIO specifically, we assign the synaptic inputs to the branches in a moving window fashion (exploiting that in the dataset, neighboring inputs were also typically neighboring synaptic contacts on the same dendritic branch of the biophysical model). The window size is controlled by the branch size, and number of branches implicitly defines the stride size to ensure equally spaced sampling across the  inputs.

\begin{figure}[t]
\centering
    \centering
    \includegraphics[width=0.9\linewidth]{neuronio_plots/neuronio_branch_results.png}
    \caption{\textbf{Coarse-grained modeling of synaptic integration significantly improves model efficiency.} \textbf{a)} The \textcolor{orangeVar}{integration mechanism dynamics} of the ELM now computes the activity of individual dendritic branches as a simple sum of their respective synaptic inputs first before passing them on to the , where  is the number of branches and  the number of synapses per branch. \textbf{b)} Accurate predictions using a \textcolor{lightblueVar}{Branch-ELM} neuron with merely 8104 parameters (for zoomed in version with model dynamics see Fig. \ref{fig:detailed_euron_inference}). \textbf{c)}. The new \textcolor{lightblueVar}{Branch-ELM} neuron improves on the \textcolor{blueVar}{ELM} neuron by about 7 in terms of parameter efficacy (same \textcolor{blueVar}{ELM} hyper-parameters). Differences in model quality are particularly obvious when examining True-Positive rate at a low False-Positive rate.}
    \label{fig:neuronio_model_and_results_branch}
\end{figure}

\looseness=-1
Surprisingly, even with this strong simplification, the  \textcolor{lightblueVar}{Branch-ELM} neuron model is able to retain its predictive performance while requiring a modest 8K trainable parameters to substantially cross the performance threshold. This corresponds to roughly 7 reduction over the vanilla  \textcolor{blueVar}{ELM} neuron. 
We also find  that a combination of  and  still achieved over  AUC with only 5479 trainable parameters, corroborating the assumption of the near-linear computation within dendritic branches and inviting future investigation of minimal required synaptic nonlinearity.  However, this simplification ideally utilizes the knowledge of the exact morphology for modeling the neuron, or the relationship structure in the input data in case of other tasks; thus most consecutive experimental reports are for the vanilla \textcolor{blueVar}{ELM} neuron.

\subsection{Evaluating temporal processing capabilities on a bio-inspired task}
\label{heidelberg_experiments}

\looseness=-1
The Spiking Heidelberg Digits (SHD) dataset comprises spike-encoded spoken digits (0-9) in German and English \cite{cramer2020heidelberg}. The digits were encoded using 700 input channels in a biologically inspired artificial cochlea. Each channel represents a narrow frequency band with the firing rate coding for the signal power in this band, resulting in an encoding that resembles the spectrogram of the spoken digit (see Figure \ref{fig:heidelberg_results}a). 

\begin{figure}[ht]
\centering
    \centering
    \includegraphics[width=0.99\linewidth]{heidelberg_plots/heidelberg_adding_results.png}
    \caption{\textbf{The ELM neuron performs well on long and sparse data using longer timescales.} \textbf{a)} Sample from the biologically motivated SHD-Adding dataset (based on~\cite{cramer2020heidelberg}), each dot is an input spike, and a vertical dashed line is a guide for the eye indicating the separation of the two digits (not communicated to the network). \textbf{b-d)} The \textcolor{blueVar}{ELM} neuron (186K params.) consistently outperforms a classic \textcolor{orangeVar}{LSTM} (956K params.), especially for smaller bin sizes (meaning longer training samples), and LSTM-performance cannot be fully recovered even for larger bin sizes. The \textcolor{lightblueVar}{Branch-ELM} (67K params.) can retain performance for fine-grained binning at a much reduced model size. Our Leaky Integrate and Fire (LIF) (51K params.) \textcolor{redVar}{Spiking Neural Network} (\textcolor{redVar}{SNN}) does not manage to achieve good performance for any bin size, and training becomes unstable for long sequences. \textbf{e) and f)} Ablations using bin size of 2ms with test set performance reported. \textbf{e)} Solving SHD-Adding requires ELM neuron to have a higher complexity than required for NeuronIO (large model unstable with ). \textbf{f)} Longer memory timescales are crucial for extracting long-range dependencies.}
    \label{fig:heidelberg_results}
\end{figure}

\looseness=-1
Motivated by recent findings that most neuromorphic benchmark datasets only require minimal temporal processing abilities \cite{yang2021rethinking}, we introduce the SHD-Adding dataset by concatenating two uniformly and independently sampled SHD digits and setting the target to their sum (regardless of language) (see Figure \ref{fig:heidelberg_results}a). Solving this dataset necessitates identifying each digit on a shorter timescale and computing their sum by integrating this information over a longer timescale, which in turn requires retaining the first digit in memory. Whether single cortical neurons can solve this exact task is unclear; however, it has been shown that even single neurons possibly encode and perform basic arithmetics in the medial temporal lobe \cite{cantlon_basic_2007,kutter_single_2018,kutter_neuronal_2022}.

\looseness=-1
The ELM neuron demonstrates its ability to solve the summing task across various temporal resolutions, as determined by the bin size. As we vary the bin size from 1ms (comprising 2000 bins in total, corresponding to the maximum temporal detail and longest required memory retention) to 100 (with 20 bins in total, corresponding to the minimum temporal detail and shortest memory retention), the ELM neuron's performance remains robust, degrading only gracefully for greater bin sizes (see Figure~\ref{fig:heidelberg_results}b-d). Further, the performance is also maintained when testing on two held-out speakers, showing that the ELM neuron remains comparatively robust out-of-distribution. 
In contrast, the LSTM struggles with this task, especially when the bin size is below 50, due to memory issues. As the bin size increases, the LSTM's performance does not improve because larger bin sizes leads to the loss of crucial temporal details. This outcome underlines the importance of a model's ability to integrate complex synaptic information effectively (see Figure~\ref{fig:heidelberg_results}e), and the utility of longer neuron-internal timescales for learning long-range dependencies, potentially necessary for cortical neuron's operation (see Figure \ref{fig:heidelberg_results}f).

\subsection{Evaluating on complex and very long temporal dependency tasks}
\label{lsm_benchmark_experiments}

\looseness=-1
To test the extent and limits of the ELM neuron's ability to extract complex long-range dependencies, we use the classic Long Range Arena (LRA) benchmark datasets \cite{tay2021long}. It consists of classification tasks; three image-derived datasets Image, Pathfinder, and Pathfinder-X (images being converted to a  grayscale pixel sequence), and three text-based datasets ListOps, Text, and Retrieval. Pixel and token sequences were encoded categorically, however, only considering 8 or 16 different grayscale levels for images. In particular, the Pathfinder-X task is notoriously difficult, as the task is to determine whether two dots are connected by a path in a  (\textasciitilde 16k) image.

\looseness=-1
\begin{table}
    \centering
    \begin{minipage}{0.99\linewidth}
        \centering
        \begin{minipage}[c]{0.99\linewidth}
            \setlength{\tabcolsep}{2pt}
            \caption{Accuracy on Long Range Arena (LRA) Benchmark \cite{tay2021long}.}
            \centering
            \begin{tabular}{lcccccc}
              \toprule
              & Image & Pathfinder & Pathfinder-X & ListOps & Text & Retrieval \\
              \midrule
              ELM Neuron (ours) & 49.62 & 71.15 & 77.29 & 46.77 & 80.3 & 84.93 \\ 
              Chr.-LSTM \cite{tallec2018can} & 46.09 & 70.79 & \hspace{1.6mm}FAIL & 44.55 & 75.4 & 82.87 \\ 
              \textcolor{greyVar}{\# parameters} & \textcolor{greyVar}{\textasciitilde 100k} &  \textcolor{greyVar}{\textasciitilde 100k} &  \textcolor{greyVar}{\textasciitilde 100k} &  \textcolor{greyVar}{\textasciitilde 100k} &  \textcolor{greyVar}{\textasciitilde 200k} &  \textcolor{greyVar}{\textasciitilde 150k} \\ 
              \midrule
              Transformer \cite{vaswani2017attention} & 42.44 & 71.4 & FAIL & 36.37 & 64.27 & 57.46 \\
              Longformer \cite{beltagy2020longformer} & 42.22 & 69.71 & FAIL & 35.63 & 62.85 & 56.89 \\
S4 \cite{guefficiently} & 87.26 & 86.05 & 88.1 & 58.35 & 76.02 & 87.09\\
              Mega \cite{ma2023mega} & 90.44 & 96.01 & 97.98 & 63.14 & 90.43 & 91.25 \\ 
              \textcolor{greyVar}{\# parameters} &  \textcolor{greyVar}{\textasciitilde 600k} &  \textcolor{greyVar}{\textasciitilde 600k} &  \textcolor{greyVar}{\textasciitilde 600k} &  \textcolor{greyVar}{\textasciitilde 600k} &  \textcolor{greyVar}{\textasciitilde 600k} &  \textcolor{greyVar}{\textasciitilde 600k} \\
              \bottomrule
            \end{tabular}
            \label{tbl:long_sequence_modelling_results}
        \end{minipage}
        \begin{minipage}[c]{0.99\linewidth}
            \vspace{1.5em}          
            \textbf{The ELM neuron can solve challenging long-range sequence modeling tasks.} The mean accuracy and approximate model size are displayed. The ELM neuron routinely scores higher than the Chrono-LSTM or the much larger Transformer or Longformer, and only the large multi-layered architectures tuned specifically for these tasks, such as S4 or Mega, are able outperform it. Surprisingly, it is also the only non purpose built model that is able to reliably solve the notoriously challenging K sample length Pathfinder-X task (albeit using much longer ). Model sizes of the bottom baseline models extracted from \cite{guefficiently}\cite{ma2023mega}\cite{tay2021long}. Training details and model hyper-parameters detailed in Appendix section \ref{datasets_and_training_details}, and Tables \ref{tbl:elm_neuron_config_lra} and \ref{tbl:lstm_neuron_config_lra}.
        \end{minipage}
    \end{minipage}
    \vspace{-5pt} \end{table}

\looseness=-1
Our results are summarized in Table \ref{tbl:long_sequence_modelling_results}, where we compare the ELM neuron against several strong baselines. The model most comparable to ours is an LSTM with derived explicit gating bias initialization for effectively longer internal timescales \cite{tallec2018can} (Chrono-LSTM).
When comparing the two, we find that both models consistently perform well, except on the Pathfinder-X\footnote{Only once during HP tuning did a single Chrono-LSTM run achieve barely above } task which only the ELM is able to reliably solve, albeit using longer  than usual. The larger self-attention-based models trail further behind, with both Transformer \cite{vaswani2017attention} and Longformer \cite{beltagy2020longformer} completely failing to solve the Pathfinder-X task \cite{tay2021long}. Only the purpose built architectures such as S4~\cite{guefficiently} and Mega~\cite{ma2023mega} (current SOTA) perform better, but they require many layers of processing and many more parameters than an ELM neuron, with merely uses 150 memory units and typically \textasciitilde 100k parameters.

\looseness=-1
Overall, the results suggest that the simple ELM neuron architecture is capable of reliably solving challenging tasks with very long temporal dependencies. Crucially this required using memory timescales initialized according to the task length, and highly nonlinear synaptic integration into 150 memory units (See appendix \ref{datasets_and_training_details}). While the LRA benchmark revealed the single ELM neurons limits, we hypothesize that assembling ELM neurons into layered networks might give it enough processing capabilities to catch up with the deep models, but leave this investigation to future work.

\vspace{-3pt} \section{Discussion}
\label{discussion}
\vspace{-3pt}

\looseness=-1
In this study, we introduced a biologically inspired recurrent cell, the Expressive Leaky Memory (ELM) neuron, and demonstrated its capability to fit the full spike-level input/output mapping of a high-fidelity biophysical neuron model (NeuronIO). Unlike previous works that achieved this fit with millions of parameters, a variant of our model only requires a few thousand, thanks to the careful design of the architecture exploiting appropriate inductive biases. Furthermore, unlike existing neuron models, the ELM can effectively model neuron without making rigid assumptions about the number of memory states and their timescales, or the degree of nonlinearity in its synaptic integration.

\looseness=-1
We further scrutinized the implications and limitations of this design on various long-range dependency datasets, such as a biologically-motivated neuromorphic dataset (SHD-Adding), and some notoriously challenging ones from the machine learning literature (LRA). Leveraging slowly decaying memory units and highly nonlinear dendritic integration into multiple memory units, the ELM neuron was found to be quite competitive, in particular, compared to classic RNN architectures like the LSTM, a notable feat considering its much simpler architecture and biological inspiration.

\looseness=-1
It should be noted though, that in spite of its biological motivation, our model cannot give mechanistic explanations of neural computations as biophysical models do. Many biological implementation details are abstracted away in favor of computational efficiency and conceptual insight, and the required/recovered ELM neuron hyper-parameters are dependent on what constitutes a sufficiently good fit and the models subsequent use-case. Furthermore, ELM learning likely relies on neuronal plasticity beyond synapses, the extent of which in biological neurons is still a subject of debate. Additionally, our neuron model is missing some prominent macroscopic features like the distinction between apical/basal dendrites, and the modeling of dendrites is rudimentary and relies on oversampling synaptic inputs so far. Finally, given the use of BPTT for training, and the comparatively larger ELM neuron sizes on the later datasets, one should be careful to directly draw conclusions about the learning capabilities of individual biological cortical neurons.

\looseness=-1
Despite these caveats, the ELM's ability to efficiently fit cortical neuron I/O and it's promising performance on machine learning tasks suggests that we are beginning to incorporate the inductive biases that drive the development of more intelligent systems. Future research focused on connecting smaller ELM neurons into larger networks could provide even more insights into the necessary and dispensable elements for building smarter machines.

\subsubsection*{Acknowledgments}

This work was supported by a Sofja Kovalevskaja Award from the Alexander von Humboldt Foundation. We acknowledge the support from the BMBF through the T\"ubingen AI Center (FKZ: 01IS18039A and 01IS18039B). AL, GM, and BS are members of the Machine Learning Cluster of Excellence, EXC number 2064/1 – Project number 39072764. Majority of computations were performed on the HPC system Raven at the Max Planck Computing and Data Facility.



\newpage
\bibliography{ms.bib}
\bibliographystyle{iclr2024_bib_style}

\include{appendix.tex}

\end{document}