\section{Experimental Results}
\label{sec:Experiment}


\subsection{Dataset}
The popular Vehicle Detection in Aerial Imagery (VEDAI) dataset  \cite{razakarivony2016vehicle} is used in the experiments, which contains cropped images obtained from the much larger Utah Automated Geographic Reference Center (AGRC) dataset. Each image collected from the same altitude in AGRC has approximately $16,000\times 16,000$ pixels, with a resolution of about $ 12.5 cm \times 12.5 cm $ per pixel. RGB and IR are the two modalities for each image in the same scenes. The VEDAI dataset consists of 1246 smaller images that focus on diverse backgrounds involving grass, highway, mountains, and urban areas. All images are in the size of $1024\times 1024$ or $512 \times 512 $. The task is to detect 11 classes of different vehicles such as car, pickup, camping, and truck. 







\subsection{Implementation Details}
Our proposed framework is implemented in PyTorch and runs on a workstation with an NVIDIA 3090 GPU. The VEDAI dataset is used to train our SuperYOLO. Following \cite{9273212}, the VEDAI dataset is devised to 10 fold cross-validation. In each split 1089 images are used for training and another 121 images are used for testing. The ablation experiments are conducted on the first fold of data, while the comparisons with previous methods are performed on the 10 folds by averaging their results. The annotations for each object in the image contain the coordinates of the bounding box center, the orientation of the object concerning the positive $x$-axis, the four corners of the bounding box, the class ID, a binary flag identifying whether an object is occluded, and another binary flag identify whether an object is cropped. We do not consider classes with fewer than 50 instances in the dataset, such as plane, motorcycle, and bus. So the annotations of the VEDAI dataset are converted to YOLOv5 format, and we transfer the ID of the interested class to $0, 1, ..., 7$, i.e., $N=8$. Then the center coordinates of the bounding box are normalized and the absolute coordinate is transformed into a relative coordinate. Similarly, the length and width of the bounding box are normalized to $[0,1]$. To realize the SR assisted branch, the input images of the network are downsampled from $1024 \times 1024$ size to $512 \times 512$ during the training process. In the test process, the image size is $512 \times 512$, which is consistent with the input of other algorithms compared. In addition, data is augmented with Hue Saturation Value (HSV), multi-scale, translation, left-right flip, and mosaic. The augmentation strategy is canceled in the test stage. The standard Stochastic Gradient Descent (SGD) \cite{bottou2010large} is used to train the network with a momentum of 0.937, weight decay of 0.0005 for the Nesterov accelerated gradients utilized, and a batch size of 2. The learning rate is set to 0.01 initially. The entire training process involves 300 epochs. 




\subsection{Accuracy Metrics}
The accuracy assessment measures the agreements and differences between the detection result and the reference mask. The recall, precision, and mAP (mean Average Precision) are used as accuracy metrics to evaluate the performance of the methods to be compared with. The calculations of the precision and recall metrics are defined as
\begin{equation}
	Precision=\frac{TP}{TP+FP}
\end{equation}
\begin{equation}
	Recall=\frac{TP}{TP+FN}.
\end{equation}
\begin{table}[htpb]
	\small
\centering
	\setlength{\tabcolsep}{1.5mm}{
		\caption{The Comparison Results of Model Size and Inference Ability in Different Baseline YOLO Frameworks on the First Fold of the VEDAI Validation Set.}
		\label{modelsize}
		\begin{tabular}{c|c|c|c|c}
			\toprule[1.2pt]
			\textbf{Method}  & \textbf{Layers} $\downarrow$ & \textbf{Params} $\downarrow$& \textbf{GFLOPs} $\downarrow$&  \textbf{$\text{mA}{{\text{P}}_{\text{50}}}$} $\uparrow$ \\
			\midrule
			YOLOv3  & 270   & 61.5M  & 52.8  & 62.6    \\
			YOLOrs  & 241   & 20.2M  & 46.4  & 55.8  \\
			YOLOv4  & 393  & 52.5M   & 38.2  & \textbf{65.7}   \\
			YOLOv5s & \textbf{224}    & \textbf{7.1M}          & \textbf{5.32}  & 62.2   \\
			YOLOv5m & 308  & 21.1M  & 16.1  & 64.5  \\
			YOLOv5l & 397& 46.6M & 36.7  & 63.9  \\
			YOLOv5x & 476  & 87.3M  & 69.7 & 64.0 \\
			\bottomrule[1.2pt]
	\end{tabular}}
\vspace{-0.1in}
\end{table} where the true positive (TP) and true negative (TN) denote correct prediction, and the false positive (FP) and false negative (FN) denote incorrect outcome. The precision and recall are correlated with the commission and omission errors, respectively. The mAP is a comprehensive indicator obtained by averaging AP values, which uses an integral method to calculate the area enclosed by the Precision-Recall curve and coordinate axis of all categories. Hence, the mAP can be calculated by
\begin{equation}
	mAP=\frac{AP}{N}=\frac{\int_{0}^{1}{p(r)dr}}{N},
\end{equation}
where  $p$ denotes Precision, $r$ denotes Recall, and $N$ is the number of categories. 

GFOLPs (Giga Floating-point Operations Per Second) and parameter size are used to measure the model complexity and computation cost. In addition, PSNR and SSIM are used for image quality evaluation of the SR branch. Generally, higher PSNR values and SSIM values represent the better quality of the generated image.



\subsection{Ablation Study}
\label{sec: Ablation Study}

First of all, we verify the effectiveness of our proposed method by designing a series of ablation experiments which are conducted on the first fold of the validation set.





\subsubsection{\textbf{Validation of the Baseline Framework}} 
In Table \ref{modelsize}, the model size and inference ability of different base frameworks are evaluated in terms of the number of layers, parameter size and GFLOPs. The detection performances of those models are measured by $\text{mA}{{\text{P}}_{\text{50}}}$, i.e., detection metric of mAP at IOU (Intersection over Union) = 0.5. Although YOLOv4 achieves the best detection performance, it has 169 more layers than YOLOv5s (393 vs. 224), its parameter size (params) is 7.4 times larger than that of YOLOv5s (52.5M vs. 7.1M), and its GFLOPs is 7.2 times higher than that of YOLOv5s (38.2 vs. 5.3). With respect to YOLOv5s, although its mAP is slightly lower than those of YOLOv4 and YOLOv5m, its number of layers, parameter size and GFLOPs are much smaller than those of other models. Therefore, it is easier to deploy YOLOv5s on board to achieve real-time performance in practical applications. The above fact verifies the rationality of YOLOv5s as the baseline detection framework.



\begin{table}[htpb]
\small
\centering
	\caption{Influence of Removing the Focus Module in the Network on the First Fold of the VEDAI Validation Set.}
	\label{Focus}
	\scalebox{0.95}{
	\begin{tabular}{c|c|c|c|c}
		\toprule[1.2pt]
		\multicolumn{2}{c|}{\textbf{Method}}  & \textbf{Params} $\downarrow$& \textbf{GFLOPs}$\downarrow$  & \textbf{$\text{mA}{{\text{P}}_{\text{50}}}$} $\uparrow$ \\
		\midrule
		\multirow{2}{*}{YOLOv5s} & Focus    & 7.0739M  & 5.3  & 62.2 \\
		& noFocus  & 7.0705M  & 20.4 & \textbf{69.5} \textcolor{blue}{(+7.3)}\\
		\midrule
		\multirow{2}{*}{YOLOv5m} & Focus   & 21.0677M  & 16.1 & 64.5\\
		& noFocus & 21.0625M    & 63.6 & \textbf{72.2} \textcolor{blue}{(+7.7)} \\
		\midrule
		\multirow{2}{*}{YOLOv5l} & Focus  & 46.6406M   & 36.7  & 63.7 \\
		& noFocus  & 46.6337M  & 145.0 & \textbf{72.5} \textcolor{blue}{(+8.8)}  \\
		\midrule
		\multirow{2}{*}{YOLOv5x} & Focus  & 87.2487M  & 69.7 & 64.0\\
		& noFocus  & 87.2400M  & 276.6 & \textbf{69.2} \textcolor{blue}{(+5.2)} \\
		\bottomrule[1.2pt]
	\end{tabular}}
\vspace{-0.1in}
\end{table} \begin{table}[htpb]
	\small
\centering
\caption{The Comparison Result of Pixel-level and Feature-level Fusions in YOLOv5s (noFocus) for Multimodal Dataset on the First Fold of the VEDAI Validation Set.}
			\label{fusioncomparison}
			\scalebox{0.95}{
				\begin{tabular}{c|c|c|c|c}
					\toprule[1.2pt]
					\multicolumn{2}{c|}{\textbf{Method}}  & \textbf{Params} $\downarrow$ & \textbf{GFLOPs} $\downarrow$& \textbf{$\text{mA}{{\text{P}}_{\text{50}}}$} $\uparrow$ \\
					\midrule
					\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Pixel-level} \\   \textbf{Fusion}\end{tabular}} &Concat & \textbf{7.0705M}   & \textbf{20.37} & 69.5   \\
				    & MF & 7.0897M  & 21.67  & \textbf{70.3} \\
					\midrule
					\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}\textbf{Feature-level} \\ \textbf{Fusion}\end{tabular}} & Fusion1   & 7.0887M   & 21.76   & 66.0  \\
					& Fusion2   & 7.0744M   & 22.04   & 68.5  \\
					& Fusion3   & 7.1442M   & 24.22   & 64.8  \\
					& Fusion4   & 7.0870M   & 24.50   & 63.8  \\  
				    \midrule
				    \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}\textbf{Multistage Feature-level}\\ \textbf{Fusion}\end{tabular}}    & 7.7545M   & 34.56   & 59.3  \\
				   \bottomrule[1.2pt]
				    
			\end{tabular}}
			\vspace{-0.1in}
\end{table} 
\subsubsection{\textbf{Impact of Removing Focus Module}}
As presented in Section~\ref{Sec:Focus Removal}, the Focus module reduces the resolution of input images, which imposes encumbrance on the detection performance of small objects in RSI. To investigate the influence of the Focus module, we conduct experiments on the four YOLOv5 network frameworks: YOLOv5s, YOLOV5m, YOLOv5l, and YOLOv5x. Note that the results here are collected after the concatenation pixel-level fusion of RGB and IR modalities. As listed in Table \ref{Focus}, after removing the Focus module, we observe a noticeable improving in the detection performance of YOLOv5s (62.2\%$\to$69.5\% in $\text{mA}{{\text{P}}_{\text{50}}}$), YOLOv5m (64.5\%$\to$72.2\%), YOLOV5l (63.7\%$\to$72.5\%), YOLOv5x (64.0\%$\to$69.2\%). This is because by removing the Focus module, not only can the resolution degradation be avoided, but also the spatial interval information be retained for small objects in RSI, thereby reducing the missing errors of object detection. Generally, removing the Focus module brings more than 5\% improvement in the detection performance ($\text{mA}{{\text{P}}_{\text{50}}}$) of the whole frameworks. 

Meanwhile, we notice that the above removal increases the inference computation cost (GFLOPs) in YOLOv5s (5.3$\to$20.4), YOLOv5m (16.1$\to$63.6), YOLOV5l (36.7$\to$145), YOLOv5x (69.7$\to$276.6). However, the GFLOPs of YOLOv5s-noFocus (20.4) is smaller than those of YOLOv3 (52.8), YOLOv4 (38.2), and YOLOrs (46.4), as shown in Table \ref{modelsize}. The parameters of these models are slightly reduced after removing the Focus module. In summary, in order to retain the resolution to better detect smaller objects, priority shall be given to the detection accuracy, for which the convolution operation is adopted to replace the Focus module.

\begin{figure}[htpb]
	\centering
\includegraphics[scale=0.74]{./Figures/midfusion.pdf}
	\centering
	\caption{The feature-level fusion of different blocks in the latent layers. Fusion-n represents the concatenation fusion operation performed in the n-th blocks. (a) and (b) is feature-level fusion and (c) is multistage feature-level fusion.}
	\vspace{-0.1in}
	\label{midfusion}
\end{figure}




\subsubsection{\textbf{Comparison of Different Fusion Methods}}
To evaluate the influence of the devised fusion methods, we compare five fusion results on YOLOv5-noFocus, as presented in Section~\ref{subsec:fusion}. As shown in Fig \ref{midfusion}, fusion1, fusion2, fusion3, and fusion4 represent the concatenation fusion operation performed in the first, second, third, and fourth blocks, respectively. The IR image is expanded to three bands in feature-level fusion to obtain the features which have equal channels for the two modes. The final result is listed in TABLE \ref{fusioncomparison}. The parameter size, GFLOPs, $\text{mA}{{\text{P}}_{\text{50}}}$ of pixel-level fusion with concatenation operation are 7.0705M, 20.37 and 69.5\%,  and these of the pixel-level fusion with MF module are 7.0897M, 21.67 and 70.3\% which are the best among all the compared methods. There are some reasons why the model parameters of the feature-level fusions are close to the pixel-level fusion. First, the feature-level fusion is completed in the latent layers rather than the whole two separate models. Second, the modules before the concatenation fusion are different, making the different fusion channels cause different parameters. However, it can be proved that calculation cost is in creased with the layer of fusion becomes deeper. In addition, we compare the multistage feature-level fusion (shown in Fig. 7 (c)) with the proposed pixel-level fusion. As shown in TABLE \ref{fusioncomparison} the accuracy of multistage feature-level fusion is only 59.3\% $\text{mA}{{\text{P}}_{\text{50}}}$ lower than that of pixel-level fusion, while its computation cost is 34.56 GFLOPs with 7.7545M parameters, which is higher than that of pixel-level fusion. These findings suggest that innovative pixel-level fusion methods are more effective than multistage shallow feature-level fusion. Because the multiple stages of fusion can lead to the accumulation of redundant information. The above results suggest that pixel-level fusion can accurately detect objects while reducing the computation. Our proposed MF fusion can improve detection accuracy with some computation costs. Overall, the proposed method only uses pixel-level fusion to contain the lower computation cost.



\begin{table}[htbp]
	\small
\centering
	\setlength{\tabcolsep}{1.2mm}{
\caption{The Influence of Different Resolutions for Input Image on Network Performance on the First Fold of the VEDAI Validation Set.}
			\label{resolution}
			\begin{tabular}{c|c|c|c|c|c}
				\toprule[1.2pt]
				\textbf{Method}  
				& \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}} \textbf{Train-Val} \\ \textbf{Size} \end{tabular}}
				& \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}} \textbf{Test} \\ \textbf{Size} \end{tabular}}
				& \textbf{Params} $\downarrow$ & \textbf{GFLOPs} $\downarrow$  & \textbf{$\text{mA}{{\text{P}}_{\text{50}}}$} $\uparrow$ \\
				\midrule
				\textbf{\multirow{4}{*}{YOLOv5s}}  & \multirow{2}{*}{512}   & 512  & {7.0739M} & \textbf{5.3}  & \textbf{62.2} \\
				&  & 1024 & {7.0739M} & 21.3 & 10.6 \\ 
				\cmidrule{2-6} 
				& \multirow{2}{*}{1024} & 1024 & {7.0739M} & 21.3 & \textbf{77.7} \\
				&   & 512 & {7.0739M} & \textbf{5.3}  & 48.2 \\
				\midrule
				\textbf{\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}YOLOv5s\\ (noFocus)\end{tabular}}} & \multirow{2}{*}{512}   & 512  & {7.0705M} & \textbf{20.4}  & \textbf{69.5} \\
				&  & 1024 & {7.0705M} & 81.5 & 13.4 \\ 
				\cmidrule{2-6} 
				& \multirow{2}{*}{1024} & 1024 & {7.0705M} & 81.5 & \textbf{79.3} \\
				&  & 512 & {7.0705M} & \textbf{20.4} & 62.9 \\ 
				\midrule
				\textbf{ \begin{tabular}[c]{@{}c@{}}YOLOv5s\\ (noFocus)\\ +SR\end{tabular}}  & 512  & 512  & {7.0705M} & \textbf{20.4} & \textbf{78.0} \\
				\bottomrule[1.2pt]  
		\end{tabular}}
\vspace{-0.1in}               
\end{table} \begin{table*}[htbp]
\small
\centering
\caption{The Ablation Experiment Results about the Influence of SR Branch on Detection Performance on the First Fold of the VEDAI Validation Set.}
	\label{ablation}
	\begin{tabular}{c|c|c|c|c|c|c|c|c|c}	
		\toprule[1.2pt]
		\multirow{8}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}YOLOv5s\\ (noFocus)\end{tabular}} } &
\textbf{\begin{tabular}[c]{@{}c@{}}Branch \\ \end{tabular}} & 
		\textbf{\begin{tabular}[c]{@{}c@{}}Small-scale\\ Detector\end{tabular}} &
		\textbf{\begin{tabular}[c]{@{}c@{}}Decoder\\ (EDSR)\end{tabular}} &
		\textbf{L1 Loss} &
		\textbf{Params} $\downarrow$&
		\textbf{GFLOPs} $\downarrow$&
		\textbf{$\text{mA}{{\text{P}}_{\text{50}}}$} $\uparrow$&
		\textbf{PSNR} $\uparrow$&
		\textbf{SSIM} $\uparrow$\\
		\midrule
& Upsample & &              &              & 7.0705M & 20.37G & 76.2   & - & - \\
		& SR & &              &              & 7.0705M & 20.37G & 78.0   & - & - \\
		& SR& $\checkmark$ &              &              & 4.8259M & 16.68G & 79.0   & 23.811 & 0.602 \\
		& SR  & $\checkmark$ & $\checkmark$ &              & 4.8259M & 16.68G & 79.9 & 23.902 & 0.604 \\
		&  SR  & $\checkmark$ & $\checkmark$ & $\checkmark$ & \textbf{4.8259M} & \textbf{16.68G} & \textbf{80.9} & \textbf{26.203} & \textbf{0.659} \\
		\bottomrule[1.2pt]   
	\end{tabular}
\vspace{-0.1in}
\end{table*}

 \begin{table}[htpb]
	\small
	\centering
	{\caption{The Effective Validation of  the Super Resolution branch for the Different  Baseline on on the First Fold of the VEDAI Validation Set.}
	\label{ablationsr}
	\begin{tabular}{c|c|c|c|c}
		\toprule[1.2pt]
        \textbf{Method} & \textbf{Layers} & \textbf{Params} $\downarrow$ & \textbf{GFLOPs} $\downarrow$ & \textbf{$\text{mA}{{\text{P}}_{\text{50}}}$} $\uparrow$ \\
		\midrule
		\textbf{YOLOv3}     & 270 & 61.5M & 52.8 & 62.6 \\
		\textbf{YOLOv3+SR}  & 270 & 61.5M & 52.8 & \textbf{71.8} \\
		\midrule
		\textbf{YOLOv4}     & 393 & 52.5M & 38.2 & 65.7 \\
		\textbf{YOLOv4+SR}  & 393 & 52.5M & 38.2 & \textbf{69.0} \\
		\midrule
		\textbf{YOLOv5s}    & 224 & 7.1M  & 5.3  & 62.2 \\
		\textbf{YOLOv5s+SR} & 224 & 7.1M  & 5.3  & \textbf{64.4}   \\ 
		\bottomrule[1.2pt]                                     
	\end{tabular}}
\vspace{-0.1in}
\end{table}


 









\subsubsection{\textbf{Impact of High Resolution}}
We compare different training and test modes to explore more possibilities in terms of the input image resolution in TABLE \ref{resolution}. First, we compare cases where the image resolutions of the training set and test set are the same.
By comparing the result of YOLOv5s, the detection metric $\text{mA}{{\text{P}}_{\text{50}}}$ is improved from 62.2\% to 77.7\%, causing 15.5\% increase  when the image size is doubled from 512 to 1024. Similarly, YOLOv5s-noFocus (1024) outperforms YOLOv5s-noFocus (512) by 9.8\% $\text{mA}{{\text{P}}_{\text{50}}}$ score (79.3\% vs. 69.5\%). The mean recall and mean precision increase simultaneously, suggesting that ensuring resolution reduces the commission and omission errors in object detection. Based on the above analysis, we argue that the characteristics of HR significantly influence the final performance of object detection. However, it is noteworthy that maintaining an HR input image of the network introduces a certain amount of calculation. The GFLOPs with a size of 1024 (high resolution) is higher than that with 512 (low resolution) in both YOLOv5s (21.3 vs. 5.3) and YOLOv5s-noFocus (81.5 vs. 20.4). 

As shown in Table \ref{resolution}, the use of different sizes of the image during the training process (train size) and the test process (test size) results in the score reduction of $\text{mA}{{\text{P}}_{\text{50}}}$, i.e.,  (10.6\% vs. 62.2\%), (48.2\% vs. 77.7\%), (13.4\% vs. 69.5\%) and (62.9\% vs. 79.3\%). This may attribute to the inconsistent scale of objects in the test process and in the training process, where the size of the predicted bounding box is not suitable for the objects of test images anymore. 


Finally, the $\text{mA}{{\text{P}}_{\text{50}}}$ of YOLOv5s-noFocus+SR is close to the YOLOv5-noFocus  HR (1024) one (78.0\% vs. 79.3\%), and the GFOLPs is equal to that of YOLOv5-noFocus LR (512)  one (20.4 vs. 20.4). Our proposed network decreased the resolution of input images in the test process to reduce computation and maintain accuracy by remaining the identical resolution of the training and testing data, thereby highlighting the advantage of the proposed SR branch.

\subsubsection{\textbf{Impact of Super Resolution Branch}}

Some ablation experiences about the SR branch are completed in Table \ref{ablation}. Compared with the upsampling operation, the YOLOv5s (noFocus) added super resolution network shows favorable performance and gets $\text{mA}{{\text{P}}_{\text{50}}}$ 1.8\% better than upsampling operation. The SR network is a learnable upsampling method with a more vital reconstruction ability that can help the feature extraction in the backbone for detection.  We deleted the PANet structure and two detectors, which are responsible for enhancing middle-scale and large-scale target detection because the objects RSI datasets, such as VEDAI are on the small scale and can be detected with the small-scale detector. When we only use one detector, the number of parameters (7.0705M vs. 4.8259M) and GFLOPs (20.37 vs. 16.68) can be decreased, and the detection accuracy can be increased (78.0\%  vs 79.0\% ). When we utilize the EDSR network (rather than three ordinal deconvolutions) as Decoder and L1 loss (rather than L2 loss) as SR loss function in the SR branch, which is powerful in the SR task, not only the performance of SR is improved but also the performance of the detection network enhanced meantime because the SR branch helps the detection network to extract more effective and superior features in the backbone, accelerating the convergence of the detection network and thus improving the performance of the detection network. The performance of super resolution and object detection is complementary and cooperative. 

Table \ref{ablationsr} shows the favorable accuracy-complexity tradeoff of the SR branch. At the different baselines, the influence of the SR branch on object detection is positive. Compared with bare baseline, baseline added super resolution shows favorable performance: YOLOv3+SR performs $\text{mA}{{\text{P}}_{\text{50}}}$ 9.2\% better than YOLOv3, YOLOv4+SR is $\text{mA}{{\text{P}}_{\text{50}}}$ 3.3\% better than YOLOv4, YOLOv5s+SR performs $\text{mA}{{\text{P}}_{\text{50}}}$ 2.2\% better than YOLOv5s. Notably, super resolution can be removed in the inference stage. Hence no extra parameters and computation costs are introduced, which is impressive considering that the SR branch does not require a lot of manpower to refine the design of the detection network. The SR branch is general and extensible and can be utilized in the existing fully convolutional network (FCN) framework.


\begin{figure*}[htpb]
	\centering
	\includegraphics[scale=0.85]{./Figures/results_FP.png}
	\centering
	\caption{Visual results of object detection using different methods involving YOLOv4, YOLOv5s, YOLOv5m and the proposed SuperYOLO. The red cycles represent the False Alarms, the yellow ones denote the False Positive detection results and the blue ones are False Negative detection results. (a)-(e) is the different images in the VEDAI dataset.}
	\vspace{-0.1in}
	\label{results}
\end{figure*}
\begin{table*}[htpb]
	\small
	\renewcommand{\arraystretch}{1.2}
	\centering
\setlength{\tabcolsep}{1mm}{
	\caption{Class-wise Average Precision AP, Mean Average Precision {\upshape$\text{mA}{{\text{P}}_{\text{50}}}$}, Parameters and GFLPs  for Proposed SuperYOLO, YOLOv3, YOLOv4, YOLOv5s-x, YOLOrs, YOLO-Fine and YOLOFusion Including Unimodal and Multimodal Configurations on VEDAI Dataset. * Represents Using Pre-trained Weight.}
	\label{comparedresult}
	\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c}
		\toprule[1.2pt]           		\multicolumn{2}{c|}{\textbf{Method}}           & \textbf{Car}            & \textbf{Pickup}         & \textbf{Camping}        & \textbf{Truck}         & \textbf{Other}          & \textbf{Tractor}        & \textbf{Boat}           & \textbf{Van}            & \textbf{$\text{mA}{{\text{P}}_{\text{50}}}$}  $\uparrow$ & \textbf{Params} $\downarrow$ & \textbf{GFLOPs} $\downarrow$ \\
		\midrule
		\textbf{\multirow{3}{*}{YOLOv3 \cite{redmon2018yolov3}}}& IR         & 80.21          & 67.03          & 65.55          & 47.78          & 25.86          & 40.11          & 32.67          & 53.33          & 51.54    &\textbf{61.5351M} &\textbf{49.55}      \\
		& RGB        & 83.06          & 71.54          & \textbf{69.14} & 59.30          & \textbf{48.93} & \textbf{67.34} & 33.48          & 55.67          & 61.06       &\textbf{61.5351M} &\textbf{49.55}   \\
		& Multi & \textbf{84.57} & \textbf{72.68} & 67.13          & \textbf{61.96} & 43.04          & 65.24          & \textbf{37.10} & \textbf{58.29} & \textbf{61.26} & 61.5354M & 49.68\\
		\midrule
		\textbf{\multirow{3}{*}{YOLOv4 \cite{bochkovskiy2020yolov4}}}  & IR         & 80.45          & 67.88          & 68.84          & 53.66          & 30.02          & 44.23          & 25.40          & 51.41          & 52.75       &\textbf{52.5082M} &\textbf{38.16}   \\
		& RGB        & 83.73          & \textbf{73.43} & 71.17          & 59.09          & \textbf{51.66} & 65.86          & \textbf{34.28} & \textbf{60.32} & 62.43         &\textbf{52.5082M} &\textbf{38.16} \\
		& Multi & \textbf{85.46} & 72.84          & \textbf{72.38} & \textbf{62.82} & 48.94          & \textbf{68.99} & \textbf{34.28} & 54.66          & \textbf{62.55} & 52.5085M  & 38.23 \\
		\midrule
		\textbf{\multirow{3}{*}{YOLOv5s \cite{yolov5}}} & IR         & 77.31          & 65.27          & 66.47          & 51.56          & 25.87          & 42.36          & 21.88          & 48.88          & 49.94       & \textbf{7.0728M}  & \textbf{\textcolor{blue}{5.24}}   \\
		& RGB        & 80.07          & 68.01          & 66.12          & 51.52          & 45.76          & \textbf{64.38} & 21.62          & 40.93          & 54.82          & \textbf{7.0728M}  & \textbf{\textcolor{blue}{5.24}} \\
		& Multi & \textbf{80.81} & \textbf{68.48} & \textbf{69.06} & \textbf{54.71} & \textbf{46.76} & 64.29          & \textbf{24.25} & \textbf{45.96} & \textbf{56.79} & 7.0739M        & 5.32 \\
		\midrule
		\textbf{\multirow{3}{*}{YOLOv5m \cite{yolov5}} }& IR         & 79.23          & 67.32          & 65.43          & 51.75          & 26.66          & 44.28          & 26.64          & 56.14          & 52.19    &\textbf{21.0659M} &\textbf{16.13}      \\
		& RGB        & 81.14          & 70.26          & 65.53          & 53.98          & \textbf{46.78} & \textbf{66.69} & \textbf{36.24} & 49.87          & 58.80     &\textbf{21.0659M} &\textbf{16.13}     \\
		& Multi & \textbf{82.53} & \textbf{72.32} & \textbf{68.41} & \textbf{59.25} & 46.20          & 66.23          & 33.51          & \textbf{57.11} & \textbf{60.69}  & 21.0677M  & 16.24 \\
		\midrule
		\textbf{\multirow{3}{*}{YOLOv5l \cite{yolov5}}} & IR         & 80.14          & 68.57          & 65.37          & 53.45          & 30.33          & 45.59          & 27.24          & \textbf{61.87} & 54.06     &\textbf{46.6383M} &\textbf{36.55}     \\
		& RGB        & 81.36          & 71.70          & 68.25          & 57.45          & 45.77          & \textbf{70.68} & 35.89          & 55.42          & 60.81    &\textbf{46.6383M} &\textbf{36.55}      \\
		& Multi & \textbf{82.83} & \textbf{72.32} & \textbf{69.92} & \textbf{63.94} & \textbf{48.48} & 63.07          & \textbf{40.12} & 56.46          & \textbf{62.16} &46.6406M &36.70 \\
		\midrule
		\textbf{\multirow{3}{*}{YOLOv5x \cite{yolov5}} }& IR         & 79.01          & 66.72          & 65.93          & 58.49          & 31.39          & 41.38          & 31.58          & 58.98          & 54.18     &\textbf{87.2458M} &\textbf{69.52}     \\
		& RGB        & 81.66          & 72.23          & 68.29          & 59.07          & 48.47          & 66.01          & \textbf{39.15} & \textbf{61.85} & 62.09        &\textbf{87.2458M} &\textbf{69.52}   \\
		& Multi & \textbf{84.33} & \textbf{72.95} & \textbf{70.09} & \textbf{61.15} & \textbf{49.94} & \textbf{67.35} & 38.71          & 56.65          & \textbf{62.65} & 87.2487M &69.71\\
		\midrule
		\textbf{\multirow{3}{*}{YOLOrs \cite{9273212}} }& IR         & 82.03          & 73.92         & 63.80          & \textbf{54.21 }        & 43.99          & 54.39          & \textbf{21.97 }         & 43.38          & 54.71     &- &-     \\
		& RGB        & \textbf{85.25}          & 72.93          & \textbf{70.31}          & 50.65         & 42.67          & \textbf{76.77}          & 18.65 & 38.92 & 57.00        &- &-  \\
		& Multi & 84.15 & \textbf{78.27} & 68.81 & 52.60 & \textbf{46.75} & 67.88 & 21.47         & \textbf{57.91}         & \textbf{59.73} & - &-\\
		\midrule
		\textbf{\multirow{2}{*}{YOLO-Fine \cite{pham2020yolo}} }& IR       & 76.77          & 74.35         & 64.74         & 63.45          & \textbf{45.04}          & \textbf{78.12}          & \textbf{70.04} & \textbf{77.91} & 68.18        &- &-   \\
		& RGB         & \textbf{79.68}         & \textbf{74.49}        & \textbf{77.09}          & \textbf{80.97}          & 37.33          & 70.65          & 60.84          & 63.56         & \textbf{68.83}     &- &-     \\
		\midrule
		\textbf{\multirow{3}{*}{YOLOFusion* \cite{qingyun2022cross}} } & IR        & 86.7          & 75.9          &66.6         & 77.1         & 43.0          & 62.3          & 70.7 & \textbf{84.3} & 70.8       &- &-  \\
		& RGB       & 91.1          & 82.3         & 75.1          & \textbf{78.3}        & 33.3          & \textbf{81.2}          & \textbf{71.8}         & 62.2          &  71.9    &- &-     \\
		& Multi & \textbf{91.7} & \textbf{85.9} & \textbf{78.9} & 78.1 & \textbf{54.7}& 71.9 & 71.7         &  75.2    & \textbf{\textcolor{blue}{75.9}}  & 12.5M &-\\
		\midrule
\textbf{\multirow{3}{*}{SuperYOLO}} & IR         & 87.90          & 81.39          & 76.90          & 61.56          & 39.39          & 60.56          & 46.08          & 71.00 & 65.60          & \textbf{\textcolor{blue}{4.8256M}}  & \textbf{16.61}\\
		& RGB        & 90.30          & 82.66 & 76.69          & 68.55          & 53.86          & 79.48 & 58.08          & 70.30          & 72.49         & \textbf{\textcolor{blue}{4.8256M}}  & \textbf{16.61}\\
& Multi & \textbf{91.13} & \textbf{85.66}        & \textbf{79.30} & \textbf{70.18} & \textbf{57.33} & \textbf{80.41}        & \textbf{60.24} & \textbf{76.50}          & \textbf{\textcolor{blue}{75.09}} & 4.8451M  & 17.98 \\
		\bottomrule[1.2pt]
	\end{tabular}}
\vspace{-0.1in}
\end{table*}
 








\subsection{Comparisons with Previous Methods}
The visual detection results of the compared YOLO methods and SuperYOLO are shown in Fig. \ref{results}, for a diverse set of scenes. It can be observed that SuperYOLO can accurately detect those objects that are not detected, or predicted into a wrong category or with uncertainty, in YOLOv4, YOLOv5s, and YOLOv5m. The objects in RSIs are challenging to detect on small scales. In particular, \textbf{Pickup} and \textbf{Car} or \textbf{Van} and \textbf{Boat} are easily confused in the detection process due to their similarities. Hence, improving the detection classification is of essential necessity in object detection tasks except for location detection, which can be accomplished by the proposed SuperYOLO with better performance. 

TABLE \ref{comparedresult} summarizes the performance of the \textbf{YOLOv3 \cite{redmon2018yolov3}}, \textbf{YOLOv4  \cite{bochkovskiy2020yolov4}}, and \textbf{YOLOv5s-x} \cite{yolov5} \textbf{YOLOrs } \cite{9273212}, \textbf{YOLO-Fine} \cite{pham2020yolo}, \textbf{YOLOFusion} \cite{qingyun2022cross} and our proposed \textbf{SuperYOLO}. Note that the AP scores of multimodal modes are significantly higher than those of unimodal (RGB or IR) modes for most classes. The overall $\text{mA}{{\text{P}}_{\text{50}}}$ of multimodal (multi) modes outperforms those of RGB or IR modes. These results confirm that multimodal fusion is an effective and efficient strategy for object detection based on information complementation between multimodal inputs. However, it should be noted that the slight increase in parameters and GFLOPs with multimodal fusion reflects the necessity of choosing pixel-level fusion rather than feature-level fusion.

It is obvious that the SuperYOLO achieves higher $\text{mA}{{\text{P}}_{\text{50}}}$ than the other frameworks except for YOLOFusion. The results of YOLOFusion are slightly better than SuperYOLO, as YOLOFusion uses pre-trained weight which is trained on MS COCO \cite{lin2014microsoft}. However, its parameter count is approximately three times that of SuperYOLO. The performance of YOLO-Fine is good on a single modality, but it lacks the development of multi-modality fusion techniques. In particular, the SuperYOLO outperforms the YOLOv5x by a 12.44\% $\text{mA}{{\text{P}}_{\text{50}}}$ score in multimodal mode. Meanwhile, parameter size and GFLOPs of SuperYOLO are about 18x and 3.8x less than YOLOv5x. 

In addition, it can be noticed that the superior performance is achieved for the classes of \textbf{Car}, \textbf{Pickup}, \textbf{Tractor} and \textbf{Camping}, which have the most training instances. YOLOv5s performs superior on GFLOPs, which depends on the Focus module to slim the input image, but results in lousy detection performance, especially for small objects. The SuperYOLO performs 18.30\% $\text{mA}{{\text{P}}_{\text{50}}}$ better than YOLOv5s. Our proposed SuperYOLO shows a favorable speed-accuracy trade-off compared to the state-of-the-art models.








\subsection{Generalization to single modal remote sensing images}
At present, although there are massive multimodal images in remote sensing, the labeled dataset in object detection tasks is lacking due to the expensive cost of manually annotating. To validate the generalization of our proposed network, we compare the SuperYOLO with different one-stage or two-stage methods using data from the single modality including a large-scale Dataset for Object Detection in Aerial images (DOTA), object DetectIon in Optical Remote sensing images (DIOR), and Northwestern Polytechnical University Very-High-Resolution 10-class (NWPU VHR-10) datasets.

\textit{1) DOTA:} The DOTA dataset was proposed in 2018 for object detection of remote sensing. It contains 2806 large images and 188 282 instances, which are divided into 15 categories. The size of each original image is $4000 \times 4000$, and the images are cropped into $1024 \times 1024$ pixels with an overlap of 200 pixels in the experiment. We select half of the original images as the training set, 1/6 as the validation set, and 1/3 as the testing set.  The size of the image is fixed to $512 \times 512$.

\textit{2) NWPU VHR-10:} The dataset of NWPU VHR-10 was proposed in 2016. It contains 800 images, of which 650 pictures contain objects, so we use 520 images as the training set and 130 images as the testing set. The dataset contains 10 categories, and the size of the image is fixed to $512 \times 512$. 

\textit{3) DIOR:} The DIOR dataset was proposed in 2020 for the task of object detection, which involves 23 463 images and 192 472 instances. The size of each image is $800 \times 800$. We choose 11 725 images as the training set and 11 738 images as the testing set. The size of the image is fixed to $512 \times 512$

\begin{table*}[htpb]
	\small
	\renewcommand{\arraystretch}{1.2}
	\centering
	\setlength{\tabcolsep}{1.6mm}{
		\caption{Performance of Different Algorithms on DOTA, NWPU and DOTA Testing Set.}
		\label{otherdataset}
		\begin{tabular}{c|ccc|ccc|ccc}
			\toprule[1.2pt]
& \multicolumn{3}{c|}{\textbf{DOTA-v1.0}}         & \multicolumn{3}{c|}{\textbf{NWPU}}                    & \multicolumn{3}{c}{\textbf{DIOR}}         \\
            \midrule
			\textbf{Method}         & \textbf{$\text{mA}{{\text{P}}_{\text{50}}}$}          & \textbf{Params(M)}  & \textbf{GFLOPs }        & \textbf{$\text{mA}{{\text{P}}_{\text{50}}}$ }               & \textbf{Params(M)}  & \textbf{GFLOPs}         & \textbf{$\text{mA}{{\text{P}}_{\text{50}}}$ }    & \textbf{Params(M)}  & \textbf{GFLOPs}         \\
			\midrule[1.2pt]
			\textbf{Faster R-CNN \cite{ren2016faster}} & 60.64 & 60.19     & 289.25   & 77.80 & 41.17     & 127.70   & 54.10 & 60.21     & 182.20   \\
			\textbf{RetainNet \cite{lin2017focal}}    & 50.39 & 55.39     & 293.36   & 89.40 & 36.29     & 123.27   & 65.70 & 55.49     & 180.62   \\
			\textbf{YOLOv3 \cite{redmon2018yolov3}}       & 60.00 & 61.63     & 198.92   & 88.30 & 61.57     & 121.27   & 57.10 & 61.95     & 122.22   \\
			\textbf{GFL \cite{li2020generalized}}          & 66.53 & 19.13     & 159.18   & 88.80 & 19.13     & 91.73    & 68.00 & 19.13     & 97.43    \\
			\textbf{FCOS \cite{tian2019fcos}}         & 67.72 & 31.57     & 202.15   & 89.65 & 31.86     & 116.63   & 67.60 & 31.88     & 123.51   \\
			\textbf{ATSS \cite{zhang2020bridging}}         & 66.84 & 18.97     & 156.01   & 90.50 & 18.96     & 89.90    & 67.70 & 18.98     & 95.50    \\
			\textbf{MobileNetV2 \cite{sandler2018mobilenetv2}}  & 56.91 & 10.30     & 124.24   & 76.90 & 10.29     & 71.49    & 58.20 & 10.32     & 76.10    \\
			\textbf{ShuffleNet \cite{zhang2018shufflenet}}   & 57.73 & 12.11     & 142.60   & 83.00 & 12.10     & 82.17    & 61.30 & 12.12     & 87.31    \\
			\textbf{O2-DNet \cite{WEI2020268}}     & 71.10          & 209.0          & -         & -                & -          & -          & 68.3     & 209.0          & -        \\
			\textbf{FMSSD \cite{2020FMSSD}}     & \textbf{\textcolor{blue}{72.43}}         &     136.04     & -         & -                & -          & -          & 69.5     & 136.03          & -        \\
			\textbf{ARSD \cite{yang2022adaptive}}         & 68.28 & 13.08     & 68.03    & 90.92 & 11.57     & 26.65    & 70.10 & 13.10     & 41.60           \\
			\textbf{SuperYOLO} & 69.99 & \textbf{\textcolor{blue}{7.70}} & \textbf{\textcolor{blue}{20.89} }& \textbf{\textcolor{blue}{93.30}}   & \textbf{\textcolor{blue}{7.68}} & \textbf{\textcolor{blue}{20.86} }&  \textbf{\textcolor{blue}{71.82}} & \textbf{\textcolor{blue}{7.70}} & \textbf{\textcolor{blue}{20.93}} \\
\bottomrule[1.2pt]
	\end{tabular}}
	\vspace{-0.1in}
\end{table*} 
The training strategy is modified to accommodate the new dataset. The entire training process involves 150 epochs for NWPU and DIOR datasets and 100 epochs for DOTA. The batch size of the DOTA and DIOR is 16 and NWPU is 8. To verify the superiority of the SuperYOLO proposed in this paper, we selected 11 generic methods for comparison: one-stage algorithms (\textbf{YOLOv3} \cite{redmon2018yolov3},  \textbf{FCOS} \cite{tian2019fcos},  \textbf{ATSS} \cite{zhang2020bridging}, \textbf{RetainNet} \cite{lin2017focal}, \textbf{GFL} \cite{li2020generalized}); two-stage method (\textbf{Faster R-CNN} \cite{ren2016faster}); lightweight models (\textbf{MobileNetV2} \cite{sandler2018mobilenetv2} and \textbf{ShuffleNet} \cite{zhang2018shufflenet}); distillation-based methods (\textbf{ARSD} \cite{yang2022adaptive}); remote sensing designed approaches (\textbf{FMSSD} \cite{2020FMSSD} and \textbf{O2DNet} \cite{WEI2020268}).

As presented in TABLE \ref{otherdataset}, our SuperYOLO achieves the optimal detection result (69.99\%, 93.30\%, 71.82\% $\text{mA}{{\text{P}}_{\text{50}}}$) and the model parameters (7.70 M, 7.68 M, and 7.70 M) and GFLOPs (20.89, 20.86, and 20.93) are much smaller than other SOTA detectors regardless of the two-stage, one-stage, lightweight or distillation-based method. The PANet structure and three detectors are responsible for enhancing small-scale, middle-scale and large-scale target detection in consideration of the big objects such as playgrounds in these three datasets. Hence the model parameters of SuperYOLO are more than those in TABLE \ref{comparedresult}. We also compare two detectors designed for remote sensing imagery such as FMSSD \cite{2020FMSSD} and O2DNet \cite{WEI2020268}. Although these models have a close performance with our lightweight model, the huger parameters and GFLOPs seem to be a massive cost in computation resources. Hence, our model has a better balance in consideration of detection efficiency and efficacy.



