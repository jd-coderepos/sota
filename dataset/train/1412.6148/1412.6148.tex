\documentclass[10pt, conference, letterpaper,onecolumn]{IEEEtranv1.8}



\usepackage{graphicx}  
\usepackage{subcaption}
\usepackage{amsmath}  
\usepackage{amsfonts}
\usepackage{balance}
\usepackage{amsthm}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{xspace}
\usepackage{booktabs}
\usepackage[noadjust]{cite}



\usepackage[colorlinks]{hyperref}
\hypersetup{
    bookmarks=true,         unicode=false,          pdftoolbar=true,        pdfmenubar=true,        pdffitwindow=false,     pdfstartview={FitH},    pdftitle={Hashing-Pursuit for Anomaly Detection}, pdfauthor={anon, et al.},     pdfsubject={Network Security},   pdfcreator={Anonymous},   pdfproducer={}, pdfkeywords={networks} {anomaly} {ddos} {detection} {hashing}, pdfnewwindow=true,      colorlinks=true,       linkcolor=red,          citecolor=blue,        filecolor=magenta,      urlcolor=cyan           }

\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\newcommand{\xref}[1]{\S~\ref{#1}}
\newcommand{\TODO}[1]{\textcolor{red}{\textbf{TODO:} #1}}

\setlength{\textheight}{9.2in}
\setlength{\oddsidemargin}{0in} \setlength{\evensidemargin}{0in} \setlength{\textwidth}{6.5in}


\newcommand\bzero{\mbox{\boldmath}}
\newcommand\beins{\mbox{\boldmath}}

\newcommand\bbe{\mbox{\boldmath}}
\newcommand\bdg{\mbox{\boldmath}}
\newcommand\beg{\mbox{\boldmath}}
\newcommand\bfeta{\mbox{\boldmath}}
\newcommand\bga{\mbox{\boldmath}}
\newcommand\bGa{\mbox{\boldmath}}
\newcommand\bGg{\mbox{\boldmath}}
\newcommand\bpsi{\mbox{\boldmath}}
\newcommand\bDg{\mbox{\boldmath}}
\newcommand\bDel{\mbox{\boldmath}}
\newcommand\bLa{\mbox{\boldmath}}
\newcommand\bPhi{\mbox{\boldmath}}
\newcommand\bPsi{\mbox{\boldmath}}
\newcommand\bzg{\mbox{\boldmath}}
\newcommand\bSig{\mbox{\boldmath}}
\newcommand\bOm{\mbox{\boldmath}}




\newcommand\hbDg{\hat{\mbox{\boldmath}}}
\newcommand\hbeg{\hat{\mbox{\boldmath}}}
\newcommand\hbeta{\hat{\mbox{\boldmath}}}
\newcommand\hbga{\hat{\mbox{\boldmath}}}
\newcommand\hbGa{\hat{\mbox{\boldmath}}}

\newcommand \Xbm{\bm X}

\newcommand\bA{{\bf A}}
\newcommand\ba{{\bf a}}
\newcommand\bB{{\bf B}}
\newcommand\bb{{\bf b}}
\newcommand\bC{{\bf C}}
\newcommand\bc{{\bf c}}
\newcommand\bI{{\bf I}}
\newcommand\bJ{{\bf J}}
\newcommand\bG{{\bf G}}
\newcommand\bh{{\bf h}}
\newcommand\bH{{\bf H}}
\newcommand\bK{{\bf K}}
\newcommand\bM{{\bf M}}
\newcommand\bm{{\bf m}}
\newcommand\bP{{\bf P}}
\newcommand\bQ{{\bf Q}}
\newcommand\bR{{\bf R}}
\newcommand\bs{{\bf s}}
\newcommand\bS{{\bf S}}
\newcommand\bT{{\bf T}}
\newcommand\bU{{\bf U}}
\newcommand\bu{{\bf u}}
\newcommand\bV{{\bf V}}
\newcommand\bv{{\bf v}}
\newcommand\bW{{\bf W}}
\newcommand\bw{{\bf w}}
\newcommand\bX{{\bf X}}
\newcommand\bx{{\bf x}}
\newcommand\bY{{\bf Y}}
\newcommand\by{{\bf y}}
\newcommand\bZ{{\bf Z}}
\newcommand\bz{{\bf z}}

\newcommand\cD{{\mathcal D}}
\newcommand\cF{{\mathcal F}}
\newcommand\cH{{\mathcal H}}
\newcommand\cL{{\mathcal L}}
\newcommand\cO{{\mathcal O}}
\newcommand\cR{{\mathcal R}}
\newcommand\cS{{\mathcal S}}
\newcommand\cT{{\mathcal T}}


\newcommand\hX{\hat X}
\newcommand\hZ{\hat Z}
\newcommand\hla{{\hat \la}}
\newcommand\hv{{\hat v}}
\newcommand\hu{{\hat u}}
\newcommand\hc{{\hat c}}

\newcommand\hbA{\widehat {\bf M}}
\newcommand\hbC{\widehat {\bf C}}
\newcommand\hbM{\widehat {\bf M}}
\newcommand\hbU{\widehat {\bf U}}
\newcommand\hbV{\widehat {\bf V}}
\newcommand\hbY{\hat {\bf Y}}
\newcommand\hbX{\hat {\bf X}}
\newcommand\hbx{\hat {\bf x}}
\newcommand\hbZ{\hat {\bf Z}}

\newcommand\tv{{\tilde v}}
\newcommand\tu{{\tilde u}}
\newcommand\tbbeta{\tilde{\bbeta}}
\newcommand\tbpsi{\widetilde{\bpsi}}
\newcommand\tbeg{\tilde{\beg}}
\newcommand\tbeta{\tilde{\bfeta}}
\newcommand\tbZ{\tilde {\bf Z}}
\newcommand\tbU{\tilde {\bf U}}
\newcommand\tbY{\widetilde{\bf Y}}

\newcommand\ip{i^\prime}
\newcommand\jp{j^\prime}
\newcommand\kp{{k^\prime}}
\newcommand\ellp{{\ell^\prime}}
\newcommand\np{n^\prime}
\newcommand\egp{\eg^{\prime}}
\newcommand\begp{\mbox{\boldmath}}
\newcommand\dint{\int\!\!\!\int}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}


\setlength{\oddsidemargin}{0mm}
\setlength{\evensidemargin}{0mm}
\setlength{\topmargin}{-15mm}
\setlength{\textwidth}{164mm}
\setlength{\textheight}{215mm}

\renewcommand\P{\mathbb P}
\newcommand\E{\mathbb E}


\theoremstyle{plain}\newtheorem{thm}{Theorem}\newtheorem{lem}{Lemma}
\newtheorem{prop} {Proposition}
\newtheorem{asum} {Assumption}
\newtheorem{cor}{Corollary}
\newtheorem{fact}{Fact}
\theoremstyle{definition}
\newtheorem{defn}{Definition}\newtheorem{conj}{Conjecture}\newtheorem{exmp}{Example}\theoremstyle{remark}
\newtheorem{rem}{Remark}
\newtheorem{note}{Note}
\newtheorem{case}{Case}

\def\HideProof#1{}

\begin{document}

\title{Hashing Pursuit for Online Identification of Heavy-Hitters in High-Speed Network Streams}


\author{
 \IEEEauthorblockN{
  Michael G. Kallitsis\,\IEEEauthorrefmark{2},
  Stilian Stoev\,\IEEEauthorrefmark{1},
 and George Michailidis\,\IEEEauthorrefmark{1} 
 }
 \IEEEauthorblockA{\IEEEauthorrefmark{1}\,Department of Statistics, University of Michigan, Ann Arbor, MI\\}
  \IEEEauthorblockA{\IEEEauthorrefmark{2}\,Merit Network, Inc., Ann Arbor, MI\\}
 \IEEEauthorblockA{\{mgkallit, sstoev, gmichail\}@umich.edu}}

\maketitle

\begin{abstract}
Distributed Denial of Service (DDoS) attacks have become more prominent recently, both in frequency of occurrence, as well as magnitude. Such attacks render key Internet resources unavailable and disrupt its normal operation. It is therefore of paramount importance to quickly identify malicious Internet activity. The DDoS threat model includes characteristics such as: (i) heavy-hitters that transmit large volumes of traffic towards ``victims", (ii) persistent-hitters that send traffic, not necessarily large, to specific destinations to be used as attack facilitators, (iii) host and port scanning for compiling lists of un-secure servers to be used as attack amplifiers, etc. This conglomeration of problems motivates the development of space/time efficient summaries of data traffic streams that can be used to identify heavy-hitters associated with the above attack vectors. This paper presents a hashing-based framework and fast algorithms that take into account the large-dimensionality of the incoming network stream and can be employed to quickly identify the culprits. The algorithms and data structures proposed provide a synopsis of the network stream that is not taxing to fast-memory, and can be efficiently implemented in hardware due to simple bit-wise operations. The methods are evaluated using real-world Internet data from a large academic network.
\end{abstract}


\section{Introduction}

Distributed Denial of Service attacks have become prominent recently both in frequency of occurrence as
well as magnitude~\cite{rossow}.  The detection and identification of such nefarious Internet activity are key problems for network engineers. The {\em time scale} at which such attacks 
are detected and {\em identified}
is of crucial importance. In practice, this time scale, referred here as the {\em relevant time scale} (RTS),
should be on the order of {\em seconds} or {\em minutes} rather than hours. 
However, processing data in a streaming fashion for network monitoring
that aims to detect and identify network anomalies
poses two fundamental computing challenges. First, one needs to work
with ``small-space" data structures;  storing snapshots of the incoming data stream  in fast memory
 is prohibitively expensive. 
Second, any data processing  on the incoming network stream 
ought to be performed efficiently; expensive and time-consuming
computations on voluminous streams may defeat the purpose of
real or near-real time network monitoring.

In addition to the need for rapid RTS detection and identification of malicious activities, another important feature is their growing
sophistication.  Attacks in complex modern networks are often distributed and coordinated~\cite{rossow}. For example, the sources involved in a 
DDoS attack may be spread through various sub-networks and individually may not stand out as heavy traffic generators. Therefore, 
detection of the attack and identification of its victim(s) is only possible if traffic is monitored  {\em simultaneously} over multiple sites on 
the network. The rapid communication (on the RTS scale) of anomaly signatures from monitoring sites to a single decision center imposes stringent 
size constraints on the data structures involved.
 






In  recent years, numerous sophisticated and accurate methods for anomaly detection have been proposed. For example,
\emph{signature-based} methods examine traffic/data for
\emph{known} malicious patterns to detect malware (worms, botnets, etc); \emph{Snort} (see, {\tt snort.org})  and \emph{Bro} 
({\tt bro.org}) are two well-known tools of this class.
On the other hand, 
a plethora of methods have appeared in the literature that look for deviations from ``normality" (e.g., see~\cite{Lakhina:2004:DNT:1030194.1015492, Barford:2002:SAN:637201.637210, Ide04eigenspace-basedanomaly, Zhang:2005:LAD:1080173.1080189}
to name a few, and the review paper~\cite{Thottan:2010:anmomaly:detection:review} for a more exhaustive list)
and do not require a priori knowledge of attack patterns.
Most of them, however, 
require heavy computation (e.g., singular value decomposition of large matrices or computation of wavelet coefficients) and/or storage and post-processing of historical 
traffic traces. This makes them essentially not applicable in practice on the RTS. 
As a consequence, much attention was paid to alleviating the high dimensionality constraint of the problem. Algorithms that involve
memory efficient data structures, called ``sketches", 
that exhibit sub-linear computational efficiency  with respect to the input space
have been well-studied by the community~\cite{Cormode:2005:IDS:1073713.1073718, Cormode:2003:FHH:1315451.1315492, Cormode:2004:HUS:1007568.1007575, Gilbert06algorithmiclinear, Krishnamurthy:2003:SCD:948205.948236, 4146856, 1354567, 2190032, Cormode1061325,  Porat:2012:STM:2095116.2095212, doi:10.1137/100816705}. These summary data structures
can accurately \emph{estimate} the 
state of the measured signal while having a low computation and space fingerprint.
However, with the  exception of the work in~\cite{4146856}, where a
hash-based algorithm and  fast 
hardware implementation is introduced, 
there is a  gap between the theoretical optimality of advanced sub-linear algorithms
and their practical implementation. 













This paper aims to bridge this gap and its key {\em contributions} include:
(i) the  development of space and time efficient \emph{algorithms  and data structures} that can be continuously 
and rapidly updated while monitoring fast traffic streams. The proposed data structures, based on permutation hashes, allow us to identify the
heavy-hitters in a traffic stream (i.e., the IPs or other \emph{keys} responsible for the, say, top- signal values in a window of interest).
Our algorithms are
memory efficient and require constant memory space that is much smaller than the 
dimension of the streaming data. Further, all computations
involve fast bit-wise  operations making them amenable for fast hardware implementation;
(ii) we propose a \emph{framework  suited for different types} of traffic signals in a unified ``hashing pursuit" perspective. For example, the signal of interest can be 
conventional traffic volume (measured in packets or bytes) or the number of different source IPs  that have accessed a given destination, etc. In 
the latter case (upon filtering out the well-known benign nodes/users/networks) the heavy hitters correspond to potential victims of DDoS attacks. Their rapid
identification (on the RTS) for the purpose of mitigation is of utmost importance to network security; (iii) we evaluate our algorithms with
\emph{real-world networking data collected at a large academic ISP} in the United States.  
 
\section{Streaming Paradigm}
\label{sec:streaming}

The theoretical framework of computation on data streams is best suited to our needs~\cite{Muthukrishnan:2005:DSA:1166409.1166410}. In this context,
a traffic link can be viewed as a stream of items  that are seen only once
as they pass through a monitoring station (e.g.\ traffic router). Due to space/time constraints, the 
entire stream cannot be recorded and hence only fast small memory sequential algorithms can be used 
for its characterization and analysis. The  are the {\em keys} and  are the updates 
(e.g., payload increments) of the stream. For example, the set of keys  could be all IPv4 addresses of
traffic sources and the payloads could include byte content, packets, port, protocol,
or other pertinent information. Alternatively, the stream keys  could be the pair of
source and destination IPv4 addresses, and one could enlist many other types of keys (e.g., IPv6 addresses). 

Over a monitoring period (e.g.\ a few seconds), the stream communicates a {\em signal} , which could
take either {\em numerical} or {\em set} values. Consider the simple scalar setting where  and
 are the number of bytes exchanged between source  and destination . Upon observing the -th update , 
the signal  is updated sequentially like a {\em cash register}\footnote{We used a programmers' syntax for  variable updates.}:

Thus, at the end of the monitoring period,   contains the number of bytes communicated for all pairs . 
The ``signal", however, is only a theoretical quantity, since in practice, it cannot be fully stored and accessed on the RTS. Nevertheless, compressed representations of , 
known as {\em sketches} have been proposed to study its characteristics~\cite{Cormode:2005:IDS:1073713.1073718, Cormode:2003:FHH:1315451.1315492, Cormode:2004:HUS:1007568.1007575, Gilbert06algorithmiclinear, Krishnamurthy:2003:SCD:948205.948236, 4146856, 1354567, 2190032, Cormode1061325,  Porat:2012:STM:2095116.2095212, doi:10.1137/100816705}. Broadly speaking sketches 
provide statistical summaries of the signal, which can be used to approximate (with high probability) features of interest.  (See also 
the related {\em compressed sensing} paradigm~\cite{4385788, Gilbert:2007:OSF:1250790.1250824, 1614066, Indyk:2008:ECC:1347082.1347086}.)

The cash register model described above is well-suited for applications such as  
monitoring IP traffic entering a network link, monitoring IPs accessing a particular service (such as Web server, cloud storage, etc), monitoring 
particular IP pairs of a particular application (i.e., source-destination pairs of all DNS traffic), and many others. 

A variation of the cash register model is obtained when the payloads 's are sequential updates
on a set. In this model, upon observing , we update the state of the signal as follows:
. For example, if  is the source IP and
 is port number ()
(we define the compact notation  to be used hereafter), then the signal , where  is the power-set of ,  is set-valued.
For a given IP , the set  consists of all different ports accessed by  during the monitoring  period and
large values of  identify potential {\em port scanning} sources .
Similarly, by considering destination IPs in place of ports we can identify hosts that perform
malicious activity such as horizontal \emph{host scanning}.

In the above two contexts, the goal is to identify heavy--hitters, i.e.\ source--destination pairs generating heavy traffic, or a source 
with a large set of accessed ports or destinations, for example.  For a signal , a heavy--hitter is formally
a key 

maximizing a {\em loss} function , where  could be a simple byte-count, size of a set, or 
in a more complex, time--series setting, the frequency maximizing the power spectrum, for example.

Our goal is to develop a unified framework for the identification of heavy hitters, which can be applied to a variety of signals and
that works on the {\em relevant time scale} for anomaly detection (i.e., the algorithms monitor the stream in real time 
and produce heavy--hitters every few seconds, minutes or every 100,000 tuples seen, etc). 
Depending on the type of signal, the identification part is bundled with a different specialized {\em sketch}, which 
allows us to handle ultra high dimensional signals. The following section describes the general methodology in terms of
meta-algorithms and then proceed to several concrete applications. 

\section{Hashing Pursuit for Identification}
\label{sec:hash-ident}

At the heart of our identification schemes lies \emph{hashing}.  Reversible hash functions are used to compress the 
\emph{domain} of our incoming signal into a smaller dimension, while at the same time uniformly distributing the 
keys onto the reduced-dimension space. Another application of hashing used below is to efficiently `thin' the 
original stream into sub-streams to help increase identification accuracy.
We apply our hash functions on a set of keys , important 
special case being the set of all IPv4 numbers, where .


\smallskip
\noindent{\bf Hash functions for IP monitoring.}
Consider a set of 
hash functions . This set is {\em complete} (or perfect),
if all  are uniquely identified by their hashes , , i.e.,
the vector--hash  is invertible.  We shall describe below a particular family of
rapidly computable complete hashes, which can also be quickly inverted. In particular, we will have that
.



We describe next the main idea behind identification over the simple case of a scalar signal ,
which tracks the number of packets  transmitted by IP  over a monitoring period. The goal is to be able to identify
the most {\em persistent user} with highest packet count.

Instead of maintaining in memory the entire signal  using an array of size , we shall maintain  hash--histograms 
, of size  each. If  is the heavy hitter, then it will contribute large packet counts
 to all of the  histograms at the bins .
If collisions are uniformly spread--out, with high probability, we will have that

Thus, by locating the bins  maximizing each of the  hash--histograms, we define

We provide lower bounds on  and show that
in practice the heavy hitter is identified w.h.p. (with high probability).
In fact, the method naturally extends to the case of multiple heavy hitters and other 
types of signals. {\em Meta-algorithms}~\ref{alg:meta-encode} and~\ref{alg:meta-decode} 
describe the general encoding  and decoding steps. 

\renewcommand{\thealgorithm}{\roman{algorithm}}

\floatname{algorithm}{Meta-Algorithm}
\begin{algorithm}[t]
\caption{Hash-based Encoding}
\label{alg:meta-encode}
\begin{algorithmic}[1]
\REQUIRE Set of complete uniform hashes 
 
 with . Initialize signal hash histograms: 
 

\STATE [Start] Begin stream monitoring.
\STATE [Hash keys] Upon observing , compute .
\STATE [Update] Compute  = ,  
\STATE [Stop] End when the monitoring period is over.
\RETURN Output hash arrays  for analysis.
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[t]
\caption{Decode and Identify}
\label{alg:meta-decode}
\begin{algorithmic}[1]
\REQUIRE Set of hash histogram arrays , as outputs of
Meta-algorithm \ref{alg:meta-encode}. An efficient  function as in \eqref{e:inverse}.

\STATE [Identify] For each hash histogram array  identify a bin index  as a
candidate bin where the heavy hitter falls.
 
\RETURN .
\end{algorithmic}
\end{algorithm}


\renewcommand{\thealgorithm}{\arabic{algorithm}}

The specific implementation of the hash functions and their inverses will be discussed in the following section, where
the abstract {\rm `update'} and {\rm `identify'} steps in the above meta--algorithms will depend on the application. 


\begin{rem} Let . The space required to maintain the raw signal , is
, while the space required for the hash histograms  is . These
exponential savings in memory allow us in practice to rapidly communicate hash--based traffic summaries at the RTS, 
enabling simultaneous and distributed monitoring and identification (see Table~\ref{tab:mem}).
\end{rem}

\begin{rem}\label{rem:update} The update step in Algorithm \ref{alg:meta-encode} depends on the type of signal. In the scalar case, we have

while in the set-valued case .
In applications, maintaining a {\em set}, however, is often not practical either. When we are only interested in the size of the set, 
we can maintain a further max--stable sketch data structure, requiring an array of size  for each  (see Section~\ref{sec:ms}).
\end{rem}


\smallskip
\noindent{\bf Construction of complete reversible hashes.}
We focus on the case where the keys represent IPv4 numbers, i.e.\ . A simple set of complete
hash functions can be constructed as follows. Let  and  and consider the base- expansion of :

The 's are simply the 4 \emph{octets} in the dotted-quad representation of the IP address . 

Clearly, the hashes 

are rapidly computable with bit--wise operations, e.g.\ .  They are also complete since
the knowledge of all  base- digits determines . In practice, however, these naive hash functions are not useful. The reason
is that the IPs are typically clustered (spatially correlated) since the address space used by a sub--network or \emph{autonomous system}
consists of contiguous ranges of IPs. Hence, one needs to permute or `mangle' the IPs
in the original space  so that all permuted keys achieve approximately uniform empirical distributions.
This can be readily resolved by composing the hashes with a permutation that can be also efficiently 
computed {\em and} inverted.  The idea of IP mangling was employed also in~\cite{4146856}. 
Next, we describe a different kind of permutation functions, starting first with a simple general result.

\begin{fact}  Equip the set of keys  with the uniform probability measure. Then:

 {\rm (i)} The hashes  in \eqref{e:hi}, viewed as random variables on ,
  are independent and uniformly distributed.
 
 {\rm (ii)} For any permutation , the hashes  are also independent
 and uniformly distributed.
 
Conversely, if  is a set of uniformly distributed independent hashes, then they are complete and
, for some permutation .
\end{fact}

\noindent The proof is elementary and omitted in the interest of space. This fact suggests that so long as a permutation  spreads-out 
a set of IPs essentially uniformly, the empirical distribution of the hashes  will be approximately uniform (see Fig.~\ref{fig:ident_typical}).
Therefore, to obtain a high--quality set of complete hashes, it suffices to find an efficiently computable and invertible permutation . 
We do so next.

\begin{fact} \label{f:RSA} Let  and suppose that , for some integers 
. Define , as follows

Then,   and  are bijections and .
\end{fact}

\noindent 
The proof is straightforward. 

\begin{rem}Similar but more computationally intensive IP mangling methods (with efficient hardware 
implementation on an FPGA) were given in \cite{4146856}. Shai Halevi \cite{halevi:2007} proposed 
alternative permutations that can be efficiently computed and inverted and exhibit good cryptographic
properties. Interesting connections with the general theory of {\em homomorphic coding} can be further pursued. 
\end{rem}



In practice, with , we consider 

We found that  with  works well. Observe also that if , then , 
for all integer . Thus, via powers of  and  in \eqref{e:2-32-factorization}, one can easily generate different permutations. 
Our experiments with naturally occurring sets of IPs show powers  or  work slightly better than .




\section{Identification Algorithms}
\label{sec:identification}

Having all  building blocks in place, we now introduce our algorithms. Table~\ref{tab:roadmap} provides a roadmap.


\begin{table}[t]
\caption{Algorithms Roadmap}
\scriptsize
\centering
\label{tab:roadmap}
\begin{tabular}{|l|l|c|}
\hline
{\bf Algorithm}                & {\bf Sketch-size}             & {\bf Application}  \\   
\hline\hline                                                              
Simple~(\ref{sec:simple})                   & )          & Scalar signals, top-1 hitter, suited  \\
                    &           &                            for stringent memory constraints  \\
\hline                    
Max-Count~(\ref{sec:max_count})                & ) & Scalar signals, top-k hitters                                                \\
\hline
Boyer-Moore~(\ref{sec:bm}) & )          & Scalar signals, top-k hitters                                                \\
\hline
Max-Sketch~(\ref{sec:ms})               & ) & Set-valued signals, top-hitter         \\
\hline                                     
\end{tabular}
\end{table}


\subsection{Simple Hashing Pursuit}
\label{sec:simple}








\floatname{algorithm}{Algorithm}
\setcounter{algorithm}{0}
\begin{algorithm}[t]
\caption{Simple Hashing Pursuit (SHP)}
\label{alg:shp}
\begin{algorithmic}[1]
\REQUIRE Number heavy-hitters: 
\STATE [Start]   Initialize array  of size .
\STATE [Permute]  Upon observing , compute 
\STATE [Hash]  Compute .  \# NOTE:  
\STATE [Update]    Update   for :
   
 \vspace{-10pt}  
\IF {(Populated  for some  time window)}
\IF{k==1}
 \STATE Find , \  
 \STATE [decode] Output . 
 \ELSE
  \STATE Initialize , \  
   \FOR{r=1 \TO k} 
        \STATE Find , \  
        \STATE   {\#Exclude for next iteration }
         \STATE [decode] Output . 
   \ENDFOR
 \ENDIF    
 \ENDIF    
\end{algorithmic}
\end{algorithm} 








Algorithm~\ref{alg:shp} is the simplest instance of IP randomization and hashing used to identify the
heavy hitter for a scalar signal.  Fig.~\ref{fig:ident_typical} shows the resulting {\em signal arrays} 
, referred also as \emph{octet arrays}. If  is the first octet of 
the (encoded) IP address , then its payload  is added to the bucket .
If the permutation  spreads out the range of IPs in the observed stream approximately uniformly,
 it is reasonable to expect that the resulting vector  
will be populated uniformly.  A heavy hitter, however, will contribute abnormally large signal value to the bucket
, where   is the first octet of its encoded key. The heavy hitter similarly stands out in its encoded octet indexes  and . By applying the inverse of the permutation  to the largest bins 
in the octet arrays, we recover the address of the anomalous user (see also Meta-Alg.\ \ref{alg:meta-decode}). 

The fact that the permutation  can be efficiently inverted is of utmost importance in practice 
since this algorithm is intended to be run online on fast network traffic streams. This is, in fact, feasible on the RTS
as demonstrated in~\cite{4146856} with an efficient FPGA implementation of a similar and slightly more 
computationally demanding IP randomization.

 
 


\begin{figure}
        \centering
        \includegraphics[width=0.34\textwidth]{./Fig_ident_alg_example_extreme_v2.pdf}
        \caption{Algorithm~\ref{alg:shp}'s data structure consists of  hash arrays of size  (here separated by the thick-dashed lines), one for each octet. Circles illustrate the selected maxima of each octet.}
        \label{fig:ident_typical}
\end{figure}

\subsection{Hashing Pursuit on Sub-streams}
\label{sec:max_count}

Algorithm~\ref{alg:max-count}, called~\emph{Max-Count Hashing Pursuit}, is an extension of SHP that minimizes collisions significantly, 
and hence improves identification accuracy. The idea is to introduce an
additional dimension  in our data structure and populate it by using another hash function . The secondary
hash is used to divide the incoming stream into  independent sub-streams (i.e., `thin' the stream). 

As stream entries arrive, we initially perform the \emph{permutation}
step discussed above to remove spatial localities.
Then, having a uniform hash function
 as discussed in Section~\ref{sec:hash-ident},
we apply it on the permuted key  to get the index  of the sub-stream.
We update the appropriate array entries corresponding to sub-stream  only.




\floatname{algorithm}{Algorithm} 
\setcounter{algorithm}{1}
\begin{algorithm}[t]
\caption{Max-Count Hashing Pursuit}
\label{alg:max-count}
\begin{algorithmic}[1]
\REQUIRE Number heavy-hitters: 
\STATE [Start]  Initialize   of size .
\STATE Perform steps 2 \& 3 of Algorithm~\ref{alg:shp}
\STATE [Thin] Calculate sub-stream index .
\STATE [Update]    Update   for :
   
 \vspace{-10pt}  
\IF {(Populated  for some  time window)}
 \IF{k==1}
	\STATE  Construct matrix  of size , i.e.,
	 \STATE , \ ,  
	 \STATE Find , \  
	 \STATE [decode]      Output . 
 \ELSE
  \FOR{r=1 \TO k} 
	 \STATE , \ ,  
	 \STATE Find , \  
	 \STATE Find , \   
	 \STATE  = 0, \   {\#Exclude for next iteration }
	 \STATE [decode]      Output . 
  \ENDFOR
 \ENDIF 
 \ENDIF   
\end{algorithmic}
\end{algorithm} 
 
 
 
 


 \subsection{Hash-thinned MJRTY Boyer-Moore}
 \label{sec:bm}
 
 Algorithm~\ref{alg:bm} is based on the \emph{Boyer-Moore} majority vote algorithm~\cite{boyer_moore1991} (not
 to be confused with the Boyer-Moore algorithm for string matching  used in
 signature-based detection tools like~\emph{Snort}),
 and the idea of stream thinning for creating sub-streams described above. The
 MJRTY Boyer-Moore algorithm can identify \emph{exactly} the element
 in stream  with the largest traffic assuming that its volume is at least 50\% of the total volume (i.e., there is a majority).
 It solves the problem in time linear in the length of the input sequence and constant memory.
 In reality, though, a single IP or flow is not usually responsible for such a high fraction of the total volume of the stream. Hence,
 identification accuracy of the plain Boyer-Moore algorithm could be lacking. To overcome this, we employ the Boyer-Moore
 idea on sub-streams of , constructed as described above. Thus, with, say,  sub-streams 
 the `majority-threshold' for each sub-stream becomes \% (\%) of  stream  volume. In practice, as Fig.~\ref{fig:ident_accuracy_vol} illustrates, this makes  the Boyer-Moore-based algorithm to perform remarkably well, but one could use a higher  to further thin stream , if needed.  
 
 We describe the original  Boyer-Moore algorithm with an analogy to the \emph{one-dimensional}
 random walk on the line of non-negative integers.  A variable  is initialized to  (i.e., the origin)
 and a  candidate variable \emph{cand} is reserved for use. Once a new IP arrives\footnote{We focus on single IPs. Everything we say here, though, apply on (src,dst) pairs,
and tuples of kind (src, sport, dst, sport) where sport/dport are the source and destination ports.},  
 we check to see if \emph{count} is 0. If it is, that IP is set to be the new candidate \emph{cand}
and  we move \emph{count} one step-up, i.e. . Otherwise, if the IP is the same as \emph{cand}, then 
\emph{cand} remains unchanged and \emph{count} is incremented, and, if not, \emph{count} moves one step-down 
(decremented). We then proceed to the next IP and repeat the procedure. Provably, when all IPs are read,
\emph{cand} will hold  the one with majority, if majority exists.  

 
We implemented a natural extension of the MJRTY Boyer-Moore algorithm to the case of byte and packet 
counts and applied it in parallel on the sub-streams resulting from hash-thinning. For each sub-stream, we also 
maintained an additional small and constant size data structure, used to estimate the signal volume.
 










\floatname{algorithm}{Algorithm} 
\setcounter{algorithm}{2} 
\begin{algorithm}[t]
\caption{Hash-thinned MJRTY Boyer-Moore}
\label{alg:bm}
\begin{algorithmic}[1]
\REQUIRE Number heavy-hitters: , 
\STATE [Start]  Initialize  , \ , . 
\STATE Initialize  ,  , \ 
\STATE Upon observing , compute .
\STATE [Thin] Calculate sub-stream index .
\STATE [Hash] Compute , 
\STATE [Update]
\IF{}
   \STATE [s] = , [s] = , [s,j] = 
\ELSE
    \IF{}
         \STATE [s,j] =  [s,j]  + , [s] =  [s] + 
     \ELSE
          \IF{}
               \STATE  =  - 
               \IF{}
                    \STATE [s] = , [s] = -[s]   {\# reset candidate}
               \ENDIF
               \STATE [s,j] =  [s,j]  + 
          \ELSE 
             \STATE  [s] = ,  [s] =  {\# reset candidate}
             \STATE  [s,j] = [s,j]  +  
          \ENDIF   
    \ENDIF
\ENDIF
\IF {(Populated  for some  time window)}
     \STATE , \ 
       \STATE Initialize 
   \FOR{r=1 \TO k} 
        \STATE Find  
        \STATE   {\#Exclude for next iteration }
         \STATE Output cand 
  \ENDFOR
\ENDIF
\end{algorithmic}
\end{algorithm} 
 
 
 
\subsection{Applications to more complex signals}
\label{sec:ms}

 In this section we extend the randomized  {\em domain hashing} approach to the detection of more 
 complex anomalies. As discussed in Section~\ref{sec:streaming}, one common scenario could involve 
a ``persistent" user  that accesses abnormally large
 number of ports.  Note that the overall volume or frequency of packets from user  need not be abnormal. Therefore, such anomalous activity may be
 nearly impossible to detect from  algorithms that merely detect large volume hitters. 
 To address this problem with constant and small-size memory, 
 we propose next another algorithm, which combines the {\em randomized hashing} and
 {\em max-stable sketches}~\cite{4221749}.
 


 These sketches exploit the {\em max-stability} property \eqref{e:max-stab} of the Fr\'echet distribution.
 Recall that a random variable  is -Fr\'{e}chet if , for , and
 0, otherwise, for some scale . If , then  is called  {\em standard -Fr\'echet}.
 Let  be independent standard 1-Fr\'{e}chet random variables. Then, it is known~\cite{4221749} that 
 
 where `' denotes equality in distribution.
Thus,  is a 1-Fr\'{e}chet variable with scale coefficient . One can easily express
the median of a 1-Fr\'{e}chet variable  with scale coefficient  by considering that
 = 1/2. By the definition of the 1-Fr\'{e}chet distribution and some algebra,
one obtains .

The theory above can be employed to estimate the number of elements of a set.
Specifically, consider the problem of finding the number of {\em unique} destinations ports
contacted by a given host. When a stream element  arrives, we generate a 1-Fr\'{e}chet
variable with seed that is a function of . Thus,  if the same port arrives, the exact 
pseudo-random number  is generated. As different ports arrive, we sequentially update 
our sketch by taking the maximum of the new 1-Fr\'{e}chet variable
and the sketch entry. Essentially, we are building one realization of the  random variable 
described above.  The number of different terms 's will correspond to the number of different ports.

Since, we want to take the median of  to estimate the scale coefficient ,
we independently keep  realizations of the above procedure (see Alg.\ \ref{alg:mshp}).
 




\begin{algorithm}[t]
\caption{Max-Stable Hashing Pursuit (MSHP)}
\label{alg:mshp}
\begin{algorithmic}[1]
\REQUIRE Size of max-sketch 
\STATE [Start]    Initialize array  of size ,
\STATE [Permute]  Upon observing , compute 
\STATE [Hash]  Compute .
\STATE [RNG]  With \emph{random seed} , and , generate  independent 1--Fr\'echet random variables .
\STATE [Update]    Update  for , :
   
 \vspace{-10pt}  
\IF {(Populated  for some  time window)}
   \STATE Construct matrix  of size , i.e.,
   \STATE , \  and  
   \STATE Find , \  
 \STATE [Decode]  Output . 
 \ENDIF    
\end{algorithmic}
\end{algorithm} 
  
  
  
  






\section{On the accuracy of identification}
\label{sec:bounds}

 The proposed algorithms (except the hash-tinned MJRTY Boyer-Moore) are closely related to the general family 
 of sub-linear algorithms in the theoretical work of \cite{Porat:2012:STM:2095116.2095212}. Performance guarantees 
 for our algorithms can thus be established in a similar way as for the so-called Euclidean 
 {\em approximate sparse recovery systems} of  \cite{doi:10.1137/100816705}.  

To understand better the role of {\em collisions}, however, we adopt a different approach and provide 
performance guarantees under the following simplifying assumptions.  All  proofs are available in the 
Appendix.

\smallskip
\noindent{\bf Assumption 1.} {\em (exact -sparcity)} The signal  has precisely  non-zero (non-empty) entries
at keys .

\smallskip
\noindent{\bf Assumption 2.} {\em (separation)} The magnitude functionals  
of the heavy-hitters are all different:

Define  to be the cumulative magnitude of the bottom  heavy hitters. By convention
set 



\smallskip
 \noindent{\bf Exact recovery guarantees.} Clearly, under Assumption 2, all our algorithms will identify 
 the top- heavy hitters {\em exactly}, provided that there are no hash-collisions. Let  denote the 
 probability that the top- heavy hitters are correctly identified for a -sparse signal . The following 
 results provide lower bounds on  under various conditions.

\begin{thm} \label{p:simple} Let  and suppose that . Then, for the Simple Hashing Pursuit algorithm:

\end{thm}
\noindent{\em Note:} With  and the choice of hash functions as in Section~\ref{sec:hash-ident}, we have   and . 
\HideProof{\begin{proof}
For each hash function , let  be the event that the top- heavy hitters are hashed into  different bins,
{\em and} at the same time, the remaining  hitters are hashed into the remaining  bins. That is, the bins of
the top- hitters involve no collisions.  By the independence of , we have

If the event  occurs, then the top- heavy hitters will be correctly identified. Thus, the
independence of the events  in  implies that 

which by \eqref{e:b-day} yields the first bound. The second bound follows from 
the {\em product comparison} inequality

valid for all . Indeed, setting  and , we get

which gives the second bound.
\end{proof}}

\begin{rem}[the birthday problem] For  and , the first bound in \eqref{e:b-day} can be interpreted as
the probability that a class of  students have all different birthdays, if the year has
 days. It is well-known that this probability decays rather quickly as  grows.
\end{rem}
The max-count algorithm addresses this curse of the {\em birthday problem} by effectively increasing the value 
.

\begin{cor}
\label{cor:1}
 Under the Assumptions of Proposition \ref{p:simple}, for the Max-count Hashing Algorithm,
the bounds in \eqref{e:b-day} apply with  replaced by .
\end{cor}
\HideProof{\begin{proof}
The result follows by observing that hash-thinning with an independent uniform 
hash function  taking  values leads to  bins in \eqref{e:simple-1}.  
\end{proof}}

Since the Boyer--Moore algorithm uses one hash function (as opposed to ), 
we obtain the following.

\begin{cor}  Under the Assumptions of Proposition \ref{p:simple}, for the hash-thinned MJRTY Boyer-Moore
Algorithm, the bounds in \eqref{e:b-day} apply with .
\end{cor}

\smallskip
\noindent
{\bf Bounds on the rate of identification.} The {\em exact} identification of the top- hitters is difficult 
as the above performance bounds indicate since a few collisions may lead to misspecification. 
Even in the presence of collisions, however, a relatively large proportion of the hitters is identified 
in practice (see e.g.\ Fig.~\ref{fig:ident_accuracy_vol} below).  This suggests examining the 
{\em rate of identification} quantity:

where  denotes the number of correctly identified heavy hitters among the top . That is, 
gives the average proportion of identified top hitters.

The combinatorial analysis needed to establish bounds on  for the
Simple Hashing and Max-count algorithms is rather involved and delegated to a follow-up paper. Under certain
conditions, however,  an appealing closed-form expression for  is available for the hash-thinned MJRTY
Boyer-More algorithm. The conditions may be relaxed with the help of technical probabilistic analysis, which merits 
a separate investigation.

\begin{thm} Suppose that  for all . Then, under Assumptions 1 \& 2, for the hash-thinned MJRTY
Boyer-Moore Algorithm, we have the exact expression

\end{thm}
\HideProof{
\begin{proof} The assumption  guarantees that 
 equals the number of distinct values in the set of hashes .
Let  if bin  is occupied and  otherwise, for  Observe that

by exchangeability. Note, however, that 

since  are independent and Uniform.
This, in view of \eqref{e:e_r} and \eqref{e:bm-expected-1}, implies 
the first relation in \eqref{e:bm-expected}. The second follows from the standard
approximation 
\end{proof}}


\begin{figure}
         \vspace{-120pt}
        \centering
        \includegraphics[width=0.75\textwidth]{./bounds.pdf}
         \vspace{-130pt}
        \caption{Performance bounds given the Assumptions of Section~\ref{sec:bounds}.}
        \label{fig:bounds}
 \end{figure}




\section{Performance Evaluation}

Next, we evaluate our algorithms with \emph{Netflow} traffic
traces provided from the academic ISP Merit Network. The traces are collected
at a large edge router. In the hourly-long trace we investigate,
our Netflow stream has  unique source addresses, 
unique destinations, and consists of  million flow records.
The total volume is  gigabytes and  million packets.
We demonstrate results using Python software implementations
of our algorithms. 


\smallskip
\noindent{\bf Accuracy performance analysis.} Fig.~\ref{fig:ident_accuracy_vol} shows the identification accuracy of 
Algorithms~\ref{alg:shp}--\ref{alg:bm}, compared to the ground truth (obtained by counting using a dynamic hash 
array, and then sorting). We search for heavy hitters both in byte volume, as well as frequency of occurrence. We 
use two metrics to evaluate our methods: {\em (i)} identification rate and {\it (ii)} exact recovery. 
The first gives the expected proportion of identified heavy hitters among the top-, while the second one is much 
more strict and measures the probability that all top- hitters are correctly recovered. 

\begin{figure*}        
        \begin{subfigure}[b]{0.24\textwidth}
                \includegraphics[width=\textwidth]{./setinclusion_frequency_accuracy_100K_v2.pdf}
                \caption{\footnotesize Frequency, ident. rate}
                \label{fig:freq_set100}
        \end{subfigure}
        \begin{subfigure}[b]{0.24\textwidth}
                \includegraphics[width=\textwidth]{./setinclusion_payload_accuracy_100K_v2.pdf}
                \caption{\footnotesize Payload, ident. rate}
                \label{fig:payload_set100}
        \end{subfigure}
        \begin{subfigure}[b]{0.24\textwidth}
                \includegraphics[width=\textwidth]{./binarymetric_frequency_accuracy_100K_v2.pdf}
                \caption{\footnotesize Frequency, exact metric}
                \label{fig:freq_binary100}
        \end{subfigure}
        \begin{subfigure}[b]{0.24\textwidth}
                \includegraphics[width=\textwidth]{./binarymetric_payload_accuracy_100K_v2.pdf}
                \caption{\footnotesize Payload, exact metric}
                \label{fig:payload_binary100}
        \end{subfigure}
        \vspace{-5pt}
        \caption{\footnotesize Identification accuracy results (window=100K).}
                \label{fig:ident_accuracy_vol}
\end{figure*}

As Fig.~\ref{fig:ident_accuracy_vol} depicts, hash-thinned Boyer-Moore  and Max-Count perform remarkably well. 
The window of records we used is K (i.e., we report the culprits every K netflow records), which corresponds (for the stream studied)
on the RTS to slots of  minutes. The performance of the SHP
deteriorates as the number  of top heavy hitters sought increases
which is an expected outcome of the \emph{birthday problem} discussed above.
Therefore, SHP is well-suited when stringent memory constraints are imposed,
and when only the top-hitter is of interest.
Fig.~\ref{fig:window_size_vary} provides a sensitivity analysis w.r.t the window size
that  one could perform to find the optimal monitoring window size.







\begin{figure*}
        \vspace{-12pt}
        \centering
        \begin{subfigure}[b]{0.3\textwidth}
                \includegraphics[width=\textwidth]{./binarymetric_frequency_accuracy_vary_window_K_1.pdf}
                \caption{\footnotesize Frequency, top heavy-hitter}
                \label{fig:win:1}
        \end{subfigure}
        \begin{subfigure}[b]{0.3\textwidth}
                \includegraphics[width=\textwidth]{./binarymetric_frequency_accuracy_vary_window_K_2.pdf}
                \caption{\footnotesize Frequency top-2 hitters}
                \label{fig:win:2}
        \end{subfigure}       
        \begin{subfigure}[b]{0.3\textwidth}
                \includegraphics[width=\textwidth]{./binarymetric_frequency_accuracy_vary_window_K_3.pdf}
                \caption{\footnotesize Frequency top-3 hitters}
                \label{fig:win:3}
        \end{subfigure}
        \caption{\footnotesize Identification accuracy (using the \emph{exact} criterion)  with varying window.}
        \label{fig:window_size_vary}
\end{figure*}

Table~\ref{tab:est:accuracy:value} shows the estimation accuracy of our methods. 
We juxtapose the actual traffic volume of the heavy-hitter versus an estimate obtain from our sketches.
For Algorithms~\ref{alg:shp} \&~\ref{alg:max-count},   we estimate 
the volume by taking the average of the 4 octets
of the ary hash arrays. For thinned Boyer-Moore,
we report the value of array . Indeed,  Algorithm~\ref{alg:shp}'s 
collisions are inevitable, but for the other two, collisions are spread nicely, and
the estimated signal closely approximates  the true value (here we round to closest digit). More sophisticated estimates  (e.g., see~\cite{Krishnamurthy:2003:SCD:948205.948236} that uses a median-based estimate
to boost confidence) can also be considered.




\begin{table}[t]
        \caption{\footnotesize Estimation accuracy (\%) of proposed algorithms
        for \emph{top} hitter  (in bytes) compared to the exact value (window size = 500K).}
        \label{tab:est:accuracy:value}
  \centering      
 \scriptsize       
\begin{tabular}{|l|cccccccl|}
\hline
{\bf Time} & {\bf 1}     & {\bf 2}     & {\bf 3}     & {\bf 4}     & {\bf 5}     & {\bf 6}     & {\bf 7}     & {\bf 8}       \\
\hline
\hline
Simple                        & 88  & 88  & 88  & 87  & 93  & 92  & 91  & 62 \\
Max-Count                     & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100  \\
Boyer-Moore                   & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100  \\
\hline
\end{tabular}
\end{table}



Table~\ref{tab:mem} illustrates the small memory requirements of our methods. The Max-Count and
Boyer-Moore-based algorithms consume considerably less than 1MB of fast memory, which remains constant. On the other hand, the na\"ive approach
of using a dynamically varying hash-table, has increasing memory requirement. 
To showcase the memory increase the algorithm was run against a stream with an hourly total byte volume of  gigabytes.
As demonstrated
in Table~\ref{tab:mem}, as the monitoring window grows (or equivalently, as the amount of traffic increases),
memory increases exponentially.



\begin{table}[t]
\caption{\footnotesize Memory Utilization (in MB)}
\label{tab:mem}
\centering
\begin{tabular}{|l| c c c c |}
\hline
{\bf Window Size ( 50K)}  & {\bf 1}    & {\bf 2--7}  & {\bf 8--19} & {\bf 20}     \\
\hline
\hline
Exact                                                 & .8 & 3  & 6  & 13  \\
Boyer-Moore (==256)                                                & .5 & .5 & .5 & .5   \\
Max-Count     (=4, =256, =50)                                       & .4 & .4 & .4 & .4  \\
\hline                                                      
\end{tabular}
\end{table}








\smallskip
\noindent{\bf Max-Stable sketch case studies.}
We illustrate Algorithm~\ref{alg:mshp}'s accuracy by looking at set-valued signals, tailored for detecting: 
(i) host scanning, (ii) port scanning, and (iii) a signal based on \emph{time to live} (TTL) values. 


Fig.~\ref{fig:max_sketch_eval} shows the accuracy on {\em host scanning}, 
where the goal is to identify the source IPs that contact large number of destination IPs.  
A filled-circle on the dotted blue line indicates the occurrences we identify 
exactly the heavy-hitter IP. The solid gray line shows the exact number of different hosts that the malicious scanner has contacted. The dotted-blue
line shows our algorithm's estimate for the set cardinality obtained by simply averaging, as described above.
The identification accuracy
is very high, and the misses occur when the set-size is indeed quite small. 

In Fig.~\ref{fig:max_sketch_eval_ports}, our signal updates involve the set of ports that a given source IP interacts with.
Hence, this could be used to identify port scanners. While the accuracy is still quite high,
we observe a few mis-identifications. This is because the number of unique ports () is much smaller than the unique IPs ().
As a result, collisions in our hash-arrays matter more in this case; background ``noise" from
non-malicious IPs could alter the ranking of even one of the buckets selected on the \emph{identification} step, and result in mis-identification.

\begin{figure*}[t]
        \centering
        \begin{subfigure}[b]{0.3\textwidth}
              \includegraphics[width=1.0\textwidth]{./max_stable_estimates_host_scanning_v2.pdf}
                \caption{\footnotesize Host scanning}
                \label{fig:max_sketch_eval}
        \end{subfigure}
        \begin{subfigure}[b]{0.3\textwidth}
                \includegraphics[width=1.0\textwidth]{./max_stable_estimates_port_scanning_v2.pdf}
                \caption{\footnotesize Port scanning}
                \label{fig:max_sketch_eval_ports}
        \end{subfigure}                
        \begin{subfigure}[b]{0.3\textwidth}
                \includegraphics[width=1.0\textwidth]{./max_stable_estimates_srcspoofing_v2.pdf}
                \caption{\footnotesize IP spoofing}
                \label{fig:max_sketch_eval_ports}
        \end{subfigure}
         \caption{\footnotesize Evaluation of Algorithm~\ref{alg:mshp} on the Netflow stream (window=100K).}
          \label{fig:max_stable_case_studies}
\end{figure*}


In Fig.~\ref{fig:max_sketch_eval_ports}, we apply the max-stable sketch on Darknet data (known also as Internet background radiation)~\cite{wustrow:2010:radiation}. 
The dataset available does not include Netflow records, but instead consists of  packets captured at a network interface
(this 1-hour long trace  has  million packets). Darknets are composed of traffic destined at unallocated address spaces 
(i.e., dark spaces). It is therefore directly
associated with malicious acts or misconfigurations.  Algorithm~\ref{alg:mshp} is used to identify packets whose source IP has been \emph{spoofed}.
To achieve this we look at the TTL values of each packet. For the problem at hand,  a heavy-hitter is a source IP whose set of unique TTL
values in the monitoring window is large -- an indication of spoofing~\cite{Beverly:2005:SPI:1251282.1251290}. 
The evaluation results illustrate that the culprit is correctly identified in around 84\% of the cases, and the result for deteriorating performance is again due to the small cardinality of the unique set of TTL values (which is  in theory, but
in practice less unique TTL values are encountered).


\section{Conclusions}

 We presented a family of algorithms that are well-suited
 for online implementation in fast network streams. In addition,
 our framework can be employed to find heavy-hitters on
 a variety of signals, including complex ones that involve operations with sets.
 Further, our algorithms are amenable for distributed implementation;
in a decentralized setting, the proposed
 data structures could be constructed at various
observation points (i.e., the switch/router)
and then transmitted for aggregation at centralized decision centers
due to two important properties: (i)  they are small and constant in size and, hence,
can be efficiently emitted over the network to a centralized location, (ii)
they can be linearly combined/aggregated  by the centralized worker and reduced to
a single sketch object that can be utilized to yield the final culprits. 



\appendix



\section{Proofs}

\subsection{Exact recovery guarantees}


The proof of Theorem~\ref{p:simple} follows.

\begin{proof}
For each hash function , let  be the event that the top- heavy hitters are hashed into  different bins,
{\em and} at the same time, the remaining  hitters are hashed into the remaining  bins. That is, the bins of
the top- hitters involve no collisions.  By the independence of , we have

If the event  occurs, then the top- heavy hitters will be correctly identified. Thus, the
independence of the events  in  implies that 

which by \eqref{e:b-day} yields the first bound. The second bound follows from 
the {\em product comparison} inequality

valid for all . Indeed, setting  and , we get

which gives the second bound.
\end{proof}


For Corollary~\ref{cor:1}, we have:

\begin{proof}
The result follows by observing that hash-thinning with an independent uniform 
hash function  taking  values leads to  bins in \eqref{e:simple-1}.  
\end{proof}


\subsection{Bounds on the rate of identification}


The proof of Theorem~\ref{e:bm-expected} is as follows.

\begin{proof} The assumption  guarantees that 
 equals the number of distinct values in the set of hashes .
Let  if bin  is occupied and  otherwise, for  Observe that

by exchangeability. Note, however, that 

since  are independent and Uniform.

This, in view of \eqref{e:e_r} and \eqref{e:bm-expected-1}, implies 
the first relation in \eqref{e:bm-expected}. The second follows from the standard
approximation 
\end{proof}






\small
\balance
\bibliographystyle{IEEEtran}
\bibliography{detection}
\end{document}
