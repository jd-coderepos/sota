\documentclass{amsart}

\usepackage{amssymb,latexsym,amsfonts,amsmath}
\usepackage{multicol} 
\usepackage{graphicx}
\include{diagrams}
\usepackage{eso-pic}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{color}
\usepackage{tikz}
\usetikzlibrary{automata}
\usetikzlibrary{shapes}
\usepackage[normalem]{ulem}
\usepackage{refcount}


\usepackage{amssymb,latexsym,amsfonts,amsmath}
\usepackage{graphicx}

\topmargin  = 0.0 in
\leftmargin = 0.9 in
\rightmargin = 1.0 in
\evensidemargin = -0.10 in
\oddsidemargin =  0.10 in
\textheight = 8.5 in
\textwidth  = 6.6 in
\setlength{\parskip}{2mm}
\setlength{\parindent}{0mm}
\def\baselinestretch{1}

\newcommand{\footnoteremember}[2]{
\footnote{#2}
\newcounter{#1}
\setcounter{#1}{\value{footnote}}
}
\newcommand{\footnoterecall}[1]{
\footnotemark[\value{#1}]
}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{construction}[theorem]{Construction}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{xca}[theorem]{Exercise}
\newtheorem{comments}[theorem]{Comments}
\newtheorem{remark}[theorem]{Remark}
\numberwithin{equation}{section}

\def\myparagraph#1{{\smallskip\noindent{\bf #1}}}

\renewcommand{\Re}{{\mathbb{R}}}
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\Q}{{\mathbb{Q}}}
\newcommand{\Ze}{{\mathbb Z}}
\newcommand{\B}{{\mathbb B}}
\newcommand{\Ce}{{\mathbb C}}
\newcommand{\N}{{\mathbb{N}}}
\newcommand{\T}{{\mathbb T}}
\newcommand{\ie}{{\it i.e. }}
\newcommand{\eg}{{\it e.g. }}
\newcommand{\ea}{{\it et al }}
\newcommand{\D}{\displaystyle}
\newcommand{\Pre}{\mathrm{Pre}}
\newcommand{\Post}{\textrm{Post}}
\newcommand{\argmax}{\textrm{arg}\max}
\newcommand{\argmin}{\textrm{arg}\min}
\newcommand{\UP}{\textrm{UP}}
\newcommand{\X}{{\mathbf{X}}}
\newcommand{\Next}{{\mathrm{Next}}}
\newcommand{\Inv}{{\mathrm{Inv}}}
\newcommand{\Reach}{{\mathrm{Reach}}}
\newcommand{\reals}{\ensuremath{(\R, <, +, -, \cdot, 0,1)}}
\newcommand{\kr}{\textrm{ker}}
\newcommand{\s}{\mathrm{span}}
\renewcommand{\dim}{\mathrm{dim}}
\newcommand{\lie}{\mathrm{Lie}}
\newcommand{\cl}{\mathrm{cl}}
\newcommand{\inte}{\mathrm{int}}
\newcommand{\bd}{\mathrm{bd}}
\newcommand{\E}{\mathrm{End}}

\def\set#1{{ \{ #1 \}}}
\def\tuple#1{{ \langle #1 \rangle}}
\def\true{{\mathit{true}}}
\def\false{{\mathit{false}}}

\newcommand{\bottom}{\perp}
\newcommand{\dbrkts}[1]{[\![#1]\!]}
\newcommand{\valuation}[1]{\{\!\{#1\}\!\}}
\newcommand{\norm}[1]{|\!|#1|\!|}
\newcommand{\normone}[1]{\ensuremath{{|\!|#1|\!|_{1}}}}
\newcommand{\normtwo}[1]{\ensuremath{|\!|#1|\!|}}
\newcommand{\refine}[1]{\Delta}

\def\etc{{\it etc.}}
\def\reals{\mathbb{R}}
\def\nats{\mathbb{N}}
\def\ints{\mathbb{Z}}

\def\qed{\rule{0.4em}{1.4ex}}
\def\del{\partial}
\def\mynote#1{{ \sf  RM: #1 }}



\def\tool{{\rm Costan}\xspace}
\def\fp{{\it{fixedpt}}}
\def\pre{{\it{pre}}}
\def\irange{{\mathsf{rng}}}
\def\typ{{\mathsf{typ}}}
\def\op{{{\oplus}}}
\def\SP{{\mathsf{SP}}}
\def\repr{{\mathsf{repr}}}
\def\fp{{\mathsf{fp}}}
\def\SHL{{\mathsf{shl}}}
\def\SHR{{\mathsf{shr}}}
\def\LSB{{\mathsf{lsb}}}

\newcommand{\IFTE}[3]{{\mathtt{ite}(#1,#2,#3)}}
\def\RETURN{{\mathtt{ret}}}
\def\ASSUME{{\mathtt{assume}}}
\def\CHOICE{{[\!]}}

\renewcommand{\s}{\sout}
\renewcommand{\r}{\textcolor{red}}

\begin{document}

\begin{abstract}
Software implementations of controllers for physical systems are at the core of many embedded systems.
The design of controllers uses the theory of dynamical systems to construct a 
mathematical control law that ensures that the controlled system has certain properties, 
such as asymptotic convergence to an equilibrium point, while optimizing some performance criteria.
However, owing to quantization errors arising from the use of fixed-point arithmetic,
the implementation of this control law can only guarantee
{\em practical} stability: under the actions of the implementation, the trajectories
of the controlled system converge to a bounded set around the equilibrium point, and the
size of the bounded set is proportional to the error in the implementation.
The problem of verifying whether a controller implementation achieves practical stability
for a given bounded set has been studied before.
In this paper, we change the emphasis from verification to automatic {\em synthesis}. 
Using synthesis, the need for formal verification can be considerably reduced thereby reducing 
the design time as well as design cost of embedded control software. 

We give a methodology and a tool to synthesize embedded control 
software that is Pareto optimal
w.r.t.\ both performance criteria and practical stability regions.
Our technique is a combination of static analysis to estimate quantization 
errors for specific controller implementations
and stochastic local search over the space of possible controllers using particle swarm optimization.
The effectiveness of our technique is illustrated using examples of
various standard control systems: 
in most examples, we achieve controllers with close
LQR-LQG performance but with implementation errors, hence regions of practical stability, 
several times as small.
\end{abstract}


\title[Synthesis of Minimal Error Control Software]{Synthesis of Minimal Error Control Software}


\author[Rupak Majumdar]{Rupak Majumdar} 
\author[Indranil Saha]{Indranil Saha} 
\author[Majid Zamani]{Majid Zamani}
\address{Max Planck Institute for Software Systems\\
Kaiserslautern, Germany}
\email{rupak@mpi-sws.org}
\urladdr{http://www.cs.ucla.edu/~rupak}
\address{Department of Computer Science\\
University of California at Los Angeles,
Los Angeles, CA 90095}
\email{indranil@cs.ucla.edu}
\urladdr{http://www.cs.ucla.edu/~indranil}
\address{Department of Electrical Engineering\\
University of California at Los Angeles,
Los Angeles, CA 90095}
\email{zamani@ee.ucla.edu}
\urladdr{http://www.ee.ucla.edu/~zamani}

\maketitle

\section{Introduction}

Software implementations of controllers for physical systems are the core of
many critical cyber-physical systems.
The design of these systems usually proceeds in two steps.
First, starting with a mathematical model of the system, one designs a
mathematical control law 
that ensures that the physical system, equipped with this control law, has certain
desirable properties such as asymptotic stability (convergence to an ideal behavior)
and performance.
Second, the control law is implemented as a software task on a specific hardware architecture.
Since the implementation has quantization errors due to the use of 
fixed-precision representation of real numbers, the quantization of a stabilizing controller may 
lead to limit cycles and chaotic behavior \cite{kalman}. Hence, the implemented system usually 
guarantees the weaker property of {\em practical} stability,
where the system is guaranteed to converge to a bounded set around the ideal behavior
and the size of the bounded set is proportional to the quantization error. 

Much recent research has focused on verifying that
a given implementation of a control law
guarantees that the practical stability region lies within a given set
\cite{PodelskiW06,PodelskiW07,feron_journal,anta,DM11}.
In this paper, we change the emphasis from verification to {\em synthesis}.
We provide a design methodology to synthesize a control implementation for which 
the effect of implementation errors on system performance is minimized. 
Hence, the need for verification can be substantially reduced.

We focus on linear systems in this paper.
For linear systems, a standard optimal control design approach uses 
the {\em linear quadratic regulator} (LQR) and {\em linear quadratic Gaussian} (LQG) algorithms~\cite{joao}, 
which finds a feedback controller stabilizing the plant while minimizing quadratic cost functions. 
The LQR cost function takes into account the deviations of the state and control inputs from ideal
values and the LQG cost function takes into account the deviation of the state from its estimation. 
However, in general, they do not take implementation errors arising from
fixed-precision arithmetic into account.
Thus, a controller optimizing only the LQR-LQG cost may have a large implementation error
because its implementation on a fixed-precision platform has large numerical
errors, but a controller ``close'' to the optimal performance may have much lower
numerical errors when implemented on the same platform.

In our methodology, we modify the LQR-LQG performance criterion to additionally minimize 
the error due to quantization in the implementation.
Technically, our methodology has two parts.
First, how can we estimate the quantization error of a given implementation?
Second, how can we find Pareto-optimal points for the two objectives given by the 
LQR-LQG and quantization error cost functions?
We proceed as follows.

For the first step, 
for a given linear feedback controller and the operating intervals of the states of the plant and the controller, 
we first perform a precise range analysis of the controller variables, 
and use the computed ranges to allocate bitwidths to each controller variable. 
We implement our range analysis based on linear programming. 
Using the allocated bitwidths, we generate code for a fixed-precision 
program implementing the control law. 
Finally, we use an algorithm based on mixed-integer linear programming to find 
a bound on the maximum difference 
between the ideal control law and the output of the fixed-precision program.

For the second step, we optimize a weighted linear combination of the two cost functions
using a stochastic local search technique.
LQR-LQG is attractive because it gives rise to a {\em convex} optimization problem,
for which efficient solutions are known.
Unfortunately, additionally tracking the quantization error results in a non-convex
optimization problem.
We solve the non-convex optimization problem using {\em particle swarm optimization} (PSO), a
population-based stochastic optimization approach \cite{kennedy,liu,jiang}. 
PSO iteratively solves an optimization problem by maintaining a population (or {\em swarm}) of candidate controllers, 
called {\em particles}, and moving them around in the search-space of possible
controllers, trying to minimize the objective function.

In more detail, our algorithm proceeds as follows.
Given a linear control design problem, we set up a non-convex optimization problem to minimize a weighted combination
of the LQR-LQG cost function and the implementation error.
We minimize this cost function using PSO.
In each step of PSO, given a new position of a particle, we check if the position represents a stabilizing
controller (by examining the eigenvalues of the controlled system).
If not, we assign the position an infinite cost.
If the position represents a stabilizing controller, we generate the best possible 
fixed-point code for this controller under a hardware budget
and perform static analysis to estimate a bound on the implementation error.
We compute the value of the objective function by taking the weighted sum of the LQR-LQG cost and this bound.
We continue PSO until convergence or until some iteration bound is met.
At this point, we output the controller that minimized the objective function.

We have implemented this methodology
on top of Matlab's Control Theory Toolbox, using an implementation of PSO proposed in~\cite{ebbesen}, 
and a custom static analysis using the lp\_solve linear programming
tool. In our experiments, we compare the LQR-LQG cost and implementation errors of
controllers generated by conventional LQR-LQG optimization (implemented in Matlab)
with controllers generated by PSO using our methodology.
In most cases, our controllers have LQR-LQG costs close to the optimal LQR-LQG controllers, but
have implementation errors that are reduced by a factor of 4 or more.
Thus, we generate controllers with guaranteed bounds on practical stability regions that are
4 times or more smaller than the pure LQR-LQG controllers.
Our work provides, for the first time, an integrated analysis and tool to take 
quantization errors into account in model-based design and implementation of controllers.

\myparagraph{Other Related Work}
Besides the related work mentioned above, we mention
the results in \cite{williamson,williamson1,liu1} which provide controller synthesis approaches minimizing some performance 
criteria while controllers are implemented using fixed-point arithmetic. 
The results in \cite{williamson,williamson1,liu1} assume some excitation conditions under which the quantization error 
can be modeled as a zero mean uniform white noise. 
Furthermore, they do not provide any bounds on regions of practical stability. 
However, the result in this paper does not have any assumption on the quantization error 
and it provides an explicit bound on the regions of practical stability.

The range analysis problem has been studied extensively in the context of 
optimum bitwidth allocation to intermediate variables in a fixed-point program, mostly in the DSP domain.
Both static \cite{LGCMLC_TCAD_06, LCN_TCAD_07, OCCLM_FPL_07} 
and simulation-based \cite{BR_RSP_05, MSBZ_TCAD_07} approaches have been used. 
Static approaches usually employ abstractions based on interval arithmetic \cite{Moore66} or affine arithmetic \cite{SF_IMPA_97}. 
Simulation-based methods, especially those performing constrained-random simulations, suffer from the lack of completeness. 
This causes the resultant system to be non-robust, incomplete simulation can lead to overflow 
conditions resulting in incorrect behavior.
We found our mixed-integer linear programming approach to be both precise and reasonably fast for our application.


\section{Preliminaries}\label{preliminaries}

\subsection{Controllers and Observers}
We use symbols , , and  for the set of nonnegative integers, real and nonnegative real numbers.
For a vector , we denote by  the {-th} element of , 
and by  the Euclidean norm of .
Recall that \mbox{}. The symbols  and  denote the identity and zero matrices in  and , respectively. A continuous function \mbox{}, is said to belong to class  
if it is strictly increasing and \mbox{};  is said to belong to class  if \mbox{} 
and \mbox{} as . 
A continuous function \mbox{} is said to belong to class  if, 
for each fixed , the map  belongs to class  with respect to 
and, for each fixed nonzero , the map  is decreasing with respect to  and  as \mbox{}. 

In this paper, we focus on \textit{linear} control systems given by the differential equation:

where, for any , , , , , and , , , and  are matrices of appropriate dimensions. 
The curve  is a \textit{trajectory} of (\ref{control_system}) if there exist curves  and  such that the time derivative of  satisfies (\ref{control_system}). In the rest of the paper, we assume that all curves  and  have some regularity assumptions, guaranteeing existence and uniqueness of the solutions of (\ref{control_system}). Note that , , , and  denote control input, disturbance, output of the system and measurement noise, respectively. We assume that  and , for any , are zero-mean Gaussian noise processes (uncorrelated from each other). For all curves , we also write  to denote the points reached at time  under the input  from initial condition . 

To describe the mismatch between the controller specifications and its software implementations 
such as digital sampling and finite precision arithmetic, which is the focus of this paper, 
we consider the discrete-time version of (\ref{control_system}), as follows:

where the matrices , , and  are given by:

and  is the sampling time. The function , for any , denotes the matrix function defined by the convergent series:

where \textsf{e} is Euler's constant. The signals , , , , and  describe the exact value of the signals , , , , and , respectively, at the sampling instants . Mathematically, we have:

The term  in (\ref{control_system1}) is the sampling error. It can be shown that by sampling sufficiently fast, the error  can be made arbitrarily small \cite{chen}. 
Since typical embedded controller implementations use sampling time in the range of milliseconds to microseconds, we will 
make the assumption that quantization errors dominate the sampling errors, and 
assume that . 
 
We assume that only output of the system  is measurable and not the full state . 
Hence, a (proportional) {\em feedback}  defines the input  based on an estimation  of the state . 
As explained in \cite{joao}, the estimation  can be constructed using the observer dynamic:

where  should be viewed as an estimate of  and the linear map  is called an observer gain. 
By applying the feedback  and combining the dynamics of control system in (\ref{control_system1}) and 
observer in (\ref{observer}), one obtains:


As shown in \cite{anta}, using a fixed-point implementation of the feedback gain as well as the observer dynamic, 
one gets the following overall dynamic:

where  and  are quantization errors in observer dynamic and feedback gain codes, respectively. 
Now, one can rewrite the control system in (\ref{overal}) as follows:

with: 

and:

Since states of the control system (\ref{control_system}) are bounded physical quantities, such as 
temperature, pressure, position, velocity, acceleration and so on, their estimations and the output of the control system are bounded quantities as well. Hence, in the rest of the paper and without loss of generality, we assume that , and , where , and  are compact.

\subsection{Stability of perturbed systems}

Here, we recall the notion of uniform global asymptotic stability with respect to a set, 
presented in \cite{lin}. 

\begin{definition}[\cite{lin}]
\label{UGAS}
A control system of the form (\ref{control_system}) is uniformly globally asymptotically stable (UGAS) 
with respect to a set  if there exists a  
function  such that for any , any , any control input , and for all possible disturbances , where , and  are compact sets, the following condition is satisfied:
 
where the {\em point-to-set distance}  is defined by \mbox{}. 
\end{definition}

When the set  is a singleton , we speak of an asymptotically stable equilibrium
point  rather than a UGAS set. The notion of UGAS for discrete-time control systems is obtained from Definition \ref{UGAS} by replacing  with .

We recall the following result describing how stability properties are affected by additive disturbances.
\begin{proposition}[\cite{anta}]
\label{prop:anta}
Consider the discrete-time linear system:

and assume that the origin is an asymptotically stable equilibrium point. Then, for any signal  satisfying  for any  and some constant , the system:

is UGAS with respect to the set:
 
where  is given by:

with . Moreover, the output  is guaranteed to converge to the set:
  
with:

\end{proposition}

In control theory,  is known as the {\em  gain} of the control system in (\ref{control_system4}) with the output .
The following proposition follows from Proposition~\ref{prop:anta} and describes the stability properties of linear control systems in (\ref{overal1}) with 
respect to disturbance, measurement noise, and implementation errors in the feedback gain and observer dynamic. 

\begin{proposition}
\label{prac_stability1}
Consider the discrete-time linear system in (\ref{overal1}). Then for any input  and  satisfying  and  for any  and some
constants , the system is UGAS with respect to the set:
 
where  and  are given by: 

with . Moreover, the output  is guaranteed to converge to the set:

where  and  are given by: 

\end{proposition}

The error vector  includes disturbance and measurement noise, depending for example on the environment and the quality of the sensors collecting measurements. Hence, the controller designer does not have any control on the value of . However, one can reduce the amount of  by appropriately choosing gains  and . On the other hand, one can reduce the amount of not only  but also  by appropriately choosing gains  and . We use Proposition~\ref{prac_stability1} in the following way.
Given a feedback gain  and an observer gain , we compute  gains  and  and an upper bound  on the implementation error . Then the output of the controlled system (with
implementation error) must converge to set  in (\ref{prac_stable_reg}).
We show later that appropriate choices of gains  and  can shrink the size of the 
set  and hence, provide a tighter bound on the set 
to which the output of the system converges.

\subsection{LQR-LQG performance}
In addition to asymptotic stability, controller designers also consider the {\em performance} of the controller,
that is, of the controllers ensuring asymptotic stability of the origin, one desires the controller that minimizes
a given cost function.
A common approach for optimal output feedback controller are the {\em linear quadratic regulator} (LQR) and {\em linear quadratic Gaussian} (LQG). 
The LQR cost function to be minimized is given by:

for some chosen weight matrices  and  that are positive definite and of appropriate dimensions. 

The LQG cost function to be minimized is given by:

where  stands for expected value and  is the estimation error for the control system in (\ref{overall0}) whose dynamic is given by:

As mentioned before,  and  are assumed to be zero-mean Gaussian noise process (uncorrelated from each other) with covariance matrices:

where  and  are some positive semi-definite matrices of appropriate dimensions.

A standard control-theoretic construction rewrites the cost function (\ref{LQR_cost}) as , where , and  is a positive definite matrix that is the unique solution for  to the Lyapunov equation:

where  is a controller making  Hurwitz.\footnote{
We call the matrix  Hurwitz if its eigenvalues are inside the unit circle, centered at the origin.} 
See \cite{joao} for detailed information. Additionally, we have

where  and  are minimum and maximum eigenvalues of , respectively. 
Therefore,  can be minimized for all possible choice of initial conditions by just minimizing the maximum eigenvalue of . Note that since  is a positive definite matrix, its maximum eigenvalue is equal to its induced 2 norm\footnote{We recall that induced 2 norm of a matrix  is given by: .} .  

Similarly, the cost function (\ref{LQG}) can be rewritten as , where  is a positive definite matrix that is the unique solution for  to the Lyapunov equation:

where  is an observer gain making  Hurwitz. See \cite{joao} for more detailed information. Therefore,  can be minimized by just minimizing .

Note that the optimal feedback  minimizing the LQR cost in (\ref{LQR_cost}) is computed using the deterministic dynamic:  On the other hand, the optimal gain  minimizing the LQG cost in (\ref{LQG}) is computed using the stochastic dynamic in (\ref{stochastic_dyn}). Thanks to the separation principle for linear control systems \cite{joao}, one concludes that the overall closed loop system in (\ref{overall0}) is  even though the gains  and  are designed separately.

\subsection{The effect of errors}

\myparagraph{Example}
We now present a simple motivating example showing how different choice of
controllers result in different steady state errors due to their fixed-point implementations, 
yet providing approximately the same LQR-LQG performance. 
Consider a simple physical model of a bicycle, borrowed from \cite{astrom}. 
The dynamics of the system is given by:

where  is the steering angular velocity,  is the steering angle,  is the role angle,  is the torque applied to the handle bars, 
 is the acceleration due to gravity,  is the height of the center of mass, 
 is the velocity of the bicycle at the rear wheel, 
 is the distance of the center of mass from a vertical line through the contact point of the rear wheel and  is the wheel base. 

The control objective is to design a feedback gain  and an observer gain  such that the feedback control law , 
where \mbox{} is the state of the observer in (\ref{observer}), 
makes the closed loop system UGAS. 
By choosing the matrices  and  inside the LQR cost function and  and  in (\ref{spectrum}), 
the feedback and observer gains minimizing the LQR and LQG costs are given by , and , respectively. 
Consider a second pair of feedback and observer gains given by  and . 
For the initial condition , the value of the LQR cost function is  for feedback gain  and 
 for . 
Moreover, the value of the LQG cost function is  for observer gain  and  for . 
That is, the gains  and  give cost functions about 7\% greater than the optimal gains  and .

\begin{figure}
  \centering\includegraphics[width=13cm]{bicycle_evolution}
  \caption{Evolution of the output  with initial state  for the pair of gains ,  and ,  using 16-bit implementation.}
\label{fig1}
\end{figure}

We now show how different choice of feedback and observer gains result in different fixed-point implementation errors.
For now, let us assume that  and , for any . 
In Figure \ref{fig1}, we show the output of the closed-loop system starting from the 
initial condition , 
when the feedback gain and observer dynamic are implemented using 16-bit fixed-point representation. 
As can be observed from Figure~\ref{fig1}, 
the output of the controlled system 
does not converge to the equilibrium point at the origin because 
of the fixed-point implementation error in the controllers. Furthermore, the practical stability region using gains  and  is much smaller than the one using gains  and .

Using bounds on implementation errors for the two controllers (described in Section~\ref{error_comp})
and Proposition~\ref{prac_stability1}, we can prove that the output of the system with feedback and observer gains  and  
(resp.\  and ) converges to a ball centered at the origin with radius  (resp.\ ), 
whenever the output of the system and the state of the observer take values in the interval  and the feedback gain and observer dynamic are implemented 
using 16-bit fixed-point implementation. 
As can be seen, given a 16-bit implementation, feedback and observer gains  and  may be preferred to gains  and  
because they have guaranteed bounds on practical stability region that is 10 times smaller than gains  and  and provide approximately similar performance. 
If one considers the effect of disturbance and measurement noise, it can be proved that the output of the system with feedback and observer gains 
 and  (resp.\  and ) converges to a ball centered at the origin with radius  (resp. ),
where  is an upper bound on the size of the vector  introduced in (\ref{overal1}).

\myparagraph{Optimization objectives}
The above example suggests that the control design should optimize for the following objectives: 
the LQR and the LQG costs for performance, error caused by disturbance and measurement noise, 
and the implementation error given by a fixed-precision encoding. 
Accordingly, we define a cost function that is weighted sum of the four factors:

where  are weighting factors,  and  are matrices, computed from Lyapunov equations in (\ref{lyapunov}) and (\ref{lyapunov1}) using standard LQR and LQG gains ( and ), 
 and  (resp.  and ) are the  gains in (\ref{l2_gain}) using feedback and observer gains  and  
(resp.  and ) and  (resp. ) is the bound on the implementation error of given feedback and observer gains 
 and  (resp.  and ). 
Minimizing the terms  and  inside \eqref{cost1} results in a tighter bound 
on the set  in Proposition~\ref{prac_stability1}. 
Since the four factors in (\ref{cost1}) have different scales, we normalized them by their values using the standard gains 
 and . 
The designer can choose  based on the priorities on LQR and LQG performances and steady state error.
Our objective is to find feedback and observer gains that minimize the cost function .

We focus on implementation errors arising out of fixed-precision arithmetic. The bound  is computed using the strategy, explained in Section \ref{error_comp}. 
Note that the cost function  is not necessarily convex with respect to the feedback and observer gains  and . 
Therefore, the proposed design strategy cannot be formulated as a convex optimization problem. We use a heuristic stochastic optimization approach
to find feedback and observer gains  and  minimizing . 

In our exposition, we consider the plant model to be precise, and only consider quantization effects as the source of error.
Our methodology can consider both additive and multiplicative uncertainties in the plant model as well \cite{green}. 
We can take those uncertainties into account by adding appropriate extra terms to the cost function in (\ref{cost1}) 
using the results provided in \cite{majid1,majid2}. 
We omit the details for simplicity.

\section{Computing Quantization Error }\label{error_comp}

In this section we show how to compute a bound on the fixed-point implementation error for given feedback and observer gains  and . 
We assume that the outputs of the controlled system and the state of the observer are restricted to compact subsets  and , respectively. 

\subsection{Best fixed-point implementation}
A {\em fixed-point representation} of a real number is a triple  
consisting of a {\em sign bit}  (for {\em signed} and {\em unsigned}),
a {\em length} , and a {\em length of the fractional part} .
The length of the integer part is . 
Intuitively, a real number is represented using  bits, of which  bits are used to store
the fractional part.
Clearly, the largest integer portion has to fit in  bits.

A variable with a fixed-point type is represented as an integer. 
We associate an integer variable  with the fixed-point representation of a real variable .
An integer variable  that represents a fixed-point variable with type  
can be interpreted as the rational number .
We deal with a signed number by separately tracking the sign and the magnitude,
performing the operations on the magnitudes using unsigned arithmetic, and
finally putting the appropriate sign bits back.

An operation using real arithmetic may have different fixed-point implementation depending on
how many bits are allocated to hold the integer part and the fraction part of the variables. 
Allocating fewer number of bits than required to hold the integer part may lead to overflow. 
On the other hand, if more than the required number of bits are allocated to the integer part, 
the quantization error increases due to assigning few bits to the fractional part. 
When we compare fixed-point implementation of different controllers, we first synthesize the best 
possible implementation of a controller. 

Let us fix the number of bits to be  for the implementation of a controller. 
With  bits, let  be the upper bound on the quantization error in a fixed-point implementation  of a controller
for a given range for the inputs. 
The fixed-point implementation  is the {\em best implementation} if  there 
does not exist another implementation  using  bits, 
for which the upper bound on the quantization error is  and .


If the ranges of the variables in the real arithmetic computation can be computed exactly,  
it is possible to synthesize the best fixed-point implementation. In the best fixed-point implementation,
the number of bits allocated to the integer part is just enough to hold the integer part of any value in that range.
For example, if the range of a variable is \mbox{[-35.55,~48.72]}, the datatype for the variable in the best
16-bit fixed-point representation is .  

The range computation problem
of variable  in an operation 

involves solving a maximization and a minimization
problem where  is the objective function and the ranges on  form the set of constraints.
If the function  is convex, the range of  can be computed exactly, and also it is straight-forward to find 
the best fixed-point implementation for the operation. 


\subsection{Error bound computation}

We apply mixed-integer linear programming based optimization technique to find out the error bound
between a computation in real arithmetic and its best fixed-point implementation.
Suppose we have an arithmetic operation , where . If , then
either  or  is a constant. If  or , then  and  can both be variables. 
We associate an integer variable  with the fixed-point representation of a real variable .
Let the range of the values for  and  and 
are , , and , respectively. Let the fixed-point representation of 
,  and  are , ,  and , 
respectively. Let  and  be bounds on the quantization errors of  and , respectively.
The optimization problem to find the bound on the error is given by:

where  is the fixed-point representation of the statement 
and 
 denotes a logical formula that relates the inputs and outputs of the
fixed-point representation . Technically,  is the {\em strongest postcondition}~\cite{Winskel} of  with respect to\ . We compute  
using an arithmetic encoding of a fixed-point computation~\cite{anta}. 
Here we illustrate the computation of the strongest
postcondition  using an example.

\smallskip
\noindent
{\bf Example.} Suppose we have the following arithmetic operation 
 
Assume the compact set for  is [-1, 1]. The fixed-point expression corresponding to  in the best fixed-point implementation is 

The strongest postcondition  of  is given by:\\
\begin{center}
\begin{tabular}{rl}
 &  \\
&  \\
&  \\
&  \\ 
&  \\ 
&  \\ 
& ,
\end{tabular}
\end{center}
where , , , and  are integer variables.

Depending on the arithmetic operation, we need to solve at most 4 instances of mixed integer linear
programming problem to solve the optimization problem in (\ref{opt_prob}), and the maximum among all of them gives the bound on the error in the
fixed-point implementation.

We use the above technique to compute the bound on the error in one operation in the 
fixed-point implementation of a gain. The implementation of a gain
involves a series of arithmetic operations. We compute the error bound for the output of one arithmetic operation
at a time. Let  is an arithmetic operation in the implementation of a gain. In the arithmetic operation,
 and  may either be a constant, a state variable or a temporary variable which captures the result of some
previous operation. If  (or ) represents a constant, and the fixed-point representation contains  bits for the 
fraction part, then the error in the fixed point representation is bounded by . If  (or ) represents
a state variable, then the fixed-point datatype can be determined from the given compact set for the state, and the fixed-point
datatype can be determined accordingly. Then the error in the fixed-point representation is bounded by , where
 is the number of bits to represent the fraction part in the fixed-point datatype of the variable. If   (or ) is a temporary variable used to hold 
the result of an earlier computation, then the range and error bound for the variable is already known.

\section{Optimal Controller Synthesis}
We now describe our controller synthesis algorithm that minimizes the cost function~\eqref{cost1}
combining LQR and LQG performance, disturbance, measurement noise and implementation errors.
Since the cost function is non-convex, we use a stochastic local search technique.

\subsection{Particle swarm optimization (PSO)}\label{PSO}
PSO is a stochastic local search approach.
It maintains a set of potential solutions (called ``particles'')
in a compact  dimensional search space , minimizing a given cost function.
The particles move in this space according to their velocity.
Each particle, indexed by , has a position , changing between  and , and a velocity vector , changing between some vectors  and . 
The terms  and  are often set to the maximum dynamic range of the variables on each dimension \cite{majid}: . 
Every particle remembers its own best position (i.e., the lowest value of the cost function achieved so far by this particle) in a 
vector .
The best position with respect to the cost function among all of the particles 
so far is stored in a vector .

PSO updates the positions and velocities of all particles iteratively.
The new velocity and position for particle  are determined as:

where the superscript  denotes the iteration number, the subscript
 denotes the index of the particle, and  is the number of particles. 
The constant  in (\ref{velocity}) is updated using the inertia weight approach \cite{ebbesen} as the following:
 
where  and  are adjusted to  and  and  is the maximum number of iterations. 
The constants  and  in (\ref{velocity}) are the acceleration constants, influencing the convergence speed of particles toward its own and global best positions
and set to  and , respectively \cite{ebbesen}. 
The constants  and  in (\ref{velocity}) are uniformly distributed random numbers on the interval .

\subsection{Overall algorithm} \label{algorithm}

The PSO algorithm is used to search for feedback and observer gains  and  
for the control system (\ref{overal}), minimizing (\ref{cost1}). 
Note that a particle in PSO represents a feedback and an observer gain  and , respectively, moving in an  dimensional search space. 
To discard those gains that make the controlled system unstable, we penalize unstable gains by including a penalty term  in the cost function
such that  if  and  are Hurwitz and  otherwise. 
The cost function for PSO is then .

The design steps can be summarized as the following:
\begin{itemize}
\item[(1)] Initialize positions of  feedback gains  and observer gains  by  and , respectively, and uniformly randomly initialize their velocities , , where  is the number of particles.
\item[(2)] Given any initial feedback gain  and observer gain , compute the cost function .
To compute , check if  and  are Hurwitz.
There are some steps to compute .
First compute  and  by solving the Lyapunov equations~\eqref{lyapunov} and (\ref{lyapunov1}), respectively, and find their induced 2 norm.
Second, compute the  gains  and . Third, compute  by solving the optimization problems from Section~\ref{error_comp}.
\item[(3)] Compare  to its own best position  so far and the global best position  so far.
If  is less than the previous personal best (resp.\ the global best), update the best position (resp.\ the global best) to  and .
\item[(4)] Modify the velocity and position of each pair  and  according to (\ref{velocity}) and (\ref{position}).
\item[(5)] If the number of iterations, denoted by , reaches the maximum, denoted by , or the value of  does not change for the global best position  for 50 consecutive iterations up to error  then go to Step (6), otherwise go to Step (2);
\item[(6)] The latest  is the optimal controller.
\end{itemize}

\section{Extension: PID Controllers}\label{PID}

PID controllers are a common class of controllers in many industries, such as automotive, power systems, servomotors, and so on. 
We now extend the analysis of Section \ref{preliminaries} to PID controllers. 
A PID controller generalizes a proportional feedback controller, and includes three terms: proportional, integrator, and differentiator. 
For an input , the output  of the PID controller is computed as follows:
   
where , , and  are called proportional, integrator, and differentiator gains, respectively. 
To describe the mismatch between the PID specifications and its software implementation, 
we consider the discrete-time version of (\ref{PID_continuous}). 
A common way of discretizing an integrator is based on the trapezoidal approximation. 
An integrator term:
  
can be discretized as follows:
  
where  is the sampling time,  and , for any . A common way of discretizing a differentiator, is based on the backward Euler method. A differentiator term:
  
can be discretized as follows:
  
where  and , for any . By using fast sampling time assumption, we can ignore the errors  and  in the discretized versions of integrator and differentiator in comparison with quantization errors. To follow the same analysis as in Section \ref{preliminaries}, we need a state space realization of PID controller. By resorting to results in control classic \cite{kailath} and using the discretization rules in (\ref{trapezoidal}) and (\ref{BE}), the state space realization of discretized PID controller with input  and output  is obtained as follows:

where

Without loss of generality, consider a single-input () single-output () discrete-time linear control system of the form:

Since the input of the PID controller is equal to the negative of the output of the plant () because of negative feedback and the output of the PID controller is equal to the input of the plant (), one obtains:

Similar to what explained in Section \ref{preliminaries}, by fixed-point implementation of the PID controller, one gets the following overall dynamic:

where  and  are quantization errors in computing the PID controller. Now, we can use the same strategy, as explained in Subsection \ref{algorithm}, to design parameters , , and  of PID controllers minimizing a performance-based cost function as well as the effect of quantization error. For example, one can consider:

where PM and GM are phase and gain margins,  are weighting factors,  is the  gain of the linear control system (\ref{overall5}) and  and  are the bounds on the implementation errors  and . Note that control over PM and GM guarantees robust stability of the closed-loop systems \cite{joao}. The phase and gain margins measure the system's tolerance to the time delay and the steady state gain, respectively.

\begin{small}
\begin{table*}[t]
 \centering
 \resizebox{\textwidth}{!}{
\begin{tabular}{|c|c|c|c|c|} \hline
Control systems & \# bits & \multicolumn{2}{c|}{Synthesized gains} &Time cost\\
\cline{3-4}
&&K&L&\\
\hline
Bicycle & 16 & [3.0253~12.6089] &&1h36m41s\\
\hline
DC motor position  & 16 & [0.1129~0.0211~0.0093] && 1h39m06s\\
\hline
Pitch angle control & 32 & [ -0.1202~42.5655~1.0001] &&8h31m53s\\
\hline
Inverted pendulum & 32 & [-1.5362~-2.0254~16.5192~2.7358]&& 9h54m17s\\
\hline
Batch reactor process & 16 & && 3h08m29s\\
\hline
\end{tabular}}
\caption{Synthesized gains and required time for synthesizing them.}
\label{table-exp1}
\end{table*}
\end{small}

\begin{small}
\begin{table*}[t]
\begin{center}
\resizebox{\textwidth}{!}{
 \begin{tabular}{|c | c | c | c | c | c | c |}\hline
Control& \multicolumn{2}{c|}{ of LQR cost}& \multicolumn{2}{c|}{LQG cost}& \multicolumn{2}{c|}{Steady state error} \\
\cline{2-7}
 systems& LQR & Synthesized & LQG & Synthesized & LQR-LQG & Synthesized \\
 &&K&&L&&gains\\
\hline
Bicycle&3956.3&4331.7&0.0229&0.0246&5.0489+0.5486&2.5341+0.0513\\
\hline
DC motor position&1001.6&1376.7&36.6315&36.6731&30.566+0.16&15.421+0.011\\
\hline
Pitch angle control&&&0.0013&0.0018&2.6781+0.4746&1.4453+0.0807\\
\hline
Inverted pendulum&&&0.3600&0.3897&83.4217+0.0432&30.3801+0.0086\\
\hline
Batch reactor process&223.1773&223.1825&0.0731&0.0949&2.9309+0.4194&2.1216+0.1642\\
\hline
\end{tabular}}
\caption{Least upper bound () on the LQR cost (\ref{LQR_cost}), for a given initial condition , the LQG cost (\ref{LQG}), and the Euclidean norm of the steady state error for the LQR-LQG and the synthesized gains.}
\label{table-exp2}
\end{center}
\end{table*}
\end{small}

\section{Experimental Results}
We implemented the algorithm presented in Section~\ref{algorithm} in Matlab. 
We use a PSO function in Matlab, introduced in~\cite{ebbesen}.
We implemented a static analyzer in Ocaml that synthesizes the best fixed-point program and computes the 
bound on the fixed-point implementation error for given feedback and observer gains  and , respectively. 
The tool gets the number of the bits in the fixed-point datatype, compact subsets  and , 
and feedback and observer gains  and , respectively, as inputs. 
The optimization problems in computing the error bound are solved using the mixed-integer linear programming tool {\sf lp\_solve}~\cite{lpsolve}. 

We applied the proposed controller synthesis approach to a number of linear control systems. 
All the experiments were done on a laptop with CPU Intel Core 2 Duo at 2.4 GHz. 
In all of the experiments, the number of the particles in PSO is , 
the maximum number of iterations is set to , 
and we choose the matrices , and  in (\ref{LQR_cost}) and , and  in (\ref{spectrum}). The value of  was chosen in such a way that appropriate gains are obtained in terms of the cost function (\ref{cost1}) (or (\ref{cost2})) for all control systems.
Moreover, we assume that the search space is  that is large enough and contains the standard LQR and LQG gains for all the examples. Furthermore, without loss of generality, we work on the compact subsets  and . All constants and variables are expressed in SI units. 

Our unstable examples include a bicycle~\cite{astrom}, a DC motor position control~\cite{cmu_examples}, a pitch angle control~\cite{cmu_examples}, an inverted pendulum~\cite{cmu_examples}, a batch reactor process~\cite{green} and another inverted pendulum for PID synthesis~\cite{cmu_examples}. See Table~\ref{table-exp1} and \ref{table-exp2} for experimental results. Note that for those examples for which 32-bit implementation is chosen, the 16-bit one provides a stability region which is even larger than the range of the variables inside the controller. As can be seen from Table \ref{table-exp2}, in comparison with the conventional LQR-LQG approach, the proposed synthesis approach in this paper worsens the LQR and LQG performances by at most  times (for
DC motor position) and  times (for Pitch angle control), respectively. 
However, the proposed synthesis approach improves the size of the region of practical stability due to quantization error by at least  times. For certain examples, the improvement goes beyond the factor of 10. For bicycle  and DC motor position, the region of practical stability due to quantization error improves by a factor of  and , respectively. 

The detailed description of the systems are available as follows. 

\myparagraph{Bicycle}
The model of a bicycle is shown in (\ref{bicycle}). The weighting factors in (\ref{cost1}) are chosen as  and . The results of the LQR, LQG and the proposed method are shown in Tables \ref{table-exp1} and \ref{table-exp2}.
Figure~\ref{fig2} shows how the value of the cost function improves with the number of iteration. The figure shows how
the value of the cost function monotonically decreases with the number of iterations. The fixed-point C code
for the synthesized controller is shown in Figure~\ref{fig:fxc}.

\begin{figure}
\begin{center}
\includegraphics[width=4in]{cost_iteration}
\end{center}
\caption{Cost of the best particle and average cost of all population vs iteration.}
\label{fig2}
\end{figure}

\begin{figure}
\centering
\begin{tt}
\begin{scriptsize}
\parbox{0cm}{
\begin{tabbing}
floa\=t output(float yin)\ \ \=\\
\{\\
\>    static int  = ;\>	// fixdt(1,16,14) \\
\>    static int  = ;\>	// fixdt(1,16,14) \\
\>    int ;	\> 			// fixdt(1,16,14) \\
\>    int ;	\>			// fixdt(1,16,14) \\
\>    int ;		\>        		// fixdt(1,16,11) \\
\\    
\>    // Intermediate variables \\
\>    int ;              \>		// fixdt(1,16,15) \\
\>    int ;              \>		// fixdt(1,16,15) \\
\>    int ;              \>		// fixdt(1,16,15) \\
\>    int ;               \>		// fixdt(1,16,14) \\
\>    int ;              \>		// fixdt(1,16,15) \\
\>    int ;              \>		// fixdt(1,16,15) \\
\>    int ;              \>		// fixdt(1,16,15) \\
\>    int ;               \>		// fixdt(1,16,15) \\
\>    int ;              \>		// fixdt(1,16,13) \\
\>    int ;              \>		// fixdt(1,16,11) \\
\\    
\>     = convert\_to\_fixedpoint();  \\
\>     = ; \\
\>     = ; \\
\>     = ; \\
\>     = ; \\
\>     = ; \\
\>     = ; \\
\>     = ; \\
\>     = ; \\
\>     = ; \\
\>     = ; \\
\>     = ; \\
\>     = ; \\
\>     = ; \\
\>    return(float()); \\ 
\}
\end{tabbing}}
\end{scriptsize}
\end{tt}
\caption{synthesized fixed-point controller C code for Bicycle.} \label{fig:fxc}
\end{figure}

\myparagraph{DC motor position control}
The dynamic of a DC motor position control, borrowed from \cite{cmu_examples}, is given by:

where  is the angle of the motor's shaft,  is the angular velocity of the motor's shaft,  is the armature current,  is the damping ratio of the mechanical system,  is the moment of inertia of the rotor,  is the electromotive force constant,  is the electric resistance,  is the electric inductance and  is the source voltage. The weighting factors in (\ref{cost1}) are chosen as  and . The LQR and LQG gains are given by \mbox{} and \mbox{} and the gains, computed by the proposed approach in this paper, are given in Table \ref{table-exp1}. The detailed results are shown in Tables \ref{table-exp1} and \ref{table-exp2}. 

\myparagraph{Pitch control}
The dynamic of the longitudinal motion of an aircraft, borrowed from \cite{cmu_examples}, is given by:

where  is the angle of attack,  is the pitch rate,  is the pitch angle, and  is elevator deflection angle. The weighting factors in (\ref{cost1}) are chosen as  and . The LQR and LQG gains are given by  and  and the gains, computed by the proposed approach in this paper, are given in Table \ref{table-exp1}. The detailed results are shown in Tables \ref{table-exp1} and \ref{table-exp2}. 

\myparagraph{Inverted pendulum}
Consider a simple physical model of an inverted pendulum on a cart, borrowed from \cite{cmu_examples}. The dynamics of the system is given by:

where , and  are the position and velocity of the cart, respectively, , and  are the angular position and velocity of the mass to be balanced,  is the applied force to the cart,  is the acceleration due to gravity,  is the length of the rod,  is the mass of the system to be balanced,  is the mass of the cart,  is the coefficient of friction of the cart, and  is the inertia of the pendulum. The weighting factors in (\ref{cost1}) are chosen as  and . The LQR and LQG gains are given by  and 

and the gains, computed by the proposed approach in this paper, are given in Table \ref{table-exp1}. The detailed results are shown in Tables \ref{table-exp1} and \ref{table-exp2}. 

\myparagraph{Batch reactor process}
Consider an unstable batch reactor process, borrowed from \cite{green}. The dynamic of the system is given by:

The weighting factors in (\ref{cost1}) are chosen as , , and . The LQR and LQG gains are given by:  

and the gains, computed by the proposed approach in this paper, are given in Table \ref{table-exp1}. The detailed results are shown in Tables \ref{table-exp1} and \ref{table-exp2}. 

\myparagraph{PID controller}
In this example, we provide a PID controller for an inverted pendulum whose dynamic is given by a transfer function. Consider the transfer function of an inverted pendulum, borrowed from \cite{cmu_examples}, given by:

where , output  is the angular position of the mass to be balanced, input  is the applied force to the cart,  is the acceleration due to gravity,  is the length of the rod,  is the mass of the system to be balanced,  is the mass of the cart,  is the coefficient of friction of the cart, and  is the inertia of the pendulum. Using standard results in control theory \cite{kailath}, one obtains the following state space realization for the inverted pendulum:
  

Our objective is to design PID gains , , and  minimizing the cost function (\ref{cost2}) with weighting factors  and the closed loop system has a settling time () of less than 5 seconds and pendulum should not move more than 0.05 radians away from the vertical axis. The latter two constrains are treated the same as the stability constraint in Subsection \ref{algorithm} by penalizing the cost function (\ref{cost2}). The synthesized gains are , , and . The closed loop system has , , , settling time , and pendulum does not move more than 0.0098 radians away from the vertical axis.

\section{Conclusion}

We have presented a generic methodology to search for optimal controller implementations that
minimize implementation errors in addition to traditional controller performance criteria.
While we have instantiated the methodology using the LQR and LQG costs and quantization errors, our algorithm
is more generally applicable to other performance criteria and other sources of modeling or implementation error.
The controller synthesis algorithm can be seamlessly added to any design and automatic code generation tool flow
to enhance its capability to generate correct-by-construction high performance controller software. 
By automatically synthesizing the minimal error controller, we sidestep the need for post-design verification.

\bibliographystyle{alpha}
\bibliography{reference,main}

\end{document} 
