



\documentclass[final]{siamltex} 

\usepackage{amsfonts,epsfig,cite,float,algorithmic,multirow}
\usepackage[cmex10]{amsmath} 

\newlength{\widthC}
\setlength{\widthC}{1.5in}
\newlength{\widthD}
\setlength{\widthD}{2.25in}

\newcounter{Lcount}
\newenvironment{adamEnumerate}{\begin{list}{\arabic{Lcount}.}
{\usecounter{Lcount} \setlength{\rightmargin}{0em}
\setlength{\leftmargin}{1.2em} \setlength{\itemsep}{1.5pt}
\setlength{\topsep}{1.5pt} \setlength{\parsep}{0pt}}}{\end{list}}

\newenvironment{adamItemize}{\begin{list}{}
{\setlength{\rightmargin}{0em}
\setlength{\leftmargin}{2.0em}
\setlength{\itemsep}{6pt}
\setlength{\topsep}{2pt}
\setlength{\parsep}{0pt}}}{\end{list}}

\newenvironment{adamItemize2}{\begin{list}{}
{\setlength{\rightmargin}{0em}
\setlength{\leftmargin}{1.0em}
\setlength{\itemsep}{4pt}
\setlength{\topsep}{2pt}
\setlength{\parsep}{0pt}}}{\end{list}}

\newcommand{\mypar}[1]{\em{#1}}
\newcommand{\field}[1]{\mathbb{#1}}
\newcommand{\la}[1]{\mbox{}}  \newcommand{\sst}[1]{\mbox{\scriptsize{#1}}}
\newcommand{\degree}{\ensuremath{^\circ}} 
\newcommand{\Frac}[2]{{{#1}/{#2}}}  

\newcommand{\pe}{\psi}
\def\d{\delta} 
\def\ds{\displaystyle} 
\def\e{{\epsilon}} 
\def\eb{\bar{\eta}}  
\def\enorm#1{\|#1\|_2} 
\def\Fp{F^\prime}  
\def\fishpack{{FISHPACK}} 
\def\fortran{{FORTRAN}} 
\def\gmres{{GMRES}} 
\def\gmresm{{\rm GMRES()}} 
\def\Kc{{\cal K}} 
\def\norm#1{\|#1\|} 
\def\wb{{\bar w}} 
\def\zb{{\bar z}} 
\def\bfE{\mbox{\boldmath}}
\def\bfG{\mbox{\boldmath}}

\floatstyle{ruled}
\newfloat{algorithm}{htp}{loa}
\floatname{algorithm}{Algorithm}
\newlength{\algSkip}
\setlength{\algSkip}{6pt}

\title{Simultaneously Sparse Solutions to Linear Inverse Problems with Multiple System Matrices and a Single Observation Vector\thanks{This document is a
   manuscript from July 21, 2008, unchanged except for updated references. 
   The content appears in the first author's September 2008 PhD thesis.}
}

\author{Adam C.~Zelinski\thanks{Research Laboratory of Electronics,
        Massachusetts Institute of Technology ({\tt
        zelinski@mit.edu}).}  \and Vivek K~Goyal\thanks{Research
        Laboratory of Electronics, Massachusetts Institute of
        Technology ({\tt vgoyal@mit.edu}).} \and Elfar
        Adalsteinsson\thanks{Research Laboratory of Electronics,
        Massachusetts Institute of Technology ({\tt elfar@mit.edu}).}}

\begin{document}

\maketitle

\begin{abstract}
  A linear inverse problem is proposed that requires the determination
  of multiple unknown signal vectors.  Each unknown vector passes
  through a different system matrix and the results are added to yield
  a single observation vector.  Given the matrices and lone
  observation, the objective is to find a {\em{simultaneously sparse}}
  set of unknown vectors that solves the system.  We will refer to
  this as the {\em{multiple-system single-output}} (MSSO) simultaneous
  sparsity problem.  This manuscript contrasts the MSSO problem with
  other simultaneous sparsity problems and conducts a thorough initial
  exploration of algorithms with which to solve it.

  Seven algorithms are formulated that approximately solve this
  NP-Hard problem.  Three greedy techniques are developed (matching
  pursuit, orthogonal matching pursuit, and least squares matching
  pursuit) along with four methods based on a convex relaxation
  (iteratively reweighted least squares, two forms of iterative
  shrinkage, and formulation as a second-order cone program).  While
  deriving the algorithms, we prove that {\em{seeking a single sparse
  complex-valued vector is equivalent to seeking two
  {\bf{simultaneously sparse}} real-valued vectors}}.  In other words,
  single-vector sparse approximation of a complex vector readily maps
  to the MSSO problem, increasing the applicability of MSSO
  algorithms.

  The algorithms are evaluated across three experiments: the first and
  second involve sparsity profile recovery in noiseless and noisy
  scenarios, respectively, while the third deals with magnetic
  resonance imaging radio-frequency excitation pulse design.  For
  sparsity profile recovery, the iteratively reweighted least squares
  and second-order cone programming techniques outperform the greedy
  algorithms, whereas in the magnetic resonance imaging pulse design
  experiment, the greedy least squares matching pursuit algorithm
  exhibits superior performance.  Overall, each algorithm is found to
  have particular merits and weaknesses, e.g., the iterative shrinkage
  techniques converge slowly, but because they update only a subset of
  the overall solution per iteration rather than all unknowns at once,
  they are useful in cases where attempting the latter is prohibitive
  in terms of system memory.  
\end{abstract}

\begin{keywords} 
sparse approximation, simultaneous sparse approximation, multiple
unknown vectors, greedy pursuit, iteratively reweighted least
squares, iterative shrinkage, second-order cone programming
\end{keywords}

\pagestyle{myheadings}
\thispagestyle{plain}
\markboth{A.~C.~ZELINSKI ET AL.}{MSSO SIMULTANEOUS SPARSE APPROXIMATION}

\section{Introduction}
\label{sec:introduction}

  In this work we propose a linear inverse problem that requires a
  {\em{simultaneously sparse}} set of vectors as the solution, i.e., a
  set of vectors where only a small number of each vector's entries
  are nonzero, and where the vectors' {\em{sparsity profiles}} (the
  locations of the nonzero entries) are equivalent (or are promoted to
  be equivalent with an increasing penalty given otherwise).

  Prior work on simultaneously sparse solutions to linear inverse
  problems involves multiple unknown vectors, a single system matrix,
  and a host of observation vectors; the th observation vector
  arises by multiplying the single system matrix with the th
  unknown vector \cite{Cot2005,Mal2005,Tro2006_I,Tro2006_II}.  Given
  the observation vectors and system matrix, one seeks out a
  simultaneously sparse set of unknown vectors that (approximately)
  solves the overall system.  We will refer to this as the
  {\em{single-system multiple-output}} (SSMO) simultaneous sparsity
  problem.

  Here we study a problem that is somewhat similar yet
  different from the aforementioned one.  This {\em{multiple-system
  single-output}} (MSSO) simultaneous sparsity problem still consists
  of multiple unknown vectors, but now each such vector is passed
  through a {\em{different}} system matrix and the outputs of the
  various system matrices undergo a linear combination, yielding only
  {\em{one}} observation vector.  Given the matrices and lone
  observation, one must find a simultaneously sparse set of vectors
  that (approximately) solves the system.  To date, this problem has
  been explored in a magnetic resonance imaging (MRI) radio-frequency
  (RF) excitation pulse design context
  \cite{Zel2007,Zel2008_CISS,Zel2008_TMI}, but it may also have
  applications to source localization using sensor arrays
  \cite{Joh1993, Kri1996} and signal denoising
  \cite{Don1995,Che1998,Fle2006,Ela2006}.

  Both SSMO and MSSO arise as generalizations of the single-system
  single-output (SSSO) sparse approximation problem, where there is
  one known observation vector, a known system matrix, and the
  solution one seeks is a single sparse unknown vector
  \cite{Gor1997,Che1998,Rao1998}.  Several styles of algorithms have
  been posed to solve the SSSO problem, such as forward-looking greedy
  pursuit \cite{Mal1993, Nat1995, Che1995, Dav1997, Cot1999},
  iteratively reweighted least squares (IRLS) \cite{Kar1970} (e.g.,
  FOCUSS \cite{Gor1997}), iterative shrinkage
  \cite{Don1994,Dau2004,Ela2006_TransIT,Ela2006}, and second-order
  cone programming (SOCP) \cite{Boy2004,Mal2005}.  Many of these have
  been extended to the SSMO problem
  \cite{Cot2005,Mal2005,Tro2006_I,Tro2006_II}.

  In this manuscript, we propose three forward-looking greedy
  techniques---matching pursuit (MP) \cite{Mal1993}, orthogonal
  matching pursuit (OMP) \cite{Nat1995,Che1995,Cot1999}, and least
  squares matching pursuit (LSMP) \cite{Cot1999}---and also develop
  IRLS-based, shrinkage-based, and SOCP-based algorithms to solve the
  MSSO simultaneous sparsity problem.  We then evaluate the
  performance of the algorithms across three experiments: the first
  and second involve sparsity profile recovery in noiseless and noisy
  scenarios, respectively, while the third deals with MRI RF
  excitation pulse design.

  The structure of this paper is as follows: in
  Sec.~\ref{sec:background}, we provide background information about
  ordinary (SSSO) sparse approximation and SSMO sparse approximation.
  In Sec.~\ref{sec:problem}, we formulate the MSSO problem.  Seven
  algorithms for solving the problem are then posed in
  Sec.~\ref{sec:algorithms}, while the details and results of the
  three numerical experiments appear in Sec.~\ref{sec:experiments}.
  Section~\ref{sec:discussion} highlights the strengths and weaknesses
  of the algorithms and presents ideas for future work.  Concluding
  remarks are given in Sec.~\ref{sec:conclusion}.

\section{Background}
\label{sec:background}

\subsection{Single-System Single-Output (SSSO) Sparse Approximation}

   Consider a linear system of equations ,
   where , , , and  and  are
   known.  It is common to use the Moore-Penrose pseudoinverse of
   , denoted , to determine  as an (approximate) solution to the system
   of equations.  When  is in the range of ,
    is the solution that minimizes , the Euclidean or  norm of .  When
    is not in the range of , no solution exists;
    minimizes  among the
   vectors that minimize .
   
   When a sparse solution is desired, it is necessary for the analogue
   to  to have only a small fraction of its entries
   differ from zero.  We are faced with a sparse approximation problem
   of the form
   
   where  denotes the number of nonzero elements of a
   vector.  The subset of  where there are
   nonzero entries in \la{g} is called the \emph{sparsity profile}.
   For general , solving (\ref{sparseopt_NP}) essentially
   requires a search over up to  nonempty sparsity profiles.
   The problem is thus computationally infeasible except for very
   small systems of equations (e.g., even for , one may need
   to search 1,073,741,823 subsets).  Formally, the problem is
   NP-Hard \cite{Dav1994,Nat1995}.

   For problems where (\ref{sparseopt_NP}) is intractable, a large
   body of work supports a greedy search over the columns of \la{F} to
   seek out a small subset of columns that, when weighted and linearly
   combined, yields a result that is close to \la{d} in the 
   sense, along with a sparse \la{g} \cite{Mal1993, Nat1995, Che1995,
   Dav1997, Cot1999}.

   A second body of research supports the {\em{relaxation}} of 
   (\ref{sparseopt_NP}) to find a sparse \la{g} \cite{Che1998}:
   
   This is a convex optimization and thus may be solved efficiently
   \cite{Boy2004}.  The solution of (\ref{sparseopt_relaxed}) does not
   always match the solution of (\ref{sparseopt_NP})---if it did, the
   intractability of (\ref{sparseopt_NP}) would be contradicted---but
   certain conditions on  guarantee a proximity of their
   solutions \cite{Don2006b, Don2006c, Tro2006_convexIT}.
   Note that (\ref{sparseopt_relaxed}) applies an  norm to
   , but an  norm (where ) may also be used to
   promote sparsity \cite{Gor1997, Che1998}; this leads to a
   non-convex problem and will not considered in this paper.

   The optimization
   
   has the same set of solutions as (\ref{sparseopt_relaxed}).  The
   first term of (\ref{sparseopt_noisy}) keeps residual error down,
   whereas the second promotes sparsity of \la{g} \cite{Tib1996,
   Che1998}.  As the control parameter, , is increased from
   zero to infinity, the algorithm yields sparser solutions and the
   residual error increases; sparsity is traded off with residual
   error.  In this paper we shall hereafter use formulation
   (\ref{sparseopt_noisy}) and its analogues rather than
   (\ref{sparseopt_relaxed}).

   It is important to understand that a problem of the form
   (\ref{sparseopt_noisy}) may arise as a proxy for (\ref{sparseopt_NP})
   or as the inherent problem of interest.  For example,
   in a Bayesian estimation setting, (\ref{sparseopt_noisy})
   yields the maximum a posteriori probability estimate of
    from  when the observation model involves 
   and Gaussian noise and the prior on  is Laplacian.
   Similar statements can be made about the relaxations of the
   \emph{simultaneous} sparse approximation problems posed in the
   following sections.

\subsection{Single-System Multiple-Output (SSMO) Simultaneous Sparse Approximation}

   In SSMO, we have  observation vectors (``snapshots''), all of
   which arise from the same system matrix:
   
   where  is known for 
   along with .  In this scenario,
   we want to constrain the number of positions at which \emph{any} of
   the s are nonzero.  Thus we seek approximate solutions in
   which the s are not only sparse, but the union of their
   sparsity patterns is small; that is, a {\em{simultaneously sparse}}
   set of vectors is desired \cite{Mal2005, Tro2006_II}.
   Unfortunately, optimal approximation with a simultaneous sparsity
   constraint is even harder than (\ref{sparseopt_NP}).

   Extending single-vector sparse approximation greedy techniques is
   one way to find an approximate solution \cite{Cot2005,Tro2006_I}.
   Another approach is to extend the relaxation
   (\ref{sparseopt_noisy}) as follows:
   
   where , ,  is the
   Frobenius norm, and
   
   i.e.,  is the  norm of the
    norms of the rows of the \la{G} matrix.\footnote{Although
   here we have applied an  norm to the  row energies
   of , an  norm (where ) could be used in
   place of the  norm if one is willing to deal with a
   non-convex objective function.  Further, an  norm (where ) rather than an  norm could be applied to each row of
    because the purpose of the row operation is to collapse
   the elements of the row into a scalar value without introducing a
   sparsifying effect.} This latter operator is a {\em{simultaneous
   sparsity norm}}: it penalizes the program (produces large values)
   when the columns of \la{G} have dissimilar sparsity profiles
   \cite{Mal2005}.  Fixing  to a sufficiently large value and
   solving this optimization yields simultaneously sparse s.
   For , (\ref{mmv1}) collapses to the base case of
   (\ref{sparseopt_noisy}).  Given the convex objective function in
   (\ref{mmv1}), one may then attempt to find a solution that
   minimizes the objective using an IRLS-based \cite{Cot2005}, or
   SOCP-based \cite{Mal2005} approach.  A formal analysis of the
   minimization of the convex objective may be found in
   \cite{Tro2006_II}.

\section{Multiple-System Single-Output (MSSO) Simultaneous Sparse Approximation}
\label{sec:problem}

   We outline the MSSO problem in a style analogous to that of SSMO
   systems in (\ref{mult_obs}, \ref{mmv1}) and then pose a second
   formulation that is useful for deriving several algorithms in
   Sec.~\ref{sec:algorithms}.

\subsection{MSSO Problem Formulation}

   Consider the following system:
   
   where  and the  are known.  Unlike the SSMO problem, there is now only one
   observation and  different system matrices.  Here we again 
   desire an approximate solution where the s are simultaneously sparse;
   formally,
   
   This is, of course, harder than the SSSO problem and thus NP-Hard.
   To keep
   the problem as general as possible, there are no constraints
   on the values of , , or , i.e., there is no explicit requirement
   that the system be overdetermined or underdetermined.  Further,
   we have used complex-valued rather than real-valued variables.

   In the first half of Sec.~\ref{sec:algorithms}, we will pose three
   approaches that attempt to solve the MSSO problem
   (\ref{msso_nphard}) in a greedy fashion.  Another approach to solve
   the problem is to apply a relaxation similar to
   (\ref{sparseopt_noisy}, \ref{mmv1}):
   
   where \la{G} and  are
   the same as in (\ref{mmv1}) and (\ref{S_norm}), respectively. In
   the second half of Sec.~\ref{sec:algorithms}, we will outline four
   algorithms for solving this relaxed problem.  By setting ,
   (\ref{mmv2_eq2}) collapses to the base case of
   (\ref{sparseopt_noisy}).

\subsection{Alternate Formulation of the MSSO Problem}

   In several upcoming derivations, it will be useful to view the system
   from a different standpoint.  To begin, we construct several new variables
   that are simply permutations of the s and s.
   First we define  new matrices:
   
   where  is the th column of .
   Next we construct  new vectors:
   
   where  is the th element of  and
    is the transpose operation.
   Given the s and s, we now pose the
   following system:
   
   Due to (\ref{C_n}, \ref{h_n}), the system in (\ref{mmv2_eq3}) is 
   {\em{equivalent}} to the one in (\ref{mmv2_eq1}).  The relationship
   between the s and s implies that if we desire
   to find a set of simultaneously sparse s to solve
   (\ref{mmv2_eq1}), we should seek out a set of s where
   many of the s equal an all-zeros vector, \la{0}, but a
   few s are high in  energy (typically with all
   elements being nonzero).  This claim is apparent if we consider the
   fact that  is equal to the
   transpose of , and that the s are only 
   simultaneously sparse when  is sufficiently small.

   Continuing with this alternate formulation, and given our desire
   to find a solution where most of the s are all-zero
   vectors and a few are dense, we relax the problem as follows:
   
   Fixing  to a sufficiently large value and solving this
   optimization yields many low-energy s (each close to
   \la{0}), along with several dense high-energy s.
   Further, because  is
   {\em{equivalent}} to ,
   this means (\ref{mmv2_eq4}) is {\em{equivalent}} to
   (\ref{mmv2_eq2}), and thus just like (\ref{mmv2_eq2}), the approach
   in (\ref{mmv2_eq4}) finds a set of simultaneously sparse
   s.

\subsection{Differences between the SSMO and MSSO Problems}

   In the SSMO problem, we see from (\ref{mult_obs}) that there are
   many different \la{d}s and a single \la{F}.  The ratio of unknowns
   to knowns always equals  regardless of the number of
   observations, .  A large  when solving SSMO is actually
   beneficial because the simultaneous sparsity of the underlying
   s becomes more exploitable; empirical evidence of
   improved sparsity profile recovery with increasing  may be found
   in both \cite{Cot2005} and \cite{Mal2005}.

   In contrast, we see from (\ref{mmv2_eq1}) that in the MSSO problem
   there is a single \la{d} and many different \la{F}s.  Here the
   ratio of unknowns to knowns is no longer constant with respect to
   ; rather it is equal to .  We will show in
   Sec.~\ref{sec:experiments} that as  increases, the underlying
   simultaneous sparsity of the s is not enough to combat
   the increasing number of unknowns, and that for large ,
   correctly identifying the sparsity profile of the underlying
   unknown s is a daunting task.
   
\section{Proposed Algorithms}
\label{sec:algorithms}

We
now derive algorithms to (approximately) solve
the MSSO problem defined in Sec.~\ref{sec:problem}.

\subsection{Matching Pursuit (MP)}

    To begin, we extend the single-vector case of matching pursuit
    \cite{Mal1993} to an MSSO context.  The classic MP technique first
    finds the column of the system matrix that best matches with the
    observed vector and then removes from the observation vector the
    projection of this chosen column.  It proceeds to select a second
    column of the system matrix that best matches with the
    {\em{residual observation}}, and continues doing so until either
     columns have been chosen (as specified by the user) or the
    residual observation ends up as a vector of all zeros.  This
    algorithm is fast and computationally-efficient because the
    best-matching column vector during each iteration is determined
    simply by calculating the inner product of each column vector with
    the residual observation and ranking the resulting inner product
    magnitudes.
    
    Now let us consider the MSSO system as posed in (\ref{mmv2_eq3}).  In
    the first iteration of standard MP, we seek out the single column
    of the system matrix that can best represent \la{d}.  But in
    the MSSO context, we need to seek out which of the  
    matrices can be best used to represent \la{d} when the columns of
     undergo an arbitrarily-weighted linear combination.
    The key difference here is that on an iteration-by-iteration
    basis, we are no longer deciding which column vector best
    represents the observation, but which {\em{matrix}} does so.  The
    intuition behind this approach is that ideal solutions should
    consist of only a few dense s and many all-zeros
    s.  For the th iteration of the algorithm, we 
    need to select the proper index  by solving:
    
    where  is the index that will be selected and 
    is the current residual observation.  For fixed , the solution
    to the inner minimization is obtained via the pseudoinverse,
    , yielding
    
    where  is the Hermitian transpose.  From (\ref{MP_2})
    we see that, analogously to standard MP, choosing the best index
    for iteration  involves computing and ranking a series of
    inner-product-like quadratic terms.

   {\em{Algorithm \ref{alg:MP}}} outlines the entire procedure.
   After  iterations, one obtains 
   (of cardinality ), a set indicating the chosen 
   matrices.  The weights to apply to each chosen matrix (i.e., the
   corresponding s) are obtained via a finalization step;
   they are the best weightings in the  residual error sense
   with which to linearly combine the columns of the chosen 
   matrices to best match the observation \la{d}.  Since  total
   matrices end up being chosen by the process, there is no penalty in
   retuning the  associated  vectors because they are
   allowed to be dense.  The  other s (and
   corresponding s) are not used.\footnote{From the
   perspective of s and s in (\ref{mmv2_eq1}),
   {\em{Algorithm \ref{alg:MP}}} determines weights to place along
   only  rows of  (leaving the other  rows zeroed
   out) that still yields a good approximation of \la{d} in the
    residual error sense.  It is seeking out the best rows of
    which, when densely filled, yield a sound approximation of
   .}

   One property of note is that if , {\em{Algorithm
   \ref{alg:MP}}} stops after one iteration.  This is because
    in this case is simply an  identity matrix for all , and thus any
   one of the s is enough to represent \la{d} exactly when
   its columns are properly weighted and linearly combined.

\begin{algorithm}
    \caption{--- MSSO Matching Pursuit}
    \label{alg:MP}
  {\small

    {\bf{Task}:} greedily choose up to  of the s
                to best represent \la{d} via .\\algSkip]
    {\bf{Precompute}:} , for .\\algSkip]
    {\bf{Iterate}:} Set  and apply:
    \begin{adamItemize}

      \item{n.}

      \item{
      \begin{algorithmic}
	\IF {}
	        \STATE 
		\STATE 
	\ELSE
		\STATE 
		\STATE 
	\ENDIF 
      \end{algorithmic}
      }

      \item{.}

      \item{.  Terminate loop if  or .  
             ends with  elements.}

    \end{adamItemize}

    {\bf{Compute Weights}:} ,
    unstack \la{x} into ; set
    remaining s to \la{0}.
  }
\end{algorithm}


\subsection{Orthogonal Matching Pursuit (OMP)}

    In single-vector MP, the residual  always ends up
    orthogonal to the th column of the system matrix, but as the
    algorithm continues iterating, there is no guarantee the residual
    remains orthogonal to column  or is minimized in the
    least-squares sense with respect to the entire set of  chosen
    column vectors (indexed by ).  Furthermore, 
    iterations of single-vector MP do not guarantee  different
    columns will be selected.  Single-vector OMP is an extension to MP
    that attempts to mitigate these problems by improving the
    calculation of the residual vector.  During the th iteration of
    single-vector OMP, column  is chosen exactly as in MP (by
    ranking the inner products of the residual vector 
    with the various column vectors), but the residual vector is
    updated by accounting for {\em{all}} columns chosen up through
    iteration  rather than simply the last one \cite{Nat1995,
    Cot1999}.

    To extend OMP to the MSSO problem, we choose matrix  during
    iteration  as in MSSO MP and then in the spirit of
    single-vector OMP compute the new residual as follows:
     
    where  and  is the best choice of
    \la{x} that minimizes the residual error .  That is, to update the residual we now employ
    all chosen matrices, weighting and combining them to best
    represent \la{d} in the least-squares sense, yielding an
     that is orthogonal to the columns of  (and
    thus orthogonal to ), which
    has the benefit of ensuring that OMP will select a new 
    matrix at each step.

    {\em{Algorithm \ref{alg:OMP}}} describes the OMP algorithm; the
    complexity here is moderately greater than that of MP because the
    pseudoinversion of an  matrix is required during each
    iteration .

\begin{algorithm}
    \caption{--- MSSO Orthogonal Matching Pursuit}
    \label{alg:OMP}
  {\small
    {\bf{Task}:} greedily choose up to  of the s
                to best represent \la{d} via .\\algSkip]
    {\bf{Precompute}:} , for .\\algSkip]
    {\bf{Iterate}:} Set  and apply:

    \begin{adamItemize}

      \item{n \notin I_{k-1}.}

      \item{}

      \item{}

      \item{.}

      \item{.  Terminate loop if  or .  
             ends with  elements.}

    \end{adamItemize}

    {\bf{Compute Weights}:} ,
    unstack \la{x} into ; set
    remaining s to \la{0}.
  }
\end{algorithm}


\subsection{Least Squares Matching Pursuit (LSMP)}

    Beyond OMP there exists a greedy algorithm with an even greater
    computational complexity known as LSMP\@.  In single-vector LSMP,
    one solves  least squares minimizations during iteration
     in order to determine which column of the system matrix may be
    used to best represent \la{d} \cite{Cot1999}.

    Thus to extend LSMP to MSSO systems, we must ensure that during
    iteration  we account for the  previously chosen 
    matrices when choosing the th one to best construct an
    approximation to \la{d}.  Specifically, index  is
    selected as follows:
    
    where  is the set of indices chosen up through iteration
    , , , and .  For fixed , the solution of the inner
    iteration is ; it is this step that ensures the residual observation
    error will be minimized by using {\em{all}} chosen matrices.
    Substituting  into (\ref{LSMP_1}) and
    simplifying the expression yields 
    
    where .
    
    {\em{Algorithm \ref{alg:LSMP}}} describes the LSMP method.  The
    complexity here is much greater than that of OMP because 
    pseudoinversions of an  matrix are required during each
    iteration .  Furthermore, the dependence of  on
    both  and  makes precomputing all such matrices infeasible
    in most cases.  One way to decrease computation and runtime might
    be to extend the projection-based recursive updating scheme of
    \cite{Cot1999} to MSSO LSMP\@.

\begin{algorithm}
    \caption{--- MSSO Least Squares Matching Pursuit}
    \label{alg:LSMP}
  {\small
    {\bf{Task}:} greedily choose  of the s
                to best represent \la{d} via .\\algSkip]
    {\bf{Initialize}:} Set , , .\\label{irls}
      \begin{split}
        \lambda \sum_{n=1}^{N} \Vert \la{h}_n \Vert_2 &
        = \lambda \sum_{n=1}^{N} \frac{\Vert \la{h}_n \Vert_2^2}{\Vert \la{h}_n \Vert_2}
        = \lambda \sum_{n=1}^{N} \frac{|\la{h}_n[1]|^2 + \cdots + |\la{h}_n[P]|^2}{\Vert \la{h}_n \Vert_2}\\
      & \approx \frac{\lambda}{2} \sum_{n=1}^{N} \left[ \la{h}_n^{\ast}[1], \ldots, \la{h}_n^{\ast}[P]  \right]
              \left[  \begin{array}{ccc}
                 \tfrac{2}{\Vert \la{h}_n \Vert_2 + \epsilon} &        &   \\
                                                 & \ddots &   \\
                                                 &        & \tfrac{2}{\Vert \la{h}_n \Vert_2 + \epsilon} \\
                      \end{array}
              \right]
              \left[ \begin{array}{c}
                       \la{h}_n[1] \\
                       \vdots \\
                       \la{h}_n[P]
                     \end{array}
              \right] \\
         & = \frac{\lambda}{2} \sum_{n=1}^{N} \la{h}_n^{\sst{H}} \la{W}_n \la{h}_n \\
         & = \frac{\lambda}{2} \left[ \la{h}_1^{\sst{H}} \cdots \la{h}_N^{\sst{H}} \right]
                         \left[ \begin{array}{ccc}
                                \la{W}_1 &         &           \\
                                         & \ddots  &           \\
                                         &         & \la{W}_N  \\
                                \end{array} \right]
                 \left [ \begin{array}{c}
                             \la{h}_1 \\
                             \vdots  \\
                              \la{h}_N \\
                              \end{array}
           \right ] = \frac{\lambda}{2} \la{h}_{\sst{tot}}^{\sst{H}} \la{W}_{\sst{tot}} \la{h}_{\sst{tot}},
      \end{split}
    \label{irls2}
    \mathop{\mbox{min}}_{\mbox{\scriptsize{}}}
    \mbox{  }
       \left \{
         \tfrac{1}{2}  \left \Vert \la{d} -
                       \la{C}_{\mbox{\scriptsize{tot}}}
                       \la{h}_{\mbox{\scriptsize{tot}}}
                       \right \Vert_2^2
        +      \tfrac{\lambda}{2}  \la{h}_{\sst{tot}}^{\sst{H}} \la{W}_{\sst{tot}} \la{h}_{\sst{tot}}
       \right \}.
    \label{irls3}
    \begin{split}
      \mathop{\mbox{min}}_{\mbox{\scriptsize{}}}
      \mbox{  }
       \left \{
                \Vert \la{d} - \la{A} \la{q} \Vert_2^2
        +      \lambda \Vert \la{q} \Vert_2^2
       \right \}\\
    \end{split}.
    \algSkip]
    {\bf{Data and Parameters}:} , , , , and  are given.\\algSkip]
    {\bf{Iterate}:} Set  and apply:

    \begin{adamItemize}

      \item{Create  from ;
            construct , 
            , 
            and let .}

      \item{Obtain  by using LSQR to solve
      \la{q}.}

      \item{Set .}

      \item{Line search: find  such that  minimizes (\ref{mmv2_eq4}).}

      \item{Set .}

      \item{.  Terminate loop when  or (\ref{mmv2_eq4}) decreases by less than .}

    \end{adamItemize}

    {\bf{Finalize}:} Unstack the last  solution into
    .
  }
\end{algorithm}


\subsection{Row-by-Row Shrinkage (RBRS)}

    The proposed IRLS technique solves for all  unknowns during
    each iteration, but this is cumbersome when  is large.  An
    alternative approach is to apply an inner loop that fixes  and
    then iteratively tunes  while holding the other
    s () constant; thus only  (rather than
    ) unknowns need to be solved for during each inner iteration.

    This idea inspires the RBRS algorithm.  The term ``row-by-row'' is
    used because each  corresponds to row  of the
     matrix in (\ref{mmv2_eq2}), and ``shrinkage'' is used
    because the  energy of most of the s will
    essentially be ``shrunk'' (to some extent) during each inner
    iteration: when  is sufficiently large and many
    iterations are undertaken, many s will be close to
    all-zeros vectors and only several will be dense and high in
    energy.

    \subsubsection{RBRS for real-valued data} Assume  and the
    s of (\ref{mmv2_eq4}) are real-valued.  We now seek to
    minimize the function by extending the single-vector sequential
    shrinkage technique of \cite{Ela2006_TransIT} and making
    modifications to (\ref{mmv2_eq4}).  Assume we have prior estimates
    of  through , and that we now desire to update
    only the th vector while keeping the other  fixed.  The
    shrinkage update of  is achieved via:
    
    where  forms
    an approximation of \la{d} using the prior solution coefficients,
    but discards the component contributed by the original th
    vector, replacing the latter via an updated solution vector,
    \la{x}.  The left-hand term promotes a solution \la{x} that keeps
    residual error down, whereas the right-hand term penalizes
    \la{x}s that contain nonzeros.  It is crucial to note that the
    right-hand term does not promote the element-by-element sparsity
    of \la{x}; rather, it penalizes the overall  energy of
    \la{x}, and thus both sparse and dense \la{x}s are penalized
    equally if their overall  energies are the same.

    One way to solve (\ref{rbrs1}) is to take its derivative with
    respect to  and find  such that the
    derivative equals .  By doing this and shuffling terms,
    and assuming we have an initial estimate of , we may solve
    for  iteratively:
    
    where ,  is a  identity matrix,
    and  is a small value that avoids ill-conditioned
    results.\footnote{Eq. (\ref{rbrs2}) consists of a direct inversion
    of a  matrix, which is acceptable in this paper
    because all experiments involve .  If  is large,
    (\ref{rbrs2}) could be solved via a CG technique
    (e.g., LSQR).}  By iterating on (\ref{rbrs2}) until (\ref{rbrs1})
    changes by less than , we arrive at a
    solution to (\ref{rbrs1}), , and this then
    replaces the prior solution vector, .  Having completed
    the update of the th vector, we proceed to update the rest of
    the vectors, looping this outer process  times or until the
    main objective function, (\ref{mmv2_eq4}), changes by less than
    .  {\em{Algorithm \ref{alg:RBRS}}} details
    the entire procedure; unlike IRLS, here we essentially repeatedly
    invert  matrices to pursue a row-by-row solution,
    rather than  matrices to pursue a solution that
    updates {\em{all}} rows per iteration.


\begin{algorithm}
    \caption{--- MSSO Row-by-Row Sequential Iterative Shrinkage}
    \label{alg:RBRS}
  {\small
    {\bf{Task}:} Minimize  using an iterative scheme when all data is {\em{real-valued}}.\\algSkip]
    {\bf{Initialize}:} Set  and 
                     (or e.g. ), unstack into .\\label{rbrs_comp2}
     \widetilde{\la{d}}   = \left [  \begin{array}{c}
                                     \mbox{Re}(\la{d}) \\ 
                                     \mbox{Im}(\la{d}) \\
                                     \end{array} \right ] \in \field{R}^{2M}, \mbox{ }\,\,\,
     \widetilde{\la{h}}_n  = \left [  \begin{array}{c}
                                     \mbox{Re}(\la{h}_n) \\ 
                                     \mbox{Im}(\la{h}_n) \\
                                     \end{array} \right ] \in \field{R}^{2P},
   \label{rbrs_comp3}
      \widetilde{\la{C}}_n = \left[ \begin{array}{cc}
                                    \mbox{Re}(\la{C}_n) & -\mbox{Im}(\la{C}_n) \\
                                    \mbox{Im}(\la{C}_n) &  \mbox{Re}(\la{C}_n) \\
                                    \end{array} \right ] \in \field{R}^{2M \times 2P}.
   \label{rbrs_comp4}
      \mathop{\mbox{min}}_{\mbox{\scriptsize{}}} 
      \mbox{  }
      \left \{
        \tfrac{1}{2}  \left \Vert \widetilde{\la{d}} -
                      \sum_{n=1}^N \widetilde{\la{C}}_n
                      \widetilde{\la{h}}_n
                      \right \Vert_2^2
       +     \lambda  \sum_{n=1}^{N} \Vert \widetilde{\la{h}}_n \Vert_2
      \right \}.
   \label{cbcs1}
      \mathop{\mbox{min}}_{\mbox{\scriptsize{}}}
      \mbox{  }
       \left  \{ \tfrac{1}{2}
                 \left \Vert \la{r} - \la{F}_p \la{x}     \right \Vert_2^2 +
         \lambda \sum_{n=1}^{N} \sqrt{ (\la{x}[n])^2 + \la{b}[n] }
       \right \},
    \label{cbcs_extra1}
       \la{r} = \la{d} + \la{F}_p \la{g}_p - \sum_{q=1}^{P} \la{F}_q \la{g}_q,
    \label{cbcs_extra2}
       \la{b}[n] = -(\la{g}_p[n])^2 + \sum_{q=1}^{P} (\la{g}_q[n])^2, \mbox{ for } n = 1, \ldots, N.
    \label{cbcs2}
      \mathop{\mbox{min}}_{\mbox{\scriptsize{}}}
      \mbox{  }
       \left  \{ \tfrac{1}{2} \left(
                 \left \Vert \la{r} - \la{F}_p \la{x}     \right \Vert_2^2 +
          \alpha \left \Vert \la{x} - \la{g}_p            \right \Vert_2^2 -
                 \left \Vert \la{F}_p (\la{x} - \la{g}_p) \right \Vert_2^2 \right) + 
         \lambda \sum_{n=1}^{N} \sqrt{ (\la{x}[n])^2 + \la{b}[n] }
       \right \},
    \label{cbcs3}
      \mathop{\mbox{min}}_{\mbox{\scriptsize{}}}
      \mbox{  }
        \left \{
        \la{c} + \la{v}^{\sst{T}} \la{x} 
               + \frac{\alpha}{2} \la{x}^{\sst{T}} \la{x}
               + \lambda \sum_{n=1}^{N} \sqrt{ (\la{x}[n])^2 + \la{b}[n] }
        \right \},
    \label{cbcs4}
      \mathop{\mbox{min}}_{\mbox{\scriptsize{}}}
      \mbox{  }
        \left \{
                 \la{v}[n] \la{x}[n] + 
                 \frac{\alpha}{2} (\la{x}[n])^2 +
                 \lambda \sqrt{ (\la{x}[n])^2 + \la{b}[n] }
        \right \},
    \label{cbcs_extra3}
         \la{v}[n] + \la{x}[n] \left ( \alpha  + 
                                        \frac{\lambda}{\sqrt{(\la{x}[n])^2 + \la{b}[n]}} 
                               \right ).
    \label{cbcs5}
         (\la{x}[n]_i) =  -\la{v}[n] \left(\alpha + 
           \frac{\lambda}{\sqrt{(\la{x}[n]_{i-1})^2 + \la{b}[n] + \epsilon}}\right)^{-1},
    \algSkip]
    {\bf{Data and Parameters}:} , , , , ,
    , , ,  are given.\\algSkip]
    {\bf{Iterate}:} Set  and apply:

    \begin{adamItemize}

      \item{Sweep over column vectors: set  and apply:}

      \begin{adamItemize}

          \item[]{Optimize a column vector: set  and apply:}

          \begin{adamItemize}

              \item{Construct .}

              \item{Construct , for .}

              \item{Construct .}

              \item{Set .}

              \item{Sweep over column elements: set  and apply:}

                   \begin{adamItemize}

                        \item[]{Optimize th element of \la{x}: set  and apply:}

                        \begin{adamItemize}

                              \item{.}\\

                              \item{.  Stop if  or
                              (\ref{cbcs4}) decreases by less than
                              .}

                        \end{adamItemize}

                        \item[]{.  Terminate when .}

                   \end{adamItemize}

          \item{Update column vector: set  to equal the final .}

          \item{.  Terminate when  or  
               (\ref{cbcs1}) decreases by less than .}

          \end{adamItemize}

      \item[]{.  Terminate when .}

      \end{adamItemize}

      \item{.  Terminate loop when  or
            (\ref{mmv2_eq2}) decreases by less than .}

    \end{adamItemize}

    {\bf{Finalize}:} If  was sufficiently large,  should be simultaneously sparse.
  }
\end{algorithm}


    \subsubsection{Extending CBCS to complex-valued data} If
    (\ref{mmv2_eq2}) contains complex-valued terms, we may structure
    the column-by-column updates as in (\ref{cbcs1}, \ref{cbcs2}),
    but the expansion and derivative of the latter equation's
    objective function does not lend itself to the simple update
    equations given in (\ref{cbcs3}, \ref{cbcs4}, \ref{cbcs5}).  One
    way to overcome this problem is to turn the complex-valued problem
    into a real-valued one.  This approach is not equivalent to the
    one used to extend RBRS to complex data.

    First we stack the target vector, \la{d}, into a real-valued vector:
    
    and then {\em{split}}, rather than stack, the unknown vectors into  new vectors:
    
   We then aggregate these vectors into . Next, we split each
    into two separate matrices, for :
   
   yielding  new real-valued matrices.

   Due to the structure of (\ref{cbcs_comp1}, \ref{cbcs_comp2},
   \ref{cbcs_comp3}), the following optimization is {\em{equivalent}}
   to (\ref{mmv2_eq2}):
   
   The equivalence arises because the first and second terms of
   (\ref{cbcs_comp4}) are equivalent to  and  in (\ref{mmv2_eq2}), respectively.

   This means we may apply CBCS to complex-valued problems by
   performing column-by-column optimization over the  real-valued
   unknown vectors.  This works because CBCS will
   pursue solutions where the  vectors are simultaneously sparse, which is
   equivalent to pursuing simultaneously sparse s.  After running CBCS on the  vectors, one may simply
   restructure them into  complex-valued s.

   Finally, let us set  and thus consider the case of
   single-vector sparse approximation.  The above derivations show
   that {\em{seeking a single sparse complex-valued vector is
   equivalent to seeking two {\bf{simultaneously sparse}} real-valued
   vectors}}.  In other words, single-vector sparse approximation of a
   complex vector readily maps to the MSSO problem, increasing the
   applicability of algorithms that solve the latter.

\subsection{Second-Order Cone Programming (SOCP)}
\label{subsec:socp}

    We now propose a seventh and final algorithm to solve the MSSO
    problem as given in (\ref{mmv2_eq2}).  We branch away from the
    shrinkage approaches that operate on individual columns or rows of
    the \la{G} matrix and again seek to concurrently estimate all 
    unknowns.  Rather than using an IRLS technique, however, we pursue
    a second-order cone programming approach, motivated by the fact
    that second-order cone programs may be solved via efficient
    interior point algorithms \cite{SeDuMi, Toh1999} and are able to
    encapsulate conic, convex-quadratic \cite{Nem2001}, and linear
    constraints.  (Quadratic programming is not an option because the
    s, s, and \la{d} may be complex.)

    Second-order conic constraints are of the form  such that
    
    The generic format of an SOC program is
    
    where ,  is the -dimensional
    positive orthant cone, and the s are second-order cones
    \cite{Nem2001}.
    To convert (\ref{mmv2_eq2}) into the SOC format, we first write
    
    where  and .  The splitting of the complex elements of
    the s mimics the approach used when extending CBCS to
    complex data, and (\ref{socp2}) makes the objective function
    linear, as required.  Finally, in order to represent the  inequality in
    terms of second-order cones, an additional step is needed.  Given
    that , the inequality
    may be rewritten as  and then expressed as a conic constraint:
    
    \cite{Nem2001,Mal2003}.  Applying these changes yields
    
    which is a fully-defined SOC program that may be implemented and
    solved numerically.  There is no {\em{Algorithm}} pseudocode for
    this technique because having set up the variables in
    (\ref{socp3}), one may simply plug them into an SOCP solver. In
    this paper we implement (\ref{socp3}) in SeDuMi
    (Self-Dual-Minimization) \cite{SeDuMi}, a free software package
    consisting of {\tt{MATLAB}} and {\tt{C}} routines.


\section{Experiments and Results}
\label{sec:experiments}
   Our motivation for solving MSSO sparse approximation problems
   comes from MRI RF excitation pulse design.
   Due to the NP-hardness of the problem (\ref{msso_nphard}),
   there is no reasonable way to check the accuracy of approximate
   solutions to these problem instances obtained with the
   algorithms introduced here.  Thus, before turning to the MRI RF
   excitation pulse design problem in Sec.~\ref{subsec:e3},
   we present several synthetic experiments.  These experiments
   allow comparisons amongst algorithms and also empirically reveal
   some properties of the relaxation (\ref{mmv2_eq2}).
   Theoretical exploration of this relaxation is also merited
   but is beyond the scope of this manuscript.

   All experiments are performed on a Linux server with a 3.0-GHz
   Intel Pentium IV processor.  The system has 16 gigabytes of random
   access memory, ample to ensure that none of the algorithms require
   the use of virtual memory and to avoid excessive hard drive paging.
   MP, LSMP, IRLS, RBRS, CBCS are implemented in {\tt{MATLAB}},
   whereas SOCP is implemented in SeDuMi.  The runtime of any method
   could be reduced significantly by implementing it in a completely
   compiled format such as {\tt{C}}.  Note: OMP is not evaluated
   because its performance always falls in between that of MP and
   LSMP\@.

\subsection{Sparsity Profile Estimation in a Noiseless Setting}
\label{subsec:e1}

   \subsubsection{Overview} We now evaluate how well the algorithms of
   Sec.~\ref{sec:algorithms} estimate sparsity profiles when the
   underlying s are each strictly and simultaneously
   -sparse and the observation  of (\ref{mmv2_eq1}) is
   known exactly and not corrupted by noise.  This corresponds to a
   high-SNR source localization scenario where the sparsity profile
   indicates locations of emitters and our goal is to find the
   locations of these emitters \cite{Joh1993, Kri1996, Mal2003,
   Mal2005}.  Our goal is to get an initial grasp of the challenges of
   solving the MSSO problem.

   We synthetically generate real-valued sets of s and
   s in (\ref{mmv2_eq1}), apply the algorithms, and record
   the fraction of correct sparsity profile entries recovered by each.
   We vary  in (\ref{mmv2_eq1}) to see how performance at solving
   the MSSO problem varies when the s are underdetermined
   vs.~overdetermined and also vary  to see how rapidly performance
   degrades as more system matrices and vectors are employed.

   \subsubsection{Details}\label{subsubsec:e1} For all trials, we fix
    in (\ref{mmv2_eq1}) and , which means each 
   vector consists of thirty elements, three of which are nonzero.  We
   consider , and .  For each of the fifty-six fixed  pairs, we create
   50 random instances of (\ref{mmv2_eq1}).  Each of the 2,800
   instances is constructed and evaluated as follows:
   \begin{adamItemize2}

   \item Pick a -element subset of  uniformly
   at random.  This is the sparsity profile.

   \item Create  total -element vectors, the s.  The
    elements of each that correspond to the sparsity profile are
   filled in with draws from a Gaussian 
   distribution; all other elements are set to zero.

   \item Create  total  matrices, the s.
   Each element of each matrix is determined by drawing from
   ; each column of each matrix is normalized to
   have unit  energy.

   \item Compute . Shuffle
   s and s into s and s via
   (\ref{C_n}, \ref{h_n}).

   \item Apply the algorithms:

       \begin{adamItemize2}

       \item[] MP, LSMP: iterate until  elements are chosen
       or the residual approximation is .  If less than 
       terms are chosen, this hurts the recovery score.

       \item[] IRLS, RBRS, CBCS, SOCP: approximate a 
       oracle: proxy for a good choice of  by looping over
       roughly seventy s in , running the given
       algorithm each time.  This sweep over  results in
       high-energy, dense solutions through negligible-energy,
       all-zeros solutions.  For each of the estimated
       s (that vary with ),
       estimate a sparsity profile by noting the largest 
       energy rows of the associated 
       matrix.\footnote{For example, if the true sparsity profile is
        and the largest  energy rows of
        are , then the fraction of
       recovered sparsity profile terms equals .  Now
       suppose only two rows of  have nonzero energy
       and the profile estimate is only .  The fraction
       recovered is now zero.} Remember the highest fraction
       recovered across all s.

       \end{adamItemize2}

   \end{adamItemize2}
   After performing the above steps, we average the results of the 50
   trials associated with each fixed  to yield the average
   fraction of recovered sparsity profile elements.

   \subsubsection{Results} Each subplot of Fig.~\ref{fig:e1_afr}
   depicts the average fraction of recovered sparsity profile elements
   versus the number of knowns, , for a fixed value of ,
   revealing how performance varies as the  matrices go from being underdetermined to
   overdetermined.

\begin{figure}
     \begin{center} \small \begin{tabular}{ccc}
       \epsfig{figure=fig_e1_P1.eps,width=\widthC} &
       \epsfig{figure=fig_e1_P2.eps,width=\widthC} &
       \epsfig{figure=fig_e1_P3.eps,width=\widthC} 
     \end{tabular}

     \begin{tabular}{cccc}
       \epsfig{figure=fig_e1_P4.eps,width=\widthC} &
       \epsfig{figure=fig_e1_P5.eps,width=\widthC} &
       \epsfig{figure=fig_e1_P6.eps,width=\widthC}
     \end{tabular}

     \begin{tabular}{cc}
       \epsfig{figure=fig_e1_P7.eps,width=\widthC} &
       \epsfig{figure=fig_e1_P8.eps,width=\widthC}
     \end{tabular}

     \caption{{\bf{Sparsity Profile Estimation in a Noiseless
     Setting}.}  Subplots depict average fraction of sparsity profile
     elements recovered over 50 trials of six algorithms as  is
     varied.   is fixed per subplot, and  and  for all
     trials.  Data is generated as described in
     Sec.~\ref{subsubsec:e1}.  Recovery scores for IRLS, RBRS, CBCS,
     and SOCP assume a good choice of  is known. For large
     , all algorithms exhibit high recovery rates; for large ,
     small , or both, the algorithms that seek to minimize
     (\ref{mmv2_eq2}, \ref{mmv2_eq4}) generally outperform those that
     greedily pursue a solution.}

     \label{fig:e1_afr}
     \end{center}
   \end{figure}


   {\em{Recovery Trends}.} As the number of knowns  increases,
   recovery rates improve substantially, which is sensible.  For large
    and small , the six algorithms behave similarly,
   consistently achieving nearly 100\% recovery.  For large  and
   moderate , however, sparsity profile recovery rates are
   dismal---as  increases, the underlying simultaneous sparsity of
   the s is not enough to combat the increasing number of
   unknowns, .  As  is decreased and especially when  is
   increased, the performance of the greedy techniques falls off
   relative to that of IRLS, RBRS, CBCS, and SOCP, showing that the
   convex relaxation approach itself is a sensible way to
   approximately solve the formal NP-Hard combinatorial MSSO
   simultaneous sparsity problem.  Furthermore, the behavior of the
   convex algorithms relative to the greedy ones coincides with the
   studies of greedy vs.~convex programming sparse approximation
   methods in single-vector \cite{Che1998, Cot1999} and SSMO contexts
   \cite{Cot2005}.  Essentially, in contrast with convex programming
   techniques, the greedy algorithms only look ahead by one term,
   cannot backtrack on sparsity profile element choices, and do not
   consider updating multiple rows of unknowns of the  matrix
   at the same time.  LSMP tends to perform slightly better than MP
   because it solves a least squares minimization and explicitly
   considers earlier chosen rows whenever it seeks to choose another
   row of .

   {\em{Convergence}.} Across most trials, IRLS, RBRS, CBCS, and SOCP
   converge rapidly and do not exceed the maximum limit of 500 outer
   iterations.  The exception is CBCS when  is small and :
   here, the objective function frequently fails to decrease by less
   than the specified .

   {\em{Runtimes}.} For several fixed  pairs,
   Table~\ref{tab:r1} lists the average runtimes of each algorithm
   across the 50 trials associated with each pair.\footnote{In the
   interest of space we do not list average runtimes for all fifty-six
    pairs.}  For IRLS, RBRS, CBCS, and SOCP, runtimes are also
   averaged over the many  runs.  Among the convex
   minimization methods, SOCP seems superior given its fast runtimes
   in three out of four cases.  Peak memory usage is not tracked here
   because it is difficult to do so when using MATLAB for such small
   problems; it will be tracked during the third experiment where the
   system matrices are vastly larger and differences in memory usage
   across the six algorithms are readily apparent.
   \begin{table}
   \begin{center}
   \small
   \begin{tabular}{|l|r|r|r|r|}
   \hline
                          & \multicolumn{4}{c|}{} \\ \hline
       {\bf{Algorithm}}     & (10,8) & (20,1) & (30,5) & (40,8) \\ \hline
    MP                    & 5.4      & 1.8     & 2.6  & 4.0  \\
    LSMP                  & 11.4     & 5.6     & 15.6 & 27.6 \\
    IRLS  & 92.6     & 10.1    & 73.2  & 175.0 \\
    RBRS  & 635.7    & 36.0    & 236.8 & 401.6 \\
    CBCS  & 609.8    & 7.1     & 191.4 & 396.3 \\
    SOCP  & 44.3     & 37.0    & 64.3  & 106.5 \\ \hline
   \end{tabular}
   \end{center}

   \caption{{\bf{Average Algorithm Runtimes for Noiseless Sparsity
   Profile Estimation.}} For several fixed  pairs, each
   algorithm's average runtime over the corresponding 50 trials is
   given in units of milliseconds;  and  for all trials
   (runtimes of the latter four algorithms are also averaged over the
   multiple  runs per trial).  MP is substantially faster
   than the other techniques, as expected.  For larger problems,
   e.g. , the runtimes of both RBRS and CBCS are
   excessive relative to those of the other convex minimization
   techniques, IRLS and SOCP\@.}  \label{tab:r1}

   \end{table}

   {\em{Closer Look: Solution Vectors}.}  We now observe how the
   algorithms that seek to minimize the convex objective behave during
   the 43rd trial when , , , and , corresponding to the base case problem of estimating one
   sparse real-valued vector, .  Fig.~\ref{fig:e1_zoom}
   illustrates estimates obtained by SOCP, CBCS, RBRS, and IRLS when
   ; for each algorithm, a subplot shows elements of
   both the estimated and actual , and lists the estimated
   sparsity profile (ESP), number of profile terms recovered, and
   value of the objective function given in (\ref{mmv2_eq2},
   \ref{mmv2_eq4}).  Although RBRS, CBCS, and SOCP yield slightly
   different solutions (among which SOCP yields the best profile
   estimate), they all yield an objective function equal to
   .  Convex combinations of the three solutions
   continue to yield the same value, suggesting that the three
   algorithms have found solutions among a convex set that is the
   global solution to the objective posed in (\ref{mmv2_eq2},
   \ref{mmv2_eq4}).  Given the fact that in this case SOCP outperforms
   RBRS and CBCS, we see that even the globally optimal solution to
   the relaxed convex objective does not necessarily optimally solve
   the true -sparse profile recovery problem.  In contrast to the
   other methods, IRLS yields a slightly higher objective function
   value, 0.030, and its solution vector is not part of the convex
   set---it does however correctly determine 2 of the 3 terms of the true
   sparsity profile.

\begin{figure}
      \begin{center}
      \small
      \begin{tabular}{cc}
        \epsfig{figure=fig_e1_zoom_socp.eps,width=2in} &
        \epsfig{figure=fig_e1_zoom_cbcs.eps,width=2in}
      \end{tabular}

      \begin{tabular}{cc}
        \epsfig{figure=fig_e1_zoom_rbrs.eps,width=2in} &
        \epsfig{figure=fig_e1_zoom_irls.eps,width=2in}
      \end{tabular}

      \caption{{\bf{Noiseless Sparsity Profile Estimation with IRLS,
      RBRS, CBCS, SOCP}.}  Here , and .  The
      algorithms are applied with  fixed at 0.03 and attempt
      to estimate the single unknown vector, , along with
      the sparsity profile.  Subplots depict the elements of both the
      estimated and actual , along with the estimated
      sparsity profile (ESP), number of profile terms recovered, and
      objective function value.  SOCP leads to a superior sparsity
      profile estimate, and SOCP, RBRS, and CBCS seem to minimize the
      convex objective given in (\ref{mmv2_eq2}, \ref{mmv2_eq4}).
      IRLS does not, but still manages to properly identify 2 out of 3 sparsity
      profile terms.}

      \label{fig:e1_zoom}
      \end{center}
   \end{figure}


   {\em{Closer Look: Objective Function Behavior}.}  Concluding the
   experiment, Fig.~\ref{fig:e1_objfun} plots the objective
   vs.~ for the 25th trial when  and ,
   studying how the objective (\ref{mmv2_eq2}, \ref{mmv2_eq4}) varies
   with  when applying SOCP, CBCS, RBRS, and IRLS\@.  For all
   seventy values of , SOCP, CBCS, and RBRS
   generate solutions that yield the same objective function value.
   For , IRLS attains the same objective
   function value as the other methods, but as  increases,
   IRLS is unable to minimize the objective function as well as SOCP,
   RBRS, and CBCS\@.  The behavior in Fig.~\ref{fig:e1_objfun} occurs
   consistently across the fifty trials of the other  pairs.

\begin{figure}\centering
      \epsfig{figure=fig_e1_objFun.eps,width=3.0in}

      \caption{{\bf{Noiseless Sparsity Profile Estimation: Objective
      Function Behavior}.} For the  trial of the
       series, SOCP, CBCS, RBRS, and IRLS are used to
      solve (\ref{mmv2_eq2}, \ref{mmv2_eq4}) for 70 values of ; the value of the objective function vs.~
      is given above.  For , IRLS's solutions
      do not minimize the objective as well as those produced by the
      three other methods.}

      \label{fig:e1_objfun}
   \end{figure}


\subsection{Sparsity Profile Estimation in the Presence of Noise}
\label{subsec:e2}

   \subsubsection{Overview} We now evaluate how well the algorithms of
   Sec.~\ref{sec:algorithms} estimate sparsity profiles when the
   underlying s are each strictly and simultaneously
   -sparse and the observation  of (\ref{mmv2_eq1}) is
   corrupted by additive white Gaussian noise.  The
   signal-to-noise ratio (SNR) and  are varied across sets of Monte
   Carlo trials in order to gauge algorithm performance across many
   scenarios.  For a given trial with a fixed SNR level in units of
   decibels (dB), the  elements of the true observation vector,
   , are corrupted with independent and
   identically distributed (i.i.d.)  zero-mean Gaussian noise with
   variance , related to the SNR as follows:
   
   This noise measure is analogous to that of \cite{Cot2005}.
    
   \subsubsection{Details}\label{subsubsec:e2}
    We fix , , and , and consider  and .
   For each fixed  pair, we generate 100 noisy
   observations and apply the algorithms as follows:
   \begin{adamItemize2}

     \item Generate the sparsity profile, s, s,
     s, and s as in Sec.~\ref{subsubsec:e1}.  The
     s are simultaneously -sparse and all terms are
     real-valued.

     \item Compute .

     \item Construct  where  and
      is given by (\ref{snr}).

     \item Apply the algorithms by providing them with
      and the system matrices:

        \begin{adamItemize2}

        \item[] MP, LSMP: iterate until  elements are
        chosen or the residual approximation is .  If less
        than  terms are chosen, this hurts the recovery score.

        \item[] IRLS, RBRS, CBCS, SOCP: using a pre-determined
        {\em{fixed}}  (see below), apply each algorithm to
        obtain estimates of the unknown vectors and sparsity profiles.

        \end{adamItemize2}

   \end{adamItemize2} After performing the above steps, we average the
   results of the 100 trials associated with each fixed  triplet to yield the average fraction of sparsity
   profile elements that each algorithm recovers.

   {\em{Control Parameter Selection}.} The  mentioned in the
   list above is determined as follows: before running the overall
   experiment, we generate three noisy observations for each
    pair.  We then apply IRLS, RBRS, CBCS, and SOCP,
   tuning the control parameter  by hand until finding a
   single value that produces reasonable solutions.  All algorithms
   then use this hand-tuned, fixed  and are applied to the
   other 100 noisy observations associated with the 
   pair under consideration.  Thus, in distinct contrast to the
   noiseless experiment, we no longer assume an ideal  is
   known for each denoising trial.

   \subsubsection{Results}\label{subsubsec:e2_res} Each subplot of
   Fig.~\ref{fig:e2_K} depicts the average fraction of recovered
   sparsity profile elements versus SNR for a fixed , revealing how
   well the six algorithms are able to recover the  elements of the
   sparsity profile amidst noise in the observation.  Each data point
   is the average fraction recovered across 100 trials.

   {\em{Recovery Trends}.} When , we see from the upper-left
   subplot of Fig.~\ref{fig:e2_K} that all algorithms have essentially
   equal performance for each SNR\@.  Recovery rates improve
   substantially with increasing SNR, which is sensible.  For each
   algorithm, we see across the subplots that performance generally
   decreases with increasing ; in other words, estimating a large
   number of sparsity profile terms is more difficult than estimating
   a small number of terms.  This trend is evident even at high SNRs.
   For example, when SNR is 30 dB and , SOCP is only able to
   recover  of sparsity profile terms.  When , the
   recovery rate falls to .  For low SNRs, e.g., -5 dB, all
   algorithms tend to perform similarly, but the greedy algorithms
   perform increasingly worse than the others as  goes from
   moderate-to-large and SNR surpasses zero dB\@.  In general, MP
   performs worse than LSMP, and LSMP in turn performs worse than
   IRLS, SOCP, RBRS, and CBCS, while the latter four methods exhibit
   essentially the same performance across all SNRs and s.  For
   , MP's performance falls off relative to IRLS, SOCP, RBRS,
   and CBCS, but LSMP's does not.  As  transitions from 3 to 5,
   however, LSMP performs as badly as MP at low SNRs, but its
   performance picks up as SNR increases.  As  continues to
   increase beyond 5, LSMP's performance is unable to surpass that of
   MP, even when SNR is large.  Overall, Fig.~\ref{fig:e2_K} shows
   that convex programming algorithms are superior to greedy methods
   when estimating sparsity profiles in noisy situations; this
   coincides with data collected in the noiseless experiment in
   Sec.~\ref{subsec:e1}, as well as the empirical findings of
   \cite{Cot1999, Cot2005}.

\begin{figure}
   \begin{center}
   \small
     \begin{tabular}{ccc}
       \epsfig{figure=fig_e2_K1.eps,width=\widthC} &
       \epsfig{figure=fig_e2_K3.eps,width=\widthC} &
       \epsfig{figure=fig_e2_K5.eps,width=\widthC}
     \end{tabular}

     \begin{tabular}{c}
       \epsfig{figure=fig_e2_Klegend.eps,width=2.5in}
     \end{tabular}

     \begin{tabular}{cc}
       \epsfig{figure=fig_e2_K7.eps,width=\widthC} &
       \epsfig{figure=fig_e2_K9.eps,width=\widthC}
     \end{tabular}

     \caption{{\bf{Sparsity Profile Estimation in the Presence of
     Noise}.}  Each subplot depicts the average fraction of recovered
     sparsity profile elements versus SNR for a fixed , revealing
     how well the algorithms recover the  elements of the sparsity
     profile amidst noise in the observation.  Each data point is the
     average fraction recovered across 100 trials; data is randomly
     generated as described in Sec.~\ref{subsubsec:e2}.   and
      are always fixed at 30, 25, and 3, respectively. For each
      pair, a ``good'' lambda is chosen by denoising
     a few cases by hand and then using this fixed  for 100
     fresh denoising trials.  Performance degrades with increasing 
     and decreasing SNR\@.  For large , the greedy algorithms perform
     worse than IRLS, SOCP, RBRS, and CBCS, whereas the latter four
     methods perform essentially identically across all
      combinations.}

     \label{fig:e2_K} 
     \end{center}
   \end{figure}


   {\em{Convergence}.} For many denoising trials, CBCS typically
   requires more iterations than the other techniques in order to
   converge.  At times, it fails to converge to within the specified
   , similarly to how it behaves during the
   noiseless experiment of Sec.~\ref{subsec:e1}.

   {\em{Runtimes}.} Across all denoising trials, MP, LSMP, IRLS, RBRS,
   CBCS, SOCP have average runtimes of 3.1, 25.1, 57.2, 247.0, 148.5,
   and 49.2 milliseconds.  It seems SOCP is best for denoising given
   that it is the fastest algorithm among the four methods that
   outperforms the greedy ones.  IRLS is nearly as fast as SOCP and
   thus is a close second choice for sparsity profile estimation.

   {\em{Closer Look: Mean Square Errors of Convex Minimization Methods
   Before and After Estimating the Sparsity Profile and Retuning the
   Solution}.}  Now let us consider the th trial of the
    pair.  We do away with the
   fixed  assumption and now assume we care (to some extent)
   not only about estimating the sparsity profile, but the true
   solution  as well.  To proxy for this, we study
   how the mean square errors (MSEs) of solutions generated by IRLS,
   SOCP, RBRS, and CBCS behave across  before and after
   identifying the sparsity profile and retuning the solution.
   Figure~\ref{fig:e2_zoom} depicts the results of this investigation.

   Running each algorithm for a particular  yields a solution
   .  The left
   subplot simply illustrates the MSEs of the
   s with respect
   to the true solution. Among SOCP, RBRS, CBCS, and IRLS, only the
   last is able to determine solutions with MSEs less than unity
   (consider the IRLS error curve for ).

   Consider now retuning each of the
   s as follows:
   unstack each into  for  and then remember the  vectors whose
    energies are largest, yielding an estimate of the
   -element sparsity profile.  Let these estimated indices be
   .  Now, generate a {\em{retuned solution}} by
   using the  matrices associated with the estimated sparsity
   profile and solving  for .  This latter vector consists of  elements and
   by unstacking it we obtain a retuned estimate of the
   s, e.g.,
    equals the first 
   elements of , and so forth, while the other
   s for  are now simply all-zeros vectors.  Reshuffling the
   retuned s yields
   s that are strictly and
   simultaneously  sparse whose weightings yield the best match to
   the noisy observation in the  sense.  Unlike the original
   solution estimate, which is not necessarily simultaneously
   -sparse, here we have enforced true simultaneous -sparsity.
   We may or may not have improved the MSE with respect to the true
   solution: for example, if we have grossly miscalculated the
   sparsity profile, the MSE of the retuned solution is likely to
   increase substantially, but if we have estimated the true sparsity
   profile exactly, then the retuned solution will likely be quite
   close (in the  sense) to the true solution, and MSE will
   thus decrease.
   
   The MSEs of these {\em{retuned solutions}} with respect to the true
    are plotted in the right subplot of
   Fig.~\ref{fig:e2_zoom}.  For all algorithms and s, MSE has
   increased relative to the left subplot, which means that in every
   case our estimate of the true underlying solution has worsened.
   This occurs because across all algorithms and s in
   Fig.~\ref{fig:e2_zoom}, the true -term sparsity profile is
   incorrectly estimated and thus the retuning step makes the
   estimated solution worse.  The lesson here is that if one is
   interested in minimizing MSE in low-to-moderate SNR regimes it may
   be best to simply keep the original estimate of the solution rather
   than detect the sparsity profile and retune the result.  If one is
   not certain that the sparsity profile estimate is accurate,
   retuning is likely to increase MSE by fitting the estimated
   solution weights to an incorrect set of generating matrices.  On
   the other hand, if one is confident that the entire sparsity
   profile will be correctly identified with sufficiently high
   probability, retuning will be beneficial; see \cite{Ela2008,
   Goy2008, Fle2006} for related ideas.

\begin{figure}
   \begin{center}
   \small
     \begin{tabular}{cc}
       \epsfig{figure=e2_zoom_mse_pre.eps,width=2.5in} &
       \epsfig{figure=e2_zoom_mse_post.eps,width=2.5in}
     \end{tabular}

     \caption{{\bf{MSEs of Convex Minimization Methods Before and
     After Estimating the Sparsity Profile and Retuning the
     Solution}.}  Here MSE vs.~ is studied during the
      trial of the 
     denoising series.  Fixing  and applying a given
     algorithm yields the solution
     .  Left subplot: MSEs of
     the s vs.~the true solution
     .  Right subplot: MSEs of the solution
     estimates after they undergo retuning to be strictly and
     simultaneously -sparse.  (Sec.~\ref{subsubsec:e2_res} outlines
     the retuning process.) For all algorithms and s, MSE
     increases substantially relative to the left subplot.  None of
     the methods correctly estimate the true -term sparsity profile
     and thus the retuning step causes the estimated solution to
     branch further away (in the MSE sense) from the actual one.}

     \label{fig:e2_zoom} 
   \end{center}
   \end{figure}


\subsection{MRI RF Excitation Pulse Design}
\label{subsec:e3}

   \subsubsection{Overview} For the final experiment we study how well
   the six algorithms design MRI RF excitation pulses.  In the
   interest of space and because the conversion of the physical
   problem into an MSSO format involves MRI physics and requires
   significant background, we only briefly outline how the system
   matrices arise and why simultaneously sparse solutions are
   necessary.  A complete formulation of the problem for engineers and
   mathematicians is given in \cite{Zel2008_CISS}; MRI pulse designers
   may refer to \cite{Zel2008_TMI}.

   \subsubsection{Formulation} For the purposes of this paper, the
   design of an MRI RF excitation pulse reduces to the following
   problem: assume we are given  points in the 2-D spatial domain,
   , along with  points in a 2-D
   ``Fourier-like'' domain, .  Each
    equals , a point in space, while
   each  equals , a point in
   the Fourier-like domain, referred to as ``-space''.  The
   s and s are in units of centimeters (cm) and
   inverse centimeters (), respectively.  The
   s are Nyquist-spaced relative to the sampling of the
   s and may be visualized as a 2-D grid located at low
    and  frequencies (where ``'' denotes the frequency
   domain axis that corresponds to the spatial  axis).  Under
   reasonable assumptions, energy placed at one or more points in
   -space produces a pattern in the spatial domain; this pattern is
   related to the -space energy via a ``Fourier-like'' transform
   \cite{Pau1989}.  Assume we place an arbitrary complex weight  (i.e., both a magnitude and phase) at each of the
    locations in -space.  Let us represent these weights using a
   vector .
   In an ideal (i.e., physically-unrealizable) setting, the following
   holds:
   
   where  is a known dense Fourier
   matrix\footnote{Formally, ,
   where  and  is a known lumped gain
   constant.} and the th element of  is
   the image that arises at , denoted , due to
   the energy deposition along the  points on the -space grid as
   described by the weights in the  vector.
   
   The goal now is to form a desired (possibly complex-valued)
   spatial-domain image  at the given set of spatial domain
   coordinates (the s) by placing energy at some of the
   given  locations while obeying a special constraint on
   how the energy is deposited.  To produce the spatial-domain image,
   we will use a ``-channel MRI parallel excitation system''
   \cite{Kat2003, Set2006}---each of the system's  channels is able
   to deposit energy of varying magnitudes and phases at the -space
   locations and is able to influence the resulting spatial-domain
   pattern  to some extent. Each channel  has a known
   ``profile'' across space, , that
   describes how the channel is able to influence the magnitude and
   phase of the resulting image at different spatial locations.  For
   example, if , then the 3rd channel is unable to
   influence the image that arises at location , regardless
   of how much energy it deposits along .
   The special constraint mentioned above is as follows: {\em{the
   system's channels may only visit a small number of points in
   -space---they may only deposit energy at  locations.}}

   We now finalize the formulation of problem: first, we construct 
   diagonal matrices  such that
   .  Now we assume that each channel
   deposits arbitrary energies at each of the  points in -space
   and describe the weighting of the -space grid by the th channel with the vector
   .  Based on the physics of the
   -channel parallel excitation system, the overall image
    that forms is the {\em{superposition}} of the
   profile-scaled subimages produced by each channel:
   
   where .
   Essentially, (\ref{mri2}) is the real-world version of (\ref{mri1})
   for -channel systems with profiles  that are
   not constant across \la{r}.

   Recalling that our overall goal is to deposit energy in -space
   to produce the image , and given the special constraint that we
   may only deposit energy among a small subset of the  points in
   -space, we arrive at the following problem:
   
   where  and  is given by
   (\ref{mri2}).  That is, we seek out  locations in -space
   at which to deposit energy to produce an image  that is
   close in the  sense to the desired image .
   Strictly and simultaneously -sparse s are the
   only valid solutions to the problem.

   One sees that (\ref{mri3}) is precisely the MSSO system given in
   (\ref{msso_nphard}) and thus the algorithms posed in
   Sec.~\ref{sec:algorithms} are applicable to the pulse design
   problem.  In order to apply the convex minimization techniques
   (IRLS, SOCP, RBRS, and CBCS) to this problem, the only additional
   step needed is to retune any given solution estimate
    into a strictly
   and simultaneously -sparse set of vectors; this retuning step is
   computationally tractable and was described in
   Sec.~\ref{subsubsec:e2_res}'s ``{\em{Closer Look}}'' subsection.

   {\em{Aside}.} An alternative approach to decide where to place
   energy at  locations in -space is to compute the Fourier
   transform of  and decide to place energy at 
   frequencies where the Fourier coefficients are largest in magnitude
   \cite{Yip2006}.  This method does yield valid -sparse energy
   placement patterns, but empirically it is always outperformed by
   the convex minimization approaches \cite{Zel2007, Zel2008_CISS,
   Zel2008_TMI} so we do not delve into the Fourier-based method in
   this paper.



   \subsubsection{Experimental Setup} Using an eight-channel system
   (i.e., ) whose profile magnitudes (the s) are
   depicted in Fig.~\ref{fig:e3_profiles}, we will design pulses to
   produce the desired complex-valued image shown in the left subplot
   of Fig.~\ref{fig:e3_tgt}.  We sample the spatial  domain at
    locations within the region where at least one of the
   profiles in Fig.~\ref{fig:e3_profiles} is active---this region of
   interest is referred to as the {\em{field of excitation}} (FOX) in MRI
   literature.\footnote{Sampling points outside of the FOX where no
   profile has influence is unnecessary because an image can never
   be formed at these points no matter how much energy any given
   channel places throughout -space.} The spatial samples are
   spaced by 0.8 cm along each axis and the FOX has a diameter of
   roughly 20 cm.  Given our choice of , we sample the s and  and
   construct the s and .  Next, we define a grid of
    points in -space that is  in size
   and extends outward from the -space origin.  The points are
   spaced by  along each -space axis
   and the overall grid is shown in the right subplot of
   Fig.~\ref{fig:e3_tgt}.  Finally, because we know the 356
   s and 225 s, we construct the 
   matrix  in (\ref{mri1}, \ref{mri2}) along with the
   s in (\ref{mri2}).  We now have all the data we need to
   apply the algorithms and determine simultaneously -sparse
   s that (approximately) solve (\ref{mri3}).

\begin{figure}
   \begin{center}
   \small
      \begin{tabular}{c}
         \epsfig{figure=profiles.eps,width=3.75in}
      \end{tabular}

      \caption{{\bf{Profile Magnitudes of the Eight-Channel Parallel
      Excitation MRI System}.} Here the magnitudes of the
      s are depicted for ; 356 samples
      of each  are taken within the nonzero region of
      influence and stacked into the diagonal matrix  used
      in (\ref{mri2}). Across space, the s are not
      orthogonal---their regions of influence overlap each other to
      some extent.}

      \label{fig:e3_profiles} 
   \end{center}
   \end{figure}


\begin{figure}
   \begin{center}
   \small
     \begin{tabular}{c}
       \epsfig{figure=excit.eps,width=3.25in}
     \end{tabular}

     \caption{{\bf{Desired Image and -Space Grid}.} Left image:
     desired complex-valued image, .  Medium-gray region =
     FOX; other regions denote locations where we want image to be
     nonzero with the given magnitudes and phases.  Sampling
      at the 356 locations within the FOX allows us to
     construct \la{d} in (\ref{mri2}). Right subplot: 
     grid of  candidate -space locations, , at which the  channels may deposit energy and
     thus influence the resulting image.  The physical constraints of
     the MRI excitation process force us to place energy at only a
     small number of grid locations.}

     \label{fig:e3_tgt} 
   \end{center}
   \end{figure}


   We apply the algorithms and evaluate designs where the use of  candidate points in -space is permitted
   (in practical MRI scenarios,  up to 30 is permissible).
   Typically, the smallest  possible that produces a version of
    to within some -fidelity is the design that the
   MRI pulse designer will use on a real system.

   To obtain simultaneously -sparse solutions with MP and LSMP, we
   set , run each algorithm once, remember the ordered list of
   chosen indices, and back out every solution for  through
    via the retuning technique of Sec.~\ref{subsubsec:e2_res}.

   For each convex minimization method (IRLS, SOCP, RBRS, and CBCS),
   we apply the following procedure: first, we run the algorithm for
   14 values of , storing
   each resulting solution,
   .  Then for fixed
   , to determine a simultaneously -sparse deposition of energy
   on the -space grid, we apply the retuning process of
   Sec.~\ref{subsubsec:e2_res} to each of the 14 solutions, obtaining
   14 strictly simultaneously -sparse retuned sets of solution
   vectors, .
   The one solution among the 14 that best minimizes the 
   error between the desired and resulting images, , is chosen as the solution for the  under
   consideration.  Essentially, we again assume we know a good value
   for  when applying each of the convex minimization
   methods.  To attempt to avoid convergence problems, RBRS and CBCS
   are permitted 5,000 and 10,000 maximum outer iterations,
   respectively (see below).

   \subsubsection{Results} 

   {\em{Image  Error vs.~Number of Energy Depositions in -Space}.}
   Figure~\ref{fig:e3_L2}'s left subplot shows the  error
   versus  curves for each algorithm.  As  is increased, each
   method produces images with lower  error, which is
   sensible: depositing energy at more locations in -space gives
   each technique more degrees of freedom with which to form the
   image.  In contrast to the sparsity profile estimation experiments
   in Sec.~\ref{subsec:e1} and Sec.~\ref{subsec:e2}, however, here
   LSMP is the best algorithm: for each fixed  considered in
   Fig.~\ref{fig:e3_L2}, the LSMP technique yields a simultaneously
   -sparse energy deposition that produces a higher-fidelity image
   than all other techniques.  For example, when  LSMP yields a
   solution that leads to an image with  error of 3.3.  In
   order to produce an image with equal or better fidelity, IRLS,
   RBRS, and SOCP need to deposit energy at  points in
   -space, and thus produce less useful designs from an MRI
   perspective.  CBCS fares the worst, needing to deposit energy at
    grid points in order to compete with the fidelity of LSMP's
    image.

\begin{figure}
   \begin{center}
   \small
       \begin{tabular}{cc}
         \epsfig{figure=L2_vs_K.eps,width=2.5in} &
         \epsfig{figure=objFun_vs_lambda.eps,width=2.5in}
       \end{tabular}

       \caption{{\bf{MRI Pulse Design Results}.} Left subplot:
        error vs.~ is given for MP, LSMP, IRLS, RBRS,
       CBCS, and SOCP\@.  For fixed , LSMP consistently outperforms
       the other algorithms.  Right subplot: objective function values
       vs.  when SOCP, CBCS, RBRS, and IRLS attempt to
       minimize (\ref{mmv2_eq2}, \ref{mmv2_eq4}).  SOCP and IRLS
       converge and seem to minimize the objective; RBRS does so as
       well for most s.  CBCS routinely fails to converge
       even after 10,000 iterations and thus its solutions yield
       extremely large objective function values.}

       \label{fig:e3_L2} 
   \end{center}
   \end{figure}


   {\em{Closer Look: Objective Function vs.~}.} The right
   subplot of Fig~\ref{fig:e3_L2} shows how well the four convex
   minimization algorithms minimize the objective function
   (\ref{mmv2_eq2}, \ref{mmv2_eq4}) before retuning any solutions and
   enforcing strict simultaneous -sparsity. For each fixed
   , SOCP and IRLS find solutions that yield the same
   objective function value.  RBRS's solutions generally yield
   objective function values equal to those of SOCP and IRLS, but at
   times lead to higher values: in these cases RBRS almost converges
   but does not do so completely.  Finally, for most s CBCS's
   solutions yield extremely large objective function values; in these
   cases CBCS completely fails to converge.

   {\em{Closer Look: Objective Function Convergence for }.}  The right subplot of Fig~\ref{fig:e3_L2} shows that for
   , IRLS, SOCP, RBRS, and CBCS generate solutions that
   yield the same objective function value, suggesting that each
   method succeeds at minimizing the objective function.
   Figure~\ref{fig:e3_objFun_vs_iters} illustrates how the algorithms
   converge in this specific case: each subplot tracks the value of an
   algorithm's objective function as it iterates.  Subplots along the
   top row all have the same  axis, giving a close look at how the
   algorithms behave.  The two subplots along the bottom row ``zoom
   out'' along the  axis to show RBRS's and CBCS's total behavior.
   IRLS and SOCP converge rapidly, within 4 and 19 iterations,
   respectively.  RBRS requires roughly 150 outer iterations, while
   CBCS requires nearly 10,000.

\begin{figure}
   \begin{center}
   \small
       \begin{tabular}{ccc}
         \epsfig{figure=irls_socp_objFun_vs_iters.eps,width=\widthC} &
         \epsfig{figure=rbrs_objFun_vs_iters_zoom.eps,width=\widthC} &
         \epsfig{figure=cbcs_objFun_vs_iters_zoom.eps,width=\widthC} 
         \\ &
         \epsfig{figure=rbrs_objFun_vs_iters_total.eps,width=\widthC} &
         \epsfig{figure=cbcs_objFun_vs_iters_total.eps,width=\widthC}
       \end{tabular}

       \caption{{\bf{Convergence Behavior of IRLS, SOCP, RBRS, CBCS}.}
       Each subplot illustrates the value of each algorithm's
       objective function (\ref{mmv2_eq2}, \ref{mmv2_eq4}) as the
       algorithm iterates.  Upper row subplots are scaled to have the
       same  axis, whereas the bottom row subplots are ``zoomed
       out'' to illustrate the overall behavior of RBRS and CBCS\@.
       IRLS and SOCP converge rapidly, within 4 and 19 iterations,
       respectively.  RBRS and CBCS require roughly 150 and 10,000
       iterations, respectively.  The runtimes of IRLS, SOCP, RBRS,
       and CBCS in this case are 29, 121, 450, and 5,542 seconds.}

       \label{fig:e3_objFun_vs_iters} 
   \end{center}
   \end{figure}


   {\em{Runtimes and Peak Memory Usage}.} Setting , we run MP
   and LSMP and record the runtime of each.  Across the 14 s,
   IRLS, RBRS, CBCS, and SOCP's runtimes are recorded and averaged.
   The peak memory usage of each algorithm is also noted; these
   statistics are presented in Table~\ref{tab:r3}.  In distinct
   contrast to the smaller-variable-size experiments in
   Sec.~\ref{subsec:e1} and Sec.~\ref{subsec:e2} where all four convex
   minimization methods have relatively short runtimes (under one
   second), here RBRS and CBCS are much slower, leaving IRLS and SOCP
   as the only feasible techniques among the four.  Furthermore, while
   LSMP does indeed outperform IRLS and SOCP on an  error
   basis (as shown in Fig.~\ref{fig:e3_L2}), the runtime statistics
   here show that LSMP requires order-of-magnitude greater runtime to
   solve the problem---therefore, in some real-life scenarios where
   designing pulses in less than 5 minutes is a necessity, IRLS and
   SOCP are superior.  Finally, in contrast to Sec.~\ref{subsec:e1}'s
   runtimes given in Table~\ref{tab:r1}, here IRLS is 1.9 times faster
   than SOCP and uses 1.4 times less peak memory, making it the
   superior technique for MRI pulse design since IRLS's  error
   performance and ability to minimize the objective function
   (\ref{mmv2_eq2}, \ref{mmv2_eq4}) essentially equal that of SOCP\@.
   \begin{table}
   \begin{center}
   \small
   \begin{tabular}{|l|c|c|}
   \hline
    {\bf{Algorithm}}   & {\bf{Runtime}} & {\bf{Peak Memory Usage (MB)}} \\ \hline
    MP               & 11 sec            & 704      \\
    LSMP             & 46 min            & 304      \\
    IRLS             & 50 sec            & 320      \\
    RBRS             & 87 min            & 320      \\
    CBCS             & 3.3 hr            & 320      \\
    SOCP             & 96 sec            & 432      \\ \hline
   \end{tabular}
   \end{center}

   \caption{{\bf{Algorithm Runtimes and Peak Memory Usage for MRI
   Pulse Design.}}  Each algorithm's runtime and peak memory usage is
   listed.  The runtimes of the latter four algorithms are averaged
   over the fourteen s per trial.  MP is again faster than
   the other techniques, but consumes more memory because of its
   precomputation step (see {\em{Algorithm \ref{alg:MP}}}).  IRLS and
   SOCP are quite similar performance-wise and minimize the convex
   objective function equally well (see Fig.~\ref{fig:e3_L2}), but we
   see here that IRLS is approximately 1.9 times faster and uses 1.4
   times less peak memory than SOCP, making the former the superior
   technique among the convex methods.}  \label{tab:r3}

   \end{table}

   {\em{Closer Look: Images and Chosen -Space Locations for
   }.}  To conclude the experiment, we fix  and observe
   the images produced by the algorithms along with the points at
   which each algorithm chooses to deposit energy along the grid of
   candidate points in -space.  Figure~\ref{fig:e3_excits}
   illustrates the images (in both magnitude and phase) that arise due
   to each algorithm's simultaneously 17-sparse set of solution
   vectors,\footnote{Each image is generated by taking the
   corresponding solution , computing  in
   (\ref{mri2}), unstacking the elements of  into ,
   and then displaying the magnitude and phase of .} while
   Fig.~\ref{fig:e3_patterns} depicts the placement pattern chosen by
   each method.  From Fig.~\ref{fig:e3_excits}, we see that each
   algorithm forms a high-fidelity version of the desired image
    given in the left subplot of Fig.~\ref{fig:e3_tgt}, but
   among the images, LSMP's most accurately represents 
   (e.g., consider the sharp edges of the LSMP image's rectangular
   box).  MP's and CBCS's images are noticeably fuzzy relative to the
   others.  The placements in Fig.~\ref{fig:e3_patterns} give insight
   into these performance differences.  Here, LSMP places energy at
   several higher frequencies along the  and  axes, which
   ensures the resulting rectangle is narrow with sharp edges along
   the spatial  and  axes.  In contrast, CBCS fails to place
   energy at moderate-to-high -space frequencies and thus
   cannot produce a rectangle with desirable sharp edges, while MP
   branches out to some extent but fails to utilize high 
   frequencies.  IRLS, RBRS, and SOCP branch out to higher 
   frequencies but not to high  frequencies, and thus their
   associated rectangles in Fig.~\ref{fig:e3_excits} are sharp along
   the  axis but exhibit less distinct transitions (more fuzziness)
   along the spatial  axis.  In general, each algorithm has
   determined 17 locations at which to place energy that yield a
   fairly good image and each has avoided the computationally
   impossible scenario of searching over all -choose-
   (225-choose-17) possible placements.

\begin{figure}
   \begin{center}
   \small
       \begin{tabular}{c}
          \epsfig{figure=excits1_s17.eps,width=4.0in} \\
          \epsfig{figure=excits2_s17.eps,width=4.0in}
       \end{tabular}

       \caption{{\bf{MRI Pulse Design: Images per Algorithm for }.}
       Each algorithm is used to solve the MRI pulse design problem
       using 17 energy depositions along the -space grid,
       attempting to produce an image close to the desired one,
       , given in the left subplot of
       Fig.~\ref{fig:e3_tgt}.  From each set of simultaneously
       17-sparse solution vectors, we calculate the resulting image
       via (\ref{mri2}) and display both its magnitude and phase.
       LSMP's image best resembles the desired one; IRLS's, RBRS's,
       and SOCP's images are nearly as accurate; MP's and CBCS's
       images lack crisp edges, coinciding with their larger 
       errors.}

       \label{fig:e3_excits} 

   \end{center}
   \end{figure}


\begin{figure}
   \begin{center}
   \small
       \begin{tabular}{c}
          \epsfig{figure=patterns_s17.eps,width=4.0in}
       \end{tabular}

       \caption{{\bf{MRI Pulse Design: Energy Deposition Patterns per
       Algorithm for }.}  Each algorithm's placement of energy
       in -space is displayed.  LSMP branches out to moderate 
       frequencies and high  frequencies, partly explaining the
       superiority of its image in Fig.~\ref{fig:e3_excits}.  IRLS,
       RBRS, and SOCP succeed in branching out to higher 
       frequencies but do not place energy at .
       MP and CBCS fail to spread their energy to high spatial
       frequencies, and thus their images in Fig.~\ref{fig:e3_excits}
       lack distinct edges and appear as ``low-pass filtered''
       versions of .}

       \label{fig:e3_patterns} 

   \end{center}
   \end{figure}


\section{Discussion}
\label{sec:discussion}

   \subsection{MRI Pulse Design vs.~Denoising and Source Localization}

   The MRI pulse design problem in Sec.~\ref{subsec:e3} differs
   substantially from the source localization problem in
   Sec.~\ref{subsec:e1}, the denoising experiment in
   Sec.~\ref{subsec:e2}, and other routine applications of sparse
   approximation (e.g. \cite{Don1995, Che1998, Fle2006, Ela2006,
   Cot1999, Cot2005, Mal2005}).  It differs not only in purpose but in
   numerical properties, the latter of which are summarized in
   Table~\ref{tab:diffs}.  While this list will not always hold true
   on an application-by-application basis, it does highlight general
   differences between the two problem classes.\\
   \begin{table}
   \begin{center}
   \small
   \begin{tabular}{|l|l|}
   \hline
    {\bf{MRI Pulse Design}} & {\bf{Denoising and Source Localization}} \\ \hline
     s overdetermined &  s underdetermined  \\
      No concept of noise: given \la{d} is  & 
        Noisy: given \la{d} is not  \\
     Sweep over  useful & 
       Ideal  unknown \\
      Metric:  & 
       Metrics: , and/or \\
        &  fraction of rec.~sparsity profile terms \\ \hline
   \end{tabular}
   \end{center}

   \caption{{\bf{Unique Trends of the MRI Pulse Design Problem}.} This
   table highlights differences between the MRI problem and standard
   denoising and source localization applications.  Items here will
   not always be true, instead providing general highlights about
   each problem class.}
   \label{tab:diffs}
   \end{table}

   \subsection{Merits of Row-by-Row and Column-by-Column Shrinkage}
   Even though LSMP, IRLS, and SOCP tend to exhibit superior
   performance across different experiments in this manuscript, RBRS
   and CBCS are worthwhile because unlike the former methods
   that update all  unknowns concurrently, the shrinkage
   techniques update only a subset of the total variables during each
   iteration.

   For example, RBRS as given in {\em{Algorithm \ref{alg:RBRS}}}
   updates only  unknowns at once, while CBCS as given in
   {\em{Algorithm \ref{alg:CBCS}}} updates but a single scalar at a
   time.  RBRS and CBCS are thus applicable in scenarios where  and
    are exceedingly large and tuning all  parameters during
   each iteration is not possible.  If storing and handling  or  matrices exceeds a system's available memory
   and causes disk thrashing, RBRS and CBCS, though they require far
   more iterations, might still be better options than LSMP, IRLS, and
   SOCP in terms of runtime.

   \subsection{Future Work}

   \subsubsection{Efficient Automated Control Parameter Selection} A
   fast technique for finding ideal values of  is an open
   problem.  It might be useful to investigate several approaches to
   automated selection such as the ``L-curve'' method \cite{Han1994},
   universal parameter selection \cite{Don1994}, and min-max parameter
   selection \cite{Joh1994}.

   \subsubsection{Runtime, Memory, and Complexity Reduction} As noted
   in Sec.~\ref{sec:algorithms}, LSMP's computation and runtime could
   be improved upon by extending the projection based recursive
   updating schemes of \cite{Cot1999, Cot2005} to MSSO LSMP\@.  In
   regards to the MRI design problem, runtime for any method might be
   reduced via a multi-resolution approach (as in \cite{Mal2005}) by
   first running the algorithm with a coarse -space frequency grid,
   noting which energy deposition locations are revealed, and then
   running the algorithm with a grid that is finely sampled around the
   locations suggested by the coarse result.  This is faster than
   providing the algorithm a large, finely-sampled grid and attempting
   to solve the sparse energy deposition task in one step.  An initial
   investigation has shown that reducing the maximum frequency extent
   of the -space grid (and thus the number of grid points, ) may
   also reduce runtime without significantly impacting image fidelity
   \cite{Zel2008_TMI}.

   \subsubsection{Shrinkage Algorithm Convergence Improvements} When
   solving the MRI pulse design problem, both RBRS and CBCS required
   excessive iterations and hence exhibited lengthy runtimes.  The
   latter was especially problematic as illustrated in
   Fig.~\ref{fig:e3_objFun_vs_iters}.  To mitigate these problems, one
   may consider extending parallel coordinate descent (PCD) shrinkage
   techniques used for single-system single-output sparse
   approximation (as in \cite{Ela2006_TransIT, Ela2006}).  Sequential
   subspace optimization (SESOP) \cite{Ela2007} might also be employed
   to reduce runtime.  Combining PCD with SESOP and adding a line
   search after each iteration would yield sophisticated versions of
   RBRS and CBCS\@.

   \subsubsection{Multiple-System Multiple-Output (MSMO) Simultaneous
   Sparse Approximation} In the future it may be useful to consider a
   combined problem where there are multiple observations as well as
   multiple system matrices.  That is, assume we have a series of 
   observations, , each of which arises
   due to a set of  simultaneously -sparse unknown vectors
   \footnote{The -term
   simultaneous sparsity profile of each set of s may or
   may not change with .}  passing through a set of  system
   matrices  and then undergoing
   linear combination, as follows:
   
   If  is constant for all  observations then the
   problem reduces to
   
   and we may stack the matrices and terms as follows:
   
   Having posed (\ref{mmv3_eq1}, \ref{mmv3_eq2}, \ref{mmv3_eq3}), one
   may formulate optimization problems similar to (\ref{mmv1},
   \ref{mmv2_eq2}) to determine simultaneously sparse s
   that solve (\ref{mmv3_eq3}).  Algorithms to solve such problems may
   arise by combining the concepts of SSMO algorithms
   \cite{Cot2005,Mal2005,Tro2006_I,Tro2006_II} with those of the MSSO
   algorithms posed earlier.

\section{Conclusion}
\label{sec:conclusion}

  We defined the linear inverse multiple-system, single-output (MSSO)
  simultaneous sparsity problem where simultaneously sparse sets of
  unknown vectors are required as the solution.  This problem differed
  from prior problems involving multiple unknown vectors because in
  this case, each unknown vector was passed through a different system
  matrix and the outputs of the various matrices underwent linear
  combination, yielding only one observation vector.

  To solve the proposed MSSO problem, we formulated three greedy
  techniques, matching pursuit, orthogonal matching pursuit, and least
  squares matching pursuit, along with algorithms based on iteratively
  reweighted least squares, iterative shrinkage, and second-order cone
  programming methodologies. The MSSO algorithms were evaluated across
  noiseless and noisy sparsity profile estimation experiments as well
  as a magnetic resonance imaging pulse design experiment; for
  sparsity profile recovery, algorithms that minimized the relaxed
  convex objective function outperformed the greedy methods, whereas
  in the noiseless magnetic resonance imagine pulse design experiment,
  greedy LSMP exhibited superior performance.

  Finally, when deriving CBCS for complex-valued data, we proved that
  seeking a single sparse complex-valued vector is equivalent to
  seeking two simultaneously sparse real-valued vectors---we mapped
  single-vector sparse approximation of a complex vector to the MSSO
  problem, increasing the applicability of algorithms that solve the
  latter.

  Overall, while improvements upon these seven algorithms (and new
  algorithms altogether) surely do exist, this manuscript has laid the
  groundwork of the MSSO problem and conducted an initial exploration
  of algorithms with which to solve it.

\section*{Acknowledgments} 

   The authors thank D.~M.~Malioutov for assistance with the
   derivation step that permitted the transition from (\ref{socp2}) to
   (\ref{socp3}) in Sec.~\ref{subsec:socp}, as well as K.~Setsompop,
   B.~A.~Gagoski, V.~Alagappan, and L.~L.~Wald for collecting the
   experimental coil profile data in Fig.~\ref{fig:e3_profiles}.

   The authors gratefully acknowledge the
   following sponsors and associated grants: National Institute of Health
   1P41RR14075, 1R01EB000790, 1R01EB006847, and 1R01EB007942; NSF
   CAREER Award CCF-643836; United States Department of Defense
   National Defense Science and Engineering Graduate Fellowship
   F49620-02-C-0041; the MIND Institute; the A.~A.~Martinos Center for
   Biomedical Imaging; and R.~J.~Shillman's Career Development Award.

\bibliographystyle{siam}
\bibliography{paper}

\end{document} 
