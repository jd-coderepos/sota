\section{Experimental Results}\label{sec:exp_result}

\textcolor{color3}{
This section is organized as follows: The effectiveness of our proposed framework is verified in Section \ref{sec:exp_result}.\textit{C}. An ablation study is conducted for the proposed MDR module in Section \ref{sec:exp_result}.\textit{D}.
}




\subsection{Implementation Details}\label{sec:settings}


Our network is trained on a single NVIDIA TITAN V GPU with 12 GB memory. \textcolor{color1}{The batch} size is set \textcolor{color1}{as} 4 for \textcolor{color1}{the} VOID \cite{void}, \textcolor{color1}{and} NYUv2 \cite{nyuv2} \textcolor{color1}{datasets}, and 2 for our custom \textcolor{color1}{point-line feature and depth (PLAD)} dataset considering the memory capacity. 
For training, the Adam optimizer with exponential decay rates of $\beta_1 = 0.9$ and $\beta_2 = 0.999$ is used.
The learning rate initialized \textcolor{color1}{as} $1 \times 10^{-4}$ decays by half at 20 epochs\textcolor{color1}{,} and is set \textcolor{color1}{as} $5 \times 10^{-5}$ up to 30 epochs.
\textcolor{color1}{In addition}, \textcolor{color1}{the} loss weights are set \textcolor{color1}{as} $w_p=1.0$, $w_s=0.6$, $w_l=0.04$, $w_1=0.15$, and $w_2=0.95$. 
After image undistortion for line extraction and cropping, $608 \times 448$ \textcolor{color1}{size images} are used for \textcolor{color1}{the} VOID and NYUv2 \textcolor{color1}{datasets}, and $1280 \times 720$ size image\textcolor{color1}{s are} used for \textcolor{color1}{the} PLAD \textcolor{color1}{dataset}.

\begin{figure}[t!]
    \centering
    \subfigure[]{\includegraphics[width=0.234\linewidth]{figure/sensorSetup.jpg}\label{fig:plad_sensor}}
    \subfigure[]{\includegraphics[width=0.365\linewidth]{figure/indoor.png}\label{fig:plad_indoor}}
    \subfigure[]{\includegraphics[width=0.365\linewidth]{figure/outdoor.png}\label{fig:plad_outdoor}}
    \caption{Our PLAD dataset: 
        \subref{fig:plad_sensor} Sensor setup consists of RGB-D camera and IMU. 
        \textcolor{color1}{S}equences \textcolor{color1}{acquired} 
        \subref{fig:plad_indoor} indoor and 
        \subref{fig:plad_outdoor} outdoor
    }
    \label{fig:plad}
\end{figure}



\subsection{\textcolor{color2}{Datasets}}
\subsubsection{VOID}
This dataset provides synchronized RGB images ($640 \times 480$), ground truth depth images ($640 \times 480$), inertial measurement unit (IMU) data from Intel D435i, and sparse point features from visual-inertial odometry \cite{xivo}. 
\textcolor{color1}{Because} Struct-MDC additionally utilizes line features, UV-SLAM \cite{uvslsam} is executed to detect point and line features \textcolor{color1}{and} generate sparse depth. However, \textcolor{color1}{owing} to the challeng\textcolor{color1}{es} with substantial motion blur of the provided sequences, \textcolor{color1}{the} ground truth depth is sampled using the feature matching results of the SLAM front-end.
Of the total 56 sequences, 47 sequences are used for training, \textcolor{color1}{8 for testing, and 1} with poor feature detection and mesh results is excluded.


\subsubsection{NYUv2}
This dataset provides synchronized RGB images ($640 \times 480$) and ground truth depth images ($640 \times 480$) from Microsoft Kinect. 
\textcolor{color1}{Because} this dataset does not provide the IMU data required to execute UV-SLAM, the ground truth depth is sampled as in the VOID dataset.
Post-processed data with filled depth values \textcolor{color1}{are} used as in \cite{s2d}. According to the official split, 48K frames are used for training \textcolor{color1}{and 654 frames} for testing.



\subsubsection{\textcolor{color1}{PLAD}}
\textcolor{color1}{A} sparse depth is inevitably sampled from the ground truth depth in the aforementioned datasets, which \textcolor{color1}{are} far from \textcolor{color1}{the} genuine depth completion from visual SLAM. Therefore, we \textcolor{color1}{generate} a PLAD dataset where sparse depth is provided by line-based visual SLAM to verify Struct-MDC.
As shown in Fig.~\ref{fig:plad}\subref{fig:plad_sensor}, the data \textcolor{color1}{are} acquired with a sensor setup consisting of \textcolor{color1}{an} Azure Kinect for ground truth depth with RGB images ($1280 \times 720$, 15 Hz) and \textcolor{color1}{an} Xsens Mti-100 for IMU data (6-axis, 200 Hz).
The depth and RGB images are synchronized with the feature depth estimated \textcolor{color1}{using} UV-SLAM. Each sequence is acquired in various environments, as shown in Figs.~\ref{fig:plad}\subref{fig:plad_indoor} and \subref{fig:plad_outdoor}.
The PLAD \textcolor{color1}{dataset} consists of 38 sequences, with 34 sequences used for training and 4 sequences for testing. More detailed information \textcolor{color1}{such as the} sensor calibration data is available at: \url{https://github.com/zinuok/line_depth_completion}.



















\subsection{Results \textcolor{color1}{of} Analysis}\label{sec:exp_main}
\begin{figure}[t]
    \centering
    \subfigure[]{\includegraphics[width=0.155\linewidth]{figure/void2drgbdpoint.pdf}\label{fig:void_point}}
    \subfigure[]{\includegraphics[width=0.155\linewidth]{figure/void2drgbdline.pdf}\label{fig:void_line}}
    \subfigure[]{\includegraphics[width=0.155\linewidth]{figure/void2dvoiced.pdf}\label{fig:void_voiced}}
    \subfigure[]{\includegraphics[width=0.155\linewidth]{figure/void2dkbnet.pdf}\label{fig:void_baseline}}
    \subfigure[]{\includegraphics[width=0.155\linewidth]{figure/void2dmy.pdf}\label{fig:void_ours}}
    \subfigure[]{\includegraphics[width=0.155\linewidth]{figure/void2dgt.pdf}\label{fig:void_gt}}
    \caption{
        Qualitative comparison results for the VOID: 
        \subref{fig:void_point} The extracted point features, 
        \subref{fig:void_line} the extracted point and line features from UV-SLAM \cite{uvslsam}.
        Completed depth from 
        \subref{fig:void_voiced} \cite{void}, 
        \subref{fig:void_baseline} \cite{baseline}, and
        \subref{fig:void_ours} Struct-MDC.
        \subref{fig:void_gt} Ground truth depth. 
        The dashed rectangles denote the area where the improvement in the Struct-MDC stands out. 
    }
    \label{fig:void_result}
    \vspace{-0.3cm}
\end{figure}
\begin{table}[t]
\centering
\renewcommand{\arraystretch}{1.10} \renewcommand{\tabcolsep}{0.8mm}  \caption{Quantitative comparison with state-of-the-arts \textcolor{color1}{methods} for the VOID \textcolor{color1}{dataset}. 
(\textit{pre}: results using their pretrained weights. 
\textit{re}: results using retrained weights.)
}

\begin{tabular}{lccccccc}
\hline
\multicolumn{2}{c}{\multirow{2}{*}{Method}} & \multicolumn{2}{c}{Error {[}mm{]} $\downarrow$} &  & \multicolumn{3}{c}{Accuracy {[}\%{]} $\uparrow$}    \\ \cline{3-4} \cline{6-8} 
\multicolumn{2}{c}{}                        & MAE                    & RMSE                   &  & $\delta_1$      & $\delta_2$      & $\delta_3$      \\ \hline
\multirow{2}{*}{\textcolor{color2}{VOICED~\cite{void}}}             & (\textit{pre}) & 167.608                & 316.017                &  & 57.499          & 74.014          & 96.974          \\
                                    & (\textit{re})  & 163.530                & 283.763                &  & 32.327          & \textbf{74.762} & 98.559          \\
\multirow{2}{*}{\textcolor{color2}{FusionNet~\cite{learning_top}}} & (\textit{pre}) & 167.600                & 346.533                &  & 55.455          & 72.721          & 97.866          \\
                                    & (\textit{re})  & 166.143                & 337.522                &  & 58.841          & 73.894          & 96.346          \\
\textcolor{color2}{KBNet (baseline)~\cite{baseline}}                    & (\textit{re})  & 143.801                & 262.643                &  & 52.286          & 69.494          & 98.724          \\
Struct-MDC (ours)                                &       & \textbf{111.332}       & \textbf{216.497}       &  & \textbf{62.074} & 74.544          & \textbf{99.003} \\ \hline
\end{tabular}
\label{table:exp_void}
\end{table}


Error metrics commonly \textcolor{color1}{adopted} for \textcolor{color1}{the} performance evaluation of depth estimation tasks \cite{baseline, error_metric3} are as follows: mean absolute error (MAE), root mean squared error (RMSE), and accuracy ratio under a threshold $\delta$ which is an absolute ratio between the estimation and the ground truth depth. 
In our experiments, overall statistic\textcolor{color1}{al analysis is based on} the MAE and RMSE, and strict thresholds $\delta_1 = 1.05$, $\delta_2 = 1.10$\textcolor{color1}{, and} $\delta_3 = 1.25^3$ are used to measure the extent of close\textcolor{color1}{ness} to the ground truth.

Struct-MDC \textcolor{color1}{used a new type of measurement: a line feature}. Therefore, depending on the dataset, the effect of line measurements is verified in various \textcolor{color1}{scenarios}.
In \textcolor{color1}{the} VOID and the NYUv2 \textcolor{color1}{datasets}, the total number of point and line features is limited to 150. 
In the PLAD \textcolor{color1}{dataset}, the point and line features estimated from the UV-SLAM are used without constraint\textcolor{color1}{s} on the total number. 


\begin{figure}[!t]
    \centering
    \subfigure[]{\label{fig:nyu_point}\includegraphics[width=0.155\linewidth]{figure/nyu2drgbdpoint.pdf}}
    \subfigure[]{\label{fig:nyu_line}\includegraphics[width=0.155\linewidth]{figure/nyu2drgbdline.pdf}}
    \subfigure[]{\label{fig:nyu_voiced}\includegraphics[width=0.155\linewidth]{figure/nyu2dvoiced.pdf}}
    \subfigure[]{\label{fig:nyu_baseline}\includegraphics[width=0.155\linewidth]{figure/nyu2dkbnet.pdf}}
    \subfigure[]{\label{fig:nyu_ours}\includegraphics[width=0.155\linewidth]{figure/nyu2dmy.pdf}}
    \subfigure[]{\label{fig:nyu_gt}\includegraphics[width=0.155\linewidth]{figure/nyu2dgt.pdf}}
    \caption{
        Qualitative comparison results for the NYUv2: \subref{fig:nyu_point} The extracted point features,
        \subref{fig:nyu_line} the extracted point and line features from UV-SLAM \cite{uvslsam}.
        Completed depth from 
        \subref{fig:nyu_voiced} \cite{void}, 
        \subref{fig:nyu_baseline} \cite{baseline}, and
        \subref{fig:nyu_ours} Struct-MDC.
        \subref{fig:nyu_gt} Ground truth depth. 
        The dashed rectangles denote the area where the improvement in the Struct-MDC stands out.
    }\label{fig:nyu_result}
    \vspace{-0.3cm}
\end{figure}
\begin{table}[!t]
\centering
\renewcommand{\arraystretch}{1.10} \renewcommand{\tabcolsep}{0.8mm}  \caption{Quantitative comparison with state-of-the-arts \textcolor{color1}{methods} for the NYUv2 \textcolor{color1}{dataset}.
(\textit{pre}: results using their pretrained weights. 
\textit{re}: results using retrained weights.
\textbf{U}: unsupervised, \textbf{S}: supervised
)}


\begin{tabular}{lcccccccc}
\hline
\multicolumn{3}{c}{Method}                                       & \multicolumn{2}{c}{Error {[}mm{]} $\downarrow$} & \multicolumn{1}{l}{} & \multicolumn{3}{c}{Accuracy {[}\%{]} $\uparrow$}    \\ \cline{4-5} \cline{7-9} 
\multicolumn{3}{c}{}                                             & MAE                    & RMSE                   & \multicolumn{1}{l}{} & $\delta_1$      & $\delta_2$      & $\delta_3$      \\ \hline
\multirow{2}{*}{\textcolor{color2}{VOICED~\cite{void}}}             & \multirow{2}{*}{\textbf{U}} & (\textit{pre}) & 244.416                & 407.979                &                      & 54.488          & 71.107          & 97.255          \\
                                    &                    & (\textit{re})  & 205.898                & 328.202                &                      & 56.965          & 74.747          & 99.401          \\
\multirow{2}{*}{\textcolor{color2}{FusionNet~\cite{learning_top}}} & \multirow{2}{*}{\textbf{U}} & (\textit{pre}) & 223.111                & 353.720                &                      & 51.878          & 69.801          & 99.282          \\
                                    &                    & (\textit{re})  & 208.399                & 360.653                &                      & 57.162          & 74.335          & 98.879          \\
\textcolor{color2}{KBNet (baseline)~\cite{baseline}}                    & \textbf{U}                  & (\textit{re})  & 179.817                & 297.872                &                      & 59.605          & 78.027          & 99.346          \\
Struct-MDC (ours)                                & \textbf{U}                  &       & \textbf{141.871}       & \textbf{245.548}       &                      & \textbf{67.991} & \textbf{81.999} & \textbf{99.698} \\ \hline
\textcolor{color2}{CSPN~\cite{cspn}}                                & \textbf{S}                  & (\textit{re})  & 163.152                & 245.864                &                      & 59.092          & \textbf{84.656} & \textbf{99.861} \\ \hline
\end{tabular}


\label{table:exp_nyu}
\end{table}


\subsubsection{Comparison for the VOID dataset}
In \textcolor{color1}{a} human-made environment, many line features are observed at the object boundary\textcolor{color1}{,} as shown in Fig.~\ref{fig:void_result}\subref{fig:void_line}.
The method proposed in \cite{void} depends only on \textcolor{color1}{the} depth from triangulation of point \textcolor{color1}{feature}. Therefore, as shown in Fig.~\ref{fig:void_result}\subref{fig:void_voiced}, the method \textcolor{color1}{frequently} generated incorrect triangles, estimating an ambiguous depth with mesh facet imprints. 
Our baseline, KBNet \cite{baseline}, has stains in several \textcolor{color1}{scenarios}, as shown in Fig.~\ref{fig:void_result}\subref{fig:void_baseline}. It can be \textcolor{color1}{seen} that traces of copy and paste remain in the final estimation \textcolor{color1}{because} they rely on the pooling layer to fill the low density of point features. 
In contrast, Struct-MDC does not contain any stains or incorrect imprints.
Ours employs the interpolation and MDR sequentially to fill the empty space. 
Therefore, the role\textcolor{color1}{s} of \textcolor{color1}{the two} module\textcolor{color1}{s are} adequately \textcolor{color1}{separated}, eliminating the stains and improving \textcolor{color1}{the} overall performance. 
An object boundary, which was disregarded in other methods, exists as \textcolor{color1}{a} line \textcolor{color1}{feature} and an edge of a triangle in the proposed algorithm. 
Consequently, \textcolor{color1}{such} boundaries \textcolor{color1}{are} more apparent in the final result, making the distinction between objects \textcolor{color1}{noticeable}. Remarkably, a curved boundary such as a circular table can also be estimated \textcolor{color1}{based on} our \textcolor{color1}{piecewise} linear assumption.  
The performance improvement \textcolor{color1}{can be} quantitatively \textcolor{color1}{seen from} TABLE~\ref{table:exp_void}. In Struct-MDC, the MAE and the RMSE are improved by $22.58\%$ and $17.57\%$, compared \textcolor{color1}{with} the state-of-the-art \cite{baseline}, respectively.



\subsubsection{Comparison for the NYUv2 dataset}
The qualitative comparison results are shown in Fig.~\ref{fig:nyu_result}. As in the experiments for the VOID \textcolor{color1}{dataset}, traces of triangulation remain in \cite{void} and stains are observed in \cite{baseline}. However, our methodology estimates \textcolor{color1}{a} depth close to the ground truth with the support of line features. 
In particular, Struct-MDC \textcolor{color1}{excels in} regions where point features are not extracted\textcolor{color1}{,} as shown in the first row or \textcolor{color1}{for} structural objects \textcolor{color1}{as shown in} the fourth row of Fig.~\ref{fig:nyu_result}. 
\textcolor{color1}{T}he performance improvement \textcolor{color1}{can be seen from the results in} TABLE~\ref{table:exp_nyu}. 
\textcolor{color1}{The} MAE and RMSE \textcolor{color1}{of Struct-MDC are} improved by $21.10\%$ and $17.57\%$, respectively, \textcolor{color1}{compared to those of the state-of-the-art method} \cite{baseline}. 
\textcolor{color3}{The average processing time of Struct-MDC was increased to 32.19 ms (31.07 Hz), compared to that of the baseline, 15.46 ms (64.68 Hz). However, it is still fast enough to be exploited to real robot applications.}
It is noteworthy that Struct-MDC outperforms the supervised method\textcolor{color1}{,} CSPN \cite{cspn}\textcolor{color1}{,} in \textcolor{color1}{terms of} all error metrics and \textcolor{color1}{the} accuracy metric of $\delta_1$, even though ours is an unsupervised method. 



\begin{figure}[t]
    \centering
    \subfigure[]{\label{fig:plad_pl}\includegraphics[width=0.24\linewidth]{figure/plad2drgbdline.pdf}}
    \subfigure[]{\label{fig:plad_mesh}\includegraphics[width=0.24\linewidth]{figure/plad2dmesh.pdf}}
    \subfigure[]{\label{fig:plad_ours}\includegraphics[width=0.24\linewidth]{figure/plad2dmy.pdf}}
    \subfigure[]{\label{fig:plad_gt}\includegraphics[width=0.24\linewidth]{figure/plad2dgt.pdf}}
    \caption{Qualitative comparison results for the PLAD: 
    \subref{fig:plad_pl} point and line features from UV-SLAM \cite{uvslsam}, 
    \subref{fig:plad_mesh} interpolated mesh, 
    \subref{fig:plad_ours} completed depth from Struct-MDC, and 
    \subref{fig:plad_gt} ground truth depth.}\label{fig:plad_result}
    \vspace{-0.3cm}
\end{figure}
\begin{table}[!t]
\centering
\renewcommand{\arraystretch}{1.10} \renewcommand{\tabcolsep}{1.0mm}  \caption{Quantitative comparison with state-of-the-art\textcolor{color1}{s methods} for the PLAD \textcolor{color1}{dataset}.
}
\begin{tabular}{lcccccc}
\hline
\multicolumn{1}{c}{\multirow{2}{*}{Method}} & \multicolumn{2}{c}{Error {[}mm{]} $\downarrow$} &  & \multicolumn{3}{c}{Accuracy {[}\%{]} $\uparrow$}  \\ \cline{2-3} \cline{5-7} 
\multicolumn{1}{c}{}                        & MAE                    & RMSE                   &  & $\delta_1$     & $\delta_2$     & $\delta_3$      \\ \hline
\textcolor{color2}{KBNet (baseline)~\cite{baseline}}                            & 5512.334               & 5595.194               &  & $\simeq$ 0.000 & $\simeq$ 0.000 & 9.183           \\
Struct-MDC (ours)                                        & \textbf{1170.303}      & \textbf{1481.583}      &  & \textbf{4.567} & \textbf{8.899} & \textbf{67.071} \\ \hline
\end{tabular}
\label{table:exp_plad}
\end{table}



\begin{figure*}[th!]
\centering
    \includegraphics[width=18cm, height=5.2cm]{figure/3dVisualize.pdf}
    \caption{3D visualization of estimated depth for the VOID (left three columns) and NYUv2 (right three columns) \textcolor{color1}{datasets}: Detected features (top row), estimated depth from our baseline (middle row), and proposed method (bottom row). The proposed method \textcolor{color1}{noticeably} aligns object boundary with reduced jittering.} 
    \label{fig:3d_visualize}
    \vspace{-0.4cm}
\end{figure*} 







\subsubsection{Comparison for the PLAD dataset}
In this dataset, the spatial distribution of the features is non-uniform, and there is \textcolor{color1}{considerable} uncertainty in the estimated depth from visual SLAM. 
As \textcolor{color1}{can be inferred from} TABLE~\ref{table:exp_plad}, the baseline \textcolor{color1}{presents} deficient performance and fail\textcolor{color1}{s} to converge. \textcolor{color1}{However}, Struct-MDC \textcolor{color1}{can} successfully estimate the entire depth \textcolor{color1}{because} the empty area \textcolor{color1}{is} significantly reduced by \textcolor{color1}{the generation of}  a convex hull using point and line features, as shown in  Fig.~\ref{fig:plad_result}\subref{fig:plad_mesh}. 

The main reason \textcolor{color1}{for the failure of} the baseline is that there is no \textcolor{color1}{method} to effectively \textcolor{color1}{deal with} the no-feature area outside the convex hull. The baseline requires the assumption that at least one sparse feature \textcolor{color1}{is} included in the kernel size of the pooling layer for \textcolor{color1}{densification}. This assumption is reasonably applied inside the convex hull where sparse features exist, \textcolor{color1}{whereas} it is \textcolor{color1}{in}effective if the proportion of the no-feature areas becomes large\textcolor{color1}{r than} the overall size of the image. Actually, the estimated point features \textcolor{color1}{in the} PLAD \textcolor{color1}{dataset} are extremely sparse, $0.02\%$, compared to \textcolor{color1}{those in} the previous two datasets, $0.05\%$. 
Struct-MDC also \textcolor{color1}{partially loses its} accuracy. Nevertheless, Struct-MDC is robust to the distribution and density condition.
This is because Struct-MDC additionally utilizes the line features which compensate the sparsity of point features from visual SLAM.  
In addition, the depth can be estimated even outside the convex hull without depth information by using the MDR module. 



\subsubsection{3D visualization results}
For a straightforward comparison, Fig.~\ref{fig:3d_visualize} \textcolor{color1}{shows} the 3D depth images estimated by the baseline and Struct-MDC \textcolor{color1}{on the} VOID and NYUv2 \textcolor{color1}{datasets}. 
The \textcolor{color1}{results of} the baseline \textcolor{color1}{show an} inaccurate jittering at the object boundary, \textcolor{color1}{which} is significantly reduced \textcolor{color1}{by} Struct-MDC. 
Struct-MDC can also more accurately estimate the depth inside the object than the baseline, by accurately \textcolor{color1}{capturing} the object boundary. 


\subsection{Ablation Study: Mesh Depth Refinement module}\label{sec:exp_ablation}


An ablation study was performed on two datasets to \textcolor{color1}{rigorously demonstrate the effectiveness of} the proposed MDR \textcolor{color1}{module}. 
If line features are employed, the model performance is expected to be improved as the given prior increases. Nonetheless, simply using \textcolor{color1}{a} line feature as a model input \textcolor{color1}{cannot} fully \textcolor{color1}{exploit} the \textcolor{color1}{potential of the line}. 
Fig.~\ref{fig:error_plot} and TABLE~\ref{table:ablation_1} show error plots and quantitative results according to input, respectively. P, L, M, and R denote point input, line input, mesh input, and refined mesh input, respectively.
The overall error \textcolor{color1}{is decreased} when the mesh \textcolor{color1}{is} employed \textcolor{color1}{compared to that} when simply using point and line features. The error was \textcolor{color1}{significantly} reduced when \textcolor{color1}{the} mesh refinement was performed.
When line features are simply used as an additional input to the network, blur holes still exist at the object boundary. 
The holes imply that the local smoothness loss\textcolor{color1}{,} $l_l$\textcolor{color1}{,} intensely reflect\textcolor{color1}{s} the penalty for accommodating the depth discontinuity \textcolor{color1}{in} the object boundary, and \textcolor{color1}{that} the network fell into a local minimum with a simple copy and paste. \textcolor{color1}{Because} the baseline employs a pooling layer for \textcolor{color1}{densification, and} the model \textcolor{color1}{is prone to} fall into this local minimum. 
In contrast, blur holes \textcolor{color1}{are} removed with the interpolated mesh depth as \textcolor{color1}{the} input\textcolor{color1}{, whereas} the object boundary \textcolor{color1}{is} captured sufficiently. \textcolor{color1}{This is because} the empty depth is filled \textcolor{color1}{by} interpolation with awareness of the object boundary \textcolor{color1}{by} CDT. 
The advantage of using \textcolor{color1}{a} mesh \textcolor{color1}{emerges} if the proposed MDR module is utilized jointly. Compared \textcolor{color1}{to} the case where point and line features \textcolor{color1}{are simply} used in the baseline model (P + L), the proposed methodology (P + L + M + R) dramatically improve\textcolor{color1}{s} the overall performance. 

\begin{figure}[t!]
\centering
    \includegraphics[width=8.7cm, height=1.9cm]{figure/errorPlot.pdf}
    \caption{Error plot\textcolor{color1}{s} of different input method\textcolor{color1}{s} with ground truth depth. In three error figures on right, \textcolor{color1}{bright color implies large} error.} 
    \label{fig:error_plot}
    \vspace{-0.3cm}
\end{figure} 
\begin{table}[t!]
\centering
\renewcommand{\arraystretch}{1.1} \renewcommand{\tabcolsep}{1.1mm}  \caption{Ablation study: Effectiveness of \textcolor{color1}{p}roposed \textcolor{color1}{m}esh \textcolor{color1}{d}epth \textcolor{color1}{r}efinement module. 
(P: point feature, L: line feature, M: interpolated mesh, R: \textcolor{color1}{p}roposed refinement module.
)}
\begin{tabular}[ht]{clcccccc}
\hline
\multirow{2}{*}{Dataset}                   & \multicolumn{1}{c}{\multirow{2}{*}{Method}} & \multicolumn{2}{c}{Error {[}mm{]} $\downarrow$} &  & \multicolumn{3}{c}{Accuracy {[}\%{]} $\uparrow$}    \\ \cline{3-4} \cline{6-8} 
                                          & \multicolumn{1}{c}{}                        & MAE                    & RMSE                   &  & $\delta_1$      & $\delta_2$      & $\delta_3$      \\ \hline
\multirow{4}{*}{VOID}                      & P (baseline)                                & 143.801                & 262.643                &  & 52.286          & 69.494          & 98.724          \\
                                          & P + L                                       & 124.243                & 235.622                &  & 58.642          & 72.745          & 98.949          \\
                                          & P + L + M                                   & 120.917                & 228.467                &  & 58.741          & 73.418          & 99.002          \\
                                          & P + L + M + R                               & \textbf{111.332}       & \textbf{216.497}       &  & \textbf{62.074} & \textbf{74.544} & \textbf{99.003} \\ \hline
\multicolumn{1}{l}{\multirow{4}{*}{NYUv2}} & P (baseline)                                & 179.817                & 297.872                &  & 59.605          & 78.027          & 99.346          \\
\multicolumn{1}{l}{}                       & P + L                                       & 163.618                & 270.276                &  & 62.230          & 79.185          & 99.500          \\
\multicolumn{1}{l}{}                       & P + L + M                                   & 144.853                & 248.293                &  & 67.381          & 81.728          & 99.687          \\
\multicolumn{1}{l}{}                       & P + L + M + R                               & \textbf{141.871}       & \textbf{245.548}       &  & \textbf{67.991} & \textbf{81.999} & \textbf{99.698} \\ \hline
\end{tabular}
\label{table:ablation_1}
\vspace{-0.4cm}
\end{table}
















