\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{caption}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}



\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy 

\def\cvprPaperID{5558} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\setcounter{page}{1}
\begin{document}

\title{Learning Parallax Attention for Stereo Image Super-Resolution}

\author{Longguang Wang, Yingqian Wang, Zhengfa Liang, Zaiping Lin, Jungang Yang, Wei An, Yulan Guo\\
	College of Electronic Science and Technology, National University of Defense Technology, China\\
	National Key Laboratory of Science and Technology on Blind Signal Processing, China\\
	{\tt\small \{wanglongguang15,yulan.guo\}@nudt.edu}}




\maketitle


\begin{abstract}
	Stereo image pairs can be used to improve the performance of super-resolution (SR) since additional information is provided from a second viewpoint. However, it is challenging to incorporate this information for SR since disparities between stereo images vary significantly. In this paper, we propose a parallax-attention stereo super-resolution network (PASSRnet) to integrate the information from a stereo image pair for SR. Specifically, we introduce a parallax-attention mechanism with a global receptive field along the epipolar line to handle different stereo images with large disparity variations. We also propose a new and the largest dataset for stereo image SR (namely, Flickr1024). Extensive experiments demonstrate that the parallax-attention mechanism can capture correspondence between stereo images to improve SR performance with a small computational and memory cost. Comparative results show that our PASSRnet achieves the state-of-the-art performance on the Middlebury, \textcolor{black}{KITTI 2012 and KITTI 2015} datasets.
	
\end{abstract}

\section{Introduction}
Super-resolution (SR) aims to reconstruct high-resolution (HR) images from their low-resolution (LR) counterparts. Recovering an HR image from a single shot is a long-standing problem \cite{2014-LearningaDeepConvolutionalNetworkforImageSuperResolution-Dong-184-199,2016-RealTimeSingleImageandVideoSuperResolutionUsinganEfficientSubPixelConvolutionalNeuralNetwork-Shi-1874-1883,2018-FastandAccurateSingleImageSuperResolutionViaInformationDistillationNetwork-Hui--}. Recently, dual cameras are becoming increasingly popular in mobile phones and autonomous vehicles. It is already demonstrated that subpixel shifts contained in LR stereo images can be used to improve SR performance \cite{2003-SuperResolutionImageReconstruction:aTechnicalOverview-Park-21-36}. However, since disparities between stereo images can vary significantly for different baselines, focal lengths, depths and resolutions, it is highly challenging to incorporate stereo correspondence for SR. 

\begin{figure}[bt]
	\centering
	\begin{minipage}[t]{0.9\linewidth}
\begin{minipage}[t]{0.339\linewidth}
			\includegraphics[width=0.95\linewidth]{./Figs_new/Fig1/patch1_bicubic.png}
			\setlength{\abovecaptionskip}{-11pt}
			\setlength{\belowcaptionskip}{-2pt}
			\caption*{Bicubic}
		\end{minipage}\begin{minipage}[t]{0.339\linewidth}
			\includegraphics[width=0.95\linewidth]{./Figs_new/Fig1/patch1_SRCNN.png}
			\setlength{\abovecaptionskip}{-11pt}
			\setlength{\belowcaptionskip}{-2pt}
			\caption*{SRCNN}
		\end{minipage}\begin{minipage}[t]{0.339\linewidth}
			\includegraphics[width=0.95\linewidth]{./Figs_new/Fig1/patch1_LapSRN.png}
			\setlength{\abovecaptionskip}{-11pt}
			\setlength{\belowcaptionskip}{-2pt}
			\caption*{LapSRN}
		\end{minipage}
		
		\begin{minipage}[t]{0.339\linewidth}
			\includegraphics[width=0.95\linewidth]{./Figs_new/Fig1/patch1_StereoSR.png}
			\setlength{\abovecaptionskip}{-11pt}
			\setlength{\belowcaptionskip}{0pt}
			\caption*{StereoSR}
		\end{minipage}\begin{minipage}[t]{0.339\linewidth}
			\includegraphics[width=0.95\linewidth]{./Figs_new/Fig1/patch1_ours.png}
			\setlength{\abovecaptionskip}{-11pt}
			\setlength{\belowcaptionskip}{0pt}
			\caption*{Ours}
		\end{minipage}\begin{minipage}[t]{0.339\linewidth}
			\includegraphics[width=0.95\linewidth]{./Figs_new/Fig1/patch1_hr.png}
			\setlength{\abovecaptionskip}{-11pt}
			\setlength{\belowcaptionskip}{0pt}
			\caption*{Groundtruth}
		\end{minipage}
	\end{minipage}

	\caption{Visual results achieved by bicubic interpolation, SRCNN \cite{2014-LearningaDeepConvolutionalNetworkforImageSuperResolution-Dong-184-199}, LapSRN \cite{2017-DeepLaplacianPyramidNetworksforFastandAccurateSuperResolution-Lai-5835-5843}, StereoSR \cite{2018-EnhancingtheSpatialResolutionofStereoImagesUsingaParallaxPrior-Jeon--} and our network for  SR. These results are achieved on ``test\_image\_002" of the KITTI 2015 dataset.}
	
	\label{fig1}
\end{figure}

Traditional multi-image SR methods \cite{2009-GeneralizingtheNonlocalMeanstoSuperResolutionReconstruction-Protter-36-51,2009-SuperResolutionwithoutExplicitSubpixelMotionEstimation-Takeda-1958-1975} use patch recurrence across images to obtain correspondence. However, these methods cannot \textcolor{black}{exploit} sub-pixel correspondence and their computational cost is high. Recent CNN-based frameworks \cite{2017-RealTimeVideoSuperResolutionwithSpatioTemporalNetworksandMotionCompensation-Caballero-2848-2857,2017-DetailRevealingDeepVideoSuperResolution-Tao-4482-4490,2018-LearningforVideoSuperResolutionthroughHROpticalFlowEstimation-LongguangWang--} incorporate optical flow estimation and SR in  unified networks to solve the video SR problem. However, these methods cannot be directly applied to stereo image SR since the disparity can be \textcolor{black}{much} larger than their receptive field.

Stereo matching has been investigated to obtain correspondence between \textcolor{black}{a stereo image pair} \cite{1982-ComputationalStereo-Barnard-553-572,2002-ATaxonomyandEvaluationofDenseTwoFrameStereoCorrespondenceAlgorithms-Scharstein--,2016-EfficientDeepLearningforStereoMatching-Luo-5695-5703}. Recent CNN-based methods \cite{2017-EndtoEndLearningofGeometryandContextforDeepStereoRegression-Kendall-66-75,2018-PyramidStereoMatchingNetwork-Chang--,2018-LearningDeepCorrespondencethroughPriorandPosteriorFeatureConstancy-Liang--,2018-LeftRightComparativeRecurrentModelforStereoMatching-Jie--} use 3D or 4D cost volumes in their \textcolor{black}{networks} to \textcolor{black}{model long-range dependency between stereo image pairs}. Intuitively, these CNN-based stereo matching methods can be	integrated with SR to provide accurate correspondence.  However, 4D cost volume based methods  \cite{2017-EndtoEndLearningofGeometryandContextforDeepStereoRegression-Kendall-66-75,2018-PyramidStereoMatchingNetwork-Chang--} suffer from a high computational and memory burden, which is unbearable for stereo image SR. Although the efficiency of 3D cost volume based methods  \cite{2018-LearningDeepCorrespondencethroughPriorandPosteriorFeatureConstancy-Liang--,2018-LeftRightComparativeRecurrentModelforStereoMatching-Jie--} is improved, these methods cannot handle stereo images with large disparity variations since a fixed maximum disparity is used to construct a cost volume.

Recently, Jeon \emph{et al.} proposed a stereo SR network (StereoSR) \cite{2018-EnhancingtheSpatialResolutionofStereoImagesUsingaParallaxPrior-Jeon--} to provide correspondence cues for SR using an image stack. Specifically, the image stack is obtained by concatenating the left image and the images generated by shifting the right image with different intervals. A direct mapping between parallax shifts and an HR image is then obtained. However, the flexibility of this method for different sensors and scenes is limited since the largest allowed disparity is fixed (\emph{i.e.}, 64 in \cite{2018-EnhancingtheSpatialResolutionofStereoImagesUsingaParallaxPrior-Jeon--}) in this algorithm.

In this paper, we propose a parallax-attention stereo SR network (PASSRnet) to incorporate stereo correspondence for the SR task. 
Given a stereo image pair, a residual atrous spatial pyramid pooling (ASPP) module is first used to generate multi-scale features.  Then, these features are fed to a parallax-attention module (PAM) to capture stereo correspondence. For each pixel in the left image, its feature similarities with all possible disparities in the right image are computed to generate an attention map. Consequently, our PAM can capture global correspondence while maintaining high flexibility. Afterwards, attention-driven feature aggregation is performed to update the features of the left image. Finally, these features are used to generate the SR result. Ablation study is performed on the KITTI 2015 dataset to test our PASSRnet. Comparative experiments are further conducted on the Middlebury, KITTI 2012 and KITTI 2015 datasets to demonstrate the superior performance of our network (as shown in Fig. \ref{fig1}).

Our main contributions can be summarized as follows: 
1) We propose a PASSRnet for SR by incorporating stereo correspondence;
2) We introduce a generic parallax-attention mechanism with a global receptive field along the epipolar line to handle different stereo images with large disparity variations. It is demonstrated that reliable correspondence can be efficiently generated by the parallax-attention mechanism for the improvement of SR performance;
3) We propose a new dataset, namely Flickr1024, \textcolor{black}{for the training of stereo image SR networks}. The Flickr1024 dataset consists of 1024 high-quality stereo image pairs and covers diverse scenes;
4) Our PASSRnet achieves the state-of-the-art performance as compared to recent single image SR and stereo image  SR methods.

\section{Related Work}
In this section, we briefly review several major works for SR and long-range dependency learning.
\subsection{Super-resolution}

\noindent 
\textbf{Single Image SR}
Since the seminal work of super-resolution convolutional neural network (SRCNN) \cite{2014-LearningaDeepConvolutionalNetworkforImageSuperResolution-Dong-184-199}, learning-based methods have dominated the research of single image SR. Kim \emph{et al.} \cite{2016-AccurateImageSuperResolutionUsingVeryDeepConvolutionalNetworks-Kim-1646-1654} proposed a very deep super-resolution network (VDSR) with 20 convolutional layers. Tai \emph{et al.} \cite{2017-ImageSuperResolutionViaDeepRecursiveResidualNetwork-Tai-2790-2798} developed a deep recursive residual network (DRRN) to control model parameters. Recently, Zhang \emph{et al.} \cite{2018-ResidualDenseNetworkforImageSuperResolution-Zhang--} proposed a residual dense network (RDN) to facilitate effective feature learning through a contiguous memory mechanism.

\noindent 
\textbf{Video SR}
\textcolor{black}{Liao \emph{et al.} \cite{2015-VideoSuperResolutionViaDeepDraftEnsembleLearning-Liao-531-539} introduced the first CNN for video SR.} They performed motion compensation to generate an ensemble of SR-drafts, and then employed a CNN to reconstruct  HR frames from the ensemble. Caballero \emph{et al.} \cite{2017-RealTimeVideoSuperResolutionwithSpatioTemporalNetworksandMotionCompensation-Caballero-2848-2857} proposed an end-to-end video SR framework by incorporating a motion compensation module with an SR module. Tao \emph{et al.} \cite{2017-DetailRevealingDeepVideoSuperResolution-Tao-4482-4490} \textcolor{black}{integrated an encoder-decoder network with LSTM} to fully use temporal correspondence. This architecture further facilitates the extraction of temporal context. Since correspondence between adjacent frames mainly exists within a local region, video SR methods focus on the \textcolor{black}{exploitation} of local dependency. Therefore, they cannot be directly applied to stereo image SR due to the non-local and long-range dependency in stereo images.

\noindent 
\textbf{Light-field Image SR}
Light-filed imaging can capture additional angular information of light at the cost of spatial resolution. To enhance spatial resolution, Yoon \emph{et al.} \cite{2015-LearningaDeepConvolutionalNetworkforLightFieldImageSuperResolution-Yoon-57-65} introduced \textcolor{black}{the first} light-field convolutional neural network (LFCNN). Yuan \emph{et al.} \cite{2018-LightFieldImageSuperresolutionUsingaCombinedDeepCNNBasedonEPI-Yuan-1359-1363a} proposed a CNN framework with a single image SR module and an epipolar plane image enhancement module. To model the correspondence between  images of adjacent sub-apertures, Wang \emph{et al.} \cite{2018-LFNet:aNovelBidirectionalRecurrentConvolutionalNeuralNetworkforLightFieldImageSuperResolution-Wang-4274-4286} developed a bidirectional recurrent CNN. Their network uses an implicit multi-scale feature fusion scheme to accumulate contextual information for SR. Note that, these methods 
are specifically proposed for light-field imaging with short baselines. Since stereo imaging usually has a much larger baseline than light-field imaging, these methods are unsuitable for stereo image SR.


\noindent 
\textbf{Stereo Image SR}
Bhavsar \emph{et al.} \cite{2010-ResolutionEnhancementinMultiImageStereo-Bhavsar-1721-1728} argued that image SR and HR depth estimation are intertwined \textcolor{black}{under stereo setting}. Therefore, they proposed an integrated approach to jointly estimate the SR image and HR disparity from LR stereo images. Recently, Jeon \emph{et al.} \cite{2018-EnhancingtheSpatialResolutionofStereoImagesUsingaParallaxPrior-Jeon--} proposed a StereoSR to employ parallax prior. Given a stereo image pair, the right image is shifted with different intervals and concatenated with the left image to generate a stereo tensor. The tensor is then fed to a plain CNN to generate the SR result by detecting similar patches within the disparity channel. However, StereoSR cannot handle different stereo images with large disparity variations since the number of shifted right images is fixed.

\begin{figure*}[ht]
	\centering
	\includegraphics[width=0.85\linewidth]{./Figs_new/fig2}
	\caption{An overview of our PASSRnet.}
	\label{fig2}
\end{figure*}

\subsection{Long-range Dependency Learning}
To handle different stereo images with varying disparities for SR, long-range dependency in stereo images should be captured. In this section, we review two types of methods for long-range dependency learning.








\noindent
\textbf{\textcolor{black}{Cost Volume}}
Cost volume is widely applied in stereo matching \cite{2017-EndtoEndLearningofGeometryandContextforDeepStereoRegression-Kendall-66-75,2018-PyramidStereoMatchingNetwork-Chang--,2018-LearningDeepCorrespondencethroughPriorandPosteriorFeatureConstancy-Liang--} and optical flow estimation \cite{2017-PWCNet:CNNsforOpticalFlowUsingPyramidWarpingandCostVolume-Sun--,2017-AccurateOpticalFlowViaDirectCostVolumeProcessing-Xu-5807-5815}. \textcolor{black}{For stereo matching}, several methods \cite{2017-EndtoEndLearningofGeometryandContextforDeepStereoRegression-Kendall-66-75,2018-PyramidStereoMatchingNetwork-Chang--} use naive concatenation to construct 4D cost volumes. These methods concatenate left feature maps with their corresponding right feature maps across all disparities to obtain a 4D cost volume (\emph{i.e.}, height{width}{disparity}{channel}). Then, 3D CNNs are usually used for matching cost learning. However, learning matching costs from 4D cost volumes suffers from a high computational and memory burden. To achieve higher efficiency, dot product is used to reduce feature dimension \cite{2018-LearningDeepCorrespondencethroughPriorandPosteriorFeatureConstancy-Liang--,2018-LeftRightComparativeRecurrentModelforStereoMatching-Jie--}, resulting in 3D cost volumes (\emph{i.e.}, height{width}{disparity}). However, due to the fixed maximum disparity in 3D cost volumes, these methods are unable to handle different \textcolor{black}{stereo image pairs} with large disparity variations.

\noindent
\textbf{\textcolor{black}{Self-attention Mechanisms}}
Attention mechanisms have been widely used to capture long-range dependency \cite{2015-DRAW:aRecurrentNeuralNetworkforImageGeneration-Gregor-1462-1471,2015-ShowAttendandTell:NeuralImageCaptionGenerationwithVisualAttention-Xu-2048-2057a}. For self-attention mechanisms \cite{2017-AttentionIsAllYouNeed-Vaswani-6000-6010,2018-SelfAttentionGenerativeAdversarialNetworks-Zhang--,2018-DualAttentionNetworkforSceneSegmentation-Fu--}, a weighted sum of all positions in spatial and/or temporal domain is calculated as the response at a position. Through matrix multiplication, self-attention mechanisms can capture the interaction between any two positions. Consequently, long-range dependency can be modeled with a small increase in computational and memory cost. Self-attention mechanisms have been successfully applied in image modeling \cite{2018-SelfAttentionGenerativeAdversarialNetworks-Zhang--} and semantic segmentation \cite{2018-DualAttentionNetworkforSceneSegmentation-Fu--}. Recent non-local networks \cite{2018-NonLocalNeuralNetworks-Wang--,2018-NonLocalRecurrentNetworkforImageRestoration-Liu--} share a similar idea and can be considered as a generalization of self-attention mechanisms. Note that, since self-attention mechanisms model dependency across the whole image, directly applying these mechanisms to stereo image SR involves unnecessary calculations.

Inspired by \textcolor{black}{self-attention mechanisms}, we develop a parallax-attention mechanism to model global dependency in stereo images. Compared to cost volumes, our parallax-attention mechanism is more flexible and efficient. Compared to self-attention mechanisms, our parallax-attention mechanism takes full use of epipolar constraints to reduce \textcolor{black}{search} space and improve efficiency. Moreover, the parallax-attention mechanism \textcolor{black}{enforces our network} to focus on the most similar feature rather than collecting all similar features for correspondence generation. It is demonstrated that the parallax-attention mechanism can generate reliable correspondence to improve SR performance (Section \ref{sec4.3.1}).

\section{Method}

Our PASSRnet takes a stereo image pair as input and super-resolves the left image. The architecture of our PASSRnet is shown in Fig. \ref{fig2} and Table \ref{tab1}.


\begin{table}[tp]
	\caption{The detailed architecture of our PASSRnet. LReLU represents leaky ReLU with a leakage factor of 0.1, dila stands for dilation rate,  denotes batch-wise matrix multiplication, and  is the upscaling factor.}
	\label{tab1}
	\begin{center}
		\scriptsize
		\setlength{\tabcolsep}{0.5mm}{
			\begin{tabular}{|c|c|c|c|c|}
				\hline 
				{Name}  & {Setting} & {Input} & {Output} \tabularnewline
				\hline
				input & & &  \tabularnewline
				\hline
				conv0 & \tabincell{c}{\\LReLU}  & & \tabularnewline
				\hline
				resblock0  & \Big[\tabincell{c}{\\}\Big] & & \tabularnewline
				\hline
				\multicolumn{4}{|c|}{\emph{Residual ASPP Module}}\tabularnewline
				\hline
				\tabincell{c}{resASPP\\1\_a} 
				&\Bigg[ \tabincell{c}{{\tabincell{c}{\\LReLU\\dila=1},
						\tabincell{c}{\\LReLU\\dila=4}, \tabincell{c}{\\LReLU\\dila=8}}\\} 
				\Bigg] 
				& &\tabularnewline
				\hline
				\tabincell{c}{resblock\\1\_a}  & \Big[\tabincell{c}{\\}\Big] & & \tabularnewline
				\hline
				\tabincell{c}{resASPP\\1\_b} 
				& \Bigg[
				\tabincell{c}{{\tabincell{c}{\\LReLU\\dila=1},
						\tabincell{c}{\\LReLU\\dila=4}, \tabincell{c}{\\LReLU\\dila=8}}\\} 
				\Bigg]  
				& &
				\tabularnewline
				\hline
				\tabincell{c}{resblock\\1\_b}  & \Big[\tabincell{c}{\\}\Big] & & \tabularnewline
				\hline
				\multicolumn{4}{|c|}{\emph{Parallax-Attention Module}}\tabularnewline
				\hline
				resblock2  & \Big[\tabincell{c}{\\}\Big] & & \tabularnewline
				\hline
				conv2\_a & \tabincell{c}{} & & \tabularnewline
				\hline
				conv2\_b & \tabincell{c}{, reshape} & & \tabularnewline
				\hline
				conv2\_c & \tabincell{c}{} & & \tabularnewline
				\hline
				att\_map & {conv2\_a}  {conv2\_b}  & \tabincell{c}{\\} & \tabularnewline
				\hline
				mult & {att\_map}  {conv2\_c}  & \tabincell{c}{\\} & \tabularnewline
				\hline
				fusion & \tabincell{c}{} & & \tabularnewline
				\hline
				\multicolumn{4}{|c|}{\emph{CNN}}\tabularnewline
				\hline
				\tabincell{c}{resblock3\\}  & \Big[\tabincell{c}{\\}\Big] & & \tabularnewline
				\hline
				sub-pixel & , pixel shuffle & & \tabularnewline
				\hline
				conv3\_b &  & & \tabularnewline
				\hline
		\end{tabular}}
	\end{center}
\end{table}


\subsection{Residual Atrous Spatial Pyramid Pooling (ASPP) Module}
Feature representation with rich context information is important for correspondence estimation \cite{2018-PyramidStereoMatchingNetwork-Chang--}. Therefore, both large receptive filed and multi-scale feature learning are required to obtain a discriminative representation. To this end, we propose a residual ASPP module to enlarge the receptive field and extract hierarchical features with dense pixel sampling rate and scales.


As shown in Fig. \ref{fig2} (a), our residual ASPP module is constructed by alternately cascading a residual ASPP block with a residual block. Input features are first fed to a residual ASPP block to generate multi-scale features. These resulting features are then sent to a residual block for feature fusion. This structure is repeated twice to produce final features. Within each residual ASPP block (as shown in Fig. \ref{fig2} (b)), we first combine three dilated convolutions (with dilation rates of 1, 4, 8) to form an ASPP group, and then cascade three ASPP groups in a residual manner. Our residual ASPP block not only enlarges the receptive field, but also enriches the diversity of convolutions, resulting in an ensemble of convolutions with different receptive regions and dilation rates. The highly discriminative feature learned by our residual ASPP module is beneficial to the overall SR performance, as demonstrated in Sec. \ref{sec4.3.1}.


\subsection{Parallax-attention Module (PAM)}
\label{sec3.2}
Inspired by \textcolor{black}{self-attention} mechanisms \cite{2018-SelfAttentionGenerativeAdversarialNetworks-Zhang--,2018-DualAttentionNetworkforSceneSegmentation-Fu--}, we develop PAM to capture global correspondence in stereo images. Our PAM efficiently integrates the information from \textcolor{black}{a stereo image pair}.


\noindent
\textbf{Parallax-attention Mechanism}
The architecture of our PAM is illustrated in Fig. \ref{fig2} (c). Given two feature maps , they are fed to a transition residual block to generate  and . Then,  is fed to a  convolution layer to produce a query feature map . 
Meanwhile,  is fed to another  convolution layer to produce  , which is then reshaped to . Batch-wise matrix multiplication is then performed between  and  and a softmax layer is applied, resulting in a parallax attention map . For more details, please refer to the supplemental material. Next,  is fed to a  convolution to generate , which is further multiplied \textcolor{black}{by}  to produce features . As a weighted sum of features at all possible disparities,  is  then integrated with its corresponding local features . Since PAM can gradually focus on the features at \textcolor{black}{accurate} disparities using feature similarities, correspondence can then be captured. Note that, once  is ready,  and  are exchanged to produce   for valid mask generation (as described below). Finally, stacked features and a valid mask  are fed to a  convolution layer for feature fusion.

\begin{figure}[tp]
	\centering
	\includegraphics[width=0.95\linewidth]{./Figs_new/fig3}
	\caption{Visual comparison between parallax-attention maps  generated by our PAM and the groundtruth. These attention maps () correspond to the regions () marked by blue and pink strokes in the left image.}
	\label{fig5}
\end{figure}

Different from self-attention mechanisms \cite{2018-SelfAttentionGenerativeAdversarialNetworks-Zhang--,2018-DualAttentionNetworkforSceneSegmentation-Fu--}, our parallax-attention mechanism \textcolor{black}{enforces our network} to focus on the most similar feature along the epipolar line rather than collecting all similar features, resulting in sparse attention maps. A comparison between parallax-attention maps generated by our PAM and the groundtruth is shown in Fig. \ref{fig5}. 
Note that,  represents the contribution of position  in the right image to position  in the left image. Consequently, the patterns in an attention map can reflect the correspondence between stereo pairs and also encode disparity information.
For more details, please refer to the supplemental material. It can be observed that our PAM produces patterns similar to the groundtruth, which indicates that reliable stereo correspondence can be captured by our PAM. It should be noted that our PASSRnet can be considered as a multi-task network to learn both stereo correspondence and SR. \textcolor{black}{However, using shared features for different tasks usually suffers from training conflict \cite{2018-MultiTaskLearningAsMultiObjectiveOptimization-Sener--}. Therefore, a transition block is introduced in our PAM to alleviate this problem.} The effectiveness of the transition block is demonstrated in Sec. \ref{sec4.3.1}.

\noindent
\textbf{Left-right Consistency \& Cycle Consistency}
Given deep features extracted from an LR stereo image pair ( and ), two parallax-attention maps ( and ) can be generated by PAM. Ideally, the following left-right consistency can be obtained if our PAM captures accurate correspondence:

where  denotes batch-wise matrix multiplication. Based on Eq. (\ref{eq2}), we can further derive a cycle consistency:

where the cycle-attention maps  and  can be calculated as:

Here, we introduce left-right consistency and cycle consistency to regularize  the training of our PAM for the generation of reliable and consistent correspondence.

\begin{figure}[bt]
	\centering
	\includegraphics[width=0.95\linewidth]{./Figs_new/fig6.png}
	\caption{Visualization of valid masks. Two left images and their occluded regions (\emph{i.e.}, yellow regions) are illustrated.}
	\label{fig6}
\end{figure}

\noindent
\textbf{Valid Masks}
Since left-right consistency and cycle consistency do not hold for occluded regions, we use an occlusion detection method to generate valid masks. We only enforce consistency on valid regions. In the parallax-attention map generated by our PAM (\emph{e.g.}, ), it is observed that pixels in occluded regions are usually assigned with small weights. Therefore, a valid mask  can be obtained by:

where  is a threshold (empirically set to 0.1 in this paper) and  is the width of stereo images. Two examples of valid masks are shown in Fig. \ref{fig6}. According to the parallax-attention mechanism,  represents the contribution of position  in the left image to position  in the right image. Since occluded pixels in the left image cannot find their correspondences in the right image, their values  are usually low. Thus, we consider these pixels as occluded ones. In practice, we use several morphological operations to handle isolated pixels and holes in valid masks. Note that, occluded regions in the left image cannot obtain additional information from the right image. Therefore, valid mask  is further used to guide feature fusion, as shown in Fig. \ref{fig2} (c).

\subsection{Losses}
\label{sec3.3}
We design four losses for the training of our PASSRnet. Other than an SR loss, we introduce three additional losses, including photometric loss, smoothness loss and cycle loss, to \textcolor{black}{help the network to fully exploit} the correspondence between stereo images. The overall loss function is formulated as:

where  is empirically set to 0.005. The performance of our network with different losses will be analyzed in Sec. \ref{sec4.3.2}.

\noindent
\textbf{SR Loss}
The mean square error (MSE) loss ifs used as the SR loss:

where  and  represent the SR result and HR groundtruth of the left image, respectively.

\noindent
\textbf{Photometric Loss}
Since collecting a large stereo dataset with densely labeled groundtruth disparities is highly challenging, we train our PAM in an unsupervised manner. Note that, if the groundtruth disparities are available, we can generate the groundtruth attention maps accordingly (see the supplemental material for more details) and train our PAM in a supervised manner. Following \cite{2017-UnsupervisedMonocularDepthEstimationwithLeftRightConsistency-Godard-6602-6611}, we introduce a photometric loss using the mean absolute error (MAE) loss. Note that, since the left-right consistency defined in Eq. (\ref{eq2}) only holds in non-occluded regions, we introduce a photometric loss as:

where  represents a pixel with a valid mask value.

\noindent
\textbf{Smoothness Loss}
To generate accurate and consistent attention in textureless regions, a smoothness loss is defined on the attention maps  and :

where . The first and second terms in Eq. (\ref{eq8}) are used to achieve vertical and horizontal attention consistency, respectively. 

\noindent
\textbf{Cycle Loss}
In addition to photometric loss and smoothness loss, we further introduce a cycle loss to achieve cycle consistency. Since  and  in Eq. (\ref{eq3}) can be considered as identity matrices, we design a cycle loss as:

where   is a stack of \textcolor{black}{} identity matrices.

\begin{table*}[ht]
	\caption{Comparative results achieved on the KITTI 2015 dataset by PASSRnet with different settings for  SR.}
	\label{tab2}
	\begin{center}
		\footnotesize
		\setlength{\tabcolsep}{2mm}{
			\begin{tabular}{|l|c|c|c|c|c|}
				\hline 
				Model & Input & PSNR & SSIM & Params. & Time
				\tabularnewline
				\hline
				PASSRnet with single input & Left & 25.27 & 0.770  & \textbf{1.32M} & \textbf{114ms} 
				\tabularnewline
				PASSRnet with replicated inputs & Left-Left & 25.29 & 0.771 & 1.42M & 176ms 
				\tabularnewline
				\hline
				PASSRnet without residual manner & Left-Right & 25.40 & 0.774  & 1.42M&176ms
				\tabularnewline
				PASSRnet without atrous convolution  & Left-Right & 25.38  & 0.773   &1.42M & 176ms 
				\tabularnewline
				\hline
				PASSRnet without PAM & Left-Right & 25.28  & 0.771   & \textbf{1.32M} & 135ms
				\tabularnewline
				PASSRnet without transition residual block & Left-Right & 25.36 & 0.773 & 1.34M & 160ms 
				\tabularnewline
				\hline				
				PASSRnet  & Left-Right & \textbf{25.43} & \textbf{0.776} & 1.42M & 176ms 
				\tabularnewline
				\hline
		\end{tabular}}
	\end{center}
\end{table*}

\section{Experimental Results}
In this section, we first introduce the datasets and implementation details, and then conduct ablation experiments to test our network. We further compare our network to recent single image SR and stereo image SR methods.



\subsection{\textcolor{black}{Datasets}}
For training, we followed \cite{2018-EnhancingtheSpatialResolutionofStereoImagesUsingaParallaxPrior-Jeon--} and downsampled 60 Middlebury \cite{2014-HighResolutionStereoDatasetswithSubpixelAccurateGroundTruth-Scharstein-31-42} images by a factor of 2 to generate HR images. We further collected 1024  stereo images from Flickr to construct a new Flickr1024 dataset. This dataset was used as the augmented training data for our PASSRnet. Please see the supplemental material for more details about the Flickr1024 dataset. For test, we used 5 images from the Middlebury dataset, 20 images from the KITTI 2012 dataset \cite{2012-AreWeReadyforAutonomousDriving?theKITTIVisionBenchmarkSuite-Geiger-3354-3361} and 20 images from the KITTI 2015 dataset \cite{2015-ObjectSceneFlowforAutonomousVehicles-Menze-3061-3070} as benchmark \textcolor{black}{datasets}. We further collected 10 close-shot stereo images (with disparities larger than 200) from Flickr to test the flexibility of our network to large disparity variations. For validation, we selected another 20 images from
the KITTI 2012 dataset.

\subsection{Implementation Details}
During the training phase, we first downsampled HR images using bicubic interpolation to generate LR images, and then cropped  patches with a stride of 20 from these LR images. Meanwhile, their corresponding patches in HR images were also cropped. The horizontal patch size was increased to 90 to cover most disparities (96\%) in our training dataset. These patches were randomly flipped horizontally and vertically for data augmentation. Note that, rotation was not performed to maintain epipolar constraints. We used peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) to test SR performance. Similar to \cite{2018-EnhancingtheSpatialResolutionofStereoImagesUsingaParallaxPrior-Jeon--}, we cropped borders to achieve fair comparison.

Our PASSRnet was implemented in Pytorch on a PC with an Nvidia GTX 1080Ti GPU. All models were optimized using the Adam method \cite{2015-Adam:aMethodforStochasticOptimization-Kingma--} with ,  and a batch size of 32. The initial learning rate was set to  and reduced to half after every 30 epochs. The training was stopped after 80 epochs since more epochs do not provide further consistent improvement.

\subsection{Ablation Study}
In this section, we present ablation experiments to justify our design choices, including the network architecture and the losses.


\subsubsection{Network Architecture}
\label{sec4.3.1}
\noindent
\textbf{Single Input vs. Stereo Input}
Compared to single images, stereo image pairs provide additional information observed from a different viewpoint. To demonstrate the effectiveness of stereo information for SR performance improvement, we removed PAM from our PASSRnet and retrained the network with single images (\emph{i.e.}, the left images). For comparison, we also used pairs of replicated left images as the input to the original PASSRnet. Results achieved on the KITTI 2015 dataset are listed in Table \ref{tab2}. 

Compared to the original PASSRnet, the network trained with single images suffers a decrease of 0.16 dB (25.43 to 25.27) in PSNR. Further, if pairs of replicated left images are fed to the original PASSRnet, the PSNR value is decreased to 25.29 dB. Without extra information introduced by stereo images, our PASSRnet with replicated images achieves comparable performance to the network trained with single images. This clearly demonstrates that stereo images can be used to improve the performance of PASSRnet.

\noindent
\textbf{Residual ASPP Module}
Residual ASPP module is used in our network to extract multi-scale features. To demonstrate the effectiveness of residual ASPP, two variants were introduced. First, to test the effectiveness of residual connections, we removed them to obtain a cascading ASPP module. Then, to test the effectiveness of atrous convolutions, we replaced them with ordinary convolutions. 

From the comparative results shown in Table \ref{tab2}, we can see that SR performance benefits from both residual connections and atrous convolutions. If residual connections are removed, the PSNR value is decreased from 25.43 dB to 25.40 dB. That is because, residual connections enable our residual ASPP module to extract features at more scales, resulting in more robust feature representations. Furthermore, if atrous convolutions are replaced by ordinary ones, the PSNR value is decreased from 25.43 dB to 25.38 dB. That is because, large receptive field of atrous convolutions facilitates our PASSRnet to employ context information in a large area. Therefore, more accurate correspondence can be obtained to improve SR performance.

\noindent
\textbf{Parallax-attention Module}
PAM is introduced to integrate the information from stereo images. To demonstrate its effectiveness, we introduced a variant by removing PAM and directly stacking the output features of the residual ASPP module. It can be observed from Table \ref{tab2} that the PSNR value is decreased from 25.43 dB to 25.28 dB if PAM is removed. That is because, long spatial distance between local features in the left image and their dependency in the right image hinders plain CNNs to integrate these features effectively.

\noindent
\textbf{Transition Block in PAM}
Transition block in PAM is introduced to alleviate the training conflict in shared layers. To demonstrate the effectiveness of transition block, we removed it from our PAM and retrained the network.
It can be observed from Table \ref{tab2} that the PSNR value is decreased  from 25.43 dB to 25.36 dB if the transition block is removed. That is because, the  transition block enhances task-specific feature learning in our PAM and alleviates training conflict in shared layers. Therefore, more representative features can be learned in shared layers.

\noindent
\textbf{PAM vs. Cost Volume}
Cost volume and 3D convolutions are commonly used to obtain stereo correspondence \cite{2017-EndtoEndLearningofGeometryandContextforDeepStereoRegression-Kendall-66-75,2018-PyramidStereoMatchingNetwork-Chang--}. To demonstrate the efficiency of our PAM in stereo correspondence generation, we replaced PAM with a 4D cost volume and two 3D convolutional layers (). It can be observed from Table \ref{tab3} that our PAM has less than half of the parameters in the cost volume formation. Moreover, our PAM achieves superior computational efficiency, with FLOPs being reduced by over 150 times. \textcolor{black}{ With PAM, our PASSRnet achieves better SR performance (\emph{i.e.}, PSNR value is increased from 25.23 dB to 25.43 dB) and efficiency (\emph{i.e.}, running time is decreased by 1.5 times).} That is because, two 3D convolutional layers are insufficient to capture long-range correspondence within the cost volume. However, adding more layers will lead to a significant increase of computational cost.


\begin{table}[bt]
	\caption{Comparison between our PAM and the cost volume formation for  SR. FLOPs are calculated on  input features, while Time/PSNR/SSIM values are achieved on the KITTI 2015 dataset.}
	\label{tab3}
	\begin{center}
		\footnotesize
		\setlength{\tabcolsep}{1mm}{
			\begin{tabular}{|c|c|c|c|c|c|}
				\hline 
				Model & Params. & FLOPs & Time & PSNR & SSIM
				\tabularnewline
				\hline
				PAM & 94K &  &  & 25.43 & 0.776 \tabularnewline
				\hline
				Cost Volume  & 221K &  &  & 25.23 & 0.768 
				\tabularnewline
				\hline
		\end{tabular}}
	\end{center}
\end{table}


\subsubsection{Losses}
\label{sec4.3.2}
To test the effectiveness of our losses, we retrained PASSRnet using different losses.

It can be observed from Table \ref{tab4} that the PSNR value of our PASSRnet is decreased from 25.43 to 25.35 if PASSRnet is trained with only SR loss. \textcolor{black}{That is because, with only this loss, our PAM learns to collect all similar features along the epipolar line and cannot focus on the most similar feature to provide accurate correspondence.} Further, the performance is gradually improved if photometric loss, smoothness loss and cycle loss are added. That is because, these losses encourage our PAM to generate reliable and consistent correspondence. Overall, our PASSRnet achieves the best performance (\emph{i.e.}, PSNR=25.43 dB and SSIM=0.776) when it is trained with all these losses.

\begin{table}[tp]
	\caption{Comparative results achieved on KITTI 2015 by our PASSRnet trained with different losses for  SR.}
	\label{tab4}
	\begin{center}
		\footnotesize
		\setlength{\tabcolsep}{0.8mm}{
			\begin{tabular}{|c|cccc|c|c|}
				\hline 
				Model &  &  &  &  & PSNR & SSIM 
				\tabularnewline
				\hline
				PASSRnet & \checkmark   &   &   &  & 25.35 & 0.771  \tabularnewline
				PASSRnet & \checkmark & \checkmark &   &  & 25.38 & 0.773 
				\tabularnewline
				PASSRnet & \checkmark & \checkmark & \checkmark &  & 25.40 & 0.774 
				\tabularnewline
				PASSRnet & \checkmark & \checkmark & \checkmark & \checkmark& \textbf{25.43}& \textbf{0.776} \tabularnewline
				\hline
		\end{tabular}}
	\end{center}
\end{table}


\begin{table*}[ht]
	\caption{Comparative PSNR/SSIM values achieved on the Middlebury, KITTI 2012 and KITTI 2015 datasets. Results marked with * are directly copied from the corresponding paper. Note that, only 2 SR results of StereoSR are presented on the KITTI 2012 and KITTI 2015 datasets since a 4 SR model is unavailable.}
	\label{tab5}
	\begin{center}
		\footnotesize
		\setlength{\tabcolsep}{1.5mm}{
			\begin{tabular}{|c|c|ccccc||cc|}
				\hline 
				\multirow{2}{*}{Dataset} &  \multirow{2}{*}{Scale} & \multicolumn{5}{c||}{Single Image SR} & \multicolumn{2}{c|}{Stereo Image SR}
				\tabularnewline
				
				&  
				&SRCNN \cite{2014-LearningaDeepConvolutionalNetworkforImageSuperResolution-Dong-184-199} 
				&VDSR \cite{2016-AccurateImageSuperResolutionUsingVeryDeepConvolutionalNetworks-Kim-1646-1654}  
				&DRCN \cite{2016-DeeplyRecursiveConvolutionalNetworkforImageSuperResolution-Kim-1637-1645}
				&LapSRN \cite{2017-DeepLaplacianPyramidNetworksforFastandAccurateSuperResolution-Lai-5835-5843} 
				&DRRN  \cite{2017-ImageSuperResolutionViaDeepRecursiveResidualNetwork-Tai-2790-2798} 
				&StereoSR \cite{2018-EnhancingtheSpatialResolutionofStereoImagesUsingaParallaxPrior-Jeon--} 
				& Ours 
				\tabularnewline
				\hline
				\multirow{2}{*}{\tabincell{c}{Middlebury\20 images)}} & 2 &29.75/0.901&30.17/0.906&30.19/0.906&30.10/0.905&30.16/0.908&30.13/0.908&\textbf{30.65/0.916} 
				\tabularnewline
				& 4 &25.53/0.764&25.93/0.778&25.92/0.777&25.96/0.779&25.94/0.773&-&\textbf{26.26/0.790} 
				\tabularnewline
				\hline
				\multirow{2}{*}{\tabincell{c}{KITTI 2015\20 images)}} & 2 &28.77/0.901&28.99/0.904&29.04/0.904&28.97/0.903&29.00/0.906&29.09/0.909&\textbf{29.78/0.919} 
				\tabularnewline
				& 4 &24.68/0.744&25.01/0.760&25.04/0.759&25.03/0.760&25.05/0.756&-&\textbf{25.43/0.776} 
				\tabularnewline
				\hline
		\end{tabular}}
	\end{center}
\end{table*}

\begin{figure*}[ht]
	\centering
	\includegraphics[width=0.98\linewidth]{./Figs_new/fig5}
	\caption{Visual comparison for  SR. These results are achieved on ``test\_image\_013" of the KITTI 2012 dataset and ``test\_image\_019" of the KITTI 2015 dataset.}
	\label{fig7}
\end{figure*}



\textcolor{black}{
	\begin{table}[bt]
		\caption{Comparison between \textcolor{black}{our PASSRnet and StereoSR \cite{2018-EnhancingtheSpatialResolutionofStereoImagesUsingaParallaxPrior-Jeon--}} on stereo images with different resolutions for 2 SR.}
		\begin{center}
			\footnotesize
			\setlength{\tabcolsep}{1.2mm}{
				\begin{tabular}{|l|cc|cc|}
					\hline 
					\multirow{2}{*}{Resolution} &  \multicolumn{2}{c|}{StereoSR \cite{2018-EnhancingtheSpatialResolutionofStereoImagesUsingaParallaxPrior-Jeon--}} & \multicolumn{2}{c|}{Ours}
					\tabularnewline
					& PSNR & FLOPs & PSNR & FLOPs
					\tabularnewline
					\hline
					High ()  & 39.27 &\textcolor{black}{1} & \textcolor{black}{\textbf{41.45}}() & \textcolor{black}{0.57}
					\tabularnewline
					\hline
					Middle () & 34.21 & \textcolor{black}{1} & \textcolor{black}{\textbf{35.04}}() & \textcolor{black}{0.58}
					\tabularnewline
					\hline
					Low () & 29.48 & \textcolor{black}{1} & \textcolor{black}{\textbf{29.88}}() & \textcolor{black}{0.36}
					\tabularnewline
					\hline
			\end{tabular}}
		\end{center}
		\label{tab6}
	\end{table}
}

\subsection{Comparison to State-of-the-arts}
We compared our PASSRnet to a number of CNN-based SR methods on \textcolor{black}{three benchmark datasets}. Recent single image SR methods under comparison include SRCNN \cite{2014-LearningaDeepConvolutionalNetworkforImageSuperResolution-Dong-184-199}, VDSR \cite{2016-AccurateImageSuperResolutionUsingVeryDeepConvolutionalNetworks-Kim-1646-1654}, DRCN \cite{2016-DeeplyRecursiveConvolutionalNetworkforImageSuperResolution-Kim-1637-1645}, LapSRN \cite{2017-DeepLaplacianPyramidNetworksforFastandAccurateSuperResolution-Lai-5835-5843} and DRRN \cite{2017-ImageSuperResolutionViaDeepRecursiveResidualNetwork-Tai-2790-2798}. We also compared our PASSRnet to the latest stereo image SR method StereoSR \cite{2018-EnhancingtheSpatialResolutionofStereoImagesUsingaParallaxPrior-Jeon--}. The codes provided by the authors of these methods were used to conduct experiments. Note that, similar to \cite{2018-EnhancingtheSpatialResolutionofStereoImagesUsingaParallaxPrior-Jeon--,2018-FastAccurateAndLightweightSuperResolutionwithCascadingResidualNetwork-Ahn--}, EDSR \cite{2017-EnhancedDeepResidualNetworksforSingleImageSuperResolution-Lim--}, RDN \cite{2018-ResidualDenseNetworkforImageSuperResolution-Zhang--} and D-DBPN \cite{2018-DeepBackProjectionNetworksforSuperResolution-Haris--} are not included in our comparison since their model sizes are larger than our PASSRnet by at least 8 times.

\noindent
\textbf{Quantitative Results}
The quantitative results are shown in Table \ref{tab5}. It can be observed that our PASSRnet achieves the best performance on the Middlebury, KITTI 2012 and KITTI 2015 datasets. Specifically, compared to single image SR methods, our PASSRnet outperforms the second best approach (\emph{i.e.}, DRRN) by 1.04 dB in terms of PSNR on the Middlebury dataset for  SR. Moreover, the PSNR value achieved by our network is higher than that of StereoSR by 1.00 dB. That is because, more reliable correspondence can be captured by our parallax-attention mechanism. 

\noindent
\textbf{Qualitative Results}
Figure \ref{fig7} illustrates the qualitative results achieved on \textcolor{black}{two} scenarios. It can be observed from zoom-in regions that single image SR methods cannot recover reliable details. In contrast, our PASSRnet uses stereo correspondence to produce finer details with fewer artifacts, such as the railings and stripe in Fig. \ref{fig7}. Compared to StereoSR, our PASSRnet explicitly captures stereo correspondence for SR. Consequently, superior visual performance is achieved.

\noindent
\textcolor{black}{
	\textbf{Flexibility}
	We further tested the flexibility of our PASSRnet and StereoSR \cite{2018-EnhancingtheSpatialResolutionofStereoImagesUsingaParallaxPrior-Jeon--} with respect to large disparity variations. Results achieved on images with different resolutions are shown in Table \ref{tab6}. More results under different baselines and depths are  available in the supplemental material. It can be observed that our PASSRnet is significantly better than StereoSR in terms of efficiency (\emph{i.e.}, FLOPs) on low resolution images. Meanwhile, our PASSRnet outperforms StereoSR by a large margin in terms of PSNR on high resolution images. That is because, StereoSR needs to perform padding for images with horizontal resolution lower than 64 pixels, which involves unnecessary calculations. For high resolution images, the fixed maximum disparity hinders StereoSR to capture longer-range correspondence. Therefore, the SR performance of StereoSR is limited.
}

\section{Conclusion}
In this paper, we propose a parallax-attention stereo super-resolution network (PASSRnet) to incorporate stereo correspondence for the SR task. Our PASSRnet introduces a parallax-attention mechanism with global receptive field to handle different stereo images with large disparity variations. We also introduce a new and the largest dataset for stereo image SR. It is demonstrated that our PASSRnet can effectively capture stereo correspondence for the improvement of SR performance. Comparison to recent single image SR and stereo image SR methods has shown that our network achieves the state-of-the-art performance.



{\small
	\bibliographystyle{unsrt}
	\bibliographystyle{ieee}
}


\begin{thebibliography}{10}
	
	\bibitem{2014-LearningaDeepConvolutionalNetworkforImageSuperResolution-Dong-184-199}
	Chao Dong, Chen~Change Loy, Kaiming He, and Xiaoou Tang.
	\newblock Learning a deep convolutional network for image super-resolution.
	\newblock In {\em ECCV}, pages 184--199, 2014.
	
	\bibitem{2016-RealTimeSingleImageandVideoSuperResolutionUsinganEfficientSubPixelConvolutionalNeuralNetwork-Shi-1874-1883}
	Wenzhe Shi, Jose Caballero, Ferenc Huszar, Johannes Totz, Andrew~P. Aitken, Rob
	Bishop, Daniel Rueckert, and Zehan Wang.
	\newblock Real-time single image and video super-resolution using an efficient
	sub-pixel convolutional neural network.
	\newblock In {\em CVPR}, pages 1874--1883, 2016.
	
	\bibitem{2018-FastandAccurateSingleImageSuperResolutionViaInformationDistillationNetwork-Hui--}
	Zheng Hui, Xiumei Wang, and Xinbo Gao.
	\newblock Fast and accurate single image super-resolution via information
	distillation network.
	\newblock In {\em CVPR}, 2018.
	
	\bibitem{2003-SuperResolutionImageReconstruction:aTechnicalOverview-Park-21-36}
	Sung~Cheol Park, Min~Kyu Park, and Moon~Gi Kang.
	\newblock Super-resolution image reconstruction: a technical overview.
	\newblock {\em IEEE signal processing magazine}, 20(3):21--36, 2003.
	
	\bibitem{2017-DeepLaplacianPyramidNetworksforFastandAccurateSuperResolution-Lai-5835-5843}
	Wei{-}Sheng Lai, Jia{-}Bin Huang, Narendra Ahuja, and Ming{-}Hsuan Yang.
	\newblock Deep laplacian pyramid networks for fast and accurate
	super-resolution.
	\newblock In {\em CVPR}, pages 5835--5843, 2017.
	
	\bibitem{2018-EnhancingtheSpatialResolutionofStereoImagesUsingaParallaxPrior-Jeon--}
	Daniel~S. Jeon, Seung-Hwan Baek, Inchang Choi, and Min~H. Kim.
	\newblock Enhancing the spatial resolution of stereo images using a parallax
	prior.
	\newblock In {\em CVPR}, 2018.
	
	\bibitem{2009-GeneralizingtheNonlocalMeanstoSuperResolutionReconstruction-Protter-36-51}
	Matan Protter, Michael Elad, Hiroyuki Takeda, and Peyman Milanfar.
	\newblock Generalizing the nonlocal-means to super-resolution reconstruction.
	\newblock {\em {IEEE} Trans. Image Processing}, 18(1):36--51, 2009.
	
	\bibitem{2009-SuperResolutionwithoutExplicitSubpixelMotionEstimation-Takeda-1958-1975}
	Hiroyuki Takeda, Peyman Milanfar, Matan Protter, and Michael Elad.
	\newblock Super-resolution without explicit subpixel motion estimation.
	\newblock {\em {IEEE} Trans. Image Processing}, 18(9):1958--1975, 2009.
	
	\bibitem{2017-RealTimeVideoSuperResolutionwithSpatioTemporalNetworksandMotionCompensation-Caballero-2848-2857}
	Jose Caballero, Christian Ledig, Andrew~P. Aitken, Alejandro Acosta, Johannes
	Totz, Zehan Wang, and Wenzhe Shi.
	\newblock Real-time video super-resolution with spatio-temporal networks and
	motion compensation.
	\newblock In {\em CVPR}, pages 2848--2857, 2017.
	
	\bibitem{2017-DetailRevealingDeepVideoSuperResolution-Tao-4482-4490}
	Xin Tao, Hongyun Gao, Renjie Liao, Jue Wang, and Jiaya Jia.
	\newblock Detail-revealing deep video super-resolution.
	\newblock In {\em ICCV}, pages 4482--4490, 2017.
	
	\bibitem{2018-LearningforVideoSuperResolutionthroughHROpticalFlowEstimation-LongguangWang--}
	Longguang Wang, Yulan Guo, Zaiping Lin, Xinpu Deng, and Wei An.
	\newblock Learning for video super-resolution through {HR} optical flow
	estimation.
	\newblock In {\em ACCV}, 2018.
	
	\bibitem{1982-ComputationalStereo-Barnard-553-572}
	Stephen~T. Barnard and Martin~A. Fischler.
	\newblock Computational stereo.
	\newblock {\em {ACM} Comput. Surv.}, 14(4):553--572, 1982.
	
	\bibitem{2002-ATaxonomyandEvaluationofDenseTwoFrameStereoCorrespondenceAlgorithms-Scharstein--}
	Dan. Scharstein and R.~Szeliski.
	\newblock A taxonomy and evaluation of dense two-frame stereo correspondence
	algorithms.
	\newblock {\em International Journal of Computer Vision}, 47(1-3), 2002.
	
	\bibitem{2016-EfficientDeepLearningforStereoMatching-Luo-5695-5703}
	Wenjie Luo, Alexander~G. Schwing, and Raquel Urtasun.
	\newblock Efficient deep learning for stereo matching.
	\newblock In {\em CVPR}, pages 5695--5703, 2016.
	
	\bibitem{2017-EndtoEndLearningofGeometryandContextforDeepStereoRegression-Kendall-66-75}
	Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta, and Peter Henry.
	\newblock End-to-end learning of geometry and context for deep stereo
	regression.
	\newblock In {\em ICCV}, pages 66--75, 2017.
	
	\bibitem{2018-PyramidStereoMatchingNetwork-Chang--}
	Jia{-}Ren Chang and Yong{-}Sheng Chen.
	\newblock Pyramid stereo matching network.
	\newblock In {\em CVPR}, 2018.
	
	\bibitem{2018-LearningDeepCorrespondencethroughPriorandPosteriorFeatureConstancy-Liang--}
	Zhengfa Liang, Yiliu Feng, Yulan Guo, Hengzhu Liu, Linbo Qiao, Wei Chen,
	Li~Zhou, and Jianfeng Zhang.
	\newblock Learning for disparity estimation through feature constancy.
	\newblock In {\em CVPR}, 2018.
	
	\bibitem{2018-LeftRightComparativeRecurrentModelforStereoMatching-Jie--}
	Zequn Jie, Pengfei Wang, Yonggen Ling, Bo~Zhao, Yunchao Wei, Jiashi Feng, and
	Wei Liu.
	\newblock Left-right comparative recurrent model for stereo matching.
	\newblock In {\em CVPR}, 2018.
	
	\bibitem{2016-AccurateImageSuperResolutionUsingVeryDeepConvolutionalNetworks-Kim-1646-1654}
	Jiwon Kim, Jung~Kwon Lee, and Kyoung~Mu Lee.
	\newblock Accurate image super-resolution using very deep convolutional
	networks.
	\newblock In {\em CVPR}, pages 1646--1654, 2016.
	
	\bibitem{2017-ImageSuperResolutionViaDeepRecursiveResidualNetwork-Tai-2790-2798}
	Ying Tai, Jian Yang, and Xiaoming Liu.
	\newblock Image super-resolution via deep recursive residual network.
	\newblock In {\em CVPR}, pages 2790--2798, 2017.
	
	\bibitem{2018-ResidualDenseNetworkforImageSuperResolution-Zhang--}
	Yulun Zhang, Yapeng Tian, Yu~Kong, Bineng Zhong, and Yun Fu.
	\newblock Residual dense network for image super-resolution.
	\newblock In {\em CVPR}, 2018.
	
	\bibitem{2015-VideoSuperResolutionViaDeepDraftEnsembleLearning-Liao-531-539}
	Renjie Liao, Xin Tao, Ruiyu Li, Ziyang Ma, and Jiaya Jia.
	\newblock Video super-resolution via deep draft-ensemble learning.
	\newblock In {\em ICCV}, pages 531--539, 2015.
	
	\bibitem{2015-LearningaDeepConvolutionalNetworkforLightFieldImageSuperResolution-Yoon-57-65}
	Youngjin Yoon, Hae{-}Gon Jeon, Donggeun Yoo, Joon{-}Young Lee, and In~So Kweon.
	\newblock Learning a deep convolutional network for light-field image
	super-resolution.
	\newblock In {\em ICCV Workshops}, pages 57--65, 2015.
	
	\bibitem{2018-LightFieldImageSuperresolutionUsingaCombinedDeepCNNBasedonEPI-Yuan-1359-1363a}
	Yan Yuan, Ziqi Cao, and Lijuan Su.
	\newblock Light-field image superresolution using a combined deep {CNN} based
	on {EPI}.
	\newblock {\em {IEEE} Signal Process. Lett.}, 25(9):1359--1363, 2018.
	
	\bibitem{2018-LFNet:aNovelBidirectionalRecurrentConvolutionalNeuralNetworkforLightFieldImageSuperResolution-Wang-4274-4286}
	Yunlong Wang, Fei Liu, Kunbo Zhang, Guangqi Hou, Zhenan Sun, and Tieniu Tan.
	\newblock Lfnet: {A} novel bidirectional recurrent convolutional neural network
	for light-field image super-resolution.
	\newblock {\em {IEEE} Trans. Image Processing}, 27(9):4274--4286, 2018.
	
	\bibitem{2010-ResolutionEnhancementinMultiImageStereo-Bhavsar-1721-1728}
	Arnav~V. Bhavsar and A.~N. Rajagopalan.
	\newblock Resolution enhancement in multi-image stereo.
	\newblock {\em {IEEE} Trans. Pattern Anal. Mach. Intell.}, 32(9):1721--1728,
	2010.
	
	\bibitem{2017-PWCNet:CNNsforOpticalFlowUsingPyramidWarpingandCostVolume-Sun--}
	Deqing Sun, Xiaodong Yang, Ming{-}Yu Liu, and Jan Kautz.
	\newblock {PWC}-{N}et: {CNN}s for optical flow using pyramid, warping, and cost
	volume.
	\newblock In {\em CVPR}, 2017.
	
	\bibitem{2017-AccurateOpticalFlowViaDirectCostVolumeProcessing-Xu-5807-5815}
	Jia Xu, Rene Ranftl, and Vladlen Koltun.
	\newblock Accurate optical flow via direct cost volume processing.
	\newblock In {\em CVPR}, pages 5807--5815, 2017.
	
	\bibitem{2015-DRAW:aRecurrentNeuralNetworkforImageGeneration-Gregor-1462-1471}
	Karol Gregor, Ivo Danihelka, Alex Graves, Danilo~Jimenez Rezende, and Daan
	Wierstra.
	\newblock {DRAW:} {A} recurrent neural network for image generation.
	\newblock In {\em ICML}, volume~37, pages 1462--1471, 2015.
	
	\bibitem{2015-ShowAttendandTell:NeuralImageCaptionGenerationwithVisualAttention-Xu-2048-2057a}
	Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron~C. Courville, Ruslan
	Salakhutdinov, Richard~S. Zemel, and Yoshua Bengio.
	\newblock Show, attend and tell: Neural image caption generation with visual
	attention.
	\newblock In {\em ICML}, volume~37, pages 2048--2057, 2015.
	
	\bibitem{2017-AttentionIsAllYouNeed-Vaswani-6000-6010}
	Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
	Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
	\newblock Attention is all you need.
	\newblock In {\em NIPS}, pages 6000--6010, 2017.
	
	\bibitem{2018-SelfAttentionGenerativeAdversarialNetworks-Zhang--}
	Han Zhang, Ian~J. Goodfellow, Dimitris~N. Metaxas, and Augustus Odena.
	\newblock Self-attention generative adversarial networks.
	\newblock In {\em NIPS}, 2018.
	
	\bibitem{2018-DualAttentionNetworkforSceneSegmentation-Fu--}
	Jun Fu, Jing Liu, Haijie Tian, Zhiwei Fang, and Hanqing Lu.
	\newblock Dual attention network for scene segmentation.
	\newblock {\em arXiv preprint arXiv:1809.02983}, 2018.
	
	\bibitem{2018-NonLocalNeuralNetworks-Wang--}
	Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He.
	\newblock Non-local neural networks.
	\newblock In {\em CVPR}, 2018.
	
	\bibitem{2018-NonLocalRecurrentNetworkforImageRestoration-Liu--}
	Ding Liu, Bihan Wen, Yuchen Fan, Chen~Change Loy, and Thomas~S. Huang.
	\newblock Non-local recurrent network for image restoration.
	\newblock In {\em NIPS}, 2018.
	
	\bibitem{2018-MultiTaskLearningAsMultiObjectiveOptimization-Sener--}
	Ozan Sener and Vladlen Koltun.
	\newblock Multi-task learning as multi-objective optimization.
	\newblock In {\em NIPS}, 2018.
	
	\bibitem{2017-UnsupervisedMonocularDepthEstimationwithLeftRightConsistency-Godard-6602-6611}
	Cl{\'{e}}ment Godard, Oisin {Mac Aodha}, and Gabriel~J. Brostow.
	\newblock Unsupervised monocular depth estimation with left-right consistency.
	\newblock In {\em CVPR}, pages 6602--6611, 2017.
	
	\bibitem{2014-HighResolutionStereoDatasetswithSubpixelAccurateGroundTruth-Scharstein-31-42}
	Daniel Scharstein, Heiko Hirschm{\"{u}}ller, York Kitajima, Greg Krathwohl,
	Nera Nesic, Xi~Wang, and Porter Westling.
	\newblock High-resolution stereo datasets with subpixel-accurate ground truth.
	\newblock In {\em GCPR}, volume 8753, pages 31--42, 2014.
	
	\bibitem{2012-AreWeReadyforAutonomousDriving?theKITTIVisionBenchmarkSuite-Geiger-3354-3361}
	Andreas Geiger, Philip Lenz, and Raquel Urtasun.
	\newblock Are we ready for autonomous driving? the {KITTI} vision benchmark
	suite.
	\newblock In {\em CVPR}, pages 3354--3361, 2012.
	
	\bibitem{2015-ObjectSceneFlowforAutonomousVehicles-Menze-3061-3070}
	Moritz Menze and Andreas Geiger.
	\newblock Object scene flow for autonomous vehicles.
	\newblock In {\em CVPR}, pages 3061--3070, 2015.
	
	\bibitem{2015-Adam:aMethodforStochasticOptimization-Kingma--}
	Diederik~P. Kingma and Jimmy Ba.
	\newblock Adam: {A} method for stochastic optimization.
	\newblock In {\em ICLR}, 2015.
	
	\bibitem{2016-DeeplyRecursiveConvolutionalNetworkforImageSuperResolution-Kim-1637-1645}
	Jiwon Kim, Jung~Kwon Lee, and Kyoung~Mu Lee.
	\newblock Deeply-recursive convolutional network for image super-resolution.
	\newblock In {\em CVPR}, pages 1637--1645, 2016.
	
	\bibitem{2018-FastAccurateAndLightweightSuperResolutionwithCascadingResidualNetwork-Ahn--}
	Namhyuk Ahn, Byungkon Kang, and Kyung{-}Ah Sohn.
	\newblock Fast, accurate, and lightweight super-resolution with cascading
	residual network.
	\newblock In {\em ECCV}, 2018.
	
	\bibitem{2017-EnhancedDeepResidualNetworksforSingleImageSuperResolution-Lim--}
	Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung~Mu Lee.
	\newblock Enhanced deep residual networks for single image super-resolution.
	\newblock In {\em CVPR Workshops}, 2017.
	
	\bibitem{2018-DeepBackProjectionNetworksforSuperResolution-Haris--}
	Muhammad Haris, Greg Shakhnarovich, and Norimichi Ukita.
	\newblock Deep back-projection networks for super-resolution.
	\newblock In {\em CVPR}, 2018.
	
\end{thebibliography}

\end{document}