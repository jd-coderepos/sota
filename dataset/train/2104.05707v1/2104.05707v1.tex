\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}       \usepackage{multirow}
\usepackage[dvipsnames]{xcolor}
\usepackage{subcaption}
\usepackage{overpic}
\usepackage{rotating}
\graphicspath{{images/}}


\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}

\def\vs{\emph{vs.\ }}
\def\eg{\emph{e.g.\ }}
\def\ie{\emph{i.e.\ }}
\def\cf{\emph{cf.\ }}
\def\etc{\emph{etc.\ }}
\def\wrt{wrt\ }
\def\etal{\emph{et al.\ }}

\def \alambic {\includegraphics[width=0.015\textwidth]{images/alembic-crop.pdf}\xspace}

\iccvfinalcopy 

\def\iccvPaperID{****} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}



\begin{document}

\title{LocalViT: Bringing Locality to Vision Transformers}




\author{Yawei Li\quad\quad Kai Zhang\quad\quad Jiezhang Cao\quad\quad Radu Timofte\quad\quad Luc Van Gool\\
Computer Vision Lab, ETH Zurich, Switzerland \qquad KU Leuven, Belgium\\
{\tt\small \{yawei.li, kai.zhang,  jiezhang.cao, timofter, vangool\}@vision.ee.ethz.ch}}



\maketitle


\begin{abstract}
We study how to introduce locality mechanisms into vision transformers. The transformer network originates from machine translation and is particularly good at modelling long-range dependencies within a long sequence. Although the global interaction between the token embeddings could be well modelled by the self-attention mechanism of transformers, what is lacking a locality mechanism for information exchange within a local region. Yet, locality is essential for images since it pertains to structures like lines, edges, shapes, and even objects. 

We add locality to vision transformers by introducing depth-wise convolution into the feed-forward network. This seemingly simple solution is inspired by the comparison between feed-forward networks and inverted residual blocks. The importance of locality mechanisms is validated in two ways: 1) A wide range of design choices (activation function, layer placement, expansion ratio) are available for incorporating locality mechanisms and all proper choices can lead to a performance gain over the baseline, and 2) The same locality mechanism is successfully applied to 4 vision transformers, which shows the generalization of the locality concept. In particular, for ImageNet2012 classification, the locality-enhanced transformers outperform the baselines DeiT-T~\cite{touvron2020training} and PVT-T~\cite{wang2021pyramid} by 2.6\% and 3.1\% with a negligible increase in the number of parameters and computational effort. Code is available at \url{https://github.com/ofsoundof/LocalViT}.
\end{abstract}

\section{Introduction}
\label{sec:introduction}


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{images/teaser.png}
    \caption{Comparison between LocalViT and the baseline transformers. The transformers enhanced by the proposed locality mechanism outperform their baselines.}
    \label{fig:teaser}
\end{figure}


Convolutional neural networks (CNNs) now define the state-of-the-art for computer vision tasks such as image classification~\cite{krizhevsky2012imagenet,simonyan2014very,he2016deep,huang2017densely}, object detection~\cite{girshick2014rich,ren2015faster}, segmentation~\cite{long2015fully,he2017mask}, low-level vision~\cite{dong2014learning,zhang2017beyond}, \etc CNNs are based on locality in that convolutional filters only perceive a local region of the input image, \ie the receptive field. By  stacking multiple layers, the effective receptive fields of a deep neural network can be enlarged progressively. This design enables the network to learn a hierarchy of deep features, which is essential for the success of CNNs. Meanwhile, the local, repetitive connections save many parameters compared with fully connected layers.
Yet, one problem is that a larger receptive field can only be achieved by combining layers, despite alternative attempts at enlarging the receptive field~\cite{yu2015multi}.

A parallel, thriving research strand incorporates global connectivity into the network via self-attention~\cite{vaswani2017attention,wang2020hat,mehta2020delight,wu2020lite}. This family of networks, \ie transformer networks, originates from machine translation and is very good at modelling long-range dependencies in sequences. There also is a rising interest in applying transformers to vision~\cite{carion2020end,dosovitskiy2020image,touvron2020training}. Vision transformers have already achieved performances quite competitive with their CNN counterparts. 

To process 2D images with transformers, the input image is first converted to a sequence of tokens which correspond to patches in the image. Then the attention module attends to all tokens and a weighted sum is computed as the tokens for the next layer.
In this way, the effective receptive field is expanded to the whole image via a single self-attention layer. Yet, the problem of visual transformers is that global connectivity contradicts the convolutional idea.

Considering the merits of CNNs vs. transformers, a natural question is \textit{whether we can efficiently combine the locality of CNNs and the global connectivity of vision transformers to improve performance while not increasing model complexity.} 

We try to fill the gap between CNNs and vision transformers. Specifically, we introduce a locality mechanism to the feed-forward network of transformers, which is inspired by examining the feed-forward network and inverted residuals~\cite{sandler2018mobilenetv2,howard2019searching}. The feed-forward network of transformers consists of two fully connected layers and the hidden dimension between them is expanded (usually by a factor of 4) to extract richer features. 
Similarly, in inverted residual blocks, the hidden channel between the two  convolutions is also expanded. The major difference between them is the efficient depth-wise convolution in the inverted residual block. 
Such depth-wise convolution can provide precisely the mechanism for local information aggregation which is missing in the feed-forward network of vision transformers.
In addition, depth-wise convolution is efficient in both parameters and computational complexity. 
 
To cope with the 2D depth-wise convolution, the image tokens of the sequence from the self-attention module must be rearranged to a 2D feature map, which is processed by the feed-forward network. The class token is split out and bypasses the feed-forward network. The derived new feature map is converted back to image tokens and concatenated with the bypassed class token. The concatenated sequence is processed by the next transformer layer. 

The effectiveness of the introduced locality mechanism is validated in two ways. Firstly, its properties are investigated experimentally. We draw four basic conclusions. \textit{i.} Depth-wise convolution alone can already improve the performance of the baseline transformer. \textit{ii.} A better activation function after depth-wise convolution can result in a significant performance gain. \textit{iii.} The locality mechanism is more important for lower layers. \textit{iv.} Expanding the hidden dimension of the feed-forward network leads to a larger model capacity and a higher classification accuracy. Secondly, as shown in Fig.~\ref{fig:teaser}, the locality mechanism is successfully applied to 4 vision transformers, which underlines its generality.  
The contributions of this paper are three-fold:
\begin{enumerate}
    \item We bring a locality mechanism to vision transformers by introducing depth-wise convolutions. The new transformer architecture combines a self-attention mechanism for global relationship modelling and a locality mechanism for local information aggregation.
    \item We analyze the basic properties of the introduced locality mechanism. The influence of each component (depth-wise convolution, non-linear activation function, layer placement, and hidden dimension expansion ratio) is singled out.
    \item We apply these ideas to vision transformers incl. DeiT~\cite{touvron2020training}, T2T-ViT~\cite{yuan2021tokens}, PVT~\cite{wang2021pyramid}, and TNT~\cite{han2021transformer}. Experiments show that the simple technique proposed in this paper generalizes well to various transformer architectures.
\end{enumerate}

\section{Related Work}
\label{sec:related_works}


\subsection{Transformers and vision transformers}
\label{subsec:transformers}

Transformers were first introduced in~\cite{vaswani2017attention} for machine translation. The proposed attention mechanism aggregates information from the whole input sequence. Thus, transformers are especially good at modelling long-range dependencies between elements of a sequence. Since then, there have been several attempts to adapt transformers towards vision tasks including object detection~\cite{carion2020end,zhu2020deformable}, image classification~\cite{dosovitskiy2020image,touvron2020training,yuan2021tokens,wang2021pyramid,han2021transformer}, segmentation~\cite{wang2020axial}, multiple object tracking~\cite{sun2020transtrack,meinhardt2021trackformer}, human pose estimation~\cite{yang2020transpose,zheng20213d}, point cloud processing~\cite{guo2020pct,zhao2020point}, video processing~\cite{girdhar2019video,neimark2021video,tan2021relaxed}, image super-resolution~\cite{mou2021cola,yang2020learning,chen2020pre}, image synthesis~\cite{esser2020taming}, etc. An extensive review is out of the scope of this paper. We focus on the most relevant works. 

Carion \etal first proposed a detection transformer (DETR) for end-to-end objection detection~\cite{carion2020end}. This method regards object detection as a set prediction problem and removes the hand-crafted designs for objection detection. DETR reasons about the relationship between the learned object queries and global image context. Following this work, image classification was targeted. Dosovitskiy \etal showed that a pure transformer can be directly applied to images and performs quite well compared with CNNs on image classification~\cite{dosovitskiy2020image}. Yet, this network relies heavily on large-scale models and datasets. Thus, Touvron \etal showed that it is possible to train vision transformers in a data-efficient way~\cite{touvron2020training}. The authors introduced an additional distillation token to the network and proposed hard-label distillation for vision transformers. Such transformers are identical to those for machine translation. Recent works propose to adapt transformers to images. Yuan \etal proposed a progressive tokenization method that can model the local information of nearby tokens and reduce the number of tokens. Wang \etal propose a pyramid architecture for vision transformers~\cite{wang2021pyramid}. Han \etal introduced an additional transformer block for the image token embeddings~\cite{han2021transformer}.
 
\subsection{Locality \vs global connectivity}

Both local information and global connectivity help to reason about the relationships between  image contents. Thus, they are both important for visual perception.
The convolution operation applies a sliding window to the input and local information is inherently aggregated to compute new representations. Thus, locality is an intrinsic property of CNNs~\cite{lecun1995convolutional}. 
Although CNNs can extract information from a larger receptive field by stacking layers and forming deep networks, they still lack global connectivity~\cite{krizhevsky2012imagenet,simonyan2014very,he2016deep}. To overcome this problem, some researchers add global connectivity to CNNs with non-local blocks~\cite{wang2018non,liu2018non}. 

By contrast, transformers are especially good at modelling long-range dependencies within a a sequence owing to their attention mechanism~\cite{vaswani2017attention,devlin2018bert,choromanski2020rethinking,kitaev2020reformer}. But, in return, a locality mechanism remains to be added for visual perception. Some works already contributed towards this goal~\cite{yuan2021tokens,liu2021swin}. Yet, they mainly focus on improving the tokenization and self-attention part. Other work introduces hybrid architectures of CNNs and transformers~\cite{carion2020end,srinivas2021bottleneck,li2021bossnas}. In summary, little attention has been paid to the feed-forward network of vision transformers. 


\subsection{Depth-wise convolution and inverted residuals}

Compared with normal convolution, the computations of depth-wise convolution are only conducted channel-wise. That is, to obtain a channel of the output feature map, the convolution is only conducted on one input feature map. Thus, depth-wise convolution is efficient both in terms of parameters and computation. Thus, Howard \etal first proposed the MobileNet architecture based on depth-wise separable convolutions~\cite{howard2017mobilenets}. This lightweight and computationally efficient network is quite friendly for mobile devices. Since then, depth-wise convolution has been widely used to design efficient models.
Inverted residual blocks are based on depth-wise convolution and were first introduced in MobileNetV2~\cite{sandler2018mobilenetv2}. 
The inverted residual blocks are composed of a sequence of  - depth-wise - convolutions. The hidden dimension between the two  convolutions is expanded. The utilization of depth-wise convolution avoids the drastic increase of model complexity brought by normal convolution. Due to the efficiency of this module, it is widely used to form the search space of neural architecture search (NAS)~\cite{howard2019searching,tan2019mnasnet,tan2019efficientnet,guo2020single,cai2018proxylessnas}. The expansion of the hidden dimension of inverted residuals is quite similar to the feed-forward network of vision transformers. This motivates us to think about the connection between them (See Sec.~\ref{subsec:locality}).


\begin{figure}[!tbp]
\begin{center}
\begin{overpic}[width=0.48\textwidth]{fig1.pdf}
\put(3.5,9.2){\color{black}{}}
\put(2.0,37){\color{black}{}}
\put(3.5,62){\color{black}{}}

\put(23.0,21){\color{black}{\footnotesize Fully Connected}}
\put(23.0,51.5){\color{black}{\footnotesize Fully Connected}}

\put(76.0,24){\color{black}{\footnotesize  Conv}}
\put(76.0,55){\color{black}{\footnotesize  Conv}}

\put(27,3){\color{black}{\footnotesize (a)}}
\put(73.0,3){\color{black}{\footnotesize (b)}}

\end{overpic}\vspace{-0.4cm}
\end{center}
\caption{Visualization of the feed-forward network in transformers from different perspectives. (a) The input is regarded as a sequence of tokens.
(b) An equivalent perspective is to still rearrange the tokens as a 2D lattice.}
\label{fig:feed_forward}
\end{figure}


\section{Methodology}
\label{sec:methodology}


Transformers are usually composed of encoders and decoders with similar building blocks. For the image classification task considered here, only the encoders are included in the network. Thus, we mainly describe the operations in the encoder layers. The encoders have two components, \ie the self-attention mechanism that relates a token to all of the tokens and a feed-forward network that is applied to every token. We specifically explain how to introduce locality into the feed-forward network.


\subsection{Input interpretation}
\label{subsec:input_interpretation}


\textbf{Sequence perspective.}
Inherited from language modelling, transformers regard the input as a sequence that contains elements of embedded vectors. Consider an input image , where  and  denote the channel and spatial dimension of the input image, respectively. The input image is first converted to a sequence of tokens , where  is the embedding dimension and . The tokens can be aggregated into a matrix .

\textit{Self-attention.} 
In the self-attention mechanism, the relationship between the tokens is modelled by the similarity between the projected query-key pairs, yielding the attention score. The new tokens are computed as the weighted sum of the project values. That is,

where the  function is applied to the rows of the similarity matrix and  provides a normalization. 
The query, key, and value are a projection of the tokens, \ie , , . The projection matrices  and  have the same size while  could have a different size. In practice, the three projection matrices usually have the same size, \ie .

\textit{Feed-forward network.}
After the self-attention layer, a feed-forward network is appended. The feed-forward network consists of two fully-connected layers and transforms the features along the embedding dimension. The hidden dimension between the two fully-connected layers is expanded to learn a richer feature representation. That is,

where , , and  denotes a non-linear activation function. For the sake of simplicity, the bias term is omitted. The dimension expansion ratio  is usually set to 4. As shown in Fig.~\ref{fig:feed_forward} (a), the input to the feed-forward network is regarded as a sequence of embedding vectors. 

\textbf{Lattice perspective.} Since the feed-forward network is applied position-wise to a sequence of tokens , an exactly equivalent representation is to rearrange the sequence of tokens into a 2D lattice as shown in Fig.~\ref{fig:feed_forward} (b). Then the reshaped feature representation is 

where  and .
The operation  converts a sequence to a 2D feature map.
Each token is placed to a pixel location of the feature map.
The benefit of this perspective is that the proximity between tokens is recovered, which provides the chance to introduce locality into the network. The fully-connected layers could be replaced by  convolutions, \ie

where  and  are reshaped from  and  and represent the convolutional kernels. The operation  converts the image feature map back to a token sequence which is used by the next self-attention layer.

\subsection{Locality}
\label{subsec:locality}

Since only  convolution is applied to the feature map, there is a lack of information interaction between adjacent pixels. Besides, the self-attention part of the transformer only captures global dependencies between all of the tokens. Thus, the transformer block does not have a mechanism to model the local dependencies between nearby pixels. It would be interesting if locality could be brought to transformers in an efficient way. 

The expansion of the hidden dimension between fully-connected layers and the lattice perspective of the feed-forward network remind us of the inverted residual block proposed in MobileNets~\cite{sandler2018mobilenetv2,howard2019searching}. As shown in Fig.~\ref{fig:locality}, both of the feed-forward network and the inverted residual expand and squeeze the hidden dimension by  convolution. The only difference is that there is a depth-wise convolution in the inverted residual block. Depth-wise convolution applies a  () convolution kernel per channel. The features inside the  kernel is aggregated to compute a new feature. Thus, depth-wise convolution is an efficient way of introducing locality into the network. Considering that, we reintroduce depth-wise convolution into the feed-forward network of transformers. And the computation could be represented as   

where  is the kernel of the depth-wise convolution. The finally used network is shown in Fig.~\ref{fig:locality} (c). The input, \ie a sequence of tokens is first reshaped to a feature map rearranged on a 2D lattice. Then two  convolutions along with a depth-wise convolution are applied to the feature map. After that, the feature map is reshaped to a sequence of tokens which are used as by the self-attention of the network transformer layer.

Note that the non-linear activation functions are not visualized in Fig.~\ref{fig:locality}. Yet, they play a quite important role in enhancing the network capacity, especially for efficient networks. In particular, we try ReLU6, h-swish~\cite{howard2019searching}, squeeze-and-excitation (SE) module~\cite{hu2018squeeze}, efficient channel attention (ECA) module~\cite{wang2020efficient}, and their combinations. A thorough analysis of the activation function is discussed in the experiments section.  




\begin{figure}[!tbp]
\begin{center}
\begin{overpic}[width=0.48\textwidth]{fig2.pdf}
\put(10,20){\color{black}{\footnotesize  Conv}}
\put(10,40){\color{black}{\footnotesize  Conv}}

\put(43,20){\color{black}{\footnotesize  Conv}}
\put(40,29.2){\color{black}{\footnotesize  DW Conv}}
\put(43,40){\color{black}{\footnotesize  Conv}}

\put(78.5,10.2){\color{black}{\footnotesize Seq2Img}}
\put(76.5,20){\color{black}{\footnotesize  Conv}}
\put(73,29.2){\color{black}{\footnotesize  DW Conv}}
\put(76.5,40){\color{black}{\footnotesize  Conv}}
\put(78.5,49.2){\color{black}{\footnotesize Img2Seq}}

\put(16.4,0.3){\color{black}{\footnotesize (a)}}
\put(49.0,0.3){\color{black}{\footnotesize (b)}}
\put(82.8,0.3){\color{black}{\footnotesize (c)}}

\end{overpic}
\end{center}\vspace{-0.3cm}
\caption{Comparison between the (a) convolutional version of the feed-forward network in vision transformers, the (b) inverted residual blocks, and (c) the finally utilized network that brings locality mechanism into transformers. ``DW'' denotes depth-wise convolution. To cope with the convolution operation, the conversion between sequence and image feature map is added by ``Seq2Img'' and ``Img2Seq'' in (c).}
\label{fig:locality}
\vspace{-0.6cm}
\end{figure}










\subsection{Class token}
\label{subsec:class_token}

To apply vision transformers to image classification, a trainable class token is added and inserted into the token embedding, \ie 

where  denotes the assignment operation,  is the class token. The new matrix has the dimension of  and  tokens. In the self-attention module, the class token exchanges information with all other image tokens and gathers information for the final classification. In the feed-forward network, the same transformation is applied to the class token and the image tokens. 

When depth-wise convolution is introduced into the feed-forward network, the sequence of tokens needs to be rearranged into an image feature map. Yet, the additional dimension brought by the class token makes the exact rearrangement impossible. To circumvent this problem, we split the  tokens in Eqn.~(\ref{eqn:attention}) into a class token and image tokens again, \ie
 
Then the new image token is passed through the feed-forward network according to Eqns.~(\ref{eqn:token_reshape}), (\ref{eqn:feed_forward_depth}), and (\ref{eqn:token_reshape_back}), leading to . The class token is not passed through the feed-forward network. Instead, it is directly concatenated with , \ie 

The split and concatenation of the class token is done for every transformer layer. Although the class token  is not passed through the feed-forward network, the performance of the overall network is not adversely affected. This is because the information exchange and aggregation is done only in the self-attention part. A feed-forward network like Eqn.~(\ref{eqn:feed_forward}) only enforces a transformation within each token.

\begin{table}[!t]
    \small
    \begin{center}
        \begin{tabular}{l|c|c|c|c|l}
            \toprule
            \multirow{2}{*}{Network} & \multirow{2}{*}{} & \multirow{2}{*}{DW} & Params & FLOPs & \multicolumn{1}{c}{Top-1} \\ 
            & & & \multicolumn{1}{c|}{(M)} & \multicolumn{1}{c|}{(G)} & \multicolumn{1}{c}{Acc. (\%)} \\\midrule
            DeiT-T~\cite{touvron2020training}   & 4 & No & 5.7 & 1.3 & 72.2 \\
            LocalViT-T  & 4 & No  & 5.7 & 1.3 & 72.5 (\textcolor{OrangeRed}{0.3}) \\
            LocalViT-T* & 4 & Yes & 5.8 & 1.3 & 73.7 (\textcolor{OrangeRed}{1.5}) \\ \midrule
            DeiT-T~\cite{touvron2020training}   & 6 & No & 7.5 & 1.6 & 73.1\textdagger \\
            LocalViT-T  & 6 & No  & 7.5 & 1.6 & 74.3 (\textcolor{OrangeRed}{1.2}) \\
            LocalViT-T* & 6 & Yes & 7.7 & 1.6 & 76.1 (\textcolor{OrangeRed}{3.0}) \\ \bottomrule
        \end{tabular}
    \end{center}
\caption{Investigation of the locality brought by depth-wise convolution. A comparison is made with DeiT. ``DW'' denotes depth-wise convolution. ``Params'' denotes the number of parameters in the network. ``FLOPs'' denotes the number of floating-point operations. Top-1 accuracy is reported. The other tables use the same evaluation metric. *ReLU6 is used as the activation function after depth-wise convolution. \textdagger Results derived by modifying the DeiT architecture and training the network with the same training protocol.}
    \label{tbl:ablation_locality}
\end{table}

\section{Experimental Results}
\label{sec:experimental_results}


This section gives the experimental results for image classification. We first study how the locality brought by depth-wise convolution can improve the performance of transformers. Then we show the influence of a non-linear activation function placed after the depth-wise convolution. The layers that are equipped with locality also influence the network capacity and this is also studied. An ablation study of the hidden dimension expansion ratio  is also made. All those experiments are based on DeiT-T~\cite{touvron2020training}. Finally, the study on the generalization to other vision transformers including T2T-ViT~\cite{yuan2021tokens}, PVT~\cite{wang2021pyramid}, TNT~\cite{han2021transformer} for image classification and the comparison with CNNs are made. The transformers that are equipped with locality are denoted as LocalViT followed by the suffix that denotes the basic architecture. 



\subsection{Implementation details}
\label{subsec:implementation_details}

In order to introduce locality into transformers, we only adapt the feed-forward network of vision transformers while the other parts such as self-attention, and position encoding are not changed. The implementation is based on the inverted residual blocks~\cite{sandler2018mobilenetv2,howard2019searching}. A batch normalization layer is appended to the 2D convolutions. The layer normalization before the feed-forward network is removed. Path drop after the feed-forward network is also removed. The same modification is also applied to the T2T module of T2T-ViT~\cite{yuan2021tokens}. The feed-forward network of the inner transformer block in TNT~\cite{han2021transformer} is not adapted. For PVT~\cite{wang2021pyramid}, the class token is only introduced in the final stage of the pyramid. Thus, the split and concatenation of the class token for the feed-forward network is only applied in the final stage.

\textbf{Experimental setup.} The ImageNet2012 dataset~\cite{deng2009imagenet} is used in this paper. The dataset contains 1.28M training images and 50K validation images from one thousand classes. We follow the same training protocol as DeiT~\cite{touvron2020training}.
The input image is randomly cropped with size . Cross-entropy is used as the loss function. Label smoothing is used. The weight decay factor is set to 0.05. The AdamW optimizer is used with a momentum of 0.9. The training continues for 300 epochs. The batch size is set to 1024. The initial learning rate is set to 
and decreases to  following a cosine learning rate scheduler. During validation, a center crop of the validation images is conducted.
In the following sections, we study different aspects that could influence the performance of the introduced locality.


\begin{table}[!t]
    \small
    \begin{center}
        \begin{tabular}{l|c|c|l}
            \toprule
            \multirow{2}{*}{Activation}  & Params & FLOPs & \multicolumn{1}{c}{Top-1} \\ 
            & \multicolumn{1}{c|}{(M)} & \multicolumn{1}{c|}{(G)} & \multicolumn{1}{c}{Acc. (\%)} \\\midrule
            Deit-T~\cite{touvron2020training} & 5.7 & 1.3 & 72.2 \\ \midrule
            ReLU6           & 5.8 & 1.3 & 73.7 (\textcolor{OrangeRed}{1.5})\\
            h-swish         & 5.8 & 1.3 & 74.4 (\textcolor{OrangeRed}{2.2})\\
            h-swish + ECA   & 5.8 & 1.3 & 74.5 (\textcolor{OrangeRed}{2.3})\\
            h-swish + SE-192& 5.9 & 1.3 & 74.8 (\textcolor{OrangeRed}{2.6})\\
            h-swish + SE-96 & 6.0 & 1.3 & 74.8 (\textcolor{OrangeRed}{2.6})\\
            h-swish + SE-48 & 6.1 & 1.3 & 75.0 (\textcolor{OrangeRed}{2.8})\\
            h-swish + SE-4  & 9.4 & 1.3 & 75.8 (\textcolor{OrangeRed}{3.6})\\ \bottomrule
             
        \end{tabular}
    \end{center}
\caption{Investigation of the non-linear activation function. The combination of h-swish, ECA~\cite{wang2020efficient}, and SE~\cite{hu2018squeeze} is studied. ``SE-**'' means the reduction ratio in the squeeze-and-excitation module. The study is based on LocalViT-T.}
    \label{tbl:results_classification}
\end{table}

\begin{table}[!t]
    \small
    \begin{center}
        \begin{tabular}{c|l|c|c|c}
            \toprule
            \multirow{2}{*}{DW Placement} & \multirow{2}{*}{Layer} & Params & FLOPs & \multicolumn{1}{c}{Top-1} \\ 
            & & \multicolumn{1}{c|}{(M)} & \multicolumn{1}{c|}{(G)} & \multicolumn{1}{c}{Acc. (\%)} \\\midrule
High & 912 & 5.78 & 1.26 & 69.1 \\ 
            Mid & 58   & 5.78 & 1.26 & 72.1 \\ 
            Low & 14   & 5.78 & 1.26 & 73.1 \\ 
            Low & 18   & 5.84 & 1.27 & 74.0 \\ 
            All & 112  & 5.91 & 1.28 & 74.8 \\ \bottomrule
        \end{tabular}
    \end{center}
\caption{Influence of the placement of locality. ``All'' means all of the transformer layers are enhanced by depth-wise convolution. ``Low'', ``Mid'', and ``High'' mean the lower, middle, and higher transformer layers are equipped with depth-wise convolution, respectively. The study is based on LocalViT-T.}
    \label{tbl:placement}
    \vspace{-0.6cm}
\end{table}


\subsection{Influence of the locality}
We first study how the local information could help to improve the performance of vision transformers. Different hidden dimension expansion ratios  are investigated. First of all, due to the change of the operations in the feed-forward network (Sec.~\ref{subsec:implementation_details}), the Top-1 accuracy of LocalViT-T is slightly increased even without the depth-wise convolution. The performance gain is 0.3\% for  and is increased to 1.2\% for . Note that compared with DeiT-T, no additional parameters and computation are introduced for the improvement. When locality is incorporated into the feed-forward network, there is a significant improvement of the model accuracy, \ie 1.5\% for  and 3.0\% for . Compared with the baseline, there only is a marginal increase in the number of parameters and a negligible increase in the amount of computation. \textbf{Thus, the performance of vision transformers can be significantly improved by the incorporation of a locality mechanism and the adaptation of the operation in the feed-forward network.}

\begin{table}[!t]
    \small
    \begin{center}
        \begin{tabular}{c|c|c|c|c}
            \toprule
            \multicolumn{1}{c|}{Expansion} & \multirow{2}{*}{SE} & Params & FLOPs & \multicolumn{1}{c}{Top-1} \\ 
            \multicolumn{1}{c|}{Ratio } & & \multicolumn{1}{c|}{(M)} & \multicolumn{1}{c|}{(G)} & \multicolumn{1}{c}{Acc. (\%)} \\\midrule
            \multirow{2}{*}{1} & No & 3.1 & 0.7 & 65.9\\
            & Yes & 3.1 & 0.7 & 66.2\\ \midrule
            \multirow{2}{*}{2} & No & 4.0 & 0.9 & 70.1 \\
            & Yes & 4.0 & 0.9 & 70.6 \\ \midrule
            \multirow{2}{*}{3} & No & 4.9 & 1.1 & 72.9 \\
            & Yes & 5.0 & 1.1 & 73.1 \\ \midrule
            \multirow{2}{*}{4} & No & 5.8 & 1.3 & 74.4 \\
            & Yes & 5.9 & 1.3 & 74.8\\ \bottomrule
        \end{tabular}
    \end{center}
\caption{Investigating the expansion ratio of hidden layers in the feed-forward network.}
    \label{tbl:expansion_ratio}
\end{table}

\subsection{Activation functions}
The non-linear activation function after depth-wise convolution used in the above experiments is simply ReLU6. The benefit of using other non-linear activation functions is also studied. First of all, by replacing the activation function from ReLU6 to h-swish, the gain of Top-1 accuracy over the baseline is increased from 1.5\% to 2.2\%. This shows the benefit of h-swish activation functions can be easily extended from CNNs to vision transformers. Next, the h-swish activation function is combined with other channel attention modules including ECA~\cite{wang2020efficient} and SE~\cite{hu2018squeeze}. By adding ECA, the performance is further improved by 0.1\%. Considering that only 60 parameters are introduced, this improvement is still considerable under a harsh parameter budget. 

\begin{table*}[!t]
    \small
    \begin{center}
        \begin{tabular}{l|r|r|c|l|c}
            \toprule
Network & Image Size & Params (M) & FLOPs (G) & Top-1 Acc. (\%) & Top-5 Acc. (\%) \\
             \midrule
             \multicolumn{4}{c}{CNNs} \\ \midrule
             ResNet-18~\cite{he2016deep}   &  & 11.7 & 1.8 & 69.8 & 89.1 \\
             ResNet-50~\cite{he2016deep}   &  & 25.6 & 4.1 & 76.1 & 92.9 \\
             DenseNet-169~\cite{huang2017densely}       &  & 14.2 & 3.4 & 75.6 & 92.8\\
             RegNet-4GF~\cite{radosavovic2020designing} &  & 20.7 & 4.0 & 80.0 & -- \\
             MobileNetV1~\cite{howard2017mobilenets}    &  & 4.2 & 0.6 & 70.6 & -- \\
             MobileNetV2 (1.4)~\cite{sandler2018mobilenetv2} &  & 6.9 & 0.6 & 74.7 & -- \\
             EfficientNet-B0~\cite{tan2019efficientnet} &  & 5.3 & 0.4 & 77.1 & 93.3 \\
             EfficientNet-B4~\cite{tan2019efficientnet} &  & 19.3 & 4.5 & 82.9 & 96.4 \\ \midrule
             \multicolumn{4}{c}{Transformers} \\ \midrule
             DeiT-T~\cite{touvron2020training}  &   &5.7 & 1.3 &72.2 & 91.1\\
             LocalViT-T   &  & 5.9 & 1.3 & 74.8 (\textcolor{OrangeRed}{2.6}) & 92.6 \\
             DeiT-T~\cite{touvron2020training}  &   & 5.9 & 1.3 & 74.5 & --\\ \hline
             DeiT-S~\cite{touvron2020training}  &  & 22.1 & 4.6 &79.8 &95.1\\
             LocalViT-S   &  & 22.4 & 4.6 & 80.8 (\textcolor{OrangeRed}{1.0}) &95.4\\ 
             DeiT-S~\cite{touvron2020training}  &   & 22.4 & 4.6 & 81.2 &--\\  \midrule
             T2T-ViT-7~\cite{yuan2021tokens}    &  & 4.3 & 1.2 & 71.7 & --\\
             LocalViT-T2T &  & 4.3 & 1.2 & 72.5 (\textcolor{OrangeRed}{0.8}) &-- \\ \midrule
             TNT-T~\cite{han2021transformer}    &  & 6.1 & 1.4 & 73.6 & 91.9 \\
             LocalViT-TNT &  & 6.3 & 1.4 & 75.9 (\textcolor{OrangeRed}{2.3}) & 93.0 \\ \midrule
             PVT-T~\cite{wang2021pyramid}       &  & 13.2 & 4.7 & 75.1 & 92.3\\ 
             LocalViT-PVT &  & 13.5 & 4.8 & 78.2 (\textcolor{OrangeRed}{3.1}) & 94.2 \\  \bottomrule
             
        \end{tabular}
    \end{center}
    \vspace{-0.2cm}
    \caption{Image classification results for different CNNs and vision transformers. The locality functionality is enabled for four different vision transformers.}
    \label{tbl:generalization_results}
\end{table*}


Another significant improvement is brought by a squeeze-and-excitation module. When the reduction ratio in the squeeze-and-excitation module is reduced from 192 to 4, the gain of Top-1 accuracy is gradually increased from 2.6\% to 3.6\%. The number of parameters is also increased accordingly. Note that, for all of the networks, the computational complexity is almost the same. This implies that if there is no strict limitation on the number of parameters, advanced non-linear activation functions could be used. In the following experiments, we use the combination of h-swish and SE as the non-linear activation function after depth-wise convolution. Additionally, the reduction ratio of the squeeze-and-excitation module is chosen such that only 4 channels are kept after the squeeze operation. This choice of design achieves a good balance between the number of parameters and the model accuracy.
\textbf{Thus, local information is also important in vision transformers. A wide range of efficient modules could be introduced into the feed-forward network of vision transformers to expand the network capacity.}


\subsection{Placement of locality}
The transformer layers where the locality is introduced can also influence the performance of the network. Thus, an ablation study based on LocalViT-T is conducted to study their effect. The results is reported in Table~\ref{tbl:placement}. There are in total 12 transformer layers in the network. We divide the 12 layers into 3 groups corresponding to ``Low'', ``Mid'', and ``High'' stages. For the former 3 rows of Table~\ref{tbl:placement}, we independently insert locality into the three stages. As the locality is moved gradually from lower stages to the higher stages, the accuracy of the network is decreased. This shows that local information is especially important for the lower layers. This is also consistent with our intuition. That is, when the depth-wise convolution is applied to the lower layers, the local information aggregated there could also be propagated to the higher layers, which is important to improve the overall performance of the network. 



When the locality is introduced only in the higher stage, the Top-1 accuracy is even lower than DeiT-T. To investigate whether locality in the higher layers always has an adverse effect, we progressively allow more lower layers to have depth-wise convolution until locality is enabled for all layers. This corresponds to the last three rows of Table~\ref{tbl:placement}. The observation is that starting from the lower layers, the performance of the network could be gradually improved as locality is enabled for more layers. \textbf{Thus, introducing the locality to the lower layers is more advantageous compared with higher layers.}


\begin{figure*}
    \centering
    \begin{subfigure}[t]{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/DeiT-T_LocalViT-T.png}
        \caption{{DeiT-T \vs LocalViT-T. Accuracy.}}
        \label{fig:accuracy1}
    \end{subfigure}
    \begin{subfigure}[t]{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/PVT-T_LocalViT-PVT.png}
        \caption{{PVT-T \vs LocalViT-PVT. Accuracy.}}
        \label{fig:accuracy1}
    \end{subfigure}
    \begin{subfigure}[t]{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/TNT-T_LocalViT-TNT.png}
        \caption{TNT-T \vs LocalViT-TNT. Accuracy.}
        \label{fig:accuracy3}
    \end{subfigure}
    
    \begin{subfigure}[t]{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/Training_lossDeiT-T_LocalViT-T.png}
        \caption{DeiT-T \vs LocalViT-T. Training loss.}
        \label{fig:training_loss1}
    \end{subfigure}
    \begin{subfigure}[t]{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/Training_lossPVT-T_LocalViT-PVT.png}
        \caption{PVT-T \vs LocalViT-PVT. Training loss.}
        \label{fig:training_loss2}
    \end{subfigure}
    \begin{subfigure}[t]{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/Training_lossTNT-T_LocalViT-TNT.png}
        \caption{TNT-T \vs LocalViT-TNT. Training loss.}
        \label{fig:training_loss3}
    \end{subfigure}
    \caption{Comparison of Top-1 accuracy, Top-5 accuracy, and training loss between the baseline transformers and the locality enhanced LocalViT. }
    \label{fig:training_log}
\end{figure*}



\subsection{Expansion ratio }

The effect of the expansion ratio of the hidden dimension of the feed-forward network is also investigated. The results are shown in Table~\ref{tbl:expansion_ratio}. Expanding the hidden dimension of the feed-forward network can have a significant effect on the performance of the transformers. As the expansion ratio is increased from 1 to 4, the Top-1 accuracy is increased from less than 70\% to nearly 75\%. The model complexity is also almost doubled. 
\textbf{Thus, the network performance and model complexity can be balanced by the hidden dimension expansion ratio . Squeeze-and-excitation can be more beneficial for smaller .}


\subsection{Generalization and comparison}

Finally, we try to generalize the knowledge derived above to more vision transformers including DeiT-S~\cite{touvron2020training}, T2T-ViT~\cite{yuan2021tokens}, TNT~\cite{han2021transformer}, PVT~\cite{wang2021pyramid}
and compare their performance with CNNs. The result is shown in Table~\ref{tbl:generalization_results}. 

We draw two major conclusions from Table~\ref{tbl:generalization_results}. \textbf{Firstly, the effectiveness of locality can be generalized to a wide range of vision transformers based on the following observations.} \textbf{\textit{I.}} Compared with DeiT, LocalViT can yield a higher classification accuracy for both the tiny and small version of the network. The increase of Top-1 accuracy is 2.6\% and 1.0\%, resp. LocalViT-T even outperforms DeiT-T which is enhanced by knowledge distillation from RegNetY-160~\cite{radosavovic2020designing}. The small version LocalViT-S is slightly worse than DeiT-S by 0.4\%. \textbf{\textit{II.}} LocalViT-T2T outperforms T2T-ViT-7 by 0.8\%. Note that T2T-ViT already tries to model the local structure information in the tokens-to-token module. \textbf{\textit{III.}} In TNT, an additional transformer block is used to extract local features for the image tokens. Thus, the locality is also considered in TNT. The modified network, \ie LocalViT-TNT could still improve the classification accuracy by a large margin of 2.3\%. \textbf{\textit{IV.}} The biggest improvement comes from PVT. Introducing the locality module leads to a gain of 3.1\% over PVT-T. \textbf{\textit{V.}} The comparison of the training log between the baseline transformers and LocalViT is shown in Fig.~\ref{fig:training_log}. During the training phase, LocalViT consistently leads to a lower training loss and higher validation accuracy than the baseline transformers.

\textbf{Secondly, some versions of the enhanced vision transformer LocalViT are already quite comparable or even outperform CNNs.} This conclusion can be drawn by making the pairwise comparison, \ie LocalViT-T \vs MobileNetV2 (1.4), LocalViT-S \vs ResNet-50, LocalViT-T2T \vs MobileNetV1, LocalViT-PVT \vs DenseNet-169 \etc




\section{Conclusion}
\label{sec:conclusion}


In this paper, we proposed to incorporate a locality mechanism into vision transformers. This is done by incorporating 2D depth-wise convolutions followed by a non-linear activation function into the feed-forward network of vision transformers. The idea is motivated by the comparison between the feed-forward network of transformers and the inverted residuals of MobileNets. In previous works, the input to the feed-forward network is a sequence of tokens embedding converted from an image. To cope with the locality mechanism, the sequence of tokens embedding is rearranged into a lattice as a 2D feature map, which is used as the input to the enhanced feed-forward network. To enable the rearrangement, the class token is split before the feed-forward network and concatenated with other image embeddings after the feed-forward network. A series of studies were made to investigate various factors (activation function, layer placement, and expansion ratio) that might influence of performance of the locality mechanism. The proposed locality mechanism is successfully applied to four different vision transformers, which validates its generality.



\vspace{0.1cm}
\noindent \textbf{Acknowledgements:} This work was partly supported by the ETH Z\"urich Fund (OK), a Huawei Technologies Oy (Finland) project, and an Amazon AWS grant.



{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
