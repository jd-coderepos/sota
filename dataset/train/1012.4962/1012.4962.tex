\documentclass[11pt,letterpaper]{article}

\usepackage{typearea}
\paperwidth 8.5in \paperheight 11in \typearea{16}


\usepackage{theorem,latexsym,graphicx}
\usepackage{amsmath,amssymb,enumerate}
\usepackage{xspace}
\usepackage{bm}
\usepackage{ifpdf}
\usepackage{color}
\usepackage[compact]{titlesec}
\usepackage{algorithm, algorithmic}
\usepackage{paralist}

\newcommand{\ignore}[1]{}


\definecolor{Darkblue}{rgb}{0,0,0.4}
\definecolor{Brown}{cmyk}{0,0.81,1.,0.60}
\definecolor{Purple}{cmyk}{0.45,0.86,0,0}

\newcommand{\mydriver}{hypertex} \ifpdf
 \renewcommand{\mydriver}{pdftex}
\fi
\usepackage[breaklinks,\mydriver]{hyperref}
\hypersetup{colorlinks=true,citebordercolor={.6 .6 .6},linkbordercolor={.6 .6 .6},citecolor=Darkblue,urlcolor=black,linkcolor=Darkblue,pagecolor=black}

\newcommand{\lref}[2][]{\hyperref[#2]{#1~\ref*{#2}}}

\makeatletter
 \setlength{\parindent}{0pt}
 \addtolength{\partopsep}{-2mm}
 \setlength{\parskip}{5pt plus 1pt}
 \addtolength{\theorempreskipamount}{-1mm}
 \addtolength{\theorempostskipamount}{-1mm}
 \addtolength{\abovedisplayskip}{-3mm}
\addtolength{\textheight}{30pt}
 \addtolength{\footskip}{-30pt}
\makeatother




\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{property}[theorem]{Property}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{example}[theorem]{Example}
\newtheorem{construction}[theorem]{Construction}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{reduction}[theorem]{Reduction}
\newtheorem{invariant}{Invariant}
\newtheorem{extension}[theorem]{Extension}


\allowdisplaybreaks

\newenvironment{Myquote}{\par\begingroup
\addtolength{\leftskip}{1em} \rightskip\leftskip }{\par
\endgroup
}

\def\bsq{\hfill}


\newenvironment{proof}{

\noindent{\bf Proof:}} {\hfill


}

\newenvironment{proofof}[1]{

\noindent{\bf Proof of {#1}:}} {\hfill


}

\newcommand{\junk}[1]{}

\newcommand{\R}[0]{{\ensuremath{\mathbb{R}}}}
\newcommand{\N}[0]{{\ensuremath{\mathbb{N}}}}
\newcommand{\Z}[0]{{\ensuremath{\mathbb{Z}}}}

\def\floor#1{\lfloor #1 \rfloor}
\def\ceil#1{\lceil #1 \rceil}
\def\seq#1{\langle #1 \rangle}
\def\set#1{\{ #1 \}}
\def\abs#1{\mathopen| #1 \mathclose|}   \def\norm#1{\mathopen\| #1 \mathclose\|}
\def\p {\ensuremath{\mathcal{P}}\xspace}
\def\flow {\ensuremath{f}\xspace}
\def\ti{\mathcal{I}}
\def\oi{\mathcal{J}}
\def\tf{\mathcal{T}}
\def\a{\ensuremath{\mathcal{A}}\xspace}
\def\rs{\mathcal{R}}
\def\cg{{\sf Cong}}
\def\g{\mathcal{G}}
\def\n{\mathcal{N}}

\def\f{\ensuremath {\mathcal{F}}\xspace}
\def\kn{\mathcal{K}}
\def\ms{\ensuremath{\mathcal{M}}\xspace}
\def\Sols{{\sf Sols}}
\def\opt{{\sf Opt}\xspace}
\def\aug{{\sf Aug}}
\def\cov{\ensuremath{\Pi}\xspace}
\def\rcov{{\sf Robust(\cov)}\xspace}
\def\robkcov{\ensuremath{{\sf Robust}}_k(\cov)\xspace}
\def\mm{{\sf MaxMin}\xspace}
\def\mmp{{\sf MaxMin(\cov)}\xspace}
\def\auga{{\sf Augment}\xspace}
\def\fst{\ensuremath {\Phi_T}\xspace}
\def\snd{\ensuremath {{\sf Augment}_T}\xspace}

\newcommand{\poly}{\operatorname{poly}}
\newcommand{\polylog}{\operatorname{polylog}}

\newcommand{\sse}{\subseteq}
\newcommand{\I}{{\Omega}}
\newcommand{\e}{\varepsilon}
\newcommand{\eps}{\varepsilon}
\newcommand{\eeminusone}{\frac{\mathrm{e}}{\mathrm{e}-1}}
\newcommand{\ts}{\textstyle}


\newcommand{\offline}{\alpha_{\sf off}}
\newcommand{\online}{\alpha_{\sf on}}
\newcommand{\optaug}{{\sf OptAug}}
\newcommand{\faug}{{\sf FracAug}}



\renewcommand{\theequation}{\thesection.\arabic{equation}}
\renewcommand{\thefigure}{\thesection.\arabic{figure}}

\newcounter{note}[section]
\renewcommand{\thenote}{\thesection.\arabic{note}}
\newcommand{\agnote}[1]{\refstepcounter{note}{\bf Anupam's Comment~\thenote:} {\sf #1}\marginpar{\tiny\bf AG~\thenote}}
\newcommand{\vnote}[1]{\refstepcounter{note}{\bf Viswa's Comment~\thenote:} {\sf #1}\marginpar{\tiny\bf VN~\thenote}}
\newcommand{\rnote}[1]{\refstepcounter{note}{\bf Ravi's Comment~\thenote:} {\sf #1}\marginpar{\tiny\bf RR~\thenote}}

\newcounter{myLISTctr}
\newcommand{\initOneLiners}{\setlength{\itemsep}{0pt}
    \setlength{\parsep }{0pt}
    \setlength{\topsep }{0pt}
}
\newenvironment{OneLiners}[1][\ensuremath{\bullet}]
    {\begin{list}
        {#1}
        {\initOneLiners}}
    {\end{list}}


\newcommand{\Tstar}{\ensuremath{T^*}\xspace}
\newcommand{\Phistar}{\ensuremath{\Phi^*}\xspace}
\def\calT{\mathcal{T}}

\usepackage{times}


\begin{document}

\title{Robust and MaxMin Optimization under \\Matroid and Knapsack
  Uncertainty Sets\thanks{An extended abstract containing the results of
    this paper and of~\cite{GNR-k-rob} appeared jointly in
    \emph{Proceedings of the 37th International Colloquium on Automata,
      Languages and Programming (ICALP), 2010}.}}

\author{
Anupam Gupta\thanks{Computer Science Department, Carnegie Mellon
    University, Pittsburgh, PA 15213, USA. Supported in part by
    NSF awards CCF-0448095 and CCF-0729022, and an Alfred P.~Sloan
    Fellowship. Email: anupamg@cs.cmu.edu}
\and Viswanath Nagarajan\thanks{IBM T.J. Watson Research Center, Yorktown Heights, NY 10598, USA. Email:
viswanath@us.ibm.com} \and R. Ravi\thanks{Tepper School of Business, Carnegie Mellon University,
  Pittsburgh, PA 15213, USA. Supported in part by NSF grant
  CCF-0728841. Email: ravi@cmu.edu}
}
\date{}
\maketitle

\begin{abstract}
  Consider the following problem: given a set system  and an edge-weighted
  graph  on the same universe , find the set  such
  that the Steiner tree cost with terminals  is as large as
  possible---``which set in  is the most difficult to connect up?''
  This is an example of a \emph{max-min problem}: find the set  such that the value of some minimization (covering) problem is as
  large as possible.

  In this paper, we show that for certain covering problems which admit
  good deterministic online algorithms, we can give good algorithms for
  max-min optimization when the set system  is given by a -system
  or knapsack constraints or both. This result is similar to results for constrained
  maximization of submodular functions. Although many natural covering problems
  are not even approximately submodular, we show that one can use
  properties of the online algorithm as a surrogate for submodularity.

  Moreover, we give stronger connections between max-min optimization
  and two-stage robust optimization, and hence give improved algorithms
  for robust versions of various covering problems, for cases where the
  uncertainty sets are given by -systems and  knapsacks.
\end{abstract}

\section{Introduction}

\newcommand{\maxf}{\textsf{Max-}\xspace}

Recent years have seen a considerable body of work on the problem of constrained submodular maximization: you are given
a universe  of elements, a collection  of ``independent'' sets and a submodular function , and the goal is to solve the optimization problem of maximizing  over the ``independent'' sets:

It is a classical result that when  is a linear function and  is a matroid, the greedy algorithm solves
this exactly. Furthermore, results from the mid-1970s tell us that even when  is monotone submodular and 
is a partition matroid, the problem becomes NP-hard, but the greedy algorithm is a -approximation---in
fact, greedy is a -approximation for monotone submodular maximization subject to \emph{any} matroid constraint.
Recent results have shed more light on this problem: it is now known that when  is a monotone submodular function
and  is a matroid, there exists a -approximation algorithm. We can remove the constraint of
monotonicity, and also generalize the constraint  substantially: the most general results say that if  is a
non-negative submodular function, and if  is a \emph{-system},\footnote{A -system is
  similar to, but more general than, the intersection of  matroids;
  it is formally defined in \lref[Section]{subsec:framework-mat}} then one
can approximate \maxf to within a factor of ; moreover, if  is the intersection of  knapsack
constraints then one can approximate \maxf to within a constant factor.

Given this situation, it is natural to ask: \emph{For which broad
  classes of functions can we approximately solve the \textsf{Max-}
  problem efficiently?} (Say, subject to constraints  that form a
-system, or given by a small number of knapsack constraints, or
both.) Clearly this class of functions includes submodular functions.
Does this class contain other interesting subclasses of functions which
are far from being submodular?

In this paper we consider the case of ``max-min optimization'': here 
is a monotone subadditive function defined by a minimization covering
problem, a natural subset of all subadditive functions. We show
conditions under which we can do constrained maximization over such
functions .  For example, given a set system , define the
``set cover'' function , where  is
the minimum number of sets from  that cover the elements in
. This function  is not submodular, and in fact, we can show
that there is no submodular function  such that  for sub-polynomial . (See
\lref[Section]{sec:lbd}.) Moreover, note that in general we cannot even
evaluate  to better than an -factor in polynomial
time.  However, our results imply  can indeed
be approximated well.  In fact, the result that one could approximately
maximize  subject to a cardinality constraint was given by Feige
et al.~\cite{FJMM07}; our results should be seen as building on their
ideas. (See also the companion paper~\cite{GNR-k-rob}.)

At a high level, our results imply that if a monotone function  is defined by a (minimization) covering problem, if
 is subadditive, and if the underlying (minimization) covering problem admits good deterministic online algorithms,
then there exist good approximation algorithms for \maxf subject to -systems and  knapsacks.  (All these terms
will be made formal shortly.) The resulting approximation guarantee for the max-min problem depends on the competitive
ratio of the online algorithm, and  and . Moreover, the approximation ratio improves if there is a better
algorithm for the offline minimization problem, or if there is a better online algorithm for a fractional version of
the online minimization problem.

\paragraph{Robust Optimization.}
Our techniques and results imply approximation algorithms for covering
problems in the framework of robust optimization as well. In the robust
optimization framework, there are two stages of decision making. E.g.,
in a generic robust optimization problem, one is not only given a set
system , but also an inflation parameter . Then
one wants to perform some actions in the first stage, and then given a
set  in the second stage, perform another set of actions
(which can now depend on ) to minimize
 subject to the constraint that
the two sets of actions ``cover'' the demand set . As an
example, in robust set cover, one is given another set system :
the allowed actions in the first and second stage are to pick some
sub-collections  and  respectively from , and the notion
of ``coverage'' is that the union of the sets in  must
contain . (If , actions are costlier in the second
stage, and hence there is a natural tension between waiting for the
identity of , and over-anticipating in the first stage without any
information about .)

Note that robust and max-min problems are related, at least in one direction: if , there is no incentive
to perform any actions in the first stage, in which case the robust problem degenerates into a max-min optimization
problem. In this paper, we show a reduction in the other direction as well---if one can solve the max-min problem well
(and if the covering problem admits a good deterministic online algorithm), then we get an algorithm for the robust
optimization version of the covering problem as well. The paper of Feige et al.~\cite{FJMM07} gave the first reduction
from the robust set-cover problem to the max-min set cover problem, for the special case when ; this
result was based on a suitable LP-relaxation. Our reduction extends this in two ways: (a) the constraint sets  can
now be -systems and  knapsacks, and (b) much more importantly, the reduction now applies not only to set cover,
but to many sub-additive monotone covering problems (those with deterministic online algorithms, as mentioned above).
Indeed, it is not clear how to extend the ellipsoid-based reduction of~\cite{FJMM07} even for the Steiner tree problem;
this was first noted by Khandekar et al.~\cite{KKMS08}.


\paragraph{Our Results and Techniques.} Our algorithm for the max-min
problem is based on the observation that the cost of a deterministic
online algorithm for the underlying minimization covering problem
defining  can be used as a surrogate for submodularity in certain
cases; specifically, we show that the greedy algorithm that repeatedly
picks an element maintaining membership in  and maximizing the cost
of the online algorithm gives us a good approximation to the max-min
objective function, as long as  is a -system.

We also show how to reduce the problem of maximizing such a function
over the intersection of  knapsacks to  runs of
approximately maximizing the function over a single partition matroid at
a loss of a factor of , or instead to
 runs of approximately maximizing over a different
partiton matroid at a loss of a factor of ---this
reduction is fairly general and is likely to be of interest in other
contexts as well. These results appear in \lref[Section]{sec:max-min}.

We then turn to robust optimization. In \lref[Section]{sec:gen-sets}, we
show that given a deterministic online algorithm for the covering
function , and an approximate max-min optimization algorithm for 
over a family , we get an algorithm for two-stage robust version of
the underlying covering problem with uncertainty set ---the
approximation guarantee depends on both the competitive ratio of the
online algorithm, as well as the approximation guarantee of the max-min
problem.

Note that we can combine this latter reduction (using max-min algorithms
to get robust algorithms) with our first reduction above (using online
algorithms to get max-min algorithms); in \lref[Section]{sec:combine}, we
give a more careful analysis that gives a better approximation than that
obtained by just naively combining the two theorems together.

Finally, in \lref[Section]{sec:lbd}, we show that some common covering
problems (vertex cover and set cover) give rise to functions  that
cannot be well-approximated (in a mutliplicative sense) by any
submodular function, but still admit good maximization algorithms by our
results in \lref[Section]{sec:max-min}.

\subsection{Related Work}
\label{sec:related-work}

Constrained submodular maximization problems have been very widely studied~\cite{NWF78I,NWF78II,S04,CCPV07,V08,KST09}.
However, as we mention above, the set cover and vertex cover functions are far from submodular. Interestingly, in a
recent paper on testing submodularity~\cite{SV10}, Seshadhri and Vondrak conjecture that the success of greedy
maximization algorithms may depend on a more general property than submodularity; this work provides further
corroboration for this, since we show that in our context online algorithms can serve as surrogates for submodularity.

Feige et al.~\cite{FJMM07} first considered the -max-min set cover subject to  (the
``cardinality-constrained'' case)---they gave an -approximation algorithm for the problem with  sets and  elements. They also
showed an  hardness of approximation for -max-min (and -robust) set cover. The
results in this paper build upon ideas in~\cite{FJMM07}, by handling more general covering problems and sets . To
the best of our knowledge, none of the -max-min problems other than min-cut have been studied earlier; note that the
min-cut function is submodular, and hence the associated max-min problem
can be solved using submodular maximization.

The study of approximation algorithms for robust optimization was initiated by Dhamdhere et al.~\cite{DGRS05,GGR06}:
they study the case when the scenarios were explicitly listed, and gave constant-factor approximations for several
combinatorial optimization problems. Again, the model with implicitly specified (and exponentially many) scenarios 
was considered in Feige et al.~\cite{FJMM07}, where they gave an -approximation for robust set cover
in the cardinality-constrained case . Khandekar et al.~\cite{KKMS08} noted that the techniques
of~\cite{FJMM07} did not seem to imply good results for Steiner tree, and developed new constant-factor approximations
for -robust versions of Steiner tree, Steiner forest on trees and facility location, again for the
cardinality-constrained case. We investigate many of these problems in the cardinality-constrained case of both the
max-min and robust models in the companion paper~\cite{GNR-k-rob}, and obtain approximation ratios better than the
online competitive factors. On the other hand, the goal in this paper is to give a framework for robust and max-min
optimization under general uncertainty sets.

\ignore{Considering the \emph{average} instead of the worst-case performance gives rise to the well-studied model of
stochastic optimization~\cite{RS04, IKMM04}.  Some common generalizations of the robust and stochastic models have been
considered (see, e.g., Swamy~\cite{Swamy08} and Agrawal et al.~\cite{ADSY09}).}




\section{Preliminaries}
\label{sec:prelim}

\subsection{Deterministic covering problems}

A covering problem \cov has a ground-set  of elements with costs , and  covering
requirements (often called demands or clients), where the solutions to the -th requirement is specified---possibly
implicitly---by a family  which is upwards closed (since this is a covering problem).
Requirement  is \emph{satisfied} by solution  iff .  The covering problem  involves computing a solution  satisfying all 
requirements and having minimum cost .  E.g., in set cover, ``requirements'' are items to be
covered, and ``elements'' are sets to cover them with. In Steiner tree, requirements are terminals to connect to the
root and elements are the edges; in multicut, requirements are terminal pairs to be separated, and elements are edges
to be cut.

The min-cost covering function associated with \cov is:


\subsection{Max-min problems} Given a covering problem  and a collection
 of ``independent sets'', the {\em max-min} problem \mmp involves finding a set 
for which the cost of the min-cost solution to  is maximized,



\subsection{Robust covering problems}
This problem, denoted \rcov, is a {\em two-stage optimization} problem, where elements are possibly bought in the first
stage (at the given cost) or the second stage (at cost  times higher). In the second stage, some subset
 of requirements (also called a \emph{scenario}) materializes, and the elements bought in both stages
must collectively satisfy each requirement in . Formally, the input to problem \rcov consists of (a) the
covering problem  as above, (b) an uncertainty set  of scenarios (possibly implicitly given), and (c)~an inflation parameter . A feasible solution
to \rcov is a set of {\em
  first stage elements}  (bought without knowledge of the
scenario), along with an {\em augmentation algorithm} that given any  outputs  such
that  satisfies all requirements in .  The objective function is  to minimize:

Given such a solution,  is called the first-stage cost and
 is the second-stage cost.


Note that by setting  in any robust covering problem, \emph{the optimal value of the robust problem equals
that of its corresponding max-min problem}.


As in~\cite{GNR-k-rob}, our algorithms for robust covering problems are based on the following type of guarantee.
In~\cite{GNR-k-rob} these were stated for -robust uncertainty sets, but they immediately extend to arbitrary
uncertainty sets.
\begin{definition}\label{defn:algo}
  An algorithm is \emph{-discriminating} iff
  given as input any instance of  and a threshold , the
  algorithm outputs
  \begin{inparaenum}[(i)]
  \item a set , and
  \item an algorithm ,
  \end{inparaenum}
  such that:
  \begin{OneLiners}
  \item[A.] For every scenario ,
    \begin{OneLiners}
    \item[(i)] the elements in  satisfy all
      requirements in , and
    \item[(ii)] the resulting augmentation cost
      .
    \end{OneLiners}
  \item[B.] Let  and  (respectively) denote the
    first-stage and second-stage cost of an optimal solution to the
     instance. If the threshold  then the first stage
    cost .
  \end{OneLiners}
\end{definition}

\begin{lemma}[\cite{GNR-k-rob}]\label{lem:apx}
  If there is an -discriminating algorithm
  for a robust covering problem , then for every 
  there is a -approximation algorithm for .
\end{lemma}

\subsection{Desirable Properties of the Covering Problem} We now formalize certain properties of the covering problem
 that are useful in obtaining our results.
Given a partial solution  and a set  of requirements, any set  such that  is called an \emph{augmentation} of  for requirements . Given , define
the min-cost augmentation of  for requirements  as:


Also define , for any .

An easy consequence of the fact that costs are non-negative is the following:
\begin{property}[Monotonicity]\label{ass:monotone}
For any requirements  and any solution , . Similarly, for any
 and solutions , .
\end{property}

From the definition of coverage of requirements, we obtain:
\begin{property}[Subadditivity]\label{ass:subadd}
  For any two subsets of requirements  and any partial
  solution , we have .
\end{property}
To see this property: if  and  are solutions corresponding to  and
 respectively, then   covers requirements ; so .

We assume two additional properties of the covering problem:
\begin{property}[Offline Algorithm]\label{ass:apx}
  There is an -approximation (offline) algorithm for the
  covering problem , for any  and .
\end{property}
\begin{property}[Online Algorithm]\label{ass:online}
  There is a polynomial-time deterministic -competitive algorithm for the online
  version of .\end{property}


\subsection{Models of Downward-Closed Families}
All covering functions we deal with are monotone non-decreasing. So we
may assume WLOG that the collection  in both \mmp and \rcov is
\emph{downwards-closed}, i.e.  and  
. In this paper we consider the following well-studied
classes:

\begin{definition}[-system]\label{defn:p-system} A downward-closed family  is called a -system iff:
 where  denotes the collection of {\em maximal subsets} in
. Sets in  are called {\em independent sets}. We assume access to a membership-oracle, that given any
subset  returns whether or not .
\end{definition}

\begin{definition}[-knapsack]\label{defn:q-knapsack}
Given  non-negative vectors  and capacities , the -knapsack constrained family is:

\end{definition}

These constraints model a rich class of downward-closed families. Some interesting special cases of -systems are
-matroid intersection~\cite{Schr-book} and -set packing~\cite{HS89,B00}; see the appendix in~\cite{CCPV07} for
more discussion on -systems. Jenkyns~\cite{J76} showed that the natural greedy algorithm is a -approximation for
maximizing linear functions over -systems, which is the best known result. Maximizing a linear function over
-knapsack constraints is the well-studied class of packing integer programs (PIPs), eg.~\cite{S99}. Again, the
greedy algorithm is known to achieve an -approximation ratio. When the number of constraints  is constant,
there is a PTAS~\cite{CK04-multidim}.




\section{Algorithms for Max-Min Optimization}
\label{sec:max-min}

In this section we give approximation algorithms for constrained max-min optimization, i.e. Problem~(\maxf) where 
is given by some underlying covering problem and  is given by some -system and -knapsack. We first consider
the case when  is a  -system. Then we show that any knapsack constraint can be reduced to a -system
(specifically a partition matroid) in a black-box fashion; this enables us to obtain an algorithm for  being the
intersection of a  -system and -knapsack. The results of this section assume Properties~\ref{ass:subadd}
and~\ref{ass:online}.

\subsection{Algorithm for -System Constraints}
The algorithm given below is a greedy algorithm, however it is relative to the objective of the online algorithm
 from Property~\ref{ass:online} rather than the (approximate) function value itself.
\begin{algorithm}
  \caption{Algorithm for \mmp under -system}
  \begin{algorithmic}[1]
\STATE \textbf{input:} the covering instance \cov that defines  and -system .

\STATE \textbf{let} current scenario , counter , input sequence .

\WHILE{( such that )}

    \STATE .

    \STATE \textbf{let} ,\,\, .

    \ENDWHILE

    \STATE \label{step:mm} \textbf{let}  be the independent set constructed by the above loop.

    \STATE \textbf{output} solution .
\end{algorithmic}
\end{algorithm}

\begin{theorem}\label{thm:maxmin-p-system}
Assuming Properties~\ref{ass:subadd} and~\ref{ass:online} there is a -approximation
algorithm for \mmp under -systems.
\end{theorem}
\begin{proof}
The proof of this lemma closely follows that in~\cite{CCPV07} for submodular maximization over a -system. We use
slightly more notation that necessary since this proof will be used in the next section as well.

Suppose that the algorithm performed  iterations; let  be the ordered set of elements added by
the algorithm. Define , , and  for
each . Note that . It suffices to show that:
 This would imply  for every
, and hence that  is the desired approximation.


We use the following claim proved in~\cite{CCPV07}, Appendix~B (this claim relies on the properties of a -system).

\begin{claim}[\cite{CCPV07}]\label{cl:ccpv}
For any , there is a partition  of  such that for all ,
\begin{enumerate}
 \item , and
 \item For every , we have .
\end{enumerate}
\end{claim}

For any sequence  of requirements and any  define . Note that this function depends on the particular online algorithm. From the second condition in
Claim~\ref{cl:ccpv}, it follows that each element of  was a feasible augmentation to  in
the  iteration of the {\bf while} loop. By the greedy choice,

Above equation~\eqref{eq:psys1} is by the definition of ,
equation~\eqref{eq:psys2} uses the subadditivity Property~\ref{ass:subadd}, and~\eqref{eq:psys3} is by the first
condition in Claim~\ref{cl:ccpv}.

Summing over all iterations , we obtain:
 where the last inequality follows from monotonicity since
   for all .

Using subadditivity~\lref[Property]{ass:subadd}, we get .

Let . i.e.  . Observe that
 is a feasible  augmentation to  that covers requirements . Thus,
 This completes the proof.
\end{proof}

\subsection{Reducing knapsack constraints to partition matroids}
\label{sec:knapsack-mat}

In this subsection we show that every knapsack constraint can be reduced to a suitable collection of partition
matroids. This property is then used to complete the algorithm for \mmp when  is given by a -system {\em
and} a -knapsack.
Observe that even a single knapsack constraint need not correspond exactly to a small -system: eg. the knapsack with
weights  and , and capacity one is only an -system (since both  and
 are maximal independent sets).
However we show that any knapsack constraint can be {\em approximately} reduced to a partition matroid (which is a
-system).
The main idea in this reduction is an enumeration method from Chekuri and Khanna~\cite{CK04-multidim}.

\begin{lemma}\label{lem:knap-to-mat}
  Given any knapsack constraint  and
  fixed , there is a polynomial-time computable
  collection  of  partition matroids such that:
  \begin{enumerate}
  \item For every , we have .
  \item .
  \end{enumerate}
\end{lemma}
\begin{proof}
Let  and . WLOG we assume that . Partition the
groundset  into  groups as follows.

Let  denote the number of non-negative integer partitions of  into  parts. Note that,


We will define a collection of  {\em partition matroids} on , each over the partition . For any integer partition  of  (i.e.  are integers and
), define a partition matroid  that has bounds  on each part
, where


Clearly this collection can be constructed in polynomial time for fixed . We now show that this collection of
partition matroids satisfies the two properties in the lemma.

{\bf (1)} Consider any  that is feasible for some partition matroid, say . The total weight of
elements  is at most . For any group , the weight of elements
 is at most:
 Hence the total weight of all elements in  is at most:

Above we use . Finally since , we obtain the first condition.

{\bf (2)} Consider  any  that satisfies the knapsack constraint, i.e. . We will show
that  is feasible in , for some integer partition  of  as above. For each
 let  denote the weight of elements in , and  be the unique integer that satisfies
. Define  to be the integer partition
. We have , which follows from the fact . By increasing s arbitrarily so that they total to , we obtain  a
feasible integer partition .
We now claim that  is feasible for . Since each element of  has weight at least , we have
 Since  is integral, we obtain . Thus we obtain the second condition.
\end{proof}

\subsection{Algorithm for -System and -Knapsack Constraints}\label{subsec:mm-knap-psystem}
Here we consider \mmp when  is the intersection of -system \ms and a -knapsack (as in
Definition~\ref{defn:q-knapsack}). The idea is to reduce the -knapsack to a single knapsack (losing factor ), then use Lemma~\ref{lem:knap-to-mat} to reduce the knapsack to a 1-system, and finally apply
Theorem~\ref{thm:maxmin-p-system} on the resulting  system. Details appear below.

By scaling weights in the knapsack constraints, we may assume WLOG that each knapsack has capacity exactly one; let
 denote the weights in the  knapsack constraints. We also assume WLOG that each singleton element
satisfies the -knapsack; otherwise such elements can be dropped from the groundset.

\begin{algorithm}
\caption{Algorithm for \mmp under -system and -knapsack}
  \begin{algorithmic}[1]
  \STATE Approximate the -knapsack by a single knapsack with weights  and capacity ; applying
  Lemma~\ref{lem:knap-to-mat} with  on this knapsack, let  denote the resulting partition matroids (note ).

 \STATE For each , define ; note that each  is a -system.

 \STATE Run the algorithm from Theorem~\ref{thm:maxmin-p-system} under each  system  to obtain
solutions .

 \STATE Let .

 \STATE Partition  into  such that each , as per Claim~\ref{cl:knap-mat-2}.
 \STATE Output  where . Here we use
 the offline algorithm from Property~\ref{ass:apx}.
\end{algorithmic}
\end{algorithm}

We now establish the approximation ratio of this algorithm.

\begin{claim}\label{cl:knap-mat-1}
.
\end{claim}
\begin{proof}
For any , we have  for all . Hence  , i.e. it satisfies the combined knapsack constraint. Now by Lemma~\ref{lem:knap-to-mat}~(2),
we obtain . Finally, since , we have  .
\end{proof}


\begin{claim}\label{cl:knap-mat-2}
For each  there exists a collection  such that
, and  for all . Furthermore, this is
computable in polynomial time.
\end{claim}
\begin{proof}
Consider any . Note that , so any subset of  is also in 
(which is downwards-closed). We will show that there is a partition of  into  such
that each  satisfies the -knapsack. This suffices to  prove the claim. Since , by Lemma~\ref{lem:knap-to-mat}~(1) it follows that .
Starting with the trivial partition of  into singleton elements, greedily merge parts as long as each part
satisfies the -knapsack, until no further merge is possible. (Note that the trivial partition is indeed feasible
since each element satisfies the -knapsack.) Let   denote the parts in the final
partition; we will show  which would prove the claim. Consider forming  pairs from
 arbitrarily. Observe that for any pair , it must be that
 violates {\em some} knapsack; so . Thus
. On the other hand, , which implies .
\end{proof}

\begin{theorem}\label{thm:max-min-psystem-knapsack}
Assuming Properties~\ref{ass:subadd},~\ref{ass:apx} and~\ref{ass:online}, there is an -approximation algorithm for \mmp under a -system and -knapsack constraint.
\end{theorem}
\begin{proof}
Let  denote the optimal value of \mmp under  system , for each . By
Claim~\ref{cl:knap-mat-1} we have , the optimal value of \mmp under . Observe that
Theorem~\ref{thm:maxmin-p-system} actually implies  for each .
Thus ; hence . Now
consider the partition  of  from Claim~\ref{cl:knap-mat-2}. By the subadditivity
property, ; i.e. there is some  with
. Thus the  found using the offline algorithm
(Property~\ref{ass:apx}) satisfies .
\end{proof}

{\bf Remark:} We can obtain a better approximation guarantee of  in
Theorem~\ref{thm:max-min-psystem-knapsack} using randomization. This algorithm is same as Algorithm~2, except for the
last step, where we output  for  chosen {\em uniformly at random}. From the above proof
of Theorem~\ref{thm:max-min-psystem-knapsack}, it follows that:


\section{General Framework for Robust Covering Problems} \label{sec:gen-sets}
\newcommand{\mapx}{\alpha_{\sf mm}}

In this section we present an abstract framework for robust covering problems under {\em any uncertainty set} ,
as long as we are given access to offline, online and max-min algorithms for the base covering problem. Formally, this
requires Properties~\ref{ass:apx},~\ref{ass:online} and the following additional property (recall the notation from
Section~\ref{sec:prelim}).
\begin{property}[Max-Min Algorithm]\label{ass:maxmin}
  There is an -approximation algorithm for the max-min problem:
  given input , .
\end{property}

\begin{theorem}\label{th:gen-p-sets}
Under \lref[Properties]{ass:subadd},~\ref{ass:apx},~\ref{ass:online} and \ref{ass:maxmin}, there is an
-approximation algorithm for the robust covering problem .
\end{theorem}
\begin{proof}
The algorithm proceeds as follows.
\begin{algorithm}
\caption{Algorithm Robust-with-General-Uncertainty-Sets}
  \begin{algorithmic}[1]
\STATE \textbf{input:} the \rcov instance and threshold .

    \STATE \textbf{let} counter , initial online
    algorithm's input , initial online solution
    .

    \REPEAT

      \STATE {\bf set} .

    \STATE \label{step:gen:1} \textbf{let}  be the scenario returned by
    the algorithm of \lref[Property]{ass:maxmin} on .

    \STATE \label{step:gen:2} \textbf{let} , and  be the current online solution.

    \UNTIL{} \label{step:gen:3}

\STATE  \textbf{set} .

    \STATE \textbf{output} first-stage solution .
    \STATE \textbf{output} second-stage solution  where for any ,  is the solution of
  the offline algorithm (\lref[Property]{ass:apx}) for the problem .
  \end{algorithmic}
\end{algorithm}


As always, let  denote the optimal first stage solution (and its cost), and  the optimal
  second-stage cost; so the optimal value is . We prove the
  performance guarantee using the following claims.
\begin{Myquote}
\begin{claim}[General 2nd stage]\label{cl:p1}
For any  and , elements  satisfy all the requirements in , and
  .
\end{claim}
\begin{proof}
It is clear that  satisfy all requirements in . By the choice of set  in
\lref[line]{step:gen:1} of the last iteration, for any  we have:
 The first inequality is by \lref[Property]{ass:maxmin}, the second
inequality uses the fact that  (since we use an online algorithm to augment in
\lref[line]{step:gen:2}),\footnote{This is the technical reason we need an online algorithm. If instead we had used an
offline algorithm to compute  in step~\ref{step:gen:2} then  and we could not upper
bound the augmentation cost  by .} and the last inequality follows from
the termination condition in \lref[line]{step:gen:3}. Finally, since  is an -approximation to
, we obtain the claim.
\end{proof}

\begin{claim}\label{cl:p2}
.
\end{claim}
\begin{proof}
Since each  (these are solutions to ), the bound on the second-stage optimal cost gives
 for all . By subadditivity (\lref[Property]{ass:subadd}) we have
, which immediately implies the claim.
\end{proof}

\begin{claim}\label{cl:p3}
.
\end{claim}
\begin{proof}
Directly from the competitiveness of the online algorithm in \lref[Property]{ass:online}.
\end{proof}

\begin{claim}[General 1st stage]\label{cl:p4}
If  then .
\end{claim}
\begin{proof}
We have 
by the choice in Step~\eqref{step:gen:3}. Combined with \lref[Claim]{cl:p3}, we have . Now using \lref[Claim]{cl:p2}, we have , and hence . Finally using \lref[Claim]{cl:p3}, we obtain .
\end{proof}
\end{Myquote}

\lref[Claim]{cl:p1} and \lref[Claim]{cl:p4} imply that the above algorithm is a
-discriminating algorithm for the robust problem . Now using \lref[Lemma]{lem:apx} we obtain the theorem.
\end{proof}


\paragraph{Explicit uncertainty sets} An easy consequence of \lref[Theorem]{th:gen-p-sets} is for the {\em  explicit scenario} model of robust covering
problems~\cite{DGRS05,GGR06}, where  is specified as a list of possible scenarios. In this case, the \mm
problem can be solved using the -approximation algorithm from \lref[Property]{ass:apx} which implies an
-approximation for the robust version. In fact, we can do slightly better---observing that in
this  case, the algorithm for second-stage augmentation is the same as the Max-Min algorithm, we obtain an
-approximation algorithm for robust covering with explicit scenarios. As an application of
this result, we obtain an  approximation for robust Steiner forest with explicit scenarios, which is the
best known result for this problem.


\section{Robust Covering under -System and -Knapsack Uncertainty
  Sets}
\label{sec:combine}

Recall that any uncertainty set  for a robust covering problem can be assumed WLOG to be {\em downward-closed},
i.e.  and  implies . Eg., in the -robust model .
Hence it is of interest to obtain good approximation algorithms for robust covering when  is specified by means
of general models for downward-closed families. In this section, we consider the two well-studied models of -systems
and -knapsacks (Definitions~\ref{defn:p-system} and~\ref{defn:q-knapsack}).

\ignore{In this section, we consider robust covering problems under uncertainty sets described via -systems and
knapsack constraints. Recall the framework defined in \lref[Section]{sec:notation}: we have a covering problem \cov
with  requirements, and the possible second-stage scenarios are given by some downward-closed .
Here we consider uncertainty sets specified by means of the following two well-studied models for downward-closed
families.}

The result of this section says the following: \emph{if we can solve
  both the offline and online versions of a covering problem well, we
  get good algorithms for \rcov under uncertainty sets given by the
  intersection of -systems and -knapsack constraints}. Naturally, the
performance depends on  and ; we note  that this is unavoidable due to complexity considerations. Based on
Theorem~\ref{th:gen-p-sets} it suffices to give an approximation algorithm for the max-min problem under -systems
and -knapsack constraints; so Theorem~\ref{thm:max-min-psystem-knapsack} combined with Theorem~\ref{th:gen-p-sets}
implies an -approximation ratio. However, we can obtain a better
guarantee by considering the algorithm for \rcov directly. Formally we show that:
\begin{theorem}\label{th:general}
  Under \lref[Properties]{ass:subadd},~\ref{ass:apx} and
  \ref{ass:online}, the robust covering problem  admits an -approximation
  guarantee when  is given by the intersection of a -system and -knapsack constraints.
\end{theorem}
The outline of the proof is same as for Theorem~\ref{thm:max-min-psystem-knapsack}. We first consider the case when the
uncertainty set is a -system (\lref[subsection]{subsec:framework-mat}); then using the reduction in
Lemma~\ref{lem:knap-to-mat} we solve a suitable instance of \rcov under a -system uncertainty set.


\subsection{-System Uncertainty Sets}
\label{subsec:framework-mat} In this subsection, we consider \rcov when the uncertainty set  is some
-system. The algorithm is a combination of the ones in Theorem~\ref{th:gen-p-sets} and
Theorem~\ref{thm:maxmin-p-system}.  We start with an empty solution, and use the online algorithm to greedily try and
build a scenario of large cost. If we do find a ``violated'' scenario which is unhappy with the current solution, we
augment our current solution to handle this scenario (again using the online algorithm), and continue. The algorithm is
given as Algorithm~4 below.

\begin{algorithm}
  \caption{Algorithm Robust-with--system-Uncertainty-Sets }
  \begin{algorithmic}[1]
  \STATE \textbf{input:} the \rcov instance and bound .

    \STATE \textbf{let} counter , initial online
    algorithm's input , initial online solution
    .

    \REPEAT

  \STATE {\bf set} .

    \STATE \textbf{let} current scenario ,
    counter .

    \WHILE{( such that )}

    \STATE .

    \STATE \textbf{let} ,
    .

    \ENDWHILE

    \STATE \label{step:mm} \textbf{let}  be the
    scenario constructed by the above loop.

    \STATE \label{step:online} \textbf{let}  be the current online solution.


    \UNTIL{} \label{step:aug}

\STATE  \textbf{set} .

    \STATE \textbf{output} first-stage solution .

  \STATE \textbf{output} second-stage solution  where for any ,  is the solution of
  the offline algorithm (Property~\ref{ass:apx}) for the problem .

\end{algorithmic}
\end{algorithm}

We first prove a useful lemma about the behavior of the \textbf{while} loop. \ignore{ though it does not return a
scenario whose cost of augmentation with respect to the current solution is the largest possible, the loop does give a
scenario which is not much worse. This key lemma is proved in the next subsection.}
\begin{lemma}[Max-Min Lemma]
  \label{lem:maxmin}
  For any iteration  of the {\bf repeat} loop, the scenario  has the property that for any
  other scenario , .
\end{lemma}
\begin{proof}
The proof is almost identical to that of Theorem~\ref{thm:maxmin-p-system}.

Consider any iteration  of the {\bf repeat} loop in  Algorithm~4  that starts with a sequence   of elements
(that have been fed to the online algorithm
  ). Let  be the ordered set of elements
  added by the algorithm in this iteration. Define
  , and 
  for each . Note that  and , and
  . It suffices to show that
   for every
  . But this is precisely Equation~\eqref{eq:max-min-psystem} from the proof of Theorem~\ref{thm:maxmin-p-system}.
\end{proof}

\ignore{

Consider any iteration  of the {\bf repeat} loop in  Algorithm~\ref{algo:p-system}
  that starts with a sequence 
  of elements (that have been fed to the online algorithm
  ). Let  be the ordered set of elements
  added by the algorithm in this iteration. Define
  , and 
  for each . Note that  and , and
  . It suffices to show that
   for every
  . We use the following claim proved in~\cite{CCPV07}, Appendix~B (the proof of this claim
  relies on the properties of a -system).

\begin{claim}[\cite{CCPV07}]\label{cl:ccpv}
For any , there is a partition  of  such that for all ,
\begin{enumerate}
 \item , and
 \item For every , we have .
\end{enumerate}
\end{claim}

For any sequence  of requirements and any  define .  Note that this function depends on the particular online algorithm. From the second condition in
Claim~\ref{cl:ccpv}, it follows that each element of  was a feasible augmentation to  in
the  iteration of the {\bf while} loop. By the greedy choice,

Above equation~\eqref{eq:psys1} is by the definition of ,
equation~\eqref{eq:psys2} uses the subadditivity Property~\ref{ass:subadd}, and~\eqref{eq:psys3} is by the first
condition in Claim~\ref{cl:ccpv}.

Summing over all iterations , we obtain:
 where the last inequality follows from monotonicity since
   for all . Using the
  subadditivity~\lref[Property]{ass:subadd}, we get .

Let . i.e.  . Observe that
 is a feasible  augmentation to  that covers requirements . Thus,
 This completes the proof of Lemma~\ref{lem:maxmin}.

}

\begin{corollary}[Second Stage]\label{cor:mk-2nd}
  For any  and , elements  satisfy all the requirements in , and
  .
\end{corollary}
\begin{proof}
Observe that , so the first part of the corollary follows from the definition of \snd.  By
\lref[Lemma]{lem:maxmin} and the termination condition on
  \lref[line]{step:aug}, we have . Now
  \lref[Property]{ass:apx} guarantees that the solution  found by this approximation algorithm has cost at most
.
  \end{proof}

It just remains to bound the cost of the first-stage solution . Below  denotes the optimal
first-stage solution (and its cost); and  is the optimal second-stage cost.
\begin{lemma}[First Stage]\label{lem:mk-1st}
If  then .
\end{lemma}
\begin{proof}
For any set  of requirements let  denote the minimum cost to satisfy . Firstly, observe that
.
This follows from the fact that each of the  scenarios  are
  in , so the bound on the second-stage optimal cost gives
   for all . By
  subadditivity (\lref[Assumption]{ass:subadd}) we have
  ,
  which immediately implies the inequality. Now, we claim that
  
  The first inequality follows directly from the competitiveness of the
  online algorithm in \lref[Assumption]{ass:online}. For the second
  inequality, we have  by the terminal
  condition in \lref[Step]{step:aug}.  Putting~ the upper and lower bounds on
  together, we have ,
  and hence . Using the competitiveness of the online
  algorithm again, we obtain  .
\end{proof}

From \lref[Corollary]{cor:mk-2nd} and \lref[Lemma]{lem:mk-1st}, it follows that our algorithm is -discriminating (cf. Definition~\ref{defn:algo}) to \rcov. Thus we obtain
Theorem~\ref{th:general} for the case .

\ignore{outputs a solution of objective value at most , where the second stage augmentation algorithm is just the offline approximation from
\lref[Property]{ass:apx}. This completes the proof of \lref[Theorem]{th:general} for the case of just matroid
constraints (i.e. ). In \lref[Subsection]{sec:knapsack-mat}, we show how knapsack constraints can be reduced to
partition matroids, so as to obtain \lref[Theorem]{th:general}.}


\subsection{Algorithm for -Systems {\em and } -Knapsacks }
Here we consider \rcov when the uncertainty set  is the intersection of   -system \ms and a -knapsack.
The algorithm is similar to that in Subsection~\ref{subsec:mm-knap-psystem}. Again, by scaling weights in the knapsack
constraints, we may assume WLOG that each knapsack has capacity exactly one; let  denote the weights in
the  knapsack constraints. We also assume WLOG that each singleton element satisfies the -knapsack. The algorithm
for \rcov under  works as follows.

\begin{algorithm}
\caption{Algorithm Robust with -system and -knapsack Uncertainty Set}
  \begin{algorithmic}[1]
  \STATE Consider a modified uncertainty set  that is given by the intersection of \ms and the {\em single knapsack}
with weight-vector  and capacity .

  \STATE Applying the algorithm in Lemma~\ref{lem:knap-to-mat} to  this single knapsack with , let
 denote the resulting partition matroids (note ).

  \STATE For each , define uncertainty-set ; note that each  is a
-system.

  \STATE Let  . Solve \rcov under  using the algorithm of
  Theorem~\ref{th:p-sets-union}.
\end{algorithmic}
\end{algorithm}

Recall Claims~\ref{cl:knap-mat-1} and~\ref{cl:knap-mat-2} which hold here as well.

\ignore{\begin{claim}\label{cl:knap-mat-1} .
\end{claim}
\begin{proof}
For any , we have  for all . Hence  , and since , . Now by
Lemma~\ref{lem:knap-to-mat}~(2), we obtain ; thus .
\end{proof}

\begin{claim}\label{cl:knap-mat-2}
For each  there exists a collection  such that , and  for all . Furthermore, this is computable in polynomial time.
\end{claim}
\begin{proof}
Consider any . Note that , so any subset of  is also in  (downwards-closed). We
will show that there is a partition of  into  such that each 
satisfies the -knapsack. This suffices to  prove the claim. Since , by
Lemma~\ref{lem:knap-to-mat}~(1) it follows that . Starting with the
trivial partition of  into singleton elements, greedily merge parts as long as each part satisfies the
-knapsack, until no further merge is possible. (Note that the trivial partition is indeed feasible since each
element satisfies the -knapsack.) Let   denote the parts in the final partition; we
will show  which would prove the claim. Consider forming  pairs from
 arbitrarily. Observe that for any pair , it must be that
 violates {\em some} knapsack; so . Thus
. On the other hand, , which implies .
\end{proof}
Based on the above two claims, the reduction from  to  loses only an  factor in approximation:}


\begin{lemma}\label{lem:knap-mat-combine}
Any -approximate solution to \rcov under  is a -approximate solution to \rcov under
uncertainty-set .
\end{lemma}
\begin{proof}
Consider the optimal first-stage solution  to \rcov under , let \Tstar denote the optimal
second-stage cost and \opt the optimal value. Let  be any scenario, with partition
 given by Claim~\ref{cl:knap-mat-2}. Using the subadditivity Property~\ref{ass:subadd}, we
have . Thus the
objective value of  for \rcov under  is at most .

Claim~\ref{cl:knap-mat-1} implies that for any solution, the objective value of \rcov under  is at most that of
\rcov under . Thus the lemma follows.
\end{proof}

For solving \rcov under , note that although  itself is not any -system, it is the {\em union  of
polynomially-many} -systems. We show below that a simple extension of the algorithm in
Subsection~\ref{subsec:framework-mat} also works for unions of -systems; this would solve \rcov under .
\begin{theorem}\label{th:p-sets-union}
There is an -approximation for \rcov when the uncertainty set is given by the  union of
polynomially-many -systems.
\end{theorem}
\begin{proof}
Let  denote the uncertainty set where each  is a -system. The algorithm for
\rcov under  is just Algorithm~4 where we replace the body of the repeat-loop (ie. lines 4-11) by:
  \begin{algorithmic}[1]
  \STATE {\bf set} .

  \FOR{()}

  \STATE \textbf{let} current scenario ,

 \WHILE{( such that )}

    \STATE .

    \STATE  .

    \ENDWHILE
 \STATE Let .

 \ENDFOR

    \STATE \textbf{let} , and .

    \STATE \textbf{let}  and   be the current online solution.
\end{algorithmic}

Consider any iteration  of the repeat loop. By Lemma~\ref{lem:maxmin} applied to each -system ,
\begin{claim}
For each , we have  for every .
\end{claim}
By the choice of scenario  and since , we obtain:
\begin{claim}
For any iteration  of the repeat loop and any , .
\end{claim}
Based on these claims and proofs identical to Corollary~\ref{cor:mk-2nd} and Lemma~\ref{lem:mk-1st}, we obtain the same
bounds on the first and second stage costs of the final solution . Thus our algorithm is -discriminating, which by Lemma~\ref{lem:apx} implies the theorem.
\end{proof}

Finally, combining Lemma~\ref{lem:knap-mat-combine} and Theorem~\ref{th:p-sets-union} we obtain
Theorem~\ref{th:general}.


\ignore{Using subadditivity, we can reduce the  knapsack constraints to just one knapsack (with ), at the loss of a
factor  in the objective value of the robust problem. Furthermore, using \lref[Lemma]{lem:knap-to-mat}, one can
reduce (in  time) this knapsack constraint to a partition matroid; this reduction loses an additional 
factor in the objective value. Hence the original uncertainty set with  knapsack and  matroid constraints reduces
to one having  matroid constraints, at the loss of an  factor. Finally we can use the algorithm for the
matroid-constrained case from the previous subsection, to obtain an 
approximation for robust covering under matroid and knapsack constraints. This completes the proof of
\lref[Theorem]{th:general}.}

{\bf Remark:} In \lref[Theorem]{th:general}, the dependence on the number of constraints describing the uncertainty set
 is inevitable (under some complexity assumptions). Consider a very special case of the robust covering problem
on ground-set , requirements  (where  is satisfied iff the solution contains ), a unit cost function
on , inflation parameter . The uncertainty set   is given by the intersection of  different
cardinality constraints coming from some {\em set packing} instance on . In this case, the optimal value of the
robust covering problem is exactly the optimal value of the set packing instance. The hardness result from
H{\aa}stad~\cite{H99} now implies that this robust covering problem is  hard to
approximate. We note that this hardness applies only to algorithms having running time that is sub-exponential in both
 and ; this is indeed the case for our algorithm.

\ignore{{\bf Remark 2:} Instead of reducing all the knapsacks to a single partition matroid and losing a factor 
as above, we can reduce each knapsack to a partition matroid (using \lref[Lemma]{lem:knap-to-mat}). This only incurs an
 factor loss, however this reduction has running time . After this reduction, we are left with an
uncertainty set that is a -system for which the algorithm of \lref[Section]{subsec:framework-mat} applies. Thus
there is an  time -approximation algorithm for \rcov under
 matroid and  knapsack constraints. This reduction also implies a -approximation algorithm for
(monotone) submodular maximization subject to a -system and  knapsacks (when  is a constant)...}



\paragraph{Results for -System and -Knapsack Uncertainty Sets.} We now list some specific results for robust covering
under uncertainty sets described by -systems and knapsack constraints; these follow directly from
\lref[Theorem]{th:general} using known offline and (deterministic) online algorithms for the relevant problems.

\begin{center}
\begin{small}
\begin{tabular}{|c|c|c|c|} \hline
{\bf Problem} & {\bf Offline ratio }& {\bf Online ratio} & {\bf -system, -knapsack Robust}\\
\hline\hline
Set Cover &  & ~\cite{AAABN03} &  \\
\hline
Steiner Tree/Forest & 2~\cite{AKR95,GW95} & ~\cite{IM91,BC97} & \\
\hline Minimum Cut & 1 & ~\cite{AAABN04,HHR03} & \\
\hline Multicut & ~\cite{GVY96} & ~\cite{AAABN04,HHR03} & \\
\hline
\end{tabular}
\end{small}
\end{center}


\section{Non-Submodularity of Some Covering Functions}
\label{sec:lbd} In this section we show that some natural covering functions are not even approximately submodular. Let
 be any monotone subadditive function. We say that  is -approximately
submodular iff there exists a submodular function  with  for all .


Consider the min-set-cover function,  minimum number of sets required to cover elements .
\begin{proposition}
The min-set-covering function is not -approximately submodular.
\end{proposition}
\begin{proof}
The proof follows from the lower bound on {\em budget-balance} for {\em cross-monotone cost allocations}. Immorlica et
al.~\cite{IMM08} showed that there is no -approximately budget-balanced cross-monotone cost allocation for the
set-cover game. On the other hand it is known (see Chapter 15.4.1 in~\cite{AlgGameThy-book}) that any submodular-cost
game admits a budget-balanced cross-monotone cost allocation. This also implies that any -approximately
submodular cost function (non-negative) admits an -approximate budget-balanced cross-monotone cost allocation.
Thus the min-set-covering function can not be -approximately submodular.
\end{proof}

Similarly, for minimum multicut ( minimum cost cut separating the pairs in ),
\begin{proposition}
The min-multicut function is not -approximately submodular.
\end{proposition}
\begin{proof}
This uses the result that the vertex-cover game does not admit -approximately budget-balanced
cross-monotone cost allocations~\cite{IMM08}. Since multicut (even on a star graph) contains the vertex-cover problem,
the proposition follows.
\end{proof}

On the other hand, some other covering functions are indeed approximately submodular.
\begin{itemize}
\item The minimum-cut function ( minimum cost cut separating vertices  from the root) is in fact submodular due to
submodularity of cuts in graphs.
\item The min-Steiner-tree ( minimum length tree that connects vertices
 to the root) and min-Steiner-forest ( minimum length forest connecting the pairs in ) functions are
-approximately submodular. When the underlying metric is a tree, these functions are submodular---in this
case they reduce to weighted coverage functions. Using probabilistic approximation of general metrics by trees, we can
write  where  is the distribution on dominating tree-metrics
(from~\cite{FRT03}) and  is the Steiner-tree/Steiner-forest function on tree . Clearly  is submodular. Since
there exists  that approximates distances in the original metric within factor ~\cite{FRT03},
it follows that  also -approximates  (resp. ).
\end{itemize}

While approximate submodularity of the covering problem \cov  (eg. minimum-cut or Steiner-tree) yields direct
approximation algorithms for \mmp, it is unclear whether they help in solving \rcov (even under cardinality-constrained
uncertainty sets~\cite{GNR-k-rob}). On the other hand, the online-algorithms based approach in this paper solves both
\mmp and \rcov, for uncertainty sets from -systems and -knapsacks.


\bibliography{../robust,../../abbrev,../../my-papers,../../embedding}
\bibliographystyle{plain}


\end{document}
