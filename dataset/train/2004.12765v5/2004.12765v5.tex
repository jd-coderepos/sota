








\documentclass[journal]{IEEEtran}























\ifCLASSINFOpdf
\else
\fi


















































\usepackage{graphicx}

\usepackage{float} 

\usepackage{hyperref}
\hyphenation{colbert-humor-detection}


\begin{document}
\title{ColBERT: Using BERT Sentence Embedding for Humor Detection}


\author{Issa ~Annamoradnejad*
\thanks{* Corresponding author: Annamoradnejad is with the Department of Computer Engineering, Sharif University of Technology, Tehran, Iran. e-mail: imoradnejad@ce.sharif.edu.},
Gohar ~Zoghi
}





\markboth{ColBERT: Using BERT Sentence Embedding for Humor Detection}{Annamoradnejad: ColBERT}












\maketitle


\begin{abstract}
Automatic humor detection has interesting use cases in modern technologies, such as chatbots and virtual assistants. In this paper, we propose a novel approach for detecting humor in short texts based on the general linguistic structure of humor. Our proposed method uses BERT to generate embeddings for sentences of a given text and uses these embeddings as inputs of parallel lines of hidden layers in a neural network. These lines are finally concatenated to predict the target value. For evaluation purposes, we created a new dataset for humor detection consisting of 200k formal short texts (100k positive and 100k negative). Experimental results show that our proposed method can determine humor in short texts with accuracy and an F1-score of 98.2 percent. Our 8-layer model with 110M parameters outperforms the baseline models with a large margin, showing the importance of utilizing linguistic structure of texts in machine learning models.
\end{abstract}

\begin{IEEEkeywords}
Computational Humor, Human-device Interaction, Natural Language Processing, Sentence Embedding, Humor Structure, Dataset.
\end{IEEEkeywords}


\textbf{Disclaimer}
“This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.”
\url{https://journals.ieeeauthorcenter.ieee.org/become-an-ieee-journal-author/publishing-ethics/guidelines-and-policies/post-publication-policies/}




\IEEEpeerreviewmaketitle



\section{Introduction}

\IEEEPARstart{I}{n}  Interstellar (2014 movie), a future earth is depicted where robots easily understand and use humor in their connections with their owners and humans can set the level of humor in their personal robots\footnote{Tarzs, in the case of the movie.}. While we may have a long road toward the astral travels, we are very close in reaching high-quality systems injected with adjustable humor. 

Humor, as a potential cause of laughter, is an important part of human communication, which not only makes people feel comfortable, it also creates a cozier environment \cite{castro2016joke}. Automatic humor detection in texts has interesting use cases in building human-centered artificial intelligence systems such as chatbots and virtual assistants. An appealing use case \cite{castro2016joke} is to identify whether an input text should be taken seriously or not, which is a critical step to understand real motive of users' queries, return appropriate answers, and enhance the overall experience of user with the system. A more advanced outcome would be the injection of humor into computer-generated responses, thus making the human-computer interaction more engaging and interesting \cite{niculescu2013making}. This is an outcome that is achievable by setting the level of humor in possible answers to a desired level, similar to the mentioned movie.

The general structure of humor states that a joke consists of a few sentences that concludes with a punchline. The punchline is responsible for bringing contradiction into the story, thus making the whole text laughable. In other words, any sentence in a joke is normally non-humorous in itself, but when we try to comprehend all sentences together in one context or in one line of story, the text become humorous. For this reason, we believe and show that it is required to view and encode each sentence separately and capture this underlying structure of humor in a proper way. As a result, our proposed classification model for the task of humor detection is based on creating parallel paths of neural network hidden layers to extract features for each sentence and combining them at the end. 

Our approach uses BERT model to encode text into a few sentence embeddings which enter into an eight-layered neural network. Sentences are separately encoded and fed into parallel hidden layers of neural network to extract mid-level features for each sentence (related to context, type of sentence, etc). The final layers combine the output of all previous lines of hidden layers in order to predict the final output. In theory, these final layers should determine the congruity of sentences and detect the transformation of reader's viewpoint after reading the punchline.

In addition to proposing an accurate model, we publish a large dataset for the task of humor detection. Several existing humor detection datasets combined some formal non-humorous texts with informal humorous short texts. Since the two part have incompatible statistics (text length, words count, etc.), it is more likely to detect humor with simple analytical models and without understanding the underlying latent lingual features and structures.

We summarize our contributions as follows:
\begin{itemize}

\item We introduce a new dataset for the task of humor detection, entitled “ColBERT dataset", which contains 200k short texts (100k positive and 100k negative). We reduced or completely removed issues of the existing datasets to build a proper dataset for the task at-hand.
\item For the first time, we propose an automated approach for humor detection that is based on an accepted linguistic structure of humor. We will introduce the model architecture and components in detail.
\item We evaluate our model on 20\% of the dataset, and compare its performance with five baselines.
\end{itemize}

 The structure of this article is as follows: Section 2 reviews past works on the task of humor detection with focus on transfer learning methods. Section 3 describes the data collection and preparation techniques, and introduces the new dataset. Section 4 elaborates on the methodology, and section 5 presents our experimental results. Section 6 is the concluding remarks. 



\section{Literature Review}

With advances in NLP, researchers applied and evaluated state-of-the-art methods for the task of humor detection. This includes using statistical and N-gram analysis \cite{taylor2004computationally}, Regression Trees \cite{purandare2006humor}, Word2Vec combined with K-NN Human Centric Features \cite{yang2015humor}, and Convolutional Neural Networks \cite{chen2018humor} \cite{weller2019humor}. 

With the popularity of transfer learning, some researchers focused on using pre-trained models for several tasks of text classification. Transfer learning in NLP, particularly models like ULMFiT \cite{howard2018universal}, Allen AI’s ELMO \cite{peters2018deep}, and Google’s BERT \cite{devlin2018bert}, focuses on storing knowledge gained from training on one problem and applying it to a different but related problem usually after fine-tuning on a small amount of data.





Among them, BERT \cite{devlin2018bert} utilizes a multi-layer bidirectional transformer encoder consisting of several encoders stacked together, which can learn deep bi-directional representations. Similar to previous transfer learning methods, it is pre-trained on unlabeled data to be later fine-tuned for a variety of tasks. It initially came with two model sizes (BERT\textsubscript{BASE} and BERT\textsubscript{LARGE}) and obtained eleven new state-of-the-art results. Since then, it was pre-trained and fine-tuned for several tasks and languages, and several BERT-based architectures and model sizes have been introduced (such as Multilingual BERT, RoBERTa \cite{liu2019roberta}, ALBERT \cite{lan2019albert} and VideoBERT \cite{sun2019videobert}).

Ref \cite{weller2019humor} focused on the task of detecting whether a joke is humorous by using a Transformer architecture. They approached this problem by building a model that learns to identify humorous jokes based on ratings taken from the popular Reddit r/Jokes thread (13884 negative and 2025 positives).

There are emerging tasks related to humor detection. Ref \cite{yang2019predicting} focused on predicting humor by using audio information, hence reached 0.750 AUC by using only audio data. A good number of research is focused on the detecting humor in non-English texts, such as on Spanish \cite{chiruzzo2019overview, ismailov2019humor, giudice2019aspie96}, Chinese \cite{yang2019predicting}, and English-Hindi \cite{khandelwal2018humor}.

\section{Data}

Existing humor detection datasets use a combination of formal texts and informal jokes with incompatible statistics (text length, words count, etc.), making it more likely to detect humor with simple analytical models and without understanding the underlying latent connections. Moreover, they are relatively small for the tasks of text classification, making them prone to over-fit models. These problems encouraged us to create a new dataset exclusively for the task of humor detection, where simple feature-based models will not be able to predict without an insight into the linguistic features.

We begin with a survey of the existing humor detection datasets (binary task), highlighting their size and data source (see Table~\ref{table-1} for an overview). There are other datasets focused on similar tasks, for example, on the tasks of punchline detection and  success (whether or not a punchline triggers laughter) \cite{chen2017predicting,hasan2019ur}, or on using speak audio and video to detect humor \cite{bertero2016deep,hasan2019ur}. 

In this section, we will introduce data sources, data filtering methods, and some general statistics on the new dataset.

\begin{table}
  \caption{Datasets for the binary task of humor classification}
  \label{table-1}
  \centering
  \begin{tabular}{l|ll}
    \hline
    \multicolumn{2}{r}{Parts}                   \\ \hline
     Dataset     &   \#Positive  &  \#Negative \\ \hline
    16000 One-Liners \cite{mihalcea2005making} & 16,000   &	16,002     \\
    Pun of the Day \cite{yang2015humor}     & 2,423  &	2,403      \\
    PTT Jokes \cite{chen2018humor}     &     1,425	&   2,551  \\
    English-Hindi \cite{khandelwal2018humor}     & 1,755	&   1,698  \\ \hline
    ColBERT     &       100,000	&   100,000  \\ \hline
  \end{tabular}
\end{table}



\begin{table*}[t]
  \caption{General statistics of the ColBERT dataset (100k positive, 100k negative)}
  \label{table-2}
  \centering
  \begin{tabular}{p{0.9cm}|p{0.7cm}p{0.8cm}p{0.8cm}p{1.6cm}p{1.2cm}p{1.2cm}p{1.2cm}p{1.7cm}}
\hline & \#chars & \#words & \#unique words & \#punctuation & \#duplicate words & \#sentences & sentiment polarity & sentiment subjectivity \\ \hline
mean & 71.561  & 12.811  & 12.371         & 2.378          & 0.440             & 1.180       & 0.051              & 0.317                  \\
std  & 12.305  & 2.307   & 2.134          & 1.941          & 0.794             & 0.448       & 0.288              & 0.327                  \\
min  & 36      & 10      & 3              & 0              & 0                 & 1           & -1.000             & 0.000                  \\
median & 71      & 12      & 12             & 2              & 0                 & 1           & 0.000              & 0.268                  \\
max  & 99      & 22      & 22             & 37             & 13                & 2           & 1.000              & 1.000     \\ \hline
  \end{tabular}
\end{table*}

\subsection{Data Collection}

We carefully analyzed existing datasets (exclusively on news stories, news headlines, Wikipedia pages, tweets, proverbs and jokes) with regard to table size, character length, word count, and formality of language. Since none of them was compatible with each other as is in the mentioned criteria, first, we selected two datasets with formal texts (one with humor texts and one without) and performed a few preprocessing actions and row cuts to make them syntactically similar.

News dataset includes 200,853 news headlines (plus links, categories and stories) from 2012-2018 obtained from Huffington Post. Headlines are scattered in all news categories, including politics, wellness, entertainment and parenting.

Jokes dataset contains 231,657 jokes/humor short texts, crawled from Reddit communities\footnote{Mostly from /r/jokes and /r/cleanjokes subreddits.}. The dataset is compiled as a single \verb+csv+ file with no additional information about each text (such as the source, date, etc) and is available at Kaggle. Ref \cite{chen2018humor} combined this dataset with the WMT162 English news crawl, but did not publicly publish the dataset. Ref \cite{weller2019humor} also combined this dataset with extracted sentences from the WMT162 news crawl and made it publicly available.

\subsection{Preprocessing and Filtering}

First, we realized that there are duplicate texts in both datasets. Dropping duplicate rows removed 1369 rows from the jokes dataset and 1558 rows from the news dataset.

Then, to make their statistics more similar, we analyzed the number of characters and words, separately for each one of them, and by realizing differences in their distributions, we performed a few cuts. In short, we only kept texts with character length between 30 and 100, and word length between 10 and 18. Resulting data parts have very similar distribution with regard to these statistics.

In addition, we noticed that headlines in the news dataset use Title Case\footnote{All words are capitalized, except non-initial articles like “a, the, and”, etc.} formatting, which was not the case with the jokes dataset. Thus, we decided to apply Sentence Case\footnote{Capitalization as in a standard English sentence, e.g., “Witchcraft is real.”.} formatting to all news headlines by keeping the first character of the sentences in capital and lower-casing the rest. This simple modification helps to prevent simple classifiers from reaching perfect accuracy.

Finally, we randomly selected 100k rows from both datasets and merged them together to create an evenly distributed dataset.

\subsection{Dataset Statistics}
\footnote{The dataset is available at: https://github.com/Moradnejad/ColBERT-Using-BERT-Sentence-Embedding-for-Humor-Detection}
Dataset contains 200k labeled short texts, equally distributed between humor and non-humor. It is much larger than the previous datasets (Table~\ref{table-1}) and it includes texts with similar textual features. Correlation between character count and the target is insignificant (+0.09), and there is no notable connection between the target value and sentiment features (correlation coefficient of -0.09 and +0.02 for polarity and subjectivity, respectively). Table~\ref{table-2} gives details on a few general statistics of the dataset.



\section{Proposed Method}
In this section, we will explore our proposed method for the task of humor detection. From a technical viewpoint, we are proposing a supervised binary classifier that takes a string as input and determines if the given text is humorous or not.

\subsection{Humor Structure}
First, we take a look at the general structure of a joke to understand the underlying linguistic features that makes a text laughable. 

There has been a long line of works in linguistics of humor that classify jokes into various categories based on their structure or content. Many believed that humor arises from the sudden transformation of an expectation into nothing \cite{kant1913kritik}. Therefore, main theories on the structure of a joke involves two or three stages of storytelling that concludes with a punchline \cite{eysenck1942appreciation, suls1972two}. Punchline is the last part of a joke that destroys the perceiver's previous expectations and bring humor for its incongruity.

Raskin \cite{raskin2012semantic} presented Semantic Script Theory of Humor (SSTH), a detailed formal semantic theory of humor. The SSTH has the necessary condition that a text has to have two distinct related scripts that are opposite in nature, such as real/unreal, possible/impossible. For example, let us review a typical joke:
    \begin{quote}
        “Is the doctor at home?” the patient asked in his bronchial whisper. “No,” the doctor’s young and pretty wife whispered in reply. “Come right in.” \cite{raskin2012semantic}
    \end{quote}
This is compatible with the two-staged theory which ends with a punchline. The punchline is related to previous sentences but is included as an opposition to previous lines in order to transform the reader's expectation of the context.

\subsection{Model}
Based on the presented short introduction to the structure of humor, if one reads sentences of a joke separately, they are most-likely to be found as normal and non-humorous texts. On the other hand, if we try to comprehend all sentences together in one context or in one line of story, the text become humorous. Our proposed method utilizes this linguistic characteristic of humor in order to view or encode sentences separately and extract mid-level features using hidden layers.

\begin{figure*}[t]
\begin {center}
 \includegraphics[scale=0.8]{model.png}
 \caption{Components of the proposed method}
  \label{figure-1}
 \end {center}
\end{figure*}

Therefore, the classification model uses separate paths of hidden layers especially designed to extract latent features from each sentence. Furthermore, we include another path to extract latent features of the whole text. Hence, our proposed neural network structure includes one parallel path to view text as a whole and several other paths to view each sentence separately. Figure \ref{figure-1} displays the architecture of the proposed method. It is comprised of a few general steps:

\begin{enumerate}
    \item First, to assess each sentence separately and extract numerical features, we separate sentences and tokenize them individually.
    \item To prepare these textual parts as proper numerical inputs for the neural network, we encode them using BERT sentence embedding. This step is performed individually on each sentence (left side in Figure \ref{figure-1}) and also on the whole text (right side in Figure \ref{figure-1}).
    \item Now that we have BERT sentence embedding for each sentence, we feed them into parallel hidden layers of neural network to extract mid-level features for each sentence (related to context, type of sentence, etc). The output of this part for each sentence is a vector of size 20.
    \item While our main idea is to detect existing relationships between sentences (specifically the punchline's relationship with the rest), it is also required to examine word-level connections in the whole text that may have meaningful impacts in determining congruity of the text. For example, existence of synonyms and antonyms in text could be meaningful. We feed BERT sentence embedding for the whole text into hidden layers of neural network (right side in Figure \ref{figure-1}). The output of this part is a vector of size 60.
    \item Finally, three sequential layers of neural network conclude our model. These final layers combine the output of all previous paths of hidden layers in order to predict the final output. In theory, these final layers should determine the congruity of sentences and detect the transformation of reader's viewpoint after reading the punchline.
\end{enumerate}


\subsection{Implementation Notes}

To achieve clean data, we performed a few textual preprocessing actions on all input texts:

\begin{itemize}
    \item Expanding contractions: We replaced all contractions with the expanded version of the expressions. For example, "is not" instead of “isn’t".
    \item Cleaning punctuation marks: We separated the punctuation marks\footnote{The punctuation marks are: period, comma, question mark, hyphen, dash, parentheses, apostrophe, ellipsis, quotation mark, colon, semicolon, exclamation point.} from words to achieve cleaner sentences. For example, the sentence “This is’ (fun).” is converted to “This is ‘ ( fun ) .”
    \item Cleaning special characters: We replaced some special characters with an alias. For example, “alpha” instead of “”.
\end{itemize}

Our approach builds on using BERT sentence embedding in a neural network. More specifically, first we obtains token representation using BERT tokenizer with the maximum sequence length of 100 (the maximum sequence length of BERT is 512). Then, we generate BERT sentence embedding by feeding tokens as input into the BERT model (vector size=768). The model will pass BERT embedding vectors of the given text and its sentences as inputs to a neural network with eight layers. For each sentence, We have a separate parallel line of three hidden layers which are concatenated in the fourth layer and continue in a sequential manner to predict the single target value. We use \verb+huggingface+ and \verb+keras.tensorflow+ packages for the BERT model and neural network implementations, respectively.

It is important to note that we used BERT model to generate sentence embedding. Therefore, training is performed on the neural network and not on the BERT model. BERT comes with two pre-trained general types (the BERT\textsubscript{BASE} and the BERT\textsubscript{LARGE}), both of which are pre-trained from unlabeled data extracted from BooksCorpus \cite{zhu2015aligning} with 800M words and English Wikipedia with 2,500M words \cite{devlin2018bert}. In our proposed method, we use the smaller sized (BERT\textsubscript{BASE}) with 12 layers, 768-hidden, 12-heads, 110M parameters, which is pre-trained on lower-cased English text (uncased).




\section{Experiments}

In this section, we compare the performance of the ColBERT model with a few baselines. For the purposes of this section, the data is split into 80\% (160k) train and 20\% (40k) test parts.

\begin{table*}[t]
  \caption{Comparison of Different Methods on the ColBERT Dataset}
  \label{table-3}
  \centering
  \begin{tabular}{l|p{3.4cm}llll}
  \hline
Method        & Configuration                                                                              & Accuracy & Precision & Recall & F1    \\ \hline
Decision Tree &         & 0.786  & 0.769    & 0.821     & 0.794 \\ 
SVM           & sigmoid, gamma=1.0  & 0.872    & 0.869     & 0.880  & 0.874 \\
Multinomial NB &    alpha=0.2        & 0.876  & 0.863    & 0.902     & 0.882 \\
XGBoost &         & 0.720  & 0.753    & 0.777     & 0.813 \\
XLNet           & XLNet-Large-Cased  & 0.916    & 0.872     & 0.973  & 0.920 \\
    \hline
Proposed       &  & 0.982    & 0.990     & 0.974  & 0.982 \\ \hline
\end{tabular}
\end{table*}



\subsection{Baselines}

We chose five baselines to evaluate the performance of our proposed method. The baseline models are:

\begin{enumerate}
    \item Decision Tree: A methodology that is commonly used as a data mining method for establishing classification systems based on multiple covariates or for developing prediction algorithms for a target variable. The method uses the train dataset to generate a branch-like segments that construct an inverted tree with a root node, internal nodes, and leaf nodes \cite{song2015decision}. For our evaluation, we used \verb+CountVectorizer+ to generate numerical word representations.
    \item SVM: A supervised model that achieved robust results for many classification and regression tasks. For this baseline, we applied \verb+TfidfVectorizer+ to generate numerical word representations with some optimization on hyper-parameters.
    \item Multinomial naïve Bayes: The model is suited when we deal with discrete integer features, such as word counts in a text. Here, we used \verb+CountVectorizer+ to generate numerical word representations.
    \item XGBoost: XGBoost is the latest step in the evolution of tree-based algorithms that include decision trees, boosting, random forests, boosting and gradient boosting. It is an optimized distributed gradient boosting that provides fast and accurate results, which achieves accurate results in less time \cite{chen2016xgboost}. We applied XGBoost on numerical word representations generated by \verb+CountVectorizer+ which resulted in better accuracy than \verb+TfidfVectorizer+.
\item XLNet: A generalized language model that aims to mitigate the issues related to BERT model and previous autoregressive language models. For the task of text classification (and some other NLP tasks), XLNet outperforms BERT on several benchmark datasets \cite{yang2019XLNet}. We used \verb+xlnet-large-cased+ that has 24 layers and 340M parameters.
\end{enumerate}

We trained these baselines for 5 epochs on the complete 160K rows of the training set.


\subsection{Results}

Our experiments on the ColBERT dataset found the proposed model’s accuracy and F1 score to be 98.2\% which outperforms all baselines with a large margin (Table~\ref{table-3}). This is a 7 percent jump from the recent state-of-the-art XLNet model (with 340M parameters). Traditional models of Decision Tree, SVM and Multinomial naïve Bayes gained less than 90\% accuracy, still an acceptable performance for a general baseline. XGBoost did not perform very well, achieving 81\% F1-score based on the selected word representations. XLNet \textsubscript{Large}, which required less optimization, was the strongest among the baselines, reaching close to 92\% accuracy, 4 percent higher than Multinomial NB.

Based on the results, we can see two important factors in achieving high accuracy in the current task. First, can clearly see the power of sentence embeddings. XLNet and the proposed model both use their own embeddings and achieve much better results than other baselines. Traditional methods of word representations such as TF-IDF could not break a limit even with the use of latest classification boosting models (such as XGBoost). Second, it is interesting to note that our model with 110M parameters and 8 layers was able to outperform XLNet with 340 parameters and 24 layers. This is a direct result of utilizing the linguistic structure of humor in designing the proposed model.

For timing and performance, it took close to 2 hours in average to perform one epoch of training on 100k rows of the dataset on a computer with NVIDIA TESLA P100 GPUs.


\section{Conclusion}

For many years, human beings have been fantasizing about (or terrified of) humanoid robots indistinguishable from humans. In making that a reality, humor cannot be missed as a major human feature, which for its subjectivity, ambiguity and semantic intricacies has been a difficult problem for researchers to tackle. This work contributes to this old human fantasy and is paving the way for creating high-quality artificial intelligence systems (such as chatbots, virtual assistants and even robots) injected with adjustable humor.

Based on the results of this study, we identified two important factors in achieving high accuracy in the current task: 1) usage of sentence embeddings, and 2) utilizing the linguistic structure of humor in designing the proposed model.

Our technical approach consists of injecting BERT sentence embedding into a neural network model that processes sentences separately in parallel hidden layers. Moreover, we built a novel and large dataset consisting of 200k formal short texts for the task of humor detection. Our method with much less parameters and layers obtained an accuracy of 0.982 and outperforms traditional and state-of-the-art models. Results showed that our hypothesis on the structure of humor is valid and can be utilized to create very accurate systems.

In addition to the task of humor detection, the proposed combined method of neural network can be used in future studies examining a wider range of tasks of text classification.


















\ifCLASSOPTIONcaptionsoff
  \newpage
\fi










\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,./cites}



















\end{document}
