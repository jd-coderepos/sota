

\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{comment}
\usepackage{amsmath,amssymb} \usepackage{color}
\usepackage{xcolor}
\usepackage{ragged2e}
\usepackage{epsfig}
\usepackage{array}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{hyperref}






\begin{document}
\pagestyle{headings}
\mainmatter
\def\ECCVSubNumber{100}  

\title{Segmenting Transparent Objects in the Wild} 

\begin{comment}
\titlerunning{ECCV-20 submission ID \ECCVSubNumber}
\authorrunning{ECCV-20 submission ID \ECCVSubNumber}
\author{Anonymous ECCV submission}
\institute{Paper ID \ECCVSubNumber}
\end{comment}


\titlerunning{Segmenting Transparent Objects in the Wild}

\author{Enze Xie \and
    Wenjia Wang\inst{2} \and
    Wenhai Wang\inst{3} \and
    Mingyu Ding\inst{1} \and \\
    Chunhua Shen\inst{4} \and
    Ping Luo\inst{1}
\
	mBER = \frac{1}C \sum\nolimits_{i = 1}^{C} (1-\frac{1}2(\frac{TP_{i}}{TP_{i}+FN_{i}} + \frac{TN_{i}}{TN_{i}+FP_{i}}))\times 100,
	\label{eqn:r_i}
	
	L = L_s +  \lambda L_b,
	\label{eqn:loss-tot}
	
	D(S_i, G_i) = \frac{2 \sum\nolimits_{x,y} (S_{i, x, y} \times G_{i, x, y})}{\sum\nolimits_{x, y} S_{i, x, y}^2 + \sum\nolimits_{x, y} G_{i, x, y}^2},
	\label{eqn:dice-coef}
	
	where  and  refer to the value of pixel  in segmentation result  and ground truth , respectively.








    \section{Experiments}
    \subsection{Implementation Details}
    We have implemented TransLab with PyTorch~\cite{pytorch}. We use the pre-trained ResNet50~\cite{resnet} as the feature extraction network. In the final stage, we use the dilation convolution to keep the resolution as . The remaining parts of our network are randomly initialized. For loss optimization, we use the stochastic gradient descent~(SGD) optimizer with momentum of 0.9 and a weight decay of 0.0005. Batch size is set to 8 per GPU. The learning rate is initialized to 0.02 and decayed by the poly strategy~\cite{bisenet} with the power of 0.9 for 16 epochs. We use 8 V100 GPU for all experiments.
    During training and inference, images are resized to a resolution of 512512.


    \subsection{Ablation Studies}
    In this part, we demonstrate the effectiveness of boundary clues for transparent object segmentation.
    We report the results of the ablation study on the \textbf{hard} set of Trans10K because it is more challenging and can clearly observe the gap of different modules.
    We simply use the DeeplabV3+ as our baseline.
    We first show only use boundary loss as auxiliary loss during training can directly improve the segmentation performance.
    We further show how to use a boundary map as a clue to attend the feature of regular stream. Experiments demonstrate that more boundary attention leading to better results. In summary, locating boundaries is essential for transparent object segmentation.

    \textbf{Boundary Loss Selection.}
    Boundary is easier to observe than the content of the transparent object.
    To obtain high-quality boundary map, the loss function is essential.
    We choose Binary Cross-Entropy Loss, Focal Loss and Dice Loss to supervise boundary stream.
    As shown in Table.~\ref{tab:ab2}, Firstly, simply using boundary loss as auxiliary loss can directly improve the performance no matter which loss function is used. We argue this is due to the benefit of multi-task learning. It means the boundary loss can help the backbone focus more on the boundary of transparent objects and extract more discriminative features. Note that under this setting, the boundary stream can be removed during inference so it will not bring computation overhead.
    Secondly, Focal Loss works better than Binary Cross-Entropy loss because the majority of pixels of a boundary mask are background, and the Focal Loss can mitigate the sample imbalance problem by decreasing the loss of easy samples. However, the Dice Loss achieves the best results without manually adjusting the loss hyper-parameters. Dice Loss views the pixels as a whole object and can establish the right balance between foreground and background pixels automatically. As a result, Dice Loss can improve baseline with 1.25\% on mIoU and 2.31\% on Acc, which is the best in three loss functions. Meanwhile, the mBer and MAE are also improved~(lower is better).



    \makeatletter
	\newcommand\figcaption{\def\@captype{figure}\caption}
	\newcommand\tabcaption{\def\@captype{table}\caption}
	\makeatother
	\begin{figure}[t]
		\begin{minipage}[t]{.45\linewidth}
			\centering
			\setlength{\tabcolsep}{1.2mm}
			\tabcaption{Ablation study for different loss functions of boundary stream.}
			\scalebox{0.8}{
			\begin{tabular}{p{30pt}<{\centering}|p{30pt}<{\centering}|p{30pt}<{\centering}|p{30pt}<{\centering}|p{30pt}<{\centering}}
	\hline
	\rule{0pt}{12pt} & mIoU~ & Acc~ & mBer~ & MAE~  \\
	\hline
	\rule{0pt}{12pt}- & 69.04 &78.07  &17.27 &0.194   \\
	\rule{0pt}{12pt}BCE & 69.33 &78.61 &16.89 &0.190   \\
	\rule{0pt}{12pt}Focal &69.41 &78.76 &16.27 &0.188  \\
	\hline
	\rule{0pt}{12pt}Dice &\textbf{70.29} &\textbf{80.38} &\textbf{15.44}  &\textbf{0.183}  \\
	\hline
\end{tabular} 			}

			\label{tab:ab2}
		\end{minipage}
		\begin{minipage}[t]{.50\linewidth}
			\centering
			\setlength{\tabcolsep}{1.2mm}
			\tabcaption{Ablation study for different setting of Boundary Attention Module.}
			\scalebox{0.8}{
			\begin{tabular}{p{60pt}<{\centering}|p{30pt}<{\centering}|p{30pt}<{\centering}|p{30pt}<{\centering}|p{30pt}<{\centering}}
	\hline
	\rule{0pt}{12pt}Method & mIoU~ & Acc~ & mBer~ & MAE~  \\
	\hline
	\rule{0pt}{12pt}BL &70.29 &80.38 &15.44 &0.183 \\
	\rule{0pt}{12pt}BL+C1 &71.05 &81.80 &13.98 &0.180   \\
	\rule{0pt}{12pt}BL+C1\&2 &71.32 &82.05 &13.69 &0.178   \\
	
	\hline
    \rule{0pt}{12pt}	BL+C1\&2\&4 &\textbf{72.10} &\textbf{83.04} &\textbf{13.30} &\textbf{0.166}  \\
	\hline
\end{tabular}
 			}

			\label{tab:ab1}
		\end{minipage}
	\end{figure}


    \textbf{Boundary Attention Module.}
    After obtaining a high-quality boundary map, boundary attention is another key step in our algorithm.
    We can fuse boundary information at different levels for the regular stream.
    In this part, we repeatedly use BAM on C1, C2, C4 feature maps to show how boundary attention module works.
    To evaluate the relationship between the quantity of boundary attention and the final prediction accuracy, we vary the number of fusion levels from 1 to 3 and report the mIoU, Acc, mBer and MAE in Table~\ref{tab:ab1}.
    Note that `BL' indicates baseline with Dice Loss.
    It can be observed that performance is improved consistently by using boundary attention module at more levels.
    Our final model that fuses boundary information in all three levels further improves mIoU from 70.29\% to 72.10\% and Acc from 80.38\% to 83.04\%. Meanwhile, the mBer and MAE are also improved~(lower is better).
    By using a high-quality boundary map for attention, the feature maps of regular stream can have higher weights on the boundary region.


    \subsection{Comparison to the State-of-the-arts}
    We select several main-stream semantic segmentation methods to evaluate on our challenging Trans10K dataset.
    Specifically, we choose FPENet~\cite{fpenet}, ContextNet~\cite{contextnet}, FastSCNN~\cite{fastscnn}, DeeplabV3+ with MobilenetV2~\cite{deeplabv3+}, CGNet~\cite{cgnet}, HRNet~\cite{hrnet}, HardNet~\cite{hardnet}, DABNet~\cite{dabnet}, LEDNet~\cite{lednet}, ICNet~\cite{icnet} and BiSeNet~\cite{bisenet} as real-time methods.
DenseASPP~\cite{denseaspp}, DeepLabV3+ with Resnet50~\cite{deeplabv3+},  FCN~\cite{fcn}, OCNet~\cite{ocnet}, RefineNet~\cite{refinenet}, DeepLabV3+ with Xception65~\cite{deeplabv3+}, DUNet~\cite{dunet}, UNet~\cite{unet} and PSPNet~\cite{pspnet} as regular methods.
We re-train each of the models on the training set of our dataset and evaluate them on our testing set. For a fair comparison, we set the size of the input image as 512512 with single scale training and testing.

    Table~\ref{tab:my_label} reports the overall quantitative comparison results on test set, where our TransLab outperforms all other methods in our Trans10K in terms of all four metrics in both easy/hard set and things/stuff categories.
    Especially, TransLab outperforms DeepLabV3+, the sota semantic segmentation method, in a large gap on all metrics as well as both things and stuff, especially in hard set.
    For instance, it surpasses DeepLabV3 by 3.97\% on `Acc'~(hard set). Fig~\ref{fig:res} also shows TransLab can predict sharp boundary with high-quality masks when compared with other methods. More analysis are shown in supplementary material.



    \begin{table}[ht]
    \centering
    \caption{Evaluated Semantic Segmentation methods. \textbf{Sorted by FLOPs}. Note that FLOPs is computed with one 512512 image.}

    \begin{center}
    (a) Comparison between things and stuff.
    \end{center}
    \begin{minipage}[ht]{0cm}
    \centerline{
    \scalebox{0.8}{
    \begin{tabular}{p{100pt}<{\centering}|p{37pt}<{\centering}|p{37pt}<{\centering}|p{37pt}<{\centering}p{37pt}<{\centering}|p{37pt}<{\centering}p{37pt}<{\centering}|p{37pt}<{\centering}p{37pt}<{\centering}}
\hline
\multirow{2}{*}{\large Method} & \multirow{2}{*}{MAE~} & \multirow{2}{*}{ACC~} & \multicolumn{2}{c|}{\rule{0pt}{10pt}IoU~} & \multicolumn{2}{c|}{BER~} & 
\multicolumn{2}{c}{Computation} \\ \cline{4-9} 
 & \rule{0pt}{10pt} & & Things & Stuff & Things & Stuff & FLOPs & Params \\ \hline
 \rule{0pt}{10pt} FPENet~\cite{fpenet} & 0.339 & 24.73 & 33.96 & 34.36 & 39.59 & 39.01 & 0.78G & 0.11M  \\
\rule{0pt}{10pt} ContextNet~\cite{contextnet} & 0.217 & 62.09 & 56.29 & 56.61 & 22.26 & 22.46 & 0.89G & 0.87M  \\
\rule{0pt}{10pt} FastSCNN~\cite{fastscnn} & 0.206 & 64.20 & 58.62 & 59.74 & 20.59 & 23.95 & 1.03G & 1.20M  \\
\rule{0pt}{10pt} DeepLabv3+MBv2~\cite{deeplabv3+} & 0.130 & 80.92 & 78.55 & 71.97 & 10.38 & 14.58 & 2.70G & 1.96M  \\
\rule{0pt}{10pt} CGNet~\cite{cgnet} & 0.216 & 59.15 & 58.33 & 56.28 & 21.02 & 24.88 & 3.52G & 0.49M  \\
\rule{0pt}{10pt} HRNet~\cite{hrnet} & 0.134 & 75.82 & 79.34 & 69.78 & 10.39 & 16.64 & 4.20G & 1.53M  \\
\rule{0pt}{10pt} HardNet~\cite{hardnet} & 0.184 & 69.17 & 64.91 & 63.15 & 17.18 & 20.63 & 4.43G & 4.11M  \\
\rule{0pt}{10pt} DABNet~\cite{dabnet} & 0.230 & 54.87 & 54.48 & 55.32 & 25.77 & 25.64 & 5.25G & 0.75M  \\
\rule{0pt}{10pt} LEDNet~\cite{lednet} & 0.168 & 75.70 & 70.37 & 64.68 & 12.68 & 17.62 & 6.32G & 2.31M  \\
\rule{0pt}{10pt} ICNet~\cite{icnet} & 0.244 & 52.65 & 53.90 & 47.38 & 19.78 & 29.46 & 10.66G & 8.46M  \\
\rule{0pt}{10pt} BiSeNet~\cite{bisenet} & 0.140 & 77.92 & 77.39 & 70.46 & 10.86 & 17.04 & 19.95G & 13.30M  \\


\hline
\rule{0pt}{10pt} DenseASPP~\cite{denseaspp} & 0.114 & 81.22 & 81.79 & 74.41 & 9.07 & 15.31 & 36.31G & 29.09M \\
\rule{0pt}{10pt} DeepLabv3+R50~\cite{deeplabv3+} & 0.081 & 89.54 & 87.90 & 81.16 & 5.31 & 10.25 & 37.98G & 28.74M  \\
\rule{0pt}{10pt} FCN~\cite{fcn} & 0.108 & 83.79 & 84.40 & 74.92 & 7.30 & 13.36 & 42.35G & 34.99M  \\
\rule{0pt}{10pt} OCNet~\cite{ocnet} & 0.122 & 80.85 & 80.55 & 73.15 & 8.91 & 16.38 & 43.43G & 35.91M  \\
\rule{0pt}{10pt} RefineNet~\cite{refinenet} & 0.180 & 57.97 & 73.65 & 58.40 & 16.44 & 27.98 & 44.34G & 29.36M  \\
\rule{0pt}{10pt} DeepLabv3+XP65~\cite{deeplabv3+} & 0.082 & 89.18 & 87.54 & 80.98 & 5.64 & 10.34 & 51.95G & 41.05M  \\
\rule{0pt}{10pt} DUNet~\cite{dunet} & 0.140 & 77.84 & 79.10 & 69.00 & 10.53 & 15.84 & 123.35G & 31.21M  \\
\rule{0pt}{10pt} UNet~\cite{unet} & 0.234 & 51.07 & 54.99 & 52.96 & 27.04 & 25.69 & 124.62G & 13.39M  \\
\rule{0pt}{10pt} PSPNet~\cite{pspnet} & 0.093 & 86.25 & 86.13 & 78.42 & 6.68 & 12.75 & 187.27G & 50.99M  \\

\hline \hline
\rule{0pt}{10pt} \textbf{TransLab} & \textbf{0.063} & \textbf{92.69} & \textbf{90.87} & \textbf{84.39} & \textbf{3.63} & \textbf{7.28} & 61.60G & 42.19M \\ 
\hline
\end{tabular} }
    }
    \end{minipage}
    \begin{center}(b) Comparison between Easy and Hard.\end{center}
    \begin{minipage}[ht]{8cm}
    \centerline{
    \scalebox{0.7}{
    \begin{tabular}{p{95pt}<{\centering}|p{31pt}<{\centering}p{31pt}<{\centering}p{31pt}<{\centering}|p{31pt}<{\centering}p{31pt}<{\centering}p{31pt}<{\centering}|p{31pt}<{\centering}p{31pt}<{\centering}p{31pt}<{\centering}|p{31pt}<{\centering}p{31pt}<{\centering}p{31pt}<{\centering}}
\hline\multirow{2}{*}{\large Method} & \multicolumn{3}{c|}{\rule{0pt}{10pt}MAE~} & \multicolumn{3}{c|}{Acc~} & \multicolumn{3}{c|}{mIoU~} & \multicolumn{3}{c}{mBER~} \\ \cline{2-13} 
 & \rule{0pt}{10pt}All & Easy & Hard & All & Easy & Hard & All & Easy & Hard & All & Easy & Hard \\ \hline
\rule{0pt}{10pt} FPENet~\cite{fpenet} & 0.339 & 0.297 & 0.492 & 24.73 & 26.50 & 19.19 & 34.17 & 36.82 & 24.41 & 39.31 & 37.88 & 44.03 \\
\rule{0pt}{10pt} ContextNet~\cite{contextnet} & 0.217 & 0.171 & 0.386 & 62.09 & 67.14 & 46.34 & 56.46 & 61.73 & 37.71 & 22.36 & 18.77 & 34.44 \\
\rule{0pt}{10pt} FastSCNN~\cite{fastscnn} & 0.206 & 0.161 & 0.373 & 64.20 & 69.42 & 48.01 & 59.18 & 64.63 & 40.27 & 22.27 & 18.74 & 34.22 \\
\rule{0pt}{10pt} DeepLabv3+MBv2\cite{deeplabv3+} & 0.130 & 0.091 & 0.275 & 80.92 & 85.90 & 65.43 & 75.27 & 80.55 & 56.17 & 12.49 & 9.08 & 24.47 \\
\rule{0pt}{10pt} CGNet~\cite{cgnet} & 0.216 & 0.173 & 0.379 & 59.15 & 64.57 & 42.26 & 57.31 & 62.41 & 39.56 & 22.95 & 19.67 & 34.33 \\ 
\rule{0pt}{10pt} HRNet~\cite{hrnet} & 0.134 & 0.092 & 0.291 & 75.82 & 82.17 & 56.04 & 74.56 & 80.43 & 53.42 & 13.52 & 9.95 & 26.17 \\
\rule{0pt}{10pt} HardNet~\cite{hardnet} & 0.184 & 0.141 & 0.345 & 69.17 & 73.83 & 54.67 & 64.03 & 69.11 & 46.18 & 18.91 & 15.58 & 30.52 \\
\rule{0pt}{10pt} DABNet~\cite{dabnet} & 0.230 & 0.187 & 0.391 & 54.87 & 59.29 & 41.07 & 54.90 & 59.45 & 38.77 & 25.71 & 22.63 & 36.15 \\
\rule{0pt}{10pt}LEDNet~\cite{lednet} & 0.168 & 0.124 & 0.331 & 75.70 & 80.62 & 60.37 & 67.54 & 73.04 & 48.38 & 15.15 & 11.83 & 26.58 \\
\rule{0pt}{10pt} ICNet~\cite{icnet} & 0.244 & 0.200 & 0.408 & 52.65 & 58.31 & 35.01 & 50.65 & 55.48 & 33.44 & 24.63 & 21.71 & 35.24 \\
\rule{0pt}{10pt} BiSeNet~\cite{bisenet} & 0.140 & 0.102 & 0.282 & 77.92 & 82.79 & 62.72 & 73.93 & 78.74 & 56.37 & 13.96 & 10.83 & 24.85 \\
\hline
 
 
\rule{0pt}{10pt} DenseAspp~\cite{denseaspp} & 0.114 & 0.078 & 0.247 & 81.22 & 86.25 & 66.55 & 78.11 & 83.11 & 60.38 & 12.19 & 8.85 & 23.71 \\
\rule{0pt}{10pt} DeepLabv3+R50\cite{deeplabv3+} & 0.081 & 0.050 & 0.194 & 89.54 & 93.22 & 78.07 & 84.54 & 89.09 & 69.04 & 7.78 & 4.91 & 17.27 \\
\rule{0pt}{12pt} FCN~\cite{fcn} &  0.108 & 0.073  & 0.239 & 83.79  &  88.55 & 68.93  & 79.67 & 84.53  & 62.51 & 10.33 & 7.36  & 20.47    \\
\rule{0pt}{12pt} OCNet~\cite{ocnet} & 0.122  & 0.087  & 0.253 & 80.85 & 85.63  & 65.96  &  76.85 & 81.53 & 59.75  & 12.65  &  9.43 & 23.69   \\ 
\rule{0pt}{12pt} RefineNet~\cite{refinenet} & 0.180  & 0.135  & 0.345 & 57.97 & 64.53 & 37.53  & 66.03 & 71.41  & 45.71 & 22.22  &  19.01 & 34.06   \\ 
\rule{0pt}{10pt} DeepLabv3+XP65~\cite{deeplabv3+} & 0.082 & 0.051 & 0.195 & 89.18 & 92.61 & 78.51 & 84.26 & 88.87 & 68.34 & 8.00 & 5.16 & 17.44 \\
\rule{0pt}{10pt} DUNet~\cite{dunet} & 0.140 & 0.100 & 0.289 & 77.84 & 83.41 & 60.50 & 74.06 & 79.19 & 55.53 & 13.19 & 9.93 & 25.01 \\
\rule{0pt}{12pt} UNet~\cite{unet} & 0.234 & 0.191  & 0.398  & 51.07  & 55.44 & 37.44  & 53.98  & 58.60 & 37.08  & 26.37  & 23.40 & 36.80    \\
\rule{0pt}{12pt} PSPNet~\cite{pspnet} & 0.093 & 0.062  & 0.211  & 86.25  & 90.41  & 73.28  & 82.38  & 86.79  & 66.35  & 9.72  & 6.67 &  20.08   \\


\hline \hline
\rule{0pt}{10pt} \textbf{TransLab} & \textbf{0.063} & \textbf{0.036} & \textbf{0.166} & \textbf{92.69} & \textbf{95.77} & \textbf{83.04} & \textbf{87.63} & \textbf{92.23} & \textbf{72.10} & \textbf{5.46} & \textbf{3.12} & \textbf{13.30}
 \\ \hline
\end{tabular}     }}


    \end{minipage}
    \label{tab:my_label}
    \end{table}
     \begin{figure}[ht]
        \centering
        \scalebox{1}{
        \includegraphics[width=0.95\textwidth]{figure/exp_display3.pdf}
        }
        \caption{Visual comparison of TransLab to other semantic segmentation methods. Our TransLab clearly outperforms others thanks to the boundary attention, especially in yellow dash region.}
        \label{fig:res}
    \end{figure}

    \section{Conclusion}
    In this work, we present the Trans10K dataset, which is the largest real dataset for transparent object segmentation.
    We also benchmark 20 semantic segmentation algorithms on this novel dataset and shed light on what attributes are especially difficult for current methods. We suggest that transparent object segmentation in the wild is far from being solved. Finally, we propose a boundary-aware algorithm, termed TransLab, to utilize the boundary prediction to improve the segmentation performance. \\
    \textbf{Acknowledgement}
This work is partially supported by the SenseTime Donation for Research, HKU Seed Fund for Basic Research, Startup Fund and General Research Fund No.27208720. Chunhua Shen and his employer received no financial support for the research, authorship, and/or publication of this article.

    \clearpage





    \appendix
    \section{Appendix}
    \subsection{Detailed annotation information.}
    Our dataset contains 20 different categories of transparent objects. As shown in Fig.~\ref{fig:my_label}(a), we count the number of images for different categories. We see that for things, the ``Cup'' appears the most frequently  while the ``Stationery'' appears the least.
    For stuff, the ``French Window'' appears the most frequently while the ``Table'' appears the least.
    Furthermore, our dataset also contains 13 scenarios. As shown in Fig.~\ref{fig:my_label}(b), we see that the ``Desktop'' is the most frequently scene while the ``Vechile'' is the least scene.
    In summary, our Trans10K contains abundant category distribution of transparent objects and scenarios, which is not available in TOM-Net~\cite{tomnet} and TransCut~\cite{transcut}.

    \begin{figure}
        \centering
        \subfigure[Histogram of image numbers for different transparent object categories. It is split by things and stuff. Event categories are ranked in an descending order based on the image numbers. Example images for specific transparent object classes are shown. Y-axis denotes for image numbers. X-axis denotes for transparent object categories.]{\includegraphics[width=1\textwidth]{figure/object_stat.pdf}}
        \subfigure[Histogram of image numbers for different scene categories. Event categories are ranked in an descending order based on the image numbers. Example images for specific scene classes are shown. Y-axis denotes for image numbers. X-axis denotes for event scene name.]{\includegraphics[width=1\textwidth]{figure/scene_stat.pdf}}
        \caption{Statistics of Trans10K dataset.}
        \label{fig:my_label}
    \end{figure}

    \subsection{More visual results Analysis.}



    \subsubsection{Failure Samples Analysis}
    As shown in Fig.~\ref{fig:failure}, our method has some limitations. For example, in Fig.~\ref{fig:failure}~(a), when facing highly-transparency objects, our method will fail to segment in some region. Fig.~\ref{fig:failure}~(b) shows that some objects with strong reflection will also make our method confuse and lead to wrong classification. In Fig.~\ref{fig:failure}~(c), we can find our method does not work well when some objects have overlap and occlusion with transparent objects. Finally, in Fig.~\ref{fig:failure}~(d), when semi-transparent objects are adjacent with transparent objects, our method will also confuse.

    \begin{figure}[ht]
    \centering
        \subfigure[High Transparency.]{\includegraphics[width=0.45\textwidth]{figure/failure1.pdf}}
        \subfigure[Strong Reflection.]{\includegraphics[width=0.45\textwidth]{figure/failure2.pdf}}
        \subfigure[Overlap \& Occlusion.]{\includegraphics[width=0.45\textwidth]{figure/failure3.pdf}}
         \subfigure[Semi-Transparency.]{\includegraphics[width=0.45\textwidth]{figure/failure4.pdf}}
    \caption{Failure cases. Our method fails to segment transparent objects in some complex scenarios. }
    \label{fig:failure}
    \end{figure}


    \subsubsection{Visual results on external data.}
    In this part, we also directly test our TransLab trained on Trans10K dataset to evaluate the generalize ability of Trans10K dataset and the robustness of TransLab.
    Firstly, we test our method on two prior datasets: TransCut~\cite{transcut} and TOM-Net~\cite{tomnet}. As shown in Fig.~\ref{fig:otherdata}, our method can clearly output very high-quality mask.
    Moreover, we also test our method on some external data randomly captured by our mobile phones or obtained from Internet videos such as YouTube, TikTok and eBay.
    We see our TransLab can also successfully segment transparent objects in most cases.
    In summary, we believe our Trans10K dataset contains high-diversity images which can easily generalize to real scene. Also, our boundary-aware algorithm TransLab is robust enough to segment unseen images.


    \subsubsection{More comparison of TransLab with other methods.}
    In this part, we demonstrate more test examples produced by our TransLab on Trans10K dataset in Fig.~\ref{fig:supple1}. From these results, it can be easily observed that with the proposed Boundary Stream and Boundary Attention Module, our method can output high-quality boundary map and better transparent object segmentation mask than other semantic segmentation methods.



    \begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{figure/extra.pdf}
    \vspace{-0.7cm}
    \caption{Some transparent objects segmentation results on two prior datasets: TransCut~\cite{transcut} and TOM-Net~\cite{tomnet}.}
    \label{fig:otherdata}
    \end{figure}

     \begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{figure/extra2.pdf}
    \vspace{-0.7cm}
    \caption{Some transparent objects segmentation results on challenging images captured by our mobile phones and obtained from Internet such as YouTube, TikTok and eBay.}
    \label{fig:internet}
    \end{figure}


    \begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{figure/exp_display2.pdf}
    \vspace{-0.7cm}
    \caption{More visual comparison of TransLab to other semantic segmentation methods. Our TransLab clearly outperforms others thanks to the boundary attention.}
    \label{fig:supple1}
    \end{figure}

    \clearpage

	\bibliographystyle{splncs}
	\bibliography{egbib}
\end{document}
