

\section{Introduction}
\label{sec:intro}
A long-standing goal of AI systems is to act reliably and learn complex tasks efficiently like human beings. 
In the process of reliable decision making, humans follow an explicit \textit{chain-of-thought} (CoT) reasoning process that is typically expressed as an explanation. 
However, machine learning models are trained mostly using a large number of input-output examples to perform a specific task. 
These black-box models only generate the final decision without reliably revealing the underlying reasoning process. 
Not surprisingly, it is unclear if they understand the task and can generalize even though they perform well on the benchmark. 
On the other hand, humans are able to learn from instructions or explanations from past experience and generalize them to novel and unseen problems. 
This helps them learn more quickly with fewer data. In this work, we explore if machines can be endowed with such reasoning abilities in the context of science-based question answering. 

\begin{figure}[th!]
 \centering
\includegraphics[width=1.0\linewidth]{figs/fig1_data_example_pull.pdf}
 \caption{We construct the \name{} dataset where a data example consists of multimodal question answering information and the grounded lecture and explanation. We study if QA models can generate a reasonable explanation to reveal the chain-of-thought reasoning.}
\label{fig:example}
\end{figure}

Recently, science problem solving benchmarks \cite{kembhavi2017you} have been used to diagnose the multi-hop reasoning ability and interpretability of AI systems. To answer science questions, a model needs to not only understand multimodal contents but also extract external knowledge to arrive at the correct answer. Since these tasks require domain-specific knowledge and explicit multi-hop reasoning, a model would be not interpretable if it fails to provide explanations to reveal the reasoning process. However, current science question datasets \cite{kembhavi2017you,kembhavi2016diagram,sampat2020visuo} mostly lack annotated explanations for the answers. To address this issue, other science datasets annotate the explanations, but they are restricted to the textual only modality and limited to small data scales \cite{jansen2018worldtree,dalvi2021explaining,mihaylov2018can} or a small set of topics \cite{Khot2020QASCAD,jhamtani2020learning}. Therefore, we collect Science Question Answering (\name{}), a large-scale multi-choice dataset that contains multimodal science questions with explanations and features rich domain diversity.

\name{} is collected from elementary and high school science curricula, and contains 21,208 examples along with lectures and explanations. Different from existing datasets \cite{kembhavi2016diagram,kembhavi2017you,sampat2020visuo}, \name{} has richer domain diversity from three different subjects: natural science, social science, and language science. A typical example consists of a question, multiple choices, multimodal contexts, a correct answer, as well as a lecture and an explanation. The lecture and explanation provide general external knowledge and specific reasons, respectively, for arriving at the correct answer.

Consider the thoughts one person might have when answering the question in Figure \ref{fig:example}. One first recalls the knowledge regarding the definition of a force learned from textbooks: ``\textit{A force is a push or a pull that ... The direction of a \red{\textbf{push}} is ... The direction of a \green{\textbf{pull}} is ...}'', then forms a line of reasoning: ``\textit{The baby's \blue{\textbf{hand}} applies a force to the cabinet \blue{\textbf{door}}}. $\rightarrow$  \textit{This force causes the \blue{\textbf{door}} to \blue{\textbf{open}}.} $\rightarrow$ \textit{The direction of this force is \green{\textbf{toward}} the baby's \blue{\textbf{hand}}.}'', and finally arrives at the correct answer: ``\textit{This force is a \green{\textbf{pull}}.}''. Following \cite{narang2020wt5}, we formulate the task to output a natural explanation alongside the predicted answer. In this paper, we train language models to generate lectures and explanations as the \textit{chain of thought} (CoT) to mimic the multi-hop reasoning process to answer \name{} questions. 

Our experiments show that current multimodal methods \cite{yu2019mcan,Anderson2017up,Kim2018,gao2019dynamic,li2019visualbert,lu2021iconqa} fail to achieve satisfactory performance on \name{} and do not generate correct explanations. Instead, we find that CoT can help large language models not only in the few-shot learning setting but also in the fine-tuning setting.
When combined with CoT to generate the lecture and explanation, the fine-tuned UnifiedQA \cite{khashabi2020unifiedqa} achieves an improvement of 3.99\% as opposed to not using CoT in the fine-tuning stage. The few-shot GPT-3 model \cite{chen2020big} via chain-of-thought prompting can obtain 75.17\% on \name{} with an improvement of 1.20\% compared to the few-shot GPT-3 without CoT. Prompted with CoT, GPT-3 can generate reasonable explanations as evaluated by automated metrics, and promisingly, 65.2\% of explanations meet the gold standard of human evaluations. We also investigate the upper bound for models to harness explanations by including them in the input. We find that doing so improves GPT-3's few-shot performance by 18.96\%, suggesting that explanations do aid models and are currently underutilized in the CoT framework. Further analysis shows that, like humans, language models benefit from explanations to learn with less data: UnifiedQA with CoT obtains the same results as UnifiedQA without CoT with only 40\% of the training data.

To sum up, our contributions are three-fold: 
(a) To bridge the gap in existing datasets in the scientific domain, we build Science Question Answering (\name{}), a new dataset containing 21,208 multimodal science questions with rich domain diversity. To the best of our knowledge, \name{} is the first large-scale multimodal dataset that annotates lectures and explanations for the answers.
(b) We show that CoT benefits large language models in both few-shot and fine-tuning learning by improving model performance and reliability via generating explanations.
(c) We further explore the upper bound of GPT-3 and show that CoT helps language models learn from fewer data.

\section{Related Work}
\label{sec:related} 

\noindent \textbf{Visual question answering.} Since the task of visual question answering (VQA) was first proposed in \cite{antol2015vqa}, there have been plenty of VQA datasets \cite{balanced_binary_vqa,zhu2016cvpr,krishna2017visual,balanced_vqa_v2,johnson2017clevr,hudson2019gqa} conducted to facilitate the research work. Although our \name{} dataset shares some features with VQA, there are several main differences between them. First, \name{} is more challenging than existing VQA datasets because it contains multimodal contexts and diverse topics in the scientific domain. In addition, most answers are annotated with lectures and explanations, which makes \name{} a suitable dataset for \fix{multi-modal} question answering and multi-hop reasoning for AI systems. Inspired by the recent remarkable performance achieved for VQA \cite{lu2018co,lu2018rvqa,gao2018question,gao2019dynamic,li2019visualbert,dosovitskiy2020image}, in this paper, we further extensively benchmark \name{} with a wide range of attention-based \cite{Anderson2017up,lu2018co,Kim2018,gao2019dynamic} and Transformer-based \cite{lu2019vilbert,li2019visualbert,li2020does,dosovitskiy2020image} methods.

\textbf{Datasets for science problems.} Science problem solving is a challenging task that requires an AI system not only to understand the multimodal information from the science curriculum but also to reason about how to answer the domain-specific questions. Current science problem datasets such as AI2D \cite{kembhavi2016diagram}, DVQA \cite{kafle2018dvqa}, VLQA \cite{sampat2020visuo}, and FOODWEDS \cite{krishnamurthy2016semantic} have contributed to multimodal reasoning in the scientific domain. For example, a portion of VLQA contains multimodal questions on science subjects.  These datasets, however, lack annotated explanations for the answers to reveal the reasoning steps. Some other datasets annotate the answers in the forms of supporting facts \cite{mihaylov2018can,Khot2020QASCAD}, entailment trees \cite{dalvi2021explaining}, explanation graphs \cite{jansen2018worldtree}, reasoning chains \cite{jhamtani2020learning}. However, these datasets are restricted to the single text modality with small data scales and limited topics. Instead, our \name{} annotates the answers with grounded lectures and explanations. Besides, \name{} features a richer domain diversity across 3 subjects, 26 topics, 127 categories, and 379 skills.

\textbf{Learning from explanations and few-shot learning.}
Explanations help humans understand a task better, and there have been several attempts to show the same for models. For example, the learning from instruction paradigm \cite{mishra2021cross,ouyang2022training,wei2021finetuned,mishra2021reframing,parmar-etal-2022-boxbart, lampinen2022can}, where the task level explanation is provided in the form of instruction, \fix{improves} model performance significantly.
An example of learning from explanations in the scientific domain is proposed in \cite{sachan2017learning} where the model interprets demonstrative solutions to solve geometry problems.
Recently, there has been a surge of interest in few-shot learning, where language models learn a specific task from a few examples \cite{perez2021true,bragg2021flex}. For instance, \cite{nye2021show,wei2022chain,lu2022dynamic} find that explanations in the format of the chain of thought can improve language models' reasoning ability in few-shot learning. In this paper, we show that the chain of thought boosts the performance of large language models like UnifiedQA \cite{khashabi2020unifiedqa} if the models generate explanations along with the answer in a fine-tuning way. Furthermore, a few-shot GPT-3 model via chain-of-thought prompting is able to improve the reasoning performance on \name{} and generate reasonable explanations.


\section{Dataset}
\label{sec:dataset}

We collect \name{}, which is a multimodal multiple-choice science question dataset containing 21,208 examples. An example in \name{} is shown in Figure \ref{fig:example}. Given the science question and multimodal contexts, the task is to select the correct answer from multiple options. Different from existing datasets \cite{sachan2017textbooks,kembhavi2016diagram,sampat2020visuo,lu2021inter,krishnamurthy2016semantic}, \name{} covers diverse topics across three subjects: natural science, social science, and language science. Moreover, most questions are annotated with grounded lectures and detailed explanations. The lecture provides general knowledge that introduces the background information for solving problems of a similar class. The explanation reveals a specific reason for the answer. To effectively answer the questions, a model often needs to be able to understand the multimodal content in the input and extract external knowledge, similar to how humans do. More importantly, the goal of \name{} is to aid development of a reliable model that is capable of generating a coherent chain of thought when arriving at the correct answer to reveal the multi-step reasoning process. For data collection details, see Appendix \ref{app:data_collection}.


\begin{figure}[htb] 
 \vspace{-1mm}
 \begin{minipage}{0.47\textwidth} 
 \centering
 \fontsize{8.0pt}{\baselineskip}\selectfont \renewcommand\tabcolsep{3.5pt} \renewcommand\arraystretch{0.85} \begin{tabular}{lc}
 \toprule
 \textbf{Statistic} & \textbf{Number} \\
 \midrule
 Total questions & 21,208 \\
 \midrule
 Questions with text context & 10,220 (48.2\%) \\
 Questions with image context & 10,332 (48.7\%) \\
 ~~~~~* Image of natural format & $\approx$2,960 (14.0\%) \\
 ~~~~~* Image of diagram format & $\approx$7,372 (34.8\%) \\
 Questions with both contexts & 6,532 (30.8\%) \\
 \fix{Questions without any context} & \fix{7,188 (33.9\%)} \\
 Questions with a lecture & 17,798 (83.9\%) \\
 Questions with a explanation & 19,202 (90.5\%) \\
 \midrule
 Different questions & 9,122 \\
 Different lectures & 261 \\
 \midrule
 Topic classes & 26 \\
 Category classes & 127 \\
 Skill classes & 379 \\
 \midrule
Average question length & 12.11 \\
 Average choice length & 4.40 \\
 Average lecture length& 125.06 \\
 Average explanation length & 47.66 \\
\bottomrule
 \end{tabular}
 \captionof{table}{Main statistics in \name{}.}
 \label{tab:statistics}
 \end{minipage} 
 \hfill
 \begin{minipage}{0.51\textwidth} 
\centering
 \includegraphics[width= 0.99\linewidth]{figs/fig_pie_user_0.6.pdf}
 \vspace{-1mm}
 \caption{Question distribution in \name{}.}
 \label{fig:que_pie}
 \end{minipage}\vspace{-1mm}
\end{figure}



\subsection{Data Analysis}

\textbf{Key statistics.} We randomly split the dataset into training, validation, and test splits with a ratio of 60:20:20. Each split has 12,726, 4,241, and 4,241 examples, respectively. Table \ref{tab:statistics} shows the main statistics of \name{}. \name{} has a large set of \fix{different} questions, totaling up to 9,122. Out of the 21,208 questions in \name{}, 10,332 (48.7\%) have an image context, 10,220 (48.2\%) have a text context, and 6,532 (30.8\%) have both. 83.9\% of the questions are annotated with a lecture, while 91.3\% of the questions feature an explanation. The cross-combination of these information sources diversifies the problem scenario: sometimes the model is given a lot of information from multiple sources, while at other times, the only source of information is the question itself. This level of complexity is very common in grade-level science exams.

\begin{figure}[ht] 
\begin{minipage}{0.5\textwidth} 
 \centering
 \includegraphics[width= 0.92\linewidth]{figs/ques_dist_datasets_new.pdf}
  \subcaption{Question length distribution of related datasets. \name{} is distributed more evenly in terms of the number of question words than other datasets.}
\end{minipage}
 \hfill
 \begin{minipage}{0.4\textwidth} 
 \centering
 \includegraphics[width= 0.82\linewidth]{figs/context_venn.pdf}
\subcaption{Question distribution with different context formats. 66.11\% of the questions in \name{} have either an image or text context, while 30.80\% have both.}
 \end{minipage}
\caption{Question length distribution (a) and context distribution in \name{} (b).}
\label{fig:ques_dist_context}
\end{figure}

\textbf{Question analysis.}
\name{} has a diverse set of science questions. Figure \ref{fig:que_pie} shows a distribution of the first four words in the question text. A large number of question lengths and formats highlight the diversity of \name{}.
The question lengths range from 3 words to 141 words, and the questions in \name{} have an average length of 12.11 words. The question length distribution is visualized against  other VQA datasets in Figure \ref{fig:ques_dist_context} (a). As shown in the diagram, \name{}'s distribution is flatter than other datasets, spanning more evenly across different question lengths. 

\noindent \textbf{Context analysis.}
Figure \ref{fig:ques_dist_context} (b) shows the number and percentage of questions with either an image context, a text context, or both. There are a total of 7,803 unique image contexts and 4,651 unique text contexts. 66.11\% of the questions have at least one type of context information. The image context is in the format of diagrams or natural images, which visualize the critical scenario necessary for question answering or simply illustrate the question for better understanding. Similarly, the textual context can provide either semantically rich information or a simple hint to the question. 
Therefore, models need to be flexible and general to understand these diverse types of contexts.


\noindent \textbf{Domain diversity.}
Each \name{} question belongs to one of the three subjects: natural science, language science, and social science. With each subject, questions are categorized first by the topic (\textit{Biology}, \textit{Physics}, \textit{Chemistry}, etc.), then by the category (\textit{Plants}, \textit{Cells}, \textit{Animals}, etc.), and finally by the specific skill (\textit{Classify fruits and vegetables as plant parts}, \textit{Identify countries of Africa}, etc.). \name{} has a total of 26 topics, 127 categories, and 379 skills. The treemap in Figure \ref{fig:sub_tree} visualizes the different subjects, topics, and categories and shows that \name{} questions are very diverse, spanning a wide range of domains.


\begin{figure}[t!]
 \centering
 \includegraphics[width=1.0\linewidth]{figs/fig_topic_distribution.pdf}
 \caption{Domain diversity in \name{}. Each color corresponds to one subject: \blue{natural science}, \magenta{social science}, and \green{language science}. For visual clarity, only the most frequent classes are shown.}
\label{fig:sub_tree}
\end{figure}

\subsection{Comparisons with Existing Datasets}
Table \ref{tab:dataset} shows a comparison of \name{} and other science problem datasets. As shown in the table, \name{} is much larger than most other datasets. \name{} also has the largest set of images, spans across all 12 grades, contains the longest questions, and has the most diverse input sources. As opposed to limiting the subject to only natural science, \name{} also includes social science and language science, largely adding to the domain diversity of the dataset. Furthermore, most of the questions in \name{} are annotated with textual lectures (83.9\%) and explanations (90.5\%), which reveal the reasoning path to the correct answer. To the best of our knowledge, \name{} is the first large-scale multimodal science question dataset that annotates the answers with detailed lectures and explanations.

\begin{table*}[th]
\centering
\fontsize{7.2pt}{\baselineskip}\selectfont \renewcommand\tabcolsep{1.3pt} \renewcommand{\arraystretch}{0.75}
\begin{tabular}{{l}*{12}{c}}
 \toprule	
 & \textbf{\#Q} & \textbf{\#I} & \textbf{AvgQ} & \textbf{MaxQ} & \textbf{Grades} & \textbf{Science subjects} & \textbf{Contexts} & \textbf{Images} & \textbf{Lecture} & \textbf{Explanation} \\ 
 \midrule
 \text{Geometry3K} \cite{lu2021inter} & 3,002 &2,342 & 10.1 & 46 & 6-12 & natural (geometry) & image & diagram & \xmark & \xmark \\
 \text{AI2D} \cite{kembhavi2016diagram} & 4,563 & 4,903 & 9.8 & 64 & 1-6 &
 natural & image & diagram & \xmark & \xmark \\
 \text{FOODWEBS} \cite{krishnamurthy2016semantic} &  $\approx$5,000 & $\approx$5,00 & - & - & 8 &
 natural (foodweb only) & image & diagram & \xmark & \xmark \\
 \text{ARC} \cite{clark2018think} & 7,787 & 0 & \textbf{20.4} & 128 & 3-9 &
 natural & \xmark & \xmark & \xmark & \xmark \\
\text{TQA} \cite{kembhavi2017you} & \textbf{26,260} &3,455 & 9.2 & 57 & 6-8 &
 natural & image, text & diagram & \cmark & \xmark \\
  IconQA \cite{lu2021iconqa} & 107,439 & 96,817 & 8.4 & 73 & PreK-3 & math & visual & diagram & \xmark & \xmark \\
 \midrule
 \text{WorldTree} \cite{jansen2018worldtree} & 1,680 & 0 & - & - & 3-5 & natural & \xmark & \xmark & \xmark  & \cmark \\
 \text{OpenBookQA} \cite{mihaylov2018can} & 5,957 & 0 & 10.6 & 68 & 1-6 & natural & \xmark & \xmark & \xmark  & \cmark \\
 \text{QASC} \cite{Khot2020QASCAD} & 9,980 & 0 & 8.0 & 25 & 1-9 & natural & \xmark & \xmark & \xmark  & \cmark \\
 \textbf{\name{} (ours)} & \underline{21,208} & \textbf{10,332} & \underline{12.1} & \textbf{141} & \textbf{1-12} &
 natural, social, language & image, text & natural, diagram& \cmark & \cmark \\
 \bottomrule
\end{tabular}
\caption{Statistics for \name{} and comparisons with existing datasets. \fix{\#Q: number of questions, \#I: number of images, AvgQ: average question length; MaxQ: maximum question length.}}
\label{tab:dataset}
\end{table*}


\section{Baselines and Chain-of-Thought Models}
\label{sec:method}
In this section, we establish baselines and develop two chain-of-thought models on \name{}.

\subsection{Baselines}
\textbf{Heuristic baselines.} The first heuristic baseline is \textit{random chance}: we randomly select one from the multiple options. Each trial is completed on the whole test set, and we take three different trials for an average result. The second heuristic baseline is \textit{human performance}. We post the task to Amazon Mechanical Turk and ask workers to answer \name{} questions. Only workers who obtain a high school or higher degree and pass the qualification examples are qualified for the study. Each worker needs to answer a set of 10 test questions, and each question is answered by three different workers. For more details of the human performance study, see Appendix \ref{app:human_performance}.

\textbf{Zero-shot and few-shot baselines.} We establish the zero-shot baselines on top of UnifiedQA \cite{khashabi2020unifiedqa} and GPT-3 \cite{chen2020big}. The zero-shot setup follows the format of  QCM$\rightarrow$A where the input is the concatenation of tokens of the question text (Q), the context text (C), and multiple options (M), while the output is to predict the answer (A) from the option set. We extract the caption from the captioning model based on ViT \cite{dosovitskiy2020image} and GPT-2 \cite{radford2019language} for the image as the visual context. In the few-shot setting, we follow the standard prompting \cite {brown2020language} where in-context examples from the training set are concatenated before the test instance. These in-context examples serve as an instruction for the language model to adjust to the specific task in \name{}. 

\textbf{Fine-tuning baselines.} We first consider the fine-tuning baselines from VQA models \cite{Anderson2017up,Kim2018,yu2019mcan,gao2019dynamic,pmlr-v139-kim21k,lu2021iconqa,li2019visualbert} proposed in recent years. These VQA baselines take the question, the context, and choices as the textual input, take the image as the visual input, and predict the score distribution over choice candidates via a linear classifier. In addition, we build the fine-tuning baseline on top of the large language model UnifiedQA \cite{khashabi2020unifiedqa}. UnifiedQA takes the textual information as the input and outputs the answer option. Similarly, the image is converted into a caption that provides the visual semantics for the language model.

\subsection{Language Models with the Chain of Thought}
\textit{A chain of thought} refers to a coherent flow of sentences that reveals the premises and conclusion of a reasoning problem \cite{wei2022chain}. A chain of thought clearly decomposes a multi-hop reasoning task into intermediate steps instead of solving the task in a black-box way. The chain of thought can be the step-by-step thought process \cite{wei2022chain} before arriving at the final answer or explanations \cite{narang2020wt5} that come after the answer. The annotated lectures and explanations in \name{} serve as \emph{demonstrations} of the chain of thought that mimics the multi-step reasoning steps of human beings. In this paper, we study if large language models can generate reasonable explanations as the chain of thought to reveal the thought process when answering \name{} questions. Further, we explore how the chain of thought can improve the reasoning ability of language models on \name{} in both few-shot and fine-tuning learning.


\textbf{UnifiedQA with the chain of thought.} UnifiedQA \cite{khashabi2020unifiedqa} is a state of the art model for multi-option question answering. The original architecture of UnifiedQA takes the question and options as the input and outputs a short phrase as the final answer. We make a format modification to develop UnifiedQA with the chain of thought (CoT), i.e., UnifiedQA is fine-tuned to generate a long sequence of text which consists of the answer followed by the lecture and explanation.
 
 \begin{figure}[t!]
\vspace{-1mm}
\centering
\footnotesize
\fontsize{8.5pt}{\baselineskip}\selectfont \begin{boxedminipage}{0.98\columnwidth}
Question: $\mathtt{question:}$ ${I_i^{ques}}$ \\
Options: (A) $\mathtt{option:}$ ${I_{i1}^{opt}}$ (B) $\mathtt{option:}$ ${I_{i2}^{opt}}$ (C) $\mathtt{option:}$ ${I_{i3}^{opt}}$ \\
Context: $\mathtt{context:}$  ${I_i^{cont}}$ \\
Answer: The answer is $\mathtt{answer:}$ $I_i^{a}$. BECAUSE: $\mathtt{lecture:}$ $I_i^{lect}$ ~ $\mathtt{explanation:}$ $I_i^{exp}$ \\
\\
Question: $\mathtt{question:}$ ${I_t^{ques}}$ \\
Options: (A) $\mathtt{option:}$ ${I_{t1}^{opt}}$ (B) $\mathtt{option:}$ ${I_{t2}^{opt}}$ (C) $\mathtt{option:}$ ${I_{t3}^{opt}}$ (D) $\mathtt{option:}$ ${I_{t4}^{opt}}$\\
Context: $\mathtt{context:}$ ${I_t^{cont}}$ \\
Answer:
\end{boxedminipage}
\caption{Prompt instruction encoding for the test example $t$ in GPT-3 (CoT). The prompt above consists of the instruction ${\{I_i\}_1}$ for the 1-shot training example and ${I_t}$ for the test example.}
\vspace{-1mm}
\label{fig:prompt_encoding}
\end{figure}

\textbf{GPT-3 via chain-of-thought prompting.} Recent research work \cite{chen2020big,mishra2022lila,lu2022dynamic} has shown that GPT-3 \cite{chen2020big} can perform various tasks when provided in-context examples in a standard prompt. Take multi-option question answering as an example, the standard prompt \cite{lu2021fantastically,zhao2021calibrate,liu2021makes} builds instructions using in-context examples with components of the question text, options, and the correct answer text. This style of few-shot learning enables the GPT-3 model to answer specific questions without parameter updates. Different from standard prompting, we build GPT-3 via chain-of-thought (CoT) prompting, as shown in Figure \ref{fig:prompt_encoding}. To be specific, for each test problem $t$, we map the prompt instruction $I: \{I_i\}_n, I_t$ into a textual format where $\{I_i\}_n$ refers to the instruction set of $n$-shot in-context examples from the training set, while  $I_t$ denotes the test instruction. Instead of the way where the explanation comes before the answer \cite{wei2022chain}, we feed the instruction $I$ into the encoder-decoder model GPT-3 to generate the answer $a$ followed by the lecture $lect$ and explanation $exp$: $M: \{I_i\}_n, I_t \rightarrow a, lect, exp$.




\section{Experiments}
\label{sec:experiment}

\subsection{Experimental Setup}
\label{sec:exp_setup}
\textbf{Evaluation metrics.} The heuristics and VQA baselines treat our \name{} task as a multi-class classification problem with multiple options and are evaluated with the accuracy metrics. UnifiedQA and GPT-3 treat \name{} as a text generation problem. So the most similar option is selected as the final prediction to evaluate the question answering accuracy. The generated lectures and explanations are evaluated by automatic metrics \cite{papineni2002bleu,lin2004rouge,reimers-2019-sentence-bert} and human scores by annotators.

\textbf{Implementation details.} The VQA baselines are trained for a maximum number of 50 epochs with a learning rate of $5e{-}5$. We fine-tune the UnifiedQA for 50$k$ iterations and evaluate every 1$k$ iteration. The training process \fix{is} stopped following the early stopping strategy with a patience period of three evaluations. For GPT-3, we use the $\texttt{text-davinci-002}$ engine, which is the most capable model version suggested in the official documentation. More details can be found in Appendix \ref{sec:exp_details}.

\subsection{Results for Question Answering}

Table \ref{tab:main_results} demonstrates the empirical results for Science Question Answering. 


\begin{table}[t!]
\centering
\fontsize{8.3pt}{\baselineskip}\selectfont \renewcommand\tabcolsep{3.2pt} \renewcommand\arraystretch{0.93} \resizebox{1.0\linewidth}{!}{
\begin{tabular}{ccc|ccc|ccc|cc|p{1.2cm}} 
\toprule
 Model & Learning & Format & NAT & SOC & LAN & TXT & IMG & NO & G1-6 & G7-12 & ~Avg \\
\midrule
 Random chance & - & M$\rightarrow$A & 40.28 & 46.13 & 29.25 & 47.45 & 40.08 & \fix{33.66} & 39.35 & 40.67 & 39.83 \\
 \midrule
 Q only \cite{Anderson2017up} & train set & Q$\rightarrow$A & 41.34 & 27.22 & 47.00 & 41.79 & 35.15 & \fix{44.60} & 39.28 & 40.87  & 39.85 \\
 C$_I$ only \cite{Anderson2017up} & train set & C$_I$$\rightarrow$A & 41.34 & 29.25 & 45.45 & 42.33 & 36.09 & \fix{42.93} & 39.21 & 41.07 & 39.87 \\
 Q+M only \cite{Anderson2017up} & train set & QM$\rightarrow$A & 52.66 & 51.86 & 60.18 & 55.57 & 50.37 & \fix{57.42} & 52.53 & 57.88 & 54.44 \\
 Q+C$_T$+M only \cite{Anderson2017up} & train set & QC$_T$M$\rightarrow$A & 57.28 & 49.04 & \underline{61.36} & 60.46 & 52.80 & \fix{\underline{58.82}} & 54.44 & 60.51 & 56.61 \\
 Q+C$_I$+M only \cite{Anderson2017up} & train set & QC$_I$M$\rightarrow$A & \underline{58.97} & \underline{53.77} & 60.45 & \underline{62.85} & \underline{54.49} & \fix{57.63} & \underline{56.72} & \underline{61.04} & \underline{58.26} \\
 \midrule
 MCAN \cite{yu2019mcan} & train set & QCM$\rightarrow$A & 56.08 & 46.23 & 58.09 & 59.43 & 51.17 & \fix{55.40} & 51.65 & 59.72 & 54.54 \\
 Top-Down \cite{Anderson2017up} & train set & QCM$\rightarrow$A & 59.50 & 54.33 & 61.82 & 62.90 & 54.88 & \fix{59.79} & 57.27 & 62.16 & 59.02 \\
 BAN \cite{Kim2018} & train set & QCM$\rightarrow$A & 60.88 & 46.57 & 66.64 & 62.61 & 52.60 & \fix{\underline{65.51}} & 56.83 & 63.94 & 59.37 \\
 DFAF \cite{gao2019dynamic} & train set & QCM$\rightarrow$A & 64.03 & 48.82 & 63.55 & 65.88 & 54.49 & \fix{64.11} & 57.12 & 67.17 & 60.72 \\
 ViLT \cite{pmlr-v139-kim21k} & train set & QCM$\rightarrow$A & 60.48 & 63.89 & 60.27 & 63.20 & 61.38 & \fix{57.00} & 60.72 & 61.90 & 61.14 \\
 Patch-TRM \cite{lu2021iconqa} & train set & QCM$\rightarrow$A & \underline{65.19} & 46.79 & \underline{65.55} & \underline{66.96} & 55.28 & \fix{64.95} & 58.04 & \underline{67.50} & 61.42 \\
 VisualBERT \cite{li2019visualbert,li2020does} & train set & QCM$\rightarrow$A & 59.33 & \underline{69.18} & 61.18 & 62.71 & \underline{62.17} & \fix{58.54} & \underline{62.96} & 59.92 & \underline{61.87} \\
 \midrule
 UnifiedQA$_\text{SMALL}$ \cite{raffel2020exploring} & zero-shot& QCM$\rightarrow$A & 47.78 & 40.49 & 46.00 & 50.24 & 44.12 & \fix{44.39} & 45.56 & 46.21 & 45.79 \\
 UnifiedQA$_\text{BASE}$ \cite{raffel2020exploring} & zero-shot& QCM$\rightarrow$A & 50.13 & 44.54 & 48.18 & 53.08 & 48.09 & \fix{46.69} & 47.58 & 50.03 & 48.46 \\
  UnifiedQA$_\text{SMALL}$ \cite{raffel2020exploring} & train set & QCM$\rightarrow$A & 53.77 & 58.04 & 61.09 & 52.10 & 51.51 & \fix{61.46} & 58.22 & 53.59 & 56.57 \\
 UnifiedQA$_\text{BASE}$ \cite{raffel2020exploring} & train set & QCM$\rightarrow$A & 68.16 & 69.18 & 74.91 & 63.78 & 61.38 & \fix{77.84} & 72.98 & 65.00 & 70.12 \\
 \textbf{UnifiedQA$_\text{BASE}$ (CoT)} & train set & QCM$\rightarrow$AE & 70.60 & 74.02 & 78.36 & 65.69 & 64.80 & \fix{81.53} & 75.48 & \underline{69.48} & 73.33$_{3.21 \uparrow}$ \\
 \textbf{UnifiedQA$_\text{BASE}$ (CoT)} & train set & QCM$\rightarrow$ALE & \underline{71.00} & \underline{76.04} & \underline{78.91} & \underline{66.42} & \underline{66.53} & \fix{\textbf{81.81}} & \underline{77.06} & 68.82 & \underline{74.11}$_{3.99 \uparrow}$  \\
 \midrule
 GPT-3 \cite{chen2020big} & zero-shot& QCM$\rightarrow$A & 75.04 & 66.59 & 78.00 & 74.24 & 65.74 & \fix{79.58} & 76.36 & \textbf{69.87} & 74.04 \\
 GPT-3 \cite{chen2020big} & 2-shot& QCM$\rightarrow$A & 74.64 & 69.74 & 76.00 & 74.44 & 67.28 & \fix{77.42} & 76.80 & 68.89 & 73.97 \\
 \textbf{GPT-3 (CoT)} & 2-shot& QCM$\rightarrow$AE & \textbf{76.60} & 65.92 & 77.55 & \textbf{75.51} & 66.09 & \fix{79.58} & \textbf{78.49} & 67.63 & 74.61$_{0.64 \uparrow}$ \\
 \textbf{GPT-3 (CoT)} & 2-shot& QCM$\rightarrow$ALE & \text{75.44} & \textbf{70.87} & \textbf{78.09} & \text{74.68} & \textbf{67.43} & \fix{\underline{79.93}} & \text{78.23} & \text{69.68} & \textbf{75.17}$_{1.20 \uparrow}$ \\
 \midrule
 Human & - & QCM$\rightarrow$A & 90.23 & 84.97 & 87.48 & 89.60 & 87.50 & \fix{88.10} & 91.59 & 82.42 & 88.40 \\
 \bottomrule
\end{tabular}
}
 \vspace{1mm}
 \caption{Evaluation of baselines over different classes in accuracy (\%). Model names: Q = question, M = multiple options, C = context, C$_T$ = text context, C$_I$ = image context, CoT = chain of thought. Format names: A = answer, AE = answer with explanation, ALE = answer with lecture and explanation. Question classes: NAT = natural science, SOC = social science, LAN = language science, TXT = text context, IMG = image context\fix{, NO = no context}, G1-6 = grades 1-6, G7-12 = grades 7-12. \fix{Segments 1: Random chance; Segment 2: Ablation studies on top of Top-Down; Segment 3: VQA baselines; Segment 4: UnifiedQA baselines and UnifiedQA with CoT; Segment 5: GPT-3 baselines and GPT-3 with CoT; Segment 6: Average human performance.}}
\label{tab:main_results}
\end{table}


\textbf{VQA baselines.} We feed the VQA baseline models with the input of QCM format to predict answers A. Out of all the VQA models we benchmarked, VisualBERT \cite{li2019visualbert,li2020does} performs the best on average (61.87\%). Interestingly, Patch-TRM \cite{lu2021iconqa} beats VisualBERT in natural science (NAT) and language science (LAN), and it also performs better in higher-grade questions (67.50\% \textit{v.s.} 59.92\%). However, in the subject of social science (SOC), VisualBERT outperforms Patch-TRM by a large margin (+22.39\%). 
Such drastic changes in performance might imply that current VQA models are not generalized to process the challenging questions in \name{}.

\textbf{Language models.} We evaluate whether large-scale pretraining on text can help language models learn scientific knowledge and thus perform better on the \name{} task. For this purpose, we have tried two of the state-of-the-art pre-trained language models: UnifiedQA and GPT-3.

\text{(i)} \textbf{UnifiedQA.} The results show that without any supervised fine-tuning (zero-shot), UnifiedQA cannot beat any VQA baseline model, while the pretraining does help the model obtain some scientific knowledge to outperform the random baseline. When fine-tuned with the answer labels in \name{}, UnifiedQA$_\text{BASE}$ reports an accuracy of 70.12\% on average. By further teaching the model to generate the answer along with lecture and explanation, the developed language model with chain-of-thought (UnifiedQA$_\text{BASE}$ (CoT)) brings additional improvements of +3.21\% (QCM$\rightarrow$AE) and +3.99\% (QCM$\rightarrow$ALE). These results show that generating the chain of thought along with the answer benefits the reasoning ability of language models. 

\text{(ii)} \textbf{GPT-3.} The positive effect of pretraining is also proved by the surprisingly good results from GPT-3 in the same zero-shot setting as UnifiedQA. Without any fine-tuning, GPT-3 already reaches almost the best performance we can get. Interestingly, prompting the GPT-3 with two training examples with only answers results in a negligible difference. However, if we prompt GPT-3 with chain-of-thought prompting (QCM$\rightarrow$ALE), we obtain the state-of-the-art result \fix{so far (75.17\%)}.

\textbf{Human performance.} Humans outperform all benchmarks consistently across question classes, context types, and grades, \textit{e.g.,} a 20.07\% gap for questions with the image context (IMG) between humans and our best performing model. The gap is to be filled by future research on multimodal reasoning for scientific question answering.
 
 
\subsection{Results for Generated Explanations}

\begin{figure}[t!]
 \centering
\includegraphics[width=1.0\linewidth]{figs/gpt3/gpt_result_0.pdf}
 \caption{One example of the predicted answer along with the chain of thought from GPT-3 (CoT).}
 \label{fig:gpt3_result}
\end{figure}

One prediction example of GPT-3 (CoT) is visualized in Figure \ref{fig:gpt3_result}. We can see that GPT-3 (CoT) predicts the correct answer and generates a reasonable lecture and explanation to mimic the human thought process.
We further report automatic metrics (\text{BLEU-1\fix{/4}} \cite{papineni2002bleu}, \text{ROUGE-L} \cite{papineni2002bleu}, and (sentence) Similarity \cite{reimers-2019-sentence-bert} to evaluate the generated lectures and explanations, as shown in Table \ref{tab:explanation_evaluation}. The  Similarity metric computes the cosine-similarity of semantic embeddings  between two sentences based on the Sentence-BERT network \cite{reimers-2019-sentence-bert}.
The results show that \text{UnifiedQA$_\text{BASE}$ (CoT)} generates the most similar explanations to the given ones. However, it's commonly agreed that automatic evaluation of generated texts only provides a partial view and has to be complemented by a human study. By asking annotators to rate the relevance, correctness, and completeness of generated explanations, we find that the explanations generated by GPT-3 (CoT) conform best to human judgment.




\begin{table}[ht]
\centering
\fontsize{8.5pt}{\baselineskip}\selectfont \renewcommand\tabcolsep{2.0pt} \renewcommand\arraystretch{1.0} \resizebox{1.0\linewidth}{!}{
\begin{tabular}{ll|ccc|c|cccc} 
\hline
Similarity\text{Model} & Format & \text{BLEU-1}  & \fix{BLEU-4} & \text{ROUGE-L}& Similarity & Relevant & Correct & Complete & Gold \\ 
\hline
\text{UnifiedQA$_\text{BASE}$ (CoT)} & QCM$\rightarrow$ALE & \textbf{0.397} & \textbf{\fix{0.370}} & \textbf{0.714} & \textbf{0.811} & 80.4\% & 76.6\% & 76.1\% & 56.9\% \\
\text{GPT-3 (CoT)} & QCM$\rightarrow$AE & 0.234 & \fix{0.048} & 0.351 & 0.561 & 76.9\% & 73.0\% & 70.5\% & 52.5\% \\
\text{GPT-3 (CoT)} & QCM$\rightarrow$ALE & 0.192 & \fix{0.052} & 0.323& 0.595 & \textbf{88.5\%} & \textbf{78.8\%} & \textbf{84.5\%} & \textbf{65.2\% }\\
\hline
\end{tabular}
}
\vspace{1mm}
\caption{Automatic metrics (\text{BLEU-1\fix{/4}}, \text{ROUGE-L}, Similarity) and human evaluation of generated explanations. Note that a gold explanation refers to one that is relevant, correct, and complete.}
\label{tab:explanation_evaluation}
\end{table}

\subsection{Analysis}
\label{sec:analysis}
\textbf{Blind studies.} Blind studies are conducted on top of the modification of the full model, Top-Down \cite{Anderson2017up}. The results achieved in blind studies of Q only and C$_I$ only are close to random chance, showing that the \name{} dataset is robust and reliable in distribution. The performance drops in Q+M only, Q+C$_T$+M only, and Q+C$_I$+M only indicate that all input components provide critical information for answering \name{} questions.

\textbf{Prompt types.} We study the effect of prompt types and visualize the comparison in Figure \ref{fig:gpt3_prompt} (a). It shows that prompting the GPT-3 model with both lectures and explanations (QCM$\rightarrow$ALE) results in the highest accuracy on average and the smallest variance. In contrast, prompting with only explanations (QCM$\rightarrow$AE) gives the largest variance, resulting in a less stable model.

\begin{figure}[ht] 
\begin{minipage}{0.49\textwidth} 
\centering
 \includegraphics[width= 0.85\linewidth]{figs/fig_gpt3_prompt_type_acc.pdf}
 \subcaption{Acc. v.s. different prompts with 4-shot examples.}
\end{minipage}
 \hfill
 \begin{minipage}{0.49\textwidth} 
 \includegraphics[width= 0.85\linewidth]{figs/fig_gpt3_example_number_acc.pdf}
 \subcaption{Acc. v.s. different \# of training examples.}
 \end{minipage}
\caption{Accuracy of GPT-3 (CoT) cross different prompt types (a) and \# of training examples (b).}
\label{fig:gpt3_prompt}
\end{figure}


\textbf{Number of in-context examples.} In Figure \ref{fig:gpt3_prompt} (b), we further investigate how different numbers of training examples encoded in prompts can affect the prediction accuracy. The QCM$\rightarrow$ALE prompt type outperforms or performs comparably the QCM$\rightarrow$A type with all numbers of examples. And we observe the peak performance of QCM$\rightarrow$ALE with 2 training examples being prompted. After that, the accuracy goes down as more training examples are added to the model.


\begin{wraptable}{r}{0.5\textwidth}
\vspace{-3.5mm}
\centering
\fontsize{8.5pt}{\baselineskip}\selectfont \renewcommand\tabcolsep{3.0pt} \renewcommand\arraystretch{1.0} \begin{tabular}{llc} 
\hline
\textbf{Prompt type} & \textbf{Sampling} & \textbf{Acc. (\%)}  \\ 
\hline
QCM$\rightarrow$ALE & Dynamic (same topic) & 75.15  \\
QCM$\rightarrow$ALE & Dynamic (same category) & 74.58 \\
QCM$\rightarrow$ALE & Dynamic (same skill) & 75.10  \\
\hline
\end{tabular}
\vspace{-1mm}
\caption{Dynamic sampling for GPT-3 (CoT).}
\vspace{-6mm}
\label{tab:gpt3_dynamic}
\end{wraptable}

\textbf{Dynamic sampling.} In Table \ref{tab:gpt3_dynamic}, instead of random sampling, we try to dynamically select the in-context examples to prompt with the same class as the test sample. However, slight differences in prediction accuracy are observed when comparing them to simple random sampling.

\textbf{Upper bound.} We search the upper bound of the GPT-3 accuracy by feeding the gold lecture and explanation in the test prompt. 
As reported in Table \ref{tab:gpt3_upper_limit}, QCME*$\rightarrow$A outperforms the QCM$\rightarrow$ALE baseline by 18.86\% and QCMLE*$\rightarrow$A outperforms QCM$\rightarrow$ALE by 18.96\%, indicating a potential improvement direction by generating correct explanations before answering science questions.


\begin{figure}[ht] 
\begin{minipage}{0.4\textwidth} 
    \centering
\centering
\fontsize{8.5pt}{\baselineskip}\selectfont \begin{tabular}{lll} 
    \hline
    \textbf{Prompt type} & \textbf{Sampling} & \textbf{Acc. (\%)}  \\ 
    \hline
    QCML*$\rightarrow$A & Random & 73.59 \\
    \fix{QCML*$\rightarrow$AE} & \fix{Random} & \fix{74.32} \\
    QCME*$\rightarrow$A & Random & 94.03$_{18.86 \uparrow}$  \\
    QCMLE*$\rightarrow$A & Random & \textbf{94.13}$_{18.96 \uparrow}$ \\
    \hline
    QCM$\rightarrow$ALE & Random & 75.17 \\
    \hline
    \end{tabular}
    \captionof{table}{Upper bound of GPT-3 (CoT).}
\label{tab:gpt3_upper_limit}
    \end{minipage}
 \hfill
 \begin{minipage}{0.59\textwidth} 
\centering
\fontsize{8.5pt}{\baselineskip}\selectfont {\color{black}
    \begin{tabular}{lll} 
    \hline
    \textbf{Prompt type} & \textbf{Sampling} & \textbf{Acc. (\%)}  \\ 
    \hline
    QCM$\rightarrow$LA & Random & 60.6 \\
    QCM$\rightarrow$EA & Random & 56.0 \\
    QCM$\rightarrow$LEA & Random & 55.4 \\
    QCM$\rightarrow$ELA & Random & 51.5 \\
    \hline
    QCM$\rightarrow$ALE & Random & \textbf{73.6} \\
    \hline
    \end{tabular}
    }
    \captionof{table}{Different positions of L/E for GPT-3 (CoT).}
    \label{tab:gpt3_ALE_or_LEA}
 \end{minipage}
 \vspace{-2mm}
\end{figure}


\textbf{Positions of lectures and explanations.} We study the performance of GPT-3 (CoT) in terms of different positions of lectures and explanations on 1,000 test examples. The results are shown in Table  \ref{tab:gpt3_ALE_or_LEA}. There could be huge accuracy decreases if GPT-3 (CoT) predicts lectures and explanations before answers. It is mainly because if GPT-3 (CoT) is formalized to generate the long lecture and explanation first, there is a greater chance that it will stop generating the prediction early or use up the maximum token limits before obtaining the required answer.

\begin{wrapfigure}{r}{0.38\textwidth}
\centering
 \vspace{-4.5mm}
 \includegraphics[width=1.0\linewidth]{figs/fig_unifiedqa_less_data_acc.pdf}
 \caption{UnifiedQA (CoT) learns efficiently with fewer training examples.}
 \vspace{-6.0mm}
 \label{fig:unifiedqa_less_data}
\end{wrapfigure}


\textbf{CoT learns with fewer data.}  To study if the chain of thought helps language models learn more efficiently, we report the accuracies of UnifiedQA and UnifiedQA (CoT) fine-tuned on different sizes of the training set in Figure \ref{fig:unifiedqa_less_data}. UnifiedQA (CoT) benefits language models by learning the coherent reasoning path when answering questions, resulting in similar accuracy with fewer training examples.

\textbf{Error analysis.} GPT-3 via chain-of-chain prompting obtains promising results but still fails to answer a wide range of challenging questions in \name{}. See examples of failure cases in Appendix \ref{app:limitations}. The failure cases can be classified into two types: (a) the model fails to understand the multimodal inputs and lacks domain-specific knowledge to arrive at the correct answer; (b) the model generates the wrong chain of thought with irrelevant, incorrect, or incomplete information.


\vspace{-2mm}
\section{Discussion and Conclusion}
\vspace{-2mm}

In this paper, we propose \name{}, a dataset that features 21,208 multi-option questions with multimodal contexts from the science curriculum. To the best of our knowledge, \name{} is the first large-scale multimodal science dataset where most questions are annotated with corresponding lectures and explanations. We establish various baselines, including recent VQA models and large language models on \name{}. We further study if language models can generate reasonable explanations and then benefit the reasoning ability. Experiments show that UnifiedQA with the chain of thought can achieve an improvement of 3.99\% and few-shot GPT-3 via chain-of-thought (CoT) prompting can obtain a satisfactory accuracy of 75.17\% on \name{}. 65.2\% of the generated explanations from GPT-3 (CoT) meet the gold standard by human evaluations.



\section{Acknowledgment}

We would like to thank the anonymous reviewers for their valuable comments and suggestions. We would also like to thank Xiaodan Liang for insightful discussions on dataset collection. We thank our colleagues at The Allen Institute of AI (AI2), Jiasen Lu and Jungo Kasai for helpful discussions. The work does not relate to Liang Qiu's position at Amazon Alexa.


\bibliographystyle{plain} 
{
\small
\bibliography{egbib}
}


\section*{Checklist}



\begin{enumerate}

\item For all authors...
\begin{enumerate}
 \item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
 \answerYes{}
 \item Did you describe the limitations of your work?
 \answerYes{Yes, we did the error analysis in Section \ref{sec:analysis} and discussed the limitations of the work in Appendix \ref{app:limitations}.}
 \item Did you discuss any potential negative societal impacts of your work?
 \answerYes{We discussed the broader impacts in Appendix \ref{sec:impact}.}
 \item Have you read the ethics review guidelines and ensured that your paper conforms to them?
 \answerYes{}
\end{enumerate}

\item If you are including theoretical results...
\begin{enumerate}
 \item Did you state the full set of assumptions of all theoretical results?
 \answerNA{}
	\item Did you include complete proofs of all theoretical results?
 \answerNA{}
\end{enumerate}

\item If you ran experiments...
\begin{enumerate}
 \item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
 \answerYes{We included 100 data examples and the data visualizer tool in the supplemental material. The whole dataset and code will be available at \url{https://scienceqa.github.io}.}
 \item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
 \answerYes{See Section \ref{sec:exp_setup} and Appendix \ref{sec:exp_details} for experimental details.}
	\item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
 \answerYes{We reported the error bars for GPT-3 (CoT) experiments in Figure \ref{fig:gpt3_prompt}, where each experiment was repeated four times.}
	\item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
 \answerYes{We discussed compute resources in Appendix \ref{sec:exp_details}.}
\end{enumerate}

\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
\begin{enumerate}
 \item If your work uses existing assets, did you cite the creators?
 \answerYes{We collected the \name{} dataset from \url{https://www.ixl.com/}. The  copyright belongs to IXL.}
 \item Did you mention the license of the assets?
 \answerYes{\name{} is under the CC BY-NC-SA 4.0 license and is used for non-commercial research purposes.}
 \item Did you include any new assets either in the supplemental material or as a URL?
 \answerYes{We included data examples and a visualizer tool in the supplemental material. The dataset will be available at \url{https://scienceqa.github.io}.}
 \item Did you discuss whether and how consent was obtained from people whose data you're using/curating?
 \answerNA{}
 \item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
 \answerYes{The collected data does not contain personally identifiable information or offensive content.}
\end{enumerate}

\item If you used crowdsourcing or conducted research with human subjects...
\begin{enumerate}
 \item Did you include the full text of instructions given to participants and screenshots, if applicable?
 \answerYes{We included screenshots of the instructions in Appendix \ref{app:human_performance} and \ref{app:human_evaluation}.}
 \item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
 \answerNA{}
 \item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
 \answerYes{We included the monetary compensation details in Appendix \ref{app:human_performance} and \ref{app:human_evaluation}.}
\end{enumerate}

\end{enumerate}


