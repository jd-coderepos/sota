\section{Experiments}

\label{details}
{\bf Training data \& Model setups.} Following~\citep{hmr:kanazawa2018end, vibe:kocabas2020vibe, maed:wan2021encoder}, we use mixed 3D video, 2D video and 2D image datasets for training. The details of model configurations and training data are described in Appendix~\ref{appendix:model} and Appendix~\ref{appendix:data}. 

{\bf Evaluation.} We report results on 3DPW and Human3.6m datasets,  using 4 standard metrics, including Procrustes-Aligned Mean Per Joint Position Error (PA-MPJPE), Mean Per Joint Position Error (MPJPE), Per Vertex Error (PVE) and ACCELeration error (ACCEL). The unit for these metrics is millimetter (mm). We evaluate the models trained w/ and wo/ 3DPW training set for comparison with previous methods.







{\bf Progressive training scheme.}  In practice, we find how to exploit these training datasets with such multi-modal supervision is critical to training the whole model well and ensuring the generalization to in-the-wild scenes. Following MAED~\citep{maed:wan2021encoder},  we develop an improved progressive training scheme adapting to our model, which consists of three training phases. Please see more details about this training scheme in Appendix~\ref{appendix:training}.



\begin{table*}
	
		\label{tab:sota}
	\caption{Comparisons to state-of-the-art models on 3DPW and Human3.6M datasets. 
INT-1 model is trained with first two phases, and separately evaluated the best performance for both datasets. INT-2 model is trained with three phases, and the best model for 3DPW is directly evaluated on Human3.6m.
		 represents training w/o 3DPW training dataset.  represents training w/ 3DPW training dataset. For fair comparison, we also list the CNN backbones (ResNet~\citep{resnet:he2016deep} or HRNet~\citep{hrnet:sun2019deep}) used in different methods.}
	\centering
	\resizebox{0.95\textwidth}{!}{
		\begin{tabular}{llccccccc}
			\toprule
			&  & & \multicolumn{4}{c}{3DPW} & \multicolumn{2}{c}{H36M} \\
			\cmidrule(lr){4-7} \cmidrule(lr){8-9} 
			& Models & Backbone & PA-MPJPE  & MPJPE  & PVE  & Accel  & PA-MPJPE  & MPJPE  \\
			
			\midrule
			
			\parbox[t]{2mm}{\multirow{12}{*}{\rotatebox[origin=c]{90}{Image-based}}}
			& 
			HMR~\citep{hmr:kanazawa2018end}& ResNet-50 & 76.7 & 130.0 & - & 37.4  & 56.8 & 88 \\
			&  Neural Body~\citep{neuralbodyfitting:omran2018neural}& ResNet-101 &   - &   - &   - &   - &    59.9 &   - \\
&   Mesh Regression~\citep{meshregression:kolotouros2019convolutional} & ResNet-50 &   70.2 &   - &   - &   -  &   50.1 &   - \\
			&   SPIN~\citep{spin:kolotouros2019learning} & ResNet-50 &   59.2 &   96.9 &   116.4 &   29.8 &    41.1 &   - \\
			& I2l-MeshNet~\citep{i2l-meshnet:moon2020i2l} &ResNet-50&57.7 & 93.2 & 110.1 & 30.9 & 41.1 & 55.7 \\
			& PyMAF~\citep{pymaf:zhang2021pymaf} &ResNet-50& 58.9& 92.8 & 110.1 & - &40.5 & 57.7\\
			& Hybrik~\citep{hybrik:li2021hybrik} &ResNet-34& 48.8 & 80.0 & 94.5 & 34.5 & 54.4 \\
			
			& ROMP~\citep{romp:sun2021monocular} &ResNet-50& 49.7 & 79.7 & 94.7 &- & -&-\\
			& ROMP~\citep{romp:sun2021monocular} &HRNet-W32& 47.3 & 76.7 & 93.4 &- & -&-\\
			
	
            & PARE~\citep{pare:kocabas2021pare}  &ResNet-50& 52.3 & 82.9 & 99.7 & - & - & - \\
			& PARE~\citep{pare:kocabas2021pare}  &HRNet-W32& 50.9 & 82.0 & 97.9 & - & - & - \\
			& PARE~\citep{pare:kocabas2021pare} &HRNet-W32& 46.5 & \textbf{74.5} & 88.6 & - & - & - \\
			
			& METRO~\citep{metro:lin2021end} &ResNet-50~& - & - & - & - & 40.6 & 56.5 \\
			& METRO~\citep{metro:lin2021end} &HRNet-W64~& 47.9 & 77.1 & 88.2 & - & 36.7 & 54.0 \\
		
			& Mesh Graphormer~\citep{meshgrahormer:lin2021mesh} &HRNet-W64& \textbf{45.6 }& 74.7 &\textbf{ 87.7} & - & \textbf{34.5} & \textbf{51.2 }\\
			
			\midrule
			
			\parbox[t]{2mm}{\multirow{12}{*}{\rotatebox[origin=c]{90}{Video-based}}} 
			& 
			HMMR~\citep{hmmr:kanazawa2019learning} & ResNet-50& 72.6 & 116.5 & 139.3 & 15.2 & 56.9 & - \\
&   Sim2Real~\citep{sim2real:doersch2019sim2real} &ResNet-50&   74.7 &   - &   - &   - &   - &    - \\
			& Temporal Context~\citep{temporalcontext:arnab2019exploiting}& ResNet (from HMR) & 72.2 & - & - & - &  54.3 & 77.8 \\
			& Skeleton-disentangled~\citep{skeleton-disentangled:sun2019human} & ResNet-50 & 69.5 & - & - & - &  42.4 & 59.1 \\
& VIBE~\citep{vibe:kocabas2020vibe}&ResNet (from SPIN) & 51.9 & 82.9 & 99.1 & 23.4 & 41.4 & 65.6 \\ 
			&TCMR~\citep{tcmr:choi2021beyond} &ResNet (from SPIN)& 55.8 & 95.0 & 111.3 & \textbf{6.7} & 41.1 & 62.3 \\
			& MPS-Net~\citep{mps-net:wei2022capturing} &ResNet (from SPIN) & 52.1 & 84.3 & 99.7 &7.4 & 47.4 & 69.4\\
&  MAED~\citep{maed:wan2021encoder} &ResNet-50& 45.7 & 79.1 & 92.6 & 17.6 & 38.7 & 56.4 \\ 
			\cmidrule(lr){2-9}
			& \textbf{INT-1 (Ours)}  &ResNet-50& 49.7   & 90.0   & 105.1   & 23.5   & 39.1   & 57.1  \\
			& \textbf{INT-2 (Ours)} &ResNet-50& \textbf{42.0}  & \textbf{75.6} & \textbf{87.9}  & 16.5  & \textbf{38.4}  & \textbf{54.9}  \\
			
			\bottomrule
			& 		\end{tabular}
		
	}\vspace{-0.2in}

\end{table*}                                                                                                                           


\subsection{Comparison with state-of-the-art methods}


                                     

{\bf Quantitative results.} In Tab.~\ref{tab:sota}, we compare our method with state-of-the-art image-based and video-based HMR methods. We evaluate the models trained w/ and w/o 3DPW training set for fair comparisons.  For 3DPW, our model substantially outperforms all these methods, mainly in PA-MPJPE, MPJPE and PVE. For Human3.6m, our model is on par with state-of-the-art methods. 
MAED~\citep{maed:wan2021encoder} is our main competitor, as both use the same backbone and training data. The results show that our model achieve superior performances in all metrics over MAED, particularly in PA-MPJPE on 3DPW gaining 8.1\% performance improvement. 
Our model achieves a significant improvement when trained with 3DPW, which indicates that using accurate SMPL pose and shape labels as supervision is critical to improve the generalization to in-the-wild scenes. 
Notably, METRO~\citep{metro:lin2021end} and Mesh Graphormer~\citep{meshgrahormer:lin2021mesh} are recent Transformer-based SOTA methods, with a stronger CNN extractor HRNet-W64~\citep{hrnet:sun2019deep} as the backbone, showing better performance. In comparison to them, our method shows superior performance in PA-MPJPE on 3DPW and comparable results in MPJPE and PVE, even using ResNet-50~\citep{resnet:he2016deep} as the backbone. These results demonstrate the effectiveness and superiority of our token-based model design.

{\bf Qualitative results.} To demonstrate qualitative mesh reconstruction results, we select a typical showcase of 3DPW dataset to compare our model with the current video-based SOTA method MAED~\citep{maed:wan2021encoder}, as shown in Fig.~\ref{fig:vis_contrast_3dpw}. We can see that MAED performs well in most frames but still produces some bad fit in hard samples. 
In contrast, our model produces accurate pixel accurate mesh alignment that better fit to 2D human silhouette, resulting in more natural and smoother motion. 
This phenomenon suggests that our scheme of separate joint rotation predictions may produce more flexible and adaptive human mesh than regressing the whole pose.
Please see more in-the-wild examples in the Appendix~\ref{appendix:examples} for reference.







\subsection{Ablation study}






\begin{table}[!htbp]\footnotesize
	\caption{Study on the effectiveness of temporal modeling. We evaluate the performances of the image-based  model and two types of video-based temporal models.}
	\label{tab:image_vs_video}
	\centering
	
			\renewcommand{\arraystretch}{1.1}
	\setlength{\tabcolsep}{0.9mm}
	\resizebox{.6\textwidth}{!}{
	\begin{tabular}{cccc}
		\toprule
	 Mode & Temporal modeling   & PA-MPJPE   & Accel  \\
		\midrule
	Image-based& N/A&  45.6   & 23.5 \\
	 Video-based &  whole-pose temporal learning  & 43.9  &   19.1 \\
	 Video-based&   separate joint temporal learning  & \textbf{42.0} & \textbf{16.5}   \\
	
		
		\bottomrule
	\end{tabular}}

	
\end{table}


\begin{figure}[t]
	\centering
	\includegraphics[width=1.\linewidth]{figures/vis_contrast.pdf}
	\caption{The qualitative comparisons between our model (top) and the reproduced MAED model~\citep{maed:wan2021encoder} (bottom).  
}
	\label{fig:vis_contrast_3dpw}
\end{figure}


{\bf Pure image-based model vs. video-based temporal models.} To study the effectiveness of the temporal modeling, we evaluate the models when trained with and without the Temporal Transformer. When using the Temporal Transformer, we conduct two types of temporal modeling schemes -- separate joint temporal learning and whole-pose temporal learning. In Tab.~\ref{tab:image_vs_video}, the results show that video-based ones have obvious advantages in the PA-MPJPE and Accel metrics compared with the image-based one.

{\bf Separate joint temporal learning vs. whole-pose temporal modeling.} We study the differences between the separate joint temporal learning and whole-pose temporal learning. We implement a new model that concatenates all joint tokens (each is -dim) together as a pose vector (-dim) and then use the Temporal Transformer to capture the overall changes in pose over time. As shown in Tab.~\ref{tab:image_vs_video}, although effective, the whole-pose temporal learning scheme still performs worse than the separate joint temporal learning.


\begin{table*}[!htbp]\small
	\caption{Comparisons with the temporal modeling of MAED. We report the MAED result we reproduced.}
	\label{tab:comparison_maed}
	\centering


\resizebox{0.9\textwidth}{!}{

	\begin{tabular}{ccccc}
		
		\toprule
\cmidrule(r){1-3} \cmidrule(r){4-5}
		Image model& Backbone & Temporal Modeling     & PA-MPJPE   & Accel  \\
		\midrule
		INT & ViT-base  &  separate joint temporal learning    & 51.6 & 25.3\\
		INT  & ResNet50+Transformer   &  separate joint temporal learnin      & \textbf{42.0} & \textbf{16.5}   \\
		INT  & ResNet50+Transformer & parallel spatial-temporal      &  43.0& 19.4\\
		MAED & ResNet50+Transformer  &  parallel spatial-temporal &45.0&18.6\\
		\bottomrule
	\end{tabular}}


\end{table*}

{\bf Comparison with MAED.} Since we use the same backbone and training data as MAED~\citep{maed:wan2021encoder}, it is relatively fair to compare with the temporal modeling of MAED. We combine the base model with the parallel spatial-temporal mechanism (abbr. parallel). The results in Tab.~\ref{tab:comparison_maed} show that, for PA-MPJPE metric, the parallel scheme performs worse 1 mm than the separate joint temporal learning; but the parallel scheme based INT model still gains 2 mm improvements over MAED.







\subsection{Observations}


Since we explicitly give certain concepts to these learnable tokens, it is desirable to know what information these defined tokens have learned. 


\begin{figure}
	\subfigure[Prior pose and shape] 
	{
		\begin{minipage}{0.25\textwidth}
			\centering         
				\label{fig:prior_pose}
			\includegraphics[scale=0.27]{figures/prior_pose.pdf}   
		\end{minipage}
	}
	\subfigure[Attention Area for different tokens] 
	{
		\begin{minipage}{0.75\textwidth}
			\centering 
				\label{fig:attention_area}
			\includegraphics[scale=0.27]{figures/prior.pdf}   
		\end{minipage}
	}
	\caption{Prior knowledge learned in the prior independent tokens. \textbf{(a)}: the pose and shape parameters are transformed from the initial joint rotation tokens and shape token by the rotation head and shape head. \textbf{(b)}: the corresponding attention areas in an image for some selected initial tokens} \vspace{-0.2in}
\end{figure}



{\bf What prior pose and shape are learned?} 
In Fig.~\ref{fig:prior_pose}, we show the prior pose and shape learned in the prior joint rotations and shape token, by transforming them to SMPL parameters by the linear rotation head and shape head. 
We can see that the prior pose and shape present a reasonable appearance, not a totally random state.
Note that we does not leverage any explicit SMPL constraints or image information on these prior joint rotation tokens and shape token. 
The only learning source is the backward gradient signals in the training process. This result also indicates that the model learns a \textit{fixed linear transformation} relationship between these joint token vectors and the realistic 3D rotations. 
In Fig.~\ref{fig:attention_area}, we show the attention areas in an image of for some selected tokens. 


\begin{figure}[h]
	\centering
\includegraphics[width=0.9\linewidth]{figures/temporal_attention_v3.pdf}\vspace{-0.1in}
	\caption{Visualized attention matrices (bottom) between different frames (top) for a video clip (128). Colored lines indicate strongly correlated joint rotation tokens between different time points, e.g., for the left hip joint, the joint tokens in the 12-th and 60-th frames have high attention score. Different joints show different rotational temporal patterns.}
	\label{fig:temporal_attention_h36m}\vspace{-0.1in}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\linewidth]{figures/temporal_attention_v2.pdf}
	\caption{Visualized attention matrices (bottom) between different frames (top) for a clip (32). Colored lines indicate strongly correlated joint rotation tokens between different time points. }
	\label{fig:temporal_attention_posetrack}\vspace{-0.1in}
\end{figure}

{\bf Rotational temporal patterns for different joints.} For joint rotation tokens, there is no information exchange between different frames before being sent into the temporal transformer.
Through the self-attention interactions in the temporal Transformer, the model builds correlation and integrates the information among different frames. For example, in Fig.~\ref{fig:temporal_attention_h36m}, the person is walking. For  and  frames, his right knee joint is in the same state of rotation (the right calf is slightly lifted), so the estimates for these two frame should be close. 
By visualizing the attention matrix in the last layer of temporal transformer, we can see there is indeed a strong correlation between the two frames for the right knee joint token.
Similarly, we can observe in Fig.~\ref{fig:temporal_attention_h36m} and Fig.~\ref{fig:temporal_attention_posetrack} that the model captures the periodic joint rotational motions.  In this way, the joint angle estimation for the current frame can be corrected and regularized by using the information from past or future frames.

\section{Conclusion}
\label{discussion}
In this paper, we propose a simple yet effective model based on the design of independent tokens to address the problem of 3D human pose and shape estimation. We introduce joint rotation tokens, shape token and camera token to encode the 3D rotations of human joints, body shape and camera parameters for SMPL-based human mesh reconstruction. 
Thanks to the design of independent tokens, we use a temporal Transformer to capture the temporal motion of each joint separately, which is beneficial for maintaining the temporal rotational coherence of each joint and reducing jitters in local joints.
Our model outperforms state-of-the-art counterparts on the challenging 3DPW benchmark and attains comparable results on the Human3.6m.
The qualitative results show that our model can produce well-fitted and robust human mesh reconstructions for video data.
We also give analysis on what prior information learned in the independent tokens and what temporal patterns are captured by the temporal model.
Since we abstract 3D human joints and shape from image pixels as independent concepts/representations, it is possible for future works to incorporate other modal supervision such as text to leverage reasonable constraints on these independent representations for multi-modal learning.





\section{Acknowledgement}

This work was supported by the National Natural Science Foundation of China under Nos. 62276061, 61773117 and 62006041.


























%
