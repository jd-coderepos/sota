\newpage

\appendix

\section{Proofs}

\subsection{Proofs regarding Weisfeiler-Lehman Test \label{appendix/wlproof}}
We first recount the theorem of~\citet{GIN} to define a GNN that is injective over a multiset (Theorem 1). We then prove that the proposed \emph{Graph Multiset Pooling} (GMPool) can map two different graphs to distinct spaces (Lemma 2). Finally, we show that our overall architecture \emph{Graph Multiset Transformer} (GMT) with a sequence of proposed \emph{Graph Multiset Pooling} (GMPool) and \emph{Self-Attention} (SelfAtt) can represent the injective function over the input multiset (Proposition 3).

\textbf{Theorem 1 (Non-isomorphic Graphs to Different Embeddings).} \emph{Let  be a GNN, and Weisfeiler-Lehman test decides two graphs  and  as non-isomorphic. Then,  maps two different graphs  and  to distinct vectors if node aggregation and update functions are injective, and graph-level readout, which operates on a multiset of node features , is injective.}

\begin{proof}
To map two non-isomorphic graphs to distinct embedding spaces with GNNs, we recount the theorem on Graph Isomorphism Network. See Appendix B of~\cite{GIN} for details.
\end{proof}

\textbf{Lemma 2 (Uniqueness on Graph Multiset Pooling).} \emph{Assume the input feature space  is a countable set. Then the output of the  with  for a seed vector  can be unique for each multiset  of bounded size. Further, the output of the full  constructs a multiset with k elements, which are also unique on the input multiset .}

\begin{proof}
We first state that the GNNs of the Graph Multi-head Attention (GMH) in a GMPool can represent the injective function over the multiset  with an adjacency information , by selecting proper message-passing functions that satisfy the WL test~\citep{GIN, WL/GNN}, denoted as follows: , where . Then, given enough elements, a  can express the sum pooling over the multiset  defined as follows: , where  and  are mapping functions (see the proof of PMA in~\citet{SetTransformer}). 


Since  is a countable set, there is a mapping from the elements to prime numbers denoted by . If we let , then  which constitutes an unique mapping for every multiset  (see~\citet{multiset/injective}). In other words,  is injective. Also, we can easily construct a function , such that  is the injective function for every multiset , where  is derived from the GNN component in the GMPool; .

Furthermore, since a  considers multiset elements without any order, it satisfies the permutation invariance condition for the multiset function.

Finally, each  block has  components such that the output of it consists of  elements as follows: , which allows multiple instances for its elements. Then, since each  is unique on the input multiset , the output of the  that consists of  outputs is also unique on the input multiset .
\end{proof}

Thanks to the universal approximation theorem~\citep{univ/approx}, we can construct such functions  and  using multi-layer perceptrons (MLPs).


\textbf{Proposition 3 (Injectiveness on Pooling Function).} 
\emph{The overall Graph Multiset Transformer with multiple GMPool and SelfAtt can map two different graphs  and  to distinct embedding spaces, such that the resulting GNN with proposed pooling functions can be as powerful as the WL test.}

\begin{proof}
By Lemma 2, we know that a \emph{Graph Multiset Pooling} (GMPool) can represent the injective function over the input multiset . If we can also show that a \emph{Self-Attention} (SelfAtt) can represent the injective function over the multiset, then the sequence of the GMPool and SelfAtt blocks can satisfy the injectiveness.

Let  be a zero matrix in SelfAtt function.  then can be approximated to the any instance-wise feed-forward network denoted as follows: . Therefore, this  is a suitable transformation  that can be easily constructed over the multiset elements , to satisfy the injectiveness.
\end{proof}

To maximize the discriminative power of the Graph Multiset Transformer (GMT) by satisfying the WL test, we assume that SelfAtt does not consider the interactions among multiset elements, namely nodes. While proper GNNs with the proposed pooling function can be at most as powerful as the WL test with this assumption, our experimental results with the ablation study show that the interaction among nodes is significantly important to distinguish the broad classes of graphs (See Table~\ref{ablation}).


\subsection{Proofs regrading Node Clustering \label{appendix/clusterproof}}
We first prove that the space complexity of the \emph{Graph Multiset Pooling} (GMPool) without GNNs can be approximated to the  with  nodes (Theorem 4). After that, we show that the GMPool can be extended to the node clustering approaches with learnable cluster centroids (Proposition 5).


\textbf{Theorem 4 (Space Complexity of Graph Multiset Pooling).} \emph{Graph Multiset Pooling condsense a graph with  nodes to  nodes in  space complexity, which can be further optimized to .}

\begin{proof}
Assume that we have key  and value  matrices in the  function of Graph Multi-head Attention (GMH) for the simplicity, which is described in the equation~\ref{GMH}. Also,  is defined as a seed vector  in the GMPool function of the equation~\ref{GMPool}. To obtain the weights on the values , we multiply the query  with key : . This matrix multiplication then maps a set of  nodes into a set of  nodes, such that it requires  space complexity. Also, we can further drop the constant term : , by properly setting the small  values; .

The multiplication of the attention weights  with value  also takes the same complexity, such that the overall space complexity of GMPool is , which can be further optimized to .
\end{proof}

The space complexity of GNNs with sparse implementation requires  space complexity, where  is the number of nodes, and  is the number of edges in a graph. Therefore, multiple GNNs followed by our GMPool require the total space complexity of  due to the space complexity of the GNN operations. However, GNNs with our GMPool are more efficient than node clustering methods, since node clustering approaches need  space complexity.

\textbf{Proposition 5 (Approximation to Node Clustering).} \emph{Graph Multiset Pooling  can perform hierarchical node clustering with learnable  cluster centroids by Seed Vector  in equation~\ref{GMPool}.}

\begin{proof}
Node clustering approaches are widely used to coarsen a given large graph in a hierarchical manner with several message-passing functions. The core part of the node clustering schemes is to generate a cluster assignment matrix , to coarsen nodes and adjacency matrix as in an equation~\ref{node/clustering}. Therefore, if our Graph Multiset Pooling (GMPool) can generate a cluster assignment matrix , then the proposed GMPool can be directly approximated to the node clustering approaches.

In the proposed GMPool, query  is generated from a learnable set of  seed vectors , and key  and value  are generated from node features  with GNNs in the Graph Multi-head Attention (GMH) block, as in an equation~\ref{GMH}. In this function, if we decompose the attention function  into the dot products of the query with all keys, and the corresponding weighted sum of values, then the first dot product term inherently generates a soft assignment matrix as follows: . Therefore, the proposed GMPool can be easily extended to the node clustering schemes, with the inherently generated cluster assignment matrix; , where one of the proper choices for the activation function  is the softmax function as follows: 


Furthermore, through the learnable seed vectors  for the query , we can learn data dependent  different cluster centroids in an end-to-end fashion.
\end{proof}

Note that, as shown in the section~\ref{experiment/recon} of the main paper, the proposed GMPool significantly outperforms the previous node clustering approaches~\citep{DiffPool, MincutPool}. This is because, contrary to them, the proposed GMPool can explicitly learn data dependent  cluster centroids by learnable seed vectors .

\section{Details for Graph Multiset Transformer Components \label{appendix/model/detail}}
In this section, we describe the \emph{Graph Multiset Pooling} (GMPool) and \emph{Self-Attention} (SelfAtt), which are the components of the proposed \emph{Graph Multiset Transformer}, in detail.

\paragraph{Graph Multiset Pooling} The core components of the Graph Multiset Pooling (GMPool) is the Graph Multi-head Attention (GMH) that considers the graph structure into account, by constructing the key  and value  using GNNs, as described in the equation~\ref{GMH}. As shown in the Table~\ref{ablation} of the main paper, this graph multi-head attention significantly outperforms the naive multi-head attention (MH in equation~\ref{MH}). However, after compressing the  nodes into the  nodes with , we can not directly perform further GNNs since the original adjacency information is useless after pooling. To tackle this limitation, we can generate the new adjacency matrix  for the compressed nodes, by performing node clustering as described in Proposition 5 of the main paper as follows:

where  is the generated cluster assignment matrix, and  is the coarsened adjacency matrix as described in the equation~\ref{node/clustering}. However, this approach is well known for their scalability issues~\citep{SAGPool, GNN/sparse/pooling:Readout}, since they require quadratic space  to store and even multiply the adjacency matrix  with the soft assignment matrix . Therefore, we leave doing this as a future work, and use the following trick. By replacing the adjacency matrix  with the identity matrix  in the GMPool except for the first block, we can easily perform multiple GMPools without any constraints, which is approximated to the GMPool with MH in the equation~\ref{MH}, rather than GMH in the equation~\ref{GMH}, as follows:


\paragraph{Self-Attention}
The Self-Attention (SelfAtt) function can consider the inter-relationships between nodes in a set, which helps the network to take the global graph structure into account. Because of this advantage, the self-attention function significantly improves the proposed model performance on the graph classification tasks, as shown in the Table~\ref{ablation} of the main paper. From a different perspective, we can regard the Self-Attention function as a graph neural network (GNN) with a complete graph. Specifically, given  nodes from the previous layer, the Multi-head Attention (MH) of the Self-Attention function first constructs the adjacency matrix among all nodes with their similarities, through the matrix multiplication of the query with key: , and then computes the outputs with the sum of the obtained weights on the value. In other words, the self-attention function can be considered as one message passing function with a soft adjacency matrix, which might be further connected to the Graph Attention Network~\citep{GAT}.

\section{Experimental Setup}
In this section, we first introduce the baselines and our model, and then describe the experimental details about graph classification, reconstruction, and generation tasks respectively.


\subsection{Baselines and Our Model \label{appendix/classification/model}}
\textbf{1) GCN.} This method~\citep{GCN} is the mean pooling baseline with Graph Convolutional Network (GCN) as a message passing layer.

\textbf{2) GIN.} This method~\citep{GIN} is the sum pooling baseline with Graph Isomorphism Network (GIN) as a message passing layer.

\textbf{3) Set2Set.} This method~\citep{Set2Set} is the set pooling baseline that uses a recurrent neural network to encode a set of all nodes, with content-based attention over them.

\textbf{4) SortPool.} This method~\citep{SortPool} is the node drop baseline that drops unimportant nodes by sorting their representations, which are directly generated from the previous GNN layers.

\textbf{5) SAGPool.} This method~\citep{SAGPool} is the node drop baseline that selects the important nodes, by dropping unimportant nodes with lower scores that are generated by the another graph convolutional layer, instead of using scores from the previously passed layers. Particularly, this method has two variants. \textbf{6.1) SAGPool(G)} is the global node drop method that drops unimportant nodes one time at the end of their architecture. \textbf{6.2) SAGPool(H)} is the hierarchical node drop method that drops unimportant nodes sequentially with multiple graph convolutional layers.  

\textbf{6) TopkPool.} This method~\citep{TopKPool} is the node drop baseline that selects the top-ranked nodes using a learnable scoring function.

\textbf{7) ASAP.} This method~\citep{ASAP} is the node drop baseline that first locally generates the clusters with neighboring nodes, and then drops the lower score clusters using a scoring function.

\textbf{8) DiffPool.} This method~\citep{DiffPool} is the node clustering baseline that produces the hierarchical representation of the graphs in an end-to-end fashion, by clustering similar nodes into the few nodes through graph convolutional layers. 

\textbf{9) MinCutPool.} This method~\citep{MincutPool} is the node clustering baseline that applies the spectral clustering with GNNs, to coarsen the nodes and the adjacency matrix of a graph.

\textbf{10) HaarPool.} This method~\citep{HaarPool} is the spectral-based pooling baseline that compresses the node features with a nonlinear transformation in a Haar wavelet domain. Since it directly uses the spectral clustering to generate a coarsened matrix, the time complexity cost is relatively higher than other pooling methods.

\textbf{11) StructPool.} This method~\citep{StructPool} is the node clustering baseline that integrates the concept of the conditional random field into the graph pooling. While this method can be used with a hierarchical scheme, we use it with a global scheme following their original implementation, which is similar to the SortPool~\citep{SortPool}.

\textbf{12) EdgePool.} This method~\citep{edgepool} is the edge clustering baseline that gradually merges the nodes, by contracting the high score edge between two adjacent nodes. 

\textbf{13) GMT.} Our Graph Multiset Transformer that first condenses all nodes into the important nodes by GMPool, and then considers interactions between nodes in a set. Since it operates on the global READOUT layer, it can be coupled with hierarchical pooling methods by replacing their last layer.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{images/appendix_layers_extended.pdf}
    \vskip -0.15in
    \caption{\small \textbf{Illustration of High-level Model Architectures. (Top):} Global Graph Classification; GCN, GIN, Set2Set, SortPool, SAGPool(G), StructPool, GMT. \textbf{(Middle:)} Hierarchical Graph Classification; DiffPool, SAGPool(H), TopKPool, MinCutPool, ASAP, EdgePool, HaarPool. \textbf{(Bottom:)} Graph Reconstruction; DiffPool, TopKPool, MinCutPool, GMT. MP denotes the message passing layer.}
    \vskip -0.15in
    \label{fig:appendix/layers}
\end{figure*}

\subsection{Graph Classification}

\paragraph{Dataset \label{appendix/classification/data}}
Among TU datasets~\citep{classification/datasets}, we select the 6 datasets including 3 datasets (D\&D, PROTEINS, and MUTAG) on Biochemical domain, and 3 datasets (IMDB-B, IMDB-M, and COLLAB) on Social domain. We use the classification accuracy as an evaluation metric. As suggested by~\cite{fair/GNN} for a fair comparison, we use the one-hot encoding of their atom types as initial node features in the bio-chemical datasets, and the one-hot encoding of node degrees as initial node features in the social datasets. Moreover, we use the recently suggested 4 molecule graphs (HIV, Tox21, ToxCast, BBBP) from the OGB datasets~\citep{OGB}. We use the ROC-AUC for an evaluation metric, and use the additional atom and bond features, as suggested by~\citet{OGB}. Dataset statistics are reported in the Table~\ref{table:results:chemical} of the main paper.

\paragraph{Implementation Details on Classification Experiments \label{appendix/classification/implementation}}
For all experiments on TU datasets, we evaluate the model performance with a 10-fold cross validation setting, where the dataset split is based on the conventionally used training/test splits~\citep{TUdataset/split, SortPool, GIN}, with LIBSVM~\citep{LIBSVM}. In addition, we use the 10 percent of the training data as a validation data following the fair comparison setup~\citep{fair/GNN}. For all experiments on OGB datasets, we evaluate the model performance following the original training/validation/test dataset splits~\citep{OGB}. We use the early stopping criterion, where we stop the training if there is no further improvement on the validation loss during 50 epochs, for the TU datasets. Further, the maximum number of epochs is set to 500. We then report the average performances on the validation and test sets, by performing overall experiments 10 times with different seeds.

For all experiments on TU datasets except the D\&D, the learning rate is set to , hidden size is set to , batch size is set to , weight decay is set to , and dropout rate is set to . Since the D\&D dataset has a large number of nodes (See Table~\ref{table:results:chemical} in the main paper), node clustering methods can not perform clustering operations on large graphs with large batch sizes, such that the hidden size is set to , and batch size is set to  on the D\&D dataset. For all experiments on OGB datasets except the HIV, the learning rate is set to , hidden size is set to , batch size is set to , weight decay is set to , and dropout rate is set to . Since the HIV dataset contains a large number of graphs compared to others (See Table~\ref{table:results:chemical} in the main paper), the batch size is set to  for fast training. Then we optimize the network with Adam optimizer~\citep{kingma2014adam}. For a fair comparison of baselines~\citep{SAGPool}, we use the three GCN layers~\citep{GCN} as a message passing function for all models with jumping knowledge strategies~\citep{JumpingKnowledge}, and only change the pooling architecture throughout all models, as illustrated in Figure~\ref{fig:appendix/layers}. Also, we set the pooling ratio as 25\% in each pooling layer for both baselines and our models.

\paragraph{Implementation Details on Efficiency Experiments \label{appendix/classification/efficiency}}
To compare the GPU \textbf{memory efficiency} of GMT against baseline models including node drop and node clustering methods, we first generate the Erdos-Renyi graphs~\citep{randomgraph} by varying the number of nodes , where the edge size  is twice the number of nodes: . For all models, we compress the given  nodes into the  nodes at the first pooling function.

To compare the \textbf{time efficiency} of GMT against baseline models, we first generate the Erdos-Renyi graphs~\citep{randomgraph} by varying the number of nodes  with  edges, following the setting of HaarPool~\citep{HaarPool}. For all models, we set the pooling ratio as  except for HaarPool, since it compresses the nodes according to the coarse-grained chain of a graph. We measure the forward time, including CPU and GPU, for all models with 50 graphs over one batch.

\subsection{Graph Reconstruction \label{appendix/reconstruction/experimentaldetail}}

\paragraph{Dataset}
We first experiment with synthetic graphs represented in a 2-D Euclidean space, such as ring and grid structures. The node features of a graph consist of their location in a 2-D coordinate space, and the adjacency matrix indicates the connectivity pattern of nodes. The goal here is to restore all node locations from compressed features after pooling, with the intact adjacency matrix.

While synthetic graphs are appropriate choices for the qualitative analysis, we further do the quantitative evaluation of models with real-world molecular graphs. Specifically, we use the subset~\citep{benchmarkingGNN} of the ZINC dataset~\citep{ZINC}, which consists of 12K real-world molecular graphs, to further conduct a graph reconstruction on the large number of various graphs. The goal of the molecule reconstruction task is to restore the exact atom types of all nodes in the given graph, from the compressed representations after pooling.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{images/appendix_graph_reconstruction.pdf}
    \vskip -0.2in
    \caption{High resolution images for synthetic graph reconstruction results in Figure~\ref{recon:synthetic}.}
    \label{fig:appendix/synthetic}
    \vskip -0.15in
\end{figure*}

\paragraph{Common Implementation Details}
Following~\citet{MincutPool}, we use the two message passing layers both right before the pooling operation and right after the unpooling operation. Also, both pooling and unpooling operations are performed once and sequentially connected, as illustrated in the Figure~\ref{fig:appendix/layers}. We compare our methods against both the node drop (TopKPool~\citep{TopKPool}) and node clustering (DiffPool~\citep{DiffPool} and MinCutPool~\citep{MincutPool}) methods. For the node drop method, we use the unpooling operation proposed in the graph U-net~\citep{TopKPool}. For the node clustering methods, we use the graph coarsening schemes described in the equation~\ref{node/clustering}, with their specific implementations on generating an assignment matrix. For our proposed method, we only use the one Graph Multiset Pooling (GMPool) without SelfAtt, where we follow the node clustering approaches as described in the subsection~\ref{paragraph/node/clustering} by generating a single soft assignment matrix with one head  in the multi-head attention function. For experiments of both synthetic and molecule reconstructions, the learning rate is set to , and hidden size is set to . We then optimize the network with Adam optimizer~\citep{kingma2014adam}.


\paragraph{Implementation Details on Synthetic Graph}
We set the pooling ratio of all models as 25\%. For the loss function, we use the Mean Squared Error (MSE) to train models. We use the early stopping criterion, where we stop the training if there is no further improvement on the training loss during 1,000 epochs. Further, the maximum number of epochs is set to 10,000. Note that, there is no other available graphs for validation of the synthetic graph, such that we train and test the models only with the given graph in the Figure~\ref{fig:appendix/synthetic}. The baseline results are adopted from~\citet{MincutPool}.


\paragraph{Implementation Details on Molecule Graph} 
We set the pooling ratio of all models as 5\%, 10\%, 15\%, and 25\%, and plot all results in the Figure~\ref{recon:ZINC} of the main paper. Note that, in the case of molecule graph reconstruction, a softmax layer is appended at the last layer of the model architecture to classify the original atom types of all nodes. For the loss function, we use the cross entropy loss to train models. We use the early stopping criterion, where we stop the training if there is no further improvement on the validation loss during 50 epochs. Further, the maximum number of epochs is set to 500, and batch size is set to 128. Note that, in the case of molecule graph reconstruction on the ZINC dataset, we strictly separate the training, validation and test sets, as suggested by~\citet{benchmarkingGNN}. We perform all experiments 5 times with 5 different random seeds, and then report the averaged result with the standard deviation. Note that, in addition to baselines mentioned in the common implementation details paragraph, we compare two more baselines: GCN with a random assignment matrix for pooling, which is adopted from~\citet{rethinking/pooling}, and StructPool~\citep{StructPool}, for the real-world molecule graph reconstruction.


\paragraph{Evaluation Metrics for Molecule Reconstruction}
For quantitative evaluations, we use the three metrics as follows: \textit{1) validity} indicates the number of reconstructed molecules that are chemically valid, \textit{2) exact match} indicates the number of reconstructed molecules that are exactly same as the original molecules, and \textit{3) accuracy} indicates the classification accuracy of atom types of all nodes.

\subsection{Graph Generation \label{appendix/generation/experimentaldetail}}

\paragraph{Common Implementation Details}
In the graph generation experiments, we replace the graph embedding function  from existing graph generation models to the proposed Graph Multiset Transformer (GMT), to evaluate the applicability of our model on generation tasks, as described in the subsection~\ref{experiment/generation} of the main paper. As baselines, we first use the original models with their implementations. Specifically, we use the MolGAN\footnote{https://github.com/yongqyu/MolGAN-pytorch}~\citep{MolGAN} for molecule generation, and Graph Logic Network (GLN)\footnote{https://github.com/Hanjun-Dai/GLN}~\citep{GLN} for retrosynthesis. For both experiments, we directly follow the experimental details of original papers~\citep{MolGAN, GLN} for a fair comparison. Furthermore, to compare our models with another strong pooling method, we use the MinCutPool~\citep{MincutPool} as an additional baseline for generation tasks, since it shows the best performance among baselines in the previous two classification and reconstruction tasks.


For MinCutPool, since it cannot directly compress the all  nodes into the  cluster to represent the entire graph, we use the following trick to replace the simple pooling operation (e.g. sum or mean) with it. We first condense the graph into the k clusters () using one MinCutPool layer, and then average the condensed nodes to get a single representation of the given graph. However, our proposed Graph Multiset Transformer (GMT) can directly compress the all  nodes into the  node with one learnable seed vector, by using the single  block. In other words, we use the one  to represent the entire graph by replacing their simple pooling (e.g. sum or mean), in which we use the following softmax activation function for computing attention weights:



\paragraph{Implementation Details on Molecule Generation}
For the molecule generation experiment with the MolGAN, we replace the average pooling in the discriminator with . We use the QM9 dataset~\citep{qm9} following the original MolGAN paper~\citep{MolGAN}. To evaluate the models, we report the validity of 13,319 generated molecules at the early stage of the MolGAN training, over 4 different runs. As depicted in Figure~\ref{gen:molgan} of the main paper, each solid curve indicates the average validity of each model with  different runs, and the shaded area indicates the half of the standard deviation for 4 different runs.

\paragraph{Implementation Details on Retrosynthesis}
For the retrosynthesis experiment with the Graph Logic Network (GLN), we replace the average pooling in the template and subgraph encoding functions with . We use the USPTO-50k dataset following the original paper~\citep{GLN}. For an evaluation metric, we use the Top-\textit{k} accuracy for both reaction class is not given and given cases, following the original paper~\citep{GLN}. We reproduce all results in Table~\ref{gen:gln} with published codes from the original paper. 


\section{Additional Experimental Results}


\begin{table*}[t]
\caption{\small Graph classification results on validation sets with standard deviations. All results are averaged over 10 different runs. Best performance and its comparable results () from the t-test are marked in blod. Hyphen (-) denotes out-of-resources that take more than 10 days (See Figure~\ref{time} for the time efficiency analysis).}
\centering
\resizebox{\textwidth}{!}{
\renewcommand{\tabcolsep}{0.9mm}
\begin{tabular}{lccccccccccc}
\toprule
                & \multicolumn{7}{c}{\textbf{Biochemical Domain}} & \multicolumn{3}{c}{\textbf{Social Domain}} \\
                \cmidrule(l{2pt}r{2pt}){2-8} \cmidrule(l{2pt}r{2pt}){9-11}
    & \textbf{D\&D} & \textbf{PROTEINS} & \textbf{MUTAG} & \textbf{HIV} & \textbf{Tox21} & \textbf{ToxCast} & \textbf{BBBP} & \textbf{IMDB-B} & \textbf{IMDB-M} & \textbf{COLLAB} \\ 
\midrule
\# graphs       & 1,178     & 1,113     & 188       & 41,127    & 7,831     & 8,576     & 2,039     & 1,000     & 1,500     & 5,000     \\
\# classes      & 2         & 2         & 2         & 2         & 12        & 617       & 2         & 2         & 3         & 3         \\ 
Avg \# nodes    & 284.32    & 39.06     & 17.93     & 25.51     & 18.57     & 18.78     & 24.06     & 19.77     & 13.00     & 74.49     \\
\midrule
GCN     & 76.17  0.65 & 77.13  0.44 & 76.56  1.75 & 81.27  0.92 & 78.80  0.40 & 65.66  0.40 & 93.35  1.08 & 77.93  0.28 & 54.29  0.23 & 83.08  0.13 \\
GIN     & 76.85  0.61 & 78.43  0.45 & \textbf{94.44}  0.52 & 82.10  1.01 & 78.20  0.45 & 66.29  0.42 & 94.64  0.36 & \textbf{78.38}  0.26 & 54.04  0.29 & 82.19  0.25 \\
\midrule
Set2Set    & 76.32  0.40 & 77.64  0.41 & 79.72  2.40 & 80.07  0.93 & 79.13  0.75 & 66.39  0.49 & 91.89  1.48 & 78.13  0.30 & 54.39  0.19 & 82.34  0.23 \\
SortPool   & 80.68  0.59 & 77.92  0.42 & 81.33  3.00 & 81.17  2.30 & 75.97  0.76 & 64.26  1.17 & 94.21  1.04 & 77.46  0.60 & 52.95  0.62 & 80.58  0.25 \\
DiffPool   & 81.33  0.33 & 79.09  0.36 & 87.94  1.93 & \textbf{83.16}  0.44 & 80.02  0.38 & \textbf{69.73}  0.79 & \textbf{96.32}  0.36 & 77.86  0.39 & 54.77  0.19 & 81.69  0.31 \\
SAGPool(G) & 76.73  0.80 & 77.01  0.58 & 88.11  1.21 & 80.55  1.89 & 77.03  0.76 & 65.51  0.91 & \textbf{95.59}  1.22 & \textbf{78.09}  0.58 & 53.73  0.42 & 81.91  0.45 \\
SAGPool(H) & 79.56  0.67 & 77.24  0.56 & 86.06  2.07 & 79.21  1.50 & 75.36  2.63 & 64.05  0.83 & 93.05  3.00 & 77.11  0.46 & 53.49  0.65 & 80.55  0.56 \\
TopKPool   & 78.54  0.73 & 75.47  0.90 & 75.06  2.12 & 79.24  1.84 & 75.06  2.30 & 64.56  0.56 & 93.31  2.32 & 76.12  0.79 & 52.75  0.58 & 79.94  0.86 \\
MinCutPool & 81.96  0.39 & 79.23  0.66 & 87.22  1.72 & \textbf{83.12}  1.27 & \textbf{81.10}  0.42 & \textbf{69.09}  1.12 & \textbf{95.99}  0.47 & 77.76  0.36 & \textbf{54.94}  0.19 & \textbf{83.37}  0.18 \\
StructPool & \textbf{82.56}  0.37 & \textbf{80.00}  0.27 & 91.5  0.95 & 81.09  1.26 & 79.61  0.70 & 66.49  1.59 & 95.18  0.59 & 77.14  0.31 & 54.13   0.39 & 79.90  0.18 \\
ASAP       & 81.58  0.38 & 78.71  0.45 & 91.33 0.65 &  79.80  1.88  & 77.33  1.34 & 63.82  0.75 & 92.96  1.09 & 77.89  0.51 & \textbf{55.17}   0.33 & 82.11  0.33 \\
EdgePool   & 80.32  0.44 & 79.61  0.25 & 87.28 1.18 & 81.84  1.32 & 78.92  0.29 & 66.21  0.64 & 94.98  0.62 & 77.50  0.25 & 54.69   0.40 &         -        \\
HaarPool   &         -        & - & 68.22 0.86 &         -        &         -        &         -        & 89.98  0.58 & 76.72  0.60 & 53.03   0.14 &         -        \\
\midrule
GMT (Ours) & 82.19  0.40 & \textbf{80.01}  0.21 & 91.00  0.82 & \textbf{83.54}  0.78 & \textbf{80.91}  0.41 & \textbf{69.77}  0.67 & 95.14  0.48 & \textbf{78.43}  0.22 & \textbf{55.14}  0.25 & \textbf{83.37}  0.11 \\
\bottomrule
\end{tabular}
}
\vspace{-0.15in}
\label{app:table:results:chemical}
\end{table*} 
\paragraph{Validation Results on Graph Classification}
We additionally provide the graph classification results on validation sets. As shown in Table~\ref{app:table:results:chemical}, the proposed GMT outperforms most baselines, or achieves comparable performances to the best baseline results even in the validation sets. While validation results can not directly measure the generalization performance of the model for unseen data, these results further confirm that our method is powerful enough, compared to baselines. Regarding the results of test sets on the graph classification task, please see Table~\ref{table:results:chemical} in the main paper.


\begin{wraptable}{t}{0.4\textwidth}
    \vspace{-0.3in}
    \small
    \centering
    \caption{\small Graph classification results for OGB test datasets with standard deviations.}
    \resizebox{0.4\textwidth}{!}{
        \renewcommand{\tabcolsep}{0.75mm}
        \renewcommand{\arraystretch}{0.9}
        \begin{tabular}{llccccc}
        \toprule
        & {\bf Model} & {\bf HIV} & {\bf Tox21} \\
        \midrule
        \multirow{2}{*}{Leaderboard} & GCN & 76.06  0.97 & 75.29  0.69 \\
        & GIN & 75.58  1.40 & 74.91  0.51 \\
        \midrule
        \multirow{2}{*}{Reproduced} & GCN & 76.81  1.01 & 75.04  0.80 \\
        & GIN & 75.95  1.35 & 73.27  0.84 \\
        \midrule
        Ours & GMT & \textbf{77.56}  1.25 & \textbf{77.30}  0.59 \\
        \bottomrule
        \end{tabular}
    }
    \vskip -0.15in
    \label{app:leaderboard}
\end{wraptable}

\paragraph{Leaderboard Results on Graph Classification}
For a fair comparison, we experiment with all baselines and our models in the same setting, as described in the implementation details of Appendix~\ref{appendix/classification/implementation}. Specifically, we average the results over 10 different runs with the same hidden dimension (128, while leaderboard uses 300), and the same number of message-passing layers (3, while leaderboard uses 5) with 10 different seeds for all models. Therefore, the reproduced results can be slightly different from the leaderboard results, as shown in Table~\ref{app:leaderboard}, since the leaderboard uses different hyper-parameters with different random seeds (See~\cite{OGB} for more details). However, our reproduction results are almost the same as the leaderboard results, and sometimes outperform the leaderboard results (See the GCN results for the HIV dataset in Table~\ref{app:leaderboard}). Therefore, while we conduct all experiments under the same setting for a fair comparison, where specific hyperparameter choices are slightly different from the leaderboard setting, these results indicate that there is no significant difference between reproduced and leaderboard results.


\begin{table}[t]
\small
\centering
\caption{\small Quantitative results of the graph reconstruction task on reconstructing the node features and the adjacency matrix for synthetic graphs, with two different minimization objectives and error calculation metrics:  and . * indicates the model without using adjacency normalization.}
\resizebox{\textwidth}{!}{
    \begin{tabular}{lccccccccccccc}
    \toprule
     Data: & \multicolumn{4}{c}{Grid Graph} & \multicolumn{4}{c}{Ring Graph} \\
     \cmidrule(l{2pt}r{2pt}){2-5} \cmidrule(l{2pt}r{2pt}){6-9}
     Objective: & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} \\
     \cmidrule(l{2pt}r{2pt}){2-3} \cmidrule(l{2pt}r{2pt}){4-5} \cmidrule(l{2pt}r{2pt}){6-7} \cmidrule(l{2pt}r{2pt}){8-9}
     Error Calculation: &  &  &  &  &  &  &  & \\
    \midrule
    DiffPool & 0.0833 & 12110194 & 0.3908 & 0.0856 & 0.0032 & 617.7706 & 0.6208 & 0.0948 \\
    MinCutPool & 0.0001 & 0.0092 & 1.2883 & 0.0051 & 0.0005 & 0.0424 & 0.5026 & 0.0128 \\
    MinCutPool* & 0.0002 & 201.7619 & 2.0261 & 0.0616 & 0.0003 & 68.23 & 0.5211 & 0.0725 \\
    GMT (Ours) & 0.0001 & 0.0102 & 0.2353 & 0.0084 & 0.0000 & 0.0331 & 0.5475 & 0.0324 \\
    \bottomrule
    \end{tabular}
}
\label{app:adj:recon}
\end{table}

\paragraph{Quantitative Results on Graph Reconstruction for Synthetic Graphs}
While we conduct experiments on reconstructing node features on the given graph, to quantify the retained information on the condensed nodes after pooling (See Section~\ref{experiment/recon} for experiments on the graph reconstruction task), we further reconstruct the adjacency matrix to see if the pooling layer can also condense the adjacency structure without loss of information. The learning objective to minimize the discrepancy between the original adjacency matrix  and the reconstructed adjacency matrix  with a cluster assignment matrix  is defined as follows: 


Then we design the following two experiments. First, pooling layers are trained to minimize the objective in Section~\ref{experiment/recon}: . After that, we measure the discrepancy between the original and the reconstructed node features: , and also measure the discrepancy between the original and the reconstructed adjacency matrix: . Second, pooling layers are trained to minimize the objective described in the previous paragraph: , and then we measure the aforementioned two discrepancies in the same way.

We experiment with synthetic grid and ring graphs, illustrated in Figure~\ref{fig:appendix/synthetic}. Table~\ref{app:adj:recon} shows that the error is large when the objective and the error metric are different, which indicates that there is a high discrepancy between the required information for condensing node and the required information for condensing adjacency matrix. In other words, the compression for node and the compression for adjacency matrix might be differently performed to reconstruct the whole graph information. 

Also, Table~\ref{app:adj:recon} shows that there are some cases where there is no significant difference in the calculated adjacency error (), when minimizing nodes discrepancies and minimizing adjacency discrepancies (See 0.0331 and 0.0324 for the proposed GMT on the Ring Graph). Furthermore, calculated errors for the adjacency matrix when minimizing adjacency discrepancies are generally larger than the calculated errors for node features when minimizing nodes discrepancies. These results indicate that the adjacency matrix is difficult to reconstruct after pooling. This might be because the reconstructed adjacency matrix should be further transformed from continuous values to discrete values (0 or 1 for the undirected simple graph), while the reconstructed node features can be directly represented as continuous values. We leave further reconstructing adjacency matrices and visualizing them as a future work.



\paragraph{Additional Examples for Molecule Reconstruction}
We visualize the additional examples for molecule reconstruction on the ZINC dataset in Figure~\ref{fig:appendix/zinc}. Molecules on the left side indicate the original molecule, where the transparent color denotes the assigned cluster for each node, which is obtained by the cluster assignment matrix  with node (atom) representations in a graph (molecule) (See Proposition 5 for more detail on generating the cluster assignment matrix). Also, molecules on the right side indicate the reconstructed molecules with failure cases denoted as a red dotted circle. 

As visualized in Figure~\ref{fig:appendix/zinc}, we can see that the same atom or the similarly connected atoms obtain the same cluster (color). For example, the atom type O mostly obtains the yellow cluster, and the atom type F obtains the green cluster. Furthermore, ring-shaped substructures that do not contain O or N mostly receive the blue cluster, whereas ring-shaped substructures that contain O and N receive the green and yellow clusters respectively.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{images/appendix_molecule.pdf}
    \vskip -0.175in
    \caption{\textbf{Molecule Reconstruction Examples (Left):} Original molecules with the assigned cluster on each node represented as color, where cluster is generated from \emph{Graph Multiset Pooling} (GMPool). \textbf{(Right):} Reconstructed molecules. Red dotted circle indicates the incorrect atom prediction.}
    \label{fig:appendix/zinc}
\end{figure*}