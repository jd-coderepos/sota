\documentclass[11pt]{article}
\usepackage{acl2015}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}





\providecommand{\cite}[1]{\citeauthoryear{#1}}
\usepackage{natbib}
\renewcommand{\cite}{\citep}

\usepackage{url}

\usepackage{subfigure}
\usepackage{times,amsmath,epsfig}
\usepackage{graphicx}
\usepackage{url}
\usepackage{enumitem}
\usepackage{amsfonts}



\usepackage{amsmath}
\usepackage[noend]{algpseudocode}
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{mathtools}
\usepackage{makecell}



\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\SAVE}[1]{}


\newtheorem{theorems}{Theorem}
\newtheorem{theorem}{Theorem}[section]



\newtheorem{definition}{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newcommand{\reminder}[1]{{\bf  [[[  #1 ]]]} }


\usepackage{mdwlist}  \usepackage{multirow}  \usepackage{amsmath} \usepackage{relsize} 

\usepackage{float}
\usepackage{flushend}

\usepackage{graphicx}
\makeatletter

\newcommand*\bigcdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
\makeatother

\usepackage{tikz}







\title{
	Enriching Pre-trained Language Model with Entity Information for Relation Classification
}

 

\author{\textbf{Shanchan Wu} \\
	Alibaba Group (U.S.) Inc., Sunnyvale, CA   \\
{\tt shanchan.wu@alibaba-inc.com} \\ \\
	\textbf{Yifan He} \\
	Alibaba Group (U.S.) Inc., Sunnyvale, CA  \\
{\tt y.he@alibaba-inc.com} \\
	\\
}

\date{}

\begin{document}
\maketitle



\begin{abstract}
Relation classification is an important NLP task
to extract relations between entities. The 
state-of-the-art methods for relation classification are primarily
based on Convolutional or Recurrent Neural Networks. 
Recently, the pre-trained BERT model
achieves very successful results in many NLP classification  / sequence labeling tasks. Relation classification differs from
those tasks in that it relies on information of both the
sentence and the two target entities.
In this
paper, we propose a model that both leverages the pre-trained BERT language model and incorporates information from the target entities to tackle the relation classification task. 
We locate the target entities and transfer the information through the pre-trained architecture
and incorporate the corresponding encoding of the two entities.
We achieve  significant improvement over the state-of-the-art method on the SemEval-2010 task 8 relational dataset. 
\end{abstract}

 

\section{Introduction}

The task of relation classification is to predict semantic relations between pairs of nominals. 
Given a sequence of text (usually a sentence) $s$ and a pair of nominals $e_1$ and $e_2$, the objective
is to identify the relation between $e_1$ and $e_2$ \cite{Hendrickx2010_semeval}. 
It is an important NLP task which is normally
used as an intermediate step in variety of NLP
applications. 
The following example shows the
Component-Whole relation between the nominals
``kitchen'' and ``house'':  ``The [kitchen]$_{e1}$ is the last renovated part of the [house]$_{e1}$.''

Recently, deep neural networks have applied to relation classification 
\cite{Socher_EMNLP_2012,Zeng_coling_2014,Yu_NIPS_Worksho_2014,Nogueira_ACL_2015,Huang_COLING_2016,Joohong_Arxiv_2019}.
These methods usually use  some features
derived from lexical resources such as Word-Net or NLP tools such as dependency parsers and
named entity recognizers (NER).


Language model pre-training has been shown to be effective
for improving many natural language processing
tasks \cite{Dai_NIPS_2015,Peters_arxiv_2017,OpenAI_2018_tech,Ruder_ACL_2018,bert_Jacobv_corr_bert_2018}. 
The pretrained model BERT proposed by \cite{bert_Jacobv_corr_bert_2018} 
has especially significant impact. It has been applied to multiple NLP tasks and obtains new state-of-the-art results on eleven tasks. The tasks that BERT has been applied to are typically modeled as classification problems and sequence labeling problems.  
It has also been applied to the
SQuAD question answering \cite{Rajpurkar_squad_2016} problem, in
which the objective is to find the starting point and ending point of an answer span. 

As far as we know, the pretrained BERT model \cite{bert_Jacobv_corr_bert_2018} has not been 
applied to relation classification, which relies 
not only on the information of the whole sentence but also 
on the information of the specific target entities.
In this paper, we apply the pretrained BERT model for relation classification. 
We insert special tokens before and after the target entities 
before feeding the text to BERT for fine-tuning, 
in order to
identify the locations of the two target
entities and transfer the information into the BERT model. 
We then locate the positions of the two target entities in the output embedding
from BERT model. We use their embeddings as well as the sentence encoding (embedding of the special first token in the setting of BERT)
as the input to a multi-layer neural network for classification.
By this way, it captures both the semantics of the sentence and
the two target entities to better fit the relation classification task.

Our contributions are as follows: (1) We put forward an innovative approach
to incorporate entity-level information into the pretrained language model for relation classification. (2) We achieve the new state-of-the-art for the relation classification task.




 \section{Related Work}  
\label{related}


There has been some work with deep learning methods for relation
classification, such as 
\cite{Socher_EMNLP_2012,Zeng_coling_2014,Yu_NIPS_Worksho_2014,Nogueira_ACL_2015}

MVRNN model \cite{Socher_EMNLP_2012} applies a recursive neural network (RNN)
to relation classification. They assign a matrix-vector representation to every node
in a parse tree and compute the representation for the complete sentence from bottom up
according to the syntactic structure of the parse tree. 
\cite{Zeng_coling_2014} propose a CNN model by  
incorporating both word embeddings and position features as input.
Then they concatenate lexical features and the output from CNN  
into a single vector and feed them into a softmax layer for prediction.
\cite{Yu_NIPS_Worksho_2014} propose a Factor-based 
Compositional Embedding Model (FCM) by constructing sentence-level and 
substructure embeddings from word embeddings, through dependency trees and named entities.
\cite{Santos_ACL_2015} tackle the 
relation classification task 
by ranking with a convolutional
neural network named CR-CNN. 
Their loss function is based on pairwise ranking. In our work, we take advantage of a pre-trained language model for
the relation classification task, without relying on CNN or RNN
architecutures.
\cite{Huang_COLING_2016} utilize a CNN encoder in conjunction with a sentence representation that weights the words by attention between the target entities and the words in the sentence to perform relation classification.
\cite{Wang-ACL2016_relation} propose a convolutional
neural network architecture with two levels of attention in order
to catch the patterns in heterogeneous
contexts to classify relations.
\cite{Joohong_Arxiv_2019} develop an end-to-end
recurrent neural model which incorporates an
entity-aware attention mechanism with a latent entity
typing for relation classification.  

There are some related work on the relation extraction based on distant supervision,
for example, \cite{Mintz_ACL_2009,Hoffmann_ACL_2011,Lin_ACL_2016,Ji_AAAI_2017,Wu_AAAI_2019}. The difference
between relation classification on regular data and on distantly supervised data is
that the latter may contain a large number of noisy labels. In this paper, we focus on the
regular relation classification problem, without noisy labels. 
  

\begin{figure*}[!t]
	\centering
	\includegraphics[width=0.74\linewidth]{figures/model.pdf}
	\caption{The model architecture. 
	} 
	\label{fig:model_arch}
\end{figure*}




\section{Methodology}  
\label{sec:methods}

\subsection{Pre-trained Model BERT}
The pre-trained BERT model \cite{bert_Jacobv_corr_bert_2018} is a multi-layer bidirectional
Transformer encoder
\cite{Vaswani_NIPS_2017}.

The design of input representation of BERT is to be able to
represent both a single text sentence and a pair of
text sentences in one
token sequence.  The input representation of each token
is constructed by the summation of the corresponding
token, segment and position embeddings.


`[CLS]' is appended to the beginning of each sequence
as the first token of the sequence. The final hidden
state from the Transformer output corresponding to the first token is used as the sentence representation
for classification tasks. In case there are two sentences
in a task, `[SEP]' is used to separate the two sentences. 

BERT pre-trains the model parameters by using a
pre-training objective: the “masked language model” (MLM), which randomly
masks some of the tokens from the input, and set the optimization objective to predict the original vocabulary id of the masked word according to its context. 
Unlike
left-to-right language model pre-training, the
MLM objective can help a state output to utilize both
the left and the right context, which allows a pre-training
system to apply a deep bidirectional Transformer. Besides the masked language model, BERT also
trains a ``next sentence prediction'' task that
jointly pre-trains text-pair representations.





\subsection{Model Architecture}

Figure \ref{fig:model_arch} shows the architecture of our approach.

For a sentence $s$ with two target entities $e_1$ and $e_2$,
to make the BERT module capture the location information of the two entities,
at both the beginning and end of the first entity, we insert
a special token `\$', and at both the beginning and end of the second entity, we insert a special token `\#'. 
We also add `[CLS]' to the beginning of each sentence. 

For example, after insertion of the special separate tokens, for a sentence with target entities ``kitchen'' and ``house'' will become to:

 \textit{``[CLS] The \$ kitchen \$ is the last renovated part of the \# house \# . ''}

Given a sentence $s$ with  entity $e_1$ and $e_2$, suppose its final hidden state output from BERT module is $H$. 
Suppose vectors $H_i$ to $H_j$  are the final hidden state vectors from BERT for
entity $e_1$, and  $H_k$ to $H_m$  are the final hidden state vectors from BERT for
entity $e_2$. We apply the average operation to get a vector representation
for each of the two target entities. Then after an activation operation (i.e. tanh), 
we add a fully connected layer to each of the two vectors, and the output
for $e_1$ and $e_2$ are $H'_1$ and $H'_2$ respectively. This process can be mathematically formalized as Equation (\ref{eq1}).
\begin{equation} \label{eq1}
\begin{split}
&H'_1 =W_1 \left[    tanh \left( \frac{1}{j-i+1} \sum_{t=i}^{j} H_t  \right)      \right] + b_1 \\
&H'_2 =W_2 \left[    tanh \left( \frac{1}{m-k+1} \sum_{t=k}^{m} H_t  \right)      \right]  + b_2\\
\end{split}
\end{equation}


We make $W_1$ and $W_2$, $b_1$ and $b_2$ share the same parameters. In other words,
we set $W_1=W_2$, $b_1=b_2$.
For the final hidden state vector of the first token (i.e. `[CLS]'), we also add an activation operation and a
fully connected layer, which is formally expressed
as:
\begin{equation} \label{eq2}
\begin{split}
&H'_0 = W_0 \left( tanh (H_0)  \right)  + b_0\\
\end{split}
\end{equation}
Matrices $W_0$, $W_1$, $W_2$ have the same dimensions, i.e. 
 $W_0 \in R^{d\times d}$, $W_1 \in R^{d\times d}$, $W_2 \in R^{d\times d}$,
 where $d$ is the hidden state size from BERT.

We concatenate $H'_0$, $H'_1$, $H'_2$ and then add a fully connected layer
and a softmax layer, which can be expressed as following:
\begin{equation} \label{eq3}
\begin{split}
&h''=W_3 \bigg [ concat \left(  H'_0, H'_1, H'_2 \right)  \bigg]  + b_3\\
&p = softmax(h'')
\end{split}
\end{equation}
where $W_3 \in R^{L\times 3d}$ ($L$ is the number of relation types),
and $p$ is the probability output.  In Equations (\ref{eq1}),(\ref{eq2}),(\ref{eq3}),
$b_0, b_1, b_2, b_3$ are bias vectors.

We use cross entropy as the loss function. We apply dropout
before each fully connected layer during training. We call our approach as R-BERT. 
\section{Experiments } \label{sec:exp}

\subsection{Dataset and Evaluation Metric} \label{sec:dataset}

We use the SemEval-2010 Task 8 dataset in our experiments. 
The dataset contains nine semantic relation types and one
artificial relation type \textit{Other}, which means that the
relation does not belong to any of the nine relation types.
The nine relation types are 
\textit{Cause-Effect,
Component-Whole, Content-Container, Entity-Destination, Entity-Origin, Instrument-Agency,
Member-Collection, Message-Topic} and \textit{Product-Producer}.
The dataset contains
10,717 sentences, with each containing two nominals e1 and e2,
and the corresponding relation type in the sentence. The relation is
directional, which means that Component-Whole(e1, e2) is 
different from Component-Whole(e2, e1). 
The dataset has already been
partitioned into 8,000 training instances and 2,717
test instances. We evaluate our solution by using the
SemEval-2010 Task 8 official scorer script. It computes
the macro-averaged F1-scores for the nine
actual relations (excluding Other) and considers directionality.


\subsection{Parameter Settings}

Table  shows the major parameters used in our experiments.

\begin{table}[H]
	\caption{Parameter settings.} \label{tab:para}
	\begin{center}
		\begin{tabular}{ |c|c|c| } 
			\hline
			Batch size   & 16  \\ 
    		Max sentence length & 128 \\
			Adam learning rate  & 2e-5 \\
    		Number of epochs & 5\\ 
			Dropout rate  & 0.1 \\
			\hline
		\end{tabular}
	\end{center}
\end{table}

We add dropout before each add-on layer. 
For the pre-trained BERT model, we use the uncased basic model. 
For the parameters of the pre-trained BERT model, please refer
to \cite{bert_Jacobv_corr_bert_2018} for details. 

\subsection{Comparison with other Methods} \label{sec:compare}

We compare our method, R-BERT, against
results by multiple methods recently published for the SemEval-2010
Task 8 dataset, including SVM, RNN, MVRNN, CNN+Softmax, FCM, CR-CNN, Attention-CNN, Entity Attention Bi-LSTM.
The SVM method by \cite{Rink_Semantic_2010} 
uses a rich feature set in a traditional way,
which was the best result
during the SemEval-2010 task 8 competition. 
Details of all other methods are briefly reviewed in Section \ref{related}.

Table \ref{tab:compare_f1} reports the results. We can see that
R-BERT significantly beats all the baseline methods. The MACRO F1 value
of R-BERT is 89.25, which
is much better than the previous best solution on this dataset.



\begin{table}[h]
	\caption{Comparison with results in the literature.}  
	\label{tab:compare_f1}
\begin{tabular}{ | c | c | }
		\hline
		\thead{Method} &  \thead{F1} \\
		\hline
		
		\makecell{SVM  \\  \cite{Rink_Semantic_2010}} &  82.2  \\
		
		\hline
		\makecell{RNN \\  \cite{Socher_EMNLP_2012}} &  77.6  \\
		
		\hline
		\makecell{MVRNN \\  \cite{Socher_EMNLP_2012}} &  82.4  \\	
		
		\hline
		\makecell{CNN+Softmax \\  \cite{Zeng_coling_2014}} &  82.7  \\		
		
		\hline
		\makecell{FCM \\  \cite{Yu_NIPS_Worksho_2014}} &  83.0 \\	
		
		\hline
		\makecell{CR-CNN \\  \cite{Santos_ACL_2015}} &   84.1  \\	
		
			\hline
		\makecell{Attention CNN \\  \cite{Huang_COLING_2016}} &   85.9  \\

	\hline
\makecell{Att-Pooling-CNN \\  \cite{Wang-ACL2016_relation}} &   88.0  \\

			\hline
\makecell{Entity Attention Bi-LSTM \\  \cite{Joohong_Arxiv_2019}} &   85.2  \\
		
		\hline
		R-BERT  &   \textbf{89.25}  \\	
		
		\hline
	\end{tabular}
\end{table}







\subsection{Ablation Studies} \label{sec:ablation}

\subsubsection{Effect of Model Components}

We have demonstrated the strong empirical results
based on the proposed approach. We further want to
understand the specific contributions by the components besides the 
pre-trained BERT component. For this purpose, we create
three more configurations.  

The first configuration is to discard the special separate tokens (i.e. `\$' and `\#') around
the two entities in the sentence and discard the hidden vector output
of the two entities from concatenating with the hidden vector output 
of the sentence. In other words, we add `[CLS]' at the beginning of the sentence and feed the sentence with the two entities into
the BERT module, and use
the first output vector for classification. We label this method as
\textbf{BERT-NO-SEP-NO-ENT}.


The second configuration is to discard the special separate tokens (i.e. `\$' and `\#') around
the two entities in the sentence, but keep the hidden vector output
of the two entities in concatenation for classification. We label this method as
\textbf{BERT-NO-SEP}.

The third configuration is to discard the hidden vector output
of the two entities from concatenation for classification, but keep the special separate tokens. We label this method as
\textbf{BERT-NO-ENT}.

Table \ref{tab:component_f1} reports the results of the ablation study
with the above three configurations. We observe that the three
methods all perform worse than R-BERT. Of the methods,  BERT-NO-SEP-NO-ENT performs worst, with its F1 8.16 absolute points worse
than R-BERT. This ablation study demonstrates that both the special separate
tokens and the hidden entity vectors make important contributions
to our approach.

 In relation classification, the relation label is dependent on 
both the semantics of the sentence and the two target entities. BERT without
special separate tokens cannot locate the target entities and lose this key 
information. 
The reason why the special separate tokens help to improve the
accuracy is that they identify the locations of the two target entities and 
transfer the information into the BERT model, which make the BERT output
contain the location information of the two entities. On the other hand,
incorporating the output of the target entity vectors further enriches the information and helps to make more accurate prediction.  
 

\begin{table}[H]
	\caption{Comparison of the BERT based methods with different components.}  
\label{tab:component_f1}
	\begin{tabular}{ | l | c | }
		\hline
		\thead{Method}  & \thead{F1} \\
		\hline
		R-BERT-NO-SEP-NO-ENT  & 81.09  \\	
		\hline
		R-BERT-NO-SEP  & 87.98 \\	
		\hline
		R-BERT-NO-ENT  & 87.99  \\	
		\hline
	    R-BERT  & \textbf{89.25} \\	
		\hline
	\end{tabular}
\end{table}



%
 

\section{Conclusions} \label{twitter_sec:conclude}

In this paper, we develop an approach for relation classification by
enriching the pre-trained BERT model
with entity information. We add special separate tokens
to each target entity pair and utilize the sentence vector
as well as target entity representations for classification. We 
conduct experiments on the SemEval-2010 benchmark dataset
and our results significantly outperform the state-of-the-art methods.
One possible future work is to extend the model to apply to 
distant supervision.  
  

\bibliographystyle{acl2015}
\bibliography{mynlp}



\end{document}
