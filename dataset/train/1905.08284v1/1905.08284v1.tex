\documentclass[11pt]{article}
\usepackage{acl2015}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}





\providecommand{\cite}[1]{\citeauthoryear{#1}}
\usepackage{natbib}
\renewcommand{\cite}{\citep}

\usepackage{url}

\usepackage{subfigure}
\usepackage{times,amsmath,epsfig}
\usepackage{graphicx}
\usepackage{url}
\usepackage{enumitem}
\usepackage{amsfonts}



\usepackage{amsmath}
\usepackage[noend]{algpseudocode}
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{mathtools}
\usepackage{makecell}



\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\newcommand{\SAVE}[1]{}


\newtheorem{theorems}{Theorem}
\newtheorem{theorem}{Theorem}[section]



\newtheorem{definition}{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newcommand{\reminder}[1]{{\bf  [[[  #1 ]]]} }


\usepackage{mdwlist}  \usepackage{multirow}  \usepackage{amsmath} \usepackage{relsize} 

\usepackage{float}
\usepackage{flushend}

\usepackage{graphicx}
\makeatletter

\newcommand*\bigcdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{}}}}}
\makeatother

\usepackage{tikz}







\title{
	Enriching Pre-trained Language Model with Entity Information for Relation Classification
}

 

\author{\textbf{Shanchan Wu} \\
	Alibaba Group (U.S.) Inc., Sunnyvale, CA   \\
{\tt shanchan.wu@alibaba-inc.com} \\ \\
	\textbf{Yifan He} \\
	Alibaba Group (U.S.) Inc., Sunnyvale, CA  \\
{\tt y.he@alibaba-inc.com} \\
	\\
}

\date{}

\begin{document}
\maketitle



\begin{abstract}
Relation classification is an important NLP task
to extract relations between entities. The 
state-of-the-art methods for relation classification are primarily
based on Convolutional or Recurrent Neural Networks. 
Recently, the pre-trained BERT model
achieves very successful results in many NLP classification  / sequence labeling tasks. Relation classification differs from
those tasks in that it relies on information of both the
sentence and the two target entities.
In this
paper, we propose a model that both leverages the pre-trained BERT language model and incorporates information from the target entities to tackle the relation classification task. 
We locate the target entities and transfer the information through the pre-trained architecture
and incorporate the corresponding encoding of the two entities.
We achieve  significant improvement over the state-of-the-art method on the SemEval-2010 task 8 relational dataset. 
\end{abstract}

 

\section{Introduction}

The task of relation classification is to predict semantic relations between pairs of nominals. 
Given a sequence of text (usually a sentence)  and a pair of nominals  and , the objective
is to identify the relation between  and  \cite{Hendrickx2010_semeval}. 
It is an important NLP task which is normally
used as an intermediate step in variety of NLP
applications. 
The following example shows the
Component-Whole relation between the nominals
``kitchen'' and ``house'':  ``The [kitchen] is the last renovated part of the [house].''

Recently, deep neural networks have applied to relation classification 
\cite{Socher_EMNLP_2012,Zeng_coling_2014,Yu_NIPS_Worksho_2014,Nogueira_ACL_2015,Huang_COLING_2016,Joohong_Arxiv_2019}.
These methods usually use  some features
derived from lexical resources such as Word-Net or NLP tools such as dependency parsers and
named entity recognizers (NER).


Language model pre-training has been shown to be effective
for improving many natural language processing
tasks \cite{Dai_NIPS_2015,Peters_arxiv_2017,OpenAI_2018_tech,Ruder_ACL_2018,bert_Jacobv_corr_bert_2018}. 
The pretrained model BERT proposed by \cite{bert_Jacobv_corr_bert_2018} 
has especially significant impact. It has been applied to multiple NLP tasks and obtains new state-of-the-art results on eleven tasks. The tasks that BERT has been applied to are typically modeled as classification problems and sequence labeling problems.  
It has also been applied to the
SQuAD question answering \cite{Rajpurkar_squad_2016} problem, in
which the objective is to find the starting point and ending point of an answer span. 

As far as we know, the pretrained BERT model \cite{bert_Jacobv_corr_bert_2018} has not been 
applied to relation classification, which relies 
not only on the information of the whole sentence but also 
on the information of the specific target entities.
In this paper, we apply the pretrained BERT model for relation classification. 
We insert special tokens before and after the target entities 
before feeding the text to BERT for fine-tuning, 
in order to
identify the locations of the two target
entities and transfer the information into the BERT model. 
We then locate the positions of the two target entities in the output embedding
from BERT model. We use their embeddings as well as the sentence encoding (embedding of the special first token in the setting of BERT)
as the input to a multi-layer neural network for classification.
By this way, it captures both the semantics of the sentence and
the two target entities to better fit the relation classification task.

Our contributions are as follows: (1) We put forward an innovative approach
to incorporate entity-level information into the pretrained language model for relation classification. (2) We achieve the new state-of-the-art for the relation classification task.




 \section{Related Work}  
\label{related}


There has been some work with deep learning methods for relation
classification, such as 
\cite{Socher_EMNLP_2012,Zeng_coling_2014,Yu_NIPS_Worksho_2014,Nogueira_ACL_2015}

MVRNN model \cite{Socher_EMNLP_2012} applies a recursive neural network (RNN)
to relation classification. They assign a matrix-vector representation to every node
in a parse tree and compute the representation for the complete sentence from bottom up
according to the syntactic structure of the parse tree. 
\cite{Zeng_coling_2014} propose a CNN model by  
incorporating both word embeddings and position features as input.
Then they concatenate lexical features and the output from CNN  
into a single vector and feed them into a softmax layer for prediction.
\cite{Yu_NIPS_Worksho_2014} propose a Factor-based 
Compositional Embedding Model (FCM) by constructing sentence-level and 
substructure embeddings from word embeddings, through dependency trees and named entities.
\cite{Santos_ACL_2015} tackle the 
relation classification task 
by ranking with a convolutional
neural network named CR-CNN. 
Their loss function is based on pairwise ranking. In our work, we take advantage of a pre-trained language model for
the relation classification task, without relying on CNN or RNN
architecutures.
\cite{Huang_COLING_2016} utilize a CNN encoder in conjunction with a sentence representation that weights the words by attention between the target entities and the words in the sentence to perform relation classification.
\cite{Wang-ACL2016_relation} propose a convolutional
neural network architecture with two levels of attention in order
to catch the patterns in heterogeneous
contexts to classify relations.
\cite{Joohong_Arxiv_2019} develop an end-to-end
recurrent neural model which incorporates an
entity-aware attention mechanism with a latent entity
typing for relation classification.  

There are some related work on the relation extraction based on distant supervision,
for example, \cite{Mintz_ACL_2009,Hoffmann_ACL_2011,Lin_ACL_2016,Ji_AAAI_2017,Wu_AAAI_2019}. The difference
between relation classification on regular data and on distantly supervised data is
that the latter may contain a large number of noisy labels. In this paper, we focus on the
regular relation classification problem, without noisy labels. 
  

\begin{figure*}[!t]
	\centering
	\includegraphics[width=0.74\linewidth]{figures/model.pdf}
	\caption{The model architecture. 
	} 
	\label{fig:model_arch}
\end{figure*}




\section{Methodology}  
\label{sec:methods}

\subsection{Pre-trained Model BERT}
The pre-trained BERT model \cite{bert_Jacobv_corr_bert_2018} is a multi-layer bidirectional
Transformer encoder
\cite{Vaswani_NIPS_2017}.

The design of input representation of BERT is to be able to
represent both a single text sentence and a pair of
text sentences in one
token sequence.  The input representation of each token
is constructed by the summation of the corresponding
token, segment and position embeddings.


`[CLS]' is appended to the beginning of each sequence
as the first token of the sequence. The final hidden
state from the Transformer output corresponding to the first token is used as the sentence representation
for classification tasks. In case there are two sentences
in a task, `[SEP]' is used to separate the two sentences. 

BERT pre-trains the model parameters by using a
pre-training objective: the “masked language model” (MLM), which randomly
masks some of the tokens from the input, and set the optimization objective to predict the original vocabulary id of the masked word according to its context. 
Unlike
left-to-right language model pre-training, the
MLM objective can help a state output to utilize both
the left and the right context, which allows a pre-training
system to apply a deep bidirectional Transformer. Besides the masked language model, BERT also
trains a ``next sentence prediction'' task that
jointly pre-trains text-pair representations.





\subsection{Model Architecture}

Figure \ref{fig:model_arch} shows the architecture of our approach.

For a sentence  with two target entities  and ,
to make the BERT module capture the location information of the two entities,
at both the beginning and end of the first entity, we insert
a special token `\ kitchen \se_1e_2HH_iH_je_1H_kH_me_2e_1e_2H'_1H'_2W_1W_2b_1b_2W_1=W_2b_1=b_2W_0W_1W_2W_0 \in R^{d\times d}W_1 \in R^{d\times d}W_2 \in R^{d\times d}dH'_0H'_1H'_2W_3 \in R^{L\times 3d}Lpb_0, b_1, b_2, b_3' and `\#') around
the two entities in the sentence and discard the hidden vector output
of the two entities from concatenating with the hidden vector output 
of the sentence. In other words, we add `[CLS]' at the beginning of the sentence and feed the sentence with the two entities into
the BERT module, and use
the first output vector for classification. We label this method as
\textbf{BERT-NO-SEP-NO-ENT}.


The second configuration is to discard the special separate tokens (i.e. `\$' and `\#') around
the two entities in the sentence, but keep the hidden vector output
of the two entities in concatenation for classification. We label this method as
\textbf{BERT-NO-SEP}.

The third configuration is to discard the hidden vector output
of the two entities from concatenation for classification, but keep the special separate tokens. We label this method as
\textbf{BERT-NO-ENT}.

Table \ref{tab:component_f1} reports the results of the ablation study
with the above three configurations. We observe that the three
methods all perform worse than R-BERT. Of the methods,  BERT-NO-SEP-NO-ENT performs worst, with its F1 8.16 absolute points worse
than R-BERT. This ablation study demonstrates that both the special separate
tokens and the hidden entity vectors make important contributions
to our approach.

 In relation classification, the relation label is dependent on 
both the semantics of the sentence and the two target entities. BERT without
special separate tokens cannot locate the target entities and lose this key 
information. 
The reason why the special separate tokens help to improve the
accuracy is that they identify the locations of the two target entities and 
transfer the information into the BERT model, which make the BERT output
contain the location information of the two entities. On the other hand,
incorporating the output of the target entity vectors further enriches the information and helps to make more accurate prediction.  
 

\begin{table}[H]
	\caption{Comparison of the BERT based methods with different components.}  
\label{tab:component_f1}
	\begin{tabular}{ | l | c | }
		\hline
		\thead{Method}  & \thead{F1} \\
		\hline
		R-BERT-NO-SEP-NO-ENT  & 81.09  \\	
		\hline
		R-BERT-NO-SEP  & 87.98 \\	
		\hline
		R-BERT-NO-ENT  & 87.99  \\	
		\hline
	    R-BERT  & \textbf{89.25} \\	
		\hline
	\end{tabular}
\end{table}



%
 

\section{Conclusions} \label{twitter_sec:conclude}

In this paper, we develop an approach for relation classification by
enriching the pre-trained BERT model
with entity information. We add special separate tokens
to each target entity pair and utilize the sentence vector
as well as target entity representations for classification. We 
conduct experiments on the SemEval-2010 benchmark dataset
and our results significantly outperform the state-of-the-art methods.
One possible future work is to extend the model to apply to 
distant supervision.  
  

\bibliographystyle{acl2015}
\bibliography{mynlp}



\end{document}
