\begin{figure*}[t!]
\begin{center}
\includegraphics[width=1.0\linewidth]{figs/overview_train_img.png}
\end{center}
\caption{
\textbf{Overview (train) -- } 
We initialize the mesh as a regular grid, and use MLPs to represent features and opacity for any point on the mesh.
For each ray, we compute its intersection points on the mesh, and alpha-composite the colors of those points to obtain the output color.
In a later training stage, we enforce \textit{binary} opacity, and perform super-sampling on features for anti-aliasing.
}
\label{fig:overview_train}
\end{figure*}
 \section{Method}
\label{sec:method}
\label{sec:method_overview}
Given a collection of (calibrated) images, we seek to optimize a representation for \textit{efficient} novel-view synthesis.
Our representation consists of a polygonal mesh~(\Figure{overview_rendering}a) whose texture maps~(\Figure{overview_rendering}b) store features and opacity.
At rendering time, given a camera pose, we adopt a two-stage \textit{deferred rendering} process:
\begin{itemize}
\item \textbf{Rendering Stage 1} -- we rasterize the mesh to screen space and construct a \textit{feature image}~(\Figure{overview_rendering}c), i.e. we create a deferred rendering buffer in GPU memory;
\item \textbf{Rendering Stage 2} -- we convert these features into a color image via a (neural) deferred renderer running in a fragment shader, i.e. a small MLP, which receives a feature vector and view direction and outputs a pixel color~(\Figure{overview_rendering}d).
\end{itemize}
Our representation is built in three \textit{training} stages, gradually moving from a classical NeRF-like continuous representation towards a discrete one:
\begin{itemize}
\item \textbf{Training Stage 1 (\Section{stage1})} -- We train a NeRF-like model with \textit{continuous} opacity, where volume rendering quadrature points are derived from the polygonal mesh;
\item \textbf{Training Stage 2 (\Section{stage2})} -- We \textit{binarize} the opacities, as while classical rasterization can easily discard fragments, they cannot elegantly deal with semi-transparent fragments.
\item \textbf{Training Stage 3 (\Section{stage3})} -- We \textit{extract} a sparse polygonal mesh, bake opacities and features into texture maps, and store the weights of the neural deferred shader.
\end{itemize}
The mesh is stored as an OBJ file, the texture maps in PNGs, and the deferred shader weights in a (small) JSON file.
As we employ the standard GPU rasterization pipeline, our real-time renderer is simply an HTML webpage.

As representing continuous signals with discrete representations can introduce aliasing, we also detail a simple, yet computationally efficient, anti-aliasing solution based on super-sampling (\Section{antialiasing}).


\subsection{Continuous training (Training Stage 1)}
\label{sec:stage1}
As \Figure{overview_train} shows, our \textit{training setup} consists of a polygonal mesh $\Mesh{=}(\Triangles,\Vertices)$ and three MLPs.
The mesh topology $\Triangles$ is fixed, but the vertex locations $\Vertices$ and MLPs are optimized, similarly to NeRF, in an auto-decoding fashion by minimizing the mean squared error between predicted colors and ground truth colors of the pixels in the training images\floatersfootnote{}:
\begin{align}
\mathcal{L}_\mathbf{C} = \expect_{\ray}
\| \mathbf{C}(\ray) - \mathbf{C}_\text{gt}(\ray) \|_2^2.
\label{eq:rgb_reconstruction}
\end{align}
where the predicted color $\mathbf{C}(.)$ is obtained by alpha-compositing the radiance $\mathbf{c}_k$ along a ray $\mathbf{r}(t) {=} \mathbf{o} + t \mathbf{d}$, at the (depth sorted) quadrature points $\QuadraturePoints{=}\{t_k\}_{k=1}^K$:
\begin{align}
\mathbf{C}(\mathbf{r}) =
\sum_{k = 1}^{K}
T_k \alpha_k \mathbf{c}_k, \;\;\;\;
T_k = \prod_{l=1}^{k-1}(1-\alpha_l)
\label{eq:alpha_compositing}
\end{align}
where \textit{opacity} $\alpha_k$ and the view-dependent \textit{radiance} $\mathbf{c}_k$ are given by evaluating the MLPs at position $\point_k {=} \ray(t_k)$:
\begin{align}
\alpha_k &= \OpacityMLP(\mathbf{p}_k; \pars_\OpacityMLP)& \OpacityMLP &: \real^3 \rightarrow [0,1]
\label{eq:opacity}
\\
\mathbf{f}_k &= \FeaturesMLP(\mathbf{p}_k; \pars_\FeaturesMLP)& \FeaturesMLP &: \real^3 \rightarrow [0,1]^8
\label{eq:features}
\\
\mathbf{c}_k &= \ShaderMLP(\mathbf{f}_k,\mathbf{d}; \pars_\ShaderMLP)&  \ShaderMLP &: [0,1]^8 \times [-1,1]^3 \rightarrow [0,1]^3
\label{eq:color}
\end{align}
The small network $\ShaderMLP$ is our \textit{deferred neural shader}, which outputs the color of each fragment given the fragment feature and viewing direction.
Finally, note that \eq{alpha_compositing} does not perform compositing with volumetric density~\cite{mildenhall2020nerf}, but rather with opacity~\cite[Eq.8]{attal2022learning}.


\paragraph{Polygonal mesh} 
Without loss of generality, we describe the polygonal mesh used in \textit{Synthetic $360^{\circ}$} scenes, and provide the configurations for \textit{Forward-Facing} and \textit{Unbounded $360^{\circ}$} scenes in \SupplementaryMaterial (\Section{supp_init_meshes}). 2D illustrations can be found in \Figure{init_mesh}.
We first define a \textit{regular} grid $\DualGrid$ of size $P {\times} P {\times} P$ in the unit cube centered at the origin; see \Figure{init_mesh}a.
We instantiate~$\Vertices$ by creating one vertex per voxel, and~$\Triangles$ by creating one quadrangle~(two triangles) per grid edge connecting the vertices of the four adjacent voxels, akin to Dual Contouring~\cite{ju2002dual,chen2022ndc}.
We locally parameterize vertex locations with respect to the voxel centers~(and sizes), resulting in $\Vertices {\in} [-.5, +.5]^{P {\times} P {\times} P {\times} 3}$ free variables.
During optimization, we initialize the vertex locations to $\Vertices{=}\mathbf{0}$, which corresponds to a regular~Euclidean lattice, and 
we regularize them to prevent vertices from exiting their voxels, and to promote their return to their neutral position whenever the optimization problem is under-constrained:
\begin{align}
\mathcal{L}_\Vertices = 
\sum_{\mathbf{v} \in \Vertices}
(10^{3}\,\mathcal{I}(\mathbf{v}) + 10^{-2}) \cdot ||\mathbf{v}||_1,
\end{align}
where the indicator function $\mathcal{I}(\mathbf{v}) {\equiv} 1$ whenever $\mathbf{v}$ is outside its corresponding voxel.


\paragraph{Quadrature}
As evaluating the MLPs of our representation is computationally expensive, we rely on an acceleration grid to limit the cardinality $|\QuadraturePoints|$ of quadrature points.
First of all, quadrature points are only generated for the set of voxels that intersect the ray; see~\Figure{quadrature}a:
Then, like InstantNGP~\cite{mueller2022instant}, we employ an acceleration grid $\Grid$ to prune voxels that are unlikely to contain geometry; see~\Figure{quadrature}b.
Finally, we compute intersections between the ray and the faces of $\Mesh$ that are incident to the voxel's vertex to obtain the final set of quadrature points; see~\Figure{quadrature}c.
We use the barycentric interpolation to back-propagate the gradients from the intersection point to the three vertices in the intersected triangle.
For further technical details on the computation of intersections, we refer the reader to \SupplementaryMaterial (\Section{supp_quadrature}).
In summary, for each input ray $\ray$:
\begin{align}
\tilde\Voxels &= \text{intersect}(\ray, \DualGrid)
\label{eq:ray_voxel_intersection}
\\
\Voxels &= \{ b \in \tilde\Voxels ~|~ \Grid[b] > \tau_\Grid \}
\label{eq:acceleration}
\\
\QuadraturePoints &= \text{intersect}(\ray, \{ \triangle \in \Triangles ~|~ \triangle \cap \Voxels \})
\label{eq:ray_meshface_intersection}
\end{align}
where $(a\cap b){=}\text{\textit{true}}$ if $a$ intersects $b$, and the acceleration grid is supervised so to upper-bound\upperboundfootnote{} the alpha-compositing visibility $T_k \alpha_k$ \textit{across} viewpoints during training.
\begin{align}
\mathcal{L}_\Grid^\text{bnd} &= \sum_k \max( \StopGradient[T_k \alpha_k] - \Grid[\mathbf{p}_k] ,0)
\end{align}
where $\StopGradient[.]$ is the stop-gradient operator that prevents the acceleration grid from (negatively) affecting the image reconstruction quality.
This can be interpreted as a way to compute the so-called ``surface field'' \textit{during} NeRF training, as opposed to \textit{after} training as in nerf2nerf~\cite{nerf2nerf}.
Similarly to Plenoxels~\cite{plenoxels}, we additionally regularize the content of the grid by promoting its pointwise sparsity (i.e. lasso), and its spatial smoothness:
\begin{align}
\mathcal{L}_\Grid^\text{sparse} = \| \Grid \|_1^1
\quad \quad
\mathcal{L}_\Grid^\text{smooth} = \| \nabla \Grid \|_2^2
\end{align}



\subsection{Binarized training (Training Stage 2)}
\label{sec:stage2}
Rendering pipelines implemented in typical hardware \textit{do not} natively support semi-transparent meshes. Rendering semi-transparent meshes requires cumbersome (per-frame) sorting so to execute rendering in back-to-front order to guarantee correct alpha-compositing.
We overcome this issue by converting the smooth opacity~$\opacity_k{\in}[0,1]$ from \eq{opacity} to a discrete/categorical opacity $\dOpacity_k{\in}\{0,1\}$.
To optimize for discrete opacities via photometric supervision we employ a \textit{straight-through estimator}~\cite{straightthrough}:
\begin{align}
\dOpacity_k = \opacity_k + \StopGradient[ \mathbbm{1}(\alpha_k>0.5) - \alpha_k ]
\label{eq:dOpacity}
\end{align}
Please note that the gradients are transparently passed through the discretization operation (i.e.~$\nabla \dOpacity \equiv \nabla \opacity$), regardless of the values of $\alpha_k$ and the resulting $\dOpacity_k{\in}\{0,1\}$.
To stabilize training, we then co-train the continuous and discrete models:
\begin{align}
\mathcal{L}_\Color^\text{bin} &=
\expect_{\ray}
\| \hat\Color(\ray) - \Color_\text{gt}(\ray) \|_2^2 \\
\mathcal{L}_\Color^\text{stage2} &=
\tfrac{1}{2} \mathcal{L}_\Color^\text{bin}
~~+
\tfrac{1}{2} \mathcal{L}_\Color
\label{eq:MSE_plus_binary}
\end{align}
where $\hat\Color(\ray)$ is the output radiance corresponding to the discrete opacity model $\dOpacity$:
\begin{align}
\hat\Color(\ray) &= \sum_{k = 1}^{K}
\hat{T}_k \dOpacity_k \mathbf{c}_k, \;\;\;\;
\hat{T}_k = \prod_{l=1}^{k-1}(1-\dOpacity_l)
\label{eq:col_binarized}
\end{align}
Once \eq{MSE_plus_binary} has converged, we will apply a fine-tuning step to the weights in $\FeaturesMLP$ and $\ShaderMLP$ by minimizing $\mathcal{L}_\Color^\text{bin}$, while fixing the weights of others.


\begin{figure}[t!]
\begin{center}
\includegraphics[width=1.0\linewidth]{figs/init_mesh_img.png}
\end{center}
\caption{
\textbf{Configurations of polygonal meshes --}
The meshes employed for the different types of scenes.
We sketch the distribution of camera poses in training views.
}
\label{fig:init_mesh}
\end{figure}  \begin{figure}[t]
\begin{center}
\includegraphics[width=1.0\linewidth]{figs/quadrature_img.png}
\end{center}
\caption{
\textbf{Quadrature points --} are obtained by
(a) identifying cells that intersect the ray; 
(b) pruning cells that do not contain geometry; and,
(c) computing explicit intersections with the mesh.}
\label{fig:quadrature}
\end{figure}  


\subsection{Discretization (Training Stage 3)}
\label{sec:stage3}
After binarization and fine-tuning, we convert the representation into an explicit polygonal mesh (in OBJ format).
We only store quads if they are at least partially visible in the training camera poses (i.e. non-visible quads are discarded).
We then create a texture image whose size is proportional to the number of visible quads, and for each quad we allocate a $K{\times}K$ patch in the texture, similarly to Disney's Ptex~\cite{burley2008ptex}.
We use $K{=}17$ in our experiments, so that the quad has a $16{\times}16$ texture with half-a-pixel boundary padding.
We then iterate over the pixels of the texture, convert the pixel coordinate to 3D coordinates, and \textit{bake} the values of the discrete opacity~(i.e.~\eq{opacity} and \eq{dOpacity}) and features~(i.e.~\eq{features}) into the texture map.
We quantize the~$[0,1]$ ranges to 8-bit integers, and store the texture into (losslessly compressed) PNG images.
Our experiments show that quantizing the $[0,1]$ range with 8-bit precision, which is not accounted for during back-propagation, does not significantly affect rendering quality.


\subsection{Anti-aliasing}
\label{sec:antialiasing}
In classic rasterization pipelines, aliasing is an issue that ought to be considered to obtain high-quality rendering.
While classical NeRF hallucinates smooth edges via semi-transparent volumes, as previously discussed, semi-transparency would require per-frame polygon sorting. 
We overcome this issue by employing anti-aliasing by super-sampling.
While we could simply execute \eq{color} four times/pixel and average the resulting color, the execution of the deferred neural shader $\ShaderMLP$ is the computational bottleneck of our technique.
We can overcome this issue by simply averaging the features, that is, \textit{averaging the input} of the deferred neural shader, rather than averaging its output.
We first rasterize features (at $2\times$ resolution):
\begin{align}
\Feature(\ray) = \sum_k T_k \alpha_k \mathbf{f}_k,
\end{align}
and then average sub-pixel features to produce the anti-aliased representation we feed to our neural deferred shader:
\begin{align}
\Color(\ray) = \ShaderMLP
\left(
\expect_{\ray_\delta \sim \ray}[\Feature(\ray_\delta)], \:
\expect_{\ray_\delta \sim \ray}[\mathbf{d}_\delta]
\right)
\label{eq:antialiased_shader}
\end{align}
where $\expect_{\ray_\delta {\sim} \ray}$ computes the average between the sub-pixels~(i.e. four in our implementation), and $\mathbf{d}_\delta$ is the direction of ray $\ray_\delta$.
Note how with this change we only query~$\ShaderMLP$ \textit{once} per output pixel. Finally, this process is analogously applied to \eq{col_binarized} for discrete occupancies $\dOpacity$.
These changes for anti-aliasing are applied in training stage 2 \eq{MSE_plus_binary}.


\subsection{Rendering}
\label{sec:rendering}
The result of the optimization process is a textured polygonal mesh (where texture maps store features rather than colors) and a small MLP~(which converts view direction and features to colors).
Rendering this representation is done in two passes using a deferred rendering pipeline:
\begin{enumerate}
\item we rasterize all faces of the textured mesh with a z-buffer to produce a $2M {\times} 2N$ feature image with 12 channels per pixel, comprising $8$ channels of learned features, a binary opacity, and a 3D view direction;
\item we synthesize an $M \times N$ output RGB image by rendering a textured rectangle that uses the feature image as its texture, with linear filtering to average the features for antialiasing. We apply the small MLP for pixels with non-zero alphas to convert features into RGB colors. The small MLP is implemented as a GLSL fragment shader.
\end{enumerate}
These rendering steps are implemented within the classic rasterization pipeline.  Since z-buffering with binary transparency is order-independent, polygons \textit{do not} need to be sorted into depth-order for each new view, and thus can be loaded into a buffer in the GPU once at the start of execution.
Since the MLP for converting features to colors is very small, it can be implemented in a GLSL fragment shader~\cite{hedman2021snerg}, which is run in parallel for all pixels.
These classical rendering steps are highly-optimized on GPUs, and thus our rendering system can run at interactive frame rates on a wide variety of devices; see~\Table{test_FPS}.
It is also easy to implement, since it requires only standard polygon rendering with a fragment shader.
Our interactive viewer is an HTML webpage with Javascript, rendered by WebGL via the \texttt{threejs} library.


