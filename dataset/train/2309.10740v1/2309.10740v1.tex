\documentclass{article}

\usepackage{spconf,graphicx}
\usepackage{hyperref}
\usepackage{amsfonts,amsmath,amssymb}
\usepackage{mathtools}
\usepackage{pifont}
\usepackage{enumitem}
\usepackage{booktabs,multirow}
\usepackage{xcolor}
\usepackage{cleveref}
\usepackage{placeins}


\setitemize{leftmargin=1.4em}

\crefformat{equation}{\textup{#2(#1)#3}}
\crefrangeformat{equation}{\textup{#3(#1)#4--#5(#2)#6}}
\crefmultiformat{equation}{\textup{#2(#1)#3}}{ and \textup{#2(#1)#3}}
{, \textup{#2(#1)#3}}{, and \textup{#2(#1)#3}}
\crefrangemultiformat{equation}{\textup{#3(#1)#4--#5(#2)#6}}{ and \textup{#3(#1)#4--#5(#2)#6}}{, \textup{#3(#1)#4--#5(#2)#6}}{, and \textup{#3(#1)#4--#5(#2)#6}}
\Crefformat{equation}{#2Equation~\textup{(#1)}#3}
\Crefrangeformat{equation}{Equations~\textup{#3(#1)#4--#5(#2)#6}}
\Crefmultiformat{equation}{Equations~\textup{#2(#1)#3}}{ and \textup{#2(#1)#3}}
{, \textup{#2(#1)#3}}{, and \textup{#2(#1)#3}}
\Crefrangemultiformat{equation}{Equations~\textup{#3(#1)#4--#5(#2)#6}}{ and \textup{#3(#1)#4--#5(#2)#6}}{, \textup{#3(#1)#4--#5(#2)#6}}{, and \textup{#3(#1)#4--#5(#2)#6}}


\let\OLDthebibliography\thebibliography
\renewcommand\thebibliography[1]{
    \OLDthebibliography{#1}
    \setlength{\parskip}{0pt}
    \setlength{\itemsep}{2.8pt}
}


\newcommand{\et}{\boldsymbol{e}_\mathrm{te}}
\newcommand{\sz}{\boldsymbol{z}}
\newcommand{\shatz}{\boldsymbol{\hat{z}}}
\newcommand{\sx}{\boldsymbol{x}}
\newcommand{\shatx}{\boldsymbol{\hat{x}}}
\newcommand{\sv}{\boldsymbol{v}}
\newcommand{\svcfg}{\sv_{\textrm{cfg}}}
\newcommand{\svcond}{\sv_{\textrm{cond}}}
\newcommand{\svuncond}{\sv_{\textrm{uncond}}}
\newcommand{\fstu}{f_{\mathrm{S}}}
\newcommand{\ftea}{f_{\mathrm{T}}}
\newcommand{\ftcfg}{f_{\mathrm{T}}^{\mathrm{cfg}}}
\newcommand{\Uint}{\mathrm{U_{int}}}
\newcommand{\solve}{\mathrm{solve}}
\newcommand{\EGen}{\mathbf{e}_{\boldsymbol{\widehat{x}}}}
\newcommand{\ERef}{\mathbf{e}_{\sx}}
\newcommand{\CLAPA}{\text{CLAP}_\text{A}}
\newcommand{\CLAPT}{\text{CLAP}_\text{T}}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}
\newcommand*{\blue}{\textcolor{blue}}
\renewcommand{\arraystretch}{.892}  


\title{Accelerating Diffusion-Based Text-to-Audio Generation\\with Consistency Distillation}


\name{
    Yatong Bai\sthanks{Work done during internship at Microsoft. Correspondence email \href{mailto:yatong_bai@berkeley.edu}{\texttt{yatong\_bai@berkeley.edu}}.}$^{1,2}$ \qquad
    Trung Dang$^{2}$ \qquad
    Dung Tran$^{2}$ \qquad
    Kazuhito Koishida$^{2}$ \qquad
    Somayeh Sojoudi$^{1}$
}
\address{
    $^{1}$University of California, Berkeley $\qquad\qquad$
    $^{2}$Applied Sciences Group, Microsoft Corporation
}


\begin{document}

\ninept  


\maketitle


\begin{abstract}
Diffusion models power a vast majority of text-to-audio (TTA) generation methods. Unfortunately, these models suffer from slow inference speed due to iterative queries to the underlying denoising network, thus unsuitable for scenarios with inference time or computational constraints. This work modifies the recently proposed consistency distillation framework to train TTA models that require only a single neural network query. In addition to incorporating classifier-free guidance into the distillation process, we leverage the availability of generated audio during distillation training to fine-tune the consistency TTA model with novel loss functions in the audio space, such as the CLAP score. Our objective and subjective evaluation results on the AudioCaps dataset show that consistency models retain diffusion models' high generation quality and diversity while reducing the number of queries by a factor of 400.
\end{abstract}


\begin{keywords}
Diffusion models, Consistency models, Audio generation, Generative AI, Neural networks
\end{keywords}



\section{Introduction} \label{sec:intro}

Text-to-audio (TTA) generation has recently gained significant popularity \cite{tango, diffsound, audioldm, audioldm-2, make-an-audio, make-an-audio-2, codi, audiogen, riffusion}. This task involves generating audio based on a user-provided textual prompt. TTA models have rapidly improved and demonstrated the ability to produce diverse, precise, and high-quality audio. Many existing TTA models are based on latent diffusion models (LDM) \cite{ldm}, which have gained popularity in various applications due to their superior generation quality. However, they suffer from slow inference speed as they require iterative queries to the underlying neural network. Such limitations pose challenges in scenarios with time or computation constraints.

This work proposes a novel approach to accelerate diffusion-based TTA models. The proposed method is based on consistency distillation (CD) \cite{cm}, which distills a pre-trained diffusion model into a consistency model that only requires a single neural network query per generation. Our approach leverages classifier-free guidance (CFG) \cite{cfg}, which has been shown to significantly enhance text-conditioned generative model performance, by incorporating it into the CD process. We explore three different approaches for using CFG: direct guidance, fixed guidance, and variable guidance. To our knowledge, this is the first work to extend CD to CFG models.

Moreover, leveraging the generated audio that is only available during consistency distillation training, we propose fine-tuning the consistency TTA model with audio space loss functions to further improve the audio quality and the audio-text correspondence and use the CLAP score as an example loss function. In contrast, back-propagation from the audio is prohibitively expensive for diffusion models due to the recurrent diffusion process.

Our experiments on the AudioCaps dataset demonstrate that the single-step consistency model is comparable with the 400-step distillation model across five objective metrics as well as subjective audio quality and audio-text correspondence. We encourage the reader to listen to our generated examples at \href{https://bai-yt.github.io/consistency_tta/demo.html}{\texttt{bai-yt.github.io/consi\\stency\_tta/demo}}.

The paper is structured as follows. \Cref{sec:background} reviews the related literature, including diffusion models and acceleration techniques. \Cref{sec:methods} outlines our proposed methods to accelerate diffusion-based TTA models. \Cref{sec:experiments} discusses our experimental results. Additional discussions and details are presented in the appendix. Throughout this paper, vectors and matrices are denoted as bold symbols whereas scalars use regular symbols.



\section{Background and related work} \label{sec:background}

\subsection{Diffusion models}

Diffusion models \cite{diffusion, ddpm}, known for their diverse, high-quality generations, have rapidly gained popularity among various conditional and unconditional generation tasks in vision and audio fields \cite{sd, edm, audioldm, noise2music}. In the vision domain, while pixel-level diffusion methods (e.g., EDM \cite{edm}) perform well on small image sizes, generating larger images usually requires latent diffusion models (LDMs) \cite{ldm}, where the diffusion process takes place in a latent space. In the audio domain, generative model applications can be further categorized into speech, music, and in-the-wild audio generation. This paper considers the in-the-wild audio setting, where the goal is to generate diversified samples that cover a variety of real-world sound clips. While some works consider autoregressive models \cite{audiogen} or Mel-space diffusion \cite{riffusion}, LDMs have emerged as the dominant approach for the TTA task \cite{tango, diffsound, audioldm, audioldm-2, make-an-audio, make-an-audio-2, codi}. 

The intuition of diffusion models is to gradually recover a clean sample from a noisy sample. During training, Gaussian noise is progressively added to a ground-truth sample $\sz_0$, forming a continuous diffusion trajectory. At the end of the trajectory, the noisy sample becomes indistinguishable from pure Gaussian noise. This trajectory is then discretized into $N$ steps, where the noisy sample at each time step is denoted as $\sz_n$ for $n = 1, \ldots, N$. In each training step, a random time step $n$ is selected, and a Gaussian noise with variance depending on $n$ is injected into the clean sample to produce $\sz_n$. A denoising neural network, often a U-Net \cite{unet}, is optimized to recover the noise distribution from the noisy sample. During inference, Gaussian noise is used to initialize the last noisy sample $\shatz_{N}$, where $\shatz_{n}$ denotes the predicted sample at the time step $n$. The diffusion model generates a clean sample by iteratively querying the denoising network step by step, producing the sequence $\shatz_{N-1}, \ldots, \shatz_0$. The final $\shatz_0$ is used as the generated sample.


\subsection{Accelerating diffusion model inference}

Diffusion models suffer from high generation latency and expensive inference computation due to iterative queries to the denoising network. To this end, several methods have been proposed to reduce the number of model queries. Such methods are mostly presented for image generation tasks and can be grouped into two main categories: improved differential equation solvers and distillation methods. 

Improved differential equation solvers can reduce the number of inference steps $N$ of existing diffusion models without additional training. Examples include DDIM \cite{ddim}, Euler \cite{euler}, Heun, DPM \cite{dpm, dpm++}, and PNDM \cite{pndm}. The best solvers can reduce $N$ to 10-50 from the hundreds required by vanilla inference using DDPM \cite{ddpm}.

On the other hand, distillation methods, where a pre-trained diffusion model serves as the teacher and a student model is trained to simulate multiple teacher steps in a single step, have been shown to reduce the number of denoising steps to below 10. One representative method is progressive distillation (PD) \cite{pd}, which iteratively halves the number of diffusion steps. While PD can reduce the number of steps to only a few, the single-step capability is unideal, and the repetitive distillation procedure can be time-consuming. To this end, consistency distillation \cite{cm} has been proposed. The training goal of CD is to reconstruct the noiseless image within a single step from an arbitrary step on the teacher model's diffusion trajectory. Note that both PD and CD were proposed for \textit{unconditional} image generation. For text-conditioned audio generation, there are additional considerations, which we discuss in \Cref{sec:methods}.


\subsection{Classifier-free guidance}

CFG \cite{cfg} is a simple yet effective method for adjusting the text conditioning strength for guided generation problems, significantly improving the performance of existing diffusion-based TTA models. CFG obtains two noise estimations from the denoising network in the diffusion model -- one with text conditioning (denoted as $\svcond$) and one without (by masking the text embeddings, denoted as $\svuncond$). The guided estimation, denoted by $\svcfg$, is obtained via
\vspace{-.4mm}
\begin{equation} \label{eq:cfg}
    \svcfg = w \cdot \svcond + (1 - w) \cdot \svuncond,
    \vspace{-.4mm}
\end{equation}
where the scalar $w \geq 0$ is the guidance strength. When $w$ is between 0 and 1, CFG interpolates the conditioned and unconditioned estimations. When $w$ is greater than 1, CFG becomes an extrapolation. For example, for TANGO, $w = 3$ produces the best overall result \cite{tango}.

Since CFG is external to the denoising network in diffusion models, it makes distillating guided models more complex than their unguided counterparts. The authors of \cite{distillcfg} outlined a two-stage pipeline for performing PD on a CFG classifier. The first stage absorbs CFG into the denoising network by letting the student network take $w$ as an additional input. The second stage performs conventional PD on top of the stage-1 student. During both training stages, the CFG strength $w$ is randomized, and the resulting distilled network allows for selecting $w$ during inference.



\section{Consistency distillation for TTA} \label{sec:methods}

We select TANGO \cite{tango} as the distillation teacher model due to its high performance. However, we highlight that most of the innovations in this paper can also be applied to other diffusion-based TTA models.


\subsection{Overall setup}

Similar to TANGO, our model has four components: a conditional U-Net, a text encoder that processes the textual prompt, a VAE encoder-decoder pair that converts the Mel spectrogram to and from the U-Net latent space, and a HiFi-GAN vocoder \cite{hifigan} that produces time-domain audio waveform from the Mel spectrogram. We only train the U-Net and freeze other components.

During training, the Mel spectrogram of the audio is processed by the VAE encoder to produce a latent representation, and the prompt is transformed by the text encoder into a text embedding. They are given to the conditional U-Net as the input and the condition. The VAE decoder and the HiFi-GAN are not used.

During inference, the text embedding is used to guide the U-Net to reconstruct a latent audio representation. The Mel spectrogram and waveform are recovered by the VAE decoder and the HiFi-GAN vocoder, respectively. The VAE encoder is not used.


\subsection{Consistency distillation}

The goal of CD is to learn a student U-Net $\fstu (\cdot, \cdot, \cdot)$ from the diffusion U-Net module in the teacher TTA model $\ftea (\cdot, \cdot, \cdot)$. The architecture of $\fstu$ is the same as the $\ftea$, taking three inputs: the noisy latent representation $\sz_n$, the corresponding time step $n$, and the text embedding $\et$. Furthermore, the parameters in $\fstu$ are initialized using $\ftea$ information.

The goal for the student U-Net is to generate a realistic latent audio representation within a single forward pass, directly producing an estimated clean example $\shatz_0$ based on $\sz_n$, where $n \in \{0, \ldots, N\}$ is an arbitrary step along the diffusion trajectory \cite[Algorithm 2]{cm}. The risk function to be minimized for achieving this goal is
\vspace{-.4mm}
\begin{equation} \label{eq:consis_risk}
    \mathbb{E}_{\substack{(\sz_0, \et) \sim \mathcal{D} \hfill \\ n \sim \Uint(1, N)}} \Big[ d \Big( \fstu (\sz_n, n, \et), \fstu (\shatz_{n-1}, n-1, \et) \Big) \Big],
    \vspace{-.4mm}
\end{equation}
where $d (\cdot, \cdot)$ is a distance measurement, $\mathcal{D}$ is the training dataset, $\Uint (1, N)$ denotes the discrete uniform distribution supported over the set $\{1, \ldots, N\}$, and $\shatz_{n-1} = \solve \circ \ftea(\sz_n, n, \et)$ is the teacher diffusion model's estimation for $\sz_{n-1}$. Here, $\solve \circ \ftea$ denotes the composite function of the teacher denoising U-Net and the solver that converts the U-Net raw output to the estimation of the previous time step. We use the $\ell_2$ distance in this latent space as $d (\cdot, \cdot)$, with additional discussions in \Cref{sec:model_details}. Intuitively, this risk measures the expected distance between the student's reconstructions from two adjacent time steps on the diffusion trajectory.

The authors of \cite{cm} used the Heun solver for querying the teacher diffusion model during distillation and adopted ``Karras noise schedule'', a discretization scheme that unevenly selects the time steps on the diffusion trajectory. In \Cref{sec:experiments}, we empirically investigate multiple solvers and noise schedules.

\begin{table*}[t]
\centering
\vspace{-1.6mm}
\caption{Ablate various guidance weights, distillation techniques, solvers, noise schedules, training lengths, loss weights, and initialization. ``CFG $w$'' represents the guidance weight; ``\# queries'' indicates the number of neural network queries during inference. U-Net modules have 557M parameters, except in variable guidance models (559M). Distillation runs are 40 epochs; inference uses FP32 precision.}
\label{tab:ablate}
\aboverulesep=0ex \belowrulesep=.4ex
\begin{footnotesize}
\begin{tabular}{lllclclccc}
    \toprule
    \# queries ($\downarrow$)   & Solver                & Noise schedule
    & CFG $w$           & Guidance method   & Min-SNR           & Initialization
    & FAD ($\downarrow$)    & FD ($\downarrow$)                 & KLD ($\downarrow$) \\
    \midrule
    \multirow{2}{*}{1}  & DDIM  & Uniform   & \multirow{2}{*}{1}    & \multirow{2}{*}{-}    & \multirow{2}{*}{\xmark}   & \multirow{2}{*}{Unguided}          & 13.48 & 45.75 & 2.409 \\
    & Heun  & Karras    &   &   &               &               & 10.97 & 50.19 & 2.425 \\
    \midrule
    \multirow{2}{*}{2}  & DDIM  & Uniform
    & \multirow{2}{*}{3}
    & \multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Direct guidance\end{tabular}}
    & \multirow{2}{*}{\xmark}   & \multirow{2}{*}{Unguided}        & 8.565 & 38.67 & 2.015 \\
    & Heun  & Karras    &   &   &               &               & 7.421 & 39.36 & 1.976 \\
    \midrule
    \multirow{4}{*}{1} & \multirow{4}{*}{Heun}  & Karras                                    & \multirow{4}{*}{\begin{tabular}[c]{@{}l@{}}3\\ \end{tabular}}
    & \multirow{4}{*}{\begin{tabular}[c]{@{}l@{}}Fixed guidance\\distillation\end{tabular}}
    & \xmark            & Unguided                                 & 5.702 & 33.18 & 1.494 \\
    &       & Uniform   &   &   & \xmark    & Unguided             & 4.168 & 28.54 & 1.384 \\
    &       & Uniform   &   &   & \cmark    & Unguided             & 3.766 & 27.74 & 1.443 \\
    &       & Uniform   &   &   & \xmark    & Guided            & 3.859 & 27.79 & 1.421 \\
    \midrule
    \multirow{3}{*}{1} & \multirow{3}{*}{Heun}  & \multirow{3}{*}{Uniform}  & 3
    & \multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}Variable guidance\\distillation\end{tabular}}
    & \multirow{3}{*}{\cmark}   & \multirow{3}{*}{Guided}       & 3.956 & 28.27 & 1.442 \\
    &       &           & 4 &   &               &               & 3.180 & 27.92 & 1.394 \\
    &       &           & 6 &   &               &               & 2.975 & 28.63 & 1.378 \\
    \bottomrule
\end{tabular}
\end{footnotesize}
\end{table*}


\subsection{Consistency distillation with classifier-free guidance} \label{sec:distill_cfg}

Since CFG is crucial to the conditional generation quality, we consider three methods for incorporating it into the distilled model.

\vspace{-11pt}
\paragraph*{Direct Guidance} directly performs CFG on the consistency model output by applying \cref{eq:cfg}. Since this method na\"ively extrapolates or interpolates on the consistency model $\sz_0$ prediction, the CFG operation will likely move the prediction outside the manifold of realistic latent representations.

\vspace{-11pt}
\paragraph*{Fixed Guidance Distillation} aims to distill from the diffusion model coupled with CFG using a fixed guidance strength $w$. Specifically, the training risk function is still \cref{eq:consis_risk}, but $\shatz_{n-1}$ is replaced with the estimation after CFG. Now, $\shatz_{n-1}$ becomes $\solve \circ \ftcfg(\sz_n, n, \et, w)$, where the guided teacher output $\ftcfg$ is
\vspace{-1mm}
\begin{align*}
    \ftcfg (\sz_n, n, \et, w) = & \\[-.6mm]
    w \cdot \ftea (& \sz_n, n, \varnothing) + (1-w) \cdot \ftea (\sz_n, n, \et), \\[-5.8mm]
\end{align*}
with $\varnothing$ denoting the masked language token. Here, $w$ should be fixed to the value corresponding to the best teacher generation quality.

\vspace{-11pt}
\paragraph*{Variable Guidance Distillation} is similar to fixed guidance distillation, but with randomized guidance strength $w$ during distillation, so that $w$ can be adjusted during inference. To make the student network compatible with adjustable $w$, we add a $w$-encoding condition branch to $\fstu$ (which now has four inputs). We use Fourier encoding for $w$ following \cite{distillcfg} and merge the embedding into $\fstu$ similarly to the time step embedding. Each training iteration samples a random guidance strength $w$ via the uniform distribution supported on $[0, 6)$.

The latter two methods are related to the two-stage distillation procedure outlined in \cite{distillcfg}, with details described in \Cref{sec:twostage_details}.


\subsection{Min-SNR training loss weighing strategy}

The literature has proposed to improve diffusion models by using the truncated signal-noise ratio (SNR) to weigh the training loss at each time step $n$ for diffusion models, and the Min-SNR strategy \cite{minsnr} is one of the latest examples. The specific calculation of Min-SNR depends on the parameterization of the diffusion model. Specifically, diffusion models can be trained to predict the clean example $\sz_0$, the additive noise $\boldsymbol{\epsilon}$, or the noise velocity $\boldsymbol{v}$. The Min-SNR weighting formulation is different for the three parameterizations.

This work investigates whether the Min-SNR strategy also improves CD. Since consistency models predict the clean sample $\sz_0$, we use the Min-SNR formulation for $\sz_0$-predicting diffusion models, which is $\omega (n) = \min \{ \mathrm{SNR} (t_n), \gamma \}$, where $\omega (n)$ is the loss weight for the $n^{\text{th}}$ time step, $\mathrm{SNR} (t)$ is the SNR at time $t$, $t_n$ is the time corresponding to the $n^{\text{th}}$ time step, and $\gamma$ is a constant defaulted to 5. For the Heun solver used in most of our experiments, $\mathrm{SNR} (t)$ is the inverse of the additive Gaussian noise variance at time $t$.


\subsection{End-to-end fine-tuning with CLAP} \label{sec:CLAP_finetune}

Since our consistency TTA model produces audio in a single neural network query, we can optimize auxiliary losses operating in the audio space along with the latent-space CD loss to improve the audio quality and semantics. On the contrary, since a diffusion model has an iterative inference process, optimizing such a model by back-propagating from the audio resembles the training of a recurrent neural network, which is known to be expensive and challenging.
This work uses the CLAP score \cite{clap} as an example of fine-tuning loss function. The CLAP score, denoted by $\mathrm{CS}$, is defined as:
\vspace{-1.2mm}
\begin{equation} \label{eq:clapscore}
    \mathrm{CS} (\shatx, \sx) = \max \left\{ 100 \times \frac{\EGen \cdot \ERef} {\| \EGen \| \cdot \| \ERef \|} , 0 \right\}, \\[-.5mm]
\end{equation}
where $\shatx$ is the generated audio waveform, $\sx$ is the reference (ground-truth waveform or textual prompt), and $\EGen$ and $\ERef$ are the corresponding embeddings extracted by the CLAP model.

We select the CLAP score due to its superior embedding quality arising from the diverse training tasks and datasets, as well as its consideration of audio-text correspondence. Since the CD training loss \cref{eq:consis_risk} does not use ground truth information, optimizing this score provides valuable feedback to the consistency model.



\section{Experiments} \label{sec:experiments}

\subsection{Dataset and experiment settings}

The experiments in this work use AudioCaps \cite{audiocaps}, a popular dataset for in-the-wild audio generation. AudioCaps is a collection of human-captioned YouTube audio, each instance having a length of at most ten seconds. Our AudioCaps copy contains 882 test instances and 45,260 training instances. Like several existing works \cite{tango, audioldm}, the core generative U-Net of our models is trained only on the AudioCaps training set, leaving larger datasets for future work.

While we explicitly use TANGO \cite{tango} as the baseline, our methods apply to diffusion-based TTA models in general. We select FLAN-T5-Large \cite{flan} as the text encoder and use the same checkpoint as \cite{tango}. For the VAE and the HiFi-GAN, we use the checkpoint pre-trained on AudioSet released by the authors of \cite{audioldm} as in \cite{tango}. For faster training and inference, we shrink the U-Net from 866M parameters used in \cite{tango} to 557M. All consistency models are distilled from this smaller TANGO model, which performs similarly to the checkpoint from \cite{tango} (\Cref{tab:compare_obj}). Additional details about our model, training setup, and evaluation are shown in Appendix \ref{sec:model_details} and \ref{sec:human_details}.

\begin{table*}[!tb]
\centering
\vspace{-1.6mm}
\caption{Compare consistency models to the diffusion baselines. Distillation runs are extended to 60 epochs for better performance; CLAP-fine-tuning uses 10 additional epochs. All CD runs use the Heun teacher solver, uniform noise schedule, variable guidance distillation, guided initialization, Min-SNR loss weights, and BF16 inference precision. Bold numbers indicate the best results among consistency models.}
\label{tab:compare_obj}
\aboverulesep=0ex \belowrulesep=.4ex
\begin{footnotesize}
\begin{tabular}{lcccccccc}
    \toprule
                                    & U-Net \# params          & CFG $w$           
    & \# queries ($\downarrow$)     & $\CLAPT$ ($\uparrow$) & $\CLAPA$ ($\uparrow$)
    & FAD ($\downarrow$)            & FD ($\downarrow$)     & KLD ($\downarrow$)  \\
    \midrule
    AudioLDM-L reported in \cite{audioldm}  & 739M          & 2
    & \multirow{4}{*}{400}          & -                     & -
    & 2.08                          & 27.12                 & 1.86 \\
    TANGO reported in \cite{tango}  & 866M                  & 3
    &                               & -                     & -
    & 1.59                          & 24.53                 & 1.37 \\
    TANGO \cite{tango} tested by us & 866M                  & 3
    &                               & 24.10                 & 72.85
    & 1.631                         & 20.11                 & 1.362 \\
    Our TANGO model                 & 557M                  & 3
    &                               & 24.57                 & 72.79
    & 1.908                         & 19.57                 & 1.350 \\
    \midrule
    \multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}Consistency model\\without CLAP fine-tuning\end{tabular}}
    & \multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}559M\end{tabular}} & 3
    & \multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}1\end{tabular}}
    &                                 21.00                 & 71.39
    & 3.202                         & 22.04                 & 1.411 \\
    &   &   4   &                   & 22.05                 & 72.08
    & 2.610                         & 21.71                 & 1.373 \\
    &   &   5   &                   & 22.50                 & 72.30
    & 2.575                         & 22.08                 & \textbf{1.354} \\
    \midrule
    \multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}Consistency model\\with CLAP fine-tuning\end{tabular}}
    & \multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}559M\end{tabular}} & 3
    & \multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}1\end{tabular}}
    &                                 24.44                 & 72.39
    & \textbf{2.182}                & \textbf{20.44}        & 1.368 \\
    &   &   4   &                   & 24.69                 & \textbf{72.54}
    & 2.406                         & 20.97                 & 1.358 \\
    &   &   5   &                   & \textbf{24.70}        & 72.53
    & 2.626                         & 21.33                 & 1.356 \\
    \bottomrule
\end{tabular}
\end{footnotesize}
\end{table*}

\begin{table*}[!tb]
\centering
\vspace{-2mm}
\caption{Compare the human evaluation results of consistency and diffusion models. Bold numbers are defined same as \Cref{tab:compare_obj}.}
\label{tab:human_eval}
\aboverulesep=0ex \belowrulesep=.4ex
\begin{footnotesize}
\begin{tabular}{lccccccccccc}
    \toprule
    & \begin{tabular}[c]{@{}l@{}}U-Net\\\# params\end{tabular}
    & \begin{tabular}[c]{@{}l@{}}\# queries\\($\downarrow$)\end{tabular}
    & \begin{tabular}[c]{@{}l@{}}CLAP\\fine-tuning\end{tabular}
    & \begin{tabular}[c]{@{}l@{}}CFG\\$w$\end{tabular}
    & \begin{tabular}[c]{@{}l@{}}Human\\Quality ($\uparrow$)\end{tabular}
    & \begin{tabular}[c]{@{}l@{}}Human\\Corresp ($\uparrow$)\end{tabular}
    & \begin{tabular}[c]{@{}l@{}}$\CLAPT$\\($\uparrow$)\end{tabular}
    & \begin{tabular}[c]{@{}l@{}}$\CLAPA$\\($\uparrow$)\end{tabular}
    & \begin{tabular}[c]{@{}l@{}}FAD\\($\downarrow$)\end{tabular}
    & \begin{tabular}[c]{@{}l@{}}FD\\($\downarrow$)\end{tabular}
    & \begin{tabular}[c]{@{}l@{}}KLD\\($\downarrow$)\end{tabular}   \\
    \midrule
    Diffusion                           & 557M                      & 400
    & \xmark            & 3     & 4.136             & 4.064         & 24.57
    & 72.79             & 1.908             & 19.57                 & 1.350 \\
    \midrule
    \multirow{2}{*}{Consistency}        & \multirow{2}{*}{559M}     & \multirow{2}{*}{1}
    & \xmark            & 5     & \textbf{3.902}    & 4.010         & 22.50
    & 72.30             & 2.575             & 22.08                 & \textbf{1.354} \\
    &   &   & \cmark    & 4     & 3.830     & \textbf{4.064}        & \textbf{24.69}
    & \textbf{72.54}    & \textbf{2.406}    & \textbf{20.97}        & 1.358 \\
    \midrule
    Ground-truth audio & - & - & - & - & 4.424 & 4.352         & 26.71
    & 100.0             & 0.000 & 0.000 & 0.000                     \\
    \bottomrule
\end{tabular}
\end{footnotesize}
\end{table*}

\begin{table*}[!ht]
\vspace{-2mm}
\centering
\aboverulesep=0ex \belowrulesep=.2ex
\caption{The generated audio noticeably varies with different random seeds. The horizontal axis is time in seconds.}
\vspace{.6mm}
\begin{footnotesize}
\begin{tabular}{cc|cc}
    \textbf{Seed 0} & \textbf{Seed 20230817} & \textbf{Seed 0} & \textbf{Seed 20230817} \\
    \toprule
    \hspace{-2mm} \includegraphics[height=27.3mm, trim=2.5mm 2mm 2.5mm 0, clip=true]{diversity_short/train_seed0.pdf} \hspace{-2mm} &
    \hspace{-2mm} \includegraphics[height=27.3mm, trim=5.5mm 2mm 2mm 0, clip=true]{diversity_short/train_seed20230817.pdf} &
    \hspace{-2mm} \includegraphics[height=27.3mm, trim=2.5mm 2mm 2.5mm 0, clip=true]{diversity_short/food_seed0.pdf} \hspace{-2mm} &
    \hspace{-2mm} \includegraphics[height=27.3mm, trim=5.5mm 2mm 2mm 0, clip=true]{diversity_short/food_seed20230817.pdf}
    \includegraphics[height=27.3mm, trim=0 0 2.8mm 0, clip=true]{diversity_short/colorbar.pdf} \hspace{-2mm} \\[.3mm]
    \multicolumn{2}{c|}{A train sounds horn and travels.} &
    \multicolumn{2}{c}{Food sizzling with some knocking and banging followed by a woman speaking.}
\end{tabular}
\end{footnotesize}
\vspace{-.7mm}
\label{tab:diversity}
\end{table*}


\subsection{Objective evaluation results}

Our objective evaluation considers five metrics: FAD, FD, KLD, $\CLAPA$, and $\CLAPT$. Specifically, FAD is the Fr\'echet distance between generated and ground-truth audio embeddings extracted by the VGGish model \cite{vggish}, FD is the Fr\'echet distance between the embeddings extracted by PANN \cite{pann}, and KLD is Kullback-Leibler divergence between the PANN embeddings. $\CLAPA$ and $\CLAPT$ are the CLAP scores with respect to the ground-truth audio waveform and the textual prompt. We use the CLAP checkpoint from \cite{laionclap} trained on LAION-Audio-630k \cite{laionclap}, AudioSet \cite{audioset}, and music data.

We first ablate the performance of the consistency TTA generation model under various training settings, with the results presented in \Cref{tab:ablate}. Note that ``guided initialization'' refers to initializing the consistency model with a guidance-aware diffusion model, whereas ``unguided initialization'' refers to initializing with the unmodified TANGO teacher weights. \Cref{tab:ablate} demonstrates that distilling with fixed or variable guidance significantly improves the performance over direct or no guidance. In terms of the teacher solver used during distillation, with $N = 18$ discretization steps as in \cite{cm}, the more accurate Heun solver is advantageous over the simpler DDIM solver. Moreover, the uniform noise schedule is preferred over the Karras schedule (see \Cref{sec:solver_discuss} for a detailed discussion). We also observe that the Min-SNR weights and the guided initialization improve the FD and FAD but slightly sacrifice the KLD.

\Cref{tab:compare_obj} compares the consistency TTA models with the diffusion baseline models. On top of the best consistency model, we perform end-to-end CLAP fine-tuning, co-optimizing three loss components: the consistency loss \cref{eq:consis_risk}, $\CLAPA$, and $\CLAPT$. \Cref{tab:compare_obj} demonstrates that fine-tuning further improves all objective metrics except KLD. Furthermore, the gap between the best consistency and diffusion models is small for all quality metrics, with the FD and KLD even surpassing the reported numbers from \cite{tango} and \cite{audioldm}.

Note that the diffusion baseline models use 200 steps following \cite{audioldm, tango}, each step requiring two noise estimations due to CFG, amounting to 400 total network queries per generation. Thus, with minimal performance drop, the proposed consistency model reduces the number of U-Net queries by a factor of 400.


\subsection{Subjective evaluation results}

Finally, we conduct subjective evaluations in two aspects: overall audio quality and audio-text correspondence. For each subject, we use 25 generated audio clips from the same set of prompts together with those from ground-truth samples. We instructed 20 evaluators to rate the audio clips on a scale of 1 to 5 for each aspect. Other details can be found in \Cref{sec:human_details}. We further confirm that the consistency model produces audios close to those of the diffusion model in terms of subjective evaluation scores. Moreover, optimizing the CLAP scores improves the text-audio correspondence score, which supports our assumption that $\CLAPT$ provides closed-loop feedback to help align the generated audio with the prompt.


\subsection{Diversity of generated audios}

We also observe that different random seeds, i.e., different initial Gaussian latent for the consistency TTA model, generate noticeably different audio, confirming that consistency models produce diverse generations like diffusion models. We present two example prompts from the CLAP-finetuned model in \Cref{tab:diversity} to illustrate this diversity.



\section{Conclusion}

This work proposes an approach to accelerate diffusion-based TTA generation models hundreds of times based on consistency distillation. The delicate design of the distillation procedure emphasizing CFG achieves this vast acceleration with minimal generation quality reduction, enabling diverse and realistic in-the-wild audio generation within one neural network query. The differentiability of the resulting model allows for end-to-end fine-tuning, unlocking possibilities for further improving the training method of such models.


\newpage
\bibliographystyle{IEEEbib}
\bibliography{papers}


\appendix
\onecolumn
\newpage


\section{Additional discussions and details} \label{sec:exp_details}

\subsection{Additional discussions regarding teacher solver} \label{sec:solver_discuss}

\Cref{tab:ablate} presents the generation quality of the consistency model $\fstu$ distilled with various solver settings, confirming our selection of the Heun solver and the uniform schedule. While \cite{cm} agreed that the Heun solver achieved better results, the authors suggested using the Karras schedule. Our explanation of this discrepancy is that TANGO was trained using the uniform schedule, whereas the teacher models in \cite{cm} were trained with the Karras schedule. It is likely beneficial to use the same scheduler to train the diffusion model.

We also compared the TANGO inference quality when coupled with various solvers (fixing $N = 18$ inference steps), including DDPM, DDIM, Euler, Heun, and DPM++(2S), and confirmed that the Heun solver with a uniform schedule yielded the best FAD and FD metrics. DPM++(2S) achieved a better KLD, but the difference is negligible.


\subsection{Relationship to two-stage progressive distillation} \label{sec:twostage_details}

Unlike PD in \cite{distillcfg}, which requires iteratively halving the number of diffusion steps, CD in our method reduces the required inference step to one within a single training process. As a result, the two distillation stages proposed in \cite{distillcfg} can be merged. Specifically, stage-2 distillation can be performed without stage 1, provided that the step of querying the stage-1 model is replaced by querying the original teacher model with CFG. Merging stage 1 and stage 2 then results in our ``variable guidance distillation'' method discussed in \Cref{sec:distill_cfg}. Subsequently, stage 1 becomes optional since it only serves to provide a guidance-aware initialization to stage 2.


\subsection{Model and training details} \label{sec:model_details}

We noticed that the audio resampling implementation has a major influence on some metrics, with FAD being especially sensitive. To ensure high training quality and fair evaluation, we use ResamPy \cite{resampy} for all resampling procedures unless the resampling step needs to be differentiable. Specifically, CLAP fine-tuning requires differentiable resampling, and we use TorchAudio \cite{torchaudio} instead.

The structure of our 557M-parameter U-Net is similar to the 866M U-Net used in \cite{tango}, with the only modification being reducing the ``block out channels'' from $(320, 640, 1280, 1280)$ to $(256, 512, 1024, 1024)$. All CD runs use two 48GB-VRAM GPUs, with a total batch size of 12 and five gradient accumulation steps. The optimizer is AdamW with a $10^{-4}$ weight decay, and the learning rate is $10^{-5}$ for CD and $10^{-6}$ for CLAP fine-tuning. The ``CD target network'' (see \cite{cm} for details) is an exponential model average (EMA) copy with a $0.95$ decay rate. We also maintain an EMA copy with a $0.999$ decay rate for the reported experiment results. All training uses BF16 numerical precision.

Regarding the distance measure $d (\cdot, \cdot)$ introduced in \cref{eq:consis_risk}, the authors of \cite{cm} considered several options for $d (\cdot, \cdot)$ for image generation tasks and concluded that using LPIPS (an evaluation metric that embeds the generated image and calculates the weighted distance in several feature spaces) as the optimization objective produced higher generation quality than using the pixel-level $\ell_2$ or $\ell_1$ distance. However, since our latent diffusion model already operates in a latent feature space, we simply use the $\ell_2$ distance in this latent space.


\subsection{Evaluation details} \label{sec:human_details}

While the maximal audio length of the AudioCaps dataset is 10.00 seconds and the U-Net module of the TTA models is trained to generate 10.00-second latent audio representations, the HiFi-GAN vocoder produces 10.24-second audio. We observe that this mismatch negatively impacts the generation quality. Specifically, the final 0.24 seconds of the generated audio is empty, and there are slight distorting artifacts near the end of the 10-second useful portion. To this end, for the objective evaluation results in Tables \ref{tab:compare_obj} and \ref{tab:human_eval}, we truncate the generated audio to 9.70 seconds. \Cref{tab:ablate} uses the full 10.24 seconds. The ground-truth reference waveforms are not truncated.

The human evaluation results in \Cref{tab:human_eval} are based on 20 evaluators each rating 25 audio clips per model, forming 500 samples per model. For each evaluator, the three models and the ground truth use the same set of prompts (the prompts vary across evaluators). Each evaluator rates each audio on a scale of 1-5, with rating criteria defined in the evaluation form. To ensure evaluation fairness, the model type generating each waveform is not disclosed to the evaluator, and the generations of the models are shuffled. We find it extremely challenging for a human to distinguish the outputs from the three generative models, with many ground truth waveforms also indistinguishable. An example evaluation form is available at \href{https://bai-yt.github.io/consistency_tta/evaluation.html}{\texttt{bai-yt.github.io/consistency\_tta/evaluation}}.



\section{Acknowledgments}

We sincerely appreciate the contributions to human evaluation from Chih-Yu Lai, Mo Zhou, Afrina Tabassum, You Zhang, Sara Abdali, Uros Batricevic, Yinheng Li, Asing Chang, Rogerio Bonatti, Sam Pfrommer, Ziye Ma, Tanvir Mahmud, Eli Brock, Tanmay Gautam, Jingqi Li, Brendon Anderson, Hyunin Lee, and Saeed Amizadeh.



\end{document}
