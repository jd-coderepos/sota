\documentclass{article}


\PassOptionsToPackage{numbers, compress}{natbib}






\usepackage[preprint]{neurips_2023}








\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         

\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{color, colortbl}
\usepackage{subcaption}
\newcommand{\TODO}[1]{\textcolor{red}{(TODO: #1)}}
\definecolor{Gray}{gray}{0.9}
\definecolor{Red}{RGB}{230, 57, 70}
\title{Boosting Human-Object Interaction Detection with Text-to-Image Diffusion Model}





\author{
  Jie Yang\textsuperscript{\rm 1,2}\thanks{co-first author}~~\thanks{This work was done when Jie Yang was intern at IDEA.}~~Bingliang Li\textsuperscript{\rm 1}~Fengyu Yang\textsuperscript{\rm 1}~Ailing Zeng\textsuperscript{\rm 2}\thanks{Corresponding author}~~ Lei Zhang\textsuperscript{\rm 2}~Ruimao Zhang \textsuperscript{\rm 1} \quad \\ 
  \textsuperscript{\rm 1}Shenzhen Research Institute of Big Data, The Chinese University of Hong Kong, Shenzhen  \quad \\ 
  \textsuperscript{\rm 2}International Digital Economy Academy (IDEA)
}




\begin{document}


\maketitle


\begin{abstract}
































This paper investigates the problem of the current HOI detection methods and introduces \texttt{DiffHOI}, a novel HOI detection scheme grounded on a pre-trained text-image diffusion model, which enhances the detector's performance via improved data diversity and HOI representation. 
We demonstrate that the internal representation space of a frozen text-to-image diffusion model is highly relevant to verb concepts and their corresponding context.
Accordingly, we propose an adapter-style tuning method to extract the various semantic associated representation from a frozen diffusion model and CLIP model to enhance the human and object representations from the pre-trained detector, further reducing the ambiguity in interaction prediction. 
Moreover, to fill in the gaps of HOI datasets, we propose \texttt{SynHOI}, a class-balance, large-scale, and high-diversity synthetic dataset containing over  HOI images with fully triplet annotations. It is built using an automatic and scalable pipeline designed to scale up the generation of diverse and high-precision HOI-annotated data. 
\texttt{SynHOI} could effectively relieve the long-tail issue in existing datasets and facilitate learning interaction representations. 
Extensive experiments demonstrate that \texttt{DiffHOI} significantly outperforms the state-of-the-art in regular detection (\textit{i.e.},  mAP) and zero-shot detection. Furthermore, \texttt{SynHOI} can improve the performance of
model-agnostic and backbone-agnostic HOI detection, particularly exhibiting
outstanding an  mAP improvement in rare classes.
 

\end{abstract}

\section{Introduction}

Human-Object Interaction Detection (HOI), a core element in human-centric vision perception tasks such as human activity recognition~\cite{heilbron2015activitynet}, motion tracking~\cite{yi2022physical}, and anomaly behavior detection~\cite{liu2018future}, has attracted considerable attention over the past decades. 
HOI primarily involves localizing correlated human-object 2D positions within an image and identifying their interactions.
Despite numerous models proposed in recent years~\cite{hou2020visual,wan2019pose,liao2022gen,wu2022mining,zhang2021mining,zhou2022human}, practical implementation remains challenging due to the inherent ambiguity of verbs and subtle distinctions among interaction categories.

















\begin{figure*}[t]
\includegraphics[width=1\linewidth]{figures/f1.pdf}
        \vspace{-0.5cm}
    \caption{We show a) the long-tail distribution issue in HICO-DET and b) the high correlation between HOI text (\textit{i.e.}, nouns and verbs) and internal image features within the frozen stable diffusion. }
    \label{fig:intro}
    \vspace{-0.6cm}
\end{figure*}

We investigate the problems of the current methods from both data and model aspects.
From the data perspective, current datasets characterize an HOI instance as a ⟨\textit{human, object, interaction}⟩ triplet, and complete with comprehensive annotations, \textit{i.e.}, human and object bounding boxes and respective interaction types. 
We carefully analyze the widely-used HICO-DET dataset~\cite{chao:wacv2018}, which encompasses  training images with  distinct types of real-world HOI triplets, and have identified its several limitations that hinder the effective learning of interaction representations:
\textbf{(1) Class imbalance.} As demonstrated in Fig.~\ref{fig:intro}-(a), we observe it has a significant long-tail distribution in the HOI triplet types. This imbalanced label distribution is not conducive to learning, particularly when identifying interaction categories with only subtle variances. 
\textbf{(2) Small data size.} Upon further investigation, we discover that among its  interaction categories,  categories have only a single image available, and  categories have less than  images. Such limited data severely impacts the learning of these rare categories.
\textbf{(3) Limited diversity.} Compared with existing datasets, real-world scenarios typically exhibit more diversity due to the complex variation in human appearance attributes, environmental backgrounds, and shooting perspectives. This diversity causes a considerable decline in the performance of existing methods when the scene content changes.


From the model standpoint, the majority of existing HOI detectors, regardless of whether they employ two-stage~\cite{chao2018learning,gao2018ican,gupta2019no,hou2020visual,wan2019pose} or one-stage~\cite{liao2020ppdm,wang2020learning,kim2020uniondet,liao2022gen,wu2022mining,zhang2021mining,zhou2022human} strategies, are built upon pre-trained object detectors to enhance the initial localization of humans and objects. 
However, it remains challenging to carry out effective interaction prediction solely based on representations of humans and objects, sometimes with their spatial relationship. 
Specifically, verb concepts and their nuanced contextual information, including human posture, orientation, attention area and overall circumstances, can critically influence prediction.
Extracting this semantic information from data is typically inefficient and cumbersome, significantly limiting existing methods' performance.













Recently, text-to-image diffusion models~\cite{balaji2022ediffi,zhou2022towards,saharia2022photorealistic,ramesh2022hierarchical,rombach2022high} trained on massive internet-scale data have achieved significant performance in conditioned image generation, which provides high-quality, versatile, and semantically controllable image generation. 
Such an advantage offers us an effective means to generate rich and diverse realistic HOI images.
In practice, these models utilize cross-attention mechanisms between text embeddings and visual representations, signifying a substantial correlation between their feature spaces and semantic concepts in language. 
Inspired by this, we use DAAM~\cite{tang2022daam} to generate pixel-level linguistic concept heatmaps based on the image features of this diffusion model. 
As depicted in Fig.~\ref{fig:intro}-(b), beyond the noun concepts highlighted in prior studies~\cite{xu2023open,li2023guiding,shiparddiversity}, we find that the internal representation space of a frozen text-to-image diffusion model is highly relevant to verb concepts and their associated contexts.  
However, the challenge of extracting these verb-associated representations from the diffusion model for downstream HOI task still exist.


To address the aforementioned issues, we introduce \texttt{DiffHOI}, a novel HOI detection scheme based on text-to-image diffusion models (e.g., Stable Diffusion). For the first time, \texttt{DiffHOI} tries to leverage the generative and representative capabilities to benefit the HOI task, \textit{i.e.}, the extracted powerful verb-associated contextual representations of SD. \texttt{DiffHOI} consists of three components: the pre-trained human-object detector, interaction decoder, and object-interaction classifiers. We introduce an adapter-style tuning approach to align global and local semantic associated representations from the SD and CLIP model in the interaction decoder. These representations serve a critical role in comprehending the nuanced disparities in interactions and reducing interaction prediction ambiguity.












To fill in the shortcomings of existing long-tail HOI datasets, we present \texttt{SynHOI}, a class-balance, large-scale, and high-diversity synthetic HOI dataset with over  fully annotated HOI images, 
which can effectively facilitate learning interaction representations. To make the flow of dataset production scalable, we present an automatic pipeline, including the HOIPrompt design, automatic labeling and filtering, and quality verification, designed to continually scale up the generation of diverse and high-precision HOI-annotated data. 
Therefore, our contributions can be summarized as follows:
(1) We introduce a novel scheme, \texttt{DiffHOI}, which leverages both the generative and representation capacities of pre-trained text-to-image diffusion models to enhance the performance of HOI detection tasks.
(2) We present an automatic and scalable pipeline to generate realistic annotated HOI images characterized by varied attributes and scene contexts and propose a class-balance, large-scale, and high-diversity synthetic HOI dataset, namely \texttt{SynHOI}.
(3) Extensive experimental results demonstrate the proposed adapter-style tuning, together with the proposed SynHOI dataset, significantly improves the performance of HOI tasks under the regular and zero-shot settings and achieves the new state-of-the-art, \textit{i.e.},  mAP on HICO-DET.











 \section{Related Work}
\textbf{HOI Detection.}
HOI detection task primarily encompasses three sub-problems, \textit{i.e.}, object detection, human-object pairing, and interaction recognition.
Previous HOI detectors can generally be divided into one-stage and two-stage paradigms.
The two-stage strategy employs an off-the-shelf detector to determine the locations and classes of objects, followed by specially-designed modules for human-object association and interaction recognition.
Most methods are dedicated to exploring additional feature streams to improve interaction classification, such as the appearance stream ~\cite{gao2018ican,li2019transferable,kim2020uniondet,hou2021detecting}, spatial stream~\cite{xu2019learning,bansal2020detecting,li2020detailed}, pose and body-parts stream~\cite{gupta2019no,wan2019pose,li2020detailed}, semantic stream~\cite{liu2020consnet,bansal2020detecting,gao2020drg}, and graph network~\cite{qi2018learning,xu2019learning,wang2020contextual,zhang2021spatially,park2023viplo}.
Instead, the one-stage strategy detects HOI triplets in a single forward pass by assigning human and object proposals to predefined anchors and then estimating potential interactions~\cite{liao2020ppdm, wang2020learning,kim2020uniondet,fang2021dirv}. 
Recently, the DETR-based HOI detectors~\cite{Tamura_2021_CVPR,tamura2021qpic,kim2021hotr,chen2021qahoi,ma2023fgahoi} have gained prominence in this paradigm, and they
formulate the HOI detection task as a set prediction problem, avoiding complex post-processing. In particular, many methods~\cite{zhang2021mining,liao2022gen,zhou2022human,chen2021reformulating} demonstrate promising performance improvements by disentangling
human-object detection and interaction classification as two decoders in a cascade manner. Our work builds on the top of the transformer-based HOI detection strategy and focuses on enhancing the learning of a dedicated interaction decoder.








\textbf{Zero-shot HOI Detection.}
Zero-shot HOI detection has emerged as a field aiming to identify unseen HOI triplet categories not present in the training data.
Previous research~\cite{bansal2020detecting,peyre2019detecting,peyre2019detecting,gupta2019no,hou2020visual,hou2021affordance,hou2021detecting,liu2020consnet} has addressed this task in a compositional manner, by disentangling the reasoning process on actions and objects during training. This approach enables the recognition of unseen HOI triplets during inference.
With the advancements in Vision-Language Models, such as CLIP~\cite{radford2021learning}, recent research~\cite{liao2022gen,wu2022end,ning2023hoiclip} has shifted focus toward transferring knowledge from CLIP to recognize unseen HOI concepts. This shift has resulted in a notable performance improvement in zero-shot settings. In this work, we aim to further explore the potential benefits of the Text-to-Image Diffusion model~\cite{rombach2022high} in enhancing zero-shot HOI detection.



\textbf{Diffusion Models for Image Generation and Vision Perception.}
Text-to-image diffusion models, such as DALLE-2~\cite{ramesh2022hierarchical}, Imagen~\cite{saharia2022photorealistic}, and Stable Diffusion~\cite{rombach2022high}, have shown considerable potentials in generating photorealistic images from free-form text prompts. This capability is attributed to the strong semantic correspondence between visual and language elements learned from a vast corpus of image-caption pairs. Recent research has proposed utilizing diffusion models to augment real datasets, assisting the training of downstream tasks~\cite{shiparddiversity,trabucco2023effective,bansal2023leaving,you2023diffusion}. Our work concentrates on generating synthetic HOI data to enhance HOI detection performance.
Moreover, text-to-image diffusion models pretrained on large-scale image-text pairs offer a high degree of control through customizable prompts. This aspect suits them for downstream tasks~\cite{xu2023open,li2023guiding}, where the additional feature representations from pretraining can bolster performance. 
This work first explores using a frozen text-image diffusion model for HOI detection.







 \input{sec/Method}
\input{sec/Experiment}
\section{Conclusion}
We investigate the issues of the current HOI detection methods and present a novel scheme, namely \texttt{DiffHOI}, which leverages both the generative and representation capacities of a pre-trained text-to-image diffusion
model to benefit the HOI detection task. Particularly, we release a class-balance, large-scale, and high-diversity synthetic HOI dataset called \texttt{SynHOI} to address the long-tail issue in previous datasets and develop an automatic and scalable pipeline to
scale up the generation of diverse and high-precision HOI-annotated data. Extensive experimental results demonstrate that our method significantly outperforms the prior state-of-the-art in regular and zero-shot detection tasks. We hope this work could inspire further research in related fields.
 



\bibliography{neurips_2023}
\bibliographystyle{unsrtnat}


\end{document}