\section{Experiments}

\begin{table*}[htb]
\centering
\scalebox{0.77}{
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{\bf Model} & \multicolumn{4}{c}{\bf Dev} & \multicolumn{2}{c}{\bf Test}  \\ 
\cmidrule(lr){2-5} \cmidrule(lr){6-7}
~ & \bf Ign F1 & \bf F1 & \bf Intra F1 & \bf Inter F1 & \bf Ign F1 & \bf F1 \\
\midrule
LSR-BERT$_{\text{base}}$ \citep{LSR} & 52.43 & 59.00 & 65.26 & 52.05 & 56.97 & 59.05 \\
GLRE-BERT$_{\text{base}}$ \citep{GLRE} & - & - & - & - & 55.40 & 57.40 \\
Reconstruct-BERT$_{\text{base}}$ \citep{Reconstruct} & 58.13 & 60.18 & - & - & 57.12 & 59.45 \\ 
GAIN-BERT$_{\text{base}}$ \citep{GAIN} & 59.14 & 61.22 & 67.10 & 53.90 & 59.00 & 61.24 \\ 
\midrule
BERT$_{\text{base}}$ \citep{finetune-bert} & - & 54.16 & 61.61 & 47.15 & - & 53.20 \\
BERT-Two-Step \citep{finetune-bert} & - & 54.42 & 61.80 & 47.28 & - & 53.92 \\
HIN-BERT$_{\text{base}}$ \citep{HIN} & 54.29 & 56.31 & - & - & 53.70 & 55.60 \\
E2GRE-BERT$_{\text{base}}$ \citep{E2GRE} & 55.22 & 58.72 & - & - & - & - \\
CorefBERT$_{\text{base}}$ \citep{CorefBERT} & 55.32 & 57.51 & - & - & 54.54 & 56.96 \\
ATLOP-BERT$_{\text{base}}$ \citep{ATLOP} & 59.11 $\pm$ 0.14$^\dagger$ & 61.01 $\pm$ 0.10$^\dagger$ & 67.26 $\pm$ 0.15$^\dagger$ & 53.20 $\pm$ 0.19$^\dagger$ & 59.31 & 61.30 \\
\midrule
\textbf{\ours(Rule)-BERT$_{\text{base}}$} & 60.36 $\pm$ 0.13 & 62.34 $\pm$ 0.08 & 68.40 $\pm$ 0.14 & 54.79 $\pm$ 0.13 & 60.23 & 62.21 \\
\textbf{\ours-BERT$_{\text{base}}$} & \textbf{60.51 $\pm$ 0.11} & \textbf{62.48 $\pm$ 0.13} & \textbf{68.47 $\pm$ 0.08} & \textbf{55.21 $\pm$ 0.21} & \textbf{60.42} & \textbf{62.47} \\
\midrule
\midrule
RoBERTa$_{\text{large}}$~\cite{CorefBERT}& 57.14 & 59.22 & - & - & 57.51 & 59.62 \\
CorefRoBERTa$_{\text{large}}$~\cite{CorefBERT}& 57.35 & 59.43 & - & - & 57.90 & 60.25\\
E2GRE-RoBERTa$_{\text{large}}$~\cite{E2GRE}& 59.55 & 62.91 & - & - & 60.29 & 62.51 \\
GAIN-BERT$_{\text{large}}$~\cite{GAIN}& 60.87 & 63.09 & - & - & 60.31 & 62.76\\
ATLOP-RoBERTa$_{\text{large}}$~\cite{ATLOP}& 61.30 $\pm$ 0.22$^\dagger$ & 63.15 $\pm$ 0.21$^\dagger$ & 69.61 $\pm$ 0.25$^\dagger$ & 55.01 $\pm$ 0.18$^\dagger$ & 61.39 & 63.40 \\
\midrule
\textbf{\ours(Rule)-RoBERTa$_{\text{large}}$} & 61.73 $\pm$ 0.07 & 63.91 $\pm$ 0.07 & 69.99 $\pm$ 0.09 & 56.27 $\pm$ 0.11 & 61.93 & 64.12 \\
\textbf{\ours-RoBERTa$_{\text{large}}$} & \textbf{62.34 $\pm$ 0.14} & \textbf{64.27 $\pm$ 0.10} & \textbf{70.36 $\pm$ 0.07} & \textbf{56.53 $\pm$ 0.15} & \textbf{62.85} & \textbf{64.79} \\
\bottomrule
\end{tabular}
}
\upv
\caption{Relation extraction results on DocRED.
We report the mean and standard deviation on the development set by conducting 5 runs with different random seeds.
We report the official test score of the best checkpoint on the development set.
Results with $\dagger$ are based on our implementation. Others are reported in their original papers. 
We separate graph-based and transformer-based methods into two groups.
}
\label{table:results}
\downv
\end{table*}

\begin{table}[htb]
\centering
\scalebox{0.67}{  
    \begin{tabular}{lcc}
         \toprule
         \bf Model & \bf CDR & \bf GDA \\
         \midrule
         LSR-BERT$_{\text{base}}$~\cite{LSR} & 64.8 & 82.2\\
         SciBERT$_{\text{base}}$~\cite{ATLOP} & 65.1 $\pm$ 0.6 & 82.5 $\pm$ 0.3 \\
         DHG-BERT$_{\text{base}}$~\cite{DHG} & 65.9 & 83.1 \\
         GLRE-SciBERT$_{\text{base}}$~\cite{GLRE}& 68.5 & - \\
         ATLOP-SciBERT$_{\text{base}}$~\cite{ATLOP}& 69.4 $\pm$ 1.1 & 83.9 $\pm$ 0.2 \\
         \midrule
         \textbf{\ours (Rule)-SciBERT$_{\text{base}}$}& \textbf{70.63} $\pm$ 0.49 & \textbf{84.54} $\pm$ 0.22 \\
         \bottomrule
    \end{tabular}}
    \upv
    \caption{Relation extraction results on CDR and GDA.}
    \label{tab::bio}
    \downv
\end{table}

\subsection{Experiment Setup}
\label{sec:setup}
\start{Datasets}
We evaluate the effectiveness of \ours on three datasets: DocRED~\cite{DocRED-paper}, CDR~\cite{CDR} and GDA~\cite{GDA}, where DocRED is the only dataset that provides evidence labels as part of the annotation. The details of the datasets are listed in Appendix~\ref{app:data}.

\smallskip
\start{Implementation Details}
Our model is implemented based on PyTorch and Huggingface's Transformers \cite{wolf2019huggingface}. We use cased-BERT$_{\text{base}}$ \cite{BERT} and RoBERTa$_{\text{large}}$ as the base encoders and optimize our model using AdamW with learning rate 5e-5 for the encoder and $1e-4$ for other parameters. We adopt a linear warmup for the first 6\% steps. The batch size (number of documents per batch) is set to 4 and the ratio between relation extraction and evidence extraction losses is set to 0.1. We perform early stopping based on the F1 score on the development set, with a maximum of 30 epochs. Our BERT$_{\text{base}}$ models are trained with one GTX 1080 Ti GPU and RoBERTa$_{\text{large}}$ models with one RTX A6000 GPU.

\smallskip
\start{Evaluation Metrics}
Following prior studies~\cite{DocRED-paper}, we use \textbf{F1} and \textbf{Ign F1} as the main evaluation metrics for relation extraction, where \textbf{Ign F1} measures the F1 score excluding the relations shared by the training and development/test set.
We also report \textbf{Intra F1} and \textbf{Inter F1}, where the former measures the performance on the co-occurred (intra-sentence) entity pairs and the latter evaluates the inter-sentence entity pairs where none of their proper-noun mentions co-occurs.
For evidence extraction, we compute the F1 score (denoted as \textbf{Evi F1}) and further introduce \textbf{PosEvi F1}, which measures the F1 score of evidence only on positive entity pairs (i.e., those with non-NA relations).

\subsection{Main Results}
We compare our methods with both \emph{Graph-based methods} and \emph{transformer-based methods}. Graph-based methods explicitly perform inference on document-level graphs. Transformer-based methods, including \ours, implicitly capture the long-distance token dependencies via transformers.
Noted that \ours is trained on gold labels and leverages the evidence extracted by our model in inference. \ours (Rule) is trained on silver evidence labels constructed by rules and also leverages them in inference. 

\smallskip
\start{Relation Extraction Results}
Tables~\ref{table:results} and \ref{tab::bio} show that \ours outperforms the DocRE baseline methods in all datasets.
Our improvement is especially large on Inter F1 (e.g., 1.21/2.01 Intra/Inter F1 compared to ATLOP-BERT$_{\text{base}}$).
We hypothesize that the bottleneck of inter-sentence pairs is to locate the relevant context, which often spreads through the whole document. \ours learns to capture important sentences in training and focuses more on these important sentences in inference.

Among the baselines, the Inter F1 of GAIN is 0.70 higher than ATLOP while the Intra F1 of ATLOP is 0.16 higher than GAIN, indicating that document-level graphs may be effective in multi-hop reasoning.
Although \ours does not involve explicit multi-hop reasoning modules, it still notably outperforms graph-based models in Inter F1.

Finally, \ours (Rule) also outperforms all the baselines in both DocRED and the two biomedical datasets which do not have evidence annotation.
The improvement on DocRED and CDR is much larger than that on GDA. We hypothesize that it is because more than 85\% relations in GDA are intra-sentence ones, making it trivial even for the single RE model to focus on these sentences.



\begin{table}[t]
\centering
\scalebox{0.75}{
    \begin{tabular}{lccc}
         \toprule
         \textbf{Model} & \bf Dev Evi F1 & \bf Test Evi F1  \\
         \midrule
         E2GRE-BERT$_{\text{base}}$ & 47.14 & 48.35 \\
         \textbf{\ours-BERT$_{\text{base}}$} & \textbf{50.71} & \textbf{51.27} \\
         \midrule
         E2GRE-RoBERTa$_{\text{large}}$ & 51.11 & 50.50 \\
         \textbf{\ours-RoBERTa$_{\text{large}}$} & \textbf{52.54} & \textbf{53.01} \\
         \bottomrule
    \end{tabular}}
    \upv
    \caption{Evidence extraction results on DocRED. We compare \ours with E2GRE \citep{E2GRE}.}
    \label{tab:evidence}
    \downv
\end{table}

\smallskip
\start{Evidence Extraction Results} 
To our knowledge, E2GRE is the only method that has reported their evidence extraction result.
The results in Table~\ref{tab:evidence} indicate that \ours outperforms E2GRE significantly (e.g., by 3.57 Dev Evi F1 under BERT$_{\text{base}}$).
The results show that it may be sufficient to train the evidence classifier only on pairs with $r\in\mathcal{R}$ and over each (entity, entity, sentence) tuple instead of (entity, entity, sentence, relation) as in E2GRE.

Our ablation studies in Table~\ref{tab:evi_ablation} show that our three heuristic rules, denoted as \textbf{Rules (ours)}, already capture most of the evidence for positive entity pairs. The high quality of silver labels explains why our model can perform well using silver labels only.
Furthermore, training the RE model and evidence extraction model separately (denoted as \textbf{NoJoint}) results in a sharp performance drop. 
As the relation and evidence classifiers share the same base encoder, discarding the relation classifier will result in insufficient training of the base encoder and harm the performance.

\begin{table}[t]
\centering
\scalebox{0.75}{
    \begin{tabular}{lccc}
         \toprule
           & Rules (ours) & \ours-BERT$_{\text{base}}$ &  NoJoint \\
         \midrule
          \bf PosEvi F1 & 77.43 & \textbf{80.33} & 51.13 \\
         \bottomrule
    \end{tabular}}
    \upv
    \caption{Ablation study for evidence extraction.}
    \label{tab:evi_ablation}
    \downv
\end{table}

\subsection{Performance Analysis}



\start{Ablation Study} Table \ref{tab:ablation} shows the ablation studies that analyzes the utility of each module in \ours.
We observe that \textbf{NoJoint} leads to sharp performance drop in DocRE.
Besides, \textbf{\ours (Rule)-Nojoint} achieves significant ``free gains'' (0.90/1.08 Ign F1/F1) by simply fusing the evidence constructed by rules in the inference of ATLOP. In principle, this inference process can be applied to general DocRE models.



\begin{table}[!t]
\centering
\scalebox{0.72}{
    \begin{tabular}{lcccc}
         \toprule
         \textbf{Ablation} & \bf Ign F1 & \bf F1 & \bf Intra F1 & \bf Inter F1 \\
         \midrule
         \ours-BERT$_{\text{base}}$ & \textbf{60.51} & \textbf{62.48} & \textbf{68.47} & \textbf{55.21} \\
            \quad NoJoint & 59.98 & 62.03 & 68.51 & 54.10 \\
            \quad NoPseudo & 59.70 & 61.53 & 67.55 & 54.01 \\
            \quad NoOrigDoc & 58.47 & 60.44 & 66.24 & 53.23 \\
            \quad NoBlending & 58.93 & 61.46 & 67.33 & 54.37 \\
            \quad FinetuneOnEvi & 60.11 & 62.29 & 68.13 & 54.84 \\
        \midrule
        \ours(Rule)-BERT$_{\text{base}}$ & \textbf{60.36} & \textbf{62.34} & \textbf{68.40} & \textbf{54.79} \\
            \quad NoJoint & 60.01 & 62.09 & 68.21 & 54.34 \\
         \bottomrule
    \end{tabular}}
    \upv
    \caption{Ablation study of \ours on DocRED.}
    \label{tab:ablation}
    \downv
\end{table}

We also remove the pseudo document (constructed from the extracted evidence) and the original document separately, denoted as \textbf{NoPseudo} and \textbf{NoOrigDoc}, respectively.
We observe that removing either source will lead to performance drops. Also, the drop of Inter F1 is much larger than Intra F1 for \textbf{NoPseudo},
indicating that our inference process is effective for inter-sentence pairs where the evidence may not be consecutive.

As for \textbf{NoBlending}, we remove the blending layer and simply take the union of the two sets of results.
The sharp drop of performance indicates
the blending layer can successfully learn a dynamic threshold to combine the prediction results.

Finally, we further finetune the RE model on ground truth evidence before feeding it the extracted evidence (denoted as \textbf{FinetuneOnEvi}) but the performance is not improved, probably because the encoded entity representations in evidence and original documents are already highly similar.

\begin{table}[!t]
\centering
\scalebox{0.8}{
    \begin{tabular}{lcccc}
         \toprule
          & \bf Co-occur & \bf Coref & \bf Bridge & \bf Total \\
         \midrule
          Count & 6711 & 984 & 3212 & 10,907 \\ Percent & 54.46\% & 7.99\% & 26.07\% & 88.52\% \\ \bottomrule
    \end{tabular}}
    \upv
    \caption{Statistics of the 12,323 relations in the DocRED development set.}
    \label{tab:breakdown}
    \downv
\end{table}





\begin{figure}[t]
\centering
\scalebox{0.95}{
    \includegraphics[width=\linewidth]{figures/breakdown_169.pdf}
}
    \upv
    \caption{Performance gains in F1 by relation categories. The gains are relative to the second best baseline (ATLOP-RoBERTa$_{\text{large}}$).}
    \label{fig:breakdown}
    \downv
    \vspace{-0.17cm}
\end{figure}

\begin{table}[b]
\centering
\scalebox{0.78}{
    \begin{tabular}{lcc}
         \toprule
         \textbf{Model} & \bf Memory & \bf Training time \\
         \midrule
            ATLOP-BERT$_{\text{base}}$ & 9,139 MB & 5.19 it/s \\
            E2GRE-BERT$_{\text{base}}$ & 36,182 MB & 0.53 it/s \\
            \ours-BERT$_{\text{base}}$ & 10,933 MB & 4.92 it/s \\
         \bottomrule
    \end{tabular}}
    \upv
    \caption{Training time and memory usage on DocRED.}
    \label{tab:efficiency}
    \downv
\end{table}

\begin{table*}[ht]
\centering
        \resizebox{2.08\columnwidth}{!}{
        \begin{tabular}{p{20cm}}
            \toprule
            \textbf{Ground Truth Relation: \colorG{Located in}} 
            \quad
            \textbf{Ground Truth Evidence Sentence(s)}: [1, 2]
            \quad
            \textbf{Extracted Evidence Sentence(s)}: \colorbox{LightYellow}{[1, 2]}
            \\
             \textbf{Document}: \colorbox{LightYellow}{[1]} The \colorR{Portland Golf Club} is a private golf club in the northwest United States , in suburban Portland, Oregon.
            \colorbox{LightYellow}{[2]} It is located in the unincorporated Raleigh Hills area of eastern \colorB{Washington County}, southwest of downtown Portland and east of Beaverton.
            [3] The club was established in the winter of 1914, when a group of nine businessmen assembled to form a new club after leaving their respective clubs \textbf{...}
             \\ 
             \textbf{Final Prediction}: \colorG{Located in} 
             \quad\quad
             \textbf{Prediction on Orig. Doc}: \colorG{Located in} 
             \quad\quad
             \textbf{Prediction on Extracted Evidences}: \colorG{Located in}
             \\
             
            \midrule
            
            \textbf{Ground Truth Relation: \colorG{Characters}}
            \hskip0.5em
            \textbf{Ground Truth Evidence Sentence(s)}: [1, 3]
            \quad
            \textbf{Extracted Evidence Sentence(s)}: \colorbox{LightYellow}{[1, 3]} 
            \\
             \textbf{Document}: \colorbox{LightYellow}{[1]} King Louie is a fictional character introduced in Walt Disney's 1967 animated musical film, \colorR{The Jungle Book}. [2] Unlike the majority of the adapted characters in the film, Louie was not featured in Rudyard Kipling's original works. \colorbox{LightYellow}{[3]} King Louie was portrayed as an orangutan who was the leader of the other jungle primates, and who attempted to gain knowledge of fire from \colorB{Mowgli}, \textbf{...}
             \\ 
             \textbf{Final Prediction}: \colorG{Characters}
             \quad\quad
             \textbf{Prediction on Orig. Doc}: NA
             \quad\quad\quad\quad\quad
             \textbf{Prediction on Extracted Evidences}: \colorG{Characters}
             \\
             
            \midrule
            \textbf{Ground Truth Relation: \colorG{Inception}} 
            \quad
            \textbf{Ground Truth Evidence Sentence(s)}: [5, 6]
            \quad\quad
            \textbf{Extracted Evidence Sentence(s)}: \colorbox{LightYellow}{[5]}
            \\
             \textbf{Document}: [1] Oleg Tinkov (born 25 December 1967 ) is a Russian entrepreneur and cycling sponsor. \textbf{...} \colorbox{LightYellow}{[5]} Tinkoff is the founder and chairman of the \colorR{Tinkoff Bank} board of directors (until 2015 it was called Tinkoff Credit Systems). [6] The bank was founded in \colorB{2007} and as of December 1, 2016, it is ranked 45 in terms of assets and 33 for equity among Russian banks. \textbf{...}
             \\ 
             \textbf{Final Prediction}: \colorG{Inception}
             \quad\quad\quad
             \textbf{Prediction on Orig. Doc}: \colorG{Inception}
             \quad\quad\quad\quad\quad
             \textbf{Prediction on Extracted Evidences}: NA
             \\
             
            \bottomrule
        \end{tabular}
        }
    \upv
    \caption{Case studies of our proposed framework \ours. We use red, blue and green to color the \colorR{head entity}, \colorB{tail entity} and \colorG{relation}, respectively. The indices of \colorbox{LightYellow}{extracted evidence sentences}are highlighted with yellow.}
    \label{tab:case}
    \downv
    \end{table*}
    


\smallskip
\start{Performance Breakdown}
To further analyze the performance of \ours on different types of entity pairs, we categorize the relations into three categories based on our three heuristic rules in Sec.~\ref{sec:rule}: \textit{Co-occur}, \textit{Coref} and \textit{Bridge}.
The number and percentage of relations covered by each rule are listed in Table~\ref{tab:breakdown}. We can see that the three categories cover over 88\% of the relations in the development set.
The results on each category are shown in Figure~\ref{fig:breakdown}. We can see that our full model has the best performance in all three categories and our ablations also outperform ATLOP.
For all our methods, the improvements over ATLOP is \textit{Bridge} $>$ \textit{Coref} $\gg$ \textit{Co-occur}. This reveals that both modules mainly improve the model's reasoning ability from multiple sentences, either by coreference reasoning or by multi-hop reasoning over a third entity.



\smallskip
\start{Efficiency Comparison}
We benchmark the time and memory usage of \ours on an RTX A6000 GPU.
Table~\ref{tab:efficiency} shows that our joint model incurs only \textasciitilde5\% training time and \textasciitilde14\% GPU memory overhead.
Experiments also show that \ours can be trained on a single consumer GPU (e.g., an 11GB GTX 1080 Ti) but E2GRE is not able to.


\subsection{Case Studies}

Table~\ref{tab:case} shows a few examples of \ours. 
Detailed statistics and error analysis are provided in Appendix \ref{app:err_ana}.
In the first example, the head entity is mentioned in the first sentence and the tail entity appears in the second. 
We can see that \ours correctly extracts these sentences as evidence. 
Since the evidence sentences are consecutive, the predictions on both the original document and the evidence sentences are correct. 
In the second example, 
the prediction using only the original document is incorrect, possibly because the ``King Louie'' in the $1^{st}$ and $3^{rd}$ sentences are so far away from each other that the model fails to recognize them as coreference. Hence, it fails to distinguish ``King Louie'' as a bridge entity and wrongly predicts ``NA''.
Instead, these two sentences are consecutive in the extracted evidence, making it easier for the model to find the bridge. 
In the last example, the $6^{th}$ sentence is missing in the extracted evidence, 
so the extracted evidence does not contain enough information to predict the relation. However, the prediction on the original document is correct, leading to the correct final result.





