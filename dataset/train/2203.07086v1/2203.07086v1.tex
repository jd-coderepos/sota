In sections~\ref{ssec:clip} - \ref{ssec:non_square} all experiments are made on MSR-VTT full clean split (see Sec.~\ref{ssec:datasets}) for 50 epochs and 60k examples per epoch. The initial learning rate is 5e-5. After each epoch we multiply learning rate by . In these experiments we freeze text backbone and train only aggregator model and text embedding projection part. 

For training on MSR-VTT, we use aggregator with 4 layers and 4 heads. On larger dataset (see Sec.~\ref{ssec:adding_images} - \ref{ssec:final_result}) aggregator has 9 layers and 8 heads.

Results are reported as \textit{mean} or just \textit{mean} over 3 experiments.

\subsection{CLIP} \label{ssec:clip}
In~\cite{mdmmt} it is shown that CLIP works as a strong visual feature extractor and outperforms other available models by large margin. We found out that CLIP text backbone also works better than other available text models, such as BERT~\cite{bert}, which was originally used in~\cite{gabeur2020multimodal}, or GPT~\cite{gpt}.

Currently there are several publicly available CLIP models. In this section we compare their performance to make sure that we use the best possible combination. Results are presented in Tab.~\ref{tab:clip}.

Our observations:
\begin{itemize}
    \item Suppose we have pre-trained CLIP: text backbone and corresponding visual backbone. We observe that if we replace original visual backbone with
a bigger/deeper one, we obtain better video retrieval system.
    \item If we use the same visual backbone with different text backbones, a text backbone of a bigger/deeper model not necessarily shows better results. \\
    In fact, if we take a look at (Tab.~\ref{tab:clip}) RN50(xN) models, the best result is achieved by a combination of the deepest visual backbone (RN50x64) and the text backbone from the most shallow model (RN50).
    \item CLIP ViT-L/14 shows the best performance both as visual and text backbone.
    
\end{itemize}



\begin{table}
	\centering
	\caption{Comparison of CLIP visual and text backbones combinations. Experts: CLIP; Metric: R@5}
	\label{tab:clip}
\begin{tabular}{|c|ccccccc|}
         \toprule 
		 \diagbox{Visual}{\rotatebox{90}{Text}}&
\rotatebox{90}{RN50} & \rotatebox{90}{RN50x4} & \rotatebox{90}{RN50x16}  & \rotatebox{90}{RN50x64} & \rotatebox{90}{ViT-B/32} &  \rotatebox{90}{ViT-B/16} & \rotatebox{90}{ViT-L/14} \\
		\midrule
		 RN50			& 40.1 & 38.7 & 39.3 & 39.3 & 40.1 & 39.8 & 39.8  \\
		 RN50x4			& 42.8 & 41.9 & 42.5 & 42.5 & 43.2 & 43.1 & 43.2   \\
		 RN50x16		& 43.9 & 43.5 & 43.6 & 43.0 & 44.4 & 44.5 & 44.4   \\
	     RN50x64		& 44.6 & 43.9 & 44.1 & 44.2 & 44.8 & 45.2 & 45.4   \\
		 ViT-B/32		& 42.0 & 41.2 & 40.9 & 40.9 & 42.5 & 42.4 & 42.2   \\
		 ViT-B/16		& 44.4 & 43.8 & 43.4 & 43.3 & 44.8 & 45.4 & 44.9   \\
		 ViT-L/14		& 46.2 & 45.7 & 45.3 & 45.3 & 46.5 & 46.8 & \B{47.2}   \\
		\bottomrule
	\end{tabular}
\end{table}



\begin{table}
	\centering
	\caption{ Experiments on different audio experts. Text backbone: CLIP ViT-B/32; Experts: CLIP ViT-B/32, irCSN152-IG65M, audio}
	\label{tab:audio_experts}	

	\begin{tabular}{|c|llll|}
		\toprule
			Audio  & \multicolumn{4}{c|}{Text  Video} \\
			expert	  & R@1 & R@5 & R@10 & MdR \\
		\midrule
		vggish~\cite{hershey2017cnn}   & 19.3  & 44.3    & 56.3 & 7.0 \\
		Slow-Fast~\cite{kazakos2021slowfast} & 19.6  & 44.9    & 57.0 & 7.0 \\
		\bottomrule
	\end{tabular}		
\end{table}

\subsection{Experts combination} \label{ssec:experts_combination}
\begin{table}
	\centering
	\caption{Experts combinations. Text backbone: CLIP ViT-B/32}
	\label{tab:experts_combination}
	
	\begin{tabular}{|ccc|lll|}
		\toprule
		\multicolumn{3}{|c|}{Experts} & \multicolumn{3}{c|}{Text  Video} \\ 
		CLIP & irCSN152-IG65M & SF & R@1 & R@5 & MdR \\
		\midrule
		& \checkmark &                       
		& 10.2  & 29.3 & 17.3\\
	    & \checkmark & \checkmark                    
		& 11.2  & 31.5 & 15.0\\
		\checkmark  &  &                        
		& 21.3  & 46.5 & 7.0\\
		\checkmark    & \checkmark &            
		& 21.5  & 46.7 & 7.0\\
		\checkmark     &  & \checkmark          
		& 22.0  & 47.8 & 6.0\\
		\checkmark    & \checkmark & \checkmark 
		& \B{22.2}  & \B{48.5} & \B{6.0}\\
		\bottomrule
	\end{tabular}	
\end{table}
Using combination of different experts allows to achieve better performance. In Tab.~\ref{tab:experts_combination} various combinations of experts are presented. Using three modalities gives the best result.
\subsection{Dealing with non-square videos} \label{ssec:non_square}
\begin{table}
	\centering
	\caption{
 Comparison of different techniques for extracting features from non-square videos. Text backbone: CLIP ViT-B/32; Experts: CLIP ViT-L/14; Metric: R@5}
	\label{tab:crop}
	
	\begin{tabular}{|c|*{4}c|}
		\toprule
		 \diagbox{Train}{Test}&Squeeze &  Center crop & Padding  & Mean \\
		\midrule
		Squeeze     & 46.3 & 46.0 & 46.0 & \textbf{47.1} \\
		Center Crop & 46.0 & 46.5 & 46.0 & \textbf{47.3} \\
		Padding     & 46.0 & 46.2 & 46.7 & \textbf{47.0} \\  
	    Mean        & 45.9 & 46.4 & 45.9 & \textbf{47.4} \\                
		\bottomrule
	\end{tabular}
\end{table}
Both irCSN152-IG65M and CLIP take videos (images) of square shape as input. Therefore it is not possible to use information from the whole video directly. It may happen that some object or action is taking place in the corner (out of the center crop) of the video. So if we use center crop to compute embeddings, the information from the corners will be lost. There are several possible solutions to this problem:
\begin{itemize}
    \item Squeeze a video to a square without saving the aspect ratio (\textit{squeeze})
    \item Pad a video to a square with blackbars (\textit{padding})
    \item Take several crops from the video, average the embeddings of these crops, and use this average as embedding (\textit{mean}) 
\end{itemize}

For the \textit{mean} technique we take three crops: left or bottom, center, right or top (depending on video orientation) and then average embeddings of these crops.

Experiments in Tab.~\ref{tab:crop} show that
\textit{squeeze} works worse than center crop,
\textit{padding} works slightly better than center crop, and \textit{mean} works the best.

We want to emphasize that using \textit{mean} during test improves video-retrieval performance even if other methods were used during train.


\subsection{Adding images} \label{ssec:adding_images}
\begin{table}
      \centering
      \caption{Datasets used in train procedure. The "Weight" column describes how often we sample examples from the dataset. The probability of obtaining an example from the
      dataset with the weight  equals to  divided by a sum of all weights}
      \label{tab:dataset-wgh}
      \begin{tabular}{|c|c|c|}
	    \toprule
	    Dataset	    & Weight & Type \\
	    \midrule
	    MSR-VTT	        & 140 & \multirow{10}{*}{\shortstack{Text-video\\ datasets \\ (10V)}}\\
	    ActivityNet	    & 100 &\\
	    LSMDC	        & 70  &\\
	    Twitter Vines   & 60  &\\
	    YouCook2	    & 20  &\\
	    MSVD	        & 20  &\\
	    TGIF	        & 102 &\\
	    SomethingV2     & 169 &\\
	    VATEX           & 260 &\\
	    TVQA            & 150 &\\
	    \midrule
	    COCO            & 280 &\multirow{3}{*}{\shortstack{Text-image\\ datasets \\ (3I)}}\\
	    Flicker30k      & 200 &\\
	    Conceptual Captions    & 160 &\\
	    \bottomrule
      \end{tabular}
\end{table}

\begin{table}
  \centering
  \caption{ Test results on MSR-VTT full clean split. Text backbone: CLIP ViT-B/32; Experts: CLIP ViT-L/14, irCSN152-IG65M, SF}
  \label{tab:10v}
  
  \begin{tabular}{|c|*{4}c|}
		  \toprule
	       \multirow{2}{*}{Dataset}  & \multicolumn{4}{c|}{Text  Video} \\ 
		 & R@1 & R@5 & R@10 & MdR \\
		  \midrule
    10V     &  30.2    & 56.6 & 67.1  &  4.0    \\
    10V+3I  &  30.9    & 57.4 & 67.8  &  4.0     \\
		  \bottomrule
	    \end{tabular}
\end{table}
In \cite{mdmmt} it is shown that the proper combination of datasets allows to train a single model that can capture the knowledge from all used datasets and in most cases the 
model trained on the combination of datasets is better than the model trained on a single dataset. 

In Tab.~\ref{tab:10v} we show that proper combination of text-video and text-image datasets allows to improve video-retrieval performance. Hyperparameters are specified in Sec.~\ref{ssec:stages}, stage S.

Weights for combining all datasets are specified in Tab.~\ref{tab:dataset-wgh}. First 10 rows are video datasets (denoted as 10V) and last 3 are image datasets (denoted as 3I). 

\subsection{Pre-training and fine-tuning}\label{ssec:stages}
Note that in our work aggregator is initialised form scratch, while text backbone is pre-trained. If we simultaneously train randomly initialised aggregator and pre-trained text backbone, then at the time when aggregator will be trained, the text backbone might degrade. That is why for final result we introduce training procedure that consists of three stages (denoted as S, S, S).

During stage S we use noisy HT100M dataset. Text backbone is frozen, only aggregator and text embedding projection part are trained.

During stage S we use crowd-labeled datasets 10V+3I . Same as in S, text backbone is frozen, only aggregator and text embedding projection part are trained.

During stage S, same as in S, we use  crowd-labeled datasets 10V+3I. Now, however, we unfreeze text backbone and train all three main components: aggregator, text backbone and text embedding projection.

Hyperparameters for these stages are listed in Tab.~\ref{tab:stages_params}.
Results for different combinations of stages are listed in Tab.~\ref{tab:stages_results}.

\begin{table}
	\centering
	\caption{Hyperparameters for different stages}
	\label{tab:stages_params}
	
	\begin{tabular}{|c|*{4}c|c|}
		\toprule
		Train & Examples & Num. & Learning& \multirow{2}{*}{} & \multirow{2}{*}{Datasets}  \\
		stage&per epoch & epochs & rate && \\
		\midrule
		S & 60k & 200 & 5e-5 & 0.98 & HT100M \\
		S & 380k & 45 & 5e-5 & 0.95 & 10V+3I \\
		S & 200k & 20 & 2e-5 & 0.8  & 10V+3I  \\
		\bottomrule
	\end{tabular}	
\end{table}

\begin{table}
	\centering
	\caption{ Test results for train stages on MSR-VTT full clean split.
  Text backbone: CLIP ViT-B/32; Experts: CLIP ViT-L/14, irCSN152-IG65M, SF}
	\label{tab:stages_results}
	
	\begin{tabular}{|*{3}c|*3{c}|}
		\toprule
		\multicolumn{3}{|c|}{Train stages} & \multicolumn{3}{c|}{Text  Video} \\ 
		S & S & S & R@1 & R@5 & MdR \\
		\midrule
		\checkmark &  &                       
		& 7.7  & 19.0 & 60.0 \\
	    & \checkmark &                   
		& 29.0 & 55.3 & 4.0  \\
		& \checkmark &  \checkmark        
		& 30.5 & 56.9 & 4.0  \\
		\checkmark  & \checkmark   &                        
		& 31.2  & 57.8 & 4.0 \\
		\checkmark    & \checkmark &  \checkmark        
		& 32.5 & 59.4 & 3.0  \\
		\bottomrule
	\end{tabular}	
\end{table}

\subsection{Final result} \label{ssec:final_result}
In this section we compare our solution with the prior art. Our best solution uses three modalities: CLIP ViT-L/14 (RGB modality), irCSN152-IG65M (motion modality), Slow-Fast trained on VGG-Sound (audio modality). Text backbone is used from CLIP ViT-L/14. To fuse modalities
we use aggregator with 9 layers and 8 heads. Training procedure is described in Sec.~\ref{ssec:stages}. Results are shown in Tab.~\ref{tab:models-msrvtt-1kA} - Tab.~\ref{tab:models-tgif}.

Center crop is used for visual features extraction during training and testing for all datasets except MSR-VTT (see Tab.~\ref{tab:models-msrvtt}), where we report two results on testing set: center crop and \textit{mean} method (see Sec.~\ref{ssec:non_square}).

Results on MSR-VTT, LSMDC, MSVD, YouCook2, TGIF are obtained using single model. Our model outperforms SOTA by 1.6, 0.6, 3.9, 4.3, 1.1 \% correspondingly on R@5. On MSR-VTT-1kA (see Tab.~\ref{tab:models-msrvtt-1kA}) we report two results with different training splits: full(7k) and 1k-A(9k). First result approaches SOTA and second result outperforms SOTA by 0.8 \% on R@5.



\begin{table}
  \centering
  \caption{Test results on MSR-VTT-1k-A dataset. Results that were obtained using original testing protocol (without dual softmax~\cite{camoe, gao2021clip2tv} on inference) are shown. Results are collected from articles and \url{https://paperswithcode.com/sota/video-retrieval-on-msr-vtt-1ka}}
  \label{tab:models-msrvtt-1kA}

\begin{tabular}{|l|*{5}l|}
    \toprule
    \multirow{2}{*}{Model} & \multicolumn{5}{c|}{MSR-VTT-1k-A text  video} \\
			   & R@1 & R@5 & R@10 & MnR & MdR \\
    \midrule
JSFusion~\cite{yu2018joint}
& 10.2 & 31.2 & 43.2 & --- & 13.0 \\
E2E~\cite{miech2020endtoend}
& 9.9 & 24.0 & 32.4 & --- & 29.5 \\
HT~\cite{miech19howto100m}
& 14.9& 40.2 & 52.8 & --- & 9.0 \\
CE~\cite{liu2020use}
& 20.9 & 48.8 & 62.4 & 28.2 & 6.0 \\
CLIP~\cite{radford2learning}
& 22.5 & 44.3 & 53.7 & 61.7 & 8.0 \\
MMT ~\cite{gabeur2020multimodal}
& 26.6   & 57.1   & 69.6   & 24.0   & 4.0 \\
AVLnet\cite{rouditchenko2020avlnet}
& 27.1 & 55.6 & 66.6 & --- & 4.0 \\
SSB~\cite{patrick2021supportset}
& 30.1 & 58.5 & 69.3 & --- & 3.0 \\
CLIP agg~\cite{portilloquintero2021straightforward}
& 31.2 & 53.7 & 64.2 & --- & 4.0 \\
MDMMT~\cite{mdmmt}
& 38.9 & 69.0 & 79.7 & 16.5 & 2.0 \\
CLIP4Clip~\cite{CLIP4Clip}
& 44.5 & 71.4 & 81.6 & 15.3 & 2.0 \\
CLIP2Video~\cite{clip2video}
& 45.6 & 72.6 & 81.7 & 14.6 & 2.0 \\
LAFF~\cite{laff}
& 45.8 & 71.5 & 82.0 & --- & --- \\
CAMoE~\cite{camoe}
& 44.6 & 72.6 & 81.8 & \B{13.3} & 2.0 \\
MDMMT-2 full (Ours)
& 46.5 & 74.3 & \B{83.3} & 14.1 & 2.0 \\
QB-Norm+CLIP2Video~\cite{qbnorm}
& 47.2 & 73.0 & 83.0 & --- & 2.0 \\
CLIP2TV~\cite{gao2021clip2tv}
& 48.3 & 74.6 & 82.8 & 14.9 & 2.0 \\
MDMMT-2 1k-A (Ours)
& \B{48.5} & \B{75.4} & \B{83.9} & 13.8 & \B{2.0} \\
    \bottomrule
  \end{tabular}
\end{table}


\begin{table}
  \centering
  \caption{Test results on MSR-VTT dataset. Results are collected from articles and \url{https://paperswithcode.com/sota/video-retrieval-on-msr-vtt}}
  \label{tab:models-msrvtt}

\begin{tabular}{|l|c|*{5}l|}
    \toprule
    \multirow{2}{*}{Model} & \multirow{2}{*}{Split} & \multicolumn{5}{c|}{MSR-VTT text  video} \\
			   & & R@1 & R@5 & R@10 & MnR & MdR \\
    \midrule
VSE~\cite{mithun2018learning} & \multirow{17}{*}{full} & 5.0 & 16.4 & 24.6 & --- & 47.0 \\
VSE++~\cite{mithun2018learning}
&& 5.7 & 17.1 & 24.8 & --- & 65.0 \\
Multi Cues~\cite{mithun2018learning}
&& 7.0 & 20.9 & 29.7 & --- & 38.0 \\
W2VV~\cite{Dong_2018}
&& 6.1 & 18.7 & 27.5 & --- & 45.0 \\
Dual Enc.~\cite{dong2019dual}
&& 7.7 & 22.0 & 31.8 & --- & 32.0 \\
CE~\cite{liu2020use}
&& 10.0 & 29.0& 41.2 & 86.8 & 16.0 \\
MMT ~\cite{gabeur2020multimodal}
&& 10.7 &  31.1 & 43.4 &  88.2 & 15.0 \\
CLIP~\cite{radford2learning}
&& 15.1 & 31.8 & 40.4 &  184.2  & 21.0 \\
CLIP agg~\cite{portilloquintero2021straightforward}
&& 21.5 & 41.1 & 50.4 &  --- & 4.0 \\
MDMMT~\cite{mdmmt}
&& 23.1 & 49.8 & 61.8 & 52.8 & 6.0 \\
TACo~\cite{yang2021taco}
&& 24.8 & 52.1 & 64.0 & --- & 5.0 \\
LAFF~\cite{laff}
&& 29.1 & 54.9 & 65.8 & --- & --- \\
CLIP2Video~\cite{clip2video}
&& 29.8 & 55.5 & 66.2 & 45.4 & 4.0 \\
CAMoE~\cite{camoe}
&& 32.9 & 58.3 & 68.4 & 42.6 & 3.0 \\
CLIP2TV~\cite{gao2021clip2tv}
&& 33.1 & 58.9 & 68.9 & 44.7 & 3.0 \\
MDMMT-2 (Ours)
&& \B{33.4} & \B{60.1} & \B{70.5} & \B{39.2} & \B{3.0} \\
MDMMT-2 test \textit{mean} (Ours) 
&& \B{33.7} & \B{60.5} & \B{70.8} & \B{37.8} & \B{3.0} \\
    \midrule
MMT~\cite{gabeur2020multimodal}
&\multirow{3}{*}{\shortstack{full\\clean}}& 10.4 & 30.2 & 42.3 & 89.4 & 16.0 \\
MDMMT~\cite{mdmmt}
&& 22.8 & 49.5 & 61.5 & 53.8 & 6.0 \\
MDMMT-2 (Ours)
&& \B{33.3} & \B{59.8} & \B{70.2} & \B{38.7} & \B{3.0} \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}
  \centering
  \caption{Test results on LSMDC dataset. Results are collected from articles and \url{https://paperswithcode.com/sota/video-retrieval-on-lsmdc}}
  \label{tab:models-lsmdc}
  
  \begin{tabular}{|l|*{5}l|}
    \toprule
    \multirow{2}{*}{Model} & \multicolumn{5}{c|}{LSMDC text  video} \\
			   & R@1 & R@5 & R@10 & MnR & MdR \\
    \midrule
CT-SAN~\cite{yu2017endtoend}
& 5.1 & 16.3  & 25.2 & --- & 46.0 \\
JSFusion~\cite{yu2018joint}
& 9.1 & 21.2  & 34.1 & --- & 36.0 \\
MEE~\cite{miech2020learning}
& 9.3 & 25.1  & 33.4 & --- & 27.0 \\
MEE-COCO~\cite{miech2020learning}
& 10.1 & 25.6 & 34.6 & --- & 27.0 \\
CE~\cite{liu2020use}
& 11.2 & 26.9 & 34.8 & 96.8 & 25.3 \\
CLIP agg~\cite{portilloquintero2021straightforward}
& 11.3 & 22.7 & 29.2 & --- & 56.5 \\
CLIP~\cite{radford2learning}
& 12.4 & 23.7 & 31.0  & 142.5 & 45.0  \\
MMT ~\cite{gabeur2020multimodal}
& 12.9 & 29.9 & 40.1 & 75.0 & 19.3 \\
MDMMT~\cite{mdmmt}
& 18.8 & 38.5 & 47.9 & 58.0 & 12.3 \\
CLIP4Clip~\cite{CLIP4Clip}
& 21.6 & 41.8 & 49.8 & 58.0 & --- \\
QB-Norm+CLIP4Clip~\cite{qbnorm}
& 22.4 & 40.1 & 49.5 & --- & 11.0 \\
CAMoE~\cite{camoe}
& 25.9 & 46.1 & 53.7 & 54.4 & --- \\
MDMMT-2 (Ours)
& \B{26.9} & \B{46.7} & \B{55.9} & \B{48.0} & \B{6.7} \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}
  \centering
  \caption{Test results on MSVD dataset. Results are collected from articles and \url{https://paperswithcode.com/sota/video-retrieval-on-msvd}}
  \label{tab:models-msvd}
  
  \begin{tabular}{|l|*{5}l|}
    \toprule
    \multirow{2}{*}{Model} & \multicolumn{5}{c|}{MSVD text  video} \\
			   & R@1 & R@5 & R@10 & MnR & MdR \\
    \midrule
LAFF~\cite{laff}
& 45.4 & 76.0 & 84.6 & --- & --- \\
CLIP4Clip~\cite{CLIP4Clip}
& 46.2 & 76.1 & 84.6 & 10.0 & 2.0 \\
CLIP2Video~\cite{clip2video}
& 47.0 & 76.8 & 85.9 &  9.6 & 2.0 \\
QB-Norm+CLIP2Video~\cite{qbnorm}
& 48.0 & 77.9 & 86.2 & --- & 2.0 \\
CAMoE~\cite{camoe}
& 49.8 & 79.2 & 87.0 & 9.4 & --- \\
MDMMT-2 (Ours)
& \B{56.8} & \B{83.1} & \B{89.2} & \B{8.8} & \B{1.0} \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}
  \centering
  \caption{Test results on YouCook2 dataset. Results are collected from articles and \url{https://paperswithcode.com/sota/video-retrieval-on-youcook2}}
  \label{tab:models-youcook2}
  
  \begin{tabular}{|l|*{5}l|}
    \toprule
    \multirow{2}{*}{Model} & \multicolumn{5}{c|}{YouCook2  text  video} \\
			   & R@1 & R@5 & R@10 & MnR & MdR \\
    \midrule
Text-Video Embedding~\cite{miech19howto100m}
& 8.2 & 24.5 & 35.3 & --- & 24.0 \\
COOT~\cite{coot}
& 16.7 & --- & 52.3 & --- & --- \\
UniVL~\cite{univl}
& 28.9 & 57.6 & 70.0 &  --- & 4.0 \\
TACo~\cite{yang2021taco}
& 29.6 & 59.7 & 72.7 & --- & 4.0 \\
MDMMT-2 (Ours)
& \B{32.0} & \B{64.0} & \B{74.8} & \B{12.7} & \B{3.0} \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}
  \centering
  \caption{Test results on TGIF dataset. Results are collected from articles and \url{https://paperswithcode.com/sota/video-retrieval-on-tgif}}
  \label{tab:models-tgif}
  
  \begin{tabular}{|l|*{5}l|}
    \toprule
    \multirow{2}{*}{Model} & \multicolumn{5}{c|}{TGIF  text  video} \\
			   & R@1 & R@5 & R@10 & MnR & MdR \\
    \midrule
W2VV++~\cite{w2vv++}
& 9.4 & 22.3 & 29.8 &  --- & --- \\
SEA~\cite{sea}
& 11.1 & 25.2 & 32.8 & --- & --- \\
LAFF~\cite{laff}
& 24.5 & 45.0 & 54.5 & --- & --- \\
MDMMT-2 (Ours)
& \B{25.5} & \B{46.1} & \B{55.7} & \B{94.1} & \B{7.0} \\
    \bottomrule
  \end{tabular}
\end{table}