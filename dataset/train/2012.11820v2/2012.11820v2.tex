

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{naaclhlt2019}
\usepackage{gb4e}
\noautomath
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage[capitalize]{cleveref}
\usepackage{amssymb}
\usepackage{multirow,multicol}
\usepackage{graphicx}
\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage{array}
\usepackage{soul, color}
\usepackage{booktabs}
\setlength\marginparwidth{2cm}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{enumitem}
\usepackage{amsthm}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tipa}
\usepackage{stackengine}
\usepackage{rotating}
\usepackage{tabularx}
\usepackage{microtype}



\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\newcommand\BibTeX{B\textsc{ib}\TeX}
\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}
\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}}
\newcolumntype{R}[1]{>{\PreserveBackslash\raggedleft}p{#1}}
\newcolumntype{L}[1]{>{\PreserveBackslash\raggedright}p{#1}}
\DeclareMathOperator*{\argmax}{\text{argmax}}
\newcommand*\rot{\rotatebox{90}}
\crefformat{section}{\S#2#1#3} \crefformat{subsection}{\S#2#1#3}
\crefformat{subsubsection}{\S#2#1#3}



\setlength\titlebox{6cm}





\title{Recognizing Emotion Cause in Conversations} 

\author{Soujanya Poria, 
Navonil Majumder, Devamanyu
Hazarika\Thanks{~Equal contribution. Randomly ordered.}~,
Deepanway Ghosal\footnotemark[1]~,\\
\textbf{Rishabh Bhardwaj, Samson Yu Bai Jian, Romila Ghosh, Niyati Chhaya,}\\
\textbf{Alexander Gelbukh, Rada Mihalcea}\1ex]
~Singapore University of Technology and Design, Singapore\\
~National University of Singapore, Singapore  \\
~CIC, Instituto Polit√©cnico Nacional, Mexico  \\
~Independent researcher, India\\
~University of Michigan, USA\\
~Adobe Research, India
}

\date{}

\definecolor{gelbukh}{rgb}{.9,1,.9} 


\newcommand\emo[1]{\textsc{#1}}
\newcommand\code[1]{\texttt{#1}}
\newcommand\ToDeleteIfNeedSpace{}

\newcommand\RECCON{\emph{recognizing emotion cause in conversations}}
\newcommand\ECRIC{ECRIC}
\newcommand\RECCONDA{RECCON}
\newcommand\RECCONDADD{RECCON-DD}
\newcommand\RECCONDAIE{RECCON-IE}

\aclfinalcopy
\begin{document}
\maketitle
\begin{abstract}
Recognizing the cause behind emotions in text is a fundamental yet under-explored area of research in NLP. Advances in this area hold the potential to improve interpretability and performance in affect-based models. Identifying emotion causes at the utterance level in conversations is particularly challenging due to the intermingling dynamic among the interlocutors. To this end, we introduce the task of \RECCON{} with an accompanying dataset named \RECCONDA. Furthermore, we define different cause types based on the source of the causes and establish strong transformer-based baselines to address two different sub-tasks of RECCON: 1)~Causal Span Extraction and 2)~Causal Emotion Entailment. The dataset is available at \url{https://github.com/declare-lab/RECCON}.

\end{abstract}

\section{Introduction}









Emotions are intrinsic to humans; consequently,
emotion understanding is a key part of human-like artificial
intelligence (AI). Language is
often indicative of one's emotions. Hence, emotion recognition has attracted much attention 
in the field of natural language processing (NLP)~\citep{kratzwald2018decision, colneric2018emotion}, due to its wide range of applications in
opinion mining, recommender systems, healthcare, and other areas.

Substantial progress has been made in the detection and classification of emotions, expressed in text or videos, according to emotion taxonomies~\cite{ekman1993facial,plutchik}. However, further reasoning about emotions, such as  
understanding the cause of an emotion expressed by a speaker,
has been less explored so far.
For example, consider the following review of a smartphone, ``\textit{I hate the touchscreen as it freezes after 2-3 touches}''.
Understanding this text implies not only detecting the expressed negative emotion, specifically \emo{disgust}, but also spotting its cause~\citep{liu2012sentiment}---in this case, ``\textit{it freezes after 2-3 touches}.''






Of a wide spectrum of emotion-reasoning tasks~\cite{ellsworth2003appraisal},
in this work, we focus on identifying the causes (also called antecedents, triggers, or stimuli) of emotions expressed specifically in conversations. In particular, we look for events, situations, opinions, or experiences in the conversational context that is primarily responsible for an elicited emotion in the target utterance. Apart from event mentions, the cause could also be a speaker's counterpart reacting towards an event cared for by the speaker (inter-personal emotional influence). 

We introduce the task of \textbf{\underline{r}ecognizing \underline{e}motion \underline{c}ause in \underline{con}\-ver\-sa\-tions}, which refers to the extraction of such stimuli behind an emotion in a conversational utterance. The cause could be present in the same or contextual utterances (conversational history). We formally define this task in~\Cref{sec:annot}.

\begin{figure}[t] 
    \centering 
    \includegraphics[width=0.9\linewidth]{figs/examples.pdf} 
    \caption{\footnotesize{Emotion causes in conversations.}}
    \label{fig:examples}
\end{figure}

In~\cref{fig:examples}, we exemplify this task. In the first example, we are interested in knowing the cause of person B's () emotion (\emo{happy}). It can be seen that  is happy due to the event---``\textit{getting married}'', and similarly,  also reacts positively to this event. Here, we could infer that 's emotion is caused either by the reference of the first utterance to the event of getting married, or by the fact that  is happy about getting married---both of which can be considered as stimulus for 's emotion. In the second conversation, the cause of 's emotion is the event ``\textit{football match}" and a negative emotion \emo{disgust} indicates 's unsatisfied experience of the match. In contrast,  takes pleasure of the match---sharing the same cause with ---with \emo{happiness} emotion. These examples demonstrate the challenging problem of recognizing emotion causes in conversations, which to the best of our knowledge, is one of the first attempts in this area of research.
































We can summarize our contributions as follows:



\begin{enumerate}[leftmargin=*]
\item We introduce a new task, \RECCON{}, and dive into many unique characteristics of this task that is peculiar to conversations. 
In particular, we define the relevant types of emotion causes (\cref{sec:types}). 
     \item We describe a new annotated dataset for this task, \RECCONDA{}\footnote{pronounced as \textit{reckon}.}, including both acted and real-world conversations (\cref{sec:dataset}).
     \item Further, we introduce two challenging sub-tasks that demand complex reasoning (\cref{sec:challenges}), and provide the corresponding baselines (\cref{sec:experiments}).
\end{enumerate}

\section{Related Work} \label{sec:related_works}

Initial works in emotion analysis and opinion mining explored different aspects of affect beyond polarity prediction, such as identifying the opinion/emotion-feeler (or holder, source)~\cite{das-bandyopadhyay-2010-finding,DBLP:conf/naacl/ChoiCRP05}. However, the task of emotion cause extraction was studied later initially by~\citet{lee-etal-2010-text}. Such initial works involved extracting cause events in a rule-driven manner~\cite{chen-etal-2010-emotion}. \citet{gui2016event} constructed an emotion cause extraction dataset by identifying events that trigger emotions. To avoid the latent emotions and implicit emotion causes associated with the informal text, the authors used news articles as the target corpus for cause extraction. Choosing news articles as the source data for cause extraction helped them reduce reasoning complexity for the annotators while extracting emotion causes. \citet{DBLP:conf/cicling/GhaziIS15} and \citet{gao2017overview} are other notable works on Emotion Cause Extraction (ECE).







Modifying the ECE task, \citet{DBLP:conf/acl/XiaD19} proposed Emotion-Cause Pair Extraction (ECPE) that jointly identifies both emotions and their corresponding causes~\cite{DBLP:conf/emnlp/ChenHCL18}. Further, \citet{chen-etal-2020-conditional} recently proposed the conditional Emotion Cause Pair (ECP) identification task, where they highlight the causal relationship to be valid only in particular contexts. We incorporate this property in our dataset construction, as we annotate multiple spans in the conversational history that \textit{sufficiently} indicate the cause. Similar to~\citet{chen-etal-2020-conditional}, we also provide negative examples of context that does not contain the causal span. 

Our work is a natural extension of these 
works. We propose a new dataset on conversations, which is more difficult to annotate and the associated task of recognizing emotion cause in conversations poses a greater hitch to solve due to numerous challenges mentioned in the following sections (see \cref{sec:challenges}), for example:
1)~expressed emotions are not always explicit in the conversations; 2)~conversations can be very informal where the phrase connecting emotion with its cause can often be implicit and thus needs to be inferred; 3)~the stimuli of the elicited emotions can be located far from the target utterance in the conversation history and detecting it requires complex reasoning and co-reference often using commonsense.






























\section{Definiton of the Task}
\label{sec:terminology}

We distinguish between emotion \textbf{evidence} and emotion \textbf{cause}:
\begin{itemize}[leftmargin=*]
    \item \textit{Emotion evidence} is a part of the text that indicates the presence of an emotion in the speaker's emotional state. It acts in the real world between the text and the reader or the system. Identifying and interpreting the emotion evidence is the underlying process of the well-known emotion detection task. 
    \item \textit{Emotion cause} is a part of the text expressing the reason for the speaker to feel the emotion given by the emotion evidence. It acts in the 
described world
    between the (described) circumstances and the (described) speaker's emotional state. Identifying the emotion cause constitutes the task we consider in this~paper.
\end{itemize}
For instance, in \cref{fig:examples}, 's turn contains evidence of 's emotion, while 's turn contains its cause.
The same text span can be both emotion evidence and cause, but generally this is not the~case.

Defining the notion of emotion cause is, in a way, the main goal of this paper. However, short of a formal definition, we will explain this notion on numerous examples and, in computational terms, via the labeled dataset. Note that a text part can be both emotion evidence and cause.

We use the following terminologies throughout the paper. 
The \textbf{target utterance}  is the  utterance of a conversation, whose emotion label  is known and whose emotion cause we want to identify.
The \textbf{conversational history}  is the set of all utterances from the beginning of the conversation till the utterance , including .
A \textbf{causal span} for an utterance  is a maximal sub-string, of an utterance from , that is a part of 's emotion cause; we will denote the set of the causal spans by .
A \textbf{causal utterance} is an utterance containing a causal span; we denote the set of all causal utterances for  by .
An \textbf{\underline utterance--\underline causal \underline span (UCS) pair} is a pair , where  is an utterance and .

Thus, \textbf{recognizing emotion cause} is the task of identifying all (correct) UCS pairs in a given~text.

In the context of our training procedure, we will refer to (correct) UCS pairs as \textbf{positive 
examples}, whereas pairs  with  are  \textbf{negative
examples}.
In \cref{sec:neg} we describe the sampling strategies for negative examples.







\section{Building the \RECCONDA{} dataset}\label{sec:dataset}

\subsection{Emotional dialogue sources}
We consider two popular conversational datasets \textbf{IEMOCAP}~\cite{iemocap} and \textbf{DailyDialog}~\cite{li2017dailydialog}, both equipped with utterance-level emotion labels.



\textbf{IEMOCAP} is a dataset of two-person conversations annotated with six emotions classes \emo{happy}, \emo{sad}, \emo{neutral}, \emo{anger}, \emo{excited}, and \emo{frustrated}. The dialogues in this dataset span across sixteen unique conversational situations. To avoid redundancy, we handpick only one dialogue from each of these situations. We denote the subset of \RECCONDA{} comprising these dialogues as \RECCONDAIE{}.

\textbf{DailyDialog} is a natural human communication dataset covering various topics about our daily lives. All utterances are labeled with emotion categories: \emo{anger}, \emo{disgust}, \emo{fear}, \emo{happy}, \emo{neutral}, \emo{sad}, and \emo{surprise}. The dataset has over  \emo{neutral} labels. Due to this skewness, we randomly selected dialogues which has at least four \textit{non-neutral} utterances. We denote this subset of \RECCONDA{}, comprising the dialogues from DailyDialog, as \RECCONDADD. Some statistics about the annotated dataset is shown in \cref{tab:stat}.

\paragraph{Need for sampling from two different datasets.} \label{sec:dataset_diffs}
Although both IEMOCAP and DailyDialog are annotated with utterance-level emotions, they differ in many aspects. Firstly, the average number of utterances per dialogue in IEMOCAP is more than , whereas DailyDialog has a shorter average length of . Secondly, the shifts between non-neutral emotions (e.g., sad to anger, happy to excited) are more frequent in IEMOCAP compared to DailyDialog (see \citep{ghosal2020utterancelevel}). Consequently, both cause detection and causal reasoning in IEMOCAP are more interesting as well as difficult. Lastly, in \cref{tab:stat}, we can see that in our annotated IEMOCAP split, almost 40.5\% utterances have their emotion cause in utterances at least  timestamps distant in the contextual history. On the contrary, this percentage is just  in our annotated DailyDialog dataset. 

\subsection{Annotation Process}\label{sec:annot}
\paragraph{Annotation guidelines.}
Given an utterance  labeled with an emotion , the annotators were asked to extract the set of causal spans  
that sufficiently represent the causes of the emotion~. If the cause of  was latent, i.e., there was no explicit causal span in the dialog,
the annotators wrote down the assumed causes that they inferred from the text. Each utterance was annotated by two human experts---graduate students with reasonable knowledge of the task.

In fact, the annotators were asked to look for the casual spans of  in the whole dialog and not only in the past history . We show one such case in \cref{fig:latent_cause} where the causal span of the emotion \emo{fear} in utterance 1 is recognized in utterance 3 -- ``someone is stalking me''.
However, they flagged only seven instances of the utterances with explicit emotion causal spans that occur in the conversational future with respect to  in the whole dataset.
As such, we discarded those spans and made a decision to consider only causal spans in ; hence the definition in~\cref{sec:terminology}.

\paragraph{Emotional expression.}
An utterance can contain 1)~a description of the triggers or stimuli of the expressed emotion, and~/~or 2)~a reactionary emotional expression. 
In our setup, by following the discrimination among emotion evidence and cause as explained in~\cref{sec:terminology}, we instructed the annotators to look beyond just emotional expressions and 
identify the 
emotion cause. We can illustrate this with
\cref{fig:causevsexpression}, where  explains the cause for \emo{happiness}; the same cause evokes the emotion \emo{excited} in . Meanwhile, the utterance  by  is merely an emotional expression (evidence).


\ToDeleteIfNeedSpace Emotion cause can also corroborate in generating an emotional expression, e.g., in \cref{fig:causevsexpression}, the event ``\textit{winning the prize}'' causes \emo{excited} emotion in  which directs  to utter the expression ``\textit{Wow! Incredible}''. This type of generative reasoning will be very important in our future work.

\begin{figure*}[t]
     \centering
     \begin{subfigure}[b]{0.33\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/target_utt_cause.pdf}
         \caption{}
         \label{fig:emotion_in_target}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/latent.pdf}
         \caption{}
         \label{fig:latent_cause}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.33\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/causevsexpression.pdf}
         \caption{}
         \label{fig:causevsexpression}
     \end{subfigure}
     \caption{\footnotesize{{No context: \ref{fig:emotion_in_target}}. {Unmentioned Latent Cause: \ref{fig:latent_cause}}. {Distinguishing emotion cause from emotional expressions: \ref{fig:causevsexpression}}}.}
\end{figure*}



\paragraph{Why span detection?}
Firstly, emotion-cause extraction has historically been defined as an information extraction task of identifying spans within the emotion-bearing sentences~\cite{DBLP:conf/acl/XiaD19,DBLP:conf/cicling/GhaziIS15}. The core assumption is that such spans are good descriptors of the underlying causes towards the generated emotions~\cite{talmy2000toward}. We extend this popular formalism into a multi-span framework.
Secondly, while recognizing emotion cause is driven by multiple controlling variables (see~\cref{sec:controlling_vars_appendix}), we adopt this setup as these spans can often represent or allude to these controlling variables. A more elaborate setup would require explaining how the spans can be combined to form the trigger and consequently evoke the emotion (\cref{fig:csk_ex}); we leave such emotion causal reasoning in conversations to future work.

\subsubsection{Annotation Aggregation}
Following \citet{gui2016event}, we aggregate the annotations in two stages: utterance-level and span-level aggregation.

\paragraph{Stage 1: Utterance-level aggregation.}
Here, we decide whether an utterance is causal by majority voting: a third expert annotator is brought in as the tie breaker.

\paragraph{Stage 2: Span-level aggregation.}
Within each causal utterance (selected in the previous step), for spans that share some sub-string across annotators, we take the union of the spans as the final causal span. In other words, for overlapping annotated spans, we take the larger boundary as the final causal span. If they do not share a sub-span, a third annotator is brought in to determine the final span from the existing spans. This third annotator is also instructed to prefer the shorter spans over the longer ones when they can sufficiently represent the cause without losing any information.
The third annotator could not break the tie for~34 causal utterances, which we discarded from the dataset.



\begin{table}[t!]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{|l|cccc|}
\toprule
\textbf{Dataset} & \textbf{Language} & \textbf{Source} & \textbf{Size} & \textbf{Format}\\
\midrule
\citet{DBLP:conf/ijcnlp/NeviarouskayaA13} & English & ABBYY Lingvo & & \\
&  & dictionary & 532 & Sentences \\
\citet{DBLP:conf/nlpcc/GuiYXLLZ14} & Chinese & Chinese Weibo & 1333 & Sentences \\
\citet{DBLP:conf/cicling/GhaziIS15} & English  & FrameNet  & 1519  & Sentences  \\
\citet{gui2016event} & Chinese  & SINA city news & 2105 & Clauses \\
\citet{gao2017overview} & Chinese/ & SINA city news/ & 2619 / &  \\
 & English & /English Novel &  2403 & Clauses \\
\RECCONDA{} (Ours) & English & IEMOCAP/ & 1154 / &  \\
 &  & DailyDialog & 9915 & Dialogues \\
\bottomrule
\end{tabular}
}
	\caption{\footnotesize{Datasets for Emotion Cause Extraction and related tasks. Datasets in \citet{DBLP:conf/acl/XiaD19,chen-etal-2020-conditional} are derived from \citet{gui2016event}.}}
	\label{tab:related_datasets}
\end{table}

\begin{figure*}[t]
     \centering
     \begin{subfigure}[b]{0.33\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/self-contagion.pdf}
         \caption{Mood Setting}
         \label{fig:self-contagion}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/generic.pdf}
         \caption{Generic Cause}
         \label{fig:generic_cause}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.33\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figs/hybrid.pdf}
         \caption{Hybrid}
         \label{fig:hybrid}
     \end{subfigure}
     \caption{\footnotesize{\textit{Self-contagion (\ref{fig:self-contagion},\ref{fig:generic_cause}):} The cause of the emotion is primarily due to a stable mood of the speaker that was induced in the previous dialogue turns; \textit{Hybrid (\ref{fig:hybrid}):} The hybrid type 
with both
     inter-personal emotional influence and self-contagion.
}}
\end{figure*}




\begin{table}[t]
\small
 	\centering
 	\resizebox{\linewidth}{!}{
	\begin{tabular}{|L{5.7cm}@{}c@{~~}c|}
		\toprule
		\textbf{Description} & \textbf{\RECCONDADD}  & \textbf{\RECCONDAIE{}}\\
		\midrule
		\# Dialogues &  &  \\
		\# Utterances &  &  \\
		\# Utterances annotated with emotion cause  &  &  \\
\# Utterances cater to background 
cause &  &  \\
		\# Utterances where cause solely lies in the same utterance &  &  \\
		\# Utterances where cause includes the same utterance along with contextual utterances &  &  \\
		\midrule
		\# Utterance with emotion Anger       & 451 & 89 \\
        \# Utterance with emotion Fear        & 74 & - \\
        \# Utterance with emotion Disgust     & 140 & - \\
        \# Utterance with emotion Frustration &  - & 109 \\
        \# Utterance with emotion Happy       & 4361 & 58 \\
        \# Utterance with emotion Sad         & 351 & 70 \\
        \# Utterance with emotion Surprise    & 484 & - \\
        \# Utterance with emotion Excited     & - & 197 \\
        \# Utterance with emotion Neutral     & 5243 & 142 \\  
		\midrule
		\# UCS pairs &  & \\
		\# Utterances having single cause &  &  \\
		\# Utterances having two causes &  &  \\
		\# Utterances having three causes &  &  \\
		\# Utterances having more than three causes &  &  \\
		\# Causes per utterance (Average) &  &  \\
		\midrule
		\# No Context &  &  \\
		\# Inter-Personal &  &  \\
		\# Self-Contagion &  &  \\
		\# Hybrid &  &  \\
		\# Latent &  &  \\
		\midrule
		\# Utterances () having cause at  &  &  \\
		\# Utterances () having cause at  &  &  \\
		\# Utterances () having cause at  &  &  \\
		\# Utterances () having cause at  &  &  \\
		\bottomrule
	\end{tabular}
	}
	\caption{\footnotesize{Statistics of the \RECCONDA{} annotated dataset.}}
	\label{tab:stat}
\end{table}

\subsection{Dataset Statistics}

We have measured two types of inter-annotator agreement scores: 1)~at the utterance level and 2)~at the span level.
Following \citet{gui2016event}, we measured the inter-annotator agreement (IAA) at the utterance level, resulting in a kappa score of 0.7928. 
However, as pointed out by \citet{brandsen-etal-2020-creating}, macro F1 score is a more appropriate approach for span extraction-type tasks. Hence, at the utterance level, we also compute the pairwise macro F1 score between all possible pairs of annotators and then average them. This gives us a 0.8839 macro F1 score. \citet{brandsen-etal-2020-creating} also suggest the removal of negative examples---in our case, the utterances in the conversational history containing no causal span for the emotion of the target utterance---for macro F1 calculation, since such examples are usually very frequent, which may lead to a skewed F1 score. As expected, adopting this yields a lower F1 score of 0.8201. At span level, the F1 score, as explained in \citet{rajpurkar2016squad}, is calculated for all possible pairs of annotators followed by taking their average. Overall, we obtain an F1 score of 0.8035 at span level.
In \cref{tab:related_datasets}, we compare our dataset with the existing datasets in terms of size, data sources, and language. The remaining statistics of \RECCONDA{} are consolidated in \cref{tab:stat}.









\section{Types of Emotion Causes}
\label{sec:types}
In our dataset, \RECCONDA{}, we observe five predominant types of emotion causes that are based on the source of the stimuli (events / situations / acts) in the conversational context, responsible for the target emotion. 
The annotators were asked to flag the utterances with latent emotion cause or emotion cause of type 2b, as explained below.
The distribution of these cause types is given in \cref{tab:stat}.

\paragraph{Type 1: No Context.} The cause is present within the target utterance itself. The speaker feeling the emotion explicitly mentions its cause in the target utterance (see~\cref{fig:emotion_in_target}).
 


\paragraph{Type 2: Inter-Personal Emotional Influence.}
The emotion cause is present in the other speaker's utterances.
We observe two possible sub-types of such influences:
\begin{enumerate}[label=2\alph*)]
    \item \textbf{Trigger Events / Situations.} The emotion cause lies within an event or concept mentioned by the other speaker.
    \item \textbf{Emotional Dependency.} The emotion of the target speaker is induced from the emotion of the other speaker over some event / situation.
\end{enumerate}

\paragraph{Type 3: Self-Contagion.}
In many cases, we observe that the cause of the emotion is primarily due to a stable mood of the speaker that was induced in some previous dialogue turns. For example, in a dialogue involving cordial greetings, there is a tendency for a \emo{happy} mood to persist across several turns for a speaker. \cref{fig:self-contagion} presents an example where such self-influences can be observed. Utterance  establishes that  likes winter. This concept triggers a \emo{happy} mood for the future utterances, as observed in utterances~3 and~5. In \cref{fig:generic_cause}, similarly, the trigger of emotion \emo{excited} in utterance~3 is mentioned by the same speaker in his or her previous utterance.

\paragraph{Type 4: Hybrid.} Emotion causes of type~2 and~3 can jointly cause the emotion of an utterance, as illustrated by \cref{fig:hybrid}.




\paragraph{Type 5: Unmentioned Latent Cause.} There are instances in the dataset where no explicit span in the target utterance or the conversational history can be identified as the emotion cause. \cref{fig:latent_cause} shows such a case. Here, in first utterance,  speaks of being terrified and fearful without indicating the cause. We annotate such cases as latent causes. Sometimes the cause is revealed in future utterances, e.g., ``\textit{someone is stalking me}'' as the reason of being fearful. However, as online settings would not have access to the future turns, we refrain from treating future spans as 
causes.


\section{Experiments}
\label{sec:experiments}

We formulate two distinct subtasks of \RECCON{}: 1)~causal span extraction and 2)~causal emotion entailment. 


\subsection{Compiling Dataset Splits}
\label{sec:dataprep}

\RECCONDADD{} is the subset of our dataset that contains dialogues from DailyDialog. For this subset, we created the training, validation, and testing examples based on the original splits in~\cite{li2017dailydialog}. However, this resulted in the validation and testing sets to be quite small, so we moved some dialogues to them from the original training set. 

The subset \RECCONDAIE{} consists of dialogues from the IEMOCAP dataset. This subset is quite small as it contains only sixteen unique dialogues (situations). So, we consider the entire \RECCONDAIE{}  as another testing set, emulating an out-of-distribution generalization test. We report results on this dataset based on models trained on \RECCONDADD{}. In our experiments, we ignore the utterances with only latent emotion causes. 

\subsubsection{Generating Negative Examples}
\label{sec:neg}
The annotated dataset, \RECCONDA{} (consisting of subsets \RECCONDADD{} and \RECCONDAIE{}) only contains positive examples, where an emotion-containing target utterance is annotated with a causal span extracted from its conversational historical context. However, to train a model for the \RECCON{} task, we need negative examples, i.e., the instances which are not cause of the utterance. In the sequel, we use the terminology introduced in \cref{sec:terminology}; the reader should 
refer to that section for clearer understanding.

We adopt the following strategy to create the negative examples:\\
\noindent \textbf{Fold 1:} Consider a dialogue  and a target utterance  in .
We construct the complete set of negative examples as 
,
where  is the conversational history and  is the set of causal utterances for .



We discuss building Fold 2 and Fold 3 in~\Cref{sec:analysis}. The statistics of the final dataset are shown in \cref{tab:finalstat}. 


\begin{table}[ht!]
\centering
\scalebox{0.7}{
\begin{tabular}{|lll|ccc|}
\toprule
& &  Data & Train & Val & Test \\
\midrule
\multirow{4}{*}{\rotatebox{90}{\textbf{\small{Fold 1}}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{DD}}}} & Positive UCS pairs & 7269 & 347 & 1894 \\
&& Negative UCS pairs & 20646 & 838 & 5330 \\
\cmidrule{2-6}
&\multirow{2}{*}{\rotatebox{90}{\textbf{\small{IEMO}}}} & Positive UCS pairs & - & - & 1080 \\
&& Negative UCS pairs & - & - & 11305 \\

\midrule
\multirow{4}{*}{\rotatebox{90}{\textbf{\small{Fold 2}}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{DD}}}} & Positive UCS pairs & 7269 & 347 & 1894 \\
&& Negative UCS pairs & 18428 & 800 & 4396 \\
\cmidrule{2-6}
&\multirow{2}{*}{\rotatebox{90}{\textbf{\small{IEMO}}}} & Positive UCS pairs & - & - & 1080 \\
&& Negative UCS pairs & - & - & 7410 \\

\midrule
\multirow{4}{*}{\rotatebox{90}{\textbf{\small{Fold 3}}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{DD}}}} & Positive UCS pairs & 7269 & 347 & 1894 \\
&& Negative UCS pairs & 18428 & 800 & 4396 \\
\cmidrule{2-6}
&\multirow{2}{*}{\rotatebox{90}{\textbf{\small{IEMO}}}} & Positive UCS pairs & - & - & 1080 \\
&& Negative UCS pairs & - & - & 7410 \\
\bottomrule
\end{tabular}
}
\caption{\footnotesize{The statistics of \RECCONDA{} comprising both positive (valid) and negative (invalid) UCS pairs. DD  \RECCONDADD{}; IEMO  \RECCONDAIE{}. Utterances with only latent emotion causes are ignored in our experiments.}}
\label{tab:finalstat}
\end{table}






\subsection{Subtask 1: Causal Span Extraction}
\label{sec:cse}
\textit{Causal Span Extraction} is the task of identifying the causal span (emotion cause) for a target non-neutral utterance. In our experimental setup, we formulate \textit{Causal Span Extraction} as a Machine Reading Comprehension (MRC) task similar to the task in Stanford Question Answering Dataset~\citep{rajpurkar2016squad}. We propose two different span extraction settings: 1)~With Conversational Context and 2)~Without Conversational Context.




\subsubsection{Subtask Description}









\paragraph{With Conversational Context (w/ CC)} We 
believe
that the presence of conversational context would be key to the span extraction algorithms. To evaluate this hypothesis, we design this subtask, where the conversational history is available to the model. In this setup, for a target utterance , the causal utterance , and a causal span  from , we construct the context, question, and answer as follows:\footnote{By ``causal span from evidence in the context'' we really mean a causal span from the conversation history .}

\noindent \textbf{Context:} The context of a target utterance  is the conversational history, i.e., a concatenation of all utterances from .
Similarly, for a negative example , where , conversational history of  is used as context.\\
\textbf{Question:} The question is framed as follows: ``\textit{The target utterance is . The evidence utterance is . What is the causal span from evidence in the context that is relevant to the target utterance's emotion ?}".\\
\textbf{Answer:} The causal span  appearing in  if . For negative examples,  is assigned an empty string. 

If a target utterance has multiple causal utterances and causal spans, then we create separate (Context, Question, Answer) instances for them. Unanswerable questions are also created from invalid (cause, utterance) pairs following the same approaches explained in \cref{sec:dataprep}. 

\paragraph{Without Conversational Context (w/o CC)}
In this formulation, we intend to identify whether the \textit{Causal Span Extraction} task is feasible when we only have information about the target utterance and the causal utterance. Given a target utterance  with emotion label , its causal utterance  where , and the causal span , the question is framed as framed as follows: ``\textit{The target utterance is . What is the causal span from context that is relevant to the target utterance's emotion ?}''. The task is to extract answer  from context . For negative examples,  is assigned an empty string.

\subsubsection{Models}
We use the following two pretrained transformer-based models to benchmark the \textit{Causal Span Extraction} task:
\paragraph{RoBERTa Base}: We use the \code{roberta-base}
model~\cite{liu2019roberta} and add a linear layer on top of the hidden-states output to compute span start and end logits. Scores of candidate spans are computed following~\citet{devlin2018bert}, and the span with maximum score is selected as the answer.




\paragraph{SpanBERT Fine-tuned on SQuAD}: We use SpanBERT~\citep{joshi2020spanbert} as the second baseline model. 
SpanBERT follows a different pre-training objective compared to RoBERTa (e.g. predicting masked contiguous spans instead of tokens) and performs better on question answering tasks. 
In this work we are using the SpanBERT base model fine-tuned on SQuAD 2.0 dataset.




\subsubsection{Evaluation Metrics}
\label{sec:metric}


\noindent \textbf{EM (Exact Match):} EM represents, with respect to the gold standard data, how many causal spans are exactly extracted by the model.\\
\textbf{F1}: This is the F1 score introduced in~\cite{rajpurkar2016squad} to evaluate predictions of extractive QA models and calculated over positive examples in the data.\\ 
\textbf{F1}: Negative F1 represents the F1 score of detecting negative examples with respect to the gold standard data. Here, for a target utterance , the ground truth are empty spans.\\
\textbf{F}: This metric is similar to F1 but calculated for every positive and negative example followed by an average over them.


While all the above metrics are important for evaluation, we stress that future works should 
particularly consider performances for EM, F1, and F.



\begin{table}[t!]
  \centering
 \small
  \resizebox{\linewidth}{!}{
\begin{tabular}{|lll|cccc|cccc|}
    \toprule
   \multicolumn{3}{|c|}{\textbf{Model}} & \multicolumn{4}{c|}{\textbf{w/o CC}} & \multicolumn{4}{c|}{\textbf{w/ CC}}\\
   & & & EM & F1 & F1 &  & EM & F1  & F1 &  \\
    \midrule
   \multirow{4}{*}{\rotatebox{90}{\textbf{\small{Fold 1}}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{\tiny{DD}}}} & RoBERTa  & 26.82 & 45.99 & \textbf{84.55} & \textbf{73.82} & 32.63 & 58.17 & 85.85 & 75.45\\
   
  &  & SpanBERT & \textbf{33.26} & \textbf{57.03} & 80.03 & 69.78 & \textbf{34.64} & \textbf{60.00} & \textbf{86.02} & \textbf{75.71} \\
    \cmidrule{2-11}
    & \multirow{2}{*}{\rotatebox{90}{\textbf{\tiny{ IEMO}}}} & RoBERTa  & 9.81 & 18.59 & \textbf{93.45} & \textbf{87.60} & 10.19 & 26.88 & \textbf{91.68} & \textbf{84.52}\\
    
  &  & SpanBERT & \textbf{16.20} & \textbf{30.22} & 87.15 & 77.45 & \textbf{22.41}  & \textbf{37.80} & 90.54 & 82.86 \\
 
      \midrule
\multirow{4}{*}{\rotatebox{90}{\textbf{\small{}}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{\tiny{DD}}}} & RoBERTa  & 37.76 & 63.87 & - & - & 39.02 & 69.13 & - & -\\
   
  &  & SpanBERT & 41.96 & 72.01 & - & - & 42.24 & 71.91 & - & -\\
    \cmidrule{2-11}
    & \multirow{2}{*}{\rotatebox{90}{\textbf{\tiny{ IEMO}}}} & RoBERTa  & 22.49 & 45.01 & - & - & 17.27 & 42.15 & - & -\\
    
  &  & SpanBERT & 26.91 & 52.22 & - & - & 31.33 & 60.14 & - & -\\








\bottomrule
   \end{tabular}
  }
\caption{\footnotesize{Results for Causal Span Extraction task on the test sets of \RECCONDADD{} and \RECCONDAIE{}. All scores are in percentage and are reported at best validation F1 scores. DD  \RECCONDADD{}; IEMO  \RECCONDAIE{}; RoBERTa  RoBERTa Base.}}
  \label{tab:cse}
\end{table}




\begin{table}[ht!]
  \centering
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{|lll|ccc|ccc|}
    \toprule
      \multicolumn{3}{|c|}{\textbf{Model}} & \multicolumn{3}{c|}{\textbf{w/o CC}} & \multicolumn{3}{c|}{\textbf{w/ CC}}\\
    && & Pos. F1 & Neg. F1 & macro F1 & Pos. F1 & Neg. F1 & macro F1\\
    
    
    \midrule
    \multirow{4}{*}{\rotatebox{90}{\textbf{\small{Fold 1}}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{DD}}}} &  Base & \textbf{56.64} & 85.13 & \textbf{70.88} & 64.28 & \textbf{88.74} & 76.51 \\
   & &  Large & 50.48 & \textbf{87.35} & 68.91 & \textbf{66.23} & 87.89 & \textbf{77.06} \\
\cmidrule{2-9}
   & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{IEMO}}}} &  Base & 25.98 & 90.73 & 58.36 & 28.02 & \textbf{95.67} & 61.85\\
  &  &  Large & \textbf{32.34} & \textbf{95.61} & \textbf{63.97} & \textbf{40.83} & \textbf{95.68} & \textbf{68.26} \\
  
    \midrule
    \multirow{4}{*}{\rotatebox{90}{\textbf{\small{}}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{DD}}}} &  Base & 93.12 & - & - & 92.64 & - & - \\
   & &  Large & 98.87 & - & - & 97.78 & - & - \\
\cmidrule{2-9}
   & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{IEMO}}}} &  Base & 71.98 & - & - & 58.52 & - & - \\
  &  &  Large & 73.92 & - & - & 74.56 & - & - \\


    
\bottomrule
   \end{tabular}
  }
  \caption{\footnotesize{Results for Causal Emotion Entailment task on the test sets of \RECCONDADD{} and \RECCONDAIE{}. Class wise F1 score and the overall macro F1 scores are reported. All scores reported at best macro F1 scores. DD  \RECCONDADD{}; IEMO  \RECCONDAIE{}. All models are RoBERTa-based models.}}
  \label{tab:cus}
\end{table}



\subsection{Subtask 2: Causal Emotion Entailment}
\label{sec:cus}

The \textit{Causal Emotion Entailment} is a simpler version of the span extraction task. In this task, given a target non-neutral utterance (), the goal is to predict which particular utterances in the conversation history  are responsible for the non-neutral emotion in the target utterance. Following the earlier setup, we formulate this task with and without historical conversational context.

\subsubsection{Subtask Description}

\paragraph{With Conversational Context (w/ CC)}:
We consider the historical conversational context  of the target utterance , and posit the problem as a triplet classification task. Here the tuple (, , ) is aimed to be classified as positive, . For the negative example, the tuple (, , ) should be classified as negative as .

\paragraph{Without Conversational Context (w/o CC)}:
We posit this problem as a binary sentence pair classification task, where (, ) should be classified as positive as . For the negative example (, ) where , the classification output should be negative.



\subsubsection{Models}
Similar to Subtask 1, we use transformer-based models to benchmark this task. We use a \code{<CLS>} token and the emotion label  of the target utterance  in front, and join the pair or triplet elements with \code{<SEP>} in between to create the input. 
The classification is performed from the corresponding final layer vector of the \code{<CLS>} token. We use the following models:

\paragraph{RoBERTa Base / Large}: We use the \code{roberta-base/-large} models from~\citep{liu2019roberta} as the baselines.



\subsubsection{Evaluation Metrics}
We use F1 score for both positive and negative examples, denoted as Pos. F1 and Neg. F1 respectively. We also report the overall macro F1.

\subsection{Main Results}
\label{sec:results}
\cref{tab:cse} reports the experimental results of the causal span extraction task where SpanBERT obtains the best performance in both \RECCONDADD{} and \RECCONDAIE{}. SpanBERT outperforms RoBERTa Base in EM, and F1 metrics. However, the performance of SpanBERT is worse for negative examples, which consequently results in a lower F1 score compared to RoBERTa Base model in both the datasets under ``w/o CC" setting. Contrary to this, the performance of the SpanBERT in the presence of context (w/ CC) is consistently higher than RoBERTa Base with respect to all the metrics in \RECCONDADD{}.


In \cref{tab:cus}, we report the performance of the Causal Emotion Entailment task. Under the ``w/o CC'' setting, in Fold , RoBERTa Base outperforms RoBERTa Large by \% in \RECCONDADD{}. In contrast to this, in \RECCONDAIE{}, RoBERTa Large performs better and beats RoBERTa Base by \% in Fold . On the other hand, RoBERTa Large outperforms RoBERTa Base in both \RECCONDADD{} and \RECCONDAIE{} under the ``w/ CC'' setting. The performance in \RECCONDAIE{} is consistently worse than in \RECCONDADD{} under various settings in both subtask 1 and 2. We reckon this can be due to multiple reasons mentioned in~\cref{sec:dataset_diffs}, making the task harder on the IEMOCAP split.



We have also analyzed the performance of the baseline models on the utterances having one or multiple causes. The models consistently perform better for the utterances having only one causal span compared to the ones having multiple causes (\% on an average calculated over all the settings and models).

In the test data of Fold , approximately 38\% of the UCS pairs (which we call as  ) have their causal spans lie within the target utterances. In \cref{tab:cse} and \ref{tab:cus}, we report the results on . According to these results, the models perform significantly better on such UCS pairs under all the settings in both the subtasks. 

The models leverage contextual information for both the subtasks in the ``w/ CC'' setting which substantially improves the performance of the non-contextual (refer to the ``w/o CC'' setting) counterpart. In this setting, SpanBERT obtains the best performance for positive examples in both \RECCONDADD{}, and \RECCONDAIE{}. On the other hand, in the same setting, RoBERTa Large outperforms RoBERTa Base and achieves the best performance in subtask 2.


The low scores of the models in the subtask  and  depicts the difficulty of the tasks. As such, we see a significant room for model improvement in these two subtasks of \RECCON{}.

\section{Analyses And Discussions} \label{sec:analysis}
To further analyze the performance obtained by the models, besides Fold 1, we adopt two more strategies to create the negative examples.
\begin{enumerate}[leftmargin=*]
\item \textbf{Fold 2:} In this scheme, we randomly sample the non-causal utterance  along with the corresponding historical conversational context  from another dialogue in the dataset to create a negative example.
\item \textbf{Fold 3:} This is similar to Fold 2 with a constraint. In this case, a non-causal utterance  along with its historical conversational context  from the other dialogue is only sampled when its emotion matches the emotion of the target utterance  to construct a negative example.
\end{enumerate}
Note that unlike Fold 1, a negative example in Fold 2 and 3 comprising a non-causal utterance  and a target utterance  belong to different dialogues. For the cases where the causal spans do not lie in the target utterance, we remove the target utterance from its historical context when creating a positive example in Fold 2 and 3. As a result, it helps to prevent the models from learning any trivial patterns. The statistics of Fold 2 and 3 are shown in \cref{tab:finalstat}.

\begin{table}[t!]
  \centering
\resizebox{\linewidth}{!}{
\begin{tabular}{|lll|cccc|cccc|}
    \toprule
   \multicolumn{3}{|c|}{\textbf{Model}} & \multicolumn{4}{c|}{\textbf{w/o CC}} & \multicolumn{4}{c|}{\textbf{w/ CC}}\\
   & & & EM & F1 & F1 &  & EM & F1  & F1 &  \\
\midrule
   \multirow{4}{*}{\rotatebox{90}{\textbf{\small{Fold 1  Fold 1}}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{DD}}}} & RoBERTa  & 26.82 & 45.99 & \textbf{84.55} & \textbf{73.82} & 32.63 & 58.17 & 85.85 & 75.45\\
   
  &  & SpanBERT & \textbf{33.26} & \textbf{57.03} & 80.03 & 69.78 & \textbf{34.64} & \textbf{60.00} & \textbf{86.02} & \textbf{75.71} \\
    \cmidrule{2-11}
    & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{IEMO}}}} & RoBERTa  & 9.81 & 18.59 & \textbf{93.45} & \textbf{87.60} & 10.19 & 26.88 & \textbf{91.68} & \textbf{84.52}\\
    
  &  & SpanBERT & \textbf{16.20} & \textbf{30.22} & 87.15 & 77.45 & \textbf{22.41}  & \textbf{37.80} & 90.54 & 82.86 \\


      \midrule
       \multirow{4}{*}{\rotatebox{90}{\textbf{\small{Fold 1  Fold 2}}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{DD}}}} & RoBERTa  & 26.82 & 45.99 & 83.52 & 72.66 & 32.95 & 59.02 & 95.36 & 87.63 \\
    &  & SpanBERT & 33.26 & 57.03 & 84.02 & 74.80 & 32.37 & 57.04 & 95.01 & 87.00 \\
    \cmidrule{2-11}
    & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{ IEMO}}}} & RoBERTa  & 9.81 & 18.59 & 92.18 & 85.41  & 10.93 & 28.26 & 95.49 & 90.85 \\
  &  & SpanBERT & 16.20 & 30.22 & 88.63 & 79.80 & 24.07 & 40.57 & 96.28 & 92.41 \\
\midrule
       \multirow{4}{*}{\rotatebox{90}{\textbf{\small{Fold 1  Fold 3}}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{DD}}}} & RoBERTa  & 26.82 & 45.99 & 81.50 & 70.26 & 32.95 & 59.02 & 95.37 & 87.65 \\
  &  & SpanBERT & 33.26 & 57.03 & 79.65 & 69.83 & 32.31 & 56.99 & 94.92 & 86.87 \\
    \cmidrule{2-11}
    & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{ IEMO}}}} & RoBERTa  & 9.81 & 18.59 & 91.82 & 84.83 & 10.93 & 28.26 & 95.47 & 90.81 \\
  &  & SpanBERT & 16.20 & 30.22 & 86.95 & 77.25 & 24.07 & 40.57 & 96.28  & 92.41  \\
\midrule
       \multirow{4}{*}{\rotatebox{90}{\textbf{\small{Fold 2  Fold 2}}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{DD}}}} & RoBERTa  & \textbf{33.26} & 58.44 & 90.14 & 82.19 & 41.61 & 73.57 & 99.98 & 92.04 \\
  &  & SpanBERT & 32.31 & \textbf{58.61} & \textbf{90.20} & \textbf{82.29} & \textbf{41.97} & \textbf{74.85} & 99.94 & \textbf{92.43} \\
    \cmidrule{2-11}
    & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{ IEMO}}}} & RoBERTa  & 15.93 & 31.74 & \textbf{92.93} & \textbf{86.50} & 30.28 & 59.14 & \textbf{99.43} & 94.58 \\
  &  & SpanBERT & \textbf{22.13} & \textbf{38.84} & 90.37 & 82.49 & \textbf{32.50} & \textbf{65.45} & 98.37 & \textbf{95.50} \\
  


      \midrule
       \multirow{4}{*}{\rotatebox{90}{\textbf{\small{Fold 2  Fold 1}}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{DD}}}} & RoBERTa  & 33.26 & 58.44 & 71.29 & 60.45 & 36.06 & 65.04 & 0.19 & 17.12 \\
    &  & SpanBERT & 32.31 & 58.61 & 72.52 & 61.70 & 31.52 & 60.81 & 0.67 & 16.19 \\
    \cmidrule{2-11}
    & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{ IEMO}}}} & RoBERTa  & 15.93 & 31.74 & 90.70 & 82.91 & 22.96 & 46.87 & 4.66 & 6.35 \\
  &  & SpanBERT & 22.13 & 38.84 & 85.03 & 74.34 & 21.85 & 49.18 & 6.36 & 7.40 \\
  
\midrule
       \multirow{4}{*}{\rotatebox{90}{\textbf{\small{Fold 3  Fold 3}}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{DD}}}} & RoBERTa  & 28.72 & 51.32 & \textbf{90.06} & \textbf{82.11} & 41.29 & 74.95 & 99.94 & 92.44 \\
  &  & SpanBERT & \textbf{30.62} & \textbf{54.96} & 89.41 & 81.21 & \textbf{42.61} & \textbf{75.36} & 99.93 & 92.46 \\
    \cmidrule{2-11}
    & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{ IEMO}}}} & RoBERTa  & 14.54 & 26.51 & \textbf{93.68} & \textbf{87.79} & 24.35 & 53.46 & 97.84 & 94.08 \\
  &  & SpanBERT & \textbf{17.41} & \textbf{31.75} & 91.85 & 84.86 & \textbf{32.87} & \textbf{62.70} & \textbf{99.54} & \textbf{95.11} \\
  
\midrule
       \multirow{4}{*}{\rotatebox{90}{\textbf{\small{Fold 3  Fold 1}}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{DD}}}} & RoBERTa  & 28.72 & 51.32 & 75.55 & 64.31 & 37.22 & 69.64 & 0.90 & 18.59 \\
  &  & SpanBERT & 30.62 & 54.96 & 75.49 & 64.46 & 31.94 & 60.81 & 0.15 & 16.00\\
    \cmidrule{2-11}
    & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{ IEMO}}}} & RoBERTa  & 14.54 & 26.51 & 92.33 & 85.61 & 21.20 & 48.34 & 11.42 & 9.76\\
  &  & SpanBERT & 17.41 & 31.75 & 89.41 & 80.94 & 21.48 & 45.49 & 4.01 & 5.84 \\
  
    \bottomrule
   \end{tabular}
  }
\caption{\footnotesize{Results for Causal Span Extraction task on the test sets of \RECCONDADD{} and \RECCONDAIE{}. All scores are in percentage and are reported at best validation F1 scores. RoBERTa  RoBERTa Base; DD  \RECCONDADD{}; IEMO  \RECCONDAIE{}.  Fold   Fold : Trained on Fold , Tested on Fold .}}
  \label{tab:cse2}
\end{table}

The use of context (w/ CC) in the baseline models improves the results (see \cref{tab:cse2} and \ref{tab:cus2}) in Fold 2 and 3 as it highlights the contextual discrepancy or coherence between the target utterance and context which should strongly aid in identifying randomly generated negative samples from the rest. For the positive examples, we achieve a much better score in Fold 2 and 3 as compared to Fold 1 (see \cref{tab:cse} and \cref{tab:cus}) for both ``w/o CC" and ``w/ CC" constraints. However, this does not validate Fold 2 and 3 as better training datasets than Fold 1. We confirm this by training the models on Fold 2 and 3 and evaluating them on Fold 1. These two experiments are denoted with \emph{Fold 2  Fold 1} and \emph{Fold 3  Fold 1}, respectively, and the corresponding results are reported in \cref{tab:cse2} and \ref{tab:cus2}. The outcomes of these experiments, as shown in \cref{tab:cse2} and \ref{tab:cus2}, show abysmal performance by the baseline models on the negative examples in Fold 1. 
This may be ascribed to the fundamental difference between Fold 1 and Fold 2, 3. Negative samples in Fold 2, 3 are easily identifiable, as compared to Fold 1, as all the model needs to do to judge the absence of a causal span in the context is to detect the contextual incoherence of the target utterance with the context. Models fine-tuned on BERT and SpanBERT are expected to perform well at deciding contextual incoherence. Identifying negative samples in Fold 1, however, requires more sophisticated and non-trivial approach as the target utterances are, just as the positive examples, contextually coherent with the context. As such, a model that correlates contextual incoherence with negative samples naturally performs poorly on Fold 1.
The  scores for \emph{Fold 2  Fold 1}, and \emph{Fold 3  Fold 1} modes under both ``w/o CC", and ``w/ CC" settings are adversely affected by the low precision of the models in both the subtasks. In other words, the baseline models in these two modes perform poor in extracting empty spans from the ground truth negative examples in subtask 1 and also classify most of the negative examples as positive in subtask 2.
\begin{table}[t!]
  \centering
\resizebox{\linewidth}{!}{
\begin{tabular}{|lll|ccc|ccc|}
    \toprule
      \multicolumn{3}{|c|}{\textbf{Model}} & \multicolumn{3}{c|}{\textbf{w/o CC}} & \multicolumn{3}{c|}{\textbf{w/ CC}}\\
    && & Pos. F1 & Neg. F1 & macro F1 & Pos. F1 & Neg. F1 & macro F1\\
    
    
\midrule
    \multirow{4}{*}{\rotatebox{90}{\textbf{\small{Fold 1  Fold 1}}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{DD}}}} &  Base & \textbf{56.64} & 85.13 & \textbf{70.88} & 64.28 & \textbf{88.74} & 76.51 \\
   & &  Large & 50.48 & \textbf{87.35} & 68.91 & \textbf{66.23} & 87.89 & \textbf{77.06} \\
\cmidrule{2-9}
   & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{IEMO}}}} &  Base & 25.98 & 90.73 & 58.36 & 28.02 & \textbf{95.67} & 61.85\\
  &  &  Large & \textbf{32.34} & \textbf{95.61} & \textbf{63.97} & \textbf{40.83} & \textbf{95.68} & \textbf{68.26} \\
\midrule
        \multirow{4}{*}{\rotatebox{90}{\textbf{\small{Fold 1  Fold 2}}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{DD}}}} &  Base & \textbf{57.50}  & 82.71  & 70.11  & 59.06  & 86.91  & 72.98 \\
   & &  Large & 56.13  & \textbf{88.33}  & \textbf{72.23}  & \textbf{60.09} & \textbf{88.00} & \textbf{74.04} \\
    \cmidrule{2-9}
   & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{IEMO}}}} &  Base & 32.60  & 89.99  & 61.30  & 27.14  & 94.16  & 60.65 \\
  &  &  Large & \textbf{36.61}  & \textbf{94.60} & \textbf{65.60}  & \textbf{37.59}  & \textbf{94.63}  & \textbf{66.11} \\  
\midrule
    \multirow{4}{*}{\rotatebox{90}{\textbf{\small{Fold 1  Fold 3}}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{DD}}}} &  Base  & 57.52 & 82.72  & 70.12  & 49.30  & 79.27  & 64.29 \\
   & &  Large & \textbf{56.04}  & \textbf{88.28}  & \textbf{72.16}  & \textbf{60.63}  & \textbf{88.30}  & \textbf{74.46} \\
    \cmidrule{2-9}
   & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{IEMO}}}} &  Base & 33.24  & 90.30  & 61.77  & 23.83  & 92.97  & 58.40 \\
  &  &  Large & \textbf{36.55}  & \textbf{94.59}  & \textbf{65.57}  & \textbf{37.87}  & \textbf{94.69}  & \textbf{66.28} \\  
\midrule
        \multirow{4}{*}{\rotatebox{90}{\textbf{\small{Fold 2  Fold 2}}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{DD}}}} &  Base & 76.21 & 91.23 & 83.72 & 89.37 & 95.21 & 92.32 \\
   & &  Large & \textbf{79.52} & \textbf{91.27} & \textbf{85.40} & \textbf{93.05} & \textbf{97.22} & \textbf{95.13} \\
    \cmidrule{2-9}
   & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{IEMO}}}} &  Base & 46.12 & \textbf{93.80} & 69.96 & \textbf{65.09} & \textbf{95.60} & \textbf{80.35} \\
  &  &  Large & \textbf{48.36} & 92.06 & \textbf{70.21} & 61.12 & 95.59 & 78.35 \\
  
  
\midrule
        \multirow{4}{*}{\rotatebox{90}{\textbf{\small{Fold 2  Fold 1}}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{DD}}}} &  Base  & \textbf{52.52} & \textbf{75.51}  & \textbf{64.02}  & 41.86 & 3.25  & 22.55 \\
   & &  Large & 51.57 & 67.58  & 59.57  & \textbf{43.25}  & \textbf{19.95} & \textbf{31.60} \\
    \cmidrule{2-9}
   & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{IEMO}}}} &  Base & \textbf{31.51}  & \textbf{92.09}  & \textbf{61.80}  & 25.22  & 74.69  & 49.96 \\
  &  &  Large & 29.64  & 87.68  & 58.66  & \textbf{26.30} & \textbf{76.44}  & \textbf{51.37} \\
  
\midrule
        \multirow{4}{*}{\rotatebox{90}{\textbf{\small{Fold 3  Fold 3}}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{DD}}}} &  Base & 74.73 & \textbf{90.33} & \textbf{82.53} & 92.64 & 96.99 & 94.81 \\
   & &  Large & \textbf{75.79} & 88.43 & 82.11 & \textbf{93.34} & \textbf{97.23} & \textbf{95.29} \\
    \cmidrule{2-9}
   & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{IEMO}}}} &  Base & \textbf{51.23} & \textbf{93.70} & \textbf{72.46} & \textbf{63.91} & \textbf{94.55} & \textbf{79.23} \\
  &  &  Large & 43.00 & 88.47 & 65.74 & 59.03 & 92.21 & 75.62 \\
  
\midrule
        \multirow{4}{*}{\rotatebox{90}{\textbf{\small{Fold 3  Fold 1}}}} & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{DD}}}} &  Base  & \textbf{52.02} & \textbf{74.59}  & \textbf{63.31}  & 41.64 & 2.99  & 22.31 \\
   & &  Large & 51.53  & 65.76  & 58.65  & \textbf{41.86} & \textbf{4.89} & \textbf{23.38} \\
    \cmidrule{2-9}
   & \multirow{2}{*}{\rotatebox{90}{\textbf{\small{IEMO}}}} &  Base & \textbf{34.74}  & \textbf{91.46}  & \textbf{63.10}  & \textbf{19.13}  & \textbf{54.25}  & \textbf{36.69} \\
  &  &  Large & 27.58  & 84.13  & 55.86  & 18.33 &  48.01  & 33.17 \\
    \bottomrule
   \end{tabular}
  }
  \caption{\footnotesize{Results for Causal Emotion Entailment task on the test sets of \RECCONDADD{} and \RECCONDAIE{}. Class wise F1 scores and the overall macro F1 scores are reported. All scores reported at best macro F1 scores. DD  \RECCONDADD{}; IEMO  \RECCONDAIE{}. All models are RoBERTa-based models. Fold   Fold : Trained on Fold , Tested on Fold .}}
  \label{tab:cus2}
\end{table}

On the other hand, we do not observe any significant performance drop for either negative or positive examples when the models trained in Fold 1 are evaluated in Fold 2 and 3. This affirms the superiority of Fold 1 as a training dataset. Besides, note that Fold 1 is a more challenging and practical choice than the rest of the two folds as in real scenarios, we need to identify causes of emotions within a single dialogue by reasoning over the utterances in it.

\section{Challenges in the Task} 
\label{sec:challenges}

This section identifies several examples that indicate the need for \textbf{complex reasoning} to solve the causal span extraction task. Abilities to accurately reason will help validate if a candidate span is causally linked to the target emotion. We believe these pointers would help further research on this dataset and solving the task in general.

\paragraph{Amount of Spans} 
One of the primary challenges of this task is determining the set of spans that can sufficiently be treated as the cause for a target emotion. The spans should have coverage to be able to formulate logical reasoning steps (performed implicitly by annotators) that include skills such as numerical reasoning (\cref{fig:numerical_reasoning}), amongst others.

\paragraph{Emotional Dynamics} Understanding emotional dynamics in conversations is closely tied with emotion cause identification. As shown in our previous sections, many causal phrases in the dataset depend on the inter-personal event/concept mentions, emotions, and self-influences (sharing causes). We also observe that emotion causes may be present across multiple turns, thus requiring the ability to model long-term information. Emotions of the contextual utterances help in this modeling. In fact, without the emotional information of the contextual utterances, our annotators found it difficult to annotate emotion causes in the dataset. Understanding cordial greetings, conflicts, agreements, and empathy are some of the many scenarios where contextual emotional dynamics play a significant role. 








\begin{figure}[t!]
    \centering
    \includegraphics[width=0.7\columnwidth]{figs/numerical_reasoning.pdf}
    \caption{\footnotesize{In this example, , in utt. , is sad because of failing to negotiate the desired amount to sell a TV. While ``\textit{the price is final}" is a valid causal span, one also needs to identify the discussion where  is ready to pay only \2500.}}
    \label{fig:numerical_reasoning}
\end{figure}

\paragraph{Commonsense Knowledge} Extracting emotion causes in conversations comprises complex reasoning steps and commonsense knowledge is an integral part of this process. The role of commonsense reasoning in emotion cause recognition is more evident when the underlying emotion the cause is latent. Consider the example below:

\begin{exe}
\ex \textit{A (happy)}: Hello, thanks for calling 123 Tech Help, I'm Todd. How can I help you?\\
\textit{B (fear)}: Hello ? Can you help me ? My computer ! Oh man ...
\label{ex:latent}
\end{exe}

In this case,  is happily offering help to . The cause of happiness in this example is due to the event ``\textit{greeting}" or intention to offer help. On the other hand,  is fearful because of his/her \textit{broken computer}. The causes of elicited emotions by both the speakers can only be inferred using commonsense knowledge.

\paragraph{Complex Co-reference} While in narratives, co-references are accurately used and often explicit, it is not the case in dialogues (see~\cref{fig:pronoun_mismatch}).  

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.7\columnwidth]{figs/pronoun_mismatch.pdf}
    \caption{\footnotesize{In this example, the emotion cause for utt. 2 may lie in phrases spoken by (and for) the counterpart () and not the target speaker () i.e., ``\textit{flashy red lines}'' in 's utterance points to the property of the ``\textit{watch}'' that  bought. One needs to infer such co-referential links to extract the correct causal spans.}}
    \label{fig:pronoun_mismatch}
\end{figure}


\paragraph{Exact vs. Perceived Cause} At times, the complex and informal nature of conversations prohibits the extraction of exact causes. In such cases, our annotators extract the spans that can be perceived as the respective cause. These causal spans can be rephrased to represent the exact cause for the expressed emotion. For example,

\begin{exe}
\ex \textit{A (neutral)}: How can I help you Sir?.\\
\textit{B (frustrated)}: I just want my flip phone to work----that's all I need.\\
\label{ex:exact1}
\end{exe}



In the above example, the cause lies in the following sentence---``\textit{I just want my flip phone to work}", with the exact cause meaning---``\textit{My flip phone is not working}". Special dialogue-act labels such as \emph{goal achieved} and \emph{goal not-achieved} can also be adopted to describe such causes.

\paragraph{From Cause Extraction to Causal Reasoning}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.7\columnwidth]{figs/temporal_reasoning.pdf}
    \caption{\footnotesize{In this example, the cause for the happy state of  (utt. 6) is corroborated by three indicated spans. First,  gets happy over receiving a ``\textit{birthday present}" (utt. 3) which is a ``\textit{gold watch}" (utt.4). Then, the emotion evoked by the 4 utterance is propagated into 's next utterance where it is confirmed that  loves the gift (``\textit{I love it!}"). Performing temporal reasoning over these three spans helps understand that  is happy because of liking a present received as a birthday gift.}}
    \label{fig:temporal_reasoning}
\end{figure}

Extracting causes of utterances involve reasoning steps. In this work, we do not ask our annotators to explain the reasoning steps pertaining to the extracted causes. However, one can still sort the extracted causes of an utterance according to their temporal order of occurrence in the dialogue. The resulting sequence of causes can be treated as a participating subset of the reasoning process as shown in \cref{fig:temporal_reasoning}. In the future, this dataset can be extended by including reasoning procedures. However, coming up with an optimal set of instructions for the annotators to code the reasoning steps is one of the major obstacles. \cref{fig:csk_ex} also demonstrates the process of reasoning where utterance  and  are the triggers of \emo{happy} emotion in the utterance . However, the reasoning steps that are involved to extract these causes can be defined as:  is happy because his/her goal to participate in the \textit{house open party} is achieved after the confirmation of  who will organize the \textit{house open party}. This reasoning includes understanding discourse~\cite{chakrabarty-etal-2019-ampersand}, logic and leveraging commonsense knowledge.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.7\columnwidth]{figs/csk.pdf}
    \caption{\footnotesize{An example of emotional reasoning where the \emph{happiness} in utt. 3 is caused by the triggers in utt. 1 and 2.}}
    \label{fig:csk_ex}
\end{figure}

More generally, \textbf{\underline{e}motion \underline{c}ausal \underline{r}easoning \underline{i}n \underline{c}on\-ver\-sa\-tions} extends the task of identifying emotion cause to determining the \textbf{function} and \textbf{explanation} of why the stimuli or triggers evoke the emotion in the target utterance. 


















\section{Conclusion}
In this work, we address the problem of \textbf{R}ecognizing \textbf{E}motion \textbf{C}ause in \textbf{CON}versations and introduce a new dataset---\RECCONDA{}. It is a dialogue-level dataset containing more than  dialogues and  utterance causal span pairs. We identify various emotion types and key challenges that make the task of \RECCON{} extremely challenging. Further, we also propose two subtasks and formulate transformer-based strong baselines to address these tasks. Our proposed dataset only incorporates dyadic conversations. Future work will target the analysis of emotion cause in multi-party settings. We also plan to annotate the reasoning steps involved in identifying causal spans of elicited emotions in conversations.

\bibliography{refs}
\bibliographystyle{acl_natbib}

\appendix


\section{Emotion Causation Variables} \label{sec:controlling_vars_appendix}

Various factors can control elicited emotions in a conversation. We identify some of these factors (see \cref{fig:controlling_vars}) as stated below:

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.7\linewidth]{figs/emo-cause.pdf}
	\caption{Emotions expressed in a dyadic conversation---between person X and Y---are governed by interactions between several variables.}
	\label{fig:controlling_vars}
\end{figure}

\textbf{Topic:} Topic is a key element that governs and drives a conversation. Without knowing the topical information, dialogue understanding can be incomplete and vague.

\textbf{Goal:} The goal of the speakers can directly impact the agency appraisal~\cite{ellsworth2003appraisal} of emotions, e.g., although it can be quite frustrating at times, a customer care agent generally tends to please the customers and try to be nice to them. Goals can often be very implicit in a conversation, and some conversations may not even have any goal, e.g., social casual talk. In this work, as the interlocutors' goals in a conversation are not available as prior information, we do not count on it. However, in several cases, our annotators could infer the goals from the conversational context and utilize that information to find relevant causal spans in the context.

\textbf{Agency ():} Agency controls the basic behavior of a person under varied circumstances. According to the theory of appraisals~\citep{ellsworth2003appraisal} in affective computing, agency can also signify different dimensions, such as values, needs, personality, intents, and more. In our case, we ignore the role of agency in emotion causal reasoning as this information is not available in the dataset. Moreover, our dataset is already pre-annotated with emotion labels; hence the variable  does not play any role in this work.



\textbf{Stimulus: Event, Situation, Experience, Statement, and Opinion ().} This variable is defined as the stimulus or trigger that evokes the emotion. The stimulus could refer to events, situations, opinions, experiences mentioned in the conversational context () (by either of the speakers) or even be based on the counterpart's reaction towards an event cared by the speaker (inter-personal emotional influence). 

Consider the following example where the first utterance by person A () is the context and the second utterance by person B () is the target. We are interested in knowing the cause of 's emotion (\emo{excited} and \emo{happy}) in this target utterance:

\begin{exe}
\ex \textit{A (\emo{excited} and \emo{happy})}: You know I am getting married!\\
\textit{B (\emo{excited} and \emo{happy})}: Wow! that's great news. Who is that lucky person? When is the ceremony?
\end{exe}

In the conversation,  listens to and positively reacts to the event ``\textit{ is getting married}''.  also feels happy due to the same event---``\textit{getting married}''. Here, we could infer that 's emotion is caused either by the reference of the first utterance to the event of getting married, or by the fact that  is happy about getting married---both of which can be considered as stimulus for 's emotions. 

Another example is given below where the cause of 's emotion is the event ``\textit{football match}" and a negative emotion \emo{disgust} indicates 's unsatisfied experience of the match. In contrast,  takes pleasure of the match with \emo{happiness} emotion. 
\begin{exe}
\ex \textit{A (\emo{disgust})}: Such a disappointing football match!\\
\textit{B (\emo{happiness})}: No! I am enjoying it.
\end{exe}
Interestingly in this example, the same event acts as trigger for both the persons and causes two contrasting emotions in them.

The conversations can be dynamic and emotions can be triggered from the situations induced in the conversations. Consider the example below:
\begin{exe}
\ex \textit{A (\emo{neutral})}: How are you? You look a bit lost.\\
\textit{B (\emo{anger})}: Don't bother me.\\
\textit{A (\emo{disgust})}: Okay, I am going.
\label{ex:1}
\end{exe}

In this example,  feels disgusted in utterance 3 because of person B's angry and unexpected response. On another side, one can infer that  gets angry or annoyed to hear person A saying ``You look a bit lost''.

A stimulus can be latent too and may require the ability of commonsense inference to identify. Our annotators were instructed to identify these cases. When identified, our annotators wrote their understanding of the inferred cause in the form of natural language. Consider the example below:

\begin{exe}
\ex \textit{A (\emo{happy})}: Hello, thanks for calling 123 Tech Help, I'm Todd. How can I help you?\\
\textit{B (\emo{fear})}: Hello ? Can you help me ? My computer ! Oh man ...
\label{ex:latent2}
\end{exe}

In this case, person A is happily offering help to customer B. The cause of happiness in this example is due to the event ``greeting" or intention to offer help. On the other hand, person B is fearful because of his/her \textit{broken computer}. The causes of elicited emotions by both the speakers can only be inferred using commonsense knowledge.

\textbf{Awareness and Inclinations ():}  represents background knowledge, prior assumptions if any, pre-existing inter-speaker relations, speaker's knowledge and opinion about the topic, and any other background or external information that are not explicitly present in the conversational history. Such knowledge usually evolves depending on how the speaker experiences the environment and interacts with it. In the process of a conversation, certain sensory or other external events can directly initiate cognition and affect. We call these inputs as . These inputs can also be non-verbal cues.

Affective reactions to these sensory inputs can occur with or without any complex cognitive modeling. When the stimulus is sudden and unexpected, the affective reaction can occur before evaluating and appraising the situation through cognitive modeling. This is called \emph{Affective Primacy}~\citep{Zajonc80feelingand}. For example, our immediate reaction when we encounter an unknown creature in the jungle without evaluating whether it is safe or dangerous.
In our case,~ is unknown and  needs to be guessed from the whole conversation. 

If we refer to example \ref{ex:1}, one can speculate that person A's opinion in utterance 1 may not be the sole reason for person B's anger. Person B may also be in a preexisting bad mood due to some prior incidents that are not captured in the course of the conversation.

\textbf{Elicited emotion ():}  encodes the emotion of the speaker at time . As proposed by the psychology theorist Lazarus in his article~\citep{lazarus1982thoughts}, the emotional-state can be triggered by cognition and thinking, we think in a conversation, this state can be controlled by Topic, Goal, , , and~. 



In this work, we identify the stimuli  that cause an expressed emotion in a conversation. We assume that these stimuli are either mentioned or can be inferred in the conversational context .





\section{Connection to Interpretability of the Contextual Models}
One of the advantages of identifying the causes of emotions in conversations is its role in interpreting a model's predictions. We reckon two situations where emotion cause identification can be useful to verify the interpretability of the contextual emotion recognition models that rely on attention mechanisms to count on the context:

\begin{itemize}[leftmargin=*]
    \item In conversations, utterances may not contain any explicit emotion bearing words or sound neutral on the surface but still carry emotions that can only be inferred from the context. In these cases, one can probe contextual models by dropping the causal utterances that contribute significantly to evoke emotion in the target utterance. It would be interesting to observe whether the family of deep networks that rely on attention mechanisms for context modeling e.g., transformer assign higher probability scores to causal contextual utterances in order to make correct predictions.
    \item As explained in \cref{sec:types}, the cause can be present in the target utterance and the model may not need to cater contextual information to predict the emotion. In such cases, it would be worth checking whether attention-based models assign high probability scores to the spans in the target utterance that contribute to the causes of its emotion. 
\end{itemize}

One should also note that a model does not always need to identify the cause of emotions to make correct predictions. For example, 

\begin{exe}
\ex \textit{A (\emo{happy})}: Germany won the match!\\
\textit{B (\emo{happy})}: That's great!
\end{exe}

Here, a model can predict the emotion of  by just leveraging the cues present in the corresponding utterance. However, the utterance by  is just an expression and the cause of the emotion is an event---``\textit{Germany won the match}". Nonetheless, identifying the causes of emotions expressed in a conversation makes the model trustworthy, interpretable, and explainable. 























\end{document}
