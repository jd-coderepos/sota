
\documentclass{article}





\usepackage[nonatbib,final]{neurips_2020}


\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      

\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{color,xcolor}
\usepackage[caption=false]{subfig}
\usepackage{booktabs}
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}
\usepackage{url}
\usepackage{courier}
\usepackage{caption}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{bbm}

\usepackage{enumitem,kantlipsum} \definecolor{blue}{rgb}{1,0,0}

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\def\v{{\mathbf{v}}}
\def\x{{\mathbf{x}}}
\def\y{{\mathbf{y}}}
\def\z{{\mathbf{z}}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newtheorem{hyp}{Hypothesis}
\newtheorem{prop}{Proposition}[section]
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}

\newcommand{\yonglong}[1]{\textcolor{red}{[Yonglong: #1]}} \newcommand{\chen}[1]{\textcolor{blue}{[Chen: #1]}} \newcommand{\bcom}[1]{\textcolor{magenta}{[Ben: #1]}} \newcommand{\header}[1]{\vskip 0.03in \noindent{\bf #1}}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}

\newcommand{\apbbox}[1]{AP}
\newcommand{\apmask}[1]{AP}
\newcommand{\apkp}[1]{AP}
\newcommand{\apdp}[1]{AP}

\newcommand{\myparagraph}[1]{\vspace{-5pt}\paragraph{#1}}

\definecolor{Gray}{gray}{0.5}
\definecolor{nicergreen}{rgb}{0.13, 0.54, 0.13}
\definecolor{nicered}{rgb}{0.83, 0.16, 0.16}
\definecolor{Highlight}{HTML}{39b54a}  

\newcommand\showdiff[1]{\textbf{\textcolor{nicergreen}{#1}}}
\newcommand\showdiffn[1]{\textcolor{black}{#1}}

\newcommand{\hl}[1]{\textcolor{nicergreen}{#1}}
\newcommand{\cgap}[2]{
\fontsize{6pt}{1em}\selectfont{({#2})}
}
\newcommand{\cgaphl}[2]{
\fontsize{6pt}{1em}\selectfont{\textcolor{nicergreen}{(\textbf{#2})}}
}

\newcommand*{\mathcolor}{}
\def\mathcolor#1#{\mathcoloraux{#1}}
\newcommand*{\mathcoloraux}[3]{\protect\leavevmode
  \begingroup
    \color#1{#2}#3\endgroup
} 
\title{What Makes for Good Views for Contrastive Learning?}

\author{
  Yonglong Tian\\
  \;\;MIT CSAIL\\
  \And
  \;\;\;\;Chen Sun\\
  \;\;\;\;Google, Brown University\\
  \And
  Ben Poole\\
  Google Research\\
  \AND
  Dilip Krishnan\\
  Google Research\\
  \And
  Cordelia Schmid\\
  Google Research\\
  \And
  Phillip Isola\\
  MIT CSAIL\\
}

\begin{document}
\maketitle

\begin{abstract}

Contrastive learning between multiple views of the data has recently achieved state of the art performance in the field of self-supervised representation learning. Despite its success, the influence of different view choices has been less studied. In this paper, we use theoretical and empirical analysis to better understand the importance of view selection, and argue that we should reduce the mutual information (MI) between views while keeping task-relevant information intact.
To verify this hypothesis, we devise unsupervised and semi-supervised frameworks that learn effective views by aiming to reduce their MI. We also consider data augmentation as a way to reduce MI, and show that increasing data augmentation indeed leads to decreasing MI and improves downstream classification accuracy. As a by-product, we achieve a new state-of-the-art accuracy on unsupervised pre-training for ImageNet classification ( top-1 linear readout with a ResNet-50)\footnote{Project page: \url{http://hobbitlong.github.io/InfoMin}}. 

\end{abstract} 
\section{Introduction}

It is commonsense that how you look at an object does not change its identity. Nonetheless, Jorge Luis Borges imagined the alternative. In his short story on \emph{Funes the Memorious}, the titular character becomes bothered that a ``dog at three fourteen (seen from the side) should have the same name as the dog at three fifteen (seen from the front)"~\cite{borges1962funes}. The curse of Funes is that he has a perfect memory, and every new way he looks at the world reveals a percept minutely distinct from anything he has seen before. He cannot collate the disparate experiences.

Most of us, fortunately, do not suffer from this curse. We build mental representations of identity that discard \emph{nuisances} like time of day and viewing angle. The ability to build up \emph{view-invariant} representations is central to a rich body of research on multiview learning. These methods seek representations of the world that are invariant to a family of viewing conditions. Currently, a popular paradigm is contrastive multiview learning, where two views of the same scene are brought together in representation space, and two views of different scenes are pushed apart.

This is a natural and powerful idea but it leaves open an important question: ``which viewing conditions should we be invariant to?" It's possible to go too far: if our task is to classify the time of day then we certainly should not use a representation that is invariant to time. Or, like Funes, we could go not far enough: representing each specific viewing angle independently would cripple our ability to track a dog as it moves about a scene. 

We therefore seek representations with enough invariance to be robust to inconsequential variations but not so much as to discard information required by downstream tasks. 
In contrastive learning, the choice of ``views" is what controls the information the representation captures, as the framework results in representations that focus on the shared information between views~\cite{oord2018representation}. Views are commonly different sensory signals, like photos and sounds \cite{arandjelovic2018objects}, or different image channels \cite{tian2019contrastive} or slices in time \cite{tschannen2019self}, but may also be different ``augmented" versions of the same data tensor~\cite{chen2020simple}. If the shared information is small, then the learned representation can discard more information about the input and achieve a greater degree of invariance against nuisance variables. How can we find the right balance of views that share just the information we need, no more and no less?



We investigate this question in two ways: 1) we demonstrate that the optimal choice of views depends critically on the downstream task. If you know the task, it is often possible to design effective views. 2) We empirically demonstrate that for many common ways of generating views, there is a sweet spot in terms of downstream performance where the mutual information (MI) between views is neither too high nor too low.

Our analysis suggests an ``InfoMin principle". A good set of views are those that share the minimal information necessary to perform well at the downstream task. This idea is related to the idea of minimal sufficient statistics \cite{soatto2014visual} and the Information Bottleneck theory \cite{tishby2000information,alemi2016deep}, which have been previously articulated in the representation learning literature. This principle also complements the already popular ``InfoMax principle"~\cite{linsker1988self}
, which states that a goal in representation learning is to capture as much information as possible about the stimulus. We argue that maximizing information is only useful in so far as that information is task-relevant. Beyond that point, learning representations that throw out information about nuisance variables is preferable as it can improve generalization and decrease sample complexity on downstream tasks \cite{soatto2014visual}.

Based on our findings, we also introduce a semi-supervised method to \emph{learn} views that are effective for learning good representations when the downstream task is known. We additionally demonstrate that the InfoMin principle can be practically applied by simply seeking stronger data augmentation to further reduce mutual information toward the sweet spot. This effort results in state of the art accuracy on a standard benchmark. 

Our contributions include:
\begin{itemize}[leftmargin=5.5mm]
    \item Demonstrating that optimal views for contrastive representation learning are task-dependent.
    \item Empirically finding a U-shaped relationship between an estimate of mutual information and representation quality in a variety of settings.
    \item A new semi-supervised method to learn effective views for a given task.
    \item Applying our understanding to achieve state of the art accuracy of  on the ImageNet linear readout benchmark with a ResNet-50. 
\end{itemize}




 
\section{Related Work}
Recently the most competitive methods for learning representations without labels have been self-supervised contrastive representation learning  \cite{oord2018representation,hjelm2018learning,wu2018unsupervised,tian2019contrastive,sohn2016improved,chen2020simple}. 
These methods learn representations by a ``contrastive'' loss which pushes apart dissimilar data pairs while pulling together similar pairs, an idea similar to exemplar learning~\cite{dosovitskiy2014discriminative}.
Models based on contrastive losses have significantly outperformed other approaches~\cite{zhang2017split,kingma2013auto,pathak2016context,tian2019contrastive,donahue2019large,noroozi2016unsupervised,doersch2015unsupervised,gidaris2018unsupervised,zhang2019aet}.

One of the major design choices in contrastive learning is how to select the similar~(or \emph{positive}) and dissimilar~(or \emph{negative}) pairs. The standard approach for generating positive pairs without additional annotations is to create multiple \emph{views} of each datapoint. For example: luminance and chrominance decomposition~\cite{tian2019contrastive}, randomly augmenting an image twice~\cite{wu2018unsupervised,chen2020simple,bachman2019learning,he2019momentum,ye2019unsupervised,srinivas2020curl,zhao2020distilling,zhuang2019local}, using different time-steps of videos \cite{oord2018representation,zhuang2019unsupervised,sermanet2018time,han2019video,gordon2020watching},  patches of the same image~\cite{isola2016cooccurrence,oord2018representation,hjelm2018learning}, multiple sensory data~\cite{morgado2020audio,chung2019perfect,patrick2020multi}, text and its context~\cite{mikolov2013distributed,yang2019xlnet,logeswaran2018efficient,Kong2020A}, or representations of student and teacher models~\cite{Tian2020Contrastive}. Negative pairs can be randomly chosen images/videos/texts. 
Theoretically, we can think of the positive pairs as coming from a joint distribution over views , and the negative pairs from a product of marginals . The contrastive learning objective InfoNCE~\cite{oord2018representation} (or Deep InfoMax~\cite{hjelm2018learning}) is developed to maximize a lower bound on the mutual information between the two views . Such connection has been discussed further in~\cite{poole2019variational,tschannen2019mutual}.

Leveraging labeled data in contrastive representation learning has been shown to guide representations towards task-relevant features that improve performance \cite{zhai2019s4l,henaff2019data,khosla2020supervised,wu2018improving}. Here we use labeled data to learn better views, but still perform contrastive learning using only unlabeled data. Future work could combine these approaches to leverage labels for both view learning and representation learning. Besides, previous work~\cite{asano2019critical} has studied the effects of augmentation with different amount of images.




 


\section{What Are the Optimal Views for Contrastive Learning?}

In this section, we first introduce the standard multiview contrastive representation learning formulation, and then investigate what would be the optimal views for contrastive learning.

\subsection{Multiview Contrastive Learning}

Given two random variables  and , the goal of contrastive learning is to learn a parametric function to discriminate between samples from the empirical joint distribution  and samples from the product of marginals . The resulting function is an estimator of the mutual information between  and , and the InfoNCE loss \cite{oord2018representation} has been shown to maximize a lower bound on . In practice, given an anchor point , the InfoNCE loss is optimized to score the correct positive  higher compared to a set of  distractors :




Minimizing this loss equivalently maximizes a lower bound (a.k.a. ) on , i.e., . 
In practice,  and  are two views of the data , such as different augmentations of the same image~\cite{wu2018unsupervised,bachman2019learning,he2019momentum,chen2020improved,chen2020simple}, different image channels~\cite{tian2019contrastive}, or video and text pairs~\cite{sun2019contrastive,miech2019end,li2020learning}. The score function  typically consists of two encoders ( for  and  for ), which may or may not share parameters depending on whether  and  are from the same domain. The resulting representations are  and  (see Fig.~\ref{fig:schematic1}a). 



\begin{definition}\label{def:sufficient}
(Sufficient Encoder) The encoder  of  is sufficient in the contrastive learning framework if and only if .
\end{definition}

\noindent Intuitively, the encoder  is sufficient if the amount of information in  about  is lossless during the encoding procedure. In other words,  has kept all the information that the contrastive learning objective requires. Symmetrically,  is sufficient if .


\begin{definition}\label{def:min_sufficient}
(Minimal Sufficient Encoder) A sufficient encoder  of  is minimal if and only if  that is sufficient.
\end{definition}

\noindent Among those encoders which are sufficient, the minimal ones only extract relevant information of the contrastive task and throw away other irrelevant information. This is appealing in cases where the views are constructed in a way that all the information we care about is shared between them. 



\begin{figure}[t]
\centering
\small
\includegraphics[width=1.0\linewidth]{fig/overall.pdf}
\caption{\small {\bf(a)} Schematic of multiview contrastive representation learning, where an image is split into two views, and passed through two encoders to learn an embedding where the views are close relative to views from other images. {\bf(b)} When we have views that maximize  and  (how much task-relevant information is contained) while minimizing  (information shared between views, including both task-relevant and irrelevant information), there are three regimes: \emph{missing information} which leads to degraded performance due to ; \emph{excess noise} which worsens generalization due to additional noise; \emph{sweet spot} where the only information shared between  and  is task-relevant and such information is complete. }
\vspace{-10pt}
\label{fig:schematic1}
\end{figure} 
The representations learned in the contrastive framework are typically used in a separate downstream task. To characterize what representations are good for a downstream task, we define the optimality of representations. To make notation simple, we use  to mean it can be either  or .
\begin{definition}\label{def:optimal_rep_for_task}
(Optimal Representation of a Task) For a task  whose goal is to predict a semantic label  from the input data , the optimal representation  encoded from  is the minimal sufficient statistic with respect to .
\end{definition}


\noindent This says a model built on top of  has all the information necessary to predict  as accurately as if it were to access . Furthermore,  maintains the smallest complexity, i.e., containing no other information besides that about , which makes it more generalizable~\cite{soatto2014visual}. We refer the reader to \cite{soatto2014visual} for a more in depth discussion about optimal visual representations and minimal sufficient statistics.


\subsection{Three Regimes of Information Captured}\label{sec:three_phase}



As our representations  are built from our views and learned by the contrastive objective with the assumption of minimal sufficient encoders, the amount and type of information shared between  and  (i.e., ) determines how well we perform on downstream tasks. As in information bottleneck \cite{tishby2000information}, we can trace out a tradeoff between how much information our views share about the input, and how well our learned representation performs at predicting  for a task. Depending on how our views are constructed, we may find that we are keeping too many irrelevant variables while discarding relevant variables,
leading to suboptimal performance on the information plane. Alternatively, we can find the views that maximize  and  (how much information is contained about the task label) while minimizing  (how much information is 
shared about the input, including both task-relevant and irrelevant information). Even in the case of these optimal traces, there are three regimes of performance we can consider that are depicted in Fig.~\ref{fig:schematic1}b, and have been discussed previously in information bottleneck literature \cite{tishby2000information,alemi2016deep,fischer2020conditional}:
\vspace{-3pt}
\begin{enumerate}[leftmargin=0.5cm]
    \item \emph{Missing information}: When , there is information about the task-relevant variable that is discarded by the view, degrading performance.
    \item \emph{Sweet spot}: When , the only information shared between  and  is task-relevant, and there is no irrelevant noise. 
    \item \emph{Excess noise}: As we increase the amount of information shared in the views beyond , we begin to include additional information that is irrelevant for the downstream task. This can lead to worse generalization on the downstream task \cite{alemi2016deep,shamir2010learning}.
\end{enumerate}
\vspace{-3pt}

We hypothesize that the best performing views will be close to the sweet spot: containing as much task-relevant information while discarding as much irrelevant information in the input as possible.  More formally, the following InfoMin proposition articulates which views are optimal supposing that we know the specific downstream task  in advance. The proof is in Section A.2 of the Appendix.
\begin{prop}\label{theory:infomin}
Suppose  and  are minimal sufficient encoders. Given a downstream task  with label , the optimal views created from the data  are , subject to . Given , the representation  (or ) learned by contrastive learning is optimal for  (Def~\ref{def:optimal_rep_for_task}), thanks to the minimality and sufficiency of  and .
\end{prop}
Unlike in information bottleneck, for contrastive learning we often do not have access to a fully-labeled training set that specifies the downstream task in advance, and thus evaluating how much task-relevant information is contained in the views and representation at training time is challenging. Instead, the construction of views has typically been guided by domain knowledge that alters the input while preserving the task-relevant variable.

\subsection{View Selection Influences Mutual Information and Accuracy}\label{sec:view_selection}

\begin{figure}[t]
\centering
\includegraphics[width=0.9\linewidth]{fig/transfer.pdf}
\vspace{-10pt}
\caption{\small As the mutual information between views is changed, information about the downstream task (green) and nuisance variables (red) can be selectively included or excluded, biasing the learned representation. {\bf (a)} depicts a scenario where views are chosen to preserve downstream task information between views while throwing out nuisance information, while in {\bf (b)} reducing MI always throws out information relevant for the task leading to decreasing performance as MI is reduced.}
\label{fig:view_transfer}
\end{figure}
 The above analysis suggests that transfer performance will be upper-bounded by a reverse-U shaped curve (Fig.~\ref{fig:schematic1}b, right), with the sweet spot at the top of the curve. In theory, when the mutual information between views is changed, information about the downstream task and nuisance variables can be selectively included or excluded, biasing the learned representation, as shown in Fig.~\ref{fig:view_transfer}. The upper-bound reverse-U might not be reached if views are selected that share noise rather than signal. But practically, a recent study~\cite{tian2019contrastive} suggests that the reverse-U shape is quite common. Here we show several examples where reducing  improves downstream accuracy. We use  as a neural proxy for , and note it depends on network architectures. Therefore for each plot in this paper, we only vary the input views while keeping other settings the same, to make the results comparable. 


\begin{figure}[ht]
\centering
\includegraphics[width=0.9\linewidth]{fig/spatial_v2.pdf}
\vspace{-7pt}
\caption{\small We create views by using pairs of image patches at various offsets from each other. As  is reduced, the downstream task accuracy firstly increases and then decreases, leading to a reverse-U shape.}
\label{fig:spatial}
\end{figure}
 \label{sec:spatial}

\textbf{Example 1:} Reducing  with spatial distance. We create views by randomly cropping two patches of size 64x64 from the same image with various offsets. Namely, one patch starts at position  while the other starts at , with  randomly generated. We increase  from  to , and sample patches from inside high resolution images in the DIV2K dataset~\cite{agustsson2017ntire}. After contrastive training stage, we evaluate on STL-10 and CIFAR-10 by freezing the encoder and training a linear classifier. The plots in Fig.~\ref{fig:spatial} shows the \emph{Mutual Information} v.s. \emph{Accuracy}. The results show that the reverse-U curve is consistent across both STL-10 and CIFAR-10. We can identify the sweet spot at . More details are provided in Appendix.

\begin{figure}[ht]
\centering
\includegraphics[width=0.9\linewidth]{fig/color_v2.pdf}
\vspace{-7pt}
\caption{\small We build views by splitting channels of different color spaces. As  decreases, the accuracy on downstream tasks (STL-10 classification, NYU-v2 segmentation) improves.}
\label{fig:color}
\end{figure}
 \textbf{Example 2:} Reducing  with different color spaces. The correlation between channels may vary significantly across different color spaces. We follow~\cite{tian2019contrastive,zhang2017split} to split each color space into two views, such as  and . We perform contrastive learning on STL-10, and measure the representation quality by linear classification accuracy on the STL-10 and segmentation performance on NYU-V2~\cite{Silberman:ECCV12} images. As shown in Fig.~\ref{fig:color}, the downstream performance keeps increasing as  decreases for both classification and segmentation.  Here we do not observe the the left half of the reverse U-shape, but in Sec.~\ref{sec:learn} we will show a learning method that generates color spaces which reveal the full shape and touch the sweet spot.


\subsection{Data Augmentation to Reduce Mutual Information between Views}

Multiple views can also be generated through augmenting an input in different ways. We can unify several recent contrastive learning methods through the perspective of view generation: despite differences in architecture, objective, and engineering tricks, all recent contrastive learning methods create two views  and  that implicitly follow the InfoMin principle. Below, we consider several recent works in this framework:


\textbf{InstDis~\cite{wu2018unsupervised} and MoCo~\cite{he2019momentum}.} These two methods create views by applying a stochastic data augmentation function twice to the same input: (1) sample an image  from the empirical distribution ; (2) sample two independent transformations  from a distribution of data augmentation functions ; (3) let  and .

\textbf{CMC~\cite{tian2019contrastive}.} CMC further split images across color channels such that  is the first color channel of , and  is the last two channels of . By this design,  is theoretically guaranteed, and we observe that CMC performs better than InstDis.

\textbf{PIRL~\cite{misra2019self}.} PIRL keeps  but transforms the other view  with random JigSaw shuffling  to get . Similary we have  as  introduces randomness.

\textbf{SimCLR ~\cite{chen2020simple}.} Despite other engineering techniques and tricks, SimCLR uses a stronger class of augmentations , which leads to smaller mutual information between the two views than InstDis.

\textbf{CPC~\cite{oord2018representation}.} Different from the above methods that create views at the image level, CPC gets views ,  from local patches with strong data augmentation (e.g., RA~\cite{cubuk2019randaugment}) which results in smaller . As in Sec.~\ref{sec:spatial}, cropping views from disjoint patches also reduces .



\begin{figure}[t]
\centering
\includegraphics[width=1.0\linewidth]{fig/parameterized_aug.pdf}
\caption{The reverse U-shape traced out by parameters of individual augmentation functions.}
\vspace{-10pt}
\label{fig:parametric}
\end{figure}
 \begin{table}[h]
\small
\centering
\vspace{-10pt}
\caption{\small Single-crop ImageNet accuracies (\%) of linear classifiers~\protect\cite{zhang2016colorful} trained on representations learned with different contrastive methods using ResNet-50~\protect\cite{he2016deep}. InfoMin Aug. refers to data augmentation using \protect\emph{RandomResizedCrop}, \protect\emph{Color Jittering}, \protect\emph{Gaussian Blur}, \protect\emph{RandAugment}, \protect\emph{Color Dropping}, and a \emph{JigSaw} branch as in PIRL~\protect\cite{misra2019self}. * indicates splitting the network into two halves.}
\label{tab:imagenet}
\vspace{5pt}
\begin{tabular}{llccccc}
\toprule
Method & Architecture        & Param. & Head & Epochs & Top-1 & Top-5 \\ \midrule
InstDis~\cite{wu2018unsupervised}   & ResNet-50    & 24  & Linear & 200  & 56.5  & -    \\
Local Agg.~\cite{zhuang2019local}   & ResNet-50    & 24  & Linear & 200  & 58.8  & -     \\
CMC~\cite{tian2019contrastive}      & ResNet-50*   & 12  & Linear & 240  & 60.0  & 82.3     \\
MoCo~\cite{he2019momentum}          & ResNet-50    & 24  & Linear & 200  & 60.6  & -     \\
PIRL~\cite{misra2019self}           & ResNet-50    & 24  & Linear & 800  & 63.6  & -  \\
CPC v2~\cite{henaff2019data}        & ResNet-50    & 24  & - &  -   & 63.8  & 85.3  \\
SimCLR~\cite{chen2020simple}        & ResNet-50    & 24   & MLP & 1000 & 69.3    & 89.0    \\
\midrule
InfoMin Aug. (Ours)& ResNet-50   & 24  & MLP    & 200 & 70.1 & 89.4 \\
InfoMin Aug. (Ours)& ResNet-50   & 24  & MLP    & 800 & \textbf{73.0} & \textbf{91.1}\\
\bottomrule
\vspace{-10pt}
\end{tabular}
\end{table} 
Besides, we also analyze how changing the magnitude parameter of individual augmentation functions trances out reverse-U shapes. We consider \emph{RandomResizedCrop} and \emph{Color Jittering}. For the former, a parameter \texttt{c} sets a low-area cropping bound, and smaller \texttt{c} indicates stronger augmentation. For the latter, a parameter \texttt{x} is adopted to control the strengths. The plots on ImageNet~\cite{deng2009imagenet} are shown in Fig.~\ref{fig:parametric}, where we identify a sweet spot at  for \emph{Color Jittering} and  for \emph{RandomResizedCrop}.

Motivated by the InfoMin principle, we propose a new set of data augmentation, called \emph{InfoMin Aug}. In combination of the JigSaw strategy proposed in PIRL~\cite{misra2019self}, our InfoMin Aug achieves  top-1 accuracy on ImageNet linear readout benchmark with ResNet-50, outperforming SimCLR~\cite{chen2020simple} by nearly , as shown in Table~\ref{tab:imagenet}. Besides, we also found that transferring our unsupervisedly pre-trained models to PASCAL VOC object detection and COCO instance segmentation consistently outperforms supervised ImageNet pre-training. More details and results are in Appendix.



\begin{table}[t]
\small
\centering
\vspace{-15pt}
\caption{\small \label{tbl:coco_r50_fpn} Results of object detection and instance segmentation fine-tuned on COCO. We adopt Mask R-CNN \textbf{R50-FPN}, and report the bounding box AP and mask AP on \texttt{val2017}. In the brackets are the gaps to the ImageNet supervised pre-training counterpart. For fair comparison, InstDis~\protect\cite{wu2018unsupervised}, PIRL~\protect\cite{misra2019self}, MoCo~\protect\cite{he2019momentum}, and InfoMin are all pre-trained for \textbf{200} epochs. In green are the gaps of at least {\fontsize{8pt}{1em}\selectfont \hl{\textbf{0.5}}} point.}
\subfloat[Mask R-CNN, R50-FPN, \textbf{1} schedule]{
\begin{tabular}{c|ccc|ccc}
\toprule
pre-train &
\apbbox{~} &
\apbbox{50} &
\apbbox{75} &
\apmask{~} &
\apmask{50} &
\apmask{75} \\
\hline
\color{Gray}random init.       & \color{Gray}32.8 & \color{Gray}50.9 & \color{Gray}35.3 & \color{Gray}29.9 & \color{Gray}47.9 & \color{Gray}32.0 \\
supervised        & 39.7 & 59.5 & 43.3 & 35.9 & 56.6 & 38.6 \\
\hline
InstDis~\cite{wu2018unsupervised}             & 
38.8\cgap{-}{0.9} & 58.4\cgap{-}{1.1} & 42.5\cgap{-}{0.8} & 35.2\cgap{-}{0.7} & 55.8\cgap{-}{0.8} & 37.8\cgap{-}{0.8}\\
PIRL~\cite{misra2019self}             & 
38.6\cgap{-}{1.1} & 58.2\cgap{-}{1.3} & 42.1\cgap{-}{1.2} & 35.1\cgap{-}{0.8} & 55.5\cgap{-}{1.1} & 37.7\cgap{-}{0.9}\\
MoCo~\cite{he2019momentum}             & 
39.4\cgap{-}{0.3} & 59.1\cgap{-}{0.4} & 42.9\cgap{-}{0.4} & 35.6\cgap{-}{0.3} & 56.2\cgap{-}{0.4} & 38.0\cgap{-}{0.6}\\
MoCo v2~\cite{chen2020improved}        &
40.1\cgap{+}{0.4} & 59.8\cgap{+}{0.3} & 44.1\cgaphl{+}{0.8} & 36.3\cgap{+}{0.4} & 56.9\cgap{+}{0.3} & 39.1\cgaphl{+}{0.5}\\
InfoMin Aug.           & 
40.6\cgaphl{+}{0.9} & 60.6\cgaphl{+}{1.1} & 44.6\cgaphl{+}{1.3} & 36.7\cgaphl{+}{0.8} & 57.7\cgaphl{+}{1.1} & 39.4\cgaphl{+}{0.8} \\
\bottomrule
\end{tabular}
}

\vspace{-5pt}

\subfloat[Mask R-CNN, R50-FPN, \textbf{2} schedule]{
\begin{tabular}{c|ccc|ccc}
\toprule
pre-train &
\apbbox{~} &
\apbbox{50} &
\apbbox{75} &
\apmask{~} &
\apmask{50} &
\apmask{75} \\
\hline
\color{Gray}random init.       & \color{Gray}38.4 & \color{Gray}57.5 & \color{Gray}42.0 & \color{Gray}34.7 & \color{Gray}54.8 & \color{Gray}37.2 \\
supervised        & 41.6 & 61.7 & 45.3 & 37.6 & 58.7 & 40.4 \\
\hline
InstDis~\cite{wu2018unsupervised}             & 
41.3\cgap{-}{0.3} & 61.0\cgap{-}{0.7} & 45.3\cgap{+}{0.0} & 37.3\cgap{-}{0.3} & 58.3\cgap{-}{0.4} & 39.9\cgap{-}{0.5}\\
PIRL~\cite{misra2019self}             & 
41.2\cgap{-}{0.4} & 61.2\cgap{-}{0.5} & 45.2\cgap{-}{0.1} & 37.4\cgap{-}{0.2} & 58.5\cgap{-}{0.2} & 40.3\cgap{-}{0.1}\\
MoCo~\cite{he2019momentum}               & 
41.7\cgap{+}{0.1} & 61.4\cgap{-}{0.3} & 45.7\cgap{+}{0.4} & 37.5\cgap{-}{0.1} & 58.6\cgap{-}{0.1} & 40.5\cgap{+}{0.1}\\
MoCo v2~\cite{chen2020improved}        &
41.7\cgap{+}{0.1} & 61.6\cgap{-}{0.1} & 45.6\cgap{+}{0.3} & 37.6\cgap{+}{0.0} & 58.7\cgap{+}{0.0} & 40.5\cgap{+}{0.1}\\
InfoMin Aug.           & 
42.5\cgaphl{+}{0.9} & 62.7\cgaphl{+}{1.0} & 46.8\cgaphl{+}{1.5} & 38.4(\cgaphl{+}{0.8} & 59.7\cgaphl{+}{1.0} & 41.4\cgaphl{+}{1.0} \\
\bottomrule
\end{tabular}
}
\vspace{-20pt}
\end{table}
 
\vspace{-3pt}
One goal of unsupervised pre-training is to learn transferable representations that are beneficial for downstream tasks. The rapid progress of many vision tasks in past years can be ascribed to the paradigm of fine-tuning models that are initialized from supervised pre-training on ImageNet. When transferring to PASCAL VOC~\cite{everingham2010pascal} and COCO~\cite{lin2014microsoft}, we found our InfoMin pre-training consistently outperforms supervised pre-training as well as other unsupervised pre-training methods. 

\textbf{COCO Object Detection/Segmentation.} Feature normalization has been shown to be important during fine-tuning~\cite{he2019momentum}. Therefore, we fine-tune the backbone with Synchronized BN (SyncBN~\cite{peng2018megdet}) and add SyncBN to newly initialized layers (e.g., FPN~\cite{lin2017feature}). Table~\ref{tbl:coco_r50_fpn} reports the bounding box AP and mask AP on \texttt{val2017} on COCO, using the Mask R-CNN~\cite{he2017mask} R50-FPN pipeline. All results are reported on \texttt{Detectron2}~\cite{wu2019detectron2}. 

\vspace{-3pt}

We have tried different popular detection frameworks with various backbones, extended the fine-tuning schedule (e.g., \textbf{6} schedule), and compared InfoMin ResNeXt-152~\cite{xie2017aggregated} trained on ImageNet-1k with supervised ResNeXt-152 trained on ImageNet-5k (6 times larger than ImageNet-1k). In all cases, InfoMin consistently outperforms supervised pre-training. Please see Section~\ref{sec:coco} for more detailed comparisons.

\begin{table}[t]
\small
\centering
\setlength{\tabcolsep}{10pt}
\vspace{-5pt}
\caption{ \small \label{tab:pascal} Pascal VOC object detection. All contrastive models are pretrained for \textbf{200} epochs on ImageNet for fair comparison. We use Faster R-CNN R50-\textbf{C4} architecture for object detection. APs are reported using the average of 5 runs. * we use numbers from~\protect\cite{he2019momentum} since the setting is exactly the same.}
\begin{tabular}{l|ccc|c}
\toprule
pre-train & AP & AP & AP & ImageNet Acc(\%)\\
\hline
\color{Gray}random init.*        & \color{Gray}60.2 & \color{Gray}33.8 & \color{Gray}33.1 & \color{Gray}{-} \\
supervised*          & 81.3 & 53.5 & 58.8 & 76.1 \\
\hline
InstDis             & 80.9~\showdiffn{(-0.4)} & 55.2~\showdiff{(+1.7)} & 61.2~\showdiff{(+2.4)} & 59.5 \\
PIRL                & 81.0~\showdiffn{(-0.3)} & 55.5~\showdiff{(+2.0)} & 61.3~\showdiff{(+2.5)} & 61.7 \\
MoCo*               & 81.5~\showdiff{(+0.2)} & 55.9~\showdiff{(+2.4)} & 62.6~\showdiff{(+3.8)} & 60.6 \\
MoCo v2             & 82.4~\showdiff{(+1.1)} & 57.0~\showdiff{(+3.5)} & 63.6~\showdiff{(+4.8)} & 67.5 \\
InfoMin Aug. (ours)      & \textbf{82.7}~\showdiff{(+1.4)} & \textbf{57.6}~\showdiff{(+4.1)} & \textbf{64.6}~\showdiff{(+5.8)} & \textbf{70.1} \\
\bottomrule
\end{tabular}
\vspace{-15pt}
\end{table} 
\textbf{Pascal VOC Object Detection.} We strictly follow the setting introduced in~\cite{he2019momentum}. Specifically, We use Faster R-CNN~\cite{ren2015faster} with R50-C4 architecture. We fine-tune all layers with 24000 iterations, each consisting of 16 images. The results are reported in Table~\ref{tab:pascal}.













\section{Learning views for contrastive learning}
Hand-designed data augmentation is an effective method for generating views that have reduced mutual information and strong transfer performance for images. However, as contrastive learning is applied to new domains, generating views through careful construction of data augmentation strategies may prove ineffective. Furthermore, the types of views that are useful depend on the downstream task. Here we show the task-dependence of optimal views on a simple toy problem, and propose an unsupervised and semi-supervised learning method to \emph{learn} views from data.
\subsection{Optimal Views Depend on the Downstream Task}
To understand how the choice of views impact the representations learned by contrastive learning, we construct a toy dataset that mixes three tasks. We build our toy dataset by combining \textbf{Moving-MNIST}~\cite{srivastava2015unsupervised} (consisting of videos where digits move inside a black canvas with constant speed and bounce off of image boundaries), with a fixed background image sampled from the STL-10 dataset~\cite{coates2011analysis}. We call this dataset \textbf{Colorful Moving-MNIST}, which consists of three factors of variation in each frame: \emph{the class of the digit}, \emph{the position of the digit}, and \emph{the class of background image} (see Appendix for more details). Here we analyze how the choice of views impacts which of these factors are extracted by contrastive learning.

\begin{figure}[t]
\centering
\includegraphics[width=0.85\linewidth]{fig/mnist.pdf}
\caption{\small Illustration of the Colorful-Moving-MNIST dataset. In this example, the first view  is a sequence of frames containing the moving digit, e.g., . The matched second view  share some factor with  that  can predict, while the unmatched view  does not share factor with .}
\vspace{-10pt}
\label{fig:mnist}
\end{figure}
 
\begin{table}[t]
    \setlength{\tabcolsep}{4pt}
	\centering
	\small
	\caption{\small We study how information shared by views  would affect the representation quality, by evaluating on three downstream tasks: digit classification, localization, and background (STL-10) classification. Evaluation for contrastive methods is performed by freezing the backbone and training a linear task-specific head}
    \begin{tabular}{cc|ccc}
    \toprule
    &  & \tabincell{c}{digit cls.  error rate (\%)} & \tabincell{c}{background cls.  error rate (\%)} & \tabincell{c}{digit loc.  error pixels} \\
    \hline
    \multirow{3}{*}{\tabincell{c}{Single \\ Factor}}& \emph{digit} &        
    \textbf{16.8} & 88.6 & 13.6\\
    & \emph{bkgd} &   88.6 & \textbf{51.7} & 16.1\\
    & \emph{pos} &     57.9 & 87.6 & \textbf{3.95}\\
    \hline
    \multirow{4}{*}{\tabincell{c}{Multiple \\ Factors}}& \emph{bkgd, digit, pos} &        88.8 & 56.3 & 16.2\\
    & \emph{bkgd, digit} &   88.2 & 53.9 & 16.3\\
    & \emph{bkgd, pos}   &   88.8 & 53.8 & 15.9\\
    & \emph{digit, pos}  &   \textbf{14.5} & 88.9 & 13.7\\
    \hline
    \multicolumn{2}{c|}{Supervised} &        3.4 & 45.3 & 0.93\\
    \bottomrule
    \end{tabular}
    \vspace{-10pt}
    \label{tbl:mnist_single}
\end{table}
 
\textbf{Setup.} We fix view  as the sequence of past frames . For simplicity, we consider  as a single image, and construct it by referring to frame . One example of visualization is shown in Fig.~\ref{fig:mnist}, and please refer to Appendix for more details.
We consider 3 downstream tasks for an image: (1) predict the digit class; (2) localize the digit; (3) classify the background image (10 classes from STL-10). This is performed by freezing the backbone and training a linear task-specific head. We also provide a ``supervised'' baseline that is trained end-to-end for comparison. 

\textbf{Single Factor Shared.} We consider the case that  and  only share one of the three factors: \emph{digit}, \emph{position}, or \emph{background}. 
We synthesize  by setting one of the three factors the same as  but randomly picking the other two.
In such cases, the mutual information  is either about \emph{digit}, \emph{position}, or \emph{background}. The
results are summarized in Table~\ref{tbl:mnist_single}, which clearly shows that the performance is significantly affected by what is shared between  and . Specifically, if the downstream task is relevant to one factor,  should include that factor rather than others. For example, when  only shares background image with , contrastive learning can hardly learn representations that capture digit class and location. 

\textbf{Multiple Factors Shared.} We further explore how representation quality is changed if  and  share multiple factors. We follow a similar procedure as above to control factors shared by  and , and present the results in Table~\ref{tbl:mnist_single}. We found that one factor can overwhelm another; for instance, whenever \emph{background} is shared, the latent representation leaves out information for discriminating or localizing digits. This might because the information bits of background predominates, and the encoder chooses the background as a ``shortcut'' to solve the contrastive pre-training task. When  and  share \emph{digit} and \emph{position}, the former is preferred over the latter.

 

\subsection{Synthesizing Views with Invertible Generators}\label{sec:learn}


In this section, we design unsupervised and semi-supervised methods that synthesize novel views following the InfoMin principle. Concretely, we extend the color space experiments in Sec.~\ref{sec:view_selection} by learning flow-based models~\cite{dinh2016density,dinh2014nice,kingma2018glow} that transfer natural color spaces into novel color spaces, from which we split the channels to get views. We still call the output of flow-based models as color spaces because the flows are designed to be pixel-wise and bijective (by its nature), which follows the property of color space conversion. After the views have been learned, we perform standard contrastive learning followed by linear classifier evaluation. 

Practically, the flow-based model  is restricted to pixel-wise 1x1 convolutions and ReLU activations, operating independently on each pixel. We try both volume preserving (VP) and non-volume preserving (NVP) flows. For an input image , the splitting over channels is represented as .  signifies the transformed image, i.e., . Experiments are conducted on STL-10, which includes 100k unlabeled and 5k labeled images. More details are in Appendix.

\subsubsection{Unsupervised View Learning: Minimize }

The idea is to leverage an adversarial training strategy~\cite{goodfellow2014generative}. Given , we train two encoders  to maximize  as in Eqn.~\ref{eq:infonce_loss}, similar to the discriminator of GAN~\cite{goodfellow2014generative}. Meanwhile,  is adversarially trained to minimize . Formally, the objective is:

Alternatively, one may use other MI bounds~\cite{belghazi2018mine,poole2019variational}, but we find  works well and keep using it. We note that the invertibility of  prevent it from learning degenerate/trivial solutions.

\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{fig/learned_color_v2.pdf}
\caption{View generator learned by (a) unsupervised or (b) semi-supervised objectives.}
\label{fig:learned_color}
\end{figure}
 
\header{Results.} We experiment with \emph{RGB} and \emph{YDbDr}. As shown in Fig.~\ref{fig:learned_color}(a), a reverse U-shape of  and downstream accuracy is present. Interestingly, YDbDr is already near the sweet spot. 
This happens to be in line with our human prior that the ``luminance-chrominance'' decomposition is a good way to decorrelate colors but still retains recognizability of objects.
We also note that another luminance-chrominance decomposition Lab, which performs similarly well to YDbDr (Fig.~\ref{fig:color}), was designed to mimic the way humans perceive color~\cite{jain1989fundamentals}. Our analysis therefore suggests yet another rational explanation for why humans perceive color the way we do -- human perception of color may be near optimal for self-supervised representation learning.

With this unsupervised objective, in most cases  between views is overly reduced. In addition, we found this GAN-style training is unstable, as different runs with the same hyper-parameter vary significantly. We conjecture it is because the view generator has no knowledge about the downstream task, and thus the constraint  in Proposition~\ref{theory:infomin} is heavily broken.
To overcome this, we further develop an semi-supervised view learning method.

\subsubsection{Semi-supervised View Learning: Find Views that Share the Label Information}\label{sec:semi}

We assume a handful of labels for the downstream task are available. Thus we can guide the generator  to retain  and . Practically, we introduce two classifiers on each of the learned views to perform classification during the view learning process. Formally, we optimize:

where  are the classifiers. The  term applies to all data while the latter two are only for labeled data. In each iteration, we sample an unlabeled batch and a labeled batch. After this process is done, we use frozen  to generate views for \emph{unsupervised} contrastive representation learning.

\begin{table}[t]
\centering
\begin{minipage}[c]{0.53\textwidth}
\centering
\small
\caption{\label{tab:generator} \small Comparison of different view generators by measuring STL-10 classification accuracy: \emph{supervised}, \emph{unsupervised}, and \emph{semi-supervised}. ``\# of Images'' indicates how many images are used to learn view generators. In representation learning stage, all 105k images are used.}
\begin{tabular}{l|cc}
\toprule
Method (\# of Images) & RGB & YDbDr \\
\midrule
unsupervised (100k) & 82.4  3.2 & 84.3  0.5 \\
supervised (5k) & 79.9  1.5 & 78.5  2.3 \\
semi-supervised (105k) & \textbf{86.0  0.6} & \textbf{87.0  0.3}\\
\midrule
raw views & 81.5  0.2 & 86.6  0.2 \\
\bottomrule
\end{tabular}
\end{minipage}\hfill
\begin{minipage}[c]{0.46\textwidth}
\captionsetup{type=figure}
\includegraphics[width=1.0\linewidth]{fig/STL_model.pdf}
\vspace{-3pt}
\caption{\label{tab:stl_model} \small Switching to larger backbones with views learned by the semi-supervised method.}
\end{minipage}
\end{table} \header{Results.} The plots are shown in Figure~\ref{fig:learned_color}(b). Now the learned views are centered around the sweet spot, no matter what the input color space is and whether the generator is VP or NVP, which highlights the importance of keeping information about . Meanwhile, to see the importance of the unsupervised term, which reduces , we train another view generator with only supervised loss. We further compare ``supervised'', ``unsupervised'' and ``semi-supervised'' (the supervised + unsupervised losses) generators in Table~\ref{tab:generator}, where we also includes contrastive learning over the original color space (``raw views") as a baseline. 
The semi-supervised view generator significantly outperforms the supervised one, validating the importance of reducing . We compare further compare  with  ( is RGB or YDbDr) on larger backbone networks, as shown in Fig.~\ref{tab:stl_model}, We see that the learned views consistently outperform its raw input, e.g.,  surpasses  by a large margin and reaches  classification accuracy.








 

\section{Conclusion}
We have characterized that good views for a given task in contrastive representation learning framework should retain task-relevant information while minimizing irrelevant nuisances, which we call \emph{InfoMin} principle. Based on it, we demonstrate that optimal views are task-dependent in both theory and practice. We further propose a semi-supversied method to learn effective views for a given task. In addition, we analyze the data augmentation used in recent methods from the \emph{InfoMin} perspective, and further propose a new set of data augmentation that achieved a new state-of-the-art top-1 accuracy on ImageNet linear readout benchmark with a ResNet-50.

 

\clearpage

\section*{Broader Impact}
This paper is on the basic science of representation learning, and we believe it will be beneficial to both the theory and practice of this field. An immediate application of self-supervised representation learning is to reduce the reliance on labeled data for downstream applications. This may have the beneficial effects of being more cost effective and reducing biases introduced by human annotations. At the same time, these methods open up the ability to use uncurated data more effectively, and such data may hide errors and biases that would have been uncovered via the human curation process. We also note that the view constructions we propose are not bias free, even when they do not use labels: using one color space or another may hide or reveal different properties of the data. The choice of views therefore plays a similar role to the choice of training data and training annotations in traditional supervised learning.



 \begin{ack}

\noindent \textbf{Acknowledgements.} This work was done when Yonglong Tian was a student researcher at Google.
We thank Kevin Murphy for fruitful and insightful discussion; Lucas Beyer for feedback on related work;
and Google Cloud team for supporting computation resources. Yonglong is grateful to Zhoutong Zhang
for encouragement and feedback on experimental design.

\noindent \textbf{Funding.} Funding for this project was provided Google, as part of Yonglong Tian's role as a student researcher at Google.

\noindent \textbf{Competing interests.} In the past 36 months, Phillip Isola has had employment at MIT, Google, and OpenAI; honorarium for lecturing at the ACDL summer school in Italy; honorarium for speaking at GIST AI Day in South Korea. P.I.’s lab at MIT has been supported by grants from Facebook, IBM, and the US Air Force; start up funding from iFlyTech via MIT; gifts from Adobe and Google; compute credit donations from Google Cloud. Yonglong Tian is a Ph.D. student supported by MIT EECS department. Chen Sun, Ben Poole, Dilip Krishan, and Cordelia Schmid are employees at Google.



\end{ack} 
\bibliographystyle{plain}
\bibliography{egbib.bib}

\clearpage
\onecolumn
\begin{center}
{\Large\bf{Appendix: What Makes for Good Views for Contrastive Learning?}\\
}
\end{center}
\maketitle

\appendix
\section{Proof of Proposition 3.1}

In this section, we provide proof for the statement regarding optimal views in proposition 3.1 of the main text. As a warmup, we firstly recap some properties of mutual information.

\subsection{Properties of MI \protect\cite{cover1991entropy}:}
\noindent(1)~Nonnegativity:

\noindent(2)~Chain Rule:

\noindent(2)~Multivariate Mutual Information:



\subsection{Proof}
\begin{prop}~\label{prop:1}
According to Proposition 1, the optimal views  for task  with label , are views such that 
\end{prop}
\begin{proof}
Since , and ,  are functions of .

Therefore , due to the nonnegativity. Then we have:

Therefore the optimal views  that minimizes  subject to the constraint yields . Also note that optimal views  are conditionally independent given , as now . 
\end{proof}

\begin{prop}
Given optimal views  and minimal sufficient encoders , , then the learned representations  or  are sufficient statistic of  or  for , i.e.,  or .
\end{prop}
\begin{proof}
Let's prove for . Since  is a function of , we have:

To prove , we need to prove .

In the above derivation  because  is a function of ;  because optimal views  are conditional independent given , see Proposition~\ref{prop:1}. Now, we can easily prove  following a similar procedure in Proposition~\ref{prop:1}. If we can further prove , then we get . By nonnegativity, we will have .

To see , recall that our encoders are sufficient. According to Definition 1, we have :

\end{proof}

\begin{prop}
The representations  and  are also minimal for .
\end{prop}
\begin{proof}
For all sufficient encoders, we have proved  are sufficient statistic of  for predicting . Namely . Now:

The minimal sufficient encoder will minimize  to . This is achievable and leads to . Therefore,  is a minimal sufficient statistic for predicting , thus optimal. Similarly,  is also optimal. \end{proof}




\section{Implementation Details}






\subsection{Spatial Patches with Different Distance}
\textbf{Why using DIV2K~\cite{agustsson2017ntire}?} Recall that we randomly sample patches with a distance of \emph{d}. During such sampling process, there is a possible bias that with an image of relatively small size (e.g., 512x512),
a large \emph{d} (e.g., 384) will always push these two patches around the boundary. To minimize this bias, we choose to use high resolution images (e.g. 2k) from DIV2K dataset.

\textbf{Setup and Training.} We use the training framework of CMC~\cite{tian2019contrastive}. The backbone network is a tiny AlexNet, following~\cite{hjelm2018learning,tian2019contrastive}. We train for  epochs, with the learning rate initialized as  and decayed with cosine annealing.

\noindent \textbf{Evaluation.} We evaluate the learned representation on both STL-10 and CIFAR-10 datasets. For CIFAR-10, we resize the image to 6464 to extract features. The linear classifier is trained for 100 epochs.

\subsection{Channel Splitting with Various Color Spaces}\label{sec:color}
\textbf{Setup and Training.} The backbone network is also a tiny AlexNet, with the modification of adapting the first layer to input of  or  channels. We follow the training recipe in~\cite{tian2019contrastive}.

\noindent \textbf{Evaluation.} For the evaluation on STL-10 dataset, we train a linear classifier for 100 epochs and report the single-crop classification accuracy. For NYU-Depth-v2 segmentation task, we freeze the backbone network and train a 4-layer decoder on top of the learned representations. We report the mean IoU for labeled classes.

\subsection{Reducing  with Frequency Separation}
\begin{figure}[ht]
\centering
\includegraphics[width=0.9\linewidth]{fig/blur.pdf}
\vspace{-5pt}
\caption{\small We create views by splitting images into low- and high-frequency pairs with a blur function parameterized by .  is maximized at . Starting from this point, either increasing or decreasing  will reduce  but interestingly they form two different trajectories. When increasing  from 0.7, the accuracy firstly improves and then drops, forming a reverse-U shape corresponding to (a) in Figure 2 of the main paper. While decreasing  from 0.7, the accuracy keeps diminishing, corresponding to (b) in Figure 2 of the main paper.}
\label{fig:blur}
\end{figure}
 
Another example we consider is to separate images into low- and high-frequency images. To simplify, we extract  and  by Gaussian blur, i.e., 

where \texttt{Blur} is the Gaussian blur function and  is the parameter controlling the kernel. Extremely small or large  can make the high- or low-frequency image contain little information. In theory, the maximal  is obtained with some intermediate . As shown in Figure~\ref{fig:blur}, we found  leads to the maximal  on the STL-10 dataset. Either blurring more or less will reduce , but interestingly blurring more leads to different trajectory in the plot than blurring less. When increasing  from 0.7, the accuracy firstly improves and then drops, forming a reverse-U shape with a sweet spot at . This situation corresponds to (a) in Figure 2 of the main paper. While decreasing  from 0.7, the accuracy keeps diminishing, corresponding to (b) in Figure 2 of the main paper. This reminds us of the two aspects in Proposition 3.1: mutual information is not the whole story; \emph{what} information is shared between the two views also matters.

\textbf{Setup and Training.} The setup is almost the same as that in color channel splitting experiments, except that each view consists of three input channels. We follow the training recipe in~\cite{tian2019contrastive}.

\noindent\textbf{Evaluation.} We train a linear classifier for 100 epochs on STL-10 dataset and 40 epochs on TinyImageNet dataset.


\subsection{Colorful Moving MNIST}
\textbf{Dataset.} Following the original Moving MNIST dataset~\cite{srivastava2015unsupervised}, we use a canvas of size 6464, which contains a digit of size 2828. The back ground image is a random crop from original STL-10 images (9696). The starting position of the digit is uniformly sampled inside the canvas. The direction of the moving velocity is uniformly sampled in , while the magnitude is kept as  of the canvas size. When the digit touches the boundary, the velocity is reflected.

\noindent\textbf{Setup.} We use the first 10 frames as  (namely ), and we construct  by referring to the 20-th frame (namely ). During the contrastive learning phase, we employ a 4-layer ConvNet to encode images and use a single layer LSTM~\cite{hochreiter1997long} on top of the ConvNet to aggregate features of continuous frames. The CNN backbone consists of 4 layers with  filters from low to high. Average pooling is applied after the last convolutional layer, resulting in a 64 dimensional representation. The dimensions of the hidden layer and output in LSTM are both 64.

\noindent\textbf{Examples}. The examples of  and  are shown in Figure~\ref{fig:mnist}, where the three rows on the RHS shows cases that only a single factor (digit, position, or background) is shared.

\begin{figure}[t]
\centering
\includegraphics[width=0.85\linewidth]{fig/mnist.pdf}
\caption{\small Illustration of the Colorful-Moving-MNIST dataset. In this example, the first view  is a sequence of frames containing the moving digit, e.g., . The matched second view  share some factor with  that  can predict, while the unmatched view  does not share factor with .}
\vspace{-10pt}
\label{fig:mnist}
\end{figure}
 
\noindent\textbf{Training.} We perform intra-batch contrast. Namely, inside each batch of size 128, we contrast each sample with the other 127 samples. We train for 200 epochs, with the learning rate initialized as  and decayed with cosine annealing.


\subsection{Un-/Semi-supervised View Learning}
\begin{figure}[h]
\centering
\vspace{-10pt}
\includegraphics[width=0.85\linewidth]{fig/flow.pdf}
\caption{Volume-preserving (a), and none volume-preserving (b) invertible model.}
\vspace{-5pt}
\label{fig:flow}
\end{figure}
 \noindent\textbf{Invertible Generator.} Figure~\ref{fig:flow} shows the basic building block for the Volume-Preserving (VP) and None-Volume-Preserving (NVP) invertible view generator. The  and  are pixel-wise convolutional function, \emph{i.e.}, convolutional layers with 11 kernel.  and  represent a single channel of the input and output respectively, while  and  represent the other two channels. While stacking basic building blocks, we alternatively select the first, second, and the third channel as , to enhance the expressivity of view generator.

\noindent\textbf{Setup and Training.} For unsupervised view learning that only uses the adversarial  loss, we found the training is relatively unstable, as also observed in GAN~\cite{goodfellow2014generative}. We found the learning rate of view generator should be larger than that of  approximator. Concretely, we use Adam optimizer~\cite{kingma2014adam}, and we set the learning rates of view generator and  approximator as - and -, respectively. For the semi-supervised view learning, we found the training is stable across different learning rate combinations, which we considered as an advantage. To be fair, we still use the same learning rates for both view generator and  approximator. 

\noindent\textbf{Contrastive Learning and Evaluation.} After the view learning stage, we perform contrastive learning and evaluation by following the recipe in Section~\ref{sec:color}. \section{Data Augmentation as InfoMin}




\subsection{InfoMin Augmentation}

\begin{figure}[h]
\centering
\includegraphics[width=0.95\linewidth]{fig/imagenet.pdf}
\vspace{-10pt}
\caption{(a) data augmentation as InfoMin on ImageNet with linear projection head; (b) illustration of step-by-step data augmentation used in InfoMin.}
\label{fig:imagenet}
\end{figure}
 
\textbf{InfoMin Aug.} We gradually strengthen the family of data augmentation functions , and plot the trend between accuracy in downstream linear evaluation benchmarks and . The overall results are shown in Figure~\ref{fig:imagenet}(a), where the plot is generated by only varying data augmentation while keeping all other settings fixed. We consider \emph{Color Jittering} with various strengths, \emph{Gaussian Blur}, \emph{RandAugment}~\cite{cubuk2019randaugment}, and their combinations, as illustrated in Figure~\ref{fig:imagenet}(b). The results suggest that as we reduce , via stronger  (in theory,  also decreases), the downstream accuracy keeps improving.

\subsection{Analysis of Data Augmentation as it relates to MI and Transfer Performance}


We also investigate how sliding the strength parameter of individual augmentation functions leads to a practical reverse-U curves, as shown in Figures \ref{fig:imagenet_crop} and \ref{fig:imagenet_color}.

\begin{figure}[ht]
\subfloat[Linear projection head]{
\includegraphics[width=0.45\linewidth]{fig/imagenet_crop_linear.pdf}
}
\subfloat[MLP projection head]{
\includegraphics[width=0.45\linewidth]{fig/imagenet_crop_mlp.pdf}
}
\caption{Different low-area cropping bounds in RandomResizedCrop.}
\label{fig:imagenet_crop}
\end{figure} \textbf{Cropping.} In PyTorch, the \texttt{RandomResizedCrop(scale=(c, 1.0))} data augmentation function sets a low-area cropping bound \texttt{c}. Smaller \texttt{c} means more aggressive data augmentation. We vary \texttt{c} for both a linear critic head~\cite{wu2018unsupervised} (with temperature 0.07) and nonlinear critic head~\cite{chen2020simple} (with temperature 0.15), as shown in Figure~\ref{fig:imagenet_crop}. In both cases, decreasing \texttt{c} forms a reverse-U shape between  and linear classification accuracy, with a sweet spot at . This is different from the widely used  in the supervised learning setting. Using  can lead to more than  drop in accuracy compared to the optimal  when a nonlinear projection head is applied.

\begin{figure}[ht]
\subfloat[Linear projection head]{
\includegraphics[width=0.45\linewidth]{fig/imagenet_colorjitter_linear.pdf}
}
\subfloat[MLP projection head]{
\includegraphics[width=0.45\linewidth]{fig/imagenet_colorjitter_mlp.pdf}
}
\caption{Different magnitudes of Color Jittering.}
\label{fig:imagenet_color}
\end{figure} \textbf{Color Jittering.} As shown in Figure~\ref{fig:imagenet}(b), we adopt a parameter  to control the strengths of color jittering function. As shown in Figure~\ref{fig:imagenet_color}, increasing  from  to  also traces a reverse-U shape, no matter whether a linear or nonlinear projection head is used. The sweet spot lies around , which is the same value as used in SimCLR~\cite{chen2020simple}. Practically, we see the accuracy is more sensitive around the sweet spot for the nonlinear projection head, which also happens for cropping. This implies that it is important to find the sweet spot for future design of augmentation functions.

\header{Details.} These plots are based on the MoCo~\cite{he2019momentum} framework. We use  negatives and pre-train for 100 epochs on 8 GPUs with a batch size of 256. The learning rate starts as  and decays following a cosine annealing schedule. For the downstream task of linear evaluation, we train the linear classifier for 60 epochs with an initial learning rate of 30, following~\cite{tian2019contrastive}. 

\subsection{Results on ImageNet Benchmark}


\begin{table}[ht]
\vspace{-5pt}
\centering
\setlength{\tabcolsep}{7pt}
\caption{\label{tab:imagenet_archs} \small Single-crop ImageNet accuracies (\%) of linear classifiers~\protect\cite{zhang2016colorful} trained on representations learned with different methods using various architectures. }
\begin{tabular}{llccccc}
\toprule
Method & Architecture        & Param. & Head & Epochs & Top-1 & Top-5 \\ 
\midrule
\multicolumn{7}{l}{\textit{Methods using contrastive learning:}}\\
InstDis~\cite{wu2018unsupervised}   & ResNet-50    & 24  & Linear & 200  & 56.5  & -    \\
Local Agg.~\cite{zhuang2019local}   & ResNet-50    & 24  & Linear & 200  & 58.8  & -     \\
CPC v2~\cite{henaff2019data}        & ResNet-50    & 24  & - &  -   & 63.8  & 85.3  \\
CMC~\cite{tian2019contrastive} & 2x ResNet-50(0.5x)& 12  & Linear  & 240 & 60.0 & 82.3 \\
CMC~\cite{tian2019contrastive} & 2x ResNet-50(1x) & 47  & Linear   & 240 & 66.2 & 87.0 \\
CMC~\cite{tian2019contrastive} & 2x ResNet-50(2x)& 188 & Linear    & 240 & 70.6 & 89.7 \\
MoCo~\cite{he2019momentum} & ResNet-50      & 24  & Linear    & 200 & 60.6 & - \\
MoCo~\cite{he2019momentum} & ResNet-50 (2x) & 94  & Linear    & 200 & 65.4 & - \\
MoCo~\cite{he2019momentum} & ResNet-50 (4x) & 375 & Linear    & 200 & 68.6 & - \\
PIRL~\cite{misra2019self} & ResNet-50      & 24  & Linear    & 800 & 63.6 & - \\
PIRL~\cite{misra2019self} & ResNet-50 (2x) & 94  & Linear    & 800 & 67.4 & - \\
SimCLR~\cite{chen2020simple} & ResNet-50      & 24  & MLP    & 1000 & 69.3 & - \\
SimCLR~\cite{chen2020simple} & ResNet-50 (2x) & 94  & MLP    & 1000 & 74.2 & - \\
SimCLR~\cite{chen2020simple} & ResNet-50 (4x) & 375 & MLP    & 1000 & 76.5 & - \\
MoCo V2~\cite{chen2020improved} & ResNet-50   & 24  & MLP    & 800 & 71.1 & - \\
InfoMin Aug. & ResNet-50   & 24  & MLP    & 100 & 67.4 & 87.9 \\
InfoMin Aug. & ResNet-50   & 24  & MLP    & 200 & 70.1 & 89.4 \\
InfoMin Aug. & ResNet-50   & 24  & MLP    & 800 & 73.0 & 91.1 \\
InfoMin Aug. & ResNet-101   & 43  & MLP    & 300 & 73.4 & - \\
InfoMin Aug. & ResNet-152   & 58  & MLP    & 200 & 73.4 & - \\
InfoMin Aug. & ResNeXt-101  & 87  & MLP    & 200 & 74.5 & - \\
InfoMin Aug. & ResNeXt-152  & 120 & MLP    & 200 & 75.2 & - \\
\midrule
\multicolumn{7}{l}{\textit{Methods NOT using contrastive
learning:}}\\
Exemplar~\cite{dosovitskiy2014discriminative,kolesnikov2019revisiting} & ResNet-50 (3x) & 211 & - & 35 & 46.0 & -\\
JigSaw~\cite{noroozi2016unsupervised,kolesnikov2019revisiting}   & ResNet-50 (2x) & 94 & - & 35 & 44.6 & -\\
Relative Position~\cite{doersch2015unsupervised,kolesnikov2019revisiting}  & ResNet-50 (2x) & 94 & - & 35 & 51.4 & -\\
Rotation~\cite{gidaris2018unsupervised,kolesnikov2019revisiting} & RevNet-50 (4x) & 86 & - & 35 & 55.4 & -\\
BigBiGAN~\cite{donahue2019large} & RevNet-50 (4x) & 86 & - & -  & 61.3 & 81.9\\
SeLa~\cite{Asano2020} & ResNet-50 & 24 & - & 400  & 61.5 & 84.0\\
\bottomrule
\end{tabular}
\end{table} 
On top of the ``RA-CJ-Blur'' augmentations shown in Figure~\ref{fig:imagenet}, we further reduce the mutual information (or enhance the invariance) of views by using PIRL~\cite{misra2019self}, i.e., adding JigSaw~\cite{noroozi2016unsupervised}. This improves the accuracy of the linear classifier from  to . Replacing the widely-used linear projection head~\cite{wu2018unsupervised,tian2019contrastive,he2019momentum} with a 2-layer MLP~\cite{chen2020simple} increases the accuracy to . When using this nonlinear projection head, we found a larger temperature is beneficial for downstream linear readout (as also reported in~\cite{chen2020improved}). All these numbers are obtained with 100 epochs of pre-training. For simplicity, we call such unsupervised pre-training as InfoMin pre-training (i.e., pre-training with our InfoMin inspired augmentation). As shown in Table~\ref{tab:imagenet_archs},
our InfoMin model trained with 200 epochs achieves , outperforming SimCLR with 1000 epochs. Finally, a new state-of-the-art,  is obtained by training for 800 epochs. Compared to SimCLR requiring 128 TPUs for large batch training, our model can be trained with as less as 4 GPUs on a single machine.

For future improvement, there is still room for manually designing better data augmentation. As shown in Figure~\ref{fig:imagenet}(a), using ``RA-CJ-Blur'' has not touched the sweet spot yet. Another way to is to learn to synthesize better views (augmentations) by following (and expanding) the idea of semi-supervised view learning method presented in Section 4.2.2 of the main paper.

\noindent \textbf{Different Architectures.} We further include the performance of InfoMin as well as other SoTA methods with different architectures in Table~\ref{tab:imagenet_archs}. Increasing the network capacity leads to significant improvement of linear readout performance on ImageNet for InfoMin, which is consistent with previous literature~\cite{tian2019contrastive,he2019momentum,chen2020simple,misra2019self}.


\subsection{Comparing with SoTA in Transfer Learning}






 


\section{Transfer Learning with Various Backbones and Detectors on COCO}\label{sec:coco}
We evaluated the transferability of various models pre-trained with InfoMin, under different detection frameworks and fine-tuning schedules. In \textbf{all} cases we tested, models pre-trained with InfoMin outperform those pre-trained with supervised cross-entropy loss. Interestingly, ResNeXt-152 trained with InfoMin on \textbf{ImageNet-1K} beats its supervised counterpart trained on \textbf{ImageNet 5K}, which is \textbf{6} times larger. Bounding box AP and mask Ap are reported on \texttt{val2017}


\subsection{ResNet-50 with Mask R-CNN, C4 architecture}
The results of Mask R-CNN with R-50 C4 backbone are shown in Table~\ref{tab:coco_r50_c4}. We experimented with \textbf{1} and \textbf{2} schedule.
\begin{table}[h]
\centering
\caption{\label{tab:coco_r50_c4} \small COCO object detection and instance segmentation. \textbf{R50-C4}. In the brackets are the gaps to the ImageNet supervised pre-training counterpart. In green are gaps of  0.5 point. * numbers are from~\protect\cite{he2019momentum} since we use exactly the same fine-tuning setting.}
\subfloat[Mask R-CNN, R50-\textbf{C4}, \textbf{1} schedule]{
\begin{tabular}{c|ccc|ccc}
\toprule
pre-train &
\apbbox{~} &
\apbbox{50} &
\apbbox{75} &
\apmask{~} &
\apmask{50} &
\apmask{75} \\
\hline
random init*       & 26.4 & 44.0 & 27.8 & 29.3 & 46.9 & 30.8 \\
supervised*        & 38.2 & 58.2 & 41.2 & 33.3 & 54.7 & 35.2 \\
\hline
MoCo*              & 38.5(0.3) & 58.3(0.1) & 
41.6(0.4) & 
33.6(0.1) & 
54.8(0.1) & 
35.6(0.1)\\

InfoMin Aug.           & 39.0(0.8) & 58.5(0.3) & 
42.0(0.8) & 
34.1(0.8) & 
55.2(0.5) & 
36.3(1.1) \\
\bottomrule
\end{tabular}
}

\subfloat[Mask R-CNN, R50-\textbf{C4}, \textbf{2} schedule]{
\begin{tabular}{c|ccc|ccc}
\toprule
pre-train &
\apbbox{~} &
\apbbox{50} &
\apbbox{75} &
\apmask{~} &
\apmask{50} &
\apmask{75} \\
\hline
random init*       & 35.6 & 54.6 & 38.2 & 31.4 & 51.5 & 33.5 \\
supervised*        & 40.0 & 59.9 & 43.1 & 34.7 & 56.5 & 36.9 \\
\hline
MoCo*              & 40.7(0.7) & 60.5(0.6) & 
44.1(1.0) & 
35.6(0.7) & 
57.4(0.8) & 
38.1(0.7)\\

InfoMin Aug.           & 41.3(1.3) & 61.2(1.3) & 
45.0(1.9) & 
36.0(1.3) & 
57.9(1.4) & 
38.3(1.4) \\
\bottomrule
\end{tabular}
}
\vspace{-5pt}
\end{table}


\subsection{ResNet-50 with Mask R-CNN, FPN architecture}
The results of Mask R-CNN with R-50 FPN backbone are shown in Table~\ref{tab:supp_coco_r50_fpn}. We compared with MoCo~\cite{he2019momentum} and MoCo v2~\cite{chen2020improved} under \textbf{2} schedule, and also experimented with \textbf{6} schedule. 
\begin{table}[h]
\centering
\caption{\label{tab:supp_coco_r50_fpn} \small COCO object detection and instance segmentation. \textbf{R50-FPN}. In the brackets are the gaps to the ImageNet supervised pre-training counterpart. In green are gaps of  0.5 point.}


\subfloat[Mask R-CNN, R50-\textbf{FPN}, \textbf{2} schedule]{
\begin{tabular}{c|ccc|ccc}
\toprule
pre-train &
\apbbox{~} &
\apbbox{50} &
\apbbox{75} &
\apmask{~} &
\apmask{50} &
\apmask{75} \\
\hline
random init       & 38.4 & 57.5 & 42.0 & 34.7 & 54.8 & 37.2 \\
supervised        & 41.6 & 61.7 & 45.3 & 37.6 & 58.7 & 40.4 \\
\hline
MoCo~\cite{he2019momentum}               & 41.7(0.1) & 61.4(0.3) & 45.7(0.4) & 37.5(0.1) & 58.6(0.1) & 40.5(0.1)\\
MoCo v2~\cite{chen2020improved}        &
41.7(0.1) & 61.6(0.1) & 45.6(0.3) & 37.6(0.0) & 58.7(0.0) & 40.5(0.1)\\
InfoMin Aug.           & 
42.5(0.9) & 62.7(1.0) & 46.8(1.5) & 38.4(0.8) & 59.7(1.0) & 41.4(1.0) \\
\bottomrule
\end{tabular}
}

\subfloat[Mask R-CNN, R50-\textbf{FPN}, \textbf{6} schedule]{
\begin{tabular}{c|ccc|ccc}
\toprule
pre-train &
\apbbox{~} &
\apbbox{50} &
\apbbox{75} &
\apmask{~} &
\apmask{50} &
\apmask{75} \\
\hline
random init       & 42.7 & 62.6 & 46.7 & 38.6 & 59.9 & 41.6 \\
supervised        & 42.6 & 62.4 & 46.5 & 38.5 & 59.9 & 41.5 \\
\hline
InfoMin Aug.           & 43.6(1.0) & 63.6(1.2) & 
47.3(0.8) & 
39.2(0.7) & 
60.6(0.7) & 
42.3(0.8) \\
\bottomrule
\end{tabular}
}
\end{table}



\subsection{ResNet-101 with Mask R-CNN, C4 architecture}
The results of Mask R-CNN with R-101 C4 backbone are shown in Table~\ref{tab:voc_r101_C4}. We experimented with \textbf{1} and \textbf{1} schedule.
\begin{table}[h]
\centering
\caption{\label{tab:voc_r101_C4} \small COCO object detection and instance segmentation. \textbf{R101-C4}. In the brackets are the gaps to the ImageNet supervised pre-training counterpart.}
\subfloat[Mask R-CNN, R101-\textbf{C4}, \textbf{1} schedule]{
\begin{tabular}{c|ccc|ccc}
\toprule
pre-train &
\apbbox{~} &
\apbbox{50} &
\apbbox{75} &
\apmask{~} &
\apmask{50} &
\apmask{75} \\
\hline
supervised        & 40.9 & 60.6 & 44.2 & 35.1 & 56.8 & 37.3 \\
InfoMin Aug.           & 42.5(1.6) & 62.1(1.5) & 46.1(1.9) & 36.7(1.6) & 58.7(1.9) & 39.2(1.9) \\
\bottomrule
\end{tabular}
}

\subfloat[Mask R-CNN, R101-\textbf{C4}, \textbf{2} schedule]{
\begin{tabular}{c|ccc|ccc}
\toprule
pre-train &
\apbbox{~} &
\apbbox{50} &
\apbbox{75} &
\apmask{~} &
\apmask{50} &
\apmask{75} \\
\hline
supervised        & 42.5 & 62.3 & 46.1 & 36.4 & 58.7 & 38.7 \\
InfoMin Aug.           & 43.9(1.4) & 63.5(1.2) & 47.5(1.4) & 37.8(1.4) & 60.4(1.7) & 40.2(1.5) \\
\bottomrule
\end{tabular}
}
\end{table}


\subsection{ResNet-101 with Mask R-CNN, FPN architecture}
The results of Mask R-CNN with R-101 FPN backbone are shown in Table~\ref{tab:voc_r101_fpn}. We experimented with \textbf{1}, \textbf{2}, and \textbf{6} schedule.
\begin{table}[h]
\centering
\caption{\label{tab:voc_r101_fpn} \small COCO object detection and instance segmentation. \textbf{R101-FPN}. In the brackets are the gaps to the ImageNet supervised pre-training counterpart.}
\subfloat[Mask R-CNN, R101-\textbf{FPN}, \textbf{1} schedule]{
\begin{tabular}{c|ccc|ccc}
\toprule
pre-train &
\apbbox{~} &
\apbbox{50} &
\apbbox{75} &
\apmask{~} &
\apmask{50} &
\apmask{75} \\
\hline
supervised        & 42.0 & 62.3 & 46.0 & 37.6 & 59.1 & 40.1 \\
InfoMin Aug.           & 42.9(0.9) & 62.6(0.3) & 47.2(1.2) & 38.6(1.0) & 59.7(0.6) & 41.6(1.5) \\
\bottomrule
\end{tabular}
}

\subfloat[Mask R-CNN, R101-\textbf{FPN}, \textbf{2} schedule]{
\begin{tabular}{c|ccc|ccc}
\toprule
pre-train &
\apbbox{~} &
\apbbox{50} &
\apbbox{75} &
\apmask{~} &
\apmask{50} &
\apmask{75} \\
\hline
supervised        & 43.3 & 63.3 & 47.1 & 38.8 & 60.1 & 42.1 \\
InfoMin Aug.           & 44.5(1.2) & 64.4(1.1) & 48.8(1.7) & 39.9(1.1) & 61.5(1.4) & 42.9(0.8) \\

\bottomrule
\end{tabular}
}

\subfloat[Mask R-CNN, R101-\textbf{FPN}, \textbf{6} schedule]{
\begin{tabular}{c|ccc|ccc}
\toprule
pre-train &
\apbbox{~} &
\apbbox{50} &
\apbbox{75} &
\apmask{~} &
\apmask{50} &
\apmask{75} \\
\hline
supervised        & 44.1 & 63.7 & 48.0 & 39.5 & 61.0 & 42.4 \\
InfoMin Aug.           & 45.3(1.2) & 65.0(1.3) & 
49.3(1.3) & 
40.5(1.0) & 
62.5(1.5) & 
43.7(1.3) \\

\bottomrule
\end{tabular}
}
\end{table}


\subsection{ResNet-101 with Cascade Mask R-CNN, FPN architecture}
The results of Cascade~\cite{cai2018cascade} Mask R-CNN with R-101 FPN backbone are shown in Table~\ref{tab:voc_r101_casfpn}. We experimented with \textbf{1}, \textbf{2}, and \textbf{6} schedule.
\begin{table}[h]
\centering
\caption{\label{tab:voc_r101_casfpn} \small COCO object detection and instance segmentation. \textbf{Cascade R101-FPN}. In the brackets are the gaps to the ImageNet supervised pre-training counterpart.}
\subfloat[Cascade Mask R-CNN, R101-\textbf{FPN}, \textbf{1} schedule]{
\begin{tabular}{c|ccc|ccc}
\toprule
pre-train &
\apbbox{~} &
\apbbox{50} &
\apbbox{75} &
\apmask{~} &
\apmask{50} &
\apmask{75} \\
\hline
supervised        & 44.9 & 62.3 & 48.8 & 38.8 & 59.9 & 42.0 \\
InfoMin Aug.           & 45.8(0.9) & 63.1(0.8) & 49.5(0.7) & 39.6(0.8) & 60.4(0.5) & 42.9(0.9) \\
\bottomrule
\end{tabular}
}

\subfloat[Cascade Mask R-CNN, R101-\textbf{FPN}, \textbf{2} schedule]{
\begin{tabular}{c|ccc|ccc}
\toprule
pre-train &
\apbbox{~} &
\apbbox{50} &
\apbbox{75} &
\apmask{~} &
\apmask{50} &
\apmask{75} \\
\hline
supervised        & 45.9 & 63.4 & 49.7 & 39.8 & 60.9 & 43.0 \\
InfoMin Aug.           & 47.3(1.4) & 64.6(1.2) & 51.5(1.8) & 40.9(1.1) & 62.1(1.2) & 44.6(1.6) \\

\bottomrule
\end{tabular}
}

\subfloat[Cascade Mask R-CNN, R101-\textbf{FPN}, \textbf{6} schedule]{
\begin{tabular}{c|ccc|ccc}
\toprule
pre-train &
\apbbox{~} &
\apbbox{50} &
\apbbox{75} &
\apmask{~} &
\apmask{50} &
\apmask{75} \\
\hline

supervised        & 46.6 & 64.0 & 50.6 & 40.5 & 61.9 & 44.1 \\
InfoMin Aug.           & 48.2(1.6) & 65.8(1.8) & 
52.7(2.1) & 
41.8(1.3) & 
63.5(1.6) & 
45.6(1.5) \\
\bottomrule
\end{tabular}
}
\end{table}


\subsection{ResNeXt-101 with Mask R-CNN, FPN architecture}
The results of Mask R-CNN with X-101 FPN backbone are shown in Table~\ref{tab:voc_x101_fpn}. We experimented with \textbf{1} and \textbf{2} schedule.
\begin{table}[h]
\centering
\caption{\label{tab:voc_x101_fpn} \small COCO object detection and instance segmentation. \textbf{X101-FPN}. In the brackets are the gaps to the ImageNet supervised pre-training counterpart. }
\subfloat[Mask R-CNN, X101-\textbf{FPN}, \textbf{1} schedule]{
\begin{tabular}{c|ccc|ccc}
\toprule
pre-train &
\apbbox{~} &
\apbbox{50} &
\apbbox{75} &
\apmask{~} &
\apmask{50} &
\apmask{75} \\
\hline
supervised        & 44.1 & 64.8 & 48.3 & 39.3 & 61.5 & 42.3 \\

InfoMin Aug.           & 45.0(0.9) & 65.3(0.5) & 
49.5(1.2) & 
40.1(0.8) & 
62.3(0.8) & 
43.1(0.8) \\
\bottomrule
\end{tabular}
}

\subfloat[Mask R-CNN, X101-\textbf{FPN}, \textbf{2} schedule]{
\begin{tabular}{c|ccc|ccc}
\toprule
pre-train &
\apbbox{~} &
\apbbox{50} &
\apbbox{75} &
\apmask{~} &
\apmask{50} &
\apmask{75} \\
\hline
supervised        & 44.6 & 64.4 & 49.0 & 39.8 & 61.6 & 43.0 \\

InfoMin Aug.           & 45.4(0.8) & 65.3(0.9) & 
49.6(0.6) & 
40.5(0.7) & 
62.5(0.9) & 
43.8(0.8) \\
\bottomrule
\end{tabular}
}
\end{table}


\subsection{ResNeXt-152 with Mask R-CNN, FPN architecture}
The results of Mask R-CNN with X-152 FPN backbone are shown in Table~\ref{tab:voc_x152_fpn}. We experimented with \textbf{1} schedule.. Note in this case, while InfoMin model is pre-trained on the standard ImageNet-1K dataset, supervised model is pre-trained on ImageNet-5K, which is \textbf{6} times larger than ImageNet-1K. That said, we found InfoMin still outperforms the supervised pre-training.
\begin{table}[h]
\centering
\vspace{-5pt}
\caption{\label{tab:voc_x152_fpn} \small COCO object detection and instance segmentation. \textbf{X152-FPN}. In the brackets are the gaps to the ImageNet supervised pre-training counterpart. Supervised model is pre-trained on ImageNet-5K, while InfoMin model is only pre-trained on ImageNet-1K.}
\subfloat[Mask R-CNN, X152-\textbf{FPN}, \textbf{1} schedule]{
\begin{tabular}{c|ccc|ccc}
\toprule
pre-train &
\apbbox{~} &
\apbbox{50} &
\apbbox{75} &
\apmask{~} &
\apmask{50} &
\apmask{75} \\
\hline
supervised        & 45.6 & 65.7 & 50.1 & 40.6 & 63.0 & 43.5 \\
InfoMin Aug.           & 46.4(0.8) & 66.5(0.8) & 50.8(0.7) & 41.3(0.7) & 63.6(0.6) & 44.4(0.9) \\
\bottomrule
\end{tabular}
}
\vspace{-5pt}
\end{table}





\section{Change Log}
\noindent \textbf{arXiv v1} Initial release.

\noindent \textbf{arXiv v2} Paper accepted to NeurIPS 2020. Updated to the camera ready version

\noindent \textbf{arXiv v3} Included more details in disclosure of funding. 
\end{document}
