\newif\ifstandalone
\standalonetrue

\ifstandalone

\documentclass[11pt]{llncs}
\usepackage{graphicx}
\usepackage[round,sort]{natbib}

\setlength{\textwidth}{6.5in}
\setlength{\textheight}{9in}
\setlength{\evensidemargin}{0in}
\setlength{\oddsidemargin}{0in}
\setlength{\topmargin}{-.5in}

\let\oldendproof\endproof
\def\endproof{\qed\oldendproof}

\DeclareSymbolFont{AMSb}{U}{msb}{m}{n}
\DeclareSymbolFontAlphabet{\Bbb}{AMSb}
\def\Ree{\ensuremath{\Bbb R}}
\def\Zee{\ensuremath{\Bbb Z}}

\begin{document}
\title{Learning Sequences}
\author{David Eppstein}

\institute{Computer Science Department\\
Donald Bren School of Information \& Computer Sciences\\
University of California, Irvine\\
\email{eppstein@uci.edu}}

\maketitle

\begin{abstract}
We describe the algorithms used by the ALEKS computer learning system for manipulating combinatorial descriptions of human learners' states of knowledge, generating all states that are possible according to a description of a learning space in terms of a partial order, and using Bayesian statistics to determine the most likely state of a student. As we describe, a representation of a knowledge space using learning sequences (basic words of an antimatroid) allows more general learning spaces to be implemented with similar algorithmic complexity. We show how to define a learning space from a set of learning sequences, find a set of learning sequences that concisely represents a given learning space, generate all states of a learning space represented in this way, and integrate this state generation procedure into a knowledge assessment algorithm. We also describe some related theoretical results concerning projections of learning spaces, decomposition and dimension of learning spaces, and algebraic representation of learning spaces.
\end{abstract}

\else

\chapter{Learning Sequences}
\label{Learning Sequences}
\vspace*{-2cm}
{\large D. Eppstein\footnote{Dept. of Computer Science, University of California, Irvine}

\fi

\def\notdiv{\mathop{\not|}}

\section{Introduction}

ALEKS (short for Assessment and Learning in Knowledge Spaces) is a computer system, and a company built around that system, which helps students learn knowledge-based academic systems such as mathematics by assessing their knowledge and providing lessons in concepts that the assessment judges them as ready to learn.
\index{ALEKS}

Rather than being based on numerical test scores and letter grades, ALEKS is based on a combinatorial description of the concepts known to the student, in the form of a \emph{learning space} \citep{doign99}. In this formulation, feasible states of knowledge of a student are represented as sets of facts or concepts; the task of the assessment routine is to determine which facts the student knows. Not all sets of facts are assumed to form feasible knowledge states; therefore, if the assessment routine can find a sequence of questions the answers to each of which roughly halve the number of remaining states consistent with the answers, then the student's knowledge can be assessed from a number of questions approximately equal to the logarithm (base two) of the number of feasible knowledge states, a number that may be significantly smaller than the number of facts in the learning space. That is, informally, the learning space model allows the system to make inferences about the student's knowledge of concepts that have not been directly tested, from the information it has about the concepts that have been tested; these inferences can significantly reduce the number of questions needed to accurately assess the student's knowledge, and thereby greatly reduce the tedium of interacting with the system. In addition to speeding students' interactions with the system in this way, the combinatorial knowledge model used by ALEKS allows it to determine sets of concepts which it judges the student ready to learn, and present a selection of lessons based on those concepts to the student, rather than forcing all students to proceed through the curriculum in a rigid linear ordering of lessons.
\index{learning space}
\index{logarithm}
\index{Doignon, J.-P.}
\index{Falmagne, J.-Cl.}

The actual assessment and inference routine in the ALEKS system is based on a Bayesian formulation in which the system computes likelihoods of each feasible knowledge state from the students' answers, aggregates these values to infer likelihoods that the student can answer each not-yet-asked question, and uses these likelihoods to select the most informative question to ask next. Once the student's knowledge is assessed, the system generates a \emph{fringe} of concepts that it judges the student ready to learn, by calculating the feasible knowledge states that differ from the assessed state by the addition of a single concept, and presents the students with a selection of online lessons based on that fringe.
\index{Bayesian statistics}
\index{fringe!of a state}

As of its 2006 implementation, ALEKS's assessment procedure must run in Java, interactively on the user's PC, so efficient computation at interactive speeds is essential to its operation. As we describe in more detail in the next chapter, the ALEKS system uses a data representation for its learning spaces based on a partial order structure of prerequisites for each concept. A clever state generation procedure allows for the efficient generation of states in learning spaces of, typically, 50 to 100 facts. For larger learning spaces, the combinatorial explosion in the total number of states makes it infeasible to generate all states; instead, the system repeatedly samples a subset of the concepts in such a way that each unsampled concept is ``near'' a sampled one, generates a learning space from the restriction of the prerequisite partial order to the sampled concepts, assesses the student's knowledge of the sample, and uses that sampled assessment to refine the portion of the learning space within which further assessment is judged necessary.
\index{Java}
\index{partial order}
\index{state generation}
\index{prerequisite}

Although quite successful, this partial order based definition of a learning space suffers from some flaws. Primary among these flaws is a lack of adaptability: the structure of prerequisite orderings between concepts must be developed by human knowledge engineers, and is difficult to change in any automated way. From the pattern of responses to ALEKS's assessments, it may be possible to infer that some sets of facts are highly unlikely to be found as knowledge states of students; eliminating such states from the system would help reduce the number of questions required for assessment. More significantly, some concepts may not readily learnable by the students when the system judges that they are; these concepts should be removed from the selection of online lessons presented to the students to avoid the frustration of unlearnable lessons. Similarly, the pattern of student assessment answers may lead us to conclude that some additional states are present among the students but not available to the system's representation; adding these states to the system would allow for more accurate assessment procedures. The ability to modify the learning space represented in the system is of special interest in the context of internationalization of ALEKS, as we would expect the different educational systems and cultures in different countries to lead to students with different typical knowledge states. It would be of great interest to the ALEKS designers to develop automated adaption systems that can take taking advantage of ALEKS's large base of user data and reduce the human engineering effort needed to adapt the system to new concepts and new cultures. An automated adaption procedure, based on the generalization of the concept of a fringe from states in a learning space to learning spaces in a system of such spaces, was developed by \citet{Thi-01}; however, this adaption procedure was not used by ALEKS because the  partial order based learning space representation is insufficiently flexible to allow the generation of new learning spaces by the insertion and removal of states.
\index{adaptation}
\index{Thi{\'e}ry, N.}

A secondary flaw relates to the mathematical definition of learning spaces. The spaces representable by partial orders in the implementation of ALEKS do not comprise all possible learning spaces in the theory developed by \citet{doign99}, but rather form a significantly restricted class of spaces known as \emph{quasi-ordinal spaces}, in which the family of feasible sets can be shown to be closed under set unions and set intersections. Closure under set unions is justified pedagogically, both at a local scale of learnability (learning one concept does not preclude the possibility of learning a different one) and more globally (if student  knows one set of concepts, and student  knows another, then it is reasonable to assume that there may be a third student who combines the knowledge of these two students). However, closure under set intersections seems much harder to justify pedagogically: it implies that any concept has a single set of prerequisites, all of which must be learned prior to the concept. On the contrary, in practice it may be that some concepts may be learned via multiple pathways that cannot be condensed into a single set of prerequisites.  The practical effect of this flaw is that the partial order based representation for learning spaces used by ALEKS does not allow certain learning spaces to be defined; instead one must define a larger space formed by the family of all intersections of sets in the desired learning space, and the larger number of sets in this intersection closure of the learning space may lead to inefficiencies in the assessment procedure. In addition, and more seriously, the inability to accurately describe the prerequisite structure for certain concepts may lead to situations where the system incorrectly assesses a student as being ready to learn a concept.
\index{quasi-ordinal space}
\index{union!closure under}
\index{intersection!closure under}

In this chapter we outline algorithms and prototype implementations for a more flexible representation, one that would allow the implementation of Thi\'ery's automatic adaptation procedure and allow more general definitions of learning spaces than the existing partial order based representation allows, while preserving the scalability and efficient implementability of that representation. We believe that these goals may be achieved by using a representation based of \emph{learning sequences}: orderings of the learning space's concepts into linear sequences in which those concepts could be learned. A learning space may have an enormous number of possible learning sequences, but, as we show, it is possible to correctly and accurately represent any learning space using only a subset of these sequences, and in many cases the number of sequences needed to define the space can be very small. For instance, for the quasi-ordinal spaces currently in use by ALEKS, a representation based on learning sequences can be constructed in which the number of learning sequences equals the maximum number of concepts in the fringe of a feasible state. We show how to generate efficiently all states of a learning space defined from a set of learning sequences, allowing for similar and similarly efficient assessment procedures to the ones currently used by ALEKS. Additionally, we show how to find efficiently a representation of this type for any learning space, using an optimal number of example sequences, and how to adapt any space defined in this way by adding or removing sets from its family of feasible states. We detail this learning sequence based representation, and the efficient algorithms based on it, after describing in more detail ALEKS's existing partial order based representation.
\index{learning sequence}
\index{state generation}
\index{adaptation}

In addition we investigate more generally the theory of and algorithms for learning spaces and related combinatorial structures. In particular we examine the mathematical structure of projections of learning spaces, the extent to which it is possible to decompose learning spaces efficiently into unions of simpler learning spaces, definitions of learning spaces via the algebraic properties of their union operation, and relations between different definitions of dimension for a learning space. These theoretical investigations are detailed in later sections of this chapter.

\section{Learning Spaces from Partial Orders}

We outline in this section the representation of learning spaces already in use by the 2006 implementation of ALEKS.
As we describe, this representation leads to efficient assessment algorithms, but is only capable of representing a limited subset of the possible learning spaces, the so-called \emph{quasi-ordinal spaces}.
\index{quasi-ordinal space}

A \emph{partial order} is a relation  among a set of objects, satisfying \emph{irreflexivity} () and
\emph{transitivity} ( and  implies ).
Although defined as a mathematical object, a partial order may be represented concisely for computational purposes by its \emph{Hasse diagram}, a directed graph containing an edge  whenever  and there does not exist  with . That is, we connect a pair of items in the partial order by an edge whenever the pair belongs to the \emph{covering relation} of the partial order.  The original partial order may be recovered easily from the Hasse diagram representing it:  if and only if there exists a directed path from  to  in the Hasse diagram.
\index{partial order}
\index{irreflexive relation}
\index{transitive relation}
\index{Hasse diagram}

To derive a learning space from a partial order on a set of concepts, we interpret the edges of the Hasse diagram as describing prerequisite relations between concepts. That is, if  and  are concepts, represented as vertices in a Hasse diagram containing the edge , then we take it as given that  may not be learned unless  has also already been learned. For instance, in elementary arithmetic, one cannot perform multi-digit addition without having already learned how to do single digit addition, so a learning space involving these two concepts should be represented by a Hasse diagram containing a path from the vertex representing single-digit addition to the vertex representing multi-digit addition.
\index{prerequisite}

With this interpretation, a state of knowledge in the learning space may be formed as a \emph{lower set}: a set  of the concepts in a given partial order, satisfying the requirement that, for any edge  of the Hasse diagram, either  or . Figure~\ref{fig:qos} shows an example of a Hasse diagram on eight concepts, and the 19 states in the learning space derived from this Hasse diagram.
\index{lower set}
\index{Hasse diagram}

\begin{figure}[t]
\centering\includegraphics[scale=0.45]{qos}
\caption{Left: a partial order, shown as a Hasse diagram in which each edge is directed from the concept at its lower endpoint to the concept at its upper endpoint. Right: the learning space derived from the partial order on the left.}
\label{fig:qos}
\end{figure}

We call a learning space derived from a Hasse diagram in this way a \emph{quasi-ordinal space}. A quasi-ordinal space must satisfy the following three properties:
\index{quasi-ordinal space}

\begin{description}
\item[Accessibility.]
For every nonempty state  in the learning space, there is a concept  such that  is also a state in the learning space. In learning terms, any state of knowledge may be reached by learning one concept at a time. ``Accessibility'' is the usual name for this property in the combinatorics literature;
in learning theory papers, it has also been referred to as ``downgradability'' \citep{doble01a}.
\index{accessible set family}
\index{downgradability}
\index{Doble, C.W.}
\index{Doignon, J.-P.}
\index{Falmagne, J.-Cl.}
\index{Fishburn, P.C.}

\item[Union Closure.]
If  and  are states of knowledge in the learning space, then  is also a state in the learning space. In learning terms, the knowledge of two individuals may be pooled to form a state of knowledge that is also feasible.
\index{union!closure under}

\item[Intersection Closure.]
If  and  are states of knowledge in the learning space, then  is also a state in the learning space. We are unaware of a natural learning based interpretation of this property.
\end{description}
\index{intersection!closure under}

A family of states satisfying only accessibility and union closure forms a mathematical structure known as an \emph{antimatroid} \citep{KorLovSch-91}, and it is this more general general class of structure that we hope to capture with our learning sequence representation of a learning space. The partial order based structure defined by ALEKS allows only a special subclass of antimatroids satisfying also the intersection closure property; mathematically such a structure forms a \emph{lattice of sets}, or, equivalently, a \emph{distributive lattice}.
Conversely, it follows from results of \citet{birkh37} that any distributive lattice can be represented via a partial order in this way. See \citet{doign99} for related representation theorems for learning spaces.
\index{antimatroid}
\index{lattice!of sets}
\index{lattice!distributive}
\index{Korte, B.}
\index{Lov{\'a}sz, L.}
\index{Schrader, R.}
\index{Doignon, J.-P.}
\index{Falmagne, J.-Cl.}
\index{Birkhoff, G.}
\index{Birkhoff's representation theorem}
\index{ALEKS}

In what follows we review briefly the algorithms used by ALEKS to perform assessments using this quasi-ordinal learning space representation.

\subsection{The Fringe}

The \emph{fringe} of any state  in a learning space is defined to be the set of concepts that, when added to or removed from , lead to another state in the learning space. Fringes are important to ALEKS because they describe the concepts that the assessed student is most ready to learn, or has the most shaky learning of. We may distinguish the \emph{outer fringe} of concepts a student is ready to learn, those concepts  such that  is also a state in the learning space, from the \emph{inner fringe} of concepts that a student may have most recently learned, those concepts  such that  is also a state. The fringe is the union of the outer and inner fringes. In a learning space (or more generally, in any medium) each state may be uniquely identified by the pair of its outer and inner fringes.
\index{fringe!of a state}
\index{ALEKS}

In a quasi-ordinal space, the inner fringe of  consists of the maximal concepts in , and the outer fringe consists of the minimal concepts not in . The fringe of  may be calculated easily in a single pass through the edges  of the Hasse diagram: if  and , then  cannot be in the fringe, and if  and  in , then  cannot be in the fringe.  We initialize a set  to consist of the whole domain, and remove  or  from  whenever we discover a covering relation that prevents it from belonging to the fringe; the remaining set at the end of this scan is the fringe.
\index{Hasse diagram}

\subsection{State Generation}

To determine the likelihood that a student knows each concept in a learning space, from a given set of results on asked questions, ALEKS uses an algorithm based on listing all states in the learning space. The algorithm used by ALEKS can be explained most easily in terms of \emph{reverse search} \citep{avis96}, a general technique for developing generation algorithms for many types of combinatorial structures.
\index{state generation}
\index{reverse search}
\index{ALEKS}
\index{Avis, D.}
\index{Fukuda, K.}

Suppose we have chosen a \emph{topological ordering} (also known as a \emph{linear extension}) of the concepts in a partial order; that is,
a sequence of the concepts such that, if  in the order, then  must appear prior to  in the sequence. For instance, one may sort the concepts by the length of the longest path leading to each concept in the Hasse diagram, with ties broken arbitrarily among concepts having paths of the same length; the resulting sorted sequence is a topological ordering.
Then, given any state  in the knowledge space, one may find another state  where  is chosen to be the concept belonging to  that has the latest position in the topological ordering. We call  the \emph{predecessor} of . In this way, we disambiguate the accessibility property of learning spaces and make a concrete choice of which concept to remove to form the predecessor of any state.
If we repeat this removal process, starting from any state, we will eventually reach the empty set, so the graph formed by connecting each state to its predecessor is a tree, rooted at the empty set.
Reverse search, applied to this predecessor relationship, amounts to performing a depth first traversal of this tree.
\index{topological ordering}
\index{linear extension}
\index{partial order}
\index{predecessor}
\index{accessible set family}
\index{depth first search}
\index{Hasse diagram}

To be more specific, we perform a recursive traversal of the tree defined above, maintaining as we do for each state  a set  of the concepts that may be added to  to form another state that is a child of  in the predecessor tree. Then, when the recursion steps from  to , we calculate  from   by removing from  any  occurring prior to  in the topological order, and adding to  any concept  reachable from  by a Hasse diagram edge  such that all other prererequisites of  already belong to .
Once we have calculated , we may output  as one of our states and continue recursively to each state  for each  in , and so on.
\index{children of a state}
\index{Hasse diagram}

Very little is needed in the way of data structures beyond the Hasse diagram itself to implement this recursive traversal efficiently. Primarily, we need a way of quickly determining whether all prerequisites of some concept  belong to the set  we are traversing.  This may be done efficiently by maintaining, for each , a count of the prerequisites of  that do not yet belong to , decrementing that count for each successor of  whenever we step from  to , and incrementing the count again when our recursive traversal returns from  to . In this way we may test prerequisites in constant time, and use time proportional to the number of Hasse diagram edges out of  to update counts whenever we step from state to state. Alternatively, the method used within the 2006 implementation of ALEKS is to store a bitmap representation of , and mask it against a bitmap representing the predecessors of  whenever such tests are needed; theoretically this method requires time proportional to the number of items in the learning space per test, but in practice it is fast because typical modern computer architectures allow for the testing of prerequisite relations for 32 concepts simultaneously in a single machine instruction.
\index{prerequisite}
\index{predecessor}
\index{bitmap}
\index{ALEKS}

\begin{figure}[t]
\centering\includegraphics[width=4in]{slowqos}
\caption{The Hasse diagram of a worst case example for the partial order state generation algorithm.
Each of  items (bottom) is connected by an edge to each of  items (top).
The corresponding learning space has  states,  of which cause the state generation algorithm to perform  prerequisite checks per state, so the total time is  per state.}
\label{fig:slowqos}
\end{figure}

The total time spent copying lists of children in this procedure can be charged using amortized time analysis against the time to generate each child state. Therefore, the bottleneck for time analysis of the procedure is the part of the time spent testing successors of  and testing whether to add those successors to . As described above, each such test can be implemented in linear time, so the total time for the algorithm is proportional to the sum, over all states in the learning space, of the number of Hasse diagram edges that are outgoing from the last concept in each state. In typical examples the Hasse diagrams are relatively sparse and the time per state may approach a constant, but even in the worst case (Figure~\ref{fig:slowqos}) the time is no more than  per state in a learning space with  concepts. It is this level of state generation efficiency that we hope to approach or meet with our more general learning space representation.
\index{amortized time analysis}
\index{worst case time complexity}
\index{state generation}
\index{children of a state}
\index{Hasse diagram}

\subsection{Assessment Procedure}

While a student is being assessed, he or she will have answered some of the assessment questions correctly and some incorrectly. We desire to infer from these results likelihoods that the student understands each of the concepts in the knowledge space, even those concepts not explicitly tested in the assessment.

The inference method used by ALEKS \citep{falmagne:88a,falma90} applies more generally to any system of sets, and can be interpreted using Bayesian probability methods.
We begin with a prior probability distribution on the feasible states of the learning space. In the most simple case, we can assume an uninformative uniform prior in which each state is equally likely, but the method allows us to incorporate prior probabilities based on any easily computable function of the state, such as the number of concepts in the set represented by the state. These prior probilities may incorporate knowledge about the student's age or grades, or results from assessments in previous sessions that the student may have had with ALEKS; for instance, if a previous session assessed the student as being in a certain state, we could use an a priori probability distribution based on distance from that state in our next assessment. However, in the 2006 implementation of ALEKS only uniform prior probabilities are used.
\index{Bayesian statistics}
\index{prior probability distribution}
\index{uniform prior}
\index{ALEKS}
\index{session}
\index{Falmagne, J.-Cl.}
\index{Doignon, J-P.}
\index{Koppen, M.}
\index{Villano, M.}
\index{Johanessen, L..}

We also assume a conditional probability that the student, in a given state, will answer a question correctly or incorrectly: if the question tests a concept within the set represented by the state, we assume a high probability of a correct answer, while if a question tests a concept not within the set represented by the state, we assume a low probability of a correct answers.
Answers opposite to what the given state predicts can be ascribed to careless mistakes or lucky guesses, and we assume that incidences of such answers are independent of each other. ALEKS' test questions are designed so that lucky guesses are very rare; therefore, the necessity for accurate assessment in the presence of careless mistakes is of much greater significance to the design of the assessment procedure, but this also means that it is necessary to ascribe different rates to these two types of events. With these assumptions, we may use Bayes' rule to calculate posterior probabilities of each state. To do so, we calculate for each state a likelihood, the product of its prior probability with a set of terms, one term per already-asked assessment question. The value of each term depends on whether the student answered the question correctly and whether the concept tested by the question belongs to the given state. Once we have calculated these likelihoods for all states, they may be converted to probabilities by dividing them all by a common constant of proportionality, the sum of the likelihoods of all states.
\index{careless mistake}
\index{lucky guess}
\index{Bayes' rule}
\index{constant of proportionality}
\index{partition function}

From these posterior probabilities of states we wish to calculate posterior probabilities of individual concepts. To do so, we sum the probabilities of all states containing a given concept. ALEKS's assessment procedure calculates the probabilities of all concepts in this way, and then chooses to test the student on the most informative concept: the one with probability closest to 50\% of being known. Eventually, all concepts will have probabilities bounded well away from 50\%, at which point the evaluation procedure terminates.
\index{posterior probability}

Although described above as a separate sum for each concept,
ALEKS implements this probability calculation via a single pass through the states of the learning space. When the state generation procedure steps from a state  to a child state , it calculates the likelihood (product of terms for each answered question) from the new state by multiplying the old likelihood by a single term for the questions based on concept . It totals the likelihood of all states descending from  in the recursive search, and adds this total likelihood to that of concept . Then, when returning from  to , it adds the total likelihood calculated for  into that for . In this way, the likelihood for each concept is calculated in constant time per state, and the constant of proportionality needed to turn these likelihoods into probabilities is calculated as the total likelihood at the root of the recursion.

If  in the partial order defining the system's learning space,  will be assessed as having higher probability than  of being known; therefore, the set of concepts returned by this assessment algorithm is guaranteed to be a feasible knowledge state for the given quasi-ordinal space.

\subsection{Hierarchical Sampling Scheme}

Although the assessment procedure described above works well for partial orders of 50 to 100 concepts, it becomes too slow for larger learning spaces due to the need for the algorithm to list all states in the space and the combinatorial explosion in the number of states generated for those spaces. Therefore, the ALEKS system resorts to a sampling scheme that allows its assessment procedure to run at interactive speeds for much larger learning spaces. This sampling scheme is based on three concepts, all depending on the details of the definition of the learning space in terms of partial orders: distance between concepts, definition of smaller learning spaces from sampled concepts, and bounding concept likelihoods from their prerequisites and postrequisites.
\index{combinatorial explosion}
\index{ALEKS}

To generate smaller samples of the set of concepts used to define a learning space, ALEKS uses a notion of distance between two concepts in a partial order. To define the distance between  and ,
define  to be the set of items whose comparison to  is different from its comparison to . That is,

Then the distance  between  and  is defined to be .
This distance satisfies the mathematical axioms defining a \emph{metric space}: , . For any , , and ,
, and the union is not disjoint as  belongs to both sides, so .
\index{distance!between concepts}
\index{metric space}

ALEKS then chooses a suitable distance threshhold , and a sample  of the concepts of the learning space such that every concept is within distance  of a member of . Although there is no mathematical proof of such a fact, the intent of this sampling technique is that assessment on a nearby sample concept is likely to be informative about the assessment of each unsampled concept.

\begin{figure}[t]
\centering\includegraphics[scale=0.45]{sampleqos}
\caption{The Hasse diagram of the partial order of Figure~\ref{fig:qos}, restricted to the sampled set of concepts , and the smaller learning space generated from the restricted partial order.}
\label{fig:sampleqos}
\end{figure}

Once a sample of concepts has been chosen, ALEKS must form a learning space describing the knowledge states possible for that sample, so that it can apply its assessment procedure to the sample.
For quasi-ordinal learning spaces, this process of forming a learning space on the sample is very simple: one need merely restrict the given partial order defining the space to the sampled concepts, and build a learning space from the restricted order (Figure~\ref{fig:sampleqos}).
\index{restriction!of a partial order}

Finally, the assessment of likelihoods on the sampled concepts is used to bound the likelihoods of the remaining unsampled concepts, to determine which ones the student is likely to know or not to know.
If ,  belongs to the sample, and the student knows  with probability , then the student is taken to know the easier concept  with probability at least . Similarly, if ,  belongs to the sample, and the student knows  with probability , then the student is taken to know the harder concept  with probability at most . However, there is something of a mismatch between these likelihood bounds and the distance-based sampling procedure: it is possible for the nearby samples to an unsampled concept  to all be incomparable to , in which case we cannot find any useful bounds for the likelihood of .

This sampling process, sample learning space construction, and likelihood bound, are used together repeatedly to refine the portion of the learning space that is relevant for the student. Initially, all states are considered relevant, and a sample with a high distance threshhold is chosen. After several steps of refinement, a larger number of concepts have likelihoods that can be bounded away to one side or another of 50\%, and a sample is chosen with a smaller distance threshhold among only those remaining informative concepts. Eventually, this refinement process converges with all concepts having likelihoods bounded away from 50\%, which we may use to construct a most likely knowledge state for the student.

For learning spaces not defined from partial orders, we may have to replace these constructions with alternative techniques. However, it will still be necessary to have some way of sampling the concepts of the learning space, building a smaller learning space from the sample, and using assessments on the sample to bound likelihoods of unsampled concepts, because this sampling procedure is crucial to limiting the number of states generated in the assessment procedure and thereby keeping the assessment calculation's times fast enough for human interaction.

\section{Learning Spaces from Learning Sequences}

We now describe an alternative method for defining and describing learning spaces, that we believe may form the basis for an efficient and more flexible implementation of learning space based knowledge assessment algorithms than the one currently used by ALEKS.

While there has been past work on algorithmic characterizations of learning spaces using the terminology of antimatroids \citep{BoyFai-DAM-90,KemLev-arXiv-03}, that work focuses on showing that certain algorithms work correctly if and only if the structure they are applied to is an antimatroid. Here instead our focus is on implementation details for allowing antimatroid-based algorithms to run efficiently.
\index{antimatroid}
\index{Boyd, E.A.}
\index{Faigle, U.}
\index{Kempner, Y.}
\index{Levit, V.E.}

\subsection{Learning Sequences}

Given any learning space, there may be many orderings through which a student, starting from no knowledge, could learn all the concepts in the space. We call such an ordering a \emph{learning sequence}; in the combinatorics literature these are also known as \emph{basic words}.  Formally, a learning sequence  can be defined as a one-to-one map from the integers  to the  concepts forming the domain of a learning space, with the property that each \emph{prefix}  is a valid knowledge state in the learning space.  The sequence of prefixes  forms a shortest path in the learning space from the empty set to the whole domain, and for any such path the sequence of items by which one state in the path differs from the next forms a learning sequence.
\index{learning sequence}
\index{basic word}
\index{prefix}

For instance, in the learning space depicted in Figure~\ref{fig:qos}, the leftmost path from the bottom state (the empty set) to the top set (the whole domain) passes through the sequence of states
, , , , , , , , and , . The learning sequence corresponding to this path is , .
Similarly, the learning sequence corresponding to the rightmost path in the figure is .
Altogether, the learning space in Figure~\ref{fig:qos} can be shown to have 41 distinct learning sequences.

\begin{figure}[t]
\centering\includegraphics[scale=0.45]{nob}
\caption{A learning space that is not defined from a partial order. Unlike partial order based learning spaces, this space is not closed under intersections:  and  are both states in the space, but their intersection  is not. However, the space still satisfies the union closure and accessibility requirements of learning spaces.}
\label{fig:nob}
\end{figure}

For a quasi-ordinal learning space, such as the one in Figure~\ref{fig:qos}, a learning sequence is essentially just a topological ordering of the partial order on concepts defining the quasi-ordinal space. However, we have defined learning sequences in a way that can be applied to any learning space. For instance, the learning space in Figure~\ref{fig:nob}, which does not come from a partial order, has four learning sequences: (1) , (2) , (3) , and (4) .
\index{quasi-ordinal space}
\index{topological ordering}

\subsection{States from Sequences}

If we are given a set  of learning sequences, we can immediately infer that all prefixes of  are states in the associated learning space. But we can also infer the existence of other states using the union-closure property of learning spaces: any set formed from a union of prefixes of  must be a state in the associated learning space.
\index{union!closure under}

For instance, suppose we have the two sequences  and  from the learning space in Figure~\ref{fig:nob}. The prefixes of these sequences are the six sets , , , , , and . However, by forming unions of prefixes we may also form the seventh set . All seven states of the learning space can be recovered in this way from unions of the prefixes of these two sequences.

In general, for any set of sequences  over a domain of concepts, define the learning space  as the family of unions of prefixes of .
If  consists of learning sequences from a learning space , then
. We will discuss, in a later section of this chapter, methods of selecting a small set 
such that . For now, we take  as given and describe the learning space  it generates.

\begin{figure}[t]
\centering\includegraphics[width=4.5in]{seqex}
\caption{Learning space  generated from three sequences
, , and . Each state is shown with its index .}
\label{fig:seqex}
\end{figure}

\subsection{Indexing States}

It is possible to name states in  by vectors in , in such a way that each state has a unique name and the concepts in each state can be reconstructed easily from the state's name. Such a naming scheme will be useful in several of our later algorithms.
\index{vector}

Given a state  in a learning space, and a set

of  learning sequences within the space, define  to be the minimum index in  of a concept excluded from ; that is,

If  is the whole domain, we define for completeness . 
Equivalently, therefore,  is the size of the largest prefix of  that is a subset of .
Define  to be the vector

That is, {\rm mex} can be viewed as a function mapping states in the learning space to vectors in .
\index{minimum excluded element}

Conversely, we can define a function , mapping vectors in  to states in the learning space, by

That is, we interpret the coordinates of  as lengths of prefixes of each of the sequences in , and form the union of these prefixes.  For any set ,
, with  if and only if  is a state in the learning space .

If  is known,  may easily be calculated, by 
taking the coordinatewise minimum of  and the positions of .
However, calculating  from  appears to be more complicated.
As part of our state generation procedure, described in a later section, we include a bitmap-based data structure that allows us to maintain a set subject to both insertions and deletions and calculate its  indices, more quickly than recalculating these indices from scratch each time they are needed.
\index{state generation}
\index{bitmap}

The indices  for an example of a learning space  are depicted in Figure~\ref{fig:seqex}. The learning space shown in the figure cannot be derived from a partial order, as it has states  and  but not their intersection .

\subsection{Axioms of Learning Spaces}

As we now argue, the space  defined from a set  of learning sequences automatically satisfies the union-closure and accessibility properties that we require of our learning spaces.

Union-closure follows from the indexing scheme we have defined above:
if  and  are states in , then

where in this formula  should be interpreted as the pointwise maximum of two vectors.
Thus,  is also a state in .
\index{union!closure under}

\begin{figure}[t]
\centering\includegraphics[width=4in]{seqpred}
\caption{Predecessors of the states in the learning space from Figure~\ref{fig:seqex}.}
\label{fig:seqpred}
\end{figure}

Accessibility also follows from the indexing scheme.
For any nonzero vector , let  be the vector formed by decreasing the last nonzero coordinate of  by one, and let  be the result of iterating the decrement operation  times. Let
 
and, for any nonempty state  in 
let

Figure~\ref{fig:seqpred} shows a link from  to  for each state  in the learning space depicted in Figure~\ref{fig:seqex}.
Then, each iteration of  causes the image of the vector under the  operation either to stay unchanged or to change by the removal of a single item. Since  is defined to be the image after the first iteration that causes a change, it differs from  by the removal of a single item, so the requirement of accessibility is achieved.
\index{accessible set family}

Conversely, any learning space satisfying union closure and accessibility can be defined as  for an appropriate set  of learning sequences. In a later section we describe how to choose  to be as small as possible, as the size of  will be directly related to the efficiency of the algorithms we use to assess learning in .

\subsection{The Fringe}

As with learning spaces derived from partial orders, we will need to calculate fringes of states.
The \emph{outer fringe} of , the items that may be added to state  to form a new state,
is particularly easy to describe: Each state  may be formed as the union of the prefixes defining  itself, together
with one more prefix , so the outer fringe of  is

The \emph{inner fringe} of , those items in  that may be removed to form another state,
is a little more complicated, but still easily expressible using our indexing scheme: it is

The formula  is true if and only if  is a state of the learning space, so we may construct the lower fringe by testing whether this formula is true for each member  of~.
\index{fringe!of a state}

\section{Efficient State Generation and Assessment}

We wish to generalize ALEKS's knowledge assessment algorithms to learning spaces derived from learning sequences, so that we may use these spaces in place of the quasi-ordinal spaces used in ALEKS. The keys to this generalization are methods for efficiently listing all states of a learning space, and for projecting learning spaces onto smaller subsets of concepts in order to speed up the algorithm even further by reducing the size of the state space. As we describe here, a representation based on learning sequences allows us to perform these operations efficiently.

\subsection{State Generation}

As with partial orders, we can list all states of a sequence-based learning space by an algorithm based on exploring the tree generated by the predecessor operation we defined earlier; refer back to Figure~\ref{fig:seqpred} for a depiction of this tree on an example learning space.
Recall that the predecessor of a state is generated by repeatedly reducing the last nonzero coordinate of its  vector until the  function maps the reduced vector to a set different from the state itself. In order to reverse this, and find the successors of a state  (that is, the states having  as its predecessor), we need merely perform the following steps:
\index{predecessor}
\index{state generation}
\index{reverse search}
\index{minimum excluded element}

\begin{enumerate}
\item Set  to the smallest value of  such that  is the union of prefixes from the first  sequences.
\item For each , such that  for all 
(that is, such that the first excluded concept in  differs from the first excluded
concept in all earlier sequences):
\begin{enumerate}
\item Let  be formed from  by adding one to its th coordinate.
\item Let  and output .
\item Recursively generate states starting from .
\end{enumerate}
\end{enumerate}

\noindent
It is not hard to show that every state generated by this procedure has  as its predecessor, and that all states having  as predecessor will be generated by this procedure.
\index{predecessor}

To generate all states in the learning space, we perform a depth first traversal of the states of the space, by applying this procedure recursively starting with the empty set.
If  is a successor of , the value of  in the search for the successors of  equals the value of  used when we generated  from its parent ; therefore, we may pass these  values through the recursive traversal and avoid calculating them at each step.  However, the  values of  and  may differ not just in the th coordinate but also in other coordinates greater than , and must be recalculated.
If the learning space is generated by  learning sequences, the time per state is ,
except for the time to calculate the  value of each new state.
\index{depth first search}
\index{worst case time complexity}

\subsection{Bitmap Structure for Minimal Excluded Elements}

In order to implement our state generation procedure efficiently, we need to be able to calculate the  of each newly generated state. When we defined the  operation we showed that removing an element from a state allows the new  to be calculated easily, but, unfortunately, in the case of the state generation procedure, each new state is generated not by removing an element but by adding one.
\index{minimum excluded element}

To speed up this part of the state generation, we maintain as part of the recursive traversal a collection of long integers, one per learning sequence defining the learning space. In each integer  we store the number  representing the set of positions of learning sequence  that are present in the current state . Each of these integers may be updated in constant time for each step forwards and backwards in the state traversal, and each  calculation may be performed by finding the first zero bit in the binary representation of each integer,
which may be performed efficiently using a combination of arithmetic and bitwise boolean operations on the integer. Specifically, the bitwise exclusive or of  and  is an integer of the form , where  is the position of the first zero in  and should be used as the value of .
\index{bitmap}

In a programming language without built-in support for integers of arbitrarily large precision, it may be appropriate to replace each value  in this structure by an array of 32-bit machine integers, each one representing the intersection of  with some 32-member subset of the concepts.

\subsection{Bayesian Assessment}

Given the state generation procedure, we can assess the likelihoods of a student knowing each concept using the same Bayesian procedure described for quasi-ordinal spaces.
\index{Bayesian statistics}
\index{prior probability distribution}

Specifically, we calculate a likelihood for each state of the space as a product of the prior probability of the state with a set of terms, one term per question asked of the student. As we generate all states, we can calculate each such likelihood as the product of the likelihood of a previously generated state by a single term. The likelihood of a concept is the sum of the likelihoods of the states containing it, and the probability of knowing a concept is this sum normalized by dividing by the sum of likelihoods of all states.

As for the assessment procedure in quasi-ordinal spaces, our state generation procedure operates recursively by adding single concepts to previously generated states. Therefore, if we maintain the sum of likelihoods in each subtree of the recursion, and add each such sums to the total likelihood of the most recently added item, we may compute the sums of likelihoods for all concepts with a single pass over all states of the learning space, in constant additional time per step.

\subsection{Smaller Spaces on Subsets of Concepts}

\begin{figure}[t]
\centering\includegraphics[width=3.5in]{seqproj}
\caption{The projection of a learning space.}
\label{fig:seqproj}
\end{figure}

If  is any family of sets, and  is any subset of , we may define
the \emph{projection}

by intersecting each state in  with .
If  is a learning space, then so is any projection of it. The projection inherits the union closure property of learning spaces from ,
as . To prove the accessibility property of learning spaces
for a set , , repeatedly use accessibility in  to remove concepts from  until a concept in  is removed, and let the predecessor of  be formed by removing the same concept from .
\index{projection!of a learning space}
\index{union!closure under}
\index{accessible set family}
\index{predecessor}

If  is a quasi-ordinal space, the projection  is also quasi-ordinal order, formed from the restriction of the partial order defining  to the items in . This concept of a learning space derived from a restricted partial order (without the more general notion of projection) is used by ALEKS, as described earlier, to speed up its assessment procedures. Similarly, if  is a learning space derived from a collection of learning sequences, the projection   can be derived from the subsequences formed by listing the items of  in the order they appear in each sequence defining , as shown in Figure~\ref{fig:seqproj}. Therefore, we may efficiently construct a learning sequence based representation of a projection of a learning space defined by learning sequences. This projected space may not necessarily use the minimum possible number of sequences in its representation, but it uses a number of sequences no more than that of the larger space it was projected from.
\index{quasi-ordinal space}

\subsection{Projection for Faster Assessment}

The choice of how many concepts to use in modeling knowledge as a learning space is somewhat arbitrary: we may model some areas of knowledge coarsely, as a relatively small set of concepts, and others more finely using many concepts. We do not expect our assessment of the likelihood that a student knows a given concept to vary significantly based on the choice of how coarsely the space is modeled. Therefore, though there is little mathematical justification for this simplification, we may greatly speed up our knowledge assessment routines by projecting to a space consisting only of the relevant concepts for the assessment: those we wish to assess, and those for which we have answers.

To be specific, suppose that  is a set of concepts on which we have tested a student's knowledge, in a learning space , and  is a concept for which we wish to infer a likelihood of the student's knowledge.
Rather than applying the state-generation based assessment algorithm to the whole learning space , we may apply it to the projected space .
\index{projection!of a learning space}

Mathematically, in order to get assessment results in the projection equal to the results in the original space , we would need to use a prior probability in each projected state  equal to the sum of the prior probabilities of the states in  that project to . For instance, if we are using the uninformative prior for , in which we assume that each state is equally likely to occur, then in the projection we should use a prior in which the probability for each state  is proportional to the number of states in  that project to . However, even for the uninformative prior, this problem of quickly calculating numbers of states seems quite difficult. Instead, we propose performing the evaluation in the projected space   as if it were the whole space. The results of the likelihood calculation in this projection may differ from those of the calculation in the whole space , but there seems little reason in principal to view one of the results as more reliable than the other, and if  is small then the projected assessment may be much more efficient than the assessment in the original space. If  is so small that  does not provide adequate coverage of the learning space, however,
the assessment may be less informative than that in ; for instance, concepts occurring in very few states of the learning space would have very low likelihood when assessed in  with  but  likelihood when assessed in . This problem of a too-small sample losing information about the number of states containing concept  may be ameliorated by adding a small random sample  of the learning space concepts to ; that is, if we perform the assessment in  in place of , we can gain an accurate estimate of the number of states containing  even when  is empty or near-empty.

One unfortunate feature of computing assessments in  rather than  is that it requires a separate run of the state generation procedure for each separate concept~, while the assessment procedure in  involves only a single run of the state generation procedure. It seems likely that, if we wish to compute likelihoods for all concepts in the knowledge space,
a faster procedure would be to partition the concepts into small collections , and to use the state generation based assessment procedure on  to compute likelihoods for each concept in . If  is chosen to be too small, this would involve many runs of the procedure, while if  is chosen to be too large, then the procedure will list too many states, so there is likely some optimal intermediate value (perhaps depending on the size of ) to use for the size of the collections  in order to make this projection based assessment procedure run at optimal speed. The tradeoffs of runtime versus collection size would need to be found by experimental analysis beyond the scope of this chapter. However, if the collections  are chosen randomly rather than as distance based clusters, they would also perform the same function as the set  above of making
the states of the projected space more representative of the numbers of states in the original space. Thus, it may be desirable for the accuracy of the assessment to chose  larger than an analysis based only on algorithmic efficiency would suggest.

We are hopeful that the speedup provided by this projection based assessment method, relative to the more naive state generation based assessment used by the current version of ALEKS, would allow student assessment to proceed in a single pass involving all concepts in the learning space, rather than the multipass clustering based approach used in the current version of ALEKS. In outline, the procedure would proceed as follows:

\begin{enumerate}
\item Initialize  to .
\item Repeat:
\begin{enumerate}
\item Partition the concepts of the learning space randomly into collections .
\item For each , generate the states of  and apply the assessment algorithm to compute likelihoods for each concept in .
\item If all concepts have likelihoods bounded away from , terminate the assessment procedure.
\item Choose a concept  with likelihood as close as possible to , assess the student's knowledge of , and add  to .
\end{enumerate}
\end{enumerate}

As discussed above, the choice of how large to make the collections  is left undetermined by this algorithm, and involves a tradeoff between algorithmic efficiency and assessment accuracy.

\subsection{Inverting Projections to Shrink the State Space}

We considered also an alternative assessment procedure to the one described in the previous section, in which we maintain sets of concepts  and  which we believe the student to know or not know respectively, and restrict the state space search used in the assessment algorithm to only those states consistent with  and . That is, rather than totalling likelihoods for all states in , we only
total those likelihoods in the states of  that project to  in ; all other states are assumed to have a priori probability zero of being the actual state of knowledge of the student. As  and  grow through the course of the assessment, this could lead to a significant reduction in the number of states that need to be listed.  However, some form of sampling and assessment based on the projection of  onto the sample would still be needed in order to achieve adequate performance in the earlier stages of the algorithm when  and  are small. One possible structure for such an algorithm is shown below.

\begin{enumerate}
\item Initialize  and  to empty.
\item Repeat:
\begin{enumerate}
\item Choose a sample  of the concepts in .
\item Repeat:
\begin{enumerate}
\item Use the likelihood calculation algorithm on , restricted to
only those states that project to  in , to compute likelihoods for .
\item If no concept in  has likelihood near , terminate the inner loop.
\item Choose a concept with likelihood as close to  as possible and test the student's knowledge of that concept.
\end{enumerate}
\item Add the concepts of  with likelihoods significantly larger than  to , and add the concepts of  with likelihoods significantly smaller than  to .
\item If all concepts of  are in  or , terminate the algorithm.
\end{enumerate}
\end{enumerate}

This algorithm appears to improve on the previous section's algorithm by requiring only a single
pass through the states of a projected learning space per question asked of the student. However we see several significant potential problems with this approach that stopped us from investigating it in greater detail. First, the assignment of states to  and  is made with certainty, making a Bayesian interpretation of this algorithm problematic. Second, while it is possible to generate the states consistent with  and  in relatively small time per state, we have not been able to find an algorithm for this task that is as efficient as the one for listing all states in a learning space. And third and most troubling, if the algorithm ever reaches a situation where the projection  does not contain  as a state, the algorithm will fail by dividing by zero in the calculation of the concept likelihoods, so it is not robust against inaccuracies in the learning space model or the assessment algorithm.

Nevertheless if we wish to implement such an algorithm we must find the set of states in  project to  in , in order to perform the likelihood calculations of the algorithm.
We may impose a tree structure on this set of states by choosing a learning sequence  of  that contains as a prefix the union of the states that project to , and defining the parent of any state  to be the state formed by adding to  the first missing concept in . A reverse search algorithm that reverses this parent relation and traverses this tree can list all states that project to , in a small number of steps per listed state, but we have not found a way to make this algorithm as efficient as the one we gave earlier for listing all states in a learning space. We omit the details.

\subsection{Distances and Clustering}

We have provided above a method based on projection that does not involve the distance-based clustering currently used in ALEKS. However, it may still be of interest to define a distance function on the concepts of the learning space. For instance, as we have discussed above, clustering based on distance may be useful for the partition into collections  in our projection based assessment algorithm. In addition, the distance from some previously assessed state may be an important ingredient in the prior probability distribution on learning space states used by the knowledge assessment algorithm.
\index{distance!between concepts}

The learning sequences we use to define our learning spaces provide a natural family of distances:
if we interpret each concept's position in each sequence as a coordinate, all such positions form a vector corresponding to the concept in , and we may use any  metric on this collection of vectors. We did not investigate more carefully the choice of metric to determine the value of  that would work best for knowledge assessment applications.

\section{Finding Concise Representations}

We have seen that a learning space may be defined from a set  of learning sequences.
The running time of our algorithms depends on the number of sequences in , so in order for this representation of a learning space to yield efficient algorithms, we need this number to be small.

For instance, for the learning space in Figure~\ref{fig:nob}, we have seen that two of the four learning sequences,  and , suffice to determine all seven states in the learning space.
For the learning space in Figure~\ref{fig:qos}, two learning sequences, 
 and , corresponding to the leftmost and rightmost paths through the diagram in the figure, similarly determine the whole learning space; the remaining 39 sequences are superfluous. More generally, we have shown \citep{eppst06} that two learning sequences suffice to describe a learning space if, and only if, the space can be drawn as a planar graph with the empty set and the whole domain as vertices on the outer face of the drawing. In that paper we outlined a complicated algorithm for finding such a pair of sequences, when it exists, in time linear in the number of states of the learning space.
\index{Eppstein, D.}
\index{graph drawing}

We show here that we may efficiently find the minimum possible set  defining any given learning space . This minimum number is known in antimatroid theory as the \emph{convex dimension} of an antimatroid, and it can be defined as the \emph{width} of a certain partial order associated with the learning space \citep{KorLovSch-91}.  Our technique applies known polynomial time algorithms for calculating widths of partial orders.
\index{convex dimension}
\index{width!of a partial order}
\index{Korte, B.}
\index{Lov{\'a}sz, L.}
\index{Schrader, R.}

For learning spaces defined from partial orders themselves, the convex dimension is the width of the original partial order from which the learning space was defined. This implies that, even for these special learning spaces, our algorithms are similarly efficient to the Hasse-diagram based algorithms currently in use by ALEKS.

\subsection{The Width of a Partial Order}

A \emph{chain} in a partial order is a set of items that are all comparable to each other; any chain can be ordered into a sequence  such that  if and only if . A \emph{chain cover} is a set of chains that together include all items in the order.
Similarly, an \emph{antichain} in a partial order is a set of items no two of which are comparable to each other, so that the partial order places no restrictions on their ordering.
The \emph{width} of a partial order is the maximum cardinality of any of its antichains.
It is well known (Dilworth's Theorem) that, for any partial order, the width is also equal to the minimum number of chains in a chain cover.
\index{antichain}
\index{chain cover}
\index{width!of a partial order}
\index{Dilworth's theorem}
\index{Dilworth, R.P.}

For instance, the partial order depicted on the left of Figure~\ref{fig:qos} can be covered by two chains,  and . It also has many antichains of two concepts, for instance . Therefore, its width is exactly two. There cannot be any antichain of three or more concepts, nor is there a single chain that covers all its concepts.

\begin{figure}[t]
\centering\includegraphics[width=4in]{bip4cd}
\caption{An optimal chain decomposition of a partial order may be found via bipartite graph matching.}
\label{fig:bip4cd}
\end{figure}

An optimal chain decomposition of a given partial order may be found by a technique based on bipartite graph matching (Figure~\ref{fig:bip4cd}).
The far left of the figure shows the same partial order as the one in Figure~\ref{fig:qos}, and the center left shows a bipartite graph derived from this order. In the bipartite graph, we have two vertices for each item in the partial order, one on the left and one on the right. We draw an edge from the vertex labeled  on the left to the vertex labeled  on the right whenever . In general this graph may be disconnected, and have vertices incident to no edges; these complications cause no difficulty for our algorithms.
A \emph{matching} in a graph is a set of edges, at most one edge incident on any vertex, and a \emph{maximum matching} is a matching maximizing the total number of edges in the set.
A maximum matching of the graph in the figure is shown in center right.
From any matching in this graph, we may derive a chain cover, by including in the same chain any two items in the partial order that form the two endpoints of a matched edge; such a cover, for the matching in the figure, is shown on the far right. In a partial order with  items, matchings with  matched edges correspond to chain covers with  chains, so the minimum chain cover can be found from the maximum matching.
\index{maximum matching}
\index{bipartite graph}

\citet{HopKar-SJC-73} showed how to find a maximum matching in a bipartite graph with  vertices and  edges in time ; for partial orders with  items this translates to an  time bound for finding an optimal chain decomposition and therefore finding the width. A maximum antichain may also be found in the same amount of time; we omit the details as they are not necessary for our learning space application.
\index{chain cover}
\index{antichain}
\index{worst case time complexity}
\index{Hopcroft, J.E.}
\index{Karp, R.M.}

\subsection{Learning Sequences for Chains of States}

Recall that a \emph{chain} is a totally ordered collection of objects in a partial order.
Therefore, a chain of states of a learning space consists of a nested family of sets.
We show here, for  any chain of states in learning space ,
how to find a learning sequence  in  such that each state of 
is a prefix of . We may assume without loss of generality that  contains  and .
\index{learning sequence}

To find , sort the sets in  by size:

We will build up  in stages; after the th stage,
we will have chosen values for  such
that all sets , , are prefixes of the chosen sequence of values
and such that all prefixes of the sequence are states in .
Initially, the empty sequence satisfies this requirement for .

In stage , we will have already generated a sequence for the elements of .
By the accessibility property of learning spaces, we can also generate a sequence
 of the elements of , such that each prefix of this sequence is a state in . We then form our new longer sequence of  values by concatenating 
the subsequence of the 's not already chosen as 's onto the end of the previously chosen 's. This concatenation preserves the property that the sets , ,
are all prefixes of the new sequence, and now  itself is the prefix consisting of the whole concatenation. In addition, each prefix of the new sequence is the union of a prefix of the old sequence of 's and of a prefix of the sequence of 's, and therefore by the union closure property of learning spaces is a state of .

Once we have completed the final stage of this process, for ,
the ordering on  provides the desired learning sequence.
We refer to the existence of this learning sequence  as the chain property of learning spaces.

In general, the algorithm described here involves a number of predecessor calculations equal to the sum of the cardinalities of the sets in the chain. 
For learning spaces derived from partial orders on  concepts, described by Hasse diagrams with  edges, a faster algorithm is possible, based on a standard method for performing topological ordering via depth first search: we sort the elements in descending order by the size of the smallest chain set they belong to, and  perform a depth first traversal of the Hasse diagram, initiating the traversal at the elements in the sorted order. The desired learning sequence is the reverse of a postorder numbering for this traversal. Thus, after ordering the elements by the smallest containing chain sets, the remaining algorithm takes time .
\index{worst case time complexity}
\index{Hasse diagram}
\index{predecessor}

\subsection{The Base of a Learning Space}

The \emph{base} of a union-closed family of sets is a minimal subfamily such that any member of the family can be reconstructed as a union of members of the subfamily.
For \emph{antimatroids} (union-closed accessible families) there is a particularly easy construction of a base.
\index{antimatroid}
\index{base}

Define a \emph{predecessor} of set  in antimatroid  to be any set  that also belongs to . If  has zero predecessors, and  is accessible, then  must be empty and can be reconstructed as an empty union of sets no matter what base is chosen.
And, if  has two or more predecessors, each of which can be reconstructed as a union of sets in the base, then  itself can be reconstructed as the union of any two of its predecessors.
Thus in either of these two cases  cannot belong to a base.
However, if  has only a single predecessor , then any proper subset of  is also a subset of : by the chain property, there must be a learning sequence containing the proper subset and  itself as prefixes, and the predecessor of  on that sequence must be . Therefore, in this case, any union of subsets of  forms a subset of  and cannot equal , so  must belong to every base.
From these considerations we see that any antimatroid  has a unique base  consisting of the sets that have a single predecessor. We view  as forming a partial order by set inclusion: if  and  are both sets in the base, then  in the partial order if and only if .
\index{predecessor}

\begin{figure}[t]
\centering\includegraphics[scale=0.45]{seqbasis}
\caption{The Hasse diagram of the base  for the learning space  from Figure~\ref{fig:seqex}.}
\label{fig:seqbasis}
\end{figure}

In a learning space  derived from a partial order, the base sets have the form . For, the only predecessor of  is , but any set not of this form is either empty or has two or more independent maximal elements, each of which can be removed to form a predecessor. Also,  if and only if . Thus the base sets correspond one-for-one with the concepts of  and the partial order on  is isomorphic to the partial order on concepts. However, for learning spaces not derived from partial orders,  may contain many more sets than there are concepts in the learning space.
Figure~\ref{fig:seqbasis} depicts the partial order formed by the base of the learning space depicted in Figure~\ref{fig:seqex}; this base has seven sets although it describes a learning space on only six concepts.
\index{predecessor}

It will also be of interest to find the base of a learning space  defined from a set  of learning sequences. For learning spaces of this type, the only states that could be in a base are the prefixes of : any state that is not a prefix can be formed as a union of smaller sets, and therefore cannot be in the base.
If state  is a prefix of one of our defining sequences , with  as the latest concept in that prefix, then  is a base if and only if the position of  in each other  is either the same as its position in  or later than the minimum excluded element of  in .
For, if this is so, no other symbol can be removed from  leaving a valid state, while if it is not true
then there exists a symbol  such that  is not dominated by , and the  for which   is maximal among other such vectors may be removed to form another valid state.
Thus, we may test whether  is a base state, once its  has been calculated,
in time proportional to the number of defining sequences.
We may find the base by listing the prefixes of each defining sequence, in order from longer prefixes to smaller ones so that we may more efficiently update the  values, and applying this test to each prefix. In a space with  concepts, defined by  sequences, this base generation algorithm takes time  and generates at most  base sets.
\index{base}
\index{prefix}
\index{worst case time complexity}

\subsection{Which Learning Sequences Define the Space?}

If  is a collection of learning sequences for a learning space ,
then .  When does the learning space  derived from  equal ?

If every base set for  is a prefix of , then all states in  can be formed as unions of those base members, and .  Conversely, if some set  is not a prefix of ,
then  can not be reconstructed as a union of any other sets in , let alone of the prefixes of , and .
\index{base}
\index{prefix}

The prefixes of a single learning sequence form a chain in the set inclusion ordering, and the base elements within that family form a chain in . Conversely, if we have any chain in , we may use the chain property to form from it a learning sequence containing as prefixes all chain members.  Thus, a set  of learning sequences determines the learning space  if and only the base prefixes for each chain form a chain cover of .
The minimum number of learning sequences needed to determine the learning space is the minimum number of chains in a chain cover of , that is, its width.
\index{chain cover}
\index{width!of a partial order}

\subsection{Calculating the Convex Dimension}

We are now ready to describe an algorithm that, given a learning space , calculates its convex dimension and finds a minimum family  of learning sequences such that . The algorithm simply performs the following steps:
\index{convex dimension}

\begin{enumerate}
\item Find the base .
\item Use bipartite matching to find a minimum chain cover of .
\item Use the chain property to extend each chain in the cover to a learning sequence for .
\item Let  be the set of learning sequences so constructed. The convex dimension is the number of learning sequences in~.
\end{enumerate}

If only the convex dimension and not the learning sequences themselves is desired, step~3 can be omitted.
Assuming a suitably algorithmic implementation of the underlying learning space , all of these steps may be performed in time polynomial in  and in the number of concepts in ,
both of which we expect to be far smaller than the total number of states in .

For learning spaces derived from partial orders on  concepts, described by Hasse diagrams with  edges, the first step is trivial, and the second step takes time . There  chains in the cover,
and any single chain may be extended to a learning sequence in time . Thus, the third step can be implemented in time , and the overall algorithm takes time .
index{worst case time complexity}
\index{Hasse diagram}

Once a minimum family  of learning sequences for  has been generated, we may redefine  as , and use our computational representation based on learning sequences for any future calculations in .

\section{Adapting a Learning Space}

\citet{Thi-01} defines the \emph{fringe} of a learning space to be the collection of sets that can be added to the space as new states, or removed from the states of the space, in order to form new learning spaces with more or fewer states. He calculates the fringes of a space as part of an algorithm for adapting a knowledge space to make it more accurately reflect the observed knowledge of students. One of the great advantages of learning sequence based construction of learning spaces, over partial orders, is their ability to allow such adaption: learning sequences can represent any learning space, and in particular can represent the spaces formed by adding states to and removing states from an existing space. Such adaptivity seems a very useful capability to add to ALEKS, as a knowledge space that more accurately models student knowledge would improve its interactions with students, and as ALEKS has in its user base a readily available supply of information about the accuracy of its model. We feel, however, that for learning spaces with numbers of states as large as those used by ALEKS, it would take impractically many moves to adapt a learning space to another one sufficiently different as to make a significant difference to ALEKS' assessment algorithms. Therefore, there seems plenty of scope for investigation of adaptivity algorithms that change more than one state of a space at a time. Nevertheless we describe here how to calculate the fringes of a learning space defined from learning sequences.
\index{adaptation}
\index{fringe!of a learning space}
\index{ALEKS}
\index{Thi{\'e}ry, N.}

As with the fringe of an individual state in a learning space, we may distinguish two subsets of the fringe of the learning space itself.  The \emph{inner fringe} of  consists of those states of  that may be removed, leaving a learning space with fewer states, while the \emph{outer fringe} of  consists of those sets that are not states of  but may be added as states, resulting in a learning space with more states.

\subsection{The Inner Fringe}

State  may be removed from learning space  if, and only if, it satisfies all of the following requirements:
\begin{enumerate}
\item  is in the base of .
\item .
\item No set , , is in the base of .
\end{enumerate}
If  were not in the base, it could be formed as the union of other sets in , and removing it would violate union closure. Similarly, if some set  were in the base, removing  would violate accessibility for that set. On the other hand, if the requirements above are all satisfied, then the space formed by removing  from  satisfies union closure and accessibility, and therefore forms a valid learning space. The base of this new space consists of the remaining base members of , together with certain sets of the form , from which we may use our concise representation algorithm to construct a representation of  in terms of a small number of learning sequences.  All removable sets may be found efficiently, for spaces constructed from learning sequences, by listing all base sets and testing the above conditions.
\index{base}

\subsection{The Outer Fringe}

The generation of sets that may be added to a learning space  to form an augmented learning space is somewhat more problematic, as it cannot be done efficiently by generating and modifying the base of . If  is any state of , the sets  that can be added to  are exactly those for which  belongs to the intersection of the outer fringes of states of the form .
For, such an  cannot belong to a state  already, as the outer fringe of that state would not contain ; adding  as a state preserves accessibility through ; and membership in the intersection ensures the union closure property of the augmented space. On the other hand, if  does not belong to the outer fringe of some , then  cannot be added to , as the resulting space would not contain the union of  and .

Thus, to generate all sets that may be added to , we need merely list all states of , list for each state  the states  by computing the outer fringe of , and intersect the outer fringes of the states . Each such set is listed only once by this procedure, as the set  for which it is listed must be unique or it would already belong to  by union closure. If we add a set  as a new state to a learning space , the base of the new learning space consists of the newly added set, together with all base members of  that are not of the form .
\index{state generation}

\section{Theoretical Investigations}

We describe here the results of some investigations on the mathematics of and algorithms for learning spaces, less directly related to efficient and flexible knowledge assessment. We state our results here in a more formal theorem-proof style than we use for the rest of this chapter.
 
\subsection{Fibers of Projections}

We discussed briefly earlier the possibility of forming reduced state spaces by inverting projections: if we believe that a student knows the concepts in a set , and that the student doesn't know the concepts in a set , what structure can we see in the subset of learning space states  consistent with those beliefs? The states in  are exactly those that project to  in . Therefore, we can view  as a \emph{fiber} of the projection, the inverse image of . What combinatorial properties does such a fiber have?
\index{projection}
\index{fiber}

We observe that, more generally, if  is any family of sets, and  denotes the subfamily of sets in  that are supersets of  and disjoint from , then closure under unions or intersections of  leads to the same property of . For, taking the union or intersection of two sets in  cannot form a set that is not a superset of  or not disjoint from .
Similarly, if  is well-graded, then  inherits the same property.
Learning spaces derived from partial orders can be characterized as well-graded set families that are closed under unions and intersections, so if  is a learning space formed in this way, so is
.

For general learning spaces, however, a fiber need not itself form a learning space. For instance, for the learning space  of Figure~\ref{fig:nob}, the projection  has two states,  and , but the inverse image  of  is the family of three states , which does not form a learning space as it fails the accessibility property. However, the fiber of a learning space projection is well-graded and closed under unions, and therefore forms a \emph{closed medium} \citep{falma02}.
\index{medium!closed}
\index{Falmagne, J.-Cl.}
\index{Ovchinnikov, S.}

If  is a learning space, we define an \emph{upper subfamily}  of  to be a subset of the states of , such that if  are two states of  with  then .
\index{upper subfamily}

\begin{theorem}
Any fiber  forms an upper subfamily of a learning space ,
and any upper subfamily  of a learning space  can be represented as a fiber  for some learning space  and sets  and .
\end{theorem}

\begin{proof}
First, let  be a fiber, and let . Then  is the unique maximal
state of , and . We assert that  is an upper subfamily of . For, if , , and ,
then  like  must contain all members of , so  must belong to .

In the other direction, suppose  is a learning space, and  is an upper subfamily of . Then,  can be represented as a fiber of a learning space, as follows. Form  by adding to  the sets of the form  where  and .
Project  onto ; then  is the inverse image of  under this projection.
\end{proof}

\begin{figure}[t]
\centering\includegraphics[width=3in]{unfiber}
\caption{A well-graded and union-closed set family that cannot be the fiber of a projected learning space.}
\label{fig:unfiber}
\end{figure}

Figure~\ref{fig:unfiber} shows a well-graded union-closed set family  that cannot be an upper subfamily  of a learning space . For  would have to contain a singleton as one of its states; by symmetry we can assume without loss of generality that this singleton is . Then  would contain . But  is not in  while its subset  is. Therefore, the family of fibers of projections of learning spaces is a proper subclass of the family of closed media.
\index{medium!closed}

\begin{theorem}
Let  be a given family of sets, the closure under union of which generates a family .
Then in time polynomial in the total size of the sets in  we can determine whether  is an upper subfamily  of a learning space , and if so find a collection of learning sequences defining .
\end{theorem}
\index{upper subfamily}
\index{worst case time complexity}
\index{learning sequence}

\begin{proof}
First, observe that we may test membership of a set  in  in polynomial time, as  if and only if . We define a set  to be \emph{safe}
if, for any , the union  also belongs to . Equivalently,  is safe if and only if, for any , the union  belongs to , so we may test safety in polynomial time.
If two sets  and  are safe, so is their union. If , then  is an upper subfamily of  if and only if all sets in  are safe.
 
We say that a set  is \emph{reachable} if we can find a sequence  of all the elements of  such that all prefixes of this sequence are safe. If  and  are both reachable, with , then we may assume  forms an initial subsequence of , for if not the concatenation of  with the members of  not belonging to  forms another sequence on the elements of  with the same property that all prefixes are reachable, by the union-closure of safe sets. Therefore, we may test reachability of  in polynomial time, and find a sequence  for any reachable , by the following greedy algorithm:
\index{greedy algorithm}

\begin{enumerate}
\item Initialize  to empty.
\item While  does not contain all elements of :
\begin{enumerate}
\item If there exists  in  that can be added to  to form a longer safe prefix, do so.
\item Otherwise, terminate the algorithm and report that  is not reachable.
\end{enumerate}
\item Return .
\end{enumerate}

If  is an upper subfamily of a learning space , and , then all sets in  must be safe, so  is reachable via a learning sequence of  for which  is a prefix. Therefore, if any set in  is not reachable, then  is not an upper subfamily of a learning space. On the other hand, if every set  in  is reachable via a sequence ,
then the unions of prefixes of these sequences form a learning space  containing  in which every set is safe; therefore  is an upper subfamily of this learning space .
The base of this learning space consists of certain prefixes of the sequences  for , and we may use our concise representation algorithm to convert this base into a collection of learning sequences representing .
\end{proof}
\index{base}
\index{learning sequence}

This result gives us hope that there exists a simple combinatorial description of the set families that form upper subfamilies of learning spaces.
However, finding the \emph{smallest} learning space for which  is an upper subfamily is considerably more difficult.

\begin{theorem}
It is NP-complete to determine, for a given family of sets ,  the closure under union of which generates a family , and for a given integer , whether  is the upper subfamily of a learning space  with .
\end{theorem}
\index{NP-completeness}

\begin{proof}
Note that, by our previous construction,  need only be polynomial in the total size of .
Therefore, if there exists a suitably small , we may exhibit it by listing the additional sets
added to  to form , and test whether it correctly solves the problem by testing the safety of the additional sets, the union-closure of the added sets, and the reachability of each set in  by a sequence each prefix of which is a union of the added sets and of other sets already in .
Therefore, the problem is in NP.

To show NP-hardness, we reduce from the Vertex Cover problem, in which we are given a connected undirected graph  and an integer , and must find a set of  or fewer vertices containing at least one endpoint of each edge of the graph. From an instance of Vertex Cover, we define  as containing a set of the two endpoints of each edge; the family  generated from  consists of all sets of two or more vertices from connected subgraphs of .
Any learning space containing  must contain a subfamily of singleton sets for the vertices in a vertex cover of , or some set in  would not be accessible, and conversely if  is any vertex cover of  we may form a learning space  with  by including a singleton set for each member of .
\end{proof}
\index{vertex cover}

\subsection{Decomposing a Learning Space}

If  and  are two learning spaces on the same set of concepts, we may define a \emph{join} operation
combining the two into a single larger learning space:

This operation is commutative, associative, and \emph{idempotent} (that is,  for any learning space ); therefore it forms a \emph{semilattice} though not, in general, a medium.
\index{join!of two learning spaces}
\index{semilattice!of learning spaces}

The problem of computing the convex dimension of a learning space , and of representing  via a minimum number of learning sequences, can be viewed in this way as expressing  as the join of a small number of simpler learning spaces: if we define a \emph{totally ordered learning space} to be a learning space formed from a single learning sequence, then  has convex dimension at most , and can be represented via  learning sequences, if and only if it is the join of at most  totally ordered learning spaces. The totally ordered learning spaces form the smallest subclass of learning spaces such that all learning spaces can be represented as joins of learning spaces in the subclass, for a learning space is irreducible for the join operation if and only if it is totally ordered.
\index{totally ordered learning space}
\index{learning sequence}

The algorithmic success of our learning sequence representation motivates us to seek more powerful forms of decompositions of learning spaces as joins of broader classes of learning spaces.
For, if  can be represented as a join of a small number of learning spaces , each of which in turn has an efficient algorithmic representation, then it seems likely that we may also perform
various algorithms efficiently for  itself, with an efficiency depending in part on the number  of learning spaces used to represent . For instance, with such a representation, a base for  may be found as a subfamily of the union of bases of . But to use representions based on joins of special classes of learning spaces, we need to be able to find such representations efficiently. We have already shown that, if  is the family of totally ordered learning spaces, then a representation of any learning space as the join of a minimum number of members of  can be found efficiently. For which other families  is such efficient decomposition possible?
\index{join!of two learning spaces}

\begin{theorem}
It is NP-complete, given a base  of a learning space , and given an integer , to determine whether  is the join of  or fewer quasi-ordinal spaces.
\end{theorem}
\index{NP-completeness}
\index{quasi-ordinal space}

\begin{proof}
If  is the join of  or fewer quasi-ordinal spaces, we may exhibit a partial order for each such space, test whether each base set of each such space belongs to , and test whether each base set of  belongs to at least one of these spaces; therefore, testing whether   is the join of  or fewer quasi-ordinal spaces can be done in NP.

To show NP-hardness, we reduce from the known NP-complete problem of graph edge coloring.
In this problem we are given an undirected graph  and an integer  and must assign  distinct colors to the edges of  in such a way that no two edges that share an endpoint have the same color.
From  we form a learning space  with one concept per vertex , and two concepts  and  per edge . We include as base sets of  the singleton sets the singleton sets  for each edge, the sets  for each edge  having an endpoint , and the sets  for each edge  having two endpoints  and .
\index{edge coloring}
\index{chromatic index}

Then, if  is a quasi-ordinal learning space containing a state , the partial order defining  must have  (and no other concept) preceding  and .
Therefore, it can contain no other state  for an edge  sharing an endpoint with . So, if  is the join of  quasi-ordinal spaces, we can find a coloring of the edges of  with  colors, by coloring each edge according to which member of the join contains state  .  Conversely, for any independent set of edges, we can find a quasi-ordinal subspace of  containing the states   for each edge in the independent set, so for each -edge-coloring of  we can find a representation of  as the join of  quasi-ordinal spaces.
\index{join!of two learning spaces}

Since decomposition into  quasi-ordinal spaces is in NP and a known NP-hard problem can be reduced to it, it is NP-complete.
\end{proof}

The same proof shows that the problem remains hard when  is fixed to any constant greater than two. A similar reduction (omitting the concepts ) shows that it is NP-complete to represent a learning space as a join of three or more \emph{hierarchies}, where a hierarchy is defined as a quasi-ordinal learning space for an order the Hasse diagram of which has at most one outgoing edge per concept. A hierarchy can be characterized as a learning space each two base sets of which are either disjoint or nested, so one can test whether a learning space  is a join of two hierarchies by forming a graph in which we connect two base sets of  by an edge whenever they do not satisfy this condition and testing whether this graph is bipartite; we omit the details. We have not yet determined whether it is possible to test efficiently whether a given learning space is the join of two quasi-ordinal spaces.
\index{hierarchy}
\index{Hasse diagram}

\subsection{Antimatroids as Algebras}

We have described learning spaces (or antimatroids) here as union-closed accessible families of sets, and they are also commonly formalized as prefix-closed formal languages satisfying a certain exchange axiom \citep{KorLovSch-91}. Media theory \citep{falma02,eppst07b} leads to alternative formalizations involving geometric graphs and finite automata, and previous work on drawing antimatroids \citep{eppst06} leads to a characterization in terms of arrangements of translated orthants. But it turns out that antimatroids also have a fairly natural algebraic characterization. That is, we view antimatroids in terms of the algebraic properties of their union operation rather than as systems of explicit sets and elements. Perhaps this is not surprising, as the first work on antimatroids \citep{dilworth40} characterized them algebraically in terms of lattices, but we use semilattices instead.
\index{formal language}
\index{prefix}
\index{exchange axiom}
\index{geometric graph}
\index{finite automaton}
\index{graph drawing}
\index{orthant}
\index{lattice}
\index{semilattice}
\index{Korte, B.}
\index{Lov{\'a}sz, L.}
\index{Schrader, R.}
\index{Falmagne, J.-Cl.}
\index{Ovchinnikov, S.}
\index{Eppstein, D.}
\index{Dilworth, R.P.}

The model for this is an algebraic description of semilattices and quasi-ordinal spaces (equivalently, finite distributive lattices or finite lattices of sets). A \emph{monoid} consists of an associative binary operation (such as multiplication or string concatenation or function composition) on a set of objects, together with an identity object for the operation (such as, respectively, 1 or the empty string or the identity function); here we write the monoid operation as multiplication. A \emph{semilattice} is a monoid that is commutative ( for all ) and idempotent ( for all ). The \emph{divisibility} relation on a semilattice ( if, for some , ) forms a partial order, and the semilattice operation can be interpreted as a join or least upper bound operation for that partial order. By analogy with ring theory we can define an \emph{irreducible} object in a semilattice to be an  such that if  then  or , and we can define a \emph{prime} object in a semilattice to be an  such that if  then  or . Lattices are normally defined in terms of two compatible semilattices, one for joins and one for meets, but as we see below the concepts of irreducibility and primality allow us to define finite lattices of sets in terms of the join operation only.
\index{quasi-ordinal space}
\index{lattice!of sets}
\index{lattice!distributive}
\index{monoid}
\index{semilattice}
\index{divisibility!in a semilattice}
\index{irreducibility!in a semilattice}
\index{primality!in a semilattice}

\begin{lemma}
In any semilattice, if  is prime then  is irreducible.
\end{lemma}

\begin{proof}
Let  be prime, and let . Since , by primality  or , and without loss of generality , but  so .
\end{proof}

\begin{lemma}
\label{lem:prodirrediv}
In a finite semilattice every object is the product of its irreducible divisors.
\end{lemma}

\begin{proof}
The result is trivial if an object  is irreducible, and otherwise if  the result follows by induction on the number of irreducible factors of  and .
\end{proof}

\begin{theorem}
A finite semilattice can be represented as the union operation of a quasi-ordinal space if and only if every irreducible is prime.
\end{theorem}
\index{quasi-ordinal space}

\begin{proof}
In a quasi-ordinal space, or more generally an antimatroid, the irreducibles are the sets of a base of the space, and as we have seen for a quasi-ordinal space these sets are of the form  for some element  of the partial order defining the space. In the semilattice for the union operation of the space, divisibility is the subset relation,  if and only if , and if  and  then  or , so the definition of primality follows.

In the other direction, suppose in a semilattice every irreducible is prime, and represent every semilattice object  by the set  of primes dividing . Then by Lemma~\ref{lem:prodirrediv},  is the product of , so each  corresponds to a single object .  If ,  by primality, so the semilattice operation is represented by the union operation on the sets . Every set  is a lower set on the restriction of the divisibility order of the semilattice to the primes, and if  is a any lower set in this order having product  then , so the original semilattice is isomorphic to the quasi-ordinal space generated by the divisibility order on the primes.
\end{proof}

Antimatroids lie between semilattices and quasi-ordinal spaces in generality, so it seems natural to axiomatize them similarly. However, it doesn't appear to work to use primes and irreducibles as with lattices: that axiomatization corresponds set elements with the minimum lattice object containing that element, but in antimatroids there may be no unique minimum object. Instead it works better to turn things upside down, and correspond elements with the maximum object not containing the element.

\begin{lemma}
In a finite semilattice,  is irreducible if and only if it has a single \emph{predecessor} , such that  and, for any , if  then  or .
\end{lemma}
\index{predecessor}

\begin{proof}
If  and  violate this definition, as do  and some  for each , the values of  would form an infinite ascending chain of proper divisors of  violating the assumption of finiteness. Conversely, if  has a single predecessor , it must be irreducible, as any product of its proper divisors would also divide  and therefore be unequal to .
\end{proof}

By analogy, in any semilattice, we define  to be \emph{singular} iff it has a single \emph{successor} , such that  and, for any , if  then  or . We identify any object  in the semilattice with the set  of singular objects that  does not divide.

\begin{lemma}
For any  and , .
\end{lemma}

\begin{proof}
If , then . But in any semilattice, for any , if  and  then , so we can conclude that  or , and therefore  or .
Conversely if , so , then  and .
\end{proof}

\begin{theorem}
\label{thm:Nsemilat}
Any finite semilattice is isomorphic to the semilattice formed by the union operation on the sets .
\end{theorem}

\begin{proof}
From the previous lemma, it remains only to show that for any , . Suppose to the contrary that  has  for some set  of singular objects. By the previous lemma,  is closed under unions, so it contains a unique maximal object, and we may choose from  some two objects  with . Let ,
and let  be a maximal object in ;  is nonempty as it contains  and a maximal  exists by finiteness. Then  must be the unique successor to , for if  had a proper multiple  not divisible by , then  would be a multiple of  and nonmultiple of  that is larger than  contradicting the maximality of . Therefore,  is singular, and  contains  but  doesn't, contradicting the assumption that .
\end{proof}

\begin{figure}[t]
\centering\includegraphics[scale=0.45]{slnam}
\caption{Two semilattices that are not antimatroids.}
\label{fig:slnam}
\end{figure}

In some sense the sets  form a minimal representation by sets for any finite semilattice. It remains to find conditions on the semilattice forcing these sets to be accessible. There are many possible conditions that do so; the simplest we have found is the following. We define an \emph{equalizing pair}  to be a pair of objects  such that there exist  and  with  and . We say that a finite semilattice has \emph{separated equalizers} if, for each equalizing pair , there exists  with  and . This definition is motivated by the two semilattices shown in Figure~\ref{fig:slnam}. In the left semilattice, the pair  is equalizing for  and , while in the right semilattice, the pair  is equalizing for  and ; however, in each case there is no  between  and . Thus, from the lemma below, the property of having separated equalizers distinguishes these two semilattices from antimatroids.
\index{antimatroid}

\begin{lemma}
Every antimatroid has separated equalizers.
\end{lemma}

\begin{proof}
Let  be an equalizing pair for  in an antimatroid. We can assume without loss of generality that . Then, in order for  but ,  must be a superset of . It must be a proper superset of this set, else we would have . Therefore,  and we can find  between  and  using the chain property for antimatroids.
\end{proof}

\begin{lemma}
\label{lem:depent}
In a semilattice with separated equalizers, suppose that there exist objects , , , and , with , , and . Then there exists another object , with , such that either , or   and  are incomparable.
\end{lemma}

\begin{proof}
An equivalent description of the premise of the lemma is that  form an equalizing pair for some  and  with . We use induction on the length of the longest chain of objects between  and . Because  form an equalizing pair for , there must exist  between  and .
If  is a proper divisor of , then  form an equalizing pair for  and  and the result follows by induction.  If , we have proven the lemma. If , then  form an equalizing pair for  and  and the result follows by induction. In the remaining case,  and  are incomparable.
\end{proof}

\begin{figure}[t]
\centering\includegraphics[scale=0.45]{depent}
\caption{Graphical explanation of Lemma~\ref{lem:depent}.}
\label{fig:depent}
\end{figure}

A graphical explanation of Lemma~\ref{lem:depent} is shown in Figure~\ref{fig:depent}.

\begin{lemma}
\label{lem:sing-is-max}
In a semilattice with separated equalizers, if  is irreducible with predecessor ,  is singular with successor , , and , then .
\end{lemma}

\begin{proof}
By singularity of , , so if  then  form an equalizing pair for . But if  and  are separated by , , then  could not be the predecessor of .
\end{proof}

\begin{lemma}
\label{lem:irredsingeq}
In a semilattice with separated equalizers, let  be irreducible with predecessor ,
 be singular with successor , , and . Then there can be no  with , , and  incomparable to .
\end{lemma}

\begin{proof}
Suppose to the contrary that  exists.
Then by singularity of , . If , then  would be an equalizing pair for  and , and the existence of  would violate the assumption that  is 's predecessor.
Otherwise,  is a proper divisor of , so  is an equalizing pair for  and .
By Lemma~\ref{lem:depent}, there exists  with  where either  or   and  are incomparable;   must be incomparable to .
By repeating this process we can find an infinite descending sequence of , , , etc., violating the assumption of finiteness of the semilattice, so  cannot exist.
\end{proof}

\begin{lemma}
\label{lem:irred-single}
In a semilattice with separated equalizers, let  be irreducible with predecessor .
Then .
\end{lemma}

\begin{proof}
 by Theorem~\ref{thm:Nsemilat}.
By Lemma~\ref{lem:irredsingeq}, any two members of  must be comparable, but by Lemma~\ref{lem:sing-is-max}, any two members must be incomparable.
Therefore .
\end{proof}
 
\begin{theorem}
A finite semilattice can be represented as an antimatroid if and only if it has separated equalizers.
\end{theorem}

\begin{proof}
For any x in such a semilattice, there is a  with : represent  as a minimal product of irreducibles, and form  by replacing one of the irreducibles in this product by its predecessor. Therefore the sets  are accessible as well as union-closed, and this set family forms an antimatroid. Conversely we have seen that any antimatroid forms a semilattice that has separated equalizers.
\end{proof}

\subsection{Different Definitions of Dimension}

Our algorithms for learning spaces have been centered around the concept of \emph{convex dimension}, the minimum number  of learning sequences needed to define the learning space .
However, there are several other natural concepts of dimension for learning spaces.
We may define the \emph{base dimension} of a learning space  to be the cardinality of its base.
The \emph{lattice dimension}  \citep{eppst05} is the minimum dimension  of an integer lattice  into which the states of  may be embedded, in such a way that the  distance between the embeddings of two states equals the cardinality of their symmetric difference; like the convex dimension it can be calculated efficiently by an algorithm based on maximum matching in an associated bipartite graph. And, the \emph{order dimension} 
is the minimum dimension  of a Euclidean space  into which the states of  may be embedded, in such a way that, for two states  and ,  if and only if the coordinates of  are all less than or equal to the corresponding coordinates of . In some sense the order dimension is very closely related to the convex dimension, as both are defined as the minimum number of sequences of items needed to define the learning space, but in the case of convex dimension the sequences are of elements of  while in the case of order dimension the sequences (formed by each coordinate of the embedding) are of states of . We may also view the cardinality  as a dimension: it is the \emph{isometric dimension} of , that is, the least dimension of a hypercube into which  can be isometrically embedded.
\index{dimension}
\index{convex dimension}
\index{base dimension}
\index{lattice dimension}
\index{order dimension}
\index{isometric dimension}
\index{Euclidean space}
\index{Eppstein, D.}
\index{base}
\index{learning sequence}

For any learning space , these different quantities satisfy the following inequalities and relations:

\begin{itemize}
\item . This follows as we may represent  using a separate learning sequence for each base set.

\item . Each element of  must be the removable element of at least one base set.

\item , as each learning sequence in a representation of  by learning sequences can contribute at most  base sets.

\item , by our characterization of  as the size of the largest antichain in the base, and Sperner's Theorem bounding the size of an antichain in any family of sets.

\item  \citep{KorLovSch-91}. This can be seen via the embedding into  in which we map  to , for this embedding satisfies the requirements of the definition of order dimension.
\index{Korte, B.}
\index{Lov{\'a}sz, L.}
\index{Schrader, R.}

\item  if and only if , from our work on drawing learning spaces \citep{eppst06}.
\index{Eppstein, D.}
\index{graph drawing}

\item : in any lattice embedding of , all states must be mapped to a single orthant of the lattice in order to satisfy the union closure property of , so the lattice embedding must again satisfy the requirements of the definition of order dimension.
\index{orthant}

\item , as the characteristic function embeds , or more generally any family of sets on the elements of , into the subset  in such a way that  distance equals symmetric difference cardinality.
\index{characteristic function}

\begin{figure}[t]
\centering\includegraphics[width=2.5in]{4cube}
\caption{A powerset on four elements.}
\label{fig:4cube}
\end{figure}

\item .
This follows from the fact that a lattice embedding of  must lie within a product of intervals  and the fact that no two members of an antichain within this product can share all but one coordinate.
\end{itemize}

\noindent
We now describe examples of learning spaces that are extremal for some of these inequalities.

\begin{description}
\item[A chain.] \hfill\\
A learning space defined from a single learning sequence has
 but
.
\index{learning sequence}

\item[A powerset.] \hfill\\
The family of all subsets of an -element set (Figure~\ref{fig:4cube}) has .
\index{power set}

\begin{figure}[t]
\centering\includegraphics[width=4in]{prefixsuffix}
\caption{The learning space defined by a learning sequence and its reverse. Figure from \citet{eppst06}.}
\label{fig:prefixsuffix}
\end{figure}

\item[A learning sequence and its reverse.] \hfill\\
The learning space defined by two learning sequences, one the reverse of the other
(Figure~\ref{fig:prefixsuffix}) has
 but
 and .
\index{Eppstein, D.}

\item[A learning space with a large base.] \hfill\\
Let  be a set of  elements, and  be a designated element from .
Define  to consist of the sets that either do not contain , or
contain at least  elements.
Then  is accessible, as for any set , if  then 
while if  then all subsets of  are in .
 is also closed under unions, so it forms  a learning space.
The base of  consists of  together with all subsets of exactly
 elements of ; the subset of the base formed by omitting  is an antichain.
Therefore ,
but  and
, matching to within a constant factor the  upper bound on .

\begin{figure}[t]
\centering\includegraphics[width=4in]{zigzag}
\caption{A learning space with large convex dimension but low lattice dimension.}
\label{fig:zigzag}
\end{figure}

\item[A three-dimensional zigzag.] \hfill\\
Let  be a given number, and let  consist of the three-dimensional integer lattice points with coordinates , , and such that, if , then  (Figure~\ref{fig:zigzag}).
The semilattice of coordinatewise maximization on  can be represented as a learning space  with . For this space,

but , as there is an antichain in the base consisting of all minimal points with . Similar examples in higher dimensions show that, in general,  can be lower bounded by ,
nearly matching our  upper bound.
\end{description}

It would be of interest to determine the algorithmic complexity of calculating the order dimension of a learning space. For arbitrary partial orders, calculating the order dimension is NP-complete \citep{Yan-SIADM-82} but it is unclear whether the reduction proving this can be made to apply to learning spaces.
\index{order dimension}
\index{worst case time complexity}
\index{Yannakakis, M.}

\section{Future Work}

Although we have made significant progress in learning space implementation, we believe there is plenty of scope for additional investigation, particularly on the following topics:

\begin{description}
\item[Visualization of learning spaces.] \hfill\\
Quasi-ordinal spaces may be visualized by drawing their Hasse diagrams as graphs, but this technique does not work so well for more general learning spaces.
In earlier work \citep{eppst04,eppst06} we found algorithms for drawing the state-transition diagrams of learning spaces and more general media, however these work best when the space being drawn has a well-behaved embedding into a low-dimensional Euclidean space. Can we generalize these drawing approaches to learning spaces with higher convex dimension? Is there a possibility of a hybrid approach that  draws portions of a learning space as a Hasse diagram on concepts and resorts to the more complex state space only when necessary?
\index{graph drawing}
\index{state-transition diagram}
\index{Euclidean space}
\index{Hasse diagram}
\index{Eppstein, D.}

\item[Counting states.] \hfill\\
Is there an efficient method for counting or estimating the number of states in a learning space, without taking the time to generate all states? This would have implications for our ability to set sample sizes appropriately in ALEKS's knowledge assessment algorithms, as well as for calculating more accurate priori probability distributions on projected states when using projections of learning spaces to speed up the assessment algorithm. From past experience with similar combinatorial counting problems \citep[e.g.,][]{JerSinVig-STOC-01} we expect that the complexity of a randomized approximation scheme for the counting problem is likely equivalent to the complexity of sampling states from the learning space uniformly at random, which seems to be of interest independently.
\index{state generation}
\index{randomized algorithm}
\index{approximation algorithm}
\index{Jerrum, M.}
\index{Sinclair, A.}
\index{Vigoda, E.}

\item[Inference of error rates.] \hfill\\
ALEKS  currently assumes fixed rates of careless mistakes and lucky guesses, which it uses in its calculation of likelihoods that a student knows each concept in a learning space. But one could also envision a more sophisticated Bayesian assessment procedure that treats the chance of careless errors as a variable with an a priori probability distribution, and attempts to infer a maximum likelihood value of this variable based on the student's answers. Such a procedure would likely be based on an EM-algorithm approach in which one alternates applications of the likelihood calculation described here with an algorithm for estimating the careless error probability given the calculated likelihoods, and would allow the system to better fit its model to each student's performance. However the details of such an approach still need to be worked out.
\index{careless mistake}
\index{lucky guess}
\index{Bayesian statistics}
\index{EM algorithm}

\item[Reconciliation of expert opinions.] \hfill\\
Along with its applications to concise representation of media, the join may be useful for a problem arising when constructing learning spaces from the answers of experts \citep{dowling:93a}: two different experts may give quite different answers when asked what they believe about the prerequisite structure of a given set of concepts, leading to quite different learning spaces on those concepts. The construction procedure of \citet{dowling:93a} involves asking experts a series of questions about whether feasible knowledge states can exist with certain combinations of concepts, but the answers to these questions have only been found reliable when the combinations involve at most two concepts at a time; the learning spaces generated by limiting the questioning to such combinations are necessarily quasi-ordinal.  To reliably generate more complex learning spaces, it seems necessary to combine the results from questioning multiple experts.
The join provides a mathematical mechanism for reconciling those answers and finding a common learning space containing as states any set of concepts believed to form a state by any of the experts,
but the learning spaces constructed in this way are likely to be much larger than necessary.
More research is needed on methods for combining information from multiple experts to generate learning spaces of size comparable to the space that would be constructed by questioning a single expert, while simultaneously taking advantage of the multiplicity of experts to generate spaces that more accurately model the students' knowledge.
\index{join!of two learning spaces}
\index{semilattice!of learning spaces}
\index{expert knowledge}
\index{knowledge engineering}
\index{Dowling, C.E.}

\item[Even faster state space generation.] \hfill\\
Our algorithm for generating the states in an -concept quasi-ordinal space takes time  per generated state, without assumption, and may often be faster, while a similarly fast time bound of  per generated state for learning spaces generated by  learning sequences can only  be shown with an additional assumption of constant time bitvector operations for maintaining and updating the  values of the generated states. Additionally, we have briefly described an algorithm for listing all states in the fiber  of a projection of a learning space, given beliefs that a student knows the concepts in  and does not know the concepts in , that do not match these efficiencies. Can these state generation algorithms be improved to the point where more efficient worst case guarantees on their performance can be proven?
\index{state generation}
\index{minimum excluded element}
\index{worst case time complexity}

\item[Faster upper fringe construction.] \hfill\\
The algorithm we implemented for our construction of the family of sets that can be added to a learning space to form new larger learning spaces involves generating all states of the learning space. However, there may be many fewer fringe sets than there are states.  In some sense, the base of a learning space consists of the minimal sets in the space, while the outer fringe consists of the maximal sets not in the space.  Thus, it is plausible that one could adapt hypergraph transversal algorithms \citep{FreKha-Algs-96}, which can be used to convert minimal sets in a family to maximal sets not in a family for certain other types of set families, to the purpose of finding the outer fringe of a learning space in time pseudopolynomial in the number of sets in the outer fringe. Such a result would also have implications for the computational complexity of inferring a learning space from questions asked of an expert \citep{dowling:93a}. However, we have not worked out the details of such an efficient  algorithm for listing upper fringe states.
\index{fringe!of a learning space}
\index{Michael L. Fredman}
\index{Khachiyan, L.}
\index{Dowling, C.E.}

\item[Structure of the family of learning spaces.] \hfill\\
We know from the work of \citet{Thi-01} that, when one learning space forms a subfamily or superfamily of the other, we can find a shortest path from one to the other by adding and removing states, such that each set family in this shortest path is also a learning space. That is, the family of learning spaces has a chain property similar to that of individual learning spaces. This fact motivates the calculation of the fringes of a learning space, as the sets in
the fringe represent potential neighbors in such paths. We also know that the family of learning spaces on a given domain forms a semilattice under the join operation, which is however not the same as simple union of set families. And we know that the family of learning spaces is not in general well-graded, so it does not form a medium under operations that add and remove sets. What other structure does the family of learning spaces have, and how can that structure help us quickly adapt a learning space to changing information about the possible knowledge states of students?
\index{Thi{\'e}ry, N.}
\index{adaptation}

\item[Question selection strategy.] \hfill\\
We have only indirectly addressed the issue of which question to ask the student next, in the assessment procedure, after likelihoods of each concept have been calculated.
As currently implemented, ALEKS selects the question with likelihood closest to 50\% of being known, but that may not be the optimal selection strategy. It seems likely that a somewhat better strategy would be to select the question with likelihood closest to 50\% of being answered correctly; due to the different rates of careless errors and lucky guesses this strategy differs from the currently implemented one, but it also depends on having an accurate estimate of the student's careless error rate, which the current assessment procedure does not supply. Also, if there are multiple questions with similar likelihoods, it may be best not to choose the one with likelihood closest to 50\%, but instead to perform some lookahead in the sequence of questions, and ask a question such that whichever answer is given will again lead to a situation where some question has likelihood close to 50\%. The effect of an improved question selection strategy could be to reduce the number of questions needed to assess each student's knowledge, over and above the reduction afforded by more accurately defining the learning space on which the assessment is based. In addition, the outcome of a question may not actually be binary: ``don't know'' may be treated differently than an incorrect answer, and the nature of the errors in an incorrect answer may yield some insight about the student's knowledge. It seems likely that these brief initial observations could be significantly expanded with more thought.
\end{description}

\section{Conclusions}

We have shown that a computer representation of learning spaces by learning sequences can approach the efficiency of the existing quasi-ordinal space representation for ALEKS's knowledge assessment algorithms, while allowing a broader class of learning spaces that may more easily be adapted by adding and removing states. We believe that the algorithms described here are sufficiently detailed and efficient to be suitable for implementation within ALEKS.

We have also performed theoretical investigations concerning learning spaces. We have described how to recognize spaces formed from a learning space when we assume certain facts about the state of a student's knowledge, and we have investigated the algorithmic complexity of recognizing learning spaces that can be decomposed into joins of a small number of simpler spaces. We have investigated alternative mathematical representations of learning spaces, and we have compared the convex dimension fundamental to our computer representation to several other important numerical measures of a learning space's size.

Finally, we have identified multiple areas where more research may lead to additional practical algorithms or theoretical insights concerning learning spaces.

\ifstandalone

\raggedright
\bibliographystyle{plainnat}
\bibliography{grand}

\end{document}
