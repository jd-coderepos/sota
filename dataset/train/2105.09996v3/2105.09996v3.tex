

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2021}
\usepackage{times}
\usepackage{latexsym}
\usepackage{soul}
\renewcommand{\UrlFont}{\ttfamily\small}

\usepackage{microtype}

\aclfinalcopy 



\newcommand\BibTeX{B\textsc{ib}\TeX}
\newcommand{\todo}[1]{{\color{red}{\small\bf\sf [TO DO: #1]}}}

\usepackage{amsfonts}
\usepackage{url}
\usepackage{xcolor}
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{bbm}



\usepackage{amsmath,bm}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{microtype}





\title{VLM: Task-agnostic Video-Language Model Pre-training\\for Video Understanding}

\date{}

\author{Hu Xu, Gargi Ghosh, Po-Yao Huang, Prahal Arora, Masoumeh Aminzadeh \\ 
\textbf{Christoph Feichtenhofer}, \textbf{Florian Metze} \and \textbf{Luke Zettlemoyer} \\
Facebook AI\\
Carnegie Mellon University\\
\texttt{\{huxu,gghosh,berniehuang,prarora,masoumeha,}\\
\texttt{feichtenhofer,fmetze,lsz\}@fb.com}
}

\begin{document}
\maketitle
\begin{abstract}


We present a simplified, task-agnostic multi-modal pre-training approach that can accept either video or text input, or both for a variety of end tasks.
Existing pre-training are task-specific by adopting either a single cross-modal encoder that requires both modalities, limiting their use for retrieval-style end tasks or more complex multitask learning with two unimodal encoders, limiting early cross-modal fusion.
We instead introduce new pretraining masking schemes that better mix across modalities (e.g. by forcing masks for text to predict the closest video embeddings) while also maintaining separability (e.g. unimodal predictions are sometimes required, without using all the input).
Experimental results show strong performance across a wider range of tasks than any previous methods, often outperforming task-specific pre-training. Code is made available at \url{https://github.com/pytorch/fairseq/tree/main/examples/MMPT}. 


\end{abstract}

\section{Introduction}
\label{sec:intro}








We study the challenge of achieving \textit{task-agnostic} pre-training for multimodal video understanding, building on recent unimodal approaches such as pretrained language models for text~\cite{peters-etal-2018-deep,devlin-etal-2019-bert}.
Although certain language models are near task-agnostic~\cite{devlin-etal-2019-bert,lewis-etal-2020-bart} on NLP tasks,
being task-agnostic on multi-modal tasks are more challenging due to cross-modal tasks such as text-video retrieval.
Existing video-and-language pre-trainings are task-specific, which adopt either (1) a cross-modal single encoder  \cite{sun2019videobert,sun2019contrastive,zhu2020actbert} favoring tasks that require cross-modal reasoning (e.g. video captioning), 
or (2) multiple unimodal encoders/decoders \cite{miech2019howto100m,miech2020end,li-etal-2020-hero,luo2020univilm,korbar2020video} combining specific tasks that require separately embedding each modality (e.g. video retrieval). We instead show that it is possible to pre-train a task-agnostic model called video-language model (VLM) that can accept text, video, or both as input.


\begin{figure}[t]
\centering    
\includegraphics[width=2.9in]{img/overview.png}
    \caption{Existing models (upper figure) adopt complex architectures and multiple task-specific training to merge two streams of data to cover a wide range of downstream tasks (such as retrieval or text generation). Our video-language model (VLM) (lower figure) uses a single BERT encoder for task-agnostic pre-training (e.g. only masking tokens, no matching or alignment for specific end tasks) in a joint feature space, while still covering a wide range of tasks (see Figure~\ref{fig:ft}).}
\vspace{-4mm}
\label{fig:overview}
\end{figure}

As shown in Figure \ref{fig:overview},
this task-agnostic single encoder approach has several advantages:
(1) it reduces the complexity of pre-training with multiple losses and models (e.g. \citet{luo2020univilm}), and
(2) it holds less assumption on being close to end tasks as in retrieval-based pre-training \citet{miech2020end} and is as general as classic LMs, and
(3) it encourages feature sharing among modalities when present, without sacrificing separability, and (4) it is more parameter efficient (see Section~\ref{sec:exp}, we achieve strong performance with  sized models).
Table~\ref{tbl:num_param} summarizes the design choices of recent models. 

Our encoder is a transformer block that combines the existing masked frame model and masked language model (MFM-MLM) ~\cite{sun2019contrastive,li-etal-2020-hero,luo2020univilm}
with two new methods to improve the learning of multi-modal fusion. First, we introduce a masking scheme called masked modality model (MMM) that randomly masks a whole modality for a portion of training examples (the rest of the examples goes for traditional MFM-MLM), thereby forcing the encoder to use the tokens from the other modality to produce tokens for the masked modality. 
We then introduce a single masked token loss to replace two (2) losses on video and text separately for MFM-MLM.
Masked token loss uses the embeddings of both video and text tokens to learn joint hidden states for the encoder.


We also show it is possible to fine-tune a single encoder for a wide range of tasks by using task-specific attention masks. 
Experiments demonstrate that it performs well on a wider range of tasks than previous models, including outperforming task-specific pre-training baselines with unimodal encoders of similar hyper-parameters by more than 2\% on retrieval tasks and 1\% on video captioning. 
Note that these results are also achieved with a much smaller model than previous approaches, further demonstrating the improved fusion and sharing across modalities. 


In summary, the main contributions of this paper are as follows: (1) we propose to pre-train a task-agnostic encoder for video understanding;
(2) we introduce masked modality model (MMM) and masked token loss for cross-modal fusion during pre-training without sacrificing separability;
(3) experimental results show that the proposed simple baseline achieves competitive performance with significantly fewer parameters.

\section{Related Work}
\label{sec:related_work}




Numerous multimodal task-specific pre-training models are proposed for downstream visual-linguistic tasks.
In video and text pre-training, existing research adopts different design choices regarding proxy tasks and neural architectures for end tasks \cite{luo2020univilm}.

On one hand, 
VideoBERT \cite{sun2019videobert}, Unicoder-VL \cite{li2020unicoder}, VL-BERT \cite{Su2020VL-BERT:}, UNITER \cite{chen2020uniter}, VLP \cite{zhou2018end},
ActBERT \cite{zhu2020actbert} adopt a \textit{shared} encoder approach, where the vision and text sequences are concatenated and input to a single Transformer\cite{vaswani2017attention} encoder. Although this approach is simple, it limits the types of downstream tasks to those that input both modalities simultaneously.
For example, \cite{sun2019videobert} may not be able to perform joint retrieval tasks and added another decoder for video captioning during fine-tuning.
\cite{zhu2020actbert} uses  token for pairwise metric-learning based retrieval (which is an easier problem but requires a quadratic number of examples and is 50 times slower as reported in \cite{luo2020univilm}).

Meanwhile, many existing approaches adopt or add task-specific pre-training to accommodate retrieval and video captioning tasks (e.g. \textit{two-stream} encoders (video and text separately) and text decoders).
For example, \cite{miech2019howto100m,miech2020end,rouditchenko2020avlnet,ging2020coot,gabeur2020multi,alayrac2020self,patrick2021supportset,multiht100m_bernie} adopts a retrieval task for pre-training.
CBT \cite{sun2019contrastive}, HERO \cite{li-etal-2020-hero}, VideoAsMT \cite{korbar2020video} and UniVL \cite{luo2020univilm} adopt multi-task learning (MTL) to learn retrieval tasks on video and text encoders.
HERO \cite{li-etal-2020-hero} and UniVL \cite{luo2020univilm} adopts another cross-encoder to further learn the fusion of different modality.
UniVL \cite{luo2020univilm} and VideoAsMT \cite{korbar2020video} add another text decoder for video captioning.
Compared with the single-stream input in the shared encoder approach, two-stream encoders typically come with a complex architecture and proxy tasks to cover more end tasks.
To the best of our knowledge, none of the existing works target task-agnostic pre-training. 

\subsection{Image-Text Pre-training}
ViLBERT \cite{lu2019vilbert}, LXMERT \cite{tan-bansal-2019-lxmert} adopt two transformers for image and text encoding separately. 
VisualBERT \cite{li2019visualbert}, Unicoder-VL \cite{li2020unicoder}, VL-BERT \cite{Su2020VL-BERT:}, UNITER \cite{chen2020uniter}, Unified VLP \cite{zhou2020unified} use one shared BERT model.
These models employ MLM and pairwise image-text matching as pretraining tasks which are effective for downstream multimodal tasks.
Our fine-tuning for video captioning is inspired by Unified VLP \cite{zhou2020unified} that adopts attention masks and language model heads of BERT for image-captioning.

\subsection{Video-Text Pre-training}
VideoBERT \cite{sun2019videobert} and CBT \cite{sun2019contrastive} are the first works to explore the capability of pre-training for video-text. 
Although VideoBERT and CBT pre-train the model on multimodal data, the downstream tasks mainly take video representation for further prediction. 
ActBERT~\cite{zhu2020actbert} is a weakly-supervised pre-training method.
It leverages global action information to catalyze mutual interactions between linguistic texts and local regional objects and introduces a transformer block to encode global actions, local regional objects, and linguistic descriptions.
HERO \cite{li-etal-2020-hero} encodes multimodal inputs in a hierarchical fashion. Besides, two new pre-training tasks, video-subtitle matching and frame order modeling, are designed to improve representation learning. 
VideoAsMT \cite{korbar2020video} and UniVL \cite{luo2020univilm} further adopt a BART-style\cite{lewis-etal-2020-bart} text generation task for downstream tasks such as video captioning and UniVL adopts a EnhancedV training stage to mask all text tokens for better learning of generation. 

\section{Pre-training}
\label{sec:pretrain}

As a reminder, our goal is to train a  \textit{task-agnostic} model for various tasks in video-text understanding. 
This section introduces task-agnostic proxies for pre-training. We first describe two masking schemes as a baseline: 
masked frame model (MFM) for video frames and masked language model (MLM) for text tokens \cite{sun2019contrastive,li-etal-2020-hero,luo2020univilm}. 
Then we
introduce masked modality model (MMM) that encourage to  learn the representations of one modality from the other.
Lastly, we introduce masked token loss that 
unifies losses on masked video and text tokens
as a single loss function.

\subsection{Vector Quantization and BERT}
Assume we have a clip  sampled from a video, where  and  corresponds to video modality and text modality, respectively.
Since videos are signals in continuous space, we first extract token embeddings from raw videos.
We decode  into frames and then feed them into a (frozen) video encoder  and a trainable MLP layer to obtain \textit{video tokens}:

where we use a bolded symbol to indicate a sequence and  is a sequence of continuous frames from a video. 
We use S3D~\cite{xie2018rethinking,miech2020end}, which is pre-trained via self-supervised learning on the Howto100M dataset.
The MLP layer allows the hidden size of video tokens to be the same as BERT's hidden sizes : .
Similarly, vectors for text tokens  are obtained via embedding lookup as in BERT.

To simplify multi-modal pre-training, we adopt a single BERT transformer with minimum changes.
We first concatenate video tokens  and text tokens  via the \texttt{[SEP]} token so video and text belongs to one corresponding segment of BERT:

We further mask  as  (detailed in the next subsection) and feed the whole sequence into BERT:

where  indicates the hidden states of the last layer of BERT.
To encourage learning video/text hidden states in a shared space for the masked token loss (introduced in Section \ref{sec:mmmmtl}), 
we use a \textit{shared} head to predict video/text token embeddings via a linear projection layer:

where  and  and  are the weights from the prediction heads of BERT.
In this way, our model learns a joint embedding space for both video and text tokens from inputs to outputs of BERT.
This allows for pre-training a single encoder directly from any existing LMs and the only layer that requires initialization is the MLP layer.

\subsection{MFM-MLM}
Inspired by \cite{sun2019contrastive,li-etal-2020-hero,luo2020univilm}, we adopt masked frame model (MFM) for videos and masked language model (MLM) for text as a baseline.
Note that unlike LMs that typically come with a fixed vocabulary with a special  token, video tokens are innumerable in the continuous space and we mask a video token by setting a video token with all zeros and ask the encoder to recover the video token.
via noisy contrastive estimation (NCE): 

where  is all indexes of video tokens and

where  indicates all non-masked video tokens within the same batch.
The final loss is the sum of both MFM and MLM:

where  is the same as BERT and we omit its details for brevity.
We experiment this classic baseline in Section \ref{sec:exp}.








\subsection{MMM and Masked Token Loss}
\label{sec:mmmmtl}
\noindent \textbf{Masked Modality Model} We introduce masked modality modal (MMM) that masking either all video or all text tokens out for a given example of video-text clip. 
This masking scheme complements MFM-MLM (e.g. in our experiments 50\% of training examples are masked as MMM and the rest 50\% are masked as MFM-MLM).
This encourages the encoder to use tokens from one modality to recover the tokens for the other modality.
This resolves the issue that an encoder may use nearby tokens from their modality for prediction just because tokens from a single modality are closer
As in the lower two (2) sub-figures in Figure \ref{fig:task}, we either mask the whole modality of video or text so this modality can be ``generated'' from the other modality. 
Our experiments indicate that this is critical for pre-training a single encoder 
for retrieval tasks. 

\begin{figure}[t]
\centering
\includegraphics[width=3.in]{img/task.png}
    \caption{Task-agnostic pre-training (e.g. w/o task on retrieval-style alignment): MFM-MLM: 50\% of training examples are masked as masked frame model (MFM) and masked language model (MLM); the rest 50\% examples are masked as masked modality model (MMM) (25\% on text as in the second row and 25\% on video as in the third row).}
\vspace{-3mm}
\label{fig:task}
\end{figure}

\noindent \textbf{Masked Token Loss} We further introduce masked token loss that unifies loss functions for MFM and MLM.
This loss encourages learning a joint token embedding space for video and text and both types of tokens contribute to the prediction of a masked (video or text) token.
This also improves the number of contrasted negative embeddings in two separate losses for MFM and MLM.

We define masked token loss  as the following:

where  is the word embeddings over the vocabulary of BERT and  excludes token  (if  is a text token).
Further,  is defined as:

Note that  can be either a video or text token and one predicted token  must be closer to the ground-truth token embedding (either a video token or word embedding) and be away from other embeddings of video/text tokens.
We perform an ablation study in Section~\ref{sec:exp} to show that  works better than .  

\section{Fine-tuning}
\label{sec:ft}

In this section, we describe how to use different types of attention masks to fine-tune VLM for a variety of tasks, as shown in Figure \ref{fig:ft}.


\begin{figure*}[t]
\centering
\includegraphics[width=6.0in]{img/ft.png}
    \caption{Fine-tuning of downstream tasks: we adopt different types of attention masks for BERT to accommodate downstream tasks that require different modalities: in each box, the upper sub-figure indicates a forward computation; the lower sub-figure indicates squared self-attention mask, where tokens from each row have a weighted sum of columns that are not in white colors.}
\vspace{-3mm}
\label{fig:ft}
\end{figure*}

\subsection{Text-Video Retrieval}
One major challenge of pre-training on a single encoder is how to adapt such a model to joint space retrieval without using unimodal encoders for task-specific pre-training on contrastive loss
(as in Howto100M~\cite{miech2019howto100m,miech2020end}). 
The main reason is that many existing models encode text and video tokens together via self-attention, and one cannot obtain hidden states for text/video alone.


To resolve this, we propose to apply an isolated attention mask with two squared masks that are diagonally placed, as shown in the lower sub-figure of the first box in Figure \ref{fig:ft}.\footnote{One can further reduce  complexity to  ( and  are lengths for video and text, respectively) by feeding video/text separately to BERT but we adopt squared masks for simplicity.}
These two squares disable video and text tokens to attend and see each other, while still allow video and text tokens to use the same self-attention layers for learning representations in the same feature space.
Further, note that the first and second \texttt{[SEP]} tokens of BERT will be used by video and text, respectively, aiming to learn sequence-level features\cite{clark2019does}. The \texttt{[CLS]} is disabled as no need to learn features across video and text.
After forwarding, all hidden states of video and text tokens are average pooled, respectively.
Then we use a contrastive loss on text-video similarity to discriminate a ground-truth video clip from other video clips in the same batch for a given text clip. 
During the evaluation, to ensure video and text are isolated (to avoid leaking ground-truth of a similar pair), we split text and video and forward them separately.
We report an ablation study in Section~\ref{sec:exp} showing that the MMM introduced in the previous section is crucial to ensure that the pre-trained hidden states (for video or text) are a good initialization for retrieval tasks.

\subsection{Action Segmentation}
Action segmentation is to assign each frame of a video with one of the pre-defined labels.
This is similar to the named entity recognition (NER) task in NLP but on video frames.
We feed in VLM with the whole video, a dummy text token, and an isolated attention mask. 
Then we add a classification head (with the number of pre-defined labels) on top of the hidden states for each video token in the last layer of VLM. 

\subsection{Action Step Localization}
In action step localization, each video belongs to a task with multiple steps, where each step is described as a short text.
Then each frame of a video needs to be aligned with a step in text form.
The challenge for applying BERT to action step localization is similar to text-video retrieval: video frames need to be aligned with textual steps in joint space and it is almost impossible for pairwise video/text matching because the number of frame/text pairs is large. 


Similar to the text-video retrieval model, we also apply isolated attention masks to video and text.
The major difference is that we pass video and text separately to BERT.
This is because the video can be several minutes long (more than 100 tokens)
but the number of text labels for each video is fixed (e.g. under 10).
To keep the format of BERT being consistent for multi-modal inputs, we add a dummy text token for video forwarding and a dummy video token for text, respectively.
For a given frame(video token), we compute the distribution of that frame over textual steps via dot products and the softmax function. 

\subsection{Multiple-choice VideoQA}
Multiple-choice VideoQA~\cite{yu2018joint} aligns each video with one out of several candidate answers in the text.
The major difference between action step localization and multiple-choice VideoQA is that the video hidden state is not on frame-level but sequence-level.
We apply isolated attention masks to BERT and forward video and text answers (with dummy tokens), respectively.
Then the answer with the maximum similarity with the video is  reported.
During fine-tuning, we apply contrastive loss on video-text similarity to rank answers.

\subsection{Video Captioning}
Another big challenge of using a single encoder is how to apply generative tasks (such as video captioning) without pre-training an explicit decoder.
We observe that a transformer decoder \cite{vaswani2017attention} has the following major differences from an encoder:
(1) an auto-regressive loss that does not allow a text token to see future tokens;
(2) a prediction head to generate texts.
To resolve (1), one can easily fine-tune the text segment of VLM as auto-regressive loss by passing in shifted tokens and a lower-triangle attention mask to the text segment, as shown in Figure~\ref{fig:ft}. 
To resolve (2), inspired by \cite{rothe-etal-2020-leveraging,zhou2020unified} that uses BERT as a decoder, one can re-use language model heads as prediction heads for generation.
Note that this setting has less architecture design than a standard transformer decoder (e.g. no explicit self-attention on text or cross-attention on video).
The implicit text decoder inside BERT shares self-attention with the video encoder so to save the total number of parameters.




\section{Experiment}
\label{sec:exp}
\subsection{Dataset}

\subsubsection{Pre-training}
We adopt the Howto100M dataset \cite{miech2019howto100m} for pre-training, which contains instructional videos originally from YouTube via searching keywords from wikihow (\url{www.wikihow.com}). 
After filtering the unavailable ones, we get 1.1M videos. We split 4000 videos as the validation set and the rest for pre-training.
On average, the duration of each video is about 6.5 minutes with 110 clip-text pairs.
After removing repeated texts within overlapped clips from ASR, we get about 7.7+ GB texts of captions, with 2.4 tokens per second on average.


\subsubsection{Fine-tuning}
\noindent \textbf{MSR-VTT} \cite{xu2016msr} is a popular dataset for \textit{text-video retrieval} and \textit{VideoQA}.
It has open domain video clips, and each training clip has 20 captioning sentences labeled by humans. 
There are 200K clip-text pairs from 10K videos in 20 categories, including sports, music, etc.
Following JSFusion\cite{yu2018joint,miech2019howto100m}, we randomly sampled 1,000 clip-text pairs as test data.
We further use the QA test data \cite{yu2018joint} as the dataset for multiple-choice VideoQA.

\noindent \textbf{Youcook2} \cite{zhou2017towards} contains 2,000 cooking videos on 89 recipes with 14K video clips from YouTube. 
The overall duration is 176 hours (5.26 minutes on average). 
Each video clip is annotated with one captioning sentence. 
Follow the split setting in\cite{miech2019howto100m}, we evaluate both text-based video retrieval and multimodal video captioning tasks. 
We filter the data and make sure there is no overlap between pre-training and evaluation data. 
After filtering out unavailable ones, we have 9,473 training clip-text pairs from 1222 videos and 3,305 test clip-text pairs from 430 videos.


\noindent \textbf{COIN} \cite{tang2019coin} are leveraged to evaluate \textit{action segmentation}.
It has 11,827 videos (476 hours) and each video is labeled with 3.91 step segments on average and 46,354 segments in total. There are 778 step labels, plus one background (\underline{O}utside) label. Since one video can last for several minutes that are much longer than the maximum length of the video segment of VLM. We apply a sliding window with step size 16 and window size 32.
During inference, we average the logits for overlapped frames from multiple windows.

\noindent \textbf{CrossTask} \cite{zhukov2019cross} is a dataset for action localization that contains 83 different tasks and 4.7k videos. Each task has a set of steps with text descriptions annotated on temporal frames of the video.
We use the testing data split via the official code\footnote{ \url{https://github.com/DmZhukov/CrossTask}}, which contains annotated 1690 videos. The rest of the 540 annotated videos are used for weakly supervised training.

\begin{table*}[t]
    \centering
    \scalebox{0.61}{
        \begin{tabular}{l c c c r c c}
            \hline
            Model & Paradigm & \#params. & \#loss & \#unimodal/cross en/decoder & Joint Retrieval & Generation\\
            \hline
            MMT\cite{gabeur2020multi} & task-specific alignment & 127.3M & 1 & 2/0/0 & yes & no\\
            ActBERT\cite{zhu2020actbert} & weakly supervised/MTL & n/a (3 typed attentions) & 4 & 0/1(modal-typed attn.)/0 & no(pair) & extra decoder \\
            VideoAsMT\cite{korbar2020video} & weakly supervised/MTL & 286M(base)/801M(large) & 1 & 1/1/1 & no (gen.) & yes\\
            HERO\cite{li-etal-2020-hero} & SSL(w/ sup. video feat.)/MTL & 159M & 5 & 1(query)/2/0 & no(pair) & extra decoder\\
            UniVL\cite{luo2020univilm} & SSL/MTL & 260M & 5 & 2/1/1 & yes & yes\\
            \hline
            VLM & SSL/Task-agnostic & \textbf{110M} & \textbf{1} & 0/\textbf{1}/0(shared w/ encoder) & yes & yes\\
            \hline
        \end{tabular}
        \vspace{-1mm}
    }
    \caption{Comparison of pre-trained models on learning paradigms (SSL means self-supervised learning; MTL means multi-task learning), number of parameters (\# params.), number of losses (\#loss), number of unimodal/cross-modal encoders/decoders, and whether to support retrieval in joint space(joint retrieval) and text generation. Types and numbers are estimated based on released code or papers: exceptions are in parenthesis (e.g. pair means pairwise matching using \texttt{[CLS]}). VLM is extremely simple with fewer parameters and limitations.}
    \label{tbl:num_param}
\end{table*}


\subsection{Hyper-parameters}
We extract video tokens from video frames using the S3D encoder pre-trained from \cite{miech2020end}. 
The fps is 30 and we extract one (1) video token per second with the dimension of 512. We apply an MLP to transform such 512 dimensions to the hidden size (768) of .

Following \cite{luo2020univilm}, we adopt  (uncased) as our base model and tuned directly from BERT's weights, so all hyper-parameters are the same as the original BERT. The maximum length of BERT is set as 96, where 32 tokens are for videos and the rest tokens are for text and special tokens.
Remind that texts are 2.4 tokens per second and video tokens are 1 token per second.
We form a text clip with a random length in-between 8 and 64 text tokens and collect the corresponding video clip to form a training example.
We randomly sample 32 video/text clip pairs from each video and use 8 videos to form a batch of size 256.
Each training example has 50\% chance for MMM (25\% for whole video masking and 25\% for whole text masking) and 50\% chance on MFM-MLM (with 15\% probability of video and text token masking).

We pre-train VLM on 8 NVIDIA Tesla V100 GPUs (each with 32 GB memory) for 15 epochs using fp16 for one (1) day.
Following \cite{liu2019roberta}, we choose Adam \cite{kingma2014adam} optimizer with initial learning rate of 5e-5 (with betas as (0.9, 0.98)), 1000 steps of warm-up and a polynomial decay learning rate scheduler. Gradients are clipped with 2.0. All fine-tuning tasks use the same hyper-parameters as pre-training except the number of warm-up steps is 122.

\subsection{Model Comparison}
We first investigate the design choices of VLM compared to other transformer-based multimodal pre-training baselines.
As shown in Table~\ref{tbl:num_param}, we collect training paradigms, model sizes, etc. of these models (estimated based on their source codes or papers).
VLM is significantly smaller than other models since it is just a  (uncased), while it is still fully self-supervised, task-agnostic (e.g. no training on retrieval or auto-regressive style tasks) and supports joint retrieval and text generation.



\subsection{Quantitative Analysis}
We investigate the performance of VLM on fine-tuning tasks with very basic setups (e.g. no augmented features, large LMs, optimized losses for particular tasks).
Note that it could be hard for fair comparisons between task-agnostic and task-specific approaches. We list other baselines by type and our goal is a simple baseline for task-agnostic pre-training as better initialization of strongly performed fine-tuning models.

\noindent \textbf{Text-video Retrieval} We use MSR-VTT and Youcook2 to evaluate the performance on text-video retrieval.
The results are shown in Table~\ref{tbl:vtt} and \ref{tbl:youcook}, respectively.
VLM achieves good performance on these two datasets, indicating that the MMM and isolated self-attention mask can be used together for joint retrieval. Ablation study shows that using an isolated self-attention mask alone does not yield good performance, indicating MMM is very important to learn features for alignment. Note that our pre-training is task-agnostic but still outperforms baselines with retrieval style pre-training.

\begin{table}[t]
\centering
\scalebox{0.63}{
    \begin{tabular}{l c c c c}
    \hline
    Methods & R@1 & R@5 & R@10 & Median R \\
    \hline
    Random & 0.1 & 0.5 & 1.0 & 500 \\
\hline
    Task-specific Alignment Pre-training\\
    MMT \cite{gabeur2020multi} & 25.8 & 57.2 & 69.3 & 4\\
    \hline
    Pairwise Matching\\
    ActBERT\cite{zhu2020actbert} & 8.6 & 23.4 & 33.1 & 36 \\
    VideoAsMT\cite{korbar2020video} & 14.7 & - & 52.8 & - \\
    \hline
    Multi-task Pre-training\\
    HERO \cite{li-etal-2020-hero} &  16.80 & 43.40 & 57.70 & - \\
    UniVL (FT-Joint) \cite{luo2020univilm} & 20.6 & 49.1 & 62.9 & 6 \\
    \hline
VLM & 28.10 & 55.50 & 67.40 & 4\\
\hline
    \end{tabular}
}
\caption{Results of text-video retrieval on MSR-VTT dataset.}
\label{tbl:vtt}
\end{table}


\begin{table}[t]
\centering
\scalebox{0.63}{
    \begin{tabular}{l c c c c}
    \hline
    Methods & R@1 & R@5 & R@10 & Median R \\
    \hline
    Random & 0.03 & 0.15 & 0.3  & 1675 \\
\hline
    Task-specific Alignment Pre-training\\
    Coot\cite{ging2020coot} & 16.7 & 40.2 & 52.3 & 9\\
    \hline
    Pairwise Matching\\
    ActBERT\cite{zhu2020actbert} & 9.6 & 26.7 & 38.0  & 19 \\
    VideoAsMT\cite{korbar2020video} & 11.6 & - & 43.9 &  - \\
    \hline
    Multi-task Pre-training\\
    UniVL (FT-Joint)\cite{luo2020univilm} & 22.2 & 52.2 & 66.2 & 5 \\
\hline
    VLM & 27.05 & 56.88 & 69.38 & 4\\
\hline
    \end{tabular}
}
\caption{Results of text-based video retrieval on Youcook2 dataset.}
\label{tbl:youcook}
\end{table}

\noindent \textbf{Action Segmentation} We report the results of action segmentation on COIN dataset in Table~\ref{tbl:coin}. VLM outperforms other baselines indicating its good token-level video representations. Note that this task only tests the hidden states of the video indicating the unimodal encoding capability of VLM is not compromised.

\begin{table}[t]
\centering
\scalebox{0.8}{
    \begin{tabular}{l c}
    \hline
Method & Frame Accuracy \\
\hline
NN-Viterbi \cite{richard2018neuralnetwork} & 21.17\\
VGG \cite{simonyan2014very} & 25.79\\
TCFPN-ISBA \cite{ding2018weakly} & 34.30\\
CBT \cite{sun2019contrastive} & 53.90\\
MIL-NCE \cite{miech2020end} & 61.00\\
ActBERT \cite{zhu2020actbert} & 56.95\\
\hline
VLM & 68.39 \\
    \hline
    \end{tabular}
}
\caption{Action segmentation on COIN dataset.}
\label{tbl:coin}
\end{table}

\noindent \textbf{Action Step Localization} We setup two (2) evaluations for the CrossTask dataset.
First, we evaluate the zero-shot transfer of VLM. Note that existing studies evaluate Crosstask with retrieval/alignment style pre-training, where the aligned hidden states are directly used for action step localization.
Our task-agnostic pre-training derives an even harder problem: applying hidden states learned from proxy tasks on video frame/text alignment for action step localization without explicitly training on alignment.
We simply use the hidden states from the last layer of VLM for video/text representation and directly compute the similarities between video frames and text descriptions.
Surprisingly, the performance is better than some baselines and closer to one supervised method. This indicates masked token loss together with MMM can learn certain video-text alignments in joint space.
Second, we use just 540 videos for weakly supervised training and we get a much better result.

\begin{table}
\centering
\scalebox{0.82}{
    \begin{tabular}{l c}
\hline
Methods & Average Recall\\
\hline
Joint Alignment\\
Alayrac \cite{alayrac2016unsupervised} & 13.3\\
Zhukov \cite{zhukov2019cross} & 22.4\\
Supervised \cite{zhukov2019cross} & 31.6\\
HowTo100M \cite{miech2019howto100m} & 33.6\\
MIL-NCE \cite{miech2020end} & 40.5\\
UniVL \cite{luo2020univilm} & 42.0\\
\hline
Pairwise Matching\\
ActBERT \cite{zhu2020actbert} & 41.4\\
\hline
VLM (task-agnostic, zero-shot) & 28.5 \\
VLM (supervised on 540 videos) & 46.5 \\
    \hline
    \end{tabular}
}
\caption{Action step localization results on CrossTask.}
\end{table}

\noindent \textbf{Video Question Answering} We use MSR-VTT QA 
to evaluate multiple-choice question answering.
Recall that this task essentially tests video-text similarity.
The performance of VLM is better than ActBERT, which leverages pairwise matching for each video/answer pair.


\begin{table}
\centering
\scalebox{0.8}{
    \begin{tabular}{l c}
    \hline
Method & Accuracy \\
\hline
Joint Retrieval\\
JSFusion\cite{yu2018joint} & 83.4 \\
\hline
Pairwise Matching\\
ActBERT\cite{zhu2020actbert} & 85.7\\
\hline
VLM & 91.64 \\
    \hline
    \end{tabular}
}
\caption{Video question answering (multiple-choices) evaluated on MSR-VTT.}
\end{table}


\noindent \textbf{Video Captioning} We lastly evaluate VLM on video captioning with autoregressive attention mask with other baselines that have an explicit text decoder.
As shown in Table~\ref{tbl:youcookcap}, our ``compact'' decoder using BERT's LM heads is surprisingly good at video captioning compared to other fine-tuning baselines with external decoders (e.g. Coot).
This indicates that it is possible to remove an explicit decoder and sharing weights between video and text tokens.


\begin{table}[t]
\centering
\scalebox{0.62}{
    \begin{tabular}{l c c c c c}
    \hline
    Methods & B-3 & B-4 & M & R-L & CIDEr \\
    \hline
Extra Decoder\\
VideoBERT \cite{sun2019videobert} & 6.80 & 4.04 & 11.01 & 27.50 & 0.49 \\
CBT \cite{sun2019contrastive} & - & 5.12 & 12.97 & 30.44 & 0.64 \\
ActBERT \cite{zhu2020actbert} & 8.66 & 5.41 & 13.30 & 30.56 & 0.65 \\
Coot\cite{ging2020coot} & 17.62 & 11.09 & 19.34 & 37.63 & -\\
\hline
w/ Pre-trained Decoder \\
VideoAsMT \cite{korbar2020video} & - & 5.3 & 13.4 & - & - \\
UniVL \cite{luo2020univilm} & 16.46 & 11.17 & 17.57 & 40.09 & 1.27 \\
\hline
VLM & 17.78 & 12.27 & 18.22 & 41.51 & 1.3869\\
\hline
    \end{tabular}
}
\caption{Video captioning results on Youcook2 dataset.}
\label{tbl:youcookcap}
\end{table}















\subsubsection{Ablation Study}
We use Youcook2 as the base task for the ablation study on text-retrieval and video captioning. 
We are interested in the following study:
(1) percentage of examples for MMM (w/ MMM x\%);
(2) minimum length of text tokens, where the length of video will be determined by the start/end timestamps of text tokens;
(3) performance of  (Equation \ref{eq:lvlm}).
The results are shown in Table~\ref{tbl:abl_retri} and Table~\ref{tbl:abl_cap}.

\noindent \textbf{Effects of MMM} Without MMM (w/ MMM 0\%, or MFM-MLM 100\%), the performance significantly dropped. This indicates that a naive adoption of traditional MFM-MLM masking may not learn joint video/text representations well, as indicated by both retrieval and captioning task. We suspect a masked token is more likely predicted from tokens of the same modality.
We further try MMM with different probabilities (30\% or 70\%) and 50\% is the best.

\noindent \textbf{Minimum Length of Texts} The length of a clip can be important for retrieval tasks \cite{miech2020end}. We ran VLM on longer (at least 16 text tokens) video/text pairs. The performance is slightly dropped, indicating pre-training on longer clips may not cover fine-tuning tasks with short clips.

\noindent \textbf{Effects of Masked Token Loss} We notice that using multi-task style loss  may reduce the performance. This indicates learning a masked token from both video/text tokens can help.


\begin{table}[!htbp]
\centering
\scalebox{0.78}{
    \begin{tabular}{l c c c c}
    \hline
    VLM & R@1 & R@5 & R@10 & Median R \\
    \hline
    w/ MMM 50\% & 27.05 & 56.88 & 69.38 & 4.0\\
    w/ MMM 0\% & 15.12 & 39.47 & 52.81 & 9.0\\
    w/ MMM 30\% & 25.30 & 54.80 & 68.96 & 4.0\\
    w/ MMM 70\% & 25.17 & 54.98 & 69.11 & 4.0\\
    w/ min. 16 text tokens & 25.84 & 54.43 & 68.29 & 5.0\\
    w/  & 26.93 & 55.92 & 69.86 & 4.0\\
    \hline
    \end{tabular}
}



\caption{Ablation study of VLM for text-based video retrieval on Youcook2.}
\label{tbl:abl_retri}
\end{table}



\begin{table}[!htbp]
\centering
\scalebox{0.72}{
    \begin{tabular}{l c c c c c}
    \hline
    VLM & B-3 & B-4 & M & R-L & CIDEr \\
    \hline
    w/ MMM 50\% & 17.78 & 12.27 & 18.22 & 41.51 & 1.3869\\
    w/ MMM 0\% & 15.47 & 10.54 & 16.49 & 38.83 & 1.2163\\
    w/ MMM 30\% & 16.57 & 11.30 & 17.55 & 40.76 & 1.3215\\
    w/ MMM 70\% & 16.94 & 11.68 & 17.67 & 41.24 & 1.3739\\
    w/ min. 16 text tokens & 17.25 & 12.00 & 17.67 & 40.62 & 1.3076 \\
    w/  & 16.66 & 11.53 & 17.34 & 40.36 & 1.3224\\
    \hline
    \end{tabular}
}



\caption{Ablation study of VLM for video captioning on Youcook2 dataset.}
\label{tbl:abl_cap}
\end{table}





\subsection{Qualitative Analysis}
\subsubsection{Error Analysis} 
\noindent \textbf{Text-video retrieval}.
We use MSR-VTT as the dataset for error analysis on text-video retrieval, as shown in Table~\ref{tbl:err_vtt} of Appendix.
We pair the query text with the text of the top-1 ranked video to show 100 errors in ranking since video tokens are harder to present.
We observe the following types of errors in video understanding: (1) objects sometimes are hard to recognize such as dog or cat; (2) attributes of objects may be hard to match the text, e.g. gender, ages, etc. (3) subtle differences of actions;  (4) specific videos for a general query or vice versa, e.g. people vs basketball player. We believe the last type may not be errors but hard for existing annotations or evaluations to separate.

\noindent \textbf{Video Captioning}.
We further examine the generated text from video captioning.
Note that our video captioning has no support from ASR or transcript so the video is the only source to generate text content and errors of video understanding can easily be reflected in the text.
From Table~\ref{tbl:error_youcookcap} of Appendix, we notice that one major type of error is from objects of similar shapes and colors, e.g. onion rings vs shrimp.

\subsubsection{Visualization}.
We observe that video tokens take the majority of space while text tokens are rather clustered together. This is probably because videos from the physical world are more diverse and sparse than text from a fixed vocabulary.

We plot the self-attention of VLM layers within and in-between each modality, as in Figure~\ref{fig:attn0_1} of Appendix.
We observe the following patterns from all 144 attention heads:
\begin{itemize}
    \item Unlike LMs, there are no recurrent (shifted) position-wise patterns for video tokens;
    \item Self-attentions in the 1st layer are more diverse than later layers. This suggests that existing video encoders might be too deep for transformers;
    \item Some attention heads show patterns of cross-modal mapping in-between video and text (e.g. sub-figure (a));
    \item Word-level cross-modal co-reference: video tokens with \textit{pouring soy sauce} refers to the text token of ``soy'' (e.g. sub-figure (b));
\end{itemize}

\section{Conclusions}
We presented a task-agnostic pre-training with new masking schemes that enable the training of a single masked language model that can accept either video or text input, or both.
We showed that this simple VLM model can be effectively tuned for a broad range of downstream tasks, such as text-video retrieval and video captioning via different types of attention masks.
Experimental results show that the proposed methods maintain competitive performance while requiring a significantly smaller number of parameters than competing methods.

\section*{Acknowledgments}
We thank Huaishao Luo (author of UniVL\cite{luo2020univilm}), Mandela Patrick (author of Support-Set\cite{patrick2021supportset}) and Luowei Zhou (author of Youcook\cite{zhou2017towards}) for supports of baseline setup.

\bibliographystyle{acl_natbib}
\bibliography{acl2021}

\clearpage
\appendix

\begin{table*}[!t]
\centering
\scalebox{0.57}{
    \begin{tabular}{l|l}
    \hline
Query & Text of Top-1 video \\
\hline
\multicolumn{2}{l}{Objects (26\%)}\\
\hline
\textbf{cartoon} show for kids & \textbf{pokemon video game} play\\
little pet shop \textbf{cat} getting a bath and washed with little brush & several \textbf{dogs} playing dead\\
\hline
\multicolumn{2}{l}{Attributes of Objects (6\%)}\\
\hline
a little \textbf{boy} singing in front of judges and crowd & a \textbf{woman} singing on the voice\\
a woman is mixing \textbf{food} in a mixing bowl & a man is stirring \textbf{something} in a pot\\
\hline
\multicolumn{2}{l}{Action (6\%)}\\
\hline
a person is \textbf{connecting} \textbf{something} to system & a man \textbf{looks at} the \textbf{battery} of a computer\\
a boy \textbf{plays} grand theft auto 5 & a narrator \textbf{explains where to find} a rare vehicle in grand theft auto\\
a man is \textbf{giving a review} on a vehicle & a person is \textbf{discussing} a car\\
a \textbf{naked child} runs through a field & the \textbf{girl} shows the \textbf{boys} her medal in this cartoon\\
a man is singing and standing in the road & a man in sunglasses and a blue shirt beat boxes\\
\hline
\multicolumn{2}{l}{Specific vs General (62\%)}\\
\hline
some cartoon characters are moving around an area & a cartoon girl and animal jumping on body of male guy girl image still shown displaying on screen\\
\textbf{baseball player} hits ball & \textbf{people} are playing baseball\\
the man in the video is showing a brief viewing of how the movie is starting & scrolling the the menu of movieclips with different movie trailers\\
a \textbf{student} explains to his \textbf{teacher} about the sheep of another student & there is a \textbf{guy} talking to his \textbf{father}\\
a video about different sports & a woman talks about horse racing\\
\hline
    \end{tabular}
}
\caption{Error analysis for text-video retrieval of MSR-VTT on 100 errors: we group errors in four (4) categories: objects, attributes of objects, actions, and specific vs general. Specific videos for general queries (or vice versa) sometimes may not be errors but hard to evaluate. 
}
\label{tbl:err_vtt}
\end{table*}

\begin{table*}
\centering
\scalebox{0.85}{
    \begin{tabular}{l|l}
    \hline
Hypothesis & Reference \\
\hline
add the lamb to the \textcolor{red}{\textbf{pan}} & add the lamb to the \textbf{pot} \\
add the cilantro \st{cilantro} and lime juice to \underline{the pot} & cut the cilantro and lime\\
add the \textcolor{red}{\textbf{onions}} to a pot of water & add \textbf{flour} to the pot and stir\\
dip the \textcolor{red}{\textbf{onion rings}} into the batter & dip the \textbf{shrimp} in the batter\\
add water to the bowl and mix & pour water into the \textbf{flour mixture} and mix\\
remove the \textcolor{red}{\textbf{mussels}} from the pot & once the \textbf{shrimps} are defrosted drain the water\\
add the sauce to the \textcolor{red}{\textbf{pan}} and stir & add the sauce to the \textbf{wok} and stir\\
add lemon juice to the pan and stir & add \textbf{rice vinegar} and lemon juice to the pan and stir\\
add the beef to the pan and stir & add the diced beef meat to it and roast it\\
\hline
    \end{tabular}
}
\caption{Error analysis for video captioning on Youcook2: VLM tends to make mistakes in recognizing objects of similar shapes and colors to generate the wrong text.}
\label{tbl:error_youcookcap}
\end{table*}















\begin{figure*}
\begin{subfigure}[t]{3.1in}
    \centering
    \includegraphics[width=2.1in]{img/attn/attn0_1.jpg}
    \caption{Layer 1, Head 1}
    \vspace{-3mm}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{3.1in}
    \centering
    \includegraphics[width=2.1in]{img/attn/attn0_5.jpg}
    \caption{Layer 1, Head 5}
    \vspace{-3mm}
\end{subfigure}
\caption{Self-attention for video HfIeQ9pzL5U from 4:03 to 4:28: darker color indicates higher weights; v0-v24 are video tokens of 25 seconds.}
\label{fig:attn0_1}
\end{figure*}


\end{document}
