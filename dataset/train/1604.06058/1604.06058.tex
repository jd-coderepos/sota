\documentclass[envcountsame,envcountsect,undated,nolinenumbers]{lnthi}

\bibliographystyle{plain}

\usepackage{epsf}

\def\FcopyforlaterA#1#2#3#4{\long\def#1{\begin{#2}\label{#3}#4\end{#2}}}

\def\FcopyforlaterB#1#2#3#4{\renewcommand{\thetheorem}{\ref{#3}}\begin{#2}#4\end{#2}\renewcommand{\thetheorem}{\arabic{section}.\arabic{theorem}}}

\def\Fcopyforlater#1#2#3#4{
  \FcopyforlaterA{#1}{#2}{#3}{#4}
  \FcopyforlaterB{#1}{#2}{#3}{#4}
}


\def\Fpasteinsection#1{#1
}

\def\Tcopyforappendix#1#2#3#4#5#6{\long\def#3{\begin{#4}#6\end{#4}}\newcounter{#1}\setcounter{#1}{\value{section}}\newcounter{#2}\setcounter{#2}{\value{theorem}}\begin{#4}\label{#5}#6\end{#4}} 

\def\Tpasteinappendix#1#2#3{\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\thetheorem}{\arabic{section}.\arabic{theorem}}\newcounter{#1old}\setcounter{#1old}{\value{section}}\newcounter{#2old}\setcounter{#2old}{\value{theorem}}\setcounter{section}{\value{#1}}\setcounter{theorem}{\value{#2}}#3
\setcounter{section}{\value{#1old}}\setcounter{theorem}{\value{#2old}}\renewcommand{\thetheorem}{\Alph{section}.\arabic{theorem}}\renewcommand{\thesection}{\Alph{section}}
}


\title{Succinct Choice Dictionaries}

\author{Torben Hagerup and Frank Kammer}
\institute{Institut f\"ur Informatik, Universit\"at Augsburg, 86135
Augsburg, Germany
\email{\{hagerup,kammer\}@informatik.uni-augsburg.de}}

\usepackage{mathdots}

\usepackage{amsmath}

\setpagewiselinenumbers

\def\Tvn#1{\hbox{\textit{#1\/}}}
\def\Tfloor#1{\lfloor #1\rfloor}
\def\Tceil#1{\lceil #1\rceil}
\def\TbbbN{\mathbb{N}}
\def\Ttwodots{\mathinner{\ldotp\ldotp}}\def\Tsup#1{^{\mbox{\scriptsize #1}}}\gdef\Tsub#1{_{\mbox{\scriptsize #1}}}\def\cchoice{\overline{\Tvn{choice}}}
\def\citerate{\overline{\Tvn{iterate}}}
\def\jj{{\bar j}}
\def\Trho{R}
\def\rbar{\overline{r}}
\newbox\Twbox\setbox\Twbox=\hbox{\small(32)}
\def\Tmyw{\copy\Twbox}
\newbox\Tsbox\setbox\Tsbox=\hbox{}
\def\Tmys{\copy\Tsbox}
\def\Ttvn#1{t_{\mathit{#1}}}

\makeatletter
\def\eqalign#1{\null\,\vcenter{\openup\jot\m@th\ialign{\strut\hfil&\hfil\crcr#1\crcr}}\,}\makeatother

\begin{document}

\maketitle{}

\begin{abstract}The \emph{choice dictionary} is introduced as a
data structure that can be initialized
with a parameter 
and subsequently maintains an initially empty subset 
of  under insertion, deletion, membership queries
and an operation \Tvn{choice} that returns
an arbitrary element of .
The choice dictionary appears to be fundamental
in space-efficient computing.
We show that there is a choice dictionary
that can be initialized
with  and an additional parameter 
and subsequently
occupies  bits of memory
and executes each of the four operations \Tvn{insert},
\Tvn{delete}, \Tvn{contains} (i.e., a membership query)
and \Tvn{choice} in  time
on a word RAM with a word length of  bits.
In particular, with ,
we can support \Tvn{insert}, \Tvn{delete},
\Tvn{contains} and \Tvn{choice} in constant time using
 bits for arbitrary fixed~.
We extend our results to maintaining several
pairwise disjoint subsets of .

A static representation of a subset  of 
that consists of  bits 
is called \emph{systematic} if
 for 
and is said to have \emph{redundancy} .
We extend the former definition to dynamic data structures
and prove that the minimum redundancy of a systematic choice
dictionary with parameter  that executes
every operation in  time on a
-bit word RAM is ,
provided that .
Allowing
a redundancy of

for arbitrary fixed ,
we can support additional -time operations
\Tvn{p-rank} and \Tvn{p-select} that
realize a bijection from  to 
and its inverse.
The bijection may be chosen arbitrarily by the
data structure, but must remain fixed
as long as  is not changed.
In particular, an element of  can be
drawn uniformly at random in constant time
with a redundancy of .

We study additional space-efficient
data structures for subsets  of , including
one that supports only insertion and an
operation \Tvn{extract-choice} that returns
and deletes an arbitrary element of~.
All our main data structures can be
initialized in constant time and
support efficient iteration over the set~, and we can allow
changes to~ while an iteration over~ is in progress.
We use these abilities crucially in designing the most
space-efficient algorithms known for solving
a number of graph and other combinatorial problems
in linear time.
In particular, given an undirected graph  with
 vertices and  edges, we can output a spanning
forest of  in  time with at most
 bits of
working memory for arbitrary fixed ,
and if  is connected,
we can output a shortest-path spanning
tree of  rooted at a designated vertex
in  time
with  bits
of working memory for arbitrary fixed .\\

{\bf Keywords.} Data structures, space efficiency, bounded universes,
constant-time initialization, lower bounds, bit probes,
graph algorithms, random generation.
\end{abstract}

\pagestyle{plain}
\thispagestyle{plain}

\section{Introduction}
\label{sec:intro}The \emph{redundancy} of
a data structure  capable of representing an arbitrary
object in a nonempty set 
is the (worst-case) number of bits
of memory occupied by  beyond
the so-called in\-for\-ma\-tion-theoretic lower bound,
i.e., beyond
---in this paper
``'' always denotes the
binary logarithm function .
If  depends on one or more size
parameters,  is said to be \emph{succinct}
if its redundancy is .
Whereas constant factors have traditionally been ignored
for both time and space bounds in the theoretical
analysis of algorithms and data structures,
in recent years there has been increased
interest in succinct data
structures \cite{BarAHM12,BroM99,Cla96,FerMM09,GeaRRR06,GolGGRR07,Jac89,MunRRR12,Pag01,Pat08,RamRS07}.
Most of the succinct data structures
developed to date are \emph{static}, i.e., they support
certain queries about the object  stored,
but no updates of~, and, in fact, even the time
to construct the data structure from  has
frequently been ignored.
Of the dynamic succinct data structures developed
to date, a major part is
concerned with navigation in trees
\cite{Arr08,DavR11,FarM11,MunRS01,NavS14,RamR03},
and there are only few other contributions in
areas such as text processing
\cite{HeM10,LeeP09,MakN08,RusNO11}
and the maintenance of arrays, dictionaries and prefix sums
\cite{BroCDMS99,GupHSV07b,RamRR01}.
We add to the rather small collection of
known dynamic succinct data structures
that represent structures other than trees.

Data structures that represent an
(arbitrary) subset  of a universe
of the form  and support various
sets of operations
have been studied in computer science for decades
\cite{Ajt88,AjtFK84,AndT07,BeaF02,BosDDHM13,DieKMHRT94,BoaKZ77,FiaN93,FreKS84,FreS89,FreW93,GupHSV07a,Joh82,PatT14,Ram96,Ruz08,TarY79,Tho00b,Wil84,Yao81}.
Our work continues this tradition and suggests
new sets of operations to be supported.
In the setting under consideration,
the condition of succinctness translates into space requirements
of  bits.
A powerful dynamic data type that we now call a
\emph{ragged dictionary}
was introduced in~\cite{ElmHK15}
and shown there to have a number of applications in
space-efficient graph algorithms.
In many situations the full power of the ragged
dictionary is not needed,
and the currently known construction of
ragged dictionaries is so involved that its
description is still in preparation.
In this paper we trim the ragged dictionary,
retaining only a set of operations that is simpler to
implement, allows a succinct realization,
and suffices in
most---but not all---applications of ragged dictionaries.
The resulting data type is characterized formally below.

\begin{definition}
\label{def:choice}A \emph{choice dictionary} is a data type that
can be initialized with an arbitrary integer
,
subsequently maintains
an initially empty subset  of 
and supports the following operations,
whose preconditions are stated in
parentheses:

\begin{tabbing}
\quad\=\hskip 2.4cm\=\hskip 2cm\=\kill
\>\>():\>
Replaces  by .\\
\>\>():\>
Replaces  by .\\
\>\>():\>
Returns 1 if , 0 otherwise.\\
\>:\>
\>Returns an (arbitrary) element of 
if , 0 otherwise.
\end{tabbing}
\end{definition}

As is common and convenient, we use the term
``choice dictionary'' also to denote data structures
that implement the choice-dictionary data type.
Following the initialization of a choice dictionary
 with an integer~,
we call (the constant)  the \emph{universe size}
of  and (the variable)  its
\emph{client set}.
The operation \Tvn{choice}, named so by
analogy with the axiom of choice, is central
and lends its name to the entire data type
as its most characteristic feature.
The operation is unusual in that a client set  is
not mapped deterministically to a unique
prescribed return value;
instead, many return values may be legal for a given~.
The operation, while not exactly new,
appears not to have been considered often in the past.
In fact, it is not uncommon for algorithms to comprise
steps that could be implemented via calls of \Tvn{choice}.
For many classic data structures, however, finding
an (arbitrary) element is no easier than finding
a certain specific element
(such as the minimum or the element most
recently inserted), for which reason such steps are often
overspecified by being formulated as queries for
specific elements.
In our setting, the flexibility inherent in
\Tvn{choice} is crucial to obtaining the most
efficient choice dictionaries and algorithms.

For integers  and  with ,
the \emph{bit-vector representation over
} of a subset
 of  is the sequence
 of  bits with
, for ,
or its obvious layout in  successive bits in memory.
If only the operations \Tvn{insert}, \Tvn{delete}
and \Tvn{contains} are to be supported,
a subset  of  can be
stored simply as its bit-vector representation over .
On the other hand, if
the operation \Tvn{delete} is omitted, the three
remaining operations are trivial to support in constant time
with close to  bits.
It is the combination of \Tvn{insert} and
\Tvn{delete} with \Tvn{choice} that makes
the choice dictionary useful and its design interesting.

It is often possible to equip a choice dictionary
with facilities beyond the four core operations.
One of the most useful extensions is an operation
\Tvn{iterate}, which allows a user to process
the elements of~ one by one.
In fact, we consider \Tvn{iterate} as a virtual
operation that is a shorthand for three concrete operations:
, which prepares
for a new iteration over~,
, which
yields the next element  of~
(we say that  is \emph{enumerated};
if all elements have already been enumerated,
0 is returned), and
, which returns 1 if
one or more elements of~ remain to be enumerated
and 0 otherwise.
When stating that a choice dictionary allows
iteration in a certain time , what we mean
is that each of the three operations
,
 and

runs in time bounded by~.
Our iterations are \emph{robust}, by
which we mean the following:
First and foremost, changes to the
client set  through insertions
and deletions can be tolerated during an iteration.
Second, every element of  present in  during
the entire iteration is certain to be enumerated
by the iteration, while on the other hand no
element is enumerated more than once or at a time when it
does not belong to~---in particular, if an element
does not belong to  at any time during the
iteration, it is certain not to be enumerated.

Another useful extension is the ability to work
not only with the client set , but also with
its complement .
This involves an operation
, which returns an arbitrary element
of  (0 if ),
and possibly a virtual operation ,
whose three concrete suboperations enumerate
.
Viewing membership in  and in 
as two different \emph{colors}, we call a choice
dictionary extended in this way a -color choice dictionary,
whereas the original bare-bones choice dictionary
will be said to be \emph{colorless}.
We extend the concept of color
to  colors, for integer .
A -color choice dictionary maintains a
\emph{semipartition} 
of , i.e.,
a sequence of (possibly empty)
disjoint subsets of  whose union is~,
called its \emph{client vector}.
The operations \Tvn{insert}, \Tvn{delete}
and \Tvn{contains} are replaced by

\begin{description}
\item[\normalfont]
( and ):
Changes the color of  to , i.e.,
moves  to  (if it is not already there).
\item[\normalfont:]
Returns the color of~, i.e.,
the unique  with .
\end{description}

Moreover,
the operations \Tvn{choice} and \Tvn{iterate}
(with its three suboperations) take an additional
(first) argument 
that indicates the set  to which the
operations are to apply;
e.g.,  returns an arbitrary
element of  (0 if ).
In applications  is often a small constant.
To emphasize this view, we may write the argument 
as a subscript of the operation name
(e.g., 
instead of ).
Initially all elements of  belong to~.
In the special case , we may write, e.g.,
\Tvn{choice} and  or
 and , as convenient.
We have not attempted to optimize our results
for large values of~.
Formally, we allow , but a choice dictionary
with just one color is trivial and useless,
and in proofs we tacitly assume .
A view equivalent to that of a semipartition
 of 
is that a -color choice dictionary with
universe size~ must
maintain an array of  values drawn from
 under certain
obvious operations.

Of course, all operations of the
colorless choice
dictionary with universe size~ and many more can be supported in
 time by a balanced binary tree.
Our interest, however, lies with data structures that
are more efficient than binary trees in terms of
both time and space.
Our model of computation is a word RAM~\cite{AngV79,Hag98}
with a word length of  bits, where we assume that  is
large enough to allow all memory words in use to be addressed.
As part of ensuring this, in the context of a universe or
an input of size , we always assume that
.
The word RAM has constant-time operations
for addition, subtraction and multiplication
modulo , division with truncation
( for ),
left shift modulo 
(,
where ),
right shift
(),
and bitwise Boolean operations
(,  and 
(exclusive or)).
We also assume a constant-time operation to
load an integer that deviates from 
by at most a constant factor---this enables the
proof of Lemma~\ref{lem:word}(a).
We do not assume the availability of
constant-time exponentiation,
a feature that would simplify some of
our data structures.
When nothing else is clear from the context,
integers manipulated algorithmically are assumed
to be of  bits, so that they can be
operated on in constant time.
Integers for which this assumption is not
made may be qualified as ``multiword''.
Multiword integers are assumed to be represented
in the positional system with base
, i.e., in
a sequence of words,  bits per word.

Our most surprising
result, proved in Section~\ref{sec:nonsystematic},
yields a colorless choice dictionary
that can be initialized for universe size~
in constant time,
that executes \Tvn{insert}, \Tvn{delete},
\Tvn{contains} and \Tvn{choice} in constant time
and whose redundancy is 
for arbitrary fixed ,
significantly better
than the best bound of 
known for ragged dictionaries
used as choice dictionaries.
We generalize to several colors and to
an upper-bound tradeoff between time and space:

\Fcopyforlater{\unsystematic}{theorem}{thm:unsystematic-c}{For every fixed ,
there is a choice dictionary that, for arbitrary
, can be initialized
for universe size ,  colors and tradeoff
parameter  in constant time and
subsequently occupies

bits and supports
\Tvn{color}, \Tvn{setcolor}, \Tvn{choice} and
robust iteration in  time.
}

When  is a power of~2, and in particular
for , we achieve a better
space bound of

bits, albeit with a time bound for
\Tvn{setcolor} of  instead of 
(Theorem~\ref{thm:unsystematic-f}).
For , this yields a redundancy of essentially
 for execution times
of , the same as that achieved
by P\v atra\c scu for a different
problem~\cite[Theorem~4]{Pat08}.
Interestingly, we employ
P\v atra\c scu's technique,
as extended by Dodis,
P\u atra\c scu and Thorup~\cite{DodPT10},
in the proof of Theorem~\ref{thm:unsystematic-c},
but not in that of Theorem~\ref{thm:unsystematic-f}.
At a technical level, the problem of realizing
\Tvn{choice} can be viewed as that of finding an arbitrary
leaf with a given color in a tree with colored leaves,
but practically no space available for
navigational information at inner nodes.
Our solution forms groups of leaves and exploits
the fact that if a leaf group lacks some color
completely, it offers a certain potential for storing
foreign (namely navigational) information.
If below an inner node  there is no such ``deficient'' leaf
group, on the other hand, the search can proceed
blindly from ---there are no ``dangerous'' subtrees.

For ,
a static data structure that represents a subset  of
 is called \emph{systematic} if
its encoding of  has the bit-vector
representation of  over  as a prefix~\cite{GalM07}---in
other words,  is stored as its ``raw'' form,
possibly followed by other information.
The definition can be applied as it is to dynamic
data structures, but then precludes initialization
in  time and, more significantly,
prevents the representation from having
a size indication such as
an encoding of the integer~ as a prefix.
We therefore use the following alternative definition:
A dynamic data structure  that represents a
subset  of  is systematic
if, beginning in a bit position that depends
only on~, it contains a sequence 
of  bits
such that for each , 
holds at all times after 's first writing to~, if any.
In a word RAM, the bit  is part of a
word  in memory, and  first writes to 
when it first stores a value in .
Until that point in time, we assume that  and therefore
 may contain arbitrary values (``be uninitialized'').
It is sometimes considered desirable
for a data structure to be systematic~\cite{GalM07}.
Our proof of Theorem~\ref{thm:unsystematic-c} does
not yield a systematic data structure,
but in Section~\ref{sec:systematic}
we propose an alternative and
systematic choice dictionary:


\Fcopyforlater{\systematic2}{theorem}{thm:systematic-2}{There is a 2-color systematic
choice dictionary that, for arbitrary ,
can be initialized for universe size  and
tradeoff parameter  in constant time
and subsequently occupies
 bits
and executes \Tvn{insert}, \Tvn{delete},
\Tvn{contains}, \Tvn{choice},   and
robust iteration over the client set
and its complement in  time.
}

For  (a condition that excludes
only cases of scant interest),
the product of the redundancy
and the execution time per operation is 
for the choice dictionary of Theorem~\ref{thm:systematic-2}.
We prove in Subsection~\ref{subsec:lower} that this is
optimal in the sense that every systematic choice
dictionary with universe size~
must have a redundancy-time product of .
Our result, in fact, is considerably more precise:
In the bit-probe model~\cite{GalM07,Yao81}, 
if a systematic choice dictionary with universe size~
has redundancy  and inspects at most  bits during
each execution of an operation, then
, where
, and
we argue that this statement
does not hold if  is replaced by an
arbitrary constant larger than~1.
In a certain sense, therefore, the tradeoff between
redundancy and operation time has been determined to within
a factor of less than~2.
While there are linear or near-linear lower bounds
for the product of redundancy and query time
for certain static systematic data structures,
such as ones that support queries
for the sum, modulo 2, of the bits in prefixes
of a fixed bit string
(the prefix-sum problem)~\cite{GalM07},
we are not aware of nontrivial previous such bounds
for dynamic data structures.

Following the introduction of the ragged dictionary,
another systematic choice dictionary was developed
independently by Banerjee,
Chakraborty and Raman~\cite{BanCR16}.
Their construction is similar to that of the
special case of Theorem~\ref{thm:systematic-2}
obtained by taking  and .
The redundancy is indicated only as ,
however, and an inspection of the proof shows
the redundancy to be ,
not the optimal  of
Theorem~\ref{thm:systematic-2}.
Moreover, the data structure of~\cite{BanCR16}
supports neither robust iteration nor
, and it cannot be initialized
in constant time.
An early choice dictionary
(with the \Tvn{choice} operation called \Tvn{choose-one})
was described by Briggs and Torczon~\cite{BriT93}.
Their data structure requires  bits.

A first space-efficient algorithm
for (essentially) the problem of linear-time
computation of a
shortest-path tree with a given root in a connected
unweighted graph was indicated in~\cite[Theorem 5.1]{ElmHK15}.
For input graphs with  vertices and  edges,
this took the form of a simple -time
reduction
to the problem of executing  operations on a
4-color choice dictionary with universe size~.
Given that the interest in~\cite{ElmHK15} was not
with constant factors, a ragged dictionary was used
for the choice dictionary, and the bound on the
necessary amount of working memory
(i.e., memory in addition to read-only memory
that holds the input)
was indicated as  bits.
Restating the reduction and plugging in their own
choice dictionary, Banerjee et al.~\cite{BanCR16} derived a new
space bound for the problem, given as  bits.
Substituting our superior choice dictionaries of
either Theorem~\ref{thm:systematic-2} or
Theorem~\ref{thm:unsystematic-c}, we could
improve the lower-order term of this bound.
We instead obtain a more substantial
improvement (Theorem~\ref{thm:bfs}) by giving a new reduction of the
shortest-path problem to that of executing
 operations on a
choice dictionary that has only 3 colors
but must support robust iteration.
With Theorem~\ref{thm:unsystematic-c}, our space
bound becomes  bits
for arbitrary fixed .

Much previous work has gone into the development of
\emph{rank-select structures} (also known as
\emph{indexable dictionaries}) that support
operations \Tvn{rank} and \Tvn{select}
\cite{Jac89}.
Formulated in terms of a client set
,
the two operations
are defined as follows:

\begin{description}
\item[\normalfont]
():
Returns .
\item[\normalfont]
():
Returns the unique  with 
if , 0 otherwise.
\end{description}

P\v atra\c scu showed that for
arbitrary fixed , there is a
static rank-select structure that occupies
 bits and executes
both \Tvn{rank} and \Tvn{select} in constant time
\cite[Theorem~2]{Pat08} (his result, in fact, is more general).
For systematic static rank-select structures
with constant query time
the optimal redundancy is known to be

\cite{Gol07,RamRS07}.
For the corresponding dynamic data type,
i.e., one that supports \Tvn{insert} and
\Tvn{delete} in addition to \Tvn{rank}
and \Tvn{select}, a lower bound of
 on the execution
time of the slowest operation~\cite{FreS89}
precludes all hope of achieving a similar performance.
Returning to the setting of  colors,
we introduce ``poor man's substitutes'' for
\Tvn{rank} and \Tvn{select} called
\Tvn{p-rank} and \Tvn{p-select} and show
in Subsection~\ref{subsec:random} that,
for arbitrary fixed  and
, for arbitrary
 and allowing a redundancy of
,
we can support \Tvn{p-rank} and \Tvn{p-select}
in  time in addition
to the usual choice-dictionary operations
(Theorem~\ref{thm:p}).
When the client vector is ,
both operations are defined in terms of a
sequence , where
 is a bijection from  to
, for :

\begin{description}
\item[\normalfont]
():
Returns , where  is the
color of .
\item[\normalfont]
( and ):
Returns  if , and 0 otherwise.
\end{description}

If the bijections  are viewed
as numbering the elements within each of the sets
,  therefore
returns the number of  in its set,
and  (that we may write
as ) returns the element
in  numbered~ (0 if there is no such element).
The sequence 
may be chosen arbitrarily by the choice dictionary,
subject only to the condition that it must remain
unchanged between calls of 
(or of  and \Tvn{delete}).
The operations \Tvn{p-rank} and \Tvn{p-select} are
approximate inverses of each other in the sense that

for all  and

for all  and all
.
The operations \Tvn{rank} and \Tvn{select}
generalize approximately to  colors
as the special cases  and
 of  and 
obtained by requiring  to be the increasing
bijection from  to ,
for .
We obtain our result through a nonobvious combination
(illustrated in Fig.~\ref{fig:select}
on p.~\pageref{fig:select})
of (usual) choice dictionaries and other data structures.

The ``\Tvn{p-}'' in \Tvn{p-rank} and \Tvn{p-select} can be thought
of as an abbreviation for ``pseudo-'' or ``permuted''.
The operations \Tvn{p-rank} and \Tvn{p-select}
are closely related to
the classic ranking and
unranking operations within static but more complicated
classes of combinatorial objects~\cite{KreS99}.
Despite the arbitrariness inherent in
\Tvn{p-rank} and \Tvn{p-select}, the latter operation
has at least one important application, namely
to the generation of random elements:

\begin{description}
\item[\normalfont]
():
Returns an element drawn uniformly at random
from  if , and 0 otherwise.
\end{description}

The realization of \Tvn{uniform-choice} in
terms of  is obvious:
A call 
draws an integer  uniformly at random
from  and returns
.
The uncolored version of
\Tvn{uniform-choice} is called \Tvn{sample}
in \cite[Problem 1.3.35]{SedW11}.

We also study choice and choice-like dictionaries
that use fewer than  bits when the number
 of elements of nonzero color is considerably
smaller than~.
In particular, we
show that constant-time
\Tvn{insert}, \Tvn{delete}, \Tvn{contains} and \Tvn{choice}
can be achieved with
 bits, for arbitrary
fixed  (Theorem~\ref{thm:m-c}),
and in Subsection~\ref{subsec:mlog}
we describe
a data structure that uses
 bits
and supports
constant-time \Tvn{insert} and \Tvn{extract-choice},
where the latter operation
removes and returns an arbitrary element
of the client set~
(if  is empty, 0 is returned).
Our data structure is similar to the \emph{pool}
data structure of~\cite{HerS08}, where the operations
\Tvn{insert} and \Tvn{extract-choice} are called
\Tvn{put} and \Tvn{get}, respectively.
To represent 
using little space, our data structure stores 
in difference form, i.e., as a sequence of differences
between consecutive elements of~.
For this to make sense,  must be sorted, but this
renders constant-time insertion in  difficult.
We set up a system of sorted reservoirs and
unsorted buffers and merge buffers into reservoirs
before they become too large.
Employing this data structure as the work-horse,
we can compute a spanning forest of an undirected
graph
with  vertices and  edges
in 
time with at most  bits of
working memory, for arbitrary
fixed  (Theorem~\ref{thm:cc}).
An algorithm that, slightly modified, can solve
the same problem in linear time was described previously
\cite[Theorem~5.1]{ElmHK15}, but the number of bits was
specified only as , and even with our best
choice dictionary the algorithm of
\cite{ElmHK15} would use at least
 bits.

Our choice dictionaries have found uses elsewhere as
modest but crucial components of space-efficient
algorithms for Euler partition and
edge coloring of bipartite graphs~\cite{HagKL17}
and recognition of
outerplanar graphs~\cite{KamKL16}.
We currently explore their applications in
space-efficient solutions to a number of
vertex-coloring problems.

Although we do not assume the memory allocated
to hold a data structure to have been initialized in
any way---it may hold arbitrary values---all
our main data structures can be initialized in constant time.
Whereas this is standard and trivial to achieve for
data structures such as binary trees, we have to
develop new techniques to achieve the same
for our succinct data structures for universes
of the form .
It is a convenient property to have, and it
is essential to some of our algorithmic applications.
What makes initialization in constant time difficult is,
above all, that small instances cannot in general
be handled by means of table lookup.

\section{A Very Simple Choice Dictionary}
\label{sec:quick}Before embarking on a more
comprehensive
development, in this section
we indicate the shortest route to one of
our results that, though elementary,
suffices for many applications:
A basic choice dictonary that supports each
of the four core operations in constant time
and uses  bits to
maintain a subset of .
The description will demonstrate, in particular,
that our choice dictionary not only uses less
space, but is also simpler than the construction
of Banerjee et al.~\cite{BanCR16}.
Readers who want more details, a greater generality,
additional operations or a tighter space bound
are referred to Sections
\ref{sec:preliminaries}--\ref{sec:systematic}.

The result is
obtained by the combination of three ingredients,
each of which is very simple.
One is a choice dictionary that is wasteful in terms
of space, one is a choice
dictionary for very small universes, and the final component is 
the combination of many choice dictionaries
in the standard pattern of a trie.

Recall that a systematic choice dictionary with
universe size~ contains a bit-vector representation
 over  of the client set
and that  immediately supports \Tvn{insert},
\Tvn{delete} and \Tvn{contains}, so that the only
remaining problem is to support \Tvn{choice}.
Once the search for a~1 in  has been narrowed
down to a \emph{group} of  consecutive bits, for
a suitable constant , it
can be concluded, e.g., by table lookup
(aiming for constant-time initialization,
we do it differently).
Representing each group by the disjunction of its
constituent bits (altogether
 bits),
we are left with the task of locating
a 1 among the group bits, i.e., the universe size
has been reduced by a factor of .
Playing the same trick once more, we have
``supergroups'' of  bits each,
some of which are empty, and the task is to find
a nonempty supergroup.
To this end we spend  bits
on storing a permutation  that sorts the
supergroups by their \empty{status},
empty or nonempty, and direct \Tvn{choice} to
the supergroup at the ``nonempty'' end of the sorted list.
When a supergroup changes its status, the sorting
can be maintained by first interchanging the
supergroup in question (located with the aid of )
with a supergroup at the border between
empty and nonempty.

\section{Preliminaries}
\label{sec:preliminaries}

We view our data structures as
``coming to life'' during an initialization that
fixes certain parameters, typically a
universe size, , and possibly a number
of colors, , and/or a tradeoff parameter, ,
that expresses the relative weight to be given
to speed versus economy of space.
After initialization, we may consider these
parameters as constants.
It is natural, e.g., to speak of a choice
dictionary \emph{with} a particular universe size.

When we state that a data structure uses a certain
number of bits of memory, this is a statement about
the number of bits occupied by the data structure
when it is in a quiescent state, i.e., between the
execution of operations.
During the execution of an operation, the data structure
may temporarily need more working space---we speak
of \emph{transient} space requirements.
By definition, the -bit word RAM uses at least
 bits whenever it executes an instruction,
so that every operation of every data structure
has transient space requirements of at least
 bits.
All our operations
get by with  bits of transient space
that will not be mentioned explicitly.
Most of our data structures must store a
constant number of integers such as the parameters
with which the structures were initialized.
In consequence, most of our space bounds include
a term of  bits that will not be
discussed in every case.
When several data structures are initialized with
the same parameters and do not need to support
independent iterations, they can generally share the
same  bits.

Many operations of a choice dictionary can be faced
with ``unusual'' situations, such as the insertion of
an element that is already present, or \Tvn{choice}
called when the client set is empty.
We have chosen---fairly arbitrarily---to define the
operations so that they either do nothing or return
the special value~0 in such circumstances.
Since the unusual situations can easily be detected,
the operations could be redefined to instead issue an
error message or take some other suitable action.

The following is an attempted formalization
of the standard
``initialization on the fly''
technique of \cite[Exercise 2.12]{AhoHU74}.

\begin{lemma}
\label{lem:2.12}There is a data structure with the following
properties:
First, for every , it can be
initialized for universe size~ and
subsequently maintains
a function  from
 to ,
initially the zero function that maps every
element of  to~0,
under evaluation of 
and the following operation:

\begin{description}
\item[\normalfont]
():
If ,
changes the value of  on  from 0 to
an element of .
Otherwise does nothing.
\end{description}

\noindent
Second, for known , the data structure uses at most
 bits, can be initialized
in constant time, and evaluates  and
supports \Tvn{allocate} in constant time.
(If  is not known, it can be stored in the
data structure in another  bits).
\end{lemma}

\begin{proof}
The data structure stores an integer ,
initially~0, and two arrays
 and  such that
for all  with ,
 and .
To execute  for 
when , increment  and store  in  and
 in .
To evaluate  for , test whether
 and .
If this is the case, return ;
otherwise return~0.
\end{proof}

The application of
Lemma~\ref{lem:2.12}
highlighted in \cite[Exercise 2.12]{AhoHU74}
is to the constant-time initialization of all
entries of an array  to some
value .
More generally,
if  is the set of values storable in cells of ,
we can allow  to be an
arbitrary function from  to 
that can be evaluated in constant time using
a negligible amount of memory.
The access to  can take the form of two functions:
, where , returns ,
and , where  and ,
assigns the value  to .
If  is an instance of the data structure of
Lemma~\ref{lem:2.12} for universe size~
and  is the function that it maintains,
\Tvn{read} and \Tvn{write} can be realized as follows:

\begin{tabbing}
\quad\=\hskip 3cm\=\kill
\>:
\>\textbf{if}  \textbf{then return} ;
\textbf{else return} ;\\
\>:
\>\textbf{if}  \textbf{then} \=;\\
\>\>;
\end{tabbing}

Thus an array of  entries can be assumed initialized
at the price of an additional  bits.
By using such an array with single-bit entries
only to keep track of the
initialization of segments of  of  bits each and
representing  by the bit pattern 
for , using the vacated bit pattern
for  to represent the value that used
to be represented by  (thus initializing
a segment amounts to clearing an area of
 bits), we can reduce the number
of additional bits to ,
where  is the number of bits
occupied by~.
These considerations imply, in particular,
that an array can always
be assumed initialized at the price of a constant-factor
overhead in the space requirements.
Stronger results are known (see~\cite{FreK16}),
but the bound indicated
suffices for our purposes.


In addition to the operations considered in the
introduction, our discussion will refer to
a number of further operations that can be
added to a -color choice dictionary with
universe size  and client vector 
and are collected here for easy reference:

\begin{description}
\item[\normalfont\Tvn{universe-size}:]
Returns~.
\item[\normalfont]
():
Returns .
\item[\normalfont]
():
Returns 1 if , and 0 otherwise.
\item[\normalfont]
():
Interchanges  and  (does nothing
if ).
\item[\normalfont]
():
Returns all elements of 
(packaged, e.g., in an array or a list).
\item[\normalfont]
( and  is an integer):
With ,
returns  if , and 0 otherwise.
\item[\normalfont]
( and  is an integer):
With ,
returns  if , and 0 otherwise.
\end{description}

The first three operations can be added to an arbitrary
choice dictionary at a very modest price,
namely constant time per call of the new operations,
constant additional time per call of the
original operations, and
 additional bits, used to store
 while preserving a
constant initialization time with
Lemma~\ref{lem:2.12}.
Similarly, using Lemma~\ref{lem:2.12},
we can realize \Tvn{swap-colors} in constant time
by storing a permutation that translates
between ``internal'' and ``external'' colors
and needs an additional  bits.
So as not to clutter the picture, these operations
were not included in the repertoire of
Definition~\ref{def:choice}.
On the other hand, they can usually be assumed
to be available.
If the original choice dictionary supports
iteration in constant time,
 can carry out its job
in  time by executing a full
iteration over~.

Whenever convenient, we can assume that the
argument  of  and
 satisfies :
For , a value of  larger
than  is always associated with a return value of~0,
a value of  smaller than 0 is
equivalent to , and  is
equivalent to  unless ,
in which case the return value is~1.

Several reductions among different operations
are obvious.
E.g., \Tvn{choice} reduces to \Tvn{p-select}
in the sense that if \Tvn{p-select} is available,
 can be implemented simply as
.
We may express this succinctly
by writing

\begin{tabbing}
\quad\=\hskip 3cm\=\kill
\>:\>;
\end{tabbing}

\noindent
Similarly, \Tvn{choice} reduces to \Tvn{iterate},
except that a call of \Tvn{choice} executed in this
manner interferes with the ongoing iteration, if any:

\begin{tabbing}
\quad\=\hskip 3cm\=\kill
\>:\>;
 \textbf{return} ;
\end{tabbing}

\noindent
For colorless data structures,
\Tvn{choice} and \Tvn{extract-choice} are
mutually reducible if insertion and
deletion are available
and calls of 
\Tvn{choice} and \Tvn{extract-choice} can be allowed
to interfere with iteration,
\Tvn{p-rank} and \Tvn{p-select}:

\begin{tabbing}
\quad\=\hskip 3cm\=\kill
\>:\>;
 ; \textbf{return} ;\\
\>:\>;
 ; \textbf{return} ;
\end{tabbing}

\noindent
The following reductions were mentioned earlier.
Again, a call of \Tvn{elements} interferes with
any ongoing iteration.
A call  is assumed to return an integer
drawn uniformly at random from .

\begin{tabbing}
\quad\=\hskip 3cm\=\kill
\>:\>
 ;\\
\>:\>
 ; ;\\
\>\>\textbf{while} 
 \textbf{do} ;\\
\>\>\textbf{return} ;
\end{tabbing}

\noindent
Finally, if  additional bits
per iteration are available to hold a
private variable~, several simultaneous
iterations reduce to any one of
\Tvn{successor}, \Tvn{predecessor}
and \Tvn{p-select}, the latter only if
robustness of the iteration is not required.
We give the details in the case of \Tvn{successor}.

\begin{tabbing}
\quad\=\hskip 3cm\=\kill
\>:\>;\\
\>:\>
 ; \textbf{return} ;\\
\>:\>
 \textbf{if}  \textbf{then return} ;
 \textbf{else return} ;
\end{tabbing}

For , let
.
If the -bit binary representation of
 is divided into  fields of
 bits each, each field contains the value~1.
The possibly multiword integer  can be
computed from  and  in 
time \cite[Theorem 2.5]{Hag15}.
Given a sequence  of
 integers and an integer ,
let  and .
The following lemma is proved with standard
word-RAM techniques,
more background on which can be found,
e.g., in~\cite{Hag98}.

\begin{lemma}
\label{lem:word}Let  and  be given integers
with  and suppose that a
sequence  with
 for 
is given
in the form of the -bit binary representation
of the integer
.
Then the following holds:
\begin{itemize}
\item[(a)]
Let  and .
Then, in  time,
we can test whether  and, if not,
compute  and .
\item[(b)]
Let  and .
Then, in  time,
we can test whether  and, if not,
compute  and .
\item[(c)]
If an additional integer 
is given, then  time
suffices to compute the integer
, where
 if  and  otherwise
for .
\item[(d)]
If  and
an additional integer 
is given, then 
can be computed
in  time.
\end{itemize}
\end{lemma}

\begin{proof}
(a)
 if and only if ,
which is trivial to test.
Assume that .
Then ,
so the problem of finding 
reduces to that of computing .
Fredman and Willard~\cite[pp.\ 431--432]{FreW93}
showed how to do this in
constant time for 
(a number of quantities needed by their algorithm,
such as , can be computed in constant time
with the methods of~\cite{Hag15}).
Testing  bits
at a time for being zero, it is easy to extend
their algorithm to the general case.
Computing
 reduces to computing 
if one replaces  by

(cf.~\cite[Eq.\ 7.1.3-(40)]{Knu10}).

(b) If the binary representation of~
is viewed as consisting of  fields of  bits each,
the task is to locate the leftmost or rightmost
zero field in~.
We reduce this problem to that solved
in part~(a) by computing an -bit
integer , each of whose fields
is nonzero if and only if the corresponding field
in  is zero:

\begin{tabbing}
\quad\=\kill
\>;\\
\>;\\
\>;\\
\>;
\end{tabbing}

\noindent
Within each field,  has a 1 in the most significant
bit position, called the position of the
\emph{test bit}, and
 has only the test bits of~.
If ,  equals , while otherwise
all test bits in  are 0.
In either case, a field is nonzero in  if
and only if it is nonzero in~.
It is now easy to see that 
has the required property.

(c) Reusing the notions of fields and test bits
of the proof of part~(b),
we first compute an integer~ such that the th
test bit in , counted from the right,
is 1 if and only if ,
for .
Disregarding the values of the test bits in 
and , this can be done by replicating the
value of  to all fields through a multiplication
by , setting all test bits in the
resulting integer  to~1, clearing them in ,
and subtracting the latter from the former
to obtain an integer~.
Subsequently the original test-bit values of 
and  are incorporated in the test to obtain 
through bitwise manipulation of ,  and .
In detail, the value of a particular test bit
in  should be 1 exactly if at least one of the
corresponding bits in  and  is~1 and
either both these bits are~1 or the corresponding
bit in  is~0.
Obtaining  from  is just a matter of
``masking away'' unwanted bits and shifting
right by  bits.

\begin{tabbing}
\quad\=\kill
\>;\\
\>;\\
\>;\\
\>;\\
\>;
\end{tabbing}

(d) The task reduces to summing the
bits in the integer  of part~(c),
which can be carried out in
 time by computing
.

For all parts of the lemma,
intermediate results should be produced and
consumed in streams,  bits at a time,
in order to keep the transient space at  bits.
\end{proof}

We assume the memory available to a data structure
to be a single sequence of -bit words.
Occasionally, however, it will be convenient to
assume the availability of  independent
memories, where  is a constant.
It is a simple matter to simulate  virtual
memories in the single actual memory.
For , let  be the number
of bits used in the th virtual memory
and take .
During times when , we store a unary encoding
of  followed by the actual
contents of the virtual memories in  bits.
When , the actual memory words are
instead distributed among the  virtual
memories in a round-robin fashion.
For , the contents of the th
virtual memory are therefore stored in the
actual memory words numbered 
so that the total number of bits used is
.
Both representations support reading and writing
of virtual memory words in constant time---in the
case of the first representation,
carrying out the necessary unary-to-binary conversion
with an algorithm of Lemma~\ref{lem:word}(a)---and
we can also switch between the two representations
in constant time.
The number of bits used is always .
When employing this technique, we will say that we use
\emph{memory interleaving}.

\section{Tries of Choice Dictionaries}
\label{sec:trie}

We shall often have occasion to combine several
choice dictionaries in a trie structure to
obtain a choice dictionary
for a larger universe.
This section explains the simple principles
involved without formalizing them completely.

Let  and suppose that we have a data
structure that realizes an ordered tree 
with the leaf set  in which
each inner
node  has an associated choice dictionary~
whose universe size equals the degree
(number of children) of~
and all leaves have the same depth and that
also maintains a \emph{current node} in~.
Let  be the root of~.
Suppose that the data structure supports
the following operations:

\begin{description}
\item[\normalfont:]
Sets the current node to be .
\item[\normalfont]
(the current node is not ):
Replaces the current node by its parent in~.
\item[\normalfont]
(the current node  is an inner node in 
and  is a positive integer bounded by the
degree of~):
Replaces the current node by its th child
(in the order from left to right).
\item[\normalfont:]
Returns the height in  of the current node.
\item[\normalfont:]
Returns the degree in  of the current node.
\item[\normalfont:]
Returns one more than the number of nodes in  of the same
height as the current node and strictly to its left.
\item[\normalfont]
(the current node  is an inner node in  and
 is a leaf descendant of ):
Returns the integer  such that the th child
of  is an ancestor of~.
\item[\normalfont:]
Returns the memory address of the choice
dictionary associated with the current node.
\end{description}

Then, after initializing ,
we can execute the operations 
of a colorless choice dictionary
with universe size~ and client set 
as described below.
For each inner node  in~,
the client set of  will contain an integer~
if and only if at least one leaf descendant of
's th child belongs to~.
In the interest of clarity, we indicate the
current node as a (first) argument of
\Tvn{height}, \Tvn{leftindex} and \Tvn{viachild}.

\begin{description}
\item[\normalfont\Tvn{choice}:]
Return 0 if .
Otherwise,
starting at  and as long as the current node
 is not a leaf, step from  to its th child,
where  is obtained with a call of .
When a leaf  is reached, return .
\item[\normalfont\Tvn{contains}:]
Starting at  and as long as the height in
 of the current node 
is at least~2, let 
and, if ,
step from  to its th child;
otherwise return~0.
If and when a node  of height 1 is reached,
return .
\item[\normalfont\Tvn{insert}:]
Starting at  and as long as
the height in  of
the current node  is at least~2,
let  and let
 be the th child of~.
If ,
initialize  (possibly not for the first time)
for universe size , where  is the degree of~,
and execute .
Subsequently step from  to~.
When a node  of height 1 is reached,
execute .
\item[\normalfont\Tvn{delete}:]
Starting at  and as long as
the height in  of the current node  is at least~2,
let .
If ,
step from  to its th child;
otherwise abandon the deletion ().
If and when a node  of height 1 is reached,
execute .
Then, as long as the current node  is not 
and ,
step from  to its parent~
and execute .
\end{description}

A tree data structure that supports the operations
\Tvn{movetoroot}, etc., in constant time is simple
to design if  is sufficiently regular.
For given , let  be
a finite or
infinite
\emph{degree sequence}
of positive integers whose product
is at least 
and, for 
take .
Let  be the smallest positive
integer with .
Then we can let  be an ordered tree on the leaf
set  in which all leaves have
depth~ and every node of height~, except
possibly the rightmost one, has degree exactly ,
for .
Suppose that we represent the current node 
through the triple , where
 and .
Then we can navigate in  through the following
simple observations:
The parent of  is
,
its th child is
,
and .
The evaluation of the operations
 and  is trivial,
and the root of  is (represented by) .
As for accessing the choice dictionary
of the current node, i.e., evaluating \Tvn{data},
suppose that  are given nonnegative
integers and that each choice dictionary of a
node of height  in  can be accommodated in
a block of memory of  bits, for .
For , let
 be
the total number of bits needed for the
blocks of nodes in  of height at most~.
Then, when the current node is  and ,
 can return 
plus the starting address of a global segment of
 bits reserved for all blocks of nodes in~.
If we also maintain , i.e., if we extend the
triple  by  as a fourth component,
\Tvn{data} can be executed in constant time as well.

If the choice dictionaries of all nodes in 
support \Tvn{iterate}, the overall choice
dictionary can also support \Tvn{iterate}
with the following procedure, which is explained
below:

\begin{description}
\item[\normalfont\Tvn{iterate}.\Tvn{init}:]
Execute 
and initialize an integer  to~0.
\item[\normalfont\Tvn{iterate}.\Tvn{more}:]
If , return .
Otherwise,
starting at  and as long as the current node
 is not a leaf and no value was returned,
return 1 if .
Otherwise step to the th child of~,
where .
If and when a leaf is reached, return~0.
\item[\normalfont\Tvn{iterate}.\Tvn{next}:]
If , return~0.
Otherwise proceed as follows:

If , start at  and, as long as the current node
 is not of height~1,
step to the th child  of ,
where ,
and execute .

If , instead start at  and, as long as the current node 
is not of height~1, step to the th child of ,
where .
Then, as long as ,
where  is the current node, step to
the parent of~.
Subsequently, as long as the current node 
is not of height~1,
step to the th child  of ,
where ,
and execute .

Whether or not ,
when a node  of height~1 is reached,
let  be its th child,
where ,
set  and return~.
\end{description}

\noindent
If we say that the choice dictionary of
a node  in  is \emph{activated}
through a call of ,
becomes \emph{exhausted} when
 first evaluates to~0,
and is \emph{active} between the two events,
the iteration procedure above maintains a single
root-to-leaf path of active choice dictionaries,
which it remembers in the integer~, with
 denoting an initial situation in which
such an \emph{active path} has not yet been
established.
The global call 
finds a first active path (if ) or
(if ) exhausts the choice dictionaries of 
the current active path in a bottom-up fashion
until reaching a node  with
,
then steps to the ``next'' child  of  and
changes the last part of the current path to
be the path from  to its ``first'' leaf descendant.

If the choice dictionaries of some nodes in 
support \Tvn{successor} (or \Tvn{predecessor})
instead of \Tvn{iterate}, the overall choice dictionary
can still support iteration through the reduction
of \Tvn{iterate} to
\Tvn{successor} (or \Tvn{predecessor})
described in Section~\ref{sec:preliminaries}.
This needs additional space for a set of
``state variables'' that record the active path,
but it is easy to see that
the single variable  can
represent these compactly,
so that the overall space cost of an iteration
is  bits.

If the nodes of height~1 in  have
-color choice dictionaries, for some ,
the overall choice dictionary can also support
 colors and therefore maintain a client vector
.
In this case we equip every node  in 
of height  with 
choice dictionaries, each with universe size
equal to the degree of~ and associated
with a different color in .
Conceptually, the choice dictionaries
associated with each color 
form an \emph{upper tree}  that realizes a 
choice dictionary , called the
choice dictionary of , with
universe size 
and with client set
 and
.
For  the choice dictionaries
in  and  are all colorless.
Because  initially, the choice dictionaries
in  and  must instead allow
two colors and use the elements of color~0
as their ``client set''.
For ,
the th leaves of all of 
are associated with the same th
\emph{lower tree}, the tree induced
by the th node of height~1 in  and the children of that node.
An additional colorless dictionary 
with universe size  is used to keep
track of which choice dictionaries of upper and lower trees
have been initialized.
The realization of the ``colored'' choice-dictionary
operations in terms of ``colorless'' operations on
upper trees and ``colored'' operations on lower
trees is easy.
For instance,
to execute ,
call  to find a lower tree 
in which the color  is ``represented'' and
call  in the choice dictionary
of (the root of)  to determine an element of .
To execute , consult the appropriate
lower tree .
In all cases, before operating on the
dictionary  of an upper or
lower tree, use  to initialize 
if this has not been done before.
The remaining details are left to the reader.
When putting together a choice dictionary as described
in this section, we will say that we apply the
\emph{trie-combination} method.

\section{Systematic and Related Choice Dictionaries}
\label{sec:systematic}

\subsection{Upper Bounds}

One is frequently faced with the problem of
maintaining a permutation  of 
initialized to the identity permutation of that
set, say, under inspection of function values
and updates of  of some kind.
Allowing an initialization time of ,
the problem is trivial.
Assume that we want the initialization time to be constant.
Proceeding as described after Lemma~\ref{lem:2.12},
we can maintain  using around  bits
for the values of  itself and  bits 
for its ``initialization on the fly'' component.
If the inverse permutation  is also
maintained in the same manner, the space
requirements grow to approximately
 bits.
In the following lemma we demonstrate how to maintain
both  and  using only about
a third of this space.
Our data structure shows some
similarity to an algorithm of Brassard and Kannan
for computing random permutations
``on the fly''~\cite{BraK88}.

The data structure of
Lemma~\ref{lem:permutation}
must be employed with a little care because the
user acquires full ``control'' over  only
gradually in the course of
 calls of an operation \Tvn{consolidate}.
More precisely, when  calls of \Tvn{consolidate}
have been executed, the value of  after
an update, which is supposed to ``rotate'' 
the function values within a
given subset of ,
is in fact known only on the  largest elements
of .
One way of coping with the associated uncertainty
is illustrated in the
proof of Theorem~\ref{thm:nlogn}.

The space savings by a factor of~3 discussed
above plays no role in our development after
Theorem~\ref{thm:nlogn}, but we consider
Lemma~\ref{lem:permutation} to be
of independent interest.

\begin{lemma}
\label{lem:permutation}There is a data structure with the following properties:
First, for every , it can be
initialized for universe size  and
subsequently maintains a pair 
composed of a permutation  of ,
initially the identity permutation
 of , and an integer , initially ,
under evaluation of  and 
and the following operations:

\begin{description}
\item[{\normalfont \Tvn{consolidate}:}]
Replaces  by .
\item[{\normalfont }]
( and  are distinct
elements of ):
Replaces  by a permutation of  that agrees
on  with the permutation
 of  with  for
, ,
and  for all
.
\end{description}

\noindent
Second, for known , the data structure uses at most
 bits,
can be initialized in constant time, executes queries
and calls of  in constant time and
executes -argument calls of 
in  time, for all .
\end{lemma}

\begin{proof}
The permutation  is represented through two
arrays  and ,
each of whose entries can hold an arbitrary element of~.
For , say that  is \emph{proper}
in  if , , and
.
Correspondingly,  is proper in 
if , , and
.
If some  is not proper in  or ,
we say that  is \emph{improper} in that array.
Observe that if  is proper in~,
then  is proper in .
When  is improper in , say, 
may contain an arbitrary value
(``be uninitialized'').
The following invariant will hold at all times
between operations:
For all ,  is proper in 
if and only if  is proper in ;
for ,  is proper in both  and .
When saying simply that  is proper,
we will mean that  is proper in both  and .
The arrays  and  represent a
permutation  of  in the following manner:
For , if  is proper,
then ; if not, .
To see that this really defines  as a
permutation of , let  is proper
and observe that  is a function from  to  that
maps  to  and is injective both on 
(because  for each )
and on .
It is easy to see that
 and  can be evaluated in
constant time on arbitrary arguments in~.
Informally,  is proper in 
and  if  is a ``plausible''
value for  (i.e., )
and that value is confirmed by 
(i.e., ).
However, only values of  and 
with  are considered trustworthy,
and if both  and  are ,
 is improper and  is ignored.
Initially, the invariant is
satisfied, and the permutation 
represented through  and  is
the identity permutation .

To execute \Tvn{consolidate} when ,
store  in both  and 
if  is improper.
Then, whether or not  is proper, decrement .
It can be seen that neither step
invalidates the invariant or changes~.

The implementation of
 is illustrated in
Fig~\ref{fig:rotation}.
To execute 
in the situation of
Fig.~\ref{fig:rotation}(a),
let  and begin by
setting  for each improper 
(Fig.~\ref{fig:rotation}(b)).
Then change  in a way that
reflects the permutation  in the
definition of \Tvn{rotate}:
Save  in a temporary variable,
then, for ,
execute , and
next store the original value of  in .
Subsequently change  accordingly
by setting  for all .
At this point  for all ,
but the invariant may be violated
(Fig.~\ref{fig:rotation}(c)).

\begin{figure}
\begin{center}
\epsffile{rotation.eps}
\end{center}
\caption{The execution of , step
by step, starting from an example permutation~.
The example is chosen to have .
Each of parts (a)--(d) shows  on the left
and  on the right.
The sets  and 
are separated by a dotted line.
(a): The initial situation.
For each ,  and  are connected by a fully
drawn line if  is proper (then ) and by a dashed line if
 is improper (then  may be arbitrary).
(b): After the execution of  for each improper .
Each  is connected to .
(c): After the actual rotation.
Now  for all ,
where  is as in Lemma~\ref{lem:permutation}.
The elements of  and 
are indicated by arrows.
(d): After the restoration of the invariant.
The final permutation is shown
with conventions as in part~(a).}
\label{fig:rotation}
\end{figure}

Let us say that the invariant is violated
at some  if  and  is
improper in exactly one of  and  or  and  is
improper in at least one of  and .
Let , 
 and .
Obviously .
It can be seen that the invariant is not
violated at any element outside of .
Moreover, for ,  is improper in 
exactly if , whereas  is improper in 
exactly if .
Therefore the invariant is violated exactly at each 
in the symmetric difference of  and~.
Observe that  and finally
restore the invariant by changing the values of 
on  to make 
map  injectively to 
and then setting  for all .
This simultaneously makes the elements
of  proper in  and makes the elements
of  improper in 
(Fig.~\ref{fig:rotation}(d)).

The data structure uses slightly
more space than claimed because  can take
arbitrary values in  and so needs
 bits for its storage.
To lower this to  bits, execute
\Tvn{consolidate} one first time already as part
of the initialization, so that  never has
the value~.
\end{proof}

Recall from Section~\ref{sec:quick} that
our main result about systematic choice dictionaries
is
obtained by the combination of three simple ingredients:
A choice dictionary that is wasteful in terms
of space (Theorem~\ref{thm:nlogn}), a choice
dictionary for very small universes
(Lemma~\ref{lem:atomic-c}), and the
trie-combination method of
Section~\ref{sec:trie}.

\begin{theorem}
\label{thm:nlogn}There is a choice dictionary that,
for arbitrary ,
can be initialized for universe size~
and  colors in constant time
and subsequently
occupies at most

bits and supports \Tvn{color}, \Tvn{p-rank}, \Tvn{p-select}
(and hence \Tvn{choice} and \Tvn{uniform-choice})
and robust iteration in constant time 
and \Tvn{setcolor} in  time.
A more precise time bound for \Tvn{setcolor} is
that the execution of a call ,
for all  and ,
takes  time, where  is the color of 
immediately before the call.
\end{theorem}

\begin{proof}
Denote the client vector by .
The choice dictionary maintains a semipartition

of , whose sets
will be called \emph{segments}.
The intended meaning of the segments is that
for ,  and 
are the sets of those elements of  that are (still)
to be enumerated and are not to be enumerated, respectively,
in the current iteration over , if any;
thus at all times .
For brevity, let us say that the elements in 
are of \emph{hue} , for , and
denote the hue of each  by .
The segments are realized via  integers
 that store ,
respectively, and a pair ,
where  is a permutation of~ and
,
together with
the convention that
 sorts the elements of  by hue, i.e.,
.
We also maintain the prefix sums
, for ,
and the hue of each element of 
explicitly in two arrays, so that 
can be determined in constant time for each .
The invariant  will hold at all times.

The pair  is maintained in an instance
 of the data structure of Lemma~\ref{lem:permutation}.
's \Tvn{rotate} operation can be used
to move elements from one segment to another.
E.g., to move an element  from  to
, where ,
execute ,
where  is the sequence obtained
from 
by eliminating duplicates, i.e., by removing every
element equal to an earlier element,
and subsequently decrement 
and each of  and increment .
This takes  time.
Note how the condition  prevents
unintended transfers of elements from one segment to another
by the \Tvn{rotate} operation.
The operations of the choice dictionary
are implemented as follows:

\begin{description}
\item[\normalfont :]
Return .
\item[\normalfont :]
If ,
then execute  and subsequently move 
from its current segment to 
and record .
\item[\normalfont :]
Return , where .
\item[\normalfont :]
Return  if
, and 0 otherwise.
\item[\normalfont :]
Merge  into , i.e.,
execute first 
and  and then .
\item[\normalfont :]
Return 1 if , i.e., if ,
and 0 otherwise.
\item[\normalfont :]
Return  if .
Otherwise execute  and subsequently
move the boundary between  and  backward
by one element and return the element that crosses the boundary.
In other words, decrement  and , increment
 and return .
\end{description}

The initialization
sets ,
 for ,
 and  for 
and initializes .
To achieve a constant
initialization time, use Lemma~\ref{lem:2.12}.
After the initialization ,
,
 and  is the identity permutation ,
so the client vector represented is
,
as required, and the invariant is satisfied.
The only operations that may decrease 
are \Tvn{setcolor} and ,
and the decrease is only by~1.
Both operations call
 before they carry out any
other change, so
the invariant  is always satisfied.
Only the operation 
calls , and therefore the elements
returned by calls of  and
\Tvn{p-select} are consistent
with bijections that do not change between
calls of \Tvn{setcolor}.
Storing elements that are moved to 
in  rather than in 
prevents the elements from being enumerated
more than once during an iteration over~.
Therefore the iterations over  are robust.

An accurate count of the size of the data
structures introduced above yields an upper bound of
 bits.
To this should be added a number of bits needed
to store the parameters  and .
On the other hand, we can omit every second
prefix sum , so the space bound stated
in the theorem is easily achievable.
\end{proof}

\begin{lemma}
\label{lem:atomic-c}There is a choice dictionary that, for arbitrary
, can be initialized for universe size~
and  colors in  time
and subsequently occupies 
bits and executes \Tvn{color} and \Tvn{setcolor}
in constant time and \Tvn{successor} and
\Tvn{predecessor} (and hence also \Tvn{choice})
in  time.
\end{lemma}

\begin{proof}
Store only the  color values, each in a
field of  bits.
The realization of  and
 is obvious---read and overwrite
the contents of the th field,
respectively.
To execute  for
 and ,
remove the  leftmost fields in a copy, replace
the value in
every remaining field by its bitwise
\textsc{xor} with , and use an algorithm
of Lemma~\ref{lem:word}(b).
The implementation of  is
analogous.
\end{proof}

\Fpasteinsection{\systematic2}

\begin{proof}
Take  and assume without loss of generality that .
Compute  so that
, but .
We compose the choice dictionaries of
Theorem \ref{thm:nlogn}
and Lemma~\ref{lem:atomic-c}, both
initialized for 2 colors,
with the trie-combination method of
Section~\ref{sec:trie} and with the degree
sequence ,
where ,
, and
 for .
Every inner node of height at most~3 in the resulting trie 
is equipped with an instance of the choice dictionary
of Lemma~\ref{lem:atomic-c},
while every node in 
of height at least~4
has an instance of
the choice dictionary of Theorem~\ref{thm:nlogn}.
Every operation
on the overall choice dictionary spends 
time on each of the three bottom levels of  above the leaves
and constant time on every other level.
Since the height of  is , this
sums to~.
The choice dictionaries of the nodes in 
of height  need a total of exactly  bits,
and the most natural layout ensures that the
overall dictionary is systematic.
The height-2 and height-3 choice dictionaries,
if present, need
 bits and  bits,
respectively.
The number of nodes in  of height 
is , so
the number of bits required for all instances
of the dictionary of Theorem~\ref{thm:nlogn}
is .
\end{proof}

If we allow 
the dictionary not to be systematic,
we can generalize to several colors and
obtain an additional space bound that depends
on the maximum size of the client set.
In order to support  simultaneous iterations,
one for each color, the theorem below requires
 additional bits.
In general, with enough additional space to keep
track of their states, a smaller or larger number
of simultaneous iterations can be supported,
here and in data structures described later.

\begin{theorem}
\label{thm:m-c}There is a choice dictionary
that, for arbitrary 
with ,
can be initialized
for universe size ,  colors
and tradeoff parameters  and 
in constant time and subsequently uses

bits of memory
and supports \Tvn{color}, \Tvn{setcolor},
\Tvn{choice} and,
given  additional bits, robust
iteration in  time.
Moreover, as long as the number of elements
of nonzero color
remains bounded by ,
the number of bits of memory used is
.
In particular, for every fixed , there is
a choice dictionary that executes all
operations in constant time and uses
 bits to store
semipartitions that never have more than~
elements of nonzero color.
\end{theorem}

\begin{proof}
If , the result follows
from Lemma~\ref{lem:atomic-c}.
Assume therefore that .
We use largely the same construction as in the previous proof
and with  and  chosen as there, but 
now for general values of  and with 
instead of .
There are two additional changes:

First, the choice
dictionaries of nodes of height~1
in the trie  are initialized for  rather
than~2 colors and, as detailed in
Section~\ref{sec:trie}, each choice dictionary
of a node of height 2 or more in 
is replaced by  independent 2-color
choice dictionaries, one for each color.
As also discussed in Section~\ref{sec:trie},
this change makes it necessary to keep track
explicitly of the initialization of
upper and lower trees.
Instead of using a single dictionary 
of universe size  as
suggested in Section~\ref{sec:trie},
we handle the initialization of the
 upper trees in a separate choice
dictionary with universe size~
(realized according to Theorem~\ref{thm:systematic-2}, say)
and equip each node  of height~2 with
a colorless instance of the choice dictionary 
of Lemma~\ref{lem:atomic-c}
that records the initialization
of the choice dictionaries at 's children.
The total number of bits needed for the
dictionaries that take the place of 
can be bounded by
.

Second, rather than reserving space
permanently for every choice dictionary,
we allocate space to the  choice dictionaries
of a node  of height  in 
only when one of them acquires its first element
( becomes \emph{nonempty}) and reclaim that
space if and when  returns to being empty.
When space for the choice dictionaries of a node  of
height~ is allocated, we also allocate space for the
choice dictionaries of all children of~
(and, recursively, for those of their children).

If , the height of  is bounded by~2,
and its choice dictionaries can be accommodated
in a total of 
 bits,
a bound easily seen to be covered
by those of the theorem (recall that ).
In the rest of the proof assume that ,
so that  is of height at least~3.
 
The total number of bits needed by the choice dictionaries
of the descendants of a node of height~3
is ,
and these choice dictionaries are accommodated in
a \emph{leaf chunk} of  bits.
An exception concerns the descendants of
the rightmost node of height~3,
whose choice dictionaries may need less space;
exactly the required number of bits
is set aside statically for these dictionaries.
In the interest of simplicity, let us ignore
this exception for most of the following
discussion and return to it briefly at the end of the proof.
The  choice dictionaries of a node  of height 
occupy  bits.
Because the neighbors of~ in  are no longer
stored in fixed places in memory, the representation
of  must be augmented by  explicit pointers
of  bits each that allow
navigation in~.
Altogether,  and its choice dictionaries can be
accommodated in an \emph{inner chunk} of
 bits.

The total number of nodes of height~3 in 
is , and
the total number  of nodes of height
 can be computed in  time.
Accordingly, the available memory is conceptually
partitioned into  \emph{leaf slots}
of  bits each
and  \emph{inner slots}
of  bits each.
When space for a chunk is needed, a free slot of
the right size is allocated to it, and returned
slots are kept in one of two \emph{free lists},
one for each chunk size, that can
easily be maintained in the free slots themselves.
When a free slot is requested, it is taken from
the relevant free list unless the latter is empty.
If the relevant free list is empty, the first
slot of the right size and
unused so far is put into service; two
simple variables suffice to keep track of the
borders between slots that were allocated at least
once and new slots.

A leaf slot is exactly as large as the choice dictionaries
that may be stored in the slot.
An inner slot is larger by the  bits
for pointers to other slots, but since the number
of inner slots is ,
the total number of
additional bits is .
Therefore the number of bits used by the entire
data structure never exceeds
.

As long as the number of elements with
nonzero colors remains bounded by~,
the data structure allocates at most the first
 inner slots and the first  leaf slots.
The total number of bits in these slots is
.
The slots cannot be packed tightly because they
are allocated from two different pools, but we can
still ensure that they come from a block of
memory of  bits by
laying out the slots in memory according to
the following pattern:
First a leaf slot, then  inner slots, then
again a leaf slot, and so on.
The space bound easily admits the few choice
dictionaries that were allocated statically above.
\end{proof}

\subsection{A Lower Bound for Systematic Choice Dictionaries}
\label{subsec:lower}

In this subsection we show that the systematic choice
dictionary of Theorem~\ref{thm:systematic-2} is optimal,
up to a constant factor, in the tradeoff that it offers
among redundancy, execution time and word length.

For all integers , let an
\emph{-language} be a
language  over

that does not contain two words of the
form  and  with
, 
and  and
for which each
 satisfies , 
and .
Here , e.g., denotes the number
of occurrences of the character  in~.
Let .

\begin{lemma}
\label{lem:counting}For all integers , the cardinality
of every -language 
is bounded by ,
where .
\end{lemma}

\noindent\textbf{Proof.}
For all integers , let
 be the maximum cardinality of
an -language.
The bound of the lemma can be shown by induction
on  using the recurrence


\begin{theorem}
\label{thm:lower}Let , let  and assume that
some systematic data structure 
can represent every subset of 
in a sequence  of  bits.
Assume further that for each ,
it is possible to distinguish
among the  subsets  of 
of size  with at most  bit probes to an
arbitrary sequence  that represents each set~
according to 's conventions.
Then 
and, if , .
\end{theorem}

\begin{proof}
The second assumption of the theorem
cannot hold for .
We can therefore assume without loss
of generality that  and that .
For an  to be chosen later,
we associate a word  over
 with each
.
Let  be an algorithm that can
distinguish among the sets in 
with at most  bit probes.
Without loss of generality, 
probes no bit more than once.
For each , we apply 
to a bit sequence  used by  to represent~
and chosen to be \emph{minimal} in the sense
that no sequence  of  bits
that also represents  satisfies ,
where  denotes the conjunction of
bitwise  in all bit positions.
Informally, the minimality of  implies
that every uninitialized bit in 
has the value~0.
Without loss of generality, assume that the first
 bits of  are the bits 
referred to in the definition of a systematic
data structure (informally, the bit-vector
representation of~).
For each ,  is obtained as follows:
Initialize  to be the empty word
and append a character to 
at each probe carried out by 
on input , choosing the character as ,
where  is the value of the bit
probed,  if the bit probed is
among the first  bits of , and 
 if the bit probed is
among the last  bits of .
At this point, since  uses at
most  probes, .
Finally increase  to exactly  
by appending  occurrences of  to .

For each
, with ,
 probes each bit at most once, and so

since  is minimal and , and

since there are only  bits in addition
to .
 is therefore an
-language.
For  with ,
we cannot have , so
Lemma~\ref{lem:counting} shows that

where .
If , choose , which turns the inequality  into
 or .
Adding , we obtain , which implies the inequality

of the theorem.
In the following assume that .

We will make sure to choose
 and therefore ,
so that .
Then, since , we may assume
without loss of generality that .
Now

If we choose , the requirement 
is certainly satisfied,  becomes

and the inequalities 
and  follow.
If  and we instead choose
,
the inequality  that was just
established shows that the requirement
 is again satisfied.
With this choice of ,
 and therefore
 and .
Thus ,
a relation that also holds if .
The theorem follows.
\end{proof}

\begin{corollary}
Let  and  and let  be a systematic
choice dictionary with universe size  that
never occupies more than  bits.
Let  and 
be upper bounds on the number of bits read from
memory during an execution of 's operations
\Tvn{delete} and \Tvn{choice}, respectively
(the two quantities may depend on~).
Then

and, if ,
.
\end{corollary}

\begin{proof}
The return value of \Tvn{choice}
cannot be independent of 's
client set , so we must have
.
We can therefore assume without loss
of generality that .

Given knowledge of
,
we can output  with  iterations of
a loop in which an element of  is first obtained
with a call of \Tvn{choice} and subsequently
output and removed from  with a call of \Tvn{delete}.
The procedure reads at most 
bits of 's representation of , i.e.,
Theorem~\ref{thm:lower} can be applied with
.
\end{proof}

With a very similar argument we can obtain a lower
bound on the amortized complexity of
\Tvn{insert}, \Tvn{delete} and \Tvn{choice}.

\begin{corollary}
Let  and  and let  be a systematic
choice dictionary with universe size  that
never occupies more than  bits.
Fix an arbitrary potential function for~
and assume that every representation of the
empty client set has the same potential.
Let , 
and  be upper bounds
on the worst-case amortized
number of bits read from memory during 's execution of
\Tvn{insert}, \Tvn{delete} and \Tvn{choice}, respectively
(the three quantities may depend on~).
Then

and, if ,
.
\end{corollary}

\begin{proof}
As above, assume without loss of generality that .
For every  and every
 with , we can take 
from its initial state with empty client set
via a state in which its client set is 
and back to a state with empty client set
using exactly  calls of each of
\Tvn{insert}, \Tvn{delete} and \Tvn{choice}.
By assumption, the final potential is the
same as the initial potential, so the total
amortized number of bits read is the same as the
total actual number of bits read.
Therefore Theorem~\ref{thm:lower} is applicable with
.
\end{proof}

With ,  and  as in Theorem~\ref{thm:lower},
the theorem states that ,
where .
We complement this result by showing that for every
sufficiently easily computable
function  with
 but , there is a 2-color
systematic choice dictionary  that, when
initialized for universe size ,
reads  bits of its internal
representation during the execution of each
operation and has a redundancy 
for which .
 is simple.
It is a three-level trie constructed as described
in Section~\ref{sec:trie}
with the degree sequence ,
where , 
and .
The two bottom levels of choice dictionaries are realized
according to Lemma~\ref{lem:atomic-c} with ,
whereas the choice dictionary of the root
is an instance of that of Theorem~\ref{thm:nlogn},
again with .
The redundancy of the overall choice dictionary is
,
and it is easy to see that the number of bits read
during the execution of an operation is bounded by
.
The product of the two, indeed, is
.

\section{Restricted and Extended Choice Dictionaries}
\label{sec:other}

\subsection{A Data Structure with \Tvn{insert} and
 \Tvn{extract-choice}}
\label{subsec:mlog}

The main result of this subsection (Theorem~\ref{thm:mlog})
is a choice-like dictionary with universe size~
that stores a client set  of size 
in fewer than  bits even when  is not much
smaller than~.
More precisely, the number of bits used is
.
Since  for , the space used by our
data structure is within a constant factor of the
information-theoretic lower bound for most
combinations of  and~.
What we show is that this tight space bound still
admits certain dynamic operations.
More precisely, we can support insertion and the
operation \Tvn{extract-choice} that returns and
deletes an (arbitrary) element of~,
but neither unrestricted deletion nor queries
about specific elements such as \Tvn{contains}.

There is an apparent conflict between fast insertion
and a very space-efficient representation in
fewer than  bits.
To represent the client set  using little space, we
can store it in difference form, i.e., as the sequence of
differences between successive elements of~
(in sorted order).
With this representation, however, insertion is
easily seen to be prohibitively expensive.
On the other hand, insertion is easy if we store the
elements of  in no particular order, but then we
need about  bits per element of ,
which is excessive.
Our solution is to store  permanently in
difference form, but to insert new elements into an
unsorted buffer.
The buffer is wasteful of space, and so has to be
sorted and merged into the rest of  before it
becomes too large.
Because every operation is supposed to work in constant time,
this entails a certain technical complexity.
As a warm-up before tackling this, we illustrate the
use of the difference form by developing a data
structure of possible independent interest, a
space-efficient stack that requires its elements
to occur in sorted order at all times.

\begin{definition}
A \emph{bounded-universe sorted stack} is a data structure that,
for every , can be
initialized for universe size~ and
subsequently maintains an initially empty
sequence  with
 while supporting
the following operations:

\begin{description}
\item[\normalfont]
():
Replaces  by 
if  and does nothing otherwise.
\item[\normalfont:]
Replaces  by 
and returns  if ; returns 0 and
does nothing else if .
\end{description}
\end{definition}

\begin{lemma}
\label{lem:stack}There is a bounded-universe
sorted stack that, for arbitrary ,
can be initialized for universe size  in constant time,
subsequently
supports \Tvn{sorted-push} and \Tvn{pop} in constant
time and, when it currently holds a sequence
 of  elements, uses at most

bits, where . 
\end{lemma}

\begin{proof}
We encode integers using a scheme quite similar
to Elias'  representation~\cite{Eli75}:
Every nonnegative integer  can be represented in binary
in  bits, where
 for all ,
but this presupposes knowledge of the length of
the representation of , i.e., of .
To add this information, we append to the
representation of  a sequence of

bits that encode , followed by another
 bits that encode  suitably in unary.
Altogether, this encodes  in a string of
at most  bits that,
read backwards, can be decoded in constant time without prior
knowledge of the length of the string.

With  and ,
we store a bit string  that encodes
, where
, for ,
is encoded as described above
and the encodings of 
are simply concatenated.
Before storing  itself, we store the
encoding of its length
, so that we can find the end of  in
constant time.
By the implicit assumption ,
 bits suffice for this purpose.
Accordingly, we always store  in a field of  bits
for some suitable constant .
In order not to waste the last part of the field,
however, we move to there a suffix of  of
the appropriate length.

It is easy to see that the operations
\Tvn{sorted-push} and \Tvn{pop} can
be supported in constant time.
 is bounded by
.
Because  and
 and  are concave
on the set of nonnegative real numbers,

by Jensen's inequality.
Since the number of bits used is
, the lemma follows.
\end{proof}

\begin{theorem}
\label{thm:mlog}There is a data structure  with the following
properties:
First, for every ,  can
be initialized for universe size~
in constant time and subsequently maintains
an initially empty multiset  with elements drawn from
 and executes the following
operations in constant time:

\begin{description}
\item[\normalfont]
(): Inserts (another copy of)  in .
\item[\normalfont:]
Removes an arbitrary (copy of an) element of 
and returns it.
\end{description}

\noindent
Second,  allows robust iteration over 
in constant time per element enumerated,
and when the current size of 
(counting each element with its multiplicity) is ,
the number of bits used by  is
.
\end{theorem}

\begin{proof}
A single bit indicates whether .
If this is not the case,  is realized as the disjoint union
of four multisets,
, ,  and .
 and  are called \emph{reservoirs}.
The elements of each reservoir are stored
in sorted order in a data structure that is
exactly as the bounded-universe sorted stack
of Lemma~\ref{lem:stack}, except that the
binary representation of each
difference between consecutive elements
is not only followed, but also
preceded by an encoding of its length.
Thus a reservoir can be read both in the
forward and in the backward direction,
and it respects the space bound even if
it contains all  elements of~.
 and  are called \emph{buffers}.
For , the elements of 
are stored in no
particular order in an array of  cells
of  bits each.
We also store  and , and the representations
of , , , ,
 and  are interleaved in
memory so that the space occupied by them is within
a constant factor of the size of a largest among them.

At all times, exactly one buffer is called \emph{active}.
New elements are always inserted in the active buffer.
Consider a particular point in time and
let  be the index of the active
buffer at that time.
As soon as  both contains at least
 elements and occupies at least as many
bits as , it stops
being active, and the other buffer, ,
which is empty at this time, takes over as the active buffer.
From this point on, in a background process carried out
piecemeal and interleaved with the execution of
calls of \Tvn{insert} and \Tvn{extract-choice},
 is sorted in linear time with
2-phase radix sort and merged with .
The resulting sequence is stored in ,
after which  and  are emptied.
While  is under construction, its size
is artificially taken to be , in the sense
that  is kept active at least until
 is finished.
The elements to be removed and returned by calls
of \Tvn{extract-choice} are taken from 
during the sorting of  and from
 during the merge of  and .
The background process should be fast enough
to keep  nonempty during the sorting of ,
to keep  nonempty once the
merge has started, and to complete before
 occupies as many bits as 
will when it is finished.

In order for the background process to require only
constant time per operation executed by~,
the merge of  with  to obtain
 must happen in  time, which
generally means sublinear time with
respect to the number of elements in the reservoir .
Using the compactness of the representation
of , we achieve this by carrying out the merge using table lookup.
Recall that  and  are stored in
difference form.
During the merge, we therefore
keep track of
the element most recently removed from  and the 
element most recently written to ,
so that future elements can be decoded or
encoded correctly.
As detailed above,
the elements of  are represented through
variable-length bit \emph{segments}, each
complete with its length encoding.
At any given time during the merge, initial parts of
 and  have been merged, while
the remaining elements are still to be processed.
To continue the merge, we use table lookup to
find the last element, , of , if any, whose segment
is fully contained in the next 
bits of the representation of .
If  exists and is no larger than the next
element  of 
( if  is exhausted), the elements of 
up to and including  can be moved from 
to the output sequence .
If  exists but , all remaining elements of 
no larger than  can be identified
with a second table lookup
(applied to the next  bits of
the representation of 
and the difference between the first elements
of  and , a total of at most
 bits)
and moved to .
In both cases, subsequently
the first remaining element of 
( is  is exhausted) can be
compared with , and the smaller of the two
can be moved to .
All of this takes constant time, and it consumes
 bits of the
representation of either  or .
Therefore the total time needed to sort
 and to merge it with  is .
As anticipated above, this shows that executing
a constant number of steps of the
background process for each call of
\Tvn{insert} and \Tvn{extract-choice}
suffices to let the process finish in time.
Thus  executes every operation in
constant time.

The tables needed for the merge occupy
 bits and can be constructed in
 time.
This happens
in another background process that is
advanced by a constant number of steps
(if it has not completed) whenever
the active buffer contains at least
 elements and that finishes before
the active buffer reaches  elements.
Whenever the number of elements in the
active buffer drops below ---in particular,
when an empty buffer becomes active---the tables
are discarded, so that the
space that they occupy is always within a
constant factor of that taken up by the active buffer.

In order to support robust iteration over~,
we add more structural information to the data structure.
Call an element of  \emph{live} if it was
present in  at the time of the most recent preceding
call of , if any,
and has not since been enumerated.
If an element of  is not live, it is \emph{dead}.
An initial part of each reservoir or buffer
that was not emptied since the most recent preceding call
of 
(informally, that existed at that time)
is marked off as its \emph{live area}.
If one or more live areas are nonempty just before
a call of , the next
element enumerated is the last element of some
live area.
If the last element of a live area is enumerated
or deleted in a call of \Tvn{extract-choice},
the live area shrinks by one element.
A call of  sets the live
area of each reservoir and of each buffer to be the
entire reservoir or buffer.
This is the only occasion on which a live area expands.

When a buffer is sorted and merged with a reservoir
to create a new reservoir, the live elements can
no longer be kept in a contiguous area.
On the other hand, constant-time iteration
is possible only if the next live element
to be enumerated can be found in constant time.
For this reason reservoirs and buffers
created since the most recent preceding call of
 have empty live areas,
and the live elements in each such
reservoir are kept in a linked list in the
opposite of the order in which they occur in the reservoir.
Each live element in the list is labeled by its distance,
measured in bits, to the next live element in the list.
It is easy to see that the labels do not add to
the total space requirements by more than a
constant factor (small labels could even
be represented in unary).
Any list labels present in the live area of
a reservoir are ignored---all elements in
the live area are alive.

Before a buffer is sorted in preparation for being
merged with a reservoir, each buffer element is
given a bit of satellite data that indicates
whether the element is alive or dead.
The bit is carried along with the element during
the sorting and used in the merge to equip the
resulting reservoir with the linked list of its
live elements described above.
The table that drives the merge,
instead of merely indicating a number of bits
that can be copied from the old to the new
reservoir, must be changed to supply a
``piece of reservoir'' complete with list labels.
The at most one label that points outside of
the current piece, namely to an element in
the part of the new reservoir that has already
been constructed
(the \emph{nonlocal pointer}),
must be filled in separately
and outside of the table lookup.
Correspondingly, it is convenient to split
the table lookup into two:
The first table lookup yields the piece of
reservoir that precedes the nonlocal pointer
(thus the piece specifies
only dead elements), and the second table lookup
yields the piece of reservoir that follows it
(if there is no nonlocal pointer, let the
first table lookup
provide the entire piece of reservoir).
Even with these additional computational
steps, the time and
space bounds established above remain valid.

Since an iteration enumerates only elements that
were present when the iteration started and were
not deleted before their enumeration, iterations
are
robust.
\end{proof}

As is easy to see, the data structure of
Theorem~\ref{thm:mlog} also supports constant-time \Tvn{choice}.
This operation,
however, is not likely to
be very useful except in the combination \Tvn{extract-choice}.

\subsection{A Data Structure with \Tvn{color} and \Tvn{setcolor}}

In this subsection we reconsider a data structure
of Dodis et al.~\cite{DodPT10} and extend it to support
constant-time initialization.
The data structure basically emulates
-ary memory, for arbitrary ,
on standard binary memory almost without
losing space.
This is essential
to the choice dictionaries
of Subsection~\ref{subsec:random}
and Section~\ref{sec:nonsystematic} in the case
where the number of colors is not a power of~2.

For  the following lemma
essentially coincides with the result
of Dodis et al.
Only in extreme cases can it be useful
to choose smaller values for~, but we need the
present form of Lemma~\ref{lem:succincter-t}
to prove Lemma~\ref{lem:unsystematic-tc}
in full generality.
In Lemma~\ref{lem:succincter-t} and Theorem~\ref{thm:succincter}
the sequence  is assumed to be represented
in a way that allows it to be communicated to the
initialization routine in constant time,
most naturally as a list of constant length
of (value, multiplicity) pairs.

\begin{lemma}
\label{lem:succincter-t}There is a data structure
that, for all given 
and for every given sequence
 of  positive integers
with  and
,
can be initialized in constant time and subsequently
occupies  bits,
needs access to a table of  bits
that can be computed in  time and
depends only on , 
and ,
and maintains a sequence
 
drawn from

under constant-time reading and writing
of individual elements in the sequence.
The data structure does not initialize the sequence.
\end{lemma}

\begin{proof}
Dodis et al.~\cite{DodPT10}
considered the fundamental problem of
realizing an array of  entries,
each drawn from a set of the
form 
for integer .
They
described a beautifully simple
construction that allows individual array entries
(called \emph{small digits} in what follows) to
be read and written in constant time, yet needs
just  bits
(the authors even argue that
 bits suffice).
A few details not considered by
Dodis et al.\ can easily be handled:
(1) The authors actually group  small
digits into one
\emph{big digit} drawn from
, where
 is chosen to make
, and accordingly store
approximately  big digits
instead of  small digits.
The proof in fact is correct for arbitrary 
as long as 
is 
(but still , so that big
digits can be manipulated in constant time);
this allows us to handle
values of  larger than .
(2) The big digits are associated with the nodes
of a binary tree~.
The proof remains correct if the big digit associated
with the root of  in fact is drawn from a smaller domain
than the other big digits;
this allows us to deal with the fact that 
may not divide~.
(3) The construction needs a
table  of  bits
of precomputed numbers that depend on  and~.
The authors do not mention the time needed
to obtain
 from  and ,
but it is easy to see that it can be done in
 time, namely constant time per
level in~.
In particular,
a part of~ indicates
the size and memory layout of the data structure.
The same time allows us to compute the
powers , for , which are needed
to extract small digits from big digits and
update small digits within big digits in
constant time.

Lemma~\ref{lem:succincter-t} follows by
splitting
the sequence
 
into
 subsequences
 with 
and 
and applying the
construction of Dodis et al.\ independently
to each such subsequence.
\end{proof}

For  and , let us call a sequence
 of  positive integers 
\emph{-balanced} if

for all  with .
The theorem below requires  to
be -balanced for some fixed~.
While this requirement is necessary for our proof,
it is a technicality of scant interest.
In order for a sequence  not to be
-balanced for any fixed , at least some of
its elements must be very large relative to~.
Indeed, provided that  for ,
our implicit convention
 for  implies
that the condition of -balance is
automatically satisfied for some fixed  if
.

\begin{theorem}
\label{thm:succincter}For all fixed ,
there is a data structure
that, for all given 
and for every given
-balanced sequence
 of  integers
with  for 
and  and
,
can be initialized in constant time and subsequently
occupies  bits
and maintains a sequence
 of  integers,
drawn from
,
under constant-time reading and writing
of individual elements in the sequence.
The data structure does not initialize the sequence.
For , the parameter  may be presented
to the data structure in the form of a pair 
of positive integers with  and
.
\end{theorem}

\begin{proof}
Let  be the table
used by the data structure
of Lemma~\ref{lem:succincter-t}.
Aside from the question of 
being -balanced,
the only essential difference between 
Theorem~\ref{thm:succincter} and
Lemma~\ref{lem:succincter-t} is that the
theorem does not assume  to be externally
available.
The theorem provides space for storing ,
but no time for computing it before the first
operation must be served.
Assume without loss of generality that 
and let  and .
Recall that 
can be computed from  in  time---if
necessary, this time bound also allows for the calculation
of  from  and 
for  via repeated squaring.

Consider first the special case .
We allocate first  bits for ,
then a block of  additional bits whose use
will be explained later, and finally
space for a data structure~.
 is a trie of constant depth with  potential leaves,
the th of which, for ,
holds  in  bits
if  has changed from its
initial value of~0.
Disregarding the question of space,
 can provide the functionality promised in
the theorem.
Choose the (constant) depth of  to
be at least .
We will use  for at most  operations.
Thus, even without paying particular attention
to economy of space, we can easily ensure
that the number of bits needed for  is
 and
therefore, for  larger than a constant, at most
.

We construct  in a background process
interleaved with the execution of the first
 operations (the \emph{first phase}),
using  to serve these  operations.
At the end of the first phase,
when  is ready, we start running an
instance  of the data structure of
Lemma~\ref{lem:succincter-t} in parallel
with , making sure that every
 that
is written to
after the first phase
has the correct associated value
in  and, if  is still present in ,
in .
Interleaved with the execution of the second
group of  operations (the \emph{second phase}), we empty 
element by element, making
sure that every element deleted from 
has the correct value in ---informally, we transfer the element
from  to .
To determine 
for some  in the second phase,
we first query .
If  is present in~,
the associated value is returned.
Otherwise we return the value
associated with  in~.
All operations can
still be executed
in constant time
during the parallel operation of
 and .
After the second phase, 
is empty and can be considered to have disappeared.

A problem that was ignored until now is
that  and  must use the same memory area
during the second phase.
Partition  into  \emph{sectors}
 of consecutive elements,
all of the same size ,
except that  may be smaller.
We call  the \emph{forbidden} sector
and  the \emph{incomplete} sector.
 is in fact split into three instances of
the data structure of Lemma~\ref{lem:succincter-t}:
, whose universe is the forbidden sector ,
and two instances  and 
whose universes are
 and 
(both translated suitably to begin at~1),
respectively.
At the end of the first phase, aided by ,
we can compute the exact number of bits required
for each of ,  and  in constant time.
Overlapping the memory space used for ,
we allocate space first for  and then for
 and .
Since  occupies at least  bits
and  occupies at most  bits,
 is the only component of  whose memory
area overlaps that of .

During the transfer of elements from  to 
in the second phase,
it is easy to handle
elements outside of the forbidden sector.
As concerns elements destined for , however,
a problem arises because
 overlaps , which is still in use.
In order to circumvent this problem, we will
ensure that during the execution of the
first  operations, no element
located in the forbidden sector
acquires a nonzero value.
We achieve this by storing the elements not directly, but
according to a rudimentary hash function that
is data-dependent and defined in a lazy manner.
The hash function takes the form of a bijection
 from  to itself.
Every element in the incomplete sector is mapped to
itself by , and the rest of  is induced
by a permutation  of 
in the following way:
For each , the th element
of  is mapped by  to the th element
of , for .
Arguments in  of operations to be executed
are mapped under  before being passed on
to one of the three components of , and
answers obtained from the components of  are
mapped under  to obtain the
appropriate return values.
The permutation  and its inverse must
be stored in the data structure, the necessary
space being furnished by the block of
 bits allocated but not used so far.

It remains to describe the permutation  of~.
When an element outside of the incomplete sector is
first given a nonzero value,
suppose that it belongs to .
Then  is defined to be an arbitrary
element of , such as its minimum.
Similarly, if  is the second sector
other than  to receive
a nonzero value,  is defined to
be an arbitrary element of
.
Continuing in the same manner, we can avoid
 as a value of  for the duration
of at least  operations, which
was our goal.

Let us now turn to the general case.
While  may contain 
distinct values, by assumption there is a constant
 for which  can be partitioned
into  \emph{segments} 
of consecutive elements such that for
, 
for all .
Informally, our plan is to run the procedure described for
a single segment in parallel for all segments,
with a shared temporary data structure 
``hiding'' in the memory area that holds
the union of the forbidden sectors of all segments.

For ,
if and when  becomes nonzero
during the execution of the first  operations,
 now stores 
in  bits.
The total number of bits, , required for 
is therefore  plus
 for a
set  with .
Let .
Since  is -balanced
and ,
,
which, for  larger than a constant, is at most
.

Call a segment \emph{small} if it contains at
most  elements, and
\emph{large} otherwise.
The reason for distinguishing between small and
large segments is that a
large segment can always be divided into 
sectors, all of the same size, except that
one segment may be smaller.
Because this is
not necessarily the case for a small segment,
the small segments do not contribute
forbidden sectors.
We must show that, even so, the forbidden sectors
together require enough space for the values
associated with their elements to ``cover''
the temporary data structure .
If this is so, the parallel procedure will
work as intended:
During the first  operations, a shared
permutation  is defined so that the at
most  elements that receive nonzero values
in the first and second phases avoid all
forbidden sectors, and in the second phase
the elements stored in  are
transferred to at most  instances
of the data structure of Lemma~\ref{lem:succincter-t}.

If  is the set of elements in small segments,
then , which, for  larger
than a constant, is at most .
Since  is -balanced,
we may conclude, still for  larger than a
constant, that
.
If this relation holds and  is the set of
elements in forbidden sectors, it is easy to see that

Thus, for  larger than a constant,
,
from which the desired conclusion follows.
\end{proof}

\subsection{Choice Dictionaries with \Tvn{p-rank} and
 \Tvn{p-select}}
\label{subsec:random}

In this subsection we describe an extended choice
dictionary that supports the
additional operations
\Tvn{p-rank} and \Tvn{p-select}.
Before delving into the technical details, we
provide a brief overview of the main ideas
involved in the special case of constant
operation times and for the special application
of uniform random generation.

First, using methods that are fairly standard,
at least if suitable tables are assumed to be available,
the operations \Tvn{p-rank} and \Tvn{p-select} and
even \Tvn{rank} and \Tvn{select} can be supported
within \emph{segments} of
polylogarithmic size (Lemma~\ref{lem:p-t}):
One maintains a summation tree  of constant depth
and almost-logarithmic degree, with each node storing the
number of elements of the client set  below each
of its children, and the procedures of accumulating
prefix sums along a root-to-leaf path in 
(for \Tvn{rank}) and of searching within the
prefix sums along such a path (for \Tvn{select})
are carried out with table lookup.

At this point the uniform generation boils down
to choosing a random segment.
The choice should not be uniform, however.
Instead a segment should have a probability
or \emph{weight}
proportional to the number of elements of 
in the segment.
This partitions the segments
dynamically into a polylogarithmic
number of \emph{weight classes}, and the uniform
generation can proceed by first picking a 
random weight class, according to a suitable probability
distribution, and subsequently picking a segment
uniformly at random within the chosen weight class.
The latter task can be solved with a data structure
that we already have, namely that of Theorem~\ref{thm:nlogn}.
It is wasteful in terms of space, but
because it is applied to segments of
polylogarithmic size and not directly
to elements of the original universe, the space
requirements can be made sufficiently small.

As for picking a random weight class, the relevant
probability distribution changes dynamically and
can be almost arbitrary,
which renders the problem difficult.
What makes it manageable nonetheless is the fact
that the number of weight classes is only
polylogarithmic.
We solve the problem using a data structure of
P\v atra\c scu and Demaine~\cite{PatD06},
slightly modified to suit our needs
(Lemma~\ref{lem:prefSum}).
Conveniently, although it is more powerful
than what is required, the same data structure can also
be used within segments.
This ends the overview of this subsection.

If the operations \Tvn{rank} and \Tvn{select}
or \Tvn{p-rank} and \Tvn{p-select} are to be realized
with the trie-combination method of
Section~\ref{sec:trie}, the inner nodes
in the trie must be generalized.
As usual, identify the leaves of the trie, in
the order from left to right, with the integers
 and consider the colorless case
and \Tvn{rank} and \Tvn{select}.
An inner node  with  children, rather than maintaining
a subset of  or, what amounts to
the same, a bit vector of length~, must maintain
a sequence  of  nonnegative integers, the th
of which, for , is the number of
leaf descendants of the th child of~
that belong to the client set, and \Tvn{rank}
and \Tvn{select} must be generalized to the
functions \Tvn{sum} and \Tvn{search}
defined below.

Given a sequence  of  integers, where
, let us denote its th entry,
for , by .
Moreover, let  be the
sequence of prefix sums of~, i.e., the
sequence of length  with
 for .
Note that  is a linear operator.
Say that an \emph{atomic ranking structure}
for  is a data structure that can return
 in constant time for
arbitrary integer~.
Let a \emph{searchable prefix-sums structure}
be a data structure that, for arbitrary
 with ,
can be initialized for parameters
 and subsequently
maintains a sequence 
of  integers, initially ,
under the following operations:

\begin{description}
\item[\normalfont]
():
Returns  if  and 0 if .
\item[\normalfont]
():
Returns .
\item[\normalfont]
( and
):
Replaces  by .
\end{description}

\noindent
The complicated precondition of \Tvn{update}
simply stipulates that  be a signed -bit
quantity whose addition to  neither causes
 to become negative nor causes
some entry in 
to exceed .
Negative values for  are assumed
to be represented suitably.
Following the initialization for parameters ,
we call  the \emph{universe size},
 the \emph{sum bit length} and
 the \emph{update bit length} of a searchable
prefix-sums structure.
Although the definition does not list an operation
\Tvn{value} such that  returns ,
for , it can easily be derived as
.

Assume that each inner node  of a trie  constructed
as described in Section~\ref{sec:trie} is equipped
with a searchable prefix-sums structure  with
universe size , where  is the degree of~,
sum bit length at least
 and update bit length~1.
Informally, the th integer maintained by~
will be the sum of the values ``below'' 's
th child.
With notation as in
Section~\ref{sec:trie}, \Tvn{rank} and \Tvn{select}
for the uncolored case
can be realized as follows:

\begin{description}
\item[\normalfont:]
Initialize a variable  to 0.
Then, starting at the root  of  and as long as the
height of the current node  is at least~2,
let ,
add  to  and step
to the th child of~.
When a node  of height~1 is reached,
return .
\item[\normalfont:]
Starting at  and as long as the current node
 is not a leaf,
let ,
subtract  from 
and step to the th child of~.
When a leaf  is reached, return .
\end{description}

The operations \Tvn{insert} and \Tvn{delete} given
in Section~\ref{sec:trie} have to
be modified in minor ways.
E.g., the test 
should be replaced by .
The details are left to the reader.
The remaining operations will not be needed.

A suitable searchable prefix-sums structure for our
purposes is the slight generalization of a data
structure due to
P\v atra\c scu and Demaine~\cite[Section~8]{PatD06}
expressed in the following lemma.
Our result differs from that of
\cite{PatD06} in that we allow
entries in the array  to be zero and
distinguish between the sum bit length 
(which bounds the values in~)
and the word size 
(which determines the computational power
of the RAM).
We provide a proof that is somewhat
simpler and more explicit than
that of~\cite{PatD06}.

\begin{lemma}
\label{lem:prefSum}There is a searchable prefix-sums structure that,
for arbitrary  with
, can be initialized
for parameters 
in constant time and subsequently
occupies  bits and supports
\Tvn{sum}, \Tvn{search} and \Tvn{update}
in  time.
\end{lemma}

\begin{proof}
For the time being ignore the claim about constant-time initialization.
Choose  as an integer with  and
.
As noted by P\v atra\c scu and Demaine,
it suffices to prove the lemma for
universe size at most~.
This is because,
similarly as in the trie-combination method,
the overall data structure can
be organized as a tree  of height
, each of whose
 nodes contains a data structure for the
same problem, but for a sequence of
length at most~.
Assume therefore that
the universe size is bounded by 
and, in fact, that it is exactly~.

Let 
and choose  with

such that for each  with , an integer

can be encoded through the binary
representation of (the nonnegative
integer)  in a field
of  bits.
A sequence of  integers encoded in this way,
called a \emph{small vector} with \emph{offset} , fits in
 bits and can be manipulated in constant time.
In particular, provided that no overflow occurs,
 can be applied to a small vector
in constant time
through multiplication by  and use of
the relation .
Let a \emph{big vector} be a sequence of 
integers drawn from .
In the following, when an integer  is used
in a context that requires a vector,
 is shorthand for the vector .
For  and
integer , let
 be the number in 
closest to , i.e., .

For simplicity, assume that the number of
calls of \Tvn{update} is infinite.
For  if the th update
is , briefly define 
as the sequence of length  with
 and  for
.
For arbitrary integers  and ,
let  if ,
 () if ,
 if
, and  otherwise.
Let \emph{phase}~0 be the period of time from the
initialization until and including the
execution of the th update and,
for  let phase~
be the time from the end of phase~
until and including the execution of
the th update.

We pretend to keep track of the number 
of updates executed so far;
it will be easy to see that it suffices
to know .
During phase , for 
we store  and 
as big vectors and
,
 and
 as small vectors with offset .
Moreover, we have an atomic ranking dictionary
 for .
Throughout the phase and piecemeal, interleaved with
the execution of updates, we add
 and

componentwise to obtain 
and compute an atomic ranking dictionary  for
.
Since the latter can also be done in 
time \cite[Corollary~8]{Hag98}, it suffices
to spend constant time per update on
this background process in order for
 and 
to be ready at the beginning of the next phase,
which is when they are needed.

To execute 
in phase~, for some ,
add  to the th components of
 and  to obtain 
and , respectively,
replace the th component of
 by 
to obtain , and increment~.
If subsequently  and hence ,
prepare for phase~ by initializing to zero
an integer variable that held 
in the phase that ends and will hold
 in the phase
that begins.

To execute  in phase~, for some ,
return the sum of the th components
of ,
 and
.

To support , it suffices to be able
to compute  for arbitrary given
.
To solve this problem
in phase~, for some , let  and
use  to identify a 
with .
Then compute  and

(of course,  can be obtained as
)
and return .
By Lemma~\ref{lem:word}(d),
this can be done in constant time.

To see that the computation of
 is correct, let

.
Informally,  is the current state
(i.e., after  updates)
of the sequence  of prefix sums, but
``normalized'' through the subtraction of 
to have the value 0 at time  and at the index~.
Of course, instead of computing
, we can just as well determine
.
We cannot compute ranks in  with
Lemma~\ref{lem:word}(d), however,
because  may contain large values, and 
can also be large.
Instead of finding  directly,
we therefore compute and return ,
where  and  can be
viewed as approximations of  and ,
respectively, that contain only small values.
What remains to be shown is that the differences
between  and  and between
 and  do not influence the result.

 and  coincide and are small.
Indeed,
.
 is defined similarly as , but
where  is  plus a constant (vector),
 is  plus a constant.
If we define the \emph{jump} in  at 
as  and the jump in  at~
analogously, it follows that for ,
either  and  have the same jump
at , or the jump in  at~ is .
We may conclude that for , if
, then .
Because , we have
.
In other words, for our purposes  is
a sufficiently good approximation of~.

To finish the argument, we must demonstrate that
.
The basic reason why this is so is
that if  is far from ,
it is also far from all components of ,
so that even the very bad approximation 
of  has the same rank in  as .
It suffices to show that if , then
 for .
But if 
and , then, by the choice of ,


For each node  in the tree , let  be the
searchable prefix-sums structure at~.
The leaves of  can be identified with
the  positions in the sequence of integers
maintained by the overall data structure, and when
 is a node in  and  is a child of~,
the value recorded for  in 
is the sum  of the values stored in the leaf
descendants of~.
In order to achieve a constant initialization
time, we (re-)initialize  only when 
changes from~0 to some other value.
Initializing  involves initializing a
constant number of simple variables and
small vectors, which can certainly happen in
constant time, and initializing two big vectors
of  bits each, which can be done
with the method of
Lemma~\ref{lem:2.12} using another  bits.
\end{proof}

\begin{lemma}
\label{lem:p-t}There is a choice dictionary that, for arbitrary
 with ,
can be initialized for universe
size ,  colors and tradeoff parameters  and 
in constant time
and subsequently occupies
 bits and,
if  or if given access to tables of  bits
that can be computed in  time
and depend only on , ,  and , executes
\Tvn{color} in constant time and \Tvn{setcolor}, 
\Tvn{rank} and \Tvn{select}
in  time.
\end{lemma}

\begin{proof}
Without loss of generality assume that .
View each of the  color values to be maintained
as a \emph{small digit} in the range 
and take  and
 ( is introduced only for
the sake of the proof of
Theorem~\ref{thm:p}).
Partition the  small digits into
 groups of 
consecutive small digits each, except that the
last group may be smaller, and represent the
small digits in each group through a \emph{big digit}
in the range ,
where , except that the last big digit
may come from a smaller range.
A natural scheme represents small digits
 through the integer
, but from the
point of view of correctness, the
\emph{encoding function} can be an arbitrary
bijection from  to
.
We realize the encoding function and its
inverse through tables  and .
In more detail, the encoding table  maps
-bit concatenations of
the binary representations of  small digits,
called a \emph{loose representation}
of the  small digits,
to the corresponding big digit, and the
decoding table  realizes the
exact inverse mapping.
If the last group of small digits contains fewer
than  small digits, it needs separate
encoding and decoding tables.
This is easy to handle and will be
ignored in the following.

We maintain the sequence of  big
digits in an instance  of the data structure of
Theorem~\ref{thm:succincter},
whose space requirements are
 bits.
Forming 
\emph{ranges} 
of  consecutive big digits each,
except that the last range may be smaller,
we initialize all big digits in a range
exactly when for the first time a
digit in the range acquires a nonzero value.
We keep track of the ranges that have been initialized
using an instance of the choice dictionary
of Theorem~\ref{thm:systematic-2} with
universe size  and therefore
negligible space requirements.

In order to support \Tvn{rank} and \Tvn{select},
we maintain for each 
in an instance   of the searchable prefix-sums structure
of Lemma~\ref{lem:prefSum}, initialized with
sum bit length
 and update bit length , a sequence
of  integers, the th of which is the
number of occurrences of the color  in the range ,
for .
An exception concerns the color~0:
Instead of storing the number  of
occurrences of 0 in ,
we store the complementary number ,
where  is the number of small digits
in  (usually ).
The reason is that  is initially zero,
which matches the initial value provided by~.
In the following we assume that  is
modified to replace counts of occurrences
communicated to and from a caller by their
complementary numbers.
The number of bits needed for 
is .

To execute \Tvn{color}, we obtain the
relevant big digit from  and use 
to convert it to the corresponding loose
representation, after which answering
the query is trivial.
The realization of \Tvn{setcolor} is similar,
except that a call 
must additionally call \Tvn{update} in
 and , where  is the color
of  just before the call under consideration.
For  larger than a constant---the only case
in which  and  are
actually needed---the tables occupy
 bits
and can, if realized according to
the natural scheme discussed above, be computed in  time.

To execute ,
where
,
we first compute  and
 and find .
The value to be returned is
 plus the number
of occurrences
of the color  among the  first small digits
of .
We compute the latter quantity by obtaining
the at most  big digits of  from  one
by one
and processing each in constant time as follows while
accumulating a count of the number of
relevant occurrences of  seen:
After obtaining the loose representation of the
big digit at hand with the aid of ,
we use the algorithm of
Lemma~\ref{lem:word}(c) to reduce the problem
of counting the number of relevant occurrences
of~ in the loose representation
to one of counting the total number of
1s in at most  fields,
each of which occupies  bits
and holds a value of either 0 or~1.
Finally the latter problem is solved
by lookup in a table .
For  larger than a constant,
 occupies
 bits
and can be computed in  time.

To execute , where  and
,
we first compute .
If ,
the th occurrence of  is in ,
and the value to be returned is the position
of the th occurrence of  in ,
where .
If ,  is larger than the total number
of occurrences of , and we return~0.
To find the th occurrence of 
in  if , we proceed similarly
as in the case of  and
obtain the big digits of  one by one
from~.
Using  and  and spending
 time, it is easy
to identify the big digit that contains the
th occurrence of  in  and the number of
occurrences of  before that big digit.
Again using 
and the algorithm of Lemma~\ref{lem:word}(c)
to replace occurrences of  by occurrences
of~1 in fields of  bits,
we finish the computation by consulting an
appropriate table  that,
for  larger than a constant,
also occupies  bits
and can be computed in  time.

For ,
each operation on  runs in
 time, 
and every consultation of 
takes constant time.
Therefore \Tvn{color} runs in constant time
and every other operation
runs in  time.
\end{proof}

\begin{theorem}
\label{thm:p}For all fixed ,
there is a choice dictionary that, for arbitrary
,
can be initialized for universe
size ,  colors and tradeoff parameter 
in constant time
and subsequently occupies

bits and executes \Tvn{color} in
constant time and \Tvn{setcolor},
\Tvn{p-rank} and \Tvn{p-select} (and hence \Tvn{choice}
and \Tvn{uniform-choice})
and, given  additional bits,
robust iteration in  time.
\end{theorem}

\begin{proof}
For the time being ignore the claim about
constant-time initialization and assume
the tables , ,
 and 
of the previous proof
to be available.
Let  be an integer constant with .
If  choose .
Otherwise let 
and choose  as a positive integer with
, but .
For , the claim follows from the previous
lemma, used with . 
For 
we use the construction 
shown
in
Fig.~\ref{fig:select}.

\begin{figure}
\begin{center}
\epsffile{select.eps}
\end{center}
\caption{An example evaluation of 
for some color  with
the combined data structure of Theorem~\ref{thm:p}.
First the argument of , 16,
is translated by 
to the triple  
(shown as \Tmyw)
consisting of the relevant
weight, , the index, 3, of the relevant segment of
weight 3 among all segments of weight 3, and the
index, 2, of the relevant element within that segment.
Then the relevant segment is identified with
the aid of , and finally the relevant
element in that segment is located
with the corresponding bottom structure,
expressed in the form of
its global index, 35, and returned.
In the interest of clarity,
the figure assumes that the 
function of  in fact coincides
with .}
\label{fig:select}
\end{figure}

Computing  as a positive integer with
, we partition the 
color values to be maintained into
 \emph{segments} of 
color values each, except that the last segment
may be smaller, and maintain each segment in an
instance of the data structure of
Lemma~\ref{lem:p-t} called a
\emph{bottom structure}.
The total number of bits occupied by all
bottom structures is
,
plus  bits
for shared tables.

For each color ,
define the \emph{-weight} of each segment
as the number of elements of color~
in the segment.
We maintain the -weights of all segments in an
instance 
of the data structure of Theorem~\ref{thm:nlogn},
with the -weights
playing the role of the colors in~.
Thus  is initialized for universe size 
and  colors.
With  equal to the number of
elements of color~ in segments of -weight~,
for , we also maintain the sequence
 in an instance  of the
searchable prefix-sum structure of Lemma~\ref{lem:prefSum},
initialized with sum bit length

and update bit length
.
As in the previous proof,  and  must
be treated slightly differently.
The total number of
bits occupied by
 is
.
As described at the end of Section~\ref{sec:trie},
we use an additional choice dictionary  with
universe size  and therefore
negligible space requirements to keep track
of and carry out the initialization of
the bottom structures and

as appropriate.

A \Tvn{color} query can be answered in
constant time by the relevant bottom structure.
Because  or ,
every operation of a bottom structure
executes in  time.
When a call of \Tvn{setcolor} changes the
color of an element, from  to , say,
this can be recorded in the relevant
bottom structure in  time, after which
the update must be reflected in ,
,  and .
Since each of the two weight changes is
by 1 or , 
each of the updates of  and 
can happen in constant time---from the
perspective of  and ,
a color changes into a neighboring color.
Similarly, in each of  and ,
the update changes two values in the sequence
maintained, each by at most~.
A change of this magnitude is covered by the
update bit length of  and ,
and the update can be executed
in  time.

Consider a call 
with  and

(Fig.~\ref{fig:select}).
In the sequence  maintained
by , each element  can be thought of
as representing  elements of the top-level
universe , namely precisely those
that have color  and
are located in segments of -weight .
In particular,  is always a multiple of~.
Provided that ,
 designates a particular element 
of color  in a natural way:
First  selects
a particular -weight, , as the weight of~.
Then  is the index of
 in the sequence of all elements of 
of color  in segments of -weight , and finally
 is the index of the segment that contains~,
among those of -weight ,
and  is the index of 
within that segment.
Here ``index'' is to be understood as relative
to the orders imposed by the operation
 in  and the
operation  in the
relevant bottom structure.
Altogether, the top-level call
 reduces to one call of
each of \Tvn{search} and \Tvn{sum} in ,
one call of \Tvn{p-select} in , and one
call of \Tvn{select} in a bottom structure.
It can therefore be executed in  time.

To execute  for
, we first consult the relevant
bottom structure to find the color 
of  and the index  of 
among the elements of color  in its segment .
Then  is queried for the -weight  of  
and the index  of  among the segments
of -weight~.
Finally the return value is obtained as
.
The procedure works in  time.

To equip the data structure with robust iteration,
we ``plant''  additional
instances of the choice dictionary of
Theorem~\ref{thm:nlogn}, one for each
color, on top of the bottom structures and
appeal to the general
trie-combination method of Section~\ref{sec:trie}.

Let us now drop the assumption that the
tables , , 
and  are available for free.
As in the proof of Lemma~\ref{lem:p-t},
define  and .

Recall that
the task of  is to
map loose representations of sequences of
 small digits to big digits in an arbitrary
bijective manner and that 
should realize the inverse mapping.
We compute  and 
in a lazy fashion that combines techniques
used already in the proofs of
Lemmas \ref{lem:2.12} and~\ref{lem:permutation}.
We begin by setting 
and  and initializing
an integer  to 0.
Subsequent loose representations are mapped to
the big digits  in the order in which
they present themselves to the encoding table.
More precisely, in order to compute
the big digit corresponding
to a loose representation , we first check
whether  was mapped previously in the same manner.
This is the case if 
and .
If so, the big digit corresponding to 
is simply .
Otherwise  is incremented, and the new
value of  becomes the big digit corresponding
to~, a fact recorded by executing
 and
.
It is easy to see that whenever an entry in
 is inspected by the data
structure of Lemma~\ref{lem:p-t}, it
has already been computed
(only encoded values are decoded).

 and  are also
provided in a lazy fashion, but present minor
additional technical difficulties.
We in fact realize  and 
not as tables, but as constant-time functions
that carry out two table lookups each.

Recall that  operates on ``binarized'' loose
representations of big digits, ones in which
all occurrences of a color  of interest have been
replaced by 1 and all occurrences of colors other than~
have been replaced by~0, with each such value stored
in a field of  bits.
Correspondingly, define a \emph{big vector} to be
a sequence of  fields, each of 
bits and containing a value drawn from ,
and view a big vector as composed of 
\emph{blocks} of  fields each.
After a slight redefinition, the task of 
is to map each pair , where  is a big
vector and , to the
sum of the  first fields in~.
We divide this task into two subtasks:
sum the fields in the first  blocks in ,
where ;
and sum the first 
fields in the th block in~.

The first subtask is solved with a table :
For each big vector ,  is the
sequence , where  is the
sum of the fields in the  first blocks in~,
for .
Thus  is a table of sequences of prefix sums.
Note that each sequence is of
 bits, so that it can be handled
in constant time
(this is the reason for introducing ).
Each use of the table needs only a single prefix sum that
must be picked out from the full sequence.
This organization of the table ensures that it can be
computed in a lazy fashion:
Each color change leads to at most two new big vectors,
the entry in  of each of which can be
computed in constant time using word parallelism
from an old entry.
The second subtask is handled in a very similar way
using a second table .

The task of  is to map each pair ,
where  is a big vector and
, to the position of
the th 1 in , if any.
Again the task is divided into two subtasks,
each of which is handled in constant time. 
For the first subtask, we find the number 
of the block in  that contains the th 1---assume
for simplicity that there is such a block---by
computing
 with the algorithm of
Lemma~\ref{lem:word}(d).
Let  be the th number in the
sequence  (0 if ).
For the second subtask, we have to
locate the th 1 in the th
block of~.
This can be done in a similar way
using  in place of .

One may remark that the  bits required to
carry out robust iteration are already contained in
the bound of the theorem except in the extreme case
.
\end{proof}

If only \Tvn{p-select} and not \Tvn{p-rank} is
to be supported (e.g., if the only goal is to
realize the operation \Tvn{uniform-choice}),
it is possible to avoid the use of Lemma~\ref{lem:prefSum}
for .
In the context of Theorem~\ref{thm:p} and
with  and  defined as in its proof,
the ``bottom'' instances of the data structure of
Lemma~\ref{lem:prefSum}
(those incorporated, via Lemma~\ref{lem:p-t}, in
the bottom structures in the proof of
Theorem~\ref{thm:p} and in Fig.~\ref{fig:select})
can easily be replaced
by tries of constant height
of data structures that maintain the prefix sums
directly, realize \Tvn{update}
via a multiplication by
, two shifts and an addition,
and execute \Tvn{search} with the algorithm
of Lemma~\ref{lem:word}(d).
In slightly greater generality, this method
yields a constant-time searchable prefix-sums structure
that maintains a sequence of  integers,
each drawn from , under
arbitrary updates of single sequence elements.
Let us call such a structure an
\emph{-structure}.

For the ``top'' instances  in the proof of
Theorem~\ref{thm:p} and in Fig.~\ref{fig:select},
avoiding Lemma~\ref{lem:prefSum} is more involved.
We sketch the construction.
The essential task of a top instance
 can be viewed as that of
maintaining a set of  indistinguishable
items, each with a weight in 
(put differently, a multiset of weights),
under insertion and deletion of some (arbitrary) item
with a given weight and
an operation \Tvn{p-select} that maps each argument
 to the pair ,
where  is the weight of the
th item and  is its index 
within the set of items of weight~,
for some ordering of the items.
A first solution to this problems stores the
items of weight  in a doubly-linked list ,
for , and marks each list item
with its weight and
its distance to the end of its list.
The  lists are stored compactly together in an
array  of  cells, each of  bits,
and the positions in  of the first items in each list
are recorded in a second array.
A new item of weight 
is stored in the first free cell in~
and inserted at the beginning of 
and computes its distance-to-end value
as one more than that
of the formerly first item in~.
To delete an item of weight ,
we first swap the first item in  with the
item stored in the last used cell in~
and then delete it,
which does not upset
the distance-to-end value of any other item.
To execute , simply return
the pair of
the weight and one more than the distance-to-end value of the
item in .

Assume .
In order to reduce the space requirements
per color
from  to ,
we aggregate the  items into \emph{superitems}
of  items of a common weight each, with up to  items
left over for each weight in .
We store the numbers of left-over items
for each weight in an
-structure .
The superitems are maintained in  lists
as described above.
Since their number is , the total
number of bits used is indeed .
Consider an execution of ,
where  is the color under consideration,
and let 
be the total number of left-over items.
If ,
compute 
and return the pair
 as for the original
top-level structure .
Otherwise, with ,
let  be the
pair returned by the list-based structure,
called with argument ,
and return the pair .
Thus the left-over items are numbered before
the items in superitems, and the global number of
an item in a superitem is  plus  times the
number of superitems before
its own superitem plus its number
within the superitem.

\section{Nonsystematic Choice Dictionaries}
\label{sec:nonsystematic}

In this section we describe our most space-efficient
but also most complicated choice dictionaries.
We first consider the (somewhat easier) case
in which the number  of colors is a power of~2---until
and including Theorem~\ref{thm:unsystematic-f}---and
subsequently detail the changes necessary to cope
with general values of~.

As the reader may recall from the introduction,
the game is basically one of squeezing navigational
information into the leaves of a tree.
Lemma~\ref{lem:j-free} below describes a leaf that
can be in either the \emph{standard representation},
which offers no potential for storing
additional information,
or the \emph{-free representation} for some
color  that happens not to be represented
at the leaf.
In the latter case, information pertaining to the
tree path that ends at the leaf can be stored
in the leaf together with the usual information
kept there.
The proof of the central Lemma~\ref{lem:smalltree} describes how
to combine many such leaves to obtain a
tree that supports the operations \Tvn{color},
\Tvn{setcolor} and \Tvn{successor}.
We first address the overall data organization of the tree
and then discuss how to navigate in the tree,
after which the implementation of the query operations
\Tvn{color} and \Tvn{successor} is fairly
straightforward.
The final part of the proof of
Lemma~\ref{lem:smalltree} describes how to
re-establish the data-representation invariants
of the tree
after a call of the update operation \Tvn{setcolor}.
Lemma~\ref{lem:unsystematic-tf} essentially shows
how to put many such trees next to each other
to cover a larger universe, and
Theorem~\ref{thm:unsystematic-f} finally obviates
the need for precomputed tables.

In the following, let  and  be given
positive integers, take  and , assume
that  is an integer and at
least~2 and let .

\begin{lemma}
\label{lem:j-free}There is a choice dictionary 
with universe size  and for  colors
that can be initialized in constant
time and subsequently occupies  bits and
executes \Tvn{color} and \Tvn{setcolor}
in  time and
\Tvn{successor} in  time.
Moreover, during periods in which
, where
 is 's client vector and
,
 supports two additional operations
that execute in  time:
Conversion from
the (initial) \emph{standard representation}
to the \emph{-free representation}
and conversion back to
the standard representation.
When  is in the -free representation, 
must be supplied as an additional argument in
calls of \Tvn{color}, \Tvn{setcolor} and \Tvn{successor}
and in requests for conversion to the standard representation
(we will, however, suppress this in our notation).
In return, the  bits
of the -free representation of 
whose positions are multiples of  are unused,
i.e., free to hold unrelated information.

Alternatively, for arbitrary fixed ,
if given access to tables of
 bits that can be computed in
 time and depend
only on~,  can execute
\Tvn{color}, \Tvn{setcolor} and \Tvn{successor}
in constant time.
\end{lemma}

\begin{proof}
We can view 's task as that of maintaining a
sequence of  \emph{digits} to base .
The standard representation is simply the concatenation,
in order, of the -bit binary representations of the 
 digits.
With this representation,
the operations can be carried out as for the
data structure of Lemma~\ref{lem:atomic-c}.

For each , the
-free representation partitions the  digits
into \emph{big groups}
of  consecutive digits each and stores
each big group in  rather
than  bits, leaving free every bit whose
position is a multiple of ,
as promised in the lemma.
First, using the increasing bijection
 from 
to , the  digits
to base  of each big group are
transformed into  digits to base .
Call this the \emph{-intermediate representation}.
Then the  transformed digits are partitioned
into  \emph{small groups} of  consecutive digits each,
and each small group is viewed as a -digit integer
written to base  and is represented in binary
in  bits, which is possible because
.
At this point, within each big group, the  bits whose
positions are multiples of  are unused.
For , we store in the st
such bit a \emph{summary bit} equal to 1 exactly if
the color 
occurs as a transformed digit in the big group.
The summary bits are redundant, but help us to
execute \Tvn{successor} in constant time.
One bit is wasted, and
the other half of the
 bits are the promised free bits.

In order to convert  from the standard to the
-free representation, for some 
with , first the function
 is applied independently
to each digit.
Say that the  consecutive bits in which
a digit is stored form a \emph{field}.
By Lemma~\ref{lem:word}(c), we can compute an
integer , each of whose fields stores~1
if the corresponding digit is 
and 0 otherwise.
The function  can now be applied in parallel
to all fields by a subtraction of .
Subsequently, within each small group of 
digits, say , we must
convert  to
.
Since the digits  are readily available
as the values of -bit fields,
this can be done in  time for all small groups
using a word-parallel version of Horner's scheme
in a straightforward manner.
Finally, for
each 
,
within each big group a summary bit
must be computed and
stored in the appropriate position
within the big group.
To this end, first apply the algorithm of
Lemma~\ref{lem:word}(c) at most twice, with  and,
if , with , followed by bitwise
Boolean operations, to obtain an integer in
which the most significant digit of each
big group is zero, while the remaining bits
of the big group are also zero
if and only if the digit  does not occur
in the big group.
A subtraction from 
followed by the computation of \textsc{and}
and \textsc{xor} with the same number and
a suitable shift finishes the computation.

For the conversion in the other direction,
i.e., the conversion from
 to
 within each small group,
after clearing the bits whose positions are
multiples of  (those that held
summary and extraneous bits),
we compute the digits 
by repeatedly obtaining the remainder modulo~,
which yields the next digit, and keeping
only the integer part of the quotient with .
Except for the division by  with
truncation, the necessary steps are easily carried out in
constant time per digit and  time altogether.
Since division is not readily amenable to
word parallelism, we replace division by 
by multiplication by its approximate inverse.
More precisely, we carry out the division in
constant time using the relation
.
To see the validity of the relation for
all integers  with ,
simply observe that

and note that there is no integer
strictly between  and .
The product 
may have more than  bits.
It has no more than  bits, however, so it can
be computed using ``triple precision'', which
we simulate by handling the small groups
in three rounds, each round operating only on
every third group.
Truncated division by  is, of course, realized
as a right shift by  bit positions followed by
a ``masking away'' of the unwanted bits.
At the very end, to get from the
-intermediate to the standard representation,
 must be
applied independently to each field.
This can be done similarly as described above
for .

As detailed above, the conversion between
the standard and the -intermediate representations
depends on , but takes only constant time.
In contrast, the conversion between the
-intermediate and the -free representations
takes  time, but is independent of~.
This observation is important to
the proof of Theorem~\ref{thm:unsystematic-f}.

Assume that  is in the -free representation, for
some , and that a
call  is to be
executed for some

and .
Suppose, for ease of discussion, that the return value
 is nonzero, and let  and  be the big
groups that contain the st and the th
digit, respectively.
Applying to  a computation that, informally,
converts the single big group  to the standard
representation, we can test whether
the th digit belongs to  and,
if so, find and return~.
Otherwise we locate  by applying an algorithm
of Lemma~\ref{lem:word}(a) to a suitable suffix of
those summary bits that pertain
to the color , with all other bits cleared,
after which  can be found by converting
 to the standard representation
as done previously for~.
The computation runs in  time, its
bottleneck being the conversions to the
standard representation.
It is easy to see that \Tvn{color} and \Tvn{setcolor}
can be executed in  time by computing
the relevant power of  via repeated squaring.

Alternatively, the conversion of single big groups
from the -intermediate
to the -free representation
and vice versa can be carried out
by table lookup.
A table for each direction
of the conversion maps each sequence of
 possible digits
to the corresponding other
representation and therefore has  entries
of  bits each.
For fixed  and
for  larger than a suitable constant,
we can instead
use repeated table lookup,
mapping at most  small groups of
 digits each at a time.
This reduces the number of bits in the tables
and the time needed to compute them to
.
In the case of the conversion to the
-free representation, each table entry
for at most  small groups
must provide suitable summary bits for the
small groups concerned, and the composition 
of such entries includes forming the bitwise
\textsc{or} of the partial summaries.
\end{proof}

\begin{lemma}
\label{lem:smalltree}There is a choice dictionary  with
universe size  and for  colors
that can be initialized in constant time,
uses  bits and
supports \Tvn{color} in  time and
\Tvn{setcolor} 
and \Tvn{successor}
in  time.

Alternatively, for arbitrary fixed ,
if given access to tables of
 bits that can be computed in
 time and depend
only on~,  can execute
\Tvn{color} and \Tvn{successor}
in  time.
\end{lemma}

\begin{proof}
Let  be a complete -ary outtree of
depth~, whose leaves, in the order from
left to right, we will identify with
the integers .
Let  be the root of  and, for all ,
take  to be the maximal subtree of 
rooted at~.

Let the client vector of  be
.
We divide the universe
 into 
\emph{segments} 
 of  consecutive integers each.
For each , call  \emph{full}
if  for all
 and all leaf
descendants  of , and
\emph{deficient} otherwise.
Informally, a deficient node is one that has a
leaf descendant with a missing color.
For each , let the \emph{spectrum} of 
be the string  of 
bits defined as follows:
If  is deficient,
then for , 
exactly if 
for some leaf descendant  of~
(informally, if the color~ is represented in ).
If  is full, as a special convention,
, a
bit combination that cannot otherwise occur.
If 
(only the color~0 is represented in ),
we say that  is \emph{empty};
this is initially the case for every node~.
If a node in  is deficient but not empty,
we call it \emph{light}.
For each inner node  in , define the
\emph{navigation vector} of  to be the concatenation
, where  are the
spectra of the  children of  in the
order from left to right.

For , let 
be the semipartition
 of 
and let  be the semipartition
of  obtained from 
by subtracting  from every element
in each of its sets.
We do not store~.
Instead, for , 
is stored in an instance  of the
choice dictionary of Lemma~\ref{lem:j-free}
called a \emph{leaf dictionary},
and  are in turn stored
in  words  of  bits each.
Two additional \emph{root bits} indicate whether the root 
of  is full and whether it is empty.
It is helpful to think of  as ``normally''
storing ,
for .
If this were always the case and all navigation vectors
were available, a call of 
could essentially use navigation vectors to find
a path from  to the leftmost leaf  in 
such that  contains an 
element larger than , if any,
and the smallest such element
could be obtained through a call of
.
Moreover, \Tvn{setcolor}
could update navigation vectors as appropriate.
However, we have no space left to store
navigation vectors, and so have to proceed differently.

The parent of every light node in  other than 
is also light, and
every deficient inner node in  has at least
one deficient child.
Let the \emph{preferred child} of a deficient
inner node be its leftmost light child if
it has at least one light child, and its leftmost
empty child otherwise.
Let  be the subgraph of  induced
by the edge set  obtained as follows:
First include in  all edges
from a light inner node to its preferred child.
Then, for every empty node 
that has an incoming edge in ,
include in  the edges on the path
from  to its leftmost leaf descendant.
 is a collection of node-disjoint paths
called \emph{light paths}, each of which ends
at a leaf in~.
When  is a light path that starts at a (light) node
 and ends at a (deficient) leaf , we call  the
\emph{top node},  the \emph{proxy}
and the leftmost leaf descendant of 
(that may coincide with~)
the \emph{historian} of  and of every node on~.
A light node in  that is neither the root nor a leaf
is a top node exactly if it is not the preferred child
of its parent, i.e., if it has at least one
light left sibling.
No proper ancestor of a top node  can have
a descendant of  as its leftmost leaf descendant,
so a leaf is the historian of at
most one light path.
If  is the historian of a light path , the
top node and the proxy of  are also said to be the
top node and the proxy, respectively, of .
These concepts are illustrated in Fig.~\ref{fig:lightpaths}.
A leaf  cannot be the historian
of one light path and the proxy of another,
since otherwise the two corresponding
top nodes would both be ancestors of 
and the path between them would contain
only gray nodes and be
part of a light path, an impossibility.
A similar argument shows that in the
left-to-right order of the leaves of~,
no historian or proxy lies
strictly between a historian and its proxy.

\begin{figure}
\begin{center}
\epsffile{lightpaths.eps}
\end{center}
\caption{Example light paths (drawn thicker).
Top nodes, historians and proxies are labeled
``'', ``'' and ``'', respectively,
and a subscript identifies the associated
light path.}
\label{fig:lightpaths}
\end{figure}

Suppose that the nodes on a light path 
are , in that order.
Then the \emph{history} of
 and of each of 
is the concatenation of the
navigation vectors of ,
in that order (, as a leaf, has
no navigation vector).
An important fact to note is that if
a leaf dictionary
is in the -free representation for some~,
then it allows the history of a light path to be stored
in its  free bits.
In order for this actually to be possible, we assume
that histories (and, by extension,
navigation vectors and spectra) are represented with 
gaps of  bit positions between consecutive bits,
so that a history spreads over up to an entire
-bit word.
To fill up the word,
we store a history of  bits
prefixed by  arbitrary bits,
so that the positions in the word of the bits that make up
the navigation vector of a node 
depend only on the height of~.

We represent  using the following
storage scheme:
Let  be the permutation of 
that maps each  to itself,
except that  and  for each
pair of a proxy  and its historian~.
Then the following holds for :
 is stored in , and
\begin{itemize}
\item
if  is a proxy (and therefore deficient),
 is in the -free representation,
where  and
---we
will say that
 is in the
\emph{compact representation}---and
 stores not only , but also the history of ;
\item
if  is empty but not a proxy,
 may not have been initialized;
equivalently,  may hold an arbitrary value;
\item
in all remaining cases, i.e., if  is neither a
proxy nor empty,

is in the standard representation.
\end{itemize}

\noindent
This paragraph tries to motivate the
not-so-natural storage scheme.
The usefulness of navigation vectors was
already observed above.
All nontrivial navigation vectors
are contained in the histories of the proxies.
As we have seen, if  is a proxy and
therefore deficient,  can
be represented sufficiently compactly to allow
the history of  to be stored with it.
However, when the history of a proxy 
is needed,  is not known, so  cannot
be located.
The top node of  and hence also the
historian  of  are known, however, so
we store  and the history of 
in  rather than in .
In return (unless ),  must hold
(the standard representation of) .
The terms ``historian'' and ``proxy'' serve as reminders
that a historian (more precisely, the
corresponding storage word) holds a history
(namely that of its proxy), whereas a proxy
holds the data of what may be
another leaf (namely of its historian).
The convention that  may be arbitrary if
 is an empty leaf that is not in use as a proxy
is necessary to guarantee
a constant initialization time.

If we represent a current node in  as suggested
in Section~\ref{sec:trie}, i.e., through
the triple , where  is the height
in  of the current node and  is one more
than the number of nodes of height  to its left,
we can navigate in  as described
in Section~\ref{sec:trie}.
One operation that was not considered there and that
we need now is computing the leafmost leaf
descendant of the current node.
This is easy:
If the current node is (represented through)
, its leftmost leaf descendant in  is
.

\begin{proposition}
\label{prop:navigation}Let  and  be inner nodes in  such that
 is a child of  and assume that we know
the navigation vector of~, whether 
(i.e., whether  belongs to a light path)
and, if  is light, its history.
Then, in constant time, we can compute the spectrum
and the navigation vector of~,
decide whether 
and whether  is a top node and, if
 is light, compute its history.
\end{proposition}

\begin{proof}
The spectrum of  can be read off the
navigation vector of~.
If  is full or empty, its navigation vector
is trivial, namely the concatenation of 
copies of either  or ,
 is not a top node, and 
exactly if  and  is
's preferred child.
The latter condition can be tested in constant
time by inspection of
the navigation vector of .
Assume now that  is light, so that .
Then  is a top node exactly
if it has at least one light
left sibling.
If this is the case, the history of~
is stored at 's leftmost leaf descendant 
(more precisely, in ),
from where it can be retrieved in constant time.
If  is not a top node, it belongs to the
same light path as ,
whose history is known by assumption.
The navigation vector of  can be extracted
from 's history in constant time.
\end{proof}

Proposition~\ref{prop:navigation} provides the general
step in an inductive argument to show that we can
traverse a root-to-leaf path in  in constant
time per edge, always---until a
leaf is reached---knowing
the navigation vector of the current node,
whether it is a top node,
whether it belongs to a light path and, except
in the case of the root  of ,
its spectrum.
As for the inductive basis, 
is a top node and belongs to a light
path if and only if  is light,
and whether this is the case is
indicated by the two root bits.
If  is a top node, its history
is available
in  and, as above, the navigation vector
of  can be extracted from its history in constant time.
If  is not a top node, it is full or empty,
and its navigation vector is trivial, as above.

Our traversals of root-to-leaf paths in 
(called ``descents'') will be carried out by
starting at  and repeatedly applying a
\emph{selection rule} at the current node~
until a leaf is reached.
The selection rule indicates the child of 
at which the descent is to be continued.
We use three different selection rules that
we name for easier reference:

\begin{description}
\item[{\normalfont``leaf-seeking''}]
( is a leaf descendant of the current node~):
Step to that child of  that is an
ancestor of .
\item[{\normalfont``proxy-seeking''}]
(the current node  belongs to a light path):
Step from  to its
preferred child.
\item[{\normalfont``color-seeking''}]
( and
the color  is represented in ,
where  is the current node):
Step from  to the leftmost child of 
in whose spectrum the st bit is set.
\end{description}

Using algorithms of Lemma~\ref{lem:word} for the rules
``proxy-seeking'' and ``color-seeking'',
we can apply each of the selection rules
above in constant time.

The permutation  is not explicitly recorded.
As the following proposition shows, however,
we can compute  for arbitrary given
.
What the proposition actually says is that we
can compute both  and the information
necessary to make sense of the 
contents of
.

\begin{proposition}
\label{prop:semi}Given , in  time
we can compute , the spectrum of~,
and whether  is a proxy.
\end{proposition}

\begin{proof}
Use a first descent in  with the selection rule
``leaf-seeking'' to 
determine if  and to
compute the spectrum of~,
which will be known when the leaf  is reached.
Now  is a proxy exactly if .
In a second descent in ,
initially again use the selection rule
``leaf-seeking''.
Starting at the time when the current node
is first a top node, if ever, always remember
the historian  of the
most recently visited top node
(the history stored in  is used for navigational
purposes anyway).
If and when the current node becomes a top node
with  as its leftmost leaf descendant,
(this will happen at some point exactly if
 is a historian),
permanently change the
selection rule to ``proxy-seeking'' and
continue the descent.
Let  be the leaf reached.
If the selection rule is still ``leaf-seeking'' at this time 
and , 
 is a proxy with historian  and
;
otherwise .
\end{proof}

\noindent
's operations are implemented as follows:

\medskip\noindent
:
To execute 
for , take
 and .
Thus  is the th element of the
th segment .
Use the algorithm of
Proposition~\ref{prop:semi} to compute
 and the related information.
If  is empty, return~0.
Otherwise determine whether 
(stored in ) is
in the -free representation for some
 and, if so, for which 
(Lemma~\ref{lem:word}(b)).
Using the information just computed to
consult ,
return .

\medskip\noindent
:
To execute 
for  and ,
initially proceed similarly as in
the case of \Tvn{color}:
Take  and 
and use the algorithm of
Proposition~\ref{prop:semi} to compute
 and the related information.
If  is empty,  and ,
return .
If  is nonempty, 
use  (stored in ) to
compute 
and, if , return .

If no value was returned until this point
(the answer could not be established locally
in the th segment),
again traverse the path  in  from  to .
With  equal to the set of right siblings 
of inner nodes on  
such that the st bit is set in 's spectrum,
determine whether
 and, if so, return~0.
Otherwise proceed as follows:
Compute the node~
in  that follows  most closely in
a preorder traversal of~
(thus  is the closest right sibling in 
of the node on  of maximum depth among
those with right siblings in~).
Carry out a partial descent in  that starts
at  and uses the selection rule
``color-seeking'' throughout.
Let  be the leaf reached and
use the algorithm of
Proposition~\ref{prop:semi} to compute 
and the related information.
If  is empty, return 
(we must have ).
Otherwise use  (stored in ) to return
.

\medskip\noindent
:
Let us use the terms ``old'' and ``new'' to refer
to states before and after the update
under consideration, respectively.
To execute 
for  and ,
once more take
 and .
Find the old color  of ,
determine whether 
after the update and use this
to compute the new spectrum of .
Save the old root bits and
traverse the path  in  from  to , collecting
the concatenation  of the navigation vectors
of all inner nodes on .
Use  to traverse  backwards
and update the histories of all old top nodes
encountered and the root bits to reflect the change,
if any, in the spectrum of~.
Since the spectrum of every inner node
in  is a simple function of those of
its children, this is a straightforward
bottom-up computation.
Also explicitly compute the concatenation

of the
new navigation vectors of the inner nodes on~.
Guided by  and , we can traverse 
in the forward and backward directions
in constant time per node visited, always
knowing both the old and the new navigation vector
of the current node, even though the
histories stored in  may be temporarily
inconsistent during the update.
What remains is to actually record the new
color of  and to modify the light paths
implicit in the values in  accordingly.
After describing a procedure for achieving this,
we will argue that whenever the procedure needs the
(old and new) navigation vector of a node
outside of , the navigation vector can
be obtained from the history stored in
a word that has not (yet) been modified in the
course of the update.

We consider three cases.
In Case~1 the set of light paths does not change.
In Case~2 the update creates a new light path,
which may shorten a single existing light path.
In Case~3 the update destroys
a light path, which may
lengthen a single existing light path.
The update either leaves invariant the status of
every node in  with respect to being
empty, light or full,
or it causes the same transition from light
to empty or full or from empty or full
to light at all nodes
on a last (bottom) part of , while the
other nodes on  remain light and no other
node changes its status.

\medskip
\emph{Case 1}:

is true after the update
if and only if it was true before the update.\\
If the update changes 's status with respect to
being empty, light or full, we must have one of
two situations:
Either  even when  is light,
in which case  has at least one
light leaf as a left sibling,
or  even when  is not light,
in which case the status of  switches between
light and empty and the light path that ends at
 when  is empty coincides with the
light path that ends at  when  is light
(informally, the switch to light of some nodes on the
path but not its first node---which
is always light---only makes
those nodes ``more preferred children'').
It is now easy to see that the update does
not change the set of light paths.

Use the algorithm of Proposition~\ref{prop:semi}
to compute  and the related information.
If  is a proxy before and therefore also after
the update, convert 
(found in ) to the standard representation
after saving the history stored in its free bits
in a temporary variable, then execute
, and finally reconvert
 to the compact representation
(which may be the -free representation for
a different ) and
store the history saved in its free bits.
If  was empty but not a proxy before the update,
let  be a newly initialized leaf dictionary, execute
 and store  in .
In the remaining case only execute
.

\medskip
\emph{Case 2}:
 holds
after the update, but not before it.\\
In this case  is the last node of
a new light path~.
Again traverse  backwards to find the
first node  on  (i.e., the node on
 of maximum height) that did not belong to
a light path before the update.
The update changes the status of  and~
from empty or full to light.
Moreover, after the update every proper descendant of 
on  is the only light child of its parent
and therefore its preferred child.

If  has at least one light left sibling
or is the root  of ,
 is not a preferred child even
after the update and so
must be an inner node in ,
i.e., we cannot have  (and
nontheless be in Case~2).
Then  is the top node of ,
 is its proxy,
and the leftmost leaf descendant 
of  is the historian of~ 
(see Fig.~\ref{fig:setcolor}(a)).
Since neither  nor  was the leftmost
leaf descendant
of a light node in  or
belonged to a light path before
the update, neither was a proxy or a
historian before the update.
Thus  changes into a permutation 
of  that coincides with , except that
 and
.
Accordingly carry out the following steps:
If  was empty before the update,
let  be a newly initialized leaf dictionary.
Subsequently, whether or not  was empty,
execute 
and convert  to the compact representation.
Then store  together with
the new history of , which
is a suffix of~, in 
while
saving the old value of  in  if .

\begin{figure}
\begin{center}
\epsffile{setcolor.eps}
\end{center}
\caption{(a) A new light path  (from  to )
is added without changes to the existing light paths.
(b) A new light path (from  to )
grabs an initial part of an old light path (from  to ).}
\label{fig:setcolor}
\end{figure}

If  has no light left sibling and is not ,
the situation is more complicated
(see Fig.~\ref{fig:setcolor}(b)).
This is because the update switches the
preferred child of the parent of 
from some sibling  of  to~.
Carry out a partial descent in ,
starting at  and with the
selection rule ``proxy-seeking'', to find the
end node  of the old light path through 
and let  be the leftmost leaf descendant of~.
Also ascend in  from  until finding
the last (deepest) node  on 
that was a top node before the update.
After the update  is still a top node,
but with proxy  instead of its old proxy .
Assume first that  is a nonempty inner node in .
Then  is a right sibling of  and a new top node with
proxy  and historian~.
Let  be the historian of 
(which does not change as a result of the update).
Before the update,  was neither a proxy nor a
historian unless ,
and  was neither a proxy nor a historian
unless .
The update therefore changes  into the permutation
 of  that coincides with , except that

\medskip

\centerline{\vbox{\tabskip=1em\halign{\hfil&#&\hfil\hfil&#&\hfil&
 \hskip 2em#\hfil\cr
\pi(h)&&h&&\pi'(p)&(only if )\cr
\pi(p)&&h_u&&\pi'(i)\cr
\pi(i)&&i&&\pi'(h_u)&(only if )\cr
\pi(h_u)&&p&&\pi'(h)\rlap{.}\cr
}}}

\medskip
\noindent
Accordingly,
move the old value of  to  (superfluous if ),
move the old value of  to 
(superfluous if ),
let  be a newly initialized leaf dictionary
if  was empty before the update
and otherwise obtain  from the old value of ,
execute ,
store the compact representation of 
together with the new history of 
(which is a suffix of ) in ,
and finally move the old value of  to .
The latter value contains  together with the
old history of .
The new history of  is a proper suffix of its
old history, but storing the latter does no harm
(the extra bits are considered unused anyway).

If  is empty, the steps executed are the same,
except that we refrain from moving
the old value of  to 
(if  the appropriate value was already stored in ,
and if 
the new value of  can be arbitrary).
Finally, if  is a nonempty leaf,
the procedure is also the same, except that
at the end,
since  stops being a proxy, 
(stored in ) must be converted to the standard representation.
Its free bits contain no relevant information.

\medskip
\emph{Case 3}:
 holds before the update, but not after it.\\
This case essentially entails undoing the steps
described for the previous case.
Traverse  backwards to find the first node~
on  that ceases to belong to  as a result of the update.
The update changes the status of  and 
from light to empty or full.

If  was a top node before the update,
 was its proxy, and neither  nor the
old historian  of  is a proxy or a
historian after the update.
Thus  changes into the permutation
 of  that coincides with ,
except that
 and
.
Accordingly obtain  from ,
convert it to the standard representation,
execute 
and store  in  while
saving the old value of  in  if .

If  was not a top node before the update,
let  be the preferred child of the parent of
 after the update and let 
be the leftmost leaf descendant of~.
If  is empty take , and otherwise let
 be the leaf reached by
a partial descent in  that starts at 
and uses the selection rule ``proxy-seeking''.
Assume first that  is a nonempty inner node in~.
Then  was a top node with proxy 
and historian  before the update.
Ascend in  from  to find the last node 
on  that was a top node before the update.
After the update  is still a top node, but
with proxy  instead of its old proxy~,
and  is not a top node.
Let  be the historian of 
(which does not change as a result of the update).
After the update,  is neither a proxy nor a historian
unless , and  is neither a proxy nor
a historian unless .
Therefore the update changes  into the
permutation  of  that coincides
with , except that

\medskip

\centerline{\vbox{\tabskip=1em\halign{\hfil&#&\hfil\hfil&#&\hfil&
 \hskip 2em#\hfil\cr
\pi(h)&&p&&\pi'(h_u)\cr
\pi(h_u)&&i&&\pi'(i)&(only if )\cr
\pi(i)&&h_u&&\pi'(p)\cr
\pi(p)&&h&&\pi'(h)&(only if ).\cr
}}}

\medskip
\noindent
Accordingly, 
move the old value of  to 
after prefixing the history stored
in  by the subsequence of  that pertains to
the part of  from  to the parent of ,
convert  (found in the old value of )
to the standard representation,
execute 
and store  in .
Finally move the old value of  to  if 
and move the old value of  to~
if .

If  is empty, the steps executed are the same,
except that in place of the old value of 
(which can be arbitrary) we use a newly initialized
leaf dictionary, converted to the compact representation
and equipped with a history that shows every node
as being empty.
If  is a nonempty leaf, the procedure is also the same, except that
since  was not a proxy before the update, the old value of ,
before being moved to , must be converted
to the compact representation and equipped
with a history equal to (the empty sequence prefixed by)
the subsequence of  that pertains to
the part of  from  to the parent of .

Observe that within each of Cases 1--3, all reading
from some of  can take place before
all writing to some of the same words.
Therefore the only possible source of inconsistency
in the data read
is the update of histories of nodes on~
carried out before the computation
splits into Cases 1--3.
Most of the update conceptually happens on the
path , on which we can navigate using
 and .
The only occasion on which we need to navigate
outside of  is during the partial descent
from  that takes place in Cases 2 and~3.
But if  is empty, the descent is trivial
and needs no inspection of histories, and if
 is light, it is necessarily to the
right of , which implies that every history
inspected during the descent from  is stored
strictly to the right of every history of a
node on~, and thus of every leaf
whose associated word might already have changed.

\medskip

Every operation of  inspects or changes
 parts of histories stored in leaf
dictionaries, which takes  time.
In addition to this, 
calls \Tvn{color} once in a leaf dictionary
and  calls \Tvn{successor}
at most twice in a leaf dictionary.
Therefore these operations execute in 
and  time, respectively.
 carries out a constant number
of conversions between standard and
compact representations in
leaf dictionaries and therefore
altogether executes in  time.
If tables are available that allow the
leaf dictionaries to execute \Tvn{color}
and \Tvn{successor} in constant time,
's operations \Tvn{color} and \Tvn{successor}
work in  time (whereas the
time bound for \Tvn{setcolor} does not change).
This ends the proof of Lemma~\ref{lem:smalltree}.
\end{proof}

\begin{lemma}
\label{lem:unsystematic-tf}There is a choice dictionary that, for
arbitrary ,
can be initialized for universe size~,
 colors and tradeoff parameter 
in constant time and that subsequently occupies
 bits
and supports \Tvn{color} in  time and
\Tvn{setcolor},
\Tvn{choice} and, given  additional bits,
robust iteration in  time.

Alternatively, for arbitrary fixed ,
if given access to tables of
 bits that can be computed in
 time and
depend only on~, the data structure supports
\Tvn{color}, \Tvn{choice} and,
given  additional bits,
robust iteration
in  time.
\end{lemma}

\begin{proof}
A data structure composed of  instances of
the data structure of Theorem~\ref{thm:systematic-2},
one for each color, can support each
operation in constant time.
Assume therefore without loss of generality that
.
A word RAM with a word length of  bits
can simulate one with a word length of  bits
with constant slowdown.
This allows us to assume not only that
, but that  is a multiple of 
and that
the available
word length is in fact .
Therefore define
 and 
and let , in accordance
with the conventions used in this
section until this point.
Define .
If , the result follows directly from
Lemma~\ref{lem:smalltree}.
The latter assumes the universe size to be
exactly , but a tree with fewer than
 leaves
can be accommodated with straightforward
changes---essentially, each node should
adapt to
the number of its children---and
an ``incomplete leaf'',
one whose universe size is smaller than~,
can be handled separately with
Lemma~\ref{lem:atomic-c}.

If , we employ the trie-combination method of
Section~\ref{sec:trie} with
the degree sequence ,
so that the overall trie is of height~2.
The 2-color choice dictionaries
of (the root of) the upper trie of height~1,
one for each of the  colors and one
to keep track of initialization, are
instances of the data structure of
Theorem~\ref{thm:systematic-2}.
The number of bits needed
for these choice dictionaries is
.

The choice dictionaries of (the roots of) the
lower tries, also of height~1,
are instances of the data
structure of Lemma~\ref{lem:smalltree}.
The same simple changes as above to reduce the
universe size yield a data structure
suitable for use at the rightmost lower trie.
Each instance uses 
a number of bits equal to  plus
 times its universe size, so the total
number of bits needed by all instances is .
This shows the space bounds of the theorem.
The time bounds follow from those of
Lemma~\ref{lem:smalltree}.
\end{proof}

\begin{theorem}
\label{thm:unsystematic-f}For every fixed ,
there is a choice dictionary that, for
arbitrary ,
can be initialized for universe size~,
 colors and tradeoff parameter 
in constant time and subsequently occupies
 bits
and supports
\Tvn{setcolor} in  time and
\Tvn{color}, \Tvn{choice} and,
given  additional bits,
robust iteration in  time.
In particular, if ,
there is a data structure with the functionality
indicated that occupies
 bits.
\end{theorem}

\begin{proof}
We essentially use the data structure of
Lemma~\ref{lem:unsystematic-tf}, but
incorporate its tables 
into the data structure itself.
We need the tables exclusively to speed
up the operations \Tvn{color} and \Tvn{successor}
of the data structure of Lemma~\ref{lem:j-free}
from  time to constant time.
The only part of the realization of these two
operations described in the proof of
Lemma~\ref{lem:j-free} that needs more than
constant time without the use of
tables is a constant number of conversions
from the compact to the -intermediate representation
within \emph{blocks}
of a certain number (approximately
) of small groups.
Let  be the table, introduced in the proof of
Lemma~\ref{lem:j-free}, that realizes this conversion. We must prove
that  can
be computed in a lazy manner so that all
entries ever inspected have the correct value.
But this is easy:
Whenever a new block arises, it does
so in an execution of \Tvn{setcolor}, and the
time bound of \Tvn{setcolor} of
the present theorem allows for the
-time conversion of the block,
as described in the proof of Lemma~\ref{lem:j-free},
after which the relevant table entry can be filled in.
In the very first call of ,
we also use  initial steps to compute
the size of the table, so that the table can
be allocated before the other parts of the
data structure.
\end{proof}

Theorem~\ref{thm:unsystematic-f} can be
used to improve the lower-order terms of
Theorem~\ref{thm:systematic-2}.
Replacing all choice dictionaries at nodes of
height at least~2 in the construction of the proof of
Theorem~\ref{thm:systematic-2} by a single
instance of the choice dictionary of
Theorem~\ref{thm:unsystematic-f} with ,
we obtain a bound of
 bits.

Lemma \ref{lem:unsystematic-tf}
and Theorem~\ref{thm:unsystematic-f}
deal with the case in which the number  of colors
is a power of~2.
We now turn to the case of general values of 
and first provide an analogue of
Lemma~\ref{lem:j-free}.

\begin{lemma}
\label{lem:j-free-c}Let ,  and  be positive integers
with  and 
and assume that  is a multiple of .
Then there is a choice dictionary  with universe size
 and for  colors that can be initialized
in constant time and subsequently,
for integers ,  and  with ,

and , stores its
state as an element of

and, if given access to tables of
 bits that can be computed in
 time and depend only on ,  and ,
where ,
executes
\Tvn{color}, \Tvn{setcolor} and
\Tvn{successor} in  time.
Moreover, during periods in which
, where 
is 's client vector and ,
 supports -time conversion to and from
a -free representation in which  can
store  unrelated bits, spaced apart by
gaps of  bits, that can be
read and written together in  time.
\end{lemma}

\begin{proof}
Write  as  for integers
,  and  with  and
, which is clearly
possible (e.g., take  and
choose  as large as possible).
 operates with three types of representations.
In the \emph{standard representation}, the
 digits, each drawn from ,
are partitioned into \emph{segments} of consecutive
digits,  segments of  digits each and a
final segment of  digits,
the digits within each segment are represented through an
integer in  for the
 first segments and in
 for the final segment,
and  stores the resulting -tuple of integers.
In the \emph{compact representation}, i.e., the
-free representation for some
,
the digits are
partitioned into \emph{small groups} of
 consecutive digits each, and the digits
in each small group are represented through
an integer in .
Since , each small group can
be stored in a field of
 bits, with one bit
in the field left unused.
The summary bits of the proof of
Lemma~\ref{lem:j-free} are not needed
in the present data structure.
 stores for each
segment the integer whose binary representation is
the concatenation of the bit sequences in the fields
of the small groups in the segment.
In the \emph{loose representation}, finally,
each of the  digits is stored in
 bits, and the entire sequence
of  digits occupies
 bits.

Conversion between the standard and compact
representations is carried out, segment by
segment, via the loose
representation and with the aid of tables.
This takes constant time per segment
and  time altogether.
As argued in the proof of Lemma~\ref{lem:j-free},
when  is in the loose representation
and ,
the functions  and 
can be applied to all digits in constant time
using word parallelism.
Because of this, the tables that map to and from
the -free representation can be made independent of~.
As a consequence, the largest conversion tables
have at most 
entries of  bits each,
so the tables are of total size
 bits and can be computed in  time.

When  is in the loose representation, it
can execute \Tvn{color}, \Tvn{setcolor} and
\Tvn{successor} in constant time as shown
in the proof of Lemma~\ref{lem:j-free}.
Since each operation requires at most two
conversions between representations,
it can be carried out in  time.
There is one unused bit for every
small group, i.e., for every  digits,
yielding a total of  unused bits,
and within each segment the unused bits are spaced
apart by gaps of  bits.
The unused bits can be read or written in
constant time per segment,
i.e., in  time altogether.
\end{proof}

Substituting Lemma~\ref{lem:j-free-c} for
Lemma~\ref{lem:j-free},
we can prove the following analogue
of Lemma~\ref{lem:unsystematic-tf}:

\begin{lemma}
\label{lem:unsystematic-tc}For every fixed ,
there is a choice dictionary that, for
arbitrary  with ,
can be initialized for universe size~,
 colors and tradeoff parameters  and 
in constant time and subsequently occupies

bits and,
if given access to tables of
 bits
that can be computed in 
time and depend only on  and ,
supports \Tvn{color}, \Tvn{setcolor},
\Tvn{choice} and,
given  additional bits,
robust iteration in  time.
\end{lemma}

\begin{proof}
Without loss of generality assume that
 and, since an arbitrary increase
of  by at most a constant factor can be
``compensated for'' by a corresponding
decrease in , that  is a multiple of
.

We use a similar construction as in the
proof of Lemma~\ref{lem:unsystematic-tf}.
Instead of storing  digits to base~
in a -bit word maintained
in an instance of the data structure of
Lemma~\ref{lem:j-free}, however, we now
store  digits to base~ in an
instance of the data structure of
Lemma~\ref{lem:j-free-c}, 
called a \emph{leaf dictionary}
and
initialized with
 chosen as an integer
constant larger than .
This choice of  ensures that the tables used
by the data structure of Lemma~\ref{lem:j-free-c}
are of
 bits,
for  larger than a constant,
and can be computed in  time.

The integers that constitute the states of all
leaf dictionaries---one for each segment---are stored in an instance
of the data structure of
Lemma~\ref{lem:succincter-t}, initialized with .
This needs  bits, requires a
table of  bits and
allows us to read and write states of
leaf dictionaries in constant time.
As a result, a leaf dictionary can execute
every operation in constant time.
The data structure of
Lemma~\ref{lem:j-free-c} may employ segments of two
different sizes (namely  and ),
which, in the context of Lemma~\ref{lem:succincter-t},
translates into a sequence 
that contains two different values.
Because Lemma~\ref{lem:succincter-t} tolerates only
a constant number of \emph{changes}, i.e.,
positions  with
, we present the values of
segments to the data structure in an order
that ensures that  has
at most one change.

Take  and .
Similarly as in the proof of Lemma~\ref{lem:unsystematic-tf},
the  color values are stored in
 complete -ary trees
of depth , ``surmounted'' by  instances
of the choice dictionary of
Theorem~\ref{thm:systematic-2} that need
 bits,
and possibly one incomplete tree that can
be dealt with as indicated
in the proof of
Lemma~\ref{lem:unsystematic-tf}.
If the rightmost leaf is ``incomplete'',
we consider it not to belong to any of the trees.
Instead we handle the associated color values separately,
storing them as up to  integers in an  
``incomplete standard representation''
that are converted to a loose representation
whenever we need to operate on them.

The crucial inequality
 shows that the free storage
offered by a leaf dictionary in the compact representation
is sufficient to hold the history of a leaf.
Using the same algorithms as in the proof of
Lemma~\ref{lem:smalltree}, we can therefore
execute \Tvn{color}, \Tvn{setcolor}, \Tvn{choice}
and robust iteration in  time.
\end{proof}

\Fpasteinsection{\unsystematic}

\begin{proof}
We use two data structures,  and ,
that interact in a way described in greater
detail in the proof of Theorem~\ref{thm:succincter}.
The first  operations are served by ,
while an interleaved background process computes
certain quantities needed by~.
After  operations 
is ready,
and during the next  operations 
and  work in parallel while a background
process gradually transfers the elements in
 of nonzero color to .
After  operations  is dropped.

 is an instance of the
choice dictionary of Theorem~\ref{thm:m-c}.
Since it is used only
during the first  operations,
it fits in  bits,
a negligible quantity in the present context.

Assume without loss of generality that
 (we could even assume ).
The second data structure, , is closely
related to that of Lemma~\ref{lem:unsystematic-tc},
initialized
with  and
with  chosen
as an integer with
,
so that .
We incorporate the tables used by the choice
dictionary of the lemma
into  itself.
What remains is essentially to show
how to compute the tables sufficiently fast.

The preprocessing for  serves to obtain
 and, with it,
the quantity 
used by the data structure of
Lemma~\ref{lem:j-free-c},
as well as , needed
to estimate the size of its tables in
preparation for their allocation.

The tables employed by the data structure of
Lemma~\ref{lem:j-free-c} are used to convert segments
in the loose representation to and from the standard and
compact representations.
Since all computation takes place on segments
in the loose representation, the two other representations can
in fact be arbitrary encodings of segments,
except that they should fit in the available space.
We can therefore deal with both the standard and
the compact representation as described in the
proof of Theorem~\ref{thm:p} in the case of the
tables  and , i.e., hand
out the codes  in that order and
compute the tables in a lazy fashion.

Finally, as concerns the data structure of
Lemma~\ref{lem:succincter-t}, we can simply replace
it by the data structure of Theorem~\ref{thm:succincter},
which uses no external tables.
The sequence 
of the previous proof is -balanced
for some fixed 
because either all segments are of the same size
or the larger segments are at least as
many as the smaller segments.
For ,  is either 
or , where  and  are
defined in the proof of Lemma~\ref{lem:j-free-c}.
Since here we have
, the sequence 
can be communicated to the data structure
of Theorem~\ref{thm:succincter} as a sequence
of the form ,
where  and each of 
is either  or 
(the computation of  was considered above).
\end{proof}

\section{Applications of Choice Dictionaries}

When considering algorithmic problems, we assume
that the input is provided in read-only memory and
the output is sent to write-only memory
and count only the bits of working memory used.
When the input includes a graph ,
we make the standard assumption that .

For all integers
 and  with , a \emph{-permutation}
of  is
a sequence of  pairwise distinct elements
of .

\begin{theorem}
For all fixed  and
for arbitrary  with ,
a -permutation of 
can be drawn uniformly at
random from the set of all -permutations
of 
and output in  time using
 bits of
working memory.
\end{theorem}

\begin{proof}
Initialize a 2-color instance of the
choice dictionary of Theorem~\ref{thm:p}
for universe size ,
call its client set  and,
 times, draw an element
uniformly at random from ,
output it and insert it in~.
\end{proof}

\begin{corollary}
For all fixed  and
for arbitrary , a
permutation of 
can be drawn uniformly at random
from the set of all permutations of 
and output
in  time using
 bits of
working memory.
\end{corollary}

Simple as the algorithm of the corollary is,
we can prove that it is close to using
the minimum possible amount of working memory.
Assume that for all ,  is
a finite language of binary strings such that
no string in  is a proper prefix of
another string in  ( is \emph{prefix-free})
and  is a function from  to
.
For the lower bound below, we relax the requirements
for what it means to output a permutation
 of .
Rather than demanding that the output be
a sequence of exactly  -bit integers,
the th of which is , for ,
we allow the output to be a sequence
 of a possibly variable
number of bit strings of possibly variable lengths
such that
the concatenation  can be written
in the form , where 
and  for .
For example, this allows several values of 
to be output in the same word or a nonstandard
representation of integers to be used.

\begin{theorem}
Let  be a randomized algorithm that
outputs a permutation
of  for some ,
with each of the  such
permutations being output with positive
probability.
Then  uses at least  bits
of working memory.
\end{theorem}

\begin{proof}
Suppose that the output of  is a
sequence  and
write 
as discussed above.
Let  be minimal
with 
and consider the point in time just
before  is output.
Now 
determines ,
where  is the permutation computed
by~.
There are
 possibilities
for the set ,
each of which occurs with positive probability,
and  cannot be the same for
any two distinct such possibilities.
Therefore, at the point
in time under consideration there must be
at least  possibilities
for the state of .
Since 
for 
and ,
,
which implies that
the number of bits used by 
is at least
.
\end{proof}

Given a directed or undirected
-vertex graph 
and a permutation 
 of , i.e., a
bijection from  to~,
we define a \emph{spanning forest} of 
\emph{consistent with}  to be a sequence
, where 
are vertex-disjoint outtrees that are
subtrees of 
(if  is directed) or of the directed version of~
(if  is undirected) and the union of whose vertex sets
is , such that for each , the root of
the tree in  that contains~
is the first vertex in the sequence
 from which  is reachable in~.
If, in addition, every path in the union
of 
is a shortest path in ,  is a
\emph{shortest-path spanning forest} of 
consistent with~.
Thus a spanning forest of  consistent with~
can be produced, e.g., by a depth-first search that,
whenever its stack of partially processed vertices
is empty, picks its new start vertex
as the first undiscovered vertex in the order
prescribed by~.
If a breadth-first search is used instead of the
depth-first search, the result will be a
shortest-path spanning forest of~
consistent with~.

In the following, by computing a spanning forest

of an -vertex graph  consistent with
a permutation  of  we will mean producing a
sequence \break of
triples with , 
and  for  such that

and such that for ,
 and  and
,
 and  
are precisely the vertex and edge sets of~,
respectively.
If, in addition, for each 
with  there is an 
with , we say that  is computed in
\emph{top-down order}.
Thus for , the root and
the edges of  are to
be output (in a top-down order),
each with the index  of its tree~.
The meaning of a shortest-path spanning forest

of  consistent with  (in top-down order)
is analogous,
except that each triple 
is extended by a
fourth component equal to the depth of 
in .
Of course, computing a spanning forest of an
undirected graph also solves the
(suitably defined) connected-components
problem.

\begin{theorem}
\label{thm:cc}Given a directed or undirected graph 
with  vertices and  edges,
a permutation  of  and a ,
a spanning forest of  consistent with 
can be computed in
top-down order in
 time with
 bits of working memory.
In particular, for every fixed ,
a spanning forest of  consistent with 
can be computed in  time
with at most  bits.
\end{theorem}

\begin{proof}
We use  bits to mark each vertex as
\Tvn{unvisited} or \Tvn{visited}.
Initially all vertices are unvisited.
In addition, we store an
initially empty set  of vertices in
an instance  of the data structure
of Theorem~\ref{thm:mlog}.
We also maintain a current tree index ,
initially~0.

Compute  with

such that when ,  occupies
at most  bits.
We will ensure that  always holds,
so the space used by the algorithm
is as stated in the theorem.

In an outermost loop, we step through  in the
order indicated by , i.e.,
in the order , and,
for each vertex  found to be unvisited at this time,
increment~
(a new tree  is begun), mark  as
visited, output
 ( is the root of 
and has no parent),
and insert  in .
Then, as long as , we use
\Tvn{extract-choice} to delete a vertex 
from  and \emph{process} .
Processing  means, for each unvisited (out)neighbor  of~,
marking  as visited,
outputting 
( belongs to  and its parent is ), and
inserting~ in~.
If and immediately when  reaches , we abandon what
we are doing and start a \emph{global sweep}.
A global sweep reinitializes  to reset
 to 
(or achieves the same though a sequence of
calls of \Tvn{extract-choice})
and iterates over , processing each visited vertex encountered.
If  reaches  during a global sweep,
the current global sweep is abandoned, and a new
global sweep is immediately begun.
Whenever  becomes empty outside of a global sweep,
the current iteration of the outermost loop
terminates (no more vertices are reachable from ).

The algorithm is easily seen to be correct.
In particular, as long as  contains an edge 
or  such that  is visited but  is not,
 belongs to  or will eventually be processed
in a global sweep.
Outside of global sweeps, the running time of
the algorithm is .
Between any two global sweeps, at least  vertices
are marked as visited.
Since this happens only once for each vertex,
the number of global sweeps is bounded by
.
A global sweep runs in  time, so the
total running time of the algorithm is
.
\end{proof}

\begin{theorem}
\label{thm:bfs}Given a directed or undirected graph 
with  vertices and  edges,
a permutation  of , a 
and a fixed ,
a shortest-path
spanning forest of  consistent with 
can be computed in
top-down order in  time with
 bits of working memory.
If  is directed, its representation must
allow iteration over the inneighbors and
outneighbors of a given
vertex in time proportional to their number
plus a constant
(in the terminology
of~\cite{ElmHK15},
 must be given with in/out adjacency lists).
\end{theorem}

\begin{proof}
Using a 3-color instance of the choice dictionary
of Theorem~\ref{thm:unsystematic-c},
we store for each vertex  a color:\
white, gray or black.
Initially all vertices are white.
We also store a current tree index, ,
initially 0, and a current distance counter, .
In the following, the prefixes ``(in)'' and ``(out)''
are intended to apply if  is directed;
if  is undirected, they should be ignored.

In an outermost loop, we step through  in
the order indicated by  and,
for each vertex  found to be white at this time,
increment~
(a new tree  is begun), set  to 0,
color  gray and output
 ( is the root  of , it
has no parent and its depth in  is~).
We also remember 
as the root
of the current tree.
Then, as long as at least one vertex is gray,
we carry out an \emph{exploration round}
followed by a \emph{consolidation round}
and then increment~.

In the exploration round, we
iterate over the gray vertices.
For each gray vertex , we test whether 
 
or
 has one or more black (in)neighbors.
If this is the case, we
process all white (out)neighbors of ,
for each such vertex 
coloring  gray and outputting 
( belongs to , its parent is 
and its depth in  is ).
In the consolidation round, we again iterate
over the gray vertices, now coloring black each gray vertex
without white (out)neighbors.
If there are no gray vertices after a
consolidation round,
the current iteration of the outermost loop
terminates (no more vertices are reachable from ).

Consider a particular value of  and
let  be the set of vertices 
reachable in  from 
but not from any vertex before  in the sequence

(i.e.,
 is the intended vertex set of the final tree ).
The following can be proved by induction on :
Immediately before an exploration round,
suppose that 
and let  be the length of a shortest path
in  from  to .
Then  is black if ,
white if ,
gray or black if  and  has no
white (out)neighbors, and
gray if  and  has at least one
white (out)neighbor.
The exploration round may enumerate some vertices
at distance  from  that were colored
gray earlier in the same round, but
the test ``is gray and either equals  or
has a black (in)neighbor''
employed in the exploration round
is passed precisely by the vertices that were
gray at the beginning of the
round.
It is now easy to see that the tuples output
by the exploration round are correct and that,
immediately after the exploration round,
 is (still) black if , gray
or black if , gray
if  and white if .
The test ``is gray and has no white (out)neighbor'' employed
in the consolidation round is
passed precisely by those vertices in 
that either have distance  from  and
are not already black or have distance  from 
and no white (out)neighbors.
Thus the induction hypothesis holds at the
beginning of the next exploration round, if any.

Since each vertex is gray for at most two
values of , i.e., for at most two
exploration rounds and two consolidation rounds,
the running time of the algorithm is readily seen
to be .
The space bound follows from
Theorem~\ref{thm:unsystematic-c}.
\end{proof}

The vertex set  of
a maximal clique in an undirected graph  can be computed
greedily by starting with  and
stepping through the vertices of~, including 
each in  if it is adjacent to all vertices already in~.
With  and , this takes  time
and uses  bits.
Below we present an output-sensitive algorithm that is potentially
faster and uses only slightly more space.
For , let  be the neighborhood of~,
i.e., .
Moreover, for , denote by  the
total degree of the vertices in ,
i.e., .

\begin{theorem}
\label{thm:clique}Given an undirected -vertex graph ,
a  and a fixed ,
the vertex set  of a maximal clique in 
can be computed in  time
with  bits of working memory.
If the adjacency lists of  are sorted, i.e.,
each lists the neighbors of a vertex in sorted order,
the problem can be solved in  time with
 bits of working memory.
\end{theorem}

\begin{proof}
We use the following algorithm:
Output the vertex 1 and initialize a set
 to .
Then, as long as  is nonempty,
output an element  of  and replace
 by .
The correctness of the algorithm
is obvious--- is always the set of neighbors common to all vertices that
were already output.

Without loss of generality assume in the
rest of the proof that .
We store  as the elements of
color~1 in a 3-color
instance of the choice dictionary
of Theorem~\ref{thm:unsystematic-c}.
To replace  by ,
temporarily color the elements of 
with color~2 in a scan over 
and subsequently replace
first the color 1 by 0 and then the
color 2 by 1 at all vertices.
Following the initialization,
the time needed is  times the sum of

and the number of color decrements.
Since the number of color decrements is bounded by
the number of color increments,
which is at most ,
the total running time is
.

Assume now that the adjacency lists of  are sorted.
Then, as long as , we store 
differently, namely through its bit-vector representation.
We can find a vertex  in  in  time,
and to replace  by , we step
through the (sorted) adjacency list of 
and the bit-vector representation of  in parallel,
clearing all bits in the latter that do not
correspond to neighbors of .
This can be done in  time.
Since the procedure is carried out at most once
with , the total time used
until  is .

When  has dropped below , we spend 
time to extract  from the bit vector and store the
set in a colorless instance  of the choice dictionary
of Theorem~\ref{thm:unsystematic-f}.
To replace  by , we scan
over  and store  in an instance 
of the data structure of Theorem~\ref{thm:mlog},
after which we empty , extract all elements
stored in  and insert them in~.
The time spent in this part of the algorithm
is obviously .
Since  never contains more than  elements,
it occupies  bits.
The space bound follows.
\end{proof}

\bibliography{all}

\end{document}
