\setcounter{section}{0}
\renewcommand\thesection{\Alph{section}}


\section{ROADMAP model}
\label{sec:sup_roadmap}

\subsection{Properties of SupAP \& comparison to SmoothAP}

We further discuss and give additional explanations of the property of our  loss function, and especially its comparison with respect to the SmoothAP \cite{smoothap} baseline. 

As shown in Fig. 1.a of the main paper, and discussed in Section 3.1 ("Comparison to SmoothAP"), 
the smooth rank approximation in \cite{smoothap} has several drawbacks, that we show below: 
 \begin{figure*}[!hb]
     \centering
     \includegraphics[scale=.55]{illustration/supplementary/point1_expl.pdf}
     \caption{Limitation of the smooth rank approximation in~\cite{smoothap}: contradictory gradient flow for the positives samples  and  (in green), vanishing gradient for the negative example  (in red), and no guarantees of having an upper bound of .}
     \label{fig:comparison_smoothap_sup}
 \end{figure*}

Specifically, we explain in more detail the following three limitations identified in the main paper for SmoothAP \cite{smoothap}, ~which comes from the use of the sigmoid function to approximate the Heaviside (step) function for computing the rank: 


\begin{enumerate}[i]
    
    \item \textbf{Contradictory gradient flow for positives samples:} Firstly we can see on the toy dataset of \cref{fig:comparison_smoothap_sup}~that the gradients of the two positive examples (in green) with SmoothAP have opposite directions. The positive with the lowest rank  has a gradient in the good direction, since it leads to increase 's score because the correct ordering is not reached (the negative instance  has a better rank). But the gradient of the positive with the highest rank  is on the wrong direction, since it tends to decrease 's score. This is an undesirable behaviour, which comes from the use of the sigmoid in . In the example of \cref{fig:comparison_smoothap_sup}, we can actually show that 
    
    
    
    
    
    
    To see this we write :


Because , we have  and  in the example of \cref{fig:comparison_smoothap_sup}, because  and  falls into the saturation regime of the sigmoid. We get a similar result for the derivative of  wrt.  :
    


Furthermore we have :




Indeed , such that  and . Similarly the derivatives of  wrt.  and  also have opposite signs: . It concludes the proof that .
    
    
    \item \textbf{Vanishing gradients:} Secondly, SmoothAP \cite{smoothap} has vanishing gradients due to its use of the sigmoid function. This is illustrated on the toy dataset in \cref{fig:comparison_smoothap_sup}. The negative instance  has a high score , but does not receive any gradient, which does not enable it to lower its score although it would improve the overall ranking. This is because the score difference between  and  is large, \ie . Similarly, . Consequently, both  and  fall into the saturation regime of the sigmoid, preventing to propagate any gradient (see Fig. 3c. in the main paper).

    \item  \textbf{Finally,  is not an upper bound of }. The use of the sigmoid means that both  and  can be over or under estimated. If  is overestimated (resp. underestimated)  underestimates  (resp. overestimates). And if  is overestimated (resp. underestimated)  overestimates  (resp. overestimated). Therefore,  can be larger or lower than  in general. In the example of \cref{fig:comparison_smoothap_sup}, we show that  is lower than .
    
\end{enumerate}





\textbf{\underline{We address those three issues with :}}
\begin{enumerate}[i]
\item \textbf{Using the the true Heaviside (step) function  for } allows to have the expected behaviour regarding the gradients of positives. When Changing  for  in \cref{fig:step_smoothap_sup}, we can see that we fix the problem of opposite gradients for the positive examples  and  - although the gradient is zero.


\item \textbf{Using  for  overcomes vanishing gradients}.~By using  in Eq. (4) in submission, we design a linear function for positive  values, where  (resp. ) is the score of a negative (resp. positive) example - see Fig. 3b in the main paper. We can see in \cref{fig:upper_smoothap} that this change enables to have gradients in the correct directions for the two positive instances  and  (tending to increase their scores), and for the negative instance  (tending to decrease its score). 


\item \textbf{ is an upper bound of }. By the proposed design of  in Eq. (4) in submission, we have . Since we do not approximate  by keeping the Heaviside function, it leads to , and therefore .
\end{enumerate}





\begin{figure*}[t]
    \centering
    \begin{subfigure}[t]{0.47\textwidth}
        \centering
        \includegraphics[scale=.46]{illustration/supplementary/step_smoothap.pdf}
        \caption{When replacing  by the Heaviside function in SmoothAP we stop the unexpected behaviour of the gradient flow. However there is still vanishing gradients.}
        \label{fig:step_smoothap_sup}
    \end{subfigure}
    ~ 
    \begin{subfigure}[t]{0.47\textwidth}
        \centering
        \includegraphics[scale=.46]{illustration/supplementary/upper_smoothap.pdf}
        \caption{Our  has gradients that do not stop until the correct ranking is achieved.}
        \label{fig:upper_smoothap}
    \end{subfigure}
    \caption{We illustrates the different steps to built . On \cref{fig:step_smoothap_sup} we change  to be the true Heaviside (step) function. On \cref{fig:upper_smoothap} we replace the sigmoid by  defined in Eq. (4) of the main paper. Using  and ,  is an upper bound of .}
    \label{fig:introb_sup}
\end{figure*}

Overall,  has all the desired properties : i) A correct gradient flow during training, ii) No vanishing gradients while the correct ranking is not reached, iii) Being an upper bound on the AP loss .


\subsection{Properties of the  loss function}

We remind the reader of the definition of the decomposability gap given in Eq. (6) of the main paper.




We illustrates the decomposability gap,  with the toy dataset of \cref{fig:dg_ap}. The decomposability gap comes from the fact that the AP is not decomposable in mini-batches as we discuss in the Sec. 3.2 of the main paper. The motivation behind  is thus to force the scores of the different batches to aligned as illustrated in the Fig. 2b of the main paper.


\begin{figure*}[ht]
    \centering
    \includegraphics[scale=.5]{illustration/supplementary/dg_ap.pdf}
    \caption{Illustration of the decomposability gap on a toy dataset.}
    \label{fig:dg_ap}
\end{figure*}





\paragraph*{Proof of Eq. (8): Upper bound on the  with no } We choose a setting for the proof of the upper bound similar to the one used for training, \ie all the batch have the same size, and the number of positive instances per batch (\ie ) is the same.

Eq. (8) from the main paper gives an upper bound for . This upper bound is given in the worst case: when the AP has the lowest value guaranteed by the AP on each batch. We illustrate this case in~\cref{fig:worst_case_global_ap}.

 \begin{figure*}[t]
     \centering
     \includegraphics[scale=.4]{illustration/supplementary/upperbound_8.pdf}
     \caption{The worst case when computing the global AP would be that each batch is juxtaposed.}
     \label{fig:worst_case_global_ap}
 \end{figure*}
 
In Eq. (8) from the main paper the  in the right hand term comes from the average of AP over all batches:


We then justify the term in the parenthesis of Eq. (8) in the main paper, which is the lower bound of the AP.
In the global ordering the positive instances are ranked after all the positive instances from previous batches giving the following : , with  the  in the batch, ~Positive instances are also ranked after all negative instances from previous batches giving : .

Therefore we obtain the resulting upper bound of Eq. (8) of the main paper:




\paragraph*{Proof of Eq. (9): Upper bound on the  with } In the main paper we refine the upper bound on  in Eq. (9) by adding  which calibrates the absolute scores across the mini-batches.

We now write that each positive instance that respects the constraint of  is ranked after the positive instances of previous batch that respect the constraint giving the following : , with  the  in the current batch. Positive instances are also ranked after the negative instances of previous batches that do not respect the constraints yielding  : .

We then write that positive instances that do not respect the constraints are ranked after all positive instances from previous batches and the positive instances respecting the constraints of the current batch giving  : . They also are ranked after all the negative instances from previous batches giving  : .


Resulting in Eq. (9) from the main paper:




\subsection{Choice of }

In the main paper we introduce  in Eq. (4) to define . We choose  as the point where the gradient of the sigmoid function becomes low , and we then have . This is illustrated in \cref{fig:choice_of_delta}. For our experiments we use  giving .


\begin{figure*}[t]
    \centering
    \includegraphics[scale=.5]{illustration/supplementary/choice_of_delta.pdf}
    \caption{Gradient of the temperature scaled sigmoid () \vs the difference of scores  of a negative pair.}
    \label{fig:choice_of_delta}
\end{figure*}



\section{Experiments}


\subsection{Metrics}

We detail here the performance metrics that we use to evaluate our models.

\paragraph{Recall@K}

The Recall@K metrics (\cref{eq:ratk}) is often used in the literature. For a single query the Recall@K is 1 if a positive instance is in the K nearest neighbors, and 0 otherwise. The Recall@K is then averaged on all the queries. Researcher use different values of K for a given dataset (\eg 1, 2, 4, 8 on CUB).



\paragraph{mAP@R}

Recently, the mAP@R (\cref{eq:mapatr}) has been introduced in \cite{musgrave2020metric}. The authors show that this metric is less noisy and better captures the performance of a model. The mAP@R is a partial AP, computed on the R first instances retrieved, with R being set to the number of positive instances wrt. a query. mAP@R is a lower bound of the AP (mAP@R = AP when the correct ranking is achieved, \ie mAP@R = AP = 1).



\subsection{Detail on experimental setup}

In this section, we describe the experimental setup used in the Sec. 4.1 of the main paper, and the Sec. B of the supplementary.

We use standard data augmentation strategy during training: images are resized so that their shorter side has a size of 256, we then make a random crop that has a size between 40 and 256, and aspect ratio between 3/4 and 4/3. This crop is then resized to 224x224, and flipped horizontally with a 50\% chance. During evaluation, images are resized to 256 and then center cropped to 224.


We use two different strategy to sample each mini-batch.
On CUB and INaturalist we choose a batch size (\eg 128) and a number of samples per classes (\eg 4). We then randomly sample classes (\eg 32) to construct our batches.
For SOP we use the hard sampling strategy from \cite{fastap}. For each pair of category (\eg bikes and coffee makers) we use the preceding sampling strategy. This sampling techniques is used because it yields harder and more informative batches. The intuition behind this sampling is that it will be harder to discriminate two bikes from one another, than a bike and a sofa.


We train the ResNet-50 models using Adam \cite{adam}.
On CUB we train our models with a learning rate of  for 200 epochs.
For SOP and INaturalist we take the same scheduling as in \cite{smoothap}. We set the learning rate for the backbone to  and the double for the added linear projection layer. We drop the learning rate by 70\% on the epochs 30 and 70. Finally the models are trained for 100 epochs on SOP and 90 on INaturalist (as in \cite{smoothap}).

We train the DeiT transformers models using AdamW \cite{adamw} as in \cite{transformer_ir}. On INaturalist we use the same schedule as when training ResNet-50, with a learning rate of . On SOP we train for 75 epochs with a learning rate of  which is dropped by 70\% at epochs 25 and 50. Finally on CUB we train the models for about 100 epochs with a learning rate of .

\subsection{Details of the backbones used}

We briefly describe the backbones used throughout out the experiments presented in the main paper and the supplementary.

\paragraph*{ResNet-50 \cite{resnet50}} We use the well-known convolutional neural network ResNet-50. We remove the linear classification layer. We also add a linear projection layer to reduce the dimension (\eg from 2048 to 512).

\paragraph*{DeiT \cite{deit}} Recently transformer models have been introduced for computer vision \cite{vit,deit}. They establish new state-of-the-art performances on computer vision tasks. We use the DeiT-S from \cite{deit} which has less parameters than the ResNet-50 ( 21 million for DeiT \vs 25 for ResNet-50). We use the pretrained version with distillation from \cite{deit} and its implementation in the \texttt{timm} library \cite{timm}.

\subsection{ROADMAP validation}

\paragraph*{Comparison to AP approximations} We compare in \cref{tab:compa_ranking_losses_sup} ROADMAP \vs other ranking losses on different settings : a batch size of 128 and two backbones (ResNet-50 and DeiT). We conduct this comparison on 5 runs to show the statistical improvement of our method compared to other ranking losses baselines.

We observe that our method outperforms recent ranking losses on the two backbones and the three datasets. On SOP and CUB, ROADMAP has a high increase for the mAP@R, of +1pt on CUB and +2pt on SOP. The performance improvement is greater on the large scale dataset INaturalist with +3.5pt with a ResNet-50 backbone and +2pt with a DeiT backbone of mAP@R. This trend is the same as in the comparison of the main paper (Table 1).

\begin{table}[h!]
    \caption{Comparison between ROADMAP and state-of-the-art AP ranking based losses on three image retrieval datasets. \textit{Bck} in the first column stands for bakcbone. Models are trained with a batch size of 128.}
    \setlength\tabcolsep{3pt}
    \label{tab:compa_ranking_losses_sup} 
    \centering
    \begin{tabularx}{\textwidth}{>{\small} l>{\small} l >{\small}c>{\small}c >{\small}c>{\small}c >{\small}c >{\small}c }
        \toprule
         && \multicolumn{2}{c}{CUB} & \multicolumn{2}{c}{SOP} & \multicolumn{2}{c}{INaturalist} \\
         \midrule
         Bck & Method & R@1 & mAP@R & R@1 & mAP@R & R@1 & mAP@R \\
         \midrule
         \multirow{5}{*}{\rotatebox[origin=c]{90}{ResNet-50}}
         & FastAP \cite{fastap} & 61.280.37 & 24.110.16 & 78.970.05 & 52.230.09 & 57.230.05 & 22.170.05 \\
         & SoftBinAP \cite{naverap} & 61.700.10 & 24.290.16 & 80.300.21 & 53.690.27 & 60.880.06 & 23.220.05 \\
         & BlackBoxAP \cite{blackboxap} & 61.960.28 & 23.830.14 & 80.970.07 & 54.490.15 & 59.530.12 & 19.620.02 \\
         & SmoothAP \cite{smoothap} & 62.450.48 & 24.320.1 & 81.130.05 & 54.740.16 & 64.480.05 & 24.330.07 \\
         & ROADMAP (ours) & \textbf{64.05}0.51 & \textbf{25.27}0.12 & \textbf{82.20} 0.09 & \textbf{56.64}0.09 & \textbf{68.15}0.10 & \textbf{27.01}0.10 \\
         \midrule
         \multirow{5}{*}{\rotatebox[origin=c]{90}{DeiT}}
         & FastAP \cite{fastap} & 73.420.22 & 31.960.06 & 82.920.07 & 59.060.03 & 62.180.07 & 25.480.10 \\
         & SoftBinAP \cite{naverap} & 74.840.11 & 33.570.08 & 84.090.05 & 60.530.07 & 65.970.13 & 27.570.09 \\
         & BlackBoxAP \cite{blackboxap} & 75.450.22 & 33.970.10 & 84.070.09 & 60.200.05 & 70.290.10 & 29.440.06 \\
         & SmoothAP \cite{smoothap} & 76.020.14 & 34.690.08 & 84.280.06 & 60.490.17 & 69.800.08 & 29.560.04 \\
         & ROADMAP (ours) & \textbf{77.14}0.12 & \textbf{36.30}0.08 & \textbf{85.44} 0.06 & \textbf{62.73}0.06 & \textbf{72.81}0.11 & \textbf{31.31}0.10 \\
         \bottomrule
    \end{tabularx}
\end{table}

We perform a paired student t-test to further asses the statistical significance of the performance boost obtained with ROADMAP. We compute the p-values for both the R@1 and mAP@R:~it turns out that the p-values are never larger than , meaning that the gain is statistically significant (with a risk less than \%).


\paragraph*{Ablation studies}

In \cref{tab:supp_ablation_study} we extend the ablation studies of the main paper (Table 2 of main paper) to other settings, including more batch sizes (32, 128, 224, 384) and two backbones (ResNet-50 and DeiT).
On all settings  outperforms the  baseline by almost +0.5pt consistently, and almost +1pt on every setting for INaturalist. When we add  the gain is further increased. As noticed in Table 2 (main paper) the gain when adding  is particularly noticeable on the large scale dataset INaturalist with boost in performances that can be up to +3.3pt of mAP@R for the ResNet-50 with a batch size 32.



\begin{table}[t]
    \caption{Ablation study for the impact of our two contribution \vs the SmoothAP baseline for the three datasets and different batch sizes, with a ResNet-50 backbone \cite{resnet50}}
    \setlength\tabcolsep{3pt}
    \label{tab:supp_ablation_study} 
    \begin{tabularx}{\textwidth}{ l l cc |YY|YY|YY  }
        \toprule
        &&&& \multicolumn{2}{c}{CUB} & \multicolumn{2}{c}{SOP} & \multicolumn{2}{c}{INaturalist} \\
        \midrule
         BS & Method &  &  & R@1 & mAP@R & R@1 & mAP@R & R@1 & mAP@R \\
         \hline
         \multirow{3}{*}{32}
         & SmoothAP &  \xmark & \xmark & 61.84 & 23.76 & 79.96 & 53.21 & 53.25 & 16.4 \\
         & SupAP & \cmark & \xmark  & 62.58 & 24.12 & 80.51 & 53.85 & 55.01 & 17.13 \\
         & ROADMAP &  \cmark & \cmark & \textbf{63.69} & \textbf{24.97} & \textbf{80.74} & \textbf{54.68} & \textbf{56.43} & \textbf{20.43}  \\
        \midrule
        \multirow{3}{*}{128}
         & SmoothAP & \xmark & \xmark & 62.81 & 24.44 & 81.19 & 54.96 & 64.53 & 24.26 \\
         & SupAP & \cmark & \xmark  & 63.18 & 24.9 & 81.72 & 55.65 & 65.79 & 24.77 \\
         & ROADMAP & \cmark & \cmark & \textbf{64.18} & \textbf{25.38} & \textbf{82.18} & \textbf{56.64} & \textbf{68.28} & \textbf{27.13} \\
        \midrule
        \multirow{3}{*}{224}
        & SmoothAP & \xmark & \xmark & 62.93 & 24.69 & 81.2 & 54.73 & 66.62 & 26.08 \\
        & SupAP & \cmark & \xmark  & 64.08 & 25.13 & 81.88 & 55.75 & 67.43 & 26.32 \\
        & ROADMAP & \cmark & \cmark & \textbf{64.65} & \textbf{25.51} & \textbf{82.3} & \textbf{56.55} & \textbf{69.28} & \textbf{27.74} \\
        \midrule
        \multirow{3}{*}{384}
        & SmoothAP & \xmark & \xmark & 63.69 & 24.89 & 81.45 & 55.1 & 67.39 & 26.77 \\
        & SupAP & \cmark & \xmark  & 64.64 & 25.27 & 81.94 & 55.78 & 68.37 & 27.24  \\
        & ROADMAP & \cmark & \cmark & \textbf{64.69} & \textbf{25.36} & \textbf{82.31} & \textbf{56.47} & \textbf{69.19} & \textbf{27.85} \\
         \bottomrule
    \end{tabularx}
\end{table}

In \cref{tab:supp_ablation_study_deit} we extend ablation studies with a transformer backbone (DeiT). We observe the same trend as in \cref{tab:supp_ablation_study}.  is consistently better than the  baseline, with gain up to more than 1pt (\eg on batch size 128 on INaturalist).  further lifts the performances on the three datasets and all batch sizes.

\begin{table}[t]
    \caption{Ablation study for the impact of our two contribution \vs the SmoothAP baseline for the three datasets and different batch sizes, with a DeiT backbone \cite{deit}}
    \setlength\tabcolsep{3pt}
    \label{tab:supp_ablation_study_deit} 
    \begin{tabularx}{\textwidth}{ l l cc |YY|YY|YY  }
        \toprule
        &&&& \multicolumn{2}{c}{CUB} & \multicolumn{2}{c}{SOP} & \multicolumn{2}{c}{INaturalist} \\
         \midrule
         BS &Method &  &  & R@1 & mAP@R & R@1 & mAP@R & R@1 & mAP@R \\
         \hline
        \multirow{3}{*}{128}
         & SmoothAP &  \xmark & \xmark & 76.2 & 34.7 & 84.16 & 60.18 & 69.83 & 29.49 \\
         & SupAP & \cmark & \xmark  & 76.33 & 34.91 & 84.74 & 61.29 & 71.12 &  30.5 \\
         & ROADMAP & \cmark & \cmark & \textbf{77.09} & \textbf{35.76} & \textbf{85.44} & \textbf{62.57} & \textbf{72.82} & \textbf{31.36} \\
\midrule
         \multirow{3}{*}{224}
         & SmoothAP &  \xmark & \xmark & 76.38 & 35.33 & 84.3 & 60.49 & 70.55 & 30.25 \\
         & SupAP & \cmark & \xmark  & 76.47 & 35.67 & 84.77 & 61.38 & 71.9 & 31.31  \\
         & ROADMAP & \cmark & \cmark & \textbf{77.14} & \textbf{36.18} & \textbf{85.56} & \textbf{62.75} & \textbf{73.64} & \textbf{31.82} \\
\midrule
        \multirow{3}{*}{384}
        & SmoothAP & \xmark & \xmark & 76.72 & 35.86 & 84.66 & 61.26 & 71.09 & 30.89 \\
        & SupAP & \cmark & \xmark & 77.13 & 36.17 & 85.01 & 61.76 & 72.55 & 31.89  \\
        & ROADMAP & \cmark & \cmark & \textbf{77.38} & \textbf{36.23} & \textbf{85.35} & \textbf{62.29} & \textbf{73.64} & \textbf{32.12} \\
         \bottomrule
    \end{tabularx}
\end{table}
\paragraph*{Comparison to state of the art method}

We show in \cref{tab:embedding_dim} the impact of increasing the embedding dimension when using ResNet-50. All metrics improve on the three datasets when the embedding dimension increases. We observe a gain particularly important on CUB and SOP with +1pt in R@1 and mAP@R. 


Choosing an embedding size of 2048 further boost the performances of ROADMAP, yielding competitive performances on CUB and state-of-the-art performances for SOP and INaturalist.


\begin{table}[h!]
    \caption{Difference in performance when using an embedding size of 512 \vs 2048 with a ResNet-50 backbone, on the three datasets. Performances are obtained with the same setup as described in the Sec. 4.2 of the main paper.}
    \label{tab:embedding_dim}
    \centering
    \begin{tabular}{ l c cc cc cc }
        \toprule
         && \multicolumn{2}{c}{CUB} &\multicolumn{2}{c}{SOP} & \multicolumn{2}{c}{INaturalist}\\
         \midrule
         Method & dim & R@1 & mAP@R & R@1 & mAP@R & R@1 & mAP@R \\
         \midrule
         ROADMAP (ours) & 512 & 68.5 & 27.97 & 83.19 & 58.05 & 69.19 & 27.85 \\
         ROADMAP (ours) & 2048 & \textbf{69.87} & \textbf{28.8} & \textbf{83.77} & \textbf{59.38} & \textbf{69.62} & \textbf{27.87} \\
         \bottomrule
    \end{tabular}
\end{table}



\paragraph*{Preliminary results on Landmarks retrieval} We show in \cref{tab:landmark_retrieval} preliminary experiments to evaluate ROADMAP on Oxford and Paris~\cite{Radenovic-CVPR18}, by training our model on the SfM-120k dataset and using the standard GitHub code for evaluation\footnote{\url{https://github.com/filipradenovic/cnnimageretrieval-pytorch}}.

We can see that ROADMAP is significantly better than~\cite{transformer_ir} with the DeiT-S~\cite{deit} on Oxford and Paris medium protocol, and has similar performances for Paris hard protocol. This highlights the relevance of using ROADMAP instead of the contrastive loss used in~\cite{transformer_ir}.

\begin{table}[h!]
    \caption{Comparison of ROADMAP vs IRT~\cite{transformer_ir} on Oxford and Paris~\cite{Radenovic-CVPR18}. Models are DeiT-S~\cite{deit}, ROADMAP is trained with a batch size of 128.}
    \label{tab:landmark_retrieval}
    \centering
    \begin{tabular}{ l cc cc }
        \toprule
         \multirow{2}{*}{Method}& \multicolumn{2}{c}{Oxford} &\multicolumn{2}{c}{Paris}\\
         & Medium & Hard & Medium & Hard \\
         \midrule
         IRT~\cite{transformer_ir} & 34.5 & 15.8 & 65.8 & 42.0 \\
         ROADMAP (ours) & \textbf{38.9} & \textbf{20.7} & \textbf{67.5} & \textbf{42.3} \\
         \bottomrule
    \end{tabular}
\end{table}




\subsection{Model analysis}

\paragraph{Hyperparameters}

In \cref{fig:supap_hyperparameters_sup} we show the impact of the hyperparameters of . We plot the mAP@R \vs  in \cref{fig:tau_supap} and mAP@R \vs  in \cref{fig:rho_supap_sup}. The experiments are conducted on SOP with a batch size of 128. 

We observe on \cref{fig:tau_supap} that  is stable with small values of , \ie in the range [0.001, 0.05]. As a reminder we use the default value  in all our results, as it was the suggested value from the SmoothAP paper \cite{smoothap}.



We conduct a study of the impact of  in \cref{fig:rho_supap_sup}. We find that  is very stable wrt. this hyperparameter. Performances are improving with a greater value of  before dropping after . The trend follows what was observed in the Fig. 4b of the main paper, although this time using a value if  yields better performances. Using cross-validation to choose an optimal value for  may lead to even better performances for .

\begin{figure*}[h!]
    \centering
    \begin{subfigure}[t]{0.32\textwidth}
        \begin{tikzpicture}[scale=0.5]
        \begin{axis}[
            title={},
            xlabel={},
            ylabel={mAP@R},
            xmin=0, xmax=1,
            ymin=0, ymax=56,
            xtick={0.001,0.01,0.1,1.0},
            ytick={},
            legend pos=south east,
            ymajorgrids=true,
            grid style=dashed,
            xmode=log,
            font=\LARGE,
        ]
        \addplot[
            line width=2,
            color=blue,
            mark=x,
            mark size=5,
            ]
            coordinates {
            (0.001,54.78)(0.01,55.28)(0.05,52.55)(0.1,46.86)(1.0,0.1)
            };
        \end{axis}
        \end{tikzpicture}
      \caption{mAP@R \vs  for .}
      \label{fig:tau_supap}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.32\textwidth}
        \begin{tikzpicture}[scale=0.5]
        \begin{axis}[
            title={},
            xlabel={},
            ylabel={},
            xmin=0, xmax=11000,
            ymin=55, ymax=56,
            xtick={0.1,1.0,10.0,100,1000,10000},
            ytick={},
            legend pos=south east,
            ymajorgrids=true,
            grid style=dashed,
            xmode=log,
            font=\LARGE,
        ]
        \addplot[
            line width=2,
            color=blue,
            mark=x,
            mark size=5,
            ]
            coordinates {
            (0.0,55.1)(0.1,55.17)(1.0,55.28)(10.0,55.37)(100,55.65)(1000,55.84)(10000,55.66)
            };
        \end{axis}
        \end{tikzpicture}
        \caption{mAP@R \vs  for .}
        \label{fig:rho_supap_sup}
    \end{subfigure}
    
    \caption{Analysis of  hyperparameters on SOP (batch size 128).}
    \label{fig:supap_hyperparameters_sup}
\end{figure*}

\paragraph{Decomposability gap}

In \cref{tab:dgap_increase} we measure the relative decrease of the decomposability gap  on SOP and CUB test sets. On both datasets we can see that  decreases the decomposability gap.


\begin{table}[h!]
    \caption{Relative decrease of the decomposability gap when adding  to  (ROADMAP).}
    \label{tab:dgap_increase}
    \centering
    \begin{tabular}{ c c }
        \toprule
         Dataset &  decrease of  \\
         \midrule
         CUB  & 3.7\% \\
         SOP & 5.4\% \\
         \bottomrule
    \end{tabular}
\end{table}


\subsection{Source code}

We describe in this section the software used for our work, and discuss the computation costs associated with training models presented in this paper.

\paragraph{Librairies} We use several Python libraries often used in image retrieval.

We use \texttt{PyTorch}~\cite{pytorch} as a general framework to implement our neural networks, losses and training loops. We use several utilities from \texttt{PyTorch Metric Learing}~\cite{PML}, an open-source Python library focused on helping researcher working on image retrieval and metric learning. We use \texttt{Faiss}~\cite{faiss} to compute metrics (\ie to perform nearest neighbours search), which is a Python library often used in image retrieval to compute the rankings or the similarity matrix. To load and use the transformer models we use \texttt{timm}~\cite{timm}, a library implementing recent computer vision models, with pretrained weights for most of them. To handle all our config files, we use \texttt{Hydra} \cite{hydra}, this library makes it possible to combine the use of Yaml configuration files and overriding them using the command line.

We use the publicly available implementation of SoftBinAP\footnote{\url{https://github.com/naver/deep-image-retrieval}}~\cite{naverap} which is under a BSD-3 license. The original codes of SmoothAP\footnote{\url{https://github.com/Andrew-Brown1/Smooth_AP}}~\cite{smoothap}, BlackBox\footnote{\url{https://github.com/martius-lab/blackbox-backprop}}~\cite{blackbox,blackboxap} are under an MIT license. For FastAP~\cite{fastap} we use the implementation from \cite{PML} (MIT license), the original implementation of FastAP\footnote{\url{https://github.com/kunhe/FastAP-metric-learning}} is also under an MIT license.



\paragraph*{Compute costs}

We use mixed-precision learning offered within PyTorch \cite{pytorch}. The time and memory consumption are reduced by a factor between  and  with no notable difference in performances. We could train all models on 16GiB GPUs, except for models trained with a batch size of 384 which requires a 32GiB GPU.

\textbf{CUB} Models take between 30 minutes and 1 hour to train on a Nvidia Quadro RTX 5000 with 16GiB.

\textbf{SOP} Models take between 4 and 8 hours to train on a Nvidia Quadro RTX 5000 with 16GiB.

\textbf{INaturalist} To train models on INaturalist we were granted access to the IDRIS HPC cluster with Tesla V-100 GPUs (of 16GiB or 32GiB). Models train for approximately 20 hours.

We could not train models with mixed-precision when using BlackBox \cite{blackboxap}. Models trained with it took longer to train (\eg 30 hours on INaturalist) and are more demanding on memory (almost 16GiB with a batch size of 128 while models trained with other loss functions required less than 10Gib).


\section{Qualitative results}

\paragraph{CUB} As a qualitative assessment, we show in \cref{fig:qualitative_results_cub} some results of ROADMAP on CUB. We show the queries (in purple) and the 10 most similar retrieved images, with relevant instances in green and irrelevant instances in red.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{illustration/supplementary/qualitative_results_cub.pdf}
    \caption{Qualitative results on CUB: a query (purple) with the 10 most similar instances. Relevant (resp. irrelevant) instances are in green (resp. red).}
    \label{fig:qualitative_results_cub}
\end{figure}


\paragraph{SOP} In \cref{fig:qualitative_results_sop} we perform the same assessment for SOP.
In SOP there are fewer relevant instances per query (in average 5). So even for queries that retrieved all the relevant instances, there will be negative instances that have high ranks (in \cref{fig:qualitative_results_sop} ranks that are lower than 10).

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{illustration/supplementary/qualitative_results_sop.pdf}
    \caption{Qualitative results on SOP: a query (purple) with the 10 most similar instances. Relevant (resp. irrelevant) instances are in green (resp. red).}
    \label{fig:qualitative_results_sop}
\end{figure}


\paragraph{INaturalist} Finally we show on \cref{fig:qualitative_results_inat} some examples of queries and the 10 most similar instances for a model trained with ROADMAP on INaturalist.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{illustration/supplementary/qualitative_results_inat.pdf}
    \caption{Qualitative results on INaturalist: a query (purple) with the 10 most similar instances. Relevant (resp. irrelevant) instances are in green (resp. red).}
    \label{fig:qualitative_results_inat}
\end{figure}