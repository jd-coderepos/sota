\documentclass[a4paper]{article}
\usepackage{makeidx}
\usepackage{url}  
\usepackage[utf8]{inputenc}  
\usepackage{latexsym}  
\usepackage{amsmath} 
\usepackage{amssymb} 
\usepackage{graphicx}
\usepackage{authblk}
\usepackage{verbatim}  
\newtheorem{proposition}{Proposition}  
\newtheorem{prop}{Property}  
\newtheorem{invariant}{Invariant}  
\newtheorem{property}{Property}  

\newcommand{\cst}{\mathit{cst}}
\newtheorem{lemma}{Lemma} 
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}
\newtheorem{definition}{Definition}
\newtheorem{problem}{Problem}
\begin{document}
\title{Linear time construction of compressed text indices in compact space\thanks{This work was partially supported by Academy of Finland under grant 250345 (CoECGR).}}
\author{Djamal Belazzougui}
\affil{Helsinki Institute for Information Technology (HIIT),
Department of Computer Science, University of Helsinki, Finland.}

\maketitle
\begin{abstract}
We show that the compressed suffix array and the compressed suffix tree for a string of length  over 
an integer alphabet of size  can both be built in  (randomized) 
time  using only  bits of working space. The previously fastest construction algorithms 
that used  bits of space took times  and 
respectively (where  is any positive constant smaller than ). 
In the passing, we show that the Burrows-Wheeler transform of a string of length  over 
an alphabet of size  can be built in deterministic  time and space 
. 
We also show that within the same time and space, we can carry many sequence analysis tasks
and construct some variants of the compressed suffix array and compressed suffix tree.

\end{abstract}
\section{Introduction}
The suffix tree~\cite{Wei73} is probably the most important text indexing data structure as it can be used for solving many string processing problems~\cite{Ap85,Gu97}. The suffix array~\cite{MM93} is another very popular data structure used for text indexing. 
Although it can not be used to solve as many problems as the suffix tree, its main advantage is the smaller constant in its space usage. 
Given a text of length  over an alphabet of 
size , a suffix tree occupies  bits of space while a suffix array occupy 
~\footnote{In this paper  stands for .} bits. 

The last decade has witnessed the rise of space efficient versions of the suffix array~\cite{GV05,FM05} and the suffix tree~\cite{Sa07a}. In contrast to their non compressed versions they occupy only  bits of space, which saves a factor  and is only a constant factor larger than the original text (which occupies  bits). 
Any operation that can be implemented on a suffix tree can also be implemented on the compressed suffix tree (henceforth \textsc{CST}) at the price of a slowdown that can vary from  to  time. Thus any algorithm or data structure that uses the suffix tree can also be implemented using the \textsc{CST} with a slowdown at most . 




While a \textsc{CST} occupies a smaller space, when compared to its non-compressed counterpart, its construction suffers from a large slowdown if it is restricted to use a space that is only a constant factor away from the final space. More precisely a \textsc{CST} can be built in  time (where  is any constant such that ) using  bits~\cite{HSS09}. Alternatively it can be built in  time but using (non-succinct)  bits of space if one uses an optimal suffix tree construction algorithm to build the (non-compressed) suffix tree~\cite{Fa97} and then compresses the representation. 




The compressed version of the suffix array (the \textsc{CSA}) does not suffer the same slowdown as the compressed version of the suffix tree, since it can be built in time ~\footnote{This bound should actually read as . } when the space is restricted to be  bits~\cite{HSS09}. 
Alternatively it can be built in (deterministic) time  using  bits of space~\cite{OS09}.


The main result of this paper is to show that both the \textsc{CST} and the \textsc{CSA} can be built in randomized linear time using  bits of space. In the passing, we show that the Burrows-Wheeler transform of a string of length  over 
an alphabet of size  can be built in deterministic  time and space . We also 
show that many sequence analysis applications can be carried out within the same time and space bound. 



We note that the (non-compressed) suffix array and suffix tree can be built in time deterministic  as opposed to the randomized  we achieve. The randomization is due to hashing. However, we also note that hashing is also needed to represent the non-compressed suffix tree if one wants to support the fundamental child operation in constant time~\footnote{The constant time child operation can be used to match a pattern of length  against the suffix tree in time .}. In that case building the representation needs randomized  time. 

\begin{comment}
Otherwise, if one insists on deterministic linear time, then the child operation (on a non-compressed suffix tree) can only be supported in  time.
If one abandons the requirement that certain operations on the compressed suffix tree be supported in constant time (but supported in  time), then we show that the compressed suffix tree can be built in deterministic  time. 
\end{comment}
\section{Organization and overview of the techniques}
In~\cite{BGOS11,BGOS13} a technique was introduced which allows to enumerate 
all the suffix tree nodes (actually the corresponding suffix array intervals) 
using space  bits and  time,
based solely on the Burrows-Wheeler transform and  bits of extra-space 
used to store a succinctly represented queue and a bitvector. 
It was predicted that the method would allow to solve many problems 
that relied on the \textsc{CST} in  time instead of 
time. In~\cite{BBO12} the method was successfully applied to the maximal repeat problem. 
In~\cite{BCKM13}, many more applications were described and 
a new enumeration technique based on the bidirectional Burrows-Wheeler transform
was introduced. 
The new technique allows to enumerate intervals in constant time per interval allowing to solve 
many problems in  time, once the required data structure were built. 
However, no efficient method to build those data structures was described. 

One of the contributions of this paper is a third enumeration technique that is more 
space-efficient than the 
previous ones and that might be of independent interest. 
It is a modification of the technique of~\cite{BGOS13} 
but uses a stack instead of a queue and eliminates the need for
a bitvector. 

The \textsc{CST} has three main components, the Burrows-Wheeler 
transform, the tree topology and the permuted longest common prefix 
array. 

We will show that the enumeration 
technique allows to easily build the \textsc{CST} topology in asymptotically 
the same time needed to enumerate the intervals. 
We will also show how to efficiently build the permuted longest common prefix array 
based on the bidirectional Burrows-Wheeler. The technique might be of independent 
interest and might prove useful to solve other kinds of problems of the same flavor. 

Finally, we will show that a variation of our new enumeration technique 
allows to build the Burrows-Wheeler transform in deterministic linear time. 
For that, we will reuse the algorithm described in~\cite{HSS09}. 
That algorithm proceeds in  steps, where each step 
involves the merging of the Burrows-Wheeler transforms of two strings 
of geometrically increasing sizes with the last step involving 
two strings of length  each. 
We show that each merging can be done in linear time, resulting in an 
overall linear time. 


The paper is organized as follows: in section~\ref{sec:back_prel}
we introduce the model and assumptions
and the basic structures from the literature 
that will be used in our algorithms. 

In section~\ref{sec:small_alpha_build} we will describe the algorithms 
that build the suffix tree topology and the longest common prefix array. 
That shows that a basic \textsc{CST} can be built in time . 
In section~\ref{sec:big_alpha_build}, we present our new enumeration technique
and use it to build the Burrows-Wheeler transform in linear time. The Burrows-Wheeler 
transform is the main component in most \textsc{CSA} variants. In the full 
version we will show that the remaining components of the \textsc{CSA}
(at least the ones used in some recent variants of the CSA) and the \textsc{CST}
can be built in randomized linear time. 
We finally outline some applications of our results in section~\ref{sec:applications}. 


\section{Background and preliminaries}
\label{sec:back_prel}
We assume the unit-cost RAM model with word size  bits and with all usual arithmetic and logic
operations taking constant time (including multiplication). 
We assume an integer alphabet . Throughout the paper, we will assume that . Otherwise, there already exist efficient methods to build the suffix tree~\cite{Fa97} (and hence the \textsc{CST} too) and the suffix array~\cite{KA03,KSPP05,KSB06} (and hence the \textsc{CSA}) in  time using  bits of space. 
In section section~\ref{sec:text_indexes} we give a brief description of the suffix array and the suffix tree.
In section~\ref{sec:succ_DS}, we describe the succinct data structures used in this paper. Finally, in section~\ref{sec:compressed_text_idx}, we describe the compressed text indices used in this paper. 
We assume that the reader is familiar with the two major text indexing data structures, the suffix tree and the suffix array. If not, he can find a brief description in section~\ref{sec:text_indexes}. We also assume that the reader is familiar with standard succinct data structures, like rank and select data structures, succinct prefix-sum representations and wavelet trees. 
\subsection{Text indexes}
\label{sec:text_indexes}
\subsubsection{Suffix trees}
A suffix tree for a string  is a compacted trie built on top of all the suffixes of the string \. 
Every leaf of the suffix tree is labeled by a suffix and stores a pointer to it. All the suffix tree leaves are sorted in 
left-to-right order according to the lexicographic order of their corresponding suffixes. 

Suppose that a node  labeled by the path  is a child of a node  labeled by the path , where  is a character and  is a possibly empty string. Then the edge that connects  to  is labeled by the character . 
Since the suffix tree is a compacted trie with no node of degree  and with  leaves, it can have at most  internal nodes. 

\subsubsection{Suffix arrays}
A suffix array for a string  is an array  such that  and  for  if and only if the suffix   is of rank  among all suffixes of  sorted in lexicographic order. 
There exists a strong relationship between suffix trees and suffix arrays. More precisely, the suffix  is exactly the one that labels the th suffix tree leaf in left-to-right order. 


\subsubsection{Suffix array intervals}
Given any factor  that appears in the text , there exists a corresponding interval  such that the subarray  contains the pointers to all the  suffixes of the text that are prefixed by . Given a factor , its suffix array interval is the same as that of the string , where  is the shortest right-maximal string prefixed by  if it exists, or the only suffix prefixed by  if not. 

There is a bijection between suffix tree nodes and the suffix array intervals. Every suffix tree node uniquely corresponds to a suffix array interval and vice-versa. More precisely, the leaves under the suffix tree node labeled by a path  are precisely all the leaves labeled by suffixes ,, in left to right order, where  is the suffix array interval that corresponds to the factor . 
The bijection implies that the total number of suffix array intervals is at most . 

\subsubsection{Weiner and Suffix links}
A suffix tree can be augmented with Weiner and Suffix links. 
A suffix link is a directed edge that connects: 
\begin{enumerate}
\item A leaf corresponding to a suffix  to the leaf corresponding to the suffix , where  is a character. 
\item An internal node labeled by the right-maximal factor  to the internal node labeled by the factor  (where  is by necessity also right-maximal), where  is a character. 
\end{enumerate}

An explicit Weiner link is a directed edge labeled by a character  that connects a node  to a node  such that:
\begin{enumerate}
\item There exists a suffix link that connects  to . 
\item The right-maximal factor that labels  is prefixed by character . 
\end{enumerate}
In other words, an explicit Weiner link connects a node labeled by path  to a node labeled by 
the path . 

An implicit Weiner link is a directed edge labeled by a character  that connects the node  to a node  such that: 
\begin{enumerate}
\item The node  is labeled by a path . 
\item No node is labeled by . 
\item The node  is labeled by , where  is the shortest string such that  is a proper 
prefix of  and  is the label of a node in the tree. 
\end{enumerate}
The total number of explicit Weiner link is linear since every node can only be the destination of one explicit Weiner link. 
It turns out that the number of implicit is also linear (see for example~\cite{BNtalg14} for a proof). 

\subsection{Succinct data structures}
~\label{sec:succ_DS}
\subsubsection{Rank and select}
Given an array  of  elements from , we would wish to support the following three operations: 
\begin{enumerate}
\item , return . 
\item , return the number of occurrences of character  in . 
\item , return the position of the occurrence number  of character  in . That 
is, return the only position  such that  and 
\end{enumerate}
In~\cite{GMR06} it is shown how to build two different data structures that both occupy , 
but with different tradeoffs for ,  and  queries.

The first one supports  in constant time and ,  in  time. 

The second one supports  in constant time,  in  time 
and  in  time. 

In~\cite{GOR10} the time of  of the second data structure was improved to
 time while maintaining the same space bound. 
We note that for the special case  ( is a bitvector) there exists older solutions which use  bits of space 
and support  and  in constant time~\cite{Cl96,Mun96}. 

\subsubsection{Prefix-sum data structure}
Given an array  of numbers which sum up to , a prefix-sum 
data structure is a structure which allows given any  
to return the sum . 
Using Elias-Fano~\cite{El74,Fa71} encoding in combination with bitvectors with constant time select support allows
to build in linear time a data structure which occupies  bits of space and that 
answers to prefix-sum queries in constant time. 


\subsubsection{Wavelet trees}
The wavelet tree~\cite{GGV03} over a sequence of  elements from  
is a data structure which occupies  bits~\cite{GGGRS07} and supports 
,  and  queries in  time. 
Thus a wavelet tree is slower but more space-efficient 
than the structures of~\cite{GMR06}. It is also considerably simpler. 


\subsubsection{Range minimum and range color reporting queries}

A range minimum query data structure (\textsc{rmq} for short)
is a data structure built on top of an integer 
array  and that is able to answer to the following queries: given 
a range , return the index  such that the element 
 is the smallest among all the elements in  (ties are broken 
arbitrarily). There exists a range minimum data structure which occupies 
bits of space and which answers to a query in constant time, without 
accessing the original array~\cite{Fi10}. 


Given an array  of  elements from , we can build a data structure of size  bits 
so that we can report all the  distinct colors in an interval  in time . 
The data structure is a \textsc{rmq} built on top of an array  where  
if and only  is the maximal index such that  and  (that 
is  stores the position of the previous occurrence of character ). 
The algorithm for reporting the colors, needs 
to do  accesses to the arrays  and . 
Such an algorithm was first described in~\cite{Mu02}. 
Subsequently, it was shown that the same result could be 
achieved by only doing  accesses to ~\cite{Sa07b}. 


\subsubsection{Succinct tree representations}
The topology of a tree of  nodes can be represented using  bits so that 
many operations can be supported in constant time~\cite{NStalg14}. Among them are basic 
navigation operations like going to a a child or to the parent of a node, but also more 
advanced operations like the  which returns the 
the lowest common ancestor of two nodes, or the operations  and 
 which for a node , return the indexes  and  
of the leftmost and rightmost leaves  and 
in the subtree of , where  and  are respectively
the number of leaves of tree that lie on the left of  and . 
 
The topology of a tree over  nodes can be described using a sequence 
of  balanced parenthesis built as follows: start 
with an empty sequence then write an opening parenthesis, 
recurse on every child of the root in left-to-right order 
and finally write a closing parenthesis. Another way to view 
the construction of the balanced parenthesis sequence is as 
follows: we do an Euler tour of the tree and write an opening 
parenthesis every time we go down and a closing parenthesis 
when we go up the tree. 

\subsubsection{Monotone minimal perfect hashing}
Given a set  with , a 
monotone minimal perfect hash function (henceforth )
is a function  from  into  such that 
for every  with . In other words 
if the set of keys  is , 
then  (the function returns the rank 
of the key it takes as an argument).  
The function is allowed 
to return an arbitrary value on any . 

In~\cite{BBPV09}, it is shown that there exists a scheme which given 
any set  with , builds 
a  on  that occupies 
bits of space and such that  can be evaluated in constant time. 



\subsection{Compressed text indexes}
\label{sec:compressed_text_idx}
\subsubsection{The Burrows-Wheeler transform}
Given a string , the Burrows-Wheeler transform (henceforth \textsc{bwt}) is obtained as follows
we sort all the  rotations of  and take the last character in each rotation in sorted order. 
There is a strong relation between the \textsc{bwt} of the string \sigma\ otherwise. 
It is well-known that the \textsc{bwt} can be built in  time~\cite{HSS09}, while using  bits of (temporary working) space. 

\subsubsection{FM-index}
The FM-index~\cite{FM05} is a succinct text index built on top of the \textsc{bwt}. There are many variants of the FM-index, but they all share the same basic components: 
\begin{enumerate}
\item The \textsc{bwt} of the original text. 
\item The array  which stores in , the number of occurrences of all characters  in \mathtt{SSA}bi=1,2\ldots n\mathtt{SSA}\mathtt{SA}[i]\mathtt{SA}[i]\bmod b=1i=nppT[1..n-1]\. The Weiner links were initially defined for suffix trees and operate on suffix tree nodes which translate to intervals of suffixes. Here we use Weiner links to operate on intervals of rotations of the \textsc{bwt}, since there is a bijection between suffixes and rotations (except for the rotation that starts with pcppc\mathtt{rank}_c(i_1-1)p'pccp'p'pcpc\mathtt{rank}_c(i_1-1)+1i_2=C[c]+\mathtt{rank}_c(i_1-1)+1n_c=\mathtt{rank}_c(j_1)-\mathtt{rank}_c(i_1-1)pccpj_2=i_2+n_c-1=(C[c]+\mathtt{rank}_c(i_1-1)+1)+(\mathtt{rank}_c(j_1)-\mathtt{rank}_c(i_1-1))-1=\mathtt{rank}_c(j_1)+C[c]\mathtt{rank}O(\log\log\sigma)\O(\log\sigma)\mathtt{select}C\mathtt{lca}\mathtt{leftmost\_leaf}\mathtt{rightmost\_leaf}[i',j']p[i,j]cpcis_i=cpx_ixi''px_ii''=\mathtt{select}_c(i-C[i])js_j=cpx_jj''px_jj''=\mathtt{select}_c(i-C[i])x=\mathtt{lca}(i'',j'')[i,j]i'=\mathtt{leftmost\_leaf}(x)j'=\mathtt{rightmost\_leaf}(x)\mathtt{select}O(1)O(\log\sigma)n\log nO(1)O(n\log\sigma)O(\log^\epsilon n)O(n\log\sigma\log\log n)O(\log\log n)O(n\log^\epsilon n)O(1)n\log\sigma+o(n)t_{\mathtt{SA}}=O(\log n\log\log n)n\log\sigma(1+o(1))t_{\mathtt{SA}}=O(\log_\sigma n\log\log n)O(n\log\log\sigma)O(n\log\sigma)4n+o(n)2.54n+o(n)2n+o(n)O(t_{\mathtt{SA}})O(1)T[1..n-1]\ (which we call reverse \textsc{bwt}), where  denotes the reverse of the text . In the context of the bidirectional \textsc{bwt} we will define the concept of left-maximal factors. 
The core operation of the bidirectional \textsc{bwt} consists in counting the number of occurrences of all characters smaller than  in some interval  of the \textsc{bwt} or the reverse \textsc{bwt}. The data structures presented in~\cite{LYLLYKW09}, \cite{SOG12} and~\cite{BCKM13} all use  bits and support the operation is respectively times ,  (using the Wavelet tree) and . 
A factor  is said to be left-maximal if and only if we have at least two distinct characters  and  such that the two factors  and  appear in T[1..n-1]p[i,j]p[i',j']\overline{p}pp\overline{p}ccp\overline{cp}O(\log\sigma)pc\overline{pc}O(\log\sigma)\mathtt{extendleft}\mathtt{extendright}\mathtt{extendleft}c[i_1,j_1]p[i'_1,j'_1]\overline{p}[i_2,j_2]cpn_c=j_2-i_2+1c\mathtt{bwt}[i_1,j_1]b<cbwt[i_1..j_1]n_b[i'_2,j'_2]\overline{cp}i'_2=i'_1+n_bj'_2=i'_2+n_c-1\mathtt{extendright}\mathtt{extendleft}\mathtt{contractleft}\mathtt{contractright}cpcp\overline{p}pcp\overline{p}\mathtt{contractleft}\mathtt{contractleft}[i_1,j_1]cp[i_2,j_2]pn_bb<c\mathtt{bwt}[i_2,j_2][i'_1,j'_1]\overline{cp}i'_2=i'_1-n_bj'_2=i'_2+(i_2-i_1)\boldsymbol{O(\lowercase{n}\log\sigma)}O(n\log^\epsilon n)O(n\log\sigma)O(n\log\log\sigma)O(n\log\sigma)O(\log^\epsilon n)O(\log^\epsilon n)O(n\cdot t_e)t_et_eO(1)O(\log\sigma)O(\log\log\sigma)[1..n]C_o[1..n]C_c[1..n][i,j]C_o[i]C_c[j]C_cC_oi1nC_o[i]C_c[i]C_cC_oO(n\log n)O(n)C_o[1..n]\log\log n\log n\log\log nC_cC_oC_o2\log\log ni/\log\log ni\log^2n-1\log n\log \log n\log nts=\lceil \log\log n(3+2\log((t+\log\log n)/\log\log n))\rceil\leq 4(\log\log n)^2\log\log n(3+2\log((t+\log\log n)/\log\log n))\leq 5\log\log n+2t\log x\leq xx\geq 17nO(n)\log\log nx\geq 01+2\lceil\log (x+1)\rceil<3+2\log (x+1)x_ii\in[1..\log\log n]\lceil \log\log n(3+2\log((t+\log\log n)/\log\log n))\rceilss\in[1..4\log\log^2 n]T[2^s,\log\log n]T[i,j]ji4(\log\log n)^22^{4(\log\log)^2}\log\log n=o(n)o(n)O(n)[i,j]C_o[i]C_c[j]\log n\log\log n2(2n-1)O(n)4n+o(n)t_eO(n\cdot t_e)O(n)i-1ij[2..n]T[i..n-1]rr-1j12nT[i..n-1]r_i\ell_iT[i..i+\ell_i-1][r_s,r_e]r_s<r_ir_ir_s,\ldots r_i-1\ell_iT[i..i+\ell_i][r_s,r_e]r_s=r_ir_s-1r_s\ell_i+1\ell_{i-1}\leq \ell_i+1\ell_ii=1i=n\ell_{i-1}[r_s,r_e]T[i-1..i+\ell_{i-1}-2][r'_s,r'_e]\overline{T[i-1..i+\ell_{i-1}-2]}\ell_iT[i..i+\ell_{i}-1]\overline{T[i..i+\ell_{i}-1]}i=1\ell_{i-1}=0[r_s,r_e][r_s,r_e][r'_s,r'_e]i=1\ell_{i-1}=0[r_s,r_e][r'_s,r'_e][1..n]\mathtt{extendright}T[i+\ell_{i-1}]\ell_0=0[r_s,r_e]r_s=r_i\ell_i=\ell_{i-1}-1\ell_{i-1}=0\ell_i=0\mathtt{extendright}T[i+\ell_{i-1}+1]T[i+\ell_{i-1}+j][r_s,r_e]r_s=r_i\ell_i=\ell_{i-1}+j[r_s,r_s][r'_s,r'_s]T[i..i+\ell_i-1]\overline{T[i..i+\ell_i-1]}T[i..i+\ell_i-1]TT[i..n]T[i..i+\ell_i]r_i-1T[i..i+\ell_i-1][r_s,r_s]T[i..i+\ell_i-1]r_s<r_ir_s\leq r_i-1T[i..i+\ell_i][r_s,r_s]T[i..i+\ell_i-1]r_s=r_ir_i-1<r_sr_i-1T[i..i+\ell_i-1]T[i+\ell_i]T[i..i+\ell_i-1]T[i..i+\ell_i-1]\mathtt{contractleft}T[i+1..i+\ell_i-1]\overline{T[i+1..i+\ell_i-1]}\mathtt{extendright}\ell_{i+1}\ell_in\mathtt{extendleft}\mathtt{contractright}tO(t\cdot n)O(n)\mathtt{extendleft}\mathtt{contractright}O(\log\sigma)n\sigmaO(n\log\sigma)\boldsymbol{O(\lowercase{n}\log\sigma)}O(n)O(n\log\sigma)S[1..n][1..\sigma]t_{\mathtt{access}}O(n)O(n\log\log\sigma)[i,j]\mathtt{occ}S[i..j]O(\mathtt{occ}(1+t_{\mathtt{access}}))S[i..j]S[i..j]S[1..i-1]O(\sigma)[i,j]\sigma\textsc{mmphf}S[1..i-1]S[i..j]S[1..n][1..\sigma]O(n)n\log\sigma+4n+o(n)\mathtt{access}\mathtt{prank}ic=S[i]\mathtt{rank}_c(i)S[1..n]\sigmaS[1..\sigma]S[\sigma+1,2\sigma]cV_cc10^{\mathtt{freq}_{(c,1)}}10^{\mathtt{freq}_{(c,2)}},\ldots,10^{\mathtt{freq}_{(c,n/\sigma)}}\mathtt{freq}_{(c,i)}ci2n+o(n)iS_iB_i2\sigma10^{\mathtt{freq}_{(1,i)}}, 10^{\mathtt{freq}_{(2,i)}},\ldots,10^{\mathtt{freq}_{(\sigma,i)}}\mathtt{freq}_{(c,i)}ciS'_i[1..\sigma]S'_i[j]S_i[j]S_ic=S_i[j]S_i[1..j]b<cS_ijSx=S'[i]c=s[i]\mathtt{select}_0(B_i,x)-xcS_i\mathtt{select}_0(B_i,x)-\mathtt{select}_1(B_i,c)Sc[1..i-1]SV_cn\log sigma+4n+o(n)\textsc{mmphf}S[1..n][1..\sigma]n\log\sigma+8n+o(n)\mathtt{access}\mathtt{prank}[i,j]\mathtt{occ}S[i..j]O(\mathtt{occ})S[i..j]S[i..j]S[1..i-1]O(\sigma)O(\log\sigma)O(n)[i,j]d\mathtt{bwt}[i,j]c[i,j]c\mathtt{bwt}[1..i-1]\mathtt{bwt}[i..j]O(d\log\sigma)O(\sigma^2\log^2n)=O(n^{2/3}\log n)=o(n)ca\geq 2cacV[1..\sigma][1..\sigma](c,[i_c,j_c])c[i_c,j_c]Y[1..\sigma]W[1..\sigma]N_WdC\alpha_1,\ldots,\alpha_d[i_1,j_1],[i_2,j_2],\ldots,[i_d,j_d]x=1,2,\ldots,dc_1,c_2,\ldots c_k\mathtt{bwt}[i_x,j_x]c_y[i_y,j_y]c_y[i_x,j_x]Y[c_y]\alpha_x[i_y,j_y]V[c_y][Y[c_y]]Y[c_y]1\alpha_xWN_WW[N_W]=c_yWc\in W[1..N_W]Y[c]>1V[c][1..Y[c]]c(\alpha,[i_\alpha,j_\alpha])V[c][1..Y[c]]\alpha[i_\alpha,j_\alpha]c\alpha_x\alpha_yV[c][1..Y[c]]\alpha_x\alpha_ycc\alpha_xc\alpha_ycV[c][1..Y[c]]V[c][1..z]z=Y[c](\alpha_1,[i_{\alpha_1},j_{\alpha_1}])\ldots (\alpha_z,[i_{\alpha_z},j_{\alpha_z}])\alpha_1<\alpha_2<\ldots <\alpha_zc[i_{\alpha_1},j_{\alpha_z}]cu_c=j_{\alpha_z}-i_{\alpha_1}+1111cV[c][1..Y[c]]Y[c]Y[c]u_c\sigma\log n2O(\sigma^2\log^2n)\log n\sigmaO(\sigma\log n)\sigma\leq n^{1/3}o(n)Tn\sigmaO(t_e)O(n\cdot t_e)o(n)TTn\sigmaO(n)O(n)T[1..n-1]\ with  
occurrences of character T'Bn'T'T_B[1..n'/B]BT'T_BT_BO(n/B)=O(n/\log_\sigma n)O((n/B)\log (n/B))=O(n\log\sigma)T^r_BT'B/2Bn'/BT'B/2+1,3B/2+1,\ldots,n'/B-B/2+1T^r_BT_BO(n'/B)T_{B/2}B/2T'T_{B/2}T_BT^r_BT_{B/2}xyy=xyyyxxV_1V_2V[1..\sigma][1..\sigma]Y_1Y_2Y[1..\sigma]WN_Wct=Y_1[c]\neq 0t'=Y_2[c]\neq 0V[c](\alpha_1,[i_{\alpha_1},j_{\alpha_1}]),\ldots..\alpha_t,[i_{\alpha_t},j_{\alpha_t}])V_1[c](\beta_1,[i_{\beta_1},j_{\beta_1}]),\ldots..\beta_t,[i_{\beta_{t'}},j_{\beta_{t'}}])V_2[c]t=t'=1\alpha_1\neq \beta_1t\geq 1t'\geq 1p[i,j][i',j']\alpha_1<\alpha_2<\ldots <\alpha_{t}x\in[1..t][i_{\alpha_x},j_{\alpha_x}][i,j]p\alpha_x\beta_1<\beta_2<\ldots <\beta_{t'}x\in[1..t'][i'_{\beta_x},j'_{\beta_x}][i',j']p\beta_x\alpha_x\beta_x\alpha_x\beta_x[i_{\alpha_x},j_{\alpha_x}]\beta_y<\alpha_x[i'_{\beta_y},j'_{\beta_y}][i_{\alpha_x},j_{\alpha_x}][j'_{\beta_y}+i_{\alpha_x},j'_{\beta_y}+j_{\alpha_x}][i_{\alpha_x},j_{\alpha_x}][j'_{\beta_y}+i_{\alpha_x},j'_{\beta_y}+j_{\alpha_x}]\beta_y[i_{\alpha_x},j_{\alpha_x}][i'-1+i_{\alpha_x},i'-1+j_{\alpha_x}][i_{\alpha_x},j_{\alpha_x}][i'-1+i_{\alpha_x},i'-1+j_{\alpha_x}]\beta_x\beta_yT'_{B/2}T'_{B/4}T'_{1}=T'T\\pi_i[j]=C_i[b[j]]+\mathtt{rank}_{b[j]}(j)C_i[c]=\sum_{1\leq k<c}{f_{c,k}}\pi^{-1}_i[j]=\mathtt{select}_{c}(j-C[c]) 

where  is the only character such that . 
It is easy to see that simulating random access to  can be done by using an \textsc{access} and a partial rank operation to the sequence . It is equally easy to see that a random access to  can be done through a predecessor query on the array  followed by a \textsc{select} operation on the sequence . 
A technique from~\cite{munro2003succinct} allows to store (implicitly) 
only one of  or  so that random access 
to any element of the permutation is supported in constant time. 
Random access to the other permutation is achieved in  time, provided 
that one uses space  bits of space. By choosing , 
 query time and space . 

In order to store , we need to store a plain representation 
of the array  in  bits. Then, in order 
to support partial , we can use a monotone 
minimal perfect hash function on the positions 
of occurrences of character  in  
which would use  bits of space. The mmphf 
can be built in randomized  time (see section~\ref{sec:build_mmphf}). 

In order to store , one uses an indexed bitvector to represent the array 
in  bits. Then a \textsc{select} query on that bitvector for a given position
, will allow to determine the character  such that .
In order to support  on , one needs 
to use prefix-sum data structure for the occurrences of character 
 which would use space  bits allow 
select operation in  time. The prefix-sum data structure can be built 
in time . 

We now describe in more detail the technique 
of~\cite{munro2003succinct}. The idea is to consider 
a permutation  of size  as a collection of cycles
(it could be just one cycle or  cycles). Let us 
define by  for , as  
and . 
Consider the smallest integer  such that 
, then one could say that the cycle that
contains  is of size . It is easy then, to 
decompose the permutation  into cycles by first 
building the cycle  
(by iteratively applying  until we get the value ) 
and then marking all the elements in the cycle. Then, one could 
build another cycle by picking the next non marked position . 
The main idea is to break each cycle of length more than  into 
 blocks of length . Then, store in a dictionary, 
the first element in a block and associate with it a back pointer 
to the first element in the block that precedes it. 
That is, given a rotation  
the dictionary will store the pairs of key values , 
for all  and the pair . Then, determining  for any value 
can be done in  time, by successively computing  and querying 
the dictionary for every element in the sequence. Then the query will be successful, for some 
 with  and the dictionary will return the pair . 
Then it suffices to compute the sequence .
If one chooses , then the space bound for storing 
the dictionary will be , the time 
to construct it is deterministic 
(say representing the dictionary using bitvector of size  with 
ones) and the time to compute  for any value  is . 





\subsection{Building monotone minimal perfect hashing}
\label{sec:build_mmphf}
We prove the following lemma:


\begin{lemma}
\label{lemma:build_mmphf}
Given a set  of  elements from universe  in sorted order, we can build in 
(randomized ) time a monotone minimal perfect hash function that occupies 
bits of space and that answers to queries in  time. 
\end{lemma}
The monotone minimal perfect hash function~\cite{BBPV09} can be built in randomized  time on  sorted elements. 
We first recall the monotone minimal perfect hash function. Given a set  represented in sorted 
order by the sequence 
where , we first partition the sequence into  blocks
of consecutive elements where each block has  elements (except possibly for the last block). 
We then for each block  compute  the longest common prefix of the elements 
in the block (starting from the most-significant bit)
of the block  ( for the last block). 
In order to compute the length of the longest common prefix of the elements in the block, 
we can take the longest common prefix of first and last element in the block(
 and  for the last block and  and 
for the others) which can be computed
in constant time using the  operation (which returns the most-significant bit 
of a given number) which can be simulated using constant number of multiplications~\cite{brodnik1993computation}. 

We then in linear time~\cite{HT01},build a minimal perfect hash function  on the set 
 which maps every key in  to an index 
in . We then store a table  of cells where . 
In other words every key of a block  stores the length of  at the position  in the table .
We also use a table , where . In other words  stores 
the rank of an element in its block.
 
The total space usage of  is  bits and the total space usage of  will be  bits. 

It has been shown that all  are distinct for all , so that every  
uniquely represents the block . We then build a minimal perfect hash function  on the set 
. We finally build a table , 
where . The hash function  occupies 
and the table  occupies  bits of space. 

Given a key  we get that , where 
gives the prefix of  of length . Given  we compute  in constant time, 
then  and . Then  gives us the number of the block 
that store  and  gives the rank of  in its block. Thus the final rank of  
is given by .

We now show how to construct minimal perfect hash functions in case 
 is very close to . We can show that we can achieve  
bits of space. For that we cut the interval  into  chunks of size 
(except for the last chunk which could possibly be of smaller size) and for each chunk 
 with more than on element build build a monotone minimal perfect hash  function on the
elements that fall in the chunk. We also build a prefix-sum data structure that stores the 
number of elements in each chunk and that occupies  bits of space.
Given a query , we first find the chunk that contains . 
That is the chunk  and count  the number of elements in 
 and finally compute the rank of  as . 
\subsection{Building range color reporting structure}
\label{sec:build_range_color_rep}
In this section, we will show that we can build a range color reporting data structure for 
a sequence of length  over an alphabet  in total (randomized) time . 
We use the method proposed in~\cite{BNV13}. The method uses the following idea which was first proposed
in~\cite{Sa07b}. We use two \textsc{rmq}s on top of the sequence of colors. The first one 
allows to enumerate all the leftmost occurrences of all distinct colors. The second allows to enumerate
all the rightmost occurrences of the colors. We then use a monotone minimal perfect hash function built 
using Lemma~\ref{lemma:build_mmphf} to compute the rank of the leftest occurrences and the rank of the rightest occurrence of each color. 
The frequency of the color is obtained by subtracting the first rank from the second and incrementing 
the result by . 
We can construct the range minimum and range maximum query data structures in linear time 
and using  bits of extra-space using the algorithm 
of~\cite{Fi10}. Each of the two occupies  bits. We can build the monotone minimal 
perfect hash function in  bits of space and randomized  time using 
the algorithm described in previous section. 
The final space for all the minimal perfect hash functions will be  bits
of space. 

\section{Efficient FM-indices for large alphabets}
\label{sec:build_ext_fm_indices}

\subsection{Building the Weiner link support}
\label{sec:build_weiner_links}
We can show that we can build the data structure recently proposed in~\cite{BNtalg14} in 
(randomized) linear time and compact space. 
Given the suffix tree, we consider all the nodes whose path is prefixed by 
a character . For every node  whose corresponding  
interval contains character  and whose corresponding path is , we know that there 
exists a path  in the suffix tree. 
We traverse the suffix tree nodes in preorder, and for each node  with corresponding 
interval  and a corresponding path  use the method described 
in subsection~\ref{subsec:linear_time_bwt_interval_enum}  (Lemma~\ref{lemma:enum_intervals})
to determine all the  distinct characters 
that appear in  and for each character  
determine the node  that is the target to a Weiner link from  and labeled
by character . This takes time . 




We can now determine whether the Weiner link is explicit 
or not. That is whether the path of  is  or not. In order to do that 
we apply a suffix link on  and determine whether the target node is  
or not. If so we conclude that the Weiner link is explicit, otherwise we 
conclude it is implicit. 

We now describe the used data structures. For each character , we 
maintain two vectors. A vector  that stores all the source nodes 
of Weiner links labeled by . The nodes are stored in sorted order and 
we use Elias  or  encoding~\cite{El75} to encode the difference between two consecutive 
nodes. In addition we store a bit vector  that states whether each Weiner link 
is implicit or explicit. 

The two vectors  are filled while traversing the suffix tree. Each time 
we determine that a node  is the source of Weiner link labeled with character 
, we append  to the vector . Then, if the Weiner link is explicit, we 
append a  to vector . Otherwise we append a . 

Given that the number of Weiner links 
will be linear, the total space used by the vectors 
 will be  bits. The total space used by the vectors  will 
be . This is by log-sum inequality. 

One detail we should take care about is that of memory allocation. 
In order to avoid fragmentation, 
we will only use static memory allocation. That is, in a first pass, we will not 
store  or , but just determine their sizes. We will thus just maintain two 
counters  and  that store the number of bits needed to store 
 and  in addition to a variable  that stores the last node visited 
node with Weiner link labeled by . That way while traversing every node of the suffix tree
we can easily determine for each Weiner link labeled , the 
space needed to store the difference between the node (using  or  coding)
and its predecessor in the list of Weiner links labeled by . 
We note that maintaining the arrays  and  
requires  bits of space. 

Once we have built the arrays  and  for all , 
we build the monotone perfect hash functions  based on 
for all . 
For that, we will use the scheme that was presented in the previous section. 
Recall that  stores all the nodes which have a Weiner link 
labeled with character  in sorted order. Suppose that 
has  elements, we will cut  into intervals of 



Finally for building the RMQ data structures we use the 
result of~\cite{Fi10} which uses  bits of space 
in addition to the original array. The building time 
is , where  is the time 
to access the  array, where  is the largest index 
such that , otherwise  
if there is no such  . It is easy to see that  
can be recovered using . 
As it is too costly to use the rank operation (which cost  
time), we note that  can be obtained in constant time through the monotone 
minimal perfect hash function built on  in linear time. 

\begin{lemma}
\label{lemma:weiner_link_support}
Given a text of length  over an alphabet of size  
whose \textsc{bwt} and suffix tree topology 
have been precomputed, we can in  randomized time and using 
bits build a data structure that occupies  bits of space
and that allow to compute a Weiner link in time , 
where  is the time to do a \textsc{select} query on the \textsc{bwt}. 
\end{lemma}
We thus conclude with the following theorem:
\begin{theorem}
Given a text of length  over an alphabet of size , 
we can in randomized  time and  bits 
of space, build an index that occupies  
bits of space and that can count the number of occurrences 
of a string of length  in time  and then report occurrences of 
the pattern  in  time per occurrence. It can also extract an arbitrary
substring of the text in  time. 
\end{theorem}

Note that the data structure referred in the theorem is described in~\cite{BNtalg14}. 
For and older and slightly slower variant we can show deterministic 
construction time:
\begin{theorem}
Given a text of length  over an alphabet of size , 
we can in deterministic  time and  bits 
of space, build an index that occupies 
 bits of space and that can count the number of occurrences 
of a string of length  in time  and then report occurrences of 
the pattern  in  time per occurrence. It can also extract an arbitrary
substring of length  of the text in  time. 
\end{theorem}

\subsection{Building the bidirectional Weiner link support}
\label{sec:build_biweiner_links}

We can show that we can build the data structure recently proposed in~\cite{BCKM13}. 
We will build a sequence of Balanced parentheses  for each character 
. That sequence will store the topology of a virtual tree that represents 
all destinations of Weiner links that are labeled by . 
We will also build a vector  that represents all the suffix tree 
nodes that are source of Weiner links labeled by . 
Our goal is to fill a vector  that for each node with Weiner 
link labeled by , stores the difference  between the number of 
occurrences of characters  in the associated suffix array interval and the same 
in all its closest descendant that have a Weiner link labeled by . 

We assume that we have already built the unidirectional Weiner link support
(using Lemma~\ref{lemma:weiner_link_support}). 
We first traverse the suffix tree nodes in depth first order. The traversal 
does not need to use a stack. We use ,  
and  operations. Each node (except the leaves) 
is traversed twice, once in descending and in ascending 
directions. 
For each traversed node  with corresponding 
interval  we enumerate distinct characters 
that appear in . That is all the characters 
that labels Weiner links starting from . 
When we traverse  in descending direction. For each character , we append an 
opening parenthesis to sequence . When we traverse  in ascending 
direction, we append a closing parenthesis to  and append the value 
of  to the vector . We do not store the value itself, but instead 
store the difference between  and the last node stored in  (we use 
a vector  to store the last node in ). 

Once we have built the Weiner link support for each character . 
Recall that our goal is to build for each character  a data structure that stores 
for each node  that has a Weiner link labeled by , the number 
of occurrences of characters  in the suffix array intervals that corresponds to node . 

We will use a temporary vector , 
where  stores the index of the last character. 
The vector occupies  bits of space. 
Initially all entries are set to zero. 
We will also use a temporary stack of capacity at most  bits. 
For a given character , our first step will be to build a tree topology 
data structure on top of vector . 
We then traverse the vector  and 
for each node  compute the number of occurrences of characters  
in the interval  that corresponds to node . This is done as follows:
we retrieve character . This character 
is the largest character  such that . We then 
use the (unidirectional) Weiner link support to count the number of occurrences of the 
character  in  in constant time time. 
We then use the bidirectional Weiner link support for character  (which by definition 
has already been built since ) to count the total number 
of characters  in  and add the total to the number of occurrences of 
 in . This will give us the total number of occurrences of characters 
 in  (which we note by ).

Whenever  is a leaf we will simply push on the stack 
a  if the interval  contains a single 
character  and push a  otherwise. 

If  is an internal node, we will pop from the stack the counters 
associated with all the children of . That is we use the topology
built on top of  to retrieve the number of children of , 
and then sum up their value. We will then subtract the sum from 
. We then append  at the end of vector 
 (we use gamma-coding to push the value).

We then push the original value of  (before subtraction) 
on the stack (actually the value is delta or gamma coded before 
pushing on the stack). We finally set . 

\begin{lemma}
\label{lemma:bi_weiner_link_support}
Given a text of length  over and alphabet of size  
such that the \textsc{bwt}, the suffix tree topology and 
the Weiner link support for both the text and its reverse 
have been precomputed. Then in  randomized time 
and using  bits of space, we can build a data structure 
that occupies  bits of space and that allows to compute a 
bidirectional Weiner link (and suffix link) in time .
\end{lemma}
We thus conclude with the following theorem:
\begin{theorem}
\label{theo:full_double_BWT}
Given a text of length  over an alphabet of size , 
we can in randomized  time, build an index that occupies 
 bits of space and that support operations , 
, , , 
 and  in  time. 
\end{theorem}
Note that the data structure built in Theorem~\ref{theo:full_double_BWT} 
is precisely the data structure number  described in~\cite{BCKM13}. 

Also, by combining lemmata~\ref{lemma:weiner_link_support},~\ref{lemma:bi_weiner_link_support} and~\ref{lemma:plcp} 
we get the following lemma:
\begin{lemma}
\label{lemma:plcp2}
Given a text of length  over an alphabet of size  
whose \textsc{bwt} and the suffix tree topology 
and the \textsc{bwt} of its reverse have been precomputed,
we can build the \textsc{plcp} array in randomized 
 time and  bits of 
additional space. 
\end{lemma}
\section{Other compressed suffix tree operations}
~\label{sec:build_cst}
Most of the suffix tree operations but not all can be supported in constant time by using combination of the suffix tree topology, the permuted lcp array and the FM-index. In particular, three important operations are supported in time . These are the string depth operation, the child operation and the String level ancestor operation. The first operation is supported in time  using all three components of the CST (which were previously shown). We show how that auxiliary data structures necessary to support the last two operations can be built in time randomized  time. 

\subsection{Suffix tree with blind child support}
\label{sec:build_blind_child_support}
In~\cite{BN11,BNtalg14} it was shown how to augment a suffix tree with 
bits of space so that a child operation can be supported in  (which translates 
to  time for the \textsc{csa} version that uses  bits of space). 
We can show that the augmentation can be built in  time as follows. 

Once we have built the suffix tree topology, we use Lemma~\ref{lemma:enum_intervals} to 
generate all suffix array intervals that correspond to internal suffix tree node along with
their child labels (in sorted order), and for each of interval
determine the suffix tree node using the tree topology. 
We finally store the labels of all children of the node 
in an  array of total size at most  bits, which stores all the children labels
of all nodes, where the nodes are sorted in order. That is for every node , we store 
the child labels in positions , where  counts the total number of children of all 
nodes  and  counts the number of children of node . We then scan the array  and 
build the monotone minimal perfect hash function on the child labels of every node . 

\subsection{String level ancestor queries}
\label{sec:build_blind_child_support}

The string level ancestor is an important operation on the suffix tree. It can be supported in time ~\cite{KKNS13} (which translates to  time using the \textsc{csa} version that uses  bits of space). 
We now show that the support for string level ancestor queries can be added in time  on top of a compressed suffix tree representation. The additional space is . 
We first describe the string level ancestor operation implementation as explained to us by Travis Gagie~\footnote{We thank Travis Gagie for explaining it to us.}. This implementation is different from the one described in described in~\cite{KKNS13} but achieves essentially the same time and space bounds. 


We sample every  node in the suffix tree and build a weighted level ancestor (\textsc{wla} for short) data structure  on it~\cite{FM96}, where the weight associated with every node will be its string depth. 
We now describe how the sampling is done. We first define the height and the depth of the tree nodes. We define the depth
of a node as the distance between the root and the node. We define the height of an internal node  as the difference between the depth of the deepest leaf in the subtree rooted at  and the depth of . 
A node will be sampled if and  only if:
\begin{enumerate}
\item Its depth is multiple of . 
\item Its height is at least . 
\end{enumerate}
We can easily show that the number of sampled nodes will be at most . This is easy to see. For every sampled node, we can associate at least  non-sampled nodes. Suppose that the sampled node  has no sampled node among its descendants. Then  has height at least  and thus must have a path that contains at least  non-sampled nodes and we can associate all the nodes in that path with . Otherwise, the sampled node  is at depth  and has at least one descendant at depth  and all the  nodes along the path between the two sampled nodes will not be sampled. Thus, we associate all those  nodes with . 

We now describe the sampling algorithm. It can be done using navigation operation ,  and  and operations  and . We do a full traversal of the suffix tree and for every node  of depth  multiple of  and height at least , we determine the string depth  of  and append the pair  to a list initially empty. During the traversal, we can easily generate the tree topology of the tree that contains only sampled nodes by generating a sequence of balanced parenthesis in the following way:
every time we traverse down a sampled node, we append an opening parenthesis and every time we traverse up we append a closing parentheses. We then generate a dictionary  that stores all the sampled nodes. This would use  bits. 
At the end we use the list of pairs  and the tree topology as input to an algorithm that generates the \textsc{wla} data structure~\cite{FM96}. 

We now describe how queries are implemented. We first note that the level ancestor query which asks given a node , to return the ancestor at depth  can be supported in constant time using the representation of~\cite{SNsoda10}. 

Given a node  and a target string depth , we first determine the depth  of . We then make a level ancestor query to determine the node  at depth , where . We then query  to find whether the node  is sampled or not. If  is not sampled and is not the root, then we replace  by its ancestor at depth . If  is the root, then its string depth is  and we keep it. 

We then query the \textsc{wla} data structure to determine the node , the deepest sampled node ancestor of  and whose depth is at most  (we return  if its depth is at most ). If that node is  itself, then we need to do a binary search over the ancestors of  of depths between  (or  if  has been replaced) and  by using the string depth and the level ancestor operations on the compressed suffix tree. If  and  is at depth . Then we need to do a binary search over ancestors of  of depths between  and . 

The total time is dominated by the binary search over a depth interval of size at most  which takes  steps and time .
\subsection{Completing the compressed suffix tree construction}
Theorem~\ref{theo:cst_large_alphabet} is proved by combining Theorem~\ref{theo:build_bwt} with lemmata~\ref{lemma:tree_topology2} and~\ref{lemma:plcp2} and using the augmentations presented in the last two subsections. 

\section{Deterministic constructions}
~\label{sec:det_building}
We can show that the deterministic construction can be done in  at the price 
of slowing down the Weiner links (in the FM-indices and the compressed suffix tree) to  time and the child operation (in the suffix tree) to  time. For that purpose we completely eliminate 
any use of monotone minimal perfect hash functions, and instead rely on  
queries which are answered in time . This slows down Weiner links to 
time. 
For the child operation, we notice that every node with  children, we can sort the  labels
and sample one in every  child and store the sampled labels in a predecessor 
structure which answers in time . The predecessor structure can be build in 
time and occupies  bits of space. The child operation is finished by doing binary search on 
an interval of  labels in time . 


\end{document}
