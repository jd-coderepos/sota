

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} 

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2023}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage{enumitem}

\usepackage[capitalize,noabbrev]{cleveref}


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[disable,textsize=tiny]{todonotes}
\setlength\marginparwidth{1.6cm}


\usepackage{multirow}
\usepackage{url}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage[caption = false]{subfig}

\definecolor{Gray}{gray}{0.85}
\definecolor{lightyellow}{rgb}{1.0, 0.98, 0.65}

\definecolor{dark2green}{rgb}{0.1, 0.65, 0.3}
\definecolor{dark2orange}{rgb}{0.9, 0.4, 0.}
\definecolor{dark2purple}{rgb}{0.4, 0.4, 0.8}
\newcommand{\first}[1]{\textbf{\textcolor{dark2green}{#1}}}
\newcommand{\second}[1]{\textbf{\textcolor{dark2orange}{#1}}}
\newcommand{\third}[1]{\textbf{\textcolor{dark2purple}{#1}}}

\newcolumntype{a}{>{\columncolor{Gray}}c}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\diam}{\mathrm{diam}}

\newcommand{\danica}[2][noinline]{\todo[color=violet!20,#1]{D: #2}}
\newcommand{\av}[2][noinline]{\todo[color=red!20,#1]{AV: #2}}

\let\mytodo\todo




\begin{document}

\twocolumn[
\icmltitle{\textsc{Exphormer}: Sparse Transformers for Graphs}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Hamed Shirzad}{equal,ubc}
\icmlauthor{Ameya Velingker}{equal,google-research}
\icmlauthor{Balaji Venkatachalam}{equal,google}
\icmlauthor{Danica J.\ Sutherland}{ubc,amii}
\icmlauthor{Ali Kemal Sinop}{google-research}
\end{icmlauthorlist}

\icmlaffiliation{ubc}{Department of Computer Science, University of British Columbia, Vancouver, BC, Canada}
\icmlaffiliation{google-research}{Google Research, Mountain View, California, USA}
\icmlaffiliation{google}{Google, Mountain View, California, USA}
\icmlaffiliation{amii}{Alberta Machine Intelligence Institute, Edmonton, Alberta, Canada}

\icmlcorrespondingauthor{Hamed Shirzad}{shirzad@cs.ubc.ca}
\icmlcorrespondingauthor{Ameya Velingker}{ameyav@google.com}
\icmlcorrespondingauthor{Balaji Venkatachalam}{bave@google.com}

\icmlkeywords{Machine Learning, Graph Neural Networks, GNN, Transformer, Attention}

\vskip 0.3in
]





\printAffiliationsAndNotice{\icmlEqualContribution} 

\begin{abstract}
 Graph transformers have emerged as a promising architecture for a variety of graph learning and representation tasks. Despite their successes, though, it remains challenging to scale graph transformers to large graphs while maintaining accuracy competitive with message-passing networks. In this paper, we introduce \textsc{Exphormer}, a framework for building powerful and scalable graph transformers. \textsc{Exphormer} consists of a sparse attention mechanism based on two mechanisms: virtual \emph{global nodes} and \emph{expander graphs}, whose mathematical characteristics, such as spectral expansion, pseduorandomness, and sparsity, yield graph transformers with complexity only linear in the size of the graph, while allowing us to prove desirable theoretical properties of the resulting transformer models. We show that incorporating \textsc{Exphormer} into the recently-proposed GraphGPS framework produces models with competitive empirical results on a wide variety of graph datasets, including state-of-the-art results on three datasets. We also show that \textsc{Exphormer} can scale to datasets on larger graphs than shown in previous graph transformer architectures. Code can be found at \url{https://github.com/hamed1375/Exphormer}.
\end{abstract}

\section{Introduction}
Graph learning has become an important and popular area of study that has yielded impressive results on a wide variety of graphs and tasks, including molecular graphs, social network graphs, knowledge graphs, and more. 
While much research around graph learning has focused on graph neural networks (GNNs), which are based on local \emph{message-passing}, a more recent approach to graph learning that has garnered much interest involves the use of \emph{graph transformers} (GTs). Graph transformers largely operate by encoding graph structure in the form of a \emph{soft inductive bias}. These can be viewed as a graph adaptation of the Transformer architecture~\citep{VaswaniSPUJGKP17} that are successful in modeling sequential data in applications such as natural language processing. 

Graph transformers allow nodes to attend to all other nodes in a graph, allowing for direct modeling of long-range interactions, in contrast to GNNs. This allows them to avoid several limitations associated with local message passing GNNs, such as oversmoothing~\citep{OonoSuzuki20}, oversquashing~\citep{AlonYahav21, ToppingGCDB22}, and limited expressivity~\citep{morris2019weisfeiler, xu2018powerful}. The promise of graph transformers has led to a large number of different graph transformer models that have been proposed in recent years~\citep{DwivediBresson21, kreuzer2021rethinking, Ying2021DoTR, MialonCSM21}.  

One major challenge for graph transformers is their poor scalability, as the standard global attention mechanism incurs time and memory complexity of , \emph{quadratic} in the number of nodes in the graph. While this cost is often acceptable for datasets with small graphs (e.g., molecular graphs), it can be prohibitively expensive for datasets containing larger graphs, where graph transformer models often do not fit in memory even for high-memory GPUs, and hence would require much more complex and slower schemes to apply.  Moreover, despite the expressivity advantages of graph transformer networks~\citep{kreuzer2021rethinking}, these architectures have often lagged message-passing counterparts in accuracy in many practical settings.

A recent breakthrough came with the advent of GraphGPS~\citep{RampasekGDLWB22}, a modular framework for constructing networks by combining local message passing and a global attention mechanism together with a choice of various positional and structural encodings. To attempt to overcome the quadratic complexity of the ``dense'' full transformer and improve scalability, the architecture allows for ``sparse'' attention mechanisms, like Performer~\citep{ChoromanskiLDSG21} or Big Bird~\citep{ZaheerGDAAOPRWY20}.
This combination of Transformers and GNNs achieves state-of-the-art performance on a wide variety of datasets.

In almost all cases, however, the best results of \citeauthor{RampasekGDLWB22} were obtained by combining a message-passing network with a full transformer;
their sparse transformers performed relatively poorly by comparison,
and indeed their ablation studies showed that on a number of datasets it is better to avoid using attention at all than to use their implementation of BigBird.
This may be related to the fact that sparse attention mechanisms like BigBird have largely been designed for \emph{sequences}; this is natural for language tasks, but graphs behave quite differently.
Thus, it is natural to ask whether one can design sparse attention mechanisms more tailored to learning interactions on general graphs.


Another major question concerns graph transformers' scalability. While BigBird and Performer are linear attention mechanisms, they still incur computational overhead that dominates the per-epoch computation time for moderately-sized graphs. The GraphGPS work tackles datasets with graphs of up to 5,000 nodes, a regime in which the full-attention transformer is in fact computationally faster than many sparse linear-attention mechanisms. Perhaps more suitable sparse attention mechanisms could enable their framework to operate on even larger graphs. Additionally, for smaller graphs, they may require a much smaller batch size to fit into the GPU memory, resulting in slower training, or the need for high-end GPUs. For instance, on the MalNet-Tiny dataset, GraphGPS with a full transformer runs out of memory (on a 40GB NVIDIA A100 GPU) with a batch size of just 16; as we shall see, our techniques allow us to extend the batch size to 256 without any memory issues.


\paragraph{Our contributions.}
We propose sparse attention mechanisms with computational cost linear in the number of nodes and edges. We introduce \textsc{Exphormer} that combines the two techniques for creating sparse overlay graphs.  The first sparse attention mechanism is to use \emph{global  nodes} --- nodes that are connected to all other nodes of the graph. The number of additional edges added to the graph is linear in the number of nodes.
We also introduce \emph{expander graphs} as a powerful primitive in designing scalable graph transformer architectures. The expander graph as an overlay graph has edges linear in the number of nodes.
Expander graphs have several desirable properties -- small diameter, spectral approximation of a complete graph, good mixing properties --  which make them a suitable ingredient in a sparse attention mechanism. We are able to show that \textsc{Exphormer}, which combines expander graphs with global nodes and local neighborhoods, spectrally approximates the full attention mechanism with only a small number of layers, and has universal approximation properties.
\mytodo{Modify the previous section according to the final theoretical results.}
Its attention scheme provides good inductive bias for places the model ``should look,''
in addition to being more efficient and less memory-intensive.

We show that \textsc{Exphormer} are a powerful Transformer that produces results comparable to  GraphGPS with a full transformer\todo{On some datasets in Tables 9-11, or always?}.  Moreover, when combined with MPNNs in the GraphGPS framework, we can achieve SOTA or close to SOTA.
\textsc{Exphormer}
(1) produces better results than other sparse transformers on all datasets we evaluate; 
(2) has results comparable to and in some cases better than the full transformer, despite having fewer parameters;
(3) achieves state-of-the-art results on many datasets, better than MPNNs, including on Long Range Graph Benchmark (LRGB; \citealp{lrgb-2022}) datasets that require long-range dependencies between nodes;
and (4) can scale to larger graphs than previously shown.\mytodo{CIFAR10, MNIST, and PATTERN}


\section{Related Work}
\paragraph{Graph Neural Networks (GNNs).} Early works in the area of graph learning and GNNs include the development of a number of architectures such as GCN~\citep{defferrard2016convolutional, kipf2017semi}, GraphSage~\citep{hamilton2017inductive}, GIN~\citep{xu2018powerful}, GAT~\citep{velickovic2018graph}, GatedGCN~\citep{bresson2017residual}, and more. GNNs are based on a message-passing architecture that generally confines their expressivity to the limits of the  1-Weisfeiler-Lehman (1-WL) isomorphism test~\citep{xu2018powerful}.

A number of recent papers have sought to augment GNNs to improve their expressivity. For instance, one approach has been to use \emph{additional features} that allow nodes to be distinguished -- such as using a {one-hot encoding} of the node~\citep{murphy2019relational} or a {random} scalar feature~\citep{sato2021random} -- or to encode positional or structural information of the graph -- e.g., skip-gram based network embeddings~\citep{qiu2018network}, substructure counts~\citep{bouritsas2020improving}, or Laplacian eigenvectors~\citep{dwivedi2021graph}. Another direction has been to \emph{modify the message passing rule} to allow the network to take further advantage of the graph structure -- including directional graph networks (DGN; \citealp{beaini2021directional}) that use Laplacian eigenvectors to define {directional flows} for anisotropic message aggregation -- or to \emph{modify the underlying graph} over which message passing occurs with \emph{higher-order GNNs}~\citep{morris2019weisfeiler} or the use of substructures such as junction trees~\citep{fey2020hierarchical} and simplicial complexes~\citep{bodnar2021weisfeiler}.\footnote{\citet{expander-graph-prop}, in work concurrent to and independent of ours, propose an expander-based graph learning mechanism for message-passing networks, alternating between layers based on the input graph with ones based on an auxiliary expander graph. This scheme is rather different from ours; we do not compare further.}

\paragraph{Graph transformer architectures.} Attention mechanisms have been extremely successful in sequence modeling since the seminal work of~\citet{VaswaniSPUJGKP17}. The GAT architecture~\citep{velickovic2018graph} proposed 
using an attention mechanism to determine how a node aggregates
information from its neighbors; it does not 
use a positional encoding for nodes, limiting its
ability to exploit global structural information.
GraphBert~\citep{zhangGraphBert} uses the graph structure
to determine an encoding of the nodes, but not for the underlying attention mechanism.

Graph transformer models typically operate on a fully-connected graph in which every pair of nodes is 
connected, regardless of the connectivity structure of the original graph. Spectral Attention 
Networks (SAN)~\citep{kreuzer2021rethinking} make use of \emph{two} attention mechanisms, one on the fully-connected graph and one on the original edges of the input graph, while using Laplacian positional encodings for the nodes. Graphormer~\citep{Ying2021DoTR} uses a single dense attention mechanism but adds structural features in the form of centrality and spatial encodings. Meanwhile, GraphiT~\citep{MialonCSM21} incorporates relative positional encodings based on diffusion kernels.

GraphGPS~\citep{RampasekGDLWB22} proposed a general framework for combining message-passing networks with attention mechanisms, while allowing for the mixing and matching of positional and structural embeddings. Specifically, the framework also allows for sparse transformer models like BigBird~\citep{ZaheerGDAAOPRWY20} and Performer~\citep{ChoromanskiLDSG21}.

Moreover, recent works have proposed sampling-based scalable graph transformers, e.g., Gophormer~\citep{gophormer21}, NAGphormer~\citep{nagphormer22}. Another line of work aims to learn data dependencies beyond the given graph structure using linear time transformers. For instance, Nodeformer~\citep{wu2022nodeformer} is inspired by Performer~\citep{ChoromanskiLDSG21} and leverages the kernelized Gumbel-Softmax operator to enable the propagation of information among all pairs of nodes in a computationally efficient manner. Another work is Difformer~\citep{wu2023difformer}, which, on the other hand, is a continuous time diffusion-based Transformer model. Even though these models can scale up to millions of nodes, both of them use a random subset of a maximum of 100K nodes as mini-batches, and the attention mechanism takes place only over the nodes in the batch. Transformers are also applied on the spectral GNNs~\citep{bo2023specformer}.

\paragraph{Sparse Transformers.} Standard (dense) transformers have quadratic complexity in the number of tokens, which limits their scalability to extremely long sequences. By contrast, \emph{sparse transformer} models improve computational and memory efficiency by restricting the attention pattern, i.e., the pairs of nodes that can interact with each other.
In addition to BigBird and Performer, there have been a number of other proposals for sparse transformers; \citet{TayDBM20} provide a survey.



\section{Sparse Attention on Graphs}
This section describes three \emph{sparse} patterns that can be used in transformers in individual layers of a graph transformer architecture.
We begin by describing graph attention mechanisms in general.

\subsection{Attention mechanism on graphs}
An attention mechanism on  tokens can be modeled by a directed graph  on , where 
a directed edge from  to  indicates a direct interaction between tokens  and , i.e., an inner 
product that will be computed by the attention mechanism.
More precisely, a transformer block can be viewed as a function on the -dimensional embeddings for each of  tokens, mapping from  to . Let . A generalized (dot-product) attention mechanism  with attention pattern given by  is defined by

where  is the number of heads and  is the head size, while  and .
(The subscript  is for ``keys,''  for ``queries,''  for ``values,'' and  for ``output.'')
Here  denotes the submatrix of  obtained by picking out only those columns corresponding to elements of , the neighbors of  in . We can see that the total number of inner product computations for all  is given by the number of edges of . A (generalized) \emph{transformer block} consists of  followed by a feedforward layer:

where , , , and .

In the standard setting, the  tokens are part of a sequence (e.g., language applications). However, we are concerned with the \emph{graph transformer} setting in which the tokens are nodes of some underlying graph  with . The attention computation is nearly identical, except that one can also optionally augment it  with edge features, as is done in SAN~\citep{kreuzer2021rethinking}:

where ,  is the  matrix whose columns are -dimensional edge features for the edges connected to node , and  denotes element-wise multiplications.


The most typical cases of graph transformers use full (dense) attention, where every token attends to every other node:  is the fully-connected directed graph. As this results in computational complexity  for the transformer block, which is prohibitively expensive for large graphs, we wish to replace full attention with a \emph{sparse} attention mechanism, where  has  edges -- ideally, . 

A number of sparse attention mechanisms have been proposed to address the aforementioned issue (see~\citealp{TayDBM20}), but the vast majority are designed specifically for functions on \emph{sequences}. \textsc{Exphormer}, on the other hand, is a \emph{graph-centric} sparse attention mechanism that makes use of the underlying structure of the input graph . We introduce three sparse patterns: expander graphs, global connectors, and local neighborhoods as three patterns that can be used in transformers. They can combine together to make an \textsc{Exphormer} layer, but it is not necessary to have all components in each layer. 

GraphGPS~\citep{RampasekGDLWB22} shows an effective way to include the transformer layers beside an MPNN model. We use the same architecture as GraphGPS, replacing only the transformer part with an \textsc{Exphormer} layer. We will show that sparse transformers can get comparable and even better results on many benchmarks compared to full transformers, or sparse transformer variants originally designed for sequences.


\subsection{The \textsc{Exphormer} Architecture}\label{sec:exphormerarch}

\begin{figure}[bt]
\vspace{-0.1in}
\captionsetup[subfloat]{}
\centering
\subfloat[][]{\includegraphics[width = 0.8in]{figures/Expander_1.pdf}} 
\hspace{0.3in}
\subfloat[][]{\includegraphics[width = 0.8in]{figures/Expander_2.pdf}}
\\
\subfloat[][]{\includegraphics[width = 0.8in]{figures/Expander_3.pdf}}
\hspace{0.3in}
\subfloat[][]{\includegraphics[width = 0.8in]{figures/Expander_4.pdf}}

\caption{The components of \textsc{Exphormer}: (a) shows local neighborhood attention, i.e., edges of the input graph. (b) shows an expander graph with degree 3. (c) shows global attention with a single virtual node. (d) All of the aforementioned components are combined into a single interaction graph that determines the attention pattern of \textsc{Exphormer}.}
\label{fig:exphormer}
\vspace{-0.1in}
\end{figure}

\begin{table*}[t]
    \vspace{-0.1in}
    \centering
    \caption{Comparison of \textsc{Exphormer} with baselines on various datasets. Best results are colored: \first{first}, \second{second}, \third{third}.}
    \label{tab:benchmarking}
    \fontsize{8.25pt}{8.25pt}\selectfont
    \setlength\tabcolsep{6.25pt} \scalebox{0.9}{
    \begin{tabular}{p{4cm}ccccc}
    \toprule
         {\bf Model} & {\bf CIFAR10} & {\bf MalNet-Tiny} & {\bf MNIST} & {\bf CLUSTER} & {\bf PATTERN} \\
          & {Accuracy } & {Accuracy } & {Accuracy } & {Accuracy } & {Accuracy }\\
    \midrule
    GCN {\tiny\cite{kipf2017semi}} & 55.710.381 & 81.0 & 90.710.218 &  68.50  0.976 & 71.89  0.334\\
    GIN {\tiny\cite{xu2018powerful}} & 55.261.527 & 88.980.557 & 96.490.252 &  64.72  1.553 & 85.39  0.136\\
    GAT {\tiny\cite{velickovic2018graph}} & 64.220.455 & 92.1 0.242 & 95.540.205 & 70.59  0.447 &  78.27  0.186\\
    GatedGCN {\tiny\cite{bresson2017residual,DwivediJLBB20}} \hangindent=1em & 67.310.311 & \third{92.230.65} & 97.340.143 & 73.84  0.326 & 85.57  0.088 \\
PNA {\tiny\cite{corso2020principal}} & 70.350.63 & -- & 97.940.12 & -- & -- \\
    DGN {\tiny\cite{beaini2021directional}} & \third{72.840.417} & -- & -- & -- & 86.680.034   \\
\midrule
CRaWl~{\tiny\cite{toenshoff2021CRaWl}} & 69.010.259 & -- &  97.940.050 & -- & -- \\
    GIN-AK+~{\tiny\cite{zhao2021stars}} & 72.190.13 & -- & -- & -- & \first{86.850.057}  \\
    \midrule
    SAN {\tiny \cite{kreuzer2021rethinking}} & -- & -- & -- & 76.690.65  & 86.580.037 \\
    K-Subgraph SAT {\tiny \cite{chen2022SAT}} & -- & -- & -- & 77.860.104  & \second{86.850.037} \\
    EGT {\tiny \cite{hussain2021edge}} & 68.700.409 & & \second{98.170.087} & \first{79.230.348} & \third{86.820.020} \\
    GraphGPS {\tiny\cite{RampasekGDLWB22}} &  \third{72.300.356} & \second{93.500.41} &  \third{98.050.126} &   \third{78.020.180}  &  86.690.059 \\
\midrule
    \rowcolor{lightyellow} \textsc{Exphormer} (ours) & \first{74.690.125} & \first{	94.02  0.209} & \first{98.55  0.039} & \second{78.07  0.037} & 86.740.015 \\
     \bottomrule
    \end{tabular}
    }
    
    
\end{table*}

We now describe the details of the construction of \textsc{Exphormer}, an expander-based sparse attention mechanism for graph transformers with  computation, where  is the underlying input graph. The \textsc{Exphormer} architecture constructs an interaction graph  that consists of three main components, as shown in \Cref{fig:exphormer}. The construction always has bidirectional edges, and so  can be viewed as an undirected graph.
The mechanism uses three types of edges:
\begin{enumerate}
    \item {\bf Expander graph attention}: Edges from a random \emph{expander graph} can be used as attention patterns. These graphs have several useful theoretical properties related to spectral approximation and random walk mixing (see~\cref{sec:theoreticalprop}), which allow propagating information between pairs of nodes that are distant in the input graph  without connecting all pairs of nodes. They introduce many alternative short paths between the nodes and avoid the information bottleneck that can be caused by the virtual nodes. In particular, we use a regular expander graph of constant degree, which allows the number of edges to be just . The process we use to construct a random expander graph is described below.
    
    \item {\bf Global attention}: The next component is \emph{global attention}, whereby a small number of virtual nodes are added to the interaction graph, and each such node is connected to all the non-virtual nodes. These nodes enable a global ``storage sink'' and they have universal approximator functions for full transformers. We will generally add a constant number of virtual nodes, in which case the total number of edges due to global attention will be .
    \item {\bf Local neighborhood attention}: Another desirable property to capture is \emph{locality}. Graphs carry much more topological structure than sequences, and the neighborhoods of individual nodes carry a lot of information about connectivity.
    Thus, we model local interactions by allowing each node  to attend to every other node that is an immediate neighbor of  in :
    that is,  includes the input graph edges  as well as their reverses, introducing  interaction edges.
    One generalization would be to allow direct attention within -hop neighborhoods, but this might introduce a superlinear number of interactions on general graphs.

    
\end{enumerate}

We use learnable embeddings for expander and global connection edge features, and virtual nodes' features. Dataset edge features are used for the local neighborhood edge features.
We always use local neighborhoods in \textsc{Exphormer} layers, but expander graphs and global attention are not always helpful; depending on the dataset, we may use both, or only one or the other.
We discuss this further in \cref{sec:expander-vs-global}.

Some previous graph-oriented transformers,
such as the SAN architecture \citep{kreuzer2021rethinking},
use separate attention mechanisms for different sources of edges.
By using a single attention mechanism,
\textsc{Exphormer} achieves a more compact model
that can still distinguish the ``types'' of attention edges based on edge features.


\paragraph{Generating a Random Regular Expander}
We now describe how we generate a random regular expander. Let  be the original graph, where . For the purposes of experimentation (in \cref{tab:benchmarking,tab:sparse-comparision,tab:lrgb,tab:largegraphs,tab:alations_comps}), we use the random graph process analyzed in \citet{Friedman03} (see \cref{thm:randomregular} in \cref{sec:expander-details}) to generate a random -regular graph  on the same node set :
\begin{itemize}[itemsep=2pt,parsep=2pt]
    \item Pick  permutations  on , each  chosen independently and uniformly among all possible permutations of  elements.
    \item Then, letting  denote , choose
    
\end{itemize}
The above process works for even , and \cref{thm:randomregular} shows that the resulting graph will be a -regular near-Ramanujan graph with high probability. In practice, we will generate expander graphs using this process and throw out any graphs that fail to be near-Ramanujan, according to a desired threshold (this is a low probability event).

We provide further details in \cref{sec:expander-details}, where we also discuss other algorithms for generating expander graphs.






\section{Theoretical Properties of \textsc{Exphormer}} \label{sec:theoreticalprop}
\textsc{Exphormer} is based on expander graphs, which have a number of properties that make them suitable as a key building block of our approach. In this section, we describe relevant properties of expander graphs along with their implications for \textsc{Exphormer}s.


\subsection{Expander Graphs Approximate Complete Graphs}
For simplicity, let us consider -\emph{regular} graphs (where every node has   neighbors). Suppose  is a -regular undirected graph on  vertices. Let  be the  adjacency matrix of . It is known that  has  real eigenvalues . The graph  is said to be an \emph{-expander} if  \citep{hoory06}.


Expander graphs are sparse approximations of complete graphs.
As we discuss,
expander graphs with only  edges
exist that can preserve certain desirable properties of the complete graph with  edges.

\subsubsection{Spectral Properties}
A useful tool to study expanders is the \emph{Laplacian} matrix of a graph, which captures several important spectral properties.
Letting  denote the  diagonal matrix whose -th diagonal entry is the degree of the -th node, we define  to be the Laplacian of .
The following theorem is well-known in spectral graph theory.
\begin{theorem} 
\citep[Section 27.2]{spielman:sagt}
 A -regular -expander  on  vertices spectrally approximates the complete graph  on  vertices:\footnote{For matrices  and , we say that  if  is a positive semi-definite matrix.}
 
\end{theorem}
Spectral approximation is known to preserve the cut structure in graphs. As a result, a sparse attention mechanism based on expander edges retains spectral properties of the full attention mechanism: cuts, vertex expansion, and so on.

\subsubsection{Mixing Properties}
Another property of expanders is that random walks mix well.
Let  be a -regular -expander. Consider a random walk  on , where
,
and then each subsequent  is one of the  neighbors of  chosen uniformly at random.
We then have , given recursively by . 
It turns out that after a logarithmic number of steps, a random walk from a starting probability distribution on the vertices is close to uniformly distributed along all nodes of the graph.


\begin{lemma}\label{lem:mixing}
\citep[Theorem 3.2]{hoory06}
Let  be a -regular -expander graph on  nodes. For any initial distribution  and any ,  satisfies

as long as
.
\end{lemma}


In an attention mechanism of a transformer, one can consider the graph of pairwise interactions (i.e.,  is connected to  if  and  attend to each other). If the attention mechanism is dense, then each node is connected to every other node and it is trivial to see that every pair of nodes interacts with each other in a single transformer layer. In a \emph{sparse} attention mechanism, on the other hand, some pairs of nodes are not directly connected, meaning that a single transformer layer will not model interactions between all pairs of nodes. However, if we stack transformer layers on top of each other, the stack will be able to model longer range interactions.  In particular, a consequence of the above lemma is that if our sparse attention mechanism is modeled after an -expander graph, then stacking at least  layers will model ``most'' pairwise interactions between nodes.


Relatedly, the {diameter} of expander graphs is asymptotically logarithmic in the number of nodes.
\begin{theorem}\citep[Section 2.4]{hoory06}
Suppose  is a -regular -expander graph on  vertices. Then, for every vertex  and , the -hop neighborhood  has
 
 for some constant  depending on 
 . 
 In particular, we have that .
\end{theorem}
As a consequence, we obtain the following, which shows that using logarithmically many successive transformer layers allows each node to propagate information to every node.
\begin{corollary}\label{cor:logdiam}
If a sparse attention mechanism on  nodes is a -regular -expander graph, then stacking  transformer layers models all pairwise node interactions.
\end{corollary}


\subsection{Universal Approximability of \textsc{Exphormer} Models}
The expander graph attention and global attention components of \textsc{Exphormer} ensure that a sublinear number of graph transformer layers is sufficient to allow each node to interact (directly or indirectly) with every other node, alleviating potential underreaching issues (even in the absence of global nodes,  layers are sufficient, by \Cref{cor:logdiam}). Nevertheless, a natural question is whether a sparse transformer model based on \textsc{Exphormer} allows universal approximation of functions.

Dense transformer models are known to be universal approximators, i.e., with the use of positional encodings, they can approximate any continuous sequence-to-sequence function on a compact domain arbitrarily closely~\citep{YunBRRK20}. In the realm of graph learning, this provides a compelling motivation for considering graph transformers over standard MPNNs, which are known to be limited in expressive power by the Weisfeiler-Lehman (WL) hierarchy.


Universal approximation properties for dense transformers do not automatically hold for sparse transformers, but we show every continuous function  can be approximated to any desired accuracy by an \textsc{Exphormer} network using either global attention or a suitable version of expander attention.  Details are in \cref{sec:proofs}.


\begin{table*}[bt]
\caption{Comparison of attention mechanisms in GPS. \textsc{Exphormer} outperforms other {sparse} transformer architectures (BigBird and Performer) while also beating the full transformer GPS models on three of four datasets. Best results are colored in \first{first}, \second{second}, \third{third}.}
\label{tab:sparse-comparision}
\centering
\fontsize{8.25pt}{8.25pt}\selectfont
\setlength\tabcolsep{6.25pt} \scalebox{1.0}{
\begin{tabular}{l|cccc} 
\toprule
Model/Dataset   & \textbf{Cifar10}       & \textbf{MalNet-Tiny}   & \textbf{PascalVOC-SP}    & \textbf{Peptides-Func}   \\ 
 & Accuracy  & Accuracy  & F1 score  & AP  \\
[0.1cm]
\midrule
GPS (MPNN-only)            & 69.948  0.499 & 92.23  0.65  & 0.3016  0.0031 & 0.6159  0.0048  \\ 
[0.05cm]
\midrule
GPS-BigBird     & 70.480  0.106 & 92.34  0.34  & 0.2762  0.0069 & 0.5854  0.0079 \\
[0.05cm]
GPS-Performer   & \third{70.670  0.338} & \third{92.64  0.78}  & \third{0.3724  0.0131} & \third{0.6475  0.0056} \\
[0.05cm]
GPS-Transformer & \second{72.305  0.344} & \second{93.50  0.41}  & \second{0.3736  0.0158} & \first{0.6535  0.0041} \\
[0.05cm]
\midrule
\textsc{Exphormer}       & \first{74.690.125}   & \first{94.02  0.21} & \first{0.3975  0.0037} & \second{0.6527  0.0043}  \\
\bottomrule
\end{tabular}
}
\end{table*}


\section{Experiments} \label{sec:experiments}
In this section, we evaluate the empirical performance of graph transformer models based on \textsc{Exphormer} on a wide variety of graph datasets with graph prediction and node prediction tasks~\cite{DwivediJLBB20,HuFZDRLCL20,FreitasDNC21,amazon_dataset,namata:mlg12-wkshp}. In particular, we present experiments on fifteen benchmark datasets, including image-based graph datasets (CIFAR10, MNIST, PascalVOC-SP, COCO-SP), synthetic SBM datasets (PATTERN, CLUSTER), code graph datasets (MalNet-Tiny), and molecular datasets (Peptides-Func, Peptides-Struct, PCQM-Contact). Moreover, we demonstrate the ability of \textsc{Exphormer} to allow graph transformers to scale to larger graphs (with more than 5,000 nodes) by including results on five transductive graph datasets consisting of citation networks (CS, Physics, ogbn-arxiv) and co-purchasing networks (Computer, Photo).


For the experimental setup, we combine \textsc{Exphormer} together with MPNNs in the GraphGPS framework~\citep{RampasekGDLWB22}, which constructs graph transformer models by composing attention mechanisms with message-passing schemes, together with an appropriate choice of positional and structural encodings.

In addition to comparisons with baselines on the aforementioned datasets, we also run ablation studies on the various attention components of \textsc{Exphormer} (from \cref{sec:exphormerarch}).


In summary, our experiments show that: (a) \textsc{Exphormer} achieves SOTA performance on a variety of datasets, (b) \textsc{Exphormer} consistently outperforms other sparse attention mechanisms while often surpassing dense transformers using fewer parameters, and (c) \textsc{Exphormer} successfully allows GraphGPS to overcome memory bottlenecks and scale to larger graphs (on  nodes) while still providing competitive performance.




\begin{table*}[t]
    \caption{Comparison of \textsc{Exphormer} with baselines from the Long-Range Graph Benchmarks (LRGB, \citealp{lrgb-2022}). Best results are colored in \first{first}, \second{second}, \third{third}.}
    \label{tab:lrgb}
    \fontsize{8.5pt}{8.5pt}\selectfont
    \setlength\tabcolsep{4pt} \centering
    \begin{tabular}{lccccc}\toprule
    \multirow{2}{*}{\textbf{Model}} &\textbf{PascalVOC-SP} &\textbf{COCO-SP} &\textbf{Peptides-Func} &\textbf{Peptides-Struct} &\textbf{PCQM-Contact} \\
    &F1 score  &F1 score  &AP  &MAE  &MRR  \\\midrule
    GCN &0.1268  0.0060 &0.0841  0.0010 &0.5930  0.0023 &0.3496  0.0013 &0.3234  0.0006 \\
    GINE &0.1265  0.0076 &0.1339  0.0044 &0.5498  0.0079 &0.3547  0.0045 &0.3180  0.0027 \\
    GatedGCN &0.2873  0.0219 &\third{0.2641  0.0045} &0.5864  0.0077 &0.3420  0.0013 &0.3218  0.0011 \\
    GatedGCN+RWSE &0.2860  0.0085 &0.2574  0.0034 &0.6069  0.0035 &0.3357  0.0006 &0.3242  0.0008 \\ \midrule
    Transformer+LapPE &0.2694  0.0098 & 0.2618  0.0031 &0.6326  0.0126 &\third{0.2529  0.0016} &0.3174  0.0020 \\
    SAN+LapPE &\third{0.3230  0.0039} &\phantom{*}0.2592  0.0158* &0.6384  0.0121 &0.2683  0.0043 &\second{0.3350  0.0003} \\
    SAN+RWSE & 0.3216  0.0027 &\phantom{*}0.2434  0.0156* &\third{0.6439  0.0075} &0.2545  0.0012 &\third{0.3341  0.0006} \\ 
    GraphGPS &\second{0.3748  0.0109} &\second{0.3412  0.0044} &\first{0.6535  0.0041} &\second{0.2500  0.0005} &0.3337  0.0006 \\ \midrule
    Exphormer (ours) & \first{0.3975  0.0037} & \first{0.3455  0.0009} & 	\second{0.6527  0.0043} & \first{0.2481  0.0007} & \first{	0.3637  0.0020} \\
    \bottomrule
    \end{tabular}
\end{table*}

\subsection{Comparison to Sparse Attention Mechanisms}\label{sec:attentioncomp}
We first discuss a comparison of our \textsc{Exphormer}-based models with other \emph{sparse} transformer architectures, which are a natural first point of comparison. In particular, we perform a series of experiments that compare \textsc{Exphormer}-based architectures with sparse transformer baselines in the GraphGPS framework. More specifically, for each dataset, we compare our best \textsc{Exphormer} model with GraphGPS models that use BigBird and Performer for the underlying attention mechanism.


The results are shown in \Cref{tab:sparse-comparision}. Note that on each of the four highlighted datasets, an \textsc{Exphormer} model outperforms the sparse attention models GPS-BigBird and GPS-Performer. Furthermore, it even outperforms the full (dense) transformer model (GPS-Transformer) on three of the datasets while performing competitively on the fourth. Since GraphGPS models make use of both attention (transformer) and message passing components, we additionally point out that the \textsc{Exphormer}-based sparse attention component is, indeed, providing lift over a standard MPNN baseline that does not make use of attention at all.



\subsection{Benchmarking GNNs Datasets} \label{sec:baselines}
We have showed in \Cref{sec:attentioncomp} that \textsc{Exphormer} consistently outperforms relevant sparse attention baselines. The natural next question is how \textsc{Exphormer} performs relative to a wider range of baselines, including not just graph transformer models but also other architectures such as MPNNs.

\Cref{tab:benchmarking} shows results on five datasets, including four from the Benchmarking GNNs collection~\citep{DwivediJLBB20} as well as the code graph dataset MalNet-Tiny~\citep{FreitasDNC21}.  We note that an \textsc{Exphormer}-based graph transformer with message-passing in the GraphGPS framework yields \emph{state-of-the-art (SOTA) performance on three of the datasets} and is competitive with the best single model accuracies on the remaining datasets.


Observe that on all five datasets, our \textsc{Exphormer} models provide better accuracy than GraphGPS models based on \emph{full (dense) transformers}. Additionally, the \textsc{Exphormer} models outperform the full transformer-based SAN model as well as a variety of MPNN baselines. 


\begin{remark}
Our \textsc{Exphormer} models often outperform full transformer models with a much smaller number of parameters. For instance, on PATTERN, our results reported in \Cref{tab:benchmarking} are obtained from an \textsc{Exphormer} model with just 90,000 parameters, compared to 340,000 parameters in the comparable full transformer GraphGPS model! Similarly, on CLUSTER, our \textsc{Exphormer} model uses just 280,000 parameters, as compared to 500,000 parameters for the full transformer GraphGPS model. This results in simpler models with time and memory advantages. For further details on hyperparameters, see \Cref{tab:hyperparams-gps-1} in the appendix.
\end{remark}

\begin{remark}\label{rmk:batchsize}
\textsc{Exphormer} models often enable larger batch sizes during training. On MalNet-Tiny, which contains some graphs with around 5,000 nodes, the full transformer GraphGPS model runs out of memory on a 40GB NVIDIA A100 GPU when using a batch size of just 16, while our sparse \textsc{Exphormer} models handle a batch size of 256.
\end{remark}
\begin{table*}[!t]
    \centering
    \caption{Accuracy of models with different attention mechanisms on transductive graph datasets (numbers in top rows, other than arXiv, are from \citealp{nagphormer22}).  \citeauthor{nagphormer22} did not report NAGphormer results on this dataset.}
    \label{tab:largegraphs}
\setlength\tabcolsep{7pt} \scalebox{0.75}{
    \begin{tabular}{lccccc}
\toprule
         {\bf Model} & {\bf ogbn-arxiv} & {\bf Computer}  & {\bf Photo} & {\bf CS}  & {\bf Physics}\\
         \midrule
         SAN & OOM & 89.83  0.16 & 94.86  0.10 & 94.51  0.15 & OOM \\
         GraphGPS &  OOM & OOM & 95.06  0.13& 93.93  0.12 & OOM \\
         NAGphormer & NA & 91.22  0.14 & 95.49  0.11 & 95.75  0.09 & 97.34  0.03 \\
\midrule
         \textsc{Exphormer} & 72.44  0.28 & 91.59  0.31& 	95.27  0.42 & 95.77  0.15 & 97.16  0.13 \\
 \bottomrule
    \end{tabular}
    }
\end{table*}
\begin{table*}[ht!]
\vspace{-0.1in}
\centering
\caption{Results for the full model versus removing each of the components. For the Peptides-Struct dataset, lower is better; for all the others, higher is better.}
\smallskip
\label{tab:alations_comps}
\setlength{\tabcolsep}{5pt}
\renewcommand{\arraystretch}{1.5}
\scalebox{0.75}{
\begin{tabular}{l|l|l|l|l} 
\toprule
Dataset & No Local Edges & No Expander Edges & No Global Nodes & All Components \\ 
\hline
Cifar10  
 &  &   &  &   \\ 
\hline
Malnet-Tiny 
 &  &  &  &   \\
 \hline
Pattern &  &  &  & \\
\hline
PascalVOC-SP &  &  &  & \\
\hline
Peptides-Struct &  &  &  & \\
\hline
Computer & 	 &  &  & \\
\bottomrule
\end{tabular}
}
\end{table*}

\subsection{Long-Range Graph Benchmark}
We have additionally conducted a set of experiments on the Long-Range Graph Benchmark (LRGB, \citealp{lrgb-2022}), which consists of five challenging datasets that test a model's ability to learn \emph{long-range dependencies} in input graphs. The results are presented in \Cref{tab:lrgb}, which shows that our \emph{\textsc{Exphormer}-based sparse attention models are able to outperform GraphGPS on three of the five datasets, achieving SOTA performance}. Furthermore, on the remaining two datasets in the LRGB suite, \textsc{Exphormer} is competitive with the best single model results, obtained by full attention GraphGPS models.


\subsection{Scaling to Larger Graphs}
One of the shortcomings of graph transformer architectures has been their poor scalability to larger graphs on thousands of nodes. Dense attention mechanisms (e.g., SAN and Graphormer) have quadratic memory complexity and time complexity, which restrict their applicability to datasets on graphs with relatively few nodes, such as molecular graphs.

GraphGPS has made use of sparse attention mechanisms, but even so, the architecture handles graphs of up to about 5,000 nodes (e.g., MalNet-Tiny,~\citealp{FreitasDNC21}), often with very small batch size (see \Cref{rmk:batchsize}).



A natural question is whether the sparse attention mechanism of \textsc{Exphormer} models allows us to extend graph transformer architectures to significantly larger graphs while providing competitive performance. Indeed, we show this is the case by training \textsc{Exphormer} models on larger datasets, including Amazon Computer, Amazon Photo, Coauthor CS, Coauthor Physics~\citep{amazon_dataset}, which has up to 35K nodes and 250K edges. We also show the use of \textsc{Exphormer} on ogbn-arxiv, which has 169K nodes and 1.1M edges. The results are shown in \Cref{tab:largegraphs}. Standard GraphGPS models suffer from Out Of Memory (OOM) issues on a number of these datasets, demonstrating the utility of \textsc{Exphormer}. To provide meaningful comparisons to other transformer architectures, we thus include NAGphormer~\citep{nagphormer22}, a scalable sampling-based graph transformer architecture, as a baseline. \textsc{Exphormer} performs competitively and, in fact, achieves SOTA accuracy on Computer: the best non-transformer method has an accuracy of 90.74~\citep{computersSOTA}.

We would also like to highlight that \textsc{Exphormer} can be used for even larger graphs, with hundreds of thousands of nodes. In particular, we provide results on ogbn-arxiv~\citep{HuFZDRLCL20}, a challenging transductive dataset consisting of a single large graph of the citation network of over 160K arXiv papers, containing over a million edges.
Specifically, we achieve a test accuracy of {\bf 0.7244} using the \textsc{Exphormer} architectures. At the time of writing, a relevant leaderboard~\citep{ogbleaderboard} shows 0.7966 as the highest reported test accuracy~\citep{zhao:variational}. \Cref{tab:arxiv} compares \textsc{Exphormer} to other MPNNs and sparse transformers. 
A dense full transformer does not even fit in memory on a 40GB NVIDIA A100 GPU (even for a tiny network of only 2 layers and 32 hidden dimensions). We then fixed the network size to 3 hidden layers and 96 hidden dimensions to be able to fit the sparse models in memory. Notice that BigBird and Performer have significantly longer epoch times and worse performance than with just a MPNN. \textsc{Exphormer} with virtual nodes only (without expander edges) improves on the MPNN and provides an accuracy of 0.7222. \textsc{Exphormer} with expander edges further improves the accuracy to 0.7244.















\subsection{Ablation Studies}
\label{sec:ablation}
Here we analyze the effect of each of the components of the model. Our \textsc{Exphormer} model has three main components: local neighborhood, expander edges, and virtual nodes. In \cref{tab:alations_comps}, we analyze which parts of the model have been useful, and which parts can be removed without reducing the performance. In all of these experiments, the MPNN part is fixed, so it gives a baseline number for the results, and the transformer part boosts over the MPNN part.
Through these experiments we can see that local neighborhood have been always a good option to add to the \emph{Exphormer}, but between virtual nodes and expander graphs, sometimes one of them causes reduction in the performance.
This issue is discussed further in \cref{sec:expander-vs-global}.




\section{Conclusion}

\textsc{Exphormer} is a new class of sparse graph transformer architectures. We introduced two types of sparse networks with virtual nodes and using expander graphs. We have shown that the mathematical properties of our architecture make \textsc{Exphormer} a suitable choice for graph learning. Our sparse architecture uses fewer training parameters, is faster to train and has memory complexity linear in the size of the graph.  These properties help us scale to larger graphs which were typically elusive to other Transformer-based methods. \textsc{Exphormer} outperforms other sparse transformers and performs comparably or better than full transformers. We have shown the applicability of \textsc{Exphormer} on a wide variety of graph learning datasets. Combining \textsc{Exphormer} with MPNN in the GraphGPS framework allows us to obtain state-of-the-art empirical results on a number of datasets. 

\section*{Acknowledgements}
This work was supported in part by the Natural Sciences and Engineering Resource Council of Canada, the Canada CIFAR AI Chairs program, the BC DRI Group, Compute Ontario, Calcul Qu\'ebec, and the Digital Resource Alliance of Canada.

\bibliography{bibliography}
\bibliographystyle{icml2023}


\newpage
\appendix
\onecolumn


\section{Dataset Descriptions} \label{sec:datasetdesc}
Below, we provide descriptions of the datasets on which we conduct experiments.

\paragraph{CIFAR10 and MNIST} \citep{DwivediJLBB20}
CIFAR10 and MNIST are the graph equivalents of the image classification datasets of the same name. A graph is created by constructing the 8-nearest neighbor graph of the SLIC superpixels of the image. These are both 10-class graph classification problems.

\paragraph{PascalVOC-SP and COCO-SP} \citep{dwivedi2021graph}
These are similar graph versions of image datasets, but they are larger images and the task is to perform node classification, i.e.\ semantic segmentation of superpixels.
These graphs are larger, and the task more complex, than CIFAR10 and MNIST.

\paragraph{CLUSTER and PATTERN} \citep{DwivediJLBB20}
PATTERN and CLUSTER are node classification problems. Both are synthetic datasets that are sampled from a Stochastic Block Model (SBM), is a popular way to model communities. In PATTERN, the prediction task is to identify if a node belongs to one of the 100 possible predetermined subgraph patterns.  In CLUSTER, the goal is to classify nodes into six different clusters with the same distribution.

\paragraph{MalNet-Tiny} \citep{FreitasDNC21}
Malnet-Tiny is a smaller dataset generated from a larger dataset for identifying malware based on function call graphs from Android APKs. The tiny dataset contains 5000 graphs, each with up to 5000 nodes. The task is to predict the graph as being benign or from one of four types of malware.





\paragraph{ogbn-arxiv} \citep{ogbPaper}
The ogbn-arxiv dataset consists of one large
directed graph of 169343 nodes and 1,166,243 edges representing a citation network between all
computer science papers on arXiv that were indexed by the
Microsoft academic graph. Nodes in the graph represent
papers, while a directed edge indicates that a paper cites
another. Each node has an 128-dimensional feature vector
derived from embeddings of words in the title and abstract
of the underlying paper. The prediction task is a 40-class
node classification problem --- to identify the primary
category of each arXiv paper, as listed by the authors.
Moreover, the nodes of the citation graph are split into
90K training nodes, 30K validation notes, and  48K test nodes.

\paragraph{Coauthor datasets} 
Coauthor CS and Physics are co-authorship graphs from Microsoft Academic Graph.  The nodes represent the authors
and two authors who share a paper are connected by an edge.  The node features are from the keywords in the papers.
The class represent the active area of study for the author.

\paragraph{Amazon datasets}
Amazon Computers and Amazon photo are Amazon co-purchase graphs. Nodes represents products purchased an edges indicate
pairs of products purchased together.  Node features are bag-of-words encoded reiews of the products. Class labels are the product category.

\paragraph{Peptides-Func, Peptides-Struct, and PCQM-Contact} \citep{dwivedi2021graph} These datasets are molecular graphs introduced as a part of the Long Range Graph Benchmark (LRGB). Graphs in these datasets have relatively large diameters: the average diameter for PCQM-Contact is , and for the Peptides datasets . Average shortest path lengths are  and   accordingly. On PCQM-Contact the task is edge-level, and we need to rank the edges. Peptides-Func is a multi-label graph classification task, with 10 labels. Peptides-Struct is graph-level regression of 11 structural properties of the molecules.

\Cref{table:datasets} shows a summary of the statistics of the
aforementioned datasets.

\begin{table}[htp]
    \centering
    \caption{Dataset statistics}
    \fontsize{8.5pt}{8.5pt}\selectfont
    \setlength\tabcolsep{6pt} \scalebox{1}{
    \begin{tabular}{lcccccc}
    \toprule
         {\bf Dataset} & {\bf Graphs} & {\bf Avg. nodes} & {\bf Avg. edges} & {\bf Prediction Level} & {\bf No. Classes} &{\bf Metric} \\
    \midrule
MNIST & 70,000&  70.6 & 564.5 & graph& 10 & Accuracy\\
CIFAR10 & 60,000 & 117.6 & 941.1 &  graph & 10 & Accuracy\\
PATTERN & 14,000 & 118.9 & 3,039.3  & inductive node & 2 & Accuracy\\
CLUSTER & 12,000 & 117.2 & 2,150.9  & inductive node & 6 & Accuracy\\
MalNet-Tiny & 5,000 & 1,410.3 & 2,859.9 & graph & 5 & Accuracy\\
\midrule
PascalVOC-SP & 11,355 & 479.4 & 2,710.5 & inductive node & 21 & F1\\
COCO-SP & 123,286 & 476.9 & 2,693.7 & inductive node & 81 & F1\\
PCQM-Contact & 529,434 & 30.1 & 61.0 & inductive link & (link ranking) & MRR\\
Peptides-func & 15,535 & 150.9 & 307.3 & graph & 10 & Average Precision\\
Peptides-struct & 15,535 & 150.9 & 307.3 & graph & 11 (regression) & Mean Absolute Error\\
\midrule
ogbn-arxiv & 1 & 169,343 & 1,166,243 & node & 40 & Accuracy\\
Amazon Computer & 1  & 13381 & 245778 & node & 10 & Accuracy \\
Amazon Photo &2 & 7487 & 119043 & node & 8 & Accuracy \\
Coauthor CS & 1 & 18333 & 81894 & node & 15 & Accuracy \\
Coauthor Physics & 1 & 34493 & 247962 & node & 5 & Accuracy \\
    \bottomrule
    \end{tabular}
    }
     \label{table:datasets}
\end{table}

\section{More Experimental Results}


\subsection{Hyperparameters}
Our hyperparameter choices, including the optimizer, positional encodings, and structural encodings, were guided by the instructions in GraphGPS \citep{RampasekGDLWB22}. There were some cases, however, when more layers with smaller dimensions gave better results in \textsc{Exphormer}. This may be due to the fact that each node gets fewer inputs for each layer, but \textsc{Exphorme} requires more layers in order to propagate well. Additionally, we observed that Equivstable Laplacian Positional Encoding (ESLapPE) performed better than normal Laplacian Positional Encoding (LapPE) in some cases, in cases we replaced you can see ESLapPE version of GraphGPS results in \Cref{tab:ablationcifar,tab:MNIST,tab:cluster,tab:pattern}. Except for the large scale graphs, which we use GCN, we have used CustomGatedGCN beside the Exphormer in all of the experiments.

Through our model, some extra hyperparameters are introduced --- the degree of the graph expander and the number of virtual nodes. For these hyperparameters, we used linear search and found that expander degree 6-22 was the most effective. Depending on the graph size, we used 1-6 virtual nodes. As the number of hyperparameters is large, grid search was not feasible on all the parameters. As a result, for other hyperparameters, we did a linear search for each parameter to find out which parameters work better.


To make fair comparisons, we used a similar parameter-budget to GraphGPS. For PATTERN and CLUSTER, we used a parameter-budget of 500K, and for CIFAR and MNIST, we used a parameter-budget of around 100K. See details in \Cref{tab:hyperparams-gps-1,tab:hyperparams-lrgb,tab:hyperparams-large}.



\begin{table}[ht]
\centering
\caption{Hyperparameters used for \textsc{Exphormer} for datasets: CIFAR10, MNIST, MalNet-Tiny, PATTERN, CLUSTER.}
\label{tab:hyperparams-gps-1}
\begin{tabular}{l|lllll} 
\toprule
{\bf Hyperparameter}    & {\bf CIFAR10} & {\bf MNIST}   & {\bf MalNet-Tiny} & {\bf PATTERN} & {\bf CLUSTER}  \\ 
\hline
Num Layers        & 5       & 5       & 5           & 4       & 20       \\
Hidden Dim        & 40      & 40      & 64          & 40      & 32       \\
Num Heads         & 4       & 4       & 4           & 4       & 8        \\
Dropout           & 0.1     & 0.1     & 0           & 0.0     & 0.0      \\
PE                & ESLapPE & ESLapPE & None        & ESLapPE & ESLapPE  \\ 
\midrule
Batch Size        & 16      & 16      & 16          & 32      & 16       \\
Learning Rate     & 0.001   & 0.001   & 0.0005      & 0.0002   & 0.0002  \\
Num Epochs        & 150     & 150     & 150         & 120     & 200      \\ 
\midrule
Expander Degree   & 10       & 10       & -           & 14       & 6        \\
Num Virtual Nodes & 1       & 1       & 4           & 4       & 3        \\
Num parameters    & 111,095 & 111,015 & 286,277     & 91,045 & 282,970  \\
\bottomrule
\end{tabular}

\end{table}


\begin{table}[ht]
\centering
\caption{Hyperparameters used for \textsc{Exphormer} for LRGB datasets.}
\label{tab:hyperparams-lrgb}
\begin{tabular}{l|lllll} 
\toprule
{\bf Hyperparameter}    & {\bf PascalVOC-SP} & {\bf COCO-SP}   & {\bf Peptides-Func} & {\bf Peptides-Struct} & {\bf PCQM-Contact}  \\ \hline
Num Layers        & 4       & 7       & 8           & 4       & 7       \\
Hidden Dim        & 96      & 72      & 64          & 88      & 64       \\
Num Heads         & 8       & 4       & 4           & 4       & 4        \\
Dropout           & 0.15    & 0       & 0.12        & 0.12    & 0      \\
PE                & LapPE   & LapPE   & LapPE       & LapPE   & LapPE  \\ 
\hline
Batch Size        & 32      & 32      & 128         & 128     & 128       \\
Learning Rate     & 0.0005  & 0.0005  & 0.0003      & 0.0003  & 0.0003    \\
Num Epochs        & 300     & 300     & 200         & 200     & 200      \\ 
\hline
Expander Degree   & 14       & 22       & -           & -       & -        \\
Num Virtual Nodes & 0       & 0       & 1           & 6       & 6        \\
Num parameters    & 509,301 & 498,993 & 445,866     & 426,427 & 395,936  \\
\bottomrule
\end{tabular}

\end{table}

\begin{table}[ht]
\centering
\caption{Hyperparameters used for \textsc{Exphormer} for large transductive graph datasets.}
\label{tab:hyperparams-large}
\begin{tabular}{l|lllll} 
\toprule
{\bf Hyperparameter}    & {\bf OGBN-Arxiv} & {\bf Computer}   & {\bf Photo} & {\bf CS} & {\bf Physics}  \\ \hline
Num Layers              & 3       & 4       & 4           & 4       & 4       \\
Hidden Dim              & 96      & 80      & 64         & 72      & 72       \\
Num Heads               & 2       & 2       & 2           & 2       & 2        \\
Dropout                 & 0.3     & 0.4    & 0.4         & 0.4     & 0.4      \\
PE                      & -       & -       & -           & -   & -  \\ 
\hline
Learning Rate           & 0.01    & 0.001   & 0.001       & 0.001  & 0.001    \\
Num Epochs              & 600     & 150     & 100         & 70     & 70      \\ 
\hline
Expander Degree         & 6       & 6       & 6           & 6       & 6        \\
Num Virtual Nodes       & 0       & 0       & 0           & 0       & 0        \\
Num parameters          & 268,264 & 302,570 & 202,632     & 686,103 & 801,293  \\
\bottomrule
\end{tabular}

\end{table}



\subsection{Full Comparison of Attention Mechanisms} \label{app:attncomp}
\begin{remark}
\textsc{Exphormer} has some conceptual similarities with BigBird, as mentioned previously. For instance, we also make use of virtual global attention nodes, corresponding to \textsc{BigBird-etc}.

However, our approach departs from that of BigBird in some important ways. While BigBird uses -width ``window attention'' to capture locality of reference, we use local neighborhood attention to capture locality and graph topology. In particular, the interaction graph due to window attention in BigBird can be viewed as a Cayley graph on , which is sequence-centric, while \textsc{Exphormer} is graph-centric and, therefore, uses the structure of the input graph itself to capture locality.
BigBird, as implemented for graphs by \citet{RampasekGDLWB22}, instead simply orders the graph nodes in an arbitrary sequence and uses windows within that sequence.

Both BigBird and \textsc{Exphormer} also make use of a random attention model. While BigBird uses an Erd\H{o}s-R\'{e}nyi graph on  nodes, our approach is to use a -regular expander for fixed constant . The astute reader may recall that a Erd\H{o}s-R\'{e}nyi graph  has spectral expansion properties for large enough . However, it is known that  is the connectivity threshold, i.e., for ,  is almost surely a disconnected graph. Therefore, in order to obtain even a connected graph in the Erd\H{o}s-R\'{e}nyi model -- let alone one with expansion properties -- one would need , giving superlinear complexity for the number of edges.
BigBird uses , keeping a linear number of edges but losing expansion properties.
Our expander graphs, by contrast, allow both a linear number of edges and guaranteed spectral expansion properties.

We will see in the practical experiments of \cref{sec:experiments} that \textsc{Exphormer}-based models often substantially outperform BigBird-based equivalents, with fewer parameters.
\end{remark}


In \Cref{sec:attentioncomp}, we presented two approaches for the comparison of models trained using different attention mechanisms --- fixing the hyperparameters and fixing a budget on the total number of trainable parameters. The results showed the advantage of \textsc{Exphormer} over other attention mechanisms for CIFAR10 (\Cref{tab:ablationcifar}) and PATTERN (\Cref{tab:pattern}). Here, we present similar results for the remaining datasets --- MNIST in \cref{tab:MNIST}; MalNet-Tiny in \Cref{tab:malnet}; PATTERN in \Cref{tab:pattern}; and CLUSTER in \Cref{tab:cluster}.



\begin{table*}[t!]
\centering
    \caption{Results with varying attention and MPNNs on CIFAR10. \textsc{Exphormer} with MPNN provides the highest accuracy. Also, pure transformer models based on \textsc{Exphormer} (without the use of MPNNs) are comparable.}
\label{tab:ablationcifar}
    \setlength\tabcolsep{7pt} \scalebox{0.8}{
    \begin{tabular}{lcccccca}
    \toprule
         {\bf Model} & {\bf \#Layers} & {\bf Hidden} & {\bf \#Positional} & {\bf Expander} & {\bf \#Parameters} & {\bf Time}& {\bf Accuracy} \\
          & & {\bf layers} & {\bf encoding} & {\bf degree} & & {\bf Epch/Total} & {\bf } \\
     \midrule
         GPS-MPNN: GatedGCN & 3 & 52 & LapPE & - & 79,654 & 43s/1.18h &  \\
         GPS: Transformer & 3 & 52 & LapPE & - & 70,762 & 40s/1.11h & \\
         GPS: Transformer + MPNN & 5 & 40 & ESLapPE & - & 111,735 & 104s/2.89h &  \\
         GPS: Transformer + MPNN & 3 & 52 & LapPE & - & 112,726 & 62s/1.72h &  \\
         GPS: Performer + MPNN & 5 & 40 & ESLapPE & - & 283,935 & 132s/3.65h &  \\
         GPS: Performer + MPNN & 3 & 52 & LapPE & - & 239,554 & 77s/2.14h &  \\
         GPS: BigBird + MPNN & 5 & 40 & ESLapPE & - & 128,335 & 243s/6.75h &  \\
         GPS: BigBird + MPNN & 3 & 52 & LapPE & - & 129,418 & 145s/4h &  \\
     \midrule
         \textsc{Exphormer} w/o MPNN& 5 & 44 & ESLapPE & 10 & 84,134 & 64s/1.78h & \\
         \textsc{Exphormer} w/o MPNN& 7 & 44 & ESLapPE & 10 & 119,022 & 80s/2.23h &  \\
         \textsc{Exphormer} & 5 & 40 & ESLapPE & 10 & 111,095 & 115s/3.21h &  \\
         \textsc{Exphormer} & 5 & 44 & ESLapPE & 10 & 133,819 & 114s/3.19h& \bf{75.03  0.186} \\
     \bottomrule
    \end{tabular}
    }

\end{table*}

\begin{table}[ht]
    \centering
    \caption{Ablation studies results for MNIST}
    \label{tab:MNIST}
    \scalebox{0.8}{
    \begin{tabular}{lcccccca}
    \toprule
         {\bf Model} & {\bf \#Layers} & {\bf Hidden} & {\bf \#Positional} & {\bf Expander} & {\bf \#Parameters} & {\bf Time} & {\bf Accuracy} \\
          & & {\bf layers} & {\bf encoding} & {\bf degree} & & {\bf Epoch/Total} & {\bf } \\
     \midrule
         GPS: Transformer + MPNN & 5 & 40 & ESLapPE & - & 111,655 & 131s/5.45h &  \\
         GPS: Transformer + MPNN & 3 & 52 & LapPE & - & 115,394 & 76s/2.13h &  \\
         GPS: Performer + MPNN & 5 & 40 & ESLapPE & - & 283,855 & 156s/6.52h &  \\
GPS: BigBird + MPNN & 5 & 40 & ESLapPE & - & 128,255 & 267s/11.11h &  \\
\midrule
         \textsc{Exphormer} w/o MPNN & 5 & 44 & ESLapPE & 10 & 92,146 & 75s/3.14h &  \\
         \textsc{Exphormer} w/o MPNN & 7 & 44 & ESLapPE & 10 & 127,698 & 93s/3.87h &   \\
         \textsc{Exphormer} & 5 & 40 & ESLapPE & 10 & 111,015 & 132s/5.49h &  \\
         \textsc{Exphormer} & 5 & 44 & ESLapPE & 10 & 133,731 & 137s/5.72h & \bf{98.424  0.018} \\
     \bottomrule
    \end{tabular}
    }
\end{table}


\begin{table}[ht]
    \centering
    \caption{Ablation studies results for MalNet-Tiny. We want to remark that these results are based on one virtual node. For the best result reported in the main paper, we have used 4 virtual nodes, which led to much better numbers. The model marked with * did not fit in memory with batch size 16, and was trained with batch size 8.}
    \label{tab:malnet}
    \label{tab:malnettiny}
    \scalebox{0.8}{
    \begin{tabular}{lcccccca}
    \toprule
         {\bf Model} & {\bf \#Layers} & {\bf Hidden} & {\bf \#Positional} & {\bf Expander} & {\bf \#Parameters} & {\bf Time} & {\bf Accuracy} \\
          & & {\bf layers} & {\bf encoding} & {\bf degree} & & {\bf Epoch/Total} & {\bf } \\
     \midrule
         GPS-MPNN: GatedGCN & 5 & 64 & - & - & 199,237 & 6s/0.25h &  \\
         GPS: Performer & 5 & 64 & - & - & 421,957 & 41s/1.73h &  \\
         GPS: Transformer + MPNN* & 5 & 64 & - & - & 282,437 & 94s/3.94h & \bf{93.50  0.41} \\
         GPS: Performer + MPNN & 5 & 64 & - & - & 527,237 & 46s/1.90h &  \\
         GPS: BigBird + MPNN & 5 & 64 & - & - & 324,357 & 130s/5.43h &  \\
     \midrule
         \textsc{Exphormer} w/o MPNN & 5 & 80 & - & 10 & 283,173 & 25.2s/1.05h &  \\
         \textsc{Exphormer} w/o MPNN & 8 & 64 & - & 10 & 296,325 & 35.2s/1.47h &  \\
         \textsc{Exphormer} & 5 & 64 & - & 10 & 286,277 & 25.1s/1.05h &  \\
     \bottomrule
    \end{tabular}
    }
\end{table}

\begin{table}[htp]
    \centering
    \caption{Ablation studies results for PATTERN}
    \label{tab:pattern}
    \scalebox{0.8}{
    \begin{tabular}{lcccccca}
    \toprule
         {\bf Model} & {\bf \#Layers} & {\bf Hidden} & {\bf \#Positional} & {\bf Expander} & {\bf \#Parameters} & {\bf Time} & {\bf Accuracy} \\
          & & {\bf dimension} & {\bf encoding} & {\bf degree} & & {\bf Epoch/Total} & {\bf } \\
     \midrule
         GPS: Transformer + MPNN & 4 & 40 & ESLapPE & - & 91,165 & 18s/0.59h &   \\
         GPS: Transformer + MPNN & 6 & 64 & LapPE-16 & - & 337,201 & 32s/0.89h &  \\
         GPS: Performer + MPNN & 4 & 40 & ESLapPE & - & 228,925 & 20s/0.68h &  \\
         GPS: BigBird + MPNN & 4 & 40 & ESLapPE & - & 104,445 & 29s/0.97h &  \\
     \midrule
         \textsc{Exphormer} w/o MPNN & 4 & 40 & ESLapPE & 14 & 56,801 & 13s/0.44h & \\
         \textsc{Exphormer} w/o MPNN & 4 & 56 & ESLapPE & 14 & 109,985 & 14s/0.47h &  \\
         \textsc{Exphormer} & 4 & 40 & ESLapPE & 14 & 91,045 & 21s/0.71h & \bf{86.734  0.008} \\
     \bottomrule
    \end{tabular}
    }
\end{table}


\begin{table}[hb]
    \centering
    \caption{Ablation studies results for CLUSTER}
    \label{tab:cluster}
    \scalebox{0.8}{
    \begin{tabular}{lcccccca}
    \toprule
         {\bf Model} & {\bf \#Layers} & {\bf Hidden} & {\bf \#Positional} & {\bf Expander} & {\bf \#Parameters} & {\bf Time} & {\bf Accuracy} \\
          & & {\bf dimension} & {\bf encoding} & {\bf degree} & & {\bf Epoch/Total} & {\bf } \\
     \midrule
GPS: Transformer + MPNN & 20 & 32 & ESLapPE & - & 285,210 & 91s/3.79h &  \\
         GPS: Transformer + MPNN & 16 & 48 & LapPE-10 & - & 502,054 & 86s/2.40h &  \\
GPS: Performer + MPNN & 20 & 32 & ESLapPE & - & 1,512,090 & 120s/5.01h & \bf{78.292  0.046} \\
GPS: BigBird + MPNN & 20 & 32 & ESLapPE & - & 328,090 & 224s/9.32h &  \\
     \midrule
         \textsc{Exphormer} w/o MPNN & 20 & 32 & ESLapPE & 6 & 171,590 & 55s/3.06h & \\
         \textsc{Exphormer} & 20 & 32 & ESLapPE & 6 & 282,970 & 102s/5.69h &  \\
     \bottomrule
    \end{tabular}
    }
    
\end{table}


\begin{table}[htbp]
\centering
    \caption{Comparison of attention mechanisms on the ogbn-arxiv dataset by fixing the network size.}
    \label{tab:arxiv}
    \fontsize{8.5pt}{8.5pt}\selectfont
    \scalebox{1}{
    \begin{tabular}{lccc}
    \toprule
         {\bf Model} & {\bf Accuracy} & {\bf Epoch times} & {\bf \#Parameters} \\
         \midrule
         \rowcolor{lightyellow} \textsc{GCN+Exphormer} with expander edges &  72.44  0.28 & 1.72 & 268,552  \\
         \rowcolor{lightyellow} \textsc{GCN+Exphormer} with virtual nodes & 72.22  0.13 & 1.76 & 269,704 \\
         GCN only & 71.35  0.31 & 0.21 &  74,152 \\
         GCN+BigBird & 71.16  0.19 & 17.85 &  325,480 \\
         GCN+Performer &  70.92  0.04 & 5.77 & 452,776 \\
 \bottomrule
    \end{tabular}
}
    \vspace{-0.1in}
\end{table}

\section{Details of Expander Graph Construction} \label{sec:expander-details}
A major component of \textsc{Exphormer} is the use of the edges of an expander graph. In this section, we provide details of the specific expander graphs we use as well and quantify their spectral expansion  properties.

\subsection{Ramanujan Graphs}
A natural question is how strong the spectral expansion properties of a -regular graph can be, i.e., for how large an  does a -regular -expander exist. The following theorem gives a bound on how large the spectral gap can be.
\begin{theorem}[Alon-Boppana]
 Let . The eigenvalues of the adjacency matrix of a -regular graph on  nodes satisfy 
 
 In other words, a -regular -expander graph can exist only for .
\end{theorem}

As it turns out, there exist -expander graphs with  achieving this bound. In fact, a -regular -expander graph satisfying  is known as a \emph{Ramanujan graph}. Ramanujan graphs are essentially the best possible spectral expanders, and several constructions have been considered over the years~\citep{LubotzkyPS88, Margulis88}.

\subsection{Random Regular Graphs}
While there exist deterministic constructions of Ramanujan graphs, they are often algebraic/number theoretic in nature and therefore exist only for specific choices of  (e.g., the constructions of \citet{LubotzkyPS88} as well as independently of \citet{Margulis88}, for which one requires  and  to be a prime). Recently, the work of \citet{Alon21} showed a construction of strongly explicit near-Ramanujan graphs of every degree, but it should be noted that the construction needs the number of nodes to be sufficiently large. It is, therefore, often convenient to use a probabilistic construction of an expander.

Below, we consider three different probabilistic constructions of expander graphs, which are compared in \Cref{tab:expander_algorithms}.

\paragraph{Standard expander graph construction}
A natural choice for an expander graph is a \emph{random} -regular graph on  vertices, formed by taking  independent uniform permutations on . \citet{Friedman03} proved a conjecture of Alon, establishing that random regular graphs are \emph{weakly-Ramanujan}.
\begin{theorem}\citep[Theorem 1.1]{Friedman03}
\label{thm:randomregular}
 Fix  and an even integer . Then, suppose  is a random graph generated by taking  independent uniformly random permutations  on  and then choosing the edge set as
 
 Then, with probability ,
  satisfies
  for , where  are the eigenvalues of the adjacency matrix of . 
\end{theorem}

In the experiments reported in \cref{tab:benchmarking,tab:sparse-comparision,tab:lrgb,tab:largegraphs,tab:alations_comps}, we use graphs generated according to the random process described above, with self-loops removed, to instantiate the expander graph component of .

\paragraph{A simple variant.} A simple variant of the aforementioned random process is to instead choose a single permutation on  elements formed by taking  copies of . More precisely, given an input graph , one can generate an expander graph  as follows:
\begin{itemize}
    \item Let .
    \item Pick a permutation  on  uniformly at random from the  possible permutations.
    \item Let  be the multiset .
\end{itemize}
For the ablation studies in \cref{tab:ablationcifar,tab:MNIST,tab:malnet,tab:pattern,tab:cluster}, we use the above variant to generate the expander graph component of  (once again removing self loops). We find that, in practice, the above variant produces near-Ramanujan graphs most of the time.

\paragraph{Hamiltonian cycle variant.} Another interesting variant of the random graph process analyzed in \cref{thm:randomregular} is the one in which one once again takes  independent random permutations, except that the permutations must be one of the  permutations consisting of a single cycle of length . In other words, one chooses  independent Hamiltonian cycles on  and takes all edges from each of the Hamiltonian cycles. The work of \citet{Friedman03}, in fact, analyzes this modified process:

\begin{theorem}\citep[Theorem 1.2]{Friedman03}
 Fix  and an even integer . Then, suppose  is a random graph generated as follows: Take independent permutations  on , where each  is chosen uniformly at random from the  permutations whose cyclic decomposition consists of a single cycle. Then choose the edge set as
 
 Then, with probability ,
  satisfies
  for , where  are the eigenvalues of the adjacency matrix of . 
\end{theorem}
The advantage of using this random process for generating an expander graph is that it automatically satisfies the universal approximation property outlined in \cref{sec:proofs} when augmented with self-loops (see \cref{thm:univapprox}).

\paragraph{Experimental analysis:} In \cref{tab:expander_algorithms}, we have compared the effect of using different algorithms for expander graph creation. We can see from the results that all approaches have almost similar results. Note that \cref{thm:univapprox} assumes the presence of Hamiltonian paths for its approximation properties.



\begin{table}
\centering
\refstepcounter{table}
\label{tab:expander_algorithms}
\caption{An experimental comparison of different approaches used for Ramanujan Expander graph generation.}
\scalebox{0.8}{
\begin{tabular}{l|lllll} 
\toprule

Expander Algorithm & Cifar10       & MNIST         & Cluster       & Pattern       & PascalVOC-SP     \\ 

\hline

Standard Permutation-Based               &  &  &  &  &   \\ 



Simple Permutation-Based Variant        &  &  &  &  &   \\ 



Hamiltonian Cycle Variant         &  &  &  &  &   \\

\bottomrule
\end{tabular}
}
\end{table}

\section{On Expander Graphs versus Global Connectors} \label{sec:expander-vs-global}
From ablation studies in \Cref{sec:ablation}, we can see that sometimes only having expander graphs leads to the best results, sometimes virtual nodes only, and sometimes using a combination of both. Although this is a hyperparameter to tune, we can have a general understanding from the datasets on which cases virtual nodes work better and vice versa. Here we remark on some of these patterns.

\textbf{Graph diameter}: Using virtual nodes, the diameter of the graph becomes exactly 2 (unless the original graph was already complete, with diameter 1). Expander graphs also help to reduce the diameter of the graph, but only achieve a (probabilistic) diameter guarantee of .

\textbf{Information Bottleneck}: Virtual nodes serve as a global sink, and so even a single node should be able to keep all the information flowing from the whole graph. In case the graphs are large or many nodes rely on the virtual nodes to pass information, though, virtual nodes are likely to cause information bottleneck, and may even reduce the performance of the model. Expander graphs, to the contrary, introduce many new paths; as they rely on the local information storage of the nodes, they do not have this same problem.

\textbf{Interference with the local structure}: Virtual nodes introduce a new node, through which all of their new connections pass. Expander edges, on the other hand, are much easier to confuse with the actual edges of the graph; this can make it more difficult for the model to use the graph structure appropriately. As our model uses shared weights among the types of activations, which substantially helps with model size, the only way to understand the edges are different is through the edge embeddings. For datasets like MalNet-Tiny which rely heavily on the structure, while node and edge features are less useful, expander edges can interfere with the information propagation.

We observed that on molecular datasets,
virtual nodes generally help substantially,
while expander edges don't help much or can even hurt.
The situation for image-based graphs is the opposite,
where especially on Pascal-VOC, expander graphs are highly useful to the model,
but virtual nodes can cause information bottleneck.

Virtual nodes can also make incorporating ideas like the batching mechanism used in GraphSAGE \citep{hamilton2017inductive} more difficult, where having virtual nodes means every batch includes the whole graph. Expander edges can still be used with batching techniques.


\section{Universality of \textsc{Exphormer}} \label{sec:proofs}
In this section, we detail the universal approximation properties of \textsc{Exphormer}-based graph transformers.


The work of \citet{YunBRRK20} showed that for sequences, transformers are universal approximators, i.e., they can approximate any \emph{permutation equivariant} function mapping one sequence to another arbitrarily closely when provided with enough parameters. A function  is said to be permutation equivariant if , i.e., if permuting the columns of an input  results in the columns of  being permuted the same way.
\begin{theorem}[\citealp{YunBRRK20}]\label{thm:univapprox-perm}
 Let  and . For any function  that is permutation equivariant, there exists a transformer network  such that .
\end{theorem}
The same work shows an extension to all (not necessarily permutation equivariant) sequence-to-sequence functions that are defined on a compact domain, say,  provided that one uses a positional encoding. More specifically, for any transformer , one can define a \emph{transformer with positional encoding}  such that . The following results shows that trainable positional encodings allow a transformer to approximate any sequence-to-sequence continuous function on the compact domain.
\begin{theorem}[\citealp{YunBRRK20}]\label{thm:univapprox-trans}
 Let  and . For any continuous function  that is permutation equivariant, there exists a transformer with positional encoding, , such that .
\end{theorem}


Note that the above theorems hold for \emph{full} (dense) transformers. However, under certain conditions about the sparsity pattern, one can obtain similar universality for sparse attention mechanisms~\citep{YunCBRRK20}.

One important consideration is that the aforementioned results hold for functions on \emph{sequences}. Since we are concerned with functions on \emph{graphs}, it is interesting to ask what the implications are for graph transformers.

We follow the approach of \citet{kreuzer2021rethinking}: Given a graph , we can view a node transformer as a function from  which uses the padded adjacency matrix of  as a positional encoding. Similarly, an edge transformer takes as input a sequence  with  and  such that  if  and  are connected in  or  otherwise. Any ordering of these vectors corresponds to the same graph. Moreover, we can capture the relevant functions going from  with permutation equivariance. Ideally, they can be approximated as closely as desired by suitable transformers on the edge input.

Now, simply observe (see~\citealt{kreuzer2021rethinking}) that one can choose a function (in either the node transformer or edge transformer case) that is (a.) invariant under node permutations and (b.) maps non-isomorphic graphs to distinct values. We would like to apply one of the above thoerems to such a function.

However, we cannot quite apply \Cref{thm:univapprox-perm} or \Cref{thm:univapprox-trans}, as it is specifically for full transformers in which all nodes are pairwise connected in the attention interaction graph.

Therefore, we provide the following theorem, which shows that, under reasonable assumptions, \textsc{Exphormer} is able to provide universal approximation properties using just  edges.

\begin{theorem}\label{thm:univapprox}
 Suppose  is the attention graph of \textsc{Exphormer} (which contains  graph nodes and potentially more virtual nodes), augmented with self loops on all nodes. Suppose  satisfies at least one of the following:
 \begin{enumerate}
     \item  contains at least one node which is connected to all  graph nodes (i.e., at least one virtual node is included).
     \item The underlying expander graph of  contains a Hamiltonian path.
 \end{enumerate}
 Then, it follows that a sparse transformer model, with positional encodings and an attention mechanism following , can universally approximate continuous functions . That is, for any  and , there exists a sparse transformer network , which uses the attention graph  and some positional encodings, such that .
\end{theorem}
Note that in the above theorem, only one of the enumerated conditions needs to be satisfied; in other words, one does not need to use both global attention and expander graph attention components (see \Cref{sec:attentioncomp}). This is relevant, as for certain datasets, it may be desirable to use one of these attention components but not both (see~\Cref{sec:expander-vs-global}), but universal approximation properties do not depend on this choice.

\begin{proof}[Proof of \Cref{thm:univapprox}]
Let  be the attention graph of a modified sparse \textsc{Exphormer} attention mechanism satisfying the conditions in \Cref{thm:univapprox}.

First, assume that condition (1) is satisfied. This implies that there is a subgraph of  which forms a star graph on the  graph nodes (plus, potentially, the virtual node).
In this case, the existence of our desired  is directly implied by the following universal approximation result of \citet{ZaheerGDAAOPRWY20}:
\begin{theorem}[\citealp{ZaheerGDAAOPRWY20}]\label{thm:bigbird-universality}
 Let  and . For any graph  on  that contains the star graph, we have that if  is a continuous function, then there exists a sparse transformer network  (with trainable positional encodings) such that .
\end{theorem}

Otherwise, assume that instead condition (2) holds. Then, we can apply Theorem 1 of \citet{YunCBRRK20}. In particular, we note that since (a.)\  contains self-loops, (b.)\  contains a Hamiltonian path, and (c.)\  due to \Cref{cor:logdiam}, it follows that the assumptions of the theorem of \citet{YunCBRRK20} are satisfied. (Note that in our setup,  is the softmax function, which satisfies their Assumption 2.) Hence, the desired  exists for any .

This completes the proof.
\end{proof}

This result implies that there exist sparse transformers based on the variant of \textsc{Exphormer} in \Cref{thm:univapprox} which can solve graph isomorphism problems. This does not, however, imply the existence of an efficient algorithm for solving graph isomorphism problems, as we have not shown these networks can be efficiently identified; see further discussion by \citet{kreuzer2021rethinking}.

\end{document}
