

\section{Results}
\label{sec:results}

In this section, we provide an evaluation of our proposed method with a comparison to existing approaches.
We evaluate on the ScanNet dataset \cite{dai2017scannet}, which contains 1513 RGB-D scans composed of 2.5M RGB-D images.
We use the public train/val/test split of 1045, 156, 312 scenes, respectively, and follow the 20-class semantic segmentation task defined in the original ScanNet benchmark.
We evaluate our results with per-voxel class accuracies, following the evaluations of previous work~\cite{dai2017scannet,qi2017pointnet++,dai2018scancomplete}.
Additionally, we visualize our results qualitatively and in comparison to previous work in Fig~\ref{fig:comparison_scannet}, with close-ups shown in Fig~\ref{fig:comparison_scannet_zoom}. Note that we map the predictions from all methods back onto the mesh reconstruction for ease of visualization.

\vspace{0.2cm}\noindent
\textbf{Comparison to state of the art.}
Our main results are shown in Tab.~\ref{table:comparison_scannet}, where we compare to several state-of-the-art volumetric (ScanNet\cite{dai2017scannet}, ScanComplete\cite{dai2018scancomplete}) and point-based approaches (PointNet++\cite{qi2017pointnet++}) on the ScanNet test set.
Additionally, we show an ablation study regarding our design choices in Tab.~\ref{table:ablation_scannet}.

The best variant of our \OURS{} network achieves 75\% average classification accuracy which is quite significant considering the difficulty of the task and the performance of existing approaches.
That is, we improve 22.2\% over existing volumetric and 14.8\% over the state-of-the-art PointNet++ architecture. 


\vspace{0.2cm}\noindent
\textbf{How much does RGB input help?}
Tab.~\ref{table:ablation_scannet} includes a direct comparison between our 3D network architecture when using RGB features against the exact same 3D network without the RGB input.
Performance improves from 54.4\% to 70.1\% with RGB input, even with just a single RGB view.
In addition, we tried out the naive alternative of using per-voxel colors rather than a 2D feature extractor.
Here, we see only a marginal difference compared to the purely geometric baseline (54.4\% vs. 55.9\%).
We attribute this relatively small gain to the limited grid resolution (cm voxels), which is insufficient to capture rich RGB features.
Overall, we can clearly see the benefits of RGB input, as well as the design choice to first extract features in the 2D domain.

\vspace{0.2cm}\noindent
\textbf{How much does geometric input help?}
Another important question is whether we actually need the 3D geometric input, or whether geometric information is a redundant subset of the RGB input; see Tab.~\ref{table:ablation_scannet}.
The first experiment we conduct in this context is simply a projection of the predicted 2D labels on top of the geometry.
If we only use the labels from a single RGB view, we obtain 27\%  average accuracy (vs. 70.1\% with 1 view + geometry); for 3 views, this label backprojection achieves 44.2\% (vs. 73.0\% with 3 views + geometry). 
Note that this is related to the limited coverage of the RGB backprojections (see Tab.~\ref{table:coverage}).

However, the interesting experiment now is what happens if we still run a series of 3D convolutions after the backprojection of the 2D labels.
Again, we omit inputting the scene geometry, but we now learn how to combine and propagate the backprojected features in the 3D grid; essentially, we ignore the first part of our 3D network; cf.~Fig.~\ref{fig:network}.
For 3 RGB views, this results in an accuracy of 58.2\%; this is higher than the 54.4\% of geometry only; however, it is much lower than our final 3-view result of 73.0\% from the joint network.
Overall, this shows that the combination of RGB and geometric information aptly complements each other, and that the synergies allow for an improvement over the individual inputs by 14.8\% and 18.6\%, respectively (for 3 views).



\begin{table*}[!htb]
	\resizebox{\textwidth}{!}{\centering
		\begin{tabular}{|l||c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c||c|}
			\hline
			& wall & floor & cab & bed & chair & sofa & table & door & wind & bkshf & pic & cntr & desk & curt & fridg & show & toil & sink & bath & other & avg \\ \hline
			ScanNet~\cite{dai2017scannet} & 70.1 & 90.3 & 49.8 & 62.4 &  69.3 & 75.7 & \textbf{68.4} & 48.9 & 20.1 & 64.6 & 3.4 & 32.1 & 36.8 & 7.0 & 66.4 & 46.8 & 69.9 & 39.4 & 74.3 & 19.5 &  50.8 \\ \hline
			ScanComplete~\cite{dai2018scancomplete} & 87.2 & 96.9 & 44.5 & 65.7 & 75.1 & 72.1 & 63.8 & 13.6 & 16.9 & 70.5 & 10.4 & 31.4 & 40.9 & 49.8 & 38.7 & 46.8 & 72.2 & 47.4 & 85.1 & 26.9  & 52.8 \\ \hline 
			PointNet++~\cite{qi2017pointnet++} & \textbf{89.5} & \textbf{97.8} & 39.8 & 69.7 & \textbf{86.0} & 68.3 & 59.6 & 27.5 & 23.7 & 84.3 & 0.0 & \textbf{37.6} & 66.7 & 48.7 & 54.7 & \textbf{85.0} & 84.8 & 62.8 & 86.1 & 30.7 & 60.2 \\ 
			\hline \hline
			\textbf{\OURS{} (ours)} & 73.9 & 95.6 & \textbf{69.9} & \textbf{80.7} &  85.9 & \textbf{75.8} & 67.8 &  \textbf{86.6} & \textbf{61.2} & \textbf{88.1} & \textbf{55.8} & 31.9 & \textbf{73.2} & \textbf{82.4} & \textbf{74.8} & 82.6 & \textbf{88.3} & \textbf{72.8} & \textbf{94.7} & \textbf{58.5} & \textbf{75.0}		\\ \hline
		\end{tabular}
	}
	\vspace{0.1cm}
	\caption{Comparison of our final trained model (5 views, end-to-end) against other state-of-the-art methods on the ScanNet dataset \cite{dai2017scannet}. We can see that our approach makes significant improvements, 22.2\% over existing volumetric and approx. 14.8\% over state-of-the-art PointNet++ architectures. 
		\vspace{-1.0cm}}
	\label{table:comparison_scannet}
\end{table*}


\vspace{0.2cm}\noindent
\textbf{How to feed 2D features into the 3D network?}
An interesting question is where to join 2D and 3D features; i.e., at which layer of the 3D network do we fuse together the features originating from the RGB images with the features from the 3D geometry.
On the one hand, one could argue that it makes more sense to feed the 2D part early into the 3D network in order to have more capacity for learning the joint 2D-3D combination.
On the other hand, it might make more sense to keep the two streams separate for as long as possible to first extract strong independent features before combining them.

To this end, we conduct an experiment with different 2D-3D network combinations (for simplicity, always using a single RGB view without end-to-end training); see Tab.~\ref{table:combining2d3d}.
We tried four combinations, where we fused the 2D and 3D features at the beginning, after the first third of the network, after the second third, and at the very end into the 3D network.
Interestingly, the results are relatively similar ranging from 67.6\%, 65.4\% to 69.1\% and 67.5\% suggesting that the 3D network can adapt quite well to the 2D features. Across these experiments, the second third option turned out to be a few percentage points higher than the alternatives; hence, we use that as a default in all other experiments.


\vspace{0.2cm}\noindent
\textbf{How much do additional views help?}
In Tab.~\ref{table:ablation_scannet}, we also examine the effect of each additional view on classification performance.
For geometry only, we obtain an average classification accuracy of 54.4\%; adding only a single view per chunk increases to 70.1\% (+15.7\%); for 3 views, it increases to 73.1\% (+3.0\%); for 5 views, it reaches 75.0\% (+1.9\%).
Hence, for every additional view the incremental gains become smaller; this is somewhat expected as a large part of the benefits are attributed to additional coverage of the 3D volume with 2D features.
If we already use a substantial number of views, each additional added feature shares redundancy with previous views, as shown in Tab.~\ref{table:coverage}.


\vspace{0.2cm}\noindent
\textbf{Is end-to-end training of the joint 2D-3D network useful?}
Here, we examine the benefits of training the 2D-3D network in an end-to-end fashion, rather than simply using a pre-trained 2D network.
We conduct this experiment with 1, 3, and 5 views.
The end-to-end variant consistently outperforms the fixed version, improving the respective accuracies by 1.0\%, 0.2\%, and 0.5\%.
Although the end-to-end variants are strictly better, the increments are smaller than we initially hoped for.
We also tried removing the 2D proxy loss that enforces good 2D predictions, which led to a slightly lower performance.
Overall, end-to-end training with a proxy loss always performed best and we use it as our default.



\vspace{-0.5cm}

\begin{table*}[!htb]
	\resizebox{\textwidth}{!}{\centering
		\begin{tabular}{|l||c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c||c|}
			\hline
			& wall & floor & cab & bed & chair & sofa & table & door & wind & bkshf & pic & cntr & desk & curt & fridg & show & toil & sink & bath & other & avg \\ \hline
			2d only (1 view) & 37.1 & 39.1 & 26.7 & 33.1 & 22.7 & 38.8 & 17.5 & 38.7 & 13.5 & 32.6 & 14.9 & 7.8 & 19.1 & 34.4 & 33.2 & 13.3 & 32.7 & 29.2 & 36.3 & 20.4 & 27.1 \\ \hline
			2d only (3 views) & 58.6 & 62.5 & 40.8 & 51.6 & 38.6 & 59.7 & 31.1 & 55.9 & 25.9 & 52.9 & 25.1 & 14.2 & 35.0 & 51.2 & 57.3 & 36.0 & 47.1 & 44.7 & 61.5 & 34.3 & 44.2 \\ \hline
			Ours (no geo input) & 76.2 & 92.9 & 59.3 & 65.6 & 80.6 & 73.9 & 63.3 & 75.1 & 22.6 & 80.2 & 13.3 & 31.8 & 43.4 & 56.5 & 53.4 & 43.2 & 82.1 & 55.0 & 80.8 & 9.3 & 58.2 \\ \hline
			Ours (3d geo only) & 60.4 & 95.0 & 54.4 & 69.5 & 79.5 & 70.6 & 71.3 & 65.9 & 20.7 & 71.4 & 4.2 & 20.0 & 38.5 & 15.2 & 59.9 & 57.3 & 78.7 & 48.8 & 87.0 & 20.6 & 54.4 \\ \hline 
			Ours (3d geo+voxel color) & 58.8 & 94.7 & 55.5 & 64.3 & 72.1 & 80.1 & 65.5 & \textbf{70.7} & 33.1 & 69.0 & 2.9 & 31.2 & 49.5 & 37.2 & 49.1 & 54.1 & 75.9 & 48.4 & 85.4 & 20.5 & 55.9 \\ \hline 
			Ours (1 view, fixed 2d) & 77.3 & 96.8 & \textbf{70.0} & 78.2 & 82.6 & \textbf{85.0} & 68.5 & 88.8 & 36.0 & 82.8 & 15.7 & 32.6 & 60.3 & 71.0 & 76.7 & 82.2 & 74.8 & 57.6 & 87.0 & 58.5 & 69.1 \\ \hline
			Ours (1 view) & 70.7 & 96.8 & 61.4 & 76.4 & 84.4 & 80.3 & 70.4 & 83.9 & 57.9 & 85.3 & 41.7 & 35.0 & 64.5 & 75.6 & 81.3 & 58.2 & 85.0 & 60.5 & 81.6 & 51.7 & 70.1 \\ \hline
			Ours (3 view, fixed 2d) & \textbf{81.1} & 96.4 & 58.0 & 77.3 & 84.7 & 85.2 & \textbf{74.9} & 87.3 & 51.2 & 86.3 & 33.5 & \textbf{47.0} & 52.4 & 79.5 & 79.0 & 72.3 & 80.8 & \textbf{76.1} & 92.5 & \textbf{60.7} & 72.8 \\ \hline
			Ours (3 view) & 75.2 & \textbf{97.1} & 66.4 & 77.6 & 80.6 & 84.5 & 66.5 & 85.8 & 61.8 & 87.1 & 47.6 & 24.7 & 68.2 & 75.2 & 78.9 & 73.6 & 86.9 & \textbf{76.1} & 89.9 & 57.2 & 73.0 \\ \hline
			Ours (5 view, fixed 2d) & 77.3 & 95.7 & 68.9 & \textbf{81.7} & \textbf{89.6} & 84.2 & 74.8 & 83.1 & \textbf{62.0} & 87.4 & 36.0 & 40.5 & 55.9 & \textbf{83.1} & \textbf{81.6} & 77.0 & 87.8 & 70.7 & 93.5 & 59.6 & 74.5 \\ \hline
			\textbf{Ours (5 view)} & 73.9 & 95.6 & 69.9 & 80.7 &  85.9 & 75.8 & 67.8 &  86.6 & 61.2 & \textbf{88.1} & \textbf{55.8} & 31.9 & \textbf{73.2} & 82.4 & 74.8 & \textbf{82.6} & \textbf{88.3} & 72.8 & \textbf{94.7} & 58.5 & \textbf{75.0}		\\ \hline
		\end{tabular}
	}
	\vspace{0.1cm}
	\caption{Ablation study for different design choices of our approach on ScanNet \cite{dai2017scannet}. We first test simple baselines where we backproject 2D labels from 1 and 3 views (rows 1-2), then run set of 3D convs after the backprojections (row 3). We then test a 3D-geometry-only network (row 4). Augmenting the 3D-only version with per-voxel colors shows only small gains (row 5). In rows 6-11, we test our joint 2D-3D architecture with varying number of views, and the effect of end-to-end training. Our 5-view, end-to-end variant performs best. \vspace{-0.5cm}}
	\label{table:ablation_scannet}
\end{table*}

\vspace{0.2cm}\noindent
\textbf{Evaluation in 2D domains using NYUv2.}
Although we are predicting 3D per-voxel labels, we can also project the obtained voxel labels into the 2D images.
In Tab.~\ref{table:denselabeling_nyu}, we show such an evaluation on the NYUv2~\cite{silberman11indoor} dataset.
For this task, we train our network on both ScanNet data as well as the NYUv2 train annotations projected into 3D.
Although this is not the actual task of our method, it can be seen as an efficient way to accumulate semantic information from multiple RGB-D frames by using the 3D geometry as a proxy for the learning framework.
Overall, our joint 2D-3D architecture compares favorably against the respective baselines on this 13-class task.


\begin{comment}
Comparison ScanNet \cite{dai2017scannet}
- ScanNet \cite{dai2017scannet}
- SceneComplete \cite{dai2018scancomplete}
- PointNet++ \cite{qi2017pointnet++}
- Ours

Ablation study on ScanNet
- original ScanNet (only 3D)							- better ScanNet (only 3D but with better features)		- better ScanNet (+ color)								- only 2D projections to 3D (single view)		
- 2D projections but no 3D geometry input	
- 3D + 1 view (best choice of ours)
- 3D + 3 views (best choice of ours)
- 3D + 5 views (best choice of ours)
- 3D + many views at test (best choice of ours)

Ablation study end-to-end vs fixed 2D network
- 3D + 1 view (fixed 2D ENet)		- 3D + 3 views (fixed 2D ENet)		- 3D + 1 view (end-to-end 2D ENet)
- 3D + 3 views (end-to-end 2D ENet)


Ablation study where to join 2D with 3D network
- beginning (1 view)	- middle (1 view)		- end (1 view)			
Ablation study how to train multi view
- 3D + 5 views (trained on 5 views)
- 3D + 5 views (trained on 3 views)
- 3D + 5 views (trained on random views 1-5)
- 3D + 10+ views (trained on 5 views or random views 1-5)
\end{comment}





\begin{figure}[t!] \centering
	\includegraphics[width=0.92\linewidth]{images/results_comparison.jpg}
	\caption{Qualitative semantic segmentation results on the ScanNet~\cite{dai2017scannet} test set. We compare with the 3D-based approaches of ScanNet~\cite{dai2017scannet}, ScanComplete~\cite{dai2018scancomplete}, PointNet++~\cite{qi2017pointnet++}. Note that the ground truth scenes contain some unannotated regions, denoted in black. Our joint 3D-multi-view approach achieves more accurate semantic predictions.
		\vspace{-0.4cm}}
	\label{fig:comparison_scannet}
\end{figure} 

\begin{table*}[!htb]
		\centering
		\begin{tabular}{|c|c|c|c|}
			\hline
			 & 1 view & 3 views & 5 views \\ \hline
			coverage & 40.3\% & 64.4\% & 72.3\% \\ \hline
		\end{tabular}
	\caption{Amount of coverage from varying number of views over the annotated ground truth voxels of the ScanNet~\cite{dai2017scannet} test scenes.
		\vspace{-0.4cm}}
	\label{table:coverage}
\end{table*}

\begin{table*}[!htb]
	\resizebox{\textwidth}{!}{\centering
\begin{tabular}{|l||c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c||c|}
		\hline
		& wall & floor & cab & bed & chair & sofa & table & door & wind & bkshf & pic & cntr & desk & curt & fridg & show & toil & sink & bath & other & avg \\ \hline
		begin & 78.8 & 96.3 & 63.7 & 72.8  & 83.3 & 81.9 & \textbf{74.5} & 81.6 & 39.5 & 89.6 & \textbf{24.8} & 33.9 & 52.6 & \textbf{74.8} & 76.0 & 47.5 & 80.1 & \textbf{65.4} & 85.9 & 49.4 & 67.6 \\ \hline
		1/3 & 79.3 & 95.5 & 65.1 & 75.2 & 80.3 & 81.5 & 73.8 & 86.0 & 30.5 & \textbf{91.7} & 11.3 & \textbf{35.5} & 46.4 & 66.6 & 67.9 & 44.1 & \textbf{81.7}  & 55.5 & 85.9  & 53.3 & 65.4 \\ \hline
		\textbf{2/3} & 77.3 & \textbf{96.8} & \textbf{70.0} & \textbf{78.2} & 82.6 & \textbf{85.0} & 68.5 & \textbf{88.8} & 36.0 & 82.8 & 15.7 & 32.6 & \textbf{60.3} & 71.0 & \textbf{76.7} & \textbf{82.2} & 74.8 & 57.6 & 87.0 & \textbf{58.5} & \textbf{69.1} \\ \hline
		end & \textbf{82.7} & 96.3 & 67.1 & 77.8 & \textbf{83.2} & 80.1 & 66.0 & 80.3 & \textbf{41.0} & 83.9 & 24.3 & 32.4 & 57.7 & 70.1 & 71.5 & 58.5 & 79.6 & 65.1 & \textbf{87.2} & 45.8 & 67.5 \\ \hline
\end{tabular}
}
	\vspace{0.1cm}
	\caption{Evaluation of various network combinations for joining the 2D and 3D streams in the 3D architecture (cf.~Fig.~\ref{fig:network}, top). We use the single view variant with a fixed 2D network here for simplicity. Interestingly, performance only changes slightly; however, the 2/3 version performed the best, which is our default for all other experiments.
\vspace{-0.4cm}}
	\label{table:combining2d3d}

\end{table*}



\begin{figure}[t!] \centering
	\includegraphics[width=0.95\linewidth]{images/results_comparison_zoom.jpg}
	\caption{Additional qualitative semantic segmentation results (close ups) on the ScanNet~\cite{dai2017scannet} test set. Note the consistency of our predictions compared to the other baselines.
		\vspace{-0.4cm}}
	\label{fig:comparison_scannet_zoom}
\end{figure} 

\begin{table*}[!htb]
	\resizebox{\textwidth}{!}{\centering
	\begin{tabular}{|l||c|c|c|c|c|c|c|c|c|c|c|c|c||c|}
		\hline
		& bed & books & ceil. & chair & floor & furn. & obj. & pic. & sofa & table & tv & wall & wind. & avg. \\ \hline
		SceneNet~\cite{handa2015scenenet} & 70.8 & 5.5 & 76.2 & 59.6 & 95.9 & 62.3 & 50.0 & 18.0 & 61.3 & 42.2 & 22.2 & 86.1 & 32.1 &  52.5 \\ \hline
		Hermans et al.~\cite{hermans2014dense} & 68.4 & 45.4 & 83.4 & 41.9 & 91.5 & 37.1 & 8.6 & 35.8 & 58.5 & 27.7 & 38.4 & 71.8 & 48.0 & 54.3 \\ \hline
		SemanticFusion~\cite{mccormac2017semanticfusion} (RGBD+CRF) & 62.0 & \textbf{58.4} & 43.3 & 59.5 & 92.7 & 64.4 & \textbf{58.3} & 65.8 & 48.7 & 34.3 & 34.3 & 86.3 & 62.3 & 59.2 \\ \hline
		SemanticFusion~\cite{mccormac2017semanticfusion,eigen2015predicting} (Eigen+CRF) & 48.3 & 51.5 & 79.0 & 74.7 & 90.8 & 63.5 & 46.9 & 63.6  & 46.5 & 45.9 & \textbf{71.5} & \textbf{89.4} & 55.6 & 63.6 \\ \hline
		ScanNet~\cite{dai2017scannet} & 81.4 & - & 46.2 & 67.6 &  \textbf{99.0} & 65.6 & 34.6 &  - & 67.2 & 50.9 & 35.8 & 55.8 & 63.1 & 60.7 \\ \hline \hline
		\textbf{\OURS{} (ours)} & \textbf{84.3} & 44.0 &  43.4 & \textbf{77.4} &  92.5 & \textbf{76.8} & 54.6 & \textbf{70.5} & \textbf{86.3} & \textbf{58.6} & 67.3 & 84.5 & \textbf{85.3} & \textbf{71.2}		\\ \hline
	\end{tabular}
}
	\vspace{0.1cm}
	\caption{We can also evaluate our method on 2D semantic segmentation tasks by projecting the predicted 3D labels into the respective RGB-D frames.
		Here, we show a comparison on dense pixel classification accuracy on NYU2~\cite{Silberman:ECCV12}. Note that the reported ScanNet classification is on the -class task.
		\vspace{-0.5cm}}
	\label{table:denselabeling_nyu}
\end{table*}

\begin{comment}
\subsection{\OURS{} on different Tasks: Scan Completion}
Here, we examine the completion task in comparison to Dai et al.~\cite{dai2018scancomplete}.
- Quantitative metric on SUNCG
- Qualitative comparison on SUNG
\MATTHIAS{possibly move to supplemental material}

\subsection{Evaluation on Matterport3D \cite{Matterport3D}}
\MATTHIAS{move this one to the supplemental material}
\end{comment}

\vspace{0.2cm}\noindent
\textbf{Summary Evaluation.}
\vspace{-0.2cm}
\begin{itemize}
	\item RGB and geometric features are orthogonal and help each other
	\item More views help, but increments get smaller with every view
	\item End-to-end training is strictly better, but the improvement is not that big.
	\item Variations of where to join the 2D and 3D features change performance to some degree; 2/3 performed best in our tests.
	\item Our results are significantly better than the best volumetric or PointNet baseline (+22.2\% and +14.8\%, respectively).
\end{itemize}

\noindent
\textbf{Limitations.}
While our joint 3D-multi-view approach achieves significant performance gains over previous state of the art in 3D semantic segmentation, there are still several important limitations.
Our approach operates on dense volumetric grids, which become quickly impractical for high resolutions; e.g., RGB-D scanning approaches typically produce reconstructions with sub-centimeter voxel resolution; sparse approaches, such as OctNet \cite{riegler2017OctNet}, might be a good remedy.
Additionally, we currently predict only the voxels of each column of a scene jointly, while each column is predicted independently, which can give rise to some label inconsistencies in the final predictions since different RGB views might be selected; note, however, that due to the convolutional nature of the 3D networks, the geometry remains spatially coherent.

