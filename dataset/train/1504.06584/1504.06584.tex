\documentclass[conference]{IEEEtran}
\usepackage{hyperref}
\usepackage[noadjust]{cite}
\usepackage[cmex10]{amsmath}
\interdisplaylinepenalty=2500
\usepackage[caption=false,font=footnotesize]{subfig}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{centernot}
\everymath{\displaystyle}
\usepackage{comment}
\let\labelindent\relax
\usepackage[shortlabels]{enumitem}
\usepackage{nameref}
\usepackage{slashbox}
\usepackage{textcomp}
\usepackage{gensymb}
\usepackage[all]{hypcap}
\usepackage{tikz}
\usetikzlibrary{patterns}

\pdfstringdefDisableCommands{\def\eqref#1{(\ref{#1})}}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\round}{round}
\DeclareMathOperator{\functioncheck}{check}
\DeclareMathOperator{\functionangle}{angle}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}



\hyphenation{al-go-rithm}
\hyphenation{orthogonality}
\hyphenation{section}

\mathchardef\UrlBreakPenalty=10000
\mathchardef\UrlBigBreakPenalty=10000





\begin{document}
\title
{
  Searching for a Compressed Polyline\\
  with a Minimum Number of Vertices
}
\author
{
\IEEEauthorblockN{Alexander Gribov}
\IEEEauthorblockA
{
  Environmental Systems Research Institute\\
  380 New York Street\\
  Redlands, CA 92373\\
  E-mail: agribov@esri.com}
}

\maketitle

\begin{abstract}
\boldmath
  There are many practical applications that require simplification of polylines. Some of the goals are to reduce the amount of information necessary to store, improve processing time, or simplify editing. The simplification is usually done by removing some of the vertices, making the resultant polyline go through a subset of the source polyline vertices. However, such approaches do not necessarily produce a new polyline with the minimum number of vertices. The approximate solution to find a polyline, within a specified tolerance, with the minimum number of vertices is described in this paper.
\end{abstract}

\begin{IEEEkeywords}
  polyline compression; polyline approximation; orthogonality; circular arcs
\end{IEEEkeywords}

\section{Introduction}

The task is to find a polyline, within a specified tolerance of the source polyline, with the minimum number of vertices. That polyline is called optimal. Usually, a subset of vertices of the source polyline is used to construct an optimal polyline~\cite{CompressionAlgorithm, CompressionReview}. However, an optimal polyline does not necessarily have vertices coincident with the source polyline vertices. One approach, to allow the resultant polyline to have flexibility in the locations of vertices, is to find the intersection between adjacent straight lines~\cite{PolylineGeneralizationCombinatorical} or geometrical primitives~\cite{PolylineGeneralization}. However, there are situations when such an approach does not work well, for example, when adjacent straight lines are almost parallel to each other or a circular arc is close to being tangent to a straight segment. The approach described in this paper evaluates a set of vertex locations (considered locations) while searching for a polyline with the minimum number of vertices.

\section{Algorithm}

\subsection
{
  Discretization of the Solution
  \label{sec:Descretization}
}

\begin{figure} [b]
  \centering
  \includegraphics[width = 6 cm, keepaspectratio]{ExampleSegment.png}
  \caption
  {
    Example of one segment (red segment) between considered locations (black dots) within tolerance of the source polyline (blue polyline).
  }
  \label{fig:ExampleSegment}
\end{figure}

Any compressed polyline must be within tolerance of the source polyline; therefore, the compressed polyline must have vertices within tolerance of the source polyline. It would be very difficult to consider all possible polylines and find one with the minimum number of vertices; therefore, as an approximation, only some locations around vertices of the source polyline are considered (see the black points around the vertices of the source polyline in Fig.~\ref{fig:ExampleSegment}).

The locations around vertices of the source polyline are chosen to be on an infinite equilateral triangular grid with the distance from vertices of the source polyline less than the specified tolerance. The equilateral triangular grid (see Fig.~\ref{fig:TriangularGrid}) has the lowest number of nodes versus other grids (square, hexagonal, etc.), satisfying that distance from any point to the closest node does not exceed the specified threshold.

\begin{figure} [htb]
  \centering
  \begin{tikzpicture}[scale = 1.5]
    \tikzstyle{every node} = [font = \tiny]

    \begin{scope}
      \clip (-0.7, -0.2) -- (1.7, - 0.2) -- (1.7, 0.86602540378443864676372317075294 + 0.2) -- (-0.7, 0.86602540378443864676372317075294 + 0.2) -- cycle;
      \draw [dotted] (0, 0) -- (-1, 0);
      \draw [dotted] (1, 0) -- (2, 0);
      \draw [dotted] (0.5, 0.86602540378443864676372317075294) -- (-0.5, 0.86602540378443864676372317075294);
      \draw [dotted] (0.5, 0.86602540378443864676372317075294) -- (1.5, 0.86602540378443864676372317075294);
      \draw [dotted] (0, 0) -- (-0.5, 0.86602540378443864676372317075294);
      \draw [dotted] (1, 0) -- (1.5, 0.86602540378443864676372317075294);
      \draw [dotted] (-1, 0) -- (-0.5, 0.86602540378443864676372317075294);
      \draw [dotted] (2, 0) -- (1.5, 0.86602540378443864676372317075294);
    \end{scope}
    
    \draw (0, 0) -- (1, 0) -- (0.5, 0.86602540378443864676372317075294) -- cycle;
    \draw [dashed] (0.5, 0.28867513459481288225457439025098) -- (0, 0);
    \draw [dashed] (0.5, 0.28867513459481288225457439025098) -- (1, 0);
    \draw [dashed] (0.5, 0.28867513459481288225457439025098) -- (0.5, 0.86602540378443864676372317075294);
    \draw (0.5, 0.28867513459481288225457439025098) node [anchor = 210] {};
    \draw (0, 0) node [anchor = 90] {};
    \draw (1, 0) node [anchor = 90] {};
    \draw (0.5, 0.86602540378443864676372317075294) node [anchor = 270] {};
  
  \end{tikzpicture}
  \caption
  {
    The worst case distance for the equilateral triangular grid is the distance from the center of the triangle  to any vertex of the equilateral triangle. If , then .
  }
  \label{fig:TriangularGrid}
\end{figure}

The choice for the side of an equilateral triangle in the equilateral triangular grid is calculated from the error it introduces. That error can be expressed as a proportion of the specified tolerance. For example,  proportion of the specified tolerance means that the side of the equilateral triangle is equal to  times the specified tolerance. This leads to about  locations per each vertex. To decrease complexity, some locations might be skipped; if they are considered in neighbor vertices of the source polyline, however, it should be done without breaking the combinatorial algorithm described in section \ref{sec:CombinatorialApproach}. If tolerance is great, it is possible to consider locations around segments of the source polyline. In this paper, to support any tolerance, only locations around vertices of the source polyline are considered. Densification of the source polyline might be necessary to find the polyline with the minimum number of vertices.

\subsection
{
  Testing a Segment to Satisfy Tolerance
  \label{sec:TestingSegmentTolerance}
}

For a compressed polyline to be within tolerance, every segment of the compressed polyline must be within tolerance from the part of the source polyline it describes. To find the compressed polyline with the minimum number of vertices, this test has to be performed many times for all combinations of possible locations of vertices (see Fig.~\ref{fig:ExampleSegment}). \cite{OptimizedCompressionAlgorithm} describes an efficient approach to perform these tests based on the convex hull. If the convex hull is stored as a polygon, the complexity of this task is , where  is the number of vertices in the convex hull~\cite{OptimizedCompressionAlgorithm}. The expected complexity of the convex hull for the  random points in any rectangle is , see~\cite{ConvexHullsComplexity}. If the source polyline has parts close to an arc, the size of the convex hull tends to increase. For the worst case, the number of vertices in the convex hull is equal to the number of vertices in the original set.

If there are no lines with thickness of two tolerances covering the convex hull completely, then one segment cannot describe this part of the source polyline. The complexity of this check is .

A convex hull for any part of the source polyline is constructed in the same way as in~\cite{OptimizedCompressionAlgorithm}.

\subsection
{
  Testing Segment End Points
  \label{sec:TestingSegmentEndPoints}
}

The test described in the previous section~\ref{sec:TestingSegmentTolerance} does not check the ends of the segment. The example in Fig.~\ref{fig:TopologycalTest} shows that the source polyline changes direction to the opposite several times (zigzag) before going up. Without checking end points and changes in direction, the compressed polyline might not describe some parts of the source polyline (Fig.~\ref{fig:TopologycalTest}a). Therefore, these tests are necessary to guarantee that the compressed polyline (Fig.~\ref{fig:TopologycalTest}b) describes the source polyline without missing any parts.

\begin{figure} [hb]
  \centering
  \begin{tabular}{c c c}
    \includegraphics[height = 1.5 cm, keepaspectratio]{NoCheck.png} &
    \includegraphics[height = 1.5 cm, keepaspectratio]{WithChecks.png} \\
    (a) &
    (b)
  \end{tabular}
  \caption
  {
    The blue polyline is the source polyline. The red polyline is the result of the algorithm without checking for end points and the source polyline direction (a) and with both checks performed (b).
  }
  \label{fig:TopologycalTest}
\end{figure}

The segment end points to be within the tolerance of the part of the source polyline are tested based on the convex hull in the same way as the test for the segment to be within tolerance performed in section~\ref{sec:TestingSegmentTolerance}.

This is equivalent to the test if the segment extended in parallel and perpendicular directions by the tolerance (see Fig.~\ref{fig:SegmentEndPoints}) contains a convex hull of the part of the source polyline it describes. If more directions are used, a better approximation of the curved polygon can be obtained. The complexity of the test is .

\begin{figure} [htb]
  \centering
  \begin{tikzpicture}[scale = 0.7]
    \draw [thick] (0, 0) -- (10, 0);
    \draw (-1, -1) -- (11, -1) -- (11, 1) -- (-1, 1) -- cycle;
    \draw (0, 1) arc (90:270:1);
    \draw (10, -1) arc (-90:90:1);
\draw [thick] (-0.4142135623730950488016887242097, 1) -- (-1, 0.4142135623730950488016887242097) -- (-1, -0.4142135623730950488016887242097) -- (-0.4142135623730950488016887242097, -1) -- (10 + 0.4142135623730950488016887242097, -1) -- (11, -0.4142135623730950488016887242097) -- (11, 0.4142135623730950488016887242097) -- (10 + 0.4142135623730950488016887242097, 1) -- cycle;
    \fill [pattern = north east lines] (0, 1) arc (90:270:1) -- (10, -1) arc (-90:90:1) -- cycle;
  \end{tikzpicture}
  \caption
  {
    The diagonal striped area is the tolerance area around the segment. The thin rectangle is the approximation of the area around the segment. A thick polygon would be a better approximation.
  }
  \label{fig:SegmentEndPoints}
\end{figure}

\subsection
{
  Testing Polyline Direction
  \label{sec:TestingZigZag}
}

The test for the source polyline to have a zigzag is performed by checking if the projection to the segment of backward movement exceeds two tolerances (, where  is the tolerance). Two tolerances are used because one vertex of the source polyline can shift forward by the tolerance and the vertex after that shift backward by the tolerance. The algorithm is based on analyzing zigzags before the processed point. Let  be the vertices of the polyline, ,  be the number of vertices in the polyline. The next algorithm constructs a table for efficient testing.
\begin{enumerate}[label={}]
  \item Define a set of directions ,\\
  where ,  is the number of directions.
  \item Cycle over each direction , .
  \begin{enumerate}[label={}]
    \item Define the priority queue with requests containing two numbers. The first number is the real value, and the second number is the index. Priority of the request is equal to the first number.
    \item Set .
    \item Cycle over each point  of the source polyline,\\
    .
    \begin{enumerate}[label={}]
      \item Calculate projection of  to the direction  (scalar product between the point and the direction vector):
      
      \item Remove all requests from the priority queue with a priority of more than . If the largest index from removed requests is larger than , set  equal to that index.
      \item Set .
      \item Add request  to the priority queue.
    \end{enumerate}
  \end{enumerate}
\end{enumerate}

To test if the part of the source polyline between vertices  and  has a zigzag.
\begin{enumerate}[label={}]
  \item First, find the closest direction  to the direction of the segment :
  ,
  where  is the direction of the segment.
  \item Second, if , then there are no zigzags for the segment describing the part of the source polyline from vertex  till .
\end{enumerate}

Let . If , then one segment cannot describe the part of the source polyline from vertex  till .

This test has some limitations:
\begin{itemize}
  \item The tested direction is approximated by the closest one, making the check approximate.
  \item For some error models, a zigzag might pass the test. For example, if errors are limited by a circle, a zigzag by two tolerances is only possible if it happens directly on the segment.
\end{itemize}

Nevertheless, it is an efficient test to avoid absurd results, like in Fig.~\ref{fig:TopologycalTest}a. The complexity of the algorithm is  and the complexity to test any segment is .

\subsection
{
  Combinatorial Approach to Find an Optimal Solution
  \label{sec:CombinatorialApproach}
}

The optimal solution is found by using the algorithm described in~\cite{PolylineGeneralizationCombinatorical}.

Let  be considered locations for vertex , where , ,  is the number of considered locations for the vertex . Let pairs , , divide the source polyline into  straight segments

describing the source polyline from vertex  till , . Notice that neighbor segments are already connected in , , and this solution avoids problems in algorithms~\cite{PolylineGeneralizationCombinatorical, PolylineGeneralization} when the intersection of neighbor segments is far away from the source polyline.

The goal of this algorithm is to find the solution with the minimum number of vertices while satisfying tolerance restriction, and among them with the minimum integral square differences. Therefore, minimization is performed in two parts
,
where the first part  is the number of segments, and the second part  is the integral of the square deviation between segments and the source polyline. The solutions are compared by the number of segments and, if they have the same number of segments, by square deviation between segments and the source polyline. The solution of this task, when the optimal polyline has vertices coincident with the source polyline, can be found in \cite{CombinatorialMinimumNumberSegments}.

Let ,  be parts of the source polyline from vertex  to .

The optimal solution is found by induction.
Define the optimal solution for polyline  as
,
.
For , construct the optimal solution for  from optimal solutions for , .

where

is the integral square difference between segment

and the source polyline from vertex  till ,

is a combination of checks described in the previous sections \ref{sec:TestingSegmentTolerance}, \ref{sec:TestingSegmentEndPoints}, and \ref{sec:TestingZigZag} to check if segment

can describe the part of the source polyline from vertex  till .

To reconstruct the optimal solution, it is necessary for

to store  when the right part is minimal.

The optimal solution is reconstructed from

by recurrently using stored  values.

\subsection
{
  Optimization
  \label{sec:Optimization}
}

It is possible to significantly reduce the complexity of the algorithm described in the previous section~\ref{sec:CombinatorialApproach} by using the approach described in~\cite{PolylineGeneralizationCombinatorical}.

where


From \eqref{eq:OptmizationBaseFormula}, it follows that

and


The maximum of \eqref{eq:OptimizationFormula1} and \eqref{eq:OptimizationFormula2} can be used to skip checking combinations between vertex  and .

The inequalities \eqref{eq:OptimizationFormula1} and \eqref{eq:OptimizationFormula2} are approximate due to the use of considered locations. However, this allows finding stricter limitations for the solution inside the interval and simultaneously finding the solution for breaking at vertex .

It is possible to construct \eqref{eq:OptimizationFormula1} and \eqref{eq:OptimizationFormula2} with exact inequalities by constructing the optimal solution when the end point is not required to end in the considered location. Similarly, the part from vertex  to  should not be required to end in considered locations for vertex . This is useful when the resultant polyline is required to go through the vertices of the source polyline. However, such an algorithm has a worse compression ratio than the one with the flexibility in joints.

See paper~\cite{PolylineGeneralizationCombinatorical} for further details of this algorithm.

\subsection{Optimal Compression of Closed Polylines}

To find the optimal compression of a closed polyline, it is necessary to know the starting vertex. It is also necessary that the resultant polyline starts and ends in the same vertex. The next algorithm will be used to find the starting vertex and construct a closed resultant polyline.
\begin{enumerate}[label={\arabic*.}, ref={\arabic*}]
  \item \label{enum:ConvexHull} Construct a convex hull for all vertices of the source polyline.
  \item \label{enum:SmallesAngle}  Find the smallest angle of the convex hull polygon.
  \item \label{enum:Reorient}  Take the vertex corresponding to the smallest angle as the starting vertex and reorient the closed polyline to start from that vertex.
  \item Apply the algorithm.
  \item \label{enum:ConstructFirstSolution} From the constructed solution, take one vertex in the middle as the new starting vertex and reorient the closed polyline to start from that vertex.
  \item Apply the algorithm once more, while for the first and the last vertex consider only the location of the previous solution for the middle vertex.
\end{enumerate}

Steps \ref{enum:ConvexHull}, \ref{enum:SmallesAngle}, and \ref{enum:Reorient} are important for a small closed polyline. For the small closed polyline, the resultant polyline is within tolerance of the source polyline, even with suboptimal orientation. As a consequence, without these steps, step \ref{enum:ConstructFirstSolution} may not find the optimal division of the source polyline, leading to a suboptimal solution.

\subsection{Optimal Compression by Straight Segments and Arcs}

This algorithm is extendible to support arcs. The arc passing through considered locations differs from the segment by the necessity to define the radius. Unfortunately, it adds significant complexity to the algorithm. Nevertheless, such an algorithm is possible. There are different ways to fit an arc to a polyline: minimum integral square differences of squares~\cite{ThomasReference2, IchokuReference3}, minimum integral square differences~\cite{RobinsonReference6, Landau4, PaperArcFitting, FittingOfCircularArcsWithO1Complexity, EfficientFittingOfCircularArcs}, minimum deviation, etc. Algorithms with complexity , where  is the number of vertices in the fitted polyline, are not suitable due to the significant increase in complexity. The algorithms with acceptable complexity  are~\cite{ThomasReference2, IchokuReference3, FittingOfCircularArcsWithO1Complexity, EfficientFittingOfCircularArcs}; however, algorithms based on integral square differences of squares~\cite{ThomasReference2, IchokuReference3} might break for small arcs and, therefore, are not suitable. Checking that the part of the source polyline is within tolerance, end points, and zigzag will be time-consuming due to complexity .

\section{Analysis of the Algorithm Complexity}

The algorithm contains three steps:
\begin{enumerate}[label={\arabic*.}]
  \item Preprocessing: construction of convex hulls (section~\ref{sec:TestingSegmentTolerance}) and filling arrays for an efficient zigzag test (section~\ref{sec:TestingZigZag}).
  \item Construction of the optimal solution (section~\ref{sec:CombinatorialApproach}).
  \item Reconstruction of the optimal solution (section~\ref{sec:CombinatorialApproach}).
\end{enumerate}

A significant amount of time is spent on constructing an optimal solution. It is difficult to evaluate the complexity described in section~\ref{sec:Optimization}; however, the worst complexity is


The complexity of the algorithm depends on the type of polyline it processes. It is very difficult to conclude what is the practical complexity of this algorithm. If the optimal polyline does not have segments describing too many vertices of the source polyline, \eqref{eq:WorseComplexity} tends to be


Fig.~\ref{fig:EstimationOfAlgorithmComplexity} shows how much time it takes to process a polyline depending on the number of vertices. The dependence is very close to linear, supporting \eqref{eq:PracticalComplexity}.

\begin{figure} [htb]
  \centering
  \includegraphics[width = \columnwidth, keepaspectratio]{TimeGraph.png}
  \caption
  {
    Time needed to process a polyline versus the number of vertices. The time is measured in CPU ticks on the processor Intel Xeon CPU . The polylines are generated by the Brownian motion process. Each next vertex is randomly incremented from the previous vertex by random vector, with components normally distributed with zero mean and ~standard deviation. The tolerance was set to one. The average reduction in the number of vertices is about ~times.
  }
  \label{fig:EstimationOfAlgorithmComplexity}
\end{figure}

\section{Examples}

Fig.~\ref{fig:Comparison} shows an example of the algorithm described in this paper. If the source polyline is the noisy version of a ground truth polyline, where the noise does not exceed some threshold, and the algorithm is provided with a tolerance slightly greater than the threshold to account for approximations inside the algorithm, then the resultant polyline will never have more vertices than the ground truth polyline.

\begin{figure} [htb]
  \centering
  \begin{tabular}{c c}
    \shortstack{(a) \\ \\ \\ \\ (b) \\ \\ \\ \\ (c)} & \includegraphics[width = 7 cm, keepaspectratio]{ComparisonCombinedImage.png}
  \end{tabular}
  \caption
  {
    Comparison of the result of approximation of the Douglas-Peucker algorithm (b) and approximation of optimal polyline compression (c).
    The green polyline is a ground truth. The red polyline is the source polyline (a), the result of the Douglas-Peucker algorithm~\cite{CompressionAlgorithm} (b), and the result of optimal polyline compression (c). The black dots around vertices of the source polyline are considered locations for the vertices of the compressed polyline. The vertices of the source polyline are deviated from the segments of the ground truth polyline by random values uniformly distributed in the interval .
  }
  \label{fig:Comparison}
\end{figure}

The effectiveness of the approach is shown in Fig.~\ref{fig:ExampleArc}. Nine segments are sufficient to represent the arc with specified precision. The algorithm not only optimizes the number of segments, it also finds the locations of the segments that minimize integral square differences. Therefore, as shown in Fig.~\ref{fig:ExampleArc}, the algorithm tends to construct segments similar in length.

\begin{figure} [t]
  \centering
\includegraphics[width = \columnwidth, keepaspectratio]{ExampleArc2.png}
  \caption
  {
    The black polyline is the source polyline. The red circles are the vertices of the optimal polyline. Ground truth is the arc of . The noise has uniform distribution in the circle of one percent of the arc radius.
  }
  \label{fig:ExampleArc}
\end{figure}

Fig.~\ref{fig:GraphCompressionEfficiency} shows the dependence from the error introduced by a discrete set of considered locations (see section~\ref{sec:Descretization}) to the efficiency of the compression. Flexibility in places where neighboring segments connect each other is very important to reach maximum compression, especially for noisy data.

\section{Optimal Compression by Orthogonal Directions}

The triangular grid for considered locations supports directions by . Reconstruction of orthogonal buildings requires support for ~\cite{ReconstructionOfOrthogonalPolygonalLines} and sometimes . The square grid for considered locations is more appropriate for this task.

Notice that because only certain directions are allowed, only segments between pairs of considered locations aligned by these directions may be parts of the resultant polyline. Suppose that the resultant segment goes between vertex  and . Because it has to be within tolerance for all vertices between  and , it goes through their considered locations (with the exception of the segment deviating close to the tolerance due to discretization of considered locations).

The optimal solution is found by induction. Define the optimal solution for polyline  as
,
where
,
,
and
 is the number of different directions. For orthogonal case , and for  case . Take directions as , .
For , construct the optimal solution for  from the optimal solution for .

were

\\
 is the check that the vector  has angle  (zero length vectors are allowed).

\begin{figure} [t]
  \centering
    \includegraphics[width = \columnwidth, keepaspectratio]{ComplexityGraph2.png}
  \caption
  {
    The number of segments versus discretization error. The polyline was generated by the Brownian motion process in the same way as in Fig.~\ref{fig:EstimationOfAlgorithmComplexity} with  vertices.
  }
  \label{fig:GraphCompressionEfficiency}
\end{figure}

The condition  corresponds to prohibiting changes in direction by .

For the  case, it is possible to restrict the resultant polyline from having sharp angles by not allowing a change of direction by  ().

Notice that there are no checks for the tolerance, direction, and end points because they are satisfied during each induction step.

Analyzing the previous solution along  direction will further reduce the amount of calculations. The total complexity of the algorithm is


For some data, the algorithm may produce an improper result. This happens when the introduction of a zero length segment lowers the penalty.

Because the correct orientation is not known in advance, it is necessary to rotate polylines by different angles and take the solution with the lowest penalty \cite[see section 6]{ReconstructionOfOrthogonalPolygonalLines}.

Fig.~\ref{fig:ExampleOrthogonalBuildings} shows an example for the reconstruction of orthogonal buildings.

\begin{figure} [htb]
  \centering
    \includegraphics[width = 6 cm, keepaspectratio]{ExampleOrthogonalBuildings.png}
  \caption
  {
    The black polylines are reconstructed buildings from lidar data \cite{ReferenceLIDARData}. The red polylines are the resultant orthogonal shapes. The blue polylines are the ground truth taken from \cite{ReferenceGroundTruthData}.
  }
  \label{fig:ExampleOrthogonalBuildings}
\end{figure}

The reconstruction of buildings with  sides are shown in Fig.~\ref{fig:Example45DegreeBuildings}.

\begin{figure} [htb]
  \centering
    \includegraphics[width = 5 cm, keepaspectratio]{ExampleDiagonalBuildings.png}
  \caption
  {
    This differs from Fig.~\ref{fig:ExampleOrthogonalBuildings} by the allowance of  segments.
  }
  \label{fig:Example45DegreeBuildings}
\end{figure}

The main difference of the algorithm described in this section and \cite{ReconstructionOfOrthogonalPolygonalLines} is in the parameters. The specification of the tolerance is easier than the specification of the penalty  for each additional segment.

\section{Conclusion}

This paper describes an approximation algorithm that finds a polyline with the minimum number of vertices while satisfying tolerance restriction. The solution is optimal with the following limitations:
\begin{itemize}
  \item The vertices of the compressed polyline are limited to considered locations (section~\ref{sec:Descretization}).
  \item The test that the vertex of the compressed polyline is located between some vertices of the source polyline is approximate due to the snapping of the breaking point (section~\ref{sec:Optimization}).
  \item The tests for end points (section~\ref{sec:TestingSegmentEndPoints}) and zigzags are approximate (section~\ref{sec:TestingZigZag}).
\end{itemize}

The performance of the algorithm can be greatly improved if the number of considered locations is decreased without losing quality. This requires further research.

\newcommand{\doi}[1]{\textsc{doi}: \href{http://dx.doi.org/#1}{\nolinkurl{#1}}}



\begingroup
\raggedright
\bibliographystyle{IEEEtran}
\bibliography{CompressedPolyline}
\endgroup



\end{document}
