\documentclass{article}
\PassOptionsToPackage{numbers, compress}{natbib}




\usepackage[preprint]{neurips_2019}




\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{subfigure}
\usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{graphicx}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{bbding}
\usepackage{graphicx}
\newcommand{\eat}[1]{}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}  \renewcommand{\algorithmicensure}{\textbf{Output:}} \title{Segmentation Is All You Need}
\author{Zehua Cheng, Yuxiang Wu\thanks{Co-first authors, contributed to this work equally.}, Zhenghua Xu\thanks{Corresponding author, email:zhenghua.xu@hebut.edu.cn.}, Thomas Lukasiewicz, Weiyang Wang\\
State Key Laboratory of Reliability and Intelligence of Electrical Equipment,\\
Hebei University of Technology, China\\
Department of Computer Science, University of Oxford, United Kingdom\\
AISA Research, Hunan Agricultural University, China\\
SnowCloud.ai, China\\}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\begin{document}
\maketitle
\begin{abstract}
Region proposal mechanisms are essential for existing deep learning approaches to object detection in images. Although they can generally achieve a good detection performance under normal circumstances, their recall in a scene with extreme cases is unacceptably low. This is mainly because bounding box annotations contain much environment noise information, and non-maximum suppression (NMS) is required to select target boxes. Therefore, in this paper, we propose the first anchor-free and NMS-free object detection model, called weakly supervised multimodal annotation segmentation (WSMA-Seg), which utilizes segmentation models to achieve an accurate and robust object detection without NMS. In WSMA-Seg, multimodal annotations are proposed to achieve an instance-aware segmentation using weakly supervised bounding boxes; we also develop a run-data-based following algorithm to trace contours of objects. In addition, we propose a multi-scale pooling segmentation (MSP-Seg) as the underlying segmentation model of WSMA-Seg to achieve a more accurate segmentation and to enhance the detection accuracy of WSMA-Seg. Experimental results on multiple datasets show that the proposed WSMA-Seg approach outperforms the state-of-the-art detectors.

\eat{
We propose a new paradigm of the detection task that is anchor-box free and NMS free. Although the current state-of-the-art model that based on region proposed method has been well-acknowledged for years, however as the basis of RPN, NMS cannot solve the problem of low recall in complicated occlusion situation. This situation is particularly critical when it faces up to complex occlusion. We proposed to use weak-supervised segmentation multimodal annotations to achieve a highly robust object detection performance without NMS. In such cases, we utilize poor annotated Bounding Box annotations to perform a robust object detection performance in the difficult circumstance. We have avoided all hyperparameters related to anchor boxes and NMS. Our proposed model has outperformed previous anchor-based one-stage and multi-stage detectors with the advantage of being much simpler. We have reached a state-of-the-art performance in both accuracies and recall rates.
}
\end{abstract}


\section{Introduction}

Object detection in images is one of the most widely explored tasks in computer vision~\cite{he2017mask,he2016deep}. Existing deep learning approaches to solve this task (e.g., R-CNN~\cite{girshick2014rich} and its variants~\cite{girshick2015fast,ren2015fasterrcnn,he2017mask}) mainly rely on region proposal mechanisms (e.g., region proposal networks (RPNs)) to generate potential bounding boxes in an image and then classify these bounding boxes to achieve object detection. Although such mechanisms can generally achieve a good detection performance under normal circumstances, their recall in a scene with extreme cases (e.g., complex occlusion (Fig.~\ref{subfig:occlusion}), poor illumination  (Fig.~\ref{subfig:lightning}), and large-scale small objects (Fig.~\ref{subfig:scale_small})) is unacceptably low.

\begin{figure}[!ht]
  \centering
  \subfigure[Complex occlusion \label{subfig:occlusion}]{
     \includegraphics[width=0.25\textwidth]{figures/occlusion.png}
  }
  \subfigure[Poor illumination \label{subfig:lightning}
  ]{
     \includegraphics[width=0.19\textwidth]{figures/lightning.png}
  }
  \subfigure[Large-scale small objects \label{subfig:scale_small}]{
     \includegraphics[width=0.45\textwidth]{figures/scale_small.png}
  }
  \caption{Extreme cases of object detection in images.\label{fig:extreme_cases}}
\end{figure}

Specifically, detecting objects under extreme cases via region proposal mechanisms encounters two challenges: First, the performance of region proposal mechanisms highly depends on the purity of bounding boxes~\cite{guo2016deep}; however, the annotated bounding boxes in extreme cases usually contain much more environment noise than those in normal cases. This inevitably increases the difficulty of model learning and decreases the resulting confidence scores of bounding boxes, which consequently weakens the detection performance. Second, non-maximum suppression (NMS) operations are used in region proposal mechanisms  to select target boxes by setting an intersection over union (IoU) threshold to filter other bounding boxes. However, it is very hard (and sometimes even impossible) to find an appropriate threshold to adapt to the very complex situations in extreme cases.


Motivated by this, in this work, we propose a \emph{weakly supervised multimodal annotation segmentation (WSMA-Seg)} approach, which uses segmentation models to achieve an accurate and robust object detection without NMS. It consists of two phases, namely, a training and a testing phase. In the training phase, WSMA-Seg first converts  weakly supervised bounding box annotations in detection tasks to multi-channel segmentation-like masks, called \emph{multimodal annotations}; then, a segmentation model is trained using multimodal annotations as labels to learn multimodal heatmaps for the training images. In the testing phase, the resulting heatmaps of a given test image are converted into an instance-aware segmentation map based on a pixel-level logic operation; then, a contour tracing operation is conducted to generate contours for objects using the segmentation map; finally, bounding boxes of objects are created as circumscribed quadrilaterals of their corresponding contours.

WSMA-Seg has the following advantages: (i) as an NMS-free solution, WSMA-Seg avoids all hyperparameters related to anchor boxes and NMS; so, the above-mentioned threshold selection problem is also avoided; (ii) the complex occlusion problem can be alleviated by utilizing the topological structure of segmentation-like multimodal annotations; and (iii) multimodal annotations are pixel-level annotations; so, they can describe the objects more accurately and overcome the above-mentioned environment noise problem. 

Furthermore, it is obvious that the performance of the proposed WSMA-Seg approach greatly depends on the segmentation performance of the underlying segmentation model. Therefore, in this work, we further propose a \emph{multi-scale pooling segmentation (MSP-Seg)} model, which is used as the underlying segmentation model of WSMA-Seg to achieve a more accurate segmentation (especially for extreme cases, e.g., very small objects), and consequently enhances the detection accuracy of WSMA-Seg. 





\eat{
The contributions of this paper are briefly  as follows: 
(i) We propose a weakly supervised multimodal annotation segmentation (WSMA-Seg) approach to achieve an accurate and robust object detection without NMS, which is the first anchor-free and NMS-free object detection approach. 
(ii) We propose multimodal annotations to achieve an instance-aware segmentation using weakly supervised bounding boxes; we also 
develop a run-data-based following 
algorithm  to trace contours of objects. 
(iii) We propose a multi-scale pooling segmentation (MSP-Seg) model  to achieve a more accurate segmentation and to enhance the detection accuracy of WSMA-Seg.
(iv) We have conducted extensive experimental studies  on the Rebar Head, WIDER Face, and MS COCO datasets; the results show that the proposed WSMA-Seg approach outperforms the state-of-the-art detectors on all testing datasets.
}

The contributions of this paper are briefly  as follows: 
\begin{itemize}
  \item We propose a weakly supervised multimodal annotation segmentation (WSMA-Seg) approach to achieve an accurate and robust object detection without NMS, which is the first anchor-free and NMS-free object detection approach. 
  \item We propose multimodal annotations to achieve an instance-aware segmentation using weakly supervised bounding boxes; we also 
develop a run-data-based following 
algorithm  to trace contours of objects. 
  \item We propose a multi-scale pooling segmentation (MSP-Seg) model  to achieve a more accurate segmentation and to enhance the detection accuracy of WSMA-Seg.
  \item  We have conducted extensive experimental studies  on the Rebar Head, WIDER Face, and MS COCO datasets; the results show that the proposed WSMA-Seg approach outperforms the state-of-the-art detectors on all testing datasets.
\end{itemize}












\eat{
After the great success of applying deep learning to computer vision tasks~\cite{krizhevsky2012imagenet}, deep learning community has made impressive progress in various difficult tasks in the field of computer vision~\cite{he2017mask,he2016deep}, natural language processing~\cite{devlin2018bert,vaswani2017attention} and esports~\cite{vinyals2017starcraft,joseph2019mmo}. 

\begin{figure}[!ht]
  \centering
  \subfigure[Occlusion \label{subfig:occlusion}]{
     \includegraphics[width=0.25\textwidth]{figures/occlusion.png}
  }
  \subfigure[Poor Illumination \label{subfig:lightning}
  ]{
     \includegraphics[width=0.19\textwidth]{figures/lightning.png}
  }
  \subfigure[Large-scale Small Objects \label{subfig:scale_small}]{
     \includegraphics[width=0.45\textwidth]{figures/scale_small.png}
  }
  \caption{Extreme Cases\label{fig:extreme_cases}}
\end{figure}

Since detection is one of the most widely adopted tasks in real life, extreme cases always occurred. The extreme cases like complex occlusion (Fig.~\ref{subfig:occlusion}), poor illumination condition (Fig.~\ref{subfig:lightning}), and large-scale small objects scene (Fig.~\ref{subfig:scale_small}) make the task more challenging. When the model encounters these extremes, the recall is unacceptably low for most widely adopted applications. To tackle this, scholars usually adopt two strategies. The first strategy would train a model over a dataset with tons of extreme cases so the model could directly learn to answer the extreme cases from the data. The other strategy may base on designing a model that could tackle the extreme situation. However, designing complex models like \cite{chi2018selective} also introduce heavy engineering and highly hyperparameters-tuning required. Moreover, lead to lousy generalization when diversity growth~\cite{song2018beyond}. For believers of the first strategy, more challenging datasets with well-annotated annotations and introduce well-classified difficult cases~\cite{yang2016wider} have been introduced. These datasets make the evaluation of extreme cases more valuable.



The most widely adopted approaches are R-CNN approaches inherit the philosophy of regional proposal networks (RPNs)~\cite{girshick2014rich,girshick2015fast,ren2015fasterrcnn,he2017mask}. These region proposed methods generating potential bounding boxes in an image and then run a classifier on these proposed Bounding Boxes. Usually, the region proposed methods can achieve high accuracy with post-processing approaches to refine the bounding boxes, eliminate duplicate detections, and restore the boxes based on other objects in the scene. However, the inherent RPN solution gets struck for the extreme cases since the Bounding Box would introduce noise which polluted the region of interest (ROI). 

In extreme cases, the region proposed method encounters unsolvable bottlenecks due to NMS operations. The NMS process first ranks the scores of all the boxes and selects the highest score and the corresponding box. Then traverse the remaining boxes, delete the boxes whose IoU is greater than a certain threshold, and then continue to select the one with the highest score from the remaining boxes.
Since all region proposed methods are highly required to use Non-Maximum Suppression (NMS) or its variants~\cite{bodla2017soft} to generate the Bounding Box. However, for complex occlusion condition, there is no appropriate threshold to achieve a good result. The successful of Cascade R-CNN~\cite{pang2017cascade} has also proved that variable IoU would significantly improve the performance. What's worse, the irrelevant information or the illumination and occlusion could be treated as a very tough noise which would definitely decrease the confidence.
Decreased confidence also increases the difficulty of the NMS operation threshold selection. The region proposed methods highly rely on the fully supervised annotations which means the performance is related to the purity of Bounding Box~\cite{guo2016deep}. And not all the people follow the existing annotating standard like~\cite{voc2011guidelines} to build the dataset. Some of the datasets may introduce more noise during the annotating. What is worse, the Bounding Box cannot represent some challenging tasks like occlusion, small object, and illumination since the noise are introduced by the scene information.



To avoid introducing noise, there is an ideal solution for introducing segmentation directly on the image. The goal of segmentation is to label each pixel of an image with a corresponding class of what is being represented. This would allow the model to perform a pixel-level classification on the images. By introducing segmentation means the model can utilize the retrained specific topological structure of the annotations, which make occlusion problem can be alleviated by applying CNNs to learn the topological of input image~\cite{lee2019ficklenet}. The pixel-level annotation could well-describe the small objects in the scene and significantly reduce the potential of the noise introduced.

However, the cost of obtaining pixel-level segmentation masks is more than  times expensive than that of annotating Bounding Box on object detection task~\cite{lin2014microsoft}. Annotating pixel-level annotations of each object class is laborious, and hampers the expansion of object classes, which have hindered the performance of CNNs that in general desire large-scale data for training. What is worse, most segmentation approaches are not instance-aware~\cite{dai2016fcn,dai2015convolutional,dai2016instance}. Transforming the segmentation result in detection without instance-aware results is hard. Even though the work finished in \cite{dai2016fcn,dai2016instance} has present an instance-aware solution, but these works have introduced multiple models to process this task, which also leads to heavy engineering.
To get over limitation as mentioned above, we propose to use the bounding box information to generate the segmentation-like information and use the segmentation model to perform the accurate detection with the task where the input is weak supervised Bounding Box. We propose a multimodal annotation to solve extreme cases. With multimodal annotations, the background, occlusion information could be utilized so a single segmentation model could solve the complex cases. 


We have viewed that our proposed multimodal annotation method is exceptionally extendable. Any new features could easily apply for its benefits over the annotations.
In our experiments, we have observed that although our results could reach a state-of-the-art performance over small objects(objects like  pixel). Some \textit{extreme small cases} where we describe the cases as size under  pixels. These cases are usually discarded in the \textit{small object cases} for its extreme size. However, in this work, we also introduce a novel approach using the vector field in the image. With the vector field assistance, the \textit{extreme small cases} are conquerable. 


Our multimodal annotation has an assumption that \textit{each Bounding Box is a circumscribed quadrilateral of the geometric shape we interested}. However, in our experiments, we have observed that it is not a strict assumption, which means the given Bounding Box does not cover the object strictly; the performance still aced the state-of-the-art methods.
This phenomenon also indicated that even with weak-supervised segmentation annotation generated from contour tracing algorithm, our proposed method could achieve strong robustness in extreme situations including complex occlusion and extreme small object.


The contributions of this paper are as follows:
\begin{enumerate}
  \item We proposed to use the weak reinforced inexact supervised approach to generate the segmentation map using the Bounding Box information.
  \item We proposed to use multimodal annotations to achieve instance aware solution.
  \item We proposed a non-NMS approach to achieve strong robustness in extreme cases.
  \item We proposed to introduce vector field in our multimodal annotations to achieve state-of-the-art performance on extremely small objects.
\end{enumerate}
}

\section{Weakly Supervised Multimodal Annotation Segmentation}



In this section, we introduce our approach to object detection using \emph{weakly supervised multimodal annotation segmentation (WSMA-Seg)}. WSMA-Seg generally consists of two phases: a training phase and a testing phase. In the training phase, as shown in Figure~\ref{fig:training_phrase}, WSMA-Seg first converts the weakly supervised bounding box annotations to pixel-level segmentation-like masks with three channels, representing interior, boundary, and boundary on interior masking information, respectively; the resulting annotations are called \emph{multimodal annotations}; then, multimodal annotations are used as labels to train an underlying segmentation model to learn corresponding multimodal heatmaps for the training images. In the testing phase, as shown in Figure~\ref{fig:testing_phrase}, we first send the given testing image into the well-trained segmentation model to obtain multimodal heatmaps; then, the resulting three heatmaps are converted into an instance-aware segmentation map based on a pixel-level logic operation; finally, a contour tracing operation is conducted to generate contours for objects using the segmentation map, and the bounding boxes of objects are created as circumscribed quadrilaterals of their contours. The rest of this section will introduce the main ingredients of WSMA-Seg. 


\begin{figure}[!t]
  \centering
     \includegraphics[width=0.70\textwidth]{figures/training_process_new.jpg}
     \caption{Training phase of WSMA-Seg.\label{fig:training_phrase}}
\end{figure}

\begin{figure}[!t]
\vspace{-0.5em}
  \centering
     \includegraphics[width=0.99\textwidth]{figures/testing_process_new.jpg}
     \caption{Testing phase of WSMA-Seg.\label{fig:testing_phrase}}
		\vspace{-1.5em}
\end{figure}


\subsection{Generating Multimodal Annotations}

Pixel-level segmentation annotations are much more representative than bounding box annotations, so they can resolve some extreme cases that are challenging for bounding box annotations. However, creating well-designed pixel-level segmentation masks is very time-consuming, which is about  times of creating bounding box annotations~\cite{lin2014microsoft}. Therefore, in this work, we propose a methodology to automatically convert  bounding box annotations to segmentation-like multimodal annotations, which are pixel-level geometric segmentation-like multichannel annotations. Here, ``geometric segmentation-like'' means that the multimodal annotations are not strict segmentation annotations; rather, they are annotations generated from simple geometries, e.g., inscribed ellipses of bounding boxes. This is motivated by the finding in~\cite{dai2015boxsup} that  pixel-level segmentation information is not fully utilized by segmentation models; we thus believe that  well-designed pixel-level segmentation annotations may not be essential to achieve a reasonable performance; rather,  pixel-level geometric annotations should be enough. Furthermore, to generate a bounding box for each object in the image, an instance-aware segmentation is required; to achieve this, multimodal annotations are designed to have multiple channels to introduce additional information.

Specifically, as shown in Figure~\ref{fig:training_phrase}, multimodal annotations use three channels to represent pixel-level masking information regarding the interior, the boundary, and the boundary on the interior of geometries. These three different pixel-level masks are generated as follows: Given an image with bounding box annotations, we first obtain an inscribed ellipse for each bounding box, then the interior mask (channel ) is obtained by setting the values of pixels on the edge of or inside the ellipses to , and setting the values of other pixels to . Then, the boundary mask (channel ) is obtained by setting the values of pixels on the edge of or within the inner width  of the ellipses to , and setting the rest to . Similarly, the boundary on the interior mask (channel ) is generated by setting the values of pixels on the edge of or within the inner width  of the area of the elliptical overlap to .


\subsection{Multi-Scale Pooling Segmentation}
It is obvious that the performance of the proposed WSMA-Seg approach  greatly depends on the segmentation performance of the underlying segmentation model. Therefore, in this work, we further propose a \emph{multi-scale pooling segmentation (MSP-Seg)} model, which is used as the underlying segmentation model of WSMA-Seg to achieve a more accurate segmentation (especially for extreme cases, e.g., very small objects), and to consequently enhance the detection accuracy of WSMA-Seg. 

As shown in Figure~\ref{fig:multiscale_model}, MSP-Seg is an improved segmentation model of Hourglass~\cite{newell2016stacked}. The main improvement of MSP-Seg is to introduce a \emph{multi-scale block} on the skip connections, performing \emph{multi-scale pooling} operations to the output feature maps of residual blocks. Specifically, as shown in Figure~\ref{fig:fox_block}, multi-scale pooling utilizes four pooling kernals with sizes , , , and  to simultaneously conduct average pooling operations on the previous feature maps generated by residual blocks on skip connections. Then, four feature maps generated by different pooling channels are concatenated to form a new feature map whose number of channels is four times of the previous feature maps. Here, to ensure that the four feature maps have the same size, the stride is set to , and zero-padding is conducted. Finally, we apply  convolution  to restore the number of channels, and element-wise addition to merge the feature maps. As shown in Figure~\ref{fig:multiscale_model}, by using multimodal annotations as labels, MSP-Seg is trained to learn three heatmaps for each image, which are called \emph{interior heatmap}, \emph{boundary heatmap}, and \emph{boundary on interior heatmap}, respectively.

Intuitively, multi-scale pooling is capable of enhancing the segmentation accuracy, because it combines features of different scales to obtain more representative feature maps. Please note that, as a highly accurate segmentation model, MSP-Seg can be widely applied to  various segmentation tasks.



\begin{figure*}
  \centering
  \includegraphics[width=0.95\textwidth]{figures/model_structure.png}
  \caption{Multi-scale pooling segmentation model.\label{fig:multiscale_model}}
\end{figure*}

\begin{figure}
  \centering
  \includegraphics[width=0.6\textwidth]{figures/foxblock.png}
  \caption{Multi-scale block.\label{fig:fox_block}}
\end{figure}



\subsection{Object Detection Using Segmentation Results and Contour Tracing}

After obtaining a well-trained segmentation model, we are now able to conduct object detection. As shown in Figure~\ref{fig:testing_phrase}, given a test image as the input of the segmentation model, WSMA-Seg first generates three heatmaps, i.e., interior, boundary, and boundary on interior heatmaps, which are denoted as \emph{I}, \emph{B}, and \emph{O}, respectively. These three heatmaps are then converted to binary heatmaps, where the values of pixels in interested area are set to , and the rest is set to . This conversion is conducted following the approach  in~\cite{suzuki1985topological}. Furthermore, a pixel-level operation, , is used to merge three heatmaps into an instance-aware segmentation map. 

Finally, a contour tracing operation is conducted to generate contours for objects using the instance-aware segmentation map, and the bounding boxes of objects are created as circumscribed quadrilaterals of their contours. One conventional way to trace a contour is to use scan-based-following algorithm~\cite{suzuki1985topological}. However, in the case of a large image with many objects (which is common in detection tasks), scan-based-following algorithm is very time consuming.

Therefore, motivated by the work in~\cite{agrawala1977sequential}, we propose a modified run-data-based (RDB) following algorithm, which greatly reduces the time and memory costs of the contour tracing operation. 
Pseudocode of the RDB following algorithm is shown in Algorithm~\ref{rdb} and an example is shown in Figure~\ref{fig:contour_tracing}. Differently from the pixel-following algorithm that requires to scan the entire image to find the starting point and tracing contour pixels along the clockwise direction to generate the results recurrently, the RDB following algorithm only needs to save two lines of pixel values and to scan the whole image once, which significantly reduces the memory consumption and increases the speed.

Specifically, RDB following algorithm first initialize two variables  and  with \emph{null} value, then scans the binary instance-aware segmentation map row by row from the top-left corner to the bottom-right corner to find contours (lines -). If a pixel's value is  and its left pixel's value is , then this pixel is on the left side of a contour, so it is assigned to ; similarly, if a pixel's value is  and its right pixel's value is , then this pixel is on the right side of a contour, so it is assigned to  (lines -). When both  and  are found, we check if there exists a pair of  and  on above line whose x-coordinates are the same as or greater/smaller by  than the corresponding x-coordinates of  and ; if so, we add  and  to the same contour set as  and ; otherwise, we create a new contour set and add  and  to it (lines -).





\begin{figure*}[!t]
  \centering
     \includegraphics[width=0.85\textwidth]{figures/contourTracing.png}\label{fig:contour_tracing}
  \caption{Contour tracing.}
  \end{figure*}




\begin{algorithm}[!t]
  \caption{Run-data-based following algorithm\label{rdb}}
  \begin{algorithmic}[1]
     \Require
     A binary image with  (height) and  (width).
     \Ensure
     A list of contour sets.
		\State , 
		\For{ in }
			\For{ in }
				\If{  and }
					\State 
				\EndIf
				\If{ and }
					\State 
				\EndIf
				\If{  !=  and  != }
					\If{there exists a pair of  and  in row  and \\ 
					\hspace{45pt}  and\\
					\hspace{45pt} }
						\State Add  and  to the same contour set as  and 
					\Else
						\State Creat a new contour set and add  and   to it
					\EndIf
					\State  and 
				\EndIf
			\EndFor
		 \EndFor
  \end{algorithmic}
\end{algorithm}


\eat{
\begin{algorithm}[!t]
  \caption{Run-data-based following algorithm\label{rdb}}
  \begin{algorithmic}[1]
     \Require
     A binary image with (height) and (width).\\
      the pixel  whose value is  and left pixel is 0.\\
      the pixel  whose value is  and right pixel is 0.
     \Ensure
     A list of contour pairs sets.
     \For{ in }
     \While{scan the row }
        \State find a pair  of  and 
           \If{ and }
              \State add the pair to the contour set of the pair 
           \Else
              \State create a new contour set with this pair
           \EndIf
     \EndWhile
     \EndFor
  \end{algorithmic}
\end{algorithm}
}


















































\eat{
In this section, we first reformulate object detection in a pixel-level annotation mask. Besides that, the inner and outer information would also bring better representation to the pixel-level annotations. 
Since the introduced additional information has highlighted the inner, outer and boundary information, it would help the FCN-based model like UNet~\cite{ronneberger2015u} to extract more details that are beyond the topological form. This would also be eliminating the side effects of the original high resolution which unavoidably sacrifice the purity of the region of the interest(RoI).
Therefore, we combine internal, external and boundary information with pixel-level annotation into the multimodal annotation.
Our proposed structure is presented in Figure~\ref{fig:process}. The system takes a color image of size  as input to produce a sequence of  coordinates of boundary pixels as output.
When training phase, our CNN target multimodal annotation heatmaps, which are produced by the given Bounding Box.
When inferencing phase, our find contour algorithm parses multimodal results to output the localization of all objects.
To further illustrate our proposed method, we discuss the details of the multimodal annotations in Section~\ref{sec:multimodal_annotation}, the details of the \textit{Contour Tracing} that presented in the Figure~\ref{fig:process} is lies in Section~\ref{sec:contour_finding}. The Section~\ref{sec:training} illustrate the details of the training process, including our proposed MultiScale Model as illustrated in Figure~\ref{fig:multiscale_model} and Figure~\ref{fig:fox_block}, which has proved to be an excellent model that could utilize the most of the details in the image.


\subsection{Multimodal Annotations\label{sec:multimodal_annotation}}
\eat{
The motivation of our multimodal annotations strategy came from a simple fact that segmentation usually has better representation and have a strong ability to solve some tough cases that the detection cannot solve. 
Usually, the efficiency of the segmentation task is higher, which turned out that requires fewer samples than the detection task to reach good performance.
However, the pixel-level segmentation mask does not fully utilize by the models~\cite{dai2015boxsup}. The generated mask is not pixel-level since the models that with strong details have the potential to over-fitting. We have taken into serious consideration about the fact that claimed at the beginning of this paragraph. Therefore, we conclude that the well-designed pixel-level segmentation mask may not essential.
The rest of the elements of the segmentation mask are \textit{pixel-level annotation} and \textit{geometric shape}. We viewed the pixel-level is an essential representation method for utilizing the geometric shape. In such cases, we simplify the essential elements of a \textbf{strong representation annotations} as \textbf{simple geometric pixel-level annotations}. With the existing information for the detection task, we turned our attention to transform the bounding box(BBox) to generate instance aware weak-supervised segmentation map with a pixel-level simple geometric shape. 

An instance-aware solution is required since it is not suitable for the jitter segmentation results to generate any bounding box.
Since the inexact supervision which usually causes a disaster in occlusion and small object condition~\cite{zhou2017weaklysupervised}, we have to solve the unsatisfying performance by introducing additional information to the annotations. This approach helps this method to have stronger representation and also achieve better performance on detection.
}
\begin{figure*}
  \centering
  \includegraphics[width=0.85\textwidth]{figures/details.png}
  \caption{Details of our proposed multimodal annotations.\label{fig:details_annotations}}
\end{figure*}


We assume that each Bounding Box is a circumscribed quadrilateral of the geometric shape of the corresponding object.
The boundary is defined as a complete contour line, which is defined as a segment of one or more pixels in width and length and is used to describe the shape of the object and its relationship with other objects. The detected contours are reinforced inexact supervision. In Figure~\ref{fig:details_annotations}, we have present an example of transforming the \textit{box-level} annotations to \textit{pixel-level} annotations.
The generated pixel-level annotations allow fully supervised segmentation to achieve reliability in learning the boundaries of objects and the relationship between their components. In given information of the Bounding Box including the width and height of the bounding box shape and the geometric center of the object. By transforming the objects, we would get a simple geometric shape of the objects. In such a case, the annotation would become a hollow shape with a . It is really hard for the models to learn the boundaries where the boundary only takes 1 pixel. Hence, we would like to suggest to have a larger  instead. The width of the annotations is essential for the model to generate the instance aware segmentation results. This process is described in details in the Algorithm~\ref{algo:multimodal}.
However, roughly applying the contour information to generate the segmentation map would be lost the semantic information since the Bounding Box has a clear boundary while merging the pixel-level annotations would break the boundary of the information. So we proposed a novel strategy to generate the information of bilateral occlusion relationship. 
As Figure~\ref{fig:details_annotations} presented, three annotations are denoted as Background, Ordinary Interior Section, and Ordinary Boundary, respectively. We combine these three conditions as additional annotations with our proposed approach to train the model.
}


\eat{
\subsection{Training Process\label{sec:training}}
\begin{figure*}
  \centering
  \includegraphics[width=0.9\textwidth]{figures/model_structure.png}
  \caption{Proposed MultiScale Model Structure\label{fig:multiscale_model}}
\end{figure*}
The process is presented in Figure~\ref{fig:process}. We discuss the details of our training process in this section. However, for some challenging tasks like WiderFace Dataset which require to effectively process and consolidate features across scales. To tackle this, we have adopted a novel structure to make full use of the details in the segmentation tasks. The model structure is presented in Figure~\ref{fig:multiscale_model} and Figure~\ref{fig:fox_block}, the first stage would produce a set of abstract feature , where  are the head of our model. The residual blocks are used to process features down to a very low resolution. We concatenate the output feature maps of the different kernel size of average pooling layer in the MultiScale Block. To cut down the parameters cost, we perform a (a 2d convolution operation with kernel size = ) to compress the channel information. The output of the  were added in the model.
\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{figures/foxblock.png}
  \caption{MultiScale Block\label{fig:fox_block}}
\end{figure}
Moreover, in each subsequent stage, the block inherits multiscale information in the previous stage to produce more robust features .
At the end, two different point-wise convolution generate segmentation result  and feature result , where  is the depth-wise convolution and  is the numebr of channel of them. 
In order to fully utilize the facial multi-scale information, we proposed to use Hourglass~\cite{newell2016stacked}, as our backbone. 
We also perform the ablation study in Table~\ref{table:rebar} and Talbe~\ref{table:widerface_ablation}.

We put the bounding box level annotations and the original image as the input. The first process would be transforming the bounding box to the multimodal annotations. We only use the generated multimodal annotations as our training target. Since the segmentation results are always coarse, our segmentation results do not have an adorable shape even the original input information is made up of simple geometry with occlusion information as stated above. Meanwhile, the output results are also numerical and our goal is not to perform segmentation on the image but to get a good performance on detection. Therefore, we transform the numerical segmentation results in binary results.

We using binary cross-entropy as our loss function(Equation~\ref{equation:bce}).



Where  is the label,  is the prediction probability of all  samples. This result would be a binary image, in which case the foreground is  and the background is .

We developed a contour tracing algorithm(see the details in Section~\ref{sec:contour_finding}) to detect the edges of a given region in the Bounding Box to generate the simple geometric shape as presented in Figure~\ref{fig:contour_tracing}.
With the generated contour information, we could generate the segmentation map inside the Bounding Box.




\begin{algorithm}
  \caption{Transfering Bounding Box Annotations to Multimodal Annotations\label{algo:multimodal}}
  \begin{algorithmic}[1]
     \Require
     \\
      is the list of Bounding Boxes.\\
      is the width of contour.
     \Ensure
     label heatmap of interior, contour, contour on interior 
     \For{ in }
        \State 
        \State Center point of inscribed ellipse  of  
        \State Interior ellipse  a solid ellipse whose axes are  and 
        \State Interior mask  a matrix of interior ellipse, other pixels are 
        \State Object ellipse  a hollow ellipse whose axes are  and 
        \State Contour mask  a matrix of interior ellipse  object ellipse, others are 
        \State Interior Heatmap Interior mask
        \State Contour Heatmap Contour mask
     \EndFor
     \State Contour on Interior HeatmapInterior heatmap intersect contour heatmap
  \end{algorithmic}
\end{algorithm}




\begin{figure*}[!ht]
  \centering
     \includegraphics[width=0.7\textwidth]{figures/contourTracing.png}\label{fig:contour_tracing}
  \caption{Contour Tracing Algorithm}
  \end{figure*}


\subsection{Localization by Contour Tracing Algorithm\label{sec:contour_finding}}
The main problem is how to localize all the existing objects. 
We utilize Contour Tracing algorithm to take the multimodal heatmaps that generated by our deep learning models(i.e. UNet~\cite{ronneberger2015u}), which is the Interior Heatmap(), the Boundary Heatmap() and the Boundary on Interior() to produce Cartesian coordinates of boundaries of all objects. Our strategy is as follows: (i) we first prune the interior heatmap and generate binary images~\cite{suzuki1985topological}; (ii) then we parse the coordinates of boundaries from the binary image.

\textbf{Generating Binary Image}
Limited by network training degree, when an object is closer to other objects, the interior of the object becomes viscous with others and the region goes larger including others,
so that the detection of object boundaries has an irreversible effect.
So we design a method to prune the viscous pixels utilizing our multimodal heatmaps.

As shown below:

Before tracing contour, the 8-bit heatmaps need to tansfer to binary image \cite{suzuki1985topological}.
So we threshold the pruned image and obtain a binary image which foreground is , background is .



\textbf{Run-data-based Following Algorithm}

One conventional way to trace contour is using scan-based-following algorithm after image binarization, pixel by pixel connected into contour\cite{suzuki1985topological}.
However, in the case of a large image with many objects, the most situation we are, time consumption will be quite high\cite{s16030353}.
The reason is that this algorithm will raster scan the whole image to find the starting 
point first and trace edge point one by one, and resume the scan from the next pixel to find 
the next starting point.

For less time and memory consuming, we propose a modified run-data-based(RDB) following algorithm that uses only one or two line buffers and loads all pixel only once\cite{agrawala1977sequential}.
So we use RDB following algorithm to trace our rebar dataset which has a host of rebar head in an image and output a sequence of coordinates of pixels of object boundaries. We view the minimum and maximum  and  value as the top-left and bottom-right coordinates of Bounding Box.

\begin{algorithm}
  \caption{Run-data-based following algorithm\label{rdb}}
  \begin{algorithmic}[1]
     \Require
     A binary image with (height) and (width).\\
      the pixel  whose value is  and left pixel is 0.\\
      the pixel  whose value is  and right pixel is 0.\\
     \Ensure
     A list of contour pairs sets.
     \For{ in }
     \While{Scan the  row}
        \State Find a pair  of  and 
           \If{ and }
              \State Add the pair to the contour set of the pair 
           \Else
              \State Create a new contour set with this pair
           \EndIf
     \EndWhile
     \EndFor
  \end{algorithmic}
\end{algorithm}







}

\section{Experiments}

To show the strength of our proposed WSMA-Seg approach in object detection, extensive experimental studies have been conducted on three benchmark datasets, namely, the Rebar Head\footnote{https://www.datafountain.cn/competitions/332/details}, WIDER Face\footnote{http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/}, and MS COCO datasets\footnote{http://cocodataset.org/}, each of which containing many extreme cases. The important parameters of WSMA-Seg are as follows: \textit{Stack} is the number of the stacked hourglass networks (see~\cite{newell2016stacked} for more details about hourglass), \textit{Base} is a pre-defined basic number, and the number of channels is always an integer  multiple of Base, and \textit{Depth} is the number of down-samplings. \textit{Stem} represents three consecutive  convolution operations with stride  before the first stack.

\subsection{Rebar Head Detection}



We first conduct experiments on the Rebar Head detection dataset, which consists of  training images (including a total of  rebar heads) and  testing images. The orignal resolution of the whole image is . Performing object detection on this dataset is very challenging, because it only contains a few training samples  and also encounters very severe occlusion situations (see Figure~\ref{fig:rebar_example}). In addition, the target rebar heads are very small: the average area of each box is  pixels, taking up only 0.13\% of the whole image. The images are also poorly annotated and rich in diverse illuminations. 


\begin{figure}[!t]
  \centering
  \includegraphics[width=0.45\textwidth]{figures/rebar_head.jpg}
  \caption{An example of complex occlusion in the Rebar Head dataset.\label{fig:rebar_example}}
\end{figure}

Two state-of-the-art anchor-based models, Faster R-CNN~\cite{ren2015fasterrcnn} and Cascade R-CNN~\cite{cai2018cascade}, are selected as the baselines. Table~\ref{table:rebar} shows the detection performances of our proposed WSMA-Seg and baselines on this dataset.
As shown in Table~\ref{table:rebar}, our proposed method with , ,  has achieved the best performance among all solutions in terms of F1 Score. In addition, the number of parameters needed for WSMA-Seg is much less than the baselines (only  of Cascade RCNN and  of Faster RCNN), while the number of training epochs for WSMA-Seg is also less than those of the baselines. Therefore, we can conclude that, compared to the state-of-the-art baselines, WSMA-Seg is much simpler, more effective, and more efficient.

\begin{table*}[!t]
  \caption{Detection performances of WSMA-Seg and baselines on the Rebar Head dataset.\label{table:rebar}}
  
  \smallskip 
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|}
  \hline
  Method&\#parms&Epoch&F1 Score\\ \hline
  Faster RCNN&23.2M&100&98.30\%\\\hline
  Cascade RCNN~\cite{cai2018cascade}&42.1M&100&98.70\%\\\hline
  WSMA-Seg(stack=1,base=72,depth=3)&6.1M&70&94.27\%\\\hline
  WSMA-Seg(stack=2,base=40,depth=5)&5.8M&70&\textbf{98.83\%}\\\hline
  WSMA-Seg(stack=4,base=28,depth=5)&5.7M&70&96.26\%\\\hline
  \end{tabular}
\end{table*}

\subsection{WIDER Face Detection}
We further conduct experiments on the WIDER Face detection dataset~\cite{yang2016wider}, which consists of  images and  faces. Face detections in this dataset are extremely challenging due to a high degree of variability in scale, pose, and occlusion. WIDER Face results in a much lower detection accuracy compared to other face detection datasets. WIDER Face has defined three levels of difficulties (i.e., \textit{Easy}, \textit{Medium}, and \textit{Hard}),  based on the detection accuracies of EdgeBox~\cite{zitnick2014edge}. 
Furthermore, the dataset also treats occlusion as an additional attribute and is partitioned into three categories: \textit{no occlusion}, \textit{partial occlusion}, and \textit{heavy occlusion}. Specifically, a face is categorized as \textit{partial occlusion} when 1\% to 30\% of the total face area is occluded, and a face with the occluded area over 30\% is categorized as \textit{heavy occlusion}. The size of the training set is , that of the validation set is , and that of the testing set is .


Twelve state-of-the-art approaches are selected as baselines, namely, Two-stage CNN, Cascade R-CNN, and LDCF+\cite{ohn2016boost}, multitask Cascade CNN~\cite{zhang2016joint}, ScaleFace~\cite{yang2017face}, MSCNN~\cite{cai2016unified}, HR~\cite{hu2017finding}, Face R-CNN~\cite{wang2017detecting}, Face Attention Networks~\cite{wang2017face}, and PyramidBox~\cite{tang2018pyramidbox}.
The experimental results in terms of F1 score are shown in Table~\ref{table:widerface_f1}.
The results show that our proposed WSMA-Seg outperforms the state-of-the-art baselines in all three categories, reaching , , and  in Easy, Medium, and Hard categories, respectively.




\begin{figure*}
  \centering
  \includegraphics[width=0.9\textwidth]{figures/WiderFace.png}
  \caption{F1 scores on the WIDER Face dataset.\label{table:widerface_f1}}
\end{figure*}

\eat{
\begin{table*}[h]
  \centering
  \caption{Ablation study on Widerface Val Dataset\label{table:widerface_ablation}}
  \begin{small}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
      \hline
         Stem&Stack&Base&Depth&\#params&Time&Epoch&Easy&Medium&Hard\\ \hline
         \Checkmark&2&40&5&6.08&32.19&65&\textbf{94.70\%}&\textbf{93.41\%}&\textbf{87.23\%}\\\hline
         &2&40&5&5.78&29.47&65&91.45\%&90.50\%&82.86\%\\\hline
         \Checkmark&2&40&5&6.07&32.16&65&93.83\%&92.56\%&84.33\%\\\hline
         \Checkmark&2&40&5&5.55&30.52&65&89.57\%&88.85\%&79.56\%\\\hline
         \Checkmark&4&28&5&5.68&34.58&65&90.53\%&90.01\%&82.54\%\\\hline
         \Checkmark&1&72&3&6.13&32.17&65&90.67\%&89.86\%&82.31\%\\\hline
      \end{tabular}
  \end{small}
\end{table*}
}


\subsection{MS COCO Detection}
Finally, we conduct experimental studies on the MS COCO detection dataset~\cite{lin2014microsoft}, which is one of the most popular large-scale detection datasets. Our results are obtained using the test-dev split (20k images) with a host of the detection method. We have constructed the training set with  samples, the validation set with  samples, and the testing set with  samples. 
We use the metrics as used in \cite{lin2014microsoft} to characterize the performance. Four types of metrics are defined and described as follows:

\begin{small}
\begin{itemize}
  \item \textbf{Average Precision (AP):}
  \begin{itemize}[noitemsep]
     \item : AP at IoU=.50:.05:.95 (primary challenge metric)
     \item : AP at IoU=.50 (PASCAL VOC metric)
     \item AP at IoU=.75 (strict metric)
  \end{itemize}
  \item \textbf{AP Across Scales:}
  \begin{itemize}[noitemsep]
     \item : AP for small objects: area  
     \item : AP for medium objects:   area  
     \item  : AP for large objects: area  
  \end{itemize}
  \item \textbf{Average Recall (AR):}
  \begin{itemize}[noitemsep]
     \item  : AR given 1 detection per image
     \item  : AR given 10 detections per image
     \item  : AR given 100 detections per image
  \end{itemize}
  \item \textbf{AR Across Scales:}
  \begin{itemize}[noitemsep]
     \item  : AR for small objects: area  
     \item  : AR for medium objects:  area 
     \item  : AR for large objects: area
  \end{itemize}
\end{itemize}
\end{small}
Seven state-of-the-art solutions are selected as baselines, and the experimental results for four types of metrics are shown in Tables~\ref{coco_precision} and~\ref{coco_recall}. The results show that our WSMA-Seg approach outperforms all state-of-the-art baselines in terms of most metrics, including the most challenging metrics, , , , and . For the other metrics, the performance of our proposed approach is also close to those of the best baselines. This proves that the proposed WSMA-Seg
approach generally achieves more accurate and robust object detection than the state-of-the-art approaches without NMS.



\eat{
\begin{table*}[!ht]
  \centering
  \caption{Average Precision on MS COCO (test-dev)\label{coco1}}
  \begin{small}
	\vspace{0.3em}
    \begin{tabular}{|c|c|c|c|c|}
      \hline
      Method&Backbone& & &\\\hline
      WSMA-Seg&FOXNet&\textbf{38.1}&\textbf{58.2}&\textbf{40.7}\\\hline
      CoupleNet~\cite{zhu2017couplenet}&ResNet-101&34.4&54.8&37.2\\\hline
      Faster R-CNN&ResNet-101-FPN~\cite{lin2017feature}&36.2&59.1&39.0\\\hline
Mask R-CNN&ResNeXt-101&39.8&62.3&43.4\\\hline
DSOD300&DS/64-192-48-1&29.3&47.3&47.0\\\hline
      SSD513&Resnet-101&31.2&50.4&33.3\\\hline
      \end{tabular}\vspace{0.3em}
      \caption{Average Precision Across Scales on MS COCO (test-dev)\label{coco2}}
\vspace{0.3em}
      \begin{tabular}{|c|c|c|c|c|}
      \hline
      Method&Backbone&&&\\\hline
      WSMA-Seg&FOXNet&\textbf{22.5}&41.0&\textbf{51.9}\\\hline
      CoupleNet~\cite{zhu2017couplenet}&ResNet-101&13.4&38.1&50.8\\\hline
Faster R-CNN&ResNet-101-FPN~\cite{lin2017feature}&18.2&39.0&48.2\\\hline
Mask R-CNN&ResNeXt-101&22.1&\textbf{43.2}&51.2\\\hline
DSOD300&DS/64-192-48-1&9.4&31.5&47.0\\\hline
      SSD513&Resnet-101&10.2&34.5&49.8\\\hline
      \end{tabular}\vspace{0.3em}\caption{Average Recall with different detections per image on MS COCO (test-dev)\label{coco3}}
			\vspace{0.3em}
      \begin{tabular}{|c|c|c|c|c|}
        \hline
        Method&Backbone&&&\\\hline
        WSMA-Seg&FOXNet&\textbf{35.2}&\textbf{52.1}&\textbf{57.8}\\\hline
        CoupleNet~\cite{zhu2017couplenet}&ResNet-101&30.0&45.0&46.4\\\hline
Faster R-CNN&ResNet-101-FPN~\cite{lin2017feature}&20.0&29.3&29.7\\\hline
Mask R-CNN&ResNeXt-101&21.6&30.2&30.6\\\hline
DSOD300&DS/64-192-48-1&27.3&40.7&43.0\\\hline
        SSD513&Resnet-101&28.3&42.1&44.4\\\hline
        \end{tabular}\vspace{0.3em}
    
        \caption{Average Recall Across Scales on MS COCO (test-dev)\label{coco4}}
				\vspace{0.3em}
        \begin{tabular}{|c|c|c|c|c|}
          \hline
          Method&Backbone&&&\\\hline
          WSMA-Seg&FOXNet&\textbf{36.1}&\textbf{58.4}&\textbf{73.2}\\\hline
          CoupleNet~\cite{zhu2017couplenet}&ResNet-101&20.7&53.1&68.5\\\hline
Faster R-CNN&ResNet-101-FPN~\cite{lin2017feature}&7.2&34.8&44.4\\\hline
Mask R-CNN&ResNeXt-101&11.2&34.6&42.2\\\hline
DSOD300&DS/64-192-48-1&16.7&47.1&65.0\\\hline
          SSD513&Resnet-101&17.6&49.2&65.8\\\hline
          \end{tabular}\vspace{0.3em}
  \end{small}
\end{table*}
}










\begin{table*}[!ht]
   \centering
   \vspace{-0.5em}
	\caption{Average precisions of WSAM-Seg and baselines on MS COCO (test-dev)\label{coco_precision}}
\begin{small}
   \begin{tabular}{|l|l|c|c|c|c|c|c|}
   \hline
   Method&Backbone&  &  &&& &  \\\hline
   DSOD300&DS/64-192-48-1&29.3&47.3&30.6&9.4&31.5&47\\\hline
   SSD513&ResNet-101&31.2&50.4&33.3&10.2&34.5&49.8\\\hline
   DSSD513&ResNet-101&33.2&53.3&35.2&13.0&35.4&51.1\\\hline
   DeNet&ResNet-101&33.8&53.4&36.1&12.3&36.1&50.8\\\hline
   CoupleNet&ResNet-101&34.4&54.8&37.2&13.4&38.1&50.8\\\hline
   Faster R-CNN w/ TDM&Inception-ResNet-v2&36.8&57.7&39.2&16.2&39.8&\textbf{52.1}\\\hline
   CornerNet511&Hourglass-52&37.8&53.7&40.1&17.0&39.0&50.5\\\hline
   WSMA-Seg&MSP-Seg&\textbf{38.1}&\textbf{58.2}&\textbf{40.7}&\textbf{22.5}&\textbf{41.0}&51.9\\\hline
  \end{tabular}\end{small}
\end{table*}

\begin{table*}[!ht]
   \centering
   \vspace{-0.5em}
	\caption{Average recalls of WSAM-Seg and baselines on MS COCO (test-dev)\label{coco_recall}}
\begin{small}
   \begin{tabular}{|l|l|c|c|c|c|c|c|}
   \hline
   Method&Backbone& & & && &  \\\hline
   DSOD300&DS/64-192-48-1&27.3&40.7&43&16.7&47.1&65\\\hline
   SSD513&ResNet-101&28.3&42.1&44.4&17.6&49.2&65.8\\\hline
   DSSD513&ResNet-101&28.9&43.5&46.2&21.8&49.1&66.4\\\hline
   DeNet&ResNet-101&29.6&42.6&43.5&19.2&46.9&64.3\\\hline
   CoupleNet&ResNet-101&30.0&45.0&46.4&20.7&53.1&68.5\\\hline
   Faster R-CNN w/ TDM&Inception-ResNet-v2&31.6&49.3&51.9&28.1&56.6&71.1\\\hline
   CornerNet511&Hourglass-52&33.9&\textbf{52.3}&57.0&35.0&\textbf{59.3}&\textbf{74.7}\\\hline
   WSMA-Seg&MSP-Seg&\textbf{35.2}&52.1&\textbf{57.8}&\textbf{36.1}&58.4&73.2\\\hline
  \end{tabular}\end{small}
\end{table*}
	
\section{Conclusion}
In this work, we have proposed a novel approach to object detection in images, called weakly su\-per\-vised multimodal annotation segmentation (WSMA-Seg), which is anchor-free and NMS-free. We observed that  NMS is one of the bottlenecks of existing deep learning 
approaches to object detection in images. The need to tune hyperparameters on NMS has seriously hindered the scalability of high-performance detection frameworks. Therefore, to realize WSMA-Seg, we proposed to use multimodal annotations to achieve an instance-aware segmentation based on weakly supervised bounding boxes, and developed a run-data-based following algorithm to trace contours of objects. In addition, a multi-scale pooling segmentation (MSP-Seg) model was proposed as the underlying segmentation model of WSMA-Seg to achieve a more accurate segmentation and to enhance the detection accuracy of WSMA-Seg. Experimental results on multiple datasets concluded that the proposed WSMA-Seg approach is superior to the state-of-the-art detectors.
{\small
\bibliographystyle{IEEEtran}
\bibliography{egbib}
}
\end{document}
