\appendix

\begin{center}
    \Large{\textbf{Appendix}}
\end{center}


\etocdepthtag.toc{appendix}
\etocsettagdepth{chapter}{none}
\etocsettagdepth{appendix}{subsection}
\tableofcontents


\section{Notation}
\label{sec:appen-notation}

We present the notation table for each symbol used in this paper in \cref{tab:append-notation}.

\begin{table}[h!]
\centering
\caption{Notation Table}
\label{tab:append-notation}
\resizebox{0.85 \textwidth}{!}{\begin{tabular}{@{}l|l@{}}
\toprule
\multicolumn{1}{c|}{Notation} &\multicolumn{1}{c}{Definition}  \\ \midrule
     & A training instance \\
     & A class index label \\
     & An imprecise label, which might contain multiple class indices \\
      &   Data. A set of data instances  of size         \\
      &   Input space where  is drawn from        \\
            &   Ground-truth labels. A set of label indices  of size           \\ 
      &   Label space where  is drawn from        \\
            &    Imprecise labels. A set of imprecise labels  of size      \\ 
     & Model backbone \\ 
     & Model classifier \\
     & Model multi-layer perceptron \\ 
     & Model mapping  \\ 
     & Learnable parameters of  \\ 
     & Output probability from model  \\ 
     & Model mapping , where  is a projected feature space \\ 
 & Dataset \\ 
     & Loss function \\ 
     & Weak data augmentation, usually is HorizontalFlip \\
     & Strong data augmentation, usually is RandAugment \citep{cubuk2020randaugment} \\
     & Projected features from  on weakly-augmented data \\
     & Projected features from  on strongly-augmented data \\
     & Memory queue in MoCo \citep{moco2020} \\
     &  A partial label, with ground-truth label contained \\
     & A set of partial labels \\ 
     & A labeled training example \\ 
     & A labeled class index \\ 
     & A unlabeled training example \\ 
     & A unknown class index for unlabeled data \\ 
      &   A set of labeled data instances       \\
            &   A set of labels for labeled data instances            \\ 
      &   A set of unlabeled data instances       \\
            &   A set of unknown labels for unlabeled data instances            \\ 
     & The maximum predicted probability on unlabeled data ) \\ 
     & The pseudo-label from the predicted probability on unlabeled data )\\
     & The threshold for confidence thresholding \\ 
     & A corrupted/noisy label \\
     & An one-hot version of the corrupted/noisy label \\
     & A set of noisy labels \\ 
      & Noise model related parameters in SOP \citep{sopliu22w} \\
     & The forward score in forward-backward algorithm \\ 
     & The backward score in forward-backward algorithm \\ 
     & The simplified noise transition model in ILL \\ 
     & The parameters in the simplified noise model \\ 
    \bottomrule
\end{tabular}}
\end{table}

\section{Related Work}
\label{sec:appen-related}

Many previous methods have been proposed for dealing with the specific types and some combinations of imprecise label configurations. In the main paper, we only introduced the representative pipelines for partial label learning, semi-supervised learning, and noisy label learning, as shown in \cref{sec:related-work}. In this section, we discuss in more detail the previous methods. We also extend discussions about the imprecise label settings to multi-instance learning and mixed imprecise label learning. 

\subsection{Partial Label Learning}


Label ambiguity remains a fundamental challenge in Partial Label Learning (PLL). 
The most straightforward approach to handling PLL is the average-based method, which assumes an equal probability for each candidate label being the ground-truth label. For instance, \citet{hullermeier2006learning} employed k-nearest neighbors for label disambiguation, treating all candidate labels of a sample's neighborhood equally and predicting the ground-truth label through voting strategies. 
However, a significant drawback of average-based methods is that false positive labels can mislead them.

To overcome these limitations, researchers have explored identification-based methods for PLL. In contrast to average-based methods, which treat all candidate labels equally, identification-based methods view the ground-truth label as a latent variable. They seek to maximize its estimated probability using either the maximum margin criterion \citep{nguyen2008classification,zhang2016partial} or the maximum likelihood criterion \citep{liu2012conditional}.
Deep learning techniques have recently been incorporated into identification-based methods, yielding promising results across multiple datasets. For example, PRODEN \citep{lv2020progressive} proposed a self-training strategy that disambiguates candidate labels using model outputs. CC \citep{Feng2020ProvablyCP} introduced classifier-consistent and risk-consistent algorithms, assuming uniform candidate label generation. LWS \citep{wen2021leveraged} relaxed this assumption and proposed a family of loss functions for label disambiguation. More recently, \citet{wang2022pico} incorporated contrastive learning into PLL, enabling the model to learn discriminative representations and show promising results under various levels of ambiguity.

Nevertheless, these methods rely on the fundamental assumption that the ground-truth label is present in the candidate set. This assumption may not always hold, especially in cases of unprofessional judgments by annotators, which could limit the applicability of these methods in real-world scenarios.








\subsection{Semi-Supervised Learning}

Consistency regularization and self-training, inspired by clusterness and smoothness assumptions, have been proposed to encourage the network to generate similar predictions for inputs under varying perturbations \citep{tarvainen2017mean,samuli2017temporal,miyato2018virtual}. Self-training \citep{lee2013pseudo,arazo2020pseudo,sohn2020fixmatch} is a widely-used approach for leveraging unlabeled data.
Pseudo Label \citep{lee2013pseudo}, a well-known self-training technique, iteratively creates pseudo labels that are then used within the same model. However, this approach suffers from confirmation bias \citep{arazo2020pseudo}, where the model struggles to rectify its own errors when learning from inaccurate pseudo labels.
Recent studies focus largely on generating high-quality pseudo-labels. MixMatch \citep{berthelot2019mixmatch}, for instance, generates pseudo labels by averaging predictions from multiple augmentations. Other methods like ReMixMatch \citep{berthelot2019remixmatch}, UDA \citep{xie2020unsupervised}, and FixMatch \citep{sohn2020fixmatch} adopt confidence thresholds to generate pseudo labels for weakly augmented samples, which are then used to annotate strongly augmented samples.
Methods such as Dash \citep{xu2021dash}, FlexMatch \citep{zhang2021flexmatch}, and FreeMatch \citep{wang2023freematch} dynamically adjust these thresholds following a curriculum learning approach. SoftMatch \citep{chen2023softmatch} introduces a novel utilization of pseudo-labels through Gaussian re-weighting.
Label Propagation methods \citep{iscen2019label} assign pseudo labels based on the local neighborhood's density. 
Meta Pseudo Labels \citep{pham2021meta} proposes generating pseudo labels with a meta learner.
SSL learning has also seen improvements through the incorporation of contrastive loss \citep{li2021comatch,zheng2022simmatch}. Furthermore, MixUp \citep{zhang2017mixup} has shown its effectiveness in a semi-supervised learning (SSL) \citep{berthelot2019mixmatch,berthelot2019remixmatch,cai2022semivit}.


\subsection{Noisy Label Learning}
Due to their large number of parameters, deep neural networks are prone to overfitting to noisy labels. Although certain popular regularization techniques like mixup \citep{zhang2017mixup} can somewhat mitigate overfitting, they fall short of completely resolving the issue of learning with noisy labels, as they lack accurate noise modeling.
Numerous techniques have been proposed to handle Noisy Label Learning (NLL) \citep{nllsurvey2022}. They can broadly be divided into three categories: robust loss functions \citep{Ghosh2017RobustLF, Zhang2018GeneralizedCE, Wang2019SymmetricCE, Ma2020NormalizedLF}, noise estimation \citep{Xiao2015LearningFM,Goldberger2016TrainingDN,Liu2020EarlyLearningRP,Zhang2021LearningNT,confidentlearn2021}, and noise correction \citep{Han2018CoteachingRT, Li2020DivideMixLW, sopliu22w}.

Designing loss functions that are robust to noise is a well-explored strategy for tackling the label noise problem. The  loss [59], which is a robust loss function, is a popular choice and has seen many recent extensions \citep{Zhang2018GeneralizedCE,Wang2019SymmetricCE,ma2020normalized, yu2020learning}. Additionally, methods that re-weight loss \citep{liu2016does} have also been explored for learning with noisy labels. However, despite enabling the model to learn faster from accurate labels, these robust loss functions still lead to overfitting to corrupted labels when used with models having a large number of parameters.

Another common strategy to handle label noise involves assuming that the noisy label originates from a probability distribution that depends on the actual label. The key task here is to estimate the underlying transition probabilities. Early works \citep{Goldberger2016TrainingDN} incorporated these transition probabilities into a noise adaptation layer that is stacked over a classification network and trained in an end-to-end fashion. More recent work, such as Forward \citep{Patrini2016MakingDN}, prefers to estimate these transition probabilities using separate procedures. However, the success of this method is contingent upon the availability of clean validation data \citep{confidentlearn2021} or additional assumptions about the data \citep{zhang2021learning}.

Noise correction has shown promising results in noisy label learning recently. During the early learning phase, the model can accurately predict a subset of the mislabeled examples \citep{Liu2020EarlyLearningRP}. This observation suggests a potential strategy of correcting the corresponding labels. This could be accomplished by generating new labels equivalent to soft or hard pseudo-labels estimated by the model \citep{tanaka2018joint,yi2019pronoisy}. Co-Teaching uses multiple differently trained networks for correcting noisy labels \citep{Han2018CoteachingRT}. SELFIE \citep{song19b} corrects a subset of labels by replacing them based on past model outputs. Another study in \cite{arazo2019unsupervised} uses a two-component mixture model for sample selection, and then corrects labels using a convex combination. They also utilize mixup \citep{zhang2017mixup} to enhance performance. Similarly, DivideMix \citep{Li2020DivideMixLW} employs two networks for sample selection using a two-component mixture model and implements the semi-supervised learning technique MixMatch \citep{berthelot2019mixmatch}.



\subsection{Multi-Instance Learning}



Multiple Instance Learning (MIL) is a sub-field of supervised learning where each instance is associated with a label. However, MIL stands out due to its handling of incomplete label knowledge in the training set. The training data in MIL is composed of 'bags', each containing several unlabeled instances. The primary objective of MIL is to predict labels for new, unseen bags.
\citet{dietterich1997solving} pioneered the concept of MIL, specifically applying it to drug activity prediction. This innovative approach has since inspired the development of numerous algorithms tailored to tackle this distinctive problem.
One pragmatic approach for dealing with label noise in this context involves counting the number of positive instances within a bag and setting a threshold for classifying positive bags. This technique was encapsulated as the threshold-based assumption for MIL in \citep{foulds2010review}. Per this assumption, a bag is classified as positive if and only if the count of positive instances exceeds a predefined threshold. This constituted the initial integration of counting information into the MIL framework.
Following this development, various efforts \citep{tao2004svm, tao2004extended} focused on carrying out bag-level predictions based on count-based assumptions.

Another significant trajectory in MIL research is centered on instance-level prediction. The framework proposed by \citet{maron1997framework} is particularly renowned for instance-level prediction within the MIL paradigm. This framework has underpinned numerous research proposals that have demonstrated effectiveness. The foundational idea behind these frameworks is the dynamic or static labeling of instances according to the bag label.
Empirical studies, as detailed in \citep{vanwinckelen2016instance}, have illustrated that superior bag-level prediction does not necessarily guarantee improved instance-level prediction.
To further the field of MIL, addressing these critical issues will be essential in order to develop more robust, flexible, and scalable MIL methods that can be applied across a broad range of real-world situations \citep{zhang2021training}.






\subsection{Mixed Imprecise Label Learning}

Various previous works have explored dealing with distinct types of imprecise labels. However, they have yet to tackle a combination of partial labels, limited labels, and noisy labels, which is a highly realistic scenario. For instance, recent attention has been paid to the issue of partial noisy label learning.



PiCO+ \citep{wang2022pico+}, an extended version of PiCO \citep{wang2022pico}, is tailored specifically for partial noisy labels. It employs a distance-based method to select clean samples and uses a semi-supervised contrastive learning algorithm to train robust classifiers. This approach distinguishes between clean and noisy samples and enhances the learning of distinctive features.
IRNet \citep{lian2022irnet} is a novel framework designed for Partial Label Learning with noisy labels. It uses two modules: noisy sample detection and label correction, transforming the scenario of noisy PLL into a more traditional PLL.
DALI \citep{xu2023dali} is another framework designed to reduce the negative impact of detection errors by creating a balance between the initial candidate set and model outputs, with theoretical assurances of its effectiveness.

Additionally, some work has focused on semi-supervised partial label learning \citep{wang2019partial,wang2020semi}. An iterative label propagation process is proposed in these studies, operating between partially labeled examples and unlabeled instances, which helps clarify the candidate label sets of the partially labeled examples and assign valid labels to unlabeled instances. However, these methods don't scale well to larger datasets like CIFAR-10.


Our study and the proposed framework hold significant importance as we have not found any existing research that can effectively address the challenge of handling a combination of partial, limited, and noisy labels simultaneously. This underscores the novelty and significance of our work, making it a valuable contribution to the field of machine learning.

















\section{Methods}


\subsection{Derivation of Variational Lower Bound}
\label{sec:append-var-lower-bound}

Evidence lower bound (ELBO), or equivalently variational lower bound \citep{dempster1977maximum}, is the core quantity in EM. 
From \cref{eq:em}, to model , we have:

where the first term is the ELBO and the second term is the KL divergence . Replacing  with  at each iteration will obtain \cref{eq:em}.


\subsection{Instantiations Mixed Imprecise Label Learning}
\label{sec:append-derive-instan}


In this setting, we have both labeled data and unlabeled data, where the labels for the labeled data are both partial and noisy. 
On the unlabeled data, the unsupervised objective is the same as the unsupervised consistency regularization of semi-supervised learning shown in \cref{eq:ill-ssl}. 
On the labeled data, it mainly follows the \cref{eq:ill-nll} of noisy label learning, with the noisy single label becoming the noisy partial labels . 
For noisy partial labels, the noisy supervised objective in Eq. 8 becomes the supervised consistency regularization as in Eq. 6 of partial label setting to train the noise transition model, and the noisy unsupervised objective becomes the consistency regularization of the prediction conditioned on noisy partial labels:


We can compute both quantity through the noise transition model:




\subsection{Forward-Backward Algorithm on the NFA of Imprecise Labels}
\label{sec-append-nfa}

\begin{figure}[t]
    \centering
    \hfill
    \subfigure[Full Label]{\label{fig:nfa_full}\includegraphics[width=0.24\textwidth]{figures/nfa_full.pdf}}
    \hfill
    \subfigure[Partial Label]{\label{fig:nfa_pll}\includegraphics[width=0.24\textwidth]{figures/nfa_pll.pdf}}
    \hfill
    \subfigure[Semi-Supervised]{\label{fig:nfa_ssl}\includegraphics[width=0.24\textwidth]{figures/nfa_ssl.pdf}}
    \hfill
    \subfigure[Noisy Label]{\label{fig:nfa_nll}\includegraphics[width=0.24\textwidth]{figures/nfa_nll.pdf}}
    \hfill
    \caption{Illustration of NFA for different imprecise label configurations on 4 instances with 3 classes. We use edge to denote the transition paths for each symbol  and circle to denote the finite state . Different label configurations  correspond to set the transition paths in the NFA, where we use red solid edge for available paths and blue dash edge for unavailable paths.  (a) Full label, the transition paths are definite . (b) Partial label, the transition paths follow candidate symbols . (c) Semi-supervised, all paths are available for unlabeled data. (d) Noisy label; all paths are available. 
}
    \label{fig:nfa}
\vspace{-0.15in}
\end{figure}

This section provides an alternative explanation for the EM framework proposed in the main paper.  We treat the problem of assigning labels  to inputs  as generating a sequence of symbols , such that  is satisfied.
Such a process can be represented as a finite-state automaton (FSA); more specifically, a non-deterministic finite automaton (NFA) \citep{hopcroft2001introduction}. 
The NFA of imprecise labels can be formally defined with a finite set of states , a finite set of symbols , a transition function , a start state of , and the accepting state of , determining the available transition paths in the NFA, as shown in \cref{fig:nfa}.
We demonstrate that, Although we can directly derive the soft-targets utilized in loss functions of each setting we study from \cref{eq:em}, they can also be computed by performing a forward-backward algorithm on the NFA.
While there is no difference in the actual value for conducting the forward-backward algorithm and deriving the soft-targets directly, the NFA indeed becomes more necessary when only collection-level imprecise information is provided, where naively considering all possible labeling requires exponential complexity, and forward-backward on NFA reduces the complexity to linear. 
The reason for the equivalence in the partial, semi-sup, noisy label is because the NFA directly collapses to  for computing the posterior without any statistical information imposed from . 

To compute the posterior probability , 
without loss of generality, we assume each training sample is drawn independently and identically, and training samples occur as a permutation-invariant sequence.
We further assume that each  is conditionally independent given the whole sequence of training samples , which allows us to re-write
\cref{eq:em} as:

The problem of computing the posterior then reduces to calculate the joint probability of  and the latent ground truth label of the -th sample taking the value  given the whole input sequence X:




Subsequently,  can be computed in linear time complexity  at maximum using the forward-backward algorithm on the NFA, similar to connectionist temporal classification (CTC) \citep{graves2006ctc}. 
More specifically, for any symbol  in the NFA of imprecise label information, the joint is calculated with the forward score  and the backward score :

where  indicates the total probability of all paths of  at state  and  indicates the total probability of all paths of  at state . 
We can then compute the posterior probability:

which can be in turn used in \cref{eq:em}. 
Now the difference in learning with various imprecise label configurations lies only in how to construct the NFA graph for different configurations.

An illustration for the NFA of full labels, partial labels, limited labels with unlabeled data, and noisy labels is shown in \cref{fig:nfa}. For full precise labels, there is only one determined transition of . For partial labels, the transition paths can only follow the label candidates, resulting in the normalized soft-targets with masses on the label candidates as we shown in \cref{sec:instantiate}. For semi-supervised learning, the transition paths for the labeled data strictly follow . For unlabeled data, since there is no constraint on the transition paths, all transitions are possible, corresponding to the soft pseudo-targets. 
For noisy labels, all transition paths are possible, but the joint probability needs to be computed with the noise transition model. 
We will show, in future work, the more extensions of the ILL framework on collection-level and statistical imprecise labels, with the help of NFA. 





\section{Experiments}
\label{sec:appen-exp}

\subsection{Additional Training Details}

We adopt two additional training strategies for the ILL framework. The first is the ``strong-weak'' augmentation strategy \citep{xie2020unsupervised}. Since there is a consistency regularization term in each imprecise label formulation of ILL, we use the soft pseudo-targets of the weakly-augmented data to train the strongly-augmented data. The second is the entropy loss \citep{bridle1991unsup} for class balancing, which is also adopted in SOP \citep{sopliu22w} and FreeMatch \citep{wang2023freematch}. We set the loss weight for the entropy loss uniformly for all experiments as 0.1. 






\subsection{Partial Label Learning}
\label{sec:append-exp-ppl}

\subsubsection{Setup}
Following previous work \citep{xu2022progressive,wen2021leveraged,wang2022pico}, we evaluate our method on partial label learning setting using CIFAR-10, CIFAR-100, and CUB-200 \citep{welinder2010caltech}.
We generate partially labeled datasets by flipping negative labels to false positive labels with a probability , which is also denoted as a partial ratio. 
Specifically, the  negative labels are uniformly aggregated into the ground truth label to form a set of label candidates. 
We consider  for CIFAR-10,  for CIFAR-100, and  for CUB-200. 
For CIFAR-10 and CIFAR-100, we use ResNet-18 \citep{he2016deep} as backbone. We use SGD as an optimizer with a learning rate of , a momentum of , and a weight decay of e. 
For CUB-200, we initialize the ResNet-18 \citep{he2016deep} with ImageNet-1K \citep{deng2009imagenet} pre-trained weights. 
We train  epochs for CIFAR-10 and CIFAR-100 \citep{krizhevsky2009learning}, and  epochs for CUB-200, with a cosine learning rate scheduler.
For CIFAR-10 and CIFAR-100, we use an input image size of 32. For CUB-200, we use an input image size of 224. 
A batch size of 256 is used for all datasets. 
The choice of these parameters mainly follows PiCO \citep{wang2022pico}.
We present the full hyper-parameters systematically in \cref{tab:append-param-pll}.


\begin{table}[h!]
\centering
\caption{Hyper-parameters for \textbf{partial label learning} used in experiments.}
\label{tab:append-param-pll}
\resizebox{0.6 \textwidth}{!}{\begin{tabular}{@{}c|ccc@{}}
\toprule
\multicolumn{1}{c|}{Hyper-parameter}   & CIFAR-10 & CIFAR-100 & CUB-200 \\ \midrule
Image Size & 32 & 32 & 224 \\
Model      &  ResNet-18 &   ResNet-18        &   \begin{tabular}{c} ResNet-18  \
    \mathcal{L}_{avg-PLL}(f(\boldsymbol{x}), \mathbf{s}) = \frac{1}{|\mathbf{s}|} \sum_{i \in \mathbf{s}} \ell(f(\boldsymbol{x}), i)

\ell(f(\mathbf{x}), i) = - \bar{g}_i(\mathbf{x}) \log g_i(\mathbf{x})
ImageNet-1K Pretrained) \end{tabular}  & Inception-ResNet-v2   \\
Batch Size        &     128     &    128       &   64  & 32    \\
Learning Rate     &     0.02     &   0.02        &   0.002 &   0.02 \\
Weight Decay      &     1e-3     &   1e-3       &   1e-3   & 5e-4  \\
LR Scheduler          &     Cosine     &   Cosine   &   MultiStep     &  MultiStep \\
Training Epochs   &     300    &   300        &  10    & 100  \\
Classes           &     10     &     100      &   14    & 50  \\ 
Noisy Matrix Scale & 1.0 & 2.0 & 0.5 & 2.5\\
\bottomrule
\end{tabular}}
\end{table}






\subsubsection{Results}

In addition to the results regarding noisy label learning provided in the main paper, we also present comparison results on CIFAR-10N and CIFAR-100N \citep{wei2021learning} in \cref{tab:append-noisy-n}. We include a full comparison on Clothing1M and WebVision, incorporating methods like Co-Teaching, Forward, and CORES, in \cref{tab:append-noisy-web}. 
As shown in \cref{tab:append-noisy-n}, the proposed ILL framework achieves performance comparable to the previous best method, SOP \citep{sopliu22w}.
On CIFAR-10N, our method yields results very close to SOP in the Random and Aggregate case noise scenarios and surpasses SOP in the Worst case noise scenario. 
However, on CIFAR-100N, our method slightly underperforms previous methods, possibly due to the oversimplified noise model utilized in ILL. 
We believe that a more realistic noise transition model and further tuning of our method could lead to improved performance.










\begin{table}[h!]
\centering
\caption{Test accuracy comparison of instance independent label noise on CIFAR-10N and CIFAR-100N for \textbf{noisy label learning}. 
The best results are indicated in \textbf{bold}, and the second best results are indicated in \underline{underline}. Our results are averaged over three independent runs with  ResNet34 as the backbone.}
\label{tab:append-noisy-n}
\resizebox{0.96 \textwidth}{!}{\begin{tabular}{@{}l|cccccc|cc@{}}
\toprule
Dataset & \multicolumn{6}{c|}{CIFAR-10N}                                                                                 & \multicolumn{2}{c}{CIFAR-100N} \\ \midrule
Noisy Type & Clean      & Random 1   & Random 2   & Random 3   & Aggregate  & Worst      & Clean               & Noisy               \\ \midrule
CE                    & 92.92\scriptsize{±0.11} & 85.02\scriptsize{±0.65} & 86.46\scriptsize{±1.79} & 85.16\scriptsize{±0.61} & 87.77\scriptsize{±0.38}                         & 77.69\scriptsize{±1.55}            & 76.70\scriptsize{±0.74}     & 55.50\scriptsize{±0.66}    \\
Forward \citep{Patrini2016MakingDN}              & 93.02\scriptsize{±0.12} & 86.88\scriptsize{±0.50} & 86.14\scriptsize{±0.24} & 87.04\scriptsize{±0.35} & 88.24\scriptsize{±0.22}                         & 79.79\scriptsize{±0.46}            & 76.18\scriptsize{±0.37}     & 57.01\scriptsize{±1.03}    \\
Co-teaching \citep{Han2018CoteachingRT}           & 93.35\scriptsize{±0.14} & 90.33\scriptsize{±0.13} & 90.30\scriptsize{±0.17} & 90.15\scriptsize{±0.18} & 91.20\scriptsize{±0.13}     & 83.83\scriptsize{±0.13}            & 73.46\scriptsize{±0.09}     & 60.37\scriptsize{±0.27}    \\
DivideMix  \citep{Li2020DivideMixLW}           & -          & \underline{95.16\scriptsize{±0.19}} & \underline{95.23\scriptsize{±0.07}} & \underline{95.21\scriptsize{±0.14}} &  95.01\scriptsize{±0.71} & 92.56\scriptsize{±0.42}            & -              & 71.13\scriptsize{±0.48}    \\
ELR \citep{Liu2020EarlyLearningRP}                  & 95.39\scriptsize{±0.05} & 94.43\scriptsize{±0.41} & 94.20\scriptsize{±0.24} & 94.34\scriptsize{±0.22} & 94.83\scriptsize{±0.10}                         & 91.09\scriptsize{±1.60}            & \underline{78.57\scriptsize{±0.12}}    & \underline{66.72\scriptsize{±0.07}}    \\
CORES \citep{cheng2020learning}                 & 94.16\scriptsize{±0.11} & 94.45\scriptsize{±0.14} & 94.88\scriptsize{±0.31} & 94.74\scriptsize{±0.03} & 95.25\scriptsize{±0.09}                         & 91.66\scriptsize{±0.09}            & 73.87\scriptsize{±0.16}     & 55.72\scriptsize{±0.42}    \\
SOP  \citep{sopliu22w}                                         & \textbf{96.38\scriptsize{±0.31}} & \underline{95.28\scriptsize{±0.13}} & \underline{95.31\scriptsize{±0.10}} & \underline{95.39\scriptsize{±0.11}} & \underline{95.61\scriptsize{±0.13}} & \underline{93.24\scriptsize{±0.21}} & \textbf{78.91\scriptsize{±0.43}} & \underline{67.81\scriptsize{±0.23}} \\ \midrule
Ours                  & \underline{96.21\scriptsize{±0.29}} & \textbf{96.06\scriptsize{±0.07}} & \textbf{95.98\scriptsize{±0.12}} & \textbf{96.10\scriptsize{±0.05}} & \textbf{96.40\scriptsize{±0.03}}  & \textbf{93.55\scriptsize{±0.14}} & 78.53\scriptsize{±0.21}     & \textbf{68.07\scriptsize{±0.33}}    \\ \bottomrule
\end{tabular}}
\end{table}

\begin{table}[h!]
\centering
\caption{Test accuracy comparison of realistic noisy labels on Clothing1M and WebVision for \textbf{noisy label learning}. The best results are indicated in \textbf{bold} and the second best results are indicated in \underline{underline}. Our results are averaged over 3 independent runs. For Clothing1M, we use ImageNet-1K pre trained ResNet50 as the backbone. For WebVision, InceptionResNetv2 is used as the backbone.}
\label{tab:append-noisy-web}
\resizebox{0.4 \textwidth}{!}{\begin{tabular}{@{}l|cc@{}}
\toprule
Dataset     & Clothing1M           & WebVision           \\ \midrule
CE        & 69.10                & -                   \\
Forward  \citep{Patrini2016MakingDN}     & 69.80                & 61.10               \\
Co-Teaching \citep{Han2018CoteachingRT} & 69.20                & 63.60               \\
DivideMix \citep{Li2020DivideMixLW}   & \textbf{74.76}               & \underline{77.32}               \\
ELR \citep{Liu2020EarlyLearningRP}        & 72.90               & 76.20               \\
CORES \citep{cheng2020learning}       & 73.20                & -                   \\
SOP  \citep{sopliu22w}       & 73.50                & 76.60               \\ \midrule
Ours        & \underline{74.02\scriptsize{±0.12}} & \textbf{79.37\scriptsize{±0.09}} \\ \bottomrule
\end{tabular}}
\end{table}



\subsection{Mixed Imprecise Label Learning}

\subsubsection{Setup}

To create a mixture of various imprecise label configurations, we select CIFAR-10 and CIFAR-100 as base datasets. 
We first uniformly sample  labeled samples from each class to form the labeled dataset and treat the remaining samples as the unlabeled dataset.
Based on the labeled dataset, we generate partially labeled datasets by flipping negative labels to false positive labels with the partial ratio . 
After obtaining the partial labels, we randomly select  percentage of samples from each class, and recreate the partial labels for them by flipping the ground truth label uniformly to another class. 
In this setting, unlabeled data, partially labeled data, and noisy labeled data exist simultaneously, which is very challenging and more closely resembles realistic situations. 
For CIFAR-10, we set , and for CIFAR-100, we set . 
Similarly in the partial label setting, we set  for CIFAR-10, and  for CIFAR-100.
For noisy labels, we set  for both datasets.

 




\subsubsection{Results}


We provide a more complete version of \cref{tab:main-mix} in \cref{tab:append-partial-noisy}. 
On partial noisy labels of CIFAR-10 with partial ratio  and of CIFAR-100 with partial ratio , most baseline methods are more robust or even fail to perform. 
However, our ILL still shows very robust performance with minor performance degradation as increase of noise ratios. 


\begin{table}[h!]
\centering
\caption{Test accuracy comparison of \textbf{mixture of different imprecise labels}. We report results of full labels, partial ratio  of  for CIFAR-10 and  for CIFAR-100, and noise ratio  of  for CIFAR-10 and CIFAR-100. 
}
\label{tab:append-partial-noisy}
\resizebox{\textwidth}{!}{\begin{tabular}{@{}c|c|c|l|cccc@{}}
\toprule
Dataset &
  \# Labels &
  Partial Ratio  &
  \multicolumn{1}{c|}{Noise Ratio }  &
  0 &
  0.1 &
  0.2 &
  0.3 \\ \midrule
 &
   &
   &
  PiCO+ \citep{wang2022pico+} &
  95.99\scriptsize{±0.03} &
  93.64 &
  93.13 &
  92.18 \\
 &
   &
   &
  IRNet \citep{lian2022irnet} &
  - &
  93.44 &
  92.57 &
  92.38 \\
 &
   &
   &
  DALI \citep{xu2023dali} &
  - &
  94.15 &
  94.04 &
  93.77 \\
 &
   &
   &
  PiCO+ w/ Mixup \citep{xu2023dali} &
  - &
  94.58 &
  94.74 &
  94.43 \\
 &
   &
   &
  DALI w/ Mixup \citep{xu2023dali} &
  - &
  95.83 &
  95.86 &
  95.75 \\
 &
   &
  \multirow{-6}{*}{0.1} &
  Ours &
  \textbf{96.55\scriptsize{±0.08}} &
  \textbf{96.47\scriptsize{±0.11}} &
  \textbf{96.09\scriptsize{±0.20}} &
  \textbf{95.83\scriptsize{±0.05}} \\ \cmidrule(l){3-8} 
 &
   &
   &
  PiCO+ \citep{wang2022pico+} &
  95.73\scriptsize{±0.10} &
  92.32 &
  92.22 &
  89.95 \\
 &
   &
   &
  IRNet \citep{lian2022irnet} &
  - &
  92.81 &
  92.18 &
  91.35 \\
 &
   &
   &
  DALI \citep{xu2023dali} &
  - &
  93.44 &
  93.25 &
  92.42 \\
 &
   &
   &
  PiCO+ w/ Mixup \citep{xu2023dali} &
  - &
  94.02 &
  94.03 &
  92.94 \\
 &
   &
   &
  DALI w/ Mixup \citep{xu2023dali} &
  - &
  95.52 &
  95.41 &
  94.67 \\
 &
   &
  \multirow{-6}{*}{0.3} &
  Ours &
  \textbf{96.52\scriptsize{±0.12}} &
  \textbf{96.2\scriptsize{±0.02}} &
  \textbf{95.87\scriptsize{±0.14}} &
  \textbf{95.22\scriptsize{±0.06}} \\ \cmidrule(l){3-8} 
 &
   &
   &
  PiCO+ \citep{wang2022pico+} &
  95.33\scriptsize{±0.06} &
  91.07 &
  89.68 &
  84.08 \\
 &
   &
   &
  IRNet \citep{lian2022irnet} &
  - &
  91.51 &
  90.76 &
  86.19 \\
 &
   &
   &
  DALI \citep{xu2023dali} &
  - &
  92.67 &
  91.83 &
  89.8 \\
 &
   &
   &
   PiCO+  w/ Mixup \citep{xu2023dali} &
  - &
  93.56 &
  92.65 &
  88.21 \\
 &
   &
   &
  DALI w/ Mixup \citep{xu2023dali} &
  - &
  95.19 &
  93.89 &
  92.26 \\
\multirow{-18}{*}{CIFAR-10} &
  \multirow{-18}{*}{50,000} &
  \multirow{-6}{*}{0.5} &
  Ours &
  \textbf{96.28\scriptsize{±0.13}} &
  \textbf{95.82\scriptsize{±0.07}} &
  \textbf{95.28\scriptsize{±0.08}} &
  \textbf{94.35\scriptsize{±0.08}} \\ \bottomrule

   \\ \toprule
   &
   &
   &
  PiCO+ \citep{wang2022pico+} &
  76.29\scriptsize{±0.42} &
  71.42 &
  70.22 &
  66.14 \\
 &
   &
   &
  IRNet \citep{lian2022irnet} &
  - &
  71.17 &
  70.10 &
  68.77 \\
 &
   &
   &
  DALI \citep{xu2023dali} &
  - &
  72.26 &
  71.98 &
  71.04 \\
 &
   &
   &
  PiCO+ w/ Mixup \citep{xu2023dali} &
  - &
  75.04 &
  74.31 &
  71.79 \\
 &
   &
   &
  DALI w/ Mixup \citep{xu2023dali} &
  - &
  76.52 &
  76.55 &
  76.09 \\
 &
   &
  \multirow{-6}{*}{0.01} &
  Ours &
  \textbf{78.08\scriptsize{±0.26}} &
  \textbf{77.53\scriptsize{±0.24}} &
  \textbf{76.96\scriptsize{±0.02}} &
  \textbf{76.43\scriptsize{±0.27}} \\ \cmidrule(l){3-8} 
 &
   &
   &
  PiCO+ \citep{wang2022pico+} &
  76.17\scriptsize{±0.18} &
  69.40 &
  66.67 &
  62.24 \\
 &
   &
   &
  IRNet \citep{lian2022irnet} &
  - &
  70.73 &
  69.33 &
  68.09 \\
 &
   &
   &
  DALI \citep{xu2023dali} &
  - &
  72.28 &
  71.35 &
  70.05 \\
 &
   &
   &
   PiCO+ w/ Mixup \citep{xu2023dali} &
  - &
  73.06 &
  71.37 &
  67.56 \\
 &
   &
   &
 DALI w/ Mixup \citep{xu2023dali} &
  - &
  76.87 &
  75.23 &
  74.49 \\
 &
   &
  \multirow{-6}{*}{0.05} &
  Ours &
  \textbf{76.95\scriptsize{±0.46}} &
  \textbf{77.07\scriptsize{±0.16}} &
  \textbf{76.34\scriptsize{±0.08}} &
  \textbf{75.13\scriptsize{±0.63}} \\ \cmidrule(l){3-8} 
 &
   &
   &
  PiCO+ \citep{wang2022pico+} &
  75.55\scriptsize{±0.21} &
  - &
  - &
  - \\
 &
   &
   &
  IRNet \citep{lian2022irnet} &
  - &
  - &
  - &
  - \\
 &
   &
   &
  DALI \citep{xu2023dali} &
  - &
  - &
  - &
  - \\
 &
   &
   &
  PiCO+ w/ Mixup \citep{xu2023dali} &
  - &
  - &
  - &
  - \\
 &
   &
   &
  DALI w/ Mixup \citep{xu2023dali} &
  - &
  - &
  - &
  - \\
\multirow{-18}{*}{CIFAR-100} &
  \multirow{-18}{*}{50,000} &
  \multirow{-6}{*}{0.1} &
  Ours &
  \textbf{76.41\scriptsize{±1.02}} &
  \textbf{75.50\scriptsize{±0.54}} &
  \textbf{74.67\scriptsize{±0.30}} &
  \textbf{73.88\scriptsize{±0.60}} \\ 
  \bottomrule

\end{tabular}}
\end{table}







\newpage 


