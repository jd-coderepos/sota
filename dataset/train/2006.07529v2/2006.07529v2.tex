


\section{Proof of Theorem \ref{thm:semi}}
\label{appendix:proof_theorem_1}
Given the pseudo-labels, we note that we can rewrite , where  and . That is, if the pseudo-label is correct, then   and otherwise,  . Similarly,  , where  and . Now,

We bound each term in Eq.~(\ref{eq:thm1:proof:theta}) separately. First, note that 

By standard Gaussian concentration, we have

Next, we bound the term . Since , applying Hoeffding inequality

Similarly, we have

Note that by triangle inequality,

Given , consider the event that 

Using union bound and the concentration inequalities Eqs.~(\ref{eq:thm1:proof:gaussian}), (\ref{eq:thm1:proof:ber1}) and (\ref{eq:thm1:proof:ber2}), we obtain the following lower bound on the probability of event :

Finally, the above equation and the triangle inequality implies that

This completes the proof.






\section{Proof of Theorem \ref{thm:standard_without_self}}
\label{appendix:proof_theorem_2}
Note that if , then for a given , .
Based on the form of the linear classifier, we know that

Finally, note that for , . Since  is assumed to be at least , the error probability of the classifier is at least . 






\section{Proof of Theorem \ref{thm:error_self}}
\label{appendix:proof_theorem_3}
We recall the following standard concentration inequality for sub-exponential random variables~\cite{wainwright2019high}. Suppose that  are i.i.d. sub-exponential random variables with parameters . Then,
7pt]
    e^{-\frac{n\delta}{2\alpha}},              & \text{for } \delta>\frac{\nu^2}{\alpha}.
\end{cases}

    \mathbb{P}\left(\left|\frac{1}{n}\sum_{i=1}^nW_i^2-1\right|\geq \delta\right)\leq 2e^{-n\delta^2/8},\quad\forall\: \delta\in(0,1).

    \mathbb{P}\left(\left|\frac{1}{N_-d}{\sum_{i=1}^N\mathbf{1}_{\{Y_i = -1\}}\frac{Z_i-k_2}{k_1\beta\sigma_1^2}}-1\right|\geq \delta\right)\leq 2e^{-N_-d\delta^2/8}.

    \mathbb{P}\left(\left|\frac{1}{N_-}{\sum_{i=1}^N\mathbf{1}_{\{Y_i = -1\}}Z_i-\left(dk_1\beta\sigma_1^2+k_2\right)}\right|\geq \delta dk_1\beta\sigma_1^2\right)\leq 2e^{-N_-d\delta^2/8}.

    \mathbb{P}\left(\left|\frac{1}{N_+}{\sum_{i=1}^N\mathbf{1}_{\{Y_i = +1\}}Z_i-\left(dk_1\sigma_1^2+k_2\right)}\right|\geq \delta dk_1\sigma_1^2\right)\leq 2e^{-N_+d\delta^2/8}.

\label{eq:self_b_bound}
    \left|b-\frac{dk_1(\beta+1)\sigma_1^2}{2} - k_2\right|\leq \frac{\delta dk_1(\beta+1)\sigma_1^2}{2}.

    \textrm{err}_{f_{ss}}&=\mathbb{P}_{(X,Y)\sim P_{XY}}\big(y(-Z+b)<0\big)\nonumber\\
    & = \mathbb{P}_{(X,Y)\sim P_{XY}}\big(y(-Z+b)<0|Y=+1\big)\mathbb{P}\big(Y=+1\big)\nonumber\\
    &\quad + \mathbb{P}_{(X,Y)\sim P_{XY}}\big(y(-Z+b)<0|Y=-1\big)\mathbb{P}\big(Y=-1\big)\nonumber\\
    & = p_+\mathbb{P}_{(X,Y)\sim P_{XY}}\big(Z>b|Y=+1\big)+p_-\mathbb{P}_{(X,Y)\sim P_{XY}}\big(Z<b|Y=-1\big)\nonumber\\
    &\leq  p_+\mathbb{P}_{(X,Y)\sim P_{XY}}\left(Z>\frac{dk_1(\beta+1)\sigma_1^2}{2} + k_2-\frac{\delta dk_1(\beta+1)\sigma_1^2}{2}\bigg|Y=+1\right)\nonumber\\
    &\quad + p_-\mathbb{P}_{(X,Y)\sim P_{XY}}\left(Z<\frac{dk_1(\beta+1)\sigma_1^2}{2} + k_2+\frac{\delta dk_1(\beta+1)\sigma_1^2}{2}\bigg|Y=-1\right). \label{eq:self_error_prob_bound}

    &\mathbb{P}\left(Z<\frac{dk_1(\beta+1)\sigma_1^2}{2} + k_2+\frac{\delta dk_1(\beta+1)\sigma_1^2}{2}\bigg|Y=-1\right)\\
    =\ &\mathbb{P}\left(Z-(dk_1\beta\sigma_1^2+k_2)<-\frac{(\beta -1 -\delta(\beta+1))}{2\beta}dk_1\beta\sigma_1^2\bigg|Y=-1\right)\\
    \leq\ &\exp\left(-d\cdot\frac{(\beta -1 -\delta(\beta+1))^2}{32\beta^2}\right),

     &\mathbb{P}\left(Z>\frac{dk_1(\beta+1)\sigma_1^2}{2} + k_2-\frac{\delta dk_1(\beta+1)\sigma_1^2}{2}\bigg|Y=+1\right)\\
    =\ &\mathbb{P}\left(Z-(dk_1\sigma_1^2+k_2)>\frac{(\beta -1 -\delta(\beta+1))}{2}dk_1\sigma_1^2\bigg|Y=+1\right)\\
    \leq\ &\begin{cases}
  \exp\left(-d\cdot\frac{(\beta -1 -\delta(\beta+1))^2}{32}\right),& \text{if } \delta\in\left[\frac{\beta-3}{\beta+1},\frac{\beta-1}{\beta+1}\right);\
For the last inequality, we note that if , ; otherwise, . Substituting the above inequalities into the error probability  (Eq.~(\ref{eq:self_error_prob_bound})) completes the proof.






\section{Experimental Details}
\label{appendix:setup-details}

\subsection{Imbalanced Dataset Details}
\label{appendix:imbalance-data-detail}
In this section, we provide the detailed information of five long-tailed imbalanced datasets we use in our experiments. Table~\ref{tab:imb-dataset-overview} provides an overview of the five datasets.

\vspace{-0.3cm}
\begin{table}[ht]
\setlength{\tabcolsep}{4pt}
\caption{\small Overview of the five imbalanced datasets used in our experiments.  indicates the imbalance ratio.}
\vspace{-2pt}
\label{tab:imb-dataset-overview}
\small
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tabular}{c|c|c|c|c|c|c|c}
\toprule[1.5pt]
Dataset          & \multicolumn{1}{c|}{\# Class} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{Head class size} & \multicolumn{1}{c|}{Tail class size} & \multicolumn{1}{c|}{\# Training set} & \multicolumn{1}{c|}{\# Val. set} & \multicolumn{1}{c}{\# Test set} \\ \midrule\midrule
CIFAR-10-LT      & 10 & 10  100 & 5,000  & 500  50 & 20,431  12,406 &  & 10,000 \\ \midrule
CIFAR-100-LT     & 100  & 10  100  & 500 & 50  5   & 19,573  10,847 &  & 10,000    \\ \midrule
SVHN-LT     & 10    & 10  100  & 1,000   & 100  10      & 4,084  2,478   &  & 26,032     \\ \midrule
ImageNet-LT      & 1,000    & 256   & 1,280  & 5 & 115,846 & \multicolumn{1}{c|}{20,000} & 50,000    \\ \midrule
iNaturalist 2018 & 8,142 & 500  & 1,000 & 2 & 437,513   & \multicolumn{1}{c|}{24,426} &  \\
\bottomrule[1.5pt]
\end{tabular}}
\end{center}
\vspace{-0.1cm}
\end{table}

\textbf{CIFAR-10-LT and CIFAR-100-LT.}
The original versions of CIFAR-10 and CIFAR-100 contain 50,000 images for training and 10,000 images for testing with class number of 10 and 100, respectively. We create the long-tailed CIFAR versions following~\cite{cui2019class,cao2019learning} with controllable degrees of data imbalance, and keep the test set unchanged. We vary the class imbalance ratio  from 10 to 100.

\textbf{SVHN-LT.}
The original SVHN dataset contains 73,257 images for training and 26,032 images for testing with 10 classes.
Similarly to CIFAR-LT, we create SVHN-LT dataset with maximally 1,000 images per class (head class), and vary  from 10 to 100 for different long-tailed versions.

\textbf{ImageNet-LT.}
ImageNet-LT~\cite{liu2019large} is artificially truncated from its balanced version, with sample size in the training set following an exponential decay across different classes.
ImageNet-LT has 1,000 classes and 115.8K training images, with number of images ranging from 1,280 to 5 images per class.

\textbf{iNaturalist 2018.}
iNaturalist 2018~\cite{inat18} is a real-world fine-grained visual recognition dataset that naturally exhibits long-tailed class distributions, consisting of 435,713 samples from 8,142 species.


\subsection{Unlabeled Data Details}
\label{appendix:unlabeled-data-detail}
We provide additional information on the unlabeled data we use in the semi-supervised settings, i.e., the unlabeled data sourcing, and how we create unlabeled sets with different imbalanced distributions.

\textbf{Unlabeled Data Sourcing.}
To obtain the unlabeled data needed for our semi-supervised setup, we follow~\cite{carmon2019unlabeled} to mine the 80 Million Tiny Images (80M) dataset~\cite{torralba200880} to source unlabeled and uncurated data for CIFAR-10. In particular, CIFAR-10 is a human-labeled subset of 80M, which is manually restricted to 10 classes. Accordingly, most images in 80M do not belong to any image categories in CIFAR-10.
To select unlabeled data that exhibit similar distributions as labeled ones, we follow the procedure as in~\cite{carmon2019unlabeled}, where an 11-class classification model is trained to distinguish CIFAR-10 classes and an ``non-CIFAR'' class.
For each class, we then rank the images based on the prediction confidence, and construct the unlabeled (imbalanced) dataset  according to our settings.

For SVHN, since its own dataset contains an extra part~\cite{netzer2011reading} with 531.1K additional (labeled) samples, we directly use these additional data to simulate the unlabeled dataset, which exhibits similar data distribution as the main dataset.
Specifically, the ground truth labels are used only for preparing , and are abandoned throughout experiments (i.e., before performing pseudo-labeling).

\textbf{Relevant (and Irrelevant) Unlabeled Data.}
To analyze the data relevance of unlabeled data with class imbalance~(cf. Sec.~\ref{sec:rethink}), we again employ the 11-way classifier to select samples with prediction scores that are high for the extra class, and use them as proxy for irrelevant data. 
We then mix the irrelevant dataset and our main unlabeled dataset with different proportions, thus creating a sequence of unlabeled datasets with different degrees of data relevance.

\textbf{Unlabeled Data with Class Imbalance.}
With the sourced unlabeled data, we construct the demanded unlabeled dataset  also with class imbalance. In Fig.~\ref{fig:unlabel-dataset-info}, we show an example of data distributions of both original labeled imbalanced dataset  and  with different unlabeled imbalance ratio.
Specifically, Fig.~\ref{fig:cifar_lt_original} presents the training and test set of CIFAR-10-LT with , where a long tail can be observed for the training set, while the test set is balanced across classes.
With labeled data on hand, we create different degrees of class imbalance in  to be (1) uniform (, Fig.~\ref{fig:cifar_lt_unlabel_1}), (2) half imbalanced as labeled set (, Fig.~\ref{fig:cifar_lt_unlabel_50}), (3) same imbalanced (, Fig.~\ref{fig:cifar_lt_unlabel_100}), and (4) double imbalanced (, Fig.~\ref{fig:cifar_lt_unlabel_200}).
Note that the total data amount of  is fixed~(e.g., 5x as labeled set) given , while different  will result in different unlabeled data distributions.

\begin{figure}[!t]
\begin{subfigure}{0.195\linewidth}
    \includegraphics[height=0.75\textwidth]{figures/cifar_lt_original.pdf}
    \caption{\small , }
    \label{fig:cifar_lt_original}
\end{subfigure}
\begin{subfigure}{0.195\linewidth}
    \includegraphics[height=0.75\textwidth]{figures/cifar_lt_unlabel_1.pdf}
    \caption{\small , }
    \label{fig:cifar_lt_unlabel_1}
\end{subfigure}
\begin{subfigure}{0.195\linewidth}
    \includegraphics[height=0.75\textwidth]{figures/cifar_lt_unlabel_50.pdf}
    \caption{\small , }
    \label{fig:cifar_lt_unlabel_50}
\end{subfigure}
\begin{subfigure}{0.195\linewidth}
    \includegraphics[height=0.75\textwidth]{figures/cifar_lt_unlabel_100.pdf}
    \caption{\small , }
    \label{fig:cifar_lt_unlabel_100}
\end{subfigure}
\begin{subfigure}{0.195\linewidth}
    \includegraphics[height=0.75\textwidth]{figures/cifar_lt_unlabel_200.pdf}
    \caption{\small , }
    \label{fig:cifar_lt_unlabel_200}
\end{subfigure}
\vspace{-0.2cm}
\caption{\small An illustration of labeled dataset () as well as its corresponding unlabeled dataset (@5x) under different unlabeled imbalance ratio . Given  with a fixed , the total amount of  is fixed, while different  will lead to different class distributions of the unlabeled data. }
\label{fig:unlabel-dataset-info}
\vspace{-0.2cm}
\end{figure}


\subsection{Implementation Details}
\label{appendix:setup-train-detail}
\textbf{CIFAR-10-LT and CIFAR-100-LT.}
Following~\cite{cui2019class,cao2019learning,abdullah2020rethinking}, we use ResNet-32~\cite{he2016deep} for all CIFAR-LT experiments. The data augmentation follows~\cite{he2016deep} to use zero-padding with 4 pixels on each side and then random crop back to the original image size, after which a random horizontal flip is performed.
We train all models for 200 epochs, and remain all other hyper-parameters the same as~\cite{cao2019learning}.
In the semi-supervised settings, we fix the unlabeled weight  for all experiments.

\textbf{SVHN-LT.}
Similarly to CIFAR-LT, we use ResNet-32 model for all SVHN-LT experiments, and fix the same hyper-parameters as in CIFAR-LT experiments throughout training.

\textbf{ImageNet-LT.}
We follow~\cite{liu2019large,kang2019decoupling} to report results with ResNet-10 and ResNet-50 models.
Since~\cite{liu2019large} only employs ResNet-10 model, we reproduce the results with ResNet-50 using the public code from the authors for fair comparison. 
During the classifier training stage, we train all models for 90 epochs, and keep all other hyper-parameters identical to those in~\cite{kang2019decoupling}.
During the self-supervised pre-training stage, we leave the hyper-parameters unchanged as in~\cite{he2019moco}, but only use samples from ImageNet-LT.

\textbf{iNaturalist 2018.}
We follow~\cite{cao2019learning,cui2019class,kang2019decoupling,abdullah2020rethinking} to use ResNet-50 model.
Similar to ImageNet-LT, we train all models for 90 epochs in the classifier training stage, and other hyper-parameters are kept the same as in~\cite{kang2019decoupling}. The self-supervised pre-training stage is remained the same as that on ImageNet-LT. 
We reproduce the results for~\cite{cao2019learning} on iNaturalist 2018 using the authors' code.





\section{Additional Results for Semi-Supervised Imbalanced Learning}
\label{appendix:semi-results}

\subsection{Different Semi-Supervised Learning Methods}
\label{appendix:semi-diff-methods}
We study the effect of different advanced semi-supervised learning methods, in addition to the simple pseudo-label strategy we apply in the main text. We select the following two methods for analysis.

\textbf{Virtual Adversarial Training.} Virtual adversarial training (VAT)~\cite{miyato2018virtual} is one of the state-of-the-art semi-supervised learning methods, which aims to make the predicted labels robust around input data point against local perturbation.
It approximates a tiny perturbation  to add to the (unlabeled) inputs which would most significantly affect the outputs of the model.
Note that the implementation difference between VAT and the pseudo-label is the loss term on the unlabeled data, where VAT exhibits a consistency regularization loss rather than supervised loss, resulting in a loss function as
. We add an additional entropy regularization term following~\cite{miyato2018virtual}.

\textbf{Mean Teacher.} The mean teacher (MT)~\cite{tarvainen2017mean} method is also a representative algorithm using consistency regularization, where a teacher model and a student model are maintained and a consistency cost between the student's and the teacher's outputs is introduced. The teacher weights are updated through an exponential moving average (EMA) of the student weights.

Similar to pseudo-label, the two semi-supervised methods can be seamlessly incorporated with our imbalanced learning framework. We present the results with these methods in Table~\ref{tab:diff-semi-method-ablation}.
For each run, we construct @5x with the same imbalance ratio as the labeled set (i.e., ).
As Table~\ref{tab:diff-semi-method-ablation} reports, across different datasets and imbalance ratios, adding unlabeled data can consistently benefit imbalanced learning via semi-supervised learning.
Moreover, by using more advanced SSL techniques, larger improvements can be obtained in general.

\begin{table}[!t]
\setlength{\tabcolsep}{8pt}
\caption{\small Ablation study of different semi-supervised learning methods on CIFAR-10-LT and SVHN-LT. We fix  for each specific setting. Best results of each column are in \textbf{bold} and the second best are \underline{underlined}.}
\vspace{-1pt}
\label{tab:diff-semi-method-ablation}
\small
\begin{center}
\begin{tabular}{l|c|c|c|c|c|c}
\toprule[1.5pt]
\multicolumn{1}{c|}{Dataset} & \multicolumn{3}{c|}{CIFAR-10-LT} & \multicolumn{3}{c}{SVHN-LT}       \\ \midrule
\multicolumn{1}{c|}{Imbalance Ratio ()} &  100  &  50  &  10  &  100  &  50  &  10  \\ \midrule\midrule
\multicolumn{1}{c|}{Vanilla CE} & 29.64 & 25.19 & 13.61 & 19.98 & 17.50 & 11.46         \\ \midrule
@5x + Pseudo-label~\cite{lee2013pseudo} & 18.74  & 18.36  & 10.86 & 14.65  & 13.16 & 10.06    \1.2pt]
@5x + MT~\cite{tarvainen2017mean} & \textbf{16.52} & \textbf{15.79} & \underline{9.53} & \textbf{12.34} & \textbf{11.12} & \textbf{8.62} \\
\bottomrule[1.5pt]
\end{tabular}
\end{center}
\vspace{-0.25cm}
\end{table}


\subsection{Class-wise Generalization Results}
\label{appendix:semi-additional-results}
In the main paper, we report the top-1 test errors as the final performance metric. To gain additional insights on how unlabeled data helps imbalanced tasks, we further look at the generalization results in each class, especially on the minority (tail) classes.

\textbf{Generalization on Minority Classes.}
In Fig.~\ref{fig:bar-errors-semi} we plot the test error on each class on CIFAR-10-LT and SVHN-LT with . As the figure shows, regardless of the base training technique, using unlabeled data can consistently and substantially improve the generalization on tail classes.

\begin{figure}[ht]
\begin{subfigure}{0.5\linewidth}
    \includegraphics[height=0.412\textwidth]{figures/bar_cifar_semi.pdf}
    \caption{\small CIFAR-10-LT}
    \label{fig:bar_cifar_semi}
\end{subfigure}
\hfill
\begin{subfigure}{0.5\linewidth}
    \includegraphics[trim={24 0 0 0},clip,height=0.412\textwidth]{figures/bar_svhn_semi.pdf}
    \caption{\small SVHN-LT}
    \label{fig:bar_svhn_semi}
\end{subfigure}
\vspace{-0.2cm}
\caption{\small Class-wise top-1 error rates. \texttt{C0} stands for the head class, and \texttt{C9} stands for the tail class. Using unlabeled data leads to better generalization on tail classes while keeping the performance on head classes almost unaffected, and can consistently boost different training techniques. Results are averaged across 5 runs.}
\label{fig:bar-errors-semi}
\end{figure}

\textbf{Confusion Matrix.}
We further show the confusion matrices on CIFAR-10-LT with and without . Fig.~\ref{fig:confusion-semi} presents the results, where for the vanilla CE training, predictions for tail classes are biased towards the head classes significantly. In contrast, by using unlabeled data, the leakage from tail classes to head classes can be largely eliminated.

\vspace{-0.1cm}
\begin{figure}[ht]
\centering
\begin{subfigure}{0.4\linewidth}
    \includegraphics[width=\textwidth]{figures/confusion_cifar10_ce_100.pdf}
    \caption{\small Standard CE training}
    \label{fig:confusion_cifar10_ce_100}
\end{subfigure}
\hspace{3ex}
\begin{subfigure}{0.4\linewidth}
    \includegraphics[width=\textwidth]{figures/confusion_cifar10_ce_100_semi.pdf}
    \caption{\small CE with unlabeled data @5x}
    \label{fig:confusion_cifar10_ce_100_semi}
\end{subfigure}
\vspace{-0.2cm}
\caption{\small Confusion matrices of standard CE training and using @5x on CIFAR-10-LT with .}
\label{fig:confusion-semi}
\end{figure}


\subsection{Effect of Unlabeled Data Amount}
\label{appendix:semi-unlabel-amount}
We study the effect of the size of the unlabeled dataset on our SSL approach in imbalanced learning.
We first fix the labeled dataset  with , the unlabeled imbalance ratio to be , and then vary the amount of  to be \texttt{\{0.5x,1x,5x,10x\}} of the size of .
Table~\ref{tab:appendix-unlabel-amount} reports the results, where we can observe that larger  consistently leads to higher gains. Furthermore, even with only \texttt{0.5x} more unlabeled data, the performance can be boosted largely compared to that without unlabeled data. Interestingly however, as the size of  becomes larger, the gains gradually diminish.

\vspace{-0.2cm}
\begin{table}[ht]
\setlength{\tabcolsep}{7pt}
\caption{\small Ablation study of how unlabeled data amount affects SSL in imbalanced learning. We fix the imbalance ratios as . We vary the amount of  with respect to labeled data amount (e.g., 0.5x means the size of  is half of ). Best results of each part are in \textbf{bold} and the second best are \underline{underlined}.}
\vspace{-1.5pt}
\label{tab:appendix-unlabel-amount}
\small
\begin{center}
\begin{tabular}{c|c|c|c|c|c|c|c|c}
\toprule[1.5pt]
Dataset & \multicolumn{4}{c|}{CIFAR-10-LT}  & \multicolumn{4}{c}{SVHN-LT}      \\ \midrule
 Size (w.r.t. )  & 0.5x   & 1x   & 5x  & 10x  & 0.5x  & 1x  & 5x  & 10x  \\ \midrule\midrule
CE      & \multicolumn{4}{c|}{25.19}        & \multicolumn{4}{c}{17.50}        \\ \midrule
CE + & 21.75 & 20.35 & \underline{18.36} & \textbf{16.88} & 14.96 & 14.13 & \underline{13.16} & \textbf{13.02} \\ \midrule\midrule
LDAM-DRW~\cite{cao2019learning} & \multicolumn{4}{c|}{19.06}    & \multicolumn{4}{c}{14.59}    \\ \midrule
LDAM-DRW +  & 17.43 & 16.59 & \underline{14.93} & \textbf{13.91} & 13.93 & 13.07 & \underline{11.26} & \textbf{11.09} \\ 
\bottomrule[1.5pt]
\end{tabular}
\end{center}
\vspace{-0.3cm}
\end{table}



\subsection{Effect of Labeled Data Amount}
\label{appendix:semi-label-amount}
Following~\cite{oliver2018realistic}, we further study how the labeled data amount affects SSL in imbalanced learning.
We fix the imbalance ratios of  and  as , and also fix the size of  to be 5x of . We vary  amount to be \texttt{\{0.5x,0.75x,1x\}} with respect to the original labeled data amount.
As Table~\ref{tab:appendix-label-amount} shows, with smaller size of labeled data, the test errors of vanilla CE training increases largely, while adding unlabeled data can maintain sufficiently low errors.
Interestingly, when unlabeled data is added, using only 50\% of labeled data can already surpass the fully-supervised baseline on both datasets, demonstrating the power of unlabeled data in the context of imbalanced learning.

\vspace{-0.2cm}
\begin{table}[ht]
\setlength{\tabcolsep}{10pt}
\caption{\small Ablation study of how labeled data amount affects SSL in imbalanced learning. We fix the imbalance ratios as , and fix unlabeled data amount to be 5x of labeled data used. We vary the amount of  with respect to their original labeled data amount (e.g., 0.5x means only half of the initial labeled data is used). }
\vspace{-1pt}
\label{tab:appendix-label-amount}
\small
\begin{center}
\begin{tabular}{c|c|c|c|c|c|c}
\toprule[1.5pt]
Dataset                 & \multicolumn{3}{c|}{CIFAR-10-LT}     & \multicolumn{3}{c}{SVHN-LT}         \\ \midrule
 Size    & 0.5x  & 0.75x       & 1x             & 0.5x  & 0.75x       & 1x             \\ \midrule\midrule
CE                      & 33.35 & 28.65       & 25.19          & 23.19 & 19.73       & 17.50          \\ \midrule
CE + @5x & 20.77 & \underline{18.67} & \textbf{18.36} & 14.80 & \underline{13.51} & \textbf{13.16} \\
\bottomrule[1.5pt]
\end{tabular}
\end{center}
\vspace{-0.1cm}
\end{table}





\section{Additional Results for Self-Supervised Imbalanced Learning}
\label{appendix:self-results}

\subsection{Different Self-Supervised Pre-Training Methods}
\label{appendix:self-diff-methods}
In this section, we investigate the effect of different SSP methods on imbalanced learning tasks. We select four different SSP approaches, ranging from pretext tasks to recent contrastive methods.

\textbf{Solving Jigsaw Puzzles.} Jigsaw~\cite{noroozi2016unsupervised} is a classical method based on pretext tasks, where an image is divided into patches, and a classifier is trained to predict the correct permutation of these patches.

\textbf{Rotation Prediction.} Predicting rotation~\cite{gidaris2018unsupervised} is another simple yet effective method, where an image is rotated by a random multiple of 90 degrees, constructing a 4-way classification problem; a classifier is then trained to determine the degree of rotation applied to an input image.

\textbf{Selfie.} Selfie~\cite{trinh2019selfie} works by masking out select patches in an image, and then constructs a classification problem to determine the correct patch to be filled in the masked location.

\textbf{Momentum Contrast.} The momentum contrast (MoCo)~\cite{he2019moco} method is one of the recently proposed contrastive techniques, where contrastive losses~\cite{he2019moco} are applied in a representation space to measure the similarities of positive and negative sample pairs, and a momentum-updated encoder is employed.

We conduct controlled experiments over four benchmark imbalanced datasets, and report the results in Table~\ref{tab:ssp-ablation}. As the table reveals, all self-supervised pre-training methods can benefit the imbalanced learning, consistently across different datasets. Interestingly however, the performance gain varies across SSP techniques.
Specifically, on datasets with smaller scale, i.e., CIFAR-10-LT and CIFAR-100-LT, methods using pretext tasks are generally better than using contrastive learning, with \emph{Rotation} performs the best. In contrast, on larger datasets, i.e., ImageNet-LT and iNaturalist, \emph{MoCo} outperforms other SSP methods by a notable margin.
We hypothesize that since MoCo needs a large number of (negative) samples to be effective, the smaller yet imbalanced datasets thus may not benefit much from MoCo, compared to those with larger size and more samples.

\vspace{-0.1cm}
\begin{table}[ht]
\setlength{\tabcolsep}{8pt}
\caption{\small Ablation study of different self-supervised pre-training methods. We set imbalance ratio of  for CIFAR-LT. Best results of each column are in \textbf{bold} and the second best are \underline{underlined}.}
\vspace{-1pt}
\label{tab:ssp-ablation}
\small
\begin{center}
\begin{tabular}{l|c|c|c|c}
\toprule[1.5pt]
\multicolumn{1}{c|}{Dataset}    & {CIFAR-10-LT} & {CIFAR-100-LT} & {ImageNet-LT} & {iNaturalist 2018} \\ \midrule\midrule
\multicolumn{1}{c|}{Vanilla CE} & 25.19         & 56.15          & 61.6          & 39.3   \\ \midrule
+ Jigsaw~\cite{noroozi2016unsupervised}   &  24.68  &  55.89   &  60.2   &  38.2   \1.2pt]
+ Rotation~\cite{gidaris2018unsupervised}  &  \textbf{21.80} & \textbf{54.96} & \underline{55.7} & \underline{36.5} \1.2pt]
CB-CE~\cite{cui2019class} & 38.06           & 16.20          & 78.69           & 47.52           \1.2pt]
CB-Focal~\cite{cui2019class} & 39.73           & 16.54          & 80.24           & 49.98           \1.2pt]
LDAM-DRW~\cite{cao2019learning} & 23.08           & 12.19          & 54.64           & 40.54           \1.2pt]
LDAM-DRW + \textbf{\textit{SSP}} & \textbf{22.95}  & \textbf{11.83} & \textbf{54.28}  & \textbf{40.33}  \\
\bottomrule[1.5pt]
\end{tabular}
\end{center}
\end{table}
