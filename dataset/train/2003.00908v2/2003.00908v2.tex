\section{Experiments}

\newcommand{\mcJ}{\mathcal{J}}  \newcommand{\mcF}{\mathcal{F}}  \newcommand{\mcG}{\mathcal{G}} 
\newcommand{\mcJF}{\mathcal{J \& F}} 
We perform experiments on three benchmarks: DAVIS 2016 \cite{perazzi2016davis}, DAVIS 2017 \cite{perazzi2016davis} and YouTube-VOS \cite{xu2018youtube2}. For YouTube-VOS, we compare on the official validation set, with withheld ground-truth. For ablative experiments, we also show results on a separate validation split of the YouTube-VOS train set, consisting of 300 videos not used for training. Following the standard DAVIS protocol, we report both the mean Jaccard  index and mean boundary  scores, along with the overall score , which is the mean of the two. For comparisons on YouTube-VOS, we report  and  scores for classes included in the training set (seen) and the ones that are not (unseen). The overall score  is computed as the average over all four scores, defined in YouTube-VOS. 
In addition, we compare the computational speed of the methods in terms of frames per second (FPS), computed by taking the average over the DAVIS 2016 validation set. For our approach, we used a V100 GPU and included all steps in Algorithm~\ref{alg:deploy} to compute the frame rates. Further results and analysis are provided in the supplement. 

\subsection{Ablation study}  

We analyze the contribution of the key components in our approach. All compared approaches are trained using the YouTube-VOS training split.

\noindent\textbf{Base net:} We construct a baseline network to analyze the impact of our target model . This is performed by replacing  with an offline-trained target encoder, and retraining the segmentation network . As for our proposed network we keep the backbone  parameters fixed. The target encoder is comprised of two convolutional layers, taking reference frame features from ResNet blocks \verb|conv4_x| and the corresponding target mask as input. Features (\verb|conv4_x|) extracted from the test frame are concatenated with the output from the target encoder and processed with two additional convolutional layers. The output is then passed to the segmentation network  in the same manner as for the coarse segmentation score  (see Section~\ref{sec:refinement}). We train this model with the same methodology as for our network. 

\noindent\textbf{F.-T:} We integrate a first-frame fine-tuning strategy into our network to compare this to our discriminative target model. For this purpose, we create an initial dataset  with 20 samples using the \emph{same} sample generation procedure employed for our approach (section~\ref{sec:discriminator}). We then fine-tune all components of the network, except for the feature extractor, with supervision on the target model (loss in~\eqref{eq:L2}) and the pre-trained segmentation network (binary cross-entropy loss) using the ADAM optimizer with 100 iterations and a batch size of four. In this setting we omitted the proposed optimization strategy of the target model and instead initialize the parameters randomly before fine-tuning.

\noindent\textbf{-only - no update:} To analyze the impact of the segmentation network , we remove it from our architecture and instead let the target-specific model  output the final segmentations. The coarse target model predictions are upsampled to full image resolution through bilinear interpolation. In this version, we only train the target model  on the first frame, and refrain from subsequent updates.

\noindent\textbf{-only:} We further enable target model updates (as described in Section \ref{sec:inference}) using the raw target predictions.

\noindent\textbf{Ours - no update:} For a fair comparison, we evaluate a variant of our approach with the segmentation network, but without any update of the target model  during inference.

\noindent\textbf{Ours:} Finally, we include target model updates with segmentation network predictions to obtain our final approach.

In Table~\ref{tab:ab}, we present the results in terms of the  score on a separate validation split of the YouTube-VOS training dataset. The base network, not employing the target model , achieves a score of . Employing fine-tuning on the first frame leads to an absolute improvement of . Remarkably, using only the linear target model  is on par with online fine-tuning. While fine-tuning an entire segmentation network is prone to severe overfitting to the initial frame, our shallow target model has limited capacity, acting as an implicit regularization mechanism that benefits robustness and generalization to unseen aspects of the target and background appearance.
Including updates results in an absolute improvement of , demonstrates that we benefit from online updates despite the coarseness of the target mode generated labels. Further adding the segmentation network  (Ours - no update) leads to a major absolute gain of . This improvement stems from the offline-learned processing of the coarse segmentations, yielding more accurate mask predictions. Finally, the proposed online updating strategy additionally improves the score to . 

\newcommand{\yes}{\checkmark}
\newcommand{\no}{\text{\ding{55}}}
\begin{table}[t!]
	\centering
	\resizebox{0.68\columnwidth}{!}{\begin{tabular}[]{lcccc}
		\toprule
		Version &  &  & Update. &  \\
		\midrule
		Base net & & \yes & &  49.8 \\
F.-T. &  & \yes &  & 58.9 \\
		-only -no update & \yes & & & 58.3  \\
		-only & \yes & & \yes & 59.6  \\
		\textbf{Ours} - no update & \yes & \yes &   & 67.9 \\
		\textbf{Ours} & \yes & \yes & \yes &  \textbf{71.4}\\
\bottomrule
	\end{tabular}
}\vspace{1mm}
	\caption{Ablative study on a validation split of 300 sequences from the YouTube-VOS train set. We analyze the different components of our approach, where  and  denote the target model and segmentation network respectively. Further, ``Update'' indicates if the target model update is enabled. Our target model  outperforms the Base net and is comparable to first-frame fine-tuning (``F.-T.'') even with updates are disabled. Further, the segmentation network significantly improves the raw predictions from the target model . Finally, the best performance is obtained when additionally updating target model .}
	\label{tab:ab}
	\vspace{-4mm}
\end{table}

\subsection{Comparison to state-of-the-art}

We compare our method to recent approaches on the YouTube-VOS, DAVIS 2017 and DAVIS 2016 benchmarks. We provide results for two versions of our approach: \textbf{Ours} and \textbf{Ours (fast)} (see Section~\ref{sec:imp-details}). Many compared methods include additional training data or employ models that have been pre-trained on segmentation data. For fair comparison we classify methods into two categories: ``seg" for methods employing segmentation networks, pre-trained on e.g PASCAL~\cite{Everingham15} or MS-COCO~\cite{lin2014microsoft} and ``synth'' for methods that perform additional training on synthetic VOS data generated from image segmentation datasets.

\noindent\textbf{YouTube-VOS}~\cite{xu2018youtube2}: The official YouTube-VOS validation dataset has 474 sequences with objects from 91 classes. Out of these, 26 classes are not present in the training set. We provide results for {\bf Ours} and {\bf Ours (fast)}, both trained on the YouTube-VOS 2018 training set. We compare our method with the results reported in \cite{xu2018youtube}, that were obtained by retraining the methods on YouTube-VOS. Additionally, we compare to PReMVOS\cite{luiten2018premvos}, AGAME \cite{johnander2018generative}, RVOS~\cite{ventura2019rvos} and STM~\cite{oh2019video}. The results are reported in Table~\ref{tab:ytvos_results}. 

Among the methods using additional training data, OSVOS~\cite{caelles2017osvos}, OnAVOS~\cite{voigtlaender2017onavos} and PReMVOS employ first-frame fine-tuning, leading to inferior frame-rates below  FPS. In addition to fine-tuning, PReMVOS constitutes a highly complex framework, encompassing multiple components and cues: mask-region proposals, optical flow based mask predictions, re-identification, merging and tracking modules. In contrast, our approach is simple, consisting of a single network together with a light-weight target model. Remarkably, our approach significantly outperforms PReMVOS by a relative margin of , yielding a final -score of . The recent STM method has the highest performance, employing feature matching with a dynamic memory to predict the target.

RVOS is trained only on YouTube-VOS, achieving a -score of  by employing recurrent networks. In addition to recurrent networks, S2S employs first-frame fine-tuning, achieving a -score of  with a significantly slower frame-rate compared to RVOS. In AGAME a generative appearance model is employed, resulting in a -score of . We further report results from a version of STM (YV18), where training has been performed solely on YouTube-VOS. This significantly degrades the performance to a -score of . {\bf Ours} outperforms all previous methods when only video data from YouTube-VOS has been used for training. We believe that, since our target model already provides robust predictions of the target on its own, our approach can achieve high performance without extensive training on additional data. Notably, {\bf Ours-fast}, maintains an impressive -score of , while being significantly faster than all previous methods at  FPS.


\begin{table}[t!]
\centering \resizebox{1.01\columnwidth}{!}{\renewcommand{\yes}{\text{\ding{51}}}
\renewcommand{\no}{-}
\centering
\centering
\begin{tabular}{lcccccc}
	\toprule
	& \multicolumn{2}{c}{Training Data}&  &  &  & FPS\\
	Method & \footnotesize seg & \footnotesize synth & \footnotesize overall & \hspace{2.9mm}\footnotesize seen  unseen & \footnotesize \hspace{2.2mm} seen  unseen &  \footnotesize DAVIS16  \\
	\midrule
	{\bf Ours}      & \no &\no& {\bf72.1} & {\bf 72.3}  {\bf 65.9} & {\bf 76.2}  {\bf 74.1} & 21.9\\
	{\bf Ours-fast} & \no &\no& 65.7 & 68.6  58.4 & 71.3  64.5 & {\bf 41.3}\\
	STM (YV18) \cite{oh2019video}&\no&\no & 68.2 & -  - & -  - & 6.25\\
	AGAME \cite{johnander2018generative}&\no&\no & 66.1 & 67.8  60.8 & 69.5  66.2 & 14.3\\
	RVOS \cite{ventura2019rvos}&\no&\no & 56.8 & 63.6  45.5 & 67.2  51.0 & 22.7\\
	S2S \cite{xu2018youtube}&\no&\no & 64.4 & 71.0  55.5 & 70.0  61.2 & 0.11\\
	\midrule
	STM \cite{oh2019video}&\no&\yes& {\bf 79.4} & {\bf 79.7}  {\bf 72.8} & {\bf 84.2}  {\bf 80.9} & 6.25\\
	PReMVOS \cite{luiten2018premvos}&\yes & \yes & 66.9 & 71.4  56.5 & -  - & 0.03\\
	OnAVOS \cite{voigtlaender2017onavos}&\yes&\no & 55.2 & 60.1  46.1 & 62.7  51.4 & 0.08\\
	OSVOS \cite{caelles2017osvos}&\yes&\no & 58.8 & 59.8  54.2 & 60.5  60.7 & 0.22\\
	\bottomrule
\end{tabular}


 }\vspace{1mm}
\caption{State-of-the-art comparison on the large-scale YouTube-VOS validation dataset, containing 474 videos. The results of our approach were obtained through the official evaluation server. We report the mean Jaccard () and boundary () scores for object classes that are \emph{seen} and \emph{unseen} in the training set, along with the overall mean (). ``seg'' and ``synth'' indicate whether pre-trained segmentation models or additional data has been used during training. Our approaches achieve superior performance to methods that only trains on the YouTube-VOS train split, while operating at high frame-rates. Furthermore, {\bf Ours-fast} obtains the highest frame-rates while performing comparable to state-of-the-art.}
\label{tab:ytvos_results}
\vspace{-4mm}
\end{table}

\noindent\textbf{DAVIS 2017}~\cite{perazzi2016davis}: The validation set for DAVIS 2017 contains 30 sequences. We provide results for {\bf Ours} and {\bf Ours-fast}, trained a combination of the YouTube-VOS and DAVIS 2017 train splits, such that DAVIS 2017 is traversed eight times per epoch, and YouTube-VOS once. We report the results on DAVIS 2017 in Table~\ref{tab:dvjoint_results}. As in the YouTube-VOS comparison above, we categorize the methods with respect to usage of training data. Since our approaches ({\bf Ours} and {\bf Ours-fast}) and AGAME~\cite{johnander2018generative}, RVOS~\cite{ventura2019rvos}, STM~\cite{oh2019video} and FEELVOS~\cite{voigtlaender2018feelvos} all include the YouTube-VOS during training, we add a third category denoted ``yv".

OnAVOS~\cite{voigtlaender2017onavos}, OSVOS-S~\cite{maninis2017osvos_s}, MGCRN~\cite{hu2018mgcrn}, PReMVOS~\cite{luiten2018premvos} employ extensive fine-tuning on the first-frame, experiencing impractical segmentation speed. The methods RGMP~\cite{oh2018rgmp}, AGAME~\cite{johnander2018generative}, RANet~\cite{wang2019ranet} and FEELVOS~\cite{voigtlaender2018feelvos} all employ mask-propagation, which is combined with feature matching in the latter three methods. {\bf Ours} outperforms these methods with an  score of . In addition, {\bf Ours-fast} is significantly faster than all previous approaches, maintaining a  score of . 
Our method is only outperformed by PReMVOS and the recent STM, achieving  scores of  and  respectively. PReMVOS, however, suffer from extremely slow frame rates: approximately 500 times lower than ours. Moreover, our approach outperforms PReMVOS on the more challenging and large scale YouTube-VOS (Table~\ref{tab:ytvos_results}) by a large margin.


We also evaluate our approach when only trained on DAVIS 2017, denoted {\bf Ours} (DV17). We compare this approach to the methods FAVOS~\cite{cheng2018favos}, AGAME (DV17)~\cite{johnander2018generative} and STM  (DV17)~\cite{oh2019video}, which have also only been trained on the DAVIS 2017 train split. Our method significantly outperform all these methods with a  score of . Moreover, this result is superior to OnAVOS, OSVOS-S, RANet, RVOS, RGMP and comparable to AGAME and FEELVOS despite their use of additional training data.


\begin{table}[t]
	\centering
	\tabcolsep=0.1cm
	\resizebox{0.9\columnwidth}{!}{\renewcommand{\yes}{\text{\ding{51}}}
\renewcommand{\no}{-}
\newcommand{\first}[1]{\textbf{\textcolor{red}{#1}}}
\newcommand{\second}[1]{\textit{\textcolor{blue}{#1}}}
\begin{tabular}{@{~}lccc@{~~~}c|c|c}
\toprule
       & \multicolumn{3}{c@{~~~}}{Training Data} & DAVIS17  & DAVIS16 &  \\
Method & yv & seg & synth &  &  & FPS \\
\midrule
{\bf Ours} (DV17)                          & \no & \no & \no & \first{68.8} & \first{81.7} & \first{21.9} \\
AGAME (DV17) \cite{johnander2018generative}& \no & \no & \no & \second{63.2} & -    & \second{14.3} \\
FAVOS \cite{cheng2018favos}                & \no & \no & \no & 58.2 & 80.8 & 0.56 \\
STM (DV17) \cite{oh2019video}              & \no & \no & \no & 43.0 & -    & 6.25 \\
\midrule
{\bf Ours}                              & \yes& \no & \no & 76.7 & 83.5 & 21.9 \\
{\bf Ours-fast}                         & \yes& \no & \no & 70.2 & 78.5 & \first{41.3} \\
RVOS \cite{ventura2019rvos}                & \yes& \no & \no & 50.3 & -    & 22.7 \\
RANet \cite{wang2019ranet}                 & \no & \no &\yes & 65.7 & 85.5 & \second{30.3} \\
AGAME \cite{johnander2018generative}       & \yes& \no &\yes & 70.0 & -    & 14.3 \\
STM \cite{oh2019video}                     & \yes& \no &\yes & \first{81.8} & \first{89.3} & 6.25 \\
RGMP \cite{oh2018rgmp}                     & \no & \no &\yes & 66.7 & 81.8 & 7.7  \\
FEELVOS \cite{voigtlaender2018feelvos}     & \yes&\yes & \no & 71.5 & 81.7 & 2.22 \\
OSNM \cite{yang2018osnm}                   & \no & \no &\yes & 54.8 & -    & 7.14 \\
PReMVOS \cite{luiten2018premvos}           & \no &\yes &\yes & \second{77.8} & \second{86.8}  & 0.03 \\
OSVOS-S \cite{maninis2017osvos_s}          & \no &\yes & \no & 68.0         & 86.5 & 0.22 \\
OnAVOS \cite{voigtlaender2017onavos}       & \no &\yes & \no & 67.9         & 85.5          & 0.08 \\
MGCRN \cite{hu2018mgcrn}                   & \no &\yes & \no & -            & 85.1          & 1.37 \\
\bottomrule
\end{tabular}
 	}\vspace{1mm}
	\caption{State-of-the-art comparison on DAVIS 2017 and DAVIS 2016 validation sets. The columns with ``yv", ``seg", and ``synth" indicate whether YouTube-VOS, pre-trained segmentation models or additional synthetic data has been used during training. The best and second best entries are shown in red and blue respectively. In addition to {\bf Ours} and {\bf Ours-fast}, we report the results of our approach when trained on \emph{only} DAVIS 2017, in {\bf Ours} (DV17). Our approach outperform compared methods with practical frame-rates. Furthermore, we achieve competitive results when trained with only DAVIS 2017, owing to our discriminative target model.}
	\vspace{-4mm}
	\label{tab:dvjoint_results}
\end{table}

\noindent\textbf{DAVIS 2016}~\cite{perazzi2016davis}: Finally, we evaluate our method on the 20 validation sequences in DAVIS 2016, corresponding to a subset of DAVIS 2017 and report the results in Table~\ref{tab:dvjoint_results}. Our methods perform comparable to the fine-tuning based approaches PReMVOS, MGCRN~\cite{hu2018mgcrn}, OnAVOS and OSVOS-S. Further, {\bf Ours} outperforms AGAME, RGMP, OSNM, FAVOS and FEELVOS. 

\begin{figure*}[!t]
	\centering 
	\newcommand{\image}{\includegraphics[width=0.141\textwidth]}
	\tabcolsep=0.01cm
	\renewcommand{\arraystretch}{0.2}
	\begin{tabular}{ccccccc}
 & {\footnotesize Ground truth} & {\footnotesize {\bf Ours}} & {\footnotesize OSVOS-S} & {\footnotesize AGAME} & {\footnotesize FEELVOS} & {\footnotesize RGMP} \\
\image{qresults/shooting/00004.jpg} &
\image{qresults/shooting/00004-gt.png} &
\image{qresults/shooting/00004.png} &
\image{qresults/shooting/00004-osvoss.png} &
\image{qresults/shooting/00004-agame.png} &
\image{qresults/shooting/00004-feelvos.png} &
\image{qresults/shooting/00004-rgmp.png} \\
\image{qresults/shooting/00039.jpg} &
\image{qresults/shooting/00039-gt.png} &
\image{qresults/shooting/00039.png} &
\image{qresults/shooting/00039-osvoss.png} &
\image{qresults/shooting/00039-agame.png} &
\image{qresults/shooting/00039-feelvos.png} &
\image{qresults/shooting/00039-rgmp.png} \\

\image{qresults/bike-packing/00034.jpg} &
\image{qresults/bike-packing/00034-gt.png} &
\image{qresults/bike-packing/00034.png} &
\image{qresults/bike-packing/00034-osvos.png} &
\image{qresults/bike-packing/00034-agame.png} &
\image{qresults/bike-packing/00034-feelvos.png} &
\image{qresults/bike-packing/00034-rgmp.png} \\
\image{qresults/bike-packing/00068.jpg} &
\image{qresults/bike-packing/00068-gt.png} &
\image{qresults/bike-packing/00068.png} &
\image{qresults/bike-packing/00068-osvos.png} &
\image{qresults/bike-packing/00068-agame.png} &
\image{qresults/bike-packing/00068-feelvos.png} &
\image{qresults/bike-packing/00068-rgmp.png} \\

\image{qresults/loading/00005.jpg} &
\image{qresults/loading/00005-gt.png} &
\image{qresults/loading/00005.png} &
\image{qresults/loading/00005-osvoss.png} &
\image{qresults/loading/00005-agame.png} &
\image{qresults/loading/00005-feelvos.png} &
\image{qresults/loading/00005-rgmp.png} \\
\image{qresults/loading/00049.jpg} &
\image{qresults/loading/00049-gt.png} &
\image{qresults/loading/00049.png} &
\image{qresults/loading/00049-osvoss.png} &
\image{qresults/loading/00049-agame.png} &
\image{qresults/loading/00049-feelvos.png} &
\image{qresults/loading/00049-rgmp.png} \\
\end{tabular}
 \vspace{1mm}
	\caption{Examples of three sequences from DAVIS 2017, demonstrating how our method performs under significant appearance changes compared to ground truth, OSVOS-S \cite{maninis2017osvos_s}, AGAME \cite{johnander2018generative}, FEELVOS \cite{voigtlaender2018feelvos} and RGMP \cite{oh2018rgmp}.}
	\label{fig:qualitative_results2}
	\vspace{-3mm}
\end{figure*}


\begin{figure}
\newcommand{\image}{\includegraphics[width=0.248\columnwidth]}
\centering 
\tabcolsep=0.01cm
\renewcommand{\arraystretch}{0.06}
\begin{tabular}{cccc}

{\footnotesize Image} & {\footnotesize Ground truth} & {\footnotesize Coarse scores} & {\footnotesize Output}  \\


\image{qresults/14dae0dc93/00050.jpg} &
\image{qresults/14dae0dc93/00050-gt.png} &
\image{qresults/14dae0dc93/00050-scores.png} &
\image{qresults/14dae0dc93/00050.png} \\

\image{qresults/731b825695/00040.jpg} &
\image{qresults/731b825695/00040-gt.png} &
\image{qresults/731b825695/00040-scores.png}  &
\image{qresults/731b825695/00040.png} \\

\image{qresults/4f414dd6e7/00105.jpg} &
\image{qresults/4f414dd6e7/00105-gt.png} &
\image{qresults/4f414dd6e7/00105-scores.png} &
\image{qresults/4f414dd6e7/00105.png} \\


\image{qresults/kite-surf/00040.jpg} &
\image{qresults/kite-surf/00040-gt.png} &
\image{qresults/kite-surf/00040.png} &
\image{qresults/kite-surf/00040-scores.png} \\

\image{qresults/a9c9c1517e/00045.jpg} &
\image{qresults/a9c9c1517e/00045-gt.png} &
\image{qresults/a9c9c1517e/00045-scores.png} &
\image{qresults/a9c9c1517e/00045.png}  \\

\end{tabular}\vspace{0.5mm}\caption{Qualitative results of our method, showing both target model score maps and the output segmentation masks. The top three rows are success cases and the last two represents failures when objects are too thin or appear too similar. }
\label{fig:qualitative_results1}
\vspace{-2mm}
\end{figure}

\subsection{Qualitative Analysis}

\noindent\textbf{State-of-the-art:} We compare our approach to some state-of-the-art methods in Figure~\ref{fig:qualitative_results2}. By using an early and a late frame from each video sequence, we study how the methods cope with large target deformations over time.
In the first sequence (first and second row), the fine-tuning based OSVOS-S struggles as the target pose changes. While the mask-propagation in RGMP and generative appearance model in AGAME are accurate on fine details, they both fail to segment the red target, possibly due to occlusions. In the second and third sequences (rows three to six), all of the above methods fail to robustly segment the different targets. In contrast, our method accurately segments all targets in these challenging video sequences.

\noindent\textbf{Target model:} Some examples of the coarse segmentation scores and the final segmentation output are visualized in Figures~\ref{fig:intro} and ~\ref{fig:qualitative_results1}. In most cases, the target model provides robust segmentation scores of the target object. It however struggles is some cases where the target object contains thin or small structures or details. An example is the challenging kite lines in the \emph{kite-surfing} sequence, which are not accurately segmented.
This is likely due to the coarse feature maps the target model is operating on. It also have problems separating almost identical targets such as the sheep. On the other hand, the model successfully handles very similar targets as in the gold-fish sequence (row 2 in Figure~\ref{fig:intro}). 
