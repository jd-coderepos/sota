\section{Experiments}
\label{sec:exp}



\subsection{Experiment Setup}

We mainly evaluate Refiner for the image classification task. Besides, we also conduct experiments on natural language processing tasks to investigate its generalizability to NLP transformer models. 
\vspace{-3mm}
\paragraph{Computer vision} We evaluate the effectiveness of the refiner on ImageNet \cite{deng2009imagenet}. For a fair comparison with other methods, we first replace the SA module with refiner on ViT-Base \cite{dosovitskiy2020image} model as it is the most frequently used one \cite{dosovitskiy2020image, touvron2020training, yuan2020revisiting, touvron2021going}. When comparing with other state-of-the-art methods, we modify the number of transformer blocks and the embedding dimensions to increase the efficiency in the same manner as \cite{yuan2021tokens, zhou2021deepvit}. 
\vspace{-3mm}
\paragraph{Natural language processing} We evaluate our model on the General Language Understanding Evaluation (GLUE) benchmark~\cite{wang2018glue}. GLUE benchmark includes various tasks which are formatted as single sentence or sentence pair classification. See Appendix for more details of all tasks.
We measure accuracy for MNLI, QNLI, QQP, RTE, SST, Spearman correlation for STS and Matthews correlation for CoLA. 
The GLUE score is the average   score for all the 8 tasks. 


\vspace{-3mm}
\paragraph{Training setup} All the experiments are   conducted upon PyTorch~\cite{paszke2019pytorch} and the timm~\cite{rw2019timm} library. The models are trained on ImageNet-1k from scratch without auxiliary dataset.
For the ablation experiments, we follow the standard training schedule and train our models on the ImageNet dataset for 300 epochs. When compared to state-of-the-art (SOTA) models, we use the advanced training recipes as proposed in \cite{touvron2020training}. Detailed training hyper-parameters   are listed in the appendix.  


\subsection{Analysis}
\label{exp:analysis}
Refiner introduces attention expansion and convolutional local kernels to augment the self-attention maps of ViTs. Here we individually investigate their effectiveness through ablative studies. 



\begin{table}[h]
    \footnotesize
    \setlength\tabcolsep{2.1pt}
    \centering
    \caption{\small  Effect of the expansion ratio in  attention expansion, which varies from 1 to 6. The model is \OURS{} with 16 blocks, 12 attention heads and 384 embedding dimension. }
\label{tab:attn_expansion_ratio}
\begin{tabular}{cccc} \toprule
    Expan. Ratio & Params & Converge  (\#Epoch) &  Top-1 (\%)\\ \midrule
     1 & 25M & 300 & 82.3\\ 
    2 & 25M & 300 & 82.8\\ 
    3 & 25M & 273 & 82.8\\
     4 & 25M & 270 & 82.9\\
    6 & 25M & 261 & 83.0\\ \bottomrule
    \end{tabular}
\end{table}


\paragraph{Effect of attention   expansion} We adopt  convolution to adjust the number of attention maps. In this ablative study, we vary the expansion ratio  from 1 to 6. From the results     in Tab.~\ref{tab:attn_expansion_ratio}, we observe that along with    increased expansion ratio (and more attention maps), the model performance   monotonically increases from 82.3\% to 83.0\%. This clearly demonstrates the effectiveness of attention expansion in improving the model performance. Note when the expansion ratio equals to 1, the attention expansion degenerates to the talking-heads attention \cite{shazeer2020talking,zhou2021deepvit}. Compared with this strong baseline, increasing the ratio to 2 brings 0.5\% top-1 accuracy improvement. Interestingly, using larger expansion ratio speeds up the convergence of model training. Using an expansion ratio of 6 saves the number of training epochs by nearly 13\% (261 vs.\ 300 epochs). This reflects that attention expansion can help   the  model learn the discriminative token features faster. However, when the expansion ratio is larger than 3, the benefits in the model accuracy attenuates. This motivates us to explore the distributed local attention for further performance enhancement.






\vspace{-2mm}
\paragraph{Effect of attention   reduction} Refiner  deploys another  convolution to reduce the number of attention maps from  to the original number ,  after the attention expansion and distributed local attention, in order to reduce the computation overhead due to expansion. We conduct experiments to study whether reduction will hurt model performance.  As observed from Tab.~\ref{tab:attn_reduction}, attention reduction   drops the accuracy very marginally, implying the attention maps have been sufficiently augmented in the higher-dimension space. 

\vspace{-2mm}
\paragraph{Effect of distributed local attention} We then evaluate whether the distributed local attention (DLA)    works consistently well across a broad spectrum of model architectures. We evaluate its effectiveness for various ViTs with 12 to 32 SA blocks, without the attention expansion. Following the common practice~\cite{dosovitskiy2020image}, the hidden dimension is 768 for the ViT-12B and 384 for all the other ViTs.   We set the local attention window size as  and use the DLA to replace all the self-attention block within ViT. From the results   summarized in Tab.~\ref{tab:convolution_effect}, DLA  can consistently boost the top-1 accuracy of various ViTs by 1.2\% to 1.7\% with negligible model size increase. Such significant performance boost clearly demonstrates  its effectively and the benefits brought by its ability in  jointly modeling the local and global context of input features. The combined effects of attention expansion and DLA are shown in Tab. \ref{tab:sota}.


\begin{table}[t]
\begin{minipage}[t]{0.55\textwidth}
    \caption{\small  Impacts of convolution on attention maps. We directly apply the  convolution on the attention maps from the multi-head self-attention of ViTs with respect to various architectures. We can observe clear improvement for all ViT variants when adding
    the proposed DLA.}
    \vspace{-2mm} 
    \label{tab:convolution_effect}
    \footnotesize
    \setlength\tabcolsep{2.1pt}
    \begin{center}
     \begin{tabular}{l c c c c c c c} 
    \toprule
     Model & \#Blocks & Hidden dim &  \#Heads  & Params &  Top-1  (\%)\\ 
    \midrule
      \textsc{ViT}&  12 & 768 & 12 &   86M & 79.5 \\ 
~ +  DLA & 12  & 768 & 12    & 86M & 81.2 \\ 
    \midrule
      \textsc{ViT}&  16 & 384 & 12   & 24M & 78.9\\ 
~  +  DLA & 16  & 384 & 12     & 24M & 80.3\\ 
     \midrule
      \textsc{ViT}&  24 & 384 & 12   & 36M & 79.3\\ 
~  +  DLA & 24  & 384 & 12   & 36M & 80.9\\ 
     \midrule
      \textsc{ViT}&  32 & 384 & 12   & 48M & 79.2\\ 
~  +  DLA & 32  & 384 & 12     & 48M & 81.1 \\ 
    \bottomrule
    \end{tabular}
\end{center}
\end{minipage}
\hfill
\begin{minipage}[t]{0.42\linewidth}
    \begin{minipage}[t]{\linewidth}
    \centering
    \small
    \setlength\tabcolsep{0.5mm}
    \renewcommand\arraystretch{1}
    \caption{\small Effect of attention reduction on Refined-ViT-16B with 384 hidden dimension.}
    \label{tab:attn_reduction}
    \vspace{-3pt}
    \begin{tabular}{ccc} \toprule
    Model & Attn. map  &Top-1 (\%)\\ \midrule[0.5pt]  
    Refined-ViT-16B & w/o reduction & 82.99 \\
    Refined-ViT-16B & w/ reduction  & 82.95 \\
    \bottomrule
  \end{tabular}
    \vspace{15.3pt}
    \end{minipage}
    
    \begin{minipage}[b]{\linewidth}
    \caption{\small Evaluation on how the  spatial span within DLA  affects the model performance. We compare the model performance with three different constraints on the local  kernels. }
    \vspace{-8pt}
    \label{tab:spatial_span_effect}
    \footnotesize
    \setlength\tabcolsep{4pt}
    \begin{center}
    \begin{tabular}{ccc}  \toprule
    Model & Constraints       &  Top-1 (\%) \\ \midrule
    Refined-ViT-16B & None    &   83.0\\ 
    Refined-ViT-16B & Spatial  & 82.7\\ 
    Refined-ViT-16B & Row+Col & 81.7 \\ \bottomrule
    \end{tabular}
\end{center}
    \end{minipage}
    
\end{minipage}
\end{table}

\begin{wrapfigure}[18]{r}{0.5\textwidth}
    \centering
    \vspace{-4mm}
    \includegraphics[width=0.5\textwidth]{figures/feature_evolving_conv.pdf}
    \vspace{-4mm}
    \caption{\small Refiner accelerates feature evolving compared with CNNs, the vanilla ViT and the Deit trained with a more complex   scheme.}
    \label{fig:feat_evo_conv}
\end{wrapfigure}

\vspace{-2mm}
\paragraph{Effect of the local attention kernels }  
Refiner directly applies  
the  convolution   onto the attention maps .
We compare this natural choice with another two feasible choices for the local kernels. The first one is to  apply spatially adjacent convolution on the  reshaped attention maps such that  the aggregated tokens by the local kernels  are spatially adjacent in the original input. Specifically,  
we reshape each row of  into a  matrix\footnote{Recall  is the number of tokens and the input image is divided into  patches.}  which together form a   tensor and apply  convolution.  
The second one is to apply the combination of row and column 1D convolution. From the results in Tab.~\ref{tab:spatial_span_effect},   we find the spatially adjacent convolution will slightly hurt the performance while only adopting the 1D convolution   leads to 1.3\% accuracy drop. Directly applying convolution to the attention maps (as what refiner does) gives the best performance, demonstrating the effectiveness of augmenting the local patterns of the attention maps.

















\paragraph{Refiner augments attention maps and accelerates feature evolving} We visualize the attention maps output from different self-attention blocks without and with the refiner in Fig.~\ref{fig:vis_atten_map}. Clearly, the attention maps after Refiner become not so uniform and present stronger local patterns. We also plot the feature evolving speed in Fig.~\ref{fig:feat_evo_conv},  in terms of the CKA similarity change w.r.t.\ the first block features and Refined-ViT evolves the features much faster than ViTs.

\begin{figure}[h]
  \centering
  \footnotesize
  \setlength\tabcolsep{0.2mm}
  \renewcommand\arraystretch{1.0}
  \begin{tabular}{cccccccc}
    \rotatebox{90}{~~~~~~SA} & 
    \addFigs{0} & 
    \addFigs{4} & 
    \addFigs{10} & 
    \addFigs{18} & 
    \addFigs{22} &
    \addFigs{28} &
\addFigs{29} \\
    \rotatebox{90}{~~~Refiner} & 
    \addFigsConv{0} & 
    \addFigsConv{8} &
    \addFigsConv{11} & 
    \addFigsConv{16} & 
    \addFigsConv{21} &
    \addFigsConv{24} &
\addFigsConv{28} \\
     & Block 1 & Block 4 & Block 11 & Block 18  &Block 23 & Block 29 
     & Block 30
     \\
  \end{tabular}
  \vspace{3pt}
  \caption{\small  Compared with the attention matrices  from the vanilla SA (top), for   deeper blocks, refiner (bottom) strengthens the local patterns of their attention maps,   making them less uniform and better model   local context. 
  }
  \label{fig:vis_atten_map}
\end{figure}





\vspace{-2mm} 
\subsection{Comparison with SOTA}
\label{exp:vision_sota}
We compare refined-ViT with   state-of-the-art models in Tab.~\ref{tab:sota}. For small-sized model, with  224224  test resolution, Refined-ViT-S (with only 25M parameters) achieves 83.6\% accuracy on ImageNet, which outperforms the strong baseline DeiT-S by   3.7\%. For medium-sized model with 384 384   test resolution, the model Refined-ViT-M  achieves the accuracy of 85.6\%, which outperforms the latest SOTA  CaiT-S36 by 0.2\% with 13\% less of parameters (55M vs.\ 68M) and outperforms LV-ViT-M by 1.6\%. Note that CaiT uses knowledge distillation based method to improve their model, requiring much more computations. 
For large-sized model finetuned with input resolution of 448 448, refined-ViT achieves 86\%, better than CaiT-M36 with 70\% less of parameters (81M vs.\ 271M). This is the new state-of-the-art results on ImageNet with less than 100M    parameters. Notably, as shown in Tab. \ref{tab:sota}, our models significantly outperform the recent  models that introduce   locality into the token features across all the model sizes. This clearly  demonstrates: first, refining the attention maps (and thus the feature aggregation mechanism) is more effective than augmenting the features for ViTs; secondly,   jointly modeling the local and global context (from DLA in the refiner)  is better than using    the convolution and self-attention separately and deploying them into different blocks.

\subsection{Receptive field calibration}
In this subsection, we present and investigate a simple approach that we find can steadily improve classification performance of  ViT. The approach is a generic one that also works well for CNNs. 

As pointed out in \cite{touvron2019fixing}, pre-processing the training images and testing images separately could lead to a distribution shift between the training and testing regimes. This issue has been named by \cite{touvron2019fixing} as the mismatch between the region of classification (RoC),
which could degrade   performance of the classification model. 
A common remedy solution is to apply random cropping to get a centrally cropped image patch for classification,  with a certain cropping ratio (less than 1),  at the testing phase. 
However, as the receptive field of a  deep neural network  at the deep layers is typically larger than the image size, we argue that it is not necessary to crop a patch from the testing images.  

Instead, we propose to pad the testing image such that all the image features can be captured by the model.   Different from directly feeding the image into the model, padding the image with zeros can align the informative region of an image to the center of the receptive filed of the model, similar to random cropping, while getting rid of information loss. Thus it can  further improve the model accuracy.  We name this padding method as \textit{receptive field calibration} (RFC).  RFC can be easily implemented by setting the cropping ratio to be larger than 1, which we call it \textit{RFC ratio},  during the testing phase. 

To investigate its effectiveness, among all the models whose pre-trained weights are publicly available, we apply RFC on the top 2 accurate CNN models  and ViTs respectively. The results are summarized in Tab.~\ref{tab:rfc}. It is observed that RFC can easily further improve the performance of current SOTA model  by 0.11\% \textit{without} fine-tuning.  It is worth noting that the selected models are pre-trained with large auxiliary datasets such as ImageNet-22k and JFT-300M and the performance is nearly saturated. The testing image resolutions   vary from 384 to 800. Under a variety of model configurations, RFC consistently improve the accuracy. With RFC, our proposed Refined-ViT achieves a new SOTA on the ImageNet (among models with less than 100M parameters). 

It should be noted that in the above experiments (Sec.~\ref{exp:analysis} and~\ref{exp:vision_sota}), including comparing Refined-ViT with SOTAs,  we did \textit{not} apply   RFC   for fair comparisons. Here we would like to share such RFC technique as an interesting finding from our recent experiments. RFC is a generic and convenient technique to improve model's classification performance. We believe it is inspiring for rethinking the common random cropping techniques. More importantly, it motivates future studies to dig into the effect  of the model's receptive field   in  classification performance and explore how to calibrate it w.r.t.\ the input images to gain better performance. 

\begin{table*}[h]
\caption{\small RFC can improve both CNN and ViT-based SOTA models on ImageNet, outperforming the strategy of random cropping. We apply RFC on the best performing     pre-trained models   available online.   RFC improves the top-1 accuracy consistently across a wide spectrum of models and configurations, and  establishes  the new SOTA on ImageNet. }
\vspace{-2mm}
\label{tab:rfc}
\footnotesize
\begin{center}
\small
\begin{tabular}{l | c c  | c c  }
\toprule
 Model  & Rand Crop. Ratio &  Top-1 (\%)  & RFC Ratio & Top-1 (\%) \\ [0.5ex] 
\midrule
EfficientNet-l2-ns-800 \cite{tan2019efficientnet} & 0.960 & 88.35 & 1.120 & 88.46  \\ 
EfficientNet-b7-ns-475 \cite{tan2019efficientnet} & 0.936 & 88.23 & 1.020 & 88.33  \\ 
Swin-large-patch4-w12-384 \cite{liu2021swin}  & 1.000 & 87.15 & 1.100 & 87.18  \\ 
CaiT-m48-448 \cite{gong2021improve} & 1.000 & 86.48 & 1.130 & 86.56  \\ 
\midrule
Refined-ViT-448 & 1.000 & 85.94 & 1.130 & 85.98  \\
\bottomrule

\end{tabular}

\end{center}
\vspace{-2mm}
\end{table*}

\subsection{Applied  to NLP tasks}
We also evaluate the performance of Refiner-ViT models for natural language processing tasks on the GLUE benchmark, to investigate whether Refiner also improves other transformer-based models. We use the BERT-small~\cite{clark2020electra} as the baseline model and replace the self-attention module with refiner, using the same pre-training dataset and recipes. From the results in Tab.~\ref{tab:gluetest}, Refiner boosts the model performance across all the tasks significantly and increases the average score by 1\%, demonstrating Refiner is well generalizable to transformer-based NLP models to improve their attentions and final performance. 

\begin{table*}[h]
\caption{\small Comparison of BERT-small w/o and w/ refiner   on the GLUE development set. }
\vspace{-2mm}
\label{tab:gluetest}
\footnotesize
\begin{center}
\small
\begin{tabular}{l c c c c c c c c c c c}
\toprule
 Model  & Params & MNLI & QNLI & QQP & RTE & SST & MRPC & CoLA & STS-B & Avg.\\ [0.5ex] 
\midrule
BERT-small \cite{clark2020electra}  & 14M & 75.8 & 83.7 & 86.8 & 57.4 & 88.4 & 83.8 & 41.6 & 83.6 & 75.1 \\ 
\quad + Refiner   & 14M & \textbf{78.1} & \textbf{86.4} & \textbf{88.0} & \textbf{57.6} & \textbf{88.8} & \textbf{84.1} & \textbf{42.2} & \textbf{84.0} & \textbf{76.1}\\
\bottomrule

\end{tabular}

\end{center}
\vspace{-2mm}
\end{table*}

\begin{table}[h]
    \centering
    \caption{\small Top-1 accuracy comparison with other methods on ImageNet \cite{deng2009imagenet}
    and ImageNet Real~\cite{beyer2020we}. All models are trained without external data. 
    With the same computation and parameter constraint, Refined-ViT consistently outperforms
    other SOTA CNN-based and ViT-based models. The results of CNNs and ViT are adopted from~\cite{touvron2021going}.}
    \label{tab:sota}
    \def \mysp {\hspace{7pt}}
    \centering \scalebox{1.0}
    {\small 
    \begin{tabular}{@{\ }lccccccc}
    \toprule
    Network  & Params & FLOPs & Train size & Test size  &  Top-1(\%)  & Real Top-1 (\%) \\
    \toprule
    \multicolumn{7}{c}{CNNs}\\
    \midrule

    EfficientNet-B5~\cite{tan2019efficientnet}    & \pzo30M & \dzo9.9B  &  &   & 83.6 & 88.3  \\
    EfficientNet-B7~\cite{tan2019efficientnet}    & \pzo66M & \pzo37.0B &  &   & 84.3 & \_      \\
    Fix-EfficientNet-B8~\cite{tan2019efficientnet, touvron2019fixing} & \pzo87M & \pzo89.5B &  &   & 85.7 & 90.0  \\
    \midrule
    NFNet-F0~\cite{brock2021high}           & \pzo72M & \pzo12.4B &  &  & 83.6 & 88.1  \\
    NFNet-F1~\cite{brock2021high}           & 133M    & \pzo35.5B &  &   & 84.7 & 88.9  \\
\toprule
    \multicolumn{7}{c}{Transformers}\\
    \midrule
    ViT-B/16~\cite{dosovitskiy2020image}           & \pzo86M & \pzo55.4B &  &  & 77.9 & 83.6 \\
    ViT-L/16~\cite{dosovitskiy2020image}           & 307M    & 190.7B    &  &   & 76.5 & 82.2 \\
    \midrule
    T2T-ViT-14~\cite{yuan2021tokens}       & \pzo22M & \dzo5.2B  &  &         & 81.5 & \_  \\
    T2T-ViT-14384~\cite{yuan2021tokens} & \pzo22M & \pzo 17.1B  &  &         & 83.3 & \_ \\
    \midrule
    CrossViT~\cite{chen2021crossvit}           & \pzo45M & \pzo56.6B &  &         & 84.1 & \_ \\
    Swin-B~\cite{liu2021swin}             & \pzo88M & \pzo47.0B   &  &         & 84.2 & \_ \\
    TNT-B~\cite{han2021transformer}              & \pzo66M & \pzo14.1B &  &        & 82.8 & \_   \\
    \midrule
    DeepViT-S~\cite{zhou2021deepvit}          & \pzo27M & \dzo6.2B &  &        & 82.3 & \_   \\
    DeepViT-L~\cite{zhou2021deepvit}          & \pzo55M & \pzo12.5B &  &        & 83.1 & \_   \\         
    \midrule
    DeiT-S~\cite{touvron2020training}             & \pzo22M & \dzo4.6B  &  &   & 79.9  & 85.7  \\
    DeiT-B~\cite{touvron2020training}              & \pzo86M & \pzo17.5B &  &  &  81.8 &   86.7 \\
    DeiT-B384~\cite{touvron2020training}          & \pzo86M & \pzo55.4B &   &   &  83.1 & 87.7 \\
    \midrule
    CaiT-S36384~\cite{touvron2021going}  & \pzo68M & \pzo48.0B &   &   &  85.4 & 89.8 \\
    CaiT-M36~\cite{touvron2021going} & 271M & \pzo53.7B &   &   &  85.1 & 89.3 \\
    \midrule
    LV-ViT-S~\cite{jiang2021token} & \pzo26M & \dzo6.6B &   &    &  83.3 & 88.1 \\
    LV-ViT-M~\cite{jiang2021token} & \pzo56M & \pzo16.0B &   &    &  84.0 & 88.4 \\
    \toprule
    
    \multicolumn{7}{c}{Transformer with locality} \\
    \midrule
    LocalViT-S~\cite{li2021localvit} & 22.4M & 4.6B & 224 & 224 & 80.8 & - \\
    LocalViT-PVT~\cite{li2021localvit} & 13.5M & 4.8B & 224 & 224 & 78.2 & - \\
    \midrule
    ConViT-S~\cite{d2021convit} & 27M & 5.4B & 224 & 224 & 81.3 & - \\
    ConViT-S+~\cite{d2021convit} & 48M & 10.0B & 224 & 224 & 82.2 & - \\
    \midrule
    BoTNet-S1-59~\cite{srinivas2021bottleneck} & 33.5M & \dzo7.3B & 224 & 224 & 81.7 & -\\
    BoTNet-S1-110~\cite{srinivas2021bottleneck} & 54.7M & \pzo10.9B & 224 & 224 & 82.8 & - \\
    BoTNet-S1-128~\cite{srinivas2021bottleneck} & 79.1M & \pzo19.3B & 256 & 256 & 84.2 & - \\
    BoTNet-S1-128384~\cite{srinivas2021bottleneck} & 79.1M & \pzo45.8B & 256 & 384 & 84.7 & - \\ \toprule
    \multicolumn{7}{c}{Our \OURS} \\
    \midrule
    \OURS-S & \pzo25M & 7.2B &   &    &  83.6 & 88.3 \\
    \OURS-S384 & \pzo25M & 24.5B &   &    & 84.6  &  88.9 \\
    \OURS-M & \pzo55M & 13.5B &   &    & 84.6  & 88.9 \\
    \OURS-M384  & \pzo55M & 49.2B &   &   &  85.6 &  89.3 \\
    \OURS-L & \pzo81M & 19.1B &   &   &  84.9 & 89.1 \\
    \OURS-L384 & \pzo81M & 69.1B &   &   &  \textbf{85.7} &  \textbf{89.7} \\
    \OURS-L448 & \pzo81M & 98.0B &   &   &  \textbf{85.9} &  \textbf{90.1} \\
    \bottomrule
    \end{tabular}}
\end{table}
