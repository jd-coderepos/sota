\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{style}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{bm}
\usepackage{enumitem}
\usepackage{bbding}
\usepackage{tablefootnote}
\usepackage[misc]{ifsym}


\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage[table]{xcolor}
\usepackage{adjustbox}
\usepackage{multirow}
\def\eg{\textit{e.g.,~}}
\def\ie{\textit{i.e.,~}}
\def\comp{\ensuremath\mathop{\scalebox{.6}{}}}
\newcommand\smallalign[1]{\begingroup\small
    \setlength{\abovedisplayskip}{0.7em}
    \setlength{\belowdisplayskip}{0.7em}
    \setlength{\abovedisplayshortskip}{0.7em}
    \setlength{\belowdisplayshortskip}{0.7em}
    {#1}\endgroup}
    
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\name{M3I Pre-training}

\newcommand{\newunderbrace}[2]{\begingroup
      \color{blue}
      \underbrace{\color{black}#1}_{\text{#2}}
      \endgroup}
\definecolor{mygray}{gray}{0.3}
\newcommand{\demph}[1]{\textcolor{mygray}{#1}}

\newcommand{\EXPTODO}[1]{\textcolor{red}{{#1}}}
\newcommand{\EXPRUNNING}[1]{\textcolor{green}{{#1}}}
\newcommand{\TODO}[1]{\textcolor{blue}{{[TODO: #1]}}}
\newcommand{\RED}[1]{\textcolor{red}{{#1}}}
\definecolor{defaultcolor}{gray}{.87}
\newcommand{\default}[1]{\textbf{#1}}

\newcommand\blfootnote[1]{\begingroup
\renewcommand\thefootnote{}\footnote{#1}\addtocounter{footnote}{-1}\endgroup
}


\begin{document}

\title{Towards All-in-one Pre-training via Maximizing \\ Multi-modal Mutual Information}


\author{
  Weijie Su,
  Xizhou Zhu\textsuperscript{\Letter},
  Chenxin Tao,
  Lewei Lu,
  Bin Li,
  Gao Huang,\\
  Yu Qiao, 
  Xiaogang Wang,
  Jie Zhou,
  Jifeng Dai\\
University of Science and Technology of China \ \ \
SenseTime Research \ \ \
Tsinghua University\\  
Shanghai Artificial Intelligence Laboratory \ \ \
The Chinese University of Hong Kong\\
{\tt\small jackroos@mail.ustc.edu.cn, \{zhuwalter,luotto\}@sensetime.com}\\
{\tt\small tcx20@mails.tsinghua.edu.cn, binli@ustc.edu.cn, \{gaohuang,jzhou,daijifeng\}@tsinghua.edu.cn}\\
{\tt\small qiaoyu@pjlab.org.cn, xgwang@ee.cuhk.edu.hk}}

\maketitle

\begin{abstract}
  \vspace{-0.5em}
  To effectively exploit the potential of large-scale models, various pre-training strategies supported by massive data from different sources are proposed, including supervised pre-training, weakly-supervised pre-training, and self-supervised pre-training. It has been proved that combining multiple pre-training strategies and data from various modalities/sources can greatly boost the training of large-scale models. However, current works adopt a multi-stage pre-training system, where the complex pipeline may increase the uncertainty and instability of the pre-training. It is thus desirable that these strategies can be integrated in a single-stage manner. In this paper, we first propose a general multi-modal mutual information formula as a unified optimization target and demonstrate that all existing approaches are special cases of our framework. Under this unified perspective, we propose an all-in-one single-stage pre-training approach, named \textbf{M}aximizing \textbf{M}ulti-modal \textbf{M}utual \textbf{I}nformation Pre-training (\textbf{\name{}}). Our approach achieves better performance than previous pre-training methods on various vision benchmarks, including ImageNet classification, COCO object detection, LVIS long-tailed object detection, and ADE20k semantic segmentation. Notably, we successfully pre-train a billion-level parameter image backbone and achieve state-of-the-art performance on various benchmarks. Code shall be released at \url{https://github.com/OpenGVLab/M3I-Pretraining}.
\end{abstract} \blfootnote{ Equal contribution. This work is done when Weijie Su and Chenxin Tao are interns at Shanghai Artificial Intelligence Laboratory. \Letter\ Corresponding author.}
\vspace{-1.5em}
\section{Introduction}
\label{sec:intro}
\vspace{-0.5em}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/fig1-comparison.pdf}
    \vspace{-2.0em}
    \caption{Comparison between different pre-training paradigms and \name{}. Existing pre-training methods are all optimizing the mutual information between the input and target representations, which can be integrated by \name{}.}
    \label{fig:comparison}
    \vspace{-1.5em}
\end{figure}


In recent years, large-scale pre-trained models~\cite{tan2019efficientnet,radford2021learning,jia2021scaling,bao2021beit,he2022masked,chen2020simple,grill2020bootstrap,zbontar2021barlow} have swept a variety of computer vision tasks with their strong performance.
To adequately train large models with billions of parameters, researchers design various annotation-free self-training tasks and obtain sufficiently large amounts of data from various modalities and sources.
In general, existing large-scale pre-training strategies are mainly divided into three types: supervised learning~\cite{tan2019efficientnet,dai2021coatnet} on pseudo-labeled data (\eg JFT-300M~\cite{xie2020self}), weakly supervised learning~\cite{radford2021learning,jia2021scaling} on web crawling images text pairs (\eg LAION-400M~\cite{schuhmann2021laion}), and self-supervised learning~\cite{chen2020simple,grill2020bootstrap,zbontar2021barlow,bao2021beit,he2022masked} on  unlabeled images. Supported by massive data, all these strategies have their own advantages and have been proven to be effective for large models of different tasks.
In pursuit of stronger representations of large models, some recent approaches~\cite{wang2022image,liu2022swin,wei2022contrastive} combine the advantages of these strategies by directly using different proxy tasks at different stages, significantly pushing the performance boundaries of various vision tasks.

Nevertheless,
the pipeline of these multi-stage pre-training approaches is complex and fragile, which may lead to uncertainty and catastrophic forgetting issues.
Specifically, the final performance is only available after completing the entire multi-stage pre-training pipeline. Due to the lack of effective training monitors in the intermediate stages, it is difficult to locate the problematic training stage when the final performance is poor.
To eliminate this dilemma, it is urgent to develop a single-stage pre-training framework that can take advantage of various supervision signals.
It is natural to raise the following question: \textit{Is it possible to design an all-in-one pre-training method to have all the desired representational properties? }

To this end, we first point out that different single-stage pre-training methods share a unified design principle through a generic pre-training theoretical framework. We further extend this framework to a multi-input multi-target setting so that different pre-training methods can be integrated systematically. In this way, we propose a novel single-stage pre-training method, termed \name{}, that all desired representational properties are combined in a unified framework and trained together in a single stage.



Specifically, we first introduce a generic pre-training theoretical framework that can be instantiated to cover existing mainstream pre-training methods. 
This framework aims to maximize the mutual information between input representation
and target representation,
which can be further derived into a prediction term with a regularization term. 
(1) The prediction term reconstructs training targets 
from the network inputs, which is equivalent to existing well-known pre-training losses by choosing proper forms for the predicted distribution.
(2) The regularization term requires the distribution of the target 
to maintain high entropy to prevent collapse, which is usually implemented implicitly through negative samples or stop-gradient operation. As shown in Fig.~\ref{fig:comparison}, by adopting different forms of input-target paired data
and their representations,
our framework can include existing pre-training approaches and provide possible directions to design an all-in-one pre-training method.

To meet the requirement of large-scale pre-training with various data sources, we further extend our framework to the multi-input multi-target setting, with which we show that multi-task pre-training methods are optimizing a lower bound of the mutual information. 
In addition, we mix two masked views from two different images as the input. The representation of one image is used to reconstruct the same view, while the other image is used to reconstruct a different augmented view. Both representations will predict their corresponding annotated category or paired texts.
In this way, we propose a novel pre-training approach, called \name{}, which can effectively combine the merits of supervised/weakly-supervised/self-supervised pre-training and enables large-scale vision foundation models to benefit from multi-modal/source large-scale data. Our contributions can be summarized as follows:
\begin{itemize}[leftmargin=1em]
    \vspace{-0.5em}
    \item We theoretically demonstrate all existing mainstream pre-training methods share a common optimization objective, \ie maximizing the mutual information between input and target representation. We also show how to instantiate our framework as distinct pre-training methods.    
    \item We propose a novel single-stage pre-training approach called \name\ to gather the benefit of various pre-training supervision signals, via extending our mutual information pre-training framework to a multi-input multi-target setting.
    \item Comprehensive experiments demonstrate the effectiveness of our approach. We successfully pre-train InternImage-H\cite{anonymous2022internimg}, a model with billion-level parameters, and set a new record on basic detection and segmentation tasks, \ie 65.4 box AP on COCO test-dev~\cite{lin2014microsoft}, 62.9 mIoU on ADE20K~\cite{zhou2019semantic}.
\end{itemize}

 \section{Related Work}
\label{sec:related}

\noindent\textbf{Supervised Pre-training (SP)} has been the mainstream over a long period of time~\cite{girshick2014rich,he2016deep,chen2017deeplab,dosovitskiy2020image,carion2020end,liu2021swin,liu2022convnet}. Most works adopt image classification on ImageNet~\cite{deng2009imagenet} as the pre-training task, for both ConvNets~\cite{he2016deep,xie2017aggregated,tan2019efficientnet,liu2022convnet} and Transformers~\cite{dosovitskiy2020image,touvron2021training,liu2021swin}. SP has benefited many downstream tasks, including object detection~\cite{girshick2014rich,carion2020end}, semantic semantation~\cite{chen2017deeplab,xiao2018unified}, and video recognition~\cite{bertasius2021space}. Some works have also explored the scaling properties of pre-training datasets~\cite{dosovitskiy2020image,zhai2022scaling} and image backbones~\cite{zhai2022scaling, abnar2021exploring}. Moreover, SP shows that mixing two inputs~\cite{zhang2017mixup,yun2019cutmix} is critical for improving accuracy~\cite{touvron2021training,Touvron2022DeiTIR}, which is rarely explored in other pre-training paradigms. Our proposed framework includes SP as a special case. \name{} can thus preserve the advantage of it in an all-in-one pre-training and surpass its performances on downstream tasks.

\vspace{0.5em}\noindent\textbf{Self-supervised Pre-training (SSP)} becomes popular in recent years~\cite{jaiswal2020survey,zhang2022survey}. It does not require annotations, and thus enables the usage of large-scale unlabeled data. SSP can be divided into two kinds of methods: Intra-view tasks create input and target from the same view, which includes auto-encoder~\cite{hinton2006reducing,vincent2008extracting}, global/dense distillation~\cite{hinton2015distilling,wei2022contrastive} and masked image modeling (MIM)~\cite{bao2021beit,he2022masked,baevski2022data2vec,chen2022context,xie2022simmim}. On the contrary, inter-view tasks adopt different augmented views as input and target, such as dense/global instance discrimination~\cite{wu2018unsupervised, oord2018representation} and siamese image modeling~\cite{anonymous2022siamese}. 
Instance discrimination contains several sub-frameworks, including contrastive learning~\cite{he2020momentum, chen2020simple}, asymmetric networks~\cite{grill2020bootstrap,chen2021exploring} and feature decorrelation~\cite{zbontar2021barlow,bardes2021vicreg}, which are found of similar mechanism~\cite{tian2021understanding,tao2022exploring}.
Some SSP methods have displayed great potential by surpassing SP on downstream tasks by a large margin~\cite{he2020momentum,chen2020simple,he2022masked}. MIM has also been proven to work well with large-scale networks~\cite{he2022masked}. SiameseIM~\cite{anonymous2022siamese} is a inter-view SSP method that better combines semantic alignment with spatial sensitivity. Our method covers different self-supervised pre-training methods in a general framework and seeks to find the most effective setting through extensive experiments. It can combine all the strengths and shows impressive performances.

\vspace{0.5em}\noindent\textbf{Weakly-supervised Pre-training (WSP)} utilizes image-text datasets~\cite{sharma2018conceptual,changpinyo2021conceptual,thomee2016yfcc100m,schuhmann2021laion} or image-hashtag datasets~\cite{mahajan2018exploring,veit2018separating,singh2022revisiting}. These pre-training methods rely on noisy supervision from the Internet and are thus scalable.
For image-hashtag datasets, some works~\cite{mahajan2018exploring,singh2022revisiting} show competitive performances in various transfer-learning settings.
For image-text datasets, earlier works~\cite{Su2020VL-BERT:,lu2019vilbert,chen2020uniter,sun2019videobert,sun2019learning,tan2019lxmert,li2020unicoder,alberti2019fusion,li2019visualbert} mainly focused on learning general representations for visual-linguistic understanding. Recently CLIP~\cite{radford2021learning} and ALIGN\cite{jia2021scaling} demonstrated the effectiveness of image-text pre-training in image recognition. They propose to learn the aligned visual-linguistic representations and achieve impressive image classification accuracy in a zero-shot manner. In our framework, WSP is shown to be a special instance. \name{} leverages the power of WSP to achieve a new height on various downstream tasks.

\vspace{0.5em}\noindent\textbf{Multi-task Pre-training} adopts multiple targets for the same input. This kind of method usually combines text target from WSP and image target from SSP~\cite{yu2022coca,singh2022flava,dong2022maskclip,mu2022slip}. Some works have also explored using both category targets from SP and image targets from SSP~\cite{khosla2020supervised,liang2022supmae}. 
Multi-task pre-training fits well into our framework with the multi-input multi-target extension. Compared with previous works, our method can be successfully applied to large-scale models and displays superior results.

\vspace{0.5em}\noindent\textbf{Multi-stage Pre-training} instead adopts stage-wise pre-training, which focuses on one pre-training target in each stage and reuses the model in the next stage~\cite{peng2022beit,wang2022image,wei2022contrastive}. Multi-stage pre-training also follows the mutual information objective in each stage. However, multi-stage pre-training suffers from a complex pipeline that may increase uncertainty and instability. On the contrary, \name{} combines different supervision signals in a single stage that avoids the problems of multi-stage pre-training.
 \section{Method}
\label{sec:theory}
\subsection{Mutual Information for Generic Pre-training}

\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.05}
\begin{table*}[t]
    \centering
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{lccccccc}
    \toprule
    Pre-training Method & \makecell[c]{Typical Work} & \makecell[c]{Input\\ Data } & \makecell[c]{Target\\ Data } & \makecell[c]{Input\\ Representation } & \makecell[c]{Target\\ Representation } & \makecell[c]{Regularization \\ }  &  \makecell[c]{Distribution \\ Form } \\
    \midrule
    \multicolumn{7}{l}{\textit{\demph{Supervised Pre-training :}}} \\
    Image Classification & ViT~\cite{dosovitskiy2020image} & view1 & category & dense feature & category embedding & negative categories & Boltzmann \\
    \midrule
    \multicolumn{7}{l}{\textit{\demph{Weakly-supervised Pre-training :}}} \\
    \makecell[l]{Contrastive Language-\\Image Pre-training} & CLIP~\cite{radford2021learning} & view1 & text & dense feature & text embedding & negative texts & Boltzmann \\
    \midrule
    \multicolumn{7}{l}{\textit{\demph{Self-supervised Pre-training (intra-view) :}}} \\
    Auto-Encoder & - & view1 & view1 & dense feature & dense pixels & - & Gaussian \\
    \RED{}Dense Distillation & FD\cite{wei2022contrastive},BEiT v2 tokenizer\cite{peng2022beit} & view1 & view1 & dense feature & dense feature & stop gradient & Gaussian \\
    Global Distillation & - & view1 & view1 & dense feature & global feature & stop gradient & Boltzmann \\
     & MAE~\cite{he2022masked} & masked view1 & view1 & dense feature & dense pixels & - & Gaussian\vspace{0.3em}\\
     & \makecell{data2vec\cite{baevski2022data2vec},MILAN\cite{MILAN2022},\\BEiT\cite{bao2021beit},BEiT v2\cite{peng2022beit}} & masked view1 & view1 & dense feature & dense feature & stop gradient & Gaussian\vspace{0.3em}\\
     & - & masked view1 & view1 & dense feature & global feature & stop gradient & Gaussian \\
    \midrule
    \multicolumn{7}{l}{\textit{\demph{Self-supervised Pre-training (inter-view) :}}} \\
    Novel View Synthesis & - & view2 & view1 & dense feature & dense pixels & - & Gaussian \\
    Dense Instance Discrimination & DenseCL~\cite{wang2021dense} & view2 & view1 & dense feature & dense feature & negative samples & Boltzmann\vspace{0.3em}\\
    \RED{}Instance Discrimination & \makecell[c]{MoCo\cite{he2020momentum},BYOL\cite{grill2020bootstrap},\\Barlow Twins~\cite{zbontar2021barlow}} & view 2 & view1 & dense feature & global feature & \makecell[c]{negative samples / stop\\ gradient / decorrelation} & \makecell[c]{Boltzmann \\ / Gaussian}\vspace{0.3em}\\
     & - & masked view2 & view1 & dense feature & dense pixels & - & Gaussian \\
     & SiameseIM~\cite{anonymous2022siamese} & masked view2 & view1 & dense feature & dense feature & stop gradient & Gaussian \\
     & MSN~\cite{assran2022masked} & masked view2 & view1 & dense feature & global feature & \makecell[c]{negative samples} & Boltzmann \\
    \bottomrule
    \end{tabular}}
    \vspace{-0.5em}
    \caption{Instances of our mutual information based pre-training framework. Methods that do not have a listed typical work have rarely been explored before as a pre-training method. We only include single-input single-target pre-training methods in this table. 
    \RED{}Input representation of Dense Distillation can be continuous (FD) or discrete (BEiT v2 tokenizer).
    \RED{}Target encoder of  can be momentum encoder (data2vec), pre-trained image encoder (MILAN), dVAE (BEiT), or discrete tokenizer distilled from pre-trained image encoder (BEiT v2). 
    \RED{}Regularization term of Instance Discrimination can be negative samples (MoCo), stop-gradient (BYOL), or decorrelation (Barlow Twins).}
    \vspace{-1.0em}
    \label{tab:comparison}
\end{table*}
 
The goal of pre-training is to learn representations that can well represent the training samples.
Suppose the training sample is , where  could be image-category pair (supervised), image-text pair (weakly-supervised), or image only (self-supervised). The input training data  and target training data  are extracted from  by some transform operations , \ie . The transform operations  is typically data-irrelevant, \eg ``apply image augmentation'', ``get annotated category'' or ``get paired text''. In vision-centric pre-training, the input data  is usually an augmented image, while the target data  is either the annotated category, the paired text, or also an augmented image.  and  denote the encoded representation for input and target, respectively. Then, the desired pre-training objective can be described as maximizing the conditional mutual information between  and  given  and  as (see Appendix for derivation):
\smallalign{}\vspace{-1em}
\smallalign{}where the first term requires high entropy of the target representation, which avoids collapse. The second term requires the posterior distribution  to be close to the target distribution . 

In practice, deterministic neural networks are used for encoding the input representation  and target representation . On the other hand, the posterior distribution  is usually intractable. To alleviate this issue, a common practice is introducing another parameterized distribution  as an approximation, Then, Eq.~\eqref{equ:mutual_info} becomes (see Appendix for derivation):
\smallalign{}\vspace{-2em}
\smallalign{}where  is the approximated posterior distribution of  given the prediction of . When  is continuous and deterministic given , the regularization term becomes intractable~\cite{belghazi2018mine,tishby2000information}. Different mechanisms would be incorporated to avoid representation collapse (see Sec~\ref{sec:conn}). Then, to maximize the mutual information in Eq.~\eqref{equ:mutual_simple}, the training loss can be derived as:
\smallalign{}Different form of  results in different loss, \eg Gaussian and Boltzmann distributions corresponding to L2-norm and Softmax cross-entropy losses, respectively:
\smallalign{}where  and  are the hyper-parameters of Gaussian and Boltzmann distributions, respectively.  is a constant that can be ignored.  iterates over all possible target representations .

Eq.~\eqref{equ:loss} is a generic pre-training loss that can be instantiated into different pre-training paradigms, including supervised, weakly-supervised, and self-supervised pre-training. 
Tab.~\ref{tab:comparison} demonstrate the actual implementation of different pre-training methods. Different methods incorporate different mechanisms to avoid representation collapse. 

\subsection{Connection with Existing Pre-training Methods}
\label{sec:conn}

\noindent\textbf{Supervised Pre-training (SP)} usually adopts \textit{Image Classification (IC)} as the pre-training task. It takes an augmented image  as input data and the corresponding annotated category  as the target data. The input representation is , while the target representation is the category embedding (\eg linear classification weight) . The classifier predicts the category based on  as . Thus, the pre-training objective is to maximize , and the SP loss can be derived as minimizing .
\smallalign{}where  is typically Boltzmann distribution (\ie Softmax cross-entropy loss). This distribution contains negative categories and naturally prevents collapse.
As a mainstream pre-training framework, SP has been proven to be helpful on many downstream tasks over a long period of time~\cite{girshick2014rich,carion2020end,chen2017deeplab,xiao2018unified}. It learns from clean human-annotated data. This helps the model to develop common semantics and converge faster on downstream tasks.

\vspace{0.5em}\noindent\textbf{Weakly-supervised Pre-training (WSP)} usually adopts \textit{Contrastive Language-Image Pre-training (CLIP)}~\cite{radford2021learning,jia2021scaling} as the pre-training task. It takes an augmented image  as input, and the corresponding paired text  as targets. Similar to supervised learning, the pre-training objective is
\smallalign{}where  is also Boltzmann distribution, which contains negative samples to prevent the collapse. WSP is able to exploit the massive image-text pairs from the Internet. With the help of image-text alignment, it not only enables many possible new tasks, \eg open-vocabulary recognition~\cite{gu2021open,radford2021learning}, but also greatly boosts the performances of classification and detection tasks in long-tail scenario~\cite{tian2022vl}.

\vspace{0.5em}\noindent\textbf{Self-supervised Pre-training (SSP)} learns representation using images only. Given a sampled training image , the input data is an augmented view of this image , the target data is another augmented view . The pre-training objective is derived from Eq.~\eqref{equ:loss} as
\smallalign{}where  and  are the input and target augmentations on the sampled image, respectively. Depending on different methods, the target encoder  could be identity, shared with  or the Exponential Moving Average (EMA) of .
 is usually Boltzmann or Gaussian distribution. When  is Boltzmann (\ie Softmax cross-entropy loss), it aims to differentiate  from different training data. When  is Gaussian (\ie L2-norm loss), it fits the value of . To prevent collapse, ``stop-gradient''~\cite{grill2020bootstrap}, feature-decorrelation~\cite{zbontar2021barlow} and negative samples~\cite{chen2020simple} are considered.

As Tab.~\ref{tab:comparison} illustrated, different choices of data transform operations  and target representation type  result in different pre-training tasks: (1) For , they could be either the the same view (\eg auto-encoder) or different views (\eg instance discrimination~\cite{chen2020simple,grill2020bootstrap,zbontar2021barlow}).  could also incorporate an additional mask operation (\eg masked image modeling~\cite{bao2021beit,he2022masked}). (2) For , its representation type could be from \{dense pixels, dense feature, global feature\}.

The advantage of SSP methods is that they can utilize large-scale unlabelled data, which facilitates the development of large models. Some SSP methods can already surpass SP on downstream tasks~\cite{he2020momentum,he2022masked,bao2021beit}. Notably, MIM~\cite{he2022masked,bao2021beit} demonstrates great dense localization ability, while SiameseIM~\cite{anonymous2022siamese} can exhibit semantic alignment and spatial sensitivity at the same time.

\begin{figure*}
    \centering
    \includegraphics[width=0.95\textwidth]{figures/fig2-overview.pdf}
    \vspace{-0.5em}
    \caption{Overview of \name{}. We mix two views from different images as the inputs. The first image needs to predict the same image view, while the other image needs to predict a different augmented view. Both images need to predict their annotated category or paired text. When predicting image targets, we add position embeddings to decoder inputs so that ``mask'' tokens can be matched with target patches. Following~\cite{anonymous2022siamese}, position embeddings are computed with respect to the left top origin of the input views.}
    \label{fig:my_label}
    \vspace{-1.0em}
\end{figure*}


\subsection{Multi-input Multi-target Pre-training}
Based on previous analysis, we can see that different pre-training tasks possess their own strengths. Naturally, we would like to maintain all these properties in one pre-training approach. 
For this purpose, we extend our framework to a multi-input multi-target setting.

Suppose the set of  multiple inputs and  multiple targets are  and , respectively. We use  and  to indicate the sets of transforms of inputs and targets. 

In practice, most methods choose to optimize the objectives of different types of targets separately. In this case, we can split the targets  into  non-overlapping groups and encode different groups independently as . With this modification, we show that the mutual information in Eq.~\eqref{equ:mutual_simple} can be bounded by (see Appendix for derivation):
\smallalign{}\vspace{-1em}
\smallalign{}where  is the group index,  is the number of targets in  group, and  is the approximated distribution for each target group.
Each prediction term corresponds to the objective of a target group. This implies that optimizing target objectives independently is equivalent to optimizing a lower bound of the mutual information.




\vspace{0.5em}\noindent\textbf{Multi-input Pre-training ()} uses multiple inputs with one target for each input ,

where  is the corresponding sampled target of .
multi-input pre-training is widely used in supervised pre-training (typically ), where different images are mixed through Mixup~\cite{zhang2017mixup} or CutMix~\cite{yun2019cutmix}. It has proven to be critical for improving accuracy and providing a more stable generalization ability. However, in other pre-training paradigms, a single input is usually adopted. The lack of multiple inputs may hinder better model performance, and also lead to inconsistency between different pre-training paradigms, which hampers the pre-training combination.

\vspace{0.5em}\noindent\textbf{Multi-target Pre-training ()} only uses multiple targets for the same input as .

Some previous works have explored the use of multiple targets~\cite{yu2022coca,singh2022flava,khosla2020supervised,liang2022supmae}. One line of research tries to combine weakly-supervised pre-training with specific forms of self-supervised pre-training, such as MaskCLIP~\cite{dong2022maskclip} and FLAVA~\cite{singh2022flava}. Another line studies the combination of supervised pre-training and self-supervised pre-training, such as SupCon~\cite{khosla2020supervised} or SupMAE~\cite{liang2022supmae}. These methods display the effectiveness of multiple targets.



\subsection{\name{}}
\label{subsec:theory_mix}
With the help of our mutual information framework, we are able to systematically integrate different pre-trainings into a whole, which we name as \name{}. 
It has two inputs and four targets, combining self-supervised and supervised / weakly-supervised pre-training as
\smallalign{}where  are the augmented input views of two different sampled images , and  are the corresponding augmented target views.  denotes the corresponding annotated category (supervised) or paired text (weakly-supervised) for each image.



\vspace{0.5em}\noindent\textbf{Input Encoder } first mixes the input views with a randomized binary mask  as ,

where  is the element-wise product,  shares the same shape as inputs. Then, the input representation is encoded by an image backbone (\eg ViT~\cite{dosovitskiy2020image}) as .
In order to make this mix strategy compatible with existing pre-training tasks like Masked Image Modeling (MIM) and Image Classification (IC), we split the mask  into patches with  size. All pixels in the same patch will be masked or unmasked together. For example,  is by default used for MIM~\cite{he2022masked, bao2021beit}. Note that the widely used Mixup~\cite{zhang2017mixup} and CutMix~\cite{yun2019cutmix} are generally incompatible with MIM.


\vspace{0.5em}\noindent\textbf{Target Encoder } is responsible for producing the target representations. 
For image targets , we use the momentum input image backbone as the encoder to generate dense target features. For category targets (supervised) or text targets (weakly-supervised) , we use a category embedding or text backbone that is jointly trained during pre-training. Notice that because of the multiple inputs, we can adopt both intra-view and inter-view self-supervised predictions: the first image  is asked to predict the same view (\ie ), and the other image  instead needs to predict a different augmented view (\ie ).

\vspace{0.5em}\noindent\textbf{Input-to-Target Decoder } predicts the target representations from the input. For simplicity, we use the separate loss form in Eq.~\eqref{equ:multi_mutual} to predict each target separately. We adopt Transformer~\cite{vaswani2017attention} layers to predict the dense representations for image targets, and an attention pooling layer~\cite{chen2022context} followed by a linear projection to predict the category embedding (supervised) or text embedding (weakly-supervised).

 \section{Experiment}
\label{sec:exp}
\noindent\textbf{Implementation Details.~}  We utilize InternImage-H~\cite{anonymous2022internimg} as image encoder in Sec~\ref{sec:exp_large_model} for large-scale model pre-training and ViT-B/16~\cite{dosovitskiy2020image} as that in other experiments for ablation study and fair comparison. For image-text dataset (\eg YFCC-15M~\cite{thomee2016yfcc100m}), a 12-layer Transformer (with the same network architecture as BERT-Base~\cite{devlin2018bert}) is utilized as text target encoder. For image classification dataset (\eg ImageNet-1k~\cite{deng2009imagenet}), we directly use the linear classifier weight as category embedding target. We employ 4-layer Transformer as decoder for image representation target, and Attention Pooling as that for category embedding or text global feature. Please see Appendix for detailed pre-training hyper-parameters.

\subsection{Pre-training of 1B Image Backbone}
\label{sec:exp_large_model}

\setlength{\tabcolsep}{3pt}
\renewcommand{\arraystretch}{1.2}
\begin{table*}[t]
    \centering
    \small
    \resizebox{1.0\linewidth}{!}{
        \begin{tabular}{cllllccccc}
        \Xhline{2\arrayrulewidth}
         Pre-training Approach & Model & Pipeline & Public Data & Private Data & \makecell{ImageNet\\val} & \makecell{COCO\\test-dev} & \makecell{LVIS\\minival}& \makecell{ADE20k\\val}\\
        \hline
        \multirow{2}{*}{\name{}} & \multirow{2}{*}{InternImage-H~\cite{anonymous2022internimg} (1B)} & \multirow{2}{*}{Single Stage: \name{}} & \multirow{2}{*}{\makecell[l]{427M image-text\\ 15M image-category}} & \multirow{2}{*}{\qquad-} &\multirow{2}{*}{89.2} & \multirow{2}{*}{\textbf{65.4}}  & \multirow{2}{*}{\textbf{62.5}}  &  \multirow{2}{*}{\textbf{62.9}} \\
        & \\
        \hline
        \multirow{2}{*}{\cite{liu2022swin}} & \multirow{2}{*}{SwinV2-G (3B)} & Stage 1: Masked Image Modeling & \multirow{2}{*}{\makecell[l]{15M image-category}} & \multirow{2}{*}{55M image-category} & \multirow{2}{*}{89.2} & \multirow{2}{*}{63.1} & \multirow{2}{*}{-} &  \multirow{2}{*}{59.9}  \\
        & & Stage 2: Image Classification \\
        \hline
        \multirow{3}{*}{\cite{wang2022image}} & \multirow{3}{*}{BEiT-3 (2B)} & Stage 1: CLIP & \multirow{3}{*}{\makecell[l]{21M image-text\\ 15M image-category}} & \multirow{3}{*}{400M image-text} & \multirow{3}{*}{\textbf{89.6}} & \multirow{3}{*}{63.7} & \multirow{3}{*}{-} & \multirow{3}{*}{62.8} \\
        & & Stage 2: Dense Distillation &  \\
        & & Stage 3: Masked Data Modeling & &  \\
        \hline
        \multirow{3}{*}{\cite{wei2022contrastive}} & \multirow{3}{*}{SwinV2-G (3B)} & Stage 1: Masked Image Modeling & \multirow{3}{*}{\makecell[l]{15M image-category}} & \multirow{3}{*}{55M image-category} & \multirow{3}{*}{89.4} & \multirow{3}{*}{64.2} & \multirow{3}{*}{-} & \multirow{3}{*}{61.4} \\
        & & Stage 2: Image Classification \\
        & & Stage 3: Dense Distillation \\
        \hline
        \multirow{2}{*}{previous best} & & & & & \multirow{2}{*}{89.1\RED{}} & \multirow{2}{*}{64.5\RED{}} & \multirow{2}{*}{59.8\RED{}} & \multirow{2}{*}{60.8\RED{}} \\
        \\
        \Xhline{2\arrayrulewidth}
        \end{tabular}
    }
    \vspace{-0.5em}
    \caption{Comparision of \name{} with existing large model pre-training methods for visual recognition. Top1 Accuracy, AP, AP, mIoU are reported on ImageNet validation set, COCO test-dev set, LVIS minival set (to avoid  data contamination following ~\cite{zhang2022glipv2}), and ADE20k validation set, respectively. We achieve state-of-the-art performance on object detection and semantic segmentation tasks. \name{} also demonstrate very competitive classification performance with only public datasets, while all other methods utilize large-scale private data (WIT-400M~\cite{radford2021learning} is used in ~\cite{wang2022image}, ImageNet-22k-ext~\cite{liu2022swin} is used in \cite{liu2022swin,wei2022contrastive}), which is strong correlated with the task of image classification. We also list previous best results on these tasks with only public training data for comparision. Results reference: \RED{}MOAT~\cite{yang2022moat}, \RED{}Group DETR v2~\cite{chen2022group}, \RED{}GLIPv2~\cite{zhang2022glipv2}, \RED{}Mask DINO~\cite{li2022mask}.} 
    \vspace{-1.0em}
    \label{tab:exp_sota}
\end{table*}
 
\vspace{0.5em}\noindent\textbf{Settings.~} We employ InternImage-H~\cite{anonymous2022internimg} (a ConvNet-based image backbone with 1B parameters) as image encoder. The network is pre-trained for 30 epochs on 427M public image-text pairs (LAION400M~\cite{schuhmann2021laion}, YFCC-15M~\cite{kalkowski2015real}, CC12M~\cite{changpinyo2021conceptual}) and 15M public image-category pairs (ImageNet-22k~\cite{deng2009imagenet}). We report the transfer performance on ImageNet~\cite{deng2009imagenet}, COCO~\cite{lin2014microsoft}, LVIS~\cite{gupta2019lvis}, and ADE20k~\cite{zhou2017scene} benchmarks.


\vspace{0.5em}\noindent\textbf{Results and Discussions.~} As shown in Tab.~\ref{tab:exp_sota}, all previous large model pre-training approaches adopt a complicated multi-stage training pipeline. Instead, our \name{} is a simple yet effective single-stage pre-training paradigm. It achieves state-of-the-art performance on COCO object detection, LVIS long-tailed object detection,  and ADE20k semantic segmentation. Very competitive performance is achieved on ImageNet classification. It validates the effectiveness of our approach. Besides, \name{} only employs pubic datasets and exhibits superior transfer performance while all other approaches include private datasets in their pre-training.

Different from SwinV2~\cite{liu2022swin}, BEiT-3~\cite{wang2022image} and FD~\cite{wei2022contrastive}, \name{} is an all-in-one single-stage training paradigm which brings the following advantages: 1) \textit{Simplicity.} \name{}\ could make use of all available supervision signals and training data in a single-stage pre-training. In contrast, both \cite{liu2022swin, wang2022image} incorporate redundant multi-stage pre-training pipelines. \cite{liu2022swin} uses the same training data in multiple pre-training stages but with different supervision signals. \cite{wang2022image} picks the pre-trained model in the previous pre-training stage as the target network for the next pre-training stage.  2) \textit{Avoiding Catastrophic Forgetting}. As shown in Tab.~\ref{tab:exp_sota}, \cite{liu2022swin,wang2022image,wei2022contrastive} all consist of multiple pre-training stages. The networks are expected to learn different representational attributes in different pre-training stages. However, due to the existence of catastrophic forgetting~\cite{french1999catastrophic}, attributes learned in the previous pre-training stage may be forgotten in the next pre-training stage.
Our \name{} naturally avoids the catastrophic forgetting issue by learning different representational attributes simultaneously in one-stage pre-training.



\setlength{\tabcolsep}{2pt}
\renewcommand{\arraystretch}{1.1}
\begin{table}[t]
    \centering
    \small
    \resizebox{1.0\linewidth}{!}{
        \begin{tabular}{clcccccc}
        \Xhline{2\arrayrulewidth}
        & Pre-training Method & \makecell[c]{Input\\ Data } & \makecell[c]{Target\\ Representation } & \makecell{ImageNet\\Top1} & \makecell{COCO\\AP} \\
        \hline
        \multicolumn{6}{l}{\textit{\demph{Self-supervised Pre-training (intra-view)}}} \\
        (a) & Auto-Encoder      & view1         & dense pixels         &  77.5 & 0.0 \\
        (b) & Dense Distillation & view1         & dense feature  & 78.8 & 32.4 \\
        (c) & Global Distillation         & view1           & global feature & 77.1 & 27.9\\
        (d) & Masked Image Modeling          & masked view1  & dense pixels   & 83.1 & 46.8 \\
        \default{(e)} & \default{Masked Image Modeling}           & \default{masked view1}  & \default{dense feature}  & \default{83.3} & \default{47.4} \\
        (f) & Masked Image Modeling & masked view1 & global feature & 83.2 & 47.5\\
        \hline
        \multicolumn{6}{l}{\textit{\demph{Self-supervised Pre-training (inter-view)}}} \\
        (g) & Novel View Synthesis                 & view2        & dense pixels     & 78.8 & 33.0 \\     
        (h) & Dense Instance Discrimination             & view2         & dense feature  & 83.2 & 50.1 \\
        (i) & Instance Discrimination                           & view2       & global feature & 83.0 & 46.4  \\
        (j) & Siamese Image Modeling    & masked view2  & dense pixels   & 78.9 & 38.1 \\
        \default{(k)} & \default{Siamese Image Modeling}     & \default{masked view2}  & \default{dense feature}  & \default{83.7} & \default{49.8}  \\
        (l) & Instance Discrimination   & masked view2  & global feature & 82.9 & 46.2 \\
        \Xhline{2\arrayrulewidth}
        \end{tabular}
    }
    \vspace{-0.5em}
    \caption{Ablation study on different self-supervised pre-training methods under our framework.  denotes no convergence in fine-tuning. }
    \vspace{-1em}
    \label{tab:exp_ablation_selfsup}
\end{table} 

\setlength{\tabcolsep}{4pt}
\begin{table}
    \centering
    \small
    \resizebox{1.0\linewidth}{!}{
        \begin{tabular}{lccccc}
        \Xhline{2\arrayrulewidth}
        \multirow{2}{*}{Pre-training Method} & \makecell{ImageNet} & \makecell{COCO} & \multicolumn{2}{c}{LVIS} & \makecell{ADE20k} \\
        & \makecell{Top1} & \makecell{AP} & AP & AP & \makecell{mIoU} \\
        \hline
        Image Classification              \textbf{}& 81.8 & 46.6 & 33.0 & 25.5 & 45.1 \\
        Best Intra-view SSP    & 83.3 & 47.4 & 31.2 & 21.9 & 40.1 \\
        Best Inter-view SSP   & 83.7 & 49.8 & 35.2 & 26.9 & 47.7\\
        \hline
        \multicolumn{6\textbf{}}{l}{\textit{\demph{Ours}}} \\
        \name{}~w/o mix  & 83.7 & 50.3 & 36.6 & 27.2 & 48.7  \\
        \default{\name{}} & \default{83.9} & \default{50.8} & \default{37.5} & \default{29.6} & \default{49.0} \\
        \Xhline{2\arrayrulewidth}
        \end{tabular}
    }
    \vspace{-0.5em}
    \caption{Ablation study of multi-input multi-target pre-training.}
    \vspace{-1em}
    \label{tab:exp_ablation_multi}
\end{table}

 
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.2}
\begin{table*}
    \centering
    \small
    \resizebox{1.0\linewidth}{!}{
        \begin{tabular}{lc|lllc|rc}
        \Xhline{2\arrayrulewidth}
        \multirow{2}{*}{Task}& \multirow{2}{*}{Metric} & & \multicolumn{2}{c}{ImageNet Pre-train} & & \multicolumn{2}{c}{YFCC Pre-train} \\
             &                           &\makecell[c]{SSP (intra-view)}&\makecell[c]{SSP (inter-view)}&\makecell[c]{SP} & M3I (ImageNet) &\makecell[c]{WSP} & M3I (YFCC) \\ 
        \hline
        ImageNet w/o Fine-tuning & Top1 acc.       & \makecell[c]{}  & \makecell[c]{} & \default{83.8} (DeiT-III) & 83.3 & 37.6 (CLIP) & 39.1~~ \\
        ImageNet Linear Classification & Top1 acc. & 79.5 (iBOT)             & 78.0 (SiameseIM)       & \default{83.8} (DeiT-III) & \default{83.8} & 66.5 (CLIP)     & 72.3 \\
        ImageNet Fine-tuning & Top1 acc.           & \default{84.2} (data2vec)         & 84.1 (SiameseIM)       & 83.8 (DeiT-III) & \default{84.2} &  80.5 (CLIP)    & 83.7 \\
        \hline                                                                                                                 
        COCO & AP                   & 51.6 (MAE)              & 52.1 (SiameseIM)       &  47.6 (Sup.)    & \default{52.2} & \makecell[c]{-} & 51.9 \\
        \hline                                                                                                                 
        \multirow{2}{*}{LVIS} &  AP & 40.1 (MAE)              & 40.5 (SiameseIM)       & 37.2 (Sup.)     & 40.6 & \makecell[c]{-} & \default{40.8}\\
        &  AP         & 38.1 (MAE)              & 38.1 (SiameseIM)       & \makecell[c]{-} & 38.2 & \makecell[c]{-} & \default{38.4} \\
        \hline                                                                                                                 
        ADE20k &  mIoU                             & 50.0 (iBOT)             & 51.1 (SiameseIM)       & 49.3 (DeiT-III) & \default{51.3} & \makecell[c]{-} & \default{51.3} \\
        \Xhline{2\arrayrulewidth}
        \end{tabular}
    }
    \vspace{-0.5em}
    \caption{System-level comparison with SoTA supervised, weakly-supervised, self-supervised pre-training methods (\ie DeiT-III~\cite{Touvron2022DeiTIR}, CLIP~\cite{radford2021learning}, data2vec~\cite{baevski2022data2vec}, MAE~\cite{he2022masked}, iBOT~\cite{zhou2021ibot}and SiameseIM~\cite{anonymous2022siamese}). The results of CLIP are obtained from \cite{mu2022slip} for a fair comparison on YFCC-15M. The results of ``Sup.'' (refers to supervised pre-training) are obtained from ~\cite{li2022exploring}. All methods adopt ViT-B/16 as the image backbone for a fair comparison. The fine-tuning schedule is 100 epochs for ImageNet, COCO, LVIS, and ADE20k.  denotes do not support.  ``ImageNet w/o Fine-tuning'' for weakly-supervised pre-training corresponds to the zero-shot transfer setting.}
    \vspace{-1.0em}
    \label{tab:exp_main}
\end{table*} 
\subsection{Ablation Study}
\label{sec:exp_ablation}


\vspace{0.5em}\noindent\textbf{Ablation Settings.~} We utilize ViT-B/16 as the image backbone for the ablation study. The pre-training schedule is set to 400 epochs on ImageNet-1k. Different pre-training methods are evaluated by the transfer performance on ImageNet-1k classification, COCO detection, LVIS detection, and ADE20k segmentation. The fine-tuning schedule is 100 epochs for ImageNet-1k. For other datasets, fine-tuning with 25 epochs is adopted.

\vspace{0.5em}\noindent\textbf{Ablation on Self-supervised Pre-training (SSP).~} As Tab.~\ref{tab:comparison} shows, the mutual information framework proposes 12 forms of SSP, some of which have not been explored as pre-training before. We compare these 12 types of SSP in Tab.~\ref{tab:exp_ablation_selfsup}. Based on the experiment results, We analyze three key factors in these approaches: 


\noindent\emph{1) Masked Input or Full Input.~} Masked input is critical for both intra-view and inter-view pre-training. The performances of Tab.~\ref{tab:exp_ablation_selfsup} (d-f,j-l) (masked) are always better or on par with Tab.~\ref{tab:exp_ablation_selfsup} (a-c,g-i)(full). The comparison for intra-view pre-training is consistent with previous studies~\cite{he2022masked}, implying that masking operation can greatly boost the model's performance. We observe that the gap for inter-view pre-training becomes smaller. The reason may be that predicting another view constitutes a more challenging task, and reduce the information redundancy to some extent.


\noindent\emph{2) Target Representation.~} The dense feature works best under almost all settings. Compared to the global feature target, the dense feature target enables the spatial discrimination ability of the network. Thus, as shown in Tab.~\ref{tab:exp_ablation_selfsup} (kl) and Tab.~\ref{tab:exp_ablation_selfsup} (hi), it achieves much better performance on COCO. On the other hand, compared to dense pixels, dense features represent the target in high-level semantic space and thus bring semantic alignment capacity. For example, Tab.~\ref{tab:exp_ablation_selfsup} (k) surpasses Tab.~\ref{tab:exp_ablation_selfsup} (j) by a large margin both in ImageNet (+4.8 points) and COCO (+11.7 points).

\noindent\emph{3) Intra-view or Inter-view.~}
The choice of intra-view or inter-view pre-training depends on whether the input data is masked or not. If full input is adopted, inter-view generally performs better than intra-view, as shown in Tab.~\ref{tab:exp_ablation_selfsup} (a-c, g-i). We conjecture that recovering the same view is too easy, and may not be suitable for pre-training.
On the other hand, if masked input is employed, both intra-view and inter-view can find a setting with good performance, \eg Tab.~\ref{tab:exp_ablation_selfsup} (ek). 



\vspace{0.5em}\noindent\textbf{Ablation on Multi-input Multi-target Pre-training.~} After we have determined the best training setting for SSP with intra-view and inter-view, we can now combine different pre-training methods into an all-in-one approach with our multi-input multi-target framework. Tab.~\ref{tab:exp_ablation_multi} demonstrates the comparison between \name{} and single-input single-target pre-training methods. Here we only consider pre-training on supervised pre-training for simplicity. 


We first compare multi-target pre-training, \ie{} \name{} w/o mix, with single-input single-target pre-training methods. It's shown that \name{} w/o mix can obtain superior or comparable results on all tasks, especially on LVIS (+1.4 points) and ADE20k (+1.0 points) benchmarks. We note that even though some pre-training methods may not perform well on some tasks, the combination is still effective to improve upon all single-input single-target methods. This is because different pre-training methods focus on different representational properties. For example, Image Classification pre-training brings better semantic information. This leads to high results on LVIS and ADE20k datasets, where long-tail classes pose high demand for semantic understanding. Intra-view SSP instead excels on spatial sensitivity and delivers good performance on the COCO dataset. \name{} w/o mix demonstrates the benefits of these methods. Our final \name{} further adopts multiple inputs to better combine these pre-training methods. Experiments show that it achieves better performances on all tasks.

\subsection{System-level Comparision with Other Methods}
\label{sec:exp_main}

We compare \name{} with previous methods using the same ViT-B/16~\cite{dosovitskiy2020image} image backbone in Tab.~\ref{tab:exp_main}. We pre-train our model for 1600 epochs and finetune it for 100 epochs on ImageNet~\cite{deng2009imagenet}, COCO~\cite{lin2014microsoft}, LVIS~\cite{gupta2019lvis} and ADE20k~\cite{zhou2017scene} datasets. We also report the results on ImageNet without finetuning and with the linear protocol. We further validate our method on YFCC-15M image-text dataset~\cite{kalkowski2015real}. For a fair comparison, the pre-training iterations are kept the same with ImageNet pre-training.

Tab.~\ref{tab:exp_main} shows that different pre-training methods possess different advantages. SP learns semantic alignment well and can already deliver good performance on ImageNet without further finetuning. WSP can enable zero-shot transfer learning, which can not be achieved through other pre-training methods. SSP presents better localization ability and is vital for dense prediction tasks. \name{} is able to achieve comparable results with the best of previous methods on all these tasks. This indicates that \name{} can maintain all these desired properties through a single-stage pre-training. 

 \section{Conclusion}
\label{sec:conclusion}
Modern large-scale networks rely on combining different pre-training methods to effectively utilize massive data, which suffers from the multi-stage pre-training practice. To derive a single-stage pre-training, we proposed a generic pre-training framework that unifies mainstream pre-training approaches. We further extended the framework to a multi-input multi-target setting, which shows that previous multi-task pre-training methods are actually optimizing a lower bound of the mutual information. Finally, we proposed an all-in-one pre-training method, \name{}. \name{} surpasses previous pre-training methods in various transfer-learning settings. 

\noindent\textbf{Limitations.} We focused on vision-centric pre-training. The proposed framework can be applied to other domains, like natural language processing or visual-linguistic tasks. We expect to explore other domains in future work.
 \vspace{-1.0em}
\paragraph{Acknowledgments} The work is partially supported by the National Natural Science Foundation of China under grants No.U19B2044, No.61836011 and No.62022048. 
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\newpage
\appendix
\section{Derivation for Mutual Information Framework}
This section describes the detailed derivation for our mutual information framework. For clarity, we list the notations in Tab.~\ref{tab:notation}.
\setlength{\tabcolsep}{2pt}
\renewcommand{\arraystretch}{1.05}
\begin{table*}[ht!]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{llrrr}
    \toprule
    \multirow{2}{*}{Notation} & \multirow{2}{*}{Meaning} & \multicolumn{3}{c}{Typical Choices in Vision-centric Pre-training Paradigms} \\
    \cmidrule{3-5}
     & & Supervised & Weakly-supervised & Self-supervised \\
    \midrule
     & \textbf{training sample} from the training dataset & image-category pair & image-text pair & image only \\
    \midrule
     & \textbf{input transform operation} applied to the sample  & apply image augmentation & apply image augmentation & apply image augmentation \\
     & \textbf{target transform operation} applied to the sample & get annotated category & get paired text & apply image augmentation\vspace{0.5em}\\
     & \textbf{the set of input transform operations} applied to the sample  & - & - & - \\
     & \textbf{the set of target transform operations} applied to the sample & - & - & - \\
    \midrule
     & \textbf{input data} for the network training & augmented image & augmented image & augmented image \\
     & \textbf{target data} for the network training & annotated category & paired text & augmented image\vspace{0.5em}\\
     & \textbf{multiple inputs} for the network training & - & - & - \\
     & \textbf{multiple targets} for the network training & - & - & - \\
     & \textbf{the  group of targets} & - & - & - \\
    \midrule
     & \textbf{input representation} from the input encoder  & image embedding & image embedding & image embedding \\
     & \textbf{target representation} from the target encoder  & category embedding & text embedding & image embedding \\
     & \textbf{target prediction} from the decoder  & predicted embedding & predicted embedding & predicted embedding\vspace{0.5em}\\
     & \textbf{input representation} from the input encoder  & - & - & - \\
     & \textbf{the  group target representation} from the target encoder  & - & - & - \\
     & \textbf{the  group target prediction} from the decoder  & - & - & - \\
    \midrule
     & \textbf{approximated target posterior} given the prediction  & Boltzmann & Boltzmann & Boltzmann~/~Gaussian\vspace{0.5em}\\
     & \textbf{approximated target posterior} given the prediction  & - & - & - \\
    \midrule
     & \textbf{regularization term} to avoid representation collapse of  & negative categories & negative texts & \makecell[r]{negative samples / stop \\gradient / decorrelation}\vspace{0.5em}\\
     & \textbf{regularization term} to avoid representation collapse of  & - & - & - \\
    \bottomrule
    \end{tabular}}
    \caption{Notation used in this paper. For single-input single-target pre-training, we also list the typical choices in different pre-training paradigm for each notation.}
    \label{tab:notation}
\end{table*} 
\subsection{Single-input Sinle-target Pre-training}
We start with the basic form of single-input single-target pre-training. The desired objective is to maximize the conditional mutual information between the input representation  and the target representation  given the input transform  and target transform :

According to the definition of conditional mutual information, we have

where the third equation holds because two representations are independent given the input and target, and in the last equation we apply the definitions of entropy and cross-entropy. Eq.~(\ref{equ:append_mutual_info}) shows that the mutual information can be divided into a prediction term and a regularization term. The prediction term requires the predicted distribution to be close to the target distribution, while the regularization term requires the target representations to maintain high entropy.

Next, we introduce parameterization to actually compute these terms. Two representations are encoded via an input encoder  and a target encoder , respectively. Because we do not know  in advance, we adopt an approximation by first predicting  and then estimating with the posterior distribution . 
The mutual information thus becomes

where the fourth inequality holds because KL Divergence will not be less than . In the fifth equality, we introduce training sample  to the expectation of the first term and move  and  from the expectation of the second term. In the last equality,  and  is moved out of the expectation because they should be deterministic once , ,  and model parameters are given. The right-hand side of Eq.~(\ref{equ:appendix_LB_mutual_info}) is a lower bound of the actual mutual information and will be equal to it if and only if the estimated distribution  matches the real distribution . We note that because  should be a deterministic feature given  during training, equality can be achieved when the decoder predicts the target representation precisely. So we have
\smallalign{}We usually deal with the regularization term in an implicit manner, such as introducing negative samples or stopping gradient to the target encoder. Therefore, the prediction term presents the loss function to be optimized in practice.


\subsection{Multi-input Multi-target Pre-training}
To derive the multi-input multi-target pre-training, we extend the input and the target to a set of  inputs  and  targets . The set of targets are split into  non-overlapping groups as . The input representations and target representations are  and , respectively. The mutual information is computed between  and  given the set of input transforms  and target transforms :

Similar to Eq.~(\ref{equ:append_mutual_info}), we can expand the mutual information as
\allowdisplaybreaks{
}where the fifth equality hold because the target representations are independent given targets, and  for .

During parameterization, we adopt different predictions  and distributions  for different target groups. Then the mutual information can be converted into
\allowdisplaybreaks{
}where the fourth inequality holds because KL Divergence will not be less than  for every summation term. The equality can be achieved if and only if every  matches .
Therefore, the mutual information for multi-input multi-target pre-training can be bounded by
\smallalign{}It's shown that different target groups are disentangled into a summation of prediction terms, so we can optimize each target objective independently.

\section{Experiment Details}


\subsection{Pre-training Settings}
\label{sec:appendix_pre}
We utilize InternImage-H as image encoder in Sec~\ref{sec:exp_large_model} for large model pre-training and ViT-B/16 as that in other experiments for ablation study and fair comparison. For image-text dataset (\eg YFCC-15M~\cite{thomee2016yfcc100m}), a 12-layer Transformer (with the same network architecture as BERT-Base~\cite{devlin2018bert}) is utilized as text target encoder. For image classification dataset (\eg ImageNet~\cite{deng2009imagenet}), we directly use the linear classifier weight as category embedding target. We employ 4-layer Transformer as decoder for image representation target, and Attention Pooling as that for category embedding or text global feature. Detailed hyper-parameters for pre-training InternImage-H and ViT-B are listed in Tab.~\ref{tab:hypers_pretrain}.

\vspace{0.5em}\noindent\textbf{Dynamic weighting} is used to balance the weights of self-supervised loss () and supervised/weakly-supervised loss (). The overall training loss can be expressed as

where  is the balance loss weight. Because the loss behavior changes dramatically during training, it's sub-optimal to set a static weight. We propose to set  dynamically according to the loss gradients. Specifically, we compute the exponential moving average of gradient norm that each loss back-propagates to input features, denoted as  and . Then  is set as , where  controls the gradient ratio between two loss terms. We find this strategy to work well in practice ( by default).

\begin{table}[h]
    \centering
    \small
    \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{lcc}
    \toprule
        Hyper-parameters & ViT-B/16 & InternImage-H\\
    \midrule
        Image-to-image decoder layers & \multicolumn{2}{c}{4} \\
        Image-to-image decoder hidden size & 768 & 1024 \\
        Image-to-image decoder FFN hidden size & 3072 & 4096 \\
        Image-to-image decoder attention heads & \multicolumn{2}{c}{16} \\
        Attention pooling input size & 768 & 1024 \\
        Attention pooling output size & \multicolumn{2}{c}{768} \\
        Attention pooling attention heads & \multicolumn{2}{c}{16} \\
    \midrule
        Data augment & \multicolumn{2}{c}{\makecell{RandomResizedCrop\\RandomHorizontalFlip\\ColorJitter\\RandomGrayscale\\GaussianBlur\\Solarize}} \\
        Mask strategy & \multicolumn{2}{c}{Blockwise mask} \\
        Mask ratio & \multicolumn{2}{c}{50\%} \\
        Input resolution &  &  \\
    \midrule
        Training epochs & \makecell{1600(ImageNet)\\138(YFCC)} & 30 \\
        Batch size & 4096 & 40000 \\
        Adam  & \multicolumn{2}{c}{(0.9, 0.95)} \\
        Peak learning rate & \multicolumn{2}{c}{} \\
        Learning rate schedule & \multicolumn{2}{c}{cosine} \\
        Warmup epochs & \makecell{40(ImageNet)\\3.5(YFCC)} &  1 \\
        Weight decay & \multicolumn{2}{c}{0.1} \\
        EMA coeff & \multicolumn{2}{c}{0.995} \\
        EMA schedule & \multicolumn{2}{c}{cosine} \\
        Label smoothing & \multicolumn{2}{c}{0.1} \\
        Stock. depth & 0.1 (linear) & 0.2 (uniform) \\
    \bottomrule
    \end{tabular}}
    \caption{Hyper-parameters for pre-training.}
    \label{tab:hypers_pretrain}
\end{table} 
\subsection{Tranfer Settings of InternImage-H}

We strictly follow \cite{anonymous2022internimg} for the transfer settings of InternImage-H on ImageNet-1k, COCO, LVIS and ADE20k. We briefly summarize the settings below.

\vspace{0.5em}\noindent\textbf{ImageNet-1k.~} For ImageNet classification, the pre-trained InternImage-H is fine-tuned on ImageNet-1k for 30 epochs.

\vspace{0.5em}\noindent\textbf{COCO.~} For COCO object detection, we double the parameters of pre-trained InternImage-H via the composite techniques~\cite{liang2022cbnet}. Then it is fine-tuned with the DINO~\cite{zhang2022dino} detector on Objects365~\cite{shao2019objects365} and COCO datasets one after another for 26 epochs and 12 epochs.

\vspace{0.5em}\noindent\textbf{LVIS.~} For LVIS long-tailed object detection, we double the parameters of pre-trained InternImage-H via the composite techniques~\cite{liang2022cbnet}. Then it is fine-tuned with the DINO~\cite{zhang2022dino} detector on Objects365~\cite{shao2019objects365} and LVIS datasets one after another for 26 epochs and 12 epochs.

\vspace{0.5em}\noindent\textbf{ADE20k.~} For ADE20k semantic segmentation, we fine-tune InternImage-H with Mask2Former~\cite{cheng2021masked}, and adopt the same settings in \cite{wang2022image,chen2022vitadapter}. 

\subsection{Transfer Settings of ViT-B/16}

\noindent\textbf{ImageNet-1k.~} The detailed fine-tuning and linear classification settings of ViT-B/16 on ImageNet-1k are listed in Tab.~\ref{tab:hypers_finetune} and Tab.~\ref{tab:hypers_linear}. 

\vspace{0.5em}\noindent\textbf{COCO and LVIS.~} We utilize ViTDet~\cite{li2022exploring} for object detection. By default, the fine-tuning schedule is set to 100 epochs for both COCO and LVIS datasets. For the ablation study, we use a short schedule of 25 epochs. Detailed hyper-parameters are listed in Tab.~\ref{tab:hypers_coco} and Tab.~\ref{tab:hypers_lvis}.

\vspace{0.5em}\noindent\textbf{ADE20k.~} Following~\cite{bao2021beit,he2022masked,anonymous2022siamese}, we employ UperNet~\cite{xiao2018unified} as the segmentation network. We use the implementation in MMSegmentation~\cite{mmseg2020}. Detailed hyper-parameters are listed in Tab.~\ref{tab:hypers_seg}.

\begin{table}[h]
    \centering
    \small
    \begin{tabular}{lc}
    \toprule
        Hyper-parameters & Value \\
    \midrule
        Erasing prob. & 0.25 \\
        Rand augment & 9/0.5 \\
        Mixup prob. & 0.8 \\
        Cutmix prob. & 1.0 \\
        Input resolution &  \\
    \midrule
        Finetuning epochs & 100 \\
        Batch size & 1024 \\
        Adam  & (0.9, 0.999) \\
        Peak learning rate & \\
        Learning rate schedule & cosine \\
        Warmup epochs & 5 \\
        Weight decay & 0.1 \\
        Layer-wise learning rate decay & 0.65 \\
        Label smoothing & 0.1 \\
        Stock. depth & 0.1 \\
    \bottomrule
    \end{tabular}
    \caption{Hyper-parameters of ViT-B for ImageNet finetuning.}
    \label{tab:hypers_finetune}
\end{table} \begin{table}[h]
    \centering
    \small
    \begin{tabular}{lc}
    \toprule
        Hyper-parameters & Value \\
    \midrule
        Data augment & \makecell{RandomResizedCrop\\RandomHorizontalFlip} \\
        Input resolution &  \\
    \midrule
        Training epochs & 90 \\
        Batch size & 16384 \\
        Optimizer & LARS \\
        Peak learning rate & 3.2 \\
        Learning rate schedule & cosine \\
        Warmup epochs & 10 \\
        Weight decay & 0.0 \\
    \bottomrule
    \end{tabular}
    \caption{Hyper-parameters of ViT-B for ImageNet linear probing.}
    \label{tab:hypers_linear}
\end{table} \begin{table}[h]
    \centering
    \small
    \begin{tabular}{lc}
    \toprule
        Hyper-parameters & Value \\
    \midrule
        Data augment & large scale jittor \\
        Input resolution &  \\
    \midrule
        Finetuning epochs & 100 \\
        Batch size & 64 \\
        Adam  & (0.9, 0.999) \\
        Peak learning rate & \\
        Learning rate schedule & step \\
        Warmup length & 250 iters \\
        Weight decay & 0.1 \\
    \midrule
        Stock. depth & 0.1 \\
    \midrule
        Relative positional embeddings & \checkmark \\
    \bottomrule
    \end{tabular}
    \caption{Hyper-parameters of ViT-B for COCO detection.}
    \label{tab:hypers_coco}
\end{table} \begin{table}[h]
    \centering
    \small
    \begin{tabular}{lc}
    \toprule
        Hyper-parameters & Value \\
    \midrule
        Data augment & large scale jittor \\
        Input resolution &  \\
    \midrule
        Finetuning epochs & 100 \\
        Batch size & 64 \\
        Adam  & (0.9, 0.999) \\
        Peak learning rate & \\
        Learning rate schedule & step \\
        Warmup length & 250 iters \\
        Weight decay & 0.1 \\
    \midrule
        Stock. depth & 0.1 \\
    \midrule
        Relative positional embeddings & \checkmark \\
    \bottomrule
    \end{tabular}
    \caption{Hyper-parameters of ViT-B for LVIS detection.}
    \label{tab:hypers_lvis}
\end{table} \begin{table}[h]
    \centering
    \small    
    \begin{tabular}{lc}
    \toprule
        Hyper-parameters & Value \\
    \midrule
        Data augment & \makecell{RandomCrop\\RandomFlip\\PhotoMetricDistortion} \\
        Input resolution &  \\
    \midrule
        Finetuning length & 160k iters \\
        Batch size & 16 \\
        Adam  & (0.9, 0.999) \\
        Peak learning rate & \\
        Learning rate schedule & linear \\
        Warmup length & 1500 iters \\
        Weight decay & 0.05 \\
    \midrule
        Stock. depth & 0.1 \\
    \midrule
        Relative positional embeddings & \checkmark \\
    \bottomrule
    \end{tabular}
    \caption{Hyper-parameters of ViT-B for ADE20k semantic segmentatioin.}
    \label{tab:hypers_seg}
\end{table} 
\subsection{More Experiments}

\setlength{\tabcolsep}{4pt}
\begin{table}
    \centering
    \small
    \resizebox{0.8\linewidth}{!}{
        \begin{tabular}{cccccc}
        \Xhline{2\arrayrulewidth}
        Gradient Ratio  & 0.2 & 0.5 & 1.0 & 2.0 & 5.0 \\
        \hline
        ImageNet Top1 & 83.1 & 83.2 & \default{83.3} & 82.8 & 82.5 \\
        COCO  & 50.2 & \default{50.5} & \default{50.5} & 48.9 & 47.6\\
        \Xhline{2\arrayrulewidth}
        \end{tabular}
    }
    \vspace{-1em}
    \caption{Ablation study of gradient ratio .}
    \vspace{-1.5em}
    \label{tab:exp_ablation_grad_ratio}
\end{table} 
\noindent\textbf{Ablation Study on Gradient Ratio .~} The gradient ratio  is used in dynamic weighting (see Eq.~\eqref{equ:appendix_dw} in Appendix~\ref{sec:appendix_pre}). We ablate the choice of  from  in Tab.~\ref{tab:exp_ablation_grad_ratio}. These models are pre-trained on ImageNet-1k for 100 epochs. Then they are fine-tuned on ImageNet-1k classification and COCO object detection. The fine-tuning schedules for ImageNet-1k and COCO are set to 100 epochs and 25 epochs respectively. As shown in Tab.~\ref{tab:exp_ablation_grad_ratio},  works quite well in both classification and detection. We choose  as our default setting for its simplicity.

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/fig-appendix-targetviews.pdf}
    \vspace{-0.5em}
    \caption{Illustration of four design choices of target views. }
    \label{fig:target_views}
\end{figure}

\setlength{\tabcolsep}{4pt}
\begin{table}
    \centering
    \small
    \resizebox{1.0\linewidth}{!}{
        \begin{tabular}{ccccc}
        \Xhline{2\arrayrulewidth}
        & First Target View & Second Target View & ImageNet Top1 & COCO \\ 
        \hline
        (a) & same      & same        & 77.2 & 48.6 \\
        (b) & different & different   & 78.5 & \default{49.8} \\
        (c) & different      & same   & 78.8 & 49.2 \\
        \default{default} & \default{same} & \default{different}  & \default{79.1} & 49.5 \\
        \Xhline{2\arrayrulewidth}
        \end{tabular}
    }
\caption{Ablation study of target views.  ImageNet fine-tuning is early-stopped at 20 epochs which we found consistent with the final performance in practice.}
    \vspace{-1.5em}
    \label{tab:exp_ablation_target_views}
\end{table} 
\vspace{0.5em}\noindent\textbf{Ablation Study on Target Views.~} Our \name{} consists of two target image views during the multi-input multi-target pre-training. Two input views of different images are mixed with a shared blockwise mask. As shown in Fig.~\ref{fig:target_views}, the visible part of the blockwise mask is filled with an augmented view of the first image, and the masked part is filled with an augmented view of the second image. The first target view and second target view are not permutable. We ablate the choices of these two target image views (either the same or different from the input image view) in Tab.~\ref{tab:exp_ablation_target_views}. These models are pre-trained on ImageNet-1k without labels (\ie they only have image representation target and do not have the category embedding target) for 200 epochs. Then, they are fine-tuned on ImageNet-1k classification and COCO object detection. The fine-tuning schedule is set to 100 epochs and 25 epochs respectively. Our default setting works best in ImageNet classification. Although (b) perform slightly better than our default setting in COCO detection, the pre-training process of it is quite unstable (FP16 loss scale is quite unstable), thus we do not choose it as our default setting. 
\end{document}
