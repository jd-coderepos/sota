\documentclass[]{eptcs}
\providecommand{\event}{EXPRESS/SOS 2015}

\usepackage[ngerman,english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{stmaryrd}
\usepackage{extarrows}
\usepackage{multirow}
\usepackage{xspace}
\usepackage{paralist}
\usepackage{graphicx}
\usepackage{color}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{calc}

\usepackage{count1to}
\usepackage{semantic}
\usepackage{url}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{cite}
\usepackage{bigdelim}
\usepackage{etoolbox}
\usepackage{hyperref}
\usepackage{cleveref}

\usepackage{macros}


\newcommand{\M}[1]{
  \textcolor{blue}{\bfseries\sffamily(#1)}
  \marginpar{\textcolor{blue}{\bfseries\sffamily M}}
}
\newcommand{\K}[1]{
  \textcolor{green}{\bfseries\sffamily(#1)}
  \marginpar{\textcolor{green}{\bfseries\sffamily K}}
}
\newcommand{\C}[1]{
  \textcolor{violet}{\bfseries\sffamily(#1)}
  \marginpar{\textcolor{violet}{\bfseries\sffamily C}}
}
\newcommand{\U}[1]{
  \textcolor{red}{\bfseries\sffamily(#1)}
  \marginpar{\textcolor{red}{\bfseries\sffamily U}}
}


\title{Encoding CSP into CCS (Extended Version)
	\thanks{Supported by the DFG via project ``Synchronous and Asynchronous Interaction in Distributed Systems".}}

\author{Meike Hatzel 
	\institute{TU Berlin}
	\and Christoph Wagner
	\institute{TU Berlin}
	\and Kirstin Peters\thanks{Supported by the German Federal and State Governments via the Excellence Initiative (Institutional Strategy).}
	\institute{TU Dresden}
	\and Uwe Nestmann
	\institute{TU Berlin}
}
\def\titlerunning{Encoding CSP into CCS}
\def\authorrunning{M.\ Hatzel, C.\ Wagner, K.\ Peters, U.\ Nestmann}


\begin{document}


\maketitle

\begin{abstract}
	We study encodings from CSP into asynchronous CCS with name passing and matching, so in fact, the asynchronous -calculus. By doing so, we discuss two different ways to map the multi-way synchronisation mechanism of CSP into the two-way synchronisation mechanism of CCS. Both encodings satisfy the criteria of Gorla except for compositionality, as both use an additional top-level context. Following the work of Parrow and Sj√∂din, the first encoding uses a central coordinator and establishes a variant of weak bisimilarity between source terms and their translations. The second encoding is decentralised, and thus more efficient, but ensures only a form of coupled similarity between source terms and their translations.
\end{abstract}


\section{Introduction}

In the context of a scientific meeting on Expressiveness in Concurrency and Structural Operational Semantics (SOS), likely very little needs to be said about the process algebras (or process calculi) CSP and CCS. Too many papers have been written since their advent in the 70's to be mentioned in our own paper; it is instructive, though, and recommended to appreciate Jos Baeten's historical overview \cite{Baeten:2005:BHP:1085667.1085669}, which also places CSP and CCS in the context of other process algebras like ACP and the many extensions by probabilities, time, mobility, etc. Here, we just select references that help to understand our motivation.

\paragraph{Differences.}
From the beginning, although CSP \cite{hoare:78csp} and CCS \cite{CCS} were intended to capture, describe and analyse reactive and interactive concurrent systems, they were designed following rather different philosophies. Tony Hoare described this nicely in his position paper \cite{Hoare2006209} as follows: ``A primary goal in the original design of CCS was to discover and codify a minimal set of basic primitive agents and operators \dots and a wide range of useful operators which have been studied subsequently are all definable in terms of CCS primitives." and ``CSP was more interested in this broader range of useful operators, independent of which of them might be selected as primitive." So, at their heart, the two calculi use two different synchronisation mechanisms, one (CCS) using binary, \ie two-way, handshake via matching actions and co-actions, the other (CSP) using multiway synchronisation governed by explicit synchronisation sets that are typically attached to parallel composition. Another difference is the focus on Structural Operational Semantics in CCS, and the definition of behavioural equivalences on top of this, while CSP emphasised a trace-based denotational model, enhanced with failures, and the question on how to design models such that they satisfy a given set of laws of equivalence.

\paragraph{Comparisons.}
From the early days, researchers were interested in more or less formal comparisons between CSP and CCS. This was carried out by both Hoare \cite{Hoare2006209} and Milner \cite{DBLP:conf/ifip/Milner86} themselves, where they concentrate on the differences in the underlying design principles. But also other researchers joined the game, but with different analysis tools and comparison criteria. 

For example, Brookes \cite{DBLP:conf/icalp/Brookes83} contributed a deep study on the relation between the underlying abstract models, synchronisation trees for CCS and the failures model of CSP. Quite differently, Lanese and Montanari \cite{Lanese200655} used the respective power to transform graphs as a measure for the expressiveness of the two calculi. 

Yet completely differently, Parrow and Sj{\"o}din \cite{sjodin:phd,parrowCoupled92} tried to find an algorithm to implement|best in a fully distributed fashion|the multiway synchronisation operator of CSP (and its variant LOTOS \cite{DBLP:conf/pstv/Brinksma85}) using the supposedly simpler two-way synchronisation of CCS. They came up with two candidates|a reasonably simple centralised synchroniser, and a considerably less simple distributed synchroniser\footnote{Recently \cite{7092761}, a slight variant of the protocol behind this algorithm was used to implement a distributed compiler for a substantial subset of LOTOS that yields reasonably efficient C code.}|and proved that the two are not weakly bisimilar, but rather coupled similar, which is only slightly weaker. Coupled simulation is a notion that Parrow and Sj\"odin invented for just this purpose, but it has proved afterwards to be often just the right tool when analysing the correctness of distribution- and divergence-sensitive encodings that involve partial commitments (whose only effect is to gradually perform internal choices) \cite{nestmannPierce00}.

The probably most recent comparison between CSP and CCS was provided by van Glabbeek~\cite{DBLP:journals/corr/abs-1208-2750}. As an example for his general framework to analyse the relative expressive power of calculi, he studied the existence of syntactical translations from CSP into CCS, for which a common semantical domain is provided via labeled transition systems (LTS) derived from respective sets of SOS rules. The comparison is here carried out by checking whether a CSP term and its translation into CCS are distinguishable with respect to a number of equivalences defined on top of the LTS. The concrete results are: (1)~there is a translation that is correct up to trace equivalence (and contains deadlocks), and (2)~there is no translation that is correct up to weak bisimilarity equivalence that also takes divergence into account.

\paragraph{Contribution.}
Given van Glabbeks negative result, and given Parrow and Sj\"odins algorithm, we set out to check whether we can define a syntactical encoding from CSP into CCS|using Parrow and Sj\"odins ideas|that is correct up to coupled simulation. We almost managed. In this paper, we report on our current results along these lines: 
(1)~Our encoding target is an asynchronous variant of CCS, but enhanced with name-passing and matching, so it is in fact the asynchronous -calculus; we kept mentioning CCS in the title of this paper, as it clearly emphasises the origin and motivation of this work. But, we could \emph{not} do without name-passing.
(2)~We exhibit one encoding that is not distributability-preserving (so, it represents a centralised solution), but is correct up to weak bisimilarity and does not introduce divergence. This does not contradict van Glabbeek's results, but suggests the observation that van Glabbeek's framework implies some form of distributability-preservation.
(3)~We exhibit another encoding that \emph{is} distributability-preserving and divergence-reflecting, but is only correct up to coupled similarity.

\paragraph{Overview.}
This paper is an extended version---providing the missing proofs and some additional in\-for\-ma\-tions---of \cite{hatzel15}.
We introduce the considered variants of CSP and CCS in \S~\ref{sec:techPrel}. There we also introduce the criteria---that are (variants of) the criteria in \cite{gorla10} and \cite{petersNestmannGoltz13}---modulo which we prove the quality of the considered encodings. In \S~\ref{sec:innerPart} we introduce the inner layer of our two encodings. It provides the main machinery to encode synchronisations of CSP. We complete this encoding with an outer layer that is either a central (\S~\ref{sec:central}) or a de-central coordinator (\S~\ref{sec:decentral}). In \S~\ref{sec:conclusion} we discuss the two encodings.



\section{Technical Preliminaries}
\label{sec:techPrel}

Let  be the countably-infinite set of names, let , and let  the set of co-names, \ie .
A process calculus  consists of a set of processes  (syntax) and a relation  (semantics).
 denotes an internal, \ie unobservable, action.
We use  to range over names and  to range over processes.
We use  to range over .
 denotes a sequence of names.
Let  and   denote the sets of free names and bound names occurring in , respectively.
Their definitions are completely standard.
We use  to range over substitutions.
A substitution is a mapping  from names to names.
The application of a substitution on a term  is defined as the result of simultaneously replacing all free occurrences of  by  for .
For all names in  the substitution behaves as the identity mapping.
We naturally extend substitutions to co-names, \ie  for all substitutions .
The relation  as defined in the semantics below defines the reduction steps processes can perform. We write  if  and call  a \emph{derivative} of .
Let  denote the reflexive and transitive closure of .
 is \emph{divergent} if it has an infinite sequence of steps .
We use \emph{barbs} or \emph{observables} to distinguish between processes with different behaviours. We write  if  has a barb , where the predicate  can be defined different for each calculus. Moreover  reaches a barb , if  a reaches a process with this barb, \ie .

We use a variant of CSP \cite{hoare:78csp}, where prefixes only occur behind external choice.
\begin{definition}\label{CSPSyntax}
The processes  are given by

	where  is a process variable, , and  is an index set.
\end{definition}

 is the parallel composition of  and , where  and  can proceed independently except for actions , on which they have to synchronise.
The process  describes \emph{divergence}.
 denotes \emph{inaction}.
The \emph{internal choice} operator  reduces to either  or  within a single internal step.
\emph{Concealment}  hides an action  and masks it as .
\emph{Renaming}  for some  with  behaves as , where  is replaced by  for all .
The \emph{recursion}  describes a process behaving like  with every occurrence of  being replaced by .
Finally the \emph{external choice}  represents a choice between the different \emph{action prefixes}  followed by the corresponding continuation . The process can perform any  and then behave like .
 
As usual we use  to denote binary external choice.
The CSP semantics is given by the rules, where we introduce labelled steps  first and then use them to define :
\vspace{-0.75em}

\vspace{-1.2em}\\
A barb of CSP is the possibility of a term, to perform an action, \ie .
Following the Definition of distributability in \cite{petersNestmannGoltz13} a CSP term  is distributable into  if  are subterms of  such that each action prefixes in  occurs exactly in one of the , where different but syntactically equivalent action prefixes are distinguished and unguarded occurrences of  may result in several copies of  within the .

As target calculus we use an asynchronous variant of CCS\cite{CCS} with name-passing and matching.

\begin{definition}\label{def:ccs_syntax}
  The processes  are given by
  
\end{definition}

 is the parallel composition of  and , where  and  can either proceed independently or synchronise on channels with matching names.
 restricts actions  on , forcing all sub-terms of  to synchronise on these actions.
 denotes input on channel .
 is output on channel .
Note that because there is no continuation we interpret this calculus as asynchronous.
We use  to denote \emph{replicated input} on channel  with the continuation .
 is the matching operator, if  then  is enabled.
 denotes inaction.

The CCS semantics is given by following transition rules:
\vspace{-0.6em}

\vspace{-1em}\\
where  denotes structural congruence given by the rules: ,
, , ,  if ,  and  if .
As discussed in \cite{petersNestmannGoltz13} a CCS term  is distributable into  if .

\subsection{Simulation Relations}

The semantics of a process is usually considered modulo some behavioural equivalence on processes.
For many calculi \emph{the} standard reference equivalence is some form of bisimulation.
Since in the context of encodings, \ie translation between different languages that can differ in their interpretation of what is considered a barb, reduction steps are easier, we use a variant of weak reduction bisimulation.
With Gorla \cite{gorla10}, we add a \emph{success} operator  to the syntax of both CSP and CCS. Since  cannot be further reduced, the semantics is left unchanged in both cases. The test for the reachability of success is standard in both languages, \ie .
To obtain a non-trivial equivalence, we require that the bisimulation respects success and the reachability of barbs.
Therefore we use the standard definition of barbs in CSP, \ie action-prefixes, for CSP-barbs.
Our encoding function will translate all source terms into closed terms, thus the standard definition of CCS barbs would not provide any information.
Instead we use a notion of translated barb () that reflects how the encoding function translates source term barbs. Its definition is given in Section~\ref{sec:innerPart}.

\begin{definition}[Bisimulation]
	A relation  is a \emph{success sensitive, (translated) barb respecting, weak, reduction bisimulation} if, whenever , then:
	\begin{compactitem}
		\item  implies 
		\item  implies 
		\item  iff 
		\item  and  reach the same (translated) barbs, where we use  for CSP and  for CCS
	\end{compactitem}
	Two terms  are \emph{bisimilar}, denoted as , if there exists a bisimulation that relates  and .
\end{definition}

\noindent
We use the symbol  to denote either bisimilarity on our target language CCS or on the disjoint union of CSP and CCS that allows us to describe the relationship between source terms and their translations. In the same way we define a corresponding variant of coupled similarity.

\begin{definition}[Coupled Simulation]
	A relation  is a \emph{success sensitive, (translated) barb respecting, weak, reduction coupled simulation} if, whenever , then:
	\begin{compactitem}
		\item  implies  and 
		\item  iff 
		\item  and  reach the same (translated) barbs, where we use  for CSP and  for CCS
	\end{compactitem}
	Two terms  are \emph{coupled similar}, denoted as , if there exists a coupled simulation that relates  and  in both directions.
\end{definition}

\subsection{Encodings and Quality Criteria}

We consider two different translations from (the above defined variant of) CSP into (the above defined variant of) CCS with name passing and matching. We denote the variant of CSP as \emph{source} and the variant of CCS as \emph{target} language and, accordingly, their terms as \emph{source terms}  and \emph{target terms} . Encodings often translate single source term steps into a sequence or pomset of target term steps. We call such a sequence or pomset a \emph{simulation} of the corresponding source term step.
Moreover we assume for each encoding the existence of a so-called renaming policy , \ie a mapping of names from the source into vectors of target term names.

To analyse the quality of encodings and to rule out trivial or meaningless encodings, they are augmented with a set of quality criteria. In order to provide a general framework, Gorla in \cite{gorla10} suggests five criteria well suited for language comparison:
\begin{compactenum}[(1)]
	\item \emph{Compositionality}: The translation of an operator  is the same for all occurrences of that operator in a term, \ie it can be captured by a context  such that  for .
	\item \emph{Name Invariance}: The encoding does not depend on particular names, \ie for every  and , it holds that  if  is injective and  otherwise, where  is such that  for every .
	\item \emph{Operational Correspondence}: Every computation of a source term can be simulated by its translation, \ie  implies  (completeness), and every computation of a target term corresponds to some computation of the corresponding source term (soundness, compare to Section~\ref{sec:decentral}).
	\item \emph{Divergence Reflection}: The encoding does not introduce divergence, \ie  always implies .
	\item \emph{Success Sensitiveness}: A source term and its encoding answer the tests for success in exactly the same way, \ie  iff .
\end{compactenum}
Operational correspondence and name invariance assume a behavioural equivalence  on the target language (that we instantiate with ). Its purpose is to describe the abstract behaviour of a target process, where abstract refers to the behaviour of the source term. By \cite{gorla10} the equivalence  is often defined in the form of a barbed equivalence (as described e.g. in \cite{milner.sangiorgi:barbed-bisimulation}) or can be derived directly from the reduction semantics and is often a congruence, at least with respect to parallel composition.  is such a relation.

Our encodings will satisfy all of these criteria except for compositionality, because both encodings consists of two layers.
\cite{petersNestmannGoltz13} shows that the above criteria do not ensure that an encoding preserves distribution and proposes a criterion for the preservation of distributability.

\begin{definition}[Preservation of Distributability]
	\label{def:distributabilityPreservation}

	An encoding  \emph{preserves distributability} if for every  and for all terms  that are distributable within  there are some  that are distributable within  such that  for all .
\end{definition}

\noindent
Here, because of the choice of the source and the target language, an encoding preserves distributability if for each sequence of distributable source term steps their simulations are pairwise distributable. In both languages two alternative steps of a term are in \emph{conflict} with each other if they reduce the same action-prefix---for CSP---or reduce either the same (replicated) input using two outputs that transmit different values, or reduce the same output using two (replicated) inputs with different continuations. Two alternative steps that are not in conflict are \emph{distributable}.

\section{Translating the CSP Synchronisation Mechanism}
\label{sec:innerPart}

CSP and CCS---or the -calculus---differ fundamentally in their communication and synchronisation mechanisms.
In CSP there is only a single kind of action , where  is a (channel) name. Synchronisation is implemented by the parallel operator  that in CSP is augmented with a set of names  containing the names that need to be synchronised at this point. By nesting parallel operators arbitrary many actions on the same name can be synchronised.
In CCS there are two different kinds of actions: inputs  and outputs . Again synchronisation is implemented by the parallel operator, but in CCS only a single input and a single matching output can ever be synchronised.

To encode the CSP communication and synchronisation mechanisms in CCS with name passing we make use of a technique already used in \cite{petersNestmann12, peters12} to translate between different variants of the -calculus. CSP actions are translated into action announcements augmented with a lock indicating, whether the respective action was already used in the simulation of a step. The other operators of CSP are then translated into handlers for these announcements and locks.
The translation of sum combines several actions under the same lock and thus ensures that only one term of the sum can ever be used.
The translation of the parallel operator combines announcements of actions that need to be synchronised into a single announcement under a fresh lock, whose value is determined by the combination of the respective underlying locks at its left and right side. Announcements of actions that do not need to be synchronised are simply forwarded.
A second layer---containing either a centralised or a de-centralised coordinator---then triggers and coordinates the simulation of source term steps.

Action announcements are of the form :  is the translation of the source term action.  is used to trigger the computation of the Boolean value of . The lock  evaluates to  as long as the respective translated action was not successfully used in the simulation of a step.  is used to guard the encoded continuation of the respective source term action. In the case of a successful simulation attempt involving this announcement, an output  allows to unguard the encoded source term continuation and ensures that all following evaluations of  return . The message  indicates an aborted simulation attempt and allows to restore  for later simulation attempts. Once a lock becomes , all request for its computation return .

\subsection{Abbreviations}

We introduce some abbreviations to simplify the presentation of the encodings. We use

to test, whether an action belongs to the set of synchronised actions in the encoding of the parallel operator.
As already done in \cite{nestmann96, nestmannPierce00} we use Boolean valued locks to ensure that every translation of an action is only used once to simulate a step.
\emph{Boolean locks} are channels on which only the Boolean values  (true) or  (false) are transmitted. An unguarded output over a Boolean lock with value  is called a positive instantiation of the respective lock, whereas an unguarded output sending  is denoted as negative instantiation. At the receiving end of such a channel, the Boolean value can be used to make a binary decision, which is done here within an \emph{-construct}.
This construct and accordingly instantiations of locks are implemented as in \cite{nestmann96, nestmannPierce00} using restriction and the order of transmitted values.

We observe that the Boolean values  and  are realised by a pair of links without parameters. Both cases of the -construct operate as guard for its subterms  and . The renaming policy  reserves the names  and  to implement the Boolean values  and .

\subsection{The Algorithm}

The encoding functions introduce some fresh names, that are reserved for special purposes. In Table~\ref{tab:resNam} we list the reserved names  and provide a hint on their purpose.
\begin{table}[t]
	\begin{tabular}{|c|c|}
		\hline
		reserved names & purpose\\
		\hline
		,  & announce the ability to perform an action\\
		, , ,  & (translated) source term channel, channel from the left/right of a parallel operator\\
		, ,  & lock, lock from the left/right of a parallel operator\\
		 & re-instantiate a positive sum lock\\
		, ,  & request the computation of the value of a lock\\
		, , ,  & simulate a source term step and unguard the corresponding continuations\\
		 & order left announcements for the same channel that need to be synchronised\\
		,  & distribute right announcements that need to be synchronised\\
		 & Boolean value ( or )\\
		 & fresh name used to announce -steps that result from concealment\\
		 & used by the centralised encoding to avoid overlapping simulation attempts\\
		 & fresh names used to encode internal choice\\
		 & fresh names used to encode divergence\\
		 & used to encode Boolean values\\
		\hline
	\end{tabular}
	\caption{Reserved Names.}
	\label{tab:resNam}
\end{table}
Moreover we reserve the names  and assume an injective mapping  that maps process variables of CSP to distinct names.
The renaming policy  for our encodings is then a function that reserves the names in  and translates every source term name into three target term names. More precisely, choose  such that:
\begin{compactenum}
	\item No name is mapped onto a reserved name, \ie  for all .
	\item No two different names are mapped to overlapping sets of names, \ie  for all  with .
\end{compactenum}
We naturally extend the renaming policy to sets of names, \ie  if .
Let  denote the projection of a -tuple to its th element, if . Moreover  for a set of -tuples  and .

\begin{figure}[htp]
	
	where  is short for ,  is short for , and  is short for .
	\caption{An encoding from CSP into CCS with value passing (inner part).}
	\label{fig:innerEncoding}
\end{figure}
The inner part of our two encodings is presented in Figure~\ref{fig:innerEncoding}. The most complex case is the translation of the parallel operator  that is based on the following four steps:
\begin{description}
	\item[Step 1:] Action announcements for channels \\
		In the case of actions on channels ---that do not need to be synchronised here---the encoding of the parallel operator acts like a forwarder and transfers action announcements of both its subtrees further up in the parallel tree.
		Two different restrictions of the channel for action announcements  from the left side  and the right side , allow to trace action announcements back to their origin as it is necessary in the following case.
		In the present case we use  to bridge the action announcement over the restrictions on .
	\item[Step 2:] Action announcements for channels \\
		Actions  need to be synchronised, \ie can be performed only if both sides of the parallel operator cooperate on this action. Simulating this kind of synchronisation is the main purpose of the encoding of the parallel operator.
		The renaming policy  translates each source term name into three target term names. The first target term name is used as reference to the original source term name and transferred in announcements. The other two names are used to simulate the synchronisation of the parallel operator in CSP. Announcements from the left are translated to outputs on the respective second name and announcements from the right to the respective third name. Restriction ensures that these outputs can only be computed by the current parallel operator encoding. The translations of the announcements into different outputs for different source term names allows us to treat announcements of different names concurrently using the term , where  is a source term name.
	\item[Step 3:] The term \\
		In  all announcements for the same source term name  from the left are ordered in order to combine each left and each right announcement on the same name. Several such announcements may result from underlying parallel operators, sums with similar summands, and junk left over from already simulated source term steps. For each left announcement a fresh instance of  is generated and restricted. The names  and  are used to transfer right announcements to the respective next left announcement, where  is used to bridge over the restriction on . This way each right announcement will eventually be transferred to each left announcement on the same name. Note that this kind of forwarding is not done concurrently but in the source language a term  also cannot perform two steps on the same name  concurrently. After combining a left and a right announcement on the same source term name a fresh set of auxiliary variables  is generated and a corresponding announcement is transmitted. The term  reacts to requests regarding this announcement and is used to simulate a step on the synchronised action.
	\item[Step 4:] The term \\
		If a request reaches  it starts questioning the left and the right side. First the left side is requested to compute the current value of the lock of the action. Only if  is returned, the right side is requested to compute its lock as well. This avoids deadlocks that would result from blindly requesting the computation of locks in the de-centralised encoding. If the locks of both sides are still valid the fresh lock  returns  else  is returned. For each case  ensures that subsequently requests will obtain an answer by looping with  or returning  to all requests, respectively. The messages  and  cause the respective underlying subterms on the left and the right side to do the same, whereas  and  cause the unguarding of encoded continuations as result of a successful simulation of a source term synchronisation step.
\end{description}

\subsection{Basic Properties and Translated Observables}

The protocol introduced by the encoding function in Figure~\ref{fig:innerEncoding} (and its outer parts introduced later) simulates a single source term step by a sequence of target term steps. Most of these steps are merely pre- and post-processing steps, \ie they do not participate in decisions regarding the simulation of conflicting source term steps but only prepare and complete simulations. Accordingly we distinguish between \emph{auxiliary steps}---that are pre- and post-processing steps---and \emph{simulation steps}---that mark a point of no return by deciding which source term step is simulated. Note that the points of no return and thus the definition of auxiliary and simulation steps is different in the two variants of our encoding.

Auxiliary steps do not influence the choice of source terms steps that are simulated. Moreover they operate on restricted channels, \ie are unobservable. Accordingly they do not change the state of the target term modulo the considered reference relations  and . We introduce some auxiliary lemmata to support this claim.

The encoding  translates source term barbs  into free announcements with  as first value and a lock  as third value that computes to . The two coordinators, \ie outer encodings, we introduce later, restrict the free -channel of .

\begin{definition}[Translated Barbs]
	Let  such that , , or .
	 has a translated barb , denoted by , if
	\begin{compactitem}
		\item there is an unguarded output ---on a free channel  in the case of  or the outermost variant of  in the case of the later introduced encodings  and ---in  or
		\item such an announcement was consumed to unguard an -construct testing  and this construct is still not resolved in 
	\end{compactitem}
	such that all locks that are necessary to instantiate  are positively instantiated.
\end{definition}

Analysing the encoding function in Figure~\ref{fig:innerEncoding} we observe that guarded subterms  of a of a source term , \eg , are translated into guarded subterms of , whereas the translations of unguarded subterms, \eg , remain unguarded.

\begin{obs}
	\\
	Let  such that  is a subterm of . Then  is guarded in  iff  is guarded in .
	\label{obs:guardedSourceVsTarget}
\end{obs}

We also observe that an encoded source term has a translated barb iff the corresponding source term has the corresponding source term barb.

\begin{obs}
	For all , it holds  iff .
	\label{obs:transBarbs}
\end{obs}

All instances of success in the translation result from success in the source. More precisely the only way to obtain  in the translation is by .

\begin{obs}
	For all , it holds  iff .
	\label{obs:success}
\end{obs}

The simplest case of a step that cannot change the state of a term modulo , is a step on a restricted channel that is not in conflict with any other reachable step of the term.

\begin{lemma}
	Let  and  be a step on an unobservable channel such that no alternative step of  or its derivatives is in conflict to the step . Then .
	\label{lem:noConflicts}
\end{lemma}

\begin{figure}[t]
	\centering
	\begin{tikzpicture}[auto]
		\node (T) at (4, 2) {};
		\node (T') at (0, 1) {};
		\node (P') at (8, 1) {};
		\node (Q') at (4, 0) {};
		
		\path[|->] (T) edge (T');
		\path[|->] (T) edge (P');
		\path[|->] (T') edge (Q');
		\path[|->] (P') edge (Q');
	\end{tikzpicture}
	\caption{Diamond Property.}
	\label{fig:diamondProp}
\end{figure}

\begin{proof}
	Let  be the reflexive closure over the set of pairs  such that  is a step on an unobservable channel and no alternative step of  or its derivatives is in conflict with the step . We show that  is a bisimulation.
	Let .
	We have to prove the following four conditions:
	\begin{compactenum}
		\item  implies :\\
			Without loss of generality assume  reduces  and  (the case of non-replicated input is similar).
			Then  and .\\
			If  then choose .  and  follow from reflexivity.\\
			Else if  is a step on , then, since there are no conflicts with , the two steps reduce different outputs on  but the same replicated input. Hence  and . Then  can perform this step such that  with . Also  can perform the same step as  such that  (compare to Figure~\ref{fig:diamondProp}). Since no alternative step of a derivative of  can be in conflict with this step, we have .\\
			Else there is  such that  and  (the case of replicated input is similar). Again  can perform this step such that  with . Also  can perform the same step as  such that . Since no alternative step of a derivative of  can be in conflict with this step, we have .
		\item  implies :\\
			Choose . Then  and, by reflexivity, . 
		\item  iff :\\
			Once success is unguarded it cannot be removed. Accordingly the step can only add an unguarded instance of success, which then is reachable from . By 1. and 2.,  and  can reach the same occurrences of success.
		\item  iff :\\
			Since there are only outputs but no inputs on the free variant of , steps can produce but not reduces free announcements. Every free announcement introduced by  is also reachable in . By 1. and 2.,  and  reach the same translated barbs.
	\end{compactenum}
\end{proof}

Many auxiliary steps implement the forwarding of announcements. They are steps on restricted channels such that there is always exactly one replicated input on this channel. This ensures that these steps cannot be in conflict with other steps of the encoding and thus do not change the state modulo .

\begin{prop}
	Let  and  be a step on a restricted channel  such that the only input on  in  and all derivatives of  is exactly one replicated input.
	Then .
	\label{prop:auxStepsForward}
\end{prop}

\begin{proof}
	Two steps reducing the same replicated input (but different outputs) are not in conflict with each other. Thus  cannot perform a step that is conflict with . Note that replicated inputs are never removed. Since this replicated input is the only input on  in all derivatives of , no alternative step of any derivative of  is in conflict with . By Lemma~\ref{lem:noConflicts}, then .
\end{proof}

Variants of the channels , and  do not carry parameters. For channels like these, a conflict can only result from two steps that reduce different (replicated) inputs, because the derivatives can differ only due to different continuations of the respective inputs.

\begin{prop}
	Let  and  reduce a restricted  such that no value is transmitted and there is at most one input or replicated input on  in  and all derivatives of .
	Then .
	\label{prop:auxStepsReq}
\end{prop}

\begin{proof}
	Since there is always at most one (replicated) input on , alternative steps on this channel can only compete for different outputs. Let . Since in  outputs have no continuation and because  does not carry a value, the continuations of two steps  and  that reduce different outputs on  but the same (replicated) input are structural congruent, \ie .
	By Lemma~\ref{lem:noConflicts} and because all other steps on different channels are not in conflict with , then .
\end{proof}

The encoding propagates announcements through the translated parallel structure. In the translation of parallel operators it combines all left and right announcements \wrt to the same channel name, if this channel needs to be synchronised. Therefore we copy announcements.
We use locks carrying a Boolean value to indicate whether an announcement was already used to simulate a source term step. These locks carry  in the beginning and are swapped to  as soon as the announcement was used. In each state there is at most one positive instantiation of each lock and as soon as a lock is instantiated negatively it never becomes positive again.

\begin{lemma}
	Let  such that . Then for each variant  of the names 
	\begin{compactenum}
		\item there is at most one positive instantiation of  in ,
		\item if there is a positive instantiation of  in  then there is no other instantiation of  in ,
		\item if there is a negative instantiation of  in  then no derivative of  contains a positive instantiation of .
	\end{compactenum}
	\label{lem:sumLocks}
\end{lemma}

\begin{proof}
	Analysing the encoding function in Figure~\ref{fig:innerEncoding} we observe that initially no instantiations of locks are unguarded.  and the translation of external choice are the only parts of the translation that introduce instantiations of locks and both restrict the respective locks.
	
	In the translation of external choice all instantiations of the lock  are guarded by an input or a replicated input on . Moreover, to unguard one of the later two instantiations within the -construct, a step on  is necessary. Therefore we need an instantiation of . The only instances of a variant of ,  are generated by . There they are guarded and to unguard them a positive instantiation of the corresponding lock has to be consumed. This way only a single positive instantiation can be unguarded, but  allows to obtain several negative instantiations of  if there are several outputs on .
	
	To unguard an instantiation of  within the -constructs in  a step on  is necessary. Initially there is only a single unguarded output on . A subsequent output on  can be unguarded  by consuming a negative instantiation of  and that requires again the consumption of a positive instantiation of . Moreover, if a negative instantiation of  is unguarded, then also  but no output on  is unguarded.
	
	Thus both cases 1.\@ and 2.\@ follow by induction over the number of steps in . Since initially only positive instantiations of  are reachable, by 1.\@ and 2., and because the unguarding of a new positive instance of  requires the consumption of a positive instantiation of , property 3.\@ holds.
\end{proof}

Moreover each target term contains at most a single input or replicated input for each variant of , and .

\begin{lemma}
	Let  such that . Then for each variant  of the names  there is at most one (replicated) input on  in .
	\label{lem:reqLocks}
\end{lemma}

\begin{proof}
	(Replicated) inputs on  are introduced by  and the translation of external choice.
	
	In  to unguard an input on  an output on  has to be consumed. Initially there is a single such output. Additional outputs on  can only be unguarded by consuming a positive instantiation of the lock . Unguarding a positive instantiation of  in turn requires to consume an input on . Unguarding a replicated input on  also unguards a negative instantiation of .
	
	The translation of external choice initially offers exactly one unguarded input on . To unguard an additional (replicated) input on , we have to consume a positive instantiation of the lock  (to obtain an instantiation of ). Unguarding a positive instantiation of  in turn requires to consume an input on . Unguarding a replicated input on  also unguards a negative instantiation of .
	
	By induction over the number of steps in , we can show that there is at most one (replicated) input on  in .
\end{proof}

 combines each left announcement of this action with each right announcement of this action. Therefore each left announcement---transmitted over  to keep track of the source term action---restricts its own version of  and . Then over each  all right announcements---initially transmitted over ---are received and forwarded to the next variant of  by a message on . Derivatives of  can differ in the order in that left announcements on  were received. Two left announcements for the same action cannot be processed concurrently, but also the source term cannot perform two steps on the same synchronised channel concurrently. We show that the different order of left announcements does not matter.

\begin{lemma}
	Let  such that  and  reduces an output on . Moreover assume that for all steps  on a variant of  with  it holds .
	Then .
	\label{lem:orderLeftAnn}
\end{lemma}

\begin{proof}
	The only part of  that provides inputs on  is . Since  is restricted in the translation of the parallel operator,  can have at most one unguarded input on  but several outputs on . Thus different steps on this channel are in conflict with each other.
	A new input on  is unguarded by reducing the replicated input . Thus the continuations of different steps on  differ by the variant of  only. Each reduction of the input on  immediately restricts new variants of  and  and provides a new output . Since all steps on variants of  do not change the state modulo  and because all variants of  are restricted, the continuations of different inputs on   cannot be distinguished by .
	Thus .
\end{proof}

\section{The Centralised Encoding}
\label{sec:central}

Figure~\ref{fig:innerEncoding} describes how to translate CSP actions into announcements augmented with locks and how the other operators are translated to either forward or combine these announcements and locks. With that  provides the basic machinery of our encoding from CSP into CCS with name passing and matching. However it does not allow to simulate any source term step. Therefore we need a second (outer) layer that triggers and coordinates the simulation of source term steps. We consider two ways to implement this coordinator: a centralised and a de-centralised coordinator. The centralised coordinator is depicted in Figure~\ref{fig:centralised}.

\begin{figure}
	
	\caption{A \textbf{centralised} encoding from CSP into CCS with value passing.}
	\label{fig:centralised}
\end{figure}

The channel  is used to ensure that simulation attempts of different source term steps cannot overlap each other. For each simulation attempt exactly one announcement is consumed. The coordinator then triggers the computation of the respective lock that was transmitted in the announcement. This request for the computation of the lock is propagated along the parallel structure induced by the translations of parallel operators until---in the leafs---encodings of sums are reached. There the request for the computation yields the transmission of the current value of the respective lock. While being transmitted back to the top of the tree, different locks that refer to synchronisation in the source terms are combined. If the computation of the lock results with  at the top of the tree, the respective source term step is simulated. Else the encoding aborts the simulation attempt and restores the consumed informations about the values of the respective locks. In both cases a new instance of  allows to start the next simulation attempt. Accordingly only some post-processing steps can overlap with a new simulation attempt.

The central coordinator respects the protocol on locks used to ensure that each announcement is only used once to simulate a source term step, \ie it preserves the properties of locks formulated in Lemma~\ref{lem:sumLocks}.

\begin{prop}
	Let  such that . Then for each variant  of the names 
	\begin{compactenum}
		\item there is at most one positive instantiation of  in ,
		\item if there is a positive instantiation of  in  then there is no other instantiation of  in , and
		\item if there is a negative instantiation of  in  then no derivative of  contains a positive instantiation of .
	\end{compactenum}
	\label{prop:sumLocksCentral}
\end{prop}

\begin{proof}
	The encoding in Figure~\ref{fig:centralised} does not introduce new instantiations of . It does provide additional instantiations of , but to unguard them again a positive instantiation of the corresponding lock  has to be consumed. Thus  preserves the properties of locks formulated in Lemma~\ref{lem:sumLocks}.
\end{proof}

Similarly Lemma~\ref{lem:reqLocks} is preserved.

\begin{prop}
	Let  such that . Then for each variant  of the names  there is at most one (replicated) input on  in .
	\label{prop:reqLocksCentral}
\end{prop}

\begin{proof}
	Follows from Lemma~\ref{lem:reqLocks} and Proposition~\ref{prop:sumLocksCentral}, because the encoding in Figure~\ref{fig:centralised} provides additional instantiations of , but to unguard them a positive instantiation of the corresponding lock  has to be consumed.
\end{proof}

Lemma~\ref{lem:orderLeftAnn} is preserved by , because the encoding in Figure~\ref{fig:centralised} does not use variants of the names , and .

\begin{prop}
	Let  such that  and  reduces an output on . Moreover assume that for all steps  on a variant of  with  it holds .
	Then .
	\label{prop:orderLeftAnnCentral}
\end{prop}

\begin{proof}
	The encoding in Figure~\ref{fig:centralised} does not use variants of the names , and . Thus this Proposition follows from Lemma~\ref{lem:orderLeftAnn}.
\end{proof}

As we prove below, the points of no return in the centralised encoding can result from the consumption of action announcements by the outer encoding in Figure~\ref{fig:centralised} if the corresponding lock computes to . Moreover the encoding of internal choice and divergence introduces simulation steps, namely all steps on variants of the channels , , and . All remaining steps of the centralised encoding are auxiliary.

\begin{definition}[Auxiliary and Simulation Steps]
	A step  such that  is called a \emph{simulation step}, denoted by , if  is a step on the outermost channel  and the computation of the value of the received lock  will return  or it is a step on a variant of , , or .
	
	Else the step  is called an \emph{auxiliary step}, denoted by .
	\label{def:auxStepsCentral}
\end{definition}

\noindent
Let  denote the reflexive and transitive closure of  and let .
Auxiliary steps do not change the state modulo .

\begin{lemma}
	 implies  for all target terms .
	\label{lem:auxStepsCentral}
\end{lemma}

\begin{proof}
	We distinguish the following cases \wrt the channel  that is reduced in the step .
	\begin{compactenum}

		\item  is a placeholder for  and , but, in contrast to  and ,  itself is never used as a channel name. Also , and  for all source term names  are never used as channels.
		\item All variants of one of the names  except for the outermost, , and  are used as simple forwarders. If we analyse the encoding functions in Figure~\ref{fig:innerEncoding} and Figure~\ref{fig:centralised}, we observe that they are always restricted and there is exactly one replicated input and no other input on the respective variant in their scope. Thus, for all target terms  such that , all steps on such channels satisfy the conditions specified by Proposition~\ref{prop:auxStepsForward}. Hence .
		\item The name  is transmitted over  in  as initial value of . Thus, similarly to  because of Proposition~\ref{prop:auxStepsForward}, .
		\item The case of  being a variant of  follows from Proposition~\ref{prop:auxStepsReq} and Proposition~\ref{prop:reqLocksCentral}.
		\item The case of  being a variant of  follows from 2.\@ and Proposition~\ref{prop:orderLeftAnnCentral}.
		\item Variants of the names  are used to implement Boolean valued locks and an -construct testing such locks. By Proposition~\ref{prop:sumLocksCentral}, there is at most one positive instantiation of each lock and by definition all negative instantiations of the same lock---and also positive ones---are structural congruent. Since each -construct restricts its own variants of  and  and because there is never a positive and a negative instantiation of the same lock (Proposition~\ref{prop:sumLocksCentral}), all conflicts between two steps on variants of  and  result into structural congruent continuations and a step on variants of  and  cannot be in conflict with any other step on a different channel of  or its derivatives. Because  and by Lemma~\ref{lem:noConflicts}, then .
		\item Variants of the names  refer to Boolean valued locks. In the centralised encoding all announcements are propagated upwards---and on their way upwards some of them are composed---until they reach the outer layer .  ensures that only a single announcement is processed at a time. A new output on  can only be unguarded by consuming an instantiation of the lock  of the announcement that is currently processed. After the consumption of an announcement the output  triggers the computation of an instantiation of . Technically an instantiation of a lock is an output on  and the corresponding inputs are part of the -construct testing this value.  is the only part of the encoding that introduces such -constructs for variants of  and there the -constructs are guarded by an input on . Each step on  unguards a nested -construct testing a variant of  and . Since outputs on variants of  move downwards along the translation of the parallel tree of the source term and because of , no two different -constructs for the same lock are ever unguarded. By Proposition~\ref{prop:sumLocksCentral}, there is at most one positive instantiation of each lock in  and if there is a positive instantiation then there is no negative instantiation of the same lock. Thus a step reducing a positive instantiation of a lock cannot be in conflict with any other step of  or derivatives of . By Lemma~\ref{lem:noConflicts}, then .\\
			By definition, all negative instantiations of the same lock are structural congruent. Thus, since there is only a single -construct, two alternative steps that reduce different negative instantiations of the same lock result into structural congruent derivatives. All steps on other channels cannot be in conflict with a step reducing a negative instantiation of the respective lock. Because  and by Lemma~\ref{lem:noConflicts}, then .
		\item In the case of  being the outermost variant of , Definition~\ref{def:auxStepsCentral} ensures that the lock  received in this step will compute to . By induction on the parallel structure of the respective source term, we show that the encoding then ensures that all instantiations of locks that were consumed to compute the instantiation of  are restored with the same truth value. This holds, because  ensures that each combination of a positive instantiated lock from the left and a negative instantiated lock from the right causes an output . This output is propagated downwards and causes the outputs  and  for each pair of positive instantiated left and right locks combined below. In the translation of external choice these outputs on variants of  cause the unguarding of a fresh positive instantiation of the respective lock. Negative instantiations do not need to be restored, because they are introduced by  that provides as many negative instantiations as there are requests  for them.
			Also, only if the lock computes to  a positive instantiation of  is unguarded and propagated downwards. Since positive instantiations of variants of  are the only way to unguard an encoded source term continuation in the translation of external choice, a step reducing an announcement such that the respective lock will be computed to  cannot influence reachability of barbs or success.
			Thus modulo some auxiliary steps considered above, \ie modulo steps that do not change the state of the term modulo , in the present case  and  differ by the consumption of the respective announcement only. Since announcements are not success, are not observable, and, because of the negative lock, this announcement is not a translated barb, the difference between  and  is not observable by , \ie .
	\end{compactenum}
\end{proof}

By distinguishing auxiliary and simulation steps, we can prove a condition stronger than operational correspondence, namely that each source term step is simulated by exactly one simulation step.

\begin{lemma}
	 iff .
	\label{lem:sourceVsSimStep}
\end{lemma}

\begin{proof}
	Let .
	\begin{compactitem}
		\item[`if'-part:] Assume . Then either there is some source term name  such that  or . In the first case at least one action prefix  is reduced. The second case results from divergence, internal choice, concealment, or recursion.
			\begin{compactenum}
				\item The encoding translates action prefixes  into announcements with  as first value. By Observation~\ref{obs:guardedSourceVsTarget} and because the outer encoding in Figure~\ref{fig:centralised} does not guard , these announcements are unguarded for all source term action prefixes that are reduced in . By induction on the structure of the source term, we show that these announcements can be transferred all the way up in the parallel tree and are combined along this way---for each parallel operator that synchronises two -actions in the source, two announcements are combined in the translation---such that a single announcement for this action reaches the outermost -channel. The coordinator performs a step on  and then receives this announcement and requests the computation of the lock by sending . Since initially all locks are instantiated positive this computation results . As a consequence  is propagated downwards and ensures that the encodings of all source term continuations that are unguarded by  can be unguarded by auxiliary steps in the translation.
					Moreover  ensures that the consumed instantiations of locks can only be re-instantiated with the value . Let  denote the result of this simulation.
					
					The negative instantiations of the locks ensure that no step of  that is in conflict with  can be simulated by  and removes translated barbs that refer to barbs removed by .
					The only non-auxiliary step in the simulation  is the simulation step that consumes the announcement on the top of the tree on the outermost -channel, \ie . With Observation~\ref{obs:transBarbs}, Observation~\ref{obs:success}, and because in the end of the simulation the encodings of the respective source term continuations are unguarded,  and  have the same ability to reach success and reach the same translated observables. Hence .
				\item Divergence is translated into the divergent target term . By Observation~\ref{obs:guardedSourceVsTarget}, simulating  in this case requires only a single simulation step on the respective variant of . Let  be the derivative of this step. Since the steps on  are not observable modulo  in this case, we have .
				\item Internal choice  is translated into . By Observation~\ref{obs:guardedSourceVsTarget}, simulating  in this case requires only a single simulation step on the respective variant of . Let this step unguard  if  unguards  and else unguard . Let  be the derivative of this step. With Observation~\ref{obs:transBarbs}, Observation~\ref{obs:success}, and because the simulation unguards the encoding of the respective source term continuation,  and  have the same ability to reach success and reach the same translated observables. Hence .
				\item In the case of concealment the source term hides a former observable action that is simulated as in 1. The translation of concealment only adds a restriction on  and renames the first value of the announcement into  such that it is never synchronised afterwards. Thus the simulation of  in this case is similar to 1.\@ except for the steps to forward the announcement within the translation of concealment.
				\item  is translated into  and . By Observation~\ref{obs:guardedSourceVsTarget}, simulating  in this case requires only a single simulation step on the respective variant of . This step unguards an instance of . Let  be the derivative of this step. With Observation~\ref{obs:transBarbs}, Observation~\ref{obs:success}, and because the simulation unguards the encoding of the respective source term continuation,  and  have the same ability to reach success and reach the same translated observables. Hence .
			\end{compactenum}
			Thus, by induction on the structure of , the encoding  can simulate each source term step  such that .
		\item[`only-if'-part:] Assume  such that  and . By Lemma~\ref{lem:auxStepsCentral}, it suffices to concentrate on the single simulation step in . In  either exactly one announcement \wrt to a positive lock is reduced by the simulation step (1.), or there is exactly one step---namely the simulation step---on a variant of either  (2.),  (3.), or  (4.).
			\begin{compactenum}
				\item Since  neither contains steps on variants of  nor  nor , no encoded source term continuation in the translation of internal choice or recursion is unguarded. Let  such that .  reduces an announcement  such that the computation of  in  will result .	By analysing the way the lock  is computed in  we can conclude on the source term prefixes and the part of the source term parallel structure that is reflected by this simulation of a source term step. Analysing the way of the announcement we can also determine whether a source term concealment was involved.
					Because auxiliary steps cannot unguard encoded source term continuations and by Observation~\ref{obs:guardedSourceVsTarget}, then we can conclude on the structure of  and construct subject to  a source term  such that  and  results from  by reducing all action prefixes whose translation are identified by the above analyse of the way the lock  is computed. In the  the respective source term continuations are unguarded. In  only auxiliary steps are necessary to unguard the translation of these source term continuations.
					With Lemma~\ref{lem:auxStepsCentral} and because the simulation step simulates all observable effects of the step , then .
				\item Since no announcements \wrt positive instantiated locks are reduced in , no translated barb are removed and no encoded source term continuation in the translation of external choice is unguarded. Since there is no step on a variant of , no encoded source term continuation in the translation of recursion is unguarded. Instead exactly one source term encoding---without loss of generality let us call this encoded source term ---due to the translation of internal choice is unguarded. This step ensures the respective other encoded source term alternative of the internal choice can never be unguarded, \ie is modulo  similar to . This is the only effect of the steps  that can be observed modulo . Therefore this internal choice translation has to be unguarded in , because auxiliary steps cannot unguard encoded source term continuations. By Observation~\ref{obs:guardedSourceVsTarget}, then  contains an unguarded internal choice with  as one of the alternatives. Then  such that this step resolves the internal choice and unguards . With Lemma~\ref{lem:auxStepsCentral} and because the simulation step simulates all observable effects of the step , then .
				\item Since no announcements \wrt positive instantiated locks are reduced in , no translated barb are removed and no encoded source term continuation in the translation of external choice is unguarded. Since there is no step on a variant of , no encoded source term continuation in the translation of internal choice is unguarded. Since the simulation step reduces a variant of , we have . Moreover, in this case,  is unguarded in , because auxiliary steps cannot unguard encoded source term continuations. By Observation~\ref{obs:guardedSourceVsTarget}, then  is unguarded in . Then  such that this step reduces . With Lemma~\ref{lem:auxStepsCentral} and because the simulation step simulates all observable effects of the step , then .
				\item Since no announcements \wrt positive instantiated locks are reduced in , no translated barb are removed and no encoded source term continuation in the translation of external choice is unguarded. Since there is no step on a variant of , no encoded source term continuation in the translation of internal choice is unguarded. Instead exactly one source term encoding---without loss of generality let us call this encoded source term ---due to the translation of recursion is unguarded. This is the only effect of the steps  that can be observed modulo . Therefore  is unguarded in , because auxiliary steps cannot unguard encoded source term continuations. By Observation~\ref{obs:guardedSourceVsTarget}, then  is unguarded in . Then  such that this step unfolds recursion and unguards . With Lemma~\ref{lem:auxStepsCentral} and because the simulation step simulates all observable effects of the step , then .
			\end{compactenum}
	\end{compactitem}
\end{proof}

\noindent
This direct correspondence between source term steps and the points of no return of their translation allows us to prove a variant of operational correspondence that is significantly stricter than the variant proposed in \cite{gorla10}.

\begin{definition}[Operational Correspondence]
	\\
	An encoding  is \emph{operationally corresponding} \wrt  if it is:
	\begin{compactitem}
		\item[\; Complete:]  implies 
		\item[\; Sound:]  implies 
	\end{compactitem}
\end{definition}

\noindent
The `if'-part of Lemma~\ref{lem:sourceVsSimStep} implies operational completeness \wrt  and the `only-if'-part contains the main argument for operational soundness \wrt . Hence  is operational corresponding \wrt to .

\begin{theorem}
	The encoding  is operational corresponding \wrt to .
	\label{thm:operationalCorrespondenceCentral}
\end{theorem}

\begin{proof}
	Completeness--- implies ---follows from the `if'-part of Lemma~\ref{lem:sourceVsSimStep} and an induction on the number of steps in .
	
	Soundness--- implies ---follows from Lemma~\ref{lem:auxStepsCentral}, the `only-if'-part of Lemma~\ref{lem:sourceVsSimStep}, and an induction on the number of simulation steps in .
\end{proof}

To obtain divergence reflection we show that there is no infinite sequence of only auxiliary steps.

\begin{lemma}
	The number of steps between two simulation steps is finite.
	\label{lem:numberStepsCentral}
\end{lemma}

\begin{proof}
	Let  be such that .
	There are only finitely many unguarded translations of encodings of source term operators in . Let  be the result of unguarding all translations of source term parts that can be unguarded using only auxiliary steps in . By induction on the number of simulation steps in  the number of such auxiliary steps is finite. Since we consider only sequences  without simulation steps, no derivative of  in this sequence can unguard additional translations of source term operators. The binary tree that results from the nesting of unguarded translations of parallel operator encodings in  and its derivatives is denoted as parallel tree in the following. Auxiliary steps are steps on the following kinds of channels:
	\begin{compactenum}
		\item Since we consider only sequences  without simulation steps, there is at most one step on  in this sequence.
		\item Steps on variants of  are used to propagate announcements through the parallel tree. Since this tree is finite and because the encoding introduces one announcement per action prefix, there are only finitely many announcements in the leafs of the parallel tree. Announcements are only propagated upwards to surrounding translations of concealment and parallel operators (of which there are only finitely many). Within the nodes of the parallel tree announcements from the left and announcements from the right are combined using variants of  and, to unguard inputs on such channels, steps on variants of  are used. By induction over the depth of the binary tree, we show that there are always only finitely many announcements from the left and finitely many announcements from the right and thus their combinations are performed by finitely many steps. Accordingly,  contains only finitely many steps on variants of .
		\item Steps on variants of  are used to trigger the computation of locks. Since we consider only sequences  without simulation steps, there is at most one request  proposed by the coordinator in this sequence. Additionally  and  can already contain unguarded requests, but only finitely many. The request  from the top of the parallel tree is propagated downwards by pushing one or two more such requests (in some nodes) on variants of  for each consumed request. Since the depth of the parallel tree is finite,  contains only finitely many steps on variants of .
		\item Steps on variants of  are used to implement and test Boolean valued locks. For each step on variants of  only a single instantiation of a lock can be consumed. By 3., there are only finitely many such steps. Additionally  and  can already contain unguarded instantiations of locks and -constructs, but only finitely many. Since each consumption of a single instantiation of a lock and its test in a -construct requires only finitely many steps,  contains only finitely many steps on variants of .
		\item  and  can only contain finitely many unguarded outputs on variants of . Additional outputs on variants of  can only be unguarded by testing the value of a lock. By 4., there are only finitely many tests of locks in . Thus there are only finitely many steps on variants of .
	\end{compactenum}
	Thus no sequence of auxiliary steps of  is infinite.
\end{proof}

Then divergence reflection follows from the combination of the above Lemma and Lemma~\ref{lem:sourceVsSimStep}.

\begin{theorem}
	The encoding  reflects divergence.
	\label{thm:divergenceReflectionCentral}
\end{theorem}

\begin{proof}
	If  is divergent then, by Lemma~\ref{lem:numberStepsCentral},  can perform an infinite sequence of steps containing infinitely many simulation steps. With Lemma~\ref{lem:sourceVsSimStep}, then  is divergent.
\end{proof}

The encoding function ensures that  has an unguarded occurrence of  iff  has such an unguarded occurrence. Operational correspondence ensures that  and  also answer the question for the reachability of  in the same way.

\begin{theorem}
	The encoding  is success sensitive.
	\label{thm:successSensitivenessCentral}
\end{theorem}

\begin{proof}
	From Observation~\ref{obs:success} and Figure~\ref{fig:centralised},  iff . With Theorem~\ref{thm:operationalCorrespondenceCentral} and because  respects , then  iff .
\end{proof}

In a similar way we can prove that a source term reaches a barb iff its translation reaches the respective translated barb.

\begin{theorem}
	 iff 
	\label{thm:respectsBarbsCentral}
\end{theorem}

\begin{proof}
	From Observation~\ref{obs:transBarbs} and Figure~\ref{fig:centralised},  iff . With Lemma~\ref{lem:sourceVsSimStep} and because  respects translated barbs, then  iff .
\end{proof}

As proved in \cite{petersGlabbeek15}, Theorem~\ref{thm:operationalCorrespondenceCentral}, the fact that  is success sensitive and respects (translated) barbs, Theorem~\ref{thm:successSensitivenessCentral}, and Theorem~\ref{thm:respectsBarbsCentral} imply that for all  it holds  and  are (success sensitive, (translated) barb respecting, weak, reduction) bisimilar, \ie .
Bisimilarity is a strong relation between source terms and their translation. On the other hand, because of efficiency, distributability preserving encodings are more interesting.
Because of  the encoding  obviously does not preserves distributability. As discussed in \cite{parrowCoupled92} bisimulation often forbids for distributed encodings. Instead they propose coupled simulation as relation that still provides a strong connection between source terms and their translations but is more flexible. Following the approach in \cite{parrowCoupled92} we consider a de-centralised coordinator next.

\section{The De-Centralised Encoding}
\label{sec:decentral}

\begin{figure}
	
	\caption{A \textbf{de-centralised} encoding from CSP into CCS with value passing.}
	\label{fig:decentralised}
\end{figure}

Figure~\ref{fig:decentralised} presents a de-centralised variant of the coordinator in Figure~\ref{fig:centralised}.
The only difference between the centralised and the de-centralised version of the coordinator is that the latter can request to check different locks concurrently. Technically  and  differ only by the use of . As a consequence the steps of different simulation attempts can overlap and even (pre-processing) steps of simulations of conflicting source term steps can interleave to a certain degree. Because of this effect,  does not satisfy the version of operational correspondence used above for , but  satisfies weak operational correspondence that was proposed in \cite{gorla10} as part of a set of quality criteria.

Similar to the central coordinator, the de-central coordinator respects the protocol on locks used to ensure that each announcement is only used once to simulate a source term step, \ie it preserves the properties of locks formulated in Lemma~\ref{lem:sumLocks}.

\begin{prop}
	Let  such that . Then for each variant  of the names 
	\begin{compactenum}
		\item there is at most one positive instantiation of  in ,
		\item if there is a positive instantiation of  in  then there is no other instantiation of  in , and
		\item if there is a negative instantiation of  in  then no derivative of  contains a positive instantiation of .
	\end{compactenum}
	\label{prop:sumLocksDecentral}
\end{prop}

\begin{proof}
	The encoding in Figure~\ref{fig:decentralised} does not introduce new instantiations of . It does provide additional instantiations of , but to unguard them a positive instantiation of the corresponding lock  has to be consumed. Thus  preserves the properties of locks formulated in Lemma~\ref{lem:sumLocks}.
\end{proof}

Similarly Lemma~\ref{lem:reqLocks} is preserved.

\begin{prop}
	Let  such that . Then for each variant  of the names  there is at most one (replicated) input on  in .
	\label{prop:reqLocksDecentral}
\end{prop}

\begin{proof}
	Follows from Lemma~\ref{lem:reqLocks} and Proposition~\ref{prop:sumLocksDecentral}, because the encoding in Figure~\ref{fig:decentralised} provides additional instantiations of , but to unguard them a positive instantiation of the corresponding lock  has to be consumed.
\end{proof}

The encoding in Figure~\ref{fig:decentralised} does not use variants of the names , and . Because of that, Lemma~\ref{lem:orderLeftAnn} is preserved by .

\begin{prop}
	Let  such that  and  reduces an output on . Moreover assume that for all steps  on a variant of  with  it holds .
	Then .
	\label{prop:orderLeftAnnDecentral}
\end{prop}

\begin{proof}
	The encoding in Figure~\ref{fig:decentralised} does not use variants of the names , and . Thus this Proposition follows from Lemma~\ref{lem:orderLeftAnn}.
\end{proof}

Since several announcements can be processed concurrently by the de-central coordinator, here all consumptions of announcements are auxiliary steps. Instead the consumption of positive instantiations of locks can mark a point of no return. In contrast to  not every point of no return in  unambiguously marks a simulation of a single source term step, because in contrast to  the encoding  introduces \emph{partial commitments} \cite{peters12,petersNestmann12}.

Consider the example .

\noindent
  \begin{minipage}[c]{0.3\textwidth-2pt}
      \begin{tikzpicture}[auto,node distance=1.2cm]
        \node (E)                        {};
        \node (T)    [right of=E]        {};
        \node (T2)    [right=of T]        {};
        \node (T1)    [above of=T2]        {};
        \node (T3)    [below of=T2]        {};

        \draw[|->,shorten >=-1pt,shorten <=-0.5pt] (E) -- (T);
        \draw[double] (E) -- (T);
        \draw[|->,shorten >=-1pt,shorten <=-0.5pt] (T) -- (T1.west) node [near end, below, rotate=40, scale = 0.7] {};
        \draw[double] (T) -- (T1.west);
        \draw[|->,shorten >=-1pt,shorten <=-0.5pt] (T) -- (T2.west) node [at end, below, scale = 0.7] {};
        \draw[double] (T) -- (T2.west);
        \draw[|->,shorten >=-1pt,shorten <=-0.5pt] (T) -- (T3.west) node [at end, below, rotate=-40, scale = 0.7] {};
        \draw[double] (T) -- (T3.west);
      \end{tikzpicture}
  \end{minipage}
  \begin{minipage}[c]{0.3\textwidth-2pt}
    \begin{center}
      \begin{tikzpicture}[auto,node distance=1.2cm]
        \node (T)                        {};
        \node (d1)    [right of=T]        {};
        \node (T2)    [right of=d1]        {};
        \node (T1)    [above of=T2]        {};
        \node (T3)    [below of=T2]        {};

        \draw[|->,shorten >=-1pt,shorten <=-0.5pt] (T) -- (T1.west) node [at end, below, rotate=40, scale = 0.7] {};
        \draw[|->,shorten >=-1pt,shorten <=-0.5pt] (T) -- (T2.west) node [at end, below, scale = 0.7] {};
        \draw[|->,shorten >=-1pt,shorten <=-0.5pt] (T) -- (T3.west) node [at end, below,rotate=-40, scale = 0.7] {};
      \end{tikzpicture}
      \end{center}
  \end{minipage}
  \begin{minipage}[c]{0.4\textwidth-2pt}
      \begin{tikzpicture}[auto,node distance=1.2cm]
        \node (E)                        {};
        \node (T)    [right of=E]        {};
        \node (P2)    [right=1cm of T]        {};
        \node (P1)    [above of=P2]        {};
        \node (T12)    [right=1cm of P2]        {};
        \node (T11)    [above of=T12]        {};
        \node (T3)    [below of=P2]        {PC_{1}\barbBisim \EncDO{P_{3}}};

        \draw[|->,shorten >=-1pt,shorten <=-0.5pt] (E) -- (T);
        \draw[double] (E) -- (T);
        \draw[|->,shorten >=-1pt,shorten <=-0.5pt] (T) -- (P1);
        \draw[double] (T) -- (P1);
        \draw[|->,shorten >=-1pt,shorten <=-0.5pt] (T) -- (P2);
        \draw[double] (T) -- (P2);
        \draw[|->,shorten >=-1pt,shorten <=-0.5pt] (P1) -- (T11.west) node [near end, below, scale = 0.7] {};
        \draw[double] (P1) -- (T11.west);
        \draw[|->,shorten >=-1pt,shorten <=-0.5pt] (P1) -- (T12.west) node [at end, below, rotate=-40, scale = 0.7] {};
        \draw[double] (P1) -- (T12.west);
        \draw[|->,shorten >=-1pt,shorten <=-0.5pt] (T) -- (T3.west) node [at end, below, rotate=-43, scale = 0.7] {};
        \draw[double] (T) -- (T3.west);
      \end{tikzpicture}
  \end{minipage}

  In the example, two sides of a parallel operator have to synchronise on either action , or action , or action  happens without synchronisation.
  In the centralised encoding  the use of  ensures that different simulation attempts cannot overlap. Thus, only after finishing the simulation of a source term step, the simulation of another source term step can be invoked. As a consequence each state reachable from encoded source terms can unambiguously be mapped to a single state of the source term. This allows us to use a stronger version of operational correspondence and, thus, to prove that source terms and their translations are bisimilar. The corresponding 1-to-1 correspondence between source terms and their translations is visualised by the first two graphs above, where .

  The de-centralised encoding  introduces partial commitments.
  Assume the translation of a source term that offers several alternative ways to be reduced. Then some encodings---as our de-central one---do not always decide on which of the source term steps should be simulated next. More precisely a partial commitment refers to a state reachable from the translation of a source term in that already some possible simulations of source term steps are ruled out, but there is still more than a single possibility left.

  In the de-centralised encoding announcements can be processed concurrently and parts of different simulation attempts can interleave. The only blocking part of the decentralised encoding are conflicting attempts to consume the same positive instantiation of a lock.
  In the presented example above there are two locks; one for each side of the parallel operator. The simulations of the step on  and  need both of these locks, whereas to simulate the step on  only a positive instantiation of the right lock needs to be consumed.
  By consuming the positive instantiation of the left lock in an attempt to simulate the step on , the simulation of the step on  is ruled out, but the simulation of the step on  is still possible. Since either the simulation of the step on  or the simulation of the step on  succeeds, the simulation of the step on  is not only blocked but ruled out. But the consumption of the instantiation of the left lock does not unambiguously decide between the remaining two simulations. The intermediate state that results from consuming the instantiation of the left lock and represents a partial commitment is visualised in the right graph above by the state .

  Partial commitments forbid a 1-to-1 mapping between the states of  a source term and its translations by a bisimulation. But, as shown in \cite{parrowCoupled92}, partial commitments do not forbid to relate source terms and their translations by coupled similarity.

Whether the consumption of a positive instantiation of a lock is an auxiliary step---does not change the state of the term modulo ---, is a partial commitment, or unambiguously marks a simulation of a single source term step depends on the surrounding term, \ie cannot be determined without the context. For simplicity we consider all steps that reduce a positive instantiation of a lock as simulation steps.
Also steps on variants of the channels , , and  are simulation steps, because they unambiguously mark a simulation of a single source term step. All remaining steps of the de-centralised encoding are auxiliary.

\begin{definition}[Auxiliary and Simulation Steps]
	A step  such that  is called a \emph{simulation step}, denoted by , if  reduces a positive instantiation of a lock or is a step on a variant of , , or .
	
	Else the step  is called an \emph{auxiliary step}, denoted by .
	\label{def:auxStepsDecentral}
\end{definition}

\noindent
Again let  denote the reflexive and transitive closure of  and let .
Since auxiliary steps do not introduce partial commitments, they do not change the state modulo . The proof of this lemma is very similar to the central case.

\begin{lemma}
	 implies  for all target terms .
	\label{lem:auxStepsDecentral}
\end{lemma}

\begin{proof}
	We distinguish the following cases \wrt the channel  that is reduced in the step .
	\begin{compactenum}

		\item  is a placeholder for  and , but, in contrast to  and ,  itself is never used as a channel name. Also , and  for all source term names  are never used as channels.
		\item All variants of one of the names , and  are used as simple forwarders. If we analyse the encoding functions in Figure~\ref{fig:innerEncoding} and Figure~\ref{fig:decentralised}, we observe that they are always restricted and there is exactly one replicated input and no other input on the respective variant in their scope. Thus, for all target terms  such that , all steps on such channels satisfy the conditions specified by Proposition~\ref{prop:auxStepsForward}. Hence .
		\item The name  is transmitted over  in  as initial value of . Thus, similarly to  because of Proposition~\ref{prop:auxStepsForward}, .
		\item The case of  being a variant of  follows from Proposition~\ref{prop:auxStepsReq} and Proposition~\ref{prop:reqLocksDecentral}.
		\item The case of  being a variant of  follows from 2.\@ and Proposition~\ref{prop:orderLeftAnnDecentral}.
		\item Variants of the names  are used to implement Boolean valued locks and an -construct testing such locks. By Proposition~\ref{prop:sumLocksDecentral}, there is at most one positive instantiation of each lock and by definition all negative instantiations of the same lock---and also positive ones---are structural congruent. Since each -construct restricts its own variants of  and  and because there is never a positive and a negative instantiation of the same lock (Proposition~\ref{prop:sumLocksDecentral}), all conflicts between two steps on variants of  and  result into structural congruent continuations and a step on variants of  and  cannot be in conflict with any other step on a different channel of  or its derivatives. Because  and by Lemma~\ref{lem:noConflicts}, then .
		\item Variants of the names  refer to Boolean valued locks. Again all announcements are propagated upwards---and on their way upwards some of them are composed---until they reach the outer layer . The de-central coordinator can process several announcements concurrently. Because of that conflicts result from different attempts to consume the same positive instantiation of a lock. However auxiliary steps can only consume negative instantiations of locks.
			By definition, all negative instantiations of the same lock are structural congruent. Moreover the encoding ensures that, as soon as the first negative instantiation of a lock is unguarded, as many negative instantiations of this lock are available as there are requests of it. The test of a negatively instantiated lock---that consumes an instantiation and reduces an -construct---always reduces to the -case such that the inner part of a nested -construct is not unguarded. Any unguarding of a -construct also releases a request on the tested lock.
			Thus, if there are two negative instantiations for the same lock, it does not matter (modulo structural congruence) which one is reduced by a -construct. Similarly, if there are two -construct testing the same lock, both can be processed concurrently and it does not matter (modulo structural congruence) which consumes which instantiation.
			All steps on other channels cannot be in conflict with a step reducing a negative instantiation of the respective lock. Because  and by Lemma~\ref{lem:noConflicts}, then .
	\end{compactenum}
\end{proof}

In contrast to the centralised encoding, the simulation of a source term step in the de-centralised encoding can require more than a single simulation step and a single simulation step not unambiguously refers to the simulation of a particular source term step. The partial commitments described above forbid for operational correspondence, but the weaker variant proposed in \cite{gorla10} is satisfied. We call this variant weak operational correspondence.

\begin{definition}[Weak Operational Correspondence]
	\\
	An encoding  is \emph{weakly operationally corresponding} \wrt  if it is:
	\begin{compactitem}
		\item[\; Complete:]  implies 
		\item[\; Weakly Sound:]  implies 
	\end{compactitem}
\end{definition}

The only difference to operational correspondence is the weaker variant of soundness that allows for  to be an intermediate state that does not need to be related to a source term directly. Instead there has to be a way from  to some  such that  is related to a source term.

\begin{theorem}
	The encoding  is weakly operational corresponding \wrt to .
	\label{thm:operationalCorrespondenceDecentral}
\end{theorem}

\begin{proof}
	\begin{compactitem}
		\item[Completeness:]  implies .\\
			We consider a single step . Completeness then follows by induction on the number of steps in .
			
			Assume . Since  and  differ only by the use of , the simulation of source term steps is similar except for the one step on channel . Hence the existence of  such that  and  can be proved by adapting the `if'-part of Lemma~\ref{lem:sourceVsSimStep} \wrt the step on .
		\item[Weak Soundness:]  implies .\\
			By Lemma~\ref{lem:auxStepsDecentral}, it suffices to concentrate on the simulation steps in the sequence . The proof is by induction on the number of simulation steps in the sequence .
			
			In the \emph{base case}---without any simulation steps in ---choose  and  then , , and, by Lemma~\ref{lem:auxStepsDecentral}, .
			
			Assume that there are  such that , , , and  contains only simulation steps necessary to resolve partial commitments (\emph{induction hypothesis}).
			
			Consider .
			\begin{compactenum}
				\item A simulation step  that consumes a positive lock can result in partial commitment, but only in the case the respective reduced -construct was the first part of a nested -construct and the second part tests a lock  of that a positive instantiation is (modulo auxiliary steps) still available. Switching the positive instantiation of  into a negative instantiation---regardless of which -construct is used to do so---resolves the partial commitment. The sequence  might already introduce several more of such partial commitments. Proposition~\ref{prop:sumLocksDecentral} ensures some important properties over the instantiation of locks but it does not ensure, that for all locks there will eventually be an instantiation available. Only -construct consume instantiations of locks. After being reduced, they restore all positive instantiation they consumed or turn them into negative instantiations. Negative instantiations remain available. Thus, to ensure that there are no deadlocks and all partial commitments can be resolved, we have to show that -constructs cannot completely block each other. In the case of the centralised encoding this follows from the use of . In the de-central encoding we make use of the same technique already used in \cite{peters12, petersNestmann15} to avoid this problem. As proved in \cite{peters12}, because we always consume first the instantiation of the lock from the left the nested -constructs cannot all be blocked and we can resolve them step by step.
					
					In the present case the step  might consume an instantiation of a lock that was necessary for the sequence . If that is not the case, no step of  is in conflict with . Because of ,  does not contain unresolved partial commitments. Hence we can choose  as the result of performing all steps of  in  followed, if necessary, by a sequence with a single simulation step  to resolve the partial commitment that may result from  such that . Then choose , if no additional step was necessary to obtain , else  and the additional simulation step are all simulation steps of the simulation of a source term step reducing action-prefixes and we choose  as the result of performing the respective source term step in . Thus . Because of  and the construction of  and , we have .
					
					Else, if there is a conflict between  and a step of , choose  as the result of applying all but the conflicting step (and all auxiliary steps that depend on this step) of  in  followed, if necessary, by a sequence with a single simulation step  to resolve the partial commitment that may result from . Because the induction hypotheses ensures that  contains only simulation steps necessary to resolve partial commitments, there are no simulation steps that depend on the conflicting step and all other simulation steps of  can be transferred to . Thus . As a consequence of this replacement the simulation of a single source term step is replaced by the simulation of another single source term step. Choose  as the result of replacing in  the respective source term step such that . Because of , the construction of  and , and Observation~\ref{obs:guardedSourceVsTarget}, we have .
				\item A simulation step  on a variant of  unguards exactly one source term encoding---let us call it ---due to the translation of internal choice. This step ensures that the respective other encoded source term alternative---let us call it ---of the internal choice can never be unguarded, \ie is modulo  similar to . This is the only effect of the steps  that can be observed modulo .
					Since the encoding restricts each variant of , the only step that can be in conflict with this step is the step unguarding . Because steps on a variant of  do not resolve partial commitments,  does not contain such a step. Then, because  can perform the step, also  contains (modulo structural congruence) the unguarded subterm . By Observation~\ref{obs:guardedSourceVsTarget} and because , then  is unguarded in .
					Hence we can choose  by replacing  in  by  such that  and we can choose  by replacing  in  by  such that .
					By Observation~\ref{obs:guardedSourceVsTarget}, because of , and by the construction of  and , then .
				\item A simulation step  on a variant of  is due to the translation of . In this case . Then, because of , there exists  such that ,  has the same simulation steps then , and . Thus we can choose .
				\item A simulation step  on a variant of  is due to the translation of recursion. In this case  unguards exactly one source term encoding---let us call it . This is the only effect of the steps  that can be observed modulo .
					Since the encoding restricts each variant of  and the only input on this channel is replicated, this step is not in conflict with any other step of  or its derivatives. Because steps on a variant of  do not resolve partial commitments,  does not contain such a step. Then, because  can perform the step, also  contains (modulo structural congruence) the unguarded subterm . By Observation~\ref{obs:guardedSourceVsTarget} and because , then  is unguarded in .
					Hence we can choose  as the result of replacing  in  by  such that  and we can choose  as the result of replacing  in  by , where all occurrences of  in  are translated to , such that .
					By Observation~\ref{obs:guardedSourceVsTarget}, because of , and by the construction of  and , then .
			\end{compactenum}
	\end{compactitem}
\end{proof}

As in the encoding , there is no infinite sequence of only auxiliary steps in .

\begin{lemma}
	The number of steps between two simulation steps is finite.
	\label{lem:numberStepsDecentral}
\end{lemma}

\begin{proof}
	In contrast to , also the consumption of announcements by the coordinator is a simulation step for . Since there are only finitely many announcements, consuming them does not lead to divergence. Because of the consumption of announcements, the coordinator can release several requests , but again only finitely many. Accordingly, the sequences of auxiliary steps can be longer in , but all such sequences result from interleaving finitely many sequences of auxiliary steps of .
	Apart from these observations the proof is similar to the proof of Lemma\ref{lem:numberStepsCentral}.
\end{proof}

Moreover each simulation of a source term requires only finitely many simulation steps (to consume the respective positive instantiations of locks). Thus  reflects divergence.

\begin{theorem}
	The encoding  reflects divergence.
	\label{thm:divergenceReflectionDecentral}
\end{theorem}

\begin{proof}
	If  is divergent then, by Lemma~\ref{lem:numberStepsDecentral},  can perform an infinite sequence of steps containing infinitely many simulation steps.
	Simulation steps either directly represent the simulation of a source term step---as in the case of recursion, divergence, and internal choice---or reduce a positive instantiation of a lock.
	Instantiations of locks are consumed by -constructs. These constructs are guarded by requests .
	By Lemma~\ref{lem:numberStepsDecentral}, without simulation steps only finitely many requests and thus instantiations of locks can be consumed. Simulation steps can only lead to new requests if they unguard the translation of a source term continuation, but then the simulation of a source term step was completed.
	Hence, if  is divergent, then  is divergent.
\end{proof}

The encoding function ensures that  has an unguarded occurrence of  iff  has such an unguarded occurrence. Operational correspondence again ensures that  and  also answer the question for the reachability of  in the same way.

\begin{theorem}
	The encoding  is success sensitive.
	\label{thm:successSensitivenessDecentral}
\end{theorem}

\begin{proof}
	From Observation~\ref{obs:success} and Figure~\ref{fig:decentralised},  iff . With Theorem~\ref{thm:operationalCorrespondenceDecentral} and because  respects , then  iff .
\end{proof}

Similarly, a source term reaches a barb iff its translation reaches the respective translated barb.

\begin{theorem}
	 iff .
	\label{thm:respectsBarbsDecentral}
\end{theorem}

\begin{proof}
	From Observation~\ref{obs:transBarbs} and Figure~\ref{fig:decentralised},  iff . With Theorem~\ref{thm:operationalCorrespondenceDecentral} and because  respects translated barbs, then  iff .
\end{proof}

Weak operational correspondence does not suffice to establish a bisimulation between source terms and their translations.
But, as proved in \cite{petersGlabbeek15}, Theorem~\ref{thm:operationalCorrespondenceDecentral}, the fact that  is success sensitive and respects (translated) observables, Theorem~\ref{thm:successSensitivenessDecentral}, and Theorem~\ref{thm:respectsBarbsDecentral} imply that  and  are (success sensitive, (translated) barbs respecting, weak, reduction) coupled similar, \ie .

It remains to show, that  indeed preserves distributability. Therefore we prove that all blocking parts of the encoding  refer to simulations of conflicting source term steps.

\begin{theorem}
	The encoding  preserves distributability.
	\label{thm:distributability}
\end{theorem}

\begin{proof}
	The de-central coordinator in Figure~\ref{fig:decentralised} computes announcements concurrently. The test of locks is technically an output and steps on  and  are restricted such that these steps are never in conflict to any other step. Thus the de-central coordinator itself does not block the concurrent simulation of distributable steps.
	
	In  all blocking, \ie all not-replicated inputs, are on variants of . Two steps on the same variant of  belong to two simulation attempts of source term steps on the same action that needs to be synchronised by a parallel operator. Since such steps are also not distributable in the source, their simulations do not have to be distributable.
	
	Two steps on the same variant of one of the names  belong to simulation attempts that need to consume the same positive instantiation of a lock. Thus these two attempts clearly try to simulate conflicting source term steps. Hence again the two simulation attempts do not have to be distributable.
	
	Similarly two steps on the same variant of  clearly belong to two simulation attempts of conflicting source term steps. Thus again they do not have to be distributable.
	
	We conclude that the simulations of distributable source terms are distributable, \ie  preserves distributability.
\end{proof}


\section{Conclusions}
\label{sec:conclusion}

We introduced two encodings from CSP into asynchronous CCS with name passing and matching.
As in \cite{parrowCoupled92} we had to encode the multiway synchronisation mechanism of CSP into binary communications and, similarly to \cite{parrowCoupled92}, we did so first using a central controller that was then modified into a de-central controller.
By doing so we were able to transfer the observations of \cite{parrowCoupled92} to the present case:
\begin{compactenum}
	\item The central solution allows to prove a stronger connection between source terms and their translations, namely by bisimilarity. Our de-central solution does not relate source terms and their translations that strongly and we doubt that any de-central solution can do so.
	\item Nonetheless, de-central solutions are possible as presented by the second encoding and they still relate source terms and their translations in an interesting way, namely by coupled similarity.
\end{compactenum}
Thus as in \cite{parrowCoupled92} we observed a trade-off between \emph{central} but \emph{bisimilar} solutions on the one-hand side and \emph{coupled similar} but \emph{de-central} solutions on the other side.

More technically we showed here instead a trade-off between central but \emph{operational corresponding} solutions on the one-hand side and \emph{weakly operational corresponding} but de-central solutions on the other side.
The mutual connection between operational correspondence and bisimilarity as well as between weak operational correspondence and coupled similarity is proved in \cite{petersGlabbeek15}.

Both encodings make strict use of the renaming policy and translate into closed terms.
Hence the criterion \emph{name invariance} is trivially satisfied in both cases.
Moreover we showed that both encodings are \emph{success sensitive}, \emph{reflect divergence}, and even \emph{respect barbs} \wrt to the standard source term (CSP) barbs and a notion of translated barbs on the target.
The centralised encoding  additionally satisfies a variant of \emph{operational correspondence} that is stricter than the variant proposed in \cite{gorla10}.
The de-centralised encoding  satisfies \emph{weak operational correspondence} as proposed in \cite{gorla10} and \emph{distributability preservation} as proposed in \cite{petersNestmannGoltz13}.
Thus both encodings satisfy all of the criteria proposed in \cite{gorla10} except for compositionality.
However in both cases the inner part is obviously compositional and the outer part adds only a fixed context.

\begin{thebibliography}{10}
\providecommand{\bibitemdeclare}[2]{}
\providecommand{\surnamestart}{}
\providecommand{\surnameend}{}
\providecommand{\urlprefix}{Available at }
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\href}[2]{\texttt{#2}}
\providecommand{\urlalt}[2]{\href{#1}{#2}}
\providecommand{\doi}[1]{doi:\urlalt{http://dx.doi.org/#1}{#1}}
\providecommand{\bibinfo}[2]{#2}

\bibitemdeclare{article}{Baeten:2005:BHP:1085667.1085669}
\bibitem{Baeten:2005:BHP:1085667.1085669}
\bibinfo{author}{J.~C.~M. \surnamestart Baeten\surnameend}
  (\bibinfo{year}{2005}): \emph{\bibinfo{title}{{A Brief History of Process
  Algebra}}}.
\newblock {\sl \bibinfo{journal}{Theor. Comput. Sci.}}
  \bibinfo{volume}{335}(\bibinfo{number}{2-3}), pp. \bibinfo{pages}{131--146},
  \doi{10.1016/j.tcs.2004.07.036}.

\bibitemdeclare{inproceedings}{DBLP:conf/pstv/Brinksma85}
\bibitem{DBLP:conf/pstv/Brinksma85}
\bibinfo{author}{E.~\surnamestart Brinksma\surnameend} (\bibinfo{year}{1985}):
  \emph{\bibinfo{title}{A tutorial on {LOTOS}}}.
\newblock In: {\sl \bibinfo{booktitle}{Proc. of PSTV}}, pp.
  \bibinfo{pages}{171--194}.

\bibitemdeclare{inproceedings}{DBLP:conf/icalp/Brookes83}
\bibitem{DBLP:conf/icalp/Brookes83}
\bibinfo{author}{S.~D. \surnamestart Brookes\surnameend}
  (\bibinfo{year}{1983}): \emph{\bibinfo{title}{On the Relationship of {CCS}
  and {CSP}}}.
\newblock In: {\sl \bibinfo{booktitle}{Proc. of ICALP}}, {\sl
  \bibinfo{series}{LNCS}} \bibinfo{volume}{154}, pp. \bibinfo{pages}{83--96},
  \doi{10.1007/BFb0036899}.

\bibitemdeclare{inproceedings}{7092761}
\bibitem{7092761}
\bibinfo{author}{H.~\surnamestart Evrard\surnameend} \&
  \bibinfo{author}{F.~\surnamestart Lang\surnameend} (\bibinfo{year}{2015}):
  \emph{\bibinfo{title}{Automatic Distributed Code Generation from Formal
  Models of Asynchronous Concurrent Processes}}.
\newblock In: {\sl \bibinfo{booktitle}{Proc. of PDP}},
  \bibinfo{publisher}{IEEE}, pp. \bibinfo{pages}{459--466},
  \doi{10.1109/PDP.2015.96}.

\bibitemdeclare{inproceedings}{DBLP:journals/corr/abs-1208-2750}
\bibitem{DBLP:journals/corr/abs-1208-2750}
\bibinfo{author}{R.~\surnamestart van Glabbeek\surnameend}
  (\bibinfo{year}{2012}): \emph{\bibinfo{title}{{Musings on Encodings and
  Expressiveness}}}.
\newblock In: {\sl \bibinfo{booktitle}{Proc. of EXPRESS/SOS 2012}}, {\sl
  \bibinfo{series}{EPTCS}}~\bibinfo{volume}{89}, pp. \bibinfo{pages}{81--98},
  \doi{10.4204/EPTCS.89.7}.

\bibitemdeclare{article}{gorla10}
\bibitem{gorla10}
\bibinfo{author}{D.~\surnamestart Gorla\surnameend} (\bibinfo{year}{2010}):
  \emph{\bibinfo{title}{{Towards a Unified Approach to Encodability and
  Separation Results for Process Calculi}}}.
\newblock {\sl \bibinfo{journal}{Information and Computation}}
  \bibinfo{volume}{208}(\bibinfo{number}{9}), pp. \bibinfo{pages}{1031--1053},
  \doi{10.1016/j.ic.2010.05.002}.

\bibitemdeclare{inproceedings}{hatzel15}
\bibitem{hatzel15}
\bibinfo{author}{M.~\surnamestart Hatzel\surnameend},
  \bibinfo{author}{C.~\surnamestart Wagner\surnameend},
  \bibinfo{author}{K.~\surnamestart Peters\surnameend} \&
  \bibinfo{author}{U.~\surnamestart Nestmann\surnameend}
  (\bibinfo{year}{2015}): \emph{\bibinfo{title}{{Encoding CSP into CCS}}}.
\newblock In: {\sl \bibinfo{booktitle}{Proc. of EXPRESS/SOS}},
  \bibinfo{series}{EPTCS}.
\newblock \bibinfo{note}{To Appear.}

\bibitemdeclare{article}{hoare:78csp}
\bibitem{hoare:78csp}
\bibinfo{author}{C.A.R. \surnamestart Hoare\surnameend} (\bibinfo{year}{1978}):
  \emph{\bibinfo{title}{{Communicating Sequential Processes}}}.
\newblock {\sl \bibinfo{journal}{Communications of the ACM}}
  \bibinfo{volume}{21}(\bibinfo{number}{8}), pp. \bibinfo{pages}{666--677},
  \doi{http://dx.doi.org/10.1145/359576.359585}.

\bibitemdeclare{article}{Hoare2006209}
\bibitem{Hoare2006209}
\bibinfo{author}{C.A.R. \surnamestart Hoare\surnameend} (\bibinfo{year}{2006}):
  \emph{\bibinfo{title}{Why ever CSP?}}
\newblock {\sl \bibinfo{journal}{Electronic Notes in Theoretical Computer
  Science}} \bibinfo{volume}{162}(\bibinfo{number}{0}), pp.
  \bibinfo{pages}{209--215},
  \doi{http://dx.doi.org/10.1016/j.entcs.2006.01.031}.

\bibitemdeclare{article}{Lanese200655}
\bibitem{Lanese200655}
\bibinfo{author}{I.~\surnamestart Lanese\surnameend} \&
  \bibinfo{author}{U.~\surnamestart Montanari\surnameend}
  (\bibinfo{year}{2006}): \emph{\bibinfo{title}{Hoare vs Milner: Comparing
  Synchronizations in a Graphical Framework With Mobility}}.
\newblock {\sl \bibinfo{journal}{Electronic Notes in Theoretical Computer
  Science}} \bibinfo{volume}{154}(\bibinfo{number}{2}), pp. \bibinfo{pages}{55
  -- 72}, \doi{http://dx.doi.org/10.1016/j.entcs.2005.03.032}.

\bibitemdeclare{book}{CCS}
\bibitem{CCS}
\bibinfo{author}{R.~\surnamestart Milner\surnameend} (\bibinfo{year}{1980}):
  \emph{\bibinfo{title}{A calculus of communicating systems}}.
\newblock \bibinfo{publisher}{Springer}, \doi{10.1007/3-540-10235-3}.

\bibitemdeclare{inproceedings}{DBLP:conf/ifip/Milner86}
\bibitem{DBLP:conf/ifip/Milner86}
\bibinfo{author}{R.~\surnamestart Milner\surnameend} (\bibinfo{year}{1986}):
  \emph{\bibinfo{title}{Process Constructors and Interpretations (Invited
  Paper)}}.
\newblock In: {\sl \bibinfo{booktitle}{{IFIP} Congress}}, pp.
  \bibinfo{pages}{507--514}.

\bibitemdeclare{inproceedings}{milner.sangiorgi:barbed-bisimulation}
\bibitem{milner.sangiorgi:barbed-bisimulation}
\bibinfo{author}{R.~\surnamestart Milner\surnameend} \&
  \bibinfo{author}{D.~\surnamestart Sangiorgi\surnameend}
  (\bibinfo{year}{1992}): \emph{\bibinfo{title}{Barbed Bisimulation}}.
\newblock In: {\sl \bibinfo{booktitle}{Proc. of ICALP}}, {\sl
  \bibinfo{series}{LNCS}} \bibinfo{volume}{623}, pp. \bibinfo{pages}{685--695},
  \doi{http://dx.doi.org/10.1007/3-540-55719-9\_114}.

\bibitemdeclare{phdthesis}{nestmann96}
\bibitem{nestmann96}
\bibinfo{author}{U.~\surnamestart Nestmann\surnameend} (\bibinfo{year}{1996}):
  \emph{\bibinfo{title}{{On Determinacy and Nondeterminacy in Concurrent
  Programming}}}.
\newblock Ph.D. thesis, \bibinfo{school}{Universit{\"a}t
  Erlangen-N{\"u}rnberg}.

\bibitemdeclare{article}{nestmannPierce00}
\bibitem{nestmannPierce00}
\bibinfo{author}{U.~\surnamestart Nestmann\surnameend} \&
  \bibinfo{author}{B.~C. \surnamestart Pierce\surnameend}
  (\bibinfo{year}{2000}): \emph{\bibinfo{title}{{Decoding Choice Encodings}}}.
\newblock {\sl \bibinfo{journal}{Information and Computation}}
  \bibinfo{volume}{163}(\bibinfo{number}{1}), pp. \bibinfo{pages}{1--59},
  \doi{10.1006/inco.2000.2868}.

\bibitemdeclare{inproceedings}{parrowCoupled92}
\bibitem{parrowCoupled92}
\bibinfo{author}{J.~\surnamestart Parrow\surnameend} \&
  \bibinfo{author}{P.~\surnamestart Sj\"odin\surnameend}
  (\bibinfo{year}{1992}): \emph{\bibinfo{title}{Multiway synchronization
  verified with coupled simulation}}.
\newblock In: {\sl \bibinfo{booktitle}{Proc. of CONCUR}}, {\sl
  \bibinfo{series}{LNCS}} \bibinfo{volume}{630},
  \bibinfo{organization}{Springer}, pp. \bibinfo{pages}{518--533},
  \doi{http://dx.doi.org/10.1007/bfb0084813}.

\bibitemdeclare{phdthesis}{peters12}
\bibitem{peters12}
\bibinfo{author}{K.~\surnamestart Peters\surnameend} (\bibinfo{year}{2012}):
  \emph{\bibinfo{title}{{Translational Expressiveness}}}.
\newblock Ph.D. thesis, \bibinfo{school}{TU Berlin}.

\bibitemdeclare{inproceedings}{petersGlabbeek15}
\bibitem{petersGlabbeek15}
\bibinfo{author}{K.~\surnamestart Peters\surnameend} \&
  \bibinfo{author}{R.~\surnamestart van Glabbeek\surnameend}
  (\bibinfo{year}{2015}): \emph{\bibinfo{title}{{Analysing and Comparing
  Encodability Criteria}}}.
\newblock In: {\sl \bibinfo{booktitle}{Proc. of EXPRESS/SOS}},
  \bibinfo{series}{EPTCS}.
\newblock \bibinfo{note}{To Appear.}

\bibitemdeclare{conference}{petersNestmann12}
\bibitem{petersNestmann12}
\bibinfo{author}{K.~\surnamestart Peters\surnameend} \&
  \bibinfo{author}{U.~\surnamestart Nestmann\surnameend}
  (\bibinfo{year}{2012}): \emph{\bibinfo{title}{{Is it a ``Good'' Encoding of
  Mixed Choice?}}}
\newblock In: {\sl \bibinfo{booktitle}{Proc. of FoSSaCS}}, {\sl
  \bibinfo{series}{LNCS}} \bibinfo{volume}{7213}, pp.
  \bibinfo{pages}{210--224}, \doi{10.1007/978-3-642-28729-9\_14}.

\bibitemdeclare{article}{petersNestmann15}
\bibitem{petersNestmann15}
\bibinfo{author}{K.~\surnamestart Peters\surnameend} \&
  \bibinfo{author}{U.~\surnamestart Nestmann\surnameend}
  (\bibinfo{year}{2015}): \emph{\bibinfo{title}{{Breaking Symmetries}}}.
\newblock {\sl \bibinfo{journal}{Mathematical Structures of Computer Science}}
  \bibinfo{volume}{FirstView}, pp. \bibinfo{pages}{1--53},
  \doi{10.1017/S0960129514000346}.

\bibitemdeclare{incollection}{petersNestmannGoltz13}
\bibitem{petersNestmannGoltz13}
\bibinfo{author}{K.~\surnamestart Peters\surnameend},
  \bibinfo{author}{U.~\surnamestart Nestmann\surnameend} \&
  \bibinfo{author}{U.~\surnamestart Goltz\surnameend} (\bibinfo{year}{2013}):
  \emph{\bibinfo{title}{{On Distributability in Process Calculi}}}.
\newblock In: {\sl \bibinfo{booktitle}{Proc. of ESOP}}, {\sl
  \bibinfo{series}{LNCS}} \bibinfo{volume}{7792},
  \bibinfo{publisher}{Springer}, pp. \bibinfo{pages}{310--329},
  \doi{10.1007/978-3-642-37036-6\_18}.

\bibitemdeclare{phdthesis}{sjodin:phd}
\bibitem{sjodin:phd}
\bibinfo{author}{P.~\surnamestart Sj{\"o}din\surnameend}
  (\bibinfo{year}{1991}): \emph{\bibinfo{title}{{From LOTOS Specifications to
  Distributed Implementations}}}.
\newblock Ph.D. thesis, \bibinfo{school}{Department of Computer Science,
  Uppsala University}.
\newblock \bibinfo{note}{Available as Report DoCS 91/31}.

\end{thebibliography}

\end{document}