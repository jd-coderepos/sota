

\documentclass[final]{cvpr}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{adjustbox}


\usepackage[pagebackref=false,breaklinks=true,colorlinks,bookmarks=false]{hyperref}

\newcommand{\vct}[1]{\bm{#1}}
\newcommand{\img}[1]{\mathbf{#1}}
\newcommand{\R}{\mathbb{R}}
\renewcommand{\thefootnote}{\fnsymbol{footnote}} 

\usepackage[misc]{ifsym}

\DeclareMathOperator{\upsample}{Upsample}


\def\cvprPaperID{5028} \def\confYear{CVPR 2021}



\begin{document}

\title{Student-Teacher Feature Pyramid Matching for \\
	Unsupervised Anomaly Detection}

\author{Guodong Wang\textsuperscript{1}, Shumin Han\textsuperscript{2}, Errui Ding\textsuperscript{2}, Di Huang\textsuperscript{1}  \and
\textsuperscript{1}School of Computer Science and Engineering, Beihang University, Beijing 100191, China\\
\textsuperscript{2}Department of Computer Vision Technology (VIS), Baidu Inc.\\
{\tt\small \{wanggd, dhuang\}@buaa.edu.cn \{hanshumin, dingerrui\}@baidu.com}
}


\maketitle


\begin{abstract}
Anomaly detection is a challenging task and usually formulated as an unsupervised learning problem for the unexpectedness of anomalies. This paper proposes a simple yet powerful approach to this issue, which is implemented in the student-teacher framework for its advantages but substantially extends it in terms of both accuracy and efficiency. Given a strong model pre-trained on image classification as the teacher, we distill the knowledge into a single student network with the identical architecture to learn the distribution of anomaly-free images and this one-step transfer preserves the crucial clues as much as possible. Moreover, we integrate the multi-scale feature matching strategy into the framework, and this hierarchical feature alignment enables the student network to receive a mixture of multi-level knowledge from the feature pyramid under better supervision, thus allowing to detect anomalies of various sizes. The difference between feature pyramids generated by the two networks serves as a scoring function indicating the probability of anomaly occurring. Due to such operations, our approach achieves accurate and fast pixel-level anomaly detection. Very competitive results are delivered on three major benchmarks, significantly superior to the state of the art ones. In addition, it makes inferences at a very high speed (with ~100 FPS for images of the size at 256256), at least dozens of times faster than the latest counterparts.
\end{abstract}


\section{Introduction}


\footnotetext{Equal contribution. This work is done when Guodong Wang is an intern at Baidu Inc.}
\footnotetext{Corresponding author.}


\label{sec1}

Anomaly detection is generally referred to as identifying samples that are atypical with respect to regular patterns in the data set and has shown great potential in various real-world applications such as video surveillance~\cite{Abati2019,Roitberg2018}, product quality control~\cite{Bergmann2019,Bergmann2020,Napoletano2018} and medical diagnosis~\cite{Schlegl2019,Schlegl2017,Vasilev2018}. Its key challenge lies in the unexpectedness of anomalies which is very difficult to deal with in a supervised way, as labeling all types of anomalous instances seems unrealistic. 

\begin{figure}
	\centering
	\includegraphics[width=0.47\textwidth]{figs/fig1.pdf}
	\caption{Visual results of our method on three defective images from the MVTec-AD dataset. ResNet-18 is used as backbone and the three bottom blocks (\textit{i.e.}, conv2\_x, conv3\_x, conv4\_x) are selected as feature extractors. Columns from left to right correspond to input images with defects (ground truth regions in red), anomaly maps of the three blocks, and the final anomaly maps respectively.}
	\label{fig1}
\end{figure}

Previous studies address this challenge by following the unsupervised learning paradigm,~\textit{i.e.}, one-class classification~\cite{Moya1993}. They approximate the decision boundary for a binary classification problem  by searching a feature space where the distribution of normal data is accurately modeled.
Deep learning, in particular convolutional neural networks (CNNs)~\cite{Lecun1998} and residual networks (ResNets)~\cite{He2016}, provides a powerful alternative to automatically build comprehensive representations at multiple levels. Such deep features prove very effective in capturing the intrinsic characteristics of the normal data manifold~\cite{An2015,Chalapathy2018,Masana2018,Ruff2018,Zhou2017}. 
Despite the promising results in their respective fields, all these methods simply predict anomalies at the image-level without spatial localization.


The pixel-level methods advance anomaly detection by means of pixel-wise comparison of image patches and their reconstructions~\cite{Baur2018,Schlegl2019,Schlegl2017} or per-pixel estimation of probability density on entire images~\cite{Abati2019,Seebock2016}, among which Auto-encoders, Generative Adversarial Networks (GANs), and their variants are dominating models. However, their performance is prone to serious degradation when images are poorly reconstructed ~\cite{Bergmann2019b} or likelihoods are inaccurately calibrated~\cite{Nalisnick2019}.



Some recent attempts leverage the knowledge from other well-studied computer vision tasks. They directly apply the networks pre-trained on image classification and show that they are sufficiently generic to image-level detection~\cite{Andrews2016,Burlina2019,Erfani2016}. Cohen and Hoshen \cite{Cohen2020} investigate this idea in pixel-level detection and calculate anomaly scores by averaging distances over a number of nearest neighbors from a collection of features extracted from a pretrained network on anomaly-free images. This method indeed delivers performance gain; unfortunately, it has the time bottleneck due to per-pixel comparison.

Bergmann~\textit{et al.}~\cite{Bergmann2020} circumvent the limitations aforementioned by implicitly learning the distribution of normal features with a student-teacher framework and reach decent results. The intuition is that the representation of anomaly-free data suffers from poor generalization outside its own manifold. The difference between the outputs of students and teacher along with the uncertainty among students' predictions serve as the anomaly scoring function. Nevertheless, two major drawbacks still remain: \ie incompleteness of transferred knowledge and inefficiency of handling scaling.
For the former, since knowledge is distilled from a ResNet-18~\cite{He2016} into a lightweight teacher network, the big gap between their model capacities~\cite{Wang2020} tends to incur loss of important information. For the latter, multiple student-teacher ensemble pairs are required to be separately trained, each for a specific respective field, to achieve scale invariance, leading to a high computational cost. Both the facts leave much room for improvement.

In this paper, we propose a simple yet powerful approach to anomaly detection, which follows the student-teacher framework for the advantages, but substantially extends it in terms of both accuracy and efficiency. Specifically, given a powerful network pre-trained on image classification as the teacher, we distill the knowledge into a single student network with the identical architecture. In this case, the student network learns the distribution of anomaly-free images by matching their features with the counterparts from the pre-trained network, and this one-step transfer preserves the crucial information as much as possible. Furthermore, to enhance the scale robustness, we embed multi-scale feature matching into the network, and this hierarchical feature alignment strategy enables the student network to receive a mixture of multi-level knowledge from the feature pyramid under a stronger supervision and thus allow to detect anomalies of various sizes (see~Figure \ref{fig1} for visualization). The feature pyramids from the teacher and student networks are compared for prediction, where a larger difference indicates a higher probability of anomaly occurrence.

Compared to the previous work, especially the preliminary student-teacher model, the benefits of our approach are three-fold. First, feature learning is tailored to the training dataset, making the student network more discriminative for the given task. Second, useful knowledge is well transferred from the pretrained network to the student network within one-step distillation, as they share the same structure. Finally, the scale invariance is conveniently reached by the proposed feature pyramid matching scheme. Due to such strengths, our approach conducts accurate and fast pixel-level anomaly detection. It reports very competitive results on three major benchmarks, outperforming the state of the art ones by a large margin, and makes prediction at a very high speed (100 FPS on a GeForce RTX 2080 Ti for images of the size 256256), 20 and 550 times faster than STAD \cite{Bergmann2020} and SPADE \cite{Cohen2020}, respectively.


\iffalse
\textcolor{blue}{Given a powerful network pretrained on image classification as the teacher, we distill the knowledge into a single student network with the identical architecture. As the teacher and student have the same capacity, the one-step knowledge transfer probably reduce the risk of information loss.
The student network implicitly learns the distribution of anomaly-free images by matching their features with the counterparts from pretrained network. The pretrained network provides a hierarchy of feature representations. Matching features generated by a single layer may limit the ability of the student network to learn the distribution of normal images and detect the anomalies. As shown in Figure~\ref{fig1}, our method can only identify the defects in the hazelnut and metal nut if we conduct feature alignment at conv2\_x and conv4\_x blocks respectively, and it fails to recognize the defective region in the screw when any single-layer feature is used (see Section~\ref{sec4-sub2} for more details on this experiment). To tackle this problem, we propose to match different levels of features. This hierarchical feature alignment strategy enables the student network to receive a mixture of multi-level knowledge from the feature pyramid and allows our method to detect anomalies of various sizes (see the five columns in Figure~\ref{fig1}).}
\fi

\iffalse
Our main contributions are as follows:
\begin{itemize}
	\item We incorporate a new feature pyramid matching technique into the student-teacher anomaly detection framework. This strategy largely simplifies the detection process, which allows our method to identify the anomalies at different scales without training multiple teacher-student pairs.
	\item We compare our approach with a large number of deep learning-based methods over three benchmark datasets. Our method achieves new state-of-the-art performance in all cases. Through our substantial ablation studies, we identify the key to success of student-teacher anomaly detection framework. \item We show that our method can be applied to small training datasets. Our approach achieves competitive results even if there are a low number of training examples. This interesting discovery may shed light on the few-shot unsupervised anomaly detection.
\end{itemize}
\fi


\iffalse
We use two networks with the identical architectures to model the distribution of training features in an implicit way, \textcolor{red}{ensuring the lossless knowledge transfer.} \textcolor{red}{Instead of comparing two features only from the top of the two networks, we embed multi-scale feature matching into the network, which is inspired by the fact that a deep network naturally provides a hierarchy of feature representation. To be specific, for} each normal training image, we match different levels of features from the student network with their counterparts from the teacher network. The feature pyramids from the teacher and student networks are compared for test images. Large difference means high probability of anomaly occurring.

\textcolor{red}{The student learns a mixture of multi-level knowledge under a stronger supervision of the teacher compared to single-level knowledge transfer}. The benefits of our approach are threefold. First, the feature learning is tailored to the training dataset, which makes the student network more discriminative towards a specific dataset. Second, useful knowledge can be completely transferred from the pretrained network to the student network, as two models have the same structure. Finally, as shown in Figure~\ref{fig1}, the hierarchical feature alignment enables our method to detect anomalies of various sizes in only a single forward pass \textcolor{red}{without retaining multiple teacher-student pairs corresponding to different scales. For an image at 256256, STAD~\cite{Bergmann2020} and  SPADE~\cite{Cohen2020} finish detection in 223 ms and 5626 ms on average respectively, while we achieve 10.2 under the same running environment.


\textcolor{red}{The main contribution is that we incorporate a new feature pyramid matching technique into the student-teacher framework for unsupervised anomaly detection and segmentation, which largely simplifies the detection of anomalies with different sizes. Despite its similarity, our method achieves new state-of-the-art performance against a number of learning-based methods over three benchmark datasets. Additionally, through our substantial ablation studies on \romannumeral1  network architectures, \romannumeral2 combination of pyramid levels, \romannumeral3 datasets used for pretrained teacher network, and \romannumeral4 the way pretraining teacher network, we identify the key to success of student-teacher framework is the intrinsic powerful discriminability of pretrained networks for encoding different images even from significantly different domains. We hope this discovery could shed light on the understanding of knowledge transfer and the development of new algorithms.}
\fi

\iffalse
\begin{itemize}
	\item We propose a student-teacher learning framework for unsupervised anomaly detection and segmentation. A new feature pyramid alignment technique is incorporated into the framework, which largely simplify the detection of anomalies with different sizes.
	\item We compare our approach with a large number of deep learning-based methods over three benchmark datasets. Our method achieves new state-of-the-art performance in all cases.
	\item We conduct substantial ablation studies that our method is independent of the backbone network structure. We also discovery a couple of interesting phenomena that perhaps shed light on the understanding of knowledge transfer and the development of new algorithms.
\end{itemize}
\fi

\section{Related Work}
\label{sec2}



\subsection{Image-Level Anomaly Detection}
\label{sec2-sub1}

Image-level techniques manifest anomalies in the form of images of a unseen category. These methods can be coarsely divided into three branches: reconstruction-based, distribution-based and classification-based.

The first group of approaches reconstruct the training images to capture the normal data manifold. An anomalous image is very likely to possess a high reconstruction error during inference, as it is drawn from a different distribution. The main weakness of these approaches comes from the excellent generalization ability of the deep models they use, including variational autoencoder~\cite{An2015}, robust autoencoder~\cite{Zhou2017}, conditional GAN~\cite{Akcay2018}, and bi-directional GAN~\cite{Zenati2018},  which probably allows anomalous images to be faithful reconstructed. It is also rather difficult to chose an appropriate loss function for evaluating the quality of reconstruction~\cite{Bergmann2019b}.

Distribution-based approaches model the probabilistic distribution of the normal images. Test images that have low probability density values are designated as anomalous. Recent algorithms such as anomaly detection GAN (ADGAN)~\cite{Deecke2018} and deep autoencoding Gaussian mixture model (DAGMM)~\cite{Zong2018} learn a deep projection that maps high-dimensional images into a low-dimensional latent space. Probability density estimation is typically much easier in the obtained spaces and thus performance has been significantly improved. Nevertheless, these methods have high sample complexity and demand large training data.

Classification-based approaches have dominated the unsupervised anomaly detection in the last decade. One useful paradigm is to feed the deep features extracted by deep generative model~\cite{Burlina2019} or transferred from pretrained networks~\cite{Andrews2016,Erfani2016} into a separate shallow classification model like one-class support vector machine (OC-SVM)~\cite{Scholkopf2001}. These hybrid techniques are sub-optimal because feature extraction is unaware of the anomaly detection task. Deep support vector
data description (Deep-SVDD)~\cite{Ruff2018}, an adaptation of SVDD~\cite{Tax2004} to the
deep regime, trains a neural network by minimizing the volume of a hypersphere that encloses the network representations of the data. One-class neural network (OC-NN)~\cite{Chalapathy2018} combines an encoder that progressively extracts rich representation of the data with a neural network that searches for a hyperplane to separate all the normal data from the origin. These two algorithms are departures from the hybrid methods in the sense that data representation in them is not task-agnostic.

Another line of research depends on self-supervised learning. Geom~\cite{Golan2018} creates a dataset by applying dozens of geometric transformations to the normal images and trains a multi-class neural network over the self-labeled dataset to discriminate such transformations. At test time, anomalies are expected to be assigned with less confidence in discriminating the transformations. The effectiveness of this method is limited by the visual transformation based self-supervision (VTSS) hypothesis~\cite{Pal2019}, which states that features obtained by self-supervised learning will be less valuable if the transformed instances are already present in the dataset.

\subsection{Pixel-Level Anomaly Detection}
\label{sec2-sub2}

Pixel-level techniques are particularly designed for visual anomaly detection. All the methods aim to precisely segment the anomalous regions in the test images. This task is much more complicated than a binary classification.

The expressive power of deep neural networks inspires a series of studies that explore how to transfer the benefits of networks pretrained on image classification tasks to unsupervised anomaly detection. Napoletano~\textit{et al.}~\cite{Napoletano2018} exploit a pretrained ResNet-18 to embed cropped training image patches into a feature space, reduce the dimension of feature vectors by PCA, and model their distribution using K-means clustering. This method requires a large number of overlapping patches to obtain a spatial anomaly map at inference time, which results in coarse-grained maps and may become a performance bottleneck.

To avoid cropping image patches and accelerate feature extraction, Sabokrou~\textit{et al.}~\cite{Sabokrou2018} extract descriptors from early future maps of a pretrained fully convolutional neural network (FCN) and adopt a unimodal Gaussian distribution to fit feature vectors of the training anomaly-free images. The resolution of the final anomaly map will decrease substantially on account of the pooling layers, especially when the receptive fields of the networks are very large. The unimodel Gaussian distribution will suffer a failure of characterizing the training feature distribution as long as the problem complexity increases.

More recently, convolutional adversarial variational autoencoder with guided attention (CAVGA)~\cite{Venkataramanan2019} incorporates Grad-CAM~\cite{Selvaraju2017} into a variational autoencoder that has an attention expansion loss to encourage the deep model itself to focus on all normal regions in the image.


We are aware of a preprint~\cite{Cohen2020} that also presents a feature pyramid matching method for unsupervised anomaly detection and achieve promising results. Our study departs from that work on several fronts. First, the proposed SPADE approach directly applies the deep pretrained features to entire image dataset, while the feature learning is tailored to the training anomaly-free dataset in our method. Second, the feature comparison of SPADE is conducted between a given image and its  nearest neighbors in the feature space, which is quite different from ours. Finally, SPADE perform image-level and pixel-level anomaly detections in two successive steps. By contract, our method can simultaneously detect the image-level and pixel-level anomalies. Therefore, our method is much faster than SPADE and can achieve much better performance.


\section{Method}
\label{sec3}



\subsection{Framework}


\begin{figure*}[!t]
	\centering
	\includegraphics[width=0.85\textwidth]{figs/architecture.pdf}
	\caption{Schematic overview of our method. The feature pyramid of a student network is trained to align with the counterpart of a pretrained teacher network. A test image (or pixel) will have high anomaly score if its features from the two models differ significantly. The hierarchical feature alignment enables our method to detect anomalies of varies sizes with a single forward pass.}
	\label{fig2}
\end{figure*}

We make use of the student-teacher learning framework to implicitly model the feature distribution of the normal training images. The teacher is a powerful network pretrained on image classification task (\textit{e.g.}, a ResNet-18 pretrained on ImageNet). To reduce information loss, the student is a network with the same architecture. This is in essence one case of feature-based knowledge distillation~\cite{Wang2020}.

We need to consider a key factor, position of selected distillation feature. Deep neural networks generate a pyramid of features for each input image. Bottom layers result in higher-resolution features encoding low-level information such as textures, edges and colors. By contrast, top layers yield low-resolution features that contain context information. Features created by bottom layers often are generic enough that they can be shared by dissimilar vision tasks~\cite{Oquab2014,Zeiler2014}. As different layers in deep neural networks correspond to distinct receptive fields, we select the features extracted by a few successive bottom layer groups (\textit{e.g.}, blocks in ResNet-18) of the teacher to guide the student's learning. This hierarchical feature matching allows our method to detect anomalies of various sizes.

Figure~\ref{fig2} gives a sketch of our method with the MVTec-AD dataset~\cite{Bergmann2020} being an example. The training and test process are formally provided as follows.

\subsection{Training Process}
\label{sec3-sub1}

Given a training data set of anomaly-free images , our goal is to capture the normal data manifold by aligning the features extracted by the  bottom layer groups of the teacher with those of the student. For an input image , where  is the height,  is the width and  is the number of color channels, the th bottom layer group of the teacher and the student outputs a feature map  and  respectively, where  and  denote the height and width of the feature map. Since there is no prior knowledge regarding the appearance of objects, we simply assume that all regions including background are anomaly-free in the training images if applicable. Note that  and   are feature vectors at position  in the feature maps from the teacher and student respectively. We define the loss at position  as -distance between the -normalized feature vectors, namely,


It is worth noting that the  distance used in (\ref{eq1}) is equivalent to the cosine similarity. So the loss . The loss for the entire image  is given as an average of the loss at each position,

and the total loss is the weighted average of the loss at different pyramid scales,

where  depicts the impact of the th feature scale on anomaly detection. We simply set the parameter  in all our experiments. Given a minibatch  sampled from the training data set , we update the student by minimizing the loss  by a certain stochastic optimization algorithm. The teacher keeps fixed during the entire training phase.

\subsection{Test Process}
\label{sec3-sub2}

In this phase, we aim to assign an anomaly map  of size  to a test image . The score  indicates how much the pixel at position  deviates from the training data manifold. We forward the test image  into the teacher and the student. Let  and  denote the feature maps generated by the th bottom layer group of the teacher and the student. We can compute an anomaly map  of size , whose element  is the loss (\ref{eq1}) at position . The anomaly map  is unsampled to size  by the bilinear interpolation technique. The final anomaly map is defined as element-wise production of these interpolations,

A test image is designated as anomaly if any regions in the image are anomalous. As a result, we simply choose the maximum value in the anomaly map  as the anomaly score for the test image .

\section{Experiments}
\label{sec4}

To evaluate the effectiveness of our method, we first conduct experiments on the MVTec Anomaly Detection (MVTec-AD)~\cite{Bergmann2019} dataset. Both image-level and pixel-level anomaly detection are considered. Our method is then applied to the ShanghaiTech Campus (STC)~\cite{Liu2018} dataset for unsupervised anomaly pixel-level detection and the CIFAR-10~\cite{Krizhevsky2009} dataset for one-class classification.

\subsection{Experiment Settings}


\noindent \textbf{Implement Details.} For all experiments, we choose the first three blocks (\textit{i.e.}, conv2\_x, conv3\_x, conv4\_x) of ResNet-18 as the pyramid feature extractors. The parameters of the teacher network take the corresponding values from the ResNet-18 pretrained on ImageNet\footnote{We adopt the default values provided by PyTorch.} while those of student network are initialized randomly. We utilize stochastic gradient descent (SGD) with the momentum of 0.9 as the optimizer. Learning rate is fixed to be 0.4 throughout 100 training epochs. The batch size is 32 and weight decay is . We find that our method is quite robust to the values of the hyper-parameters. To strike a balance between detection accuracy and efficiency, we resize all images in the MVTec-AD and STC datasets to 256256. Data augmentation is not used, since some standard augmentation techniques may lead to ambiguous determination of anomalies. For instance, flipped metal nut in the MvTec-AD dataset is regraded as anomalous product.
\noindent \textbf{Evaluation Metrics.} The performance of our method is measured by two well-established metrics. The first is the area under the receiver operating characteristic curve (AUC-ROC), which can be computed on different levels when anomalous images and pixels are treated as positive respectively. However,  the pixel-level AUC-ROC is in favor of large anomalous regions. In order to weight ground-truth anomaly regions of various sizes equally, we introduce the Per-Region-Overlap (PRO) curve metric proposed in~\cite{Bergmann2020}. Given a threshold for anomaly scores, we obtain a binary mask that indicates whether a pixel is anomalous. The ground-truth anomalous regions are separated into multiple connected components. The PRO score is defined by the average proportion of the pixels in each component that are detected as anomalous. We scan over the PRO value by increasing the threshold until the average false positive rate (FPR) reaches 30\%. The normalized integral of the PRO curve is used as a measure.


\subsection{MVTec Anomaly Detection Dataset}
\label{sec4-sub2}

Our method is first evaluated on the MVTec-AD dataset, which is specifically created to benchmark algorithms for unsupervised detection of anomalous regions~\cite{Bergmann2019}. It collects more than 5000 high-resolution images of industrial products from 15 different categories. For each category, the training set includes defect-free images merely and the test set comprises normal images as well as images with different types of industrial defects.

\begin{table}[!t]
	\centering
	\caption{Image-level anomaly detection on the MvTec-AD dataset. The performance is measured by average AUC-ROC across 15 categories. The best results are highlighted by boldface. Results for all approaches except ours are quoted from~\cite{Cohen2020}.}
	\label{tab1}
	\vspace{0.5em}
\begin{tabular}{cccccc}
			\hline
			Geom  & GANomaly & -AE & ITAE  & SPADE & Ours\\
			\hline
			0.672 & 0.762  & 0.754  & 0.839 & 0.855 & \textbf{0.955} \\
			\hline
		\end{tabular}
\end{table}

We begin with the task of finding anomalous images. As defective regions usually occupy a small proportion of the whole image, the test anomalies differ in subtle way from the training images. This makes the MVTec-AD dataset more challenging than those previously used in the literature (\textit{e.g.}, MNIST and CIFAR-10) where anomalous images come from other different categories. Table~\ref{tab1} compares our method against several state-of-the-art approaches, Geom~\cite{Golan2018}, GANomaly~\cite{Akcay2018}, -AE~\cite{Aytekin2018}, ITAE~\cite{Huang2020} and SPADE~\cite{Cohen2020}. We see clearly that our approach outperforms other methods by a large margin. In particular, the performance is improved up to around 11.7\% compared with the second-best SPADE~\cite{Cohen2020}. This demonstrates that our method can learn category-specific features which are superior to the generic features directly transferred from image classification.



 

\begin{table}[!t]
	\centering
	\caption{Pixel-precise anomaly detection on the MvTec-AD dataset. The performance is measured by the pixel-level AUC-ROC for each category. The best results are highlighted in boldface. Results for all approaches except ours are quoted from~\cite{Cohen2020}.}
	\label{tab2}
	\vspace{0.5em}
	\begin{adjustbox}{width=0.48\textwidth}
		\begin{tabular}{c|c|c c c c c}
			\hline
			& Category & SSIM-AE &  AnoGAN & CNN-Dict & SPADE & Ours \\ \hline
			\multirow{5}{*}{\rotatebox{90}{Textures}}
			&  Carpet     & 0.87 & 0.54 & 0.72 & 0.975 & \textbf{0.988} \\
			& Grid       & 0.94 & 0.58 & 0.59 & 0.937 & \textbf{0.990} \\
			& Leather    & 0.78 & 0.64 & 0.87 & 0.976 & \textbf{0.993} \\
			& Tile       & 0.59 & 0.50 & 0.93 & 0.874 &\textbf{0.974} \\
			& Wood       & 0.73 & 0.62 & 0.91 & 0.885 &\textbf{0.972} \\ \hline
			\multirow{10}{*}{\rotatebox{90}{Objects}}
			& Bottle     & 0.93 & 0.86 & 0.78 & 0.984 & \textbf{0.988} \\
			& Cable      & 0.82 & 0.78 & 0.79 & \textbf{0.972} & 0.955 \\
			& Capsule    & 0.94 & 0.84 & 0.84 & \textbf{0.990} & 0.983 \\
			& Hazelnut   & 0.97 & 0.87 & 0.72 & \textbf{0.991} & 0.985 \\
			& Metal nut  & 0.89 & 0.76 & 0.82 & \textbf{0.981} & 0.976 \\
			& Pill       & 0.91 & 0.87 & 0.68 & 0.965 & \textbf{0.978} \\
			& Screw      & 0.96 & 0.80 & 0.87 & \textbf{0.989} & 0.983 \\
			& Toothbrush & 0.92 & 0.93 & 0.90 & 0.979 & \textbf{0.989} \\
			& Transistor & 0.90 & 0.86 & 0.66 & \textbf{0.941} & 0.825 \\
			& Zipper     & 0.88 & 0.78 & 0.76 & 0.965 & \textbf{0.985} \\ \hline \hline
			& Mean       & 0.87 & 0.74 & 0.78 & 0.965 &\textbf{0.970} \\
			\hline
		\end{tabular}
	\end{adjustbox}
\end{table}












\begin{table*}[!t]
	\centering
	\caption{Pixel-level anomaly detection on the MvTec-AD dataset. The performance is measured by the normalized area under the Per-Region-Overlap (PRO) curve up to an average false positive rate (FPR) of 30\% for each category. The best results are highlighted in boldface. Results for all approaches except ours are quoted from~\cite{Bergmann2020}.}
	\label{tab3}
	\vspace{0.5em}
\begin{tabular}{c|c|c c c c c c c c c c c c}
			\hline
			& Category & 1-NN & K-means & -AE & VAE & SSIM-AE & AnoGAN & CNN-Dict & STAD & SPADE & Ours\\  \hline
			\multirow{5}{*}{\rotatebox{90}{Textures}} &
			Carpet       &  0.512 & 0.253 & 0.456 & 0.501 & 0.647 & 0.204 & 0.469 & 0.695 & 0.947 & \textbf{0.958} \\
			& Grid       &  0.228 & 0.107 & 0.582 & 0.224 & 0.849 & 0.226 & 0.183 & 0.819 & 0.867 & \textbf{0.966} \\
			& Leather    &  0.446 & 0.308 & 0.819 & 0.635 & 0.561 & 0.378 & 0.641 & 0.819 & 0.972 & \textbf{0.980} \\
			& Tile       &  0.822 & 0.779 & 0.897 & 0.870 & 0.175 & 0.177 & 0.797 & 0.912 & 0.759 &\textbf{0.921} \\
			& Wood       &  0.502 & 0.411 & 0.727 & 0.628 & 0.605 & 0.386 & 0.621 & 0.725 & 0.874 &\textbf{0.936} \\ \hline
			\multirow{10}{*}{\rotatebox{90}{Objects}}
			& Bottle     &  0.898 & 0.495 & 0.910 & 0.897 & 0.834 & 0.620 & 0.742 & 0.918 & \textbf{0.955} & 0.951 \\
			& Cable      &  0.806 & 0.513 & 0.825 & 0.654 & 0.478 & 0.383 & 0.558 & 0.865 & \textbf{0.909} & 0.877 \\
			& Capsule    &  0.631 & 0.387 & 0.862 & 0.526 & 0.860 & 0.306 & 0.306 & 0.916 & \textbf{0.937} & 0.922\\
			& Hazelnut   &  0.861 & 0.698 & 0.917 & 0.878 & 0.916 & 0.698 & 0.844 & 0.937 & \textbf{0.954} & 0.943 \\
			& Metal nut  &  0.705 & 0.351 & 0.830 & 0.576 & 0.603 & 0.320 & 0.358 & 0.895 & 0.944 &\textbf{0.945} \\
			& Pill       &  0.725 & 0.514 & 0.893 & 0.769 & 0.830 & 0.776 & 0.460 & 0.935 & 0.946 &\textbf{0.965} \\
			& Screw      &  0.604 & 0.550 & 0.754 & 0.559 & 0.887 & 0.466 & 0.277 & 0.928 & \textbf{0.960} & 0.930 \\
			& Toothbrush &  0.675 & 0.337 & 0.822 & 0.693 & 0.784 & 0.749 & 0.151 & 0.863 & \textbf{0.935} & 0.922\\
			& Transistor &  0.680 & 0.399 & 0.728 & 0.626 & 0.725 & 0.549 & 0.628 & 0.701 & \textbf{0.874} & 0.695 \\
			& Zipper     &  0.512 & 0.253 & 0.839 & 0.549 & 0.665 & 0.467 & 0.703 & 0.933 & 0.926 &\textbf{0.952} \\ \hline \hline
			& Mean       &  0.640 & 0.423 & 0.790 & 0.639 & 0.694 & 0.443 & 0.515 & 0.857 & 0.917 & \textbf{0.921} \\
			\hline
		\end{tabular}
\end{table*}

We then consider the task of pixel-precise anomaly detection and compare our method with a series of state-of-the-art approaches listed in~\cite{Cohen2020} and~\cite{Bergmann2020}. Table~\ref{tab2} reports the performance in terms of the AUC-ROC metric. Our method can achieve much better results in most cases, though most of the other approaches use different varieties of autoencoder. This validates our pyramid alignment for feature learning. SPADE is very competitive, but utilizes a cumbersome Wide-ResNet502 network to extract features. Table~\ref{tab3} compares all approaches in terms of PRO metric. This is a pixel-level accuracy measure that gives equal weights to anomalies of different sizes. As shown in Table~\ref{tab3}, our method achieves the state-of-the-art against all other methods. Although STAD~\cite{Bergmann2020} leverages the student-teacher learning framework, its performance is always inferior to that of our method. Perhaps this inferiority can
be attributed to the information loss in its two-step knowledge transfer process and the low capacity of the student networks.

SPADE is very competitive in this experiment. However, it utilizes a cumbersome Wide-ResNet502 network to extract features. Besides, our method is much more efficient. For an image at 256256, STAD~\cite{Bergmann2020} and SPADE~\cite{Cohen2020} finish detection in 223 ms and 5626 ms on a GeForce RTX 2080 Ti, respectively, while it only takes 10 ms for our method. 


\subsection{Shanghai Tech Campus Dataset}
\label{sec4-sub3}

\begin{table}[!t]
	\centering
	\caption{Pixel-level anomaly detection on the STC dataset. The performance is measured by the average AUC-ROC across 12 scenes. The best results are highlighted in boldface. Results for all approaches except ours are quoted from~\cite{Cohen2020}.}
	\label{tab4}
	\vspace{0.5em}
	\begin{tabular}{c c c c c}
		\hline
		SSIM-AE & -AE& CAVGA-& SPADE & Ours\\
		\hline
		0.76 & 0.74 & 0.85  & 0.899 & \textbf{0.913} \\
		\hline
	\end{tabular}
\end{table}


We further evaluate our method for pixel-level anomaly detection on the STC dataset. This dataset is originally created for anomaly detection in 12 surveillance scenes. Following~\cite{Venkataramanan2019}, we construct the training and test set by extracting every fifth frame from each scene. The training set only contains normal images while the test set contains both normal and anomalous images. Different from the MvTec-AD dataset, this dataset focuses more on motion anomaly detection such as fighting and car intruding.


Table~\ref{tab4} shows that the performance of our method exceeds that of CAVGA-~\cite{Venkataramanan2019} by a significant margin. Note that CAVGA- has reported state-of-the-art result in the literature on unsupervised anomaly detection. Once again, our method outperforms SPADE in spite of a lightweight network used.

Figure~\ref{fig3} gives the visual results of our method on three anomalous images from the STC dataset. We clearly find that the anomalous regions in the images are very difficult to be precisely localized if we use a specific level of features merely. Comparison of the features in a hierarchical fashion is certainly a good way to delineate the anomalous regions regardless of their sizes.

\subsection{CIFAR-10}
\label{sec4-sub4}

Finally, we test whether our method can generalize well on small-sized images. We conduct experiments on CIFAR-10 which is traditionally used for image-level one-class classification. For this dataset, the sizes of feature maps from three blocks are , , and  respectively. The hyper-parameter settings are the same as those used in the MVTec-AD experiment. Following \cite{Bergmann2020}, we take one class as normal and the remaining nine classes as anomalous in turn, totally resulting in ten runs. We see clearly from table~\ref{tab5} that our performance is much better. For each category, we rank the test images according to their anomaly scores. The top five samples and the bottom five sample for five categories are given in Figure~\ref{fig4}. It is clear that our method predicts the test images belonging to the given category as normal. The images from other categories are correctly classified as anomalous.

\iffalse
\begin{figure*}[!t]
	\centering
	\includegraphics[width=0.8\textwidth]{figs/STC.pdf}
	\caption{Visual results of our method on three anomalous images from the STC dataset. ResNet-18 is used as backbone and the three bottom blocks (\textit{i.e.}, conv2\_x, conv3\_x, conv4\_x) are selected as feature extractors. Columns from left to right correspond to input images, ground truth regions, anomaly maps of the three blocks, and the combined anomaly maps respectively.}
	\label{fig3}
\end{figure*}
\fi


\begin{figure}[!t]
	\centering
	\includegraphics[width=0.48\textwidth]{figs/STC.pdf}
	\caption{Visual results of our method on three anomalous images from the STC dataset. ResNet-18 is used as backbone and the three bottom blocks (\textit{i.e.}, conv2\_x, conv3\_x, conv4\_x) are selected as feature extractors. Columns from left to right correspond to input images, ground truth regions, anomaly maps of the three blocks, and the combined anomaly maps respectively.}
	\label{fig3}
\end{figure}


\begin{figure}[!t]
	\centering
	\includegraphics[width=0.48\textwidth]{figs/cifar10.pdf}
	\caption{Results of our method on the CIFAR-10 dataset. ResNet-18 is used as backbone and the three bottom blocks (\textit{i.e.}, conv2\_x, conv3\_x, conv4\_x) are selected as feature extractors.}
	\label{fig4}
\end{figure}


\section{Ablation Studies and Discussions}
\label{sec5}

We conduct a series of ablation studies on the MVTec-AD dataset to answer the following questions: which block provide most informative features for anomaly detection? Is feature pyramid matching superior to single feature alignment? Is our method independent of the backbone network structure? Is the teacher pretrained on other datasets still useful? Is our method applicable to small training dataset?

\subsection{Feature Matching}
\label{sec5-sub1}

We first investigate the effectiveness of feature extraction by each block of ResNet-18. Considering that the first block is a simple convolutional layer, we exclude it from comparison. We train the student by matching features extracted by its second, third, fourth and fifth blocks with the counterparts of the teacher respectively. As shown in Table~\ref{tab6}, feature alignments conducted at the end of the third and fourth blocks can achieve better performance. This is in good agreement with the previous discovery that the middle-level features play a more important role in knowledge transfer~\cite{Oquab2014}.

\begin{table}[!t]
	\centering
	\caption{Image-level anomaly detection on CIFAR-10. The performance is measured by the average AUC-ROC across 10 categories.The best results are highlighted in boldface. Results for all approaches except ours are quoted from~\cite{Bergmann2020}.}
	\label{tab5}
	\vspace{0.5em}
	\begin{adjustbox}{width=0.48\textwidth}
		\begin{tabular}{c c c c c c c}
			\hline
			OCGAN & 1-NN   & OC-SVM & -AE & VAE    & STAD & Ours \\ \hline
			0.657 & 0.819  & 0.739  & 0.790  & 0.750  & 0.820  & \textbf{0.832}\\
			\hline
		\end{tabular}
	\end{adjustbox}
\end{table}

We then test three different combinations of the successive blocks of ResNet-18. For each combination, we align the features extracted by the corresponding blocks from the teacher and the student. Table~\ref{tab6} shows that the mixture of the second, third and fourth blocks outperform other combinations as well as the single components. This implies that Feature pyramid matching is a better way for feature learning. This finding is also validated in Figure~\ref{fig1}. Anomaly maps generated by low-level features are more suitable for precise anomaly localization, but it is likely to include background noise. By contrast, anomaly maps generated by high-level features is desirable to segment big anomalous regions. The aggregation of anomaly maps at different scales can accurately detect anomalies of various sizes.



\subsection{Backbone Network Structure}
\label{sec5-sub2}

\begin{table}[!]
	\centering
	\caption{Alation studies for feature matching on the MVTec-AD dataset. The performance is measured by the average image-level AUC-ROC (AR), average pixel-level AUC-ROC (AR) and average PRO across 15 categories. The best results are highlighted in boldface. }
	\label{tab6}
	\vspace{0.5em}
	\begin{adjustbox}{width=0.48\textwidth}
		\begin{tabular}{c|ccccccc}
			\hline
			Metric & 2 & 3 & 4 & 5 & [2, 3] & [2, 3, 4] & [2, 3, 4, 5]\\
			\hline
			AR & 0.808 & 0.917 & 0.934 & 0.819 & 0.849 & \textbf{0.955} & 0.949\\
			AR & 0.915 & 0.953 & 0.957 & 0.860 & 0.950 & \textbf{0.970} &  0.969\\
			PRO & 0.815 & 0.897 & 0.835 & 0.504 & 0.886 & \textbf{0.921} & 0.886\\
			\hline
		\end{tabular}
	\end{adjustbox}
\end{table}

\begin{table*}[!]
	\centering
	\caption{Alation studies for backbone network structure on the MVTec-AD dataset. The performance is measured by the average image-level AUC-ROC (AR), average pixel-level AUC-ROC (AR) and average PRO across 15 categories. The best results are highlighted in boldface. }
	\label{tab7}
	\vspace{0.5em}
	\begin{adjustbox}{width=0.7\textwidth}
		\begin{tabular}{c|cccccc}
			\hline
			Metric & ResNet-18 & ResNet-34 & ResNet-50 & Wideresnet50x2 & SqueezeNet & DenseNet\\
			\hline
			AR & \textbf{0.955} & 0.939 & 0.897 & 0.909 & 0.874 & 0.884\\
			AR & \textbf{0.970} & 0.962 & 0.966 & 0.968 & 0.933 & 0.951 \\
			PRO & \textbf{0.921} & 0.902 & 0.913 & 0.919 & 0.833 & 0.887 \\
			\hline
		\end{tabular}
	\end{adjustbox}
\end{table*}

We further check whether our method is network-agnostic. Our algorithm is applied to backbone networks with different architectures. At the beginning, we conduct experiments using the first successive three bottom blocks of ResNet-34 and ResNet-50 as feature extractors. Table~\ref{tab7} shows that our method is quite robust and it can achieve competitive performance regardless of the depth of the residual network. We then introduce a cumbersome network Wideresnet50x2 that has been used in~\cite{Cohen2020}. As shown in Table~\ref{tab7}, the performance of this network is almost the same as those of the ResNets. This validates that our method is independent of the backbone network structure. We further make comparison with two lightweight networks, SqueezeNet~\cite{Iandola2016} and DenseNet~\cite{Huang2017}. From Table~\ref{tab7}, we see clearly that the performance of these two approaches is slight inferior. This is quite understandable, as they have comparatively smaller sizes and relatively low expressive capacity. Nevertheless, the results are still satisfactory, which are better than those obtained by other methods in Table~\ref{tab1},~\ref{tab2} and~\ref{tab3}.


\subsection{Pretained Datasets}
\label{sec5-sub3}

\begin{table}[!]
	\centering
	\caption{Alation studies for pretrained datasets on the MVTec-AD dataset. The performance is measured by the average image-level AUC-ROC (AR), average pixel-level AUC-ROC (AR) and average PRO across 15 categories. The best results are highlighted in boldface. }
	\label{tab8}
	\vspace{0.5em}
	\begin{adjustbox}{width=0.48\textwidth}
		\begin{tabular}{c|cccccc}
			\hline
			Metric & ImageNet & MNIST & CIFAR-10 & CIFAR-100 & SVHN\\
			\hline
			AR & \textbf{0.955} & 0.619 & 0.826 & 0.835 & 0.796\\
			AR & \textbf{0.970} & 0.759 & 0.931 & 0.937 & 0.902\\
			PRO & \textbf{0.921} & 0.528  & 0.863 & 0.842 & 0.742\\
			\hline
		\end{tabular}
	\end{adjustbox}
\end{table}

To answer the fourth questions, we pretrain the ResNet-18 on a couple of image classification benchmarks, including MNIST, CIFAR-10, CIFAR-100~\cite{Krizhevsky2009}, and SVHN~\cite{Netzer2011}. These pretrained models are exploited to train the student network in our method. The MNIST and SVHN datasets simply contain digital numbers from 0 to 9. We see from Table~\ref{tab8} that the teacher network pretrained on these two datasets yield worse results. This can be attribute to that the features learned from these two pretrained models generalize poorly on the MVTec-AD dataset. By contrast, the features extracted from the teacher networks pretrained on CIFAR-10 and CIFAR-100 exhibit better generalization, as they include natural images that are quite similar to the images in the MVTec-AD dataset. Note that the performance of these two pretrained teacher is slightly inferior to that of the teacher pretrained on ImageNet. This is because that the ImageNet dataset consists of a huge number of high-resolution natural images.



\iffalse
\subsection{Self-Supervised trained Teacher}
\label{sec5-sub4}

We further investigate the whether we can benefit from self-supervised learning. To this end, we design two alternative baselines. The ImageNet pretrained teacher is replaced by a random network and a self-supervised trained network respectively, which have the same architecture. The resulted two models are denoted as Ours-RD and Ours-SS. We adopt the contrastive learning \cite{He2020} to pretrain Ours-SS on images from ImageNet. During the training phase, we push different views of same images together and pull views of different images away.

Ours-RD exhibits the worst performance (see Table~\ref{tab9}), because the parameters are fully random. In contrast, self-supervised learning encourages network to learn discriminative representations for encoding images. Ours-SS achieves better results than Ours-RD, but is still inferior to our method especially in terms of PRO. Our method distills useful knowledge from the classification network pretrained on ImageNet, which is a key factor of its success. It gives evidence that anomaly detection can benefit from pretrained networks even if they come from different vision tasks.

\begin{table}[!]
	\centering
	\caption{Ablation studies for self-supervised trained teacher on the MVTec-AD dataset. The performance is measured by the average image-level AUC-ROC (AR), average pixel-level AUC-ROC (AR) and average PRO across 15 categories. The best results are highlighted in boldface. }
	\label{tab9}
	\vspace{0.5em}
	\begin{adjustbox}{width=0.32\textwidth}
		\begin{tabular}{c|ccc}
			\hline
			Metric & Ours & Ours-RD & Ours-SS \\
			\hline
			AR & \textbf{0.955} & 0.589 & 0.906 \\
			AR & \textbf{0.970} & 0.792 & 0.961 \\
			PRO & \textbf{0.921} & 0.534  & 0.875 \\
			\hline
		\end{tabular}
	\end{adjustbox}
\end{table}
\fi

\subsection{Number of Training Samples}
\label{sec5:sub4}


\begin{table}[!]
	\centering
	\caption{Ablation studies for number of training samples on the MVTecAD dataset. The performance is measured by the average image-level AUC-ROC (AR), average pixel-level AUC-ROC (AR) and average PRO across 15 categories.}
	\label{tab9}
	\vspace{0.5em}
	\begin{adjustbox}{width=0.35\textwidth}
		\begin{tabular}{c|cc|cc}
			\hline
			& \multicolumn{2}{c|}{5\%} & \multicolumn{2}{c}{10\%} \\
			\hline
			Metric & Ours & SPADE & Ours & SPADE \\
			\hline
			AR & 0.871 & 0.782 & 0.907 & 0.797 \\
			AR & 0.961 & 0.932 & 0.967 & 0.955 \\
			PRO & 0.892 & 0.842 & 0.913 & 0.890 \\
			\hline
		\end{tabular}
	\end{adjustbox}
\end{table}



We investigate the effect of the training set size in this experiment. Only 5\% and 10\% anomaly-free images are used to train our model. Considering that SPADE is very competitive, we compare it with our approach in these cases. The detection performance is summarized in Table~\ref{tab9}. It can be seen that our model still achieves a satisfactory performance even if there is a few number of training data. By contrast, SPADE suffers a serious performance degradation. This can be explained by the usage of the tailored feature learning. Our model profits from this strategy and require a few training data to capture the feature distribution of anomaly-free images. Furthermore, our method use only 5\% training samples to outperform the preliminary student-teacher framework. This validates the effectiveness of the proposed feature pyramid alignment technique. 


\section{Conclusion}
\label{sec6}




We present a new feature pyramid matching technique and incorporate it into the student-teacher anomaly detection framework. Given a powerful network pretrained on image classification as the teacher, we use its different levels of features to guide a student network with same structure to learn the distribution of anomaly-free images. The anomaly scoring function of a test image can be defined as the difference between feature pyramids generated by the two models. This one-step knowledge transfer largely simplifies the detection procedure. On account of the hierarchical feature alignment, our method is capable of detecting anomalies of various sizes with only a single forward pass. Our approach circumvents \textit{expensive computation} of the up-to-date STAD~\cite{Bergmann2020} and SPADE~\cite{Cohen2020}. Experimental results on three benchmark datasets show that our method achieves superior performance against state-of-the-art approaches by a significant margin. In addition, our method is applicable to the small training sets. Even with a low number of training examples it still achieves competitive results. This interesting fact may shed light on the few-shot anomaly detection.


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
