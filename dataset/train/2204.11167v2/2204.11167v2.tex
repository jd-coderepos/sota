
\documentclass{article} \usepackage{iclr2022_conference,times}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{$\eta$}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}

\usepackage{etoc}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{lipsum}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{pifont}
\usepackage{wrapfig}
\usepackage{xcolor}
\usepackage{colortbl}
\definecolor{Gray}{gray}{0.9}
\definecolor{mygreen}{rgb}{0.0, 0.5, 0.0}
\definecolor{myred}{rgb}{0.8, 0.25, 0.33}
\definecolor{myblue}{rgb}{0.19, 0.55, 0.91}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{enumitem}

\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsthm}
\newtheorem{assumption}{Assumption}
\newcommand{\assumptionautorefname}{assumption}
\newtheorem{theorem}{Theorem}
\renewcommand{\theoremautorefname}{Theorem}
\newtheorem{lemma}{Lemma}
\newcommand{\lemmaautorefname}{Lemma}
\newtheorem{corollary}{Corollary}
\newcommand{\corollaryautorefname}{Corollary}
\newtheorem{proposition}{Proposition}
\newcommand{\propositionautorefname}{Proposition}
\newtheorem{remark}{Remark}
\newcommand{\remarkautorefname}{Remark}
\newtheorem{definition}{Definition}
\newcommand{\definitionautorefname}{Definition}

\renewcommand{\figureautorefname}{Figure}
\renewcommand{\subfigureautorefname}{Figure}
\newcommand{\algorithmautorefname}{Algorithm}
\newcommand{\algorithmicautorefname}{Algorithm}
\renewcommand{\sectionautorefname}{Section}
\renewcommand{\subsectionautorefname}{Section}
\renewcommand{\subsubsectionautorefname}{Section}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\renewcommand{\paragraph}[1]{\noindent\textbf{#1.}}

\newcommand{\xm}[1]{\textcolor{blue}{[XM: #1]}}
\newcommand{\wn}[1]{\textcolor{orange}{[wn: #1]}}
\newcommand{\chaowei}[1]{\textcolor{cyan}{[Chaowei: #1 ] }}
\newcommand{\zy}[1]{\textcolor{red}{[ZY: #1]}}
\newcommand{\rev}[1]{\textcolor{blue}{#1}}
\newcommand{\hj}[1]{\textcolor{magenta}{[HJ: #1]}}


\usepackage{soul}

\newcommand{\image}{\mathbf{I}}
\newcommand{\relvit}{RelViT }
\newcommand{\loss}{\mathcal{L}}


\title{RelViT: Concept-guided Vision Transformer\\
for Visual Relational Reasoning}
\iclrfinalcopy


\author{Xiaojian Ma$^1$ , Weili Nie$^2$ , Zhiding Yu$^2$ , Huaizu Jiang$^3$ , Chaowei Xiao$^{2,4}$ , Yuke Zhu$^{2,5}$, \\ \textbf{Song-Chun Zhu$^{1}$ , Anima Anandkumar$^{2,6}$}  \\
$^1$UCLA~~~$^2$NVIDIA~~~$^3$Northeastern University~~~$^4$ASU~~~$^5$UT Austin~~~$^6$Caltech \\
\texttt{\small{\{xiaojian.ma@, sczhu@stat.\}ucla.edu}} \\
\texttt{\small{\{wnie,zhidingy,chaoweix,aanandkumar\}@nvidia.com}}\\
\texttt{\small{h.jiang@northeastern.edu, yukez@cs.utexas.edu}}
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}



\maketitle

\begin{abstract}
Reasoning about visual relationships is central to how humans interpret the visual world. This task remains challenging for current deep learning algorithms since it requires addressing three key technical problems jointly: 1) identifying object entities and their properties, 2) inferring semantic relations between pairs of entities, and 3) generalizing to novel object-relation combinations, \emph{i.e.} systematic generalization. In this work, we use vision transformers (ViTs) as our base model for visual reasoning and make better use of concepts defined as object entities and their relations to improve the reasoning ability of ViTs.
Specifically, we introduce a novel \textit{concept-feature dictionary} to allow flexible image feature retrieval at training time with concept keys. This dictionary enables two new concept-guided auxiliary tasks: 1) a \textit{global} task for promoting relational reasoning, and 2) a \textit{local} task for facilitating semantic object-centric correspondence learning. 
To examine the systematic generalization of visual reasoning models, we introduce systematic splits for the standard HICO and GQA benchmarks. We show the resulting model, \emph{Concept-guided Vision Transformer} (or RelViT for short) significantly outperforms prior approaches on HICO and GQA by 16\% and 13\% in the original split, and by 43\% and 18\% in the systematic split. Our ablation analyses also reveal our model's compatibility with multiple ViT variants and robustness to hyper-parameters. \textcolor{blue}{\href{https://github.com/NVlabs/RelViT}{Code}} is available.
\end{abstract}


\vspace{-6pt}
\section{Introduction}
\vspace{-3pt}
\label{sec:intro}


Deep neural networks have achieved great success in visual recognition. However, their ability for visual relational reasoning, \emph{i.e.} reasoning with entities and their relationships in a visual scene, still falls short of human-level performances, especially in real-world domains. The challenges of common visual relational reasoning tasks, \emph{e.g.} HICO and GQA benchmarks~\citep{hico,gqa} are manifested in three aspects:
1) \textbf{object-centric learning} to identify objects (including humans) as well as their visual properties; 2) \textbf{relational reasoning} to infer all pairwise relationships between the object entities; and 3) \textbf{systematic generalization} to reason with visual entities and relations on novel object-relation combinations and extrapolate to longer reasoning hops~\citep{systematic2,systematic1}. While existing models have leveraged pre-trained object detectors~\citep{frcnn,jiang2020defense} and/or explicit symbolic reasoning methods~\citep{nsvqa} to tackle these challenges, they leave ample space for improvement.


More recently, \textbf{vision transformers} (ViTs) have become the new paradigm for visual recognition and have made great strides in a broad range of visual recognition tasks~\citep{vit,pvt,liu2021swin}. Several properties of ViT make it a compelling model choice for visual relational reasoning. First, the \textbf{self-attention mechanism} in ViT offers a strong relational inductive bias, explicitly modeling the relations between input entities. Second, the design of \textbf{image as patches} facilitates the learning of object-centric representations, as evidenced by recent works, \emph{e.g.} DINO and EsViT~\citep{dino,esvit}, that demonstrate ViTs trained with self-supervised learning (SSL) capture objects in the image without label annotations.

To investigate the efficacy of the ViT backbone for visual relational reasoning, in particular on systematic generalization, we introduce new systematic splits to canonical benchmarks and compare the ViT backbone with the CNN backbone. Results on GQA show that switching to ViTs in MCAN model~\citep{mcan} brings an immediate 11\% gain in accuracy. However, the performance gap between the original GQA testing split and the new systematic split remains considerable (15\% in accuracy) for both backbones. 
It suggests that generic ViTs still need to be improved to tackle the reasoning task, especially on systematic generalization. 
Recent works have shown that neural networks can learn representations with better generalization, by learning certain auxiliary tasks of predicting human-specified concepts~\citep{hill2020grounded,koh2020concept}. 
A natural question emerges: {\em can we exploit these concepts to improve the reasoning ability of ViTs?}

\begin{figure}[t!]
    \vskip -0.2in
    \centering
    \includegraphics[width=0.93\textwidth]{figs/main_diagram.png}
    \vspace{-14pt}
    \caption{An overview of our method. \textcolor{myred}{Red}+\textcolor{mygreen}{Green}: the learning pipeline of DINO~\citep{dino} and EsViT~\citep{esvit}; \textcolor{myred}{Red}+\textcolor{myblue}{Blue}: our pipeline. We introduce a \emph{concept-feature dictionary}, where the key is a concept $c$ and its value is a queue of image features $f$ with the same concept, to allow flexible feature retrieval with the concept keys. With the proposed dictionary, we further develop our concept-guided global and local tasks. EMA denotes the exponential moving average.
    }
    \label{fig:main_diagram}
    \vspace{-16pt}
\end{figure}

\textbf{Our approach} is to make better use of concepts (e.g. the labels in the original training dataset) in the ViT training for better relational reasoning. To this end, we first introduce a novel \emph{concept-feature dictionary}, where each key is a concept and its value is a queue of image features with the same concept, as shown in Figure \ref{fig:main_diagram}. It allows dynamic and flexible training-time image feature retrieval during training. Based on this dictionary, we then augment the canonical ViT training pipeline with two auxiliary tasks: First, to facilitate high-level reasoning about relationships, we design a \textbf{global task} that helps cluster images with the same concept together to produce semantically consistent relational representations. Second, to learn better object-centric representations, we develop a \textbf{local task} that guides the model to discover object-centric semantic correspondence across images~\citep{liu2010sift}. Thanks to the plug-and-play feature of our concept-feature dictionary, our auxiliary tasks can be easily incorporated into existing ViT training pipelines without additional input pre-processing. We term the resulting model \emph{concept-guided vision transformer} (or RelViT for short).




\begin{wrapfigure}{r}{0.35\textwidth}
  \begin{center}
    \vskip -0.22in
    \includegraphics[width=0.34\textwidth]{./figs/fig1_comp.pdf}
  \end{center}\vskip -0.2in
  \caption{\textbf{Results on HICO}. Our method improves the best baseline by 16\%, 43\%, and 7\% on the original non-systematic and two new systematic splits. \textbf{Sys.}: systematic.}
  \label{fig:motiv}
  \vskip -0.12in
\end{wrapfigure}
We evaluate our method on two standard visual relational reasoning benchmarks: HICO and GQA. Beyond the original independent and identically distributed (I.I.D.) training-testing split, we introduce new systematic splits for each dataset to examine the ability of systematic generalization, \textit{i.e.}, recognizing novel object-relation combinations. 
Our results show that RelViT~significantly outperforms previous approaches. On HICO, it improves the best baseline by 16\%, 43\%, and 7\% on the original non-systematic and two new systematic splits, respectively, as shown in~\autoref{fig:motiv}. On GQA, it further closes the gap of overall accuracy between models using visual backbone feature only and models using additional bounding box features (obtained from pre-trained object detectors) by 13\% and 18\% on the two splits. We also show that our method is compatible with various ViT variants and robust to hyper-parameters. Finally, our qualitative inspection indicates that RelViT does improve ViTs on learning relational and object-centric representations.


Our main contributions are summarized as follows:
\vskip -0.1in
\setlength{\leftmargini}{0.85em}
\begin{itemize}[topsep=0pt]
    \item We propose RelViT, by incorporating visual relational concepts to the ViT training with the newly-introduced concept-guided global and local auxiliary tasks, where a \emph{concept-feature dictionary} is proposed to enable dynamic and flexible image feature retrieval with the concept keys.
    \item In extensive experiments on the original non-systematic and new systematic split of the HICO and GQA datasets, we demonstrate the advantages of RelViT over various strong baselines for visual relational reasoning. 
    \item We perform ablation studies on \relvit to show the contributions of its key components, its compatibility to various ViT architectures, and its robustness to hyper-parameters. We provide qualitative results to confirm our improved learning of relational and object-centric representations.
\end{itemize}


\vspace{-10pt}
\section{Methodology} 
\label{sec:method}
\vspace{-8pt}
\subsection{Background}
\vspace{-5pt}
\label{sec:overview}
\paragraph{Vision transformers} 
Here we briefly review the architecture of multi-staged ViTs~\citep{vit}. Given an image $\image \in \mathbb{R}^{H\times W\times C}$, a ViT model $g$ first tokenizes the input into $N$ image tokens (patches) with a resolution of $(T, T)$: $\text{\texttt{tokenize}}(\image) = [t_1, \cdots, t_{N}], t_{i}\in \mathbb{R}^{T^2\times C}, N=HW/T^2$, where $(H, W)$ and $C$ denotes the original resolution and number of channel of the image, respectively. Then in each stage, a \emph{patch embedding} and a \emph{multi-head self attention} (MHSA) module is applied to these tokens to produce input for the next stage. 
The final output of ViT $g(\image)$ is a sequence of tokens $[z_1, \cdots, z_N]$ that correspond to the aforementioned input tokens. 
For global prediction tasks, \emph{e.g.} image categorization, a summary of the input image can be obtained by either inserting an extra \texttt{[CLS]} token to the input sequence of image tokens or performing an extra pooling operation over the output tokens~\citep{zhai2021scaling}. 




\paragraph{Self-supervised learning with DINO and EsViT} 
Our method is developed upon the recently proposed self-supervised learning (SSL) approach \textit{self-distillation with no labels} (DINO)~\citep{dino} and its follow-up EsViT~\citep{esvit}. As shown in Figure \ref{fig:main_diagram}, their main idea is to encourage the output consistency between a teacher $g_t$ and a student network $g_s$, parameterized by $\theta_t$ and $\theta_s$, respectively. Given an input image $\image$, both networks map it to a probability distribution $P_t(\image) = h_t(g_t(\image))$ and $P_s(\image) = h_s(g_s(\image))$ via an extra projection head $h(\cdot)$. The teacher and student network will be updated alternatively by following these two rules: (1) For the student network: $\theta_s \leftarrow \argmin_{\theta_s}\loss_{\operatorname{Global}}$, where $\loss_{\operatorname{Global}} = -P_t(\image)\log P_s(\image)$; (2) For the teacher network, $\theta_t$ is updated using an exponential moving average (EMA)
on $\theta_s$: $\theta_t \leftarrow \lambda\theta_t + (1-\lambda)\theta_s$, where $\lambda$ controls the updating momentum. In practice, multiple views of the input image $\image$ will be generated via data augmentation and the teacher and student networks will receive different views, preventing the task from being trivialized. EsViT further extends the image-level loss $\loss_{\operatorname{Global}}$ to patch-level by applying dense SSL~\citep{densecl} for learning correspondence between the different views, enhancing the performance on dense prediction. Readers are encouraged to refer to~\cite{dino} and~\cite{esvit} for more details about these two works.



\vspace{-0.14in}
\subsection{RelViT}
 \vskip-0.1in
\label{sec:relvit}
\relvit is a concept-guided ViT that makes better use of the concepts in the ViT training for the improved relational reasoning. In this section, we first introduce a \emph{concept-feature dictionary} to store and retrieve image features with their concept keys. We then augment the canonical ViT training pipeline with two auxiliary tasks: a global level task and a local level task, both are concept-guided by resorting to the concept-feature dictionary. Intuitively, the global task help cluster images with the same concept together to produce semantically consistent relational features, while the local task guides the model to discover object-centric semantic correspondence across images. 


\paragraph{Concept-feature dictionary}
We assume the total number of concepts is $M$, and the set of all concepts $\mathcal{C} = \{c_1, \cdots, c_M\}$. A \textit{concept-feature dictionary} is denoted by $D = \{(c_1, Q_1), \cdots, (c_M, Q_M)\}$, where each concept $c_i$ is associated with a queue $Q_i$ of image features. 
During training, each image $\image$ may come with multiple concepts, which we denote by $\mathcal{C}_\image \subset \mathcal{C}$. For instance, there may exist several human-object interactions in an image from the HICO dataset, each of which may correspond to a concept. 
As shown in Figure \ref{fig:main_diagram}, whenever a new image-concept pair $(\image, \mathcal{C}_\image)$ comes, we uniformly draw a concept code $c$ from $\mathcal{C}_\image$, pick up the queue $Q$ from the dictionary that corresponds to $c$, and then retrieve the image feature $f$ from $Q$.
Meanwhile, we pass the input image $\image$ to the teacher network $g_t$ to get the new image feature $f' = g_t(\image)$, and \emph{enqueue} it to $Q$. Note that if $Q$ is full already, we first need to \emph{dequeue} the oldest image feature from $Q$. During training, we use the retrieved image feature $f$ for the two auxiliary tasks below, rather than the input image feature $f'$.

Furthermore, the sampling strategy, \emph{i.e.} how to retrieve image feature $f$ from $Q$, plays an important role in the overall performance of our method. We consider the following two sampling strategies:
\setlength{\leftmargini}{1.5em}
\begin{itemize}[noitemsep,topsep=0pt]
    \item \textit{Uniform sampling.}~~Each image feature is drawn with equal probability from the queue, \emph{i.e.} suppose we have $N$ features in the queue, then the probability of each feature being sampled is $1/N$. This tactic encourages the diversity of the retrieved image features, benefiting the overall performance. However, some older features in the queue may largely fall behind the current model if the teacher network $g_t$ is updated quickly, eliciting unstable training.
    \vspace{3pt}
    \item \textit{``Most-recent'' sampling.}~~The sampling probability mass is allocated based on the freshness of image features, and the most recent feature has the highest chance to be retrieved. Specifically, suppose we have $N$ features in the queue $Q$ ($|Q|>=N$). Then for the $i$-th newest feature $f$, we define its weight $w_i = N-i+1$. Finally, the probability of the $i$-th newest feature being sampled is $w_i/\sum_{j=1}^N w_j$. This tactic ensures we retrieve more up-to-date features and thereby stabilizes the learning. But it may hurt the overall performance due to a lack of feature diversity, as the chance of older features being sampled is small.
\end{itemize}
Note that the feature queue is empty at the beginning of training. In this case, we simply use the input image feature $f'$ for the auxiliary tasks, and also \emph{enqueue} it to $Q$ that corresponds to the concept of the input image.
As we can show in the next, now our proposed global and local tasks reduce to DINO~\citep{dino} and EsViT~\citep{esvit}, respectively. 

\paragraph{Concept-guided global task}
Suppose we have two views $\{\image^{(1)}, \image^{(2)}\}$ of an image $\image$, 
the main idea of our concept-guided global task is to replace $\image^{(1)}$ in the DINO loss~\citep{dino} with the image feature $f$ sampled from the concept-feature dictionary, which becomes
\vskip -0.25in
\begin{align}
    \loss_{\operatorname{Global}} = -h_t(f) \log h_s(g_s(\image^{(2)})),
\end{align}
\vskip -0.15in
where $h_t$ and $h_s$ are the projection head of the teacher and student network, respectively, and $g_s$ is the student network. Intuitively, minimizing the global loss is equivalent to encouraging the similarity of any two different image features with the same concept. Hence, it can help produce more semantically consistent relational representations, in particular when the concepts stored in the concept-feature dictionary are themselves relational. 


Similar inter-class representation learning techniques have been explored before~\citep{wang2017transitive,caron2018deep}. However, these approaches require a rather complex pre-processing stage, \emph{e.g.} the images have to be split in terms of the concept before training, making them not directly applicable to existing training pipelines. Rather, with our proposed concept-feature dictionary that dynamically saves \& retrieves image features from the running storage, our concept-guided global task becomes a plug-n-play task to existing training pipelines. 


\paragraph{Concept-guided local task}
As we mentioned earlier, our concept-guided local task aims at facilitating object-centric learning, by the means of correspondence learning~\citep{liu2010sift,wang2019learning}. Recent studies have unveiled the possibility of learning correspondence with SSL~\citep{densecl,esvit}. However, only low-level correspondence between two augmented (\emph{e.g.} rotated) views of an image can be discovered, while the semantic information of objects is missing. To remedy this, we bring concepts to these methods, endowing them the capability of learning semantic correspondence from images.

Specifically, suppose we have two views $\{\image^{(1)}, \image^{(2)}\}$ of an image $\image$, and we also tokenize the image feature into a sequence of $N$ local image tokens. Then at the output of ViT, we obtain $g_t(\image^{(1)}) = [z_1^{(1)}, \cdots, z_N^{(1)}]$ and  $g_s(\image^{(2)}) = [z_1^{(2)}, \cdots, z_N^{(2)}]$, where $z$ denotes the local feature. Prior work, such as EsViT~\citep{esvit}, relies on the local features $g_t(\image^{(1)})$ and $g_t(\image^{(2)})$ for the local task. Instead, we replace $g_t(\image^{(1)})$ with the image feature $f$ retrieved from the concept-feature dictionary using the concept of the image $\image$. We then split $f$ into multiple local features, \emph{i.e.} $f = [z_1^{(f)}, \cdots, z_N^{(f)}]$ and our concept-guided local loss becomes
\vskip -0.25in
\begin{align}
    \loss_{\operatorname{Local}} = -\frac{1}{N}\sum^{N}_{i=1}h_t(z_{j^\star}^{(f)})\log h_s(z_i^{(2)}),~~j^\star = \argmax_{j}\operatorname{CosineDistance}(z_j^{(f)}, z_i^{(2)}),
\end{align}
\vskip -0.15in
where $h_t(\cdot), h_s(\cdot)$ are the projection heads that map local features to probability distributions\footnote{Note that the projection head here is different from DINO's: it works on all output local features. While in DINO, the projection head only works on the summary of input image, \emph{i.e.} the resulting feature after a max-pooling operation or the feature that corresponds to \texttt{[CLS]} in the input.}. 
Intuitively, it greedily matches the output between two local regions that have minimal feature distance -- bootstrapping the object-level semantic correspondence among images with the same concept.

\paragraph{Overall loss}
By combining the global and local tasks, we add an auxiliary task loss $\loss_{\operatorname{aux}}$ to the main loss $\loss_{\operatorname{main}}$ (\emph{e.g.} cross-entropy loss of the reasoning task). The eventual objective is
\vskip -0.25in
\begin{align}\label{loss:main}
    \loss = \loss_{\operatorname{main}} + \alpha\loss_{\operatorname{aux}},~~\loss_{\operatorname{aux}} = \loss_{\operatorname{Global}} + \loss_{\operatorname{Local}},
\end{align}
\vskip -0.1in
where a trade-off weight $\alpha$ is added for better flexibility. As we mentioned above, our method will reduce to EsViT, a baseline without concept-guided auxiliary tasks, when we use the current input features $g_t(\image^{(1)})$ instead of $f$ retrieved from our dictionary for computing $\loss_{\operatorname{Global}}$ and $\loss_{\operatorname{Local}}$. 


\vspace{-0.1in}
\section{Experiments}
\label{sec:exp}
\vspace{-0.1in}




We conduct experiments on two challenging visual relational reasoning datasets: HICO~\citep{hico} and GQA~\citep{gqa}. Besides their original non-systematic split, we introduce the systematic splits of each dataset to evaluate the systematic generalization of our method. 
First, we compare our method against various strong baselines~\citep{mallya2016learning,girdhar2017attentional,hudson2018compositional} on visual relational reasoning, as well as state-of-the-art ViTs. 
Second, we perform the ablation analysis to examine the key components of our method: ViT backbones, concept-feature dictionaries, and auxiliary tasks.
Finally, we provide qualitative results to justify the emerging image clustering in terms of concepts and the learned semantic correspondence.
Please see more details of all the evaluated tasks in the supplementary material.


\vspace{-0.1in}
\subsection{Main results I: Human-object Interaction Recognition}
\label{sec:exp_hoi}
\vspace{-0.05in}

\paragraph{Overview} HICO~\citep{hico} features the human-object interaction (HOI) recognition, \emph{i.e.} predicting all the possible HOI categories of the input image. It contains 600 HOI categories with 117 unique actions and 80 object classes. The training set includes 38116 images and the test set includes 9658 images. For a fair comparison, we follow the standard practice and mainly focus on those previous methods that do not require extra supervision~\citep{fang2018pairwise} 
or data~\citep{li2020pastanet,li2019hake,jin2021object}.
By default, we choose PVTv2-b2~\citep{wang2021pvtv2} as the ViT backbone. Regarding the concept-feature dictionary, we use the \textit{``most-recent'' sampling} and a queue length $|Q|$ of 10. The trade-off weight $\alpha$ in the overall loss is fixed to 0.1. Other hyper-parameters are inherited from DINO~\citep{dino}.

\paragraph{Systematic split} The systematic generalization in HICO has been studied before under the name ``zero-shot HOI recognition''~\citep{zeroshothoi}. The main idea is to remove some HOI categories from the training set while ensuring all the single actions and objects can still be kept in the remaining HOI categories. We thereby reuse the systematic splits offered by~\cite{vcl}. There are two splits: \emph{systematic-easy}, where only the rare HOI classes are removed from the training set; \emph{systematic-hard}, where only the non-rare HOI classes are removed besides the rare ones.
The systematic-hard split contains much fewer training instances and thereby is more challenging.

\paragraph{Concepts} In HICO, we simply use the 600 HOI categories as our default concepts. 
We also report results with other concepts (\emph{e.g.} actions, objects) in the ablation study.

\begin{table}[t!]
    \centering
    \setlength\tabcolsep{1.5pt}
    \begin{tabular}{lccccccc}
    \toprule
         \multicolumn{1}{c}{\multirow{2}{*}{Method}} & \multicolumn{1}{c}{\multirow{2}{*}{Ext. superv.}}  & \multicolumn{1}{c}{\multirow{2}{*}{Backbone}} & \multicolumn{1}{c}{\multirow{2}{*}{Orig.}} & \multicolumn{2}{c}{Systematic-easy} & \multicolumn{2}{c}{Systematic-hard}  \\ \cline{5-8}
         &&&&\small{Full cls.}&\small{Unseen cls.}&\small{Full cls.}&\small{Unseen cls.} \\
    \midrule 
         \cite{mallya2016learning}$^*$& & ResNet-101 &33.8 & - & - &-&-\\  \cite{girdhar2017attentional}$^*$ &bbox &ResNet-101& 34.6 & - & -& -&-\\  \cite{fang2018pairwise}$^*$  &pose &ResNet-101& 39.9 & - &-& -&-\\  \cite{vcl}$^\dagger$ &&ResNet-101 &28.57 &26.65 &11.94&21.76 &10.58\\  \midrule
         ViT-only & &PVTv2-b2&35.48 &31.06 &11.14&19.03 &18.85\\  EsViT~(\citeyear{esvit})  &&PVTv2-b2 &38.23 &35.15 &11.53 &22.55&21.84 \\  \rowcolor[gray]{0.9}
         RelViT (Ours)  &&PVTv2-b2 & 39.4 & 36.99 & 12.26&22.75&22.66\\  \rowcolor[gray]{0.9}
         RelViT + EsViT (Ours) & &PVTv2-b2& \textbf{40.12} & \textbf{37.21} & \textbf{12.51} &\textbf{23.06}&\textbf{22.89} \\  \bottomrule
    \end{tabular}
    \vskip -0.1in
    \caption{\textbf{Results on HICO dataset.} Some methods requires extra supervision. Bbox/Pose means object-detection or pose estimation is needed. All results are reported in mAP. $^*$Results reported in the original papers; $^\dagger$Introduces the systematic split we use in the experiments. \textbf{Full cls.}: results reported on all 600 HOI categories; \textbf{Unseen cls.}: results reported on the held-out HOI categories from the training set for testing systematic generalization. \textbf{Ext. superv.}: extra supervision.}
    \label{tab:main_hico}
    \vskip -0.25in
\end{table}

\paragraph{Results} In~\autoref{tab:main_hico}, we compare our method with several counterparts. The results read that even a simple model with PVTv2-b2 (\textbf{25.4M parameters}) backbone can outperform many previous methods using ResNet-101 (\textbf{44.7M parameters}) and lots of bell and whistles. This confirms the great potentials of ViTs in visual relation reasoning. By further adding our global and local tasks, we attain 4-6 mAP gain on original and systematic splits. We also observe that EsViT~\citep{esvit}, a recently proposed SSL approach, also outperforms the ViT-only baseline. Therefore, we combine their SSL task and our concept-guided tasks and reach the peak performance (40.12 mAP) on the original HICO split. Although \textbf{we do not utilize any extra supervison}, RelViT+EsViT beats the current state-of-the-art \cite{fang2018pairwise} that uses the additional ``pose'' supervision that does not exist in the HICO dataset.
Overall, we raise the results of a fair counterpart~\citep{girdhar2017attentional} that only exploits extra bbox supervision (which is included in HICO) by 16\% (34.6 $\rightarrow$ 40.12) on the original split. For systematic splits, we raise the results of~\cite{vcl} by 43\% (26.65 $\rightarrow$ 37.21) on the systematic-easy split and 7\% (21.76 $\rightarrow$ 23.06) on the systematic-hard split.
Finally, although the gap between systematic and non-systematic split still exists (partly due to the much smaller training set for systematic splits), our method makes significant progress, especially on unseen classes (+12.3 mAP on systematic-hard). This further demonstrates the advantages of our concept-guided ViT in systematic generalization.  

\vspace{-0.1in}
\subsection{Main results II: Visual Question Answering}
\vspace{-0.05in}
\label{sec:exp_vqa}

\paragraph{Overview} GQA~\citep{gqa} is a recent visual question answering (VQA) dataset with a focus on relational reasoning. Each question is also labeled with semantics. By default, it offers both pretrained-CNN grid features and region features obtained through Faster R-CNN~\citep{frcnn}. For counterparts, we focus on fair comparisons and therefore exclude those that require massive vision-language pretraining~\citep{li2019visualbert}. Notably, \textbf{we do not use extra supervision, such as scene graph}~\citep{vg}. The RelViT configuration is almost the same as in HICO, except that we apply the \emph{uniform sampling} instead as we find it empirically works better. We employ MCAN-Small~\citep{mcan} as our VQA model and the ImageNet1K-pretrained PVTv2-b2 as our vision backbone.
The results are reported on the full validation set of GQA.

\paragraph{Systematic split} In GQA, we especially examine the facet of \emph{productivity} in systematic generalization, \emph{i.e.} the ability of reasoning with longer reasoning hops~\citep{systematic1}. To this end, we exploit the extra semantics label associated with the GQA questions. We observe that the semantics in GQA break down each question into a sequence of ``reasoning hops'', where a distribution of reasoning hops can be found in Figure 3. See the supplementary material for examples. Therefore, our idea is to exclude questions with longer reasoning hops from the training set.  We end up only keeping questions with less than 5 reasoning hops in the training set. We refer to this setting as the systematic split (``Sys.'') in the results.

\paragraph{Concepts} Inspired by recent research on vision-language pretraining~\citep{tan2019lxmert,li2019visualbert,li2020oscar}, we obtain concepts by parsing the questions into keywords. Specifically, we only keep certain verbs, nouns, and adjectives that contain significant meanings (e.g. actions, objects, characteristics, etc), ending up with 1615 concepts. 
Due to the space limit, readers may find more details on concept parsing in the supplementary material. 

\paragraph{Results} We report the comparison results on the original and systematic splits in~\autoref{tab:main_gqa}. The main goal of our experiments on GQA mainly is to verify if 
our method can help reduce the gap between models using backbone features only and models using additional bbox features (with dense object detectors). Besides, we also examine to which extent our method can improve systematic generalization.
Firstly, we observe that using ViT can largely alleviate the aforementioned gap (51.1 $\rightarrow$ 56.62), suggesting that the object-centric representations emerge in ViTs. It implies the potential of using ViTs in eliminating the need for external object detectors. By further adding our proposed auxiliary tasks, we achieve the peak performance and raise the results of MCAN-Small w/o bbox features by 13\% (51.1 $\rightarrow$ 57.87) on the original split and 18\% (30.12 $\rightarrow$ 35.48) on the systematic split.
\textbf{Without any detection pretraining or bbox features}, 
our method achieves very close results to MCAN-Small w/ bbox features on both two splits.
The additional results in appendix demonstrate that the marginal gap could be further eliminated if we apply larger backbone models (PVTv2-b2 has much fewer parameters than ResNet-101).

\begin{table}[t!]
\begin{minipage}{0.62\textwidth}
    \centering
    \setlength\tabcolsep{2pt}
    \begin{tabular}{lcccc}
    \toprule
         Method & Bbox feat.$^*$ & Backbone & Orig. & Sys.  \\
    \midrule 
         BottomUp~(\citeyear{topdownbottomup}) &\ding{51}& ResNet-101 & 53.21  & -\\ MAC~(\citeyear{mac})&\ding{51}& ResNet-101& 54.06  &- \\ MCAN-Small~(\citeyear{mcan}) &\ding{51}& ResNet-101 &58.35  &36.21 \\ MCAN-Small~(\citeyear{mcan})& & ResNet-101&51.1  &30.12 \\ \bottomrule
         ViT-only& & PVTv2-b2& 56.62&31.39 \\ EsViT~(\citeyear{esvit})& & PVTv2-b2&56.95&31.76 \\ \rowcolor[gray]{0.9}
         RelViT (Ours)& & PVTv2-b2&  57.87&35.48 \\ \bottomrule
    \end{tabular}
    \vskip-0.1in
    \captionof{table}{\textbf{Results on GQA dataset.} All results are reported in overall accuracy. $^*$With extra Faster R-CNN bbox features.}
    \label{tab:main_gqa}
\end{minipage}
\hfill
\begin{minipage}{0.36\textwidth}
\centering
    \includegraphics[width=2in]{./figs/fig3.pdf}
    \vskip -0.15in
    \captionof{figure}{    \label{fig:gqa_multihops}
 Histogram of reasoning hops over GQA training questions.}
\end{minipage}
\vskip -0.25in
\end{table}


\vspace{-8pt}
\subsection{Why do our auxiliary tasks work?}
\label{sec:exp_qual}
\vspace{-3pt}

The results in the previous section suggest that RelViT outperforms its counterparts on the challenging relational reasoning tasks. Now we would like to provide more insights into our method design by answering the question: why do our auxiliary tasks work? To this end, we perform a diverse set of analyses on accessing the impact of key components in \relvit. We also qualitatively justify the intuitions of two auxiliary tasks. These results are reported on the HICO dataset.

\vspace{-8pt}
\subsubsection{Ablation study}
\label{sec:exp_abl_rcl}
\vspace{-3pt}

\paragraph{Different ViT architectures}
The first aspect we examine is the ViT architecture. Besides the default choice on PVTv2-b2, we test our method with the original ViT-S/16~\citep{vit} and another prevalent architecture Swin-Small~\citep{liu2021swin}. The results are presented in~\autoref{fig:a} and~\autoref{fig:b}, respectively. These two architectures can both benefit from our auxiliary tasks and we have similar advantages over counterparts as in the default setting, which confirms our compatibility to various ViT variants. Full quantitative results are provided in the supplementary.

\paragraph{Implementation of concept-feature dictionary}
We conduct ablations on three facets of concept-feature dictionary: the choice of concepts, sampling tactics, and the size of queue $|Q|$. In~\autoref{fig:c}, we compare three different concept choices: actions, objects, and HOIs with our best model. The results suggest that all three choices can bring improvement to the baseline without any feature queue (denoted as ``None'') while using HOIs and objects brings larger improvement. We hypothesize that the proposed auxiliary tasks need more ``delicate'' concepts to guide the ViT training but actions in HICO tend to be vague and even ambiguous~\citep{zeroshothoi}. Therefore, albeit the consistent advantages of our method in terms of different concept selections, a careful design of concept space could still be pivotal to achieve the peak performance in relational reasoning. 

Furthermore, we show the interplay between sampling strategies and queue size $|Q|$ in~\autoref{fig:d}. Interestingly, $|Q|$ has a much smaller impact on the performance with the \textit{``most-recent'' sampling} than that with the \textit{uniform sampling}. As we mentioned in~\autoref{sec:relvit}, the \textit{uniform sampling} could help with more diverse features but could also elicit unstable training. Larger $|Q|$ makes the two consequences in the \textit{uniform sampling} more prominent, thus causing worse performance when stable training is the bottleneck (\emph{e.g.} in a small dataset like HICO). Rather, the \textit{``most-recent'' sampling} can be less sensitive to $|Q|$ as only the recent features could be sampled even when $|Q|$ is large.

\paragraph{Auxiliary tasks}
In~\autoref{fig:e}, we show the results of only adding our global or local task in $\loss_{\operatorname{aux}}$. Surprisingly, just using the local task is enough to deliver competitive results in the HICO task. This suggests that the real bottleneck in ViTs seems to be better object-centric representations, as our local task is designed for this. Nonetheless, adding our global task can still elicit clear advantages over other counterparts that do not exploit concept-guided learning.

\paragraph{Robustness to $\alpha$}
We sweep the trade-off weight $\alpha$ from 0.02 to 0.5 and report the results in~\autoref{fig:f}, where \textbf{solid} and \textbf{dash} represent our method and the baseline, respectively. It is observed that adding the proposed auxiliary tasks always achieves better performances than the baseline, indicating our method is robust to hyper-parameters.  Moreover, the improvements become slightly more significant when $\alpha$ is relatively large (but not too large). The peak performances in different splits all appear around $\alpha=0.1$, which we thus use as our default choice. 

\begin{figure}[t!]
     \centering
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{./figs/fig4_vit.pdf}
         \vskip-0.08in
         \caption{RelViT with ViT-S/16.}
         \label{fig:a}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{./figs/fig4_swin.pdf}
         \vskip-0.08in
         \caption{RelViT with Swin-Small.}
         \label{fig:b}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{./figs/fig4_useofconcept.pdf}
         \vskip-0.08in
         \caption{Different choice of concepts.}
         \label{fig:c}
     \end{subfigure}
     \vfill
         \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{./figs/fig4_queue.pdf}
         \vskip-0.08in
         \caption{Different queue length $|Q|$.}
         \label{fig:d}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{./figs/fig4_localglobal.pdf}
         \vskip-0.08in
         \caption{Global or local tasks}
         \label{fig:e}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \includegraphics[width=\textwidth]{./figs/fig4_alpha.pdf}
         \vskip-0.13in
         \caption{Robustness to $\alpha$.}
         \label{fig:f}
     \end{subfigure}
    \vskip-0.1in
     \caption{\textbf{Ablation study on HICO.} We investigate the impact of ViT architectures, implementation of concept-feature dictionary, auxiliary tasks, and the weight $\alpha$ on the performance of our method. \textbf{Sys.}: systematic.}
     \label{fig:ablation}
\end{figure}


\begin{figure}[t]
    \vskip -0.15in
    \centering
    \includegraphics[width=\textwidth]{./figs/fig5_tsne.png}
    \vskip-0.2in
    \caption{Visual illustrations of image features against HOI categories on the HICO test set via t-SNE. We compare the features obtained by ViT without any auxiliary task (ViT-only), ViT with non-concept auxiliary tasks (EsViT), and RelViT. Besides those clusters that are identified with the other two baselines, \textbf{clusters that can only be identified with RelViT are highlighted.}}
    \label{fig:tsne}
        \vskip-0.15in
\end{figure}

\vspace{-2pt}
\subsubsection{Qualitative inspection}
\vspace{-3pt}

\paragraph{Features vs. concepts} To further justify whether our global task can truly facilitate the learned representation to be more relational, we illustrate the learned output features (max-pooling on all the output tokens) by t-SNE visualization in~\autoref{fig:tsne}. Different colors correspond to different HOI categories, \emph{i.e.} the concepts we used in RelViT. The results read that more clusters can be identified over the image features extracted by RelViT; therefore our concept-guided global task can encourage the learned features to be more discriminative regarding the relational concepts than the baselines.  

\label{sec:exp_rcl_ins}

\paragraph{Semantic correspondence} We also probe the learned semantic correspondence that could be encouraged by our local task, by intuition. We aim at comparing the correspondence extracted from a model trained with different auxiliary tasks, \emph{i.e.} no auxiliary task, no-concept auxiliary tasks, and our auxiliary tasks. We consider two settings: 1) semantic setting (two images that belong to the same concept, \emph{e.g.} both contains a cat), and 2) non-semantic setting (two views of the same image). Results in~\autoref{fig:corr} highlight the high-similarity matches. 
Although our method and non-concept baseline (EsViT) both work fine in the non-semantic setting, our method can identify the semantic correspondence on more objects thanks to the concept guidance.
Not surprisingly, baseline w/o any auxiliary task (ViT-only) performs the worst as it may suffer from over-smoothness~\citep{gong2021improve} and lose all the meaningful spatial information after fine-tuning on the target task.


\vspace{-0.1in}
\begin{figure}[t!]
    \centering
    \includegraphics[width=\textwidth]{./figs/fig5_corr.png}
        \vskip-0.2in
    \caption{Visualization of correspondence. The correspondence is extracted between two views of the same image (upper) and two images that belong to the same concept (lower), using the learned model on HICO. \textbf{RelViT can extract correspondence on more objects in the two images (semantic correspondence) setting}. Best viewed on screen.}
    \label{fig:corr}
    \vskip -0.2in
\end{figure}

\section{Related Work}
\vskip-0.1in
\label{sec:related_work}


\paragraph{Systematic generalization in visual reasoning} Systematic generalization~\citep{systematic1,systematic2} characterizes to which extent a learning agent can identify and exploit the underlying entities and relations of the training data, and generalize to novel combinations and longer reasoning hops. There has been extensive research on inspecting and tackling systematic generalization in visual reasoning~\citep{clevr,shape3d,dsprite,kuhnle2017shapeworld}. However, most of them only focus on controlled and synthetic domains~\citep{gscan,raven,pgm,halma,bongard-logo}, while the open-ended real-world domains are largely neglected with very few exceptions~\citep{zeroshothoi,vprom,bongard_hoi}. In this paper, we tackle systematic generalization in visual relational reasoning with natural images, thereby filling the gap between synthetic and real domains.

\paragraph{Object-centric and relational representations} Many seminal research reveals that ML models can benefit from object-centric and relational representations with better sample efficiency and generalization~\citep{phrases,oogen1,oogen4}. However, obtaining such representations from unstructured inputs, \emph{i.e.} raw images, still remains challenging~\citep{iodine,slotattention,disentanglebenchmark,yu2021unsupervised}. Prevalent approaches adopt a latent variable model to explicitly infer the foreground-background split as well as objects \& relations~\citep{air,space,aog}, while recent findings suggest that they can be an emerging property of transformers trained with self-supervised objectives~\citep{dino,esvit}. Our goal aligns better with the later regime, as it enables implicit representations and thus could be more versatile and efficient. A key difference is that these methods do not exploit concepts in reasoning benchmarks, making them less capable of learning semantic representations.


\paragraph{Self-supervised learning for ViTs} The recent resurgence on self-supervised learning (SSL) of image models has delivered impressive results on many few-shot or zero-shot tasks~\citep{cpc}. From a high-level view, these approaches can be categorized into \textit{contrastive}~\citep{moco,simclr} and \textit{non-contrastive}~\citep{simsiam}. However, not all SSL avenues work well with vision transformers (ViTs) and some delicate design may be needed. \cite{dino} found their non-contrastive learning objective (DINO) manifested better quantitative results and emerging properties on ViTs. \cite{mocov3} brought similar results on contrastive SSL. \cite{esvit} further introduced patch-level SSL objective to ViTs for dense prediction tasks. In this paper, instead of proposing a new SSL approach, we make better use of concepts for ViT training, which can be directly applied to the existing SSL objectives for the improved visual reasoning.

\vspace{-8pt}
\section{Conclusion}
\label{sec:conclusion}
\vskip-0.1in
In this paper, our goal is to seek a better inductive bias for visual relational reasoning, especially on real-world data. We found ViTs to be a promising candidate due to their potential on relational reasoning, object-centric learning, and systematic generalization. We further presented RelViT, a simple yet efficient method for exploiting concepts in the visual relational reasoning tasks to boost the performances of ViTs. In specific, we proposed two auxiliary tasks in \relvit: a global task for semantically consistent relational representation, and a local task for learning object-centric semantic correspondence. These two tasks are made possible through the use of our proposed concept-feature dictionary. RelViT largely outperforms other counterparts on two challenging visual relational reasoning benchmarks. While we mainly focus on extending ViTs to visual reasoning using auxiliary tasks, further exploration of combining our work with architectural modification over ViTs to enable better generalization could be a new direction for future work.




\bibliography{main}
\bibliographystyle{iclr2022_conference}

\newpage
\appendix


\section{A formal description of learning in RelViT}

Algorithm 1 formally depicts the execution flow of RelViT.


\begin{algorithm}[H]
\small
\caption*{\small \textbf{Algorithm 1} RelViT: Concept-guided Vision Transformer 
\label{alg:main}}
\begin{algorithmic}[1]
\REQUIRE A set of training images with concepts $\{(\image_1, C_1), \cdots\}$, an image augmentation function \texttt{aug}$(\cdot)$, momentum update factor $\lambda$, loss weight $\alpha$, a concept-feature dictionary $D$, teacher and student ViT $g_t$ and $g_s$, parameterized by $\theta_t$ and $\theta_s$, respectively.
\FOR{$(\image_i, C_i)$ in $\{(\image_1, C_1), \cdots\}$}
  \STATE $\image_i^{(1)},~\image_i^{(2)} = \text{\texttt{aug}}(\image_i),~ \text{\texttt{aug}}(\image_i)$    
  \STATE Uniformly draw a concept code $c \sim C_i$. 
  \STATE Retrieve $Q$ from $D$ with $c$.
  \IF{$Q$ is not empty}
    \STATE Sample feature $f \sim Q$, following some sampling tactics.
    \STATE $\loss_{\operatorname{aux}} = \loss_{\operatorname{Global}}(f,~g_s(\image_i^{(2)}))$ + $\loss_{\operatorname{Local}}(f,~g_s(\image_i^{(2)}))$
    \STATE Insert feature $g_t(\image_i^{(1)})$ into $Q$; if it is full, remove the oldest feature.
    \ELSE
    \STATE $\loss_{\operatorname{aux}} = \loss_{\operatorname{Global}}(g_t(\image_i^{(1)}),~g_s(\image_i^{(2)}))$ + $\loss_{\operatorname{Local}}(g_t(\image_i^{(1)}),~g_s(\image_i^{(2)}))$
  \ENDIF
  \STATE Update $\theta_s$ with the loss function $\loss = \loss_{\operatorname{main}} + \alpha\loss_{\operatorname{aux}}$.
  \STATE Update $\theta_t$ using an EMA: $\theta_t \leftarrow \lambda\theta_t + (1-\lambda)\theta_s$.
\ENDFOR \\
\end{algorithmic}
\end{algorithm}


\section{Additional details on \relvit}

\subsection{Input pipeline}

We adopt the following data augmentation pipeline for the generating the additional views for our two auxiliary tasks
\begin{enumerate}
    \item Randomly crop and resize the image into $(224, 224)$ with scale ratio $(0.2, 1.0)$;
    \item Randomly jitter the color of the image on brightness, contrast saturation and hue with probability of $(0.4, 0.4, 0.4, 0.1)$, respectively;
    \item Randomly turn the image into gray scale with probability $0.2$;
    \item Randomly apply Gaussian blur with kernel size $23$ and sigma $(0.1, 2.0)$ and probability $0.5$;
    \item Randomly flip the image horizontally.
\end{enumerate}

Notably, we apply a random crop operation to ensure that all the input images for our auxiliary tasks contain the same number of patches.

\subsection{Hyper-parameters and baselines}

\autoref{tab:alg_param} summarizes the hyper-parameters used by RelViT. We inherit most of the parameters from DINO~\citep{dino}.

\begin{table}[H]
\centering
\caption{Hyperparameters for RelViT.} 
\begin{tabular}{l|l}
\toprule
Parameter &  Value\\
\midrule
Optimizer &AdamW with epsilon $1e^{-1}$ (HICO) / $1e${-5} (GQA)\\
Gradient clipping norm & No grad clipping (HICO) / $0.5$ (GQA) \\
Base learning rate & $1.5e^{-4}$ (HICO) / $3e^{-5}$ (GQA) \\
Learning rate schedule & $0.1$ scale with milestones $[15, 25]$ (HICO) / $[8, 10]$ (GQA)\\
Batch size & 16 (HICO) / 64 (GQA) \\
Total training epochs & 30 (HICO) / 12 (GQA) \\
\midrule
Temperature $\tau$ in DINO loss & $0.04$ for teacher and $0.1$ for student, we don't use schedule. \\
Momentum $m$ for teacher & $0.999$ \\
Center $m$ for center features & $0.9$ \\
\midrule
Sampling method & \textit{``most-recent''} (HICO) / \textit{uniform} (GQA) \\
Queue size $|Q|$ & 10 \\
\bottomrule
\end{tabular}
\label{tab:alg_param}
\end{table}

\autoref{tab:baselines} summarizes the key details about the loss implementation of different baselines and RelViT.

\begin{table}[h]
\centering
\caption{Key details about the loss implementation in baselines and \relvit.}
\begin{tabular}{llll}
\toprule
               & $\loss_{\operatorname{Global}}$ & $\loss_{\operatorname{Local}}$ & Compare \texttt{student(aug(img))} with                                                 \\ \midrule
DINO           & x                               &                                                                              & \texttt{teacher(aug(img))}                                                              \\
EsViT          & x                               & x                                                                            & \texttt{teacher(aug(img))}                                                              \\
RelViT         & x                               & x                                                                            & \texttt{queues{[}concept(img){]}.pop()}                                                 \\
RelViT + EsViT & x                               & x                                                                            & \makecell[tl]{\texttt{teacher(aug(img))} and\\ \texttt{queues{[}concept(img){]}.pop()}} \\ \bottomrule
\end{tabular}
\label{tab:baselines}
\end{table}

\section{Additional details on the datasets}

\subsection{HICO}

\subsubsection{Original and systematic splits}
Besides the official training/testing split, we adopt the splits for systematic generalization presented in~\citep{vcl}. It offers two splits that follow different strategies to select held-out HOI categories. \textbf{Systematic-easy} only select \textit{rare} HOI categories (with less than 10 training samples), while \textbf{Systematic-hard} select \textit{non-rare} categories instead. Therefore, the training set of \textbf{Systematic-hard} will contain much fewer samples and become more challenging. Some basic statistics of these training/testing splits can be found in~\autoref{tab:hico_split}.

\begin{table}[h]
    \centering
    \begin{tabular}{ccccc}
    \toprule
    Splits & \#Training samples & \#Training HOIs & \#Testing samples & \#Testing HOIs   \\
    \midrule
    Original & 38118 &600 & 9658 &600 \\
    Systematic-easy &37820& 480 &9658 &600 \\
    Systematic-hard &9903& 480&9658 &600 \\
    \bottomrule
    \end{tabular}
    \caption{Statistics of the splits of HICO dataset.}
    \label{tab:hico_split}
\end{table}

\subsubsection{Implementation of $\loss_{\operatorname{main}}$}

In HICO, there might be multiple HOIs for a single image. We, therefore, formulate the HOI prediction task as a multi-class classification problem. Specifically, the model makes 600 binary classifications and $\loss_{\operatorname{main}}$ in~\eqref{loss:main} is a binary cross-entropy loss.

\subsection{GQA}

\subsubsection{Original and systematic splits}

We introduce a systematic split for the GQA dataset that is based on reasoning hops. Specifically, we remove those questions that have more than 4 reasoning hops from the training set. Some basic statistics of these training/testing splits can be found in~\autoref{tab:gqa_split}.

\begin{table}[h]
    \centering
    \begin{tabular}{ccc}
    \toprule
    Splits & \#Training samples & \#Testing samples  \\
    \midrule
    Original &943000 &132062 \\
    Systematic &711945 &32509 \\
    \bottomrule
    \end{tabular}
    \caption{Statistics of the splits of GQA dataset.}
    \label{tab:gqa_split}
\end{table}

\subsubsection{Reasoning hops}

Since all the questions and answers in the GQA dataset are synthetic, it additionally provides ``semantics'' that characterizes the reasoning procedure that generates the answer from a question and a visual scene. These semantics are composed of multiple ``reasoning primitives'' that act like functions: receiving arguments and generating output for the next reasoning step. It is believed they can reflect whether a question will require complex multi-hop reasoning -- a pivotal angle of systematic generalization. Therefore we develop our systematic split with it. \autoref{tab:gqa_reasoning} provides a few examples on semantics.

\begin{table}[h]
    \centering
    \begin{tabular}{l|l}
    \toprule
    Question & Semantics (Reasoning hops) \\
    \midrule     
    \makecell[l]{Is the pizza with the pepper small \\and covered?} & \makecell[l]{\small{\texttt{relate([0], pizza, with, s(1130674));}} \\
    \small{\texttt{filter([1], pizza);}} \\
    \small{\texttt{verify([2], covered);}} \\
    \small{\texttt{verify\_size([2], small);}} \\
    \small{\texttt{and([3,4]);}}} \\
    \midrule
    \makecell[l]{Do you see any tablecloths or \\dressers?} & \makecell[l]{
    \small{\texttt{select([], dreser);}}\\
    \small{\texttt{exist([0], ?);}}\\
    \small{\texttt{select([], tablecloth);}}\\
    \small{\texttt{exist([2], ?);}}\\
    \small{\texttt{or([1, 3], ?);}}
    } \\
    \midrule
    \makecell[l]{Are there microwave ovens to the\\ right of the appliance near the\\ window?} & \makecell[l]{\small{\texttt{select([], window);}}\\
    \small{\texttt{relate([0], appliance, near, s(1297947))}} \\
    \small{\texttt{relate([1], microwave, right, s(1297947));}} \\
    \small{\texttt{exist([2], ?);}}
    }\\
    \bottomrule
    \end{tabular}
    \caption{Examples of semantics (reasoning hops) in GQA dataset.}
    \label{tab:gqa_reasoning}
\end{table}

\subsubsection{Concept parsing}
We obtain the concepts in the GQA dataset by parsing the questions into word tokens. Specifically, we construct a set of concepts that contain nouns, verbs, and adjectives that are with significant meaning. We also manually filter some ambiguous words from this set. The resulting concept set contains 1615 concepts. 

We use the python nltk package to process the question. The parsing procedure starts with part-of-speech tagging, where we only keep nouns (\texttt{NN}), verbs (\texttt{VB}) and adjectives (\texttt{JJ}). Then we lemmatize the remaining words to obtain the minimal form of them. Finally, we remove those that do not present in the pre-selected concept list. Additionally, we skip questions with ``No'' as the answer as the question may be unrelated to the image. We provide the statistics of the concepts in GQA in~\autoref{tab:gqa_concepts}. The number of associated questions of all the 1615 concepts and a histogram on the number of concepts for each question is presented in \autoref{fig:histo_question} and \autoref{fig:histo_concept}, respectively.

\begin{table}[h]
    \centering
    \begin{tabular}{l|l}
    \toprule
    Item & Value \\
    \midrule  
    Questions without concept & 166217 out of 943000 (17.6\%)\\
    Concepts without any question & 14 \\
    Concepts with $<10$ questions & 209\\
    Averaged \#questions per concept & 1030.9\\
    Median \#questions per concept & 106\\
    \midrule
    Top 20 concepts and their \#associated questions & \makecell[tl]{
\small{man 52295}\\
\small{animal 44070}\\
\small{furniture 36523}\\
\small{white 33141}\\
\small{front 30779}\\
\small{person 28751}\\
\small{vehicle 26133}\\
\small{woman 25769}\\
\small{bottom 22624}\\
\small{black 22517}\\
\small{device 21962}\\
\small{food 19683}\\
\small{fence 19172}\\
\small{chair 18872}\\
\small{table 18649}\\
\small{hold 18090}\\
\small{shirt 16483}\\
\small{blue 15434}\\
\small{car 14838}\\
} \\
    \bottomrule
    \end{tabular}
    \caption{Statistics of concepts in GQA training set.}
    \label{tab:gqa_concepts}
\end{table}

\begin{figure}[h]
    \centering
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
    \includegraphics[width=\textwidth]{./figs/histo_question.pdf}
    \caption{Histogram of number of questions per concept.}
    \label{fig:histo_question}
     \end{subfigure}
     \hfill
    \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{./figs/histo_concept.pdf}
         \caption{Histogram of number of concepts per question.}
    \label{fig:histo_concept}
     \end{subfigure}
     \caption{Histograms of concepts in GQA training set.}
\end{figure}

\subsubsection{Implementation of $\loss_{\operatorname{main}}$}

GQA is formulated as a classification problem, \emph{i.e.} the learner needs to select an answer from the pre-defined answer set; thus $\loss_{\operatorname{main}}$ in~\eqref{loss:main} is a cross-entropy loss.

\section{Full quantitative results on ablation studies}
We provide the full quantitative results of the ablation studies in \autoref{tab:quant_ablations}.

\begin{table}[h]
    \centering
    \setlength\tabcolsep{3pt}
    \begin{subtable}[t]{0.49\textwidth}
        \centering
    \begin{tabular}{cccc}
    \toprule
         Method & Orig.& Sys.-easy & Sys.-hard  \\
    \midrule
         ViT-only &27.67 &26.72 &15.23 \\
         EsViT &30.83 &30.28 &17.67 \\
         RelViT &31.45 &30.88 &18.33 \\
         EsViT+RelViT &33.15 &31.09 &19.24 \\
    \bottomrule
    \end{tabular}
    \caption{RelViT with ViT-S/16}
    \label{tab:quant_a}
    \end{subtable}
    \hfill
    \setlength\tabcolsep{3pt}
    \begin{subtable}[t]{0.49\textwidth}
        \centering
    \begin{tabular}{cccc}
    \toprule
         Method & Orig. &Sys.-easy & Sys.-hard  \\
    \midrule
         ViT-only &36.08 &31.22 &20.88 \\
         EsViT &38.11 &35.22 &21.82 \\
         RelViT &39.07 &36.27 &21.81 \\
         EsViT+RelViT &39.86 &37.17 &22.82 \\
    \bottomrule
    \end{tabular}
    \caption{RelViT with Siwn-Small}
    \label{tab:quant_b}
    \end{subtable}
    \vfill
    \setlength\tabcolsep{3pt}
    \begin{subtable}[t]{0.49\textwidth}
    \centering
    \begin{tabular}{cccc}
    \toprule
         Concepts & Orig.& Sys.-easy & Sys.-hard  \\
    \midrule
         None &35.48 &31.06 &19.03 \\
         Actions &38.8 &35.14 &22.34 \\
         Objects &39.24 &36.59 &21.65 \\
         HOIs &40.12 &37.31 &22.79 \\
    \bottomrule
    \end{tabular}
    \caption{Different choice of concepts}
    \label{tab:quant_c}
    \end{subtable}
    \hfill
    \setlength\tabcolsep{3pt}
    \begin{subtable}[t]{0.49\textwidth}
        \centering
    \begin{tabular}{ccc}
    \toprule
         $|Q|$ & Most-recent &Uniform  \\
    \midrule
         5 &39.71 &39.75  \\
         10 &40.12 &39.93 \\
         30 &39.78 &39.06 \\
         50 &39.41 &38.49 \\
    \bottomrule
    \end{tabular}
    \caption{Different queue length $|Q|$}
    \label{tab:quant_d}
    \end{subtable}
    \vfill
    \setlength\tabcolsep{3pt}
    \begin{subtable}[t]{0.49\textwidth}
        \centering
    \begin{tabular}{cccc}
    \toprule
         Tasks & Orig.& Sys.-easy & Sys.-hard  \\
    \midrule
         None & 35.48 &31.06 &19.03 \\
         Global &37.63 &34.88 &21.07 \\
         Local &39.54 &36.74 &22.55 \\
         Both &40.12 &37.31 &22.79 \\
    \bottomrule
    \end{tabular}
    \caption{Global or local tasks}
    \label{tab:quant_e}
    \end{subtable}
    \hfill
    \setlength\tabcolsep{3pt}
    \begin{subtable}[t]{0.49\textwidth}
    \centering
    \begin{tabular}{cccc}
    \toprule
         $\alpha$ & Orig.& Sys.-easy & Sys.-hard  \\
    \midrule
         0.02 &38.45 &36.32 &21.85 \\
         0.05 &39.49 &36.99 &22.62 \\
         0.1 &40.12 &37.31 &23.06 \\
         0.2 &40.04 &36.67 &22.94 \\
         0.5 &39.54 &35.42 &22.79 \\
    \bottomrule
    \end{tabular}
    \caption{Robustness to $\alpha$}
    \label{tab:quant_f}
    \end{subtable}
    \caption{Full quantitative results (on full class of HICO) of the ablation studies.}
    \label{tab:quant_ablations}
\end{table}

\section{Additional results}








\subsection{RelViT with larger backbone models}

As we mentioned in~\autoref{sec:exp_hoi}, the ViT backbone we use (PVTv2-b2) only has \textbf{25.4M} parameters, even less than the commonly-used ResNet-101 (\textbf{44.7M} parameters). Therefore, we testify RelViT with larger state-of-the-art ViT backbones: PVTv2-b3 (\textbf{45.2M} parameters) and Swin-base (\textbf{88M} parameters)~\citep{liu2021swin} and provide the results on HICO and GQA below:

\begin{table}[h]
\centering
\caption{Results with larger ViT models on HICO.}
\setlength\tabcolsep{4pt}
\begin{tabular}{ccccc}
\toprule
HICO mAP        & \cite{fang2018pairwise} & \makecell[tc]{RelViT + EsViT \\(PVTv2-b2)} & \makecell[tc]{RelViT + EsViT\\(PVTv2-b3)} & \makecell[tc]{RelViT + EsViT\\(Swin-base)} \\
\midrule
Original        & 39.9               & 40.12                     & 42.61                     & \textbf{43.98}                  \\
Systematic-easy & -                  & 37.21                     & 39.92                     & \textbf{42.04}                  \\
Systematic-hard & -                  & 23.06                     & 25.56                     & \textbf{28.36} \\                
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Results with larger ViT models on GQA.}
\setlength\tabcolsep{4pt}
\begin{tabular}{ccccc}
\toprule
GQA overall accuracy & \makecell[tc]{MCAN-Small\\(w/ bbox)} & \makecell[tc]{RelViT\\(PVTv2-b2)} & \makecell[tc]{RelViT\\(PVTv2-b3)} & \makecell[tc]{RelViT\\(Swin-base)} \\
\midrule
original             & 58.35             & 57.87             & 61.41             & \textbf{65.54}          \\
systematic           & 36.21             & 35.48             & 36.25             & \textbf{37.51}         \\
\bottomrule
\end{tabular}
\end{table}

\section{Additional discussion on the related work}

\subsection{Discussion on the memory bank mechanism}

Intuitively, the idea of using the concept-feature dictionary to the ViT training could be similar to the memory bank mechanism in MoCo \citep{moco}, where the features are stored in a queue for replaying later. However, the difference is also clear: we have multiple queues that are indexed by concept codes while MoCo only has a single queue. Similar use of memory bank can also be found in~\cite{wu2018unsupervised,tian2020contrastive} but they follow MoCo, and therefore it is used for providing negative samples when computing the self-supervised contrastive learning loss. Rather, our concept-feature dictionary is designed to make better use of the concept supervision via our concept-guided global and local losses to improve the performance on visual relational reasoning.

\subsection{Comparison to MAC~\citep{mac}}

Here we highlight the difference between our concept-feature dictionary and the knowledge base in MAC~\citep{mac}: The knowledge base in MAC is used during a single VQA reasoning pass (i.e. it will be cleared \& initialized with the new image features (visual knowledge) whenever a new \texttt{<image, question>} pair comes), and thus is used \textbf{in both the training and testing time} for the VQA reasoning. However, the concept-feature dictionary in RelViT is used to store \& retrieve features according to the concept of the current input image and help compute our local and global losses that encourage learning better representations. Therefore, we use it \textbf{in the training time only} as these two losses wont be computed \& optimized during testing.

 \end{document}
