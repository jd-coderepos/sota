\begin{table}[tb]
\centering
  \begin{adjustbox}{width=\linewidth}
    \begin{tabular}{|l|c|c|}
    \hline
    Model & UAS & LAS\\
    \hline
     SynTr & 75.62 & 70.04 \\
     SynTr+RNGTr (T=1) & 76.37 & 70.67 \\
     SynTr+RNGTr (T=3) w/o \textit{stop} & 76.33 & 70.61  \\
     SynTr+RNGTr (T=3) & 76.29 & 70.84  \\ 
    \hline
     UDify~\cite{Kondratyuk_2019} & 72.78 & 65.48 \\
     UDify+RNGTr (T=1) & 74.13 & 68.60 \\
     UDify+RNGTr (T=3) w/o \textit{stop} & 75.68 & 70.32 \\
     UDify+RNGTr (T=3) & 75.91 & 70.66 \\
    \hline
    
    \end{tabular}
  \end{adjustbox}
\caption{\label{result-turkish} Dependency parsing scores for different variations of the RNG Transformer model on the development set of UD Turkish Treebank (IMST).}
\end{table}

\begin{table*}[bt]
  \centering
  \begin{adjustbox}{width=\linewidth}
  \begin{tabular}{|c|c|c|c|cl|cl|c|}
    \hline
    \multirow{2}{*}{Language} & Train &  Mono & Multi & Multi & Multi+Mono & Mono & Mono & Mono \\
    &  Size & [1] & UDPipe & UDify & UDify+RNGTr & SynTr & SynTr+RNGTr & Empty+RNGTr \\
    \hline
    Arabic & 6.1K & 81.8 & 82.94 & 82.88 & \textbf{85.93} (+17.81\%) & \textbf{86.23} & \textbf{86.31} (+0.58\%) & \textbf{86.05} \\
    Basque & 5.4K & 79.8 & 82.86 & 80.97 & 87.55 (+34.57\%) & 87.49 & \textbf{88.2} (+5.68\%) & \textbf{87.96} \\
    Chinese & 4K & 83.4 & 80.5 & 83.75 & 89.05 (+32.62\%) & 89.53 & \textbf{90.48} (+9.08\%)  & 89.82 \\
    English & 12.5K & 87.6 & 86.97 & 88.5 & 91.23 (+23.74\%) & \textbf{91.41} & \textbf{91.52} (+1.28\%) & 91.23 \\
    Finnish & 12.2K & 83.9 & 87.46 & 82.03 & \textbf{91.87} (+54.76\%) & \textbf{91.80} & \textbf{91.92} (+1.46\%) & \textbf{91.78} \\
    Hebrew & 5.2K & 85.9 & 86.86 & 88.11 & 90.80 (+22.62\%) & \textbf{91.07} & \textbf{91.32} (+2.79\%) & 90.56 \\
    Hindi & 13.3K & 90.8 & 91.83 & 91.46 & 93.94 (+29.04\%) & 93.95 & \textbf{94.21} (+4.3\%) & 93.97 \\
    Italian &13.1K & 91.7 & 91.54 & 93.69 & 94.65 (+15.21\%) & \textbf{95.08} & \textbf{95.16} (+1.62\%) & \textbf{94.96} \\
    Japanese & 7.1K & 92.1 & 93.73 & 92.08 & \textbf{95.41} (+42.06\%) & \textbf{95.66} & \textbf{95.71} (+1.16\%) & \textbf{95.56} \\
    Korean & 4.4K & 84.2 & 84.24 & 74.26 & \textbf{89.12} (+57.73\%) & \textbf{89.29} & \textbf{89.45} (+1.5\%) & \textbf{89.1} \\
    Russian & 48.8K & 91.0 & 92.32 & 93.13 & \textbf{94.51} (+20.09\%) & \textbf{94.60} & \textbf{94.47} (-2.4\%) & 94.31 \\
    Swedish & 4.3K & 86.9 & 86.61 & 89.03 & 92.02 (+27.26\%) & 92.03 & \textbf{92.46} (+5.4\%) & \textbf{92.40} \\
    Turkish & 3.7K & 64.9 & 67.56 & 67.44 & 72.07 (+14.22\%) & \textbf{72.52} & \textbf{73.08} (+2.04\%) & 71.99 \\
    \hline
    Average & - & 84.9 & 85.81 & 85.18 & 89.86 & 90.05 & 90.33 & 89.98 \\
    \hline
  \end{tabular}
  \end{adjustbox}
  \caption{\label{ud-las} Labelled attachment scores on UD Treebanks for monolingual ([1]~\cite{Kulmizev_2019} and SynTr) and multilingual (UDPipe~\cite{straka-2018-udpipe} and UDify~\cite{Kondratyuk_2019}) baselines, and the refined models (+RNGTr) pre-trained with BERT~\cite{devlin2018bert}.
    The relative error reduction from RNGTr refinement is shown in parentheses.
    Bold scores are not significantly different from the best score in that row (with $\alpha=0.01$).
}
\end{table*}





\begin{table*}[tb]
  \centering
  \begin{adjustbox}{width=\textwidth}
  \begin{tabular}{|c|c|cc|cc|cc|}
    \hline
    \multirow{2}{*}{Model} &
&
      \multicolumn{2}{c|}{English\,(PTB)} &
      \multicolumn{2}{c|}{Chinese\,(CTB)} & 
      \multicolumn{2}{c|}{German\,(CoNLL)} \\
    & Type & UAS & LAS & UAS & LAS & UAS & LAS \\
    \hline
    \newcite{chen-manning-2014-fast} & T & 91.8 & 89.6 & 83.9 & 82.4 & - & - \\
    \newcite{dyer-etal-2015-transition} & T & 93.1 & 90.9 & 87.2 & 85.7 & - & - \\
    \newcite{ballesteros-etal-2016-training} & T & 93.56 & 91.42 & 87.65 & 86.21 & 88.83 & 86.10 \\
    \newcite{cross-huang-2016-incremental} & T & 93.42 & 91.36 & 86.35 & 85.71 & - & - \\
    \newcite{weiss-etal-2015-structured} & T & 94.26 & 92.41 & - & - & - & - \\
    \newcite{andor-etal-2016-globally} & T & 94.61 & 92.79 & - & - & 90.91 & 89.15 \\
    \newcite{mohammadshahi2019graphtograph} & T & 96.11 & 94.33 & - & - & - & - \\
    \newcite{ma-etal-2018-stack} & T & 95.87 & 94.19 & 90.59 & 89.29 & 93.65 & 92.11 \\
    \newcite{FerGomNAACL2019} & T & 96.04 & 94.43 & - & - & - & - \\
    \hline
    \newcite{kiperwasser-goldberg-2016-simple} & G & 93.1 & 91.0 & 86.6 & 85.1 & - & - \\
    \newcite{wang-chang-2016-graph} & G & 94.08 & 91.82 & 87.55 & 86.23 & - & - \\
    \newcite{cheng-etal-2016-bi} & G & 94.10 & 91.49 & 88.1 & 85.7 & - & - \\
    \newcite{kuncoro-etal-2016-distilling} & G & 94.26 & 92.06 & 88.87 & 87.30 & 91.60 & 89.24 \\
    \newcite{ma2017neural} & G & 94.88 & 92.98 & 89.05 & 87.74 & 92.58 & 90.54 \\
    \newcite{ji-etal-2019-graph} & G & 95.97 & 94.31 & - & -& - & - \\
    \hline
    \newcite{li2019global}+ELMo & G & 96.37 & 94.57 & 90.51 & 89.45 & - & - \\
    \newcite{li2019global}+BERT & G & 96.44 & 94.63 & 90.89 & 89.73 & - & - \\
    \hline
    Biaffine~\cite{dozat2016deep} & G & 95.74 & 94.08 & 89.30 & 88.23 & 93.46 & 91.44 \\
    Biaffine+RNGTr & G & 96.44 & 94.71 & 91.85 & 90.12 & 94.68 & 93.30 \\
    \hline
    SynTr & G & \textbf{96.60} & \textbf{94.94} & 92.42 & 90.67 & \textbf{95.11} & \textbf{93.98} \\
    SynTr+RNGTr & G & \textbf{96.66} & \textbf{95.01} & \textbf{92.98} & \textbf{91.18} & \textbf{95.28} & \textbf{94.02} \\
    \hline
  \end{tabular}
  \end{adjustbox}
  \caption{\label{penn-results} Comparison of our models to previous SOTA models on English (PTB) and Chinese (CTB5.1) Penn Treebanks, and German CoNLL 2009 shared task treebank. "T" and "G" specify "Transition-based" and "Graph-based" models.
    Bold scores are not significantly different from the best score in that column (with $\alpha=0.01$).
}
\end{table*}

\section{Results and Discussion}

After some initial experiments to determine the maximum number of refinement iterations, we report the performance of the RNG Transformer model on the UD treebanks, Penn treebanks, and German CoNLL 2009 treebank.\footnote{The number of parameters and run times of each model on the UD and Penn Treebanks are provided in Appendix~\ref{apx:runtime}.}
The RNGTr models perform substantially better than previously proposed models on every dataset, and RNGTr refinement improves over its initial parser for almost every dataset. 
We also perform various analyses to understand these results better.


\subsection{The Number of Refinement Iterations}
\label{sec:iteration-results}

Before conducting a large number of experiments, we investigate how many iterations of refinement are useful, given the computational costs of additional iterations.  
We evaluate different variations of our RNG Transformer model on the Turkish Treebank (Table~\ref{result-turkish}).\footnote{We choose the Turkish Treebank because it is a low-resource Treebank and there are more errors in the initial parse for RNGTr to correct.} We use both SynTr and UDify as initial parsers. The SynTr model significantly outperforms the UDify model, so the errors are harder to correct by adding the RNGTr model (2.67\% for SynTr versus 15.01\% for UDify of relative error reduction in LAS after integration).  In both cases, three iterations of refinement achieve more improvement than one iteration, but not by a large enough margin to suggest the need for additional iterations.  The further analysis reported in Section~\ref{sec:refine} supports the conclusion that, in general, additional iteration would neither help nor hurt accuracy.  The results in Table~\ref{result-turkish} also show that it is better to include the stopping strategy described in Section~\ref{sec:stopping}.
In subsequent experiments, we use three refinement iterations with the stopping strategy, unless mentioned otherwise.


\subsection{UD Treebank Results}
\label{sec:ud-results}

Results for the UD treebanks are reported in Table~\ref{ud-las}.
We compare our models with previous state-of-the-art results (both trained mono-lingually and multi-lingually), based on labelled attachment score.\footnote{Unlabelled attachment scores are provided in Appendix~\ref{apx:uas-ud}.  All results are computed with the official CoNLL 2018 shared task evaluation script~(\url{https://universaldependencies.org/conll18/evaluation.html}).}

The results with RNGTr refinement demonstrate the effectiveness of the RNGTr model at refining an initial dependency graph.
First, the UDify+RNGTr model achieves significantly better LAS performance than the UDify model in all languages.
Second, although 
the SynTr model significantly outperforms previous state-of-the-art models on all these UD Treebanks,\footnote{ In particular, SynTr significantly outperforms UDify, even though they are very similar models. In addition to the model differences discussed in Section~\ref{sec:initial},
there are some differences in the way UDify and SynTr models are trained that might explain this improvement, in particular, that UDify is a multi-lingual multi-task model, whereas
SynTr is a mono-lingual single-task model.
}
the SynTr+RNGTr model achieves further significant improvement over SynTr in four languages, and no significant degradation in any language.  Of the nine languages where there is no significant difference between SynTr and SynTr+RNGTr for the given test sets, RNGTr refinement results in higher LAS in eight languages and lower LAS in only one (Russian).

The improvement of SynTr+RNGTr over SynTr is particularly interesting because it is a controlled demonstration of the effectiveness of the graph refinement method of RNGTr.  The only difference between the SynTr model and the final iteration of the SynTr+RNGTr model is the graph inputs from the previous iteration (Equations~\eqref{eq:syntr} versus \eqref{eq:rng-main}).
By conditioning on the full dependency graph, the SynTr+RNGTr model's final RNGTr iteration can capture any kind of correlation in the dependency graph, including both global and between-edge correlations both locally and over long distances.
This result also further demonstrates the generality and effectiveness of the G2GTr architecture for conditioning on graphs (Equations~\eqref{eq:g2g-attn1} and~\eqref{eq:g2g-attn3}).

As expected, we get more improvement when combining the RNGTr model with UDify, because UDify's initial dependency graph contains more incorrect dependency relations for RNGTr to correct.
But after refinement, there is surprisingly little difference between the performance of the UDify+RNGTr and SynTr+RNGTr models, suggesting that RNGTr is powerful enough to correct any initial parse.  
To investigate the power of the RNGTr architecture to correct any initial parse, we also show results for a model with an empty initial parse, Empty+RNGTr.  For this model, we run four iterations of refinement (T=4), so that the amount of computation is the same as for SynTr+RNGTr.
The Empty+RNGTr model achieves competitive results with the UDify+RNGTr model (i.e.\ above the previous state-of-the-art), and close to the results for SynTr+RNGTr.  This accuracy is achieved despite the fact that the Empty+RNGTr model has half as many parameters as the UDify+RNGtr model and the SynTr+RNGTr model since it has no separate initial parser.
These Empty+RNGTr results indicate that RNGTr architecture is a very powerful method for graph refinement.

\subsection{Penn Treebank and German corpus Results}

UAS and LAS results for the Penn Treebanks and German CoNLL 2009 Treebank are reported in Table~\ref{penn-results}.
We compare to the results of previous state-of-the-art models and SynTr, and we use the RNGTr model to refine both the Biaffine parser~\cite{dozat2016deep} and SynTr, on all Treebanks.\footnote{Results are calculated with the official evaluation script: (\url{https://depparse.uvt.nl/}). For German, we use \url{https://ufal.mff.cuni.cz/conll2009-st/eval-data.html}.}

\begin{figure*}[!ht]
  \centering
  \hspace{-2ex}
  \includegraphics[width=0.33\linewidth]{deplen.png}
  \includegraphics[width=0.33\linewidth]{root.png}
  \includegraphics[width=0.33\linewidth]{senlen.png}
  \hspace{-2ex}
  \caption{Error analysis, on the concatenation of UD Treebanks, of initial parsers (UDify and SynTr), their integration with the RNGTr model, and the Empty+RNGTr model.
    \vspace{-1ex}
  }
  \label{fig:errors-ud}
\end{figure*}

Again, the SynTr model significantly outperforms previous state-of-the-art models, with a 5.78\%, 9.15\%, and 23.7\% LAS relative error reduction in English, Chinese, and German, respectively.  Despite this level of accuracy, adding RNGTr refinement improves accuracy further under both UAS and LAS.  For the Chinese Treebank, this improvement is significant, with a 5.46\% LAS relative error reduction.
When RNGTr refinement is applied to the output of the Biaffine parser~\cite{dozat2016deep}, it achieves a LAS relative error reduction of 10.64\% for the English Treebank, 16.05\% for the Chinese Treebank, and 27.72\% for the German Treebank.
These improvements, even over such strong initial parsers, again demonstrate the effectiveness of the RNGTr architecture for graph refinement.

\subsection{Error Analysis}
\label{sec:error-an}

To better understand the distribution of errors for our models, we follow \newcite{mcdonald-nivre-2011-analyzing} and plot labelled attachment scores as a function of dependency length, sentence length and distance to root.\footnote{We use the MaltEval tool~\cite{nilsson-nivre-2008-malteval} for calculating accuracies in all cases.}
We compare the distributions of errors made by the UDify~\cite{Kondratyuk_2019}, SynTr, and refined models (UDify+RNGTr, SynTr+RNGTr, and Empty+RNGTr).  Figure~\ref{fig:errors-ud} shows the accuracies of the different models on the concatenation of all development sets of UD Treebanks.
Results show that applying RNGTr refinement to the UDify model results in a substantial improvement in accuracy across the full range of values in all cases,  
and little difference in the error profile between the better performing models.
In all the plots, the gains from RNGTr refinement are more pronounced for the more difficult cases, where a larger or more global view of the structure is beneficial.

As shown in the leftmost plot of Figure~\ref{fig:errors-ud}, adding RNGTr refinement to UDify results in particular gains for the longer dependencies, which are more likely to interact with other dependencies.
The middle plot illustrates the accuracy of models as a function of the distance to the root of the dependency tree, which is calculated as the number of dependency relations from the dependent to the root.
When we add RNGTr refinement to the UDify parser, we get particular gains for the problematic middle depths, which are neither the root nor leaves.  Here, SynTr+RNGTr is also particularly strong on these high nodes, whereas SynTr is particularly strong on low nodes.
In the plot by sentence length, the larger improvements from adding RNGTr refinement (both to UDify and SynTr) are for the shorter sentences, which are surprisingly difficult for UDify.  Presumably, these shorter sentences tend to be more idiosyncratic, which is better handled with a global view of the structure. (See Figure~\ref{fig:refineexample} for an example.)
In all these cases, the ability of RNGTr to capture any kind of correlation in the dependency graph gives the model a larger and more global view of the correct output structure.

\begin{figure*}[!ht]
  \centering
  \hspace{-2ex}
  \includegraphics[width=0.4\linewidth]{dep_syntr.png}
  \includegraphics[width=0.4\linewidth]{root_syntr.png}
  \hspace{-2ex}
  \caption{Error analysis of SynTr and SynTr+RNGTr models on Chinese CTB Treebank.
    \vspace{-1ex}
    \label{fig:error-ctb}
  }
\end{figure*}

To further analyse where RNGTr refinement is resulting in improvements,
we compare the error profiles of the  SynTr and SynTr+RNGTr models on the Chinese Penn Treebank, where adding RNGTr refinement to SynTr results in significant improvement (see Table~\ref{penn-results}).
As shown in Figure~\ref{fig:error-ctb}, RNGTr refinement results in particular improvement on longer dependencies (left plot), and on middle and greater depth nodes (right plot), again showing that RNGTr does particularly well on the difficult cases with more interactions with other dependencies.


\subsection{Refinement Analysis}
\label{sec:refine}

\begin{figure}
\centering
  \includegraphics[width=0.55\linewidth]{refineexample.pdf}
  \caption{The shortest example corrected by UDify+RNGTr in the English UD Treebank.}
  \label{fig:refineexample}
\end{figure}

To better understand how the RNG Transformer model is doing refinement, we perform several analyses of the trained UDify+RNGTr model.\footnote{We choose UDify as the initial parser because the RNGTr model makes more changes to the parses of UDify than SynTr, so we can more easily analyse these changes. Results with SynTr as the initial parser are provided in Appendix~\ref{apx:syntr-refine}.}
An example of this refinement is shown in Figure~\ref{fig:refineexample}, where the UDify model predicts an incorrect dependency graph, but the RNGTr model modifies it to build the gold dependency tree.

\paragraph{Refinements by Iteration:}
To measure the accuracy gained from refinement at different iterations, we define the following metric:
\begin{equation}
{\tt REL}^t = {\tt RER}({\tt LAS}^{t-1},{\tt LAS}^t)
\label{eq:refine}
\end{equation}
where ${\tt RER}$ is relative error reduction, and $t$ is the refinement iteration. ${\tt LAS}^0$ is the accuracy of the initial parser, UDify in this case.

\begin{table}
\centering
  \begin{adjustbox}{width=\linewidth}
  
  \begin{tabular}{|c|c|c|c|}
    \hline
    Dataset Type & $t=1$ & $t=2$ & $t=3$ \\
    \hline
     Low-Resource & +13.62\% & +17.74\%  & +0.16\% \\
     High-Resource & +29.38\% & +0.81\% & +0.41\% \\
     




\hline
    
    \end{tabular}
  \end{adjustbox}
\caption{Refinement Analysis (LAS relative error reduction) of the UDify+RNGTr model for different refinement steps on the development sets of UD Treebanks.}
\label{table:refine}
\end{table}

To illustrate the refinement procedure for different dataset types, we split UD Treebanks based on their training set size into "Low-Resource" and "High-Resource" datasets.\footnote{We consider languages that have training data more than 10k sentences as "High-Resource".}
Table~\ref{table:refine} shows the refinement metric (${\tt REL}^t$) after each refinement iteration of the UDify+RNGTr model on these sets of UD Treebanks.\footnote{For these results we apply MST decoding after every iteration, to allow proper evaluation of the intermediate graphs.}  Every refinement step achieves an increase in accuracy, on both low and high resource languages.  But the amount of improvement generally decreases for higher refinement iterations.  Interestingly, for languages with less training data, the model cannot learn to make all corrections in a single step but can learn to make the remaining corrections in a second step, resulting in approximately the same total percentage of errors corrected as for high resource languages.
In general, different numbers of iterations may be necessary for different datasets, allowing efficiency gains by not performing unnecessary refinement iterations.


\begin{table}
\centering
  \begin{adjustbox}{width=\linewidth}
    \begin{tabular}{|l|c|c|c|}
    \hline
    Dependency Type & $t=1$ & $t=2$ & $t=3$ \\
    \hline
{\tt goeswith } & +57.83\% & +0.00\%  & +2.61\% \\ [-0.5ex]
{\tt aux } & +66.04\% & +3.04\%  & +3.12\%  \\ [-0.5ex]
{\tt cop } & +48.17\% & +2.21\%  & +3.01\%  \\ [-0.5ex]
{\tt mark } & +44.97\% & +2.44\%  & +0.00\%  \\ [-0.5ex]
{\tt amod } & +45.58\% & +2.33\%  & +0.00\% \\ [-0.5ex] 
{\tt det } & +34.48\% & +0.00\%  & +2.63\%  \\ [-0.5ex]
{\tt acl } & +33.01\% & +0.89\%  & +0.00\%  \\ [-0.5ex]
{\tt xcomp } & +33.33\% & +0.80\%  & +0.00\%  \\ [-0.5ex]
{\tt nummod } & +28.50\% & +0.00\%  & +1.43\% \\ [-0.5ex]
{\tt advcl } & +29.53\% & +1.26\%  & +0.25\%  \\ [-0.5ex]
{\tt dep } & +22.48\% & +2.02\%  & +0.37\%  \\ 
    \hline
    \end{tabular}
  \end{adjustbox}
\caption{Relative F-score error reduction of a selection of dependency types for each refinement step on the concatenation of UD Treebanks (with UDify as the initial parser).
\label{table:acc-deptype}}
\end{table}


\paragraph{Dependency Type Refinement:}
Table~\ref{table:acc-deptype} shows the relative improvement of different dependency types for the UDify+RNGTr model at each refinement step, ranked and selected by the total relative error reduction.
A huge amount of improvements is achieved for all these dependency types at the first iteration step, and then we have a considerable further improvement for many of the remaining refinement steps.
The later refinement steps are particularly useful for idiosyncratic dependencies which require a more global view of the sentence, such as auxiliary (aux) and copula (cop).
A similar pattern of improvements is found when SynTr is used as the initial parser, reported in Appendix~\ref{apx:syntr-refine}.


\begin{table}
\centering
  \begin{adjustbox}{width=\linewidth}
    \begin{tabular}{|l|c|c|c|}
    \hline
    Tree Type & $t=1$ & $t=2$ & $t=3$ \\
    \hline
    Non-Projective & +22.43\% & +3.92\% & +0.77\% \\
    Projective & +29.6\% & +1.13\% & +0.0\% \\
    \hline
    \end{tabular}
  \end{adjustbox}
\caption{Relative F-score error reduction of projective and non-projective trees on the concatenation of UD Treebanks (with UDify as the initial parser).
}
\label{table:pr-non-proj}
\end{table}

\paragraph{Refinement by Projectivity:}
Table~\ref{table:pr-non-proj} shows the relative improvement of each refinement step for projective and non-projective trees.  Although the total gain is slightly higher for projective trees, non-projective trees require more iterations to achieve the best results.  Presumably, this is because non-projective trees have more complex non-local interactions between dependencies, which requires more refinement iterations to fix incorrect dependencies.  This seems to contradict the common belief that non-projective parsing is better done with factorised graph-based models, which do not model these interactions.
