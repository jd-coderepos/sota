


\documentclass[preprint]{sigplanconf}
\usepackage[varg]{txfonts}
\let\iint=\relax
\let\iiint=\relax
\let\iiiint=\relax
\let\idotsint=\relax



\usepackage{amssymb,amsfonts,amsmath}

\usepackage[mathscr]{eucal}

\usepackage{stmaryrd}

\usepackage{amsbsy}

\usepackage{bm}

\usepackage{braket}




\newcommand{\Tra}{{\sf T}} 
\newcommand{\parens}[1]{(#1)}
\newcommand{\Parens}[1]{\left(#1\right)}
\newcommand{\dsquare}[1]{\llbracket #1 \rrbracket}
\newcommand{\Dsquare}[1]{\left\llbracket #1 \right\rrbracket}
\newcommand{\curly}[1]{\{ #1 \}}
\newcommand{\Curly}[1]{\left\{ #1 \right\}}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\qtext}[1]{\quad\text{#1}\quad}

\newcommand{\V}[2][]{{\bm{#1\mathbf{\MakeLowercase{#2}}}}} 
\newcommand{\VE}[3][]{#1{\MakeLowercase{#2}}_{#3}} 
\newcommand{\Vn}[3][]{{\bm{#1\mathbf{\MakeLowercase{#2}}}}^{(#3)}} 
\newcommand{\VnTra}[3][]{{\bm{#1\mathbf{\MakeLowercase{#2}}}}^{(#3)\Tra}} 
\newcommand{\VnE}[4][]{#1{\MakeLowercase{#2}}^{(#3)}_{#4}} 

\newcommand{\M}[2][]{{\bm{#1\mathbf{\MakeUppercase{#2}}}}} 
\newcommand{\Mn}[3][]{{\bm{#1\mathbf{\MakeUppercase{#2}}}}^{(#3)}} 
\newcommand{\MnTra}[4][]{{\bm{#1\mathbf{\MakeUppercase{#2}}}}^{(#3)\Tra}} 
\newcommand{\MC}[3][]{\V[#1]{#2}_{#3}} 
\newcommand{\MnC}[4][]{\Vn[#1]{#2}{#3}_{#4}} 
\newcommand{\MnCTra}[4][]{\VnTra[#1]{#2}{#3}_{#4}} 
\newcommand{\ME}[3][]{#1{\MakeLowercase{#2}}_{#3}} 
\newcommand{\MnE}[4][]{#1{\MakeLowercase{#2}}^{(#3)}_{#4}} 

\newcommand{\T}[2][]{\boldsymbol{#1\mathscr{\MakeUppercase{#2}}}} 
\newcommand{\TS}[3][]{\M[#1]{#2}_{#3}}
\newcommand{\TE}[3][]{#1{\MakeLowercase{#2}}_{#3}}
\newcommand{\Mz}[3][]{\M[#1]{#2}_{(#3)}}

\newcommand{\Oprod}{\circ} 
\newcommand{\Kron}{\otimes} 
\newcommand{\Khat}{\odot} 
\newcommand{\Hada}{\ast} 
\newcommand{\BigHada}{\mathop{\mbox{\fontsize{18}{19}\selectfont }}} 
\newcommand{\Divi}{\varoslash}


 




\usepackage{amsmath}
\usepackage[]{algorithm2e, setspace}
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{color}
\usepackage[colorlinks]{hyperref}
\usepackage{url}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{observation}[theorem]{Observation}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}




\begin{document}

\newcommand{\TODO}[1]{\textcolor{blue}{\textbf{TODO XXXXX: #1}}}
\newcommand{\GB}[1]{\textcolor{red}{\textbf{GB: #1}}}
\newcommand{\vecmat}[1]{\text{vec}\left(#1\right)}
\newcommand{\nnz}[1]{\text{nnz}\left(#1\right)}
\newcommand{\diag}[1]{\text{diag}\left(#1\right)}
\newcommand{\bc}[3]{\left\langle#1, #2, #3\right\rangle}
\newcommand{\alg}[3]{\Dsquare{#1, #2, #3}}
\newcommand{\dims}[3]{#1 \times #2 \times #3}
\newcommand{\daxpy}{\texttt{daxpy}}
\newcommand{\dgemm}{\texttt{dgemm}}
\newcommand{\lt}{\left}
\newcommand{\rt}{\right}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{PPoPP '15}{Month d--d, 2015, San Francisco, CA, USA}
\copyrightyear{2015} 
\copyrightdata{978-1-nnnn-nnnn-n/yy/mm} 
\doi{nnnnnnn.nnnnnnn}







\titlebanner{}\preprintfooter{}

\title{A Framework for Practical Parallel Fast Matrix Multiplication}




\authorinfo{Austin R. Benson}
           {Stanford University}
           {arbenson@stanford.edu}
\authorinfo{Grey Ballard}
           {Sandia National Laboratories}
           {gmballa@sandia.gov}


\maketitle

\begin{abstract}
Matrix multiplication is a fundamental computation in many scientific disciplines.
In this paper, we show that novel fast matrix multiplication algorithms can significantly outperform vendor implementations of the classical algorithm and Strassen's fast algorithm on modest problem sizes and shapes.
Furthermore, we show that the best choice of fast algorithm depends not only on the size of the matrices but also the shape.
We develop a code generation tool to automatically implement multiple sequential and shared-memory parallel variants of each fast algorithm, including our novel parallelization scheme.
This allows us to rapidly benchmark over 20 fast algorithms on several problem sizes.
Furthermore, we discuss a number of practical implementation issues for these algorithms on shared-memory machines that can direct further research on making fast algorithms practical.
\end{abstract}

\category{G.4}{Mathematical software}{Efficiency}
\category{G.4}{Mathematical software}{Parallel and vector implementations}



\keywords
fast matrix multiplication, dense linear algebra, parallel linear algebra, shared memory

\section{Introduction}

Matrix multiplication is one of the most fundamental computations in numerical linear algebra and scientific computing.
Consequently, the computation has been extensively studied in parallel computing environments \cite{van1997summa, solomonik2011communication, irony2004communication, ballard2012communication, volkov2008benchmarking}.
In this paper, we show that fast algorithms for matrix-matrix multiplication can achieve higher performance on sequential and shared-memory parallel architectures for modestly sized problems.
By fast algorithms, we mean ones that perform asymptotically fewer floating point operations and communicate asymptotically less data than the classical algorithm.
We also provide a code generation framework to rapidly implement sequential and parallel versions of over 20 fast algorithms.
Our performance results in Section~\ref{sec:performance} show that several fast algorithms can outperform the Intel Math Kernel Library (MKL)  (double precision general matrix-matrix multiplication) routine and Strassen's algorithm~\cite{strassen1969gaussian}.
In parallel implementations, fast algorithms can achieve a speedup of 5\% over Strassen's algorithm and greater than 15\% over MKL.

However, fast algorithms for matrix multiplication have largely been ignored in practice.
For example, numerical libraries such as Intel's MKL, AMD's Core Math Library (ACML),
and the Cray Scientific Libraries package (LibSci) do not provide implementations of fast algorithms.
Why is this the case?
First, users of numerical libraries typically consider fast algorithms to be of only theoretical interest and never practical for reasonable problem sizes.
We argue that this is \emph{not} the case with our performance results in Section~\ref{sec:performance}.
Second, fast algorithms do not provide the same numerical stability guarantees as the classical algorithm.
In practice, there is some loss in precision in the fast algorithms, but they are not nearly as bad as the worst-case guarantees \cite{higham2002accuracy,lipshitz2012communication}.
Third, the LINPACK benchmark used to rank supercomputers by performance forbids fast algorithms \cite{TOP500}.
We suspect that this has driven effort away from the study of fast algorithms.

Strassen's algorithm is the most well known fast algorithm, but this paper explores a much larger class of recursive fast algorithms based on different base case dimensions.
We review these algorithms and methods for constructing them in Section~\ref{sec:fast}.
The structure of these algorithms makes them amenable to code generation, and we describe this process and other performance tuning considerations in Section~\ref{sec:implementation}.
In Section~\ref{sec:parallel}, we describe three different methods for parallelizing fast matrix multiplication algorithms on shared-memory machines.
Our code generator implements all three parallel methods for each fast algorithm.
We evaluate the sequential and parallel performance characteristics of the various algorithms and implementations in Section~\ref{sec:performance} and compare them with MKL's implementation of the classical algorithm as well as an existing implementation of Strassen's algorithm.

The goal of this paper is to help bridge the gap between theory and practice of fast matrix multiplication algorithms.
By introducing our tool of automatically translating a fast matrix multiplication algorithm to high performance sequential and parallel implementations, we enable the rapid prototyping and testing of theoretical developments in the search for faster algorithms.
We focus the attention of theoretical researchers on what algorithmic characteristics matter most in practice, and we demonstrate to practical researchers the utility of several existing fast algorithms besides Strassen's, motivating further effort towards high performance implementations of those that are most promising.

Our contributions are summarized as follows:
\begin{itemize}
\item 
By using new fast matrix multiplication algorithms, we achieve better performance than Intel MKL's , both sequentially and with 6 and 24 cores on a shared-memory machine.

\item 
We demonstrate that, in order to achieve the best performance for matrix multiplication, the choice of fast algorithm depends on the size and shape of the matrices.
Our new fast algorithms outperform Strassen's algorithm on the multiplication of rectangular matrices.

\item
We show how to use code generation techniques to rapidly implement fast matrix multiplication algorithms.

\item
We provide a new hybrid parallel algorithm for shared-memory fast matrix multiplication.

\item
We implement a fast matrix multiplication algorithm with asymptotic complexity  for square  matrices.
In terms of asymptotic complexity, this is the fastest matrix multiplication algorithm implementation to date.
However, our performance results show that this algorithm is not practical for the problem sizes that we consider.



\end{itemize}

Overall, we find that Strassen's algorithm is hard to beat for square matrix multiplication, both in serial and in parallel.
However, for rectangular matrices (which occur more frequently in practice), other fast algorithms can perform much better.
The structure of the fast algorithms that perform well tend to ``match the shape" of the matrices, an idea that we will make clear in Section~\ref{sec:performance}.
We also find that bandwidth is a factor towards scalability in shared-memory parallel implementations of fast algorithms.
Finally, we find that algorithms that are theoretically fast in terms of asymptotic complexity do not perform well on problems of modest size that we consider on shared-memory parallel architectures.

All of the software used for this paper will be made publicly available.

\subsection{Related work}
\label{sec:related}

Strassen's fast matrix multiplication algorithm has been implemented for both shared-memory~\cite{kumar1995tensor, d2011exploiting} and distributed-memory architectures~\cite{grayson1996high, luo1995scalable, ballard2012communication}.
For our parallel algorithms in Section~\ref{sec:parallel}, we use the ideas of breadth-first and depth-first traversals of the recursion trees, which were first considered by Kumar \emph{et al}.~\cite{kumar1995tensor} and Ballard \emph{et al}.~\cite{ballard2012communication} for minimizing memory footprint and communication.

Apart from Strassen's algorithm, a number of fast matrix multiplication algorithms have been developed,
but only a small handful have been implemented.
Furthermore, these implementations have only been sequential.
Hopcroft and Kerr showed how to construct recursive fast algorithms where the base case is multiplying a  by a  matrix~\cite{hopcroft1971minimizing}.
Bini \emph{et al}. introduced the concept of arbitrary precision approximate (APA) algorithms for matrix multiplication and demonstrated a method for multiplying  by  matrices which leads to a general square matrix multiplication APA algorithm that is asymptotically faster than Strassen's \cite{BCRL79}.
Sch\"{o}nhage also developed an APA algorithm that is asymptotically faster than Strassen's, based on multiplying  by  matrices \cite{Schonhage81}.
These APA algorithms suffer from severe numerical issues---both lose at least half the digits of accuracy with each recursive step.
While no exact solution can have the same complexity as Bini's algorithm \cite{hopcroft1971minimizing},
it is still an open question if there exists a fast algorithm with the same complexity as Sch\"{o}nhage's.
Pan used factorization of trilinear forms and a base case of  matrix multiplication to construct an exact algorithm asymptotically faster than Strassen's algorithm~\cite{pan1978strassen}.
This algorithm was implemented by Kaporin~\cite{kaporin2004aggregation}, and the running time was competitive with Strassen's algorithm in practice.
Recently, Smirnov presented optimization tools for finding many fast algorithms based on factoring bilinear forms~\cite{smirnov2013bilinear},
and we will use these tools for finding our own algorithms in Section~\ref{sec:fast}.
Other automated approaches have also been used to discover fast algorithms, but these have focused on multiplying  by  matrices \cite{JM86,CBH11,OKM13}.

There are several lines of theoretical research~\cite{coppersmith1990matrix, stothers2010complexity, williams2012multiplying} that prove existence of fast APA algorithms with much better asymptotic complexity than the algorithms considered in this paper.
Unfortunately, there is still a large gap between the substantial theoretical work and what we can practically implement.

Renewed interest in the practicality of Strassen's and other fast algorithms is motivated by the observation that not only is the arithmetic cost reduced when compared to the classical algorithm, the communication costs also improve asymptotically \cite{ballard2012graph}.
That is, as the relative cost of moving data throughout the memory hierarchy and between processors increases, we can expect the benefits of fast algorithms to grow accordingly.
We note that communication lower bounds (\cite[Theorem 1.4]{ballard2012graph} and \cite[Theorem 1]{BDHLS12-RECT}) apply to all the algorithms presented in this paper, and in nearly all cases they are attained by the implementations used in this paper.

\subsection{Notation and tensor preliminaries}

The relevant notation for our work is in Table~\ref{tab:notation}.
Throughout, scalars are represented by lowercase Roman or Greek letters (), vectors by lowercase boldface (),
matrices by uppercase boldface (), and tensors by boldface Euler script letters ().
For a matrix , we use  and  to denote the th column and  entry, respectively.
We briefly review basic tensor preliminaries, following the notation of Kolda and Bader \cite{kolda2009tensor}.
A tensor is a multi-dimensional array, and in this paper we deal exclusively with order-3, real-valued tensors; \emph{i.e.}, .
The th \emph{frontal slice} of  is .
For , , , we define the outer product tensor
 with entries .
Addition of tensors is defined entry-wise.
The \emph{rank} of a tensor  is the minimum number of rank-one tensors that generate  as their sum.
Decompositions of the form  lead to fast matrix multiplication algorithms (Section~\ref{sec:tensor_fact}), and we use  to denote the decomposition, where , , and  are matrices with  columns given by , , and .
Of the various flavors of products involving tensors, we will need to know that, for  and ,
, with , or .

\begin{table}[tb]
\centering
\caption{
Summary of notation.
}
\begin{tabular}{l l}
\toprule \\
 & ``base case'' computation, multiplying an   \\
& matrix by a  matrix \\
 & dimensions of actual matrices that are multiplied \\
& ( matrix multiplied by  matrix) \\
 & factor matrices corresponding to a tensor \\
& decomposition that provides a fast algorithm \\
 & order three tensor \\
 & rank-1 tensor with entries  \\
 & th frontal slice of , the matrix of entries  \\
,  & th column and  entry of matrix  \\
 & row-order vectorization of the entries of  \\
 & number of non-zero entries of an object \\
\bottomrule
\end{tabular}
\label{tab:notation}
\end{table}

\section{Fast matrix multiplication}
\label{sec:fast}

We now review the preliminaries for fast matrix multiplication algorithms.
In particular, we focus on factoring tensor representations of bilinear forms,
which will facilitate the discussion of the implementation in Sections~\ref{sec:implementation}~and~\ref{sec:parallel}.

\subsection{Recursive multiplication}
\label{sec:recursive}

Matrices are self-similar, \emph{i.e.}, a submatrix is also a matrix.
Arithmetic with matrices is closely related to arithmetic with scalars, and
we can build recursive matrix multiplication algorithms by manipulating submatrix blocks.
For example, consider multiplying ,

where we have partitioned the matrices into four submatrices.
Throughout this paper, we denote the block multiplication of  and  matrices by .
Thus, the above computation is .
Multiplication with the classical algorithm proceeds by combining a set of eight matrix multiplications with four matrix additions:

\vspace{-0.7cm} 
The multiplication to form each  is recursive and the base case is scalar multiplication.
The number of flops performed by the classical algorithm for  matrices, where  is a power of two, is:

This is a standard recurrence relation with .
We have assumed that the matrices are square and powers of two.
In Section~\ref{sec:all_dimensions}, we explain how to handle all dimensions.

The idea of fast matrix multiplication algorithms is to perform fewer recursive matrix multiplications at the expense of more matrix additions.
Since matrix multiplication is asymptotically more expensive than matrix addition, this tradeoff results in faster algorithms.
The most well known fast algorithm is due to Strassen, and follows the same block structure:



We have explicitly written out terms like  to hint at the generalizations provided in Section~\ref{sec:tensor_fact}.
Strassen's algorithm uses 7 matrix multiplications and 18 matrix additions.
The number of flops performed by the algorithm is:

and .


There are natural extensions to Strassen's algorithm.
We might try to find an algorithm using fewer than 7 multiplications; unfortunately, we cannot~\cite{winograd1971multiplication}.
Alternatively, we could try to reduce the number of additions.
This leads to the Strassen-Winograd algorithm, which reduces the 18 additions down to 15, which is also optimal \cite{Probert76}.
We explore such methods in Section~\ref{sec:cse}.
We can also improve the constant on the leading term by choosing a bigger base case dimension (and using the classical algorithm for the base case).
This turns out not to be important in practice because the base case will be chosen to optimize performance rather than flop count.
Lastly, we can use blocking schemes apart from , which we explain in the remainder of this section.
This leads to a host of new algorithms, and we show in Section~\ref{sec:performance} that they are often faster in practice.

\subsection{Fast algorithms as low-rank tensor decompositions}
\label{sec:tensor_fact}

The approach we use to devise fast algorithms exploits an important connection between matrix multiplication (and other bilinear forms) and tensor computations.
We detail the connection in this section for completeness; see \cite{Brent70,Kruskal77,Knuth81} for earlier explanations.



A bilinear form on a pair of finite-dimensional vector spaces is a function that maps a pair of vectors to a scalar and is linear in each of its inputs separately.
A bilinear form  can be represented by a matrix  of coefficients: , where we note that  and  may have different dimensions.
In order to describe a set of  bilinear forms , , we can use a three-way tensor  of coefficients:

or, in more succinct tensor notation, .

\subsubsection{Low-rank tensor decompositions}

The advantage of representing the operations using a tensor of coefficients is a key connection between the rank of the tensor to the arithmetic complexity of the corresponding operation.
Consider the ``active'' multiplications between elements of the input vectors (\emph{e.g.}, ).
The conventional algorithm, following Equation \eqref{eqn:conventional}, will compute an active multiplication for every nonzero coefficient in .
However, suppose we have a rank- decomposition of the tensor, , so that

for all , , , where , , and  are matrices with  columns each.
We will also use the equivalent notation .
Substituting Equation \eqref{eqn:lowrank} into Equation \eqref{eqn:conventional} and rearranging, we have for ,

which reduces the number of active multiplications (now between linear combinations of elements of the input vectors) to .
Here we highlight active multiplications with  notation,  and  are temporary vectors that store the linear combinations of elements of  and , and  is the temporary vector that stores the element-wise product of  and .  
In matrix-vector notation, we have , , and , where () denotes element-wise vector multiplication.

Assuming , this reduction of active multiplications, at the expense of increasing the number of other operations, is valuable when active multiplications are much more expensive than the other operations.
This is the case for recursive matrix multiplication, where the elements of the input vectors are (sub)matrices, as we describe below.


\subsubsection{Tensor representation of matrix multiplication}

Matrix multiplication is a bilinear operation, so we can represent it as a tensor computation. 
In order to match the notation above, we vectorize the input and output matrices , , and  using row-wise ordering, so that , , and . 

For every triplet of matrix dimensions for valid matrix multiplication, there is a fixed tensor that represents the computation so that  holds for all , , and .
For example, if  and  are both , the corresponding  tensor  has frontal slices

This yields, for example,


By Strassen's algorithm, we know that although this tensor has 8 nonzero entries, its rank is at most 7.
Indeed, that algorithm corresponds to a low-rank decomposition represented by the following triplet of matrices, each with 7 columns:

And, as in the previous section, for example,

Note that in the previous section, the elements of the input matrices are already interpreted as submatrices (\emph{e.g.},  and ); here we represent them as scalars (\emph{e.g.},  and ).



We need not restrict ourselves to the  case; there exists a tensor for matrix multiplication given any set of valid dimensions.
When considering a base case of  by  matrix multiplication (denoted ),
the tensor has dimensions  and  non-zeros.
In particular,  if the following three conditions hold 

and otherwise  (here we assume entries are 1-indexed).

\subsubsection{Approximate tensor decompositions}
The APA algorithms discussed in Section~\ref{sec:related} arise from \emph{approximate} tensor decompositions.
With Bini's algorithm, for example, the factor matrices have entries  and .
As , the low-rank tensor approximation approaches the true tensor.
However, as  gets small, we suffer from loss of precision in the floating point calculations of the resulting fast algorithm.
Setting  minimizes the loss of accuracy in Bini's algorithm, where  is machine precision.







\subsection{Finding fast algorithms}
\label{sec:finding}

We conclude this section with a description of a method for searching for and discovering fast algorithms for matrix multiplication.
Our search goal is to find low-rank decompositions of tensors corresponding to matrix multiplication of a particular set of dimensions, which will identify fast, recursive algorithms with reduced arithmetic complexity. 
That is, given a particular base case  and the associated tensor , we seek a rank  and matrices , , and  that satisfy Equation \eqref{eqn:conventional}.
Table~\ref{tab:algorithms} summarizes the algorithms that we find and use for numerical experiments in Section~\ref{sec:performance}.

The rank of the decomposition determines the number of active multiplications, or recursive calls, and therefore the exponent in the arithmetic cost of the algorithm.
The number of other operations (additions and inactive multiplications) will affect only the constants in the arithmetic cost.
For this reason, we would like to have sparse , , and  matrices with simple values (like ), but that goal is of secondary importance compared to minimizing the rank .
Note that these constant values do affect performance of these algorithms for reasonable matrix dimensions in practice, though mainly because of how they affect the communication costs of the implementations rather than the arithmetic cost.
We discuss this in more detail in Section~\ref{sec:matrix_additions}.

\subsubsection{Equivalent algorithms}

Given an algorithm  for base case , we can transform it to an algorithm for any of the other 5 permutations of the base case dimensions with the same number of multiplications.
This is a well known property \cite{HM73}; here we state the two transformations that generate all permutations in our notation.
We let  be the permutation matrix that swaps row-order for column-order in the vectorization of an  matrix.
In other words, if  is , .


\begin{proposition}
\label{prop:perm1}
Given a fast algorithm  for ,  is a fast algorithm for . 
\end{proposition}


\begin{proposition}
\label{prop:perm2}
Given a fast algorithm  for ,
 is a fast algorithm for .
\end{proposition}




We also point out that fast algorithms for a given base case belong to equivalence classes.
Two algorithm are equivalent if one can be generated from another based on the following transformations \cite{deGroote78a,JM86}.


\begin{proposition}
\label{prop:eqalgs}
If  is a fast algorithm for , then the following are also fast algorithms for :

for any permutation matrix ;

for any diagonal matrices , , and  such that ;

for any nonsingular matrices , , .
\end{proposition}

\subsubsection{Numerical search}

Given a rank  for base case , Equation \eqref{eqn:conventional} defines  polynomial equations of the form given in Equation \eqref{eqn:lowrank}.
Because the polynomials are trilinear, alternating least squares (ALS) can be used to iteratively compute an approximate (numerical) solution to the equations.
That is, if two of the three factor matrices are fixed, the optimal 3rd factor matrix is the solution to a linear least squares problem.
Thus, each outer iteration of ALS involves alternating among solving for , , and , each of which can be done efficiently with the QR decomposition.
This approach was first proposed for fast matrix multiplication search by Brent \cite{Brent70}, but ALS has been a popular method for general low-rank tensor approximation for as many years (see \cite{kolda2009tensor} and references therein).

The main difficulties ALS faces for this problem include getting stuck at local minima, encountering ill-conditioned linear least-squares problems, and, even if ALS converges to machine-precision accuracy, computing dense , , and  matrices with floating point entries.
We follow the work of Johnson and McLoughlin \cite{JM86} and Smirnov \cite{smirnov2013bilinear} in addressing these problems.
We use multiple starting points to handle the problem of local minima, add regularization to help with the ill-conditioning, and encourage sparsity in order to recover exact factorizations (with integral or rational values) from the approximations.

The most useful techniques in our search have been (1) exploiting the transformations given in Proposition~\ref{prop:eqalgs} to encourage sparsity and obtain discrete values and (2) using and adjusting the regularization penalty term \cite[Equations (4-5)]{smirnov2013bilinear} throughout the iteration. 
As described in earlier efforts, algorithms for small base cases can be discovered nearly automatically.
However, as the values , , and  grow, more hands-on tinkering using heuristics seems to be necessary to find discrete solutions.

\begin{table}[tb]
\centering
\caption{
Summary of fast algorithms.
Algorithms without citation were found by the authors using the ideas in Section~\ref{sec:finding}.
An asterisk denotes an approximation (APA) algorithm.
The number of multiplications  is equal to the rank  of the corresponding tensor decomposition.
The multiplication speedup per recursive step is the expected speedup if
matrix additions were free.
Note that this speedup does not determine the fastest algorithm because the maximum number
of recursive steps depend on the size of the subproblems created by the algorithm.
By Propositions~\ref{prop:perm1}~and~\ref{prop:perm2}, we also have fast algorithms for all permutations of the base case .
}
\begin{tabular}{l c c c c}
\toprule
                & Number of  & Number of    & Multiplication \\
Algorithm              & multiplies    &  multiplies     &  speedup per \\
base case &   (fast)         & (classical)     & recursive step \\\midrule
 & 11 & 12 & 9\% \\
 & 18 & 20 & 11\% \\
 \cite{strassen1969gaussian} & 7 & 8 & 14\% \\
 & 14 & 16 & 14\% \\
 & 23 & 26 & 17\% \\
 & 15 & 18 & 20\% \\
 & 20 & 24 & 20\% \\
 & 26 & 32 & 23\% \\
 & 29 & 36 & 24\% \\
 & 38 & 48 & 26\% \\
 \cite{smirnov2013bilinear} & 40 & 54 & 35\% \\
\midrule
* \cite{BCRL79} & 10 & 12 & 20\% \\
* \cite{Schonhage81} & 21 & 27 & 29\% \\
\bottomrule
\end{tabular}
\label{tab:algorithms}
\end{table}

\section{Implementation and practical considerations}

We now discuss our code generation method for fast algorithms and the major implementation issues.
All experiments were conducted on a single compute node on NERSC's Edison.
Each node has two 12-core Intel 2.4 GHz Ivy Bridge processors and 64 GB of memory.

\label{sec:implementation}

\subsection{Code generation}
\label{sec:codegen}

Our code generator automatically implements a fast algorithm in C++ given the , , and  matrices representing the algorithm.
The generator simultaneously produces both sequential and parallel implementations.
We discuss the sequential code in this section and the parallel extensions in Section~\ref{sec:parallel}.
For computing , the following are the key ingredients of the generated code:
\begin{itemize}
\item
Using the entries in the  and  matrices, form the temporary matrices  and , , via matrix additions and scalar multiplication.
The  and  are linear combinations of sub-blocks of  and , respectively.
For each  and , the corresponding linear combination is customly generated.
Scalar multiplication by  is replaced with native addition / subtraction operators.
The code generator can produce three variants of matrix additions, which we describe in Section~\ref{sec:matrix_additions}.

When a column of  or  contains a single non-zero element, there is no matrix addition (only scalar multiplication).
In order to save memory, the code generator does not form a temporary matrix in this case.
The scalar multiplication is piped through to subsequent recursive calls and is eventually used in a base case call to .

\item
Recursive calls to the fast matrix multiplication routine compute , .


\item
Using the entries of , linear combinations of the  form the output .
Matrix additions and scalar multiplications are again handled carefully, as above.

\item
Common subexpression elimination detects redundant matrix additions, and the code generator can automatically implement
algorithms with fewer additions.
We discuss this process in more detail in Section~\ref{sec:cse}.

\item
Dynamic peeling handles arbitrary matrix dimensions to make the implementation general.
We review this procedure in in Section~\ref{sec:all_dimensions}.

\end{itemize}

\begin{figure}[tb]
\centering
\includegraphics[width=2.5in]{comp_perf}
\caption{
Effective performance (Equation~\eqref{eqn:eff_perf}) of our code generator's implementation of Strassen's algorithm against MKL's  and a tuned implementation of the Strassen-Winograd algorithm \cite{d2011exploiting}.
The problems sizes are square.
The generated code easily outperforms MKL and is competitive with the tuned code.
}
\label{fig:comp_perf}
\end{figure}

Figure~\ref{fig:comp_perf} benchmarks the performance of the code generator's implementation.
In order to compare the performance of matrix multiplication algorithms with different computational costs,
we use the \emph{effective} GFLOPS metric for  matrix multiplication:

We note that effective GFLOPS is only the true GFLOPS for the classical algorithm
(the fast algorithms perform fewer floating point operations).
However, this metric lets us compare all of the algorithms on an inverse-time scale, normalized by problem size \cite{lipshitz2012communication}.

We compare our code-generated Strassen implementation with MKL's  and a tuned implementation of Strassen-Winograd from D'Alberto \emph{et al.} \cite{d2011exploiting} (recall that Strassen-Winograd performs the same number of multiplications but fewer matrix additions than Strassen's algorithm).
The code generator's implementation outperforms MKL and is competitive with the tuned implementation.
Thus, we are confident that the general conclusions we draw with code-generated implementations of fast algorithms will also apply to hand-tuned implementations.



\subsection{Handling matrix additions}
\label{sec:matrix_additions}

While the matrix multiplications constitute the bulk of the running time,
matrix additions are still an important performance optimization.
We call the linear combinations used to form , , and  \emph{addition chains}.
For example,  is an addition chain in Strassen's algorithm.
We consider three different implementations for the addition chains:

\begin{enumerate}
\item \textbf{Pairwise}:
With  fixed, compute  and  using the  BLAS routine for all matrices in the addition chain.
This requires  calls to  to form  and  calls to form . 
After the  matrices are computed recursively, we follow the same strategy to form the output.
The th sub-block (row-wise) of  requires   calls.\footnote{Because  computes , we make a call for each addition in the chain as well as one call for an initial copy.}


\item \textbf{Write-once}: With  fixed, compute  and  with only one write for each entry (instead of, for example,  writes for  with the pairwise method).
In place of , stream through the necessary submatrices of  and  and combine the entries to form  and .
This requires reading some submatrices of  and  several times,
but writing to only one output stream at a time.
Similarly, we write the output matrix  once and read the  several times.

\item \textbf{Streaming}: Read each input matrix once and write each temporary matrix  and  once.  Stream through the entries of each sub-block of  and , and update the corresponding entries in \emph{all} temporary matrices  and .
Similarly, stream through the entries of the  and update \emph{all} submatrices of .
\end{enumerate}

Each  call requires two matrix reads and one matrix write (except for the first call in an addition chain, which is a copy and requires one read and one write).
Let .
Then the pairwise additions perform  submatrix reads and  submatrix writes.
However, the additions use an efficient vendor implementation.

The write-once additions perform  submatrix reads and at most  submatrix writes.
We do not need to write any data for the columns of  and  with a single non-zero entry.
These correspond to addition chains that are just a copy, for example,  in Strassen's algorithm.
While we perform fewer reads and writes than the pairwise additions,
the complexity of our code increases (we have to write our own additions), and we can no longer use a tuned  routine.
However, we do not worry about code complexity because we use code generation.
Since the problem is bandwidth-bound and compilers can automatically vectorize \texttt{for} loops, we don't expect the latter concern to be an issue.

Finally, the streaming additions perform  submatrix reads and at most  submatrix writes.
This is fewer reads than the write-once additions, but we have increased the complexity of the writes.
Specifically, we alternate writes to different memory locations, whereas with the write-once algorithm, we write to a single output stream.


The three methods also have different memory footprints.
With pairwise or write-once,  and  are formed just before computing .
After  is computed, the memory becomes available.
On the other hand, the streaming algorithm must compute all temporary matrices  and  simultaneously,
and hence needs  times as much memory for the temporary matrices.
We will explore the performance of the three methods at the end of Section~\ref{sec:cse}.

\subsection{Common subexpression elimination}
\label{sec:cse}

The , , and  matrices often share subexpressions.
For example, in our  fast algorithm (see Table~\ref{tab:algorithms}),
 and  are:


Both  and  share the subexpression , up to scalar multiplication.
Thus, there is opportunity to remove additions / subtractions:


\begin{table}[tb]
\centering
\caption{
Number of additions saved by greedily eliminating length-two common subexpressions in the formation of
the  and  matrices.
Since a single subexpression may be used several times, the number of additions saved is greater than the
number of subexpressions eliminated.
}
\begin{tabular}{c c c c c}
\toprule
Algorithm   & Original & CSE & Subexpressions & Additions \\
base case  &              &                 & eliminated & saved \\ \midrule
   & 97 & 70 & 18 & 27\\
   & 189 & 138 & 25 & 51\\
   & 96 & 72 & 13 & 24\\
   & 164 & 125 & 26 & 39\\
   & 53 & 43 & 7 & 10 \\
\bottomrule
\end{tabular}
\label{tab:cse}
\end{table}


Table~\ref{tab:cse} shows how many additions are saved when greedily eliminating length-two expressions.
At face value, eliminating additions would appear to improve the algorithm.
However, there are two important considerations.
First, using  with the pairwise or write-once approaches requires additional memory (with the streaming approach it requires only additional local variables).

Second, we discussed in Section~\ref{sec:matrix_additions} that an important metric
is the number of reads and writes.
If we use the write-once algorithm, we have actually \emph{increased} the number of reads and writes.
Originally, forming  and  required six reads and two writes.
By eliminating the common subexpression, we performed two fewer reads in forming  and  but needed
an additional two reads and one write to form .
In other words, we have read the same amount of data and written \emph{more} data.
In general, eliminating the same length-two subexpression  times reduces the number of matrix reads and writes by .
Thus, a length-two subexpression must appear at least four times for elimination to reduce the total number of reads and writes in the algorithm.

In Figure~\ref{fig:adds_cse}, we benchmark all three matrix addition algorithms from Section~\ref{sec:matrix_additions}, with and without common subexpression elimination.
In general, we see that the write-once algorithm without common subexpression elimination performs the best on the rectangular matrix multiplication problem sizes.
For these problems, common subexpression elimination lowers performance of the write-once algorithm and has little to modest effect on the streaming and pairwise algorithms.
For square matrix problems, the best variant is less clear, but write-once with no elimination often performs the highest.
We use write-once without elimination for the rest of our performance experiments.


\begin{figure*}[tb]
\centering
\includegraphics[width=1.72in]{adds_cse_424_1}
\includegraphics[width=1.72in]{adds_cse_424_2}
\includegraphics[width=1.72in]{adds_cse_423_1}
\includegraphics[width=1.72in]{adds_cse_423_2}
\caption{
Effective performance (Equation~\eqref{eqn:eff_perf}) comparison of common subexpression elimination (CSE) and the three matrix addition methods: write-once, streaming, and pairwise (see Section~\ref{sec:matrix_additions}).
We use the code generator to implement six variants of fast algorithms for  and : using CSE or not for each of the three addition variants.
The  fast algorithm computed  (``outer product" shape) for varying ,
and the  fast algorithm computed  (square multiplication).
For the  fast algorithm, no CSE with write-once additions has the highest performance;
for the  fast algorithm, it is less clear.
The pairwise variants tend to be slower because they perform more reads and writes.
}
\label{fig:adds_cse}
\end{figure*}

\subsection{Recursion cutoff point}
\label{sec:recursion_cutoff}

In practice, we take only a few steps of recursion before calling a vendor-tuned library classical routine as the base case (in our case, Intel MKL's ).
One method for determining the cutoff point is to benchmark each algorithm and measure where the implementation
outperforms .
While this is sustainable for the analysis of any individual algorithm, we are interested in a large class of fast algorithms.
Furthermore, a simple set of cutoff points limits understanding of the performance and will have to be re-measured for different architectures.
Instead, we provide a rule of thumb based on the performance of .

Figure~\ref{fig:dgemm_curves} shows the performance of Intel MKL's sequential and parallel  routines.
We see that the routines exhibit a ``ramp-up" phase and then flatten for sufficiently large problems.
In both serial and parallel, multiplication of square matrices ( computation) tends to level at a higher performance than
the problem shapes with a fixed dimension ( and ).
Our principle for recursion is to take a recursive step only if the sub-problems fall on the flat part of the curve.
If the ratio of performance drop in the DGEMM curve is greater than the speedup per step (as listed in Table~\ref{tab:algorithms}),
then taking an additional recursive step cannot improve performance.\footnote{Note that the inverse is not necessarily true, the speedup depends on the overhead of the additions.}
Finally, we note that some of our parallel algorithms call the sequential  routine in the base case.
Both curves will be important to our parallel fast matrix multiplication algorithms in Section~\ref{sec:parallel}.

\begin{figure*}[tb]
\centering
\includegraphics[width=3in]{dgemm_curves_seq} 
\includegraphics[width=3in]{dgemm_curves_par_nodynamic}
\caption{
Performance curves of MKL's  routine in serial (left) and in parallel (right) for three different problem shapes.
The performance curves exhibit a ``ramp-up" phase and then flatten for large enough problems.
Performance levels near  in serial and  in parallel.
For large problems in both serial and parallel,  multiplication is faster than , which is faster than .
We note that sequential performance is faster than parallel performance due to Intel Turbo Boost, which increases the clock speed from 2.4 to 3.2 GHz.
With Turbo Boost, peak sequential performance is 25.6 GFLOPS.
Peak parallel performance is 19.2 GFLOPS/core.
}
\label{fig:dgemm_curves}
\end{figure*}

\subsection{Handling arbitrary matrix dimensions}
\label{sec:all_dimensions}

The algorithms implemented by our code generator work for any matrix dimensions.
In order to use fast algorithms, however, the submatrices must be the same size---we have to add them together to form the  and  matrices.
There are several strategies for handling matrices whose dimensions are not evenly divided.
These include padding the matrix with zeroes, overlapping submatrices \cite{douglas1994gemmw}, and dynamic peeling \cite{thottethodi1998tuning}.
We choose dynamic peeling, which handles the boundaries of the matrix at each recursive level, in order to keep the code generation simple and limit memory consumption.

\section{Parallel algorithms for shared memory}
\label{sec:parallel}

We present three algorithms for parallel fast matrix multiplication: depth-first search (DFS), breadth-first search (BFS), and a hybrid of the two (HYBRID).
In this work, we target shared memory machines, although the same ideas generalize to distributed memory.
For example, DFS and BFS ideas are used for a distributed memory implementation of Strassen's algorithm~\cite{lipshitz2012communication}.

\subsection{Depth-first search}
\label{sec:par_dfs}

The DFS algorithm is straightforward: when recursion stops, the classical algorithm uses all threads on each sub-problem.
In other words, we use parallel matrix multiplication on the leaf nodes of a depth-first traversal of the recursion tree.
At a high-level, the code path is exactly the same as in the sequential case, and the main parallelism is in library calls.
The advantages of DFS are that the memory footprint matches the sequential algorithm and the code is simpler---parallelism in multiplications is hidden inside library calls.
Furthermore, matrix additions are trivially parallelized.
The key disadvantage of DFS is that the base case must be large enough to see a speed-up because the ramp-up curve is flatter (Figure~\ref{fig:dgemm_curves}).
For Edison's 24-core compute node, the base case should be around .

\subsection{Breadth-first search}
\label{sec:par_bfs}

The BFS algorithm uses task-based parallelism.
Each leaf node in the matrix multiplication recursion tree is an independent task.
The recursion tree also serves as a dependency graph: we need to compute all , , (children) before forming the result (parent).
The major advantage of BFS is we can take more recursive steps because the recursion cutoff point is based on the sequential  curves.
Matrix additions to form  and  are part of the task to form the .
In the first level of recursion, matrix additions to form  from the  
are handled in the same way as DFS, since all threads are available.

The BFS approach has two distinct disadvantages.
First, it is difficult to load balance the tasks because the number of threads may not divide the number of tasks evenly.
Also, with only one step of recursion, the number of tasks can be smaller than the number of threads.
For example, one step of Strassen's algorithm produces only 7 tasks and one step of the fast  algorithm produces only 15 tasks.
Second, BFS requires additional memory since the tasks are executed independently.
In a fast algorithm for  with  multiplies, each recursive step requires a factor  more memory than the output matrix  to store the .
There are additional memory requirements for the  and  matrices, as discussed in Section~\ref{sec:matrix_additions}.

\subsection{Hybrid}
\label{sec:par_hybrid}

Our hybrid algorithm compensates for the load imbalance in BFS by applying the DFS approach on a subset of the base case problems.
With  levels of recursion and  threads, the hybrid algorithm applies task parallelism (BFS) to the first  multiplications.
The number of BFS subproblems is a multiple of , so this part of the algorithm is load balanced.
On the remaining  sub-problems, all threads are used on each multiplication (DFS).

An alternative approach uses another level of hybridization: evenly assign as many as possible of the remaining  multiplications to disjoint subsets of  threads (where  divides ), and then finish off the still-remaining multiplications with all  threads.
This approach reduces the number of small multiplications assigned to all  threads where perfect scaling is harder to achieve.
However, it leads to additional load balancing concerns in practice and requires a more complicated task scheduler.

\subsection{Implementation}
\label{sec:par_implementation}

The code generation from Section~\ref{sec:codegen} produces code that can compile to the DFS, BFS, or HYBRID parallel algorithms.
We use OpenMP to implement each algorithm.
The overview of the parallelization is:
\begin{itemize}
\item \textbf{BFS}: Each recursive matrix multiplication routine and the associated matrix additions are launched as an OpenMP task.
At each recursive level, the \texttt{taskwait} barrier ensures that all  matrices are available to form the output matrix.
\item \textbf{DFS}: Each  call uses all threads.
Matrix additions are always fully parallelized.
\item \textbf{HYBRID}:
Matrix multiplies are either launched as an OpenMP task (BFS), or the number of MKL threads is adjusted for a parallel  (DFS).
This is implemented with the \texttt{if} conditional clause of OpenMP tasks.
Again, \texttt{taskwait} barriers ensure that  matrices are computed to form the output matrix.
We use an explicit synchronization scheme with OpenMP locks to ensure that the DFS steps occur \emph{after} the BFS tasks complete.
This ensures that there is no oversubscription of threads.
\end{itemize}

\subsection{Shared-memory bandwidth limitations}
\label{sec:bandwidth}

The performance gains of the fast algorithms rely on the cost of matrix multiplications to be much larger than the cost of matrix additions.
Since matrix multiplication is compute-bound and matrix addition is bandwidth-bound, these computations scale differently with the amount of parallelism.
For large enough matrices, MKL's  achieves near-peak performance of the node (Figure~\ref{fig:dgemm_curves}).
On the other hand, the STREAM benchmark \cite{mccalpin1995survey} shows that the node achieves around a five-fold speedup in bandwidth with 24 cores.
In other words, in parallel, matrix multiplication is near 100\% parallel efficiency and matrix addition is near 20\% parallel efficiency.
The bandwidth bottleneck makes it more difficult for parallel fast algorithms to be competitive with parallel MKL.
To illuminate this issue, we will present performance results with both 6 and 24 cores.
Using 6 cores avoids the bandwidth bottleneck and leads to much better performance per core.

\subsection{Performance comparisons}

\begin{figure*}[tb]
\centering
\includegraphics[width=2.3in]{comparison_square.eps}
\includegraphics[width=2.3in]{comparison_outer.eps}
\includegraphics[width=2.3in]{comparison_tssquare.eps}
\caption{
Effective performance (Equation~\eqref{eqn:eff_perf}) comparison of the BFS, DFS, and HYBRID parallel implementations on a few fast algorithms and problem sizes.
We use 6 and 24 cores to show the bandwidth limitations of the matrix additions.
(Left): 
Strassen's algorithm on square problems.
With 6 cores, we see significant speedups on large problems.
(Middle):
The  fast algorithm (26 multiplies) on  problems.
HYBRID performs the best in all cases.
With 6 cores, the fast algorithm consistently outperforms MKL.
With 24 cores, the fast algorithm can achieve significant speedups for small problem sizes.
(Right):
The  fast algorithm (29 multiplies) on  problems.
HYBRID again performs the best.
With 24 cores, the fast algorithm gets modest speedups over MKL and achieves significant speedups on small problems.
}
\label{fig:comparisons}
\end{figure*}

Figure~\ref{fig:comparisons} shows the performance of the BFS, DFS, and HYBRID parallel methods with both 6 and 24 cores for three  representative algorithms.

The left plot shows the performance of Strassen's algorithm on square problems.
With 6 cores, HYBRID does the best for small problems.
Since Strassen's algorithm uses 7 multiplies, BFS has poor performance with 6 cores when using one step of recursion.
While all 6 cores can do 6 multiplies in parallel, the 7th multiply is done sequentially (with HYBRID, the 7th multiply uses all 6 cores).
With two steps of recursion, BFS has better load balance but is forced to work on smaller subproblems.
As the problems get larger, BFS outperforms HYBRID due to synchronization overhead when HYBRID switches from BFS to DFS steps.
When the matrix dimension is around 15,000, the fast algorithm achieves a 25\% speedup over MKL.
Using 24 cores, HYBRID and DFS are the fastest.
With one step of recursion, BFS can achieve only seven-fold parallelism.
With two steps, there are 49 subproblems, so one core is assigned 3 subproblems while all others are assigned 2.
In general, we see that it is much more difficult to achieve speedups with 24 cores.
However, Strassen's algorithm has a modest performance gain over MKL for large problem sizes ( 5\% faster).

The middle plot of Figure~\ref{fig:comparisons} shows the  fast algorithm (26 multiplies) for  problems.
With 6 cores, HYBRID is fastest for small problems and BFS becomes competitive for larger problems,
where the performance is 15\% better than MKL.
In Section~\ref{sec:performance}, we show that  is also faster than Strassen's algorithm for these problems.
With 24 cores, we see that HYBRID is drastically faster than MKL on small problems.
For example, HYBRID achieves a 75\% speedup on . \footnote{
This result is an artifact of MKL's parallelization on these problem sizes and is not due to the speedups of the fast algorithm.
We achieved similar speedups using our code generator and a classical,  recursive algorithm (24 multiplies).
}
As the problem sizes get larger, we experience the bandwidth bottleneck and HYBRID achieves around the same performance as MKL.
BFS uses one step of recursion and is consistently slower since it parallelizes 24 of 26 multiplies and uses only 2 cores on the last 2 multiplies.
While multiple steps of recursion creates more load balance, the subproblems are small enough that performance degrades even more.
DFS follows a similar ramp-up curve as MKL, but the subproblems are still too small to see a performance benefit.

The right plot shows the  fast algorithm (29 multiplies) for .
We see similar trends as for the other problem sizes.
With 6 cores, HYBRID does well for all problem sizes.
Speedups are around  5\% for large problems.
With 24 cores, HYBRID is again drastically faster than MKL for small problem sizes and about the same as MKL for large problems.

\section{Performance experiments}
\label{sec:performance}

We now present performance results for a variety of fast algorithms on several problem sizes.
Based on the results of Section~\ref{sec:bandwidth},
we take the best of BFS and HYBRID when using 6 cores and the best of DFS and HYBRID when using 24 cores.
For rectangular problem sizes in both sequential and parallel, we take the best of one or two steps of recursion.
And for square problem sizes, we take the best of one, two, or three steps of recursion.
Additional recursive steps do not improve the performance for the problem sizes we consider.
The square problem sizes for parallel benchmarks require the most memory---for some algorithms, three steps of recursion results in out-of-memory errors.
In these cases, the original problem consumes 6\% of the memory.
For these algorithms, we only record the best of one or two steps of recursion in the performance plots.
Finally, all timings are the median of five trials.

\subsection{Sequential performance}
\label{sec:perf_sequential}

Figure~\ref{fig:sequential_perf} summarizes the sequential performance of several fast algorithms.
For  problems, we test the algorithms in Table~\ref{tab:algorithms} and some of their permutations (top row of plots in Figure~\ref{fig:sequential_perf}).
For example, we test  and , which are permutations of .
In total, over 20 algorithms are tested for square matrices.
Two of these algorithms, Bini's  and Sch\"{o}nhage's  are APA algorithms.
We note that APA algorithms are of limited practical interest; even one step of recursion causes numerical errors in at least half the digits (a better speedup with the same or better numerical accuracy can be obtained by switching to single precision).
For the problem sizes  and , we evaluate the APA algorithms and list performance for algorithms that are comparable to, or outperform, Strassen's algorithm.
The results are summarized as follows:

\begin{enumerate}
\item
All of the fast algorithms outperform MKL for large enough problem sizes.
These algorithms are implemented with our code generator and use only the high-level optimizations described in Section~\ref{sec:codegen}.
Since the fast algorithms perform less computation and communication, we expect this to happen.

\item
For square matrices, Strassen's algorithm often performs the best.
This is mostly due to its relatively small number of matrix additions in comparison to other fast algorithms.
On large problem sizes, Strassen's algorithm provides around a 20\% speedup over MKL's .
In the top right plot of Figure~\ref{fig:sequential_perf}, we see that some algorithms become competitive with Strassen's algorithm for larger problem sizes.
These algorithms tend to have large speedups per recursive step (see Table~\ref{tab:algorithms}).
While Strassen's algorithm can take more recursive steps, memory constraints and the cost of additions with additional recursive steps cause Strassen's algorithm to be on par with these other algorithms.

\item\label{itm:nonsquare}
Although Strassen's algorithm has the highest performance for square matrices, other fast algorithms have higher performance for  and  problem sizes (bottom row of Figure~\ref{fig:sequential_perf}).
The reason is that the fixed dimension constrains the number of recursive steps that can be taken by the fast algorithms.
With multiple recursive steps, the matrix sub-blocks become small enough so that  does not achieve good performance on the subproblem.
Thus, fast algorithms that get a better speedup per recursive step typically have higher performance for these problem sizes.


\item
For rectangular matrices, algorithms that ``match the shape" of the problem tend to perform the best.
For example,  and  both have the ``outer product" shape of the  problem sizes and have the highest performance.
Similarly,  and  have the highest performance of the exact algorithms for  problem sizes.
The   and  algorithms provide around a 5\% performance improvement over Strassen and a 10\% performance improvement over MKL on  and , respectively.
The reason follows from the performance explanation from Result~\ref{itm:nonsquare}.
Only one or two steps of recursion improve performance.
Algorithms that match the problem shape land have high speedups per recursive step perform the best.

\item
Bini's  APA algorithm typically has the highest performance on rectangular problem sizes.
However, we remind the reader that the approximation used by this algorithm results in severe numerical errors.

\end{enumerate}

\begin{figure*}[tb]
\centering
\includegraphics[width=2.30in]{edison_square_seq1}
\includegraphics[width=2.30in]{edison_square_seq2}
\includegraphics[width=2.30in]{edison_square_seq3} \\
\includegraphics[width=2.30in]{edison_outer_seq}
\includegraphics[width=2.30in]{edison_tssquare_seq}
\caption{
Effective sequential performance (Equation~\eqref{eqn:eff_perf}) of fast matrix multiplication algorithms.
Each data point is the best of one, two, or three steps of recursion: additional recursive steps did not improve performance.
MKL is a call to ,
Bini and Sch\"{o}nhage are approximate algorithms, 
and all others are exact fast algorithms.
(Top row):
Performance of a variety of fast algorithms on  problem sizes distributed across three plots.
MKL and Strassen are repeated on all three plots for comparison.
All of the fast algorithms outperform MKL for large enough problem sizes, and Strassen's algorithm usually performs the best.
(Bottom left):
Performance on an ``outer product" shape, .
Exact fast algorithms that have a similar outer product shape (\emph{e.g.}, ) tend to have the highest performance.
(Bottom right):
Performance of multiplication of tall-and-skinny matrix by a small square matrix, .
Again, fast algorithms that have this shape (\emph{e.g.}, ) tend to have the highest performance.
}
\label{fig:sequential_perf}
\end{figure*}


\subsection{Parallel performance}
\label{sec:perf_parallel}

Figure~\ref{fig:parallel_square_perf} shows the parallel performance for multiplying square matrices and Figure~\ref{fig:parallel_nonsquare_perf} shows the parallel performance for  and  problem sizes.
We include performance on both 6 and 24 cores in order to illustrate the bandwidth issues discussed in Section~\ref{sec:bandwidth}.
We observe the following patterns in the parallel performance data:

\begin{enumerate}
\item
With 6 cores, bandwidth scaling is not a problem, and we find many of the same trends as in the sequential case.
All fast algorithms outperform MKL.
Apart from the APA algorithms, Strassen's algorithm is typically fastest for square matrices.
The  fast algorithm has the highest performance for the  problem sizes, 
while  and  have the highest performance for the .
These algorithms match the shape of the problem.

\item
With 24 cores, MKL's  is typically the highest performing algorithm for rectangular problem sizes (bottom row of Figure~\ref{fig:parallel_nonsquare_perf}).
In these problems, the ratio of time spent in matrix additions to time spent in matrix multiplication is too large, and bandwidth limitations prevent the fast algorithms from outperforming MKL.

\item 
With 24 cores and square problem sizes (bottom row of Figure~\ref{fig:parallel_square_perf}),
several algorithms outperform MKL.
Strassen's algorithm provides a modest speedup (around 5\%) and is one of highest performing exact algorithms.
The  and  fast algorithms outperform MKL and are competitive with Strassen.
The square problem sizes spend a large fraction of time in matrix multiplication, so the bandwidth costs for the matrix additions have less impact on performance.

\item
Again, the APA algorithms (Bini's algorithm and Sch\"{o}nhage's algorithm) have high performance on rectangular problem sizes.
It is still an open question if there exists a fast algorithm with the same complexity as Sch\"{o}nhage's algorithm.
Our results show that a significant performance gain is possible with such an algorithm.

\end{enumerate}

\begin{figure*}[tb]
\centering
\includegraphics[width=2.30in]{edison_square_par6_1}
\includegraphics[width=2.30in]{edison_square_par6_2}
\includegraphics[width=2.30in]{edison_square_par6_3} \\
\includegraphics[width=2.30in]{edison_square_par24_1}
\includegraphics[width=2.30in]{edison_square_par24_2}
\includegraphics[width=2.30in]{edison_square_par24_3}
\caption{
Effective parallel performance (Equation~\eqref{eqn:eff_perf}) of fast algorithms on square problems using only 6 cores (top row) and all 24 cores (bottom row).
With 6 cores, bandwidth is not a bottleneck and we see similar trends to the sequential algorithms.
With 24 cores, speedups over MKL are less dramatic, but Strassen (bottom left),  (bottom left), and  (bottom right) all outperform MKL and have similar performance.
Bini and Sch\"{o}hage have high performance, but they are APA algorithms and suffer from severe numerical problems.
}
\label{fig:parallel_square_perf}
\end{figure*}

\begin{figure*}[tb]
\centering
\includegraphics[width=3in]{edison_outer_par6}
\includegraphics[width=3in]{edison_tssquare_par6} \\
\includegraphics[width=3in]{edison_outer_par24}
\includegraphics[width=3in]{edison_tssquare_par24}
\caption{
Effective parallel performance (Equation~\eqref{eqn:eff_perf}) of fast algorithms on rectangular problems using only 6 cores (top row) and all 24 cores (bottom row).
Problem sizes are an ``outer product" shape,  (left column) and
multiplication of tall-and-skinny matrix by a small square matrix,  (right column).
With six cores, all fast algorithms outperform MKL, and new fast algorithms achieve about a 5\% performance gain over Strassen.
With 24 cores, bandwidth is a bottleneck and MKL outperforms fast algorithms.
}
\label{fig:parallel_nonsquare_perf}
\end{figure*}

We also implemented the asymptotically fastest implementation of square matrix multiplication.
The algorithm is based on the  fast algorithm (Table~\ref{tab:algorithms}).
The square algorithm consists of composing , ,  algorithms.
In other words, at the first recursive level, we use ; at the second level ; and at the third, .
The composed fast algorithm is for .
Each step of the composed algorithm computes  matrix multiplications.
The asymptotic complexity of this algorithm is , with .

Although this algorithm is \emph{asymptotically} the fastest, it does not perform well for the problem sizes considered in our experiments.
For example, with 6 cores and BFS parallelism, the algorithm achieved only 8.4 effective GFLOPS/core multiplying square with dimension .
This is far below MKL's performance (Figure~\ref{fig:parallel_square_perf}).
We conclude that while the algorithm may be of theoretical interest, it does not perform well on the modest problem sizes of interest on shared memory machines.








\section{Discussion}

Our code generation framework lets us benchmark a large number of existing and new fast algorithms and test a variety of implementation details, such as how to handle matrix additions and how to implement the parallelism.
However, we performed only high-level optimizations; we believe more detailed tuning of fast algorithms can provide more performance gains.
Based on the performance results we obtain in this work, we can draw several conclusions in bridging the gap between the theory and practice of fast algorithms. 

First, in the case of multiplying square matrices, Strassen's algorithm consistently dominates the performance of exact algorithms (in sequential and parallel).
Even though the exact algorithm for  and Sch\"{o}nhage's APA algorithm\footnote{We note that the performance of Sch\"{o}nhage's APA algorithm accurately represents an exact algorithm for  with 21 multiplies, though it remains an open question whether such an exact algorithm exists.} for  are asymptotically faster in theory, they never outperform Strassen's for reasonable matrix dimensions in practice (sequential or parallel) because the overheads of the additions outweigh the reduction in multiplications.
This sheds some doubt on the prospect of finding a fast algorithm that will outperform Strassen's on square matrices; it will likely need to have a very small base case and still offer a significant reduction in multiplications.

On the other hand, another conclusion from our performance results is that for multiplying rectangular matrices (which typically occurs much more frequently than square in practice), there is a rich space for improvements.
In particular, fast algorithms with base cases that match the shape of the matrices tend to have the highest performance.
There are many promising algorithms in this space, and we suspect that algorithm-specific optimizations will prove fruitful.

Third, in the search for new fast algorithm, our results confirm an important metric.
Given a matrix multiplication tensor corresponding to base case , the rank of the decomposition  (\emph{i.e.}, the number of columns in each matrix) determines the exponent of the arithmetic complexity, and the number of nonzeros in the factor matrices determines the constant prefactor.
Our performance data demonstrates that for a given rank, minimizing the nonzeros in the factor matrices is indeed an important secondary goal.
Although the arithmetic cost associated with the sparsity of  is negligible in practice, the communication cost associated with each nonzero can be performance limiting.
We note that the communication costs of the streaming additions algorithm is independent of the sparsity, but the highest-performing additions algorithm in practice is the write-once algorithm, which is sensitive to the number of nonzeros.

Fourth, we have identified a parallel scaling impediment for fast algorithms, at least on shared memory architectures. 
Because the memory bandwidth often does not scale with the number of cores, and because the additions and multiplications are separate computations in our framework, the overhead of the additions compared to the multiplications worsens in the parallel case.
Short of fundamentally restructuring the fast algorithm implementations, this hardware bottleneck is unavoidable.
We note that on the distributed-memory architectures, this memory-bandwidth scaling bottleneck does not occur---the aggregate memory bandwidth scales with the number of nodes.

We would like to extend our framework to the distributed-memory case, in part because of the better prospects for parallel scaling.
A larger fraction of the time is spent in communication for the classical algorithm on this architecture, and fast algorithms can reduce the communication cost in addition to the computational cost in this case \cite{ballard2012communication}.
Similar code generation techniques will be helpful in exploring the performance of all the algorithms presented in this paper.

As matrix multiplication is the central computational kernel in linear algebra libraries, we would also like to incorporate these fast algorithms into frameworks like BLIS \cite{BLIS1} and PLASMA \cite{kurzak2013multithreading} to see how they affect a broader class of algorithms in numerical linear algebra.
We also plan to develop similar code generation techniques to explore fast algorithms on distributed-memory architectures.

Finally, we have not explored the numerical stability of the exact algorithms in order to compare their results.
While theoretical bounds can be derived from each algorithm's  representation, it is an open question which algorithmic properties are most influential in practice; our framework will allow for rapid empirical testing.
As numerical stability is an obstacle to widespread use of fast algorithms, extensive testing can help alleviate (or confirm) common concerns.



\acks

This research was supported in part by an appointment to the Sandia National Laboratories Truman Fellowship in National Security Science and Engineering, sponsored by Sandia Corporation (a wholly owned subsidiary of Lockheed Martin Corporation) as Operator of Sandia National Laboratories under its U.S. Department of Energy Contract No. DE-AC04-94AL85000.
Austin R. Benson is also supported by an Office of Technology Licensing Stanford Graduate Fellowship.

This research used resources of the National Energy Research Scientific Computing Center, which is supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231.



\bibliographystyle{abbrvnat}

\bibliography{bibliography}










\end{document}
