\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{cvpr}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{placeins}
\usepackage{graphics,epsfig,color}
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\def\cvprPaperID{770} \def\confName{CVPR}
\def\confYear{2022}


\begin{document}

\title{Putting People in their Place: Monocular Regression of 3D People in Depth}
\makeatletter
\g@addto@macro\@maketitle{
\centering
  \includegraphics[width=0.94\textwidth]{Figures/Figure1_new.jpeg}\vspace{-4mm}
\captionof{figure}{\textbf{Monocular reconstruction of multiple 3D people with coherent depth reasoning.} We introduce BEV, a monocular one-stage method with an efficient new ``bird's-eye-view" representation that enables the network to explicitly reason about people in 3D. }\vspace{5mm} \label{fig:teaser}
}

\author{Yu Sun$^1$\thanks{This work was done when Yu Sun was an intern at Explore Academy of JD.com. }\quad
Wu Liu$^{2}$\thanks{Corresponding author.} \quad
Qian Bao$^2$ \quad
Yili Fu$^{1\dagger}$\quad
Tao Mei$^2$\quad
Michael J. Black$^3$ \quad \\

$^1$Harbin Institute of Technology, Harbin, China \quad $^2$ Explore Academy of JD.com, Beijing, China \\
$^3$Max Planck Institute for Intelligent Systems, T\"ubingen, Germany\\
{\tt\small \texttt{yusun@stu.hit.edu.cn, liuwu1@jd.com, baoqian@jd.com, meylfu@hit.edu.cn}}\\
{\tt\small\texttt{tmei@jd.com, black@tuebingen.mpg.de}}\vspace{-2mm}
}

\makeatother

\maketitle

\begin{abstract}
Given an image with multiple people, our goal is to directly regress the pose and shape of all the people as well as their relative depth. Inferring the depth of a person in an image, however, is fundamentally ambiguous without knowing their height. This is particularly problematic when the scene contains people of very different sizes, e.g. from infants to adults. To solve this, we need several things. First, we develop a novel method to infer the poses and depth of multiple people in a single image. While previous work that estimates multiple people does so by reasoning in the image plane, our method, called BEV, adds an additional {\em imaginary} Bird's-Eye-View representation to explicitly reason about depth. BEV reasons simultaneously about body centers in the image and in depth and, by combing these, estimates 3D body position. Unlike prior work, BEV is a single-shot method that is end-to-end differentiable. Second, height varies with age, making it impossible to resolve depth without also estimating the age of people in the image. To do so, we exploit a 3D body model space that lets BEV infer shapes from infants to adults. Third, to train BEV, we need a new dataset. Specifically, we create a ``Relative Human" (RH) dataset that includes age labels and relative depth relationships between the people in the images. Extensive experiments on RH and AGORA demonstrate the effectiveness of the model and training scheme. BEV outperforms existing methods on depth reasoning, child shape estimation, and robustness to occlusion. The code\footnote{\url{https://github.com/Arthur151/ROMP}} and dataset\footnote{\url{https://github.com/Arthur151/Relative_Human}} are released for research purposes.
\end{abstract}

\section{Introduction}
\label{sec:intro}


In this article, we focus on simultaneously estimating the 3D pose and shape of all people in an RGB image along with their relative depth.
There has been rapid progress \cite{liu2022recent} on regressing the 3D pose and shape of individual (cropped) people \cite{kocabas2020vibe,hmr,sun2019dsd-satn,kolotouros2019spin,zhou2018unsupervised,zhang2021pymaf,Kocabas_SPEC_2021,moon2020pose2pose,pavlakos2019texturepose,keep,Zeng_2020_CVPR,zhang2021lightweight} as well as the direct regression of groups \cite{romp,jiang2020coherent}.
Neither class of methods explicitly reasons about the depth of people in the scene.
Such depth reasoning is critical to enable a deeper understanding of the scene and the multi-person interactions within it.
To address this, we propose a unified method that jointly regresses multiple people and their relative depth relations in one shot from an RGB image.

While previous multi-person methods perform well in constrained experimental settings, they struggle with severe occlusion, diverse body size and appearance, the ambiguity of monocular depth, and in-the-wild cases \cite{jiang2020coherent,moon2019camera,zhen2020smap,wang2020hmor}.
These challenges lead to unsatisfactory performance in crowded scenes, including detection misses, similar predictions for overlapping people, and all predictions having a similar height. We observe two inter-related limitations that result in these failures.
First, the architecture of the regression networks is closely tied to the 2D image, while the people actually inhabit 3D space. 
We address this with a new architecture that reasons in 3D.
Second, depth estimation is fundamentally ambiguous due to the unknown height of the people in the image and it is difficult to obtain training data of images with ground-truth height and depth.
To address this, we present a new dataset and novel losses that allow training without having metric depth.



We observe that crowded scenes contain rich information about the relative relationships between people, which can be exploited for both training and validation of depth reasoning.
However, we still lack a powerful representations to learn from these cases.
A few learning-based methods have been proposed for reasoning about the depth of predicted body meshes~\cite{jiang2020coherent} or 3D poses \cite{moon2019camera,zhen2020smap,wang2020hmor}.
Unfortunately, they all reason about depth via 2D representations, such as RoI-aligned features \cite{moon2019camera,jiang2020coherent} or a 2D depth map \cite{zhen2020smap,wang2020hmor}.
These regression-based 2D representations have inherent drawbacks for representing the 3D world.
The lack of an explicit 3D representation in the networks makes it challenging for these methods to deal with crowded scenes in which people overlap at different depths.
Therefore, we argue that an explicit 3D representation is needed.




To achieve this, we develop BEV (for Bird's Eye View), a unified one-stage method for monocular reconstruction and depth reasoning of multiple 3D people.
We take inspiration from ROMP~\cite{romp}, a one-stage, multi-person, regression method that directly estimates multiple 2D front-view maps for 2D human detection, positioning, and mesh parameter regression without depth reasoning.
With ROMP, the network can only reason about the 2D location of people in the image plane.
To go beyond this, we need to enable the network to efficiently reason about depth as well.
To that end, we introduce a new {\em imaginary} 2D ``bird's-eye-view" map that represents the likely centers of bodies in depth. 
To be clear, BEV takes only a single 2D image; the overhead view is inferred, not observed.
BEV uses a powerful and efficient localization pipeline, performing bird's-eye-view-based coarse detection and fine localization in parallel.
We employ the 2D heatmaps for coarse detection from both the front (image) and bird's eye views.
BEV combines these heatmaps to obtain a 3D heatmap, as illustrated in Fig.~\ref{fig:framework}.
By learning the front and the bird's-eye view together, BEV explicitly models how people appear in images and in depth.
This enables BEV to learn from available 2D and 3D annotations. BEV also uses a novel 3D Offset map to refine the initial coarse detections.
From these coarse and fine maps, we obtain the 3D translation of all people in the scene.
BEV transforms these predictions from the latent 3D Center-map space to an explicit camera-centric 3D space.
Given these 3D translation predictions, BEV samples the features of all the people from a predicted mesh feature map and regresses the final SMPL \cite{smpl} parameters.
Distinguishing people at different depths enables BEV to estimate multiple people even with severe occlusion as illustrated in Fig.~\ref{fig:teaser}.


Even with a powerful 3D representation, we need an appropriate training scheme to ensure generalization. The main reason is that without knowing subject height, we lack effective constraints to alleviate the depth/height ambiguity under perspective projection. In particular, height varies with age, making it impossible to resolve depth without also estimating the age of people in the image. The ambiguity causes incorrect depth estimates for children and infants, limiting the generalization of existing methods. 
Unfortunately, existing 3D datasets with multiple people have limited diversity in height and age, so they cannot be used to improve or evaluate generalization. 


Since collecting ground-truth 3D data in the wild is difficult, we instead train BEV using cost-effective weak labels of in-the-wild images. 
Specifically, we collect a dataset, named ``Relative Human" (RH), that contains weak annotations of {\em depth layers} and human ages categorized into the groups adult, teenager, child, and infant.
Moreover, we propose a weakly supervised training scheme (WST) to effectively learn from these weak supervision signals.
For instance, we use a piece-wise loss function that exploits the depth layers to penalize incorrect relative depth orders.
Exploiting age information to constrain height is tricky.
While age and height are correlated, heights can vary significantly within the same age group.
Consequently, we develop an ambiguity-compatible mixed loss function that encourages body shapes with heights that lie within an appropriate range for each age group.

We evaluate BEV on three multi-person datasets: in-the-wild using the 2D RH dataset and in 3D using the real CMU Panoptic \cite{cmu_panoptic} and the synthetic AGORA \cite{patel2021agora} datasets.
On RH, compared with previous methods \cite{moon2019camera,jiang2020coherent,zhen2020smap,wang2020hmor}, BEV is more accurate in relative depth reasoning and pose estimation. 
On CMU Panoptic, BEV outperforms previous methods~\cite{zanfir2018deep,zanfir2018monocular,jiang2020coherent,romp,choi20223dcrowdnet} in 3D pose estimation.
On AGORA, BEV significantly improves detection and achieves state-of-the-art results on ``AGORA kids" in terms of the mesh reconstruction error.
Also, fine-tunning on RH in a weakly supervised manner significantly improves the results for all age groups, especially for young people.

In summary, the main contributions are: (1)  We construct a 3D representation to alleviate the monocular depth ambiguity via combining a front-view representation with an imaginary bird's eye view.
(2) We collect the Relative Human dataset with weak annotations of in-the-wild images, which facilitates the training and evaluation on monocular depth reasoning in multi-person scenes.
(3) We develop a weakly supervised training scheme to learn from weak depth annotations and to exploit age information.



\begin{figure*}[t]
	\centerline{\includegraphics[width=1.00\textwidth]{Figures/Framework_final.png}}
	\vspace{-3mm}
	\caption{Overview. Given an RGB image, BEV first estimates the 3D translation of all people in the scene via compositing the front-view and the bird's-eye-view predictions. Then guided by the 3D translation, we sample the mesh feature of each person to regress their age-aware SMPL+A parameters. See Sec.~\ref{sec:overview} for details.}
	\label{fig:framework}
\end{figure*}


\section{Related Work}
\label{sec:related_work}
\textbf{Monocular 3D mesh regression from natural scenes. }
Here, we focus on regressing a 3D body mesh using a parametric model like SMPL from a single RGB image. 
Most methods can be divided into multi-stage or single-stage approaches. 
For general multi-person cases, most existing methods~\cite{keep,hmr,pavlakos2019texturepose,kolotouros2019spin,moon2020pose2pose} are based on a typical two-stage framework, which first detects people and then estimates the parameters of each person separately.
Recent methods focus on exploring various supervision~\cite{rong2019delving} signals, such as temporal coherence~\cite{kocabas2020vibe}, contour alignment~\cite{xiu2022icon,Dwivedi_DSR_2021,pavlakos2018learning},  self-contact~\cite{muller2021self}, ground constraints~\cite{rempe2021humor,yi2022mover}, or global human trajectory~\cite{yuan2022glamr} to enhance the geometric/dynamic consistency. 
However, for depth reasoning about all people in the scene, these multi-stage methods are not ideal.
The processing of individual cropped people cannot exploit the scene context or reason about depth ordering. 

A few one-stage methods~\cite{romp,mehta2018single} estimate multiple 3D people simultaneously. 
Given a single image, ROMP \cite{romp} outputs a 2D Body Center Heatmap, Camera Map, and Parameter Map for 2D human detection, positioning, and mesh parameter regression, respectively.
At the position parsed from the 2D Body Center heatmap, ROMP samples the final mesh parameters from the Camera and Parameter maps.
These one-stage methods enjoy a holistic view of the image, which is more suitable for depth reasoning.
However, they are based on 2D representations that do not represent depth.
Like most methods, they model adults (with SMPL),  train on images of adults, and therefore only predict adults.
To tackle the limitations of their 2D representation and age bias, we propose BEV and its training scheme of learning age priors that constrain body height. 


\textbf{Monocular depth reasoning. } 
Most previous methods place bodies in depth via post-processing.
Due to their 2D-based pipeline and lack of height prior for different age groups, their results are unsatisfying.
A few learning-based methods, like 3DMPPE~\cite{moon2019camera} and CRMH~\cite{jiang2020coherent}, address multi-stage depth reasoning. 
3DMPPE uses image features to refine the bounding-box-based depth predictions.
CRMH learns from instance segmentation to distinguish the relative depth between overlapping people. 
However, instance segmentation is expensive and unable to promote the learning of depth relations in cases without overlapping.
SMAP~\cite{zhen2020smap} and HMOR~\cite{wang2020hmor} employ a 2D depth map to represent the root depth of 3D pose at each pixel. 
However, in crowded scenes, these 2D representations are ambiguous.
In contrast, BEV adopts a novel bird's-eye-view-based 3D representation to distinguish people at different depths, therefore, it is more robust to the overlapping cases.
Most recently, Ugrinovic et al.~\cite{ugrinovic2021body} propose an optimization-based method to refine the 3D translation of estimated body meshes.
They fit the 3D body mesh to the detected 2D poses and force the feet to touch the ground. 
In contrast, our learning-based, one-stage, framework is more efficient and flexible, and can adapt to more scenarios, such as jumping.
Albiero et al.~\cite{albiero2021img2pose} estimate the depth of all faces in a crowd in one shot by regressing their 6DoF pose; they do not deal with shape variation or articulation.




\section{Method}
\label{sec:method}

\subsection{Overview}\label{sec:overview}

The overall framework is illustrated in Fig.~\ref{fig:framework}.
BEV adopts a multi-head architecture. 
Given a single RGB image as input, BEV outputs 5 maps.
For coarse-to-fine localization, we use the first 4 maps, which are the Body Center heatmaps and the Localization Offset maps in the front view and bird's-eye view. 
We first expand the front-/bird's-eye-view maps in depth/height  and then combine them to generate the 3D Center/Offset maps.
For coarse detection, we extract the rough 3D position of people from the 3D Center map. 
For fine localization, we sample the offset vectors from the 3D Offset map at the corresponding 3D center position. 
Adding these gives the 3D translation prediction.
For 3D mesh parameter regression, we use the estimated 3D translation $(x_i, y_i, d_i)$ and the Mesh Feature map.
The depth value $d_i$ of 3D translation is mapped to a depth encoding.
At $(x_i, y_i)$, we sample a feature vector from the Mesh Feature map and add it to the depth encoding for final parameter regression.
Finally, we convert the estimated parameters to body meshes using the SMPL+A model.

\subsection{SMPL+A: Mesh Representation for All Ages}

The SMPL~\cite{smpl} and SMIL~\cite{hesse2018learning} models are developed to parameterize 3D body meshes of adults and infants into low-dimensional parameters. Recently, AGORA~\cite{patel2021agora} further extends SMPL to support children by linearly blending the SMIL and SMPL template shapes with a weight $\alpha\in[0,1]$, which we refer to as an ``age offset."
While blending the templates to address scale and proportion differences between adults and children, AGORA uses the adult shape space regardless of age. 
Additionally, AGORA does not address the representation of infants.
We make a small, but important, change to better support all ages.

Following the notation of SMPL \cite{smpl}, the SMPL+A model defines a piece-wise function $\vec{B}=\mathcal{M}(\vec{\theta},\vec{\beta},\alpha)$ that maps 3D pose $ \vec{\theta}$, shape $ \vec{\beta}$, and age offset $\alpha$ to a 3D body mesh $\vec{B}\in\mathbb{R}^{6890 \times 3}$.
The pose parameters, $ \vec{\theta} \in \mathbb{R}^{6 \times 22}$, correspond to the 6D  rotations \cite{Zhou_2019_CVPR} of the first 22 body joints of SMPL.
The shape parameter $\vec{\beta} \in \mathbb{R}^{10}$ are the top-10 PCA coefficients of either the SMPL gender-neutral shape space or the SMIL shape space.

The adult shape space of AGORA produces shape deformations that are too large for an infant body, resulting in a distorted mesh when posed.
Therefore, we use SMIL for infants when the age offset $\alpha$ is above a threshold $t_{\alpha}$.
When $\alpha$ \textgreater $t_{\alpha}$, $\mathcal{M}(\vec{\theta},\vec{\beta},\alpha)$ is the SMIL model $\mathcal{M_I}(\vec{\theta},\vec{\beta})$.
When the age offset $\alpha \leq t_{\alpha}$, we use the AGORA formulation
\begin{equation}
\begin{aligned}
      \mathcal{M}(\vec{\theta},\vec{\beta},\alpha)=W(T_A(\vec{\theta},\vec{\beta},\alpha;\boldsymbol{\overline{T}},\boldsymbol{T_I}), J(\vec{\beta}), \vec{\theta}, \boldsymbol{\mathcal{W}}),\\
      T_A(\cdot)=
   (1-\alpha)\boldsymbol{\overline{T}}+\alpha\boldsymbol{\overline{T}_I}+B_S(\vec{\beta})+B_P(\vec{\theta}),
\end{aligned}
\label{eq:SMPL+A}
\end{equation}
where $W(\cdot)$ performs linear blend-skinning with weights $\boldsymbol{\mathcal{W}}$ to convert the T-posed mesh $T_A(\cdot)$ to the target pose $\vec{\theta}$ based on the skeleton joints $J(\cdot)$.
The T-posed mesh $T_A(\cdot)$ is the weighted sum of the templates ($\boldsymbol{\overline{T}},\boldsymbol{\overline{T}_I}$), shape-dependent deformation $B_S(\cdot)$, and pose-dependent deformation $B_P(\cdot)$. 
The age offset $\alpha \in [0,1]$ is used to interpolate between the adult SMPL template $\boldsymbol{\overline{T}}$ and the infant SMIL template $\boldsymbol{\overline{T}_I}$. 
The larger the $\alpha$, the lower the mesh template height.

The 3D joints $\vec{J}$ of the output mesh are derived via $\mathcal{J}\vec{B}$, where $\mathcal{J} \in \mathbb{R}^{K \times 6890}$ is a sparse weight matrix that linearly maps the vertices $\vec{B}$ to the $K$ body joints.
To supervise 3D joints $\vec{J}$ with 2D keypoints, regression methods~\cite{hmr,romp} typically adopt a weak-perspective camera model to project $\vec{J}$ into the image plane. 
For better depth reasoning, we employ a perspective camera model to perform projection; see Sup.~Mat.~for the details of our camera model.

\begin{figure}[t]
	\centerline{\includegraphics[width=0.98\columnwidth]{Figures/RH_demos-min.png}}
	\vspace{-5mm}
	\caption{Example images from the Relative Human (RH) dataset with weak annotations: depth layers (DLs) and age group classification. Examples are a) adults at different DLs, and b) people of different age groups at the same DL.}
	\label{fig:RH_dmeos}\vspace{-3mm}
\end{figure}

\subsection{Relative Human dataset}
\label{sec:relative_human}
Existing in-the-wild datasets lack groups of overlapping people with annotations.
Since acquiring 3D annotations of large crowds is challenging, we exploit more cost-effective weak annotations. 
We collect a new dataset, named Relative Human (RH), to support in-the-wild monocular human depth reasoning.


The images are collected from multiple sources to ensure diversity in age, ethnicity, gender, and scene.
Most images are collected from the existing 2D pose datasets~\cite{coco,crowdpose,zhang2019pose2seg}. 
They contain few infants so we collect additional open-source family photos from Pexels~\cite{pexels} and then annotate their 2D poses.
As shown in Fig.~\ref{fig:RH_dmeos}, we annotate the relative depth relationship between all people in the image.
We treat subjects whose depth difference is less than one body-width ($\gamma=0.3m$) as people in the same layer.
We then classify all people into different depth layers (DLs).
Unlike prior work, which labels the ordinal relationships between pairs of joints of individuals~\cite{chen2016single}, DLs capture the depth order of multiple people.
Additionally, we label people with four age categories: adults, teenagers, children, and babies.

In total, we collect about 7.6K images with weak annotations of over 24.8K people.
More than $21\%$ of the subjects are young people (5.3K), including teenagers, children, and babies.
For more analysis, please refer to Sup.~Mat.

\subsection{Representations}\label{sec:representations}
\noindent Figure~\ref{fig:framework} gives an overview of BEV's representations.


\textbf{Heatmaps}:
We build on the body-center heatmap representation from ROMP \cite{romp}.
The front-view heatmap of size $\mathbb{R}^{1 \times H \times W}$ is aligned with the pixel space and represents the likelihood of a body being centered at a 2D location using Gaussian kernels.
We go beyond ROMP to add a second 2D heatmap of size $\mathbb{R}^{1 \times D \times W}$ that represents an {\em unseen} bird's-eye-view.
This heatmap represents the likelihood of a person being at some point in depth; this map, however, does not represent metric depth.
BEV composes and refines these two maps into a 3D heatmap, $\boldsymbol{M_C^{3D}} \in  \mathbb{R}^{1\times D \times H \times W}$, which represents the 3D position of the detected human body centers with 3D Gaussian kernels.




\textbf{Offset maps}:
The discretized Center Heatmaps coarsely localize the body but we want the network to produce more precise estimates.
To improve the granularity of 3D localization, we use additional maps that, at each position, add an estimated offset vector to refine the coarse detection.
The front-view Offset map of size $\mathbb{R}^{3 \times H \times W}$ contains 3D offset vectors.
The bird's-eye-view Offset map of size $\mathbb{R}^{1 \times D \times W}$ contains 1D offset vectors for depth correction.
$\boldsymbol{M_O^{3D}} \in \mathbb{R}^{3 \times D \times H \times W}$ corresponds to the 3D Center map and contains a 3D offset vector at each 3D position.


\begin{figure}[t]
	\centerline{	\includegraphics[width=0.92\columnwidth]{Figures/cam_repre_new.png}}
	\vspace{-0.1in}
	\caption{Pre-defined 3D camera anchor maps.}
	\label{fig:camera_anchors}\vspace{-3mm}
\end{figure}

\textbf{3D camera anchor maps}:
Each discretized coordinate in the 3D Center map corresponds to a set of camera parameters, representing its 3D position in the world.
The anchor map serves as a mapping function to transform the coordinates of the 3D Center map to the 3D position in a predefined perspective camera space.
To establish a one-to-one mapping from the square Center map to a pyramidal camera space, as shown in Fig.~\ref{fig:camera_anchors}, we voxelize camera space.
Each voxel center corresponds to a discretized 3D coordinate in the Center map.
The 3D position vector $(x,y,d)$ of voxel center is the anchor value of 3D camera anchor map.
Voxels of equal depth form a depth plane, corresponding to a 2D (x-y) slice of the 3D camera anchor map.
During inference, the 3D camera anchor map is sampled at the same coordinate of 3D Center map to obtain the coarse 3D translation of the corresponding detection. 

\textbf{Mesh feature map}:
$\boldsymbol{M_F} \in \mathbb{R}^{128 \times H \times W}$ contains a 128-D mesh feature vector at each 2D position.
These features are aligned with the input 2D image at the pixel level. 
After a 3D-center-based sampling process, the relevant features are used for the regression of SMPL+A parameters.


\subsection{BEV}
To effectively establish the 3D representation, the front-view and the bird's-eye-view must work together to estimate the image position and depth of corresponding subjects.
Independently estimating the map of two views in parallel would inevitably cause misalignment, leading to the failure of 3D heatmap-based detection. 
To connect the two views, we estimate the bird's-eye-view maps conditioned on the front-view maps (i.e.~Center and Offset maps).
Specifically, to estimate the bird's-eye-view maps, we take the concatenation of the front-view maps and the backbone feature maps as input.
The front-view 2D body-centered heatmap is used as a form of robust attention to people in the image, which helps the model focus on exploring depth during bird's-eye view estimation.
Then we expand and composite the 2D maps from the front and BEV views to generate the 3D maps.
To integrate 2D features from two views and enhance 3D consistency, we further perform 3D convolution on the composited 3D maps for refinement.

Next, we extract the 3D translation from the estimated 3D maps, $\boldsymbol{M_C^{3D}},\boldsymbol{M_O^{3D}}$.
High-confidence 3D positions of the 3D Center map are where we sample 3D offset vectors from the 3D Offset map. 
From the same 3D position in the 3D camera anchor maps (Fig.~\ref{fig:camera_anchors}), we obtain the 3D anchor values, which are positions in camera space of the corresponding 3D center voxel. 
Adding the 3D offset vectors to the 3D anchor values gives the 3D translation as output. 

Finally, we take the estimated 3D translation $(x_i, y_i, d_i)$ and Mesh Feature maps $\boldsymbol{M_F}$ for parameter regression.
We sample the pixel-level mesh feature vectors at $(x_i, y_i)$ of $\boldsymbol{M_F}$. 
Inspired by positional embeddings~\cite{transformer}, we learn an embedding space to differentiate people at different depths, especially for the overlapping cases.
The predicted depth value $d_i$ is mapped to a 128-dim encoding vector via an embedding layer.
We sum up the depth encodings and the mesh feature vectors to differentiate the features of people at different depths, enabling individual estimates for different subjects.
Then we estimate the SMPL+A parameters $(\vec{\theta},\vec{\beta},\alpha)$ via a fully-connected block.
The output body meshes are obtained via $\mathcal{M}(\vec{\theta},\vec{\beta},\alpha)$. 



\subsection{Loss Functions}\label{sec:loss_functions}

Our loss functions are divided into two groups illustrated in Fig.~\ref{fig:framework}: relative losses (in gold) and the standard mesh losses (in black).
BEV is supervised by the weighted sum of all loss items.
First, we introduce two relative loss functions for weakly supervised training (WST).

\textbf{Piece-wise depth layer loss $\boldsymbol{\mathcal{L}_{depth}}$.}
$\boldsymbol{\mathcal{L}_{depth}}$ is designed to supervise the predicted depth $d_i, d_j$ of subject $i, j$ by their depth layers $r_i, r_j$ via
\begin{equation}
\setlength{\abovedisplayskip}{1pt}
\setlength{\belowdisplayskip}{1pt}
\small
\begin{aligned}
\begin{cases} 
      (d_i-d_j)^2, & r_i=r_j\\
      log(1+e^{d_i-d_j})\prod((d_i-d_j)-\gamma(r_i-r_j)), & r_i < r_j \\
      log(1+e^{d_j-d_i})\prod(\gamma(r_i-r_j)-(d_i-d_j)), & r_i > r_j ,\\
\end{cases}
\end{aligned}
\label{eq:hierarchical_depth}
\end{equation}
where $\prod$ is a binarization function that maps positive values to 1 and negative values to 0.
$\prod$ is used to judge whether the BEV prediction is consistent with the depth relationship of the ground truth  DLs.
$\boldsymbol{\mathcal{L}_{depth}}$ is 0, if the predicted depth difference is within an acceptable range; that is, greater than the product of the DL difference and body-width $\gamma$.
Otherwise, $\boldsymbol{\mathcal{L}_{depth}}$ will encourage the model to achieve it.

Previous ordinal depth losses~\cite{chen2016single,pavlakos2018ordinal} encourage the model to enlarge the depth difference between people at different depth layers as much as possible.
In contrast, the penalty in $\boldsymbol{\mathcal{L}_{depth}}$ is controlled within a  range.
This helps avoid pushing remote subjects too far away.

\textbf{Ambiguity-compatible age loss $\boldsymbol{\mathcal{L}_{age}}$.}
The classification of age categories (infant, child, teenager, adult) is inherently ambiguous, especially for teenagers and children.
Also, while height is correlated with age, one can easily find children who are taller than some adults.
Consequently, we formulate an ambiguity-compatible mixed loss $\boldsymbol{\mathcal{L}_{age}}$. 

Rather than supervise height directly, we supervise the $\alpha$ parameter that controls the blending between the SMIL infant body and the SMPL adult body.
To do so, we define ranges of $\alpha$ values for each age group; i.e.~(lower-bound, middle, upper-bound).
We do this using the statistical data of heights for each age category that we then relate these to ranges of $\alpha$ values.
Formally, the ranges are  $(\alpha_{l}^k,\alpha_{m}^k,\alpha_{u}^k),k=1\cdots4$ where $k$ is the annotated age class number; see Sec.~\ref{sec:experiments} for details.

BEV is then trained to predict the body shape as well as an $\alpha$ value for each person.
Given the predicted $\alpha$ and ground truth age class $k_{g}$, the loss $\boldsymbol{\mathcal{L}_{age}}$ is defined as
\begin{equation}
\setlength{\abovedisplayskip}{1pt}
\setlength{\belowdisplayskip}{1pt}
\small
\begin{aligned}
\boldsymbol{\mathcal{L}_{age}}(\alpha)=
\begin{cases} 
      0, & \alpha_{l}^{k_{g}}< \alpha \leq \alpha_{u}^{k_{g}}\\
      (\alpha-\alpha_{m}^{k_{g}})^2, & \mathrm{otherwise.}
\end{cases}
\end{aligned}
\label{eq:age_loss}
\end{equation}

\textbf{Other losses.}
Following the previous methods~\cite{romp,hmr}, we employ the standard mesh losses to supervise the output maps and regressed SMPL+A parameters. 
$\boldsymbol{\mathcal{L}_{cm}}$ is the focal loss~\cite{romp} of the front-view Body Center heatmap.
In the same pattern, we further use a 3D focal loss $\boldsymbol{\mathcal{L}_{cm3D}}$ to supervise the 3D Center map via converting $\boldsymbol{\mathcal{L}_{cm}}$'s 2D operation to 3D. 
$\boldsymbol{\mathcal{L}_{pm}}$ consists of three parts, $\boldsymbol{\mathcal{L}_{\theta}},\boldsymbol{\mathcal{L}_{\beta}}$, and $\boldsymbol{\mathcal{L}_{prior}}$.
$\boldsymbol{\mathcal{L}_{\theta}}$ and $\boldsymbol{\mathcal{L}_{\beta}}$ are $L_2$ losses of SMPL+A pose $\vec{\theta}$ and shape $\vec{\beta}$ parameters respectively.
$\boldsymbol{\mathcal{L}_{prior}}$ is the Mixture of Gaussian pose prior~\cite{keep,smpl} on $\vec{\theta}$.
To supervise the 3D body joints $\vec{J}$, we use $\boldsymbol{\mathcal{L}_{j3d}}$, which is composed of $\boldsymbol{\mathcal{L}_{mpj}}$ and $\boldsymbol{\mathcal{L}_{pmpj}}$.
$\boldsymbol{\mathcal{L}_{mpj}}$ is the $L_2$ loss of 3D joints $\vec{J}$.
To alleviate the domain gap between training datasets, we follow~\cite{romp,sun2019dsd-satn} to calculate the $L_2$ loss $\boldsymbol{\mathcal{L}_{pmpj}}$ of the predicted 3D joints after Procrustes alignment with the ground truth.
$\boldsymbol{\mathcal{L}_{pj2d}}$ is the $L_2$ loss of the 2D projection of 3D joints $\vec{J}$.
Lastly, $w_{(.)}$ denotes the corresponding weight of these losses.


\begin{table*}[t]
\setlength\tabcolsep{1.5mm}
\hspace{2mm}
\parbox{.5\linewidth}{
\centering
	\footnotesize
	\begin{tabular}{l|ccccc|c}
	\toprule
	\multirow{2}{*}{Method} & \multicolumn{5}{c|}{{PCDR$^{0.2}$}(\%)$\uparrow$} & \multirow{2}{*}{{mPCK$_{h}^{0.6}$}$\uparrow$} \\
	\cline{2-6}
	& \multicolumn{1}{c}{Baby} & \multicolumn{1}{c}{Kid} & \multicolumn{1}{c}{Teen} & \multicolumn{1}{c}{Adult} & \multicolumn{1}{c|}{All} & \\
    \midrule
    3DMPPE$^\dagger$~\cite{moon2019camera} & 39.33 & 51.42 & 60.91 & 57.95 & 57.47 & - \\ CRMH~\cite{jiang2020coherent} & 34.74 & 48.37 & 59.11 & 55.47 & 54.83 & 0.781  \\ SMAP~\cite{zhen2020smap} & 31.58 & 40.29 & 47.35 & 41.65 & 41.55 & - \\ ROMP~\cite{romp}& 30.08 & 48.41 & 51.12 & 55.34 & 54.81 & 0.866 \\ BEV w/o WST & 34.27 & 50.81 & 54.34 & 57.43 & 57.17 & 0.850 \\ 
    BEV w/o $\boldsymbol{\mathcal{L}_{depth}}$ & 43.61 & 51.55 & 50.88 & 57.27 & 55.97 & 0.794 \\ 
    BEV w/o $\boldsymbol{\mathcal{L}_{age}}$ & 49.09 & 56.55 & 60.92 & 62.47 & 61.47 & 0.810 \\ 
    BEV & \textbf{60.77} & \textbf{67.09} & \textbf{66.07} & \textbf{69.71} & \textbf{68.27} & \textbf{0.884} \\ 
	\bottomrule
    \end{tabular}
    \vspace{-2mm}
	\caption{Accuracy of relative depth relations (PCDR$^{0.2}$) and projected 2D poses (mPCK$_h^{0.6}$) on RH. $^\dagger$ uses the ground truth bounding boxes. }
	\label{tab:relative_human}}
\hspace{5mm}
\parbox{.4\linewidth}{
\centering
\footnotesize
    \begin{tabular}{l|cccc|c}
    \toprule
    Method & Haggl. & Mafia & Ultim. & \multicolumn{1}{c|}{Pizza} & Mean\\
   \midrule
        Zanfir et. al.~\cite{zanfir2018deep} & 141.4 & 152.3 & 145.0 & 162.5 & 150.3 \\
        MSC~\cite{zanfir2018monocular} & 140.0 & 165.9 & 150.7 & 156.0 & 153.4 \\
        CRMH~\cite{jiang2020coherent} & 129.6 & 133.5 & 153.0 & 156.7 & 143.2 \\
        ROMP~\cite{romp} & 110.8 & 122.8 & 141.6 & 137.6 & 128.2\\
        3DCrowdNet~\cite{choi20223dcrowdnet} & 109.6 & 135.9 & 129.8 & 135.6 & 127.3 \\
        \textbf{BEV} & \textbf{90.7} & \textbf{103.7} & \textbf{113.1} & \textbf{125.2} & \textbf{109.5}\\
   \bottomrule
    \end{tabular}
    \vspace{-3mm}
    \caption{Comparisons to the state-of-the-art methods on CMU Panoptic in MPJPE. Results are obtained from the original papers.}  \label{tab:CMU Panoptic}}
\end{table*}\vspace{-3mm}


\begin{table*}[t]
\setlength\tabcolsep{3pt}
	\footnotesize
	\centering
\begin{tabular}{l|ccc|cc|cc|ccc|cc|cc}
	\toprule
 \multirow{3}{*}{Method}&  \multicolumn{7}{c|}{Kid subset} & \multicolumn{7}{c}{Full set} \\
	\cline{2-15}
& \multicolumn{3}{c|}{Detection$\uparrow$} & \multicolumn{2}{c|}{Matched$\downarrow$}  &  \multicolumn{2}{c|}{All$\downarrow$} & \multicolumn{3}{c|}{Detection$\uparrow$} & \multicolumn{2}{c|}{Matched$\downarrow$}  &  \multicolumn{2}{c}{All$\downarrow$} \\
\cline{2-15}
&  \multicolumn{1}{c}{F1 score} & \multicolumn{1}{c}{Precision} & \multicolumn{1}{c|}{Recall} & \multicolumn{1}{c}{MVE} & \multicolumn{1}{c|}{MPJPE} & \multicolumn{1}{c}{NMVE} & \multicolumn{1}{c|}{NMJE} &  \multicolumn{1}{c}{F1 score} & \multicolumn{1}{c}{Precision} & \multicolumn{1}{c|}{Recall} & \multicolumn{1}{c}{MVE} & \multicolumn{1}{c|}{MPJPE} & \multicolumn{1}{c}{NMVE} & \multicolumn{1}{c}{NMJE} \\
\midrule
PARE~\cite{kocabas2021pare}& 0.55 & 0.44 & 0.74 & 186.4 & 193.9 & 338.9 & 352.5 & 0.84 & 0.96 & 0.75 & 140.9 & 146.2 & 167.7 & 174.0\\
SPIN~\cite{patel2021agora}& 0.31 & 0.21 & 0.60 & 186.7 & 191.7 & 602.3 & 618.4 & 0.77 & 0.91 & 0.67 & 148.9 & 153.4 & 193.4 & 199.2\\
SPEC~\cite{Kocabas_SPEC_2021} & 0.52 & 0.40 & 0.73 & 163.2 & 171.0 & 313.8 & 328.8 & 0.84 & 0.96 & 0.74 & 106.5 & 112.3 & 126.8 & 133.7 \\
ROMP~\cite{romp} & 0.50 & 0.37 & 0.80 & 156.6 & 159.8 & 313.2& 319.6 & 0.91 & 0.95 & 0.88 & 103.4 & 108.1 & 113.6 & 118.8\\
BEV w/o WST  & \textbf{0.58} & \textbf{0.44} & \textbf{0.86} & 146.0 & 148.3 & 251.7 & 255.7 & 0.93 & 0.96 & 0.90 & 105.6 & 109.7 & 113.5 & 118.0\\
BEV  & 0.55 & 0.41 & 0.85 & \textbf{125.9} &\textbf{129.1} & \textbf{228.9} & \textbf{234.7} & \textbf{0.93} & \textbf{0.96} & \textbf{0.90} & \textbf{100.7} & \textbf{105.3} & \textbf{108.3} & \textbf{113.2} \\
\bottomrule
\end{tabular}
\vspace{-0.1in}
	\caption{Comparison of SOTA methods on AGORA test set. All methods are fine-tuned on the AGORA training set or synthetic data~\cite{Kocabas_SPEC_2021} generated in the same way as AGORA. We fine-tune ROMP~\cite{romp} using the public implementation; results from  the AGORA leaderboard.}
	\label{tab:agora}
\end{table*}



\section{Experiments}
\label{sec:experiments}

\subsection{Implementation Details}

\textbf{Training details.}
For basic training, we use two 3D pose datasets (Human3.6M~\cite{h36m} and MuCo-3DHP~\cite{mehta2018single}) and four 2D pose datasets (COCO~\cite{coco}, MPII~\cite{mpii}, LSP~\cite{lsp_extended}, and CrowdPose~\cite{crowdpose}). 
We also use the pseudo SMPL annotations from~\cite{joo2020eft} and WST on RH.
Most samples in RH are collected from 2D pose datasets~\cite{coco,crowdpose,zhang2019pose2seg}.
For a fair comparison, we only use the samples that are also used for training in compared methods~\cite{romp,moon2019camera,jiang2020coherent,zhen2020smap,kolotouros2019spin,Kocabas_SPEC_2021}.
To compare with~\cite{Kocabas_SPEC_2021,patel2021agora}, we further fine-tune our model and ROMP on AGORA.
The threshold for the age offset is set to $t_{\alpha}=0.8$.
The age offset ranges $(\alpha_{l}^k,\alpha_{m}^k,\alpha_{u}^k)$ are: adults $(-0.05, 0, 0.15)$, teenagers $(0.15, 0.3, 0.45)$, children $(0.45, 0.6, 0.75)$, and infants $(0.75, 0.9, 1)$.
See Sup.~Mat.~for more details. 


\textbf{Evaluation benchmarks.}
We evaluate BEV on three multi-person datasets, RH, CMU Panoptic, \cite{cmu_panoptic} and AGORA~\cite{patel2021agora}, containing 257 child scans and significant person-person occlusion. 

\textbf{Evaluation matrix.}
To evaluate the accuracy of depth reasoning, we employ the Percentage of Correct Depth Relations (\textbf{PCDR$^{0.2}$}), and set the threshold for equal depth to $0.2m$.
To evaluate the accuracy of projected 2D poses on RH, we also report the mean Percentage of Correct Keypoints (\textbf{mPCK$_{h}^{0.6}$}),  setting the matching threshold to $0.6$ times the head length.

Also, following AGORA~\cite{patel2021agora}, we evaluate the accuracy of 3D pose/mesh estimation while considering missing detections.
To evaluate the detection accuracy, we report \textbf{Precision}, \textbf{Recall}, and \textbf{F1 score}. For matched detections, we report the Mean Per Joint Position Error (\textbf{MPJPE}) and Mean Vertex Error (\textbf{MVE}). 
To punish misses and false alarms in detection, we normalize the MPJPE and MVE by F1 score to get Normalized Mean Joint Error (\textbf{NMJE}) and Normalized Mean Vertex Error (\textbf{NMVE}). 


\subsection{Comparisons to the state-of-the-art methods}

\textbf{Monocular depth reasoning.}
We first evaluate BEV on monocular depth reasoning in Tab.~\ref{tab:relative_human} using the RH dataset.
Results in Tab.~\ref{tab:relative_human} are obtained using the official implementations of compared methods. 
BEV uses the same training samples as~\cite{romp} to perform WST.
We first compare with the most competitive methods~\cite{moon2019camera,jiang2020coherent,zhen2020smap}, which solve depth relations in monocular images.
We also compare with ROMP~\cite{romp}, for one-stage multi-person mesh recovery.
Their 3D translation results are obtained by solving the PnP algorithm (RANSAC~\cite{fischler1981random}) between their 3D pose and projected 2D pose predictions.
As shown in Tab.~\ref{tab:relative_human}, BEV outperforms all these methods in the accuracy of both depth reasoning and projected 2D poses by a large margin.


\textbf{Monocular detection and mesh regression.}
We also run BEV on AGORA and CMU Panpotic to evaluate the detection and 3D mesh accuracy.
We compare with the state-of-the-art (SOTA) multi-stage methods~\cite{zanfir2018deep,zanfir2018monocular,jiang2020coherent,kocabas2021pare,Kocabas_SPEC_2021,patel2021agora,choi20223dcrowdnet} and the one-stage ROMP~\cite{romp}.
Benefiting from the superiority in recall, in Tab.~\ref{tab:agora}, BEV outperforms SOTA methods on detection by $5.2\%$ and $2.2\%$ in terms of F1 score on the kid and full subset, respectively.
This is evidence that the 3D representation helps alleviate depth ambiguity in crowded scenes.
On the kid subset, BEV significantly outperforms previous methods in terms of mesh reconstruction.
Especially, compared with ROMP~\cite{romp}, BEV reduces errors over $19.6\%$ and $26.9\%$ in terms of matched MVE and all NMVE on AGORA kids, indicating that BEV effectively reduces the age bias using WST.
Also, as shown in Tab.~\ref{tab:CMU Panoptic}, on CMU Panpotic, BEV significantly reduces 3D pose errors by $13.9\%$ compared to multi-person SOTA methods.
For qualitative results, see Fig.~\ref{fig:teaser} and
Fig.~\ref{fig:qualitative_results}.

\begin{figure*}[t]
	\centerline{	\includegraphics[width=1\textwidth]{Figures/qualitative_results4.jpeg}}
	\vspace{-0.1in}
	\caption{Qualitative results on AGORA, RH, and Internet images~\cite{pexels}. Note how children and adults are properly placed in depth.}\vspace{-1mm}
	\label{fig:qualitative_results}
\end{figure*}

\subsection{Ablation Studies}





\textbf{Bird's-eye-view representation \&\/ BEV w/o WST.}
To further test the effectiveness of BEV's 3D representation, we train it without performing WST on RH and compare it with SOTA methods on AGORA and RH.
On RH in Tab.~\ref{tab:relative_human}, compared with CRMH~\cite{jiang2020coherent}, the depth reasoning accuracy of BEV w/o WST is 4.1\% higher (\textbf{PCDR$^{0.2}$} of all).
BEV w/o WST outperforms the 2D representation-based network ROMP~\cite{romp}.
These results point to the effectiveness of our 3D representation for dealing with monocular depth ambiguity.
On AGORA, as shown in Tab.~\ref{tab:agora}, BEV w/o WST significantly outperforms ROMP in all detection metrics.
Additionally, the strong detection ability of the 3D representation makes BEV w/o WST outperform the SOTA methods~\cite{romp,Kocabas_SPEC_2021,patel2021agora} in terms of NMVE and NMJE. 



\textbf{Weakly supervised training (WST) losses, $\boldsymbol{\mathcal{L}_{depth}}$ and $\boldsymbol{\mathcal{L}_{age}}$.}
Results in Tab.~\ref{tab:relative_human} show that performing WST significantly improves the accuracy of depth reasoning, especially for the young groups.
Also, Tab.~\ref{tab:relative_human} shows that separately using $\boldsymbol{\mathcal{L}_{depth}}$ or $\boldsymbol{\mathcal{L}_{age}}$ make BEV produce better depth reasoning than BEV w/o WST, and, when using both terms, BEV performs best.


\textbf{3D Offset map (OM) and Front-view condition (FVC) for 3D localization}. 
FVC is taking the front-view 2D body-centered heatmap as a robust attention signal to explore the depth of detected persons during bird's-eye view estimation. 
Results in Tab.~\ref{tab:ablation_study} verify that OM and FVC significantly improve the granularity of 3D localization. 




\textbf{Piece-wise depth layer loss $\boldsymbol{\mathcal{L}_{depth}}$ v.s. ordinal depth loss~\cite{wang2020hmor}.}
Unlike an ordinal depth loss, $\boldsymbol{\mathcal{L}_{depth}}$ keeps the penalty within a reasonable range (see Sec.~\ref{sec:loss_functions}).
As shown in Tab.~\ref{tab:Loss}, on AGORA validation set, training with $\boldsymbol{\mathcal{L}_{depth}}$ reduces the 3D translation error, especially in depth.


\begin{table}[t]
\footnotesize
\centering
	\begin{tabular}{l|cc|ccc}
	\toprule
	\multirow{2}{*}{ Method}  &  \multicolumn{2}{c|}{ Relative Human} & \multicolumn{3}{c}{ AGORA} \\
	\cline{2-6}
	& \multicolumn{1}{c}{PCDR$^{0.2}$} & \multicolumn{1}{c|}{mPCK$_{h}^{0.6}$} & \multicolumn{1}{c}{ F1} & \multicolumn{1}{c}{ NMVE} & \multicolumn{1}{c}{ NMJE} \\
    \midrule
    BEV & \textbf{68.27} & \textbf{0.884} & \textbf{0.93} & \textbf{108.3} & \textbf{113.2}\\ 
    { w/o FVC} & 67.99 & 0.880 & 0.89 & 118.9 & 123.0 \\
    { w/o OM} & 60.76 & 0.620 & 0.87 & 126.6 & 130.7 \\
	\bottomrule
    \end{tabular}
    \vspace{-2mm}
    \caption{Ablation study of front-view condition (FVC) and 3D Offset map (OM) on RH and AGORA.}\vspace{-2mm}
    \label{tab:ablation_study}
\end{table}

\begin{table}[t]
	\footnotesize
	\centering
	\begin{tabular}{l|c|ccc}
	\toprule
Method & Dist.$\downarrow$ & X$\downarrow$ & Y$\downarrow$& Depth$\downarrow$\\
\midrule
Ordinal loss~\cite{wang2020hmor} & 0.608 & 0.153 & 0.184 & 0.509 \\
Piece-wise $\boldsymbol{\mathcal{L}_{depth}}$ (\textbf{ours})  & \textbf{0.518} & \textbf{0.128} & \textbf{0.166} & \textbf{0.423} \\
\bottomrule
\end{tabular}
\vspace{-2mm}
	\caption{3D translation error on AGORA validation set with different depth losses.}\vspace{-2mm}
	\label{tab:Loss}
\end{table}


\vspace{-2mm}
\section{Conclusion, Limitations, Ethics, Risks}\vspace{-1mm}

In this paper, we introduce BEV, a unified one-stage method for monocular regression and depth reasoning of multiple 3D people.
By introducing a novel bird's eye view representation, we enable powerful 3D reasoning that reduces the monocular depth ambiguity.
Exploiting the correlation between body height and depth, BEV learns depth reasoning from complex in-the-wild scenes by exploiting relative depth relations and age group classification.
We make available an in-the-wild dataset to promote the training and evaluation of monocular depth reasoning in the wild.
The ablation studies point to the value of the 3D representation and the fine-grained localization in the network, the importance of our training scheme, and the value of the collected dataset. 
BEV is a preliminary attempt to explore complex multi-person relationships in the 3D world, and we hope the framework will serve as a simple yet effective foundation for future progress. 


\textbf{Limitations.} 
While BEV goes beyond current methods to cover more diverse ages, it is not trained to capture diverse weights, gender, ethnicity, etc. 
BEV also assumes a constant focal length. 
Our labeling approach, however, suggests that weak labels can produce strong results; i.e.~improved metric accuracy.
Note that BEV is not trained or designed to deal with large ``crowds" (e.g.~100's of people). 

\textbf{Ethics and data.}
We collected RH images from a free photo website \cite{pexels} under a Creative Commons license that enables sharing.
We strove to have a dataset that is diverse in age, ethnicity, and gender.
Also, our weak annotations do not contain any personal information and the annotators, themselves, are anonymous and were not studied.

\textbf{Potential Negative Societal Impacts.}
Methods for monocular 3D pose and shape estimation might be used for automated surveillance, tracking, and behavior analysis, which may violate people's privacy. 
To help prevent this, BEV is released for research only.

\noindent \textbf{Acknowledgements:} This work was supported by the National Key R\&D Program of China under Grand No. 2020AAA0103800. 

\noindent \textbf{Disclosure:} MJB has received research funds
from Adobe, Intel, Nvidia, Facebook, and Amazon and has financial interests in Amazon, Datagen Technologies, and Meshcapade GmbH. While he was part-time at Amazon during this project, his research was performed solely at Max Planck. 


{\small
\bibliographystyle{ieee_fullname}
\bibliography{arxiv}
}

\end{document}