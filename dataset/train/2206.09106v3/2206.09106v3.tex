In this Appendix, we provide extended qualitative results, and more details about our Multi-step Projection Gradient as feature, evaluation, and implementation. \textbf{All code will be released for research purposes}. For supplementary videos, please refer to our \href{https://zhengyiluo.github.io/projects/embodied_pose/}{project page}. 

\section{Qualitative Results}

\subsection{Supplementary Video}
As motion is best seen in videos, we provide a supplemental video at \href{https://zhengyiluo.github.io/projects/embodied_pose/}{our project site} to showcase our method. Specifically, we provide qualitative results on the PROX and H36M datasets and also compare with state-of-the-art RGB-based methods (HuMoR-RGB and HyberIK). From the visual result, we can see that our method can estimate physically valid human motion from 2D keypoint inputs in a causal fashion and control a simulated character to interact with the scenes. Compared to non-physics-based method, ours demonstrates less penetration and remain relatively stable under occlusion. For future work, incorporating more physical prior and simulating soft objects such as cushions can further improve motion quality. 





\section{Multi-step Projection Gradient (MPG)}
In this section, we provide a detailed formulation for our multi-step projection gradient as a feature formulation. First, we provide our keypoint definition based on forward kinematics in Sec. \ref{sec:keypoints}. Then, we will describe the geometric and learning-based transformation function  for augmenting MPG. 

\subsection{Forward Kinematics and Keypoint Definition}
\label{sec:keypoints}
\begin{wrapfigure}{R}{0.5\textwidth}
\vspace{-5mm}
\caption{Visualization of the 12 joints we use for pose estimation and the 2D keytpoins obtained from SMPL using forward kinematics. Red is the DEKR 2D keypoints detection result \cite{Geng2021BottomUpHP}, white is our pose estimation result, and green (numbered) is the original 2D keypoints obtained from the projection joints of SMPL.}
\label{fig:joints2d}
\centering
\includegraphics[width=0.5\textwidth]{images/j2d.pdf}
\vspace{-5mm}
\end{wrapfigure}

The SMPL body model defines 24 body joints (green dots visualized in Fig. \ref{fig:joints2d}), which correspond to the 24 angles of the joints. For datasets such as H36M \cite{h36m_pami} and COCO \cite{Lin2014MicrosoftCC}, some joints of the SMPL human body do not have a one-to-one correspondence. Thus, prior SMPL-based human pose estimation methods often make use of a predefined joint regressor to recover the corresponding joints from the body vertices. As described in Sec.3.1, for the same body shape, the lengths of the limbs can change according to body pose, and changing the joint regressor may even increase performance \cite{Hedlin2022ASM}. However, it is computationally expensive to regress 3D joint locations with linear blend skinning at every MPG step. Thus, we opt to use a skeleton with fixed limb length and forward kinematics for our embodied agent toward real-time humanoid control.
To obtain limb length, we use the rest pose (T-pose) of the SMPL body and recover the joint positions using the pre-defined joint regressor. From the joint positions, we compute the fixed limb lengths following the parent-child relationship defined in the kinematics tree. 
 
For both the COCO 2D keypoints \cite{Lin2014MicrosoftCC} and H36M 3D joint positions of H36M \cite{h36m_pami}, we use a subset of these points to obtain the corresponding points on the SMPL kinematics tree. Specifically, we use 12 joints, a similar joint definition to HuMoR \cite{rempe2021humor}, as visualized in Fig. \ref{fig:joints2d}. To obtain correspondences, we choose the following joint numbers from the 24 keypoints based on SMPL: , , , , , , , , ,  without any processing. Then we average the position of joint  and  to obtain the mid-chest point. The pelvis point is obtained by averaging the 3 joints , , and . Following the process mentioned above, we obtain a sparse set of points (12) that we can directly obtain from the SMPL body pose parameters using forward kinematics to match visual observations and evaluation. Since not all 2D keypoints are confidently estimated, we use a confidence threshold of 0.1. For detected 2D keypoints of confidence < 0.1, we do not consider them and omit them during MPG calculation. 



\subsection{MPG and Geometric and Learning-based Transform }
We define the 3D body pose for time step  as  where .  and  are root and body joint orientations in axis-angles, while  is the global translation. 

\paragraph{Geometric Transform of  for .} Due to depth ambiguity and severe occlusion, recovering global translation accurately from 2D observations alone is an ill-posed problem. Previous attempts that use image features\cite{Moon2019CameraDT} and 2D keypoints \cite{Pavllo20193DHP} can often result in unreliable translation estimates (as seen in Tab.2 and our quantitative video). Our pose gradient, on the other hand, provides \textit{temporal} movement information instead of estimating global translation in one-shot. Using the temporal smoothness of human movement and the laws of physics, our pose gradient is more robust to occlusion and will not produce sudden jumps in pose estimates. To provide better root translation, we also take advantage of a geometric transformation step to further improve . Given 2D keypoints  and body pose  in this gradient descent iteration, we can treat a human skeleton as a rigid body to obtain a better translation estimate. Specifically, we compute the world-space 3D keypoints using forward kinematics  and optimize the following reprojection error:

We solve this equation using SVD to obtain  and  is updated by Eq. \ref{eq:update_geotrans}.

Notice that  is guaranteed to lead to a 2D reprojection error no bigger than the original , so we use a direct additive transform to obtain a better global root translation gradient. 

\paragraph{Learning-based Transform of  for .}
Unlike global translation where depth ambiguity can cause sudden jumps in depth perception, we find that global rotation can be robustly estimated using a sequence of 2D keypoints only. Inspired by PoseTriplet \cite{Gong2022PoseTripletC3}, we use the temporal convolution model (TCN) \cite{Pavllo20193DHP} to recover the camera space 3D body orientation. Unlike PoseTriplet \cite{Gong2022PoseTripletC3}, we \textit{only} regress the 3D body orientation and do not regress the 3D body pose: the TCN model is not quite robust to occluded or incorrect 2D keypoints (e.g. missing leg joints, which is prevalent when people are moving behind furniture). However, we notice that the TCN can still recover a reasonable root orientation by reasoning about reliable body joints (such as arms and torsos). Thus, we use the TCN to regress the camera space root orientation  based on a window of 2D keypoints. We then calculate the difference between current root rotation  and : . We directly add  as an additional entry to MPG instead of modifying  since there is no guarantee that the 2D reprojection error from using  will be better. We compute  only once instead of K times. Combining the above two components, we obtain our complete MPG feature. 







\section{Details about Evaluation}
\subsection{Evaluation Metrics}
Here we provide a detailed list of our metric definition.
\begin{itemize}
    \item : success rate measures whether the pose sequence has been estimated successfully. For optimization-based methods, LEMO \cite{lemo:2021} and HuMoR \cite{rempe2021humor}, it indicates whether the optimization process has successfully converged or not. For physics-based methods (ours), it indicates whether the agent has lost balance or not during pose estimation. We measure losing balance as when the position of the simulated agent  and the output of the kinematic policy  deviates too much from each other (> 0.3 meters per joint on average). For regression-based methods, \eg, HyberIK \cite{li2021hybrik}, we do not use this metric. 
    \item  (mm): one-way  average chamfer distance measures the chamfer distance between the estimated human mesh and the point cloud captured from the RGB-D camera. This metric is a proxy measurement of how well the policy can estimate global body translation, as used in HuMoR \cite{rempe2021humor}. Notice that RGB-D based methods such as HuMoR-RGBD and Prox-D both directly optimize this value, while our method does not use any depth information. Unstable optimization and degenerate solutions (where the humanoid flies away) can lead to a very large ACD value. 
    \item  (mm/frame): acceleration measures the joint acceleration and indicates whether the estimated motion is jittery. From the MoCap motion sequences from the AMASS dataset, a value of around 3  10 mm/frame indicates human motion that are smooth. When there is no ground-truth motion available, the joint acceleration serves as a proxy measurement for physical realism. 
    \item  (): frequency of penetration measures how often a scene or ground penetration of more than 100 mm occurs. We obtain the scene penetration measurement by using the mesh vertices obtained from the SMPL body model. 
    \item  (mm): penetration measures the mean absolute values of the penetrated vertices in the scene / ground, similar to the penetration metric in SimPOE \cite{yuan2021simpoe}. 
    \item  (mm): mean per-joint position error is a popular pose-based metric that measures how well the estimated joint matches the ground-truth MoCap joint locations. This metric is calculated in a root-relative fashion where the root joint (, as shown in Fig.\ref{fig:joints2d} is aligned. 
    \item  (mm): mean per-joint position error after procrustes alignment.  
    \item  (mm): absolute mean per-joint position error measures the absolute (non root-relative) joint position error in the world coordinate frame. 
\end{itemize}

\subsection{Modified Scenes}
\begin{wrapfigure}{R}{0.4\textwidth}
\vspace{-5mm}
\caption{Comparison between the scanned scene and our modified scene based on simple shapes. Notice that the scanned scene has much thicker chairs and tables, which leave smaller gaps for the agent to sit down.}
\label{fig:scene_reason}
\centering
\includegraphics[width=0.4\textwidth]{images/scene_simple_reason.pdf}
\end{wrapfigure}

As described in Sec.3.2, we make modified scenes based on the scanned meshes to better approximate the actual 3D scene. Due to the inaccuracy of scanning procedures and point cloud to mesh algorithms, the scanned meshes are often bulkier than the real environment. As a result, the gaps between chairs and tables are often too small in the mesh form, as visualized in Fig. \ref{fig:scene_reason}. These inaccuracies can often cause the humanoid to get stuck between chairs and tables when trying to follow the 2D observations.  To resolve this issue, we created modified scenes from the scanned meshes to better match the videos. We create these scenes semi-automatically by selecting keypoints in the mesh vertices and create rectangular cuboids and cylinders. Another advantage of using these modified scenes is that the signed distance functions for these modified geoms can be computed efficiently. Modified scenes are created for all scenes from the Prox and H36M datasets. All scenes are visualized in Fig.\ref{fig:all_scenes}. 


\subsection{Clarification about evaluation on H36M}
We conduct quantitative evaluations on the H36M dataset, as shown in Table \ref{tab:h36m}. Since we do not make use of a joint regressor, we cannot perform the standard evaluation using the 14 joints commonly used for the evaluation on the H36M dataset. Instead, we use the 12 joints defined in Sec.\ref{sec:keypoints} to calculate  and . As input, we use the same set of 12 2D keypoints. The 2D keypoints are estimated using a pre-trained DEKR \cite{Geng2021BottomUpHP} without additional fine-tuning on the H36M dataset. Our method only uses a sparse set of detected 2D keypoints (12 keypoints) as input and does not directly use any image feature. We also evaluate a different subset of 3D keypoints, as our use of forward kinematics  causes our joint definition to be different from prior works. For quantitative results, please refer to our supplementary video. This experiment also shows our method's ability to estimate pose in a generic setting where there are almost no scene elements (except for the ground and sporadically a chair).












\section{Implementation Details}

\subsection{Shape-based Universal Humanoid Controller}
We extend the universal humanoid controller (UHC) proposed in Kin\_poly \cite{Luo2021DynamicsRegulatedKP} to also support humanoids with different body shapes. Specifically, we added the shape and gender parameter  to the state space of UHC to form our body shape conditioned UHC: . In the original UHC, the body shape information is discarded during training, and the policy is learned using a humanoid based on the mean SMPL body shape. In ours, we create different humanoids for each body shape during training using the ground-truth body shape annotation from AMASS. In this way, we can learn a humanoid controller that can control humanoids with different body shapes. The action space and policy network architecture, and the reward function remain the same. 

\noindent \textbf{Training procedure.}
We train on the training split of the AMASS dataset \cite{Mahmood2019AMASSAO} and remove motion sequences that involve human-object interactions (such as stepping on stairs or leaning on tables). This results in 11402 high-quality motion sequences. At the beginning of each episode, we sample a fixed-length sequence (maximum length 300 frames) to train our motion controller. Reference state initialization \cite{Peng2018DeepMimicED} is uesed to randomly choose the starting point for our sampled sequence and we terminate the simulation if the simulated character deviates too far from the reference motion . To choose which motion sequence to learn from, we employ a hard-negative mining technique based on the success rate of the sequence during training. For each motion sequence , we maintain a history queue (of maximum length 50) for the most recent times the sequence is sampled:  where each  is a Boolean variable indicating whether the controller has successfully imitated the sequence during training. We compute the expected success rate  of the sequence based on the exponentially weighted moving average using the history . The probability of sampling sequence  is then  where  is a temperature parameter. Intuitively, the more we fail at imitating a sequence, the more likely we will sample it.

\subsection{Embodied Scene-aware Kinematic Policy}
\begin{table}[b]
\caption{Hyperparameters for our scene-aware kinematic policy.} 
\label{tab:hyper_kin}
\centering
\resizebox{\linewidth}{!}{\begin{tabular}{lccccccc}
\toprule
   & Batch Size & Learning Rate  &  Voxel grid&   &  
  \\ \midrule
Value     & 5000 &  &  16  16  16 & 5 & 0.0001 
\\ \midrule
& TCN window & MPG - steps  &  MLP-hidden & RNN-hidden &  primitives 
  \\ \midrule
Value   & 81    & 5    & [1024, 512, 256]    & 512    & 6   & \\
\bottomrule 
\end{tabular}}\\ 
\end{table}
\paragraph{Network Architecture and Hyperparameters}



\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{images/Sceneplus_network.pdf}
    \caption{Network architecture for our scene-aware kinematic policy }
    \label{fig:policy}
\end{figure}

\begin{figure}[h]
\label{fig:amass_train}
\centering
\includegraphics[width=\textwidth]{images/scene_amass.pdf}
\caption{Visualization of a randomly sampled motion sequence and paired 2D keypoints for training. Here, we sample from the AMASS dataset (sequence name in the caption).}
\end{figure}


Our kinematic policy is an RNN-based policy with multiplicative compositional control \cite{Peng2019MCPLC}. It consists of a Gated Recurrent Unit (GRU)\cite{Cho2014LearningPR} based feature extractor to provide memory for our policy. Then, 6 primitives  are used to provide a diverse set of pose estimates based on 2D observation  (MPG) and current body state . We process the environmental features separately from the pose features due to its high dimensionality, using an MLP .  The output of , concatenated with the RNN output, is fed into a composer  to produce the action weights. Intuitively, the primitive produces potential body movement based on the 2D observation and current body pose. Then, the composer chooses plausible action based on the environmental features. The detailed data flow of our policy is visualized in Fig. \ref{fig:policy}. The hyperparameters we used for training are shown in Table \ref{tab:hyper_kin}. 





\paragraph{Training procedure.}

To train our kinematic policy, we sample motion sequences from AMASS \cite{Mahmood2019AMASSAO}, H36M \cite{h36m_pami}, and Kin\_poly \cite{Luo20203DHM} to generate synthetic paired 2D keypoints. To learn a network that is more robust to 2D keypoints detected from real videos, we randomly sample confidence from a uniform  distribution as the detection confidence. Keypoints with confidence < 0.1 will be omitted in our model. For each episode, we randomly sample a sequence, randomly change its heading, and sample a valid starting point in the camera frustum as visualized in Fig.\ref{fig:amass_train}. We constrain the newly sampled sequence to be always visible in the image plane and re-sample if not. With a probability of 0.5, we also sample a random scene from the PROX dataset to let our agent learn human-scene interactions. Although the randomly sampled motion sequence does not always yield reasonable human-scene interactions (e.g. the reference motion may walk through a chair), collision and physics simulation will stop our agent automatically. In this way, our agent can learn to use the scene interaction term based on acting inside the scene. 


\section{Broader Social Impact}
This research focuses on estimating physically valid human poses from monocular videos. Such a method can be positively used for animation, telepresence, at home robotics, etc. where natural looking human pose can be served as the basis for higher-level methods. It can also lead to malicious use cases, such as illegal surveillance and video synthesis. As our method focuses on fast and causal inference, it is possible to significantly improve its speed and deploy it to real-time scenarios. Thus, it is essential to deploy these algorithms with care and make sure that the extracted human poses are with consent and not misused.

