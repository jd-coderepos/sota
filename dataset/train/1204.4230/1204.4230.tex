

 Let $\mathscr{G}$ be the set of all finite undirected graphs and let $\mathscr{L}$ be the family of all finite subsets of  $\mathscr{G}$.
  Thus   every element ${\cal F} \in \mathscr{L}$ is a finite set of graphs and
throughout the paper we assume that  ${\cal F} $ is explicitly given.   In this paper we study the following \fd{} problem. 


\smallskip

\defparproblem{\fd{}}{A graph $G$  and a non-negative integer $k$.}{$k$}{Does there exist $S \subseteq V(G)$, $|S| \leq k$,  
 such that $G \setminus S$ contains no graph from ${\cal F}$ as a minor?}
 
 


\smallskip

\noindent 
The \fd{} problem defines  a wide subclass of node (or vertex) removal problems studied from the 1970s. By the classical theorem of  Lewis and Yannakakis  \cite{LewisY80}, deciding if  removing at most $k$ vertices results with a subgraph with property $\pi$ is  NP-complete for every non-trivial property $\pi$.
By a celebrated result of Robertson and Seymour, every \fd{} problem is non-uniformly  fixed-parameter tractable (FPT). 
That is, for every $k$ there is an algorithm solving the problem  in time  $O(f(k) \cdot n^3)$ \cite{RobertsonS13}.  The 
importance of the result comes from the fact that it simultaneously gives FPT algorithms for a variety of important 
problems such as {\sc Vertex Cover}, {\sc Feedback Vertex Set}, {\sc Vertex Planarization},  etc.  
It is conceivable that meta theorems for vertex deletion problems might be formulated by addressing problems that are expressible in logics such as first order and monadic second order. However, since these capture problems that are known 
to be intractable, for example {\sc Independent Set} or {\sc Dominating Set}, we do not expect to have a theorem 
that guarantees tractability for vertex deletion problems through this route. Therefore, 
the systematic study of the \fd{} problems is the more promising way forward to obtain meta-theorems 
for vertex removal problems on general undirected graphs.


In this paper we show that when  ${\cal F}\in \mathscr{L}$ contains at least one planar graph, it is possible to obtain 
 a number of generic results advancing known tractability borders of  \fd{}. 
  The case when $\cal F$ contains a planar graph, while being considerably more restricted than the general case, already encompasses a number of the well-studied  
 instances of \fd{}. For  example, when ${\cal F}=\{K_2\}$, a complete graph on two
vertices, this is the {\sc Vertex Cover} problem. When ${\cal F}=\{C_3\}$, a cycle on three
vertices, this is the {\sc Feedback Vertex Set} problem. Another fundamental problem, which is a special case of  \fd{}, is 
 {\sc Treewidth $\eta$-Deletion} or  {\sc
$\eta$-Transversal} which is  to delete
at most $k$ vertices to obtain a graph of treewidth at most $\eta$. Since any graph of treewidth $\eta$ excludes a 
$(\eta+1)\times (\eta+1)$ grid as a minor, we have that the set $\cal F$ of forbidden minors of treewidth 
$\eta$ graphs contains a planar graph. 
{\sc Treewidth $\eta$-Deletion}  plays important role in 
 generic efficient polynomial time approximation schemes based on Bidimensionality
Theory~\cite{FominLRS11,FominLS12}. Among other examples of   \fd{} that can be found in the literature on approximation and parameterized algorithms, are 
the cases of ${\cal F}$ being 
$\{K_{2,3}, K_4\}$, $ \{K_4\}$, $\{\theta_c\}$,  and $ \{K_{3}, T_2\}$, which correspond to removing vertices to
obtain an  outerplanar graph, a series-parallel graph,  a diamond graph,   and a graph  of pathwidth one,  respectively. 


The main algorithmic contributions of our work is the following set of results for \fd{}  for the case  
when  $\cal F$ contains a planar graph:
\begin{itemize}
\setlength{\itemsep}{-2pt}
\item A constant factor approximation algorithm   for an optimization version of \fd{};  
\item A linear time and single exponential parameterized algorithm for \fd{} when all graphs in ${\cal F}$ are connected, that is, an algorithm running in time $O(2^{O(k)} n)$, where $n$ is the input size;
\item A polynomial kernel for \fd.
\end{itemize}
We use $\mathscr{F}$ to denote the subclass of $\mathscr L$ such that every ${\cal F}\in \mathscr{F}$ contains a planar graph. 









\paragraph{Methodology.} 
All  our  results -- constant factor approximation, polynomial kernelization and   FPT algorithms for  \fd{} -- have a common theme of  polynomial time preprocessing. Preprocessing as a strategy for coping with hard problems is universally applied 
in practice and the notion of {\em kernelization} in parameterized complexity  provides a mathematical framework for analyzing the quality of preprocessing strategies. 
  In parameterized complexity each problem instance comes with a parameter $k$ and  a central notion in parameterized complexity is {\em fixed parameter tractability (FPT)}. This means, for a given instance $(x,k)$, solvability in time $f(k)\cdot p(|x|)$, where $f$ is an arbitrary function of $k$ and $p$ is a polynomial in the input size. 
  The parameterized problem is said to admit a {\it polynomial kernel} if there is a polynomial time algorithm (the degree of polynomial is independent of $k$), called a {\em kernelization} algorithm, that reduces the input instance down to an instance with size bounded by a polynomial $p(k)$ in $k$, while preserving the answer.  
  
  
 

\begin{figure}[t]
\begin{center}
\includegraphics[width=14cm]{figures/diagram.pdf}
\caption{General  view of our approach }\label{fig:diagram}
\end{center}
\end{figure}

  
 Thus the goal of kernelization is to apply reduction rules such that  the size of the reduced instance can be upper bounded by a function of the parameter. However, if we want to use preprocessing for approximation or  FPT algorithms, it is not necessary 
 that  the size of the reduced instance has to be upper bounded. What we need is a preprocessing procedure that allows us  to 
 navigate the solution search space efficiently.  Our first contribution is a notion of preprocessing  that is 
  geared towards  approximation and FPT algorithms. This notion relaxes the demands of kernelization and thus it is possible 
  that a larger set of problems may admit this simplification procedure, when compared to kernelization. 
For approximation and FPT algorithms, we use the notion of {\em $\alpha$-cover} as a measure of good preprocessing. 
For  $0<\alpha\leq 1$, we say that a vertex subset $S\subseteq V(G)$ is an \emph{$\alpha$-cover}, if 
the sum of vertex degrees     $\sum_{v \in S} d(v) $ is at least  $2 \alpha |E(G)|$.
  For example, every vertex cover of a graph is also a $1$-cover. 
  The  defining property of this preprocessing is that the equivalent simplified instance of the problem
 admits some optimal solution which is also  an $\alpha$-cover.  If we succeed with this goal, then for  an edge  selected 
 uniformly at random,   with a constant probability   at least  one of its endpoints belong to some optimal solution. Using this as a basic step, we 
can construct approximation and FPT algorithms. 
 But how to  achieve this kind of preprocessing?
 


To achieve our goals we use the idea of graph replacement dating back to Fellows and
Langston~\cite{FellowsL89}.  Precisely, what we use is the modern notion of ``protrusion reduction"  
that has been recently employed  in \cite{H.Bodlaender:2009ng,FominLST10} for obtaining meta-kernelization 
theorems for problems on sparse graphs like planar graphs, graphs of bounded genus \cite{BodlaenderFLPST09}, 
graphs excluding a fixed graph as a minor or induced subgraph \cite{FominLST10,FominLMPS11},  or graphs excluding a fixed graph as a topological minor~\cite{abs-1201-2780}.  In this method, we find a large protrusion -- 
a graph of small treewidth and small boundary -- and then the preprocessing rule replaces this 
protrusion by a protrusion of constant size. One repeatedly applies this until no longer possible. Finally, by using 
combinatorial arguments one upper bounds the size of the reduced induced (a graph without large protrusion). 
The FPT algorithms use the replacement technique developed in \cite{BodlaenderFLPST09,FominLMPS11}, while for approximation algorithm we need another type of protrusion reduction. The reason why the normal protrusion replacement 
does not work for approximation algorithms is the same as why the NP-hardness reduction is not always an approximation preserving reduction. While the normal protrusion replacement works fine for preserving exact solutions, we needed a notion of protrusion reduction that also preserves approximate solutions. To this end, we develop a new notion  of {\em lossless protrusion reduction}, and show that several problems do admit lossless protrusion reductions. We exemplify the usefulness of the new concept by  obtaining constant factor approximation algorithms for \ofd{}. These FPT and approximation algorithms are obtained by showing that  solutions to the instances 
 of the problem that do not contain protrusion form an $\alpha$-cover for some fixed constant $\alpha$.   



 
Our final result is about kernelization for \fd{}. 
While  protrusion replacements work well for constant factor approximation and optimal FPT algorithms, we do not know how to use this technique for kernelization algorithms for \fd. The technique was developed and used successfully  for kernelization algorithms 
  on  sparse graphs~\cite{BodlaenderFLPST09,FominLST10}
  but there are several limitations of this techniques which do not allow to use it on general graphs. Even for a sufficiently simple case of \fd, namely when $\cal{F}$ is a graph with two vertices and constant number of parallel edges, to apply protrusion replacements we have to do a lot of  additional work to reduce large vertex degrees in a graph  \cite{FominLMPS11}. We do not know how to push these techniques for more complicated families  families  $\cal{F}$ and therefore, employ a different strategy. 
   The new conceptual contribution here is the notion of a \emph{near-protrusion}. Loosely speaking, a near-protrusion is a subgraph which can become a protrusion in the future, after removing some vertices of some optimal solution. The usefulness of   near-protrusions is that they allow to find an irrelevant edge, i.e.,  an edge  which removal does not change the problem. However, finding an irrelevant edge is highly non-trivial, and it requires the usage of well-quasi-ordering  for 
   graphs of bounded treewdith  and bounded boundary as a subroutine.   



As far as we are equipped with new tools and concepts:   $\alpha$-cover, lossless protrusion reduction and pseudo-protrusions, we are able to 
proceed with algorithms for  \fd{}. These algorithms  unify and generalize a multitude of results in the literature. In what follows we survey 
earlier results in each direction and discuss our results.





\vspace{-.3cm}
\paragraph{Approximation.} In the optimization version of \fd{}, we want to 
compute the minimum set $S$, which removal leaves input graph $G$   ${\cal F}$-minor-free. We denote this optimization problem by \ofd.
Characterising graph  properties  for which the corresponding vertex deletion problem can be approximated 
within a constant factor is a long standing open problem in approximation algorithms  \cite{Yannakakis94}.
In spite of long history of research, we are still far from a complete understanding. 
Constant factor approximation algorithms  for  {\sc Vertex Cover}  are known since 1970s \cite{NemT74,Bar-YehudaE81}.
 Lund and Yannakakis observed that 
 the	vertex deletion	problem	for	any	hereditary	property	with	a finite number of minimal forbidden subgraphs can be approximated with a constant ratio \cite{LundY93}. They also conjectured that 
   for every nontrivial, hereditary property with an infinite number of minimal forbidden subgraphs, the  vertex deletion problem cannot be approximated with constant ratio.  However, it appeared later that   {\sc  Feedback Vertex Set} admits  a constant factor approximation~\cite{BarYGJ98,BafnaBF99}
  and thus  the dividing line of   approximability lies somewhere else.
  On a related matter, Yannakakis  \cite{Yannakakis79} showed that  approximating  the number of vertices to delete in order to obtain  \emph{connected} graph with some property $\pi$ within factor $n^{1-\varepsilon}$ is NP-hard, see
\cite{Yannakakis79} for the definition of the property $\pi$.  This result holds for very wide class of  properties, in particular for  properties  being acyclic and  outerplanar.  There was no much progress on approximability/non-approximability of vertex deletion  problems until recent work of Fiorini et al.  \cite{Fiorini:2009ipco} who gave  a constant factor approximation algorithm for 
 \fd{} for the case when ${\cal F}$ is a diamond graph, i.e., a graph with two vertices and three parallel edges.
 
Our  first contribution   is the theorem stating that  every graph property $\pi$ expressible by a finite set of forbidden minors containing at least one planar graph, the	vertex deletion	problem for property $\pi$ admits a constant factor approximation algorithm. In other words, we prove the following theorem
\begin{theorem}\label{thm:approx_thm}
For every set  ${\cal F} \in \mathscr{F}$, 
\ofd{}
admits a  randomized constant  ratio approximation algorithm. 
\end{theorem}
Let us remark that for all known constant factor approximation algorithms of vertex deletion to a  hereditary property $\pi$, property $\pi$ is either
  characterized by an   finite number of minimal forbidden subgraphs or by finite number of forbidden minors, one of which is planar.   Theorem~\ref{thm:approx_thm} together with the result of Lund and Yannakakis, not only encompass all known  vertex deletion problems with constant factor approximation ratio but significantly extends   known tractability 
  borders for   such types of problems.
 
  






\vspace{-.3cm}
\paragraph{Kernelization.}
The study of kernelization is a major research frontier of 
Parameterized Complexity and many important recent advances in the area
are on kernelization. These include 
general results
showing that  certain classes of parameterized problems have polynomial kernels~\cite{Alon:2010vp,H.Bodlaender:2009ng,FominLST10,KratschW12}. 
The recent development of a framework for ruling out polynomial kernels under
certain complexity-theoretic
assumptions~\cite{BDFH08,Dell:2010sh,FS08}    has added a new dimension to
the field and strengthened its connections to classical complexity.  For overviews of kernelization we  refer to surveys~\cite{Bodlaender09,GN07SIGACT}  and to the corresponding chapters in books on Parameterized Complexity 
\cite{FlumGroheBook,Niedermeierbook06}.

While the initial interest in kernelization was driven mainly by practical applications, the notion of kernelization turned out to be very important in theory as well. It is well known, see e.g. \cite{DowneyF99}, that a parameterized problem is fixed parameter tractable, or belongs to the class FPT,  if and only if it has a (not necessarily polynomial) kernel. Kernelization enables us to classify problems within the class FPT further, based on the sizes of the problem kernels. 
One of the fundamental challenges in the area is the possibility of characterising general classes of parameterized problems possessing kernels of polynomial sizes.  

Polynomial kernels for several special cases of \fd{} were studied in the literature.  
 Different kernelization techniques were invented for {\sc Vertex Cover}, eventually resulting in a
 $2k$-sized vertex kernel~\cite{AFLS07,ChenKJ01,DFRS04,Hochbaum:1994kl}.
 For the kernelization of {\sc  Feedback Vertex Set}, there has been a sequence of
 dramatic improvements starting from an $O(k^{11})$ vertex kernel by
 Buragge et al.~\cite{BEFLMR2006}, improved to  $O(k^3)$ by
 Bodlaender~\cite{Bod07}, and then finally to $O(k^2)$ by
 Thomass\'e~\cite{T09}.  A polynomial kernel for \fd{}  for class  ${\cal F}$ consisting of a graph with two vertices and several parallel edges is given in \cite{FominLMPS11}.
 Philip et al.~\cite{PhilipRS09} and Cygan et al.~\cite{CyganPPW10} obtained polynomial kernels for {\sc  Pathwidth 1-Deletion}. Our next theorem generalizes all these kernelization results.   

 \begin{theorem}\label{thm:kernel_thm}
 For every set  ${\cal F} \in \mathscr{F}$,  
\fd{} admits a polynomial kernel.
\end{theorem}

In fact, we prove  more general result---the kernelization algorithm of Theorem~\ref{thm:kernel_thm} always outputs a minor of the input graph. 
This has interesting combinatorial consequences. By Robertson and Seymour theory every non-trivial minor-closed class of graphs can be characterized by a finite set of  forbidden minors or \emph{obstructions}. 
While Graph Minors Theory insures that many interesting graph properties have finite obstructions sets, these seem to be disappointingly huge in many cases. 
There are a number of results that bound the size of the obstructions for specific minor closed families of graphs. Fellows and Langston \cite{FellowsL89_focs,FellowsL94}
suggested  a systematic method of computing the obstructions sets for many natural properties,  see also the recent work of Adler et al.  
\cite{AdlerGK08}.
 Bodendiek and Wagner  gave bounds on sizes of obstructions of genus at most $k$
\cite{BodendiekW89}, later improved by   Djidjev and   Reif
\cite{DjidjevR91}.  Gupta and Impagliazzo studied bounds     
on the size of a planar intertwine of two given planar graphs
 \cite{GuptaI91}. Lagergren \cite{Lagergren98} showed that the number of edges in every obstruction to a graph  of treewidth $k$ is at most double exponential in   $O(k^5)$.  
Dvo{\v{r}}{\'a}k et al.  \cite{Dvorak11} provide similar bound on obstructions to graphs of tree-depth at most $k$. Dinneen and  Xiong have shown that the number of vertices in connected  obstruction  for graphs with vertex cover at most  $k$  is at most  $2k+1$ \cite{Dinneen02VC}. Obstructions for graphs with feedback vertex set of size at most $k$ is discussed in the work of Dinneen et al. 
 \cite{DinneenCF01}.
 

For a finite set of graphs  $\cal{F}$, let ${\cal{G}}_{{\cal{F}},k}$ be a class of graphs such that for every $G\in {\cal{G}}_{{\cal{F}},k}$ there is a subset of vertices $S$ of size at most $k$ such that $G\setminus S$ has no minor from $\cal{F}$.  As a corollary of kernelization algorithm, we obtain the following combinatorial result.  
\begin{theorem}\label{thm:kernel_thm}
 For every set  ${\cal F} \in \mathscr{F}$,  
every minimal obstruction for   $ {\cal{G}}_{{\cal{F}},k}$ is of size  polynomial in $k$. 
\end{theorem}



\vspace{-.5cm}
\paragraph{Fast FPT Algorithms.} 
The study of parameterized problems proceeds in several steps. The first step is to establish if the  problem on hands is fixed parameter tractable or not. If the problem is in FPT, then the next steps are to identify if the problem admits a polynomial kernel and to find the fastest possible FPT algorithm solving the problem. 
The running time  of every FPT algorithm is   $O(f(k) n^c)$, that is, the product of a super-polynomial function $f(k) $ depending only on the parameter $k$ and polynomial $n^c$, where $n$ is the input size and $c$ is some constant.  Both steps, minimizing super-polynomial function $f(k)$ and minimizing  the exponent
$c$ of the polynomial part, are important parts in the design and analysis of parameterized algorithms. 

 The \fd{} problem was introduced by Fellows and Langston~\cite{FellowsL88}, who gave a
non-constructive algorithm running in time $O(f(k)\cdot n^{2})$ for some function
$f(k)$~\cite[Theorem $6$]{FellowsL88}.  This result was improved by Bodlaender
\cite{Bodlaender97} to  $O(f(k)\cdot n)$,  for  $f(k)=2^{2^{O(k \log k)}}$. 
There is a substantial amount of  work  on  improving the exponential function $f(k)$ for special cases of  \fd{}.
For the \textsc{Vertex Cover} problem
the existence of single-exponential algorithms is well-known
since almost the beginnings of the field of Parameterized
Complexity, the current best algorithm being by
Chen et al.~\cite{ChenKX10}. Randomized parameterized single exponential algorithm for 
\textsc{Feedback Vertex Set} was given by Becker et al.
\cite{BeckerBRG00-Ra} but 
   existence of deterministic single-exponential algorithms for
\textsc{Feedback Vertex Set} was open for a while and it took some time and 
discovery of iterative compression  \cite{ReedSmithVetta04}
 to reduce the running time to  $2^{O(k)}n^{O(1)}$~\cite{CaoCL10,ChenFLLV08,CNP+11,DehneFLRS07,GuoGHNW06,RamanSS06}.  The current  champion for
\textsc{Feedback Vertex Set} are the deterministic algorithm of 
Cao et al.  \cite{CaoCL10} with running time $O(3.83^k k n^2)$ and the randomized of Cygan et al.
with running time time~$3^kn^{O(1)}$ ~\cite{CNP+11}. Recently, Joret et al.~\cite{JoretPSST11} showed 
that  \fd{} for ${\cal F}
=\{ \theta_c\}$, where  $\theta_c$ is the graph with two vertices and $c$ parallel edges,     can be solved in time $2^{O(k)}n^{O(1)}$ for 
every fixed $c$. Philip et al.~\cite{PhilipRV10} studied {\sc Pathwidth $1$-Deletion} 
and obtained an algorithm with running time $O(7^kn^{2})$ that was later improved to $O(4.65^k n^{O(1)})$ in~\cite{CyganPPW10}.
Kim et al. \cite{KimPG12} gave a single exponential algorithm for  ${\cal F}
=\{ K_4\}$.  Unless Exponential Time Hypothesis (ETH) fails~\cite{CCF+05,ImpagliazzoPZ01}, single exponential dependence on the parameter $k$ is asymptotically the best bound one can obtain for \fd{}, and thus our next theorem provides asymptotically  optimal bounds on the exponential function of the parameter   and polynomial contribution of the input. 

We call a family ${\cal F} \in \mathscr{F}$ connected if every graph in $\cal F$ is connected. 

  \begin{theorem}\label{thm:fpt_thm1}
  For every connected  set  ${\cal F} \in \mathscr{F}$  containing a planar graph,  there is a randomized algorithm solving 
\fd{}   in time $O(c^k n)$ for some constant $c>1$. 
\end{theorem}

We finally give a deterministic algorithm for \fd{}. Surprisingly, our algorithm does not use iterative compression but  is based on branching on degree sequences. 


  \begin{theorem}\label{thm:fpt_thm2}
  For every connected set  ${\cal F} \in \mathscr{F}$   containing a planar graph,  
\fd{}  is solvable   in time $O(c^k n\log^2{n})$  for some constant $c>1$. 
\end{theorem}





 









