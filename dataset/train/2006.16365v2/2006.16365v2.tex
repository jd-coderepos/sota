\documentclass{ecai}
\usepackage{times}
\usepackage{graphicx}
\usepackage{latexsym}



\PassOptionsToPackage{hyphens}{url} \usepackage[hidelinks]{hyperref} 

\usepackage{lipsum} \usepackage[disable]{todonotes} 

\usepackage{graphicx} \usepackage{subfigure}
\usepackage{tikz} \usepackage{adjustbox}  

\usepackage{booktabs} \usepackage{multirow} \usepackage{multicol} \usepackage{makecell} \usepackage{array, tabularx, boldline} 

\usepackage{amsmath} \usepackage{amssymb} \let\proof\relax  \let\endproof\relax
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{bm} \usepackage{mathtools} \usepackage{mathdots} \usepackage{xfrac} \usepackage{faktor} \usepackage{cancel} 

\usepackage{algorithm}
\usepackage{algpseudocode}  \algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}

\makeatletter
\def\blfootnote{\gdef\@thefnmark{}\@footnotetext}  \makeatother



\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sE{{\mathbb{E}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}


\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}} \newcommand{\Ls}{\mathcal{L}} \newcommand{\R}{\mathbb{R}} \newcommand{\emp}{\tilde{p}} \newcommand{\lr}{\alpha} \newcommand{\reg}{\lambda} \newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}} \newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak







\newcommand{\diag}{{\text{diag}}} \newcommand{\concat}{{\mathbin{+\mkern-10mu+}}} \newcommand{\horizontalconcat}{{^{\frown}}}
\newcommand{\directsum}{{\oplus}}


\theoremstyle{plain}  \newtheorem{thm}{Theorem}  \newtheorem{lem}[thm]{Lemma}  \newtheorem{prop}[thm]{Proposition}
\newtheorem*{cor}{Corollary}  \newtheorem{corn}{Corollary}[thm]  \theoremstyle{definition}  \newtheorem{defn}{Definition}
\newtheorem{conj}{Conjecture}
\newtheorem{exmp}{Example}
\theoremstyle{remark}  \newtheorem*{rem}{Remark}

\newcommand{\proofmanual}{\vspace*{-1ex} \noindent {\bf Proof: }}
\newcommand{\proofsketchmanual}{\vspace*{-1ex} \noindent {\bf Proof Sketch: }}
\newcommand{\qedmanual}{\hfill\rule{2mm}{2mm}}
\newcommand{\round}[1]{{\lfloor{#1}\rceil}}
\renewcommand{\ceil}[1]{{\lceil{#1}\rceil}}  \renewcommand{\floor}[1]{{\lfloor{#1}\rfloor}}  \newcommand{\roundbig}[1]{{\left\lfloor{#1}\right\rceil}}
\newcommand{\ceilbig}[1]{{\left\lceil{#1}\right\rceil}}
\newcommand{\floorbig}[1]{{\left\lfloor{#1}\right\rfloor}}
\newcommand{\parens}[1]{\left({#1}\right)}
\newcommand{\brackets}[1]{\left[{#1}\right]}
\newcommand{\braces}[1]{\left\{{#1}\right\}}
 

\begin{document}

\title{Multi-Partition Embedding Interaction with Block Term Format for Knowledge Graph Completion} 

\author{Hung-Nghiep Tran\institute{The Graduate University for Advanced Studies, SOKENDAI, Japan.} \and Atsuhiro Takasu\institute{National Institute of Informatics, Japan. \{nghiepth, takasu\}@nii.ac.jp}  }


\maketitle
\bibliographystyle{ecai}


\begin{abstract} Knowledge graph completion is an important task that aims to predict the missing relational link between entities. Knowledge graph embedding methods perform this task by representing entities and relations as embedding vectors and modeling their interactions to compute the matching score of each triple. Previous work has usually treated each embedding as a whole and has modeled the interactions between these whole embeddings, potentially making the model excessively expensive or requiring specially designed interaction mechanisms. In this work, we propose the multi-partition embedding interaction (MEI) model with block term format to systematically address this problem. MEI divides each embedding into a multi-partition vector to efficiently restrict the interactions. Each local interaction is modeled with the Tucker tensor format and the full interaction is modeled with the block term tensor format, enabling MEI to control the trade-off between expressiveness and computational cost, learn the interaction mechanisms from data automatically, and achieve state-of-the-art performance on the link prediction task. In addition, we theoretically study the parameter efficiency problem and derive a simple empirically verified criterion for optimal parameter trade-off. We also apply the framework of MEI to provide a new generalized explanation for several specially designed interaction mechanisms in previous models. The source code is released at \url{https://github.com/tranhungnghiep/MEI-KGE}. \blfootnote{\scriptsize{In Proceedings of the European Conference on Artificial Intelligence (ECAI), 2020.}}
\end{abstract}


\section{Introduction} \label{sect:intro} Knowledge graphs are a popular data format for representing knowledge about entities and their relationships as a collection of triples, with each triple  denoting the fact that relation  exists between head entity  and tail entity . Large real-world knowledge graphs, such as Freebase \cite{bollacker_freebasecollaborativelycreated_2008} and Wikidata \cite{vrandecic_wikidatafreecollaborative_2014} have found important applications in many artificial intelligence tasks, such as question answering, semantic search, and recommender systems, but they are usually incomplete. Knowledge graph completion, or link prediction, is a task that aims to predict new triples based on existing triples. Knowledge graph embedding methods perform this task by representing entities and relations as embeddings and modeling their interactions to compute a score that predicts the existence of each triple. These models also provide the embeddings as a useful representation of the whole knowledge graph that may enable new applications of knowledge graphs in artificial intelligence tasks \cite{tran_exploringscholarlydata_2019}.

In a knowledge graph embedding model, the matching score is computed based on the \textit{interaction} between the entries of embeddings. The \textit{interaction mechanism} is the function that computes the score from the embedding entries. The \textit{interaction pattern} specifies which entries interact with each other and how; thus, it can define the interaction mechanism in a simple manner. For example, in DistMult \cite{yang_embeddingentitiesrelations_2015}, the interaction pattern is the diagonal matching matrix between head and tail embedding vectors, as detailed in Section \ref{sect:relatedwork}.

Most previous works treat embedding as a whole and model the interaction between the whole embeddings. For example, the bilinear model RESCAL \cite{nickel_threewaymodelcollective_2011} and the recent model TuckER \cite{balazevic_tuckertensorfactorization_2019} can model very general interactions between every entry of the embeddings, but they cannot scale to large embedding size. One popular approach to this problem is to design special interaction mechanisms to restrict the interactions between only a few entries, for example, DistMult \cite{yang_embeddingentitiesrelations_2015} and recent state-of-the-art models HolE \cite{nickel_holographicembeddingsknowledge_2016}, ComplEx \cite{trouillon_complexembeddingssimple_2016}, and SimplE \cite{kazemi_simpleembeddinglink_2018,lacroix_canonicaltensordecomposition_2018}. However, these interaction mechanisms are specifically designed and fixed, which may pose questions about optimality or extensibility on a specific knowledge graph. 

In this work, we approach the problem from a different angle. We explicitly model the internal structure of the embedding by dividing it into multiple partitions, enabling us to restrict the interactions in a triple to only entries in the corresponding embedding partitions of head, tail, and relation. The local interaction in each partition is modeled with the classic Tucker format \cite{tucker_mathematicalnotesthreemode_1966} to learn the most general linear interaction mechanisms, and the score of the full model is the sum score of all local interactions, which can be viewed as the block term format \cite{delathauwer_decompositionshigherordertensor_2008a} in tensor calculus. The result is a multi-partition embedding interaction (MEI) model with block term format that provides a systematic framework to control the trade-off between expressiveness and computational cost through the partition size, to learn the interaction mechanisms from data automatically through the local Tucker core tensors, and to achieve state-of-the-art performance on the link prediction task using popular benchmarks.



In general, our contributions include the following.
\begin{itemize}
	\item We introduce a new approach to knowledge graph embedding, the multi-partition embedding interaction, which models the internal structure of the embeddings and systematically controls the trade-off between expressiveness and computational cost.
	
	\item In this approach, we propose the standard multi-partition embedding interaction (MEI) model with block term format, which learns the interaction mechanism from data automatically through the Tucker core tensors.
	
	\item We theoretically analyze the framework of MEI and apply it to provide intuitive explanations for the specially designed interaction mechanisms in several previous models. In addition, we are the first to formally study the parameter efficiency problem and derive a simple optimal trade-off criterion for MEI.
	
	\item We empirically show that MEI is efficient and can achieve state-of-the-art results on link prediction using popular benchmarks.
\end{itemize}


\section{Related Work} \label{sect:relatedwork} In this section, we introduce the notations and review the related knowledge graph embedding models.

\subsection{Background} \label{sect:background} In general, we denote scalars by normal lower case such as , vectors by bold lower case such as , matrices by bold upper case serif such as , and tensors by bold upper case sans serif such as . 

A knowledge graph is a collection of triples , with each triple denoted as a tuple , such as \textit{(UserA, Movie1, Like)}, where  and  are head and tail entities in the entity set  and  belongs to the relation set . A knowledge graph can be modeled as a labeled-directed multigraph, where the nodes are entities and each edge corresponds to a triple, with the relation being the edge label. A knowledge graph can also be represented by a third-order binary \textit{data tensor} , where each entry .

Knowledge graph embedding models usually take a triple  as input and then represent it as embeddings and model their interactions to compute a matching score  that predicts the existence of that triple. 

\subsection{Knowledge Graph Embedding Methods} Knowledge graph embedding is an active research topic with many different methods. Based on the interaction mechanisms, they can be roughly divided into three main categories: (1) \textit{semantic matching models} are based on similarity measures between the head and tail embedding vectors, (2) \textit{neural-network-based models} are based on neural networks as universal approximators to compute the matching score, and (3) \textit{translation-based models} are based on the geometric view of relation embeddings as translation vectors \cite{tran_analyzingknowledgegraph_2019,wang_knowledgegraphembedding_2017}.

\paragraph{Semantic Matching Models}
RESCAL \cite{nickel_threewaymodelcollective_2011} is a general model that uses a bilinear map to model the interactions between the whole head and tail entity embedding vectors, with the relation embedding being used as the matching matrix, such that 

where  are the embedding vectors of  and , respectively, and  is the relation embedding matrix of , with  being the embedding size. However, the matrix  grows quadratically with embedding size, making the model expensive and prone to overfitting. TuckER \cite{balazevic_tuckertensorfactorization_2019} is a recent model extending RESCAL by using the Tucker format \cite{tucker_mathematicalnotesthreemode_1966}. However, it also models the interactions between the whole head, tail, and relation embedding vectors, making the core tensor in the Tucker format grow cubically with the embedding size, and also quickly becomes expensive.

One approach to reducing computational cost is to design special interaction mechanisms that restrict the interactions between a few entries of the embeddings. For example, DistMult \cite{yang_embeddingentitiesrelations_2015} is a simplification of RESCAL in which the relation embedding is a diagonal matrix, equivalently a vector , such that . Its score function can also be written as a trilinear product

which is an extension of the dot product to three vectors. 

DistMult is fast but restrictive and can only model symmetric relations. Most recent models focus on designing interaction mechanisms that aim to be richer than DistMult while achieving a low computational cost. For example, HolE \cite{nickel_holographicembeddingsknowledge_2016} uses a circular correlation between the head and tail embedding vectors; ComplEx \cite{trouillon_complexembeddingssimple_2016} uses complex-valued embedding vectors, , and a special complex-valued vector trilinear product; and SimplE \cite{kazemi_simpleembeddinglink_2018,lacroix_canonicaltensordecomposition_2018} represents each entity as two role-based embedding vectors and augments an inverse relation embedding vector. In our previous work \cite{tran_analyzingknowledgegraph_2019}, we analyzed knowledge graph embedding methods from the perspective of a weighted sum of trilinear products to propose a more advanced Quaternion-based interaction mechanism and showed its promising results, which were later confirmed in a concurrent work \cite{zhang_quaternionknowledgegraph_2019}. However, these interaction mechanisms are specially designed and fixed, potentially causing them to be suboptimal or difficult to extend.

In this work, we propose a multi-partition embedding interaction framework to automatically learn the interaction mechanism and systematically control the trade-off between expressiveness and computational cost.

Semantic matching models are related to tensor decomposition methods where the embedding model can employ a standard tensor representation format in tensor calculus to represent the data tensor, such as the CP tensor rank format \cite{hitchcock_expressiontensorpolyadic_1927}, Tucker format \cite{tucker_mathematicalnotesthreemode_1966}, and block term format \cite{delathauwer_decompositionshigherordertensor_2008a}. However, when applied to knowledge graph embedding, there are some differences, such as changing from continuous tensor to binary tensor, relaxation of constraints for data analysis, and different solvers \cite{kolda_tensordecompositionsapplications_2009}. We analyze the connections to the related tensor decomposition methods in Section \ref{sect:theory}.

\paragraph{Neural-Network-based Models}
These models aim to learn a neural network, to automatically model the interaction. Recent models using convolutional neural networks such as ConvE \cite{dettmers_convolutional2dknowledge_2018} can achieve good results by sharing the convolution weights. However, they are restricted by the input format to the neural network \cite{dettmers_convolutional2dknowledge_2018}, and the operations are generally less expressive than direct interactions between the entries of the embedding vectors \cite{nickel_holographicembeddingsknowledge_2016}. We will empirically compare with them.

\paragraph{Translation-based Models}
The main advantages of these models are their simple and intuitive mechanism with the relation embeddings as the translation vectors \cite{bordes_translatingembeddingsmodeling_2013}. However, it has been shown that they have limitations in expressiveness \cite{kazemi_simpleembeddinglink_2018}. The recent model TorusE \cite{ebisu_toruseknowledgegraph_2018} improves the translation-based models by embedding in the compact torus space instead of real-valued vector space and achieves good results. We will also empirically compare with them.


\begin{figure*}[t]
	\centering
	\includegraphics[width=.9\textwidth]{MEI_architecture}
	\caption{MEI architecture: multi-partition embedding vectors that interact only between the corresponding partitions. This figure illustrates a MEI model with block term format in three different views for the local-partition interaction: Tucker format, parameterized bilinear format, and neural network format.}
	\label{fig:mei_architecture}
\end{figure*}

\section{Multi-Partition Embedding Interaction with Block Term Format} \label{sect:model} In this section, we motivate, formulate, and analyze the MEI model, illustrated in Fig. \ref{fig:mei_architecture}. We construct MEI with two main concepts: 
\begin{enumerate}
	\item \textit{Multi-Partition Embedding Interaction:} Each embedding vector  is divided into  partitions, and the interactions in each triple are restricted to only entries in the corresponding partitions . For simplicity, we assume all partitions have the same size , then  can be denoted conveniently as a matrix , where , each row vector  is called a partition, and each column vector  is called a component. 

	\item \textit{Modeling the Interaction with Block Term Format:} The local interaction is modeled with the Tucker format \cite{tucker_mathematicalnotesthreemode_1966}, which is the most general linear model that computes the weighted sum of all entry product combinations in the interacting partitions. The block term format \cite{delathauwer_decompositionshigherordertensor_2008a} emerges from the sum score of all local interactions.
\end{enumerate}

Note that the concept of multi-partition embedding interaction is highly general and intuitive, as discussed in Section \ref{sect:fullmeitheory}. In this paper, we specifically adopt the Tucker and block term tensor formats to realize a simple yet general standard MEI model.

\subsection{The Model}
In each triple , the entities and relations embedding vectors , and  are divided into multiple partitions conveniently denoted as the multi-partition embedding matrices , and , respectively. Note that the embedding sizes of entity and relation are not necessarily the same. 

Formally, the score function of MEI is defined as the sum score of  local interactions, with each local interaction being modeled by the Tucker format, 

where  denotes all parameters in the model;  is the global core tensor at partition ; , , and  are the corresponding partitions  \footnote{Here and below, partitions are column vectors, transpose notation is omitted for simplicity. Illustration as row is just for easy visualization.}; and  denotes the \textit{-mode tensor product with a vector} \cite{kolda_tensordecompositionsapplications_2009}, which contracts the modes of the resulting tensor to make the final result a scalar. The tensor product can be expanded as the following weighted sum

where  is a scalar element of the core tensor  and , and  denote the entries in the local partitions .

\subsection{Theoretical Analysis} \label{sect:theory} Let us discuss the theoretical foundations of MEI, draw connections to previous models, and study the optimal parameter efficiency.

\subsubsection{Local Interaction Modeling} \label{sect:localtheory}
We first focus on analyzing the local interactions in MEI, called local MEI, which are the building blocks of the full MEI model.

\paragraph{Tucker Format and Block Term Format}
We choose to model the local interaction at each partition by the \textit{Tucker format} \cite{tucker_mathematicalnotesthreemode_1966} of third-order tensor

because the Tucker format provides the most general linear interaction mechanism between the embedding vectors, and its core tensor totally defines the interaction mechanism. With local interactions in Tucker format, the full MEI model computed by summing the scores of all local MEI models is in \textit{block term format} \cite{delathauwer_decompositionshigherordertensor_2008a}. Both Tucker format and block term format are standard representation formats in tensor calculus. When applied in knowledge graph embedding, there are some important modifications, such as the data tensor contains binary instead of continuous values, which change the data distribution assumptions, guarantees, constraints, and the solvers. In our work, we express the model as a neural network and use deep learning techniques to learn its parameters as detailed below.

Recently, the Tucker format was independently used in knowledge graph embedding for modeling the interactions on the embedding vector as a whole \cite{balazevic_tuckertensorfactorization_2019}, while we only use the Tucker format for modeling the local interactions in our model. Thus, their model corresponds to a vanilla Tucker model, which is the special case of MEI when . Note that this vanilla Tucker model suffers from the scalability problem when the embedding size increases, whereas MEI essentially solves this problem. Moreover, MEI provides a general framework to reason about knowledge graph embedding methods, as discussed in Section \ref{sect:fullmeitheory}.

\paragraph{Parameterized Bilinear Format}
To better understand how the core tensor defines the interaction mechanism in local MEI, we can view the local interaction in Eq. \ref{eq:scoremeitensorproductatk} as a \textit{parameterized bilinear model}, by rewriting the tensor products as 

where  denotes the matching matrix of the bilinear model. Note that  defines the interaction patterns of the bilinear map between  and , but itself is defined by . Specifically, each element  of the matching matrix  is a weighted sum of the entries in , weighted by the mode- tube vector  of . Therefore, the core tensor  defines the \textit{interaction patterns} or the \textit{interaction mechanisms} at partition . Compared with the standard bilinear model RESCAL, local MEI is more flexible and efficient because its matching matrices are generated from the relation embedding vectors. Moreover, the global core tensors enable information sharing between all entities and relations, which is particularly useful when the data are sparse.



\paragraph{Dynamic Neural Network Format}
For parameter learning, we express the Tucker format as a \textit{neural network} to employ standard deep learning techniques such as dropout \cite{srivastava_dropoutsimpleway_2014} and batch normalization \cite{ioffe_batchnormalizationaccelerating_2015} to reduce overfitting and improve the convergence rate. Specifically, Eq. \ref{eq:scoremeibilinearatk} can be seen as a \textit{linear neural network}, where  is the input of the network,  is the weight of the hidden layer,  is the output of the hidden layer,  is the weight of the output neuron, and  is the output of the network. Note that the weight of the hidden layer, , can be seen as the output of another neural network, where  is the input and the core tensor  is the weight. Under this format, there are four layers to apply dropout and batch normalization: , , , and , which are tuned as hyperparameters.

\subsubsection{Multi-Partition Embedding Interaction} \label{sect:fullmeitheory}
There are several reasons why \textit{Multi-Partition Interaction} is superior and preferable to \textit{Local-Partition Interaction}. Here, we present some interpretations of the full MEI model to explain its properties.

\paragraph{Sparse Modeling}
The full MEI model can be seen as a special form of \textit{sparse parameterized bilinear models}. The matching matrix of the full MEI model is constructed by the direct sum of the matching matrices of all local MEI models, and the result is a sparse parameterized block-diagonal matrix 

The score function of the full MEI model can then be written as a bilinear model

where , , and  are the original embedding vectors before dividing into  partitions. Similarly, we can view MEI in the form of a special \textit{sparse Tucker model}, where the sparse core tensor  of MEI is constructed by the direct sum of the  local core tensors  and the score function is written as

This view provides a concrete explanation for the interaction mechanism in the MEI model, as it can be seen as imposing a sparsity constraint on the core tensor, or equivalently the matching matrices, to make the model efficient.



\paragraph{Multiple Interactions and the Ensemble Boosting Effect}
An intuitive explanation of MEI is that it models \textit{multiple relatively independent interactions} between the head and tail entities in a knowledge graph. These interactions correspond to the separate local partitions of the embedding vectors and together define the final matching score. Technically, MEI forms an ensemble of  local interactions by summing their scores, as seen in Eq. \ref{eq:scoremeitensorproduct}, similarly to ensemble averaging. However, we argue that MEI works as an \textit{ensemble boosting} model in a similar manner to gradient boosting methods because the summing operation is done in training and all local MEI models are optimized together. This view intuitively explains the success of MEI when each local interaction is very simple, such as when the partition size is only  or . It also suggests the empirical benefit of the ensemble boosting effect in MEI with  over the vanilla Tucker.



\paragraph{Vector-of-Vectors Embedding and the Meta-Dimensional Transforming--Matching Framework}
An important insight of MEI is that the embedding can be seen as a \textit{vector of vectors}, which means a meta-vector where each meta-dimension corresponding to a local partition contains a vector entry instead of a scalar entry. Compared to scalar entry, a vector entry contains more information and allows more expressive yet simple transformation on each entry. By using this notion of vector-of-vectors embedding, we can view MEI as a \textit{transforming--matching framework}, where the model simply transforms each meta-dimension entry of head embedding then matches it with the corresponding meta-dimension entry of tail embedding. This framework can serve as a novel general design pattern of knowledge graph embedding methods, as we show in Section \ref{sect:connection} how it can explain the previous specially designed models.



\subsubsection{Connections to Previous Specially Designed Interaction Mechanisms}\label{sect:connection}


There exist a few generalizations of previous embedding models that include DistMult, ComplEx, and SimplE; such as \cite{kazemi_simpleembeddinglink_2018} explaining them using a bilinear model, \cite{balazevic_tuckertensorfactorization_2019} using a vanilla Tucker model, and \cite{tran_analyzingknowledgegraph_2019} using a weighted sum of trilinear products. However, these generalizations consider the embedding as a whole, here we present a new generalization that considers the embedding as a multi-partition vector to provide a more intuitive explanation of these models and their specially designed interaction mechanisms. 

We first construct the multi-partition embedding vector for these models. DistMult is trivial with  and . For ComplEx and SimplE,  and . In ComplEx, each partition  consists of the real and imaginary components of the entry  in a ComplEx embedding vector. In SimplE, each partition  consists of the two entries  in the two role-based embedding vectors. With this correspondence, these previous models can be written in the sparse bilinear model form of MEI in Eq. \ref{eq:sparsebilinear} and Eq. \ref{eq:scoremeibilinearsparse}. For DistMult, each matching block  is just a scalar entry of the relation embedding vector. More interestingly, for ComplEx, each matching block is a  matrix with the \textit{rotation pattern}, parameterized by the relation embedding vector,  For SimplE, each matching block is a  matrix with the \textit{reflection pattern}, parameterized by the relation embedding vector,  where  is the augmented inverse relation embedding vector. CP \cite{hitchcock_expressiontensorpolyadic_1927} is similar to SimplE, but missing , making the matching matrix lose the geometrical interpretation, which is probably the reason why CP does not generalize well to new data, as reported in \cite{tran_analyzingknowledgegraph_2019}. 

The interaction mechanisms of these models are totally characterized by the simple and fixed patterns in their matching blocks , which also specify the interaction restriction between the entries. In MEI, the interaction restriction can be varied by setting the partition size, and more importantly, the interaction patterns can be automatically learned from data.

\subsubsection{Computational Analysis} \paragraph{Complexity} For simplicity, we consider the same embedding size  for both entity and relation. The parameters in a MEI model include the embedding vectors of all entities, all relations, and the core tensors. On a knowledge graph with  entities and  relations, the number of parameters in MEI is . In this paper's experiments, we restrict them to the simplified case of one single shared-core tensor for all  partitions, so the number of parameters in this case is .

We note a few interesting observations. First, the core tensor size of the vanilla Tucker (when ) is much larger than the sparse core of MEI, up to  times in non-shared-core MEI and  times in shared-core MEI. These factors can become crucial in practice; for example, with  and , the vanilla Tucker core has 1 billion parameters, making it infeasible on most GPUs, while shared-core MEI has only 1 million parameters in the core tensor. Second, the partition size  can be set independently from the embedding size ; thus, the core tensor sizes can be considered as growing linearly with  in the former case of non-shared-core MEI, and as constant in the latter case of shared-core MEI.

\paragraph{Parameter Efficiency} By using Tucker format for local interactions, MEI with block term format is \textit{fully expressive}. However, in practice, we usually do not care about the parameter \textit{upper bound} for fully expressiveness of the model. The more interesting property of the model is its ability to efficiently capture complex patterns in the knowledge graph. In this regard, we define the criteria to measure the expressiveness and parameter efficiency of the model. To the best of our knowledge, we are the first to formally study the parameter efficiency in knowledge graph embedding.

From the interpretation of MEI as a transforming--matching framework in Section \ref{sect:fullmeitheory}, where the model first transforms each head embedding partition then simply matches it with the corresponding tail embedding partition, we see that the ability to capture complex patterns depends totally on the transformation system.
\begin{defn} \textbf{\textit{(Expressiveness)}} \label{def:expressive}
	The expressiveness of the MEI model is measured by the degrees of freedom of the model provided by its transformation system. 
\end{defn}
For example, a linear transformation in a -dimensional space has 9 degrees of freedom: 3 for translation, 3 for rotation, and 3 for scaling. For a MEI model with two partitions of size , the sum score of two local interactions has  degrees of freedom.

As mentioned earlier, the vanilla Tucker model can become excessively expensive when the embedding size is large, in which case, it is necessary to use a MEI model with a smaller partition size. To compare fairly across models, we define the \textit{parameter efficiency}.
\begin{defn} \textbf{\textit{(Parameter efficiency)}} \label{def:paramefficient}
	The parameter efficiency of a model is measured by the ratio of its expressiveness and the number of parameters. 
\end{defn}

The size of a MEI model depends on the number of partitions and the partition size. Changing any of them affects the parameter count of the model, its expressiveness, and its parameter efficiency. The effect is rather complicated; when the partition size is small, the expressiveness and model size depend mainly on the number of entities and relations; however, when the partition size becomes large enough, the effects of the core tensor outweigh that of the embeddings. Interestingly, we show that the optimal partition size can be determined on any dataset with mild assumptions as stated in the following theorem.

\begin{thm} \textbf{\textit{(Optimal parameter efficiency)}} \label{thm:optimalefficiency}
	Given any MEI model that represents an arbitrary knowledge graph over  entities and  relations, it is optimal in terms of maximizing the parameter efficiency  if and only if the partition size  where  denotes a special rounding function that selects the floor or ceiling values depending on where  evaluates to a larger value.
\end{thm}

\begin{proof}
	Consider an arbitrary knowledge graph over  entities and  relations, where  fixed for this knowledge graph, and an arbitrary MEI model representing the given knowledge graph with partition size , number of partitions , and embedding size , where . The total parameter count is  There are  distinct matching matrices corresponding to the number of relations, each of which include  local interactions, so the total expressiveness of the model is  The parameter efficiency of the model as defined in Definition \ref{def:paramefficient} is . For simplicity, consider its inverse,  and assume its continuous extension by interpolation\footnote{Not to be confused with analytic continuation of analytic functions.}. Noting that  only depends on , we can take its first derivative w.r.t.  as  which evaluates to  when . The second derivative of  w.r.t.  is  which is positive everywhere.\\
	() By the derivative tests,  is the global maximum of the unimodal parameter efficiency function ; thus, the optimal partition sizes must be its floor or ceiling values, which are selected depending on  evaluations, that is, . When the embedding size , we use the largest possible partition size; thus, the optimal , as required.\\
	() By Fermat's theorem on stationary points, all local maxima occur at critical points.  is the only feasible critical point; thus,  must be the only possible optimal partition sizes, as required.
\end{proof}

Theorem \ref{thm:optimalefficiency} predicts that on WN18 and WN18RR with  entities and relations, the optimal partition size would be . On FB15K and FB15K-237 with  entities and relations, the optimal partition size would be . When  increases,  increases and is maximized at the optimal partition sizes and then starts decreasing. Thus, when the computational budget is high enough for a large embedding size , it is more parameter efficient to keep the partition size  close to the optimal value and increase the number of partitions . These predictions are empirically verified in Section \ref{sect:result}. Note that this criterion only provides a general guideline for choosing model size, but there are other detailed factors that can affect the model performance in practice, such as data sparsity, data distribution, and the ensemble boosting effect. When the dataset is very large, sparse, and unevenly distributed, it may be preferable to restrict  and try to maximize the empirical benefit of the ensemble boosting effect with a large number  of small local MEI models.

\subsection{Learning}
The learning problem in knowledge graph embedding methods can be modeled as the binary classification of every triple as existence and nonexistence. Because the number of nonexistent triples w.r.t. a knowledge graph is usually very large, we only sample a subset of them by the negative sampling technique \cite{mikolov_efficientestimationword_2013}, which replaces the  or  entities in each existent triple  with other random entities to obtain the locally related nonexistent triples  and  \cite{bordes_translatingembeddingsmodeling_2013}. The set of existent triples is called the true data , and the set of nonexistent triples is called the negative sampled data .

To construct the loss function, we define a Bernoulli distribution over each entry of the binary data tensor  to model the existence probability of each triple as . The predicted probability of the model is computed by using the standard logistic function on the matching score as . We can then learn both the embeddings and the core tensor from data by minimizing the cross-entropy loss:

where  in  and  in . 


\section{Experiments} \label{sect:experiment} 

\subsection{Experimental Settings} \label{sect:expsetting} \paragraph{Datasets}
We use four popular benchmark datasets for link prediction, as shown in Table \ref{tab:data}. WN18 \cite{bordes_translatingembeddingsmodeling_2013} and WN18RR \cite{dettmers_convolutional2dknowledge_2018} are subsets of WordNet \cite{millergeorgea._wordnetlexicaldatabase_1995}, which contains lexical relationships between words. FB15K \cite{bordes_translatingembeddingsmodeling_2013} and FB15K-237 \cite{toutanova_observedlatentfeatures_2015} are subsets of Freebase \cite{bollacker_freebasecollaborativelycreated_2008}, which contains general facts. WN18 and FB15K are more popular, whereas WN18RR and FB15K-237 are recently built and more competitive.

\begin{table}

	\caption{Datasets statistics.}
	\label{tab:data}
	\centering
	\begin{adjustbox}{max width=\columnwidth}
		\begin{tabular}{@{\extracolsep{0pt}}lrrrrr}
			\toprule
			Dataset &  &  & Train & Valid & Test\\
			\hline 
			WN18 & 40,943 & 18 & 141,442 & 5,000 & 5,000\\
			FB15K & 14,951 & 1,345 & 483,142 & 50,000 & 59,071\\ 
			WN18RR & 40,943 & 11 & 86,835 & 3,034 & 3,134\\
			FB15K-237 & 14,541 & 237 & 272,115 & 17,535 & 20,466\\ 
			\bottomrule
		\end{tabular}
	\end{adjustbox}
\end{table}

\paragraph{Evaluations}
We evaluate and analyze MEI on the link prediction task \cite{bordes_translatingembeddingsmodeling_2013}. In this task, for each true triple  in the test set, we replace  and  by every other entity to generate corrupted triples  and , respectively. The goal of the model is to rank the true triple  before the corrupted triples based on the score . We compute popular evaluation metrics including  (mean reciprocal rank, which is robust to outlier rankings) and  for  (Hits at , which is how many true triples are correctly ranked in the top ) \cite{trouillon_complexembeddingssimple_2016}. The higher  and  are, the better the model performs. To avoid false-negative error, i.e., some corrupted triples are actually existent, we follow the protocols used in other works for filtered metrics \cite{bordes_translatingembeddingsmodeling_2013}. In this protocol, all existent triples in the training, validation, and test sets are removed from the corrupted triples set before computing the rank of the true triple.

\paragraph{Baselines}
To evaluate the prediction on the optimal parameter efficiency, we compare MEI (vanilla Tucker model) and MEI. The aim is to show that the model with optimal parameter efficiency can achieve better results with even fewer parameters. We also evaluate MEI against several strong baselines including classic models such as TransE, RESCAL, DistMult, and recent state-of-the-art models such as ComplEx, SimplE, and ConvE. We also compare MEI with TorusE that uses larger embedding size; ComplEx at  that was retuned with N3 weight decay, reciprocal relation, and full softmax loss; and RotatE without the adversarial sampling technique as this technique is not subjected to a specific model.

\paragraph{Implementations}
We trained MEI using mini-batch stochastic gradient descent with Adam optimizer \cite{kingma_adammethodstochastic_2015}. We followed the 1-N scoring procedure in \cite{dettmers_convolutional2dknowledge_2018} for negative sampling of , where negative samples are reused multiple times for computation efficiency and the number of negative samples is different for each triple. The results of MEI are reproduced from the vanilla Tucker model in \cite{balazevic_tuckertensorfactorization_2019}; note that the relation embedding size  on WN18 and WN18RR only. All hyperparameters of MEI are tuned by random search \cite{bergstra_randomsearchhyperparameter_2012}, including batch size, learning rate, decay rate, batch normalization, and dropout rates, which we will publish together with the code. Note that in these experiments, we restrict them to the simplified case of one single shared-core tensor for all  partitions, as an analogy to single interaction patterns in previous specially designed models.

\subsection{Main Results} \label{sect:result} \begin{table*}[ht]


	\caption[Link prediction results on WN18 and FB15K.]{Link prediction results on WN18 and FB15K.  are reported in \cite{nickel_holographicembeddingsknowledge_2016},  are reported in \cite{trouillon_complexembeddingssimple_2016}, other results are reported in their papers. Best results are in bold, second-best results are underlined.}
	\label{tab:result}
	\centering
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{@{\extracolsep{2pt}}lcccccccc@{}}
			
			\toprule
			
			& \multicolumn{4}{c}{\textbf{WN18}} & \multicolumn{4}{c}{\textbf{FB15K}} \\
			\cmidrule(lr){2-5} \cmidrule(lr){6-9}
			& MRR & H@1 & H@3 & H@10 & MRR & H@1 & H@3 & H@10 \\ 
			\hline
			
TransE \cite{bordes_translatingembeddingsmodeling_2013}  & 0.495 & 0.113 & 0.888 & 0.943 & 0.463 & 0.297 & 0.578 & 0.749 \\


ConvE \cite{dettmers_convolutional2dknowledge_2018} & 0.943 & 0.935 & 0.946 & 0.956 & 0.657 & 0.558 & 0.723 & 0.831 \\


RESCAL \cite{nickel_threewaymodelcollective_2011}  & 0.890 & 0.842 & 0.904 & 0.928 & 0.354 & 0.235 & 0.409 & 0.587 \\
DistMult \cite{yang_embeddingentitiesrelations_2015}  &  0.822 & 0.728 & 0.914 & 0.936 & 0.654 & 0.546 & 0.733 & 0.824 \\
			ComplEx \cite{trouillon_complexembeddingssimple_2016} & 0.941 & 0.936 & 0.945 & 0.947 & 0.692 & 0.599 & 0.759 & 0.840 \\
			SimplE \cite{kazemi_simpleembeddinglink_2018} & 0.942 & 0.939 & 0.944 & 0.947 & 0.727 & 0.660 & 0.773 & 0.838  \\  
			
TorusE \cite{ebisu_toruseknowledgegraph_2018} & 0.947 & 0.943 & 0.950 & 0.954 & 0.733 & 0.674 & 0.771 & 0.832 \\
			ComplEx new tuning \cite{lerer_pytorchbiggraphlargescalegraph_2019} & -- & -- & -- & -- & 0.790 & -- & -- & 0.872 \\  

\hline
			




			MEI & \textbf{0.953} & \textbf{0.949} & \textbf{0.955} & \textbf{0.958} & \underline{0.795} & \underline{0.741} & \underline{0.833} & \underline{0.892} \\  

			MEI & \underline{0.950} & \underline{0.946} & \underline{0.952} & \underline{0.957} & \textbf{0.806} & \textbf{0.754} & \textbf{0.843} & \textbf{0.893} \\  



			\bottomrule
			
		\end{tabular}
	\end{adjustbox}
\end{table*}

\begin{table*}

	\caption[Link prediction results on WN18RR and FB15K-237.]{Link prediction results on WN18RR and FB15K-237.  are reported in \cite{ebisu_generalizedtranslationbasedembedding_2019},  are reported in \cite{dettmers_convolutional2dknowledge_2018}, other results are reported in their papers. Best results are in bold, second-best results are underlined.}
	\label{tab:result_hard}
	\centering	
	\begin{adjustbox}{max width=\textwidth}
		\begin{tabular}{@{\extracolsep{2pt}}lcccccccc@{}}
			
			\toprule
			
			& \multicolumn{4}{c}{\textbf{WN18RR}} & \multicolumn{4}{c}{\textbf{FB15K-237}} \\
			\cmidrule(lr){2-5} \cmidrule(lr){6-9}
			& MRR & H@1 & H@3 & H@10 & MRR & H@1 & H@3 & H@10 \\ 
			\hline
			
			TransE \cite{bordes_translatingembeddingsmodeling_2013}  & 0.182 & 0.027 & 0.295 & 0.444 & 0.257 & 0.174 & 0.284 & 0.420 \\


ConvE \cite{dettmers_convolutional2dknowledge_2018} & 0.43 & 0.40 & 0.44 & 0.52 & 0.325 & 0.237 & 0.356 & 0.501 \\


			DistMult \cite{yang_embeddingentitiesrelations_2015}  & 0.43 & 0.39 & 0.44 & 0.49 & 0.241 & 0.155 & 0.263 & 0.419 \\
			ComplEx \cite{trouillon_complexembeddingssimple_2016}  & 0.44 & 0.41 & 0.46 & 0.51 & 0.247 & 0.158 & 0.275 & 0.428 \\
			
TorusE \cite{ebisu_generalizedtranslationbasedembedding_2019} & 0.452 & 0.422 & 0.464 & 0.512 & 0.305 & 0.217 & 0.335 & 0.484 \\
			RotatE w/o adv \cite{sun_rotateknowledgegraph_2019} & -- & -- & -- & -- & 0.297 & 0.205 & 0.328 & 0.480 \\  

\hline
			






			MEI & \textbf{0.470} & \textbf{0.443} & \textbf{0.482} & \textbf{0.526} & \underline{0.358} & \textbf{0.266} & \underline{0.394} & \textbf{0.544} \\  



			MEI & \underline{0.458} & \underline{0.426} & \underline{0.470} & \underline{0.521} & \textbf{0.359} & \textbf{0.266} & \textbf{0.395} & \textbf{0.544} \\  

			\bottomrule
			
		\end{tabular}
	\end{adjustbox}
\end{table*}

\paragraph{Link Prediction Performance}
Tables \ref{tab:result} and \ref{tab:result_hard} show the main results. In general, MEI strongly outperforms the baselines. MEI and ConvE both aim to learn the interaction between the embedding vectors, and interestingly, the multi-partition embedding interaction used in MEI can achieve better results than the convolutional neural networks used in ConvE. MEI also outperforms the general bilinear model RESCAL and other recent state-of-the-art bilinear models DistMult, ComplEx, and SimplE, which is explained by the fact that they are special cases of MEI with specific interaction patterns, as shown in Section \ref{sect:theory}. Compared with TorusE, the results show that an expressive interaction mechanism can help a smaller model outperform a much larger model. There are some recent techniques that help to improve the performance of old models, but we show that MEI can still outperform retuned ComplEx and RotatE reported with comparable settings. Moreover, note that MEI is highly general and potentially preferable for sophisticated datasets.

\paragraph{Optimal Parameter Efficiency}
Empirical results agree very well with the predictions of Theorem \ref{thm:optimalefficiency} about the optimal parameter efficiency. On WN18 and WN18RR, MEI consistently outperforms MEI using fewer parameters. On FB15K and FB15K-237, the model sizes are reversed due to different numbers of entities and relations, with MEI having two times more parameters than MEI. On FB15K, as predicted, MEI consistently outperforms MEI. On FB15K-237, MEI outperforms MEI most of the time, although not by a large margin, but uses only half the number of parameters. These results are particularly interesting because they suggest that when the embedding size  is large enough, MEI with  can both scale to larger embedding sizes and have better results than MEI with  partition.

\subsection{Analyses} \paragraph{Parameter Scale Comparison}
Table \ref{tab:paramscale} compares the performance of MEI with that of ConvE \cite{dettmers_convolutional2dknowledge_2018}, which aims to learn interaction mechanisms by a neural network, at different parameter scales. The results show that MEI achieves better results than ConvE at the same parameter count. Moreover, the small MEI model at 0.95M parameters remarkably outperforms the other model at 1.89M parameters. These results suggest that MEI is an effective framework to utilize the parameters of the model and to learn the interaction mechanisms automatically for knowledge graph embedding.

\begin{table}\caption{Parameter scaling on FB15K-237.}
	\label{tab:paramscale}
	\centering	
	\begin{adjustbox}{max width=\columnwidth}
		\begin{tabular}{@{\extracolsep{0pt}}lcccccc}
			\toprule
			& Param. & Emb. & & \multicolumn{3}{c}{{H@}}   \\
			Model & count & size & MRR & 1 & 3 & 10 \\
			\hline
ConvE & 1.89M & 96 & .32 & .23 & .35 & .49  \\
			ConvE & 0.95M & 54 & .30 & .22 & .33 & .46  \\
\hline
MEI & 1.89M & 340 & .34 & .25 & .38 & .53  \\ MEI & 0.95M & 320 & .33 & .24 & .36 & .51  \\ \bottomrule
		\end{tabular}
	\end{adjustbox}
\end{table}

\paragraph{Parameter Trade-off Analysis}
There are two kinds of parameters in the MEI model, the embeddings and the core tensors. Theorem \ref{thm:optimalefficiency} provides a guideline to trade-offs between them. For example, on FB15K-237, the parameter efficiency increases when the partition size increases up to . However, there are other factors affecting this trade-off, such as the ensemble boosting effect that favors larger  and smaller . We argue that due to this effect, MEI with  has an empirical advantage compared with MEI with . To evaluate this claim, we analyze the performance of MEI models with approximately the same parameter counts but different core-tensor sizes on FB15K-237. To disambiguate the effects of larger core tensor, we made sure that the models with larger core tensors would have smaller parameter counts. Table \ref{tab:trade-off} shows that the models with larger core tensor consistently achieve better results with even fewer total parameters, agreeing very well with Theorem \ref{thm:optimalefficiency}. Interestingly, MEI with  achieves competitive results compared with MEI with , which suggest that the ensemble boosting effect benefits MEI with , as we argued.

\begin{table}\caption{Parameter trade-off analysis on FB15K-237.}
	\label{tab:trade-off}
	\centering	
	\begin{adjustbox}{max width=\columnwidth}
		\begin{tabular}{@{\extracolsep{0pt}}ccccccc}
			\toprule
			Emb. & Param. &  & & \multicolumn{3}{c}{{H@}}   \\
			size & count & size & MRR & 1 & 3 & 10 \\
			\hline
			1211 & 1.95M & 1K & 0.335 & 0.247 & 0.367 & 0.514  \\
			\,\,\,621 & 1.87M & 9K & 0.339 & 0.249 & 0.371 & 0.518  \\
			\,\,\,340 & 1.84M & 64K & \textbf{0.344} & 0.253 & \textbf{0.378} & \textbf{0.527}  \\
			\,\,\,182 & 1.76M & 551K & \textbf{0.344} & \textbf{0.255} & \textbf{0.378} & 0.522  \\
			\bottomrule
		\end{tabular}
	\end{adjustbox}
\end{table}

\section{Conclusion and Future Work} \label{sect:conclusion} In this work, we proposed MEI, the multi-partition embedding interaction model with block term format, to systematically control the trade-off between expressiveness and computational cost, to learn the interaction mechanisms from data automatically, and to achieve state-of-the-art performance on the link prediction task. In addition, we theoretically studied the parameter efficiency problem and derived a simple criterion for optimal parameter trade-off. We discussed several interpretations and insights of MEI as a novel general design pattern for knowledge graph embedding, and we applied the framework of MEI to present a new generalized explanation for several specially designed interaction mechanisms in previous models.

In future work, we plan to conduct more experiments with MEI, especially regarding the ensemble boosting effect and the meta-dimensional transforming--matching framework. Other interesting directions include more in-depth studies of the embedding internal structure and the nature of multi-partition embedding interaction, especially with applications in other domains such as natural language processing, computer vision, and recommender systems.


\ack
This work was supported by the Cross-ministerial Strategic Innovation Promotion Program (SIP) Second Phase, ``Big-data and AI-enabled Cyberspace Technologies'' by the New Energy and Industrial Technology Development Organization (NEDO).


\begin{thebibliography}{10}

\bibitem{balazevic_tuckertensorfactorization_2019}
Ivana Bala{\v z}evi{\'c}, Carl Allen, and Timothy~M. Hospedales, `{{TuckER}}:
  {{Tensor Factorization}} for {{Knowledge Graph Completion}}', in {\em
  Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural
  Language Processing}}}, pp. 5185--5194, (2019).

\bibitem{bergstra_randomsearchhyperparameter_2012}
James Bergstra and Yoshua Bengio, `Random {{Search}} for {{Hyper-Parameter
  Optimization}}', {\em Journal of Machine Learning Research}, {\bf 13},
  281--305, (2012).

\bibitem{bollacker_freebasecollaborativelycreated_2008}
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor,
  `Freebase: A {{Collaboratively Created Graph Database}} for {{Structuring
  Human Knowledge}}', in {\em Proceedings of the 2008 {{ACM SIGMOD
  International Conference}} on {{Management}} of {{Data}}}, pp. 1247--1250,
  (2008).

\bibitem{bordes_translatingembeddingsmodeling_2013}
Antoine Bordes, Nicolas Usunier, Alberto {Garcia-Duran}, Jason Weston, and
  Oksana Yakhnenko, `Translating {{Embeddings}} for {{Modeling Multi-Relational
  Data}}', in {\em Advances in {{Neural Information Processing Systems}}}, pp.
  2787--2795, (2013).

\bibitem{cai_grouprepresentationtheory_2019}
Chen Cai, `Group {{Representation Theory}} for {{Knowledge Graph Embedding}}',
  {\em arXiv:1909.05100 [cs, math]}, (2019).

\bibitem{delathauwer_decompositionshigherordertensor_2008a}
Lieven De~Lathauwer, `Decompositions of a {{Higher-Order Tensor}} in {{Block
  Terms}}\textemdash{{Part II}}: {{Definitions}} and {{Uniqueness}}', {\em SIAM
  Journal on Matrix Analysis and Applications}, {\bf 30}(3),  1033--1066,
  (2008).

\bibitem{dettmers_convolutional2dknowledge_2018}
Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel,
  `Convolutional {{2D Knowledge Graph Embeddings}}', in {\em Proceedings of the
  32nd {{AAAI Conference}} on {{Artificial Intelligence}}}, pp. 1811--1818,
  (2018).

\bibitem{ebisu_toruseknowledgegraph_2018}
Takuma Ebisu and Ryutaro Ichise, `{{TorusE}}: {{Knowledge Graph Embedding}} on
  a {{Lie Group}}', in {\em Proceedings of the 32nd {{AAAI Conference}} on
  {{Artificial Intelligence}}}, pp. 1819--1826, (2018).

\bibitem{ebisu_generalizedtranslationbasedembedding_2019}
Takuma Ebisu and Ryutaro Ichise, `Generalized {{Translation-based Embedding}}
  of {{Knowledge Graph}}', {\em IEEE Transactions on Knowledge and Data
  Engineering}, {\bf 32}(5),  941--951, (2019).

\bibitem{hitchcock_expressiontensorpolyadic_1927}
Frank~L. Hitchcock, `The {{Expression}} of a {{Tensor}} or a {{Polyadic}} as a
  {{Sum}} of {{Products}}', {\em Journal of Mathematics and Physics}, {\bf
  6}(1-4),  164--189, (1927).

\bibitem{ioffe_batchnormalizationaccelerating_2015}
Sergey Ioffe and Christian Szegedy, `Batch {{Normalization}}: {{Accelerating
  Deep Network Training}} by {{Reducing Internal Covariate Shift}}', in {\em
  International {{Conference}} on {{Machine Learning}}}, pp. 448--456, (2015).

\bibitem{kazemi_simpleembeddinglink_2018}
Seyed~Mehran Kazemi and David Poole, `{{SimplE Embedding}} for {{Link
  Prediction}} in {{Knowledge Graphs}}', in {\em Advances in {{Neural
  Information Processing Systems}}}, pp. 4289--4300, (2018).

\bibitem{kingma_adammethodstochastic_2015}
Diederik~P. Kingma and Jimmy Ba, `Adam: {{A Method}} for {{Stochastic
  Optimization}}', in {\em International {{Conference}} on {{Learning
  Representations}}}, p.~15, (2015).

\bibitem{kolda_tensordecompositionsapplications_2009}
Tamara~G. Kolda and Brett~W. Bader, `Tensor {{Decompositions}} and
  {{Applications}}', {\em SIAM Review}, {\bf 51}(3),  455--500, (2009).

\bibitem{lacroix_canonicaltensordecomposition_2018}
Timoth{\'e}e Lacroix, Nicolas Usunier, and Guillaume Obozinski, `Canonical
  {{Tensor Decomposition}} for {{Knowledge Base Completion}}', in {\em
  International {{Conference}} on {{Machine Learning}}}, pp. 2863--2872,
  (2018).

\bibitem{lerer_pytorchbiggraphlargescalegraph_2019}
Adam Lerer, Ledell Wu, Jiajun Shen, Timothee Lacroix, Luca Wehrstedt, Abhijit
  Bose, and Alex Peysakhovich, `{{PyTorch-BigGraph}}: {{A Large-scale Graph
  Embedding System}}', in {\em Proceedings of the 2nd {{SysML Conference}}},
  p.~12, (2019).

\bibitem{mikolov_efficientestimationword_2013}
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean, `Efficient
  {{Estimation}} of {{Word Representations}} in {{Vector Space}}', in {\em
  Workshop {{Proceedings}} of the 2013 {{International Conference}} on
  {{Learning Representations}}}, p.~12, (2013).

\bibitem{millergeorgea._wordnetlexicaldatabase_1995}
{Miller, George A.}, `{{WordNet}}: A {{Lexical Database}} for {{English}}',
  {\em Communications of the ACM}, {\bf 38}(11),  39--41, (1995).

\bibitem{nickel_holographicembeddingsknowledge_2016}
Maximilian Nickel, Lorenzo Rosasco, and Tomaso Poggio, `Holographic
  {{Embeddings}} of {{Knowledge Graphs}}', in {\em Proceedings of the 30th
  {{AAAI Conference}} on {{Artificial Intelligence}}}, pp. 1955--1961, (2016).

\bibitem{nickel_threewaymodelcollective_2011}
Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel, `A {{Three-Way Model}}
  for {{Collective Learning}} on {{Multi-Relational Data}}', in {\em
  International {{Conference}} on {{Machine Learning}}}, pp. 809--816, (2011).

\bibitem{ruffinelli_youcanteach_2020}
Daniel Ruffinelli, Samuel Broscheit, and Rainer Gemulla, `You {{CAN Teach}} an
  {{Old Dog New Tricks}}! {{On Training Knowledge Graph Embeddings}}', in {\em
  International {{Conference}} on {{Learning Representations}}}, p.~20, (2020).

\bibitem{srivastava_dropoutsimpleway_2014}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov, `Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from
  {{Overfitting}}', {\em The Journal of Machine Learning Research}, {\bf
  15}(1),  1929--1958, (2014).

\bibitem{sun_rotateknowledgegraph_2019}
Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang, `{{RotatE}}:
  {{Knowledge Graph Embedding}} by {{Relational Rotation}} in {{Complex
  Space}}', in {\em International {{Conference}} on {{Learning
  Representations}}}, p.~18, (2019).

\bibitem{toutanova_observedlatentfeatures_2015}
Kristina Toutanova and Danqi Chen, `Observed versus latent features for
  knowledge base and text inference', in {\em Proceedings of the 3rd
  {{Workshop}} on {{Continuous Vector Space Models}} and Their
  {{Compositionality}}}, pp. 57--66, (2015).

\bibitem{tran_analyzingknowledgegraph_2019}
Hung-Nghiep Tran and Atsuhiro Takasu, `Analyzing {{Knowledge Graph Embedding
  Methods}} from a {{Multi-Embedding Interaction Perspective}}', in {\em
  Proceedings of the {{Data Science}} for {{Industry}} 4.0 {{Workshop}} at
  {{EDBT}}/{{ICDT}}}, p.~7, (2019).

\bibitem{tran_exploringscholarlydata_2019}
Hung-Nghiep Tran and Atsuhiro Takasu, `Exploring {{Scholarly Data}} by
  {{Semantic Query}} on {{Knowledge Graph Embedding Space}}', in {\em
  Proceedings of the 23rd {{International Conference}} on {{Theory}} and
  {{Practice}} of {{Digital Libraries}}}, pp. 154--162, (2019).

\bibitem{tran_multipartitionembeddinginteraction_2020}
Hung-Nghiep Tran and Atsuhiro Takasu, `Multi-{{Partition Embedding
  Interaction}} with {{Block Term Format}} for {{Knowledge Graph Completion}}',
  in {\em Proceedings of the {{European Conference}} on {{Artificial
  Intelligence}}}, pp. 833--840, (2020).

\bibitem{tran_meimmultipartitionembedding_2022}
Hung-Nghiep Tran and Atsuhiro Takasu, `{{MEIM}}: {{Multi-partition Embedding
  Interaction Beyond Block Term Format}} for {{Efficient}} and {{Expressive
  Link Prediction}}', in {\em Proceedings of the {{Thirty-First International
  Joint Conference}} on {{Artificial Intelligence}}}, pp. 2262--2269, (2022).

\bibitem{trouillon_complexembeddingssimple_2016}
Theo Trouillon, Johannes Welbl, Sebastian Riedel, {Eric Gaussier}, and
  {Guillaume Bouchard}, `Complex {{Embeddings}} for {{Simple Link
  Prediction}}', in {\em International {{Conference}} on {{Machine Learning}}},
  pp. 2071--2080, (2016).

\bibitem{tucker_mathematicalnotesthreemode_1966}
Ledyard~R Tucker, `Some {{Mathematical Notes}} on {{Three-Mode Factor
  Analysis}}', {\em Psychometrika}, {\bf 31}(3),  279--311, (1966).

\bibitem{vrandecic_wikidatafreecollaborative_2014}
Denny Vrande{\v c}i{\'c} and Markus Kr{\"o}tzsch, `Wikidata: A {{Free
  Collaborative Knowledgebase}}', {\em Communications of the ACM}, {\bf
  57}(10),  78--85, (2014).

\bibitem{wang_knowledgegraphembedding_2017}
Q.~Wang, Z.~Mao, B.~Wang, and L.~Guo, `Knowledge {{Graph Embedding}}: {{A
  Survey}} of {{Approaches}} and {{Applications}}', {\em IEEE Transactions on
  Knowledge and Data Engineering}, {\bf 29}(12),  2724--2743, (2017).

\bibitem{yang_embeddingentitiesrelations_2015}
Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li~Deng, `Embedding
  {{Entities}} and {{Relations}} for {{Learning}} and {{Inference}} in
  {{Knowledge Bases}}', in {\em International {{Conference}} on {{Learning
  Representations}}}, p.~12, (2015).

\bibitem{zhang_quaternionknowledgegraph_2019}
Shuai Zhang, Yi~Tay, Lina Yao, and Qi~Liu, `Quaternion {{Knowledge Graph
  Embedding}}', in {\em Advances in {{Neural Information Processing Systems}}},
  pp. 2735--2745, (2019).

\end{thebibliography}
 

\clearpage
\appendix

\section{Extra Experiments}
In this section, we present extra results obtained with well-tuned hyperparameters and recent training techniques in our new source code \url{https://github.com/tranhungnghiep/MEI-KGE}.

\subsection{Results of Well-tuned Small Models}
When data is large, the embedding size needs to increase to fit the data. However, real-world knowledge graphs are very large with billions of entities, so even the largest practical embedding sizes are relatively small compared to the data sizes. To simulate and study such scenarios, we examine the performance of small models with embedding size  on four benchmark datasets WN18, FB15K, WN18RR, FB15K-237.

We compare three small models, the previous state-of-the-art small model ComplEx \cite{lacroix_canonicaltensordecomposition_2018}, the small model MEI \cite{tran_multipartitionembeddinginteraction_2020}, and the improved model MEIM \cite{tran_meimmultipartitionembedding_2022}. These models were well-tuned with recent training techniques including softmax cross-entropy loss \cite{dettmers_convolutional2dknowledge_2018} \cite{lacroix_canonicaltensordecomposition_2018} \cite{ruffinelli_youcanteach_2020}. We also report RotatE results as a reference of previous state-of-the-art large model \cite{sun_rotateknowledgegraph_2019}.

Table \ref{tab:result_tuned_small} shows that MEI strongly outperforms the previous state-of-the-art small model ComplEx on all datasets. Moreover, the improved MEIM even outperforms the much larger RotatE and RotatE models. These results supports our theoretical analysis on the advantage of multi-partition embedding interaction, and agrees with recent group-theoretic analyses \cite{cai_grouprepresentationtheory_2019} on the limitations of RotatE due to partition size , which our models systematically address. In summary, the results demonstrate our models' strong point of being both efficient and expressive.

\begin{table}\begin{center}
		\caption{Results of small MEI \cite{tran_multipartitionembeddinginteraction_2020} and MEIM \cite{tran_meimmultipartitionembedding_2022} models well-tuned with recent training techniques. ComplEx represents a previous state-of-the-art small model, tuned by \cite{lacroix_canonicaltensordecomposition_2018} and reported on their github page. RotatE represents a previous state-of-the-art large model \cite{sun_rotateknowledgegraph_2019}, as a reference. Best results of small models are in bold and second best results are underlined. Best results of large models are in bold and italicized.}
		\label{tab:result_tuned_small}
		
		\begin{adjustbox}{max width=\columnwidth}
			\begin{tabular}{@{\extracolsep{-3.5pt}}llrcccc}
				\toprule
				& & Param. & & \multicolumn{3}{c}{H@} \\
				& & count & MRR & 1 & 3 & 10 \\
\midrule
				
				& RotatE & 40.961M & 0.949 & 0.944 & 0.952 & 0.959 \\ \cmidrule{3-7}
				\multirow{1}{*}{WN18} & ComplEx & 4.098M & \underline{0.950} & 0.940 & 0.950 & 0.950 \\
				
				& MEI & 4.099M & \underline{0.950} & \underline{0.945} & \textbf{0.953} & \underline{0.957} \\ & MEIM & 4.108M & \textbf{0.951} & \textbf{0.946} & \textbf{0.953} & \textbf{0.960} \\ \midrule
				\midrule
				
				& RotatE & 32.592M & 0.797 & 0.746 & \textit{\textbf{0.830}} & \textit{\textbf{0.884}} \\ \cmidrule{3-7}
				\multirow{1}{*}{FB15K} & ComplEx & 1.630M & 0.780 & 0.730 & 0.810 & 0.860 \\
				
				& MEI & 1.631M & \underline{0.790} & \underline{0.746} & \underline{0.817} & \underline{0.870} \\ & MEIM & 1.640M & \textbf{0.800} & \textbf{0.757} & \textbf{0.823} & \textbf{0.878} \\ \midrule
				\midrule
				
				
				& RotatE & 40.954M & 0.476 & 0.428 & 0.492 & \textit{\textbf{0.571}} \\ \cmidrule{3-7}
				\multirow{1}{*}{WN18RR} & ComplEx & 4.097M & 0.460 & 0.430 & 0.470 & 0.520 \\
				
				& MEI & 4.098M & \underline{0.468} & \underline{0.434} & \underline{0.482} & \underline{0.531} \\ & MEIM & 4.107M & \textbf{0.481} & \textbf{0.446} & \textbf{0.494} & \textbf{0.550} \\ \midrule
				\midrule
				
				
				& RotatE & 29.556M & 0.338 & 0.241 & 0.375 & \textit{\textbf{0.533}} \\ \cmidrule{3-7}
				\multirow{1}{*}{FB15K-237} & ComplEx & 1.502M & 0.340 & 0.250 & 0.370 & 0.520 \\
				
				& MEI & 1.503M & \underline{0.347} & \underline{0.256} & \underline{0.380} & \underline{0.531} \\ & MEIM & 1.512M & \textbf{0.350} & \textbf{0.258} & \textbf{0.385} & \textbf{0.533} \\ 

				\bottomrule
			\end{tabular}
		\end{adjustbox}
	\end{center}
\end{table}

\vfill\break  

\subsection{Results of Well-tuned Base Models}
The previous results of MEI was obtained using old training techniques such as binary cross-entropy loss and under-tuned hyperparameters. To see true performance of MEI in these extra experiments, we use recent training techniques including softmax cross-entropy loss, larger batch sizes, and well-tuned hyperparameters as presented in our published source code. Detailed analysis on the settings and hyperparameters' effects will be published in the future. 

Table \ref{tab:result_tuned} shows the results on three standard benchmark datasets WN18RR, FB15K-237, and YAGO3-10. We see can MEI achieves very good results with well-tuned settings and hyperparameters. This demonstrates the high quality and performance of MEI.

\begin{table}\begin{center}
		\caption{Results of MEI models well-tuned with recent training techniques.}
		\label{tab:result_tuned}
		
		\begin{adjustbox}{max width=\columnwidth}
			\begin{tabular}{@{\extracolsep{-2pt}}llrcccc}
				\toprule
				& & Param. & & \multicolumn{3}{c}{H@} \\
				& & count & MRR & 1 & 3 & 10 \\
\midrule
				
				WN18RR & MEI & 13.3M & 0.483 & 0.447 & 0.497 & 0.553 \\ 

				FB15K-237 & MEI & 5.4M & 0.364 & 0.270 & 0.398 & 0.550 \\ 

				YAGO3-10 & MEI & 62.6M & 0.578 & 0.505 & 0.622 & 0.710 \\ 

				\bottomrule
			\end{tabular}
		\end{adjustbox}
	\end{center}
\end{table}


\end{document}
