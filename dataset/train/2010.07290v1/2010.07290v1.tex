\documentclass{article}





\usepackage[preprint]{neurips_2020}





\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{graphicx}
\usepackage{color}
\newcommand{\todo}[1]{{\color{red}TODO: #1}}

\title{XPDNet for MRI Reconstruction: an Application to the fastMRI 2020 Brain Challenge}



\author{Zaccharie Ramzi \\
  CEA/NeuroSpin\\
  CEA/Cosmostat\\
  Inria/Parietal\\
  \texttt{zaccharie.ramzi@inria.fr} \\
\And
   Philippe Ciuciu \\
   CEA/NeuroSpin\\
   Inria/Parietal\\
   \AND
   Jean-Luc Starck \\
   CEA/Cosmostat\\
}
\hypersetup{draft}
\begin{document}

\maketitle

\begin{abstract}
  We present a modular cross-domain neural network the \emph{XPDNet} and its application to the MRI reconstruction task. This approach consists in unrolling the PDHG algorithm as well as learning the acceleration scheme between steps. We also adopt state-of-the-art techniques specific to Deep Learning for MRI reconstruction. At the time of writing, this approach is the best performer in PSNR on the fastMRI leaderboards for both knee and brain at acceleration factor 4.
\end{abstract}

\section{MRI reconstruction context}

Magnetic Resonance Imaging (MRI) is a modality used to probe soft tissues.
The acceleration of the examination time can be done using Compressed Sensing (CS)~\citep{Lustig2007} and Parallel Imaging (PI).
However, to have access the underlying anatomical object , one must solve the following idealized inverse problem:

where  is a mask indicating which points in the Fourier space (also called k-space) are sampled,  are the sensitivity maps for each coil,  is the 2D Fourier transform, and  are the coil measurements in the k-space.

This problem is typically solved with optimization algorithms that introduce a regularisation term to solve its indeterminacy.

One point to note, is that  is not generally given (although it can be sometimes roughly pre-acquired). 
In this particular instance, it is to be estimated from the data.
A classical technique is for example EsPIRIT~\citep{Uecker2014ESPIRiT-AnAccess}.

\section{Cross-domain networks}
\subsection{General}
The general intuition behind cross-domain network is that we are going to alternate the correction between the image space and the measurements space.
The key tool for that is the unrolling of optimisation algorithms introduced in~\citep{Gregor2010}.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{Figures/cross-domain-networks-general-tikz.png}
\caption{
\label{fig:cd-net} 
General cross-domain networks architecture. Skip and residual connection are omitted for the sake of clarity.  are the under-sampled measurements, in our case the k-space measurements,  is the under-sampling scheme,  is the measurement operator, in our case the Fourier Transform, and  is the recovered solution.
}
\end{figure}

An illustration of what cross-domain networks generally look like is provided in Figure~\ref{fig:cd-net}.

\subsection{Implementation}

\paragraph{General backbone}The \emph{XPDNet} is a particular instance of cross-domain networks.
It is inspired by the PDNet introduced in~\citep{Adler2018} by unrolling the PDHG algorithm~\citep{Chambolle2011} and applied to MRI in~\citep{Ramzi2020BenchmarkingDatasets}.
In particular, a main feature of the PDNet is its ability to learn the acceleration pattern through the unrolled optimisation steps with a buffer, here of size 5.

\paragraph{Image correction network}The plain Convolutional Neural Network (CNN) is replaced by an Multi-scale Wavelet CNN (MWCNN)~\citep{Liu}, but the code\footnote{\url{https://github.com/zaccharieramzi/fastmri-reproducible-benchmark}} allows for it to be any denoiser, hence the X.
This network was chosen after benchmarking different networks introduced as denoisers and listed in~\citep{Tian2020DeepOverview}, embedded in the \emph{XPDNet} but for single-coil reconstruction.
Following the insights of~\citep{Ramzi2020BenchmarkingDatasets} we chose to use a smaller image correction network than that presented in the original paper~\citep{Liu}, in order to afford more unrolled iterations in memory.
Additionally, because we use a small batch size, we removed batch normalization layers from the network.

\paragraph{k-space}In this challenge, since the data is multicoil, we do not use a k-space correction network which would be very heavy in memory use.
However, following the idea of~\citep{Sriram2020End-to-EndReconstruction}, we introduce a refinement network for , initially estimated from the lower frequencies of the retrospectively under-sampled coil measurements.
This sensitivity maps refiner is chosen to be a simple U-net~\citep{Ronneberger} like in~\citep{Sriram2020End-to-EndReconstruction}.

We therefore have 25 unrolled iterations, an MWCNN that has twice as less filters in each scale, a sensitivity maps refiner smaller than that of~\citep{Sriram2020End-to-EndReconstruction} and no k-space correction network.

\subsection{Training details}

The loss used for the network training was a compound loss introduced by~\citep{Pezzotti2020AnChallenge}, weighted sum of an  loss and the Multi-scale SSIM (MSSIM)~\citep{Wang2004}.

The optimizer was the Rectified ADAM (RAdam)~\citep{Liu2020OnBeyond} with default parameters\footnote{\url{https://www.tensorflow.org/addons/api_docs/python/tfa/optimizers/RectifiedAdam}}.

The training was carried for 100 epochs, each epoch consisting of a pass through all the volumes, with one slice selected at random at each epoch and scaled by a factor of  as in~\citep{Ramzi2020BenchmarkingDatasets}.
The batch size is 1.
Masks offset for the equidistant masks\footnote{To see more about the exact nature of the masks: \url{https://github.com/facebookresearch/fastMRI/issues/54}} are sampled on-the-fly.
Training is carried out separately for acceleration factors 4 and 8.
The networks are then fine-tuned for each contrast for 10 epochs.

With one V100 GPU, the training lasted 1 week for each acceleration contrast.


\section{Conclusion and Discussion}
We managed to gather insights from many different works on computer vision and MRI reconstruction to build a module approach.
Currently this approach outperforms all others in PSNR and NMSE for both the multicoil knee and brain tracks at the acceleration factor of 4.
Furthermore, the modularity of the current architecture allows to use the newest denoising architectures when they become available.

However, the fact that this approach fails to outperform the others on the SSIM metric is to be investigated in further work.


\begin{ack}
We acknowledge the financial support of the Cross-Disciplinary Program on Numerical Simulation of CEA, the French Alternative Energies and Atomic Energy Commission for the project entitled SILICOSMIC.

We also acknowledge the French Institute of development and ressources in scientific computing (IDRIS) for their AI program allowing us to use the Jean Zay supercomputers' GPU partitions.
\end{ack}



\medskip



\bibliography{references.bib}
\bibliographystyle{unsrtnat}

\end{document}