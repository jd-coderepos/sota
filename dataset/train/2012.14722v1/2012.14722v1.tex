\section{Experiments}\label{section-5}
This section presents the experimental results on real-world datasets and detailed analysis.

\subsection{Description of Datasets}
We conduct experiments on three real-world datesets.
\begin{itemize}
	\item \textbf{ACM-3}:
    Following \citet{DBLP:conf/www/WangJSWYCY19}, we extract a subset of ACM from AMiner \footnote{https://www.aminer.cn/citation} \cite{DBLP:conf/kdd/TangZYLZS08}, which contains papers published in three areas: Data Mining (KDD, ICDM), Database (VLDB, SIGMOD) and Wireless Communication (SIGCOMM, MobiCOMM). Finally we construct a heterogeneous graph containing papers (P), authors (A) and terms (T). 
    
    \item \textbf{ACM-5}: 
    We also extract a larger subset of ACM from AMiner, which includes papers published in five areas: Data Mining (KDD, ICDM, WSDM, CIKM), Database (VLDB, ICDE), Artificial Intelligence (AAAI, IJCAI), Computer Vision (CVPR, ECCV) and Natural Language Processing (ACL, EMNLP, NAACL).
    
    \item \textbf{IMDB} \footnote{https://data.world/data-society/imdb-5000-movie-dataset}:
    We extract a subset of IMDB and consruct a heterogeneous graph containing movies (M), directors (D) and actors (A). The movies are divided into three classes: Action, Comedy, Drama.
\end{itemize}

For ACM-3 and ACM-5, we use TF-IDF \cite{ramos2003using} 
to extract keywords of the abstract and title in papers. Paper attributes are the bag-of-words representation of abstracts. Author attributes are the average representation of their published papers. Term attributes are represented as the one-hot encoding of the title keywords. For IMDB, movie attributes are the bag-of-words representation of plot keywords. Director/actor attributes are the average representation of their directing/acting movies.

Details of the datasets are summarized in \tabref{tab:dataset}.
\begin{table}[!htbp]
\centering
\scriptsize
\caption{Statistics of the datasets.}
\label{tab:dataset}
\setlength{\tabcolsep}{1.5mm}{
\begin{tabular}{|c|c|c|c|c|}
\hline
Dataset & Node                                                                                                 & Relation                                                                                  & Attribute                                                             & Data Split                                                                             \\ \hline
ACM-3   & \begin{tabular}[c]{@{}c@{}}\# Paper (P): 6,782\\ \# Author (A): 1,637\\ \# Term (T): 200\end{tabular}      & \begin{tabular}[c]{@{}c@{}}\# P-A: 13,498\\ \# P-T: 18,974\\ \# P-P: 14,925\end{tabular} & \begin{tabular}[c]{@{}c@{}}P:2,000\\ A:2,000\\ T:200\end{tabular}   & \begin{tabular}[c]{@{}c@{}}Train: 1,358\\ Validation: 678\\ Test: 4,746\end{tabular}   \\ \hline
ACM-5   & \begin{tabular}[c]{@{}c@{}}\# Paper (P): 13,328\\ \# Author (A): 2,975\\ \# Term (T): 200\end{tabular}     & \begin{tabular}[c]{@{}c@{}}\# P-A: 23,662\\ \# P-T: 36,186\\ \# P-P: 22,632\end{tabular} & \begin{tabular}[c]{@{}c@{}}P:2,000\\ A:2,000\\ T:200\end{tabular}   & \begin{tabular}[c]{@{}c@{}}Train: 2,668\\ Validation: 1,331\\ Test: 9,329\end{tabular} \\ \hline
IMDB    & \begin{tabular}[c]{@{}c@{}}\# Movie (M): 4,076\\ \# Director (D): 1,999\\ \# Actor (A): 5,069\end{tabular} & \begin{tabular}[c]{@{}c@{}}\# M-D: 4,076\\ \# M-A: 12,228\end{tabular}                  & \begin{tabular}[c]{@{}c@{}}M:1,537\\ D:1,537\\ A:1,537\end{tabular} & \begin{tabular}[c]{@{}c@{}}Train: 817\\ Validation: 407\\ Test: 2,852\end{tabular}     \\ \hline
\end{tabular}
}
\end{table}

\begin{table*}[t]
\centering
\caption{Experimental results on the node classification task.}
\label{tab:classification_results}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Data}   & \multirow{2}{*}{Metrics}  & \multirow{2}{*}{Training} & \multirow{2}{*}{MLP} & \multirow{2}{*}{GCN} & \multirow{2}{*}{GAT} & \multirow{2}{*}{RGCN} & \multirow{2}{*}{HAN} & \multirow{2}{*}{HetGNN} & \multirow{2}{*}{HGT} & \multirow{2}{*}{HGConv} \\
                        &                           &                             &                      &                      &                      &                       &                      &                         &                      &                       \\ \hline
\multirow{10}{*}{ACM-3} & \multirow{5}{*}{Macro-F1} & 20\%                        & 0.6973               & 0.8955               & 0.8852               & 0.8981                & 0.8991               & 0.6727                  & 0.8965               & \textbf{0.9150}       \\  
                        &                           & 40\%                        & 0.7740               & 0.9012               & 0.8993               & 0.9191                & 0.9175               & 0.7736                  & 0.9188               & \textbf{0.9255}       \\  
                        &                           & 60\%                        & 0.8013               & 0.9032               & 0.9053               & 0.9262                & 0.9237               & 0.8060                  & 0.9264               & \textbf{0.9286}       \\  
                        &                           & 80\%                        & 0.8249               & 0.9068               & 0.9063               & 0.9267                & 0.9268               & 0.8242                  & \textbf{0.9329}      & 0.9306                \\  
                        &                           & 100\%                       & 0.8330               & 0.9079               & 0.9058               & 0.9299                & 0.9240               & 0.8342                  & \textbf{0.9343}      & 0.9320                \\ \cline{2-11} 
                        & \multirow{5}{*}{Micro-F1} & 20\%                        & 0.6943               & 0.8869               & 0.8754               & 0.8893                & 0.8906               & 0.6710                  & 0.8885               & \textbf{0.9089}       \\  
                        &                           & 40\%                        & 0.7710               & 0.8923               & 0.8903               & 0.9124                & 0.9103               & 0.7709                  & 0.9117               & \textbf{0.9194}       \\  
                        &                           & 60\%                        & 0.7966               & 0.8948               & 0.8968               & 0.9201                & 0.9172               & 0.8016                  & 0.9203               & \textbf{0.9221}       \\  
                        &                           & 80\%                        & 0.8205               & 0.8989               & 0.8981               & 0.9202                & 0.9205               & 0.8190                  & \textbf{0.9268}      & 0.9241                \\  
                        &                           & 100\%                       & 0.8277               & 0.9000               & 0.8979               & 0.9238                & 0.9176               & 0.8282                  & \textbf{0.9284}      & 0.9256                \\ \hline
\multirow{10}{*}{ACM-5} & \multirow{5}{*}{Macro-F1} & 20\%                        & 0.6156               & 0.8221               & 0.8253               & 0.8148                & 0.8191               & 0.6022                  & 0.8100               & \textbf{0.8270}       \\  
                        &                           & 40\%                        & 0.6585               & 0.8317               & 0.8367               & 0.8368                & 0.8404               & 0.6476                  & 0.8428               & \textbf{0.8478}       \\  
                        &                           & 60\%                        & 0.7252               & 0.8440               & 0.8441               & 0.8630                & 0.8526               & 0.7133                  & 0.8573               & \textbf{0.8701}       \\  
                        &                           & 80\%                        & 0.7503               & 0.8448               & 0.8459               & 0.8699                & 0.8610               & 0.7445                  & 0.8692               & \textbf{0.8766}       \\  
                        &                           & 100\%                       & 0.7594               & 0.8492               & 0.8466               & 0.8721                & 0.8617               & 0.7565                  & 0.8715               & \textbf{0.8792}       \\ \cline{2-11} 
                        & \multirow{5}{*}{Micro-F1} & 20\%                        & 0.6469               & 0.8364               & 0.8388               & 0.8333                & 0.8334               & 0.6420                  & 0.8286               & \textbf{0.8428}       \\  
                        &                           & 40\%                        & 0.6887               & 0.8433               & 0.8475               & 0.8501                & 0.8525               & 0.6872                  & 0.8573               & \textbf{0.8616}       \\  
                        &                           & 60\%                        & 0.7354               & 0.8545               & 0.8544               & 0.8722                & 0.8626               & 0.7248                  & 0.8668               & \textbf{0.8794}       \\  
                        &                           & 80\%                        & 0.7642               & 0.8554               & 0.8562               & 0.8809                & 0.8715               & 0.7592                  & 0.8780               & \textbf{0.8855}       \\  
                        &                           & 100\%                       & 0.7745               & 0.8597               & 0.8572               & 0.8841                & 0.8720               & 0.7721                  & 0.8825               & \textbf{0.8889}       \\ \hline
\multirow{10}{*}{IMDB}  & \multirow{5}{*}{Macro-F1} & 20\%                        & 0.4506               & 0.5003               & 0.4998               & 0.5124                & 0.5118               & 0.4281                  & 0.5171               & \textbf{0.5323}       \\  
                        &                           & 40\%                        & 0.4870               & 0.5338               & 0.5350               & 0.5578                & 0.5645               & 0.4865                  & 0.5577               & \textbf{0.5760}       \\  
                        &                           & 60\%                        & 0.5188               & 0.5559               & 0.5640               & 0.5823                & 0.5912               & 0.5110                  & 0.5781               & \textbf{0.6006}       \\  
                        &                           & 80\%                        & 0.5268               & 0.5713               & 0.5698               & 0.5939                & 0.6092               & 0.5239                  & 0.6018               & \textbf{0.6183}       \\  
                        &                           & 100\%                       & 0.5563               & 0.5845               & 0.5798               & 0.6130                & 0.6212               & 0.5453                  & 0.6159               & \textbf{0.6342}       \\ \cline{2-11} 
                        & \multirow{5}{*}{Micro-F1} & 20\%                        & 0.4598               & 0.5062               & 0.5072               & 0.5212                & 0.5263               & 0.4533                  & 0.5210               & \textbf{0.5414}       \\  
                        &                           & 40\%                        & 0.4874               & 0.5355               & 0.5378               & 0.5601                & 0.5723               & 0.4942                  & 0.5605               & \textbf{0.5792}       \\  
                        &                           & 60\%                        & 0.5186               & 0.5611               & 0.5669               & 0.5850                & 0.5968               & 0.5146                  & 0.5792               & \textbf{0.6017}       \\  
                        &                           & 80\%                        & 0.5269               & 0.5771               & 0.5757               & 0.5952                & 0.6129               & 0.5237                  & 0.6020               & \textbf{0.6193}       \\  
                        &                           & 100\%                       & 0.5538               & 0.5888               & 0.5837               & 0.6147                & 0.6242               & 0.5478                  & 0.6163               & \textbf{0.6343}       \\ \hline
\end{tabular}
\end{table*}

\begin{table*}[t]
\centering
\caption{Experimental results on the node clustering task.}
\label{tab:clustering_results}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline
Data                   & Metrics & MLP    & GCN    & GAT    & RGCN   & HAN    & HetGNN & HGT    & HGConv    & \%Improv.           \\ \hline
\multirow{2}{*}{ACM-3} & ARI     & 0.6105 & 0.7179 & 0.7319 & 0.7973 & 0.7732 & 0.6077 & 0.7944 & \textbf{0.8166} & 2.4\% \\ 
                       & NMI     & 0.5535 & 0.6806 & 0.6965 & 0.7536 & 0.7317 & 0.5520 & 0.7560 & \textbf{0.7752} & 2.5\% \\ \hline
\multirow{2}{*}{ACM-5} & ARI     & 0.5969 & 0.7010 & 0.7155 & 0.7766 & 0.7347 & 0.5931 & 0.7732 & \textbf{0.7903} & 1.8\% \\ 
                       & NMI     & 0.5501 & 0.6687 & 0.6789 & 0.7345 & 0.7056 & 0.5461 & 0.7319 & \textbf{0.7543} & 2.7\% \\ \hline
\multirow{2}{*}{IMDB}  & ARI     & 0.2011 & 0.2435 & 0.2264 & 0.3069 & 0.2777 & 0.1957 & 0.2982 & \textbf{0.3164} & 3.1\% \\ 
                       & NMI     & 0.1811 & 0.2099 & 0.2005 & 0.2647 & 0.2400 & 0.1723 & 0.2566 & \textbf{0.2757} & 4.2\% \\ \hline
\end{tabular}
\end{table*}

\subsection{Compared Methods}
We compare our method with the following baselines:
\begin{itemize}
	\item \textbf{MLP}: 
    MLP ignores the graph structure and solely focuses on the focal node attributes by leveraging the multilayer perceptron.
    
    \item \textbf{GCN}:
    GCN performs graph convolutions in the Fourier domain by leveraging the localized first-order approximation \cite{DBLP:conf/iclr/KipfW17}.
    
    \item \textbf{GAT}:
    GAT introduces the attention mechanism into GNNs and assigns different importance to the neighbors adaptively \cite{DBLP:conf/iclr/VelickovicCCRLB18}. 
    
    \item \textbf{RGCN}:
    RGCN designs specialized transformation matrices for each type of relations in the modelling of knowledge graphs \cite{DBLP:conf/esws/SchlichtkrullKB18}.
    
    \item \textbf{HAN}:
    HAN leverages the attention mechanism to aggregate neighbor information via multiple manually designed meta-paths \cite{DBLP:conf/www/WangJSWYCY19}.
    
    \item \textbf{HetGNN}:
    HetGNN considers the heterogeneity of node attributes and neighbors, and then utilizes Bi-LSTMs to integrate heterogeneous information \cite{DBLP:conf/kdd/ZhangSHSC19}.
    
    \item \textbf{HGT}:
    HGT introduces type-specific transformation matrices to capture characteristics of different nodes and relations with the Transformer architecture \cite{DBLP:conf/www/HuDWS20}.
\end{itemize}

\begin{figure*}[t]
    \centering
    \includegraphics[scale = 0.5]{figures/ACM_5_visualization.png}    
    \caption{Visualization of node representation on ACM-5. Each point indicates a paper and its color denotes the published area.}
    \label{fig:acm_5_node_visualization}
\end{figure*}

\subsection{Experimental Setup}
As some methods require meth-paths, we use ,  and  as meta-paths for ACM-3 and ACM-5, and choose  and  as meta-paths for IMDB. Following \citet{DBLP:conf/www/WangJSWYCY19}, we test GCN and GAT on the homogeneous graph generated by each meta-path and report the best performance from meta-paths (Experiments show that the best meta-paths on ACM-3, ACM-5, IMDB are , , and  respectively). All the meta-paths are directly fed into HAN. Adam \cite{kingma2014adam} is selected as the optimizer. Dropout \cite{DBLP:journals/jmlr/SrivastavaHKSS14} is utilized to prevent over-fitting. The grid search is used to select the best hyperparameters, including dropout in  and learning rate in . The dimension of node representation is set to 64. We train all the methods with a fixed 300 epochs and use early stopping strategy with a patience of 100, which means the training process is terminated when the evaluation metrics on the validation set are not improved for 100 consecutive epochs. 

For HGConv, the numbers of attention heads in micro/macro level convolution are both set to 8, and the dimension of each head's attention vector is set to 8. We build HGConv with two layers, since two layers could achieve satisfactory performance and stacking more layers cannot improve the performance significantly. The proposed HGConv is implemented with PyTorch \footnote{https://pytorch.org/} \cite{DBLP:conf/nips/PaszkeGMLBCKLGA19} and Deep Graph Library (DGL) \footnote{https://www.dgl.ai/} \cite{wang2019deep}. Experiments are conducted on an Ubuntu machine equipped with two Intel(R) Xeon(R) CPU E5-2667 v4 @ 3.20GHz with 8 physical cores, and the GPU is NVIDIA TITAN Xp, armed with 12 GB of GDDR5X memory running at over 11 Gbps.

\subsection{Node Classification}
We conduct experiments to make comparison on the node classification task. Following \cite{DBLP:conf/www/WangJSWYCY19}, we split the datasets into training, validation and testing sets with the ratio of 2:1:7. The ratios of training data are varied in . To make comprehensive comparison, we additionally use 5-fold cross-validation and report the average classification results. Macro-F1 and Micro-F1 are adopted as the evaluation metrics. For ACM-3 and ACM-5, we aim to predict the area of papers. For IMDB, the goal is to predict the class of movies.  and  are adopted as evaluation metrics. Experimental results are shown in \tabref{tab:classification_results} \footnote{Experimental results with variations and hyper-parameter settings of all the methods are shown in the appendix.}. By analyzing the results, some conclusions could be summarized.

Firstly, the performance of all the methods is improved with the increase of training data, which proves that feed more training data would help deep learning methods learn more complicated patterns and achieve better results.

Secondly, compared with MLP, the performance of other methods is significantly improved by taking graph structure into consideration in most cases, which indicates the power of graph neural networks in considering the information of both nodes and edges. 

Thirdly, methods designed for heterogeneous graphs achieve better results than methods designed for homogeneous graphs (i.e., GCN and GAT) in most cases, which demonstrates the necessity of leveraging the properties of different nodes and relations in heterogeneous graphs.

Fourthly, although HetGNN is designed for heterogeneous graph learning, it only achieves competitive or even worse results than MLP. We owe this phenomenon to the following two reasons: 1) there are several hyper-parameters (e.g., the return possibility and length of random walk, the numbers of type-grouped neighbors) in HetGNN, making the model difficult to be fine-tuned; 2) the random walk strategy may break the intrinsic graph structure and lead to structural information loss, especially when the graph structure contains valuable information.

Finally, HGConv outperforms all the baselines consistently with the varying training data ratio in most cases. Compared with MLP, GCN and GAT, HGConv takes both the graph topology and graph heterogeneity into consideration. Compared with RGCN and HAN, HGConv utilizes the specific characteristic of different nodes and relations without the requirement of domain knowledge. Compared with HetGNN, HGConv leverages intrinsic graph structure directly, which alleviates the structural information loss issue introduced by random walk. Compared with HGT, HGConv learns multi-level representation by the hybrid micro/macro level convolution, which provides HGConv with sufficient representation ability.

\subsection{Node Clustering}
The node clustering task is conducted to evaluate the learned node representations. We first obtain the node representation via feed forward on the trained model and then feed the normalized node representation into k-means algorithm. We set the number of clusters to the number of real classes for each dataset (i.e., 3, 5 and 3 for ACM-3, ACM-5 and IMDB respectively). We adopt  and  as evaluation metrics. Since the result of k-means tends to be affected by the initial centroids, we run k-means for 10 times and report the average results in \tabref{tab:clustering_results}.

Experimental results on the node clustering task show that HGConv outperforms all the baselines, which demonstrates the effectiveness of the learned node representation. Moreover, methods based on GNNs usually obtain better results.
We could also observe that methods achieving satisfactory results on node classification tasks (e.g., RGCN, HAN and HGT) also have satisfactory performance on node clustering tasks, which indicates that a good model could learn more universal node embedding that could be applicable to various tasks.

\subsection{Node Visualization}
To make an more intuitive comparison, we also visualize nodes in the heterogeneous graph into a low dimensional space. In particular, we project the learned node representation by HGConv into a 2-dimensional space using t-SNE \cite{maaten2008visualizing}. The visualization of node representation on ACM-5 is shown in \figref{fig:acm_5_node_visualization} \footnote{Please refer to the appendix for results on ACM-3 and IMDB.}, where the color of nodes denote their corresponding published area .

From \figref{fig:acm_5_node_visualization}, we could observe the baselines could not achieve satisfactory performance. They either fail to gather papers within the same area together, or could not provide clear boundaries of papers belonging to different areas. HGConv performs best in the visualization, as papers within the same area are closer and boundaries between different areas are more obvious.

\subsection{Ablation Study}
We conduct the ablation study to validate the effect of each component in HGConv. We remove the micro-level convolution, macro-level convolution and weighted residual connection from HGConv respectively and denote the three variants as HGConv w/o Micro, HGConv w/o Macro and HGConv w/o WRC. 
Detailed implements of the three variants are introduced as follows:
\begin{itemize}
    \item \textbf{HGConv w/o Micro.}
    This variant replaces the micro-level convolution by performing simple average pooling on nodes within the same relation. 
    \item \textbf{HGConv w/o Macro.}
    This variant replaces the macro-level convolution by performing simple average pooling across different relations.
    \item \textbf{HGConv w/o WRC.}
    This variant removes the weighted residual connection in each layer and only uses the aggregated neighbor information as the output of each layer.
\end{itemize}
Experimental results of the variants and HGConv on the node classification task are shown in \figref{fig:ablation_study}.
\begin{figure}[!htbp]
    \centering
    \includegraphics[scale = 0.36]{figures/merge_ablation_study.png}    
    \caption{Effects of the components in the proposed model.}
    \label{fig:ablation_study}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[scale = 0.6]{figures/parameter_sensitivity.png}    
    \caption{Parameter Sensitivity of the proposed model on IMDB.}
    \label{fig:parameter_sensitivity}
\end{figure*}

From \figref{fig:ablation_study}, we could observe that HGConv achieves the best performance when it is equipped with all the components and removing any component would lead to worse results. The effects of the three components vary in different datasets, but all of them contribute to the improvement in the final performance. In particular, the micro-level convolution enables HGConv to select more important nodes within the same relation, and the macro-level convolution helps HGConv distinguish the subtle difference across relations. The weighted residual connection provides HGConv with the ability to consider the different contribution of focal node's inherent attributes and neighbor information. 

\subsection{Parameter Sensitivity Analysis}
We also investigate on the sensitivity analysis of several parameters in HGConv. We report the results of node classification task under different parameter settings on IMDB and experimental results are shown in \figref{fig:parameter_sensitivity}.

\textbf{Number of convolution layers}.
We build HGConv with different number of heterogeneous graph convolution layers and report the result in \figref{fig:parameter_sensitivity} (). It could be observed that with the increment of layers, the performance of HGConv raises at first and then starts to drop gradually. This indicates that stacking a suitable number of layers helps the model to receive information from further neighbors, but too many layers would lead to the overfitting problem.

\textbf{Number of attention heads}.
We validate the effect of multi-head attention mechanism in the hybrid convolution by changing the number of attention heads. The result is shown in \figref{fig:parameter_sensitivity} (). From the results, we could conclude that increasing the number of attention heads would improve the model performance at first. When the number of attention heads is enough (e.g., equal to or bigger than 4),  the performance reaches the top and remains stable.

\textbf{Dimension of node representation}.
We also change the dimension of node representation and report the result in \figref{fig:parameter_sensitivity} (). We could find that the performance of HGConv grows with the increment of the node representation dimension and achieves the best performance when the dimension is set between 64 and 256 (we select 64 as the final setting). The performance decreases when the dimension becomes bigger further because of the overfitting problem.

\subsection{Interpretability of the Hybrid Convolution}
The principle components in HGConv are the micro-level convolution and macro-level convolution. Thus, we provide a detailed interpretation to better understand the learned importance of nodes within the same relation and difference across relations by the hybrid convolution. We first randomly select a sample from ACM-3 and then calculate the normalized attention scores from the last heterogeneous graph convolution layer. The selected paper  proposes an effective ranking-based clustering algorithm for heterogeneous information network, and it is published in the Data Mining area. The visualization is shown in \figref{fig:attention_case_study}.
\begin{figure}[!htbp]
    \centering
    \includegraphics[scale = 0.45]{figures/case_study_attention.png}    
    \caption{Visualization of the learned attention scores.}
    \label{fig:attention_case_study}
\end{figure}

\textbf{Interpretation of the micro-level convolution}.  
It could be observed that in the  relation, both \textit{Jiawei Han} and \textit{Yizhou Sun} have higher attention scores than \textit{Yintao Yu} among the authors, since the first two authors contribute more in the academic research. In the  relation, keywords that are more relevant to  (i.e., \textit{clustering} and \textit{ranking}) have higher attention scores. Moreover, the scores of references that studies more relevant topics to  are also higher in the  relation. The above observations indicate that the micro-level convolution could select more important nodes within the same relation by assigning higher attention scores. 

\textbf{Interpretation of the macro-level convolution}.
The attention score of the  relation is much higher than that of the  or  relation, in line with the fact that GCN and GAT achieved the best performance on the  meta-path. This finding demonstrates that the macro-level convolution could distinguish the importance of different relations automatically without empirical manual design, and the learned importance could implicitly construct more important meta-paths for specific downstream tasks.