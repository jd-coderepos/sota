\documentclass[11pt]{article}



\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage{fullpage}

\usepackage{cite}

\usepackage{epsfig}
\usepackage[usenames,dvipsnames]{color}
\usepackage{tabularx}

\usepackage{textcomp}
\usepackage{marvosym}
\usepackage{stmaryrd}
\usepackage{wasysym}
\usepackage{float}

\usepackage{comment}








\newcommand{\tpend}[1]{#1} 
\newcommand{\tnext}[1]{#1} 
\newcommand{\tdone}[1]{#1} 

\newcommand{\cred}[1]{\textcolor{Red}{#1}}
\newcommand{\vone}[1]{\cred{#1}}
\newcommand{\vtwo}[1]{#1}
\newcommand{\vthree}[1]{#1}
\newcommand{\vfour}[1]{#1}


\newcommand{\ens}[1]{\ensuremath{#1}}
\newcommand{\ith}{\ens{i^{\mbox{\hspace{.2mm}\scriptsize th}}}}
\newcommand{\jth}{\ens{j^{\mbox{\hspace{.2mm}\scriptsize th}}}}
\newcommand{\kth}{\ens{k^{ \mbox{\hspace{.2mm}\scriptsize th}}}}




\newcommand{\cwi}[1]{#1}
\newcommand{\cwid}[1]{}
\newcommand{\todo}[1]{\cwid{\textcolor{BrickRed}{(\textsc{ToDo } \textit{#1})}}}
\newcommand{\stynote}[1]{\cwid{\textcolor{stylenote}{(...\textit{#1}...)}}} 
\newcommand{\was}[1]{\cwid{\textcolor{old}{was: #1}}}
\newcommand{\note}[1]{\cwid{\textcolor{Blue}{(#1)}}} 



\newcommand{\Ds}{Mergeable Dictionary}
\newcommand{\prob}{Mergeable Dictionary}
\newcommand{\probs}{Mergeable Dictionaries}



\definecolor{litegr}{rgb}{.7,.7,.7}
\definecolor{goodtogo}{rgb}{.7,0,0}
\definecolor{active}{rgb}{0,.75,0}
\definecolor{old}{rgb}{.55,.55,.0}
\definecolor{stylenote}{rgb}{.2,.4,.7}



\newcommand{\kwMs}{Make-Set}
\newcommand{\kwSpl}{Split}
\newcommand{\kwJoinadj}{Join}
\newcommand{\kwUnion}{Merge}
\newcommand{\kwSrc}{Search}
\newcommand{\kwFind}{Find}


\newcommand{\Ms}{\mbox{\textsc{\kwMs}}}
\newcommand{\Msx}[1]{\mbox{\textsc{\kwMs(\ensuremath{#1})}}}
\newcommand{\Spl}{\mbox{\textsc{\kwSpl}}}
\newcommand{\Splx}[2]{\mbox{\textsc{\kwSpl(\ensuremath{#1,#2})}}}
\newcommand{\Joinadj}{\mbox{\textsc{\kwJoinadj}}}
\newcommand{\Joinadjx}[2]{\mbox{\textsc{\kwJoinadj(\ensuremath{#1,#2})}}}
\newcommand{\Union}{\mbox{\textsc{\kwUnion{}}}}
\newcommand{\Unionx}[2]{\mbox{\textsc{\kwUnion(\ensuremath{#1,#2})}}}
\newcommand{\Src}{\mbox{\textsc{\kwSrc}}}
\newcommand{\Srcx}[2]{\mbox{\textsc{\kwSrc(\ensuremath{#1,#2})}}}
\newcommand{\Find}{\mbox{\textsc{\kwFind}}}
\newcommand{\Findx}[1]{\mbox{\textsc{\kwFind(\ensuremath{#1})}}}



\newcommand{\Bslfsrc}{\mbox{\textsc{BSL-FSearch}}}
\newcommand{\Bslfsrcx}[2]{\mbox{\textsc{BSL-FSearch(\ensuremath{#1,#2})}}}
\newcommand{\Bslfspl}{\mbox{\textsc{BSL-FSplit}}}
\newcommand{\Bslfsplx}[1]{\mbox{\textsc{BSL-FSplit(\ensuremath{#1})}}}
\newcommand{\Bslfjoin}{\mbox{\textsc{BSL-FJoin}}}
\newcommand{\Bslfjoinx}[2]{\mbox{\textsc{BSL-FJoin(\ensuremath{#1,#2})}}}
\newcommand{\Bslfrew}{\mbox{\textsc{BSL-FRew}}}
\newcommand{\Bslfrewx}[2]{\mbox{\textsc{BSL-FRew(\ensuremath{#1,#2})}}}
\newcommand{\Bslfins}{\mbox{\textsc{BSL-FIns}}}
\newcommand{\Bslfinsx}[2]{\mbox{\textsc{BSL-FIns(\ensuremath{#1,#2})}}}


\newcommand{\Bslsrc}{\mbox{\textsc{BSL-Search}}}
\newcommand{\Bslsrcx}[2]{\mbox{\textsc{BSL-Search(\ensuremath{#1,#2})}}}
\newcommand{\Bslspl}{\mbox{\textsc{BSL-Split}}}
\newcommand{\Bslsplx}[2]{\mbox{\textsc{BSL-Split(\ensuremath{#1,#2})}}}
\newcommand{\Bsljoin}{\mbox{\textsc{BSL-Join}}}
\newcommand{\Bsljoinx}[2]{\mbox{\textsc{BSL-Join(\ensuremath{#1,#2})}}}
\newcommand{\Bslrew}{\mbox{\textsc{BSL-Rew}}}
\newcommand{\Bslrewx}[3]{\mbox{\textsc{BSL-Rew(\ensuremath{#1,#2,#3})}}}
\newcommand{\Bslins}{\mbox{\textsc{BSL-Ins}}}
\newcommand{\Bslinsx}[2]{\mbox{\textsc{BSL-Ins(\ensuremath{#1,#2})}}}


\newcommand{\opcount}{4}
\newcommand{\opcountmone}{3}
\newcommand{\opcountmtwo}{2}
\newcommand{\opcounttext}{four}
\newcommand{\opcountmonetext}{three}
\newcommand{\opcountmtwotext}{two}
\newcommand{\segments}{{\segment}s}
\newcommand{\segment}{segment}
\newcommand{\Segments}{{\Segment}s}
\newcommand{\Segment}{Segment}
\newcommand{\cons}[1]{\ensuremath{c_{#1}}}
\newcommand{\cnsa}{\cons a}
\newcommand{\cnsb}{\cons b}
\newcommand{\cnsc}{\cons c}
\newcommand{\cnsd}{\cons d}
\newcommand{\cnse}{\cons e}
\newcommand{\cnsf}{\cons f}
\newcommand{\cnsg}{\cons g}
\newcommand{\cnsh}{\cons h}
\newcommand{\cnsi}{\cons i}
\newcommand{\cnsj}{\cons j}
\newcommand{\cnsk}{\cons k}
\newcommand{\cnsl}{\cons l}
\newcommand{\invzero}{\ensuremath{\mathbf{(I0)}}}
\newcommand{\invone}{\ensuremath{\mathbf{(I1)}}}
\newcommand{\invtwo}{\ensuremath{\mathbf{(I2)}}}
\newcommand{\isodd}{\ensuremath{\sigma}}
\newcommand{\lasta}{z}
\newcommand{\lastb}{v}
\newcommand{\universe}{\ensuremath{\mathcal U}}
\newcommand{\maxrank}{\ensuremath{R}}
\newcommand{\prevheight}{\ensuremath{h'}}


\newcommand{\nodeheight}[1]{\ensuremath{h(#1)}}
\newcommand{\intheight}[1]{\ensuremath{H(#1)}}
\newcommand{\intmax}[1]{\ensuremath{\max(\intl #1)}}
\newcommand{\intmin}[1]{\ensuremath{\min(\intl #1)}}
\newcommand{\intsubs}[2]{\ensuremath{I(\set{#1},\set{#2})}}
\newcommand{\intnum}[2]{\ensuremath{|I(\set{#1},\set{#2})|}}
\newcommand{\intl}[1]{\ensuremath{#1}}
\newcommand{\beforeop}[1]{\ensuremath{#1'}}
\newcommand{\intweight}[1]{{\ensuremath{W(\intl #1)}}}
\newcommand{\interweight}[3]{{\ensuremath{W_{[#2,#3]}(#1)}}}
\newcommand{\setweight}[1]{{\ensuremath{W(\set #1)}}}
\newcommand{\nodeweight}[1]{{\ensuremath{w(\node #1)}}}
\newcommand{\nodenewweight}[1]{{\ensuremath{w'(\node #1)}}}
\newcommand{\noderank}[1]{\ensuremath{r(\node #1)}}
\newcommand{\nodenewrank}[1]{\ensuremath{r'(\node #1)}}
\newcommand{\node}[1]{\ensuremath{#1}}
\newcommand{\nodeposition}[2]{pos\ensuremath{_{\set #1}(\node #2)}}
\newcommand{\set}[1]{\ensuremath{#1}}
\newcommand{\collection}[1]{\ensuremath{\mathcal #1}}
\newcommand{\numgap}[2]{\ensuremath{g_{\set #1}(#2)}}
\newcommand{\agap}[1]{\ensuremath{a_{#1}}}
\newcommand{\leftofa}[1]{\ensuremath{a_{#1}'}}
\newcommand{\rightofa}[1]{\ensuremath{a_{#1}''}}
\newcommand{\bgap}[1]{\ensuremath{b_{#1}}}
\newcommand{\leftofb}[1]{\ensuremath{b_{#1}'}}
\newcommand{\rightofb}[1]{\ensuremath{b_{#1}''}}
\newcommand{\maxgap}[1]{\ensuremath{#1^{+}}}
\newcommand{\mingap}[1]{\ensuremath{#1^{-}}}
\newcommand{\leftof}[1]{\ensuremath{\gap #1'}}
\newcommand{\rightof}[1]{\ensuremath{\gap #1''}}
\newcommand{\intfun}[1]{\ensuremath{F(\intl #1)}}
\newcommand{\gap}[1]{\ensuremath{#1}}
\newcommand{\potlossa}[1]{\ens{\mathsf{pl}(\agap{#1})}}
\newcommand{\potlossb}[1]{\ens{\mathsf{pl}(\bgap{#1})}}
\newcommand{\pbound}[1]{\ensuremath{\kappa_{\gap #1}}}
\newcommand{\maxpotlossa}[1]{\ensuremath{\alpha_{#1}}}
\newcommand{\maxpotlossb}[1]{\ensuremath{\beta_{#1}}}
\newcommand{\prebsltempl}[4]{\ensuremath{#1{[\raisebox{#4}{}]}}}
\newcommand{\prebsl}[2]{\ensuremath{\prebsltempl{#1}{#2}{\scriptstyle}{0.3mm}}}
\newcommand{\prebslsub}[2]{\ensuremath{\prebsltempl{#1}{#2}{\scriptscriptstyle}{0.2mm}}}
\newcommand{\postbsltempl}[4]{\ensuremath{#1{[\raisebox{#4}{}]}}}
\newcommand{\postbsl}[2]{\ensuremath{\postbsltempl{#1}{#2}{\scriptstyle}{0.3mm}}}
\newcommand{\postbslsub}[2]{\ensuremath{\postbsltempl{#1}{#2}{\scriptscriptstyle}{0.2mm}}}
\newcommand{\bsl}[1]{\ensuremath{#1}}
\newcommand{\pre}[2]{\text{pred}\ensuremath{_{#1}(#2)}}
\newcommand{\suc}[2]{\text{succ}\ensuremath{_{#1}(#2)}}
\newcommand{\lvlpre}[2]{\ensuremath{L_{#1}(#2)}}
\newcommand{\lvlsuc}[2]{\ensuremath{R_{#1}(#2)}}
\newcommand{\datast}[1]{\ensuremath{D_{#1}}}
\newcommand{\indatast}[1]{\ensuremath{L_{#1}}}
\newcommand{\tallest}[1]{\ensuremath{u_{#1}}}
\newcommand{\hide}[1]{}
\newcommand{\locpotfun}[1]{\ensuremath{\Phi_{e}(#1)}}
\newcommand{\potfun}[1]{\ensuremath{\Phi(#1)}}
\newcommand{\plasetclosed}[3]{\ensuremath{S_{[#1,#2]}^#3}}
\newcommand{\plasetopen}[3]{\ensuremath{S_{(#1,#2)}^#3}}
\newcommand{\plasetsmall}[1]{\ensuremath{\plasetopen{0}{a}{#1}}}
\newcommand{\plaseta}[1]{\ensuremath{\plasetclosed{a}{a}{#1}}}
\newcommand{\plasetgood}[1]{\ensuremath{\plasetopen{a}{b}{#1}}}
\newcommand{\plasetb}[1]{\ensuremath{\plasetclosed{b}{b}{#1}}}
\newcommand{\plasetfat}[1]{\ensuremath{\plasetopen{b}{2b}{#1}}}
\newcommand{\plasetobese}[1]{\ensuremath{\plasetclosed{2b}{2b+1}{#1}}}
\newcommand{\plats}[1]{\ensuremath{\mathcal P_{#1}}}
\newcommand{\vionum}[1]{\ensuremath{v_{#1}}}
\newcommand{\amcost}[1]{\ensuremath{\hat{c_{#1}}}}
\newcommand{\actcost}[1]{\ensuremath{c_{#1}}}
\newcommand{\op}[1]{#1}
\newcommand{\initbsl}[1]{\ensuremath{\Lambda_{#1}}}
\newcommand{\nodesatlevel}[1]{\ensuremath{N_{h}{(#1)}}}
\newcommand{\ranksatlevel}[1]{\ensuremath{N_{r}{(#1)}}}


\newcounter{count}
\newtheorem{thm}[count]{Theorem}
\newtheorem{lemma}[count]{Lemma}
\newtheorem{defn}[count]{Definition}



\title{\probs}
    
\author{John Iacono\thanks{Research supported by NSF grant CCR-0430849 and by a Alfred P.~Sloan Fellowship.} 
\and
\"Ozg\"ur \"Ozkan\thanks{Research supported by US Department of Education Grant P200A090157.}
}


\date{Department of Computer Science and Engineering\\ Polytechnic Institute of NYU\\2 MetroTech Center\\Brooklyn, NY 11201-3840 USA}








\begin{document}

\maketitle

\begin{abstract} 
A data structure is presented for the \Ds{} abstract data type, which supports the following operations on a collection of disjoint sets of totally ordered data: 
\op{Predecessor-Search}, \op{\kwSpl} and \op{\kwUnion}. While \op{Predecessor-Search} and \op{\kwSpl} work in the normal way, the novel operation is \op{\kwUnion}. While in a typical mergeable dictionary (e.g.~2-4 Trees), the \op{\kwUnion} operation can only be performed on sets that span disjoint intervals in keyspace, the structure here has no such limitation, and permits the merging of arbitrarily interleaved sets. 
Tarjan and Brown present a data structure~\cite{journals/jacm/BrownT79} which can handle arbitrary \op{\kwUnion} operations in  amortized time per operation if the set of operations is restricted to exclude the \op{\kwSpl} operation. In the presence of \op{\kwSpl} operations, the amortized time complexity of their structure becomes . A data structure which supports both \op{\kwSpl} and \op{\kwUnion} operations in  amortized time per operation was given by Farach and Thorup~\cite{journals/algorithmica/FarachT98}. 
In contrast, our data structure supports all operations, including \op{\kwSpl} and \op{\kwUnion}, in  amortized time, thus showing that interleaved \op{\kwUnion} operations can be supported at no additional cost vis-\`{a}-vis disjoint \op{\kwUnion} operations.
\end{abstract}







\section{Introduction} 

Consider the following operations on a data structure which maintains a dynamic collection \collection S of disjoint sets  which partition some totally ordered universal set \universe: 

\begin{itemize} 

\item : Returns the set  that contains . 

\item : Returns the largest element in \set S that is at most . 

\item : Splits \set S into two sets  and . \set S is removed from \collection S while \set A and \set B are inserted. 

\item : Creates . \set C is inserted into \collection S while \set A and \set B are removed. 

\end{itemize} 

We call a data structure that supports these\hide{ \opcount{}} operations a \emph{\Ds{}}. In this paper we present a data structure, which implements these operations in amortized time , where  is the total number of items in \universe{}. What makes the concept of a \Ds{} interesting is that the \Union{} operation does not require that the two sets being merged occupy disjoint intervals in keyspace. As we discuss in full detail in Section~\ref{sec:previous}, a data structure for merging arbitrarily interleaved sets has appeared independently in the context of Union-Split-Find, Mergeable Trees and string matching in Lempel-Ziv compressed text. In all three cases, a  bound on mergeable dictionary operations could not be achieved. We present a data structure that is able to break through this bound though use of a novel weighting scheme applied to an extended version of the Biased Skip List data structure~\cite{journals/algorithmica/BagchiBG05}. Another alternative would be to extend and use Biased Search Trees~\cite{journals/siamcomp/BentST85} but we believe extending Biased Skip Lists will be easier, at least in terms of presentation.

We first present a high-level description of the core data structure of the previous work, and show at a high level the method and motivation we use to improve the runtime. Given this description, we then can discuss in some detail the three aforementioned works. Finally, we present the full details of our result. 


\subsection{High-Level Description} 

The basic idea of the structure is simple. As a first attempt we show how to achieve  time, which we outline here and fully present in Section~\ref{sec:LogSQ}. Store each set using an existing dictionary that supports \Src{}, \Spl{} and \Joinadj{}\footnote{\Joinadj{} merges two sets but requires that the sets span disjoint intervals in keyspace} in  time (e.g.~2-4 trees). Thus, the only operation that requires a non-wrapper implementation is \Union{}. One first idea would be to implement \Union{} in linear time as in \emph{Merge-Sort}, but this performs poorly, as one would expect. A more intelligent idea is to use a sequence of searches to determine how to partition the two sets into sets of \emph{\segments{}} that span maximal disjoint intervals. Then, use a sequence of \Spl{}s to split each set into the \segments{} and a sequence  \Joinadj{} operations to piece together the \segments{} in sorted order. As the number of \segments{} between two sets being merged could be , the worst-case runtime of such an implementation is , even worse than the  of a brute-force merge. However, it is impossible to perform many \Union{}s with a high number of \segments{}, and an amortized analysis bears this out; there are only  amortized \segments{} per \Union{}. Thus, since each \segment{} can be processed in  time, the total amortized cost per \Union{} operation is . 

In \cite{klaithesis}, it was shown that there are sequences of operations that have  amortized \segments{} per \Union{}. This, combined with the worst-case lower bound 
of  for the dictionary operations needed to process each \segment{} seemingly gives a strong argument for a  lower bound, which was formally conjectured by Lai. 
It would appear that any effort to circumvent this impediment would require abandoning storing each set in sorted order. 
We show this is not necessary, as a weighting scheme allows us finesse the balance between the cost of processing each \segment{}, and the number of \segments{} to be processed; we, in essence, prevent the worst-case of these two needed events from happening simultaneously. 
Our scheme, combined with an extended version of Biased Skip Lists, allows us 
to speed up the 
processing of each \segment{} to  when there are many of them, yet gracefully degrades to the information-theoretically mandated  worst-case time when there are only a constant number of \segments{}. The details, however, are numerous. We show in Section~\ref{subsec:BSLExtOps} how to augment Biased Skip Lists to support weighted finger versions of operations that we need. Given this, in Section~\ref{sec:DS} a full description of our structure is presented, and in Section~\ref{sec:Analysis} the runtime analysis is proved. 

\subsection{Relationship to existing work} 
\label{sec:previous} 

The underlying problem addressed here has come up independently three times in the past, in the context of Union-Split-Find, Mergeable Trees and string matching in Lempel-Ziv compressed text. All three of these results, which were initially done independently of each other, bump up against the same  issue with merging, and all have some variant of the  structure outlined above at their core. While the intricacy of the latter two precludes us claiming here to reduce the squared logarithmic terms in their runtimes, we believe that we have overcome the fundamental obstacle towards this improvement. 


\subsubsection{Searching in Lempel-Ziv} 

In the paper of Farach and Thorup, an algorithm is presented for string matching in a Lempel-Ziv compressed string \cite{journals/algorithmica/FarachT98}. They show how to search for a string of length  in a compressed string of length  that was compressed by a factor of  in time . The algorithm is complex, but at its heart needs a data structure that can hold a set \set S of at most  integers in the range , and can perform an operation that shifts all values in some interval in \set S by some specified integer amount, so long as all values remain the range . They show how to do this in time  using a variant of the  structure described above. This variant allows a whole set to be shifted, and thus shifting an interval can be done with two splits and two merges in addition to a shift. The potential function they use to bound the number of \segments{} is the same one as in our presentation of the  structure. A simple extension of our structure can speed up the needed shifts by a 
 factor. However, since they do not completely use the aforementioned shifting data structure as a black box (there is an ``unwinding" of some work performed there, among other subtleties), we leave the improvement of the runtime to  only as a conjectured application of our result. Recently, Pawe\l~\cite{DBLP:conf/esa/Gawrychowski11} achieved this bound using a different approach utilizing grammar-based compression.


\subsubsection{Mergable Trees} 


In the paper of Georgiadis, Tarjan, Werneck \cite{conf/soda/GeorgiadisTW06} and the follow-up tech report which adds the authors Kaplan and Shafrir \cite{DBLP:journals/talg/GeorgiadisKSTW11}, the problem of \emph{Mergeable Trees} is studied. This paper is concerned with maintaining a dynamic collection of heaps, subject to operations which constitute the \emph{dynamic tree} ADT: \op{parent}, \op{root}, \op{nca}, \op{insert}, \op{link}, \op{cut}, \op{delete}, and one additional operation, \Union{}, which makes things interesting. In the \Union{} operation, two nodes are specified, and the two root-to-node paths, which are each in sorted order due to the heap property, are merged. The idea of a \emph{Mergeable Tree} ADT originated in an algorithm to compute the structure of a 2-maniford in . (The original paper has an algorithm which is shown in \cite{journals/dcg/AgarwalEHW06} to take time  per operation.) 
This ADT is a generalization of our \Ds{}, as if the heaps are restricted to be paths, both structures have identical functionality. They too obtain a  amortized bound on the number of \segments{}, using the same potential function as in our  presentation, albeit using link-cut trees \cite{journals/jcss/SleatorT83} as the underlying  structure due to their need to have the operations be performed on a tree paths in a heap rather than a totally ordered set, thus obtaining a  amortized bound on their operations. We conjecture that using the weighting scheme presented in this paper will allow the development of a Mergeable Tree with  runtime for all operations. This would require the development of a weighted variant of link-cut trees that support weighted finger searches. In effect, link-cut trees extend  trees to allow operations on a tree topology, while biased skip lists allow sophisticated weighting operations. We would need a combination of both in order to achieve  amortized time Mergable Trees. While we conjecture such a combination is possible, the details to be worked out are numerous. 

\subsubsection{Union-Split-Find} 

In the Master's thesis of Lai, the problem of \emph{Union-Split-Find} is studied \cite{klaithesis}. This is proposed as a variant of the classic \emph{Union-Find} data structure of \cite{journals/jacm/Tarjan75}. Our \Ds{} ADT implements \Find{} as \Src{} which is a stronger operation than \Find{} (e.g. \Find{} can be implemented by returning the maxima of a set). 
They propose the  structure outlined above, and show that its amortized performance is . They conjecture (correctly) that there is a potential function that gives  runtime, but do not discover it, instead listing several potential functions which do not work.  They show that there is a lower bound of  for this problem which follows from dynamic connectivity lower bounds of Mihai P\v{a}tra\c{s}cu \cite{conf/stoc/PatrascuD04}. This lower bound indicates our use of the stronger  instead of the weaker  comes at no additional asymptotic amortized cost. They conjecture (incorrectly) that this problem has a lower bound of  per operation; our  result refutes this. Our results directly provide an amortized optimal  time solution to their problem, while the best upper bound they could prove was . 



\subsection{Future Work} 

This paper opens up several avenues for future work. First, as previously stated we believe that our approach can remove a  factor from the runtimes of the Lempel-Ziv searching algorithm and of Mergeable Trees. Both of these results will require integrating our result into each of these different and complicated results. But, since in both cases the same potential function is used as in the  structure presentation below, we believe that our data structure provides the fundamental breakthrough needed to improve these results. 


Secondly, for simplicity we do not consider the dynamic case: our data structure always stores a collection of sets that partitions the same totally ordered set. Extending our result to allow insertion and deletion will require adding additional complexity to our weighting scheme. Finally, we note that the idea that arbitrarily interleaved dictionaries can be merged in  amortized time is probably a surprising one, which at first glance probably appears to to impossible (which is supported by Lai's inability to get a  solution). While previous results had elements of the \Ds{} ADT, and a  solution to it, here is the first clear abstraction of it as a pure dictionary problem. 
The fact that the three results above were initially discovered independently of each other also speaks to the ``buried'' nature of the fundamental problem in the previous work. 
We hope that this result finds applications which were not considered because of the seeming impossibility at first glance of a  solution. 


\section{A Simple Heuristic for \Union{} and the  Amortized Bound} 
\label{sec:LogSQ} 


As mentioned previously, the main difficulty in designing a data structure for our problem with   worst-case time complexity lies in being able to perform the \Union{} operation fast. This is confirmed by a lower bound of  on the worst-case time complexity of merging two arbitrary sets \cite{conf/soda/DemaineLM00}. We will describe a heuristic for the \Union{} operation presented in \cite{conf/soda/DemaineLM00} and used in previous work \cite{DBLP:journals/talg/GeorgiadisKSTW11, klaithesis}, and show that the use of this heuristic yields  amortized bounds as a warm up. 


Consider the \Unionx{A}{B} operation and a maximal subset in either set \set A or \set B such that all the elements of the other set are less than or greater than each element of the subset. We call this maximal subset a \segment. We can view the \Unionx{A}{B} operation as gluing the appropriate \segments{} of set \set A and \set B. Consider, for instance, the \textit{Merge} algorithm of \textit{Merge-Sort}, which could be used to implement the \Union{} operation. The \textit{Merge} algorithm linearly scans each \segment{} until it locates its maximum element. The \textit{\segment{} merging heuristic} is based on that idea that there are more efficient methods of locating the maximum element of a set than a linear scan. 



Next, we make the notion of \segments{} slightly more precise, describe the heuristic, and describe the potential function which yields an upper bound of amortized  time. 


\subsection{The \Segment{} Merging Heuristic} 
\label{subsec:LogSQIntSubApp} 

Define a \emph{\segment} of the \Unionx{A}{B} operation to be a maximal subset \intl S of either set \set A or set \set B such that no element in  lies in the interval . 
  
Each set in the collection is stored as a balanced search tree (i.e.~2-4 tree) with level links. The \hide{\Ms{}, }\Find{}, \Src{}, and \Spl{} operations are implemented\footnote{See \cite{klaithesis} for a detailed description of this implementation.} in a standard way to run in  worst-case time. The \Unionx{A}{B} operation is performed as follows: 
  
We first locate the minimum and maximum element of each \segment{} of the \Unionx{A}{B} operation using the \Src{} operation and the level links, then extract all these \segments{} using the \Spl{} operation, and finally we merge all the \segments{} in the obvious way using a standard \Joinadj{} operation. Therefore since each operation takes  worst-case time, the total running time is  where  is the number of \segments{}. 
  
We now analyze all the operations using the potential method \cite{amortizedcomplexity}, with respect to two parameters: ,\hide{the total number of \Ms{} operations} the size of the universe, and , the total number of all operations. 
  
Let \datast i represent the data structure after operation , where \datast 0 is the initial data structure. Operation  has a cost of \actcost i and transforms  into . We have a potential function  such that  and  for all . The amortized cost of operation , , with respect to  is defined as . The total amortized cost of  operations will be 
 
since  and . Thus, the amortized cost will give us an upper bound on the worst-case cost. 


Next, we describe a potential function which yields an amortized bound of  on the running time. This potential function was essentially used in \cite{DBLP:journals/talg/GeorgiadisKSTW11, journals/algorithmica/FarachT98} which are the only instances where a  solution has been presented. 


\subsection{The Potential Function} 
\label{subsec:LogSQPotFun} 
We need to define some terminology before describing the potential function. Let \nodeposition{S}{x} be the position of  in set , or more formally . Then , the size of the \kth\ gap of set \set S, is the difference of positions between the element of position  and  of set \set S in universe . In other words,  where  and  . For the boundary cases, let . 
  
Recall that  is the data structure containing our dynamic collection of disjoint sets,  after the \ith\ operation. Finally, let . Then we define the potential after the \ith\ operation as follows: 
 
where \cnsa{} is a positive constant to be determined later. 
  
Note that since the collection of sets consists of the  singleton sets, the data structure initially has 0 potential (). Furthermore, because any gap has size at least 1, the data structure always has non-negative potential (). 



\subsection{The Amortized  Bound} 
\label{subsec:LogSQBound} 

The \hide{\Ms{}, }\Find{}, \Src{}, and \Spl{} operations have worst-case  running times. The first \opcountmtwotext{} of these operations do not change the structure and therefore do not affect the potential. Observe that the \Spl{} operation can only decrease the potential. Thus, the amortized cost of all \opcountmonetext{} operations is . 

Now, suppose the \ith\ operation is \Unionx{A}{B} where \set  and \set  are sets in \datast{i-1}. Assume w.l.o.g.~that the minimum element in  is an element of \set . Let  be the set of \segments{} of operation \Unionx{A}{B}, where  and  for , and  for all . 
As previously noted, the worst-case cost of the \Union{} operation is . Let  be the size of the gap between the maximum element of  and the minimum element of , or more formally let . Define  similarly. Now, let 
 
 
 
and 
 
 
 
\begin{figure}
\centering 
\includegraphics[scale=.6]{logsqgaps} 
\caption{Gaps \agap i, \bgap i, \leftofa i, \rightofb i, \leftofb i, and \rightofb i are defined with respect to the \Unionx{A}{B} operation.} 
\label{fig:lgsqgaps} 
\end{figure} 
 
Note that  and  (see Figure \ref{fig:lgsqgaps}). During the analysis we will take into account whether \intnum{A}{B} is odd or even. Let , and . We are now ready to bound the amortized cost of the \Union{} operation. We have 
 
 
 

 
and 
 
This gives us 
{\allowdisplaybreaks 
 
We have  and similarly . Also note that since , we have  Similarly,  
} 


Recall that the worst-case cost of the \Union{} operation, , is . Let  be a constant such that . Then the above bound yields 
 


Thus, the amortized cost of the \Union{} operation is . Combined with the arguments before, this gives us the following theorem. 
 



\begin{thm} 
\label{thm:logsq} 
The \prob{} problem can be solved such that a sequence of  \hide{\Ms{}, }\Find{}, \Src{}, \Union{}, and \Spl{} operations\hide{,  of which are \Ms{} operations, } can be executed in   worst-case time. 
\end{thm} 



In order to obtain a data structure with an amortized running time of  per operation, we certainly need a new potential function. 
To see this, observe the case when we have two singleton sets  and  with elements  and  such that . If we use the potential function defined in Section~\ref{subsec:LogSQPotFun}, the potential increase alone as a result of a \Unionx{A}{B} operation is . We want to eliminate the extra  factor in the potential function but this implies we need to be able to join \segments{} in amortized  time. We ultimately want a data structure such that each operation except \Union{} can be performed in worst-case  time, and \Union{} can be performed in worst-case 
 
time where  and  are \segments{} involved in the operation, \intfun{A_i} and \intfun{B_j} denote the time it takes to process \segments{}   and  respectively, and  is no more than the decrease in potential. 



In the next section, we describe biased skip lists, the underlying data structure we will be using in our data structure. 


\section{Biased Skip Lists with Extended Operations} 
\label{sec:BSL} 
Biased skip lists are a variant of skip lists \cite{journals/cacm/Pugh90} which we assume the reader is familiar with. Biased skip lists as described in \cite{journals/algorithmica/BagchiBG05} are missing some operations which will be vital in the implementation of our data structure. Therefore, in order to be able to design a highly tuned \Union{} operation, we will extend biased skip lists. 

First, we describe essential biased skip list details. The reader is referred to \cite{journals/algorithmica/BagchiBG05} for further details on biased skip lists. 


\subsection{Biased Skip Lists} 
\label{subsec:BSLBSL} 

 
We will first cover basic definitions followed by the three key invariants of the structure. 
 

\paragraph{Definitions} 
A biased skip list (BSL)  stores an ordered set  where each element  corresponds to a node\footnote{We will use the terms ``element", ``node", ``key", and ``item" interchangeably; the context clarifies any ambiguity.}  with \textit{weight} , which is user-defined, and integral \textit{height} , which is initially computed from the weight of the node. For our purposes, we will assume that the weights are bounded from below by 1 and bounded from above by a polynomial in . 

Each node  is represented by an array of length  called the \textit{tower} of node \node x. The \textit{level-j predecessor}, \lvlpre{j}{x}, of  is the largest node  in \bsl S such that  and . The \textit{level-j successor}, \lvlsuc{j}{x}, is defined symmetrically. The \jth\ element of the tower of node \node x, contains pointers to the \jth\ elements of towers of node \lvlpre{j}{x} and node \lvlsuc{j}{x} with the exception of towers of adjacent nodes where pointers between any pair of adjacent nodes  and  on level  are nil and the pointers below this level are undefined. Node levels progress from top to bottom. Two distinct elements  and  are called \textit{consecutive} if and only if they linked together in ; or equivalently if and only if for all , . A \textit{plateau} is a maximal set of consecutive nodes of the same height. The \textit{rank} of a node \node x is defined as  where  is a constant to be specified later. For our purposes, we will set . 

 
Additionally, let  be the predecessor of  in set \set X, and let  be the successor of  in set \set . Let . Let  and . Let . Also let .

For convenience, we imagine sentinel nodes  and  of height \intheight S at the beginning and end of biased skip list \bsl S. These sentinels are not actually stored or maintained. 

 The \textit{left profile} of \node x in a biased skip list \bsl S is defined as . Similarly the \textit{right profile} of \node x in a biased skip list \bsl S is defined as . The \textit{profile} of node  in a biased skip list \bsl S is the union of its left profile and right profile. 

 The \textit{left cover} of a biased skip list \bsl S is defined as . 
Similarly, the \textit{right cover} of a biased skip list \bsl S is defined as . 
The \textit{cover} of biased skip list \bsl S is the union of its left cover and right cover. 
 

\paragraph{Invariants} 
The three invariants of biased skip lists are listed below. Note that  and  can be suitable constants satisfying the definition of -biased skip lists. For our purposes it is sufficient to set . 
\begin{defn} 
For any  and  such that , an -biased skip list is a biased skip list with the following properties: 
\begin{description} 
\item {\invzero} Each item  has height . 
\item {\invone} There are never more than  consecutive items of any height. 
\item {\invtwo} For each node  and for all  such that , there are at least  nodes of height  between  and any consecutive node of height at least . 
\end{description} 
\end{defn} 
\noindent In the remainder of the paper, we will refer to -biased skip lists simply as biased skip lists. 
\subsection{Operations} 
\label{subsec:BSLOps} 

We now describe the original biased skip list operations we will be using in our data structure. 

\begin{itemize} 

 \item : Performs a standard search in biased skip list  using search key . This operation runs in worst-case  time. 
 \item : Starting from a given finger\footnote{This operation is originally described with three arguments in \cite{journals/algorithmica/BagchiBG05} as . However, note that  is a redundant argument here as we already have a pointer to an element of , namely, to .} to a node  in some biased skip list  perform a predecessor search in   using  as the search key. This operation runs in 
 worst case time. 
 \item : Splits the biased skip list  at  into two biased skip lists \bsl A and \bsl B storing sets  and  respectively, and returns an ordered pair of handles to  and . This operation runs in worst-case  time. 

\item \Bslrewx{S}{i}{w}: Changes the weight of node  to . This operation runs in worst-case  time. 


\end{itemize} 

\subsection{Extended Operations} 
\label{subsec:BSLExtOps} 
Biased skip lists support finger searches, however we need to extend biased skip lists to also support finger split, finger join, and finger reweight operations. 


\paragraph{Finger Split} Given a pointer to node , \Bslfsplx{f} splits biased skip list  into two biased skip lists, \bsl A and \bsl B, storing sets  and  respectively, and returns an ordered pair of handles to  and . 


\vspace{5mm} 
\noindent : 
\begin{enumerate} 
\item Disconnect the pointers between the each node in the right profile of node \node f and the left profile of node \node , effectively splitting \bsl S and forming  and . More precisely, disconnect pointers between the \jth\ level of node \lvlsuc{j}{f} and \lvlpre{j}{f'} where . Pointers below this level are already null. 

\item Restore \invtwo{} in .
We will process the nodes of the right cover of \bsl A after Step~1 in the order of increasing height. Denote the current node being processed by \node u. Let \prevheight{} be the height of the node which was most recently processed. 
\begin{enumerate} 
\item If , then set  and demote the height of \node u to \noderank u. 

\item  If \invtwo{} is not violated at \node u, then stop if , and set  and iterate with the next node, \lvlpre{\nodeheight{u}+1}{A}, otherwise\footnote{Note that \lvlpre{\nodeheight{u}+1}{A} can be computed in  time due to \invone{}; and  can be computed during Step~1.}. 

\item If \invtwo{} is violated at node \node u, then demote the height of  to . Set  to the height of \node u before the demotion. 

\begin{itemize} 
\item If a demotion causes an \invone{} violation at the new height of \node u by creating a plateau of  nodes, then promote the height of the median of these nodes by 1, and iterate at the level above to check for percolating \invone{} violations. 
\item Once all the \invone{} violations are fixed, iterate with the next node, \lvlpre{\prevheight +1}{A}, to fix the next potential \invtwo{} violation. 
\end{itemize} 

\end{enumerate} 

Restore \invtwo{} in  essentially symmetrically. 
\end{enumerate} 

\paragraph{Correctness} All the pointers in \bsl S connecting any node in \bsl A and any node in \bsl B are precisely those described in Step 1. Therefore Step 1 splits the nodes of  and  correctly. 

We need to ensure that all the invariants are preserved. When we perform demotions in Step 2 we make sure that we do not demote the height of any node lower than its rank. Therefore \invzero{} is preserved. Note that removing predecessors or successors cannot cause an \invone{} violation. 

Observe that in , \invtwo{} can only be violated at the nodes in the right cover of \bsl A after Step~1, once per level. A symmetric argument holds for \invtwo{} violations in \bsl B. 
When we demote a node \node u this fixes the \invtwo{} violation at that node because the height of the node is either demoted to \noderank u which by definition implies the violation is fixed, or to \prevheight{}. 
If the height of node \node u is demoted to \prevheight{}, this implies that there was another node \node{u'} with height \prevheight{}, consecutive to \node u, before the \Bslfspl{} operation. 
The only way we change the height of any node between node \node u and node \node{u'} during \Bslfsplx{f} is an \invone{} promotion, which, cannot cause an \invtwo{} violation. Note that there cannot be an \invtwo{} violation at these nodes since they are not in the right cover of \bsl A after Step~1. We assume there were no \invtwo{} violations before the \Bslfspl{} operation. Then it holds that there can be no \invtwo{} violations at node \node u at any level less than or equal to \prevheight{}. Therefore, demoting \node u to level \prevheight{} fixes any \invtwo{} violations at this node. 

Any of the demotions may cause an \invone{} violation which are also fixed by Step~2. Because we promote the median, this cannot cause an \invtwo{} violation since . Also note that the \invone{} promotions caused by the demotion of a node never percolate higher than the height of the node before the demotion. Thus, nodes on the right cover of \bsl A after Step~1 that have not yet been processed during Step~2 are not removed from the right cover due to an \invone{} promotion. 

Observe that if a node \node u in the right cover of \bsl A after Step~1 has height greater than   and there is no \invtwo{} violation at this node, then no other node of greater height in \bsl A can have an \invtwo{} violation. Symmetric arguments again apply for \bsl B. Thus, by induction, iterating Step 2 fixes all \invtwo{} violations. 


\paragraph{Finger Join} 
Given pointers to  and , the maximum and minimum nodes of two distinct biased skip lists  and  respectively, \Bslfjoinx{\ell}{r} returns a new biased skip list  containing all elements of  and  assuming .  and  are destroyed in the process. 


\vspace{5mm} 
\noindent :
\begin{enumerate} 
\item Connect pointers between each node in the right cover of \bsl A and the left cover of \bsl B, effectively joining \bsl A and \bsl B, and forming \bsl S. 
More precisely, create pointers between the \jth\ level of node \lvlsuc{j}{r'} and \lvlpre{j}{\ell'} for all , where  is the  sentinel node of \bsl A, and  is the  sentinel node of \bsl B. Pointers below this level need to be null. 

\item Restore \invone{} in \bsl S. For  up through , if there is an \invone{} violation at level  caused by a plateau of  nodes, then promote the height of the median of these nodes by 1, and iterate at the next level. For  up through , if there is no \invone{} violation at level , then stop\footnote{ can be computed during Step~1.}. Otherwise promote the median node of the plateau of  nodes causing the violation to restore \invone{} and iterate at the next level. 
\end{enumerate} 

\paragraph{Correctness} 
Joining \bsl A and \bsl B only affects the right cover of \bsl A and the left cover of \bsl B. Therefore, Step 1 connects the two biased skip lists \bsl A and \bsl B and forms \bsl S correctly. Joining two biased skip lists cannot create any \invtwo{} violations assuming there were no such violations before the operation; and fixing \invone{} violations cannot create \invtwo{} violations since . Therefore, \invtwo{} is preserved. Note that after Step 1, any level in the range  could have at most  ( from \bsl A,  from \bsl B, and  due to a promotion from the level below) consecutive nodes of the same height; which is fixed in Step 2. By induction, iterating Step 2 fixes all \invone{} violations. 



\paragraph{Finger Reweight} 
Given a pointer to a node , changes its weight to  while preserving invariants \invzero{}, \invone{}, and \invtwo{} of the biased skip list containing . 


\vspace{5mm} 
\noindent: 
\begin{enumerate} 
\item Let \nodenewrank f be the new rank of \node f. If , then stop. 
\item If  and , then stop. 
\item If  and , then promote the height of  to \nodenewrank f. Restore \invtwo{} as in Step 2 of \Bslfspl{} but start from the first node in the left profile of \node f that has height greater than \nodeheight f; and symmetrically from the first node in the right profile of \node f that has height greater than \nodeheight f. Then, restore \invone{} as in Step 2 of \Bslfjoin{} but start from level \nodenewrank f. 
\item If , then demote the height of \node f to \nodenewrank f. Restore \invone{} as in Step 2 of \Bslfjoin{} but starting from \nodenewrank f. Then, restore \invtwo{} as in Step 2 of \Bslfspl{}, but starting from the first node in the left profile of \node f that has height greater than \nodeheight f; and symmetrically from the first node in the right profile of \node f that has height greater than \nodeheight f. 
\end{enumerate} 

\paragraph{Correctness} If the rank of node \node f does not change, there are no structural changes to the biased skip list and therefore Step 1 is correct. 

If the rank of \node f increases but it is still less than its height, then \invzero{} is preserved. By not changing the height of the node we ensure that \invone{} and \invtwo{} are preserved as well. Therefore, Step 2 is also correct. 


If the rank of a node \node f becomes greater than its height, then \invzero{} is violated and we promote \node f to its new rank to fix the violation. Observe that this promotion can cause \invtwo{} to be violated at the nodes in the left profile of \node f that have height greater than the old height of \node f; and symmetrically at the nodes in the right profile of \node f that have height greater than the old height of \node f, once per level. Step 3, by the correctness of Step 2 of \Bslfspl{}, fixes all \invtwo{} violations. The promotion can also cause \invone{} to be violated at level \nodenewrank f. Step 3, by the correctness of Step 2 of \Bslfjoin{}, fixes all \invone{} violations. Therefore, Step 3 is correct. 

If the rank of a node \node f decreases, we demote \node f to its new rank so \invtwo{} cannot be violated at this node. Observe that this demotion can cause \invone{} to be violated at level \nodenewrank f. Step 4, by the correctness of Step 2 of \Bslfjoin{}, fixes all \invone{} violations. The demotion can also cause \invtwo{} to be violated at the nodes in the left profile of \node f that have height greater than the old height of \node f; and symmetrically at the nodes in the right profile of \node f that have height greater than the old height of \node f, once per level. Step 4, by the correctness of Step 2 of \Bslfspl{}, fixes all \invtwo{} violations. Therefore, Step 4 is correct. 

Since steps 1-4 exhaust all possible scenarios, all the invariants are preserved and the \Bslfrew{} operation is correct. 



Before moving on, we need to analyze the time complexity of these new operations. 


\subsection{The Analysis of Extended Operations} 
\label{subsec:BSLAnalysisExtOps} 



We now analyze the time complexity of the extended operations described above using the potential method. 



The extended operations, \Bslfspl{}, \Bslfjoin{}, and \Bslfrew{}, are executed on the elements of a set of biased skip lists. Let \indatast k be the set of biased skip lists after the \kth\ operation, where \indatast 0 is the initial set of biased skip lists we are given. 

Let \plats k be the set of plateaus which contain an element in the cover of some biased skip list in \indatast{k}. For a plateau , let  be the number of nodes contained in plateau . We define the following sets of plateaus. Let  and . 
We can now define a potential function as follows. 
 
where \cnsg\ is some constant. Observe that  for any . 
For each extended operation, we will use this potential function to prove upper bounds on the amortized time complexity of the operation as well as worst-case time complexity of a sequence of operations. 


\begin{lemma} 
\label{lem:costofbslfsplit} 
The  operation, where , has an amortized time complexity of 
 
where  and . 
\end{lemma} 


\begin{proof} 
Let \Bslfsplx{f} be the \kth\ operation. Observe that Step 1 takes  time. 
Step 2 takes constant time at each level from level  to level . We will show that the time spent by Step~2 on levels greater than  is essentially negligible by showing that it equals the decrease in potential at these levels. 

We now analyze the contribution of Step 1 and Step 2 to the potential change, . 

\paragraph{Step 1} 


Due to the split, the plateaus on the cover of \bsl S become plateaus on the covers of \bsl A and \bsl B. Additionally, the plateaus on levels less than or equal to  on the right cover of \bsl A and left cover of \bsl B are added to \plats k. 
Therefore, the contribution of Step 1 to the potential change is at most . 

\paragraph{Step 2} 
We now look at the contribution of Step 2 to the potential change, . Note that at any level less than or equal to , the potential increase can be at most a constant. Therefore, the maximum contribution of Step~2 to the potential change in these levels is . Next, we bound the contribution of Step~2 to the potential change in levels greater than . 

\paragraph{Demotions} 
Consider a demotion operation to restore \invtwo{} at some node \node x. This demotion could cause a change in potential in two ways. 

First, the plateau  which was causing the \invtwo{} violation could have had less than  nodes, and now has more. 

Note that since we demote node \node x to the level of , there must be a plateau of at least  nodes (due to \invtwo{}) on the other side of node \node x. Therefore,  will have at least  nodes. Also, the plateau on the other side of node \node x cannot have more than  nodes. Therefore,  will have at most  nodes. This implies that  can only  have between  and  nodes. 
If  has between  and  nodes, the contribution of this part to the potential change is . 
If  has  nodes, the contribution of this part to the potential change is . 
If  has between  and  nodes, the contribution of this part to the potential change is . 
Therefore, the maximum contribution of this part to the potential change is . 


Note that  might not exist (have zero nodes) prior to the demotion of . In this case, the maximum contribution of this part to the potential change is .  However, this case is only possible if  has height less than or equal to . 


Second, the demotion of  could cause the plateau , which  was a part of before the demotion, to have less nodes. 
If  had  nodes or more and now has between  and  nodes, the contribution of this part to the potential change is . 
If  had between  and  nodes and now has  nodes, the contribution of this part to the potential change is . 
If  had  nodes and now has between  and  nodes, the contribution of this part to the potential change is . 
If  had between  and  nodes and now has  nodes, the contribution of this part to the potential change is . 
If  had  nodes and now has between  and  nodes, the contribution of this part to the potential change is . 
Additionally, if  had between  and  nodes and now has zero nodes, the contribution of this part to the potential change is . 

Therefore, combining the first and second part, the maximum contribution of a demotion to the potential change is  on levels less than or equal to , and  on levels greater. 

Note that the demotion of \node x could cause a high number of plateaus which did not have any elements in the cover of their biased skip list to now have an element in the cover and thus enter \plats k. In order for this case to occur, the height of \node x must be demoted at least two levels. By the description of Step~2, this implies there were no nodes of height  in the biased skip list containing \node x after Step~1. Since \bsl S has no \invtwo{} violations before Step~1, this implies there must be nodes of height  in the other biased skip list. Therefore, this case is only possible in levels less than or equal to . 

\paragraph{Promotions} 
Consider a promotion operation at a node \node x to restore an \invone{} violation on the plateau  node \node x is on. This promotion could cause a change in potential in two ways. 

First, the plateau  could have had between  and  nodes, and now has less. 
If  had between  and  nodes, then the promotion of node \node x splits  into two plateaus. Only one of these plateaus remain in the cover of the biased skip list unless nodes of  were at the highest level of the biased skip list. 
The plateau remaining in the cover must have size greater than  and less than . Therefore,  will now have between  and  nodes, and the contribution of this part to the potential change is . 
In the special case of both plateaus remaining in the cover, then both of them will have between  and  nodes, and the contribution of this part to the potential change is . 
If  has  or more nodes, then the promotion of \node x splits  into two plateaus. Only one of these plateaus remain in the cover of the biased skip list unless nodes of  were at the highest level of the biased skip list. 
The plateau remaining in the cover must have size either  or . 
If it has  nodes, the contribution of this part to the potential change is . 
If it has  nodes, the contribution of this part to the potential change is . 
In the special case of both plateaus remaining in the cover, then either one of them has  nodes, and the other one  has  nodes, and the contribution of this part to the potential change is ; or both of them have  nodes, and the contribution of this part to the potential change is . 


Second, the promotion of  could cause the plateau , which  becomes a part of after the promotion, to have more nodes. 
If  had more than  nodes, note that it must have had at most  nodes. Promotion of  increases the size of  to . Thus, it does not change its set and the contribution of this part to the potential change is . 
Note that, in general, if the promotion does not change the set of , then the contribution of this part to the potential change is . 
If  had\hide{between  and}  nodes and now has   nodes, the contribution of this part to the potential change is . 
If  had  nodes and now has  nodes, the contribution of this part to the potential change is . 
If  had\hide{ between  and}  nodes and now has  nodes, the contribution of this part to the potential change is . 
If  had  nodes and now has  nodes, the contribution of this part to the potential change is . 
If  had\hide{ between  and}  nodes and now has  nodes, the contribution of this part to the potential change is . 
Additionally, if  had zero nodes and now has  node, the contribution of this part to the potential change is . However, this can only happen if an earlier demotion caused the only node of  to be demoted. Therefore,  first has  node; then a demotion causes  to have  nodes; and then a promotion associated with that demotion causes  to have  node again. The effect on the potential change is zero. 

Therefore, combining the first and second part, the maximum contribution of a promotion to the potential change is . 

Let constant \cnsh\ be an upper bound on the worst-case running time of any single demotion or promotion operation. Let \amcost k and \actcost k respectively be the amortized and worst-case running time of \Bslfsplx{f} and let \vionum k be the number of violations that are restored during Step~2 above level . 
Then we have . 
Combining the bounds on the maximum contribution of Step~1 and Step~2 to the potential change and setting , we have . 
This yields 
 
This completes the proof. 
\end{proof} 



\begin{lemma} 
\label{lem:wccostofbslfsplit} 
Given a set  of biased skip lists, a sequence of  operations, for  where   and , can be executed in worst-case time 
 
 
where  and . 
\end{lemma} 



\begin{proof} 
Let \amcost k and \actcost k respectively be the amortized and worst-case running time of \Bslfsplx{f_k}. Note that . Then we have 
{\allowdisplaybreaks 
 
} 
\end{proof} 


\begin{lemma} 
\label{lem:costofbslfjoin} 
The  operation, where , , has an amortized time complexity of 
  
\end{lemma} 


\begin{proof} 
Let \Bslfjoinx{\ell}{r} be the \kth\ operation. Observe that Step 1 takes  time. 
Then, Step 2 takes constant time at each level from level  to level . We will show that the time spent by Step~2 on levels greater than  is essentially negligible by showing that it equals the decrease in potential at these levels. 

We now analyze the contribution of Step 1 and Step 2 to the potential change, . 

\paragraph{Step 1} 

Due to the join, the plateaus on levels greater than  as well as the plateaus on left cover of \bsl A and right cover of \bsl B becomes plateaus on the cover of \bsl S. The plateaus on levels less than or equals to  on the right cover of \bsl A and on the left cover of \bsl B are not on the cover of \bsl S and thus are not added to \plats k. 

The only way Step~1 can increase the potential is if all the plateaus on the covers of \bsl A and \bsl B becomes plateaus on the cover of \bsl S and the plateaus containing nodes \intmax A and \intmin B merge and form a larger plateau with more weight with respect to the potential function. 
Therefore, the contribution of Step 1 to the potential change is at most . 



\paragraph{Step 2} 

Note that Step 2 only involves promotions. Any promotion on level less than or equal to  will increase the potential by at most a constant and total contribution to the potential change is bounded by . We will focus on promotions which occur on higher levels. 

Consider a promotion operation at a node  to restore an \invone{} violation on the plateau  node \node x is on where  is on a level greater than . The promotion could cause a change in potential in two ways. 


First, the plateau  could have had  nodes, then the promotion of node \node x splits  into two plateaus with between  and  nodes each. Then, the contribution of this part to the potential change is . 


Second, the promotion of  could cause the plateau , which  becomes a part of after the promotion, to have more nodes. Note that due to \invone{},  could not have had more than  nodes. If  had  nodes and now has  nodes, the contribution of this part to the potential change is . If  had less than  nodes and now has at most  nodes, the potential could increase by at most a constant, but the promotion will not cause an \invone{} violation at , and Step 2 will terminate.


Therefore, combining the first and second part, the maximum contribution of a promotion to the potential change is  except possibly for the last promotion. 


Recall that the worst-case running time of any single demotion or promotion operation is bounded by constant \cnsh. Let \amcost k and \actcost k respectively be the amortized and worst-case running time of \Bslfjoinx{\ell}{r} and let \vionum k be the number of violations that are restored during Step~2 above level . 
Then we have . 
Combining the bounds on the maximum contribution of Step~1 and Step~2 to the potential change and setting , we have . 
This yields 

 
This completes the proof. 
\end{proof} 



\begin{lemma} 
\label{lem:wccostofbslfjoin} 

Given a set  of biased skip lists, and a sequence of  operations, for  where ,   and , can be executed in worst-case time 
 
 
\end{lemma} 


\begin{proof} 
Let \amcost k and \actcost k respectively be the amortized and worst-case running time of \Bslfjoinx{\ell_k}{r_k}. Note that . Then we have 

{\allowdisplaybreaks 
 
} 
\end{proof} 


\begin{lemma} 
\label{lem:costofbslfreweight} 
The  operation, where , has a worst-case and amortized time complexity of 
 
\end{lemma} 


\begin{proof} 
\Bslfrewx{f}{w} only spends constant time at each level between  and  for promoting or demoting \node f; and at most constant time at each level between  and  for restoring invariants. Therefore, the worst-case complexity of \Bslfrewx{f}{w} is . 

Let \Bslfrewx{f}{w} be the \kth\ operation. Note that no plateaus that are on levels less than  are affected by the operation. Since the potential increase associated with each level is bounded by a constant, we have . Thus, the lemma follows. 
\end{proof} 


\begin{lemma} 
\label{lem:logncostofextended} 
The \Bslfsrc{}, \Bslfspl{}, \Bslfjoin{}, and \Bslfrew{} operations all have a worst-case time complexity of . 
\end{lemma} 


\begin{proof} 
Let  be the sum of the weights of all elements in the sets involved in any of these four operations. By our previous assumptions,  is bounded from above by a polynomial in . Since these versions of the operations are more efficient than the non-finger versions which take  time in the worst case, these operations have a worst-case time complexity of  . 
\end{proof} 





We now present our data structure, the \Ds{}, which improves the worst-case bound in Theorem~\ref{thm:logsq} by a factor of , matching the lower bound of \cite{conf/stoc/PatrascuD04}. 




\section{Our Data Structure: The Mergeable Dictionary} 
\label{sec:DS} 


The \Ds{} stores each set in the collection \collection S as a biased skip list.  The weight of each node in each biased skip list is determined by \collection S. When the collection of sets is modified, for instance via a \Union{} operation, in order to reflect this change in the data structure, besides splitting and joining biased skip lists, we need to ensure the weights of the affected nodes are properly updated and biased skip list invariants \invzero{}, \invone{}, and \invtwo{} are preserved. For simplicity we assume that \datast 0 is the collection of singleton sets and \datast i, for all , partitions the universe \universe. This lets us precompute, for each node \node x, \nodeposition{\universe}{x}, the global position of \node x. 
  
For the \Union{} algorithm, we will use the same basic approach outlined in Section~\ref{sec:LogSQ}, the \segment{} merging heuristic, which works by extracting the \segments{} from each set and then gluing them together to form the union of the two sets. 

As previously mentioned, while we do not have control over the number of \segments{} that need to be processed which has been shown to have an amortized lower bound of  per operation, we need to process each \segment{} faster. In order to do so, we depart from balanced search trees and instead use Biased Skip Lists \cite{journals/algorithmica/BagchiBG05} with the extended operations we introduced in Section~\ref{sec:BSL}. 

  
Before we discuss the implementation of each operation in detail, we need to describe the weighting scheme. 

\subsection{Weighting Scheme} 
\label{subsec:DSWeights} 
Let the weight of a node , , be the sum of the sizes of its adjacent gaps. In other words, if  for some node , then we have 
 
Recall that . Observe that this implies for any set \set S, . 

\subsection{The \hide{\Ms{}, }\Find{}, \Src{}, and \Spl{} Operations} 
\label{subsec:DSOthers} 


The \Findx{x} operation can simply return the maximum element of the set which contains  by invoking \Bslfsrcx{i}{+\infty}. The \Srcx{X}{i} operation can be performed by simply invoking \Bslsrcx{X}{i}. The \Splx{X}{i} operation can be performed by simply invoking \Bslsplx{X}{i} and running \Bslfrew{} on one node in each of the resulting biased skip lists to restore the weights. 


\subsection{The \Union{} Operation} 
\label{subsec:DSUnion} 
We will use the \Ds{} in Figure \ref{fig:example} to illustrate the \Union{} operation. \Unionx{A}{B} operation can be viewed as having four essential phases: finding the \segments{}, extracting the \segments{}, updating the weights, and gluing the \segments{}. 
A more detailed description follows. 




\paragraph{Phase I: Finding the \segments{}} 
Assume  w.l.o.g. Let  and . Recall that  is the set of \segments{} associated with the \Unionx{A}{B} operation where  and  are the \ith\ \segment{} of  and  respectively. We have  and . Given \intmin{A_i} and \intmin{B_i}, we find \intmax{A_i} by invoking \Bslfsrcx{\intmin{A_i}}{\intmin{B_i}}. Similarly, given \intmin{B_i} and \intmin{A_{i+1}} we find \intmax{B_i} by invoking \Bslfsrcx{\intmin{B_i}}{\intmin{A_{i+1}}}. Lastly, given \intmax{A_i} and \intmax{B_i}, observe that  and . 
Note that the \suc{}{} operation is performed in constant time in a biased skip list using the lowest successor link of a node. 
At the end of this phase, all the \segments{} are found (see Figure \ref{fig:findints}). Specifically, we have computed for all  and  (\intmin{A_i}, \intmax{A_i}) and (\intmin{B_j}, \intmax{B_j}). 

\paragraph{Phase II: Extracting the \segments{}} 

Since we know where the minimum and maximum node of each \segment{} is from the previous phase, we can extract all the \segments{} easily in order by invoking \Bslfsplx{\intmax{A_i}} for , and \Bslfsplx{\intmax{B_j}} for  (see Figure \ref{fig:extractints}). 



\begin{figure}[H]
\centering 
\includegraphics[scale=1]{phase0} 
\caption{We have two biased skip lists \bsl A (top) and \bsl B (bottom). The tower of each node is represented by a set of vertically stacked squares which represent individual levels of a node. The predecessor/successor pointers are also shown. The lightly shaded levels exist due to promotions. The position of each node \node x, \nodeposition{\universe}{x}, is indicated at the bottom of the tower of each node.} 
\label{fig:example} 
\end{figure}
\begin{figure}[H]
\centering 
\includegraphics[scale=1]{phase1} 
\caption{At the end of Phase~I, we have all the minimum and maximum nodes of the \segments{} of the \Unionx{A}{B} operation. The minimum nodes are colored blue, the maximum nodes are colored red.} 
\label{fig:findints} 
\end{figure}




\paragraph{Phase III: Updating Weights} Next, we need to update the weights of the affected nodes. Let the new weight of item  be . Then 




{\allowdisplaybreaks 

}
We also have 
 
If  is even, we have 
 
If  is odd, we have 
  We can perform these weight updates (see Figure \ref{fig:updateweights}) by invoking 


\begin{itemize} 
\item \Bslfrewx{\intmin{A_i}}{\nodenewweight{\intmin{A_i}}} \quad for  , 
\item \Bslfrewx{\intmax{A_i}}{\nodenewweight{\intmax{A_i}}} \quad for  , 
\item \Bslfrewx{\intmin{B_j}}{\nodenewweight{\intmin{B_j}}} \quad for  , 
\item \Bslfrewx{\intmax{B_j}}{\nodenewweight{\intmax{B_j}}} \quad for  . 
\end{itemize} 




\paragraph{Phase IV: Gluing the \segments{}} Since we assumed w.l.o.g.~that , the correct order of the \segments{} is  by construction. We can glue all the \segments{} by invoking \Bslfjoinx{\intmax{A_{i}}}{\intmin{B_i}} for  and \Bslfjoinx{\intmax{B_{i}}}{\intmin{A_{i+1}}} for  (see Figure \ref{fig:glueints}). 

This concludes the presentation of our data structure. In the next section we will analyze the running time of each of the \opcount{} operations. 


\begin{figure}[H]
\centering 
\includegraphics[scale=1]{phase2} 
\caption{At the end of Phase~II, all the \segments{} are extracted. These extractions cause invariants \invone{} and \invtwo{} to be violated, which are then restored by \Bslfspl{}. Hollow levels denote demotions.} 
\label{fig:extractints} 
\end{figure}

\begin{figure}[H]
\centering 
\includegraphics[scale=1]{phase3} 
\caption{The weights of the minimum and maximum nodes of each \segment{} are affected by the \Unionx{A}{B} operation. Accordingly, in Phase~III, we update the weights of these nodes. This update causes \invzero{} violations. The \Bslfrew{} operation first restores \invzero{}. Restoring \invzero{} causes \invone{} and \invtwo{} violations which are then again restored by \Bslfrew{}. The hollow levels denote demotions and green levels denote promotions.} 
\label{fig:updateweights} 
\end{figure}

\begin{figure}[H]
\centering 
\includegraphics[scale=1]{phase4} 
\caption{In Phase~IV, we join all the \segments{} to obtain the union of \set A and \set B. These joins cause invariants \invone{} and \invtwo{} to be violated, which are then restored by \Bslfjoin{}. The green levels denote promotions.} 
\label{fig:glueints} 
\end{figure} 






\section{Analysis of the Mergeable Dictionary} 
\label{sec:Analysis} 



Before we can analyze the amortized time complexity of the \Ds{} operations, we need a new potential function which we will present in Section~\ref{subsec:AnalysisPotential}. We then prove that all the operations except \Union{} have a worst-case and amortized time complexity of  in Section~\ref{subsec:AnalysisOthers}. Lastly, we will show that the \Union{} operation has an amortized time complexity of  in Section~\ref{subsec:AnalysisUnion}. 


\subsection{The New Potential Function} 
\label{subsec:AnalysisPotential} 


Let \datast i be the data structure containing our dynamic collection of disjoint sets,  after the \ith\ operation. Let 
 
 Then we define the potential after the \ith\ operation as follows. 

where  is a constant to be determined later. 

Note that the main difference between this 
function and the one in Section~\ref{subsec:LogSQPotFun} is the elimination of the  term. 


\subsection{The Analysis of the \hide{\Ms{}, }\Find{}, \Src{}, and \Spl{} Operations} 
\label{subsec:AnalysisOthers} 
We now show that all the operations except \Union{} have a worst-case time  complexity of , and they do not cause a substantial increase in the potential which yields that  their amortized time complexity is also . 

\begin{thm} 
\label{thm:otheropsbound} 
The worst-case and amortized time complexity of the \Findx{x}, \Srcx{S}{x}, and \Splx{S}{x} operations is . 
\end{thm} 

\begin{proof} 
The worst-case time complexity of the \Bslfsrc{} operation invoked by the \Find{} and \Src{} operations is  by Lemma~\ref{lem:logncostofextended}.  Recall that since these operations do not change the structure, the potential remains the same. Therefore, worst-case and amortized time complexity of \Find{} and \Src{} is . 
The worst-case time-complexity of the \Bslspl{} and \Bslrew{} operations invoked by the \Spl{} operation is  by Lemma~\ref{lem:logncostofextended}. Observe that \Spl{} can only decrease the potential. Therefore, the worst-case and amortized time complexity of \Spl{} is . 
\end{proof} 


\subsection{The Analysis of the \Union{} Operation} 
\label{subsec:AnalysisUnion} 
All we have left to do is show that the amortized time complexity of the \Union{} operation is .  In order to do this, first we will show that the worst-case time complexity of the \Unionx{A}{B} operation is . We define \intfun{A_i} and \intfun{B_j} next. 

\begin{defn} 
Consider the \Unionx{A}{B} operation. 
Recall that \nodenewweight{x} is the new weight of node \node x after the \Unionx{A}{B} operation. Also recall that  and . 
Then, for , let 
 
and for , let 
 
For the boundary cases, let . 
\end{defn} 


\subsubsection{The Worst-Case Time Complexity} 
\label{subsubsec:AnalysisUnionWorst} 


We need to bound the worst-case time complexity of each phase of the \Unionx{A}{B} operation. 


\begin{lemma} 
\label{lem:phaseone} 
Phase~I of the \Union{} operation has a worst-case time complexity of  
\end{lemma} 

\begin{proof} 
During Phase~I, \Bslfsrcx{\intmin{A_i}}{t} is invoked for  where  and ;  and \Bslfsrcx{\intmin{B_j}}{s} is invoked for  where  and . Observe that  and . Therefore, by the definition of \intfun{A_i} and \intfun{B_j}, and Lemma~\ref{lem:logncostofextended}, the worst-case time complexity of Phase~I is . 
\end{proof} 

We will need the following lemma to bound the worst-case time complexity of Phase~II-IV. 

\begin{lemma} 
\label{lem:height} 
Given a biased skip list \bsl S and any node , recall that . Then, 
 
\end{lemma} 

\begin{proof} 
Let  and . Also, let . 
Observe that since , due to \invtwo{}, .
This yields

Then we have 
 
\end{proof} 


\begin{lemma} 
\label{lem:phasetwo} 
Phase~II of the \Union{} operation has a worst-case time complexity of  
\end{lemma} 

\begin{proof} 
During Phase~II, \Bslfsplx{\intmax{A_i}} is invoked for , and \Bslfsplx{\intmax{B_j}} is invoked for . 
By Lemma~\ref{lem:wccostofbslfsplit}, 
where ,
and Lemma~\ref{lem:height}, the worst-case time complexity of Phase~II is 
 
 
Observe that  and . Then, by Lemma~\ref{lem:logncostofextended} and the definitions of \intfun{A_i} and \intfun{B_j}, the worst-case complexity of Phase~II is 
 
\end{proof} 


\begin{lemma} 
\label{lem:phasethree} 
Phase~III of the \Union{} operation has a worst-case time complexity of  
\end{lemma} 

\begin{proof} 
During Phase~III, we invoke 

\begin{itemize} 
\item \Bslfrewx{\intmin{A_i}}{\nodenewweight{\intmin{A_i}}} \quad for  , 
\item \Bslfrewx{\intmax{A_i}}{\nodenewweight{\intmax{A_i}}} \quad for  , 
\item \Bslfrewx{\intmin{B_j}}{\nodenewweight{\intmin{B_j}}} \quad for  , 
\item \Bslfrewx{\intmax{B_j}}{\nodenewweight{\intmax{B_j}}} \quad for  . 
\end{itemize} 



Let . If \intnum{A}{B} is even, let , otherwise let . Observe that  if and only if . 
Then, by Lemma~\ref{lem:costofbslfreweight} and Lemma~\ref{lem:height}, \Bslfrewx{x}{\nodenewweight{x}} has a worst-case time complexity of 
\begin{itemize} 
\item  for all 
, 
\item  for all 
, 
\item  for all 
, 
\item  for all 
, 
\end{itemize} 
and a worst-case time complexity of  for . 

Therefore, by Lemma~\ref{lem:logncostofextended} and the definitions of \intfun{A_i} and \intfun{B_j}, the worst-case time complexity of Phase~III is  

\end{proof} 


\begin{lemma} 
\label{lem:phasefour} 
Phase~IV of the \Union{} operation has a worst-case time complexity of  
\end{lemma} 

\begin{proof} 

During Phase~IV, we invoke \Bslfjoinx{\intmax{A_{i}}}{\intmin{B_i}} for  and we invoke \Bslfjoinx{\intmax{B_{i}}}{\intmin{A_{i+1}}} for . 
By Lemma~\ref{lem:wccostofbslfjoin}, where , and Lemma~\ref{lem:height}, the worst-case time complexity of Phase~IV is 
 
 
which by Lemma~\ref{lem:logncostofextended} and the definitions of \intfun{A_i} and \intfun{B_j} yields 
 
\end{proof} 

The next theorem bounds the worst-case time complexity of the \Unionx{A}{B} operation. 




\begin{thm} 
\label{thm:unionworstcase} 
The \Unionx{A}{B} operation has a worst-case time complexity of 
 
\end{thm} 
\begin{proof} 
The worst-case time complexity of the \Unionx{A}{B} operation is determined by the time it spends on each of the four phases. 
Therefore, by Lemmas \ref{lem:phaseone}, \ref{lem:phasetwo}, \ref{lem:phasethree}, and \ref{lem:phasefour}, the theorem follows. 


\end{proof} 



\subsubsection{Amortized Time Complexity} 
\label{subsubsec:AnalysisUnionAmortized} 

Before we can show that the amortized time complexity of the \Unionx{A}{B} operation is , we will need to prove 3 lemmas. Let us first define the potential loss associated with a gap. Recall the definitions of gaps  and similarly  first defined in Section~\ref{subsec:LogSQBound}. 

\begin{defn} 
We define \potlossa i and \potlossb j, the potential loss associated with gap \agap i and the potential loss associated with gap \bgap i respectively, for  and , as follows: 


 
Assume w.l.o.g.~that . Then we also let  and 
 
If , then we have  and 
 
Otherwise, if , we have  and 
 

\end{defn} 

Note that the potential loss associated with operation \Unionx{A}{B} is \cnsd{} times the sum of all defined \potlossa i and \potlossb j, where \cnsd{} is the constant in the potential function. 


\begin{lemma} 
\label{lem:gapratio} 
Consider gap \agap i for any . Let  and . Then we have 
 
\noindent Similarly, consider gap \bgap j for any , where  and . Then we have 
 
\end{lemma} 



\begin{proof} 
We will prove the first part dealing with gap \agap i. The proof of the second part is symmetric. Assume w.l.o.g.~that . By definition, we have
 
Since we have  this yields
 
On the other hand, since , we have
 
Lastly, observe that 
 
and 
 
This concludes the proof. Second part of the lemma can be proven using symmetric arguments. 

\end{proof} 


\begin{lemma} 
\label{lem:costeqpotloss} 
Let  be the set of \segments{} of  with respect to operation \Unionx{A}{B}. For any  and , where  and , let 
 
and 
 
Then for  and , we have 
 
\end{lemma} 


\begin{proof} 


We will present the proof of the first equality. The proof of the second one is analogous. Let . Then, by Lemma~\ref{lem:gapratio} we have 

 
which imply 
 
and 
 
Note that \leftof{a_i} = \rightof{b_{i-1}}, and \rightof{b_{i-2}} = \leftof{a_{i-1}}. By Lemma~\ref{lem:gapratio}, we have 

which by (\ref{eq:robim0}) and (\ref{eq:robim1}) imply 
 
and 
 
Note that \rightof{a_{i-2}} = \leftof{b_{i-2}}, and \leftof{b_{i}} = \rightof{a_{i}}. By Lemma~\ref{lem:gapratio}, we have 
 
which by (\ref{eq:robim2}) and (\ref{eq:robim3}) imply 
 
and 
 
Note that \leftof{a_{i+1}} = \rightof{b_{i}}. By Lemma~\ref{lem:gapratio}, we have 
 
which by (\ref{eq:robim4}) and (\ref{eq:robim5}) implies 
 
Similarly, by Lemma~\ref{lem:gapratio}, we have 
 
which imply 
 
and 
 
By Lemma~\ref{lem:gapratio}, we have 
 
which by (\ref{eq:aim1}) and (\ref{eq:bim1}) imply 
 
and 
 
By Lemma~\ref{lem:gapratio}, we have 
 
which by (\ref{eq:ai}) and (\ref{eq:bi}) imply 
 
and 
 
By Lemma~\ref{lem:gapratio}, we have 
 
which by (\ref{eq:aim2}) implies 
 


We proceed as follows. For , we have 
{\allowdisplaybreaks 
 
} 
The proof of the second equality,  for , is analogous. 

\end{proof} 



\begin{lemma} 
\label{lem:maxmappingbound} 
For  and , we have 
 
\end{lemma} 

\begin{proof} 
Observe that a gap  can be mapped to at most seven times by unique 's and 's; namely only by . Similarly, a gap \bgap{k} can be mapped to at most seven times by unique 's and 's; namely only by . The lemma follows. 
\end{proof} 


We are now ready to bound the amortized time complexity of the \Union{} operation. 


\begin{thm} 
\label{thm:unionamortizedcase} 
The \Unionx{A}{B} operation has an amortized time complexity of . 
\end{thm} 
\begin{proof} 
We will analyze the \Union{} operation using the potential method \cite{amortizedcomplexity}. Recall that \datast i represent the data structure after operation , where  is the initial data structure. The amortized cost of operation  is . Then the amortized cost of the \Unionx{A}{B} operations is 


{\allowdisplaybreaks
}
\end{proof} 
We can now state our main theorem. 

\begin{thm} 
\label{thm:maintheorem} 
The \Ds{} executes a sequence of  \hide{\Ms{}, }\Find{}, \Src{}, \Spl{}, and \Union{} operations \hide{,  of which are \Ms{} operations, }in worst-case  time. 
\end{thm} 

\begin{proof} 
Follows directly from Theorem~\ref{thm:otheropsbound} and Theorem~\ref{thm:unionamortizedcase}. 
\end{proof} 




\bibliographystyle{plain}
\bibliography{lgfussbib}









\end{document}
