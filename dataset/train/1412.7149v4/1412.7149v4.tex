\documentclass[10pt,twocolumn,letterpaper]{article}

\clearpage{}




\newcommand{\choice}[2]{\left(\!\!\! \begin{array}{c} #1 \\ #2\end{array} \!\!\!\right)}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\defeq}{:=}
\newcommand{\real}{\mathbb{R}}


\newcommand{\given}{\|}
\newcommand{\indep}[2]{{#1} \perp {#2}}
\newcommand{\condindep}[3]{{#1} \perp {#2} | {#3}}
\newcommand{\condindepG}[3]{{#1} \perp_G {#2} | {#3}}
\newcommand{\condindepP}[3]{{#1} \perp_p {#2} | {#3}}
\newcommand{\depend}[2]{{#1} \not \perp {#2}}
\newcommand{\conddepend}[3]{{#1} \not \perp {#2} | {#3}}

\newcommand{\trans}[1]{{#1}^{T}}
\newcommand{\inv}[1]{{#1}^{-1}}

\newcommand{\ra}{\rightarrow}
\newcommand{\lra}{\leftrightarrow}
\newcommand{\Ra}{\Rightarrow}
\newcommand{\la}{\leftarrow}
\newcommand{\st}{\; \mathrm{s.t.} \;}
\newcommand{\size}{\mathrm{size}}
\newcommand{\trace}{\mathrm{trace}}

\newcommand{\pemp}{p_\mathrm{emp}}
\newcommand{\dom}{\mathrm{dom}}
\newcommand{\bel}{\mathrm{bel}}
\newcommand{\dsep}{\mathrm{dsep}}
\newcommand{\sep}{\mathrm{sep}}
\newcommand{\entails}{\models}
\newcommand{\range}{\mathrm{range}}
\newcommand{\myspan}{\mathrm{span}}
\newcommand{\nullspace}{\mathrm{nullspace}}
\newcommand{\adj}{\mathrm{adj}}
\newcommand{\pval}{\mathrm{pvalue}}
\newcommand{\NLL}{\mathrm{NLL}}


\newcommand{\betadist}{\mathrm{Beta}}
\newcommand{\Betadist}{\mathrm{Beta}}
\newcommand{\bernoulli}{\mathrm{Ber}}
\newcommand{\Ber}{\mathrm{Ber}}
\newcommand{\Binom}{\mathrm{Bin}}
\newcommand{\NegBinom}{\mathrm{NegBinom}}
\newcommand{\binomdist}{\mathrm{Bin}}
\newcommand{\cauchy}{\mathrm{Cauchy}}
\newcommand{\DE}{\mathrm{DE}}
\newcommand{\DP}{\mathrm{DP}}
\newcommand{\Dir}{\mathrm{Dir}}
\newcommand{\discrete}{\mathrm{Cat}}
\newcommand{\Discrete}{\discrete}
\newcommand{\expdist}{\mathrm{Exp}}
\newcommand{\expon}{\mathrm{Expon}}
\newcommand{\gammadist}{\mathrm{Ga}}
\newcommand{\Ga}{\mathrm{Ga}}
\newcommand{\GP}{\mathrm{GP}}
\newcommand{\GEM}{\mathrm{GEM}}
\newcommand{\gauss}{{\cal N}}
\newcommand{\erlang}{\mathrm{Erlang}}
\newcommand{\IG}{\mathrm{IG}}
\newcommand{\IGauss}{\mathrm{InvGauss}}
\newcommand{\IW}{\mathrm{IW}}
\newcommand{\Laplace}{\mathrm{Lap}}
\newcommand{\logisticdist}{\mathrm{Logistic}}
\newcommand{\Mu}{\mathrm{Mu}}
\newcommand{\Multi}{\mathrm{Mu}}
\newcommand{\NIX}{NI\chi^2}
\newcommand{\GIX}{NI\chi^2}
\newcommand{\NIG}{\mathrm{NIG}}
\newcommand{\GIG}{\mathrm{NIG}}
\newcommand{\NIW}{\mathrm{NIW}}
\newcommand{\GIW}{\mathrm{NIW}}
\newcommand{\MVNIW}{\mathrm{NIW}}
\newcommand{\NW}{\mathrm{NWI}}
\newcommand{\NWI}{\mathrm{NWI}}
\newcommand{\MVNIG}{\mathrm{NIG}}
\newcommand{\NGdist}{\mathrm{NG}}
\newcommand{\prob}{p}
\newcommand{\Poi}{\mathrm{Poi}}
\newcommand{\Student}{{\cal T}}
\newcommand{\student}{{\cal T}}
\newcommand{\Wishart}{\mathrm{Wi}}
\newcommand{\Wi}{\mathrm{Wi}}
\newcommand{\unif}{\mathrm{U}}
\newcommand{\etr}{\mathrm{etr}}


\newcommand{\mse}{\mathrm{mse}}
\newcommand{\pon}{\rho}
\newcommand{\lse}{\mathrm{lse}}
\newcommand{\softmax}{\calS}
\newcommand{\soft}{\mathrm{soft}}
\newcommand{\cond}{\mathrm{cond}}
\newcommand{\sign}{\mathrm{sign}}
\newcommand{\iid}{\mbox{iid}}
\newcommand{\mle}{\mbox{mle}}
\newcommand{\myiff}{\mbox{iff}}
\newcommand{\pd}{\mbox{pd}}
\newcommand{\pdf}{\mbox{pdf }}
\newcommand{\cdf}{\mbox{cdf}}
\newcommand{\pmf}{\mbox{pmf}}
\newcommand{\wrt}{\mbox{wrt}}
\newcommand{\matlab}{{\sc MATLAB}}
\newcommand{\NETLAB}{{\sc NETLAB}}
\newcommand{\MLABA}{\mbox{PMTK}}
\newcommand{\BLT}{\mbox{PMTK}}
\newcommand{\PMTK}{\mbox{PMTK}}
\newcommand{\mywp}{\mathrm{wp}}

\newcommand{\KLpq}[2]{\mathbb{KL}\left({#1}||{#2}\right)}
\newcommand{\KL}{\mathbb{KL}}
\newcommand{\MI}{\mathbb{I}}
\newcommand{\MIxy}[2]{\mathbb{I}\left({#1};{#2}\right)}
\newcommand{\MIxyz}[3]{\mathbb{I}\left({#1};{#2}|{#3}\right)}
\newcommand{\entrop}{\mathbb{H}}
\newcommand{\entropy}[1]{\mathbb{H}\left({#1}\right)}
\newcommand{\entropypq}[2]{\mathbb{H}\left({#1}, {#2}\right)}

\newcommand{\myvec}[1]{\mathbf{#1}}
\newcommand{\myvecsym}[1]{\boldsymbol{#1}}
\newcommand{\ind}[1]{\mathbb{I}(#1)}



\newcommand{\vzero}{\myvecsym{0}}
\newcommand{\vone}{\myvecsym{1}}

\newcommand{\valpha}{\myvecsym{\alpha}}
\newcommand{\vbeta}{\myvecsym{\beta}}
\newcommand{\vchi}{\myvecsym{\chi}}
\newcommand{\vdelta}{\myvecsym{\delta}}
\newcommand{\vDelta}{\myvecsym{\Delta}}
\newcommand{\vepsilon}{\myvecsym{\epsilon}}
\newcommand{\vell}{\myvecsym{\ell}}
\newcommand{\veta}{\myvecsym{\eta}}
\newcommand{\vgamma}{\myvecsym{\gamma}}
\newcommand{\vGamma}{\myvecsym{\Gamma}}
\newcommand{\vmu}{\myvecsym{\mu}}
\newcommand{\vmut}{\myvecsym{\tilde{\mu}}}
\newcommand{\vnu}{\myvecsym{\nu}}
\newcommand{\vkappa}{\myvecsym{\kappa}}
\newcommand{\vlambda}{\myvecsym{\lambda}}
\newcommand{\vLambda}{\myvecsym{\Lambda}}
\newcommand{\vLambdaBar}{\overline{\vLambda}}
\newcommand{\vomega}{\myvecsym{\omega}}
\newcommand{\vOmega}{\myvecsym{\Omega}}
\newcommand{\vphi}{\myvecsym{\phi}}
\newcommand{\vPhi}{\myvecsym{\Phi}}
\newcommand{\vpi}{\myvecsym{\pi}}
\newcommand{\vPi}{\myvecsym{\Pi}}
\newcommand{\vpsi}{\myvecsym{\psi}}
\newcommand{\vPsi}{\myvecsym{\Psi}}
\newcommand{\vtheta}{\myvecsym{\theta}}
\newcommand{\vthetat}{\myvecsym{\tilde{\theta}}}
\newcommand{\vTheta}{\myvecsym{\Theta}}
\newcommand{\vsigma}{\myvecsym{\sigma}}
\newcommand{\vSigma}{\myvecsym{\Sigma}}
\newcommand{\vSigmat}{\myvecsym{\tilde{\Sigma}}}
\newcommand{\vtau}{\myvecsym{\tau}}
\newcommand{\vxi}{\myvecsym{\xi}}

\newcommand{\vmuY}{\vb}
\newcommand{\vmuMu}{\vmu_{x}}
\newcommand{\vmuMuGivenY}{\vmu_{x|y}}
\newcommand{\vSigmaMu}{\vSigma_{x}}
\newcommand{\vSigmaMuInv}{\vSigma_{x}^{-1}}
\newcommand{\vSigmaMuGivenY}{\vSigma_{x|y}}
\newcommand{\vSigmaMuGivenYinv}{\vSigma_{x|y}^{-1}}
\newcommand{\vSigmaY}{\vSigma_{y}}
\newcommand{\vSigmaYinv}{\vSigma_{y}^{-1}}

\newcommand{\muY}{\mu_{y}}
\newcommand{\muMu}{\mu_{\mu}}
\newcommand{\muMuGivenY}{\mu_{\mu|y}}
\newcommand{\SigmaMu}{\Sigma_{\mu}}
\newcommand{\SigmaMuInv}{\Sigma_{\mu}^{-1}}
\newcommand{\SigmaMuGivenY}{\Sigma_{\mu|y}}
\newcommand{\SigmaMuGivenYinv}{\Sigma_{\mu|y}^{-1}}
\newcommand{\SigmaY}{\Sigma_{y}}
\newcommand{\SigmaYinv}{\Sigma_{y}^{-1}}

\newcommand{\hatf}{\hat{f}}
\newcommand{\haty}{\hat{y}}
\newcommand{\const}{\mathrm{const}}
\newcommand{\sigmoid}{\mathrm{sigm}}


\newcommand{\va}{\myvec{a}}
\newcommand{\vb}{\myvec{b}}
\newcommand{\vc}{\myvec{c}}
\newcommand{\vd}{\myvec{d}}
\newcommand{\ve}{\myvec{e}}
\newcommand{\vf}{\myvec{f}}
\newcommand{\vg}{\myvec{g}}
\newcommand{\vh}{\myvec{h}}
\newcommand{\vj}{\myvec{j}}
\newcommand{\vk}{\myvec{k}}
\newcommand{\vl}{\myvec{l}}
\newcommand{\vm}{\myvec{m}}
\newcommand{\vn}{\myvec{n}}
\newcommand{\vo}{\myvec{o}}
\newcommand{\vp}{\myvec{p}}
\newcommand{\vq}{\myvec{q}}
\newcommand{\vr}{\myvec{r}}
\newcommand{\vs}{\myvec{s}}
\newcommand{\vt}{\myvec{t}}
\newcommand{\vu}{\myvec{u}}
\newcommand{\vv}{\myvec{v}}
\newcommand{\vw}{\myvec{w}}
\newcommand{\vws}{\vw_s}
\newcommand{\vwt}{\myvec{\tilde{w}}}
\newcommand{\vWt}{\myvec{\tilde{W}}}
\newcommand{\vwh}{\hat{\vw}}
\newcommand{\vx}{\myvec{x}}
\newcommand{\vxt}{\myvec{\tilde{x}}}
\newcommand{\vy}{\myvec{y}}
\newcommand{\vyt}{\myvec{\tilde{y}}}
\newcommand{\vz}{\myvec{z}}

\newcommand{\vra}{\myvec{r}_a}
\newcommand{\vwa}{\myvec{w}_a}
\newcommand{\vXa}{\myvec{X}_a}


\newcommand{\vA}{\myvec{A}}
\newcommand{\vB}{\myvec{B}}
\newcommand{\vC}{\myvec{C}}
\newcommand{\vD}{\myvec{D}}
\newcommand{\vE}{\myvec{E}}
\newcommand{\vF}{\myvec{F}}
\newcommand{\vG}{\myvec{G}}
\newcommand{\vH}{\myvec{H}}
\newcommand{\vI}{\myvec{I}}
\newcommand{\vJ}{\myvec{J}}
\newcommand{\vK}{\myvec{K}}
\newcommand{\vL}{\myvec{L}}
\newcommand{\vM}{\myvec{M}}
\newcommand{\vMt}{\myvec{\tilde{M}}}
\newcommand{\vN}{\myvec{N}}
\newcommand{\vO}{\myvec{O}}
\newcommand{\vP}{\myvec{P}}
\newcommand{\vQ}{\myvec{Q}}
\newcommand{\vR}{\myvec{R}}
\newcommand{\vS}{\myvec{S}}
\newcommand{\vT}{\myvec{T}}
\newcommand{\vU}{\myvec{U}}
\newcommand{\vV}{\myvec{V}}
\newcommand{\vW}{\myvec{W}}
\newcommand{\vX}{\myvec{X}}
\newcommand{\vXs}{\vX_{s}}
\newcommand{\vXt}{\myvec{\tilde{X}}}
\newcommand{\vY}{\myvec{Y}}
\newcommand{\vZ}{\myvec{Z}}
\newcommand{\vZt}{\myvec{\tilde{Z}}}
\newcommand{\vzt}{\myvec{\tilde{z}}}

\newcommand{\vxtest}{\myvec{x}_*}
\newcommand{\vytest}{\myvec{y}_*}


\newcommand{\ftrue}{f_{true}}

\newcommand{\myprec}{\mathrm{prec}}
\newcommand{\precw}{\lambda_{w}} \newcommand{\precy}{\lambda_{y}} \newcommand{\fbar}{\overline{f}}
\newcommand{\xmybar}{\overline{x}}
\newcommand{\ybar}{\overline{y}}
\newcommand{\rbar}{\overline{r}}
\newcommand{\zbar}{\overline{z}}
\newcommand{\vAbar}{\overline{\vA}}
\newcommand{\vxbar}{\overline{\vx}}
\newcommand{\vXbar}{\overline{\vX}}
\newcommand{\vybar}{\overline{\vy}}
\newcommand{\vYbar}{\overline{\vY}}
\newcommand{\vzbar}{\overline{\vz}}
\newcommand{\vZbar}{\overline{\vZ}}
\newcommand{\xbar}{\overline{x}}
\newcommand{\wbar}{\overline{w}}
\newcommand{\Xbar}{\overline{X}}
\newcommand{\Ybar}{\overline{Y}}
\newcommand{\Gbar}{\overline{G}}
\newcommand{\Jbar}{\overline{J}}
\newcommand{\Lbar}{\overline{L}}
\newcommand{\Nbar}{\overline{N}}
\newcommand{\Qbar}{\overline{Q}}
\newcommand{\Tbar}{\overline{T}}
\newcommand{\Sbar}{\overline{S}}
\newcommand{\vSbar}{\overline{\vS}}
\newcommand{\Rbar}{\overline{R}}

\newcommand{\vtaubar}{\overline{\vtau}}
\newcommand{\vtbar}{\overline{\vt}}
\newcommand{\vsbar}{\overline{\vs}}
\newcommand{\mubar}{\overline{\mu}}
\newcommand{\phibar}{\overline{\phi}}


\newcommand{\htilde}{\tilde{h}}
\newcommand{\vhtilde}{\tilde{\vh}}
\newcommand{\Dtilde}{\tilde{D}}
\newcommand{\Ftilde}{\tilde{F}}
\newcommand{\wtilde}{\tilde{w}}
\newcommand{\ptilde}{\tilde{p}}
\newcommand{\pstar}{p^*}
\newcommand{\xtilde}{\tilde{x}}
\newcommand{\Xtilde}{\tilde{X}}
\newcommand{\ytilde}{\tilde{y}}
\newcommand{\Ytilde}{\tilde{Y}}
\newcommand{\vxtilde}{\tilde{\vx}}
\newcommand{\vytilde}{\tilde{\vy}}
\newcommand{\ztilde}{\tilde{\z}}
\newcommand{\vthetaMAP}{\hat{\vtheta}_{MAP}}
\newcommand{\vthetaS}{\vtheta^{(s)}}
\newcommand{\vthetahat}{\hat{\vtheta}}
\newcommand{\thetahat}{\hat{\theta}}
\newcommand{\thetabar}{\overline{\theta}}
\newcommand{\vthetabar}{\overline{\vtheta}}
\newcommand{\pibar}{\overline{\pi}}
\newcommand{\vpibar}{\overline{\vpi}}

\newcommand{\RSS}{\mathrm{RSS}}
\newcommand{\mydof}{\mathrm{dof}}



\newcommand{\vvec}{\mathrm{vec}}
\newcommand{\kron}{\otimes}
\newcommand{\dof}{\mathrm{dof}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\energy}{E}
\newcommand{\expectAngle}[1]{\langle #1 \rangle}
\newcommand{\expect}[1]{\mathbb{E}\left[ {#1} \right]}
\newcommand{\expectQ}[2]{\mathbb{E}_{{#2}} \left[ {#1} \right]}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\std}[1]{\mathrm{std}\left[{#1}\right]}
\newcommand{\varQ}[2]{\mathrm{var}_{{#2}}\left[{#1}\right]}
\newcommand{\corr}[1]{\mathrm{corr}\left[{#1}\right]}
\newcommand{\mode}[1]{\mathrm{mode}\left[{#1}\right]}
\newcommand{\median}[1]{\mathrm{median}\left[{#1}\right]}




\newcommand{\sech}{\mathrm{sech}}
\newcommand{\kurt}{\mathrm{kurt}}
\newcommand{\proj}{\mathrm{proj}}
\newcommand{\myskew}{\mathrm{skew}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\blkdiag}{\mathrm{blkdiag}}
\newcommand{\bias}{\mathrm{bias}}
\newcommand{\union}{\cup}
\newcommand{\intersect}{\cap}


\newcommand{\myc}{c}
\newcommand{\myi}{i}
\newcommand{\myj}{j}
\newcommand{\myk}{k}
\newcommand{\myn}{n}
\newcommand{\myq}{q}
\newcommand{\mys}{s}
\newcommand{\myt}{t}



\newcommand{\kernelfn}{\kappa}

\newcommand{\Nsamples}{S}
\newcommand{\Ndata}{n}
\newcommand{\Ntrain}{n_{\mathrm{train}}}
\newcommand{\Ntest}{n_{\mathrm{test}}}
\newcommand{\Ndim}{d}
\newcommand{\Ndimx}{d_x}
\newcommand{\Ndimy}{d_y}
\newcommand{\Nhidden}{h}
\newcommand{\Noutdim}{d_y}
\newcommand{\Nlowdim}{l}
\newcommand{\Ndimlow}{l}
\newcommand{\Nstates}{K}
\newcommand{\Nfolds}{K}
\newcommand{\Npastates}{L}
\newcommand{\Nclasses}{C}
\newcommand{\Nclusters}{K}
\newcommand{\NclustersC}{C}
\newcommand{\Ntime}{T}
\newcommand{\Ntimes}{T}
\newcommand{\Niter}{T}
\newcommand{\Nnodes}{D}

\newcommand{\assign}{\leftarrow}







\newcommand{\ki}{i}
\newcommand{\kj}{j}
\newcommand{\kk}{k}
\newcommand{\kC}{C}
\newcommand{\kc}{c}

\newcommand{\supp}{\mathrm{supp}}
\newcommand{\query}{\calQ}
\newcommand{\vis}{\calE}
\newcommand{\nuisance}{\calN}
\newcommand{\hid}{\calH}

\newcommand{\advanced}{*}




\newcommand{\bbI}{\mathbb{I}}
\newcommand{\bbL}{\mathbb{L}}
\newcommand{\bbM}{\mathbb{M}}
\newcommand{\bbS}{\mathbb{S}}


\newcommand{\calA}{{\cal A}}
\newcommand{\calB}{{\cal B}}
\newcommand{\calC}{{\cal C}}
\newcommand{\calD}{{\cal D}}
\newcommand{\calDx}{{\cal D}_x}
\newcommand{\calE}{{\cal E}}
\newcommand{\cale}{{\cal e}}
\newcommand{\calF}{{\cal F}}
\newcommand{\calG}{{\cal G}}
\newcommand{\calH}{{\cal H}}
\newcommand{\calHX}{{\cal H}_X}
\newcommand{\calHy}{{\cal H}_y}
\newcommand{\calI}{{\cal I}}
\newcommand{\calK}{{\cal K}}
\newcommand{\calM}{{\cal M}}
\newcommand{\calN}{{\cal N}}
\newcommand{\caln}{{\cal n}}
\newcommand{\calNP}{{\cal NP}}
\newcommand{\calMp}{\calM^+}
\newcommand{\calMm}{\calM^-}
\newcommand{\calMo}{\calM^o}
\newcommand{\Ctest}{C_*}
\newcommand{\calL}{{\cal L}}
\newcommand{\calU}{{\cal U}}
\newcommand{\calP}{{\cal P}}
\newcommand{\calq}{{\cal q}}
\newcommand{\calQ}{{\cal Q}}
\newcommand{\calR}{{\cal R}}
\newcommand{\calS}{{\cal S}}
\newcommand{\calSstar}{\calS_*}
\newcommand{\calT}{{\cal T}}
\newcommand{\calV}{{\cal V}}
\newcommand{\calv}{{\cal v}}
\newcommand{\calX}{{\cal X}}
\newcommand{\calY}{{\cal Y}}

\newcommand{\Lone}{}
\newcommand{\Ltwo}{}

\newcommand{\score}{\mbox{score}}
\newcommand{\AIC}{\mbox{AIC}}
\newcommand{\BIC}{\mbox{BIC}}
\newcommand{\BICcost}{\mbox{BIC-cost}}
\newcommand{\scoreBIC}{\mbox{score-BIC}}
\newcommand{\scoreBICL}{\mbox{score-BIC-L1}}
\newcommand{\scoreL}{\mbox{score-L1}}

\newcommand{\ecoli}{\mbox{{\it E. coli}}}
\newcommand{\doPearl}{\mathrm{do}}
\newcommand{\data}{\calD}
\newcommand{\model}{\calM}
\newcommand{\dataTrain}{\calD_{\mathrm{train}}}
\newcommand{\dataTest}{\calD_{\mathrm{test}}}
\newcommand{\dataValid}{\calD_{\mathrm{valid}}}
\newcommand{\Xtrain}{\vX_{\mathrm{train}}}
\newcommand{\Xtest}{\vX_{\mathrm{test}}}
\newcommand{\futuredata}{\tilde{\calD}}
\newcommand{\algo}{\calA}
\newcommand{\fitAlgo}{\calF}
\newcommand{\predictAlgo}{\calP}
\newcommand{\err}{\mathrm{err}}
\newcommand{\logit}{\mathrm{logit}}

\newcommand{\nbd}{\mathrm{nbd}}
\newcommand{\anc}{\mathrm{anc}}
\newcommand{\desc}{\mathrm{desc}}
\newcommand{\pred}{\mathrm{pred}}
\newcommand{\mysucc}{\mathrm{suc}}
\newcommand{\nondesc}{\mathrm{nd}}
\newcommand{\pa}{\mathrm{pa}}
\newcommand{\parent}{\mathrm{pa}}
\newcommand{\copa}{\mathrm{copa}}
\newcommand{\ch}{\mathrm{ch}}
\newcommand{\mb}{\mathrm{mb}}
\newcommand{\connects}{\sim}
\newcommand{\nd}{\mathrm{nd}}
\newcommand{\bd}{\mathrm{bd}}
\newcommand{\cl}{\mathrm{cl}}



\newcommand{\be}{}
\newcommand{\bea}{}
\newcommand{\beaa}{}



\newcommand{\conv}[1]{\,\,\,\displaystyle{\operatorname*{\longrightarrow}^{\,_{#1}\,}}\,\,\,}
\newcommand{\dconv}{\conv{D}}
\newcommand{\pconv}{\conv{P}}
\newcommand{\asconv}{\conv{AS}}
\newcommand{\lpconv}[1]{\conv{L^{#1}}}

\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{n}




\newcommand{\vfj}{\vf_j}
\newcommand{\vfk}{\vf_k}

\newcommand{\entropyBethe}{\mathbb{H}_{\mathrm{Bethe}}}
\newcommand{\entropyKikuchi}{\mathbb{H}_{\mathrm{Kikuchi}}}
\newcommand{\entropyEP}{\mathbb{H}_{\mathrm{ep}}}
\newcommand{\entropyConvex}{\mathbb{H}_{\mathrm{Convex}}}

\newcommand{\freeEnergyBethe}{F_{\mathrm{Bethe}}}
\newcommand{\freeEnergyKikuchi}{F_{\mathrm{Kikuchi}}}
\newcommand{\freeEnergyConvex}{F_{\mathrm{Convex}}}

\newcommand{\sigmaMle}{\hat{\sigma}^2_{mle}}
\newcommand{\sigmaUnb}{\hat{\sigma}^2_{unb}}


\newcommand{\keywordSpecial}[2]{{\bf #1}\index{keywords}{#2@#1}}
\newcommand{\bfidx}[1]{{\bf #1}}
\newcommand{\keywordDefSpecial}[2]{{\bf #1}\index{keywords}{#2@#1|bfidx}}




\newcommand{\keywordDef}[1]{{\emph{#1}}}
\clearpage{} 


\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{url}
\usepackage{amsmath,amsthm,amssymb,amsfonts,amsthm}
\usepackage{wrapfig}
\usepackage{subfigure}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{mdwlist}
\usepackage{pbox}
\usepackage{fancyhdr}


\newtheorem{mydefinition}{Definition}
\newtheorem{mydefinition1}{Definition}
\newtheorem{mydefinition2}{Definition}
\newtheorem{mydefinition3}{Definition}
\newtheorem{mydefinition4}{Definition}
\newtheorem{mydefinition5}{Definition}
\newtheorem{mydefinition6}{Definition}

\newtheorem{lemma}[mydefinition1]{Lemma}
\newtheorem{theorem}[mydefinition2]{Theorem}
\newtheorem{remark}[mydefinition3]{Remark}
\newtheorem{corollary}[mydefinition4]{Corollary}
\newtheorem{assumption}[mydefinition5]{Assumption}
\newtheorem{proposition}[mydefinition6]{Proposition}

\DeclareMathOperator*{\argmax}{\mathrm{argmax}}
\DeclareMathOperator*{\argmin}{\mathrm{argmin}}
\DeclareMathOperator*{\sgn}{\mathrm{sgn}}
\DeclareMathOperator*{\tr}{\mathrm{tr}}
\DeclareMathOperator*{\cov}{\mathrm{Cov}}
\DeclareMathOperator*{\var}{\mathrm{Var}}
\DeclareMathOperator*{\mini}{\mathrm{minimize}}
\DeclareMathOperator*{\maxi}{\mathrm{maximize}}

\newcommand{\fixme}[1]{{\color{red} [TODO: #1]}}

\newcommand{\eq}[1]{(\ref{#1})}
\newcommand{\mymatrix}[2]{\left[\begin{array}{#1} #2 \end{array}\right]}

\newcommand{\inner}[2]{\left\langle #1,#2 \right\rangle}
\newcommand{\rbr}[1]{\left(#1\right)}
\newcommand{\sbr}[1]{\left[#1\right]}
\newcommand{\cbr}[1]{\left\{#1\right\}}
\newcommand{\nbr}[1]{\left\|#1\right\|}
\newcommand{\abr}[1]{\left|#1\right|}
\newcommand{\ceil}[1]{\left\lceil{#1}\right\rceil}
\newcommand{\floor}[1]{\left\lfloor{#1}\right\rfloor}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}

\newcommand{\one}{\mathbf{1}}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}




\usepackage{hyperref}

\hypersetup{
  pagebackref=true,
  breaklinks=true,
  letterpaper=true,
  colorlinks,
  bookmarks=false
}

\iccvfinalcopy 

\def\iccvPaperID{1700} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi
\begin{document}

\title{Deep Fried Convnets}



\author{
  Zichao Yang \quad Marcin Moczulski \quad Misha Denil\\
  {Nando de Freitas} \quad {Alex Smola} \quad {Le Song}
  \quad {Ziyu Wang} \\vH_2 := \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix} \mbox{  and }
\vH_{2d} := \begin{bmatrix} \vH_d & \vH_d \\ \vH_d & -\vH_d \end{bmatrix}.
  \frac{\partial E}{\partial \vS} = \diag \cbr{\frac{\partial E}{\partial \vh_{l+1}}
  (\vH \vG \vPi \vH \vB \vh_{l})^{\top}}.

  \frac{\partial E}{\partial \vh_{S}} & = \vS^{\top}\frac{\partial
                                      E}{\partial \vh_{l+1}} &
  \frac{\partial E}{\partial \vh_{H1}} & = \vH^{\top}\frac{\partial E}{\partial
    \vh_{S}} \nonumber \\
  \frac{\partial E}{\partial \vG} & = \diag \cbr{\frac{\partial E}{\partial \vh_{H1}}
    \vh_{G}^{\top}} &
  \frac{\partial E}{\partial \vh_{G}} & = \vG^{\top}\frac{\partial E}{\partial
    \vh_{H1}} \nonumber \\
  \frac{\partial E}{\partial \vh_{\Pi}} & = \vPi^{\top} \frac{\partial
                                        E}{\partial \vh_{G}} &
  \frac{\partial E}{\partial \vh_{H2}} & = \vH^{\top}\frac{\partial E}{\partial
    \vh_{\Pi}} \nonumber \\
  \frac{\partial E}{\partial \vB} & = \diag \cbr{\frac{\partial E}{\partial
    \vh_{H2}} \vh_{l}^{\top} } &
  \frac{\partial E}{\partial \vh_{l}} & = \vB^{\top}\frac{\partial E}{\partial
    \vh_{H2}}.

    & k(\vx,\vx') =  \int \alpha e^{-i\vw^\top(\vx-\vx')} \,p(\vw) d\vw \\
    & = \alpha \E_\vw[\cos(\vw^\top\vx)\cos(\vw^\top\vx') 
      + \sin(\vw^\top\vx)\sin(\vw^\top\vx') ],

\vphi_\mathrm{rbf}(\vW\vx) = \sqrt{\alpha/n}\rbr{\cos(\vW\vx), \sin(\vW\vx)}^\top.
\label{eqn:feature}

  k(\vx, \vx^{\prime}) = \exp\rbr{-\frac{\nbr{\vx-\vx'}^2}{2\ell^2}},

  \vphi_{\mathrm{relu}}(\vW \vx) & = \sqrt{1/n}\max(0, \vW \vx)
  \label{eq:relu-feature}.

  \vW = \vU\vS\vV^{\top},

where  and  and  is a  diagonal matrix. In order to reduce the parameters, we truncate all but the  largest singular values, leading to the approximation

where  and  and  has been absorbed into the other two factors.  If  is sufficiently small then storing  and  is less expensive than storing  directly, and this parameterization is still learnable.



It has been shown that training a factorized representation directly leads to poor performance~\cite{DenilSDRF13} (although it does work when applied only to the final logistic regression layer~\cite{SainathKSAR13}).  However, first training a full model, then preforming an SVD of the weight matrices followed by a fine tuning phase preserves much of the performance of the original model~\cite{XueLG13}.  We compare our deep fried approach to SVD followed by fine tuning, and show that our approach achieves better performance per parameter in spite of training a compressed parameterization from scratch.  We also compare against a post-processed version of our model, where we train a deep fried convnet and then apply SVD plus fine-tuning to the final softmax layer, which further reduces the number of parameters.  


Results of these post-processing experiments are shown in Table~\ref{tab:imagenet_svd}. For the SVD decomposition of each of the three fully connected
layers in the reference model we set  in SVD-half and
 in SVD-quarter. SVD-half-F and SVD-quarter-F mean that the
model has been fine tuned after the decomposition.

There is 1\% drop in accuracy for SVD-half and 3.5\%
drop for SVD-quarter. Even though the increase in the error for the SVD can be mitigated by finetuning (the drop decreases to 0.1\% for SVD-half-F and 1.3\% for
SVD-quarter-F), deep fried convnets still perform better both in terms of the accuracy and the number of parameters.

Applying a rank 600 SVD followed by fine tuning to the final softmax layer of the Adaptive Fastfood 32 model removes an additional 12.5M parameters at the expense of 0.7\% top-1 error.

For reference, we also include the results of Collins and Kohli~\cite{Collins2014}, who pre-train a full network and use a sparsity regularizer during fine-tuning to encourage connections in the fully connected layers to be zero.  They are able to achieve a significant reduction in the number of parameters this way, however the performance of their compressed network suffers when compared to the reference model.  Another drawback of this method is that using sparse weight matrices requires additional overhead to store the indexes of the non-zero values.  The index storage takes up space and using sparse representation is better than using a dense matrix
  only when number of nonzero entries is small.


\begin{table}
  \centering
  \begin{tabular}{l|r|r|r}
    Model & Error & Params & Ratio \\
    \hline
    Collins and Kohli~\cite{Collins2014} & 44.40\% & --- & ---\\
    SVD-half & 43.61\% & 46.6M & 0.8 \\
    SVD-half-F & 42.73\% &  46.6M & 0.8 \\
    Adaptive Fastfood 32 & \textbf{41.93\%} & 32.8M & 0.55\\
    SVD-quarter&  46.12\% & 23.4M  & 0.5 \\
    SVD-quarter-F & 43.81\%  & 23.4M & 0.5 \\
    Adaptive Fastfood 16 & 42.90\% & 16.4M & 0.28 \\
    \hline
    Ada. Fastfood 32 (F-600) & 42.61\% & 20.3M & 0.35 \\
\hline
    Reference Model & 42.59\% & 58.7M & 1\\
  \end{tabular}
  \caption{Comparison with other methods.
    The result of \cite{Collins2014} is based on
    the the Caffe AlexNet model (similar but not identical to the Caffe reference
    model) and achieves 4x reduction in memory usage,
    (slightly better than Fastfood 16 but with a noted drop in performance).
    SVD-half: 9216-2048-4096-2048-4096-500-1000 structure.
    SVD-quarter: 9216-1024-4096-1024-4096-250-1000 structure.
    \texttt{F} means after fine tuning.}
\label{tab:imagenet_svd}
\end{table}




\section{Conclusion}

Many methods have been advanced to reduce the size of convolutional networks at test time. In contrast to this trend, the Adaptive Fastfood transform introduced in this paper is end-to-end differentiable and hence it enables us to attain reductions in the number of parameters even at train time.

Deep fried convnets capitalize on the proposed Adaptive Fastfood transform to achieve a substantial reduction in the number of parameters without sacrificing predictive performance on MNIST and ImageNet. They also compare favorably against simple test-time low-rank matrix factorization schemes.

Our experiments have also cast some light on the issue of random versus adaptive weights. The structured random transformations developed in the kernel literature perform very well on MNIST without any learning; however, when moving to ImageNet, the benefit of adaptation becomes clear, as it allows us to achieve substantially better performance.  This is an important point which illustrates the importance of learning which would not have been visible from experiments only on small data sets.

The Fastfood transform allows for a theoretical reduction in computation from  to . However, the computation in convolutional neural networks is dominated by the convolutions, and hence deep fried convnets are not necessarily faster in practice.


It is clear looking at out results on ImageNet in Table~2 that the remaining parameters are mostly in the output softmax layer. The comparative experiment in Section~7 showed that the matrix of parameters in the softmax can be easily compressed using the SVD, but many other methods could be used to achieve this. One avenue for future research involves replacing the softmax matrix, at train and test times, using the abundant set of techniques that have been proposed to solve this problem, including low-rank decomposition, Adaptive Fastfood, and pruning.

The development of GPU optimized Fastfood transforms that can be used to replace linear layers in arbitrary neural models would also be of great value to the entire research community given the ubiquity of fully connected layers layers.





{\small
\bibliographystyle{ieee}
\bibliography{deepBib,embedding}
}



\end{document}
