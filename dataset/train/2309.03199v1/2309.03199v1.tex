\pdfoutput=1
\RequirePackage{fix-cm}
\documentclass[british]{article}

\usepackage{spconf} 

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[british]{babel}





\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}

\newcommand{\tqdb}{\textquotedbl} 





\usepackage{graphicx}
\usepackage{xcolor}
\graphicspath{{./figures/}}
\DeclareGraphicsExtensions{.pdf,.png,.jpeg,.jpg}

\usepackage{amsmath} \usepackage{amssymb}
\interdisplaylinepenalty=2500 

\usepackage{bm} \usepackage{esint} \usepackage{commath} \usepackage{nicefrac} 



\usepackage{subcaption} 





\usepackage{booktabs} \usepackage{array} 

\usepackage{multirow} \usepackage{rotating} 

\usepackage{enumitem} \setlist{nolistsep} 

\usepackage[unicode=true,pdfusetitle,pdfauthor={Shivam Mehta, Ruibo Tu, Jonas Beskow, Éva Székely, Gustav Eje Henter},pdfkeywords={diffusion models, flow matching, speech synthesis, text-to-speech, acoustic modelling},bookmarks=true,bookmarksnumbered=true,bookmarksopen=false,breaklinks=true,pdfborder={0 0 0},backref=false,colorlinks=true,citecolor=blue]{hyperref}

\usepackage[capitalise]{cleveref} \Crefname{section}{Section}{Sections}
\crefname{section}{Sec.}{Secs.}
\Crefname{figure}{Figure}{Figures}
\crefname{figure}{Fig.}{Figs.}
\Crefname{table}{Table}{Tables}
\crefname{table}{Table}{Tabs.}

\newcommand{\bb}[1]{\boldsymbol{#1}}
\newcommand{\seq}[1]{\underline{#1}}
\newcommand{\est}[1]{\widehat{#1}}
\newcommand{\given}[2]{\left(#1\,\middle|\,#2\right)}
\newcommand{\givenpar}[3]{\left(#1\,\middle|\,#2;\,#3\right)}

\renewcommand{\mid}{\,\ifnum\currentgrouptype=16 \middle\fi|\,}

\newcommand{\prob}{\mathbb{P}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\mean}{\mathbb{E}}
\newcommand{\var}{\mathup{Var}}
\newcommand{\cov}{\mathup{Cov}}

\newcommand{\x}{\boldsymbol{x}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\data}{\mathcal{D}}
\newcommand{\model}{\mathcal{M}}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\diag}{diag}

\newcommand{\customurl}[1]{\url{#1}} 

\newcommand{\webpageurl}{https://shivammehta25.github.io/Matcha-TTS/}
\newcommand{\webpageurltext}{shivammehta25.github.io/Matcha-TTS/}

\ninept \setlength\tabcolsep{5pt} 

\newcommand{\tablebf}[1]{\pdfliteral direct {2 Tr 0.5 w}#1\pdfliteral direct {0 Tr 0 w}}

\let\oldmarginpar\marginpar
\renewcommand\marginpar[1]{\-\oldmarginpar[\raggedleft\footnotesize #1]{\raggedright\footnotesize #1}}





\pagestyle{plain} 











\title{Matcha-TTS: A fast TTS architecture with conditional flow matching}


\name{Shivam Mehta, Ruibo Tu, Jonas Beskow, {\'E}va Sz{\'e}kely, Gustav Eje Henter\thanks{This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation
and by the Industrial Strategic Technology Development Program (grant no. 20023495) funded by MOTIE, Korea.}}
\address{Division of Speech, Music and Hearing, KTH Royal Institute of Technology, Stockholm, Sweden}


\begin{document}
\maketitle
\begin{abstract}
We introduce Matcha-TTS, a new encoder-decoder architecture for speedy TTS acoustic modelling, trained using optimal-transport conditional flow matching (OT-CFM). This yields an ODE-based decoder capable of high output quality in fewer synthesis steps than models trained using score matching. Careful design choices additionally ensure each synthesis step is fast to run. The method is probabilistic, non-autoregressive, and learns to speak from scratch without external alignments. Compared to strong pre-trained baseline models, the Matcha-TTS system has the smallest memory footprint, rivals the speed of the fastest models on long utterances, and attains the highest mean opinion score in a listening test.
\end{abstract}
\begin{keywords}Diffusion models, flow matching, speech synthesis, text-to-speech, acoustic modelling
\end{keywords}
\section{Introduction}
\label{sec:intro}


Diffusion probabilistic models (DPMs) (cf.\ \cite{song2019generative}) are currently setting new standards in deep generative modelling on continuous-valued data-generation tasks such as image synthesis \cite{dhariwal2021diffusion,rombach2022high}, motion synthesis \cite{alexanderson2023listen,mehta2023diff}, and speech synthesis \cite{chen2021wavegrad, chen2021wavegrad2, popov2021grad, jeong2021diff, kongdiffwave} -- the topic of this paper.
DPMs define a diffusion process which transforms the \emph{data} (a.k.a.\ \emph{target}) distribution to a \emph{prior} (a.k.a.\ \emph{source}) distribution, e.g., a Gaussian.
They then learn a \emph{sampling process} that reverses the diffusion process.
The two processes can be formulated as forward- and reverse-time stochastic differential equations (SDEs) \cite{song2021score}.
Solving a reverse-time SDE initial value problem  generates samples from the learnt data distribution.
Furthermore, each reverse-time SDE has a corresponding ordinary differential equation (ODE), called the \emph{probability flow ODE} \cite{song2021score,albergo2022building}, which describes (and samples from) the exact same distribution as the SDE.
The probability flow ODE is a deterministic process for turning source samples into data samples,
similar to continuous-time normalising flows (CNF) \cite{chen2018neural}, but without the need to backpropagate through expensive ODE solvers or approximate the reverse ODE using adjoint variables \cite{chen2018neural}.


The SDE formulation of DPMs is trained by approximating the score function (the gradients of the log probability density) of the data distribution \cite{song2021score}.
The training objective takes the form of a mean squared error (MSE) which can be derived from an evidence lower bound (ELBO) on the likelihood.
This is fast and simple and, unlike typical normalising flow models, does not impose any restrictions on model architecture.
But whilst they allow efficient training without numerical SDE/ODE solvers, DPMs suffer from slow synthesis speed, since each sample requires numerous iterations (steps), computed in sequence, to accurately solve the SDE.
Each such step requires that an entire neural network be evaluated.
This slow synthesis speed has long been the main practical issue with DPMs.











This paper introduces \emph{Matcha-TTS}\footnote{We call our approach Matcha-TTS because it uses flow matching for TTS, and because the name sounds similar to ``matcha tea'', which some people prefer over Taco(tron)s.}, a probabilistic and non-autoregressive, fast-to-sample-from TTS acoustic model based on continuous normalising flows.
There are two main innovations:
\begin{enumerate}
\item We propose an improved encoder-decoder TTS architecture that uses a combination of 1D CNNs and Transformers in the decoder. This reduces memory consumption and is fast to evaluate, improving synthesis speed.
\item Second, we train these models using optimal-transport conditional flow matching (OT-CFM) \cite{lipman2023flow},
which is a new method to learn ODEs that sample from a data distribution.
Compared to conventional CNFs and score-matching probability flow ODEs, OT-CFM defines simpler paths from source to target, enabling accurate synthesis in fewer steps than DPMs.
\end{enumerate}
Experimental results demonstrate that both innovations accelerate synthesis, reducing the trade-off between speed and synthesis quality.
Despite being fast and lightweight, Matcha-TTS learns to speak and align without requiring an external aligner.
Compared to strong pre-trained baseline models, Matcha-TTS achieves fast synthesis with better naturalness ratings.
Audio examples and code are provided at \href{\webpageurl}{\webpageurl}.


\section{Background}
\label{sec:background}
\subsection{Recent encoder-decoder TTS architectures}


DPMs have been applied to numerous speech synthesis tasks with impressive results, including waveform generation \cite{chen2021wavegrad,kongdiffwave} and end-to-end TTS \cite{chen2021wavegrad2}.
Diff-TTS \cite{jeong2021diff} was the first to apply DPMs for acoustic modelling.
Shortly after, Grad-TTS \cite{popov2021grad} conceptualised the diffusion process as an SDE.
This was swiftly adopted by other models like Grad-StyleSpeech \cite{kang2023gradstylespeech} and Emo-Diff \cite{guo2023emodiff}, both performing few-/one-shot synthesis using an additional reference encoder.
TorToiSe \cite{betker2023better} demonstrated DPMs for transforming quantised latents from an autoregressive model into acoustic features.


The above models -- like many modern TTS acoustic models -- use an encoder-decoder architecture with Transformer blocks
in the encoder.
Many models, e.g., FastSpeech 1 and 2 \cite{ren2019fastspeech,ren2021fastspeech2},
use sinusoidal position embeddings for positional dependences.
This has been found to generalise poorly to longer sequence lengths; cf.\ \cite{press2022train}.
Glow-TTS \cite{kim2020glow}, VITS \cite{kim2021vits}, and Grad-TTS instead use relative positional embeddings \cite{shaw2018self}.
Unfortunately, these do not differentiate between inputs beyond a threshold window length, often resulting in unnatural prosody.
LinearSpeech \cite{zhang2021linearspeech} instead employed rotational position embeddings (RoPE) \cite{su2021roformer}, which have computational and memory advantages over relative embeddings and generalise to longer distances \cite{wennberg2021case,press2022train}.
Matcha-TTS thus uses Transformers with RoPE
in the encoder, reducing memory use compared to Grad-TTS.
To the best of our knowledge, ours is the first SDE or ODE-based TTS method to use RoPE.


Modern TTS architectures also differ in terms of decoder network design.
The normalising-flow based methods Glow-TTS \cite{kim2020glow} and OverFlow \cite{mehta2023overflow} use dilated 1D-convolutions.
DPM-based methods like Diff-TTS \cite{jeong2021diff}, ProDiff \cite{huang2022prodiff}, and WaveGrad \cite{chen2021wavegrad} likewise use 1D convolutions to synthesise mel spectrograms.
Grad-TTS \cite{popov2021grad}, in contrast, 
uses a U-Net with 2D-convolutions, thereby treating mel spectrograms as images and implicitly assuming translation invariance in both time and frequency.
However, speech mel-spectra are not fully translation-invariant along the frequency axis, and 2D decoders also generally require more memory as they introduce an extra dimension to the tensors.
Meanwhile, non-probabilistic models like FastSpeech 1 and 2
have demonstrated that (1D) Transformers can be used in the decoder, to learn long-range dependencies and fast, parallel synthesis.
Matcha-TTS also uses Transformers in the decoder, but in a 1D U-Net design inspired by the 2D U-Nets in the Stable Diffusion image-generation model \cite{rombach2022high}.

Whilst some TTS systems, e.g., FastSpeech \cite{ren2019fastspeech}, rely on externally-supplied alignments, most systems are capable of learning to speak and align at the same time,
although it has been found to be important to encourage or enforce monotonic alignments
\cite{watts2019where,mehta2022neuralhmm}
for fast and effective training.
One mechanism for this is monotonic alignment search (MAS), used by methods such as Glow-TTS \cite{kim2020glow} and VITS \cite{kim2021vits}.
Grad-TTS \cite{popov2021grad}, in particular, uses a MAS-based mechanism
which they term \emph{prior loss} to quickly learn to align input symbols with output frames.
These alignments are also used to train a deterministic duration predictor minimising MSE in the log domain.
Matcha-TTS uses these same methods for alignment and duration modelling.
Finally, Matcha-TTS uses \emph{snake beta} activations from BigVGAN \cite{lee2023bigvgan}
in all feedforward layers of the Transformer decoder.














\subsection{Flow matching and TTS}


Currently, some of the highest-quality TTS systems either utilise DPMs
\cite{popov2021grad, betker2023better}
or discrete-time normalising flows \cite{kim2021vits,mehta2023overflow}, with continuous-time flows being less explored.
Lipman et al.\ \cite{lipman2023flow} recently introduced a framework for synthesis using ODEs that unifies and extends probability flow ODEs and CNFs.
They were then able to present an efficient approach to learn ODEs for synthesis, using a simple vector-field regression loss called \emph{conditional flow matching} (CFM), as an alternative to learning score functions for DPMs or using numerical ODE solvers at training time like classic CNFs \cite{chen2018neural}.
Crucially, by leveraging ideas from optimal transport, CFM can be set up to yield ODEs that have simple vector fields that change little during the process of mapping samples from the source distribution onto the data distribution, since it essentially just transports probability mass along straight lines.
This technique is called OT-CFM.
The simple paths means that the ODE can be solved accurately using few discretisation steps, i.e., accurate model samples can be drawn with a lower number of function evaluations (NFEs; equivalent to the number of neural-network evaluations) than DPMs, enabling much fewer synthesis steps for the same quality.







CFM is a new technique and has to our knowledge only been used for speech synthesis by the Voicebox model from Meta  \cite{le2023voicebox}.
Voicebox (VB) is a system that performs various text-guided speech-infilling tasks based on large-scale training data, with its English variant (VB-En) being trained on 60k hours of proprietary data.
VB differs substantially from Matcha-TTS:
VB performs TTS, denoising, and text-guided acoustic infilling trained using a combination of masking and CFM, whereas Matcha-TTS is a pure TTS model trained solely using OT-CFM.
VB uses convolutional positional encoding with AliBi self-attention bias, whilst our text encoder uses RoPE.
We train on standard data and make code and models publicly available, whilst VB is proprietary.
VB-En consumes 330M parameters, which is 18 times larger than the Matcha-TTS model in our experiments.
Also, VB uses external alignments for training whereas Matcha-TTS learns to speak and align without them.



\section{Method}
\label{sec:method}
We now outline flow matching training (in \cref{ssec:cfm}) and then (in \cref{ssec:matcha}) give details on our proposed TTS architecture.

\subsection{Optimal-transport conditional flow matching}
\label{ssec:cfm}
We here give a high-level overview to flow matching by first introducing the probability density path generated by a vector field and the flow matching and CFM objectives.
We then write down the OT-CFM objective used in our proposed method.
Our exposition mainly follows the notation and definitions from \cite{lipman2023flow}.

Let 
 denote an observation in the data space , sampled from a complicated, unknown data distribution . 
A \emph{probability density path} is a time-dependent probability density function, .
One way to generate samples from the data distribution  is to construct a probability density path , where  and  is a prior distribution, such that  approximates the data distribution .
For example, CNFs first define a vector field , which generates the flow  through the ODE 

This generates the path  as the marginal probability distribution of the data points.
We can sample from the approximated data distribution  by solving the initial value problem Eq.\ \eqref{eq:ode}. 




Suppose there exists a known vector field  that generates a probability path  from  to .
The flow matching loss is

where  and  is a neural network with parameters .
Nevertheless, flow matching is intractable in practice because it is non-trivial to get access to the vector field  and the target probability .
Therefore, conditional flow matching instead considers

This replaces the intractable marginal probability densities and the vector field with conditional probability densities and conditional vector fields. Crucially, these are in general tractable and have closed-form solutions, and one can furthermore show that  and  both have identical gradients with respect to  \cite{lipman2023flow}.
\begin{figure}[!t]
\centering
\includegraphics[width=\columnwidth]{architectural_overview.pdf}
\caption{Overview of the proposed approach at synthesis time.}
\label{fig: model architecture overview}
\vspace{-\baselineskip}
\end{figure}

Matcha-TTS is trained using optimal-transport conditional flow matching (OT-CFM) \cite{lipman2023flow}, which is a CFM variant with particularly simple gradients.
The Matcha-TTS loss function can be written

defining  as the flow from  to  where each datum  is matched to a random sample  as in \cite{lipman2023flow}.
Its gradient vector field -- the target for the learning -- is then , which is linear, time-invariant, and only depends on the values of  and .
These properties enable easier and faster training, faster generation, and better performance compared to DPMs.


In the case of Matcha-TTS,  are acoustic frames and  are the conditional mean values of those frames, predicted from text using the architecture described in the next section.
 is a hyperparameter with a small value (\texttt{1e-4} in our experiments).








\subsection{Proposed architecture}
\label{ssec:matcha}


Matcha-TTS is a non-autoregressive encoder-decoder architecture for neural TTS.
An overview of the architecture is provided in \cref{fig: model architecture overview}.
Text encoder and duration predictor architectures follow \cite{kim2020glow,popov2021grad}, but use rotational position embeddings \cite{su2021roformer} instead of relative ones.
Alignment and duration-model training follow use MAS and the prior loss  as described in \cite{popov2021grad}.
The predicted durations, rounded up, are used to upsample (duplicate) the vectors output by the encoder to obtain , the predicted average acoustic features (e.g., mel-spectrogram) given the text and the chosen durations.
This mean is used to condition the decoder that predicts the vector field  used for synthesis, but is not used as the mean for the initial noise samples  (unlike Grad-TTS).

The Matcha-TTS decoder architecture is shown in \cref{fig: decoder architecture}.
Inspired by \cite{rombach2022high}, it is a U-Net containing 1D convolutional residual blocks to downsample and upsample the inputs, based on an embedding of the flow matching step , in our experiments computed by a network comprising two affine layers stacked.
Each residual block is followed by a Transformerblock, whose feedforward nets use snake beta activations \cite{lee2023bigvgan}.
These Transformers do not use any position embeddings, since between-phone positional information already has been baked in by the encoder, and the convolution and downsampling operations act to interpolate these between frames within the same phone and distinguish their relative positions from each other.
This decoder network is significantly faster to evaluate and consumes less memory than the 2D convolutional-only U-Net used by Grad-TTS \cite{popov2021grad}.
\begin{figure}[!t]
\centering
\includegraphics[width=0.65\columnwidth]{decoder_architecture.pdf}
\caption{Matcha-TTS decoder (the flow prediction network in \cref{fig: model architecture overview}).}
\label{fig: decoder architecture}
\vspace{-\baselineskip}
\end{figure}













\section{Experiments}
\label{sec:experiments}
To validate the proposed approach we compared it to three pre-trained baselines in several experiments, including a listening test.
All experiments were performed on NVIDIA RTX 3090 GPUs.
See \href{\webpageurl}{\webpageurltext} for audio and code.

\subsection{Data and systems}
\label{ssec:systems}


We performed our experiments on the standard split of the LJ Speech dataset\footnote{\customurl{https://keithito.com/LJ-Speech-Dataset/}} (a female US English native speaker reading public-domain texts), training a version of the Matcha-TTS architecture on this data.
We used the same encoder and duration predictor (i.e., the same hyperparameters) as \cite{popov2021grad}, just different position embeddings in the encoder.
Our trained flow prediction network (decoder) used two downsampling blocks, followed by two midblocks and two upsampling blocks, as shown in \cref{fig: decoder architecture}.
Each block had one Transformer layer with hidden dimensionality 256, attention dimensionality 64, and `snakebeta' activations \cite{lee2023bigvgan}.
Phonemizer\footnote{\customurl{https://github.com/bootphon/phonemizer}} \cite{bernard2021phonemizer} with the \texttt{espeak-ng} backend was used to convert input graphemes to IPA phones.
We trained for 500k iterations on 2 GPUs with batch size 32 and learning rate \texttt{1e-4}, labelling our trained system \textbf{MAT}.

MAT was compared to three widely used neural TTS baseline approaches with pre-trained checkpoints available for LJ Speech, namely Grad-TTS\footnote{\customurl{https://github.com/huawei-noah/Speech-Backbones/tree/main/Grad-TTS}} \cite{popov2021grad} (label \textbf{GRAD}), a strong DPM-based acoustic model, FastSpeech 2 (\textbf{FS2}), a fast non-probabilistic acoustic model, and \textbf{VITS}\footnote{\customurl{https://github.com/jaywalnut310/vits}}, a strong probabilistic end-to-end TTS system with discrete-time normalising flows.
The baselines used the official checkpoints from the respective linked repositories.
For FS2, which does not provide an official implementation, we instead used the checkpoint from Meta's FairSeq\footnote{\customurl{https://github.com/facebookresearch/fairseq}}.
To decouple the effects of CFM training from those due to the new architecture, we also trained the GRAD architecture using the OT-CFM objective instead, using the same optimiser hyperparameters as for MAT.
This produced the system labelled \textbf{GCFM}.
For all acoustic models (i.e., all systems except VITS), we used the pre-trained HiFi-GAN \cite{kong2020hifi} LJ Speech checkpoint \texttt{LJ\_V1}\footnote{\customurl{https://github.com/jik876/hifi-gan/}} for waveform waveform generation,
with a denoising filter as introduced in \cite{prenger2019waveglow} at a strength of \texttt{2.5e-4}.
Our experiments also included vocoded held-out speech, labelled \textbf{VOC}.\begin{table}[!t]
\centering
\begin{tabular}{@{}l|cc|ccc@{}}
\toprule 
Condition & Params. & RAM & RTF () & WER & MOS\tabularnewline
\midrule
FS2 & 41.2M  & \hphantom{0}6.0 & \tablebf{0.010}0.004 & 4.18 & 3.290.09\tabularnewline
VITS & 36.3M & 12.4 & 0.0740.083 & 2.52 & 3.710.08\tabularnewline
GRAD-10 & \tablebf{14.8M} & \hphantom{0}7.8 & 0.0490.013 & 3.44 & 3.490.08\tabularnewline
GRAD-4 & \textquotedbl & \textquotedbl & 0.0190.006 & 3.69 & 3.200.09\tabularnewline
GCFM-4 & \textquotedbl & \textquotedbl & 0.0190.004 & 2.70 & 3.570.08\tabularnewline
\midrule
MAT-10 & 18.2M & \hphantom{0}\tablebf{4.8} & 0.0380.019 & \tablebf{2.09} & \tablebf{3.84}0.08\tabularnewline
MAT-4 & \textquotedbl & \textquotedbl & 0.0190.008 & 2.15 & 3.770.07\tabularnewline
MAT-2 & \textquotedbl & \textquotedbl & 0.0150.006 & 2.34 & 3.650.08\tabularnewline
\bottomrule
\end{tabular}
\caption{Conditions in the evaluation (with the number of Euler steps for ODE-based methods) and their number of parameters, minimum GPU RAM needed to train (GiB), real-time factor (including vocoding time) on the test set, ASR WER in percent, and mean opinion score with 95\%-confidence interval.
The best number in each column is bold.
VOC had a WER of 1.97 and a MOS of 4.130.07.}
\label{tab:results}
\vspace{-\baselineskip}
\end{table}%
 


ODE-based models, e.g., DPMs, allows trading off speed against quality.
We therefore evaluated synthesis from the trained ODE-based systems a different number of steps for the ODE solver (which determines how many times the decoder network must be evaluated).
This gave rise to multiple \emph{conditions} for some systems.
We labelled these conditions \textbf{MAT-}, \textbf{GRAD-}, and \textbf{GCFM-},  being the number of steps of the Euler forward ODE solver.
We used at most 10 steps, since \cite{popov2021grad} reported that 10 and 100 steps gave the same MOS for Grad-TTS (50 steps is the official code default).
All conditions used a temperature of 0.667 for synthesis, similar to \cite{popov2021grad}.
\cref{tab:results} provides an overview of the conditions in the evaluation.




\subsection{Evaluations, results, and discussion}
\label{ssec:results}
We evaluated our approach both objectively and subjectively.
First we measured parameter count and maximum memory use during training (at batch size 32 and fp16) of all systems, with results tabulated in \cref{tab:results}.
We see that MAT is approximately the same size as GRAD/GCFM, and smaller than all other systems.
In particular, it is smaller than VITS also after including the vocoder (13.9M parameters) in the parameter count.
More importantly, it uses less memory than all baselines, which (more than parameter count) is the main limiter on how large and powerful models that can be trained.

After training the systems, we assessed the synthesis speed and intelligibility of the different conditions, by computing the real time factor (RTF) mean and standard deviation when synthesising the test set, and evaluating the word error rate (WER) when applying the Whisper \texttt{medium}
\cite{radford2023robust} ASR system to the results, since the WERs of strong ASR systems correlate well with intelligibility \cite{taylor2021confidence}.
The results, in \cref{tab:results}, suggest that MAT is the most intelligible system, even using only two synthesis steps.
MAT is also much faster than VITS, equally fast or faster than GRAD/GCFM at the same number of steps, and only slightly slower than FS2 when at the fastest setting.






To evaluate the naturalness of the synthesised audio we ran a mean opinion score (MOS)
listening test.
We selected 40 utterances (4 groups of 10) of different lengths from the test set and synthesised each utterance using all conditions, normalising every stimulus
using EBU R128.
80 self-reported native speakers of English (all of whom reported using headphones for the test) were recruited from \href{https://prolific.co/}{Prolific} to listen to and rate these stimuli.
For each stimulus listeners were asked ``How natural does the synthesised speech sound?'', and provided provided responses on an integer rating scale from 1 (``Completely unnatural'') to 5 (``Completely natural'') adopted from the Blizzard Challenge \cite{prahallad2013blizzard}.
Each group of 10 utterances was evaluated by 20 listeners, who were paid \textsterling 3 for a median completion time of 13 mins.
Inattentive listeners were filtered out and replaced in exactly the same way as in \cite{mehta2023overflow}.
In the end we obtained 800 ratings for each condition.
The resulting MOS values, along with confidence intervals based on a normal approximation, are tabulated in \cref{tab:results}.
We note that the absolute numbers (and MOS values in general) are not comparable to those in other studies/papers, due to differences in listener demographics and instructions, as can be seen from the replication experiments in \cite{chiang23why, kirkland2023stuck}.\begin{figure}[!t]
\centering
\includegraphics[width=\columnwidth]{time_vs_txtlength.pdf}
\caption{Scatterplot of prompt length vs.\ synthesis time for acoustic models. Regression lines show as curves due to the log-log axes.}
\label{fig: rtf slopes}
\vspace{-\baselineskip}
\end{figure}

Applying -tests to all pairs of conditions, all differences were found to be statistically significant at the  level except the pairs (MAT-10,MAT-4), (MAT-4,VITS), (VITS,MAT-2), (MAT-2,GCFM-4), and (GCFM-4,GRAD-10).
This means that MAT always had significantly better rated naturalness than GRAD with the same number of steps, and always surpassed FS2.
Both the new architecture and training method contributed to the naturalness improvement, since MAT-4>GCFM-4>GRAD-4.
The fact that GRAD-10 was much better than GRAD-4 whilst MAT-10 and MAT-4 performed similarly suggests that GRAD requires many steps for good synthesis quality, whereas MAT reached a good level in fewer steps.
Finally, VITS performed similarly to MAT-2 and MAT-4 in terms of MOS.
MAT-10, although close to MAT-4 in rating, was significantly better than VITS.
For any given , MAT- always scored higher than any system with equal or faster RTF.
In summary, Matcha-TTS achieved similar or better naturalness than all comparable baselines.




Finally, we evaluated how synthesis speed scaled with utterance length for the acoustic models studied, by generating 180 sentences of different lengths using a GPT-2\footnote{\customurl{https://huggingface.co/gpt2}} model and plotting wall-clock synthesis time in \cref{fig: rtf slopes}, also fitting a least-squares regression line to this data.
The results show that MAT-2 synthesis speed becomes competitive with FS2 at longer utterances, with MAT-4 not far behind.
The major contributor to this appears to be the new architecture (since GRAD-4 and GCFM-4 perform similarly, and much slower), and the gap from MAT to GRAD only grows with longer utterances.



\section{Conclusions and future work}
\label{sec:conclusion}
We have introduced Matcha-TTS, a fast, probabilistic, and high-quality ODE-based TTS acoustic model trained using conditional flow matching.
The approach is non-autoregressive, memory efficient, and jointly learns to speak and align.
Compared to three strong pre-trained baselines, Matcha-TTS provides superior speech naturalness and can match the speed of the fastest model on long utterances.
Our experiments show that both the new architecture and the new training contribute to these improvements.


Compelling future work includes making the model multi-speaker, adding probabilistic duration modelling, and applications to challenging, diverse data such as spontaneous speech \cite{szekely2019spontaneous}.

\bibliographystyle{IEEEtran}
\bibliography{refs_abbrev}

\end{document}
