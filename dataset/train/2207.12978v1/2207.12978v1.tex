\documentclass[runningheads]{llncs}
\usepackage{graphicx}


\usepackage{tikz}
\usepackage{comment}
\usepackage{amsmath,amssymb} \usepackage{color}
\usepackage{makecell}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[export]{adjustbox}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{colortbl}
\usepackage{color}
\usepackage{subcaption}
\usepackage{graphicx} 
\definecolor{mygray}{RGB}{220,220,220}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage[pagebackref,breaklinks,citecolor=blue,colorlinks,linkcolor=red,bookmarks=false]{hyperref}
\usepackage{siunitx}
\usepackage{booktabs}
\usepackage{blindtext}
\usepackage{hyperref}



\setlength{\textfloatsep}{5mm plus 1mm minus 1mm}
\setlength{\intextsep}{5mm plus 1mm minus 1mm}
\setlength{\floatsep}{5mm plus 1mm minus 1mm}
\setlength{\abovecaptionskip}{4mm minus 1mm}

\usepackage{mathtools, nccmath}


\usepackage{caption}
\DeclareCaptionFont{9pt}{\fontsize{9pt}{10pt}\selectfont}
\captionsetup[figure]{font=9pt}
\captionsetup[table]{font=9pt}

\makeatletter
\setlength{\@fptop}{0pt}
\makeatother

\usepackage[capitalize]{cleveref}
\usepackage[T1]{fontenc}
\usepackage[font=small,labelfont=bf,tableposition=top]{caption}

\DeclareCaptionLabelFormat{andtable}{#1~#2  \&  \tablename~\thetable}


\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}

\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother

\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\ourmetric{TETA\xspace}
\def\oursoft{CEM\xspace}
\def\ourmodel{TETer\xspace}


\newcommand{\parsection}[1]{\vspace{1mm}\noindent\textbf{#1 }}
\renewcommand{\thesection}{\Alph{section}}
\usepackage[accsupp]{axessibility}  


\usepackage{ruler}
\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}
\usepackage{titlesec}

\begin{document}
\titlespacing*{\section}{0pt}{0.28cm}{0.28cm}
\titlespacing*{\subsection}{0pt}{0.2cm}{0.2cm}


\pagestyle{headings}
\mainmatter




\title{Supplementary Material for Tracking Every Thing in the Wild} 



\titlerunning{Tracking Every Thing in the Wild} 
\author{Siyuan Li, Martin Danelljan, Henghui Ding, Thomas E. Huang, Fisher Yu}
\institute{Computer Vision Lab, ETH Z{\"u}rich\\
\url{http://vis.xyz/pub/tet}}

\maketitle

This document gives more details on the ablation study regarding our Tracking-Every-Thing Accuracy (TETA) metric and our Tracking-Every-Thing tracker (TETer) and provides additional evaluation training and implementation details. 

\section{BDD100K Per-class Evaluation Results}

We provide per-class evaluation results using CLEARMOT~\cite{MOTA} and TETA metrics on the BDD100K~\cite{bdd100k} validation set in~\tableautorefname~\ref{tab:bdd-perclass-val}.
Data distribution in BDD100k is long-tailed. The Car category consists of most of the tracks in the dataset. The rest of the categories are rare compared to the dominant ones. Thus, we characterized them as rare classes.
TETer can achieve significant improvements across all rare classes on both established MOTA, IDF1, and our TETA metrics compared to the previous state-of-the-art QDtrack.
In particular, TETer boosts MOTA of buses by over 7 points on the validation set and TETA by over 6 points. 
We also compare our TETer results with CEM with its class agnostic counterpart, where the model only uses the AET strategy without CEM. The result shows that our model gains significant improvements over rare classes where the class agnostic instance association cannot be well trained due to lacking annotations. For instance, we gain +3.8 MOTA on buses and +4.7 MOTA on riders. Further, we can observe improvements on the TETA score, where we gain +2 on train and +1.5 on motorcycle. This demonstrates that TETer can better handle tracking rare classes. With CEM, we exploit the semantic annotations offered by large-scale object detection datasets. It can integrate fine-grained cues required for classification (\eg the difference between a big red bus and a red truck), which are difficult to learn effectively with class-agnostic appearance training on the long-tailed datasets.


\begin{table*}[t]
\centering
\caption{Per-class evaluation results on the BDD100K validation set using CLEARMOT and TETA metrics. Rare classes are highlighted in gray}
\label{tab:bdd-perclass-val}
\scalebox{0.9}{
\begin{tabular}{@{}l|l|ccc|cccc@{}}
\toprule
Method & Category & MOTA & MOTP & IDF1 & TETA & LocA & AssocA & ClsA \\ \hline
\multirow{9}{*}{QDTrack~\cite{qdtrack}} 
& Pedestrian & 49.3 & 78.4 & 59.9 & 52.1 &50.9 &46.8 &58.7 \\
 & Rider & 35.0 & 77.5 & 51.5 &45.1 &47.3 &39.6 &48.5 \\
 & Car & 69.8 & 84.6 & 75.0 &69.1 &62.2 &65.6 &79.5 \\
 & Truck & 39.2 & 85.4 & 58.2 &55.5 &57.0 &55.6 &53.9 \\
 & Bus & 40.8 & 86.2 & 62.3 &57.9 &58.1 &57.5 &58.1 \\
 & Train & 0.0 & - & 0.0 &12.1 &15.6 &20.7 &0.0 \\
 & Motorcycle & 28.8 & 76.9 & 56.0 &46.4 &41.6 &54.0 &43.5 \\
 & Bicycle & 30.0 & 76.2 & 50.1  &44.6 &34.2 &48.0 &51.4 \\\cline{2-9} 
 & Average & 36.6 & 70.7 & 51.6 &47.8 &45.9 &48.5 &49.2 \\ \hline
  \multirow{9}{*}{\makecell{AET \\ (Class-agnostic)}}
&Pedestrian &47.8 &79.1 &59.3 &53.0 &52.7 &47.0 &59.4 \\
&Rider &35.9 &76.4 &53.2 &48.2 &51.6 &48.1 &45.0 \\
&Car &69.6 &85.4 &75.0 &70.5 &64.1 &66.5 &80.8 \\
&Truck &41.7 &85.2 &59.8 &59.5 &59.0 &63.2 &56.1 \\
&Bus &44.4 &86.1 &66.2 &61.8 &61.1 &65.4 &58.8 \\
&Train &-2.6 &- &0.0 &12.4 &15.7 &21.4 &0.0 \\
&Motorcycle &31.6 &76.8 &57.8 &47.0 &42.8 &55.3 &42.8 \\
&Bicycle &29.8 &76.9 &49.7 &45.5 &35.2 &48.7 &52.4 \\\cline{2-9} 
 & Average   & 37.3 & 70.7 & 52.6 &49.7 &47.8 &52.0 &49.4 \\ \hline
 \multirow{9}{*}{\textbf{TETer~(CEM)}} 
&Pedestrian &49.7 &79.1 &59.9 &54.1 &52.3 &47.9 &62.2 \\
&Rider &40.5 &76.5 &56.6 &49.9 &50.4 &49.3 &50.1 \\
&Car &69.7 &85.4 &74.2 &70.5 &63.8 &65.6 &82.1 \\
&Truck &43.3 &85.5 &59.7 &59.1 &58.2 &60.1 &59.2 \\
&Bus &48.2 &86.2 &67.6 &63.7 &60.0 &65.8 &65.3 \\
&Train &0.0 &- &0.0 &14.4 &15.2 &28.1 &0.0 \\
&Motorcycle &31.6 &77.0 &58.2 &48.5 &42.8 &56.3 &46.6 \\
&Bicycle &30.1 &77.1 &50.1 &46.3 &34.7 &50.0 &54.2 \\
\cline{2-9} 
 & Average & \textbf{39.1} & \textbf{70.8} & \textbf{53.3} & \textbf{50.8} & \textbf{47.2} & \textbf{52.9} & \textbf{52.4} \\ \bottomrule
\end{tabular}}
\end{table*}


\begin{table}[t]
\centering
\caption{Changing the margin $r$ with a fixed $\alpha$}
\begin{tabular}{@{}l|c|cccccc@{}}

\hline
Method                         & $r$  & TETA   & LocA   & AssocA & ClsA & LocRe & LocPr    \\ \hline
\multirow{3}{*}{QDTrack}       & 50 & 30.0 & 50.5 & 27.4 & 12.1 & 53.1 & 75.8 \\
                               & 75 & 30.6 & 52.4 & 27.4 & 12.1 & 53.1 & 84.7 \\
                              & 90 & 30.8 & 53.0 & 27.4 & 12.1 & 53.1 & 91.1 \\ \hline
\multirow{3}{*}{\textbf{TETer (ours)}}  & 50 & 33.2 & 51.6 & 35.0 & 13.2 & 54.3 & 75.3 \\
                              & 75 & 33.9 & 53.6 & 35.0 & 13.2 & 54.3 & 84.3\\
                              & 90 & 34.1 & 54.1 & 35.0 & 13.2 & 54.3 & 90.5\\ \hline
\end{tabular}
\label{tab:fixa}
\end{table}


\begin{figure}[t]
	\centering
\includegraphics[width=0.5\linewidth]{figs/supp/Precision-recall_curves_tao_val_full_compare_ours_softmax_revised.pdf}
	\caption{Precision-Recall (PR) curves of in-sequence instance retrieval based on CEM and category prediction on the TAO validation set. A retrieved result is correct if it has the same labeled category with the target instance }
	\label{fig:PR_TAO_FULL}
\end{figure}

\section{TAO Per Frequency Group Evaluation Results}

We provide evaluation results per frequency group on the TAO validation set using TETA in~\tableautorefname~\ref{tab:tao-freq}.
We first observe that TETA can effectively evaluate methods across different frequency groups, despite difficulties introduced by classification errors.
Although ClsA drops significantly for both QDTrack~\cite{qdtrack} and TETer as categories become more rare, LocA and AssocA are relatively stable.
This enables us to compare different methods even in large-scale, long-tailed settings where classification is the bottleneck.

Compared to QDTrack, TETer can obtain consistent improvements in TETA, LocA, and AssocA across all frequency groups, at the cost of a small degradation in ClsA.
The improvements are more prominent on common and rare categories, where TETer can achieve over 3 points improvement in TETA.
For rare categories, TETer achieves 1.8 points improvement in LocA and over 7 points in AssocA.
Even on frequency categories, TETer can still improve AssocA by over 6 points.
~\tableautorefname~\ref{tab:tao-freq} also shows that the major differences between frequent and rare categories lies in classification. The localization and association capabilities of both trackers already generalize very well on rare categories. 


\begin{table}[t]
\centering
\caption{Per frequency group results on the TAO validation set using TETA}
\label{tab:tao-freq}
\small
\begin{tabular}{@{}l|l|cccc@{}}
\toprule
Method & Freq. Group & TETA & LocA & AssocA & ClsA \\ \hline
\multirow{4}{*}{QDTrack~\cite{qdtrack}}
& Frequent & 36.3 & 52.4 & 32.0 & 24.5 \\
 & Common & 23.9 & 47.2 & 21.7 & 2.9 \\
 & Rare & 26.7 & 52.7 & 27.4 & 0.0 \\
 & All & 30.0 & 50.5 & 27.4 & 12.1 \\ \hline
\multirow{4}{*}{\textbf{TETer~(ours)}}
 & Frequent & 39.4 & 53.9 & 38.7 & 25.7 \\
 & Common & 27.3 & 47.3 & 30.4 & 4.1 \\
 & Rare & 30.1 & 54.5 & 35.3 & 0.4 \\
 & All & \textbf{33.2} & \textbf{51.6} & \textbf{35.0} & \textbf{13.2} \\ \bottomrule
\end{tabular}
\end{table}




\section{Exemplar-based Classification}


Given an example object, exemplar-based classification means classifying objects by comparing with the given example to determine whether they belong to the same class.
Given two neighboring frames $t_1$ and $t_2$ in a video sequence, all objects in $t_1$ will be treated as exemplars.
For each exemplar, we find all target objects in $t_2$ that belong to the same class as the exemplar.

In this experiment, we compare our Class Exemplar Matching (CEM) with a hard prior baseline that matches objects with the same predicted class label.
We evaluate both methods on the TAO validation set
and compute precision-recall (PR) curves for comparison.
A true positive (TP) match is a match between two objects that belong to the same category.
A false positive (FP) match is a match between two objects that belong to different categories.
A false negative (FN) is a non-match between two objects that belong to the same category.
To compute the PR curve, we sample 10 thresholds from 0 to 0.99 with a fixed step size.


\figurename~\ref{fig:PR_TAO_FULL} shows the results of the experiment.
The hard prior baseline takes the argmax of the predictions of a softmax classifier from Faster R-CNN, thus there is only a single value in the PR curve. CEM significantly outperforms the hard prior baseline.


\section{TETA Details}

We provide additional details regarding our TETA metric about how it disentangles classification and how it deals with evaluation on datasets with complete annotations.

\subsection{Disentangling Classification}

The most direct way to disentangle classification is not to consider per-class performance and evaluate every object class-agnostically.
However, on large-scale, long-tailed datasets, such evaluation will be dominated by objects of the few common categories,
and the overall performance will not reflect the improvements on rare classes.
On the other hand, per-class evaluation requires us to select prediction results for each class, which is sensitive to classification performance.
If the classification is wrong, the contribution in localization and association will be ignored. 
TETA can naturally deal with this issue with the local cluster evaluation since we select predictions based on their location rather than class.
To evaluate a particular class, we access predictions in the local clusters of ground-truth objects belonging to the chosen class.
Thus, we can evaluate the localization and association performance even when the class predictions are wrong.

\subsection{TETA with Complete Annotations}

\parsection{Multiple categories}
TETA can also work with complete annotations. First, the localization accuracy is not affected.
In the case of incomplete annotations, we treat every unmatched predictions in each cluster as false positives.
If we have exhaustive annotations, we still treat those unmatched predictions as false positives.
The remaining question is how to penalize predictions that are not in any clusters.
For such predictions,
we know that they are not highly overlapped with any ground truth box, since we have exhaustive annotations.
The predictions thus false classify background as one of the foreground classes,
and so we treat them as classification false positives.

\parsection{Single category}
For single category with exhaustive annotations, the classification term of TETA is meaningless and can be ignored.
Also, since we do not need to perform per-class evaluation, the margin of the local cluster does not matter either.
Thus, we can set the margin $r$ to 0.
With these changes, TETA becomes similar to the HOTA~\cite{hota} metric with the only difference being that we use arithmetic mean instead of geometric mean.


\subsection{Ablation Study of TETA}

We provide an ablation study of the local cluster IoU margin $r$ of TETA. We perform this experiment on incomplete dataset TAO.

The results are shown in~\tablename~\ref{tab:fixa}. The LocRe and LocPr represent the localization recall and precision. As we can see, with a larger $r$, the LocPr increases since TETA becomes more conservative regarding identifying FPs. In the mean time, TETA makes fewer mistakes where the objects with no annotations are wrongly identified as FPs. In extreme crowded scenarios with incomplete annotations, it's recommended to set a higher $r$ to avoid false punishment.



\section{Qualitative Results}

We provide additional qualitative results of TETer.


\begin{figure*}[t]
	\centering
\includegraphics[width=0.9\linewidth]{figs/supp/cross-dataset.pdf}
	\vspace{-0.12in}
	\caption{Qualitative comparison of tracker optimized for the TAO~\cite{tao} metric (top) and tracker optimized for TETA (bottom) on BDD100K. The tracker optimized for TAO produces more false positives}
	\label{fig:sup-crossdataset}
\end{figure*}


\subsection{Category label prediction vs. CEM}

\begin{figure*}[t]
	\centering
\includegraphics[width=0.9\linewidth]{figs/supp/ID switches.pdf}
	\vspace{-0.12in}
 	\caption{Qualitative comparison of QDTrack~\cite{qdtrack} (top) and TETer (bottom) on BDD100K. QDTrack has ID Switches due to classification errors. (Same color represent the same track)}
	\label{fig:sup-idswitch}
\end{figure*}

We first compare the QDTrack~\cite{qdtrack} which uses class prediction as hard prior to associate objects with TETer which uses CEM.
In~\figurename~\ref{fig:sup-idswitch}, we show an example of QDTrack producing ID Switches due to errors in classification, whereas TETer is more robust to such issues.


\subsection{Class-agnostic vs. CEM}
We further show the comparison between the class-agnostic association (AET baseline in Section 5.5) and association with CEM. We observe that most class agnostic association errors happen in rare classes where there are not enough videos to train the class-agnostic instance association module well. For instance, ~\figurename~\ref{fig:clsag_cem} (a) shows the bicycle (16) is wrongly associated with the car with class-agnostic association, while using the CEM module helps to avoid the mistake.
The CEM module utilizes the supervision from large-scale object detection datasets to learn fine-grained class appearance differences, which helps the association on rare classes.

\begin{figure*}[ht]
	\centering
\includegraphics[width=1\linewidth]{figs/supp/failure_cases.pdf}
	\vspace{-0.12in}
	\caption{Failure cases of TETer on TAO. Big appearance changes (top) and occlusions (bottom)}
	\label{fig:sup-failure}
\end{figure*}

\begin{figure*}[t]
\centering
	\footnotesize
	\setlength{\tabcolsep}{0.1em}
	\adjustbox{width=1\linewidth}{
		\begin{tabular}{cccc}
		   \rotatebox{90}{\hspace{7mm}Class-agnostic}&\includegraphics[width=0.3\linewidth]{figs/supp/clsag_cem/fram30-clsag.png}& \includegraphics[width=0.3\linewidth]{figs/supp/clsag_cem/b22e02cd-6af68e18-fram34-clsag.png}& \includegraphics[width=0.3\linewidth]{figs/supp/clsag_cem/b22e02cd-6af68e18-fram43-clsag.png} \\
   		   \rotatebox{90}{\hspace{14mm}CEM}&\includegraphics[width=0.3\linewidth]{figs/supp/clsag_cem/frame-30-cem.png}& \includegraphics[width=0.3\linewidth]{figs/supp/clsag_cem/frame-34-cem.png}& \includegraphics[width=0.3\linewidth]{figs/supp/clsag_cem/frame-43-cem.png} \\
   		   \multicolumn{4}{c}{(a) The class agnostic tracker false associates a bicycle (16) to a car (16). }\\
		   \rotatebox{90}{\hspace{7mm}Class-agnostic}&\includegraphics[width=0.3\linewidth]{figs/supp/clsag_cem/fram131-clsag-car.png}& \includegraphics[width=0.3\linewidth]{figs/supp/clsag_cem/fram133-clsag-car.png}& \includegraphics[width=0.3\linewidth]{figs/supp/clsag_cem/fram139-clsag-car.png} \\
   		   \rotatebox{90}{\hspace{14mm}CEM}&\includegraphics[width=0.3\linewidth]{figs/supp/clsag_cem/frame-131-cem.png}& \includegraphics[width=0.3\linewidth]{figs/supp/clsag_cem/frame-133-cem.png}& \includegraphics[width=0.3\linewidth]{figs/supp/clsag_cem/frame-139-cem.png} \\	
   		      		   \multicolumn{4}{c}{(b) The class agnostic tracker false associates a car (31) to a pedestrian (31). }\\
   		      		   
	\end{tabular}}

    \caption{The qualitative comparison between CEM and class-agnostic association. CEM can exploit the semantic supervision offered by large scale datasets to learn fine-grained class appearance differences. Therefore, it can help trackers to avoid the association mistakes in class-agnostic association}
    \label{fig:clsag_cem}
\end{figure*}


\subsection{Tracking results comparison: TAO metric vs. TETA }
We provide results for cross-dataset analysis.
In~\figurename~\ref{fig:sup-crossdataset}, we show predictions from trackers that are optimized either for the TAO metric or TETA. The tracker optimized for the TAO metric generates more false positives that highly overlap, producing results that are difficult to use in practice. On the other hand, the tracker optimized for TETA produces cleaner results.

\subsection{Rare class retrieval}

We perform the class retrieval experiments in the rare classes on TAO to show the effectiveness of the CEM embeddings. We take the objects in the first frame of each ground truth track and use them as the retrieval templates to retrieve ground truth objects in the whole TAO validation set. The softmax prediction means we use the softmax confidence to retrieve objects that are predicted as the same class as the template. The CEM means we use the CEM embedding similarity to perform the retrieval. ~\figurename~\ref{fig:cls_retrievl_good} shows the CEM embedding can successfully retrieve the examples in the rare classes, while its softmax fails. 
~\figurename~\ref{fig:cls_retrivel_bad} shows some failures cases where the CEM module retrieves the wrong class due to occlusion or high visual similarities. 


\subsection{TETer failure cases}
We also show some common failure cases of TETer on TAO in~\figurename~\ref{fig:sup-failure}. Note that TAO is annotated at 1 FPS. Thus, fast-moving objects usually have huge appearance changes in neighboring frames.
Due to the large appearance and location variations, tracking is challenging on TAO. Also, TETer suffers from localization errors caused by occlusion.




\begin{figure*}[t]
\centering
	\footnotesize
	\setlength{\tabcolsep}{0.1em}
	\adjustbox{width=1\linewidth}{
		\begin{tabular}{ccccc}
		   \rotatebox{90}{\hspace{10mm}Softmax}&\includegraphics[width=0.29\linewidth]{figs/supp/clsag_cem/hockey_stick-target-tensor(1.0000).png}& \includegraphics[width=0.29\linewidth]{figs/supp/clsag_cem/cigarette-0-tensor(0.9448).png}& \includegraphics[width=0.29\linewidth]{figs/supp/clsag_cem/bow_(weapon)-15-tensor(0.6465).png}& \includegraphics[width=0.29\linewidth]{figs/supp/clsag_cem/bow_(weapon)-17-tensor(0.6094).png} \\
		   &hockey stick (target)&cigarette&bow (weapon)&bow (weapon)\\
   		   \rotatebox{90}{\hspace{14mm}CEM}&\includegraphics[width=0.29\linewidth]{figs/supp/clsag_cem/hockey_stick-target-tensor(1.0000).png}& \includegraphics[width=0.29\linewidth]{figs/supp/clsag_cem/hockey_stick-1-tensor(0.9605).png}& \includegraphics[width=0.29\linewidth]{figs/supp/clsag_cem/hockey_stick-2-tensor(0.9507).png}& \includegraphics[width=0.29\linewidth]{figs/supp/clsag_cem/hockey_stick-3-tensor(0.9327).png} \\
   		   &hockey stick (target)&hockey stick&hockey stick&hockey stick\\
   		   \multicolumn{5}{c}{}\\
   		   \multicolumn{5}{c}{}\\
   		   \multicolumn{5}{c}{}\\
   		   \rotatebox{90}{\hspace{10mm}Softmax}&\includegraphics[width=0.29\linewidth]{figs/supp/clsag_cem/new/cape-target-tensor(1.0000).png}& \includegraphics[width=0.29\linewidth]{figs/supp/clsag_cem/new/grocery_bag-0-tensor(1.).png}& \includegraphics[width=0.29\linewidth]{figs/supp/clsag_cem/new/ball-15-tensor(1.).png}& \includegraphics[width=0.29\linewidth]{figs/supp/clsag_cem/new/mixer_(kitchen_tool)-9-tensor(1.).png} \\
   		   &cape (target)&grocery bag&ball&mixer\\
   		   
   		  
   		   
   		   \rotatebox{90}{\hspace{14mm}CEM}&\includegraphics[width=0.29\linewidth]{figs/supp/clsag_cem/new/cape-94597.png}& \includegraphics[width=0.29\linewidth]{figs/supp/clsag_cem/new/cape-1-tensor(0.9202).png}& \includegraphics[width=0.29\linewidth]{figs/supp/clsag_cem/new/cape-2-tensor(0.9134).png}& \includegraphics[width=0.29\linewidth]{figs/supp/clsag_cem/new/cape-3-tensor(0.9130).png} \\
		   &cape (target)&cape&cape&cape\\
\end{tabular}}
	  \caption{Rare class retrieval. Objects in the most left column are the retrieval targets. The rest columns are the retrieval examples using softmax classifier or CEM. The retrieval results are ranked by confidence. Columns from left to right are corresponding to the results from high to low confidence. The class name under each example is the ground truth class for the example. The CEM module generalizes well on rare classes and can successfully retrieve them}
    
    \label{fig:cls_retrievl_good}
\end{figure*}

\begin{figure*}[t]
\centering
	\footnotesize
	\setlength{\tabcolsep}{0.1em}
	\adjustbox{width=1\linewidth}{
		\begin{tabular}{ccccc}
		   \rotatebox{90}{\hspace{10mm}Softmax}&\includegraphics[width=0.29\linewidth]{figs/supp/clsag_cem/rat-target-tensor(1.0000).png}& \includegraphics[width=0.29\linewidth]{figs/supp/clsag_cem/cat-0-tensor(0.9995).png}& \includegraphics[width=0.29\linewidth]{figs/supp/clsag_cem/cat-1-tensor(0.9990).png}& \includegraphics[width=0.29\linewidth]{figs/supp/clsag_cem/cat-2-tensor(0.9990).png} \\
		   &rat (target)&cat&cat&cat\\
   		   \rotatebox{90}{\hspace{14mm}CEM}&\includegraphics[width=0.29\linewidth]{figs/supp/clsag_cem/rat-0-tensor(1.0000).png}& \includegraphics[width=0.29\linewidth]{figs/supp/clsag_cem/rat-1-tensor(0.9924).png}& \includegraphics[width=0.29\linewidth]{figs/supp/clsag_cem/baby-2-tensor(0.9891).png}& \includegraphics[width=0.29\linewidth]{figs/supp/clsag_cem/baby-3-tensor(0.9881).png} \\
   		   &rat (target)&rat&human&human\\
   		   
   		   \multicolumn{5}{c}{}\\
   		   \multicolumn{5}{c}{}\\
   		   \multicolumn{5}{c}{}\\
\rotatebox{90}{\hspace{10mm}Softmax}&\includegraphics[width=0.29\linewidth]{figs/supp/clsag_cem/gorilla-target-tensor(1.0000) (1).png}& \includegraphics[width=0.29\linewidth]{figs/supp/clsag_cem/elephant-0-tensor(0.9995).png}& \includegraphics[width=0.29\linewidth]{figs/supp/clsag_cem/elephant-4-tensor(0.9995).png}& \includegraphics[width=0.29\linewidth]{figs/supp/clsag_cem/elephant-6-tensor(0.9995).png} \\
   		   &gorilla (target)&elephant&elephant&elephant\\
   		   \rotatebox{90}{\hspace{14mm}CEM}&\includegraphics[width=0.29\linewidth]{figs/supp/clsag_cem/gorilla-target-tensor(1.0000).png}& \includegraphics[width=0.29\linewidth]{figs/supp/clsag_cem/monkey-1-tensor(0.9776).png}& \includegraphics[width=0.29\linewidth]{figs/supp/clsag_cem/baby-3-tensor(0.9728).png}& \includegraphics[width=0.29\linewidth]{figs/supp/clsag_cem/bear-4-0.9718.png} \\
		   &gorilla (target)&monkey&baby&bear\\
   		      	
   		      		   
	\end{tabular}}

    \caption{Failure retrieval cases. The first two rows show that the CEM module retrieve the human hand as rat due to occlusion. The last two rows show the CEM retrieve monkey, bear as the gorilla due to the high visual similarities}
    
    \label{fig:cls_retrivel_bad}
\end{figure*}



\section{More Implementation Details}
We provide more implementation and training details of our method and evaluation setup in different benchmarks.

\subsection{Network architecture}
We use the popular Faster R-CNN ~\cite{ren2015faster} with ResNet as the backbone. Specifically, we use the ResNet-101~\cite{he2016deep} for TETer on TAO and ResNet-50 on BDD100K. For the exemplar encoder, we use \textit{4conv-3fc} head with group normalization ~\cite{wu2018group}. The final output channel numbers are 1230 for TETer on TAO~\cite{tao} and 256 for BDD100K~\cite{bdd100k}. We use the same network architecture for the instance appearance encoder but with only \textit{1fc} layers for the final output. The channel number of the instance appearance encoder is 256 by default on both datasets.

\subsection{Training}
\parsection{TAO}
We train TETer following the TAO~\cite{dave2020tao} set up with a mixed LVISv0.5~\cite{lvis} and COCO~\cite{coco} dataset. We set the batch size to 16 and the learning rate to 0.02. We train 24 epochs in total and decrease the learning by 0.1 after 16 and 22 epochs. 
For data augmentation, we randomly flip the images horizontally with a 0.5 ratio. We randomly resize the training images to keep their short edges between 640 to 800. We randomly sample images to form mini-batches with additionally repeat sampling for rare classes~\cite{lvis}. We set the repeat factor to 0.001. We train the instance appearance encoder on the TAO training set following the same setting as QDTrack~\cite{qdtrack}.


\parsection{BDD100K}
We use the same object detector as QDTrack~\cite{qdtrack}. For training the exemplar encoder, we freeze the object detector above and train with 8 BDD100K MOT categories using the BDD100K Detection set, which contains 70K images. For data augmentation, we randomly flip the images horizontally with a 0.5 ratio. We randomly resize the training images to keep their short edges between 640 to 800. We randomly sampled images to form mini-batches. We set the batch size to 128. 

\subsection{Inference and evaluation}

\parsection{TAO}
We evaluate our model on the TAO validation set with TETA. For the close-set setting, the TAO validation set contains 988 videos with 302 classes, a subset with LVIS classes. For the open-set setting, we merge the additional free-form classes~\cite{tao} as one unknown class. During inference, we use the fixed image scale with 800 at the short edge. We initialize a new track if the object has detection confidence higher than 0.0001.  

\parsection{BDD100K}
The BDD100K contains 200 videos (40k) for validation and 400 videos (80k) for testing. We use both the BDD100K validation set and the test set for evaluation. For the inference, we use the $(1296, 720)$ image scale. We use the best performed model in the validation set which is saved at 2 epoch. We initialize a new track if the detection confidence is higher than 0.7. 


\bibliographystyle{splncs04}
\bibliography{egbib}
\end{document}
