In this work, we consider two sets of sparse keypoints $\mathbf{X} \in \mathbb{R}^{|\mathbf{X}|\times 3}$ and $\mathbf{Y} \in \mathbb{R}^{|\mathbf{Y}|\times 3}$ from a pair of partially overlapping 3D point clouds, {with each keypoint having} an associated local descriptor. 
The input putative correspondence set $C$ can be generated by nearest neighbor search using the local descriptors. Each correspondence ${c_i} \in {C}$ is denoted as ${c_i} = (\bm{x_i, y_{i}}) \in \mathbb{R}^6$, where $\bm{x_i}\in \mathbf{X}, \bm{y_{i}}\in \mathbf{Y}$ are the coordinates of {a pair of 3D keypoints from the two sets.}
{Our objective is to find an inlier/outlier label for $c_i$, being $w_i=1$ and $0$, respectively,}
and recover {an optimal 3D rigid} transformation $\mathbf{\hat R},\mathbf{\hat t}$ between the two point sets.
The pipeline of our network~\Name~is shown in Fig.~\ref{fig:pipeline} and can be summarized as follows:
\begin{enumerate}[itemsep=-1mm]
\item We embed the input correspondences into high dimensional geometric features using the \nonlocal~module
(Sec.~\ref{subsec:nonlocal}).
    \item We estimate the initial confidence $v_i$ of each correspondence $c_i$ to select a limited number of highly confident and well-distributed \textit{seeds} (Sec.~\ref{subsec:seed}).
\item For each \textit{seed}, we search for its $k$ nearest neighbors in the feature space and perform neural spectral matching~(NSM) to obtain {its} confidence {of} being an inlier. The confidence values are used to {weigh} the least-squares fitting for computing a rigid transformation for each seed (Sec.~\ref{subsec:nsm}). 


\item The best transformation matrix is selected from all the hypotheses as the one that maximizes the number of inlier correspondences (Sec.~\ref{subsec:hypo_select}).
\end{enumerate}






\subsection{\Name~vs. RANSAC}
\label{sec:vs_ransac}


Here, we clarify the difference between \Name~and RANSAC {to help understand the insights behind our algorithm}.
{Despite} not being designed for improving
classic RANSAC, our \Name~shares {a} \textit{hypothesize-and-verify} pipeline {similar to RANSAC}. In the sampling step, instead of randomly sampling minimal subsets iteratively, we utilize the learned embedding space to retrieve a pool of larger correspondence subsets in one shot~(Sec.~\ref{subsec:nonlocal} and Sec.~\ref{subsec:seed}). {The correspondences in such subsets} have higher probabilities {of being} inliers {thanks to the highly confident seeds and the discriminative embedding space}. In the model fitting step, our neural spectral matching module~(Sec.~\ref{subsec:nsm}) effectively prunes the potential outliers in the retrieved subsets, producing a correct model even when starting from a not-all-inlier sample. In this way, \Name~can tolerate large outlier ratios and produce highly precise registration results, without needing exhaustive iterations. 




\subsection{Geometric Feature Embedding}
\label{subsec:nonlocal}

The first module of our network is the \nonlocal~module, which receives the correspondences $C$ as input and produces a geometric feature for each correspondence. Previous networks~\cite{choy2020deep, pais20203dregnet} learn the feature embedding through generic operators,
ignoring the unique properties of 3D rigid transformations.
Instead, our \nonlocal~module explicitly utilizes the spatial consistency between inlier correspondences to learn a discriminative 
{embedding space, where inlier correspondences are close to each other.} 



\begin{figure}[t]
\setlength{\abovecaptionskip}{0.05cm}
\setlength{\belowcaptionskip}{-0.35cm}
	\vspace{-0.4cm}
	\centering
    \includegraphics[width=5.5cm]{figures/nonlocal.png}
    \caption{The spatial-consistency guided nonlocal layer. $\bm{\beta}$ represents the spatial consistency matrix calculated using Eq.~\ref{eq:spatial_consistency} and $\bm{F}$ is the feature from {the} previous layer. 
}
\label{fig:nonlocal}
\end{figure}

{As illustrated in Fig.~\ref{fig:pipeline}}, our \nonlocal~module has 12 blocks, each of which consists of a shared Perceptron layer, a BatchNorm layer with ReLU, and the proposed nonlocal layer. Fig.~\ref{fig:nonlocal} {illustrates this new nonlocal layer}. Let $\bm{f_i} \in \bm{F}$ be the intermediate {feature} representation for correspondence $c_i$. {The design of our nonlocal layer {for updating the features} draws inspiration from the well-known nonlocal network~\cite{wang2018non}, {which captures the long-range dependencies using nonlocal operators.}
Our contribution is to introduce a novel {spatial consistency term to complement the feature similarity in nonlocal operators.}
Specifically, we update the features using the following equation:  }
\begin{equation}
\setlength\abovedisplayskip{3.5pt}\setlength\belowdisplayskip{3.5pt}
\bm{f_i} = \bm{f_i} + \text{MLP}(\sum\nolimits_{j}^{|C|}\text{softmax}_j(\bm{\alpha\beta}) g(\bm{f_j})) ~,
	\label{eq:nonlocal}
\end{equation}
where $g$ is a linear projection function. The feature similarity term $\bm{\alpha}$ is defined as the embedded dot-product similarity~\cite{wang2018non}.
The {spatial consistency} term $\bm{\beta}$ {is defined based on} the {length constraint} of 3D rigid transformations, 
as illustrated in Fig.~\ref{fig:ambiguity}a~($c_1$ and $c_2$). 


Specifically, we {compute} $\bm{\beta}$ by measuring the {length difference} {between the line segments of point pairs in $\mathbf{X}$ and its corresponding segments in $\mathbf{Y}$:} 
\begin{equation}
\setlength\abovedisplayskip{2pt}
\setlength\belowdisplayskip{2.5pt}
    \beta_{ij} = [1 - \frac{d_{ij}^2}{\sigma_d^2}]_+, ~~
d_{ij} = \big | \left\Vert \bm{x_i-x_j}\right\Vert - \left\Vert \bm{y_i-y_j}\right\Vert \big |,
    \label{eq:spatial_consistency}
\end{equation}
where $[\cdot]_+$ is the $\max (\cdot, 0)$ operation to ensure {a non-negative value of $\beta_{ij}$}, and $\sigma_d$ is a {distance} parameter~(see Sec.~\ref{sec:imp}) to control the sensitivity to the length difference. Correspondence pairs having the length difference larger than $\sigma_d$ are considered to be incompatible and get zero for $\bm{\beta}$. 
In contrast, ${\beta}_{ij}$ gives a large value only if the two correspondences {$c_i$ and $c_j$} are spatially compatible, serving as a reliable regulator to the feature similarity term. 

Note that other forms of spatial consistency can also be easily incorporated here. However, taking 
{an angle-based spatial consistency} constraint as an example, the normals of input keypoints might not {always} be available {for} outlier rejection and the normal estimation {task is challenging on its own} especially for LiDAR point clouds~\cite{zhao2019robust}.
Our \nonlocal~module produces {for each correspondence $c_i$} a feature representation $\bm{f_i}$, which will be used in both seed selection and neural spectral matching module.


\begin{figure}[tb]
\setlength{\abovecaptionskip}{0.2cm}
\setlength{\belowcaptionskip}{-0.35cm}
\vspace{-0.5cm}
\includegraphics[width=8.5cm]{figures/ambiguity_seed.png}
    \caption{(a) 
{Inlier correspondence pairs~($c_1,c_2$) always satisfy the length consistency, while outliers~(e.g.~$c_4$) {are usually not spatially consistent with} either inliers ($c_1,c_2$) or {other} outliers~(e.g.~$c_3$).
However, there exist ambiguity when inliers~($c_2$) and outliers~($c_3$) {happen to} satisfy the length consistency.}
The feature similarity term $\bm{\alpha}$ provides the possibility to alleviate {the ambiguity issue}. (b) The correspondence subsets of {a seed (blue line)}
     found by spatial kNN~(Left) and feature-space
kNN~(Right). } 
    \label{fig:ambiguity}
\end{figure}
















\subsection{Seed Selection}
\label{subsec:seed}
{As mentioned before, the traditional spectral matching {technique} has {difficulties} in finding {a} dominant inlier cluster in low overlapping cases, {where it would fail} to provide a clear separation between inliers and outliers~\cite{yang2019extreme}. {In such cases}, directly using the output from spectral matching in weighted least-squares fitting~\cite{besl1992method} for transformation estimation may lead to a sub-optimal solution since
there are still many outliers not being explicitly rejected.} 
To address this issue, inspired by~\cite{cavalli2020adalam}, we design a seeding mechanism to apply neural spectral matching locally. We first find reliable and well-distributed correspondences as seeds, {and around them search for consistent correspondences in the feature space.} Then each subset is expected to have a higher inlier ratio {than the input correspondence set}, {and is thus} easier for neural spectral matching to find {a} correct cluster.  


To select the seeds, we first adopt an MLP to estimate the initial confidence $v_i$ of each correspondence using the feature $\bm{f_i}$ learned by the \nonlocal~module,
and then apply Non-Maximum Suppression~\cite{lowe2004distinctive} over the confidence to find the well-distributed seeds.
{The selected seeds will be used to form multiple correspondence subsets for the neural spectral matching.}







\subsection{Neural Spectral Matching}
\label{subsec:nsm}
In this step, we leverage the learned feature space to augment each seed with {a subset of} consistent correspondences by performing $k$-nearest neighbor searching in the feature space.  
We then adopt {the proposed }neural spectral matching~(NSM)
over each subset to estimate {a} transformation as one hypothesis. Feature-space kNN has several advantages over spatial kNN, as illustrated in Fig.~\ref{fig:ambiguity}b. 
First, {the} neighbors found in the feature space are more likely to follow a similar transformation as the seeds, thanks to the \nonlocal~module. Second, the neighbors chosen in {the} feature space can be located far apart in {the} 3D space,
leading to more robust {transformation} estimation results. 

\begin{figure}[tb]
\setlength{\belowcaptionskip}{-0.35cm}
	\centering
	\vspace{-0.5cm}
    \includegraphics[width=6cm]{figures/assignment_graph.png}
    \caption{Constructing the compatibility graph {and associated matrix~(Right)} from the input correspondences~(Left). We set the matrix diagonal to zero following~\cite{leordeanu2005spectral}.
The weight of each graph edge represents the pairwise compatibility between two {associated} correspondences. } 
    \label{fig:assignment_graph}
\end{figure}

Given the correspondence subset $C' \subseteq C~(|C'|=k)$ of each seed constructed by kNN search, we apply NSM to estimate the inlier probability, which is subsequently used in the weighted least-squares
fitting~\cite{besl1992method} for transformation estimation.  Following~\cite{leordeanu2005spectral}, we first construct a matrix $\mathbf{M}$ representing a compatibility graph associated with $C'$, 
as illustrated in Fig.~\ref{fig:assignment_graph}. Instead of solely relying on the length consistency as~\cite{leordeanu2005spectral}, {we further incorporate the geometric feature similarity to tackle the ambiguity problem}
as illustrated in Fig.~\ref{fig:ambiguity}a. Each entry ${M_{ij}}$ measures the compatibility between correspondence $c_i$ and $c_j$ from $C'$, which is defined as 
\begin{equation}
\setlength\abovedisplayskip{5pt}
\setlength\belowdisplayskip{5pt}
    M_{ij} = \beta_{ij} * \gamma_{ij},
\label{eq:compatibility}
\end{equation}
\begin{equation}
\gamma_{ij} = [1 - \frac{1}{\sigma_f^2}  \left\Vert \bm{\bar f_i - \bar f_j} \right\Vert^2]_+
     \label{eq:feat_sm}
\end{equation}
where $\beta_{ij}$ is {the same as} in Eq.~\ref{eq:spatial_consistency},
$\bm{\bar f_i}$ and $\bm{\bar f_j}$ are the L2-normalized feature vectors, and $\sigma_f$ is a parameter to control sensitivity to {feature difference~(see Sec.~\ref{sec:imp}).} 


 
  
 The elements of $\mathbf{M}$ defined above {are} always non-negative and increase with the compatibility between correspondences. Following~\cite{leordeanu2005spectral}, we consider the leading eigenvector of matrix $\mathbf{M}$ as the \textit{association} of each correspondence with {a} main cluster. Since this main cluster is statistically formed by the inlier correspondences, it is natural to interpret this \textit{association} as the inlier probability. The higher the association to the main cluster, the higher the probability of {a correspondence} being an inlier.
The leading eigenvector $\bm{e} \in \mathbb{R}^k$ can be efficiently computed by the power iteration algorithm~\cite{mises1929praktische}. 
We regard $\bm{e}$  
as the inlier probability, since only the relative value of $\bm{e}$ matters.
{Finally} we use the probability $\bm{e}$ as the weight to estimate the transformation through {least-squares fitting},
\begin{equation}
\setlength\abovedisplayskip{5pt}\setlength\belowdisplayskip{5pt}
\mathbf{R'},\mathbf{t'} = \arg\min_{\mathbf{R}, \mathbf{t}} \sum\nolimits_{i}^{|C'|} \bm{e_i} \left\Vert \mathbf{R}\bm{x_i} + \mathbf{t} - \bm{y_i}  \right\Vert^2.
    \label{eq:solve_r_t}
\end{equation}
Eq.~\ref{eq:solve_r_t} can be solved in closed form by SVD~\cite{besl1992method}. For the sake of completeness, {we provide} its derivation in the supplementary~\ref{supp:proof}. By {performing}
such steps for each seed {in parallel}, the network produces a set of transformations $\{\mathbf{R'},\mathbf{t'}\}$ for hypothesis selection.









\subsection{Hypothesis Selection}
\label{subsec:hypo_select}
The final stage of \Name~involves selecting the best hypothesis among the transformations produced by the NSM module. 
The criterion for selecting the best transformation is based on the number of correspondences satisfied by {each}
transformation, 
\begin{equation}
\setlength\abovedisplayskip{5pt}\setlength\belowdisplayskip{5pt}
    \mathbf{\hat R},\mathbf{\hat t} = \arg\max_{\mathbf{R', t'}} \sum\nolimits_{i}^{|C|}  \big \llbracket  \vert\vert \mathbf{R'}\bm{x_i} + \mathbf{t'} - \bm{y_i} \vert\vert < \tau \big \rrbracket,
\end{equation}
where $\llbracket \cdot \rrbracket$ is the Iverson bracket and $\tau$ denotes {an} inlier threshold. The final inlier/outlier labels $\bm{w}\in \mathbb{R}^{|C|}$ {are} given by ${w_i} = \llbracket \vert\vert \mathbf{\hat R}\bm{x_i} + \mathbf{\hat t} - \bm{y_i} \vert\vert< \tau \big \rrbracket$. 
We then 
recompute the transformation matrix using all the surviving inliers in a least-squares manner, which is a common practice~\cite{chum2003locally, barath2018graph}. 






\subsection{Loss Formulation}


{Considering the compatibility graph illustrated in Fig.~\ref{fig:assignment_graph}, previous works~\cite{choy2020deep, pais20203dregnet} mainly adopt node-wise losses, which supervise each correspondence individually. In our work, we further design an edge-wise loss to supervise the pairwise relations between the correspondences.}

\noindent\textbf{Node-wise supervision.} We denote $\bm{w}^* \in \mathbb{R}^{|C|}$ as the ground-truth inlier/outlier labels constructed by 
\begin{equation}
\setlength\abovedisplayskip{5pt}\setlength\belowdisplayskip{5pt}
	{w}^*_i = \llbracket \vert\vert \mathbf{R^*}\bm{x_i} + \mathbf{t^*} - \bm{y_i} \vert\vert < \tau \big \rrbracket,
\end{equation} where $\mathbf{R^*}$ {and} $\mathbf{t^*}$ are the ground{-}truth rotation and translation matries, respectively.
Similar to~\cite{choy2020deep,pais20203dregnet}, we first adopt the binary cross entropy loss as the node-wise supervision for learning the initial confidence by 
\begin{equation}
\setlength\abovedisplayskip{4pt}\setlength\belowdisplayskip{4pt}
L_{class} = \text{BCE}(\bm{v, w^*}),
\end{equation}
where $\bm{v}$ is the initial confidence predicted (Sec.~\ref{subsec:seed}).




\noindent\textbf{Edge-wise supervision} 
We further propose the spectral matching loss as our edge-wise supervision, formulated as
\begin{equation}
\setlength\abovedisplayskip{5pt}\setlength\belowdisplayskip{5pt}
L_{sm} = \frac{1}{|C|^2}\sum\nolimits_{ij} (\gamma_{ij} - \gamma^*_{ij})^2, 
    \label{eq:sm_loss}
\end{equation} 
where $\gamma^*_{ij}=\llbracket c_i, c_j \text{~are both inliers}\rrbracket$ is the ground-truth compatibility value and $\gamma_{ij}$ is the estimated compatibility value based on the feature similarity defined in Eq.~\ref{eq:feat_sm}.
This loss supervises the relationship between each pair of correspondences, serving as a complement to the node-wise supervision. Our experiments~(Sec.~\ref{subsec:ablation}) show that the proposed $L_{sm}$ remarkably improves the performance.




The final loss is a weighted sum of the two losses,
\begin{equation}
\setlength\abovedisplayskip{5pt}\setlength\belowdisplayskip{5pt}
	L_{total} = L_{sm} + \lambda L_{class},\label{eq:full_loss}
\end{equation}
where $\lambda$ is a hyper-parameter to balance the two losses.


