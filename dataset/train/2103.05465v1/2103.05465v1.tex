In this work, we consider two sets of sparse keypoints  and  from a pair of partially overlapping 3D point clouds, {with each keypoint having} an associated local descriptor. 
The input putative correspondence set  can be generated by nearest neighbor search using the local descriptors. Each correspondence  is denoted as , where  are the coordinates of {a pair of 3D keypoints from the two sets.}
{Our objective is to find an inlier/outlier label for , being  and , respectively,}
and recover {an optimal 3D rigid} transformation  between the two point sets.
The pipeline of our network~\Name~is shown in Fig.~\ref{fig:pipeline} and can be summarized as follows:
\begin{enumerate}[itemsep=-1mm]
\item We embed the input correspondences into high dimensional geometric features using the \nonlocal~module
(Sec.~\ref{subsec:nonlocal}).
    \item We estimate the initial confidence  of each correspondence  to select a limited number of highly confident and well-distributed \textit{seeds} (Sec.~\ref{subsec:seed}).
\item For each \textit{seed}, we search for its  nearest neighbors in the feature space and perform neural spectral matching~(NSM) to obtain {its} confidence {of} being an inlier. The confidence values are used to {weigh} the least-squares fitting for computing a rigid transformation for each seed (Sec.~\ref{subsec:nsm}). 


\item The best transformation matrix is selected from all the hypotheses as the one that maximizes the number of inlier correspondences (Sec.~\ref{subsec:hypo_select}).
\end{enumerate}






\subsection{\Name~vs. RANSAC}
\label{sec:vs_ransac}


Here, we clarify the difference between \Name~and RANSAC {to help understand the insights behind our algorithm}.
{Despite} not being designed for improving
classic RANSAC, our \Name~shares {a} \textit{hypothesize-and-verify} pipeline {similar to RANSAC}. In the sampling step, instead of randomly sampling minimal subsets iteratively, we utilize the learned embedding space to retrieve a pool of larger correspondence subsets in one shot~(Sec.~\ref{subsec:nonlocal} and Sec.~\ref{subsec:seed}). {The correspondences in such subsets} have higher probabilities {of being} inliers {thanks to the highly confident seeds and the discriminative embedding space}. In the model fitting step, our neural spectral matching module~(Sec.~\ref{subsec:nsm}) effectively prunes the potential outliers in the retrieved subsets, producing a correct model even when starting from a not-all-inlier sample. In this way, \Name~can tolerate large outlier ratios and produce highly precise registration results, without needing exhaustive iterations. 




\subsection{Geometric Feature Embedding}
\label{subsec:nonlocal}

The first module of our network is the \nonlocal~module, which receives the correspondences  as input and produces a geometric feature for each correspondence. Previous networks~\cite{choy2020deep, pais20203dregnet} learn the feature embedding through generic operators,
ignoring the unique properties of 3D rigid transformations.
Instead, our \nonlocal~module explicitly utilizes the spatial consistency between inlier correspondences to learn a discriminative 
{embedding space, where inlier correspondences are close to each other.} 



\begin{figure}[t]
\setlength{\abovecaptionskip}{0.05cm}
\setlength{\belowcaptionskip}{-0.35cm}
	\vspace{-0.4cm}
	\centering
    \includegraphics[width=5.5cm]{figures/nonlocal.png}
    \caption{The spatial-consistency guided nonlocal layer.  represents the spatial consistency matrix calculated using Eq.~\ref{eq:spatial_consistency} and  is the feature from {the} previous layer. 
}
\label{fig:nonlocal}
\end{figure}

{As illustrated in Fig.~\ref{fig:pipeline}}, our \nonlocal~module has 12 blocks, each of which consists of a shared Perceptron layer, a BatchNorm layer with ReLU, and the proposed nonlocal layer. Fig.~\ref{fig:nonlocal} {illustrates this new nonlocal layer}. Let  be the intermediate {feature} representation for correspondence . {The design of our nonlocal layer {for updating the features} draws inspiration from the well-known nonlocal network~\cite{wang2018non}, {which captures the long-range dependencies using nonlocal operators.}
Our contribution is to introduce a novel {spatial consistency term to complement the feature similarity in nonlocal operators.}
Specifically, we update the features using the following equation:  }

where  is a linear projection function. The feature similarity term  is defined as the embedded dot-product similarity~\cite{wang2018non}.
The {spatial consistency} term  {is defined based on} the {length constraint} of 3D rigid transformations, 
as illustrated in Fig.~\ref{fig:ambiguity}a~( and ). 


Specifically, we {compute}  by measuring the {length difference} {between the line segments of point pairs in  and its corresponding segments in :} 

where  is the  operation to ensure {a non-negative value of }, and  is a {distance} parameter~(see Sec.~\ref{sec:imp}) to control the sensitivity to the length difference. Correspondence pairs having the length difference larger than  are considered to be incompatible and get zero for . 
In contrast,  gives a large value only if the two correspondences { and } are spatially compatible, serving as a reliable regulator to the feature similarity term. 

Note that other forms of spatial consistency can also be easily incorporated here. However, taking 
{an angle-based spatial consistency} constraint as an example, the normals of input keypoints might not {always} be available {for} outlier rejection and the normal estimation {task is challenging on its own} especially for LiDAR point clouds~\cite{zhao2019robust}.
Our \nonlocal~module produces {for each correspondence } a feature representation , which will be used in both seed selection and neural spectral matching module.


\begin{figure}[tb]
\setlength{\abovecaptionskip}{0.2cm}
\setlength{\belowcaptionskip}{-0.35cm}
\vspace{-0.5cm}
\includegraphics[width=8.5cm]{figures/ambiguity_seed.png}
    \caption{(a) 
{Inlier correspondence pairs~() always satisfy the length consistency, while outliers~(e.g.~) {are usually not spatially consistent with} either inliers () or {other} outliers~(e.g.~).
However, there exist ambiguity when inliers~() and outliers~() {happen to} satisfy the length consistency.}
The feature similarity term  provides the possibility to alleviate {the ambiguity issue}. (b) The correspondence subsets of {a seed (blue line)}
     found by spatial kNN~(Left) and feature-space
kNN~(Right). } 
    \label{fig:ambiguity}
\end{figure}
















\subsection{Seed Selection}
\label{subsec:seed}
{As mentioned before, the traditional spectral matching {technique} has {difficulties} in finding {a} dominant inlier cluster in low overlapping cases, {where it would fail} to provide a clear separation between inliers and outliers~\cite{yang2019extreme}. {In such cases}, directly using the output from spectral matching in weighted least-squares fitting~\cite{besl1992method} for transformation estimation may lead to a sub-optimal solution since
there are still many outliers not being explicitly rejected.} 
To address this issue, inspired by~\cite{cavalli2020adalam}, we design a seeding mechanism to apply neural spectral matching locally. We first find reliable and well-distributed correspondences as seeds, {and around them search for consistent correspondences in the feature space.} Then each subset is expected to have a higher inlier ratio {than the input correspondence set}, {and is thus} easier for neural spectral matching to find {a} correct cluster.  


To select the seeds, we first adopt an MLP to estimate the initial confidence  of each correspondence using the feature  learned by the \nonlocal~module,
and then apply Non-Maximum Suppression~\cite{lowe2004distinctive} over the confidence to find the well-distributed seeds.
{The selected seeds will be used to form multiple correspondence subsets for the neural spectral matching.}







\subsection{Neural Spectral Matching}
\label{subsec:nsm}
In this step, we leverage the learned feature space to augment each seed with {a subset of} consistent correspondences by performing -nearest neighbor searching in the feature space.  
We then adopt {the proposed }neural spectral matching~(NSM)
over each subset to estimate {a} transformation as one hypothesis. Feature-space kNN has several advantages over spatial kNN, as illustrated in Fig.~\ref{fig:ambiguity}b. 
First, {the} neighbors found in the feature space are more likely to follow a similar transformation as the seeds, thanks to the \nonlocal~module. Second, the neighbors chosen in {the} feature space can be located far apart in {the} 3D space,
leading to more robust {transformation} estimation results. 

\begin{figure}[tb]
\setlength{\belowcaptionskip}{-0.35cm}
	\centering
	\vspace{-0.5cm}
    \includegraphics[width=6cm]{figures/assignment_graph.png}
    \caption{Constructing the compatibility graph {and associated matrix~(Right)} from the input correspondences~(Left). We set the matrix diagonal to zero following~\cite{leordeanu2005spectral}.
The weight of each graph edge represents the pairwise compatibility between two {associated} correspondences. } 
    \label{fig:assignment_graph}
\end{figure}

Given the correspondence subset  of each seed constructed by kNN search, we apply NSM to estimate the inlier probability, which is subsequently used in the weighted least-squares
fitting~\cite{besl1992method} for transformation estimation.  Following~\cite{leordeanu2005spectral}, we first construct a matrix  representing a compatibility graph associated with , 
as illustrated in Fig.~\ref{fig:assignment_graph}. Instead of solely relying on the length consistency as~\cite{leordeanu2005spectral}, {we further incorporate the geometric feature similarity to tackle the ambiguity problem}
as illustrated in Fig.~\ref{fig:ambiguity}a. Each entry  measures the compatibility between correspondence  and  from , which is defined as 


where  is {the same as} in Eq.~\ref{eq:spatial_consistency},
 and  are the L2-normalized feature vectors, and  is a parameter to control sensitivity to {feature difference~(see Sec.~\ref{sec:imp}).} 


 
  
 The elements of  defined above {are} always non-negative and increase with the compatibility between correspondences. Following~\cite{leordeanu2005spectral}, we consider the leading eigenvector of matrix  as the \textit{association} of each correspondence with {a} main cluster. Since this main cluster is statistically formed by the inlier correspondences, it is natural to interpret this \textit{association} as the inlier probability. The higher the association to the main cluster, the higher the probability of {a correspondence} being an inlier.
The leading eigenvector  can be efficiently computed by the power iteration algorithm~\cite{mises1929praktische}. 
We regard   
as the inlier probability, since only the relative value of  matters.
{Finally} we use the probability  as the weight to estimate the transformation through {least-squares fitting},

Eq.~\ref{eq:solve_r_t} can be solved in closed form by SVD~\cite{besl1992method}. For the sake of completeness, {we provide} its derivation in the supplementary~\ref{supp:proof}. By {performing}
such steps for each seed {in parallel}, the network produces a set of transformations  for hypothesis selection.









\subsection{Hypothesis Selection}
\label{subsec:hypo_select}
The final stage of \Name~involves selecting the best hypothesis among the transformations produced by the NSM module. 
The criterion for selecting the best transformation is based on the number of correspondences satisfied by {each}
transformation, 

where  is the Iverson bracket and  denotes {an} inlier threshold. The final inlier/outlier labels  {are} given by . 
We then 
recompute the transformation matrix using all the surviving inliers in a least-squares manner, which is a common practice~\cite{chum2003locally, barath2018graph}. 






\subsection{Loss Formulation}


{Considering the compatibility graph illustrated in Fig.~\ref{fig:assignment_graph}, previous works~\cite{choy2020deep, pais20203dregnet} mainly adopt node-wise losses, which supervise each correspondence individually. In our work, we further design an edge-wise loss to supervise the pairwise relations between the correspondences.}

\noindent\textbf{Node-wise supervision.} We denote  as the ground-truth inlier/outlier labels constructed by 
 where  {and}  are the ground{-}truth rotation and translation matries, respectively.
Similar to~\cite{choy2020deep,pais20203dregnet}, we first adopt the binary cross entropy loss as the node-wise supervision for learning the initial confidence by 

where  is the initial confidence predicted (Sec.~\ref{subsec:seed}).




\noindent\textbf{Edge-wise supervision} 
We further propose the spectral matching loss as our edge-wise supervision, formulated as
 
where  is the ground-truth compatibility value and  is the estimated compatibility value based on the feature similarity defined in Eq.~\ref{eq:feat_sm}.
This loss supervises the relationship between each pair of correspondences, serving as a complement to the node-wise supervision. Our experiments~(Sec.~\ref{subsec:ablation}) show that the proposed  remarkably improves the performance.




The final loss is a weighted sum of the two losses,

where  is a hyper-parameter to balance the two losses.


