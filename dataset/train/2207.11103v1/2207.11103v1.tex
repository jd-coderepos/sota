\documentclass[runningheads]{llncs}
\usepackage{graphicx}


\usepackage{tikz}
\usepackage{comment}
\usepackage{amsmath,amssymb,bm} \usepackage{color}
\usepackage[inline]{enumitem}
\usepackage{enumerate}
\usepackage[super]{nth}



\definecolor{citecolor}{RGB}{34,139,34}

\let\emptyset\varnothing

\usepackage{epsfig}
\usepackage{bbm}
\usepackage{xcolor}
\usepackage{mathtools}


\usepackage{tabularx}
\usepackage{multirow}
\usepackage[pagebackref=true,breaklinks=true,colorlinks,citecolor=citecolor,bookmarks=false]{hyperref}

\usepackage{booktabs}
\usepackage{makecell}
\usepackage{pifont}


\usepackage{adjustbox}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\usepackage{rotating}

\usepackage{listings}
\lstset{language=C,breaklines=true}

\usepackage{amsfonts}

\usepackage{fontenc}
\usepackage{multicol}
\usepackage{array}

\newcolumntype{H}{>{\setbox0=\hbox\bgroup}c<{\egroup}@{}}

\setlength{\tabcolsep}{4pt} \clearpage{}\newcommand{\adria}[1]{\textcolor{red}{\textbf{Adria: }{#1}}}
\newcommand{\tim}[1]{\textcolor{orange}{\textbf{Tim: }{#1}}}
\newcommand{\gui}[1]{\textcolor{blue}{\textbf{Guillem: }{#1}}}
\newcommand{\lau}[1]{\textcolor{magenta}{\textbf{Laura: }{#1}}}
\newcommand{\todo}[1]{\textcolor{red}{\textbf{TODO: #1}}}

\newcommand{\detr}{\mbox{DETR}}
\newcommand{\vistr}{\mbox{VisTR}}
\newcommand{\evis}{\mbox{DeVIS}}


\newcommand{\ie}{i.e.}
\newcommand{\eg}{e.g.}


\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\newcommand{\noobject}{\varnothing}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}

\newcommand{\denc}{d_{\rm enc}}
\newcommand{\ddec}{d_{\rm dec}}
\renewcommand{\Re}{\mathbb{R}}
\newcommand{\hy}{\hat{y}}
\newcommand{\hb}{\hat{b}}
\newcommand{\hp}{\hat{p}}
\newcommand{\ty}{\tilde{y}}

\renewcommand{\Sigma}{\mathfrak{S}}

\newcommand{\mhattn}{\text{\rm mh-attn}}
\newcommand{\mhsattn}{\text{\rm mh-s-attn}}
\newcommand{\attn}{\text{\rm attn}}
\newcommand{\xq}{X_{\rm q}} \newcommand{\xqout}{\tilde{X}_{\rm q}} \newcommand{\xqbb}{X'_{\rm q}} 

\newcommand{\xkv}{X_{\rm kv}} \newcommand{\weit}{T} \newcommand{\Nq}{N_{\rm q}} \newcommand{\Nkv}{N_{\rm kv}} \newcommand{\ques}{Q}
\newcommand{\keys}{K}
\newcommand{\vals}{V}
\newcommand{\posq}{P_{\rm q}} \newcommand{\poskv}{P_{\rm kv}} \newcommand{\proj}{L}

\newcommand{\dmodel}{d}
\newcommand{\dk}{d'}

\newcommand{\indic}[1]{\mathds{1}_{\{#1\}}}

\newcommand{\loss}[1]{{\cal L}(#1)}
\newcommand{\closs}[1]{{\cal L}_{\rm class}(#1)}
\newcommand{\bloss}[1]{{\cal L}_{\rm box}(#1)}
\newcommand{\maskloss}[1]{{\cal L}_{\rm mask}(#1)}
\newcommand{\iouloss}[1]{{\cal L}_{\rm iou}(#1)}
\newcommand{\diceloss}[1]{{\cal L}_{\rm DICE}(#1)}
\newcommand{\focaloss}[1]{{\cal L}_{\rm Focal}(#1)}
\newcommand{\hloss}[1]{{\cal L}_{\rm Hungarian}(#1)}

\newcommand{\lmatch}[1]{{\cal L}_{\rm match}(#1)}



\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
\clearpage{}
\begin{document}
\pagestyle{headings}
\mainmatter
\def\ECCVSubNumber{523}  

\newif\ifarxiv
\arxivtrue

\title{DeVIS: Making Deformable Transformers \\ Work for Video Instance Segmentation} 

\begin{comment}
\titlerunning{ECCV-22 submission ID \ECCVSubNumber} 
\authorrunning{ECCV-22 submission ID \ECCVSubNumber} 
\author{Anonymous ECCV submission}
\institute{Paper ID \ECCVSubNumber}
\end{comment}


\titlerunning{DeVIS}
\author{Adrià Caelles\inst{1} \and Tim Meinhardt\inst{2} \and
Guillem Brasó\inst{2} \and Laura Leal-Taixé\inst{2}}
\authorrunning{Caelles et al.}
\institute{Technical University of Catalonia \\
\email{adria.caelles@estudiantat.upc.edu}
\and
Technical University of Munich\\
\email{\{tim.meinhardt,guillem.braso,leal.taixe\}@tum.de}}


%
 
\maketitle

\begin{abstract}






Video Instance Segmentation (VIS) jointly tackles multi-object detection, tracking, and segmentation in video sequences.
In the past, VIS methods mirrored the fragmentation of these subtasks in their architectural design, hence missing out on a joint solution.
Transformers recently allowed to cast the entire VIS task as a single set-prediction problem.
Nevertheless, the quadratic complexity of existing Transformer-based methods requires long training times, high memory requirements, and processing of low-single-scale feature maps.
Deformable attention provides a more efficient alternative but its application to the temporal domain or the segmentation task have not yet been explored.

In this work, we present Deformable VIS (\evis{}), a VIS method which capitalizes on the efficiency and performance of deformable Transformers.
To reason about all VIS subtasks jointly over multiple frames, we present temporal multi-scale deformable attention with instance-aware object queries.
We further introduce a new image and video instance mask head with multi-scale features, and perform near-online video processing with multi-cue clip tracking.
\evis{} reduces memory as well as training time requirements, and achieves state-of-the-art results on the YouTube-VIS 2021, as well as the challenging OVIS dataset.

Code is available at~\url{https://github.com/acaelles97/DeVIS}.

\keywords{video instance segmentation, deformable transformers}

\end{abstract} 





\section{Introduction}
Video Instance Segmentation (VIS) simultaneously aims to detect, segment, and track multiple classes and object instances in a given video sequence. Thereby, VIS provide rich information for scene understanding in applications such as autonomous driving, robotics, and augmented reality.


Early methods~\cite{Yang2019vis,sip_mask,cross_vis} took inspiration from the multi-object tracking (MOT) field by applying tracking-by-detection methods to VIS, \eg, ~\cite{Yang2019vis} extends Mask R-CNN~\cite{he2017mask} with an additional tracking head.
The frame-by-frame mask prediction and track association allow for real-time processing but fail to capitalize on temporal consistencies in video data.
Hence, more recent VIS methods~\cite{stem_seg,mask_prop,prop_reduce,stmask,sg_net} moved towards an offline or near-online processing of clips by treating instance segmentations as 3D spatio-temporal volumes.

The recent success of Transformers~\cite{attention_is_all_you_need} in object recognition
inspired a new generation of VIS approaches.
Both \vistr{}~\cite{vistr} and IFC~\cite{IFC} deploy an encoder-decoder Transformer architecture, and closely mirror \detr{}'s~\cite{DETR} approach to object detection by formulating the VIS task as a set-prediction problem.
In this paradigm, instance masks are obtained in a single end-to-end trainable forward pass for all frames in a clip.
While their formulation is simple and appealing, both methods are limited by the quadratic complexity of full attention. 
\vistr{} achieves communication between frames by concatenating and encoding the pixels of all frames jointly. 
Such a multi-frame processing only amplifies the expensive attention computation and makes~\cite{vistr} suffer from long training times and high memory requirements.
IFC~\cite{IFC} tries to mitigate these issues by introducing inter-frame memory tokens to encode each frame individually.
However,~\cite{IFC} still relies on full attention for single frames and hence inherits the limitations of~\cite{DETR,vistr} to only process low- and single-scale feature maps.













Deformable attention~\cite{deformable_detr} resolves many of \detr{}`s efficiency and performance issues, and circumvents the quadratic computational complexity for its inputs by subsampling attention keys around a spatial reference point assigned to each query.
As shown in our experiments, a naive~\textit{deformabilization} of~\vistr{},~\ie, replacing full attention with deformable attention in the encoder-decoder of Figure~\ref{fig:method}, does not achieve satisfactory train time nor segmentation performance.
This is largely due to two problems: (i) learnable reference point offsets and attention weights introduce an unfeasible amount of new parameters for long clip sizes, and (ii) attention with spatially local reference points is not well-suited for the detection and tracking of objects moving through a sequence.


To make deformable attention work for VIS, we present~\textit{Deformable VIS} (\evis{}), a Transformer encoder-decoder which applies temporal deformable attention over multiple frames.
The reduction in computational complexity reduces training time and makes the processing of high-res feature maps at multiple scales feasible.
Furthermore, we motivate the alignment of reference points for decoder object queries individually for each object instance.
Our newly proposed image and video instance segmentation head takes full advantage of multi-scale features and improves mask quality significantly.
To run sequences with arbitrary lengths, we also introduce an improved multi-cue clip tracking.
The presented~\evis{} method achieves state-of-the-art results on the challenging YouTube-VIS~\cite{Yang2019vis} 2021 and OVIS~\cite{ovis} datasets and substantially reduces training time with respect to~\vistr{}.



In summary, our key \textbf{contributions} are:

\begin{itemize}
\item We present Deformable VIS (\evis{}), a VIS method which introduces temporal multi-scale deformable attention and instance-aware object queries. 

\item We present a new instance mask prediction head which takes full advantage of deformable attention and encoded multi-scale feature maps.

\item Our improved multi-cue clip tracking incorporates mask and class information to connect overlapping clips to sequences of arbitrary length.

\item Our method provides efficient training and achieves state-of-the-art performance on YouTube-VIS 2021 and the challenging OVIS dataset. 

\end{itemize}




























%
 \section{Related work}

We discuss VIS methods following the progression from tracking-by-detection to clip-level approaches culminating in modern Transformer-based architectures.

\noindent \textbf{Tracking-by-detection.}
The inception of the VIS task with the YouTube-VIS 2019~\cite{Yang2019vis} dataset also created Mask-Track R-CNN~\cite{Yang2019vis}.
As it is common in the multi-object-tracking community, Mask-Track R-CNN processes sequences frame-by-frame in an online tracking-by-detection manner.
To this end,~\cite{Yang2019vis} extends Mask R-CNN~\cite{he2017mask} with a tracking branch which allows it to not only detect and segment objects, but also assign instance identities via similarity matching of instance embeddings in the current frame and a memory queue.
SipMask~\cite{sip_mask} applies the same tracking head but with a single-stage detector and light-weight spatial mask preservation module.
The crossover learning scheme of CrossVIS~\cite{cross_vis} allows for a localization of pixel instance features in other frames.

As a clip-level method,~\evis{} processes sequences in clips, which greatly improves quality and robustness of instance mask as well as identity predictions.


\noindent \textbf{Clip-level.}
Clip-level processing allows for offline or near-online VIS methods.
The latter clip a sequence into multiple parts and hence rely on an additional clip tracking step.
The STEm-Seg~\cite{stem_seg} method took inspiration from offline trackers and is the first to model object instances as 3D spatio-temporal volumes by predicting pixel embeddings with Gaussian variances.
For a hybrid tracking-by-detection and clip-level method, the authors of MaskProp~\cite{mask_prop} extend Mask R-CNN with a mask propagation branch that operates between all frames in a video clip.
The~\textit{Produce-Reduce} heuristic applied in SeqMask-RCNN~\cite{prop_reduce} generates object instance proposals and reduces redundant identities based on multiple key frames.
STMask~\cite{stmask} and SG-Net~\cite{sg_net}, on the other hand, move beyond Mask R-CNN by applying improved one-stage detection methods.

Albeit their early success, these clip-level methods usually tackle the detect, segment, and track subtasks with separately trainable multi-stage pipelines.
Our~\evis{} approach enjoys the advantages of clip-level methods but in a unified and end-to-end trainable manner through the application of Transformers.

\noindent \textbf{Clip-level with Transformers.}
The~\vistr{}~\cite{vistr} method introduced Transformers~\cite{attention_is_all_you_need} to VIS by extending the \detr{}~\cite{DETR} object detector to the temporal domain.
Its unified Transformer encoder-decoder architecture concatenates and encodes all frames in a clip by computing attention between all pixels.
The decoder reasons about detection and tracking via multi-frame cross-attention between object queries and the encoded pixels, and produces instance masks with a subsequent segmentation head.
To avoid the expensive computation multi-frame pixel attention, the authors of~\cite{IFC} encode each frame separately and introduce memory tokens for a high-level inter-frame communication.


Our proposed~\evis{} method mitigates the aforementioned efficiency issues while still benefiting from the simultaneous encoding of multiple frames at once through the application of temporal multi-scale deformable attention with instance-aware reference point sampling.
We further propose a new segmentation head which takes full advantage of the Transformer encoded multi-scale feature maps, and an improved multi-cue clip-tracking.











%
 \begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{figures/devis_method_v2.png}
\caption{
An overview of our~\textbf{~\evis{} method} which applies temporal multi-scale deformable attention in a Transformer encoder-decoder architecture.
The encoder computes deformable attention between pixels across scales and frames in a given clip without suffering from quadratic complexity of full attention.
In the decoder, object queries attend to multiple frames, thereby providing consistent identity predictions.
    }
    \label{fig:method}
\end{figure} 
\section{\evis{}}
In this section, we present \textit{Deformable VIS} (\evis{}), a near-online end-to-end trainable Transformer encoder-decoder architecture for VIS.
We details its key components: (i) Temporal multi-scale deformable attention on a (ii) Transformer architecture with instance-aware object queries, (iii) a new multi-scale deformable mask head, and (iv) multi-cue clip tracking.

\subsection{Clip-level VIS with Transformers}
\label{sec:clip_level_vis_trans}

In this section, we present an overview of our~\evis{} method, shown in Figure~\ref{fig:method}, which follows the general Transformer-based clip processing pipeline of~\cite{vistr}.

\noindent \textbf{Clip-level VIS.} Given a video with  frames, the goal of VIS is to detect, track, and segment all  objects in a sequence.
This is achieved by providing a set of instance predictions  with .
A single prediction consists of the class , bounding box , and mask  for instance identity  at frame .
The VIS task expects a constant  for all .
If an object instance with identity  is not present for the entire sequence, the final set  can include less than  predictions with .
A clip-level VIS method processes clips with  frames and first provides subsets of instance predictions  with distinct sets of identities.
Usually, clips include overlapping frames to perform a final clip tracking/stitching step, which is responsible for merging identities of overlapping clip instance predictions.

\noindent \textbf{VIS with Transformers.}
To generate a set of clip predictions , Transformer encoder-decoder methods first extract feature maps for each frame independently with a convolutional neural network (CNN).
Feature maps are then concatenated to form a clip and temporal-positional encoding is added.
Treating each pixel as an input query, the subsequent Transformer encoder shares information spatially and temporally between frames via self-attention~\cite{attention_is_all_you_need}.
A set of learned object queries~\cite{DETR} computes self-attention and cross-attention with all encoded pixels in the Transformer decoder.
The total amount of object queries is equally distributed over the frames, hence, a query attends to all pixels in the clip but is responsible for the predictions on a fixed frame.
The decoder outputs a set of object query embeddings which are passed through separate multi-layer perceptrons to predict  and  for each frame in the clip.
The instance mask predictions  are obtained by computing attention maps for each object embedding, and feeding these together with the backbone feature maps into an additional instance mask head.
The entire model is trained end-to-end by matching the predicted outputs  via a Hungarian cost matrix to the ground truth.
We refer to~\cite{DETR} and ~\cite{vistr} for more details on the matching and loss computation.
Instance identities  are predicted by matching a fixed set of queries each from a different frame to the same identity during training.
At inference, all objects detected and segmented by one of these query sets are then assumed to belong to the same identity.
Intuitively, object queries in~\detr{} learn to detect objects in certain regions of the image.
For VIS, each set of queries is responsible for certain types of spatio-temporal object trajectories through the clip.

\subsection{Roadmap to temporal deformable attention}
\label{sec:temp_ms_def_att}

The authors of~\cite{deformable_detr} introduced deformable attention to~\detr{}, thereby reducing the computational footprint and training time substantially.
This allowed~\cite{deformable_detr} to improve single-image object detection performance by running the Transformer encoder-decoder on multiple feature scales.
While they only operated on single images, we present deformable attention for the \textit{temporal domain} which simultaneously captures spatial and temporal dependencies across multiple scales.

\subsubsection{Multi-Head Attention.} The original Transformer~\cite{attention_is_all_you_need} applies full attention between two sets of input queries  and keys .
An element of each of these sets is denoted by  and  with feature representations  and  of hidden size , respectively.
We denote the set of all  as .
The Multi-Head Attention (MHA) for query  and  attention heads is then computed via:



with learnable weight matrices  and  where .
The attention weights  are computed via dot product between  and  and are normalized over all keys .
The case where  is usually referred to as self-attention.
The Transformer encoder in~\detr{}~\cite{DETR} applies self-attention, where every entry in the feature map corresponds to a query.
Due to the computation of , which scales quadratically with the number of queries/keys, it is only feasible to run~\cite{DETR} on a single feature scale.


\subsubsection{Deformable attention.}
To mitigate~\detr{}'s computational issues around attention, the authors of~\cite{deformable_detr} suggest deformable attention which works on subsets of queries and weight computation of linear complexity.
To this end, each query  is assigned a reference point  in the feature map domain.
A subset of  queries is sampled around the reference point based on sample offsets  which are learnable via linear projection over the query feature .
For simplicity we omit the summation over multiple attention heads and denote the resulting \textit{Deformable Attention} (DA) for a single attention head  as:



The attention weights are normalized over the sample points  and also obtained via linear projection which avoids the expensive computation of dot product between queries.
Furthermore,~\cite{deformable_detr} present a multi-scale version of Equation~\ref{eq:dmha} which computes deformable attention across feature maps.

\subsubsection{Temporal multi-scale deformable attention.}
To encode spatio-temporal dependencies, which are crucial for for VIS, we present multi-scale deformable attention for the temporal domain.
That is, the sets of queries and keys include pixels from multiple scales and all frames in a clip.
Hence, we re-define  to be the stack of  multi-scale backbone features with  frames where .
Intuitively, a query  from frame  has the ability to compute attention with sampled keys from all frames and feature levels in a clip.
In comparison to full attention between all pixels, the sampling with offsets around a query`s reference point  reduces the computational effort substantially.
The number of keys is independent of the input resolution and only scales linearly with the number of feature scales and frames.
We define Temporal Multi-Scale Deformable Attention (TMSDA) module for a single  over a clip as:





As in~\cite{deformable_detr}, each reference point is represented with normalized coordinates  and re-scaled by  to allow for a sampling across feature maps  with different resolutions.
The scalar attention weight  is normalized by . 
We introduce  which adapts the number of keys sampled from a given frame and present two scenarios depending on whether a query  samples keys from its corresponding or other temporal frames:

Adding the temporal dimension allows each query to simultaneously sample keys from all feature scales as well as its current, and temporal frames.
Such a design is particularly beneficial for a consistent detection and identity prediction of objects moving and changing size over the sequence.
 removes all temporal connections and reverts back to the original deformable attention from~\cite{deformable_detr}.

In the following paragraphs, we give further details on how our temporal deformable attention is applied in our Transformer encoder-decoder architecture.




\subsection{TMSDA for VIS Transformers}
\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{figures/attention_maps_new.png}
    \caption{
    \textbf{Attention map visualization} of all frames in a clip for two object queries assigned to detect and segment object on the first, second and third frame.
The temporal attention computed on other frames successfully follows each object and hence provides consistent identity predictions.
Furthermore, we visualize the instance-aware reference point alignment (red dot) which adjusts the reference point of each first-frame query to the position of the respective object in the other frames.
    }


    \label{fig:attention_qual}
\end{figure}
%
 
Following the formal introduction of Temporal Multi-Scale Deformable Attention (TMSDA), we detail its integration into our Transformer encoder-decoder and present a novel design of instance-aware reference points for object queries.

\subsubsection{Transformer encoder.}
We replace the common full self-attention between all pixels in the encoder with TMSDA as in Equation~\ref{eq:tmsdmha}. 
The sampling design of deformable attention allows each pixel to only connect to a subset of other pixels close to spatial location of the reference points  for the current and other frames. 
The amount of temporal information being considered is controlled by the clip size  and the number of temporal sampling points .
Computing deformable attention allows us to encode all  ResNet~\cite{resnet} backbone feature scales, thereby replacing the role of a FPN~\cite{FPN}.
We apply an additive encoding to each pixel which indicates it spatial, temporal, and feature scale position.
The spatial locality of sampled keys limits the available temporal information potentially required for long clips with large object motions.
We believe our deformable approach is superior to~\cite{vistr,IFC} as it allows for a more fine-grained inter-frame communication on multiple scales in a unified formulation.




\subsubsection{Transformer decoder.}
Our Transformer decoder computes two attention steps for its object queries: self-attention and cross-attention between queries and the encoded frame features. 
For single-image detection~\cite{DETR}, the decoder self-attention helps to avoid duplicate object detections.
But for VIS Transformers, object queries also need to communicate about instance identities.
As explained in Section~\ref{sec:clip_level_vis_trans}, a subset of object queries are assigned to each frame of the clip.
To extract object information with an object query from its corresponding and future as well as past frames, we compute temporal deformable multi-scale cross-attention.
In contrast to the Transformer encoder, the reference points of object queries required for the cross-attention are learnable.
Each object query can leverage not only meaningful, local information from its particular object on its assigned frame, but from each of the other frames of the input clip.   
This helps improve consistent mask and identity predictions over the sequence.
Furthermore, we apply the same bounding box refinement as~\cite{deformable_detr} at each layer of the decoder.
This allows the initial reference point, \ie, sampling area, of a query to be adapted to the currently assumed coordinates of the object bounding box.












\subsubsection{Instance-aware object queries.}
Each object query of the decoder is by design able to learn different sampling reference points for different frames in the clip.
This not only allows to distinguish between a query's assigned frame and other frames, but to adjust the reference points on other frames according to its instance identity.
Hence, we introduce~\textit{Instance-aware object queries} which exploit the identity consistency across queries to adapt their reference points on other frames to the predicted bounding boxes belonging to their respective object identity.
This instance-aware reference point sampling is applied before each Transformer decoder layer, see Figure~\ref{fig:method}, and works in conjunction with the bounding box refinement of the reference point on the query frame.
If an object query successfully predicts the object bounding box on its own frame , other object queries belonging to the same instance but from other frames will benefit from an improved sampling on frame . 
Instance-aware object queries provide an additional communication between object queries of different frames and improve track consistency.




















\subsection{Multi-scale deformable mask head}\label{sec:def_segme_head}

\begin{figure*}
    \centering


    \includegraphics[width=1.0\columnwidth]{figures/architecture_mask-head_v2.png}
    
    \caption{
Overview of our new \textbf{multi-scale mask head} for video and image instance segmentation.
The upsampling of Transformer-encoded feature maps and multi-scale attention maps boosts performance significantly.
Attention maps are generated by computing multi-head attention between feature maps and object queries.
We indicate the hidden size and reduced set of object queries with N and C, respectively.
New connections to the decoder (blue) and encoder (red) are shown colored.
     }
    
    \label{fig:arch_mask-head}
\end{figure*} 






Processing multi-scale features currently only benefits the detection and tracking performance of the Transformer encoder-decoder.
We further explore the potential of Transformer encoded feature maps for mask prediction, and present a new multi-scale deformable instance mask segmentation head applicable to both image and video segmentation.





In Figure~\ref{fig:arch_mask-head}, we present an overview of its architecture.
Our encoder-decoder architecture outputs encoded backbone feature maps and object embeddings per clip.
As shown in Figure~\ref{fig:method}, the mask head predicts binary instance masks by generating attention maps for each of the embeddings and applying a series of convolutions and upsampling operations.
Attention maps are generated by computing Multi-Head Attention (MHA), as in Equation~\ref{eq:mha}, between an embedding an the encoded feature maps of its corresponding frame in the clip.
At different scales of the upsampling process, corresponding backbone feature maps are added to improve segmentation performance.
The mask head predicts all instance masks in a clip at once and does not consider any explicit temporal information. 
In particular, each query only computes attention maps for its own frame as opposed to the deformable cross-attention computation in the decoder.

To take full advantage of the encoded multi-scale features, we propose the following design changes in order to improve training time and segmentation performance for VIS and image instance segmentation: 

\noindent \textbf{Training only positive matches.}
During the training of~\detr{}, the detection loss is computed via Hungarian matching between the ground truth and the object embeddings produced by the decoder.
We reduce the mask head training time by only computing masks and their corresponding losses embeddings which receive a positive matching with a ground truth object.
Since object queries are designed to exceed the number of objects per frame by a large margin, this results in a substantial reduction of training time.

\noindent \textbf{Encoded feature maps.}
The deformable attention allows us to encode all but the largest (H/4 x W/4) backbone feature maps.
While~\cite{DETR} only encodes the lowest (H/32 x W/32) feature map and hence must use raw backbone features in its mask head, we are able to add and upsample multiple Transformer-encoded feature maps as visualized in Figure~\ref{fig:arch_mask-head}.
The application of encoded feature maps allows for a direct connection between the mask head and Transformer encoder.

\noindent \textbf{Multi-scale attention maps.}
To obtain informative attention maps, the object embeddings must compute MHA with the feature maps in the preceding Transformer encoder-decoder.
Therefore, we are able to generate attention maps not only for a single but multiple scales and concatenate these at the corresponding stages of the upsampling process.
The original mask head only generates attention maps for the smallest scale (H/32 x W/32) and is not able to benefit from the additional connections to the object embeddings, \ie{}, the Transformer decoder, during the upsampling process.
It should be noted, that although our Transformer encoder-decoder computes deformable attention as in Equation~\ref{eq:tmsdmha}, the attention maps are generated via regular attention as in Equation~\ref{eq:mha}.

\noindent \textbf{MDC.}
Furthermore, we replace the convolutions in the mask head with Modulated Deformable Convolutions (MDC) ~\cite{zhu2018deformable}.
This not only boosts performance and convergence time but presents a more unified deformable approach. 

\noindent \textbf{End-to-end training.}
The additional connections of our mask head in to the preceding encoder-decoder blocks, see colored lines in Figure~\ref{fig:arch_mask-head}, result in a significant performance boost for an end-to-end training of the full model, \ie{}, including the backbone and encoder-decoder.
Without these connections, the authors of~\cite{DETR} did not observe any improvement for a similar end-to-end training.







\subsection{Multi-cue clip tracking}
For~\evis{} to run on sequences with arbitrary length, we apply a near-online clip tracking/stitching similar to~\cite{IFC,stem_seg,mask_prop}.
To this end, we sequentially match instance identities from consecutive and overlapping clips. 
The final set of tack predictions  is computed by matching the current  with the next  via the Hungarian algorithm~\cite{hungarian}.
Instances in  without any match in  start a new instance identity.
For tracking-by-detection/online methods it is common to perform the data association with multiple cues~\cite{Yang2019vis}.
We are the first to extend this idea to clip-level tracking and compute multiple cost terms for identities  and  in  and , respectively:

\begin{itemize}
    \item Mask cost via negative volumetric soft IoU~\cite{IFC} between consecutive overlapping sets of masks  and .
    \item Class cost  if , and else 0, rewards consistent categories.
\item Score cost  matches clips with similar confidence.
\end{itemize}

The contribution of each cue is controlled by their corresponding weights ,  and . In cases of strong occlusion and imperfect mask predictions, the identity matching can additionally rely on consistent class and score information which further improves the identity preservation across a sequence.



































%
 \section{Experiments}
This section provides the most relevant details of our implementation, and the experimental setup for the following ablation studies and benchmark evaluations.
If not otherwise specified all results in the paper are obtained with a ResNet-50~\cite{resnet} backbone and follow the hyperparameters of~\cite{deformable_detr}.
For additional implementation and training details we refer to the appendix.



\noindent \textbf{Multi-scale deformable mask head.}
We train our mask head jointly with a pre-trained Deformable~\detr{}~\cite{deformable_detr} on COCO~\cite{COCO} for 24 epochs, decaying the learning by  after epoch 15.
For batch size 2, we use 8 GPUs with 32GB memory for 2 days (345 GPU hours).
The initial learning rates of the backbone, encoder-decoder, and mask head are , , and , respectively. 


\noindent\textbf{\evis{}.}
We initialize our~\evis{} model from the preceding end-to-end instance mask head training and then fine-tune for additional 10 epochs on the respective VIS dataset.
With one clip per GPU, we use 4 GPUs for 1.5 days (120 GPU hours) with 18GB memory for YouTube-VIS 2019~\cite{Yang2019vis} dataset.
The increased number of objects per frame in both YouTube-VIS 2021~\cite{Yang2019vis} and OVIS~\cite{ovis} require 24GB of memory.
In contrast to~\cite{vistr,IFC}, we are able to train with data augmentations on different input scales and apply a learned additive temporal encoding.
If not otherwise specified, all models use clip size , stride  and reference point keys .
We run multi-cue clip tracking with . 
For the ablations, we report best validation scores after training 10 epochs.




\noindent \textbf{Datasets and metrics.}
We evaluate results on the \textit{YouTube-VIS 2019/2021}~\cite{Yang2019vis} datasets which contain 2883 and 3859 high quality videos with 40 unique object categories, respectively.
The latter provides improved ground truth annotations.
The OVIS~\cite{ovis} dataset includes severe occlusion scenarios on 901 sequences.
We measure the Average Precision (AP) as well as Average Recall (AR).
For instance mask prediction on images, we report the AP based on mask IoU on the common COCO~\cite{COCO} dataset.


\begin{table*}[t]

\caption{
Ablation of our main~\textbf{\evis{} contributions} and the incremental built from a naive Deformable~\vistr{} to our final model.
\textit{Increase spatial inputs} denotes multi-scale training on higher input resolutions.
}

\label{tab:ablation_main}

\centering

\resizebox{\columnwidth}{!}{

\begin{tabular}{lrccc | cccHc}

\toprule



Method  &  \thead{Clip \\ size }   &  &   &\thead{Feature \\ scales} & AP &  AP &\thead{Training \\ GPU hours} & FPS &\thead{\#params}\\

\toprule
\multirow{2}*{\shortstack[l]{\vistr{}}}

& 36  & All  & All   & 1 & 36.1 & -- & 350 & 69.9 &57M\\
& 6  & All  & All   & 4 & -- & -- &OOM & X &64M\\




\midrule
\multirow{4}*{\shortstack[l]{Deformable \\ \vistr{}}}       
                    
                    &36  &4  &4    &1   &34.2  & -- &260 &64.3 &47M\\
                    


                    &36  & 4  & 0  &1   &35.3   & -- &155 &83.2 &36M\\
                    






                    &6  &4  &4   & 1 &   34.0   & -- &48  &31.8  &41M\\
                    
                    &6  &4  &0   & 1 &  32.4    & -- &30  &XX  &36M\\



\midrule
\evis{}                             &  &  &     & &   &  && &\\
+ Increase spatial inputs          &6  &4  &4      & 4 & 35.9  & +1.9  & 93    & 25.5  & 48M\\
+ Instance-aware object queries     &6  &4  &4      & 4 & 37.0  & +1.1  & 112    & 22.3  & 48M\\
+ Multi-scale mask head             &6  &4  &4      & 4 & 40.2  & +3.2  & 120      & 23.4  & 48M\\
+ Multi-cue clip tracking           &6  &4  &4      & 4 & 41.9  & +1.7  & 120      & 18.4  & 48M\\
+ Auxiliary loss weighting  &6  &4  &4  & 4 &44.0  & +2.1  & 120   & 18.4  & 48M\\

\bottomrule

\end{tabular}
}

\vspace{-0.5cm}

\end{table*} 
\subsection{Ablation studies}

We present ablations demonstrating the effectiveness of our contributions for~\evis{} and provide detailed insights on the new mask head and clip tracking.

\noindent \textbf{Making~\evis{} work for VIS.}
In Table~\ref{tab:ablation_main}, we demonstrate the shortcomings of a naive deformabilization, \ie, replacement of~\vistr{}'s~\cite{vistr} full attention with deformable attention.
We indicate full attention over all pixels wit .
Running temporal deformable attention offline, \ie, with clip size , reduces training time compared to~\vistr{}, but converges with an unsatisfactory performance of 34.2.
This is due to the incapability of spatially restricted reference points to compute meaningful temporal attention connections over large clip sizes.
This assumption is supported by the 1.1 improvement of an offline Deformable~\vistr{} without any temporal connections, \ie, .
A reduction of the clip size to  in a near-online fashion mitigates the reference point issues while also requiring only a fraction of the original training time (155 vs. 48 GPU hours).
To further support our hypothesis, we demonstrate how removing temporal connections  for smaller clip sizes  does indeed deteriorate the performance.
However, without fully capitalizing on the efficiency of deformable attention its best version with 34.0 is still inferior to~\vistr{}.

The first~\evis{} row demonstrates the potential gains (1.9) from increasing the number of feature scales to  and training on higher input resolutions.
The transition to multiple scales is a prerequisite for the application of our new mask head and only feasible to train on smaller clip sizes.
The same model with full attention (second row) results in out-of-memory (OOM) errors.
Naturally, this increases the total number of parameters and training time but both remain far below~\vistr{}.
Instance-aware object queries come with a neglectable increase in training time but provide a 1.1 AP boost.
The individual contributions of the mask head and clip tracking additions are ablated in Table~\ref{tab:ablation_mask_head} and~\ref{tab:ablation_multi_cues}, respectively.
Both result in additional performance boosts without substantially increasing the computational costs.
A cascaded weighting of the decoder auxiliary loss terms as applied in~\cite{AuxLoss} increases results further.
Our final model benefits from deformable attention with low training times and parameter counts surpassing a naive Deformable~\vistr{} approach by 11.1 points.


In Table~\ref{tab:ablation_temporal}, we ablate different clip sizes  and number of temporal sampling keys  for our final configuration.
We used the optimal clip size  for our~\evis{} ablations in Table~\ref{tab:ablation_main}.
Both smaller and larger clip sizes resulted in worse performance either due to the lack of temporal connections or the aforementioned problem of spatially local reference points in the encoder, respectively.
Running all  feature scales was only possible for a clip length of up to .
Furthermore, we ablate the removal of temporal connections which resulted in a large relative drop for the challenging OVIS~\cite{ovis} dataset.

\begin{table*}[t]
\parbox[t][][t]{.48\linewidth}{

\centering

\caption{Ablation for the \textbf{multi-scale mask head} on COCO~\cite{COCO}.
The baseline applies the original mask head as in~\detr{} with Deformable~\detr{}~\cite{deformable_detr}.
}
\label{tab:ablation_mask_head}

\resizebox{0.48\columnwidth}{!}{\begin{tabular}{l | cHHcH}

\toprule
Mask head & \thead{Mask \\ mAP} &AP &AP & \thead{Training \\ GPU hours} & FPS\\

\midrule





Baseline as in~\cite{DETR} with~\cite{deformable_detr}          & & & &242 & X\\
+ Train only positive matches       &  & & &58 & X \\
+ Encoded feature maps              &  & & &56  & X\\
+ Multi-scale attention maps        &  & & &59  & X\\
+ MDC                               &  & & &78  & X\\
+ End-to-end training                  &  & & &345  & X\\


\bottomrule

\end{tabular} }
}\hfill
\parbox[t][][t]{.48\linewidth}{
\centering

\caption{
Removing \textbf{temporal connections} with  results in performance drops across all datasets.
We observe an optimal clip size of .
}
 
 \label{tab:ablation_temporal}
 


\resizebox{0.48\columnwidth}{!}{

\begin{tabular}{cc|cc|cc|cc}
  \toprule
    \multirow{2}*{\thead{Clip \\ size }}& \multirow{2}*{} & \multicolumn{2}{c|}{YT-VIS 19~\cite{Yang2019vis} } & \multicolumn{2}{c|}{YT-VIS 21~\cite{Yang2019vis}} & \multicolumn{2}{c}{OVIS~\cite{ovis}} \\
    
    \cmidrule{3-8}
    


  & & AP &  AP & AP &  AP & AP &  AP \\
  \midrule
  6 & 4 &  & -- & & -- & & --\\
  \midrule
  6 & 0 &  & -3.2 & & -3.6 & & -4.1 \\
  3 & 4 & 41.0 & -3.4 &-- & -- &-- & --\\
  9  & 4 & 42.4 & -2.0 &-- & -- &-- & --\\
  12  & 4 & 41.6 & -2.8 &-- & -- &-- & --\\
  \bottomrule
 \end{tabular} }
}

\vspace{-0.3cm}

\end{table*}

 
\noindent \textbf{Multi-scale mask head.}
We evaluate our contributions on the mask head in Table~\ref{tab:ablation_mask_head} on COCO~\cite{COCO} instance segmentation.
The baseline represents a straightforward application of the original~\detr{}~\cite{DETR} mask head with the Deformable~\detr{}~\cite{deformable_detr} detector, which is not only 9.6 points worse than~\cite{DETR} (see Table~\ref{tab:eval_COCO}), but suffers from an unfeasible long training time largely due to the increased number of object queries in~\cite{deformable_detr}.
By computing instance masks only for queries positively matched with a ground truth object, we are able to reduce the training time 4-fold.
The following two additions take full advantage of the encoded multi-scale features of~\cite{deformable_detr} and result in a mask AP of 29.2.
For top performance, we further add MDC~\cite{zhu2018deformable} and train the entire model end-to-end.
Our mask head without end-to-end training is still inferior to~\detr{}.
This can be attributed to the sparse computation of deformable attention which makes the generated attention maps less suitable for pixel-level segmentation.
However, the end-to-end training fully realizes the potential of the additional connections between our mask head and the encoder-decoder.
The increased training time is justified by the overall 4.7 point improvement over~\detr{}.





\begin{table*}[t]
\parbox[t][][t]{.48\linewidth}{

\centering

\caption{
Comparison of instance segmentation results on~\textbf{COCO}~\cite{COCO}.
\mbox{Mask R-CNN} is from detectron2~\cite{wu2019detectron2}.
}
\label{tab:eval_COCO}

\resizebox{0.48\columnwidth}{!}{\begin{tabular}{l | cccccc|c}
\toprule
Methods &AP &AP &AP  &AP  &AP  &AP  &FPS\\
\midrule

\detr{}~\cite{DETR}                & & & & & & &--\\
IFC~\cite{IFC}                 & & --      & -- &-- &-- &-- &--\\
Mask R-CNN~\cite{he2017mask}      & & & & & & &21.4\\
Mask2Former~\cite{Mask2Former}  & &-- &-- & & & &13.5\\
\midrule
\textbf{Ours}                & & & & &
& &12.1\\

\bottomrule
\end{tabular} 
}
}\hfill
\parbox[t][][t]{.48\linewidth}{
\centering

\caption{
Contribution of the additional class and score cost terms in our \mbox{\textbf{multi-cue}} \textbf{clip tracking}.
}
 \label{tab:ablation_multi_cues}

\resizebox{0.48\columnwidth}{!}{

\begin{tabular}{l|ccc}
  \toprule
    Clip tracking cues  & AP &AP &AP\\
  \midrule
Vol. soft mask IoU & & &  \\
Vol. soft mask IoU + Score & & &   \\
  Vol. soft mask IoU + Class & &  &   \\
  Vol. soft mask IoU + Score + Class & &  &  \\

  \toprule
 \end{tabular} }
}

\vspace{-0.2cm}

\end{table*} 
\noindent \textbf{Multi-cue clip tracking.}
To improve the track consistency between clips, we introduce additional cues to the common mask-based clip tracking.
The clip tracking row in Table~\ref{tab:ablation_main} replaces the original~\vistr{} mask IoU cost term with a combination of volumetric soft mask IoU, class, and score costs.
After tuning the cost weighting parameters, we ablate their individual contributions in Table~\ref{tab:ablation_multi_cues}.
The new class and score terms provide an overall boost of 2.3 AP points.














\begin{table*}[t]

\caption{
Comparison of VIS methods on the~\textbf{YouTube-VIS 2019/2021}~\cite{Yang2019vis} validation sets.
FPS measurements denoted with  are extracted from~\cite{VISOLO}.
With  and the  we denote a joint training with COCO~\cite{COCO} and unpublished methods, respectively.
}

\vspace{-0.5cm}

\label{tab:eval_vis_all}
\begin{center}
\resizebox{\columnwidth}{!}{\begin{tabular}{l l|c|r|ccccc|ccccc|ccc}
\toprule
\multicolumn{2}{c|}{\multirow{2}*{Method}} & \multicolumn{1}{c|}{\multirow{2}*{Backbone}} & \multicolumn{6}{c}{YT-VIS 19~\cite{Yang2019vis}} & \multicolumn{5}{|c}{YT-VIS 21~\cite{Yang2019vis}} & \multicolumn{3}{|c}{OVIS~\cite{ovis}}\\
\cmidrule{4-17}
& &  & \multicolumn{1}{c|}{FPS} & AP & AP & AP & AR & AR & AP & AP & AP & AR & AR & AP & AP & AP\\

\midrule
\multirow{7}*{\rotatebox[origin=c]{90}{Online}} & MaskTrack-RCNN~\cite{Yang2019vis} & R50 & 26.1 & 30.3 & 51.1 & 32.6 & 31.0 & 35.5 &  28.6 & 48.9 & 29.6 & 26.5 & 33.8 &  &  &  \\
& SipMask~\cite{sip_mask} & R50 & 35.5 & 33.7 & 54.1 & 35.8 & 35.4 & 40.1  & 31.7 & 52.5 & 34.0 & 30.8 & 37.8 &  &  &  \\
& SG-Net~\cite{sg_net} & R50 & 23.0 & 34.8 & 56.1 & 36.8 & 35.8 & 40.8 & -- & -- & -- & -- & -- & -- & -- & --\\
& CompFeat~\cite{CompFeat} & R50 &  & 35.3 & 56.0 & 38.6 & 33.1 & 40.3 & -- & -- & -- & -- & -- & -- & -- & --\\
& CrossVIS~\cite{cross_vis} & R50 & 39.8 & 36.3 & 56.8 & 38.9 & 35.6 & 40.7 & 34.2 & 54.4 & 37.9 & 30.4 & 38.2 &  &  &  \\
& STMask~\cite{stmask} & R50-DCN & 28.6 & 33.5 & 52.1 & 36.9 & 31.1 & 39.2 & 30.6 & 49.4 & 32.0 & 26.4 & 36.0 & -- & -- & --\\
& VISOLO\cite{VISOLO} &50 & 40.0 & 38.6 & 56.3 & 43.7 & 35.7 & 42.5 & 36.9 & 54.7 & 40.2 & 30.6 & 40.9 & -- & -- & --\\

\midrule
\multirow{8}*{\rotatebox[origin=c]{90}{Offline}}
& VisTR~\cite{vistr} & R50 & 69.9  & 36.2 & 59.8 & 36.9 & 37.2 & 42.4 & -- & -- & -- & -- & -- & -- & -- & --\\
& SeqMask-RCNN~\cite{prop_reduce} & R50 & 3.8 & 40.4 & 63.0 & 43.8 & 41.1 & 49.7 & -- & -- & -- & -- & -- & -- & -- & --\\
& IFC~\cite{IFC} & R50 & \textbf{107.1} & 41.2 & 65.1 & 44.6 & 42.3 & 49.6 & 35.2 & 57.2 & 37.5 &-- &-- & -- & -- & --\\

& IFC~\cite{IFC} & R101 & 89.4 & 42.6 & 66.6 & 46.3 & 43.5 & 51.4 & -- & -- & -- &-- &-- & -- & -- & --\\
& SeqFormer~\cite{SeqFormer} & R50 & 12 & 45.1 & 66.9 & 50.5 & 45.6 & 54.6 & 40.5 & 62.4 &43.7 &36.1 & 48.1 & -- & --\\
& SeqFormer~\cite{SeqFormer} & Swin-L & -- & 59.3 & 82.1 & 66.4 & 51.7 & 64.4 & 51.8 & 74.6 &58.2 &42.8 & 58.1 & -- & --\\


& Mask2Former~\cite{Mask2Former} &R50 &-- & \textbf{46.4} & \textbf{68.0} & \textbf{50.5} &-- & -- & 40.6  & 60.9 &41.8 &-- & -- &-- &-- & --\\
& Mask2Former~\cite{Mask2Former} &Swin-L &-- & \textbf{60.4} & \textbf{84.4} & \textbf{67.0} &-- & -- & 52.6  & 76.4 & 57.2 &-- & -- &-- &-- & --\\

\midrule
\multirow{5}*{\rotatebox[origin=c]{90}{Near-online}}
& MaskProp~\cite{mask_prop} (T=12) & R50 & -- & 40.0 & -- & 42.9 & -- & -- & -- & -- & -- & -- & -- & -- & -- & --\\
& STEm-Seg~\cite{stem_seg} & R50 & 3.0 & 34.6 & 55.8 & 37.9 & 34.4 & 41.6 & -- & -- & -- & -- & -- &  &  &  \\
& IFC~\cite{IFC} (T=5, S=1) & R50 & 46.5 & 39.0 &60.4 &42.7 & 41.7 & 51.6 & -- & -- & -- & -- & -- & -- & -- & --\\
& \textbf{\evis{}} (T=6, S=4) & R50 & 18.4  &44.4 & 66.7 &48.6 &42.4 &51.6 &\textbf{43.1} & \textbf{66.8} & \textbf{46.6} & \textbf{38.0} & \textbf{50.1} &\textbf{23.8} &\textbf{48.0} &\textbf{20.8}\\
& \textbf{\evis{}} (T=6, S=4) &SwinL & 18.4  &57.1 & 80.8 & 66.3 &50.8 &61.0 & \textbf{54.4} & \textbf{77.7} & \textbf{59.8} & \textbf{43.8} & \textbf{57.8} &\textbf{34.6} &\textbf{58.7} &\textbf{36.8}\\




\noalign{\vskip 2mm}    

\bottomrule
\end{tabular}}
\end{center}

\vspace{-0.8cm}

\end{table*}


 

\subsection{Benchmark evaluation}
\vspace{-0.2cm}
\noindent \textbf{Video instance segmentation.}
To demonstrate the effectiveness of~\evis{} in comparison with other VIS methods, we report results on the YouTube-VIS 2019 and 2021~\cite{Yang2019vis} dataset in Table~\ref{tab:eval_vis_all}.
Our method achieves state-of-the-art performance with respect to all previous methods on the YouTube-VIS 2021 dataset by significant margins of 2.5 and 1.8 for ResNet-50~\cite{resnet} and Swin-L~\cite{SwinTransformer} backbones, respectively.
The benefits of our temporal connections and multi-scale feature encoding in conjunction with the mask head are most apparent by our 7.9 improvement over IFC~\cite{IFC}.
Both ~\vistr{} and IFC achieve lower runtimes mirroring the relation between Deformable~\detr{}~\cite{deformable_detr} and~\detr{}~\cite{DETR}.
Furthermore, both methods rely on an expensively pretrained~\detr{} limiting their adaptability dramatically.
In addition to YouTube-VIS, we are the first Transformer-based method to present results on the OVIS~\cite{ovis} dataset.
We surpass the previous best method~\cite{cross_vis} by 5.7 points for ResNet-50.
Due to its pixel-level encoding of image features,~\evis{} excels on OVIS' challenging occlusions scenarios.


\noindent \textbf{Image instance segmentation.}
The new multi-scale mask head does not only boost VIS performance but also excels on image segmentation.
In Table~\ref{tab:eval_COCO}, we evaluate performance on COCO~\cite{COCO} even surpassing Mask R-CNN~\cite{he2017mask}.
We present this as contribution beyond the VIS community and regard our mask head as a valuable completion of Deformable~\detr{}. 
Interestingly,~\evis{} is superior to Mask2Former despite the inferiority of our mask head for image segmentation.
We regard this as a strong sentiment for our~\evis{} video approach.



\vspace{-0.2cm} \vspace{-0.2cm}

\section{Conclusion}
\vspace{-0.2cm}
We have proposed a novel VIS method which applies a Transformer encoder-decoder architecture to clips of frames in a near-online fashion.
To mitigate the efficiency issues of previous Transformer-based methods suffering from quadratic input complexity, we propose temporal deformable attention with instance-aware object queries.
Deformable attention allows us to benefit from multi-scale feature maps and led to the introduction of a new powerful instance mask head.
Furthermore, we present multi-cue tracking with class and score terms.
Our~\evis{} method achieves state-of-the-art results on two VIS datasets and hopefully paves the way for future applications of deformable attention for VIS.

 
\clearpage


\ifarxiv
    \appendix

    \def\suppabstract{This section provides additional material for the main paper:
\S\ref{sec:imp_details_appendix} contains further implementation details for our \evis{} method and its training. 
In \S\ref{sec:ablation_appendix}, we discuss further ablations on the effect of the clip size and temporal sampling points.
Furthermore, we complement the qualitative results of the main paper with selected illustrations in (\S\ref{sec:qual_results_appendix}).
These include qualitative results, more detailed attention maps and failure cases.
    }

    \section*{Appendix}
    \suppabstract
    
    \newcommand{\sref}[1]{Sec.~\ref{#1}}
    \setcounter{table}{0}
    \renewcommand{\thetable}{A.\arabic{table}}
    
    \setcounter{figure}{0}
    \renewcommand{\thefigure}{A.\arabic{figure}}
    
    





\section{Implementation details}

\label{sec:imp_details_appendix}

\noindent \textbf{Multi-scale mask head}
To improve convergence of the end-to-end full model training, we increase the dice and mask loss weights to .
Furthermore, we add both loss terms to the auxiliary losses of the \nth{3} decoder layer.
For trainings of the new mask head with~\evis{}, we keep the original mask loss weights but also add the corresponding terms to the \nth{3} auxiliary loss.
To further speed up the inference, we reduce the \textit{top k} from 100 as in ~\cite{deformable_detr} to 50.



\noindent \textbf{\evis{}}
We train our model with a total of 60 object queries for YouTube-VIS 2019 and 180 object queries for YouTube-VIS 2021 and OVIS.
With a clip size of  this assigns 10 or 30 queries to each frame.
OVIS includes sequences with up to 44 unique instances and hence requires an increased number of object queries.
The class head is solely responsible for categorization of each object query.
This means, the mask head predicts a single mask for each object query and does not produce per-category outputs as Mask R-CNN~\cite{he2017mask}.
In order to associate a single class with a trajectory of queries, we compute the mean score over each class.
This results in a total of number of classes times number of object queries per frame trajectories.
The final output is selected in a top-k manner from the total set of trajectories.
It should be noted, if k is larger than the number of queries per frame, a single trajectory is associated with multiple labels.
To work as similar as possible to~\vistr{}~\cite{vistr}, our ablation experiments apply .
For our benchmark experiments, we increase the value to  and  for YouTube-VIS 2019 and YouTube-VIS 2019/OVIS, respectively.

For the auxiliary loss weighting~\cite{AuxLoss}, we use the following incremental weights from the first to the final layer: , , , ,  and .
Such a weighting decreases the influence of early decoder layers with an emphasized focus on optimizing the outputs of the final layer.
In contrast to~\cite{AuxLoss}, we keep the same weighting during the entire training.
However, the weighting is not applied to the mask auxiliary loss on the third layer.
As the total contribution from non-mask losses is heavily reduced, we use , since we did not observe any benefit in this case from increasing its contribution.  
Furthermore, we use  weight for the class cost and matching, following~\cite{vistr}. 
In comparison to~\cite{deformable_detr}, our encoder-decoder model introduces only changes to the dimensions of a few parameters, namely, object queries and linear projections for sample offset and attention weight.
We train the object queries, the introduced temporal learned embedding and the classification head from scratch. 
For the linear projections, we duplicate the existing pre-trained weights for each new temporal position, adapted to the number of points . 
We apply initial learning rates of  and  for the backbone, and rest of the model including the mask head, respectively. 
We drop these by  at epoch  and  for YouTube-VIS 2019,  and  for YouTube-VIS 2021 and  and  for OVIS.

\noindent \textbf{Learnable sample offset and attention weight parameters}
To circumvent the quadratic input complexity of regular attention, deformable attention~\cite{deformable_detr} learns linear projections which infer the sample offsets  and attention weights  for a query , sampling point  and attention head .
In our temporal deformable attention formulation, we separately learn linear projections for the current and temporal frames, see Table~\ref{tab:learn_offset_and_weight_params_dims} for their dimensions.
This allows to individually set  and  and ablate configurations without temporal connections.
Given a clip with , the queries assigned to a frame  apply their  learned temporal projections to the remaining frames in the following frame order:

We follow VIS convention to run inference on reduced input resolutions with 360 pixels but upsample to the required benchmark resolution before the clip tracking.
The runtime frames per second (FPS) measurement do not include this upsampling.


\begin{table}
\centering
\begin{tabular}{c | c}
    Query frame  & Temporal frame indices \\
    \midrule
    0 & [1, 2, 3, 4, 5] \\
    1 & [0, 2, 3, 4, 5] \\
    2 & [0, 1, 3, 4, 5] \\
    3 & [0, 1, 2, 4, 5] \\
    4 & [0, 1, 2, 3, 5] \\
    5 & [0, 1, 2, 3, 4]
\end{tabular}

\end{table}


This requires the same projection parameters to predict offsets/weights with different temporal distances to the current frame which is possible due to the learned temporal encoding added to each query.
Furthermore, the explicit discrimination between the current and temporal frames with respect to a query allows decoder object queries to focus on predictions for their frame while taking additional temporal information under consideration.
It should also be noted that all the terms in Table~\ref{tab:learn_offset_and_weight_params_dims} scale linearly with the number of frames in a clip and and do not depend on the input resolution.

\begin{table*}
\centering
\caption{
Dimensions of learnable sample offset and attention weight parameters with object queries , attention heads , feature scales  and clip size .
    }

\label{tab:learn_offset_and_weight_params_dims}

\begin{tabular}{r | c | c}
\toprule
                    & Current frame & Temporal frames \\
\midrule

Attention weights   &  &   \\
Sampling offsets    &  &  \\

\bottomrule
\end{tabular}
\end{table*} 
\section{Ablation studies}

\label{sec:ablation_appendix}

In Table~\ref{tab:ablation_temporal_extended}, we extend the analysis of the main paper on the effect of adding more or less temporal information by altering the number of  for the encoder and decoder separately.
The ablation of different  require individually trained models with differing number of parameters.
Given a trained~\evis{} model, the final runtime and performance can be modulated by adjusting the clip stride , \ie, instance overlap, during inference.
For experiments with differing clip size , we adjust the stride to keep the number of overlapping frames between constant.
We set  as in~\cite{deformable_detr} for all experiments and obtain the same optimal value for .





\begin{table*}[t]

 \caption{
Removing \textbf{temporal connections} with  results in performance drops across all datasets.
We observe an optimal clip size of .
}
 
 \label{tab:ablation_temporal_extended}
 
  \center
\begin{tabular}{cHc|Hcc|cc|cc}
  \toprule
    \multirow{2}*{\thead{Clip \\ size }}& \multirow{2}*{\thead{Clip \\ stride }} & \multirow{2}*{} & \multicolumn{3}{c|}{YT-VIS 19~\cite{Yang2019vis} } & \multicolumn{2}{c|}{YT-VIS 21~\cite{Yang2019vis}} & \multicolumn{2}{c}{OVIS~\cite{ovis}} \\
    
    \cmidrule{4-9}
    


  & & & FPS & AP &  AP & AP &  AP & AP &  AP \\
  \midrule
  6 & 4 & 4 & 18.4 & 44.4 & -- & 43.1 & -- & 23.8 & --\\
  \midrule
  
  3 & 1 & 4 &16.3 & 41.0 & -3.4 &-- & -- &-- & --\\
  9 & 7 & 4 &34.1  & 42.4 & -2.0 &-- & -- &-- & --\\
  12 & 10 & 4 & 36.6 & 41.6 & -2.8 &-- & -- &-- & --\\
\midrule
  6 & 4 & 0 & 57.7 & 41.2 & -3.2 & 39.5 & -3.6 & 19.7 & -4.1 \\
  6 & 4 & 1 & 40.1 & 41.2 & -3.2 &-- & -- &-- & --\\
  6 & 4 & 2 & 38.8 & 43.4 & -1.0 &-- & -- &-- & --\\
  6 & 4 & 3 & 31.1 & 43.6 & -0.8 &-- & -- &-- & --\\
    
  \bottomrule
 \end{tabular}
 
\end{table*} 
\section{Qualitative results}

\label{sec:qual_results_appendix}


In Figure~\ref{fig:qual_results_youtube} and~\ref{fig:qual_results_ovis}, we present additional qualitative results for the YouTube-VIS 2019/2021 and OVIS, respectively.
In Figure~\ref{fig:qual_results_instance_aware_queries}, we demonstrate the instance-aware object queries and their reference point alignment.
To this end, we visualize the attention maps with reference points (cross) at the first, third and last layer for the query from the third frame.
The reference point from that query on other frames aligns with the bounding box positions on these respective frames.
Finally, we present failure cases in Figure~\ref{fig:qual_failure_category} and~\ref{fig:qual_failure_segm}.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{figures/YT21_qualitative1.png}
    \includegraphics[width=1\textwidth]{figures/YT21_qualitative2.png}
    \includegraphics[width=1\textwidth]{figures/YT21_qualitative3.png}
    \caption{
    Qualitative results on the YouTube-VIS 2019/2021~\cite{Yang2019vis} datasets.
    }
    \label{fig:qual_results_youtube}
\end{figure}
%
 \begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{figures/ovis_qualitaitve1.png}
    \includegraphics[width=1\textwidth]{figures/ovis_qualitaitve2.png}
    \includegraphics[width=1\textwidth]{figures/ovis_qualitative3.png}
    \caption{
    Qualitative results on the OVIS~\cite{ovis} dataset.
    }
    \label{fig:qual_results_ovis}
\end{figure}
%
 
\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{figures/Category_FailureCases.png}
    \caption{
    Failure predictions due to the category. Most of our errors on YouTube-VIS 2019/2021~\cite{Yang2019vis} datasets are from failed categories, specially the ones that are more under-represented on the training data. 
    }
    \label{fig:qual_failure_category}
\end{figure} \begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{figures/Yt21_FailedSegmentation.png}
    \caption{
    Segmentation failure on YouTube-VIS 2021~\cite{Yang2019vis} dataset. We also struggle sometimes to segment overlapping instances from the same category on YouTube-VIS dataset. We do a better job on similar scenarios \ref{fig:qual_results_ovis} on OVIS, and we argue it is because the model sees a lot more of these examples during training in this other dataset. 
    }
    \label{fig:qual_failure_segm}

\end{figure} 
\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{figures/qual_instance-aware_bears.png}\\
    \vspace{+0.7cm}
    \includegraphics[width=1\textwidth]{figures/qual_instance-aware_cows.png}

    \caption{
    Visualization of the instance-aware object queries and their alignment of reference points for temporal frames on example sequences from~\cite{Yang2019vis}.
To this end, we plot the not yet aligned reference points applied in the first decoder layer (first row) and their subsequent alignment to the predicted bounding box centers after the \nth{2} and \nth{6} layer.
    }
    \label{fig:qual_results_instance_aware_queries}
\end{figure}
%
 










 \fi

\clearpage
\bibliographystyle{splncs04}
\bibliography{egbib}

\end{document}
