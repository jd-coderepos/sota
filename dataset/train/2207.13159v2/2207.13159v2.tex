\section{Experiment Settings and Results} \label{cap:results}

In this section we presents the settings used in our experiments, the achieved results, and the performed ablation study.


\subsection{Datasets}

As already stated in \secref{sec:intro}, we cannot share the dataset related to our industrial application.
Moreover, in order to fairly evaluate our model, and to compare it with other works in the CD field,
we used the following public and widely adopted aerial building images datasets: LEVIR-CD \cite{chen2020spatial} and WHU-CD \cite{ji2018fully}\footnote{
    Both the adopted datasets have been obtained from \url{https://github.com/wgcban/SemiCD} in an already pre-processed version.
}. 
Notice that the task defined by these datasets is particularly close to the faced industrial one, that is the driver of our research work.
In these two datasets the model has to track some specific patters, those corresponding to buildings, and carefully segments the eventually occurred changes.

LEVIR-CD contains 637 pairs of high resolution aerial images. 
Starting from these images, patch pairs of size  each have been extracted.
After that, the pair instances have been partitioned accordingly to the authors' original indications.
This step produced 7120, 1024, and 2048 pair instances for the train, validation, and test dataset respectively.

WHU-CD contains just one pair of images having resolution  as a crop of a wider geographic area\footnote{
    The whole dataset depicts the city of Christchurch, in New Zealand. 
    The crop, aimed to be used in CD tasks, is a sub-area acquired in two different times.
}.
Following \cite{bandara2022revisiting}, the images have been split in non overlapping patches with resolution .
After that, a randomly partitioning of the dataset have been performed obtaining 5947, 743, and 744 pairs for train, validation, and test respectively.

\subsection{Loss function and evaluation metrics}

As stated in \secref{sec:classifier}, we cast the CD problem in a pixel-wise binary classification setting.
In fact, the role of the final MLP block is to output the per-pixel change probability.

Since the reference mask is a binary mask (0 for unchanged pixels, 1 for changed pixels), and since we are comparing 
probabilities, one loss function that can be used is the Binary Cross Entropy (BCE). It is defined as:


where we denoted with  the ground truth mask, with  the model prediction, and with  and  the set of indices relative to height and width.

Notice that the BCE loss function is widely used in other SOTA models such as \cite{bandara2022transformer,chen2021remote}.
In contrast, other researchers implemented more sophisticated loss functions like the one presented in \cite{chen2020spatial}.
We decided to use the simpler BCE in order to attribute the improvement in performances to the model and not to an ad hoc built-in loss function.
For completeness, in appendix \ref{appx:A1} we report other experiments conducted using other widely adopted loss functions.

To evaluate the performances achieved by our model, we calculated the
\emph{Precision (PR)}, \emph{Recall (RC)}, \emph{F1 score (F1)}, \emph{Intersection over Union (IoU)}
and \emph{Overall Accuracy (OA)} with respect to the change class, as defined below:

where , , ,  are computed on the change class, and represent the true positives, true negatives, false positives, and false negatives respectively.
To retrieve the change mask we applied a  threshold to the output mask.

\subsection{Implementation details}

We implemented our model using PyTorch~\cite{paszke2019pytorch} and we trained it on an NVIDIA GeForce RTX 2060 6GB GPU.
As described in \secref{sec:backbone}, we selected the first four blocks of the EfficientNet version  backbone pretrained on the ImageNet dataset.
All other weights of the model have been initialized randomly\footnote{
    To make our results reproducible, we fixed the random seed at the beginning of each experiment.
}.

As optimizer, we adopted AdamW \cite{loshchilov2017decoupled}.
To optimize its hyperparameters, i.e. learning rate, weight decay and \emph{amsgrad} variant, 
and also to verify the robustness of our model with respect to the choice of these parameters,
we firstly run a Hyper-Parameters Optimization (HPO) task for each dataset using the package Neural Network Intelligence (NNI) \cite{nni2021}.
After this, we fixed the learning rate equal to , and the weight decay equal to , for the LEVIR-CD dataset.
Moreover, we fixed the learning rate equal to , and the weight decay equal to , for the WHU-CD dataset.
For both datasets, \emph{amsgrad} have been set to \emph{False}.
An example of the HPO procedure is reported in Appendix \ref{appx:A1}.
Due to computational resource limitations, no other hyperparameters have been tuned.
We have not experimented network architecture search techniques (NAS).

To dynamically adjust the learning rate during the training, 
we adopted the cosine annealing strategy as described in \cite{loshchilov2016sgdr},
but avoiding the warm restart.

Since aerial images are spatially registered, we applied the geometric data augmentation operators 
simultaneously to the reference/comparison images and their associated ground-truth mask.
Also, non-geometric augmentations are applied independently on the reference and the comparison images.

The applied geometric augmentations are Random Flip on both X and Y axes, and Random Rotation with free degree.
Moreover, the applied non-geometric augmentations are Gaussian Blur and Random Brightness/Contrast change.
To achieve all the adopted augmentations, we used the Albumentations library \cite{buslaev2020albumentations}.

Finally, due to the limited GPU memory capacity and computational power, we fixed the batch size to 8, and trained for just 100 epochs.


\subsection{Comparison with SOTA models}

To demonstrate the effectiveness of our approach, we compared our results with those reported in \cite{chen2021remote,bandara2022transformer}. 
As baseline, we used the three models presented in \cite{daudt2018fully}. 
Moreover, to compare our model with other works adopting both spatial and channel attention mechanisms,
we dealt with \cite{liu2020building,chen2020spatial,zhang2020deeply,fang2019dual}. 
Finally, given the success achieved by Transformers applied to the computer vision field, 
we also compared our results with those obtained in \cite{chen2021remote,bandara2022transformer}. 

The results reported in \tabref{tab:metrics-levir} and \tabref{tab:metrics-whu} show the superior performance of our model 
on the LEVIR-CD and WHU-CD building change detection datasets. 

\begin{table}[ht]
    \caption{
        Performance metrics on the LEVIR-CD dataset.
        To improve results readability, we adopted a color ranking convention to represent the \green{First}, \red{Second}, and \blue{Third} results respectively.
        The metrics are reported in percentage.
    }
    \centering
\begin{tabular}{l|ccccc}
        \multicolumn{6}{c}{LEVIR-CD} \\
        \hline 
        Model & Pr & Rc & F1 & IoU & OA \\
        \hline 
        FC-EF \cite{daudt2018fully}                 & 86.91 & 80.17 & 83.40 & 71.53 & 98.39 \\
        FC-Siam-diff \cite{daudt2018fully}          & 89.53 & 83.31 & 86.31 & 75.92 & 98.67 \\
        FC-Siam-conc \cite{daudt2018fully}          & 91.99 & 76.77 & 83.69 & 71.96 & 98.49 \\
        DTCDSCN \cite{liu2020building}              & 88.53 & 86.83 & 87.67 & 78.05 & 98.77 \\
        STANet \cite{chen2020spatial}               & 83.81 & \green{91.00} & 87.26 & 77.40 & 98.66 \\
        IFNet \cite{zhang2020deeply}                & \green{94.02} & 82.93 & 88.13 & 78.77 & 98.87 \\
        SNUNet \cite{fang2019dual}                  & 89.18 & 87.17 & 88.16 & 78.83 & 98.82 \\
        BIT \cite{chen2021remote}                   & 89.24 & \blue{89.37}  & \blue{89.31} & \blue{80.68} & \blue{98.92} \\
        Changeformer \cite{bandara2022transformer}  & \blue{92.05} & 88.80  & \red{90.40} & \red{82.48} & \red{99.04} \\
        \hline
        Ours & \red{92.68} & \red{89.47} & \green{91.05} & \green{83.57} & \green{99.10} \\
        \hline
    \end{tabular}
\label{tab:metrics-levir}
\end{table}

\begin{table}[ht]
    \caption{
        Performance metrics on the WHU-CD dataset. 
        To improve results readability, we adopted a color ranking convention to represent the \green{First}, \red{Second}, and \blue{Third} results respectively.
        The metrics are reported in percentage.
    }
    \centering
\begin{tabular}{l|ccccc}
        \multicolumn{6}{c}{WHU-CD} \\
        \hline
        Model & Pr & Rc & F1 & IoU & OA \\
        \hline
        FC-EF \cite{daudt2018fully}         & 71.63 & 67.25 & 69.37 & 53.11 & 97.61 \\
        FC-Siam-diff \cite{daudt2018fully}  & 47.33 & 77.66 & 58.81 & 41.66 & 95.63 \\
        FC-Siam-conc \cite{daudt2018fully}  & 60.88 & 73.58 & 66.63 & 49.95 & 97.04 \\
        DTCDSCN \cite{liu2020building}      & 63.92 & \blue{82.30}  & 71.95 & 56.19 & 97.42 \\
        STANet \cite{chen2020spatial}       & 79.37 & \red{85.50 }  & 82.32 & 69.95 & 98.52 \\
        IFNet \cite{zhang2020deeply}        & \green{96.91} & 73.19 & 83.40 & 71.52 & \red{98.83} \\
        SNUNet \cite{fang2019dual}          & 85.60 & 81.49 & \blue{83.50} & \blue{71.67} & 98.71 \\
        BIT \cite{chen2021remote}           & \blue{86.64} & 81.48 & \red{83.98} & \red{72.39 } & \blue{98.75} \\
        \hline
        Ours & \red{91.72} & \green{91.76} & \green{91.74} & \green{84.74} & \green{99.34} \\
        \hline
    \end{tabular}
\label{tab:metrics-whu}
\end{table}

The baseline models FC-Siam-diff and FC-Siam-conc \cite{daudt2018fully} are the architectures most similar to ours. 
With respect to these two baseline models, we increased the F1 score by 4.73 points on LEVIR-CD, and by more than 20 points on the WHU-CD.
With respect to the best model we found in the literature \cite{bandara2022transformer}, our performance increment on the LEVIR-CD dataset is more limited.
However, as we can see from \tabref{tab:parameters}, our model is 146.50 times smaller.

\begin{table}[ht]
    \caption{
        Parameters, complexity, and performance comparison. 
        The metrics are reported in percentage, parameters in Millions (M), and complexity in GFLOPs (G).
    }
    \centering
\begin{tabular}{l|c|c|c|c|c}
        \hline
        Model & Param (M) & \thead{Param\\ ratio} & FLOPs (G) &\thead{LEVIR-CD\\F1} &\thead{WHU-CD\\F1} \\
        \hline
        DTCDSCN \cite{liu2020building}& 41.07 & 146.67 & 7.21 &  87.67 & 71.95 \\
        STANet \cite{chen2020spatial}& 16.93 & 60.46 & 6.58 &  87.26 &  82.32  \\
        IFNet \cite{zhang2020deeply}& 50.71 & 181.10 & 41.18 &  88.13 &  83.40 \\
        SNUNet \cite{fang2019dual}& 12.03 & 42.96 & 27.44 &  88.16 &  83.50  \\
        BIT \cite{chen2021remote}& 3.55 & 12.67 & 4.35 &  89.31 &  83.98 \\
        Changeformer \cite{bandara2022transformer}& 41.02 & 146.50 & N.D. &  90.40 & N.D. \\
        \hline
        Ours & 0.28 & 1 & 1.45 & 91.05 & 91.74 \\
        \hline
    \end{tabular}
\label{tab:parameters}
    \end{table}

In view of these results, we can conclude that our model, despite the lower complexity and the lower number of employed parameters, 
is very effective on the buildings CD task.
Moreover, having not used any global attention mechanism, we have a confirmation of our intuitions: 
in the faced CD task, low level information is sufficient to reach high-quality results. 
Also, the information contained in each single pixel at different resolutions, is very rich and can be exploited to effectively classify changes.

In \figref{fig:bitvsours} a visual/qualitative comparison between the masks created by our model, and those created by BIT 
\cite{chen2021remote} on the LEVIR-CD test dataset, is reported.
Generally speaking, both models perform well and we end up our analysis by conjecturing that the performance difference reported in 
\tabref{tab:metrics-levir} and \tabref{tab:metrics-whu} are more related to missing or hallucinated change regions, than region quality issues.
Nevertheless, we can find some examples were there are significant differences between the ground truth masks (GT) and those created by the two models.
In \figref{fig:bitvsours} it is interesting to note that there are examples where both models fail similarly in the same regions,
despite the two models being based on very different approaches (local versus global).

\ffigure{fig:bitvsours}{maschere}{
    Visual comparison between outputs obtained by our model and BIT. 
    We highlighted with red bounding boxes those regions containing significant differences
    between the ground-truth and the generated masks.
}{10cm}{12cm}


\subsection{Ablation study}
\label{ch:ablation}

In this section we describe the adopted ablation study steps and the achieved results.

\subsubsection{Backbone dimension and final PW-MLP}

The first ablation study we conducted concerns the size of the backbone and the use of the final MLP. 
Regarding the backbone size, we considered both the whole EfficientNet-b4 except the final classifier, 
and a sliced version of the EfficientNet-b4 network including just the first 3 blocks.
Moreover, to assess the effectiveness of the final classification PW-MLP block, 
we considered both the architecture including it, 
and the one that produces its output directly from the last up-sampling block by forcing it output just one channel\footnote{
    We employed the sigmoid activation on this output.
}.
The results shown in \tabref{tab:backbonemlp} confirm our intuition on low-level features.
In fact, our solution with the sliced backbone and final PW-MLP, turns out to be the one with the best performances on both datasets.
Furthermore, we note that, to get the best performances, the backbone slicing and PW-MLP classifier must be coupled.
In fact, on LEVIR-CD the backbone slicing only model shows poor performances, 
while the use of the PW-MLP classifier helps the full backbone architecture to improve the quality of the segmentations.
In contrast, on the WHU-CD the architecture with sliced backbone and the PW-MLP classifier obtains better scores 
than the one with full backbone but without PW-MLP, remaining the performances of the latter still unsatisfactory and far from those obtained by our model.

\begin{table}[!ht]
    \caption{
        Performance comparison between versions of our model including and excluding the backbone slicing and the PW-MLP classifier.
    }
    \centering
    \begin{tabular}{l|ccccc|c}
        \hline
        \multicolumn{6}{c}{LEVIR-CD}\\
    \hline
        Model & Precision & Recall & F1 score & IoU & Accuracy & Params \\ 
        \hline
        Full w/o MLP & 83.05 & 94.00 & 88.19 & 78.88 & 98.71 & 17740598 \\ 
        Full w MLP & 92.65 & 89.26 & 90.92 & 83.36 & 99.09 & 17743288 \\ 
        Sliced w/o MLP & 46.15 & 94.52 & 62.02 & 44.95 & 94.10 & 282438 \\ 
        Sliced w MLP & 92.68 & 89.47 & 91.05 & 83.57 & 99.10 & 285128 \\ 
    \hline
        \multicolumn{6}{c}{WHU-CD}\\
        \hline
        Model & Precision & Recall & F1 score & IoU & Accuracy & Params \\ 
        \hline
        Full w/o MLP & 43.08 & 88.12 & 57.87 & 40.72 & 94.91 & 17740598 \\ 
        Full w MLP & 91.00 & 92.14 & 91.57 & 84.45 & 99.32 & 17743288 \\ 
        Sliced w/o MLP & 76.16 & 89.05 & 82.10 & 69.64 & 98.84 & 282438 \\ 
        Sliced w MLP & 91.72 & 91.76 & 91.74 & 84.74 & 99.34 & 285128 \\ 
        \hline
    \end{tabular}
    \label{tab:backbonemlp}
\end{table}


\subsubsection{Impact of skip connection with MAMB}

To quantitatively confirm the usefulness of the skip connections, we trained a model without them 
and compared the achieved results in \tabref{tab:skipnoskip}. 

\begin{table}[ht]
    \caption{Performance comparison between the model with/without skip connections on both datasets LEVIR-CD and WHU-CD.}
    \centering
    \begin{tabular}{l|ccccc}
        \hline
        \multicolumn{6}{c}{LEVIR-CD}\\
        \hline 
        Model type & Pr & Rc & F1 & IoU & OA \\
        \hline
        No Skip & 92.35 & 88.50 & 90.38 & 82.45 & 99.04\\
        Skip & 92.68 & 89.47 & 91.05 & 83.57 & 99.10 \\
        \hline
        \multicolumn{6}{c}{WHU-CD}\\
        \hline
        Model type & Pr & Rc & F1 & IoU & OA \\
        \hline
        No Skip & 90.56 & 89.77 & 90.16 & 82.09 & 99.22\\
        Skip & 91.72 & 91.76 & 91.74 & 84.74 & 99.34 \\
        \hline
    \end{tabular}
    \label{tab:skipnoskip}
\end{table}

As can be seen, all the metrics confirm the beneficial effects of skip connections in the model.
In \figref{fig:maskvis} we reported an example of the intermediate masks that our model creates in the skip-connections.

\ffigure{fig:maskvis}{multi_mask}{
    Visualization of the intermediate masks at different resolutions and the final binary mask for one example image pair.
}{10cm}{12cm}

As can be seen, the masks created with the MAMB block at resolution 64 highlight the objects that musk be tracked (red pixels).
The intermediate masks at resolution 128 act more like an edge detector.
Finally, the masks at resolution 256, obtained applying the MAMB block directly to the original images  and , 
distinguish between object classes like buildings and street (dark blue), vegetation (light green), and shadows (red). 
The ability to highlight shadows is very effective since it helps the model to detect objects and to refine their edges.


\subsubsection{Comparison with other simple mixing strategy}

In \tabref{tab:groupedvsconv} we compare our mixing strategy, described in \secref{sec:mixing}, 
with other widely used feature fusion blocks. We tested the following alternatives:
\begin{itemize}
    \item subtraction, both in the bottleneck and in skip connections;
    \item concatenation + convolution, both in the bottleneck and in skip connections.
\end{itemize}
We selected these two alternatives since our mixing strategy can be seen as a generalization of the pixel-wise subtraction\footnote{
    In fact, if we initialize all of our 2-depth kernels with the "central" weights to  and , 
    and all the rest to , we have the standard subtraction.
}.
However, our mixing block \secref{sec:mixing} is fully trainable with the spirit of feature re-use \cite{bengio2013representation}.
Moreover, concatenation + convolution can be seen as generalization of our mixing block. 
However, the number of trainable parameters to be tuned for this mixing block is much bigger than ours.
More precisely, the number of parameters in our mixing block is ,
\footnote{
    The parentheses are highlighting the size of each kernel and the number of kernels.
}
where  is the number of channels, ,  are the convolutional kernel sizes.
By comparison, a convolution working on the concatenated feature tensors contains  parameters.
 
\begin{table}[!ht]
    \caption{
        Performance comparison between the model with our mixing strategy, subtraction, and concatenation + convolution (C+C) respectively.
    }
    \centering
    \begin{tabular}{l|ccccc|cc}
    \hline 
    \multicolumn{8}{c}{LEVIR-CD} \\
    \hline
        Model type & Pr & Rc & F1 & IoU & OA & Param. tot. & GFLOPs  \\
        \hline 
        Subtraction & 92.13 & 89.41 & 90.75 & 83.07 & 99.07 & 282939 & 1.43 () \\ 
        C+C & 92.55 & 89.61 & 91.06 & 83.59 & 99.10 & 368468 & 1.75 () \\
        \hline
        Our & 92.68 & 89.47 & 91.05 & 83.57 & 99.10 & 285128 & 1.45 \\
        \hline
        \multicolumn{8}{c}{WHU-CD} \\
        \hline
        Model type & Pr & Rc & F1 & IoU & OA & Param. tot. & GFLOPs  \\
        \hline
        Subtraction & 90.10 & 91.55 & 90.82 & 83.19 & 99.26 & 282939 & 1.43 () \\
        C+C & 92.19 & 91.25 & 91.72 & 84.71 & 99.34 & 368468 & 1.75 () \\
        \hline
        Our & 91.72 & 91.76 & 91.74 & 84.74 & 99.34 & 285128 & 1.45 \\
        \hline
    \end{tabular}
    \label{tab:groupedvsconv}
\end{table}

In \tabref{tab:gridsearchmix} we reported the results of a more detailed study on mixing strategies.
We alternated the use of subtraction/concatenation + convolution with our respective proposal 
to mix the features in the bottleneck/skip connections.

\begin{table}[!htb]
    \caption{
        Evaluation of subtraction and concatenation + convolution mixing strategies.
        We reported F1 score for the two datasets LEVIR-CD (F1-L) and WHU-CD (F1-W).
        We used \cmark to indicate where we changed our proposed option with subtraction or concatenation + convolution.
        In contrast, \xmark represents our bottleneck mixing block or MAMB.
    }
    \label{tab:gridsearchmix}
    \begin{subtable}{.5\linewidth}
      \centering
        \caption{Subtraction}
        \begin{tabular}{c|c|c|c|c}
        Mix & Skip &  F1-L & F1-W & Param\\
        \hline
            \cmark & \cmark &  90.75 & 90.82 & 282939 \\
            \xmark & \cmark &  90.75 & 91.51 & 284004 \\  
            \cmark & \xmark &  90.71 & 89.58 & 284063 \\
            \hline
            \xmark & \xmark &  91.05 & 91.74 & 285128 \\
            \hline
        \end{tabular}
    \end{subtable}\begin{subtable}{.5\linewidth}
      \centering
        \caption{Concatenation+Convolution}
        \begin{tabular}{c|c|c|c|c}
        Mix & Skip &  F1-L & F1-W & Param \\
        \hline
            \cmark & \cmark &  91.06 & 91.72 & 368468 \\ 
            \xmark & \cmark &  91.06 & 91.08 & 313028 \\ 
            \cmark & \xmark &  90.90 & 91.71 & 340568 \\
            \hline
            \xmark & \xmark &  91.05 & 91.74 & 285128 \\
            \hline
        \end{tabular}
    \end{subtable} 
\end{table}

The obtained results confirm that our proposal can be considered an effective generalization of the subtraction, 
with little impact on the size and complexity of the model.
On the other hand, the overhead introduced by the concatenation + convolution mixing strategy,
seems to produce little differences in terms of performance.


\subsubsection{Channel-wise MLP vs CycleMLP}

As reported in \secref{chp:relationexisting}, several MLP blocks have recently been studied with the intent of 
incorporating both spatial and channel-specific information.
As previously described, we used the MLPs only along the channels in the final classifier, 
and coupled to our mixing strategy in the MAMB blocks to obtain space-time correlation.
We then decided to deal with the CycleMLP block proposed in \cite{chen2021cyclemlp}.
The results reported in \tabref{tab:mlpvscyclemlp} suggest the superiority of our proposed
use of MLPs compared to that proposed in \cite{chen2021cyclemlp}.
A heuristic explanation for these results can be the following:
the MLP blocks proposed in \cite{chen2021cyclemlp} have shown to obtain excellent performances 
when they are used to construct a hierarchical architecture to generate pyramid features.
This makes us think that the advantage of CycleMLPs may be more significant 
when the features are more refined than the low-level features we use.

\begin{table}[!ht]
    \caption{
        Performance comparison between MLP and CycleMLP \cite{chen2021cyclemlp} on LEVIR-CD and WHU-CD.
        We used \cmark to indicate experiments where we changed our proposed block with a CycleMLP one,
        while \xmark represents our proposed architecture.
    }
    \centering
    \begin{tabular}{c|c|ccccc|c}
    \hline 
    \multicolumn{7}{c}{LEVIR-CD} \\
    \hline
        Skip & Class. & Pr & Rc & F1 & IoU & OA & Param. tot. \\
        \hline 
        \cmark & \xmark & 92.47 & 88.48 & 90.43 & 82.53 & 99.04 & 309300 \\ 
        \cmark & \cmark & 92.45 & 88.49 & 90.42 & 82.52 & 99.04 & 314542 \\ 
        \xmark & \cmark & 92.58 & 88.96 & 90.73 & 83.04 & 99.07 & 290370 \\ 
        \hline
        \xmark & \xmark & 92.68 & 89.47 & 91.05 & 83.57 & 99.10 & 285128 \\ 
        \hline
        \multicolumn{7}{c}{WHU-CD} \\
        \hline
        Skip & Class. & Pr & Rc & F1 & IoU & OA & Param. tot. \\
        \hline
        \cmark & \xmark & 89.76 & 89.06 & 89.41 & 80.85 & 99.16 & 309300 \\ 
        \cmark & \cmark & 92.25 & 90.51 & 91.37 & 84.12 & 99.32 & 314542 \\ 
        \xmark & \cmark & 90.20 & 85.84 & 87.96 & 78.52 & 99.06 & 290370 \\ 
        \hline
        \xmark & \xmark & 91.72 & 91.76 & 91.74 & 84.74 & 99.34 & 285128 \\ 
        \hline
    \end{tabular}
    \label{tab:mlpvscyclemlp}
\end{table}
