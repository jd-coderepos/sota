\section{Results}\label{sec:e}
\subsection{Full ImageNet Classification}
In this section we describe our experiments and compare our approach with other strong baseline methods.  As stated in \S\ref{sec:i}, there are three aspects to consider in evaluating a method: (1) its performance on the few-shot set $\mathcal{D}_{\text{few}}$, (2) its performance on the large-scale set $\mathcal{D}_{\text{large}}$, and (3) its computation overhead of adding novel categories and the complexity of image inference. In the following paragraphs, we will cover the settings of the baseline methods, compare the performances on the large-scale and the few-shot sets, and discuss their efficiencies.

\paragraph{Baseline Methods}
The baseline methods must be applicable to both large-scale and few-shot learning settings. We compare our method with a fine-tuned $50$-layer ResNet~\cite{DBLP:conf/cvpr/HeZRS16}, Learning like a Child~\cite{mao2015learning} with a pre-trained $50$-layer ResNet as the starting network, Siamese-Triplet Network~\cite{siamese,lin2017transfer} using three $50$-layer ResNets with shared parameters, and the nearest neighbor using the pre-trained $50$-layer ResNet convolutional features. We will elaborate individually on how to train and use them.

\begin{table*}
    \small
    \centering
    \setlength{\tabcolsep}{1.0em}
    \begin{tabular}{lrccll:rr}
        \toprule
        Method & $\mathcal{D}_{\text{large}}$ & $\mathcal{D}_{\text{few}}$ & FT & Top-1 $\mathcal{C}_{\text{large}}$ & Top-5 $\mathcal{C}_{\text{large}}$ & Top-1 $\mathcal{C}_{\text{few}}$ & Top-5 $\mathcal{C}_{\text{few}}$\\
        \midrule
        NN + Cosine  & 100\% & 1 & N & 71.54\% & 91.20\% & 1.72\% & 5.86\% \\
        NN + Cosine  &  10\% & 1 & N & 67.68\% & 88.90\% & 4.42\% & 13.36\% \\
        NN + Cosine  &   1\% & 1 & N & 61.11\% & 85.11\% & 10.42\% & 25.88\% \\
        Triplet Network~\cite{siamese,lin2017transfer}  & 100\% & 1 & N & 70.47\% & 90.61\% & 1.26\% & 4.94\% \\
        Triplet Network~\cite{siamese,lin2017transfer}   &  10\% & 1 & N & 66.64\% & 88.42\% & 3.48\% & 11.40\% \\
        Triplet Network~\cite{siamese,lin2017transfer}   &   1\% & 1 & N & 60.09\% & 84.83\% & 8.84\% & 22.24\% \\
        Fine-Tuned ResNet~\cite{DBLP:conf/cvpr/HeZRS16} & 100\% & 1 & Y & 76.28\% & 93.17\% & 2.82\% & 13.30\% \\
        Learning like a Child~\cite{mao2015learning} & 100\% & 1 & Y & 76.71\% & 93.24\% & 2.90\% & 17.14\% \\
        \hdashline
        Ours-$\phi^1$         & 100\% & 1 & N & 72.56\% & 91.12\% & {\bf\color{blue}19.88\%} & {\bf\color{blue}43.20\%} \\
        Ours-$\phi^2$         & 100\% & 1 & N & 74.17\% & 91.79\% & {\bf\color{red}21.58\%} & {\bf\color{red}45.82\%} \\
        Ours-$\phi^{2*}$      & 100\% & 1 & N & 75.63\% & 92.92\% & 14.32\% & 33.84\% \\
        \midrule
        NN + Cosine  & 100\% & 2 & N & 71.54\% & 91.20\% & 3.34\% & 9.88\% \\
        NN + Cosine  &  10\% & 2 & N & 67.66\% & 88.89\% & 7.60\% & 19.94\% \\
        NN + Cosine  &   1\% & 2 & N & 61.04\% & 85.04\% & 15.14\% & 35.70\% \\
        Triplet Network~\cite{siamese,lin2017transfer}  & 100\% & 2 & N & 70.47\% & 90.61\% & 2.34\% & 8.30\% \\
        Triplet Network~\cite{siamese,lin2017transfer}  &  10\% & 2 & N & 66.63\% & 88.41\% & 6.10\% & 17.46\% \\
        Triplet Network~\cite{siamese,lin2017transfer}  &   1\% & 2 & N & 60.02\% & 84.74\% & 13.42\% & 32.38\% \\
        Fine-Tuned ResNet~\cite{DBLP:conf/cvpr/HeZRS16} & 100\% & 2 & Y & 76.27\% & 93.13\% & 10.32\% & 30.34\% \\
        Learning like a Child~\cite{mao2015learning} & 100\% & 2 & Y & 76.68\% & 93.17\% & 11.54\% & 37.68\% \\
        \hdashline
        Ours-$\phi^1$         & 100\% & 2 & N & 71.94\% & 90.62\% & {\bf\color{blue}25.54\%} & {\bf\color{blue}52.98\%} \\
        Ours-$\phi^2$         & 100\% & 2 & N & 73.43\% & 91.13\% & {\bf\color{red}27.44\%} & {\bf\color{red}55.86\%} \\
        Ours-$\phi^{2*}$      & 100\% & 2 & N & 75.44\% & 92.74\% & 18.70\% & 43.92\% \\
        \midrule
        NN + Cosine  & 100\% & 3 & N & 71.54\% & 91.20\% & 4.58\% & 12.72\% \\
        NN + Cosine  &  10\% & 3 & N & 67.65\% & 88.88\% & 9.86\% & 24.96\% \\
        NN + Cosine  &   1\% & 3 & N & 60.97\% & 84.95\% & 18.68\% & 42.16\% \\
        Triplet Network~\cite{siamese,lin2017transfer}  & 100\% & 3 & N & 70.47\% & 90.61\% & 3.22\% & 11.48\% \\
        Triplet Network~\cite{siamese,lin2017transfer}  &  10\% & 3 & N & 66.62\% & 88.40\% & 8.52\% & 22.52\% \\
        Triplet Network~\cite{siamese,lin2017transfer}  &   1\% & 3 & N & 59.97\% & 84.66\% & 17.08\% & 38.06\% \\
        Fine-Tuned ResNet~\cite{DBLP:conf/cvpr/HeZRS16} & 100\% & 3 & Y & 76.25\% & 93.07\% & 16.76\% & 39.92\% \\
        Learning like a Child~\cite{mao2015learning} & 100\% & 3 & Y & 76.55\% & 93.00\% & 18.56\% & 50.70\% \\
        \hdashline
        Ours-$\phi^1$         & 100\% & 3 & N & 71.56\% & 90.21\% & {\bf\color{blue}28.72\%} & {\bf\color{blue}58.50\%} \\
        Ours-$\phi^2$         & 100\% & 3 & N & 72.98\% & 90.59\% & {\bf\color{red}31.20\%} & {\bf\color{red}61.44\%} \\
        Ours-$\phi^{2*}$      & 100\% & 3 & N & 75.34\% & 92.60\% & 22.32\% & 49.76\% \\
        \bottomrule
    \end{tabular}
    \caption{Comparing 1000-way accuracies with feature extractor $\mathbf{a}(\cdot)$ pre-trained on $\mathcal{D}_{\text{large}}$. For different $\mathcal{D}_{\text{few}}$ settings, red: the best few-shot accuracy, and blue: the second best.}
    \label{tab:real1}
\end{table*}



As mentioned in \S\ref{sec:id}, we first train a $900$-category classifier on $\mathcal{D}_{\text{large}}$.
We will build other baseline methods using this classifier as the staring point. For convenience, we denote this classifier as \smash{$\mathcal{R}_\text{large}^{\text{pt}}$}, where pt stands for ``pre-trained''.
Next, we add the novel categories $\mathcal{C}_{\text{few}}$ to each method. For the $50$-layer ResNet, we fine tune \smash{$\mathcal{R}_\text{large}^{\text{pt}}$} with the newly added images by extending the fully connected layer to generate $1000$ classification outputs. Note that we will limit the number of training samples of $\mathcal{C}_{\text{few}}$ for the few-shot setting.
For Learning like a Child, however, we fix the layers before the global average pooling layer, extend the fully connected layer to include $1000$ classes, and only update the parameters for $\mathcal{C}_{\text{few}}$ in the last classification layer.
Since we have the full access to $\mathcal{D}_{\text{large}}$, we do not need Baseline Probability
Fixation~\cite{mao2015learning}. The nearest neighbor with cosine distance can be directly used for both tasks given the pre-trained deep features.

The other method we compare is Siamese-Triplet Network ~\cite{siamese,lin2017transfer}. Siamese network is proposed to approach the few-shot learning problem on Omniglot dataset~\cite{DBLP:conf/cogsci/LakeSGT11}.
In our experiments, we find that its variant Triplet Network~\cite{lin2017transfer,DBLP:conf/iccv/WangG15} is more effective since it learns feature representation from relative distances between positive and negative pairs instead of directly doing binary classification from the feature distance. Therefore, we use the Triplet Network from~\cite{lin2017transfer} on the few-shot learning problem, and upgrade its body net to the pre-trained \smash{$\mathcal{R}_\text{large}^{\text{pt}}$}. We use cosine distance as the distance metric and fine-tune the Triplet Network. For inference, we use nearest neighbor with cosine distance.
We use some techniques to improve the speed, which will be discussed later in the efficiency analysis.

\paragraph{Few-Shot Accuracies}
We first investigate the few-shot learning setting where we only have several training examples for $\mathcal{C}_{\text{few}}$. Specifically, we study the performances of different methods when $\mathcal{D}_{\text{few}}$ has for each category 1, 2, and 3 samples.
It is worth noting that our task is much harder than the previously studied few-shot learning: we are evaluating the top predictions out of $1000$ candidate categories, \textit{i.e.}, $1000$-way accuracies while previous work is mostly interested in $5$-way or $20$-way accuracies~\cite{pmlr-v70-finn17a,siamese,lin2017transfer,ravi2017optimization,DBLP:conf/nips/VinyalsBLKW16}.

With the pre-trained \smash{$\mathcal{R}_\text{large}^{\text{pt}}$}, the training samples in $\mathcal{D}_{\text{few}}$ are like invaders to the activation space for  $\mathcal{C}_{\text{large}}$.
Intuitively, there will be a trade-off between the performances on $\mathcal{C}_{\text{large}}$ and $\mathcal{C}_{\text{few}}$. This is true especially for non-parametric methods. Table~\ref{tab:real1} shows the performances on the validation set of ILSVRC 2015~\cite{ILSVRC15}. The second column is the percentage of data of $\mathcal{D}_{\text{large}}$ in use, and the third column is the number of samples used for each category in $\mathcal{D}_{\text{few}}$.
Note that fine-tuned ResNet~\cite{DBLP:conf/cvpr/HeZRS16} and Learning like a Child~\cite{mao2015learning} require fine-tuning while others do not.

Triplet Network is designed to do few-shot image inference by learning feature representations that adapt to the chosen distance metric. It has better performance on $\mathcal{C}_{\text{few}}$ compared with the fine-tuned ResNet and Learning like a Child when the percentage of $\mathcal{D}_{\text{large}}$ in use is low. However, its accuracies on $\mathcal{C}_{\text{large}}$ are sacrificed a lot in order to favor few-shot accuracies. We also note that if full category  supervision is provided, the activations of training a classifier do better than that of training a Triplet Network. We speculate that this is due to the less supervision of training a Triplet Network which uses losses based on fixed distance preferences.
Fine-tuning and Learning like a Child are training based, thus are able to keep the high accuracies on $\mathcal{D}_{\text{large}}$, but perform badly on $\mathcal{D}_{\text{few}}$ which does not have sufficient data for training.
Compared with them, our method shows state-of-the-art accuracies on $\mathcal{C}_{\text{few}}$ without compromising too much the performances on $\mathcal{C}_{\text{large}}$.

Table~\ref{tab:real1} also compares $\phi^2$ and $\phi^{2*}$, which are trained to minimize Eq.~\ref{eq:mixed} and Eq.~\ref{eq:1}, respectively. Since during training $\phi^{2*}$ only mean activations are sampled, it shows a bias towards $\mathcal{C}_{\text{large}}$. However, it still outperforms other baseline methods on $\mathcal{C}_{\text{few}}$.
In short, modeling using Eq.~\ref{eq:mixed} and Eq.~\ref{eq:1} shows a tradeoff between $\mathcal{C}_{\text{large}}$ and $\mathcal{C}_{\text{few}}$.



\paragraph{Oracles}
Here we explore the upper bound performance on $\mathcal{C}_{\text{few}}$.
In this setting we have all the training data for $\mathcal{C}_{\text{large}}$ and $\mathcal{C}_{\text{few}}$ in ImageNet.
For the fixed feature extractor $\mathbf{a}(\cdot)$ pre-trained on $\mathcal{D}_{\text{large}}$, we can train a linear classifier on $\mathcal{C}_{\text{large}}$ and $\mathcal{C}_{\text{few}}$, or use nearest neighbor, to see what are the upper bounds of the pre-trained $\mathbf{a}(\cdot)$. Table~\ref{tab:oracle} shows the results.
The performances are evaluated on the validation set of ILSVRC 2015~\cite{ILSVRC15} which has $50$ images for each category. The feature extractor pre-trained on $\mathcal{D}_{\text{large}}$ demonstrates reasonable accuracies on $\mathcal{C}_{\text{few}}$ which it has never seen during training for both parametric and non-parametric methods.

\begin{table}
    \small
    \centering
    \vspace{0.05in}
    \setlength{\tabcolsep}{0.5em}
    \begin{tabular}{lcccc}
        \toprule
        Classifier & Top-1 $\mathcal{C}_{\text{large}}$ & Top-5 $\mathcal{C}_{\text{large}}$ & Top-1 $\mathcal{C}_{\text{few}}$ & Top-5 $\mathcal{C}_{\text{few}}$\\
        \midrule
        NN & 70.25\% & 89.98\% & 52.46\% & 80.94 \\
        Linear & 75.20\% & 92.38\% & 60.50\% & 87.58 \\
        \bottomrule
    \end{tabular}
    \caption{Oracle 1000-way accuracies of the feature extractor $\mathbf{a}(\cdot)$ pre-trained on $\mathcal{D}_{\text{large}}$.}
    \label{tab:oracle}
\end{table}

\paragraph{Efficiency Analysis}
We briefly discuss the efficiencies of each method including ours on the adaptation to novel categories and the image inference.
The methods are tested on NVIDIA Tesla K40M GPUs.
For adapting to novel categories, fine-tuned ResNet and Learning like a Child require re-training the neural networks.
For re-training one epoch of the data, fine-tuned ResNet and Learning like a Child both take about 1.8 hours on 4 GPUs.
Our method only needs to predict the parameters for the novel categories using $\phi$ and add them to the original neural network.
This process takes 0.683s using one GPU for adapting the network to $100$ novel categories with one example each.
Siamese-Triplet Network and nearest neighbor with cosine distance require no operations for adapting to novel categories as they are ready for feature extraction.

For image inference, Siamese-Triplet Network and nearest neighbor are very slow since they will look over the entire dataset. Without any optimization, this can take 2.3 hours per image when we use the entire $D_{\text{large}}$.
To speed up this process in order to do comparison with ours, we first pre-compute all the features.
Then, we use a deep learning framework to accelerate the cosine similarity computation.
At the cost of 45GB memory usage and the time for feature pre-computation, we manage to lower the inference time of them to 37.867ms per image.
Fine-tuned ResNet, Learning like a Child and our method are very fast since at the inference stage, these three methods are just normal deep neural networks.
The inference speed of these methods is about 6.83ms per image on one GPU when the batch size is set to 32.
In a word, compared with other methods, our method is fast and efficient in both the novel category adaptation and the image inference.

\paragraph{Comparing Activation Impacts}\label{sec:cai}
In this subsection we investigate what $\phi^1$ has learned that helps it perform better than the cosine distance, which is a special solution for one-layer $\phi$ by setting $\phi$ to the identity matrix $\mathds{1}$.
\begin{figure}
    \begin{subfigure}{.455\linewidth}
        \hspace{-0.2in}\vspace{0.1in}
        \includegraphics[width=\linewidth]{img/matrix.pdf}
    \end{subfigure}
    \vrule
    \begin{subfigure}{.53\linewidth}
        \hspace{0.05in}
        \includegraphics[width=\linewidth]{img/os.pdf}
    \end{subfigure}
    \caption{Visualization of the upper-left $256\times256$ submatrix of $\phi^1$ in $\log$ scale (left) and top-$k$ similarity between $\phi^1$, $\mathds{1}$ and \smash{$\mathbf{w}_{\text{large}}^{\text{pt}}$} (right). In the right plotting, red and solid lines are similarities between $\phi^1$ and \smash{$\mathbf{w}_{\text{large}}^{\text{pt}}$}, and green and dashed lines are between $\mathds{1}$ and \smash{$\mathbf{w}_{\text{large}}^{\text{pt}}$.}}
    \label{fig:vis}
\end{figure}
We first visualize the matrix $\phi^1_{ij}$ in $\log$ scale as shown in the left image of Figure~\ref{fig:vis}. Due to the space limit, we only show the upper-left $256\times256$ submatrix. Not surprisingly, the values on the diagonal dominates the matrix.
We observe that along the diagonal, the maximum is $0.976$ and the minimum is $0.744$, suggesting that different from $\mathds{1}$, $\phi^1$ does not use each activation channel equally.
We speculate that this is because the pre-trained activation channels have different distributions of magnitudes and different correlations with the classification task.
These factors can be learned by the last fully connected layer of \smash{$\mathcal{R}_{\text{large}}^{\text{pt}}$} with large amounts of data but are assumed equal for every channel in cosine distance.
This motivates us to investigate the impact of each channel of the activation space.

For a fixed activation space, we define the \emph{impact} of its $j$-th channel on mapping $\phi$ by
$I_j(\phi)=\sum_i |\phi_{ij}|$.
Similarly, we define the activation impact $I_j(\cdot)$ on \smash{$\mathbf{w}_{\text{large}}^{\text{pt}}$} which is the parameter matrix of the last fully connected layer of \smash{$\mathcal{R}_{\text{large}}^{\text{pt}}$}. For cosine distance, $I_j(\mathds{1})=1$, $\forall j$. Intuitively, we are evaluating the impact of each channel of $\mathbf{a}$ on the output by adding all the weights connected to it.
For \smash{$\mathbf{w}_{\text{large}}^{\text{pt}}$} which is trained for the classification task using large-amounts of data, if we normalize \smash{$I(\mathbf{w}_{\text{large}}^{\text{pt}})$} to unity,
the mean of \smash{$I(\mathbf{w}_{\text{large}}^{\text{pt}})$} over all channel $j$ is $2.13\text{e-}2$ and the standard deviation is $5.83\text{e-}3$.
\smash{$\mathbf{w}_{\text{large}}^{\text{pt}}$} does not use channels equally, either.

In fact, $\phi^1$ has a high similarity with \smash{$\mathbf{w}_{\text{large}}^{\text{pt}}$}. We show this by comparing the orders of the channels sorted by their impacts.
Let $\text{top-}k(S)$ find the indexes of the top-$k$ elements of $S$. We define the top-$k$ similarity of $I(\phi)$ and \smash{$I(\mathbf{w}_{\text{large}}^{\text{pt}})$} by
\begin{equation}
  \footnotesize
  \text{OS}(\phi, \mathbf{w}_{\text{large}}^{\text{pt}}, k)=\mathbf{card}\left(\text{top-}k( I(\phi))\cap\text{top-}k(I(\mathbf{w}_{\text{large}}^{\text{pt}}))\right) / k
\end{equation}
where $\mathbf{card}$ is the cardinality of the set.
The right image of Figure~\ref{fig:vis} plots the two similarities, from which we observe high similarity between $\phi$ and \smash{$\mathbf{w}_{\text{large}}^{\text{pt}}$} compared to the random order of $\mathds{1}$.
From this point of view, $\phi^1$ outperforms the cosine distance due to its better usage of the activations.

\subsection{MiniImageNet Classification}
In this subsection we compare our method with the previous state-of-the-arts on the MiniImageNet dataset.
Unlike ImageNet classification, the task of MiniImageNet is to find the correct category from $5$ candidates, each of which has $1$ example or $5$ examples for reference. The methods are only evaluated on $D_{\text{few}}$, which has $20$ categories.
For each task, we uniformly sample $5$ categories from $D_{\text{few}}$.
For each of the category, we randomly select one or five images as the references, depending on the settings, then regard the rest images of the $5$ categories as the test images.
For each task, we will have an average accuracy over this $5$ categories.
We repeat the task with different categories and report the mean of the accuracies with the $95\%$ confidence interval.

\begin{table}
    \small
    \centering
    \vspace{0.05in}
    \setlength{\tabcolsep}{0.5em}
    \begin{tabular}{lcc}
        \toprule
        Method & 1-Shot & 5-Shot \\
        \midrule
        Fine-Tuned Baseline & 28.86 $\pm$ 0.54\% & 49.79 $\pm$ 0.79\% \\
        Nearest Neighbor & 41.08 $\pm$ 0.70\% & 51.04 $\pm$ 0.65\% \\
        Matching Network~\cite{DBLP:conf/nips/VinyalsBLKW16} & 43.56 $\pm$ 0.84\% & 55.31 $\pm$ 0.73\% \\
        Meta-Learner LSTM~\cite{ravi2017optimization} & 43.44 $\pm$ 0.77\% & 60.60 $\pm$ 0.71\% \\
        MAML~\cite{pmlr-v70-finn17a} & 48.70 $\pm$ 1.84\% & 63.11 $\pm$ 0.92\% \\
        \hdashline
        Ours-Simple & {\bf\color{blue}54.53} $\pm$ 0.40\% & {\bf\color{blue}67.87} $\pm$ 0.20\% \\
        Ours-WRN & {\bf\color{red}59.60} $\pm$ 0.41\% & {\bf\color{red}73.74} $\pm$ 0.19\% \\
        \bottomrule
    \end{tabular}
    \caption{$5$-way accuracies on MiniImageNet with $95\%$ confidence interval. Red: the best, and blue: the second best.}
    \label{tab:mini}
\end{table}

Table~\ref{tab:mini} summarizes the few-shot accuracies of our method and the previous state-of-the-arts.
For fair comparisons, we implement two convolutional neural networks.
The convolutional network of Ours-Simple is the same as that of Matching Network~\cite{DBLP:conf/nips/VinyalsBLKW16} while Ours-WRN uses WRN-28-10~\cite{Zagoruyko2016WRN} as stated in \S\ref{sec:id}.
The experimental results demonstrate that our average accuracies are better than the previous state-of-the-arts by a large margin for both the Simple and WRN implementations.

It is worth noting that the methods \cite{pmlr-v70-finn17a,ravi2017optimization,DBLP:conf/nips/VinyalsBLKW16} are not evaluated in the full ImageNet classification task.
This is because the architectures of these methods, following the problem formulation of Matching Network~\cite{DBLP:conf/nips/VinyalsBLKW16}, can only deal with the test tasks that are of the same number of reference categories and images as that of the training tasks, limiting their flexibilities for classification tasks of arbitrary number of categories and reference images.
In contrast, our proposed method has no assumptions regarding the number of the reference categories and the images, while achieving good results on both tasks.
From this perspective, our methods are better than the previous state-of-the-arts in terms of both the performance and the flexibility.
