\section{Results}\label{sec:e}
\subsection{Full ImageNet Classification}
In this section we describe our experiments and compare our approach with other strong baseline methods.  As stated in \S\ref{sec:i}, there are three aspects to consider in evaluating a method: (1) its performance on the few-shot set , (2) its performance on the large-scale set , and (3) its computation overhead of adding novel categories and the complexity of image inference. In the following paragraphs, we will cover the settings of the baseline methods, compare the performances on the large-scale and the few-shot sets, and discuss their efficiencies.

\paragraph{Baseline Methods}
The baseline methods must be applicable to both large-scale and few-shot learning settings. We compare our method with a fine-tuned -layer ResNet~\cite{DBLP:conf/cvpr/HeZRS16}, Learning like a Child~\cite{mao2015learning} with a pre-trained -layer ResNet as the starting network, Siamese-Triplet Network~\cite{siamese,lin2017transfer} using three -layer ResNets with shared parameters, and the nearest neighbor using the pre-trained -layer ResNet convolutional features. We will elaborate individually on how to train and use them.

\begin{table*}
    \small
    \centering
    \setlength{\tabcolsep}{1.0em}
    \begin{tabular}{lrccll:rr}
        \toprule
        Method &  &  & FT & Top-1  & Top-5  & Top-1  & Top-5 \\
        \midrule
        NN + Cosine  & 100\% & 1 & N & 71.54\% & 91.20\% & 1.72\% & 5.86\% \\
        NN + Cosine  &  10\% & 1 & N & 67.68\% & 88.90\% & 4.42\% & 13.36\% \\
        NN + Cosine  &   1\% & 1 & N & 61.11\% & 85.11\% & 10.42\% & 25.88\% \\
        Triplet Network~\cite{siamese,lin2017transfer}  & 100\% & 1 & N & 70.47\% & 90.61\% & 1.26\% & 4.94\% \\
        Triplet Network~\cite{siamese,lin2017transfer}   &  10\% & 1 & N & 66.64\% & 88.42\% & 3.48\% & 11.40\% \\
        Triplet Network~\cite{siamese,lin2017transfer}   &   1\% & 1 & N & 60.09\% & 84.83\% & 8.84\% & 22.24\% \\
        Fine-Tuned ResNet~\cite{DBLP:conf/cvpr/HeZRS16} & 100\% & 1 & Y & 76.28\% & 93.17\% & 2.82\% & 13.30\% \\
        Learning like a Child~\cite{mao2015learning} & 100\% & 1 & Y & 76.71\% & 93.24\% & 2.90\% & 17.14\% \\
        \hdashline
        Ours-         & 100\% & 1 & N & 72.56\% & 91.12\% & {\bf\color{blue}19.88\%} & {\bf\color{blue}43.20\%} \\
        Ours-         & 100\% & 1 & N & 74.17\% & 91.79\% & {\bf\color{red}21.58\%} & {\bf\color{red}45.82\%} \\
        Ours-      & 100\% & 1 & N & 75.63\% & 92.92\% & 14.32\% & 33.84\% \\
        \midrule
        NN + Cosine  & 100\% & 2 & N & 71.54\% & 91.20\% & 3.34\% & 9.88\% \\
        NN + Cosine  &  10\% & 2 & N & 67.66\% & 88.89\% & 7.60\% & 19.94\% \\
        NN + Cosine  &   1\% & 2 & N & 61.04\% & 85.04\% & 15.14\% & 35.70\% \\
        Triplet Network~\cite{siamese,lin2017transfer}  & 100\% & 2 & N & 70.47\% & 90.61\% & 2.34\% & 8.30\% \\
        Triplet Network~\cite{siamese,lin2017transfer}  &  10\% & 2 & N & 66.63\% & 88.41\% & 6.10\% & 17.46\% \\
        Triplet Network~\cite{siamese,lin2017transfer}  &   1\% & 2 & N & 60.02\% & 84.74\% & 13.42\% & 32.38\% \\
        Fine-Tuned ResNet~\cite{DBLP:conf/cvpr/HeZRS16} & 100\% & 2 & Y & 76.27\% & 93.13\% & 10.32\% & 30.34\% \\
        Learning like a Child~\cite{mao2015learning} & 100\% & 2 & Y & 76.68\% & 93.17\% & 11.54\% & 37.68\% \\
        \hdashline
        Ours-         & 100\% & 2 & N & 71.94\% & 90.62\% & {\bf\color{blue}25.54\%} & {\bf\color{blue}52.98\%} \\
        Ours-         & 100\% & 2 & N & 73.43\% & 91.13\% & {\bf\color{red}27.44\%} & {\bf\color{red}55.86\%} \\
        Ours-      & 100\% & 2 & N & 75.44\% & 92.74\% & 18.70\% & 43.92\% \\
        \midrule
        NN + Cosine  & 100\% & 3 & N & 71.54\% & 91.20\% & 4.58\% & 12.72\% \\
        NN + Cosine  &  10\% & 3 & N & 67.65\% & 88.88\% & 9.86\% & 24.96\% \\
        NN + Cosine  &   1\% & 3 & N & 60.97\% & 84.95\% & 18.68\% & 42.16\% \\
        Triplet Network~\cite{siamese,lin2017transfer}  & 100\% & 3 & N & 70.47\% & 90.61\% & 3.22\% & 11.48\% \\
        Triplet Network~\cite{siamese,lin2017transfer}  &  10\% & 3 & N & 66.62\% & 88.40\% & 8.52\% & 22.52\% \\
        Triplet Network~\cite{siamese,lin2017transfer}  &   1\% & 3 & N & 59.97\% & 84.66\% & 17.08\% & 38.06\% \\
        Fine-Tuned ResNet~\cite{DBLP:conf/cvpr/HeZRS16} & 100\% & 3 & Y & 76.25\% & 93.07\% & 16.76\% & 39.92\% \\
        Learning like a Child~\cite{mao2015learning} & 100\% & 3 & Y & 76.55\% & 93.00\% & 18.56\% & 50.70\% \\
        \hdashline
        Ours-         & 100\% & 3 & N & 71.56\% & 90.21\% & {\bf\color{blue}28.72\%} & {\bf\color{blue}58.50\%} \\
        Ours-         & 100\% & 3 & N & 72.98\% & 90.59\% & {\bf\color{red}31.20\%} & {\bf\color{red}61.44\%} \\
        Ours-      & 100\% & 3 & N & 75.34\% & 92.60\% & 22.32\% & 49.76\% \\
        \bottomrule
    \end{tabular}
    \caption{Comparing 1000-way accuracies with feature extractor  pre-trained on . For different  settings, red: the best few-shot accuracy, and blue: the second best.}
    \label{tab:real1}
\end{table*}



As mentioned in \S\ref{sec:id}, we first train a -category classifier on .
We will build other baseline methods using this classifier as the staring point. For convenience, we denote this classifier as \smash{}, where pt stands for ``pre-trained''.
Next, we add the novel categories  to each method. For the -layer ResNet, we fine tune \smash{} with the newly added images by extending the fully connected layer to generate  classification outputs. Note that we will limit the number of training samples of  for the few-shot setting.
For Learning like a Child, however, we fix the layers before the global average pooling layer, extend the fully connected layer to include  classes, and only update the parameters for  in the last classification layer.
Since we have the full access to , we do not need Baseline Probability
Fixation~\cite{mao2015learning}. The nearest neighbor with cosine distance can be directly used for both tasks given the pre-trained deep features.

The other method we compare is Siamese-Triplet Network ~\cite{siamese,lin2017transfer}. Siamese network is proposed to approach the few-shot learning problem on Omniglot dataset~\cite{DBLP:conf/cogsci/LakeSGT11}.
In our experiments, we find that its variant Triplet Network~\cite{lin2017transfer,DBLP:conf/iccv/WangG15} is more effective since it learns feature representation from relative distances between positive and negative pairs instead of directly doing binary classification from the feature distance. Therefore, we use the Triplet Network from~\cite{lin2017transfer} on the few-shot learning problem, and upgrade its body net to the pre-trained \smash{}. We use cosine distance as the distance metric and fine-tune the Triplet Network. For inference, we use nearest neighbor with cosine distance.
We use some techniques to improve the speed, which will be discussed later in the efficiency analysis.

\paragraph{Few-Shot Accuracies}
We first investigate the few-shot learning setting where we only have several training examples for . Specifically, we study the performances of different methods when  has for each category 1, 2, and 3 samples.
It is worth noting that our task is much harder than the previously studied few-shot learning: we are evaluating the top predictions out of  candidate categories, \textit{i.e.}, -way accuracies while previous work is mostly interested in -way or -way accuracies~\cite{pmlr-v70-finn17a,siamese,lin2017transfer,ravi2017optimization,DBLP:conf/nips/VinyalsBLKW16}.

With the pre-trained \smash{}, the training samples in  are like invaders to the activation space for  .
Intuitively, there will be a trade-off between the performances on  and . This is true especially for non-parametric methods. Table~\ref{tab:real1} shows the performances on the validation set of ILSVRC 2015~\cite{ILSVRC15}. The second column is the percentage of data of  in use, and the third column is the number of samples used for each category in .
Note that fine-tuned ResNet~\cite{DBLP:conf/cvpr/HeZRS16} and Learning like a Child~\cite{mao2015learning} require fine-tuning while others do not.

Triplet Network is designed to do few-shot image inference by learning feature representations that adapt to the chosen distance metric. It has better performance on  compared with the fine-tuned ResNet and Learning like a Child when the percentage of  in use is low. However, its accuracies on  are sacrificed a lot in order to favor few-shot accuracies. We also note that if full category  supervision is provided, the activations of training a classifier do better than that of training a Triplet Network. We speculate that this is due to the less supervision of training a Triplet Network which uses losses based on fixed distance preferences.
Fine-tuning and Learning like a Child are training based, thus are able to keep the high accuracies on , but perform badly on  which does not have sufficient data for training.
Compared with them, our method shows state-of-the-art accuracies on  without compromising too much the performances on .

Table~\ref{tab:real1} also compares  and , which are trained to minimize Eq.~\ref{eq:mixed} and Eq.~\ref{eq:1}, respectively. Since during training  only mean activations are sampled, it shows a bias towards . However, it still outperforms other baseline methods on .
In short, modeling using Eq.~\ref{eq:mixed} and Eq.~\ref{eq:1} shows a tradeoff between  and .



\paragraph{Oracles}
Here we explore the upper bound performance on .
In this setting we have all the training data for  and  in ImageNet.
For the fixed feature extractor  pre-trained on , we can train a linear classifier on  and , or use nearest neighbor, to see what are the upper bounds of the pre-trained . Table~\ref{tab:oracle} shows the results.
The performances are evaluated on the validation set of ILSVRC 2015~\cite{ILSVRC15} which has  images for each category. The feature extractor pre-trained on  demonstrates reasonable accuracies on  which it has never seen during training for both parametric and non-parametric methods.

\begin{table}
    \small
    \centering
    \vspace{0.05in}
    \setlength{\tabcolsep}{0.5em}
    \begin{tabular}{lcccc}
        \toprule
        Classifier & Top-1  & Top-5  & Top-1  & Top-5 \\
        \midrule
        NN & 70.25\% & 89.98\% & 52.46\% & 80.94 \\
        Linear & 75.20\% & 92.38\% & 60.50\% & 87.58 \\
        \bottomrule
    \end{tabular}
    \caption{Oracle 1000-way accuracies of the feature extractor  pre-trained on .}
    \label{tab:oracle}
\end{table}

\paragraph{Efficiency Analysis}
We briefly discuss the efficiencies of each method including ours on the adaptation to novel categories and the image inference.
The methods are tested on NVIDIA Tesla K40M GPUs.
For adapting to novel categories, fine-tuned ResNet and Learning like a Child require re-training the neural networks.
For re-training one epoch of the data, fine-tuned ResNet and Learning like a Child both take about 1.8 hours on 4 GPUs.
Our method only needs to predict the parameters for the novel categories using  and add them to the original neural network.
This process takes 0.683s using one GPU for adapting the network to  novel categories with one example each.
Siamese-Triplet Network and nearest neighbor with cosine distance require no operations for adapting to novel categories as they are ready for feature extraction.

For image inference, Siamese-Triplet Network and nearest neighbor are very slow since they will look over the entire dataset. Without any optimization, this can take 2.3 hours per image when we use the entire .
To speed up this process in order to do comparison with ours, we first pre-compute all the features.
Then, we use a deep learning framework to accelerate the cosine similarity computation.
At the cost of 45GB memory usage and the time for feature pre-computation, we manage to lower the inference time of them to 37.867ms per image.
Fine-tuned ResNet, Learning like a Child and our method are very fast since at the inference stage, these three methods are just normal deep neural networks.
The inference speed of these methods is about 6.83ms per image on one GPU when the batch size is set to 32.
In a word, compared with other methods, our method is fast and efficient in both the novel category adaptation and the image inference.

\paragraph{Comparing Activation Impacts}\label{sec:cai}
In this subsection we investigate what  has learned that helps it perform better than the cosine distance, which is a special solution for one-layer  by setting  to the identity matrix .
\begin{figure}
    \begin{subfigure}{.455\linewidth}
        \hspace{-0.2in}\vspace{0.1in}
        \includegraphics[width=\linewidth]{img/matrix.pdf}
    \end{subfigure}
    \vrule
    \begin{subfigure}{.53\linewidth}
        \hspace{0.05in}
        \includegraphics[width=\linewidth]{img/os.pdf}
    \end{subfigure}
    \caption{Visualization of the upper-left  submatrix of  in  scale (left) and top- similarity between ,  and \smash{} (right). In the right plotting, red and solid lines are similarities between  and \smash{}, and green and dashed lines are between  and \smash{.}}
    \label{fig:vis}
\end{figure}
We first visualize the matrix  in  scale as shown in the left image of Figure~\ref{fig:vis}. Due to the space limit, we only show the upper-left  submatrix. Not surprisingly, the values on the diagonal dominates the matrix.
We observe that along the diagonal, the maximum is  and the minimum is , suggesting that different from ,  does not use each activation channel equally.
We speculate that this is because the pre-trained activation channels have different distributions of magnitudes and different correlations with the classification task.
These factors can be learned by the last fully connected layer of \smash{} with large amounts of data but are assumed equal for every channel in cosine distance.
This motivates us to investigate the impact of each channel of the activation space.

For a fixed activation space, we define the \emph{impact} of its -th channel on mapping  by
.
Similarly, we define the activation impact  on \smash{} which is the parameter matrix of the last fully connected layer of \smash{}. For cosine distance, , . Intuitively, we are evaluating the impact of each channel of  on the output by adding all the weights connected to it.
For \smash{} which is trained for the classification task using large-amounts of data, if we normalize \smash{} to unity,
the mean of \smash{} over all channel  is  and the standard deviation is .
\smash{} does not use channels equally, either.

In fact,  has a high similarity with \smash{}. We show this by comparing the orders of the channels sorted by their impacts.
Let  find the indexes of the top- elements of . We define the top- similarity of  and \smash{} by

where  is the cardinality of the set.
The right image of Figure~\ref{fig:vis} plots the two similarities, from which we observe high similarity between  and \smash{} compared to the random order of .
From this point of view,  outperforms the cosine distance due to its better usage of the activations.

\subsection{MiniImageNet Classification}
In this subsection we compare our method with the previous state-of-the-arts on the MiniImageNet dataset.
Unlike ImageNet classification, the task of MiniImageNet is to find the correct category from  candidates, each of which has  example or  examples for reference. The methods are only evaluated on , which has  categories.
For each task, we uniformly sample  categories from .
For each of the category, we randomly select one or five images as the references, depending on the settings, then regard the rest images of the  categories as the test images.
For each task, we will have an average accuracy over this  categories.
We repeat the task with different categories and report the mean of the accuracies with the  confidence interval.

\begin{table}
    \small
    \centering
    \vspace{0.05in}
    \setlength{\tabcolsep}{0.5em}
    \begin{tabular}{lcc}
        \toprule
        Method & 1-Shot & 5-Shot \\
        \midrule
        Fine-Tuned Baseline & 28.86  0.54\% & 49.79  0.79\% \\
        Nearest Neighbor & 41.08  0.70\% & 51.04  0.65\% \\
        Matching Network~\cite{DBLP:conf/nips/VinyalsBLKW16} & 43.56  0.84\% & 55.31  0.73\% \\
        Meta-Learner LSTM~\cite{ravi2017optimization} & 43.44  0.77\% & 60.60  0.71\% \\
        MAML~\cite{pmlr-v70-finn17a} & 48.70  1.84\% & 63.11  0.92\% \\
        \hdashline
        Ours-Simple & {\bf\color{blue}54.53}  0.40\% & {\bf\color{blue}67.87}  0.20\% \\
        Ours-WRN & {\bf\color{red}59.60}  0.41\% & {\bf\color{red}73.74}  0.19\% \\
        \bottomrule
    \end{tabular}
    \caption{-way accuracies on MiniImageNet with  confidence interval. Red: the best, and blue: the second best.}
    \label{tab:mini}
\end{table}

Table~\ref{tab:mini} summarizes the few-shot accuracies of our method and the previous state-of-the-arts.
For fair comparisons, we implement two convolutional neural networks.
The convolutional network of Ours-Simple is the same as that of Matching Network~\cite{DBLP:conf/nips/VinyalsBLKW16} while Ours-WRN uses WRN-28-10~\cite{Zagoruyko2016WRN} as stated in \S\ref{sec:id}.
The experimental results demonstrate that our average accuracies are better than the previous state-of-the-arts by a large margin for both the Simple and WRN implementations.

It is worth noting that the methods \cite{pmlr-v70-finn17a,ravi2017optimization,DBLP:conf/nips/VinyalsBLKW16} are not evaluated in the full ImageNet classification task.
This is because the architectures of these methods, following the problem formulation of Matching Network~\cite{DBLP:conf/nips/VinyalsBLKW16}, can only deal with the test tasks that are of the same number of reference categories and images as that of the training tasks, limiting their flexibilities for classification tasks of arbitrary number of categories and reference images.
In contrast, our proposed method has no assumptions regarding the number of the reference categories and the images, while achieving good results on both tasks.
From this perspective, our methods are better than the previous state-of-the-arts in terms of both the performance and the flexibility.
