\section{Experiments}

In this section, we conduct extensive experiments to answer the following questions.
\begin{itemize}
       \item \textbf{(Q1)} How does FGCNN perform, compared to the state-of-the-art models for CTR prediction task?
       \item \textbf{(Q2)} Can FGCNN improve the performance of other state-of-the-art models by using their structures in  \emph{Deep Classifier}?
       \item \textbf{(Q3)} How does each key structure in FGCNN boost the performance of FGCNN?
       \item \textbf{(Q4)} How do the key hyper-parameters of FGCNN
(i.e., size of convolutional kernels,number of convolutional layers, number
of generated features) impact its performance?
       \item \textbf{(Q5)} How does  \emph{Feature Generation}  perform when the order of raw features are randomly shuffled?
\end{itemize}

Note that when studying questions other than \textbf{Q2}, we adpot IPNN as the \emph{Deep Classifier} in FGCNN.

\subsection{Experimental Setup}
\begin{table}[t]
\caption{Dataset Statistics}
\vspace{-3ex}
\label{table:dataset}
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{l|llll}
\hline
Dataset & \#instances & \#features & \#fields & pos ratio \\ \hline
Criteo &  &  & 39 & 0.5 \\
Avazu &  &  & 24 & 0.17 \\
Huawei App Store &  &  & 29 & 0.05 \\\hline
\end{tabular}
}
\vspace{-1ex}
\end{table}
\begin{table}[h]
\vspace{-3ex}
\caption{Parameter Settings}
\vspace{-3ex}
\label{table:parameter}
\scriptsize
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{|l|l|l|l|}
\hline
Param   & Criteo                                                                                                                       & Avazu                                                                                                                               & Huawei App Store                                                                                                                                            \\ \hline
General & \begin{tabular}[c]{@{}l@{}}bs=2000\\ opt=Adam\\ lr=1e-3\end{tabular}                                                         & \begin{tabular}[c]{@{}l@{}}bs=2000\\ opt=Adam\\ lr=1e-3\end{tabular}                                                                & \begin{tabular}[c]{@{}l@{}}bs=1000\\ opt=Adam\\ lr=1e-4\\ l2=1e-6\end{tabular}                                                                     \\ \hline
LR      & --                                                                                                                           & --                                                                                                                                  & --                                                                                                                                                 \\ \hline
GBDT    & \begin{tabular}[c]{@{}l@{}}depth=25\\ \#tree=1300\end{tabular}                                                               & \begin{tabular}[c]{@{}l@{}}depth=18\\ \#tree=1000\end{tabular}                                                                      & \begin{tabular}[c]{@{}l@{}}depth=10\\ \#tree=1400\end{tabular}                                                                                     \\ \hline
FM      & k=20                                                                                                                         & k=40                                                                                                                                & k=40                                                                                                                                               \\ \hline
FFM     & k=4                                                                                                                          & k=4                                                                                                                                 & k=12                                                                                                                                               \\ \hline
CCPM    & \begin{tabular}[c]{@{}l@{}}k=20\\ conv= 7*1\\ kernel=[256]\\ net=[256*3,1]\end{tabular}                                      & \begin{tabular}[c]{@{}l@{}}k=40\\ conv: 7*1\\ kernel = [128]\\ net= [128*3,1]\end{tabular}                                          & \begin{tabular}[c]{@{}l@{}}k=40\\ conv= 13*1\\ kernel= [8,16,32,64]\\ net=[512,256,128,1]\\ drop= 0.8\end{tabular}                                  \\ \hline
DeepFM  & \begin{tabular}[c]{@{}l@{}}k=20\\ LN=T\\ net=[700*5,1]\end{tabular}                                                          & \begin{tabular}[c]{@{}l@{}}k=40\\ LN=T\\ net=[500*5,1]\end{tabular}                                                                 & \begin{tabular}[c]{@{}l@{}}k=40\\ net=[2048,1024,512,1]\\ drop=0.9\end{tabular}                                                                    \\ \hline
XdeepFM & \begin{tabular}[c]{@{}l@{}}k=20\\ net=[400*3,1]\\ CIN=[100*4]\end{tabular}                                                   & \begin{tabular}[c]{@{}l@{}}k=40\\ net=[700*5,1]\\ CIN:[100*2]\end{tabular}                                                          & \begin{tabular}[c]{@{}l@{}}k=40\\ net=[2048,1024,512,1]\\ drop=0.9\\ CIN:[100*4]\end{tabular}                                                                \\ \hline
IPNN    & \begin{tabular}[c]{@{}l@{}}k=20\\ LN=T\\ net=[700*5,1]\end{tabular}                                                          & \begin{tabular}[c]{@{}l@{}}k=40\\ LN=T\\ net=[500*5,1]\end{tabular}                                                                 & \begin{tabular}[c]{@{}l@{}}k=40\\ net=[2048,1024,\\ 512,256,128,1]\\ drop=0.7\end{tabular}                                                         \\ \hline
PIN     & \begin{tabular}[c]{@{}l@{}}k=20\\ LN=T\\ net=[700*5,1]\\ subnet=[40,5]\end{tabular}                                          & \begin{tabular}[c]{@{}l@{}}k=40\\ LN=T\\ net=[500*5,1]\\ sub-net=[40,5]\end{tabular}                                                & \begin{tabular}[c]{@{}l@{}}k=40\\ net=[2048,1024,512,1]\\ drop=0.9\\ sub-net=[80,10]\end{tabular}                                                  \\ \hline
FGCNN   & \begin{tabular}[c]{@{}l@{}}k=20\\ conv=9*1\\ kernel=[38,40,42,44]\\ new=[3,3,3,3]\\ BN=T\\ ruenet=[4096,2048,1]\end{tabular} & \begin{tabular}[c]{@{}l@{}}k=40\\ conv=7*1\\ kernel=[14,16,18,20]\\ new=[3,3,3,3]\\ BN=T\\ net=[4096,2048,\\ 1024,512,1]\end{tabular} & \begin{tabular}[c]{@{}l@{}}k=40\\ conv=13*1\\ kernel=[6, 8, 10, 12]\\ new=[2,2,2,2]\\ BN=T\\ net=[2048,1024,\\ 512,256,128,1]\\ drop=0.8\end{tabular} \\ \hline
\end{tabular}
}
Note: bs=batch size, opt=optimizer, lr=learning rate, l2=l2 regularization on Embedding Layer, k=embedding size, conv=shape of convolutional kernels, kernel=number of convolutional kernels, pool=max-pooling shape, net=MLP structure, sub-net=micro metwork, drop=dropout rate, LN=layer normalization, BN= batch normalization\{T=True\}, new=number of kernels for new features.
\vspace{-5ex}
\end{table}
\begin{table*}[t]
\centering
\vspace{-4ex}
\caption{Overall Performance\\
 (two tailed t-test)}
\vspace{-3ex}
\label{table:overall}
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{lll|ll|ll}
 \hline\hline
        & \multicolumn{2}{|c|}{Criteo}                                    & \multicolumn{2}{|c|}{Avazu}                                              & \multicolumn{2}{|c}{Huawei App Store}                        \\ \hline
\multicolumn{1}{l|}{Model}   & AUC                 & \multicolumn{1}{l|}{Log Loss}           & AUC(\%)                  & \multicolumn{1}{l|}{Log Loss}               & AUC                      & Log Loss                \\ \hline
\multicolumn{1}{l|}{LR}      & 78.00\%             & \multicolumn{1}{l|}{0.5631}             & 76.76\%                  & \multicolumn{1}{l|}{0.3868}                 & 90.12\%                  & 0.1371                  \\
\multicolumn{1}{l|}{GBDT}    & 78.62\%             & \multicolumn{1}{l|}{0.5560}             & 77.53\%                  & \multicolumn{1}{l|}{0.3824}                 & 92.68\%                  & 0.1227                  \\
\multicolumn{1}{l|}{FM}      & 79.09\%             & \multicolumn{1}{l|}{0.5500}             & 77.93\%                  & \multicolumn{1}{l|}{0.3805}                 & 93.26\%                  & 0.1191                  \\
\multicolumn{1}{l|}{FFM}     & 79.80\%             & \multicolumn{1}{l|}{0.5438}             & 78.31\%                  & \multicolumn{1}{l|}{0.3781}                 & 93.58\%                  & 0.1170                  \\
\multicolumn{1}{l|}{CCPM}    & 79.55\%             & \multicolumn{1}{l|}{0.5469}             & 78.12\%                  & \multicolumn{1}{l|}{0.3800}                 & 93.71\%                  & 0.1159                 \\
\multicolumn{1}{l|}{DeepFM}  & 79.91\%             & \multicolumn{1}{l|}{0.5423}             & 78.36\%                  & \multicolumn{1}{l|}{0.3777}                 & 93.91\%                  & 0.1145                  \\
\multicolumn{1}{l|}{xDeepFM} & 80.06\%             & \multicolumn{1}{l|}{0.5408}             & 78.55\%                  & \multicolumn{1}{l|}{0.3766}                 & 93.91\%                  & 0.1146                 \\
\multicolumn{1}{l|}{IPNN}    & 80.13\%             & \multicolumn{1}{l|}{0.5399}             & 78.68\%                  & \multicolumn{1}{l|}{0.3757}                 & \underline{93.95}\%                  & \underline{0.1143}                  \\
\multicolumn{1}{l|}{PIN}     & \underline{80.18\%}\footnotemark[11] & \multicolumn{1}{l|}{\underline{0.5393}} & \underline{78.72\%}      & \multicolumn{1}{l|}{\underline{0.3755}}     & 93.91\%      & 0.1146      \\ \hline
\multicolumn{1}{l|}{FGCNN}   &      & \multicolumn{1}{l|}{}    &  & \multicolumn{1}{l|}{} &  & \\ \hline\hline
\end{tabular}
}
\vspace{-3ex}
\end{table*}


\subsubsection{Datasets} Experiments are conducted in the following three datasets:

\textit{Criteo}: Criteo\footnote{http://labs.criteo.com/downloads/download-terabyte-click-logs/} contains one month of click logs with billions of data samples. A small subset of Criteo was published in Criteo Display Advertising Challenge 2013 and FFM was the winning solution~\cite{pin, ffm}. We select ``day 6-12'' as training set while select ``day 13'' for evaluation. Due to the enormous data volume and serious class imbalance (i.e., only 3\% samples are positive), negative sampling is applied to keep the positive and negative ratio close to 1:1. We convert 13 numerical fields into one-hot features through bucketing, where the features in a certain field appearing less than 20 times are set as a dummy feature ``other``.

\textit{Avazu}: Avazu\footnote{http://www.kaggle.com/c/avazu-ctr-prediction} was published in the contest of Avazu Click-Through Rate Prediction, 2014. The public dataset is randomly splitted into training and test sets at 4:1. Meanwhile, we remove the features appearing less than 20 times to reduce dimensionality.

\textit{Huawei App Store}: In order to evaluate the performance in industrial CTR prediction problems, we conduct experiments on Huawei App Store Dataset. We collect users' click logs from Huawei App Store while logs from 20180617 to 20180623 are used for training and logs of 20180624 are used for test. Negative sampling is applied to reduce data amount and to adjust the ratio of positive class and negative class. The dataset contains app features (e.g., identification, category), user features (e.g., user's behavior history) and context features (e.g., operation time).




In addition, the statistics of the three datasets are summarized in Table~\ref{table:dataset}.

\subsubsection{Baselines}

We compare nine baseline models in our experiments, including LR~\cite{lr}, GBDT~\cite{gbdt}, FM~\cite{fm}, FFM~\cite{ffm}, CCPM~\cite{ccpm}, DeepFM~\cite{deepfm}, xDeepFM~\cite{xdeepfm}, IPNN and PIN~\cite{pin}. Wide \& Deep is not compared here because some state-of-the-art models (such as xDeepFM, DeepFM, PIN) have shown better performance in their publications. We use XGBoost~\cite{gbdt} and libFFM\footnote{https://github.com/guestwalk/libffm} as the implementation of GBDT and libFFM, respectively.  In our experiments, the other baseline models are implemented with Tensorflow\footnote{https://www.tensorflow.org/}.

\subsubsection{Evaluation Metrics} The evaluation metrics are \textbf{AUC} (Area Under ROC) and \textbf{log loss} (cross entropy).

\subsubsection{Parameter Settings}




Table~\ref{table:parameter} summerizes the hyper-parameters of each model.
For \textit{Criteo} and \textit{Avazu} Datasets, the hyper-parameters of baseline models are set to be the same as in~\cite{pin}. Notice that when conducting experiments on Criteo and Avazu, we observed that FGCNN uses more parameters in \emph{Deep Classifier} than other models. To make fair comparisons, we also conduct experiments to increase the parameters in MLPs of other deep models. However, all these models cannot achieve better performance than the original settings\footnote{Due to the limited pages, we do not show the experimental result in the paper.}. The reason could be the overfitting problem where such models simply use embedding of raw features for training but use a complex structure. On the other hand, since our model augments the feature space and enriches the input, more parameters in \emph{Deep Classifier} can boost the performance of our model.

In FGCNN model, \textit{new} is the number of kernels when generating new features. The number of generated features can be calculated as .



\subsubsection{Significance Test} We repeat the experiments 10 times by changing the random seed for FGCNN and the best baseline model. The two-tailed pairwise t-test is performed to detect significant differences between FGCNN and the best baseline model. \begin{table*}[]
\caption{Compatibility Study of FGCNN}
\vspace{-3ex}
\label{table:integrete}
\resizebox{0.7\textwidth}{!}{
\begin{tabular}{c|ll|ll|ll}
\hline\hline
& \multicolumn{2}{c|}{Criteo} & \multicolumn{2}{c|}{Avazu} & \multicolumn{2}{c}{Huawei App Store} \\ \hline
& \multicolumn{1}{l}{AUC} & \multicolumn{1}{l|}{Log Loss} & \multicolumn{1}{l}{AUC} & \multicolumn{1}{l|}{Log Loss} & \multicolumn{1}{l}{AUC} & \multicolumn{1}{l}{Log Loss} \\ \hline
FM & 79.09\% & 0.5500 & 77.93\% & 0.3805 & 93.26\% & 0.1191 \\
FGCNN+FM & \textbf{79.67\%} & \textbf{0.5455} & \textbf{78.13\%} & \textbf{0.3794}  & \textbf{93.66}\% & \textbf{0.1165} \\ \hline
DNN & 79.87\% & 0.5428 & 78.30\% & 0.3778 & 93.85\% &0.1149 \\
FGCNN+DNN & \textbf{80.09\%} &\textbf{0.5402} & \textbf{78.55\%} & \textbf{0.3764} & \textbf{94.00\%} & \textbf{0.1139} \\ \hline
DeepFM & 79.91\% & 0.5423 & 78.36\% & 0.3777 & 93.91\% & 0.1145 \\
FGCNN+DeepFM & \textbf{79.94\%} & \textbf{0.5421}&\textbf{78.44\%} & \textbf{0.3771}& \textbf{93.93\%} & \textbf{0.1145} \\ \hline
IPNN & 80.13\% & 0.5399 & 78.68\% & 0.3757 & 93.95\% & 0.1143 \\
FGCNN+IPNN & \textbf{80.22\%} & \textbf{0.5388} & \textbf{78.83\%} & \textbf{0.3746} & \textbf{94.07\%} & \textbf{0.1134} \\ \hline
\hline

\end{tabular}
}
\vspace{-2ex}
\end{table*}



\subsection{Overall Performance (Q1)}
\footnotetext[11]{According to the experimental recordings, the results of PIN in ~\cite{pin} uses the trick of adaptive embedding size. Here, we use fixed embedding size for all the deep models.}

In this subsection, we compare the performance of different models on the test set. Table~\ref{table:overall} summarizes the overall performance of all the compared models on the three datasets, where the underlined numbers are the best results of the baseline models and bold numbers are the best results of all models.
We have the following observations:

Firstly, in majority of the cases, non-neural network models perform worse than neural network models. The reason is that deep neural network can learn complex feature interactions much better than the models where no feature interaction is modeled (i.e., LR), or feature interactions are modeled by simple inner product operations (i.e., FM and FFM).

Secondly,  FGCNN achieves the best performance among all the models on the three evaluated datasets. It is significantly better than the best baseline models with 0.05\%, 0.14\% and 0.13\% improvements in AUC (0.09\%, 0.24\% and 0.79\% in log loss) on Criteo, Avazu and Huawei App Store datasets, which demonstrates the effectiveness of FGCNN.  In fact, a small improvement in offline AUC is likely to lead to a significant increase in online CTR. As reported in~\cite{cheng2016wide}, compared with LR, Wide \& Deep improves offline AUC by 0.275\% and the improvement of online CTR is 3.9\%. The daily turnover of Huawei App Store is millions of dollars. Therefore, even a few lifts in CTR brings extra millions of dollars each year. 

Thirdly, with the help of the generated new features, FGCNN outperforms IPNN by 0.11\%, 0.19\% and 0.13\% in terms of AUC (0.2\%, 0.29\% and 0.79\% in terms of log loss) on Criteo, Avazu and Huawei App Store datasets. It demonstrates that the generated features are very useful and  they can effectively reduce the optimization difficulties of traditional DNNs thus leading to better performance.

Fourthly, CCPM, which applies CNN directly, achieves the worst performance among neural network models.  Moreover, CCPM performs worse than FFM on Criteo and Avazu datasets. It shows that directly using traditional CNN for CTR prediction task is inadvisable, as CNN is designed to generate neighbor patterns while the arrangement order of feature is usually no meaning in recommendation scenarios. However, in FGCNN, we leverage the strength of CNN to extract local patterns while complementing it with Recombination Layer to extract global feature interactions and generate new features. Therefore, better performance is achieved.
\subsection{Compatibility of FGCNN with Different Models (Q2)}

As stated in Section ~\ref{sec:clf}, \emph{Feature Generation}  can augment the original feature space and \emph{Deep Classifier} of FGCNN can adopt any advanced deep neural networks. Therefore, we select several models  as \emph{Deep Classifeir} to verify the utility of \emph{Feature Generation}, including non-deep models (FM), deep learning models (DNN, DeepFM, IPNN).

Table~\ref{table:integrete} summarizes the performance. We have the following observations: Firstly, with the help of the generated new features, the performance of all models are improved, which demonstrates the effectiveness of the generated features and shows the compatibility of FGCNN. Secondly, we observe that when only using raw features, DeepFM always outperforms DNN. But when using the augmented features, FGCNN+DNN outperforms FGCNN+DeepFM. The possible reason is that DeepFM sums up the inner product of input features to the last MLP layer which may cause contradictory gradient updates (compared with MLP) on embeddings.This could be one of the reasons why IPNN (feeding the product into MLP) outperforms DeepFM in all datasets.

In a word, the results show that our FGCNN model can be viewed as a general framework to enhance the existing neural networks by generating new features automatically.

\subsection{Effectiveness of FGCNN Variants(Q3)}
\begin{table*}[t]
\vspace{-2ex}
\caption{Performance of Different FGCNN Variants}
\vspace{-3ex}
\label{table:remove}
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{l|ll|ll|ll}
\hline\hline
       & \multicolumn{2}{c|}{Criteo} & \multicolumn{2}{c|}{Avazu} & \multicolumn{2}{c}{Huawei App Store} \\ \hline
Method & AUC          & Log Loss     & AUC         & Log Loss     & AUC          & Log Loss     \\ \hline
FGCNN  & \textbf{80.22\%}      & \textbf{0.5388}      & \textbf{78.83\%}     & \textbf{0.3746}       & \textbf{94.07\%}      & \textbf{0.1134}       \\
Removing Raw Features   & 80.21\%      & 0.5390       & 78.66\%     & 0.3757        & 94.01\%      &0.1138       \\
Removing New Features   & 80.13\%      & 0.5399       & 78.68\%     & 0.3757       &  93.95\%      &  0.1143      \\
Applying MLP for Feature Generation   & 80.12\%      & 0.5402       & 78.58\%     & 0.3761       & 94.04\%             &0.1135              \\
Removing Recombination Layer    & 80.11\%      & 0.5403       & 78.74\%     & 0.3753       & 94.04\%             & 0.1135             \\
\hline\hline
\end{tabular}
}
\vspace{-2ex}
\end{table*}


 We conduct experiments to study how each component in FGCNN contributes to the final performance. Each variant is generated by removing or replacing some components in FGCNN, which is described as follows:

\begin{itemize}
\item \textbf{Removing Raw Features}: In this variant, raw features are not input into \textit{Deep Classifier} and only the  generated new  features are fed to \emph{Deep Classifier}.
 \item \textbf{Removing New Features}: This variant removes \emph{Feature Generation}. Actually, it is equivalent to IPNN.
 \item \textbf{Applying MLP for \textit{Feature Generation}}: \emph{Feature Generation} is replaced by MLP which takes the neurons in each layer as new features. This variant uses the same hidden layers and generates the same number of features in each layer as FGCNN.
 \item \textbf{Removing Recombination Layer}: This variant is to evaluate how \emph{Recombination Layer} complements CNN to capture global feature interactions.  The \emph{Recombination Layer} is removed from \emph{Feature Generation}   so that the output of pooling layer serves as new features directly. The number of generated new features in each layer keeps the same as FGCNN.
\end{itemize}

As shown in  Table~\ref{table:remove}, removing any component in FGCNN leads to a drop in performance. We have the following observations:

Firstly, FGCNN with raw features alone or with new generated features alone performs worse than the FGCNN with both of them. This result demonstrates that the generated features are good supplementaries to the original features, which are both crucial.

Secondly, the performance decrease of \textit{Appling MLP for Feature Generation}, compared to FGCNN, shows the ineffectiveness of MLP to identify the sparse but important feature combinations from a large number of parameters. CNN simplifies the learning difficulties by using the shared convolutional kernels, which has much fewer parameters to get the desired combinations. Moreover,  MLP recombines the neighbor feature interactions, which is extracted by CNN, to generate global feature interactions.

Thirdly, removing the Recombination Layer will limit the generated features as neighbor feature interactions. Since the arrangement order of raw features has no actual meanings in the CTR prediction task,  the restriction can lead to losing important non-neighbor feature interactions thus resulting in worse performance.







\subsection{Hyper-parameter Investigation (Q4)}

Our FGCNN model has several key hyper-parameters, i.e., the height of convolutional kernels, number of convolutional kernels, number of convolutional layers, and the number of kernels for generating new features. In this subsection, to study the impact of these hyper-parameters, we investigate how FGCNN model works by changing one hyper-parameter while fixing the others on Criteo and Huawei App Store datasets.

\subsubsection{Height of Convolutional Kernels}
 
The height of convolutional kernels controls the perception range of convolutional layers. The larger the height is, the more features are involved in the neighbor patterns, but more parameters need to be optimized. To investigate its impact, we increase the height from 2 to the number of fields of a dataset. As shown in the top of Figure~\ref{fig:param_all},
the performance generally ascends first and then descends as the height of convolutional kernels increases\footnote{Due to that \textit{Feature Generation} and \textit{Deep Classifier} interrelate with each other, the curve has some fluctuations.}.
\begin{figure}[t]
 \centering
 \includegraphics[width=0.45\textwidth,height=9cm]{parameter_all_odd.pdf}
 \vspace{-4ex}
 \caption{Parameter study of height of convolutional kernel, number of convolutional layers and number of  kernels for new features (from top to bottom)}
 \vspace{-3ex}
 \label{fig:param_all}
\end{figure}

The results show that as more features are involved in the convolutional kernels, higher-order feature interactions can be learned so that the performance increases. However, due to that useful feature interactions are usually sparse, larger heights can cause more difficulties to learn them effectively, leading to a decrease in performance. This observation is consistent with the finding in Section 3.4, i.e., the performance decreases in Appling MLP for Feature Generation.

\subsubsection{Number of Convolutional Layers}
As shown in  the middle of  Figure~\ref{fig:param_all}, as the number of convolutional layers increases, the performance of FGCNN is improved. Notice that more layers usually lead to higher-order feature interactions. Therefore, the result also shows the effectiveness of high-order feature interactions. 

\subsubsection{Number of Kernels for Generating New Features}
We study how the number of generated features affects the performance of FGCNN. We use the same number of kernels for new features in different \emph{Recombination Layers}.  As can be observed in the bottom of Figure~\ref{fig:param_all}, the performance is gradually improved with more features generated. The results verify our research idea that it is useful to identify the sparse but important feature interactions first, which can effectively reduce the learning difficulties of DNNs. However, the useful feature interactions can be sparse and limited. If too many  features are generated, the extra new features are noisy which will increase the learning difficulties of MLP,  leading to the decrease in the performance.

\subsection{Effect of Shuffling Order of Raw Features (Q5)}
\begin{figure}[h]
 \centering
\vspace{-2ex}
 \includegraphics[width=0.45\textwidth]{shuffle_criteo_company.pdf}\vspace{-3ex}
 \caption{Shuffling the order of raw features}\label{fig:shuffle}
 \vspace{-3ex}
 \label{shuffle}
\end{figure}

As mentioned before, CNN is designed to capture local neighbor feature patterns so that it is sensitive to the arrangement order of raw features. In our FGCNN model, the design of Recombination Layer is to learn global feature interactions based on CNN's extracted local patterns. Intuitively, our model should have more stable performance than traditional CNN's structure if the order of raw features is shuffled. Therefore,  to verify it, we compare the performance of two cases: with/without Recombination Layer. The arrangement order of raw features is shuffled many times at random where the two compared cases are performed for the same shuffled arrangement order. 


As shown in  Figure~\ref{fig:shuffle}, the case with Recombination Layer achieves better and more stable performance than that of without Recombination Layer. It demonstrates that with the help of Recombination Layer, FGCNN can greatly reduce the side effects of changing the arrangement order of raw features, which also demonstrates the robustness of our model.





