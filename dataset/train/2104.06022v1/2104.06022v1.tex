\pdfoutput=1


\documentclass[11pt]{article}

\usepackage{emnlp2021}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}


\usepackage{graphicx}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\makeatletter
\renewcommand{\fnum@algorithm}{\fname@algorithm}
\makeatother



\title{Lessons on Parameter Sharing across Layers in Transformers}

\author{Sho Takase \\
  Tokyo Institute of Technology \\
  \texttt{sho.takase@nlp.c.titech.ac.jp} \\\And
  Shun Kiyono \\
  RIKEN / Tohoku University \\
  \texttt{shun.kiyono@riken.jp} \\
 }


\begin{document}
\maketitle
\begin{abstract}
We propose a parameter sharing method for Transformers~\cite{NIPS2017_7181}.
The proposed approach relaxes a widely used technique, which shares parameters for one layer with all layers such as Universal Transformers~\cite{dehghani2019}, to increase the efficiency in the computational time.
We propose three strategies: \textsc{sequence}, \textsc{cycle}, and \textsc{cycle (rev)} to assign parameters to each layer.
Experimental results show that the proposed strategies are efficient in the parameter size and computational time.
Moreover, we indicate that the proposed strategies are also effective in the configuration where we use many training data such as the recent WMT competition.
\end{abstract}

\section{Introduction}
\label{sec:intro}
Transformer-based methods have achieved notable performance in various NLP tasks~\cite{NIPS2017_7181,devlin-etal-2019-bert,NEURIPS2020_1457c0d6}.
In particular, \newcite{NEURIPS2020_1457c0d6} indicated that the larger parameter size we prepare, the better performance the model achieves.
However, the model which is composed of many parameters occupies a large part of a GPU memory capacity.
Thus, it is important to explore a parameter efficient way, which achieves better performance than a basic model with the same parameter size.


Parameter sharing is a widely used technique as a parameter efficient way~\cite{dehghani2019,Dabre_Fujita_2019,lan2020}.
\newcite{dehghani2019} proposed Universal Transformer which consists of parameters for only one layer of a Transformer-based encoder-decoder, and uses the parameters  times for an -layered encoder-decoder.
\newcite{Dabre_Fujita_2019} and \newcite{lan2020} also used such parameter sharing across layers for their Transformers.


\newcite{dehghani2019} reported that Universal Transformer achieved better performance than the vanilla Transformer in machine translation if they consist of the same number of parameters.
However, when we prepare the same number of parameters for Universal Transformer and basic Transformer, Universal Transformer requires much more computational time because weight matrices for each layer in Universal Transformer are much larger.
For example, we demonstrate that Universal Transformer requires twice as much training time as the basic Transformer in WMT En-De, which is a widely used machine translation dataset.


In this paper, we propose a new parameter sharing method which is faster than using the same parameters for all layers such as Universal Transformer.
Instead of preparing parameters for only one layer, we prepare parameters for  layers to construct an -layered encoder-decoder, where .
In other words, the proposed method relaxes the parameter sharing strategy used in previous studies~\cite{dehghani2019,Dabre_Fujita_2019,lan2020}.
For the parameter assignment to each layer, we provide several strategies and compare them empirically.
Experimental results show that the proposed method achieves comparable scores to the method assigning parameters of one layer to all layers with smaller computational time.

\begin{figure}[!t]
  \centering 
  \includegraphics[width=8cm]{./method_examples.pdf}
   \caption{Examples of three parameter assignment strategies proposed by this study when we set  and .}
   \label{fig:overview}
\end{figure}

\section{Proposed Method}
\begin{figure}[!t]
  \begin{algorithm}[H]
  \caption{Encoder Construction}
  \begin{algorithmic}[1]
   \renewcommand{\algorithmicrequire}{\textbf{Input:}}
   \renewcommand{\algorithmicensure}{\textbf{Output:}}
   \Require the total number of layers , the number of independent layers , the sharing strategy \textsc{Type}  \{\textsc{sequence, cycle, cycle (rev)}\}
   \Ensure , ..., 
   \For{ \textrm{in} }
     \If{ == }
     \State 
     \ElsIf{\textsc{Type} == \textsc{sequence}}
       \If{}
       \State 
       \Else
       \State 
       \EndIf
     \ElsIf{\textsc{Type} == \textsc{cycle}}
       \If{}
       \State 
       \Else
       \State 
       \EndIf
     \ElsIf{\textsc{Type} == \textsc{cycle (rev)}}
       \If{}
       \State 
       \ElsIf{}
       \State 
       \Else
       \State        
       \EndIf
     \EndIf
   \EndFor
  \end{algorithmic}
  \end{algorithm}
  \caption{The proposed parameter assignment strategies for an encoder construction. CreateNewLayer is a function which creates a new encoder layer.}
  \label{fig:enc_construction}
\end{figure}

As described in Section \ref{sec:intro}, we use parameters for  layers in the construction of an -layered Transformer-based encoder-decoder.
For the parameter assignment, we provide three strategies: \textsc{sequence}, \textsc{cycle}, and \textsc{cycle (rev)}.
We describe these strategies in this section.

Figure \ref{fig:overview} shows examples of three parameter assignment strategies for an encoder-side when we set  and .
Let  be the -th layer of an encoder.
Figure \ref{fig:enc_construction} describes the algorithm to assign each parameter to each layer for the encoder.
For the decoder-side, we assign each parameter with the same manner.


\subsection{\textsc{sequence}}
The simplest strategy is to assign the same parameters to sequential  layers.
We name this strategy \textsc{sequence}.
For example, when we set  and , sequential  layers share their parameters as illustrated in Figure \ref{fig:overview}.



\subsection{\textsc{cycle}}
In \textsc{cycle}, we stack  layers whose parameters are independent from each other.
Then, we repeat stacking the  layers with the identical order to the first  layers until the total number of layers reaches .
When we set  and , we stack  layers twice as illustrated in Figure \ref{fig:overview}.


\subsection{\textsc{cycle (rev)}}
\newcite{liu-etal-2020-understanding} reported that higher decoder layers obtain larger gradient norms when we use the post layer normalization setting, which is originally used in \newcite{NIPS2017_7181} and widely used in machine translation.
Their report implies that higher layers require more degrees of freedom than lower layers for their expressiveness.
In other words, lower layers probably have redundant parameters in comparison with higher layers.
Thus, we propose the strategy \textsc{cycle (rev)} reusing parameters of lower layers in higher layers.


In this strategy, we repeat stacking  layers in the same as \textsc{cycle} until  layers.
For the rest of the layers, we stack  layers in reverse order.
When we set  and , we stack  layers and then stack the  layers in reverse order as illustrated in Figure \ref{fig:overview}.
Thus, the lowest layer and highest layer share their parameters.



\section{Layer Normalization in Transformers}
For layer normalizations in Transformers, most recent studies used the pre layer normalization setting (Pre-LN) when they stacked many layers~\cite{wang-etal-2019-learning,NEURIPS2020_1457c0d6} because Pre-LN makes a training more stable than the post layer normalization setting (Post-LN)~\cite{iwslt-2019-transformer,layer-normalization-icml2020}.
However, Transformers with Post-LN achieve better performance if we succeed in their training~\cite{iwslt-2019-transformer,liu-etal-2020-understanding}.
To make a training Transformers with Post-LN stable, \newcite{liu-etal-2020-understanding} proposed Admin which smooths the impacts of each parameter in the early stage of training.
In this study, we also use Admin for the stable training.


\section{Experiments}
\begin{table}[!t]
  \centering
\begin{tabular}{ c | c | c } \hline
  Setting & Genuine & Synthetic \\ \hline
  Standard & 4.5M & - \\
  High Resource & 44.2M & 284.3M \\ \hline
  \end{tabular}
  \caption{The number of translation sentence pairs in training datasets for each experiment.}
  \label{table:mt_dataset}
\end{table}


\begin{table*}[!t]
  \centering{}
  \footnotesize
  \begin{tabular}{ l | c c | r | r | c c c c c c c | c} \hline
  Method &  &  & \multicolumn{1}{c|}{\#Params} & Speed & 2010 & 2011 & 2012 & 2013 & 2014 & 2015 & 2016 & Average \\ \hline \hline
  Original & 6 & 6 & 61M & 1.00 & 24.16 & 22.01 & 22.33 & 26.13 & 27.13 & 29.83 & 34.41 & 26.57 \\
  Admin & 6 & 6 & 61M & 0.97 & 24.14 & 21.93 & 22.25 & 26.14 & 27.05 & 29.59 & 34.23 & 26.48 \\
  Universal & 1 & 6 & 63M & 0.48 & 24.37 & 22.33 & 22.70 & 26.40 & 27.65 & 30.24 & 34.60 & 26.90 \\
  Original (big) & 6 & 6 & 210M & 0.39 & 24.31 & 22.21 & 22.75 & 26.39 & 28.28 & 30.35 & 33.40 & 26.81 \\
  \textsc{sequence} & 6 & 12 & 61M & 0.63 & 24.65 & 22.32 & 22.83 & \textbf{26.98} & 27.88 & 30.27 & \textbf{34.99} & \textbf{27.13} \\  
  \textsc{cycle} & 6 & 12 & 61M & 0.63 & 24.51 & 22.43 & 22.69 & 26.61 & \textbf{27.91} & \textbf{30.37} & 34.77 & 27.04 \\
  \textsc{cycle (rev)} & 6 & 12 & 61M & 0.63 & \textbf{24.66} & \textbf{22.47} & \textbf{22.87} & 26.68 & 27.72 & \textbf{30.37} & 34.81 & 27.08 \\ \hline
  \textsc{sequence} & 6 & 18 & 61M & 0.47 & 24.53 & 22.44 & 22.73 & 26.59 & 27.73 & 30.30 & 34.80 & 27.02 \\
  \textsc{cycle} & 6 & 18 & 61M & 0.47 & 24.74 & 22.60 & 23.04 & \textbf{26.89} & \textbf{28.14} & 30.54 & 34.79 & 27.25 \\
  \textsc{cycle (rev)} & 6 & 18 & 61M & 0.47 & \textbf{24.93} & \textbf{22.77} & \textbf{23.09} & 26.88 & 28.09 & \textbf{30.60} & \textbf{34.84} & \textbf{27.31} \\ \hline
  \end{tabular}
  \caption{The number of layers, the number of parameters, computational speeds based on the original Transformer, BLEU scores on newstest2010-2016, and averaged scores when we trained each method on widely used WMT 2016 training dataset. Scores in bold denote the best result for each set.\label{tab:exp_main_result}}
\end{table*}

\begin{table}[!t]
  \centering{}
  \footnotesize
  \begin{tabular}{ l | c | c c c } \hline
  Method & \#Params & 2014 & 2018 & 2019 \\ \hline \hline
  \multicolumn{5}{c}{Genuine training data} \\ \hline \hline
  Original (big) & 242M & 31.40 & 47.11 & 42.80  \\
  \textsc{sequence} & 242M & 31.90 & 48.15 & 43.12 \\
  \textsc{cycle} & 242M & \textbf{32.10} & 48.11 & 43.19 \\
  \textsc{cycle (rev)} & 242M & 32.06 & \textbf{48.34} & \textbf{43.43} \\ \hline \hline
  \multicolumn{5}{c}{+ Synthetic (back-translated) data} \\ \hline \hline
  \newcite{kiyono-etal-2020-tohoku} & 514M & 33.1 \  & 49.6 \  & 42.7 \  \\
  \textsc{cycle (rev)} & 343M & \textbf{33.54} & \textbf{49.55} & 42.18 \\ \hline
  \end{tabular}
  \caption{BLEU scores on newstest2014, 2018, and 2019. We focused these test sets to compare the top system on WMT 2020~\cite{kiyono-etal-2020-tohoku}.\label{tab:exp_wmt2020_result}}
\end{table}

We focus on the English-to-German translation task in the same as previous studies~\cite{NIPS2017_7181,dehghani2019,kiyono-etal-2020-tohoku}.
We conduct experiments on two types of English-to-German translation tasks.
The difference is the amount of training data.
Table \ref{table:mt_dataset} summarizes the number of training data in each configuration.


\subsection{Standard Setting}
\label{sec:exp_standard_setting}

\paragraph{Datasets}
We used WMT 2016 training dataset, which is widely used in previous studies~\cite{NIPS2017_7181,ott-etal-2018-scaling}.
The dataset contains 4.5M English-German sentence pairs.
Following the previous studies, we constructed a vocabulary set with BPE~\cite{sennrich-etal-2016-neural} in the same manner.
We set the number of BPE merge operations at 32K and shared the vocabulary between both the source and target languages.
We measured case-sensitive detokenized BLEU with SacreBLEU~\cite{post-2018-call}\footnote{The BLEU score computed by SacreBLEU is often lower than the score by the procedure of \newcite{NIPS2017_7181} as reported in \newcite{ott-etal-2018-scaling}. In fact, when we used the same procedure as \newcite{NIPS2017_7181}, the best model in Table \ref{tab:exp_wmt2020_result} achieved 35.14 in the averaged BLEU score in newstest2014. However, \newcite{post-2018-call} encouraged using SacreBLEU for the compatibility of WMT results.}.


\paragraph{Methods}
For the proposed parameter assignment strategies, we fixed  and set  based on the Transformer (base) setting in \newcite{NIPS2017_7181}.
We compare the proposed strategies with the following baselines.

\noindent{\textbf{Original} and \textbf{Original (big)}}: These are the original Transformer (base) and (big) settings in \newcite{NIPS2017_7181}, respectively.

\noindent{\textbf{Admin}}: We applied Admin~\cite{liu-etal-2020-understanding} to the Transformer (base) setting.

\noindent{\textbf{Universal}}: As the parameter sharing strategy in previous studies such as Universal Transformers~\cite{dehghani2019}, we set \footnote{The original Universal Transformers~\cite{dehghani2019} use the sinusoidal positional encoding for each layer and adaptive computation time technique~\cite{graves2017adaptive} but we omitted them in this study to focus on the difference among parameter sharing strategies.}.
In this setting, we increased the dimension of each layer for a fair comparison in terms of the number of parameters.
We used the Universal Transformer base setting in \newcite{dehghani2019}.



\paragraph{Results}
Table \ref{tab:exp_main_result} shows BLEU scores on newstest2010-2016 for each method.
We trained three models with different random seeds, and reported the averaged scores.
In addition, Table \ref{tab:exp_main_result} shows the total number of parameters and computational speeds\footnote{We regard processed tokens per second during the training as the conputational speed.}.
The computational speeds are based on the speed of Original.


In the comparison between Universal and Original, Universal achieved better scores although their parameter sizes are almost the same.
This result is consistent with the report in \newcite{dehghani2019}.
Moreover, Universal outperformed Original (big) in the averaged score even though the parameter size of Universal is much smaller than the one of Original (big).
On the other hand, the proposed strategies (\textsc{sequence}, \textsc{cycle}, and \textsc{cycle (rev)}) were faster and achieved slightly better scores than Universal when we set  and .
Since Admin did not have positive influence on the BLEU scores as shown in Table \ref{tab:exp_main_result}, our strategies caused the improvements.
Thus, our proposed parameter sharing strategies are more efficient in terms of the parameter size and computational time.


In  and , \textsc{sequence}, \textsc{cycle}, and \textsc{cycle (rev)} achieved almost the same scores.
In contrast, the scores of \textsc{sequence} were lower than other two strategies in  and .
This result indicates that \textsc{cycle} and \textsc{cycle (rev)} are better strategies when we construct a deep Transformer  with small .
In  and , \textsc{cycle (rev)} improved 0.41 from Universal in the averaged BLEU score even though their computational speeds were almost the same.


\subsection{High Resource Setting}
\label{sec:exp_high_resource}

\paragraph{Datasets}
In the high resource setting, we constructed 44.2M translation sentence pairs as a training dataset with the procedures of \newcite{kiyono-etal-2020-tohoku} which achieved the best result in the WMT 2020 news translation task.
In addition, we augmented the training data by using the back-translation technique~\cite{sennrich:2016:backtrans} in the same manner as \newcite{kiyono-etal-2020-tohoku}.
We obtained 284.3M pairs as the synthetic training data.


\paragraph{Methods}
We used the original Transformer (big) setting~\cite{NIPS2017_7181} as our baseline in using genuine training data.
For the proposed strategies, we used  and .

In using both of the genuine and synthetic (back-translated) dataset, we applied \textsc{cycle (rev)} to the \textsc{Base} setting in \newcite{kiyono-etal-2020-tohoku} because \textsc{cycle (rev)} achieved the best BLEU scores on most test sets in Table \ref{tab:exp_main_result}.
We also used  and  in this configuration.
We compare the reported score of the best model described in \newcite{kiyono-etal-2020-tohoku}.
Their model is composed of 9 layers ( and , in other words), and thus, it contains much more parameters than ours.


\paragraph{Results}
Table \ref{tab:exp_wmt2020_result} shows BLEU scores on newstest2014, 2018, and 2019.
We focused on these test data to compare \newcite{kiyono-etal-2020-tohoku}.
In the same as experiments in Section \ref{sec:exp_standard_setting}, we reported the averaged scores of three morels trained with different random seeds.
Table \ref{tab:exp_wmt2020_result} also shows the total number of parameters\footnote{The parameter sizes of Original (big) in Tables \ref{tab:exp_main_result} and \ref{tab:exp_wmt2020_result} are different from each other due to the difference of sharing embeddings. Following \newcite{kiyono-etal-2020-tohoku}, we did not share embeddings in the high resource setting.}.

Table \ref{tab:exp_wmt2020_result} shows that our proposed strategies achieved better BLEU scores than Original (big) when we prepared the same number of parameters.
This result indicates that our proposed strategies are also useful in the high resource setting.


When we used additional synthetic data for the training in the same manner as \newcite{kiyono-etal-2020-tohoku}, \textsc{cycle (rev)} achieved comparable BLEU scores to the best system of \newcite{kiyono-etal-2020-tohoku} except for newstest2019\footnote{For newstest2019, synthetic data might harm the quality of a model because models trained with only genuine data outperformed models trained with both data.} even though the parameter size of \textsc{cycle (rev)} were smaller than theirs.
This result indicates that \textsc{cycle (rev)} is also efficient in the construction of models for the recent competitive tasks.
In addition, this result implies that our proposed strategies can be used in the configuration where we train many parameters with a tremendous amount of data such as recent pre-trained language models, e.g., GPT series~\cite{NEURIPS2020_1457c0d6}.


\section{Conclusion}
We proposed three parameter sharing strategies: \textsc{sequence}, \textsc{cycle}, and \textsc{cycle (rev)}, for internal layers in Transformers.
In contrast to the previous strategy, which prepares parameters for only one layer and shares them across layers such as Universal Transformers~\cite{dehghani2019}, our proposed strategies prepare parameters for  layers to construct  layers.
Experimental results show that the proposed strategies achieved comparable BLEU scores to ones of Universal with small computational time when we prepare almost the same parameters for each method.
In other words, the proposed strategies are efficient in the parameter size and computational time.
Moreover, \textsc{cycle} and \textsc{cycle (rev)} are appropriate to the construction of a deep model.
Through experiments on the high resource setting, we demonstrated that \textsc{cycle (rev)} can achieve the comparable scores to the top system of WMT 2020 news translation task~\cite{kiyono-etal-2020-tohoku} with smaller parameters.





\begin{thebibliography}{16}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei}]{NEURIPS2020_1457c0d6}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
  Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
  Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
  Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
  and Dario Amodei. 2020.
\newblock Language models are few-shot learners.
\newblock In \emph{Advances in Neural Information Processing Systems 33
  (NeurIPS)}, pages 1877--1901.

\bibitem[{Dabre and Fujita(2019)}]{Dabre_Fujita_2019}
Raj Dabre and Atsushi Fujita. 2019.
\newblock Recurrent stacking of layers for compact neural machine translation
  models.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  33:6292--6299.

\bibitem[{Dehghani et~al.(2019)Dehghani, Gouws, Vinyals, Uszkoreit, and Łukasz
  Kaiser}]{dehghani2019}
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz
  Kaiser. 2019.
\newblock Universal transformers.
\newblock In \emph{Proceedings of the 7th International Conference on Learning
  Representations (ICLR)}.

\bibitem[{Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova}]{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies (NAACL-HLT)}, pages 4171--4186.

\bibitem[{Graves(2017)}]{graves2017adaptive}
Alex Graves. 2017.
\newblock \href {http://arxiv.org/abs/1603.08983} {Adaptive computation time
  for recurrent neural networks}.

\bibitem[{Kiyono et~al.(2020)Kiyono, Ito, Konno, Morishita, and
  Suzuki}]{kiyono-etal-2020-tohoku}
Shun Kiyono, Takumi Ito, Ryuto Konno, Makoto Morishita, and Jun Suzuki. 2020.
\newblock Tohoku-{AIP}-{NTT} at {WMT} 2020 news translation task.
\newblock In \emph{Proceedings of the Fifth Conference on Machine Translation
  (WMT)}, pages 145--155.

\bibitem[{Lan et~al.(2020)Lan, Chen, Goodman, Gimpel, Sharma, and
  Soricut}]{lan2020}
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
  Radu Soricut. 2020.
\newblock {ALBERT}: A lite bert for self-supervised learning of language
  representations.
\newblock In \emph{Proceedings of the 8th International Conference on Learning
  Representations (ICLR)}.

\bibitem[{Liu et~al.(2020)Liu, Liu, Gao, Chen, and
  Han}]{liu-etal-2020-understanding}
Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. 2020.
\newblock Understanding the difficulty of training transformers.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 5747--5763.

\bibitem[{Nguyen and Salazar(2019)}]{iwslt-2019-transformer}
Toan~Q. Nguyen and Julian Salazar. 2019.
\newblock Transformers without tears: Improving the normalization of
  self-attention.
\newblock In \emph{Proceedings of the 16th International Conference on Spoken
  Language Translation (IWSLT)}.

\bibitem[{Ott et~al.(2018)Ott, Edunov, Grangier, and
  Auli}]{ott-etal-2018-scaling}
Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. 2018.
\newblock Scaling neural machine translation.
\newblock In \emph{Proceedings of the Third Conference on Machine Translation
  (WMT)}, pages 1--9.

\bibitem[{Post(2018)}]{post-2018-call}
Matt Post. 2018.
\newblock A call for clarity in reporting {BLEU} scores.
\newblock In \emph{Proceedings of the Third Conference on Machine Translation
  (WMT)}, pages 186--191.

\bibitem[{Sennrich et~al.(2016{\natexlab{a}})Sennrich, Haddow, and
  Birch}]{sennrich:2016:backtrans}
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016{\natexlab{a}}.
\newblock Improving neural machine translation models with monolingual data.
\newblock In \emph{Proceedings of the 54th Annual Meeting of the Association
  for Computational Linguistics (ACL)}, pages 86--96.

\bibitem[{Sennrich et~al.(2016{\natexlab{b}})Sennrich, Haddow, and
  Birch}]{sennrich-etal-2016-neural}
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016{\natexlab{b}}.
\newblock Neural machine translation of rare words with subword units.
\newblock In \emph{Proceedings of the 54th Annual Meeting of the Association
  for Computational Linguistics (ACL)}, pages 1715--1725.

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones,
  Gomez, Kaiser, and Polosukhin}]{NIPS2017_7181}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, \L~ukasz Kaiser, and Illia Polosukhin. 2017.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems 30
  (NIPS)}, pages 5998--6008.

\bibitem[{Wang et~al.(2019)Wang, Li, Xiao, Zhu, Li, Wong, and
  Chao}]{wang-etal-2019-learning}
Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek~F. Wong, and
  Lidia~S. Chao. 2019.
\newblock Learning deep transformer models for machine translation.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics (ACL)}, pages 1810--1822.

\bibitem[{Xiong et~al.(2020)Xiong, Yang, He, Zheng, Zheng, Xing, Zhang, Lan,
  Wang, and Liu}]{layer-normalization-icml2020}
Ruibin Xiong, Yunchang Yang, Di~He, Kai Zheng, Shuxin Zheng, Chen Xing,
  Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. 2020.
\newblock On layer normalization in the transformer architecture.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning (ICML)}.

\end{thebibliography}
 \bibliographystyle{acl_natbib}

\end{document}
