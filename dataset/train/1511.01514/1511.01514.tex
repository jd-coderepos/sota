\subsection{Simulation Framework}\label{sec:simulation_framework}

The main challenge in evaluating a scheme such as the one we propose is to create
a reliable client-server connection model. One important obstacle is the scarcity of
authentic traces of real-world connections (as this data is
privacy-sensitive). However, the framework that we developed and that we describe in this section
emulates realistic connections on a worldwide scale. To be precise, the traffic
of 112 countries (for which sufficient statistical data was available) is
simulated. Only a fraction of clients execute the given gossip protocol. This
proportion of clients is referred to as the \emph{gossip factor}, denoted .
The MMD is set to 2 hours, as for the first pilot logs run by Google and
exactly one new STH is produced by the log every MMD. In each country, the number
of Internet users was estimated with the total population~\cite{population2014} and
the percentage of individuals using the Internet~\cite{internet-usage2012}.

In order to determine how many connections users perform during the day and how
these connections are distributed among users, we used a private 24-hour-long trace of
real HTTP/HTTPS traffic from 2014 provided directly by \emph{SWITCH}
(major manager of networks among universities and research facilities in Switzerland).
This data contains more than 104 million entries for HTTP and more than 74 million
entries for HTTPS, where each entry is a triplet of the form: relative time
(in seconds), client ID (anonymized), and server ID (an\-onym\-ized).
For each hour, we approximated the parameters of a negative
binomial distribution\footnote{Another simpler model that can express the number
of events occurring in a period of time is the Poisson distribution. It relies
on a single parameter that corresponds to both the variance and the mean, but
this is not sufficient in our context since the variance
exceeds the mean to a large extent. The negative binomial is a
generalization of the Poisson distribution~\cite{cameron2013regression} that
allows the variance to be different from the mean.}
by using maximum-likelihood estimation.
Random numbers of connections can then be generated and used in the simulation.
As different types of traffic are generated during different periods
of the day (for each country), we also take time zones into consideration.

Then, it must be determined to which websites these users connect. Amazon's Alexa
Web Information Service provides a vast quantity of precious information in this
regard. In particular, we collected data (in June 2014) about the top 100 domains for each of
the 112 countries. This includes the number of page views per million, i.e., the
number of page views that a particular site generates among one million pages
that are viewed by typical users. Based on this distribution, a random domain can be
picked. We have also taken into consideration the
possibility that a client connects to a domain outside of the
top 100 by reducing the total number of connections that a client perform
proportionally.

We scanned the servers of all these top domains to determine not only if they support
HTTPS (e.g., by verifying that the port 443 is open), but also if their certificate is valid,
if a connection can be established, and if they automatically redirect clients
to HTTPS. Globally, this condition is met by 287 out of 5107 servers scanned
(i.e., around 5.62\%). In our simulation, all these servers (and only these) are
both CT-enabled and gossiping, unless stated otherwise. This assumption is both realistic, because
major websites (that already enforce the usage of TLS) are more likely to adopt new security
mechanisms rapidly; and sufficient, because these sites generate a substantial
part of the global Internet traffic (about 33.5\% of the page views of a country
on average).
The validity period of all certificates is set to 24 months (moderately below
the maximum of 39 months imposed by the CA/B Forum~\cite{CABForum} for certificates
issued after 1 April 2015). For each certificate, the date of issuance is chosen
uniformly at random in the 24 months preceding the start of the simulation.

\subsection{Results}\label{sec:simulation_results}

We show simulation results in four distinct cases. The first basic situation
in which no gossip protocol is used can be subdivided into two cases: when clients keep
track of the SCTs they received and verified, and when they do not. We 
compare these cases with the two protocols presented in \S\ref{sec:protocols}. All
results were collected during a simulation of 365 days, and the
gossip factor was set to 0.1\% (2,462,216 gossiping clients), unless stated otherwise.
Results are usually rounded to the nearest hundredth, except when more precision is needed.
The storage limit  was set to 10,000 messages, but, as the storage requirements of
our protocols are low, this limit was never reached in the 365 days period.

\myparagraph{STH distribution.}
Since one of our strategies consists of keeping STHs with the
largest tree size, it is worth investigating how effective our protocols are in spreading
recent STHs. Figure~\ref{fig:sth_distribution} illustrates how the last 12 STHs (i.e.,
those that have been generated in the last 24 hours, since the MMD is set to 2) are distributed
as a percentage of clients having them stored at the end of an MMD.
We can observe that the two non-gossiping cases form a lower
and an upper bound for the distribution of the latest STH. Indeed, when SCTs
are saved, after some time, clients do not need to contact the log very often as they tend
to visit the same domains most of the time. The distribution is almost
uniform, because clients can keep the same STH as long as they do not connect to a website
they had never visited before. In this case, more than 78\% of the clients hold an STH 
that is more than one-day old. On the other hand, when SCTs are not saved, the same
information (including the latest STH) must be fetched from the log every time an HTTPS
connection is established with a CT-enabled server. Under these circumstances, 
only less than 26\% of gossiping clients do not hold one of the last 12 STHs.
This performance cannot be exceeded by a gossip protocol that relies on existing
client-driven connections.
There might be a bias in the results coming from the fact that we used a 24-hour trace of
HTTP(S) traffic. This trace does not necessarily contain some clients that establish connections
less often (e.g. once a month). The lack of longer authentic traces prevents us from modelling
the system more accurately; nevertheless, the presented results are relevant and undoubtedly
helpful to analyze the protocols.

\begin{figure}
\centering
\includegraphics[width=\columnwidth]{sth-distribution}
\caption{Average distribution of the last 12 STHs among clients at the end of an
MMD.  On the horizontal axis,  represents the last STH generated by the log,
 represents the STH that was generated 1 MMD before that, and so on.}
\label{fig:sth_distribution}
\end{figure}

Although distribution measurements are not directly useful to demonstrate how well
a protocol performs in detecting attacks, they can show how much clients collaborate to obtain
their vision of the log when combined with another metric. Indeed, there are
only two ways for clients to get a recent STH: contacting the log or receiving an appropriate
gossip message. If the distribution is close to the upper bound and the number of STHs
fetched from the log is
low, it means that the protocol achieves an efficient communication of log-related information
between clients, and attacks are more likely to be detected rapidly. The number of STH
queries~(\texttt{getSTH()} function) per MMD was measured to be about 7.02 million when SCTs
are not saved (one query for each HTTPS connection), and only about 10,000 when SCTs are saved
(this concerns both gossip protocols).

\myparagraph{Overhead.}
Another decisive characteristic that must be analyzed is the overhead that
gossip protocols introduce.
More precisely, we express the overhead as the number of log connections
strictly generated by the gossip protocol (not by the standard CT framework, i.e., we do not
consider queries for the latest STH and the audit proof, but only queries for consistency
proofs that are needed upon receipt of certain gossip messages) over the total number of
HTTPS connections performed by gossiping clients.
The average number of HTTPS connections per MMD was measured to be approximately 7.02 million
in all cases, and the overhead was about 8.58\% (602,263.2 log connections per MMD)
for the STH-only protocol and only about 0.0058\% (407.58 log connections) for
the second protocol, when proofs are gossiped. This constitutes a substantial overhead  reduction and shows, therefore, that gossiping consistency proofs is indeed pertinent.
In the first protocol, exactly half of the gossip-related queries to the log originate
from clients and the other half from servers, since they need to request consistency proofs
under the same conditions, i.e., when the STHs they exchange have a different tree size.
In contrast, when the second protocol is used, more queries (72.09\%) come from clients.

\myparagraph{Storage.}
For both gossip protocols, the storage required on the client-side (strictly for
gossiping) is fixed: a few kilobytes for, at most, a consistency proof and the pair of
corresponding STHs. Moreover, the average number of SCTs stored by clients was measured to be
about 11--12 (recall that only one hundred websites are considered for each country).
Considering that SCTs are only a few hundred bytes, this storage is almost negligible compared
to other data that browsers usually store: history, cookies, cached media files, and so on.
On the server-side, the second protocol needs to store gossip messages. The size of the map used
to store these messages is monotonically increasing and the maximum number of messages that
can be stored at a certain point in time is related to the number of different STHs that have
been generated by the log. We observed that the largest map contained 4380 messages at the end
of the 365 days of simulation. As an example, a limit of 10,000 messages would require less than
16 MB of storage while being able to hold more than two years worth of log data.

\myparagraph{Scalability.}
Table~\ref{tab:scalability} shows how the protocols
behave when we vary the gossip factor . We also compare the situation in which HTTPS servers
in the top 100 domains are gossiping (as before) with the situation in which only the global top domain (google.com) is gossiping while HTTPS servers are still CT-enabled. We observe that both protocols perform better, in terms of distribution of recent STHs, when more clients or more servers participate and that the overhead of the second protocol gets even smaller when the gossip factor is increased. It is worth noting that even when only the global top domain (google.com) is gossiping, the protocol achieves a performance close to the results we presented before (with many more servers). This is explained by the fact that google.com is responsible for an important part of the total number of page views in all countries. Moreover, we remark that, although the distribution results are very similar
for both protocols, the overhead is substantially lower when proofs are gossiped.
Figure~\ref{fig:scalability} shows how the overhead evolves when we change the number
of gossiping servers. The second protocol not only generates a much lower overhead
than the first one, but we see that it scales well, as the overhead increases
steadily but slowly when more servers participate to the protocol.

\begin{table*}
\centering
\small
\renewcommand{\arraystretch}{0.95}

\begin{tabular}{@{}rm{60pt}rrrcrrr@{}}
\toprule
&\multirow{2}{*}{\vspace{-0.35em}Gossiping} & \multicolumn{3}{c}{Global top domain} & \phantom{abc} & \multicolumn{3}{c}{Top 100 domains for each country} \\
\cmidrule{3-5} \cmidrule{7-9}
&&  &  &  &&  &  &  \\
\cmidrule{2-9}
\multirow{3}{*}{\begin{turn}{90}Prot. 1\end{turn}} & Latest STH & 6.06 \% & 8.9 \% & 9.03 \% && 6.07 \% & 9.86 \% & 11.13 \% \\
& Last 12 STHs & 47.67 \% & 66.88 \% & 66.79 \% && 54.18 \% & 72.1 \% & 73.98 \% \\
& Overhead & 6.71 \% & 6.77 \% & 6.77 \% && 9.09 \% & 8.64 \% & 8.58 \% \\
\cmidrule{2-9}
\multirow{3}{*}{\begin{turn}{90}Prot. 2\end{turn}} & Latest STH & 5.75 \% & 7.73 \% & 8.6 \% && 6.88 \% & 8.8 \% & 9.45 \% \\
& Last 12 STHs & 51.87 \% & 63.23 \% & 64.48 \% && 53.83 \% & 69.01 \% & 70.01 \% \\
& Overhead &  \% &  \% &  \% &&  \% &  \% &  \% \\
\bottomrule
\end{tabular}

\smallskip{}
\caption{Average number of gossiping clients with the latest STH or any of the last 12 STHs, average overhead (as defined before), and storage usage defined as the average number of messages stored by servers over the number of different STHs generated by the log, for both gossip protocols. Different gossip factors  and different sets of gossiping servers are considered.}
\label{tab:scalability}
\end{table*}

\begin{figure}
\centering
\includegraphics[width=\columnwidth]{scal}
\caption{Overhead in function of the number of gossiping HTTPS servers in the top  domains of each country (the gossip factor is fixed to 0.01\%), for both gossip protocols.}
\label{fig:scalability}
\end{figure}
