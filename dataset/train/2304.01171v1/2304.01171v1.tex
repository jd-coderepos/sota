\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{subfig}
\usepackage{tabularx}
\usepackage{multirow}

\usepackage{bm}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

 \newcolumntype{Y}{>{\centering\arraybackslash}X}
 \newcolumntype{Z}{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}X}
 \newcolumntype{L}{>{\RaggedRight\hangafter=1\hangindent=0em}X}



\iccvfinalcopy 

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{Rethinking Context Aggregation in Natural Image Matting}

\author{Qinglin Liu\textsuperscript{1} ,
Shengping Zhang\textsuperscript{1}\thanks{denotes corresponding author. } ,
\ Quanling Meng\textsuperscript{1} ,
Ru Li\textsuperscript{1} ,
Bineng Zhong\textsuperscript{2} ,
Liqiang Nie\textsuperscript{1} \\
\textsuperscript{1}Harbin Institute of Technology \qquad \textsuperscript{2}Guangxi Normal University
}

\maketitle
\begin{abstract}
For natural image matting, context information plays a crucial role in estimating alpha mattes especially when it is challenging to distinguish foreground from its background.
Exiting deep learning-based methods exploit specifically designed context aggregation modules to refine encoder features. 
However, the effectiveness of these modules has not been thoroughly explored. 
In this paper, we conduct extensive experiments to reveal that the context aggregation modules are actually not as effective as expected. 
We also demonstrate that when learned on large image patches, basic encoder-decoder networks with a larger receptive field can effectively aggregate context to achieve better performance.
Upon the above findings, we propose a simple yet effective matting network, named AEMatter, which enlarges the receptive field by incorporating an appearance-enhanced axis-wise learning block into the encoder and adopting a hybrid-transformer decoder. 
Experimental results on four datasets demonstrate that our AEMatter significantly outperforms state-of-the-art matting methods (e.g., on the Adobe Composition-1K dataset, \textbf{25\%} and \textbf{40\%} reduction in terms of SAD and MSE, respectively, compared against MatteFormer). 
The code and model are available at \url{https://github.com/QLYoo/AEMatter}.



\end{abstract}




\begin{figure}[!t]
    \begin{center}
\subfloat[Basic Encoder-Decoder Matting Network]{
\label{fig:basicm}
\includegraphics[width=0.99\linewidth]{figure/d1.pdf}}
\vspace{-1pt}
\subfloat[Pooling-based Context Aggregation Module]{
\label{fig:pla}
\includegraphics[width=0.99\linewidth]{figure/d2.pdf}}
\vspace{-1pt}
\subfloat[Affinity-based Context Aggregation Module]{
\label{fig:afa}
\includegraphics[width=0.99\linewidth]{figure/d3.pdf}}
\vspace{-1pt}
    \end{center}
    \caption{Illustration of a basic matting network and context aggregation modules. (a) The basic matting network uses an encoder to extract context features from inputs, and a decoder to predict alpha mattes. Our AEMatter also follows this scheme. (b) The pooling-based context aggregation module uses average pooling to aggregate contexts from  surrounding regions. (c) The affinity-based context aggregation module uses masked correlation to aggregate contexts from globally related regions.}
\label{fig:overs}
\end{figure}




\section{Introduction}
\label{sec:intro}
Natural image matting aims to estimate the alpha matte of the foreground in a given image for composition.
This technology has numerous real-world applications, such as image editing~\cite{2009Sketch2Photo,2017Robust} and film post-production~\cite{2015Integrated,Wang_2021_ICCV}. Formally, a given image $\bm{I}$ can be represented as a combination of a foreground image $\bm{F}$ and background image $\bm{B}$ as
\begin{equation}
\label{sca}
{{I}}_i = {\alpha}_i {F}_i + (1-{ \alpha}_i ){B}_i 
\end{equation}
where ${\alpha}_i$ is the alpha matte of the foreground image at pixel $i$.
Since the foreground image $\bm{F}$, background image $\bm{B}$, and alpha matte $\bm{\alpha}$ are all unknown, the image matting problem is highly ill-defined.
To tackle this challenging problem, an auxiliary trimap input has been introduced to provide additional information that specifies the known foreground regions, known background regions, and unknown regions in the given image. 
Existing trimap-based matting methods can be broadly categorized into three categories: sampling-based methods, propagation-based methods, and deep learning-based methods.



Sampling-based methods~\cite{berman1998method,ruzon2000alpha,chuang2001a,wang2007optimized,gastal2010shared} and propagation-based methods~\cite{grady2005random,levin2008a,levin2008spectral,he2010fast,chen2013knn} estimate alpha mattes by sampling foreground and background colors based on location and color similarity or propagating color information in a local region, respectively. 
However, these traditional methods struggle with scenarios where foreground and background color distributions overlap in local regions.
Deep learning-based methods~\cite{xu2017deep,lu2019indices} use an encoder to extract context features from the input and then 
estimate the alpha matte through a decoder, as shown in Figure~\ref{fig:overs}(a). 
Due to the powerful representation ability of the learned context features,  these methods significantly outperform traditional sampling-based and propagation-based methods.
Recently, some attempts focus on refining context features to further improve performance in challenging scenarios ~\cite{cai2019disentangled,li2020natural,forte2020fbamatting,yu2020high,liu2021lfpnet,park2022matteformer}.
These studies usually adopt specifically designed pooling-based or affinity-based context aggregation modules, as illustrated in Figures~\ref{fig:overs}(b) and \ref{fig:overs}(c).
Despite the excellent performance achieved by these matting networks, the effectiveness of the context aggregation modules has not been fully explored. 



In this paper, we first conduct extensive experiments and find that the context aggregation modules do not significantly improve the performance,  which reveals the limited effectiveness of these modules.
Furthermore, we evaluate basic encoder-decoder matting networks with various backbones and find that these networks can learn from large training image patches to aggregate context.
We also observe that a matting network with a larger receptive field can achieve better performance.
Based on these findings, we propose a simple yet effective matting network without relying on any context aggregation modules, named AEMatter,  which enlarges the receptive field by incorporating an appearance-enhanced axis-wise learning block into the encoder and adopting a hybrid-transformer decoder. 
Extensive experimental results on four datasets, namely Adobe Composition-1K~\cite{xu2017deep}, Distinctions-646~\cite{2020Attention},  Transparent-460~\cite{cai2022TransMatting}, and Semantic Image Matting~\cite{sun2021sim} demonstrate that the proposed AEMatter significantly outperforms state-of-the-art matting methods by a large margin. 



To summarize, the contributions of this paper are as follows:



\begin{itemize}
    \item We present the first empirical analysis of the context aggregation modules in matting networks, and reveal their limited effectiveness.
Additionally, we demonstrate that basic encoder-decoder networks can learn from large image patches to effectively aggregate context and achieve high performance.

    \item  We propose a simple yet effective matting network, named AEMatter, which enlarges the receptive field by incorporating an appearance-enhanced axis-wise learning block into the encoder and adopting a hybrid-transformer decoder.
    
    \item   Extensive experimental results demonstrate that our AEMatter significantly outperforms state-of-the-art methods.
    Taking the Adobe Composition-1K dataset as an example, AEMatter achieves \textbf{25\%} and \textbf{40\%} reduction in terms of SAD and MSE, respectively, compared against MatteFormer.
\end{itemize}

\section{Related Work}
\noindent \textbf{Sampling-based methods.}
Sampling-based methods~\cite{berman1998method,ruzon2000alpha,wang2007optimized,gastal2010shared,he2011a,shahrian2013improving} samples candidate foreground and background colors for pixels in the unknown regions for alpha matte estimation.
To improve the robustness, researchers have made a lot of improvements to the sampling strategy.
Berman \emph{et al.}~\cite{berman1998method} propose to estimate the alpha matte with the known pixels sampled around unknown pixels as candidate foreground and background colors.
Bayesian Matting~\cite{chuang2001a} models the foreground and background color with a Gaussian distribution, and introduces spatial location information to help improve accuracy.
Global Matting~\cite{he2011a} proposes to sample the pixels in all known regions to avoid information loss and improve the robustness.




\noindent \textbf{Propagation-based methods.}
Propagation-based methods \cite{sun2004poisson,grady2005random,levin2008a,levin2008spectral,he2010fast,chen2013knn,li2013motion,aksoy2017designing} use the assumption that the foreground and background colors are smooth in the local regions to estimate the alpha matte.
To facilitate inference and improve speed, researchers have constructed many optimized object functions for estimating the alpha matte.
Poisson Matting~\cite{sun2004poisson} uses the boundary information from the given trimap to solve the Poisson equation, which helps to estimate the alpha matte with rough trimap.
Close-form matting~\cite{levin2008a} introduces a color-line assumption and provides a closed-form solution for alpha matte estimation.
He \emph{et al.}~\cite{he2010fast} use a large kernel of the Laplace matrix to reduce the number of iterations, which speeds up the inference.



\noindent \textbf{Deep learning-based methods.}
Deep learning-based methods~\cite{xu2017deep,2018AlphaGAN,li2020natural,forte2020fbamatting,sun2021sim,cai2019disentangled,tang2019very,2021Prior,wang2021ImprovingDeepImageMatting,dai2021learning,2020Context} train the networks on image matting datasets to estimate the alpha matte.
Early methods~\cite{xu2017deep,lu2019indices} typically employ a basic encoder-decoder network and modify the network structure to improve performance. 
DIM~\cite{xu2017deep} introduced a refinement module to the decoder to improve the performance.
IndexNet~\cite{lu2019indices} retains the indices of the downsampled features for upsampling features, which improves the gradient accuracy of the predicted alpha matte.
Recent advancements in deep image matting methods have designed pooling-based or affinity-based context aggregation modules to refine context features and adopt other techniques to improve performance. 
Pooling-based methods~\cite{forte2020fbamatting,yu2020mask,sun2021sim,liu2021lfpnet,park2022matteformer,cai2022TransMatting} use average pooling to aggregate contexts from surrounding regions for context feature refinement.
FBAMatting~\cite{forte2020fbamatting} adopts pyramid pooling module (PPM)~\cite{Zhao2016Pyramid} and introduces the groupnorm~\cite{wu2018group} and weight standardization~\cite{weightstandardization} tricks to improve the matting performance.
SIM~\cite{sun2021sim} adopts Atrous Spatial Pyramid Pooling (ASPP)~\cite{chen2018deeplab} and uses semantic segmentation of the image to help to predict alpha mattes.
MGMatting~\cite{yu2020mask} adopts ASPP and designs a progressive refinement decoder to estimate fine alpha mattes from coarse segmentation.
LFPNet~\cite{liu2021lfpnet} proposes a center-surround pyramid pooling to aggregate long-range contexts to help estimate the alpha matte.
MatteFormer~\cite{park2022matteformer} proposes a trimap-guided token pooling module and adopts the Swin-Tiny~\cite{liu2021Swin} backbone to improve the prediction.
TransMatting~\cite{cai2022TransMatting} proposes a global-pooling guided fusion module to improve the prediction for transparent objects.
Affinity-based methods~\cite{li2020natural,yu2020high,Yu_2021_ICCV,dai2022boosting} use the masked correlation to construct an affinity matrix and enhance the context features with the contexts from globally related regions. 
GCAMatting~\cite{li2020natural} adopts the guided context attention module to improve the prediction in the transparent region.
HDMatt~\cite{yu2020high} proposes a cross-patch feature association module to implement patch-based alpha matte estimation.
TIMI-Net~\cite{Liu_2021_ICCV} proposes a tripartite information module and multi-branches architecture to improve the estimated alpha matte.
RMat~\cite{dai2022boosting} proposes a low-level feature assembling module and the Segformer~\cite{xie2021segformer} backbone as well as strong data augmentation to improve the robustness.



\section{Empirical Study}
We aim to assess the context aggregation mechanism of matting networks from the perspective of the network architecture.
To this end, we conduct diagnostic experiments to analyze the effectiveness of the context aggregation modules.
Furthermore, we compare basic encoder-decoder networks with state-of-the-art networks to understand the impact of network design on performance.


\subsection{Exploring Context Aggregation Module}
To advance the understanding of the context aggregation mechanism in matting networks, 
we conduct diagnostic experiments on existing matting networks under multi-configuration and analyze the results.

\noindent \textbf{Patch-based Inference.}
Existing matting networks utilize context aggregation modules to refine context features by aggregating the contexts in large regions, which helps to improve the estimation.
In this regard, a large amount of context information contributes to the refinement of context features in the local regions, ultimately improving the overall performance of the matting networks.
Existing matting networks process whole images during inference, which preserves all context information. 
However, when image patches are extracted from these images, the context information is lost, leading to a decline in the performance of the matting methods.
In light of this,  we evaluate the effectiveness of context aggregation modules in existing matting methods through patch-based inference experiments.

\begin{figure}[!t]
    \begin{center}
    \includegraphics[width=0.495\linewidth]{figure/infsad.pdf}
    \includegraphics[width=0.495\linewidth]{figure/infmse.pdf}
    \end{center}
    \caption{  \textbf{Inference Patch Size} vs \textbf{Prediction Errors}. As the inference patch size increases, the prediction errors of the compared matting methods~\cite{lu2019indices,li2020natural,Liu_2021_ICCV,forte2020fbamatting,park2022matteformer} first decrease and then remain flat. }
\label{fig:pvse}
\end{figure}



We evaluate existing matting methods, including IndexNet~\cite{lu2019indices} without a context aggregation module and GCAMatting~\cite{li2020natural}, TIMI-Net~\cite{Liu_2021_ICCV}, FBAMatting~\cite{forte2020fbamatting}, and MatteFormer~\cite{park2022matteformer} with a context aggregation module.
The evaluation was conducted on image patches of varying sizes, ranging from $256 \times 256$, $512 \times 512$, $768 \times 768$, and $1024 \times 1024$, and on the whole images. 
The results are summarized in Figure~\ref{fig:pvse}, and show a decrease in errors for all methods as the patch size increases until stabilization. 
Remarkably, the matting networks with context aggregation modules do not result in a significant alteration in the performance trend when compared to IndexNet without such modules, which indicates that these modules have limited effectiveness to utilize the context information during inference and do not significantly improve performance.


\noindent \textbf{Patch-based Training.}
Context aggregation modules are also designed to aggregate context information to refine the context features for the decoders during the training stage, ultimately improving matting performance. 
However, existing methods use cropped image patches for training, which leads to limited context features extracted by the encoders. 
As a result, the matting networks with context aggregation modules only learn to predict alpha mattes with partially refined context features, which may not achieve optimal performance when applied to the whole image.
In contrast, the matting networks without context aggregation modules make predictions based solely on the locally extracted context features, which may not be affected by patch-based training. 
Hence, we hypothesize that a network with a context aggregation module demonstrates improved performance trends when trained on large image patches compared to a network without such modules. 
To validate this hypothesis, we evaluate existing matting networks with and without context aggregation modules that are trained on image patches of different sizes. 


\begin{table}[!t]
  \centering
  \caption{Comparison of state-of-the-art matting methods~\cite{lu2019indices,forte2020fbamatting,park2022matteformer} trained on Adobe Composition-1K dataset using image patches of different sizes. }
\resizebox{1\linewidth}{!}{
    \begin{tabular}{l|c|c|c|c|c}
    \toprule
    Method & Patch Size & SAD   & MSE   & Grad  & Conn \\
    \midrule
    IndexNet~\cite{lu2019indices} & 256 & 38.52  & 8.74  & 18.02  & 36.43  \\
    IndexNet~\cite{lu2019indices} & 512 & 33.64  & 7.05  & 14.35  & 30.21 \\
        IndexNet~\cite{lu2019indices} & 768 & 31.12  & 6.40  & 12.83  & 27.63  \\
            IndexNet~\cite{lu2019indices} & 1024 & 30.91 & 6.73  & 13.72  & 27.17  \\
    \midrule
    FBAMatting~\cite{forte2020fbamatting} & 256 & 43.18  & 10.41   & 21.13  &42.39 \\
    FBAMatting~\cite{forte2020fbamatting} & 512 & 33.36  & 7.26   & 15.75  & 29.84 \\
    FBAMatting~\cite{forte2020fbamatting} & 768 & 29.89  & 5.73   & 14.05  & 26.18 \\
    FBAMatting~\cite{forte2020fbamatting} & 1024 & 30.76  & 5.74   & 15.19  & 27.03 \\
    \midrule
    MatteFormer~\cite{park2022matteformer} & 256 & 28.52  & 5.51  & 12.00  & 24.06 \\
    MatteFormer~\cite{park2022matteformer} & 512 & 23.61  & 3.78  & 9.23  &18.52 \\
        MatteFormer~\cite{park2022matteformer} & 768 &22.78  & 3.59  & 8.38  & 17.50 \\
            MatteFormer~\cite{park2022matteformer} & 1024 &23.68 & 3.62  &8.81  & 18.66 \\
    \bottomrule
    \end{tabular}
    }
  \label{tab:res}\end{table}


We evaluate IndexNet without a context aggregation module and FBAMatting and MatteFormer with a context aggregation module. 
All compared methods are first trained on image patches with sizes of $256 \times 256$, $512 \times 512$, $768 \times 768$, and $1024 \times 1024$, and then evaluated on the validation set. 
Note that, we do not introduce the auxiliary task of predicting foreground and background when training FBAMatting.
The results are summarized in Table~\ref{tab:res}. 
We can observe that the performance of all methods exhibits an upward trend followed by stability as the training data size increases. 
In particular, all compared methods show very little performance improvement when the size of the training data achieves $1024 \times 1024$, and the trends of the networks with and without context aggregation modules are similar. 
These results contradict our hypothesis that the context aggregation module could enhance network performance by utilizing contexts across large regions.
Therefore, the context aggregation modules have little effect on the training of the matting networks, as the networks without these modules also achieve good performance with large training image patches.


\subsection{Exploring Basic Matting Network}
\label{sec:33}
Based on the above experiments, we hypothesize that basic encoder-decoder networks can learn from large training image patches to aggregate context for alpha matte estimation. 
To validate the hypothesis, we construct several basic encoder-decoder matting networks using the backbones employed by existing matting methods~\cite{lu2019indices,li2020natural,dai2021learning,forte2020fbamatting,park2022matteformer} and compare them with current state-of-the-art matting methods. 
Specifically, we adopt MobileNet~\cite{sandler2018mobilenetv2}, ResNet-34~\cite{he2016deep}, ResNet-50~\cite{he2016deep}, and Swin-Tiny~\cite{liu2021Swin} to construct basic matting networks without any context aggregation modules.
Note that, we simply adopt IndexNet as the MobileNet based basic matting network.
Then, we follow the training pipeline of GCAMatting to train the basic matting networks on image patches with the size of $1024 \times 1024$.
Finally, we compare these basic networks with state-of-the-art networks including, IndexNet, GCAMatting, FBAMatting, A2UNet~\cite{dai2021learning}, TIMI-Net, FBAMAtting, LSAMatting~\cite{lsam}, and MatteFormer.
As the results summarized in Table~\ref{tab:bks} show, our basic networks (referred to as BasicNet in the table)  significantly outperform the state-of-the-art networks that adopt the same backbones. 
Furthermore, the Swin-Tiny based network outperforms the MobileNet, ResNet-34, and ResNet-50 based networks and achieves a new state-of-the-art performance, which indicates that a network that has a larger receptive field can better learn context aggregation to achieve higher performance.
In the appendix, we provide more detailed analysis of the effect of training patch size and receptive field on the performance, and compare the robustness of the basic networks and state-of-the-art networks to coarse trimaps.


\begin{table}[!t]
  \centering
  \caption{Comparison of the basic matting networks with state-of-the-art matting methods~\cite{lu2019indices,li2020natural,dai2021learning,Liu_2021_ICCV,forte2020fbamatting,sun2021sim,lsam,park2022matteformer,cai2022TransMatting} on Adobe Composition-1K. * denotes the backbone adopts the dilated convolution trick.}
  \resizebox{1\linewidth}{!}{
    \begin{tabular}{l|c|c|c|c|c}
    \toprule
    Method & Backbone & SAD   & MSE   & Grad  & Conn \\
    \midrule
    IndexNet~\cite{lu2019indices} & MobileNet & 45.80  & 13.00  & 25.90  & 43.70  \\
    \textbf{BasicNet (Ours)}  & MobileNet & \textbf{30.91} & \textbf{6.73}  & \textbf{13.72}  & \textbf{27.17} \\
    \midrule
    GCAMatting~\cite{li2020natural} & ResNet-34 & 35.28  & 9.00  & 16.90  & 32.50  \\
    A2UNet~\cite{dai2021learning} & ResNet-34 & 32.10  & 7.80 & 16.33 & 29.00 \\
    TIMI-Net~\cite{Liu_2021_ICCV} & ResNet-34 & 29.08  & 6.00  & 11.50  & 25.36  \\
    \textbf{BasicNet (Ours)} & ResNet-34 & \textbf{28.08} & \textbf{5.06}  & \textbf{11.39} & \textbf{24.32} \\
    \midrule
        SIM~\cite{sun2021sim}   & ResNet-50* & 28.00    & 5.80   & 10.8  & 24.80 \\
    FBAMatting~\cite{forte2020fbamatting} & ResNet-50* & 26.40  & 5.40   & 10.6  & 21.50 \\
    LSAMatting~\cite{lsam}& ResNet-50 & 25.90    & 5.40   &9.25& 21.50 \\
\textbf{BasicNet (Ours)} & ResNet-50 &\textbf{23.82} & \textbf{4.27}  & \textbf{8.08}   & \textbf{19.02} \\
    \midrule
        Transmatting~\cite{cai2022TransMatting} & Swin-Tiny &26.83 &5.22& 10.62& 22.14  \\
    MatteFormer~\cite{park2022matteformer} & Swin-Tiny & 23.80  & 4.03  & 8.68  & 18.90 \\
    \textbf{BasicNet (Ours)} & Swin-Tiny & \textbf{19.72}  & \textbf{2.97}  & \textbf{6.27}  & \textbf{14.43} \\
    \bottomrule
    \end{tabular}}
  \label{tab:bks}\end{table}

\begin{figure*}[!t]
    \begin{center}
    \includegraphics[width=0.99\linewidth]{figure/aearch4.pdf}
    \end{center}
    \vspace{-0.4cm}
    \caption{Overview of AEMatter. 
AEMatter adopts a basic encoder-decoder architecture.
The encoder first adopts a stem-enhanced Swin-Tiny backbone with an appearance-enhanced axis-wise learning (AEAL) block to extract context features.
The decoder then adopts a hybrid-Transformer structure to refine the context features and estimate the alpha matte. }
    \label{fig:arch}
\end{figure*}


\subsection{Experimental Findings}
Based on the experimental results presented above, we have three main findings:
(1). Our experiments reveal that the context aggregation module has limited impacts on both the inference and training phases of matting networks, which indicates that these modules are not necessary. 
(2). We demonstrate that basic encoder-decoder networks without aggregation modules can outperform state-of-the-art matting methods, which shows the ability of these basic networks to learn to aggregate context from large training image patches.
(3). We find that the matting networks built on the Swin-Tiny backbone exhibit high performance, which highlights the importance of a network with a large receptive field for image matting.
These findings motivate us to design a simple yet effective network.




\section{Proposed Method}
As illustrated in Figure~\ref{fig:arch}(a), our AEMatter adopts a basic encoder-decoder architecture. 
The encoder adopts a stem-enhanced backbone with an appearance-enhanced axis-wise learning (AEAL) block to extract context features. 
The decoder adopts a hybrid-Transformer to refine the context features and estimate the alpha matte.

\subsection{Encoder}
To extract low-level features and context features from the inputs and enlarge the receptive field, we adopt a stem-enhanced backbone with an appearance-enhanced axis-wise learning (AEAL) block to construct the encoder.

\subsubsection{Stem-Enhanced Backbone}
Although the Swin-Tiny~\cite{liu2021Swin} based matting network performs best in the above experiments, Swin-Tiny is primarily  designed for high-level semantic tasks and ignores extracting low-level features, which limits its effectiveness in image matting. 
Prior studies~\cite{park2022matteformer,dai2022boosting} address this issue by incorporating additional shortcut modules to extract low-level features, but their backbones cannot utilize the shortcut features, resulting in subpar performance. 
In contrast, we replace the patch-embedding stem with convolution blocks to extract rich low-level features.
The structure of the convolution block is illustrated in Figure~\ref{fig:arch}(b).
To preserve the image details, we omit the normalization layers in the stem as they cause internal covariate shift, which hurts the matting performance. 
To overcome the gradient vanishing issue resulting from the absence of normalization, we incorporate PReLU~\cite{he2015delving} as the activation function, which introduces learnable negative  slopes to facilitate network training.
Afterward, we use the Swin blocks of Swin-Tiny to extract high-level context features.


\subsubsection{Appearance-Enhanced Axis-Wise Learning}
The Swin-Tiny based backbone adopts the hierarchical structure that is effective in capturing and integrating context features within nearby spatial regions.
However, its limited receptive field limits the performance of the matting network. 
While one possible solution to this issue is to employ more downsampling layers and Swin blocks to extract context features across larger regions, such an approach can hinder the training and increase the risk of overfitting.
To address this issue, we incorporate an appearance-enhanced axis-wise learning  (AEAL) block after the Swin-Tiny backbone, which leverages an appearance-enhanced (AE) block to facilitate training and axis-wise attention to further enlarge the receptive fields.


The structure of AEAL block is illustrated in Figure~\ref{fig:arch}(d).
To mitigate high computational overheads incurred by the high-dimension context features from the backbone, we use residual blocks and $1 \times 1$ convolutions to produce the compressed the context features $\bm{F}_c$ from the fourth-stage features $\bm{F}_4$ of the backbone.
Additionally, we use $\bm{F}_4$ to guide the extraction of appearance features from third-stage features $\bm{F}_3$ of the backbone with convolution and residual blocks, generating the context-guided appearance features $\bm{F}_a$. 
Subsequently, we employ three cascaded learning modules to process $\bm{F}_c$ and $\bm{F}_a$. 
To facilitate network training, we first introduce an AE block to generate the appearance-enhanced context features $\bm{F}_{ac}$ with $\bm{F}_c$ and $\bm{F}_a$ as 
\begin{equation}
\label{eq:alpha}
\begin{aligned}
\bm{F}_{ac} =\bm{F}_c + {\rm Conv(Res( Conv(Cat(}\bm{F}_c, \bm{F}_a))))
\end{aligned}
\end{equation}
where $\rm Cat(\cdot,\cdot)$, $\rm Conv(\cdot)$, and $\rm Res(\cdot)$ denote the concatenation, $1 \times 1$ convolution, residual block, respectively.
To capture context features over large regions, we propose axis-wise attention, which involves dividing $\bm{F}_{ac}$ into axis-wise rectangular regions and applying multi-head self-attention.
Specifically, we first zero-pad $\bm{F}_{ac}$ to a size that is an integer multiple of width $W$, then split the padded feature $\bm{F}_{acp}$ into two features, $\bm{F}_{acpx}$ and $\bm{F}_{acpy}$, along the channel dimension. 
These two features are then further divided into two sets of axis-wise features ${\bm{F}_{cx}^1, \bm{F}_{cx}^2, \dots, \bm{F}_{cx}^{n_x}}$ and ${\bm{F}_{cy}^1, \bm{F}_{cy}^2,\dots, \bm{F}_{cy}^{n_y}}$, respectively. 
Next, we apply multi-head self-attention to process the features within each set, which are re-assembled to form the refined context feature $\bm{F}_{rc}$.
Finally, we adopt the MLP network as the FPN for feature transformation, following the vanilla Transformer~\cite{vaswani2017attention}. 
 
\subsection{Decoder}
To further enlarge the receptive field of AEMatter and improve the alpha matte estimation, we adopt a hybrid-Transformer decoder that employs Swin blocks which have a large receptive field to refine the context features from the encoder and convolution blocks for predicting. 
Specifically, we first concatenate the refined context feature $\bm{F}_{rc}$ with the fourth-stage features $\bm{F}_4$ from the encoder, and apply Swin blocks  to generate the initial decoder feature $\bm{F}_d$. 
We then upsample $\bm{F}_d$ and concatenate it with the features of the corresponding scale of the encoder, and apply another Swin blocks  for feature refinement. 
This process is repeated three times to obtain the refined decoder features $\bm{F}_{rd}$.
To fuse the image details for alpha matte estimation, we upsample $\bm{F}_{rd}$ and concatenate it with the low-level features extracted by the stem of the encoder, and process it using convolution blocks that omit the normalization layers to avoid internal covariate shift. 
We perform this process twice and then use a $3 \times 3$ convolution to predict the alpha matte $\bm{\alpha}$. 
Finally, we clip the predicted alpha matte $\bm{\alpha}$ to the range of 0 to 1 using the clamp operation.



\subsection{Loss Functions}
To train the proposed AEMatter, we follow existing methods~\cite{xu2017deep,forte2020fbamatting} to construct the network loss $\mathcal{L}_{\alpha}$ as 
\begin{equation}
\begin{aligned}
\mathcal{L}_{\alpha} = \mathcal{L}_{l1} + \mathcal{L}_{cb} + \mathcal{L}_{lap}
\end{aligned}
\end{equation}
where  $\mathcal{L}_{l1}$,  $\mathcal{L}_{cb}$, and $\mathcal{L}_{lap}$  are the L1  loss,  Charbonnier L1 loss,  and Laplacian loss, which are defined as
\begin{gather}
\mathcal{L}_{l1} =   \left |\bm{\alpha}-\bm{\alpha}^{gt}\right|\\
\mathcal{L}_{cb} = \frac{1}{|\mathcal{T}^U|}\sum_{{i\in\mathcal{T}^U}}{\sqrt{(\alpha_i-\alpha_i^{gt})^2+\epsilon^2}}\\
\mathcal{L}_{lap} =  \sum_{j}{2^{j}\left |{\rm L}_j(\bm{\alpha})-{\rm L}_j(\bm{\alpha}^{gt})\right|}
\end{gather}
where $\bm{\alpha}$ and $\bm{\alpha}^{gt}$ are the predicted alpha matte and ground truth alpha matte of the input image $\bm{I}$, respectively. 
$\mathcal{T}^U$ denotes  a set of indices of the unknown pixels in the trimap.
$\epsilon$ is the Charbonnier penalty coefficient. 
${\rm L}_j(\bm{\alpha})$ and ${\rm L}_j(\bm{\alpha}^{gt})$ are the $j$-th level of the Laplacian pyramid representations of $\bm{\alpha}$ and $\bm{\alpha}^{gt}$, respectively.


\begin{figure*}[th]
\includegraphics[width=1\linewidth]{figure/adbresall.pdf}
  \vspace{-0.5cm}
	\caption{Qualitative comparison of the alpha matte results on the Adobe Composition-1K dataset.  }
	\label{fig:aim}
 \vspace{-0.2cm}
\end{figure*}


\section{Experiments}
In this section, we present a comprehensive evaluation of the proposed AEMatter. 
First, we provide the implementation details of AEMatter.
Then, we compare the performance and  generalization ability of our AEMatter with state-of-the-art matting methods on four popular datasets, namely Adobe Composition-1K~\cite{xu2017deep}, Distinctions-646~\cite{2020Attention},  Transparent-460~\cite{cai2022TransMatting}, and Semantic Image Matting~\cite{sun2021sim}.
Finally, we conduct ablation studies to verify the effectiveness of the network components of AEMatter.
Note that the appendix includes more qualitative results and model complexity analysis.

\subsection{Implementation Details}

The proposed AEMatter is implemented using the PyTorch~\cite{NEURIPS2019_9015} framework.
Axis-wise attention with a width of $W=5$ is used in the implementation. 
The coefficients in the loss functions are set as $\epsilon = 10^{-6}$, and $j = 4$.
The network weights are initialized using the Kaiming initializer~\cite{he2015delving}. 
To avoid overfitting, the backbone weights are initialized with the weights pre-trained on the ImageNet~\cite{deng2009imagenet} dataset.
The training is conducted on the Adobe Composition-1K dataset~\cite{xu2017deep}, using an NVIDIA RTX 3090 GPU with a batch size of 2 for 100 epochs. 
The RAdam optimizer~\cite{liu2019radam} is employed to optimize the network weights with weight decay of $10^{-6}$ and betas of $(0.5, 0.999)$.
The initial learning rate is set to $2.5 \times 10^{-5}$ and decays to zero using a cosine annealing scheduler.
Data augmentation techniques, including random affine transformation, random saturation transformation, random grayscale transformation, random gamma transformation, random contrast transformation, and random composition are applied to the training data.
The trimap is generated from the alpha matte ground truth using erosion and dilation with kernel sizes ranging from 1 to 30 pixels. 
To facilitate network training, the image and trimap are randomly cropped into patches of size $1024 \times 1024$ and fed to the network.




\begin{table}[!t]
  \centering
  \caption{Quantitative results on Adobe Composition-1K.
  }
\begin{tabular}{l|c|c|c|c}
        \toprule
    Method & {SAD} & {MSE} & {Grad} & {Conn} \\
    \midrule
    DIM~\cite{xu2017deep} &50.40  & 17.00  & 36.70  & 55.30  \\
    IndexNet~\cite{lu2019indices}& 45.80  & 13.00  & 25.90  & 43.70   \\
    GCAMatting~\cite{li2020natural} & 35.28  & 9.00  & 16.90  & 32.50  \\
TIMI-Net~\cite{Liu_2021_ICCV}& 29.08  & 6.00  & 11.50  & 25.36   \\
    SIM~\cite{sun2021sim}  &27.70  & 5.60  & 10.70  & 24.40  \\
    FBAMatting~\cite{forte2020fbamatting} & 26.40  & 5.40  & 10.60  & 21.50  \\
    LSAMatting~\cite{lsam} & 25.90  & 5.40  & 9.25  & 21.50  \\
        TransMatting~\cite{cai2022TransMatting}& 24.96 &4.58& 9.72& 20.16  \\
    LFPNet~\cite{liu2021lfpnet} & 23.60  & 4.10  & 8.40  & 18.50    \\
    MatteFormer~\cite{park2022matteformer}& 23.80  & 4.03  & 8.68  &18.90  \\
    RMat~\cite{dai2022boosting} & 22.87  & 3.90  & 7.74  &17.84  \\
        \midrule
    AEMatter (Ours)  &\bf{17.79}&\bf{2.39}&\bf{4.81}&\bf{12.64} \\
        \bottomrule
    \end{tabular}\label{tab:adb}\end{table}


\subsection{Results on Adobe Composition-1K}
We compare AEMatter with state-of-the-art matting methods, including DIM~\cite{xu2017deep}, IndexNet~\cite{lu2019indices}, GCAMatting~\cite{li2020natural}, FBAMatting~\cite{forte2020fbamatting}, SIM~\cite{sun2021sim}, TIMI-Net~\cite{Liu_2021_ICCV}, LFPNet~\cite{liu2021lfpnet}, LSAMatting~\cite{lsam}, MatteFormer~\cite{park2022matteformer}, and RMat~\cite{dai2022boosting} on the Adobe Composition-1K dataset. 
Table~\ref{tab:adb} and Figure~\ref{fig:aim} summarize the quantitative and qualitative results of all compared matting methods. 
Our AEMatter significantly outperforms state-of-the-art methods with its exceptional performance in terms of SAD, MSE, Grad, and Conn metrics. 
Moreover, AEMatter produces visually appealing alpha mattes, especially in regions where the foreground and background colors are very similar. 


\subsection{Generalization on Various Datasets}
We evaluate the generalization ability of DIM, IndexNet, GCAMatting, FBAMatting, LFPNet, MatteFormer, and our proposed AEMatter on the Distinctions-646, Transparent-460, and Semantic Image Matting datasets.
\begin{table}[!t]
  \centering
  \caption{Generalization results on the Distinction-646. All methods are trained on Adobe Composition-1K.} \begin{tabularx}{\linewidth}{l|Z|Z|Z|Z}
        \toprule
    Method & {SAD} & {MSE} & {Grad} & {Conn} \\
    \midrule
    DIM~\cite{xu2017deep} &63.88 & 25.77 & 53.23 & 66.31 \\ IndexNet~\cite{lu2019indices} & 44.93 &9.23 & 41.30 &44.86\\ TIMI-Net~\cite{Liu_2021_ICCV} & 42.61 & 7.75& 45.05 & 42.40 \\ GCAMatting~\cite{li2020natural} & 36.37 &8.19 & 32.34 &36.00\\FBAMatting~\cite{forte2020fbamatting} & 32.28 &5.66 & 25.52 & 32.39 \\LFPNet~\cite{liu2021lfpnet} & 22.36 & 3.41& 14.92 & 20.50 \\ Matteformer~\cite{park2022matteformer} & 23.60 & 3.12 & 13.56 & 21.56 \\ \midrule
    AEMatter (Ours)   &\bf{18.97}&\bf{2.17}&\bf{10.05}&\bf{17.02}   \\  
    \bottomrule
    \end{tabularx}\vspace{-0.2cm}
  \label{tab:d646}\end{table}\begin{table}[!t]
  \centering
  {
  \caption{Generalization results on Transparent-460. All methods are trained on Adobe Composition-1K.}
    \begin{tabular}{l|c|c|c|c}
    \toprule
    Method & {SAD} & {MSE} & {Grad} & {Conn} \\
    \midrule
    DIM~\cite{xu2017deep} &356.20& 49.68& 146.46 &296.31\\
    IndexNet~\cite{lu2019indices} & 434.14 &74.73 &124.98& 368.48\\
    TIMI-Net~\cite{Liu_2021_ICCV} & 328.08& 44.20 &142.11& 289.79\\
    MGMatting~\cite{yu2020mask} &344.65 &57.25 &74.54& 282.79\\
    TransMatting~\cite{cai2022TransMatting} & 192.36& 20.96& 41.80 &158.37\\
\midrule
    AEMatter (Ours)    & \bf{125.73}& \bf{7.13}& \bf{35.61}& \bf{115.13} \\ 
    \bottomrule
    \end{tabular}\label{tab:460}}
\end{table}All compared methods are pre-trained on Adobe Composition-1K and the trimaps for Distinctions-646 and Semantic Image Matting are generated using morphological operations.
We present the quantitative results in Tables~\ref{tab:d646}, \ref{tab:460}, and \ref{tab:simd}.
Additionally, we provide the qualitative results in the appendix.
The results clearly demonstrate that AEMatter outperforms state-of-the-art methods, which indicates its exceptional generalization ability.



\begin{table}[!t]
  \centering
  \caption{Generalization results on Semantic Image Matting. All methods are trained on Adobe Composition-1K.} \begin{tabular}{l|c|c|c|c}
        \toprule
    Method & {SAD} & {MSE} & {Grad} & {Conn} \\
    \midrule
    DIM~\cite{xu2017deep} & 95.96&54.25&29.84&100.65 \\ IndexNet~\cite{lu2019indices} & 66.89&25.75&22.07&67.61\\ GCAMatting~\cite{li2020natural} &51.84&19.46&24.16&51.98\\  FBAMatting~\cite{forte2020fbamatting} & 26.87 &5.61 & 9.17 & 22.87 \\TIMI-Net~\cite{Liu_2021_ICCV} & 54.08& 16.59 &18.91& 53.79\\ LFPNet~\cite{liu2021lfpnet} & 23.05&4.28&23.30&18.19 \\ Matteformer~\cite{park2022matteformer} & 23.90&4.73&7.72&19.01 \\\midrule
    AEMatter (Ours)   &\bf{19.80}&\bf{2.89}&\bf{4.72}&\bf{14.89} \\ \bottomrule
    \end{tabular}\vspace{-0.2cm}
  \label{tab:simd}\end{table}


\subsection{Results on Real-world Images}
To comprehensively assess the performance of AEMatter in real-world scenarios, we conduct a comparative study with state-of-the-art MatteFormer on real-world images. 
We first collect high-resolution real-world images from the internet and manually annotate them with trimaps.
Then, we qualitatively evaluate AEMatter and MatteFormer on these data. 
The results, as shown in Figure~\ref{fig:real}, demonstrate that AEMatter achieves significantly higher prediction accuracy than MatteFormer, highlighting its immense potential for practical applications, such as image editing.


\subsection{Ablation Study}

\noindent \textbf{Hybrid-Transformer Decoder.}
We introduce a hybrid-transformer decoder to enlarge the receptive field of the decoder. 
To evaluate the effectiveness of this design, we evaluate AEMatter with convolution, residual block, and hybrid-transformer decoders and summarize the results in Table~\ref{tab:abs}. 
Experimental results demonstrate that our hybrid-transformer decoder outperforms the other two designs.

\begin{figure}[!t]
	\includegraphics[width=1\linewidth]{figure/realres.pdf}
  \vspace{-0.2cm}
	\caption{Qualitative comparison of the alpha matte results on real-world images.  }
 \vspace{-0.2cm}
	\label{fig:real}
\end{figure}


\begin{table}[!t]
  \centering
  \caption{Ablation study on the decoder architecture and additional learning blocks. Decoder denotes the decoder adopted, AL denotes the additional learning block adopted, and AE denotes whether the appearance-enhanced block is used. The additional learning blocks considered are Vanilla, Window, and Axis, representing vanilla self-attention, window attention, and our axis-wise attention, respectively. }
    \resizebox{1\linewidth}{!}{
    \begin{tabular}{l|c|c|c|c|c|c}
    \toprule
    Decoder&AL&AE & {SAD} & {MSE} & {Grad} & {Conn} \\
    \midrule
    Convolution&- &×&19.57 &2.76& 5.84 &14.36\\
    Residual& -&×& 19.23 &2.82 &5.73& 14.09\\
    Hybrid-Trans& -&×& 18.91 &2.66 &5.13& 13.87\\
    Hybrid-Trans&Vanilla&×& 19.07& 2.65 &5.86& 13.92\\
    Hybrid-Trans&Window& ×&18.30 &2.61 &5.56& 13.11\\
    Hybrid-Trans&Axis& ×&18.09&2.51&5.04&13.05\\ 
    Hybrid-Trans&Axis& \checkmark&\bf{17.79} &\bf{2.39} &\bf{4.81} &\bf{12.64}\\
    \bottomrule
    \end{tabular}}
    \vspace{-0.2cm}
  \label{tab:abs}\end{table}

\noindent \textbf{Appearance-Enhanced Axis-Wise Learning.}
We introduce an AEAL block to enlarge the receptive field of the encoder, which adopts AE blocks to enhance the appearance information of context features and axis-wise attention to learn large-scale contexts. 
To evaluate the effectiveness of these designs, we evaluate AEMatter, AEMatter without AE blocks, AEMatter with the vanilla self-attention~\cite{vaswani2017attention} or the window attention~\cite{liu2021Swin} on Adobe Composition-1K, as summarized in Table~\ref{tab:abs}. 
Experimental results demonstrate that AEMatter with axis-wise attention outperforms AEMatter with the vanilla self-attention and
window attention, and that the AE block further improves performance, highlighting the effectiveness of the proposed AEAL block. 

\section{Conclusion}
In this paper, we present a comprehensive study on context aggregation modules of matting networks.
We conduct extensive experiments and find that the context aggregation modules do not significantly improve the performance, which reveals the limited effectiveness of these modules.
We also demonstrate that when learned on large image patches, basic encoder-decoder networks with a larger receptive field
can effectively aggregate context to achieve better performance.
Based on these findings, we propose a new matting network, named AEMatter, which utilizes an appearance-enhanced encoder and a hybrid-transformer decoder to improve the matting performance by enlarging the receptive field.
Extensive experimental results on four datasets demonstrate our AEMatter significantly outperforms state-of-the-art matting methods.

\setcounter{figure}{0}    
\setcounter{table}{0}    
\appendix
\section*{Appendix}
The appendix complements the main text by presenting additional analysis and qualitative results.
We begin by providing more analysis on basic encoder-decoder matting networks in Section~\ref{sec:cc3}.  
Then,  we provide the model complexity analysis of the proposed AEMatter in Section~\ref{sec:cc1}. 
Finally, we provide more qualitative results of AEMatter and the compared methods in Section~\ref{sec:cc4}. 

\section{Analysis on Basic Matting Networks}
\label{sec:cc3}
In this section, we present a comprehensive analysis of the basic encoder-decoder matting networks. 
First, we investigate the impact of varying  training image patch sizes on the performance of the basic matting networks.
Second, we evaluate the effect of varying receptive field sizes on the performance of the basic matting networks.
Third, we compare the robustness of the basic matting networks with state-of-the-art matting networks to coarse trimaps.

\subsection{Analysis on Training Image Patch Sizes}
In the main text, we explore the impact of varying the training image patch sizes on the performance of existing matting networks.
We hypothesize that using larger image patches during training helps the basic network to learn better context aggregation and thus achieve higher performance without relying on context aggregation modules. 
To validate this hypothesis, we train the Resnet-34~\cite{he2016deep} based and Swin-Tiny~\cite{liu2021Swin} based basic matting networks on image patches of different sizes, including $256 \times 256$, $512 \times 512$, $768 \times 768$, and $1024 \times 1024$, and evaluate them. 
The results, summarized in Table~\ref{tab:drr}, show that the performance of the network improves with larger training image patches, which supports our hypothesis.
To further investigate the impact of the sizes of training image patches on network performance, we follow Luo et al.~\cite{luo2016understanding} to visualize the effective receptive field of the ResNet-34 based networks that are trained on image patches of different sizes using gradient feedback in Figure~\ref{fig:rf}. 
The visualization also confirms that the networks that are trained on larger image patches learn better context aggregation.

\begin{table}[!t]
  \centering
  \caption{Diagnostic experiment on the size of training image patches. }
     \resizebox{\linewidth}{!}{
    \begin{tabular}{l|c|cccc}
    \toprule
    Backbone & Patch Size & SAD   & MSE   & Grad  & Conn \\
    \midrule
    Resnet-34 & $256$    & 41.74 & 12.51  & 22.51 & 40.14 \\
    Resnet-34 & $512$     & 33.16 & 7.08 & 15.27 & 29.80 \\
    Resnet-34 & $768$     &  27.70     & 5.41      & 11.23   & 23.89 \\
    Resnet-34 & $1024$     & 28.08 & 5.06  & 11.39 & 24.32  \\
        \midrule
    Swin-Tiny & $256$    & 27.99 & 5.30  & 11.23 & 23.96 \\
    Swin-Tiny & $512$     & 22.42& 3.72 &7.46 & 17.54 \\
    Swin-Tiny & $768$     & 20.37     & 2.96    & 6.55   & 16.89 \\
    Swin-Tiny & $1024$     & 19.72 & 2.97  & 6.27 &14.43  \\ 
    \bottomrule
    \end{tabular}}
  \label{tab:drr}\end{table}

\begin{figure}[!t]
    \begin{center}
    \includegraphics[width=1.\linewidth]{figure/erf.pdf}
       \end{center}
            \vspace{-15pt}
    \caption{ Visualization of the receptive field of basic matting networks trained on image patches of different sizes. (a) Untrained network. (b) Network trained on $256 \times 256$ image patches. (c) Network trained on $512 \times 512$ image patches. (d) Network trained on $768 \times 768$ image patches. (e) Network trained on $1024 \times 1024$ image patches. }
    \label{fig:rf}
    \vspace{-5pt}
\end{figure}


\subsection{Analysis on Receptive Field}
In the main text, we evaluate multiple basic encoder-decoder matting networks and observe that the networks that have larger receptive fields, such as Swin-Tiny~\cite{liu2021Swin} or ResNet-50~\cite{he2016deep} based networks, demonstrate better performance. 
This observation leads us to hypothesize that the performance of a network is positively correlated with its receptive field size. 
To verify this hypothesis, we compare the performance of basic matting networks with different kernel sizes.
Specifically, we build basic matting networks with ResNet-34~\cite{he2016deep} and ResNet-50 backbones. 
Then, we replace part of $3 \times 3$  convolution kernels in these networks with $1 \times 1$ convolution kernels and  $5 \times 5$  convolution kernels to control the receptive field size.
We evaluate the performance of the modified networks and summarize the results in Table~\ref{tab:arf}.
The results show that the matting network with larger convolution kernels achieves better performance, which provides empirical evidence that supports our hypothesis that networks with larger receptive fields have better performance.



\begin{table}[!t]
  \centering
  \caption{Diagnostic experiment on the kernel size. } \resizebox{\linewidth}{!}{
    \begin{tabular}{l|c|cccc}
    \toprule
    Backbone & Kernel Size & SAD   & MSE   & Grad  & Conn \\
    \midrule
    Resnet-34 & $1 \times 1$   & 31.28 & 6.14  & 13.41 & 28.05 \\
    Resnet-34 & $3 \times 3$     & 28.08 & 5.06  & 11.39 & 24.32 \\
    Resnet-34 & $5 \times 5$     &  26.72     &  4.74     &  10.08     &22.75 \\
    Resnet-50 & $1 \times 1$    & 28.70  & 5.79  & 10.96 & 24.98 \\
    Resnet-50 & $3 \times 3$     & 23.82 & 4.27  & 8.08  & 19.02 \\
    Resnet-50 & $5 \times 5$     & 23.34 & 3.92  & 7.42  & 18.89 \\
    \bottomrule
    \end{tabular}}
  \label{tab:arf}\end{table}

\begin{figure}[!t]
    \includegraphics[width=0.95\linewidth]{figure/tmse.pdf}
    \vspace{-0.1cm}
    \caption{  \textbf{Trimap Dilation Distance} vs \textbf{Prediction Error}. Note that, the networks with the same backbone are represented by lines with the same color.  As the trimap dilation distance increases, the prediction errors (MSE) of all compared matting methods increase. }
        \vspace{-0.3cm}
    \label{fig:pvse2}
\end{figure}

\subsection{Robustness to Coarse Trimap}
Recent matting research~\cite{yu2020mask,dai2022boosting} highlights the robustness of matting networks to coarse trimaps.
To evaluate whether basic matting networks can handle such trimaps, we compare the performance of the basic encoder-decoder matting networks and existing matting networks on a modified Adobe Composition-1K dataset that has trimaps of varying dilation distances. 
We compare basic matting networks based on ResNet-34, ResNet-50, and Swin-Tiny backbones with state-of-the-art matting methods such as GCAMatting, FBAMatting, and MatterFormer.
In Figure~\ref{fig:pvse2}, we present the results of our comparison study, where we refer to the basic networks as BasicNet. 
The performance of all matting networks deteriorates as the dilation distance increase.
Notably, the state-of-the-art methods with the context aggregation modules do not outperform the basic encoder-decoder network, indicating that the basic matting networks are capable of handling coarse trimaps.



\begin{table}[!t]
  \centering
  \caption{Comparison of the computational complexity and parameter amounts of image matting methods. }
    \begin{tabular}{l|c|c}
    \toprule
    Method & MACs (G) & Params (M) \\
    \midrule
    DIM~\cite{xu2017deep}   & 727.4 & 130.5  \\
    IndexNet~\cite{lu2019indices} & 116.6 & 8.2  \\
    GCAMatting~\cite{li2020natural} & 257.3 & 24.1  \\
    FBAMatting~\cite{forte2020fbamatting} & 686.0   & 34.8  \\
    SIM~\cite{sun2021sim}   & 1001.9 & 44.5  \\
    TIMI-Net~\cite{Liu_2021_ICCV}  & 351.3 & 35.0  \\
    LFPNet~\cite{liu2021lfpnet} & 1539.4 & 112.2  \\
    MatteFormer~\cite{park2022matteformer} & 233.3 & 44.9  \\
    \midrule
    AEMatter (Ours) & 295.4 & 52.0  \\
    \bottomrule
    \end{tabular}\label{tab:cs}\end{table}\section{Model Complexity}
\label{sec:cc1}
In this section, we analyze the computational complexity and parameter amounts of different image matting methods.
We measure the number of multiply-accumulates (MACs) required by each method to infer a $1024 \times 1024$ image, as well as the number of trainable parameters in the model.
The results, summarized in Table~\ref{tab:cs}, demonstrate that AEMatter has similar computational complexity and parameter amounts to other existing matting methods.



\section{Results on Image Matting Datasets}
\label{sec:cc4}
In this section, we present additional qualitative results on matting datasets. 
We summarize the comparison results on Distinction-646~\cite{qiao2020attention}, Transparent-460~
\cite{cai2022TransMatting}, and Semantic Image Matting~\cite{sun2021sim} in Figures~\ref{fig:d646}, \ref{fig:d460}, and \ref{fig:simd}, respectively.
These figures demonstrate that AEMatter achieves better performance than existing matting methods in predicting alpha mattes for object edges and transparent objects.
In addition, we randomly select the predicted alpha mattes of AEMatter on four matting datasets~\cite{xu2017deep,qiao2020attention,cai2022TransMatting,sun2021sim} and present them in Figures~\ref{fig:adbc}, \ref{fig:646c}, \ref{fig:460c}, and \ref{fig:sic}.


\begin{figure*}[!t]
\includegraphics[width=1\linewidth]{figure/d646.pdf}
	\caption{Qualitative comparison of the alpha matte results on the Distinction-646 dataset.  }
	\label{fig:d646}
\end{figure*}


\begin{figure*}[!t]
\includegraphics[width=1\linewidth]{figure/d460.pdf}
	\caption{Qualitative comparison of the alpha matte results on the Transparent-460 dataset.  }
	\label{fig:d460}
\end{figure*}

\begin{figure*}[!t]
\includegraphics[width=1\linewidth]{figure/dsi.pdf}
	\caption{Qualitative comparison of the alpha matte results on the Semantic Image Matting dataset.  }
	\label{fig:simd}
\end{figure*}

\begin{figure*}[!t]
\centering
\includegraphics[width=0.9\linewidth]{figure/adbc.pdf}
	\caption{Qualitative results on the Adobe Composition-1K dataset.  }
	\label{fig:adbc}
\end{figure*}

\begin{figure*}[!t]
\centering
\includegraphics[width=0.9\linewidth]{figure/646c.pdf}
	\caption{Qualitative results on the Distinction-646 dataset.  }
	\label{fig:646c}
\end{figure*}

\begin{figure*}[!t]
\centering
\includegraphics[width=0.9\linewidth]{figure/460c.pdf}
	\caption{Qualitative results on the Transparent-460 dataset. }
	\label{fig:460c}
\end{figure*}

\begin{figure*}[!t]
\centering
\includegraphics[width=0.9\linewidth]{figure/sic.pdf}
	\caption{Qualitative results on the Semantic Image Matting dataset.  }
	\label{fig:sic}
\end{figure*}


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
