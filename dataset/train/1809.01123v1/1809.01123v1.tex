

\section{Experimental Results}
In the following we first provide implementation details before evaluating the proposed approach on a variety of datasets using a variety of metrics.

\subsection{Implementation details, Training and Evaluation}

To obtain the features $\bx$, we found ResNet-101~\cite{HeCVPR16} as the backbone with dilated convolutions~\cite{ChenArxiv16} to perform well. More specifically, we use the representation from the top convolutional layer in the network as $\bx_t$. The feature maps have spatial resolution 8 times smaller than the input image. In the experiments, we set $K=20$, $d_c=100$, $c_1=0.95$ and $c_2=0.4$. We initialized the parameters using the model pretrained on Pascal VOC~\cite{EveringhamIJCV15,HariharanICCV11} for semantic image segmentation. We trained the entire network end-to-end using the Adam optimizer~\cite{kingma2014adam}. We set the initial learning rate to $10^{-5}$ and gradually decreases over time. The weight decay factor is $0.0005$.

To training our matching network, we use any two randomly chosen frames in a video sequence as training pairs. Importantly, the two frames are not required to be consecutive in time which provides an abundance of training data. We augmented the training data by random flipping, cropping and scaling between a factor of 0.5 to 1.5. We use Tensorflow to implement the algorithm.  Training takes around 4 hours for 1000 iterations  on an Nvidia Titan X. At test time, a forward pass with an input image of size $480\times 854$ takes around 0.17 seconds. 




\para{\bf Training:} We trained the proposed network using the 30 video sequences available in the DAVIS-16 training set~\cite{PerazziCVPR16} for 1000 iterations and evaluated on the DAVIS-16 validation set. Similarly, we used the 60 sequences in the DAVIS-17 training set~\cite{pont2017DAVIS} for training when testing on the DAVIS-17 validation set. Although the model is trained on DAVIS, we found it to generalize well to other datasets. Therefore, we use the model trained on the DAVIS-17 training set for evaluation on both the JumpCut~\cite{FanTOG15} and the YouTube-Objects~\cite{PrestCVPR12} datasets. 

\para{\bf Evaluation:}
We validate the effectiveness of our method on the DAVIS-16~\cite{PerazziCVPR16} validation, the DAVIS-17~\cite{pont2017DAVIS} validation, the JumpCut~\cite{FanTOG15} and the YouTube-Objects~\cite{PrestCVPR12} datasets. For the YouTube-Objects dataset, we use the subset with groundtruth segmentation masks provided by~\cite{JainECCV2014}, containing 126 video sequences. All of the datasets provide pixel-level groundtruth segmentation. More specifically, binary (foreground-background) ground truth is provided in the DAVIS-16, JumpCut, and YouTube-Objects datasets, while there is instance-level segmentation groundtruth available for the DAVIS-17 dataset. Challenges such as occlusion, fast motion, and appearance change are presented in the four datasets. Thus, these four datasets serve as a good test bed to evaluate different video object segmentation techniques.



\subsection{Evaluation metrics}

\para{\bf Jaccard index (mIoU):} Jaccard index is a common evaluation metric to evaluate the segmentation quality. It is calculated as the intersection over union (IoU) of the predicted and groundtruth masks. We compute the mean of the IoU across all the frames in a sequence and thus also refer to this metric as mIoU.

\para{\bf Contour accuracy (F)~\cite{PerazziCVPR16}:} To measure the quality of the predicted mask, we assess the contour accuracy by computing a bipartite matching between the contour points of the predicted segmentation and the contour points of the groundtruth segmentation. Based on the matching result we calculate the contour accuracy via the F-1 score. 


\para{\bf Error rate~\cite{FanTOG15}:} Following the evaluation protocol in~\cite{FanTOG15}, we compute the error rate on the JumpCut dataset. We select key frames $i=\{0,16,...,96\}$ in each sequence and for the $i$-th keyframe, we compute the error in the predicted segmentation of the $i+d$-th frames, given the groundtruth segmentation mask of the $i$-th frame. Intuitively, we measure the transfer (or matching) error of methods with respect to a certain transfer distance $d$. The error is equal to the number of false positive and false negative pixels (the mislabeled pixels) divided by the number of all positive pixels (all foreground pixels) in the predicted segmentation of the $i+d$-th frame. We use $d=16$ in the experiments and compute the average of the errors to obtain the error rate.

\subsection{Quantitative results}

We carefully evaluated the proposed approach and compared the proposed method with a wide variety of video object segmentation methods~\ie, MSK~\cite{PerazziCVPR17}, SFL~\cite{ChenICCV17}, OSVOS~\cite{caelles2016one}, OnAVOS~\cite{voigtlaender2017online}, PLM~\cite{yoon2017pixel}, MaskRNN~\cite{hu2017maskrnn}, Lucid~\cite{khoreva2017lucid}, SEA~\cite{AvinashCVPR14}, HVS~\cite{GrundmannCVPR2010}, JMP~\cite{FanTOG15}, FCP~\cite{PerazziICCV15}, BVS~\cite{MaerkiCVPR16}, OFL~\cite{TsaiCVPR2016}, CTN~\cite{JangCVPR17}, VPN~\cite{JampaniCVPR17}, 
SVC~\cite{WangTIP2017}, JFS~\cite{NagarajaICCV15}, LTV~\cite{OchsPAMI2014}, HBT~\cite{GodecICCV2011}, AFS~\cite{VijayanarasimhanECCV2012}, SCF~\cite{JainECCV2014}, RB~\cite{BaiSIGGRAPH2009} and DA~\cite{ZhongTOG12}. Note that MSK, OSVOS, SFL, OnAVOS, PLM, MaskRNN, Lucid employ fine-tuning during testing.

We present the quantitative results on four datasets: DAVIS-16~\cite{PerazziCVPR16}, YouTube-Objects~\cite{PrestCVPR12}, JumpCut~\cite{FanTOG15} and DAVIS-17~\cite{pont2017DAVIS}. Our method outperforms state-of-the-art methods by $0.4\%$ in mIoU and by 0.71 in error rate on Youtube-Objects and JumpCut datasets, respectively. On DAVIS-16 and DAVIS-17 datasets, our approach performs on par with state-of-the-art techniques while not using fine-tuning. The quantitative results are summarized in \tabref{tab:nofinetuning},~\ref{tab:youtube},~\ref{tab:jumpcut},~\ref{tab:davis17} and \figref{fig:davis16}. The best method is highlighted in bold and the second-best method is underlined. Details are described in the following.

\para{\bf Evaluation on the DAVIS-16 dataset:}
In~\tabref{tab:nofinetuning}, we compare our method with deep net baselines that do not require fine-tuning as well, such as VPN~\cite{JampaniCVPR17} and CTN~\cite{JangCVPR17}. We also compare to OSVOS~\cite{caelles2016one}, MSK~\cite{PerazziCVPR17}, OnAVOS~\cite{voigtlaender2017online} and SFL ~\cite{ChenICCV17}, disabling their fine-tuning step. We use the super-script `$^-$' to denote methods with a disabled fine-tuning step. In~\tabref{tab:nofinetuning}, we report the mean IoU and the average running time per frame for each method tested on the DAVIS-16 dataset. Our method achieves the best mIoU, outperforming the baselines by more than 6\% while running efficiently. Our method without the outlier removal (denoted as OURS-NU in~\tabref{tab:nofinetuning}) runs 2 times faster while achieving competitive performance.

In~\figref{fig:davis16}, we compare our method which does not require fine-tuning with baselines that may or may not need fine-tuning. We report the mIoU vs average computational time per frame in~\figref{fig:davis16}(a) and the contour accuracy vs running time per frame in~\figref{fig:davis16}(b). Note that the average running time per frame also includes the fine-tuning step for those methods requiring fine-tuning. Since the network employed in our method is general enough to learn how to match we observe competitive performance at a fraction of the time required by other techniques. Note that the time axis scaling is logarithmic.

\begin{table}[t]
	\centering
	{\small
		\caption{Comparisons with deep net methods \emph{without} fine-tuning (VPN and CTN) or with fine-tuning step disabled (denoted with $^-$) on DAVIS-16 validation set. OURS-NU: our method without online update and outlier removal.
		}
		\label{tab:nofinetuning}
		\tabcolsep=5pt
		\resizebox{0.9\linewidth}{!}{
\begin{tabular}{lcccccccc}
	\toprule
		&OURS            & OURS-NU & OSVOS$^-$ & MSK$^-$  & OnAVOS$^-$ & SFL$^-$  & VPN   & CTN    \\
	\midrule
	mIoU             & \bf 0.810            & {\ul 0.792}  & 0.525 & 0.699   & 0.736 & 0.674 & 0.702 & 0.735 \\
	Speed (s) & 0.32              & 0.17   & \bf 0.12  & {\ul 0.15}    & 3.55  & 0.3   & 0.63  & 29.95\\
	\bottomrule
\end{tabular}
		}
	}
\end{table}


 

\setlength{\figwidth}{0.495\textwidth}
\begin{figure*}[t]
\begin{center}
	   \hfill
		\begin{subfigure}[b]{\figwidth}
		\includegraphics[width=\linewidth,height=0.8\linewidth]{Figs/davis16/a.pdf} \\
		\centering (a) mIoU \vs speed
	\end{subfigure}\hfill
	\begin{subfigure}[b]{\figwidth}
		\includegraphics[width=\linewidth,height=0.8\linewidth]{Figs/davis16/b.pdf} \\
		\centering (b) F \vs speed
	\end{subfigure}\hfill
\end{center}
\caption{\tb{Performance comparison on the DAVIS-16 validation set.} The x axis denotes the average running time per frame in seconds (log scale) and the y axis is  (a) mIoU (Jaccard index) and (b) F score (contour accuracy). }
\label{fig:davis16}
\end{figure*}
 
\para{\bf Evaluation on the YouTube-Objects dataset:}
We present the evaluation results on the YouTube-Objects dataset~\cite{PrestCVPR12,JainECCV2014} in~\tabref{tab:youtube}. Our method outperforms the baselines despite the fact that our network is not fine-tuned, but other baselines such as OnAVOS and MSK and OSVOS are. Thus, our method is more favorable both in terms of computational time and in terms of accuracy.

\begin{table}[t]
	\centering
	{\footnotesize
\caption{Evaluation on the Youtube-Object dataset~\cite{PrestCVPR12,JainECCV2014} using Jaccard index (mIoU).}
		\label{tab:youtube}
		\tabcolsep=3pt
		\resizebox{\linewidth}{!}{
\begin{tabular}{lcccccccccccc}
	\toprule
	Sequence  & OURS  & OnAVOS & MSK   & OSVOS & OFL   & JFS   & BVS   & SCF   & AFS   & FST   & HBT   & LTV   \\
	\midrule
	Fine-tuned?& - & Yes & Yes & Yes & - & - & - & - & - & - & - & -\\
	\midrule
	Aeroplane & 0.880 & 0.902  & 0.816 & 0.882 & 0.899 & 0.89  & 0.868 & 0.863 & 0.799 & 0.709 & 0.736 & 0.137 \\
	Bird      & 0.873 & 0.879  & 0.829 & 0.857 & 0.842 & 0.816 & 0.809 & 0.81  & 0.784 & 0.706 & 0.561 & 0.122 \\
	Boat      & 0.805 & 0.816  & 0.747 & 0.775 & 0.74  & 0.742 & 0.651 & 0.686 & 0.601 & 0.425 & 0.578 & 0.108 \\
	Car       & 0.779 & 0.738  & 0.670 & 0.796 & 0.809 & 0.709 & 0.687 & 0.694 & 0.644 & 0.652 & 0.339 & 0.237 \\
	Cat       & 0.788 & 0.759  & 0.696 & 0.708 & 0.683 & 0.677 & 0.559 & 0.589 & 0.504 & 0.521 & 0.305 & 0.186 \\
	Cow       & 0.771 & 0.787  & 0.750 & 0.778 & 0.798 & 0.791 & 0.699 & 0.686 & 0.657 & 0.445 & 0.418 & 0.163 \\
	Dog       & 0.803 & 0.809  & 0.752 & 0.813 & 0.766 & 0.703 & 0.685 & 0.618 & 0.542 & 0.653 & 0.368 & 0.18  \\
	Horse     & 0.688 & 0.742  & 0.649 & 0.728 & 0.726 & 0.678 & 0.589 & 0.54  & 0.508 & 0.535 & 0.443 & 0.115 \\
	Motorbike & 0.774 & 0.663  & 0.498 & 0.735 & 0.737 & 0.615 & 0.605 & 0.609 & 0.583 & 0.442 & 0.489 & 0.106 \\
	Train     & 0.811 & 0.838  &  0.777 & 0.757 & 0.763 & 0.782 & 0.652 & 0.663 & 0.624 & 0.296 & 0.392 & 0.196 \\
	\midrule
	Average   & \bf{ 0.797} & {\ul 0.793}  & 0.718 & 0.783 & 0.776 & 0.74  & 0.68  & 0.676 & 0.625 & 0.538 & 0.463 & 0.155\\
	\bottomrule
\end{tabular}	
}
}
\end{table} 
\para{\bf Evaluation on the JumpCut dataset:}
We present the evaluation results on the JumpCut dataset~\cite{FanTOG15} in~\tabref{tab:jumpcut}.  We follow the evaluation in~\cite{FanTOG15} and compute the error rates of different methods. The transfer distance $d$ is equal to $16$. In this experiment we don't apply the outlier removal described in \secref{sec:OnlineUpdate} to restrict mask transfer between non-successive frames. Again, our method outperforms the baselines on this dataset with an average error rate that is 0.34 lower than the best competing baseline SVC~\cite{WangTIP2017}.
\begin{table}[t]
	\centering
	{\small
		\caption{Error rates on the JumpCut dataset~\cite{FanTOG15}. The transfer distance $d$ is $16$.
		}
		\label{tab:jumpcut}
		\tabcolsep=3pt
		\resizebox{\linewidth}{!}{
\begin{tabular}{llrrrrrrrllrrrrrrr}
	\toprule
	&               & RB  & DA  & SEA   & JMP   & SVC   & PLM   & OURS &         &            & RB  & DA  & SEA   & JMP   & SVC   & PLM   & OURS \\ \hline

	&Fine-tuned? &- & - & - & - & -& Yes & - &  &  & - & - & - & - & - & Yes & -\\
	\midrule
	ANIMAL & bear          & 4.58  & 4.48  & 4.21  & 4     & {\bf 2.11}  & {\ul 3.45}  & 5.14 & SNAPCUT & animation  & 11.9  & 6.38  & 6.78  & {\ul 4.55}  & \bf 3.35  & 5.86  & 6.15 \\
	& giraffe       & 22    & 11.2  & 17.4  & \bf 7.4   & {\ul 9.67}  & 17.4  & 11.96 &         & fish       & 51.8  & 21.7  & 25.7  & 17.5  & {\ul 7.67}  & \bf 7.42  & 12.21 \\
	& goat          & 13.1  & 13.3  & 8.22  & \bf 4.14  & 4.97  & 15.2  & {\ul 4.73} &         & horse      & 8.39  & 45.1  & 37.8  & {\ul 6.8}   & \bf 4.84  & 7.94  &  8.25\\ \cline{11-18} 
	& pig           & 9.22  & 9.85  & 10.3  & {\ul 3.43}  & \bf 3.24  & 5.15  & 5.12 &         & Avg.       & 24.03 & 24.39 & 23.43 & 9.62  & \bf 5.29  & {\ul 7.07}  & 8.87 \\ \cline{2-18} 
	& Avg.          & 12.23 & 9.71  & 10.03 &\bf 4.74  & {\ul 5.00}  & 10.30 & 6.74 & FAST    & bball      & 18.4  & 8.47  & 8.89  & \bf 3.9   & {\ul 4.16}  & 8.04  & 6.19 \\ \cline{1-9}
	HUMAN  & couple        & 17.5  & 16    & 23.4  & \bf 5.13  & {\ul 8.49}  & 9.14  & 11.77 &         & cheetah    & 31.5  & 16.6  & 7.68  & 8.16  & \bf 7.1   & 11.8  &  {\ul 7.61} \\
	& park          & 11.8  & 6.54  & 6.91  & {\ul 5.39}  & \bf 5.33  & 10.2  & 11.42 &         & dance      & 56.1  & 50.8  & 43    & 18.7  & 26.5  & \bf 14.7  & {\ul 17.31} \\
	& station       & 8.85  & 20.9  & 21.3  & 9.01  &{\ul 8.42}  & \bf 4.68  & 9.98 &         & hiphop     & 67.5  & 51.1  & 33.7  & 14.2  & 21.9  & {\ul 13.6}  & \bf 10.49 \\ \cline{2-9}
	& Avg.          & 12.72 & 14.48 & 17.20 & \bf 6.51  & {\ul 7.41}  & 8.01  & 11.06 &         & kongfu     & 40.2  & 40.8  & 17.9  & 8     & \bf 3.77  & 6.25  & {\ul 4.05} \\ \cline{1-9}
	STATIC & car           & {\bf 1.76}  & 5.93  & 5.08  & 2.26  & 2.57  & 2.18  & {\ul 1.86} &         & skater     & 38.7  & 40.8  & 29.6  & 22.8  & 21.4  &{\bf 12.6}  & {\ul 13.57} \\
	& cup           & 5.45  & 12.9  & 9.31  & \bf 2.15  & {\ul 2.4}   & 6.04  & 5.38 &         & supertramp & 129   & 60.5  & 57.4  & 42.9  & 27.1  & \bf 20.7  & {\ul 22.12} \\
	& pot           & {\ul 2.43}  & 5.03  & 2.98  & 2.95  & \bf 1.79  & 2.66  & 5.55 &         & tricking   & 79.4  & 70.9  & 35.8  & 21.3  & 21.2  & {\ul 15.7}  &  \bf 8.32 \\ \cline{11-18} 
	& toy           & \bf 1.28  & 3.19  & 2.16  & {\ul 1.3}   & 1.49  & 2.25  & 2.81 &         & Avg.       & 57.60 & 42.50 & 29.25 & 17.50 & 16.64 & {\ul 12.92} & \bf 11.21 \\ \cline{2-9}
	& Avg.          & 2.73  & 6.76  & 4.88  & {\ul 2.17}  & \bf 2.06  & 3.28  & 3.90 &         &            &       &       &       &       &       &       &      \\ 
	\midrule[1pt]
	Average & & 28.68  & 23.75 & 18.89 & 9.82 & {\ul 9.07} & 9.23 & \bf 8.73 &         &            &       &       &       &       &       &       &     \\
	\bottomrule
\end{tabular}
}
}
\end{table} 
\para{\bf Evaluation on the DAVIS-17 dataset:}
\begin{table}[t]
	\centering
	{\footnotesize
		
		\caption{Evaluation on the DAVIS-17 validation set.
		}
		\label{tab:davis17}
		\tabcolsep=2pt
\resizebox{\linewidth}{!}{
\begin{tabular}{lcccccccccc}
	\toprule
	& OURS            & OFL   & OSVOS$^-$ & OnAVOS$^-$ & MaskRNN$^-$ & OSVOS & OnAVOS & MaskRNN & OnAVOS$^+$ & OURS-FT   \\
	\midrule
	Fine-tuned? & -&-&-&-&-&Yes&Yes&Yes&Yes&Yes\\
	\midrule
	mIoU             & 0.565 & 0.549  & 0.366   & 0.395    & 0.455 & 0.521  & 0.610    & 0.605   & \bf 0.645             & {\ul 0.614} \\
	Speed (s) & {\ul 0.35}  & 130    & \bf 0.13    & 3.78     & 0.6   & 5    & 13      & 9     & 30                & 2.62 \\
	\bottomrule
\end{tabular}
}
}
\end{table} We show the experiments on instance-level video object segmentation using the DAVIS-17 validation set. The results are shown in~\tabref{tab:davis17}. Our method performs reasonably well when compared to methods without finetuning,~\ie, OSVOS$^-$, OnAVOS$^-$, MaskRNN$^-$ and OFL. We further finetune our method (denoted as OURS-FT), and the performance is competitive among the baselines while the computational time is much faster. Note that OnAVOS$^+$~\cite{DAVIS2017_5th} in~\tabref{tab:davis17} is OnAVOS with upsampling layers on top and model ensembles.



\setlength{\figwidth}{0.35\textwidth}
\begin{figure*}[t]
\begin{center}
	\begin{subfigure}[b]{\figwidth}
		\includegraphics[width=\linewidth,height=0.8\linewidth]{Figs/ablation/d.pdf} \\ 
		\centering (a) Effect of K in Top K 
	\end{subfigure} \quad\quad
	\begin{subfigure}[b]{\figwidth}
		\includegraphics[width=\linewidth,height=0.8\linewidth]{Figs/ablation/c.pdf} \\
		\centering (b) Effect of fine-tuning
	\end{subfigure}\hfill
\end{center}
\caption{\tb{Sensitivity analysis and finetuning.} (a) The effect of K when computing the Top K similarity scores  in the soft matching layer. (b) The effect of fine-tuning of our approach compared with other baselines. Both results are shown using the DAVIS-16 validation dataset.}
\label{fig:ablation}

\end{figure*}
 
\subsection{Ablation study}

We study the important components of the proposed method. Subsequently, we discuss the effect of outlier removal and online update, the effect of $K$, the effect of foreground and background matching, the effect of fine-tuning and the memory consumption of the proposed approach.

\begin{table}[t]
\centering
\caption{Ablation study of the three modules in our approach: (1)  outlier removal, (2) online background update, and (3) online foreground update, assessed on the DAVIS-16 validation set.}
\label{tab:ablation}
		\tabcolsep=7pt
\begin{tabular}{cccc}
	\toprule
Outlier removal & BG update & FG update & mIoU  \\
\midrule
-               		& -         & -         & 0.792 \\
$\checkmark$   & -         & -         & 0.805 \\
$\checkmark$   & $\checkmark$ & -         & 0.809 \\
$\checkmark$   & $\checkmark$ & $\checkmark$ & 0.810 \\
\bottomrule
\end{tabular}
\end{table} 
\para{\bf Effect of $K$:} We study the effect of $K$ in the proposed soft matching layer where we compute the average similarity scores of top $K$ matchings. We present the performance on DAVIS-16 with different settings of K in~\figref{fig:ablation}~(a). We varied $K$ to be between $1$ and $100$. The performance when $K$ is equal to $1$ (`hard matching') is $0.753$ while the performance increases when $K$ is larger than $1$ (`soft matching') until $K$ is equal to $20$. When $K$ is larger than $20$, the performance keeps decreasing and the performance of computing the average similarity scores among all matchings is $0.636$. Intuitively, a point is a good match to a region if the feature of the point is similar to a reasonable amount of pixels in that region, which motivates the proposed  soft matching layer.

\para{\bf Outlier removal and online update:} 
In~\tabref{tab:ablation}, we study the effects of outlier removal, online background feature update and foreground feature update. We found that our method with neither outlier removal nor online update performs competitively, achieving $0.792$ on DAVIS-16. Removing of outliers improves the performance by $0.013$. If we incorporate  the online background feature update, the performance improves by $0.004$ and having the foreground feature updated as well further improves the performance, achieving $0.810$ in mIoU on the DAVIS-16 dataset.



\setlength{\figwidth}{0.25\textwidth}
\begin{figure}[t]
\begin{center}
		\begin{subfigure}[b]{0.98\figwidth}\includegraphics[width=\linewidth]{Figs/qual/davis16_1/00001.jpg} \\
\includegraphics[width=\linewidth]{Figs/qual/youtube_1/00001.jpg} \\
\includegraphics[width=\linewidth]{Figs/qual/jumpcut_1/00001.jpg} \\
\includegraphics[width=\linewidth]{Figs/qual/davis17_1/00001.jpg} \\
\end{subfigure}\hfill
		\begin{subfigure}[b]{0.98\figwidth}\includegraphics[width=\linewidth]{Figs/qual/davis16_1/00014.jpg} \\
\includegraphics[width=\linewidth]{Figs/qual/youtube_1/00003.jpg} \\
\includegraphics[width=\linewidth]{Figs/qual/jumpcut_1/00003.jpg} \\
\includegraphics[width=\linewidth]{Figs/qual/davis17_1/00012.jpg} \\
\end{subfigure}\hfill
		\begin{subfigure}[b]{0.98\figwidth}\includegraphics[width=\linewidth]{Figs/qual/davis16_1/00035.jpg} \\
\includegraphics[width=\linewidth]{Figs/qual/youtube_1/00005.jpg} \\
\includegraphics[width=\linewidth]{Figs/qual/jumpcut_1/00005.jpg} \\
\includegraphics[width=\linewidth]{Figs/qual/davis17_1/00031.jpg} \\
\end{subfigure}\hfill
		\begin{subfigure}[b]{0.98\figwidth}\includegraphics[width=\linewidth]{Figs/qual/davis16_1/00049.jpg} \\
\includegraphics[width=\linewidth]{Figs/qual/youtube_1/00006.jpg} \\
\includegraphics[width=\linewidth]{Figs/qual/jumpcut_1/00007.jpg} \\
\includegraphics[width=\linewidth]{Figs/qual/davis17_1/00047.jpg} \\
\end{subfigure}
	\end{center}
\caption{\tb{Visual results of our approach}. Testing videos are from DAVIS-16 (1\nd{st} row), Youtube-Objects (2\nd{nd} row), JumpCut (3\nd{rd} row),  and DAVIS-17  datasets (4\nd{th} row).
}
	\label{fig:qual}
\setlength{\figwidth}{0.25\textwidth}
\begin{center}
		\begin{subfigure}[b]{0.98\figwidth}\includegraphics[width=\linewidth]{Figs/fail/f4.jpg} \\
		\end{subfigure}
		\begin{subfigure}[b]{0.98\figwidth}\includegraphics[width=\linewidth]{Figs/fail/f4_2.jpg} \\
		\end{subfigure}\hfill
		\begin{subfigure}[b]{0.98\figwidth}\includegraphics[width=\linewidth]{Figs/fail/f5.jpg} \\
		\end{subfigure}
		\begin{subfigure}[b]{0.98\figwidth}\includegraphics[width=\linewidth]{Figs/fail/f5_2.jpg} \\
		\end{subfigure}
	\end{center}
\caption{\tb{Failure cases of our approach.} For each case, we show the results of our approach at the beginning and toward the end of the video sequence.}
	\label{fig:failure}
\end{figure} 

\para{\bf Matching foreground and background:}
As shown in~\figref{fig:Overview}, we match the input image with not only the foreground region but also the background region in the template and thus we have two soft matching layers for computing the foreground similarity and the background similarity. We found that having both foreground and background models is important for good performance. Specifically, the performance of matching only the foreground,~\ie, only having one soft matching layer to compute foreground similarity, is only $0.527$ in mIoU on DAVIS-16 while having both foreground and background similarity computed achieves $0.792$. 

\para{\bf Online fine-tuning:} We would like to point out that the network in our method can be fune-tuned during testing when observing the groundtruth mask of the first frame. We show the trade-off between fine-tuning time and performance on DAVIS-16 in~\figref{fig:ablation}~(b). Specifically, we show the average running time per frame taking the fine-tuning step into account, and  compare with OSVOS, OSVOS-BS (OSVOS without the post-processing step), OnAVOS and OnAVOS-NA (OnAVOS without test time augmentation). We report the results of OnAVOS and OnAVOS-NA without a CRF as post-processing. Note that the time axis scaling is again logarithmic. The bottom left  point of each curve denotes performance without fine-tuning. Clearly, the performance of our approach outperforms other baselines if fine-tuning is prohibited. After fine-tuning, our method can be further improved and still runs efficiently, taking 2.5 seconds per frame while other baselines require more than 10 seconds to achieve their peak performance. Note that we don't have any post-processing step to refine the segmentation mask in our method while still achieving competitive results. 




\vspace*{-0.15cm}
\subsection{Qualitative results}

In~\figref{fig:qual}, we show  visual results of our method on DAVIS-16 (1st row), Youtube-Objects (2nd row), JumpCut (3rd row),  and DAVIS-17 datasets (4th row). We observe our method can accurately segment the foreground objects with challenges such as fast motion, cluttered background and appearance change. We also observe the proposed method produce accurate instance level segmentation on DAVIS-17 datasets.


We show the failure cases of our method in~\figref{fig:failure}. Possible reasons for our method to fail include tiny objects and similar appearance of different instances.


