\documentclass[11pt]{article}

\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{url}
\usepackage{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{graphicx,tikz}
\usepackage{amsopn,amssymb,amsthm,amsmath}
\usepackage{xspace}
\usepackage{subfigure,color}



\newcommand{\R}{\mathbb{R}}
\newcommand{\range}{\operatorname{range}}
\newcommand{\dist}{\operatorname{dist}}
\newcommand{\svd}{\operatorname{SVD}}
\newcommand{\eig}{\operatorname{eig}}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\qr}{\operatorname{qr}}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\spn}{\operatorname{span}}
\newcommand{\nnz}{\operatorname{nnz}}
\newcommand{\poly}{\operatorname{poly}}
\newcommand{\trace}{\operatorname{trace}}
\newcommand{\FD}{\operatorname{FD}}
\newcommand{\Tr}{\operatorname{Tr}}
\newcommand{\eps}{\varepsilon}
\newcommand{\rows}{\operatorname{rows}}
\newcommand{\nul}{\operatorname{null}}
\newcommand{\E}{\operatorname{E}}

\newcommand{\edo}[1]{{\color{red} {\sffamily\small Edo: }#1}}
\newcommand{\mina}[1]{{\color{blue} {\sffamily\small Mina: }#1}}
\newcommand{\fd}{\textsc{FrequentDirections}\xspace}
\newcommand{\sfd}{\textsc{SparseFrequentDirections}\xspace}
\newcommand{\SFD}{\textsc{SFD}\xspace}

\newcommand{\op}{\ensuremath{{\perp}}}
\newcommand{\s}[1]{{\small \textsf{#1}}}
\newcommand{\etal}{\emph{et al.}\xspace}
\newcommand{\si}{\textsc{SimultaneousIteration}\xspace}
\newcommand{\SSh}{\textsc{SparseShrink}\xspace}
\newcommand{\BSSh}{\textsc{BoostedSparseShrink}\xspace}
\newcommand{\DSh}{\textsc{DenseShrink}\xspace}
\newcommand{\VSSh}{\textsc{VerifiedSparseShrink}\xspace}
\newcommand{\PowM}{\textsc{PowerMethod}\xspace}
\newcommand{\vs}{\textsc{VerifySpectral}\xspace}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{property}{Property}[section]
\newtheorem{fact}{Fact}[section]
\newtheorem{definition}{Definition}[section]
\newcommand{\Paragraph}[1]{\paragraph*{\sffamily \textbf{#1}}}



\newcommand{\denselist}{\itemsep -2pt\parsep=-1pt\partopsep -2pt}
\newcommand{\todo}[1]{\textcolor{red}{[#1]}}
\newlength{\figsize} \setlength{\figsize}{0.22\textwidth}


\title{Efficient Frequent Directions Algorithm for Sparse Matrices}
\author{
Mina Ghashami\\ University of Utah \\ \texttt{ghashami@cs.utah.edu}
\and
Edo Liberty\\ Yahoo Labs\\ \texttt{edo.liberty@yahoo.com}
\and
Jeff M. Phillips\thanks{Thanks to support by NSF CCF-1350888, IIS-1251019, ACI-1443046, and CNS-1514520.}\\ University of Utah \\ \texttt{jeffp@cs.utah.edu}
}
\date\nonumber

\begin{document}

\maketitle
\begin{abstract}
This paper describes Sparse Frequent Directions, a variant of Frequent Directions for sketching sparse matrices. 
It resembles the original algorithm in many ways:
both receive the rows of an input matrix  one by one in the streaming setting and compute a small sketch .
Both share the same strong (provably optimal) asymptotic guarantees with respect to the space-accuracy tradeoff in the streaming setting. However, unlike Frequent Directions which runs in  time regardless of the sparsity of the input matrix , Sparse Frequent Directions runs in  time.
Our analysis loosens the dependence on computing the Singular Value Decomposition (SVD) as a black box within the Frequent Directions algorithm. Our bounds require recent results on the properties of fast approximate SVD computations.  
Finally, we empirically demonstrate that these asymptotic improvements are practical and significant on real and synthetic data.  
\end{abstract}

\section{Introduction}
\label{sec:intro}
It is very common to represent data in the form of a matrix. 
For example, in text analysis under the bag-of-words model, a large corpus of documents can be represented as a matrix whose rows refer to the documents and columns correspond to words. A non-zero in the matrix corresponds to a word appearing in the a document. 
Similarly, in recommendation systems~\cite{drineas2002competitive}, preferences of users are represented as a matrix with rows corresponding to users and columns corresponding to items. Non-zero entires correspond to user ratings or actions.

A large set of data analytic tasks rely on obtaining a low-rank approximation of the data matrix. 
These include clustering, dimension reduction, principal component analysis (PCA), signal denoising, etc. 
Such approximations can be computed using the Singular Value Decompositions (SVD).  
For an  matrix  () computing the SVD requires  time and  space in memory on a single machine. 
In many scenarios, however, data matrices are extremely large and computing their SVD exactly is infeasible. 
Efficient approximate solutions exist for distributed setting or when data access otherwise is limited.
In the row streaming model, the matrix rows are presented to the algorithm one by one in an arbitrary order.
The algorithm is tasked with processing the stream in one pass while being severely restricted in its memory footprint. 
At the end of the stream, the algorithm must provide a sketch matrix  which is a good approximation of  even though it is significantly more compact. This is called matrix sketching.

Matrix sketching methods are designed to be parallelizable, space and time efficient, and easily updatable.  
Computing the sketch on each machine and then combining the sketches together should be as good as sketching the combined data from all the different machines.
The streaming model is especially attractive since a sketch can be obtained and maintained as the data is being collected.
Therefore, eliminating the need for data storage altogether.

Often matrices, as above, are sparse;  most of their entries are zero. 
The work of \cite{dhillon2001concept} argues that typical term-document matrices are sparse; documents contain no more than  of all words.  
On wikipedia, most words appear on only a small constant number of pages.    
Similarly, in recommendation systems, in average a user rates or interacts with a small fraction of the available items:  
less than  in some user-movies recommendation tasks~\cite{asendorfalgorithms} and much fewer in physical purchases or online advertising.
As such, most of these datasets are stored as sparse matrices.  


There exist several techniques for producing low rank approximations of sparse matrices whose running time is 
 for some error parameter . 
Here  denotes the number of non-zeros in the matrix .
Examples include the power method~\cite{golub2012matrix}, random projection techniques~\cite{sarlos2006improved}, projection-hashing~\cite{clarkson2013low}, and instances of column selection techniques~\cite{drineas2006fast2}. 

However, for a recent and popular technique \fd (best paper of KDD 2013~\cite{liberty2013simple}), 
there is no known way to take advantage of the sparsity of the input matrix.  
While it is deterministic and its space-error bounds are known to be optimal for dense matrices in the row-update model~\cite{ghashami2015frequent}, it runs in  time to produce a sketch of size .  
In particular, it maintains a sketch with  rows and updates it iteratively over a stream, periodically invoking a full SVD which requires  time.  
Reliance on exact SVD computations seems to be the main hurdle in reducing the runtime to depend on .  
This paper shows a version of \fd whose runtime depends on . This requires a new understanding and a more careful analysis of \fd.  
It also takes advantage of block power methods (also known as Subspace Iteration, Simultaneous Iteration, or Orthogonal Iteration) that run in time proportional to  but incur small approximation error~\cite{musco2015stronger}.  

\subsection{Linear Algebra Notations}

Throughout the paper we identify an  matrix  with a set of  rows  where each  is a vector in .
The notation  stands for the th row of the matrix .  By  we mean the row vector  appended to the matrix  as its last row. 
Similarly,  stands for stacking two matrices  and  vertically.
The matrices  and  denote the -dimensional identity matrix and the full zero matrix of dimension  respectively. 
The notation  denotes the distribution over  matrices whose entries are drawn independently from the normal distribution .
For a vector  the notation  refers to the Euclidian norm .
The Frobenius norm of a matrix  is defined as , and the operator (or spectral) norm of it is 
.


The notation  refers to the number of non-zeros in , and  denotes relative density of .
The Singular Value Decomposition of a matrix  for  is denoted by .
It guarantees that , , , , ,
and  is a non-negative diagonal matrix such that  and . 
It is convenient to denote by , and  the matrices containing the first  columns of  and  and by  the top left  block of .
The matrix  is the best rank  approximation of  in the sense that . In places where we use , we mean rank  SVD of .

The notation  denotes the projection of the rows of  on the span of the rows of . In other words,  where  indicates taking the Moore-Penrose psuedoinverse.
Alternatively, setting , we have . We also denote , the right projection of  on the top  right singular vectors of . 






\section{Matrix Sketching Prior Art}
This section reviews only matrix sketching techniques that run in input sparsity time and whose output sketch is independent of the number of rows in the matrix.  We categorize all known results into three main approaches (1) column/row subset selection (2) random projection based techniques and (3) iterative sketching techniques. 



\Paragraph{Column selection techniques} 
These techniques, which are also studied under the Column Subset Selection Problem (CSSP) in literature~\cite{frieze2004fast,drineas2003pass,boutsidis2009improved,deshpande2006adaptive,drineas2011faster,boutsidis2011near}, form the sketch  by selecting a subset of ``important'' columns of the input matrix . They maintain the sparsity of  and make the sketch  to be more interpretable. These methods are not typically streaming, nor running in input sparsity time. The only method of this group which achieves both is~\cite{drineas2006fast2} by Drineas \etal that uses reservoir sampling to become streaming. 
They select  columns proportional to their squared norm and achieve the Frobenius norm error bound  with time complexity of .  
In addition, they show that the spectral norm error bound  holds if one selects  columns. Rudelson \etal~\cite{rudelson2007sampling} improved the latter error bound to  by selecting  columns, where  is the numeric rank of . Note that in the result by~\cite{drineas2006fast2}, one would need  columns to obtain the same bound.

Another similar line of work is the CUR factorization~\cite{boutsidis2014optimal,drineas2003pass,drineas2006fast,drineas2008relative,mahoney2009cur} where methods select  columns and  rows of  to form matrices ,  and , and constructs the sketch as . The only instance of this group that runs in input sparsity time is~\cite{boutsidis2014optimal} by Boutsidis and Woodruff, where they select  rows and columns of  and construct matrices  and  with  such that with constant probability . Their algorithm runs in  time.






\Paragraph{Random projection techniques} 
These techniques~\cite{papadimitriou1998latent,vempala2004random,sarlos2006improved,liberty2007randomized} operate data-obliviously and maintain a  matrix  using a  random matrix  which has the Johnson-Lindenstrauss Transform (JLT) property~\cite{matouvsek2008variants}.  
Random projection methods work in the streaming model, are computationally efficient, and sufficiently accurate in practice~\cite{desai2015improved}.
The state-of-the-art method of this approach is by Clarkson and Woodruff~\cite{clarkson2013low} which was later improved slightly in~\cite{NN13}. 
It uses a hashing matrix  with only one non-zero entry in each column.  Constructing this sketch takes only   time, and guarantees that for any unit vector  that 

For these sparsity-efficient sketches using  also guarantees that 
.    


\Paragraph{Iterative sketching techniques} 
These operate in streaming model, where random access to the matrix is not available. 
They maintain the sketch  as a linear combination of rows of , and update it as new rows are received in the stream.  
Examples of these methods include different version of iterative SVD~\cite{golub2012matrix, hall1998incremental, levey2000sequential, brand2002incremental,ross2008incremental}. These, however, do not have theoretical guarantees~\cite{desai2015improved}.   
The \fd algorithm~\cite{liberty2013simple} is a unique in this group in that it offers strong error guarantees. 
It is a deterministic sketching technique that processes rows of an  matrix  in a stream and maintains a  sketch  (for ) such that the following two error bounds hold for any 

and

Setting  and , respectively, achieves bounds 

and

Although \fd does not run in input sparsity time, we will explain it in detail in the next section, as it is an important building block for the algorithm we introduce.  

\subsection{Main Results} 
We present a randomized version of \fd, called as \sfd that receives an  sparse matrix  as a stream of its rows. 
It computes a  sketch  in  space. 

It guarantees that with probability at least  (for  being the failure probability), for  and any ,

and
 
Note that setting  yields 

and setting  yields 


The expected running time of the algorithm is
 
In the likely case where  and , the runtime is dominated by .  
We also experimentally validate this theory, demonstrating these runtime improvements on sparse data without sacrificing accuracy.  





\section{Preliminaries}

In this section we review some important properties about \fd and \si which will be necessary for understanding and proving bounds on  \sfd.  

\subsection{Frequent Directions}
\label{related:fd}

The \fd algorithm was introduced by Liberty~\cite{liberty2013simple} and received an improved analysis by Ghashami et al.\cite{ghashami2015frequent}. 
The algorithm operates by collecting several rows of the input matrix and letting the sketch grow.
Once the sketch doubles in size, a lossy \textsc{DenseShrink} operation reduces its size by a half.
This process repeats throughout the stream.
The running time of \fd and its error analysis are strongly coupled with the properties of the 
SVD used to perform the \textsc{DenseShrink} step.

An analysis of~\cite{desai2015improved} slightly generalized the one in~\cite{ghashami2015frequent}.
Let  be the sketch resulting in applying \fd with a potentially different shrink operation to . 
Then, the \fd asymptotic guarantees hold as long as the shrink operation exhibits three properties, for any positive  and a constant .
\begin{enumerate} \item Property 1:  For any vector , .
\item Property 2: For any unit vector , .
\item Property 3: .
\end{enumerate}
For completeness, the exact guarantee is stated in Lemma~\ref{lem:desai}.

\begin{lemma}[Lemma 3.1 in~\cite{desai2015improved}]\label{lem:desai}
Given an input  matrix  and an integer parameter , any sketch  matrix  which satisfies the three properties above (for some any  and ),
guarantees the following error bounds

and

where  represents the projection operator onto , the top  singular vectors of .  
\end{lemma}

Another important property of \fd is that its sketches are mergeable~\cite{ghashami2015frequent}. To clarify, consider partitioning a matrix  into  blocks  so that . Let  denotes the  sketch of the matrix block , and  denotes the  sketch of all s combined together.
It is shown that  has at most as much covariance and projection error as , i.e. the sketch of the whole matrix . It follows that this divide-sketch-and-merging can also be applied recursively on each matrix block without increasing the error.



The runtime of \fd is determined by the number of shrinking steps. 
Each of those computes an  of  which takes  time. 
Since the SVD is called only every  rows this yields a total runtime . This effectively means that on average we are spending  operations per row, even if the row is sparse. 

In the present paper, we introduce a new method called as \sfd that uses randomized SVD methods instead of the exact SVD to approximate the singular vectors and values of intermediate matrices .  We show how this new method tolerates the extra approximation error and runs in time proportional to . Moreover, since it received sparse matrix rows, it can observe more the  rows until the size of the sketch doubles.
As a remark, Ghashami and Phillips~\cite{ghashami2014relative} showed that maintaining any rescaled set of  rows of  over a stream is not a feasible approach to obtain sparsity in \fd. It was left as an open problem to produce some version of \fd that took advantage of the sparsity of .  




\subsection{Simultaneous  Iteration}
Efficiently computing the singular vectors of matrices is one of the most well studies problems in scientific computing.
Recent results give very strong approximation guarantees for block power method techniques \cite{rokhlin2009randomized}\cite{woolfe2008fast}\cite{liberty2007randomized}\cite{halko2011finding}. Several variants of this algorithm were studied under different names in the literature e.g. Simultaneous  Iteration, Subspace Iteration, or Orthogonal Iteration~\cite{golub2012matrix}. 
In this paper, we refer to this group of algorithms collectively as \si. 
A generic version of \si for rectangular matrices is described in Algorithm~\ref{alg:sim_itr}.
\begin{algorithm}[H]
\caption{\si}
\label{alg:sim_itr}
\begin{algorithmic}
\STATE \textbf{Input}: , rank , and error 
\STATE 
\STATE 
\STATE 
\STATE \textbf{return}  \hfill \# 
\end{algorithmic}
\end{algorithm}

While this algorithm was already analyzed by \cite{golub2012matrix}, the proofs of~\cite{rokhlin2009randomized, halko2011finding, musco2015stronger, witten2013randomized} manage to prove stable results that hold for any matrix independent of spectral gap issues.
Unfortunately, an in depth discussion of these algorithms and their proof techniques is beyond the scope of this paper. 

For the proof of correctness of \sfd, the main lemma proven by \cite{musco2015stronger} suffices.
\si (Algorithm~\ref{alg:sim_itr}) guarantees the three following error bounds with high probability:
\begin{enumerate}
\item Frobenius norm error bound: 

\item Spectral norm error bound: 

\item Per vector error bound: 


for all . Here  denotes the th left singular vector of , and  is the ()th singular value of , and  is the th column of the matrix  returned by \si.  
\end{enumerate}

In addition, for a constant , \si runs in  time.

In this paper, we show that \sfd can replace the computation of an exact SVD by using the results of~\cite{musco2015stronger} with  being a constant.  This alteration does give up the optimal asymptotic accuracy (matching that of \fd).


\section{Sparse Frequent Directions}
The \sfd (\SFD) algorithm is described in Algorithm \ref{alg:SparseFD}, and is an extension of \fd to sparse matrices.
It receives the rows of an input matrix  in a streaming fashion and maintains a sketch  of  rows. Initially  is empty. 
On receiving rows of , \SFD stores non-zeros in a buffer matrix . 
The buffer is deemed full when it contains  non-zeros or  rows. 
\SFD then calls \BSSh to produce its sketch matrix  of size .
Then, it updates its ongoing sketch  of the entire stream by merging it with the (dense) sketch  using \DSh.
\begin{algorithm}[H]
\caption{\sfd}
\label{alg:SparseFD}
\begin{algorithmic}
\STATE \textbf{Input:} , an integer , failure probability 
\STATE , \; 
\FOR {}
\STATE 
\IF { {\bf or} } \STATE 
\STATE 
\STATE 
\ENDIF
\ENDFOR
\STATE \textbf{return}  \end{algorithmic}
\end{algorithm}

\BSSh amplifies the success probability of another algorithm \SSh in Algorithm \ref{alg:SparseShrink}.   \SSh runs \si instead of a full SVD to take advantage of the sparsity of its input .  However, as we will discuss, by itself \SSh has too high of a probability of failure.  
Thus we use \BSSh which keeps running \SSh and probabilistically verifying the correctness of its result using \vs, until it decides that the result is correct with high enough probability. 
Each of \DSh, \SSh, and \BSSh produce sketch matrices of size .



\begin{algorithm}[H]
\begin{algorithmic}
\caption{\textsc{SparseShrink}}
\label{alg:SparseShrink}
\STATE \textbf{Input}: , an integer 

  \STATE  
  \STATE , \;  
  \STATE 
  \STATE  
\STATE \textbf{return} 
\end{algorithmic}
\end{algorithm}



\begin{algorithm}[H]
\begin{algorithmic}
\caption{\BSSh}
\label{alg:BoostedSparseShrink}
\STATE \textbf{Input}: , integer , failure probability 
\WHILE{True}
  \STATE 
  \STATE  \;\;\; for 
  \IF {}
    \STATE \textbf{return} 
  \ENDIF
\ENDWHILE
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[H]
\caption{\textsc{DenseShrink}}
\label{alg:DenseFD}
\begin{algorithmic}
\STATE \textbf{Input}: , an integer 
\STATE  \STATE \STATE 
\STATE \textbf{Return}  \end{algorithmic}
\end{algorithm}





Our main result is stated in the next theorem. It follows from combining the proofs contained in the subsections below.
\begin{theorem}[main result]\label{thm:bounds}
Given a sparse matrix  and an integer , \sfd computes a small sketch  such that with probability at least  for  and any ,

and

The total memory footprint of the algorithm is  and its expected running time is 

\end{theorem}


\subsection{Success Probability}
\label{sec:suc-prob}
\SSh, described in Algorithm \ref{alg:SparseShrink}, calls \si to approximate the top rank  subspace of .
As \si is randomized, it fails to converge to a good subspace when the initial choice of the random matrix  does not sufficiently align with the top  singular vectors of  (see Algorithm \ref{alg:sim_itr}). This occurs with probability at most .
In Section \ref{subsub:SSh}, we prove that with probability of at least  that \SSh satisfies the three properties required for Lemma \ref{lem:desai} using  and , but replacing Property  with a stronger version
\begin{itemize}
\item Property 2 (strengthened):  

\end{itemize}
where  denotes the th singular value of .




However, for the proof of \sfd we require that {\it all} \SSh runs to be successful. 
The failure probability of \SSh, which is upper bounded by , is high enough that a simple union bound would not give a meaningful bound on the failure probability of \sfd. We therefore reduce the failure probability of each \BSSh, by wrapping each call of \SSh in the verifier \vs.  If \vs does not verify the correctness, then it reruns \SSh and tries again until it can verify it.   
But to perform this verification efficiently, we need to loosen the definition of correctness.  In particular, we say \SSh is successful if the sketch  computed from its output satisfies  (the original Property 2 specification in Section \ref{related:fd}), where .  Combining the two inequalities through , a successful run implies that .  
\vs verifies the success of the algorithm by approximating the spectral norm of ; it does so by running the power method for  steps for some constant .  













\begin{algorithm}[H]
\caption{\vs}
\label{alg:verifyspectral}
\begin{algorithmic}
\STATE \textbf{Initialization} persistent  ( retains its state between invocations of this method)
\STATE \textbf{Input}: Matrix , failure probability 
\STATE  and 
\STATE Pick  uniformly at random from the unit sphere in .  
\STATE \textbf{if}  \textbf{return} \textsc{True}
\STATE \textbf{else} \textbf{return} \textsc{False}
\end{algorithmic}
\end{algorithm}
\begin{lemma} \label{lem:vs}
The  algorithm returns \textsc{True} if . 
If  it returns \textsc{False} with probability at least .
\end{lemma}
\begin{proof}
If  than .
If , consider execution  of the method. Let  denote the top singular vector of . Then  , for some constant  as long as  . Let  denote the density function of the random variable 
. Then . Setting the failure probability to be at most , we conclude that  with probability at least . 
\end{proof}

Therefore, \vs fails with probability at most   during execution . 
If any of \vs runs fail, \BSSh and hence \sfd potentially fail.
Taking the union bound over all invocations of \vs we obtain that \sfd fails with probability at most , hence it succeeds with probability at least . 








\subsection{Space Usage and Runtime Analysis}\label{sec:runtime_analysis}
Throughout this manuscript we assume the constant-word-size model. 
Integers and floating point numbers are represented by a constant number of bits. 
Random access into memory is assumed to require  time.
In this model, multiplying a sparse matrix  by a dense vector requires  operations and storing  requires  bits of memory. 

\begin{fact}
The total memory footprint of \sfd is .
\end{fact}
\begin{proof}
It is easy to verify that, except for the buffer matrix , the algorithm only manipulates  matrices; in particular, observe that the () condition in \sfd ensures that  in \SSh, and in \DSh also .
Each of these  matrices clearly require at most  bits of memory. 
The buffer matrix  contains at most  non-zeros and therefore does not increase the space complexity of the algorithm.
\end{proof}


We turn to bounding the expected runtime of \sfd which is dominated by the cumulative running times of  \DSh and \BSSh.
Denote by  the number of times they are executed. It is easy to verify . 
Since \DSh runs in  time deterministically, the total time spent by \DSh through  iterations is .

The running time of \BSSh is dominated by those of \SSh and \vs, and its expected number of iterations. 
Note that, in expectation, they are each executed on any buffer matrix  a small constant number of times because \vs succeeds with probability (much) greater than . For asymptotic analysis it is identical to assuming they are each executed once.

Note that the running time of \SSh on  is . 
Since  we obtain a total running time of .
The th execution of \vs requires  operations. 
This, because it multiplies  by a single vector  times, and both  and .
In expectation \vs is executed  times, therefore total running time of it is 




Combining the above contributions to the total running time of the algorithm we obtain Fact~\ref{lem:sparseshrink_runtime}.
\begin{fact}
\label{lem:sparseshrink_runtime}
Algorithm \sfd runs in expected time of 

\end{fact}

\subsection{Error Analysis}
\label{sec:error}

We turn to proving the error bounds of Theorem \ref{thm:bounds}.  
Our proof is divided into three parts.  
We first show that \SSh obtains the three properties needed for Lemma \ref{lem:desai} with probability at least , and with the constraint on Property 2 strengthed by a factor .
Then we show how loosening Property 2 back to its original bound enables \BSSh to succeed with probability  for some . 
Finally we show that due to the mergeability of \fd~\cite{Lib12}, discussed in Section~\ref{related:fd}, the \sfd algorithm obtains the same error guarantees as \BSSh with probability  for a small  of our choice. 


In what follows, we mainly consider a single run of \SSh or \BSSh and let  and  denote the th singular value and th left singular vector of , respectively.

\subsubsection{Error Analysis: \SSh} 

Here we show that with probability at least  that  computed from  satisfies the three properties discussed in Section \ref{related:fd} required for Lemma \ref{lem:desai}.  
\begin{itemize} \denselist
\item Property 1: For any unit vector , ,
\item Property 2 (strengthened): For any unit vector , , 
\item Property 3: .
\end{itemize}


\label{subsub:SSh}
\begin{lemma} 
Property 1 holds deterministically for \SSh:  for all vectors .
\end{lemma}
\begin{proof}
Let  be as defined in \SSh. Consider an arbitrary unit vector , and let .

and 

therefore .
\end{proof}



\begin{lemma}
With probability at least , Property 2 holds for \SSh: for any unit vector , . \label{fact1}
\end{lemma}
\begin{proof}
Consider an arbitrary unit vector , and note that

We bound each term individually.  The first term is bounded as
 
Where transition 5 is true because  is a projection. Transition 7 also holds by the spectral norm error bound of \cite{musco2015stronger} for .
To bound the second term, note that , since  as defined in \SSh. 

where last inequality follows by the Courant-Fischer min-max principle, i.e. as  is the th singular value of the projection of  onto , then . Summing the two terms yields .
\end{proof}

The original bound  discussed in Section~\ref{sec:suc-prob} is also immediately satisfied.





\begin{lemma}
With probability at least , Property 3 holds for \SSh: .  
\label{lem:prop3}
\end{lemma}
\begin{proof}
In addition, 

The last inequality holds by the per vector error bound of~\cite{musco2015stronger} for  and , i.e. 
, which means .
Therefore 

\end{proof}



\subsubsection{Error Analysis: \BSSh and \sfd}
We now consider the \BSSh algorithm, and the looser version of Property 2 (the original version) as
\begin{itemize} \denselist
\item Property 2: For any unit vector , .  
\end{itemize}
By invoking , then \vs always returns \textsc{True} if 
 (as is true of the input with probability at least  by Lemma \ref{fact1}), and 
\vs catches a failure event where  with probability at least  by Lemma \ref{lem:vs}.  
As discussed in Section~\ref{sec:suc-prob} all invocations of \vs succeed with probability at most , hence all runs of \BSSh succeed and satisfy Property 2 (as well as Properties 1 and 3) with  and , and with probability at least .
Finally, we can invoke the mergeability property of ~\cite{Lib12} and Lemma \ref{lem:desai} to obtain the error bounds in our main result, Theorem \ref{thm:bounds}.  




\section{Experiments}
\label{sec:exp}
In this section we empirically validate that \sfd matches (and often improves upon) the accuracy of \fd, while running significantly faster on sparse real and synthetic datasets.  

We do not implement \sfd exactly as described above.  Instead we directly call \SSh in Algorithm \ref{alg:SparseFD} in place of \BSSh.  The randomized error analysis of \si indicates that we may occasionally miss a subspace within a call of \si and hence \SSh; but in practice this is not a catastrophic event, and as we will observe, does not prevent \sfd from obtaining small empirical error.  

The empirical comparison of \fd to other matrix sketching techniques is now well-trodden~\cite{ghashami2015frequent,desai2015improved}.  \fd (and, as we observe, by association \sfd) has much smaller error than other sketching techniques which operate in a stream.  However, \fd is somewhat slower by a factor of the sketch size  up to some leading coefficients.  
We do not repeat these comparison experiments here.  

\Paragraph{Setup} 
We ran all the algorithms under a common implementation framework to test their relative performance as accurately as possible. We ran the experiments on an Intel(R) Core(TM) 2.60 GHz CPU with 64GB of RAM running Ubuntu 14.04.3. 
All algorithms were coded in C, and compiled using gcc 4.8.4.
All linear algebra operation on dense matrices (such as SVD) invoked those implemented in LAPACK. 

\Paragraph{Datasets} 
We compare the performance of the two algorithms on both synthetic and real datasets. Each dataset is an  matrix  containing  datapoints in  dimensions. 

The real dataset is part of the  Newsgroups dataset~\cite{Lang95}, that is a collection of approximately  documents, partitioned across  different newsgroups. However we use the `by date' version of the data, where features (columns) are tokens and rows correspond to documents. This data matrix is a zero-one matrix with  rows and  columns. In our experiment, we use the transpose of the data and picked the first  columns, hence the subset matrix has  rows and  columns; roughly  of the subset matrix is non-zeros.


The synthetic data generates  rows i.i.d.  
Each row receives exactly  non-zeros (with default  and ), with the remaining entries as .  
The non-zeros are chosen as either  or  at random.  
Each non-zero location is chosen without duplicates among the columns. 
The first  columns (e.g., 150), the ``head'', have a higher probability of receiving a non-zero than the last  columns, the ``tail''.   The process to place a non-zero first chooses the head with probability  or the tail with probability .  For whichever set of columns it chooses (head or tail), it places the non-zero uniformly at random among those columns.  


\Paragraph{Measurements} 
Each algorithm outputs a sketch matrix  of  rows.  For each of our experiments, we measure the efficiency of algorithms against one parameter and keep others fixed at a default value.
Table \ref{tbl:param} lists all parameters along with their default value and the range they vary in for synthetic dataset.
We measure the accuracy of the algorithms with respect to:
\begin{itemize}
\item Projection Error:  
\textsf{proj-err} ,
\item Covariance Error: 
\textsf{cov-err} ,
\item Runtime in seconds. 
\end{itemize}
In all experiments, we have set . Note that \textsf{proj-err} is always larger than , and for \fd and \sfd the \textsf{cov-err} is always smaller than  due to our error guarantees.



\begin{table*}[t!!!!]
\caption{Parameter values} \label{tbl:param}
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{|c||c|c|}
\hline
 & \textbf{default} & \textbf{range} \\
\hline
\textbf{ datapoints } &  &  \\
\hline
\textbf{dimension } &  &  \\
\hline
\textbf{sketch size } &  &  \\
\hline
\textbf{nnz per row } &  & \\
\hline
\end{tabular}
\end{sc}
\end{small} 
\end{center}
\end{table*}

\begin{table*}
\begin{tabular}{|c|c|c|c|c|} \hline
\rotatebox{90}{\hspace{1mm}  \small \textsf{Projection Error }} &
\includegraphics[width=\figsize]{n-proj.pdf} &
\includegraphics[width=\figsize]{d-proj.pdf} &
\includegraphics[width=\figsize]{ell-proj.pdf} &
\includegraphics[width=\figsize]{sparsity-proj.pdf} \\ \hline 
\rotatebox{90}{\hspace{1mm}  \small \textsf{Covariance Error }} &
\includegraphics[width=\figsize]{n-cov.pdf} &
\includegraphics[width=\figsize]{d-cov.pdf} &
\includegraphics[width=\figsize]{ell-cov.pdf} &
\includegraphics[width=\figsize]{sparsity-cov.pdf} \\ \hline
\rotatebox{90}{\hspace{5mm} \small \textsf{Run Time }} &
\includegraphics[width=\figsize]{n-time.pdf} &
\includegraphics[width=\figsize]{d-time.pdf} &
\includegraphics[width=\figsize]{ell-time.pdf} &
\includegraphics[width=0.93\figsize]{sparsity-time.pdf} \\ \hline
\; &
\textsf{number of data points} &
\textsf{dimension} &
\textsf{sketch size} &
\textsf{nnz per row} \\ \hline
\end{tabular}
\caption{
\label{fig:n}
Comparing performance of \fd and \sfd on synthetic data. Each column reports the measurement against one parameter; ordered from left to right it is number of datapoints , dimension , sketch size , and number of non-zeros  per row. 
Table \ref{tbl:param} lists default value of all parameters. } 
\end{table*}





\begin{figure*}[t!]
\begin{centering}
\includegraphics[width=1.2\figsize]{20news-proj.pdf} \;\;\;\;\;\;
\includegraphics[width=1.2\figsize]{20news-cov.pdf} \;\;\;\;\;\;
\includegraphics[width=1.2\figsize]{20news-time.pdf}
\caption{
\label{fig:real}
Comparing performance of \fd and \sfd on  Newsgroups dataset.  We plot Projection Error, Covariance Error, and Run Time as a function of sketch size ().} 
\end{centering}
\end{figure*}


\subsection{Observations}

By considering Table \ref{fig:n} on synthetic data and Figure \ref{fig:real} on the real data, we can vary and learn many aspects of the runtime and accuracy of \sfd and \fd.  

\Paragraph{Runtime}
Consider the last row of Table \ref{fig:n}, the ``Run Time'' row, and the last column of Figure \ref{fig:real}.  
\sfd is clearly faster than \fd for all datasets, except when the synthetic data becomes dense in the last column of the ``Run Time'' row, where  and  in the right-most data point.  For the default values the improvement is between about a factor of x and , but when the matrix is very sparse the improvement is x or more.  Very sparse synthetic examples are seen in the left data points of the last column, and in the right data points of the second column, of the ``Run Time'' row. 

In particular, these two plots (the second and fourth columns of the ``Run Time'' row) really demonstrate the dependence of \sfd on  and of \fd on .  
In the last column, we fix the matrix size  and , but increase the number of non-zeros ; the runtime of \fd is basically constant, while for \sfd it grows linearly.  
In the second column, we fix  and , but increase the number of columns ; the runtime of \fd grows linearly while the runtime for \sfd is basically constant.  

These algorithms are designed for datasets with extremely large values of ; yet we only run on datasets with  up to  in Table \ref{fig:n}, and  in Figure \ref{fig:real}.  However, both \fd and \sfd have runtime that grows linearly with respect to the number of rows (assuming the sparsity is at an expected fixed rate per row for \sfd).  This can also be seen empirically in the first column of the ``Run Time'' row where, after a small start-up cost, both \fd and \sfd grow linearly as a function of the number of data points .  Hence, it is valid to directly extrapolate these results for datasets of increased .  

\Paragraph{Accuracy}
We will next discuss the accuracy, as measured in Projection Error in the top row of Table \ref{fig:n} and left plot of Figure \ref{fig:real}, and in Covariance Error in the middle row of Table \ref{fig:n} and middle plot of Figure \ref{fig:real}.  
We observe that both \fd and \sfd obtain very small error (much smaller than upper bounded by the theory), as has been observed elsewhere~\cite{ghashami2015frequent,desai2015improved}.  Moreover, the error for \sfd always nearly matches, or improves over \fd.  We can likely attribute this improvement to being able to process more rows in each batch, and hence needing to perform the shrinking operation fewer overall times.  
The one small exception to \sfd having less Covariance Error than \fd is for extreme sparse datasets in the leftmost data points of Table \ref{fig:n}, last column -- we attribute this to some peculiar orthogonality of columns with near equal norms due to extreme sparsity.  






\bibliographystyle{plain}
\bibliography{sparsefd}






\end{document}
