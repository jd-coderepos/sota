\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}






\usepackage[final]{neurips_2019}




\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \PassOptionsToPackage{numbers, compress}{natbib}

\usepackage{wrapfig}


\usepackage[table]{xcolor}
\definecolor{mydarkred}{rgb}{0.6,0,0}
\definecolor{mydarkgreen}{rgb}{0,0.6,0}
\usepackage[colorlinks,
linkcolor=mydarkred,
citecolor=mydarkgreen]{hyperref}
\usepackage{url}

\usepackage{booktabs}       \usepackage{amsfonts} \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{algorithmic}
\usepackage{algorithm}
\usepackage[algo2e,linesnumbered]{algorithm2e}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{epstopdf}
\usepackage{multicol}

\usepackage{caption}
\usepackage{subfigure}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{pifont}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\usepackage{amssymb}
\usepackage{pifont}
\newcommand{\cmark}{\text{\ding{51}}}
\newcommand{\xmark}{\text{\ding{55}}}


\title{Are Anchor Points Really Indispensable\\ in Label-Noise Learning?}




\author{
  Xiaobo Xia\quad
  Tongliang Liu\quad
  Nannan Wang\\
  \textbf{Bo Han\quad Chen Gong\quad Gang Niu\quad Masashi Sugiyama}\
\label{eq:anchor-estim}P(\bar{Y}=j|X=x)=\sum_{k=1}^C T_{kj}P(Y=k|X=x)=T_{ij}.

\label{eq:importance}&&R(f)=\mathbb{E}_{(X,Y)\sim D}[\ell(f(X),Y)]=\int_{x}\sum_iP_D(X=x,Y=i)\ell(f(x),i)dx\nonumber\\
&&=\int_{x}\sum_iP_{\bar{D}}(X=x,\bar{Y}=i)\frac{P_D(X=x,\bar{Y}=i)}{P_{\bar{D}}(X=x,\bar{Y}=i)}\ell(f(x),i)dx\nonumber\\
&&=\int_{x}\sum_iP_{\bar{D}}(X=x,\bar{Y}=i)\frac{P_D(\bar{Y}=i|X=x)}{P_{\bar{D}}(\bar{Y}=i|X=x)}\ell(f(x),i)dx\\
&&=\mathbb{E}_{(X,Y)\sim \bar{D}}[\bar{\ell}(f(X),Y)],\nonumber

\label{eq:em_importance}{
\bar{R}_{n,w}(T,f)=\frac{1}{n}\sum_{i=1}^{n}\frac{{g}_{\bar{Y}_i}(X_i)}{(T^\top{g})_{\bar{Y}_i}(X_i)}\ell(f(X_i),\bar{Y}_i),}

\label{eq:theorem}\mathbb{E}[\bar{R}_{n,w}(\hat{T}+\Delta \hat{T},\hat{f})]- \bar{R}_{n,w}(\hat{T}+\Delta \hat{T},\hat{f})\leq \frac{2BCL(\sqrt{2d\log2}+1)\Pi_{i=1}^{d}M_i}{\sqrt{n}}+CM\sqrt{\frac{\log{1/\delta}}{2n}}.\nonumber

R(f)=\mathbb{E}_{(X,Y)\sim D}[\ell(f(X),Y)].

R_n(f)=\frac{1}{n}\sum_{i=1}^{n}\ell(f(x_i),y_i).

f_\rho(x)=\arg\max_{i\in\{1,2,\ldots,C\}}P(Y=i|X=x).

\bar{R}_n(f)=\frac{1}{n}\sum_{i=1}^{n}\ell({f}(X_i),\bar{Y}_i).

\bar{h}(X)=\arg\max_{i\in\{1,2,\ldots,C\}}(T^\top {g})_i(X),

\bar{R}_n(\bar{h})=\frac{1}{n}\sum_{i=1}^{n}\ell(\bar{h}(X_i),\bar{Y}_i)

\mathbb{E}_{\bar{Y}}[\tilde{\ell}({f}(X),\bar{Y})]=\ell(f(X),Y)

\mathbb{E}_{(X,Y,\bar{Y})}\tilde{\ell}({f}(X),\bar{Y})=R(f).

{\mathcal{L}}(f(X),{\bf {Y}})=[{\ell}(f(X),{Y}=1),\ldots,{\ell}(f(X),{Y}=C)]^\top

\tilde{\mathcal{L}}(f(X),{\bf \bar{Y}})=[\tilde{\ell}(f(X),\bar{Y}=1),\ldots,\tilde{\ell}(f(X),\bar{Y}=C)]^\top=(T^\top)^{-1}{\mathcal{L}}(f(X),{\bf \bar{Y}}).

\mathbb{E}_{\bar{Y}|Y}[\tilde{\mathcal{L}}(f(X),{\bf \bar{Y}})]=T^\top\tilde{\mathcal{L}}(f(X),{\bf \bar{Y}})={\mathcal{L}}(f(X),{\bf {Y}}).

\bar{R}_{n,w}(\hat{T}+\Delta T,f)=\frac{1}{n}\sum_{i=1}^{n}\frac{{g}_{\bar{Y}_i}(X_i)}{((\hat{T}+\Delta T)^\top{g})_{\bar{Y}_i}(X_i)}\ell(f(X_i),\bar{Y}_i),

\Phi(S)=\sup_{\Delta T,f}(\bar{R}_{n,w}(\hat{T}+\Delta T,f)-\mathbb{E}_S[\bar{R}_{n,w}(\hat{T}+\Delta T,f)]).

\mathbb{E}[\bar{R}_{n,w}(\hat{T}+\Delta \hat{T},\hat{f})]- \bar{R}_{n,w}(\hat{T}+\Delta \hat{T},\hat{f})\leq \mathbb{E}[\Phi(S)]+CM\sqrt{\frac{\log{1/\delta}}{2n}}.

 \mathbb{E}[\Phi(S)]\leq 2\mathbb{E}\left[\sup_{\Delta T,f}\frac{1}{n}\sum_{i=1}^{n}\sigma_i\frac{{g}_{\bar{Y}_i}(X_i)}{((\hat{T}+\Delta T)^\top{g})_{\bar{Y}_i}(X_i)}\ell(f(X_i),\bar{Y}_i)\right],

\mathbb{E}\left[\sup_{\Delta T,f}\frac{1}{n}\sum_{i=1}^{n}\sigma_i\frac{{g}_{\bar{Y}_i}(X_i)}{((\hat{T}+\Delta T)^\top{g})_{\bar{Y}_i}(X_i)}\ell(f(X_i),\bar{Y}_i)\right]\leq \mathbb{E}\left[\sup_{f}\frac{1}{n}\sum_{i=1}^{n}\sigma_i\ell(f(X_i),\bar{Y}_i)\right].

 \ell(f(X),\bar{Y})=-\sum_{i=1}^{C}1_{\{\bar{Y}=i\}}\log(g_i(X)).

\mathbb{E}\left[\sup_{f}\frac{1}{n}\sum_{i=1}^{n}\sigma_i\ell(f(X_i),\bar{Y}_i)\right]\leq CL\mathbb{E}\left[ \sup_{h\in H }\frac{1}{n}\sum_{i=1}^{n}\sigma_ih(X_i)\right],

\mathbb{E}\left[ \sup_{h\in H }\frac{1}{n}\sum_{i=1}^{n}\sigma_ih(X_i)\right]\leq \frac{B(\sqrt{2d\log2}+1)\Pi_{i=1}^{d}M_i}{\sqrt{n}}.

\Phi(S)-\Phi(S^i)\leq \sup_{\Delta T,f}\frac{1}{n}\left(\frac{{g}_{\bar{Y}_i}(X_i)\ell(f(X_i),\bar{Y}_i)}{((\hat{T}+\Delta T)^\top{g})_{\bar{Y}_i}(X_i)}-\frac{{g}_{\bar{Y}'_i}(X'_i)\ell(f(X'_i),\bar{Y}'_i)}{((\hat{T}+\Delta T)^\top{g})_{\bar{Y}'_i}(X'_i)}\right).

\Phi(S)-\Phi(S^i)\leq \frac{CM}{n}.

\Phi(S)-\mathbb{E}[\Phi(S)]\leq CM\sqrt{\frac{\log(1/\delta)}{2n}}.

\mathbb{E}_\sigma\left[\sup_{\Delta T,f}\frac{1}{n}\sum_{i=1}^{n}\sigma_i\frac{{g}_{\bar{Y}_i}(X_i)}{((\hat{T}+\Delta T)^\top{g})_{\bar{Y}_i}(X_i)}\ell(f(X_i),\bar{Y}_i)\right]\leq \mathbb{E}_\sigma\left[\sup_{f}\frac{1}{n}\sum_{i=1}^{n}\sigma_i\ell(f(X_i),\bar{Y}_i)\right].

\begin{aligned}
&\mathbb{E}_\sigma\left[\sup_{\Delta T,f}\frac{1}{n}\sum_{i=1}^{n}\sigma_i\frac{{g}_{\bar{Y}_i}(X_i)}{((\hat{T}+\Delta T)^\top{g})_{\bar{Y}_i}(X_i)}\ell(f(X_i),\bar{Y}_i)\right]\\
&=\mathbb{E}_{\sigma_1,\ldots,\sigma_{n-1}}\left[\mathbb{E}_{\sigma_n}\left[\sup_{\Delta T,f}\frac{1}{n}\sum_{i=1}^{n}\sigma_i\frac{{g}_{\bar{Y}_i}(X_i)}{((\hat{T}+\Delta T)^\top{g})_{\bar{Y}_i}(X_i)}\ell(f(X_i),\bar{Y}_i)\right]\right].
\end{aligned}

\begin{aligned}
&\frac{{g}_{\bar{Y}_n}(X_n)}{((\hat{T}+\Delta T)^\top{g})_{\bar{Y}_n}(X_n)}\ell(f_1(X_n),\bar{Y}_n)+s_{n-1}(\Delta T,f_1)\\
&\geq (1-\epsilon)\sup_{\Delta T, f}\left(\frac{{g}_{\bar{Y}_n}(X_n)}{((\hat{T}+\Delta T)^\top{g})_{\bar{Y}_n}(X_n)}\ell(f(X_n),\bar{Y}_n)+s_{n-1}(\Delta T,f)\right)
\end{aligned}

\begin{aligned}
&-\frac{{g}_{\bar{Y}_n}(X_n)}{((\hat{T}+\Delta T)^\top{g})_{\bar{Y}_n}(X_n)}\ell(f_2(X_n),\bar{Y}_n)+s_{n-1}(\Delta T,f_2)\\
&\geq (1-\epsilon)\sup_{\Delta T, f}\left(-\frac{{g}_{\bar{Y}_n}(X_n)}{((\hat{T}+\Delta T)^\top{g})_{\bar{Y}_n}(X_n)}\ell(f(X_n),\bar{Y}_n)+s_{n-1}(\Delta T,f)\right).
\end{aligned}

\begin{aligned}
&(1-\epsilon)\mathbb{E}_{\sigma_n}\left[\sup_{\Delta T, f}\left(\sigma_n\frac{{g}_{\bar{Y}_n}(X_n)}{((\hat{T}+\Delta T)^\top{g})_{\bar{Y}_n}(X_n)}\ell(f(X_n),\bar{Y}_n)+s_{n-1}(\Delta T,f)\right)\right]\\
&=\frac{(1-\epsilon)}{2}\sup_{\Delta T, f}\left(\frac{{g}_{\bar{Y}_n}(X_n)}{((\hat{T}+\Delta T)^\top{g})_{\bar{Y}_n}(X_n)}\ell(f_1(X_n),\bar{Y}_n)+s_{n-1}(\Delta T,f_1)\right)\\
&+\frac{(1-\epsilon)}{2}\sup_{\Delta T, f}\left(-\frac{{g}_{\bar{Y}_n}(X_n)}{((\hat{T}+\Delta T)^\top{g})_{\bar{Y}_n}(X_n)}\ell(f_2(X_n),\bar{Y}_n)+s_{n-1}(\Delta T,f_2)\right)\\
&\leq\frac{1}{2}\left(\frac{{g}_{\bar{Y}_n}(X_n)}{((\hat{T}+\Delta T)^\top{g})_{\bar{Y}_n}(X_n)}\ell(f_1(X_n),\bar{Y}_n)+s_{n-1}(\Delta T,f_1)\right.\\
&\left.+ (s_{n-1}(\Delta T,f_2)-\frac{{g}_{\bar{Y}_n}(X_n)}{((\hat{T}+\Delta T)^\top{g})_{\bar{Y}_n}(X_n)}\ell(f_2(X_n),\bar{Y}_n)\right)\\
&\leq\frac{1}{2}\left(s_{n-1}(\Delta T,f_1)+ s_{n-1}(\Delta T,f_2)+C|\ell(f_1(X_n),\bar{Y}_n)-\ell(f_2(X_n),\bar{Y}_n)|\right),
\end{aligned}

\begin{aligned}
&(1-\epsilon)\mathbb{E}_{\sigma_n}\left[\sup_{\Delta T, f}\left(\sigma_n\frac{{g}_{\bar{Y}_n}(X_n)}{((\hat{T}+\Delta T)^\top{g})_{\bar{Y}_n}(X_n)}\ell(f(X_n),\bar{Y}_n)+s_{n-1}(\Delta T,f)\right)\right]\\
&\leq\frac{1}{2}\left(s_{n-1}(\Delta T,f_1)+ s_{n-1}(\Delta T,f_2)+sC(\ell(f_1(X_n),\bar{Y}_n)-\ell(f_2(X_n),\bar{Y}_n))\right)\\
&=\frac{1}{2}\left(s_{n-1}(\Delta T,f_1)+sC\ell(f_1(X_n),\bar{Y}_n) \right)+\frac{1}{2}\left(s_{n-1}(\Delta T,f_2)-sC\ell(f_2(X_n),\bar{Y}_n)\right)\\
&\leq \frac{1}{2}\sup_{f\in F}\left(s_{n-1}(\Delta T,f)+sC\ell(f(X_n),\bar{Y}_n) \right)+\frac{1}{2}\sup_{f\in F}\left(s_{n-1}(\Delta T,f)-sC\ell(f(X_n),\bar{Y}_n)\right)\\
&=\mathbb{E}_{\sigma_n}\left[\sup_{\Delta T, f}\left(\sigma_n\ell(f(X_n),\bar{Y}_n)+s_{n-1}(\Delta T,f)\right)\right].
\end{aligned}

\begin{aligned}
&\mathbb{E}_{\sigma_n}\left[\sup_{\Delta T, f}\left(\sigma_n\frac{{g}_{\bar{Y}_n}(X_n)}{((\hat{T}+\Delta T)^\top{g})_{\bar{Y}_n}(X_n)}\ell(f(X_n),\bar{Y}_n)+s_{n-1}(\Delta T,f)\right)\right]\\
&\leq \mathbb{E}_{\sigma_n}\left[\sup_{\Delta T, f}\left(\sigma_n\ell(f(X_n),\bar{Y}_n)+s_{n-1}(\Delta T,f)\right)\right].
\end{aligned}

\begin{aligned}
&\mathbb{E}_{\sigma}\left[\sup_{\Delta T, f}\sum_{i=1}^n\sigma_i\frac{{g}_{\bar{Y}_i}(X_i)}{((\hat{T}+\Delta T)^\top{g})_{\bar{Y}_i}(X_i)}\ell(f(X_i),\bar{Y}_i)\right]\leq \mathbb{E}_{\sigma}\left[\sup_{f\in F}\sum_{i=1}^n\sigma_i\ell(f(X_i),\bar{Y}_i)\right].
\end{aligned}

\begin{aligned}
&\mathbb{E}\left[\sup_{\Delta T, f}\sum_{i=1}^n\sigma_i\frac{{g}_{\bar{Y}_i}(X_i)}{((\hat{T}+\Delta T)^\top{g})_{\bar{Y}_i}(X_i)}\ell(f(X_i),\bar{Y}_i)\right]\leq \mathbb{E}\left[\sup_{f\in F}\sum_{i=1}^n\sigma_i\ell(f(X_i),\bar{Y}_i)\right].
\end{aligned}

{\ell}(f(X),\bar{Y}) = -\sum_{i=1}^{C}1_{\{\bar{Y}=i\}}\log(g_i(X))= -\log\left(\frac{\exp(h_{\bar{Y}}(X))}{\sum_{i=1}^C \exp(h_i(X))}\right).
 \label{derivative1}
\begin{aligned}
&\frac{\partial {\ell}(f(X),\bar{Y})}{\partial h_i(X)} = \frac{\exp(h_i(X))}{\sum_{i=1}^c \exp(h_i(X))}.
\end{aligned}
 \label{derivative2}
\begin{aligned}
&\frac{\partial {\ell}(f(X),\bar{Y})}{\partial h_i(X)} = -1+ \frac{\exp(h_i(X))}{\sum_{i=1}^c \exp(h_i(X))}.
\end{aligned}

\begin{aligned}
&\mathbb{E}\left[\sup_{f}\frac{1}{n}\sum_{i=1}^{n}\sigma_i\ell(f(X_i),\bar{Y}_i)\right]\\
&= \mathbb{E}\left[\sup_{f=\arg\max\{h_1,\ldots,h_c\} }\frac{1}{n}\sum_{i=1}^{n}\sigma_i{\ell}(f(X_i),\bar{Y}_i)\right] \\
&= \mathbb{E}\left[\sup_{\max\{h_1,\ldots,h_c\} }\frac{1}{n}\sum_{i=1}^{n}\sigma_i{\ell}(f(X_i),\bar{Y}_i)\right] \\
&\leq \mathbb{E}\left[ \sum_{k=1}^C \sup_{h_k\in H }\frac{1}{n}\sum_{i=1}^{n}\sigma_i{\ell}(f(X_i),\bar{Y}_i)\right] \\
&= \sum_{k=1}^C \mathbb{E}\left[ \sup_{h_k\in H }\frac{1}{n}\sum_{i=1}^{n}\sigma_i{\ell}(f(X_i),\bar{Y}_i)\right] \\
&\leq CL\mathbb{E}\left[ \sup_{h_k\in H }\frac{1}{n}\sum_{i=1}^{n}\sigma_ih_k(X_i)\right] \\
&=CL\mathbb{E}\left[ \sup_{h\in H }\frac{1}{n}\sum_{i=1}^{n}\sigma_ih(X_i)\right],
\end{aligned}

\text{sym-:}
& \quad
T =
\begin{bmatrix}
1-\epsilon & \frac{\epsilon}{C-1} & \dots  & \frac{\epsilon}{C-1} & \frac{\epsilon}{C-1}\\
\frac{\epsilon}{C-1} & 1-\epsilon & \frac{\epsilon}{C-1} & \dots & \frac{\epsilon}{C-1}\\
\vdots &  & \ddots &  & \vdots\\
\frac{\epsilon}{C-1} & \dots & \frac{\epsilon}{C-1} & 1-\epsilon & \frac{\epsilon}{C-1}\\
\frac{\epsilon}{C-1} & \frac{\epsilon}{C-1} & \dots  & \frac{\epsilon}{C-1} & 1-\epsilon
\end{bmatrix}.


\section{More discussions about Figure 3}
We represent Figure 3 in the main paper as Figure 1 in this appendix. 

From the figure, we can compare the transition matrices learned by the proposed T-revision method and the traditional anchor point based method. Specifically, as shown in Figure 1, at epoch 0, the estimation error corresponds to the estimation error of transition matrix learned by identifying anchor points \cite{thekumparampil2018robustness} (the traditional method to learn transition matrix). Note that the method with "-N/A" in its name means it runs on the modified datasets where instances with large clean class posterior probobilities are removed (anchor points are removed); while the method with "-A" in its name means it runs the original intact dataset (may contain anchor points). Clearly, we can see that the estimation error will increase by removing possible anchor points, meaning that anchor points is crucial in the traditional transition matrix learning. Moreover, as the number of epochs grows, the figures show how the estimation error varies by running the proposed revision methods. We can see that the proposed Reweight method always leads to smaller estimation errors, showing that the proposed method works well in find a better transition matrix.

Figure 1 also shows the comparison of learning transition matrices between the risk-consistent estimator based method and the classifier-consistent method based method. For classifier-consistent algorithms, we can also modify the transition matrix by adding a slack variable and learning it jointly with the classifier, e.g., Forward-A-R and Forward-N/A-R. However, we can find that the classifier-consistent algorithm based method Forward-N/A-R may fail in learning a good transition matrix, e.g., Figure 1(a). This is because there is no reason to learn the transition matrix by minimizing the classifier-consistent objective function. It is reasonable to learn the transition matrix by minimizing the risk-consistent estimator because a favorable transition matrix should make the classification risk w.r.t. clean data small. This is verified by comparing Forward-A-R and Forward-N/A-R with the proposed Reweight-A-R and Reweight-N/A-R, we can find that the risk-consistent estimator Reweight always leads to smaller estimation errors for learning transition matrix.

\begin{figure}[t]
\centering
\vspace{-5px}
\includegraphics[width=0.32\textwidth]{mnist-symmetry20-Estimation-Error.pdf}
\includegraphics[width=0.32\textwidth]{cifar-10-symmetry20-Estimation-Error.pdf}
\includegraphics[width=0.32\textwidth]{cifar-100-symmetry20-Estimation-Error.pdf}
\subfigure[\textit{MNIST}]
{\includegraphics[width=0.32\textwidth]{mnist-symmetry50-Estimation-Error.pdf}}
\subfigure[\textit{CIFAR-10}]
{\includegraphics[width=0.32\textwidth]{cifar-10-symmetry50-Estimation-Error.pdf}}
\subfigure[\textit{CIFAR-100}]
{\includegraphics[width=0.32\textwidth]{cifar-100-symmetry50-Estimation-Error_250epochs.pdf}}
\vspace{-10px}
\caption{Comparing the estimation error of the transition matrix by employing classifier-consistent and risk-consistent estimators. The first row is about sym-20 label noise while the second row is about sym-50 label noise. The error bar for STD in each figure has been highlighted as a shade. }
\label{fig:estimation}
\vspace{-5px}
\end{figure}

\end{document}
