[{'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Average (%)', 'Score': '86.5'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Average (%)', 'Score': '70'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'TruthfulQA', 'Metric': 'MC1', 'Score': '0.59'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'DROP Test', 'Metric': 'F1', 'Score': '80.9'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'DROP Test', 'Metric': 'F1', 'Score': '64.1'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'InfiMM-Eval', 'Metric': 'Overall score', 'Score': '74.44'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'InfiMM-Eval', 'Metric': 'Deductive', 'Score': '74.86'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'InfiMM-Eval', 'Metric': 'Abductive', 'Score': '77.88'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'InfiMM-Eval', 'Metric': 'Analogical', 'Score': '69.86'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'InfiMM-Eval', 'Metric': 'Params', 'Score': '-'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering', 'Dataset': 'MM-Vet', 'Metric': 'GPT-4 score', 'Score': '67.7±0.3'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering', 'Dataset': 'MM-Vet', 'Metric': 'GPT-4 score', 'Score': '67.6±0.1'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering', 'Dataset': 'MM-Vet', 'Metric': 'GPT-4 score', 'Score': '60.2±0.3'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering', 'Dataset': 'BenchLMM', 'Metric': 'GPT-3.5 score', 'Score': '58.37'}}, {'LEADERBOARD': {'Task': 'Code Generation', 'Dataset': 'HumanEval', 'Metric': 'Pass@1', 'Score': '67.0'}}, {'LEADERBOARD': {'Task': 'Code Generation', 'Dataset': 'HumanEval', 'Metric': 'Pass@1', 'Score': '48.1'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'ARC (Challenge)', 'Metric': 'Accuracy', 'Score': '96.3'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'ARC (Challenge)', 'Metric': 'Accuracy', 'Score': '85.2'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'WinoGrande', 'Metric': 'Accuracy', 'Score': '87.5'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'WinoGrande', 'Metric': 'Accuracy', 'Score': '81.6'}}, {'LEADERBOARD': {'Task': 'Sentence Completion', 'Dataset': 'HellaSwag', 'Metric': 'Accuracy', 'Score': '95.3'}}, {'LEADERBOARD': {'Task': 'Sentence Completion', 'Dataset': 'HellaSwag', 'Metric': 'Accuracy', 'Score': '85.5'}}, {'LEADERBOARD': {'Task': 'Arithmetic Reasoning', 'Dataset': 'GSM8K', 'Metric': 'Accuracy', 'Score': '92'}}, {'LEADERBOARD': {'Task': 'Arithmetic Reasoning', 'Dataset': 'GSM8K', 'Metric': 'Accuracy', 'Score': '57.1'}}]
