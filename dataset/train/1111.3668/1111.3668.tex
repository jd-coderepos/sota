\documentclass[11pt,twoside]{article}
\usepackage{acta-info}
\usepackage{euler}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{slashbox}
\usepackage{listings}
\usepackage{epstopdf}


\frenchspacing

\newcommand{\vol}{\textbf{3,} 2 (2011) 172--191}   \setcounter{page}{172}  
\newcommand{\amss}{\amssubj{65F60 11Y55
}}     \newcommand{\CRs}{\CRsubj{B.2.4
}}   \newcommand{\keyw}{\keywords{pseudo random number generators, linear recurring sequences, uniform distribution, 
matrix exponentiation, parallel arithmetic, FPGA design, hardware acceleration of computations, hardware implementation of computations
}}


\begin{document}
\title{
Modular exponentiation of matrices on FPGA-s
}
\maketitle


\twoauthors{\href{http://www.inf.unideb.hu/~herendi/}{Tam\'as HERENDI}
}{\href{http://www.inf.unideb.hu/}{University of Debrecen}
}{\href{mailto:herendi.tamas@inf.unideb.hu}{herendi.tamas@inf.unideb.hu}
}{Roland S\'andor MAJOR
}{\href{http://www.inf.unideb.hu/}{University of Debrecen}
}{\href{mailto:mroland@digikabel.hu}{mroland@digikabel.hu}
}


\short{T. Herendi, R. Major
}{Modular exponentiation of matrices on FPGA-s
}


\begin{abstract}
We describe an efficient FPGA implementation for the exponentiation of large matrices. The research is related to an algorithm for constructing uniformly distributed linear recurring sequences. The design utilizes the special properties of both the FPGA and the used matrices to achieve a very significant speedup compared to traditional architectures.
\end{abstract}


\section{Introduction}

Field-programmable gate arrays (FPGA) offer a number of special options in computation. Utilizing the unique properties of an FPGA, some algorithms that are impractical to implement on a more traditional architecture can become both convenient to create and resource-efficient. The programmable array of look-up tables commonly found on an FPGA provide both flexibility in creating logic to suit specific needs and naturally lend themselves to great parallelism in computations.

Fast operations on matrices are of great practical interest. Ways to speed up certain matrix calculations still find their way into numerous applications.

Faster implementations of matrix algorithms can be achieved either from a ``software'' point of view, by improving upon the algorithm itself, or from a ``hardware'' point of view, by using faster or differently structured architectures.

Theoretical improvements on matrix algorithms include Strassen's algorithm \cite{c12} and the Coppersmith-Winograd algorithm \cite{c2}. The naive algorithm for matrix multiplication is a well-known  algorithm. Strassen's algorithm uses an idea similar to the Karatsuba-multiplication. It has a time complexity of  by dividing the matrices into sub-matrices. Then by multiplying them in a different arrangement, it manages an overall lower multiplication count compared to the classical algorithm. Research implementing it on the Cell Broadband Engine can be found in \cite{c5}. Strassen's algorithm and its applicability to the project is briefly discussed in Section 7. The Coppersmith-Winograd algorithm further improves the complexity to  by combining the idea of Strassen with the Salem-Spencer theorem. \cite{c9} discusses and compares the performance of implementations of these algorithms.

Numerous research has been done on creating efficient realizations of different matrix operations on different architectures. \cite{c8} and \cite{c10} both use FPGAs to perform matrix inversion.

The design presented here is an implementation of matrix multiplication on an FPGA. Works of similar nature can be found in \cite{c1} and \cite{c4}, dealing with FPGA configurations used for floating point matrix multiplication. \cite{c11} uses an FPGA design for digital signal processing. \cite{c3} discusses another FPGA implementation for accelerating matrix multiplication.

The research in this paper is related to an algorithm for the construction of pseudo random number generators. It requires the exponentiation of large matrices to an extremely high power. This allows for numerous optimizations to be made on the FPGA implementation, resulting in an extremely fast design. A speedup factor of  200 is achieved compared to a highly optimized program on a more traditional architecture.

We give the details of a design implemented on a Virtex-5 XC5VLX110T FPGA that multiplies two  sized matrices. The matrices are defined over the mod 4 residue class ring. Using this property and the fact that the hardware uses 6-LUTs (Lookup Tables), we describe first a module that computes the dot product of vectors taken from  in a single clock cycle at 100MHz clock speed. With these modules we construct a matrix multiplier module that computes the  product matrix of  and  in  clock cycles at 100MHz. The significance of the value 28 in the implementation and its experimental determination is also discussed. Finally, we describe how to use these modules for multiplying matrices taken from . The proposed algorithm deals with the management of stored data in such a way that it can be accomplished completely in parallel with the computations. The resulting design completes the multiplication in 64800 clock cycles at 100MHz.

Future work for increasing the size of the used matrices, and further optimizing the design's performance using Strassen's algorithm is also described.

\section{Mathematical background}

The present work is initiated by a method for the construction of uniformly distributed pseudo random number generators. (See \cite{c7}.) The generator uses recurring sequences modulo powers of 2 of the form

The theoretical background can be found in \cite{c6}.

The construction assumes that the values  are such that

holds for some  irreducible polynomial. It is practical to choose  to have maximal order, since the order of  is closely related to the period length of the corresponding recurring sequence.
The sequence  obtained this way does not necessarily have uniform distribution, however exactly one of the following four sequences does:

For the details see \cite{c7}. Finding the sequence with uniform distribution is of interest. 
Let


be the companion matrix of sequence . To find which of the above sequences has a uniform distribution, we have to compute . If  equals the identity matrix, then the period length of  is , which means it is not the sequence we are searching for.

The exponentiation of matrices to high powers can quickly become time consuming on traditional computers. The aim of the project was to utilize the special properties of an FPGA to achieve a significant upgrade in speed compared to implementations on more traditional architectures.

\section{Hardware used in the implementation}

The project was implemented on a Xilinx XUPV505-LX110T development platform. The board features a variety of ports for communication with the device. As a first approach the RS-232 serial port was used to send data between the board and a PC. A high-speed PCI Express connection is also available if the amount of data transferred would necessitate its use.

The board's most prominent feature is the Virtex-5 XC5VLX110T FPGA. The FPGA's main tool for computation is the array of 6-input look-up tables, arranged into 17280 Slices, with four look-up tables found in each Slice, adding up to a total of 69120 LUTs. A single 6-input LUT can store 64 bits of data, where its six input bits are used as an address to identify the single bit of data that is to be outputted. By manipulating the 64 bit content of the look-up table, it can be configured to carry out arbitrary Boolean functions with at most six input bits. In our design they are used to create LUTs performing a multiply-accumulate function, which are hierarchically arranged into larger and more complex modules. One out of four LUTs on the device can also be used as a 32 bit deep shift register; these are the basis to implement containers storing the data, which is directly fed to the computational module.

Attached to the board, there is a 256MB DDR2 SODIMM module, which is used for storing data exceeding the amount that can be practically stored on the FPGA.

\section{Structure of modules used in the computation}

The basic elements of the design are the LUTs denoted by , where  and  are two-digit binary numbers. The function carried out by  is a multiply-accumulate (for short: MA) function, i.e.:

Let , , , , where  , and  where  and  are two single bit LUTs, according to the following:
\\
\begin{itemize}
\item 
\item 
\end{itemize}
\begin{figure}[h!]
\centering
\includegraphics[width=3.17in,height=1.2in]{L3.eps}
\caption{The structure of }
\end{figure}

We remark that while  needs only three input bits to accomplish its function,  requires all six bits of input.

The LUTs  and  were configured to the values shown in Table 1 and Table 2 to perform the multiply-accumulate function.

\begin{table}[h!]
\centering
\begin{tabular} { | c | c | c |}
\hline
\backslashbox{()}{} & 0 & 1 \\ \hline
(0,0) & 0 & 1 \\ \hline
(0,1) & 0 & 1 \\ \hline
(1,0) & 0 & 1 \\ \hline
(1,1) & 1 & 0 \\ \hline
\end{tabular}
\caption{Contents of }
\end{table}


\begin{table}[h!]
\centering
\begin{tabular} { | c | c | c | c | c |}
\hline
\backslashbox{()}{} & 0 & 1 & 2 & 3 \\ \hline
(0,0) & 0 & 0 & 1 & 1 \\ \hline
(0,1) & 0 & 0 & 1 & 1 \\ \hline
(0,2) & 0 & 0 & 1 & 1 \\ \hline
(0,3) & 0 & 0 & 1 & 1 \\ \hline
(1,0) & 0 & 0 & 1 & 1 \\ \hline
(1,1) & 0 & 1 & 1 & 0 \\ \hline
(1,2) & 1 & 1 & 0 & 0 \\ \hline
(1,3) & 1 & 0 & 0 & 1 \\ \hline
(2,0) & 0 & 0 & 1 & 1 \\ \hline
(2,1) & 1 & 1 & 0 & 0 \\ \hline
(2,2) & 0 & 0 & 1 & 1 \\ \hline
(2,3) & 1 & 1 & 0 & 0 \\ \hline
(3,0) & 0 & 0 & 1 & 1 \\ \hline
(3,1) & 1 & 0 & 0 & 1 \\ \hline
(3,2) & 1 & 1 & 0 & 0 \\ \hline
(3,3) & 0 & 1 & 1 & 0 \\ \hline
\end{tabular}
\caption{Contents of }
\end{table}


With the help of these basic units one can compute the dot product  of two vectors  and . Let us define a module  by cascading  MA units denoted by . In this module  we use the output of a given MA unit as the sum input of the next unit, i.e.  for , where  and  are the  input and  output of .
\begin{figure}[h!]
\centering
\includegraphics[width=4.5in,height=1.2in]{m.eps}
\caption{The structure of }
\end{figure}

Therefor  is a function that accepts a pair of vectors  of two-digit numbers of length  and outputs on  the two-digit dot-product of the two vectors, i.e. .

In total, the number of LUTs used in  is . Note that vectors of arbitrary length can be used in the computation if we connect the output of module  to the sum input of  (), and then iteratively shift  and  onto the module's input by  elements at a time:
\\
\\
\\

\begin{tabbing}
Function \=  \ \ \ // \\
\>1. Define ,  \\
\>2. for \=  to  do \ \ \ // fill  and  with 0's\\
\> \>3. if  then  else  \\
\> \>4. if  then  else  \\
\>5. end for \\
\>6. Define , let  \\
\>7. for  to  do \ \ \ // shift  and  to  and \\
\> \>8.  \\
\> \>9.  \\
\> \>10.  \\
\>11. end for \\
\>12. return  \\
end Function \\
\end{tabbing}

Here  and  are the extensions of  and  by 0's.

We shall see that the number chosen for  is critical in setting many characteristics of the entire project. The experiment used for determining  will be discussed in the following chapter.

Our aim is to obtain a module that performs the matrix multiplication of , where  is the mod 4 residue class ring. In the following, let  be the output matrix, such that . Furthermore, let  be the th row of matrix  and let  be the th column of matrix .

The multiplier units denoted by  are used to create more complex modules in a hierarchical manner. First, by taking ten  multiplier blocks we create a row of multipliers . This is used to compute ten consecutive elements of a single row of the output matrix:

where . The input vector  is used by all ten multiplier units of . The length of these vectors, as mentioned above, can be arbitrary, but vectors of length greater than  will need to be iteratively shifted to the input of .

By taking ten row multipliers we can create a unit  which outputs a  sub-matrix of :

\newpage



Finally, four such units are arranged so that a  sub-matrix of  could be obtained as output:



The 's inputs are twenty vectors from both matrices  and .
Because of hardware constraints --- in particular the number of LUTs on the used device --- a larger arrangement of multipliers would be impractical to implement. The module  is comprised of 400  multiplier units. Figure 3 shows the hierarchy of units used to build .

\begin{figure}[h!]
\centering
\includegraphics[width=3.91in,height=1.82in]{M20x20_2.eps}
\caption{The structure of }
\end{figure}

The  unit can be used iteratively to multiply matrices of arbitrary size, producing  sub-matrices of the output matrix  with each iteration. After inputting twenty rows from matrix  and twenty columns from matrix  and obtaining the desired output, we can simply repeat the process for a set of rows and columns of  and  respectively, until we obtain the entire output matrix :

\begin{tabbing}
Function \=  \\
\>1. Define ,  \\
\>2. for \=  to  do \\
\> \ \ 3. for  to  do \\
\> \>4. if  and   else  \\
\> \>5. if  and   else  \\
\>6. end for end for \\
\>7. for  to  do\\
\> \ \ 8. for  to  do \\
\> \>9.  \\
\>10. end for end for \\
\>11. return  \\
end Function \\
\end{tabbing}

Here


Note that in the naive algorithm , during the main loop (lines 7-10), for each twenty rows read from , the entire matrix  is read. During the whole procedure, matrix  will be read entirely exactly once, while matrix  will be read  times. Methods improving on this number are described in section 6.

Since for almost all practical cases the size  of matrices  will be greater than the parameter , the vectors taken from these matrices will need to be iteratively shifted onto the input of the multiplier ,  elements at a time. Therefore, an efficient way to both store and then use the vectors taken from the matrices is the creation of FIFO type containers made of shift registers.

Let  be a shift register of width  and depth . It means that  can store at most  vectors of length , or equivalently a single vector of length at most . We choose  such that , thus it can store one row or column from the input matrices  or . Let the vector filling  be , where . In practice,  is a queue data structure. In a single step,  outputs a vector of length  and shifts its content by  places. For the  activation, the container will output .  After  activations, the container becomes empty.

One container  is used to store a single row or column of matrices  or  respectively. Connecting twenty of them in parallel, denoted by , we obtain a container that stores twenty rows or column\-s. This is exactly the amount of data the  multiplier structure requires as input in  iteration steps. After  activations  has shifted all its stored data to , broken up into pieces of length  for each activation. Two such  containers are connected to , one for the rows taken from matrix  and one for the columns taken from matrix .

\begin{figure}[h!]
\centering
\includegraphics[width=3.5in,height=1.6in]{tnd.eps}
\caption{The structure of }
\end{figure}

Using  and  in a proper structure, we can execute one iteration cycle of the computation. After filling one  container with the desired twenty rows from matrix  and one  container with the desired twenty columns from matrix , we simply send  activation signals to the containers. This will shift the data onto , which computes the  product matrix in the way described in function . The number of steps in one iteration cycle is .

\section{Experimental determination of parameters}

Now, we turn to the determination of  (how many MA modules should be connected into a single multiplier ). This sets the length of the vectors that we use in the computation in a single step and thus has an effect on many other technical parameters of the design. The goal was to find the greatest number such that the multiplier would still reliably produce the correct dot product in a single clock cycle. Clearly, this number dependents on the used hardware and the clock frequency. For the device used, the chosen clock frequency was 100 MHz, the default frequency provided by the board.

The following experiment was devised to determine the value of :

Let  be a multiplier , called the ``Subject'', and let  be ten more  multipliers, called the ``Examiners''. Informally, the Examiners' duty was to verify the answers given by the Subject to questions they already knew the answer to. The ``questions'' here are test data: two vectors  of length  generated by the following sequence to obtain suitable pseudo-random values:

where .

More formally, let  be a counter that cycles between values , incrementing its value by one with each clock cycle, and returning to value  after . For each clock cycle during the experiment, the following happens depending on the value of :
\begin{itemize}
\item The output of  is checked for equality with the output of . If inequality is detected, then an error is noted.
\item The test data  is currently working on is given to .
\item New test data is given to .
\end{itemize}

\begin{tabbing}
Procedure \= testing \\
\>1. Let  be  multipliers \\
\>2. Let  be the test data generator \\
\>3. Let  \\
\>4. forever \= do \\
\> \>5.  \\
\> \>6.  \\
\> \>7. if  then return ERROR \\
\> \>8.  \\
\> \>9.  \\
\>10. end forever \\
end Procedure \\
\end{tabbing}

\begin{figure}[h!]
\centering
\includegraphics[width=4.5in,height=2.9in]{teszt.eps}
\caption{Activity of testing module when counter's value is }
\end{figure}

Note that the output of  is checked every clock cycle, which yields that  has only a single cycle to calculate its answer to the question it was given in the preceding clock cycle. A given Examiner, however, has ten times more time to work on its test data. Once in every ten clock cycles, new data is given to the Examiner to work on, and its output is only checked nine clock cycles later, just before it is given new input again. This way the Examiners have enough time to compute the correct answer to the question by the time it is needed.

As the initial value for , we have chosen 16, a number small enough to be reasonably expected to pass the criteria set for , but large enough to be of interest. If the experiment reported no error, meaning the Subject was flawlessly able to calculate the dot product for a sufficiently long time, then the value of  was increased and the experiment repeated. After the first error was encountered, meaning the Subject was not able to keep up with the calculations, the largest value was chosen for  for which there were no errors.

On the used device, the largest such value was found to be  at a clock speed of 100 MHz and setting the length of  multipliers to  were able to work error-free for days without interruption.

\section{Computation of large matrices}

In the Section 4 we gave an algorithm for using the described modules for computing the product of large matrices. Following the description, the implemented design would make use of the parallelism offered by the FPGA only in the computation of dot products. Making further use of parallel operations, the design's performance can be significantly improved. In this section we describe the implementation choices made to raise the overall performance.

The biggest factor to consider is the management of data. When computing the product of large matrices, the amount of data to store and to move between the computation modules can easily exceed the size which can be practically stored on the FPGA. Fortunately, as mentioned before, a 256MB DDR2 SODIMM is connected to the board as the main data storage device. A module is generated using the Memory Interface Generator v3.5 intellectual property core provided by Xilinx to implement the logic needed to communicate with the DDR2 RAM. The module is structured hierarchically, connecting the memory device to a user interface. All communication with the device is done through two FIFO queues: one queue to send the command and address signals, while the other queue is used for write data and write data mask (when masking is allowed).

A naive utilization of the memory would be to simply read the required data before each iteration of the computation, and writing the output back after it is finished. An undesirable effect of this approach would be that the design would spend significantly more time with memory management than with the actual computation. The desired result would be that memory management (and all other auxiliary operations) were done during the time interval of the computation. Note that since both the size of the matrices and the multiplier module is fixed, the time the multiplication consumes is a fixed constant, which cannot be lowered. Optimally, the time of the computation should be an upper bound for the running time of the entire design. The difficulty of reaching this optimum lies in the high speed of the multiplier modules compared to the memory module.

One way to resolve the problem caused by slow transmission speed is to increase the amount of data stored on the FPGA. Informally, the main idea is to keep enough data in a prepared state, i.e. by the time the multiplier module finishes all of its computations, we have enough new data to continue working. More formally, let us define the following quantities:
\begin{itemize}
\item Let  be the time necessary to complete one iteration of the computation. As described in the previous sections, this is equal to the depth of the containers .
\item Let , where  is the size of the matrices. () This quantity is already used in algorithm . For the rest of the section, it is practical to think of  and  as  sized block matrices, where each element is a  matrix.
\item Let  be an arbitrary algorithm executing matrix multiplication on  and , including the memory management needed for the computation. Let  be the number of times the algorithm needs to fill a  container, i.e. the number of times it has to read twenty rows or columns from the matrices. Note that completely reading either input matrices once means filling  containers  times, since one  can store twenty rows or columns at a time. Algorithm 's main loop (starting at line 7) reads twenty rows from matrix  (filling a  once) and reads matrix  entirely for each step. Since the loop has  steps, it follows that .
\item Let  be the time it takes to fill a  container. This quantity depends on both the width and depth of the container. The total time  spends on reading from memory to fill the containers is .
\item Let  be the total time the design has to spend with memory management. This is the sum of the time it spends on reading matrices  and  from the memory and the time it spends on writing the product matrix  into the memory. The number of times  has to read  and  from the memory depends on . Note that since the size of the total output matrix  is the same as the size of  and , writing  into the memory takes time equal to reading either matrices once from the memory. In other words, it takes  time. The total time the design has to spend with memory management is .
\item Let  be the time  spends on the computation itself. From the definition of  and  it follows that .
\end{itemize}

The goal here is to reduce  in such a way that the data required for the next iteration of the computation is always ready by the time the previous iteration ends. If this arrangement is achieved then  becomes the upper bound for the running time of the design.

Storing more data on the FPGA can be done by adding more  containers to the design. During an iteration only two such containers are used directly. The rest can be used to load data necessary for the forthcoming iteration steps.

Suppose the design has  pieces of  containers. We assign  of the containers to store rows from matrix , called ``row-stores'', and two of them to store columns from matrix , called ``column-stores''. With this arrangement, we can carry out  iterations of the computation, using up the data stored in  row-stores and one column-store. This leaves one row-store and one column-store to load new data into during the computation. Using the above definitions, the allover computation takes  time.

\begin{figure}[h!]
\centering
\includegraphics[width=4.7in,height=3.13in]{iter.eps}
\caption{Configuration of data stored on the FPGA}
\end{figure}

If we use all  row-stores and one column-store for the computation while the remaining column-store is devoted to loading new columns into, then we would have to load all  row-stores with new rows once we read all the columns before we can continue the computation. This would take  time for each case where we read all the columns but haven't read all the rows yet, which happens  times. In total, it would add  to the running time.

Instead, the computation of the output matrix moves slightly diagonally. See Figure 7. The  row-stores used in the computations store a total of  rows. Initially, the row-stores are filled with rows . New rows are loaded in at a slower pace than columns are. By the time all columns are read once, the contents of the row-stores have shifted exactly to the next segment of data needed, the next  rows. After matrix  is completely read once, the row-stores are filled with rows . Reading rows and columns proceeds in this manner until we've completely read matrix  once. For this reason, it is practical to choose  such that . All together we read matrix   times and matrix  once. During each  iterations shown in Figure 6, twenty new columns and  new rows are loaded into the column-store and row-store currently unused by the computation. When the unused row-store is filled with twenty new rows, it becomes active, to be used in the following iterations. The row-store containing the rows with the least index becomes inactive in the computation and starts accepting the new rows read.

\begin{figure}[h!]
\centering
\includegraphics[width=4.7in,height=2.6in]{comp.eps}
\caption{Progression of computations through matrix }
\end{figure}


\begin{tabbing}
Function \=  \\
\>1. Define ,  \\
\>2. for \=  to  do \\
\>3. for  to  do \\
\> \>4. if  and  then  else  \\
\> \>5. if  and  then  else  \\
\>6. end for end for \\
\>7. Fill the row-stores with rows  \\
\>8. Fill the column-stores with columns  \\
\>9. For  to  \\
\>Do in parallel:  \=perform  iterations of the computation\\
\> \> READ the next 20 columns mod \\
\> \> READ the next  rows mod \\
\> \> WRITE the result of the previous  iterations\\
\>11. return  \\
end Function \\
\end{tabbing}

The possible values for the parameters used in this section depend on the used hardware.

The size of the matrices used in the implementation are determined by parameters  and . The LUTs on the device that comprise the  containers can be configured as  bit deep shift registers. For this reason the matrices are of size . Rows with length  are the largest that can be stored in containers that are one LUT deep, making them any larger would double the number of LUTs needed for creating a . Because of the limited number of LUTs which can be used for storage purposes,  was chosen. This yields that twelve  containers are defined in the design. Dealing with matrices larger than  is part of future work.

For convenience, time quantities are measured in clock cycles at 100MHz, the clock speed of the  multiplier.

The value of  depends on the DDR2 RAM used. The device was used at 200MHz, and has a 64 bit wide physical data bus.

From these values we determine the following parameters:
\begin{itemize}
\item ,
\item ,
\item  clock cycles at 100MHz,
\item  clock cycles at 100MHz,
\item  clock cycles at 100MHz.
\end{itemize}
The goal of  is a\-chieved, meaning that the running time of the design is equal to the time used by the computation.

The speedup provided by the configuration can be shown by comparing its performance to a similar implementation created on a more traditional architecture. A highly optimized C++ program was created for a machine using an Intel E8400 3GHz Dual Core processor with 2GB RAM. The algorithm is strongly specialized for the task, making use of all available options for increasing performance. It uses 64 bit long variables to perform multiplication on 16 pairs of two-digit elements at once in parallel on both processor cores.

The running time of the multiplication of matrices of the same size is over 100 ms. The FPGA implementation, as mentioned above, achieves a runtime of 0.6 ms. On average, a speedup factor of 200 is reached using the described FPGA design.

\section{Future work}

The future course of research will focus on increasing the size of the used matrices.

As mentioned in the previous section, simply increasing the depth  of the  containers would be impractical. Since a single LUT on the device can only be configured as a 32 bit deep shift register, setting  would double the number of LUTs needed for a , and the design is already using well over half of the device's LUTs that can be configured this way (13440 out of 17280, to be exact). Increasing the size of the matrices this way would require the restructuring of both the multiplier module and the algorithm used for memory management.

Instead, the currently implemented module can be used as a basic unit for the multiplication of larger matrices. Then the entries of the large matrices are  blocks.

This also allows for further optimization using Strassen's algorithm. Suppose we double the matrix sizes, interpreting them as matrices with four blocks. Using the classical algorithm, multiplying two  sized matrices would take eight multiplication of the blocks. Using a divide-and-conquer strategy, we can exchange one multiplication for a few extra additions.

where

This algorithm, with its  time complexity, could speed up the design on large matrices. We should note however, that the speed of the extra additions have to be carefully considered. Since the multiplication is already extremely fast, a similar improvement may also be necessary for additions if the overall performance upgrade is to remain significant.


\section*{Acknowledgements}
Research supported by the T\'AMOP 4.2.1/B-09/1/KONV-2010-0007 project and TARIPAR3 project grant Nr. TECH 08-A2/2-2008-0086.




\begin{thebibliography}{99}


\bibitem{c1} F. Bensaali, A. Amira, R. Sotudeh, Floating-point matrix product on FPGA, \emph{IEEE/ACS International Conference on Computer Systems and Applications}, Amman, Jordan, 2007, pp. 466--473.

\bibitem{c2} D. \href{http://www.arnetminer.org/viewperson.do?naid=1131379&name=D.\%2520Coppersmith}{Coppersmith}, S. Winograd, Matrix multiplication via arithmetic progressions, \href{http://www.elsevier.com/wps/find/journaldescription.cws_home/622902/description}{\textit{J. Symbolic Comput.}} \textbf{9,} 3 (1990) 251--280.

\bibitem{c3} N. \href{http://arnetminer.org/viewperson.do?naid=1089695}{Dave}, K. \href{http://arnetminer.org/viewperson.do?naid=1407331}{Fleming}, M. King, M. Pellauer, M. Vijayaraghavan, Hardware acceleration of matrix multiplication on a Xilinx FPGA, \emph{MEMOCODE '07 Proc. 5th IEEE/ACM International Conference on Formal Methods and Models for Co-Design,} Nice, France, 2007, pp. 97--100.

\newpage
\bibitem{c4} Y. Dou, S. \href{http://ce.et.tudelft.nl/person.php?id=2}{Vassiliadis}, G. K. Kuzmanov, G. N. Gaydadjiev, 64-bit floating-point FPGA matrix multiplication, \emph{Proc.  2005 ACM/SIGDA 13th international symposium on Field-programmable gate arrays}, Monterey, CA, USA, 2005, pp. 86--95.

\bibitem{c5} T. J. Earnest, Strassen's Algorithm on the Cell Broadband Engine, 2008,  \href{http://mc2.umbc.edu/docs/earnest.pdf}{http://mc2.umbc.edu/docs/earnest.pdf}

\bibitem{c6} T. \href{http://www.inf.unideb.hu/~herendi/}{Herendi}, Uniform distribution of linear recurrences modulo prime powers, \href{http://www.elsevier.com/wps/find/journaldescription.cws_home/622831/description}{\textit{J. Finite Fields}} \emph{Appl.} \textbf{10,} 1 (2004) 1--23.

\bibitem{c7} T. \href{http://www.inf.unideb.hu/~herendi/}{Herendi}, Construction of uniformly distributed linear recurring sequences modulo powers of  2 (to appear).

\bibitem{c8} A. \href{http://ucsd.academia.edu/AliIrturk/About}{Irturk}, S. Mirzaei, R. Kastner, An Efficient FPGA Implementation of Scalable Matrix Inversion Core using QR Decomposition, \emph{UCSD Technical Report}, CS2009-0938, 2009.

\bibitem{c9} B. \href{http://cs.stanford.edu/people/boyko/}{Kakaradov}, Ultra-fast matrix multiplication, An empirical analysis of highly optimized vector algorithms, \emph{Stanford Undergraduate} \href{http://surj.stanford.edu/2004/pdfs/kakaradov.pdf}{\textit{Research Journal}} \textbf{3} (2004) 33--36.

\bibitem{c10} M. \href{http://www.ece.rice.edu/~marjan/}{Karkooti}, J. R. \href{http://www.ece.rice.edu/~cavallar/}{Cavallaro}, C. Dick, FPGA implementation of matrix inversion using QRD-RLS algorithm, \emph{Proc. 39th Asilomar Conference on Signals, Systems, and Computers}, Pacific Grove, CA, USA, 2005, pp. 1625--1629.

\bibitem{c11} S. M. Qasim, A. A. \href{http://faculty.ksu.edu.sa/atelba/}{Telba}, A. Y. AlMazroo, FPGA design and implementation of matrix multiplier architectures for image and signal processing applications, \emph{IJCSNS International Journal of} \href{http://www.ijcsns.org/}{\textit{Computer Science}} \emph{and Network Security}  \textbf{10,} 2 (2010) 168--176.

\bibitem{c12} V. \href{http://www.math.uni-konstanz.de/~strassen/}{Strassen}, Gaussian elimination is not optimal, \href{http://www.springer.com/mathematics/numerical+and+computational+mathematics/journal/211}{\textit{Numer. Math.}} \textbf{13} (1969) 354--356.

\end{thebibliography}

\bigskip
\rightline{\emph{Received: May 17, 2011 {\tiny \raisebox{2pt}{\!}} Revised: October 11, 2011}} 
\end{document}
