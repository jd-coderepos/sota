\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{amsfonts}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} \usepackage{multirow}
\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{iccv}
\usepackage[caption = false]{subfig}
\usepackage[capitalize,noabbrev]{cleveref}



\usepackage{tikz}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=0.5pt] (char) {#1};}}
\iccvfinalcopy 

\def\iccvPaperID{12333} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\providecommand{\keywords}[1]
{
  \small	
  \textbf{\textit{Keywords---}} #1
}

\begin{document}

\title{: Range Regularization for Model Compression and Quantization}

\author{Arnav Kundu
\and
Chungkuk Yoo
\and
Srijan Mishra
\and
Minsik Cho
\and
Saurabh Adya\\
Apple Inc.\\
\{a\_kundu, ckyoo, srijan, minsik, sadya\}@apple.com
}

\maketitle
\vspace{0.3in}
\begin{abstract}Model parameter regularization is a widely used technique to improve generalization, but also can be used to shape the weight distributions for various purposes. In this work, we shed light on how weight regularization can assist model quantization and compression techniques, and then propose range regularization () to further boost the quality of model optimization by focusing on the outlier prevention.
By effectively regulating the minimum and maximum weight values from a distribution, we mold the overall distribution into a tight shape so that model compression and quantization techniques can better utilize their limited numeric representation powers. We introduce  regularization, its extension margin regularization and a new soft-min-max regularization to be used as a regularization loss during full-precision model training. Coupled with state-of-the-art quantization and compression techniques, models trained with  perform better on an average, specifically at lower bit weights with 16x compression ratio. We also demonstrate that  helps parameter constrained models like MobileNetV1 achieve significant improvement of around 8\% for 2 bit quantization and 7\% for 1 bit compression.
\end{abstract}\\
\keywords{Regularization, Quantization, Compression, Outlier prevention}

\section{Introduction}
Deep neural networks have become popular in human computer interaction, anomaly detection, financial markets, etc. Since a lot of applications running these models run on edge devices, running these compute-intensive models requires a balance of accuracy, latency, power efficiency and size for these models. Quantization of neural networks is an effective way of reducing the power, latency, and size of these models. This requires that these quantized models are executed on
specialized platforms with low-bit supports and at the cost of accuracy drop \cite{hawq,haq}. The accuracy degradation can be more significant with post-training quantization on weights for large models \cite{bengio2013estimating}. Therefore, training time quantization techniques are preferred over post-training approaches~\cite{bengio2013estimating,zhou2016dorefa}, which allow usage of stochastic gradient descent to quantize and optimize the weights (i.e., mapping each floating point number to a set of discrete bins according to the precision target). Since these bits are decided very simply on the basis of the range of the weights, weight range ends up affecting the performance of quantization itself. 









\begin{figure}[t]
\vskip 0.0in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/exmp_weight_dist_r2.pdf}}
\vskip -0.0in
\caption{Example weight distribution for weight quantization with three bins. Black: without a range regularizer and \textcolor{red}{Red}: with a range regularizer.}
\label{fig_weight_dist}
\end{center}
\vskip -0.0in
\end{figure}

Setting an effective bin size for quantization becomes a hard problem due to outliers in weights. For example, in \cref{fig_weight_dist}, if we set the bin size as entire weight range divided by a desired number of bins, most of weights fall into the zero bin and only few weights are quantized to -1 and 1. As most of weights in the quantized model are zero, its accuracy would drop significantly. This problem gets worse for low bit quantization and compression such as one or two bit quantization. In state-of-the-art quantization works, statistical approaches are used to initialize a bin size during quantization-aware training. They clip outlier weights if they are too small or too large assuming the weight distribution would follow the normal distribution ~\cite{esser2019learned, lee2021network}. But, the model weights can have a long-tail distribution so that this statistical initialization would not be effective. Also the clip function is not differentiable, so most of quantization works approximate its gradient as 1 ~\cite{bengio2013estimating}. However, this approximation makes quantization unstable with gradient explosion ~\cite{sakr2022optimal}. 

Model compression using techniques like Deep Compression \cite{HanMD15} and DKM \cite{cho2021dkm} can also get compromised in accuracy due to outliers in weights. This is because the centroids computed in both these methods by the clustering algorithm will get skewed due to the presence of the outliers. 

We hypothesize that pre-trained models with fewer outliers in weights will yield better quantized or compressed models. Therefore, in this paper, we propose:
\begin{itemize}
\item
{Range-regularization (), a concept to ensure that the learnt layer weights do not have any outliers during the pre-training stage (i.e, better weight distribution), which allows model compression and quantization techniques to effectively optimize the pre-trained network.}
\item
{3 different techniques to perform range regularization and show the characteristics of weight distributions generated by each of them}.
\item
{We use  for training ResNet-18 \cite{he2016deep}, MobileNetV1 \cite{howard2017mobilenets}, and MobileNetV2 \cite{sandler2018mobilenetv2} and then use the trained model to improve the accuracy of the quantized (2bit, 4 bit) and compressed models (1bit, 2bit, 4bit) with state of the art quantization and compression algorithms. \textbf{We quantize/compress the entire model including the first and last layer.}}
\item
{We use  for training ResNet-18 and MobileNetV1 and then use the trained model to improve the accuracy of multi-dimensional compressed (2bit-2dim, 4bit-4dim) models.}
\item
{We also apply  on MobileBERT \cite{sun2020mobilebert} for Question-answering NLI \cite{rajpurkar2016squad} to show that the proposed method scales across tasks.}
\end{itemize}






 \section{Range Regularization}
We introduce  as a regularization technique to reduce the range of weights for every layer. Our technique keeps the quantization/compression process simple without any intrusion to the actual training flow just like  and  regularization. Contrary to  or  regularization,  only affects the outliers in weights by penalizing them while maintaining accuracy of the full precision model. While, such a constrained optimization can be formulated in various ways we propose 3 different ways of defining . We start from basic  loss, extend it to margin loss and finally introduce soft-min-max loss for adding  to the training loss. These formulations are further elaborated as follows:


\begin{itemize}
    \item  regularization: This method tries to penalize only the outliers in an iterative manner during training by adding  as a regularization term for every layer in the model.
    
    \item Margin range-regularization: This is an extension of  regularization, where, we define a margin for the range of allowed weights. Any weight outside this margin is penalized. In addition, we also penalize the width of the margin to ensure that the range of the overall weight distribution is small. The regularization loss for a given weight W is shown in \cref{eq:margin}. Here  is a learnable parameter per layer.
    
    \item Soft-min-max range-regularization: In this approach, we propose an asymmetric range regularization, to eliminate the constraint on the magnitude of weights and strictly enforce it on the range of weights. We hypothesize that such a technique will improve asymmetrically quantized models using techniques such as DKM \cite{cho2021dkm}. The loss for a given weight W is described in \cref{eq:sminmax}. Here temperature  is a learnable parameter per layer.  term in the regularization loss , encourages temperature  to increase during training time optimization process to approach hard-min-max regularization towards the end of train time optimization.
    
\end{itemize}


All the above mentioned range-regularization techniques were employed during training time of the base model itself and not during quantization or compression. This was done because the purpose of the  is to provide effective initial weights for compression or quantization. Range-regularization was added to the cross entropy loss by multiplying a regularization weight to adjust importance between the regularization and the loss during the base training phase. This regularization weight has an initial value which differs across the three regularizers. Employing range-regularization during base model training and not during quantization or compression also ensures extensibility of range-regularization with any quantization or compression technique.  \section{Experiment}
\label{experiment}
\begin{table*}[h]
\caption{Top-1 accuracies (\%) of ResNet18 (RN), MobileNet-V1 (MN1) on ImageNet-1K with weight and activation quantization.}
\label{table_quant_exp}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccccc}
\toprule
Pre-train (FP32) & \multicolumn{2}{c}{RN} & \multicolumn{2}{c}{MN1} & \multicolumn{2}{c}{MN2} \\
\midrule
\circled{0} w/o     & \multicolumn{2}{c}{69.76} & \multicolumn{2}{c}{70.92} & \multicolumn{2}{c}{71.88}\\
\circled{1} R\_Linf    & \multicolumn{2}{c}{70.15} & \multicolumn{2}{c}{70.59} & \multicolumn{2}{c}{70.75}\\
\circled{2} R\_M    & \multicolumn{2}{c}{70.08} & \multicolumn{2}{c}{70.28} & \multicolumn{2}{c}{70.94}\\
\circled{3} R\_SMM    & \multicolumn{2}{c}{69.84} & \multicolumn{2}{c}{70.34}  & \multicolumn{2}{c}{70.75}\\
\midrule
Quant Method & 2bit & 4bit & 2bit & 4bit & 2bit & 4bit\\
\midrule
PACT    & 51.97 & 66.90 & 22.39 & 64.64 & 21.02 & 65.23\\
PACT + R\_Linf    & 55.26 & \textbf{68.45} & 35.99 & \textbf{68.40} & 36.55 & 67.94\\
PACT + R\_M    & \textbf{56.24} & 68.30 & \textbf{40.41} & \textbf{68.40} & \textbf{37.86} & \textbf{68.12}\\
PACT + R\_SMM    & 55.64 & 68.36 & 33.26 & 68.32 & 35.92 & 67.98\\
\midrule
LSQ from \circled{0} & 58.33 & \textbf{69.90} & 38.78 & 69.00 & 33.78 & \textbf{69.00}\\
LSQ from \circled{1} & 62.23 & 69.55 & 43.05 & 69.28 & 39.05 & 68.85\\
LSQ from \circled{2} & 62.25 & 69.56 & 40.85 & \textbf{69.64}  & \textbf{39.30} & 68.46\\
LSQ from \circled{3} & \textbf{62.47} & 69.45 & \textbf{46.72} & 69.37 & 38.04 & 68.98\\
\midrule
EWGS from \circled{0} & 65.42 & \textbf{70.19} & 59.81 & 69.61 & 55.69 & \textbf{70.21}\\
EWGS from \circled{1} & \textbf{65.72} & 70.17 & 59.43 & 69.32  & 56.46 & 69.73\\
EWGS from \circled{2} & 64.27 & 69.77 & 60.18 & 69.63  & \textbf{57.39} & 69.88\\
EWGS from \circled{3} & 64.94 & 69.80 & \textbf{60.63} & \textbf{69.79} & 56.65 & 69.54\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}

\subsection{Experiment settings}
\subsubsection{Pre-training from scratch with Range Regularizers}
We train ResNet-18\cite{he2016deep} and MobileNet-V1\cite{howard2017mobilenets} on ImageNet 1K\cite{deng2009imagenet} with proposed Range Regularizers on two x86 Linux machines with eight GPUs to get pre-trained models before model compression and quantization-aware training. We set initial learning rates to 1.0, 0.4 and 0.2 for ResNet-18, MobileNet-V1 and MobileNet-V2 respectively. We use SGD with 0.9 of momentum and 1e-4 of weight decay with Nesterov. Range Regularizer weights for each regularizers are set to 0.01. For Margin range regularizer, the margin threshold is initialized with 2x the standard deviation . In Soft-min-max regularizer training, the learnable parameter  is initially set to 0.1. For comparison, we use pre-trained models of Resnet-18 from Torchvision. As Torchvision did not provide a pre-trained model for MobileNet-V1, we trained MobileNet-V1 without Range Regularizer using the same settings above. It can be observed from Table \ref{table_quant_exp} that  doesnt significantly affect the performance of the full precision models as well therefore provides a strong initial checkpoint for the model to be quantized. In addition, for a large model like ResNet-18  also shows a regularization effect similar to weight decay, therefore, better validation accuracy than the one without .
\subsubsection{Model compression and quantization}
Range Regularizer is not a model compression nor quantization method. It regularizes weights during training of the base model from scratch. To evaluate the effectiveness of the regularizers with model compression and quantization, we apply state-of-the-art compression/quantization techniques, DKM\cite{cho2021dkm}, LSQ\cite{esser2019learned}, and EWGS\cite{lee2021network}, to the pre-trained model with Range Regularizer. 
Except EWGS, since other works do not provide official implementation, we had to implement those techniques ourselves. We follow the same hyper-parameters used in the works, but \textbf{we apply compression and quantization for all layers (and activations in case of quantization) including the first and last layers.} It is important to compress/quantize all layers including first and last layers considering computation burden at the first layer with a large convolutional filter size such as 7x7 convolutional kernal size of the first layer in ResNet and the large number of weights in the last linear layer, e.g., 1.2M of weights in the last layer of MobileNet-V1 which has 4.2M of weights in total. We have demonstrated this burden in Table \ref{table_model_size} for more clarity. We represent  range-regularization as , Margin range-regularization as  and Soft-min-max range-regularization as  in the results.
\begin{table}[t]
\caption{Model size of compressed MobileNet-V1 (in M bytes). F: first layer, L: last layer} \label{table_model_size}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}



\begin{tabular}{lcc}
\toprule
bit & Quant excl. F \& L & Quant all \\
\midrule
32bit & 16.1   & 16.1 \\
\midrule
4bit & 7.1   & 4.2 \\
2bit & 5.6   & 2.2 \\
1bit & 4.8   & 1.2 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


\subsection{Model quantization}
We apply state-of-the-art quantization techniques like PACT \cite{choi2018pact} while training the models from scratch using . For other quantization aware training methods like, EWGS \cite{lee2021network} and LSQ \cite{esser2019learned} we initialize the model to pre-trained ResNet-18 (RN) \cite{he2016deep}, MobileNet-V1 (MN1) \cite{howard2017mobilenets} and MobileNet-V2 (MN2) \cite{sandler2018mobilenetv2} with range-regularization. As shown in \cref{table_quant_exp}, range-regularization helps the quantization techniques in improving their accuracy, especially for extremely low bit quantization such as at 2 bit. For all models and all quantization methods, models initialized with range-regularization show considerably higher accuracy than original quantization works for 2 bit quantization such as 58.33\% vs. 62.47\%, 38.78\% vs. 46.72, and 33.78\% vs. 39.30\% for ResNet-18, MobileNet-V1, and MobileNet-V2, respectively.


\subsection{Model compression}
We evaluate the effectiveness of range-regularization for compression with the state-of-the-art compression technique, DKM \cite{cho2021dkm}, for ResNet-18 and MobileNet-V1. The bit-dim ratio,  is an important factor in the DKM algorithm which effectively defines the kind of compression a DKM palettized model would see. We ran these experiments for both scalar and vector palettization. For scalar palettization() we ran 1 bit, 2 bit and 4 bit compression. These experiments would yield roughly 32x, 16x and 8x compressed models respectively. \cref{table_comp_exp} shows that range-regularization significantly improves accuracy from original scalar palettized DKM 1 bit, 2 bit and 4 bit models. We also expand the application of range-regularization to vector palettization() DKM \cite{cho2021dkm} as demonstrated in \cref{table_comp_exp_multi}. For these experiments, we kept the effective bit-dim ratio,  equivalent to 1 so as to see variation across the most heavily compressed model which would be close to 32x compressed. Since a vector palettized model will require range constraining for all dimensions, we applied multi-dimensional range-regularization for all layers that would be palettized during compression. For vector palettized ResNet-18 there is an average absolute improvement of  using models pre-trained with range-regularization, and for vector palettized MobileNet-V1 it the gain ranges from 5\%(2 bit) to 3\% (4 bit). 

Finally we also validated that  scales to other domains as well by applying it in compressing MobileBERT \cite{sun2020mobilebert}. For Question Answering (QNLI) \cite{rajpurkar2016squad} using MobileBERT, the proposed regularization improved the performance of the model by 2\% absolute as demonstrated in Table \ref{exp_mobilebert}. Note that we applied  to a QNLI fine-tuning task based on a pre-trained MobileBERT \cite{wolf2020transformers}. It might be necessary to apply  to the entire training task of MobileBERT from scratch so that  would have more chances to get effective weight distribution for model compression. Through these experiments across a variety of tasks(Image Classification, Question Answering etc.) we can also see that the application of the proposed range-regularizers is task-invariant and yields solid results across domains.

\begin{table}[t]
\caption{Top-1 accuracies(\%) of compression using DKM and range-regularization with ResNet18 (RN), MobileNet-V1 (MN1) on ImageNet.} \label{table_comp_exp}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{llcccc}
\toprule
Method &  & RN & MN1\\
\midrule
32bit &   & 69.76 & 70.92\\


\midrule
\multirow{4}{*}{DKM 1-bit, 1-dim}    & None        & 58.97 & 45.54\\
                        & R\_Linf    & 59.52 & 49.74 \\
                        & R\_M    &  \textbf{59.70} & 47.21\\
                        & R\_SMM    & 59.27 & \textbf{52.58} \\
                        \midrule
\multirow{4}{*}{DKM 2-bit, 1-dim}    & None     & 67.64 & 65.95 \\
                                     & R\_Linf  & 68.53 & 67.06 \\
                                     & R\_M     & 68.33 & 67.50 \\
                                     & R\_SMM   & \textbf{68.63} & \textbf{67.62} \\
                        \midrule
\multirow{4}{*}{DKM 4-bit, 1-dim}    
                        & None     & 70.22 & 69.29 \\
                        & R\_Linf    & 70.34 & 69.43 \\
                        & R\_M    & 70.33 & 68.52 \\
                        & R\_SMM    & \textbf{70.52} & \textbf{69.63} \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{table}[t]
\caption{Top-1 accuracies(\%) of compression using multi-dimensional DKM and range-regularization with ResNet18 (RN), MobileNet\_V1 (MN1) on ImageNet.} \label{table_comp_exp_multi}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{llcccc}
\toprule
Method &  & RN & MN1 \\
\midrule
32bit &   & 69.76 & 70.92 \\
\midrule
\multirow{4}{*}{DKM 2-bit, 2-dim}    & None        & 63.52 & 48.16 \\
                        & R\_Linf    & 64.69 & 53.91 \\
                        & R\_M    &  \textbf{64.70} & 53.95\\
                        & R\_SMM    & 64.64 & \textbf{53.99}\\
                        \midrule
\multirow{4}{*}{DKM 4-bit, 4-dim}    & None        & 64.89 & 58.55\\
                        & R\_Linf    & 65.31 & \textbf{61.40}\\
                        & R\_M    & 65.90 & 59.70\\
                        & R\_SMM    & \textbf{66.10} & 60.05 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}









\begin{table}[t]
\caption{Question-answering NLI (QNLI) accuracies of MobileBERT using single dimension DKM}
\label{exp_mobilebert}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccc}
\toprule
Method &Pre-train & 1-bit & 2-bit \\
\midrule
DKM    & 90.41 & 61.34 & 80.12 \\
DKM + R\_Linf  & 90.66 & \textbf{63.17} & \textbf{82.13}        \\
DKM + R\_M  & 89.09 & 61.80 & 80.98        \\
DKM + R\_SMM & 90.83 & 61.49 & 80.87        \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}



\subsection{Weight distribution}

\begin{figure*}[t]
\subfloat[conv1]{
\includegraphics[width=7in]{figures/conv1.png}}\\
\subfloat[block2-conv1]{
\includegraphics[width=7in]{figures/block2.conv1.png}}\\
\subfloat[block2-downsample]{
\includegraphics[width=7in]{figures/block2.downsample1.png}}\\
\subfloat[fc]{
\includegraphics[width=7in]{figures/fc.png}}\\
\caption{Weight distribution of various layers of ResNet-18 during training time using no range regularization and the proposed methods. Final weight distributions of fc layer with R\_SMM  loss regularizer shows that it can produce non-zero centered distributions.}
\label{fig:weight_dist}
\end{figure*}


To understand behavior of range-regularization, we analyse weight distribution change during the pre-training stage of ResNet-18 with and without range-regularization. Figure \ref{fig:weight_dist} shows the weight distribution change of the first layer (conv1), first convolution of second residual block (block2.conv1), first downsampling layer in second residual block and the final linear layer of ResNet-18. For the no range regularization case (original) we observe that all the demonstrated weights follow a normal distribution with outliers distributed wide apart. Contrary to that, the weights of the model trained with range-regularization follows a more uniform distribution as expected in \cite{kure}. We observe the same trend as shown in \cref{table_resnet_dist}. The weight range for each layer with range-regularization is less than the model trained without range-regularization while both models achieve similar accuracy, 70.15\% and 69.76\%  with and without range-regularization respectively, in full-precision evaluation. Therefore, we demonstrate that range-regularization effectively removes the outliers in weights without noticeable accuracy regression in a pre-training stage. Infact, from \cref{table_resnet_dist} it can also be observed that even the standard deviation of the weights for most of the layers are the same with and without range-regularization. This indicates that the proposed approach doesn't cause a major shift in most of the weights, therefore the ensuring minimal regression to the accuracy of the pre-trained full precision model. In addition, for ResNet-18, range regularization shows some effect in addressing over-fitting as seen from the pre-train full precision metrics in \cref{table_quant_exp}. Contrary to L-Inf and margin range regularization, Soft-min-max allows for a skewed distribution of weights as seen for the  layer in Figure \ref{fig:weight_dist}.



\begin{table}[t]
\caption{Weight range and standard deviation for each layer in ResNet-18 (max(weights) - min(weights)) after a pre-training stage.} \label{table_resnet_dist}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{Layer} & \multicolumn{2}{c}{weight range} & \multicolumn{2}{c}{weight std}\\
 & w/  & w/o  & w/  & w/o \\
\midrule
conv1 &  0.63 & 1.86 & 0.11 & 0.13\\
1.0.conv1 &   0.32 & 1.42 & 0.05 & 0.05 \\
1.0.conv2 &   0.20 & 0.77 & 0.04 & 0.05 \\
1.1.conv1 &   0.20 & 1.24 & 0.04 & 0.05 \\
1.1.conv2 &   0.17 & 0.68 & 0.03 & 0.04 \\
2.0.conv1 &   0.23 & 0.66 & 0.04 & 0.04 \\
2.0.conv2 &   0.25 & 0.77 & 0.03 & 0.03 \\
2.0.downsample &   0.25 & 1.56 & 0.05 & 0.07 \\
2.1.conv1 &   0.22 & 0.82 & 0.03 & 0.03 \\
2.1.conv2 &   0.19 & 0.68 & 0.03 & 0.03 \\
3.0.conv1 &   0.23 & 0.75 & 0.03 & 0.03 \\
3.0.conv2 &   0.22 & 0.66 & 0.02 & 0.03 \\
3.0.downsample &   0.12 & 0.52 & 0.03 & 0.03 \\
3.1.conv1 &   0.19 & 0.55 & 0.02 & 0.02 \\
3.1.conv2 &   0.16 & 0.64 & 0.02 & 0.02 \\
4.0.conv1 &   0.16 & 0.65 & 0.02 & 0.02 \\
4.0.conv2 &   0.16 & 0.63 & 0.02 & 0.02 \\
4.0.downsample &   0.20 & 1.38 & 0.03 & 0.03 \\
4.1.conv1 &   0.15 & 0.46 & 0.02 & 0.02 \\
4.1.conv2 &   0.14 & 0.45 & 0.01 & 0.01 \\
fc &   0.60 & 0.99 & 0.07 & 0.07 \\






\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

 \section{Related Works}
\label{related_works}

\subsection{Model compression}

The simplest and one of the most effective form of compression involves sharing weights within a layer. Deep Compression \cite{HanMD15} introduced k-means clustering based weight sharing for compression. Initially, all weights belonging to the same cluster are share weight of the cluster centroid. During forward pass loss is calculated using the shared weights which are then updated during backward pass. This leads to a loss of accuracy and model train-ability because the weight to cluster assignment is intractable during weight update \cite{yinl2019}. DKM \cite{cho2021dkm} introduces differentiable k-means clustering, therefore making cluster assignments tractable. During forward clustered weights are used, however during backward the gradient is applied on the original weights.

\subsection{Model quantization}
Model quantization reduces the memory footprint of a model by reducing the representative bits per weight for a given model. In this paper we have compared our results with various training time quantization algorithms like EWGS \cite{lee2021network}, LSQ \cite{esser2019learned} and DoReFa \cite{zhou2016dorefa} used in PACT \cite{choi2018pact}. PACT clips activation values with a trainable parameter for activation quantization and uses DoReFa for weight quantization. LSQ quantizes weights and activations with learnable step size (scale or bin size). EWGS applies gradient scaling with respect to position difference in between original full precision weights and quantized weights based on LSQ.

\subsection{Regularization for quantization}
As we mentioned earlier, range regularization is not a quantization or compression technique. It helps compression and quantization by removing outliers in the weight distribution. \cite{kure} show that uniform distribution of weights are more robust to quantization than normally-distributed weights. To this they propose KURE (KUrtosis REgularization) to minimize the kurtosis of weights and ensure a uniform distribution. This method is independent of the quantization bit-width, therefore supports Post-Training Quantization in addition to QAT (Quantization Aware Training). However, this method is best suited for generic models which need to be specifically tuned for a given bit precision use case. To reduce the accuracy drop due to quantization \cite{binregularization} proposes to constrain weights to predefined bins based on the quantization bit-width. However, selecting these bins is a difficult process and the output of the models in very sensitive to the bin selection. In addition to that these methods ignore the effect of quantizing the first and last layers of the models. \section{Conclusion}
In this paper, we introduced range regularization as an effective technique to get rid of outliers in the weight distribution during training. This serves as a good initialization for state of the art quantization aware training and compression strategies, therefore can be coupled with any of them. This helps to augment the accuracy gained from such quantization aware training and compression techniques. With the three proposed formulations of range-regularization we wanted to demonstrate that there can multiple approaches of defining  and the final metric depends on these formulations. We also demonstrated how the proposed method converts a normally distributed weight to a more desired uniform distribution for model quantization. Finally, we establish that range regularization doesn't affect the performance of the floating point models as well and in some over parameterized models it also has a regularization effect to address over-fitting. While being beneficial to model compression and quantization range regularization is sensitive to parameters like range regularization weight, selection of initial margin width for margin range regularization and selection of  for soft-min-max regularization. We also observe that for some quantization techniques like EWGS the incremental benefit of applying range regularization is marginal especially at higher bits. {\small
\bibliographystyle{ieee_fullname}
\bibliography{refs}
}

\end{document}
