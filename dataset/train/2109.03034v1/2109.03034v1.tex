\section{Experiment}
\subsection{Experimental Setup}
\noindent {\bf Datasets.}
We conduct the experiments on two commonly-used datasets: Math23K~\cite{wang-etal-2017-deep-neural} and MAWPS~\cite{koncel-kedziorski_mawps:_2016}. Math23K is a large-scale Chinese dataset that contains 23,162 math word problems and their corresponding expression solutions. MAWPS is a English dataset containing 2,373 problems. All the problems are one-unknown-variable linear problems and can be solved with a single expression.

\vspace{2mm}
\noindent {\bf Baselines.}
We compare our model with the following baselines including the state-of-the-art models: DNS~\cite{wang-etal-2017-deep-neural} uses a vanilla Seq2Seq model to generate expressions. Math-EN~\cite{wang_translating_2018} uses the equation normalization to avoid equation duplication problem. T-RNN~\cite{wang_template-based_2019} applies recursive neural networks to model the tree structures of expressions. S-Aligned~\cite{chiang_semantically-aligned_2019} tracks the semantic meanings of operands with a stack during decoding. Group-ATT~\cite{li_modeling_2019} leverages the attention mechanism to enrich problem representation. Both AST-Dec~\cite{liu_tree-structured_2019} and GTS~\cite{xie_goal-driven_2019} develop a tree-based decoder to generate expressions. Graph2Tree~\cite{zhang_graph--tree_2020} proposes to build a quantity cell graph and a comparison graph to better capture the quantity relationships of the problem. Multi-E/D~\cite{shen_solving_2020} is an ensemble model which combines multiple encoders and decoders. 

\vspace{2mm}
\noindent {\bf Implementation Details.}
We use the PyTorch\footnote{\url{https://pytorch.org/}} implementations and pre-trained language models provided by the Transformers library\footnote{\url{https://github.com/huggingface/transformers}}. Since the Math23K dataset is a Chinese dataset and officially released BART is only for English, we switch to mBART25~\cite{liu_multilingual_2020}, which is a multilingual BART for 25 languages including Chinese. For the MAWPS dataset, we also use mBART25. We optimize our model with AdamW~\cite{DBLP:conf/iclr/LoshchilovH19}. The training hyperparameters are set as follows. We set the batch size to 128, the learning rate to 5e-5 and the warm-up ratio to 0.1. The weight decay is set to 0.01. The number of epochs  for fine-tuning and multi-task training are set to 50. We set beam size  to 10 in beam search and expression bank size to 20 unless otherwise stated. All experiments are carried out on NVIDIA Tesla V100. We use 8 GPUs for training and 1 for testing. For our proposed framework, the training time is 1.5 hours for one epoch and testing time is 15 minutes for the whole test set.

\vspace{2mm}
\noindent {\bf Evaluation Metric.} Both MAWPS and Math23K are evaluated with a metric of ``solution accuracy'', that is, the expression is considered as correct if it induces the same number as the ground-truth. For the Math23K dataset, some baselines are evaluated using the public available test set while others use the results of 5-fold cross-validation. We report our results on both settings. For the MAWPS dataset, models are evaluated with 5-fold cross-validation.

\subsection{Results and Analysis}
Evaluation results of our model and baselines are summarized in Table \ref{tab:main_result}. We observe that: (1) direct fine-tuning of mBART already outperforms the state-of-the-art models on Math23K, which shows the powerful generation ability of mBART. (2) on MAWPS, mBART outperforms most Seq2Seq baselines but is worse than GTS and Graph2Tree. These two models leverage tree structure of expressions during decoding which is critical for math word problem solving. We believe that pre-trained language models would achieve a better performance if combined with structure information, and we leave it as a future work\footnote{One may think that the sequence decoder might not always generate valid expressions. However, we check all expressions generated by mBART and find that 99.9\% are valid.}. (3) \method\ framework further improves mBART and achieves new state-of-the-art results. In particular, \method\ outperforms mBART baselines by more than {\bf 4\%} in all the evaluation settings and also outperforms the previous best models by {\bf 7\%} on Math23K, {\bf 7.4\%} on 5-fold cross-validation Math23K. The improvement over pre-trained mBART demonstrates the effectiveness of our multi-task training framework.

\begin{table}[h]
    \centering
    \resizebox{0.48\textwidth}{!}{
    \begin{tabular}{l|ccc}
    \hline
       Model  & Math23K & Math23K & MAWPS \\
    \hline
       DNS   & - & 58.1& 59.5 \\ 
       Math-EN & 66.7 & - & 69.2 \\
       T-RNN  & 66.9 & - & 66.8\\
       S-Aligned & - & 65.8 & -\\
       Group-ATT & 69.5 & 66.9& 76.1\\
       AST-Dec & 69.0 & -& -\\
       GTS & 75.6& 74.3 & 82.6\\
       Graph2Tree & 77.4& 75.5& 83.7\\
       Multi-E/D & 78.4& 76.9 & -\\
       \hdashline
       mBART  & 80.8 & 80.0& 80.1\\
       Generate \& Rank & {\bf 85.4} & {\bf 84.3}& {\bf 84.0} \\
    \hline
\end{tabular}}
    \caption{Solution accuracy on MAWPS and Math23K.  refers to the result of test set and  denotes the result of 5-fold cross-validation. ``-'' means that the results are not reported in the original papers.} 
    \label{tab:main_result}
\end{table}

\subsection{Ablation Study and Model Analysis}

To better understand our model, we further conduct ablation study on Math23K to show how the proposed components affect performance.



\subsubsection{Effect of Joint Training}
To investigate the effect of joint training, we introduce the baseline of two-stage training (i.e., w/o Joint), which means we first train the generator, then train the ranker, and the modules are trained independently. We also study the effect of joint training on generation and perform comparison between mBART and our generator (i.e., w/o Ranker).  The results are listed in Table \ref{tab:joint}. We can see that the joint training brings 2.2\% improvement compared with the two-stage training and 2.6\% for the generator compared with the mBART trained alone, suggesting that the joint training of generator and ranker benefits each other. Besides, the joint training is more space efficient since we only need to save one unified model rather than two.

\begin{table}[h]
    \centering
    \begin{tabular}{l|c}
        Model & Acc \\
    \hline
        \method & {\bf 85.4} \\
         w/o Joint & 83.2 \\
         w/o Ranker & 83.4 \\
         w/o both (mBART) & 80.8
    \end{tabular}
    \caption{Effect of joint training.}
    \label{tab:joint}
\end{table}



\subsubsection{Effect of Expression Bank Strategy}
We investigate the effect of different strategies to construct the expression bank. Here we choose a random sampling strategy as our baseline, where the set of expressions that appeared in the training data is sampled as the expression bank. We evaluate different strategies with and without online updating and summarize the results in Table \ref{tab:negative_sample}. 

\begin{table}[h]
    \centering
    \resizebox{0.45\textwidth}{!}{
    \begin{tabular}{l|c|c}
        Strategy &  Online & w/o Online \\
    \hline
        Random Sample & 75.2 & 69.7  \\
    \hdashline
        Model & 84.2& 83.2 \\
        Model+Tree & {\bf 85.4} & 83.1\\
    \end{tabular}}
    \caption{Accuracy for different expression bank strategies. The expression bank size is 20 for all settings.}
    \label{tab:negative_sample}
\end{table}



We can see that our strategies outperform the random sampling strategy. Since the ground-truth can not be accessed during model inference, we cannot use the tree-based disturbance to generate candidate expressions as in the training phase. This discrepancy between training and inference leads to poor performance if we only use tree-based disturbance to construct the expression bank. However, combining the  tree-based disturbance and model-based generation strategies, we can obtain better results than the only model-based generation, which gives evidence that the tree-based disturbance contains some informative examples that the generator does not cover and it is possible to improve the performance based on the human knowledge of math expression. 

We can also see that strategies have a performance drop without online updating. We conjecture that without online updating the ranker may tend to memorize existing negative expressions thus generalize poorly on new problems. As for strategies with model-based generation, there is another possible reason: the generator keeps updating during multi-task training, so the previously generated expressions are no longer good samples of the current model, and newly generated expressions are more informative. To summarize, both strategies of constructing the expressions bank and online updating play an important role in the success of the ranker.

\subsubsection{Impact of Expression Bank Size}
We further analyze the impact of expression bank size on the ranker and results are shown in Figure \ref{fig:bank_size}. If the model-based generation is used, performance reaches the best at expression bank size 20. This suggests that the expression bank size should not be too small nor too large. One possible reason may be that the generated expressions cannot cover possible mistakes when the expression bank is too small, and when the expression bank is too large, low-quality expressions may be generated and hinder ranker training. Tree-based disturbance has a similar trend and the best bank size is 10.


\begin{figure}[th]
    \centering
    \includegraphics[width=0.5\textwidth]{fig/length_plot.pdf}
    \caption{Accuracy with different expression bank sizes from 5 to 30.}
    \label{fig:bank_size}
\end{figure}

















\subsubsection{Model Analysis}
In Table \ref{tab:error}, we list how the model accuracy changes with respect to the number of operations in expressions. We do not discuss the case of 6 operators since it has too few examples and high variance. For expressions less than 6 operators, all models perform worse when the expression gets longer. This is as expected since longer expressions require more steps of reasoning and have less data to train. In addition, we also observe that \method\ training has larger improvement over fine-tuned mBART on longer expressions. This implies that our model is more suitable to handle complex problems and expressions.

\begin{table}[t]
    \centering
    \resizebox{0.48\textwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c}
        \#Op & Pro& AST-Dec&G2T& mBART& \method  \\
    \hline
        1 & 17.3 & 82.7 & 85.5 & 90.2& 90.8 (+0.6)\\
        2 &  52.2 & 74.5 & 83.7 & 88.1&90.2 (+2.1)\\
        3 & 19.1 & 59.9 & 71.7 & 71.2&79.1 (+7.9)\\
        4 & 6.6 & 42.4 & 51.5 & 53.0 &63.6 (+10.6)\\
        5 & 3.4 & 44.1 & 38.2 & 41.2&58.8 (+17.6)\\
        6 & 0.9 & 55.6 & 55.6 & 55.6 & 88.8 (+33.2)\\
    \end{tabular}}
    \caption{Accuracy for increasing length of expressions. \#Op is the number of operations in expressions. Pro denotes proportion of expressions with different lengths.}
    \label{tab:error}
\end{table}

Following \citet{liu_tree-structured_2019}, we also examine the performance of our model in different domains. The domain of each problem is defined by whether it contains any keywords of this domain and we use the same keyword list as \citet{liu_tree-structured_2019}. Table \ref{tab:domain} shows the results. We observe the similar pattern that the fine-tuned mBART has limitations in geometry which requires external knowledge such as formulas for the circumference and area of a circle. Interestingly, our proposed model mainly improves on these domains. This suggests that the ranking task may be a better choice to learn and use mathematical knowledge than generating.
\begin{table}[h]
    \centering
    \resizebox{0.47\textwidth}{!}{
    \begin{tabular}{l|c|c|c}
        Domain & Pro& mBART & \method  \\
        \hline
         Distance \& Speed& 11.8 & 83.9& 83.9 \\
         Tracing& 2.7 & 85.2& 85.2\\
         Engineering& 5.8 & 86.2& {\bf 87.9}\\
         Interval& 0.6 & 66.7& 66.7 \\
         Circle Geometry& 1.9 & 73.7 & {\bf 78.9}\\
         Plane Geometry& 1.2 & 75.0 & {\bf 83.3}\\
         Profit& 1.1 & 72.7 & 72.7\\
         Solid Geometry& 1.6 & 81.3 & {\bf 87.5}\\
         Interest Rate&0.9 & 100.0 &100.0\\
         Production & 0.4 & 100.0 & 100.0
    \end{tabular}}
    \caption{Accuracy for different problem domains. Pro denotes the proportion of each domain in the test data. Note that the sum of proportion is not 100\% since there are problems not belonging to any specified domain.}
    \label{tab:domain}
\end{table}
