\documentclass{llncs}
\usepackage[draft]{botex}
\usepackage{verbatim}
\usepackage{tikz}
\usetikzlibrary{shapes,decorations}
\usepackage{prettyref}
\usepackage{algorithmic}
\usepackage{algorithm}

\pagestyle{plain}
\pagenumbering{arabic}
\newrefformat{cond}{Condition \ref{#1}}

\newcommand{\tsys}{\ensuremath{\mathcal{T}}}
\newcommand{\seq}{\ensuremath{\sigma}}
\newcommand{\sched}{\ensuremath{S}}
\newcommand{\backlog}{\ensuremath{\mathbf{B}}}
\newcommand{\backplus}{\ensuremath{\mathbf{B^+}}}
\newcommand{\supp}{\ensuremath{\mathrm{supp}}}
\newcommand{\conp}{\ccconp}

\newcommand{\suchthat}{\ensuremath{\,\,|\,\,}}
\title{Feasibility analysis of sporadic real-time multiprocessor task systems} 
\author{Vincenzo Bonifaci\inst{1} \and Alberto Marchetti-Spaccamela\inst{2}}
\institute{Max-Planck Institut f\"ur Informatik, Saarbr\"ucken, Germany \\
\email{bonifaci@mpi-inf.mpg.de}
\and
Sapienza Universit\`a di Roma, Rome, Italy \\
\email{alberto@dis.uniroma1.it}}

\begin{document}
\maketitle
\begin{abstract}
We give the first algorithm for testing the feasibility of a system of sporadic real-time tasks on a set of identical processors, solving one major open problem in the area of multiprocessor real-time scheduling \cite{Baruah:2009:open}. We also investigate the related notion of schedulability and a notion that we call online feasibility. Finally, we show that discrete-time schedules are as powerful as continuous-time schedules, which answers another open question in the above mentioned survey. 
\end{abstract}


\section{Introduction}


As embedded microprocessors become more and more common, so does the need to design systems that are guaranteed to meet deadlines in applications that are safety critical, where missing a deadline might have severe consequences. In such a real-time system, several tasks may need to be executed on a multiprocessor platform and a scheduling policy needs to decide which tasks should be active in which intervals, so as to guarantee that all deadlines are met. 

The \emph{sporadic task model} is a model of recurrent processes in hard real-time systems that has received great attention in the last years (see for example \cite{Baker:2007,Baruah:2009:open} and references therein). A sporadic task  is characterized by a worst-case compute time , a relative deadline , and a minimum interarrival separation . Such a sporadic task generates a potentially infinite sequence of jobs: each job arrives at an unpredictable time, after the minimum separation  from the last job of the same task has elapsed; it has an execution requirement less than or equal to  and a deadline that occurs  time units after its arrival time. 
A sporadic task system  is a collection of such sporadic tasks. Since the actual interarrival times can vary, there are infinitely many job sequences that can be generated by . 

We are interested in designing algorithms that tell us when a given sporadic task system can be feasibly scheduled, with preemption and migration, on a set of  identical processors. The question can be formulated in several ways: 
\begin{itemize}
\item \emph{Feasibility}: is it possible to feasibly schedule on  processors any job sequence that can be generated by \tsys?  
\item \emph{Online feasibility}: is there an online algorithm that can feasibly schedule on  processors any job sequence that can be generated by \tsys? 
\item \emph{Schedulability}: does the given online algorithm  feasibly schedule on  processors any job sequence that can be generated by \tsys? 
\end{itemize}

\paragraph{Previous work.}
Most of the previous work in the context of sporadic real-time feasibility testing has focused on the case of a single processor \cite{Baruah:2003}. The seminal paper by Liu and Layland  \cite{Liu:1973} gave a best possible fixed priority algorithm for the case where deadlines equal periods (a fixed priority algorithm initially orders the tasks and then -- at each time instant -- schedules the available job with highest priority).  
It is also known that  the Earliest Deadline First (EDF) algorithm, that schedules at any time the job with the earliest absolute deadline, is optimal in the sense that for any sequence of jobs it produces a valid schedule whenever a valid schedule exists \cite{Dertouzos:1974}. Because EDF is an online algorithm, this implies that the three questions of feasibility, of online feasibility and of schedulability with respect to EDF are equivalent. It was known for some time that EDF-schedulability could be tested in exponential time and more precisely that the problem is in \conp\ \cite{Baruah:1990}. 
The above results triggered a significant research effort within the scheduling community and many results have been proposed for specific algorithms and/or special cases; nonetheless, we remark that the feasibility problem for a single processor remained open for a long time and that only recently it has been proved \conp-complete \cite{Eisenbrand:2010}. 


The case of multiple processors is far from being as well understood as the single processor case. For starters, EDF is no longer optimal -- it is not hard to construct feasible task systems for which EDF fails, as soon as .  Another important difference with the single processor case is that here clairvoyance does help the scheduling algorithm: there exists a task system that is feasible, but for which no online algorithm can produce a feasible schedule on every job sequence \cite{Fisher:2009}. Thus, the notions of feasibility and on-line feasibility are distinct.  


On the positive side there are many results for special cases of the problem; however we remark that \emph{no optimal scheduling algorithm is known, and no test -- of whatsoever complexity -- is known} that correctly decides the feasibility or the online feasibility of a task system. This holds also for \emph{constrained-deadline} systems, in which deadlines do not exceed periods. The question of designing such a test has been listed as one of the main algorithmic problems in real-time scheduling \cite{Baruah:2009:open}.



Regarding schedulability, many schedulability tests are known for specific algorithms (see \cite{Baker:2007} and references therein), but, to the best of our knowledge, the only general test available is a test that requires exponential space  \cite{Baker:2007:b}.  





\paragraph{Our results.}
We study the three above problems in the context of constrained-deadline multiprocessor systems and we provide new  results for each of them.

First, for the feasibility problem, we give the first correct test, thus solving \cite[Open Problem 3]{Baruah:2009:open} for constrained-deadline systems. The test has high complexity, but it has the interesting consequence that a job sequence that witnesses the infeasibility of a task system \tsys\ has without loss of generality length at most doubly exponential in the bitsize of \tsys. 

Then we give the first correct test for the online feasibility problem. The test has exponential time complexity and is constructive: if a system is deemed online feasible, then an optimal online algorithm can be constructed (in the same time bound). Moreover, this optimal algorithm is without loss of generality \emph{memoryless}: its decisions depend only on the current (finite) state and not on the entire history up to the decision point (see Section \ref{sec:definitions} for a formal definition). These results suggest that the two problems of feasibility and online feasibility might have different complexity.

For the schedulability problem, we provide a general schedulability test showing that the schedulability of a system by any memoryless algorithm can be tested in polynomial space. This improves the result of Baker and Cirinei \cite{Baker:2007:b} (that provided an exponential space test for essentially the same class of algorithms).


We finally consider the issue of discrete time schedules versus continuous time schedules. The above results are derived with the assumption that the time line is divided into indivisible time slots and preemptions can occur only at integral points, that is, the schedule has to be discrete. In a continuous schedule, time is not divided into discrete quanta and preemptions may occur at any time instant. We show that in a sporadic task system a discrete schedule exists whenever a continuous schedule does, thus showing that the discrete time assumption is without loss of generality. Such  equivalence is known for periodic task systems (i.e. task system in which each job of a task is released exactly after the period  of the task has elapsed); however, the reduction does not extend to the sporadic case and the problem is cited among the important open problems in real-time scheduling  \cite[Open Problem 5]{Baruah:2009:open}. 

\begin{comment}
\begin{table}[t]
\begin{center}
\caption{Summary of the results}
\label{tab:bounds}
\begin{tabular}{lc}
\textbf{Problem} & \textbf{Complexity bound} \\
\hline
Feasibility & EXPSPACE \\
Online feasibility & EXPTIME \\
Schedulability & PSPACE
\end{tabular}
\end{center}
\end{table}
\end{comment}

All our results can be extended to the arbitrary-deadline case, at the expense of increasing some of the complexity bounds. In this extended abstract we restrict to the constrained-deadline case to simplify the exposition. 


Our main conceptual contribution is to show how the feasibility problem, the online feasibility problem and the schedulability problem can be cast as the problem of deciding the winner in certain two-player games of infinite duration played on a finite graph. We then use tools from the theory of games to decide who has a winning strategy. In particular, in the case of the feasibility problem we have a game of imperfect information where one of the players does not see the moves of the opponent, a so-called \emph{blindfold game} \cite{Reif:1984}. This can be reformulated as a one-player (i.e., solitaire) game on an exponentially larger graph and then solved via a reachability algorithm. However, a technical complication is that in our model a job sequence and a schedule can both have infinite length, which when the system is feasible makes the construction of a feasible schedule challenging. We solve this complication by an application of K\"onig's Infinity Lemma from graph theory \cite{Diestel:2005}. This is the technical ingredient that, roughly speaking, allows us to reduce the infinite job  sequences with infinite  length  to finite sequences and ultimately to obtain the equivalence between continuous and discrete schedules. 

The power of our new approach is its generality: it can be applied to all three problems and -- surprisingly -- it yields proofs that are not technically too complicated. 
We hope that this approach might be useful to answer similar questions for other real-time scheduling problems. 



\paragraph{Organization.} The remainder of the paper is structured as follows. In Section \ref{sec:definitions} we formally define the model and set up some common notation. In Section \ref{sec:algorithms} we describe and analyze our algorithms for feasibility and schedulability analysis. The equivalence between continuous and discrete schedules is treated in Section \ref{sec:continuous}, and we finish with some concluding remarks in Section \ref{sec:conclusion}. 

\section{Definitions}
\label{sec:definitions}


Let  and . 
Given a set , with  we denote the set of all -subsets of . 

Consider a task system \tsys\ with  tasks, and  processors; without loss of generality, . Each task  is described by three parameters: a worst-case \emph{compute time} , a \emph{relative deadline} , and a \emph{minimum interarrival time} . We assume these parameters to be positive integers and that  for all .  


\newcommand{\rct}{\ensuremath{\mathbf{C}}}
\newcommand{\ttd}{\ensuremath{\mathbf{D}}}
\newcommand{\tta}{\ensuremath{\mathbf{P}}}
\newcommand{\zero}{\ensuremath{\mathbf{0}}}
\newcommand{\scheddec}{\ensuremath{\mathbf{S}}}
\newcommand{\alg}{\ensuremath{\mathrm{Alg}}}

Let , , , . 
A \emph{job sequence} is a function . The interpretation is that  iff, for each  with , a new job from task  is released at time  with compute time , and no new job from task  is released if . A \emph{legal} job sequence has the additional property that for any distinct  and any , if  and , then . A job sequence is \emph{finite} if  for all  greater or equal to some ; in this case, we say that the sequence has \emph{length} . 

Let . A \emph{schedule} is a function ; we interpret  as the set of those  tasks () that are being processed from time  to time  \footnote{Since , there can be at most one pending job from task . In the arbitrary-deadline case, this can be generalized by considering  jobs.}. We allow that  contains a task  even when there is no pending job from  at time ; in that case there is no effect (this is formalized below). 

A \emph{backlog configuration} is an element of . 
At time , a backlog configuration  \footnote{For notational convenience, here we have reordered the variables so as to have -tuples of triples, instead of triples of -tuples.} will denote the following: 
\begin{itemize}
\item  is the \emph{remaining compute time} of the unique pending job from task , if any; if there is no pending job from task , then ; 
\item  is the \emph{remaining time to deadline} of the unique pending job from task , if any; if there is no pending job from task , or the deadline has already passed, then ; 
\item  is the \emph{minimum remaining time to the next activation} of task , that is, the minimum  such that a new job from task  could be legally released at time . 
\end{itemize}
A configuration  is a \emph{failure configuration} if for some task ,  and .  
\begin{remark}
\label{rmk:state-size}
The set  is finite, and its size is , where  is the input size of  (number of bits in its binary encoding). 
\end{remark}

Given a legal job sequence  and a schedule \sched, we define in the natural way an infinite sequence of backlog configurations . The initial configuration is , and given a backlog configuration , its successor configuration  is obtained as follows: 
\begin{itemize}
\item if , then , where  is 1 if , and 0 otherwise; moreover,  and ; 
\item if , then , where  is defined as above; moreover,  and .  
\end{itemize}
We can now define a schedule  to be \emph{feasible for}  if no failure configuration appears in . Finally, a task system  is \emph{feasible} when every legal job sequence admits a feasible schedule. Stated otherwise, a task system is not feasible when there is a legal job sequence for which no schedule is feasible. We call such a job sequence a \emph{witness} of infeasibility. 

A \emph{deterministic online algorithm} \alg\ is a sequence of functions: 

By applying an algorithm  to a job sequence , one obtains the schedule  defined by  Then  feasibly schedules  whenever  does. 
\newcommand{\malg}{\ensuremath{\mathrm{Malg}}}
A \emph{memoryless} algorithm is a single function ; it is a special case of an online algorithm in which the scheduling decisions at time  are based only on the current backlog configuration and on the tasks that have been activated at time . 


Finally, a task system \tsys\ is \emph{online feasible} if there is a deterministic online algorithm \alg\ such that every legal job sequence from \tsys\ is feasibly scheduled by \alg. We then say that \alg\  is  \emph{optimal} for \tsys, and that \tsys\ is \emph{schedulable} by \alg.  
Online feasibility implies feasibility, but the converse fails: there is a task system that is feasible, but that does not admit any optimal online algorithm \cite{Fisher:2009}. 

\section{Algorithms for feasibility and schedulability analysis}
\label{sec:algorithms}


\subsection{Feasibility}
\label{sec:feas}
We first model the process of scheduling a task system as a game between two players over infinitely many rounds. At round , the first player (the ``adversary'') selects a certain set of tasks to be activated. Then the second player (acting as the scheduler) selects a set of tasks to be processed, and so on. The game is won by the first player if a failure configuration is eventually reached. 

In order to capture the definition of feasibility correctly, the game must proceed so that the adversary has no information at all on the moves of the scheduler; in other words, the job sequence must be constructed obliviously from the schedule. This is because \emph{if the task system is infeasible, then a single witness job sequence must fail all possible schedules simultaneously}. 
Models of such games, where the first player has no information on the moves of the opponent, have been studied in the literature under the name of \emph{blindfold games} \cite{Reif:1984}. One approach to solving these games is to construct a larger one-player game, in which each state encodes all positions that are compatible with at least one sequence of moves for the second player. 

Given a task system \tsys, we build a bipartite graph . Nodes in  () will correspond to decision points for the adversary (scheduler). A node in  or  will encode mainly two kinds of information: (1) the counters that determine time to deadlines and next earliest arrival dates; and (2) the set of all plausible remaining compute times of the scheduler. 

Let  Each of  and  is a copy of , so each node of  is identified by a distinct element from , and similarly for . 
We now specify the arcs of . Consider an arbitrary node  and let  be its identifier, where . Its successors in  are all nodes  for which there is a tuple  such that: 
\begin{enumerate}
\item  for all , where  (this ensures that each task in  can be activated); 
\item , and  for all  (activated jobs cannot be reactivated before  time units); 
\item  and  for all  (counters of other tasks are not affected); 
\item \label{cond:complete1} each  is obtained from some  in the following way:  for all , and  for all  (in every possible scheduler state, the remaining compute time of each activated job is set to the one prescribed by );    
\item  contains all  that satisfy \prettyref{cond:complete1}.    
\end{enumerate}
Now consider an arbitrary node , say . The only successor of  will be the unique node  such that: 
\begin{enumerate}
\item ,  for all  (this models a ``clock-tick''); 
\item \label{cond:complete2} for each , there are an element  and some  such that  for all  and  for all  (each new possible state of the scheduler is obtained from some old state after the processing of at most  tasks); 
\item \label{cond:complete3}
 for each , one has, for all ,  whenever  (this ensures that the resulting scheduler state is valid); 
\item  contains all  that satisfy \prettyref{cond:complete2} and \prettyref{cond:complete3}. 
\end{enumerate}
That is, the only successor to  is obtained by applying all possible decisions by the scheduler and then taking  to be the set of all possible (valid) resulting scheduler states. Notice that because we only keep the valid states (\prettyref{cond:complete3}), the set  might be empty. In this case we say that the node  is a \emph{failure state}; it corresponds to some deadline having been violated. Also notice that any legal job sequence  induces an alternating walk in the bipartite graph  whose -th arc corresponds to .    

Finally, the \emph{initial state} is the node  for which  for all , and for which the only possible scheduler state is \zero. (See \prettyref{fig:state-graph} in the Appendix for a partial illustration of the construction in the case of the task system , for .) Note that, given two nodes of , it is easy to check their adjacency, in time polynomial in . 

\newcommand{\valid}{\ensuremath{\mathrm{valid}}}
\begin{definition}
For a legal job sequence \seq, the set of \emph{possible valid scheduler states} at time  is the set of all  for which there exists a schedule  such that (i)  with no configuration  being a failure configuration, and (ii) the first component of  is . We denote this set by . 
\end{definition}

\begin{lemma}
\label{lem:graph-step}
Let  and let  be the node reached by following for  steps the walk induced by \seq\ in the graph . Then . 
\end{lemma}
\begin{proof}[sketch]
By induction on . When  the claim is true because the only possible scheduler state is the \zero\ state. For larger  it follows from how we defined the successor relation in  (see in particular the definition of ). 
\qed
\end{proof}


\begin{lemma}
\label{lem:reachability}
Task system \tsys\ is infeasible if and only if, in the graph , some failure state is reachable from the initial state.  
\end{lemma}
\begin{proof}[sketch]
If there is a path from the initial state to some failure state, by \prettyref{lem:graph-step} we obtain a legal job sequence \seq\ that witnesses that for some , , that is, there is no valid scheduler state for \seq\ at time ; so there cannot be any feasible schedule for \seq. 

Conversely, if no failure state is reachable from the initial state, for any legal job sequence \seq\ one has  for all  by \prettyref{lem:graph-step}. This immediately implies that no \emph{finite} job sequence can be a witness of infeasibility. We also need to exclude witnesses of infinite length. To do this, we apply K\"onig's Infinity Lemma \cite[Lemma 8.1.2]{Diestel:2005} (also stated in the Appendix). Consider the infinite walk induced by \seq\ in  and the corresponding infinite sequence of nonempty sets of possible valid scheduler states , where . Each scheduler state  () has been derived by some scheduler state in  and so  and  can be thought of as neighbors in an infinite graph on the disjoint union of  (see Figure \ref{fig:koenig-valid} in the Appendix). Then K\"onig's Lemma implies that there is a sequence  (with ) such that for all ,  is a neighbor of . This sequence defines a feasible schedule for .
\qed 
\end{proof}



\begin{theorem}
\label{thm:main}
The feasibility problem for a sporadic constrained-deadline task system  can be solved in time , where  is the input size of . Moreover, if \tsys\ is infeasible, there is a witness job sequence of length at most . 
\end{theorem}
\begin{proof}
The graph has  nodes, so the first part follows from Lemma \ref{lem:reachability} and the existence of linear-time algorithms for the reachability problem. The second part follows similarly from the fact that the witness sequence  can be defined by taking  as the set of task activations corresponding to the -th arc on the path from the initial state to the reachable failure state.  
\qed
\end{proof}


\newcommand{\reach}{\textsc{Reach}}

\begin{algorithm}[t]
\caption{Algorithm for the feasibility problem}
\label{alg:feasibility}
\begin{algorithmic}
\FORALL{failure states }
\IF{\reach(, , )} \RETURN \textbf{infeasible} \ENDIF
\ENDFOR 
\RETURN \textbf{feasible}
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[t]
\caption{\reach(, , )}
\label{alg:reach}
\begin{algorithmic}
\IF{} \RETURN \textbf{true} if , \textbf{false} if  \ENDIF
\IF{} \RETURN \textbf{true} if , \textbf{false} otherwise \ENDIF
\FORALL{}
\IF{\reach(, , ) \textbf{and} \reach(, , )}
	\RETURN\textbf{true}
\ENDIF
\ENDFOR
\RETURN\textbf{false}
\end{algorithmic}
\end{algorithm}



We can in fact improve exponentially the amount of memory needed for the computation. The idea is to compute the state graph as needed, instead of storing it explicitly (Algorithm \ref{alg:feasibility}). We enumerate all failure nodes; for each failure node , we check whether there exists a path from  to  in  by calling the subroutine \reach\ (Algorithm \ref{alg:reach}). This subroutine checks recursively whether there is a path from  to  of length at most  by trying all possible midpoints . Some readers might recognize that \reach\ is nothing but Savitch's reachability algorithm \cite{Savitch:1970}. This yields the following improvement. 

\begin{theorem}
The feasibility problem for a sporadic constrained-deadline task system  can be solved in space , where  is the input size of . 
\end{theorem}
\begin{proof}
Any activation of Algorithm \ref{alg:reach} needs  space, and the depth of the recursion is at most .
\qed
\end{proof}

\subsection{Online feasibility}
\label{sec:onlinefeas}
An issue with the notion of feasibility as studied in the previous section is  that, when the task system turns out to be feasible, one is still left clueless as to how the system should be scheduled. The definition of online feasibility (see Section \ref{sec:definitions}) addresses this issue. 
It could be argued from a system design point of view that one should focus on the notion of online feasibility, rather than on the notion of feasibility. 
In this section we discuss an algorithm for testing online feasibility. 





The idea is again to interpret the process as a game between the environment and the scheduler, with the difference that now the adversary can observe the current state of the scheduler (the remaining compute times). In other words, the game is no longer a blindfold game but a perfect-information game. We construct a graph  where  and . The nodes in  are decision points for the adversary (with different outgoing arcs corresponding to different tasks being activated) and the nodes in  are decision points for the scheduler (different outgoing arcs corresponding to different sets of tasks being scheduled). There is an arc  if  for some tuple  of jobs that can legally be released when the backlog configuration is ; notice the crucial fact that whether some tuple  can legally be released can be decided on the basis of the backlog configuration  alone. There is an arc  if  and  is a backlog configuration that can be obtained by  after scheduling some subset of tasks; again this depends only on  and . In the interest of space we omit the formal description of the adjacency relation (it can be found in the Appendix). 

The game is now played with the adversary starting first in state . The two players take turns alternately and move from state to state by picking an outgoing arc from each state. The adversary wins if it can reach a state in  corresponding to a failure configuration. The scheduler wins if it can prolong play indefinitely while never incurring in a failure configuration. 
\begin{lemma}
\label{lem:onlinefeas}
The first player has a winning strategy in the above game on  if and only if \tsys\ is not online feasible. Moreover, if \tsys\ is online feasible, then it admits an optimal memoryless deterministic online algorithm. 
\end{lemma}
\begin{proof}[sketch]
If the first player has a winning strategy , then for any online algorithm \alg, the walk in  obtained when player 1 plays according to  and player 2 plays according to \alg, ends up in a failure configuration. But then the job sequence corresponding to this walk in the graph (given by the odd-numbered arcs in the walk) defines a legal job sequence that is not feasibly scheduled by \alg. 

If, on the other hand, the first player does not have a winning strategy, from the theory of two-player perfect-information games it is known (see for example \cite{Graedel:2002,McNaughton:1993}) that the second player has a winning strategy and that this can be assumed to be, without loss of generality, a deterministic strategy that depends only on the current state in  (a so-called memoryless, or positional, strategy). Hence, for each node in  it is possible to remove all but one outgoing arc so that in the remaining graph no failure configuration is reachable from . The set of remaining arcs that leave  implicitly defines a function from  to , that is, a memoryless online algorithm, which feasibly schedules every legal job sequence of \tsys.
\qed  
\end{proof}

\begin{theorem}
The online feasibility problem for a sporadic constrained-deadline task system  can be solved in time , where  is the input size of . If \tsys\ is online feasible, an optimal memoryless deterministic online algorithm for \tsys\ can be constructed within the same time bound. 
\end{theorem}
\begin{proof}
We first construct  in time polynomial in . We then apply the following inductive algorithm to compute the set of nodes  from which player 1 can force a win; its correctness has been proved before (see for example \cite[Proposition 2.18]{Graedel:2002}). Define the set  as the set of nodes from which player 1 can force a win in at most  moves, so . The set  is simply the set of all failure configurations. The set  is computed from  as follows: 

At any iteration either  (and then ) or  contains at least one node. Since there are  nodes, this means that  for some . Because every iteration can be carried out in time , it follows that the set  can be computed within time . By \prettyref{lem:onlinefeas}, \tsys\ is online feasible if and only if . 

The second part of the claim follows from the second part of \prettyref{lem:onlinefeas} and from the fact that a memoryless winning strategy for player 2 (that is, an optimal memoryless scheduler) can be obtained by selecting, for each node , any outgoing arc that does not have an endpoint in .  
\qed
\end{proof}

\subsection{Schedulability}




In the case of the schedulability problem, we observe that the construction of Section \ref{sec:feas} can be applied in a simplified form, because for every node of the graph there is now \emph{at most one} possible valid scheduler state, which can be determined by querying the scheduling algorithm. 
This implies that the size of the graph reduces to . By applying the same approach as in Section \ref{sec:feas}, we obtain the following. 


\begin{theorem}
The schedulability problem for a sporadic constrained-deadline task system  can be solved in time  and space , where  is the input size of . 
\end{theorem}
\begin{proof}[sketch]
Any activation of Algorithm \ref{alg:reach} needs  space, and the depth of the recursion is at most , so in total a space of  is enough. The running time can be found by the recurrence  which gives  and finally . 
\qed
\end{proof}




\section{Continuous versus discrete schedules}
\label{sec:continuous}
In this section we show that, under our assumption of integer arrival times for the jobs, the feasibility of a sporadic task system does not depend on whether one is considering discrete or continuous schedules. 

Let  be the (possibly infinite) set of jobs generated by a job sequence . In this section we do not need to keep track of which tasks generate the jobs, so it will be convenient to use a somewhat different notation. Let , ,  denote respectively the release date, compute time and absolute deadline of a job ; so job  has to receive  units of processing in the interval . A \emph{continuous schedule} for  on  processors is a function  such that:
\begin{enumerate}
\item  for all  and ;
\item  for all .   
\end{enumerate}
Quantity  is to be interpreted as the total amount of processing dedicated to job  during interval . Thus, the first condition forbids the parallel execution of a job on more than one processor; the second condition limits the total volume processed in the interval by the  processors. 
The continuous schedule  is \emph{feasible} for \seq\ if it additionally satisfies 
\begin{enumerate}
\item[3.]  for all .
\end{enumerate}

Finally, a task system \tsys\ is \emph{feasible with respect to continuous schedules} if any legal job sequence \seq\ from \tsys\ has a feasible continuous schedule. For the sake of clarity we call a system that is feasible in the sense defined in Section \ref{sec:definitions} \emph{feasible with respect to discrete schedules}. 

\begin{theorem}
\label{thm:continuous}
A sporadic constrained-deadline task system \tsys\ is feasible with respect to continuous schedules iff it is feasible with respect to discrete schedules. 
\end{theorem}
\begin{proof}
If a task system is feasible with respect to discrete schedules, it is obviously also feasible with respect to continuous schedules: a discrete schedule is just a special case of a continuous schedule where . So assume that a task system \tsys\ is feasible with respect to continuous schedules, but not with respect to discrete schedules. Then there must be a witness job sequence \seq\ that cannot be scheduled by any discrete schedule, but can be scheduled by some continuous schedule. By Theorem \ref{thm:main}, we can assume that \seq\ has some finite length . So it generates a \emph{finite} collection of jobs . But any feasible continuous schedule for a finite collection of jobs can be converted into a feasible discrete schedule \cite{Baruah:1996,Baruah:1990,Horn:1974} (see Appendix for a self-contained proof). This contradicts the initial assumption. 
\qed
\end{proof}


\section{Conclusion}
\label{sec:conclusion}
We have given upper bounds on the complexity of testing the feasibility, the online feasibility, and the schedulability of a sporadic task system on a set of identical processors. It is known that these three problems are at least \conp-hard \cite{Eisenbrand:2010}; however, no sharper hardness result is known. A natural question is to characterize more precisely the complexity of these problems, either by improving on the algorithms given here, or by showing that these problems are hard for some complexity class above \conp. 



\emph{Acknowledgments.}
We thank Sanjoy K.\ Baruah, Nicole Megow and Sebastian Stiller for useful discussions. 
\vspace{-0.3cm}

\bibliography{journals-algo,bonifaci,complexity,game-theory,real-time}
\bibliographystyle{abbrv}

\newpage
\appendix
\section{Appendix}


\subsection{K\"onig's Infinity Lemma}


A \emph{ray} is an infinite graph  of the form


\spnewtheorem*{koenig}{K\"onig's Infinity Lemma}{\rmfamily\bfseries}{\itshape}

\begin{koenig}
Let	 be an infinite sequence of disjoint nonempty finite sets of nodes, and let  be a graph on their union. Assume that every node  in a set  with  has a predecessor  in , so that  is an arc of . Then  contains a ray  with  for all . 
\end{koenig}
\begin{proof}
See for example \cite[Lemma 8.1.2]{Diestel:2005} (the result is stated there in terms of undirected graphs, but the proof works equally well for the directed case.)  
\qed
\end{proof}

\subsection{Definition of  in Section \ref{sec:onlinefeas}}
Recall that  and that , where  and . 
We specify the adjacency relation . Consider an arbitrary node  with . Its successors in  are all nodes  with  such that: 
\begin{enumerate}
\item  for all , where  (this ensures that each task in  can be activated); 
\item , and  for all  (activated jobs cannot be reactivated before  time units); 
\item  and  for all  (counters of other tasks are not affected); 
\item  for all , and  for all  (the remaining compute time of each activated job is set to the one prescribed by );    
\end{enumerate}
Now consider an arbitrary node , say . Its successors in  are all nodes  for which there is some  such that: 
\begin{enumerate}
\item ,  for all  (this models a ``clock-tick''); 
\item  for all  and  for all  (the new remaining compute times are obtained from the old remaining compute times by processing at most  tasks). 
\end{enumerate}




\subsection{Missing details for \prettyref{thm:continuous}}
\begin{proof}[Missing details for \prettyref{thm:continuous}]
We show that any finite job set  has a discrete schedule whenever it has a continuous schedule. 
We setup an instance of a maximum flow problem whose solutions correspond to continuous schedules for \seq, and whose integral solutions correspond to discrete schedules for \seq; see also a similar construction in \cite{Baruah:1996,Baruah:1990,Horn:1974}.   
We build a network  consisting of four types of nodes: 
\begin{enumerate}
\item a source node ; 
\item for every , a node ; 
\item for every job  in \seq, a node ; 
\item a sink node .  
\end{enumerate}
The arcs of the network are as follows:
\begin{enumerate}
\item from  to each Type 2 node, an arc with capacity  ( is the number of processors); 
\item from each Type 2 node  to each Type 3 node  such that , an arc with capacity 1; 
\item from each Type 3 node  to , an arc with capacity .  
\end{enumerate}
Let  be the sum of the capacities of Type 3 edges. 

Assume that a feasible continuous schedule  exists for \seq. We now define a flow by setting the flow on each arc  to be ; the flow on each arc  to ; and the flow on each arc  to . Now conditions (1) and (2) in the definition of continuous schedule for  ensure that the capacity constraints are satisfied. Condition (3) ensures that the amount of flow entering any Type 3 node  is at least 

Notice that some Type 3 node might have more flow entering the node than leaving it; but in that case we can still obtain a feasible flow of the same value by decreasing the incoming flow, and the capacities will not be violated.   

Since all the capacities are integral we know there must be an optimal integral flow; its value will be . From this we can extract a discrete schedule by setting  equal to the flow on arc ; this value must be either 0 or 1 by integrality. The total flow collected at each node  is exactly . We obtain a feasible discrete schedule for \seq, contradicting our initial assumption.  
\qed
\end{proof}


\newcommand{\stext}[1]{\begin{minipage}{2.2cm}\centering #1\end{minipage}}
\begin{figure}
\begin{center}
\tikzstyle{my}=[circle split,draw=black,text centered]\begin{tikzpicture}[->,thick,node distance=4 cm] 
\node (v11) [my,label={above left:}] {\stext{: 0, : 0,\\ : 0, : 0} \nodepart{lower} \stext{: 0, : 0\\ \ \\I }}; 
\node (v21) [right of=v11,my] {\stext{: 0, : 0,\\ : 0, : 0} \nodepart{lower} \stext{: 0, : 0\\ \ \\II }}; 
\node (v24) [above of=v21,node distance=4cm,my] {\stext{: 2, : 2,\\ : 0, : 0} \nodepart{lower} \stext{: 1, : 0\\ \ \\II}}; 
\path (v11) edge (v24); 
\node (v14) [above of=v11] {...}; 
\path (v24) edge [bend left] (v14); 
\path (v11) edge [bend left] (v21);
\path (v21) edge [bend left] (v11);  
\node (v12) [below of=v11,my] {\stext{: 1, : 1,\\ : 1, : 1} \nodepart{lower} \stext{: 0, : 2\\ : 1, : 1\\ : 1, : 2\\I }}; 
\node (v22) [below of=v21,my] {\stext{: 2, : 2,\\ : 2, : 2} \nodepart{lower} \stext{: 1, : 2\\ \ \\II }}; 
\path (v11) edge (v22); 
\path (v22) edge [bend left] (v12); 
\node (v23) [below of=v22,my] {\stext{: 1, : 1,\\ : 1, : 1} \nodepart{lower} \stext{: 0, : 2\\ : 1, : 1\\ : 1, : 2\\II }}; 
\path (v12) edge (v23); 
\node (v13) [left of=v23,my] {\stext{: 0, : 0,\\ : 0, : 0} \nodepart{lower} \stext{\\ \ \\I } }; 
\path (v23) edge [bend left] (v13); 
\node (vxx) [left of=v11,my] {\stext{: 0, : 0,\\ : 2, : 2} \nodepart{lower} \stext{: 0, : 2\\ \ \\II }}; 
\path (v11) edge [bend left] (vxx); 
\node (vyy) [above of=vxx] {...}; 
\path (vxx) edge [bend left] (vyy); 
\end{tikzpicture}
\caption{A subgraph of the graph  for the task system  and . Nodes labeled with ``I'' are in , nodes labeled with ``II'' are in .}
\label{fig:state-graph}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\tikzstyle{my}=[thin,minimum size=2.25cm,circle split,draw=black,text centered]\tikzstyle{vss}=[thin,circle,draw=black]
\begin{tikzpicture}[->,thick,node distance=3.25cm] 
\node (v1) [label={below left:},label={above left:},my] {}; 
\node (v2) [label={below left:},my,right of=v1] {}; 
\node (v3) [label={below left:},my,right of=v2] {}; 
\node (v4) [label={below left:},my,right of=v3] {}; 
\node (v5) [right of=v4,node distance=1.8cm] {}; 
\path (v1) edge (v2); 
\path (v2) edge (v3); 
\path (v3) edge (v4); 
\node (vss1) [vss,below of=v1,node distance=0.5cm] {}; 
\node (vss21) [vss,below of=v2,node distance=0.25cm] {}; 
\node (vss22) [vss,below of=v2,node distance=0.75cm] {}; 
\node (vss31) [vss,below of=v3,node distance=0.20cm] {}; 
\node (vss32) [vss,below of=v3,node distance=0.55cm] {}; 
\node (vss33) [vss,below of=v3,node distance=0.90cm,pin={[pin edge={-}]-120:valid scheduler state}] {}; 
\node (vss41) [vss,below of=v4,node distance=0.25cm] {}; 
\node (vss42) [vss,below of=v4,node distance=0.75cm] {}; 
\path (vss1) edge [dashed] (vss21); 
\path (vss1) edge [] (vss22); 
\path (vss21) edge [dashed] (vss31); 
\path (vss22) edge [] (vss32); 
\path (vss22) edge [dashed] (vss33); 
\path (vss32) edge [] (vss41); 
\path (vss32) edge [dashed] (vss42); 
\node (vss51) [below of=v5,node distance=0.5cm] {}; 
\path (vss41) edge [dashed] (vss51); 
\draw (0,1.5) -- (9.5,1.5) node [pos=0.5,label={above:Walk in  (nodes in )}] { }; 
\end{tikzpicture}
\caption{Illustration of how K\"onig's Lemma applies to the proof of Lemma \ref{lem:reachability}. A prefix of an infinite ray is shown in solid lines.}
\label{fig:koenig-valid}
\end{center}
\end{figure}



\end{document}
