\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{wacv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}




\newcommand{\rarrow}[1]{\overrightarrow{#1\vphantom{b}}}
\usepackage{booktabs}




\def\wacvPaperID{24} 

\wacvfinalcopy 

\ifwacvfinal
\def\assignedStartPage{1} \fi






\ifwacvfinal
\usepackage[breaklinks=true,bookmarks=false]{hyperref}
\else
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}
\fi
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=magenta,
}
\urlstyle{same}
\ifwacvfinal
\setcounter{page}{\assignedStartPage}
\else
\pagestyle{empty}
\fi

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\begin{document}

\title{Oriented Object Detection in Aerial Images with Box Boundary-Aware Vectors}

\author{Jingru Yi, Pengxiang Wu, Bo Liu, Qiaoying Huang, Hui Qu, Dimitris Metaxas\\
Department of Computer Science\\
Rutgers University\\
Piscataway, NJ 08854, USA\\
{\tt\small jy486@cs.rutgers.edu}
}

\maketitle


\begin{abstract}
    Oriented object detection in aerial images is a challenging task as the objects in aerial images are displayed in arbitrary directions and are usually densely packed. Current oriented object detection methods mainly rely on two-stage anchor-based detectors. However, the anchor-based detectors typically suffer from a severe imbalance issue between the positive and negative anchor boxes. To address this issue, in this work we extend the horizontal keypoint-based object detector to the oriented object detection task. In particular, we first detect the center keypoints of the objects, based on which we then regress the box boundary-aware vectors (BBAVectors) to capture the oriented bounding boxes. The box boundary-aware vectors are distributed in the four quadrants of a Cartesian coordinate system for all arbitrarily oriented objects. To relieve the difficulty of learning the vectors in the corner cases, we further classify the oriented bounding boxes into horizontal and rotational bounding boxes. In the experiment, we show that learning the box boundary-aware vectors is superior to directly predicting the width, height, and angle of an oriented bounding box, as adopted in the baseline method. Besides, the proposed method competes favorably with state-of-the-art methods. Code is available at \url{https://github.com/yijingru/BBAVectors-Oriented-Object-Detection}.
\end{abstract}

\section{Introduction}
\label{Introduction}
Object detection in aerial images serves as an essential step for numerous applications such as urban planning, traffic surveillance,  port management, and maritime rescue \cite{azimi2018towards,zhang2018toward}. The aerial images are taken from the bird's-eye view. Detecting objects in aerial images is a challenging task as the objects typically have different scales and textures, and the background is complex. Moreover, the objects are usually densely packed and displayed in arbitrary directions. Consequently, applying the horizontal bounding boxes to oriented object detection would lead to misalignment between the detected bounding boxes and the objects \cite{ding2019learning}. To deal with this problem, oriented bounding boxes are preferred for capturing objects in aerial images.

\begin{figure}[t!]
\begin{center}
   \includegraphics[width=\linewidth]{images/pic1.pdf}
\end{center}
\caption{Oriented bounding box (OBB)  descriptions for (a) baseline method, termed Center++, where  are the width, height and angle of an OBB. Note that  and  of the OBBs are measured in different rotating coordinate systems for each object; (b) the proposed method, where  are the top, right, bottom and left box boundary-aware vectors. The box boundary-aware vectors are defined in four quadrants of the Cartesian coordinate system for all the arbitrarily oriented objects; (c) illustrates the corner cases where the vectors are very close to the -axes.}
\label{fig:figure1}
\end{figure}


Current oriented object detection methods are mainly derived from the two-stage anchor-based detectors \cite{girshick2014rich,girshick2015fast,ren2015faster}. Generally, in the first stage, those detectors spread anchor boxes on the feature maps densely and then regress the offsets between the target box and the anchor box parameters in order to provide region proposals. In the second stage, the region-of-interest (ROI) features are pooled to refine the box parameters and classify the object categories. Notably, they use the center, width, height, and angle as the descriptions of an oriented bounding box. The angle is learned either in the first stage or in the second stage. For instance, RCNN \cite{jiang2017r2cnn}, Yang et al. \cite{yang2018position}, and ROI Transformer \cite{ding2019learning} regress the angle parameters from the pooled horizontal region proposal features in the second stage; similarly, RPN \cite{zhang2018toward}, R-DFPN \cite{yang2018automatic} and ICN \cite{azimi2018towards} generate oriented region proposals in the first stage. These oriented object detection methods share the same drawbacks with the anchor-based detectors. For example, the design of the anchor boxes is complicated; and the choices of aspect-ratios and the size of the anchor boxes need to be tuned carefully. Besides, the extreme imbalance between the positive and negative anchor boxes would induce slow training and sub-optimal performance \cite{duan2019centernet}. Moreover, the crop-and-regress strategies in the second stage are computationally expensive \cite{zhou2019bottom}. Recently, the keypoint-based object detectors \cite{law2018cornernet,zhou2019bottom,duan2019centernet} have been developed to overcome the disadvantages of anchor-based solutions \cite{ren2015faster,liu2016ssd,yi2019assd} in the horizontal object detection task. In particular, these methods detect the corner points of the bounding boxes and then group these points by comparing embedding distances or center distances of the points. Such strategies have demonstrated improved performance, yet with one weakness that the grouping process is time-consuming. To address this issue, Zhou's CenterNet \cite{zhou2019objects} suggests detecting the object center and regressing the width () and height () of the bounding box directly, which achieves faster speed at comparable accuracy. Intuitively, Zhou's CenterNet can be extended to the oriented object detection task by learning an additional angle  together with  and  (see Fig.~\ref{fig:figure1}a). However, as the parameters  and  are measured in different rotating coordinate systems for each arbitrarily oriented object, jointly learning those parameters may be challenging for the model.

In this paper, we extend Zhou's CenterNet to the oriented object detection task. However, instead of regressing the ,  and  at the center points, we learn the box boundary-aware vectors (BBAVectors, Fig.~\ref{fig:figure1}b) to capture the rotational bounding boxes of the objects. The BBAVectors are distributed in the four quadrants of the Cartesian coordinate system.
Empirically we show that this design is superior to directly predicting the spatial parameters of bounding boxes.
In practice, we observe that in the corner cases, where the vectors are very close to the boundary of the quadrants (i.e., -axes in Fig.~\ref{fig:figure1}c), it would be difficult for the network to differentiate the vector types. To deal with this problem, we group the oriented bounding box (OBB) into two categories and handle them separately. Specifically, we have two types of boxes: horizontal bounding box (HBB) and rotational bounding box (RBB), where RBB refers to all oriented bounding boxes except the horizontal ones. We summarize our contributions as follows:
\begin{itemize}
    \item We propose the box boundary-aware vectors (BBAVectors) to describe the OBB. This strategy is simple yet effective. The BBAVectors are measured in the same Cartesian coordinate system for all the arbitrarily oriented objects. Compared to the baseline method that learns the width, height and angle of the OBBs, the BBAVectors achieve better performance.
    
    \item We extend the center keypoint-based object detector to the oriented object detection task. Our model is single-stage and anchor box free, which is fast and accurate. It achieves state-of-the-art performances on the DOTA and HRSC2016 datasets.
    
\end{itemize}


\section{Related Work}
\label{Related Work}
\subsection{Oriented Object Detection}
The horizontal object detectors, such as R-CNN \cite{ma2015hierarchical}, fast R-CNN \cite{girshick2015fast}, faster R-CNN \cite{ren2015faster}, SSD \cite{liu2016ssd}, YOLO \cite{redmon2016you}, are designed for horizontal objects detection. These methods generally use the horizontal bounding boxes (HBB) to capture the objects in natural images. Different from the horizontal object detection task, oriented object detection relies on oriented bounding boxes (OBB) to capture the arbitrarily oriented objects. Current oriented object detection methods are generally extended from the horizontal object detectors. For example, RCNN \cite{jiang2017r2cnn} uses the region proposal network (RPN) to produce the HBB of the text and combines different scales of pooled ROI features to regress the parameters of OBB. RPN \cite{zhang2018toward} incorporates the box orientation parameter into the RPN network and develops a rotated RPN network. RPN also utilizes a rotated ROI pooling to refine the box parameters.  R-DFPN \cite{yang2018automatic} employs the Feature Pyramid Network (FPN) \cite{lin2017feature} to combine multi-scale features and boost the detection performance. Based on the DFPN backbone, Yang \textit{et al}. \cite{yang2018position} further propose an adaptive ROI Align method for the second-stage box regression. RoI Transformer \cite{ding2019learning} learns the spatial transformation from the HBBs to OBBs. ICN \cite{azimi2018towards} develops an Image Cascade Network that enhances the semantic features before adopting R-DFPN. RRD \cite{liao2018rotation} uses active rotation filters to encode the rotation information. Gliding Vertex \cite{xu2020gliding} glide the vertex of the horizontal bounding boxes to capture the oriented bounding boxes. All these methods are based on anchor boxes. Overall, the anchor-based detectors first spread a large amount of anchor boxes on the feature maps densely, and then regress the offsets between the target boxes and the anchor boxes. Such anchor-based strategies suffer from the imbalance issue between positive and negative anchor boxes. The issue would lead to slow training and sub-optimal detection performances \cite{law2018cornernet}. 


\begin{figure*}[t!]
\begin{center}
   \includegraphics[width=0.9\linewidth]{images/pic2.pdf}
\end{center}
\caption{The overall architecture and the oriented bounding box (OBB) descriptions of the proposed method. The input image is resized to  before being fed to the network. The architecture is built on a U-shaped network. Skip connections are adopted to combine feature maps in the up-sampling process. The output of the architecture involves four maps: the heatmap , offset map , box parameter map , and orientation map . The locations of the center points are inferred from the heatmap and offset map. At the center points, the box boundary-aware vectors (BBAVectors) are learned. The resolution of the output maps is . HBBs refer to the horizontal bounding boxes. RBBs indicate all oriented bounding boxes except the HBBs. The symbols  refer to the top, right, bottom and left vectors of BBAVectors,  and  are the external width and height of an OBB. The decoded OBBs are shown in red bounding boxes.}
\label{fig:figure2}
\end{figure*}


\subsection{Keypoint-Based Object Detection}
The keypoint-based object detectors \cite{law2018cornernet,zhou2019bottom,zhou2019objects,yi2019multi} capture the objects by detecting the keypoints and therefore provide anchor-free solutions. Keypoint detection is extensively employed in the face landmark detection \cite{merget2018robust} and pose estimation \cite{newell2016stacked,sun2018integral}. In the horizontal object detection task, the keypoint-based detection methods propose to detect the corner points or the center points of the objects and extract the box size information from these points. Cornernet \cite{law2018cornernet} is one of the pioneers. It captures the top-left and bottom-right corner points of the HBB using heatmaps. The corner points are grouped for each object by comparing the embedding distances of the points. Duan's CenterNet \cite{duan2019centernet} detects both corner points and center points. ExtremeNet \cite{zhou2019bottom} locates the extreme and center points of the boxes. These two methods both use the center information to group the box points. However, the post-grouping process in these methods is time-consuming. To address this problem, Zhou's CenterNet \cite{zhou2019objects} proposes to regress the width and height of the bounding box at the center point without a post-grouping process, which makes the prediction faster. The keypoint-based object detectors show advantages over the anchor-based ones in terms of speed and accuracy, yet the keypoint-based detectors are barely applied to oriented object detection task.


\paragraph{Baseline method.}  In this paper, we extend Zhou's CenterNet to the oriented object detection task. In particular, we first build a baseline method that directly regresses the width  and height  as well as the orientation angle  of the bounding boxes. We term this baseline method as Center (see Fig.~\ref{fig:figure1}a). We compare the proposed method with Center to demonstrate the advantages of box boundary-aware vectors.

\section{Method}
In this section, we first describe the overall architecture of the proposed method, and then explain the output maps in detail. The output maps are gathered and decoded to generate the oriented bounding boxes of the objects.

\subsection{Architecture}
\label{sub:architecture}
The proposed network (see Fig.~\ref{fig:figure2}) is built on a U-shaped architecture \cite{ronneberger2015u}. We use the ResNet101 Conv1-5 \cite{he2016deep} as the backbone. At the top of the backbone network, we up-sample the feature maps and output a feature map that is 4 times smaller (scale ) than the input image. In the up-sampling process, we combine a deep layer with a shallow layer through skip connections to share both the high-level semantic information and low-level finer details. In particular, we first up-sample a deep layer to the same size of the shallow layer through bilinear interpolation. The up-sampled features map is refined through a  convolutional layer. The refined feature map is then concatenated with the shallow layer, followed by a  convolutional layer to refine the channel-wise features. Batch normalization and ReLU activation are used in the latent layers. Suppose an input RGB image is , where  and  are the height and width of the image. The output feature map  ( in this paper) is then transformed into four branches: heatmap (), offset (), box parameter (), and the orientation map (), where  is the number of dataset categories and  refers to the scale. The transformation is implemented with two convolutional layers with  kernels and  channels.



\subsection{Heatmap}
\label{sub:heatmap}
Heatmap is generally utilized to localize particular keypoints in the input image, such as the joints of humans and the facial landmarks \cite{merget2018robust,newell2016stacked,sun2018integral}. In this work, we use the heatmap to detect the center points of arbitrarily oriented objects in the aerial images. Specifically, the heatmap  used in this work has  channels, with each corresponding to one object category. The map at each channel is passed through a sigmoid function. The predicted heatmap value at a particular center point is regarded as the confidence of the object detection.

\paragraph{Groundtruth} Suppose  is the center point of an oriented bounding box, we place a 2D Gaussian  exp (see Fig.~\ref{fig:figure2}) around each  to form the groundtruth heatmap , where  is a box size-adaptive standard deviation \cite{zhou2019objects,law2018cornernet}, point  indexes the pixel points on .

\paragraph{Training Loss} When training the heatmaps, only the center points  are positive. All the other points including the points in the Gaussian bumps are negative. Directly learning the positive center points would be difficult due to the imbalance issue. To handle this problem, following the work of \cite{law2018cornernet}, we decrease the penalty for the points inside the Gaussian bumps and use the variant focal loss to train the heatmap:

where   and  refer to the ground-truth and the predicted heatmap values,  indexes the pixel locations on the feature map,  is the number of objects,  and  are the hyperparameters that control the contribution of each point. We choose  and  empirically as in \cite{law2018cornernet}.

\subsection{Offset}
\label{sub:offsets}
In the inference stage, the peak points are extracted from the predicted heatmap  as the center point locations of the objects. These center points  are integers. However, down-scaling a point from the input image to the output heatmap generates a floating-point number. To compensate for the difference between the quantified floating center point and the integer center point, we predict an offset map . Given a ground-truth center point  on the input image, the offset between the scaled floating center point and the quantified center point is:

The offset is optimized with a smooth  loss \cite{girshick2015fast}:

where  is the total number of objects,  refers to the ground-truth offsets,  indexes the objects. The smooth  loss can be expressed as:


\subsection{Box Parameters}
To capture the oriented bounding boxes, one natural and straightforward way is to detect the width , and height , and angle  of an OBB from the center point. We term this baseline method as Center (see Fig.~\ref{fig:figure1}a). This method has several disadvantages. First, a small angle variation has marginal influence on the total loss in training, but it may induce a large IOU difference between the predicted box and the ground-truth box. Second, for each object, the  and  of its OBB are measured in an individual rotating coordinate system that has an angle  with respect to the -axis. Therefore, it is challenging for the network to jointly learn the box parameters for all the objects. In this paper, we propose to use the box boundary-aware vectors (BBAVectors, see Fig.~\ref{fig:figure1}b) to describe the OBB. The BBAVectors contain the top , right , bottom  and left  vectors from the center points of the objects. In our design, the four types of vectors are distributed in four quadrants of the Cartesian coordinate system. All the arbitrarily oriented objects share the same coordinate system, which would facilitate the transmission of mutual information and therefore improve the generalization ability of the model. We intentionally design the four vectors instead of two (i.e.,  and , or  and ) to enable more mutual information to be shared when some local features are obscure and weak.


The box parameters are defined as , where  are the BBAVectors,  and  are the external horizontal box size of an OBB, as described in Fig.~\ref{fig:figure2}. The details of  and  are explained in Section \ref{sub: orientation}. Totally, the box parameter map  has  channels with  vectors and  external size parameters. We also use a smooth  loss to regress the box parameters at the center point:

where  and  are the predicted and ground-truth box parameters, respectively.


\subsection{Orientation}
\label{sub: orientation}
In practice, we observe that the detection would fail in situations where the objects nearly align with -axes (see Fig.~\ref{fig:figure4}b). The reason would be that at the boundary of the quadrant, the types of the vectors are difficult to be differentiated. We term this problem as corner cases (see Fig.~\ref{fig:figure1}c). To address this issue, in this work we group OBBs into two categories and process them separately. In particular, the two types of boxes are HBB and RBB, where RBB involves all the rotation bounding boxes except the horizontal ones. The benefit of such a classification strategy is that we transform the corner cases into the horizontal ones, which can be dealt with easily. When the network encounters the corner cases, the orientation category and the external size ( and  in Fig.~\ref{fig:figure2}) can help the network to capture the accurate OBB. The additional external size parameters also enrich the descriptions of an OBB. 

We define the orientation map as . The output map is finally processed by a sigmoid function. To create the ground-truth of the orientation class , we define:

where IOU is the intersection-over-union between the oriented bounding box (OBB) and the horizontal bounding box (HBB). The orientation class is trained with the binary cross-entropy loss:

where  and  are the predicted and the ground-truth orientation classes, respectively. 



\begin{table*}[t!]
\begin{center}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|c|ccccccccccccccc}
\toprule
Method & mAP & Plane & BD & Bridge & GTF & SV & LV & Ship & TC & BC & ST & SBF & RA & Harbor & SP & HC\\
\midrule
YOLOv2 \cite{redmon2017yolo9000}& 25.49 &52.75 &24.24 &10.6 &35.5 &14.36& 2.41 &7.37 &51.79& 43.98 &31.35& 22.3& 36.68& 14.61 &22.55 &11.89\\
FR-O \cite{xia2018dota}&54.13
&79.42&77.13&17.7&64.05&35.3&38.02&37.16&89.41&69.64&59.28&50.3&52.91&47.89&47.4&46.3 \\
R-DFPN \cite{yang2018automatic}&57.94&80.92 &65.82& 33.77& 58.94 &55.77& 50.94& 54.78 &90.33& 66.34 &68.66& 48.73& 51.76& 55.10& 51.32& 35.88 \\
RCNN \cite{jiang2017r2cnn}&60.67&80.94&65.75&35.34& 67.44	&59.92	&50.91	&55.81	&90.67&	66.92&	72.39&	55.06	&52.23&	55.14&	53.35&	48.22\\
Yang \textit{et al}. \cite{yang2018position}& 62.29&81.25 &71.41 &36.53 &67.44& 61.16& 50.91& 56.60& 90.67 &68.09& 72.39 &55.06& 55.60 &62.44& 53.35& 51.47 \\
ICN \cite{azimi2018towards}&68.16& 81.36& 74.30& 47.70& 70.32& 64.89& 67.82& 69.98& 90.76& 79.06& 78.20& 53.64& \color{blue}\textbf{62.90}& \color{blue}\textbf{67.02} &64.17& 50.23\\
ROI Trans.  \cite{ding2019learning}&67.74&88.53&77.91&37.63&\color{blue}\textbf{74.08}&66.53&62.97&66.57&90.5&79.46&76.75&\color{red}\textbf{59.04}&56.73&62.54&61.29&55.56\\
ROI Trans.+FPN  \cite{ding2019learning}& 69.56& \color{red}\textbf{88.64}&78.52&43.44&\color{red}\textbf{75.92}&68.81&73.68&83.59&90.74&77.27&81.46&\color{blue}\textbf{58.39}&53.54&62.83&58.93&47.67\\
BBAVectors+ & 71.61& 88.54& 76.72& 49.67& 65.22& 75.58& \color{blue}\textbf{80.28}& 87.18 & 90.62& \color{blue}\textbf{84.94}&\color{blue}\textbf{84.89}& 47.17& 60.59& 65.31& 63.91& 53.52\\
BBAVectors+&\color{blue}\textbf{72.32}& 88.35& \color{blue}\textbf{79.96}& \color{blue}\textbf{50.69}& 62.18& \color{red}\textbf{78.43}& 78.98& \color{blue}\textbf{87.94}& \color{blue}\textbf{90.85}& 83.58& 84.35& 54.13& 60.24& 65.22& \color{blue}\textbf{64.28}& \color{blue}\textbf{55.70}\\
BBAVectors+&\color{red}\textbf{75.36}& \color{blue}\textbf{88.63} & \color{red}\textbf{84.06} & \color{red}\textbf{52.13} & 69.56 & \color{blue}\textbf{78.26} & \color{red}\textbf{80.40} & \color{red}\textbf{88.06} & \color{red}\textbf{90.87} & \color{red}\textbf{87.23} & \color{red}\textbf{86.39} & 56.11 & \color{red}\textbf{65.62}& \color{red}\textbf{67.10} & \color{red}\textbf{72.08} & \color{red}\textbf{63.96}\\
\bottomrule
\end{tabular}
}
\end{center}
\caption{Detection results on the testing set of DOTA-v1.0. The performances are evaluated through the online server. Symbol  shows the result with a larger training batch size (i.e., 48 on 4 Quadro RTX 6000 GPUs). {\color{red}Red} and {\color{blue}Blue} colors label the best and second best detection results in each column.}
\label{Table:Table1}
\end{table*} 
\section{Experiments}
\subsection{Datasets}
We evaluate our method on two public aerial image datasets: DOTA \cite{xia2018dota} and HRSC2016 \cite{icpram17}.

\paragraph{DOTA.} We use DOTA-v1.0  \cite{xia2018dota} dataset for the oriented object detection. It contains 2,806 aerial images with a wide variety of scales, orientations, and shapes of objects. These images are collected from different sensors and platforms. The image resolution ranges from  to . The fully-annotated images contain 188,282 instances. The DOTA-V1.0 has 15 categories: Plane, Baseball Diamond (BD),    Bridge,    Ground Track Field (GTF), Small Vehicle (SV), Large Vehicle    (LV), Ship, Tennis Court (TC), Basketball Court (BC), Storage Tank (ST), Soccer-Ball Field (SBF), Roundabout (RA), Harbor, Swimming Pool (SP) and Helicopter (HC). The DOTA images involve the crowd and small objects in a large image. For accurate detection, we use the same algorithm as ROI Transformer \cite{ding2019learning} to crop the original images into patches. In particular, the images are cropped into  patches with a stride of 100.  The input images have two scales  and .  The trainval set and testing set contain 69,337 and 35,777 images after the cropping, respectively. The trainval set refers to both training and validation sets \cite{ding2019learning}. Following the previous works \cite{azimi2018towards,ding2019learning}, we train the network on the trainval set and test on the testing set. The detection results of cropped images are merged as the final results. Non-maximum-suppression (NMS) with a 0.1 IOU threshold is applied to the final detection results to discard repetitive detection. The testing dataset is evaluated through the online server.

\noindent\paragraph{HRSC2016.} Ship detection in aerial images is important for port management, cargo transportation, and maritime rescue \cite{yang2018automatic}. The HRSC2016 \cite{icpram17} is a ship dataset collected from Google Earth, which contains 1,061 images with ships in various appearances. The image sizes range from  to . The dataset has 436 training images,  validation images, and  testing images. We train the network on the training set and use the validation set to stop the training when the loss on the validation set no longer decreases. The detection performance of the proposed method is reported on the testing set.



\subsection{Implementation Details}
We resize the input images to  in the training and testing stage, giving an output resolution of . We implement our method with PyTorch. The backbone weights are pre-trained on the ImageNet dataset. The other weights are initialized under the default settings of PyTorch. We adopt the standard data augmentations to the images in the training process, which involve random flipping and random cropping within scale range . We use Adam \cite{kingma2014adam} with an initial learning rate of  to optimize the total loss . The network is trained with a batch size of 20 on four NVIDIA GTX 1080 Ti GPUs. We train the network for about 40 epochs on the DOTA dataset and 100 epochs on the HRSC2016 dataset. We additionally report an experiment with a larger batch size 48 on 4 NVIDIA Quadro RTX 6000 GPUs, we mark the results with symbol  in Table \ref{Table:Table1}. The speed of the proposed network is measured on a single NVIDIA TITAN X GPU on the HRSC2016 dataset.



\subsection{Testing Details}
To extract the center points, we apply an NMS on the output heatmaps through a  max-pooling layer. We pick the top 500 center points from the heatmaps and take out the offsets (), box parameters (), and orientation class () at each center point (). The heatmap value is used as the detection confidence score. We first adjust the center points by adding the offsets . Next, we rescale the obtained integer center points on the heatmaps by . We obtain the RBB when , and we get the HBB otherwise. We use the top-left (), top-right (), bottom-right () and bottom-left () points of the bounding box as the final decoded points. Specifically, for a center point , the decoded RBB points are obtained from:

For a HBB, the points are:
 
We gather the RBBs and HBBs as the final detection results.



\subsection{Comparison with the State-of-the-arts}
We compare the performance of the proposed method with the state-of-the-art algorithms on the DOTA and HRSC2016 datasets. To study the impact of orientation classification, we define two versions of the the proposed method: BBAVectors and BBAVectors. BBAVectors only learns the box boundary-aware vectors to detect OBB, which contains box parameters . BBAVectors additionally learns the orientation class  and the external size parameters ( and ).


 
\begin{figure*}[tbh!]
\begin{center}
   \includegraphics[width=0.92\linewidth]{images/pic3.pdf}
\end{center}
\caption{Visualization of the detection results of BBAVectors+ on DOTA dataset.}
\label{fig:figure3}
\end{figure*}


\paragraph{DOTA.} The detection results on the DOTA dataset are illustrated in Table~\ref{Table:Table1}. YOLOv2 \cite{redmon2017yolo9000} and FR-O \cite{xia2018dota} are trained on HBB \cite{xia2018dota} and their performances are comparably lower than the other methods. Notably, although the one-stage detector YOLOv2 runs faster, its accuracy is lower than the two-stage anchor-based detectors. R-DFPN \cite{yang2018automatic} learns the angle parameter from faster R-CNN \cite{ren2015faster} and improves performance from 54.13\% to 57.94\%. RCNN \cite{jiang2017r2cnn} pools multiple sizes of region proposals at the output of RPN and improves the accuracy from 57.94\% to 60.67\%. Yang \textit{et al}. \cite{yang2018position} use the adaptive ROI align to extract objects and achieve 1.62\% improvement over 60.67\%. ICN \cite{azimi2018towards} adopts the Image Cascaded Network to enrich features before R-DFPN and boost the performance from 62.29\% to 68.16\%. ROI Transformer  \cite{ding2019learning} transfers the horizontal ROIs to oriented ROIs by learning the spatial transformation, raising the accuracy from 68.16\% to 69.56\%. Different from these methods, the proposed method offers a new concept of oriented object detection, a keypoint-based detection method with box boundary-aware vectors. As shown in Table \ref{Table:Table1}, Without orientation classification, the BBAVectors improves 2.05\% over 69.59\% of ROI Transformer+FPN \cite{lin2017feature} and 3.87\% over 67.74\% of ROI Transformer without FPN. As the proposed method is single-stage, this result demonstrates the detection advantages of the keypoint-based method over the anchor-based method. With the orientation classification and additional external OBB size parameters ( and ), the proposed BBAVectors achieves 72.32\% mAP, which exceeds ROI Transformer+FPN by 2.76\%. Besides, BBAVectors runs faster than ROI Transformer (see Table~\ref{Table:Table2}). With a larger training batch size, the BBAVectors achieves about 3 points higher than BBAVectors. The visualization of the detection results of BBAVectors on DOTA dataset is illustrated in Fig.~\ref{fig:figure3}. The background in the aerial images is complicated and the objects are arbitrarily oriented with different sizes and scales. However, the proposed method is robust to capture the objects even for the tiny and crowd small vehicles.




\paragraph{HRSC2016.} The performance comparison results between the proposed method and the state-of-the-arts on HRSC2016 dataset is illustrated in Table~\ref{Table:Table2}. The RPN \cite{zhang2018toward} learns the rotated region proposals based on VGG16 backbone, achieving 79.6\% AP. RRD \cite{liao2018rotation} adopts activate rotating filters and improves the accuracy from 79.6\% to 84.3\%. ROI Transformer   \cite{ding2019learning} without FPN produces 86.2\%, while BBAVectors+ achieves 88.2\%. BBAVectors performs slightly higher (0.4\% over 88.2\%) than BBAVectors. In the inference stage, the proposed method achieves 12.88 FPS on a single NVIDIA TITAN X GPU, which is 2.18x faster than ROI Transformer. 


\begin{table}
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|c|c|c|c|c}
\toprule
Method & Backbone & Image Size&GPU& FPS & AP \\
\midrule
RPN \cite{zhang2018toward}&VGG16&-&-&-&79.6\\
RRD \cite{liao2018rotation}&VGG16&&-&-&84.3\\
ROI Trans. \cite{ding2019learning}&ResNet101&&TITAN X&5.9&86.2\\
BBAVectors+&ResNet101&&TITAN X&12.88&88.2\\
BBAVectors+&ResNet101&&TITAN X&11.69&\textbf{88.6}\\
\bottomrule
\end{tabular}}
\end{center}
\caption{Detection results on the testing dataset of HRSC2016. The speed of the proposed method is measured on a single NVIDIA TITAN X GPU.}
\label{Table:Table2}
\end{table} 

\begin{figure}[tbh!]
\begin{center}
   \includegraphics[width=0.95\linewidth]{images/pic4.pdf}
\end{center}
\caption{Comparison of BBAVectors+ and BBAVectors+.}
\label{fig:figure4}
\end{figure}


\subsection{Ablation Studies}
\label{sub:ablation studies}
We compare the performances of the proposed BBAVectors and BBAVectors to study the impact of orientation classification. As we mentioned before, BBAVectors refers to box parameters .  BBAVectors corresponds to . BBAVectors has MB parameters with  FLOPs, while BBAVectors has MB parameters and  FLOPs. 

As can be seen in Fig.~\ref{fig:figure4}, the BBAVectors can hardly capture the bounding boxes that nearly align with the -axes. These are the corner cases as discussed above. The reason for the failure detection would be that it is difficult for the network to differentiate the type of vectors near the quadrant boundary (i.e., classification boundary). To address this problem, we separate the OBB into RBB and HBB by learning an orientation class  and we use the external parameters ( and ) to describe HBB. As illustrated in Fig.~\ref{fig:figure4}, the BBAVectors excels in capturing the oriented bounding box at the corner cases. On the DOTA dataset (see Table~\ref{Table:Table1}), the BBAVectors improves 0.71\% over BBAVectors. On the HRSC2016 dataset (see Table~\ref{Table:Table2}), BBAVectors achieves 0.4\% improvement over BBAVectors. 


\subsection{Comparison with Baseline}
To explore the advantage of the box boundary-aware vectors, we also compare our method with the baseline Center++ (see Fig.~\ref{fig:figure1}a), which employs the width (), height () and angle () as the descriptions of OBB. Note that the baseline method shares the same architecture as the proposed method except for the output box parameter and orientation map. The training procedure is the same as the proposed method. Here we do not explicitly handle the corner cases for a fair comparison. From Table~\ref{Table:Table3}, we can see that the proposed method performs 4.82\% and 2.74\% better than Center++ on HRSC2016 and DOTA datasets, respectively. The results suggest that the box boundary-aware vectors are better for oriented object detection than learning the  of OBB directly. The reason would be that the box boundary-aware vectors are learned in the same Cartesian coordinate systems, while in the baseline method, the size parameters ( and ) of an OBB are measured in different rotating coordinate systems that have an angle  with respect to the -axis. Jointly learning those parameters would be difficult for the baseline method.


\begin{table}
\begin{center}
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{l|c|c|c}
\toprule
Method & Dataset & Backbone& mAP \\
\midrule
Center++ & HRSC2016 & ResNet101 & 83.40\\
BBAVectors+ & HRSC2016 & ResNet101& \textbf{88.22}\\
Center++ & DOTA & ResNet101& 68.87\\
BBAVectors+ & DOTA  & ResNet101& \textbf{71.61}\\
\bottomrule
\end{tabular}
}
\end{center}
\caption{Comparison between baseline method Center++ and the proposed method BAVectors. }
\label{Table:Table3}
\end{table}

%
 



\section{Conclusion}
In this paper, we propose a new oriented object detection method based on box boundary-aware vectors and center points detection. The proposed method is single-stage and is free of anchor boxes.  The proposed box boundary-aware vectors perform better in capturing the oriented bounding boxes than the baseline method that directly learns the width, height, and angle of the oriented bounding box. The results on the HRSC2016 and DOTA datasets demonstrate the superiority of the proposed method over the state-of-the-arts.


\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{azimi2018towards}
Seyed~Majid Azimi, Eleonora Vig, Reza Bahmanyar, Marco K{\"o}rner, and Peter
  Reinartz.
\newblock Towards multi-class object detection in unconstrained remote sensing
  imagery.
\newblock In {\em Asian Conference on Computer Vision}, pages 150--165.
  Springer, 2018.

\bibitem{ding2019learning}
Jian Ding, Nan Xue, Yang Long, Gui-Song Xia, and Qikai Lu.
\newblock Learning roi transformer for oriented object detection in aerial
  images.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 2849--2858, 2019.

\bibitem{duan2019centernet}
Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi, Qingming Huang, and Qi Tian.
\newblock Centernet: Keypoint triplets for object detection.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision}, pages 6569--6578, 2019.

\bibitem{girshick2015fast}
Ross Girshick.
\newblock Fast r-cnn.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 1440--1448, 2015.

\bibitem{girshick2014rich}
Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik.
\newblock Rich feature hierarchies for accurate object detection and semantic
  segmentation.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 580--587, 2014.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{jiang2017r2cnn}
Yingying Jiang, Xiangyu Zhu, Xiaobing Wang, Shuli Yang, Wei Li, Hua Wang, Pei
  Fu, and Zhenbo Luo.
\newblock R2cnn: Rotational region cnn for orientation robust scene text
  detection.
\newblock {\em arXiv preprint arXiv:1706.09579}, 2017.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em International Conference on Learning Representations (ICLR)},
  2015.

\bibitem{law2018cornernet}
Hei Law and Jia Deng.
\newblock Cornernet: Detecting objects as paired keypoints.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 734--750, 2018.

\bibitem{liao2018rotation}
Minghui Liao, Zhen Zhu, Baoguang Shi, Gui-song Xia, and Xiang Bai.
\newblock Rotation-sensitive regression for oriented scene text detection.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 5909--5918, 2018.

\bibitem{lin2017feature}
Tsung-Yi Lin, Piotr Doll{\'a}r, Ross Girshick, Kaiming He, Bharath Hariharan,
  and Serge Belongie.
\newblock Feature pyramid networks for object detection.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2117--2125, 2017.

\bibitem{liu2016ssd}
Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed,
  Cheng-Yang Fu, and Alexander~C Berg.
\newblock Ssd: Single shot multibox detector.
\newblock In {\em European conference on computer vision}, pages 21--37.
  Springer, 2016.

\bibitem{icpram17}
Zikun Liu, Liu Yuan, Lubin Weng, and Yiping Yang.
\newblock A high resolution optical satellite image dataset for ship
  recognition and some new baselines.
\newblock In {\em Proceedings of the 6th International Conference on Pattern
  Recognition Applications and Methods - Volume 1: ICPRAM,}, pages 324--331.
  INSTICC, SciTePress, 2017.

\bibitem{ma2015hierarchical}
Chao Ma, Jia-Bin Huang, Xiaokang Yang, and Ming-Hsuan Yang.
\newblock Hierarchical convolutional features for visual tracking.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 3074--3082, 2015.

\bibitem{merget2018robust}
Daniel Merget, Matthias Rock, and Gerhard Rigoll.
\newblock Robust facial landmark detection via a fully-convolutional
  local-global context network.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 781--790, 2018.

\bibitem{newell2016stacked}
Alejandro Newell, Kaiyu Yang, and Jia Deng.
\newblock Stacked hourglass networks for human pose estimation.
\newblock In {\em European conference on computer vision}, pages 483--499.
  Springer, 2016.

\bibitem{redmon2016you}
Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi.
\newblock You only look once: Unified, real-time object detection.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 779--788, 2016.

\bibitem{redmon2017yolo9000}
Joseph Redmon and Ali Farhadi.
\newblock Yolo9000: better, faster, stronger.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 7263--7271, 2017.

\bibitem{ren2015faster}
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
\newblock Faster r-cnn: Towards real-time object detection with region proposal
  networks.
\newblock In {\em Advances in neural information processing systems}, pages
  91--99, 2015.

\bibitem{ronneberger2015u}
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In {\em International Conference on Medical image computing and
  computer-assisted intervention}, pages 234--241. Springer, 2015.

\bibitem{sun2018integral}
Xiao Sun, Bin Xiao, Fangyin Wei, Shuang Liang, and Yichen Wei.
\newblock Integral human pose regression.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 529--545, 2018.

\bibitem{xia2018dota}
Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Belongie, Jiebo Luo, Mihai
  Datcu, Marcello Pelillo, and Liangpei Zhang.
\newblock Dota: A large-scale dataset for object detection in aerial images.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 3974--3983, 2018.

\bibitem{xu2020gliding}
Yongchao Xu, Mingtao Fu, Qimeng Wang, Yukang Wang, Kai Chen, Gui-Song Xia, and
  Xiang Bai.
\newblock Gliding vertex on the horizontal bounding box for multi-oriented
  object detection.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  2020.

\bibitem{yang2018automatic}
Xue Yang, Hao Sun, Kun Fu, Jirui Yang, Xian Sun, Menglong Yan, and Zhi Guo.
\newblock Automatic ship detection in remote sensing images from google earth
  of complex scenes based on multiscale rotation dense feature pyramid
  networks.
\newblock {\em Remote Sensing}, 10(1):132, 2018.

\bibitem{yang2018position}
Xue Yang, Hao Sun, Xian Sun, Menglong Yan, Zhi Guo, and Kun Fu.
\newblock Position detection and direction prediction for arbitrary-oriented
  ships via multitask rotation region convolutional neural network.
\newblock {\em IEEE Access}, 6:50839--50849, 2018.

\bibitem{yi2019multi}
Jingru Yi, Pengxiang Wu, Qiaoying Huang, Hui Qu, Bo Liu, Daniel~J Hoeppner, and
  Dimitris~N Metaxas.
\newblock Multi-scale cell instance segmentation with keypoint graph based
  bounding boxes.
\newblock In {\em International Conference on Medical Image Computing and
  Computer-Assisted Intervention}, pages 369--377. Springer, 2019.

\bibitem{yi2019assd}
Jingru Yi, Pengxiang Wu, and Dimitris~N Metaxas.
\newblock Assd: Attentive single shot multibox detector.
\newblock {\em Computer Vision and Image Understanding}, 189:102827, 2019.

\bibitem{zhang2018toward}
Zenghui Zhang, Weiwei Guo, Shengnan Zhu, and Wenxian Yu.
\newblock Toward arbitrary-oriented ship detection with rotated region proposal
  and discrimination networks.
\newblock {\em IEEE Geoscience and Remote Sensing Letters}, 15(11):1745--1749,
  2018.

\bibitem{zhou2019objects}
Xingyi Zhou, Dequan Wang, and Philipp Kr{\"a}henb{\"u}hl.
\newblock Objects as points.
\newblock {\em arXiv preprint arXiv:1904.07850}, 2019.

\bibitem{zhou2019bottom}
Xingyi Zhou, Jiacheng Zhuo, and Philipp Krahenbuhl.
\newblock Bottom-up object detection by grouping extreme and center points.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 850--859, 2019.

\end{thebibliography}
 
\end{document}
