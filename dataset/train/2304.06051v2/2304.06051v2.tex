

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}              

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}


\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\cvprPaperID{7} \def\confName{CVPR}
\def\confYear{2023}

\usepackage{multirow} 



\begin{document}

\title{Open-TransMind: A New Baseline and Benchmark for 1st Foundation Model Challenge of Intelligent Transportation}

\author
{ 
Yifeng Shi\textsuperscript{1}\thanks{indicates equal contribution}\thanks{Yifeng Shi(shiyifeng@baidu.com is the corresponding author.)}
\quad
Feng Lv\textsuperscript{1}
\quad
Xinliang Wang\textsuperscript{1}
\quad
Chunlong Xia\textsuperscript{1}\\
\quad
Shaojie Li\textsuperscript{1}
\quad
Shujie Yang\textsuperscript{1}
\quad
Teng Xi\textsuperscript{1}
\quad
Gang Zhang\textsuperscript{1}
\mAP@K=\frac{1}{m} \times \sum_{i=1}^{K}{p(i)*\Delta r(i)}\Delta r(i)=r(i)-r(i-1)
where  is the recall of the  retrieved results and .



\section{Baseline}
In intelligent transportation scenarios, the types of tasks mainly involve multiple fields such as classification, detection, and segmentation. We can usually solve the corresponding problems by building a single-task model, but the generalization ability of the model will be poor due to the small size of the data. Considering the similarity between different tasks in the traffic scene, we propose a multi-task co-evolution model to promote mutual learning between different tasks, of course, there may be conflicts. In addition, traditional image retrieval methods usually use image attribute recognition to achieve retrieval capabilities. However, introducing cross-modal capabilities based on existing models, such as image-text retrieval, can further improve the flexibility of image retrieval applications. Therefore, we can use a multi-task co-evolutionary model to build an image model for text retrieval. Finally, we combine multi-task co-evolution and multi-modal text-image retrieval into a model called Open-TransMind.

\subsection{Task-MoE}
Our Open-TransMind model is built based on the UFO~\cite{xi2022ufo} multi-task and multi-path training framework. Due to the differences in the visual representation of the data used by different tasks, there may be conflicts between tasks. Further, we can alleviate conflicts by introducing Task-MoE. As shown in the Figure \ref{fig:moe}, the multi-path FFN module is set in the encoding module. Each task has two different path options, that is, to choose shared FFN (state1 in Figure \ref{fig:moe}) or specific FFN (state2 in Figure \ref{fig:moe}). All tasks will update the parameters of shared FFN, and specific tasks will only update the specific FFN parameters to achieve collaborative optimization between different tasks, and greatly improve the generalization ability of the model.


\subsection{Model Design for All in One}
The framework of the large multi-task model for this track is shown in Figure \ref{fig:all-in-one}. We select the backbone of the transformer architecture as the multi-task feature encoder. Based on this common feature, we respectively perform feature decoding for image classification, object detection, and semantic segmentation tasks and predict the results of corresponding tasks. The feature decoder for classification tasks consists of a linear-projection layer and a full-connection layer. The function of the linear-projection layer is to provide a layer of isolation between classification and multi-task features; the object detection decoder refers to the detection head of the DINO ~\cite{2022DINO} model, and the semantic segmentation decoder uses the progressive upsampling module of the SETR ~\cite{2021Rethinking} model.

\subsection{Model Design for Cross-Modal Image Retrieval}
The text-image retrieval model of this track is implemented using the same method as CLIP, and the overall framework is shown in the lower half of Figure \ref{fig:all-in-one}. We use the same transformer structure as Track 1 as the visual encoder, and use a text encoder to complete feature extraction of text data. During the training process, visual encoders and text encoders calculate visual and text features respectively, and achieve comparative learning of picture text pairs through contrastive training.

\section{Experiments}
\subsection{All In One}
\textbf{Implementation Details.} We chose vit-huge and vit-base models of transformers with different sizes as the backbone of Open-TransMind and set different output heads for the three different tasks of detection, segmentation, and classification. We merged three tasks and used a unified hyperparameter, such as the AdamW optimizer with an initial learning rate of 0.0001 and a weight decay of 0.0005. We adopt CosineAnnealingLR to drop the learning rate and implement different data augmentation methods for different tasks, including random resize, random cropping, random scaling in the range of 0.5 to 2.0, and random horizontal flipping, etc. Finally, we trained 120 epochs based on the ImageNet pre-training model with a batch size of 8 on eight A100 GPUs.


\begin{table}[t]
\centering
\begin{tabular}{c|cc}
\hline
Training Method                                  & Test & Val \\ \hline
training from scratch                      & 21.25 & 53.84  \\ 
CLIP                                    & 28.11 & 16.44  \\ 
CLIP finetune                           & 50.96 & 72.26  \\ 
CLIP finetune + All in One off-the-shelf & \textbf{53.54} & \textbf{72.56}  
\\ \hline
\end{tabular}
\caption{The quantitative results of the cross-modal text-image retrieval task on the test and validation sets we constructed.}
\label{table:track2_results}
\end{table}


\textbf{Evaluation Results.}
Based on the above experimental configuration, we conducted multiple sets of comparative ablation experiments. We chose the state-of-the-art methods described on \url{https://paperswithcode.com/} as baselines for comparison in order to confirm the efficacy of our approach, such as CAP, CABNet, and NiseNet, which are currently the best methods for classification, detection, and segmentation datasets of this track. To begin, we compared the performance of multi-task and single-task large models using Open-TransMind based on the vit-base backbone to validate the benefits of multi-task learning. As shown in Table \ref{table:all}, the multi-task joint training large model has significant advantages in the two tasks of segmentation and classification, which are respectively positive by 2.58\% and 8.86\%, and the detection task is slightly positive. At the same time, we compared the models trained using Task-MoE method and found that the classification and segmentation tasks performed better than multi-task joint training, while the detection task performance was basically the same, indicating that this method can alleviate conflicts between tasks. Multi-task joint training of large models has been shown to have higher generalization ability than single-task large models.
Second, we evaluated state-of-the-art methodologies against our Open-TransMind method built on the vit-huge backbone. Our vit-huge multi-task large model outperforms the vit-huge single-task large model and the vit-base large model, as can be observed. Meanwhile, it can achieve the state-of-the-art performance that is more advanced than baselines. Considering that large models need to occupy more hardware resources and take more training time, we provide Open-TransMind with vit-base backbone as our challenge code base.


\begin{figure}[t]
  \centering
\includegraphics[width=1.0\linewidth]{picture/retrieval_person.png}
   \caption{Visualization of similarity between different input texts and pedestrians with different attributes. Note that bright yellow represents high similarity, while dark blue represents low similarity.}
   \label{fig:retrieval_person}
\end{figure}


\subsection{Cross-Modal Image Retrieval}
\textbf{Implementation Details.} In the experiment, we use a transformer-based vit-base model as the visual feature encoder in the cross-modal Open-TransMind, and a naive transformer as the text feature encoder. We use an AdamW optimizer with an initial learning rate of 0.0001 and weight decay of 0.0005, and we adopt piecewise decay to reduce the learning rate. We only use basic resizing for data augmentation. The entire training process is conducted on 8 A100 GPUs for 20 epochs with a batch size of 128 for each GPU.

\textbf{Quantitative Results.} We conduct four comparative experiments: (1) training from scratch; (2) directly loading CLIP pre-training weights; (3) fine-tuning on the basis of CLIP pre-training weights; and (4) loading CLIP pre-training weights for fine-tuning and using the All-in-One model in Track 1 in an off-the-shelf manner, as an additional visual feature encoder to provide richer semantic information about the traffic scene.

As shown in Table \ref{table:track2_results} of the experimental results, we can see that the CLIP pre-training weights already have good multi-traffic participant text and image retrieval capabilities. After fine-tuning with real traffic scene data, CLIP achieves even better results. Among all the comparative experiments, the ``with All in One off-the-shelf'' method achieves the best results, which indicates that the All-in-One model has excellent traffic participant feature extraction ability through learning a large amount of traffic scene data.


\begin{figure}[t]
  \centering
\includegraphics[width=1.0\linewidth]{picture/retrieval_car.png}
   \caption{Visualization of similarity between different input texts and vehicles with different attributes. Note that bright yellow represents high similarity, while dark blue represents low similarity.}
   \label{fig:retrieval_car}
\end{figure}


\textbf{Qualitative Results.} Figure \ref{fig:retrieval_person} and Figure \ref{fig:retrieval_car} show the visualization of text-image retrieval, from which it can be seen that our model can effectively distinguish fine-grained differences in cross-modal information. In the pedestrian retrieval example in Figure \ref{fig:retrieval_person}, the query text in the first and second lines has the same attributes except for the different bag attributes. Open-TransMind can effectively distinguish this detailed attribute. The query text in the third and fourth lines has slight differences in the sleeve length of the shirt, and Open-TransMind can also provide correct retrieval results. In the vehicle retrieval example shown in Figure \ref{fig:retrieval_car}, the keywords in the query text in the first line are ``blue'', ``Dongfeng brand'', and ``truck''. Although the remaining three challenging query text changes the three attributes respectively, Open-TransMind can provide correct retrieval results. In addition, from this example, we can see that our model has certain knowledge emergence capabilities: there are three types of text and corresponding images in the training data: ``Blue Mercedes Benz'', ``Dongfeng brand'', and ``Truck'', but there is no text and corresponding images of ``Blue Dongfeng Truck''. The trained Open-TransMind can effectively complete the retrieval of ``Blue Dongfeng Truck'', as shown in the first column of row 1.

\section{Conclusion}
We propose a new baseline and benchmark for the first foundation model challenge of intelligent transportation, called Open-TransMind. Meanwhile, Open-TransMind is the first open-source foundation model of intelligent transportation with multi-task and multi-modal capabilities that we are aware of. Furthermore, Open-TransMind can achieve cutting-edge performance on traffic scenario detection, classification, and segmentation datasets, while also possessing cross-modal text-image retrieval capabilities. Contestants and researchers can investigate new solutions for multi-task co-evolution as well as the interrelationships of cross-modal. In the future, we plan to add more tasks, such as text recognition and point cloud detection. More researchers are expected to join the investigation of foundation model technology.
{\small
\bibliographystyle{ieee_fullname}
\bibliography{Open-TransMind}
}

\end{document}
