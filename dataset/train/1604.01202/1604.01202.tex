\documentclass[journal]{IEEEtran}
\linespread{1}
\usepackage{amsfonts}
\usepackage{amssymb}   
\usepackage[cmex10]{amsmath}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{verbatim}
\usepackage{cite}
\usepackage{color}
\usepackage{booktabs}\usepackage[ruled]{algorithm2e}
\usepackage{multirow}
\usepackage{diagbox}
\usepackage{bm}
\usepackage{array}
\usepackage{amsthm}
\usepackage{enumerate}

\newcommand{\bff}{{\mathbf{f}}}
\newcommand{\bY}{{\mathbf Y}}
\newcommand{\bW}{{\mathbf W}}
\newcommand{\by}{{\mathbf y}}
\newcommand{\ba}{{\mathbf a}}
\newcommand{\bb}{{\mathbf b}}
\newcommand{\bc}{{\mathbf c}}
\newcommand{\be}{{\mathbf e}}
\newcommand{\bX}{{\mathbf X}}
\newcommand{\bx}{{\mathbf x}}
\newcommand{\bZ}{{\mathbf Z}}
\newcommand{\bz}{{\mathbf z}}
\newcommand{\bI}{{\textbf{1} }}
\newcommand{\bp}{{\mathbf p}}
\newcommand{\bv}{{\mathbf v}}
\newcommand{\bPsi}{\Psi}
\newcommand{\bpsi}{{\psi}}
\newcommand{\btheta}{{\boldsymbol\theta}}
\newcommand{\bvpho}{{\boldsymbol\varrho}}
\newcommand{\bsigma}{{\boldsymbol\sigma}}
\newcommand{\bomega}{{\boldsymbol\omega}}
\newcommand{\bSigma}{{\boldsymbol\Sigma}}
\newcommand{\bOmega}{{\boldsymbol\Omega}}
\newcommand{\bvtheta}{{\boldsymbol\vartheta}}
\newcommand{\brho}{{\boldsymbol\rho}}
\newcommand{\bpi}{{\boldsymbol\pi}}
\newcommand{\diag}{\mbox{diag}}
\newtheorem{Def}{Definition}
\newtheorem{Pro}{Proposition}
\newtheorem{Prf}{Proof}
\newtheorem{Exa}{Example}
\newtheorem{Lem}{Lemma}
\newtheorem{Rem}{Remark}
\allowdisplaybreaks
\newcommand{\proproof}[1]{\noindent\textbf{Proof. } See Appendix #1.}
\IEEEoverridecommandlockouts
\begin{document}

\title{Multi-object Tracking  for Generic Observation Model Using Labeled Random Finite Sets}
\author{
Suqi Li, Wei Yi, Reza Hoseinnezhad, Bailu Wang and Lingjiang Kong\\
\IEEEauthorblockA{{University of Electronic Science and Technology of China, School of Electronic Engineering, Chengdu City, China} \\
{Email: kussoyi@gmail.com}}}

\author{
Suqi Li, Wei Yi, Reza Hoseinnezhad, Bailu Wang, Lingjiang Kong
\thanks{This work was supported
by the Chang Jiang Scholars Program, the National Natural Science Foundation of China under Grant 61771110, the Fundamental Research Funds of Central Universities under Grants ZYGX2016J031, the Chinese Postdoctoral Science Foundation under Grant 2014M550465 and Special Grant 2016T90845,  and the Australian Research Council (ARC) through the Linkage Project Grant LP160101081. (\textit{Corresponding author: Wei Yi.})


S. Li, B. Wang, W. Yi,  and L. Kong  are with the School of  Electronic Engineering, University of Electronic Science and Technology of China, Chengdu 611731, China (Email: qi\_qi\_zhu1210@163.com;  kussoyi@gmail.com; w\_b\_l3020@163.com; lingjiang.kong@gmail.com).

R. Hoseinnezhad is with the School of Aerospace, Mechanical and Manufacturing Engineering, RMIT University, Victoria 3083, Australia (Email: reza.hoseinnezhad@rmit.edu.au). 



}
}


\maketitle

\begin{abstract}
This paper presents  an exact Bayesian filtering solution for the multi-object tracking problem with the generic observation model. The proposed solution is designed in the labeled random finite set framework, using the product styled representation of labeled multi-object densities, with the standard multi-object transition kernel and no particular simplifying assumptions on the multi-object likelihood. 
Computationally tractable solutions are also devised by applying a principled approximation involving the replacement of the full multi-object density with a labeled multi-Bernoulli density that minimizes the Kullback-Leibler divergence and preserves the first-order moment. To achieve the fast performance, a dynamic grouping procedure based implementation is presented with a step-by-step algorithm. The performance of the proposed filter and its tractable implementations are verified and compared with the state-of-the-art in numerical experiments.
\end{abstract}
\IEEEpeerreviewmaketitle
\section{Introduction}
Finite set statistics (FISST)~\cite{refr:Mahler_book} has become a hot spot  in multi-object inference for the random finite set (RFS) framework can perfectly accommodate relatively accurate models for the behavior of multi-object dynamic systems, especially in terms of its ability to capture the randomness of both the number of, and the values of object states, as well as their statistical correlations. FISST has attracted
substantial interest from academia as well as the commercial
sector with applications spanning many areas such as, biology~\cite{refr:biology}, physics~\cite{refr:physics}, computer vision~\cite{refr:vedio-tracking}, 
multi-object
tracking~\cite{refr:tracking-1,refr:tracking-2,refr:GCI-MB}, and robotics~\cite{refr:robotics}. At the core of multi-object tracking is Bayes filter which is usually intractable due to suffering from the curse of dimensionality with computing set integrals and the combinatorial growth of computations involved with increasing number of objects.  In order to solve these problems, several tractable approximations of multi-object Bayes filter have been proposed successively, namely the  probability hypothesis density (PHD) filter~\cite{refr:tracking-1,refr:PHD}, the  cardinalized PHD filter~\cite{refr:CPHD,refr:CPHD-2}, and the multi-Bernoulli filter~\cite{refr:Mahler_book,refr:MeMber_filter1,refr:MeMber_filter}.

With the recent development of labeled set  filters~\cite{refr:label_1,refr:label_2,refr:label_3,refr:label_4, refr:label_5, refr:label_6, refr:label_7,Vo-Vo-JMS,robust-distributed-fusion} and their enhanced performance  compared to previous unlabeled versions, 
the study on the FISST-based multi-object tracking has recently turned its focus on the labeled random set filters. Vo \textit{et al.}~\cite{refr:label_1} proposed  a class of generalized labeled multi-Bernoulli (GLMB) densities\footnote{GLMB distribution was also termed as Vo-Vo distribution by Mahler in his book~\cite{refr:tracking-2}.}  and the relevant tracking filter, the GLMB filter.  The advantages of GLMB RFS family are that they are conjugate priors with standard multi-object likelihood, and are closed under the multi-object Chapman-Kolmogorov equation with respect to the standard multi-object transition kernel. 
Nevertheless, the -GLMB filter involves exponential growth in the number of posterior components with the number of objects and therefore, tractable techniques for truncating the posterior and prediction densities are also proposed in~\cite{refr:label_2}. Later, to further  decrease the computational costs, principled approximations of the  GLMB filter were proposed, including the labeled multi-Bernoulli (LMB) filter~\cite{refr:label_5} and the marginalized -GLMB filter~\cite{refr:label_3}.  These two filters are not only computationally cheaper, but also preserve the key statistical properties of the full multi-object posterior density.

All of the aforementioned labeled set filters are originally designed  for the standard observation model, and are not necessarily suitable for the generic observation model (GOM) which involves no simplifying assumptions made on the multi-object likelihood. In many applications, there might be sensor observations that cannot be accurately modelled by the standard multi-object likelihood. Examples include
the   track-before-detect (TBD) problem~\cite{refr:MeMber_filter,refr:tbd-2,refr:tbd-3,refr:tbd-4,refr:tbd-5,haichao1,haichao2}, superpositional sensors~\cite{refr:superpositional-1, refr:superpositional-2, refr:superpositional-3, refr:label_7}, merged observations~\cite{refr:label_4}, and extended objects~\cite{refr:extended-1}. Sensors providing such non-standard observations are widely used in  applications such as 
vehicle tracking using automotive radars, person tracking using laser sensors, acoustic amplitude sensors~\cite{refr:amplitude},  
and video tracking~\cite{refr:computer-vision, refr:vedio-tracking}. Consequently, there is a substantial demand for devising  multi-object tracking algorithms that work with the GOM.

 
There is no specified class of labeled RFSs that can be closed under the Bays' rule with respect to the GOM. In an independent work from this paper, Papi \textit{et al.}~\cite{refr:label_6} proposed  a decomposition of the general case of the  labeled multi-object (LMO) density, as the product of the joint existence probability of the label set and the joint probability density of states conditional on their corresponding labels. This decomposition  provides an explicit expression for the  LMO density, and is  fundamental  in the labeled multi-object filtering context especially with  the GOM.
Papi~\textit{et al.}~\cite{refr:label_6} also proposed an extension of the -GLMB filter that  works with the GOM,  by replacing the multi-object posterior with a principled -GLMB density approximation that  minimizes the  Kullback-Leibler divergence (KLD), and preserves the cardinality distribution and the first-order moment.
To distinguish it from the conventional -GLMB filter, it is referred to as the -GLMB-GOM filter in this paper.

Unlike the -GLMB-GOM filter which is an approximate solution for the  multi-object tracking problem with the GOM, the novel solution presented in this paper, the  LMO-GOM filter\footnote{Preliminary results have been published in~\cite{refr:conference-GOM}. This paper provides a complete and detailed picture with extended results, proofs, and experiments.}, is an exact solution for the same problem.
The prediction equations of the LMO-GOM filter are exact under the standard multi-object transition kernel which embeds  the basic assumptions commonly made with multi-object tracking solutions, such as Markovian dynamics for object states, and the independence of the  birth process from other object states.  The update equations of the LMO-GOM filter are not  based on any approximations or simplifying assumptions with the multi-object likelihood model. Essentially, the -GLMB-GOM filter~ is an  approximation of the LMO-GOM filter with the multi-object posterior  approximated as a principled -GLMB density. 





Another major contribution is a generalization of the LMB filter, called the  LMB-GOM filter, that works with generic multi-object likelihoods. The LMB-GOM filter is devised by approximating the original multi-object posterior  with the closest LMB density in terms of its KLD. The approximate LMB density also matches the  first-order moment of the original multi-object posterior.  
Our analysis shows that    the computational cost of the LMB-GOM filter  is  less than the -GLMB-GOM filter.

A third major contribution of is  this paper is a variant of the proposed LMB filter, called the grouping based LMB-GOM (G-LMB-GOM) filter which is essentially an efficient implementation  of the  LMB-GOM filter.  The G-LMB-GOM filter is based on a dynamic grouping procedure which enables parallelization.
This parallel implementation significantly reduces both the number and the dimension of integrals,    leading to a substantial improvement in computational costs  as well as the numerical accuracy  when the computing and memory  resources are  limited.  In some cases, the resulting improvements in the numerical accuracy are well beyond the extent of inaccuracies stemmed from the grouping procedure.




The performance of the proposed algorithms including the LMO-GOM and LMB-GOM/G-LMB-GOM filters, implemented via Sequential Monte Carlo (SMC) method, are presented and  demonstrated in numerical experiments.

The rest of the paper is organized as follows. A background on  notations,  labeled RFSs and the formal statement of the labeled multi-object tracking problem is provided in Section \ref{chp:2}. Section \ref{chp:4} proposes the LMO-GOM filter and Section \ref{chp:5} presents the ``best'' LMB  approximation for the general LMO density and the resulting LMB-GOM filter. Section \ref{chp:6} provides a comparative summary for different labeled multi-object tracking algorithms with the GOM.  Section \ref{chp:7} demonstrates the performance of the proposed algorithms via numerical experiments. Conclusions are remarked in Section \ref{chp:8}.

\section{Background}\label{chp:2}
\subsection{Notations}
We adhere to the convention that single-object states are
represented by lowercase letters, e.g., , , while multi-object
states are represented by uppercase letters, e.g., , .  To distinguish labeled states and distributions from the
unlabeled ones, bold-type letters are adopted for the labeled
ones, e.g., , , . Moreover, blackboard bold letters represent spaces, e.g., the
state space is represented by , the label space by . The collection of all finite subsets of 
is denoted by .

The labeled single-object state  is constructed by augmenting
a state  with a label . The labels are usually
drawn from a discrete label space, , where
all s are distinct and   is the set of positive
integers.

We use the multi-object exponential notation

for real-valued function , with  by convention.

To admit arbitrary arguments like sets, vectors and integers, the generalized Kronecker delta function and the inclusion function are repectively given by

If  is a singleton, i.e., , the notation  is used instead of . For functions  and  defined on , the inner product is denoted by
.

\subsection{Labeled RFS}
The notion of labeled RFSs was firstly proposed in \cite{refr:label_1} to address the uniqueness of tracks.   A \textit{labeled RFS} ~\cite{refr:label_1,refr:label_2} with (kinematic) state space  and (discrete) label space
  is an RFS on  such that each realization  has distinct labels. Let  be the projection , and hence  is the set of labels of . A labeled RFS and the set of its labels have the same cardinality, namely, . The function  is called the distinct label indicator.
\subsubsection{Decomposition of LMO Density}
For an arbitrary labeled RFS, its multi-object density can be decomposed as the product of the joint existence probability of the label set and the  joint probability density of states conditional on their corresponding labels~\cite{refr:label_6}. 
The definitions of necessary quantities  and the decomposition of the LMO density are briefly reviewed by providing a more rigorous  definition.

The set of labels  of a labeled RFS  (distributed according to ) is distributed according to the marginal

The quantity  is referred to as the joint existence probability of the label set  in this paper.
\begin{Def}\label{definition:1}
Given an LMO density  on , we define a function  on  as (\ref{joint probability density}).
\begin{figure*}

\hrulefill
\end{figure*}

\end{Def}
\begin{Rem}\label{remark:1}
Given a certain  set of distinct labels , if  and  the weight ,  is essentially a joint probability density on  conditional on their corresponding labels . Indeed, from Definition \ref{definition:1}, the LMO density  can be decomposed as 
 
\end{Rem}

\subsubsection{Common Labeled RFSs}
The most commonly used labeled RFSs in  existing labeled multi-object filtering algorithms belong to  the GLMB RFS family \cite{refr:label_1}  \cite{refr:label_2}. 
They are distributed according to 

where  is a discrete space, each  is a probability density, and each  is non-negative with .

The class of LMB RFSs is a  subclass of the GLMB RFS family. An  LMB RFS with state space  and label space  
is distributed according to \cite{refr:label_1,refr:label_2,refr:label_5}

where

and  represents the existence probability of track , and 

 is the probability density of the kinematic state of track  given its existence. 

From  (\ref{LMB}) and (\ref{weight-LMB}), an LMB RFS is completely determined by the parameters  for each  and a function  defined on . 
Also, an LMB RFS can be completely characterized by its
LMB
parameters, i.e.,

with 

Note that the definition in (\ref{p-alpha}) is applied for all the LMB RFSs throughout the paper.
\begin{Def}\label{definition:2}
Given the 
LMB
parameters , the labeled Bernoulli component  is referred to as  track , with  representing the joint spatial and label density, and    the probability of existence of  track .
\end{Def}


\subsection{Multi-object Bayes Filter}
Multi-object Bayes filter is at the core of multi-object filtering in RFS framework. This subsection provides  a   review of  the multi-object Bayes filter in the formulation of labeled multi-object state, which is firstly presented in \cite{refr:label_1}.  To incorporate object tracks, objects are identified by an ordered pair of integers , where  is the time of birth, and  is
a unique index to distinguish objects born at the same time.
The label space for objects born at time , denoted as , is
then . An object born at time  has  a state .
The label space for objects at time  (including those born
prior to ), denoted as , is constructed recursively by
. A multi-object state  at time , is a
finite subset of . Note that  and  are disjoint.

The multi-object posterior density  is propagated forward recursively by the multi-object Bayes filter,

where  is the  multi-object predicted density from time  to time ;  is the multi-object transition density;  is the multi-object likelihood function and  denotes the observations of multi-object state  at time . Note that  is a general notation which can represent a vector observation , or  a set  observation , depending on the observation model  adopted.

For convenience, in what follows we omit explicit references
to the time index , and denote ,   , , , , , and .
\subsection{Multi-object Transition Kernel}
This paper considers the standard multi-object transition model  \cite{refr:Mahler_book,refr:label_1}. Given a labeled multi-object state , each state  either continues to exist at the next time step with probability  and evolves to a new state  with probability density , or dies with probability . According to Definition~\ref{definition:1}, The  set of new objects born at  the next time step defined on  is distributed according to

Note that  the birth density  also can be specified as an LMB density of form (\ref{LMB}) or a  GLMB density of form (\ref{GLMB}).


A multi-object state  is  the superposition of surviving objects and  newly born objects. Assuming that the surviving and the newly born object states evolve independently, the multi-object transition function is given by~\cite{refr:label_2}
 
where
 
 
\subsection{Generic Observation Model}
The standard formulation in the RFS based multi-object tracking is based on the standard observation model \cite{refr:tracking-2} where observation data is assumed to have been preprocessed into thresholded detections,  each object is assumed to cause at most one detection, and each detection is assumed to be either a false alarm (clutter) or generated from one object. The tracking filters under the standard observation model have  been well investigated. One remarkable development is    the GLMB family of densities are conjugate prior with respect to the standard multi-object likelihood. Utilizing this property, the GLMB filter is proposed as a closed form of the multi-object Bayes filter under the standard observation model, and its   performance has been well demonstrated in \cite{refr:label_1,refr:label_2}.

This paper considers the \textit{generic observation model }which  has the indication that  no simplifying assumptions on the multi-object likelihood are made. The terminology ``generic measurement (observation) model''  first arised in \cite{refr:label_6} for  the -GLMB filter with the  generic multi-object likelihood.




The  considered GOM  covers both the standard and non-standard observation models.  As the first  category of sensor models has been well  investigated, this paper mainly focuses on the non-standard  sensor models.  Below, we present two typical examples for the non-standard observation models, namely, the pixled TBD model
 and the acoustic amplitude sensor model.
 
{\noindent \textit{\textbf{Example 1} - Pixeled TBD Model: }
The surveillance
region is divided into  cells. The observations at the current time step  are collected in
the vector , with  being the intensity
observation obtained in the th cell. An object  can  illuminate several cells of its surroundings.
Within the effective template of ,
   the intensity contribution from  to the th cell follows a point spread function \cite{refr:beyond_kalm_filer}  

  where  is the source intensity,  is the blurring factor,  and  are the cell side lengths, and  denotes the position of the th cell in a two-dimensional image of the surveillance region.  For the cells beyond the  effective template of , the intensity contribution .
  
 The  observations obtained from different cells
are assumed to be independently distributed conditioned on the multi-object state , and thus the multi-object likelihood is
where  denotes the likelihood of the th cell.
The distribution of  varies from different applications. For instance, in the infrared/image application  
\cite{refr:MeMber_filter,refr:beyond_kalm_filer}, 
the likelihood  is assumed to be a Gaussian distribution, 

where  denotes the Gaussian probability density
evaluated at  with mean  and covariance matrix .
  


\noindent \textit{\textbf{Example 2} - Acoustic Amplitude Sensor:}
We consider a wireless sensor network consisting of  sensors \cite{refr:amplitude}. At a certain time step,   sensor  () acquires an observation . 
Each object   emits a sound with amplitude  that is assumed to be constant. For  sensor  located at  position , the received sound amplitude due to 
target  is modelled as , where    is the position of object ,  and  is the path loss exponent. The scalar observation  obtained by sensor   is then given by

with

where  for  are zero-mean Gaussian  noise variables of equal variance . Assume that  are mutually independent and independent of . The likelihood function  between  observation vector  and  multi-object state  is given by






Note that both (\ref{eq:TBD}) and  (\ref{likelihood-acoustic}) are highly non-linear likelihoods which are not closed under the Bayes  update equation (\ref{update}).




\section{The LMO-GOM Filter}\label{chp:4}
In this section, we derive the multi-object prediction and update equations of Bayes filter by specifying the multi-object prior and posterior as the product styled LMO densities provided in (\ref{P-LMO}). The result is an exact solution for labeled multi-object  Bayes filter with GOM under the standard multi-object transition model, and thus is called as  the  LMO-GOM filter. Furthermore, an SMC implementation of the LMO-GOM filter is presented. 
\subsection{Recursive Equations of the LMO-GOM Filter}
\begin{Pro}\label{pro:1}
Suppose that  the current multi-object prior density is a general LMO density of the form  (\ref{P-LMO})
and the birth density is also a general LMO density of the form (\ref{LMO born}),
then the multi-object predicted density under the multi-object transition function of the form (\ref{multi-object transition function}) is given by

on state space  and label space ,
where

with
 

\end{Pro}

\proproof{A}

Proposition \ref{pro:1} explicitly describes how to calculate   and   of the  multi-object predicted density from   and  of the multi-object  prior  density. We note that  is  the density of the surviving objects with  and   shown in (\ref{ws}) and (\ref{Ps}).  For a  given label set ,  is the weighted sum  of the prior weights  over all subsets of  that contain the surviving set~.  
The function  given a certain  label set   is also a weighted sum of  terms over all subsets of   that contain the surviving set . 
  in (\ref{Ps-I})  for a certain  is a non-normalized joint density evolved from the prior joint probability density   with the ``pseudo'' transition density .   is conditional on that the previous label set is  and  only the objects with the label set  exist after evolving.  in (\ref{where_0}) is essentially the normalizing constant of  with .

Utilizing the independence of surviving objects and newly born objects, the multi-object predicted density can be obtained by multiplying   the weights  and the corresponding joint probability densities of newly born objects and surviving objects.
\begin{Pro}\label{pro:2}
Suppose that the current multi-object predicted density is a general LMO density of the form (\ref{multi-object predicted density}), then the multi-object posterior density under a generic multi-object likelihood  is given by

on state space  and the label space ,
where
 
with
   
\end{Pro}

\proproof{B}

Proposition \ref{pro:2} explicitly describes how to calculate the parameters  and   of the  multi-object posterior density from the parameters  and  of the multi-object  predicted   density. For a given label set , the posterior joint probability density  in (\ref{where_p}) is computed from the prior joint probability density     via ``Bayes' rule'' with likelihood . For a given label set , the posterior weight  is  proportional to the predicted weight   scaled by the normalizing constant .


 \subsection{The SMC Implementation of the LMO-GOM Filter}
In the above subsection, the combination of Propositions \ref{pro:1} and \ref{pro:2} provides an exact Bayesian solution  by adopting the decomposition of  the LMO  density  in the form of (\ref{P-LMO}). Hence, an intuitive  implementation of the LMO-GOM filter is to recursively compute the functions  and  at each time step. However, when implementing the LMO-GOM filter,   the  approximation of     is not straightforward since   (defined on ) is not a probability density.  To this end, we  represent the product styled of the LMO density of the form (\ref{P-LMO}) in another equivalent form  as  Remark \ref{mixture-LMO}.
\begin{Rem}\label{mixture-LMO}
An LMO density   on  can be expressed as a mixture of multi-object densities, 

where

in which the definitions of  and  are given in Definition \ref{definition:1}, 
 denotes the  existence probability of the label set  satisfying , and   is the multi-object probability density (defined on ) conditional on the existence of the label set  . Hence,  is completely characterized by a set of parameters .
\end{Rem}



The integrals of  and  have the following relationship. For any , and given an arbitrary function  on , we have

Eq. (\ref{integal-1}) indicates that the set integral of  is equivalent to  the Euclidean notion of integral of the joint probability density  on  


Utilizing the formulas provided in Remark \ref{mixture-LMO},  implementing the LMO-GOM filter, based on Propositions \ref{pro:1} and \ref{pro:2}, amounts to computing the predicted parameter set   
with  
and 
and the posterior parameter set  
with 
forwards in time.


As  it was mentioned earlier, our algorithms are mainly designed for the non-standard observation model which usually involves the non-Gaussian/non-linear model and has no closed-from solution. Hence, in this subsection, we provide an SMC implementation of the LMO-GOM filter.  Each  is represented by a set of weighted particles. Associated weights, and normalizing constants can be computed from 
particles and their weights.

Suppose that the current prior parameter set is  where each  is approximated with a set of particles , i.e.,

Utilizing (\ref{integal-1}), the  quantities in the prediction step are computed as 


Then   is computed by substitution of (\ref{SMC-eta-I-J})  into (\ref{ws}), and   is computed by substitution of the computed  into (\ref{w-add}).


For each label set ,  firstly, choose a subset  of   according to  
Utilizing (\ref{p-add}), (\ref{Ps}) and (\ref{where_p}), the posterior parameter  is computed as

where

By employing the idea of the auxiliary particle filter \cite{refr:tbd-2,refr:tbd-3,refr:tbd-4}, sampling from (\ref{update-P}) can be achieved by sampling from the higher dimensional joint density

where the auxiliary variable  is the previous label set from which the current label set  is evolved, and  the auxiliary variable  is the index on the sample at the previous time step conditional on the previous label set .
The auxiliary variables aid in the sampling of suitable values of the multi-target state . They are discarded after the sampling procedure is completed. States , the previous label set ,
and particle indices ￼ ￼ are drawn from an importance density  for , and the un-normalized weight is computed as 

A  feasible choice of the  proposal function  is as follows:

In this case, the un-normalized weight is computed as 

Note that it is possible to design a more sophisticated proposal density than (\ref{proposal-function}), but it is beyond the scope of this paper.


Utilizing (\ref{integal-1}), the quantity  is computed as 

and the posterior parameter  is computed by substitution of (\ref{SMC-eta}) into (\ref{where_w}).

Resampling and Implementation Issues: After the update step, for each , perform resampling \cite{refr:SMC_Gordon} to obtain an evenly weighted particle
set. To reduce the growing number of parameters, the pair of posterior parameters  with existence probabilities  below a threshold are discarded \cite{refr:label_1,refr:label_3,refr:label_6}.






\subsection{Discussions and Analysis}
The LMO-GOM filter provides an exact Bayesian solution for the labeled multi-object tracking problem under the GOM and the standard transition kernel. Nevertheless, in general, the LMO-GOM filter can be computationally prohibitive, especially for a large number of objects. Computing integrals on high-dimensional spaces (the integral of ) and exponential growth of the number of parameters with the number of objects are the two main reasons
in many applications. 

Observing  Proposition \ref{pro:1}, the   prediction step of the LMO-GOM filter can be further simplified for particular multi-object priors such as the -GLMB and LMB densities. 
This can be achieved due to the independence assumption between object motions when formulating the multi-object transition kernel. The -GLMB-GOM  filter~\cite{refr:label_6} is essentially derived  by approximating the multi-object posterior as a principled -GLMB density and assuming the -GLMB prior. 
\begin{Rem}\label{remark:3}
The implementation of the LMO-GOM  filter only involves  one source of inaccuracy which is the numerical error caused by the Monte Carlo (MC) approximation of the high-dimensional integral. 
Based on the convergence properties of the MC approximation~\cite{refr:convergence_partical}, when the number of samples approaches infinity, the numerical errors of the integral computations approach zero, and the LMO-GOM filter is implemented with perfect accuracy.  Hence, with sufficient computing resources, the LMO-GOM filter is expected to exhibit the optimal performance, and 
possibly served as a performance benchmark in labeled multi-object tracking with the standard transition kernel. 
In comparison,  the implementation of the -GLMB filter involves two sources of inaccuracy. One is related to the approximation of posteriors and the other to the MC approximation.   Due to the first source of inaccuracy, even if the number of particles approaches infinity, the -GLMB filter is not exact.
\end{Rem}


\section{The LMB-GOM Filter}\label{chp:5}
As an efficient approximation of the LMO-GOM filter,  the -GLMB-GOM filter  alleviates the computation burden by simplifying the prediction equation. However, the -GLMB prediction can be still intractable (both memory- and computational-wise) due to the exponential growth of the number of  terms of multi-object exponentials in the predicted -GLMB density  with the number of objects.

 In this section, we further explore  more tractable approximations of the LMO-GOM filter.  \cite{refr:label_5} proposed an LMB  filter for the standard multi-object likelihood which approximates the -GLMB multi-object posterior as a principled LMB density preserving the first-order moment, and further proposed a dynamic grouping procedure based implementation  which drastically reduces execution time with slight accuracy promise by exploiting the mathematical formulation of the LMB prior.
Motivated by the LMB filter  and its fast implementation proposed in \cite{refr:label_5}, in this section, we seek the ``best'' LMB   approximation to   replace the full  multi-object posterior under the GOM, and consequently,  develop an extension of the LMB filter for  the GOM,  referred to as the LMB-GOM filter.  Furthermore, we also present an efficient implementation   for the  LMB-GOM filter based on a dynamic grouping procedure. 
 
 


\subsection{The ``Best'' LMB Approximation}
In this subsection,  we  derive  the ``best'' LMB approximation of the general LMO density.   Herein, the ``best'' approximation means the  best information-theoretic fit in terms of the minimal KLD. Propositions \ref{pro:3} and \ref{pro:4}, respectively, derive the explicit formulas of the labeled PHDs for the general LMO density and the LMB density, which are the basis of  the derivation of the ``best" LMB approximation.  Proposition~\ref{pro:5}  provides the explicit formula for the  ``best'' LMB  approximation of the general LMO density.
\begin{Pro}\label{pro:3}
Given an arbitrary LMO density  on state space  and label space , the labeled PHD of  is
 
where ``'' denotes set difference, and 
 
\end{Pro}
\proproof{C}





\begin{Pro}\label{pro:4}
Given an LMB RFS  with the LMB parameters , the labeled PHD of  is

with 
.
\end{Pro}

\proproof{D}




\begin{Pro}\label{pro:5}
Given an arbitrary LMO density with the parameter set ,
the LMB  density in the class defined in (\ref{LMB}) which minimizes the Kullback-Leibler divergence from , and  preserves the first-order moment of  is given by  where 

The density  is referred to as the ``best'' LMB approximation of .
\end{Pro}

\proproof{E}
\subsection{Recursive Equations of the LMB-GOM Filter}
In this subsection, we apply  the derived ``best'' LMB approximation  to the labeled multi-object filtering problem, and develop the LMB-GOM filter.  The following proposition provides the update equations of the LMB-GOM filter.
\begin{Pro}\label{pro:6}
 Suppose that the current multi-object predicted density is an LMB density with the LMB parameters . Under a generic multi-object likelihood , the best ``LMB'' approximation of the multi-object posterior is , where


with

\end{Pro}




\proproof{F}





Proposition \ref{pro:6} explicitly describes how to calculate the posterior 
LMB parameters    from the predicted multi-Bernoulli parameters . The update stage of the LMB-GOM filter  has three steps:

\noindent-- Write the predicted LMB density in the general LMO density form, i.e., ;

\noindent-- Compute the full multi-object posterior density  from the general LMO density form according to Proposition \ref{pro:2}, resulting in ; 


\noindent-- Approximate   with its ``best'' LMB approximation  according to Proposition \ref{pro:5}.
\begin{Rem}\label{remark:4}
From Propositions \ref{pro:5} and \ref{pro:6}, we can deduce that in the LMB filter proposed in \cite{refr:label_5}, the LMB RFS which matches the first-order moment of the -GLMB posterior, also minimizes the KLD from the -GLMB posterior, among all the LMB densities.
\end{Rem}


Utilizing Proposition \ref{pro:6} and the prediction equations of the LMB filter in~\cite{refr:label_5}, we   can obtain the recursive equations of the LMB-GOM filter. 
Under the standard object motion model, the  multi-object predicted density 
 is  an LMB density if the multi-object prior is an LMB density~\cite{refr:label_5}. Moreover, based on  Proposition \ref{pro:6}, the  multi-object posterior
density  can  be approximated as a principled LMB density under the GOM, if the  multi-object predicted density  is an LMB density. The specified  prediction  and update steps of the LMB-GOM filter are given via the following:
\\
\\
\textit{\textbf{LMB prediction}}: Given the current prior LMB density with the LMB parameters  and the LMB multi-object birth with the LMB parameters  ,  the multi-object prediction is  another LMB density on state space  and finite label space  given by 
 
 where
 
\textit{\textbf{LMB update}}: Given the current predicted LMB density
 and 
  the generic multi-object likelihood function , the  approximate multi-object  posterior   is another LMB density     computed by  (\ref{update_r}) and (\ref{update_p}).
\begin{Rem}\label{remark:5}
Compared with the -GLMB-GOM filter, the LMB-GOM filter involves  less computation in its prediction step because it not only reduces the integration space to single-object space, but also involves a number of integrals that increases linearly with the object number.  Actually, the computational efficiency of the LMB-GOM filter can be achieved because  the ``best'' LMB approximation completely loses correlation between object states,  while  the ``best''  -GLMB approximation still preserves part of the correlation between object states.  The -GLMB density has the ability to depict the statistical dependence between points \cite{refr:label_6}. However, unlike the general LMO density,  the points in a -GLMB RFS are assumed statistically independent conditional on their existences with a set of distinct labels. This  assumption can lead to a scarification of some part of information on correlation between object states when approximating the full multi-object posterior as the ``best'' -GLMB  approximation. As for the LMB density, the points (including object states and their labels) are  assumed to be statistically independent.  Hence, information on correlation between object states is completely discarded when approximating the full multi-object posterior as the ``best'' LMB approximation. 
\end{Rem}
\subsection{The SMC Implementation of the LMB-GOM Filter}



Suppose that the current  LMB prior is parameterised by  , where each single object density 
  is approximated by a set of weighted particles. 

At the prediction stage, for each label  of the surviving objects, the predicted existence probability  and the probability density   are evaluated using the particles and the corresponding weights of . For explicit calculation formulas, refer to the SMC implementation of multi-Bernoulli filter\cite{refr:MeMber_filter1}.

At the update stage, in the first place, we evaluate the parameter set  of the full multi-object posterior .  Similar to the SMC implementation of the LMO-GOM filter presented in Subsection III-B, for each label set , the multi-object density   is approximated  by a set of weighted particles  ,
where each particle  for  is drawn from a   properly designed  importance density.

Then for each label , the updated LMB parameters  and  can be calculated from the parameter set   utilizing the particles and the corresponding weights of  each  . A key term when calculating the single object density  is    in (\ref{update_p}).  By utilizing (\ref{update_where}) and (\ref{integal-1}), this term  is evaluated  as,

After the update step, the resampling and truncation processes are also applied similar to the SMC implementation of the LMO-GOM filter.


\subsection{Grouping based LMB-GOM Filter}
The proposed LMB-GOM filter can be seen as  an extension of the LMB filter proposed in~\cite{refr:label_5} that accommodates generic multi-object likelihood. To enhance the implementation efficiency of the LMB filter for the  standard observation model, the parallel group update via the construction of the so called ``groups'' was proposed in~\cite{refr:label_5}. Each group contains only closely spaced objects and their associated measurements. This method can achieve significant reductions in computation because updating independent groups in parallel is usually much faster than updating the entire multi-target state. In this subsection, we also extend the parallel group update to the LMB-GOM filter.
Combining the prediction step of the LMB-GOM filter with the parallel group update leads to a variant of the LMB-GOM filter,  called the grouping based LMB-GOM (G-LMB-GOM) filter.


In this subsection, the observation set is considered as an RFS  defined on the observation space .  
By  exploiting the  mathematical formulation of LMB RFSs, the LMB predicted density  admits an exact decomposition based  an arbitrary partition of tracks in the  label space , denoted by , i.e.,

where  . The decomposition in (\ref{De-LMB-Prediction}) is achieved by utilizing the independence between  Bernoulli components and the convolution formula given in [1, p.385].


Having the flexible decomposition of the LMB prediction, as long as there exist one  partition of the tracks such that the multi-object likelihood can be decomposed as

where  for  denotes the observation subset associated with the tracks in  and , then the parallel group update can be achieved, i.e., 


The decomposition of the multi-object likelihood  in (\ref{De-likelihood}) essentially demands that the effects of different multi-object subsets  on the observations can be separated. Specifically, each observation subset  is only correlated with the multi-object subset .
Nevertheless, this demand is not necessarily valid  for the GOM. Hence, in the following, we firstly discuss the constrains on the observation model. Then we provide a principled method to partition  tracks and observations for which the decomposition in (\ref{De-likelihood}) holds approximately. Finally, the parallel group update is formulated. 
\subsubsection{Decomposition of the Likelihood
} 
The following assumptions on the observation model are made.


\textit{A.1}: The observations  are  conditionally independent under   the multi-object state ;

\textit{A.2}: The object with state   only contributes to the observations within a region .


The first assumption is common in multi-object tracking (see, for example, \cite{refr:MeMber_filter,refr:tbd-5,refr:superpositional-2}).  
The second assumption indicates that each observation  is generated by a set of objects  (  can also be an empty  set).   is referred to as the valid observation region (VOR) of  object , and  is referred to as the VOR of the state set .  The VOR is related to the sensing characteristic of a sensor.

\begin{Pro}\label{pro:7}
Given an observation model characterized by multi-object likelihood ,  and satisfing Assumptions A.1 and A.2, if a subset of object states,   satisfies 

 then the observation subset  is statistically independent of  object states , and the observation subset  is statistically independent of  object states , i.e., the multi-object likelihood can be represented as
 
Herein,  is called as an  isolated object cluster. 
\end{Pro}
\proproof{G}


According to Proposition \ref{pro:7},  as long as  one isolated object cluster arises,  the multi-object likelihood can be further  decomposed as (\ref{Decomposition-likelihood}). Observing (\ref{isolated-object-cluster}), one can easily obtain that the smaller the size of the VOR  is, the more likely it is  for an isolated object cluster to arise. Generally, the sensor models can be divided into three categories in terms of different types of VORs. 
 

 \textit{Type I: Completely confined VOR. The size of   is relatively small compared with the observation space , namely,  an object  can only affect  the observations in a  very limited region,  and then the  contribution of  on the observations beyond this region is zero. } For example, in video tracking \cite{refr:MeMber_filter,refr:computer-vision,refr:vedio-tracking}, a  rigid body   can only occupy several  pixels of its surroundings. For this category of sensors, it is easy to produce  isolated object clusters  and then exactly decompose the likelihood according to Proposition \ref{pro:7}. The pixeled TBD observation model employed in subsection III-E belongs to this type.




Except for Type I sensors, there also exist sensors  whose VOR is  the whole observation space (or a region having a comparable  size with the whole observation space). Hence,  all the objects   contribute to almost all  the observations, making the  observations correlated with all the objects. These sensors    can be further classified into two types as follow.

 \textit{Type II: Approximately confined VOR. Correlation between  observation  and  object  decays as the ``distance'' between  and  increases. } The acoustic sensor network  observation model \cite{refr:amplitude} shown in Section III-E is a typical example. When the distance  between the sensor  and the object   is sufficiently large, the received sound amplitude  at sensor  due to the object  decays rapidly according to . Hence, the contribution of  the object  to the observation  at sensor  can be negligible. Consequently,   by suitably truncating the complete VOR,  the decomposition of the multi-object likelihood according to (\ref{Decomposition-likelihood}) can be achieved with an affordble approximation error. 

  \textit{Type III: Full VOR.  In this case, observations are strongly correlated to all the objects, and the decomposition of likelihood  is  not possible.}
For instance, when estimating the slowly diffusing sources using a sensor network, the received observations  at a certain sensor are strongly affected by  all the remote sources \cite{diffusion_source}. 

\subsubsection{Grouping and Parallel Group Update}
For  the standard observation model, track grouping is based on a standard gating procedure which also partitions the observation set~\cite{refr:JPDA,refr:label_5}.  
Inspired by this, this subsection provides  a principled method to construct independent groups of tracks and observations for a wide variety of observation models. 
The following two definitions  will be used in formulating our method. 

\begin{Def}\label{definition:3}
Let  be a density function of a random variable . A measurable subset of  the sample space  of , denoted by  is called the highest  density  region (H.D.R.) of confidence  if
 
a) ;
 
b) for  and , .
\end{Def}

\begin{Rem}\label{remark:6} The concept of H.D.R. is provided in \cite{refr:HPD,refr:HDR}. The posterior density for every point inside the H.D.R.  is greater than that for every point outside of region. Thus, the region includes the more probable values of . Usually, the confidence  is set to be very close to one, e.g. . Thus  is negligible for  and can be approximated with 0.
\end{Rem}

\begin{Def}\label{definition:4}
Consider an LMB density with the LMB parameters .  Denote the H.D.R. of
confidence  for   by , with .    is called as the VOR of track 
. Tracks  and  are referred to as the coupling tracks   if their  VORs  have intersection, i.e., .
\end{Def}



 Given the LMB prediction with the LMB parameters ,  the predicted label set is partitioned as  such that no track in  is coupled with any track in  for any , where  denotes the disjoint union.  In other words, ,

Accordingly,  the multi-object observation set  is partitioned as  where 

  denotes the observation subset related to the group of tracks with label subset ,  , and 

 denotes the observation subset  having no associated tracks.

The above partitions of the predicted label set and the observation set naturally produce a set of  pairs  with each , ,  referred to as a group. 


Consider the multi-object state  with  confidence  sufficiently large. According to Definition \ref{definition:4}, for each group , we have 
 then by the combination of (\ref{grouping-c}), the observation subset  of  any other group with  has the following relationship,
 Under Assumption \textit{A.1}, by utilizing the independence between any  and  (), the multi-object  likelihood can be  decomposed 
as

and consequently the posterior density is decomposed as

where  denotes the posterior density of the th group.

For the multi-object state  with  confidence  sufficiently large, the predicted density  is negligible, and consequently the corresponding posterior density  is negligible.


As a result,  the full Bayes update  can be approximated as  a group  of parallel  updates.  Specifically, the LMB prediction  for the th group, 
is updated by the  likelihood   resulting in the posterior density  of the th group.
\subsubsection{Partition Criterion}
An important issue of the partition procedure is  the choice of the criterion used to  judge whether two tracks are coupling or not.  A straightforward criterion according to the previous subsections  is 
the  predicted tracks  and  exhibit significant coupling  if their VORs have the intersection, i.e., 





In practice, the criterion can be simplified by the combination of the specific observation model.  Taking the two observation models provided in Section III-E as examples, we provide principled criterions as follow.


 -- For the pixeled TBD model, 
 as suggested by \cite{refr:tbd-2,refr:tbd-3, refr:tbd-4},  the  predicted tracks  and  exhibit significant coupling 
if their distance is small, i.e.,

where   is a grouping threshold and  is a distance function which depends on the way in which observations are acquired and the statistics of the predicted tracks.  A feasible  distance function is  

where  is the  predicted position of the track , and  denotes 2-norm distance~\cite{refr:tbd-2,refr:tbd-3, refr:tbd-4}. 
In this case, the threshold  is mainly decided by both the covariance of    and the VOR . Analytical details of the selection of the threshold can be found in~\cite{refr:tbd-3}. 
Another suitable distance function can be the Mahalanobis distance (MHD) which depicts the impacts of both state and covariance estimate, and then the threshold is mainly  decided by the VOR .
 
-- For the acoustic amplitude sensor model, a feasible    criterion is   the predicted tracks  and  exhibit coupling if

 where   is a given threshold for which the value  is sufficiently small. 


After the criterion is established, we can obtain the partition of tracks by adopting suitable clustering algorithms~\cite{refr:clustering_method}. 
Then according to (\ref{grouping-Z-1}) and (\ref{grouping-Z-2}), the associated observation subset of each group can be obtained.







\begin{algorithm}[t]\label{algorithm: generate lgmb}
\caption{The G-LMB-GOM filter.}
\begin{minipage}{0.93\columnwidth}
{\small
\textbf{Input: } the prior LMB density with the LMB parameters .  
\begin{enumerate} []
\item  
Perform LMB prediction  and
 compute the predicted LMB density 
with the LMB parameters 
  using (\ref{prediction-LMB})-(\ref{where_1}).
\item 
Partition the predicted tracks in  and the observation set  into  groups   according to (\ref{grouping-c})(\ref{grouping-Z-2}).
\item \textbf{for }\textit{each group} ,  \textbf{do}

\item Perform LMB update under the multi-object likelihood  using (\ref{update_r})-(\ref{where_2}) and get the ``best'' LMB  approximation of posterior multi-object density .
\item \textbf{end}
\end{enumerate} 
\textbf{Output:} the posterior LMB density with the LMB parameters .}
\end{minipage}
\end{algorithm}

\begin{Rem}\label{remark:8}
The G-LMB-GOM filter can be extended to the case of vector observations easily, because a random vector can be equivalently transformed to a labeled RFS having a constant cardinality~\cite{set-jpda}. 
\end{Rem} 

\begin{Rem}\label{remark:8}
If a Type III sensor is used or all tracks are too close to be isolated, then the partition of tracks and observations is not possible. In this case, the G-LMB-GOM filter degenerates to the LMB-GOM filter automatically. 
\end{Rem} 
\subsubsection{Summary}
Algorithm 1 summarizes the steps through which the G-LMB-GOM filter can be implemented.  The advantages of  the G-LMB-GOM filter are two-fold:

\noindent-- Firstly, it  improves the computational efficiency dramatically by exploiting the  parallel  implementation. The detailed computational complexity is analyzed later in Section V. 

\noindent-- Secondly, it has the potential to improve the tracking performance especially when computing and memory resources (e.g., the  number of particles that can be handled in real-time applications) are limited. On one hand, the performance compromise incurred by the grouping procedure  is slight  when the grouping threshold is sufficiently large. On the other hand,   the densities required to be approximated (by particles) after grouping at the update stage have much lower dimensions than those in the original LMB-GOM and -GLMB-GOM filters.  Since the number of particles required to keep a certain tracking performance increases exponentially with the dimension of the state space to be sampled \cite{refr:curse_dimension,refr:tbd-3}, the performance improvement stemmed from the better numerical approximation of the lower dimensional densities (given a fixed number of particles)  can sometimes go   beyond the inaccuracy due to the grouping procedure.






\section{Computational Complexity Analyses and Schematics}\label{chp:6}
In this section, we compare the LMO-GOM, -GLMB-GOM, LMB-GOM and G-LMB-GOM filters
 in terms of 
computational complexities of their respective prediction and update equations, as shown in Table I. Fig.~\ref{fig:flow} shows how these filters operate at the conceptual level.  
All these algorithms can accommodate the GOM because they all embed the  LMO-GOM update (or the parallel group LMO-GOM update)  which is  an exact solution with the generic multi-object likelihood. 



\begin{table}[h]
\caption{Computational complexity analysis}
\hrule\vspace{0.7mm}\hrule
\vspace{2mm}

\textit{\textbf{LMO-GOM filter}}:  In the prediction equations, a dominant portion of computation is for calculating the quantities  in (\ref{where_0}) which involve  integrals each to be computed on  with  varying from  to , where   denotes the number of possible combinations of  objects from a set of  objects. In the update equation, computation is dominated by calculation of the quantities  in (\ref{where_eta}) which involve computing  integrals on  with  varying from  to . 
\vspace{2mm}


\textit{\textbf{-GLMB-GOM filter}}:  In the prediction equations, the main part of computation is for the quantities  which involve computing  integrals on  . In the update equation, computation is mainly for the quantities  with its computational complexity being the same as that of the LMO-GOM filter. 
\vspace{2mm}


\textit{\textbf{LMB-GOM filter}}:  In the prediction equations, a major part of computation is for the quantities  in (\ref{where_1}) which involve computing  integrals on  . In the update equation, the main part of computation is for the quantities  in (\ref{where_2}) whose computational complexity are also the same as that of the LMO-GOM filter. 
\vspace{2mm}


\textit{\textbf{G-LMB-GOM filter}}:  The computational complexity of prediction equations is same as that of the LMB-GOM filter. If the label space  is  partitioned into , then  the main part of computation in the update step is for calculating  groups of  quantities . For a certain group ,  it involves computing  integrals on  with  varying from  to .  Another computation lies in the clustering algorithm  for the partition procedure. 
Taking the  hierarchical clustering    algorithm \cite{refr:clustering_method} as an example,
the computational expense  is , which is much cheaper than the computational expense for filtering.
\vspace{2mm}\hrule
\end{table}
\begin{figure}[t]
\centering
\includegraphics[width=9cm]{TBD_LMO_all.pdf}\\
  \caption{Schematic presentation of how the LMO-GOM, -GLMB-GOM, LMB-GOM and G-LMB-GOM filters operate in  prediction and update steps.}
  \label{fig:flow}
\end{figure}

 \section{Performance Assessment}\label{chp:7}
 In this section,  the performance of the  proposed algorithms including the LMO-GOM,  LMB-GOM, and G-LMB-GOM filters is examined and compared with the state-of-the-art in comprehensive numerical experiments. The two observation models listed in Subsection II-E, i.e., the pixeled TBD model and the acoustic amplitude model are considered in our experiments.  As we  analysed in Subsection IV-D, these two observation models are two typical examples of Type I and Type II sensors, respectively.  All the algorithms are implemented using the SMC approximation method. 


\begin{table}
\caption{ Particle Number of  Parameters for  Different Algorithms}
\begin{tabular}{p{0.85cm}|p{1.375cm}|p{1.375cm}|p{1.375cm}|p{1.375cm}}
\hline\hline
&
LMO-GOM Filter & -GLMB-GOM Filter&LMB-GOM Filter& G-LMB-GOM Filter\\
\hline
\multirow{3}{*}{Prior} 
&   &  &  &  \\
\cline{2-5}
&  &  &  & \\
\hline
\multirow{3}{*}{Posterior} 
&   &  &  &  \\
\cline{2-5}
&  &  &  &  \\
\hline
\end{tabular}
\end{table}

The standard  multi-object transition kernel provided in Section II-D is adopted. The kinematic object state variable is a vector of the plannar position and velocity , where ``'' denotes matrix transpose. The single-object transition model is  linear Gaussian with

where  and  denote the  identity and zero matrices respectively,  s is the sampling period, and  is the standard deviation of the process noise.  The probability of object survival   is set to be 0.98. 



The optimal sub-pattern assignment (OSPA) error \cite{refr:OSPA} serves as the main performance metric with the cut-off value \,m and the order parameter . All performance metrics are averaged over 100 MC runs.
\subsection{Pixeled TBD Model}
 The  efficacy of the proposed algorithms   is  first evaluated  in a typical TBD scenario which presents object crossing, objects in a close proximity for a long time,  and  well-separated objects.  Observations are collected on a   array of cells with cell lengths . The blurring factor for the Gaussian point spread function is set to be . The effective template  is the  pixel  square region whose center is closest to .  The SNR value of each object is set to be 15\,dB.  Figs. \ref{fig: scenario and observation map}(a) and (b) show the trajectories of five objects and an  observation map at a certain time step, respectively. 
 The duration of this scenario is  \,s.
 
We compare our methods with  the -GLMB-GOM filter and the  MB-TBD filter~\cite{refr:MeMber_filter}. The SMC implementation for the MB-TBD filter adopts  particles for each Bernoulli component.   The particles employed by 
the other algorithms are assigned according to Table II, with  .  With the  G-LMB-GOM filter, we choose the partition criterion given in  (\ref{grouping}) with the distance function  (\ref{distance-function}), and the grouping threshold is set to be \,m.

 One of the main purposes of this experiment is  to verify that the LMO-GOM filter is possibly served as the theoretical performance upper bound under the standard observation model as we analysed in Remark \ref{remark:3}. Hence, in order to  guarantee a negligible numerical error with a sufficiently large but tractable number of particles  (i.e., ), the uncertainties of parameters are set to be relatively low. Specifically, all filters assume no object births and are initialized from the regions around  the correct object positions. Also the five trajectories are considered with only slight maneuverability, i.e., .  
The aim of this setting is to ensure a controlled experiment in which the objects can approach each other in a small distance for a relatively long period.  The duration of this scenario is  \,s.

   \begin{figure}[ht]
\setlength{\abovecaptionskip}{-7pt}
\setlength{\belowcaptionskip}{-7pt}
\begin{minipage}[b]{0.48\linewidth}
  \centering
\centerline{\includegraphics[width=4.9cm]{for_paper_LMB_GOM_scenario_for_LMO_TBD.pdf}}
\centerline{\small{\small{(a)}} }\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
  \centering
  \centerline{\includegraphics[width=4.95cm]{snapshot_LMB_GOM.pdf}}
\centerline{\small{\small{(b)}}}\medskip
\end{minipage}
\caption{(a) The trajectories of five objects in  plane with the initial positions of objects indicated by several crosses; (b) An observation map  at time \,s.\label{fig: scenario and observation map}}
\end{figure}
\begin{figure}[ht]
\begin{minipage}[b]{0.49\linewidth}
  \centering
\centerline{\includegraphics[width=4.85cm]{for_paper_LMB_GOM_single_run_for_LMO_TBD.pdf}}
\centerline{\small{\small{(a)}} }\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.49\linewidth}
  \centering
\centerline{\includegraphics[width=4.85cm]{for_paper_LMB_GOM_computing_time.pdf}}
\centerline{\small{\small{(b)}}}\medskip
\end{minipage}
\caption{ (a) The respective outputs   of the LMB-GOM and MB-TBD filters for a single MC run; (b)  Execution time per frame for the -GLMB-GOM, LMO-GOM and G-LMB-GOM filters.\label{fig: computing time and single-run output}}
\end{figure}
   \begin{figure}[ht]
\setlength{\abovecaptionskip}{-7pt}
\setlength{\belowcaptionskip}{-7pt}
\begin{minipage}[b]{0.48\linewidth}
  \centering
\centerline{\includegraphics[width=5.5cm]{for_paper_LMB_GOM_Three_algorithm_TBD.pdf}}
\centerline{\small{\small{(a)}} }\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
  \centering
\centerline{\includegraphics[width=5.5cm]{for_paper_LMB_GOM_Four_algorithm_TBD.pdf}}
\centerline{\small{\small{(b)}}}\medskip
\end{minipage}
\caption{(a) OSPA errors: (a) the -GLMB-GOM, LMB-GOM, G-LMB-GOM and LMO-GOM filters; (b) all five algorithms.}
  \label{fig: performance of five targets}
\end{figure}

Fig.~\ref{fig: computing time and single-run output}(a) shows the respective outputs of the LMB-GOM  and MB-TBD filters for a single MC run.   It can be seen that the LMB-GOM filter performs accurately and consistently for the entire scenario
in the sense that it maintains locking on all tracks and correctly
estimates object positions.
On the other hand, the MB-TBD filter performs considerably worse.  Specifically,  it loses object tracks very quickly after object crossing since  object superpositions are 
not formulated in the MB-TBD filter.


Fig. \ref{fig: computing time and single-run output}(b) shows the execution times per frame for the -GLMB-GOM, LMB-GOM and G-LMB-GOM filters. It can be seen that the execution  time of the LMB-GOM filter is only slightly less than the -GLMB-GOM filter since the scenario only considers a relative small and fixed number of objects without object birth, i.e., . However, due to the utilization of  parallel group updates, the execution time of the G-LMB-GOM filter is dramatically less than its other competitors especially when more separated objects exist during periods 1\,--\,10\,s and 19\,--\,28\,s.


Fig.~\ref{fig: performance of five targets}(a) shows  the estimation errors over time in terms of average OSPA errors   for the LMO-GOM,  LMB-GOM,  G-LMB-GOM and -GLMB-GOM  filters.  We observe comparable performance from the LMB-GOM and -GLMB-GOM filters at all times except for the periods 7\,--\,12\,s and 16\,--\,21\,s during which objects the very close to each other. As the performance upper bound, the LMO-GOM filter still performs the best. Moreover, the G-LMB-GOM filter  has even better performance than both the LMB-GOM and -GLMB-GOM filter, because the grouping of objects alleviates the combinational and  high-dimension problem at the update stage.

Fig.~\ref{fig: performance of five targets}(b)  shows average OSPA errors for the MB-TBD filter and others. The results observed are consistent  with that of the single run of the MB-TBD filter. When objects are far away from each other before the time of 8\,s, the MB-TBD filter has decent accuracy, then its error begins to increase as objects get close to each other, and finally it diverges.

\subsection{Acoustic Amplitude Model}
To further assess  the capabilities of the LMB-GOM and G-LMB-GOM filter,  the scenario considers the problem of tracking an unknown and time varying number of objects using acoustic amplitude sensors.   
A number of  961 acoustic sensors  are dispersed evenly over a two-dimensional surveillance region   as shown in Fig. \ref{fig: scenario and time}(a).   At most four objects appear and travel with the standard deviation of the process noise .  The path loss exponent is set to be .  The duration of this scenario is  \,s.
 
 


This case is quite different from the pixeled TBD observation model in the sense that the VOR  is able to cover the whole observation space, which can be reflected from Fig. \ref{fig: scenario and time}(b) drawing the received sound amplitude at each acoustic sensor. For 
the G-LMB-GOM filter, the grouping criterion  (\ref{grouping-threshold}) is utilized  with the  threshold \,m  by approximately  truncating . 
The particles employed by each algorithm are set according to Table II  with .  The birth procedure for each algorithm  is as follows.  At each time step, the birth process   is an LMB RFS with the parameter set  where  and  with ,  and .  

\begin{table}[b]
\renewcommand{\arraystretch}{1.2}
	\caption{The average OSPA errors (m) for   different sound amplitudes.\label{tab_scenario_2}}

\begin{center}
	\footnotesize
\begin{tabular*}{0.48\textwidth}{@{\extracolsep{\fill}}c| c c c }
\hline\hline
Sound Amplitude 	&	10         &  7.9        & 5.6      \\
		\hline
-GLMB-GOM      &     2.0296 &2.4284     &3.7133                \\
\hline
LMB-GOM                      & 2.1012        &2.4510   &3.7404\\
\hline
G-LMB-GOM                  &1.9852        & 2.4092   &3.8196\\
\hline
\end{tabular*}
\label{tabone}
	\normalsize
\end{center}
\end{table}



\begin{figure}[h]
\setlength{\abovecaptionskip}{-7pt}
\setlength{\belowcaptionskip}{-7pt}
\begin{minipage}[b]{0.48\linewidth}
  \centering
\centerline{\includegraphics[width=4.9cm]{for_acoustic_sensor_network_scenario_v1.pdf}}
\centerline{\small{\small{(a)}} }\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.48\linewidth}
  \centering
\centerline{\includegraphics[width=4.9cm]{for_acoustic_sensor_network_energy_span_18db.pdf}}
\centerline{\small{\small{(b)}}}\medskip
\end{minipage}
\caption{(a)  the trajectories of four objects in  plane; (b) the received sound amplitude at each acoustic sensor under .}
  \label{fig: scenario and time}
\end{figure}
\begin{figure}[h]
\setlength{\abovecaptionskip}{-7pt}
\setlength{\belowcaptionskip}{-7pt}
\begin{minipage}[b]{0.48\linewidth}
  \centering
\centerline{\includegraphics[width=5.0cm]{for_acoustic_sensor_network_ospa_comparison.pdf}}
\centerline{\small{\small{(a)}} }\medskip
\end{minipage}
\hfill
\begin{minipage}[b]{0.47\linewidth}
  \centering
\centerline{\includegraphics[width=5.cm]{for_acoustic_sensor_network_cardinality_comparison.pdf}}
\centerline{\small{\small{(b)}}}\medskip
\end{minipage}
\caption{Performance metrics for the -GLMB-GOM, LMB-GOM, G-LMB-GOM filters under : (a) average OSPA errors; (b) cardinality estimates and true cardinalities.}
  \label{fig: performance of four targets}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=8cm,height=4cm]{for_acoustic_sensor_network_time_comparison.pdf}\\
  \caption{Average execution times  for the -GLMB-GOM, LMB-GOM, G-LMB-GOM filters under  .}
  \label{fig:time comparison}
\end{figure}





Figs. \ref{fig: performance of four targets} and \ref{fig:time comparison} show the execution times, average OSPA errors and the cardinality estimates over time for the LMB-GOM, G-LMB-GOM and -GLMB filters under the sound amplitude , respectively. We observe a comparable performance from the LMB-GOM and -GLMB-GOM filters  in terms of both the cardinality estimates and the OSPA errors, while the LMB-GOM filter achieves a more evident reduction in the execution time compared to  the -GLMB-GOM filter  due to the incorporation of the object birth process in this scenario. On one hand, whenever an object is born, the tracking error of the -GLMB-GOM filter  sharply increases but retracts to the normal level quickly, while the LMB-GOM filter can handle the births of objects well.  On the other hand, the tracking errors of the LMB-GOM filter are slightly higher than the -GLMB-GOM filter at the stable stage.   The tracking performance of the G-LMB-GOM filter is also comparable with the other two filters. More importantly, the OSPA errors  of the G-LMB-GOM filter is even lower than the other two algorithms during 20\,s\,--\,40\,s when more isolated  tracks have appeared. Also, the execution time for the G-LMB-GOM filter is dramatically reduced compared with the other two algorithms. However, one can also observe that when objects die (at times 40\,s and 50\,s), the OSPA error of the G-LMB-GOM sharply increases but retracts to the normal level quickly, while the other two algorithms can handle the deaths of objects better. 
The reason is that 
the performance loss arising from the grouping error can be larger than the improvement in the numerical accuracy due to the parallel group update, when an object dies.
The results of this experiment also verify that the G-LMB-GOM filter can also be effective for a Type II sensor.

Further,  we investigate how the performances of different algorithms are affected by  different values of the sound amplitude . The  post-transient values of the OSPA errors under  averaged over 100 MC runs and 60 time steps are presented in Table III.



 

\section{Conclusion}\label{chp:8}
An exact  Bayesian filtering solution using labeled random finite sets, for the multi-object tracking problem under the generic observation model (GOM) and the  standard transition kernel, was presented. 
The proposed exact solution can be served as the theoretical performance benchmark in multi-object tracking under the standard transition kernel.  We also proposed a generalization of the  LMB filter, named LMB filter for GOM (LMB-GOM filter) which is derived by approximating the full multi-object density with the closest  LMB density  in terms of Kullback-Leibler divergence (and it is proven to preserve the first moment  as well). A variant of the LMB-GOM filter, called grouping based LMB-GOM (G-LMB-GOM) filter was devised    and presented through a step-by-step algorithm.  The G-LMB-GOM filter can be viewed as a computationally tractable way to implement the LMB-GOM filter. The efficacy of the proposed algorithms is demonstrated using the sequential Monte Carlo implementation  under  two types of   non-standard observation models. 

Possible future works  incorporate the study on the numerical implementation methods of the proposed algorithms, e.g., the unscented  Kalman filter, the cubature Kalman filter.
\appendices
\section{Proof of Proposition \ref{pro:1}}
The density of the surviving multi-object state at the next time is given by the Chapman-Kolmogorov equation

Substituting    of form (\ref{Ps-I}) into (\ref{surving-density}), we have

According to Definition \ref{definition:1}, we can compute the joint probability of the label set  for  as,

Substitution of  in (\ref{where_0}) results in

Also, we can compute the joint probability density of the states  conditional on  by Definition \ref{definition:1}, 

Hence,   can be presented as



For the predicted multi-object density, recall the birth density (\ref{LMO born}), then we have

Using (\ref{w-add}) and (\ref{p-add}), (\ref{predict-density}) can be computed by

\section{Proof of Proposition \ref{pro:2}}
Based on the Bayes' rule, the numerator of the multi-object posterior  density  can be computed as 

Substitution of    in (\ref{where_eta}), and    in (\ref{where_p}), (\ref{fenzi}) can be further computed by

Then, the denominator of    (\ref{update}) can be computed by

Hence, the multi-object posterior density  is

where  is given in (\ref{where_w}).
\section{Proof of Proposition \ref{pro:3}}
According to the definition of the PHD~\cite{refr:Mahler_book}, the labeled PHD of  can be computed as

 Substituting  of form (\ref{marginal_label})  into (\ref{labeled-PHD-LMO-proof_1}), (\ref{labeled-PHD-LMO-proof_1}) leads to 

Hence, the Proposition holds.
\section{Proof of Proposition \ref{pro:4}}
The LMB density can  be presented as the form of (\ref{P-LMO}) with
\begin{small}

\end{small}
According to Proposition \ref{pro:3}, substituting (\ref{LMB-w}) and (\ref{LMB-p})  in (\ref{labeled-PHD-LMO}), we can obtain the labeled PHD of LMB density  as,

Hence, the Proposition holds.


\section{Proof of Proposition \ref{pro:5}}
Given an arbitrary LMO density  of the form (\ref{P-LMO}) on state space  and label space , we can easily obtain  the  LMB density   
matching the   labeled PHD  of  by comparing  the labeled PHDs  of the general labeled RFS and the LMB RFS shown in (\ref{labeled-PHD-LMO}) and (\ref{labeled-PHD-LMB}) respectively.
Specifically, the parameters of   of the form (\ref{LMB})
can be computed by

where  is the labeled PHD of .

In the following, we   prove that   which matches the labeled PHD of  also  minimizes the KLD from  over the class of LMB RFS family. 

The KLD from  and any LMB density   of the form (\ref{LMB}) with the parameters  and , is given by

where 

Observing (\ref{D_KL_LMB}), one can find that  is the sum of two parts. We define the first part as
 
and the second part as

First, we consider the part , and it can be computed by

where  is  a constant  having no functional dependence on .


According to  Proposition 2a in~\cite{refr:tracking-1}, i.e.,

with  being the PHD of , we have


According to Proposition \ref{pro:3},   has the form of (\ref{labeled-PHD-LMO}). Substituting (\ref{labeled-PHD-LMO}) and (\ref{log_p}) into (\ref{D_KL_p}), we have


The substituting (\ref{existence_probability}) and (\ref{probability_density}) into (\ref{D_KL_p2}), we have

where

which is  a constant that
has no functional dependence on  .
Hence,   is minimized only if
 for each .

Secondly, consider the part .
According to the definition of KLD, we have

where  is a constant independent of .

It is obvious that

with  shown in (\ref{existence_probability}).
Thus (\ref{D_KL_w}) can be  presented as


We define two Bernoulli distributions  and  for each  as
\begin{small}
\end{small}
Then, Eq. (\ref{D_KL_w2}) yields to

where 

which is a constant having no functional dependence on any . Hence,   is minimized only if
  for each .

According to (\ref{D_KL_LMB}) is minimized only if both  and   are minimized. Hence,  is minimized by  over the class of LMB RFS family.
\section{Proof of Proposition 6}
Firstly, one can write the LMB prediction  in the general LMO density form,

with   shown as (\ref{w-add}) and .


Then, according to Proposition 2,   we can obtain the following multi-object posterior under the generic observation likelihood ,

where   and  are computed  using (\ref{update_w-lmb}) and (\ref{update_P-lmb}), respectively.

According to Proposition 5, the LMB RFS that matches exactly the labeled first-order moment of  as well as minimizes the Kullback-Leibler divergence  from   can be computed by
 
where  and  is computed by (\ref{LMB_where_r}) and (\ref{LMB_where_p}).
\section{Proof of Proposition 7}
According to Assumption \textit{A.1}, for a subset of , denoted by , the belief mass function [1] of the observation  can be presented as


Also, according to Assumption \textit{A.2},   which indicates that  observations  are generated only by object states in , and hence are independent from , i.e.,

Similarly, the observations  are independent from . 
As a result, the belief mass function given in (\ref{belief-mass-function}) can be calculated  as follows:


By computing the set derivative of the above mass believe function, the multi-object likelihood can be represented as 




\bibliographystyle{IEEEtran}
\bibliography{LMB_TBD}
\begin{IEEEbiography}
[{\includegraphics[width=0.9\columnwidth,draft=false]{Suqi_Li.pdf}}]{Suqi Li}  is born in 1990.  She received the B.E. degree in electronic engineering from the University of Electronic Science and Technology of China, Chengdu, in 2011. 
Since September 2011, she has been pursuing the Ph.D. degree at the School of Electronic Engineering, University of Electronic Technology and Science of China.  
Currently, she is a visiting student with the Dipartimento di Ingegneria dell' Informazione (DINFO), Universit degli Studi di Firenze, Italy. Her research interests include the random finite set, multi-target tracking, nonlinear filtering, sensor networks and data fusion. 
\end{IEEEbiography}
 \begin{IEEEbiography}
[{\includegraphics[width=0.9\columnwidth,draft=false]{Wei_Yi.pdf}}]{Wei Yi}
 received the B.E. degree in electronic engineering from the University of Electronic Science and Technology of China, Chengdu, in 2006.

Since 2007, he has been pursuing the Ph.D. degree at the School of Electronic Engineering of the University of Electronic Technology and Science of China. 

From March 2010 to February 2012, he was a visiting student in the Melbourne Systems Laboratory, University of Melbourne, Australia. His research interests include particle filtering and target tracking (particular emphasis on multiple target tracking and track-before-detect techniques).

Mr. Yi received the ``Best Student Paper Award'' at the 2012 IEEE Radar Conference, Atlanta, United States and the ``Best Student Paper Award'' at the 15th International Conference on Information Fusion, Singapore, 2012.
\end{IEEEbiography}
 \begin{IEEEbiography}
[{\includegraphics[width=0.9\columnwidth,draft=false]{Reza_Hoseinnezhad.pdf}}]{Reza Hoseinnezhad}
received his B.Sc., M.Sc. and Ph.D. degrees in Electronic, Control and Electrical Engineering all from the University of Tehran, Iran, in 1994, 1996 and 2002, respectively. Since 2002, he has held various academic positions at the University of Tehran, Swinburne University of Technology, the University of Melbourne and RMIT University. He is currently an Associate Professor with the School of Aerospace, Mechanical and Manufacturing Engineering, RMIT University, Victoria, Australia. His research is currently focused on development of robust estimation and visual tracking methods in a point process framework.
\end{IEEEbiography}


\begin{IEEEbiography}
[{\includegraphics[width=0.9\columnwidth,draft=false]{Bailu_Wang.pdf}}]{Bailu Wang} received his B.S. degree from the University of Electronic Science and
Technology of China (UESTC) in 2011. He is now working toward his Ph.D. degree
on signal and information processing at UESTC.

From August  2016, he has been a visiting student with the Dipartimento di Ingegneria dell' Informazione (DINFO), Universit degli Studi di Firenze, Italy. His current research interests include
radar and statistical signal processing, and multi-sensor multi-target fusion.
\end{IEEEbiography}
\begin{IEEEbiography}
[{\includegraphics[width=0.9\columnwidth,draft=false]
{Lingjiang_Kong.pdf}}]{Lingjiang Kong} was born in 1974. He received the B.S., M.S., and Ph.D. degrees from the
University of Electronic Science and Technology of China (UESTC) in 1997, 2000
and 2003, respectively.

From September 2009 to March 2010, he was a visiting researcher with the
University of Florida. 

He is currently a professor with the School of
Electronic Engineering, University of Electronic Science and Technology of
China (UESTC). His research interests include multiple-input multiple-output
(MIMO) radar, through the wall radar, and statistical signal processing.
\end{IEEEbiography}






\end{document}
