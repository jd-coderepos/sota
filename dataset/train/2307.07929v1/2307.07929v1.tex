\section{Experiments}
\graphicspath{ {./images/} }

\paragraph{Datasets and Tasks.} We use three datasets in our experiments, IIT-CDIP document collection~\cite{lewis2006building}, CORD~\cite{park2019cord} and FUNSD~\cite{jaume2019funsd}. We follow the convention in the literature~\cite{xu2020layoutlm,xu2021layoutlmv2,hong2021bros,appalaraju2021docformer} to pre-train DocTr on the IIT-CDIP document collection, which is a large-scale dataset with 11 million unlabeled documents.  CORD~\cite{park2019cord} is a receipt dataset with 800 training, 100 validation, and 100 testing samples. Each receipt in this dataset is labeled with a list of line items and key-value pair groups. FUNSD~\cite{jaume2019funsd} consists of scanned forms, with 149 training and 50 testing examples. Each form is labeled with key/value entities together with links to indicate which keys and values are associated.

\begin{figure}[t]
  \centering
  \includegraphics[width=1.0\linewidth]{tasks}
  \vspace{-1.5em}
  \captionof{figure}{Illustration of the three tasks in the experiments.}
  \label{fig:tasks}
  \vspace{-1.5em}
\end{figure}

We evaluate our model's performance on three tasks, receipts parsing, entity labeling and entity linking. For \textit{receipt parsing}, a model not only has to extract each receipt's entities but also correctly link entities to form line items and key-value pair groups. Fig.~\ref{fig:tasks} (a) shows a sample receipt from CORD and its expected output after parsing. The sample contains two line items and four key-value pairs. For line items, it requires identifying each line item related entity (class and text) and group the entities of the same line item together. For key-value pairs, we identify class labels of the keys and return only text of the corresponding values. We use the same evaluation protocols and metrics as defined in \cite{hwang2020spatial} to evaluate the receipt parsing performance.

\textit{Entity labeling} and \textit{entity linking} are commonly adopted tasks~\cite{xu2020layoutlm,hwang2020spatial} to evaluate a pre-trained model's performance, which however are simplified versions of what we have defined in Sec.~\ref{sec:new_formulation}. Entity labeling requires assigning a class label to each word of the document. Fig.~\ref{fig:tasks} (b) shows a sample from FUNSD where the task is to identify if a word belongs to a key (red), a value (green) or a title (blue). In entity linking, the assumption is that the key/value entities are correctly detected, and the task is to identify which keys and values should be linked (See Fig.~\ref{fig:tasks} (c), red arrows). We evaluate entity labeling/linking by checking if the words/links are correctly labeled using F1-score as the metric.

\subsection{Comparison with Existing Solutions}

We compare DocTr with the existing methods on receipts parsing, entity labeling, and entity linking tasks, respectively.

For \textit{receipts parsing}, SPADE~\cite{hwang2020spatial} and Donut~\cite{kim2022ocr} are the only two other publicly available solutions (to the best of our knowledge) that address this task on CORD. The other existing general-purpose models~\cite{xu2021layoutlmv2,hong2021bros, huang2022layoutlmv3} are not able to directly address this structured information extraction task out-of-the-box. For a fair comparison with our method, we fine-tune the officially released general-purpose models under two settings: using the standard IOB tagging for receipts parsing or using our proposed formulation. From Table~\ref{tab:receipts_parsing}, we can see that DocTr outperforms general-purpose models BROS, LayoutLMv2 and LayoutLMv3 by a noticeable margin when they are fine-tuned with the IOB tagging setting. When fine-tuned with our proposed formulation, the general-purpose models' performance improved but they are still behind DocTr, which shows the effectiveness of the proposed encoder-decoder solution for the anchor word based structure information extraction.

For \textit{entity labeling}, we follow the general-purpose models~\cite{xu2020layoutlm} to only fine-tune DocTr for IOB tagging and evaluate based on its IOB tagging outputs. We note that this is less favorable for DocTr since the architecture and is dedicated to address our new formulation, and the pre-training strategy is not a main focus of this paper. However, we observe DocTr noticeably outperforming the existing solutions with comparable model sizes (``Base'' models in Table~\ref{tab:entity_labeling}). Even when compared with larger pre-trained models, DocTr's performance is comparable or better on the CORD dataset. 

For \textit{entity linking}, we apply the objective introduced in Eq.~(\ref{eq:entity_linking}) to train our model to link keys and values in FUNSD documents. We remove the entity extraction loss (Eq.~(\ref{eq:entity_extraction})) but use ground truth entities as per the task definition. The results are shown in Table~\ref{tab:entity_linking} -- DocTr also outperforms the existing solutions by a noticeable margin in this task.


\begin{table}[t]
\captionsetup{font=footnotesize}
\begin{minipage}{0.45\columnwidth}{
    \begin{center}
        \tablestyle{4pt}{1.05}
        \begin{tabular}{lc}
        \bf model & \bf F1 \\
        \midrule
        ~\cite{kim2022ocr}\textsuperscript{} & 87.8 \\
        ~\cite{hwang2020spatial} & 92.5 \\
        \midrule
        ~\cite{xu2021layoutlmv2} w/ IOB & 91.4 \\
        ~\cite{hong2021bros}  w/ IOB & 91.8 \\
        ~\cite{huang2022layoutlmv3}  w/ IOB & 92.2 \\
        \midrule
        ~\cite{xu2021layoutlmv2} w/ ours & 92.7 \\
        ~\cite{hong2021bros} w/ ours & 92.9 \\
        ~\cite{huang2022layoutlmv3} w/ ours & 93.6 \\
        \midrule
         & \bf{94.4} \\
        \end{tabular}
    \vspace{-.5em}
    \caption{Comparison with existing solutions on receipts parsing with the CORD dataset.}
    \label{tab:receipts_parsing}
    \end{center}
}\end{minipage}
\hspace{1em}
\begin{minipage}{0.45\columnwidth}{
    \begin{center}
        \tablestyle{4pt}{1.05}
        \begin{tabular}{lc}
            \multicolumn{1}{c}{\bf model} & \bf F1 \\
            \midrule
            ~\cite{hwang2020spatial}                  & 41.7  \\
            ~\cite{hong2021bros}                       & 71.5  \\
            ~\cite{li2021structext}              & 44.1  \\
                                           & \bf 73.9  \\

        \end{tabular}
    \vspace{-.5em}
    \caption{Comparison with existing solutions on entity linking with the FUNSD dataset.}
    \label{tab:entity_linking}
    \end{center}
}\end{minipage}

\vspace{.2em}
\scriptsize{\textsuperscript{}We take the official model from~\cite{kim2022ocr} and report numbers using the metric from~\cite{hwang2020spatial}.}
\vspace{-1em}
\end{table}

\begin{table}[t]
    \centering
    \tablestyle{4pt}{1.05}
    \resizebox{\linewidth}{!}{\begin{tabular}{lccc}
        \multicolumn{1}{c}{\bf model} & \bf FUNSD & \bf CORD & \bf \#params  \\
        \midrule
        ~\cite{hwang2020spatial}                  & 71.6                  & -                     & -    \\
        ~\cite{xu2020layoutlm}      & 78.7                  & 94.7                  & 113M \\
        ~\cite{hong2021bros}            & 83.1                  & 96.5                  & 110M \\
        ~\cite{hong2021bros}       & 83.3                  & 96.3                  & 183M \\
        ~\cite{xu2021layoutlmv2}  & 82.8                  & 95.0                  & 200M \\
        ~\cite{li2021structext}              & 83.4                  & -                     & 107M \\
                               & \bf{84.0}      & \bf 98.2              & 153M \\
        \midrule
        ~\cite{xu2020layoutlm}     & 79.0                  & 95.0             & 343M  \\
        ~\cite{hong2021bros}           & 84.5                  & \bf 97.3             & 340M  \\
        ~\cite{hong2021bros}      & 84.5                  & 97.0             & 536M  \\
        ~\cite{xu2021layoutlmv2} & 84.2                  & 96.0             & 426M  \\
         ~\cite{lee2022formnet}                 & \bf{84.7}      & \bf{97.3} & 345M \\
\end{tabular}}
    \vspace{-0.7em}
    \caption{Comparison with existing solutions on entity labeling (with FUNSD and CORD datasets).}
    \label{tab:entity_labeling} \vspace{-1em}
\end{table}

\begin{table}[!t]
    \centering
    \tablestyle{4pt}{1.05}
    \begin{tabular}{ccc}
    \bf formulation                        & \bf text serial. & \bf parsing (C) \\
    \midrule
    IOB tagging~\cite{ramshaw1999text}   & raster scan      & 93.2  \\
    SPADE~\cite{hwang2020spatial}        & raster scan      & 93.0  \\
    \bf DocTr (ours)                     & raster scan      & \bf 94.4  \\
    \cmidrule{1-3}
    IOB tagging~\cite{ramshaw1999text}   & oracle           & 94.1  \\
    SPADE~\cite{hwang2020spatial}        & oracle           & 93.9  \\
    \bf DocTr (ours)                     & oracle           & \bf 95.0  \\
    \end{tabular}
    \vspace{-0.7em}
    \caption{Comparison of different SIE formulations under two text serialization settings, raster scan and oracle.}
    \label{tab:formulation} \vspace{-1em}
\end{table}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{cord_parsing_comparison}
  \vspace{-1.5em}
  \captionof{figure}{Visualization of receipt parsing results using different SIE formulations. Each result consists of the visualization of model predictions, and the parsing outputs (given the model predictions). \textbf{(a) IOB tagging} visualizes the predicted tags of OCR words. \textbf{(b) SPADE decoding} visualizes the decoded graph, and arrows between words indicate that the words are linked in the same entity. \textbf{(c) DocTr} visualizes the predicted anchor words and their bounding boxes. For the parsing outputs, green/red text means the predicted text matches/does not match ground truth. Strikethrough text means the ground truth text is missed from prediction.}
  \label{fig:cord_parsing_comparison} \vspace{-1em}
\end{figure}

\subsection{Model Properties} \label{sec:alation_study}
We analyze DocTr's design and consider other choices.
\vspace{-1em}
\paragraph{Problem Formulation.} We use DocTr as the backbone network for the encoding of document inputs (image and OCR words) and apply different formulations to decode structured information. Specifically, we compare our formulation with IOB tagging and graph based solutions. For IOB tagging, we follow the literature~\cite{lee2022formnet,xu2020layoutlm} and assign BIOES tags to each token and decode entities according to the tagged entity spans. Note that IOB tagging does not support entity linking. For a fair comparison, we link entities using a way similar to the anchor word association method introduced in Sec.~\ref{sec:doctr}. We treat the ``B'' tag or ``S'' tag of entities as the anchor words and link entities via decoding of entity linking affinity matrices. For graph based SIE, we follow the literature~\cite{hong2021bros,hwang2020spatial} by attaching a SPADE~\cite{hwang2020spatial} decoder at the end of DocTr. We fine-tune DocTr and decode graphs using the same way as specified in the original SPADE method. To understand the sensitivity of the SIE formulations with regard to the reading orders of input text, we evaluate them under two text serialization settings, raster scan and oracle. For oracle, we first order the ground truth entities in a raster scan manner, then order text while preserving the entity order.

Table~\ref{tab:formulation} shows the receipt parsing results on the CORD dataset. Our proposed formulation achieves the best performance in both text serialization settings. We notice that, compared with the other two formulations, our formulation is less sensitive to text serialization with only 0.6 score drop (vs. 0.9 drop by IOB tagging or SPADE) while switching from oracle to raster scan text serialization. We also observe that our formulation can better address cases where there is dense text with multiple entities near each other. Fig.~\ref{fig:cord_parsing_comparison} shows an example visualization (see supplementary material for more results). For IOB tagging, it can tag most of the words well. However, even a single tagging error can cause failures of entity decoding, and an entity is missed from the parsing outputs. For SPADE, the dense words result in a challenge for constructing an entity graph, and the model incorrectly merges the two \texttt{sub\_nm}'s as a single entity. In comparison, DocTr only requires identifying the anchor words which is an easier task and, with bounding box predictions, all the entities are correctly extracted.

\begin{table}[!t]
    \centering
    \tablestyle{4pt}{1.05}
    \begin{tabular}{cccc}
    \bf anchor word  &  \bf primary anchor & \bf parsing (C) \\
    \midrule
    first            &   name       &    94.2     \\
    last             &   name       &    94.1     \\
    first + last     &   first      &    94.0     \\
first + last     &   name      &  \bf{94.4}   \\
    \end{tabular}
    \vspace{-0.7em}
    \caption{Receipt parsing (CORD) results under different choices of anchor words and primary anchors.
    }
    \label{tab:anchor_word} \vspace{-1em}
\end{table}

\vspace{-1em}
\paragraph{Anchor Word and Primary Anchor.} We investigate different ways of designating anchor word and primary anchor. In Sec.~\ref{sec:new_formulation}, we introduced using the first word (in terms of reading order) of an entity as the anchor word. Here, we consider two alternatives: 1) using the last word or 2) both the first and last word as the anchor. Table~\ref{tab:anchor_word} (row 1-2, 4) shows the comparison of these three choices. We notice that there is no significant differences (94.2 vs. 94.1) between using the first word and last word as the anchor word. Using both first and last as the anchor word gives slightly better performance. We hypothesize that this is because first and last words help better identify the boundary of an entity.

For primary anchor, we investigate its choices for line-item extraction. We consider two candidates: 1) using the anchor word of the first entity in a line-item, or 2) using the anchor word of \texttt{name} as the primary anchor. From Table~\ref{tab:anchor_word} (row 3 and 4), we see the latter is a better choice with 0.4 improvement. This is reasonable since the first entity in a line-item may vary semantically (i.e., it could be \texttt{name}, \texttt{cnt} or other entity types), and thus it is harder to identify. However, this choice is also more flexible than using \texttt{name} as the primary anchor because there may be no \texttt{name} in an line-item. For CORD, each line-item always has a \texttt{name}, so this is not a concern (see supplementary material for primary anchor choices of other entity categories).

\begin{table}[!t]
    \centering
    \tablestyle{4pt}{1.05}
    \begin{tabular}{ccccc}
    \bf pre-training            & \bf parsing (C) & \bf ELB (F) & \bf ELK (F) \\
    \midrule
     none                       & 82.3            &  14.2      & 12.0       \\
     MVLM \cite{xu2020layoutlm} & 90.9            &  82.7      & 73.0       \\
     MDM                        & \bf{94.4}       &  \bf 84.0  & \bf 73.9  \\
    \end{tabular}
    \vspace{-0.7em}
    \caption{Receipt parsing (CORD), entity labeling (FUNSD) and entity linking (FUNSD) results using different pre-training tasks.}
    \label{tab:pretraining_tasks} \vspace{-1em}
\end{table}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{pretraining_outputs}
  \vspace{-2.1em}
  \caption{Example pre-training predictions on CORD images. For inputs, we visualize masked word boxes, and their text is replace by \texttt{[MASK]}. For predictions, we visualize the predicted word boxes of the masked inputs. Under each box prediction, we also visualize its corresponding word token predictions.}
  \label{fig:pretraining_outputs} \vspace{-1em}
\end{figure}

\vspace{-1em}
\paragraph{Pre-training.} We evaluate the effectiveness of the pre-training task (MDM) introduced in Sec.~\ref{sec:training}. We consider three settings: 1) without pre-training, 2) with MVLM and 3) with MDM. Table~\ref{tab:pretraining_tasks} compares their performances. Without pre-training, the performance drops significantly. With MVLM, the performance improves but still falls behind using MDM. This shows the effectiveness of having MDM for document understanding pre-training. In particular, we see more benefit of using MDM for receipt parsing. This is because our proposed formulation requires bounding box regression, and \textit{MDM helps learn better box predictions}.

We also show example MDM pre-training predictions in Figure~\ref{fig:pretraining_outputs}. Note that since both the input OCR box and text are masked, the model will need to not only predict what is masked but also predict where to find the masked word. We can see in most of the cases, the model can predict both kinds of information well. There are cases where the box (e.g., ``25,000'' in row 1) or text (e.g., ``Bun'' in row 2) is not accurately predicted. But the errors are reasonable. We also notice that the model can predict words that cannot be inferred through only text context, such as prices. This shows \textit{the usage of visual information}.

\begin{table}[!t]
    \centering
    \tablestyle{4pt}{1.05}
    \begin{tabular}{cccccccc}
    & \bf vis. enc. & \bf VL-dec.     & \bf LCQ    & \bf parsing (C) & \bf ELB (F) & \bf ELK (F) \\
    \midrule
    1) &            &                 & N/A        & 92.3            &  82.1      & 73.2       \\
    2) &            & \checkmark      & \checkmark & 92.6            &  83.0      & 73.3       \\
    3) & \checkmark & \checkmark      &            & 90.7            &  14.9      & 9.5        \\
    4) & \checkmark & \checkmark      & \checkmark & \bf{94.4}       &  \bf 84.0 & \bf 73.9       \\
    \end{tabular}
    \vspace{-0.7em}
    \caption{Impact of different architectural components: vision encoder, VL-decoder and language conditioned queries. \checkmark means the component is included in the architecture.}
    \label{tab:architecture_ablation} \vspace{-1em}
\end{table}

\vspace{-1em}
\paragraph{Architecture Design.} We then ablate the impact of the architectural components of DocTr. Table~\ref{tab:architecture_ablation} shows the ablation results. The first row is a DocTr model with only the language encoder which is equivalent to the LayoutLM~\cite{xu2020layoutlm} model without visual inputs. The second row is a model with both the language encoder and VL-decoder but no vision encoder. These two models are close in performance. This is reasonable as without visual inputs the VL-decoder does not add much of information for decoding. Row 4 is the full model with both the vision encoder and VL-decoder. Compared with row 1 and 2, the performance improves noticeably. This suggests the importance of using visual information.

For row 3 and 4, we study the effectiveness of using the proposed language conditioned queries (LCQ). Specifically, we apply Eq.~(\ref{eq:language_conditioined_ca}) to the cross-attention module when LCQ is checked. Otherwise, the standard cross-attention is used. We can see that LCQ is important since it helps to guide this one-to-one mapping between OCR and outputs, which is required by our proposed formulation. 