\documentclass[a4paper,11pt]{article}
\pdfoutput=1

\usepackage[vmargin=25mm,hmargin=25mm]{geometry}
\usepackage{amsmath,amssymb,amsthm,booktabs,color,doi,enumitem,framed,graphicx,latexsym,url,verbatim}
\usepackage[numbers,sort&compress]{natbib}
\usepackage[small]{caption}
\usepackage{microtype}
\usepackage{multibib}
\usepackage[low-sup]{subdepth}
\usepackage{hyperref}

\DeclareMathOperator{\prev}{prev}
\DeclareMathOperator{\next}{next}
\DeclareMathOperator{\Construct}{construct}
\DeclareMathOperator{\Delete}{delete}
\DeclareMathOperator{\Undelete}{undelete}
\DeclareMathOperator{\Unwind}{unwind}
\DeclareMathOperator{\Advance}{advance}
\DeclareMathOperator{\Small}{small}
\DeclareMathOperator{\Peek}{peek}

\newcommand{\mydot}[1]{\smash{\dot{#1}}}
\newcommand{\myddot}[1]{\smash{\ddot{#1}}}

\urlstyle{sf}
\newcites{online}{Online Forums and Code Repositories}
\newcites{software}{Software and Hardware}
\newcommand{\rmfcite}{\citesoftware[p.~1507]{r-manual}}

\newtheorem{lemma}{Lemma}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    citecolor=black,
    filecolor=black,
    urlcolor=black,
}

\pagestyle{plain}

\hypersetup{
    pdfauthor={Jukka Suomela},
    pdftitle={Median filtering is equivalent to sorting},
}

\newcommand{\algorithm}[2]{\begin{framed}\begin{description}[style=nextline,parsep=2pt,itemsep=9pt,#1]#2
\end{description}\vspace{-4mm}\end{framed}\vspace{-4mm}}

\newcommand{\block}[1]{\begin{itemize}[nolistsep,label=,parsep=2pt,topsep=0pt,leftmargin=10mm]\item #1\end{itemize}\vspace{1pt}}

\newcommand{\codelabel}[1]{\hfill #1}

\begin{document}

\vspace*{\stretch{1}}
\begin{flushleft}
{\huge\bf Median Filtering is Equivalent to Sorting \par}
\bigskip
\bigskip
\textbf{Jukka Suomela}

Helsinki Institute for Information Technology HIIT,\\
Department of Information and Computer Science, \\ Aalto University, Finland \\
jukka.suomela@aalto.fi
\end{flushleft}

\bigskip
\paragraph{Abstract.}

This work shows that the following problems are equivalent, both in theory and in practice:
\begin{itemize}
    \item \emph{median filtering}: given an -element vector, compute the sliding window median with window size ,
    \item \emph{piecewise sorting}: given an -element vector, divide it in  blocks of length  and sort each block.
\end{itemize}
By prior work, median filtering is known to be at least as hard as piecewise sorting: with a single median filter operation we can sort  blocks of length . The present work shows that median filtering is also as easy as piecewise sorting: we can do median filtering with one piecewise sorting operation and linear-time postprocessing. In particular, median filtering can directly benefit from the vast literature on sorting algorithms---for example, adaptive sorting algorithms imply adaptive median filtering algorithms. 

The reduction is very efficient in practice---for random inputs the performance of the new sorting-based algorithm is on a par with the fastest heap-based algorithms, and for benign data distributions it typically outperforms prior algorithms.

The key technical idea is that we can represent the sliding window with a pair of sorted doubly-linked lists: we delete items from one list and add items to the other list. Deletions are easy; additions can be done efficiently if we \emph{reverse the time} twice: First we construct the full list and delete the items in the reverse order. Then we \emph{undo each deletion} with Knuth's \emph{dancing links} technique.

\thispagestyle{empty}
\vspace*{\stretch{1.3}}
\clearpage
\setcounter{page}{1}

\section{Introduction}\label{sec:intro}

\paragraph{Median filter.}

We study the following problem, commonly known as the \emph{median filter}, \emph{sliding window median}, \emph{moving median}, \emph{running median}, \emph{rolling median}, or \emph{median smoothing}:
\begin{itemize}[noitemsep]
    \item \textbf{Input:} vector  and window size .
    \item \textbf{Output:} vector , where  is the median of .
\end{itemize}
Median filtering and its multidimensional versions are commonly used in digital signal processing \citesoftware{r-manual,mathematica-mf,matlab-mf,octave-mf,scipy-mf} and image processing \citesoftware{photoshop-mf,gimp-mf}; see Figure~\ref{fig:example} for a simple example that demonstrates how efficiently a median filter can recover a corrupted signal.

\paragraph{Contribution.}

This work gives a new, simple and efficient algorithm for median filtering. The new algorithm is based on sorting; there are two phases:
\begin{enumerate}[noitemsep]
    \item \textbf{Piecewise sorting}: divide the input vector in  blocks of length , and sort each block.
    \item \textbf{Postprocessing}: compute the output vector in linear time.
\end{enumerate}
If we use a comparison sort, the worst-case running time is , which matches the previous heap-based algorithms~\cite{astola89median,juhola91comparison,hardle95median-smooth}. However, in the new algorithm we can easily plug in any sorting algorithm that exploits the properties of our input vectors (e.g., integer sorting and adaptive sorting), and we can also benefit from sorting algorithms designed for modern computer architectures (e.g., cache-efficient sorting and GPU sorting).

The new algorithm is asymptotically optimal for any reasonable input distribution and model of computing, assuming that we have an optimal sorting algorithm for the same setting. There is a matching lower bound~\cite{juhola91comparison,krizanc05range-mode} that shows that median filtering is at least as hard as piecewise sorting: with a single median filter operation we can sort  vectors of length .

The new sorting-based median filter algorithms (with off-the-self sorting algorithm implementations) is \textbf{very efficient in practice} on modern hardware---for random inputs the performance is in the same ballpark as the performance of the best heap-based algorithms, and e.g.\ for partially sorted inputs it typically outperforms the heap-based algorithms by a large factor. Both a simple Python implementation and a highly optimised C++ implementation are available online~\citeonline{suomela-mf}, together with a testing framework and numerous benchmarks that compare the new algorithm with 9 other implementations---including those from \emph{R}, \emph{Mathematica}, \emph{Matlab}, \emph{Octave}, and \emph{SciPy}.

\paragraph{Techniques.}

On a high-level, the postprocessing phase maintains a pair of sorted doubly-linked lists,  and , so that their union  represents the sliding window. Initially,  contains the first block of data and  is empty. We remove old items from  and add new items to  until  becomes empty and  contains the second block of data. We repeat this for each block of input.

To efficiently find the median of , we can maintain a pair of pointers, one pointing to  and another pointing to , and proceed as if we were in the middle of merging two sorted lists.

The key challenge is related to the maintenance of . Deletions from a sorted doubly-linked list are easy, but insertions are hard. The key idea is to \textbf{reverse the time}: instead of adding some elements  to  one by one, we start with a list that contains all of these elements and delete them one by one, in the reverse order . Now to solve the original problem of adding elements one by one, it is sufficient to \textbf{undo the deletions} one by one. With doubly linked lists, this is very efficiently achieved with Knuth's \textbf{dancing links} technique~\cite{knuth00dancinglinks}.

\begin{figure}
    \centering
    \includegraphics{example.pdf}
    \caption{The median filter can recover corrupted data much better than e.g.\ moving average filters. (a)~Original data, . (b)~25\% of data points corrupted, some random noise added. (c)~Moving average filter applied, window size . (d)~Median filter applied, window size . In all figures, the shaded area represents the original data.}\label{fig:example}
\end{figure}

\section{Prior Work}

\paragraph{Algorithms.}

There is, of course, a trivial algorithm for median filtering in time : simply \emph{find the median separately for each window}. This approach, together with sorting networks, can be attractive for hardware implementations of median filters~\cite{oflazer83median}, but as a general-purpose algorithm it is inefficient.

Non-trivial algorithms presented in the literature are unanimously based on the following idea: \emph{maintain a data structure that represents the sliding window}. Such a data structure needs to support three operations: ``construct'', ``find the median'', and ``remove the oldest element and add a new element''. With such a data structure, one can first construct it with elements , and then process elements  one by one, in this order. Concrete ideas for the implementation of the window data structure can be classified as follows:
\begin{enumerate}
    \item Data structures for -bit integers. For a small , we can easily maintain a histogram with  buckets. However, to find the new median we need to find an adjacent unoccupied bucket. The following approaches have been discussed in the literature:
    \begin{enumerate}[noitemsep]
        \item linear scanning \cite{huang79median,ateman80median,juhola91comparison}: worst-case running time 
        \item binary trees \cite{ateman80median,juhola91comparison}: worst-case running time 
        \item van Emde Boas trees \cite{juhola91comparison}: worst-case running time .
    \end{enumerate}
    \item Efficient comparison-based data structures with a  worst-case running time:
    \begin{enumerate}[noitemsep]
        \item a maxheap-minheap pair \cite{astola89median,juhola91comparison,hardle95median-smooth}
        \item binary search trees \cite{juhola91comparison}
        \item finger trees \cite{juhola91comparison}.
    \end{enumerate}
    \item Inefficient comparison-based data structures with a  worst-case running time:
    \begin{enumerate}[noitemsep]
        \item doubly-linked lists \cite{juhola91comparison}
        \item sorted arrays \cite{ahmad87median,juhola91comparison}.
    \end{enumerate}
\end{enumerate}
In summary, the search for efficient median filter algorithms has focused on the design of an efficient data structure for the sliding window. While it is known that 2-dimensional median filtering can benefit from a clever traversal order \cite{perreault07median}, it seems that all existing algorithms for 1-dimensional median filtering are based on the idea of a doing a single, uniform, in-order traversal of the input vector.

It seems that the present work is the first deviation from this trend in the long history of median filtering algorithms. In essence, we see median filtering as an algorithmic challenge---instead of asking how to construct an efficient \emph{data structure} for the sliding window, we ask how to \emph{pre-process} the input vector so that the sliding window is much easier to maintain.

\paragraph{Applications and Implementations.}

Median filtering has been applied in statistical data analysis at least since 1920s \cite{king24seasonal}, and it was popularised by Tukey in 1970s \cite[Section 7A]{tukey77eda}.

Nowadays, a median filter is a standard subroutine in numerous scientific computing environments and signal processing packages. In \emph{R} it is called ``runmed'' \rmfcite{}, and in \emph{Mathematica} it is called ``MedianFilter'' \citesoftware{mathematica-mf}. \emph{Matlab}'s Signal Processing Toolbox, \emph{GNU Octave}'s ``signal'' package, and \emph{SciPy}'s module ``scipy.signal'' all provide a median filter function called ``medfilt1'' \citesoftware{scipy-mf,matlab-mf,octave-mf}.

Multidimensional generalisations of the median filter are commonly used in image processing. For example, in \emph{Photoshop} there is a noise reduction filter called ``Median'' \citesoftware{photoshop-mf}, and in \emph{Gimp} there is a ``Despeckle'' filter, which is a generalisation of the 2-dimensional median filter \citesoftware{gimp-mf}.

Surprisingly, most of the existing implementations of the median filter in scientific computing environments are very inefficient for a large . The experiments conducted in this work demonstrate that the median filter functions in the current versions of \emph{Matlab}, \emph{Mathematica}, \emph{Octave}, and \emph{SciPy} all exhibit approximately  complexity for random inputs (see Figure~\ref{fig:othera} for examples). It should be noted that these software packages typically provide very efficient routines for sorting, which would make the algorithm presented in this work relatively easy to implement.

The only major software package with an efficient  median filter implementation seems to be \emph{R}\@. For large values of , the runmed function in \emph{R} applies a high-quality implementation of the double-heap data structure \cite{astola89median,juhola91comparison,hardle95median-smooth}. The end result is very efficient both in theory and in practice, for a wide range of  and~ (see Figures \ref{fig:othera} and \ref{fig:otherb} for examples).

We are aware of only one general-purpose median filter implementation that consistently outperforms \emph{R}: an open source C implementation by AShelly from 2011~\citeonline{ashelly-answer,ashelly-code}. This is, again, an implementation of the double-heap technique. Raffel~\citeonline{raffel-code} has adapted this implementation to C++, and we will use Raffel's version as a baseline in our experiments.


\paragraph{Lower Bounds.}

There is a simple argument that shows that median filtering is at least as difficult as piecewise sorting---see, e.g., Juhola et al.~\cite{juhola91comparison} and Krizanc et al.~\cite{krizanc05range-mode}. Assume that , and assume that we want to sort  blocks of size . Construct the input vector  so that before each block we have  times the value  and after each block we have  times the value . If we now apply the median filter, it is easy to see that in the output each block is sorted.

Hence with some linear-time preprocessing and postprocessing, and  invocations of the median filter operation, we can sort  blocks of length . This work shows that the converse is also true.


\section{Algorithm Overview}

Figure~\ref{fig:overview} provides an illustration of the key definitions and the overall behaviour of the algorithm.

\begin{figure}
    \centering
    \includegraphics[page=1]{figs.pdf}
    \caption{Algorithm overview.}\label{fig:overview}
\end{figure}


\paragraph{Preliminaries.}
To keep the presentation easy to follow, we will assume that  and , for integers  and . Here  is the size of a \emph{half-window} and  is the number of \emph{blocks}. Extending the algorithm to arbitrary  and  is straightforward.

Throughout this work, we use arrays with bracket notation and \textbf{0-based indexing}: if  is an array of length , then its elements are . We will partition input vector  and output vector  in  arrays of length , as follows:
\newcommand{\myunderbrace}[2]{\underbrace{#1}_{\displaystyle #2}}
5pt]
    \myunderbrace{\bot, \dotsc, \bot, y_1}{Y_0},
    \ \myunderbrace{y_{2}, y_{3}, \dotsc, y_{k+1}}{Y_1},
    \ \dotsc,
    \ \myunderbrace{y_{bk-2k+2}, y_{bk-2k+3}, \dotsc, y_{bk-k+1}}{Y_{b-1}}.

    \alpha_A = X_{j-1}, \quad
    \alpha_B = X_{j}, \quad
    \pi_A = P_{j-1}, \quad
    \pi_B = P_{j}, \quad
    \beta = Y_j.

    \label{eq:del}
    \prev[\next[i]] \gets \prev[i], \quad \next[\prev[i]] \gets \next[i].

    \label{eq:undo-del}
    \prev[\next[i]] \gets i, \quad \next[\prev[i]] \gets i.

    \Delete(B,15), \Delete(B,3), \Undelete(B,3), \Undelete(B,15).

    \Delete(B,15), \Delete(B,3), \Undelete(B,15), \Undelete(B,3).
\label{eq:inv}
    s_A + s_B = h, \quad
    S_A \cup S_B = H_{AB}.

        S_{\myddot A} = \emptyset, \quad S_{\myddot B} = H_{\myddot A \myddot B}.
    
        s_{\mydot A} = s_A - 1, \quad
        s_{\mydot B} = s_B, \quad
        s_{\mydot A} + s_{\mydot B} = h - 1.
    
        a_A &= \alpha_A[i], &
        p_A &= \max S_A, &
        q_A &= \min (L_A \setminus S_A), \\
        a_B &= \alpha_B[i], &
        p_B &= \max S_B, &
        q_B &= \min (L_B \setminus S_B).
    
        S_{\mydot A} \subseteq H_{\mydot A B}, \quad
        S_{\mydot A} \subseteq H_{\mydot A \mydot B}, \quad
        S_{\mydot B} \subseteq H_{\mydot A \mydot B}, \quad
        S_{\myddot A} \cup S_{\myddot B} = H_{\myddot A \myddot B}.
        \qedhere
    
\end{proof}


\subsection{Implementations}

Two implementations of the sorting-based median algorithm are available online~\citeonline{suomela-mf}:
\begin{enumerate}[noitemsep]
    \item A simple Python implementation that is easy to follow.
    \item A highly optimised C++11 version.
\end{enumerate}
A compact version of the Python implementation without comments and assertions is also reproduced in Appendix~\ref{app:python}.


\section{Experiments}

We will now present the experiments in which we compare the performance of the new sorting-based median filter algorithm with \textbf{9 other implementations} of median filter algorithms, including the median filter functions from \emph{R}, \emph{Matlab}, \emph{GNU Octave}, \emph{SciPy}, and \emph{Mathematica}. It turns out that our sorting-based median filter algorithm performs consistently very well in comparison with the other implementations.

For a broad range of window sizes (between  and ) and for various input distributions, our implementation never loses by more than 20~\% in comparison with the fastest median filter algorithm from prior work. In many cases, our algorithm outperforms all competing implementations by a large factor---by a factor up to  for uniform random inputs and by a factor up to  for more benign input distributions.


\subsection{Implementations}\label{ssec:impl}

We will now describe the 11 implementations that we benchmarked. We start with our new algorithm and two simple baseline algorithms---all of these are optimised C++11 implementations:
\begin{description}
    \item[SortMedian:] The sorting-based median algorithm described in this work. For sorting, we use std::sort from the C++ standard library.
    \item[TreeMedian:] The sliding window is maintained as a pair of balanced search trees. We use std::multiset from the C++ standard library---this is typically a highly optimised implementation of a red-black tree.
    \item[MoveMedian:] The sliding window is maintained as a sorted array. Binary search is used to locate the part of the array that needs to be moved in order to accommodate the new element. Standard library routines std::copy and std::copy\_backward are used to efficiently move a block of data.
\end{description}
We have also included an efficient open source median filter implementation in our testing framework---while the algorithm idea dates back to 1980s, this is a modern C++ implementation from 2011:
\begin{description}
    \item[HeapMedian:] The sliding window is maintained as a double heap~\cite{astola89median,juhola91comparison,hardle95median-smooth}. This is Raffel's adaptation~\citeonline{raffel-code} of AShelly's implementation~\citeonline{ashelly-answer,ashelly-code}, with very minor modifications.
\end{description}
The source code of the above algorithms, as well as a unified testing framework, is available online~\citeonline{suomela-mf}. To ensure correctness, there is also a verification tool that compares the outputs of all four implementations against each other.

In addition to these C++ implementations, we also benchmark median filter routines that are available in the following scientific computing environments and signal processing packages:
\begin{itemize}[noitemsep]
    \item \emph{R} \citesoftware{r}, a free software for statistical computing,
    \item \emph{Matlab} \citesoftware{matlab}, a commercial numerical computing environment,
    \item \emph{GNU Octave} \citesoftware{octave}, a free numerical computing environment,
    \item \emph{SciPy} \citesoftware{scipy}, a collection of Python modules for scientific computing,
    \item \emph{Mathematica} \citesoftware{mathematica}, a commercial symbolic computing environment.
\end{itemize}
In total, six algorithm implementations were benchmarked:
\begin{description}
    \item[\emph{R}, runmed(``Turlach''):] The standard routine ``runmed'' \rmfcite{} in \emph{R}, with parameter ``algorithm'' set to ``Turlach''. This implementation maintains the sliding window as a double heap.
    \item[\emph{R}, runmed(``Stuetzle''):] As above, but with parameter ``algorithm'' set to ``Stuetzle''. This implementation maintains the sliding window as a sorted array.
    \item[\emph{Octave}, medfilt1:] Function ``medfilt1'' \citesoftware{octave-mf} in \emph{GNU Octave}'s ``signal'' package. Based on the source code, this function maintains a sorted array.
    \item[\emph{Matlab}, medfilt1:] Function ``medfilt1'' \citesoftware{matlab-mf} in \emph{Matlab}. Based on the source code, this function finds the median separately for each possible location of the sliding window.
    \item[\emph{SciPy}, scipy.signal.medfilt:] Function ``scipy.signal.medfilt'' \citesoftware{scipy-mf} in \emph{SciPy}. Based on the source code, this function finds the median by sorting the sliding window.
    \item[\emph{Mathematica}, MedianFilter:] Function ``MedianFilter'' \citesoftware{mathematica-mf} in \emph{Mathematica}. No source code or information on the algorithm is publicly available.
\end{description}
Finally, to be fair with software packages that rely on median filter implementations written in high-level languages, we also tested a \emph{very slow} implementation of our sorting-based algorithm:
\begin{description}
    \item[SortMedian.py:] The simple Python implementation from Appendix~\ref{app:python}.
\end{description}


\subsection{Comparison of All Implementations}\label{ssec:comp-other}

We will start with a broad comparison of all 11 implementations described in Section~\ref{ssec:impl}. The experiments were conducted as follows (with a few exceptions, see below):
\begin{itemize}
    \item We keep  fixed and vary . This is approximately equivalent to keeping the size of input vector  fixed and varying the window size .
    \item Input consists of independent, uniformly distributed, random 32-bit integers, or its closest equivalent that is supported in the computing environment that we benchmark.
    \item Each experiment was ran 10 times with different random seeds.
    \item The plots report the median running times.
    \item The experiments were ran on the same OS X computer equipped with a 1.7 GHz Intel Core i7 processor and 8 GB of RAM.
\end{itemize}
The following exceptions were made:
\begin{itemize}
    \item \emph{Mathematica}: Only 1 experiment was ran, as this was by far the slowest implementation.
    \item \emph{Matlab}: This implementation required huge amounts of memory. In the end, we resorted to a high-end Linux computer equipped with a 2.8 GHz Intel Xeon processor and 256 GB of RAM. Only 1 experiment was ran, as this is clearly not among the fastest implementations.
    \item \emph{R}: The running times of the fastest experiments (below 10\,ms) are averages of 10 or 100 trials.
\end{itemize}
Detailed information on the software versions and computing platforms is given in Table~\ref{tab:versions}. The source code of the test suite and the raw test results are available online~\citeonline{suomela-mf}.

\begin{table}
    \centering
    \begin{tabular}{@{}l@{\qquad}l@{\qquad}l@{\qquad}l@{}}
\toprule
Software
& Function
& Versions
& Platform
\\
\midrule
\emph{R} \citesoftware{r}
& runmed \citesoftware{r-manual}
& R 3.1.0
& OS X
\\
\addlinespace
\emph{Octave} \citesoftware{octave}
& medfilt1 \citesoftware{octave-mf}
& GNU Octave 3.8.1
& OS X
\\
&
& signal 1.3.0
&
\\
\addlinespace
\emph{Matlab} \citesoftware{matlab}
& medfilt1 \citesoftware{matlab-mf}
& Matlab R2014a (8.3.0.532)
& Linux
\\
\addlinespace
\emph{SciPy} \citesoftware{scipy}
& scipy.signal.medfilt \citesoftware{scipy-mf}
& Python 2.7.7
& OS X
\\
&
& numpy 1.8.1
&
\\
&
& scipy 0.14.0
&
\\
\addlinespace
\emph{Mathematica} \citesoftware{mathematica}
& MedianFilter \citesoftware{mathematica-mf}
& Mathematica 9.0.1.0
& OS X
\\
\midrule
\multicolumn{4}{@{}l@{}}{\small OS X: Intel Core i7, 1.7 GHz, 8 GB RAM.} \\
\multicolumn{4}{@{}l@{}}{\small Linux: Intel Xeon, 2.8 GHz, 256 GB RAM.} \\
\bottomrule
\end{tabular}
     \caption{The software versions and platforms used in the experiments of Section~\ref{ssec:comp-other} and Figures \ref{fig:othera}--\ref{fig:otherb}.}\label{tab:versions}
\end{table}

First we ran experiments with small parameter values  and  for all implementations; the results are reported in Figure~\ref{fig:othera} in the appendix. From the log-log plots it is easy to see that most of the implementations exhibit running times that are approximately proportional to . Only four implementations provide a decent performance and scalability: SortMedian, HeapMedian, TreeMedian, and \emph{R}'s ``Turlach'' implementation.

Then we repeated the experiments with the most promising implementations for larger parameter values  and . The results are reported in Figure~\ref{fig:otherb}. The key finding is that SortMedian and HeapMedian consistently outperform all other implementations for large inputs. In the next section, we will focus on these two implementations.


\subsection{Comparison of HeapMedian and SortMedian}

We will now do a more detailed comparison of the fastest algorithms, HeapMedian and SortMedian. These tests were conducted as follows:
\begin{itemize}
    \item We keep  fixed and vary . In total, we use 66 different combinations of  and~.
    \item We use 2 different versions of the implementations: one for 32-bit inputs and one for 64-bit inputs.
    \item We use 7 different generators to produce the input array :
    \begin{enumerate}[noitemsep]
        \item \emph{asc}: ascending values, .
        \item \emph{desc}: descending values, .
        \item \emph{r-asc}: ascending values + small uniform random noise, .
        \item \emph{r-desc}: descending values + small uniform random noise, .
        \item \emph{r-large}: large uniform random integers (32-bit or 64-bit).
        \item \emph{r-small}: small uniform random integers, .
        \item \emph{r-block}: piecewise constant data + small uniform random noise.
    \end{enumerate}
    \item For each combination of a version and a generator, we run the experiment for 5 times, with different random seeds.
    \item The plots report the median (solid curve) and the region from the 2nd decile to the 9th decile (shading). That is, the shaded area represents 80~\% of the experiments.
    \item The experiments were ran on Linux, using the Intel Xeon nodes of the Triton cluster \citesoftware{triton}, with one processor allocated for each experiment.
    \item To compile the code, we used GCC version 4.8.2 and GCC's implementation of the C++ standard library (a.k.a.\ libstdc++).
\end{itemize}
In total, this setup results in  experiments per algorithm. The full source code and the raw test results are available online~\citeonline{suomela-mf}. An overview of the results is given in Figure~\ref{fig:summary} in the appendix, and selected examples of generator-specific results are shown in Figures \ref{fig:summary-r-large}--\ref{fig:summary-r-desc}. Note that the  axis is linear in these plots.

As we can see from the plots, SortMedian typically performs better than HeapMedian. The running times are consistently low. HeapMedian is a clear winner only for very small window sizes, while SortMedian typically wins by a large factor for larger windows.

One the most interesting findings is shown in Figures \ref{fig:summary-r-asc} and \ref{fig:summary-r-desc}. These plots demonstrate that SortMedian makes a very effective use of partially sorted inputs, while such inputs are actually more difficult for HeapMedian than uniform random inputs. Perhaps the most important factor here is the locality of memory references and cache efficiency.


\section*{Acknowledgements}

Computer resources were provided by the Aalto University School of Science ``Science-IT'' project~\citesoftware{triton}. Many thanks to David Eppstein, Geoffrey Irving, Petteri Kaski, Pat Morin, and Saeed for comments and discussions. This problem has been discussed online on Theoretical Computer Science Stack Exchange \citeonline{suomela-mf1} and Google+ \citeonline{suomela-mf2}.

\setlength{\bibsep}{5pt}
\bibliographystyle{plain}
\bibliography{articles}
{
\renewcommand{\section}{\subsection}
\bibliographystylesoftware{plain}
\bibliographysoftware{software}
\bibliographystyleonline{plain}
\bibliographyonline{online}
}

\newpage
\appendix
\section{Appendix}

\subsection{Python Implementation}\label{app:python}

\verbatiminput{SortMedian.py}

\newpage

\begin{figure}
    \centering
    \includegraphics{plot2/summary-10000.pdf}\\
    \includegraphics{plot2/summary-100000.pdf}
    \caption{Comparison of all implementations, for  and .}\label{fig:othera}
\end{figure}

\begin{figure}
    \centering
    \includegraphics{plot2/summary-1000000.pdf}\\
    \includegraphics{plot2/summary-10000000.pdf}
    \caption{Comparison of the fastest implementations, for  and .}\label{fig:otherb}
\end{figure}

\newcommand{\figexpl}{Solid lines are medians. The shaded area represents 80~\% of the experiments.}

\begin{figure}
    \centering
    \includegraphics{plot/summary-1000000.pdf}\\
    \includegraphics{plot/summary-10000000.pdf}\\
    \includegraphics{plot/summary-100000000.pdf}
    \caption{Comparison of SortMedian and HeapMedian. \figexpl}\label{fig:summary}
\end{figure}

\begin{figure}
    \centering
    \includegraphics{plot/summary-1000000-r-large.pdf}\\
    \includegraphics{plot/summary-10000000-r-large.pdf}\\
    \includegraphics{plot/summary-100000000-r-large.pdf}
    \caption{Comparison of SortMedian and HeapMedian for generator \emph{r-large}. \figexpl}\label{fig:summary-r-large}
\end{figure}

\begin{figure}
    \centering
    \includegraphics{plot/summary-1000000-r-asc.pdf}\\
    \includegraphics{plot/summary-10000000-r-asc.pdf}\\
    \includegraphics{plot/summary-100000000-r-asc.pdf}
    \caption{Comparison of SortMedian and HeapMedian for generator \emph{r-asc}. \figexpl}\label{fig:summary-r-asc}
\end{figure}

\begin{figure}
    \centering
    \includegraphics{plot/summary-1000000-r-desc.pdf}\\
    \includegraphics{plot/summary-10000000-r-desc.pdf}\\
    \includegraphics{plot/summary-100000000-r-desc.pdf}
    \caption{Comparison of SortMedian and HeapMedian for generator \emph{r-desc}. \figexpl}\label{fig:summary-r-desc}
\end{figure}

\end{document}
