\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsthm,amsmath,amssymb}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage{multirow}
\usepackage{paralist}
\usepackage{overpic}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}


\def\blu#1{\textbf{\color{blue} #1}} \def\red#1{\textbf{\color{red}\underline{#1}}} 



\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{8501} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{Visual Saliency Transformer}

\author{
	Nian Liu\footnotemark[1]
	\hspace{25pt}
	Ni Zhang\footnotemark[1]
	\hspace{25pt}
	Kaiyuan Wan
	\hspace{25pt}
	Ling Shao
	\hspace{25pt}
	Junwei Han\footnotemark[2]
	\hspace{25pt}
	\\
	Inception Institute of Artificial Intelligence
	\hspace{8pt}
	Northwestern Polytechnical University
	\\
	{\tt\small
    \{liunian228, nnizhang.1995, kaiyuan.wan0106, junweihan2010\}@gmail.com, ling.shao@ieee.org
    }
}

\maketitle
\footnotetext[1]{Equal contribution.}
\footnotetext[2]{Corresponding author.}

\ificcvfinal\thispagestyle{empty}\fi


\begin{abstract}
    Existing state-of-the-art saliency detection methods heavily rely on CNN-based architectures. 
    Alternatively, we rethink this task from a convolution-free sequence-to-sequence perspective and predict saliency by modeling long-range dependencies, which can not be achieved by convolution.
    Specifically, we develop a novel unified model based on a pure transformer, namely, Visual Saliency Transformer (VST), for both RGB and RGB-D salient object detection (SOD). It takes image patches as inputs and leverages the transformer to propagate global contexts among image patches.
    Unlike conventional architectures used in Vision Transformer (ViT), we leverage multi-level token fusion and propose a new token upsampling method under the transformer framework to get high-resolution detection results. 
    We also develop a token-based multi-task decoder to simultaneously perform saliency and boundary detection by introducing task-related tokens and a novel patch-task-attention mechanism.
    Experimental results show that our model outperforms existing methods on both RGB and RGB-D SOD benchmark datasets. Most importantly, our whole framework not only provides a new perspective for the SOD field but also shows a new paradigm for transformer-based dense prediction models. Code is available at \url{https://github.com/nnizhang/VST}.
\end{abstract}
\section{Introduction}
SOD aims to detect objects that attract peoples' eyes and can help many vision tasks, \eg, \cite{shimoda2016distinct,gan2015devnet}.
Recently, RGB-D SOD has also gained growing interest with the extra spatial structure information from the depth data.
Current state-of-the-art SOD methods are dominated by convolutional architectures \cite{lecun1998gradient}, on both RGB and RGB-D data. They often adopt an encoder-decoder CNN architecture \cite{noh2015learning,ronneberger2015unet}, where the encoder encodes the input image to multi-level features and the decoder integrates the extracted features to predict the final saliency map. Based on this simple architecture, most efforts have been made to build a powerful decoder for predicting better saliency results. To this end, they introduced various attention models \cite{liu2018picanet,zhang2018pagr,chen2020dpanet}, multi-scale feature integration methods \cite{hou2018dss,MINet-CVPR2020,fan2020bbsnet,luo2020Cas-Gnn}, and multi-task learning frameworks \cite{wang2018salient,zhang2019capsal,zhao2019EGNet,CVPR2020_LDF,Wei2020CoNet}. An additional demand for RGB-D SOD is to effectively fuse cross-modal information, \ie, the appearance information and the depth cues. Existing works propose various modality fusion methods, such as feature fusion \cite{han2017cnns,chen2018progressively,fan2020bbsnet,Fu2020JLDCF,specificity_rgbd_sod}, knowledge distillation \cite{piao2020a2dele}, dynamic convolution \cite{HDFNet-ECCV2020}, attention models \cite{Li2020CMWNet,zhang2020ATSA}, and graph neural networks \cite{luo2020Cas-Gnn}.
Hence, CNN-based methods have achieved impressive results \cite{wang2019salient1,zhou2021rgb}.

However, all previous methods are limited in learning global long-range dependencies. Global contexts \cite{goferman2011context,zhao2015saliency,ren2015exploiting,luo2017non,liu2018picanet} and global contrast \cite{zhai2006visual,borji2012exploiting,cheng2014global} have been proved crucial for saliency detection for a long time. Nevertheless, due to the intrinsic limitation of CNNs that they extract features in local sliding windows, previous methods can hardly exploit the crucial global cues.
Although some methods utilized fully connected layers \cite{liu2016dhsnet,han2017cnns}, global pooling layers \cite{luo2017non,liu2018picanet,wang2017stagewise}, and non-local modules \cite{liu2020S2MA,chen2020dpanet} to incorporate the global context, they only did such in certain layers and the standard CNN-based architecture remains unchanged.

Recently, Transformer \cite{vaswani2017attention} was proposed to model global long-range dependencies among word sequences for machine translation. The core idea is the self-attention mechanism, which leverages the query-key correlation to relate different positions in a sequence. Transformer stacks the self-attention layers multiple times in both encoder and decoder, thus can model long-range dependencies in every layer.
Hence, it is natural to introduce the Transformer to SOD, leveraging the global cues in the model all the way.

In this paper, for the first time, we rethink SOD from a new sequence-to-sequence perspective and develop a novel unified model for both RGB and RGB-D SOD based on a pure transformer, which is named Visual Saliency Transformer.
We follow the recently proposed ViT models \cite{dosovitskiy2020image,yuan2021tokens} to divide each image into patches and adopt the Transformer model on the patch sequence. Then, the Transformer propagates long-range dependencies between image patches, without any need of using convolution.
However, it is not straightforward to apply ViT for SOD. On the one hand, how to perform dense prediction tasks based on pure transformer still remains an open question. On the other hand, ViT usually tokenizes the image to a very coarse scale. How to adapt ViT to the high-resolution prediction demand of SOD is also unclear.

To solve the first problem, we design a token-based transformer decoder by introducing task-related tokens to learn decision embeddings. Then, we propose a novel patch-task-attention mechanism to generate dense-prediction results, which provides a new paradigm for using transformer in dense prediction tasks. Motivated by previous SOD models \cite{zhao2019EGNet,Zhou2020ITSD,zhang2020select,Wei2020CoNet} that leveraged boundary detection to boost the SOD performance, we build a multi-task decoder to simultaneously conduct saliency and boundary detection by introducing a saliency token and a boundary token. This strategy simplifies the multitask prediction workflow by simply learning task-related tokens, thus largely reduces the computational costs while obtaining better results. To solve the second problem, inspired by the Tokens-to-Token (T2T) transformation \cite{yuan2021tokens}, which reduces the length of tokens, we propose a new reverse T2T transformation to upsample tokens by expanding each token into multiple sub-tokens. Then, we upsample patch tokens progressively and fuse them with low-level tokens to obtain the final full-resolution saliency map.
In addition,
we also use a cross modality transformer to deeply explore the interaction between multi-modal information for RGB-D SOD. Finally, our VST outperforms existing state-of-the-art SOD methods with a comparable number of parameters and computational costs, on both RGB and RGB-D data.

Our main contributions can be summarized as follows:
\begin{compactitem}
\item For the first time, we design a novel unified model based on the pure transformer architecture for both RGB and RGB-D SOD, from a new perspective of sequence-to-sequence modeling.

\item We design a multi-task transformer decoder to jointly conduct saliency and boundary detection by introducing task-related tokens and patch-task-attention.

\item We propose a new token upsampling method for transformer-based framework.

\item Our proposed VST model achieves state-of-the-art results on both RGB and RGB-D SOD benchmark datasets, which demonstrates its effectiveness and the potential of transformer-based models for SOD.

\end{compactitem}

\section{Related Work}

\subsection{Deep Learning Based SOD}
CNN-based approaches have become a mainstream trend in both RGB and RGB-D SOD and achieved promising performance.
Most methods \cite{hou2018dss,wang2017stagewise,MINet-CVPR2020,GateNet,fan2020bbsnet} leveraged a multi-level feature fusion strategy
by using UNet \cite{ronneberger2015unet} or HED-style \cite{xie2015hed} network structures.
Some works introduced the attention mechanism to learn more discriminative features, including spatial and channel attention \cite{Piao2019dmra,zhang2018pagr,fan2020bbsnet,chen2020dpanet} or pixel-wise contextual attention \cite{liu2018picanet}.
Other works \cite{liu2016dhsnet,wang2018rfcn,deng2018r3net,liu2019salient,chen2020PGAR} tried to design recurrent networks to refine the saliency map step-by-step.
In addition, some works introduced multi-task learning, \eg, fixation prediction \cite{wang2018salient}, image caption \cite{zhang2019capsal}, and edge detection \cite{qin2019basnet, zhao2019EGNet,CVPR2020_LDF,zhang2020select,Wei2020CoNet} to boost the SOD performance.

As for RGB-D SOD, 
many methods have designed various models to fuse RGB and depth features and obtained significant results.
Some models \cite{chen2018progressively, chen2019three, Fu2020JLDCF} adopted simple feature fusion methods, \ie, concatenation, summation, or multiplication.
Some others \cite{zhao2019contrast, li2020icnet, Piao2019dmra, Li2020CMWNet} leveraged the depth cues to generate spatial or channel attention to enhance the RGB features.
Besides, dynamic convolution \cite{HDFNet-ECCV2020}, graph neural networks \cite{luo2020Cas-Gnn}, and knowledge distillation \cite{piao2020a2dele} were also adopted to implement multi-modal feature fusion.
In addition, \cite{liu2020S2MA, liu2020ReDWeb-S, chen2020dpanet} adopted the cross-attention mechanism to propagate long-range cross-modal interactions between RGB and depth cues.

Different from previous CNN-based methods, we are the first to rethink SOD from a sequence-to-sequence perspective and propose a unified model based on pure transformer 
for both RGB and RGB-D SOD. In our model, we follow \cite{qin2019basnet, zhao2019EGNet,CVPR2020_LDF,zhang2020select,Wei2020CoNet} to leverage boundary detection to boost the SOD performance. However, different from these CNN-based models, we design a novel token-based multitask decoder to achieve this goal under the transformer framework.

\subsection{Transformers in Computer Vision}
Vaswani \etal \cite{vaswani2017attention} first proposed a transformer encoder-decoder architecture for machine translation, where multi-head self-attention and point-wise feed-forward layers are stacked multiple times.
Recently, more and more works have introduced the Transformer model to various computer vision tasks and achieved excellent results. 
Some works combined CNNs and transformers into hybrid architectures for object detection \cite{carion2020end,zhu2020deformable}, panoptic segmentation \cite{wang2020maxdeeplab}, lane shape prediction \cite{liu2021end}, and so on.
Typically, they first use CNNs to extract image features and then leverage the Transformer to incorporate long-range dependencies.


\begin{figure*}[!t]
  \graphicspath{{Figures/Network/}}
  \centering
  \includegraphics[width=1\linewidth]{Network1.pdf}
  \caption{Overall architecture of our proposed VST model for both RGB and RGB-D SOD. It first uses an encoder to generate multi-level tokens from the input image patch sequence. Then, a convertor is adopted to convert the patch tokens to the decoder space, and also performs cross-modal information fusion for RGB-D data. Finally, a decoder simultaneously predicts the saliency map and the boundary map via the proposed task-related tokens and the patch-task-attention mechanism.
  An RT2T transformation is also proposed to progressively upsample patch tokens.
The dotted line represents exclusive components for RGB-D SOD.}
  \label{VST}
  \vspace{-0.4cm}
\end{figure*}

Other works design pure transformer models to process images from the sequence-to-sequence perspective. ViT \cite{dosovitskiy2020image} divided each image into a sequence of flattened 2D patches and then adopted the Transformer for image classification.
Touvron \etal \cite{touvron2020training} introduced a teacher-student strategy 
to improve the data-efficiency of ViT and Wang \etal \cite{wang2021pvt} proposed a pyramid architecture to adapt ViT for dense prediction tasks.
T2T-ViT \cite{yuan2021tokens} adopted the T2T module to model local structures, thus generating multiscale token features.
In this work, we adopt T2T-ViT as the backbone and propose a novel multitask decoder and a reverse T2T token upsampling method. It is noteworthy that our usage of task-related tokens is different from previous models. 
In \cite{dosovitskiy2020image,touvron2020training}, the class token is directly used for image classification via adopting a multilayer perceptron on the token embedding. 
However, 
we can not obtain dense prediction results directly from a single task token. Thus, we propose to perform patch-task-attention between patch tokens and the task tokens to predict saliency and boundary maps. We believe our strategy will also inspire future transformer models for other dense prediction tasks.

Another related work to ours is \cite{zheng2020rethinking}, which introduces transformer into the semantic segmentation task.
The authors adopted a vision transformer as a backbone and then reshaped the token sequences to 2D image features.
Then, they predicted full-resolution segmentation maps using convolution and bilinear upsampling. 
Their model still falls into the hybrid architecture category. In contrast, our model is a pure transformer architecture and does not rely on any convolution operation and bilinear upsampling.

\section{Visual Saliency Transformer}
Figure~\ref{VST} shows the overall architecture of our proposed VST model.
The main components include a transformer encoder based on T2T-ViT, a transformer convertor to convert patch tokens from the encoder space to the decoder space, and a multi-task transformer decoder.

\subsection{Transformer Encoder}
Similar to other CNN-based SOD methods, which often utilize pretrained image classification models such as VGG \cite{simonyan2014vgg} and ResNet \cite{he2016resnet} as the backbone of their encoders to extract image features, we adopt the pretrained T2T-ViT \cite{yuan2021tokens} model as our backbone, as detailed below.

\begin{figure*}[!t]
  \graphicspath{{Figures/Network/}}
  \centering
  \includegraphics[width=1\linewidth]{T2T.pdf}
  \caption{(a) T2T module merges neighbouring tokens into a new token, thus reducing the length of tokens. (b) Our proposed reverse T2T module upsamples tokens by expanding each token into multiple sub-tokens.}
  \label{T2T}
  \vspace{-0.3cm}
\end{figure*}


\vspace{-3mm}
\subsubsection{Tokens to Token}
Given a sequence of patch tokens  with length  from the previous layer, T2T-ViT iteratively applies the T2T module, which is composed of a re-structurization step and a soft split step, to model the local structure information in  and obtain a new sequence of tokens.

\vspace{-3mm}
\paragraph{Re-structurization.}
As shown in Figure~\ref{T2T}(a), the tokens  is first transformed using a transformer layer to obtain new tokens :

where MSA and MLP denote the multi-head self-attention and multilayer perceptron in the original Transformer \cite{vaswani2017attention}, respectively. Note that layer normalization \cite{ba2016layer} is applied before each block.
Then,  is reshaped to a 2D image , where , to recover spatial structures, as shown in Figure~\ref{T2T}(a).

\vspace{-3mm}
\paragraph{Soft split.}
After the re-structurization step,  is first split into  patches with  overlapping.  zero-padding is also utilized to pad image boundaries. Then, the image patches are unfolded to a sequence of tokens , where the sequence length  is computed as:

Different from ViT \cite{dosovitskiy2020image}, the overlapped patch splitting adopted in T2T-ViT introduces local correspondence within neighbouring patches, thus bringing spatial priors.

The T2T transformation can be conducted iteratively multiple times. In each time, the re-structurization step first transforms previous token embeddings to new embeddings and also integrates long-range dependencies within all tokens. Then, the soft split operation aggregates the tokens in each  neighbour into a new token, which is ready to use for the next layer. Furthermore, when setting , the length of tokens can be reduced progressively.

We follow \cite{yuan2021tokens} to first soft split the input image into patches and then adopt the T2T module twice. Among the three soft split steps, the patch sizes are set to , the overlappings are set to , and the padding sizes are set to .
As such, we can obtain multi-level tokens , , and .
Given the width and height of the input image as  and , respectively, then , , and . We follow \cite{yuan2021tokens} to set  and use a linear projection layer on  to transform its embedding dimension from  to .
\vspace{-3mm}
\subsubsection{Encoder with T2T-ViT Backbone}
The final token sequence  is added with the sinusoidal position embedding \cite{vaswani2017attention} to encode 2D position information. Then,  transformer layers are used to model long-range dependencies among  to extract powerful patch token embeddings .

For RGB SOD, we adopt a single transformer encoder to obtain RGB encoder patch tokens  from each input RGB image. For RGB-D SOD, we follow two-stream architectures to further use another transformer encoder to extract the depth encoder patch tokens  from the input depth map in a similar way, as shown in Figure~\ref{VST}.


\subsection{Transformer Convertor}
We insert a convertor module between the transformer encoder and decoder to convert the encoder patch tokens  from the encoder space to the decoder space, thus obtaining the converted patch tokens .
\vspace{-3mm}
\subsubsection{RGB-D Convertor}
We fuse  and  in the RGB-D converter to integrate the complementary information between the RGB and depth data.
To this end, we design a Cross Modality Transformer (CMT), which consists of  alternating cross-modality-attention layers and self-attention layers.

\vspace{-3mm}
\paragraph{Cross-modality-attention.}
Under the pure transformer architecture, we modify the standard self-attention layer to propagate long-range cross-modal dependencies between the image and depth data, thus obtaining the cross-modality-attention, which is detailed as follows.

First, similar with the self-attention in \cite{vaswani2017attention},  is embedded to queries , keys , and values  through three linear projections.
Similarly, we can obtain the depth queries , keys , and values  from .

Next, we compute the ``Scaled Dot-Product Attention" \cite{vaswani2017attention} between the queries from one modality with the keys from the other modality. Then, the output is computed as a weighted sum of the values, formulated as:


We follow the standard Transformer architecture in \cite{vaswani2017attention} and adopt the multi-head attention mechanism in the cross-modality-attention. The same positionwise feed-forward network, residual connections, and layer normalization \cite{ba2016layer} are also used, forming our CMT layer.

After each adoption of the proposed CMT layer, we use one standard transformer layer 
on each RGB and depth patch token sequence, further enhancing their token embeddings. After alternately using CMT and transformer for  times, we fuse the obtained RGB tokens and depth tokens by concatenation and then project them to the final converted tokens , as shown in Figure~\ref{VST}.

\vspace{-3mm}
\subsubsection{RGB Convertor}
To align with our RGB-D SOD model, for RGB SOD, we simply use  standard transformer layers on  to obtain the converted patch token sequence .

\subsection{Multi-task Transformer Decoder}
Our decoder aims to decode the patch tokens  to saliency maps. 
Hence,
we propose a novel token upsampling method with multi-level token fusion and a token-based multi-task decoder.
\subsubsection{Token Upsampling and Multi-level Token Fusion}
We argue that directly predicting saliency maps from  can not obtain high-quality results since the length of  is relatively small, \ie, , which is limited for dense prediction.
Thus, we propose to upsample patch tokens first and then conduct dense prediction.
Most CNN-based methods \cite{GateNet,zhao2019EGNet,liu2020S2MA,Fu2020JLDCF} adopt bilinear upsampling to recover large scale feature maps. 
Alternatively, we propose a new token upsampling method under the transformer framework. Inspired by the T2T module \cite{yuan2021tokens} that aggregates neighbour tokens to reduce the length of tokens progressively, we propose a reverse T2T (RT2T) transformation to upsample tokens by expanding each token into multiple sub-tokens, as shown in Figure~\ref{T2T}(b).

Specifically, we first project the input patch tokens to reduce their embedding dimension from  to .
Then, we use another linear projection to expand the embedding dimension from  to .
Next, similar to the soft split step in T2T, each token is seen as a  image patch and neighbouring patches have  overlapping. Then, we can fold the tokens as an image using  zero-padding. The output image size can be computed using \eqref{softsplit} reversely, \ie, given the length of the input patch tokens as , the spatial size of the out image is . Finally, we reshape the image back to the upsampled tokens with size , where .
By setting , the RT2T transformation can increase the length of the tokens. Motivated by T2T-ViT, we use RT2T three times and set , , and . Thus, the length of the patch tokens can be gradually upsampled to , equaling to the original size of the input image.

Furthermore, motivated by the widely proved successes of multi-level feature fusion in existing SOD methods \cite{hou2018dss,MINet-CVPR2020,GateNet,fan2020bbsnet,luo2020Cas-Gnn}, we leverage low-level tokens with larger lengths from the T2T-ViT encoder, \ie,  and , to provide accurate local structural information.
For both RGB and RGB-D SOD, we only use the low-level tokens from the RGB transformer encoder.
Concretely, we progressively fuse  and  with the upsampled patch tokens via concatenation and linear projection. Then, we adopt one transformer layer to obtain the decoder tokens  at each level , where . The whole process is formulated as:

where  means concatenation along the token embedding dimension. ``Linear" means linear projection to reduce the embedding dimension after the concatenation to . Finally, we use another linear projection to recover the embedding dimension of  back to .

\subsubsection{Token Based Multi-task Prediction}
Inspired by existing pure transformer methods \cite{yuan2021tokens,dosovitskiy2020image}, which add a class token on the patch token sequence for image classification, we also leverage task-related tokens to predict results.
However, we can not obtain dense prediction results by directly using MLP on the task token embedding, as done in \cite{yuan2021tokens,dosovitskiy2020image}. Hence, we propose to perform patch-task-attention between the patch tokens and the task-related token to perform SOD.

In addition, motivated by the widely used boundary detection in SOD models \cite{zhao2019EGNet,CVPR2020_LDF,zhang2020select,Wei2020CoNet}, we also adopt the multi-task learning strategy to jointly perform saliency and boundary detection, thus using the latter to help boost the performance of the former.

To this end, we design two task-related tokens, \ie, a saliency token  and a boundary token .
At each decoder level , we add the saliency and boundary tokens  and  on the patch token sequence , and then process them using  transformer layers. As such, the two task tokens can learn image-dependent task-related embeddings from the interaction with the patch tokens. After this, we take the updated patch tokens as input and perform the token upsampling and multi-level fusion process in \eqref{decoder} to obtain upsampled patch tokens . Next, we reuse the updated  and  in the next level  to further update them and . We repeat this process until we reach the last decoder level with the  scale.

For saliency and boundary prediction, we perform patch-task-attention between the final decoder patch tokens  and the saliency and boundary tokens  and .
For saliency prediction, we first embed  to queries  and embed  to a key  and a value .
Similarly, for boundary prediction, we embed  to  and embed  to  and .
Then, we adopt the patch-task-attention to obtain the task-related patch tokens:

Here we use the sigmoid activation for the attention computation since in each equation we only have one key.

\begin{table*}[t]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.0}
\renewcommand{\tabcolsep}{1.0mm}
\caption{Ablation studies of our proposed model. ``Bili'' denotes bilinear upsampling. ``F" means multi-level token fusion. ``TMD" denotes our proposed token-based multi-task decoder, while ``C2D'' means using conventional two-stream decoder to perform saliency and boundary detection without using task-related tokens. The best results are labeled in \blu{blue}.
}
\begin{tabular}{l|l|cccc|cccc|cccc|cccc}
\hline
\multicolumn{2}{l|}{\multirow{2}{*}{Settings}} & \multicolumn{4}{c|}{NJUD \cite{ju2014njud}} & \multicolumn{4}{c|}{DUTLF-Depth \cite{Piao2019dmra}} & \multicolumn{4}{c|}{STERE \cite{niu2012stere}} & \multicolumn{4}{c}{LFSD \cite{li2014lfsd}}\\
\multicolumn{2}{l|}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{maxF } & \multicolumn{1}{l}{} & \multicolumn{1}{l|}{MAE }
                      & \multicolumn{1}{l}{} & \multicolumn{1}{l}{maxF } & \multicolumn{1}{l}{} & \multicolumn{1}{l|}{MAE }
                      & \multicolumn{1}{l}{} & \multicolumn{1}{l}{maxF } & \multicolumn{1}{l}{} & \multicolumn{1}{l|}{MAE }
                      & \multicolumn{1}{l}{} & \multicolumn{1}{l}{maxF } & \multicolumn{1}{l}{} & \multicolumn{1}{l}{MAE }
  \\ \hline

\multicolumn{2}{l|}{Baseline}       &0.869 &0.862 &0.931 &0.073   &0.889 &0.887 &0.942 &0.062   &0.868 &0.853 &0.927 &0.075   &0.842 &0.845 &0.893 &0.103\\ \hline
\multicolumn{2}{l|}{+CMT}           &0.873 &0.867 &0.934 &0.072   &0.889 &0.890 &0.942 &0.063   &0.869 &0.854 &0.928 &0.075   &0.849 &0.855 &0.900 &0.100\\ \hline
\multicolumn{2}{l|}{+CMT+Bili}      &0.906 &0.902 &0.944 &0.045   &0.926 &0.930 &0.961 &0.032   &0.889 &0.877 &0.939 &0.051   &0.856 &0.858 &0.895 &0.081\\
\multicolumn{2}{l|}{+CMT+RT2T}       &0.915 &0.915 &0.951 &0.039   &0.934 &0.940 &0.964 &0.028   &0.896 &0.889 &0.943 &0.046   &0.867 &0.873 &0.903 &0.073\\ \hline
\multicolumn{2}{l|}{+CMT+RT2T+F}    &\blu{0.923} &\blu{0.923} &\blu{0.954} &\blu{0.035}   &0.936 &0.943 &0.963 &0.028   &0.910 &0.903 &0.947 &0.040   &0.876 &0.880 &0.909 &0.067\\
\multicolumn{2}{l|}{+CMT+RT2T+F+TMD}  &0.922 &0.920 &0.951 &\blu{0.035}   &\blu{0.943} &\blu{0.948} &\blu{0.969} &\blu{0.024}   &\blu{0.913} &\blu{0.907} &\blu{0.951} &\blu{0.038}   &\blu{0.882} &\blu{0.889} &\blu{0.921} &\blu{0.061}\\ \hline
\multicolumn{2}{l|}{+CMT+RT2T+F+C2D}  &0.922 &0.921 &\blu{0.954} &0.036   &0.941 &0.947 &0.968 &0.026   &0.911 &0.906 &0.949 &0.040   &0.874 &0.878 &0.909 &0.069\\ \hline
\end{tabular}
\label{ablationTab}
\vspace{-3mm}
\end{table*}



Since  and  are at the  scale, we adopt the third RT2T transformation to upsample them to the full resolution.  Finally, we apply two linear transformations with the sigmoid activation to project them to scalars in , and then reshape them to a 2D saliency map and a 2D boundary map, respectively. The whole process is given in Figure~\ref{VST}.


\section{Experiments}
\subsection{Datasets and Evaluation Metrics}
For RGB SOD, we evaluate our VST model on six widely used benchmark datasets, including \textbf{ECSSD} \cite{yan2013ECSSD} (1,000 images), \textbf{HKU-IS} \cite{li2015HKUIS} (4,447 images), \textbf{PASCAL-S} \cite{li2014PASCALS} (850 images), \textbf{DUT-O} \cite{yang2013DUTO} (5,168 images), \textbf{SOD} \cite{movahedi2010SOD} (300 images), and \textbf{DUTS} \cite{wang2017duts} (10,553 training images and 5,019 testing images).
For RGB-D SOD, we use nine widely used benchmark datasets: \textbf{STERE} \cite{niu2012stere} (1,000 image pairs), \textbf{LFSD} \cite{li2014lfsd} (100 image pairs), \textbf{RGBD135} \cite{cheng2014rgbd135} (135 image pairs), \textbf{SSD} \cite{zhu2017ssd} (80 image pairs), \textbf{NJUD} \cite{ju2014njud} (1,985 image pairs), \textbf{NLPR} \cite{peng2014nlpr} (1,000 image pairs), \textbf{DUTLF-Depth} \cite{Piao2019dmra} (1,200 image pairs), \textbf{SIP} \cite{fan2020SIP} (929 image pairs), and \textbf{ReDWeb-S} \cite{liu2020ReDWeb-S} (3,179 image pairs).

We adopt four widely used evaluation metrics to evaluate our model performance comprehensively. Specifically, 
Structure-measure  \cite{fan2017structure} evaluates region-aware and object-aware structural similarity. Maximum F-measure (maxF) jointly considers precision and recall under the optimal threshold. Maximum enhanced-alignment measure  \cite{Fan2018Enhanced} simultaneously considers pixel-level errors and image-level errors. Mean Absolute Error (MAE) computes pixel-wise average absolute error. To evaluate the model complexity, we also report the multiply accumulate operations (MACs) and the number of parameters (Params).

\subsection{Implementation Details}
For fair comparisons,
we follow most previous methods to use the training set of DUTS to train our VST for RGB SOD and use 1,485 images from NJUD, 700 images from NLPR, and 800 images from DUTLF-Depth to train our VST for RGB-D SOD.
We follow \cite{zhao2019EGNet} to use a sober operator to generate the boundary ground truth from GT saliency maps.
For depth data preprocessing, we normalize the depth maps to [0,1] and duplicate them to three channels.
Finally, we resize each image or depth map to  pixels and then randomly crop  image regions as the model input and use random flipping as data augmentation.

We use the pre-trained T2T-ViT-14 \cite{yuan2021tokens} model as our backbone since it has similar computational complexity as ResNet50 \cite{he2016resnet} does. This model uses the efficient Performer \cite{2020performer} and  in T2T modules, and sets . In our convertor and decoder, we set  and  according to experimental results.
We set the batchsizes as 11 and 8, and the total training steps as 40,000 and 60,000, for RGB and RGB-D SOD, respectively.
For both of them, Adam \cite{Adam2015} is adopted as the optimizer and the binary cross entropy loss is used for both saliency and boundary prediction.
The initial learning rate is set to 0.0001 and reduced by a factor of 10 at half and three-quarters of the total step, respectively.
Deep supervision is also used to facilitate the model training, where we use the patch-task attention to predict saliency and boundary at each decoder level.
We implemented our model using Pytorch \cite{paszke2019pytorch} and trained it on a GTX 1080 Ti GPU.

\begin{table*}[t]
  \centering
  \footnotesize
  \renewcommand{\arraystretch}{1.0}
  \renewcommand{\tabcolsep}{0.7mm}
 \caption{Quantitative comparison of our proposed VST with other 14 SOTA RGB-D SOD methods on 9 benchmark datasets. \red{Red} and \blu{blue} denote the best and the second-best results, respectively. `-' indicates the code or result is not available.}
  \begin{tabular}{lr|cccccccccccccc|c}
  \hline

    Dataset
    & Metric
    & A2dele &JL-DCF & SSF-RGBD & UC-Net & MA & PGAR & DANet & cmMS & ATST  & CMW & Cas-Gnn & HDFNet & CoNet & BBS-Net & VST\\
    &
    & \cite{piao2020a2dele} &\cite{Fu2020JLDCF} & \cite{zhang2020select}   & \cite{zhang2020ucnet} & \cite{liu2020S2MA} & \cite{chen2020PGAR} & \cite{zhao2020DANet} & \cite{li2020cmMS} & \cite{zhang2020ATSA} &\cite{Li2020CMWNet} & \cite{luo2020Cas-Gnn} & \cite{HDFNet-ECCV2020} & \cite{Wei2020CoNet} & \cite{fan2020bbsnet}\\ \hline

   \multicolumn{2}{r|}{MACs (G)}  &41.86 &211.06 &46.56 &16.16 &141.19	&44.65 &66.25 &134.77 &42.17 &208.03 &-	&91.77 &20.89 &31.2 &30.99\\
   \multicolumn{2}{r|}{Params (M)} &30.34 &143.52 &32.93 &31.26 &86.65	&16.2 &26.68 &92.02 &32.17 &85.65 &- &44.15	&43.66 &49.77 &83.83\\
     \hline
     
  \multirow{4}{*}{NJUD}
    &    &0.871 &0.902 &0.899 &0.897 &0.894 &0.909 &0.899 &0.900 &0.885 &0.870 &0.911 &0.908 &0.896 &\blu{0.921} &\red{0.922}\\
    & maxF  &0.874 &0.904 &0.896 &0.895 &0.889 &0.907 &0.898 &0.897 &0.893 &0.871 &0.916 &0.911 &0.893 &\blu{0.919} &\red{0.920} \\
    &  &0.916 &0.944 &0.935 &0.936 &0.930 &0.940 &0.935 &0.936 &0.930 &0.927 &0.948 &0.944 &0.937 &\blu{0.949} &\red{0.951}\\
  \cite{ju2014njud}& MAE &0.051 &0.041 &0.043 &0.043 &0.054 &0.042 &0.046 &0.044 &0.047 &0.061 &\blu{0.036} &0.039 &0.046 &\red{0.035} &\red{0.035} \\
     \hline
  \multirow{4}{*}{NLPR}
    &    &0.899 &0.925 &0.915 &0.920 &0.916 &0.917 &0.920 &0.919 &0.909 &0.917 &0.919 &0.923 &0.912 &\blu{0.931} &\red{0.932}\\
    & maxF  &0.882 &\blu{0.918} &0.896 &0.903 &0.902 &0.897 &0.909 &0.904 &0.898 &0.903 &0.906 &0.917 &0.893 &\blu{0.918} &\red{0.920}\\
    &  &0.944 &\red{0.963} &0.953 &0.956 &0.953 &0.950 &0.955 &0.955 &0.951 &0.951 &0.955 &\red{0.963} &0.948 &0.961 &\blu{0.962}\\
   \cite{peng2014nlpr}& MAE &0.029 &\red{0.022} &0.027 &0.025 &0.030 &0.027 &0.027 &0.028 &0.027 &0.029 &0.025 &\blu{0.023} &0.027 &0.023 &0.024\\
     \hline
  \multirow{4}{*}{}
    &   &0.885 &0.906 &0.915 &0.871 &0.904 &0.899 &0.899 &0.912 &0.916 &0.797 &0.920 &0.908 &\blu{0.923} &0.882 &\red{0.943}\\
    DUTLF& maxF &0.891 &0.910 &0.923 &0.864 &0.899 &0.898 &0.904 &0.913 &0.928 &0.779 &0.926 &0.915 &\blu{0.932} &0.870 &\red{0.948}\\
    -Depth&   &0.928 &0.941 &0.950 &0.908 &0.935 &0.933 &0.939 &0.940 &0.953 &0.864 &0.953 &0.945 &\blu{0.959} &0.912 &\red{0.969}\\
     \cite{Piao2019dmra}& MAE &0.043 &0.042 &0.033 &0.059 &0.043 &0.041 &0.042 &0.036 &0.033 &0.098 &0.030 &0.041 &\blu{0.029} &0.058 &\red{0.024}\\
     \hline
  \multirow{4}{*}{ReDWeb-S}
    &    &0.641 &\blu{0.734} &0.595 &0.713 &0.711 &0.656 &-	 &0.699 &0.679 &0.634 &- &0.728 &0.696 &0.693 &\red{0.759}\\
    & maxF   &0.603 &\blu{0.727} &0.558 &0.710 &0.696 &0.632 &- &0.677 &0.673 &0.607 &- &0.717 &0.693 &0.680 &\red{0.763}\\
    &   &0.674 &\blu{0.805} &0.710 &0.794 &0.781 &0.749 &- &0.767 &0.758 &0.714 &- &0.804 &0.782 &0.763 &\red{0.826}\\
    \cite{liu2020ReDWeb-S}& MAE  &0.160 &\blu{0.128} &0.189 &0.130 &0.139 &0.161 &- &0.143 &0.155 &0.195 &- &0.129 &0.147 &0.150 &\red{0.113}\\
     \hline

  \multirow{4}{*}{STERE}
    &    &0.879 &0.903 &0.837 &0.903 &0.890 &0.894 &0.901 &0.894 &0.896 &0.852 &0.899 &0.900 &0.905 &\blu{0.908} &\red{0.913} \\
    & maxF  &0.880 &\blu{0.904} &0.840 &0.899 &0.882 &0.880 &0.892 &0.887 &0.901 &0.837 &0.901 &0.900 &0.901 &0.903 &\red{0.907} \\
    &  &0.928 &\blu{0.947} &0.912 &0.944 &0.932 &0.929 &0.937 &0.935 &0.942 &0.907 &0.944 &0.943 &\blu{0.947} &0.942 &\red{0.951} \\
    \cite{niu2012stere}  & MAE &0.045 &0.040 &0.065 &0.039 &0.051 &0.045 &0.044 &0.045 &\blu{0.038} &0.067 &0.039 &0.042 &\red{0.037} &0.041 &\blu{0.038}\\
     \hline

  \multirow{4}{*}{SSD}
    &    &0.803 &0.860 &0.790 &0.865 &0.868 &0.832 &0.864 &0.857 &0.850 &0.798 &0.872 &\blu{0.879} &0.851 &0.863 &\red{0.889}\\
    & maxF  &0.777 &0.833 &0.762 &0.855 &0.848 &0.798 &0.843 &0.839 &0.853 &0.771 &0.863 &\blu{0.870} &0.837 &0.843 &\red{0.876} \\
    &   &0.862 &0.902 &0.867 &0.907 &0.909 &0.872 &0.914 &0.900 &0.920 &0.871 &0.923 &\blu{0.925} &0.917 &0.914 &\red{0.935}\\
    \cite{zhu2017ssd}& MAE &0.070 &0.053 &0.084 &0.049 &0.053 &0.068 &0.050 &0.053 &0.052 &0.085 &0.047 &\blu{0.046} &0.056 &0.052 &\red{0.045}\\
     \hline

  \multirow{4}{*}{RGBD135}
    &    &0.886 &0.931 &0.904 &0.934 &\blu{0.941} &0.886 &0.924 &0.934 &0.917 &0.934 &0.894 &0.926 &0.914 &0.934 &\red{0.943}\\
    & maxF  &0.872 &0.923 &0.885 &0.930 &\blu{0.935} &0.864 &0.914 &0.928 &0.916 &0.931 &0.894 &0.921 &0.902 &0.928 &\red{0.940} \\
    &  &0.921 &0.968 &0.940 &\blu{0.976} &0.973 &0.924 &0.966 &0.969 &0.961 &0.969 &0.937 &0.970 &0.948 &0.966 &\red{0.978} \\
    \cite{cheng2014rgbd135}& MAE &0.029 &0.021 &0.026 &0.019 &0.021 &0.032 &0.023 &\blu{0.018} &0.022 &0.022 &0.028 &0.022 &0.024 &0.021 &\red{0.017} \\
     \hline

  \multirow{4}{*}{LFSD}
    &   &0.825 &0.853 &0.851 &\blu{0.856} &0.829 &0.808 &0.841 &0.845 &0.845 &0.776 &0.838 &0.846 &0.848 &0.835 &\red{0.882} \\
    & maxF  &0.828 &\blu{0.863} &\blu{0.863} &0.860 &0.831 &0.794 &0.840 &0.858 &0.859 &0.779 &0.843 &0.858 &0.852 &0.828 &\red{0.889} \\
    &   &0.866 &0.894 &0.892 &\blu{0.898} &0.865 &0.853 &0.874 &0.886 &0.893 &0.834 &0.880 &0.889 &0.895 &0.870 &\red{0.921}\\
    \cite{li2014lfsd}& MAE  &0.084 &0.077 &\blu{0.074} &\blu{0.074} &0.102 &0.099 &0.087 &0.082 &0.078 &0.130 &0.081 &0.085 &0.076 &0.092 &\red{0.061} \\
     \hline

  \multirow{4}{*}{SIP}
    &   &0.829 &0.880 &0.799 &0.875 &0.872 &0.838 &0.875 &0.872 &0.849 &0.705 &- &\blu{0.886} &0.860 &0.879 &\red{0.904}\\
    & maxF  &0.834 &0.889 &0.786 &0.879 &0.877 &0.827 &0.876 &0.876 &0.861 &0.677 &- &\blu{0.894} &0.873 &0.884 &\red{0.915}\\
    &   &0.890 &0.925 &0.870 &0.919 &0.919 &0.886 &0.918 &0.911 &0.901 &0.804 &- &\blu{0.930} &0.917 &0.922 &\red{0.944}\\
    \cite{fan2020SIP} & MAE &0.070 &0.049 &0.091 &0.051 &0.058 &0.073 &0.055 &0.058 &0.063 &0.141 &- &\blu{0.048} &0.058 &0.055 &\red{0.040}\\
     \hline

  \end{tabular}
  \label{RGBD_SOTA}
   \vspace{-4mm}
\end{table*}

\subsection{Ablation Study}\label{sec:ablation}
Since our RGB-D VST is built by adding one more transformer encoder and additional CMT based on our RGB VST, while the other parts of the two models are the same, we conduct ablation studies based on our RGB-D VST to verify all of our proposed model components.
The experimental results on four RGB-D SOD datasets, \ie, NJUD, DUTLF-Depth, STERE, and LFSD, are given in Table~\ref{ablationTab}.
We remove the transformer convertor and the decoder from our RGB-D VST as the baseline model. 
Specifically, it uses the two-stream transformer encoder to extract RGB encoder patch tokens  and the depth encoder patch tokens , and then directly concatenate them and predict the saliency map with 1/16 scale by using MLP on each patch token.

\vspace{-3mm}
\paragraph{Effectiveness of CMT.}
For cross-modal information fusion, we deploy our proposed CMT right after the transformer encoder to substitute the concatenation fusion method in the baseline model, shown as ``+CMT" in Table~\ref{ablationTab}.
Compared to the baseline, CMT brings performance gain especially on the NJUD and LFSD datasets, hence demonstrating its effectiveness.

\vspace{-3mm}
\paragraph{Effectiveness of RT2T.}
Based on ``+CMT'' model, we further simply use bilinear upsampling (``+CMT+Bili") to progressively upsample tokens to the full resolution and then predict the saliency map.
The results show using bilinear upsampling to increase the resolution of the saliency map can largely improve the model performance.
Then, we replace bilinear upsampling with our proposed RT2T token upsampling method (``+CMT+RT2T"). We find that RT2T leads to obvious performance improvement compared with using bilinear upsampling, which verifies its effectiveness. 

\vspace{-3mm}
\paragraph{Effectiveness of multi-level token fusion.}
We progressively fuse  and  in our decoder (``+CMT+RT2T+F") to supply low-level fine-grained information. We find that this strategy further improves the model performance. Hence, leveraging low-level tokens in transformer is as important as fusing low-level features in CNN-based models.

\begin{table*}[t]
  \centering
  \footnotesize
  \renewcommand{\arraystretch}{1.0}
  \renewcommand{\tabcolsep}{1mm}
 \caption{Quantitative comparison of our proposed VST with other 12 SOTA RGB SOD methods on 6 benchmark datasets. ``-R" and ``-R2" means the ResNet50 and Res2Net backbone, respectively.}
  \begin{tabular}{lr|cccccccccccc|c}
  \hline

    Dataset
    & Metric
    & PiCANet & BASNet & CPD-R & PoolNet  & AFNet & TSPOANet & EGNet-R & ITSD-R & MINet-R & LDF-R & CSF-R2 & GateNet-R & VST\\
    &
    & \cite{liu2018picanet} &\cite{Qin19BASNet} & \cite{Wu_CPD}   & \cite{Liu19PoolNet} & \cite{Feng_AFNet}& \cite{Liu_TSPOANet} & \cite{zhao2019EGNet} & \cite{Zhou2020ITSD} & \cite{MINet-CVPR2020} &\cite{CVPR2020_LDF} & \cite{gao2020sod100k} &\cite{GateNet}\\ \hline

   \multicolumn{2}{r|}{MACs (G)}  &54.05 &127.36 &17.77 &88.89	&21.66	&-	&157.21	&15.96 &87.11 &15.51 &18.96	&162.13	&23.16\\
   \multicolumn{2}{r|}{Params (M)}  &47.22	&87.06 &47.85 &68.26 &35.95	&- &111.64 &26.47 &162.38 &25.15 &36.53	&128.63	&44.48\\
     \hline
     
  \multirow{4}{*}{DUTS}
    &   &0.863 &0.866 &0.869 &0.879 &0.867 &0.860 &0.887 &0.885 &0.884 &\blu{0.892} &0.890 &0.891 &\red{0.896} \\
    & maxF &0.840 &0.838 &0.840 &0.853 &0.838 &0.828 &0.866 &0.867 &0.864 &\red{0.877} &0.869 &\blu{0.874} &\red{0.877}\\
    &  &0.915 &0.902 &0.913 &0.917 &0.910 &0.907 &0.926 &0.929 &0.926 &0.930 &0.929 &\blu{0.932} &\red{0.939}\\
  \cite{wang2017duts}& MAE &0.040 &0.047 &0.043 &0.041 &0.045 &0.049 &0.039 &0.041 &\blu{0.037} &\red{0.034} &\blu{0.037} &0.038 &\blu{0.037}\\
     \hline
  \multirow{4}{*}{ECSSD}
    &    &0.916 &0.916 &0.918 &0.917 &0.914 &0.907 &0.925 &0.925 &0.925 &0.925 &\blu{0.931} &0.924 &\red{0.932}\\
    & maxF  &0.929 &0.931 &0.926 &0.929 &0.924 &0.919 &0.936 &0.939 &0.938 &0.938 &\blu{0.942} &0.935 &\red{0.944}\\
    &  &0.953 &0.951 &0.951 &0.948 &0.947 &0.942 &0.955 &0.959 &0.957 &0.954 &\blu{0.960} &0.955 &\red{0.964} \\
   \cite{yan2013ECSSD}& MAE &0.035 &0.037 &0.037 &0.042 &0.042 &0.047 &0.037 &0.035 &\blu{0.034} &\blu{0.034} &\red{0.033} &0.038 &\blu{0.034}\\
     \hline
  \multirow{4}{*}{HKU-IS}
    &   &0.905 &0.909 &0.906 &0.916 &0.905 &0.902 &0.918 &0.917 &0.919 &0.920 &- &\blu{0.921} &\red{0.928} \\
    & maxF &0.913 &0.919 &0.911 &0.920 &0.910 &0.909 &0.923 &0.926 &0.926 &\blu{0.929} &- &0.926 &\red{0.937} \\
    &  &0.951 &0.952 &0.950 &0.955 &0.949 &0.950 &0.956 &\blu{0.960} &\blu{0.960} &0.958 &- &0.959 &\red{0.968}\\
     \cite{li2015HKUIS}& MAE &0.031 &0.032 &0.034 &0.032 &0.036 &0.039 &0.031 &0.031 &\blu{0.029} &\red{0.028} &- &0.031 &0.030\\
     \hline
  \multirow{4}{*}{PASCAL-S}
    &    &0.846 &0.837 &0.847 &0.852 &0.849 &0.841 &0.852 &0.861 &0.856 &0.861 &\blu{0.863} &\blu{0.863} &\red{0.873}\\
    & maxF  &0.824 &0.819 &0.817 &0.830 &0.824 &0.817 &0.825 &\blu{0.839} &0.831 &\blu{0.839} &\blu{0.839} &0.836 &\red{0.850}\\
    &   &0.882 &0.868 &0.872 &0.880 &0.877 &0.871 &0.874 &\blu{0.889} &0.883 &0.888 &0.885 &0.886 &\red{0.900} \\
    \cite{li2014PASCALS}& MAE &0.072 &0.083 &0.077 &0.076 &0.076 &0.082 &0.080 &\blu{0.071} &\blu{0.071} &\red{0.067} &0.073 &\blu{0.071} &\red{0.067}\\
     \hline

  \multirow{4}{*}{DUT-O}
    &   &0.826 &0.836 &0.825 &0.832 &0.826 &0.818 &\blu{0.841} &0.840 &0.833 &0.839 &0.838 &0.840 &\red{0.850} \\
    & maxF  &0.767 &0.779 &0.754 &0.769 &0.759 &0.750 &0.778 &\blu{0.792} &0.769 &0.782 &0.775 &0.782 &\red{0.800}\\
    &  &0.865 &0.872 &0.868 &0.869 &0.861 &0.858 &0.878 &\blu{0.880} &0.869 &0.870 &0.869 &0.878 &\red{0.888}\\
    \cite{yang2013DUTO} & MAE &0.054 &0.057 &0.056 &0.056 &0.057 &0.062 &\blu{0.053} &0.061 &0.056 &\red{0.052} &0.055 &0.055 &0.058\\
     \hline

  \multirow{4}{*}{SOD}
    &    &0.813 &0.799 &0.797 &0.823 &0.811 &0.802 &0.824 &\blu{0.835} &0.830 &0.831 &0.826 &0.827 &\red{0.854} \\
    & maxF  &0.824 &0.808 &0.804 &0.832 &0.819 &0.809 &0.831 &\blu{0.849} &0.835 &0.841 &0.832 &0.835 &\red{0.866} \\
    &   &0.871 &0.846 &0.860 &0.873 &0.867 &0.852 &0.875 &\blu{0.889} &0.878 &0.878 &0.883 &0.877 &\red{0.902}\\
    \cite{movahedi2010SOD}& MAE  &0.073 &0.091 &0.089 &0.085 &0.085 &0.094 &0.080 &0.075 &0.074 &\blu{0.071} &0.079 &0.079 &\red{0.065}\\
     \hline

  \end{tabular}
  \label{RGB_SOTA}
\end{table*}

\begin{figure*}[t]
  \graphicspath{{Figures/qualitative/}}
  \centering
  \begin{overpic}[width=1\linewidth]{merge1.pdf}
  \put(1.2,1.6){\scriptsize Image}
  \put(6.9,1.6){\scriptsize Depth}
  \put(13.3,1.6){\scriptsize GT}
  \put(18.2,1.6){\scriptsize VST}
  \put(22.5,0){\scriptsize \shortstack[c] {BBS-Net\\ \cite{fan2020bbsnet}}}
  \put(28.8,0){\scriptsize \shortstack[c] {CoNet\\ \cite{Wei2020CoNet}}}
  \put(33.7,0){\scriptsize \shortstack[c] {HDFNet\\ \cite{HDFNet-ECCV2020}}}
  \put(39.7,0){\scriptsize \shortstack[c] {JLDCF\\ \cite{Fu2020JLDCF}}}
  \put(45,0){\scriptsize \shortstack[c] {UC-Net\\ \cite{zhang2020ucnet}}}
  \put(51.2, 1.6){\scriptsize Image}
  \put(57.5,1.6){\scriptsize GT}
  \put(62.5,1.6){\scriptsize VST}
  \put(67.2,0){\scriptsize \shortstack[c] {GateNet\\ \cite{GateNet}}}
  \put(73.7,0){\scriptsize \shortstack[c] {CSF\\ \cite{gao2020sod100k}}}
  \put(79.2,0){\scriptsize \shortstack[c] {LDF\\ \cite{CVPR2020_LDF}}}
  \put(84.2,0){\scriptsize \shortstack[c] {MINet\\ \cite{MINet-CVPR2020}}}
 \put(90.3,0){\scriptsize \shortstack[c] {ITSD\\ \cite{Zhou2020ITSD}}}
  \put(95.2,0){\scriptsize \shortstack[c] {EGNet\\ \cite{zhao2019EGNet}}}
  \end{overpic}
  \caption{Qualitative comparison against state-of-the-art RGB-D (left) and RGB (right) SOD methods. (GT: ground truth)}
  \label{visualcmp}
  \vspace{-0.3cm}
\end{figure*}
\vspace{-3mm}
\paragraph{Effectiveness of the multi-task transformer decoder.}
Based on ``+CMT+RT2T+F'', we further use our token-based multi-task decoder (TMD) to jointly perform saliency and boundary detection (``+CMT+RT2T+F+TMD"). It shows that using boundary detection can bring further performance gain for SOD on three out of four datasets. To very the effectiveness of our token-based prediction scheme, we try to directly use a conventional two-stream decoder (C2D) by using the ``+RT2T+F" architecture twice to predict the saliency map and boundary map via MLP, without using task-related tokens. This model is denoted as ``+CMT+RT2T+F+C2D'' in Table~\ref{ablationTab}.
The parameters and MACs of TMD vs. C2D are 17.22 M vs. 20.35 M and 17.70 G vs. 28.27 G, respectively.
The results show that using our TMD can achieve better results than using C2D on three out of four datasets, and also with much less computational costs. This clearly demonstrates the superiority of our proposed token-based transformer decoder.

\subsection{Comparison with State-of-the-Art Methods}
For RGB-D SOD, we compare our VST with 14 state-of-the-art RGB-D SOD methods, \ie, A2dele \cite{piao2020a2dele}, JL-DCF \cite{Fu2020JLDCF}, SSF-RGBD \cite{zhang2020select}, UC-Net \cite{zhang2020ucnet}, MA \cite{liu2020S2MA}, PGAR \cite{chen2020PGAR}, DANet \cite{zhao2020DANet}, cmMS \cite{li2020cmMS}, ATSA \cite{zhang2020ATSA}, CMW \cite{Li2020CMWNet}, Cas-Gnn \cite{luo2020Cas-Gnn}, HDFNet \cite{HDFNet-ECCV2020}, CoNet \cite{Wei2020CoNet}, and BBS-Net \cite{fan2020bbsnet}.
For RGB SOD, we compare our VST with 12 state-of-the-art RGB SOD models, including GateNet \cite{GateNet}, CSF \cite{gao2020sod100k}, LDF \cite{CVPR2020_LDF}, MINet \cite{MINet-CVPR2020}, ITSD \cite{Zhou2020ITSD}, EGNet \cite{zhao2019EGNet}, TSPOANet \cite{Liu_TSPOANet}, AFNet \cite{Feng_AFNet}, PoolNet \cite{Liu19PoolNet}, CPD \cite{Wu_CPD}, BASNet \cite{Qin19BASNet}, and PiCANet \cite{liu2018picanet}. 
Table~\ref{RGBD_SOTA} and Table~\ref{RGB_SOTA} show the quantitative comparison results for RGB-D and RGB SOD, respectively.
The results show that our VST outperforms all previous state-of-the-art CNN-based SOD models on both RGB and RGB-D benchmark datasets, with comparable number of parameters and relatively small MACs, hence demonstrating the great effectiveness of our VST.
We also show visual comparison results among best-performed models in Figure~\ref{visualcmp}. It shows our proposed VST can accurately detect salient objects in very challenging scenarios, \eg, big salient objects, cluttered backgrounds, foreground and background having similar appearances, etc.

\section{Conclusion}
In this paper, we are the first to rethink SOD from a sequence-to-sequence perspective and develop a novel unified model based on a pure transformer, for both RGB and RGB-D SOD. To handle the difficulty of applying transformers in dense prediction tasks, we propose a new token upsampling method under the transformer framework and fuse multi-level patch tokens. We also design a multi-task decoder by introducing task-related tokens and a novel patch-task-attention mechanism to jointly perform saliency and boundary detection. Our VST model achieves state-of-the-art results for both RGB and RGB-D SOD without relying on heavy computational costs, thus showing its great effectiveness. We also set a new paradigm for the open question of how to use transformer in dense prediction tasks. 

\vspace{-4mm}
\paragraph{Acknowledgments:}
This work was supported in part by the National Key R\&D Program of China under Grant 2020AAA0105702, the National Science Foundation of China under Grant 62027813, 62036005, U20B2065, U20B2068.


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage

\begin{table*}[!ht]
\centering
\footnotesize
\renewcommand{\arraystretch}{1.1}
\renewcommand{\tabcolsep}{1.3mm}
\caption{Ablation studies of our proposed model on RGB SOD datasets. ``RC'' means RGB Convertor.  ``Bili'' denotes bilinear upsampling and ``F" means multi-level token fusion. ``TMD" denotes our proposed token-based multi-task decoder, while ``C2D'' means using conventional two-stream decoder to perform saliency and boundary detection without using task-related tokens. The best results are labeled in \blu{blue}.
}
\begin{tabular}{l|l|cccc|cccc|cccc|cccc}
\hline
\multicolumn{2}{l|}{\multirow{2}{*}{Settings}} & \multicolumn{4}{c|}{DUTS \cite{wang2017duts}} & \multicolumn{4}{c|}{HKU-IS \cite{li2015HKUIS}} & \multicolumn{4}{c|}{PASCAL-S \cite{li2014PASCALS}} & \multicolumn{4}{c}{SOD \cite{movahedi2010SOD}}\\
\multicolumn{2}{l|}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{maxF} & \multicolumn{1}{l}{} & \multicolumn{1}{l|}{MAE}
                      & \multicolumn{1}{l}{} & \multicolumn{1}{l}{maxF} & \multicolumn{1}{l}{} & \multicolumn{1}{l|}{MAE}
                      & \multicolumn{1}{l}{} & \multicolumn{1}{l}{maxF} & \multicolumn{1}{l}{} & \multicolumn{1}{l|}{MAE}
                      & \multicolumn{1}{l}{} & \multicolumn{1}{l}{maxF} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{MAE}

  \\ \hline

\multicolumn{2}{l|}{Baseline}  &0.824  &0.780  &0.909  &0.071 &0.858  &0.854  &0.938  &0.075 &0.826 &0.795 &0.878  &0.096 &0.802 &0.803  &0.880 &0.100 \\ \hline
\multicolumn{2}{l|}{+RC}      &0.827  &0.785  &0.913  &0.070 &0.860  &0.856  &0.939  &0.074 &0.830 &0.797 &0.879  &0.095 &0.804 &0.805  &0.880 &0.100  \\ \hline
\multicolumn{2}{l|}{+RC+Bili} &0.867  &0.835  &0.929  &0.048 &0.901  &0.901  &0.956  &0.044 &0.856 &0.827 &0.891  &0.074 &0.833 &0.836  &0.891 &0.077   \\
\multicolumn{2}{l|}{+RC+RT2T} &0.881  &0.856  &0.934  &0.043 &0.914  &0.918  &0.961  &0.037 &0.864 &0.838 &0.896  &0.070 &0.844 &0.850  &0.894 &0.069    \\ \hline
\multicolumn{2}{l|}{+RC+RT2T+F}  &0.895  &0.874  &\blu{0.939} &0.039 &0.925 &0.932 &0.966  &0.032 &0.871 &0.845 &0.897  &0.068 &0.851 &0.861  &0.899 &0.068  \\
\multicolumn{2}{l|}{+RC+RT2T+F+TMD}  &\blu{0.896}  &\blu{0.877} &\blu{0.939} &\blu{0.037} &\blu{0.928} &\blu{0.937} &\blu{0.968} &\blu{0.030} &\blu{0.873} &\blu{0.850} &\blu{0.900} &\blu{0.067} &\blu{0.854} &\blu{0.866} &\blu{0.902} &\blu{0.065}\\ \hline
\multicolumn{2}{l|}{+RC+RT2T+F+C2D}  &0.891  &0.870 &0.937 &0.040 &0.924 &0.931 &0.966 &0.033 &0.869 &0.844 &0.896 &0.069 &0.852 &0.860 &0.898 &0.067\\ \hline
\end{tabular}
\label{ablationTab2}
\end{table*}


\begin{table*}[t!]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.1}
\renewcommand{\tabcolsep}{1.1mm}
\caption{Comparison of using different numbers of transformer layers in our VST model. The final model setting is labeled in \blu{blue}.}
\begin{tabular}{c|cccc|cr|cccc|cccc|cccc|cccc}
\hline
 ID & \multicolumn{4}{c|}{Layer Num}  & \multicolumn{1}{c}{MACs} & \multicolumn{1}{c|}{Params}
 & \multicolumn{4}{c|}{NJUD \cite{ju2014njud}} & \multicolumn{4}{c|}{DUTLF-Depth \cite{Piao2019dmra}} & \multicolumn{4}{c|}{STERE \cite{niu2012stere}}   & \multicolumn{4}{c}{LFSD \cite{li2014lfsd}}      \\
 & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \multicolumn{1}{l|}{}
& \multicolumn{1}{r}{(G)} & \multicolumn{1}{r|}{(M)}
& \multicolumn{1}{l}{} & \multicolumn{1}{l}{maxF} & \multicolumn{1}{l}{} & \multicolumn{1}{l|}{MAE}
& \multicolumn{1}{l}{} & \multicolumn{1}{l}{maxF} & \multicolumn{1}{l}{} & \multicolumn{1}{l|}{MAE}
& \multicolumn{1}{l}{} & \multicolumn{1}{l}{maxF} & \multicolumn{1}{l}{} & \multicolumn{1}{l|}{MAE}
& \multicolumn{1}{l}{} & \multicolumn{1}{l}{maxF} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{MAE}
\\ \hline

I& 8 &8 &4 &4  &48.35 &119.30 &0.925 &0.925 &0.955 &0.033 &0.940  &0.947 &0.966 &0.026 	&0.910 &0.902 &0.948 &0.039 &0.878 &0.884 &0.914 &0.066 \\ \hline
II& 8 &8 &2 &2  &36.78 &113.39 &0.923 &0.922 &0.955 &0.035 &0.943  &0.947 &0.968 &0.025 	&0.911 &0.904 &0.948 &0.039 &0.874 &0.878 &0.908 &0.069  \\
III& 8 &6 &2 &2  &36.20 &110.43 &0.921 &0.920 &0.952 &0.036 &0.940  &0.945 &0.966 &0.026 	&0.910 &0.904 &0.948 &0.040 &0.875 &0.883 &0.911 &0.067  \\
IV& 8 &4 &2 &2  &35.61 &107.47 &0.921 &0.920 &0.951 &0.036 &0.942  &0.947 &0.968 &0.026 	&0.911 &0.904 &0.949 &0.040 &0.876 &0.880 &0.912 &0.068   \\
V& 8 &2 &2 &2  &35.03 &104.52 &0.922 &0.921 &0.952 &0.036 &0.940  &0.944 &0.965 &0.026 	&0.912 &0.906 &0.949 &0.039 &0.873 &0.875 &0.908 &0.068 \\ \hline
VI& 6 &4 &2 &2  &33.30 &95.65  &0.923 &0.921 &0.952 &0.036 &0.943  &0.948 &0.968 &0.024 	&0.913 &0.906 &0.949 &0.039 &0.875 &0.878 &0.912 &0.067\\
VII& \blu{4} &\blu{4} &\blu{2} &\blu{2}  &30.99 &83.83  &0.922 &0.920 &0.951 &0.035 &0.943  &0.948 &0.969 &0.024 	&0.913 &0.907 &0.951 &0.038 &0.882 &0.889 &0.921 &0.061\\
VIII& 2 &4 &2 &2  &28.68 &72.00  &0.923 &0.921 &0.953 &0.036 &0.938  &0.943 &0.963 &0.028 	&0.912 &0.906 &0.950 &0.039 &0.881 &0.887 &0.917 &0.062\\ \hline
\end{tabular}
\label{layerTab}
\vspace{-3mm}
\end{table*}


\section{Supplementary materials}
\subsection{Ablation Study on RGB SOD Datasets}

We further report the results of ablation studies on four RGB SOD datasets, \ie, DUTS, HKU-IS, PASCAL-S, and SOD, in Table~\ref{ablationTab2} to demonstrate the effectiveness of our VST model components.

The baseline model is using transformer encoder to extract patch tokens  and then directly using  to predict the saliency map with 1/16 scale by using MLP on each patch token.
Based on the baseline, we insert RGB convertor right after the transformer encoder, shown as ``+RC'' in Table~\ref{ablationTab2}.
Compared to the baseline, RC brings performance gains especially on the DUTS and PASCAL-S datasets, which demonstrates its effectiveness.
For other components, \ie, RT2T, multi-level token fusion, and multi-task transformer decoder, we get consistent conclusions with the ablation studies on RGB-D SOD datasets as follows.

First, using bilinear upsampling (``+RC+Bili") can significantly improve the model performance while using our proposed RT2T (``+RC+RT2T") can further bring performance gains, hence demonstrating the effectiveness of our proposed RT2T.
Second, based on ``+RC+RT2T'', multi-level token fusion (``+RC+RT2T+F") can lead to better performance on all four datasets, which verifies its effectiveness.
Third, using multi-task transformer decoder (``+RC+RT2T+F+TMD") can improve the model performance on all four datasets and it is also superior to the conventional two-stream decoder (``+RC+RT2T+F+C2D").

To this end, the results of ablation studies on both RGB and RGB-D SOD datasets strongly demonstrate the effectiveness of our proposed VST components.


\subsection{Layer Number Study}
We conduct experiments to study the optimal numbers of different transformer layers, \ie,  in the transformer convertor and  in the multi-task transformer decoder, jointly considering computational costs and model performance. Note that there are three decoder modules at three scales in the multi-task transformer decoder, thus we set different transformer layer numbers for them, \ie,  for 1/16 scale,  for 1/8 scale, and  for 1/4 scale.
The experimental results on four RGB-D SOD datasets, \ie, NJUD, DUTLF-Depth, STERE, and LFSD, are given in Table~\ref{layerTab}.

In our initial model setting, we set . Since  and  are used at relatively large scales, we initially set both of them to 4, as shown in row I in Table~\ref{layerTab}. Then, we start to change the numbers of different layers.

We first reduce  and  from 4 to 2 to save computational costs.
The experimental results on row II show that it can get comparable performance with less computational costs compared with row I.
Hence, we set  and start to change  from 8 to 6, 4, 2, respectively, which are shown in row III, IV, V in Table~\ref{layerTab}.
We find that as  decreases, the computation costs decrease gradually while the results are generally comparable.
However, the model performance on row IV is better than that on row V on DUTLF-Depth and LFSD datasets.
Thus, we set  and start to change  from 8 to 6, 4, 2, respectively, which are shown in row VI, VII, VIII.
It can be seen that the performance on row VII is the best and the model has acceptable computational costs.
Hence, we set  and  as our final model setting.


\subsection{More Visual Comparison with State-of-the-art Methods}
We give more visual comparison results with the state-of-the-art RGB and RGB-D SOD methods in Figure~\ref{visualcmpRGB} and Figure~\ref{visualcmpRGBD}, respectively. It shows that our VST model can handle well in many challenging scenarios, \ie, big salient objects, cluttered backgrounds, foregrounds and backgrounds with very similar appearance, etc, while existing methods are heavily disturbed in these scenarios.
Besides, we also show the boundary maps predicted by our RGB VST and RGB-D VST models in Figure~\ref{visualcmpRGB} and Figure~\ref{visualcmpRGBD}, respectively. It can be seen that our models can predict clear boundaries for salient objects.

\begin{figure*}[t]
  \graphicspath{{Figures/qualitative/}}
  \centering
  \begin{overpic}[width=1\linewidth]{RGB_qualitative_supp_v2.pdf}
  \put(1.5, 1.6){\scriptsize Image}
  \put(9.5,1.6){\scriptsize GT}
  \put(15.5,1.6){\scriptsize VST}
  \put(21,0){\scriptsize \shortstack[c] {GateNet\\ \cite{GateNet}}}
  \put(29,0){\scriptsize \shortstack[c] {CSF\\ \cite{gao2020sod100k}}}
  \put(35.1,0){\scriptsize \shortstack[c] {LDF\\ \cite{CVPR2020_LDF}}}
  \put(41.6,0){\scriptsize \shortstack[c] {MINet\\ \cite{MINet-CVPR2020}}}
  \put(48.5,0){\scriptsize \shortstack[c] {ITSD\\ \cite{Zhou2020ITSD}}}
  \put(54.8,0){\scriptsize \shortstack[c] {EGNet\\ \cite{zhao2019EGNet}}}
  \put(61.5,0){\scriptsize \shortstack[c] {TSPOA\\ \cite{Liu_TSPOANet}}}
  \put(68.2,0){\scriptsize \shortstack[c] {AFNet\\ \cite{Feng_AFNet}}}
  \put(74.8,0){\scriptsize \shortstack[c] {PoolNet\\ \cite{Liu19PoolNet}}}
  \put(82.3,0){\scriptsize \shortstack[c] {CPD\\ \cite{Wu_CPD}}}
  \put(87.8,0){\scriptsize \shortstack[c] {BASNet\\ \cite{Qin19BASNet}}}
  \put(94.5,0){\scriptsize \shortstack[c] {PiCANet\\ \cite{liu2018picanet}}}
  \end{overpic}
  \caption{Qualitative comparison against state-of-the-art RGB SOD methods. (GT: ground truth)}
  \label{visualcmpRGB}
  \vspace{-0.3cm}
\end{figure*}



\begin{figure*}[t]
  \graphicspath{{Figures/qualitative/}}
  \centering
  \begin{overpic}[width=1\linewidth]{RGBD_qualitative_supp_v2.pdf}
  \put(1.3,1.6){\scriptsize Image}
  \put(7,1.6){\scriptsize Depth}
  \put(13.3,1.6){\scriptsize GT}
  \put(18.2,1.6){\scriptsize VST}
  \put(23,0){\scriptsize \shortstack[c] {BBS-Net\\ \cite{fan2020bbsnet}}}
  \put(29,0){\scriptsize \shortstack[c] {CoNet\\ \cite{Wei2020CoNet}}}
  \put(34,0){\scriptsize \shortstack[c] {HDFNet\\ \cite{HDFNet-ECCV2020}}}
  \put(39.5,0){\scriptsize \shortstack[c] {Cas-Gnn\\ \cite{luo2020Cas-Gnn}}}
  \put(45,0){\scriptsize \shortstack[c] {CMW\\ \cite{Li2020CMWNet}}}
  \put(50.8,0){\scriptsize \shortstack[c] {ATST\\ \cite{zhang2020ATSA}}}
  \put(56.5,0){\scriptsize \shortstack[c] {cmMS\\ \cite{li2020cmMS}}}
  \put(62.2,0){\scriptsize \shortstack[c] {DANet\\ \cite{zhao2020DANet}}}
  \put(67.5,0){\scriptsize \shortstack[c] {PGAR\\ \cite{chen2020PGAR}}}
  \put(72.8,0){\scriptsize \shortstack[c] {MA\\ \cite{liu2020S2MA}}}
  \put(78.2,0){\scriptsize \shortstack[c] {UC-Net\\ \cite{zhang2020ucnet}}}
  \put(83.5,0){\scriptsize \shortstack[c] {SSFRGBD\\ \cite{zhang2020select}}}
  \put(90,0){\scriptsize \shortstack[c] {JLDCF\\ \cite{Fu2020JLDCF}}}
  \put(95,0){\scriptsize \shortstack[c] {A2dele\\ \cite{piao2020a2dele}}}
  \end{overpic}
  \caption{Qualitative comparison against state-of-the-art RGB-D methods. (GT: ground truth)}
  \label{visualcmpRGBD}
  \vspace{-0.3cm}
\end{figure*}

\end{document}
