

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}              

\usepackage[accsupp]{axessibility}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{times}
\usepackage{epsfig}

\usepackage{url}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{amsfonts} 
\usepackage{bbding}
\usepackage{makecell}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{microtype}
\usepackage{bm}
\usepackage{wrapfig}
\newtheorem{myDef}{Definition}
\newtheorem{myTheo}{Theorem}
\newtheorem{myAxio}{Axioms}

\makeatletter
\newcommand{\ssymbol}[1]{}
\makeatother


\newcommand{\tabfootnotesize}{\fontsize{8}{9}\selectfont}

\newcommand{\myparagraph}[1]{\textbf{#1}\hspace{1.8ex}}
\newcommand{\mysubparagraph}[1]{\textit{#1}\hspace{1.8ex}}


\definecolor{cred}{HTML}{FF6B6B}
\definecolor{corange}{HTML}{FF5200}
\definecolor{cgreen}{HTML}{70AD47}
\definecolor{cblue}{HTML}{686EE2}
\definecolor{cpurple}{HTML}{A149FA}
\definecolor{ggray}{RGB}{127,127,127}
\definecolor{aliceblue}{rgb}{0.94, 0.97, 1.0}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\cvprPaperID{4337} \def\confName{CVPR}
\def\confYear{2023}

\begin{document}



\title{Video-Text as Game Players:\\ Hierarchical Banzhaf Interaction for Cross-Modal Representation Learning}

\author{Peng Jin \quad
    Jinfa Huang \quad
    Pengfei Xiong \quad
    Shangxuan Tian \quad
    Chang Liu \quad
    \and
    Xiangyang Ji \quad
    Li Yuan\footnotemark[1] \quad
    Jie Chen\footnotemark[1]\
\textrm{S}_{\bm{v},\bm{t}}=\frac{1}{2}(\underbrace{\sum_{i=1}^{N_v} \omega_v^i\ \underset{j}{\textrm{max}}\ a_{ij}}_{\textrm{video-to-text similarity}}+\underbrace{\sum_{j=1}^{N_t} \omega_t^j\ \underset{i}{\textrm{max}}\ a_{ij}}_{\textrm{text-to-video similarity}}),

\begin{aligned}
\mathcal{L}_{C}=-\frac{1}{2}[\frac{1}{B}\sum_{k=1}^{B}\log\frac{\exp(\textrm{S}_{\bm{v}_k,\bm{t}_k}/\tau)}{\sum_{l}^{B}\exp(\textrm{S}_{\bm{v}_k,\bm{t}_l}/\tau)} +\\  \frac{1}{B}\sum_{k=1}^{B}\log\frac{\exp(\textrm{S}_{\bm{v}_k,\bm{t}_k}/\tau)}{\sum_{l}^{B}\exp(\textrm{S}_{\bm{v}_l,\bm{t}_k}/\tau)}],
\end{aligned}

\begin{aligned}
\mathcal{I}([\{i,j\}])=\!\sum_{\!\mathcal{C} \subseteq \mathcal{N} \setminus \{i,j\} }p(\mathcal{C})[\phi(\mathcal{C}\cup \{[\{i,j\}]\})+\phi(\mathcal{C})\\-\phi(\mathcal{C}\cup\{i\})-\phi(\mathcal{C}\cup\{j\})],
\end{aligned}
\label{BI}

\begin{aligned}
\mathcal{D}_{v2t}^{\mathcal{I}}&=[p_{i,1}^{\mathcal{I}},p_{i,2}^{\mathcal{I}},...,p_{i,N_t}^{\mathcal{I}}],\\
\mathcal{D}_{t2v}^{\mathcal{I}}&=[\hat p_{1,j}^{\mathcal{I}},\hat p_{2,j}^{\mathcal{I}},...,\hat p_{N_v,j}^{\mathcal{I}}],
\end{aligned}
\label{Dis}

\mathcal{L}_{I}=\mathbb{E}_{\bm{v},\bm{t}} [\textrm{KL}(\mathcal{D}_{v2t}^{\mathcal{R}}\|\mathcal{D}_{v2t}^{\mathcal{I}})+\textrm{KL}(\mathcal{D}_{t2v}^{\mathcal{R}}\|\mathcal{D}_{t2v}^{\mathcal{I}})].

\rho_i=\textrm{exp}(-\frac{1}{K}\sum_{v^{k}_{f}\in \textrm{KNN}(v^{i}_{f})}\Vert v^{k}_{f}-v^{i}_{f} \Vert^2),

\delta_i=
\begin{cases}
\underset{j:\rho_j>\rho_i}{\textrm{min}} \Vert v^{k}_{f}-v^{i}_{f} \Vert^2, & \text{if  s.t. .}\\
\ \ \underset{j}{\textrm{max}} \ \ \Vert v^{k}_{f}-v^{i}_{f} \Vert^2, & \text{otherwise.}
\end{cases}

\mathcal{L}^e\!=\!\mathcal{L}_{C}^e\!+\!\alpha \mathcal{L}_{I}^e,\quad
\mathcal{L}^a\!=\!\mathcal{L}_{C}^a\!+\!\alpha \mathcal{L}_{I}^a,\quad
\mathcal{L}^o\!=\!\mathcal{L}_{C}^o\!+\!\alpha \mathcal{L}_{I}^o,
\label{loss0}

\mathcal{L}_{D}^{e2a}=\mathbb{E}_{\bm{v},\bm{t}} [\textrm{KL}(\mathcal{D}_{v2t}^{a}\|\mathcal{D}_{v2t}^{e})+\textrm{KL}(\mathcal{D}_{t2v}^{a}\|\mathcal{D}_{t2v}^{e})].
\label{loss1}

\mathcal{L}_{total}=\underbrace{\mathcal{L}^{e}+\mathcal{L}^{a}+\mathcal{L}^{o}}_{\textrm{deep supervision}}+\beta \underbrace{(\mathcal{L}_{D}^{e2a}+\mathcal{L}_{D}^{e2o})}_{\textrm{self-distillation}},

\mathcal{B}(i|\mathcal{N})=\sum_{\mathcal{C} \subseteq \mathcal{N} \setminus \{i\}}p(\mathcal{C})(\phi(\mathcal{C}\cup \{i\})-\phi(\mathcal{C})),

\begin{aligned}
\mathcal{I}([\{i,j\}])\!=\!\sum_{\!\mathcal{C} \subseteq \mathcal{N} \setminus \{i,j\} }p(\mathcal{C})[\phi(\mathcal{C}\cup \{[\{i,j\}]\})+\phi(\mathcal{C})\\-\phi(\mathcal{C}\cup\{i\})-\phi(\mathcal{C}\cup\{j\})],
\end{aligned}
\label{apendix:BI}

where  is the likelihood of  being sampled. ``'' denotes removing  from .
\end{myDef}

Similar to Banzhaf value axioms~\cite{grabisch1999axiomatic}, the following axioms convey intuitive properties that a cross-modal interaction score should satisfy.

\begin{myAxio}
Given a set  of players, a characteristic function , and a coalition , following properties are met for the interaction score .
(a) \textbf{Symmetry:} If \ \ ,\ then\ ;
(b) \textbf{Dummy:} If \ \ ,\ then\ ;
(c) \textbf{Additivity:}  If  and  have the interaction scores  and  respectively, then the interaction score for the game with value function  is ;
(d) \textbf{Recursivity:}  let  denote the Banzhaf value, then .
\end{myAxio}

\begin{myTheo}
The Banzhaf Interaction index satisfies \textbf{Symmetry}, \textbf{Dummy}, \textbf{Additivity} and \textbf{Recursivity} axiom.
\end{myTheo}

\begin{figure*}[tbp]
\centering
\includegraphics[width=.98\linewidth]{image/appendix0.pdf}
\caption{\textbf{The structure of the prediction header.} We choose four several popular structures, \ie, ``MLP'', ``CNN'', ``MLP+SA'' and ``CNN+SA''.  represent the number of visual tokens, the number of textual tokens, and the number of feature channels, respectively.}
\label{fig:prediction}
\end{figure*}

\subsection{Symmetry Axiom}
Symmetry states that if changing the value of two coalitions has the same effect on the output under all values of the other variables, then both coalitions should have an identical interaction score. 

\myparagraph{Proof.} We consider  fixed. Let us choose , and consider the unanimity game. Clearly, . That is, for every ,  and  produce the same benefits. Thus, Banzhaf Interaction satisfies Symmetry axiom, \ie, .

\subsection{Dummy Axiom}
Dummy states that if changing the value of a coalition  has no effect on the output under all values of other variables, then the interaction value of  should be zero. 

\myparagraph{Proof.} We consider  fixed. Let us choose , and consider the unanimity game. Clearly, . For every ,  has no interaction with any player. Thus, Banzhaf Interaction satisfies Dummy axiom, \ie, .

\subsection{Additivity Axiom}
Additivity states the sum of the interaction scores of the two characteristic functions is equal to the interaction score of the sum of these characteristic functions. 

\myparagraph{Proof.} Let us choose , and consider the unanimity game. Clearly, for the characteristic function , . That is, for every , the sum of the scores of the two characteristic functions () is equal to the score of the sum of these characteristic functions . Thus, Banzhaf Interaction satisfies Additivity axiom.

\subsection{Recursivity Axiom}
We hypothesize that the interaction score should depend on the values of  when  is absent, and  when  is absent. And somehow, their interaction should also be taken into account. Specifically,
Recursivity states that if the interaction is positive, then the interaction score of  should be greater than simply the sum of individual values. If the interaction is negative, the interaction score of  should be less than the sum.

\myparagraph{Proof.} We can rewrite Eq.~\ref{apendix:BI} as . Clearly, the above formula is equivalent to Recursivity axiom. Thus, Banzhaf Interaction satisfies Recursivity axiom.

\section{Discussions}
\subsection{Banzhaf Interaction Estimator}
Since the calculation of the exact Banzhaf Interaction is an NP-hard problem~\cite{matsui2001np}, existing methods mainly use sampling-based methods~\cite{leech2002computation,bachrach2010approximating} to obtain unbiased estimates. To speed up the computation of Banzhaf Interaction for many data instances, we pre-train a tiny model to learn a mapping from a set of input features to a result using MSE loss. The tiny model consists of a convolutional layer for encoding features, a self-attention module for capturing global interaction, and a convolutional layer for decoding. The tiny model has 64 hidden channels. The input is the similarity matrix of video frames and text tokens, and the output is the estimation of Banzhaf Interaction. 

To explore the impact of the Banzhaf Interaction estimator on our method, we compare the sampling-based method and pre-trained tiny model estimator in Tab.~\ref{tab:ban}. Given the costly training time, the ablation study is based on a subset of MSRVTT dataset (3K videos, each with 20 text descriptions). We find that the pre-trained tiny model maintains the estimation accuracy while avoiding intensive computations. The average training time is reduced from 19.79 seconds per iteration to 3.14 seconds per iteration.

\begin{table}[tb]
\tabfootnotesize
\centering
\setlength{\tabcolsep}{1.5pt}
\begin{tabular}{lccccc}
\toprule[1.25pt]
\multirow{2}{*}{Method} & \multicolumn{4}{c}{{Text-\textgreater{}Video}} & Iteration \\
\cline{2-5}
  & R@1 & R@5 & R@10 & MnR & Time  \\
 \midrule
Baseline & 40.0 & 66.8 & 77.0 & 16.5 & 2.06 s\\
\midrule
w/ Sampling-based method  & 41.5 & \textbf{68.6} & 78.9 & \textbf{15.1} & 19.79 s\\
\rowcolor{aliceblue!60} w/ Tiny estimator  & \textbf{41.8} & 67.5 & \textbf{79.0} & 15.2  & \textbf{3.14} s\\
\bottomrule[1.25pt]
\end{tabular}
\caption{\textbf{Effect of the Banzhaf Interaction Estimator.}  ``'' denotes that higher is better. ``'' denotes that lower is better.}
\label{tab:ban}
\end{table}

\subsection{The Structure of the Prediction Header}
Due to the disparity in semantic similarity and interaction index, we design a prediction header to predict the fine-grained relationship  between the  video frame and the  text word. To explore the impact of the structure of the prediction header on our method, we compare four popular structures, \ie, ``MLP'', ``CNN'', ``MLP+SA'' and ``CNN+SA''. Fig.~\ref{fig:prediction} illustrates the structures.

\begin{table}[tb]
\centering
\tabfootnotesize
\setlength{\tabcolsep}{5.2pt}
\begin{tabular}{lccccc}
\toprule[1.25pt]
\multirow{2}{*}{Method} &  \multicolumn{5}{c}{{Text-\textgreater{}Video}}  \\
\cline{2-6}
  & R@1 & R@5 & R@10 & MdR & MnR \\ \midrule
MLP  & 47.2 & 73.7 & 83.5 & \textbf{2.0} & 12.3\\
CNN  & 47.3 & 73.5 & \textbf{83.7} & \textbf{2.0} & 12.2\\
\midrule
MLP+SA  & 46.6 & 74.0 & \textbf{83.7} & \textbf{2.0} & 12.3\\
\rowcolor{aliceblue!60} CNN+SA   & \textbf{48.6} & \textbf{74.6} & 83.4 & \textbf{2.0} & \textbf{12.0} \\ \bottomrule[1.25pt]
\end{tabular}
\caption{\textbf{Effect of the structure of the prediction header on MSRVTT dataset.} ``SA'' is the self-attention module. ``'' denotes that higher is better. ``'' denotes that lower is better.}
\label{tab:prediction head}
\end{table}

\subsection{Self-Distillation}

\begin{table}[tb]
\tabfootnotesize
\centering
\setlength{\tabcolsep}{3.5pt}
\begin{tabular}{lccccc}
\toprule[1.25pt]
\multirow{2}{*}{Method} & \multicolumn{5}{c}{{Text-\textgreater{}Video}}\\
\cline{2-6}
  & R@1 & R@5 & R@10 & MdR & MnR \\
 \midrule
E-\textgreater{}A & 48.1 & 73.6 & 82.9 & \textbf{2.0} & 11.9\\
E-\textgreater{}O & 48.0 & 74.1 & 83.2 & \textbf{2.0} & \textbf{11.8}\\
A-\textgreater{}O & 48.0 & 73.0 & 83.1 & \textbf{2.0} & 12.0\\
E-\textgreater{}A +\ A-\textgreater{}O & 48.2 & 74.1 & 82.9 & \textbf{2.0} & \textbf{11.8}\\
\rowcolor{aliceblue!60} E-\textgreater{}A\ +\ E-\textgreater{}O & \textbf{48.6} & \textbf{74.6} & \textbf{83.4} & \textbf{2.0} & 12.0\\
\bottomrule[1.25pt]
\end{tabular}
\caption{\textbf{Ablation study about the self-distillation of our method on MSRVTT dataset.}  denote entity level, action level, and event level, respectively.  indicates the distillation direction. For example,  indicates the distillation from  to . ``'' denotes that higher is better. ``'' denotes that lower is better.}
\label{tab:self}
\end{table}

\begin{table}[tb]
\centering
\tabfootnotesize
\setlength{\tabcolsep}{1.8pt}
\begin{tabular}{ccccc}
\toprule[1.25pt]
{ Banzhaf} & {Deep} & { Self} & \multirow{2}{*}{Top1 Acc } & \multirow{2}{*}{Top5 Acc }  \\ 
  {Interaction} & {Supervision} & {Distillation} & & \\ \midrule
& &  & 45.2 & 73.1 \\ \midrule
 \scriptsize{\Checkmark} & &  & 45.8 & 73.7 \\
 & \scriptsize{\Checkmark} &  & 46.0 & 74.0 \\
  & \scriptsize{\Checkmark} & \scriptsize{\Checkmark}  & 46.0 & 74.1 \\
\scriptsize{\Checkmark}  & \scriptsize{\Checkmark} & & 46.1 & \textbf{74.2} \\
\rowcolor{aliceblue!60}  \scriptsize{\Checkmark} & \scriptsize{\Checkmark} & \scriptsize{\Checkmark}  & \textbf{46.2} & \textbf{74.2} \\
\bottomrule[1.25pt]
\end{tabular}
\caption{\textbf{Ablation study about the importance of each part on MSRVTT-QA dataset.} ``'' denotes that higher is better.}
\label{tab:a-vqa}
\end{table}

\textbf{``MLP''} consists of a linear layer with a Relu activation function for encoding features and a linear layer for decoding. The dimension of the hidden channels is 64.
\textbf{``CNN''} consists of a convolutional layer with a Relu activation function for encoding features and a convolutional layer for decoding. The dimension of the hidden channels is 64.
\textbf{``MLP+SA''} consists of a linear layer with a Relu activation function for encoding features, a self-attention module for capturing global interaction, and a linear layer for decoding. The dimension of the hidden channels is 64.
\textbf{``CNN+SA''} consists of a convolutional layer with a Relu activation function for encoding features, a self-attention module for capturing global interaction, and a convolutional layer for decoding. The dimension of the hidden channels is 64.

\begin{table}[tb]
\tabfootnotesize
\centering
\setlength{\tabcolsep}{5.2pt}
\begin{tabular}{lccccc}
\toprule[1.25pt]
\multirow{2}{*}{Method} & \multicolumn{5}{c}{{Text-\textgreater{}Video}}\\
\cline{2-6}
  & R@1 & R@5 & R@10 & MdR & MnR \\
 \midrule
Baseline & 46.6 & 73.1 & 83.0 & \textbf{2.0} &  13.3\\
\midrule
One level & 47.5 & 73.7 & 83.0 & \textbf{2.0} & 12.0\\
Two levels & 48.1 & 73.6 & 82.9 & \textbf{2.0} & \textbf{11.9}\\
\rowcolor{aliceblue!60} Three levels & \textbf{48.6} & \textbf{74.6} & \textbf{83.4} & \textbf{2.0} & 12.0\\
\bottomrule[1.25pt]
\end{tabular}
\caption{\textbf{Effect of the number of semantic levels (the number of stacked token merge modules) on MSRVTT dataset.} ``'' denotes that higher is better. ``'' denotes that lower is better.}
\label{tab:num}
\end{table}

\begin{figure}[tbp]
\centering
\includegraphics[width=.98\linewidth]{image/appendix1.pdf}
\caption{\textbf{Performance at each semantic level of text-to-video retrieval and video-to-text retrieval task.}}
\label{fig:self}
\end{figure}

As shown in Tab.~\ref{tab:prediction head}, we find that the combination of CNN and attention (``CNN+SA'') can capture both local and global interaction, so it is beneficial for predicting the fine-grained relationship between video and text. As a result, we adopt ``CNN+SA'' to achieve the best performance.

\begin{figure*}[tbp]
\centering
\includegraphics[width=0.98\linewidth]{image/retrieval_1.pdf}
\caption{\textbf{Visualization of the text-to-video retrieval.} Only the correct videos are highlighted in green.}
\label{fig:retrieval}
\end{figure*}

\begin{figure*}[tbp]
\centering
\includegraphics[width=0.98\linewidth]{image/qa2.pdf}
\caption{\textbf{Visualization of the video-question answering.}}
\label{fig:vqa}
\end{figure*}

\begin{figure*}[tbp]
\centering
\includegraphics[width=1\linewidth]{image/hierarchical_interaction1.pdf}
\caption{\textbf{Visualization of the hierarchical interaction.} Here, the degree of confidence from high to low is represented by red, orange, green and blue lines, respectively. Entity-level interactions demonstrate the semantic correlation between frames and words. Action-level interactions indicate the semantic correlation between clips and phrases. Event-level interactions show the semantic correlation between segments and paragraphs.}
\label{fig:hierarchical_0}
\end{figure*}

\begin{figure*}[tbp]
\centering
\includegraphics[width=1.\linewidth]{image/hierarchical_interaction2.pdf}
\caption{\textbf{Visualization of the hierarchical interaction.} Here, the degree of confidence from high to low is represented by red, orange, green and blue lines, respectively. Entity-level interactions demonstrate the semantic correlation between frames and words. Action-level interactions indicate the semantic correlation between clips and phrases. Event-level interactions show the semantic correlation between segments and paragraphs.}
\label{fig:hierarchical_1}
\end{figure*}

\begin{figure*}[tbp]
\centering
\includegraphics[width=1\linewidth]{image/hierarchical_interaction3.pdf}
\caption{\textbf{Visualization of the hierarchical interaction.} Here, the degree of confidence from high to low is represented by red, orange, green and blue lines, respectively. Entity-level interactions demonstrate the semantic correlation between frames and words. Action-level interactions indicate the semantic correlation between clips and phrases. Event-level interactions show the semantic correlation between segments and paragraphs.}
\label{fig:hierarchical_2}
\end{figure*}

Fig.~\ref{fig:self} shows the performance of each semantic level. We find that the entity level converges first in the training process. This is because higher-level semantic features are merged from lower-level semantic features. When lower-level semantic features do not converge, it is difficult for higher-level semantic features to learn semantic information. Based on this observation, we propose using lower-level semantic features to guide the learning of higher-level semantic features. Thus, we distill the entity-level similarity to the other two semantic levels.

To illustrate the impact of the self-distillation of our method, we conduct ablation experiments on MSRVTT dataset in Tab.~\ref{tab:self}. As we can see, self-distillation improves the generalization ability. Distilling from the entity level to the other two semantic levels achieves the best results. As a result, we distill the entity-level similarity to the other two semantic levels as default in practice.

\subsection{Ablation for Video-Question Answering Task}
To illustrate the importance of each part of our method for the video-question answering, we conduct ablation experiments on MSRVTT-QA dataset in Tab.~\ref{tab:a-vqa}. As we can see, Banzhaf Interaction boosts the baseline with the improvement up to 0.6\% at Top1 accuracy. Moreover, deep supervision and self-distillation significantly improve the generalization ability. Self-distillation provides limited improvement for video-question answering compared to text-video retrieval. This is because reasoning relies primarily on high-level semantic features. Therefore, it is difficult for low-level semantic features to guide high-level semantic features. Our full model achieves the best performance and outperforms the baseline by 1.0\% at Top1 accuracy.

\subsection{The Number of Semantic Levels}
To efficiently generate coalitions among game players, we cluster the original visual (textual) tokens and compute the Banzhaf Interaction between the merged tokens. By stacking token merge modules, we get cross-modal interaction efficiently at different semantic levels.

To explore the impact of the number of semantic levels on our method, we conduct ablation experiments on MSRVTT dataset in Tab.~\ref{tab:num}. We find that the performance of the model increases with the number of semantic levels. These results indicate that stacking more token merge modules can provide more coalitions, which enables the model to learn more diverse semantic interaction information. We make a trade-off between the number of semantic levels and computation cost and set the number of semantic levels to 3 in practice.

\subsection{Limitations of our Work}
The cross-modal contrastive approach typically exploits the coarse-grained labels of video-text pairs to learn a global semantic interaction. To move a step further, we model video-text as game players with multivariate cooperative game theory to handle the uncertainty during fine-grained semantic interaction with diverse granularity, flexible combination, and vague intensity. Therefore, our method inevitably requires more training time costs. Although our method takes less time than TS2-Net~\cite{liu2022ts2} during the inference stage (see Tab.~5 in the main paper), more effort could be paid to obtain an efficient structure in the future.

\section{Visualizations}
\subsection{Text-to-Video Retrieval}
We show two retrieval examples from the MSR-VTT testing set for text-to-video retrieval in Fig.~\ref{fig:retrieval}. As shown in Fig.~\ref{fig:retrieval}, our method successfully retrieves the ground-truth video. These results demonstrate that our method can align video and text effectively.

\subsection{Video-Question Answering}
We show the visualization of the video-question answering in Fig.~\ref{fig:vqa}. As shown in Fig.~\ref{fig:vqa}, our method succeeds in getting the ground-truth answer. These results demonstrate that our method can deal with cross-modal inference task effectively.

\subsection{Hierarchical Interaction}
To better understand the proposed method, we show the visualization of the hierarchical interaction in Fig.~\ref{fig:hierarchical_0}, Fig.~\ref{fig:hierarchical_1} and Fig.~\ref{fig:hierarchical_2}. This experiment shows that our Hierarchical Banzhaf Interaction (HBI) can effectively handle fine-grained semantic interaction with diverse granularity, flexible combination, and vague intensity. More encouragingly, the visualization illustrates that the proposed method can be used as a tool for visualizing the cross-modal interaction and help us understand the cross-modal model.

\end{document}
