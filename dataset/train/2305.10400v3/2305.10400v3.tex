\documentclass{article}


\PassOptionsToPackage{numbers, compress}{natbib}






\usepackage[preprint]{neurips_2023}








\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         



\title{What You See is What You Read? \\ Improving Text-Image Alignment Evaluation}




\author{Michal Yarom\textsuperscript{G*} \\
  \And
  Yonatan Bitton\textsuperscript{H,G}\thanks{Equal contribution. Yonatan participated in this work as part of an internship at Google Research.} \\
  \And
  Soravit Changpinyo\textsuperscript{G} \\
  \And
  Roee Aharoni\textsuperscript{G} \\
  \And
  Jonathan Herzig\textsuperscript{G} \\
  \And
  Oran Lang\textsuperscript{G} \\
  \And
  Eran Ofek\textsuperscript{G} \\
  \And
  Idan Szpektor\textsuperscript{G}
  \And
  \vspace{-10pt}
  \\
  \textsuperscript{G}Google Research \hspace{20px} \textsuperscript{H}The Hebrew University of Jerusalem\\
  \texttt{michalyarom@google.com, yonatan.bitton@mail.huji.ac.il}
  \\
}






\usepackage{multirow}
\usepackage{diagbox}
\usepackage{booktabs}
\usepackage{xcolor}
\definecolor{mygreen}{HTML}{2CB600}
\usepackage{subfig}
\usepackage{xspace}
\usepackage{appendix}
\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{amsmath}
\usepackage{pifont}
\usepackage{tabularx}
\usepackage{seqsplit}
\usepackage{nameref}
\usepackage{varioref}
\usepackage{hyperref}
\usepackage{cleveref}

\usepackage[section]{placeins}
\usepackage{wrapfig}

\newcommand{\resolved}[1]{}

\newcommand{\com}[1]{}

\newcommand{\draftcomment}[3]{{\textcolor{#3}{[#1]#2}}}
\renewcommand{\draftcomment}[3]{}  

\newcommand{\yonatan}[1]{\draftcomment{#1}{\textsc{yonatan}}{blue}}
\newcommand{\yonatanst}[1]{\yonatan{\sout{#1}}}
\newcommand{\yonatanrep}[2]{\yonatanst{#1}\yonatan{#2}}

\newcommand{\michal}[1]{\draftcomment{#1}{\textsc{michal}}{red}}
\newcommand{\michalst}[1]{\michal{\sout{#1}}}
\newcommand{\michalrep}[2]{\michalst{#1}\michal{#2}}

\newcommand{\roee}[1]{\draftcomment{#1}{\textsc{roee}}{purple}}
\newcommand{\roeest}[1]{\roee{\sout{#1}}}
\newcommand{\roeerep}[2]{\roeest{#1}\roee{#2}}

\newcommand{\jonathan}[1]{\draftcomment{#1}{\textsc{jonathan}}{orange}}
\newcommand{\jonathanst}[1]{\jonathan{\sout{#1}}}
\newcommand{\jonathanrep}[2]{\jonathanst{#1}\jonathan{#2}}

\newcommand{\beer}[1]{\draftcomment{#1}{\textsc{beer}}{teal}}
\newcommand{\beerst}[1]{\beer{\sout{#1}}}
\newcommand{\beerrep}[2]{\beerst{#1}\beer{#2}}

\newcommand{\oran}[1]{\draftcomment{#1}{\textsc{oran}}{magenta}}
\newcommand{\oranst}[1]{\oran{\sout{#1}}}
\newcommand{\oranrep}[2]{\oranst{#1}\oran{#2}}

\newcommand{\idan}[1]{\draftcomment{#1}{\textsc{idan}}{red}}
\newcommand{\idanst}[1]{\idan{\sout{#1}}}
\newcommand{\idanrep}[2]{\idanst{#1}\idan{#2}}

\newcommand{\eran}[1]{\draftcomment{#1}{\textsc{eran}}{red}}
\newcommand{\eranst}[1]{\eran{\sout{#1}}}
\newcommand{\eranrep}[2]{\eranst{#1}\eran{#2}}

\newcommand{\datasetname}[0]{SeeTRUE\xspace}
\newcommand{\VQSQR}[0]{VQ\xspace}
\newcommand{\website}[0]{\url{anonymous}}


\begin{document}

\maketitle

\begin{abstract}
\setcounter{footnote}{0}
\yonatan{Take a look at the ensemble scores at Table 2}Automatically determining whether a text and a corresponding image are semantically aligned is a significant challenge for vision-language models, with applications in generative text-to-image and image-to-text tasks. In this work, we study methods for automatic text-image alignment evaluation. We first introduce \datasetname: a comprehensive evaluation set, spanning multiple datasets from both text-to-image and image-to-text generation tasks, with human judgements for whether a given text-image pair is semantically aligned. We then describe two automatic methods to determine alignment: the first involving a pipeline based on question generation and visual question answering models, and the second employing an end-to-end classification approach by finetuning multimodal pretrained models. Both methods surpass prior approaches in various text-image alignment tasks, with significant improvements in challenging cases that involve complex composition or unnatural images. Finally, we demonstrate how our approaches can localize specific misalignments between an image and a given text, and how they can be used to automatically re-rank candidates in text-to-image generation.\footnote{Data and code are attached to this submission.}%
 \begin{figure}[!h]
    \centering
    \includegraphics[width=0.9\textwidth]{images/fig1.pdf}\\
    \caption{Overview of our approach to text-image alignment evaluation using \datasetname. We curate diverse pairs of real and synthetic text and images and use automatic contradiction generation and human evaluation to create a benchmark dataset. We propose two methods for text-image alignment evaluation: VQ and VNLI, demonstrated with example pairs. 
    }
    \label{fig:fig1}
\end{figure} \end{abstract}

\section{Introduction}
\label{sec:intro}
The recent success and proliferation of multi-modal large language models (LLMs) for text-to-image and image-to-text generation \citep{ramesh2021zero, ramesh2022hierarchical, rombach2022high, yu2022scaling, saharia2022photorealistic,li2022blip, li2023blip, alayrac2022flamingo} make such technology increasingly useful for a wide range of creative applications. However, such models still struggle in generating semantically-aligned image-text pairs; in text-to-image generation, models do not cope with complex specifications \citep{marcus2022very, liu2021learning} or fail to map words in the prompt to visual entities \citep{rassin2022dalle, chefer2023attend}. In image captioning, object hallucination is a long-standing challenge \citep{rohrbach-etal-2018-object} with generated captions still being inferior to human-written ones \citep{kasai-etal-2022-transparent}. 

Given the above, the task of automatically determining whether a given text-image pair is semantically aligned is highly important, as it is useful both for \textit{evaluating} and for \textit{improving} text-to-image and image-to-text models. However, existing evaluation approaches are still far from ideal; common methods like CLIP \citep{radford2021learning} or BLIP \citep{li2022blip,li2023blip} are based on encoding the image and text as fixed-size embeddings, making it hard to model complex semantics \citep{yuksekgonul2022and}. In addition, while the task is relevant both to text-to-image and image-to-text generation, it is usually studied in silo while considering only one of the applications, thus impeding progress.

In this work, we promote a comprehensive approach to evaluating image-text alignment. We introduce \datasetname, a diverse evaluation suite which includes a wide range of image-text pairs with human judgments that determine if the image and text are semantically aligned. 
\datasetname encompasses both real and synthetic images and text, allowing the assessment of text-image alignment models' generalization capabilities across various tasks and 31,855 labeled examples from diverse sources. As part of constructing \datasetname, we also introduce a novel method for generating contradicting captions from existing ones by prompting a large language model with tailored instructions.

We present two approaches for automatic image-text alignment evaluation. The first, \VQSQR, utilizes question generation and visual question answering by generating questions related to the text \citep{changpinyo2022all} and ensuring that the correct answer is obtained when asking these questions with the provided image. The second method, Visual Entailment\footnote{We use the terms Entailment and Natural Language Inference (NLI) interchangeably.}  (VNLI), involves directly fine-tuning a large pretrained multimodal model to predict if a given image-text pair is semantically aligned. Both strategies are inspired by recent studies on evaluating factual consistency between two texts \citep{honovich2021q, honovich-etal-2022-true-evaluating, summac_laban_2022, tang2022understanding}.

We conduct comprehensive experiments on \datasetname, demonstrating that both our \VQSQR and VNLI methods outperform a wide range of strong baselines, including various versions of CLIP~\citep{radford2021learning}, COCA~\citep{yu2022coca}, BLIP~\citep{li2022blip,li2023blip}, and OFA~\citep{wang2022ofa}. While previous work showed that vision-and-language models tend to exhibit sub-optimal ``bag-of-words'' behavior \citep{yuksekgonul2022and}, the \VQSQR method particularly excels on datasets with compositional challenges, achieving state-of-the-art results on the Winoground dataset \citep{thrush2022winoground} e.g. by improving the \emph{group score} from 16\% to 30.5\%. Our methods also demonstrate improved performance when evaluating synthetic images (e.g. on DrawBench \cite{saharia2022photorealistic} and EditBench \cite{wang2022imagen}). Finally, we showcase how \VQSQR can identify specific sources of misalignment for a given text-image pair and how our methods can re-rank generated image candidates for a given prompt.

To summarize, our contributions are as follows: (1) We introduce the \datasetname benchmark for meta-evaluation of image-text alignment. (2) We introduce a novel method to generate contradicting image captions from given captions with LLMs. (3) We suggest two reference-free metrics for image-text alignment evaluation: \VQSQR, based on question generation and visual question answering, and VNLI, based on fine-tuning large multimodal language models. (4) We conduct extensive evaluation of the above approaches against strong baselines, demonstrating superior performance over multiple datasets. (5) We release our evaluation suite, models and code to foster future work.
 

\section{SeeTRUE: A Comprehensive Text-Image Alignment Benchmark}
\label{sec:datasets}

We begin by introducing \datasetname, a diverse benchmark for meta-evaluation of image-text alignment methods, covering the 4-way combinations of real and synthetic text-and-image pairs. It addresses limitations in current benchmarks, which mainly focus on natural images and often lack challenging negative captions. SeeTRUE allows to better assess the generalization abilities of text-image alignment models across various tasks.  

Defining how image-text alignment is assessed has a direct impact on the construction of evaluation datasets. As images can display more details than described in their caption or text prompt, we define image-text alignment as the case where all the details described in the text are accurately represented within the image. Inspired by the Textual Entailment task \citep{dagan2010recognizing} which judges for two pieces of text whether one (the ``hypothesis'') can be inferred given the other (the ``premise''), our definition maps the image to the premise and the text to the hypothesis, resulting in the task of predicting whether the information in the text can be inferred from the given image. 

\subsection{Datasets}
\begin{table}[!t]
\centering
\caption{\datasetname: a benchmark for image-text alignment encompassing 31,855 real and synthetic image-text pairs from diverse datasets and tasks.
An example from each dataset is presented below.}
\vspace{5px}
\resizebox{\columnwidth}{!}{
\begin{tabular}{@{}>{\centering\arraybackslash}m{1.95cm} >{\centering\arraybackslash}m{1.95cm} >{\centering\arraybackslash}m{1.95cm} >{\centering\arraybackslash}m{1.95cm} >{\centering\arraybackslash}m{1.95cm} >{\centering\arraybackslash}m{1.95cm} >{\centering\arraybackslash}m{1.95cm} >{\centering\arraybackslash}m{1.95cm} >{\centering\arraybackslash}m{1.95cm}}
\toprule
\multicolumn{1}{l}{} & \multicolumn{2}{c}{\textbf{Real Text + Real Images}} & \multicolumn{3}{c}{\textbf{Real Text + Synthetic Images}}    & \textbf{Synthetic + Real} & \textbf{Synthetic + Synthetic} \\ \cmidrule(l){1-1} \cmidrule(l){2-3} \cmidrule(l){4-6} \cmidrule(l){7-7} \cmidrule(l){8-8}
\multicolumn{1}{l}{\textbf{Dataset Name}}        & SNLI-VE                         & Winoground                      & DrawBench         & EditBench   & COCO t2i  & COCO-Con            & PickaPic-Con        \\ 
\multicolumn{1}{l}{\textbf{\# Test Examples}} & 17,901 & 1,600 & 1,968 & 3,827 & 2,586 & 1,992 & 1,981 \\
\multicolumn{1}{l}{\textbf{\% Positive / Total}} & 33.3\% & 50\% & 55.7\% & 36.9\% & 63.6\% & 52.7\% & 44.1\% \\
\multicolumn{1}{l}{\textbf{Labeled in this work?}} & \ding{55} & \ding{55} & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\


\midrule
\multicolumn{1}{l}{\textbf{Image}} & \includegraphics[height=1.95cm,width=1.95cm]{images/datasets_examples/2725441885.jpg} & \includegraphics[height=1.95cm,width=1.95cm]{images/datasets_examples/ex_44_img_0.png} & \includegraphics[height=1.95cm,width=1.95cm]{images/datasets_examples/id_23_Colors_sd_2_1_0.png} & \includegraphics[height=1.95cm,width=1.95cm]{images/datasets_examples/1324_a=pink_o=candle_s=indoor.png} & \includegraphics[height=1.95cm,width=1.95cm]{images/datasets_examples/id_194_sd_1_4_3.png} & \includegraphics[height=1.95cm,width=1.95cm]{images/datasets_examples/COCO_val2014_000000459757.jpg} & \includegraphics[height=1.95cm,width=1.95cm]{images/datasets_examples/ecb0f673-333a-4834-86a8-0a4c0207b9b9.png} \\
\multicolumn{1}{l}{\textbf{Text}} & the player swings his bat & the heavy oncoming traffic is contrasted with the light outgoing traffic & A blue cup and a green cell phone & a few pink candles and some cream on top of a cake. & A person on a snow board high up in the air. & A giraffe leaned over in a plush field next to some cows & a doctor wearing a white coat in the middle of a street \\
\multicolumn{1}{l}{\textbf{Human Label}} & True & True & False & True & False & False & True \\ \bottomrule
\end{tabular}}
\vspace{-15px}
\label{tab:datasets_table}
\end{table}
 
We describe the datasets included in our benchmark, with a high-level overview in Table \ref{tab:datasets_table}.

\paragraph{Real text and real images.} For pairs of human-written text and real (non-generated) images, we include the SNLI-VE \citep{xie2019visual} and Winoground \citep{thrush2022winoground} datasets. SNLI-VE is a widely adopted VNLI dataset containing an image, a text, and a label of the alignment between the two -- entailment, contradiction, or neutral. Winoground is a challenging dataset for compositional understanding, where each example includes two images and two text captions, where the task is to match each text to its corresponding image. The captions only differ in a few words, which should result in distinct visual interpretations. For example, ``some plants surrounding a lightbulb'' vs. ``a lightbulb surrounding some plants''.

\paragraph{Real text and synthetic images.} For datasets that represent text-to-image generation tasks we use EditBench \citep{wang2022imagen} which offers prompts and images generated by various text-to-image models given those prompts, accompanied by alignment ratings. To encourage more diversity in the data, we also create new datasets by generating images using Stable Diffusion models \citep{rombach2022high} (V1.4 and V2.1) and Imagen \citep{saharia2022photorealistic} by prompting them with COCO \citep{lin2014microsoft} captions and text prompts from DrawBench \citep{saharia2022photorealistic}, creating the COCO text-to-image (``COCO t2i'') and the DrawBench text-to-image datasets.

\paragraph{Synthetic text and real images.} This category includes a new dataset which we name COCO-Con. COCO-Con is generated using a novel automatic method which we describe in detail in Section \ref{ssec:gen_contradictions}. Specifically, we generate synthetic contradicting captions for COCO images based on their original captions by prompting a large language model, and verify the resulting captions with human raters.

\paragraph{Synthetic text and synthetic images.} We utilize PickaPic \cite{kirstain2023pickapic}, a source of user-generated and ranked synthetic images. We create synthetic captions using BLIP2 \citep{li2023blip} and employ our automatic contradiction generation method (Section \ref{ssec:gen_contradictions}) to produce unaligned captions. This category evaluates synthetic text that is generated by image captioning models, e.g. for improving textual image search. 

We note that some of the datasets are only used for testing (e.g., Winoground, DrawBench, EditBench) while others include both training and test sets (e.g., SNLI-VE, COCO t2i, COCO-Con, PickaPic-Con). This allows us to investigate different training configurations and their effect on performance.


\subsection{Human Annotation and Evaluation}
\label{sec:human_annotation}

To standardize the labeling scheme across datasets, we follow TRUE \citep{honovich-etal-2022-true-evaluating} and use binary annotations for alignment/misalignment. In datasets with three-way annotations (e.g. Entailment, Contradiction, Neutral) we convert the labels to binary labels by collapsing all non-entailment/non-alignment labels to a single negative label.

Some datasets, such as COCO-Con and PickaPic-Con, start with automatically generated labels, while others lack annotations entirely (e.g. DrawBench). To make sure we have high quality labels we conduct human annotation for all test examples in such datasets.
We ask three crowd-workers from Amazon Mechanical Turk (AMT) to evaluate whether a given image-text pair is aligned, by answering the question: ``Does the image present all the details described in the text correctly?'' with ``Yes'' or ``No''. If the answer is ``No'', the workers are also requested to describe the main misalignment to enhance the annotation quality. 
While the random chance of agreement is 25\%, the annotators reached consensus in 80\% of cases. Furthermore, we measured a Fleiss-Kappa \citep{fleiss1971measuring} score of 0.722, showing a good level of agreement between the annotators. Full annotation details, AMT user interface example, and agreement numbers per dataset can be found in \cref{sec:human_annotation_appendix}.

The datasets we annotated include DrawBench, COCO t2i, COCO-Con and PickaPic-Con, with statistics presented in Table~\ref{tab:datasets_table}. These datasets vary in their positive/negative distribution, with COCO t2i having the highest percentage of positives (63.6\%) and DrawBench having the lowest (36.9\%). The agreement with the auto-label is 94\% for COCO-Con and 77\% for PickaPic-Con. 
To prevent the inclusion of offensive images, particularly those that are synthetically generated, annotators are asked to mark any images that may be considered offensive and these were discarded.


\subsection{ConGen: Generating Contradicting Captions by Prompting LLMs} 
\label{ssec:gen_contradictions}

\begin{figure}
    \centering
    \includegraphics[width=0.975\textwidth]{images/fig2.pdf}
    \caption{
    (a) The SeeTRUE generation process. (i) An image-text pair from a dataset is used to generate a contradicting caption using ConGen. (ii) An image (real or synthetic) is passed through a captioning model to generate a caption, which is then passed to ConGen to generate a contradicting caption. (iii) A text-to-image model is applied on captions from the dataset to create multiple image-text pairs. All the resulting examples are evaluated by human raters to create SeeTRUE. 
    (b) The contradiction generation process (ConGen) takes a caption as input and instructs an LLM to generate variants that contradict it. An NLI model is used to select the variant with the lowest entailment score. }
    \label{fig:fig_contrast_sets}
\end{figure} We propose an automatic method for generating unaligned captions from existing, aligned image-and-text pairs, with the goal of creating challenging examples for evaluation and training. Our method is inspired by the concept of contrast sets: given an original example with a corresponding label, we create a minimally perturbed example where the perturbation changes the corresponding label \citep{gardner2020evaluating, bitton2021automatic, li-etal-2020-linguistically, rosenman-etal-2020-exposing}. Contrast sets address the issue of supervised models exploiting data artifacts in i.i.d. train/test splits to achieve high test scores, while their performance degrades significantly on samples outside their training distribution. 

To create contrast sets for image-text alignment, we go over the text captions from the image-text pairs in the COCO and PickaPic datasets, covering both natural and synthetic images. For each caption we instruct PaLM \citep{chowdhery2022palm}, a large language model, to generate several contradicting captions via few-shot inference with 7 positive and 8 negative examples. For instance, for the caption ``\textit{a \textbf{knife} sitting next to carrots on top of a cutting board}'', the model replaces the word \emph{knife} with \emph{spoon} (see Fig.~\ref{fig:fig_contrast_sets}, left). We then use a Natural Language Inference (NLI) model \cite{honovich2021q} to score whether the generated caption is indeed contradicting the original, and select the generated caption with the highest contradiction score. Figure \ref{fig:fig_contrast_sets} illustrates this process. Human annotators verified that the resulting contradicting captions are of high quality, with 94\% agreement with human labels in COCO and 77\% agreement with human labels in PickaPic (more details in \cref{sec:human_annotation}).
 
\section{Methods}
\label{sec:method}
\begin{figure}[!t]
    \centering
    \includegraphics[width=\textwidth]{images/vqa_pipeline_3.pdf}\\
    \vspace{-10px}
    \caption{The V pipeline: (a) given a text and an image, (b) generate question and answer pairs from the text, (c)  re-write each pair as a yes-no question and obtain the 'yes' answer probability from the image as an alignment score. (d) Finally, average all alignment pair scores as the final  score.
    }
    \vspace{-10px}
    \label{fig:fig_zeroshot_method}
\end{figure}
 Using our \datasetname benchmark, we would like to reassess the performance of multimodal alignment approaches. In this section we introduce two image-text alignment methods. In Section \ref{sec:visual_entailment} we will compare their performance against established, previously published methods.

\subsection{\texorpdfstring{: Zero-Shot Alignment via Question Generation and Visual Question Answering}{VQ2: Zero-Shot Alignment via Question Generation and Visual Question Answering}}


Inspired by recent work on factual consistency evaluation in text-to-text tasks \citep{wang-etal-2020-asking, honovich2021q, scialom-etal-2021-questeval}, we propose a zero-shot approach for automatically evaluating image-text alignment based on question generation and question answering. Figure \ref{fig:fig_zeroshot_method} provides an overview of the method.
For a given image-text pair , we first extract a set of candidate answer spans  from the given text . 
Then, we use a question generation (QG) model to generate a question for each answer candidate . 
Each generated question-answer pair  is scored with a question answering (QA) model, and if  returns a low score, we filter out the corresponding pair.
This results in a subset of  question-answer pairs . 

Each generated question-answer pair  is then independently validated based on the image  using a visual question answering (VQA) model, obtaining an answer alignment score  (more details on how this score is computed are given in \ref{ascessing_qa_alignment}).
The overall alignment score for a image-text pair, denoted as the  score, is the average over all  scores for all the generated  pairs.
 We next describe each step in more detail.

\paragraph{Generating question-answer pairs.}

We follow the  method~\citep{changpinyo2022all} to generate question and answer pairs given an image caption in three steps. First, answer spans are extracted from text  using SpaCy \cite{spacy2}, based on Part-of-Speech (POS) and dependency parse tree annotations. Then, for each answer span, a question  is generated given the answer span and the full caption as input using a T5-XXL model fine-tuned on SQuAD1.1 \citep{rajpurkar-etal-2016-squad}. 
Finally, each candidate question-answer pair  is validated by answering  on  using a QA model, which is trained by fine tuning a T5-XXL model on SQuAD2.0 \citep{rajpurkar-etal-2018-know} and Natural Questions \citep{kwiatkowski-etal-2019-natural}. Finally, we match the output answer  to the expected answer  using token-level F1 comparison. As suggested in \cite{changpinyo2022all}, if the answer comparison F1 score is lower than 0.54 the question-answer pair is filtered out.

\paragraph{Assessing question-answer pair alignment against the image.}\label{ascessing_qa_alignment}
To determine if the information conveyed by the text  is presented correctly in the image , we use a VQA model based on PaLI-17B~\citep{pali2} as follows. We reformulate each question and answer candidate pair  into a new yes-no predicate question  using the format \textit{``is  true for  in this image?''}. For example, for the text \textit{``two girls are sitting on some grass''}, and the automatically induced question-answer pair \{\textit{``what are the girls sitting on?''}, \textit{``some grass''}\}, the reformulated question is ``is \textit{on some grass} true for \textit{what are the girls sitting on?} in this image?''.
The VQA model is then invoked to answer the predicate question  over image . We define the alignment score  as the probability of the model for answering ``yes''. We note that we also experimented with other answer alignment methods, e.g. ones that directly ask the generated question without formulating it as a yes/no question. However, the yes-no approach worked best. More details can be found in \cref{sec:vq2_ablations}.


\subsection{End-to-end VNLI Models}

Another approach is to train end-to-end Visual NLI models (VNLI) that receive an image and text as input, and directly predict an alignment score. We do so by fine-tuning multimodal pretrained models while formatting the examples as yes/no questions using the prompt: ``Does this image entail the description: \{text\}?'', followed by a binary ``yes'' or ``no'' answer. In inference time we measure the probabilities of predicting ``yes'' or ''no'', and use the relative ratio between the two as the alignment score. Specifically, we finetune BLIP2~\cite{li2023blip} and PaLI-17B~\cite{pali2} using a dataset comprising 110K text-image pairs labeled with alignment annotations. This includes 44K examples from COCO-Con, 3.5K from PickaPic-Con, 20K from COCO t2i and 40K from the training split of the SNLI-VE dataset. We generate COCO-Con and COCO t2i based on the COCO train split and PickaPic-Con with a distinct set of images, to ensure that there is no overlap with samples in the \datasetname benchmark. More technical details and training hyperparameters are described in \cref{sec:reproducibility}. 
 
\section{Experiments}
\label{sec:visual_entailment}


\subsection{Models and Metrics}

We evaluate  and fine-tuned VNLI models based on PaLI and BLIP2 (Section \ref{sec:method}) against several state-of-the-art multimodal models: (a) CLIP \citep{radford2021learning} and two larger versions - CLIP RN50x64 and CLIP ViT-L 14 \citep{dosovitskiy2020image}, (b) CoCa \citep{yu2022coca}, (c) BLIP Large \citep{li2022blip}, (d) BLIP2 FlanT5-XXL \citep{li2023blip}, and (e) OFA Large \citep{wang2022ofa}, and (f) TIFA \citep{hu2023tifa}. First five models were typically trained with either a contrastive objective or an image-text matching objective that samples positive or negative caption-label pairs. TIFA, like \VQSQR, employs a VQA model with generated question-answer pairs. However, TIFA contrasts textual and visual answer candidates provided by the model, while our method checks if the textual answer is accurate given the image.

We assess each method's ability to detect misalignments in each dataset in \datasetname. We use a binary labeling scheme and report the Area Under the ROC Curve (ROC AUC) for each method. For Winoground, we use existing metrics: (1) \textit{text score}: accuracy in selecting the right caption for an image; (2) \textit{image score}: accuracy in choosing the correct image given a caption; (3) \textit{group score}: accuracy requiring all four image-caption pairs to be correct for a successful example.

\subsection{Results}

We present our main results in Table \ref{tab:main_res}. Notably, our V approach excels as the top-performing zero-shot model across all datasets, surpassing other zero-shot baselines and even outperforming most of the fine-tuned models while achieving the highest score on the challenging Winoground dataset. This shows the robustness of the \VQSQR approach, which decomposes the alignment decision by generating multiple yes/no verification questions.

When looking at finetuned models, the PaLI variant finetuned on all the available datasets outperforms all the rest with an average score of 82.9, achieving the best results on 3 out of 7 datasets. The SNLI-VE-only variant is behind with an average score of 79.7, while achieving the highest scores for 2 out of 7 datasets. This shows that integrating synthetic training data leads to notable improvements on synthetic images on DrawBench (+4\%), EditBench (+11.7\%), , COCO t2i (+5.5\%), PickaPic-Con (+2.2\%). Nevertheless, the inclusion of synthetic training data did not enhance performance on the COCO-Con dataset, comprised solely of natural images. This indicates that the variation in image types could be a contributing factor that calls for additional exploration. Notably, the last row shows a simple average between \VQSQR and our leading fine-tuned PaLI model, that produces higher performance, suggesting that they complement each other effectively.


\begin{table*}[!t]
\footnotesize
\begin{center}
\vspace{-5pt}
\caption{Main Results on \datasetname, split into zero-shot and end-to-end fine-tuned methods across the real and synthetic image-text eval-sets. The numbers in the table are ROC AUC.\yonatan{added ensemble, and it looks good. Still missing EditBench \VQSQR scores. I think we can add it to the submission}}
\label{tab:main_res}
\vspace{-5pt}
\resizebox{\columnwidth}{!}{
\begin{tabular}{@{}llcccccccc@{}}
\toprule
\multirow{2}{*}{}           & Text \& Images                                                          & \multicolumn{2}{c}{Real + Real} & \multicolumn{3}{c}{Real + Synthetic}    & Synthetic + Real & Synthetic + Synthetic & \multirow{2}{*}{\textbf{Avg.}} \\ \cmidrule(l){2-2} \cmidrule(l){3-4} \cmidrule(l){5-7} \cmidrule(l){8-8} \cmidrule(l){9-9} 
                            & Model                                                                   & SNLI-VE       & Winoground      & DrawBench & EditBench & COCO t2i & COCO-Con         & PickaPic-Con          \\ \midrule
\parbox[t]{2mm}{\multirow{10}{*}{\rotatebox[origin=c]{90}{zero-shot}}} & 
  CLIP RN50x64                                                            & 66.6 & 53.6 & 59.2 & 67.1 &  58.8 & 71.1  & 66.8 & 63.3 \\
& CLIP ViT-L14                                                            & 65.8 & 53.3 & 60.5 & 62.1 & 58.8  & 70.7  & 66.8 & 62.6 \\
& COCA ViT-L14                                                            & 68.5 &  53.1  & 67.4 & 66.3   & 62.1  & 74.2  & 68.1 & 65.7 \\
& \begin{tabular}[c]{@{}c@{}}COCA   ViT-L14 \\ (f.t on COCO)\end{tabular} & 70 &  53.1  & 66.2 &   68.3 &  66.2 & 76.5  & 67.2 & 66.8 \\
& BLIP                                                                    & 75.2 & 58.2 & 60.5 & 68 & 70.7  & 84.2  & 76.6 & 70.5  \\
& BLIP2                                                                   & 76.4 & 56.9 & 58.5 & 67.5 & 66.9  & 84.3  & 76.9 & 69.6 \\
& BLIP 2 (f.t. COCO)                                                      & 75.9 & 60  & 65.7 & 70 & 73.3 & 85.8   & 78 & 72.7   \\
& PaLI  & 65.4  & 53.6   & 60.2 & 56.7  &  53.3 & 65.5  &  60.5 & 59.3 \\
& TIFA & -- & 58.0 & 73.4 &  67.8 &  72.0  & --  &  -- & --  \\
& \VQSQR (Ours)   & 88.0 & \textbf{63.5}& 82.6& 73.6& \textbf{83.4} & 87.1 &   81.7 & \textbf{80.0} \\ \midrule
\parbox[t]{2mm}{\multirow{2}{*}{\rotatebox[origin=c]{90}{f.t. snli-ve}}} & \begin{tabular}[c]{@{}l@{}}OFA Large\end{tabular}  & 80.5  &  53.3  & 77.6 & 70.9 & 67.5  & 75.4 & 69.5 & 70.7 \\
& BLIP2     & 82.3          &        58.5         & 64.3      & 58.7      &    60.5             & 82.6             &            66.9 &    67.7      \\               
& PaLI     & \textbf{95.1}          &        61.7         & 82.8      & 65.5      & 77.7                 & \textbf{91.2}            & 83.7  & 79.7    \\ 
& PaLI + Synthetic Data     & 94.2          &        61.8         & \textbf{86.8}      & \textbf{77.2}      & 83.2                 & 91            & \textbf{85.9} & \textbf{82.9}    \\ \midrule & Avg(\VQSQR, PaLI+Syn)     & 93.9          &        \textbf{63.5}         & \textbf{87.8}      & \textbf{78.4}      & \textbf{85.1}                 & \textbf{93}            & \textbf{87.3} & \textbf{84.1}    \\ \addlinespace[1mm] 
\bottomrule
\end{tabular}}\end{center}
\vspace{-5px}
\end{table*} \begin{table*}[t]
\scriptsize
\begin{center}
\vspace{-5pt}
\caption{Results on the Winoground dataset, reporting text score, image score, and group score.}
\label{tab:winoground}
\vspace{-5pt}
\begin{tabular}{@{}lccc@{}}
\toprule
Model                                                    & Text Score  & Image Score   & Group Score   \\ \midrule
V (Ours)  & \textbf{47.00} & \textbf{42.20} & \textbf{30.50} \\
PaLI (ft SNLI-VE + Synthetic Data) & 46.5 & 38 & 28.75 \\
PaLI (ft SNLI-VE) & 45.00 & 41.50 & 28.70 \\
BLIP2 (f.t. COCO)                                        & 44.00          & 26.00            & 23.50          \\
IAISlarge \citep{ren2021learning} & 42.50        & 19.75         & 16.00            \\
VinVL \citep{thrush2022winoground}          & 37.75       & 17.75         & 14.50          \\
TIFA                                                     & 19.00          & 12.50          & 11.30          \\
CLIP RN50x64                                             & 26.50        & 13.75         & 10.25         \\
OFA Large (f.t. SNLI-VE)                                 & 27.70        & 14.30          & 9.00             \\
COCA ViT-L14 (f.t on COCO)                               & 28.25       & 11.50          & 8.25          \\\midrule
Random Chance \citep{thrush2022winoground}  & 25.00          & 25.00            & 16.67         \\
Humans \citep{thrush2022winoground}         & 89.50        & 88.50          & 85.50          \\ \bottomrule
\end{tabular}
\end{center}
\vspace{-10px}
\end{table*} \begin{figure}[!b]
    \centering
    \begin{tabular}{ccc}
        \subfloat[``the orange lollipop is sad and the red lollipop is surprised'' \newline Q: What is the orange lollipop feeling? A: sad]{\includegraphics[width=0.3\columnwidth,height=2.5cm]{images/ex_41_img_1.png}} &
        \subfloat[``Someone in a blue hat standing on a snowy hill'' \newline Q: What is the person wearing? A: blue hat]{\includegraphics[width=0.3\columnwidth,height=2.5cm]{images/COCO_val2014_000000009727.jpg}} &
        \subfloat[``A black apple and a green backpack'' \newline Q: What color is the apple? A: black]{\includegraphics[width=0.3\columnwidth,height=2.5cm]{images/drawbench_apple.png}} \\
        (1) Winoground & (2) CocoCon & (3) DrawBench
    \end{tabular}
    \caption{Contradicting captions and the question/answer pairs with lower  alignment score, indicating the contradiction reason.}
    \label{fig:show_contradictions}
\end{figure}
 
\paragraph{Winoground Results.} Table \ref{tab:winoground} shows the performance of the different methods on the challenging Winoground dataset, which requires strong visual reasoning and compositional understanding skills. Our zero-shot approach, \VQSQR, achieves state-of-the-art results on this dataset, surpassing other strong baselines, with a group score of 30.5\%. This again indicates that \VQSQR's approach that decomposes the alignment task into multiple question-answer pairs is a promising path for image-text alignment. 


\paragraph{Contradiction Generation.} We assessed the \VQSQR method's capacity to detect image-text contradictions, as shown in \cref{fig:show_contradictions}. Contradiction generation relies on identifying the question-answer pair with the lowest VQA score, signaling the least likely alignment between the image and the text. Three paper authors evaluated whether a particular contradiction (consisting of a question and an answer) accurately represents the primary discrepancy between the image and the text. The majority vote among the authors determined the final outcome, yielding the following accuracy rates: 88\% for Coco-Con, 74\% for DrawBench, and 80\% for Winoground. This indicates that our method is capable of identifying these contradictions by investigating the structure and content of the given caption and image. As a result, our method can achieve strong results, particularly on datasets that require compositional understanding.


\begin{figure}[!t]
    \centering
    \includegraphics[width=\textwidth]{images/t2i_models_drawbench_coco_stats.pdf}\\
\caption{Highly correlated \VQSQR and PaLI scores vs. human rankings of text-to-image models}
\vspace{-20px}
    \label{fig:t2i_comparison}
\end{figure} 








\begin{wraptable}{r}{0.55\textwidth}
\footnotesize
\centering
\caption{Comparison of human-labeled quality scores for top-ranked images with model breakdown}
\label{tab:ranking_generated_images}
\begin{tabular}{@{}llccccc@{}}
\toprule
Dataset   & Model & Random & CLIP & PaLI & VQ \\ \midrule
\multirow{2}{*}{COCO t2i} 
                          & SD 1.4 & 68.6 & 74.6 & 88.2 & 86.4 \\
                          & SD 2.1 & 71.3 & 81.2 & 84.5 & 87.3 \\ \midrule
\multirow{2}{*}{DrawBench}  
                           & SD 1.4 & 66.7 & 77.4 & 77.4 & 87.1 \\
                           & SD 2.1 & 59.0 & 78.0 & 87.0 & 82.0 \\ \bottomrule
\end{tabular}
\end{wraptable}
 
\paragraph{Comparing Generative Models.} \VQSQR's ability to compare between generative models is demonstrated on the results of DrawBench and COCO-t2i, which include generated images from different models, together with human quality ratings.
\cref{fig:t2i_comparison} shows that the \VQSQR and our fine-tuned PaLI ranking correlates very well with human ranking (). In addition, since unlike human annotations, the \VQSQR score is consistent across datasets, it offers a way to evaluate dataset difficulty on an absolute scale. 

\paragraph{Reranking Using Alignment Assessment.} Alignment scores can also be used for reranking candidate generations, on top of evaluation. To demonstrate this, we re-rank the image candidate per prompt in the DrawBench and COCO-t2i datasets. We do so using \VQSQR and CLIP and measure the human-labeled quality of the top-ranked image for each method. The results, presented in \cref{tab:ranking_generated_images}, show that ranking with \VQSQR consistently achieves higher quality scores when compared to ranking with CLIP. One such example is shown in \cref{fig:fig_image_gen_ranking}, where both \VQSQR and our top-performing fine-tuned PaLI model demonstrate superior ranking by placing the brown-and-white cats above the white-only cats. This consistency between \VQSQR and PaLI highlights their alignment evaluation models' potential for enhancing text-to-image systems, which contrasts with the divergent ranking exhibited by CLIP.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{images/image_ranking.pdf}
    \caption{Four COCO-t2i text-to-image model outputs ranked by VQSQR scores, correlating with top PaLI model. Image order and CLIP (RN50x64) similarity scores given, but not aligned with \VQSQR/PaLI ranks.}
    \label{fig:fig_image_gen_ranking}
\end{figure}  


\section{Related Work}
\label{sec:related}
Our work advances research in visual entailment (VE)~\citep{xie2019visual}, visual question answering (VQA)~\citep{antol2015vqa}, text-to-image alignment evaluation, and cross-task consistency for multi-modal models, with a focus on enhancing the semantic understanding of image-caption relationships.

Textual Entailment (TE)~\citep{dagan2010recognizing,SNLI} evaluates the truthfulness of a textual hypothesis given a textual premise, providing a key benchmark for the semantic capabilities of neural network models~\citep{BERT,T5,InstructGPT,chowdhery2022palm}. Recently, TE has been adapted to the multimodal domain as Visual Entailment (VE)~\citep{xie2019visual} to assess the semantic alignment between images and text. Vision-and-language models like CLIP~\citep{radford2021learning}, CoCa~\citep{yu2022coca}, BLIP~\citep{li2022blip}, BLIP2~\citep{li2023blip} and OFA~\citep{wang2022ofa} often act as bag-of-words models, lacking a deep comprehension of language compositionality~\citep{yuksekgonul2022and}. Our approach addresses this by generating multiple questions probing diverse semantic aspects, thereby improving performance on challenging compositional tasks like Winoground~\citep{thrush2022winoground} and unnatural images as in Drawbench~\citep{saharia2022photorealistic}.

Unlike DrawBench~\citep{saharia2022photorealistic} and DALL-Eval~\citep{cho2022dall} which depend on human feedback and operate within a discrete set of alignments, our approach produces automated scores for a broader range of text-image alignments, facilitating efficient evaluation of vision-and-language models. Our approach also surpasses the recently proposed TIFA~\citep{hu2023tifa}, which may be due to employing more question-answer pairs and tailored models for question generation and answering.

Several works have explored cross-task consistency in multi-modal models across various modalities. VQA studies have tackled inconsistencies and enhanced consistency using data augmentation and contrastive loss. NLP researchers have improved consistency across tasks or within a single task by employing counterfactual instances or contrast sets~\citep{dagan2010recognizing,SNLI}. Our research aligns with studies that evaluate natural text and images~\citep{maharana2023exposing}; however, extending the focus to synthetic images and texts, and aligning with synthetic image understanding research~\citep{gokhale2022benchmarking, li2022dall, bitton2023breaking, wu2023better, borji2022generated, stockl2022evaluating}. We introduce two unique approaches to address the complexities of image-text alignment.

Another related effort is PickScore~\cite{kirstain2023pickapic}, which predicts human preferences for image quality and aesthetics by ranking or choosing between two images. In contrast, our methods independently score a single image and focus specifically on image-text alignment.  




\section{Limitations}
\label{sec:limitations}
We recognize that in some cases, making a binary decision for whether a text and an image are aligned may be be difficult, also for humans. To tackle this limitation, we provided human annotators with comprehensive guidelines, which resulted in a high inter-annotator agreement (Fleiss-Kappa score of 0.722 with 80\% of the cases where all annotators agreed on the entailment label). 

Although many images in our datasets were obtained by others and not created by us, we made an effort to ensure that they do not contain harmful or potentially harmful content, such as NSFW or biased imagery. During the annotation process, three individuals examined each image and indicated if it could be considered offensive. Additionally, two of the authors manually reviewed the images for any harmful content. However, we understand that the perception of harmful or offensive content may vary among individuals and may be subject to personal interpretation. 
\section{Conclusion}
\label{sec:conclusion}
We addressed the task of image-text alignment evaluation, which we find very close to the Visual Entailment (VNLI) task. We first introduced the \datasetname benchmark, which covers the mix of real and synthetic text and image pairs in text-to-image and image-to-text generation tasks, and includes challenging cases based on generated contradictions. We then proposed two methods, \VQSQR and end-to-end VNLI, which outperform strong baselines on \datasetname and can serve as a starting point for future research on the task.

In future work, we would like to employ our automatic evaluation models for guiding the training of text-to-image and image-to-text models towards more aligned outputs, following recent trends in text-to-text generation \citep{rashkin-etal-2021-increasing,aharoni2022mface}. For example, such models may be useful either for filtering training examples or as a reward when training models using reinforcement learning.




 
\bibliographystyle{unsrtnat}
\bibliography{references}

\vfill\null
\clearpage
\appendix
\section{Appendix}
\label{sec:appendix}

\subsection{Dataset Supplementary Materials}
\begin{enumerate}
  \item Dataset documentation, metadata, and download instructions: \website.
  \item Intended uses: we hope \datasetname will be used by researchers to evaluate image-text matching models. 
  \item Author statement: We bear all responsibility in case of violation of right in using our benchmark. 
  \item Each dataset's license is described below. Our additional human annotations and geenrated images are licensed under CC-BY 4.0 license \url{https://creativecommons.org/licenses/by/4.0/legalcode}.
  \item Hosting \& preservation: the dataset will be hosted in Huggingface Datasets, accessible and available for open research. 
  \item \datasetname fields are presented in \cref{tab:seetrue_example}.
\end{enumerate}

Additional licensing details: we do publish the datasets we annotated in this work, and do not re-publish the existing SNLI-VE and Winoground datasets. Full licenses: 
\begin{enumerate}
    \item MS COCO \cite{lin2014microsoft}: \url{https://cocodataset.org/#termsofuse}
    \item EditBench \cite{wang2022imagen}: \url{https://research.google/resources/datasets/editbench/}, \url{https://www.apache.org/licenses/LICENSE-2.0}
    \item DrawBench \cite{saharia2022photorealistic}: \url{https://imagen.research.google/}, \url{https://docs.google.com/spreadsheets/d/1y7nAbmR4FREi6npB1u-Bo3GFdwdOPYJc617rBOxIRHY/edit#gid=0}
    \item Pick-a-Pick \cite{kirstain2023pickapic}: \url{https://huggingface.co/datasets/yuvalkirstain/pickapic_v1}
    \item SNLI-VE \cite{xie2019visual}: \url{https://github.com/necla-ml/SNLI-VE}
    \item Winoground \cite{thrush2022winoground}: \url{https://huggingface.co/datasets/facebook/winoground}

\end{enumerate}

\begin{table}[!h]
\caption{\datasetname Rows Examples}
\label{tab:seetrue_example}
\resizebox{\columnwidth}{!}{
\begin{tabular}{@{}lllll@{}}
\toprule
image & text                                                     & label & original\_dataset\_id  & dataset\_source \\ \midrule
img1  & A zebra to the right of a fire hydrant.                  & 0     & text\_133\_image\_1228 & drawbench       \\
img2  & A group of people standing next to bags of luggage.      & 1     & text\_105\_image\_1377 & coco\_t2i       \\
img3  & a tiny figurine is surrounded by cell phones on a table. & 1     & 3786                   & editbench       \\ \bottomrule
\end{tabular}}
\end{table}

\subsection{Human Annotation Process}
\label{sec:human_annotation_appendix}
\begin{figure}[!htb]
    \centering
    \includegraphics[width=\textwidth]{images/fig_mturk_ui.pdf}\\
    \caption{Annotation interface for determining whether a given image-text pair are aligned.}
    \label{fig:mturk_ui}
\end{figure} 
In order to provide reliable human labels for our datasets, we conducted an annotation process using the \datasetname platform. The process comprised several steps, including setting qualification requirements, providing instructions, and evaluating annotator agreement.

We set the basic requirements for our annotation task as follows: a percentage of approved assignments above 98\%, more than 5,000 approved HITs, and annotator locations limited to the US, UK, Australia, or New Zealand. We selected 5 examples from our dataset for a qualification test and screened the annotators' results.
\cref{fig:mturk_ui} displays a sample of the Mechanical Turk user interface. The payment for the crowd-workers was 15-18 USD hourly. 

The instructions provided were as follows:

\textit{Evaluate the given image and text to determine if they match, selecting either “Yes” or “No”. Some images may be synthetically generated by a text-to-image model. To assess the match, mentally generate a textual description for the image (no need to write it down) and compare this generated description to the given text. If the descriptions closely resemble each other, mark “Yes”. If not, mark “No” and provide feedback on the specific issue causing the misalignment, focusing on the primary issue if multiple misalignments are present. If you encounter an image or text that may be offensive due to bias, race, NSFW content, etc., mark the checkbox to indicate this issue.}

Full agreement metrics are presented in \cref{tab:agreement}. As shown in the table, the percentage of cases where all annotators agreed and the Fleiss-Kappa scores vary across the datasets, with COCO-Con exhibiting the highest level of agreement and Drawbench the lowest. These differences highlight the varying levels of complexity within the datasets.

\begin{table}[!h]
\centering
\caption{Agreement metrics for different datasets.}
\label{tab:agreement}
\begin{tabular}{@{}llllll@{}}
\toprule
Dataset       & Full  & Drawbench & COCO t2i & COCO-Con & PickaPic-Con \\ \midrule
\# Items      & 8,527 & 1,968     & 2,586    & 1,992    & 1,981        \\
\% all agreed & 80    & 76        & 78       & 86       & 77           \\
Fless-Kappa   & 0.72  & 0.66      & 0.68     & 0.81     & 0.69         \\ \bottomrule
\end{tabular}
\end{table}

\subsection{\texorpdfstring{Comparing  variants}{Comparing VQ2 variants}}
\label{sec:vq2_ablations}
 consists of two main parts: generating question-answer pairs and assessing question-answer pair alignment against the image. We have experimented with different variants of the  zero-shot method.
\paragraph{Assessing question-answer pair alignment methods} Given a question-answer pair, we would like to assess the question based on the image and compare it to the information in the text.
We experimented with several configurations for answer alignment:

\begin{enumerate}
    \item Type A: Given a question-answer pair  generated from the text, we answer the question using a VQA model and obtain an answer based on the image .
    We compare a pair of  with a pair of  using an Natural Language Inference (NLI) model, where the pair based on the text serves as the premise and the other as the hypothesis. We define the alignment score  as the probability of the NLI model for answering ``entailed''.
    \item Type B: As done in type A, we answer the question using a VQA model and obtain the answer .
    We use the VQA model again to compare the the two answers and determine whether they are the same. The question is formulated as \textit{``Is  in this image?''}. We define the alignment score  as the probability of the VQA model for answering ``yes''.
    \item Type C: We reformulate each question and answer candidate pair  into a new yes-no predicate question  using the format \textit{``is  true for  in this image?''}. The VQA model is then invoked to answer the predicate question  over image . We define the alignment score  as the probability of the model for answering ``yes''.
\end{enumerate}

\paragraph{Generating question-answer pairs} To produce question-answer pairs from the text, we first extract informative spans in the text . 
We extract as answer candidates all named entities and noun phrases in  using spaCy.
We noticed that for short text , this method doesn't produce enough question-answer pairs to assess the alignment between the text and the image. Thus, we extend the answer candidates by adding multi-word spans, such as adjectives ("black and white") and location "in the air". We use the extended answer candidate extraction in all of our experiments.

Table \ref{tab:vq2_ablation} summarize the results of the  variants on the EditBench dataset. Our zero-shot  method is  type C, it outperforms other configurations and it is more efficient, since it requires a single run of the VQA model for the question-answer pair assessment.

\begin{table}[!h]
\centering
\caption{Comparing  configurations on all EditBench categories}
\label{tab:vq2_ablation}
\resizebox{\columnwidth}{!}{
\begin{tabular}{@{}llllllll@{}}
\toprule
Method  & Models used &  \multicolumn{6}{c}{\textbf{EditBench}} \\
& for assessment & Color & Count & Material & Shape & Size & Mean \\ \midrule
 (A) & VQA \& NLI  & 75.7 & 63.6 & 71.9 & 67.6 & 75.9 & 70.9\\
 (B) & VQA \& VQA & \textbf{80.2} & 72.3 & 75.6 & \textbf{73.5} & 75.4 & 75.4 \\
 (C) w.o. multi-word answers & VQA  & 77.2 & 71.5 & 73.6 & 71.9 & 75.5 & 73.9\\
 (C) & VQA & 78.5 & \textbf{73.5} & \textbf{76.9} & 71.7 & \textbf{78.2} & \textbf{75.8} \\
\bottomrule
\end{tabular}}
\end{table}

\subsection{Reproducibility}
\label{sec:reproducibility}
To fine-tune BLIP2, we adjust only the Q-former parameters of the model using the Adam optimizer. We train the model for two epochs and designate 10\% of the training set as a validation set for early stopping and use learning rate selection between \{1e-5, 5e-5\}. A single training took 5 hours on a linux server with one A6000 GPU. All experiments took <2 days.

Zero-shot : For 10,000 text-image pairs, the inference time of every step is as follows.
Answer candidate generation: when using extended answer candidates -- about 1 day. Otherwise, ~12 hours.
Question generation and filtering: When using extended answer candidates, about 2 days, otherwise, 1 day.
The last step only takes a few minutes.


 

\end{document}