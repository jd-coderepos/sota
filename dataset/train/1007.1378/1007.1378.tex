\documentclass[a4paper,10pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{appendix}
\usepackage{color}	
\usepackage{times}
\usepackage{a4wide}
\usepackage{authblk}


\newcommand{\qed}{\hfill\smallskip}
\newcommand\aco[1]{\textcolor{red}{#1}}
\newcommand{\remove}[1]{}



\newtheorem{claim}{Claim}\renewcommand{\theclaim}{\arabic{claim}}

\newtheorem{proposition}{Proposition}\renewcommand{\theproposition}{\arabic{proposition}}


\newtheorem{corollary}{Corollary}\renewcommand{\thecorollary}{\arabic{corollary}}

\newtheorem{definition}{Definition}\renewcommand{\thedefinition}{\arabic{definition}}

\newtheorem{lemma}{Lemma}\renewcommand{\thelemma}{\arabic{lemma}}
	

\newtheorem{theorem}{Theorem}\renewcommand{\thetheorem}{\arabic{theorem}}




\newenvironment{proof}{\noindent{\bf Proof\@:}}{\hfill \\}

\newenvironment{theoremproof}[1]{\noindent{\bf Proof of Theorem #1\@:}}{\hfill \\}
\newenvironment{lemmaproof}[1]{\noindent{\bf Proof of Lemma #1\@}}{\hfill \\}
\newenvironment{claimproof}[1]{\noindent{\bf Proof of Claim #1\@:}}{\hfill \\}
\newenvironment{propositionproof}[1]{\noindent{\bf Proof of Proposition #1\@:}}{\hfill \\}


\newcommand{\myshow}[1]{#1}

\renewcommand{\Pr}[1]{Pr[#1]}


\newcommand\ecorrect{\color{red}{\epsilon_0}}


\newcommand\dist{\mathrm{dist}}



\newcommand\PHI{\vec\Phi} 
\newcommand\SIGMA{\vec\sigma} 

\newcommand\cutnorm[1]{\left\|{#1}\right\|_{\qed}} 
\newcommand\cA{\mathcal{A}} 
\newcommand\cB{\mathcal{B}} 
\newcommand\cC{\mathcal{C}} 
\newcommand\cD{\mathcal{D}} 
\newcommand\cF{\mathcal{F}} 
\newcommand\cG{\mathcal{G}} 
\newcommand\cE{\mathcal{E}} 
\newcommand\cU{\mathcal{U}} 
\newcommand\cN{\mathcal{N}} 
\newcommand\cQ{\mathcal{Q}} 
\newcommand\cH{\mathcal{H}} 
\newcommand\cS{\mathcal{S}} 
\newcommand\cT{\mathcal{T}} 
\newcommand\cI{\mathcal{I}} 
\newcommand\cK{\mathcal{K}} 
\newcommand\cJ{\mathcal{J}} 
\newcommand\cL{\mathcal{L}} 
\newcommand\cM{\mathcal{M}} 
\newcommand\cP{\mathcal{P}} 
\newcommand\cX{\mathcal{X}} 
\newcommand\cY{\mathcal{Y}} 
\newcommand\cV{\mathcal{V}} 
\newcommand\cW{\mathcal{W}} 
\newcommand\cZ{\mathcal{Z}} 

\def\cR{{\mathcal R}}
\def\cC{{\mathcal C}}
\def\cE{{\cal E}}
\def\bC{{\bf C}}
\def\bT{{\bf T}}
\def\bM{{\bf M}}


\newcommand\Fix{\texttt{Fix}} 
 
\newcommand\eul{\mathrm{e}} 
\newcommand\eps{\varepsilon} 
\newcommand\ZZ{\mathbf{Z}} 
\newcommand\Var{\mathrm{Var}} 
\newcommand\Erw{\mathrm{E}} 
\newcommand\pr{\mathrm{P}} 

\newcommand{\matone}{\vec{J}}
\newcommand{\matid}{\vec{E}}

\newcommand{\vecone}{\vec{1}}

\newcommand{\bd}{\bar d}
\newcommand{\Vol}{\mathrm{Vol}}


\newcommand\EE{\mathcal{E}} 
\newcommand{\set}[1]{\left\{#1\right\}} 
\newcommand{\Po}{{\rm Po}} 
\newcommand{\Bin}{{\rm Bin}} 

\newcommand\TV[1]{\left\|{#1}\right\|_{TV}} 
 
\newcommand{\bink}[2] {{{#1}\choose {#2}}} 


\newcommand\alert[1]{\mathbf{#1}} 

\newcommand\ra{\rightarrow} 

\newcommand\bc[1]{\left({#1}\right)} 
\newcommand\cbc[1]{\left\{{#1}\right\}} 
\newcommand\bcfr[2]{\bc{\frac{#1}{#2}}} 
\newcommand{\bck}[1]{\left\langle{#1}\right\rangle} 
\newcommand\brk[1]{\left\lbrack{#1}\right\rbrack} 
\newcommand\scal[2]{\bck{{#1},{#2}}} 
\newcommand\norm[1]{\left\|{#1}\right\|} 
\newcommand\abs[1]{\left|{#1}\right|} 
\newcommand\uppergauss[1]{\left\lceil{#1}\right\rceil} 
\newcommand\lowergauss[1]{\left\lfloor{#1}\right\rfloor}
\newcommand\ug[1]{\left\lceil{#1}\right\rceil} 
\newcommand\RR{\mathbf{R}} 
\newcommand\RRpos{\RR_{\geq0}} 
\newcommand{\Whp}{W.h.p.} 
\newcommand{\whp}{w.h.p.} 
\newcommand{\stacksign}[2]{{\stackrel{\mbox{\scriptsize #1}}{#2}}} 
\newcommand{\tensor}{\otimes}



\newcommand{\Karonski}{Karo\'nski}
\newcommand{\Erdos}{Erd\H{o}s}
\newcommand{\Renyi}{R\'enyi}
\newcommand{\Lovasz}{Lov\'asz}
\newcommand{\Juhasz}{Juh\'asz}
\newcommand{\Bollobas}{Bollob\'as}
\newcommand{\Furedi}{F\"uredi}
\newcommand{\Komlos}{Koml\'os}
\newcommand{\Luczak}{\L uczak}
\newcommand{\Kucera}{Ku\v{c}era}
\newcommand{\Szemeredi}{Szemer\'edi}
\newcommand{\ratio}[2]{\mbox{}}
\newcommand{\bbD}[1]{\bar{{\bf D}}^{(#1)}}
\newcommand{\gap}[1]{\mbox{\hspace{#1 in}}}
\newcommand{\bD}[1]{{\bf D}^{(#1)}}
\newcommand{\hbD}[1]{\hat{{\bf D}}^{(#1)}}
\newcommand{\bTT}[1]{{\bf T}^{(#1)}}
\newcommand{\limninf}{\mbox{}}
\newcommand{\proofstart}{{\bf Proof\hspace{2em}}}
\newcommand{\tset}{\mbox{}}
\newcommand{\proofend}{\hspace*{\fill}\mbox{}}
\newcommand{\bfm}[1]{\mbox{\boldmath }}
\newcommand{\reals}{\mbox{\bfm{R}}}
\newcommand{\expect}{\mbox{\bf E}}
\newcommand{\Exp}{\mbox{\bf E}}
\newcommand{\card}[1]{\mbox{}}
\newcommand{\scaps}[1]{\mbox{\sc #1}}
\newcommand{\rdup}[1]{{\mbox{}}}

\newcommand\Lem{Lemma}
\newcommand\Prop{Proposition}
\newcommand\Thm{Theorem}
\newcommand\Cor{Corollary}
\newcommand\Sec{Section}
\newcommand\Chap{Chapter}
\newcommand\Rem{Remark}





 	
		

\date{\today}

\title{\bf On independent sets in random 
graphs\thanks{Supported by EPSRC grant EP/G039070/2 and DIMAP.} 
\thanks{A preliminary version of this work appeares in the proceedings of
ACM-SIAM SODA 2011}}

\author[1]{Amin Coja-Oghlan}
\author[2]{Charilaos Efthymiou}
\affil[1]{Goethe University, Mathematics Dept., Frankfurt 60054, Germany\\
\texttt{acoghlan@math.uni-frankfurt.de}}
\affil[2]{University of Warwick, Mathematics and Computer Science, Coventry CV4 7AL, UK\\
\texttt{c.efthymiou@warwick.ac.uk}}



	 
\begin{document}
\maketitle
\thispagestyle{empty}




\begin{abstract}
The independence number of a sparse random graph  of average degree  is well-known to be
	 with high probability,
		with  in the limit of large .
Moreover, a trivial greedy algorithm \whp\ finds an independent set of size , i.e.,
about half the maximum size.
Yet in spite of 30 years of extensive research no efficient algorithm has emerged to
produce an independent set with  for any \emph{fixed}  (independent of both  and ).
In this paper we prove that the combinatorial structure of the independent set problem in random
graphs undergoes a phase transition as the size  of the independent sets passes
the point .
Roughly speaking, we prove that independent sets of size  form an 
intricately ragged landscape, in which local search algorithms seem to get stuck.
We illustrate this phenomenon by providing an exponential lower bound for the Metropolis
process, a Markov chain for sampling independents sets.

\medskip
\noindent
{\bf Key words:}
random graphs, independent set problem, Metropolis process, phase transitions.
\end{abstract}

\setcounter{page}{1}
\section{Introduction and Results}
\subsection{Probabilistic analysis  and the independent set problem}

In the early papers on the subject, the motivation for the probabilistic
analysis of algorithms was to alleviate the glum of worst-case
analyses by establishing a brighter `average-case' scenario~\cite{AverAlg1,AverAlg2,AverAlg3}.
This optimism was stirred by early analyses of simple, greedy-type
algorithms, showing that these perform rather well on randomly
generated input instances, at least for certain ranges of the
parameters. Examples of such analyses include Grimmett and
McDiarmid~\cite{grimmett} (independent set problem), Wilf
\cite{Wilf}, Achlioptas and Molloy~\cite{AchListColouring} 
(graph coloring), and Frieze and Suen~\cite{FrSu} (-SAT). Yet, remarkably, in spite of 30
years of research, for many problems no efficient algorithms,
howsoever sophisticated, have been found to outperform those simple
greedy algorithms markedly.


The independent set problem in random graphs  is a case
in point. Recall that  is a graph on  vertices obtained
by choosing  edges uniformly at random (without replacement).
We say that  has a property \emph{with high probability}
if the probability that the property holds tends to 1 as .
One of the earliest results in the theory of random graphs is a
non-constructive argument showing that for 
the independence number of  is 
\whp~\cite{BollobasIS,erdos-1st,matula}. Grimmett and McDiarmid~\cite{grimmett}
analysed a simple algorithm that just constructs an inclusion-maximal
independent set greedily on : it yields an independent set
of size  \whp, about half the maximum size. But no
algorithm is known to produce an independent set of size  for any fixed  in polynomial time with a non-vanishing
probability, neither on the basis of a rigorous analysis, nor on
the basis of experiments or other evidence. In fact, devising such
an algorithm is probably the most prominent open problem in the
algorithmic theory of random graphs~\cite{friezeRSA,KarpOLD}. (However,
note that one can find a maximum independent set \whp\ by trying all
 possible sets of size .)


Matters are no better on sparse random graphs. If we let 
denote the average degree, then non-constructive arguments yield
	
for .
In the case , the proof of this is via a simple
second moment argument~\cite{BollobasIS,matula}. By contrast, for
, the second moment argument breaks down and
additional methods such as large deviations inequalities are 
needed~\cite{frieze-is}. Yet in either case, no algorithm is known
to find an independent set of size  in 
polynomial time with a non-vanishing probability, while `greedy' yields
an independent set of size  \whp\
In the sparse case, the time needed for exhaustive search scales
as , i.e., the complexity grows as
 decreases.


The aim of this paper is to explore the tenacity
of finding large independent sets in random graphs. The focus is on
the sparse case, both conceptually and computationally  the most
difficult case. We exhibit a phase transition in the structure of the
problem that occurs as the size of the independent sets passes the
point  up to which efficient algorithms are
known to succeed. Roughly speaking, we show that independent sets of
sizes bigger than  form an intricately
ragged landscape, which plausibly explains why local-search algorithms get
stuck. Thus, ironically, instead of showing that the `average case'
scenario is brighter, we end up suggesting that random graphs provide
an excellent source of difficult examples. Taking into account the (substantially)
different nature of the independent set problem, our work complements
the results obtained in~\cite{AchCoOg} for random constraint satisfaction
problem such as -SAT or graph coloring.



\subsection{Results}\label{sec:Results}

Throughout the paper we will be dealing with sparse random graphs 
where the average degree  is `large' but remains bounded
as . To formalise this sometimes we work 
with functions  that tend to zero as  gets large.\footnote{The reason why we need to speak about  `large' is
	that 	the sparse random graph  is not connected. This
	implies, for instance, that algorithms can find independent sets
	of size  for some  by optimizing
	carefully over the small tree components of . Our
	results/proofs actually carry over to the case that  tends
	to infinity as  grows, but to keep matters as simple as possible,
	we will	confine ourselves to fixed .}
Thus  and the greedy
algorithm finds independent sets of size  \whp, where . However, no efficient
algorithm is known to find independent sets of size  for any \emph{fixed} .


For a graph  and an integer  we let  denote the set
of all independent sets in  that have size exactly . What we
will show is that in  the set  undergoes a
phase transition as . For two sets  we let  denote the symmetric difference of .
Moreover,  is the Hamming distance of
 viewed as vectors in .


To state the result for  smaller than , we need
the following concept. Let  be a set of subsets of , and
let  be an integer. We say that  is {\em -connected}
if for any two sets  there exist 
such that , , and 
for all . If  is -connected for
some , one can easily define various simple Markov
chains on  that are ergodic.

\begin{theorem}\label{thrm:Connectivity}
There exists  and for any  a number  (independent
of ) such that  is -connected \whp\
for any
	
\end{theorem}


\noindent
The proof of \Thm~\ref{thrm:Connectivity} is `constructive' in the
following sense. Suppose given  we set up an auxiliary
graph whose vertices are the independent sets  with
. In the auxiliary graph
two independent sets  are adjacent if
. Then the proof of \Thm~\ref{thrm:Connectivity}
yields an algorithm  for finding paths of length  between
any two elements of  \whp\ Thus, intuitively \Thm~\ref{thrm:Connectivity}
shows that for  the set
 is easy to `navigate' \whp



By contrast, our next result shows that for 
the set  is not just disconnected \whp, but that
it shatters into exponentially many, exponentially tiny pieces.


\begin{definition}\label{Def_shattering}
We say that  {\bf shatters} if there exist constants
 such that \whp\ the set  admits a
partition into subsets such that
\begin{enumerate}
\item Each subset contains at most 
		independent sets.
\item If  belong to different subsets, then
	.
\end{enumerate}
\end{definition}


\begin{theorem}\label{theorem:shattering}
There is  so that  shatters for all
 with

\end{theorem}

\noindent
\Thm s~\ref{thrm:Connectivity} and~\ref{theorem:shattering} deal
with the geometry of a single `layer'  of independents
of a specific size. The following two results explore if/how a 
`typical' independent set in  can be extended 
to a larger one. To formalize the notion of `typical', we let
 signify the set of all pairs , where
 is a graph on  with  edges and
. Let  be the probability
distribution on   induced  by the following 
experiment.
\begin{quote}
Choose a graph  at random.\\
If ,
choose an independent set  uniformly at random
and output .
\end{quote}
We say a pair  chosen from the distribution 
has a property  \emph{with high probability} if the probability
of the event  tends to one as .



\begin{definition}
Let , let  be a graph, and let  be
an independent set of . We say that  is {\bf -expandable} if  has an independent set  such that 
 and
.
\end{definition}


\begin{theorem}\label{theorem:non-maximal}
There are  such that for any 
the following is true. For 	 a
pair  chosen from the distribution  is
-expandable \whp
\end{theorem}


\noindent
\Thm~\ref{theorem:non-maximal} shows that  \whp\ in a random graph
 almost all independent sets of size  are contained in \emph{some} bigger independent set of size
. That is, they can be expanded beyond
the critical size  where shattering occurs.
However, as  approaches the critical size ,
i.e., as , the typical potential for expansion diminishes.


\begin{theorem}\label{theorem:maximal}
There is  such that for any  satisfying 
and 	 \whp\ a pair 
chosen from the distribution  is not -expandable
for any  and 

\end{theorem}

\noindent
In other words, \Thm~\ref{theorem:maximal} shows that for , a typical 
cannot be expanded to an independent set of size ,
 without first \emph{reducing} its size below 
 (However,  a random independent set of
size  is typically not inclusion-maximal
because, for instance, it is unlikely to contain \emph{all} isolated
vertices of the random graph .)


Metaphorically, the above results show that \whp\ the independent sets of  form 
a ragged mountain range.
 Beyond the `plateau level'
 there is an abundance of smaller `peaks',
i.e., independent sets of sizes  for any ,
almost all of which are
not expandable (by much).

The algorithmic equivalent of a mountaineer aiming to ascend
to the highest summit is a Markov chain
called the \emph{Metropolis process}, \cite{KirkpatricMetropolis,Metropolis1st}.
For a given graph  its state space is the set of all independent
sets of . Let  be the state at time . In step , the
chain chooses a vertex  of  uniformly at random. If ,
then with probability  the next state is , and with probability  we let , where  is a `temperature' parameter. If  (with  the neighbourhood of ), then
. Finally, if , then .
It is well know that the probability of an independent set  of  in the stationary
distribution equals , where

is the partition function.
Hence, the larger , the higher the mass of large independent
sets. Let

denote the average size of an independent set of  under the
stationary distribution.


It is easy to see that every state in  
communicates with every other in the Metropolis process.
Thus the process is ergodic and possesses a unique stationary 
distribution. Let  denote the stationary 
distribution of the Metropolis process with parameter , 
for some . It is well known 
that   
where  (e.g. \cite{jerrum-planted}).  


Here, we are interested in finding the rate at which the Metropolis
process converges to its equilibrium. There are a number of ways
of quantifying the closeness to stationarity. Let  denote the distribution of the state at time 
given that  was the initial state. The {\em total variation
distance} at time  with respect to the initial state 
is 

Starting from , the rate of convergence to stationarity may then be measured by the function


\noindent
The {\bf mixing time} of the Metropolis process  is defined as



Our above results on the structure of the sets 
imply that \whp\ the mixing time of the Metropolis process is
exponential if the parameter  is tuned so that the
Metropolis process tries to ascend to independent sets bigger
than .

\begin{corollary}\label{cor:MixingTimeBound}
There is  such that for  with

the mixing time of the Metropolis process on  is
 \whp
\end{corollary}




\subsection{Related work}
To our knowledge, the connection between transitions in the geometry
of the `solution space' (in our case, the set of all independent sets of a given size)
and the  apparent failure of {\em local algorithms}
in finding a solution has been pointed first out in the statistical mechanics 
literature~\cite{FuAnderson,MPZ-Science,1RSBPaper}.
In that work, which mostly deals with CSPs such as -SAT,
the shattering phenomenon goes by the name of `dynamic replica symmetry breaking.'
Our present work is clearly inspired by the statistical mechanics ideas,
although we are unaware of explicit contributions from that line of work addressing the
independent set problem in the case of random graphs with average degree .
Generally, the statistical mechanics work is based on deep, insightful, but, alas, 
mathematically non-rigorous techniques.



In the case that the average degree  satisfies ,
the independent set problem in random graphs is conceptually
somewhat simpler than in the case of .
The reason for this is that for 
the second moment method can be used to show that the \emph{number}
of independent sets is concentrated about its mean. As we will see
in \Cor~\ref{cor:transfer-theorem} below, this is actually untrue for
sparse random graphs.


The results of the present paper extend the main results from Achlioptas
and Coja-Oghlan~\cite{AchCoOg}, which dealt with constraint satisfaction
problems such as -SAT or graph coloring, to the independent set
problem. This requires new ideas, because the natural questions are
somewhat different (for instance, the concept of `expandability'
has no counterpiece in CSPs). Furthermore, in~\cite{AchCoOg} we
conjectured but did not manage to prove the counterpiece of
\Thm~\ref{thrm:Connectivity} on the connectivity of .
On a technical level, we owe to~\cite{AchCoOg} the idea of analysing
the distribution  via a different distribution ,
the so-called `planted model' (see \Sec~\ref{sec:PlantedModel}
for details). However, the proof that this approximation is indeed
valid (\Thm~\ref{thrm:transfer-theorem} below) requires a rather
different approach. In~\cite{AchCoOg} we derived the corresponding
result from the second moment method in combination with sharp
threshold results. By contrast, here we use an indirect approach
that reduces the problem of estimating the number 
of independent sets of a given size to the problem of (very accurately)
estimating the independence number . Indeed, the
argument used here carries over to other problems, particularly
random -SAT, for which it yields a conceptually simpler proof
than given in~\cite{AchCoOg} (details omitted).

Subsequently to~\cite{AchCoOg}, it was shown in~\cite{MRT} that
in many random CSPs the threshold for the shattering of the solution
space into exponentially small components coincides asymptotically
with the \emph{reconstruction threshold}. Roughly speaking, the
reconstruction threshold marks the onset of long-range correlations
in the Gibbs measure. More precisely, it is shown in~\cite{MRT} that
for a class of `symmetric' random CSPs the reconstruction threshold
derives from the corresponding threshold on random trees, and that
it happens to coincide with the shattering threshold. Our
\Thm~\ref{theorem:shattering} determines the threshold for shattering
in the independent set problem in random graphs. Furthermore,
Bhatnagar, Sly, and Tetali~\cite{BST10} recently studied the
reconstruction problem for the independent set problem on -regular
trees. It would be most interesting to obtain a result
similar to~\cite{MRT}, namely that the reconstruction threshold
on the  random graph is given by the reconstruction threshold
on trees and that it coincides with the shattering threshold from
\Thm~\ref{theorem:shattering}.



The work that is perhaps most closely related to ours is  a remarkable
paper of Jerrum~\cite{jerrum-planted}, who studied the Metropolis
process on random graphs  with average degree .
The main result is that \whp\  \emph{there exists} an initial
state from which the expected time for the Metropolis process to
find an independent set of size  is
superpolynomial.
This is quite a non-trivial achievement, as it is a result about the
\emph{initial} steps of the process where the states might potentially
follow a very different distribution than the stationary distribution.
The proof of this fact is via a concept called `gateways', which is
somewhat reminiscent of the expandability property in the present work.
However, Jerrum's proof hinges upon the fact that the number of
independent sets of size  is
concentrated about its mean. The techniques from the present work
(particularly \Thm~\ref{thrm:transfer-theorem} below) can be used
to extend Jerrum's result to the sparse case quite easily, showing
that the expected time until a large independent set is found is fully
exponential in  \whp\ Yet as also pointed out in~\cite{jerrum-planted},
an unsatisfactory aspect of this type of result is that it only shows that
\emph{there exists} a `bad' initial state, 	while it seems natural to
conjecture that indeed most specific initial states (such as the empty set)
are `bad'. Since we are currently unable to establish such a
stronger statement, we will confine ourselves to proving an exponential
lower bound on the mixing time (\Cor~\ref{cor:MixingTimeBound}).



For \emph{extremely} sparse random graphs, namely ,
finding a maximum independent set in  is easy. More
specifically, the greedy matching algorithm of Karp and
Sipser~\cite{KarpSipser} can easily be adapted so that it
yields a maximum independent set \whp\ But this approach does
not generalize to average degrees  (see, however,
\cite{Gamarnik} for a particular type of weighted independent
sets).



Recently Rossman~\cite{monotone-circuit} obtained a monotone circuit
lower bound for the clique problem on random graphs that is exponential
in the size of the clique. The setup of~\cite{monotone-circuit} is
somewhat orthogonal to our contribution, as we are concerned with the
case that the size of the desired object (i.e., the independent set)
is linear in the number of vertices, while~\cite{monotone-circuit}
deals with the case that the size of the clique is  in terms
of the order of the graph. Nevertheless, the punchline of viewing
random graphs as a potential source of hard problem is similar.


In the course of the analysis in this paper we need a lower bound
on  which is bigger what is calculated in \cite{frieze-is}.
For this reason, in \cite{arxivTR}, a previous version of this
work, we improved slightly on the value of . The
analysis is similar to that in \cite{frieze-is}, i.e. combine
vanilla second moment with Talagrand's inequality. 
A bit later our result was improved even more by Dani and Moore
\cite{W2ndM}. Raughly speaking, the authors show that a 
of expected degree 
has an independent set of size  w.h.p.
In comparison to \cite{W2ndM}, our bound on  in \cite{arxivTR}
is . To absolve
our work from the tendious second moment calculations we make direct
use of the result \cite{W2ndM}.




\subsection{Organisation of the paper}


The remaining material of this work is organised as follows:
For completeness, in Section \ref{sec:Prelim} we provide some 
very elementary results, which are either known  or easy to
derive. 
In Section \ref{sec:PlantedModel} we analyse the so-called `planted
model' to approximate the distribution . 
Then in Section \ref{section:thrm:Connectivity} we show Theorem
\ref{thrm:Connectivity}. In Section \ref{sec:theorem:shattering}
we show Theorem \ref{theorem:shattering}. In Section \ref{section:theorem:non-maximal}
we show Theorem \ref{theorem:non-maximal}. In Section
\ref{section:theorem:maximal} we show  Theorem \ref{theorem:maximal}.
In Section \ref{sec:cor:MixingTimeBound} we show Corollary \ref{cor:MixingTimeBound}.






\section{Preliminaries and notation}\label{sec:Prelim}


\noindent
We will need the following Chernoff bounds on the tails of a sum
of independent Bernoulli variables~\cite{MotwRandAlgBook}.

\begin{theorem}\label{chernoffbounds}
Let  be independent Bernoulli variables.
Let  and .
Then

\end{theorem}


Let  be random graph on  vertices obtained as follows:
choose  pairs of vertices independently out of all  possible
pairs; insert the  edges induced by these pairs, omitting
self-loops and replacing multiple edges by single edges. For technical
reasons it will sometimes be easier to first work with 
and  then transfer the results to . The two distributions
are related as follows.


\begin{lemma}\label{lemma:model-equivalence}
For any fixed  and  we have

\end{lemma}
\begin{proof}
This is a standard counting argument. The random graph  is
obtained by choosing one of the  possible sequences of vertex
pairs uniformly at random. Out of these  sequences, precisely
 sequences induce simple graphs with  edges (where
 denotes the falling factorial). Indeed, each of the
 simple graph with  edges can be turned into
a sequence of pairs by ordering the edges arbitrarily (a factor ),
and then choosing for each edge in which order its vertices appear in
the sequence (a factor ). Hence, letting  denote the event
that  is a simple graph with  edges, we see that
	
Furthermore, given that the event  occurs,  is
just a uniformly distributed (simple) graph with  edges. 
Therefore,  (\ref{eqGnm*}) yields
	
as claimed.
\end{proof}


\begin{corollary}\label{corollary:expectation-equivalence}
Suppose that  for a fixed .
For a graph  let .
Then for any  we have

\end{corollary}
\begin{proof}
Let  be a set of size , and let  if  is
independent in , and set  otherwise. The total number
of sequences of  vertex pairs such that  is an independent set
in the corresponding graph  equals
	 (just avoid the  pairs of vertices in ).
Hence,
	
Combining~(\ref{eqexpequi1}) with (\ref{eqexpequi2}) and using
 as , we obtain
	
Hence, by the linearity of expectation,
	
Taking logarithms and recalling that  completes the proof.
\end{proof}

\noindent
Finally we present a lemma that it will be very useful 
in the course of this paper.


\begin{lemma}[Expectation.]\label{lemma:expectation}
Let  for a real .
Let  and set
	
If  is the number of independent sets of size   in , then
	
for  as .
\end{lemma}
\begin{proof}
Since  is obtained by choosing  independent pairs of
vertices, we have

Let .
By Stirling's formula and the fact that for  it holds that
 for  some , we get
that

where .
As , we obtain

Note that both  tend to zero with .
Combining~(\ref{eqexpcomput1}) and~(\ref{eqexpcomput2}) 
yields the assertion.
\end{proof}

\noindent
We also need the following theorem from Dani and Moore~\cite{W2ndM} on
the independence number of .
\begin{theorem}\label{lemma:SMBound}
There is a constant  such that
for any  and any  the following is true.
Suppose that

and let .
Then

\whp
\end{theorem}

\noindent
{\bf Remark.}
In a previous version of this work \cite{arxivTR} we derived a slightly 
weaker bound on , i.e. . 
As opposed to the weighted second moment in \cite{W2ndM}, 
our approach is based on  ``vanilla'' second moment calculations and the
use of a Talagrand type inequality, i.e. similar to that in \cite{frieze-is}.
\\ \vspace{-.3cm}

\noindent
From \cite{W2ndM} we, also, have the following corollary.

\begin{corollary}\label{cor:SMBoundRev}
Let  denote the largest positive root  of the equation
. W.h.p. it holds that

for any constant . Expanding  asymptotically in  
we have that

\end{corollary}

\noindent
It is well known that the independence number  of
the random graph is tightly concentrated. More precisely, the following 
lower tail  bound follows from a standard application of Talagrand's
large deviations inequality~\cite{TalagrandIneq}, similar to the
one used in~\cite[\Sec~7.1]{janson} to establish concentration for
.

\begin{theorem}\label{thrm:TalagrandTailBound}
Suppose that  are as in Theorem \ref{lemma:SMBound}. 
Then for  and for any positive integer  
it holds that
	
\end{theorem}
\begin{proof}
Consider the graph  where  and let 
denote the number of its edges. It holds that

From the above derivations and Theorem \ref{lemma:SMBound}, 
it is direct that

A vertex exposure argument allows to apply Talagrand's large 
deviation inequality for the independence number of 
(in the form that appears in \cite{janson}, page 41 (2.39)).
The following holds:

Using (\ref{eq:GnpLB}) we get

Working as in (\ref{eq:GnpLB}) we get that

The theorem follows.
\end{proof}


\begin{corollary}\label{theorem:Reverse-Frieze}
For the integer  let
	
There is a constant  such that for  and 
of expected degree  it holds that

Also, for  it holds that
.
\end{corollary}
\begin{proof}
Let  be of expected degree ,
where  is as in the statement. Also, let  be such that
. By Theorem \ref{thrm:TalagrandTailBound}
we have that

where the last inequality follows from the fact that . The
tail bound in (\ref{eq:tail4aG}) will follow by bounding appropriately
. We bound  by using the fact that

Set  and . Let  be the difference of the l.h.s.
minus r.h.s. in the above equality, written in terms of .
Clearly, it holds that that . That is

For , it is direct to verify that for 
 and sufficiently small  
it holds that . Furthermore, it is easy to see that

For any  and sufficiently small  we have that
. This yields to the fact
that for any  and sufficiently small
 we have . Thus, we get that .
Plugging this  into (\ref{eq:talagrand1821}) we get that

which implies (\ref{eq:tail4aG}).

For the rest of the proof, consider  with expected degree
.  Assume that we add to  edges at random so
as to increase the expected degree to  
and get the graph .
That is, we need to insert into  as many as 
random edges. Therefore, each independent set of size  in
 is also an independent set of  with probability
. Let . It is direct
that 

Using Corollary \ref{corollary:expectation-equivalence} we get that

Furthermore, using the fact that ,
for , it is direct that

Combining (\ref{eq:Relation1896A}), (\ref{eq:Relation1896B}) and
(\ref{eq:Relation1896C}), we get that 

The upper bound for  follows by using the
above inequality and noting that , i.e.
.
\end{proof}



\begin{corollary}\label{cor:ExistenceGnm}
For the graph  of expected degree  it holds that

where  as  increases.
\end{corollary}
\begin{proof}
Consider  of expected degree  and let  be such that
, 
where  is defined in the statement of Corollary \ref{cor:SMBoundRev}. 
Using Corollary \ref{cor:SMBoundRev} and Theorem \ref{thrm:TalagrandTailBound}, 
we get that

The corollary follows by using Lemma \ref{lemma:model-equivalence}.
\end{proof}


\noindent
The following is taken from \cite[p.~156]{janson}.

\begin{lemma}\label{Lemma_isoGnm}
Let  be fixed and .
Let  be the number of isolated vertices in .
Then  \whp
\end{lemma}





\section{Approaching the distribution }\label{sec:PlantedModel}


\subsection{The planted model}

The main results of this paper deal with properties of `typical'
independents sets of a given size in a random graph, i.e., the
probability distribution . In the theory of random
discrete structures often the conceptual difficulty of analysing
a probability distribution is closely linked to the computational
difficulty of sampling from that distribution (e.g., \cite[Chapter~9]{janson}).
This could suggest that analysing  is a formidable task,
because for  there is no efficient procedure known
for finding an independent set of size  in a random graph ,
let alone for sampling one at random. In effect, we do not know of
an efficient method for sampling from .

To get around this problem, we are going to `approximate' the
distribution  by another distribution 
on the set  of graph/independent set pairs, the
so-called planted model, which is easy to sample from. This
distribution is induced by the following experiment:
\begin{quote}
Choose a subset  of size  uniformly at random.\\
Choose a graph  with  edges \emph{in which  is an
 independent set} uniformly at random.\\
Output the pair .
\end{quote}
In other words, the probability assigned to a given pair  is
	
i.e.,  is nothing but the uniform distribution on .
The key result that allows us to study the distribution  is the following.

\begin{theorem}\label{thrm:transfer-theorem}
There is  such that  for  the following is true.
If  is an event such that
	
then .
\end{theorem}
Hence, \Thm~\ref{thrm:transfer-theorem} allows us to bound the 
probability of some `bad' event  in the distribution 
by bounding its probability in the distribution .

To establish \Thm~\ref{thrm:transfer-theorem}, we need to find
a way to compare  and . Suppose that
 is such that  \whp\
Then the probability of a pair 
under the distribution  is

(because we first choose a graph uniformly, and then an independent
set of that graph). Hence, the probabilities assigned to 
under~(\ref{eqProbUniform}) and~(\ref{eqProbPlanted}) coincide
(asymptotically) iff

A moment's reflection shows that the expression on the r.h.s.
of~(\ref{eqProbCoincide}) is precisely the \emph{expected}
number  of independent sets of size .
Thus,  and  coincide asymptotically iff
the number  of independents sets of size  is
concentrated about its expectation.


This is indeed the case in `dense' random graphs with .
For this regime one can perform a `second moment' computation to show
that  \whp, (e.g. see \cite[Chapter~7]{janson}) 
whence the measures  and  are interchangeable.
This fact forms (somewhat implicitly) the foundation of the proofs
in~\cite{jerrum-planted}.



By contrast, in the sparse case  a straight second
moment argument fails utterly. As it turns out, this is because
the quantity  simply it not concentrated about
its expectation anymore. In fact, maybe somewhat surprisingly
\Thm~\ref{thrm:transfer-theorem} can be used to infer the following
corollary, which shows that in sparse random graphs the expectation
 `overestimates' the typical number of independent
sets by an exponential factor \whp\

\begin{corollary}\label{cor:transfer-theorem}
There exist functions  and  such that for
 we have

\end{corollary}
The proof of Corollary \ref{cor:transfer-theorem} appears in Section
\ref{sec:cor:transfer-theorem}.


Conversely, in order to prove \Thm~\ref{thrm:transfer-theorem} we
need to bound the `gap' between the typical value of 
and its expectation from above. This estimate can be summarized as
follows.


\begin{proposition}\label{Lemma_gap}
There is  such that for  we have

with probability at least .
\end{proposition}
Before we prove \Prop~\ref{Lemma_gap} in \Sec~\ref{section:concentration},
let us indicate how it implies \Thm~\ref{thrm:transfer-theorem}.


\begin{corollary}\label{Cor_quantexchange}
There is  such that for  the following is true.
Let

Then , and for any event  we have
	
\end{corollary}
\begin{proof}
\Prop~\ref{Lemma_gap} directly implies that
	
Furthermore, by the definition~(\ref{eqProbUniform}) of the uniform distribution, 
	
The assertion is immediate from~(\ref{eqtransfer-theoremproof1}) and~(\ref{eqtransfer-theoremproof2}).
\end{proof}

\smallskip
\begin{theoremproof}{\ref{thrm:transfer-theorem}}
The theorem follows directly from \Cor~\ref{Cor_quantexchange}.
\end{theoremproof}


\subsection{Proof of Proposition \ref{Lemma_gap}}\label{section:concentration}

Since the second moment method fails to yield a lower bound on
the typical number  of independent sets , we
need to invent a less direct approach to prove Proposition \ref{Lemma_gap}.
Of course, the demise of the second moment argument also presented
an obstacle to Frieze~\cite{frieze-is} in his proof that
		
However, unlike the \emph{number}  of independent
sets , the \emph{size}  of the largest one actually
is concentrated about its expectation. In fact, an arsenal of large
deviations inequalities applies (e.g., Azuma's and Talagrand's
inequality), and~\cite{frieze-is} uses these to bridge the gap
left by the second moment argument.Unfortunately, these large
deviations inequalities draw a blank on . Therefore,
we are going to derive the desired lower bound on 
directly from~(\ref{eqAlan}).


To simplify our derivations we consider the model of random graphs
 and we show the following proposition.
\begin{proposition}\label{proposition:star-conctration}
There is  such that for  we 
have


with probability at least .
\end{proposition}
Then, Proposition \ref{Lemma_gap} follows by Lemmas~\ref{lemma:model-equivalence}
and~\ref{corollary:expectation-equivalence}.


Given some integer  and , let 	
and let 		
In words,  is the largest number of edges that we can squeeze
in while keeping the probability that   has an independent
set of size  above . The following lemma summarizes the key
step of our proof of \Prop~\ref{proposition:star-conctration}. The
idea is that \Lem~\ref{lemma:concentration-1} gives a tradeoff
between the \emph{likely} number of independent set of size 
in the random graph with  edges and the \emph{expected}
number of such independent sets in the random graph with 
edges.

\begin{lemma}\label{lemma:concentration-1}
Suppose that  are such that . Then

\end{lemma}
\begin{proof}
Let . The random graph  is obtained by choosing
 pairs of vertices independently	and inserting the corresponding
edges (while omitting loops and reducing multiple edges to single edges).
Let us think of the  pairs as being generated in two rounds.
In the first round, we generate  pairs, which induce the
random graph . In the second round, we choose a
further  pairs independently and add the corresponding edges
to  (again, omitting self-loops and reducing multiple edges
to single edges) to obtain .

By the linearity of the expectation and because the  (resp.\ )
pairs that the random graph  (resp.\ ) consists of are
chosen independently, we have  (cf.~(\ref{eqexpgeneral}))
	
Furthermore, with respect to the 	number of independent sets of
size  in  \emph{given} their number in the outcome 
of the `first round', we have
	
Indeed, for each independent set  of size  in  each of
the  additional random pairs has its two vertices in  with
probability . Hence, (\ref{eqtrajectory1}) follows because
these  pairs are independent and by the linearity of the expectation.

\noindent
Now, let  be the event that
	
Then by and Markov's inequality and~(\ref{eqtrajectory1}),
	
whence
	
Combining~(\ref{eqtrajectory2}) and~(\ref{eqtrajectory3}), we see that
	
as claimed.
\end{proof}


\noindent{\bf Proof of \Prop~\ref{proposition:star-conctration}.}
Consider  of expected degree  and let . We are going to 
show that (\ref{eq:prop:star-conct}) holds for  and
 with probability at least .

Consider, now, the graph  of expected degree 
, where .
According to \ref{theorem:Reverse-Frieze} it holds that 

and
.

The proposition will follow by just showing that ,
i.e. , and using Lemma \ref{lemma:concentration-1}.
Note, first, that

Using the above, it is elementary to derive that .
Then, it follows that  as promised.
\qed




\subsection{Proof of Corollary \ref{cor:transfer-theorem}}\label{sec:cor:transfer-theorem}

In this section we keep the assumptions of \Cor~\ref{cor:transfer-theorem},
i.e., we let  be such that , with
 sufficiently slowly in the limit of large .



\begin{lemma}\label{Lemma_isoPnm}
There is a number  such that the following is true. Let
 be a pair chosen from the distribution .
Let  be the number of isolated vertices in . Then

\end{lemma}
\begin{proof}
Let . It is convenient to first consider the following
variant of the planted distribution: given a set 
of size , let  be the random graph obtained by including each
of the  possible edges that do not link two
vertices in  with probability

independently. Hence, the total number of edges in  is binomially
distributed with mean . By Stirling's formula, the event 
that  has precisely  edges has probability ,
and given that  occurs, the pair  has the same
distribution as the pair  chosen from the distribution
. Therefore, for any event  we have 



\noindent
Now, consider the number  of vertices in  that are
isolated in . Since each possible edge is present in 
with probability  independently, the degree of each vertex
 has a binomial distribution  with mean

In particular, for each  we have

Furthermore, because  is an independent set, the degrees
of the vertices in  are mutually independent. Hence, 
has a binomial distribution 
with mean

provided that  is sufficiently large. Since  is binomially 
distributed, Chernoff bounds yield a number  such
that 

Finally, combining~(\ref{eqiso2}) and~(\ref{eqG'sigma}), we obtain

as claimed.
\end{proof}



\noindent{\bf Proof of Corollary \ref{cor:transfer-theorem}.}
Let  be the set of all pairs 
such that  has fewer than  isolated vertices. \Lem s~\ref{Lemma_isoGnm} and~\ref{Lemma_isoPnm} entail that 

Since  is the uniform distribution over ,
(\ref{eqcor:transfer-theorem1}) implies that 

Now, let  be the set of all pairs
 such that 	,
and assume for contradiction that there is a fixed  such
that . Then~(\ref{eqcor:transfer-theorem1})
implies that

Therefore,

which contradicts~(\ref{eqcor:transfer-theorem2}). Hence, ,
as claimed.
\qed








\section{Proof of Theorem \ref{thrm:Connectivity}}\label{section:thrm:Connectivity}


Instead of the random graph model   we consider the model , 
where  for fixed real  and we prove the following theorem.

\begin{theorem}\label{thrm:ConnectivityGnp}
There is  such that  is -connected 
for any , with probability at least 
.
\end{theorem}

\noindent
Theorem \ref{thrm:Connectivity} follows by using standard arguments, i.e. the following corollary.
\begin{corollary}
For any fixed ,  and any graph property  it holds that
.
\end{corollary}  
\begin{proof}
Let  be the number of edges in . It holds that

 is binomially distributed with parameters  and .
Straightforward calculations yield to that .
The corollary follows.
\end{proof}



\begin{figure}
\begin{minipage}{0.5\textwidth}
	\centering
		\includegraphics[width=0.4\textwidth]{./chain}
	\caption{ The short chains}
	\label{fig:Chains}
\end{minipage}
\begin{minipage}{0.5\textwidth}
	\centering
		\includegraphics[width=0.75\textwidth]{./G-Property}
	\caption{  with Property  }
	\label{fig:G-Property}
\end{minipage}
\end{figure}


\noindent
{\bf Remark.} 
We show Theorem \ref{thrm:ConnectivityGnp} by just considering 
the adjacent independent sets with Hamming distance at most .
\\ \vspace{-.3cm}



\noindent
For every vertex  in  we let   (or )
denote the  set vertices which are  adjacent to .
A sufficient condition for establishing the connectivity of
 is requiring this space to have what
we call  Property :\\ \vspace{-.3cm}

\noindent
{\bf Property .}
For any two   there 
exist chains  and  of 
independent sets in  connected as in Figure \ref{fig:Chains}.
Furthermore, we have that 
and . In particular it holds
that .\\ \vspace{-.3cm}

\noindent
The following result is straightforward.

\begin{corollary}\label{cor:GPropertyToConnect}
If  has Property , then it is connected.
\end{corollary}



\noindent
Using Corollary \ref{cor:GPropertyToConnect}, Theorem \ref{thrm:ConnectivityGnp} 
will follow by showing that with probability  the set  
has Property , for  .
For this, we need to introduce the notion of ``augmenting vertex''.



\begin{definition}[Augmenting vertex]
For the pair  the vertex
 is {\em augmenting} if
one of the following ,  holds.
\begin{description}
	\item[A.] 
	\item[B.]  and there are
	 {\em terminal sets}  and  of size at most  such that
	\begin{itemize}
		\item  is an independent set of 
		\item   
		\item  it holds that  and 
	\end{itemize}
	The corresponding conditions should hold for , as well.
\end{description}
\end{definition} 

\noindent
Figure \ref{fig:G-Property} shows an example of a pair of independent
sets where the vertex  is an {\em augmenting vertex}. 


We will show that for a pair 
that has an {\em augmenting vertex } we can find short chains
  and  . That is,
if we can find an augmenting vertex for any two members of ,
then  has Property .


First, let us show how  we can create  short chains as in
Figure \ref{fig:Chains} for two independent sets 
with augmenting vertex . For this, we introduce a process
called {\em Collider}.  This process takes as an input , 
  and the augmenting vertex   and returns the independent
sets  and  of the chains. \\ \vspace{-.3cm}

\noindent
{\bf Collider :} 
\\ \vspace{-.3cm}\\
{\em  Phase 1.} \hspace*{0.5cm} /*Creation of  and .*/ \vspace{-.1cm} 
\begin{enumerate}
\item Derive  from  by removing the all its vertices in 
and by inserting .
	\item Do the same for . 
\end{enumerate}
{\em  Phase 2.} \hspace*{0.5cm} /* Creation of  and */. \vspace{-.1cm}
\begin{enumerate}
	\item  is derived from  by deleting one  (any) vertex
	from .
	\item  is derived from  by deleting one  (any) vertex 
	from .
\end{enumerate}
 {\em Return}   and .\\ \vspace{-.3cm}\\
{\bf End}\\ \vspace{-.3cm}

\noindent
Figure \ref{fig:Phase1} shows the changes that have taken place 
to the independent sets in Figure \ref{fig:G-Property} at the 
end of ``Phase 1''. Note that after Phase 1 both  contain the 
augmenting vertex , i.e. the overlap has increased as 
.
\begin{figure}
\begin{minipage}{0.5\textwidth}
	\centering
		\includegraphics[width=0.6\textwidth]{./Phase1}
	\caption{The independent sets , . }
	\label{fig:Phase1}
\end{minipage}
\begin{minipage}{0.5\textwidth}
	\centering
		\includegraphics[width=0.6\textwidth]{./finalstep}
	\caption{Final sets }
	\label{fig:finalstep}
\end{minipage}
\end{figure}
After ``Phase 2'', the independent sets in  Figure \ref{fig:Phase1}
are transformed to those in Figure \ref{fig:finalstep}. There the
vertices  and  are removed from  and ,
correspondingly.

In the following lemma we show that {\em Collider} has all the
desired properties we promise above.


\begin{lemma}\label{lemma:TrnsfrmProperties}
Let  with augmenting vertex .
Let  and  be the two sets of vertices that are 
returned from {\em Collider()} .
The two sets have the following properties:
\begin{enumerate}
	\item ,
	\item ,
	\item There are  
	such that  (resp. ) is adjacent to both 
	 and  (resp.  and ).
\end{enumerate}
\end{lemma}
\begin{proof}
First we show that  and , as returned by {\em Collider
}, are independent sets. The same arguments
apply to both  and . For this reason we only consider 
the case of , the other case would then be obvious.


Let  be an augmenting vertex for the pair , .
Assume that , at the end of the process, is not 
an independent set, i.e. there is an edge between
two vertices in . Clearly, this edge must be either 
between two new vertices, i.e. , or
between some  newly inserted vertex  and an old one.


The first case cannot be true since the assumption that 
is an augmenting vertex implies 
is an independent set. As far as the second case is considered note that both 
and  have the same neighbours in . During the
process {\em Collider} all the vertices in 
 that are adjacent to  and  are removed
(Phase 1, step 1).  The second case cannot occur either. 
Thus  and  are independent sets.


For showing Property 1 it suffices to show that .
This is straightforward by just counting how many vertices are inserted
into  (resp. ) and how many are removed. Property 2
follows by noting that .
Property 3 follows directly by noting that  and
 are at most .
\end{proof}


\noindent
Since for every pair  with
augmenting vertex we can construct short chains as in Figure
\ref{fig:Chains} by using Collider, we have the following corollary:


\begin{corollary}\label{cor:Augment2GProperty}
If for any two  there is an
{\em augmenting} vertex , then  has Property
.
\end{corollary}


We are going to use the first moment method to show that with
probability , the graph  has no pair of
independent sets in  with no augmenting
vertex. According to Corollary \ref{cor:Augment2GProperty}, this
implies that with probability  the set  has Property . Then, Theorem
\ref{thrm:ConnectivityGnp} follows from Corollary \ref{cor:GPropertyToConnect}.



We compute, first,  the probability for a pair in 
to have an augmenting vertex.


\begin{proposition}\label{lemma:PropertyGVertex}
For some integers , consider , two sets of
vertices each of size  such that . Let
 denote  conditional that each of
 is an independent set. Also, let   be
the probability that the pair  has an {\em augmenting
vertex} in . Then, there exists 
such that for any  and
 the following is true

\end{proposition}
The proof of Proposition \ref{lemma:PropertyGVertex} appears in 
Section \ref{sec:PropoGVertex}.\\



\begin{theoremproof}{\ref{thrm:ConnectivityGnp}}
Let  be the number of pairs of independent sets of size 
in   that do not have an augmenting vertex. From Corollary
\ref{cor:Augment2GProperty} and Corollary \ref{cor:GPropertyToConnect},
it suffice to show that ,
where  and  with .
For this, we are going to use Markov's inequality, i.e.

and we are going to show that .


First consider the case where 
and  is as defined in the statement of Proposition \ref{lemma:PropertyGVertex}.
Using Proposition \ref{lemma:PropertyGVertex} we get that 

It follows easily that

Thus, from (\ref{eq:ExpctPairsNonAugm}) we get that there is 
 with  such that 

for any , where .

Consider now the case where . For a pair of independent
sets any vertex that is not adjacent to the vertices of the pair is an
augmenting vertex. Let ,  be a pair of independent sets
each of size , for . 
Let  be the vertices not in  but 
not adjacent to any vertex in , as well.
Every   belongs to  independently of the
other vertices with probability at least  .
Thus, . Using Chernoff bounds 
we get

Since  consists of {\em augmenting vertices} for
the pair , the probability for  not to 
have any  augmenting vertex is upper bounded by .
For   it holds that

The theorem follows.
\end{theoremproof}





\subsection{Proof of Proposition \ref{lemma:PropertyGVertex}}\label{sec:PropoGVertex}

Consider  an arbitrary pair 
where  and .
For the rest of the proof assume that  where
. Also, let  be such that . Clearly, it holds that . 
For proving the proposition, we consider two cases.
In the first one we take . In the second  we take .



Take . We will show that with sufficiently large
probability  there exists a non-empty set  of augmenting
vertices for the pair , . The set  contains a
specific kind of augmenting vertices. That is, the cardinality of
 will be a lower bound on the actual number of augmenting
vertices. So as to specify , we need  the following definitions:


\begin{description}
\item[:]   contains those vertices that have exactly one neighbour
in .

\item [:]  
is the  set of vertices that have at least one neighbour in .


\item [:]  Every  has the following properties: 
\begin{description}
	\item[S-]  and 
	. 
	\item[S-] There exists  that contains exactly
	one neighbour of each  in 
	and no other vertex. Furthermore,  is an independent set.
\end{description}
\end{description}
In an analogous manner we define  and .


For each augmenting vertex   the following should hold:
{\bf (A)} , 
{\bf (B)}  and
,
{\bf (C)}   and  .
\\ \vspace{-.1cm}


\noindent
{\bf Remark.}
Observe that each  is not necessarily
augmenting. However, if, additionally,  it does not have any neighbours 
in  , then it is augmenting.
\\ \vspace{-.2cm}


\noindent
Consider a process where we reveal all the sets ,
for  in steps. In each step we reveal a certain amount of
information regarding these six sets. Since  is symmetric 
to  for every  we just presents results related to 
 while those for  follow immediately. The results 
appear as a series of claims whose proofs appear after the proof of this 
proposition.



In  {\tt Step 1}, we reveal the sets , .  
There we have  the following result.


\begin{claim}\label{claim:Q1Claim}
Let .  It holds that 
,
where  as  grows. Furthermore, it holds that 

\end{claim}


\noindent
{\bf Remark.}
After {\tt Step 1}, for each  we have the information that both
the number of edges that connect  with  
and the number edges that connect  with 
are different than 1. 
\\ \vspace{-.3cm}



\noindent
Then, we proceed with {\tt Step 2} where we reveal  and 
. Also reveal the edges between  and 
as well as the edges between  and . There we have 
the following result.



\begin{claim}\label{claim:Q2Claim}
Let . For , it holds that

where .
\end{claim}

\noindent
Revealing the sets  and  is, technically,
a more complex task. Let us make some observations regarding these
sets. Assume that some vertex 
satisfies condition\footnote{In the definition of set .}
. So as  to belong to  there should exist a set 
as specified in the condition . However, the possibility 
of edges between vertices in  leaves open whether we 
can have such a set for . To this end consider the following.

\begin{definition}
For every , let  be the family of subsets
 of cardinality   which have the following 
property:  There exists independent set 
that contains exactly one  neighbour of each  in 
and no other vertex.
\end{definition}

\noindent
That is, a vertex  which satisfies  satisfies  also  (
i.e.  belongs to ) only if , 
for some appropriate  or .
Observe that the families  are uniquely determined by the
edges whose both ends are in . In  {\tt Step 3} we reveal
exactly these edges, i.e. with both ends either in  or in .
This results  to the following.


\begin{claim}\label{claim:GoodFraction}
Let  and .
For every  it holds that

\end{claim}
It is direct to see that it always holds that .


Let the set .
In {\tt Step 4}, we reveal the vertices that belong in .
This step amounts to revealing the edges between each vertex   
and the sets  and , for .
In particular, revealing the edges between  and the set 
(resp. ) specifies whether 
(resp. ), or not.

Despite the information we have for , from {\tt Step 1}, the edge
events between  and the vertices in  are 
independent of the edge events between  and the vertices in
. That is 
.
Also, it is easy to observe that 
independently of the other vertices in .



For every  let  be an indicator random variable such that
 if  and  otherwise.
The observations in the previous paragraph suggest that s  
are independent with each other and . 



\begin{claim}\label{Lemma:Q3}
Let the event  .
For every , it holds that 
\end{claim}

\noindent
Let  where  varies over all vertices in .  
Using Claim \ref{claim:Q1Claim}  and Claim \ref{Lemma:Q3} we get that

Applying Chernoff bounds and get that 



\noindent
Finally, in {\tt Step 5} we reveal which vertices in 
are augmenting, i.e. those which are adjacent to .
Only these vertices will belong the set .

Due to edge independence in , every 
is augmenting independently of all the rest vertices  with probability 
. 
Let the event  and . It is direct
that .
Since , there exists  
such that . Applying Chernoff bounds we get that

Using Claim \ref{claim:Q1Claim}, Claim \ref{claim:Q2Claim}, Claim \ref{claim:GoodFraction} 
and (\ref{eq:X3Chernoff}) we get that .
Combining the probability bound for  with (\ref{eq:ConditionalF3Q0})
we get that

as .



It remains to study the case where .
There, it holds that . Let  be the 
set of vertices, outside , that are not adjacent to any
vertex in . 
Every   belongs to  independently of the
other vertices with probability .
Thus, . Using Chernoff bounds 
we get

Since  consists of {\em augmenting vertices} for
the pair , the probability that there is no augmenting
vertex is upper bounded by . The proposition
follows from (\ref{eq:Q0BoundForSmallA}) and (\ref{eq:Q0BoundForBigA}).
 \qed \\



\begin{claimproof}{\ref{claim:Q1Claim}}
Let  be the probability for a vertex  outside ,
to have exactly one neighbour in . It holds
that 

Of course, with the same probability   has exactly one neighbour
in . Then, the probability for  to be in 
 is . Observe that  belongs
to  independently of the other vertices.
It is direct that there exists
 such that 

The claim follows by applying the Chernoff bounds.
\end{claimproof}




\begin{claimproof}{\ref{claim:Q2Claim}}
Due to symmetry each vertex  is adjacent to
exactly one random vertex in , independently
of the other vertices in . An equivalent way of
looking adjacencies between vertices in  and
 is by assuming that the vertices in
 are balls and each vertex in  is a bin and each ball is thrown into a {\em random} bin.
The non-empty bins correspond to vertices in . The
claim will follow by deriving an appropriate tail bound on the
number of occupied bins.

Let  denote the number or balls and  denote the number of
bins, it holds that  and .
For , let  be the probability that there is a
subset of bins of size  that contains all the balls. For 
a fixed subset of bins of size  and for a fixed ball , it
holds that

For  we have that

It is easy to check that for any  we have . Hence, letting  be the event that ``there is a 
subset of at most  bins that has all the balls'', it
holds that

The claim follows.
\end{claimproof}



\begin{claimproof}{\ref{claim:GoodFraction}}
The cardinality of each family , for ,
depends on the edges whose both ends are in . 
As a  first step we estimate how many are these vertices 
conditional on the event .

Let  be the set of edges whose both ends are in .
The bound on  and the cardinality of  that
 specifies as well as the fact that each edge appears
independently with probability  yields to the following relation.

where . Chernoff bounds yield to the following inequality.



\noindent
Let the event   and . 

Next, we compute .  Note that the event 
specifies, only, an upper bound on  and it does not tell
where the edges are placed. That is, all subsets of 
of cardinality  are symmetric thus they belong to 
with the same probability. By the linearity  of expect we get that


\noindent
Let  be the family of subsets of , each of 
cardinality , such that for each  the following
is true:  The set   contains  exactly one neighbour of
each vertex  and no other vertex. By definition the 
family  must have at least one member. Moreover, if there 
exists one set in  which is independent, then .  


When we reveal the edges between the vertices in 
it is easy to see that the probability that  contains no 
independent set is maximized when  is a singleton.
Given  and , observe that each pair of vertices in 
 is adjacent with probability  at most 
.  Each subset of  of cardinality 
has expected number of adjacent vertices ,
for large .  That is, the probability that  does not contain
an independent set is  at most . Thus, 


\noindent
Having calculated a lower bound for  we will show that
given the event ,   is tightly concentrated about its 
expectation. Then, claim will be immediate. So as to show the concentration
result,  we use an edge exposure martingale argument for the edges in  
and then we apply Azuma's inequality (see e.g. \cite{janson} Theorem 2.25). 


Observe that the revelation of each edge in  cannot reduce the
cardinality of  by more than 
sets. Standard arguments with Azuma's inequality  yield to that for any 
 it holds that

Setting  we get that

where the last derivation follows by using the fact that ,
 and . 
The claim follows by just using the law of total probability and get that

\end{claimproof}







\begin{claimproof}{\ref{Lemma:Q3}}
Consider some .
Let  be the number of vertices in  which are adjacent to . Also, let 
the event  for 
and .
By the law of total probability we get that

We impose the bound   since no vertex in 
can have more than  neighbours in .
Conditional on ,  all the
subsets of size  in  are equiprobably adjacent to .
Thus, we get that

where . Also, it is easy to see that 



\noindent
Let the event `` and ''.
Observe that the variable   is distributed as in 
conditional on the event . Using this  along with 
(\ref{eq:Target1stBound}) and (\ref{eq:4TargetOneH1})
we can rewrite (\ref{eq:PruQ3FirstBound}) as follows:

where the last inequality follows from the fact that 
and a simple derivation which implies that 
.
Also, note that

The last inequality follows by noting that the summation on the l.h.s. of the first line
is equal to the probability  and  bounding it by using
Chernoff bound  (as it appears in Theorem 2.1 in \cite{janson}).
Using (\ref{eq:ComplementBound}), we get that

The claim follows by plugging (\ref{eq:sumupto7dBin}) into (\ref{eq:PQ3OneBeforeEnd}) and
get that .
\end{claimproof}








\section{Proof of Theorem \ref{theorem:shattering}}\label{sec:theorem:shattering}


The following proposition reduces the problem of establishing shattering to an exercise in calculus.

\begin{proposition}\label{Prop_calcShattering}
There exist a constant  and  such that for all
 the following is true. Suppose that   for
 and let
	
If there is a real  such that
	
then  shatters, with  and .
\end{proposition}

\noindent
{\bf Proof of \Thm~\ref{theorem:shattering} (assuming \Prop~\ref{Prop_calcShattering}):}
Let  be as in \Prop~\ref{Prop_calcShattering}, assume that  is sufficiently large,
let  and set
	
Moreover, let .
We are going to verify~(\ref{eqcalcShattering1}) and~(\ref{eqcalcShattering2}).
Then \Thm~\ref{theorem:shattering} will follow from \Prop~\ref{Prop_calcShattering}.
Indeed, using the elementary inequality , we find
	
Hence, for  sufficiently large our choice of  ensures that
	
Thus, we have verified~(\ref{eqcalcShattering1}).

Starting from~(\ref{eqpftheorem:shattering1}), we see that for any  and  large,
	
because  for all .
By comparison, for  we have
	
Combining~(\ref{eqpftheorem:shattering2}) and~(\ref{eqpftheorem:shattering3}), we obtain
	
as .
Thus, we have got~(\ref{eqcalcShattering2}).

Lemma \ref{lemma:SameAsShaterringThrm} (in a following section)
states explicitly what is implied in this proof.  That is there
exists  such that (\ref{eqcalcShattering1}) and
(\ref{eqcalcShattering2}) hold.
Thus, we are going to use the proof here for Lemma 
\ref{lemma:SameAsShaterringThrm}.
\qed


\subsection{Proof of \Prop~\ref{Prop_calcShattering}}

Let  be a pair chosen from the planted model .
To prove the proposition, we are going to show that under the
assumptions~(\ref{eqcalcShattering1}) and~(\ref{eqcalcShattering2})
the independent set  belongs to a small `cluster' of
independent sets that is separated from the others by a linear
Hamming distance with a probability very close to one.
We will then use \Thm~\ref{thrm:transfer-theorem} to transfer
this result to the distribution , which will imply
that  shatters \whp


Let  be the number of independent sets 
such that
	.

\begin{lemma}\label{Lemma_condexp}
We have
	
\end{lemma}
\begin{proof}
Let  be such that . 
The total number of graphs with  in which both 
are independent sets equals
	
For we can choose any  edges out of those potential edges that
do not join two vertices of either  or .
Since both  have size  and ,
the number of such `bad' potential edges is 
by inclusion/exclusion.
Since  is chosen uniformly among all 
graphs in which  is independent, we thus get
	
Furthermore, the total number of ways to choose a set  with
 equals 	
(choose the  vertices in the intersection 
and then choose the remaining  vertices).
By the linearity of the expectation, we get from~(\ref{eqLemmacondexp1})
	
Taking logarithms and dividing by  completes the proof.
\end{proof}

Let us call an independent set  of size  of a graph 
\emph{-good} if  has no independent set 
such that  and if
		.
Moreover, let
	

\begin{corollary}\label{Cor_condexp}
Suppose that  is such that~(\ref{eqcalcShattering1}) and~(\ref{eqcalcShattering2}) hold.
Then there exist  such that
	
\end{corollary}
\begin{proof}
The function  is continuous.
Therefore, if (\ref{eqcalcShattering1}) and~(\ref{eqcalcShattering2}) 
are satisfied for some  then there exist  and  such that
	
Let  be the number of  such that .
Then \Lem~\ref{Lemma_condexp}, (\ref{eqcalcShattering1a}), and Markov's
inequality yield
	
The last inequality follows by taking 
and then 
Similarly, let  be the number of 
such that . Moreover, let  and let

where in the last step we used Stirling's formula.
Using~(\ref{eqcalcShattering2a}) and Markov's inequality, we find that
	
Combining~(\ref{eqCorcondexp1}) and~(\ref{eqCorcondexp2}) with 
\Cor~\ref{Cor_quantexchange}, and letting, say, , we see that
	
as claimed.
\end{proof}

\noindent{\bf Proof of \Prop~\ref{Prop_calcShattering}:}
Let  be the event that
	
\Cor~\ref{Cor_condexp} implies that there exists 
such that given , 	\whp\  has the property that all
but  independent sets 
are -good. Let  denote this event.
As \Lem~\ref{Lemma_gap} ensures that  \whp, 
we have
		
As a consequence, we just need to show that
	the two conditions in Definition~\ref{Def_shattering} are satisfied if  occurs.

Thus, let . We construct a decomposition of  
into pairwise disjoint subsets  inductively
as follows. Suppose .
If the set  does not contain 
a -good anymore, let , set
	
and stop.
Otherwise, choose some  that is -good, let
	
and proceed to .

Let . We claim that this construction satisfies the two
conditions in Definition~\ref{Def_shattering}. Indeed, each  is
-good for all, we have 	 
for all . Furthermore, as  we have 
.
Thus, the partition  satisfies the first condition
in Definition~\ref{Def_shattering}.


With respect to the second condition, let  and 
with . Assume for contradiction that .
Then, for some  we have that
	
and thus .
This contradicts the fact that  is good (which implies that there is no
	independent set  such that .
Thus, we have established the second condition in Definition~\ref{Def_shattering}.
\qed









\section{Proof of Theorem \ref{theorem:non-maximal}}\label{section:theorem:non-maximal}


In this section we assume that  for some large enough constant .
Moreover, let  be a function of  that tends to  sufficiently slowly,
and assume that  for some .

Our goal is to show that for a random pair  chosen from 
\whp\ there is a larger independent set  in  that contains  as a subset.
More precisely,  is supposed to have size .
In order to construct such a set  we need the following concept.

\begin{definition}
A vertex  is called {\bf -pure} in  if it is not adjacent to any vertex in .
\end{definition}
Basically, in order to expand  we are going to show that  has an independent set 
of size   consisting of -pure vertices.
Then  is the desired larger independent set.
We begin by estimating the number of -pure vertices and the density of the graph
that they span.




\begin{lemma}\label{lemma:pure-graph}
Let  be chosen from ,
	where
		  with . 
Let  be the set of -pure vertices.
Then with probability  the following two statements hold.
\begin{enumerate}
\item Let . Then .
\item Let  be the number of edges in the induced subgraph .
	Then , with .
\end{enumerate}
\end{lemma}

\begin{proof}
Instead of working directly with the distribution ,
let us consider the following variant . First,
choose a set  of size  uniformly at random.
Then, constrict a graph  by inserting each of the  possible
edges that do not join two vertices in  with probability
 independently.

Thus, the number of edges in  is binomially distribution with
mean . Furthermore, given that  has precisely  edges, it
is a uniformly random graph with this property in which 
is an independent set. Therefore, for any event  we have
	
where the last step follows from Stirling's formula.

Now, let  be the number of -pure vertices in .
For each vertex  the number of neighbours in 
is binomially distributed with mean . In effect,  is pure with
probability . Since these events are mutually independent for
all ,  has a binomial distribution .
Hence, letting , we have
	
provided that  is sufficiently big.
Letting , we obtain from \Thm~\ref{chernoffbounds} (the Chernoff bound)
	
for  large enough.
Together with~(\ref{eqP'knm}) this implies the first assertion.

To prove the second assertion, we need an upper bound on .
Once more by the Chernoff bound,
	
for  large enough.
Let  be the set of -pure vertices in .
Since each potential edge that does not link two vertices in 
is present in  with probability  independently, given the value
of  the number  of edges spanned by  is binomially distributed
with mean .
Therefore,
	
provided that  is large.
Hence, by the Chernoff bound and~(\ref{eqlemma:pure-graph1}),
	
for  big.
Finally, the second assertion follows from~(\ref{eqP'knm}) and~(\ref{eqlemma:pure-graph2}).
\end{proof}


\noindent{\bf Proof of \Thm~\ref{theorem:non-maximal}.}
Suppose that . Let  be a pair
chosen from the distribution . Let  be the set
of -pure vertices and let  be as in \Lem~\ref{lemma:pure-graph}.
Crucially, given , , , the induced subgraph 
is just a uniformly random graph on  vertices with  edges,
because the conditioning only imposes the absence of --edges.
In other words,  is nothing but a random graph .
We are going to use this observation to show that 
contains a large independent set \whp\

Let  be the event that  and
. Then by \Lem~\ref{lemma:pure-graph}

Given , the average degree of  is

Let  be the event that .
Since  is distributed as , \Cor~\ref{cor:ExistenceGnm} 
implies that

Combining~(\ref{eqtheorem:non-maximal1}) and~(\ref{eqtheorem:non-maximal2}) with \Thm~\ref{thrm:transfer-theorem}, we thus get
	

\noindent
Now assume that .
Let  be the largest independent set of .
Then
	
Since  is independent, (\ref{eqtheorem:non-maximal4}) shows that  is
	-expandable.
Thus, the assertion follows from~(\ref{eqtheorem:non-maximal3}).
\qed





\section{Proof of Theorem \ref{theorem:maximal}}\label{section:theorem:maximal}



Let .
In this section we assume that  with ,
	and that  for some large enough constant .
Assuming that  are reals such that
	
we are going to show that in a pair  chosen from the distribution ,
 is not -expandable.

To see why this is plausible, consider a pair  chosen from the distribution .
(The following argument is not actually needed for our proof of \Thm~\ref{theorem:maximal}; it is only included
	to facilitate understanding.)
Then for each vertex  the \emph{expected} number of neighbours of 
inside of  is greater than .
Indeed, one could easily show that for each vertex  the number of neighbours in 
dominates a Poisson variable .
Hence, the probability that  is -pure is bounded by
	,
and thus the expected number of -pure vertices is .
In effect, in order to expand  significantly we would have to include some
vertices that are \emph{not} -pure.
But each such vertex would `displace' some other vertex from 
	(by the very definition of -pure).
In fact, most vertices that are not -pure have several neighbours in ,
and thus it seems impossible to expand  substantially without first removing
a significant share of its vertices.

To actually prove \Thm~\ref{theorem:maximal} we use a first moment argument.
We begin by analysing the planted model.

\begin{lemma}\label{lemma:maximality}
With  sufficiently large and  as above, we have

\end{lemma}
\begin{proof}
Let .
For  chosen from the distribution ,
let  be the number of independent sets  such that
	
The total number of ways to choose a set  satisfying~(\ref{eqlemma:maximality1}) is
	
(first choose  vertices from , then choose the remaining 
vertices from ).
Furthermore, for any  satisfying~(\ref{eqlemma:maximality1}) the probability of being independent is
	
Indeed, in order for both  and  to be independent we have to forbid all edges
that connects two vertices in either set, and the number of potential such edges is
	 by inclusion/exclusion.
This explains the numerator in~(\ref{eqlemma:maximality3}), and the denominator simply reflects that  is chosen randomly
from all graphs in which  is independent.

Combining~(\ref{eqlemma:maximality2}) and~(\ref{eqlemma:maximality3}) and using the linearity of the expectation, we see that
	
We are going to show that  and then apply Markov's inequality to obtain the lemma.

We begin by estimating  and  separately.
For  we get
	
As we assume that 
 and  and , we have
	 and .
Furthermore, the function  is monotonically increasing for .
Hence, if , then .
If, on the other hand, , then
	.
In either case we obtain
	
With respect to , we have 
Since  and , the elementary inequality  yields
	
Finally, plugging~(\ref{eqlemma:maximality5}) and~(\ref{eqlemma:maximality6}) into~(\ref{eqlemma:maximality4}), we get
for  large enough
	
Thus, the assertion follows from Markov's inequality.
\end{proof}


\noindent
Theorem \ref{theorem:maximal} follows directly from \Lem~\ref{lemma:maximality} and \Thm~\ref{thrm:transfer-theorem}.






\section{Proof of Corollary \ref{cor:MixingTimeBound}}\label{sec:cor:MixingTimeBound}



Let  slowly. Throughout this section we assume that



\noindent
The proof of Corollary \ref{cor:MixingTimeBound} amounts to showing
that the Metropolis process can be ``trapped'' in a relatively
small group of independent sets and it escapes only after an
exponentially large number of steps. To be more specific, let

We show that  can be partitioned into
disconnected parts, i.e.  it is not possible for the process to
move from one part to another without using independent sets of
size much smaller than the minimum . However, we show that
once the process gets to a ``typical'' independent set in 
it will need to wait for exponential time so as to escape by
visiting a small independent set.



Before showing Corollary \ref{cor:MixingTimeBound} we provide some
auxiliary results. The following proposition shows that for a given
parameter  the stationary distribution of the Metropolis
process concentrates on a small range of sizes of independent sets.



\begin{proposition}\label{prop:MostLikelyIS}
With probability at least  the
random graph  has the following property.
\begin{quote}
For an independent set  chosen from the stationary distribution
of the Metropolis process on  we have
	
(where in~(\ref{eq:TheK2}) probability is taken over the choice of  only).
\end{quote}
\end{proposition}
The proof of Proposition \ref{prop:MostLikelyIS} appears in Section 
\ref{sec:prop:MostLikelyIS}.


\begin{lemma}\label{lemma:ShateringTube}
\Whp\ the random graph  has the following property.
The set 
admits a partition into classes  such that the following three statements hold.
\begin{description}
\item[C1.] The distance between any two independent sets in different classes is at least .
\item[C2.] For a random set  chosen from the stationary distribution of the Metropolis process we have
	
\item[C3.] Furthermore, . 
\end{description}
\end{lemma}
The proof of Lemma \ref{lemma:ShateringTube} appears in
Section \ref{sec:lemma:ShateringTube}.


\medskip\noindent{\bf Proof of \Cor~\ref{cor:MixingTimeBound}:}
Let  be as in (\ref{eq:TheK}) and assume that  is such
that  has a partition  satisfying {\bf C1--C3} in
Lemma~\ref{lemma:ShateringTube}.
We are going to show that the mixing time of the Metropolis process exceeds .
The proof is by contradiction. Thus, assume that the mixing time
of the Metropolis process is .
Let  be the state of the Metropolis process at time ().

Let  and  .
Since  is the mixing time, for any  the
 distribution of  is extremely close to the stationary distribution.
More precisely, if  chosen from the stationary distribution, then for any  we have
	
Therefore, {\bf C3} implies that for any ,

Applying the union bound, we get for  large enough

In other words, we have shown that to get from 
to , the Metropolis process very likely only
passes through independent  sets from .

Most likely, the two independent sets , 
belong to different classes of the partition ,
because the time difference  is much bigger than the
mixing time . Formally, if  is chosen from the
stationary distribution and  such that ,
then by {\bf C2}

Combining~(\ref{eq:NotInRare}) and~(\ref{eq:InDifferentClasses}),
we thus get



\noindent
Thus, assume that there are two distinct  such that
 and .
Let  be the first time that .
Then by definition of the Metropolis process, .  Consequently, 
because otherwise there would be two independent sets in different
classes at distance one. Thus,

in contradiction to (\ref{eq:NotInRare}) and (\ref{eq:RandomOverlap}).
\qed



\subsection{Proof of Proposition \ref{prop:MostLikelyIS}}
\label{sec:prop:MostLikelyIS}
For a graph , let 
 
It is easy to deduce from the definition of  Metropolis process
(see e.g. \cite{jerrum-planted}) that for any set of  integers 
 it holds that

Therefore, we have

Consider some  that satisfies (\ref{eq:themuglambda'}).
Then, Proposition \ref{prop:MostLikelyIS} will follow by bounding
appropriately the rightmost ratio above, for  (as
defined in (\ref{eq:TheK})) and  being a typical instance of
.
\\ \vspace{-.3cm}

\noindent
{\bf Remark.}
Observe that when the graph  is distributed as in  the
quantity  is a random variable which depends {\em only on the
underlying graph.} 
\\ \vspace{-.3cm}

\noindent
Before proving the proposition we need some preliminary results.
With the parameter  and the expected degree  in mind, 
for any  we define the following function:

It is straightforward to verify that .
 is twice differentiable, as a matter of fact it holds that

For any  and  it holds that . 
That is,  is strictly decreasing. Furthermore, if 
for given  there exists  such that 

then  is a global maximum for .
Since  is strictly decreasing, for any given
 and , we can find unique  such 
that  is maximized when .


\begin{claim}\label{claim:UnlikeIS}
Take  and let  be such that 
is maximized for . Then for any  such that 
it holds that

\end{claim}
\begin{proof}
From (\ref{eq:f-lambdaDPrime}) it is easy to show that for any ,
it holds that . Also, for any  we can find 
appropriate   such that 

as promised.
\end{proof}

\noindent
Let  be such that  is maximized for 
. 


\begin{lemma}\label{lemma:MassOfMaximum}
For  and , it holds that

\end{lemma}
\begin{proof}
The lemma follows directly from Proposition  \ref{Lemma_gap}.
\end{proof}



\begin{lemma}\label{lemma:AlmostMostLikelyIS}
For ,  let 
and
 
It holds that

\end{lemma}
\begin{proof}
Observe that for any integer  it holds that
. 
Since the function  is increasing for every 
 and decreasing for , for 
 and sufficiently large  it holds  that

Furthermore, using Claim \ref{claim:UnlikeIS} we get that

Let . 
It holds that

The lemma follows by applying Markov's inequality. That is,
for sufficiently large  it holds that

as promised.
\end{proof}



\begin{propositionproof}{\ref{prop:MostLikelyIS}}
Let , for .

Observe that quantity  for fixed  and
 distributed as in  is a random variable which depends
only on the graph .
We are going to show that for  it holds that

Observe that once we have the above tail bound,  the proposition
follows easily from Lemma \ref{lemma:AlmostMostLikelyIS}. In particular
(\ref{eq:MuLambdaCBound}) implies that 

Also, from Lemma \ref{lemma:AlmostMostLikelyIS} and (\ref{eq:ratio2boundforMetr})
we have the following: 
Consider the Metropolis process with underlying graph 
and parameter . Then, with probability at least 
over the graph instances , if we choose  according
to the stationary distribution of the Metropolis process, then 

where .
The proposition follows from (\ref{eq:PropArg1}) and (\ref{eq:PropArg2}).

It remains to show (\ref{eq:MuLambdaCBound}). By definition we have
that for any fixed graph  it holds that 
,
where . From Lemma
\ref{lemma:AlmostMostLikelyIS} we have that with probability at least
 over the graph instances  
it holds that

and 

Combining (\ref{eq:ZGLBoundWHP}) and (\ref{eq:ZGLKBoundWHP})
we get that with probability at least 
over  it holds that

for some . Then, it is elementary to verify that
the summation on the r.h.s. is a convex combination of values of  in
. That is, the summation is at most  and at 
least . Then (\ref{eq:MuLambdaCBound}) follows.
\end{propositionproof}





\subsection{Proof of Lemma \ref{lemma:ShateringTube}}\label{sec:lemma:ShateringTube}


As in (\ref{eqeventZdk}) let 



\begin{lemma}\label{lemma:ShateringTubeTest}
Let  be distributed as in ,
for , where  and  are as in (\ref{eq:TheK}) and 
(\ref{eq:themuglambda}), respectively.
The set  admits a partition into classes 
 such that
\begin{enumerate}
\item , for any 
\item 
\item The distance between two independent sets in different classes is at least .
\end{enumerate}
\end{lemma}



\begin{lemmaproof}{\ref{lemma:ShateringTube}}
{\bf (Given Lemma \ref{lemma:ShateringTubeTest}):}
Consider  and the Metropolis process with parameter 
, for  as in (\ref{eq:themuglambda'}).
Let the independent set  be chosen according to 
the stationary distribution of the process. 

Conditional that ,  is distributed
uniformly at random in , for any .
For any  it holds that

the last inequality follows from the fact that 
is a convex combination of  for .
Also, it holds  that

Hence,

Also, from the law of total probability we get that

\noindent
The statement   holds  from the statement 3 in Lemma
\ref{lemma:ShateringTubeTest}. Setting    in (\ref{eq:1896Z})
and using Statement 1 from Lemma \ref{lemma:ShateringTubeTest}, we
get the statement . Similarly, statement 
follows by setting  in (\ref{eq:1896Z})
and  using Statement 2  from Lemma \ref{lemma:ShateringTubeTest}.
\end{lemmaproof}



\subsection{Proof of Lemma \ref{lemma:ShateringTubeTest}}

Consider a uniform pair , for some
. For fixed , and , let  be the number of independent sets  such that . Also, for 
consider   and let the independent
set  be called -{\em good} if
 has no independent set  such 
\begin{itemize}
	\item 
	\item 
\end{itemize}
while .

\begin{lemma}\label{lemma:PsiXiExpctation}
For  is as defined in statement of Proposition \ref{Prop_calcShattering}
and , it holds that 
 
where 

 
\end{lemma}
\begin{proof}
Let  be such that  and .
With application of inclusion/exclusion principle we get that the total number of graphs
with  edges in which  and  are independent sets equals

Since  is chosen uniformly at random among all 
graphs on  vertices and  edges such that  is an independent set, we get that

The total number of ways to choose a set of vertices  of 
size  such that  is 
equal to . 
By the linearity of expectation, we get that

By definition (see  Proposition \ref{Prop_calcShattering}), 
it holds that 

Combining (\ref{eq:EXbgBound}) and (\ref{eq:psibunfold}) we get that

since 


\noindent
Taking the logarithm and dividing by  the quantities in 
(\ref{eq:ratioEZbgVsexppsib}) we get the lemma.
\end{proof}


\begin{lemma}\label{lemma:SameAsShaterringThrm}
There exist a constant  and  such that for
all  the following is true:
Suppose that , where ,
then for   we have that

\end{lemma}
The lemma above states explicitly what is implied by the proof of 
Theorem \ref{theorem:shattering}. Thus, the proof of Lemma  \ref{lemma:SameAsShaterringThrm} 
is exactly the same as the one of Theorem \ref{theorem:shattering}.

\remove{
\begin{proof}
Let . Assume that  for some
. Consider the functions  
and  as defined in the statement of Lemma
\ref{lemma:PsiXiExpctation}. 
Working as in the proof of Theorem \ref{theorem:shattering} (i.e. (\ref{eqpftheorem:shattering1}))
we get that

  Furthermore, for  
we get that 

From (\ref{eq:psi(c)bound}), for any  we get that

since  for every .
By comparison, for  we have
	
From  (\ref{eq:InverseExpctLbound}) and (\ref{eq:psi(beta)UBound}),
it is direct that (\ref{eq:PsiSupBound}) holds.
\end{proof}
}


\begin{lemma}
There  is  such that for  the following is true: For ,
and  
there is  such that

\end{lemma}
\begin{proof}
Let . Assume that  for some
. Consider the functions  
and  as defined in the statement of Lemma
\ref{lemma:PsiXiExpctation}. 
In what follows take . Let 

where .
Our choices for  and  ensure that for any
 it holds that


\noindent
Using (\ref{eq:XiBound}) and (\ref{eq:PsiBound}), from Lemma \ref{lemma:SameAsShaterringThrm},
we get that 

The function  is continuous, therefore there exist  
and  such that

The last relation follows from (\ref{eq:PsiSupBound}), of Lemma \ref{lemma:SameAsShaterringThrm}
and (\ref{eq:XiBound}).

Let , be the number of  such that
. Then,  Markov's
inequality yields

where  and . Using Lemma
\ref{lemma:PsiXiExpctation} we get 


\noindent
Let  be the number of  such that . Moreover, let 
For the derivation in the second line, see in the proof of Corollary
\ref{Cor_condexp}.
For  and , it holds that



\noindent
The lemma follows by noting the following for , 

as claimed.
\end{proof}

\noindent
Now,  Lemma \ref{lemma:ShateringTubeTest} follows from the above lemma and
by using arguments very similar to those in the proof of Proposition \ref{Prop_calcShattering}.




\begin{thebibliography}{00}


\bibitem{AchCoOg}D.~Achlioptas and A.~Coja-Oghlan. 
{\em Algorithmic Barriers from Phase Transitions}.  
In proc. of FOCS 2008, pp 793-802.



\bibitem{AchListColouring} D.~Achlioptas and M.~Molloy. 
{\em The Analysis of a List-Coloring Algorithm on a Random Graph}. 
In proc. of FOCS 1997, pp 204-212.



\bibitem{BST10} N.~Bhatnagar, A.~Sly and P.~Tetali. 
{\em Reconstruction Threshold for the Hardcore Model}. 
In proc. of APPROX-RANDOM 2010, pp 434-447.



\bibitem{Bollobas} B.~\Bollobas. 
{\em Random graphs},
2nd edition Cambridge (2001).



\bibitem{BollobasIS} B.~\Bollobas~and P.~\Erdos. 
{\em Cliques in random graphs}.
Math. Proc. Comb. Phil. Soc. \textbf{80}, (1976) pp 419-427.


\bibitem{arxivTR} A.~Coja-Oghlan and C.~Efthymiou.
{\em On independent sets in random graphs}. Tech. Report.
http://arxiv.org/abs/1007.1378.

\bibitem{W2ndM} V.~Dani and C.~Moore. {\em Independent sets in
random graphs from the weighted second moment method.} 
In Proc. of APPROX-RANDOM 2011, pp 472-482.


\bibitem{AverAlg1} M.~E.~Dyer and A.~M.~Frieze. 
{\em Fast algorithms for some random NP-hard problems}, 
Journal of Algorithms \textbf{10}, (1986) pp 451-489.



\bibitem{erdos-1st} P.~\Erdos. 
{\em Some remarks on the theory of graphs}. 
Bull. Amer. Math. Soc. \textbf{53} (1947) pp 292--294



\bibitem{frieze-is} A.~M.~Frieze. 
{\em  On the independence number of random graphs}. 
Discrete Mathematics \textbf{81} (1990) pp 171-175.



\bibitem{friezeRSA} A.~M.~Frieze and C.~McDiarmid.
{\em Algorithmic theory of random graphs}. 
Random Struct. Algorithms \textbf{10}, (1997) pp 5-42.



\bibitem{FrSu} A.~Frieze and  S.~Suen. 
{\em Analysis of two simple heuristics on a random instance of -SAT}. 
Journal of Algorithms \textbf{20} (1996) pp 312-355.


\bibitem{FuAnderson}
Y.~Fu, P.~W.~Anderson.
{\em Applications of statistical mechanics to NP-complete problems in
 combinatorial optimization}. J.\ Phys.\ A {\bf 19} (1986) 1605.



\bibitem{Gamarnik}
D.~Gamarnik, T.~Nowicki and G.~Swirscsz
{\em Maximum Weight Independent Sets and Matchings in Sparse Random Graphs}.
Random Structures and Algorithms \textbf{28} (2005) pp. 76-106.



\bibitem{grimmett} G.~R.~Grimmett and C.~J.~H.~McDiarmid. 
{\em On colouring random graphs}.
Math. Proc. Cambridge Philos. Soc., \textbf{77} (1975), pp 313 - 324.



\bibitem{jerrum-planted} M.~R.~Jerrum. 
{\em Large cliques elude the Metropolis process}.
Random Structures and Algorithms \textbf{3} (1992), pp 347-359.





\bibitem{KarpOLD} R.~M.~Karp. 
{\em The probabilistic analysis of some combinatorial search algorithms}, 
in Algorithms and Complexity: New Directions and Recent Results, J.F. Traub, ed., 
Academic Press (1976), pp 1-19.




\bibitem{KarpSipser} R. Karp and M. Sipser. 
{\em Maximum matchings in sparse random graphs}. 
In proc. of FOCS 1981, pp. 364 375.



\bibitem{KirkpatricMetropolis}
S. Kirkpatrick, C. Gelatt, and M. Vecchi.
{\em Optimisation by simulated annealing}, Science, \textbf{220}, (1983), pp 671-680.



\bibitem{1RSBPaper}F.~Krzakala, A.~Montanari, F.~Ricci-Tersenghi, G.~Semerjianc, 
L.~Zdeborova. {\em Gibbs states and the set of solutions of random constraint 
satisfaction problems.} In Proc. National Academy of Sciences \textbf{104} (2007)
pp 10318-10323.




\bibitem{kucera}L.~\Kucera.  
{\em Expected Behavior of Graph Coloring Algorithms}.
In Fundamentals of Computation Theory, LNCS Vol. 56, Springer Verlag (1977), pp 447-451.



\bibitem{AverAlg3}
L.~\Kucera. {\em Graphs with small chromatic number are easy to color},
Inf. Process. Lett. \textbf{30}, (1989), pp 233-236.



\bibitem{matula} D.~ Matula. 
{\em The largest clique size in a random graph}.
Tech. Rep., Dept. Comp. Sci., Southern Methodist University, Dallas, Texas, 1976.


\bibitem{MPZ-Science} M.~Mezard, G.~Parisi and R.~Zecchina. {\em Analytic and Algorithmic 
Solution of Random Satisfiability Problems}. In Science \textbf{297} (2002), pp 812-815.


\bibitem{Metropolis1st}
N.~Metropolis, A.~W.~Rosenbluth, M.~N.~Rosenbluth,  A.~H.~Teller, E.~Teller
{\em Equation of state calculations by fast computing machines.}
J.~Chem.~Phys. \textbf{21} (1953) pp 1087-1092.



\bibitem{MRT} A. Montanari, R. Restrepo and P. Tetali. {\em Reconstruction and
clustering thresholds in random CSPs}. SIAM J.\ Discrete Math.
{\bf 25} (2011), pp 771-808.



\bibitem{MotwRandAlgBook} R.~Motwani and P.~Raghavan. {\em Randomized Algorithms}.
Cambridge University Press, 1995.



\bibitem{janson}S.~Janson, T.~Luczak and A.~Ruci\'nski. 
{\em Random graphs},
Wiley and Sons, Inc. 2000.



\bibitem{monotone-circuit} B.~Rossman. {\em The Monotone Complexity of k-Clique 
on Random Graphs}. In Proc. of FOCS 2010, pp 193-201. 



\bibitem{TalagrandIneq} M.~Talagrand.
{\em Concentration of measure and isoperimetric inequalities in product spaces}.
Ins. Hautes \' Etudes Sci. Publ. Math. \textbf{81} 
(1995) pp 73-205.



\bibitem{AverAlg2} J.~S.~Turner. {\em Almost all k-colorable graphs are easy to color}
J. Algorithms, \textbf{9}, (1988), pp 63-82.

\bibitem{Wilf}
H.~S.~Wilf.
{\em Backtrack: An expected O(1) time algorithm for the graph coloring problem}
Inf. Proc. Lett. {\bf 18} (1984), pp 119-122.

\end{thebibliography}



\end{document}
