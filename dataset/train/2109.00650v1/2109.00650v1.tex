\documentclass{article}
\usepackage{fullpage}


\usepackage{graphicx}  \usepackage{amssymb}
\usepackage{amsmath}  \usepackage{amstext}  \usepackage{amsthm}   \usepackage{natbib}
\usepackage[flushleft]{threeparttable}  \usepackage{float}  \usepackage{hyperref}  \newtheorem{definition}{Definition}
\usepackage{algorithm,algorithmic}
\newtheorem{thm}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{cor}{Corollary}
\newtheorem{ass}{Assumption}
\newtheorem{obs}{Observation}
\hypersetup{colorlinks = true, citecolor = blue}  \DeclareMathOperator*{\argmin}{arg\,min}  \DeclareMathOperator{\Trace}{Trace} \usepackage{subfigure}
\usepackage{xcolor}
\usepackage{booktabs} \usepackage{caption}





\def \E {\mathrm{E}}
\def \x {\mathbf{x}}
\def \g {\mathbf{g}}
\def \Lmath {\mathcal{L}}
\def \D {\mathbf{D}}
\def \z {\mathbf{z}}
\def \u {\mathbf{u}}
\def \H {\mathcal{H}}
\def \w {\mathbf{w}}
\def \R {\mathbb{R}}
\def \S {\mathcal{S}}
\def \regret {\mbox{regret}}
\def \Uh {\widehat{U}}
\def \W {\mathcal{W}}
\def \N {\mathcal{N}}
\def \A {\mathcal{A}}
\def \B {\mathcal{B}}
\def \q {\mathbf{q}}
\def \v {\mathbf{v}}
\def \M {\mathcal{M}}
\def \c {\mathbf{c}}
\def \ph {\widehat{p}}
\def \d {\mathbf{d}}
\def \p {\mathbf{p}}
\def \q {\mathbf{q}}
\def \db {\bar{\d}}
\def \dbb {\bar{d}}
\def \I {\mathcal{I}}
\def \xt {\widetilde{\x}}
\def \f {\mathbf{f}}
\def \a {\mathbf{a}}
\def \b {\mathbf{b}}
\def \ft {\widetilde{\f}}
\def \bt {\widetilde{\b}}
\def \h {\mathbf{h}}
\def \bts {\widetilde{b}}
\def \fts {\widetilde{f}}
\def \Gh {\widehat{G}}
\def \bh {\widehat{b}}
\def \fh {\widehat{f}}
\def \vb {\bar{v}}
\def \zt {\widetilde{\z}}
\def \zts {\widetilde{z}}
\def \s {\mathbf{s}}
\def \gh {\widehat{\g}}
\def \vh {\widehat{\v}}
\def \Sh {\widehat{S}}
\def \rhoh {\widehat{\rho}}
\def \hh {\widehat{\h}}
\def \C {\mathcal{C}}
\def \V {\mathcal{V}}
\def \t {\mathbf{t}}
\def \xh {\widehat{\x}}
\def \Ut {\widetilde{U}}
\def \wt {\widetilde{w}}
\def \Th {\widehat{T}}
\def \Ot {\tilde{\mathcal{O}}}
\def \X {\mathcal{X}}
\def \nb {\widehat{\nabla}}
\def \P {\mathcal{P}}
\def \Q {\mathcal{Q}}
\def \T {\mathcal{T}}
\def \F {\mathcal{F}}
\def \ft{\widetilde{f}}
\def \xt {\widetilde{x}}
\def \Rt {\mathcal{R}}
\def \V {\mathcal{V}}
\def \Rb {\bar{\Rt}}
\def \wb {\bar{\w}}
\def \al {\pmb{\alpha}}
\def \K {\mathcal{K}}
\def \S {\mathbf{S}}
\def \w {\mathbf{w}}
\def \E {\mathrm{E}}
\def \v {\mathbf{v}}
\def \wh {\widehat{w}}
\def \g {\mathbf{g}}
\def \R {\mathbb{R}}
\def \u {\mathbf{u}}
\def \Fh {\widehat{F}}
\def \x {\mathbf{x}}
\def \y {\mathbf{y}}
\def \yh {\widehat{\y}}
\def \X {\mathcal{X}}
\def \Y {\mathcal{Y}}
\def \hh {\widehat{h}}
\def \fv {\vec{f}}
\def \hv {\vec{h}}
\def \hT {\widetilde{h}}
\def \nh {\widehat{\nabla}}
\def \Pt {\widetilde{\P}}
\def \Lt {\widetilde{\mathcal L}}
\def \Dt {\widetilde{D}}
\def \Alg {Dash}



\usepackage{authblk}
\begin{document}
\title{\bf Dash: Semi-Supervised Learning with Dynamic Thresholding}
\author[1]{Yi Xu \thanks{yixu@alibaba-inc.com}}
\author[1]{Lei Shang \thanks{sl172005@alibaba-inc.com}}
\author[1]{Jinxing Ye \thanks{zhengze.yjx@alibaba-inc.com}}
\author[1]{Qi Qian \thanks{qi.qian@alibaba-inc.com}}
\author[2]{Yu-Feng Li \thanks{liyf@nju.edu.cn}}
\author[1]{Baigui Sun \thanks{baigui.sbg@alibaba-inc.com}}
\author[1]{Hao Li \thanks{lihao.lh@alibaba-inc.com}}
\author[1]{Rong Jin \thanks{jinrong.jr@alibaba-inc.com}}
\affil[1]{Machine Intelligence Technology, Alibaba Group}
\affil[2]{National Key Laboratory for Novel Software Technology, Nanjing University}
\date{\today}
\maketitle




\begin{abstract}
   While semi-supervised learning (SSL) has received tremendous attentions in many machine learning tasks due to its successful use of unlabeled data, existing SSL algorithms use either all unlabeled examples or the unlabeled examples with a fixed high-confidence prediction during the training progress. However, it is possible that too many correct/wrong pseudo labeled examples are eliminated/selected. In this work we develop a simple yet powerful framework, whose key idea is to select a subset of training examples from the unlabeled data when performing existing SSL methods so that only the unlabeled examples with pseudo labels related to the labeled data will be used to train models. The selection is performed at each updating iteration by only keeping the examples whose losses are smaller than a given threshold that is dynamically adjusted through the iteration. Our proposed approach, Dash, enjoys its adaptivity in terms of unlabeled data selection and its theoretical guarantee. Specifically, we theoretically establish the convergence rate of Dash from the view of non-convex optimization. Finally, we empirically demonstrate the effectiveness of the proposed method in comparison with state-of-the-art over benchmarks. 
\end{abstract}


\section{Introduction}
In spite of successful use in a variety of classification and regression tasks, supervised learning requires large amount of \emph{labeled} training data. In many machine learning applications, labeled data can be significantly more costly, time-consuming and difficult to obtain than the \emph{unlabeled} data~\citep{zhu2005semi}, since they usually require experienced human labors from experts (e.g., a doctor in detection of covid-19 by using X-ray images). Typically, only a small amount of labeled data is available, but there is a huge amount of data without label. This is one of key hurdles in the development and deployment of machine learning models. 

Semi-supervised learning (SSL) is designed to improve learning performance by leveraging an abundance of unlabeled data along with limited labeled data~\citep{chapelle2006semi}. In much recent work, SSL can be categorized into several main classes in terms of the use of unlabeled data: consistency regularization, pseudo labeling, generic regularization (e.g., large margin regularization, Laplacian regularization, etc~\citep{chapelle2006semi}), and their combinations. With the image data augmentation technique, consistency regularization uses unlabeled data~\citep{baird1992document,schmidhuber2015deep} based on the condition that the model predictions between different perturbed versions of the same image are similar. Another line of work is to produce artificial label for unlabeled data based on prediction model and add them to 
the training data set. With different approaches of artificial label production, varies of SSL methods have been proposed in the literature including self-training~\citep{yarowsky1995unsupervised, lee2013pseudo,rosenberg2005semi, sajjadi2016regularization, laine2017temporal, xie2020self} and co-training~~\citep{blum1998combining, zhou2005tri, sindhwani2008rkhs, wang2008random, yu2008bayesian, wang2010new,  chen2011automatic}. Due to its capability to handle both labeled data and unlabeled data, SSL has been widely studied in diverse machine learning tasks such as image classification~\citep{sajjadi2016regularization,laine2017temporal, tarvainen2017mean, xie2020unsupervised, berthelot2019mixmatch, berthelot2019remixmatch}, natural language processing~\citep{turian2010word}, speech recognition~\citep{yu2010active}, and object detection~\citep{misra2015watch}. 

\begin{figure*}[t]
    \centering
    \subfigure[Number of selected unlabeled examples with correct pseudo labels]{\includegraphics[width=0.45\textwidth]{Figure1a.pdf}}
    \subfigure[Number of selected unlabeled examples with wrong pseudo labels]{\includegraphics[width=0.45\textwidth]{Figure1b.pdf}}
    \caption{An example of experimental results on Wide ResNet-28-8 for CIFAR-100 with 400 labeled images illustrates the reason of dynamically selecting unlabeled data to train learning models. Pseudo labels are generated based on the prediction models. FixMatch selects unlabeled example if its confidence prediction is greater than , while the proposed Dash algorithm selects unlabeled example based on a dynamic threshold through optimization iterations. (a) The proposed Dash selects more examples with correct pseudo labels than that of FixMatch. (b) The proposed Dash maintains much more examples with wrong pseudo labels at the beginning but it will drop off more examples with wrong pseudo labels after several epochs, comparing to FixMatch.}\label{fig:dash}
\end{figure*}
Numerous empirical evidences show that unlabeled data in SSL can help to improve the learning performance~\citep{berthelot2019mixmatch,berthelot2019remixmatch,sohn2020fixmatch}, however, a series of theoretical studies~\citep{ben2008does,singh2009unlabeled,li2011towards, balcan2005pac} have demonstrated that this success is highly relying on a necessary condition that labeled data and unlabeled data with pseudo label come from the same distribution during the training process~\citep{zhu2005semi,van2020survey}.
Unfortunately, it has been shown that this condition does not always hold in real applications and thus it could hurt the performance~\citep{li2017learning,oliver2018realistic}. For example, the pseudo label of an unlabeled example generated by conventional SSL methods during the training progress is not correct~\citep{hataya2019unifying, li2020dividemix}. In this case, the degradation of model performance has been observed when using unlabeled data compared to the simple supervised learning model not using any unlabeled data at all~\citep{chapelle2006semi,oliver2018realistic}. Thus, {not all unlabeled data are needed in SSL}.


To improve SSL performance, multiple studies~\citep{guosafe2020,ren2020not} examined the strategies of weighting different training unlabeled examples by solving a bi-level optimization problem. It is also a popular idea to select a subset of training examples from unlabeled examples for SSL. For example, FixMatch~\citep{sohn2020fixmatch} uses the unlabeled examples with a fixed high-confidence prediction (e.g., ) in classification tasks using cross entropy loss. However, the fixed threshold may lead to eliminate too many unlabeled examples with correct pseudo labels (see Figure~\ref{fig:dash} (a)) and may lead to select too many unlabeled examples with wrong pseudo labels (see Figure~\ref{fig:dash} (b)). That is to say, the fixed  threshold is possible not good enough during the training progress and thus it could degrade the overall performance.

Unlike the previous work, we aim to the proposed approach enjoys its adaptivity in terms of unlabeled data selection and its theoretical guarantee. This inspires us to consider answering the following question in this study. \begin{center}
    {\bf Can we design a provable SSL algorithm that selects unlabeled data with dynamic thresholding?}
\end{center}

To this end, we propose a generic SSL algorithm with {\bf D}yn{\bf a}mic Thre{\bf sh}olding (\Alg) that can dynamically select unlabeled data during the training process. Specifically, \Alg~firstly runs over labeled data and obtains a threshold for unlabeled data selection. It then selects the unlabeled data whose loss values are smaller than the threshold to the training data-set. The value of threshold is gradually decreased over the optimization iterations. It can be integrated with existing SSL methods like FixMatch~\citep{sohn2020fixmatch}. From the view of optimization, we show that eventually the proposed \Alg~can non-asymptotically converge with theoretical guarantee. Empirical evaluations on image benchmarks validate the effectiveness of \Alg~comparing with the state-of-the-art SSL algorithms. 
 
\section{Related Work}
There has been growing interest in semi-supervised learning for training machine learning and deep learning~\citep{flach2012machine,goodfellow2016deep}. A number of SSL methods have been studied by leveraging the structure of unlabeled data including consistency regularization~\citep{bachman2014learning,sajjadi2016regularization,laine2017temporal,tarvainen2017mean,miyato2018virtual,xie2020unsupervised}, entropy minimization~\citep{grandvalet2005semi,lee2013pseudo}, and other interesting approaches~\citep{berthelot2019mixmatch,berthelot2019remixmatch}. In addition, several studies on SSL have been proposed to keep SSL performing safe when using unlabeled data, which is known as safe SSL~\citep{li2014towards}. An non-exhaustive list of those studies include~\citep{cozman2003semi,singh2009unlabeled,li2011improving,balsubramani2015optimally, loog2015contrastive,li2017learning,krijthe2017projected,li2019towards,mey2019improvability,guosafe2020}. For example, \cite{ren2020not} proposed a new SSL framework that uses an individual weight for each unlabeled example, and it updates the individual weights and models iteratively by solving a bi-level optimization problem approximately. In this paper, we mainly focus on improved deep SSL methods with the use of unlabeled data selection. Comprehensive surveys on SSL methods could be refer to~\citep{zhu2005semi,chapelle2006semi,zhu2009introduction,hady2013semi,van2020survey}.

The use of unlabeled data selection by a threshold is not new in the literature of SSL. As a simple yet widely used heuristic algorithm, pseudo-labeling~\citep{lee2013pseudo} (a.k.a. self-training~\citep{mclachlan1975iterative,yarowsky1995unsupervised,rosenberg2005semi,sajjadi2016regularization,laine2017temporal,xie2020self}) uses the prediction model itself to generate pseudo labels for unlabeled images. Then the unlabeled images whose corresponding pseudo label's highest class probability is larger than a predefined threshold will be used for the training. Nowadays, pseudo-labeling has been become an important component of many modern SSL methods~\citep{xie2020unsupervised,sohn2020fixmatch}. 

With the use of weak and strong data augmentations, several recent works such as UDA~\citep{xie2020unsupervised}, ReMixMatch~\citep{berthelot2019remixmatch} and FixMatch~\citep{sohn2020fixmatch} have been proposed in image classification problems. Generally, they use a weakly-augmented~\footnote{Both UDA and ReMixMatch use crop and flip as ``weak" augmentation while FixMatch uses flip and shift.} unlabeled image to generate a pseudo label and enforce consistency against strongly-augmented~\footnote{For ``strong" augmentation, UDA uses RandAugment~\citep{cubuk2020randaugment}, ReMixMatch uses CTAugment~\citep{cubuk2019autoaugment}, and FixMatch uses both.} version of the same image.  In particular, UDA and FixMatch use a fixed threshold to retain the unlabeled example whose highest probability in the predicted class distribution for the pseudo label is higher than the threshold. For example, UDA sets this threshold to be 0.8 for CIFAR-10 and SVHN, and FixMatch sets the threshold to be 0.95 for all data-sets. To encourage the model to generate high-confidence predictions, UDA and ReMixMatch sharpen the guessed label distribution by adjusting its temperature and then re-normalize the distribution. \cite{sohn2020fixmatch} have shown that the sharpening and thresholding pseudo-labeling have a similar effect.

By contrast, the proposed \Alg~method selects a subset of unlabeled data to be used in training models by a data-dependent dynamic threshold, and its theoretical convergence guarantee is established for stochastic gradient descent under the non-convex setting, which is applicable to deep learning.  

\section{Preliminary and Background}
\subsection{Problem Setting}
We study the task of learning a model to map an input  onto a label . In many machine learning applications,  refers to the feature and  refers to the label for classification or regression. For simplicity, let  denote the input-label pair , i.e. . We denote by  the underlying distribution of data pair , then . The goal is to learn a model  via minimizing an optimization problem whose objective function  is the expectation of random loss function :

where  is an expectation taking over random variable . The optimization problem (\ref{prob:eq:labeled}) covers most machine learning and deep learning applications. 
In this paper, we consider the classification problem with -classes, whose loss function is the cross-entropy loss given by

where  is the prediction function and  is the cross-entropy between  and . 
In this paper, we do not require the function  to be convex in terms of , which is applicable to various deep learning tasks.   

In SSL, it consists of labeled examples and unlabeled examples. Let

be the {labeled} training data. Given {unlabeled} training examples , one can generate pseudo label  based on the predictions of a supervised model on labeled data. Different SSL methods such as pseudo-labeling~\citep{lee2013pseudo},  Adversarial Training~\citep{miyato2018virtual}, UDA~\citep{xie2020unsupervised},  
and FixMatch~\citep{sohn2020fixmatch} have been proposed to generate pseudo labels. We denote by 

the unlabeled data, where  is the pseudo label. Although it contains pseudo label, we still call  unlabeled data for simplicity in our analysis. Usually, the number of unlabeled examples is much larger than the number of labeled examples, i.e., . Finally, the training data consists of labeled data  and unlabeled data with pseudo label , and thus the training loss of an SSL algorithm usually contains supervised loss  and unsupervised loss  with a weight : , where  is constructed on  and  is constructed on . In image classification problems,  is just the standard cross-entropy loss:

where  and  is defined in (\ref{app:loss:CE}). Thus, different constructions of the unsupervised loss  lead to different SSL methods. Typically, there are two ways of constructing : one is to use pseudo labels to formulate a  ``supervised" loss such as cross-entropy loss (e.g., FixMatch), and another one is to optimize a regularization that does not depend on labels such as consistency regularization (e.g., -Model). Next, we will introduce a recent SSL work to interpret how to generate pseudo labels and construct unsupervised loss .


\subsection{FixMatch: An SSL Algorithm with Fixed Thresholding} 
Due to its simplicity yet empirical success, we select FixMatch~\citep{sohn2020fixmatch} as an SSL example in this subsection. Besides, we consider FixMatch as a warm-up of the proposed algorithm, since FixMatch uses a fixed threshold to ratain unlabeled examples and it will be used as a pipeline in the proposed algorithm.

The key idea of FixMatch is to use a separate weak and strong augmentation when generating model's predicted class distribution and one-hot label in unsupervised loss. Specifically, based on a supervised model  and a weak augmentation , FixMatch predict the class distribution

for a weakly-augmented version of a unlabeled image , where  is the prediction function. Then it creates a pseudo label by

Following by~\citep{sohn2020fixmatch}, the  applied to a probability distribution produces a ``one-hot" probability distribution. 
To construct the unsupervised loss, it computes the model prediction for a strong augmentation  of the same unlabeled image :

The unsupervised loss is defined as the cross-entropy between  and : 

 Eventually, FixMatch only uses the unlabeled examples with a high-confidence prediction by selecting based on a {\bf fixed threshold} . Therefore, in FixMatch the cross-entropy loss with pseudo-label and confidence for unlabeled data is given by

where  is an indicator function. 

As we discussed in introduction, this fixed threshold may lead to eliminate/select too many unlabeled examples with correct/wrong pseudo labels (see Figure~\ref{fig:dash}), which eventually could drop off overall performance. It is a natural choice: the threshold is not fixed across the optimization iterations. Thus, in the next section, we are going to propose a new SSL scheme having a dynamic threshold. 


\begin{algorithm*}[t]
\caption{Dash: Semi-Supervised Learning with {\bf D}yn{\bf a}mic Thre{\bf sh}olding}\label{alg:dash}
\begin{algorithmic}
\STATE Input: learning rate  and mini-batch size  for stage one, learning rate  and parameter  of mini-batch size for stage two, two parameters  and  for computing threshold, and violation probability .
\STATE {\color{gray}// Warm-up Stage: run SGD in  iterations.}
\STATE Initialization: 
\FOR{}
    \STATE Sample  examples   from ,
    \STATE  where 
\ENDFOR
\STATE {\color{gray}// Selection Stage: run SGD in  iterations.}
\STATE Initialization:  .
\STATE Compute the value of  as in (\ref{eqn:rho:hat}). {\color{gray}// In practice,  can be obtained as in (\ref{eqn:rho:0}).} 
\FOR{}
	\STATE 1) Sample  examples from , where the pseudo labels in  are generated by FixMatch 
    \STATE 2) Set the threshold .
    \STATE 3) Compute truncated stochastic gradient  as (\ref{grad:truncated}). 
    \STATE 4) Update solution by SGD using stochastic gradient  and learning rate : .
\ENDFOR
\STATE Output: 
\end{algorithmic}
\end{algorithm*}

\section{Dash: An SSL Algorithm with Dynamic Thresholding}
Before introducing the proposed method, we would like to point out the importance of unlabeled data selection in SSL from the theoretical view of optimization. Classical SSL methods~\citep{zhu2005semi,chapelle2006semi,zhu2009introduction,hady2013semi,van2020survey} assume that labeled data and unlabeled data are from the same distribution. That is to say,  holds for . Then SSL methods aim to solve the optimization problem (\ref{prob:eq:labeled}) by using a standard stochastic optimization algorithm like mini-batch stochastic gradient descent (SGD). Specifically, at iteration , mini-batch SGD updates intermediate solutions by

where  is the mini-batch size,  is sampled from training data ,  is the gradient of  in terms of . In this situation, the theoretical convergence guarantee of SSL algorithms can be simply established under mild assumptions on objective function  such as smoothness and bounded variance~\citep{ghadimi2016mini}. If the labeled data and unlabeled data are not from the same distribution such as some of pseudo labels are not correct, classical SSL methods with standard stochastic optimization algorithm may lead to the performance drops~\citep{chapelle2006semi,oliver2018realistic}. Besides, the theoretical guarantee of the optimization algorithm for this case is not clear. This inspires us to design a new algorithm to overcome this issue. To this end, we proposed an SSL method that can dynamically select unlabeled examples during the training progress. 


First, let us define the loss function for the proposed method. Same as FixMatch, the supervised loss  for the proposed method is the standard cross-entropy loss on labeled data~: 

where  is sampled from , , and  is the weakly-augmented version of . 
Since the new dynamic threshold is not fixed, we let it rely on the optimization iteration  and it is denoted by . Then the unsupervised loss is given by

where  is sampled from ,  ,  is the strongly-augmented version of , and the pseudo label  is generated based on (\ref{fixmatch:h}) and (\ref{fixmatch:y}) using prediction model  and a unlabeled image . The unsupervised loss (\ref{dash:unsup}) shows that the \Alg~will retain the unlabeled example whose loss is smaller than the threshold .
If we rewrite the indicator function  in (\ref{fixmatch:loss:unsup}) to an equivalent expression

we can consider  as a cross-entropy loss for one-hot label. Roughly speaking, FixMatch retains the unlabeled images with the loss  smaller than . It is worth nothing that the loss  contains the information of weakly-augmented images, while the loss  in (\ref{dash:unsup}) includes the information of both  weakly-augmented and strongly-augmented images, meaning that the proposed method considers the entire loss function.



\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{Figure2.pdf}
    \caption{Comparison of fixed threshold and dynamic threshold. Fixed threshold is in the scale of negative log: , dynamic threshold .}\label{fig:threshold}
\end{figure}
We then need to construct . Intuitively, with the increase of the optimization iteration , the loss function would decrease in general, so that  is also required to decrease. Mathematically, we set the {\bf dynamic threshold}  as a decreasing function of , which is given by

where  are two constants. For example, we set  in our experiments and thus at first iteration (i.e., ) the unlabeled examples whose loss values are smaller than  will be used in the training. Figure~\ref{fig:threshold} shows a comparison of fixed threshold used in FixMatch and dynamic threshold  in (\ref{rho:t}) with  and different , where the threshold in FixMatch is in the scale of negative log. It seems our thresholding strategy matches the curve of training loss in many real applications (e.g., see Figure 1 (a) of~\citep{zhang2016understanding}). 

Next, it is important to estimate the value of . In theory, it can be estimated by 
where contains several parameters related to the property of considered problem~(\ref{prob:eq:labeled}) whose detailed definitions can be found in Theorem~\ref{thm:main}. Please note that the estimation of  in (\ref{eqn:rho:hat}) is for the use of convergence analysis only. In practice, we can use the following averaged loss from the training set  as the approximate :
 
where  is the number of examples in , and  can be learned on . We can see from (\ref{eqn:rho:0}) with (\ref{dash:unsup}) and (\ref{rho:t}) that the unlabeled examples whose losses are smaller than the averaged loss of labeled examples will be maintained during the training process. 


Finally, it is ready to describe the proposed \Alg~algorithm in details that contains two stages: warm-up stage and selection stage. In the warm-up stage, it runs SGD to train a model over labeled data  in certain steps.  Not only for warm-up, this stage is also used for estimating  in (\ref{eqn:rho:0}). 
In the selection stage, we conduct SGD against  using  as the initial solution. At each iteration , we sample  training examples from , where  is a parameter defined in (\ref{def:m}). We compute the stochastic gradients according to (\ref{dash:unsup}):

Since  is small, in practice we can also construct the stochastic gradient by using all labeled data as 

where  and . 
The solution is then updated by mini-batch SGD, whose update step is given by

The detailed updating steps of the proposed algorithm are presented in Algorithm~\ref{alg:dash}, which called SSL with {\bf D}yn{\bf a}mic Thre{\bf sh}olding (\Alg).



\section{Convergence Result}
To establish the convergence result of the proposed Dash algorithm, we need to give some preliminaries. 
Recall that the training examples for the labeled data  follow the distribution , and we aim to minimize the optimization problem (\ref{prob:eq:labeled}).
For the examples coming from the unlabeled data , suppose it is a mixture of two distributions,  and . More specifically, with a probability , we will sample an example from  and with a probability  sample from :

We define the objective function  as the expected loss for distribution , i.e.


For the simplicity of convergence analysis, we do not consider the weak and strong augmentations, i.e., let  in (\ref{dash:sup}) and (\ref{dash:unsup}). Without loss of generality, we assume that our loss function is non-negative and is bounded by , i.e.  for any  and . Then by (\ref{prob:eq:labeled}) and (\ref{prob:eq:unlabeled}), we have  and . In order to differentiate the two distribution, we follow the idea of Tsybakov noisy condition~\citep{mammen1999smooth,tsybakov2004optimal}, and assume, for any solution , if , then

where  is an indicator function, , and  is constant.

Finally, we made a few more assumptions that are commonly used in the studies of non-convex optimization (e.g., deep learning)~\citep{ghadimi2013stochastic,yuan2019stagewise}. Throughout this paper, we also make the assumptions on the problem (\ref{prob:eq:labeled}) as follows.
\begin{ass}\label{ass:2}
Assume the following conditions hold: 
\begin{itemize} 
\item[(i)]  The stochastic gradient  is unbiased, i.e.,  and there exists a constant , such that  
\item[(ii)]  is smooth with a -Lipchitz continuous gradient, i.e., it is differentiable and there exists a constant  such that 
\end{itemize}
\end{ass}
Assumption~\ref{ass:2} (i) assures that the stochastic gradient of the objective function is unbiased and the gradient of  in terms of  is upper bounded. Assumption~\ref{ass:2} (ii) says the objective function is -smooth, and it has an equivalent expression which is , 
   

We now introduce an important property regarding , i.e. the Polyak-{\L}ojasiewicz (PL) condition~\citep{polyak1963gradient} of . 
\begin{ass}\label{ass:3}
There exists  such that 
\end{ass}
This PL property has been theoretically and empirically observed in training deep neural networks~\citep{allen2019convergence, yuan2019stagewise}. This condition is widely used to establish convergence in the literature of non-convex optimization, please see~\citep{yuan2019stagewise, wang2019spiderboost, karimi2016linear, li2018simple, charles2018stability} and references therein.  

Now, we are ready to provide the theoretical result for \Alg. Without loss of generality, let  in the analysis. Please note that this is a common property observed in training deep neural networks~\citep{zhang2016understanding,allen2019convergence,du2019gradient,arora2019fine,chizat2019lazy, hastie2019surprises,yun2019small}.
The following theorem states the convergence guarantee of the proposed \Alg~algorithm. We include its proof in the Appendix. 
\begin{thm}\label{thm:main}
Under Assumptions~\ref{ass:2} and \ref{ass:3}, suppose that  and , for any , , , let , ,

in Algorithm~\ref{alg:dash}, then with a probability , we have
    .
\end{thm}
{\bf Remark. } We can see from the above result that one can set  the iteration number  to be large enough to ensure the convergence of \Alg. Specifically, in order to have an  optimization error, one can set , then . The total sample complexity of \Alg~is 
 This rate matches the result of supervised learning in~\citep{karimi2016linear} when analyzing the standard SGD under Assumptions~\ref{ass:2} and \ref{ass:3}.





\begin{table*}[t]
\caption{Comparison of top-1 testing error rates for different methods using Wide ResNet-28-2 for CIFAR-10, Wide ResNet-28-8 for CIFAR-100 (in , mean  standard deviation).}\label{table:CIFAR}
\begin{center}
  \def\sym#1{\ifmmode^{#1}\else\fi}
  \begin{tabular}{cccccccccc}
    \hline
   &  \multicolumn{3}{c}{CIFAR-10} & \multicolumn{3}{c}{CIFAR-100}  \\
     \cmidrule(lr){2-4}\cmidrule(lr){5-7} 
  Algorithm  & \multicolumn{1}{c}{40 labels} & \multicolumn{1}{c}{250 labels} & \multicolumn{1}{c}{4000 labels} & \multicolumn{1}{c}{400 labels}
     & \multicolumn{1}{c}{2500 labels} & \multicolumn{1}{c}{10000 labels}  \\
    \hline
    -Model & - & 54.263.97 & 14.010.38 & - & 57.250.48 & 37.880.11  \\
    Pseudo-Labeling & - & 49.780.43 & 16.090.28 & - & 57.380.46 & 36.210.19 \\
    Mean Teacher & - & 32.322.30 & 9.190.19 & - & 53.910.57 & 35.830.24 \\
    MixMatch & 47.5411.50 & 11.050.86 & 6.420.10 & 67.611.32 & 39.940.37 & 28.310.33 \\
    UDA & 29.055.93 & 8.821.08 & 4.880.18 & 59.280.88 &  33.130.22 & 24.500.25  \\
    ReMixMatch & 19.109.64 & 5.440.05 & 4.720.13 & {\bf44.28}2.06 & {27.43}0.31 & 23.030.56  \\
    RYS (UDA) &- & 5.530.17 & 4.750.28 & - & - & -  \\
    RYS (FixMatch)& - & 5.050.12 & 4.350.06 & - &- & -  \\
    \hline
    FixMatch (CTA) & 11.393.35 & 5.070.33 & 4.310.15 & 49.953.01 & 28.640.24 & 23.180.11 \\  
  {\bf Dash (CTA, ours)}& {\bf9.16}4.31  &   4.780.12 &   4.130.06 &   44.831.36   & 27.850.19    &   22.770.21   \\
    \hline
    FixMatch (RA) & 13.813.37 & 5.070.65& 4.260.05& 48.851.75& 28.290.11&22.600.12\\
    {\bf Dash (RA, ours)}& 13.223.75& {\bf4.56}0.13& {\bf4.08}0.06& 44.760.96 & {\bf27.18}0.21& {\bf 21.97}0.14\\
    \hline
  \end{tabular}
  \end{center}
\end{table*}


\section{Experiments}
In this section, we present some experimental results for image classification tasks. To evaluate the efficacy of \Alg, we compare it with several state-of-the-art (SOTA) baselines on several standard SSL image classification benchmarks including CIFAR-10, CIFAR-100~\citep{krizhevsky2009learning}, SVHN~\citep{Netzer201137648}, and STL-10~\citep{coates2011analysis}. Specifically, SOTA baselines are MixMatch~\citep{berthelot2019mixmatch}, UDA~\citep{xie2020unsupervised}, ReMixMatch~\citep{berthelot2019remixmatch}, FixMatch~\citep{sohn2020fixmatch} and the algorithm RYS from~\citep{ren2020not}\footnote{Since the authors did not name their algorithm, we use RYS to denote their algorithm for simplicity, where RYS is the combination of initials for 
last names of the authors.}. Besides, -Model~\citep{rasmus2015semi}, Pseudo-Labeling~\citep{lee2013pseudo} and Mean Teacher~\citep{tarvainen2017mean} are included in the comparison. 

\subsection{Data-sets} 
The original CIFAR data-sets have 50,000 training images and 10,000 testing images of 3232 resolutions, and CIFAR-10 has 10 classes containing 6,000 images each, while CIFAR-100 has 100 classes containing 600 images each. The original SVHN data-set has 73,257 digits for training and 26,032 digits for testing, and the total number of classes is 10. The original STL-10 data set has 5,000 labeled images from 10 classes and 100,000 unlabeled images, which contains out-of-distribution unlabeled images. 

Following by~\citep{sohn2020fixmatch}, we train ten benchmarks with different settings: CIFAR-10 with 4, 25, or 400 labels per class, CIFAR-100 with 4, 25, or 100 labels per class, SVHN with 4, 25, or 100 labels per class, and the STL-10 data set. For example, the benchmark CIFAR-10 with 4 labels per class means that there are 40 labeled images in CIFAR-10 and the remaining images are unlabeled, and then we denote this data set by CIFAR-10 with 40 labels. For fair comparison, same sets of labeled images from CIFAR, SVHN and STL-10 were used for the proposed \Alg~ method and other baselines in all experiments.

\subsection{ Models and Hyper-parameters}
We use the Wide ResNet-28-2 model~\citep{zagoruyko2016wide} as the backbone for CIFAR-10 and SVHN, Wide ResNet-28-8 for CIFAR-100, and Wide ResNet-37-2 for STL-10. In the proposed \Alg, we use FixMatch~\footnote{In our experiments, the FixMatch codebase is used: \url{https://github.com/google-research/fixmatch}} as our pipeline to generate pseudo labels and to construct supervised and unsupervised losses. 
We employ CTAugment (CTA)~\citep{cubuk2019autoaugment} and RandAugment (RA)~\citep{cubuk2020randaugment} for the strong augmentation scheme following by~\citep{sohn2020fixmatch}. Similar to~\citep{sohn2020fixmatch}, we use the same training protocol such as optimizer, learning rate schedule, data preprocessing, random seeds, and so on. 


The total number of training epochs is set to be 1024 and the mini-bach size is fixed as 64. For the value of weight decay, we use  for CIFAR-10, SVHN and STL-10,  for CIAR-100. The SGD with momentum parameter of  is employed as the optimizer. The cosine learning rate decay schedule~\citep{loshchilov2016sgdr} is used as~\citep{sohn2020fixmatch}. The initial learning rate is set to be  for all data-sets. We use (\ref{grad:truncated}) to compute stochastic gradients. 

At the first  epochs, we do not implement the selection scheme and thus the algorithm uses all selected training examples, meaning that the threshold is infinite, i.e., . After that, we use the threshold to select unlabeled examples, and we choose  in  to reduce the dynamic threshold until its value to be . That is to say, in practice we give a minimal value of dynamic threshold, which is \footnote{In practice, we use .}. We fix the constant  as  and estimate the value of  by using (\ref{eqn:rho:0}). We decay the dynamic threshold every 9 epochs. We use the predicted label distribution as soft label during the training and it is sharpened by adjusting its temperature of , which is similar to MixMatch. Once the dynamic threshold is reduced to , we turn it to one-hot label in the training since the largest label probability is close to . 


\begin{table*}[t]
\caption{Comparison of top-1 testing error rates for different methods using Wide ResNet-28-2 for SVHN and Wide ResNet-37-2 for STL-10 (in , mean  standard deviation).}\label{table:SVHN}
\begin{center}
  \def\sym#1{\ifmmode^{#1}\else\fi}
  \begin{tabular}{ccccccccccc}
    \hline
    & \multicolumn{3}{c}{SVHN} & \multicolumn{1}{c}{STL-10} \\
     \cmidrule(lr){2-4}  \cmidrule(lr){5-5}
  Algorithm   & \multicolumn{1}{c}{40 labels} & \multicolumn{1}{c}{250 labels} & \multicolumn{1}{c}{1000 labels} & \multicolumn{1}{c}{1000 labels} \\
    \hline
    -Model &  - & 18.961.92 & 7.540.36  & 26.230.82 \\
    Pseudo-Labeling &  - & 20.211.09 & 9.940.61 & 27.990.83\\
    Mean Teacher &  - & 3.570.11 & 3.420.07 & 21.432.39\\
    MixMatch &  42.5514.53 & 3.980.23 & 3.500.28 & 10.410.61 \\
    UDA &  52.6320.51 & 5.692.76& 2.460.24 & 7.660.56\\
    ReMixMatch &  3.340.20 & 2.920.48 & 2.650.08 & 5.230.45 \\
    RYS (UDA) &- & 2.450.08 &  2.320.06 & - \\
    RYS (FixMatch)&  - & 2.630.23 &   2.340.15 & -  \\
   \hline
    FixMatch (CTA) &  7.657.65 & 2.640.64 & 2.360.19 & 5.170.63 \\  
  {\bf Dash (CTA, ours)}&    3.141.60 &   2.38 &   2.140.09 & {\bf3.96}0.25 \\
    \hline
    FixMatch (RA) &  3.962.17 & 2.480.38 & 2.280.11 & 7.981.50 \\  
  {\bf Dash (RA, ours)}&    {\bf3.03}1.59 &   {\bf2.17}0.10 &   {\bf2.03}0.06  & 7.260.40  \\
  \hline
  \end{tabular}
  \end{center}
\end{table*}


\subsection{ Results} We report the top-1 testing error rates of the proposed \Alg~along within other baselines for CIFAR in Table~\ref{table:CIFAR} and for SVHN and STL-10 in Table~\ref{table:SVHN}, where all the results of baselines are from~\citep{sohn2020fixmatch} except that the results of RYS are from~\citep{ren2020not}. All top-1 testing error rates are averaged over 5 independent random trails with their standard deviations using the same random seeds as baselines used. 

We can see from the results that the proposed \Alg~method has the best performance on CIFAR-10, SVHN and STL-10. For CIFAR-100, the proposed \Alg~is comparable to ReMixMatch, where ReMixMatch performs a bit better on 400 labels and \Alg~using RA is a bit better on 2500 labels and 10000 labels. This reason is that the proposed \Alg~uses FixMatch as its pipeline, and ReMixMatch uses distribution alignment (DA) to encourages the model to predict balanced class distribution (the class distribution of CIFAR-100 is balanced), while FixMatch and \Alg~do not use DA. We further conduct 
\Alg~with DA technique on CIFAR-100 with 400 labels, and the top-1 testing error rate is , which is better than ReMixMatch (). We also find that Dash performs well on the data set with out-of-distribution unlabeled images, i.e., STL-10. The result in Table~\ref{table:SVHN} shows that \Alg~with CTA has the SOTA performance of  on top-1 testing error rate.

Besides, the proposed \Alg~can always outperform FixMatch, showing that the use of dynamic threshold is important to the overall performance. We find the proposed \Alg~ has large improvement when the labeled examples is small (the data-sets with 4 labels per classes), comparing to FixMatch. By using CTA, on CIFAR-100 with 400 labels, on CIFAR-10 with 40 labels, and on SVHN with 40 labels, the proposed \Alg~method outperforms FixMatch result more than , , and  in the terms of top-1 testing error rate, respectively. While by using RA, the corresponding improved rates are , , and  respectively. These results reveal that the dynamic unlabeled example selection is an important term in SSL when the labeled data is small. 

\subsection{Ablation study}
\begin{table}[t]
\centering
\caption{Comparison of top-1 testing error rates for different values of  on CIFAR-10 (in ).}\label{table:diff:gamma}
\begin{tabular}{ccccc}
\hline
 & 1.01 & 1.1 & 1.2 & 1.3 \\ 
\hline
250 labels &  4.85 & {\bf4.76} & 4.99 & 4.82  \\
4000 labels & 4.39 &  4.28 & {\bf 4.11} & 4.31\\
\hline
\end{tabular}
\end{table}
\begin{table}[t]
\centering
\caption{Comparison of top-1 testing error rates for PL and Dash with PL on CIFAR-10 (in ).}\label{table:PL}
\begin{tabular}{ccc}
\hline
Algorithm & PL & Dash-PL \\ 
\hline
250 labels & 49.78 & {\bf 46.90}  \\
4000 labels & 16.09 & {\bf 15.59} \\
\hline
\end{tabular}
\end{table}
In this subsection, we provide two ablation studies using data sets CIFAR-10 with 250 labels and CIFAR-10 with 4000 labels. The first one is to use different  in the dynamic threshold, and the second one is to change FixMatch to Pseudo-Labeling as the pseudo label generator in Dash. 

{\bf Different values of .} Since  is a key component of the dynamic threshold, we conduct an ablation study on different values of  in Dash. For simplicity, we only implement the CTA case. We try four different values of  and summarize the results in Table~\ref{table:diff:gamma}. Comparing these results with that in Table~\ref{table:CIFAR}, we will find that the choice of  in the previous subsection is not the best one. The results also show that Dash is not so sensitive to  in a certain range.


{\bf Dash with Pseudo-Labeling.} Since Dash can be integrated with many existing SSL methods, we use Pseudo-Labeling (PL)~\citep{lee2013pseudo} as the pipeline to generate pseudo labels in Dash. The results are listed in Table~\ref{table:PL}, showing that Dash can improve PL, especially when the labeled images is small. 



\section{Conclusion}
We propose a method \Alg~that dynamically selects unlabeled data examples to train learning models. Its selection strategy keeps the unlabeled data whose loss value does not exceed a dynamic threshold at each optimization step. The proposed \Alg~method is a generic scheme that can be easily integrated with existing SSL methods. We demonstrate the use of dynamically selecting unlabeled data can help to the performance of existing SSL method FixMatch in the semi-supervised image classification benchmarks, indicating the importance of dynamic threshold in SSL. The theoretical analysis shows the convergence guarantee of the proposed \Alg~under the non-convex optimization setting. 



\section*{Acknowledgements}
The authors would like to thank the anonymous ICML 2021 reviewers for their helpful comments. 

\bibliographystyle{plainnat} 
\bibliography{ref}


\newpage
\appendix
\section{Proof of Theorem~\ref{thm:main}}\label{sec:proof}
In this section, we present the proof of our main theoretical result. To this end, we divide the analysis into two part, with the first part devoted to examining the properties of  learned in the first step and the second part devoted to the convergence for the iterations. As the setting of SSL, we assume that the number of unlabeled data is large, i.e.,  is sufficiently large. 
\subsection{Properties of Solution }
We first give the property of  in the following lemma, whose proof can be found in the Appendix. 
\begin{lemma}\label{lem:0}
Given , run the Warm-up Stage of Algorithm~\ref{alg:dash} with ,  and , then with a probability  we have
    . 
\end{lemma}
\begin{proof}
Since the objective function  has a Lipchitz continuous gradient in Assumption~\ref{ass:2} (ii), we have

where (a) follows the update of ;  (b) follows from Assumption~\ref{ass:3} and . 
Since  and

using concentration inequality in Lemma 4 of \citep{ghadimi2016mini}, 
we have with a probability ,

Using the above bound , we can further bound  by using (\ref{lem:0:ineq:0}) and (\ref{lem:0:ineq:1}) as

which implies 

Be selecting  and , we have

Therefore the condition in (\ref{eqn:condition}) is applicable in this case. On the other hand, by the definition of  in (\ref{def:rho:hat}), we know, with a probability , that

\end{proof}

\subsection{Analysis of Iterative Algorithm}
The key to our analysis is to show that for each iteration , with a high probability, we have . We will prove this statement by induction.  
Before we carry out our analysis, we define a few important constants

When , we have  according to Lemma~\ref{lem:0}. At each iteration , given the solution , according to our inductive assumption, with a probability , we have

For the  training examples sampled from , we divide it into two sets for the analysis use only, i.e. set  that includes examples sampled from  and set  that includes examples sampled from . We furthermore denote by  and  the subset of examples in  and  whose loss is smaller than the given threshold , i.e.

where  with . Evidently, the samples used for computing  is the union of  and . The following result bounds the size of  and .
With a probability , we have

where  and  are defined in (\ref{eqn:A0}) and (\ref{eqn:B0}).
Using the Hoeffding's inequality, 
we have, with a probability ,

Furthermore, using Markov inequality, 
we have

where (a) uses the fact that . Using the property in (\ref{eqn:1}), we can bound the size of  by Hoeffding's inequality, 
i.e., with a probability , we have

where  is defined in (\ref{eqn:A0}). 
Using the inequality in (\ref{eqn:condition}) we know

where (a) is due to ; (b) and (c) are due to .
Using the property in (\ref{eqn:3}), we can bound the expectation of  given , i.e.,

We can bound the size of  by a concentration inequality in Theorem 8 of~\citep{chung2006concentration}, 
with a probability ,

where  is defined in (\ref{eqn:B0}). Therefore, we complete the proof of (\ref{prop:1}).

As indicated in (\ref{eqn:2}) and (\ref{eqn:5}),  increases exponentially over iteration while  remains upper bounded by a constant. It implies that our dynamically adjusted threshold help us select more and more examples from the unlabeled data that are relevant to the labeled data. In the same time, the number of mistakes we made in the selection process remain to be at most a constant. As a result, our optimization is able to make significant progress by using the selected examples from the unlabeled data. Below we will show that with a high probability, , the key step of the inductive analysis. 
Finally, we will prove that with a probability , we have


To this end, using the notation of  and , we can rewrite  as

where , and  is the proportion of samples from  that

Following the classical analysis of non-convex optimization, since  is -smooth by Assumption~\ref{ass:2}~(ii), we have

where (a) follows the update of ;  (b) is due to the definition of  and the convexity of ; (c) follows from Assumption~\ref{ass:3}, Assumption~\ref{ass:2} (i), and . Since  and

using concentration inequality in Lemma 4 of \citep{ghadimi2016mini}, 
we have with a probability ,

Using the above bound , we can further bound  as

where  is defined in (\ref{eqn:AB}). Hence, we have

Let select 
,
we know by using (\ref{inq:induction}) the inequality (\ref{thm:2:inq:3}) will become

By the setting of  
we have

Therefore, we complete the proof by induction.
\end{document}
