\pdfoutput=1


\documentclass[11pt]{article}

\usepackage[]{EMNLP2023}

\usepackage{times}
\usepackage{latexsym}
\usepackage{dsfont}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{inconsolata}
\usepackage{microtype}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{tabularx, booktabs}

\usepackage{bbding}
\usepackage[T1]{fontenc}
\usepackage{amsfonts}
\usepackage[utf8]{inputenc}
\usepackage{graphicx} \usepackage{microtype}

\usepackage{inconsolata}
\usepackage{amsmath, amssymb, mathtools}
\usepackage{amsfonts,amssymb}
\usepackage{cleveref}
\crefname{section}{§}{§§}
\Crefname{section}{§}{§§}




\title{An Expression Tree Decoding Strategy for Mathematical Equation Generation}


\author{Wenqi Zhang, Yongliang Shen , Qingpeng Nong, Zeqi Tan\\
  {\bf Yanna Ma, \bf Weiming Lu}\\
  College of Computer Science and Technology, Zhejiang University \\
  Zhongxing Telecommunication Equipment Corporationy\\
  University of Shanghai for Science and Technology\\
  \texttt{\{zhangwenqi, luwm\}@zju.edu.cn} } 
  

\begin{document}
\maketitle
\renewcommand{\thefootnote}{\fnsymbol{footnote}} \footnotetext[2]{Corresponding author.}  \renewcommand{\thefootnote}{\arabic{footnote}}

\begin{abstract}



Generating mathematical equations from natural language requires an accurate understanding of the relations among math expressions. Existing approaches can be broadly categorized into token-level and expression-level generation. The former treats equations as a mathematical language, sequentially generating math tokens. Expression-level methods generate each expression one by one. However, each expression represents a solving step, and there naturally exist parallel or dependent relations between these steps, which are ignored by current sequential methods. Therefore, we integrate tree structure into the expression-level generation and advocate an expression tree decoding strategy. To generate a tree with expression as its node, we employ a layer-wise parallel decoding strategy: we decode multiple independent expressions (leaf nodes) in parallel at each layer and repeat parallel decoding layer by layer to sequentially generate these parent node expressions that depend on others. Besides, a bipartite matching algorithm is adopted to align multiple predictions with annotations for each layer. Experiments show our method outperforms other baselines, especially for these equations with complex structures.

























\end{abstract}

\section{Introduction}


Generating corresponding mathematical equations and solutions from text is important for a range of tasks, such as dialogues, question answering, math word problem (MWP), etc. It necessitates an accurate comprehension of semantics and the complex relations between mathematical symbols.

We investigate the existing approaches from two perspectives: the generation is at the \textbf{token-level or expression-level}, and the order is based on \textbf{sequence or tree structure}. Firstly, sequence-to-sequence methods (seq2seq in Figure \ref{Fig.f1}) \citep{wang2017deep,wang2018translating,chiang-chen-2019-semantically,li-etal-2019-modeling} have considered mathematical symbols as a special kind of language (i.e., mathematical language) and employ sequential generation for the equation. These methods belong to the token-level sequential generation. Then a great deal of work \citep{xie2019goal, zhang2020graph, patel2021nlp,li-etal-2020-graph-tree, HGEN, shen2022seeking} has proposed a tree-order decoding process (seq2tree in Figure \ref{Fig.f1}) at the token-level. This process considers it as an equation tree generation and predicts pre-order tokens one by one.

\begin{figure}[t] \centering \includegraphics[width=0.49\textwidth]{figure/fig1.pdf} \caption{Four equation generation methods: Seq2Seq generates mathematical tokens one by one at the token-level; Seq2Tree generates tokens using prefix order. In contrast, Seq2Exp generates expressions one by one at expression-level. Our expression tree decoding strategy predicts multiple math expressions in parallel and forms a tree, with the minimum decoding steps.} \label{Fig.f1} \end{figure}

Recently, some researchers have explored expression-level approaches for mathematical equation generation, including \citep{kim-etal-2020-point, cao2021bottom, jie-etal-2022-learning,zhang2022elastic, zhang-etal-2022-multi-view}. These approaches mostly place emphasis on generating a mathematical expression step by step (seq2exp), rather than a token. These seq2exp methods belong to a sequential generation at expression-level. 



However, it is imperative to recognize that each mathematical expression represents a problem-solving step, and there inherently exists a parallel or dependent relation among these steps. The existing seq2exp approach may struggle to capture these relations since they only produce expressions in sequence. Therefore, there is a pressing need for a versatile decoding strategy capable of simultaneously generating independent expressions in parallel at one step, while sequentially producing expressions that depend on others step by step.

Based on this belief, we propose an expression tree decoding strategy by combining the seq2exp with a tree structure at the expression level. Differing from the prior seq2tree, each node in this tree represents an expression, rather than a token. To construct an expression-level tree, we generate multiple expressions in parallel at each step. These expressions are independent to each other and act as the leaf node in the expression tree. Those expressions depend on others, they act as parent nodes and are sequentially generated based on their child nodes. As shown in Figure \ref{Fig.f1}, the two expressions (, ) are completely independent and generated in parallel at Step 1. The third expression depends on the first two, forming an expression tree. It not only empowers the model to exploit the inherent structure of the equation but also shortens its decoding path (the minimum steps in Figure \ref{Fig.f1}).































To achieve this, we design a layer-wise parallel decoding strategy. At each decoder's layer, it can generate multiple independent expressions in parallel and then proceeds to the next layer, and repeat parallel prediction layer by layer. This layer-wise parallel decoding process ensures that these independent expressions are produced in parallel, while these expressions depending on others are sequentially generated layer by layer, eventually constructing the entire expression tree.

Besides, to decode multiple expressions in parallel, we take inspiration from query-based object detection \citep{carion2020end, jaegle2021perceiver, li2023blip}. Similar to detecting multiple visual objects by queries, we also utilize queries to identify multiple mathematical relations for expression generation in parallel. Lastly, we adopt a bipartite matching algorithm for loss calculation between multiple predicted expressions and the label. 

\citet{cao2021bottom} shares some similarities with us but the challenges and approaches are different. \citet{cao2021bottom} perform a bottom-up structure to extract multiple equations (e.g., x+y=3, y-2=4), whereas our method considers how to predict multiple expressions in parallel for a complex equation at each step (e.g., output ,  simultaneously for ). Besides, \citet{cao2021bottom} enumerates all possible expression combinations but we first introduce bipartite matching to achieve parallel prediction. 






Our contributions are threefold:

\begin{itemize}

  \setlength{\parskip}{2pt}
  \setlength{\itemsep}{0pt plus 1pt}









\item We introduce an expression tree decoding strategy by combining seq2exp with tree structure. It considers the dependent or parallel relation among different expressions (solving steps). To the best of our knowledge, this is the first effort to integrate query-based object detection techniques with equation generation in the literature.











\item We design a layer-wise parallel decoding process to construct an expression tree. It predicts multiple independent expressions in parallel and repeats parallel decoding layer by layer. Besides, we employ bipartite matching to align predicted expressions with labels. 







\item To assess our method, we evaluate on MWP task and outperform prior baselines with higher accuracy and shorter steps.


\end{itemize}

By aligning the decoding process with the inherent structure of the equation, our approach paves the way for more intuitive, efficient equation generation. Moreover, it provides insights that could be applicable to many other structured tasks.







\section{Related work}
In advancing toward general-purpose AI, dependable reasoning remains imperative. The quest for human-equivalent reasoning has been rigorously explored in domains including NLP \citep{kojima2022large}, RL \citep{ijcai2022p654}, and Robotics \citep{Zhang2021LearningTN}. Recently, leveraging the planning and reasoning capabilities of LLMs paves the way for the development of numerous intelligent applications \citep{wei2022chain, shen2023hugginggpt, zhang2023data}. Accurate generation of mathematical equations is an important manifestation of reasoning abilities, which has been extensively investigated in a plethora of NLP tasks, e.g., Math Word Problems \citep{wang2017deep, ling-etal-2017-program, xie2019goal, wang2022structure}, Question Answering \citep{yu2018qanet, wu2020biased}, Dialogue \citep{bocklisch2017rasa, wu2022towards, wu2023focus}, etc. These tasks necessitate an accurate understanding of the semantics within the text as well as mathematical symbols. 











\indent \textbf{Token-level Generation} Mathematical equation was treated as a translation task from human language into the mathematical token (symbol) \citep{wang2017deep,chiang-chen-2019-semantically}. Many seq2seq-based methods were proposed with an encoder-decoder framework. \citet{li-etal-2019-modeling} introduced a group attention to enhance seq2seq performance. \citet{lan2021mwptoolkit} utilized a transformer model for equation generation. Except for seq2seq methods, some researchers \citep{liu-etal-2019-tree,xie2019goal,zhang2020graph,HGEN} studied the decoding structures and proposed a tree-based decoder using prefix sequence. \citet{wu2020knowledge,wu-etal-2021-math,qin-etal-2021-neural,yu-etal-2021-improving} introduced mathematical knowledge to solve the complex math reasoning. \citet{ijcai2021-485} improved accuracy by knowledge distillation between the teacher and student. \citet{li2021seeking} proposed a prototype learning through contrasting different equations. \citet{shen2022textual,liang-etal-2022-analogical} also used contrastive learning at the semantic and symbolic expression levels. \citet{yang2022logicsolver} improved the interpretability by explicitly retrieving logical rules. These generative approaches were token-level generation in infix order or prefix order.

\indent \textbf{Expression-level Generation} Expression-level generation has opened up a new perspective for math solving. \citet{kim-etal-2020-point} proposed an expression pointer generation. \citet{cao2021bottom} introduced a DAG structure to extract two quantities from bottom to top. \citet{jie-etal-2022-learning,wang2023msat} further treated this task as an iterative relation extraction to construct an expression at each step. \citet{zhang2022elastic} treated expression generation as a program generation and execution. Besides, \citet{lee2023recursion,zhang2023interpretable,he2023solving,zhu2022solving} harness the capabilities of LLMs and prompt engineering to bolster mathematical reasoning under the few-shot setting. These methods treat equation generation as a multi-step expression generation and achieve impressive performance. However, these methods generate only one expression per step using pre-defined order, which may potentially impede the model's acuity in accurately understanding mathematical logic. In contrast, our method generates multiple expressions in parallel per step.







\section{Methodology}
\begin{figure*}[t] \centering \includegraphics[width=1\textwidth]{figure/fig2.pdf} \caption{We propose an expression tree decoding strategy by layer-wise parallel decoding. During training, we feed six queries into the decoder, where each decoder's layer generates six mathematical expressions. Then, we transform the original equation label into multiple label sets and employ a bipartite matching algorithm to align the label sets with the six predicted expressions for loss calculation. Thereafter, we update the problem representation using valid expressions and feed it into the next decoding layer. The whole process forms an expression tree.} \label{Fig.f2} \end{figure*}





\subsection{Overview} \label{section3.1}
The task is to generate a complete equation based on the problem description. The generation process contains two vocabularies: number and operator vocabulary ( ). The number is either from the original text or the math expression results from previous steps. 



Similar to object detection, where queries are utilized to detect multiple objects, we also feed multiple learnable queries to identify possible math relations. As shown in Figure \ref{Fig.f2}, a standard decoder and multiple queries are adopted to generate candidate expressions at each layer. To construct an expression tree, we must consider two structures simultaneously: parallel and sequential. For these expressions that have no dependent relations, we employ a parallel strategy to generate them. Conversely, for these expressions that depend on others, we generate them layer by layer (\Cref{section3.2}). We also provide detailed cases in Figure \ref{Fig.f3} for decoding. When training, we utilize a bipartite matching algorithm to align the multiple predicted expressions with the label set for loss calculation (\Cref{section3.3}). 















\subsection{Layer-wise Parallel Decoding} \label{section3.2}
We devise a problem encoder, a decoder, and multiple learnable queries, where the query is used to identify a specific mathematical relation and then produce a candidate expression at each layer. 

\textbf{Problem Encoder} Given a text description  with  number words, we adopt the pre-trained language model \citep{devlin-etal-2019-bert,liu2019roberta} to obtain the contextualized problem representations . We obtain number vocabulary  from , which denotes the embeddings of the  number tokens from text. In addition, we randomly initialize the learnable embedding for each operator and a \emph{None} label .

\textbf{Learnable Query} The decoder is designed for extracting all candidate expressions in parallel based on problem representation. Firstly, we design learnable embeddings as query , where  means the number of the query. As shown in Figure \ref{Fig.f2}, the  queries are firstly fed into the decoder and then are utilized to predict  possible expressions at each layer. 

Specifically, the decoder is the standard transformer decoder which contains a stack of identical layers. At the -th layer, the problem embeddings  and the query embeddings  from the previous layer are fed into the current decoder's layer as inputs, and then interact with each other by self-attention and cross-attention mechanism:

where  means  query embeddings at -th layer. 

\textbf{Parallel Decoding}
After obtaining  query vectors, we use them to predict  expressions in parallel, each of which contains one operator and two operands . Firstly, we use query to calculate the operator and operands embedding:

where  denotes the -th query vectors in . Then, we predict three distributions as follows:

where  and  represent the number and operator embedding in vocabulary respectively. , , and  denotes the three distributions for two operands and one operator. Lastly, we calculate the embedding for this -th expression:


We totally predict  independent expressions from  queries at this layer. Then we continue to the next layer for these expressions depending on previous results (e.g., in Figure \ref{Fig.f2}).

\textbf{Layer by Layer Prediction} The  predicted expressions contain  valid expressions and  invalid expressions. An invalid expression implies that the operator in  is predicted to \emph{None}. We will discuss it in section (\Cref{section3.3}). First, we concat all valid expression embeddings with the problem representations  for the next layer:

where , , ....,  means  new generated expression embeddings in -th layer.  means the concatenation of the vectors. Besides, we also update the number embedding:  using  new expression embeddings as new number embeddings.

As shown in Figure \ref{Fig.f2}, we proceed to the next layer of decoding using  and , as Equation \ref{eq.1}:

At layer , we still decode  expressions in parallel and continue to the next layer. If all predicted expressions are invalid, i.e.,  operators are classified as , it signifies the equation generation is complete.


\subsection{Loss For Parallel Decoding} \label{section3.3}


As mentioned before, each decoder's layer generates multiple mathematical expressions. However, the annotations of equations in the dataset are usually serial (e.g., ``50'', ``'', ``5'', ``+'', ``60'', ``'', ``4''), and it is crucial to design a method to utilize these annotations for training parallel decoding. 


To this end, we first convert the original equation annotations into multiple label sets, each set comprising  mathematical expressions. Then, bipartite matching is employed to align the  predicted expressions with the  mathematical expressions in the label set to compute model loss.

\textbf{Label Set} 
As shown in Figure \ref{Fig.f2}, we initially convert the label equations from infix order to prefix order, thus forming an equation tree. Starting from the leaf nodes, we iterative gather two leaves and their parent node into a label set for each step, eventually producing multiple label sets (e.g. set1 = , , set2 = ). Each label set contains several non-dependent expressions that can be generated in parallel. Each label set is also padded with a specific label  to ensure all sets contain  elements. We provide two detailed cases for this process in Figure \ref{Fig.f3}.

\textbf{Bipartite Match} For each layer,  candidate mathematical expressions are predicted. We compute the function loss for the  predicted expressions based on the corresponding label set, which also contains  golden expressions. However, as the  expressions are unordered, it is difficult to calculate the loss directly. For instance, if the label set is , and the prediction is , the loss in this case should be 0. To address this, we adopt a bipartite matching algorithm to align the two sets, i.e., align the  predictions with the  golden labels. As shown in Figure \ref{Fig.f2}, six golden expressions align with six predicted expressions. Specifically, we denote the golden expression in the label set as , and the set of predicted expressions by . To find an optimal matching, we search for a permutation () of  elements with the lowest cost. As shown in Figure \ref{Fig.f2}, the optimal permutation for predicted set is . It can be formalized as:

where  is a pair matching cost between the golden expression  and the predicted expression  with index . We use the Hungarian algorithm \citep{kuhn1955hungarian} to compute this pair-matching cost. Each golden expression contains two operands and one operator, i.e.  and each predicted expression has three distributions, i.e. . We calculate  as follow:


After we get the optimal matching , we calculate the final loss  as:


We calculate the predicted loss for each decoder layer after aligning two sets. A detailed match process is provided in Figure \ref{Fig.ablation}. 


\subsection{Training and Inference}
During training, we calculate the loss after employing the bipartite match for each layer. Besides, we also adopt the teacher forcing \citep{williams1989learning} by using golden expressions for the next layer (Equation \ref{equ5}, \ref{equ6}). During inference, each layer predicts  expressions in parallel. Those expressions whose predicted operator is  are filtered out before proceeding to the next layer generation. The entire process is finished when all  expressions are predicted to be . 



\section{Experiments}
\begin{table}[t]\small
\centering
\begin{tabular}{c|l|c}
\toprule[1pt]
                         &\textbf{Model}  &\textbf{Test Acc.}  \\ \midrule[0.5pt]
\multirow{9}*{\rotatebox{90}{Seq2Seq / Tree}}   &GroupAttn\citep{li-etal-2019-modeling}&70.4\\
& GTS \citep{xie2019goal} &71.3\\
& G2T\citep{zhang2020graph}	&72.0\\
& BERT-T\citep{liang2021mwp} &73.8\\
& mBERT\citep{tan2021investigating} 	&77.1\\ 
& T-Dis\dag\citep{ijcai2021-485}  &73.1 \\
& Prototype \citep{li2021seeking}  &76.3 \\
& Textual-CL\dag \citep{shen2022textual} &78\\
& Ana-CL \citep{liang-etal-2022-analogical}&79.6 \\

\midrule[0.5pt]

\multirow{6}*{\rotatebox{90}{Seq2Exp}}  
& E-pointer\dag \citep{kim-etal-2020-point} &73.5\\
&M-Tree\dag\citep{wang2022structure}  &76.5\\
&RE-Ext\citep{jie-etal-2022-learning}  &78.6 \\
&M-View\citep{zhang-etal-2022-multi-view}&79.5\\
&Elastic \citep{zhang2022elastic}  &80.3 \\
&MWP-NAS\citep{DBLP:journals/corr/abs-2305-04556}&79.2\\
\midrule[0.5pt]

\multirow{2}*{\rotatebox{90}{LLM}}  
& gpt-3.5-turbo\dag \citep{Chatgpt} &42.6\\
& Self-Consistency\dag\citep{wang2022self}  &50.7\\
\midrule[0.5pt]

\multirow{2}*{\rotatebox{90}{}}  
& Ours &\makecell[c]{\textbf{81.5} \scriptsize 0.13}  \\ 
& Ours (Layer-Shared)  &\makecell[c]{\textbf{81.1} \scriptsize 0.23} \\ 
\bottomrule[1pt]
\end{tabular}
\caption{Results on MathQA. \dag \ means our reproduction.  means we reproduce \emph{M-View} using the standard dataset 
without data Augmentation (their report).  means Elastic use a different data pre-processing method and operators, so we reproduce their method.}
\label{tab:mathqa}
\end{table}



\begin{table}[t]\small
\centering
\begin{tabular}{c|l|c|c}
\toprule[1pt]
                         &\textbf{Model}  &\textbf{ Test}   &\textbf{ 5-fold}  \\ \midrule[0.5pt]
\multirow{15}*{\rotatebox{90}{Seq2Seq / Tree}}   &GroupAttn\citeyearpar{li-etal-2019-modeling}&69.5&66.9\\
& GTS \citeyearpar{xie2019goal} &75.6&74.3\\
& G2T\citeyearpar{zhang2020graph}	&77.4&75.5\\
& mBERT\citeyearpar{tan2021investigating} 	&75.1&-\\  
& Symbol-Dec\citeyearpar{qin-etal-2021-neural} &-&75.7\\
& BERTGen\citeyearpar{lan2021mwptoolkit}     &76.6&-\\  
& PLM-Gen \citeyearpar{lan2021mwptoolkit}	&76.9&-\\

& H-Reasoner\citeyearpar{yu-etal-2021-improving} &83.9&82.2 \\  
& BERT-T\citeyearpar{liang2021mwp} &84.4&82.3\\
&Rank\citeyearpar{shen-etal-2021-generate-rank}	&85.4&-\\ 
& Logic-Dec\citeyearpar{yang2022logicsolver} &83.4 &- \\
& T-Dis\citeyearpar{ijcai2021-485}  &79.1 &77.2\\
& Prototype \citeyearpar{li2021seeking}  &83.2&- \\
& Textual-CL \citeyearpar{shen2022textual} &85.0&82.6\dag\\
& Ana-CL \citeyearpar{liang-etal-2022-analogical}&85.6 &83.2\dag\\
\midrule[0.5pt]

\multirow{7}*{\rotatebox{90}{Seq2Exp}}
& E-Pointer\dag\citeyearpar{kim-etal-2020-point} &78.7&76.5 \\
& DAG \citeyearpar{cao2021bottom} &77.5&75.1    \\
&M-Tree\citeyearpar{wang2022structure}  &82.5 & 80.8\dag \\
&RE-Ext\citeyearpar{jie-etal-2022-learning}  &85.4 & 83.3 \\
&M-View\citeyearpar{zhang-etal-2022-multi-view}  &85.6 & 83.1 \\
&Elastic\citeyearpar{zhang2022elastic}  & 84.8 & 82.9\\
&MWP-NAS\citeyearpar{DBLP:journals/corr/abs-2305-04556}&84.4\\
\midrule[0.5pt]
\multirow{2}*{\rotatebox{90}{LLM}}  
& gpt-3.5-turbo\dag \citeyearpar{Chatgpt} &54.8& -\\
& Self-Consistency\dag\citeyearpar{wang2022self}  &66.1& -\\
\midrule[0.5pt]

\multirow{2}*{\rotatebox{90}{}}  
& Ours &\makecell[c]{\textbf{86.2} \scriptsize 0.30}  &\makecell[c]{\textbf{84.1} \scriptsize   0.65 }\\ 
& Ours (Layer-Shared) &\makecell[c]{\textbf{85.6}  \scriptsize 0.25}  &\makecell[c]{\textbf{83.4}  \scriptsize   0.38 }\\ 


\bottomrule[1pt]
\end{tabular}
\caption{Testing and five-fold Acc. on Math23k.}
\label{tab:math23k}
\end{table}









\begin{table}[t]\small
\centering
\begin{tabular}{c|l|c}
\toprule[1pt]
                         &\textbf{Model}  &\textbf{5-fold Acc.}  \\ \midrule[0.5pt]
\multirow{13}*{\rotatebox{90}{Seq2Seq / Tree}}   
& GroupAttn\citeyearpar{li-etal-2019-modeling}&76.1\\
& GTS \citeyearpar{xie2019goal} &82.6\\

& G2T\citeyearpar{zhang2020graph}	&85.6\\
& Rank\citeyearpar{shen-etal-2021-generate-rank}	&84.0\\ 
& BERTGen\citeyearpar{lan2021mwptoolkit}     &86.9\\
& PLM-Gen \citeyearpar{lan2021mwptoolkit}	&88.4 \\
& PLM-GTS \citeyearpar{liang2021mwp} &88.5 \\
& PLM-G2T \citeyearpar{liang2021mwp} &88.7 \\
& H-Reasoner\citeyearpar{yu-etal-2021-improving} &89.8 \\
& T-Dis\citeyearpar{ijcai2021-485}  &84.2 \\
& Prototype \citeyearpar{li2021seeking}  &89.6 \\
& Textual-CL \citeyearpar{shen2022textual} &91.3\\
& Ana-CL\citeyearpar{liang-etal-2022-analogical}&91.8 \\

\midrule[0.5pt]

\multirow{5}*{\rotatebox{90}{Seq2Exp}}  
& E-Pointer \citeyearpar{kim-etal-2020-point} &83.4\\
&M-Tree\citeyearpar{wang2022structure}  &82.0\\
&RE-Ext\citeyearpar{jie-etal-2022-learning}  &92.2 \\
&M-View \citeyearpar{zhang-etal-2022-multi-view}  &92.1\\
&Elastic \citeyearpar{zhang2022elastic}  &91.8 \\
&MWP-NAS\citep{DBLP:journals/corr/abs-2305-04556}&88\\
\midrule[0.5pt]
\multirow{2}*{\rotatebox{90}{LLM}}  
& gpt-3.5-turbo\dag \citeyearpar{Chatgpt} &91.5\\
& Self-Consistency\dag\citeyearpar{wang2022self}  &\textbf{92.5}\\
\midrule[0.5pt]
\multirow{2}*{\rotatebox{90}{}}  
& Ours  &\makecell[c]{92.3 \scriptsize 0.41}  \\ 
& Ours (Layer-Shared) &\makecell[c]{92.2 \scriptsize 0.28} \\ 
\bottomrule[1pt]
\end{tabular}
\caption{Five-fold cross-validation results on MAWPS.}
\label{tab:mawps}
\end{table}





 \textbf{Math Word Problem Task} We first evaluate our expression tree decoding strategy on the Math Word Problem task (MWP). MWP represents a challenging mathematical reasoning task where the model is required to comprehend the semantics of a given problem and generate a complete equation and the solution. Encompassing a spectrum of topics, including engineering, arithmetic, geometry, and commerce, MWP effectively evaluates the model's prowess in semantic understanding as well as mathematical symbol generation. We use three standard MWP datasets\footnote{The criteria for the selection of the dataset: 1. Dataset size. 2. Datasize label includes not just answers but also complete equations. 3. Extensively employed in prior research.} across two languages: MathQA \citep{amini-etal-2019-mathqa}, Math23K \citep{wang2017deep}, and MAWPS \citep{koncel2016mawps}. We follow \citep{jie-etal-2022-learning, zhang-etal-2022-multi-view} to preprocess datasets. The statistics of datasets are reported in \Cref{dataset}.

\textbf{Baselines} We compare our method with three types of baselines: (1) \textbf{Seq2Seq/Tree}: PLM-Gen \citep{lan2021mwptoolkit}, Rank \citep{shen-etal-2021-generate-rank}, Symbol-Dec \citep{qin-etal-2021-neural}, H-Reasoner \citep{yu-etal-2021-improving}, Logic-Dec \citep{yang2022logicsolver}, Prototype \citep{li2021seeking}, T-Dis \citep{ijcai2021-485}, Textual-CL \citep{shen2022textual}, Ana-CL \citep{liang-etal-2022-analogical} and several representative methods. (2) \textbf{Seq2Exp}: \textbf{E-Pointer} \citep{kim-etal-2020-point}, DAG \citep{cao2021bottom}, RE-Ext \citep{jie-etal-2022-learning}, M-View \citep{zhang-etal-2022-multi-view}, ELASTIC\citep{zhang2022elastic}, M-Tree \citep{wang2022structure} and  MWP-NAS \citep{DBLP:journals/corr/abs-2305-04556}. Besides, we also compare with gpt-3.5-turbo \citep{Chatgpt} and Self-Consistency \citep{wang2022self} prompted by one demonstration through the OpenAI API. More details are listed in \Cref{baseline}.





\textbf{Training Details} Following most previous works \citep{zhang-etal-2022-multi-view,jie-etal-2022-learning}, we report the average accuracy (five random seeds) with standard deviation for Math23K and MathQA, and 5-fold cross-validation for Math23K and MAWPS. The test-set accuracy is chosen by the best dev-set accuracy step. Since most of the math problems only require two or three mathematical expressions to be generated in parallel for each step, we set the number of queries  to be 6, which is sufficient to cover all cases. Except for using a standard decoder for layer-wise decoding (\textbf{Our}), we also explore an alternate variant (\textbf{Our Layer-Shared}), in which parallel decoding is performed at every  transformer layer, but each decoding step shares the parameter of these  layers. This model is efficient with fewer parameters. Model details are reported in Figure \ref{Fig.two_models}.





\subsection{Results}
As shown in Table \ref{tab:mathqa}, \ref{tab:math23k} and \ref{tab:mawps}, our expression tree decoding strategy achieves SoTA performance on two large datasets, especially on the most difficult MathQA with +1.2\% gains. Similarly, we gain +0.6\% (test) and +0.8\% (5-fold) improvements on Math23K and comparable performance on the MAWPS. Moreover, we also notice that our performance still substantially outperforms LLMs in the case of complex mathematical equation generation (MathQA: +30.8\% and Math23K: +20.1\%). Furthermore, our variant model (Our w/ Layer-Shared) has also demonstrated comparable performance (+0.8\% on MathQA), with fewer parameters.


From the view of three types of methods, our method is more stable and effective, with +1.9\% and +1.2\% gains against the best Seq2Tree baseline (Ana-CL) and best Seq2Exp baseline (Elastic) on MathQA. The Seq2Exp focuses on generation at the sequence at the expression-level, while Seq2Tree absorbs the feature of a tree structure. In contrast, our expression tree decoding strategy, more than generating the expressions, also integrates the tree structure into the parallel decoding process, performing well for complex equations.

\begin{table}[t]\small
\centering
\begin{tabular}{c|c|c|c|c}
\toprule
\textbf{Math23K} & \textbf{Seq2Exp} & \textbf{Our} & \textbf{Seq2Seq} & \textbf{Seq2Tree} \\
\midrule
Avg Step & 2.4 & \textbf{1.92} & 7.01 & 5.62 \\
Std Step & 1.22 & \textbf{0.8} & 3.4 & 2.1 \\
Max Step & 9 & \textbf{8} & 27 & 19 \\
\toprule[1pt]
\textbf{MathQA} & \textbf{Seq2Exp} & \textbf{Our} & \textbf{Seq2Seq} & \textbf{Seq2Tree} \\
\toprule[1pt]
Avg Step & 4.33 & \textbf{3.2} & 16.74 & 9.87 \\
Std Step & 2.26 & \textbf{1.6} & 11.03 & 5.51 \\
Max Step & 11 & \textbf{8} & 109 & 55 \\
\bottomrule
\end{tabular}
\caption{The statistics for decoding steps between four types of methods.}
\label{table:decoding step}
\end{table}
 In addition to the accuracy comparison, we also analyze the difference in the number of decoding steps. We report the average decoding steps, step standard deviation, and maximum steps for the four types of methods (Seq2Seq: mBERT, Seq2Tree: Ana-CL, Seq2Exp: RE-Ext, and Expression-tree: Ours) in \Cref{table:decoding step}. We observe that the decoding steps of the token-level generation methods (e.g., Seq2Seq and Seq2Tree) are significantly higher than those of Expression-level methods (about four to five times). Compared to other methods, our parallel decoding method requires fewer decoding steps, especially on the more complex MathQA. We offer some intuitive examples in \Cref{Fig.f3}. These results suggest that our parallel strategy \textbf{not only offers superior accuracy but also reduces the number of decoding steps.}

\iffalse
\begin{table}[t] \small
\centering
\begin{tabular}{*{4}{c}}
\toprule
 \multicolumn{2}{c}{Parallel Strategy} & \multicolumn{1}{c}{Decoder} & \multirow{3}*{\makecell[c]{Acc.\\Avg(std)}}\\
\cmidrule(l l r){1-2}\cmidrule(l r){3-3}

\makecell[c]{Multiple\\query}
& \makecell[c]{Bipartite\\Matching} 
& \makecell[c]{Layer-wise} \\
\midrule
\checkmark & \checkmark & \checkmark  & 81.5 \scriptsize 0.13 \\ 
\XSolidBrush & \checkmark & \checkmark  &80.5 \scriptsize 0.31 \\
\checkmark & \XSolidBrush & \checkmark  &81.2 \scriptsize 0.18 \\
\XSolidBrush & \XSolidBrush & \XSolidBrush & 78.5 \scriptsize 0.42 \\ 
\checkmark & \checkmark & \checkmark  &80.6 \scriptsize 0.19 \\
\checkmark & \checkmark & \checkmark  &80.9 \scriptsize 0.12 \\
\bottomrule
\end{tabular}
\caption{Ablation on MathQA (w/o search) by 5 seeds.} 
\label{tab:ablation experiment}
\end{table}
\fi

\begin{table}[t]\small
\renewcommand\arraystretch{1.15}
\centering
\setlength\tabcolsep{2pt}\begin{tabular}{l c }
\toprule
     \textbf{Variant} & \textbf{Acc.} \\
\midrule[0.25pt]
                         Bipartite Matching            &\textbf{81.5 } \scriptsize  0.13       \\ 

Sequence Matching                      &78.8  \scriptsize  0.27 \\
Random Matching                    &20.1  \scriptsize   1.55 \\
w/ Operand  Loss                      &80.7   \scriptsize  0.36  \\
w/o Operator  Loss                      &79.6   \scriptsize  0.21  \\
w/o Parallel decoding  &79.9   \scriptsize  0.31  \\
\bottomrule
\end{tabular}
\caption{Ablation on MathQA about bipartite matching.}
\label{ablation_table}
\end{table} \subsection{Ablations}
Bipartite matching is essential for our parallel strategy. Therefore, we study how it works:
\uppercase\expandafter{\romannumeral1}. \textbf{Sequence Matching}. Firstly, we ablate bipartite matching and instead use a simple matching strategy for multiple expressions: sequence matching. It means we align the first expression predicted by the first query with the first label, and then the second predicted expression aligns with the second label, and so on. \uppercase\expandafter{\romannumeral2}. \textbf{Random Matching}. Then we random match the predicted expressions with the labels. \uppercase\expandafter{\romannumeral3}. \textbf{Operand None Loss}. As illustrated in Equation \ref{equ10}, for these labels padded with the  category, we only compute the loss for the operator. At this point, we add two operands' loss between  to analyze its effect. \uppercase\expandafter{\romannumeral4}. \textbf{Operator None Loss}. We remove the operator loss for the  category. \uppercase\expandafter{\romannumeral5}. \textbf{Parallel Decoding}. Lastly, we remove the whole parallel decoding, i.e., adopt only one query per layer. We provide a detailed visualization for these designs in Figure \ref{Fig.ablation}.  

As shown in \Cref{ablation_table}, when we replace with sequence matching, there is a notable degradation in accuracy (-2.7\%). In this case, the performance is similar to the Seq2Exp (RE-Ext:78.6\% vs Ablation: 78.8\%). It undermines the advantages of expression tree decoding since aligning process still introduces manually-annotated order. Secondly, we find random matching may lead to a training collapse. Then we observe disregarding  operator loss or adding the  operands loss, both having a negative impact (-0.9\%, -0.8\%). In the last ablation experiments, our performance drops from 81.5\% to 79.9\% (-1.6\%) when we remove the parallel decoding from our system. More comparisons can be found in \Cref{Ablating on Parallel Decoding}.

\subsection{Analysis}
We investigate the efficacy of the expression tree decoding strategy in scenarios involving complex equation generation. We conduct the analysis along two dimensions: the structure of the equation and the length of the equation. Besides, we also analyze the impact of different query numbers on parallel decoding performance.


\begin{figure}[t] \centering \includegraphics[width=0.49\textwidth]{figure/structure.pdf} \label{Fig.structure} \vspace{-20pt} \end{figure}

\textbf{Equation Structure} 
\begin{table}[t]\small
\centering
\begin{tabular}{l|c|c|c|c}
\toprule[1pt]
\textbf{Model} 
&\textbf{\makecell[c]{Single}}  
&\textbf{\makecell[c]{Exp Chain}}
&\textbf{\makecell[c]{Exp Tree}}
&\textbf{\makecell[c]{Overall}}
\\ 
\midrule[0.5pt]
\multirow{5}*{\makecell[l]{M-View\\RE-Ext\\M-Tree\\Ana-CL\\Ours}} 
& 79.7 & 82.7 & 65.7 &  79.5 \\  
& 77.4 & 82.5 & 64.7& 78.6 \\  
& 75.2 & 80.2 &  64.5 & 76.5 \\
& 79.9 & \textbf{82.8} & 67.8 & 79.6\\
& \textbf{80.1} & 82.0 &\textbf{75.2}  &\textbf{81.5}   \\  

\bottomrule[1pt]
\end{tabular}
\caption{We categorize the structures of equations into three types: Single, Chain and Tree, and evaluate the performance of five methods on three structures.}
\label{table:structure}
\end{table} In Table \ref{table:structure}, we categorize the structures of equations into three types: (1) Single expression, where the entire equation consists of only one expression; (2) Expression chain, where the equation is comprised of multiple expressions forming a chain; (3) Expression Tree, which involves complex tree structures composed of multiple expressions. We evaluate the model's accuracy on three types of equation structures.



As shown in \Cref{table:structure}, our expression tree decoding strategy gains comparable performance to other baselines in the first two evaluations (Single Expression and Expression Chain). In contrast, in the Expression Tree evaluation, most of these instances in this type involve sophisticated equation structures and complex solving processes. Our method significantly outperforms the other baselines (). Specifically, in comparison to the seq2tree approach, we achieve a +7.4\% improvement (Our:75.2\% vs Ana-CL:67.8\%), and gain a more substantial advantage (+9.5\%) relative to the seq2exp method. 
Under this case, our method outperforms seq2tree, and seq2tree in turn outperforms seq2exp. This clearly demonstrates that introducing the structural feature of equations indeed contributes to the capable of handling equations with complex structures.














\begin{figure}[t] \centering \includegraphics[width=0.49\textwidth]{figure/expression_num.pdf} \caption{Performance on the sample with the different number of expressions.} \label{expression_num} \end{figure}



\textbf{Equation Length} 
An equation comprises multiple mathematical expressions, with each expression representing a reasoning step. Complex equations usually contain more expressions. Therefore, we evaluate the performance on the instance with different numbers of expressions. In \Cref{expression_num}, as the number of expressions increases, the equation becomes more complex and the performance decreases rapidly. However, our method consistently maintains high accuracy (70\% on MathQA) across all cases, especially on complex cases. Compared with baselines, our advantage increases from +1.0\% (\#2) to +6.4\% (\#5). For the equation with the longest expressions (\#8), our strategy maintains an improvement by nearly +6\%, showing expression tree decoding strategy is more stable for complex equations.

\textbf{Query Number} 
We further analyze the impact of the number of queries on parallel decoding performance. The number of queries is set from 1 to 30. As shown in \Cref{table: query_num}, as the number of queries increases, the performance initially increases notably and then decreases. Specifically, when there is only one query per layer (\# query = 1), the parallel decoding strategy is removed. Conversely, when we adopt too many queries (\# query >= 10), the performance of parallel decoding drops rapidly. We speculate that this might be because most of the queries are redundant and are matched to the "None" label under this case. Too many queries may lead to instability in training. Apart from too many or too few queries, the performance gains attributed to parallel decoding remain both stable and pronounced. For instance, as the number of queries fluctuates between 4 and 8, the improvement consistently remains within the range of +1.2\% to +1.5\%. It suggests that although the number of queries is a hyperparameter, it does not need to be carefully tuned.

\subsection{Case Study and Visualization}
We explore the role of learnable queries in the expression tree decoding process. We first calculate the similarity between query vectors and problem representations for each layer and then visualize the results in Figure \ref{Fig.case_study}. As shown in the first case, the sixth query is activated twice through two steps, thus performing two division operations. In the second case, the first and second queries generate two valid expressions () in parallel in the first layer, and then the last query in the second layer outputs a division operation () using two results from the first layer. These examples illustrate that our method is highly flexible and can adaptively predict expressions in parallel or sequentially based on the context, thereby composing an expression tree.











\section{Conclusion}
We devise an expression tree decoding strategy for generating mathematical equations layer by layer. Each layer produces multiple mathematical expressions in parallel which are non-dependent and order-free, achieving flexible decoding. During the training, we employ a bipartite matching algorithm to align the multiple generated expressions with the golden labels and compute the parallel prediction loss under the optimal matching scheme. Extensive experiments demonstrate that our expression tree decoding strategy can effectively absorb the structural features of equations and enhance the capacity for generating complex equations for math reasoning.




\section*{Limitations}


Firstly, when faced with a mathematical problem that requires an extensive number of solving steps, we have to increase the number of decoder layers. It consequently leads to an increase in the model parameters. This is due to our layer-wise decoding strategy, where more complex equations require additional decoding layers. To address this, we have designed a variant model with shared parameters (Layer-Shared in Figure \ref{Fig.two_models}), which achieves comparable results without modifying layer number. 

Secondly, some hyperparameters (e.g., the number of queries and layer), need to be manually adjusted according to the dataset. In the future, we will explore how to utilize our query-based, layer-wise expression tree decoding strategy to address a broader range of structured generation tasks.

\section*{Acknowledgments}
This work is supported by the Fundamental Research Funds for the Central Universities (No. 226-2023-00060), Key Research and Development Program of Zhejiang Province (No. 2021C01013), National Key Research and Development Project of China (No. 2018AAA0101900), Joint Project DH-2022ZY0013 from Donghai Lab, and MOE Engineering Research Center of Digital Library.











\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix

\clearpage
\renewcommand\thefigure{\Alph{section}\arabic{figure}}    
\setcounter{figure}{0}    
\renewcommand\thetable{\Alph{section}\arabic{table}}    
\setcounter{table}{0}   
\section{Appendix}





\subsection{Baselines} \label{baseline}
In recent years, the MWP task has garnered widespread attention \citep{zhou2023learning,xiong2022self,lan2022improving,yang2022unbiased,cobbe2021training,tsai2021sequence,huang2021recall}. We divide the prior baselines into two categories: Seq2Seq/Tree and Seq2Exp. In Seq2Seq/Tree, \citet{li-etal-2019-modeling} (\textbf{GroupAttn}) applied a multi-head attention approach using a seq2seq model. \citet{xie2019goal} proposed a seq2tree generation (\textbf{GTS}). \citet{zhang2020graph} (\textbf{G2T}) introduced a graph encoder. \citet{patel-etal-2021-nlp,liang2021mwp} added a PLMs encoder to GTS and G2T (\textbf{PLM-GTS, BERT-T}). \citet{tan2021investigating} proposed a multilingual model (\textbf{mBERT}). \citet{lan2021mwptoolkit} utilized Transformer for generation (\textbf{BERTGen}). \citet{shen-etal-2021-generate-rank} proposed a multi-task method \textbf{(Rank)}. \citet{qin-etal-2021-neural} introduced a neural symbolic method (\textbf{Symbol-Dec}). \citet{yu-etal-2021-improving} extracted hierarchical features for encoder (\textbf{H-Reasoner}). \citet{yang2022logicsolver} designed logical rules to guide decoding (\textbf{Logic-Dec}). \citet{li2021seeking} proposed a prototype learning (\textbf{Prototype}). \citet{ijcai2021-485} adopted a teacher model for discrimination (\textbf{T-Dis}). \citet{shen2022textual} distinguished examples with similar semantics but different logics (\textbf{Textual-CL}). \citet{liang-etal-2022-analogical} adopted an analogy identification to improve the generalization (\textbf{Ana-CL}). 

In Seq2Exp, \citet{cao2021bottom} used a bottom-up DAG construction method (\textbf{DAG)}. \citet{jie-etal-2022-learning} introduced a relation extraction method (\textbf{RE-Ext}). \citet{wang2022structure} treated MWP as tagging annotation by M-Tree coding (\textbf{M-Tree}). \citet{DBLP:journals/corr/abs-2305-04556} introduced a uniﬁed tree structure using a non-autoregressive model (MWP-NAS). \citet{zhang-etal-2022-multi-view} aligned the representation of different traversal order for consistency (\textbf{M-View}). ELASTIC \citep{zhang2022elastic} designs a computer synthesis process to handle numerical reasoning. We also compare our results with the gpt-3.5-turbo in the few-shot setting. We design a prompt consisting of both directive instructions and a demonstration to guide gpt-3.5 step-by-step reasoning. 



\label{sec:appendix}
\begin{figure}[t] \centering \includegraphics[width=0.49\textwidth]{figure/two_model_structure.pdf} \caption{Except for using a standard decoder for layer-wise decoding (Left), we also explore an alternate model (Right), in which parallel decoding is performed at every  transformer layer, but each decoding step shares the parameter of these  layers. The left model is more accurate, and the right has fewer parameters.} \label{Fig.two_models} \end{figure}




\subsection{Training Details} \label{training details}
Following most previous works \citep{zhang-etal-2022-multi-view,jie-etal-2022-learning}, we adopt Roberta-base and Chinese-BERT as encoder from HuggingFace \citep{wolf-etal-2020-transformers} for multilingual datasets. We consider five mathematical operators, containing \emph{Addition, Subtraction, Multiplication, Division, Exponentiation}, and various constants () as previously. Our query decoder is a transformer decoder with multiple layers, each having 768 hidden units. In our experiments, we perform parallel decoding once at each transformer layer. We use an AdamW optimizer with a 5e-5 learning rate, batch size of 32 for MathQA and 26 for Math23K. We set the maximum layer number as 8. The other parameters are set as previous works \citep{zhang-etal-2022-multi-view,jie-etal-2022-learning}. All experiments were set up on an NVIDIA RTX A6000. 





\begin{figure*}[t] \centering \includegraphics[width=1\textwidth]{figure/fig3.pdf} \caption{Two cases for Label Set and Expression Tree decoding processes.} \label{Fig.f3} \end{figure*}


\begin{figure*}[t] \centering \includegraphics[width=1\textwidth]{figure/ablation.pdf} \caption{Up: The process of the Bipartite Matching. Down: Several ablation designs.} \label{Fig.ablation} \end{figure*}



\begin{figure*}[!htb] \centering \includegraphics[width=1\textwidth]{figure/case_study.pdf} \caption{We visualize the expression tree decoding process at each layer. We calculate the cosine similarity between query vectors and problem representations for each layer. In the first case, the prediction expressions are output by a query. In the second case, the first and second queries are activated for two expressions in parallel.} \label{Fig.case_study} \end{figure*}

\subsection{MWP Dataset Statistics} \label{dataset}
The statistics of the dataset are shown in Table \ref{tab:dataset statistics}.
\begin{table}[ht]\small
\renewcommand\arraystretch{1.15}
\centering
\setlength\tabcolsep{2pt}\begin{tabular}{l | c |c |c | c}
\toprule
     \textbf{Dataset} & \textbf{\#Train/\#Valid/\#Test} & 
     \textbf{\makecell[c]{\#Avg.\\Token}} &
     \textbf{\makecell[c]{\#Avg.\\ Exp}} &\textbf{\makecell[c]{\#Max.\\ Exp}}\\
\midrule[0.25pt]
                         MathQA            & 16191 /2415/1606 & 39.6  & 4.17 & 12     \\ 
Math23K                      &21162/1000/1000
  &26.6 & 2.26 & 20 \\
MAWPS                      &1589/199/199
   &30.3 & 1.42& 7 \\

\bottomrule
\end{tabular}
\caption{Statistics for three standard datasets.}
\label{tab:dataset statistics}
\end{table} 

\begin{table*}[t]\small
\centering
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c}
\toprule[1pt]
\textbf{\# Query} & 1 & 2 & 4 & 5 & 6 & 7 & 8 & 10 & 15 & 20 & 30 \\
\midrule[0.5pt]
\textbf{Acc.(MathQA) \%} & 79.9 & 80.6 & 81.4 & 81.4 & 81.5 & 81.3 & 81.1 & 80.8 & 80.2 & 79.8 & 79.1 \\

\textbf{Compared to \#1} & 0 & +0.7 & +1.5 & +1.5 & +1.6 & +1.4 & +1.2 & +0.9 & +0.3 & -0.1 & -0.8 \\
\midrule[0.5pt]
\textbf{Acc.(Math23K) \%} & 85.2 & 85.8 & 86.2 & 86.0 & 86.2 & 85.9 & 85.6 & 85.2 & 84.8 & 83.5 & 83.3 \\

\textbf{Compared to \#1} & 0 & +0.6 & +1 & +0.8 & +1 & +0.7 & +0.4 & 0 & -0.4 & -1.7 & -1.9 \\
\toprule[1pt]
\end{tabular}
\caption{The impact of query number on our parallel decoding performance.}
\label{table: query_num}
\end{table*}






\begin{table}[t]\small
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \textbf{MathQA} & \textbf{Math23K} \\
\midrule
w/ parallel decoding & 81.5 & 86.2 \\
w/o parallel decoding & 79.9 & 85.2 \\
E-pointer & 73.5 & 78.7 \\
M-View & 79.5 & 85.6 \\
RE-Ext & 78.6 & 85.4 \\
Elastic & 80.3 & 84.8 \\
\bottomrule
\end{tabular}
\caption{The ablation study on parallel decoding.}
\label{table: further ablation}
\end{table}




 
\subsection{Ablating on Parallel Decoding} \label{Ablating on Parallel Decoding}
Parallel decoding is the key to constructing expression trees. We provide a more detailed comparison of our parallel decoding strategy. The detailed results are as shown in \Cref{table: further ablation}. When we ablate parallel decoding from our framework, i.e., adopt only one query per layer, our performance drops from 81.5\% to 79.9\% (-1.6\%) on MathQA. A similar trend is seen on Math23K (-1.0\%). Besides, without the parallel decoding strategy, our performance is similar to the Seq2Exp baselines (e.g., E-pointer, M-View, RE-Ext, Elastic, etc.), which generate one expression at each step. Compared to them, parallel decoding brings noticeable and consistent improvements (E-pointer: +8\%, M-View: +2\%, RE-Ext: +2.9\%, Elastic: +1.2\%).





\end{document}
