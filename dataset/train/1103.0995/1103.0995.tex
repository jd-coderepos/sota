\section{Introduction}

The best rank \math{k} approximation to a matrix
\math{\matA \in \R^{m \times n}} is
 where
\math{\sigma_1\ge\sigma_2\ge\cdots\ge\sigma_k\ge 0} are the top \math{k}
 singular values of , with associated left and right singular vectors
\math{\u_i \in \R^m} and~\math{\v_i \in \R^n} respectively. (See Section~\ref{sxn:notation} for notation and recall that the singular values and singular vectors of  can be computed via the Singular Value Decomposition (SVD) of  in \math{O( mn\min\{m,n\})} time.) It is well-known that  optimally approximates  among all rank  matrices, with respect to any unitarily invariant norm.  There is considerable interest (e.g. \cite{CH92,DR10,DV06,DRVW06,DKR02,FKV98,HMT,LWMRT07,MD09}) in determining a minimum set of  columns of   which is approximately as good as \math{\matA_k} at reconstructing \math{\matA}.
Such columns are important for interpreting data \cite{MD09}, building robust machine learning algorithms \cite{CH92}, feature selection, etc.

Let  and let  consist of  columns of  for some .
We are interested in the reconstruction errors

for  (see Section~\ref{sxn:notation} for notation).
The former is the reconstruction error
for \math{\matA} using the columns in \math{\matC};
the latter is the error from the
best  rank \math{k}
reconstruction of
\math{\matA} (under the appropriate norm) within the column space of .
For fixed , , and , we would like these errors to be as close
to  as possible.
We present polynomial-time near-optimal constructions for arbitrary
,
settling important open questions regarding column-based matrix reconstruction.
\begin{itemize}
\item {\bf Spectral norm:}
What is the best reconstruction error with
\math{r > k} columns? We present polynomial-time (deterministic and
randomized) algorithms with approximation error asymptotically matching a lower
bound proven in this work. Prior work had focused on the  case and presented near-optimal polynomial-time algorithms~\cite{ DR10,GE96}.
\item{\bf Frobenius norm:} How many columns are needed for relative error
approximation, i.e. a reconstruction error of at most

for ? We show that

columns contain a rank-\math{k} subspace which
reconstructs \math{\matA} to relative error,
and we present the first  sub-SVD (in terms of running time) randomized algorithm to identify these columns. This matches
the \math{\Omega(k/\epsilon)} lower bound in~\cite{DV06} and improves the
best known upper bound of \math{O(k\log k+ k/\epsilon)}~\cite{DR10,DV06,DMM06b,Sar06}.
\end{itemize}

\subsection{Notation}\label{sxn:notation}

\math{\matA,\matB,\ldots} are matrices; \math{\a,\b,\ldots} are
column vectors.  is the 
identity matrix;   is the  matrix of zeros;  is the  vector of ones;  is the standard basis (whose dimensionality will be clear from the context);  is the rank of .
The Frobenius and the spectral matrix-norms are:

and ;
  is used if a result holds for
both norms  and .
The Singular Value Decomposition (SVD) of
, with  is

with singular values \math{\sigma_1\ge\ldots\sigma_k\geq\sigma_{k+1}\ge\ldots\ge\sigma_\rho > 0}.
We will use  to denote the -th
singular value of  when the matrix is not clear from the context.
The matrices
 and  contain the left singular vectors of~, and, similarly, the matrices  and  contain the right singular vectors of~. It is well-known that  minimizes \math{\XNorm{\matA - \matX}} over all
matrices \math{\matX \in \R^{m \times n}} of rank at most . We use  to denote the matrix . Also,
 denotes the Moore-Penrose pseudo-inverse of .
For a symmetric positive definite matrix ,  denotes the -th eigenvalue of .

Finally, given a matrix  and a matrix  with , we formally define the matrix  as the best approximation to  within the column space of  that has rank at most
.  
minimizes the residual

over all
\math{\hat\matA} in the column space of
\math{\matC} that have rank at most \math{k} (one can write
\math{\Pi_{\matC,k}^{\xi}(\matA)=\matC\matX} where
\math{\matX\in \mathbb{R}^{r \times n}} has rank at most ).
In general,  Section~\ref{sec:bestrankk} discusses the computation of~.





\subsection{Main results}\label{sec:main0}

Since

we will state all our bounds in terms of the latter quantity. Note that we chose to state our Frobenius norm bounds in terms of the square of the Frobenius norm; this choice facilitates comparisons with prior work and simplifies our proofs (see also Table~\ref{table:results} for a summary of our results).
\begin{theorem}[Deterministic spectral norm reconstruction]
\label{theorem:intro1}
Given \math{\matA\in\R^{m \times n}} of rank \math{\rho}
and a target rank \math{k < \rho},
there exists a deterministic polynomial-time algorithm to
select \math{r > k} columns of \math{\matA} and form a matrix
 such that
\eqan{
\TNorm{\matA - \Pi^2_{\matC,k}(\matA)}
\leq
\textstyle\left(1 + \frac{1 + \sqrt{ (\rho-k)/r }}{ 1 - \sqrt{ k/r }  }  \right)
\TNorm{\matA - \matA_k}
= O\left(\sqrt{{\rho}/{r}}\right)
\TNorm{\matA - \matA_k}.
}
The matrix  can be computed in
\math{T_{SVD} + O\left(rn\left(k^2+\left(\rho-k\right)^2\right)\right)} time, where  is the time needed to compute all  right singular vectors of .
\end{theorem}


Our algorithm uses
 the matrices  and  of the right singular vectors of . These matrices can be computed in  time via the SVD. The asymptotic multiplicative error of the above theorem matches a lower bound that we prove in Section~\ref{sec:lower}. This is the first spectral reconstruction algorithm with asymptotically optimal guarantees for arbitrary . Previous work presented near-optimal algorithms for  \cite{GE96}. We note that in Section~\ref{sec:PROOFS} we will present a result that achieves a slightly worse error bound (essentially replacing  by  in the accuracy guarantee), but only uses the top  right singular vectors of  (i.e., the matrix  from the SVD of ).

\begin{theorem}[Deterministic Frobenius norm reconstruction]
\label{theorem:intro2}
Given \math{\matA\in\R^{m\times n}} of rank  and a target rank ,
there exists a deterministic
polynomial-time algorithm to
select \math{r>k} columns of \math{\matA} and form a matrix

such that

The matrix  can be computed in
\math{T_{\matV_k} + O\left(mn + nrk^2\right)} time, where  is the time needed to compute the top  right singular vectors of .
\end{theorem}


Our bound implies a constant-factor approximation.
Previous work presents deterministic near-optimal algorithms for  \cite{DR10}; we are unaware of any deterministic algorithms for .

The next two theorems guarantee (up to small constant factors) the same bounds as Theorems~\ref{theorem:intro1} and~\ref{theorem:intro2},
but the proposed algorithms are considerably more efficient. In particular, there is no need to exactly compute the right singular vectors of ,
because approximations suffice.

\begin{theorem}[Fast spectral norm reconstruction]
\label{thmFast1}
Given \math{\matA\in\R^{m\times n}} of rank , a target rank , and ,
there exists a randomized algorithm to select \math{r > k} columns of \math{\matA} and form a matrix
 such that
\eqan{
\Expect{ \TNorm{\matA - \Pi_{\matC,k}^2(\matA)} }
\leq
\textstyle\left(\sqrt{2}+\epsilon\right) \left( \frac{1 + \sqrt{ n/r }}{1 - \sqrt{ k/r }}\right)
\TNorm{\matA-\matA_k}
=
O\left(\sqrt{{n}/{r}}\right)\TNorm{\matA-\matA_k}.
}
The matrix  can be computed in  time.
\end{theorem}
\begin{theorem}[Fast Frobenius norm reconstruction]
\label{thmFast2}
Given \math{\matA\in\R^{m\times n}} of rank , a
target rank , and  ,
there exists a randomized algorithm to
select \math{r > k} columns of \math{\matA} and form a matrix
 such that

The matrix  can be computed in  time.
\end{theorem}

\begin{table}
\begin{center}
\begin{tabular}{l|c@{\hspace*{0.05in}}l|c@{\hspace*{0.05in}}l}
  & \multicolumn{2}{c|}{Spectral norm ()} & \multicolumn{2}{c}{Frobenius norm ()}\\
&&&&\-8pt]
Deterministic
&   &(Thm.~\ref{theorem:intro1}) &  & (Thm.~\ref{theorem:intro2})\\
&&&&\-8pt]
Randomized\math{^*}
&   & (Thm.~\ref{thmFast1})
&  &(Thm.~\ref{thmFast3})    \
r =\frac{2k}{\epsilon}\bigl(1+o(1)\bigr)
\mynorm{\matA-\hat\Pi_{\matC,k}^{2}(\matA)}_2;\frac{\XNorm{\matA-\Pi_{\matC,k}^{\xi}(\matA)}^2}{\XNorm{\matA-\matA_k}^2},-9pt]
\hline
&&&&\-7pt]
\hline
&&&&\5pt]
\hline
\end{tabular}
\medskip
\caption{\noindent Lower bounds for the approximation ratio \math{\mynorm{\matA-\Pi^\xi_{\matC,k}(\matA)}_{\xi}^2/\mynorm{\matA-\matA_k}_{\xi}^2}. Here .
\label{table:lower}}
\end{center}
\end{table}

\subsubsection{The Frobenius norm case}

We present known upper bounds for the approximation ratio

We start with the  case.~\cite{BMD09a} describes a 
time randomized algorithm which provides an upper bound  with constant probability.
This bound was subsequently improved in~\cite{DR10}. More precisely,
Theorem 8 of~\cite{DR10} gives a  deterministic approximation
running in  time;
this upper bound matches the lower
bound in~\cite{DRVW06}.
\cite{DR10} also presents three randomized algorithms such that .
These randomized algorithms are presented in Theorem 7, Proposition 16, and Proposition 18 and run in
, , and  time, respectively.
Moreover, Theorem 9 in~\cite{DR10} presents a randomized algorithm that runs in time 

such that, with constant probability, 
,
for any . Finally,~\cite{GS2011} improved upon the running time of the results in~\cite{DR10}.
More precisely, Theorem 2 in~\cite{GS2011} gives an  time randomized algorithm with
a  multiplicative error (in expectation).

When , relative-error approximations are known.
\cite{DMM06b} presented the first result that achieved such a bound, using random sampling of the columns of  according to the Euclidean norms of the rows of , the so-called leverage scores~\cite{MD09}. More specifically, a \math{(1+\epsilon)}-approximation was proven using \math{r = \Omega\left(k\epsilon^{-2}\log \left(k\epsilon^{-1}\right)\right)} columns in  time. \cite{Sar06} argued
that the same technique gives a \math{(1+\epsilon)}-approximation
using \math{r = \Omega\left(k\log k + k \epsilon^{-1}\right)} columns. It also  showed how to improve the running time to , where  contains the right singular vectors of an approximation to  and can be computed in
\math{o(mn\min\{m,n\})} time, which is less than the time needed to compute the SVD of .
In~\cite{DV06}, the authors leveraged volume sampling and presented an approach that achieves a relative error approximation using \math{O(k^2\log k + k\epsilon^{-1})} columns in \math{O(mnk^2\log k)} time. Also, it is possible to combine the fast volume sampling approach in~\cite{DR10} (setting, for example, ) with \math{O(\log k)} rounds of adaptive sampling as described in~\cite{DV06} to achieve a relative error approximation using \math{O\left(k\log k +k\epsilon^{-1}\right)} columns. The running time of this combined algorithm is .
The techniques in \cite{DMM06b} do not apply to general \math{r>k}, since \math{\Omega(k\log k )} columns must be sampled in order to preserve rank with random sampling.

A related line of work (including~\cite{DV07,FL11, FMSW10,SV07})
has focused on the construction of coresets and
sketches for high dimensional subspace approximation with respect
to general  norms. In our setting, \math{p=2} corresponds to
Frobenius norm matrix reconstruction, and Theorem 1.3 of~\cite{SV07}
presents an exponential in  algorithm to select
  columns that
guarantee a relative error approximation. It would be interesting to
understand if the techniques of~\cite{DV07,FL11, FMSW10,SV07}
can be extended to match our results here in the special case
of \math{p=2}.

The recent work in~\cite{GS2011} presents a deterministic and a randomized algorithm for arbitrary
 that guarantee upper bounds for the ratio .
More precisely, Theorem 1 in~\cite{GS2011} presents an  time deterministic algorithm with bound , which is tight up to
low order terms if . Also, Theorem 2 in~\cite{GS2011} presents an  time randomized algorithm
which achieves the same bound in expectation. We should notice that it is not obvious how to
extend the results in~\cite{GS2011} to obtain comparable bounds for the ratio


\subsubsection{The spectral norm case}

We present known guarantees for the approximation ratio

In general,
results for spectral norm have been rare.
When \math{r=k},
the strongest bound emerges from the Strong Rank Revealing
 QR (RRQR) \cite{GE96} (specifically Algorithm 4 in \cite{GE96}), which, for
\math{f>1}, runs in  time and guarantees an
\math{ f^2 k(n-k) +1 } approximation. For \math{r>k}, to the
best of our knowledge, there is no easy way to extend the
RRQR guarantees. In fact we are only aware of one bound that is
applicable to this domain, other than those obtained by trivially
extending the Frobenius norm bounds, because
any -approximation in the Frobenius norm gives an
-approximation in the spectral norm:

Finally, recent work~\cite{AB11} describes a deterministic  time algorithm that guarantees an approximation error 
 for any .

\section{Matrix norm properties and the computation of }
\label{sec:prel}

\subsection{Matrix norm properties}

Recall notation from Section~\ref{sxn:notation}; for any matrix  of rank at most , it is well-known that  and . Also, the best rank  approximation to  satisfies  and . For any two matrices  and  of appropriate dimensions, \math{\TNorm{\matA}\le\FNorm{\matA}\le\sqrt{\rho}\TNorm{\matA}}, , and . The latter two properties are stronger versions of the standard submultiplicativity property. We refer to the next lemma as matrix-Pythogoras:
\begin{lemma}\label{lem:pyth}
If \math{\matX,\matY\in\R^{m\times n}} and
\math{\matX\matY\transp=\bm{0}_{m \times m}} or \math{\matX\transp\matY=\bm{0}_{n \times n}}, then
\eqan{
&\FNorm{\matX+\matY}^2 = \FNorm{\matX}^2+\FNorm{\matY}^2,& \\
&\max\{\TNorm{\matX}^2,\TNorm{\matY}^2\}\le
\TNorm{\matX+\matY}^2 \le \TNorm{\matX}^2+\TNorm{\matY}^2.&
}
\end{lemma}
\begin{proof}
Suppose \math{\matX\matY\transp=\bm{0}_{m \times m}}.
Then, \math{(\matX+\matY)(\matX+\matY)\transp=\matX\matX\transp + \matY\matY\transp}.
For ,

For , let  be any vector in . Then,

We have that
 is at most

and that

since  is non-negative for any vector .
We get the same lower bound with \math{\TNormS{\matY}} instead, which means
we can lower bound by \math{\max\{\TNormS{\matX},\TNormS{\matY}\}}.
The case when \math{\matX\transp\matY=\bm0_{n\times n}} can be proven similarly.
\end{proof}

A projection operator \math{\matP} equals
its own square, \math{\matP^2=\matP}.
Projection operators play an important role in our analysis.
 The following lemma is well known
for nontrivial
symmetric projection matrices,
but also holds for non-symmetric (oblique) projection matrices.
\begin{lemma}[\cite{Szy06}]\label{lemma:oblique}
Let \math{\matP} be a non-null projection. Then,
\math{\mynorm{\matI-\matP}_2\le\mynorm{\matP}_2}.
\end{lemma}
(If in addition \math{\matI-\matP}, also a projection,
is non-null, then we get equality in the
above lemma.)


\subsection{Computing the best rank \math{k} approximation
} \label{sec:bestrankk}

Let , let  be an integer, and let  with . Recall that  is the best rank \math{k} approximation to \math{\matA} in the column space of \math{\matC}. We can write , where

In order to compute (or approximate)  given ,
, and , we will use the following algorithm:
\begin{center}
\begin{algorithmic}[1]
\STATE Orthonormalize the columns of  in  time to construct the matrix .
\STATE Compute
  via SVD
in \math{O(mnr+ nr^2)} -- the best rank- approximation of
\math{\matQ\transp\matA}.
\STATE Return  in  time.
\end{algorithmic}
\end{center}
\medskip
Clearly,  is a rank  matrix that lies in the column span of . Note that though   can depend on \math{\xi}, our algorithm computes the same matrix, independent of \math{\xi}. The next lemma, which is essentially Lemma 4.3 in~\cite{CW09} combined with an improvment of Theorem 9.3 in~\cite{HMT},
proves that this algorithm computes  and a constant factor approximation to .
\begin{lemma}\label{lem:bestF}
Given ,  and an integer ,  the matrix  described above (where \math{\matQ} is an orthonormal basis for the columns of \math{\matC}) can be computed in  time and satisfies:
3pt]
\mynorm{\matA-\matQ(\matQ\transp \matA)_k}_2^2 &\leq& 2\TNormS{\matA-\Pi_{\matC,k}^{2}(\matA)}.
\FNormS{\matA-\Pi^{\mathrm{F}}_{\matC,k}(\matA)} = \FNormS{\matA-\Pi^{\mathrm{\mathrm{F}}}_{\matQ,k}(\matA)} = \min_{\MatPsi:\rank(\MatPsi)\leq k}\FNormS{\matA-\matQ\MatPsi}.\TNormS{\matQ\matQ\transp\matA-(\matQ\matQ\transp\matA)_k}=\sigma_{k+1}^2(\matQ\matQ\transp\matA)\le\sigma_{k+1}^2(\matA) =
\TNormS{\matA-\matA_k}.\matA=\matB\matZ\transp+\matE,\label{eqn1}
\mynorm{\matA - \Pi^{\xi}_{\matC,k}(\matA)}_\xi^2 \leq \XNormS{\matE} +
\mynorm{\matE\matS (\matZ\transp \matS)^+}_\xi^2;
\label{eqn2}
\XNormS{\matA - \Pi^{\xi}_{\matC,k}(\matA)}
\leq \XNormS{\matE} \cdot \TNormS{ \matS (\matZ\transp \matS)^+}.
\XNormS{\matA - \Pi^{\xi}_{\matC,k}(\matA)}
\leq \XNormS{\matA - \matX}, \matB \matZ\transp - \matB (\matZ\transp \matS) (\matZ\transp \matS)^+ \matZ\transp = \bm{0}_{m \times n}.\matE\matS(\matZ\transp\matS)^+\matZ\transp \matE\transp = \bm{0}_{m \times n},\matP=\matS(\matZ\transp\matS)^+\matZ\transp,\matP^2=\matS(\matZ\transp\matS)^+\matZ\transp\matS(\matZ\transp\matS)^+\matZ\transp=\matS(\matZ\transp\matS)^+\matZ\transp,\matA = \matA\matV_k\matV_k\transp + \left(\matA-\matA_k\right),\label{eqn3}
\mynorm{\matA - \Pi_{\matC,k}^{\xi}(\matA)}_\xi^2
\leq \XNormS{\matA-\matA_k} + \mynorm{(\matA-\matA_k) \matS (\matV_k\transp \matS)^+}_\xi^2;
\label{eqn4}
\XNormS{\matA - \Pi^{\xi}_{\matC,k}(\matA)}
\leq \XNormS{\matA-\matA_k} \cdot \TNormS{ \matS (\matV_k\transp \matS)^+}.
\Expect{\TNorm{\matE}} \leq \left(\sqrt{2}+\epsilon\right)
\TNorm{\matA - \matA_k}.\Expect{\FNormS{\matE}} \leq (1+{\epsilon})
\FNormS{\matA - \matA_k}.
\sigma_{k}(\matV\transp\matS)
\ge
1 - \sqrt{{k}/{r}}
\qquad
\hbox{and}
\qquad
\sigma_{1}(\matU\transp\matS)
\le 1 + \sqrt{{\ell}/{r} }.
\TNorm{\matA - \Pi_{\matC,k}^2(\matA)} \leq
\frac{1 + \sqrt{ n/r }}{1 - \sqrt{ k/r }} \cdot \TNorm{\matA-\matA_k}. \TNorm{\matA - \Pi^{\xi}_{\matC,k}(\matA)} \le \TNorm{\matE} \cdot \left(1+\sqrt{n/r} \right) \cdot \left( 1 - \sqrt{ k/r } \right)^{-1}.p_i = {\TNormS{\b_{i}}}/{\FNormS{\matB}},\Expect{ \mynorm{ \matA - \Pi_{\matC,k}^{\mathrm{F}}(\matA) }_F^2 } \le \FNormS{ \matA - \matA_k } + \frac{k}{s} \norm{\matB}_F^2.c_0=\left(1+\epsilon_0\right) \left(1+{1}/{\left(1-\sqrt{k/\hat r} \right)^2} \right),\hat r=\ceil{dk}.s = \ceil{c_0k/
\epsilon}\matC = [\matC_1\ \ \matC_2]\in\R^{n\times\left(\hat{r}+s\right)}
\Exp_{\matC_2}\left[\left.\FNormS{\matA - \Pi^{\mathrm{F}}_{\matC,k}(\matA)}
\right|\matC_1\right]
\le \FNormS{\matA-\matA_k}+\frac{k}{s} \FNormS{\matB}.
\Exp_{\matC_1}\left[\FNormS{\matA-\matC_1\matC_1^+\matA}\right]
\le \Exp_{\matC_1}\left[\FNormS{\matA-\Pi^{\mathrm{F}}_{C_1,k}(\matA)}\right] \le c_0
\FNormS{\matA-\matA_k}.r=\hat r+s=dk+c_0k/\epsilon.d=(1+\alpha)^2,\alpha=\sqrt[3]{(1+\epsilon_0)/\epsilon}.
r=k\left( \alpha^3+\left(1+\alpha\right)^3\right)=\frac{2k}{\epsilon}\left(1+O\left(\epsilon_0+\epsilon^{1/3} \right) \right)

\epsilon_0=\epsilon^{2/3},
O\left(mnk+nk^3+n\log\epsilon^{-1}\right)\Expect{\TNorm{\matA - \matY \matY^+ \matA }} \leq
\left(1 + \sqrt{\frac{k}{p-1}} + \frac{e \sqrt{k+p}}{p} \sqrt{ \min\{ m,n\} -k }
\right)^{\frac{1}{2q+1}}\TNorm{\matA - \matA_k}, \TNorm{\matP \matX} \le \left( \TNorm{ \matP (\matX \matX\transp)^q \matX  }  \right)^{\frac{1}{2q+1}} \Expect{ x } \leq \left( \Expect{ x^h } \right)^{\frac{1}{h}}.
\matX^\xi =
\argmin_{\Psi \in \mathbb{R}^{r \times n}:\rank(\Psi)\leq k}\XNormS{\matA-\matC\Psi}.
\XNormS{\matA-\matC\matX^\xi} = \XNormS{\matA-(\matC\matX^\xi)
(\matC\matX^\xi)^+\matA} \leq \XNormS{\matA-(\matC\matY)(\matC\matY)^+\matA},\XNormS{\matA-\matC\matX^\xi} \le \XNormS{\matA-(\matC\matX^\xi)
(\matC\matX^\xi)^+\matA}.
\XNormS{\matA - \matC\matX^\xi}
&=&
\XNormS{\left(\matI_m-(\matC\matX^\xi)(\matC\matX^\xi)^+\right)\matA-
\matC\matX^\xi(\matI_n-(\matC\matX^\xi)^+\matA)}\\
&\ge&
\XNormS{\left(\matI_m-(\matC\matX^\xi)(\matC\matX^\xi)^+\right)\matA}.
\Expect{\TNorm{\matA - \Pi_{\matY,k}^2(\matA) } } \leq
\left(1 + \sqrt{\frac{k}{p-1}} + \frac{e \sqrt{k+p}}{p} \sqrt{ \min\{ m,n\} -k } \right)^{\frac{1}{2q+1}}
\TNorm{\matA - \matA_k}\TNorm{\matA-\Pi_{\matY,k}^2(\matA)} =
\TNorm{(\matI_m-(\matY\matX_1)(\matY\matX_1)^+)\matA} \leq
\TNorm{(\matI_m-(\matY\matX_2)(\matY\matX_2)^+)\matA}.
\TNorm{(\matI_m-(\matY\matX_2)(\matY\matX_2)^+)\matA}&\leq&
\TNorm{\left(\matI_m-\left(\matY\matX_2\right)\left(\matY\matX_2\right)^+\right)\left(\matA\matA\transp\right)^q\matA}^{\frac{1}{2q+1}}\\
&=&
\TNorm{\matB-\left(\matY\matX_2\right)\left(\matY\matX_2\right)^+\matB}^{\frac{1}{2q+1}}\\
&=&
\TNorm{\matB-\Pi_{\matY,k}^2(\matB)}^{\frac{1}{2q+1}},

\TNorm{\matA-\Pi_{\matY,k}^2(\matA)} \leq \TNorm{\matB-\Pi_{\matY,k}^2(\matB)}^{\frac{1}{2q+1}}.
\label{eqn:PPD22}
\Expect{\TNorm{\matA-\Pi_{\matY,k}^2(\matA)}} \leq \left(\Expect{\TNorm{\matB-\Pi_{\matY,k}^2(\matB)}}\right)^{\frac{1}{2q+1}}.
\TNormS{ \matB - \Pi^{2}_{\matY,k}(\matB) }
\leq  \TNormS{\matB-\matB_k} + \TNormS{(\matB-\matB_k)\matR(\matV_{\matB,k}\transp\matR)^+ }
\le
\TNormS{\matSig_{\matB,\tau}} +
\TNormS{\matSig_{\matB,\tau} \matOmega_2 \matOmega_1^+ }.

\TNorm{ \matB - \Pi^{2}_{\matY,k}(\matB) }\le
\TNorm{\matSig_{\matB,\tau}} +
\TNorm{\matSig_{\matB,\tau} \matOmega_2 \matOmega_1^+ }.

{\bf E}_{\matOmega_2}\left[ \TNorm{ \matSig_{\matB,\tau} \matOmega_2 \matOmega_1^+ }|\matOmega_1\right]
\nonumber &\leq&   \TNorm{ \matSig_{\matB,\tau}} \FNorm{\matOmega_1^+ }
+ \FNorm{ \matSig_{\matB,\tau}} \TNorm{\matOmega_1^+ }.
\Expect{\FNorm{\matOmega_1^+ }}\le
\Expect{\FNormS{\matOmega_1^+ }}^{1/2},\FNorm{ \matSig_{\matB,\tau}}\le\sqrt{\min(m,n)-k}
\TNorm{ \matSig_{\matB,\tau}},
\TNorm{\matB-\matB_k}=\TNorm{\matA-\matA_k}^{2q+1}.q = \ceil{ \frac{ \log\left( 1+
\sqrt{\frac{k}{k-1}} + \frac{e \sqrt{2k}}{k} \sqrt{ \min\{ m,n\} -k } \right)
}{2 \log\left(1+\epsilon/\sqrt{2}\right)  - 1/2 }}, \left(1 + \sqrt{\frac{k}{p-1}} + \frac{e \sqrt{k+p}}{p} \sqrt{ \min\{ m,n\} -k } \right)^{\frac{1}{2q+1}} \leq 1 + \frac{\epsilon}{\sqrt{2}}.\TNorm{\matE} \leq \TNorm{\matA - \matQ(\matQ\transp\matA)_k}
\le\sqrt{2} \TNorm{\matA - \Pi_{\matY,k}^2(\matA) }.\Expect{\FNorm{\matA - \matY\matY^+\matA }} \leq \left( 1 + \frac{k}{p-1} \right)^{\frac{1}{2}} \FNorm{\matA - \matA_k}.\Expect{\FNormS{\matA - \Pi_{\matY,k}^{\mathrm{F}}(\matA) }} \leq
\left( 1 + \frac{k}{p-1} \right) \FNormS{\matA - \matA_k}.\FNormS{ \matA - \Pi_{\matY,k}^{\mathrm{F}}(\matA) } \le
\FNormS{\matA - \matA_k} + \FNormS{ \matSig_{\rho-k} \matOmega_2 \matOmega_1^+} ,\FNormS{\matA -  \matQ(\matQ\transp\matA)_k} =
\FNormS{\matA - \Pi_{\matY,k}^{\mathrm{F}}(\matA) }.\FNormS{\matE} \leq \FNormS{\matA - \matQ(\matQ\transp\matA)_k}
=\FNormS{\matA - \Pi_{\matY,k}^2(\matA) }.\Expect{\FNormS{\matE}} \leq \left( 1 + \frac{k}{p-1} \right) \FNormS{\matA - \matA_k}
\leq \left(1+\epsilon\right)\FNormS{\matA - \matA_k},\label{eqn:weight}
    t^{-1} = \frac{1}{2}\left(U(\u_j,\delta_\scu,\matB_\tau,\scu_\tau) + L(\v_j,\delta_\scl, \matA_\tau,\scl_\tau)\right).
    
\delta_\scl = 1;
\qquad
\delta_\scu = \frac{1+\sqrt{\frac{\ell}{r}}}{1-\sqrt{\frac{k}{r}}}.
\label{eqn:PD1}
\scl_\tau = r\left(\frac{\tau}{r}-\sqrt{\frac{k}{r}}\right) = \tau - \sqrt{rk};\\
\scu_\tau = \frac{(\tau-r)\left(1+\sqrt{\frac{\ell}{r}}\right)
               +r\left(1+\sqrt{\frac{\ell}{r}}\right)^2}
               {1-\sqrt{\frac{k}{r}}}=\delta_{\scu}\left(\tau + \sqrt{\ell r}\right).
\label{eqn:PD2}
\phil(\scl, \matA) =  \sum_{i=1}^k\frac{1}{\lambda_i(\matA)-\scl};
\qquad
\phiu(\scu, \matB) =  \sum_{i=1}^\ell\frac{1}{\scu-\lambda_i(\matB)}.
\label{eqn:PD3}
L(\v, \delta_\scl, \matA, \scl)  =  \frac{\v\transp(\matA-(\scl + \delta_\scl)\matI_k)^{-2}\v}
{\phil(\scl + \delta_\scl, \matA)-\phil(\scl,\matA)} -\v\transp(\matA-(\scl + \delta_\scl)\matI_k)^{-1}\v.
\label{eqn:PD4}
U(\u, \delta_\scu, \matB, \scu )  = \frac{\u\transp((\scu + \delta_\scu)\matI_\ell-\matB)^{-2}\u}
{\phiu(\scu, \matB)-\phiu(\scu + \delta_\scu,\matB)}
+\u\transp((\scu + \delta_\scu)\matI_\ell-\matB)^{-1}\u.
\s_{\tau} =
\left[\s_{\tau}[1],\ldots, \s_{\tau}[n]\right]\matA_\tau=\sum_{i=1}^n \s_{\tau}[i]\v_i\v_i\transp
\qquad \mbox{and} \qquad \matB_\tau=\sum_{i=1}^n\s_{\tau}[i]\u_i\u_i\transp.\label{eqn:PD5}
U(\u_j,\delta_\scu,\matB_\tau,\scu_\tau) \le t^{-1} \le L(\v_j,\delta_\scl,\matA_\tau,\scl_\tau).
\phil_0=-k/\scl_0=k/\sqrt{rk}=\sqrt{k/r}.
\nonumber \cl E\sum_{i=1}^k\frac{1}
{(\lambda_i-\scl_{\tau+1})(\lambda_i-\scl_{\tau})}
&=&\frac{1}{\delta_{\scl}}
\sum_{i=1}^k\frac{1}{(\lambda_i-\scl_{\tau+1})^2(\lambda_i-\scl_{\tau})}\\
&-&
\delta_\scl
\left(\sum_{i=1}^k\frac{1}{(\lambda_i-\scl_{\tau})
(\lambda_i-\scl_{\tau+1})}\right)^2\\
\nonumber &\ge&
\frac{1}{\delta_{\scl}}\sum_{i=1}^k\frac{1}{(\lambda_i-\scl_{\tau+1})^2(\lambda_i-\scl_{\tau})}\\
&-&
\delta_\scl
\sum_{i=1}^k\frac{1}{(\lambda_i-\scl_{\tau+1})^2(\lambda_i-\scl_{\tau})}
\sum_{i=1}^k\frac{1}{\lambda_i-\scl_\tau}\\
\label{eqn:PDD1} &=&
\left(\frac{1}{\delta_{\scl}}-\delta_\scl\phil_\tau\right)
\sum_{i=1}^k\frac{1}{(\lambda_i-\scl_{\tau+1})^2(\lambda_i-\scl_{\tau})}.
\delta_\scl^{-1}-\delta_\scl\phil_\tau\ge \delta_\scl^{-1}-\delta_\scl\phil_0=1-\sqrt{k/r}>0\label{eqn:UF}
U_F\left(\a,\delta_\scu\right) = \delta_\scu^{-1} \a\transp \a.
\matA_\tau=\sum_{i=1}^n s_{\tau,i}\v_i\v_i\transp.\matB_\tau=\sum_{i=1}^ns_{\tau,i}\a_i\a_i\transp,\trace\left(\matW+t\v\v\transp\right)\le\scu+\delta_\scu.\lambda_{\min}(\matA_\tau)\ge\scl_\tau \qquad \mbox{and}\qquad
\trace(\matB_\tau)\le \scu_\tau.
\scl_r=r\left(1-\sqrt{k/r}\right),\scu_r=r\left(1-\sqrt{k/r}\right)^{-1}\sum_{i=1}^n\TNormS{\a_i}.\frac{\TNorm{\matA-\matC\matC^+\matA}^2}{\TNorm{\matA-\matA_k}^2} \ge\frac{n+\alpha^2}{r+\alpha^2}.\matA = [\e_1+\alpha\e_2, \e_1+\alpha\e_3,\ldots, \e_1+\alpha\e_{n+1}] \in\R^{(n+1)\times n},\matA\transp\matA=\bm{1}_n\bm{1}_n\transp+\alpha^2\matI_{n}, \qquad
\sigma_1^2(\matA)=n+\alpha^2, \qquad \mbox{and}  \qquad
\sigma_i^2(\matA)=\alpha^2 \mbox{\ \ for\ \ } i>1.\matC\x=\e_1\sum_{i=1}^r x_i+\alpha\sum_{i=1}^rx_i\e_{i+1}.\hat\matA\transp\hat\matA=
\left[
\begin{matrix}
\bm0_{r\times r}&\bm0_{r\times (n-r)}\\
\bm0_{(n-r)\times r}&\matZ
\end{matrix}
\right],\matZ=\frac{\alpha^2}{r+\alpha^2}\bm1_{n-r}\bm1_{n-r}\transp +\alpha^2\matI_{n-r}.\XNormS{\matA-\Pi_{\matC,k}^{\xi}(\matA)}/\XNormS{\matA-\matA_k},\XNormS{\matA-\matC\matC^+\matA}/\XNormS{\matA-\matA_k}, \XNormS{\matA-\matC\matC^+\matA}/\XNormS{\matA-\matA_k} \le \XNormS{\matA-\Pi_{\matC,k}^{\xi}(\matA)}/\XNormS{\matA-\matA_k}. \FNormS{\matA-\Pi_{\matC,k}^{\mathrm{F}}(\matA)}/\FNormS{\matA-\matA_k}.\FNormS{\matA-\matC\matC^+\matA}/\FNormS{\matA-\matA_k}.\frac{\FNorm{\matB-\matC\matC^+\matB}^2}{\FNorm{\matB-\matB_k}^2}
\ge\frac{\ell-r}{\ell-k}\left(1+\frac{k}{r+\alpha^2}\right).
\FNormS{\matB-\matB_k}=k\FNormS{\matA-\matA_1}=k(n-1){\alpha}'^2
=(\ell-k){\alpha'}^2.\FNormS{ \matA - \matC\matC^+\matA} \le \gamma \cdot \FNormS{ \matA - \matC_{opt}\matC_{opt}^+\matA}, 
where  is the approximation factor. Also, how do the lower bounds change if we replace  with ?

\section*{Acknowledgments}
We would like to thank A. Deshpande, D. Feldman, K. Varadarajan, and J. Tropp for useful discussions, and D. Feldman and K. Varadarajan for pointing out the connections between the subspace approximation line of research~\cite{DV07,FL11, FMSW10,SV07} and our work. We would also like to thank an anonymous reviewer for 
pointing out \cite{Szy06} and how this result on oblique projections
implies Eqn.~(\ref{eqn2}) and its implications to Theorems~\ref{thmFast1} 
and~\ref{theorem:spectralIn}; this avenue 
suggested by the reviewer slightly improved our previous bounds.

This work has been supported by NSF CCF-1016501 and NSF DMS-1008983 to P. Drineas and M. Magdon-Ismail. C. Boutsidis acknowledges the support from XDATA program of the Defense Advanced Research Projects Agency (DARPA), administered through Air Force Research Laboratory contract FA8750-12-C-0323.
