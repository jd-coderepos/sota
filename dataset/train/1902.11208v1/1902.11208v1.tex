\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{pgfplots}
\usepackage[vlined]{algorithm2e}

\usepackage[T1]{fontenc}
\usepackage[acronym,shortcuts,smallcaps]{glossaries}



\renewcommand*{\firstacronymfont}[1]{\textsc{#1}}
    \renewcommand*{\acronymfont}[1]{\textsc{#1}}
\newcommand*{\acsu}[1]{\acs{#1}\glsunset{#1}}  

\renewcommand{\ac}[1]{\gls{#1}}
\renewcommand{\acp}[1]{\glspl{#1}}


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{filecontents*}{iam-scores-dropout-initial-learning-rate-0.005-mdlstm-sizes-4-20-100-CER.csv}
Epoch,CER_mdlstm_layers_2-10-50, CER_mdlstm_layers_4-20-50, CER_last_mdlstm_layer_50_DROPOUT,CER_last_mdlstm_layer_100_DROPOUT
1,79.13200551,81.09045967,80.78623378,81.50163997
2,80.5081523,78.61862433,83.84750677,65.87916528
3,81.79160527,71.69273185,81.68702762,35.93192946
4,79.92346818,62.97951229,79.99001759,27.76061225
5,79.99001759,52.01074298,81.0072729,22.37010981
6,79.57408376,43.67067548,62.5065361,20.45443742
7,76.79089224,38.3562295,51.88952797,17.82098208
8,73.9411513,35.0596568,43.78000665,18.22740885
9,74.20259543,31.47311879,37.62180919,17.29334031
10,73.63930218,28.55445168,32.75419499,17.37890384
11,71.45505538,26.60550459,27.03569901,16.23092646
12,72.60540952,26.43200076,25.12715691,15.68189381
13,40.06987688,24.84669867,23.79141513,15.72705234
14,26.12539811,24.46879308,22.64581452,14.89043115
15,20.98920949,22.98093835,21.20787184,15.28972762
16,18.60531445,21.78780244,20.97019537,14.87141703
17,16.76569853,21.22926273,19.48234064,14.96411085
18,16.44245853,20.82045919,19.2755621,14.95698056
19,15.34439321,21.29581214,19.06878357,13.73294671
20,15.25882968,19.95056329,18.91667063,14.53391643
21,15.23268527,19.63683035,18.36526121,13.67590436
22,14.66463849,18.79545563,16.94157912,13.14826259
23,14.56719114,19.57978799,16.98436089,12.99139611
24,14.05618672,18.18938062,17.02714265,12.84641346
25,14.2059229,17.93506679,17.81860531,12.31877169
26,14.63374055,17.47159766,16.47573323,13.27423112
27,14.59095879,18.43181062,17.33849884,12.33065551
28,14.36278937,17.63321766,16.29272235,12.07871845
29,13.94923231,17.07230118,15.39668204,12.12625374
30,13.8898131863,17.66649237,15.83162999,12.33303228
31,14.2487046632,17.08656177,16.13823264,11.95037315
32,14.241574369,16.95108618,15.28022056,11.73408756
33,14.7359414365,16.49237059,15.37766792,11.59148168
34,15.0734420307,16.49474735,15.54166469,11.95750345
35,14.4174549603,16.19289823,15.09007938,12.48989875
36,14.2772258402,16.31648999,16.04553881,11.36806579
37,14.778723202,16.20478205,15.48462233,11.69843609
38,16.5517897039,16.28083852,14.4958882,11.4346152
39,14.5315396682,16.45434235,14.82150497,11.47739697
40,14.9023149689,16.0669297,14.46499026,11.39183344
41,14.8856776156,15.66525645,14.80962114,11.03294196
42,14.9189523221,16.18576793,14.43409231,11.0638399
43,14.8833008509,16.14536293,14.99025526,11.24685079
44,15.2707135048,16.28321529,15.04016732,11.01630461
45,14.9664876171,16.24281029,14.01815848,10.75010695
46,15.1423682084,15.79122498,14.03241907,10.68831107
47,14.9141987926,16.10258117,13.93972525,10.79526548
48,15.1019632077,16.1881447,14.27484908,10.53857489
49,15.0377905595,16.2214194,13.99439084,10.98065314
50,14.7739696725,16.1002044,13.7472073,10.6526596
51,15.6581261587,15.86015116,14.39368731,10.68831107
52,15.3134952702,15.72705234,14.67652232,10.76912107
53,15.4893758616,15.76508057,14.47212055,10.64077578
54,14.9023149689,15.68664734,13.7876123,10.49341636
55,14.8904311451,15.87916528,13.69967201,10.61463136
56,15.2160479156,15.81261587,13.69967201,10.49816989
57,15.1708893854,15.98136616,14.19641584,10.59324048
58,15.0306602652,,14.01102819,10.26049342
59,15.1067167372,,13.36692494,10.76912107
60,15.1352379142,,13.29086847,10.46489518
61,14.8595332034,,12.65389552,10.35081048
62,15.1423682084,,12.90345582,10.57660313
63,15.5345343918,,12.37581404,10.70019489
64,,,12.4257261,10.51005371
65,,,12.33778581,10.52193754
66,,,12.71093787,10.18443694
67,,,12.50415934,9.918239293
68,,,12.1024861,10.16304606
69,,,12.53268052,10.36031754
70,,,12.29738081,10.08223606
71,,,11.98602462,10.09649665
72,,,11.37994961,10.21533489
73,,,11.74597138,10.06084518
74,,,11.55820697,10.19869753
75,,,11.50591814,10.234349
76,,,11.53206256,10.22008842
77,,,11.6033655,9.827922232
78,,,11.32290726,10.35556401
79,,,11.36806579,9.849313115
80,,,11.42035461,9.958644293
\end{filecontents*}

\begin{filecontents*}{iam-scores-dropout-initial-learning-rate-0.005-mdlstm-sizes-4-20-100-CTC-LOSS.csv}
Epoch,CTC-LOSS_mdlstm_layers_2-10-50,CTC-LOSS_mdlstm_layers_4-20-50,CTC-LOSS_last_mdlstm_layer_50_DROPOUT,CTC-LOSS_last_mdlstm_layer_100_DROPOUT
1,171.1453634,162.7803048,151.727759,151.8545585
2,138.6732548,123.9387876,136.2876921,128.6554132
3,136.2836059,107.2426609,135.9807118,83.34145815
4,132.4086573,96.39438223,135.8803175,56.92442504
5,131.5411148,86.06605186,134.2681082,46.64070481
6,131.3590468,74.85400812,106.70061,41.65954716
7,128.6145516,65.76584971,92.76394361,38.4199232
8,126.1811269,58.34655215,80.36384874,36.38213713
9,125.6067122,52.61467377,70.04402736,34.683694
10,124.3687615,48.23055982,62.28943324,33.57590081
11,122.2967035,44.41167411,56.51957173,32.63385264
12,112.6609384,41.38554159,52.09801305,31.91813928
13,85.08038988,38.85991915,48.83244041,31.41317571
14,51.55419034,36.77942878,46.1522937,30.52399076
15,37.52607478,34.73559653,44.11933431,30.1918926
16,30.77922586,33.11413299,42.64247888,29.6301296
17,26.92206612,31.63781212,41.17557286,29.43241676
18,24.20717973,30.38693406,39.91301978,29.12039473
19,22.2162321,29.03097871,38.97852384,28.90815374
20,20.52752805,28.0491429,37.88671156,28.49273457
21,19.10311962,27.05153585,37.40970364,28.60233448
22,18.02665648,26.06606738,36.72948013,25.68679518
23,17.09390025,25.2410378,36.04258553,24.71983377
24,16.22727994,24.44111556,35.38689684,24.13324822
25,15.4148126,23.70847851,35.11604526,23.78152616
26,14.66277729,22.98344203,34.59445059,23.52994158
27,14.10049198,22.35027132,34.08035773,23.41896077
28,13.52884922,21.71444149,33.64682695,22.98132498
29,12.98970835,21.15161023,33.18766617,22.80420056
30,12.4937264447,20.60851798,32.87530959,22.45504644
31,11.7881079577,20.06465519,32.54359865,22.21863426
32,11.2480800558,19.70040273,32.34336423,22.16423999
33,10.7963578295,19.19826594,31.9206096,21.9764364
34,10.4537096441,18.67153866,31.91133767,21.7320013
35,10.2674218081,18.29719336,31.66662109,21.47270819
36,9.8868954149,17.86088554,31.41170733,21.50042706
37,9.5743748208,17.44118119,31.06273442,21.21795387
38,9.3808266776,17.06879113,31.21619947,21.15830171
39,9.1534714919,16.69689219,30.9652569,21.10845967
40,8.8280499784,16.35361763,30.59955477,21.05487938
41,8.7011637468,16.011736,30.55856816,19.70225203
42,8.4577738239,15.6580684,30.43133841,19.20301004
43,8.3839107988,15.29277253,30.28668465,18.97652851
44,8.0863086134,14.99555255,30.05471683,18.76224592
45,8.0159694193,14.66903383,29.86979775,18.71972621
46,7.9613872893,14.32528473,29.68977786,18.60420973
47,7.5902538838,14.06026505,29.57153805,18.3123582
48,7.5511761041,13.73812392,29.29957053,18.24143037
49,7.2474767975,13.50171366,29.12303284,18.0729334
50,7.4355921548,13.1509116,29.37362752,17.97930784
51,7.391169848,12.87458117,29.05244761,17.91260092
52,7.1820008557,12.65141346,29.00145932,17.75578522
53,6.9817773977,12.42093242,28.91292576,17.7681012
54,6.9738297539,12.15811718,28.77427407,17.55202817
55,6.7103812771,11.85784497,28.7060817,17.53954943
56,6.6163294502,11.60572063,28.52988313,17.48951345
57,6.6050692165,11.41575709,28.48988567,17.37338301
58,6.347542654,,28.32645348,17.25923574
59,6.4571112718,,28.39119458,17.26816504
60,6.4639336272,,28.17872606,17.0720275
61,6.3638785226,,25.78740632,17.01352935
62,6.1781196232,,25.22795645,16.91853052
63,6.1472587574,,24.92665972,16.98060616
64,,,24.6275276,17.0157866
65,,,24.45663029,16.83718561
66,,,24.281775,16.79692561
67,,,24.10314869,16.1787688
68,,,23.85276514,15.91841379
69,,,23.77508313,15.76720469
70,,,23.87593253,15.62571242
71,,,23.66500618,15.65685434
72,,,22.4965876,15.56836505
73,,,21.80471782,15.46893422
74,,,21.61749517,15.49761353
75,,,21.42589737,15.37362777
76,,,21.3509143,15.33940431
77,,,21.29545843,15.2598119
78,,,21.10634686,15.19986051
79,,,21.05502999,15.17326068
80,,,20.95211666,15.08203981
\end{filecontents*}


\begin{filecontents*}{iam-scores-dropout-initial-learning-rate-0.005-mdlstm-sizes-4-20-100-WER.csv}
Epoch,WER_mdlstm_layers_2-10-50, WER_mdlstm_layers_4-20-50, WER_last_mdlstm_layer_50_DROPOUT, WER_last_mdlstm_layer_100_DROPOUT
1,,122.3159078,119.3142215,107.6447442
2,147.1613266,102.945475,100.8993817,94.35637999
3,110.578977,98.56098932,106.5430017,75.88532884
4,131.163575,92.61382799,119.5165823,72.59134345
5,131.9955031,87.38617201,99.88757729,61.2366498
6,171.410905,81.83249016,91.07363687,57.52670039
7,103.06914,77.65036537,86.45306352,52.14165261
8,114.7611017,74.48004497,82.15851602,54.51377178
9,107.925801,70.86003373,76.14390107,52.00674536
10,107.7684092,67.70095559,72.44519393,52.1866217
11,109.7020798,65.45250141,66.53175942,49.56717257
12,98.43732434,65.8010118,63.49634626,48.07195053
13,80.23608769,63.41765037,61.43901068,48.2405846
14,65.65486228,62.47329961,60.28105677,46.36312535
15,56.68353007,60.48341765,58.11129848,47.56604834
16,53.37830242,58.44856661,58.10005621,46.46430579
17,49.79201799,57.70657673,55.435638,47.50983699
18,49.56717257,57.03204047,55.05340079,47.6222597
19,46.83530073,57.93142215,54.71613266,43.69870714
20,46.01461495,55.88532884,55.07588533,46.13827993
21,46.5767285,55.99775155,53.16469927,42.86677909
22,45.13771782,53.54693648,50.30916245,41.69758291
23,45.69983137,54.6599213,51.46711636,42.04609331
24,44.14839798,52.38898257,50.56773468,42.07982012
25,44.17088252,51.97301855,52.55761664,40.93310849
26,44.82293423,51.18605958,49.60089938,43.92355256
27,45.01405284,53.20966835,51.68071951,40.50590219
28,44.03597527,50.86003373,50.10680157,39.64024733
29,43.83361439,50.66891512,47.50983699,40.41596402
30,44.0022484542,51.33220911,49.30860034,40.47217538
31,44.1596402473,50.53400787,49.96065205,39.28049466
32,44.5193929174,50.16301293,47.45362563,38.70713884
33,45.9471613266,49.54468803,47.17256886,38.30241709
34,46.3631253513,49.25238898,48.10567735,39.55030916
35,44.8903878583,48.63406408,46.93648117,40.85441259
36,44.4294547499,48.75772906,49.71332209,38.57223159
37,45.6098931984,48.83642496,47.90331647,38.07757167
38,48.8926363125,49.07251265,45.57616639,38.53850478
39,45.7448004497,49.21866217,46.08206858,38.56098932
40,45.7448004497,48.48791456,45.55368184,38.34738617
41,45.9134345138,47.39741428,46.18324902,37.35806633
42,46.2282181001,48.7802136,45.74480045,37.48173131
43,46.048341765,48.48791456,46.97020798,37.56042721
44,46.9027543564,49.76953345,47.22878021,37.31309725
45,46.0146149522,48.76897133,44.81169196,36.68353007
46,46.9477234401,47.65598651,45.04777965,36.42495784
47,45.7672849916,48.57785273,44.53063519,36.61607645
48,47.0376616076,48.34176504,45.35132097,35.22203485
49,46.4530635188,48.7802136,44.32827431,37.20067454
50,45.9584035975,48.80269815,43.68746487,36.11017426
51,47.6110174255,48.28555368,46.28442945,36.0539629
52,47.2849915683,47.40865655,46.93648117,36.3237774
53,47.1725688589,47.37492974,45.9584036,35.77290613
54,46.3293985385,47.56604834,44.00224845,35.71669477
55,45.9921304103,48.40921866,44.11467116,35.80663294
56,46.7678471051,47.48735245,44.06970208,35.84035975
57,46.7116357504,48.8589095,45.67734682,35.82911748
58,46.3968521641,,44.55311973,35.46936481
59,46.2057335582,,42.88926363,36.97582912
60,46.8015739179,,43.01292861,35.975267
61,46.1382799325,,42.09106239,35.06464306
62,46.6779089376,,41.98988196,35.98650927
63,47.8583473862,,40.6183249,36.16638561
64,,,41.6638561,36.15514334
65,,,40.66329399,36.22259696
66,,,41.742552,35.12085441
67,,,41.09050028,34.05283867
68,,,40.10118044,34.97470489
69,,,41.67509837,35.41315346
70,,,41.09050028,34.85103991
71,,,40,34.40134907
72,,,38.61720067,34.99718943
73,,,39.67397414,34.80607083
74,,,39.16807195,35.1658235
75,,,39.15682968,35.33445756
76,,,38.67341203,35.25576166
77,,,39.40415964,33.88420461
78,,,38.34738617,35.23327712
79,,,38.98819562,33.97414278
80,,,38.77459247,34.63743676
\end{filecontents*}


\begin{filecontents*}{iam-scores-dropout-initial-learning-rate-0.005-mdlstm-sizes-4-20-100-TOTAL-CORRECT.csv}
Epoch,TOTAL-CORRECT_mdlstm_layers_2-10-50,TOTAL-CORRECT_mdlstm_layers_4-20-50,TOTAL-CORRECT_last_mdlstm_layer_50_DROPOUT,TOTAL-CORRECT_last_mdlstm_layer_100_DROPOUT
1,0,0,0,0
2,0,0,0,0
3,0,0,0,0
4,0,0,0,1
5,0,0,0,5
6,0,0,0,6
7,0,0,0,9
8,0,0,0,14
9,0,1,0,10
10,0,1,1,15
11,0,2,1,20
12,0,4,2,20
13,0,3,4,19
14,8,1,5,28
15,11,3,6,20
16,7,7,3,23
17,15,0,6,16
18,27,2,7,20
19,27,2,10,27
20,39,3,11,22
21,26,6,16,29
22,30,6,10,30
23,27,7,9,27
24,33,7,9,28
25,34,8,10,29
26,31,8,16,22
27,34,5,17,32
28,37,9,8,33
29,30,11,14,38
30,34,11,15,30
31,37,11,17,31
32,35,11,16,28
33,26,9,16,39
34,35,15,14,38
35,34,16,16,37
36,40,13,12,34
37,26,10,17,46
38,24,15,21,44
39,37,16,22,38
40,29,21,24,42
41,28,19,21,41
42,25,14,18,48
43,22,17,16,39
44,28,18,11,44
45,30,18,17,52
46,28,24,14,39
47,27,22,21,44
48,23,20,22,55
49,27,19,20,50
50,22,19,20,58
51,22,19,20,50
52,28,25,18,54
53,28,22,19,46
54,26,20,22,58
55,29,20,19,50
56,30,21,20,51
57,22,18,25,48
58,26,,23,42
59,29,,23,31
60,28,,27,45
61,33,,30,51
62,36,,21,40
63,27,,38,42
64,,,26,42
65,,,29,44
66,,,29,48
67,,,33,53
68,,,29,52
69,,,25,54
70,,,31,48
71,,,37,45
72,,,36,52
73,,,37,48
74,,,37,48
75,,,33,43
76,,,35,44
77,,,31,52
78,,,32,51
79,,,33,53
80,,,39,55
\end{filecontents*}



\newglossaryentry{MDLSTM}
{
  name={MDLSTM},
  description={multi-dimensional long short-term memory},
  first={\glsentrydesc{MDLSTM} (\glsentrytext{MDLSTM})},
  plural={MDLSTMs},
  descriptionplural={multi-dimensional long short-term memories},
  firstplural={\glsentrydescplural{MDLSTM} (\glsentryplural{MDLSTM})}
} 

\newglossaryentry{LSTM}
{
  name={LSTM},
  description={long short-term memory},
  first={\glsentrydesc{LSTM} (\glsentrytext{LSTM})},
  plural={LSTMs},
  descriptionplural={long short-term memories},
  firstplural={\glsentrydescplural{LSTM} (\glsentryplural{LSTM})}
} 

\newglossaryentry{RNN}
{
  name={MDLSTM},
  description={recurrent neural network},
  first={\glsentrydesc{RNN} (\glsentrytext{RNN})},
  plural={RNNs},
  descriptionplural={recurrent neural networks},
  firstplural={\glsentrydescplural{RNN} (\glsentryplural{RNN})}
} 

\newglossaryentry{CER}
{
  name={CER},
  description={character error rate},
  first={\glsentrydesc{CER} (\glsentrytext{CER})},
} 

\newglossaryentry{WER}
{
  name={WER},
  description={word error rate},
  first={\glsentrydesc{WER} (\glsentrytext{WER})},
} 

\newglossaryentry{LMBR}
{
  name={LMBR},
  description={last-minute batch-padding},
  first={\glsentrydesc{LMBR} (\glsentrytext{LMBR})},
} 

\newglossaryentry{HTR}
{
  name={HTR},
  description={handwritten text recognition},
  first={\glsentrydesc{HTR} (\glsentrytext{HTR})},
} 

\newglossaryentry{HR}
{
  name={HR},
  description={handwriting recognition},
  first={\glsentrydesc{HR} (\glsentrytext{HR})},
}

\newglossaryentry{NHR}
{
  name={NHR},
  description={neural handwriting recognition},
  first={neural handwriting recognition (\glsentrytext{NHR})},
} 



\newcommand{\EXCLUDE}[1]{}

    
\begin{document}


\title{No Padding Please:\\ Efficient Neural Handwriting Recognition} 



\author{\IEEEauthorblockN{1\textsuperscript{st} Gideon Maillette de Buy Wenniger}
\IEEEauthorblockA{\textit{The Adapt Centre}\\ \textit{Dublin City University} \\
Dublin, Ireland \\
gemdbw@gmail.com}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Lambert Schomaker}

\IEEEauthorblockA{
\textit{Bernoulli Institute for Mathematics,}\\ 
\textit{Computer Science and}\\
\textit{Artificial Intelligence}\\
\textit{University of Groningen} \\
Groningen, The Netherlands \\
l.r.b.schomaker@rug.nl}
\and
\IEEEauthorblockN{3\textsuperscript{th} Andy Way}
\IEEEauthorblockA{\textit{The Adapt Centre}\\ \textit{Dublin City University} \\
Dublin, Ireland \\
andy.way@adaptcentre.ie}
}


\maketitle


\begin{abstract}
\Gls{NHR} is the recognition of handwritten text with deep learning models,
such as \ac{MDLSTM} recurrent neural networks. 
Models with \ac{MDLSTM} layers have achieved state-of-the art results on handwritten text recognition
tasks. While multi-directional \ac{MDLSTM}-layers have an unbeaten ability to capture the complete context in all directions, this strength 
limits the possibilities for parallelization, and therefore comes at a high computational cost.

In this work we develop methods to create efficient \ac{MDLSTM}-based models for \ac{NHR}, particularly a method 
aimed at eliminating computation waste that results from padding. This proposed method, called \emph{example-packing},
replaces wasteful stacking of padded examples with efficient tiling in a 2-dimensional grid.
For word-based \ac{NHR} this yields a speed improvement of factor 6.6 over an already efficient baseline of minimal 
padding for each batch separately. For line-based \ac{NHR} the savings are more modest, but still significant. 

In addition to example-packing, we propose: 1) a technique to optimize parallelization for dynamic graph definition 
frameworks including PyTorch, using convolutions with grouping, 2) a method for parallelization across GPUs for variable-length
example batches.
All our techniques are thoroughly tested on our own PyTorch re-implementation of \ac{MDLSTM}-based \ac{NHR}
models. A thorough evaluation on the IAM dataset shows that our models are performing similar to earlier implementations of state-of-the art models. 
Our efficient \ac{NHR} model and some of the reusable techniques discussed with it offer ways to realize relatively efficient models 
for the omnipresent scenario of variable-length inputs in deep learning.  


\end{abstract}

\begin{IEEEkeywords}
variable length input, example-packing, multi-dimensional long short-term memory, handwriting recognition, deep learning, fast deep learning
\end{IEEEkeywords}


\section{Introduction}

Over the last few years, end-to-end deep learning models for automatic handwriting recognition \cite{Bluche2013FeatureEW,PhamEtAl2014,Voigtlaender2016} have started to become competitive with earlier approaches, such as those based on hidden Markov models \cite{MartiAndBunke:2001, BengioEtAl2004}.\footnote{Sometimes handwriting recognition is  called \ac{HTR} in the literature. We opted for ``handwriting recognition'' for brevity, and also taking into account a markedly higher google n-gram frequency.}
A key ingredient to the success of these models has been the application of \acp{MDLSTM} \cite{mdlstmsGraves2007}.
However, the successful application of \acp{MDLSTM} for \ac{HR} is complicated by two main factors: 1) their computational 
cost and 2) their instability during learning. Because of these challenges, some researchers have questioned the need for using 
\acp{MDLSTM} in the first place \cite{Puigcerver2017}, and/or suggested to (partly) replace them by convolutional layers which are better understood 
and easier to use out of the box in most deep learning frameworks \cite{CarbonellEtAl2018}.
But despite the difficulties, \acp{MDLSTM} and variants have a strong theoretical strength, 
which is their ability to capture the complete surrounding context at every cell in an \ac{MDLSTM}-layer. This is particularly true 
for the multi-directional version of \acp{MDLSTM}, for example the 4-directional \ac{MDLSTM} combines the complete context of all 
cells around a particular cell, in each of the four scanning directions. Whereas deep convolutional networks might be argued to be able
to approximate this, the conceptual elegance of \acp{MDLSTM} combined with their empirical success for \ac{HR}
as described in recent literature \cite{PhamEtAl2014,Voigtlaender2016} make it unattractive to dismiss them altogether. 
As such, this paper focusses on the question: when MDLSTMs are applied for neural \ac{HR}, 
how can this be done effectively and efficiently?

Concerning computational cost, a major problem with the original implementations of \acp{MDLSTM} was that 
the inherent sequential dependencies in the computation forced these implementations to compute each layer cell-by-cell.
A big leap has been made by the insight that in fact the dependencies in \acp{MDLSTM} computation still allow for 
sets of cells to be computed in parallel, conceptually scanning an image diagonally in parallel, rather than column by column 
as in the naive earlier implementations. Furthermore, using a smart reorganization 
of the input, which we will refer to in this paper as the \emph{input-skewing trick} \cite{VanDenOord:2016:PixelRecurrentNeuralNetworks}, this parallel computation can be done efficiently 
on GPUs using convolution as a the workhorse. 

Concerning computational stability, it has been found that \acp{MDLSTM} exhibit a severe conceptual problem in that the values of the memory-state tensor of 
\acp{MDLSTM} can grow drastically over time and even become infinite. This can hamper learning, or even derail it altogether. 
This phenomenon and the mathematical principles behind it have been thoroughly described by \cite{LeifertEtAl2014}. More importantly, 
the authors also describe improved versions of \acp{MDLSTM}, the most effective of which is called \emph{Leaky-LP cell}. These \emph{stable cells} overcome 
the instability problems of \acp{MDLSTM} while retaining their key ability to preserve state, i.e. keep things in memory, over a long time. The application of 
these stable cells was found to be crucial by \cite{Voigtlaender2016}, and we share their finding when re-building models to reproduce state-of-the art 
literature results for neural \ac{HR}.

Inspired by the earlier work of  \cite{PhamEtAl2014}, \cite{Voigtlaender2016}
and \cite{VanDenOord:2016:PixelRecurrentNeuralNetworks}, we started out on this work with the conviction that it should be possible 
to build effective as well as computationally efficient \ac{MDLSTM}-based \ac{HR} models while sticking to existing deep learning frameworks. 
This paper is the result of this mission. It contributes 
to the field with a thorough discussion of what is needed to reproduce state-of-the art \ac{HR} results with \ac{MDLSTM}-based models. 
It introduces an array of techniques that can be applied to make \acp{MDLSTM} fast, while using standard deep learning frameworks. Finally, it proposes 
a new technique called \emph{example-packing} which can be used to eliminate the majority of padding when dealing with variable-sized inputs. This technique 
alone can yield major computational gains by drastically reducing the waste of GPU memory and computation spend on padding. The saved memory enables larger 
batch sizes, yielding significant speedups. 



\begin{figure*}[h!]
\begin{center}
  \includegraphics[scale=0.20]{Model-network-structure.pdf}
\end{center}
\caption{Network model structure, adapted from \cite{PhamEtAl2014}, with places where packing/unpacking are applied for efficient computation.}
\label{figure:network-model-structure}
\end{figure*}

\section{Overview}
\glslocalreset{NHR}
The term handwriting recognition is sometimes used to cover the whole range of sub-tasks associated with handwritten text, 
including for example \emph{word spotting} \cite{RathAndManmatha2007, FischerEtAl:2012}, which groups word images into clusters of similar 
words. In the context of this publication however, we use \ac{NHR} to refer specifically to the task of creating text output from 
handwritten text images. More precisely: 1) the input is in the form of (cut-out) images of handwritten text, i.e. word-strips or line-strips, 2) output is in the form of character or word-sequences, 3) a deep learning model is used to produce sequences 
of character probabilities, this is used in combination with connectionist temporal classification (CTC) \cite{Graves:2006:CTC}
and the associated loss-function, 4) the whole system is trained end-to-end with mini-batches of labeled examples, using 
back-propagation and other standard deep learning techniques.
Our  model is chosen to be almost identical to the one used in \cite{PhamEtAl2014}, 
with the difference that we share the last, fully-connected layer across directions.
Figure \ref{figure:network-model-structure} shows the model structure.\footnote{Note that input/stride sizes, e.g.  , are of the form height  width in this diagram.}
It also includes the places where \emph{example-packing} can be applied,
to process variable-sized examples more efficiently using minimal padding. Example-packing is discussed in section \ref{section:example-packing}.
In Appendix C we discuss details regarding the software that has been used in this work.


\section{Leaky LP cells: a stable version of MDLSTMs}
\label{section:leaky-lp-cells}

\acp{MDLSTM} \cite{mdlstmsGraves2007} are the multi-dimensional extension of \acp{LSTM}. 
In this paper we focus on the 2-dimensional version of \acp{MDLSTM}, which is the version 
used for \ac{NHR}. For readers unfamiliar with the details of this type of network cells, it is helpful 
to make a comparison with one-dimensional \acp{RNN} and \acp{LSTM} \cite{Hochreiter:1997:LSMT}, 
see Figure \ref{figure:2dmdlstm-explained-from-1d}.
For simple 1D-\acp{RNN} there is one input and hidden unit, and one output. For \acp{MDLSTM} this is extended with an additional 
input state and output state. In contrast, for 2-D-\acp{MDLSTM} there are two 
neighboring \emph{predecessor} cells in a conceptual 2D grid of cells used for computation. Each neighbor provides a pair of 
of \emph{hidden}, \emph{state} inputs, with the other input called \emph{cell input}   
coming from the computed cell, as in the 1D case. 
Furthermore,  stacking the output of four different \acp{MDLSTM}, one for each possible direction 
the 2-D grid can be scanned, yields a 4-directional 2-D-\ac{MDLSTM}. 
This version is the one typically used for \ac{NHR}.
Figure \ref{figure:2dmdlstm-compuational-graph} shows the computational graph for \acp{MDLSTM}. A crucial role is played 
by two forget gates used to weigh and then combine the states  and , obtained as inputs from the two predecessor cells. 
The value of these gates is in the range zero to one, and here lies a problem. When the sum of the gate activations becomes larger than one, 
the absolute values of the state entries, i.e. the norm of the state, can grow over time. As thoroughly discussed in \cite{LeifertEtAl2014}
and confirmed in our own experiments, this is a real problem and not just a theoretical one. States can grow rapidly over time, 
and even become infinity, and gradient clipping cannot fix this either. A naive solution would be to simply multiply 
the output of both gates by a factor 0.5, guaranteeing that the combined gate activation never exceeds 1. But \cite{LeifertEtAl2014}
note that while fixing the instability problem, this causes a new problem by making it impossible for the cell to preserve state, that is 
remember, over a long time. Therefore, they proposed a more rigorous solution, introducing so-called \emph{lambda-gates} that 
produce a weighted sum of the inputs, with the weights being predicted by the gate and summing to one, see Figure \ref{figure:leaky-lp-cell-compuational-graph}. 
This solves the severe problems of \ac{MDLSTM} instability, as also reported by \cite{Voigtlaender2016}, 
while retaining the crucial ability to preserve state over a long time. 

Based on the idea of lambda-gates, multiple variants of stable \ac{MDLSTM} cells are possible, as discussed in \cite{LeifertEtAl2014}, with the best performing 
one being the Leaky LP cell, but all of them yielding solid results. In this work, we use a slight variant of the Leaky LP cell: 
the previous memory state is used in place of the newly computed memory state as (memory) input to the two output gates. 
This variant yielded faster learning and superior results in our experiments.



\begin{figure*}[!htb]
\begin{center}
\begin{minipage}{.54\textwidth}
  \includegraphics[scale=0.30]{2D-MDLSTM-explained-based-on-1D-RNN-and-1D-LSTM.pdf}
 \caption{1-D LSTM versus 2-D-MDLSTM computational structure}
\label{figure:2dmdlstm-explained-from-1d}
\end{minipage}
\begin{minipage}{.42\textwidth}
  \includegraphics[scale=0.5]{input-skewing-trick.pdf}
 \caption{The \emph{input-skewing} trick.} 
\label{figure:input-skewing-trick}
\end{minipage}
\end{center}
\end{figure*}


\begin{figure*}
\begin{center}
\hspace{-2cm}
\begin{subfigure}[b]{0.37\textwidth}
  \includegraphics[scale=0.3]{2D-MDLSTM-Computational-Graph.pdf}
 \caption{2-D MDLSTM computational graph.}
\label{figure:2dmdlstm-compuational-graph}
\end{subfigure}
\hspace{2.5cm}
\begin{subfigure}[b]{0.37\textwidth}
 \includegraphics[scale=0.3]{LeakyLPCell-Computational-Graph.pdf}
 \caption{2D Leaky LP cell computational graph: with  gates.}
\label{figure:leaky-lp-cell-compuational-graph} 
\end{subfigure}
\caption{Computational graph for \ac{MDLSTM} and its stable variant Leaky LP cell.}
\end{center}
\end{figure*}



\section{Parallelizing the computation}

To make  training and application of \ac{MDLSTM}-based \ac{NHR} models computationally feasible, 
parallelization is crucial. 
 Parallelization over the batch computation is the simplest form of parallelization, and is 
available without any additional effort in most deep learning frameworks. 
However, due to the large amount of memory required by each example and its \ac{MDLSTM} representations, 
it is typically not possible to drastically increase the batch size as a simple way to increase parallelization.
The exact batch size possible for \ac{NHR} will depend per problem and setup\footnote{Being determined by GPU memory, height, width, number of (color) channels of the examples, network structure and parameterization and other factors.},
but in practice very large batch sizes are not possible. 
For this reason, additional other forms of parallelization should be considered and implemented wherever possible. 


\subsection{Parallel column computation using the input-skewing trick}
The next optimization with large impact is the parallelization of \ac{MDLSTM} computation within image columns, introduced by \cite{VanDenOord:2016:PixelRecurrentNeuralNetworks},
increasing the level of parallelism by a factor as large as the example height. Notably, a similar trick was discovered
independently by \cite{Voigtlaender2016}, but their variant relied on low-level implementation rather than leveraging existing tools, particularly efficient implementations of convolution, within deep learning frameworks.
As explained in section \ref{section:leaky-lp-cells}, computation of 2-D \acp{MDLSTM} can be modeled as a grid of computational ``cells'', 
whereby each cell takes context input on two neighboring ancestor cells, one above and one on the left (as defined by the scanning direction). 
Figure \ref{figure:input-skewing-trick} shows what we will call the \emph{input-skewing} trick: 
each of the  image rows  is shifted  
with an increasing number of  pixels. The results is a diagonally skewed input image that can be  
applied for efficient computation of the \ac{MDLSTM} using convolution. 
The input-skewing trick transforms the original input into a row-shifted version. This produces a conceptual grid of computational cells,
whereby each column of cells depends only on cells in the previous column. 
The skewed input image and corresponding computational grid, contains cells that correspond to valid/invalid inputs, 
marked by black/red squares in Figure \ref{figure:input-skewing-trick}. 
A binary mask tensor with ones/zeros for the valid/invalid cells is used to mask invalid inputs during computation.
Skewed input image plus mask thereby enable fast computation with framework-native convolutional layers, 
without low-level programming.



\subsection{Convolutions with Grouping}

Another technique to further increase parallelism, 
relevant to deep learning frameworks with \emph{dynamic graph definition} including PyTorch, 
is to make use of \emph{convolutions with grouping}.\footnote{The technique may not be relevant for tensorflow and other frameworks working with static computational graph definition. At the expense of less flexibility, the pre-compilation and optimization of the computational graph in such frameworks solve some of
the problems of poor automatic parallelization that occur when working with dynamic computational graphs.}
While conceivably this technique has been used in other domains, to the best of our knowledge, it has not been proposed for \ac{NHR}.
In convolution with grouping, the computation is partitioned into  input groups and  
output groups. Each of the  outputs is only connected to the nodes of one of the  input groups, whereby  must be an exact multiple of .
The output of a convolution network with  input and  output groups is the same as for  separate networks, each with th of the inputs connected to one th of the outputs; but the difference is that these  grouped computations are performed in parallel rather than sequentially.

Figure \ref{figure:2dmdlstm-compuational-graph} shows the computational graph for 2D-\ac{MDLSTM} computation. 
Taking into account the structure of the graph, there are three main observations that enable 
optimization of \ac{MDLSTM} computation (and analogously Leaky LP cell computation), using convolutions with grouping: 
\begin{enumerate}
\item The computation of one MDLSTM cell / a column of MDLSTM cells (using the input-skewing trick), can be divided into 
a set of operations that are mutually independent, and hence can be computed in parallel. 
\item Most of the computations rely on the current pixel input and/or the ancestor hidden and memory states as inputs. 
\item Even when the inputs differ, it remains possible to parallelize multiple computations using a single convolution with grouping.
This is done by having multiple input groups in addition to multiple output groups.
Finally, in cases where the number of outputs per input differs per input, 
this can be solved by replicating some of the inputs multiple times. This gets around the typical restriction that  must 
be an exact multiple of . 
\end{enumerate}

In the concrete case of 2D-\ac{MDLSTM} computation (see Figure \ref{figure:2dmdlstm-compuational-graph}), there are five matrix computations necessary to get input activations for the 
input node, input gate, two forget gates, and output gate. 
Since these input activation computations depend only on
the input, either an image or input from the previous network layer, they can all be computed in parallel using a 2-dimensional
convolution. This convolution is of size  for both filters and stride, to avoid overlap between convolution kernel applications.
It is then further parallelized using grouping, in this case with just one input group and five output groups.
Convolutions with grouping are similarly applied to parallelly compute the multiple tensors 
that use the same hidden or memory states as the input. Details are given in Appendix A.


\subsection{Example-list based multiple-GPU training}
Parallelization over multiple GPUs is another way to further increase the speed of computation.
This type of parallelization is typically supported natively in deep learning frameworks, for example 
in PyTorch there is a method \emph{DataParallel} that supports it. However, the problem with DataParallel 
is that it only works with tensors of the same size. Essentially it expects a data and label tensor 
with uniform dimensions, so that it can divide these into a number of chunks, and provide one data, label
chunk pair to each GPU. For different-sized example images, which are the norm in \ac{NHR}, this 
approach breaks down. To fix it, we create a custom DataParallel that accepts the data to come in the form of a list of 
variable-size image tensors. The list is then split into approximately equal-size sub-lists, and a data sub-list with 
corresponding label sub-tensor is provided to each GPU. 
 

\section{Example-Packing}
\label{section:example-packing}

\acp{MDLSTM} parallelization is restricted by the inherent recurrent dependency 
upon previous hidden and memory-states.
In the case of \ac{NHR} based on line-strips, parallelization 
within the computations for a line-strip pixel column is possible. But parallelization 
across pixel columns is not, because of the computational dependencies.
Therefore, it is desirable to exploit the ways of parallelization that are possible to their limit. 
One obvious way to increase parallelization is to increase the batch size.
But this approach, when naively applied, requires all the examples to be 
padded to the same size. For handwriting line-strips or word-strips, which 
are naturally of different dimensions, this approach is computationally 
wasteful in two ways. 
\begin{enumerate}
\item The padding pixels use up a lot of wasted computation. 
\item This  wasted computation coincides with memory waste: 
the space required for the padding could have been used to fit in more real pixels, 
needed for the end result.    
\end{enumerate}

The question is: can we overcome the limitation that all examples need to be of the 
same size, while still respecting the constraints of efficient GPU computation? 
The conclusion is, we can, by using a combination of:

\begin{enumerate}
\item \textbf{Tiling} together examples together to using a greedy space-filling algorithm  to use 
the available space as much as possible.
\item \textbf{Separating pixels} between the tiled examples.
\item \textbf{Binary masking} to block the hidden-state and memory-state input from predecessor cells 
that are not valid input cells but padding cells or separator cells according to the binary mask.    
\end{enumerate}

This is best explained by an example. Figure \ref{figure:example-packing-artificial-example-input} shows 
a set of artificial, packed examples, and Figure \ref{figure:example-packing-artificial-example-mask}
the corresponding mask.

\begin{figure}[h!]
\begin{center}
  \begin{subfigure}[b]{0.24\textwidth}
  \includegraphics[scale=0.15]{artificial_packing_example.png}
  \caption{Packed artificial examples}
  \label{figure:example-packing-artificial-example-input}
  \end{subfigure}
  \begin{subfigure}[b]{.24\textwidth}
  \includegraphics[scale=0.30]{artificial_packing_example_mask.png}
  \caption{Corresponding binary mask}
  \label{figure:example-packing-artificial-example-mask}
  \end{subfigure}
\end{center}
\caption{Packed artificial data example and corresponding mask.}
\label{figure:example-packing-artificial-example}
\end{figure}

\begin{figure*}[h!]
\begin{center}
  \begin{subfigure}[b]{0.3\textwidth}
  \includegraphics[scale=0.3]{IAM_word_example_packed_skewed.png}
  \caption{Packed IAM words data example}
  \label{figure:example-packing-iam-words}
  \end{subfigure}
  \begin{subfigure}[b]{0.6\textwidth}
  \includegraphics[scale=0.4]{IAM_lines_example_packed_skewed.png} 
  \caption{Packed IAM lines data example}
  \label{figure:example-packing-iam-lines} 
  \end{subfigure}
\end{center}
\caption{Packed IAM words and lines data}
\end{figure*}

Note that examples of the same height are arranged 
in a single row. This is a requirement for allowing column-parallelized \ac{MDLSTM} 
computation with the \emph{input-skewing} trick. 
Furthermore, note that one row of pixels is added between every pair of 
examples. These rows of pixels get skewed diagonally along with the 
input images, as a result of the input-skewing trick. 
Figure \ref{figure:example-packing-iam-words} shows the result of 
example-packing for IAM data examples, consisting of IAM word-strips.
Here, while there is still quite some padding, a large part of it is 
caused by the \emph{input-skewing} trick. But note that while not perfect, 
packing saves out a lot of padding. Without it, every example needs to be padded 
to the largest width and height dimensions occurring in the mini-batch, in this case making
all examples as wide as the first row example and as high as the last row example.
Finally, one may expect that working with line-strips instead of word-strips, 
which for many dataset, such as IAM, is the default, the differences between 
the sizes of the image strips are smaller singe the word-lengths average out.
This is indeed partly true, see Figure \ref{figure:example-packing-iam-lines},
however, at the same time the remaining height-differences imply that 
example-packing can still yield significant savings, predominantly 
in the height dimension.

\subsection{Efficient packing}

With the basic principles behind example-packing explained, the next question is 
how to find a packing that optimally uses the space, tiling the examples in 
a mini-batch such a way to add as little padding as possible. First note that 
there is no need that the dimensions across mini-batches are the same, and this 
fact is exploited in our approach. Second, note the earlier mentioned constraint:
to allow efficient skewing and un-skewing of rows of examples, all examples in the 
row must be of the same height.\EXCLUDE{\footnote{Relaxing this constraint, example (un)skewing 
must be done for every example separately, and a more complicated packing 
algorithm that considers 2-dimensions is required. We leave this for future work.}}
This motivates a four step approach: 1) bucket the examples by height, 2) 
pack the examples in each height bucket, filling up rows, by repeatedely and greedily 
adding the widest still fitting example, 3) tiling the examples first within packed 
rows, then across rows, adding separating pixels in between, 4) applying the input 
skewing trick to create a skewed version of the resulting packed tensor and mask.
In Appendix B we provide pseudocode for the packing algorithm.


Packing is performed just before the computation of each \ac{MDLSTM} layer, based on a list of input tensors for that layer.
After the \ac{MDLSTM} activations are computed on the packed tensors, unpacking is performed on these activations. Figure 
\ref{figure:network-model-structure} indicates these places where packing/unpacking is apllied in the network.
Unpacking is packing in reverse, and consists of the following two steps: 1) the inverse of the input skewing trick is performed to restore the tensor format before skewing, 2) using the original example indices corresponding to the packed examples and their sizes, 
the activations per input example are extracted, while discarding parts of the activations corresponding to separating pixels 
in the input.

\subsection{Packing for block-strided convolution layers}
\label{subsection:packing-for-block-strided-convolution}
Block-strided convolutional layers are convolutional layers with a stride  width and height (``block-size'') corresponding to the 
size of the convolution kernel, such that there is no overlap in input for different  kernel applications. 
These layers layers are used to merge the output of \ac{MDLSTM} layers and decrease resolution. They
require their own pre- and post-processing algorithms to allow efficient processing of input lists obtained from 
\ac{MDLSTM} layers.\footnote{The last fully-connected layer may be considered a special case with a block size of 
1  1.} These two algorithms perform a sort of simplified packing/unpacking.
The \emph{tensor-list chunking} (packing) algorithm chunks a list of input tensors of different sizes into blocks of given size, in our case 
the block-stride of the block-strided convolution layer. The chunking produces for each tensor a list of blocks, and stacks all these blocks 
on the batch simension. This stacked block tensor can be processed very efficiently by a standard 2-D convolution layer.
After computation of the convolutional features, using the size of the tensors in the original input list, a \emph{de-chunking}
algorithm (un-packing) concatenates the output activation blocks again together. 
This application of tensor-list chunking/de-chunking to block-strided convolution is indicated in Figure \ref{figure:network-model-structure} as well. The thus parallely computed result list is equal to what would be 
obtained if the block-strided convolution was computed for each input tensor separately and the result tensors collected in a list.


\section{Effective Optimization} 

We found the use of gradient clipping, particularly the technique proposed in \cite{pmlr-v28-pascanu13}, 
which constitutes rescaling of the gradient to normalize the gradient norm, to be necessary to achieve 
stable learning. Whereas the use of Leaky LP cells was another crucial component, in our experience 
gradient clipping was still needed on top of that to obtain good results. In addition to that, we found 
that the learning rate, max norm for gradient clipping and optimizer needed to be chosen well together.
Unfortunately, this is mostly an empirical matter, in which previous literature can at most help.
We obtained good results, using the Adam optimizer \cite{KingmaEtAl2014} with an initial learning rate of 0.005, and gradient 
clipping using a maximum gradient norm of 10 to be effective. We used the technique of \cite{Denowski2017} to improve Adam, 
by halving the learning rate and resetting the Adam state when the validation scores (WER and CER) got worse, resuming 
training from the best last model. 
Following \cite{Voigtlaender2016}, we trained for a maximum of 80 epochs, 
after which we selected the best performing model on the validation-set, and used this model 
to evaluate on the test-set.




 
\begin{figure*}
\captionsetup[subfigure]{justification=centering}
\centering
\hspace{-1.0cm}
\begin{subfigure}[b]{.26\linewidth}
\centering
\hspace{-1.9cm}  \scalebox{0.65}{
\begin{tikzpicture}[every mark/.append style={mark size=0.5pt}]
\begin{axis}[
legend style={font=\fontsize{9}{9}\selectfont, at={(1.1,1.25)}},
ymin=0,
xtick={10,20,30,40,50,60,70,80},
ytick={5,10,20,30,40,50,60,70,80},
axis lines=middle,
axis line style={->},
x label style={at={(axis description cs:0.5,-0.1)},anchor=north, font=\large},
y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south, font=\large},
xlabel={Epochs},
ylabel={CER}]
\addplot+[line width=0.5pt, dotted] table [x=Epoch, y=CER_mdlstm_layers_2-10-50, col sep=comma, mark options={scale=10}] {iam-scores-dropout-initial-learning-rate-0.005-mdlstm-sizes-4-20-100-CER.csv};]
\addlegendentry{MDLSTM layer sizes=[2,10,50]}
\addplot+[line width=0.5pt, loosely dashed] table [x=Epoch, y=CER_mdlstm_layers_4-20-50, col sep=comma, mark options={scale=10}] {iam-scores-dropout-initial-learning-rate-0.005-mdlstm-sizes-4-20-100-CER.csv};]
\addlegendentry{MDLSTM layer sizes=[4,20,50]}
\addplot+[line width=0.5pt, dashed] table [x=Epoch, y=CER_last_mdlstm_layer_50_DROPOUT, col sep=comma, mark options={scale=10}] {iam-scores-dropout-initial-learning-rate-0.005-mdlstm-sizes-4-20-100-CER.csv};]
\addlegendentry{MDLSTM layer sizes=[4,20,50], Dropout}
\addplot+[line width=0.5pt] table [x=Epoch, y=CER_last_mdlstm_layer_100_DROPOUT, col sep=comma, mark options={scale=10}] {iam-scores-dropout-initial-learning-rate-0.005-mdlstm-sizes-4-20-100-CER.csv};]
\addlegendentry{MDLSTM layer sizes=[4,20,100], Dropout}
\end{axis}
\end{tikzpicture}}
\caption{Character Error Rate (CER)}\label{figure:CERIAM}
\end{subfigure}
\hspace{-0.5cm}
\begin{subfigure}[b]{.24\linewidth}
\centering
\raisebox{0.0cm}{
\scalebox{0.65}{
\begin{tikzpicture}[every mark/.append style={mark size=0.5pt}]
\begin{axis}[
legend style={font=\fontsize{9}{9}\selectfont, at={(1.1,1.25)}},
ymin=25,
xtick={10,20,30,40,50,60,70,80},
ytick={30,40,50,60,70,80,90,100,110,120, 130, 140, 150, 160},
axis lines=middle,
axis line style={->},
x label style={at={(axis description cs:0.5,-0.1)},anchor=north, font=\large},
y label style={at={(axis description cs:-0.15,.5)},rotate=90,anchor=south, font=\large},
xlabel={Epochs},
ylabel={WER}]
\addplot+[line width=0.5pt, dotted] table [x=Epoch, y=WER_mdlstm_layers_2-10-50,  col sep=comma, mark options={scale=10}] {iam-scores-dropout-initial-learning-rate-0.005-mdlstm-sizes-4-20-100-WER.csv};]
\addlegendentry{MDLSTM layer sizes=[2,10,50]}
\addplot+[line width=0.5pt, loosely dashed] table [x=Epoch, y=WER_mdlstm_layers_4-20-50,  col sep=comma, mark options={scale=10}] {iam-scores-dropout-initial-learning-rate-0.005-mdlstm-sizes-4-20-100-WER.csv};]
\addlegendentry{MDLSTM layer sizes=[4,20,50]}
\addplot+[line width=0.5pt, dashed] table [x=Epoch, y=WER_last_mdlstm_layer_50_DROPOUT,  col sep=comma, mark options={scale=10}] {iam-scores-dropout-initial-learning-rate-0.005-mdlstm-sizes-4-20-100-WER.csv};]
\addlegendentry{MDLSTM layer sizes=[4,20,50], Dropout}
\addplot+[line width=0.5pt] table [x=Epoch, y=WER_last_mdlstm_layer_100_DROPOUT, col sep=comma, mark options={scale=10}] {iam-scores-dropout-initial-learning-rate-0.005-mdlstm-sizes-4-20-100-WER.csv};]
\addlegendentry{MDLSTM layer sizes=[4,20,100], Dropout}
\end{axis}
\end{tikzpicture}}}
\caption{Word Error Rate (WER)}\label{figure:WERIAM}
\end{subfigure}\hspace{1.8cm}
\begin{subfigure}[b]{.22\linewidth}
\centering
\hspace{-1.5cm}  \scalebox{0.65}{
\begin{tikzpicture}[every mark/.append style={mark size=0.5pt}]
\begin{axis}[
legend style={font=\fontsize{9}{9}\selectfont, at={(1.1,1.25)}},
ymin=0,
xtick={10,20,30,40,50,60,70,80},
ytick={20,40,60,80,100,120,140,160},
axis lines=middle,
axis line style={->},
x label style={at={(axis description cs:0.5,-0.1)},anchor=north,font=\large},
y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south,font=\large},
xlabel={Epochs},
ylabel={Average CTC traning loss}]
\addplot+[line width=0.5pt, dotted] table [x=Epoch, y=CTC-LOSS_mdlstm_layers_2-10-50,  col sep=comma, mark options={scale=10}] {iam-scores-dropout-initial-learning-rate-0.005-mdlstm-sizes-4-20-100-CTC-LOSS.csv};]
\addlegendentry{MDLSTM layer sizes=[2,10,50]}
\addplot+[line width=0.5pt, loosely dashed] table [x=Epoch, y=CTC-LOSS_mdlstm_layers_4-20-50,  col sep=comma, mark options={scale=10}] {iam-scores-dropout-initial-learning-rate-0.005-mdlstm-sizes-4-20-100-CTC-LOSS.csv};]
\addlegendentry{MDLSTM layer sizes=[4,20,50]}
\addplot+[line width=0.5pt, dashed] table [x=Epoch, y=CTC-LOSS_last_mdlstm_layer_50_DROPOUT,  col sep=comma, mark options={scale=10}] {iam-scores-dropout-initial-learning-rate-0.005-mdlstm-sizes-4-20-100-CTC-LOSS.csv};]
\addlegendentry{MDLSTM layer sizes=[4,20,50] Dropout}
\addplot+[line width=0.5pt] table         [x=Epoch, y=CTC-LOSS_last_mdlstm_layer_100_DROPOUT, col sep=comma, mark options={scale=10}] {iam-scores-dropout-initial-learning-rate-0.005-mdlstm-sizes-4-20-100-CTC-LOSS.csv};]
\addlegendentry{MDLSTM layer sizes=[4,20,100], Dropout}
\end{axis}
\end{tikzpicture}}
\caption{Average CTC Loss per minibatch}\label{figure:AverageCTCLossIAM}
\end{subfigure}\caption{Results on the IAM validation set for MDLSTM \ac{NHR} networks trained with or without dropout. 
For these results, decoding is done using a beam-search decoder without language model.}
\label{figure:ResultGraphsIAM}
\end{figure*}


\section{Experiments}


\subsection{Dataset}
\label{subsection:dataset}

We perform experiments on the IAM-database \cite{IAM-Database}, which is an English multi-writer 
handwriting dataset based on material from the Lancaster-Oslo/Bergen (LOB) corpus. We chose this 
dataset as it is one of the most frequently used benchmark datasets in the field, and a such 
facilitates easy comparison to other works including \cite{PhamEtAl2014, Voigtlaender2016, Puigcerver2017}. 
As such, during our reimplementation of \ac{MDLSTM}-based \ac{NHR} models from scratch,
the dataset was invaluable for testing where we stood with our system in comparison to earlier work.
The IAM database is of moderate size. It contains material of 657 different writers, 
and is partitioned into subsets for validation, training and testing of 161, 966 and 2 915 lines.
This data split corresponds to the split of the IAM (lines) dataset used in \cite{PhamEtAl2014}, \cite{Voigtlaender2016} and \cite{Puigcerver2017}. For the IAM words dataset we used the same split files, and obtained subsets for  training, validation and testing of 
55079, 8895 and 25920 words. Unfortunately, somehow these sizes for the words dataset do not match the word-set sizes for 
training, validation and testing  reported in  \cite{PhamEtAl2014} (80421, 16770, 17991)
even though they were derived from the same data spit files. This makes an exact quality comparison with \cite{PhamEtAl2014} for the word recognition systems not possible.\footnote{We took the data splits from Th\'{e}odore Bluche, as available from his website http://www.tbluche.com/resources.html. This yielded matching sizes for IAM lines, but for some reason not for IAM words. }
However, since our main quality comparison is on line recognition, this is not a major problem, 
and we leave further investigation of this issue for future work.
In combination with the IAM dataset, we use a domain-specific language model trained on material from the 
(unused parts of the) LOB corpus and the Brown corpus.


\subsection{Results}
Figure \ref{figure:ResultGraphsIAM} shows graphs tracking the model performance across training progress, in addition 
Table \ref{fig:results_on_iam_test-set_using_language_model} shows the results on the validation and test-set, using the 
best performing validation model and applying it to the test-set. Figure \ref{figure:CERIAM}
and \ref{figure:WERIAM} show the \ac{CER} and \ac{WER} scores on the validation set, without use of a language model during decoding; and 
Figure \ref{figure:AverageCTCLossIAM} shows the average CTC loss per epoch. 
From these graphs, and the table, a few observations can be made. First, the models trained using dropout
outperform these without dropout. Second, whereas in case dropout is used the larger model with \ac{MDLSTM} layer sizes of  4, 20 and 100
performs best, without dropout this model performs worse than the smaller model with \ac{MDLSTM} layer sizes of 2, 10 and 50. This is in 
line with what has been reported earlier in the literature, i.e. in \cite{PhamEtAl2014}. Third, the models without dropout 
have a CTC loss graph that keeps going down, whereas the \ac{CER} and \ac{WER} start to increase (i.e. worsen) again after a certain number 
of epochs, indicating over-fitting. In contrast, the CTC loss graphs for the models that use dropout flatten out faster after some point,
whereas the quality of the models as measured by \ac{CER} and \ac{WER} for these models keeps increasing nearly till the end.
Last, both systems with dropout not only give final better results, but also improve faster than the models without dropout, with the largest 
model with dropout showing a markedly faster improvement towards a decent system than all other systems. 
In summary, these graphs show that dropout is highly effective both in delivering superior results, as well as in our case in 
delivering them faster.

Table \ref{fig:results_on_iam_comparison_to_literature} shows a comparison of our best models against the results by \cite{PhamEtAl2014} and
\cite{Voigtlaender2016} in the literature. Comparing to the results of \cite{PhamEtAl2014}, it can be noted that our best system using dropout
is still slightly outperformed by theirs, but the difference is small. The remaining difference might still be explained by the fact that 
they use a form of curriculum learning, training first on the words and then on the lines, as well as using a different optimizer.  


We mostly followed \cite{Voigtlaender2016} in our training approach, not using curriculum learning and using Adam instead of SGD (or RMSPRob)
as an optimizer. The lower performance we obtain in comparison to \cite{Voigtlaender2016} is explainable from the our different network structure, 
which is chosen almost identical as \cite{PhamEtAl2014}. 
As our work focusses on proposing computational gains, which apply also for even fancier networks, 
for example adding max-pooling layers and layer normalization, we consider the fact that in terms of recognition accuracy our model performs slightly 
below state-of the-art not to be a big problem.

Table \ref{fig:results_on_iam_words} shows our results on IAM words and the reported results on IAM words from \cite{PhamEtAl2014}. Our results on 
IAM words without vocabulary are worse than those of \cite{PhamEtAl2014} in this setting. However, as discussed before, these scores cannot really be compared. 
Using the same dataset split as for IAM lines,  our training set became considerably smaller than the size reported in \cite{PhamEtAl2014}, 
which probably explains the score-differences.



\begin{table}
\caption{Recognition quality results on the IAM  lines validation-set and test-set.}
\centering
 \begin{tabular}{|l|l|l|l|l|}
 \hline
 & \multicolumn{2}{|c|}{validation} &  \multicolumn{2}{|c|}{test}\\
 \hline
 System & WER & CER & WER & CER \\
 \hline
 Leaky LP Cell [2,10,50], no dropout  & 43.8 & 13.9 & 52.2 & 18.8  \\
 \ \ \ \ \ \ \ \ + Vocabulary and LM  & 15.6  & 6.4 & 22.1 &  9.9 \\
 Leaky LP Cell [4,20,50], no dropout  & 47.4 & 15.7  & 54.6  &  20.4  \\
 \ \ \ \ \ \ \ \ + Vocabulary and LM  &  19.1  & 8.9  & 25.1 &  12.9  \\
 \hline
 Leaky LP Cell [4,20,50], dropout  & 38.4 & 11.3 & 44.0 & 14.5 \\
 \ \ \ \ \ \ \ \ + Vocabulary and LM  & 17.7 &  7.8 & 18.6 &  8.3 \\
 Leaky LP Cell [4,20,100], dropout  & 33.9 & 9.8& 40.8 & 12.9  \\
 \ \ \ \ \ \ \ \ + Vocabulary and LM  & 15.5 & 6.4 & 15.9 & 6.6 \\
 \hline
 \end{tabular}
\label{fig:results_on_iam_test-set_using_language_model}
\end{table}


\begin{table}
\caption{Comparison to literature results on IAM lines.}
\centering
 \begin{tabular}{|l|l|l|l|l|}
 \hline
 & \multicolumn{2}{|c|}{validation} &  \multicolumn{2}{|c|}{test}\\
 \hline
 System & WER & CER & WER & CER \\
 \hline
 Leaky LP Cell [4,20,100], dropout  & 33.9 & 9.8& 40.8 & 12.9  \\
 \ \ \ \ \ \ \ \ + Vocabulary and LM  & 15.5 & 6.4 & 15.9 & 6.6 \\
 \hline 
 \hline 
 Pham et.al (2014), no dropout & 36.5 & 10.4 &  43.9 & 14.4 \\
 \ \ \ \ \ \ \ \ + Vocabulary and LM & 12.1 & 4.2 &  15.9 & 6.3 \\
 Pham et.al (2014), dropout & 27.3 & 7.4 &  35.1 & 10.8  \\
 \ \ \ \ \ \ \ \ +  Vocabulary and LM & 11.2 & 3.7 &  13.6 & 5.1  \\
 Voigtlaender et.al (2016) & 7.1 & 2.4 & 9.3 & 3.5  \\
 \hline
 \end{tabular}
 \label{fig:results_on_iam_comparison_to_literature}
\end{table}


\begin{table}
\caption{Recognition quality for IAM words and literature results using a larger training set (see section \ref{subsection:dataset}). }
\centering
 \begin{tabular}{|p{3.9cm}|l|l|l|l|}
 \hline
 & \multicolumn{2}{|c|}{validation} &  \multicolumn{2}{|c|}{test}\\
 \hline
 System & WER & CER & WER & CER \\
 \hline
 Leaky LP Cell [4,20,100], dropout  & 33.58 & 14.11 & 42.05 & 19.15  \\
 \ \ \ \ \ \ \ \ + Vocabulary  & 20.12 & 10.42 & 25.66 & 14.29 \\
 \hline 
 \hline 
 Pham et.al (2014), dropout &  --- & --- & 31.44 & 14.02  \\
 \hline
 \end{tabular}
 \label{fig:results_on_iam_words}
\end{table}


\begin{table}
\caption{Memory and time usage for models with and without example-packing, with batch sizes chosen the maximal possible 
given the observed maximum GPU memory usage.} 
\begin{tabular}{|p{2.0cm}| p{0.4cm} |p{1cm}|p{0.9cm}|p{1.1cm}|p{1.1cm}|}
\hline
Preparation of batch examples & batch size & time per epoch (HH:MM: SS)  & examples per second & max GPU1 memory use (MB)   & max GPU2 memory use (MB) \\
\hline
\multicolumn{6}{|c|}{IAM lines} \\
\hline
batch-padding & 8 &  07:24:06 & 0.243 & 10824  & 10675  \\   
\hline
example-packing & 12 &   05:04:45 &  0.355  & 10694  & 10780  \\    
\hline
\multicolumn{6}{|c|}{IAM words} \\
\hline
batch-padding & 20 &  06:26:48 & 2.38 & 11074 & 11144 \\
\hline
example-packing & 200 &   00:58:22. &  16.1  & 10827  & 10849  \\  
\hline
\end{tabular}

\label{table:memory-and-time-usage-with-or-without-packing}
\end{table}

\subsection{Impact of packing on the training time}

In this section we show the impact of packing on the training times in both the (IAM) line-recognition and 
(IAM) word-recognition scenario. In the setting were packing is not used, we instead use a strategy we will call 
\ac{LMBR}, padding examples for each batch (on-the-fly) to the maximum height and width occurring within that batch. 
\ac{LMBR} is already a faster baseline than padding all examples within the training set to the same maximum height and with, which is a simple 
but computationally wasteful strategy.


Table \ref{table:memory-and-time-usage-with-or-without-packing} shows the time consumed 
per epoch, examples per second and maximum GPU memory usages for identical models that 
were trained with or without example-packing. For these experiments, we used our best performing
model with MDLSTM layers of sizes 4, 20 and 100 plus dropout, while the batch 
sizes were chosen to be maximal given the peak GPU memory consumption for the setting. This yielded 
for line recognition batch sizes of 8 when no packing was used, and 12 when it was used and 
for word recognition batch sizes of 20 when no packing was used, and 200 when it was used 
As can be seen, these settings yield to similar maximum memory usage, and neither of the batch sizes could
be further increased without running out of the total available GPU memory (11178 MB).\footnote{In case of IAM words, we approximated the maximum batch size not giving out of memory problems by trial with a step size of 5. 
Given the large difference in the maximum possible batch sizes with and without packing in this setting, this level of precision is adequate for the purpose of our comparison, 
and finding the exact maximal possible batch size would not significantly change the results.}

For line recognition, looking at the times per epoch or  examples per second, it can be observed that using packing the same 
computation can be done in 69\% of the time used without packing. 
When testing on line strips, the savings come mostly from avoiding the need to pad 
al examples within a batch to the same height, as in this case the width differences between words average out to 
a large extent. Even so, already in this settings packing makes a noticeable difference.
Looking at word recognition next, the speed improvements are more drastic. Whereas without packing, one epochs takes about six and a half hours, 
with packing it takes only about an hour, thanks to the major gains in efficiency packing yields in this setting, indicated by the 
much larger possible batch size of 200. This major speedup, by a factor 6.6, is even relevant for models which use convolutional layers to 
replace MDLSTMs \cite{Puigcerver2017}. Since while these systems are perhaps more efficient to begin with, they still waste a lot of computation on padding 
and could therefore benefit from packing, with some adaptations for the changed network structure.

\textit{Are variable batch sizes an alternative to packing?}\\
Whereas in case of word-recognition, without packing on average about 75\% of the input pixels consists of padding, this factor four saving does not 
explain the even larger difference in possible batch sizes. The reason that the maximum batch size using packing (200) is ten times larger rather than ``just'' four times 
larger than the size without packing (20), is that for the maximum possible batch size the ``worst-case'' batch counts, and not the average batch. 
That is, a single batch with one very high and one very wide example creates a peak in memory usage, and the batch size must be chosen to allow this peak value 
to still fit in the maximal GPU memory. In contrast, when packing is used (and padding mostly avoided), the fluctuation in effective input size and consequently GPU
memory usage is much smaller.
This suggests that as a partial alternative to packing, some savings could also be made by using a variable batch size, which resizes 
based on the size of the examples within the batch.
However, this gives its own complications in terms of efficient data loading. It may also require additional measures to be taken in order to keep stable learning. Finally, note that saving out most of the factor-four blowup in size because of padding using example-packing is by itself substantial, and this saving can only be realized by packing, 
not by using variable batch sizes.




\section{Discussion}
While the packing techniques as applied in this paper are specific to \acp{MDLSTM}, the general principle 
of efficient packing and unpacking of variable-size examples provided as a list is general enough to be adapted for large speed 
improvements of many deep learning models that work with variable-sized inputs. Notably, the tensor-list chucking algorithm as discussed in 
section \ref{subsection:packing-for-block-strided-convolution} is itself an illustration of how the idea first developed for \ac{MDLSTM}
layers was then adapted for convolution layers with non-overlapping strides as well. Whereas the generalization of this algorithm to 
general convolution layers is slightly more complex than the algorithm described here, it is of a similar form.
In future work we would like to further generalize the principle of packing, so that more different network types dealing with variable-size 
inputs can benefit from it.

\section{Conclusion} 

We presented several new methods that can help to drastically increase the speed of deep learning models using \acp{MDLSTM}. 
One of these techniques, \emph{example-packing}, achieved a factor 6.6 speed improvement on word-based handwriting-recognition,
by avoiding wasted computation on padding. Our methods were thoroughly tested on a \ac{MDLSTM}-based implementation of 
state-of-the-art neural handwriting recognition models implemented with PyTorch.


\section*{Acknowledgment}  


This research has been supported by the ADAPT Centre for Digital Content Technology which is funded under the SFI Research Centres Programme (Grant 13/RC/2106) and is co-funded under the European Regional Development Fund.
\noindent 
\includegraphics[width=1cm]{flag_yellow_low.jpg}
This work has also received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Sk{\l}odowska-Curie grant agreement No 713567.
Many thanks to Joost Bastings for invaluable consultation on deep learning technology and best practices. Special thanks also to Paul Voigtlaender and 
Th\'{e}odore Bluche for their helpful and generous advise which has been important in getting MDLSTMs for handwriting recognition (from scratch) to work.


\bibliographystyle{IEEEtran}
\bibliography{references}

\clearpage

\onecolumn


\section*{Appendix}

\subsection{Further application of convolutions with grouping}
\label{appendix:application-convolution-with-grouping}

For computing fully-connected layers that take input from hidden states  and memory states, convolutions with grouping 
are applied to increase parallelization. There are five matrix computations necessary 
for each of the two hidden states  and , and three for each of the memory states  and . 
Since the hidden- and memory state computations have the same input dimensionality, 
we can all compute them using a single convolution with grouping, using replication of the input to overcome the fact that 
the number of output groups differs for the hidden- and memory states. 
Finally, we can shift the output by once cell for the second hidden- and memory state respectively, 
to get a vector of activations of the top ancestor states but overlayed with the activations of the \emph{left} ancestor states. 
That way the activation tensors for the two hidden/memory states can be directly summed to combine them for further computation.


\subsection{Packing Algorithm Details}
\label{appendix:packing-algorithm-pseudocode}

\begin{algorithm*}
 \KwData{List of examples}
 \KwResult{, : tensors,  :
  2-D array datastructure storing the original example indices for the examples in the filled rows}
 := Bucket the examples into groups of the same height ;\\
   :=  [] ;  = [] ;  := [] ;  := [] ; \\
  \For{height\_bucket  height\_buckets}{
  \While {not\_empty(height\_bucket)}{
,  := largest\_fitting\_example(, ) ; \\
    \uIf{example != None}{      
      .append(example) ; current\_indices\_row.append(original\_example\_index) ; \\
   }
   \Else{
    \# Current row is full, start a new row \\
    .append(current\_row) ; .append(current\_indices\_row) ; \\
     = [] ; current\_indices\_row = [] ;
   }
  }
  }
   := concatenate\_packing\_rows\_adding\_separation\_pixels() ;\\
   := create\_mask() ; \# all example indices replaced by 1, and all separator pixels replaced by 0   \\
   := apply\_input\_skewing() ;  := apply\_input\_skewing() ; \\
  \Return , , 
  \vspace{0.2cm}
 \caption{Packing Algorithm}
 \label{algorithm:packing_algorithm}
\end{algorithm*}

\subsection{Experimental details and source-code}
\label{appendix:code}
All our experiments were done using PyTorch. We used two NVIDIA GEFORCE\textsuperscript{\textregistered} GTX 1080 graphic cards to run our experiments.
In our work, for implementing ctc-loss, we used PyTorch bindings for warp-ctc \cite{deep-speech2016} by Baidu research.\footnote{https://github.com/baidu-research/warp-ctc}
Our version of the code\footnote{https://github.com/gwenniger/warp-ctc.}, was forked and slightly adapted from the one by Sean Naren and others\footnote{https://github.com/SeanNaren/warp-ctc}.
Additionally we used a slightly adapted version\footnote{https://github.com/gwenniger/ctcdecode.} of the ctc beam-search decoder for PyTorch, 
developed by Ryan Leary and others\footnote{https://github.com/parlance/ctcdecode}.
This decoder supports KenLM \cite{Heafield-kenlm} word-based n-gram
language models, which we use in our experiments. The rest of our source-code is intended to be made available together with a peer-reviewed publication of the work.



\end{document}