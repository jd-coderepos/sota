\section{Introduction}
In recent years, machine learning has achieved great success with the help of well-collected datasets, where the number of samples is artificially balanced among classes \cite{krizhevsky2009learning,DBLP:journals/ijcv/RussakovskyDSKS15}. However, the real-world datasets are generally imbalanced in the sense that only a few classes have numerous samples (\textit{i.e.}, the majority ones), while the others are associated with only a few samples (\textit{i.e.}, the minority ones) \cite{DBLP:journals/ijcv/EveringhamGWWZ10,DBLP:conf/eccv/LinMBHPRDZ14,DBLP:conf/cvpr/HornASCSSAPB18}. Owing to this issue, a na\"ive Empirical Risk Minimization (ERM) learning process will be biased towards the majority classes, and the generalization on the minority ones becomes challenging. Hence, the imbalanced learning problem has attracted increasing attention in recent years \cite{DBLP:journals/tkde/HeG09,DBLP:journals/corr/abs-1709-01450,DBLP:journals/pami/OksuzCKA21,DBLP:journals/corr/abs-2110-04596}.

One simple yet effective approach for imbalanced learning is to modify the na\"ive loss function, such that the learning process can pay more attention to the minority classes (Please refer to Appendix \ref{app:realted_work} for more orthogonal approaches). In this direction, existing approaches generally fall into two categories: re-weighting \cite{DBLP:conf/icml/MorikBJ99,DBLP:conf/cvpr/CuiJLSB19} and logit-adjustment \cite{DBLP:conf/nips/CaoWGAM19,DBLP:conf/iclr/MenonJRJVK21,DBLP:journals/corr/abs-2001-01385,DBLP:conf/cvpr/TanWLLOYY20,DBLP:conf/nips/KiniPOT21}. The former category assigns larger weights to the losses of the minority classes. Although intuitive, this approach might lead to difficulties and instability in optimization \cite{DBLP:conf/cvpr/CuiJLSB19,DBLP:conf/nips/CaoWGAM19,DBLP:conf/cvpr/HuangLLT16}. To tackle this issue, \citet{DBLP:conf/nips/CaoWGAM19} propose an effective scheme named Deferred Re-Weighting (DRW), where the re-weighting approach is applied only during the terminal phase of training. The latter category adjusts the logits by class-dependent terms. For example, the Label Distribution Aware Margin (LDAM) loss enforces larger margins for minority classes to achieve strong regularization \cite{DBLP:conf/nips/CaoWGAM19}. The Logit-Adjustment (LA) loss \cite{DBLP:conf/iclr/MenonJRJVK21} and the Class-Dependent Temperatures (CDT) loss \cite{DBLP:journals/corr/abs-2001-01385} utilize additive and multiplicative terms to adjust the logits, respectively. Most recently, Kini et al. \cite{DBLP:conf/nips/KiniPOT21} combine the two types of terms and proposes a unified loss named Vector-Scaling (VS) for imbalanced learning.

Although existing loss-modification methods have achieved promising performance, the theoretical insights are still fragmented and coarse-grained. To be specific, \citet{DBLP:conf/nips/CaoWGAM19} and \citet{DBLP:conf/nips/RenYSMZYL20} utilize the classic margin theory to explain the necessity of the additive terms in the LDAM loss. However, the theory fails to explain the significant improvement induced by the DRW scheme. \citet{DBLP:conf/iclr/MenonJRJVK21} analyzes the Fisher consistency property \cite{10.5555/2371238} of the additive terms in the LA loss, while providing no further generalization analysis. \citet{DBLP:conf/nips/KiniPOT21} provides a generalization analysis of the VS loss, but the results can only explain the role of the multiplicative terms under the assumption that a linear model is trained on linearly separable data. Besides, we find that the VS loss is rather incompatible with the DRW scheme, which is also out of the scope of existing theory. Hence, a gap still exists between the theory and the practice of the loss-modification approaches.

To bridge this gap, this paper provides a systematical and fine-grained analysis of loss-modification approaches. After revisiting prior arts, we find that the only property of the loss function utilized in existing proofs is the classic Lipschitz continuity \cite{10.5555/2371238,ledoux1991probability}. However, this property is global in nature such that the whole analysis provides no insight into how the losses handle different classes. Inspired by this observation, we extend the classic Lipschitz continuity with a local technique. In this way, the local Lipschitz constants on different classes exactly correspond to the class-dependent terms of the modified loss functions. And a fine-grained generalization bound is established by a novel technique named data-dependent contraction. By applying this bound to the VS loss, the mystery of re-weighting and logit-adjustment is finally uncovered. Last but not least, a principled learning algorithm is proposed based on our theoretical insights. 

To sum up, the main contributions of this paper are listed as follows:
\begin{itemize}[leftmargin=*]
    \item \textbf{New technique.} We extend the classic Lipschitz continuity and propose a novel technique named data-dependent contraction to obtain a fine-grained generalization bound for imbalanced learning.
    \item \textbf{Theoretical insights.} Based on the fine-grained bound, a systematical analysis succeeds in explaining the role of re-weighting and logit-adjustment in a unified manner, as well as some empirical results that are out of the scope of existing theories.
    \item \textbf{Principled Algorithm.} A principled algorithm is proposed based on the insights, where the re-weighting term is aligned with the generalization bound, and the multiplicative logit-adjustment term is removed during the DRW phase to avoid the incompatibility between terms.
    \item \textbf{Empirical Validation.} The empirical results on multiple benchmark datasets not only validate the theoretical results, but also demonstrate the superiority of the proposed method.
\end{itemize}

\section{Preliminary}
We first introduce the basic notations and the imbalanced learning problem in Sec.\ref{subsec:notation}. Then, we briefly review existing generalization analysis for imbalanced learning in Sec.\ref{subsec:existing_analysis}.
\subsection{Notations and Problem Definition}
\label{subsec:notation}
We assume that the samples are drawn \textit{i.i.d.} from a product space , where  is the input space and  is the label space. Let  be the imbalanced training set sampled from the imbalanced distribution  defined on ,  be the set of samples from the class ,  denote the size of ,  and . Without loss of generality, we assume that .

Let  be the balanced distribution defined on . Specifically, a class  is first uniformly sampled from , and then the input  is sampled from the class-conditional distribution . Then, our task is to learn a score function  to minimize the risk defined on the balanced distribution:

where  is the risk defined on the class , and  is the measure that evaluates the model performance at . For example, one of the most popular choices is to check whether the top-1 prediction is right: ,
where  is the indicator function. Since  is generally non-differential and thus hard to optimize, one has to select a differential surrogate loss , which induces the following surrogate risk:

Let   denote the hypothesis set. Next, we consider a family of loss functions named Vector-Scaling (VS) \cite{DBLP:conf/nips/KiniPOT21}:

The advantage behind this loss family is two-fold. On one hand, the VS loss generalizes popular re-weighting and logit-adjustment methods. For example, when , it becomes the traditional CE loss \cite{10.5555/2371238}. When , re-weighting terms  and   recover the classic balanced loss \cite{DBLP:conf/icml/MorikBJ99} and Class-Balanced (CB) loss \cite{DBLP:conf/cvpr/CuiJLSB19}, respectively.  yield the LA loss \cite{DBLP:conf/iclr/MenonJRJVK21}. When , we can deduce the CDT loss \cite{DBLP:journals/corr/abs-2001-01385}. On the other hand, an ideal surrogate loss should be Fisher consistent such that minimizing  not only can put more emphasis on minority classes, but also helps bound  \cite{10.5555/2371238,DBLP:conf/icml/MenonNAC13}. Fortunately, prior arts \cite{DBLP:conf/iclr/MenonJRJVK21} have shown that a subset of the VS loss family satisfies such a property:

where  is an arbitrary positive constant.

\subsection{Existing Generalization Analysis for Imbalanced Learning}
\label{subsec:existing_analysis}
In balanced learning, we can directly minimize the empirical balanced risk defined on the balanced datasets  sampled from :

Then, the generalization guarantee is available by traditional concentration techniques \cite{10.5555/2371238}. However, in imbalanced learning, we can only minimize the empirical risk on the imbalanced dataset :

To handle this issue, \citet{DBLP:conf/nips/CaoWGAM19} and \citet{DBLP:conf/nips/RenYSMZYL20} aggregate the class-wise generalization bound directly with a union bound over class-wise results \cite{10.5555/2371238}:
\begin{proposition}[Union bound for Imbalanced Learning \cite{DBLP:conf/nips/CaoWGAM19}]
    \label{prop:ldam_bound}
    Given the function set  and a loss function , then for any , with probability at least  over the training set , the following generalization bound holds for all :
    
    where  is the empirical risk on ; 
    denotes the empirical complexity of the function set , and  are sampled from independent distributions such as the uniform distribution with ;  denotes the asymptotic notation that omits undominated terms, that is, .
\end{proposition}
To further bound the complexity term , \citet{DBLP:conf/nips/CaoWGAM19} assume that the loss function  satisfies the Lipschitz continuity and applies the traditional contraction lemma \cite{ledoux1991probability}:
\begin{definition}[Lipschitz Continuity]
    \label{def:lipschitz}
    Let  denote the 2-norm. Then, we say the loss function  is Lipschitz continuous with constant  if for any , ,
    
\end{definition}
\begin{lemma}[Contraction Lemma]
    \label{lem:contract_lemma}
    Assume that the loss function  is Lipschitz continuous with a constant . Then, the following inequality holds:
    
\end{lemma}

Finally, the standard margin-based generalization bound \cite{DBLP:conf/nips/KakadeST08} is directly applied to obtain the upper bound of . However, this union bound has the following limitations: 
\begin{itemize}[leftmargin=*]
    \item Theoretically, this generalization bound is coarse-grained and not sharp enough. To be specific, the differences among different loss functions lie in the choice of . However, the Lipschitz continuity, which is the only property of  utilized in the proof, is global in nature and thus obscures these differences. Although the margin theory can provide some theoretical insights into the role of , the roles of  are still a mystery. Besides, since 
    
    a sharper bound might be available if we can bound  directly.
    \item Empirically, although the induced LDAM loss outperforms the CE loss,  the improvement is not so significant. Fortunately, when combining the Deferred Re-Weighting (DRW) technique \cite{DBLP:conf/nips/CaoWGAM19}, where  \cite{DBLP:conf/cvpr/CuiJLSB19} during the terminal phase of training, the improvement becomes much more impressive. However, Eq.(\ref{eq:ldam_bound}) fails to explain this phenomenon.
\end{itemize}

Recently, \citet{DBLP:conf/nips/KiniPOT21} provide a generalization analysis for the VS loss. However, the results, which only hold for linear models with linearly separable data, can only explain the roles of . For the role of , they resort to analyzing the gradient of the VS loss and provide a coarse-grained analysis.

To sum up, existing generalization analysis for imbalanced learning is coarse-grained and fragmented. Next, we aim to build a more fine-grained and systematical generalization bound that can unify the roles of both re-weighting and logit-adjustment. 

\section{Fine-Grained Generalization Analysis for Imbalanced Learning}
In Sec.\ref{subsec:ddc}, we first establish a sharp generalization bound based on a novel technique named data-dependent contraction. Then, in Sec.\ref{subsec:application}, we apply this generalization bound to the VS loss to provide a series of theoretical insights. Finally, in Sec.\ref{subsec:algorithm}, a principled algorithm is proposed based on the theoretical insights.

\subsection{Generalization Bound Induced By Data-Dependent Contraction}
\label{subsec:ddc}
Different from Eq.(\ref{eq:ldam_bound}), we hope to build a direct bound between  and . To this end, our analysis is based on the following lemma, whose proof can be found in Appendix \ref{app:basic_lem}:
\begin{restatable}{lemma}{basiclem}
    \label{lem:basic_lem}
    Given the function set  and a loss function , then for any , with probability at least  over the training set , the following generalization bound holds for all :
    
    where  contains the empirical risk on  and the  term. 
\end{restatable}
\begin{remark}
    Recall that . Hence, this lemma reveals how the model performance depends on the imbalance degree of the data. 
\end{remark}

As shown in Sec.\ref{subsec:existing_analysis}, the fine-grained analysis is unavailable due to the global nature of the classic Lipschitz continuous property. In view of this, we extend this traditional definition with a local technique \cite{DBLP:conf/colt/BartlettBM02}:
\begin{definition}[Local Lipschitz Continuity]
    We say the loss function  is local Lipschitz continuous with constants  if for any , , 
    
\end{definition}
\noindent Then, the following data-dependent contraction inequality helps us obtain a sharper bound, whose proof is given in Appendix \ref{app:data_dependent_contraction}.

\begin{assumption}
    \label{asm:complexity}
    Next, we assume that . Note that this result holds for kernel-based models with traditional techniques \cite{10.5555/2371238} and neural networks with the latest techniques \cite{DBLP:conf/colt/GolowichRS18,DBLP:conf/iclr/LongS20}. And the prior arts also adopt this assumption \cite{DBLP:conf/nips/CaoWGAM19}.
\end{assumption}

\begin{restatable}[Data-Dependent Contraction]{lemma}{datadeplem}
    \label{lem:data_dependent_contraction}
    Assume that the loss function  is local Lipschitz continuous with constants . Then, the following inequality holds under Asm.\ref{asm:complexity}:
    
\end{restatable}

Combining Lem.\ref{lem:basic_lem} and Lem.\ref{lem:data_dependent_contraction}, we have the following theorem:
\begin{theorem}[Data-Dependent Bound for Imbalanced Learning]
    \label{thm:main}
    Given the function set  and a loss function , for any , with probability at least  over the training set , the following generalization bound holds for all :
    
\end{theorem}

At the first glance, Eq.(\ref{eq:main}) seems a little loose since . In fact, this intuition holds when local Lipschitz continuity degenerates to Def.\ref{def:lipschitz}. However, when  is decreasing \textit{w.r.t.} , a shaper bound might be available. To build an intuitive understanding, we present the following proposition, whose proof can be found in Appendix \ref{app:when_sharper}.
\begin{restatable}{proposition}{whensharper}
    \label{prop:when_sharper}
    Assume that . Then, when , the data-dependent bound presented in Thm.\ref{thm:main} is sharper than the union bound defined in Prop.\ref{prop:ldam_bound}.
\end{restatable}

\subsection{Application to the VS Loss}
\label{subsec:application}
Next, we apply Thm.\ref{thm:main} to the VS loss to reveal the role of both re-weighting and logit-adjustment. To this end, it is necessary to analyze the local Lipschitz property of the VS loss, whose proof is presented in Appendix \ref{app:Lip_of_vs}.
\begin{restatable}{lemma}{lipofmargin}
    \label{lem:Lip_of_vs}
    Assume that the score function is bounded. Then, the VS loss is local Lipschitz continuous with constants , where 
    
    ;  denotes the softmax function;  denotes the minimal prediction on the ground-truth class , i.e., .
\end{restatable}

\begin{remark}
     is closely related to the minimal margin defined by . It is not difficult to check that . Hence, as we improve the model performance on class , the RHS of the above inequality, i.e., the gap between  and  will decrease, and both the minimal margin and  will increase.
\end{remark}

Then, combining Thm.\ref{thm:main} and Lem.\ref{lem:Lip_of_vs}, we have the following proposition, which reveals how the existing loss-oriented methods improve generalization performance by exploiting the data priors.
\begin{proposition}[Data-Dependent Bound for the VS Loss]
    \label{prop:vs_generalization}
    Given the function set  and the VS loss  , for any , with probability at least  over the training set , the following generalization bound holds for all :
    
\end{proposition}

From Eq.(\ref{eq:bound_margin}), we have the following insights, whose empirical validation can be found in Sec.\ref{subsec:theory_validation}.

\textbf{(In1) Why re-weighting and logit-adjustment are necessary?} Due to the term  and , the generalization bound is also imbalanced among classes. Both re-weighting and logit-adjustment can obtain a sharper generalization bound by assigning different weights to the classes with different  and . In this process,  mainly rebalances the generalization performance among classes, \textit{i.e.}, , while  and  focus on adjusting the imbalance of the terms  among classes.

\textbf{(In2) Why the deferred scheme is necessary?} As pointed out in \cite{DBLP:conf/cvpr/CuiJLSB19,DBLP:conf/cvpr/HuangLLT16}, weighting up the minority classes will cause difficulties and instability in optimization, especially when the distribution is extremely imbalanced. To fix this issue, \citet{DBLP:conf/nips/CaoWGAM19} develops a deferred scheme, where  and  during the initial and terminal phase of training, respectively. Although this scheme shows significant improvement, there is still a lack of theoretical explanation. 

Fortunately, Prop.\ref{prop:vs_generalization} can give us some inspiration. Specifically, although a weighted loss can boost the optimization on the minority classes, it is harmful to the further improvement on the majority classes, as shown in Fig.\ref{fig:drw_cifar100}. Hence, the majority/minority classes will have relatively small/large  respectively, and the generalization bound becomes even looser. By contrast, in the DRW scheme, we have  during the initial phase of training. Such a warm-up phase will encourage the model to focus on the majority classes and induce a small  for both majority and minority classes after weighting up the minority classes. On top of this, the generalization bound can become sharper, which explains the effectiveness of the deferred scheme.

\textbf{(In3) How does our result explain the design of existing losses?} On one hand, for re-weighting losses,  should decrease as  increases, which is consistent with the balanced loss with  \cite{DBLP:conf/icml/MorikBJ99} and  \cite{DBLP:conf/cvpr/CuiJLSB19}. On the other hand, from the insight \textbf{(In2)}, we know that when ,  will be increasing \textit{w.r.t.} . Hence, for logit-adjustment losses, both  and  should increase as  increases. This insight is consistent with the LDAM loss () \cite{DBLP:conf/nips/CaoWGAM19}, the logit-adjusted loss () \cite{DBLP:conf/iclr/MenonJRJVK21}, and the CDT loss () \cite{DBLP:journals/corr/abs-2001-01385}. 

\textbf{(In4) Are re-weigting and logit-adjustment fully compatible?} \textbf{(a)} Unfortunately, the answer is negative. To be specific, the re-weighting term  is decreasing \textit{w.r.t.} , whereas the multiplicative logit-adjustment term  is increasing \textit{w.r.t.} . As a result,  will weaken the effect of . \textbf{(b)} Fortunately,  is compatible with the additive logit-adjustment term  since both terms can induce a sharper generalization bound.

\subsection{Principled Learning Algorithm induced by the Theoretical Insights}
\label{subsec:algorithm}
In this part, we present a principled learning algorithm induced by the theoretical insights in Sec.\ref{subsec:application}. First, according to \textbf{(In1)-(In3)}, it is crucial to comprehensively utilize re-weighting, logit-adjustment, and the DRW scheme, as they all contribute to improving the generalization bound. Second, according to \textbf{(In4)}, we propose a Truncated Logit-Adjustment (TLA) scheme to avoid the conflict between  and . In this scheme,  still increases \textit{w.r.t.}  during the initial phase of training but is truncated to  during the terminal phase of training. Third, we set  to align  with , which we name Aligned DRW (ADRW). Note that such a re-weighting scheme also follows the Fisher consistency property presented in \cite{DBLP:conf/iclr/MenonJRJVK21}. Finally, the overall algorithm is summarized in Alg.\ref{alg:algorithm}, where the logit-adjustment methods mentioned in Sec.\ref{subsec:notation} are all reasonable options for  in the line 5 and  in the line 7.

\begin{algorithm}[t]
    \setstretch{1.3}
    \caption{Principled Learning Algorithm induced by the Theoretical Insights}
    \label{alg:algorithm}
    \begin{algorithmic}[1]
        \REQUIRE Training set  and a model  parameterized by .
        \STATE Initialize the model parameters  randomly.
        \FOR{}
            \STATE  \hfill  A mini-batch of  samples
            \IF {} 
                \STATE Set  \hfill  Adjust logits during the initial phase
            \ELSE 
                \STATE Set  \hfill  TLA and ADRW
            \ENDIF
            \STATE  \hfill  Calculate the loss
            \STATE  \hfill  One SGD step
            \STATE Optional: anneal the learning rate . \hfill  Required when 
        \ENDFOR
    \end{algorithmic} 
\end{algorithm}

\begin{figure}[t]
    \centering
    \subfigure[CIFAR-10 LT]{
      \includegraphics[width=0.23\columnwidth]{imgs/alpha/cifar10,exp.png}
     }
    \subfigure[CIFAR-10 Step]{
      \includegraphics[width=0.23\columnwidth]{imgs/alpha/cifar10,step.png}
     }
    \subfigure[CIFAR-100 LT]{
      \includegraphics[width=0.23\columnwidth]{imgs/alpha/cifar100,exp.png}
     }
    \subfigure[CIFAR-100 Step]{
    \includegraphics[width=0.23\columnwidth]{imgs/alpha/cifar100,step.png}
    }
    \caption{The balanced accuracy of the CE loss and the LDAM loss \textit{w.r.t.}  on the CIFAR datasets, where the imbalance ratio . Both re-weighting and logit-adjustment boost the model performance, which is consistent with the theoretical insight \textbf{(In1)} and \textbf{(In4-b)}.}
    \label{fig:alpha}
\end{figure}

\begin{figure}[!t]
    \centering
    \subfigure[CIFAR-10 LT]{
      \includegraphics[width=0.42\columnwidth]{imgs/sensitivity/cifar10,exp,sen.png}
     }
    \subfigure[CIFAR-10 Step]{
      \includegraphics[width=0.42\columnwidth]{imgs/sensitivity/cifar10,step,sen.png}
     }
    \caption{Sensitivity analysis of VS+ADRW \textit{w.r.t.}  and  on the CIFAR-10 dataset, where the imbalance ratio . Both re-weighting and logit-adjustment boost the model performance, which is consistent with the theoretical insights \textbf{(In1)} and \textbf{(In4-b)}.}
    \label{fig:sensitivity_cifar10}
\end{figure}

\section{Experiments}
\label{sec:experiment}
\subsection{Experiment Protocols}
Here, we briefly introduce the experiment protocols, and more details can be found in Appendix \ref{app:more_exp_protocols}.

\textbf{Datasets.} We conduct the experiments on four popular benchmark datasets for imbalanced learning. \textbf{(a) CIFAR-10 and CIFAR-100}: Following the protocol in \cite{DBLP:journals/nn/BudaMM18,DBLP:conf/cvpr/CuiJLSB19,DBLP:conf/nips/CaoWGAM19}, we consider two types of imbalance: long-tailed imbalance (LT) and step imbalance (Step). For both imbalance types, we report the balanced accuracy averaged over 5 random seeds with an imbalance ratio . \textbf{(b) ImageNet-LT and iNaturalist}: We use the long-tailed version of the ImageNet dataset\footnotemark[2] \cite{DBLP:journals/ijcv/RussakovskyDSKS15} proposed by \cite{DBLP:conf/cvpr/0002MZWGY19}, and iNaturalist\footnotemark[3] \cite{DBLP:conf/cvpr/HornASCSSAPB18} is a real-world long-tailed dataset.
\footnotetext[1]{\url{https://www.cs.toronto.edu/~kriz/cifar.html}. Licensed MIT.}
\footnotetext[2]{\url{https://image-net.org/index.php}. Licensed MIT.}
\footnotetext[3]{\url{https://github.com/visipedia/inat_comp/tree/master/2017}. Licensed MIT.}

\textbf{Baselines and Competitors.} For the CIFAR datasets, we aim to validate the theoretical results and the performance gain induced by the proposed method. Hence, we select the following baselines: the CE loss (CE) \cite{10.5555/2371238}, the LDAM loss (LDAM) \cite{DBLP:conf/nips/CaoWGAM19}, LDAM+DRW \cite{DBLP:conf/nips/CaoWGAM19}, and the VS loss (VS) \cite{DBLP:conf/nips/KiniPOT21} that generalizes the LA loss \cite{DBLP:conf/iclr/MenonJRJVK21} and the CDT loss \cite{DBLP:journals/corr/abs-2001-01385}. We tune all the hyperparameters according to the suggestions in the original papers. For the ImageNet-LT and iNaturalist datasets, we select state-of-the-art methods, listed in Tab.\ref{tab:ima_ina}, as the competitors to validate the effectiveness of the method.

\textbf{Implementation Details.} We implement three instances of the proposed learning algorithm: the CE loss equipped with the ADRW scheme ({\textbf{CE+ADRW}}), the LDAM loss equipped with the ADRW scheme ({\textbf{LDAM+ADRW}}), and the VS loss equipped with the TLA and the ADRW scheme ({\textbf{VS+TLA+ADRW}}). We tune the hyperparameter , and the other hyperparameters follow those used in the baselines. In addition, we incorporate the Sharpness-Aware Minimization (SAM) technique \cite{DBLP:conf/iclr/ForetKMN21} to facilitate the optimization of the minority classes, allowing them to escape saddle points and converge to flat minima \cite{rangwani2022escaping}. 

\begin{figure}[t]
    \centering
    \subfigure[CIFAR-100 LT ()]{
        \includegraphics[width=0.28\columnwidth]{imgs/drw2/cifar100,exp,acc.png}
    }
    \subfigure[CIFAR-100 LT ()]{
    \includegraphics[width=0.28\columnwidth]{imgs/drw/cifar100,exp,ratio.png}
    }
    \subfigure[CIFAR-100 LT ()]{
      \includegraphics[width=0.28\columnwidth]{imgs/drw/cifar100,exp,acc.png}
    }
    \caption{(a) Training accuracy of CE+DRW () and the CB loss \textit{w.r.t.} training epoch. (b)  \textit{w.r.t.} the DRW epoch , where  and  denote the training accuracy of the best model on the minority/majority classes, respectively. (c) The test accuracy of the best model \textit{w.r.t.} the DRW epoch . We can find that the DRW scheme balances the training accuracy between the majority classes and the minority classes and thus improves the model performance on the test set, which is consistent with the theoretical insight \textbf{(In2)}.}
    \label{fig:drw_cifar100}
\end{figure}

\begin{figure}[t]
    \centering
    \subfigure[CIFAR-10 LT]{
      \includegraphics[width=0.23\columnwidth]{imgs/gamma/cifar10,exp,gamma.png}
     }
    \subfigure[CIFAR-10 Step]{
      \includegraphics[width=0.23\columnwidth]{imgs/gamma/cifar10,step,gamma.png}
     }
    \subfigure[CIFAR-100 LT]{
      \includegraphics[width=0.23\columnwidth]{imgs/gamma/cifar100,exp,gamma.png}
     }
    \subfigure[CIFAR-100 Step]{
    \includegraphics[width=0.23\columnwidth]{imgs/gamma/cifar100,step,gamma.png}
    }
    \caption{The balanced accuracy of the VS loss \textit{w.r.t.}  on the CIFAR datasets, where the imbalance ratio . We can find that VS+DRW performs inferior to VS+None, especially when  is large, which is consistent with the theoretical insight \textbf{(In4-a).}}
    \label{fig:gamma}
\end{figure}

\subsection{Theory Validation}
\label{subsec:theory_validation}
In this part, we aim to validate our theoretical insights presented in Sec.\ref{subsec:application} on the CIFAR datasets. Some more empirical results can be found in Appendix \ref{app:more_exp_results}.

\textbf{Validation of (In1) and (In4-b).} We report the model performance of the baselines \textit{w.r.t.} the hyperparameters in Fig.\ref{fig:alpha} and Fig.\ref{fig:sensitivity_cifar10}. From these results, we can find that (1) Both {\color[RGB]{255, 221, 129} CE+ADRW} and {\color[RGB]{94, 227, 206} LDAM} perform better than {\color[RGB]{127, 231, 153} CE}. In other words, either re-weighting or logit-adjustment can boost the model performance. (2) {\color[RGB]{252, 136, 123} LDAM+ADRW} outperforms both {\color[RGB]{255, 221, 149} CE+ADRW} and {\color[RGB]{94, 227, 206} LDAM}, and for VS+ADRW, increasing the hyperparameters  and  appropriately can also bring performance gains. All these results validate the compatibility between re-weighting and the additive logit-adjustment.

\textbf{Validation of (In2).} We present a series of results in Fig.\ref{fig:drw_cifar100}, where the training accuracy of different classes represents the corresponding . To be specific, Fig.\ref{fig:drw_cifar100}(a) demonstrates the trend of training accuracy on the majority classes and the minority classes. For the CB loss, the learning process only focuses on the minority classes and hinders the performance improvement on the majority classes ({\color[RGB]{64, 182, 251} CB Major} \textit{v.s.} {\color[RGB]{64, 215, 189} CB Minor}). Hence, an extremely imbalanced  induces a poor generalization performance. By contrast, the DRW scheme first focuses on the majority classes ({\color[RGB]{252, 136, 123} CE+DRW Major} \textit{v.s.} {\color[RGB]{255, 213, 135} CE+DRW Minor} with the training epoch ) and then pays more attention to the minority classes during the terminal phase of training ({\color[RGB]{252, 136, 123} CE+DRW Major} \textit{v.s.} {\color[RGB]{255, 213, 135} CE+DRW Minor} with the training epoch ). Benefiting from the DRW scheme, both the majority classes and the minority classes are well-trained and thus have a balanced term  (Fig.\ref{fig:drw_cifar100}(b)), leading to a corresponding improvement on the test accuracy (Fig.\ref{fig:drw_cifar100}(c)). Even if we remove the re-weighting term ({\color[RGB]{250, 152, 146}CE+None}), the imbalance degree of  is still consistent with the test performance. Note that the learning rate of CE+None is also decreased at the corresponding epoch , making the line in Figure 3(b) and 3(c) not constant.

\textbf{Validation of (In4-a).} In Fig.\ref{fig:gamma}, we present the model performance of the VS loss \textit{w.r.t.} the hyperparameter of the multiplicative logit-adjustment . We can find that {\color[RGB]{252, 136, 123} VS+DRW} performs inferior to {\color[RGB]{64, 207, 211} VS}, especially when  is large. This phenomenon validates the incompatibility between re-weighting and multiplicative logit-adjustment.

\begin{table}[!t]
    \centering
    \caption{The balanced accuracy averaged over 5 random seeds on the CIFAR datasets. The best and the runner-up method for each protocol are marked with \Topone{red} and \Toptwo{blue}, respectively. The best baseline model is marked with \underline{underline}.}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{l|cccc|cccc}
            \toprule
            Dataset & \multicolumn{4}{c|}{CIFAR-10} & \multicolumn{4}{c}{CIFAR-100} \\
            \midrule
            Imbalance Type & \multicolumn{2}{c}{LT} & \multicolumn{2}{c|}{Step} & \multicolumn{2}{c}{LT} & \multicolumn{2}{c}{Step} \\
            \midrule
            Imbalance Ratio & 100   & 10    & 100   & 10    & 100   & 10    & 100   & 10 \\
            \midrule
            \multicolumn{9}{c}{w/o SAM} \\
            \midrule
            CE    & 71.5 & 87.0 & 64.8 & 85.1 & 38.3 & 56.7 & 38.6 & 54.4 \\
            LDAM  & 73.8 & 86.4 & 65.8 & 85.0 & 39.9 & 55.7 & 39.2 & 50.5 \\
            VS    & 78.8 & \underline{88.7} & 76.1 & \underline{88.3} & 41.8 & \underline{58.4} & \underline{46.2} & \underline{59.9} \\
            \midrule
            CE+DRW & 75.8 & 87.9 & 72.2 & 88.0 & 40.8 & 58.1 & 45.4 & 59.1 \\
            LDAM+DRW & 77.7 & 87.5 & 77.8 & 87.8 & \underline{42.7} & 57.5 & 45.3 & 56.9 \\
            VS+DRW & \underline{80.1} & 88.6 & \underline{78.2} & 88.1 & 41.3 & 57.6 & 44.0 & 58.0 \\
            \midrule
            {\textbf{CE+ADRW}} & 78.6 & 88.2 & 75.5 & 88.5 & 41.8 & 58.3 & 46.5 & 59.2 \\
            {\textbf{LDAM+ADRW}} & 79.1 & 87.6 & 78.5 & 88.1 & \textcolor[rgb]{ .4,  .671,  .867}{\textbf{43.0}} & 58.0 & 45.8 & 57.6 \\
            {\textbf{VS+TLA+DRW}} & \textcolor[rgb]{ .4,  .671,  .867}{\textbf{80.8}} & \textcolor[rgb]{ .4,  .671,  .867}{\textbf{88.8}} & \textcolor[rgb]{ .4,  .671,  .867}{\textbf{80.0}} & \textcolor[rgb]{ .4,  .671,  .867}{\textbf{89.2}} & 43.0 & \textcolor[rgb]{ .4,  .671,  .867}{\textbf{58.9}} & \textcolor[rgb]{ .4,  .671,  .867}{\textbf{46.8}} & \textcolor[rgb]{ .4,  .671,  .867}{\textbf{60.0}} \\
            {\textbf{VS+TLA+ADRW}} & \textcolor[rgb]{ .957,  .533,  .439}{\textbf{81.1}} & \textcolor[rgb]{ .957,  .533,  .439}{\textbf{89.0}} & \textcolor[rgb]{ .957,  .533,  .439}{\textbf{80.9}} & \textcolor[rgb]{ .957,  .533,  .439}{\textbf{89.3}} & \textcolor[rgb]{ .957,  .533,  .439}{\textbf{43.4}} & \textcolor[rgb]{ .957,  .533,  .439}{\textbf{59.2}} & \textcolor[rgb]{ .957,  .533,  .439}{\textbf{47.8}} & \textcolor[rgb]{ .957,  .533,  .439}{\textbf{60.5}} \\
            \midrule
            \multicolumn{9}{c}{w/ SAM} \\
            \midrule
            CE+DRW & 80.5 & 89.8 & 79.5 & 90.2 & 44.7 & 60.7 & 48.5 & \underline{61.7} \\
            LDAM+DRW & 81.6 & 89.4 & 81.2 & 89.4 & 45.2 & 59.9 & \underline{49.1} & 59.3 \\
            VS    & \underline{82.6} & \underline{90.0} & \underline{83.2} & \underline{90.5} & \underline{45.9} & \underline{61.0} & 47.4 & 61.6 \\
            \midrule
            {\textbf{CE+ADRW}} & 82.6 & \textcolor[rgb]{ .4,  .671,  .867}{\textbf{90.1}} & \textcolor[rgb]{ .4,  .671,  .867}{\textbf{82.8}} & \textcolor[rgb]{ .4,  .671,  .867}{\textbf{90.2}} & 44.9 & \textcolor[rgb]{ .4,  .671,  .867}{\textbf{61.0}} & 48.9 & \textcolor[rgb]{ .4,  .671,  .867}{\textbf{62.1}} \\
            {\textbf{LDAM+ADRW}} & \textcolor[rgb]{ .4,  .671,  .867}{\textbf{83.0}} & 89.7 & 82.4 & 90.0 & \textcolor[rgb]{ .4,  .671,  .867}{\textbf{46.3}} & 60.3 & \textcolor[rgb]{ .957,  .533,  .439}{\textbf{49.3}} & 60.3 \\
            {\textbf{VS+TLA+ADRW}} & \textcolor[rgb]{ .957,  .533,  .439}{\textbf{83.6}} & \textcolor[rgb]{ .957,  .533,  .439}{\textbf{90.3}} & \textcolor[rgb]{ .957,  .533,  .439}{\textbf{83.8}} & \textcolor[rgb]{ .957,  .533,  .439}{\textbf{90.8}} & \textcolor[rgb]{ .957,  .533,  .439}{\textbf{46.4}} & \textcolor[rgb]{ .957,  .533,  .439}{\textbf{61.9}} & \textcolor[rgb]{ .4,  .671,  .867}{\textbf{49.1}} & \textcolor[rgb]{ .957,  .533,  .439}{\textbf{62.3}} \\
            \bottomrule
        \end{tabular}}
    \label{tab:caifar}\end{table}

\begin{table*}[t]
    \centering
    \caption{The balanced accuracy on the ImageNet-LT and iNaturalist datasets.}
    \renewcommand{\arraystretch}{0.9}
    \begin{tabular}{l|c|cccc|cccc}
      \toprule
      \multicolumn{1}{c}{\multirow{2}[4]{*}{Method}} & \multicolumn{1}{c}{\multirow{2}[4]{*}{One stage}} & \multicolumn{4}{c}{ImageNet-LT} & \multicolumn{4}{c}{iNaturalist} \\
  \cmidrule{3-10}    \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & Many  &  Med. & Few   & \multicolumn{1}{c}{All } & Many  &  Med. & Few   & All  \\
      \midrule
      OLTR \cite{DBLP:conf/cvpr/0002MZWGY19}  &  & 43.2  & 35.1  & 18.5  & 35.6  & 59.0  & 64.1  & 64.9  & 63.9  \\
      LFMR \cite{DBLP:conf/eccv/XiangDH20}  &  & 47.1  & 35.0  & 17.5  & 37.2  & -     & -     & -     & - \\
      BBN \cite{DBLP:conf/cvpr/ZhouCWC20}  &  & -     & -     & -     & -     & 49.4  & \underline{70.8}  & 65.3  & 66.3  \\
      cRT \cite{DBLP:conf/iclr/KangXRYGFK20}  &  & 61.8  & 46.2  & 27.3  & 49.6  & 69.0  & 66.0  & 63.2  & 65.2  \\
      -norm \cite{DBLP:conf/iclr/KangXRYGFK20} &  & 59.1  & 46.9  & 30.7  & 49.4  & 65.6  & 65.3  & 65.5  & 65.6  \\
      DiVE \cite{DBLP:conf/iccv/HeWW21}  &  & \underline{64.1}  & 50.4  & 31.5  & 53.1  & 70.6  & 70.0  & 67.6  & 69.1  \\
      DisAlign \cite{DBLP:conf/cvpr/ZhangLY0S21} &  & 61.3  & \underline{52.2}  & 31.4  & 52.9  & 69.0  & \textbf{71.1}  & 70.2  & \underline{70.6}  \\
      WB \cite{DBLP:conf/cvpr/AlshammariWRK22}   &  & 62.5  & 50.4  & \textbf{41.5}  & \underline{53.9}  & \underline{71.2}  & 70.4  & 69.7  & 70.2  \\
      \midrule
      CE \cite{DBLP:conf/iclr/KangXRYGFK20}   &  & \textbf{65.9}  & 37.5  & 7.7   & 44.4  & \textbf{72.2}  & 63.0  & 57.2  & 61.7  \\
      CE+CB \cite{DBLP:conf/cvpr/CuiJLSB19} &  & 39.6  & 32.7  & 16.8  & 33.2  & 53.4  & 54.8  & 53.2  & 54.0  \\
      Focal \cite{DBLP:conf/cvpr/CuiJLSB19} &  & 36.4  & 29.9  & 16.0  & 30.5  & -     & -     & -     & 61.1  \\
      De-confound \cite{DBLP:conf/nips/TangHZ20} &  & 62.7  & 48.8  & 31.6  & 51.8  & -     & -     & -     & - \\
      DRO-LT \cite{DBLP:conf/iccv/SamuelC21} &  & 64.0  & 49.8  & 33.1  & 53.5  & -     & -     & -     & 69.7  \\
      SAM \cite{rangwani2022escaping} &  & 62.0  & 52.1  & 34.8  & 53.1  & 64.1  & 70.5  & \underline{71.2}  & 70.1  \\
      \midrule
      Ours &  & 62.9  & \textbf{52.6}  & \underline{37.1}  & \textbf{54.1}  & 64.7  & 70.7  & \textbf{72.1}  & \textbf{70.7}  \\
      \bottomrule
    \end{tabular}\label{tab:ima_ina}\end{table*}

\subsection{Performance Comparison}

We report the empirical results on the CIFAR datasets in Tab.\ref{tab:caifar}, where the imbalance ratio . From these results, we have the following observations: (1) The proposed learning algorithm consistently outperforms the baselines, especially when the dataset is more imbalanced. Such performance gains validate the effectiveness of the proposed methods (2) Both re-weighting and logit-adjustment can improve the model performance, which is consistent with the theoretical insight \textbf{(In1)} and \textbf{(In4-b)}. (3) When  or on the CIFAR-100 dataset, VS+DRW performs inferior to VS. Fortunately, when equipped with the proposed TLA scheme, VS+TLA+DRW outperforms both VS and VS+DRW. These results again validate our theoretical insight \textbf{(In4-a)}. (4) When , CE+ADRW outperforms LDAM+ADRW, and similar counter-intuitive phenomena are also observed in \cite{rangwani2022escaping}. We conjecture that in this case, re-weighting is enough to rebalance the generalization bound, and the additional LDAM loss might induce other issues such as inconsistency.

We present the overall balanced accuracy on the ImageNet-LT and iNaturalist datasets in Tab.\ref{tab:ima_ina}, where SAM and Ours denotes LDAM+DRW+SAM and VS+TLA+ADRW+SAM, respectively. These results demonstrate that the proposed learning algorithm outperforms the competitors, especially the one-stage ones, which again confirms the effectiveness of the proposed learning algorithm.

\section{Conclusion and Future Work}
\label{sec:conclusion}
In this work, with the proposed local Lipschitz property and the data-dependent contraction technique, we present a unified generalization analysis of the loss-modification approaches for imbalanced learning. Benefiting from this fine-grained analysis, we not only reveal the role of both re-weighting and logit-adjustment approaches but also explain some empirical phenomena that are out of the scope of existing theories. Moreover, a principled learning algorithm is proposed based on the theoretical insights. Finally, extensive experimental results on benchmark datasets validate our theoretical analysis and the effectiveness of the proposed method. 

Theoretically, one important future work is to provide a systematical Fisher consistency analysis for the VS loss, providing more insights to design re-weighting and logit-adjustment terms. Methodologically, it might be a promising direction to design an adaptive scheme that can automatically determine the hyperparameters of the learning algorithm.

\section*{Acknowledgments}
This work was supported in part by the National Key R\&D Program of China under Grant 2018AAA0102000, in part by National Natural Science Foundation of China: 62236008, U21B2038, U2001202, 61931008, 6212200758 and 61976202, in part by the Fundamental Research Funds for the Central Universities, in part by Youth Innovation Promotion Association CAS, in part by the Strategic Priority Research Program of Chinese Academy of Sciences (Grant No. XDB28000000) and in part by the Innovation Funding of ICT, CAS under Grant No. E000000. 