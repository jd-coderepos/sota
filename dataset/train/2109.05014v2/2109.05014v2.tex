\begin{table}[t]
\small
\centering
\vspace{-0.0in}
\begin{tabular}{ c l || c c}
    \hline
    & Image Repr. & \textbf{\texttt{Base}} & \textbf{\texttt{Full}} \\
    \hline
    (a) & Blind & 24.2 & 30.1 \\
    (b) & Tags & 39.3 & 44.6 \\
    (c) & VinVL-Caption-CC & 37.0 & 44.0 \\
    (d) & API-Caption & 39.1 & 45.2\\
    (e) & VinVL-Caption-COCO & 42.0 & 46.9 \\
    (f) & GT-Caption-1 & 42.1 & 48.7\\
    (g) & GT-Caption-5 & 48.0 & \underline{53.3} \\
    \hline
    (h) & VinVL-Caption-CC+Tags & 41.5 & 46.0 \\
    (i) & API-Caption+Tags & 41.9 & 47.4 \\
    (j) & VinVL-Caption-COCO+Tags & 43.3 & \textbf{48.0}\\
    \hline
\end{tabular}
\vspace{-2mm}
\caption{\small Ablation study on different textual representations for images on OK-VQA. () indicates the oracle performance. The best accuracy and oracle performance are highlighted in bold and underline, respectively.}
\vspace{-2mm}
\label{table:content}
\end{table}


\subsection{Textual Representation for Images}
We provide ablation study on how to best represent images in the textual format for GPT-3. Specifically, we compare the following methods.
\vspace{-2pt}
\begin{itemize} 
\setlength\itemsep{-2pt}
\item \textbf{Blind.} Blind is the baseline that uses an empty string to represent the image, which indicates the VQA performance without looking at the image. We also use the question similarity alone for in-context example selection to enforce the blind setting.
\item \textbf{Tags.} We represent the image as a list of tags predicted by an automatic tagging model. All tags are concatenated as a string with a comma separator.
\item \textbf{VinVL-Caption-COCO.} This is the caption used for the results in Tables~\ref{table:okvqa} and \ref{table:shots}. We fine-tune the VinVL-base pre-trained checkpoint with the COCO 2014 training set to obtain the image captions on the OK-VQA test set, which contains images from COCO 2014 validation set.
\item \textbf{VinVL-Caption-CC.} To follow a more \emph{strict} few-shot setting and avoid seeing images from the same COCO dataset, we train a VinVL-base captioning model with the Conceptual Captions dataset~\cite{sharma2018conceptual}.
\item \textbf{API-Caption.} This indicates the caption generated from the public Azure API in Footnote 2.
\item \textbf{GT-Caption.} We include the ground-truth COCO captions as the oracle with ideal image descriptions. We use either  randomly sampled ground truth or the concatenation of all  captions as the image description.
\item \textbf{Caption+Tags.} We represent the image as the concatenation of the caption string and tag list string.
\vspace{-2pt}
\end{itemize}
Table~\ref{table:content} shows the OK-VQA accuracies with different formats of textual representations. 
We summarize our findings as follows.
() All formats of textual descriptions in rows (b-j) provide a good image representation, as all methods significantly outperform the blind baseline in row (a), which only takes the question-answer pair as input.
() Despite never seeing COCO images, the VinVL-Caption-CC method in row (c) achieves a decent accuracy of  with . The performance can be further improved to  when including in-context example selection and multi-query ensemble, which surpasses the supervised state of the art.
() When comparing different predicted captions being used, VinVL-Caption-COCO in row (e) achieves the best performance. In general, we find that detailed and thorough descriptions lead to better VQA performance.
() The ground-truth COCO caption (row (f)) provides a more accurate description of the image, thus leading to an oracle accuracy of . Concatenating all the ground-truth captions as shown in row (g) provides a more thorough description of the image content, thus further improving the accuracy to .
() Inspired by the effectiveness of concatenating multiple captions, we also experiment with combining multiple formats of textual descriptions, as shown in the bottom part of the table. We observe that combining captions with tags provides complementary information and helps VQA. For example, in row (j), combining VinVL captions and tags results in a -shot accuracy of , compared with the accuracy in rows (b,e) of  and , respectively. Similar improvements are observed in other combinations as shown in rows (h,i).

\begin{table}[t]
\small \centering
\begin{tabular}{ c l || c c }
    \hline
    & Methods & CLIP & RoBERTa \\
    \hline
    (a) & Random & \multicolumn{2}{c}{43.3}  \\
    (b) & Question & 45.8 & 45.4 \\
    (c) & Question & 40.1 & 40.9 \\
    (d) & Question+Answer & \underline{49.1} & 48.4 \\
    \hline
    (e) & Image only & 44.1 & - \\
    (f) & Image+Question & \textbf{46.5} & - \\
    \hline\end{tabular}
\vspace{-2mm}
\caption{\small Ablation study on different in-context example selection methods on OK-VQA with  examples. () indicates the oracle performance. The best accuracy and oracle performance are highlighted in bold and underline, respectively.}
\label{table:qaselect}
\vspace{-2mm}
\end{table}


\begin{figure*}[t]
\begin{center}
  \centerline{\includegraphics[width=16cm]{figure/visu_okvqa_qar.pdf}}
\end{center}
\vspace{-0.3in}
    \caption{\small Qualitative examples of our proposed \textbf{\texttt{PICa}} method on the OK-VQA dataset. The upper part shows GPT-3 predicted answers, and the bottom part includes the answer rationales generated in a zero-shot manner.
	}
\vspace{-0.1in}
\label{fig:okvisu}
\end{figure*}


\subsection{Example Selection and Multi-query Ensemble}
\noindent\textbf{In-context example selection.}
Results are summarized in Table~\ref{table:qaselect}. 
Row (a) shows the \textbf{\texttt{PICa-Base}} performance where examples are randomly selected. Row (b) selects examples based on the similarity of the question textual features. We experiment with choosing the most dissimilar examples in row (c), and observe that ``bad'' examples indeed lead to worse performance. Row (d) shows an oracle number that includes the answer similarity in example selection. This serves as an upper bound, and shows that properly selecting examples can significantly improve the VQA accuracy. Rows (e,f) include image visual features for example selection. Specifically, row (e) selects examples based on image feature similarity computed by CLIP~\cite{radford2021learning}. Row (f) presents the approach in \textbf{\texttt{PICa-Full}} that jointly considers the question and image similarities. 

The improvement of row (b) over row (a) shows that in-context example selection indeed helps few-shot VQA. Row (c) presents an expected low accuracy of  with dissimilar examples, indicating the effectiveness of using question similarity to guide the in-context example selection. By selecting the ``ideal'' examples in row (d), the oracle accuracy reaches . We observe a slightly better performance when computing the feature with the CLIP text encoder~\cite{radford2021learning} than a pure language model RoBERTa~\cite{liu2019roberta}. 
Example selection with image similarity alone also improves the random baseline, as shown in row (e). The improvement is smaller than using question similarity alone in row (b), as question similarity is more informative in the VQA task. \textbf{\texttt{PICa-Full}} jointly considers the question and image similarities, and further improves the accuracy to , as in row (f).


\vspace{2mm}
\noindent\textbf{Multi-query ensemble.}
Multi-query ensemble allows the model to use more in-context examples at inference time, thus potentially further improving the performance. Multi-query ensemble can be seamlessly used together with in-context example selection. Table~\ref{table:ensemble} shows the results of combining them together. Rows (a,b) are the baseline results without multi-query ensemble. Rows (c-d) show that by increasing the number of prompts , the OK-VQA accuracy can be consistently improved on all shot numbers .


\begin{figure*}[t]
\begin{center}
  \centerline{\includegraphics[width=16cm]{figure/visu_vqav2.pdf}}
\end{center}
\vspace{-0.3in}
    \caption{\small Representative success~(the left four examples) and failure (the right two examples) cases of \textbf{\texttt{PICa}} on the VQAv2 dataset.
	}
\vspace{-0.1in}
\label{fig:vqavisu}
\end{figure*}

\subsection{Qualitative Analysis}
\label{sec:example}

\noindent\textbf{Representative cases.}
The upper part of Figure~\ref{fig:okvisu} shows some qualitative examples of our \textbf{\texttt{PICa}} predictions. We observe that \textbf{\texttt{PICa}} works well on questions that require different kinds of external knowledge. For example, in Figure~\ref{fig:okvisu}(a), GPT-3 understands that ``\texttt{this type of transportation}'' in the question refers to the ``\texttt{train}'' in the image, and provides the correct answer that ``\texttt{train was invented in 1804}''.\footnote{\url{www.google.com/search?q=when+was+train+invented}} Similarly, in Figure~\ref{fig:okvisu}(b), the model knows that ``\texttt{motorcycle was invented in 1885}''.\footnote{\url{www.google.com/search?q=when+was+motorcycle+invented}} Other than factual or encyclopedia knowledge, \textbf{\texttt{PICa}} also works well on questions that need commonsense knowledge. For example, in Figure~\ref{fig:okvisu}(c), the model understands that people can get bananas from grocery stores. The disagreement among ground-truth answers in this example also shows that the open-ended answer generation could produce different formats of the correct answer, making the evaluation challenging. Similarly, in Figures~\ref{fig:okvisu}(d,e), the model correctly answers the question with the implicit knowledge of ``\texttt{train stops at the train station}'' and ``\texttt{there could be sharks in the sea when surfacing}''.

\begin{table}[t]
\small \centering
\centering
\begin{tabular}{ c l || c c c }
    \hline
    & \# of ensembles & =1 & =4 & =16\\
    \hline
    (a) & =1 w/o selection & 34.0 & 39.7 & 43.3  \\
    (b) & =1 & 36.4 & 43.0 & 46.5  \\
    (c) & =3 & 40.0 & 45.2 & 47.7  \\
    (d) & =5 & 40.8 & 45.4 & \textbf{48.0}  \\
    \hline\end{tabular}
\vspace{-2mm}
\caption{\small The multi-query ensemble performance on OK-VQA. Experiments are based on performing in-context example selection with question and image similarity.  is the number of prompts to ensemble. Row (a) corresponds to \textbf{\texttt{PICa-Base}}, while row (d) correspond to \textbf{\texttt{PICa-Full}}.}
\label{table:ensemble}
\vspace{-3mm}
\end{table}

\vspace{2mm}
\noindent\textbf{Answer rationale.}
One may wonder how GPT-3 correctly answers the knowledge-based questions in an open-ended manner without being fine-tuned for the task. The inaccessibility of the GPT-3 raw model makes it difficult to conduct an in-depth analysis of the language model's behavior. Alternatively, we perform answer rationale prediction~\cite{li2018vqa,park2018multimodal,zellers2019recognition} in a zero-shot manner, and generate answer rationale as an open-ended text generation task. Specifically, we construct a prompt that is the concatenation of question string , predicted answer , and a prompt head ``\texttt{This is because}''. We then take GPT-3's generated text as the answer rationale.

The bottom part of Figure~\ref{fig:okvisu} shows the rationales for the predicted answers. We observe that GPT-3 generates reasonable rationales for questions that need different types of knowledge. For example, in Figure~\ref{fig:okvisu}(a), the rationale is the core encyclopedia knowledge that ``\texttt{the first locomotive was invented in 1804}''. Figure~\ref{fig:okvisu}(c) shows an example that the model provides the commonsense knowledge of ``\texttt{grocery store is a common place to get food}''.


\section{Experiments on VQAv2}
Despite the good performance on knowledge-based VQA, one limitation of our method is that the image is abstracted as text. Captions or tags only provide a partial description of the image, and might miss important visual details necessary for question answering, such as questions on detailed visual attribute prediction. In this section, we benchmark~\oursmodel~on the VQAv2 dataset~\cite{goyal2017making} that contains questions focusing on the detailed image contents.
\begin{table}[t]
\small
\centering
\vspace{-0.0in}
\begin{tabular}{ l l c || c }
    \hline
    Method & Image Repr. & Few-shot & Acc. \\
    \hline
    \small{Oscar~\cite{li2020oscar}} & Feature Emb. & \xmark & 73.8 \\
    \hline
    Frozen & Feature Emb. & \cmark & 38.2 \\
    \textbf{\texttt{PICa-Base}} & Caption & \cmark & 53.2 \\
    \textbf{\texttt{PICa-Base}} & Caption+Tags & \cmark & 54.3 \\
    \textbf{\texttt{PICa-Full}} & Caption & \cmark & 55.9 \\
    \textbf{\texttt{PICa-Full}} & Caption+Tags & \cmark & \textbf{56.1} \\
    \textbf{\texttt{PICa-Full}} & GT-Caption-5 & \cmark & \underline{59.7} \\
    \hline
\end{tabular}
\vspace{-2mm}
\caption{\small Results on the VQAv2 validation set. The upper part shows the supervised state of the art. The bottom part shows the few-shot accuracy. () indicates the oracle performance.}
\vspace{-2mm}
\label{table:vqav2}
\end{table}
\vspace{2mm}
\noindent\textbf{Dataset and setup.} 
The VQAv2 dataset~\cite{goyal2017making} annotates question-answer pairs based on the COCO image corpus~\cite{lin2014microsoft}. VQAv2 questions are designed to be highly relevant to the image content. It reports the human performance of  with questions only, and  with questions and captions, compared with  with both questions and images. We follow Frozen~\cite{tsimpoukelli2021multimodal}, and report the accuracy on the validation set. 
Instead of treating VQA as a classification task over a pre-selected answer vocabulary~\cite{goyal2017making,li2020oscar}, we predict the answer in an open-ended text generation manner.

\vspace{2mm}
\noindent\textbf{Results.}
Table~\ref{table:vqav2} summarizes our results on VQAv2. \textbf{\texttt{PICa-Full}} achieves an accuracy of , surpassing the previous few-shot accuracy of  by a significant margin~\cite{tsimpoukelli2021multimodal}. The proposed in-context example selection and multi-query ensemble methods also work well on the VQAv2 dataset (\cf, \textbf{\texttt{PICa-Base}}: , \textbf{\texttt{PICa-Full}}: ). Compared with the supervised performance of  by Oscar~\cite{li2020oscar}, the proposed method is still around  lower in accuracy with failure cases discussed in Figure~\ref{fig:vqavisu}. Nonetheless, the promising few-shot results show that the proposed approach is one strong baseline in approaching few-shot vision-language tasks.


\vspace{2mm}
\noindent\textbf{Limitations.}
Figures~\ref{fig:vqavisu}(a-d) and (e,f) show qualitative examples of the success and failure cases of \textbf{\texttt{PICa-Full}}, respectively. 
A subset of VQAv2 questions can be answered with commonsense knowledge, where~\oursmodel~generally performs well. For example, the implicit knowledge of ``\texttt{the sign above doorway can be the exit sign}'' in Figure~\ref{fig:vqavisu}(a) and ``\texttt{cow laying down because of being tired}'' in Figure~\ref{fig:vqavisu}(b). A large portion of VQAv2 questions is about detailed image contents, such as the object color in Figures~\ref{fig:vqavisu}(c-e). In the success cases,~\oursmodel~answers such questions with relevant textual descriptions if available, or by guessing via object properties. For example,  the description ``\texttt{a sliver refrigerator}'' in Figure~\ref{fig:vqavisu}(c) and the guess ``\texttt{bedroom walls are usually white}'' in Figure~\ref{fig:vqavisu}(d). However, by only looking at the incomplete textual description of the image,~\oursmodel~does fail on many questions. For example, it fails to predict the correct color in Figure~\ref{fig:vqavisu}(e) and the number of giraffes in Figure~\ref{fig:vqavisu}(f). We expect an end-to-end vision encoder tuning can help answer such questions better.