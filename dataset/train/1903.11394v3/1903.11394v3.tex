\def\year{2020}\relax
\documentclass[letterpaper]{article} \usepackage{aaai20}  \usepackage{times}  \usepackage{helvet} \usepackage{courier}  \usepackage[hyphens]{url}  \usepackage{graphicx} \urlstyle{rm} \def\UrlFont{\rm}  \usepackage{graphicx}  \frenchspacing  \setlength{\pdfpagewidth}{8.5in}  \setlength{\pdfpageheight}{11in}  


\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{pifont}\newcommand{\xmark}{\ding{55}}



\usepackage{colortbl}
\usepackage{booktabs}
\usepackage[table,x11names,dvipsnames,xcdraw]{xcolor}

\usepackage{adjustbox}
\newcommand{\widthscalefive}{0.10}
\newcommand{\widthscalesix}{0.15}


\newcommand{\vecpn}{{\mathbf{p}_n}}
\newcommand{\vecpb}{{\mathbf{p}_b}}
\newcommand{\vecpm}{{\mathbf{p}_m}}
\newcommand{\vecpnu}{{p_n^u}}
\newcommand{\vecpnv}{{p_n^v}}
\newcommand{\mloss}[1]{\mathcal{L}_{main_#1}}
\newcommand{\aloss}[1]{\mathcal{L}_{aux_#1}}









\setcounter{secnumdepth}{0} 

\setlength\titlebox{2.5in} \title{Region-Adaptive Dense Network for Efficient Motion Deblurring}


\author{Kuldeep Purohit \qquad A. N. Rajagopalan \\
 Indian Institute of Technology Madras, India 
\\
{\tt\small kuldeeppurohit3@gmail.com, raju@ee.iitm.ac.in}} 




\begin{document}

\maketitle

\begin{abstract}
In this paper, we address the problem of dynamic scene deblurring in the presence of motion blur. Restoration of images affected by severe blur necessitates a network design with a large receptive field, which existing networks attempt to achieve through simple increment in the number of generic convolution layers, kernel-size, or the scales at which the image is processed. However, these techniques ignore the non-uniform nature of blur, and they come at the expense of an increase in model size and inference time. We present a new architecture composed of region adaptive dense deformable modules that implicitly discover the spatially varying shifts responsible for non-uniform blur in the input image and learn to modulate the filters. This capability is complemented by a self-attentive module which captures non-local spatial relationships among the intermediate features and enhances the spatially varying processing capability. We incorporate these modules into a densely connected encoder-decoder design which utilizes pre-trained Densenet filters to further improve the performance. Our network facilitates interpretable modeling of the spatially-varying deblurring process while dispensing with multi-scale processing and large filters entirely. Extensive comparisons with prior art on benchmark dynamic scene deblurring datasets clearly demonstrate the superiority of the proposed networks via significant improvements in accuracy and speed, enabling almost real-time deblurring. 
\end{abstract}

\section{Introduction}

Though computational imaging has made tremendous progress over the years, handling motion blur in captured content remains a challenge. Motion blur is caused by motion of objects in the scene or the camera during sensor exposure. Apart from significantly degrading the visual quality, the distortions arising from blur lead to considerable performance drop for many vision tasks \cite{vasiljevic2016examining}. There exist a few commercially available cameras  which can capture frames at a high frame-rate and thus experience less blur but they suffer from noise at high resolution and are quite expensive. 

\begin{figure}[t]
\begin{center}
  \includegraphics[clip, trim=0cm 2.5cm 1cm 0cm, width=0.8\linewidth]{ParameterNo_singleimage-eps-converted-to.pdf}

\end{center}
  \caption{Our network outperforms all existing approaches on the dynamic scene deblurring benchmark in terms of accuracy as well as inference time.}
\label{fig:time}
\end{figure}

Motion deblurring is a challenging problem in computer vision due to its ill-posed nature. The past decade has witnessed significant advances in deblurring, wherein major efforts have gone into designing priors that are apt for recovering the underlying undistorted image and the camera trajectory \cite{vasu2017local,yan2017image}. An exhaustive survey of uniform blind deblurring algorithms can be found in \cite{lai2016comparative}. Few approaches~\cite{chakrabarti2016neural,schuler2016learning} have proposed hybrid algorithms where a Convolutional Neural Network (CNN) estimates the blur kernel, which is then used in an alternative optimization framework for recovering the latent image. 


However, these methods have been developed based on a rather strong constraint that the scene is planar and that the blur is governed by only \textit{camera} motion. This precludes commonly occurring blur in most practical settings. Real-world blur arises from various sources including moving objects, camera shake and depth variations, causing different pixels to acquire different motion trajectories. A class of algorithms involve segmentation methods to relax the static and fronto-parallel scene assumption by independently restoring different blurred regions in the scene~\cite{hyun2013dynamic}. However, these methods depend heavily on an accurate segmentation-map. Few methods \cite{sun2015learning,gong2017motion} circumvent the segmentation stage by training CNNs to estimate locally linear blur kernels and feeding them to a non-uniform deblurring algorithm based on patch-level prior. However, they are limited in their capability when it comes to general dynamic scenes.  


The afore-mentioned methods are not end-to-end systems and share a severe disadvantage of involving iterative, time-intensive, and cumbersome optimization schemes at the network output for getting the final deblurred result. Use of fully convolutional CNNs to directly estimate the latent sharp image was proposed in~\cite{nah2017deep} and adopted by recent works to further advance the state-of-the-art. They offer the advantage of enabling generalized dynamic scene deblurring at low latency, by circumventing the iterative optimization stage involving fitting of hand-designed motion models.  \cite{nah2017deep,tao2018scale,gao2019dynamic} proposed multi-scale residual networks to aggregate features in a coarse-to-fine manner, while showing benefits of selective parameters sharing and/or recurrent layers. Recently, \cite{zhang2019deep} proposed a multi-patch hierarchical network and stacked its copies along depth to achieve state-of-the-art performance.


However, there are two major limitations shared by prior deblurring works. Firstly, the filters of a generic CNN are spatially invariant (with spatially-uniform receptive field), which is a suboptimal modeling of the dynamic scene deblurring process and offers limited accuracy. Secondly, existing methods attempt to increase receptive field by increasing the model's computational footprint, making them unsuitable for real-time applications. As the only other work of this kind, \cite{zhang2018dynamic} recently proposed a design composed of multiple CNNs and Recurrent Neural Networks (RNN) to learn spatially varying weights for deblurring. However, their performance is inferior to the state-of-the-art~\cite{zhang2019deep} in several aspects. Reaching a trade-off among inference time, accuracy of restoration, and receptive field is a non-trivial task which we address in this paper (see Fig. \ref{fig:time}). We investigate position and motion-aware CNN architecture, which can efficiently handle multiple image regions experiencing motion with different magnitude and direction. 



Following recent developments, we adopt an end-to-end learning based approach to directly estimate the restored sharp image. For single image deblurring, we build a fully convolutional architecture equipped with filter-transformation and feature modulation capability suited for the task of motion deblurring. Our design leverages the fact that motion blur is essentially an aggregation of various spatially varying transformations of the image, and a network that implicitly adapts to the location and direction of such motion, is a better candidate for the restoration task. Its advantages over prior art are three-fold: 1. It is a dynamic and computationally efficient as it requires only a single forward pass through each layer, obviates the need for repeated processing of the image (at different scales/patch-levels). 2. Its components can be easily introduced into other architectures to improve their performance. 3. The transformations estimated by the network are dynamic and hence can be meaningfully interpreted for any test image. 


The efficacy of our architecture is demonstrated through comprehensive comparisons with the state-of-the-art on image deblurring benchmark. The major contributions of our work are:
\begin{itemize}
\item We propose an efficient motion deblurring architecture built using dense deformable modules that facilitate position-specific dynamic filter transformation. 
\item Our network benefits from estimating image dependent spatial attention-maps to process local features jointly with their globally distributed interdependencies.
\item We embed the above adaptive modules into an densely connected fully convolutional design which benefits from pre-trained filters of DenseNet. 
\item Extensive experiments are presented on dynamic scene deblurring benchmark to show state-of-the-art accuracy and near real-time deblurring achieved by our architecture and the interpretability of its dynamic modules. 
\end{itemize}

\section{Proposed Architecture}

An existing technique for accelerating various image processing operations is to down-sample the input image, execute the operation at low resolution, and up-sample the output \cite{chen2016bilateral}. However, this approach discounts the importance of resolution, rendering it unsuitable for image restoration tasks where high-frequency content of the image is of prime importance (deblurring, super-resolution).

Another efficient alternative is a CNN with a fixed but very large receptive field, e.g. Cascaded dilated network~\cite{chen2017fast}, which was proposed to accelerate various image-to-image tasks. However, simple dilated convolutions are not appropriate for restoration tasks (as shown in \cite{liu2018multi} for image super-resolution). After several layers of dilated filtering, the output only considers a fixed sparse sampling of input locations, resulting in significant loss of information. 

Till date, the driving force behind performance improvement in deblurring has been use of large number of layers, larger filters, and multi-scale processing which assist in increasing the ``static'' receptive field of a CNN. Not only do these techniques offer a suboptimal design, they are also difficult to scale since the effective receptive field of deep CNNs is much smaller than the theoretical one (investigated in \cite{luo2016understanding}).

We claim that a superior alternative to such image-agnostic models is a convolutional framework wherein the filters and the receptive field dynamically adapts to input image instances. Our experiments show that the latter approach is a considerably better choice due to its task-specific efficacy and utility for computationally limited environments, and it delivers consistent performance across diverse magnitudes of blur. Now, we explain the motivation for designing a deblurring network with asymmetric filters. Given a 2D image  and a blur kernel , the motion blur process can be formulated as:




 
where  is the blurred image,  represents pixel coordinates, and  is the size of the blur kernel. At any given location , the sharp intensity can be represented as  


\begin{small}
     
\end{small}

which is a 2D infinite impulse response (IIR) model, whose recursive expansion would eventually lead to an expression which contains values from only the blurred image and the kernel.

The dependence of  on a large number of locations in  shows that the deconvolution process requires infinite signal information. If we assume that the boundary of the image contains zeros, eq. \ref{iir} is equivalent to applying an inverse filter to . As visualized in \cite{zhang2018dynamic}, the non-zero region of such an inverse deblurring filter is typically much larger than the blur kernel. Thus, if we use a CNN to model the process, a large receptive field should be considered to cover all the pixel positions that are necessary for deblurring. Eq. \ref{iir} also shows that only a few coefficients (which are  for ) need to be estimated by the deblurring model, provided we can find an appropriate model with large enough receptive field. 


For this theoretical analysis, we will temporarily assume that the motion blur kernel  is linear (as assumed in prior deblurring works \cite{sun2015learning,gong2017motion}). Now, consider an  image  which is affected by motion blur in the horizontal direction (without loss of generality), implying  for  (non-zero values present only in the middle row of the kernel). For this case, eq. \ref{iir} translates to
 

We observe that for this case,  can be expressed as a function of only one row of pixels in the blurred image , which implies that for a horizontal blur kernel, the deblurring filter is also purely horizontal. We use this observation to state a hypothesis that holds for any motion blur kernel: ``Deblurring filters are directional/asymmetric in shape''. The reason behind this is the well known inherently directional nature of motion blur kernels. Such an operation can be efficiently learnt by a CNN with adaptive and asymmetric filters and this forms the basis for our work. Next, we describe our proposed network in detail.



\begin{figure}[]
\centering
\includegraphics[width=1.02\linewidth]{AAAI_DeblurringArchitecture.pdf}
\caption{Proposed deblurring network and its components.}
\label{fig:architecture}
\end{figure}

\subsection{Dense encoder decoder backbone}

Inspired by the success of fully convolutional networks that directly estimate the intensities of the deblurred image, we build an encoder-decoder architecture composed of densely connected modules. These modules immensely improve feature extraction capability by reusing features across multiple layers and their connections maximize information flow along the intermediate layers and result in better convergence. Hence, they are more efficient and learn more complex features than a network with residual connections (used extensively in recent deblurring methods).

A key component of our Region-Adaptive Dense Network is the encoder which progressively extracts a feature pyramid from the input image. The first convolution begins after a space-to-depth module that transforms the image pixels to channel-space using pixel-shuffling by a factor of 2. This allows subsequent computationally intensive operations to be performed at lower spatial resolution, hence reducing computational and memory footprint while increasing the receptive field. This layer is followed by three dense-blocks containing , , and  dense units, respectively, with growth rate (GR) set to  and each dense unit consisting of batch norm, ReLU,  conv (GR channels) followed by batch norm, ReLU,  conv (GR channels). The first two dense blocks are followed by  conv and pooling to down-sample the features-maps. Our design allows all these filters to be initialized using initial layers of pre-trained DenseNet-121 \cite{huang2017densely}, and our experiments demonstrate its advantage. 

The decoder is built using our dynamically adaptive components namely, Self-Attention (SA) module and Dense Deformable Module (DDM). SA module accepts the low-resolution output of the encoder and generates a non-locally enhanced feature-map. These features are sequentially processed by  DDMs and deconvolution layers to reach the output image resolution. Similar to U-net, intermediate features with in the decoder are concatenated with the corresponding-sized encoder features. Further, result of the final deconvolution layer is enhanced through multi-scale context aggregation through pooling and upsampling at 4 scales, before being fed to the final reconstruction layer. The output is the residual between the ground-truth sharp image and the input blurred image. Note that unlike several prior works, our network does not use large () filters. A schematic of the proposed architecture is shown in Fig. \ref{fig:architecture}. 



\begin{figure}[]
\centering
\includegraphics[width=\linewidth]{Deformable_Layer.pdf}
\caption{Schematic of our dense deformable module.}
\label{fig:deformableblock}
\end{figure}

\subsection{Dense deformable module (DDM)}
CNNs operate on fixed locations in a regular grid which limits their ability to model unknown geometric transformations. Spatial Transform Networks (STN) \cite{jaderberg2015spatial} introduced spatial transformation learning into CNNs, wherein an image-dependent global parametric transformation is estimated and applied on the feature map. However, such warping is computationally expensive and the transformation is considered to be global across the whole image, which is not the case for motion in dynamic and 3D scenes where different regions are affected by different magnitude and direction of motion. To introduce such motion-awareness in our network, we adopt deformable convolutions~\cite{dai2017deformable}, which enable local transformation learning in an efficient manner. Unlike regular convolutional layers, the deformable convolution also learns to estimate the shapes of convolution filters conditioned on an input feature map. While maintaining filter weights invariant to the input, a deformable convolution layer first learns a pixel-level offset map from the input, and applies it to the regular feature map for re-sampling.

Our DDM (shown in Fig. \ref{fig:deformableblock}) contains a densely connected set of deform units capable of learning positioning of filters on the feature sampling grid. Each deform unit in DDM contains a pair of layers: one to estimate the 2D filter offsets for each spatial location and another to apply the learnt filters on values sampled from these locations. The 2D offsets (shown as red-arrows) are encoded in the channel dimension of an estimated tensor (shown in green).

At each spatial coordinate , the deform unit estimates  offsets for  sampling locations. Our DDM is composed of  kernels, which correspond to  and the fixed offsets . If  denotes the learnt filter weight for the  location, the processed feature values  at coordinate  are then obtained from input feature  as:

where  are the dynamically estimated sampling offsets for all the channels in the input feature map, which determine the shifting of the  filter locations along horizontal and vertical axes. As a result, the regular convolution filter operates on an irregular grid of pixels. Since the offsets can be fractional, bilinear interpolation is used to sample from the input feature map. All the parts of our network are trainable end-to-end, since bilinear sampling and the grid generation of the warping module are both differentiable \cite{paszke2017automatic}. The 6 deformable layers in our DDM are followed by a  conv layer to reduce the number of channels. 


Although the focus of our work is an effective network design, our analysis also presents an effective way to boost any existing deblurring network's performance. Replacing ordinary convolution layers with deformable layers is much more efficient than going deeper or wider since the receptive field and the spatial sampling locations become dynamically adaptable according to the scale, shape, and location of blur.
 
 


\subsection{Self-attention module (SA)}

Recent deblurring works have emphasized the advantages of multi-scale/patch processing. It efficiently captures different scales of motion blur, and increases the receptive field of the network. Although it facilitates local growth in receptive field, it does not leverage the relationship between two distant locations in the scene. While this coarse-to-fine approach helps to handle different magnitudes of blur, it cannot leverage the relationship among blurred regions from a global perspective, which is also beneficial for the restoration task at hand. In this work, we employ a better strategy: attention based learnable non-local connections among features at different spatial locations.

\begin{figure}[]
\centering
\includegraphics[width=0.9\linewidth]{ICCV_SA.pdf}
\caption{A schematic of our self-attention module.}
\label{fig:attentionblock}
\end{figure}

Trainable attention over features for modeling long-range dependencies has shown its benefits in several tasks spanning across language~\cite{shen2018disan,vaswani2017attention} and vision~\cite{wang2018non}, but has not been explored for deblurring. Our work is inspired by the recent work of \cite{zhang2018self} that utilizes non-local attention to connect different scene regions and uses it to improve image generation quality. 

 Our SA module selectively aggregates the features at each position by a weighted sum of the features at all positions. This efficient design ensures that similar features are connected to each other regardless of their spatial distances, which helps in directly connecting regions with similar blur. It has two advantages: First, it overcomes the issue of limited receptive field, as any pixel has access to features at every other pixel in the image. Second, it implicitly acts as a gate for propagating only relevant information across the layers. These properties make it suitable for deblurring, since blur affecting various scene-edges is often correlated.

As illustrated in Fig. \ref{fig:attentionblock}, input feature-map  is transformed into two new feature maps  and   ().  Next, we reshape  and   to  (), perform matrix multiplication between their transposed versions, and pass the product through a softmax layer to calculate the attention map :

where  measures the  position's impact on  position. Note that similarity between feature representations of any two position contributes to greater correlation (higher attention) between them. 

Finally,  is processed to obtain  which is then reshaped to . A matrix multiplication between  and the transpose of  yields an enhanced feature-map residual, which is added to  to obtain the final output  as:



The resulting feature   at each position is a weighted sum of the features at all positions and original features. Therefore, it has global context and selectively aggregated contexts according to the spatial attention map, causing similar features to reinforce gains and irrelevant features to get subdued. We place the SA module at the beginning of our region adaptive decoder to minimize memory footprint.




\begin{figure*}[htb]
	\scriptsize
	\centering
\begin{tabular}{cccccccc}
\includegraphics[width=0.10\textwidth]{deblurring/000197_blur} &
		        \includegraphics[width=\widthscalefive \textwidth]{deblurring/000197_blur_patch} & 
				\includegraphics[width=\widthscalefive \textwidth]{deblurring/000197_dcp_patch} & \includegraphics[width=\widthscalefive \textwidth]{deblurring/000197_nah_patch} & \includegraphics[width=\widthscalefive \textwidth]{deblurring/000197_deblurgan_patch} & \includegraphics[width=\widthscalefive \textwidth]{deblurring/000197_srn_patch} &
				 \includegraphics[width=\widthscalefive \textwidth]{deblurring/000197_dmphn_patch} & \includegraphics[width=\widthscalefive \textwidth]{deblurring/000197_ours_patch}
				\\
				\includegraphics[width=0.10\textwidth]{deblurring/000227_blur}
				&
				\includegraphics[width=\widthscalefive \textwidth]{deblurring/000227_blur_patch} & \includegraphics[width=\widthscalefive \textwidth]{deblurring/000227_dcp_patch} & \includegraphics[width=\widthscalefive \textwidth]{deblurring/000227_nah_patch} & \includegraphics[width=\widthscalefive \textwidth]{deblurring/000227_deblurgan_patch} & \includegraphics[width=\widthscalefive \textwidth]{deblurring/000227_srn_patch} &
				\includegraphics[width=\widthscalefive \textwidth]{deblurring/000227_dmphn_patch} &				 \includegraphics[width=\widthscalefive \textwidth]{deblurring/000227_ours_patch}				\\
\\
				\includegraphics[width=0.10\textwidth]{deblurring/000006_blur}
				&
				\includegraphics[width=\widthscalefive \textwidth]{deblurring/000006_blur_patch} & \includegraphics[width=\widthscalefive \textwidth]{deblurring/000006_dcp_patch} & \includegraphics[width=\widthscalefive \textwidth]{deblurring/000006_nah_patch} & \includegraphics[width=\widthscalefive \textwidth]{deblurring/000006_deblurgan_patch} & \includegraphics[width=\widthscalefive \textwidth]{deblurring/000006_srn_patch} & 
				\includegraphics[width=\widthscalefive \textwidth]{deblurring/000006_dmphn_patch} & 				\includegraphics[width=\widthscalefive \textwidth]{deblurring/000006_ours_patch}				

				\\ 
				\includegraphics[width=0.10\textwidth]{deblurring/000041_blur}
				&
				\includegraphics[width=\widthscalefive \textwidth]{deblurring/000041_blur_patch} & \includegraphics[width=\widthscalefive \textwidth]{deblurring/000041_dcp_patch} & \includegraphics[width=\widthscalefive \textwidth]{deblurring/000041_nah_patch} & \includegraphics[width=\widthscalefive \textwidth]{deblurring/000041_deblurgan_patch} & \includegraphics[width=\widthscalefive \textwidth]{deblurring/000041_srn_patch} & 
				\includegraphics[width=\widthscalefive \textwidth]{deblurring/000041_dmphn_patch} & 				\includegraphics[width=\widthscalefive \textwidth]{deblurring/000041_ours_patch}				

				\\ 				
								(a) Blurred Image&

				(b) Blurred patch& (c) Whyte et al. & (d) MS-CNN & (e) DelurGAN& (f) SRN & (g) \tiny{Stack(4)-DMPHN} & (h) Ours\\
	\end{tabular}
\caption{Visual comparisons of deblurring results on images from the GoPro test set~\cite{nah2017deep}. Key blurred patches are shown in (b), while zoomed-in patches from the deblurred results are shown in (c)-(h).}\label{fig:dynamic}
\end{figure*} 




\section{Experimental Setup}
\label{sec:experiments}



Since the prime application of our work is efficient deblurring of general 3D and dynamic scenes, we perform training and evaluation of our network on the dynamic scene deblurring benchmark \cite{nah2017deep}, following recent learning-based works. This dataset is constructed using fps videos captured using GoPro camera and contains diverse 3D scenes captured in presence of significant object and camera motion. Following the same train-test split as in \cite{nah2017deep}, we use  pairs for training and  pairs for evaluation. Training is done for  iterations using Adam optimizer with learning rate  on patches of  and batch-size of . We conduct our experiments on a PC with Intel Xeon E5 CPU,  GB RAM and an NVIDIA Titan X GPU.


\section{Experimental Results}

In this section, we carry out quantitative and qualitative comparisons of our architectures with state-of-the-art end-to-end learning based methods for image deblurring, including MSCNN~\cite{nah2017deep}, DeblurGAN~\cite{kupyn2017deblurgan}, SVRNN~\cite{zhang2018dynamic}, SRN~\cite{tao2018scale}, PSS-SRN~\cite{gao2019dynamic}, and Stack(4)-DMPHN~\cite{zhang2019deep}. Due to the complexity of the blur present in general dynamic scenes, conventional blur model based approaches struggle to perform well. Nevertheless, we also compare with conventional non-uniform deblurring approaches of \cite{whyte2012non}, \cite{hyun2013dynamic} and MBMF~\cite{gong2017motion}. Public implementations with default parameters were used to obtain qualitative results on selected test images.


\noindent \textbf{Quantitative Evaluation:}
Quantitative comparisons using PSNR and SSIM scores obtained on the GoPro testing set ( images) are presented in Table \ref{table:PSNR}. Since traditional methods cannot model combined effects of general camera shake and object motion~\cite{whyte2012non} or forward motion and depth variations~\cite{hyun2013dynamic}, they fail to faithfully restore most of the images in the test-set. The below par performance of MBMF can be attributed to the fact that it uses synthetic and simplistic blur kernels to train a CNN and employs a traditional deconvolution method to estimate the sharp image, which severely limits its applicability to general dynamic scenes. End-to-end residual networks such MS-CNN, SRN and PSS-SRN use multi-scale strategy or alternative losses to improve large blur handling capability, but fail in challenging situations. The proposed RADN(final) significantly outperforms these works, including the spatially varying model SVRNN. Importantly, our method fares significantly better than the nearest competitor Stack(4)-DMPHN, in terms of inference-time ( faster, supporting real-time deblurring at 28fps), and accuracy (improvement of  dB), while requiring  less parameters. RADN(final) represents our results obtained using geometric self-ensemble~\cite{lim2017enhanced}, which offers significant boost in test performance without requiring further training or additional parameters.


\noindent \textbf{Qualitative Evaluation:}
Visual comparisons on different dynamic and 3D scenes are given in Fig.~\ref{fig:dynamic}. It shows that results of prior works suffer from incomplete deblurring or artifacts. In contrast, our network is able to restore scene details (text, edges etc.) more faithfully due to its effectiveness in handling large dynamic blur. An additional advantage over \cite{hyun2013dynamic,whyte2012non} is that our model waives-off the requirement of parameter tuning during test phase. Additional experiments and qualitative comparisons on other benchmarks are provided in the supplementary material. 



\begin{table}[t]
\makebox{\begin{tabular}{|l|c|c|c|c|}
\hline
Models & \small PSNR & \small SSIM & \small Runtime\\ \hline
(Hyun et al. 2013) & 23.8 & 0.8358 & {\small 2000000}\\
\cite{whyte2012non} & 24.6 & 0.8458 & {\small 700000}\\
MBMF & 24.64 & 0.8429  & {\small 120000} \\
MSCNN & 29.23 & 0.9162  & {\small 6000}\\
DeblurGAN & 28.70 & 0.8580 & {\small 1000}\\
SVRNN & 29.19 & 0.9306 & {\small 1400}\\
SRN & 30.10 & 0.9323  & {\small 1600}\\ 
PSS-SRN & 30.92 & 0.9421  & {\small 1600}\\ 
Stack(4)-DMPHN & 31.20 & 0.9451  & 700 \\ 
\hline
RADN1 & 30.77 & 0.9417 & 32\\
RADN2 & 31.17 & 0.9454 & 32\\
RADN3 & 31.42 & 0.9494 & 34\\
RADN4 & 31.58 & 0.9503 & 36\\
RADN(Final) & \textbf{31.76} & \textbf{0.9530} & \textbf{38}\\
RADN(Final) & \textbf{32.15} & \textbf{0.9560} & \textbf{-}\\
\hline
\end{tabular}}
\caption{\small Quantitative analysis of our proposed model on the GoPro dataset \cite{nah2017deep}, Runtime is in milliseconds.} 
\label{table:PSNR}
\vspace{-0.3cm}
\end{table}

 \begin{figure}[t]
 \begin{center}
 \begin{tabular}{cc}
\hspace{-0.5cm}
     \includegraphics[scale=0.24]{ting_aaai-eps-converted-to.pdf} & 
      \includegraphics[scale=0.24]{ting-eps-converted-to.pdf} \\
      (a)  & (b) \\
      
\end{tabular}
    \caption{Network analysis through comparison of training performance of various ablations of our models.}
         \label{fig:ablation}
         \end{center}
\vspace{-0.5cm}
\end{figure}




\begin{figure*}[htb]
\begin{center}
\begin{tabular}{cccccc}
  \includegraphics[width=0.175\linewidth]{deformable_visualizations/000107} &
  \includegraphics[width=0.175\linewidth]{deformable_visualizations/000203} &
  \includegraphics[width=0.175\linewidth]{deformable_visualizations/003023} &
  \includegraphics[width=0.175\linewidth]{deformable_visualizations/003082} &
  \includegraphics[width=0.175\linewidth]{deformable_visualizations/004002} \\
  \includegraphics[width=0.175\linewidth]{deformable_visualizations/out_000107}&
  \includegraphics[width=0.175\linewidth]{deformable_visualizations/out_000203} &
  \includegraphics[width=0.175\linewidth]{deformable_visualizations/out_003023} &
  \includegraphics[width=0.175\linewidth]{deformable_visualizations/out_003082} &
  \includegraphics[width=0.175\linewidth]{deformable_visualizations/out_004002} \\
    \includegraphics[width=0.175\linewidth]{attention_visualisaitons/107/ttt}&
  \includegraphics[width=0.175\linewidth]{attention_visualisaitons/203/ttt}&
  \includegraphics[width=0.175\linewidth]{attention_visualisaitons/3023/ttt}&
  \includegraphics[width=0.175\linewidth]{attention_visualisaitons/3082/ttt}&
  \includegraphics[width=0.175\linewidth]{attention_visualisaitons/4004/ttt}\\
  \end{tabular}   \\
\end{center}
\vspace{-1.5mm}
\caption{Visualizations of the spatial variations in the estimated filter offsets and attention-based feature modulations on blurred images from the GoPro test set. The second row shows the spatial distribution of the horizontal-offset values for the filter in a DDM of our network. Third row shows the corresponding enhanced feature-map residuals estimated by our SA module.}
\label{fig:visualization_deformable}
\end{figure*}


\begin{figure}[htb]
\begin{center}
\begin{tabular}{ccc}
\includegraphics[width=0.31\linewidth]{deformable_visualizations/000107} &
  \includegraphics[width=0.31\linewidth]{deformable_visualizations/000005} &
  \includegraphics[width=0.31\linewidth]{deformable_visualizations/000006} \\
    (a) 2.34 & (b) 3.53 & (c) 4.80 \\
\end{tabular}   \\
\end{center}
\vspace{-3mm}
  \caption{\textbf{Offset magnitudes}: Values mentioned under each image (from GoPro test set) depict the average magnitude of the offsets estimated by all the DDMs in our network.}  
\label{fig:deformable_magnitude}
\end{figure}

\section{Ablation studies}



\begin{table}[t]
\centering
\caption{Quantitative comparison of different ablations of our baseline residual deblurring network on GoPro testset.
\label{TableAblationSingle}} 
\resizebox{8.5cm}{!}{
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Conv layers  & 6 & 6 & 3 & 0 & 0\\
Deformable layers  & 0 & 0 & 3 & 6 & 6\\
SA module & \xmark & \checkmark & \xmark & \xmark & \checkmark \\
\hline 
PSNR (dB)&  29.8 & 30.64 & 30.69 & 31.05 & 31.13\\
Size (MB)&  10.6 & 10.7 & 10.9 & 11.2 & 11.2 \\
\hline
\end{tabular}
}
	\vspace{-1.0em}
\end{table}

\subsection{Ablations of our proposed network}

We analyze the effect of individual components of our network on its training and testing performance. The test scores of various ablations our network are reported in Table \ref{table:PSNR} and  a comparison of their training performance is shown in Fig. \ref{fig:ablation}(a). Decoder is a key component of our deblurring network and To evaluate its importance, we compare the proposed model RADN(Final) with  different versions of the decoder design while using the same dense encoder. In version RADN4, we replace the deform units within DDMs with ordinary convolutional layers, forcing the modules to apply rigid filters at all the spatial locations in the features. The drastic decrease in performance demonstrates the importance of spatially adaptive filter-offset learning capability. In version RADN3, we further remove the SA module from RADN4 and observe notable drop in performance. We chose to keep a single SA module in our network, since performance improvement beyond it was marginal and it serves as a good balance between restoration accuracy and processing-time. In RADN2, we replace DDMs with bottleneck blocks~\cite{huang2017densely} which are a simpler alternative to dense blocks. The significant fall in accuracy can be attributed to the efficacy of densely-connected layers and validates our design choice. Finally, to verify the effectiveness of pretraining, we train a baseline RADN1 which is identical to RADN2 except that its encoder layers are not pre-trained (using DenseNet). The PSNR and loss difference clearly shows that such pre-training leads to better initialization in parameter space and eventually, better convergence.


\subsection{Analysis on a baseline deblurring network}

Here, we evaluate the advantages of our region-adaptive modules by introducing them into a simpler residual encoder decoder baseline adopted from SRN (one of the competing methods). We differentiate this baseline's design from SRN in terms of compactness and computational footprint by employing only  filters for economy and removing the recurrent units. Next, we describe experiments with inclusion of our region-adaptive modules which significantly improve the representational capacity and performance of this network without sacrificing the computational efficiency (more details in supplementary document).

One of the key hyper-parameters in this analysis is the number of deformable units that replace normal convolutions (in the trunk of the network). To study its effectiveness, we designed and trained  versions of the network wherein the number of such replacements are ,  and , respectively. Fig. \ref{fig:ablation}(b) shows comparisons of the convergence plots of these models. It can be observed that the training performance as well as the quantitative results get better with increase in the number of deformable blocks, as it introduces additional filter adaptability into the network. Also note that the performance of the model which contains SA but no deformable layers, is expectedly lower than other models but better than the plain CNN. Finally, the best training performance is delivered by our final model which contains 6 deformable replacements and SAs, which shows that the advantages of the two modules are complementary and their union leads to a superior model. These improvements are also reflected in the quantitative values reported in Table \ref{TableAblationSingle}. 

The table also shows that our modules are lightweight since their inclusion has only a marginal effect on the model size. Although the proposed network is already quite efficient, replacing the standard convolutions in our network with grouped convolution and/or separable convolutions can lead to further improvements. 





\section{Network Visualization}
This section provides further insights into the effectiveness of our approach by visualizing the filter offsets and feature-attention maps dynamically estimated by our network. 

\noindent \textbf{Dense deformable module:} To establish our DDM's interpret-ability and their sensitivity to local motion in the scene, we investigate visible association between the estimated filter transformation-maps and the dominant motion blur in the input image. In first and second row of Fig. \ref{fig:visualization_deformable}, we show various test images and a representation of the corresponding offset-maps, respectively. The offset value at each spatial location is calculated using the difference between the horizontal (\mbox{x}) offsets of the  and  columns of the  filters from the  DDM of our network. It can be seen that the offset values are higher for the foreground regions and regions undergoing large motion. This is important for the deblurring process since the pixels corresponding to the foreground objects (e.g., people, vehicles) experience different motion compared to other parts of the scene, due to independent motion or depth differences. This demonstrates that the proposed network can distinguish between differently blurred regions and that the estimated offsets are correlated with the direction and magnitude of the motion blur.

Next, we measure the correlation between the amount of scene motion and the average magnitude of the filter offsets estimated by our network. In Fig. \ref{fig:deformable_magnitude}, we show various test images and the corresponding average value of offsets calculated by all the DDMs. It can be seen that the offset values are small for the first scene (Fig. \ref{fig:deformable_magnitude}(a)) where only few regions are blurred. As the blur increases and affects larger number of pixels in the scene (Fig. \ref{fig:deformable_magnitude}(b) and (c)), the average offset magnitude rises to accommodate the large blur. This shows that our network features a dynamic receptive field and adapts to scenes with varying degrees of blur without altering the filter weights, yielding efficiency to the process.



\noindent \textbf{Self-attention module:}
Here, we visualize the inputs and outputs of the SA module of our network for additional insights. In third row of Fig. \ref{fig:visualization_deformable}, we show the residuals estimated within our SA module ( in Eq.\ref{equsa}) for corresponding images in the first row. It can be observed that the SA module learns to amplify the correlation among feature-map magnitudes for all pixels undergoing similar (large) blur, and the spatial distributions agree with corresponding dynamic regions and depth variations. Such enhancement of the intermediate feature-maps is the key reason behind the observed improvement in training and test performance.






\section{Conclusions}
We proposed an efficient motion deblurring architecture composed of convolutional modules that enable spatially adaptive feature learning through filter transformations and feature attention, namely dense deformable module (DDM) and self-attention (SA) module. The DDMs implicitly address the shifts responsible for the local blur in the input image, while the SA module meaningfully connects non-locally distributed blurred regions. Introducing these modules awards higher capacity to any deblurring network without any notable increase in computational footprint. This effectiveness is shown by incorporating them into a new densely connected encoder decoder backbone, wherein we also show the benefits of pre-training of encoder filters. Compared against existing deep deblurring frameworks, our model achieves the state-of-the-art performance and is able to run at 28fps for 720p images. We believe our spatially-aware design can be utilized for other image processing and vision tasks as well, and we shall explore them in the future.

{\small
\bibliographystyle{aaai}
\bibliography{egbib}
}

\end{document}
