
\begin{figure*}
\centering
\extdata{fig/data/sample}
\begin{tabular}{cc}
{
\begin{tikzpicture}
    \tikzstyle{every node}=[font=\scriptsize]
\begin{axis}[width=0.45\textwidth,
  height=0.25\textwidth,
  xlabel={Memory sets},
xmode=log,
    grid=both,
  ylabel= {Accuracy (\%)},
  xminorticks=false,
xtick={1, 15, 200, 1000, 2500},
  xticklabels={ImageNet-LT, YFCC, LAION, Webli, All},  
legend cell align={left},
  legend pos=outer north east,
  legend style={at={(0.5,-0.3)},anchor=north, font =\scriptsize, fill opacity=0.8, row sep=-2.5pt, legend columns=3},
]

  \addplot[color=blue,     solid, mark=*,  mark size=1.5, line width=1.0] table[x=scale, y expr={\thisrow{b16}}] \singlelayermemvals; \leg{ViT-B16}
  \addplot[color=red,     solid, mark=*,  mark size=1.5, line width=1.0] table[x=scale, y expr={\thisrow{l16}}] \singlelayermemvals; \leg{ViT-L16}
  \addplot[color=gray,     solid, mark=*,  mark size=1.5, line width=1.0] table[x=scale, y expr={\thisrow{g14}}] \singlelayermemvals; \leg{ViT-g14}
  \addplot[color=orange,     solid, mark=*,  mark size=1.5, line width=1.0] table[x=scale, y expr={\thisrow{bigg14}}] \singlelayermemvals; \leg{ViT-G14}
  \addplot[color=olive,     solid, mark=*,  mark size=1.5, line width=1.0] table[x=scale, y expr={\thisrow{t5base}}] \singlelayermemvals; \leg{T5-Base}

\end{axis}
\end{tikzpicture}
}

&

{
\begin{tikzpicture}
    \tikzstyle{every node}=[font=\scriptsize]
\begin{axis}[width=0.45\textwidth,
  height=0.25\textwidth,
  xlabel={},
xmode=log,
    grid=both,
  xminorticks=false,
ytick={77, 78, 79, 80},
  xtick={1, 10, 50, 100},
  xticklabels={1, 10, 50, 100},  
  legend cell align={left},
  legend pos=outer north east,
  legend style={at={(0.5,-0.3)},anchor=north, font =\scriptsize, fill opacity=0.8, row sep=-2.5pt, legend columns=2},
]

  \addplot[color=blue,     solid, mark=*,  mark size=1.5, line width=1.0] table[x=k, y expr={\thisrow{webli}}] \k; \leg{Webli}
  \addplot[color=red,     solid, mark=*,  mark size=1.5, line width=1.0] table[x=k, y expr={\thisrow{laion}}] \k; \leg{LAION}
  \addplot[color=gray,     solid, mark=*,  mark size=1.5, line width=1.0] table[x=k, y expr={\thisrow{yfcc}}] \k; \leg{YFCC}
  \addplot[color=orange,     solid, mark=*,  mark size=1.5, line width=1.0] table[x=k, y expr={\thisrow{imnet}}] \k; \leg{ImageNet-LT}

\end{axis}
\end{tikzpicture}
}


\end{tabular}

 \caption{\textbf{Ablation study on ImageNet-LT.} \textbf{Left}: We show the impact of different memory sets and memory value encoders. We set  for this experiment. \textbf{Right}: We show the impact of  for different memory sets. We use T5-Base to represent memory values for this experiment.  
\label{fig:ablation} 
}
\end{figure*}


\section{Experiments}

In this section, we first describe the downstream datasets used in our experiments and detail our experimental setup.
We then conduct a rigorous study showing the impact of different memory datasets and memory value encoders.
Finally, we compare our results against the existing methods in the literature.

\subsection{Experimental setup}
\label{sec:setup}

We report experiments for three different image classification tasks: long-tailed recognition, learning with noisy labels, and fine-grained classification. 
We now describe the downstream datasets that we use for each of these tasks.

\head{Long-tailed Recognition.}
Long-tailed recognition assumes that there is a strong imbalance in terms of images per class in the training set.
The goal is to learn robust classifiers that can accurately classify every class, regardless of the number of times it appears in the training set.

We use two datasets for this task: ImageNet-LT~\cite{OLTR} and Places-LT~\cite{OLTR}.
ImageNet-LT has  classes and the number of training images per class varies from  to .
It is created by taking a subset of the original ImageNet dataset~\cite{ImageNet}, so that the number of images per class follows a long-tailed distribution.

Similarly, Places-LT is created by taking a subset of the original Places365 dataset~\cite{zhou2017places}, such that the number of training images per class follows a long-tailed distribution.
There are  classes in this dataset, and the number of images per class varies from  to .

The validation sets for both datasets are balanced.
We report the top-1 overall accuracy for both datasets. 
We also report the accuracy for \emph{many-shot} classes (more than 100 images per class), \emph{mid-shot} classes (between 20 and 100 images) and \emph{few-shot} classes (less than 20 images), separately, following the protocol in~\cite{OLTR}.


\head{Learning with noisy labels.}
The Webvision dataset~\cite{li2017webvision} contains M images and  classes.
The data is collected from the web, and the labels are assigned without human supervision.
Therefore, some of the labels are noisy.
Training set is imbalanced, meaning that there are different number of examples for each class.
We report the top-1 overall accuracy for the validation set.

\head{Fine-grained classification.}
We use the iNaturalist2021-Mini dataset~\cite{van2021benchmarking} for this task.
This dataset contains fine-grained images of species, \eg insects, plants, birds \etc 
There are  classes and  images per class, making it a total number of  training images.
The validation set contains  images.
We report the top-1 overall accuracy.

\head{Implementation details.}
We use a frozen ViT-B/16 as the visual encoder  to represent the query vectors from the downstream datasets and memory key embeddings.
Unless otherwise specified, training lasts  epochs, with a learning rate of  and batch size of .
The learning rate follows a warm-up schedule of  epoch, and then is reduced in each epoch using a cosine decay schedule~\cite{LH16}.
We use an Adam optimizer~\cite{kingma2017adam} with a weight decay of .
We also use label smoothing~\cite{szegedy2016rethinking} during training to prevent over-fitting.
For the Memory Attention Module (MAM)~\eqref{eq:attnModel}, we use  layers, \ie .
We retrieve  examples from the memory, unless otherwise specified.

\subsection{Impact of the memory}
\label{sec:expmemory}

We study the effect of different choices for the memory construction in this section.
Section~\ref{sec:memory} introduces various memory datasets and memory value encoders in detail. 
We now investigate how different memory datasets and value encoders behave in the ImageNet-LT downstream dataset.

Figure~\ref{fig:ablation} (left) shows the impact of different memory value encoders in different memory datasets. 
We set  for this experiment.
We see that the textual memory value encoder (T5-Base) obtains a better accuracy compared to visual memory value encoders.
Even much larger visual memory value encoders, such as ViT-G/14, does not outperform a smaller T5-Base model.
We believe that this behavior is due to the fact that T5-Base represents a different modality (text), which otherwise is not available to the input.
Textual memory values are thus complementary to the visual signals, which are extracted from the input query in any case.

Figure~\ref{fig:ablation} (left) also shows that the accuracy generally improves as the size of the memory dataset becomes larger. 
Larger relative improvements are especially observed for visual memory value encoders as the size of the memory dataset increases.
The accuracy for visual memory value encoders continues to increase even when all the memory datasets are combined (B examples). 

Figure~\ref{fig:ablation} (right) shows the impact of  for different memory sets.
This hyperparameter controls the number of keys and values retrieved from the memory dataset.
We use textual memory value embeddings (T5-Base) for this experiment.
We see that the large memory datasets, \eg YFCC, LAION, Webli, are less sensitive to the choice of , whereas the accuracy starts to decrease for a smaller memory dataset such as ImageNet-LT.
That is because ImageNet-LT only has a few positive examples for certain images.
As  becomes larger, the retrieved set of vectors mostly contain noise.
That is not the case for larger memory datasets, as they are likely to have many relevant examples for each image.
Thus, they are less sensitive to larger .

\subsection{Comparison to baselines}

\begin{table*}
\small
\setlength{\tabcolsep}{4pt}
\begin{center}
\begin{tabular}{lccccccccccc}
\toprule
					& 				&				& \multicolumn{4}{c}{ImageNet-LT} & \multicolumn{4}{c}{Places-LT} \\
\midrule 
Method 				&	Retrieval	& Backbone	& Many-shot 	& Mid-shot 		& Low-shot 	& All 	& Many-shot 	& Mid-shot 		& Low-shot 	& All				\\
\midrule
									 \multicolumn{11}{c}{\textsc{\textbf{Baselines}}}																																														\\
\midrule
Linear Classifier 	& 				& ViT-B16 \faLock		& 76.5 			& 72.6 					& 66.5 						& 73.5  				& 44.5							& 44.4 					& 44.0 						& 44.3  			\\
MLP Classifier 		&				& ViT-B16 \faLock		& 80.1 			& 74.1 					& 66.9 						& 75.2  				& 48.6							& 46.1 					& 41.3 						& 46.0  			\\
Mean -NN  		& \checkmark	& ViT-B16 \faLock		& 75.9 			& 75.8 					& \textbf{75.7} 			& 75.8  				& 44.3							& 45.2 					& 45.5 						& 44.9  			\\
\midrule
									 \multicolumn{11}{c}{\textsc{\textbf{Existing methods}}}																																							\\
\midrule
PaCo~\cite{cui2021parametric} 		  & 				& ResNext-101	& 68.2 			& 58.7 			& 41.0 		& 60.0  	& 36.1			& 47.9 			& 35.3 		& 41.2  			\\

VL-LTR~\cite{tian2022vl}		  &					& ViT-B16				& 84.5 			& 74.6 			& 59.3 		& 77.2  		& \textbf{54.2}		& 48.5 			& 42.0 		& 50.1  			\\
RAC~\cite{long2022retrieval}  		  & \checkmark	& ViT-B16				& - 			& - 			& - 		& -  			& 48.7				& 48.3 			& 41.8 		& 47.2  			\\
RAC~\cite{long2022retrieval} & \checkmark	& ViT-B16 \faLock		& 80.9 			& 76.0 			& 67.5 		& 76.7  		& 50.3				& 48.3 			& 42.5 		& 47.9  				\\
RAC~\cite{long2022retrieval} & \checkmark	& ViT-B16 				& \textbf{85.9} & 79.3 			& 69.3 		& 80.5  		& 51.9				& 49.8 			& 46.8 		& 50.0  				\\
\midrule
\textbf{Ours} 		  		& \checkmark		& ViT-B16 \faLock		& 80.6  		& 77.5 			& 74.5 			& 78.3 			& 50.9	& 49.9 				& 47.5				& 49.9  	\\
\textbf{Ours + FT} 		  	& \checkmark		& ViT-B16				& 85.4	& \textbf{81.5} & \textbf{76.4} & \textbf{82.3} & 52.4	& \textbf{52.0} 	& \textbf{48.5} 	& \textbf{51.4}  	\\
\bottomrule
\end{tabular}
\end{center}


 \caption{\textbf{Comprehensive evaluation on ImageNet-LT and Places-LT.} The accuracy for many-shot ( images), mid-shot (- images) and few-shot ( images) classes are reported separately. \textbf{Top:} We report the results for various baselines. \textbf{Bottom: } We compare our method against the existing methods in the literature. RAC denotes our re-implementation of RAC~\cite{long2022retrieval} in exactly the same setting as our method. \faLock ~means that the visual encoder is frozen during the downstream task training.
  \label{tab:baseline}}
\end{table*}

In this section, we show the benefit of our Memory Attention Module (MAM) by comparing it with different baselines. 
We report the accuracy for the following baselines.
\emph{Linear} classifier learns a fully connected layer on top of frozen downstream dataset embeddings.
\emph{MLP} classifier is a two-layer MLP with non-linearity in between the two layers.
Linear and MLP classifiers do not use an external memory dataset for retrieval. 
\emph{Mean -NN} computes the mean of the retrieved memory values; it do not learn the contribution of each retrieved memory value.
See Section~\ref{sec:method} for more details.
We use the WebLI memory dataset and T5-Base memory values for this experiment.

Table~\ref{tab:baseline} (Top) shows the accuracy for many-shot, mid-shot, and low-shot classes separately for all the baselines on ImageNet-LT and Places-LT datasets.
All the methods are trained with the LACE~\cite{menon2021longtail} loss, which has a balancing effect between the low-shot and many-shot classes.
Nevertheless, the low-shot accuracy suffers for the methods that do not use retrieval, \eg linear and MLP classifiers.

On the other hand, retrieval-based methods, \eg mean -NN and MAM (Ours), are less prone to over-fitting on many-shot classes.
However, mean -NN overcompensates for the low-shot classes by sacrificing the accuracy for the many-shot classes on ImageNet-LT.
This is not the case for our method, which achieves the highest overall accuracy by performing well across all three class categories.
Similar observations can be made in the Places-LT dataset. 
Our method achieves the highest accuracy on many-shot, mid-shot and low-shot classes, and the highest accuracy overall.

The experiments on Table~\ref{tab:baseline} (Top) demonstrate that the retrieval augmented classification alone does not always improve the classification accuracy.
This is evidenced by the performance of \emph{mean -NN}.
On the other hand, as we learn the contribution of the each retrieved example from the memory, we are able to filter out the noisy examples more accurately.
This results in the highest accuracy overall, while not sacrificing the accuracy for the many-shot, mid-shot and low-shot classes.

\begin{figure*}
\begin{center}
\includegraphics[width=0.8\textwidth]{fig/vis_examples.pdf}
\end{center}
\caption{
\textbf{Qualitative Examples.} 
We present the output of our method visually. 
We conduct this experiment by choosing the ImageNet-LT dataset as the query and memory dataset.
We display the query images from the test set on the left.
Their -NN from the training set are displayed on the right, and ordered from left to right.
We display the attention weight assigned to each -NN above the corresponding image.
\label{fig:qual}
\vspace{-0.2cm}
}
\end{figure*}


Table~\ref{tab:other} shows the comparison of our method against the baselines for fine-grained classification (iNaturalist2021-Mini) and learning with noisy labels (Webvision). 
We use WebLI as the memory dataset, and T5-Base as the memory value encoder for these experiments.
We observe that our method displays consistent improvement in both datasets.
Note that it overperforms the state-of-the-art in Webvision, without finetuning the visual encoder like the existing work.
This shows that our method is suitable for various classification tasks, and not only long-tailed recognition.

\begin{table}
\small{
    \scalebox{0.99}{  \centering
        \begin{tabular}{lcc}
        \toprule
                                                                & iNat2021-Mini    & WebVision     \\
        \midrule
        \multicolumn{3}{c}{\textsc{\textbf{Baselines}}}                                                    \\
        \midrule
        Linear Classifier                                       & 58.8              & 78.1                 \\ 
        MLP Classifier                                          & 59.6              & 81.0                  \\
        Mean -NN                                             & 58.9              & 78.2                  \\
        \midrule
        \multicolumn{3}{c}{\textsc{\textbf{Existing methods}}}                                              \\
        \midrule
        MILe~\cite{rajeswar2021multi}                           & --                & 75.2                  \\
        Heteroscedastic~\cite{collier2021correlated}            & --                & 76.6                  \\
        NCR~\cite{iscen2022learning}                            & --                & 76.8                  \\
        CurrNet~\cite{guo2018curriculumnet}                     & --                & 79.3                 \\
        \midrule
        \textbf{Ours}                                           & \textbf{66.2}     & \textbf{83.6}         \\
        \bottomrule
        \end{tabular}
    }
}
 \caption{\textbf{Evaluation on iNaturalist2021-Mini and Webvision.} We compare our method against the baselines and existing work in iNaturalist2021-Mini (fine-grained classification) and Webvision (learning with noisy labels) downstream datasets.
  \label{tab:other}}
\end{table}

\subsection{Comparison to existing methods}

We now compare our method against the state-of-the-art.
Table~\ref{tab:baseline} (Bottom) shows the accuracy of the prior art in ImageNet-LT and Places-LT datasets.
VL-LTR~\cite{tian2022vl} and RAC~\cite{long2022retrieval} use the same ViT-B/16 backbone as our method.
However the pre-training of the ViT-B/16 differs between different methods.
VL-LTR uses the ViT-B/16 pre-trained with CLIP~\cite{radford2021clip}, whereas RAC uses an ImageNet-21k pre-trained ViT-B/16 architecture~\cite{dosovitskiy2020image}.
Both methods finetune the visual encoder on the downstream dataset.
In this paper, we use a ViT-B/16 visual encoder pre-trained on the JFT-3B dataset~\cite{DBLP:conf/cvpr/Zhai0HB22}.
We also re-implement RAC with our visual and text encoder (T5-Base) for a better comparison in Table~\ref{tab:baseline}, and denote this variant as RAC.


Table~\ref{tab:baseline} shows that the VL-LTR achieves the highest many-shot accuracy on both datasets.
Nevertheless, this comes at the expense of a poor performance for low-shot classes.
RAC, an existing retrieval augmented classification method, shows a more balanced performance between many, mid and low-shot classes.
Our method achieves the highest accuracy on both datasets by obtaining high accuracy across different categories. 
For example, we do not achieve the highest many-shot nor low-shot accuracy on ImageNet-LT.
However, because we do not favor any category of classes above others, we have higher performance across different categories and achieve the highest overall accuracy.

\head{Fine-tuning the visual encoder.} 
In Table~\ref{tab:baseline}, we also include a variant of our method which fine-tunes the visual encoder  while learning the memory attention module.
We denote this variant by \emph{Ours + FT}.
The k-NN search is still done with a pre-trained, frozen vision encoder as in previous experiments.
We also include a variant of RAC which follows this setup in Table~\ref{tab:baseline}.
Our method achieves even further gains of accuracy when fine-tuning the vision encoder along with the memory attention module.


\subsection{Qualitative examples}

We present some of the qualitative examples in Figure~\ref{fig:qual}.
We use ImageNet-LT as both the downstream task and the memory dataset for this task.
We display the query images on the left, and the top- retrieved nearest neighbors on the right.
The retrieved images are ordered such that left-most image is the closest one.
Above each retrieved image, we display the learned attention value of our method. 

We see that our method assigns higher attention weights to the semantically correct images from the -NN list.
We observe this even if there is only one correct example in the k-NN list (\eg the \emph{beagle} query).
When there are multiple relevant images, all relevant examples get higher attention weights (\eg \emph{cassette} and \emph{shovel} queries).
The original rank position does not matter much for our method.
For example, one of the relevant retrieved images for the \emph{shovel} query has originally rank eight, but receives the second highest attention weight from our method.
Figure~\ref{fig:plqual} in Appendix shows the qualitative examples in Places-LT dataset.
