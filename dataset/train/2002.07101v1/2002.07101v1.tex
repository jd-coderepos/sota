\documentclass{article}
\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}
\usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{todonotes}
\usepackage{tikz}
\usepackage{times}



\usepackage{amsmath}


\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 



\let\ab\allowbreak



\newcommand{\diag}{\mathop{\mathrm{diag}}\nolimits}
\newcommand{\vvv}{\textnormal{vec}}
\newcommand{\vepsilon}{\bm{\epsilon}}
\newcommand{\gMN}{\mathcal{MN}}
\newcommand{\Tr}{\textnormal{Tr}}
\newcommand{\ddd}{\mathrm{d}}
\newcommand{\subN}{\textnormal{sub}\gN}


\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


\newcommand{\e}{\epsilon}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
 \usepackage{bm}
\usepackage[normalem]{ulem}
\usepackage{todonotes}
\usepackage{enumitem}
\usepackage{amsthm}
\newtheorem{example}{Example} 
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma} 
\newtheorem{proposition}{Proposition} 
\newtheorem{remark}{Remark}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{conjecture}{Conjecture}
\newtheorem{axiom}{Axiom}
\newtheorem{property}{Property}
\newtheorem{assumption}{Assumption}

\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{cancel}

\usepackage{siunitx}

\usepackage{thm-restate}
\declaretheorem[name=Theorem]{thm} \usepackage[accepted]{icml2020}
\icmltitlerunning{Augmented Normalizing Flows}


\begin{document}

\twocolumn[
\icmltitle{Augmented Normalizing Flows: \linebreak
Bridging the Gap Between Generative Flows and Latent Variable Models}

\begin{icmlauthorlist}
\icmlauthor{Chin-Wei Huang}{mil}
\icmlauthor{Laurent Dinh}{goo}
\icmlauthor{Aaron Courville}{mil,cif}
\end{icmlauthorlist}

\icmlaffiliation{mil}{Mila}
\icmlaffiliation{goo}{Google}
\icmlaffiliation{cif}{CIFAR fellow}

\icmlcorrespondingauthor{Chin-Wei Huang}{chin-wei.huang@umontreal.ca}



\icmlkeywords{Normalizing flows, generative flows, variational autoencoders, latent variable models, Hamiltonian ODE, universal approximation, invertible neural networks}

\vskip 0.3in
]

\printAffiliationsAndNotice{}  

\begin{abstract}
In this work, we propose a new family of generative flows
on an augmented data space,
with an aim to improve expressivity 
without drastically increasing the computational cost of sampling and evaluation of a lower bound on the likelihood.
Theoretically, we prove the proposed flow can approximate a Hamiltonian ODE as a universal transport map. 
Empirically, we demonstrate state-of-the-art performance on standard benchmarks of flow-based generative modeling. 
\end{abstract}


\section{Introduction}
\begin{figure*}
    \centering
    \includegraphics[width=1.0\textwidth]{figures/circle_all.png}
    \vspace{-0.7cm}
    \caption{\small 
    Transforming data  (\emph{left}) via augmented normalizing flows:
    Black dots and blue dots are marginal and joint data points, respectively.
    \emph{First step}: augment the data  with an independent noise .
    \emph{Second step}: transform the augmented data  conditioned on  into . 
    \emph{Third step}: transform the original data  conditioned on  into , resulting in a Gaussianized joint distribution of }
    \label{fig:2d_vis}
\end{figure*}
Deep invertible models have recently gained increasing interest among machine learning researchers as they constitute a powerful probabilistic toolkit. 
They allow for the tracking of  changes in probability density and have been widely applied in many tasks, including
\begin{enumerate}[label=(\roman*)]
    \item generative models~\citep{dinh2016density, kingma2018glow, chen2019residualflows},
    \item variational inference~\citep{rezende2015variational,kingma2016improved,berg2018sylvester},
    \item density estimation~\citep{papamakarios2017masked,huang2018neural},
    \item reinforcement learning~\citep{mazoure2019leveraging,ward2019improving}, etc. 
\end{enumerate} 

The main challenges in designing an invertible model for the above use cases are
to ensure (1) the mapping  is invertible, (2) the log-determinant of the Jacobian of  is cheap to compute, and (3)  is expressive. 
For use case (i), ideally we would also like to (4) invert  efficiently. 

In general, it is hard to design a family of functions that satisfy all of the above. 
Most work within this line of research is dedicated to improving the expressivity of the bijective mapping while maintaining the computational tractability of the log-determinant of Jacobian \citep{dinh2016density, kingma2016improved, huang2018neural, chen2019residualflows}. 

Aside from the unfortunate trade-off between the computational budget of inversion/Jacobian log-determinant and the expressivity of the invertible mapping, generative flows suffer from the limitation of local dependency. 
Unlike latent variable models such as Variational Autoencoders (VAEs; \citealt{kingma2013auto, rezende2014stochastic}) and Generative Adversarial Networks (GANs; \citealt{goodfellow2014generative}) which model the high dimensional data as coordinates in another space, most generative flows model the dependency among features only locally. 
Dependencies of features far away from each other can only be propagated through composition of mappings, which progressively enlarges the receptive field. 
Special design of parameterization like the attention mechanism can be made to address this issue \citep{ho2019flow++}.

In this paper, we propose to construct an invertible model on an augmented input space, which when combined with the block-wise coupling of \citet{dinh2016density} satisfies all criteria (1-4). 
The motivation is that to transform some distribution (such as the marginal distribution of  pictured in Figure~\ref{fig:2d_vis}) into another (e.g. standard normal) in the original input space,  needs to be capable of transporting the probability mass ``non-uniformly'' across its domain, 
whereas in an augmented input space it is possible to find a smoother transformation.
For instance, if we couple the data  with an independent noise , we can first transform  conditioned on  into , so that conditioned on different values of ,  can be more easily centered and Gaussianized. 
Our proposed method also generalizes multiple variants of VAEs and possesses the advantage of transforming the data in a more globally coherent manner via first embedding the data in the augmented state space.
Finally, operating on an augmented state space allows us to sidestep the topology preserving property of a diffeomorphism, which means input space can potentially be more freely deformed \citep{dupont2019augmented}. 

\paragraph{Our contributions:}
we introduce \emph{Augmented Normalizing Flows} (ANFs), an invertible generative model on the real-valued data  coupled with an independent noise . 
We propose a parameter estimation principle called \emph{Augmented Maximum Likelihood Estimation} (AMLE), which we show amounts to maximizing a lower bound on the marginal likelihood of the original data . 
Theoretically, we show that the family of ANFs with additive coupling can universally transform arbitrary data distribution into a standard Gaussian prior, augmented with a degenerate deterministic variable. 
To the best of our knowledge, this is the first attempt in understanding how expressivity can be improved via composing flow layers  rather than widening the flow \citep{huang2018neural}. 
Experimentally, we apply the proposed method to a suite of standard generative modelling tasks and demonstrate state-of-the-art performance in density estimation and image synthesis. 






\section{Background}
\label{sec:background}
Given a training set , where , and a family of density models , where  is a collection of sets of parameters that can sufficiently describe the density function, the \emph{Maximum Likelihood Principle} estimates the parameters by maximizing the chance of the data being generated by the assumed model:

where the latter expectation is over the empirical distribution  ( with uniformly distributed random index ). 
 is known as the maximum likelihood estimate (MLE) for the parameter . 
Below, we review two families of likelihood-based density models. 


\paragraph{1. Invertible Generative Models}
Assume .
Assume the data is generated via a bijective mapping . 
Then the probability density function of  evaluated at  can be written as

Equivalently, one can parameterize the inverse transformation  with invertible mapping , and define the generative transformation as .

Much of the design effort has been dedicated to ensuring (1) the invertibility of the transformation , and (2) efficiency in computing the log-determinant of the Jacobian in Equation~\ref{eq:cov}. 
For example, \citet{dinh2016density} propose the affine coupling:

where  and  are parameterized by neural networks and  and  are two partitioning of the data vector, 
and compose multiple layers of transformations intertwined with permutation of elements of . 


Invertible models allow for exact computation of the likelihood, and can be composed to increase modelling capacity. 
Nevertheless, the expressivity of the transformation is limited due to the need to satisfy invertibility and to reduce the cost of computing the Jacobian determinant. 
For a comprehensive review of this topic, see \citet{kobyzev2019normalizing} and \citet{papamakarios2019normalizing}.


\begin{figure*}
    \centering
    \resizebox{1.0\textwidth}{!}{


\tikzset{every picture/.style={line width=0.75pt}} 

\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]



\draw (70,0) node   {};


\draw (10,180) node   {};
\draw (10,100) node   {};
\draw (10,20) node   {};
\draw (130,180) node   {};
\draw (130,100) node   {};
\draw (130,20) node   {};


\draw (70,160) node   {};
\draw (70,80) node   {};
\draw    (54,148) -- (86,148) -- (86,172) -- (54,172) -- cycle  ;
\draw    (54,68) -- (86,68) -- (86,92) -- (54,92) -- cycle  ;



\draw    (10,168) -- (10,114) ;
\draw [shift={(10,112)}, rotate = 90] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
\draw    (130,168) -- (130,114) ;
\draw [shift={(130,112)}, rotate = 90] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
\draw    (10,88) -- (10,34) ;
\draw [shift={(10,32)}, rotate = 450] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
\draw    (130,88) -- (130,34) ;
\draw [shift={(130,32)}, rotate = 450] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;




\draw    (22,177) -- (52,167) ;
\draw    (88,155) -- (127,142) ;
\draw [shift={(128,141.67)}, rotate = 521.1600000000001] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;


\draw    (118,97) -- (88,87) ;
\draw    (52,75) -- (13,62) ;
\draw [shift={(12,61.67)}, rotate = 377.91999999999996] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;








\draw (250,0) node   {};


\draw (190,180) node   {};
\draw (190,100) node   {};
\draw (190,20) node   {};
\draw (310,180) node   {};
\draw (310,100) node  {};
\draw (310,20) node   {};

\draw (250,160) node   {};
\draw (250,80) node   {};

\draw    (234,147.5) -- (266,147.5) -- (266,172.5) -- (234,172.5) -- cycle  ;
\draw    (234,67.5) -- (266,67.5) -- (266,92.5) -- (234,92.5) -- cycle  ;


\draw    (190,114) -- (190,168) ;
\draw [shift={(190,168)}, rotate = 270] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
\draw    (310,114) -- (310,168) ;
\draw [shift={(310,168)}, rotate = 270] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
\draw    (310,88) -- (310,34) ;
\draw [shift={(310,88)}, rotate = 270] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
\draw    (190,88) -- (190,34) ;
\draw [shift={(190,88)}, rotate = 270] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;




\draw    (202,177) -- (232,167) ;
\draw    (268,155) -- (307,142) ;
\draw [shift={(308,141.67)}, rotate = 521.1600000000001] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;



\draw    (298,97) -- (268,87) ;
\draw    (232,75) -- (193,62) ;
\draw [shift={(192,61.67)}, rotate = 377.91999999999996] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;















\draw (490,0) node   {};

\draw (370,180) node   {};
\draw (370,100) node   {};
\draw (370,20) node   {};

\draw (490,180) node   {};
\draw (490,100) node  {};
\draw (490,20) node   {};

\draw (610,180) node   {};
\draw (610,100) node   {};
\draw (610,20) node   {};

\draw (430,120) node   {};
\draw (550,120) node   {};
\draw (430,80) node   {};
\draw (550,80) node   {};
\draw    (414,108) -- (446,108) -- (446,132) -- (414,132) -- cycle  ;
\draw    (534,108) -- (566,108) -- (566,132) -- (534,132) -- cycle  ;
\draw    (414,68) -- (446,68) -- (446,92) -- (414,92) -- cycle  ;
\draw    (534,68) -- (566,68) -- (566,92) -- (534,92) -- cycle  ;


\draw    (370,168) -- (370,114) ;
\draw [shift={(370,112)}, rotate = 90] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
\draw    (490,168) -- (490,114) ;
\draw [shift={(490,112)}, rotate = 89.75] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
\draw    (610,168) -- (610,114) ;
\draw [shift={(610,112)}, rotate = 89.24] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;

\draw    (370,88) -- (370,34) ;
\draw [shift={(370,32)}, rotate = 450.66] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
\draw    (490,88) -- (490,34) ;
\draw [shift={(490,32)}, rotate = 449.95] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;
\draw    (610,88) -- (610,34) ;
\draw [shift={(610,32)}, rotate = 450.49] [fill={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]  [draw opacity=0] (8.93,-4.29) -- (0,0) -- (8.93,4.29) -- cycle    ;






\draw    (382,103) -- (412,113) ;
\draw    (448,125) -- (487,138) ;
\draw [shift={(488,138.33)}, rotate = 200] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;

\draw    (502,103) -- (532,113) ;
\draw    (568,125) -- (607,138) ;
\draw [shift={(608,138.33)}, rotate = 200] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;

\draw    (598,97) -- (568,87) ;
\draw    (532,75) -- (493,62) ;
\draw [shift={(492,61.67)}, rotate = 377.91999999999996] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;

\draw    (478,97) -- (448,87) ;
\draw    (412,75) -- (373,62) ;
\draw [shift={(372,61.67)}, rotate = 377.91999999999996] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;



\draw    (448,125) -- (532,125) ;
\draw    (448,75) -- (532,75) ;




\end{tikzpicture}
     }
    \vspace{-0.5cm}
    \caption{\small 
    (\emph{a}) Augmented normalizing flow with block coupling and (\emph{b}) the reverse path for generation. 
    (\emph{c}) Hierarchical augmented normalizing flow. 
    The horizontal connections indicate deterministic features that will be concatenated with the stochastic features in the next transform block.}
    \label{fig:anfs}
\end{figure*}


\paragraph{2. Variational Autoencoders}
Assume the data follows the generating process:  where . 
For simplicity, we assume  is the standard Gaussian distribution and drop the dependency on  henceforward. 
Our goal is to find the MLE for , but the log marginal density  is generally not tractable since it involves integration. 
Instead, one can maximize a surrogate objective known as the \emph{evidence lower bound} (ELBO):

where  is an inference network that amortizes the cost of parameterizing the variational distribution per input instance  via conditioning. 
Learning and inference can be jointly achieved by drawing a stochastic estimate of the gradient of the ELBO via reparameterization (i.e. change of variable):

if  with  follows the same density as . Conventionally,  is a multivariate Gaussian distribution with diagonal covariance. 
We write it as . 
One choice of reparameterization is  with . 


VAEs allow one to embed the data in another space (usually of lower dimensionality), and to generate via an arbitrarily parameterized mapping. 
However, the log likelihood of the data is no longer tractable, so we can only maximize an approximate log likelihood.
The performance of the model highly depends on the choice of the encoding distribution and the decoding distribution, as they are closely related to the tightness of the lower bound \citep{cremer2018inference}.




\section{Augmented Maximum Likelihood}
\label{sec:amle}
For augmented maximum likelihood, we couple each data point with an independent random variable  drawn from  (in all our experiments we set ), and consider a family of joint density models . 
Instead of maximizing the marginal likelihood of 's, we maximize the joint likelihood:

where the expectation is over .
We refer to this extremum estimator as the \emph{Augmented Maximum Likelihood Estimator} (AMLE).
The benefit of maximizing the joint likelihood is that it allows us to make use of the augmented state space to induce structure on the marginal distribution of  in
the original input space.


\paragraph{Lower bounding the log marginal likelihood} 
Since the entropy of  is constant wrt the model parameter ,  is equal to the maximizer of  averaged over all 's.
For any , the quantity  can be written as the KL divergence:


Since KL is non-negative, maximizing the joint likelihood according to Equation~\ref{eq:amle} is equivalent to maximizing a lower bound on the log marginal likelihood of .
We refer to this as the \emph{Augmentation Gap}, as it reflects the incapability of the joint density to model the marginal of  independently of . 



\begin{figure*}[th]
    \centering
    \includegraphics[height=6.0cm]{figures/marginal_VAEx1_elu_lr001.png}
    \hspace{0.6cm}
    \includegraphics[height=6.0cm]{figures/VAEx1_elu_lr001_transform_all_labels.png}
    \vspace{-2mm}
    \caption{\small 
    Density modeling of 1D MoG with VAE (aka 1-step ANF). \emph{Left}: marginal distribution in the -space. \emph{Right}: joint distribution in the -space. 
    The first row is the inference path, where the joint data density  is mapped by an encoding transform (transforming  into  conditioned on ) followed by a decoding transform (transforming  into  conditioned on ). 
    The second row is the generation path, where the joint prior density  is transformed by the inverse decoding (transforming  into ) followed by the inverse encoding (transforming  into ).}
    \label{fig:anf_1d_1}
\end{figure*}


\paragraph{Estimating the log marginal likelihood}
The log marginal likelihood  of the data can be estimated in a way similar to \citet{burda2015importance}, by drawing  i.i.d. samples of  per  to estimate the following stochastic lower bound:

which can be shown to be a consistent estimator for  and is monotonically tighter in expectation as we increase . 


\section{Augmented Normalizing Flows (ANF)}
\label{sec:anf}

We now demonstrate how to leverage the augmented input space to model the complex marginal distribution of the data. 
We consider maximizing the joint likelihood of  coupled with a random noise .
Let  be drawn from some simple distribution, such as independent Gaussian. 
Assume the data  is deterministically generated via an invertible mapping , with inverse . 
Then analogous to Equation~\ref{eq:cov},  has a joint density 



For simplicity, we can choose  to be the standard normal distribution. 
What we are left with is the choice of an invertible  that can harness the augmented state space  to induce a complex marginal on . 
Inspired by the affine coupling proposed by \citet{dinh2016density}, we conditionally transform  and , hoping the structure in the marginal of  can ``leak'' into  and make the joint more easily Gaussianized. 
Concretely, we define two types of affine coupling

We refer to the pair of encoding transform and decoding transform as the \emph{autoencoding} transform. 
We stack them up in alternating order, i.e. 
 for  steps, where  is the set of all parameters.
See Figure \ref{fig:anfs}-(a,b) for an illustration.

\begin{figure*}
    \centering
    \includegraphics[width=1.00\textwidth,clip=true, trim=0 10px 0 5px]{figures/samples/VAEx5_relu_lr001_transform_all_labels.png}
    \caption{\small 5-step ANF on 1D MoG. In the inference path (top row), we start with an encoding transform that maps  to  conditioned on , followed by a decoding transform that maps  into  conditioned on . 
    We reuse the same encoder and decoder to refine the joint variable repeatedly to obtain  and . 
    In the generative path (bottom row), we reverse the process, starting with the inverse transform of the decoding, followed by the inverse transform of the encoding, etc. 
    }
    \label{fig:anf_1d_5}
\end{figure*}

\subsection{VAE as ANF}
Variational Autoencoders are a special case of augmented normalizing flows with only ``one step'' of encoding and decoding transform \citep{dinh2014nice}. 
To see this, assume the decoding distribution  is a factorized Gaussian with mean  and standard deviation .
By letting  and  and applying the change of variable formula to both  and , we get from Equation (\ref{eq:elbo_rep}) 
\begingroup\makeatletter\def\f@size{8.501}\check@mathfonts
\def\maketag@@@#1{\hbox{\m@th\large\normalfont#1}}

\endgroup

Averaging over the data distribution , we obtain the expected joint likelihood (up to the constant )


The variational gap between the log marginal likelihood and the evidence lower bound is equal to the augmentation gap since the KL divergence is invariant under the transformation between :



\begin{figure}
    \centering
    \includegraphics[width=0.48\textwidth,clip=true, trim=100px 25px 0px 5px]{figures/ablation_LS_grid.png}
    \vspace{-7mm}
    \caption{\small Comparing increasing number of layers of stochastic units () versus increasing number of layers of autoencoding transforms (). (\emph{x-axis}): number of updates. (\emph{y-axis}): upper bound of bits per dim (BPD) on CIFAR 10 test data.
    }
    \label{fig:layers}
\end{figure}


This gives us an alternative interpretation of \emph{inference suboptimality}~\citep{cremer2018inference}: the inaccuracy of inferring the true posterior  can be attributed to the incapability of the joint density to model the augmented data . 

To illustrate this phenomenon, we model the density of a one dimensional mixture of Gaussian (1D MoG).
In Figure \ref{fig:anf_1d_1} (left), we plotted the density histograms of the MoG distribution (blue) and a one-step ANF, i.e. VAE with Gaussian encoder and decoder (orange), trained on the MoG samples. 
Not surprisingly, the latter fails to represent two well separated modes of probability mass. 
In Figure \ref{fig:anf_1d_1} (right), we visualize the joint density of the augmented data  throughout the transformation. 
We see that the transformed data  is not perfectly Gaussianized.
In fact, if we project it horizontally we can see that the ``aggregated posterior'' (marginal of ) does not match the prior distribution . 
As a result, the pushforward  of  does not follow the augmented data distribution  well. 
When we fix different values of , we have different slices of density functions for , indicating that  and  are dependent and that  deviates from . 

We carry out the same experiment on 1D MoG with multiple flow layers, which generalizes a VAE with Gaussian encoder and decoder. 
We set the number of flow layers (i.e. \emph{steps}) to be .
To furthermore demonstrate the benefit of transformation composition, we also tie the parameters of each encoder and decoder step, separately.  
That is, the same set of parameters are used at different steps of encoding and decoding to make sure capacity stays constant. 
Since the conditional independence assumption in VAE is relaxed,
the augmented data is more successfully Gaussianized, as can be seen in Figure~\ref{fig:anf_1d_5}. The generated samples also follow the target joint density more closely. 



\begin{table*}[ht]
\centering
\small
\begin{tabular}{lccccccccc} 
\toprule
{\bf Model} & {\bf MNIST} & {\bf CIFAR 10} & {\bf ImageNet 32} & {\bf ImageNet 64} & {\bf CelebA-HQ} \\ 
\midrule 
\textbf{\small Models with autoregressive components} \\
{VAE + IAF {\small \citep{kingma2016improved}}} & {--} & 3.11 & {--} & {--} & {--}\\
{PixelCNN {\small \citep{oord2016pixel}}} & {--} & 3.14 & {--} & {--} & {--}\\
{PixelCNN (multiscale) {\small \citep{reed2017parallel}}} & {--} & {--} & 3.95 & 3.70 & {--}\\
{PixelSNAIL {\small \citep{chen2017pixelsnail}}} & {--} & \textbf{2.85} & \textbf{3.80} & {--} & {--} \\
{SPN {\small \citep{menick2018generating}}} & {--} & {--} & \textbf{3.79} & \textbf{3.52} & \textbf{0.61}\\
\midrule
\textbf{\small Flow-based models} \\
{Real NVP {\small \citep{dinh2016density}}}  & 1.06 & 3.49 & 4.28 & 3.98 & {--} \\
{Glow {\small \citep{kingma2018glow}}}  &  1.05 & 3.35 & 4.09 & 3.81 & 1.03 \\
{FFJORD {\small \citep{grathwohl2018ffjord}}}  & 0.99 & 3.40 & {--} & {--} & {--} \\
{Residual {\small \citep{chen2019residualflows}}}  & 0.97 & 3.28 & 4.01 & 3.76 & 0.99 \\
{Flow++ {\small \citep{ho2019flow++}}}  & {--} & 3.09 & \textbf{3.86} & 3.69 & {--} \\
{MaCow {\small \citep{ma2019macow}}} & {--} & 3.16 & {--} & 3.69 & \textbf{0.67} \\
\midrule
{ANF (ours)} & \textbf{0.93} & \textbf{3.05} & 3.92 & \textbf{3.66} & 0.72\\
\bottomrule
\end{tabular}
\caption{\small Bits-per-dim estimates of standard benchmarks (the lower the better).
Results of Flow++, MaCow, and ANF are models that employ variational dequantization instead of uniform noise injection. Details can be found in the appendix. 
}
\label{tab:density}
\end{table*}


\subsection{Hierarchical Augmented Normalizing Flows}
The information flow of the encoding-decoding transform just described is limited to the size of the random vector , which makes it hard to optimize for more realistic settings such as natural images. 
We thus propose a second architecture by following the hierarchical variational autoencoder, which is defined by two pairs of joint distributions~\footnote{This particular factorization of the variational distribution is known as the bottom-up inference. We leave the top-down inference~\citep{kingma2016improved} and the bidirectional inference~\citep{maaloe2019biva}, which benefit more from parameter sharing, for future work.} 

When all the conditionals are Gaussian distributions, the corresponding ELBO can be similarly rearranged to be the loss function of an ANF (see Figure~\ref{fig:anfs}-(c)). 
The encoding transform for each  is conditioned on the ``transformed'' preceding variables   due to the conditioning in .
The decoding transform on the other hand is conditioned on the ``original'' preceding variables , which is block-wise inverse autoregressive~\citep{kingma2016improved}. 
When the conditioning mappings are convolutional, the lower level transformation preserves information of the input locally, which is then combined with the deterministic path of the decoding that ``sees'' more of the input. More details on the architecture are described in Appendix \ref{app:raeb}.
 



\section{ANFs as Approximate Hamiltonian ODE and Universality}
The affine-coupling autoencoding transform with augmented variable is reminiscent of the leap-frog integration of the Hamiltonian system \citep{neal2011mcmc}. 
More recently, it has been shown by \citet{taghvaei19a} that solving a family of Hamiltonian ordinary differential equations (ODE) with an infinite time horizon gives us a transport map from the initial (data) distribution to an arbitrary target distribution with a log-concave density function. 
This suggests we can develop an approximation theorem by using ANFs to approximately, numerically solve the ODE.

Formally, we define scaling coefficients  and .
Let  be the standard normal density, and  be the data distribution.
Let  and  be some convex function. 
Define the Hamiltonian ODE:

where  and  are the time derivatives of  and  at time , and  is the marginal density of . 

Second, we construct a sequence of encoding and decoding functions  and  parameterized by neural networks, and define the following (additive) invertible mappings

with  and . 
The step size parameter  will be chosen to depend on the depth coefficient , i.e. the number of steps of the joint transformation. 

Assume our target distribution lies within a family of distributions  satisfying Assumption \ref{assumption} in the Appendix \ref{app:proofs} (some smoothness condition on the time derivatives and ). 
We can then set the encoding and decoding functions to be arbitrarily close to the time derivatives by the universal approximation of neural networks~\citep{cybenko1989approximation}, and by taking the depth  to be arbitrarily large, we can approximate the transport map induced by the Hamiltonian ODE arbitrarily well, which gives rise to the following universal approximation theorem (the proof is relegated to the Appendix \ref{app:proofs}):
\begin{restatable}{thm}{anfdist}
\label{thm:anf_dist}
For any , we can find a sequence  of ANFs of the additive form (\ref{eq:x_nn_main},\ref{eq:e_nn_main}), such that if  and , then  in distribution. 
\end{restatable}




\begin{table*}[ht]
\centering
\resizebox{0.98\textwidth}{!}{\begin{tabular}{cc|cc|cccc|c|cc}
\toprule 
& & {\small PixelCNN} & {\small PixelIQN} & {\small i-ResNet} & {\small Glow} & {\small Residual Flow} & {\small VAE+Glow} & {\small ANF} & {\small DCGAN} & {\small WGAN-GP (TTUR)}
\\ 
\midrule
IS & () & 4.60 & 5.29 & {--} & {--} & {--} & {--} & \textbf{6.49} & 6.16 & 7.86 \\
FID & () & 65.93 & 49.46 & 65.01 & 46.90 & 46.37 & 42.14 & \textbf{30.60} & 37.7 & 29.3 (24.8) 
\\
\bottomrule
\end{tabular}
}
\caption{\small Evaluation on Inception Score (IS, the higher the better) and Fréchet Inception Distance (FID, the lower the better) of models trained on CIFAR 10. Results taken from \citet{ostrovski2018autoregressive}, \citet{chen2019residualflows}, \citet{vaeflow}, and \citet{gulrajani2017improved,heusel2017gans}. 
Parenthesis indicates two time-scale update rule for WGAN-GP.
}
\label{tab:scores}
\end{table*}
\begin{figure}[t!]
    \centering
\includegraphics[width=0.48\textwidth]{figures/samples/mnist_gen_3_8.png}
    
    \vspace{1mm}
    
\includegraphics[width=0.48\textwidth]{figures/samples/cifar_gen_3_8.png}
    
    \vspace{1mm}
    
\includegraphics[width=0.48\textwidth]{figures/samples/celeba_gen_3_6.png}
    \vspace{-3mm}
    \caption{\small Unconditionally generated samples from models trained on MNIST (\emph{top row}), CIFAR 10 (\emph{middle row}), and 5-bit CelebA (\emph{bottom row}). 
}
    \label{fig:samples}
\end{figure}


\section{Related Work}
In the literature of normalizing flows, much work has been done to improve expressivity while maintaining computational tractability. 
For example, \citet{dinh2014nice,dinh2016density} introduce an affine coupling that partitions the features into two groups so that the Jacobian is a block-triangular matrix. 
The resulting mapping is relatively restricted since it only models partial dependency. 
\citet{kingma2016improved} further exploits the ordered dependency by constructing an inverse autoregressive mapping but its inversion requires a computation time linear in dimensionality~\citep{papamakarios2017masked}, 
and does not even have a closed-form formula in the more general non-affine setting \citep{huang2018neural}.
\citet{behrmann2018invertible} propose a residual form of  whose Jacobian log-determinant can be stochastically estimated~\citep{chen2019residualflows} but inversion is achieved iteratively, not in one pass. 

Normalizing flows have also been used as (1) an inference machine in the context of variational inference for continuous latent variable models~\citep{kingma2016improved,tomczak2016improving,berg2018sylvester}, and (2) a trainable component of the latent variable model \citep{chen2016variational,agrawal2016deep,huang2017learnable}.
ANFs lie at the intersection of normalizing flows and latent variable models when a specific type of block-conditioning transformation is applied, and allow us to 
unifyingly view
flow-based priors as marginal transformation in the space of , and amortized flows for improving posterior inference as different variants of the encoding transform. 
Another way of improving the inference machine's expressivity is to consider a hierarchical model; in fact, ANFs can be viewed as a generalization of the auxiliary variable method for hierarchical variational inference \citep{agakov2004auxiliary,ranganath2016hierarchical}; see Appendix \ref{app:anf_vi} for the connection and \ref{app:related} for more discussion on future direction. 

Finally, \citet{dupont2019augmented} also employs augmentation to improve the expressivity and stability of a neural ODE \citep{chen2018neural}, and they believe such a method can be used to reduce the cost of training a continuous normalizing flow \citep{grathwohl2018ffjord}.

\begin{figure}
    \centering
    \includegraphics[width=0.44\textwidth]{figures/samples/recons.png}
    \caption{Lossy reconstruction. \emph{Left}: original data. \emph{Right}: reconstruction from the topmost representation.}
    \label{fig:recon}
\end{figure}



\begin{figure*}
    \centering
    \includegraphics[width=0.24\textwidth,clip=true, trim=0 4px 0 0px]{figures/rescale_vis.png}
    \hfill
    \includegraphics[width=0.725\textwidth,clip=true, trim=0 64px 0 0px]{figures/samples/interpolates_r_celeba.png}
    \caption{\emph{Left}: comparison of linear and rescaled interpolations. \emph{Right}: rescaled interpolation of input data (first and last columns).}
    \label{fig:interpolate}
\end{figure*}



\section{Large-Scale Experiments}

\subsection{Quantitative results}
In the more realistic settings, we augment the data with a hierarchy of noise, as described in the last part of Section \ref{sec:anf}. 
See Appendix \ref{app:exp} for more experimental details. 

\paragraph{Stochastic vs. deterministic features} We conduct an ablation study on the effect of composing multiple encoding-decoding transformations ( steps) versus increasing the number of stochastic layers ( layers). 
We monitor the bits per dim (BPD) of the test set of CIFAR 10~\citep{krizhevsky2009learning} throughout training.
Figure \ref{fig:layers} shows that increasing the number of flow layers can more effectively improve the likelihood of the model than increasing the number of stochastic layers.

\paragraph{Density estimation}
We perform density modelling on the MNIST handwritten digit dataset~\citep{lecun1998gradient}, CIFAR 10~\citep{krizhevsky2009learning}, downscaled versions of ImageNet ( and )~\citep{oord2016pixel} and the celebrity face dataset CelebA~\citep{liu2015faceattributes}, and compare with other state-of-the-art density models.  
In Table \ref{tab:density}, we see that ANFs set a few new records in terms of BPD on the standard benchmarks in the non-autoregressive category.
We use the importance sampling described in Section \ref{sec:amle} to estimate the log likelihood. 
The augmentation gap is around 0.01 BPD for all benchmarks, indicating the augmented flow is capable of achieving good likelihood estimate and high inference precision at the same time. 

\subsection{Qualitative results}

\paragraph{Sample quality} For quantitative evaluation of sample quality, we report the Inception Score (IS) \citep{salimans2016improved} and the Fréchet Inception Distance (FID) \citep{heusel2017gans}, expanding the table of \citet{ostrovski2018autoregressive}. 
We found the FID score of WGAN-GP~\citep{gulrajani2017improved} reported in \citet{ostrovski2018autoregressive} is worse than the one reported in the literature, so we include the original values of IS and FID of GANs from the original works of \citet{gulrajani2017improved} and \citet{heusel2017gans} for more realistic comparison.
In Table \ref{tab:scores}, we see that ANF obtains better scores than all the other explicit density models, and is close to matching the FID of the orignal WGAN-GP by~\citet{gulrajani2017improved}. 
The generated samples are presented in Figure~\ref{fig:samples} and Appendix \ref{app:samples}.
Since the encoding-decoding transformation has a receptive field that is wide enough to cover the entire raw data, the generated samples also look more globally coherent. 

\paragraph{Lossy reconstruction}
As a hierarchical model, ANF can be used to perform inference for the higher level representation, and sample the lower level details for reconstruction. 
We do this by sampling , obtaining the corresponding , randomizing all but the last representations , and reconstructing from the new joint representation . 
Similar to other hierarchical models~\citep{gulrajani2017pixelvae, belghazi2018hierarchical}, 
ANF is also capable of retaining global, semantic information of the raw data stored in its higher level code; this is shown in Figure~\ref{fig:recon}.

\paragraph{Interpolation}
We also perform interpolation in the latent space between real images. 
Previous works such as \citet{kingma2018glow} perform linear interpolation of the form  for , which we observe has a non-smooth transition (e.g. sudden color change). 
We hypothesize this is due to the fact that convex combination of two vectors would result in an increase and then a decrease in the density of the standard Gaussian prior. 
This is undesirable since the interpolated points are atypical because Gaussian samples are known to concentrate around the shell (of radius proportional to square root dimensionality). 
Hence, we propose the \emph{rescaled interpolation}

where  denotes the L2 norm, to make sure the scale of the resulting point is a linear interpolation of the scales of the two input vectors.
The result in Figure~\ref{fig:interpolate} shows that the transition is extremely smooth (see Appendix \ref{app:inter} for a side-by-side comparison with linear interpolation) and the intermediate images are realistic looking. 




\section{Conclusion}
In this work, we propose the \emph{Augmented Normalizing Flows} and a corresponding variational lower bound on the marginal likelihood.
We show that the proposed method can be used to approximate a Hamiltonian dynamical system as a universal transport map and achieves competitive or better results than state-of-the-art flow-based methods. 


\iftrue
\section*{Acknowledgements}{
CW would like to thank 
Matt Hoffman for a discussion on deterministic Langevin transitions and Amirhossein Taghvaei for referencing the work of Wang \& Li. 
Special thanks to people who have provided their feedback and advice during discussion or while reviewing the manuscript, including Valentin Thomas, Joey Bose, and Eeshan Dhekane; to
Taoli Cheng and Bogdan Mazoure for volunteering for the internal review at Mila; and to Ahmed Touati, Christos Tsirigotis and Jose Gallego for proofreading parts of the proof. 
}
\fi

\bibliography{reference}
\bibliographystyle{icml2020}



\clearpage
\appendix
\onecolumn

\section{Interpolation}
\label{app:inter}
We compare linear interpolation with rescaled interpolation (rescaling is done separately for each stochastic layer). 
We see that the middle points of linear interpolation tend to be more yellowish, and rescaled interpolation results in a smoother and direct transition between two input vectors.
\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/samples/interpolates_l_celeba_25.png}
    \hfill
    \includegraphics[width=0.49\textwidth]{figures/samples/interpolates_r_celeba_25.png}
    \caption{(\emph{left}) linear interpolation, (\emph{right}) rescaled interpolation}
    \label{fig:interpolate_compare}
\end{figure*}





\clearpage

\section{Experiment Details}
\label{app:exp}
To model natural images, we employ a more intricate architecture with a higher modeling capacity described in \ref{app:raeb}. 
Section \ref{app:init} describes the parameter initialization scheme and parameterization constraints that are imposed to stablize training. 
In \ref{app:warm}, we propose an objective-annealing technique that biases the autoencoding transform to 
focus on Gaussianizing the raw data more at the early stage of training.
We found this to be helpful for optimization. 
All the hyperparameters used in the experiments are summarized in \ref{app:hparam}. 


\subsection{Residual autoencoding blocks}
\label{app:raeb}
\begin{figure*}[h!]
	\centering
	\includegraphics[width=0.95\textwidth]{figures/autoencode_transform_noborder}
	\caption{Architecture of a single encode transform block and a single decode transform block.}
	\label{fig:autoencoding_block}
\end{figure*}

We parameterize all of the feedforward layers with weight normalization \citep{salimans2016weight}. 
For the encoding transforms, we first map  using a convolutional layer composed with an activation function, followed by a pooling layer to obtain the first set of conditional features . 
We now use these features to conditionally transform each of the stochastic units in the order , using an \emph{encoding block}.
The encoding block outputs a set of modified conditional features, which is then used to modify the next  (except when the feature map size is halved, in which case an additional pair of convolutional layer and pooling layer is first applied).

 

\paragraph{Encode transform and decode transform} 
Each encoding block has a nested residual structure, taking in the conditional features as input to transform the corresponding stochastic unit , illustrated in Figure~\ref{fig:autoencoding_block}. 
The conditional features are first convolved and then fed into a nested Real NVP block.   
The output of the Real NVP block (applied twice, see below) is then convolved and added to the convolved conditional features.
We apply non-linearity before convolving again and adding to the the original conditional features via skip connection.


The decode transform is similar, except we convolve the incoming stochastic unit to modify the conditional features, thus having a shorter computational path to reconstruct the data. 



\paragraph{Real NVP block}
The stochastic unit is split into two halves ( and ) using the checkerboard mask. 
The convolved conditional feature is concatenated with the masked stochastic unit (the part that is not masked is denoted as ) to transform the part of the stochastic unit being masked out (). 
The same Real NVP block is reused (using the same set of parameters), alternating the pattern of the mask to transform the other half of the stochastic unit (with  and  swapped). 
We found sharing the parameters of these two consecutive Real NVP transformations to improve the convergence of the likelihood. 


\subsection{Variational dequantization}
We use the variational dequantization proposed by ~\citet{ho2019flow++} in all our density estimation experiments. 
We first map the input image  into a deterministic feature  space using a convolutional network of the following form: 

where \texttt{act} denotes activation function. 
Note that the input to this convolution network is rescaled to  via , where \texttt{n\_bits} is the number of bits.
We then use the Real NVP block to transform a standard Gaussian noise, where  acts as the conditional feature. 
We apply two Real NVP blocks to obtain , and transform it into  using the logistic sigmoid activation function so that each element of  lies within .
We then perturb the data by , which is then passed through a \texttt{clip} operator for numerical stability. 
We define \texttt{clip} to be 

where . 
Finally, we pass the clipped value into the logit function (inverse sigmoid) to obtain the dequantized data. 
We have taken into account the probability density of  and all the changes of variable (i.e. sigmoid, rescaling, clipping, and logit) when computing the lower bound.

\subsection{Initialization and parameterization constraint}
\label{app:init}
Unless it is otherwise stated, we initialize all the convolutional kernels using truncated normal distribution with standard deviation , and for weight normalization, the rescaling parameter is set to be  and the shifting parameter . 
Only for the last layer of the Real NVP block we initialize  to be . 
We apply a split operator to this last layer to obtain a ``shift'' coefficient and ``log scale'' coefficient for affine transformation. 
The last layer has double the dimensionality of the stochastic unit to be transformed.
The split operator simply splits it in two parts. 
For the log scale coefficient, we apply the log-sigmoid function to make sure after exponentiation, it is bounded between  and  (similar to \citet{kingma2018glow}). 
Since the pre-log-sigmoid is initialized to be , we add in a constant that depends on the total number of transformations that will be apply to the stochastic unit such that the overall transformation (after composition) will rescale the raw input by a factor of  (without considering activation normalization). 
This is to ensure the entire transformation is more robust to variation of depths at initialization. 

We also apply activation normalization \citep{kingma2018glow} with data-dependent initialization that standardizes the transformed feature, after each encoding transform and each decoding transform. 
We clip the log scale coefficient at .


\subsection{Deterministic warm up}
\label{app:warm}
Due to our choice of flow, our instantiation of ANF resembles a VAE. 
It has been previously shown that starting off with less regularization and noise injection is beneficial to training, a technique known as deterministic warm up \citep{raiko2007building,sonderby2016ladder}. 
Similarly, if we expand the objective of ANF with autoencoding transform (affine coupling), we get

where  is the dimensionality of the augmented data , and  are defined recursively as 
 
with the initial values , .
We modify the objective by lowering the weighting of  and  such that the network can focus more on Gaussianizing the raw input . 
We defined the modified objective as

where  is all the trainable parameters.
We linearly anneal the weighting coefficient  from  to  for the first  iterations of the training.
Note that in practice we apply the same  to all augmented data in the hierarchical setup. 




\newpage
\subsection{Hyperparameters}
\label{app:hparam}
{Notation summary for hyperparameters:}
\vspace{-3mm}
\begin{itemize}\setlength\itemsep{0.0em}
\item : number of stochastic units ().
\item : number of steps (encoding-decoding pairs).
\item : number of samples for importance sampling.
\item : decoupled weight decay coefficient for Adam.
\item : number of channels (all deterministic features). 
\item : number of channels for the 'th stochastic unit (stochastic features). 
Power denotes repetition.   
\item : feature map size (squared).  
\item : kernel size (except for the data space layer). 
\item : batch size. 
\item : step size.
\item : annealing schedule (number of parameter updates).
\item : number of updates.
\item : activation function
\end{itemize}

\begin{table*}[ht]
\centering
\begin{tabular}{cccccc} 
\toprule
{\bf } & {\bf MNIST} & {\bf CIFAR 10} & {\bf ImageNet 32} & {\bf ImageNet 64} & {\bf CelebA-HQ} \\ 
\midrule
 & 4 & 8 & 8 & 6 & 5\\
 & 5 & 5 & 5 & 5 & 5\\
 & 5000 & 1000 & 1000 & 1000 & 1000\\
 & 1e-5 & 1e-5 & 0 & 0 & 0\\
 & 64 & 160 & 256 & 256 & 160\\
 & 2,2,2,2 & 32,28,...,4 & 32,28,...,4 & 24,20,...,4 & 20,16,...,4\\
 & 14\textsuperscript{2},7\textsuperscript{2} & 16\textsuperscript{4},8\textsuperscript{4} & 16\textsuperscript{4},8\textsuperscript{4} & 32\textsuperscript{2},16\textsuperscript{2},8\textsuperscript{2} & 128,64,32,16,8 \\
 & 3 & 3 & 3 & 3 & 3\\
 & 64 & 64 & 64 & 64 & 12\\
 & 0.001 & 0.001 & 0.001 & 0.0005 & 0.0005\\
 & 20K & 20K & 20K & 20K & 20K\\
 & 1M & 1M & 2M & 2M & 2M\\
 & Swish & Swish & Swish & Swish & Swish\\
\bottomrule
\end{tabular}
\caption{Hyperparameter details of density estimation tasks.}
\label{tab:exp_details}
\end{table*}





\clearpage

\section{Extended related work and future direction}
\label{app:related}
\paragraph{Normalizing flows.}
The term \emph{Normalizing Flow} was originally coined by \citet{tabak2010density,tabak2013family} where it was used for density estimation.
Differentiable bijective models were first introduced to the deep learning community as likelihood-based generative models by \citet{rippel2013high,dinh2014nice}, and as an inference machine by \citet{rezende2015variational}. 
Most development within this line of research is dedicated to improving the expressivity of the bijective mapping while maintaining computational tractability of the log-determinant of the Jacobian. 
Each family of flows can be characterized by the ``trick'' used to achieve this, e.g.
\begin{itemize}
\item \emph{Partial ordered dependency}. 
If the mapping has a partial and ordered dependency, its Jacobian matrix will be a triangular matrix, the determinant of which can be computed in linear time. This includes the following:
\begin{itemize}
    \item \citet{dinh2014nice,dinh2016density,kingma2018glow,ho2019flow++} use a block-wise conditioning in the mapping, and
    \item \citet{kingma2016improved,chen2016variational, papamakarios2017masked,huang2018neural} generalize block-wise dependency to temporal dependency wherein all the variables prior to the current variable of a given ordering are inputs of the conditioning to transform the current variable.
\end{itemize}
\item \emph{Low rank transform}.
If the mapping is of a particular residual form, the Jacobian determinant can be computed readily using the matrix determinant lemma \citep{rezende2015variational}
or its higher rank generalization \citep{berg2018sylvester}.
\item \emph{Lipschitz residual flow}. If the nonlinear block of a residual mapping is no more than 1-Lipschitz, the overall mapping is invertible, and its Jacobian can be estimated using power series expansion, the Hutchinsons trace estimator \citep{behrmann2018invertible} and the Russian roulette estimator \citep{chen2019residualflows}. 
\item \emph{Special convolutional forms}. Certain structure of convolutional kernels can also be designed to to ensure tractability, such as via using  convolution \citep{kingma2018glow}, masking \citep{oord2016pixel,NIPS2016_6527,hoogeboom2019emerging,song2019mintnet,ma2019macow} or imposing certain repeated structure \citep{karami2019invertible}.
\end{itemize}

In this work, we introduce the augmentation trick, which generalizes flow-based methods in an orthogonal yet complementary manner. 
In particular, we employ the coupling proposed by \citet{dinh2016density} to transform the augmented data; one potential alternative is to replace it with any of the tricks mentioned above. 

\paragraph{Architectures and parameter sharing.} As the autoencoding transform we use generalizes VAEs and hierarchical VAEs, one potential direction is to consider parameterizations that have shared components which are shown to be conducive to training, such as the ResNet with top-downn inference \citep{kingma2016improved} and the bidirectional inference machine \citep{maaloe2019biva}.
As a generalization of VAEs, ANFs can also be applied to latent variable models of different graphical representations, such as variational recurrent neural networks \citep{chung2015recurrent} and models of sets \citep{edwards2016towards}; for example, the \emph{set flow} proposed by \citet{rasul2019set} is an instance of permutation-invariant ANF applied to sets. 
Another avenue for improving parameter efficiency is to consider tying the weights of different steps of transformations. 
As our theory suggests, consecutive transformations of the discretized Hamiltonian ODE would differ only slightly if the time derivatives are smooth enough. 
This means it would be sufficient to consider a single network which also takes in time embedding as input for all transformations.

\paragraph{Approximate Hamiltonian flows.}
Our approximation theory builds on the result of \citet{wang2019accelerated}, which follows the optimal control framework of \citet{wibisono2016variational}.
The augmented variable is treated as the costate, which is deterministically dependent on the state, i.e. the input data. 
Therefore we set the initial augmented distribution to be a Dirac point mass for the approximation theory to hold. 
Our theorem can be improved if one can show some time trajectories with 
the augmented variable drawn independently from a non-degenerate initial distribution are convergent to the prior distribution. 
We leave this for future work. 
Meanwhile, the same proof technique can be used to study the approximation capability of different families of flows.
In particular, the residual flows \citep{behrmann2018invertible,chen2019residualflows} and their continuous counterpart \citep{chen2018neural,grathwohl2018ffjord} can be used to approximate the deterministic Langevin diffusion, since
(1) one can replace the Brownian motion term with the gradient of the log marginal density without modifying its Fokker-Planck equation (see \citet{hoffman2019langevin} or the appendix of \citet{wang2019accelerated}) and (2) the first-order Langevin dynamic is known to be convergent to its stationary distribution \citep{roberts1996exponential}. 


\paragraph{Gradient-based flows.}
As the theory suggests, gradient of the potential can be used to guide the evolution of the particle.
This has been previously explored by \citet{duvenaud2016early}. 
\citet{salimans2015markov} on the other hand propose a hierarchical model inspired by the Hamiltonian dynamic, and \citet{song2017nice,levy2017generalizing} generalize Hamiltonian Monte Carlo (HMC) with trainable neural components. 
Similarly, one can parameterize a Gradient-based augmented generative flow to model the data distribution.


\paragraph{Normalizing flows for variational inference.}
The most well-known application of normalizing flows is to improve the variational distribution to approximate posterior distribution of (1) the latent representations \citep{rezende2015variational, kingma2016improved,tomczak2016improving,berg2018sylvester} and (2) the parameters of neural networks \citep{louizos2017multiplicative, krueger2017bayesian, huang2019stochastic}. 
ANF can also be applied to inference problems, with slight modification of the target potential. 
We show in Appendix \ref{app:anf_vi} that one can augment the target distribution with an independent distribution and infer the joint target altogether.
This boils down to the hierarchical variational method \citep{agakov2004auxiliary,ranganath2016hierarchical} as a special case when one step of autoencoding transform is applied. 

\paragraph{Variational gap.}
The joint likelihood that we maximize is a variational objective lower-bounding the marginal likelihood of the data. 
One potential avenue for improvement is to reduce this bias (the augmentation gap) throughout training, by closing up the gap via importance sampling \citep{burda2015importance} or using an unbiased estimate of the marginal likelihood \citep{luo2020sumo}.

\paragraph{Representation learning.}
Considering invertible transformations in an augmented data space allows us to sidestep the topology-preserving property of a homeomorphism. 
The issue of this property is discussed and addressed by \citet{cornish2019localised} by converting the flow into a latent-variable model. 
\citet{dupont2019augmented} adopt the same technique by augmenting the data space and apply the augmented continuous time flow to discriminative tasks. 
We hypothesize this can potentially improve the representation learned by an invertible model, for example in a semi-supervised setting \citep{nalisnick2019hybrid,atanov2019semi} or as a component of a reversible model for memory-efficient backpropagation \citep{gomez2017reversible}. 


\clearpage

\section{Augmented Normalizing Flows for Variational Inference}
\label{app:anf_vi}
Augmented normalizing flows can also be used for inference tasks where our goal is to approximate an unnormalized density  with a parametric distribution . 
This includes variational training of energy based models \citep{dai2017calibrating, zhai2016generative}, entropy regularized policy gradient in reinforcement learning \citep{mazoure2019leveraging, ward2019improving}, probability distillation \citep{oord2017parallel}, and variational Bayesian inference of latent variables \citep{kingma2013auto}. 

We focus on the case of variational inference (but the same technique can be used for other applications), where , and our goal is to maximize the ELBO

where we can apply the standard change of variable to get  with  as described in Section~\ref{sec:background}.
Alternatively, we can augment the target distribution  with an independent , and jointly transform a base distribution  into  to approximate  via an invertible map .
Concretely, we maximize the following quantity

where  and  denote the first and the second coordinates, respectively.
This lower-bounds the ELBO since 

is non-negative. 

{\bf Auxiliary variable for hierarchical variational inference.} The above derivation for applying ANF to variational inference is reminiscent of the \textit{auxiliary variable method} \citep{agakov2004auxiliary, ranganath2016hierarchical}.
To see this, assume we parameterize  as the composition , where 

with .
Then Equation (\ref{eq:elbo_anf})
becomes

where , which is equivalent to

where  and .
This shows hierarchical variational methods are a special case of ANF, and the latter can potentially be used to improve the joint expressivity of the former through additional composition. 


\newpage
\section{More samples}
\label{app:samples}
\subsection{CIFAR 10}
\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/samples/cifar_grid_20_20.png}
    \caption{CIFAR 10 samples}
    \label{fig:cifar_grid}
\end{figure*}

\newpage
\subsection{Celeba 64}
\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/samples/celeba64_grid_15_15.png}
    \caption{5-bit CelebA 64 samples}
    \label{fig:cifar_grid}
\end{figure*}



\newpage
\section{Proofs}
\label{app:proofs}

Define the scaling coefficients  and .
Let  be the standard normal density, and  be the data distribution.
Let  and  be some continuous function. 
Define the following Hamiltonian ordinary differential equation (ODE):

where  and  are the time derivatives of  and  at time , and  is the marginal density of . 

\begin{proposition}
For some convex , the trajectories of  and  following (\ref{eq:time_der_x},\ref{eq:time_der_e}) converge in distribution to  and , respectively, where  and  (i.e. a point mass at ).
\label{prop:hode}
\end{proposition}
\begin{proof}
By Theorem 1 of \citet{taghvaei19a} and Appendix C.4 of \citet{wang2019accelerated} (for an extension to high dimensional cases), 
since ,  and  satisfy the scaling condition in \citet{taghvaei19a} and  is convex,
 converges in KL divergence to  and  converges to  almost surely (which implies convergence in distribution). 
Pinsker's inequality implies  in total variation, , which has a dual representation:

This implies for any bounded, continuous ,

which converges to  as . 
By Portmanteau's Lemma,  in distribution.
\end{proof}

We first construct a sequence of encoding functions  and decoding functions  parameterized by neural networks, and define the following (volume preserving) invertible mappings

with  and . 
The step size parameter  will be chosen to depend on the depth coefficient , i.e. the number of layers of the joint transformation. 

Below we prove ANF of the above form can universally transform  into . 
We make the following assumption on the family of :
\begin{assumption}
\label{assumption}
We assume the gradient of the convex function in Proposition (\ref{prop:hode})  is continuous, and that
 and 
 have a bounded second time derivative (on the trajectories  and  which are also functions of time), and are uniformly Lipschitz; that is,

for some , where we define the single-argument vector functions  and  as the time derivatives of the trajectories . 
\end{assumption}

We denote by  the family of probability measures that satisfies this assumption. 

Before we move on to approximation, we start with a lemma for bounding approximation error by solving recursion using the technique of generating functions. 
\begin{lemma}
\label{lem:recursive_error}
If for any ,  is a sequence of real numbers satisfying

for some constant , then 
\end{lemma}
\begin{proof}
We would like to bound the error  explicitly.
To do so, we first note that the sequence  is no larger than , which is recursively defined as 

for ,
where for simplicity we let .

Now to express  explicitly, we use the method of generating function, following the recipe of \citet{wilf2005generatingfunctionology} (see Chapter 1 for a brief introduction). 
Define function  to be a power series whose coefficients are 's; that is, . 
Multiply both sides of (\ref{rec_err:xx}) by  and summing over the indices of non-negative integers  give us


After rearrangement, we have

which can be decomposed into the partial fractions

where  and  are the roots of the quadratic function ,
which satisfy  and . 

For sufficiently small , we can break (\ref{eq:partial_fraction}) into the geometric series


This means for , since , the coefficient of  can be expressed as 



Now let  be the larger root. 
Solving  yields 

where .

We show that the parenthesis in (\ref{eq:Ex}) can be controlled asymptotically (i.e. does not exceed certain constant for sufficiently large ), and that since  diminishes,  converges. 
First, since ,  and


Second, since  for  and ,

which converges to  as . 

Finally, since  as  and ,  for all  as . 
\end{proof}


We are now ready to show the result of the pointwise approximation of the Hamiltonian ODE using ANFs with affine (more specifically, additive) coupling. 
\begin{proposition}
\label{prop:anf_pointwise}
Let  and  be trajectories (mappings of ) following the Hamiltonian ODE (\ref{eq:time_der_x},\ref{eq:time_der_e}) described in Proposition \ref{prop:hode} dependent on some initial distribution . 
For each , we can choose some number of layers  of the joint transformation and a sequence of pairs of  and  (dependent on ) for , such that  and  as  pointwise for . 
\end{proposition}
\begin{proof}
Fix  and  and some compact subset .
We first consider all points  in , and show that  can be used to approximate  uniformly well. 

We consider a -step joint transformation, and set . 
We start with approximating  by .
Since  is , by the universal approximation theorem (UAT) of neural networks \citep{cybenko1989approximation}, we can choose some  such that  for all .

We proceed with an approximate leap-frog integration of the dynamic, using the neural encoders and decoders to approximate the time derivatives. 
Let  where , which is compact, since  is compact and  is continuous wrt .
Again, by the UAT, we can choose some  such that  for all . 
Likewise, we let  where  with  being the identity map, such that  is also compact since  is continuous wrt , and choose  such that  for all .

Repeating the same construction for  and  for , 
we have

with  and  chosen such that 
\begin{enumerate}
    \item  for all  where  is a continuous map of ; and
    \item  for all  where  is a continuous map of .
\end{enumerate}
Such choices of  and  are possible since by construction  and  are compact. 

Equations (\ref{eq:x_nn},\ref{eq:e_nn}) are approximate midpoint methods as they use functions to approximate the time derivatives evaluated at midpoints of their counterparts. 
The exact midpoint method has a cubic error rate of , for some  between the midpoint and the approximating point, where  is the interval width of each iteration; see Section 5.4 of \citet{epperson2013introduction}. 
That is,

for some  between the two steps. 

Subtracting (\ref{eq:x_nn}) from (\ref{eq:x_mid}) yields


By triangle inequality, we have


The error on the RHS consists of two parts: (1) the first two terms constitute the propagated error from the previous steps and (2) the third term is a newly introduced truncation error due to the Taylor expansion. 

By triangle inequality again,

\resizebox{1.00\linewidth}{!}{
\begin{minipage}{\linewidth}

\end{minipage}
}

Again the RHS can be decomposed into two error parts: (1) a midpoint deviation resulting from performing midpoint numerical integration which would not vanish even if the neural network is replaced with the true time derivative, and (2) an approximation error due to the inaccuracy of approximating the time derivative. 

Letting  and , and applying the properties of the Assumption \ref{assumption}, we have

owing to the uniform error bound of the neural decoder
 for all  and the fact that  since .  

The same can be done to obtain a bound on  by subtracting (\ref{eq:e_nn}) from (\ref{eq:e_mid}), which yields


To summarize, we have

where .


Summing  and subtracting  from both sides yield

Note that . 
Similarly, summing  and subtracting  from both sides yield


To recursively express  in terms of itself (except for ), 
we sum over the sequence  again 


Substituting into (\ref{err:xe}) yields


Since ,
,
 and , the above can be rearranged and further bounded by



The same can be done for (\ref{err:ex}) to analyze . 





By Lemma \ref{lem:recursive_error}, we know that the elements of both sequences of error  and  converge uniformly on  to  as . 
In particular, for all ,  and compact subset  of , there exists some large enough integer  for which a joint transformation of  layers parameterized by some neural encoders and decoders satisfies
 and  for all .


Consider some positive value . 
We let ,  and . 
We can find a sequence of models with an error rate  and  converging pointwise on  to  as .
This implies 

pointwise as . 
The same holds for the augmented variable . 
\end{proof}


The lemma below shows if one can approximate the solution of an ODE (, i.e.  and  are asymptotically indistinguishable) and if the limit of the solution is a transport map (), then the approximation also forms a transport map (). 
\begin{lemma}
\label{lem:conv_indistinguishable}
Let ,  and  be random variables. 
If  in distribution and if  almost surely as , then  in distribution. 
\end{lemma}
\begin{proof}
Let  be an arbitrary \emph{bounded} and \emph{Lipschitz continuous} function. 
Then 


First, since  in distribution and since  is bounded and continuous, by the Portmanteau Lemma the first term of the RHS converges to  as . 
Second,
since  is almost surely asymptotically indistinguishable from  (let  be the almost sure set),
and since the Lipschitzness of  implies uniform continuity, the following are true
\begin{itemize}
    \item For all , there exists a  such that  implies .
    \item For any , there exists a integer  such that for all ,  for all . 
\end{itemize}
These imply  on .
Then

converges to , since (1)
boundedness of  and the \emph{Bounded Convergence Theorem} imply  and (2)  implies  = 0. 
Finally, since  is arbitrary,
by the Portmanteau Lemma again,  converges in distribution to  as . 
\end{proof}


We now are ready to prove Theorem \ref{thm:anf_dist}, which we restate below.
The main idea is to notice that ANFs can be made pointwise inseparable from the Hamiltonian ODE, which implies weak convergence since the Hamiltonian ODE converges in distribution. 
\anfdist*
\begin{proof}
First, by Proposition \ref{prop:hode},  in distribution as . 
Second,  and  chosen from Proposition \ref{prop:anf_pointwise} are almost surely asymptotically indistinguishable.
Thus, by Lemma \ref{lem:conv_indistinguishable},  converges in distribution to .
The same holds for the augmented variable . 
Let  and  denote such sequences. 
By Theorem 2.7 of \citet{van2000asymptotic},  in distribution (as  is a constant).
\end{proof}





\end{document}
