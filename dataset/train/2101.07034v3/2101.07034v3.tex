
To validate the proposed AGRNet, we conduct extensive experiments on face parsing as well as on human parsing for generalizability.

\subsection{Datasets and Metrics}

\begin{table*}[htbp]
\centering
\caption{Comparison with the state-of-the-art methods on face parsing datasets (in F1 score).}
\label{table:comparison}
    \begin{subtable}{\textwidth}
    \centering
    \begin{tabular}{c|cccccccc|c}
    \toprule
    Methods & Skin & Nose & U-lip & I-mouth & L-lip & Eyes & Brows & Mouth & Overall \\ 
    \midrule
    
    Liu \et \cite{liu2017face} & 92.1 & 93.0 & 74.3 & 79.2 & 81.7 & 86.8 & 77.0 & 89.1 & 88.6 \\ 
    Lin \et \cite{lin2019face} & 94.5 & 95.6 & 79.6 & 86.7 & 89.8 & 89.6 & 83.1 & 95.0 & 92.7 \\ 
    Wei \et \cite{wei2019accurate} & \textbf{95.6} & 95.2 & 80.0 & 86.7 & 86.4 & 89.0 & 82.6 & 93.6 & 91.7 \\ 
    Yin \et \cite{yin2020end} & - & \textbf{96.3} & 82.4 & 85.6 & 86.6 & 89.5 & 84.8 & 92.8 & 91.0 \\ 
    Liu \et \cite{liu2020new} & 94.9 & 95.8 & \textbf{83.7} & 89.1 & \textbf{91.4} & 89.8 & 83.5 & 96.1 & 93.1 \\
    Te \et\cite{te2020edge} & 94.6 & 96.1 & 83.6 & 89.8 & 91.0 & \textbf{90.2} & 84.9 & 95.5 & 93.2 \\
    \midrule
    Ours & 95.1 & 95.9 & 83.2 & \textbf{90.0} & 90.9 & 90.1 & \textbf{85.0} & \textbf{96.2} & \textbf{93.2} \\
    \bottomrule
    \end{tabular}
    \vspace{0.5em}
    \caption{The Helen dataset}
    \end{subtable}

    \begin{subtable}{\textwidth}
    \centering
    \begin{tabular}{c|cccccccccc|c}
    \toprule
    Methods & Skin & Hair & L-Eye & R-Eye & U-lip & I-mouth & L-lip & Nose & L-Brow & R-Brow & Mean \\ 
    \midrule
    Zhao \et \cite{zhao2017pyramid} & 93.5 & 94.1 & 86.3 & 86.0 & 83.6 & 86.9 & 84.7 & 94.8 & 86.8 & 86.9  & 88.4 \\
    Liu \et \cite{liu2020new}       & 97.2 & 96.3 & 88.1 & 88.0 & 84.4 & 87.6 & 85.7 & 95.5 & 87.7 & 87.6 & 89.8 \\
    Te \et \cite{te2020edge}         & 97.3 & 96.2 & 89.5 & 90.0 & 88.1 & 90.0 & 89.0 & 97.1 & 86.5 & 87.0 & 91.1 \\
    Luo \et \cite{luo2020ehanet} & 95.8 & 94.3 & 87.0 & 89.1 & 85.3 & 85.6 & 88.8 & 94.3 & 85.9 & 86.1 & 89.2 \\
    Wei \et \cite{wei2019accurate} & 96.1 & 95.1 & 88.9 & 87.5 & 83.1 & 89.2 & 83.8 & 96.1 & 86.0 & 87.8 & 89.4 \\
    \midrule
    Ours & \textbf{97.7} & \textbf{96.5} & \textbf{91.6} & \textbf{91.1} & \textbf{88.5} & \textbf{90.7} & \textbf{90.1} & \textbf{97.3} & \textbf{89.9} & \textbf{90.0} & \textbf{92.3} \\
    \bottomrule
    \end{tabular}
    \vspace{0.5em}
    \caption{The LaPa dataset}
    \end{subtable}
    
    \begin{subtable}{\textwidth}
    \centering
    \begin{tabular}{c|ccccccccc|c}
    \toprule
    \multirow{2}{*}{Methods}  & Face    & Nose  & Glasses & L-Eye & R-Eye & L-Brow  & R-Brow   & L-Ear & R-Ear & \multirow{2}{*}{Mean} \\
                              & I-Mouth & U-Lip & L-Lip   & Hair  & Hat   & Earring & Necklace & Neck  & Cloth &                       \\
    \midrule
    \multirow{2}{*}{Zhao \et \cite{zhao2017pyramid}}   & 94.8    & 90.3  & 75.8    & 79.9  & 80.1  & 77.3    & 78.0       & 75.6  & 73.1  & \multirow{2}{*}{76.2} \\
                              & 89.8    & 87.1  & 88.8    & 90.4  & 58.2  & 65.7    & 19.4     & 82.7  & 64.2  &                       \\
    \midrule
    \multirow{2}{*}{Lee \et\cite{CelebAMask-HQ}} & 95.5    & 85.6  & \textbf{92.9}    & 84.3  & 85.2  & 81.4    & 81.2     & 84.9  & 83.1  & \multirow{2}{*}{80.3} \\
                              & 63.4    & 88.9  & 90.1    & 86.6  & \textbf{91.3}  & 63.2    & 26.1     & \textbf{92.8}  & 68.3  &                       \\
    \midrule
    \multirow{2}{*}{Luo \et\cite{luo2020ehanet}} & 96.0 & 93.7 & 90.6 & 86.2 & 86.5 & 83.2 & 83.1 & 86.5 & 84.1  & \multirow{2}{*}{84.0} \\
                              & 93.8 & 88.6 & 90.3 & 93.9 & 85.9 & 67.8 & 30.1 & 88.8 & 83.5  &                       \\
    \midrule
    \multirow{2}{*}{Wei \et \cite{wei2019accurate}} & 96.4    & 91.9    & 89.5    & 87.1  & 85.0  & 80.8   & 82.5     & 84.1    & 83.3  & \multirow{2}{*}{82.1} \\
                              & 90.6      & 87.9  & 91.0    & 91.1  & 83.9  & 65.4    & 17.8     & 88.1  & 80.6  &    \\  
    \midrule
    \multirow{2}{*}{Te \et\cite{te2020edge}}     & 96.2    & \textbf{94}    & 92.3    & 88.6  & 88.7  & \textbf{85.7}    & 85.2     & 88    & 85.7  & \multirow{2}{*}{85.1} \\
                              & \textbf{95.0}      & 88.9  & \textbf{91.2}    & \textbf{94.9}  & 87.6  & 68.3    & \textbf{27.6}     & 89.4  & \textbf{85.3}  &    \\  
    \midrule
    \multirow{2}{*}{Ours}     & \textbf{96.5}    & 93.9    & 91.8    & \textbf{88.7}  & \textbf{89.1}  & 85.5    & \textbf{85.6}     & \textbf{88.1}    & \textbf{88.7}  & \multirow{2}{*}{\textbf{85.5}} \\
                              & 92.0      & \textbf{89.1}  & 91.1    & 95.2  & 87.2  & \textbf{69.6}    & \textbf{32.8}     & 89.9  & 84.9  &    \\  
    \bottomrule
    \end{tabular}
    \vspace{0.5em}
    \caption{The CelebAMask-HQ dataset}
    \end{subtable}
\end{table*}





\textbf{Face Parsing} The Helen dataset \cite{le2012interactive} includes 2,330 images with 11 categories: background, skin, left/right brow, left/right eye, upper/lower lip, inner mouth and hair. Specifically, we keep the same training/validation/test protocol as in \cite{le2012interactive}. The number of the training, validation and test samples are 2,000, 230 and 100, respectively. 
The LaPa dataset \cite{liu2020new} is a large-scale face parsing dataset, consisting of more than 22,000 facial images with abundant variations in expression, pose and occlusion, and each image is provided with an 11-category pixel-level label map and 106-point landmarks. 
The CelebAMask-HQ dataset \cite{CelebAMask-HQ} is composed of 24,183 training images, 2,993 validation images and 2,824 test images. The number of categories in CelebAMask-HQ is 19. In addition to facial components, the accessories such as eyeglass, earring, necklace, neck, and cloth are also annotated.

\textbf{Human Parsing} 
The LIP dataset \cite{gong2017look} is a large-scale dataset focusing on semantic understanding of person, which contains 50,000 images with elaborated pixel-wise annotations of 19 semantic human part labels as well as 2D human poses with 16 key points. The images collected from the real-world scenarios contain human appearing with challenging poses and views, serious occlusions, various appearances and low-resolutions.

\textbf{Metrics}: We employ three evaluation metrics to measure the performance of our model: pixel accuracy, intersection over union (IoU) and F1 score. The mean value is calculated by the average of total categories.
Directly employing the pixel accuracy metric ignores the scale variance amid facial components, while the mean IoU and F1 score are better for evaluation. 
To keep consistent with the previous methods, we report the overall F1 score on the Helen dataset, which is computed over the merged facial components: brows (left + right), eyes (left + right), nose, mouth (upper lip + lower lip + inner mouth).

\subsection{Implementation Details}



During the training, we employ the random rotation and scale augmentation. The rotation angle is randomly selected from  and the scale factor is randomly selected from . The ground truth of the edge mask is extracted according to the semantic label map. If the label of a pixel is different from any of its four neighbors, it is regarded as an edge pixel. For the Helen dataset, we pre-process the face image by face alignment as in other works. For the LaPa and CelebAMask-HQ datasets, we directly utilize the original images without any pre-processing.


We take the ResNet-101\cite{he2016deep} as the backbone and extract the output of conv\_2 and conv\_5 layers as the low-level and high-level feature maps for multi-scale representations. 
To reduce the information loss in the spatial space, we utilize the dilation convolution in the last two blocks and the output size of the final feature map is 1/8 of the input image. 
To fully exploit the global information in high-level features, we employ a spatial pyramid pooling operation \cite{zhao2017pyramid} to learn multi-scale contextual information. 
In the edge prediction branch, we concatenate the output of conv\_2, conv\_3, conv\_4 layers and apply a 1  1 convolution to predict the edge map. 
In the differentiable graph projection, we set top-\textbf{4} pixels as representative vertices for each face component, and thus the number of graph vertices is 4 times of the category number. 
We choose the hyper-parameters in \eqref{eq:loss_total} as  = 1,  = 1,  = 1,  = 0.5,  = 0.1 by grid search and according to prior knowledge\footnote{The first four losses in \eqref{eq:loss_total} are in the form of logarithm, while the last loss is in the form of the Euclidean distance. Hence, the weighting parameters of the first four losses should be larger than the parameter of the last loss.} of the scale of each loss for the initial setting of grid search.
In the proposed discriminative loss in \eqref{eq:discriminative_loss}, we normalize the vertex features and set .

All the experiments are implemented with 4 NVIDIA RTX 2080Ti GPUs. Stochastic Gradient Descent (SGD) is employed for optimization. We initialize the network with a pre-trained model on ImageNet. The input size is  and the batch size is set to 8. The learning rate starts at 0.001 with the weight decay of 0.0005. 
The batch normalization is implemented with In-Place Activated Batch Norm \cite{rotabulo2017place}.  For fair comparisons, these settings are adopted for all the compared methods.

\begin{figure*}[tbp]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/comparision.pdf}
\caption{Visualization of parsing results from different methods on the CelebAMask-HQ dataset.}
    \label{fig:comparison}
\end{figure*}

\subsection{Face Parsing}

We perform comprehensive experiments on the commonly used three face parsing datasets and present the experimental results as follows.


\subsubsection{Ablation study}

The ablation study is conducted from multiple aspects on the LaPa dataset.

\textbf{On composition of modules.} 
We evaluate the improvement brought by different modules in AGRNet. 
Specifically, we remove some components and train the model from scratch under the same initialization. The quantitative results are reported in Table~\ref{table:ablation}-(a), where {\it ResNet} denotes the baseline model trained with the network consisting of the backbone and the pyramid spatial pooling. It achieves 90.9\% in Mean F1 score.
{\it Spatial} and {\it Adaptive} refer to the respective schemes of graph construction. 
{\it Spatial} denotes uniform spatial pooling as in the previous work \cite{te2020edge} and {\it Adaptive} denotes the proposed adaptive pooling method in this paper, which improves the F1 score of Model 1 ({\it ResNet}) by 0.2\% and 0.5\%, respectively.
This result validates the effectiveness of the proposed adaptive pixel-to-vertex projection. 
{\it Edge} and {\it Graph} represent the edge module and graph reasoning module, respectively. 
Compared with Model 3, the edge module achieves improvement of 0.2\% in Mean F1 score, while the {\it Graph} module improves the Mean F1 score by 0.5\%. 
This demonstrates the superiority of the proposed graph reasoning. 
By employing all the modules, we achieve the best performance of 92.3\% in Mean F1 score.

\textbf{On loss functions.} We evaluate the effectiveness of different loss functions described in Section~\ref{subsec:loss} and show the comparison results in Table~\ref{table:ablation}-(b). We observe that the Discriminative loss leads to 0.5\% of improvement and the Boundary-Attention loss brings 0.9\% of improvement in Mean F1 score compared with the traditional cross entropy loss. The best performance is obtained by utilizing all of these loss functions.

\textbf{On vertex numbers.} Table~\ref{table:ablation}-(c) presents the performance with respect to different number of vertices. Our model achieves the best mIoU and overall F1 under the top 4 setting. We suppose that too many vertices result in redundant noisy vertices whereas fewer vertices fail to represent enough semantic information.

\textbf{On FLOPs and parameters.} 
We compute the complexity of different modules in FLOPs and Parameter size in Table~\ref{table:ablation}-(d). 
All the numbers are computed with the input image size of 473  473 when the batch size is set to 1.
ResNet refers to the backbone network, which has 6.652GFLOPs and 14.10M of parameters. The Edge Perceiving module brings only about 0.25GFLOPs and 0.03M of parameters. 
The Graph Projection module further leads to only 0.471G and 0.02M of increase in FLOPs and parameters, respectively. After integrating the Graph Reasoning module, the network has 7.411GFLOPs and 14.15M of parameters. 
We see that our model improves the F1 score by 1.4\% at the cost of mere 0.8GFLOPs and 0.05M of parameters compared with the ResNet backbone.  

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/Hyper.png}
    \caption{Visualization of the hyper-parameter search.}
    \label{fig:hyper}
\end{figure}


\textbf{On hyper-parameters.}

In the experiments, we set the parameters  in \eqref{eq:loss_total} based on the setting of the LaPa work \cite{liu2020new}, which achieves convincing performance on several face parsing datasets. As to the weighting parameter of the discriminative loss---, we assign its value according to the approximate proportion of loss scale.
For more rigorous parameter settings, we additionally adopt the traditional way of performing hyper-parameter optimization---{\it grid search} or a parameter sweep, which is an exhaustive searching through a manually specified subset of the hyper-parameter space of a learning algorithm. 
In our case, the grid search algorithm is guided by the evaluation metric of Overall F1 score on the test set of the {\it Helen} dataset. 

Specifically, we train our model under a finite set of reasonable settings of hyper-parameters . We change a single parameter at a time, and report the respective results in Table \ref{table:hyper}. 
We see that a choice of these parameters  is empirically optimal in this case, which is our setting. Thus, in general, our choice of the parameters is empirically optimal.

Further, we illustrate the grid search in Fig.~\ref{fig:hyper}. 
We observe that the results are insensitive to , while they show sensitivity to ---the weighting parameter of the discriminative loss. This is mainly because the discriminative loss has less contribution to the final segmentation result than the cross entropy loss. 
Therefore, assigning a large weight to the discriminative loss will weaken the supervision of the ground truth parsing map and thus influence the overall performance.

\begin{table}[t]
\centering
\caption{The parsing results under different hyper-parameters (measured by overall F1).}
\label{table:hyper}
\begin{tabular}{c|cccccccc|c}
\toprule
Parameter & =0.1 & =0.5 & =1 & =2 & =5 \\ 
\midrule

 & 92.8 & 93.0 & \textbf{93.1} & 93.1 & 92.7 \\
 & 92.9 & 92.8 & \textbf{93.1} & 93.0 & 92.4 \\
 & 92.2 & 92.6 & \textbf{93.1} & 93.0 & 92.1 \\
 & 93.1 & \textbf{93.2} & 93.1 & 93.0 & 92.9 \\
 & \textbf{93.1} & 92.0 & 91.5 & 90.0 & 85.4 \\
\bottomrule
\end{tabular}
\end{table}





\begin{table*}[t]
\centering
\caption{The performance over domain gaps (measured in F1 score).}
\label{table:cross}
\begin{tabular}{c|c|cccccccc|c}
\toprule
Train & Test & Skin & Nose & U-lip & I-mouth & L-lip & Eyes & Brows & Mouth & Overall \\ 
\midrule

Helen-Train & Helen-Test & 95.0 & 96.0 & 83.1 & 90.0 & 90.7 & 89.9 & 85.1 & 96.2 & 93.1 \\
\midrule
CelebAMask-HQ & Helen-Test  & 93.2 & 94.7 & 82.7 & 88.5 & 90.1 & 89.1 & 79.6 & 92.4 & 91.8  \\
\midrule
Helen-Primary & Helen-Reference & 92.6 & 92.0 & 74.1 & 91.0 & 89.2 & 81.2 & 83.1 & 95.6 & 90.9  \\
Helen-Reference & Helen-Primary & 61.3 & 34.8 & 4.6 & 0 & 45.1 & 69.4 & 30.7 & 33.2 & 42.9  \\
\bottomrule
\end{tabular}
\end{table*}

\begin{table*}[htbp]
\centering
\caption{Experimental comparison on the LIP dataset for human parsing (in IoU score).}
\label{table:LIP}
\resizebox{\textwidth}{!}{
\begin{tabular}{c|cccccccccccccccccccc|c}
\toprule 
     & background & hat   & hair  & glove & glasses & u-cloth & dress & coat  & socks & pants & j-suits & scarf & skirt & face & l-arm & r-arm & l-leg & r-leg & l-shoe & r-shoe & mean \\
\midrule 
SegNet~\cite{badrinarayanan2017segnet} &70.62&26.60&44.01 &0.01 &0.00 &34.46 &0.00 &15.97 &3.59 &33.56 &0.01 &0.00 &0.00 &52.38 &15.30 &24.23 &13.82&13.17 &9.26 &6.47  &18.17\\
FCN-8s~\cite{long2015fully} &78.02&39.79&58.96 &5.32 &3.08 &49.08&12.36&26.82&15.66&49.41 &6.48 &0.00 &2.16 &62.65 &29.78 &36.63 &28.12&26.05&17.76&17.70 &28.29\\
DeepLabV2~\cite{chen2018deeplab} &84.53&56.48&65.33&29.98&19.67&62.44&30.33&51.03&40.51&69.00&22.38&11.29&20.56&70.11 &49.25 &52.88 &42.37&35.78&33.81&32.89 &41.64\\
Attention~\cite{chen2016attention} &84.00&58.87&66.78&23.32&19.48&63.20&29.63&49.70&35.23&66.04&24.73&12.84&20.41&70.58 &50.17 &54.03 &38.35&37.70&26.20&27.09 &42.92\\
Attention+SSL~\cite{gong2017look} &84.56&59.75&67.25&28.95&21.57&65.30&29.49&51.92&38.52&68.02&24.48&14.92&24.32&71.01 &52.64 &55.79 &40.23&38.80&28.08&29.03 &44.73\\
ASN~\cite{luc2016semantic} &84.01&56.92&64.34&28.07&17.78&64.90&30.85&51.90&39.75&71.78&25.57 &7.97 &17.63&70.77 &53.53 &56.70 &49.58&48.21&34.57&33.31 &45.41\\
SSL~\cite{gong2017look} &84.64&58.21&67.17&31.20&23.65&63.66&28.31&52.35&39.58&69.40&28.61&13.70&22.52&74.84 &52.83 &55.67 &48.22&47.49&31.80&29.97 &46.19\\
MMAN~\cite{luo2018macro} &84.75&57.66&65.63&30.07&20.02&64.15&28.39&51.98&41.46&71.03&23.61 &9.65 &23.20&69.54 &55.30 &58.13 &51.90&52.17&38.58&39.05 &46.81\\
SS-NAN~\cite{zhao2017self} &\textbf{88.67}&63.86 &70.12&30.63&23.92&\textbf{70.27}&33.51&\textbf{56.75}&40.18&72.19&27.68&\textbf{16.98}&\textbf{26.41}&75.33&55.24&58.93&44.01&41.87&29.15&32.64
 &47.92\\
CE2P~\cite{ruan2019devil} & 87.6 & 65.29 & 72.54 & 39.09 & 32.73   & 69.46   & 32.52 & 56.28 & 49.67 & 74.11 & 27.23   & 14.19 & 22.51 & \textbf{75.5} & \textbf{65.14} & 66.59 & \textbf{60.1}  & \textbf{58.59} & \textbf{46.63}  & \textbf{46.12}  & 53.1 \\
\midrule 
Ours & 87.4 & \textbf{67.56} & \textbf{72.63} & \textbf{42.96} & \textbf{36.65} & 69.15 & \textbf{35.35} & 55.74 & \textbf{51.13} & \textbf{74.19} & \textbf{30.49} & 15.69 & 22.61 & 75.01 & 65.04 & \textbf{67.76} & 58.51 & 57.13 & 45.79 & 45.37 & \textbf{53.8} \\
\bottomrule
\end{tabular}}
\end{table*}

\subsubsection{Comparison with the state of the art}

We compare our method with the state-of-the-art approaches on three datasets, including small-scale and large-scale ones, as presented in Table~\ref{table:comparison}. 
The score of the eyes/brows is the sum of scores of the left and right ones, and the overall score is calculated by the average score of the mouth, eyes, nose, and brows.
It is worth noting that, because the codes of some algorithms are not publicly available and the requirements of training data are different, it is hard to re-implement all the methods on every dataset. Therefore, we focus on implementing and testing the latest methods while dropping several previous methods, including Liu \et \cite{liu2017face},  Lin \et \cite{lin2019face} and Lee \et \cite{CelebAMask-HQ}. 
Specifically, the work by Lin \et \cite{lin2019face} cannot be tested on the LaPa dataset and the CelebAMask-HQ dataset, because there is a lack of fine facial segmentation bounding boxes in both datasets, which is however required by \cite{lin2019face}. Liu \et \cite{liu2017face} and Lee \et \cite{CelebAMask-HQ} are improved by follow-up works such as Wei \et \cite{wei2019accurate} and Luo \et \cite{luo2020ehanet}, thus we show the better results of Wei \et \cite{wei2019accurate} and Luo \et \cite{luo2020ehanet}.


On the small-scale Helen dataset, our method achieves comparable performance with the state-of-the-art approaches. 
On the two large-scale face parsing datasets---LaPa and CelebAMask-HQ, our method outperforms the previous methods by a large margin. 
Specifically, our model achieves improvement over Te \et \cite{te2020edge} by 1.2\% on the LaPa dataset and by 0.4\% on the CelebAMask-HQ dataset, especially on brows and eyes. 

We also visualize some parsing results of the CelebAMask-HQ dataset in comparison with competitive methods in Fig.~\ref{fig:comparison}. 
We see that our results exhibit accurate segmentation even over delicate details such as the categories of hair, earring and necklace. For example, in the first row, our model distinguishes the hand from the hair component accurately even if they are overlapped, while other methods assign the label of the hair to the hand. 
In the second row, our model generates the most consistent parsing result for the thin necklace while other results are fragmented. 
In the third row, our model separates the delicate earring from the hair, producing the finest parsing result. 


\subsubsection{Performance over domain gaps}
Although there is no significant domain gap among multiple datasets, there are different aspects to discuss the domain gap \cite{luo2021category,luo2019significance}. The first is to train/test on different datasets, while the second is to explore the diversity of face poses. The evaluation results are listed in Table \ref{table:cross}.

Firstly, we conduct the cross test with respect to different datasets, including the CelebAMask-HQ and Helen datasets. 
We divide the Helen dataset into the training set (denoted as "Helen-Train") and the testing set (denoted as "Helen-Test"). 
Specifically, we utilize the model trained on the CelebAMask-HQ dataset and test directly on "Helen-Test". However, these two datasets are labeled with different categories, so we select the common facial components for testing and mark the other labels as the background. 
Consequently, finer labels such as ``Glasses” and ``Ears" are categorized as the background, which affects the performance when compared with the result of evaluating on the Helen dataset as shown in Table \ref{table:cross}. 
Besides, among the common labels, the segmentation accuracy decreases mostly on "Brows" and "Mouth".

{Secondly, we conduct the cross test between data with different face poses. 
Specifically, we detect the pitch angle of each input image, and divide the Helen dataset into two categories according to whether the pitch angle is within . If the angle is between  and , we refer to the images as the "Helen-Primary" dataset, and otherwise the "Helen-Reference" dataset. 
Specifically, "Helen-Reference" contains 798 images out of 2330 images in the Helen dataset, which occupies 34\% of the complete dataset. 
As presented in Table \ref{table:cross}, the performance only drops by 1.4\% if we take "Helen-Primary" as the training dataset and "Helen-Reference" as the testing dataset. 
In comparison, when we swap the datasets, the parsing result is only 42.9\% in Overall F1 score. This is because the "Helen-Reference" dataset with large pitch angles does not provide enough information for understanding complete faces, especially for subtle mouth components.}

In addition, the cross test between data with different face poses also demonstrates the performance of AGRNet when some parts of the face missing, \eg, due to the self-occlusion, where non-front faces are self-occluded in general. The good performance of training on "Helen-Primary" and testing on "Helen-Reference" shows our method generalizes to self-occluded faces well. 
\textit{This generalization ability gives credits to the intrinsic structure embedded in the proposed graph representation.} That is, even though self-occluded data may demonstrate rather different distribution characteristics, there may be an intrinsic structure embedded in the data. Such intrinsic structure may be regarded to be better maintained from seen datasets to unseen datasets. Consequently, the graph representation learned from the face structure provides extra insights from the structure domain, in addition to the data domain, that finally enhances the generalizability of the network. 

\begin{figure}[t]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/Mat1.png}
    \vspace{-0.2cm}
    \caption{Visualization of the learned graph adjacency matrix.}
    \label{fig:graph}
\end{figure}

\subsubsection{Visualization of vertices and response}

Further, we present some visualization examples of vertices and the pixel-to-vertex projection matrix in order to provide intuitive interpretation. Fig.~\ref{fig:anchor_vis} shows the selected vertices with respect to specific facial components, where keypoints are marked as yellow. 
We observe that vertices lie in the interior region of the corresponding facial component. 
Besides, symmetric components are well separated, such as left and right brows. 

Also, we visualize the adjacency matrix trained on the Helen dataset in Fig. \ref{fig:graph}. As shown by the color bar, lighter colors indicate larger edge weights. 
We observe that the pair of left brow ("lb" in the figure) and the right brow ("rb" in the figure) has the strongest positive correlation, while the pair of inner mouth ("imouth" in the figure) and nose has the strongest negative correlation, which is reasonable to some extent.  

Furthermore, we visualize the pixel-to-vertex projection matrix via response maps. 
As in Fig.~\ref{fig:response_vis}, given a projection matrix , we visualize the weight of each pixel that contributes to semantic vertices. Since there are 4 vertices corresponding to each component, we sum them up as a complete component response map.
Brighter color in Fig.~\ref{fig:response_vis} indicates higher response. 
We observe that pixels demonstrate high response to vertices in the corresponding face component in general, which validates the effectiveness of the proposed adaptive graph projection.   
In particular, edge pixels show higher response in each component, thanks to the proposed edge attention and Boundary-Aware loss. 
Note that, there exist outliers if some component is blocked. For example, while one ear is occluded in both images from row 3 and row 4, several pixels still exhibit high response to the other ear region in the response map.

\begin{figure}[tbp]
    \centering
    \includegraphics[width=0.43\textwidth]{figures/anchor_vis.pdf}
    \caption{\textbf{Visualization of projected vertices with respect to facial components.} The yellow points represent vertices acquired from the proposed adaptive graph projection, and each column represents a certain category. Note that, the vertices significant overlap with the mainstream facial landmark layout \cite{liu2019grand, wu2018look}, which brings the advantages towards learning and reasoning the semantic representation for face parsing}
    \label{fig:anchor_vis}
\end{figure}




\begin{figure}[tbp]
    \centering
    \includegraphics[width=0.35\textwidth]{figures/lip.pdf}
    \caption{\textbf{Human parsing results on the LIP dataset.} Our parsing maps are clear and smooth along the boundaries, and even exhibit better results of shorts compared to the ground truth as depicted in the third row.}
    \label{fig:lip}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figures/failure.png}
    \caption{\textbf{Failure cases.} The earring is mislabeled in the first sample, the hair region is not continuous in the second sample, and some background is classified as nose in the third one.}
    \label{fig:failure}
\end{figure}


\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/response.pdf}
    \caption{\textbf{Response maps with respect to different facial components.} Brighter color indicates higher response. The response maps exhibit high response to ambiguous pixels along the inter-component boundaries.}
    \label{fig:response_vis}
\end{figure*}

\subsubsection{Failure cases}
We show several unsatisfactory examples in the CelebAMask-HQ dataset in Fig.~\ref{fig:failure}, where most of incorrect labels lie along the boundaries or "accessories" categories (\eg, earring and necklace). The cause of failure cases is mostly the significant imbalance of pixel numbers in different categories or the disturbance of adjacent pixels along the boundary.

\subsection{Human Parsing}
Human parsing is another segmentation task which predicts a pixel-wise label to each semantic human part. 
Unlike face parsing, human structure is hierarchical, where a large component could be decomposed into several fragments. 
Unlike recent works which often construct a fixed graph topology based on hand-crafted human hierarchy \cite{liu2019braid,wang2019learning,wang2020learning}, our model learns the graph connectivity adaptively. 
Following the standard training and validation split as described in \cite{ruan2019devil}, we evaluate the performance of our model on the validation set of the LIP dataset, and present the results in Table~\ref{table:LIP}.
Several comparison methods involve the attention module, including SS-NAN \cite{zhao2017self} and Attention \cite{chen2016attention}. 


The results in Table~\ref{table:LIP} show that our model outperforms the competitive method CE2P \cite{ruan2019devil} by 0.7\% in mean IoU, which validates that our model is generalizable to human parsing. 
Also, it is worth noting that our model achieves significant improvement in elaborate categories, such as socks, glove and j-suits.

Furthermore, we provide visualization examples in Fig.~\ref{fig:lip}. 
The visualized results demonstrate that our model leads to accurate prediction of human components, especially around the boundaries between components, which gives credits to the proposed component-level modeling.  
