\section{Experiments}\label{sec:exp}

\subsection{Implementation Details}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{dataset.pdf}
    \caption{
        Sample faces (a) and dataset statistics (b and c) on SCAF.
    }
    \label{fig:dataset}
    \vspace{-4mm}
\end{figure}

\noindent\textbf{Data collection.}\quad Current research on AIFR lacks a large-scale face dataset of millions of face images with a large age gap. To advance the development of AIFR and FAS, we create and release a new large cross-age face dataset~(LCAF) with 1.7M faces from cross-age celebrities. Specifically, we first use the public Azure Facial API~\cite{azure} to estimate the ages and genders of faces from the clean MS-Celeb-1M dataset provided by~\cite{deng2019arcface}. Then, we randomly select faces from a total of 5M faces to check whether the faces are correctly labeled, and try our best to manually correct them if any apparent mistakes; we mainly focus on the young ages under 20 that are often mislabeled by the API~\cite{azure}. Finally, a large-scale balanced age dataset is constructed by balancing both age and gender. We further build a subset of cross-age face dataset~(SCAF) containing about 0.5M images from 12K individuals following~\cite{wang2019decorrelated,wang2018orthogonal} for fair comparisons. We note that the training (LCAF) and testing data may have very little, or even no identities overlapping as \cite{deng2019arcface} already removed 500+ identities from their clean MS-Celeb-1M dataset by checking the similarity of faces between training and testing data. Following the mainstream literature~\cite{he2019s2gan,lihierarchical,li2019age,liu2019attribute,yang2018learning} with the time span of 10 years for each age group, the ages in this paper are divided into seven non-overlapping groups; \ie, 10-, 11-20, 21-30, 31-40, 41-50, 51-60, and 61+. Note that it is a much more challenging problem to perform FAS on seven groups than on four groups in previous works. Fig.~\ref{fig:dataset} presents example images and dataset statistics of SCAF. 

\noindent\textbf{Training details.}\quad We adopted ResNet-50 similar to~\cite{deng2019arcface} as the encoder . In the decoder , the identity age condition is bilinearly upsampled and processed with multi-level high-resolution features extracted from  by two ResBlocks~\cite{he2016deep}, each of which follows the instance normalization~\cite{ulyanov2016instance} and ReLU activation, the synthesized faces of size  were produced by one  convolutional layer. There are four ICBs in ICM. In the discriminator , six convolutional layers with strides of 2, 2, 2, 2, 1, 1 follow the spectral normalization~\cite{miyato2018spectral} and leaky ReLU activation except the last one, outputting  confidence map. The AIFR is optimized by SGD with an initial learning rate of  and momentum of 0.9 while the ICM, decoder , and  are trained by Adam with a fixed learning rate of ,  of 0.9 and  of 0.99 for the face age synthesis. We trained all models with a batch size of 512 on 8 NVIDIA GTX 2080Ti GPUs, 110K iterations for LCAF and 36K iterations for SCAF. The learning rate of AIFR was warmed up linearly from 0 to 0.1, reduced by a factor of 0.1, at iterations 5K, 70K, and 90K on LCAF and 1K, 20K, 23K on SCAF, respectively. The hyper-parameters in the loss functions were empirically set as follows:  was ,  was ,  was ,  was , and  was . The multiplicative margin and scale factor of CosFace loss~\cite{wang2018cosface} were set to  and , respectively. All images were aligned to , with five facial landmarks detected by MTCNN~\cite{zhang2016joint}, and linearly normalized to .

\subsection{Evaluation on AIFR}

\begin{table*}[t]
    \centering
    \scriptsize
    \begin{subtable}[b]{0.2\textwidth}
        \begin{tabular}{lc}
            \toprule
            Method                                      & Acc~(\%) \\
            \midrule
            RJIVE~\cite{sagonas2017recovering}          & 55.20      \\
            VGG Face~\cite{parkhi2015deep}              & 89.89      \\
            Center Loss~\cite{wen2016discriminative}    & 93.72      \\
            SphereFace~\cite{liu2017sphereface}         & 91.70      \\
            CosFace~\cite{wang2018cosface}              & 94.56      \\
            ArcFace~\cite{deng2019arcface}              & 95.15      \\
            DAAE~\cite{lihierarchical}                  & 95.30      \\
            \midrule
            \methodname(\textbf{ours})                               & \textbf{96.23}        \\
            \bottomrule
            \end{tabular}
            \caption{\bd{AgeDB-30}}\label{tab:agedb}
    \end{subtable}
    \hspace{0.05\textwidth}
    \begin{subtable}[b]{0.2\textwidth}
    \begin{tabular}{lc}
        \toprule
        Method        & Acc~(\%) \\
        \midrule
        HUMAN-Individual  & 82.32      \\
        HUMAN-Fusion      & 86.50      \\
        \midrule
        Center Loss~\cite{wen2016discriminative}   & 85.48      \\
        SphereFace~\cite{liu2017sphereface}        & 90.30      \\
        VGGFace2~\cite{cao2018vggface2}            & 90.57     \\
        ArcFace~\cite{deng2019arcface}             & 95.45     \\
        \midrule
        \methodname(\textbf{ours}) & \textbf{95.62}      \\
        \bottomrule
    \end{tabular}
    \caption{\bd{CALFW}}\label{tab:calfw}
    \end{subtable}
    \hspace{0.05\textwidth}
    \begin{subtable}[b]{0.2\textwidth}
    \begin{tabular}{lc}
        \toprule
        Method        & Acc~(\%) \\
        \midrule
        HFA~\cite{gong2013hidden}                 & 84.40      \\
        CARC~\cite{chen2015face}                  & 87.60      \\
        VGGFace~\cite{parkhi2015deep}             & 96.00      \\
        Center Loss~\cite{wen2016discriminative}  & 97.48     \\
        LF-CNN~\cite{wen2016latent}               & 98.50      \\
        Marginal Loss~\cite{deng2017marginal}     & 98.95      \\
        OE-CNN~\cite{wang2018orthogonal}          & 99.20      \\
        AIM~\cite{zhao2019look}                   & 99.38      \\
        DAL~\cite{wang2019decorrelated}           & 99.40      \\
        \midrule
        \methodname(\textbf{ours}) & \textbf{99.55}      \\
        \bottomrule
    \end{tabular}
    \caption{\bd{CACD-VS}}\label{tab:cacdvs}
    \end{subtable}
    \hspace{0.05\textwidth}
    \begin{subtable}[b]{0.2\textwidth}
    \begin{tabular}{lc}
        \toprule
        Method        & Rank-1~(\%) \\
        \midrule
        Park~\etal~\cite{park2010age}                   & 37.40      \\
        Li~\etal~\cite{li2011discriminative}            & 47.50      \\
        HFA~\cite{gong2013hidden}                       & 69.00      \\
        MEFA~\cite{gong2015maximum}                     & 76.20      \\
        CAN~\cite{xu2017age}                            & 86.50      \\
        LF-CNN~\cite{wen2016latent}                     & 88.10      \\
        AIM~\cite{zhao2019look}                         & 93.20      \\
        DAL~\cite{wang2019decorrelated}                 & 94.50      \\
        \midrule
        \methodname(\textbf{ours}) & \textbf{94.78}      \\
        \bottomrule
    \end{tabular}
    \caption{\bd{FG-NET~{(\footnotesize leave-one-out)}}}\label{tab:fgnet}
    \end{subtable}\\
    \vspace{4mm}
    \begin{subtable}[b]{0.2\textwidth}
    \begin{tabular}{lc}
        \toprule
        Method        & Rank-1~(\%) \\
        \midrule
        FUDAN-CS\_SDS~\cite{wang2017multi}          & 25.56      \\
        SphereFace~\cite{liu2017sphereface}            & 47.55      \\
        TNVP~\cite{Duong_2017_ICCV}           & 47.72      \\
        OE-CNN~\cite{wang2018orthogonal}           & 52.67      \\
        DAL~\cite{wang2019decorrelated}           & \textbf{57.92}      \\
        \midrule
        \methodname(\textbf{ours}) & 57.18      \\
        \bottomrule
    \end{tabular}
    \caption{\bd{FG-NET~(MF1)}}\label{tab:fgnet_mf1}
    \end{subtable}
    \hspace{0.04\textwidth}
    \begin{subtable}[b]{0.405\textwidth}
    \begin{tabular}{lccccc}
        \toprule
        Model & AgeDB-30 & CALFW & CACD-VS & FG-NET \\ 
        \midrule
        Baseline & 95.52 & 94.27 & 99.12 & 93.64 \\
        \quad+Age & 95.32 & 94.35 & 99.15 & 93.88 \\ 
        \quad+AFD~(CA) & 95.63 & 94.50 & 99.32 & 94.05 \\ 
        \quad+AFD~(SA) & 95.85 & 94.43 & 99.25 & 94.38 \\ 
        \quad+AFD~(CBAM) & 96.08 & 94.32 & 99.18 & 94.36 \\ 
        \quad+AFD & 95.90 & 94.48 & 99.30 & 94.58 \\ 
        \midrule
        \methodname(\textbf{ours}) & \textbf{96.23} & \textbf{94.72} & \textbf{99.38} & \textbf{94.78}\\ 
        \bottomrule
    \end{tabular}
    \caption{\bd{Ablation Study}}\label{tab:ablation_study}
    \end{subtable}
    \hspace{0.04\textwidth}
    \begin{subtable}[b]{0.28\textwidth}
    \begin{tabular}{lcc}
        \toprule
        Method        & LFW   & MF1-Facescrub \\
        \midrule
        SphereFace~\cite{liu2017sphereface}    & 99.42 & 72.73         \\
        CosFace~\cite{wang2018cosface}         & 99.33 & 77.11         \\
        OE-CNN~\cite{wang2018orthogonal}       & 99.35 & N/A           \\
        DAL~\cite{wang2019decorrelated}        & 99.47 & \textbf{77.58}         \\
        \midrule
        \methodname(\textbf{ours}) & \textbf{99.52} & 77.06         \\ 
        \bottomrule
    \end{tabular}
    \caption{\bd{General Face Recognition}}\label{tab:gfr}
    \end{subtable}
\caption{
        Experimental results on several benchmark AIFR and GFR datasets with the best results in bold.
    We reported the verification rate~(\%) for AgeDB, CALFW, CACD-VS, and LFW, and the rank-1 identification rate~(\%) for FG-NET and MF1.
    }
    \label{tab:results}
\end{table*}

Next, we evaluate the \methodname on several benchmark cross-age datasets, including CACD-VS~\cite{chen2015face}, CALFW~\cite{zheng2017cross}, AgeDB~\cite{moschoglou2017agedb}, and FG-NET~\cite{fgnet}, to compare with the state-of-the-art methods. Note that MORPH is excluded since the version in~\cite{wang2019decorrelated,wang2018orthogonal,zhao2019look} is prepared for commercial use only.

\noindent\textbf{Result on AgeDB.}\quad AgeDB~\cite{moschoglou2017agedb} contains 16,488 face images of 568 distinct subjects with manually annotated age labels. It provides four protocols for age-invariant face verification protocols under the different age gaps of face pair; \ie 5, 10, 20, and 30 years. Similar to LFW~\cite{huang2008labeled}, this dataset is split into 10 folds for each protocol, where each fold consists of 300 intra-class and 300 inter-class pairs. We strictly follow the protocol of  years to perform the 10-fold cross-validation since the protocol of  years is the most challenging. We use the models trained on SCAF to evaluate the performance on AgeDB for fair comparisons. Table~\ref{tab:agedb} shows the verification accuracy of our models compared against the other state-of-the-art AIFR methods, demonstrating the superior performance of the proposed method.

\noindent\textbf{Result on CALFW.}\quad Cross-age labeled faces in the wild~(CALFW) dataset~\cite{zheng2017cross} is designed for unconstrained face verification with large age gaps, which contains 12,176 face images of 4,025 individuals collected using the same identities in LFW. Similarly, we follow the same protocol as the LFW, where each fold consists of 600 positive and negative pairs. We train the model on LCAF to evaluate our method on this dataset, and the results are shown in Table~\ref{tab:calfw}. Particularly, our method outperforms the recent state-of-the-art AIFR methods by a large margin, establishing a new state-of-the-art on the CALFW.

\noindent\textbf{Result on CACD-VS.}\quad As a public age dataset for AIFR, cross-age celebrity dataset~(CACD) contains 163,446 face images of 2,000 celebrities in the wild, with significant variations in age, illumination, pose, and so on. Since collected by search engine, CACD is noisy with mislabeled and duplicate images. Therefore, a carefully annotated version, CACD verification sub-set or CACD-VS, is constructed for fair comparisons, which also follows the protocol of LFW. Table~\ref{tab:cacdvs} presents the comparison of the proposed method with other state-of-the-arts on CACD-VS~\cite{chen2015face}. Our \methodname surpasses other state-of-the-arts by a large margin, introducing an improvement of 0.15 against the recent one.

\noindent\textbf{Result on FG-NET.}\quad FG-NET~\cite{fgnet} is the most popular and challenging age dataset for AIFR, which consists of 1,002 face images from 82 subjects collected from the wild with huge age variations ranging from child to elder. We strictly follow the evaluation pipeline in~\cite{wang2019decorrelated,wang2018orthogonal}. Specifically, the model is trained on SCAF and tested under the protocols of leave-one-out and MegaFace challenge 1~(MF1). In the leave-one-out protocol, faces are used to match the rest faces, repeating 1,002 times. Table~\ref{tab:fgnet} reports the rank-1 recognition rate. Our method outperforms prior work by a large margin. On the other hand, the MF1 contains additional 1M images as the distractors in the gallery set from 690K different individuals, where models are evaluated under the large and small training set protocols. The small protocol requires the training set less than 0.5M images. The small protocol is strictly followed to evaluate our trained model on FG-NET, and the experimental results are reported in Table~\ref{tab:fgnet_mf1}. Our method achieves competitive performance against other methods since the distractors in MF1 contains a large number of mislabeled probe and gallery face images.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.83\linewidth]{qualitative_results_opt.pdf}
    \caption{Qualitative results by applying our \methodname trained on SCAF dataset to three external datasets : a) LCAF excluding identities in SCAF; b) MORPH; and c) FG-NET. Red boxes indicate input faces.}
    \label{fig:qualitative_results}
\end{figure*}

\noindent\textbf{Ablation study.}\quad To investigate the efficacy of different modules in~\methodname, we perform ablation studies based on four benchmark datasets for AIFR by considering the following variants of our method:
1) Baseline: we remove all extra components but only the CosFace loss to train the face recognition model. 
2) +Age: this variant is jointly trained under the supervision of both CosFace and age estimation loss, similar to~\cite{wang2019decorrelated,zheng2017age}. 
3) +AFD~(CA), +AFD~(SA), +AFD~(CBAM), +AFD: these four variants utilize the proposed attention-based feature decomposition to highlight the age-related information at different level, by the different attention modules including CA~\cite{hu2018squeeze}, SA~\cite{woo2018cbam}, CBAM~\cite{woo2018cbam}, and the proposed one. 
4) Ours: our proposed~\methodname is trained simultaneously by the AFD and cross-age domain adaption loss. The experimental results are reported in Table~\ref{tab:ablation_study}. 
We note that the verification rate of the baseline model on AgeDB-30 is higher than those of ArcFace and DAAE since the training data (\ie, SCAF) is age-balanced, which is an important feature of our collected dataset. Even though the age estimation task is performed in the face recognition model, it cannot introduce any improvement of AIFR compared to the baseline model. On the other hand, AFD achieves remarkable performance improvement on all cross-age datasets. Nevertheless, as the AFD highlights the age-related information at both channel and spatial levels in parallel, our method achieves consistent performance improvements, demonstrating its effectiveness compared to the single level~(CA and SA) or sequential level~(CBAM). Furthermore, the use of cross-age domain adversarial training leads to an additional performance improvement.


\begin{figure}[ht!]
    \centering
    \includegraphics[width=.8\linewidth]{qualitative_comparison_opt.pdf}
    \caption{Qualitative comparisons with prior work on FG-NET~(top 3 rows) and MORPH~(bottom 3 rows).}
    \label{fig:qualitative_comparison}
\end{figure}

\subsection{Evaluation on GFR}

To validate the generalization ability of our \methodname for the general face recognition~(GFR), we further conduct experiments on the LFW~\cite{huang2008labeled} and MegaFace Challenge 1 Facescrub~(MF1-Facescrub)~\cite{kemelmacher2016megaface} datasets. LFW~\cite{huang2008labeled} is the most popular public benchmark dataset for GFR, which contains 13,233 face images from 5,749 subjects. The MF1-Facescrub~\cite{kemelmacher2016megaface} uses the Facescrub dataset~\cite{ng2014data} of 106,863 face images from 530 celebrities as a probe set. The most challenging problem of MF1 is that it uses an additional 1M face images in the gallery set to distract the face matching. That is, the results on MF1 are not as reliable as LFW due to the extremely noisy distractors in MF1. We strictly follow the same procedure as~\cite{wang2019decorrelated,wang2018orthogonal}, where the training dataset contains 0.5M images~(SCAF). Table~\ref{tab:gfr} reports the verification rate on LFW and the rank-1 identification rate in MF1-Facescrub against the state-of-the-art GFR methods. Our method achieves competitive performance on both datasets, demonstrating the strong generalization ability of our~\methodname. We highlight that our \methodname can provide photo-realistic synthesized faces to improve model interpretability, which is absent in other methods~\cite{wang2019decorrelated,wang2018orthogonal}.

\subsection{Evaluation on FAS}\label{sec:qualitative_comparison}

We further evaluate the model trained on SCAF for FAS. Fig.~\ref{fig:qualitative_results} presents some sample results on the external datasets including LCAF, MORPH, and FG-NET. Our method is able to simulate the face age synthesis process between age groups with high visual fidelity. Although there exist variations in terms of race, gender, expression, and occlusion, the synthesized faces are still photo-realistic, with natural details in the skin, muscles, and wrinkles while consistently preserving identities, confirming the generalization ability of the proposed method.

We conduct qualitative comparisons with prior work including CAAE~\cite{zhang2017age} and AIM~\cite{zhao2019look} on MORPH and FG-NET. Fig.~\ref{fig:qualitative_comparison} shows that both CAAE and AIM produce oversmoothed faces due to their image reconstruction while our \methodname uses the identity age condition to synthesize faces based on multi-level features extracted from the encoder. Note that the results of competitors are directly referred from their own papers for a fair comparison, which is widely adopted in the FAS literature such as~\cite{he2019s2gan,lihierarchical,li2019age,liu2019attribute,yang2018learning} to avoid any bias or error caused by self-implementation. 
See Appendix for quantitative comparisons with CAAE~\cite{zhang2017age}, IPCGAN~\cite{wang2018face}, and an ablation study of identity conditional module in terms of two evaluation criteria---age accuracy and identity preservation.