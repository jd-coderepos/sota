In this Appendix, we present the following items:
\begin{itemize}
\item[\textit{(i)}] Additional qualitative examples of zero-shot VideoQA predictions (Section~\ref{sec:addquali})
\item[\textit{(ii)}] A qualitative analysis of the \emph{frozen} self-attention patterns in \model{} (Section~\ref{sec:attention})
\item[\textit{(iii)}] Additional information about our experimental setup (Section~\ref{sec:adddetails}), including datasets (Section~\ref{sec:adddata}) and implementation details (Section~\ref{sec:addimplem})
\item[\textit{(iv)}] Additional experimental results (Section~\ref{sec:addexperiments}), including a comparison to BLIP~\cite{li2022blip} in their zero-shot VideoQA settings (Section~\ref{sec:blip}), results on zero-shot image-VQA (Section~\ref{sec:imagevqa}), detailed zero-shot VideoQA results segmented per question type (Section~\ref{sec:qtype}), zero-shot results with different random seeds (Section~\ref{sec:seed}), additional ablation studies in few-shot settings (Section~\ref{sec:addfewshot}), zero-shot settings (Sections~\ref{sec:multitoken} and~\ref{sec:addzs}) and fully-supervised settings (Section~\ref{sec:addablation})
\end{itemize}

\begin{figure*}[!t]
\centering
\begin{subfigure}{1.\linewidth}
\caption{\textbf{Zero-Shot open-ended VideoQA.}
The first row illustrates successful predictions on the iVQA dataset~\cite{yang2021just} (leftmost example) and the ActivityNet-QA dataset~\cite{yu2019activitynet} (three rightmost examples).
The second row illustrates incorrect predictions on the iVQA dataset.}
\label{fig:qualitativesup1}
\vspace{-0.2cm}
\includegraphics[width=1.\linewidth]{figures/qualitativesup1}
\end{subfigure}
\begin{subfigure}{1.\linewidth}
\caption{\textbf{Zero-shot video-conditioned fill-in-the-blank} successful predictions on the LSMDC-FiB dataset~\cite{maharaj2017dataset}.}
\label{fig:qualitativesup2}
\vspace{-0.2cm}
\includegraphics[width=1.\linewidth]{figures/qualitativesup2}
\end{subfigure}
\begin{subfigure}{1.\linewidth}
\vspace{+0.2cm}
\caption{\textbf{Zero-shot multiple-choice VideoQA.} The first and second rows illustrate successful predictions on the How2QA dataset~\cite{li2020hero} and the TVQA dataset~\cite{lei2018tvqa}, respectively.}
\label{fig:qualitativesup3}
\vspace{-0.2cm}
\includegraphics[width=1.\linewidth]{figures/qualitativesup3}
\end{subfigure}
\vspace{-0.4cm}
\caption{\small \textbf{Zero-Shot VideoQA.} Qualitative comparison between Just Ask~\cite{Yang2022LearningTA} (row 3 in Table~\ref{table:zeroshot}), our model (row 4 in Table~\ref{table:zeroshot}), its \textit{unfrozen} variant (row 2 in Table~\ref{table:parameters}) and its text-only variant (row 2 in Table~\ref{table:modalities}), for zero-shot VideoQA.
The last column of each row illustrates a single video example with two frames, while other columns illustrate each video example with one frame.
We show more examples on our webpage~\cite{frozenbilmwebpage}.}
\label{fig:qualitativesup}
\end{figure*}

\section{Qualitative examples for zero-shot VideoQA}\label{sec:addquali}
To complement the qualitative examples shown in Figure~\ref{fig:qualitative}, Figure~\ref{fig:qualitativesup} and the video \textit{video\_examples.mp4} illustrate additional qualitative results of zero-shot VideoQA for our \model{} model and compares them to Just Ask~\cite{Yang2022LearningTA}, as well as to variants of our approach that do not \textit{freeze} the language model (\textit{UnFrozenBiLM}) and use no visual modality (text-only), as evaluated in Section~\ref{sec:ablations}.
Consistently with the analysis done in Section~\ref{sec:zssota}, we observe that the \textit{unfrozen} variant can predict answers that lack text-only commonsense reasoning, \textit{e.g.}~in the first example of Figure~\ref{fig:qualitativesup2}, the word \textit{follow} is grammatically incorrect; in the second example of Figure~\ref{fig:qualitativesup2}, it is unlikely that a singer \textit{plays} a toad.
The text-only variant does have strong language understanding, but makes visually-unrelated predictions.
In contrast, consistently with our quantitative results (see Tables~\ref{table:parameters}, \ref{table:modalities} and~\ref{table:zeroshot}), our model \model{} is able to correctly answer various questions in the diverse VideoQA paradigms (open-ended VideoQA, video-conditioned fill-in-the-blank, multiple-choice VideoQA), showing both a strong textual commonsense reasoning and a complex multi-modal understanding. 

Our zero-shot model still underperforms compared to VideoQA-supervised models (see Table~\ref{table:fewshot}) and we analyze its failure cases in Figure~\ref{fig:qualitativesup1}.
Qualitatively, we find that the zero-shot model can fail on examples requiring complex temporal or spatial understanding \textit{e.g.}~in the third example of the second row, the model does not detect a toaster behind the woman; in the second example of the second row, it gets confused as the person browses through many different tabs from their phone.
It can also be semantically inaccurate, as in the first example of the second row, the model confuses a restaurant with a bakery; in the fourth example of the second row, it confuses a chicken with another kind of bird.

\begin{figure*}[t]
\centering
\includegraphics[width=1.\linewidth]{figures/attnsup}
\caption{\small \textbf{\model{} self-attention visualization for zero-shot VideoQA.} 
Visualization of the attention weights between the different visual tokens from the video prompt and the textual tokens from the text embedder, for the second example of the first row in Figure~\ref{fig:qualitativesup}. 
A column corresponds to the weights of the different visual and text tokens for the given token.
These attention weights are averaged across all 24 heads, and renormalized by the maximum weight for each token (\textit{i.e.}~each column) for the purpose of visualization.
Lighter colors correspond to higher attention
weights (see the colorbar on the right).
In the first layers (left), we observe that the multi-modal interactions mainly flow through the [CLS], [MASK] and [SEP] tokens, and that there is little interaction between the different visual tokens.
In the last layers (right), we observe that visual tokens attend to each other and the [MASK] token attends to the visual tokens, while the [CLS] and [SEP] tokens mainly attend to text tokens.
Note that the self-attention weights are \textit{frozen} after text-only pretraining.}
\label{fig:attnsup}
\end{figure*}

\section{Qualitative analysis of the \emph{frozen} self-attention patterns in \model{}}\label{sec:attention}
We show in Section~\ref{sec:ablations} that the visual modality is crucial for the zero-shot VideoQA performance.
Here we further analyze qualitatively \emph{how}, for zero-shot VideoQA, our model makes use of the visual modality through self-attention layers which are \textit{frozen} after text-only pretraining.
Figure~\ref{fig:attnsup} illustrates the self-attention patterns in \model{} for the second example in the first row of Figure~\ref{fig:qualitativesup}. 
Despite the freezing, we observe that these layers actually enable visual-linguistic interactions.
Indeed, in the first layer (Figure~\ref{fig:qualitativesup}, left), the [CLS], [MASK] and [SEP] tokens significantly attend to the visual tokens.
Moreover, we observe substantially different patterns in the last layer (Figure~\ref{fig:qualitativesup}, right): while the [MASK] token still attends to visual tokens, the different visual tokens at different timesteps attend between each other and the [CLS] and [SEP] tokens mainly attend to other text tokens.
Consistently with results
presented in Section~\ref{sec:ablations}, this qualitative analysis suggests that the \emph{frozen} self-attention layers in \model{} do enable visual-linguistic interactions.

\section{Experimental setup}\label{sec:adddetails}

In this section we first present additional information on the used datasets (Section \ref{sec:adddata}) and then describe implementation details (Section \ref{sec:addimplem}).

\subsection{Datasets}\label{sec:adddata}

In this section, we give further details about the downstream datasets we use.
Their licenses are mentioned in our code in the separate folder \textit{code}.

\noindent \textbf{LSMDC}-FiB~\cite{maharaj2017dataset} is an open-ended video-conditioned fill-in-the-blank task which consists in predicting masked words in sentences that describe short movie clips~\cite{rohrbach15dataset, rohrbach17movie}.
It contains 119K video clips and 349K sentences, split into 297K/22K/30K for training/validation/testing.

\noindent \textbf{iVQA}~\cite{yang2021just} is a recently introduced open-ended VideoQA dataset, focused on objects, scenes and people in instructional videos~\cite{miech19howto100m}.
It excludes non-visual questions, and contains 5 possible correct answers for each question for a detailed evaluation.
It contains 10K video clips and 10K questions, split into 6K/2K/2K for training/validation/testing.

\noindent \textbf{MSRVTT-QA}~\cite{xu2017video}, \textbf{MSVD-QA}~\cite{xu2017video} and \textbf{TGIF-FrameQA}~\cite{jang2017tgif} are popular open-ended VideoQA benchmarks automatically generated from video descriptions~\cite{chen2011collecting, tgif-cvpr2016, xu16msrvtt}.
Questions are of five types for MSRVTT-QA and MSVD-QA: what, who, how, when and where; and four types for TGIF-QA: object, number, color and location.
MSRVTT-QA contains 10K video clips and 243K question-answer pairs, split into 158K/12K/73K for training/validation/testing.
MSVD-QA contains 1.8K video clips and 51K question-answer pairs, split into 32K/6K/13K for training/validation/testing.
TGIF-QA contains 46K GIFs and 53K question-answer pairs, split into 39K/13K for training/testing.

\noindent \textbf{ActivityNet-QA}~\cite{yu2019activitynet} is an open-ended VideoQA dataset consisting of long videos~\cite{caba2015activitynet} (3 minutes long on average), and covering 9 question types (motion, spatial, temporal, yes-no, color, object, location, number and other).
It contains 5.8K videos and 58K question-answer pairs, split into 32K/18K/8K for training/validation/testing.

\noindent \textbf{How2QA}~\cite{li2020hero} is a multiple-choice VideoQA dataset focused on instructional videos~\cite{miech19howto100m}.
Each question is associated with one correct and three incorrect answers. 
It contains 28K video clips and 38K questions, split into 35K/3K for training/validation.

\noindent \textbf{TVQA}~\cite{lei2018tvqa} is a multiple-choice VideoQA dataset focused on popular TV shows.
Each question is associated with one correct and four incorrect answers. 
It contains 22K video clips and 153K questions, split into 122K/15K/15K for training/validation/testing.
The test set is hidden and only accessible a limited number of times via an online leaderboard.

\subsection{Implementation details}\label{sec:addimplem}

\noindent \textbf{Architecture hyperparameters.} 
We truncate text sequences up to $L=256$ tokens.
Video features are extracted by sampling $T=10$ frames, each resized at $224 \times 224$ pixels, from the video.
These frames are sampled at temporally equal distance, with a minimum distance of 1 second.
For videos shorter than $T$ seconds, we pad the video prompt up to $T$ tokens.
The dimension of the visual features from ViT-L/14~\cite{dosovitskiy2021an} is $D_f=768$.
The transformer encoder from DeBERTa-V2-XLarge~\cite{he2021deberta} has 24 layers, 24 attention heads, a hidden dimension of $D=1536$ and an intermediate dimension in the feed-forward layers of 6144.
For the adapters~\cite{houlsby2019parameter}, we use a bottleneck dimension of $D_h=\frac{D}{8}=192$.

\noindent \textbf{Training.} 
For all training experiments, we use the Adam optimizer~\cite{kingma15adam} with $\beta=(0.9, 0.95)$ and no weight decay.
We use Dropout~\cite{srivastava2014dropout} with probability $0.1$ in the adapters and in the transformer encoder.
When finetuning the language model weights, we divide the batch size by a factor 2 so to accommodate with the GPU memory constraints.

\noindent \textbf{Cross-modal training.}
To train on WebVid10M, we use a total batch size of 128 video-caption pairs split in 8 NVIDIA Tesla V100 GPUs.
We use a fixed learning rate of $3e^{-5}$ for the variant with adapters.
We find that the variant without adapters that freezes the language model weights prefers a higher learning rate of $3e^{-4}$, and that the variant \textit{UnfrozenBiLM} that finetunes the language model weights prefers a lower one of $1e^{-5}$.

\noindent \textbf{Downstream task finetuning.}
To finetune our model on downstream datasets, we use a total batch size of 32 video-question-answer triplets (respectively 32 video-sentence pairs) split in 4 NVIDIA Tesla V100 GPUs for open-ended VideoQA datasets (respectively video-conditioned fill-in-the-blank datasets) and 16 video-question pairs split in 8 NVIDIA Tesla V100 GPUs for multiple-choice VideoQA datasets.
We train for 20 epochs for all downstream datasets except LSMDC-FiB for which we find that training for 5 epochs leads to similar validation results.
We warm up the learning rate linearly for the first 10\% of iterations, followed by a linear decay of the learning rate (down to 0) for the remaining 90\%. 
On each dataset, we run a random search and select the learning rate based on the best validation results.
We search over 10 learning rates in the range [$1e^{-5}$, $1e^{-4}$] for variants that freeze the language model weights, and [$5e^{-6}$, $5e^{-5}$] for the variant \textit{UnfrozenBiLM} that finetunes the language model weights.

\noindent \textbf{Answer vocabulary for open-ended VideoQA.}
In the zero-shot setting, we use an answer vocabulary composed of the top $1,000$ answers in the corresponding training dataset, following~\cite{zellers2021merlot}. 
In the fully-supervised setting, we experiment both with the vocabulary composed of the top $1,000$ answers and the vocabulary composed of all answers appearing at least twice in the corresponding training dataset and choose the one leading to best validation results.
Following~\cite{zellers2021merlot}, questions with out-of-vocabulary answer are not used for finetuning, and are automatically considered as incorrect during evaluation.

\section{Experiments}\label{sec:addexperiments}

In this section, we complement the experiments presented in Section~\ref{sec:experiments}.
We first present a comparison with BLIP~\cite{li2022blip} in their zero-shot settings in Section~\ref{sec:blip}.
In Section~\ref{sec:qtype} we show detailed zero-shot VideoQA results segmented per question category and compare our method with Just Ask~\cite{yang2021just}.
Next we analyze the impact of the random seed used in the cross-modal training on the zero-shot VideoQA results in Section~\ref{sec:seed}.
We also show the importance of freezing the language model in few-shot settings in Section~\ref{sec:addfewshot}.
We present additional ablation studies in the zero-shot setting in Section~\ref{sec:addzs}.
Finally we show the benefit of cross-modal training and adapter training in fully-supervised settings in Section~\ref{sec:addablation}.

\begin{table}[t]
\begin{center}
\setlength\tabcolsep{1pt}
\resizebox{1.\linewidth}{!}{
\begin{tabular}{lll|ccccc}
Method & Pretraining Data & Finetuning Data & iVQA & MSRVTT-QA & MSVD-QA & ActivityNet-QA & TGIF-QA \\
\hline
BLIP~\cite{li2022blip} & 129M image-text pairs & VQA & --- & 19.2 & 35.2 & --- & --- \\
\model{} (no image-VQA training) & WebVid10M & $\emptyset$ & 26.8 & 16.7 & 33.8 & 25.9 & 41.9 \\ 
\model{} (no cross-modal training) & $\emptyset$ & VQA & 14.6 & 6.9 & 12.6 & 22.6 & 33.3 \\
\model{} (Ours) & WebVid10M & VQA & \textbf{34.6} & \textbf{22.2} & \textbf{39.0} & \textbf{33.1} & \textbf{43.4} \\
\end{tabular}}
\vspace{+0.2cm}
\caption{\small Results of our model after cross-modal training, finetuning on the open-ended image-VQA dataset~\cite{antol2015vqa} and directly evaluating on open-ended VideoQA without using any VideoQA supervision, as in BLIP~\cite{li2022blip}.}
\label{table:img2vid}
\end{center}
\end{table}

\begin{table*}[t]
\setlength\tabcolsep{10pt}
\begin{center}
\resizebox{1.\linewidth}{!}{	
\begin{tabular}{l|ccccccccc}
Method & Motion & Spatial & Temporal & Yes-No & Color & Object & Location & Number & Other \\ \hline
Just Ask~\cite{yang2021just} & 2.3 & 1.1 & 0.3 & 36.3 & 11.3 & 4.1 & 6.5 & 0.2 & 4.7 \\ \model{} & \textbf{12.7} & \textbf{6.8} & \textbf{1.6} & \textbf{53.2} & \textbf{16.5} & \textbf{17.9} & \textbf{18.1} & \textbf{26.2} & \textbf{25.8} \\ \end{tabular}
}
\caption{\small Zero-shot VideoQA results segmented per question type on the ActivityNet-QA dataset, compared with Just Ask~\cite{yang2021just}.}
\label{table:qtypeact}
\end{center}
\end{table*}

\begin{table*}[!t]
\begin{center}
\resizebox{1.\linewidth}{!}{	
\begin{tabular}{l|cccccc|cccccc}
Method & \multicolumn{6}{c}{MSRVTT-QA} & \multicolumn{6}{c}{MSVD-QA} 
\\ 
& What & Who & Number & Color & When & Where & What & Who & Number & Color & When & Where \\ \hline Just Ask~\cite{yang2021just} & 1.8 & 0.7 & \textbf{66.3} & 0.6 & 0.6 & 4.5 
& 7.8 & 1.7 & \textbf{74.3} & 18.8 & 3.5 & 0.0 \\ \model{} & \textbf{10.7} & \textbf{28.7} & 55.0 & \textbf{11.4} & \textbf{9.2} & \textbf{9.3} & \textbf{26.0} & \textbf{45.0} & 69.9 & \textbf{56.3} & \textbf{5.2} & \textbf{17.9} \\ \end{tabular}
}
\caption{\small Zero-shot VideoQA results segmented per question type on the MSRVTT-QA dataset (left) and the MSVD-QA dataset (right), compared with Just Ask~\cite{yang2021just}.}
\label{table:qtype}
\end{center}
\end{table*}

\subsection{Comparison with BLIP}\label{sec:blip}
In addition to the zero-shot results presented in Section~\ref{sec:zssota}, we here investigate a different but related \textit{zero-shot} setting defined in BLIP~\cite{li2022blip}, where a network trained on manually annotated image-VQA annotations is evaluated directly on open-ended VideoQA datasets. 
In detail, BLIP uses the open-ended image-VQA dataset~\cite{antol2015vqa} for finetuning after pretraining on 129M image-text pairs, 
including COCO~\cite{chen2015microsoft} and Visual Genome~\cite{visualgenome} which are manually annotated.
To adapt our model to this setting, we finetune our model \model{} pretrained on WebVid10M on the image-VQA dataset using the same procedure as for finetuning on VideoQA datasets (see Section~\ref{sec:downstream}), \textit{i.e.}~notably with a \emph{frozen} language model.
In particular, we finetune on VQA for 10 epochs with an initial learning rate of $1e^{-5}$ which is warmed up for the first 10\% iterations, and linearly decayed to 0 for the remaining 90\% iterations.
Table~\ref{table:img2vid} shows that the resulting model not only improves over our model without image-VQA finetuning (\textit{i.e.}~in zero-shot mode as defined in Section~\ref{sec:intro}) or our model trained on VQA only (\textit{i.e.}~without cross-modal training), but also substantially outperforms BLIP on both MSRVTT-QA and MSVD-QA.
These results further demonstrate the strong capabilities of \model{} in settings where no VideoQA annotation is available.

\subsection{Results on zero-shot image-VQA}\label{sec:imagevqa}
We next evaluate our pretrained model on the VQAv2~\cite{antol2015vqa} validation set in the zero-shot setting, \textit{i.e.}, without any supervision of visual questions and answers. Frozen~\cite{tsimpoukelli2021multimodal} achieves 29.5\% accuracy in this setting using an autoregressive language model. In comparison, our \model{} model is 7 times smaller than Frozen and achieves 45.0\% accuracy. We conclude that our model can perform competitively on the image-VQA tasks despite being tailored for videos.

\subsection{Detailed zero-shot VideoQA results segmented per question category}\label{sec:qtype}
We complement the comparison to the state of the art for zero-shot VideoQA given in Section~\ref{sec:zssota} with results segmented per question type for ActivityNet-QA in Table~\ref{table:qtypeact}, and for MSRVTT-QA and MSVD-QA in Table~\ref{table:qtype}.
Compared to Just Ask~\cite{yang2021just}, we observe large and consistent improvements over all question categories, except for the \textit{number} category on MSRVTT-QA and MSVD-QA.
These results show that our approach is efficient in the diverse question categories of zero-shot VideoQA.

\begin{table}[t]
\begin{center}
\setlength\tabcolsep{1pt}
\resizebox{1.\linewidth}{!}{
\begin{tabular}{ll|c|ccccc|cc}
\multirow{2}{*}{Method} & \multirow{2}{*}{Training Data} & Fill-in-the-blank &
\multicolumn{5}{c|}{Open-ended} &
\multicolumn{2}{c}{Multiple-choice} \\ 
& & LSMDC &
iVQA & 
MSRVTT-QA & 
MSVD-QA & 
ActivityNet-QA & 
TGIF-QA &
How2QA & 
TVQA \\ 
\hline
Random & --- & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 & 25 & 20 \\
CLIP ViT-L/14~\cite{radford2021learning} & 400M image-texts & 1.2 & 9.2 & 2.1 & 7.2 & 1.2 & \underline{3.6} & 47.7 & \underline{26.3} \\
Just Ask~\cite{Yang2022LearningTA} & \makecell[l]{HowToVQA69M + \\ WebVidVQA3M} & --- & \underline{13.3} & 5.6 & \underline{13.5} & \underline{12.3} & --- & \underline{53.1} & --- \\
Reserve~\cite{zellers2022merlot} & YT-Temporal-1B & \underline{31.0} & --- & \underline{5.8} & --- & --- & --- & --- & --- \\
$\model{}$ (Ours) & WebVid10M & \textbf{51.5$\pm$0.1} & \textbf{28.3$\pm$0.9} & \textbf{14.4$\pm$1.4} & \textbf{30.0$\pm$2.2} & \textbf{25.4$\pm$0.7} & \textbf{39.7$\pm$2.1} & \textbf{57.9$\pm$0.6} & \textbf{57.9$\pm$1.2} \\ \end{tabular}}
\vspace{+0.2cm}
\caption{\small Comparison with the state of the art for zero-shot VideoQA, reporting mean and standard deviation over 5 cross-modal training runs with different random seeds.
Results on TVQA are reported on the validation set given that the hidden test set can only be accessed a limited number of times.}
\label{table:variance}
\end{center}
\end{table} 

\begin{table}[!t]
\begin{center}
\setlength\tabcolsep{1pt}
\resizebox{1.\linewidth}{!}{
\begin{tabular}{lcc|c|ccccc|cc}
& Variant & Supervision &
Fill-in-the-blank &
\multicolumn{5}{c|}{Open-ended} &
\multicolumn{2}{c}{Multiple-choice} \\ 
& & &
LSMDC &
iVQA &
MSRVTT-QA & 
MSVD-QA & 
ActivityNet-QA & 
TGIF-QA &
How2QA & 
TVQA
\\ 
\hline
1. & \textit{UnFrozenBiLM} & 0\% (zero-shot) & 37.1 & 21.0 & \textbf{17.6} & 31.9 & 20.7 & 30.7 & 45.7 & 45.6 \\
2. & \model{} & 0\% (zero-shot) & \textbf{51.5} & \textbf{26.8} & 16.7 & \textbf{33.8} & \textbf{25.9} & \textbf{41.9} & \textbf{58.4} & \textbf{59.2} \\
\hline
3. & \textit{UnFrozenBiLM} & 1\% (few-shot) & 46.2 & 23.5 & 33.4 & 43.7 & 31.6 & 51.7 & 68.0 & 68.6 \\ 4. & \model{} & 1\% (few-shot) & \textbf{56.9} & \textbf{31.1} & \textbf{36.0} & \textbf{46.5} & \textbf{33.2} & \textbf{55.1} & \textbf{71.7} & \textbf{71.8} \\ \hline
5. & \textit{UnFrozenBiLM} & 10\% (few-shot) & 52.6 & 29.5 & 38.9 & 49.8 & 36.5 & 57.8 & 73.2 & 74.8 \\ 6. & \model{} & 10\% (few-shot) & \textbf{59.9} & \textbf{35.3} & \textbf{41.7} & \textbf{51.0} & \textbf{37.4} & \textbf{61.2} & \textbf{75.8} & \textbf{77.3} \\ \hline
7. & \textit{UnFrozenBiLM} & 100\% (fully-supervised) & 58.9
& 37.7 & 45.0 & 53.9 & \textbf{43.2} & 66.9 & \textbf{87.5} & 79.1 \\ 
8. & \model{} & 100\% (fully-supervised) & \textbf{63.5} 
& \textbf{39.6} & \textbf{47.0} & \textbf{54.8} & \textbf{43.2} & \textbf{68.6} & 86.7 & \textbf{82.4} \\
\end{tabular}}
\vspace{+0.2cm}
\caption{\small Few-shot results, by finetuning $\model{}$ using a small fraction of the downstream training dataset, compared with the variant \textit{UnFrozenBiLM} which does not freeze the language model weights.
Results on TVQA are reported on the validation set given that the hidden test set can only be accessed a limited number of times.}
\label{table:addfewshot}
\end{center}
\end{table}

\subsection{Impact of the random seed on zero-shot VideoQA}\label{sec:seed}
To verify the robustness of our approach with respect to the random seed, we run cross-modal training for \model{} with 5 different random seeds.
We report the mean and standard deviation of zero-shot accuracy in Table~\ref{table:variance}, compared with state-of-the-art approaches that only report their results based on a single run.
We observe that the random seed does not affect the comparison to prior work done in Section~\ref{sec:zssota} in the main paper, as our model improves over previous work for zero-shot VideoQA~\cite{radford2021learning, Yang2022LearningTA, zellers2022merlot} by significant margins.

\subsection{Freezing the language model is also beneficial in few-shot settings}\label{sec:addfewshot}
Sections~\ref{sec:ablations} and~\ref{sec:sota} demonstrate that freezing the language model combined with training adapters outperforms finetuning the language model in the zero-shot and fully-supervised settings.
In Table~\ref{table:addfewshot}, we further show that freezing the language model combined with training adapters outperforms finetuning the language model in the few-shot setting as defined in Section~\ref{sec:sota} (compare rows 3 and 4, or rows 5 and 6).
Interestingly, the difference is larger when using 1\% of the downstream training dataset (rows 3 and 4) compared to using 10\% (rows 5 and 6) or 100\% (rows 7 and 8).
These results demonstrate that our approach is particularly efficient in settings where VideoQA annotations are scarce. 

\begin{table}[t]
\begin{center}
\setlength\tabcolsep{6pt}
\resizebox{1.\linewidth}{!}{
\begin{tabular}{ll|c|ccccc}
& Inference Strategy &
Fill-in-the-blank &
\multicolumn{5}{c}{Open-ended} \\ 
& & LSMDC & iVQA & MSRVTT-QA & MSVD-QA & ActivityNet-QA & TGIF-QA \\
\hline
1. & Average token embeddings & \textbf{51.5} & 26.8 & 16.7 & 33.8 & 25.9 & 41.9 \\ 
2. & Multiple mask tokens & 51.0 & \textbf{27.0} & \textbf{17.1} & \textbf{34.4} & \textbf{26.1} & \textbf{42.0} \\
\end{tabular}}
\vspace{+0.2cm}
\caption{\small Impact of the inference strategy on the zero-shot open-ended VideoQA performance.}
\label{table:multitoken}
\end{center}
\end{table}

\subsection{Ablation of the multi-token inference strategy}\label{sec:multitoken}
For multi-token answers in the open ended VideoQA setting, our \model{} simply averages the weights of different answer tokens.
However, such simple scheme does not preserve the semantic structure of the answer.
Hence we here investigate and compare another possible inference strategy in the zero-shot setting and discuss potential sources of improvement.
We take inspiration from \cite{jiang2020x} and performs zero-shot VideoQA inference by using multiple mask tokens decoded in parallel. 
Then, for each video-question pair, we do one forward pass through the model per possible number of mask tokens (typically, 1 to 5) in order to score all possible answers in vocabulary $\mathcal{A}$. 
The score of a given answer is then obtained by multiplying the probability of its individual tokens, possibly normalized by its number of tokens. 
As shown in Table~\ref{table:multitoken}, we observe that such a decoding strategy (row 2) does not significantly improve the accuracy of our model over the one used in \model{} (row 1). 
We hypothesize that this is due to the fact that the current open-ended VideoQA datasets~\cite{jang2017tgif, xu2017video, yang2021just, yu2019activitynet} contain a great majority of short answers, e.g. 99\% of the answers in the MSRVTT-QA test set are one-token long with our tokenizer~\cite{kudo2018sentencepiece}. 
Additionally, a possible solution to further improve the decoding in this alternative scheme is to increase the length of the masked spans at pretraining, as in \cite{joshi2020spanbert}. 
\cite{salazar2019masked} provides another potential solution to score multi-token answers in our framework, by masking tokens one by one and computing pseudo-likelihood scores.

\begin{table}[t]
\begin{center}
\setlength\tabcolsep{4pt}
\resizebox{1.\linewidth}{!}{
\begin{tabular}{lccc|c|ccccc|cc}
& T & $D_h$ & Visual &
Fill-in-the-blank &
\multicolumn{5}{c|}{Open-ended} &
\multicolumn{2}{c}{Multiple-choice} \\ 
& & & Backbone &
LSMDC &
iVQA &
MSRVTT-QA & 
MSVD-QA & 
ActivityNet-QA & 
TGIF-QA &
How2QA & 
TVQA
\\ 
\hline
1. & 1 & 192 & ViT-L/14 (CLIP) & 50.4 & 24.8 & 12.4 & 28.3 & 24.9 & 41.5 & 54.3 & 54.6 \\
2. & 10 & 96 & ViT-L/14 (CLIP) & \textbf{52.4} & \textbf{28.6} & 13.7 & 29.0 & 25.1 & \textbf{42.3} & \textbf{59.3} & 58.0 \\
3. & 10 & 384 & ViT-L/14 (CLIP) & 51.4 & 27.5 & 15.6 & 31.2 & 23.9 & 41.8 & 58.0 & 57.8 \\
4. & 10 & 192 & ViT-B/16 (ImageNet) & 49.4 & 23.8 & 13.3 & 25.7 & 25.1 & 36.8 & 56.5 & 57.2 \\
5. & 10 & 192 & ViT-B/16 (CLIP) & 50.8 & 25.5 & 14.6 & 30.3 & 25.6 & 41.0 & 57.6 & 58.2 \\
6. & 10 & 192 & ViT-L/14 (CLIP) & 51.5 & 26.8 & \textbf{16.7} & \textbf{33.8} & \textbf{25.9} & 41.9 & 58.4 & \textbf{59.2} \\
\end{tabular}}
\vspace{+0.2cm}
\caption{\small Impact of the number of frames $T$ used by the model, the hidden dimension $D_h$ in the adapters and the visual backbone on the zero-shot VideoQA results.
All models are trained on WebVid10M and use multi-modal inputs (video, speech and question) at inference.} 
\label{table:addzsablation}
\end{center}
\end{table}

\begin{table}[t]
\begin{center}
\setlength\tabcolsep{1pt}
\resizebox{1.\linewidth}{!}{
\begin{tabular}{ll|ccccc}
& Template & iVQA & MSRVTT-QA & MSVD-QA & ActivityNet-QA & TGIF-QA \\
\hline
1. & \texttt{\color{greencode}``[CLS] Question: <Question>? Answer: [MASK]. Subtitles: <Subtitles> [SEP]``} & 26.8 & \textbf{16.7} & \textbf{33.8} & \textbf{25.9} & \textbf{41.9} \\ 
2. & \texttt{\color{greencode}``[CLS] Q: <Question>? A: [MASK]. S: <Subtitles> [SEP]``} & \textbf{27.4} & 16.2 & 32.5 & 25.5 & \textbf{41.9} \\
3. & \texttt{\color{greencode}``[CLS] <Question>? [MASK]. <Subtitles> [SEP]``} & 23.1 & 13.6 & 28.0 & 21.6 & 25.2 \\
\end{tabular}}
\vspace{+0.2cm}
\caption{\small Impact of the prompt on the zero-shot open-ended VideoQA performance.}
\label{table:promptoe}
\end{center}
\end{table}

\begin{table}[t]
\begin{center}
\setlength\tabcolsep{1pt}
\resizebox{1.\linewidth}{!}{
\begin{tabular}{ll|cc}
& Template & How2QA & TVQA \\
\hline
1. & \texttt{\color{greencode}``[CLS] Question: <Question>? Is it ’’<Answer Candidate>”? [MASK]. Subtitles: <Subtitles> [SEP]``} & \textbf{58.4} & \textbf{59.7} \\ 
2. & \texttt{\color{greencode}``[CLS] Q: <Question>? Is it ’’<Answer Candidate>”? [MASK]. S: <Subtitles> [SEP]``} & 57.7 & 58.2 \\
3. & \texttt{\color{greencode}``[CLS] <Question>? <Answer Candidate>? [MASK]. <Subtitles> [SEP]``} & 47.6 & 55.0 \\
\end{tabular}}
\vspace{+0.2cm}
\caption{\small Impact of the prompt on the zero-shot multiple-choice VideoQA performance.}
\label{table:promptmc}
\end{center}
\end{table}

\subsection{Additional ablation studies in the zero-shot setting}\label{sec:addzs}
We here complement zero-shot ablation studies reported in Section~\ref{sec:ablations}. 
We analyze the impact of the number of frames $T$ used by the model, the hidden dimension in the adapters $D_h$ and the size and pretraining of the visual backbone in Table~\ref{table:addzsablation}.
All models use the same setting as described in Section~\ref{sec:ablations} and detailed in Section~\ref{sec:adddetails}.
We first observe that using 10 frames significantly improves over using a single frame (compare rows 1 and 5).
Next we note that using a hidden dimension of $96$ or $384$ in the adapters instead of $192$ does not change the results significantly (see rows 2, 3 and 6).
Moreover, we find that scaling up the size of the visual backbone is beneficial, as using ViT-L/14 instead of ViT-B/16, both being trained on CLIP~\cite{radford2021learning}, slightly improves the results (compare rows 4 and 6).
Furthermore, we observe that the pretraining of the visual backbone is crucial, as using ViT-B/16 pretrained on 400M image-text pairs from CLIP significantly improves over using ViT-B/16 pretrained on ImageNet-21K, \textit{i.e.}~22M image-label pairs (compare rows 4 and 5).

Finally, we ablate the importance of the prompt design on the zero-shot VideoQA performance.
We report results with alternative prompts in Tables~\ref{table:promptoe} and~\ref{table:promptmc}.
We find that replacing the words “Question”, “Answer” and “Subtitles” by “Q”, “A” and “S”, respectively, in the templates described in Section~\ref{sec:downstream} does not impact the zero-shot VideoQA accuracy (compare rows 2 and 1 in Tables~\ref{table:promptoe} and~\ref{table:promptmc}).
However, completely removing “Question”, “Answer”, “Subtitles” and “is it” in the templates results in a significant drop of performance (compare rows 3 and 1 in Tables~\ref{table:promptoe} and~\ref{table:promptmc}). 
We conclude that it is important to have tokens that link the different textual inputs.

\begin{table}[!t]
\begin{center}
\setlength\tabcolsep{1pt}
\resizebox{1.\linewidth}{!}{
\begin{tabular}{lcccl|c|ccccc|cc} & Cross-modal & 
Frozen 
& \multirow{2}{*}{Adapters}
& \# Trained & Fill-in-the-blank &
\multicolumn{5}{c|}{Open-ended} &
\multicolumn{2}{c}{Multiple-choice} \\ & Training & LM & & Params & LSMDC &
iVQA & 
MSRVTT-QA & 
MSVD-QA & 
ActivityNet-QA & 
TGIF-QA &
How2QA & 
TVQA \\ 
\hline
1. & \cmark & \xmark & \xmark & 890M & 58.9 & 37.7 & 45.0 & 53.9 & \textbf{43.2} & 66.9 & \textbf{87.5} & 79.1 \\ 2. & \cmark & \cmark & \xmark & \textbf{1M} & 60.4
& 38.2 & 43.2 & 51.7 & 38.3 & 66.5 & 79.3 & 78.2 \\ 3. & \xmark & \cmark & \cmark & 30M & 57.1
& 34.3 & 46.2 & 51.9 & 41.8 & 67.4 & 75.8 & 70.8 \\ 4. & \cmark & \cmark & \cmark & 30M & \textbf{63.5} 
& \textbf{39.6} & \textbf{47.0} & \textbf{54.8} & \textbf{43.2} & \textbf{68.6} & 86.7 & \textbf{82.4} \\
\end{tabular}} \vspace{+0.2cm}
\caption{\small Importance of cross-modal training and training various parameters for fully-supervised VideoQA.
All models are finetuned on downstream VideoQA datasets, and use multi-modal inputs (video, speech and question) at inference.}
\label{table:addsupervised}
\end{center}
\end{table}

\subsection{Cross-modal training and adapters are crucial for fully-supervised performance}\label{sec:addablation}
We have examined the impact of cross-modal training and training various parameters of our architecture on the zero-shot VideoQA performance in Section~\ref{sec:ablations}.
In Table~\ref{table:addsupervised}, we complement these ablation studies by analyzing the importance of cross-modal training and training various parameters for the fully-supervised VideoQA performance.
For this, we train on downstream datasets a variant with no adapters, and a variant without cross-modal training, following the same procedure as explained in Section~\ref{sec:downstream} and detailed in Section~\ref{sec:adddetails}.
We find that cross-modal training is significantly beneficial for the fully-supervised setting (compare rows 3 and 4).
Similar to conclusions made in Section~\ref{sec:sota}, training adapters while freezing the language model outperforms finetuning the language model in fully-supervised settings (see rows 1 and 4).
Finally, we note that training adapters has a considerable importance on the performance in fully-supervised settings (compare rows 2 and 4).
These results further demonstrate the strength of our approach in the fully-supervised setup.


