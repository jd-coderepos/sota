\def\year{2022}\relax
\documentclass[letterpaper]{article} \usepackage{aaai22}  \usepackage{times}  \usepackage{helvet}  \usepackage{courier}  \usepackage[hyphens]{url}  \usepackage{graphicx} \urlstyle{rm} \def\UrlFont{\rm}  \usepackage{natbib}  \usepackage{caption} \DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} \frenchspacing  \setlength{\pdfpagewidth}{8.5in}  \setlength{\pdfpageheight}{11in}  \usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{marvosym}
\definecolor{ao(english)}{rgb}{0.0, 0.5, 0.0}
\newcommand{\tableSUPERVISION}{
    \begin{table}[]
        \resizebox{1.0\linewidth}{!}{
            \begin{tabular}{c|l|lllll}
            \toprule
            \multirow{2}{*}{Loss}                & \multirow{2}{*}{Modality} & \multicolumn{5}{c}{IoU with GT (moment or its matched sent.)}\\ \cline{3-7} 
                                                 &                           & 1    & (0.5, 1) & (0, 0.5) & 0    & {\bf other video} \\ \midrule
            \multirow{2}{*}{Mutual Matching} & vid.                      & pos. & -        & neg.     & neg. & neg.        \\
                                                 & {\bf sent.}               & pos. & -        & neg.     & neg. & neg.        \\
                                                 \midrule
            \multicolumn{1}{l|}{IoU Regression}  & vid.                      & max  & high     & low      & min  & -           \\ \bottomrule
            \end{tabular}
        }
        \caption{Supervision signals of our MMN. We categorize moments/sentences as pos./neg. samples according to IoU of this moment (or this sentence's corresponding moment) with the GT moment. We introduce two novel types of supervisions: 1) negative sentence samples in mutual matching scheme (row 2), and 2) inter-video negatives (column 7). }
        \label{tab:supervision}
    \end{table}
}


\newcommand{\tablePairDiscrimination}{
    \begin{table}[t]
        \centering \resizebox{1.0\linewidth}{!}{\scalebox{1.0}{
        \begin{tabular}{c|cc|cc|cc|cc} \toprule
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}BCE\end{tabular}}  &\multicolumn{2}{c}{intra-video}& \multicolumn{2}{c}{inter-video}&   \multicolumn{2}{c}{R@1  }& \multicolumn{2}{c}{R@5  } \\ 
            &vid. & sent. &vid. & sent. &   IoU0.5 & IoU0.7 & IoU0.5 & IoU0.7 \\ 
            \midrule
            \checkmark&& & & &  &  &  &     \\
            
            \checkmark&\checkmark& & &  &  &  &  &    \\ 
            \checkmark&\checkmark& \checkmark& &  &  &  &  &    \\ 
            \checkmark&& &\checkmark & \checkmark&  &  &  &     \\\hline
            \checkmark&\checkmark&\checkmark & \checkmark&  &  &  &  &    \\
            \checkmark&\checkmark&\checkmark & &\checkmark&    &  &  &    \\\hline
            &\checkmark&\checkmark & \checkmark& \checkmark&  &  &  &     \\
            \checkmark&\checkmark&\checkmark & \checkmark&\checkmark&   &  &  &    \\
            \midrule
        \end{tabular}
            }
        }
        
        \resizebox{1.0\linewidth}{!}{\scalebox{1.0}{
        \begin{tabular}{c|cc|cc|ccc|ccc} \toprule
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}BCE\end{tabular}} &\multicolumn{2}{c}{intra-video}& \multicolumn{2}{c}{inter-video}&   \multicolumn{3}{c}{R@1  }& \multicolumn{3}{c}{R@5  } \\ 
            &vid. & sent. &vid. & sent. &  IoU0.3 & IoU0.5 & IoU0.7 & IoU0.3 &IoU0.5 & IoU0.7 \\ 
            \midrule
            \checkmark&& & & &  &  &  &    &  & \\
            \checkmark&\checkmark& & &  &  &  &  &  & & \\ 
            \checkmark&\checkmark& \checkmark& &  &  &  &  &    & & \\
            \checkmark&& &\checkmark& \checkmark  & &  &  &    &    &   \\\hline
            \checkmark&\checkmark&\checkmark & \checkmark&  &  &  &  &    &  &\\
            \checkmark&\checkmark&\checkmark & &\checkmark&   &  &  &  & & \\\hline
            &\checkmark& \checkmark&\checkmark& \checkmark  & &  &  &    &    &   \\
            \checkmark&\checkmark&\checkmark & \checkmark&\checkmark&   &  &  &   &   &   \\
            \bottomrule
        \end{tabular}
        }
    }
    \caption{Ablation on negative samples in our mutual matching scheme. (top) Charades-STA; (bottom) ActivityNet Captions. Intra/inter-video means we use video moments (vid.) or sentences (sent.) inside or cross videos as negatives.}
    \label{tab:pd}
    \end{table}
}
\newcommand{\tableNUMBER}{
\begin{table}[t]
\hspace{-0.1cm}
    \centering \resizebox{1.05\linewidth}{!}{
        \scalebox{1.0}{
            \begin{tabular}{l|ccc|ccc} \toprule
\multirow{2}{*}{{ Method}}&   \multicolumn{3}{c|}{R@1  }& \multicolumn{3}{c}{R@5  } \\ 
            &   IoU0.3 &IoU0.5 & IoU0.7 & IoU0.3 &IoU0.5 & IoU0.7 \\ 
            \midrule
            Only Intra& &  &  &    & & \\
            \#neg. = Intra &  &  &  &    &    &   \\
            Intra + Inter  &  &  &  &    &    &   \\
            
            \bottomrule
            \end{tabular}
        }
    }
\caption{Ablation on number of negatives (ActivityNet).}
\label{tab:number}
\end{table}
}

\newcommand{\tableAGGREGATION}{
    \begin{table}[t]
        \centering \resizebox{0.8\linewidth}{!}{\scalebox{1.0}{
        \begin{tabular}{l|cc|cc} \toprule
\multirow{2}{*}{Charades-STA}&   \multicolumn{2}{c|}{R@1  }& \multicolumn{2}{c}{R@5  } \\ 
         &   IoU0.5 & IoU0.7 & IoU0.5 & IoU0.7 \\ 
        \midrule
        Class Embedding `CLS' &  &  &  &    \\
        Average Pooling &  &  &  &    \\
        \bottomrule
        \toprule
         &  &  &  &    \\
         &  &  &  &    \\
         &  &  &  &    \\
        \midrule
         & &  &  &    \\
         &  &  &  &    \\
         &  &  &  &    \\
        \midrule
         &  &  &  &    \\
         &  &  &  &    \\
        \bottomrule
        \end{tabular}
        }
    }
    \caption{Ablation on (top) text feature aggregations and (bottom) hyper-parameters in the loss function.}
    \label{tab:aggregation}
    \end{table}
}

\newcommand{\tableHYPER}{
    \begin{table}[t]
        \centering \resizebox{0.7\linewidth}{!}{\scalebox{1.0}{
        \begin{tabular}{l|cc|cc} \toprule
\multirow{2}{*}{Hyper-parameters}&   \multicolumn{2}{c|}{R@1  }& \multicolumn{2}{c}{R@5  } \\ 
         &   IoU0.5 & IoU0.7 & IoU0.5 & IoU0.7 \\ 
        \midrule
         &  &  &  &    \\
         &  &  &  &    \\
         &  &  &  &    \\
        \midrule
        \midrule
         & &  &  &    \\
         &  &  &  &    \\
         &  &  &  &    \\
        \midrule
        \midrule
         &  &  &  &    \\
         &  &  &  &    \\
        \bottomrule
        \end{tabular}
        }
    }
    \caption{Ablation on hyper-parameters (Charades-STA).}
    \label{tab:hyper}
    \end{table}
}



\newcommand{\tableANET}{
    \begin{table}[t]
        \centering \resizebox{1\linewidth}{!}{\scalebox{1.0}{
        \begin{tabular}{l|ccc|ccc} \toprule
\multirow{ 2}{*}{Method}&   \multicolumn{3}{c|}{R@1  }& \multicolumn{3}{c}{R@5  } \\ 
        &   IoU0.3 &IoU0.5 & IoU0.7 & IoU0.3 &IoU0.5 & IoU0.7 \\ 
        \midrule
        MCN \cite{DBLP:conf/iccv/HendricksWSSDR17} &  & &  &  &   &   \\
        CTRL \cite{DBLP:conf/iccv/GaoSYN17}  & & &  &  &  &   \\
QSPN \cite{DBLP:conf/aaai/Xu0PSSS19}  & & &  & &  &   \\
SCDM~\cite{DBLP:conf/nips/YuanMWL019} & & &  &  &  &  \\
        PMI \cite{DBLP:conf/eccv/ChenJLJ20} & & &  & & &   \\
        LGI~\cite{DBLP:conf/cvpr/MunCH20} & & &  & & &   \\
        CMIN \cite{DBLP:journals/tip/LinZZZC20}  & & &  &&  &   \\
        DRN~\cite{DBLP:conf/cvpr/ZengXHCTG20} & & &  &  &  &  \\
        2D-TAN~\cite{DBLP:conf/aaai/ZhangPFL20} & & &  & &  &   \\
IVG-DCL~\cite{DBLP:conf/cvpr/NanQXLLZL21}&  &&  & & &\\
        CBLN~\cite{DBLP:conf/cvpr/LiuQDZ00XX21} &  & &&  & &\\
        CPN~\cite{DBLP:conf/cvpr/ZhaoZZL21} & & & & & &\\
        FVMR~\cite{Gao_2021_ICCV} & & & & & &\\
        SSCS~\cite{Ding_2021_ICCV}& & & & & &\\
        \midrule
        Our MMN  &  &  &  &    &    &   \\
        \bottomrule
        \end{tabular}
        }
    }
    \caption{Performance comparison on ActivityNet Captions (C3D feature).}
    \label{tab:anet}
    \end{table}
}

\newcommand{\tableCHARADES}{
    \begin{table}[t]
        \centering \resizebox{1.0\linewidth}{!}{\scalebox{1.0}{
        \begin{tabular}{l|cc|cc} \toprule
\multirow{2}{*}{Method} &  \multicolumn{2}{c|}{R@1  }& \multicolumn{2}{c}{R@5  } \\ 
          &IoU0.5 & IoU0.7 & IoU0.5 & IoU0.7 \\ 
        \midrule
        MCN~\cite{DBLP:conf/iccv/HendricksWSSDR17} &   &  &   &   \\
        
        SAP~\cite{DBLP:conf/aaai/ChenJ19a} &   &  &   &   \\
MAN~\cite{DBLP:conf/cvpr/ZhangDWWD19} &   &  &   &   \\
        {\color{gray} DRN~\cite{DBLP:conf/cvpr/ZengXHCTG20}} &  &  &  &  \\
        2D-TAN~\cite{DBLP:conf/aaai/ZhangPFL20} &  &  &  &    \\ 
CBLN~\cite{DBLP:conf/cvpr/LiuQDZ00XX21} &   &  & &\\
        CPN~\cite{DBLP:conf/cvpr/ZhaoZZL21} &   & & & \\
        FVMR~\cite{Gao_2021_ICCV} &   &  & &\\
        SSCS~\cite{Ding_2021_ICCV} &   & & & \\   
        \midrule
        Our MMN  &  &  &  &    \\
        \bottomrule
        \end{tabular}
        }
        }
    \caption{Performance comparison on Charades-STA (VGG feature). our re-production with official code is much lower than the value reported in paper.}
    \label{tab:charades}
    \end{table}
}


\newcommand{\tableTACOS}{
    \begin{table}[t]
        \centering \resizebox{\linewidth}{!}{\scalebox{0.9}{
        \begin{tabular}{l|ccc|ccc} \toprule
        \multirow{2}{*}{Method}&   \multicolumn{3}{c|}{R@1  }& \multicolumn{3}{c}{R@5  } \\ 
        &  IoU0.1 & IoU0.3 & IoU0.5 & IoU0.1 & IoU0.3 & IoU0.5 \\ 
        \midrule
        MCN \cite{DBLP:conf/iccv/HendricksWSSDR17} &  &  &  &  &  &   \\
        CTRL \cite{DBLP:conf/iccv/GaoYSCN17}  &  &  &  &  &  &   \\
QSPN \cite{DBLP:conf/aaai/Xu0PSSS19}  &  &  &  &  &  &   \\
SCDM~\cite{DBLP:conf/nips/YuanMWL019} & &  &  &  &  &   \\
        CMIN~\cite{DBLP:journals/tip/LinZZZC20}  &  &  &  &  &  &   \\
        DRN~\cite{DBLP:conf/cvpr/ZengXHCTG20} &   &  &  &  &  &  \\
        2D-TAN~\cite{DBLP:conf/aaai/ZhangPFL20} &   &  &  &  &  &   \\
        IVG-DCL~\cite{DBLP:conf/cvpr/NanQXLLZL21}  & & & & &&  \\
        CBLN~\cite{DBLP:conf/cvpr/LiuQDZ00XX21} & & & &&  &\\
FVMR~\cite{Gao_2021_ICCV} & & & & & & \\
        SSCS~\cite{Ding_2021_ICCV} &   & & & & & \\
        \midrule
        Our MMN  &   &  &  &  &  &  \\
        \bottomrule
        \end{tabular}
        }
        }
        \caption{Performance comparison on TACoS (C3D feature).}
        \label{tab:tacos}
    \end{table}
}
\newcommand{\tableSTVG}{
\begin{table}[t]
    \begin{center}
    \resizebox{1.0\linewidth}{!}{\begin{tabular}{l|c|c|c}
            \toprule
            Method & m\_vIoU & vIoU@0.3 & vIoU@0.5 \\
            \midrule
            2D-TAN + WSSTG &  &  &  \\
STGVT~\cite{DBLP:journals/corr/abs-2011-05049} & &  &  \\
            STVGBert~\cite{Su_2021_ICCV} &  &  &  \\
            Yu \etal~\cite{DBLP:journals/corr/abs-2106-07166} & & &\\
            {\color{gray}Tan \etal ~\cite{DBLP:journals/corr/abs-2106-10634}} & {\color{gray}}& {\color{gray}}&{\color{gray}} \\
            Our stage-1+2D-TAN &  &  &  \\
            \midrule
            Our stage-1+MMN &  &  & \\
            \bottomrule
            \end{tabular}
    }
    \end{center}
    \caption{Performance comparison on HC-STVG val set. use smaller train set, values reported in~\cite{DBLP:journals/corr/abs-2011-05049}. uses an ensemble of 10 models.}
    \label{table:stvg}
\end{table}
}

\newcommand{\tableGLOVE}{
    \begin{table}[h]
            \centering \resizebox{1.0\linewidth}{!}{\scalebox{1.0}{
            \begin{tabular}{l|cc|cc} \toprule
\multirow{2}{*}{Language Models}&   \multicolumn{2}{c|}{R@1  }& \multicolumn{2}{c}{R@5  } \\ 
             &   IoU0.5 & IoU0.7 & IoU0.5 & IoU0.7 \\ 
            \midrule
            GloVe (6B) &  &  &  &    \\
            GloVe (6B) &  &  &  &    \\
            GloVe (840B) &  &  &  &    \\
            DistilBERT &  &  &  &    \\
            \bottomrule
            \end{tabular}
            }
        }
        \caption{Comparisons between different language models in 2D-TAN~\cite{DBLP:conf/aaai/ZhangPFL20} model on Charades-STA dataset, including different pre-trained GloVe embeddings and DistilBERT model.  means our implementation of 2D-TAN with GloVe 6B.}
        \label{tab:glove}
    \end{table}
}

\newcommand{\tableLSTM}{
\begin{table}[t]
        \centering \resizebox{1.0\linewidth}{!}{\scalebox{1.0}{
        \begin{tabular}{l|cc|cc} \toprule
        \multirow{2}{*}{Language Models}&   \multicolumn{2}{c|}{R@1  }& \multicolumn{2}{c}{R@5  } \\ 
         &   IoU0.5 & IoU0.7 & IoU0.5 & IoU0.7 \\ 
        \midrule
        DistilBERT w/ BCE loss &  &  &  &    \\
        DistilBERT w/ both losses&  &  &  &    \\
        Performance gap  &  &  &  &    \\
        \midrule
LSTM w/ BCE loss &   &   &     &    \\
        LSTM w/ both losses &  &  &  &    \\
        Performance gap  &  &  &  &    \\
        \bottomrule
        \end{tabular}
        }
    }
    \caption{Comparisons between different language models in our proposed MMN on Charades-STA dataset.  the version we used as MMN's final performance.}
    \label{tab:lstm}
\end{table}
}


\newcommand{\tableDistilBERT}{
    \begin{table}[t]
        \centering \resizebox{1.0\linewidth}{!}{\scalebox{0.9}{
        \begin{tabular}{l|c|cc|cc} \toprule
\multirow{2}{*}{Charades-STA}& \multirow{2}{*}{Feature} & \multicolumn{2}{c|}{R@1  }& \multicolumn{2}{c}{R@5  } \\ 
         &  & IoU0.5 & IoU0.7 & IoU0.5 & IoU0.7 \\ 
        \midrule
DRN~\cite{DBLP:conf/cvpr/ZengXHCTG20} &C3D&  &  &  &    \\
        DRN w/ DistilBERT &C3D&  &  &  &    \\
        \midrule
        \midrule
        2D-TAN \cite{DBLP:conf/aaai/ZhangPFL20} &VGG&  &  &  &    \\
        2D-TAN &VGG&  &  &  &    \\ 
        2D-TAN w/ DistilBERT &VGG&  &  &  &    \\
\bottomrule
        \end{tabular}
        }
    }
    \caption{Previous methods with DistilBERT.  means our implementation.}
    \label{tab:bert}
    \end{table}
}

\newcommand{\tableDRN}{
\begin{table}[t]
        \centering \resizebox{1.0\linewidth}{!}{\scalebox{1.0}{
        \begin{tabular}{l|cc|cc} \toprule
\multirow{2}{*}{I3D feature}&   \multicolumn{2}{c|}{R@1  }& \multicolumn{2}{c}{R@5  } \\ 
         &   IoU0.5 & IoU0.7 & IoU0.5 & IoU0.7 \\ 
        \midrule
        RGB-only, Inception-v1 &  &  &  &    \\
        Two-stream, ResNet-50 &  &  &  &    \\
        \bottomrule
        \end{tabular}
        }
    }
    \caption{Comparisons between different I3D features in DRN on Charades-STA dataset.  means our implementation.}
    \label{tab:drn}
\end{table}
}


 \newcommand{\figFIRST}{
    \begin{figure}[t]
        \begin{center}
        \includegraphics[width=8cm]{figures/first_visual.pdf}
        \end{center}
        \caption{(a) In addition to matching moments given a query, we propose a new auxiliary task named {\em mutual matching} to differentiate the matched query (red dash line) among unmatched queries intra- (green) or inter-video (purple) for the GT moment (red box). (b) Most previous methods only consider intra-video negative moments (green). (c) Our {\em mutual matching} uses negative pairs from both modalities intra- (green) or inter-video (purple). {\bf Best view with colors}.}
        \label{fig:first}
\end{figure}
        
}


\newcommand{\figOVERVIEW}{
    \begin{figure*}[t]
        \begin{center}
        \includegraphics[width=1.0\linewidth]{figures/overview.pdf}
        \end{center}
           \caption{Overview of our framework. Different from the baseline, we adopt a late modality fusion strategy and learning the feature embedding for moments and sentences on two independent spaces with pair discrimination and BCE losses, respectively. The dots and triangles are the feature of moments and sentences. The red dash lines are matched moment-sentence pairs to be pulled in, while green/purple dash lines are negative samples intra/inter-video to be pushed away. {\bf Best view with colors}.}
        \label{fig:overview}
\end{figure*}
}


\newcommand{\figSANITY}{
    \begin{figure}[t]
            \begin{center}
            \includegraphics[height=3cm]{figures/sanity_check.pdf}
            \end{center}
\caption{Sanity Check on the Charades-STA dataset. values are obtained from~\cite{DBLP:conf/bmvc/OtaniNRH20}.}
            \label{fig:sanity}
\end{figure}
}

\newcommand{\figVISUAL}{
    \begin{figure*}[ht]
        \begin{center}
        \includegraphics[width=17cm]{figures/visualization.pdf}
        \end{center}
        \caption{Visualizations on two datasets. Green `{\color{ao(english)}x}' in the first three maps shows location of GT moment. In the last map, the matched moment (i.e., has the highest IoU among candidate moments) is a green `{\color{ao(english)}x}' and the given sentence is a green `{\color{ao(english)}+}'. The colors of unmatched moments shows their IoU with GT moment.}
        \label{fig:visual_anet_charades}
\end{figure*}
}

\newcommand{\figSTVG}{
\begin{figure}[t]
        \begin{center}
        \includegraphics[width=8cm]{figures/stvg.png}
        \end{center}
        \caption{Architecture of our STVG method with DMN.}
        \label{fig:stvg_arch}
\end{figure}
}

\newcommand{\figVIOU}{
\begin{figure*}[t]
        \begin{center}
        \includegraphics[height=4cm]{figures/vIoU.png}
        \end{center}
\caption{Visualization of how we use vIoU supervision to automatically select the right tube.}
        \label{fig:viou}
\end{figure*}
}


 \newcommand{\etal}{\textit{et al.}}
\usepackage{newfloat}
\usepackage{listings}
\lstset{basicstyle={\footnotesize\ttfamily},numbers=left,numberstyle=\footnotesize,xleftmargin=2em,aboveskip=0pt,belowskip=0pt,showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
\pdfinfo{
/Title (AAAI Press Formatting Instructions for Authors Using LaTeX -- A Guide)
/Author (AAAI Press Staff, Pater Patel Schneider, Sunil Issar, J. Scott Penberthy, George Ferguson, Hans Guesgen, Francisco Cruz, Marc Pujol-Gonzalez)
/TemplateVersion (2022.1)
}



\setcounter{secnumdepth}{2} 





\title{Negative Sample Matters: A Renaissance of Metric Learning \\ for Temporal Grounding}
\author{Zhenzhi Wang \quad Limin Wang\thanks{Corresponding author.}\quad Tao Wu\quad  Tianhao Li\quad Gangshan Wu}
\affiliations{State Key Laboratory for Novel Software Technology, Nanjing University, China \\
{\tt\small \{zhenzhiwang,tianhaolee\}@outlook.com, \{lmwang,gswu\}@nju.edu.cn, wt@smail.nju.edu.cn}
}
\begin{document}
\maketitle

\begin{abstract}
Temporal grounding aims to localize a video moment which is semantically aligned with a given natural language query. Existing methods typically apply a detection or regression pipeline on the fused representation with the research focus on designing complicated prediction heads or fusion strategies. Instead, from a perspective on temporal grounding as a metric-learning problem, we present a Mutual Matching Network (MMN), to directly model the similarity between language queries and video moments in a joint embedding space. This new metric-learning framework enables fully exploiting negative samples from two new aspects: constructing negative cross-modal pairs in a mutual matching scheme and mining negative pairs across different videos. These new negative samples could enhance the joint representation learning of two modalities via cross-modal mutual matching to maximize their mutual information. Experiments show that our MMN achieves highly competitive performance compared with the state-of-the-art methods on four video grounding benchmarks. Based on MMN, we present a winner solution for the HC-STVG challenge of the 3rd PIC workshop. This suggests that metric learning is still a promising method for temporal grounding via capturing the essential cross-modal correlation in a joint embedding space. Code is available at \url{https://github.com/MCG-NJU/MMN}.
\end{abstract}

\section{Introduction}
\label{sec:intro}
Video analysis is a fundamental problem in computer vision and has drawn increasing attention in recent years because of the potential applications in surveillance, robotics, and Internet videos. While considerable progress has been made in video classification~\cite{DBLP:conf/eccv/WangXW0LTG16,DBLP:conf/cvpr/WangL0G18,DBLP:conf/iccv/Feichtenhofer0M19,DBLP:conf/cvpr/0002TJW21} and action localization~\cite{DBLP:conf/iccv/ZhaoXWWTL17,DBLP:conf/iccv/LinLLDW19,DBLP:conf/eccv/LiW0W20}, such tasks are still limited to recognizing a pre-defined list of activities, such as playing baseball or peeling potatoes. As videos often contain complex activities that may cause a combinatorial explosion if described by a list of actions and objects, the task of grounding language queries in videos~\cite{DBLP:conf/iccv/HendricksWSSDR17,DBLP:conf/iccv/GaoSYN17,DBLP:journals/corr/abs-2011-05049}, as a generalization of the action localization task to overcome the constraints, has recently gained plenty of interest~\cite{DBLP:conf/aaai/HeZHLLW19,DBLP:conf/cvpr/ZhangDWWD19,DBLP:conf/cvpr/ZengXHCTG20,DBLP:conf/nips/YuanMWL019,DBLP:conf/aaai/ZhangPFL20,DBLP:journals/corr/abs-2011-05049} in both computer vision and language community. Formally, given a verbal description, the goal of temporal grounding is to determine the temporal moment (i.e., the start and end time) that semantically corresponds to the query best in a given video.

\figFIRST
Although temporal grounding opens up great opportunities for detailed video perception by using the new language modality to capture the complex relations between sentences and videos, most of the previous approaches~\cite{DBLP:conf/cvpr/ZhangDWWD19,DBLP:conf/nips/YuanMWL019,DBLP:conf/aaai/ZhangPFL20,DBLP:conf/cvpr/ZengXHCTG20,DBLP:journals/corr/abs-2011-05049} still tackle this problem in a {\em detection/regression} way with {\em early-fusion} designs, e.g., using the fused multi-modal features to predict the offset of action moments from anchors; or directly regressing the desired region on a globally aggregated multi-modal feature. These indirect grounding methods typically ignore the essential relation between all cross-modal pairs (i.e., pairs of moment and language queries), and they only simply utilize the IoU-based scores between the given sentence and moments from the {\em same video} as supervision (Fig.~\ref{fig:first}(b)). However, we argue that the negative relations between the given moment and other {\em unmatched descriptions} are also important for learning a joint cross-modal embedding. Intuitively, the auxiliary task, given a video moment with visually informative actions, training the model to contrast the matched query and unmatched descriptions, (Fig.~\ref{fig:first}(a)) is beneficial for the temporal grounding task. We propose to use this auxiliary task for the first time in temporal grounding, which shows advantages in less extra computational cost compared with previous auxiliary tasks. In order to model such relations from mutual directions (Fig.~\ref{fig:first}(c)), we revisit the temporal grounding task from a {\em metric-learning perspective}, which allows us to directly model the essential similarity measurement in joint cross-modal embedding space, rather than designing sophisticated detection/regression strategies based on the fused representation as in many previous methods.  

In our metric-learning view, sentences and moments play an equally important role in temporal grounding and both matching directions are adopted, i.e., select the right instance in a modality given the groundtruth of another modality. Thus, supervisions are constructed in a symmetric form for two modalities by our approach (Fig.~\ref{fig:first}(c)), creating more supervisions than previous methods. Our framework shows several advantages: Firstly, the metric-learning perspective enables us to mine negative samples from a {\em mutual matching scheme}. In this sense, an {\em unmatched relation} is also informative by implying that this moment and sentence should be pushed away in the joint space. To achieve this, we adopt a cross-modal mutual matching objective to contrast pos./neg. moment-sentence pairs inspired by a cross-modal pretraining method~\cite{DBLP:journals/corr/abs-2001-05691}. 
Secondly, we exploit much more negative samples to improve our representation learning by adopting {\em inter-video} negative samples. In contrast, most of the previous methods only utilize {\em intra-video} moment-sentence pairs as supervision due to the early-fusion strategy.
Finally, we model the video and language feature in a Siamese-alike network architecture and use a simple dot-product for cross-modal similarity computation. Thus, our framework has less computation cost by sharing moment features among sentences inside each video.

While using the negative pairs can lead to a better feature, the binary supervision signal itself (i.e., matched or unmatched) is still weak to precisely rank the moments inside a video. In addition, although the negative pairs used in our mutual matching scheme provide much more supervision signals, most of them are easy negatives (especially the inter-video ones). So we adopt another embedding space for learning the cross-modal similarity for precisely ranking moments based on the IoU supervision, which enables us to estimate finer relations (i.e., a scalar instead of a binary signal) and accurately differentiate the correct pair among hard negatives (e.g., negative moments with high IoU). These two complementary objectives share the same feature encoders as backbone and have their own customized heads.

Our main contributions are three-fold: (1) We revisit the metric-learning perspective on the temporal grounding task in a {\em late-fusion} fashion and leverage {\bf inter-video} negative visual-language pairs. Our framework shows advantages on both performance and training cost. (2) Our metric-learning view enables a new auxiliary task for temporal grounding, named {\bf cross-modal mutual matching} to significantly add more supervision signals. Instead of heavy additional networks for previous auxiliary tasks, ours has a more direct idea and is also more effective. (3) Comprehensive quantitative and qualitative analyses are conducted on four benchmarks from both temporal and spatio-temporal video grounding tasks to show our method's generalization ability.

\section{Related Works}
\subsection{Temporal Grounding}
\noindent\textbf{Temporal Grounding Methods}. Previous methods can be mainly categorized into four groups: \textbf{(1) Regression based methods} directly predict the boundaries (i.e., start and end) of the target moment from the fused multi-modal features relying on either a clip-wise boundary classification in local regions~\cite{DBLP:conf/naacl/GhoshAPH19,DBLP:conf/wacv/OpazoMSLG20, DBLP:conf/acl/ZhangSJZ20,DBLP:conf/aaai/ChenLTXZTL20} or a direct boundary regression on the aggregated global feature~\cite{DBLP:conf/aaai/YuanM019,DBLP:conf/aaai/Wang0J20,DBLP:conf/cvpr/MunCH20}. Some methods also introduce some heuristics, e.g., aggregating clip-wise actionness scores modeled by compositional reasoning~\cite{DBLP:conf/eccv/LiuYCHFN18} or taking the expectation of probability for start/end~\cite{DBLP:conf/naacl/GhoshAPH19}. \textbf{(2) Detection based methods} often first generate candidate moments and then evaluate them on the fused multi-modal features~\cite{DBLP:conf/iccv/GaoSYN17, DBLP:conf/wacv/GeGCN19}. Their evaluation on moments utilize various designs, e.g., LSTM~\cite{DBLP:conf/emnlp/ChenCMJC18,DBLP:conf/aaai/Xu0PSSS19}, dynamic filtering~\cite{DBLP:conf/cvpr/ZhangDWWD19,DBLP:conf/wacv/OpazoMSLG20} or modulation~\cite{DBLP:conf/nips/YuanMWL019}, graph convolution~\cite{DBLP:conf/cvpr/ZhangDWWD19}, anchor-free detectors~\cite{DBLP:conf/emnlp/LuCTLX19,DBLP:conf/cvpr/ZengXHCTG20}, and 2D moment map~\cite{DBLP:conf/aaai/ZhangPFL20}.  \textbf{(3) Reinforcement Learning based methods}~\cite{DBLP:conf/aaai/HeZHLLW19,DBLP:conf/cvpr/WangHW19,DBLP:conf/aaai/WuLLL20} localize the target moment iteratively by defining the {\em state} and {\em action} on the video and treating this task as a sequential decision-making process. \textbf{(4)} The only previous \textbf{metric learning based method}~\cite{DBLP:conf/iccv/HendricksWSSDR17} uses a triplet loss with  distance as the similarity measurement to match the given sentence to the correct video moment. However, it lacks both the important supervision of negative sentence samples, i.e., our novel {\em mutual matching scheme} and the effective relation modeling of moments, thus achieves much worse results than ours.

\noindent\textbf{Multi-modal Fusion.} Many methods from the group 1) to 3) mainly adopt a {\em early-fusion} pipeline for cross-modal modeling, e.g., by concatenation~\cite{DBLP:conf/emnlp/ChenCMJC18,DBLP:journals/tip/LinZZZC20,DBLP:conf/aaai/Wang0J20}, dynamic convolution~\cite{DBLP:conf/wacv/OpazoMSLG20,DBLP:conf/cvpr/ZhangDWWD19,DBLP:conf/nips/YuanMWL019}, cross-attention~\cite{DBLP:conf/emnlp/LuCTLX19,DBLP:journals/corr/abs-2009-11232}, or hadamard product~\cite{DBLP:conf/cvpr/ZengXHCTG20,DBLP:conf/cvpr/MunCH20,DBLP:conf/aaai/ZhangPFL20}. On the contrary, we use a simple inner-product in the joint visual-language space to measure the cross-modal similarity in a {\em late-fusion} manner. It not only enables our mutual matching scheme, but also shows advantages in computational cost during training by sharing the video feature among sentences in the same video.

\noindent\textbf{Auxiliary Tasks in Temporal Grounding.} Query reconstruction as an auxiliary task for temporal grounding was explored by~\cite{DBLP:journals/tip/LinZZZC20,DBLP:conf/aaai/Xu0PSSS19} who added a video caption loss following image-based grounding methods~\cite{DBLP:conf/cvpr/RamanishkaDZS17}. Yet it introduces extra overhead of parameters and computational cost, e.g., LSTM~\cite{DBLP:conf/aaai/Xu0PSSS19} or Transformer~\cite{DBLP:journals/tip/LinZZZC20}. Our proposed auxiliary task of mutual matching avoid the usage of the heavy additional network and is also more direct and effective. 

\subsection{Spatio-Temporal Video Grounding}
As the recent progress achieved in spatio-temporal action localization~\cite{DBLP:conf/eccv/LiW0W20}, spatio-temporal video grounding~\cite{DBLP:journals/corr/abs-2011-05049} is also proposed as an extension of temporal grounding. By adapting our MMN on the linked human bounding boxes~\cite{DBLP:conf/iccv/KalogeitonWFS17a} for temporal trimming, significant performance gain is achieved over the previous transformer-based method~\cite{DBLP:journals/corr/abs-2011-05049}.

\subsection{Metric Learning}
The family of metric learning loss has been explored to learn powerful representations with the supervised setting~\cite{DBLP:conf/cvpr/HadsellCL06,DBLP:conf/nips/KhoslaTWSTIMLK20}, where the positive sample is chosen from the same class and the negative one from other classes; or self-supervised setting~\cite{DBLP:conf/cvpr/WuXYL18,DBLP:conf/icml/ChenK0H20,DBLP:conf/cvpr/He0WXG20}, which select positive samples with data augmentation or co-occurrence. Different from both setting, we choose pos./neg. samples according to the groundtruth in a {\em supervised} way, yet we have {\em no pre-defined categories}. Furthermore, most aforementioned methods use single-modal samples, e.g., images, while we utilize cross-modal moment-sentence pairs. There are some video-language pre-training methods~\cite{DBLP:journals/corr/abs-2001-05691,DBLP:conf/cvpr/MiechASLSZ20} similar to our setting, yet they aim to learn {\em video-level} representations in an {\em unsupervised} way while we want to enhance {\em proposal-level} features in a {\em supervised} way, where no pre-training dataset is used. We adopt cross-modal pair discrimination loss~\cite{DBLP:journals/corr/abs-2001-05691} for our mutual matching scheme, where each instance in both modalities defines an unique class and the binary classes for cross-modal pairs, i.e., matched or unmatched, provide us valuable supervisions. Cross-modal image/video retrieval methods~\cite{DBLP:conf/cvpr/WangLL16,DBLP:conf/eccv/Gabeur0AS20} also utilize negative samples in a metric learning framework, yet they still treat the {\em image/video as a whole}. Thus we address a different problem from them.
\figOVERVIEW

\section{Model}
We propose to use negative sentence samples to construct a cross-modal mutual matching scheme for modeling bi-directional matching relations in visual and language modalities. To enable this auxiliary task, we adopt a late modality fusion from the metric-learning prospective with the advantage of less computational cost than the baseline~\cite{DBLP:conf/aaai/ZhangPFL20}. Sec.~\ref{sec:task} introduces the formulation of temporal grounding. Sec.~\ref{sec:arch} illustrates how to construct our MMN from the baseline. Then, we analyze the two complementary loss of IoU regression and mutual matching in Sec.~\ref{sec:loss}.

\subsection{Problem Formulation}
\label{sec:task}
Given an untrimmed video  and a natural language query , the temporal grounding task aims to localize a temporal moment  that matches the query. We denote the video as a sequence of frames , where  is a frame and  is the total number of frames; the query sentence as a sequence of words , where  is a word and  is the total number of words. Ideally, the retrieved moment  should deliver the same semantic as the sentence . We adopt the feature vectors in the joint space to represent sentence  and moments , therefore the inner product of visual and text features after -norm should be maximized.

\subsection{Architecture}
\label{sec:arch}
Our MMN adopts a Siamese-alike network architecture with a late modality fusion by a simple inner product in the joint visual-language space, as illustrated in Fig.~\ref{fig:overview}.

\noindent{\bf Language Encoder.} Previous works commonly utilize a LSTM~\cite{DBLP:journals/neco/HochreiterS97} upon a sequence of word vectors embedded by GloVe~\cite{DBLP:conf/emnlp/PenningtonSM14}. Yet, some concerns about unfair comparisons in previous methods, e.g., GloVe models pre-trained on different corpora, motivate us to adopt a standard language encoder. We choose DistilBERT~\cite{DBLP:journals/corr/abs-1910-01108} for its light-weighted model capacity. For each input sentence , we first generate the tokens of words by the tokenizer and add a class embedding token `[CLS]' in the beginning. Then we feed the tokens into DistilBERT to get a feature sequence , where  and =768 is the feature dimension. There are two commonly used aggregation approaches to get the whole sentence's embedding: 1) global average pooling over all tokens
; 2) the class embedding `[CLS]'. Our experiment shows that global average pooling has faster convergence and better performance in this task, so we take average pooling as default. 





\noindent{\bf Video Encoder.} We extract features of the input video and encodes them as a 2D temporal moment feature map following the baseline 2D-TAN~\cite{DBLP:conf/aaai/ZhangPFL20}. We segment the input video stream into small video clips  with each clip  containing  frames, then extract clip-level features with an off-the-shelf pre-trained CNN model (e.g., C3D). We perform a fixed-length sampling to obtain  clip-level features for each video by an even stride  and pass the fixed-length features through an FC layer to reduce their dimension, denoted as , where . Then we build up the 2D feature map for candidate moments following the baseline~\cite{DBLP:conf/aaai/ZhangPFL20} as , where we adopt the max-pooling as the moment-level feature aggregation strategy following. We also utilize the sparse sampling strategy which removes highly overlapped moments to reduce the number of candidate moments as well as computation cost following 2D-TAN. Different from 2D-TAN~\cite{DBLP:conf/aaai/ZhangPFL20}, we directly model the relations of moments  by  layers of 2D convolution with kernel size  only based on visual features. As discussed in~\cite{DBLP:conf/bmvc/OtaniNRH20}, the performance of 2D-TAN has no significant drop when the order of input video clips is randomly permuted (i.e., the `sanity check'), which is counter-intuitive. It indicates that to some extent 2D-TAN ignores the visual features and overfits the bias in the dataset, such as temporal distributions of actions conditioned on sentences. The design of our MMN forces the convolution filters to actually utilize the visual features. 

\noindent{\bf Joint Visual-Language Embeddings.} Finally, we estimate the matching quality of each moment based on the similarity of two modalities for both supervision signals (i.e., IoU regression and cross-modal mutual matching). We adopt a LayerNorm~\cite{DBLP:journals/corr/BaKH16} for more stable convergence in language feature aggregation. We use a linear projection layer or a x convolution to project the language and visual features into the same dimension  respectively. The final representations of sentence feature are  for cross-modal mutual matching (subscript ) and IoU regression (subscript ). Moment features are . 



where  and  are learnable parameters,  is 2D convolution with kernel size  and stride  for 2D feature map . The subscripts  or  means the weights of two branches are independent. Then we regard the cosine similarity as moments' estimation scores for both losses. 


where we enforce the embedding  via a -normalization layer. 


\subsection{Loss Functions}
\label{sec:loss}
Our MMN integrates two complementary losses: a binary cross entropy loss for regressing the IoU and a pair discrimination loss for learning discriminative features. 

\noindent{\bf Binary Cross Entropy.} We follow 2D-TAN~\cite{DBLP:conf/aaai/ZhangPFL20} to adopt the scaled IoU values  for each candidate moments as the supervision signal, as shown in Tab.~\ref{tab:supervision} (IoU regression). The IoU values are linearly scaled from (,) to (0,1) and truncate the values beyond (0,1). We directly use the value of and  reported in 2D-TAN for fair comparisons. We notice that the range of cosine similarity is  (-1,1), yet the IoU signal is  (0,1). So we adopt a commonly-used sigmoid function  to highlight the value change near the neutral region (e.g., =0.5). We heuristically amplify the  by a factor of 10 to make the range of final prediction of our model  cover most of the regions in . The regression branch of our MMN is trained by a BCE loss:
\begin{small}

\end{small}
where  is the final score of a moment and  is the total number of valid candidates.
\tableSUPERVISION

\noindent{\bf Cross-modal Mutual Matching.} As discussed in Sec.~\ref{sec:intro}, our cross-modal mutual matching creates more supervision signals for temporal grounding. By contrasting the positive moment-sentence pairs with the negative ones sampled from {\em both intra and inter videos}, encoders will learn a more discriminative features for both modalities {\em without any extra pre-training dataset}. As shown in Tab.~\ref{tab:supervision}, our mutual matching objective introduces two novel aspects of supervision signals: 1) pairs of the ground-truth moment and pos./neg. sentences (row 2); and 2) inter-video negative pairs (column 7). In contrast, previous detection/regression methods only sample pos./neg. samples based on IoU signals (row 3), and the previous metric learning method~\cite{DBLP:conf/iccv/HendricksWSSDR17} only has single direction of matching (i.e., only row 1) yet lacks the important cross-modal mutual matching (i.e., both row 1 and row 2). Specifically, we adapt the cross-modal pair discrimination loss~\cite{DBLP:journals/corr/abs-2001-05691} from video-level to proposal-level to learn features for moments  and sentences , where these features should be similar if the moment-sentence pair is semantically matched and dissimilar if it is semantically unrelated. We adopts the following conditional distribution in a non-parametric softmax form:
\begin{small}

\end{small}
where the  sentence or moment define a instance-level class  or , the feature embedding  and  are  normalized,  and  are temperatures and  and  are total numbers of sampled instances in the batch. Although the conditional distribution of video moments and sentences are similar, their differences are non-trivial due to the selection of negative samples, i.e.,  negative video moments are sampled from low IoU moments inside videos or moments from other videos; and  negative sentences are sampled from other sentences in the video or from other videos. To enable a steady training process, we only adopt the moments whose IoU with ground-truth moment lower than a threshold as negative samples (e.g.,  0.5). To further reduce the potential false negative signals, the sentences similar to the ground-truth sentence is automatically removed from the negative sample set, i.e., by computing their matched moments' IoU with ground-truth moment and removing them if their IoU  0.5. When we construct the negative sample set, we hold an assumption that only a small portion of inter-video negative samples (both sentences and moments) will semantically close to the positive sample. It is reasonable if the size of the training video corpus is large enough.

The objective of our cross-modal mutual matching is to maximize the likelihood  where  is the total number of moment-sentence pairs for training. The cross-correlation enables the network to effectively capture the mutual information between modalities by guiding the learning process of feature representation learning with the binary pair supervision. The loss function is as follows



Our final loss function  is a linear combination of binary cross entropy loss and mutual matching loss, and the final prediction score  for candidate moments given the query sentence are these two scores' product.


\section{Experiments}
\subsection{Datasets}
\noindent{\bf ActivityNet-Captions}~\cite{DBLP:conf/iccv/KrishnaHRFN17} is built on ActivityNet v1.3 dataset~\cite{DBLP:conf/cvpr/HeilbronEGN15}, where videos cover a wide range of complex human actions. It is originally designed for video captioning, and recently introduced into temporal grounding. There are 37,417, 17,505, and 17,031 moment-sentence pairs for training, validation, and testing respectively. Following the setting of 2D-TAN~\cite{DBLP:conf/aaai/ZhangPFL20}, we report the evaluation result on val\_2 set. 

\noindent{\bf TACoS} consists of 127 videos selected from the MPII-Cooking dataset~\cite{DBLP:conf/eccv/RohrbachRAAPS12}. It is comprised of 18,818 video-language pairs of different cooking activities in the kitchen annotated by \cite{DBLP:journals/tacl/RegneriRWTSP13}. A standard split~\cite{DBLP:conf/iccv/GaoSYN17} consists of 10,146, 4,589, and 4,083 moment-sentence pairs for training, validation and testing, respectively. We report evaluation results on test set in our experiments.

\noindent{\bf Charades-STA}~\cite{DBLP:conf/iccv/GaoSYN17} is an extended version of action recognition and localization dataset Charades~\cite{DBLP:conf/eccv/SigurdssonVWFLG16} by \cite{DBLP:conf/iccv/GaoSYN17} for temporal grounding. It contains 5,338 videos and 12,408 query-moment pairs in the training set, and 1,334 videos and 3,720 query-moment pairs in the test set.

\noindent{\bf HC-STVG} dataset is introduced by \cite{DBLP:journals/corr/abs-2011-05049} to spatio-temporally localize an action tubelet of the target person from an untrimmed video based on a given textual description. The dataset contains 5,660 videos selected from AVA~\cite{DBLP:conf/cvpr/GuSRVPLVTRSSM18} where each video has one video-sentence pair. A standard split consists of 4,500 pairs for training and 1,160 pairs for testing.

\subsection{Experimental Settings}
\label{sec:details}
\noindent{\bf Evaluation Metrics.} Following previous setting~\cite{DBLP:conf/iccv/GaoSYN17} of temporal grounding, we evaluate our model by computing {\it Rank n@m}. It is defined as the percentage of sentence queries having at least one correctly localized moment, i.e. IoU , in the top- retrieved moments. There are specific settings of  and  for different datasets. Specifically, we report the results as  for Charades-STA,  for TACoS and  for ActivityNet Captions dataset with . 

For spatio-temporal video grounding, we report the mean of vIoU and vIoU@\{0.3, 0.5\} following HC-STVG benchmark~\cite{DBLP:journals/corr/abs-2011-05049}, where vIoU is computed by the top-1 predicted tube with the GT tube. 

\noindent{\bf Implementation Details.} 
We adopt standard off-the-shelf video feature extractors without fine-tuning on each dataset. Our convolution network for 2D proposal feature modeling uses exactly the same settings with 2D-TAN~\cite{DBLP:conf/aaai/ZhangPFL20} (max-pool version) for fair comparisons, including visual features (VGG feature for Charades and C3D feature for ActivityNet-Captions and TACoS), number of sampled clips , number of 2D convolution network layers , kernel size  and channels , non maximum suppression (NMS) threshold, scaling thresholds  and . We set the dimension of the joint feature space , and temperatures . We use the HuggingFace~\cite{DBLP:journals/corr/abs-1910-03771} implementation of DistilBERT~\cite{DBLP:journals/corr/abs-1910-01108} with pre-trained model `distilbert-base-uncased' for better standardization in temporal grounding. In pair discrimination loss, we only sample negative moments lower than IoU=0.5 and set margin  as ,  and  for TACoS, ActivityNet-Captions and Charades-STA respectively. We use AdamW~\cite{DBLP:conf/iclr/LoshchilovH19} optimizer with learning rate of  and batch size  for Charades, learning rate of  and batch size  for ActivityNet Captions and learning rate of  and batch size  for TACoS. We set  of pair discrimination loss as  for ActivityNet Captions and  for Charades and TACoS following the principle that both loss should contribute equally weighted gradients. We early stop the pair discrimination loss when we observe the performance on validation set starts to drop. The learning rate of DistilBERT is always  of our main model. Each mini-batch has  {\it videos} instead of  {\it moment-sentence pairs}, where  is batch size.
\tablePairDiscrimination
\tableNUMBER
\subsection{Ablation Study}
\label{sec:ablation}
\noindent{\bf Cross-modal Mutual Matching.} We investigate the most important part in our method, i.e., `cross-modal mutual matching', by ablating different types of negative samples on both Charades-STA and ActivityNet Captions datasets, as shown in Tab.~\ref{tab:pd}. Row 1 is the baseline of our model for not using mutual matching. Row 2 uses the same information with previous works (e.g., intra-video moment negatives), which shows similar performance with row 1. By introducing sentence negatives, the notable performance gain is observed in row 3 to row 6 when compared to row 1 and row 2. It proves the effectiveness of our mutual matching scheme. Besides, we conclude some observations based on the differences inside row 3 to row 8: (1) Due to the larger number of negatives, only using inter-video negatives (row 4) tend to be more effective than only using intra-video negatives (row 3), especially on smaller datasets (e.g., Charades-STA); (2) Best results are often obtained by both intra and inter-video negatives (row 8), and sub-optimal results are often achieved in row 5 and 6 by discarding some specific type of inter-video negatives. It indicates hard negative mining of inter-video samples could be further investigated; (3) Only using our mutual matching scheme achieves similar or better performance than original BCE loss on two datasets (row 7 vs. row 1), demonstrating the powerful additional supervision signals introduced by our mutual matching scheme.
\tableDistilBERT
\tableAGGREGATION
\figSANITY
\begin{figure}[t]
	\centering
	\includegraphics[height=4cm]{figures/r1_05.pdf}
	\caption{Performance comparisons decomposed by moment length on Charades-STA, R@1(IoU=0.5).}
	\label{fig:charades1}
\end{figure}

\noindent{\bf Number vs. Type of Negatives.} We have shown inter-video negatives are beneficial by introducing much more negatives than previous methods in Tab.~\ref{tab:pd}. However, whether the number or the type of negatives is more important in our method? To ablate this, we construct a baseline that randomly picks the same numbers of negatives with intra-video negatives among all intra and inter-video negatives (row 2). Therefore, it uses the {\bf same} number with only intra-video negatives (row 1) {\em in each iteration} yet will eventually see more types of negatives in the training process. Tab.~\ref{tab:number} shows that the performance of this baseline (row 2) is similar to only using intra-video negatives (row 1) in R@1 metrics yet is better in R@5 metrics, indicating the importance of more types of negatives introduced by our mutual matching scheme.

\noindent{\bf Previous Methods with DistilBERT}. Two baselines (DRN~\cite{DBLP:conf/cvpr/ZengXHCTG20} and 2D-TAN~\cite{DBLP:conf/aaai/ZhangPFL20}) with DistilBERT are constructed in Tab.~\ref{tab:bert}, which shows that the usage of DistilBERT has very little impact to the final performance of both methods. Therefore, we believe comparisons of MMN with previous SOTA methods are fair.

\noindent{\bf Aggregation of Text Feature.} In Tab.~\ref{tab:aggregation} (top), we compare two aggregation strategies for the sentence feature. While many methods in NLP including BERT~\cite{DBLP:conf/naacl/DevlinCLT19} directly adopt the class embedding as the representation of the whole sentence, we find the average pooling over all words is more robust in our task. It indicates that the scale of video grounding datasets is limited in vocabulary size or annotated sentences, e.g., TACoS dataset only has a vocabulary size of 2255 words and a training set of 10146 sentences, while NLP pre-training datasets commonly have millions of sentences. We empirically use average pooling as default.

\noindent{\bf Hyper-parameters.} In Tab.~\ref{tab:aggregation} (bottom), we ablate important hyper-parameters of pair discrimination loss like margin , temperature  and weight . Due to the small size of Charades-STA, they do have some influences but no notable change of the gap between our MMN and baseline~\cite{DBLP:conf/aaai/ZhangPFL20}. We set them by just a few tries on each dataset.

\label{sanity-check}
\noindent{\bf Sanity Check on Visual Input.} \cite{DBLP:conf/bmvc/OtaniNRH20} observe that the blind baselines which only uses training set priors or conditional distributions of moments given queries already outperform many previous SOTA methods. So they define a test named `sanity check' to examine the contribution of visual inputs by evaluating the performance gap between the ordered input clips and randomly shuffled ones. We follow this setting on Charades-STA in Fig.~\ref{fig:sanity}: Our model with randomized visual input predicts an even lower result (37.5 vs. 37.8) than Blind-TAN~\cite{DBLP:conf/bmvc/OtaniNRH20} which does not use any visual input. Our performance gap between ordered inputs and shuffled ones is also larger than 2D-TAN's, which means our MMN shows better result in this sanity check. It indicates our method better exploits visual information. 
\tableCHARADES
\tableANET
\tableTACOS

\noindent{\bf Performance Comparisons by Moment Length.} To further discover our improvement over the baseline~\cite{DBLP:conf/aaai/ZhangPFL20}, we decompose the performance comparisons by the ratio of moment length over the video length in Fig.~\ref{fig:charades1}. The x-axis is grouped by the moment length ratio which stops at 0.6 because the maximum length of moments on Charades-STA is less than 0.6. It is worth noticing that the number of moments in each ratio interval is not balanced, e.g., GT moments have 1020 instances in (0.1, 0.2] while only 16 in (0.5, 0.6]. Our MMN consistently outperforms 2D-TAN in various moment lengths.
\figVISUAL
\subsection{Comparison with the State of the Art} 
We achieve highly competitive results on three datasets in temporal grounding: ActivityNet Captions, Charades-STA and TACoS, as reported in Tab.~\ref{tab:anet}, Tab.~\ref{tab:charades}, and Tab.~\ref{tab:tacos}. The top-2 performance values in tables are highlighted by {\bf bold} or \underline{ underline}. Based on the results, we have several observations. {\bf Firstly}, our proposed MMN outperforms the strong baseline~\cite{DBLP:conf/aaai/ZhangPFL20} with a significant margin by leveraging our mutual matching scheme (i.e., using both video and sentence negative samples from both intra and inter videos). As a result, it outperforms or is on par with most of the previous or recently proposed SOTA methods. Specifically, we compare our MMN with several concurrent approaches which share the similar loss function or architecture with ours, i.e., IVG-DCL~\cite{DBLP:conf/cvpr/NanQXLLZL21},  SSCS~\cite{Ding_2021_ICCV} and FVMR~\cite{Gao_2021_ICCV}. IVG-DCL and SSCS use contrastive loss to align the sentences and video {\em clips} instead of the sentences and {\em moments} in MMN. Thus they lack our important auxiliary task `mutual matching' and ignore the important sentence negative samples. They are also more complex than ours due to the additional IVG module (in IVG-DCL) or captioning/support-set module (in SSCS). As a result, they achieve similar or worse results than ours. They also do not address temporal grounding from a metric-learning perspective, thus their proposed modules will only add more computations to their baselines instead of reducing them, which is different from ours. FVMR also address temporal grounding from a metric-learning perspective, yet our supervision signals are totally different: 1) FVMR adopts a knowledge distillation loss where the teacher is the original hadamard product of cross-modal features for fusion with a MLP for prediction and the student is the inner-product in the common space. Our MMN adopts a contrastive loss and directly uses a inner-product in the common space without other fusion strategies as guidance. 2) FVMR also ignores the important sentence negative samples and lacks our proposed `mutual matching scheme', thus it uses the same supervisions as 2D-TAN and fails to construct more supervisions like ours. Besides, FVMR utilizes a complex semantic role tree for extracting phrase-level fine-grained sentence feature, while we use a simple average pooling over all words to obtain our sentence features. We achieve a better performance than FVMR on two datasets out of three, including the largest ActivityNet. 
{\bf Secondly}, it is worth noting that our framework is independent to the moment-relation modeling network or language encoders, so our MMN could be implemented in any proposal-based temporal grounding methods and further boost the performances on strong baselines, e.g., MS-2D-TAN~\cite{DBLP:journals/corr/abs-2012-02646} with multi-scale modelling capacity. 
{\bf Thirdly}, our computational cost is significantly reduced compared with 2D-TAN~\cite{DBLP:conf/aaai/ZhangPFL20} due to shared video features between sentences inside each video. The training process of the baseline takes 36 GPU hours to converge on ActivityNet-Captions dataset while ours only takes 10 GPU hours on the same GPU server. The testing cost could also be reduced if there are multiple sentences in a video, e.g., there are hundreds of sentences per video in TACoS dataset.

\noindent{\bf Visualizations.} In Fig.~\ref{fig:visual_anet_charades}, we show the visualizations on test set of two datasets. Invalid regions of the first three maps are manually set to be in color `grey'. Due to highly unbalanced pos./neg. samples of BCE loss in 2D-TAN~\cite{DBLP:conf/aaai/ZhangPFL20}, the absolute value of top-1 result is lower than our expectation. Yet, the gap between top-1 prediction and backgrounds is still discriminative for localizing moments. We conclude that IoU scores tend to be sharp near the GT location yet could be wrong in many situations, while pair discrimination scores tend to cover the right location yet are not sharp among hard negatives. These two score maps are complementary to each other. In our MMN, IoU regression branch is already improved by sharing the same backbone enhanced by pair discrimination loss. The final performance could be further improved a bit (about ) by fusing their scores with a simple product in our experiments. In the last map, we use t-SNE~\cite{JMLR:v9:vandermaaten08a} to visualize features in the joint space (=256) to a 2D image. Based on the distribution of moments, we believe our joint space has learnt an effective similarity measurement for both video and language modalities.

\subsection{Spatio-Temporal Video Grounding}
We also extend MMN to spatio-temporally localize action tubes given queries to further show its generalization ability. We find that MMN is even more effective on the HC-STVG dataset due to its smaller scale, where our additional supervisions are very important for limited annotations.

\noindent{\bf Adapting MMN to the STVG task.} We adopt a two-stage method for this task: In stage 1, we first detect human bounding boxes by Faster R-CNN~\cite{DBLP:conf/nips/RenHGS15} and link them to be candidate tubes following ACT~\cite{DBLP:conf/iccv/KalogeitonWFS17a}; then we use CSN~\cite{DBLP:conf/iccv/TranWFT19} to extract 1024-d RoI features for each bounding box to form a feature sequence per candidate tube. In stage 2, we use our MMN to temporally refine the candidate tubes with proper adaptation. There are several candidate tubes (as feature sequences) generated in stage 1 in each video, while temporal grounding only have one feature sequence per video. Thus, we generate a 2D moment map for each candidate tube, yet there is only one positive moment per video selected by the highest vIoU globally and all the rest moments including other 2D moment maps are negatives. The IoU regression branch also uses the scaled vIoU as the groundtruth. As a result, vIoU-based supervisions will automatically guide our MMN to find the right tube and the appropriate temporal refinement and we do not need to design complex evaluation module (e.g., transformer~\cite{DBLP:journals/corr/abs-2011-05049}) to select the right candidate tube to be refined.

\noindent{\bf Performance Comparisons.} In Tab.~\ref{table:stvg}, our MMN shows significant performance gain over some baselines or recently proposed transformer-based methods, e.g., 2D-TAN+WSSTG~\cite{DBLP:conf/acl/ChenMLW19}, STGVT~\cite{DBLP:journals/corr/abs-2011-05049} and STVGBert~\cite{Su_2021_ICCV}. Our MMN also shows notable performance gain over 2D-TAN~\cite{DBLP:conf/aaai/ZhangPFL20} on the same candidate tubes generated from our stage 1. Finally, our method outperforms some competitive opponents~\cite{DBLP:journals/corr/abs-2106-07166,DBLP:journals/corr/abs-2106-10634} which use strong multi-modal pre-training models (e.g., LXMERT~\cite{DBLP:conf/emnlp/TanB19} or MDETR~\cite{DBLP:journals/corr/abs-2104-12763}) and ranks first in the HC-STVG challenge of the 3rd PIC workshop\footnote{\url{http://www.picdataset.com/challenge/task/hcvg/}}. In contrast, cross-modal interaction modeling in our method is solely achieved by our MMN and no multi-modal pre-training model on extra large-scale video datasets is used.
\tableSTVG

\section{Conclusion}
In this paper, we propose the Mutual Matching Network (MMN) in the metric-learning prospective for temporal grounding. Particularly, we first propose to use the auxiliary task of {\em mutual matching} which asks the model to select the correct sentence in a constructed negative sentence set for video moments in addition to existing supervisions. By leveraging the powerful textual negatives, more discriminative features for both modalities are learned by cross-modal mutual matching. Moreover, inter-video negatives in temporal grounding are effectively exploited for the first time in our method. With this simple, effective and efficient framework, we achieve state-of-the-art performance on four challenging video grounding benchmarks: Charades-STA, TACoS, ActivityNet-Captions, and HC-STVG.

\section*{Acknowledgements} 
This work is supported by the National Science Foundation of China (No. 62076119, No. 61921006), Program for Innovative Talents and Entrepreneur in Jiangsu Province, and Collaborative Innovation Center of Novel Software Technology and Industrialization. Authors would like to thank Yixuan Li for her effort in the HC-STVG challenge.
\clearpage
\bibliographystyle{aaai22}
\bibliography{aaai22}
\end{document}
