\documentclass[english,a4paper,10pt,final]{article}

\pdfpagewidth=8.5in
\pdfpageheight=11in

\setlength{\textwidth}{7.0in}
\setlength{\textheight}{9.10in}
\setlength{\hoffset}{-0.9in}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{array}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{subfigure}
\usepackage{algorithm}
\usepackage{babel}
\usepackage{authblk}

\makeatletter

\numberwithin{equation}{section}
\numberwithin{figure}{section}

\newtheorem{remark}{Remark}

\date{}
\makeatother




\begin{document}
\title{\bf Proportional-Integral Clock Synchronization \\ in Wireless Sensor Networks}

\author[1]{Kasım~Sinan~Yıldırım}
\author[2]{Ruggero~Carli}
\author[2]{Luca~Schenato}
\affil[1]{Department of Computer Engineering, Ege University, 35100, Bornova, İzmir, TURKEY, sinan.yildirim@ege.edu.tr}
\affil[2]{Department of Information Engineering, University of Padova, ITALY, \{carlirug,schenato\}@dei.unipd.it}

\maketitle

\begin{abstract}
In this article, we present a new control theoretic distributed time synchronization algorithm, named PISync, in order to synchronize sensor nodes in Wireless Sensor Networks (WSNs). PISync algorithm is based on a Proportional-Integral (PI) controller. It applies a proportional feedback (P) and an integral feedback (I) on the local measured synchronization errors to compensate the differences between the clock offsets and clock speeds. We present practical flooding-based and fully distributed protocol implementations of the PISync algorithm, and we provide theoretical analysis to highlight the benefits of this approach in terms of improved steady state error and scalability as compared to existing synchronization algorithms. We show through real-world experiments and simulations that PISync protocols have several advantages over existing protocols in the WSN literature, namely no need for memory allocation, minimal CPU overhead and code size independent of network size and topology, and graceful performance degradation in terms of network size.
\end{abstract}

\maketitle

\section{Introduction}

Built-in clocks of the sensor nodes are not sufficient alone to provide synchronized time notion in Wireless Sensor Networks (WSNs). This phenomenon arises from the fact that their crystal oscillators operate at different and unstable frequencies. On the other hand, being able to access to synchronized time notion is a prerequisite for the correct and efficient operation of WSNs \cite{Akyildiz2010}. In order to fulfil this requirement, sensor nodes are required to participate in \emph{time synchronization} periodically. At each synchronization round, they communicate with the other nodes to exchange time information and they process the received data to establish a function which represents the network-wide global time. However, various error sources arising from communication prevent from receiving time information perfectly. Moreover, limited resources and capabilities of the sensor nodes restrict the frequency of the communication as well as the computational and memory overhead of the information processing. These characteristics make time synchronization still an interesting and challenging problem in WSNs. Indeed, 
In the next section we provide a literature review of clock-synchronization in WSNs and the current state-of-the-art.  

\subsection{Literature review}

There is an ample body of studies in time synchronization in WSNs from both theoretical and practical perspectives. An initial gross classification of these works can be based whether the network synchronization is based on one or more reference nodes, referred as \emph{flooding-based} methods, or based on no reference node at all, referred as  \emph{fully-distributed} methods.

In the flooding-based methods, there is typically one node that is selected as reference for all other nodes either a-priori, as for example when stable time sources such as Coordinated Universal Time (UTC) time are available, or on-the-fly via leader election procedures. The reference node periodically disseminates its stable information through the network via \emph{flooding}, which is a common method to achieve network-wide time synchronization in WSNs thanks to its being a basic and simple communication primitive. Since there is no demand to construct and maintain a topological infrastructure, e.g. a spanning tree, flooding is also robust to dynamic topological changes. Each receiver node calculates its offset and frequency difference with respect to the received time information, and synchronizes its clock. There are several successful implementations of time synchronization with this simple and robust approach, as presented in \cite{Maroti2004,RATS,Lenzen2009Optimal,Schmid:2010,Yildirim2013,Yildirim2013_2}. These implementations calculate the offset and frequency difference of the clocks by employing \emph{least-squares regression} \cite{Wu:10}. However, this method has considerable overhead in terms of computation and memory allocation. In fact, to perform regression, each sensor node is required to keep a repository to store the received time information of the reference node. The capacity of this repository is determined in advance and it has a great impact on the estimation accuracy. Each time a new message is received, the most outdated received time information is deleted and the recently received information is stored at the repository. Then, least-squares regression is performed on the stored information to calculate the least-squares line. These calculation steps consist of many floating point multiplication and division operations, which are computationally expensive for sensor nodes \cite{Yildirim2012}. Moreover, recent studies  \cite{Lenzen2009Optimal,Yildirim2013} showed that slow-propagation of time information together with least-squares regression leads to exponentially increasing synchronization error with the network diameter, i.e. poor performance scalability. In order to improve synchronization performance and scalability, fast and reliable flooding is proposed in \cite{Lenzen2009Optimal}. However, the establishment of an efficient fast flooding architecture in WSNs is difficult \cite{FlashFlooding2009}. Recent solutions employ fragile hardware dependent software steps \cite{ferrari2011efficient}, demand different hardware capabilities \cite{kuo2012reconfiguring} and have serious scalability problems \cite{wang2012exploiting,wang2012triggercast}. As an alternative solution to the method of least-squares, which has several drawbacks in WSNs as we mentioned, the method of maximum likelihood estimation \cite{Chaudhari:10,Leng:11,Cao:13}, belief propagation \cite{Leng:11b}, and convex closure \cite{Berthaud:00} have also been proposed. However, these methods also have considerable computational and memory overheads and it is not clear whether they can be practically implemented in real WSNs since only simulative results are presented.

Differently from the flooding-based methods, in the \emph{fully-distributed} methods all sensor nodes interact only with their direct neighbours in a peer-to-peer fashion and there is no any special node which acts as a time reference for the other nodes. In mobile WSNs which are prone to frequent topological changes and node failures, \emph{fully-distributed} methods are the standard choice. Moreover, these methods are known to provide better synchronization among the neighbouring nodes since they share the so-called \textit{gradient property} \cite{Fan2006}, which is more appropriate for protocols such as TDMA. There are several fully distributed time synchronization protocols in the literature \cite{Firefly,Sommer2009Gradient,SchenatoFiorentin:2011,Choi:12}, however, they have fundamental shortcomings. The Reachback Firefly Algorithm (RFA)\cite{Firefly} can only provide synchronicity, hence it only ensures that sensor nodes agree on a firing period and phase but does not provide access to common notion of time. On the other hand, Average TimeSynch (ATS) \cite{SchenatoFiorentin:2011}, Gradient Time Synchronization Protocol (GTSP)\cite{Sommer2009Gradient} and Distributed Asynchronous Clock
Synchronization (DCS) \cite{Choi:12} require sensor nodes to store the time information of the neighbouring nodes for achieving distributed agreement. This requirement introduces a significant memory overhead in dense networks thus making infeasible for sensor nodes to keep track of all their neighbours due to their memory constraints. In fact, they suffer from the crucial problem of deciding which neighbours to keep track and which ones to discard, which decreases their scalability in dense networks. Moreover, their computation overhead is quite high and they require large amount of information to be exchanged among the neighbouring nodes. 

Finally, almost all protocols available in the literature, both in flooding-based methods and in fully distributed methods, and in particular those based on least-squares regression, cast the problem of time synchronization as a problem of the simultaneous estimation of relative clock drift and offset among two or more nodes, which give rise to time-synchronization algorithms  which are non-linear in the measurement noise. As a consequence, the effect of various error sources appear in the time synchronization error dynamics as multiplicative noise which makes the global time synchronization error to approximately grow exponentially with the diameter of the network, thus with poor performance scaling properties. Recently, a different approach based on control theory has been independently proposed in \cite{Chen:10} and \cite{Carli_2011} where synchronization is achieved by using linear feedback on the measured local synchronization error. The major advantage of this approach is that the  error sources appear as additive noise so that the global time synchronization error approximately grows as the square root of the network diameter.    

 
\subsection{Contributions}

The existing time synchronization solutions for WSNs presented above suffer from two common drawbacks: high overhead in terms of computation and memory, and poor scalability in terms of global synchronization error. In this article, inspired by \cite{Carli_2011}, we propose and implement a control theoretic approach to time synchronization in WSNs which helps in eliminating these drawbacks.
In particular, we devise a distributed synchronization algorithm, named \textit{PISync}, which compensates the clock offsets and the differences in clock speeds based on a \textit{Proportional-Integral (PI) controller}. Roughly speaking,  nodes transmit their current time information periodically and upon receiving a synchronization message, they achieve time synchronization by applying a proportional feedback (P) and an integral feedback (I) on measured relative offsets  to compensate the differences between the clock offsets and the clock speeds respectively. PISync is linear, simple and easy to implement, which makes it suitable for both flooding-based and fully-distributed architectures. More precisely, we present specific time synchronization protocols based on PISync algorithm for the flooding-based scenario and for the fully distributed scenario. We provide simple design guidelines for the proportional (P)  and integral (I) feedback gains. We introduce an adaptive strategy for the integral gain in order to balance the trade-off between the convergence time and the steady state error. Moreover, we present a theoretical performance analysis of the proposed protocols and we show experimental comparisons with some popular time synchronization protocols, namely GTSP  \cite{Sommer2009Gradient}, FTSP \cite{Maroti2004} and PulseSync \cite{Lenzen2009Optimal}, based on a  grid and on a 20-node line WSNs.

At the light of our theoretical analysis, experiments and simulations, the main advantages of the PISync protocols over the existing time synchronization protocols available in the WSNs literature, can be summarized as follows:
\begin{itemize}
\item \textbf{memory overhead}: all PISync protocols very small overhead in terms of main memory allocation, even in networks having very big densities. They do not store any distinct time information. 
\item \textbf{code footprint}: the code size of PISync protocols is very small, which provides remarkable gain for sensor nodes where flash memory (ROM) is a scarce resource.
\item \textbf{CPU overhead}: all PISync protocols are lightweight in terms of CPU usage. They have more than 97\% less computation overhead compared to the available protocols.
\item \textbf{blind communication}: all PISync protocols work in a completely blind manner and do not keep track of the neighbouring nodes. Hence, they are more robust to topological changes, node failures and new nodes joining into the network compared to existing approaches. This feature makes them also particularly suitable for mobile applications and for privacy-aware applications where nodes do not want to disclose their IDs.
\item \textbf{performance scalability:} all PISync protocols are scalable in terms of steady state global synchronization error, which grows with the square root of the network diameter.
\end{itemize}

\subsection{Organization of the Article}

The remainder of this article is organized as follows. In Section \ref{sec:System-Model}, we propose a system model for the clock of the nodes and the communication network in order to present and analyze our algorithms and protocols. We present a control theoretic analysis of the PISync algorithm and compare it with the classical least-squares based time synchronization in Section \ref{sec:PI}. In Section \ref{sec:Protocols}, we present and analyze fully distributed and flooding based time synchronization protocols based on PISync. Our implementation details, experimental evaluation and simulation results are presented in Section \ref{sec:Experiments}. Finally, conclusions and future research directions are presented in Section \ref{sec:Conclusion-and-Future}.

\section{System Model}

\label{sec:System-Model}

In this section, we propose a hardware clock model, a logical clock model and a network model which we use for the presentation and analysis of the synchronization algorithms in this article.

\subsection{Hardware Clock Model}

We start by proposing a model of each built-in \textbf{hardware clock}, i.e. local clock, of the sensor nodes which is simple enough but which captures the main difficulty of the synchronization problem, namely, the fact that the time is an unknown variable, which has to be estimated. 

Assume that each hardware clock has an oscillator capable to produce an event at time , . The hardware clock has to use these ticks in order to estimate the time. The following cumulative function well describes the time evolution of the tick counter that can be implemented in the hardware clock
  
where  is the unit step function, i.e.,

In this way the counter output is the step shaped function shown in Figure \ref{fig:sdit}.
Notice that the function

can be interpreted as the oscillator frequency at time  and that . Typically a nominal value  of  is known together with a lower bound  
 and an upper bound  such that  where .

\begin{figure}
\begin{center}
\begin{tabular}{c}
\includegraphics[width=0.5\columnwidth]{figures/fucts1}
\end{tabular}
\caption{\label{fig:sdit} The graphs of  (continuous line) and of its approximation  (dashed line).  Due to their dynamic frequencies, the tick events of clocks do not occur at regular time intervals. Hence,  is a step shaped function.}
\end{center}
\end{figure}

\subsection{Logical Clock Model}

Collecting time information from the neighboring nodes, each node calculates a logical clock which represents the network-wide global time. Let  be the latest time at which the logical clock has been updated. From the counter, one can build a time estimate  at any time  by letting

where  is an estimate of the oscillation period  in the period . It is reasonable to initialize  to . The time estimate  can be considered as the value of the \textbf{logical clock} at time  and represents the network-wide global time. It can be observed that the estimate  , which we will also refer to as the \textbf{rate multiplier}, represents the progress rate (speed) of the logical clock.

Both the logical clock value  and its progress rate  can be modified when the node obtains information allowing it to improve its time and oscillator frequency estimates. Assume that these corrections are applied at time instants , where , called updating time instants. In this case we have \footnote{Given the time , with the symbol  we mean the time instant just after .}

where  and  denote the control inputs applied to  and , respectively. For ,  is updated according to \eqref{eq:hat_t}, while  is left unchanged. Specifically,


\subsection{Network Model}

We represent a WSN  by a graph  where the vertex set  represents sensor nodes. For each node , let  be the evolution of the oscillator frequency of the hardware clock of that node. Moreover, for , let  denote the state of the logical clock of node . We assume that the nodes can exchange the states of their logical clocks according to a graph  where  whenever the node  can send its logical clock state to the node .  
 
Specifically, each node , broadcasts its estimate , i.e., the value of its logical clock, to its neighbours at time instants , , and can use any information it receives from the neighbouring nodes to apply a control at the time instants , .

More precisely

where  is the control action applied at time .
Moreover for  we assume that the state  is updated according to~\eqref{eq:Update}. In Figure \ref{fig:Updating} we depict the behaviour of the logical clock  and of the rate multiplier .


\begin{figure}[!t]
\centering
\includegraphics[scale=0.35]{figures/Updating}\,\,\,\,\includegraphics[scale=0.35]{figures/Updating1}
\caption{Behavior of the logical clock  ({\bf left panel}). Behavior of the rate multiplier  ({\bf right panel}).
}
\label{fig:Updating}
\end{figure}


\section{PI Synchronization (PISync) Algorithm }
\label{sec:PI}

In this section, we devise a synchronization algorithm suitable for WSNs, named PISync, which synchronizes the logical clocks of the nodes based on a Proportional-Integral (PI) Controller. 
To make easier the description of the algorithm and of its stability properties we assume in the Section that the oscillator frequencies  are constant, i.e., for ,  for all . 

\vspace{0.1cm}

From \eqref{eq:hat_t}, it follows that~\eqref{eq:Update}, for the -th logical clock, can be equivalently rewritten as

 
 The objective is to find a control strategy yielding the clock synchronization, namely such that there exist constants  and  such that synchronization errors 

converge  to zero or remain small. In other words, the objective is to synchronize the variables , , namely, to find a law which allows the logical clocks of the nodes to obtain the same time estimate.

Now, without loss of generality, assume that node  transmits, at a generic transmission time , the information  to node . Due to transmission delays, 
the information  is received by node  at a delayed time

where  is a non-negative real number representing the deliver delay between  and  (see Figure \ref{fig:tx}).

\begin{figure}
\begin{center}
\begin{tabular}{c}
\includegraphics[width=0.6\columnwidth]{figures/Transmission.jpg}
\end{tabular}
\caption{\label{fig:tx}The transmission delay during the communication between nodes i and j.}
\end{center}
\end{figure}

Based on the information received, node  instantaneously applies to its current state  the following correction

where ,  are two control parameters to be designed.

Observe that, according to the above model, we have that , therefore we can write:


The update rule above indicates that only the \textit{estimated synchronization error}  between nodes  and , and the two control parameters  are used to update the value and the rate multiplier of the logical clock of node . Hence, PISync does not require any extra information.

\begin{remark}
Observe that the control law above introduced can be seen as a PI controller where  and  represent, respectively, the proportional and the integral part. It is worth mentioning that this strategy has been inspired by the PI consensus controller strategy proposed in \cite{Carli_2011}. However in \cite{Carli_2011} the synchronization protocol is analyzed under the following assumptions:
\begin{enumerate}
\item all the nodes perform the transmitting and the updating actions synchronously (\emph{synchronous implementation}), i.e.,  for all  and for all , where  is a pre-assigned sampling time;
\item the control parameters  and  are assumed constant;
\item the hardware frequencies ,  are quite close to each other.
\end{enumerate}

Under the above assumptions, the authors in \cite{Carli_2011} provided sufficient conditions on  and  which guarantee that the synchronization is reached. Specifically they showed that it must hold  and  where  depends on some properties of the Laplacian matrix associated to the graph , which can be calculated only having a global knowledge of the network. A similar control oriented approach has been proposed in \cite{Chen:10} where the parameters  and  provided in that work  play the role of  and  here. However, the design guidelines for   and  are derived by considering only two nodes, therefore as shown in \cite{Carli_2011}, they might give rise to unstable behaviours of the algorithm for large scale networks. 

The goal of this paper is to extend the control-based synchronization strategy with PI feedback to the more realistic scenarios where the nodes do not communicate synchronously with each other, and to be suitable to time-varying scenarios where new nodes can be added or existing nodes can die and move. In particular we endow each node with adaptive laws for  the real time design of the parameters  and  based only on the information coming from their neighbouring nodes. Moreover, the design strategy for  and  provided in this work, is independent of the network topology and revolves the potential instability problem present in the protocol suggested in \cite{Chen:10}.

\end{remark}

\subsection{Pairwise control-based time synchronization: rate of converge}
\label{subsec:pairwise_control}

Before providing some design guidelines for the parameters  and  we would like to give an intuitive explanation of the proposed control-based solution for time synchronization and highlight the major differences with the standard solutions available in the literature based on least-squares. To this purpose, we assume there are only two nodes  and , where  plays the role of the reference clock. Moreover, without loss of generality, we assume that node  is perfectly synchronized with respect to the absolute time, i.e.  and , therefore Eqn.\eqref{eq:SynchErrors} becomes

We also assume that the reference node  periodically with period  transmits a message with its own logical clock, which in this case coincides with the hardware clock, and that there is no delay, i.e. , therefore we have



In the context of \textbf{control-based time synchronization}, with a little abuse of notation, we denote  and similarly for  and , therefore the update in Eqn.\eqref{eq:Update_i1} becomes

By substituting the previous equations into Eqn.~\ref{eq:UpdateDelayed-f_constant} with  and by recalling the definition of  and that , we obtain a state space description of the error dynamics:


where  and  are arbitrary. By inspecting these dynamics, it is immediate to see that the dynamics of  is a discrete time integrator driven by the error signal  where the parameter  is referred as the \emph{integrator feedback gain}. The dynamics of the synchronization error  are affected by the output of the integrator  and includes also a proportional feedback on itself via the \emph{proportional feedback gain} . If the gains  and  are chosen such that the matrix  is strictly stable, i.e. its two eigenvalues have modulus strictly smaller than unity, then the dynamical system must converge asymptotically to a steady state, i.e.  and  which must satisfy the following expressions:

which shows that time synchronization is eventually achieved, since . The role of the proportional gain is to compensate for the different initial clock offset , while the role of the integrator gain is to compensate for different clock drifts , which are both not directly measurable. In fact, if the integrator is disabled and the proportional gain is chosen to keep the dynamics stable, then the a steady state error is present:

Note that the steady state error is directly proportional to both the difference of the relative clock speed and the transmission period . In fact, if all clocks have the same drift, i.e. , then the proportional feedback alone would suffice to drive the synchronization error to zero. 

As mentioned above, the PI synchronization strategy achieves synchronization only if the matrix  is stable. Moreover, linear dynamical system theory also shows that the rate of convergence is given by the eigenvalues of such matrix. The eigenvalues of the matrix  are given by the solution of the following second order systems:

We then would like to find the set of values for the gains  and  for which these roots are stable, i.e., . After some simple calculations, we obtain:

which shows that if  is positive but sufficiently small, then synchronization is achieved.

 It is also possible to find the optimal value for  for any fixed and feasible value of  to maximize the rate of convergence, which after some simple calculations is given by:

From this expression it is clear that the integrator gain  should be decreased if the communication period  is increased, in order to maintain the same eigenvalues of the matrix , i.e., if the same rate of convergence is desired. 
It is also of interest, as shown in the next section, to compute the rate of convergence for the special case , which gives



\begin{figure} 
\center

\includegraphics[scale=0.4]{figures/alphaVSiterations} 

\caption{\label{fig:alphaVSiterations} Plot of the trade-off between the value of  (rescaled by ) and the number of iterations required by the algorithm to reduce the error of 90\% of its initial value. The plot has been obtained assuming , sec and .}
\end{figure}

In Figure \ref{fig:alphaVSiterations} we plotted the trade-off between the value of  (rescaled by ) and the number of iterations required by the algorithm to reduce the error of 90\% of its initial value. 


\subsection{Pairwise control-based time synchronization: steady-state error}
\label{sec:noise2}

When dealing with wireless devices, there are several noise sources, like quantization effects, presence of unreliable communication channels, measurements errors, time-varying oscillator frequencies, transmission delays, which prevent any synchronization algorithm from reaching the exact network synchronization. 

In this section we consider the presence of transmission noise and time-varying frequencies and we compute their effect in the steady state error. Specifically, assume that node  transmits, at time , the information  to node . Due to quantization effects and in the presence of unreliable channels, 
the information received by node  can be conveniently modelled as

where  is a zero mean white noise of variance , i.e., and 
where  is an adimensional parameters which is typically in the order of unity.
The standing assumption is that noises related to different transmissions are independent from each other.  Additionally we assume the frequency of a generic node  at time  to be given by 

where  is a zero mean-noise uniformly distributed in  with corresponding variance 
where  is an adimensional parameter which refers to the typical relative frequency change over the course of one synchronization period .


Now, without loss of generality, let us assume that node  is the reference node and let node  be a child of node . Next, we provide a mean-square analysis to characterize the synchronization error between node  and node . 
In our analysis we assume that , where  is a given sampling time and . Moreover we assume that the transmission delays are negligible, thus . For the sake of the notational convenience let , , , , for . For simplicity assume that  for all . We have that

and 

where

We further consider the approximation  since the true frequencies are very close to the nominal value . Finally, we set . The reason for choosing  comes from the observation provided in the previous section that with , then the time of convergence is minimized, and we interested in computing the steady state error as a function of .
Recalling that , and by defining , then after some straightforward calculation we get: 


and

Let  and . It follows

Hence

and, in turn,

From the previous expression it is clear the error is an increasing function of  and that even for  there is still some non-zero steady state error.


\begin{figure} 
\center
\includegraphics[scale=0.3]{figures/errorVSalpha} 
\caption{\label{fig:errorVSalpha} Plot of the trade-off between the mean-square asymptotic error and the value of  (re-scaled by ). The plot has been obtained assuming , sec, , sec and .}
\end{figure}

In Figure \ref{fig:errorVSalpha}  we plotted the trade-off between the mean-square asymptotic error and the value of  (re-scaled by ). One can see that the smaller the value of  is, the smaller the value of the error is. However, as seen in the previous Figure, as  approaches , the number of iterations required to converge becomes larger and larger. 


\subsection{Control-based vs least-squares-based time synchronization}
\label{subsec:control_vs_least_squares}


Under the same hypothesis, we now would like to provide a similar intuitive presentation of the \textbf{least-squares-based time synchronization}.  In this context, the equations for the logical clocks of the two nodes can be written as:


where  and   have to be designed to drive the synchronization error  to zero. We assume that  and  are kept constant for the first  steps so that we can write

Under least-square-based synchronization the values for the compensating parameters is given by

Note that  and  are known to node  as long as node  transmits either  or  since  is known. In the specific case when , the solution is given by:

If no measurements errors are considered, then the previous solutions provide exact synchronization, i.e., . In practice, this is not the case due, for example, to transmission delay or quantization, and therefore the previous procedure has to be repeated periodically. 

At the light of the derivations above about the control-based and least-squares-based time synchronization a number of observations are in order:
\begin{itemize}
\item Under the ideal scenarios above, the rate of convergence for the control-based synchronization is asymptotic, while using least-squares can be achieved in finite time. The better performance in terms of convergence rate for the least-squares-based methods continues to hold also in the more general WSNs scenarios as shown in Section \ref{sec:Experiments}.
\item If measurements errors are present, due for example to transmission delay or quantization, then we have to substitute   into Eqn.~\eqref{eqn:state_space} and  into Eqn.~\eqref{eqn:LS}, where 's represent the measurement noise at iteration . As the consequence the time synchronization error dynamics for the control-based strategy becomes:

while the equation  for the first iteration in the least-squares approach becomes

These two equations clearly show that in control-based approach the disturbances enter the synchronization error dynamics linearly, while in the least-square dynamics non-linearly. One of the consequences is that, roughly speaking, under the first approach the global time synchronization error grows as the square root of the diameter  of the WSN, i.e., , while under the second it grows exponentially when , i.e.,
 for some . This fact will be evident when comparing FloodPISynch and FTSP in Section~\ref{sec:Experiments}. Also note that in the control-based approach the measurement errors are amplified by the control gains , which therefore should be kept small to reduce the steady state error. However, this comes at the price of slower convergence rate since the modulus of the largest eigenvalue of  approaches unity for .

\item Memory and CPU requirements for the control-based strategy are minimal since only two additions and two multiplication are needed and no data storage is necessary. Differently, in the least-squares-based approach it is necessary to store  measurements and perform  multiplications,  additions and a  matrix inversion and multiplication. This difference becomes even more pronounced in the fully-distributed scenario in WSN since, while in the control-based strategy memory and CPU requirements are the same, in the least-squares strategy each node has to store  measurements from each of its neighbours. If we indicate with  the average number of neighbours per node, then memory and CPU complexity for the least-squares strategy is  and , respectively, while it is only  for the control-based strategy. 

\item When considering general WSNs with a large number of nodes, Eqn.~\eqref{eqn:alpha_beta} might not be valid since they have been derived by considering only two nodes. Indeed, under an ideal scenario of synchronized communication in the WSN, the stability condition becomes

\end{itemize} 
where  is a constant smaller than unity that depends on the topology of the WSN and can be arbitrarily small. As a consequence it is not possible to obtain ``universal'' stabilizing parameters that are guaranteed to work for any WSN. To overcome this problem, we will propose an \emph{adaptive Integral gain} that is disabled if the local synchronization errors are large, i.e.,  if . Similarly as shown above, it is possible to show that if  and , the synchronization error dynamics is stable, although a steady state error is present due to the uncompensated clock speeds. Nonetheless, this guarantees stability for any network topology.   


\subsection{Design of the control parameters  and }
\label{sec:parameter-adaptation}

In this section we provide an adaptive law for the real-time design of the parameter  and , , based only on the local information that node  receives from its neighbours. 

We start with the \textbf{proportional gain} . Loosely speaking, the role of input  in \eqref{eq:control-u} is that of reducing the distance between the logical clock  and the logical clock . Observe that the first equation of \eqref{eq:Update_i1} can be conveniently rewritten as

If 
, then  turns out to be a convex combination of  and  and it is easy to verify that

Although the condition  is more restrictive than ~\eqref{eqn:alpha_beta}, this has the advantage to guarantee that the global time synchronization error cannot increase for any network topology, i.e.

 and therefore cannot lead to instability. Basically the role of the parameter  is to reduce the difference between the logical clock of the node transmitting information and the node receiving that information.
 In the next section, when introducing AvgPISynch, FloodPISync and PulsePISync protocols we will assume that  is constant and such that , for all , i.e.
 

For what concerns the design of the \textbf{integrator gain} , an adaptive strategy to improve both convergence rate and stability is proposed. As discussed in the previous Section~\ref{subsec:control_vs_least_squares} the main role of the integrator is to compensate for the different clock speeds, whose absence would otherwise give rise to the typical saw-tooth behaviour of the time synchronization error. However, the integrator feedback comes at the price of few limitations. First, the rate of convergence is typically reduced as compared to a proportional feedback only. Second, if  is too large, it might drive the algorithm to instability. Third, integrator feedback suffers from the so called windup problem, i.e. the integrator is active even when the measured synchronization is not due to the different clock speeds, but only to large offset differences (see Section~3.5 in \cite{Astrom:95}). Consider for example, the same scenario as in the previous subsection with only two nodes where  and , i.e. node  is slower than node , but the initial clock offsets are such that . As a consequence, if node  sends a message to node , then update at node  is such that , i.e. the speed of the logical clock  is increased even further and initially the time synchronization error between these two nodes is likely to increase rather than decrease. Eventually, this situation is reversed and the PI controller will synchronize the two clocks, but the initial transient performance is degraded. To overcome this limitation, we propose to use an adaptive integrator gain , whose value is calculated by considering the derivative of the local synchronization error  and the derivative of the rate multiplier   according to the following equation:
 where 

 



The intuition behind this approach can be explained as follows: As long as  is greater than , the integrator is disabled, since the observed error is mainly due to large initial offsets. This typically occurs at the on start of the synchronization. Whenever it is required to turn on the integrator since , then  is set to . As shown in Section~\ref{subsec:pairwise_control}, the pair  provides the faster rate of convergence. However, this choice also gives rise to the largest steady state error as shown in Section~\ref{sec:noise2}, therefore we would need a strategy that would gradually reduce the value of the gain  at steady state. The multiplicative parameter  serves this objective. In fact, if the observed error  is due to uncompensated different clock differences, which typically occurs during the transient or after a sudden change in frequency of one or more nodes, then we have  and , therefore , which implies that , i.e. the integrator gain is increased to quickly compensate clock differences. After the algorithm has converged to steady state, then the error will typically appear as white noise, i.e.  oscillates around zero. This implies that typically  and , therefore , which implies that , i.e. the integrator gain is decreased to reduce steady-state error. The condition  if  is necessary to avoid that  remains equal to zero forever, while condition  if  to avoid division-by-zero numerical problems. Finally the saturation condition, provided by the  operator, is used to avoid to set  which as shown in Section~\ref{subsec:pairwise_control}, would negatively impact the performance by increasing both the time-to-converge and the steady-state error.  




The design problem is now shifted to designing the parameters  and . For what concerns the first two parameters , we propose to use the optimal values in terms of rate of convergence given by Eqn.~\eqref{eqn:alpha_star}, i.e.
 which under the ideal scenario would provide convergence to the steady-state in a finite number of iterations. 

As for the second parameter , we observed in the previous subsection, that any difference in the clock speeds will appear as a steady state error  if the integrator feedback is not applied. This implies that if , then the integrator will be never active and the steady state error will not decrease. Since potentially there could be two neighbouring nodes such that  by assumption, this implies that   must be bigger that the previous quantity. On the other hand, a large value for  would lead to the integrator windup, and therefore longer convergence rate, therefore  should be chosen close to its lower bound, i.e.


Figure \ref{fig:alpha_design} shows a numerical simulation using realistic values for the noise and topology. It can be seen that at the beginning  is large but later is decreases, thus providing lower steady state error. However, at time 800, we manually changed the frequency of one clock, thus simulating a sudden change in frequency. As shown in the picture, the gain  rapidly increases and quickly compensate for it.

\begin{figure} 
\center

\includegraphics[scale=0.7]{figures/alpha_design} 

\caption{\label{fig:alpha_design} Plot of the evolution of  assuming , sec and .}
\end{figure}

\section{Time Synchronization Protocols Based on PI Synchronization Algorithm}
\label{sec:Protocols}

In this section, we propose three time synchronization protocols for sensor networks based on the PISync algorithm. We first present a \textit{fully distributed} time synchronization protocol, in which each sensor node considers only the logical clock values of its neighbouring nodes to update the value and the rate of its logical clock. 
 Then, we present two \textit{flooding-based} time synchronization protocols in which each sensor node synchronizes to the clock of a reference node.

All of the protocols that we propose have very little memory requirement and are lightweight in terms of computation since they require only a few arithmetic operations to update the logical clocks, independently of network size and topology. Moreover, the amount of information to be exchanged among the sensor nodes is quite small since sensor nodes only broadcast the value of their logical clocks.

\subsection{Fully Distributed PISync Protocol: Average PISync Protocol (AvgPISync)}

In this subsection, we present a fully distributed PISync protocol, named Average PISync (\textit{AvgPISync}), which synchronizes each sensor node to their direct neighbours. The pseudo-code of this approach is presented in Algorithm \ref{alg:PI-dist}.

\begin{algorithm}

\label{alg:PI-dist}
1:~  \textbf{Initialization} \\
2:~ ; ; ; \\
3:~ \\
4:~  \textbf{Upon receiving}  \\
5:~  \\
6:~  \\
7:~\\
8:~  \textbf{Upon}  \textbf{is a multiple of } \\
9:~  \textbf{if}   \textbf{then}  \textbf{endif} \\
10:~ \\
11:~;  \\
12:~broadcast 

\caption{AvgPISync pseudo-code for node .}
\end{algorithm}

Each sensor node  maintains two variables related to its logical clock: time estimate  and  oscillator frequency estimate .\footnote{i.e. rate multiplier} Initially when the node is powered on, the time estimate is set to zero and the oscillator frequency estimate is set to , i.e. the nominal frequency, to progress the time estimate at the same speed of the hardware clock. Two other variables,  and  are required to calculate the average synchronization error to the neighboring nodes. These variables are also initalized with zero (Lines 1-2).

Whenever a synchronization message from any neighbouring node is received (Line 4), the difference of the received time estimate  and the time estimate  of node  at the reception time, i.e. \textit{clock skew},  is calculated. This value is added to the  variable (Line 5) and the number of received clock values is incremented (Line 6).

In order to inform its neighbouring nodes, node  broadcasts its up-to-date time information approximately every  seconds, where  denotes the \textit{beacon period}. Whenever the hardware clock is a multiple of  (Line 8), if the average clock skew  is greater than , i.e. the maximum difference that can be observed due to different clock speeds between subsequent synchronization message reception, then the clock skew is mainly due to the large offsets between the clocks. Otherwise, the frequency of the oscillator is required to be adjusted (Line 9). Finally, the time estimate  of node  is also updated (Line 10). It should be noted that the rate multiplier  is updated by using the strategy given by  Eqn.~\eqref{eqn:new_adaptation_rule} and parameter  is chosen to be equal to unity according to the Eqn. (\ref{eqn:alpha_star_global}). The variables  and  are initialized (Line 11)  and the clock value is broadcasted (Line 12).

It should be noted that AvgPISync operates in a completely blind fashion since it requires neither to know the sender node nor to store its time information. Hence, AvgPISync is quite robust to topological changes and it can even work efficiently in topologies with very high densities.

\subsection{Flooding Based PISync Protocols: FloodPISync and PulsePISync}

We now present a flooding based version of the PISync protocol, namely Flooding PISync Protocol (\textit{FloodPISync}),  which synchronizes each sensor node to the clock of a reference node. In FloodPISync, a dynamically elected or a predefined reference node floods its stable time information into the network. Each sensor node collects this time information, updates its clock according to PISync algorithm and also broadcasts its clock value for its neighbouring nodes to achieve network-wide time synchronization. The pseudo-code of FloodPISync is presented in Algorithm \ref{alg:PI}. For simplicity, assume that  is the predefined reference node. 

\begin{algorithm}

\label{alg:PI}
1:  \textbf{Initialization} \\
2: ; ;   \\
3: \\
4:  \textbf{Upon receiving}  \textbf{such that}  \\
5: \textbf{if}  \textbf{then} update  \textbf{endif}\\
6:    \\
7:    \\ 
8: \\
9:  \textbf{Upon}  \textbf{is a multiple of } \\
10: \textbf{if}  \textbf{then}  \textbf{endif} \\
11: broadcast 

\caption{FloodPISync pseudo-code for node  with a fixed reference node .}
\end{algorithm}

Apart from the variables of the fully distributed version, each sensor node  also maintains a sequence number  to store the largest sequence number received from the reference node. Initially when the node is powered on, the sequence number is also set to zero (Lines 1-2).

The reception of a synchronization message carrying a greater sequence number than  indicates that the reference node has initiated a new synchronization round recently (Line 4). Hence, the received time information can be considered as a fresh estimate of the reference clock. Similar to the fully distributed version, the difference of the received time estimate  and the time estimate  of node  at the reception time is considered to update the oscillator frequency estimate . Similar to AvgPISync, this update is performed by setting the parameter  equal to unity. As a consequence, the neighbours whose time information have been received later have weight 0 and they are not considered to update time estimate. The time estimate  of node  and the sequence number are also updated (Lines 6-7). 

Approximately every  seconds, solely the reference node increments its sequence number and hence initiates a new flooding round. Since all of the sensor nodes broadcast their time estimates, time information of the reference node is propagated and network-wide synchronization is achieved (Lines 9-11). 

As can be observed, the  time information of the reference node may follow many paths to reach any node  in any synchronization round. Node  takes into account only the firstly received message since it can be considered as carrying the most up-to-date estimate of the reference clock. The other messages belonging to the same synchronization round are discarded by node . In practice, this gives rise to a tree-like synchronization topology since each node tends to use data received by one its neighbouring nodes closer to the root.

In FloodPISync, each node does not propagate the time information of the reference node as soon as it receives it. Instead, nodes wait  to reach a multiple of . It has been shown in \cite{Lenzen2009Optimal} that waiting times at each hop increase the estimation errors. Moreover, in Section \ref{subsec:control_vs_least_squares}, we have shown that with least-squares the effect of various error sources appear in the time synchronization error dynamics as multiplicative noise. Hence, it is desirable to propagate fresh time information as fast as possible through the pulses. Based on this observation, we also propose a fast flooding based synchronization protocol as in \cite{Lenzen2009Optimal}, namely \textit{PulsePISync}, for architectures such as Glossy \cite{ferrari2011efficient} which allow efficient and reliable fast-flooding. The pseudo-code of PulsePISync which is quite similar to FloodPISync is presented in Algorithm \ref{alg:PulsePI}.

\begin{algorithm}

\label{alg:PulsePI}
1:  \textbf{Initialization} \\
2: ; ;   \\
3: \\
4:  \textbf{Upon receiving}  \textbf{such that}  \\
5: \textbf{if}  \textbf{then} update  \textbf{endif}\\
6:    \\
7:    \\ 
8:	 broadcast \\
9: \\
10:  \textbf{Upon}  \textbf{is a multiple of } \textbf{at the reference node }\\
11:  \\
12: broadcast 
\caption{PulsePISync pseudo-code for node  with a fixed reference node .}
\end{algorithm}

The main difference of PulsePISync from FloodPISync is that each node propagates its current time information as soon as it receives the time information of the reference node (Line 8). Hence, the propagated time information suffers only from the various error sources through communication, but not from the estimation errors. Only the reference node   broadcasts the value of its hardware clock together with its current sequence number every  seconds (Lines 10-12). Hence, each node waits an up-to-date time information from the reference node and broadcasts that information as fast as possible to reduce the estimation errors due to the waiting times.

\begin{figure} 
\center

\includegraphics[scale=0.5]{figures/comparison}

\caption{\label{fig:comparison} Comparison of the PI synchronization protocols in terms of the weights given to the neighbours during time information update.}
\end{figure}

A comparison of the fully distributed AvgPISync and flooding based FloodPISync and PulsePISync in terms of weights given to the neighbours during time information update is presented in Figure \ref{fig:comparison}. In AvgPISync, time information received from neighbouring nodes have equal weights when updating current time information, since the average synchronization error is considered. On the other hand, in flooding based protocols FloodPISync and PulsePISync, only one of the neighbouring nodes which propagated current time information of the reference node earliest during the current synchronization round is considered for this update, namely the node from which the first largest sequence number is received. All other nodes are neglected. 

\subsection{Analysis in presence of transmission noise for flooding-based PISync protocols}
\label{sec:noise}

In this section we consider the Flooding based PISync Protocol in presence of transmission noise and time-varying frequencies. We restrict our analysis to a line graph (see Figure \ref{fig:topology}) with  nodes, labelled  through . We assume that node  receives information from node  and that node  is the reference node.

We denote the different sources of noises with the same formalism adopted in Section \ref{sec:noise2}. In particular we assume that noises related to different transmissions are independent from each other, but have the same variance. 

Let us consider node . Similarly to what done in Section \ref{sec:noise2}, let  and let . The asymptotic value of the mean-square error of , i.e., , is given in \eqref{eq:AsympError}.

Now let node  be a child of node . We have that a time  node  receives from node  the information . Let us define  and, accordingly, . Since 
noises related to different transmissions are independent from each other and since all the noises are assumed to have the same variance, we obtain that

The above reasoning can be extended to any node in the graph. Specifically, denoting by  the asymptotic variance of the synchronization error between node  and node , we have that

where  is the distance on the graph between node  and node . In other words the synchronization error variance (resp., the standard deviation) grows linearly (resp., as the square root) with the distance to the reference node.
Some remarks are now in order:

\begin{itemize}
\item Observe that, from Eqn. \eqref{eq:AsympError}, it follows that, the smaller the value of  is, the smaller the value of the asymptotic synchronization error is. Nonetheless it is worth recalling that, as  approaches , the algorithm becomes slower and slower to converge. 

\item In the previous analysis, we considered only transmission errors and time-varying frequencies. 
However, our analysis could be extended to include also the presence of transmission delays, in the case an estimate of  the mean of the delays is available and a correcting term trying to compensate the effects due to the delivery delay is applied in the control law. More precisely, observe that

where the unknown quantities  and  might be suitably approximated by  and , being  a known estimate of the mean of the delay \footnote{The information about 
 might be available from a-priori statistical studies as suggested in \cite{ZZ-PPC-TH:09}.}. In this way we would obtain

and the control inputs in \eqref{eq:control-u} might be modified as

Observe that, in the above model, the transmission delays can be treated as noises of zero-mean.
\item  A similar analysis might be provided also for the AvgPISync algorithm following Section V of \cite{Carli_2011}. However in \cite{Carli_2011}, few unrealistic conditions are assumed, i.e., all the transmitting and updating steps are performed synchronously and all the clocks have the same frequency. To generalize this analysis to the AvgPISync algorithm is quite complicated and goes beyond the scope of this paper. We limit ourselves to the numerical results reported in Section \ref{subsec:AvgPISync-GTSP}.
\end{itemize}

\section{Real-world Experiments and Simulations}
\label{sec:Experiments}

In this section, we present an experimental evaluation of AvgPISync, FloodPISync and PulsePISync in our testbed of 20 sensor nodes. We first present the experimental results of the fully distributed protocol AvgPISync and its comparison with another fully distributed protocol GTSP. Then, we present experimental results related to the flooding based protocols FloodPISync and PulsePISync, and their comparison with other flooding based time synchronization protocols FTSP and PulseSync. Finally, we present a comparison of these protocols through simulations in terms of their scalability by considering networks with larger diameters.

\subsection{Hardware Platform and Implementation Details}

The experimental platform of our experiments is based on a WSN composed by MICAz nodes from Memsic\footnote{http://www.memsic.com/} instrumented with a 7.37Mhz 8-bit Atmel Atmega128L microcontroller. MICAz nodes are equipped with 4kB RAM, 128kB program flash and Chipcon CC2420 radio chip which provides a 250 kbps data rate at 2.4 GHz frequency. We used 7.37 MHz quartz oscillator on the MICAz board as the clock source for the timer used for timing measurements. The timer operates at 1/8 of that frequency and thus each timer tick occurs at approximately every 921 kHz, i.e. approximately 1 microsecond.

For our real-world experiments, we used the publicly available implementation of FTSP coming with TinyOS 2.1.2.\footnote{http://www.tinyos.net} We also implemented PulseSync, GTSP, AvgPISync, FloodPISync and PulsePISync ourselves in TinyOS. The CC2420 transceiver on the MICAz board has the capability to timestamp synchronization packets at MAC layer  with the timer used for timing measurements. This is a well-known method that increases the quality of time synchronization by reducing the effect of the non-deterministic error sources arising from communication \cite{Maroti2004}. We used the packet level time synchronization interfaces provided by TinyOS to timestamp synchronization messages at MAC layer \cite{Packet-level-time-synchronization:2008}.

\subsection{Testbed Setup}

In our testbed, sensor nodes are placed in the communication range of a \emph{reference broadcaster}. This node periodically broadcasts \emph{query packets} which will be received approximately at the same time by all nodes. Each sensor node transmits a \emph{reply packet} carrying its logical clock value at the reception time of the query packet. A \emph{base station} node connected to the PC collects these reply packets and forwards them to the serial port for logging. At the end of the experiments, we analyze the logged experimental data. 

\begin{figure}
\center

\includegraphics[scale=0.4]{figures/topology}

\caption{\label{fig:topology} The placement of sensor nodes in order to construct the line
and grid topologies for the experiments.}
\end{figure} 

We constructed line and grid topologies by forcing nodes to accept only the packets from their neighbouring nodes in the corresponding topology. Sensor nodes discard the packets from the other nodes. The grid topology has been selected to evaluate performance of all protocols in moderately dense WSN. In this scenario, fully-distributed time synchronization approaches are likely to perform better than flooding-based synchronization approaches since contention, congestion and packet collisions are more likely to occur in this topology. Moreover, experimental results of the line topology are also important to observe the scalability of both fully distributed and flooding based schemes, since it has been shown in \cite{Lenzen2009Optimal} that the synchronization error grows with the network diameter. The placement of sensor nodes during the experiments is shown in Figure \ref{fig:topology}.





\subsection{Experimental Parameters}

During our experiments, the beacon period  was 30 seconds for all of the protocols. For FTSP, GTSP and PulseSync, the capacity of the least-squares regression table was fixed to  elements. Each sensor node was programmed by assigning an unique ID between 1 and 20. Since it takes several rounds for FTSP to dynamically elect the reference node, we modified it to work with a fixed reference node. For the flooding based protocols PulseSync, FTSP, FloodPISync and PulsePISync, the node with ID "1" has been set as reference node for the other nodes in the network. At the beginning of the experiments, sensor nodes are powered at random times within the first 2 minutes of operation and each experiment took 10000 seconds (approximately 2.8 hours).

The adaptation of the  parameter is a major point to consider in the practical implementation of the PISync algorithm. We followed the strategy presented in Section \ref{sec:parameter-adaptation} to adjust its value. Since the reported drift of the sensor nodes is defined  ppm, i.e.  and the communication period  is 30 seconds, then according to  Eqn.~\eqref{eqn:emax_lower_bound} we should have  and according to Eqn.~\eqref{eqn:alpha_star_global} we should have

Table \ref{tab:PI-Experimental-Parameters} summarizes the parameter values used for the PI synchronization protocols during our experiments.

\begin{table}

\center
\caption{Experimental Parameters used for the PI synchronization protocols.\label{tab:PI-Experimental-Parameters}}

\begin{tabular}{cccc}
 &  &  &   \\ 
1 & 30 seconds & 0.006 seconds &  
\end{tabular}


\end{table}

\subsection{Synchronization Performance metrics}

In order to evaluate the synchronization performance, we consider local and global synchronization errors. More precisely, we define \textit{instantaneous global skew of a node} as the maximum difference of the logical clock values observed between that node and any other arbitrary node at a given time instant. Similarly, we define \textit{instantaneous local skew of a node} as the maximum difference of the logical clocks observed between that node and any of its neighbouring nodes at a given time instant. 

By considering instantaneous global and local skews of nodes, we used the following metrics for the evaluation of the aforementioned protocols: 
\begin{itemize}

\item \textit{Instantaneous maximum global skew of the network} is defined as the maximum global skew among the nodes at a given time instant, i.e.  

\item \textit{Instantaneous average global skew of the network} is defined as the average of the global skews of the nodes at a given time instant, i.e. 

\item 
\textit{Instantaneous maximum local skew of the network} is defined as the maximum local skew among the nodes at a given time instant i.e. 



\item 
\textit{Instantaneous average local skew of the network} is defined as the average of the local skews of the nodes at a given time instant, i.e. 

\end{itemize}

The first two metrics  and  are important to understand how tight is the synchronization across the whole network. Typically, flooding-based synchronization methods perform well in these metrics since rapidly propagate information across the network. The second two metrics  and  are important to evaluate how tight is the synchronization among neighbours. These metrics are particularly important in applications where local synchronization rather than global synchronization is critical such as TDMA-based communication protocols. In general, fully distributed synchronization  methods perform better since they use all packets from the neighbours to set their logical clocks. Differently, in flooding-based methods only one packet is used, therefore physically closed neighbours can have larger synchronization errors than in fully distributed methods (see Fig.~\ref{fig:comparison}).


\subsection{ Effect on performance of   and  parameters}
\label{sec:eval-alpha}
Before presenting a detailed analysis of the aforementioned protocols in terms of their energy consumption, memory overhead and synchronization performance, we first evaluate the effects of the parameter   and  defined in Section \ref{sec:parameter-adaptation} in terms of rate of convergence and steady state synchronisation errors based on real-world experiments. Based on the theoretical analysis in Section~\ref{subsec:control_vs_least_squares} and Section~\ref{sec:noise}, the smaller  , the smaller the global skew (\textit{steady state error}) but the longer the time took to achieve network-wide tight synchronization (\textit{time to convergence}). In order to show this effect in practice, we performed experiments with the FloodPISync on the line topology of 20 sensor nodes with different   parameters. Table \ref{tab:alpha} presents steady state error and time to convergence values observed during these experiments. 

\begin{table}
\center
\caption{Steady state error and Time to convergence for different  values on a line topology of 20 MICAz sensor nodes executing FloodPISync.\label{tab:alpha}.}

\begin{tabular}{cccc}
 &  & \textbf{Steady state error (AGS)} & \textbf{Time to convergence} \\
   & 0.006 sec &  5 sec &  2000 sec\\ 
   & 0.006 sec &  6 sec & < 500 sec \\ 
   & 0.006 sec &  > 10000 ticks & no convergence \\ 
\end{tabular}

\end{table}

\begin{figure}
\center

\includegraphics[scale=0.5]{figures/experiments/pi/alpha_evolution2}

\caption{\label{fig:alpha-evolution} Evolution of the  parameter on the  grid topology of 20 MICAz sensor nodes for FloodPISync.}
\end{figure}

Figure \ref{fig:alpha-evolution} illustrates the evolution of the  parameters  according to the adaptive strategy given by Eqn.~\eqref{eqn:new_adaptation_rule} when  . Initially, there are big differences between the logical clocks of the sensor nodes and this difference is  mainly due to the large offsets.  Hence, the  parameters of the nodes are zero and the integrator part of the PISync algorithm is off. When the integrator is turned on
and the offset differences tends to get smaller, the  values decrease from  to very small values close to zero.

As can be observed, with the maximum value of the  we presented in Table \ref{tab:PI-Experimental-Parameters} and the adaptive strategy given by Eqn.~\eqref{eqn:new_adaptation_rule}, we get tight synchronization with a reasonable time to convergence and hence these values are quite satisfactory for the practical implementation. 


Another observation, based on experiments not reported here in the interest of space, is that choosing  to be smaller than the lower bound given by inequality of Eqn.~\eqref{eqn:emax_lower_bound} would lead to nodes that do not turn on their integrator so that different clock speeds are not compensated for, thus leading to poor steady state synchronisation performance. Conversely, a large value of  leads to a longer convergence late since integrator is turned on too early and exhibits the so called wind-up behaviour. Hence, setting  to its lower bound value as presented in inequality \ref{eqn:emax_lower_bound} is quite satisfactory in practical implementations.

In conclusion, the selection of the design parameters  and  and the adaptive gain strategy proposed in Section \ref{sec:parameter-adaptation} are quite satisfactory for the real-world implementation of the PISync protocols, as also confirmed by the experimental results in the following subsections.

\subsection{Evaluation of Energy Consumption and Memory Requirements}

We first start to evaluate PISync protocols, GTSP, FTSP and PulseSync by considering their energy consumption and memory requirements. From practical perspective, computation, communication and memory overheads of a time synchronization protocol are important points to consider. For flooding based time synchronization protocols, the amount of memory which is allocated to store collected reference time information determines their major main memory requirements (RAM). For fully distributed protocols which require to keep track of the neighbouring nodes, the amount of memory which is allocated to store neighbourhood information determines their major main memory requirements. Since the flash memory (ROM) is also a scarce resource which stores the program code of the applications, the overhead of the time synchronization protocol on the application code size must also be taken into account. An important point to consider in terms of energy consumption is the communication frequency and the length of the synchronization messages: in fact more frequent communication and longer synchronization messages give rise to higher energy consumption to transmit and receive these messages.  Moreover, if the length of the synchronization messages are short, i.e. the amount of information to be exchanged to achieve synchronization is small, then this enables to piggyback synchronization data to messages belonging to the application and other protocols. Computation overhead is another factor which affects the energy consumption: from the reception time of the reference time information to the time it is processed and logical clock is updated, considerable amount of energy is required. Table \ref{tab:Resource} summarizes the CPU overhead, synchronization message length and memory requirements of FTSP, PulseSync, GTSP and PISync protocols. 

\begin{table}
\center

\caption{CPU overhead, synchronization message length and memory requirements of FTSP, PulseSync, GTSP and PISync protocols during the experiments.  is used to denote maximum neighbourhood cardinality. \label{tab:Resource}}

\begin{tabular}{lcccc}
\multicolumn{1}{l}{} & \textbf{FTSP} & \textbf{PulseSync} & \textbf{GTSP} & \textbf{PISync} \\
\textbf{CPU Overhead }(ticks) & {\scriptsize  5440} & {\scriptsize  5440} & {\scriptsize  5610 } & {\bf \scriptsize  145 }\\
\textbf{Message Length} (bytes)& {\scriptsize 9  } & {\scriptsize 9  } & {\scriptsize 14 } & {\bf \scriptsize 4-9 }\\ 
\textbf{Main Memory Overhead} (bytes) & {\scriptsize 52  } & {\scriptsize 52 } & {\scriptsize 64{*} + 12} & {\bf \scriptsize 16}  \\
\textbf{Flash Memory Requirements} (bytes)& {\scriptsize {18000} } & {\scriptsize {17856} } & {\scriptsize {22092} } & \bf {\scriptsize {15432 }}\\ 
\end{tabular}

\end{table}

\begin{table}
\center

\caption{The current consumption of the MICAz platform. The numeric values are taken from Figure 6 in
\cite{Telos:2005}. \label{tab:The-current-consumptions}}

{
\begin{tabular}{|l|r|}
\hline 
\textbf{Operation} & \textbf{MICAz}\\
\hline 
Minimum Voltage  & 2.7 V\\
\hline 
Standby & 27.0 A\\
MCU Idle & 3.2 mA\\
MCU Active & 8.0 mA\\
MCU + Radio RX  & 23.3 mA\\
MCU + Radio TX (0 dbm) & 21.0 mA\\
\hline
\end{tabular}
}
\end{table}

Upon receiving a new synchronization message from the reference node, FTSP and PulseSync are required to store recently received time information into a regression table of 8 elements and perform a least-squares regression. This calculation consists of many integer multiplication and floating point division operations. GTSP not only performs these steps, but also it calculates the average of the clock offset and rate multipliers of the neighbouring nodes by considering stored neighbourhood time information. These operations consist of additional arithmetic operations on floating point numbers. For PISync protocols, only a few arithmetic operations are required and hence the CPU overhead of time synchronization decreases significantly. PISync decreases the computation overhead by more than 97\% compared to FTSP, PulseSync and  GTSP. The computation overhead also leads to overhead in terms of energy-efficient duty-cycling operation of the WSNs. Consider Table \ref{tab:The-current-consumptions} which presents the current consumptions in MICAz platform. When the micro-controller is active, the power consumption is approximately 21.6 mW, on the other hand, while receiving, the power consumption is approximately 62.91 mW. When Table \ref{tab:Resource} is considered,  processing a synchronization message will consume more or less 3132 nano Joules for PISync, while it will consume more or less 117504 nano Joules for PulseSync and 146880 nano Joules for GTSP. If it takes approximately 1500 microseconds to receive a TinyOS packet\footnote{In TinyOS, packets are composed of 11 bytes header, 28 bytes payload and 7 bytes footer, a total of 46 bytes. With 250 kps data rate, it takes approximately 1500  secs to send and receive these packets.}, the power consumption while receiving will be approximately 94365 nano Joules. Hence, in PulseSync and GTSP, the energy consumed for processing any synchronization message is greater than that consumed for transmitting or receiving any packet. This shows that CPU overhead is also an important factor to be considered in the design of time synchronization protocols. PISync protocols are lightweight in terms of energy consumption considerably.

\begin{figure}

\center

\includegraphics[scale=0.92]{figures/tinyos}


\caption{\label{fig:tinyos} The main lines of the TinyOS implementation of FloodPISync protocol where the received time information is processed and logical clock is updated. The \textit{localTime} field of the message, which stores the hardware clock value of the receiver node when this message was sent, is used to calculate the logical clock value at that time via the \textit{local2Global} interface. To calculate the clock skew, this value is subtracted from the \textit{globalTime} field of the message which holds the sender's logical clock value. If the skew is smaller than  value, the integrator gain is turned on and the adaptive value of the  parameter is calculated. Finally, the offset compensation is performed via the proportional gain and the logical clock parameters are updated.}
\end{figure}

For FTSP and PulseSync, synchronization messages carry the identifiers of the reference node and the sender node, sequence number and the logical clock value of the current node. A total of 9 bytes are required to carry this information. For flooding based versions FloodPISync and PulsePISync, the carried information is identical to that in FTSP and PulseSync. Hence, the length of synchronization messages is 9 bytes. For fully distributed version AvgPISync, there is no need to carry reference node and current node identifier, and the sequence number, since there is not any reference node and any flooding round. Each node just processes any message received from any of its neighbouring nodes. Hence, the synchronization messages are quite short in length, i.e. only 4 bytes. If GTSP is considered, since each node keeps track of its neighbouring nodes and stores their time information, sender node identifier, its logical clock value, hardware clock value and the value of the rate multiplier must be included in the synchronization messages. Hence, the length of synchronization messages is 14 bytes in GTSP. It can be concluded that AvgPISync decreases the amount of carried information approximately by 70\%  when compared to another fully distributed protocol GTSP. 


Considering Eqn. (\ref{eq:hat_t}), the aforomentioned protocols require to store the value of their hardware clock , the updated value of their logical clock  and their rate multiplier  at the latest update time . Each of these variables are 4 bytes, and hence 12 bytes of main memory is required to the variables related to the logical clock. As mentioned previously, FTSP and PulseSync store the time information of the reference node at a regression table of 8 pairs. In practical implementation, 40 bytes of memory is allocated for this table. For GTSP, 64 bytes of memory is allocated to keep track each neighbouring node, which is an extremely big memory overhead. For PISync protocols, no memory storage is required to keep track of reference node in FloodPISync and PulsePISync,  and neighbouring nodes in the fully distributed AvgPISync. According to Eqn. (\ref{eqn:new_adaptation_rule}) which presents the adaptation of the alpha parameter, the error observed at the reception time of the last synchronization message, i.e. , should be stored, which is 4 bytes. Hence, the total main memory overhead of PISync protocols are very small compared to the other protocols.


Finally, as shown in Table \ref{tab:Resource}, PISync protocols have significantly reduced application code size. Their implementations are really simple and only require quite a few arithmetic operations as shown in Figure \ref{fig:tinyos}. Since flash memory (ROM) which stores the program code of the applications is a scarce resource, PISync protocols led to a remarkable gain for time synchronization in terms of ROM requirements. 






\subsection{ Performance comparisons: AvgPISync vs GTSP}\label{subsec:AvgPISync-GTSP}

\begin{figure*}

\center

\subfigure[Global Skew]{
\includegraphics[scale=0.5]{figures/experiments/pi/avg/grid_global}\includegraphics[scale=0.5]{figures/experiments/gtsp/grid_global}
}

\subfigure[Local Skew]{
\includegraphics[scale=0.5]{figures/experiments/pi/avg/grid_local}\includegraphics[scale=0.5]{figures/experiments/gtsp/grid_local}
}

\subfigure[Rate multipliers]{
\includegraphics[scale=0.5]{figures/experiments/pi/avg/grid_mult}\includegraphics[scale=0.5]{figures/experiments/gtsp/grid_mult}
}

\subfigure[Maximum local skew per node]{
\includegraphics[scale=0.5]{figures/experiments/pi/avg/grid_gradient}\includegraphics[scale=0.5]{figures/experiments/gtsp/grid_gradient}
}

\caption{\label{fig:pi-gtsp-grid} Global skew, local skew, rate multipliers and maximum local skew per node on the {\bf grid topology} for \textbf{AvgPISync} (left column) and \textbf{GTSP} (right column), respectively.}
\end{figure*}

Since fully distributed protocols exploit time information of the neighboring nodes to establish synchronization, it is fair to evaluate their performances on the grid topology. Figure \ref{fig:pi-gtsp-grid} shows global and local skews, rate multipliers and the maximum local skew observed per node for fully distributed AvgPISync and GTSP protocols on the grid topology, respectively. It can be observed that AvgPISync outperformed GTSP in terms of steady state local and global skews. On the other hand, the convergence time for GTSP, which employs distributed averaging for clock speed and offset adjustment, is faster than that of AvgPISync. This is due to the fact that, in GTSP, nodes are able to estimate instantaneous logical clock values of their neighboring nodes since they keep track of them. Using these estimates, they can calculate the average of the logical clock values of their neighboring nodes more precisely and hence they can catch-up the average neighborhood clock faster. However, since both protocols establish synchronization in a peer-to-peer, we conclude based on a few preliminary experiments that the convergence time of the rate multipliers and the maximum local skew per node will decrease as the density of the network increases. The results of the whole experiments are summarized in Table \ref{table:pi-gtsp}. Although the local skews of these protocols are quite comparable on this small grid topology, our simulations presented in subsection \ref{sec:simulations} shows that AvgPISync outperforms GTSP on larger networks.




\begin{table}

\center

\caption{ Summary of the clock skews in microseconds and convergence time in seconds with AvgPISync and GTSP on the grid topology. \label{table:pi-gtsp}}

\begin{tabular}{lcc}
 & \textbf{AvgPISync} & \textbf{GTSP} \\
\textbf{Max. Global}       	& {\bf 13}  & { 16} \\ 
\textbf{Max. Avg. Global }  & { \bf 9}  & { 12} \\ 
\textbf{Max. Local }        & { \bf 10}  & { 10} \\ 
\textbf{Max. Avg. Local }   & {\bf 4}   & { 5} \\ 
\textbf{Convergence Time }  & {\bf  2000 }  & {  1000}\\ 
\end{tabular}

\end{table}

\subsection{Performance comparisons: FloodPISync and FTSP}

\begin{figure*}
\center
\subfigure[Global Skew]{
\includegraphics[scale=0.5]{figures/experiments/pi/flood/grid_global}\includegraphics[scale=0.5]{figures/experiments/ftsp/grid_global}
}
\subfigure[Local Skew]{
\includegraphics[scale=0.5]{figures/experiments/pi/flood/grid_local}\includegraphics[scale=0.5]{figures/experiments/ftsp/grid_local}
}
\subfigure[Rate multipliers]{
\includegraphics[scale=0.5]{figures/experiments/pi/flood/grid_mult}\includegraphics[scale=0.5]{figures/experiments/ftsp/grid_mult}
}

\subfigure[Maximum skew to the reference node ]{
\includegraphics[scale=0.5]{figures/experiments/pi/flood/grid_root}\includegraphics[scale=0.5]{figures/experiments/ftsp/grid_root}
}

\caption{\label{fig:pi-ftsp-grid} Global skew, local skew, rate multipliers and maximum skew to the reference node on the \textbf{grid topology} for \textbf{FloodPISync} (left column) and \textbf{FTSP} (right column), respectively.}
\end{figure*}


\begin{figure*}
\center

\subfigure[Global Skew]{
\includegraphics[scale=0.5]{figures/experiments/pi/flood/line_global}\includegraphics[scale=0.5]{figures/experiments/ftsp/line_global}
}
\subfigure[Local Skew]{
\includegraphics[scale=0.5]{figures/experiments/pi/flood/line_local}\includegraphics[scale=0.5]{figures/experiments/ftsp/line_local}
}
\subfigure[Rate multipliers]{
\includegraphics[scale=0.5]{figures/experiments/pi/flood/line_mult}\includegraphics[scale=0.5]{figures/experiments/ftsp/line_mult}
}

\subfigure[Maximum skew to the reference node ]{
\includegraphics[scale=0.5]{figures/experiments/pi/flood/line_root2}\includegraphics[scale=0.5]{figures/experiments/ftsp/line_root}
}

\caption{\label{fig:pi-ftsp-line} Global skew, local skew, rate multipliers and maximum skew to the reference node on {\bf line topology} for \textbf{FloodPISync} (left column) and \textbf{FTSP} (right column), respectively.}
\end{figure*}


Figures \ref{fig:pi-ftsp-grid} and \ref{fig:pi-ftsp-line} show the skew values, rate multipliers and the maximum skew observed from the reference for FloodPISync and FTSP on the on the  grid  and line topologies, respectively. It can be observed that in FTSP as the distance from the reference node 1 gets larger, the rate multipliers exhibit an oscillatory behaviour and this instability is reflected in the larger synchronization errors from the reference node. Due to the slow propagation of time information, waiting times at each hop increase the error of the reference time estimation. Even in a line topology of only 20 sensor nodes, we observed more than 0.5 milliseconds maximum global and local skew values with FTSP and an exponential growth of the synchronization error at each hop, which is consistent with the results presented in \cite{Lenzen2009Optimal}.

Differently, despite the use of the identical message pattern, FloodPISync protocol outperformed FTSP drastically in terms of clock skew. Especially on the line topology, it can be observed that the synchronization error grows very slowly unlike the exponential growth of FTSP, thus confirming the theoretical analysis in Section~\ref{sec:noise}. The rate multipliers in FloodPISync are quite stable and they do not exhibit fluctuations as the distance from the reference node increases. On the grid topology, there are many alternative paths for the time information of the reference node to be received by any sensor node. Even in this case, FTSP exhibited quite poor performance in terms of synchronization error. and it is outperformed by the FloodPISync protocol.

\begin{figure*}

\center

\subfigure[Global Skew]{
\includegraphics[scale=0.5]{figures/experiments/pi/pulse/grid_global}\includegraphics[scale=0.5]{figures/experiments/pulse/grid_global}
}

\subfigure[Local Skew]{
\includegraphics[scale=0.5]{figures/experiments/pi/pulse/grid_local}\includegraphics[scale=0.5]{figures/experiments/pulse/grid_local}
}

\subfigure[Rate multipliers]{
\includegraphics[scale=0.5]{figures/experiments/pi/pulse/grid_mult}\includegraphics[scale=0.5]{figures/experiments/pulse/grid_mult}
}

\subfigure[Maximum skew to the reference node ]{
\includegraphics[scale=0.5]{figures/experiments/pi/pulse/grid_root}\includegraphics[scale=0.5]{figures/experiments/pulse/grid_root}
}

\caption{\label{fig:pi-pulseSync-grid} Global skew, local skew, rate multipliers and maximum skew to the reference node on the {\bf grid topology} for \textbf{PulsePISync} (left column) and \textbf{PulseSync} (right column), respectively.}
\end{figure*}

\begin{figure*}

\center

\subfigure[Global Skew]{
\includegraphics[scale=0.5]{figures/experiments/pi/pulse/line_global}\includegraphics[scale=0.5]{figures/experiments/pulse/line_global}
}

\subfigure[Local Skew]{
\includegraphics[scale=0.5]{figures/experiments/pi/pulse/line_local}\includegraphics[scale=0.5]{figures/experiments/pulse/line_local}
}

\subfigure[Rate multipliers]{
\includegraphics[scale=0.5]{figures/experiments/pi/pulse/line_mult}\includegraphics[scale=0.5]{figures/experiments/pulse/line_mult}
}

\subfigure[Maximum skew to the reference node ]{
\includegraphics[scale=0.5]{figures/experiments/pi/pulse/line_root}\includegraphics[scale=0.5]{figures/experiments/pulse/line_root}
}

\caption{\label{fig:pi-pulseSync-line} Global skew, local skew, rate multipliers and maximum skew to the reference node on the {\bf line topology} for \textbf{PulsePISync} (left column) and \textbf{PulseSync} (right column), respectively.}
\end{figure*}




\subsection{Performance comparisons: PulsePISync vs PulseSync}

PulseSync and PulsePISync propose reliable and fast flooding of the time information in order to reduce the effects of the waiting times. Figures \ref{fig:pi-pulseSync-grid} and \ref{fig:pi-pulseSync-line} show global and local skews, the rate multipliers and the maximum skew observed from the reference node measured for fast flooding based PulsePISync and PulseSync on the  grid and line topologies, respectively. As we have shown in Section \ref{subsec:control_vs_least_squares} that the effect of various error sources appear in the time synchronization error dynamics as multiplicative noise with least-squares. Hence, it is desirable to propagate fresh time information as fast as possible through the pulses.  It can be observed that PulseSync reduced the fluctuations of the rate multipliers considerably when compared to FTSP since it employs a rapid flooding approach and reduces the amplification of the estimation errors at each hop.  With the same flooding speed, the performances of PulseSync and PulsePISync are quite comparable after steady state has reached. PulseSync exhibited a slightly better performance considering the average global and local skews. This is due to the fact that in least-squares regression a history is kept to calculate the estimated regression line. Thanks to this history, nodes are less sensitive to the small errors. However, due to the lack of any history in PulsePISync, nodes are more affected by these errors. The results of the experiments with flooding based protocols are summarized in Table \ref{table:ftsp-pulsesync-pi}.

\begin{table}

\center

\caption{Summary of the clock skews in microseconds and convergence time in seconds with FloodPISync, PulsePISync, FTSP and PulseSync on the grid and line topologies. \label{table:ftsp-pulsesync-pi}}

\begin{tabular}{lcccc||cccc}
\multicolumn{1}{l}{} & \multicolumn{2}{c}{\textbf{FTSP}} & \multicolumn{2}{c}{\textbf{FloodPISync}} & \multicolumn{2}{c}{\textbf{PulseSync}} & \multicolumn{2}{c}{\textbf{PulsePISync}} \tabularnewline
\multicolumn{1}{l}{}      & Grid & Line & Grid & Line & Grid & Line & Grid & Line \\
\textbf{Max. Global}       &  23  &  518  &  \bf 12  & \bf 21  & { 9} & { 17} & { \bf 10} & { \bf 14} \\ 
\textbf{Max. Avg. Global } &  20   & 476  & \bf 8   & \bf 17  & { 6} & { 12} & { \bf 8} & { \bf 10} \\ 
\textbf{Max. Local }       &  16   & 387  & \bf 9  & \bf 15   & { 6} & { 5}  & { \bf 8} & { \bf 12} \\ 
\textbf{Max. Avg. Local }  &  5   & 55   &  \bf 3   & \bf 4   & { 2}  & { 2}  & { \bf  3} & { \bf 3} \\ 
\textbf{Convergence Time}  &  750  &   2000 & { \bf < 500} & { \bf < 750} & { < 500}  & {  1250}  & { \bf < 500} & { \bf < 500} \\ 
\end{tabular}
\end{table}

As a summary of our overall experiments, it can be concluded that PISync protocols outperforms FTSP and GTSP, and has performance in terms of synchronization error which is comparable to PulseSync but with a significantly lower CPU and memory overhead. The fully distributed AvgPISync exhibits smaller local skew since each node synchronizes to their neighbouring nodes. For applications and protocols which require tight synchronization among the neighbouring nodes, this property is desirable. For external synchronization, flooding based PISync protocols are quite satisfactory. However, the best synchronization performance is achieved by employing fast flooding strategy with PulsePISync. For architectures which allows fast flooding, such as Glossy \cite{ferrari2011efficient}, PulsePISync is a good solution. 

\subsection{Reaction of PISync to dynamic changes in the network}

 Within the context of reaction to the network dynamics, the following main points should be considered: robustness against faulty (or bad) nodes broadcasting incorrect time information, and the perturbation of the network due to new nodes joining the network and due to instability of the clock frequencies caused by environmental factors such as instantaneous temperature changes.  

In our implementation of PISync, we followed a simple strategy to detect bad and faulty nodes, which is also implemented in the publicly available implementation of FTSP: When a node receives time information and observes that its synchronization error is greater than a predefined value\footnote{e.g. \textit{ENTRY THROWOUT LIMIT} in the FTSP implementation coming with TinyOS 2.1.2.}, it considers the received information as incorrect and discards it. However, if it successively observes this situation\footnote{i.e. if it observes 3 times successively in the FTSP implementation coming with TinyOS 2.1.2.}, this indicates that the reference time has changed and it decides to adapt its logical clock  to the received information.

In order to observe the reaction of the fully distributed and flooding based versions of the PISync to the network dynamics, we also performed experiments with AvgPISync and FloodPISync by turning some nodes off and then on at specific time instants. We used a fully connected topology of 6 sensor nodes as shown in Figure \ref{fig:pi-dynamics1}. For the FloodPISync, the node with identifier 1 is used as the reference node.

\begin{figure}
\center

\includegraphics[scale=0.5]{figures/dynamic}

\caption{\label{fig:pi-dynamics1} The fully connected topology of 6 Tmote Sky sensor nodes used for the observation of the reaction of the fully distributed AvgPISync (left) and flooding based FloodPISync (right) to the network dynamics. }

\end{figure}

During the experiments with the AvgPISync, node 3 was turned off and then on after 5000 seconds from the beginning of the experiment, as shown in Figure~\ref{fig:pi-dynamics}. When this node started operating again, it listened its neighbours and tried to synchronize during a predefined fixed period of time. After this phase, it joined the network and started broadcasting its logical clock value periodically. As it can be observed, node 3 did not perturbed the logical clocks of its neighbouring nodes too much since they were already synchronized and communicating with each other. Moreover, since nodes do not keep track of their neighbouring nodes, no additional step was required to join the network. 

\begin{figure}
\center

\includegraphics[scale=0.5]{figures/experiments/pi/avg_dynamics}
\includegraphics[scale=0.5]{figures/experiments/pi/flood_dynamics}

\caption{\label{fig:pi-dynamics} Reaction of the fully distributed AvgPISync (left) and flooding based FloodPISync (right) to the network dynamics on the fully connected topology of 6 MICAz sensor nodes. }

\end{figure}

For the experiments with FloodPISync, we turned off and then on node 3 after 5000 seconds from the beginning of the experiment. Similarly, when the node 3 joins the network, it first try to achieve an initial synchronization by only collecting up-to-date time information of the reference node and it does not broadcast its logical clock for a predefined amount of time. After initial synchronization is achieved, it starts to broadcast its logical clock. Since nodes collect only the first synchronization message belonging to the current flooding round which is broadcasted by the reference node and discard the other messages, the newly joined node 3 did not disturb the network when it rejoined the network, as can be observed in Figure \ref{fig:pi-dynamics}. It should be noted that even in this small topology, a node may not collect time information from the reference node directly due to packet losses. Hence, there is a probability that any node may receive current time information of the reference node through a path of length two or more. If this path includes the newly joined node, network synchronization could be disturbed.

When FloodPISync and AvgPISync are compared in terms of the time took for the network to get adapted to the newly joined node 3, it can be observed that the re-synchronization period of the network with FloodPISync took approximately 150 seconds while it took approximately 250 seconds for AvgPISync. The adaptation time of AvgPISync is longer than that of FloodPISync since each sensor node tries to synchronize all of its neighboring nodes, thus increasing the adaptation time. On the contrary, in FloodPISync, each sensor node only considers the time information of the reference node.

The main factors effecting the frequency stability of the crystal oscillators are the environmental factors such as instantaneous temperature changes. For FloodPISync as well as for FTSP, PulseSync and many other flooding-based protocols in the literature, all nodes are treated equal and there is no distinction between the stable and unstable nodes. This issue has been discussed in \cite{Schmid:2010} for flooding based protocols. The authors proposed two solutions based on detecting stable neighbors and using time information flowing over them. These solutions can also easily be incorporated to FloodPISync to make it robust against unstable nodes. However, in order to select the most stable node among the neighbors, keeping track of the neighboring nodes and extra calculations are required.  For fully distributed protocols like GTSP and AvgPISync,  the issue of detecting unstable neighbors is overlooked in the literature. Discarding time information from unstable neighbors may be a simple solution. However, the network connectivity should be preserved in order to achieve network-wide synchronization. Hence, more sophisticated solutions are required that deserve to be a separate study itself. From our point of view, the solutions for the detection of the unstable nodes and adapting time synchronization should be generic and not specific to the time synchronization protocol.


In conclusion, PISync protocols are robust to changes in the network topology and re-synchronization is achieved quite rapidly without perturbing the logical clocks of the synchronized part of the network. However, FloodPISync  seems to be faster compared to the fully-distributed AvgPISync, thus making it more desirable in networks with frequent topological changes and node failures.

\subsection{Performance scalability in terms of network size}\label{sec:simulations}

The experimental results collected from small networks of 20 sensor nodes showed that FloodPISync outperforms FTSP drastically, PulsePISync catches the performance of PulseSync and AvgPISync outperforms GTSP in terms of synchronization error with significantly less computation and memory overhead. In order to observe and compare the performances of the aforementioned protocols for larger networks, we performed simulations for networks with different diameters. The error of time synchronization depends mainly on the network diameter as shown in \cite{Lenzen2009Optimal} and in Section~\ref{sec:noise}, therefore we employed simulations on grid   topologies having diameters ranging from 4 to 128 to observe the scalability of the protocols. 

For simulations, we used JProwler \cite{JProwler} which is a discrete event simulator implemented in Java for prototyping, verifying and analyzing communication protocols of TinyOS ad-hoc wireless networks. JProwler models the important aspects of all levels of the communication channel. We used Gaussian wireless channel model of JProwler to simulate radio propagation. For MAC layer, we used the MAC protocol in JProwler, which is a variant of CSMA protocol implemented for Mica2 nodes. Since hardware clocks are not modelled in JProwler, we made some modifications and implemented constant drift hardware clock model, which models the drifts as uniformly distributed within the interval of  50 ppm. We implemented PulseSync, GTSP and PISync protocols in Java language to work with our modified version of JProwler. We performed 10 simulation runs for each diameter, which simulated a real-world experiment of  seconds (about 5 days). For each diameter, we averaged the maximum global and local skews of these 10 simulations, and the corresponding results are presented in Figure \ref{fig:simulations}.

\begin{figure}
\center

\includegraphics[scale=0.5]{figures/experiments/sim_global}
\includegraphics[scale=0.5]{figures/experiments/sim_local}

\caption{\label{fig:simulations} Global skew (left) and local skew (right) values for PulseSync, GTSP, AvgPISync, FloodPISync and PulsePISync during simulations on networks with different diameters.}
\end{figure}

Our simulation results are consistent with the results collected from our small testbed. Among PISync protocols, the local skew of the fully distributed AvgPISync protocol is smaller than that of the flooding based protocol FloodPISync on longer networks. This is evident since AvgPISync exploits all neighbourhood information and it strives to locally optimize the synchronization error. On the other hand, the global skew of AvgPISync is bigger than that of FloodPISync and PulsePISync since local optimization comes at the price of slow dissemination of time information. Flooding based protocols disseminate current time information faster. It can be concluded that AvgPISync is suitable for WSN applications which demand tightly synchronized neighbouring nodes, even in larger networks. FloodPISync and PulsePISync is more suitable for applications with the requirement of tight global skew. 

It can also be observed that AvgPISync performs better than GTSP in terms of both global and local skew on larger networks. For flooding based protocols, PulsePISync performs better than FloodPISync since it disseminates time information via fast flooding. During the dissemination of the time information in flooding based PISync protocols, the clocks of the nodes are instantaneously updated to the received time information\footnote{Line 6 of the Algorithm \ref{alg:PI} and Algorithm \ref{alg:PulsePI}.}. Hence, the logical clocks in PulsePISync are more sensitive to the small errors occurred due to message delays when compared to those in PulseSync, since PulseSync maintains a history, i.e. a regression table. Although the performance of PulseSync is slightly better than PulsePISync in larger networks due to this reason, it has considerable memory and CPU overhead as we mentioned previously.

In conclusion, the global and local skews of the PISync protocols grow very slowly with the network diameter. Hence, these protocols are quite scalable, and their synchronization performances are quite satisfactory in larger networks. 

\section{Conclusion and Future Work}

\label{sec:Conclusion-and-Future}

In this article, we took into consideration recent flooding based and fully distributed time synchronization protocols in the WSN literature. We emphasized that these protocols are heavy in terms of computation and memory allocation overhead, need to keep track of time information of the other nodes which is a serious shortcoming for dense networks, require large amount of information to be exchanged among the sensor nodes, and have poor performance scalability. We proposed a new control theoretic distributed time synchronization algorithm, named PISync, which is based on a Proportional-Integral (PI) controller. We presented flooding-based and fully distributed PISync protocols which are based on PISync algorithm and observed their performances through real-world experiments and simulations. We revealed that PISync protocols have several superiorities over existing protocols: (i) they do not store any distinct time information and have very little memory allocation overhead, (ii) they have very little CPU overhead, (iii) they require very little amount of information to be exchanged, (iv) they have very small code footprint, (v) they are  quite scalable in terms of steady state global synchronization error performance. Future work may include employing PISync algorithm for TDMA applications in order to control sleep and wake-up schedules of the sensor nodes to achieve very low-power consumption. Observing the behaviour of PISync in mobile networks is also interesting future research avenue.

\section*{{\large Acknowledgments}}
This work is supported by the European Community's Seventh Framework Programme [FP7/2007-2013] under grant agreement n. 257462 HYCON2 Network of excellence.

\bibliography{references}

\end{document}
