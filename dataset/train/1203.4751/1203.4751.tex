\documentclass[11pt,pdftex,letterpaper]{article}
\pdfoutput=1
\usepackage{floatrow}
\newfloatcommand{capbtabbox}{table}[][\FBwidth]
\usepackage{amssymb}
\usepackage{cite}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{floatflt}
\usepackage{flushend}
\usepackage[margin=0.9in]{geometry}




\ifx\pdftexversion\undefined
  \usepackage[dvips]{graphicx}
\else
  \usepackage[pdftex]{graphicx}
  \DeclareGraphicsRule{*}{mps}{*}{}
\fi

\PassOptionsToPackage{pdftex}{graphicx}



\usepackage{tikz}
\usepackage{macros}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{ioa_code}


\def\algofont{\footnotesize}

\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{explanation}[theorem]{Explanation}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\hfill \2mm]}
\newenvironment{reptheorem}[1][Theorem]{\noindent\textbf{#1}}{}
\def\lf{\tiny}
\def\rrnnll{\setcounter{linenumber}{0}}
\def\nnll{\refstepcounter{linenumber}\lf\thelinenumber}
\newcounter{linenumber}

\hyphenation{LS-lineari-za-bi-li-ty}

\newcounter{excount}
\setcounter{excount}{0}
\newenvironment{example}[1]{\stepcounter{excount} {\it Example \arabic{excount} (#1)} }
{}

\newenvironment{smallitemize}[1][]{\begin{list}{#1}{\setlength{\leftmargin}{0.25in} 
}}{\end{list}
}

\def\P{\ensuremath{\mathcal{P}}}
\def\DP{\ensuremath{\Diamond\mathcal{P}}}
\def\DS{\ensuremath{\Diamond\mathcal{S}}}
\def\T{\ensuremath{\mathcal{T}}}
\def\Time{\mathbb{T}}
\def\S{\ensuremath{\mathcal{S}}}
\def\D{\ensuremath{\mathcal{D}}}
\def\W{\ensuremath{\mathcal{W}}}
\def\A{\ensuremath{\mathcal{A}}}
\def\B{\ensuremath{\mathcal{B}}}
\def\F{\ensuremath{\mathcal{F}}}
\def\R{\ensuremath{\mathcal{R}}}
\def\N{\ensuremath{\mathbb{N}}}
\def\I{\ensuremath{\mathcal{I}}}
\def\O{\ensuremath{\mathcal{O}}}
\def\Q{\ensuremath{\mathcal{Q}}}
\def\K{\ensuremath{\mathcal{K}}}
\def\L{\ensuremath{\mathcal{L}}}
\def\M{\ensuremath{\mathcal{M}}}
\def\V{\ensuremath{\mathcal{V}}}
\def\E{\ensuremath{\mathcal{E}}}
\def\C{\ensuremath{\mathcal{C}}}
\def\X{\ensuremath{\mathcal{X}}}
\def\Y{\ensuremath{\mathcal{Y}}}
\def\Nat{\ensuremath{\mathbb{N}}}
\def\Om{\ensuremath{\Omega}}
\def\ve{\varepsilon}
\def\fd{failure detector}
\def\cfd{\ensuremath{?\P+\DS}}
\def\afd{timeless}
\def\env{\ensuremath{\mathcal{E}}}
\def\bounded{one-shot}
\def\cons{\textit{cons}}
\def\val{\textit{val}}
\def\code{\textit{code}}

\def\HSS{\mathit{h}}
\def\argmin{\mathit{argmin}}
\def\proper{\mathit{proper}}
\def\content{\mathit{content}}
\def\Level{\mathit{L}}
\def\Blocked{\mathit{Blocked}}
\def\Set{\mathit{Set}}

\newcommand{\LS}{LS}

\newcommand{\correct}{\mathit{correct}}
\newcommand{\faulty}{\mathit{faulty}}
\newcommand{\infi}{\mathit{inf}}
\newcommand{\live}{\mathit{live}}
\newcommand{\true}{\lit{true}}
\newcommand{\false}{\lit{false}}
\newcommand{\stable}{\mathit{Stable}}
\newcommand{\setcon}{\mathit{setcon}}
\newcommand{\remove}[1]{}

\newcommand{\Wset}{\textit{Wset}}
\newcommand{\Rset}{\textit{Rset}}
\newcommand{\Dset}{\textit{Dset}}

\newcommand{\parts}{\textit{parts}}

\newcommand{\TryC}{\textit{tryC}}
\newcommand{\TryA}{\textit{tryA}}
\newcommand{\ok}{\textit{ok}}

\newcommand{\trylock}{\textit{trylock}}
\newcommand{\multitrylock}{\textit{multi-trylock}}
\newcommand{\CAS}{\textit{CAS}}
\newcommand{\mCAS}{\textit{mCAS}}

\def\Nomega{\ensuremath{\neg\Omega}}
\def\Vomega{\ensuremath{\overrightarrow{\Omega}}}

\newcommand{\id}[1]{\mbox{\textit{#1}}}\newcommand{\res}[1]{\mbox{\textbf{#1}}}


\newcommand{\vincent}[1]{{\bf (((VG: #1)))}}
\newcommand{\srivatsan}[1]{{\bf (((SR: #1)))}}	
\newcommand{\petr}[1]{{\bf (((PK: #1)))}}
\newcommand{\STM}{-STM}
\newcommand{\inc}[1]{#1\text{++}}

\newcommand{\HOH}{\ms{HOH}}
\newcommand{\TPL}{\ms{2PL}}
\newcommand{\TM}{\M}
\newcommand{\MP}{\ms{MP}}
\newcommand{\CR}{\ms{CR}}
\newcommand{\OP}{\ms{OP}}
\newcommand{\LL}{\ms{LL}}
\newcommand{\ignore}[1]{}

\newcommand{\myparagraph}[1]{\vspace{1mm}\noindent\textbf{#1}}





\begin{document}

\title{Optimism for Boosting Concurrency}
\author{
Vincent Gramoli\,~~~Petr Kuznetsov\,~~~Srivatsan Ravi\,\thanks{Contact author:
          srivatsan@srivatsan.in, FG INET, MAR 4-4, Marchstr. 23,
          10587 Berlin, Germany}\\
\small  NICTA and University of Sydney\\
\small  T\'el\'ecom ParisTech\\
\small  TU Berlin/ Purdue University
}

\date{}\maketitle
\thispagestyle{empty}

\begin{abstract}
Modern concurrent programming benefits from a large variety of synchronization techniques.
These include conventional pessimistic locking, as well as optimistic
techniques based on conditional synchronization primitives or 
transactional memory. Yet, it is unclear which of these approaches 
better leverage the concurrency inherent to multi-cores.


In this paper, we compare the level of concurrency one can obtain by converting a 
sequential program into a concurrent one using optimistic or
pessimistic techniques. 
To establish fair comparison of such implementations, we 
introduce a new correctness
criterion for concurrent programs, defined independently 
of the synchronization techniques they use. 

We treat a program's concurrency as its ability to accept a
concurrent schedule, a metric inspired by the theories of both databases
and transactional memory. We show that pessimistic locking can
provide strictly higher concurrency than transactions for some
applications whereas transactions can provide strictly higher
concurrency than pessimistic locks for others. 
Finally, we show that combining the benefits of the two synchronization techniques 
can provide  strictly more concurrency than any of them individually.
We propose a list-based set algorithm that is optimal in the sense that it accepts all
correct concurrent schedules.
As we show via experimentation, the optimality in terms of concurrency is
reflected by scalability gains.      
\end{abstract}


\newpage
\pagenumbering{arabic}\setcounter{page}{1}
\section{Introduction}
\label{sec:intro}
To exploit concurrency provided by modern multi-cores, conventional lock-based synchronization 
pessimistically protects accesses to  the shared memory before
executing them.
Speculative synchronization, achieved using transactional memory (TM) or
conditional primitives, such as CAS or LL/SC,  
optimistically executes memory operations with a risk of aborting them in the future.
A programmer typically uses these synchronization techniques 
as ``wrappers'' to allow every process (or thread) to \emph{locally} run its sequential code while ensuring 
that the resulting concurrent execution is \emph{globally} correct.

Unfortunately, it is difficult for programmers to tell in advance 
which of the 
techniques
will establish more concurrency in their resulting programs.
By speculatively executing concurrent accesses that would have to
block in a lock-based implementation,
TMs~\cite{HM93,ST95,HLR10} seemingly provide high
concurrency.   
However, TMs conventionally ensure 
\emph{serializability}~\cite{Pap79-serial} or even
stronger properties~\cite{tm-book}, which may prohibit concurrent
scenarios allowed by the sequential specification of the specific data
structure we intend to implement~\cite{GG14}.


\ignore{
Consider the concurrent scenario in Figure~\ref{fig:locksvstm}. 
Here  (resp. ) represents a read (resp. write) on some shared 
object .
Suppose that the first thread reads  and overwrites  and
expects both the read value of  and the overwritten value of
 
to
be in the memory at the same time. Similarly, the second thread requires the value
it writes to   and the value it reads from  to be in the memory at the same time. 
This example is intentionally oversimplified, nevertheless 
it can be useful when a pointer to  is stored in .
The threads can progress concurrently meeting their expectations 
if they acquire distinct exclusive locks (one on  and one on ) 
but not if they run transactions: 
the underlying transactional memory would typically enforce serializability 
preventing one of the transactions from committing.
This simple example  illustrates why transactional memory may not be
the best candidate
to maximize concurrency, however, 
it does not say whether optimism is badly suited.


\begin{figure}[b]
  \setlength\tabcolsep{5pt}
    \footnotesize
    \begin{minipage}[b]{0.4\columnwidth}
      \begin{tabular}{l|l}
	Thread 1 & Thread 2 \\ \hline
	{\bf lock(X)} & {\bf lock(Y)}  \\
	 & \\
	&  \\
	&  \\
	 &  \\
	{\bf unlock(X)} & {\bf unlock(Y)}  \\
      \end{tabular}
    \end{minipage}
    \hspace{4em}
    \begin{minipage}[b]{0.4\columnwidth}
      \begin{tabular}{l|l}
	Thread 1 & Thread 2 \\ \hline
	{\bf txn} \{ & {\bf txn} \{ \\
	~~~~ &  \\
	 & ~~~~ \\
	& ~~~~ \\
	~~~~ &  \\
        {\bf \}}  &      {\bf \}}\\
      \end{tabular}
    \end{minipage}
  \caption{Intuitively, locks may provide more concurrency than
    transactions: the schedule is not serializable and thus is
    rejected by most TMs}
\label{fig:locksvstm}
\end{figure}
}

In this paper, we analyze the ``amount of concurrency'' one can obtain by turning a sequential program into a concurrent one.
In particular, we compare the use of 
optimistic and pessimistic
synchronization techniques,  whose popular examples are transactions and locking, respectively.
To fairly compare concurrency provided by implementations
based on various techniques,    
one has (1)~to  define what it means for a concurrent program to be
correct regardless of the type of  synchronization it uses and 
(2)~to define a metric of concurrency. 


\vspace{1mm}\noindent\textbf{Correctness.}
We begin by defining a novel consistency criterion,
namely \emph{locally-serializable linearizability}.
We say that a concurrent implementation of a given sequential data type is
\emph{locally serializable} if it
ensures that the local execution of each 
operation 
is equivalent to \emph{some} execution of its sequential implementation.
This condition is weaker than serializability
since it does not require that there exists a \emph{single} sequential 
execution  that is consistent with all local executions.
It is however sufficient to guarantee that optimistic
executions do not observe an inconsistent transient state that could 
lead, for example, to a fatal error like division-by-zero.

Furthermore, the implementation should ``make sense'' globally, 
given the \emph{sequential type} of the data structure we implement.
The high-level history of every execution 
of a concurrent implementation must be 
\emph{linearizable}~\cite{HW90,AW04} with respect to 
this sequential type.
The combination of local serializability and linearizability gives
a correctness criterion that we call \emph{\LS-linearizability},
where {\LS} stands for ``locally serializable''.
We show that LS-linearizability is, as the original  linearizability,
compositional~\cite{HW90,HS08-book}: a composition of LS-linearizable 
implementations is also LS-linearizable. 
Unlike linearizability, however, it is not non-nonblocking: local
serializability may prevent an operation in a finite LS-linearizable
history from completing in a non-blocking manner. 

We apply the criterion of LS-linearizability to  
two broad classes of \emph{pessimistic} and \emph{optimistic}
synchronization techniques. 
Pessimistic implementations capture what can be achieved 
using classic conservative locks like mutexes, 
spinlocks, reader-writer locks.
In contrast, optimistic implementations proceed speculatively and
may roll back in the case of conflicts, e.g.,  
relying on classical 
TMs, like TinySTM~\cite{FFR08} or NOrec~\cite{norec},  or more relaxed forms of
optimistic techniques, such as ``lazy''
synchronization~\cite{HHL+05},  elastic transactions~\cite{FGG09} or
view transactions~\cite{AMT10}.


\vspace{1mm}\noindent\textbf{Concurrency metric.}
We measure the amount of concurrency provided by an LS-linearizable implementation as the set of schedules it accepts.
To this end, we define a concurrency metric 
inspired by the analysis of parallelism in database concurrency control~\cite{Yan84,Her90}
and transactional memory~\cite{GHF10}.
More specifically, we assume an external scheduler that defines which
processes execute which steps of the corresponding sequential program 
in a dynamic and unpredictable fashion. 
This allows us to define concurrency provided by an implementation as the set of \emph{schedules} 
(interleavings of steps of concurrent sequential operations) 
it \emph{accepts} (is able to effectively process).


Our concurrency metric is platform-independent and it allows for
measuring relative concurrency of LS-linearizable implementations
using arbitrary synchronization techniques.   
We do not claim that this metric necessarily captures 
efficiency, as it does not account for other factors, 
like cache sizes, cache coherence protocols, or computational costs of 
validating a schedule, which may also affect performance on
multi-core architectures.
However, our experimental evaluations show that the gain in concurrency 
may translate into better scalability.

\vspace{1mm}\noindent\textbf{Measuring concurrency.}
This paper provides 
a framework to compare the concurrency one can get
by choosing a particular synchronization technique for a specific data type.
For the first time, we analytically capture the
inherent incomparability of TM-based and pessimism-based
implementations in exploiting concurrency.
We illustrate this using a popular sequential list-based set
implementation~\cite{HS08-book}, concurrent implementations of which
are our running examples.
More precisely, we show that there exist TM-based implementations that, for some workloads, 
allow for more concurrency than \emph{any} pessimistic implementation,
but we also show that there exist pessimistic implementations that, for other workloads, allow for more 
concurrency than \emph{any} TM-based implementation.

Intuitively, an implementation based on transactions 
may abort an operation based on the way
concurrent steps are scheduled, 
while a pessimistic implementation 
has to proceed eagerly without knowing about how future steps will be 
scheduled, sometimes over-conservatively rejecting 
a potentially acceptable schedule.  
By contrast, pessimistic implementations designed to exploit
the semantics of the data type can supersede the
``semantics-oblivious'' TM-based implementations.


More surprisingly, we demonstrate that combining the benefit of pessimistic implementations, 
namely their semantics awareness, 
and the benefit 
of transactions,
namely their optimism, 
enables implementations that are strictly 
better-suited for exploiting concurrency 
than any of them individually.
We describe a generic optimistic implementation of 
the list-based set that is \emph{optimal} with respect to our
concurrency metric: we show that, essentially, it accepts \emph{all} 
correct concurrent schedules.
Our implementation, designed with our theoretical concurrency metric
in mind,  is surprisingly reminiscent of the
state-of-the-art ``pragmatic'' list-based set implementations~\cite{HHL+05,harris-set}. 
Indeed, our experimental results confirm
that optimal concurrency leads to higher performance
than popular pessimistic algorithms, like hand-over-hand list-based
sets, or generic TM-based optimistic ones.

Our concurrency analysis is focused on a specific example of a
list-based set, but our findings demonstrate the potential
of the concurrency-based  approach in analyzing and comparing wider classes of
LS-linearizable data structures. 
 \section{Preliminaries}
\label{sec:prel}
\vspace{1mm}\noindent\textbf{Sequential types and implementations.}
An \emph{object type}  is a tuple
 where
 is a set of operations,
 is a set of responses,  is a set of states,  is an
initial state and 
 
is a transition relation that determines, for each state
and each operation, the set of possible
resulting states and produced responses~\cite{AFHHT07}. 
For any type , each high-level object  of this type has a \emph{sequential implementation}. 
For each operation , 
 specifies a deterministic procedure that 
performs \emph{reads} and \emph{writes} on a collection of objects
 that encode a state of , and returns a response . 

As a running example, we consider the sorted linked-list based implementation of the type \emph{set}, commonly referred to 
as the \emph{list-based set}~\cite{HS08-book}.
The \emph{set} type exports operations ,  and
, with .
We consider a sequential implementation 
of the \emph{set} type using a sorted linked list where 
each element (or \emph{object}) stores an integer value, , and a pointer to its successor, , so that elements are 
sorted in the ascending order of their value.  
Both element fields are accessed atomically.
Every operation invoked with a parameter  traverses the list starting from the
 up to the element storing value .
If , then  returns ,  unlinks the 
corresponding element and returns , and  returns . Otherwise, 
 and  return
 while  adds a new element with value
 to the list and returns . 
The list-based set 
is denoted by  (cf. formal definition in Appendix~\ref{app:seq}).

\vspace{1mm}\noindent\textbf{Concurrent implementations.}
We tackle the problem of turning the sequential
implementation  of type  into a \emph{concurrent} one, shared by 
 \emph{processes}  ().
The implementation provides the processes with algorithms 
for the reads and writes on objects.
We refer to the resulting implementation as a concurrent implementation of .
We assume an asynchronous shared-memory system in which the processes communicate by
applying primitives on shared \emph{base objects}~\cite{Her91}.
We place no upper bounds on the number of versions an object may maintain or on the size of this object.
Throughout this paper, the term \emph{operation} refers to some
high-level operation of the type, 
while read-write operations on objects are referred simply 
as \emph{reads} and \emph{writes}.

An implemented read or write may \emph{abort} by returning a special response
. In this case we say that the corresponding high-level
operation is \emph{aborted}. 
The  event is treated both as the response event of the read or
write operation and as the response of the corresponding high-level operation.   

\vspace{1mm}\noindent\textbf{Executions and histories.}
An \emph{execution} of a concurrent implementation is a sequence
of invocations and responses of high-level operations of type , invocations and responses of read and write
operations, and invocations and responses of base-object primitives.
We assume that executions are \emph{well-formed}:
no process invokes a new read or write, or high-level operation before
the previous read or write, or a high-level operation, resp., 
returns, or takes steps outside its read or write operation's interval.


Let  denote the subsequence of an execution 
restricted to the events of process .
Executions  and  are \emph{equivalent} if for every process
, .
An operation  \emph{precedes} another operation  in an execution
, 
denoted , 
if the response of  occurs before the invocation of .
Two operations are \emph{concurrent} if neither precedes
the other. 
An execution is \emph{sequential} if it has no concurrent 
operations. 
A sequential execution  is \emph{legal} 
if for every object , every read of  in  
returns the latest written value of .
An operation is \emph{complete} in  if the invocation event is
followed by a \emph{matching} (non-) response or aborted; otherwise, it is \emph{incomplete} in .
Execution  is \emph{complete} if every operation is complete in .

The \emph{history exported by an execution } is
the subsequence of  reduced to the invocations and responses
of operations,  reads and writes, except for the reads
and writes that return . 

\vspace{1mm}\noindent\textbf{High-level histories and linearizability.}
A \emph{high-level history}  of an execution  is the subsequence of  consisting of all
invocations and responses of \emph{non-aborted} operations.
A complete high-level history  is \emph{linearizable} with 
respect to an object type  if there exists
a sequential high-level history  equivalent to  such that
(1)  and
(2)  
is consistent with the sequential specification of type .
Now a high-level history  is linearizable if it can be
\emph{completed} (by adding matching responses to a subset of
incomplete operations in  and removing the rest)
to a linearizable high-level history~\cite{HW90,AW04}.

\vspace{1mm}\noindent\textbf{Obedient implementations.}
We only consider implementations that satisfy the following condition:
Let  be any complete sequential execution of a concurrent implementation .
Then in every execution of  of the form 
where each  () is the complete execution of a
read, every read returns the value written by the last write that does
not belong to an aborted operation.

Intuitively, this assumption restricts our scope to
``obedient'' implementations of reads and writes, where no
read value may depend on some future write.   
In particular, we filter out implementations in which the
complete execution of a high-level operation is performed within the
first read or write of its sequential algorithm.

\vspace{1mm}\noindent\textbf{Pessimistic implementations.}
Informally, a concurrent implementation is \emph{pessimistic} if the exported history contains
every read-write event that appears in the execution. 
More precisely, no execution of a pessimistic implementation includes
operations that returned .  

For example, a class of pessimistic implementations are those based on \emph{locks}.
A lock provides
shared or exclusive access to an object  through 
synchronization primitives  (\emph{shared mode}),
 (\emph{exclusive mode}),  
and .
When  (resp. ) invoked
by a process  returns, we say that  \emph{holds
a lock on  in shared (resp. exclusive) mode}.
A process \emph{releases} the object it holds by invoking
.  
If no process holds a shared or exclusive
lock on , then 
eventually returns;
if no process holds an exclusive
lock on , then 
eventually returns; and
if no process holds a
lock on  forever, then every  or 
eventually returns. 
Given a sequential implementation of a data type, 
a corresponding lock-based concurrent one 
is derived by inserting the synchronization primitives
to provide read-write access to an object. 

\vspace{1mm}\noindent\textbf{Optimistic implementations.}
In contrast with pessimistic ones, optimistic implementations may, under
certain conditions, abort an operation:
some read or write may return ,
in which case the corresponding operation also returns .

Popular classes of optimistic implementations are those based on
``lazy synchronization''~\cite{HHL+05,HS08-book} (with the ability of
returning  and re-invoking an operation) or   
 \emph{transactional memory} (\emph{TM})~\cite{HM93,ST95,HLR10}.
A TM provides access to a
collection of objects via \emph{transactions}.
A transaction is a sequence of read and write operations on
objects. A transaction may \emph{commit},
or one of the read or write performed by the transaction may \emph{abort}.
Given a sequential implementation of a data type, 
a corresponding TM-based concurrent one 
puts each sequential operation within a
transaction and replaces each read and write of an object  with the
transactional read and write implementations, respectively. 
If the transaction commits, then the result of the operation is
returned to the user; otherwise if one of the transactional operations aborts,  is returned.


\section{Locally serializable linearizability}
\label{sec:lin}
We are now ready to define the correctness criterion that we impose on our
concurrent implementations.

Let  be a history and let  be a high-level operation in . 
Then  denotes the subsequence of  consisting of the events
of , except for the last aborted read or write, if any.
Let  be a sequential implementation of an object of type
 and , the set of histories of . 
\begin{definition}[LS-linearizability]
\label{def:lin}
A history  is \emph{locally serializable with respect to}
 if for every high-level operation  in ,
there exists  such that .
A history  is \emph{\LS-linearizable with respect to
}  (we also write  is -LSL)  if:
(1)  is locally serializable with respect to
 and (2) the corresponding high-level history  
is linearizable with respect to .
\end{definition}
Observe that local serializability stipulates that the execution is 
witnessed sequential by every operation.
Two different operations (even when invoked by the same process) are not
required to witness mutually consistent sequential executions.

A concurrent implementation  is \emph{\LS-linearizable with respect to
} (we also write  is -LSL)
if every history exported by  is -LSL.   
Throughout this paper, when we refer to a concurrent implementation of , 
we assume that it is \LS-linearizable with respect to .

Just as linearizability, \LS-linearizability is
\emph{compositional}~\cite{HW90,HS08-book}: a composition of LSL 
implementations is also LSL. (cf. Appendix~\ref{app:comp}).
However, it is not \emph{non-nonblocking}: local
serializability may prevent an operation in a finite LSL
history from completing in a non-blocking manner. 

\begin{figure*}[t]
 \includegraphics[scale=0.52]{ex1.0}
 \caption{\small{A concurrency scenario for a list-based set, initially , where value  is stored at node :
    and   can proceed
   concurrently with , the history is
   LS-linearizable but not serializable. (We only depict important read-write
   events here.)}}\label{fig:ex1}\end{figure*}

\vspace{1mm}\noindent\textbf{LS-linearizability versus other criteria.}
LS-linearizability is a two-level consistency criterion which makes it
suitable to compare concurrent implementations of a sequential data
structure, regardless of synchronization techniques they use.
It is quite distinct from related criteria designed for database and software
transactions, such as serializability~\cite{Pap79-serial,WV02-book} and
multilevel serializability~\cite{Wei86,WV02-book}.

For example, serializability~\cite{Pap79-serial} prevents sequences of reads and writes from conflicting in a cyclic way, 
establishing a  global order of transactions.
Reasoning only at the level of reads and writes may be overly conservative:
higher-level operations may commute even if their reads and writes conflict~\cite{Wei88}.
Consider an execution of a concurrent \emph{list-based set} depicted in 
Figure~\ref{fig:ex1}.
We assume here that the set initial state is .
Operation  is concurrent, first with 
operation  and then with operation . 
The history is not serializable:
 sees 
the effect of  because  by  returns the value
of  that is updated by  and
thus should be serialized after it. But  misses
element  in the linked list, but must see the
effect of  to perform the read of , i.e., the element created by .  
However, this history is LSL since each of the three local histories is consistent with some
sequential history of . 

Multilevel serializability~\cite{Wei86,WV02-book} was 
proposed to reason in terms of multiple semantic levels in the same execution.
\LS-linearizability, being defined for two levels only, does not require a global serialization of low-level operations as
-level serializability does. 
LS-linearizability simply requires each process  to observe a local serialization, which can be different from one
process to another. Also, to make it more suitable for concurrency
analysis of a concrete data structure, instead of semantic-based commutativity~\cite{Wei88}, we use the sequential
specification of the high-level behavior of the object~\cite{HW90}.

Linearizability~\cite{HW90,AW04} only accounts for high-level
behavior of a data structure,  so it does not imply
LS-linearizability. For example, Herlihy's universal
construction~\cite{Her91} provides a linearizable implementation for
any given object type, but does not guarantee that each execution locally appears
sequential with respect to any sequential implementation of the type.    
Local serializability, by itself, does not require any synchronization
between processes and can be trivially implemented without
communication among the processes.
Therefore, the two parts of LS-linearizability indeed complement each other.  



\section{The concurrency metric}\label{sec:concurrency}
To characterize the ability of a concurrent implementation to process arbitrary interleavings of sequential code, we introduce 
the notion of a \emph{schedule}.
Intuitively, a schedule describes the order in which complete high-level
operations, and sequential reads and writes are invoked by the user. 
More precisely, a schedule is 
an equivalence class of complete histories that agree on
the \emph{order} of invocation and response events of reads, writes and high-level operations, but 
not necessarily on read \emph{values} or high-level responses.
Thus, a schedule can be treated as a history, where responses of reads and operations
are not specified. 

We say that an implementation  \emph{accepts} a schedule  if 
it exports a history  such that  exhibits
the order of , where  is the subsequence of 
that consists of the events of the complete operations that returned a matching response. 
We then say that the execution (or history) \emph{exports} . 
A schedule  is 
-LSL if there
exists an -LSL history that exports .


A \emph{synchronization technique} is a set of concurrent implementations.
We define below a specific optimistic synchronization technique and then
a specific pessimistic one.


\vspace{1mm}\noindent\textbf{The class .}
Let  denote the execution of a TM implementation and
, 
the set of transactions each of which performs at least one event in .
Let  denote the prefix of  up to the last event of transaction .
Let  denote the set of subsequences of   that
consist of all the events of transactions that are committed and some
transactions that started committing in . 
We say that  is \emph{strictly serializable} if 
there exists a legal sequential execution  equivalent to
a sequence in 
such that . 

This paper focuses on TM-based implementations that are strictly
serializable and, in addition, guarantee that every
transaction (even aborted or incomplete) observes correct (serial)
behavior.  
More precisely, an execution  is  \emph{safe-strict serializable} if
(1)  is strictly serializable, and
(2) for each operation  that is incomplete or returned  in
, there exist a legal sequential execution of transactions
 and
 such that  and .

Safe-strict serializability captures nicely both local serializability
and linearizability. 
If we transform a sequential implementation
 of a type  into a concurrent one using any
\emph{safe-strict serializable} TM, 
we obtain an LSL TM-based
implementation of . 
Indeed, by running each operation of  within a transaction of
a safe-strict serializable TM, we make sure that operations in 
committed transactions witness the same execution of , and
every operation that returned  is consistent with some
execution of  based on previously completed operations.
Formally,  denotes the set of TM-based LSL
implementations.
(We discuss the relations to similar but stronger TM criteria, such as
opacity~\cite{tm-book}, \emph{TMS1}~\cite{TMS09} and
\emph{VWC}~\cite{damien-vw} in Section~\ref{sec:related}.)   

\vspace{1mm}\noindent\textbf{The class .}
This denotes the set of \emph{deadlock-free} pessimistic
LSL implementations: assuming that every process takes
enough steps, 
at least one of the concurrent operations return a matching response~\cite{HS11-progress}.
Note that  includes implementations that are not necessarily safe-strict serializable. 
In the next section, we describe a pessimistic implementation of the list-based set that accepts non-serializable schedules by
fine-tuning to the semantics of the \emph{set} type.
\begin{figure*}
 \includegraphics[scale=0.45]{ex2.0}
 \caption{\small{(a) a history exporting schedule , with initial state
   , accepted by ; 
(b) a history exporting a problematic schedule , with initial state 
   , which should be accepted by any  if it accepts }}\label{fig:ex2}\vspace{-0.35mm}
\end{figure*}
\section{On the incomparability of synchronization techniques}\label{sec:incomparability}
We now provide a concurrency analysis of synchronization techniques  and 
in the context of the list-based set.
We describe a pessimistic implementation of , ,
that accepts non-serializable schedules: each read operation performed by  
acquires the \emph{shared lock} on the object, 
reads the  field of the element before releasing the shared lock on the predecessor element 
in a \emph{hand-over-hand} manner~\cite{BS88}.
Update operations ( and
) acquire the \emph{exclusive lock} on the  during  
and release it at the end. Every other read operation performed 
by update operations simply reads the element  field to traverse the list. The write operation
performed by an  or a  acquires the exclusive lock, writes the value
to the element and releases the lock.
There is no real concurrency between any two update operations since the process holds the 
exclusive lock on the  throughout the operation execution.
Note that  is deadlock-free and -LSL.

On the one hand, the schedule of  depicted in Figure~\ref{fig:ex1}, which we denote by , is not serializable
as explained in Section~\ref{sec:lin}
and must be rejected by any implementation in .
However, there exists an execution of  that exports 
since there is no read-write conflict on any two consecutive elements accessed.

On the other hand, consider the schedule  of  in Figure~\ref{fig:ex2}(a).
Clearly,  is serializable and is accepted by most (progressive~\cite{tm-theory}) TM-based implementations since there
is no read-write conflict.
However, we prove that  is not accepted by any implementation in .
Our proof technique is interesting in its own right: we show that
if there exists any implementation in  that accepts , it must also
accept the schedule  depicted in Figure~\ref{fig:ex2}(b). In , 
 overwrites the write on \emph{head} performed by 
resulting in a lost update. By deadlock-freedom, there exists an extension of  in which
a  returns ; but this is not a linearizable schedule. 
\begin{theorem}[Incomparability]
\label{th:mpl}
There exist schedules  and  
of  such that
(1)  is accepted by an -LSL implementation 
but not accepted by \emph{any} -LSL implementation in , and
(2)  is accepted by an -LSL implementation 
but not accepted by \emph{any} -LSL implementation in
.
(The proof is in Appendix~\ref{app:smp}.)
\end{theorem}
The second part of
Theorem~\ref{th:mpl} 
may look surprising, as 
the class  includes implementations that are relaxed (not safe-strict serializable) and 
fine-tuned to the semantics of the type whereas implementations in the class  are oblivious to the semantics of the data type.
However, since TM-based implementations are optimistic, i.e., every read-write operation remains tentative, 
the implementation does not need to be overly conservative and could return  in case a matching response to the 
operation cannot be returned.
\section{On the benefits of being optimistic and relaxed }
\label{sec:svlock}
We now combine the benefits of relaxation and optimism to derive an optimistic implementation 
of the list-based set
that supersedes every implementation in classes  and
 in terms of concurrency.
Our implementation, denoted  provides processes with algorithms for 
implementing read and write operations on the elements of the list for each operation of the 
list-based set (Algorithm~\ref{alg:elastic}).
\begin{algorithm*}[t]
\caption{Code for process  implementing reads and writes in implementation }
\label{alg:elastic}
  \begin{algorithmic}[1]
  	\begin{multicols}{2}
	{\size
	
	\Part{Shared variables}{
\State for each object :
	  \State ~~~~~~, initially 0
	  \State ~~~~~~, initially 
	  \State ~~~~~~ supports 
	  \State ~~~~~~~~~~~ operations, initially 
	}\EndPart	
	
   	\Statex
	  
	\Part{Local variables of process }{
\State ;  cyclic buffer of size ,  
	  \State ~~~initially 
	}\EndPart	
	
	\Statex


   	\Part{ executed by , , }{	  
	  \State  \label{line:rver} \Comment{get versioned lock}
	  \State  \Comment{get value} \label{line:linr}
	  \State  \label{line:reread}
	  \State  \Comment{reget versioned lock}
	  \If{} \label{line:flagcheck}
\Return  \label{line:elastic:abort1} \EndReturn
	  \EndIf
\State 
          \Comment{override penultimate entry}
\Return  \EndReturn \label{line:tx-read}
	}\EndPart
	  


	\newpage
	
	
\Part{ executed by }{
\State {\bf let}  be such that 
	  \State 
	  \If{} \label{line:linw}
\Return   \label{line:elastic:abort2}\Comment{grab lock or abort}	  	\EndReturn
	  \EndIf
\State {\bf let}  be such that 
	  \If{} \label{line:linw2}
\Return   \label{line:elastic:abort2}\Comment{grab lock or abort}	  	\EndReturn
	  \EndIf
	  \State  \label{line:elastic:gc}  \Comment{mark element for deletion}
	      
\State  \Comment{update memory} \label{line:commit-update}
	  \State \Comment{release locks} \label{line:release}
	  \State  \label{line:release2}
	  \Return  \EndReturn
	}\EndPart

	\Statex
\Part{ executed by }{
\State {\bf let}  be such that 
	  \State 
	  \If{} \label{line:inswrite}
\Return   \label{line:elastic:abort3}\Comment{grab lock or abort}	  	\EndReturn
	  \EndIf
\State  \Comment{update memory} \label{line:inscommit}
      \State \Comment{release locks} \label{line:insrel}
	  \Return  \EndReturn
	}\EndPart

	
  }
  \end{multicols}
  \end{algorithmic}\vspace{-1em}
\end{algorithm*}

 

Every object (or element)  is
specified 
by the following shared variables: 
stores the \emph{value}  of ,  stores
a boolean indicating if  is \emph{marked for deletion}, 
 stores a tuple of the \emph{version number} of  and a \emph{locked} flag; 
the latter indicates whether a concurrent process is performing a write to .

Any operation with input parameter  traverses the list starting from the
 element up to the element storing value  without writing to shared memory.
If a read operation on an element conflicts with a write operation to the same element or
if the element is marked for deletion, the operation terminates by returning .
While traversing the list, the process maintains the last two read elements and their version numbers 
in the local rotating buffer . If none of the read operations performed by  return 
and if , then  returns ; otherwise it returns .
Thus, the  does not write to shared memory.

To perform write operation to an element as part of an update operation ( and ), the process 
first retrieves the version of the object that belongs to its rotating buffer.
It returns  if the version has been changed since the previous read of the element 
or if a concurrent process is executing a write to the same element.
Note that, technically,  is returned only if .
If , then we attempt to lock the element with the current version
and return  if there is a concurrent process executing a write to the same element.
But we avoid expanding
on this step in our algorithm pseudocode.
The write operation performed by the  operation, additionally checks if the element to be removed
from the list is locked by another process; if not, it sets a flag on the element to mark it for deletion.
If none of the read or write operations performed during the  or  returned ,
appropriate matching responses are returned as prescribed by the sequential implementation .
Any update operation of  uses at most two expensive synchronization
patterns~\cite{AGK11-popl}.

Theorem~\ref{th:lr} in Appendix~\ref{app:lltm} shows that  is -LSL.
The pseudocode in Algorithm~\ref{alg:elastic} is given for managed languages as there is no explicit garbage collector, 
but one could add 
an epoch-based garbage collector that deallocates a node as soon as all operations concurrent with its removal are complete.

Now we show that  supersedes, in terms of concurrency, \emph{any} implementation in classes
 or .
The proof is based on a more general optimality result, interesting in its own right: 
any finite schedule rejected by  is not \emph{observably
LS-linearizable} (or simply \emph{observable}). 
A schedule  is observable 
if it has an extension  such that for all , 
 extended with a complete execution of  that returns a matching response is LS-linearizable. 
Intuitively, a schedule is observable if it incurs no lost updates.
One example of a non-observable schedule is  in
Figure~\ref{fig:ex2}(b): since one of the two concurrent
updates overwrites the effect of the other,  extended with a
complete execution of  
is not linearizable with respect to .
\begin{theorem}[Optimality]
\label{th:lrelaxed}
 accepts all schedules that are observable with respect to .
\end{theorem}
\begin{proofsketch}
We prove that any schedule rejected by  is  
not observable.
We go through the cases when a read or write returns  (implying the operation fails to return a matching response) and
thus the current schedule is rejected:
(1)  returns  in line~\ref{line:elastic:abort1}
when  or when , (2)
 performed by  either returns 
in line~\ref{line:linw} when the  operation on
 returns  or returns 
in line~\ref{line:linw2} when the  operation on the element that
stores  returns , and (3)  performed by  returns 
in line~\ref{line:inswrite} when the  operation on
 returns .

Consider the subcase (1a),  is set  by a
preceding or concurrent  (line~\ref{line:elastic:gc}).
The high-level operation performing this  is a  
that marks the corresponding list element as removed. 
Since no removed element can be read in a sequential execution
of , the corresponding history is not locally serializable.
Alternatively, in subcase (1b), the version of  read previously in line~\ref{line:rver}
has changed. Thus, an update operation has concurrently performed a write to .
However, there exist executions that export such schedules.

In case , the  performed by a  operation returns .
In subcase (2a),  is currently
locked. 
Thus, a concurrent high-level operation has previously locked  (by successfully
performing  in line~\ref{line:linw}) and has not yet released
the lock (by writing  to
 in line~\ref{line:release}). 
In subcase (2b), the current version of  (stored in ) differs 
from the version of  witnessed by a preceding . 
Thus, a concurrent high-level operation completed a write to 
\emph{after} the current high-level operation  performed a  of .
In both (2a) and (2b), a concurrent high-level updating operation 
( or ) has written or is about to perform a  to .  
In subcase (2c), the  on the element  (element that stores the value ) executed by  
returns  (line~\ref{line:linw2}).
Recall that by the sequential implementation , 
 performs a  of
 prior to the , where 
refers to .
If the \emph{cas} on  fails, there exists a process 
that concurrently performed a  to , but
after the  of  by .
In all cases, we observe that if we did not abort the write to , then
the schedule extended by a complete execution of  is not LSL.

In case , the  performed by an  operation returns . Similar arguments to
case  prove that any schedule rejected is not observable LSL.
\end{proofsketch}
Theorem~\ref{th:lrelaxed} implies that the schedules exported by the
histories in Figures~\ref{fig:ex1} and \ref{fig:ex2}(a) and that are not
accepted by any  and any , respectively,
are indeed accepted by .
But it is easy to see that implementations in  and  can only
accept observable schedules.  
As a result,  can be shown to strictly supersede any
pessimistic or TM-based implementation of the list-based set.  
\begin{corollary}
\label{cr:mrp}
 accepts every schedule accepted by any implementation in  and .
Moreover,  accepts schedules  and  that are rejected by any
implementation in  and , respectively. 
\end{corollary}
One take-away from these results is that generic optimistic implementations,
appropriately relaxed, are able to provide strictly more concurrency
than pessimistic or strongly consistent optimistic ones.
Our implementation  is in fact optimal with respect to
concurrency, while still incurring minimal cost in terms of step-complexity and
use of expensive synchronization patterns. 
\section{Related work}
\label{sec:related}
Sets of accepted schedules are commonly used as a
metric of concurrency provided by a shared-memory
implementation.
For static database transactions, 
Kung and Papadimitriou~\cite{KP79} use the metric to 
capture the parallelism of a locking scheme,
While acknowledging that the metric is theoretical, they 
insist that it may
have ``practical significance as
well, if the schedulers in question have relatively small
scheduling times as compared with waiting and execution
times.'' 
Herlihy~\cite{Her90} employed the metric to compare various
optimistic and pessimistic synchronization techniques using
commutativity
of operations constituting high-level transactions.   
A synchronization technique is implicitly considered in~\cite{Her90} as highly
concurrent, namely ``optimal'',
if no other technique accepts more schedules. 


By contrast, we focus here on a \emph{dynamic} model where the scheduler cannot 
use the prior knowledge of all the shared addresses to be accessed. 
Also, unlike~\cite{KP79,Her90}, we require \emph{all} operations, including aborted ones, to observe (locally) consistent states.
As we confirm experimentally, 
our (provably optimal) optimistic implementation incurs negligible scheduling overhead, which makes the motivation of the metric proposed
in~\cite{KP79} applicable. 

Gramoli et al.~\cite{GHF10} defined a concurrency metric, the \emph{input
acceptance}, as the ratio of committed transactions over aborted
transactions when TM executes the given schedule.   
Unlike our metric, input acceptance does not apply to
lock-based programs. 


\ignore{ 
A quite different but popular concurrency notion, called
\emph{disjoint-access parallelism}~\cite{israeli-disjoint}, 
was recently considered in the TM context~\cite{AHM09,EFKMT12}.
A TM is disjoint-access parallel (DAP) if it guarantees 
that two transactions accessing the same meta-data also access 
the same transactional object. 
Our concurrency metric not only applies to lock-based implementations but is also
more fine-grained ones \petr{did not get this point}
as it allows to determine relative concurrency provided 
by different DAP implementations.
}


Similar to other relaxations of opacity~\cite{tm-book} like \emph{TMS1}~\cite{TMS09} and \emph{VWC}~\cite{damien-vw},  
safe-strict serializable implementations () require that every transaction (even aborted and incomplete) observes
``correct'' serial behavior. 
However, unlike TMS1, we do not require the \emph{local} serial executions
to always respect the real-time order among transactions. Unlike VWC, we model transactional
operations as intervals with an invocation and a response and does not
assume unique writes (needed to define causal past in VWC). 
Therefore,  appears weaker than both TMS1 and VWC.
Though weak and possibly not very pragmatic, it still allows us 
to show that resulting TM-based LSL implementations 
reject some schedules accepted by pessimistic locks. 
However, we can easily extend our results to show that even opaque TMs accept
some schedules rejected by any pessimistic algorithm.   
    
The problem of transforming a
sequential implementation of a list-based set into a concurrent
one was considered before in special settings.
Vechev and Yahav~\cite{vechev08} considered using locks while Felber et al.~\cite{FGG09} considered using elastic transactions.
Our framework applies to generic
concurrent transformations of sequential implementations
using arbitrary synchronization techniques. 
 \section{Discussion and concluding remarks}
\label{sec:conc}
To confirm the practicality of our optimistic list-based set, ,
we compared its Java implementation against a pessimistic
implementation \emph{HOHL}
that uses hand-over-hand locking (we adopted the Java pseudocode by Herlihy and 
Shavit~\cite[Chapter 9]{HS08-book}). 
Figure~\ref{fig:alg2-vs-hoh-10} presents the throughput 
(the number of completed operations per millisecond) 
of the two algorithms on a -way machine 
where up to  threads run \% updates 
(either  or  with the same probability) 
and \%  operations on a list, initially populated
with  integer values. 
 outperforms both HOHL and E-STM~\cite{FGG09}, and 
the reason for this could be 
that  is optimal in terms of concurrency (Theorem~\ref{th:lrelaxed}), 
while the HOHL is serializable~\cite{ARR10} and, thus, rejects
large classes of correct schedules (e.g., of the kind of  in~Figure~\ref{fig:ex1}). 
E-STM does not provide optimal concurrency but rejects less schedules than HOHL.
Additionally, our implementation uses asymptotically less expensive
memory barriers and read-modify-write primitives~\cite{AGK11-popl}
than HOHL.
We deduce that accepting all observable schedules may be 
quite efficient on some applications.
For the sake of simplicity, we did not optimize our code, e.g., 
by removing wrappers or using partial aborts.
(More experimental results are given in Appendix~\ref{sec:expe}.)


\begin{floatingfigure}[right]{0.27\paperwidth}
{\small
\hspace{-2em}\includegraphics[clip=true,viewport=5 0 375 112,scale=.9]{alg2-hoh-estm-intel-5}
   \caption{ vs. HOHL\label{fig:alg2-vs-hoh-10}}
}\end{floatingfigure}


 is surprisingly reminiscent of the
list-based set implementations in~\cite{HHL+05} and \cite{harris-set}
(state-of-the-art, to the best of our knowledge).
However, because of specific optimizations of the 
operation, strictly speaking, none of these two algorithms is locally
serializable, and thus LSL.
The implementations in~\cite{HHL+05,harris-set}
use the logical deletion technique to associate a \emph{marked} field to an element to indicate if it
is contained in the list.
The  of the lazy list-based set~\cite{HHL+05} may read an element marked for deletion, whereas
the  of Harris list-based set~\cite{harris-set} even uses  to remove logically deleted nodes.
But the apparent similarities between our  and the algorithms
in~\cite{HHL+05,harris-set} suggest
that looking for a concurrency optimal LS-linearizable
implementation also helps in optimizing performance.  
An implementation, once proven to be concurrency optimal, may
be optimized further to boost performance, as is possible with .


We derived our concurrency lower bounds in the context of the list-based set, a data structure
that is suitable for exploiting concurrency because of its localized
updates, but we believe that it should be possible to generalize our results to a wider class of search structures. 
This paper provides some preliminary hints in the quest for 
the ``right'' synchronization technique to develop highly concurrent and efficient implementations of data 
types.
Our results are relevant to programmers leveraging
multi-core architectures as well as to computer manufacturers 
who aim at defining new instruction sets.
This work suggests directions to identify the ``killer'' application
for existing and emerging synchronization techniques.
 \bibliographystyle{abbrv}
\newpage
\bibliography{references}

\appendix
\section{Sequential implementation of the set type}
\label{app:seq}
Recall that an \emph{object type}  is a tuple
 where
 is a set of operations,
 is a set of responses,  is a set of states,  is an
initial state and 
 
is a transition relation that determines, for each state,
and each operation, the set of possible
resulting states and  produced responses. 
Hence,  implies that when
an operation  is applied on an object of type 
in state , the object moves to state  and returns a response .
We consider only types that are \emph{total}
 i.e., for every ,
, there exist   and
 such that .
We assume that every type  is \emph{computable}, i.e., 
there exists a Turing machine that, 
for each input , , , computes
a pair  such that .

Formally, the \emph{set} type is defined by the tuple  where:
\begin{enumerate}
\item[] ;  
\item[]  
\item[] is the set of all finite subsets of ; 
\item[] is defined as follows:
\begin{enumerate}
\item[:]

\item[:]

\item[:]

\end{enumerate}
\end{enumerate}
\begin{algorithm*}[t]
\caption{Sequential implementation {\LL} (\textit{sorted linked list}) of \emph{set} type}
\label{alg:lists}
  \begin{algorithmic}[1]
  	\begin{multicols}{2}
  	{\size
	
	\Part{Shared variables}{
		\State Initially , ,
		\State ~~~, 
		\State ~~~
	}\EndPart
	
	\Statex
	
	\Part{)}{
                \State   		 			\Comment{copy the address}
		\State   			\Comment{fetch the head node struct}
		\State  		\Comment{next element is stored}
		\While{}
\State   \Comment{pointer to the next element}
			\State  				\Comment{move on}
\State  	\Comment{fetch from memory}
		\EndWhile
		\If{}							\Comment{val is stored locally}
			\State  	\Comment{v and addr. of curr}
\State  	\Comment{stores val and next field}
		\EndIf
		\Return  						
		\EndReturn
   	}\EndPart
	
	\newpage
	
	\Part{)}{

	        \State   					
		\State   			\Comment{fetch the head node struct}
		\State  		\Comment{next field is stored locally}
		\While{} 	
					\Comment{the val field is stored locally}
                       \State 
                       \Comment{pointer to the next element}
			\State  				\Comment{move on}
\State  	\Comment{fetch from memory}
		\EndWhile
		\If{}							\Comment{val is stored locally}
\State 
\EndIf
		\Return  						
		\EndReturn	
   	}\EndPart
	
	\Statex
	
	\Part{)}{
		\State   			\Comment{fetch the head node struct}
		\State  		\Comment{next field is stored locally}
		\While{} 						\Comment{the val field is stored locally}
			\State  				\Comment{move on}
\State  	\Comment{fetch from memory}
		\EndWhile
 	   	\Return 						\Comment{val is stored locally}
 	   	\EndReturn
   	 }\EndPart

		
	}
	\end{multicols}
  \end{algorithmic}
\end{algorithm*}
 The sequential implementation \LL~ of the \emph{set} type is
presented in Algorithm~\ref{alg:lists}. The implementation uses a \emph{sorted linked list} data structure 
in which each element (except the \emph{tail}) maintains a \textit{next} field to provide a pointer to the
successor node. Initially, the \emph{next} field of the \emph{head} element points to \emph{tail}; \emph{head}
(resp. \emph{tail}) is initialized with values  (resp. ) that is smaller (resp. greater) than
the value of any other element in the list.


\section{\LS-linearizability is compositional}
\label{app:comp}
We define the composition of two distinct object types  and  
as a type  as follows: 
, ,\footnote{Here we treat each  as a distinct type by adding
index  to all elements of , , and .}   
,
, and   is such that  if and only if for , if 
 then   .

Every sequential implementation  of an object   of a
composed type  naturally induces two sequential
implementations  and  of objects  and ,
respectively. 
Now a correctness criterion 

is \emph{compositional} if for every
history  on an object composition , 
if 

holds for  with
respect to , for , then

holds for  with
respect to .
Here,  denotes the subsequence of  consisting of events on .
\begin{theorem}
\label{th:comp}
\LS-linearizability is compositional. 
\end{theorem}
\begin{proof}
Let , a history on ,  be \LS-linearizable
with respect to . 
Let each , 
, 
be \LS-linearizable with respect to . 
Without loss of generality,  we assume that  is complete (if 
is incomplete, we consider any completion of it containing
\LS-linearizable completions of   and ).

Let  be a completion of the high-level history corresponding to  such that
 and  are linearizable with respect to 
and , respectively. Since linearizability is
compositional~\cite{HW90,HS08-book},  is linearizable with respect to .

Now let, for each operation ,  and  be any two sequential histories of
 and   such that , for 
(since 

and  are \LS-linearizable such histories exist).
We construct a sequential history  by interleaving events of
 and  so that , 
.
Since each  acts on a distinct component  of , every such  is a sequential history of .
We pick one  that respects the local history ,
which is possible, since  is consistent with both    
 and . 

Thus, for each , we obtain a history of  that agrees with
. Moreover, the high-level history of  is linearizable. Thus,  is \LS-linearizable with respect to . 
\end{proof}




\section{Formal proofs}
\label{app:proofs}
\subsection{ vs. }
\label{app:smp}
\paragraph{A pessimistic implementation  of .}
The implementation  is a lock-based implementation that associates every object with a 
distinct \emph{lock} and another base object that stores the \emph{value} of the object.
In , the  operation uses shared \emph{hand-over-hand 
locking}~\cite{BS88,HS08-book}.
Each read operation performed by  
acquires the \emph{shared lock} on the object, 
reads the  field of the element before releasing the shared lock on the predecessor element. 
Update operations ( and
) acquire the \emph{exclusive lock} on the  during  
and release it at the end. Every other read operation performed 
by an update simply reads the  field of the element to traverse the list. The write operation
performed by a  or  acquires the exclusive lock, writes the value
to the element and releases the lock.
There is no real concurrency between any two update operations since the process holds the 
exclusive lock on the  throughout the operation execution.
Intuitively, it is easy to observe that  is LS-linearizable with respect to .

\begin{theorem}[Part 1 of  Theorem~\ref{th:mpl}]
There exists a schedule  of  that is
accepted by , but not accepted by \emph{any}
-LSL implementation .
\end{theorem}
\begin{proof}
Let  be the schedule of  depicted in Figure~\ref{fig:ex1}.
Suppose by contradiction that , where  is an implementation of  based on any safe-strict serializable TM.
Thus, there exists an execution  of  that exports .
Now consider two cases:
\begin{itemize}
\item
Suppose that the read of  by  
returns the value of  that is updated by .
Since , 
 must precede  in any sequential execution  equivalent to . 
Also, since  reads  prior to its update by , 
 must precede  in . 
But then the read of  is not legal in ---a contradiction since  must be serializable.
\item
Suppose that 
reads the initial value of , i.e., its value prior to the write to  by , 
where  points to the
\emph{tail} of the list (according to our sequential implementation ).
But then, according to ,  cannot access
 in ---a contradiction.  
\end{itemize}
Consider the pessimistic implementation :
since the  operation traverses the list using shared hand-over-hand locking, 
the process  executing 
can release the lock on element  prior to the acquisition of the exclusive lock on  by .
Similarly,  can acquire the shared lock on  immediately after the release of the 
exclusive lock on  by the process executing  while still holding 
the shared lock on element . Thus, there exists an execution of  that exports .
\end{proof}\paragraph{An optimistic implementation  of .}
Recall that  denotes the set of concurrent implementations based on TMs
that ensure the following safety condition in every execution.

Let  denote the execution of a TM-based implementation and
, 
the set of operations each of which performs at least one event in .
Let  denote the prefix of  up to the last event of operation .
Let  denote the subsequence of   that
consists of the events of the complete operations in . 
We say that  is \emph{strictly serializable} if 
there exists a legal sequential execution  equivalent to

such that . 

An execution  of a TM-based implementation is 
\emph{safe-strict serializable} if
(1)  is strictly serializable, and
(2) for each operation  that is incomplete or returned  in
, there exists a legal sequential execution of operations
 such that  and .

The implementation  is
based on a TM that ensures the following condition in every execution~\cite{GK09-progressiveness}: 
if a transaction  aborts, then it encounters a conflict with a transaction , i.e.,  
 and  are concurrent, both access the same object , and
at least one of these is a .
An implementation of such a safe-strict serializable TM can be
found in~\cite{KR11}. 
\begin{theorem}[Part 2 of Theorem~\ref{th:mpl}]
There exists a schedule  of  
that is accepted by an -LSL  implementation ,
but not accepted by \emph{any} -LSL implementation in .
\end{theorem}
\begin{proof}
We show first that the schedule  of  depicted
in Figure~\ref{fig:ex2}(a) is not accepted by any implementation in .
Suppose the contrary and let  be exported by an execution . 
Here  starts with three sequential  operations with
parameters , , and . The resulting ``state'' of the set is
, where value  is stored in object .   

Suppose, by contradiction, that some  accepts . 
We show that  then accepts the schedule  depicted in Figure~\ref{fig:ex2}(b), which starts with a sequential
execution of  storing value  in object . 

Let  be any history of  that exports .
Recall that we only consider obedient implementations:
in : the read of \emph{head} by  in  refers to  (the next element to be read by ). 
In , element  stores value ,
i.e.,  can safely return , while in
,  stores value , i.e., the next step of
 must be a write to \emph{head}.       
Thus, no process
can distinguish  and  
before the
read operations on  return. 
Let  be the prefix of  ending with 
executed by .
Since  is deadlock-free, we have an extension of  in
which both  and  terminate; we show that this extension violates linearizability. 
Since  is locally-serializable, to respect our sequential implementation
of , both operations should complete the write to \emph{head}
before returning. 
Let  be the first operation to write to
\emph{head} in this extended execution. 
Let  be the other insert operation.
It is clear that  returns  even though  overwrites the update of  on 
and also returns . 
Recall that implementations in  are deadlock-free. 
Thus,   
we can further extend the execution with a complete 
that will return  (the element inserted to the list by 
is lost)---a contradiction since  is linearizable with respect to \emph{set}. 
Thus,  for any .

On the other hand, the schedule  is accepted by , since
there is no conflict between the two concurrent update operations.
\end{proof}





\section{Relaxed optimistic implementation: proof of correctness}
\label{app:lltm}
Let  be an execution of  and 
denote the total-order on events in .
For simplicity, we assume that  starts with an artificial
sequential execution of an insert operation  that inserts  and sets . 
Let  be the history exported by , where 
all reads and writes are sequential. 
We construct  by associating a linearization point  with each
non-aborted read or write operation  performed in   
as follows:
\begin{itemize}
\item  if  is a read, then performed by process ,
   is the base-object  in line~\ref{line:linr};
\item  if  is a write within an  operation,
   is the base-object  in line~\ref{line:linw};
\item  if  is a write  within a  operation,
   is the base-object   in line~\ref{line:inswrite}.
\end{itemize}
We say that a  of an element  within an operation 
is \emph{valid} in  
(we also say that  is \emph{valid}) if 
there does not exist any  operation  that \emph{deallocates}  (removes  from the list)
such that . 
\begin{lemma}
\label{lem:invar1}
Let  be any operation performing  followed by  in . 
Then (1) there exists an  operation that sets  prior to , and
(2)  and  are \emph{valid} in .
\end{lemma}
\begin{proof} 
Let  be any operation in  that performs  followed by .
If  and  are \emph{head} and \emph{tail} respectively,  (by assumption). Since no  operation
deallocates the \emph{head} or \emph{tail}, the  of  and  are \emph{valid} in .

Now, let  be the \emph{head} element and suppose that  performs  followed by ;  in .
Clearly, if  performs a , there exists an
operation  that has previously set .
More specifically,  performs the action in line~\ref{line:linr} after the
write to shared memory by  in line~\ref{line:inscommit}. 
By the assignment of linearization points to tx-operations, .
Thus, there exists an  operation that sets  prior to  in .

For the second claim, we need to prove that the  by  is \emph{valid} in .
Suppose by contradiction that  has been deallocated by some  operation prior to  by .
By the rules for linearization of read and write operations, the action in line~\ref{line:commit-update} precedes the action in line~\ref{line:linr}.
However,  proceeds to perform the check in line~\ref{line:flagcheck} and returns  since the flag corresponding to the element  is previously set by . Thus,  does not contain ---contradiction.


Inductively, by the above arguments, every non-\emph{head}  by  
is performed on an element previously created by an  operation
and is valid in .
\end{proof} 
\begin{lemma}
\label{lem:rls}
 is locally serializable with respect to .
\end{lemma}
\begin{proof}
By Lemma~\ref{lem:invar1}, every element  read 
within an operation  
is previously created by an  operation and is valid in .
Moreover, if the read operation on  returns , then 
 stores a pointer to another valid element that
stores an integer value .
Note that the series of reads performed by  terminates as soon as 
an element storing value  or higher is found. Thus,  performs at most
 reads, where  is the value of the second element read by .  
Now we construct  as a sequence of  operations,
that insert values read by , one by one, followed by . 
By construction, .
\end{proof}
It is sufficient for us to prove that every finite high-level history 
of  is linearizable.
First, we obtain a completion  of  as follows.
The invocation of an incomplete \lit{contains} operation is discarded.
The invocation of an incomplete 
operation that has not returned successfully from the 
operation is discarded; otherwise, it is completed with response .


We obtain a sequential high-level history  equivalent to  by associating a linearization point  
with each operation  as follows.
For each  that returns  in ,  is associated with 
the first  performed by  in ; otherwise
 is associated with the last  performed by  in . For  that returns , 
is associated with the last  performed in ; otherwise  is associated with the  of \emph{head}.
Since linearization points are chosen within the intervals of 
operations of , for any two operations
 and  in , if , then .
\begin{lemma}
\label{lem:rlegal}
 is consistent with the sequential specification of type \textit{set}.
\end{lemma}
\begin{proof}
Let  be the prefix of  consisting of
the first  complete operations. 
We associate each  with a set  of objects that were
successfully inserted and not subsequently successfully removed in .
We show by induction on  that the sequence of state transitions in
 is consistent with operations' responses in  with respect to the \textit{set} type. 

The base case  is trivial: the \textit{tail} element containing
 is successfully inserted.
Suppose that  is consistent with the \emph{set} type and
let  with argument  and response 
be the last operation of .  
We want to show that  is consistent with the \textit{set} type. 
\begin{enumerate}
\item[(1)]
If  returns  in , there does not exist any other  that returns  in  such that there does not exist any  that returns ; .
Suppose by contradiction that such a  and  exist.
Every successful  operation performs its penultimate  on an element  that stores a value  and the last read is performed on an element that stores a value . Clearly,  also performs a  on .
By construction of ,  is linearized at the release of the \emph{cas} lock on element .
Observe that  must also perform a  to the element  (otherwise one of  or  would return ).
By assumption, the write to  in shared-memory by 
(line~\ref{line:inscommit}) precedes the corresponding write to  in
shared-memory by . If , then  cannot return ---a contradiction.
Otherwise, if ,
then  reaches line~\ref{line:linw} and return
. This is because either  attempts to acquire the
\emph{cas} lock on  while it is still held by  or the value
of  contained in the \emph{rbuf} of the process executing 
has changed---a contradiction. 

If  returns  in , there exists a  that returns  in  such that there does not exist any  that returns ; . 
Suppose that such a  does not exist. Thus,  must perform
its last  on an element that stores value , perform
the action in Line~\ref{line:inscommit} and return ---a contradiction.

It is easy to verify that the conjunction of the above two claims prove that ; ,  satisfies .
\item[(2)]
If , similar arguments as applied to  prove that ; ,  satisfies .

\item[(3)]
If  returns  in , there exists  that returns \emph{true} in  such that there does not exist any  that returns \emph{true} in  such that .
The proof of this claim immediately follows from Lemma~\ref{lem:invar1}.


Now, if  returns  in , 
there does not exist an  that returns \emph{true} such that 
there does not exist any  that returns \emph{true}; 
.
Suppose by contradiction that such a  and  exist. 
Thus, the action in line~\ref{line:inscommit} by the  operation that updates some element, 
say  precedes the action in line~\ref{line:linr} by  that is associated with its 
first  (the \emph{head}).
We claim that  must read the element  newly created by
 and return ---a contradiction to the initial assumption that it returns .
The only case when this can happen is if there exists a 
operation that forces  to be unreachable from \emph{head} i.e. concurrent to the 
to  by , there exists a  that sets  to 
after the action in line~\ref{line:inswrite} by .
But this is not possible since the \emph{cas} on  performed by the 
would return .
\end{enumerate}
Thus, inductively, the sequence of state transitions in 
satisfies the sequential specification of the \textit{set} type. 
\end{proof}
Lemmas~\ref{lem:rls} and~\ref{lem:rlegal} imply:
\begin{theorem}
\label{th:lr}
 is \LS-linearizable with respect to .
\end{theorem}






\section{Complementary experiments}\label{sec:expe}
To confirm the practicality of our highly concurrent optimistic list-based set algorithm we compared its performance against the state-of-the-heart list-based set synchronized with hand-over-hand locking (or lock coupling).
To this end, we implemented the pseudocode of Algorithm~\ref{alg:elastic} in Java without further optimizations and compared it against the Java code from Herlihy and 
Shavit of the hand-over-hand lock-based linked list~\cite{HS08-book}. 

Figure~\ref{fig:alg2-vs-hoh} gives the throughput as the number of operations per millisecond by having from 1 to 64 threads running between 5\% and 
20\% of updates (either remove or insert with same probability) and the rest of contains operations. The list is initially populated with 512 values that are integers taken from 0 to 1024 with uniform distribution. The machine has 
2 8-core Intel Xeon E5-2450 running at 2.1GHz (32-way as each code is hyperthreaded).
Java is 1.7.0\_55 and the JVM is the OpenJDK 64-Bit Server VM. Each point of the graph results from the average of 10 runs of 5 seconds plus 5 seconds to warmup the JVM.
\begin{figure}
  \begin{center}
    \includegraphics[scale=0.9]{alg2-hoh-estm-intel}
    \caption{Performance of the list-based set synchronized with our Algorithm~\ref{alg:elastic} (), with hand-over-hand locking (HOHL) and with elastic transactions (E-STM)\label{fig:alg2-vs-hoh}}
  \end{center}
\end{figure}

We can observe that our optimistic algorithm () outperforms, in most cases, the list based set synchronized with hand-over-hand locking (HOHL). 
This performance is due to the optimal concurrency of our algorithm (all correct schedules are accepted as shown by Theorem~\ref{th:lrelaxed}) and the low overhead of our algorithm (a valid schedule is efficiently identified and a constant number of  are needed, only during update operations).
We also observe that the peak performance of E-STM is better than the peak performance of  after 10\% updates.
This is due to the fact that our code, kept intentionally simple, 
is not optimized with partial aborts, hence it always restarts from the beginning of the list upon abort. By contrast, E-STM is optimized to re-read 
only one node upon some conflict detection~\cite{FGG09}.   The optimal concurrency of our algorithm makes it scale despite contention whereas E-STM does not scale even starting at 5\% udpates.
Worth noting is that there is a large body of work on concurrent list-based set algorithms, and we are not claiming our algorithm to be the most efficient. 
Algorithms that are not optimal with respect to concurrency can achieve better results on some workload with a lower overhead. 
An interesting question is how far can our implementation be optimized. In particular, we know that partial aborts and wrappers inlinining could 
boost the performance of our algorithm while retaining its concurrency optimality.



 \end{document}
