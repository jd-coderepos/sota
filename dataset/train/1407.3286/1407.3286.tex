\documentclass[11pt]{article} 
\usepackage{listings}
\usepackage{fullpage}
\date{}
\usepackage{amsfonts,amssymb,amsmath,amsthm}


\parskip        0mm
\oddsidemargin  0in
\evensidemargin 0in
\textwidth      6in
\topmargin      0in
\marginparwidth 30pt
\textheight     9.2in
\headheight     .1in
\headsep        .1mm 



\usepackage{verbatim}

\newtheorem{proposition}{Proposition}[section]
\newtheorem{theorem}[proposition]{Theorem}
\newtheorem{observation}[proposition]{Observation}
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{corollary}[proposition]{Corollary}
\newtheorem{definition}[proposition]{Definition}



\newcommand{\ident}[1]{\textit{#1}\rule{0cm}{1ex}}
\gdef\dash---{\thinspace---\hskip.16667em\relax}
\gdef\smdash--{\thinspace--\hskip.16667em\relax}
\gdef\op|{\,|\;}
\newcommand{\emn}[1]{{\em #1\/}}
\newcommand{\quotefill}{}

\newcommand{\mymod}{\,(\mathrm{mod})\,}





\newcommand{\T}{\mathcal{T}}
\newcommand{\E}{\mathcal{E}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\W}{\mathcal{W}}
\newcommand{\FD}{\mathcal{D}}

\newcommand{\Naturals}{\mathbb{N}}


\newcommand{\PFD}{\ensuremath{\mathcal{P}}}
\newcommand{\SFD}{\ensuremath{\mathcal{S}}}
\newcommand{\DS}{\Diamond\mathcal{S}}
\newcommand{\WFD}{\ensuremath{\mathcal{W}}}
\newcommand{\MFD}{\ensuremath{\mathcal{M}}}
\newcommand{\DP}{\Diamond\mathcal{P}}

\newcommand{\AF}{\mathcal{AF}}

\newcommand{\seqs}[1]{\langle #1 \rangle}
\newcommand{\seq}{w}
\newcommand{\conf}{\Sigma}
\newcommand{\setConf}{\Sigma^\star}
\newcommand{\initConf}{\hat{\conf}}
\newcommand{\initSetConf}{\hat{\Sigma}^\star}
\newcommand{\seqprobconf}{W(\initSetConf,\setConf)}
\newcommand{\problem}{P} \newcommand{\Alg}{A}
\newcommand{\Trans}[1]{\ident{T}_{#1}}
\newcommand{\TrAlg}{\widetilde{A}}
\newcommand{\redCT}{\succeq^\ident{CT}}
\newcommand{\redJT}{\succeq^\ident{JT}}
\newcommand{\Dsubseteq}{\sqsubseteq}
\newcommand{\redsubset}{\succeq^*}
\newcommand{\redSolv}{\succeq^{\ident{s}}}
\newcommand{\equivSolv}{\equiv^s}
\newcommand{\tab}{~~~~}

\newcommand{\false}{\mbox{\sc false}}
\newcommand{\true}{\mbox{\sc true}}

\newcommand{\faulty}{\ident{faulty}}
\newcommand{\corr}{\ident{correct}}

\newcommand{\DsP}{\Delta_\problem}
\newcommand{\vs}{\emn{vs}.\ }




\lstdefinelanguage{delta}
{
    keywords={Variables, Augmentations, Task, const, boolean, integer, array, begin, read, write, send, receive, while, if, then, else, do, true, false, procedure, requires, ensures, function, wait, until, forall, foreach, return, exit, abort, crash, define, type, new, initially, set},
    sensitive=true,
    literate={:=}{{}}{1} {<=}{{}}{1} {>=}{{}}{1} {/=}{{}}{1},
    morecomment=[s]{/*}{*/},
    commentstyle=\itshape
}

\lstdefinestyle{nonumbers}{numbers=none}
\lstdefinestyle{numbers}{numbers=left, numberstyle=\normalsize, stepnumber=1, numbersep=7pt, xleftmargin=10pt}

\renewcommand{\lstlistingname}{Alg.}
\renewcommand{\lstlistlistingname}{Alg.}


\bibliographystyle{alpha}




\begin{document}

\title{Solvability-Based Comparison of Failure Detectors}

\author{
\begin{tabular}{c c}
Srikanth Sastry & Josef Widder\\
Google, Inc. & TU Wien\\
\end{tabular}
}




\maketitle 

\begin{abstract}   

Failure detectors are oracles that have been introduced to provide
     processes in asynchronous systems with information about faults.
This information can then be used to solve problems otherwise unsolvable in asynchronous systems.
A natural question is on the ``minimum amount of information'' a
     failure detector has to provide for a given problem.
This question is classically addressed using a relation that states
     that a failure detector~ is stronger (that is, provides
     ``more, or better, information'') than a failure detector  if  can
     be used to implement~.
It has recently been shown that this classic implementability relation
     has some drawbacks.
To overcome this, different relations have been defined, one of which
     states that a failure detector  is stronger than  if
      can solve all the time-free problems solvable by~.
In this paper we compare the implementability-based hierarchy of failure detectors to the
     hierarchy based on  solvability.
This is done by introducing a new proof technique for establishing the
     solvability relation.
We apply this technique to known failure detectors from the literature
     and demonstrate significant differences between the hierarchies.
 \end{abstract}



\section{Introduction}

Failure detectors \cite{chan:ufdfr} provide an oracular mechanism to
     circumvent  the impossibility of several problems in fault-prone
     asynchronous systems~\cite{FLP,fich:03:hirdc}.
Intuitively, the idea is to enrich asynchronous systems with
     information about failures that may be useful to overcome the
     difficulties posed by process crashes.
Chandra and Toueg~\cite{chan:ufdfr} and Chandra, Hadzilacos, and
     Toueg~\cite{chan:twfdf1} demonstrated landmark results relating to failure detectors: the results in \cite{chan:ufdfr} demonstrated the use of failure
     detectors to solve consensus and other related problems, while
     the results in \cite{chan:twfdf1} showed that any failure
     detector that can be used to solve consensus can also be used to
     implement a failure detector called~.
Since~ is also sufficient to solve consensus, it is the
     weakest failure detector to solve consensus.
To arrive at these important results, \cite{chan:ufdfr} and
     \cite{chan:twfdf1} introduced a relation to compare the ``power''
     of failure detectors: denoted by , a failure detector  to said to be
     stronger than , if  can be used to \emph{implement}
     .
     
Since \cite{chan:twfdf1}, the relation  has been used to prove
     similar results for several other problems and has motivated the
     view that failure detectors could be used as ``computability
     benchmark'' \cite{FGK11}; that is, an answer to the question on
     the weakest failure detector  to solve a problem  is
     said  to provide the minimal synchrony assumptions necessary to
     solve  in fault-prone systems \cite{chan:twfdf1,FGK11}.
This viewpoint is based on several implicit assumptions, one of which
     is that the  hierarchy of failure detectors induced by the
     relation~ is similar to hierarchies induced by other
     natural relations, in other words, that it is robust.


In the work presented here, we focus on this assumption and explore
     the nature of relations that compare failure detectors.
Incidentally, the robustness of the  relation has been
     challenged in recent work
     \cite{jayanti:ephawfd,charron-bost:10:isolt,cornejoetalAFD,cornejoetalAFD-TR}, where it was
     observed that the relation has several drawbacks; for instance,
      is not reflexive.
To overcome the drawbacks of the~ relation, new relations have
     been proposed in \cite{jayanti:ephawfd} and
     \cite{charron-bost:10:isolt}.

Jayanti and Toueg introduced a new relation in \cite{jayanti:ephawfd},
     which we denote , with a different notion of what it
     means to \emph{implement} a failure detector.
The new relation  extends  and avoids several
     drawbacks of the  relation\footnote{We provide detailed
     descriptions of the  and  relations in Section
     \ref{sec:comparisonRelations}.}.
Based on the  relation, Jayanti and Toueg then demonstrate
     that every problem has a weakest failure detector.
The results in \cite{jayanti:ephawfd} actually holds true for a
     specific class of problems, and in fact, later work by Bhatt and
     Jayanti~\cite{bhatt:oteow} shows that there exist a different
     class of problems that do not have a weakest failure detector.
The apparent contradiction\footnote{There is no real
     contradiction here.
The reconciliation between \cite{jayanti:ephawfd} and
     \cite{bhatt:oteow} is explained in \cite{bhatt:oteow}.} between
     \cite{jayanti:ephawfd} and \cite{bhatt:oteow} regarding the
     existence of weakest failure detectors demonstrates the
     significant dependence of weakest failure detector results on the
     definition of a ``problem''  and choice of the failure detector
     comparison relation.



In \cite{charron-bost:10:isolt}, Charron-Bost et al.\  advocate a
     new comparison relation denoted by~.
By definition,  if  every (time-free)
     problem solvable by  is also solvable by .
In contrast to the  and  relations, which are based on
     implementing one failure detector using another, the 
     relation depends on the set of problems solvable by each failure
     detector.
If , or , then any problem
     solvable with~ can be solved using .
Consequently, it is straightforward that~ extends 
     and~.
However, given two failure detectors  and ,
     \cite{charron-bost:10:isolt} provides no mechanism for
     demonstrating  without having to establish  or .
In effect, it is not clear how the  relation differs from
     the  and  relations.

\medskip

\paragraph{Summary of results.} 

In this paper, we address the aforementioned issues by providing a new
     proof technique to establish the  relation.
Our approach is based on algorithm transformations, and to our
     knowledge, we are first to do so in the context of failure
     detector comparison.
Although, from a technical viewpoint, the proofs are similar to
     existing proofs that establish  and  relations,
     the relationships resulting from our proof technique differ
     significantly from existing relationships among some failure
     detectors.
     
In order to illustrate the difference between  and ,
     we consider three families of failure detectors: the perfect
     failure detector  \cite{chan:ufdfr}, the Marabout failure
     detector  \cite{guer:01:hfap}, and the~ sequence
     \cite{bhatt:oteow}.\footnote{We describe these failure detectors
     in detail and give their definitions in
     Section~\ref{sec:examples}.} 

The results in \cite{guer:01:hfap} established that  and .
In contrast, we show that  may be used to solve all the problems
     solvable using , and furthermore, there are problems that
     are solvable using  but not solvable using~.
In other words, we show that  and .

The results in \cite{bhatt:oteow} show that 
     and .
In contrast, we show that  and  can be used to
     solve the same set of problems, that is, for any , the failure
     detectors   and  are equivalent with respect
     to the  relation.



The results in \cite{guer:01:hfap} and \cite{bhatt:oteow} employ  the
      relation\footnote{Using arguments similar to the ones
     presented in \cite{guer:01:hfap} and \cite{bhatt:oteow}, one can
     easily show that  and~ are incomparable and 
     is strictly stronger than   with respect to the
      relation as well.} to prove that certain failure detectors
     cannot be the weakest ones to solve the given problem.
In contrast, our results show that these conclusions drawn in
     \cite{guer:01:hfap} and \cite{bhatt:oteow} do not hold if the
     failure detectors are compared using the  relation.
Thus, different natural relations to compare failure detectors lead to
     significantly different results.

\section{The failure detector model}\label{sec:FDmodel}

We recall the basic definitions of the failure detector model
     \cite{chan:ufdfr}.
Informally, it consists of a set of crash-prone processes that are
     connected via reliable asynchronous links and have access to a failure-detector
     oracle that provides information.
In this paper, we only consider failure detectors where this
     information has the form of a subset of the processes in
     the~system.

More formally, the system consists of a finite set of \emph{processes}
     .
We assume that each process  in  has a link  to
     every process  in  over which \emph{messages} can be
     sent.
There is a discrete global time base~, and for simplicity we
     assume its range of values is the natural numbers .


\paragraph{Failures and failure patterns.}  

A \emph{failure pattern} is a function .
This means that if  then  has failed by time .
We consider crash faults only, and so , for all
     times~.
We say that  is live at time  if , and define
     the set of live processes at time  as
     .
A process  is correct in  if  is always live, that is,
     .
We say processes that are not correct are faulty\dash---or
     crashed\dash---and we abbreviate .
An \emn{environment}  is defined as a non-empty set of failure
     patterns.
In this paper, we consider the environment that consists of all
     failure patterns for .




\paragraph{Failure detectors.} 

A failure detector history  is a function .\footnote{The failure detectors considered in
     this paper always output a set of processes.
So we do not need the more general original
     definition~\cite{chan:ufdfr} here.} If  denotes the
     set of all possible histories, then a \emph{failure detector} is
     a function .

\paragraph{States and configurations.} 

Each process is modeled as a (possibly infinite) state machine 
     over the set of states  for each process .
An algorithm  is a collection of all such state machines
     .
There exists a non-empty set of states 
     that are the \emph{initial states} of .

Each communication link  is also represented by a set of
     states, and the state of each link , denoted
      is the set of messages in transit from  to
     .
The state of a link with no messages in transit is said to be the
     \emph{initial state} of the link.

The \emph{configuration} of a system is a vector 
     where  is the state of  and  is the state of
     the link .
Then a configuration in which all the processes and links are in
     initial states is called an \emph{initial configuration}.
The set of all configurations of a system is denoted  and
     the set of all initial configurations is denoted
     .
The notation  denotes the state of  in configuration ,
     and  denotes the vector of states of the processes in
      in configuration .
Similarly, the notation  denotes the state of the link
      in configuration .





\paragraph{Steps.} 

Each transition of the state machine \dash---or \emph{step} of
     the process \dash---takes as input the current state  of
     the process, zero or one message  (the ``received''
     message), and an output~ from the failure detector; it
     produces as output a new state  for the process and may send
     a message  to another process  via the corresponding
     communication link (the ``sent'' message).
Incidentally, the receipt of a message by a process  from 
     removes the message from the link  and the sending of
     a message by  to  adds the message to the link
     ; this step can then be identified by the tuple , where  is~ if no message is received
     and   otherwise, and similarly,  is   if no
     message is sent and   otherwise.






\paragraph{Schedules.} 

A schedule  of an algorithm  is a sequence of steps taken by
     processes executing ; the th step of  is denoted
     .
A \emph{projection} of a schedule  over a process~ is the
     subsequence of  consisting of only the steps executed by
      and is denoted~.




\paragraph{Time-Sequences.} 

A time-sequence  is a sequence of increasing values in ; the
     th element in  is denoted  (which represents
     the time at which the step  occurs).
Again, we define a \emph{projection} of a time-sequence  over a
     process  as a subsequence of  consisting of only the
     times at which  executes steps and is denoted~.




\paragraph{Runs.}

A run  of an algorithm  using a failure detector  is a tuple
     , where  is a failure
     pattern,  is a failure detector history,  is an initial configuration of~,  is a
     schedule of , and  is a time-sequence.
Run  is \emph{valid for }\dash---or just \emph{valid} for
     short\dash---if correct processes take an infinite number of
     steps and if
 for each , the step  satisfies the following properties.      
\begin{itemize}

\item The process  is live at time ; that is, .

\item  is an output of the failure detector  at time ;
     formally, .

\item There are no spurious messages, that is, if  is of the form
     , then there exists some  such that  is a
     message that was sent by  to  in step  identified by
     .

\item Message transmission is reliable, that is, if  is of the
     form , then there is at most one  such that
     step  is of the form .
Furthermore, if  is correct, then there is exactly one such step.

\item If  is the first step of process in  in run , then
     .


\item The state of a process does not change between consecutive steps by that process; that
     is, if  takes another step, then the first step of 
     after   is of the form .
\end{itemize} 


\paragraph{Configuration sequences induced by runs.}
     Given a run , the
     configuration of the system after  steps are taken is given by
     .
The sequence  is the
     \emph{configuration sequence of run}~.
The state of process~ after~ takes  steps in the run is
     given by ; if process  crashes and takes
     only~ steps, then we use the convention that
      for .

Note that if two runs share the same  and  (but differ, for
     instance, at the times steps are taken), then they induce the
     same configuration sequence.



\section{Solving problems}\label{sec:problemDefinition}

We now define the notion of a \emph{problem} and what it means to
     \emph{solve} a problem.
Problems traditionally depend on initial values (as in consensus
     \cite{FLP}) and transitions to certain states depending on the
     initial values.
So we have to define a problem by referring to problem states.
Problems also depend on the correctness of processes.
For instance, faulty processes are not required to make progress.
In the failure-detector model, faults are modeled by failure patterns,
     which define after what time faulty processes must not take
     steps.
However, before that, processes need not take steps.
As we want to get rid of all time dependencies in the problem
     definition, it is hence natural to restrict problems by the set
     of processes that appear in the failure pattern rather than
     restricting the problems by the times at which  processes appear
     in the failure pattern.
This is done in the \emph{crash time independence} property described
     later.

Moreover, as we define problems to be solvable in asynchronous
     systems, we have to consider the nature of runs in such systems.
Since message delays and process speeds are unconstrained in
     asynchronous systems, processes may take finitely many idempotent
     or no-op steps while waiting for a message, or while waiting on
     some local predicate to become true.
To reflect this, we require that problems are tolerant to \emph{finite
     stuttering} which is described after the following preliminary
     definitions.

We start by defining  as a set of \emph{problem states}.
By  we denote the set of \emph{initial problem states},
     with .
A \emph{problem configuration}  for a system of size  is an
     -dimensional vector of problem states.
We denote by , the problem state associated with process
      in the problem configuration .
A problem configuration consisting only of initial problem states is
     called an \emph{initial problem configuration} .
We denote  to be the set of all possible problem
     configurations, and we denote  to be the set of all
     possible initial problem configurations; note that .
We denote  to be the set of all sequences of problem
     configurations that start with an initial problem configuration.

Further, let  be a finite problem configuration
     sequence starting with an initial problem configuration,  let
      be a problem configuration sequence, and
     let  and  be two problem configurations.
Let  be any problem configuration such that for each
     process , either  or
     .
Then, for any problem configuration sequence , the sequence  is a \emph{-stutter} of  denoted by
     .
Inductively for each , we define  to be an
     \emph{-stutter} of~, denoted by , if there is a sequence  such that .
Further, we define~ to be a \emph{stutter} of , denoted by
     , if either  or there is an
     , , such that .

\paragraph{Problems.}
Briefly, a problem is a predicate over a problem configuration
     sequence that starts with an initial problem configuration, and a
     fault pattern.
More precisely, a \emph{time-free problem}  over
       
in fault
     environment~\dash---or just \emph{problem} for short\dash---
     is a predicate  on  with
     the following properties: 
\begin{itemize}

\item \emph{Crash time independence.}   For all failure patterns  and  in
      and for all  in~,   implies .

\item \emph{Finite stuttering.} For any failure pattern , and any
     two problem configuration sequences  and  in
     ,  implies
     .
\end{itemize}




\paragraph{Solving a problem}

Let  be an algorithm, and let a problem  be defined
     for  and .
Let an \emph{interpretation}  be a function that maps the states
      of  to  (the problem states that constitute
     ), such that the initial states of the
     algorithm~ are mapped onto 
     (surjective).
This naturally extends to a function  that maps configurations
      to problem configurations.
An \emph{interpreted run} is a sequence of problem configurations
     obtained by applying   to the configuration sequence of a
     valid run  of~; it is
     denoted by .
Further, the set of all interpreted runs of algorithm 
     using~ with failure pattern  interpreted by  is
     denoted by .

Algorithm  solves a problem  using failure detector
      in environment , if there is a function   such
     that for all  in  and any ,  the predicate  holds.
If there is an algorithm that solves problem  using failure
     detector  we say that failure detector~ \emph{can be used
     to solve} , or in other words  \emph{is
     solvable using} .

\medskip

The definition of a problem encompasses many common problems in distributed computing, including classic agreement problems.
The set of problem states of consensus, for instance, can be defined
     as . A problem state  at process  signifies a state where a process  has
      as its proposed initial value, and  is its decision; if  has not yet decided, then 
     , and otherwise  is 's final decision.
The set of initial problem configurations  is the set of
     all -element vectors where each -th element is a problem state of  and is of the form .
One can then naturally define the consensus properties agreement,
     termination, and validity as predicates on problem configuration
     sequences, and consensus as the conjunction of these predicates.



\section{Comparison relations}\label{sec:comparisonRelations}

\paragraph{Chandra-Toueg relation.}  

We recall from~\cite{chan:ufdfr,chan:twfdf1} that  is
     defined via failure detector transformation as follows.
An  algorithm  uses  to maintain a
     variable  at every process~.
This variable emulates the output of  at .
Let  be the history of all the  variables in run
     , that is,  is the value of  at
     time~ in run~.
Algorithm  \emph{transforms}~
     \emph{into}  if for every valid run  of  using ,
     .
If such an algorithm  exists, then .

\paragraph{Jayanti-Toueg relation.}  

The relation , introduced in \cite{jayanti:ephawfd}, differs
     from  in that the notion of what it means to transform a
     failure detector is different from the one used
     in~\cite{chan:ufdfr}; partly by changing the computational model.
Instead of using the failure detector value at the time the step
     occurs, the ``query mechanism'' is modeled via a query to the
     failure detector at time~ and a response from the failure
     detector at some time .
Specifically, an algorithm  uses  and
     transforms  to  if and only if, for every valid run of
     , there exists a history  of 
     under the failure pattern of the run such that the following is
     true.
For each process~, and for each query by  to
      which happens at some time ,
      responds with an output  at some
     time , and .
Hence, the definition of transformation does not require  maintaining
     a variable  but rather requires ensuring
     consistency of the query and response events.


\paragraph{Solvability relation.} 

The relation , introduced in \cite{charron-bost:10:isolt},
     states that a failure detector  is stronger than 
     with respect to the solvability relation, denoted
     , if~ {can be used to solve} any problem
     solvable using .


\bigskip



The definitions of  and  provide a straightforward
     proof technique to demonstrate the claims  and .
In order to prove  or   one has to
     provide an algorithm  that has the
     properties described above.


If  then every problem solvable with  is solvable
     with  \cite{chan:ufdfr,chan:twfdf1} and thus  
     extends .
Similarly, one sees that  extends  as well.
However, if  , no proof technique has been given so
     far to establish .

\section{New technique for proving the solvability relation} 

Our approach is based on the following idea.
If a problem  is solvable using , then there exists an
     algorithm  that uses  and solves .
If we can transform  to another algorithm  such that
      uses  and solves , then we have shown that
     problem  is also solvable using .
Furthermore, if we demonstrate the aforementioned result for every
     problem solvable using , then we have shown that
     .

More generally, the proof technique focuses on defining a
     transformation function~ whose domain is the set of
     all algorithms that use  and whose range is the set of
     algorithms that use  such that if algorithm   uses
      to solve , then  uses 
     and solves .

In order to prove that the function~ actually has this
     desired property, we consider an arbitrary problem 
     solvable using .
We do so by considering an algorithm  that solves 
     using .
By definition, such an algorithm must exist.
Moreover, there is a function  which maps configurations of
     each valid run  of  using  to a sequence of problem
     configurations that satisfy .
Using , we define a new function  that maps
     the configurations of  to problem
     configurations.
We then have to show that for any interpreted run ,  the
     predicate  holds.



\section{Failure detectors under consideration}\label{sec:examples}


\subsection{Definitions}

In this section we define the three kinds of failure detectors that we
     are going to use in this paper.
The \emph{perfect failure detector}  was originally proposed in
     \cite{chan:ufdfr}.
Informally,  eventually and permanently suspects crashed
     processes and never suspects live processes.
More precisely,  is defined to ensure \emph{strong completeness}:   

and \emph{strong accuracy}:



The \emph{Marabout} failure detector  was introduced in
     \cite{guer:01:hfap}\footnote{Although the definition printed in
     \cite{guer:01:hfap} is slightly different (only failure detector
     outputs of correct processes instead of live processes are
     restricted), we claim that actually the definition given here is
     used in the proof sketches in \cite{guer:01:hfap}.
Otherwise, for instance, the proof sketch of
     \cite[Proposition~3.3]{guer:01:hfap} would fail; one could easily
     construct a case where a process that is going to crash in the
     future decides differently from a correct process.},
and it always outputs the set of faulty processes.
It is defined as:  




     
 

The  failure detector was introduced in \cite{bhatt:oteow}
     (using the notation ``'' which we find somewhat
     inconsistent with the rest of our notations).
Informally,  can provide arbitrary information about processes
     that crash before or at time .
For correct processes and processes that crash after time ,
      never suspects these processes before they crash, and
      eventually and permanently suspects these processes
     after they crash.
Formally,  satisfies the properties -Completeness: 

and -Accuracy:



\subsection{Comparing  and .}

In \cite{guer:01:hfap} it was shown that  and  are not
     comparable with respect to .
Informally, the arguments for the result are as follows.
No algorithm can tell by message exchange or from looking at the
     output of~ at a certain time which processes will
     eventually crash (in the future), therefore .
For showing , note that faulty processes should
     not be put into the set of suspected processes too early by
     , as this would violate strong accuracy.
However, by strong completeness of , crashed processes have to
     be added to the set eventually.
The outputs of  do not allow us to reconcile these two
     requirements.
Hence, no algorithm that queries  can implement ; in other
     words, .
Similar arguments also apply to the  relation, and it can be
     shown that  and  are incomparable with respect to the
      relation as well.

In this paper, we show for the solvability relation, that
      and .
Demonstrating  is straightforward.
It is sufficient to give a problem solvable using  and not
     solvable using .
Consider the following variant of consensus, called \emn{strong
     consensus}, which requires that all the correct processes have to
     output the input value of some unique \emph{correct} process in
     the system, if there is a correct process, and otherwise output
     anything.

Solving this problem using  is straightforward.
Each process sends its input to all the processes and waits for inputs
     from  the set of processes not suspected by .
Since the processes not suspected by  are the correct processes,
     if each process decides on the input of the correct process with
     the smallest ID, the problem is solved.
However, as  does not provide information on process crashes in
     the future, we can show that there is no algorithm that solves
     strong consensus using .
So we conclude that .

In order to establish that  is strictly stronger than , it
     remains to show that .
We shall do so in Section~\ref{sec:MP} in which we introduce a general
     transformation \emph{Stall-on-Suspect} that transforms any
     algorithm  using  into an algorithm  using
     .
Intuitively, Stall-on-Suspect ensures that faulty processes do not
     participate in the algorithm.
Given an algorithm , each process first queries  to
     determine whether it is correct or faulty.
If a process  queries  and discovers that it is faulty,
     then  stops participating in the algorithm by performing
     only no-op steps and sends no messages until it crashes.
Otherwise, process~ follows the original algorithm 
     faithfully.
We show in Section~\ref{sec:MP} that each valid run of the modified
     algorithm using  is indistinguishable from some valid run
     of the original algorithm using  where faulty processes
     crash initially, at time .
Since, by assumption, the original algorithm solves the  problem using
     , the same problem is solvable by  as well.
Thus, we show that every problem solvable by  is also solvable
     by .

\subsection{Comparing  failure detectors}

In \cite{bhatt:oteow}, the series of  failure detectors were
     proposed to solve FCFS mutual exclusion.
Note that various values of  instantiate different failure
     detectors, and it was shown in \cite{bhatt:oteow} for all  that  and .
The proof of the former is based on the observation that the trivial
     transformation (namely, at each step, write the current failure detector
     output into ) is sufficient to implement
      using ; intuitively, correctness follows because the histories of
      are a strict subset of the histories of
     .\footnote{This argument is in general not sufficient
     to prove  as shown in~\cite{charron-bost:10:isolt}.
It works in this case, as  belongs to the class of failure
     detectors called ``time-free'' in~\cite{charron-bost:10:isolt};
     they allow finite stuttering.}

The latter () is established by
     showing that no algorithm that queries  can reliably
     detect if some process has crashed at time , which is a
     necessary requirement to implement .
Similar arguments show  for all  that  and   


In this paper, we show for all  that .
Demonstrating  is straightforward and it
     follows from the result  from
     \cite{bhatt:oteow} and the observation that  extends
      \cite{chan:ufdfr}.

Therefore, it remains to be shown that .
We do so in Section~\ref{sec:DK} using a general transformation
     \emph{Delay-a-Step} which just adds a no-op step at the beginning
     of each execution for each algorithm.
Given an algorithm  that solves some problem  using
     failure detector , in the delay-a-step transformation,
     each process  first executes a no-op step in which 
     neither receives nor sends any message; thereafter, 
     executes the algorithm  but queries  instead of
     .
We show in Section \ref{sec:DK} that each valid run of the modified
     algorithm using  induces an interpreted run that is
     also an interpreted run (with ``shifted'' failure pattern) of the
     original algorithm using .
Since, by assumption, the original algorithm solves  using
     , problem  is solvable by  as well.
Thus, we show that every problem solvable by  is also solvable
     by .




\section{Every problem solvable using  is solvable using }
\label{sec:MP}

\subsection{Algorithmic transformation: Stall-on-Suspect}
\label{subsec:algTrans}

Informally, the \emph{Stall-on-Suspect} transformation (SoS) converts
     an algorithm  to an algorithm  such that 
     at a process  behaves exactly like  if the failure
     detector at  does not suspect itself initially.
Otherwise,  goes into a special stall state in which it
     remains for the remainder of the execution.
 
More precisely, the SoS transformation is defined by a function
      that maps an algorithm  that uses a failure detector that
     outputs a list of suspected processes to a new algorithm .
The new algorithm  is constructed as follows.
First, for each process , we add a new set of states
      to the states of , such that .
The states in  are not initial states in .
We define a bijective function  that maps the initial states of process
      to states in .


The state transitions in  differ only in the transitions
from initial states:
If a process~ of~ is in state , and if
     the failure detector output of a step of  contains ,
     then~ sends no message and goes into state
     .
Otherwise, 's step is the one specified by~.
If a process  of  is in , then 
     sends no message and remains in state  in each step.




\subsection{Solving  using }

Consider the algorithm .
Let  be an arbitrary valid run of~ using .
Let  and  be the schedule and time sequence obtained by
     removing the entries corresponding to steps of processes in
      from  and ,
     respectively.


\begin{proposition}\label{prop:RvalidMFDrun}
If  is a valid run of  using failure detector
     , then  is a valid run
     of  using  where no faulty process takes a step.
\end{proposition}
\begin{proof}
To show this proposition, one has to check that the consistency
     requirements of a valid run from Section \ref{sec:FDmodel} are
     met in~.
Since the output of  at a faulty process always suspects itself,
     in the first step of a faulty process in , the process
     transitions to a state in~ and never sends a message.
Therefore, faulty processes do not send messages  in
     run~ of~.
Since correct processes never suspect themselves, they take the same
     steps in  and~ by construction.
Consequently,  does not contain any steps in which a message from a
     faulty process is received.
Apart from this, the consistency of  follows from the consistency
     of~.
\end{proof}

Given a failure pattern , let  be the \emn{initial crash
     scenario}, that is, the failure pattern where   and for any , .

\begin{proposition}\label{prop:R0validMFDrun}
If  is a valid run of  using
      where no faulty process takes a step, then  is a valid run of  using
     .
\end{proposition}
\begin{proof}
We prove this proposition by showing that  satisfies the
     consistency conditions of a valid run as specified in Section
     \ref{sec:FDmodel}.
Note that in  all faulty processes crash at time~; therefore,
     no faulty process takes a steps in .
Since , the history~ is a valid history of
      for fault pattern .
Since  and  share the same schedule  and  is a valid
     run of  using , remaining consistency conditions for
      follows from the consistency of .
\end{proof}

From the definition of  and  one observes that in initial
     crash scenarios, the history of  is in the set of allowed
     histories of , and therefore we find: 

\begin{proposition}\label{prop:R0validPFDrun}
If  is a valid run of 
     using , then  is a valid run of  using .
\end{proposition}

From the three propositions above we infer

\begin{theorem}\label{thm:MPcorr}
For any valid run  of  using  there is a valid
     run  of  using
     .
\end{theorem}


Next, we argue that if algorithm  solves problem~
     using , then  solves  using~.
Assuming that  solves ,  there is an interpretation
      such that for all  in  and any ,  the predicate  holds.
As any interpreted run of  using  satisfies the problem,
     and since by Theorem~\ref{thm:MPcorr} every valid run of 
     using  can be mapped to a valid run of   using
     , we have to show that the mapping from 
     to  ensures that  also solves the problem using
     .


To this end, we obtain  by defining for each
     process  a new function  as a mapping of
     each state of  in  to a problem state: for states  we define , and for all other states  of
      we define .

As , we just speak of faulty
     (or correct) processes in the following, as no confusion may
     occur.

\begin{proposition}\label{prop:correctProcessSameVi}
If   is valid run of  using failure detector 
     and if~ is a valid run of
      using , then for any correct process  and for any
     index : 

\end{proposition}

\begin{proof}
Since,  is constructed from  by deleting the
     no-op steps taken by faulty processes, we know that each correct
     process  follows the same sequence of states in
      and .
That is, .
Since  is correct,  is never suspected by both  and
     .
Therefore, in ,  is never in any state in
     .
Hence, for each state  that  is in ,
     .
In other words, .
\end{proof}



\begin{proposition}\label{prop:faultyProcessSameVi}
If   is valid run of  using failure detector 
     and if~ is a valid run of  using ,
     then for any faulty process  and for any index :
 

\end{proposition} 

\begin{proof}
Since faulty processes do not take any steps in , we know that
     for each faulty process , and each index  in
     run , .

In run , we know from the construction of algorithm
      that each faulty process , initially, in state
     , enters a state  in its first step where , and remains there until it crashes.
Therefore, for each faulty process , and each index 
     in run , .

From the definition of , we know that
     , and
     .
As , we obtain
     .
Therefore, for each faulty process , and each index 
     in run , .

Since each process  is in the same initial state in
      and , we have .
Therefore, .
\end{proof}




\begin{theorem} 
If  solves  using~ then
      solves  using .
\end{theorem}
\begin{proof}
 Since   solves  using , we know that there exists a function  such that for any ,  the predicate  is true.

Let , and let  be an
     arbitrary valid run of  using .
Let  be a valid run of 
     using  where , 
     and  are obtained by deleting the entries associated with
     faulty processes in  and ,
     respectively.
From Propositions  \ref{prop:RvalidMFDrun}, \ref{prop:R0validMFDrun}
     and \ref{prop:R0validPFDrun}, we know that  is a valid run
     of  using .
Therefore, each   satisfies
     .

Let  be a function derived from  as
     described earlier in this section.
From Propositions \ref{prop:correctProcessSameVi} and
     \ref{prop:faultyProcessSameVi}, we conclude that  for all
     processes  and all indexes  in runs 
     and , .
Note that there is no re-ordering of steps of correct processes
     between  and ; however, steps of faulty
     processes may be missing in .
Thus, we infer .
From the finite stuttering property of problems and
     Theorem~\ref{thm:MPcorr}, we conclude that if  solves
      using~ then 
     solves  using .
\end{proof}


\begin{corollary}
 and .
\end{corollary}

\section{Equivalence among  failure detectors}
\label{sec:DK}


\subsection{Algorithmic transformation: Delay-a-Step}
\label{subsubsec:atdas}

Informally, the \emph{Delay-a-Step} transformation (DaS) converts an
     algorithm~ to an algorithm~ such that in 
     each process  first executes a single no-op step, and
     subsequently~ behaves exactly like it does in .
We define a transformation function  that maps an
     algorithm    to a new
     algorithm .
The new state space of~ is constructed as follows.
For each process , we add a new set of states , which are the initial states of ~, such
     that , to obtain the set of states
     for~.
This implies that the states in  are \emph{not} initial
     states of .
We define a bijective function .


The state transitions of  are the state transition of 
     and the following rules for initial states : if a
     process  is in state  when it takes a step,
     then  neither receives nor sends messages and goes into
     state .




\subsection{Showing  is at least as strong as }

Let  be an algorithm that solves some problem  using a
     failure detector~, and let .
The remainder of this section  shows that  solves 
     using the failure detector .

Let  be a
     valid run of  using .
In the following, we construct (in several steps)  a new initial
     configuration , a new schedule , a new time-sequence
     , a new failure pattern , and a new history  such that
     the run  is a valid run of
      using the failure detector~.
We then show that if  is a valid run of  using , then
      solves problem  using .


First, we construct the initial configuration  as follows.
For each process , .

Next, we construct the new schedule  and a new time-sequence
      as follows.
For each process , let  denote the
     index of the first entry of the form 
     in~.
The schedule  is obtained by deleting for each process  the
     step 
     from~.
A time-sequence  is obtained by deleting for each process 
     the entry  from~.

\begin{proposition}\label{lem:RprimeIsARunOfAlg}
If  is a valid run of
      using  then  is a valid run of 
     using .
\end{proposition}

\begin{proof}
By construction, the first step of each process  in  is
     of the form , and all the subsequent steps
     of  are the same as in .
Since  is a schedule of , we see that for
     each process ,  is the
     first step of  executing , and is therefore a
     no-op step of the form .
Also, note that upon executing a no-op step from
     state~, process  transitions to state
      which, by construction, is equal to
     the state~.  

Hence, by deleting the  step for each
     process~ from~, we obtain a valid schedule
     for ; that is,  is a valid schedule for a run of
     .
Similarly, by deleting the times at which the
      step occurred for each process 
     from , we obtain a valid time-sequence for ;
     that is,  is a valid time-sequence for the schedule  in
     a run of .
The proposition follows.
\end{proof}


Then we define the new failure pattern  by , for .
Intuitively, each faulty process crashes one time unit earlier in 
     than in .
Similarly, the new history~ is defined by , for all  and .

\begin{proposition}\label{lem:HinDk}
If   then .
\end{proposition} 

\begin{proof} 
Since , it follows from
-Accuracy that

and from -Completeness


Since , and ,
     substituting these functions in Equations~(\ref{eqn:k+1-accuracy})
     and~(\ref{eqn:k+1-completeness}) we obtain

and since ,

We observe that the failure detector whose histories are as described in
Equations~(\ref{eqn:k-accuracy}) and~(\ref{eqn:k-completeness}) 
     satisfies -Accuracy and -Completeness.
\end{proof}


Because  is obtained by removing the time of the first step of each
     process, it follows that for any , .
We may thus define the new time-sequence  as 
     with .


\begin{proposition}\label{lem:RIsAValidRun}
If  is a
     valid run of  using , then  is a valid run of  using .
\end{proposition}
\begin{proof}
From the construction of , we know that in run , each process
      takes the same steps as in , but each step taken
     at time  in  is taken at time  in .
From the construction of , we see that the output of the failure
     detector queried in run  at a time~ is identical to the
     output of the failure detector queried in run  at time .
Similarly, in the failure pattern , each process that crashes at time
      in  crashes at time~ in~.
Therefore, the run  is the run  after every step and the
     associated failure detector output in  is moved earlier in
     time by  unit.

Also, recall that  , from Proposition~\ref{lem:HinDk}.
Therefore, if  is a valid run of  using failure detector
     , then  is a valid run of  using .
\end{proof}
 

As  solves  using , for each process 
     there exists a function  that maps each state of  to a
     problem state.
For each process  we define a new function  as
     follows.
For each (initial) state , , and for each state  ,
     .


\begin{theorem}
If  solves problem  using failure detector ,
     then Algorithm  solves problem  using failure
     detector .
\end{theorem}
\begin{proof}
Let  be a
     valid, run of  using .
Applying Propositions \ref{lem:RprimeIsARunOfAlg}, \ref{lem:HinDk},
     and \ref{lem:RIsAValidRun}, we see that from  we
     can construct a unique run 
     that is a valid run of  using .

Note that by assumption  solves problem  using failure
     detector .
Hence there is an interpretation  which ensures that
      holds.
Since , applying the crash time independence
     property from Section \ref{sec:problemDefinition}, we obtain that
      is true.

Note that for each process ,  is never is a state  in run , and for each state  ,
     .
Therefore, .

Also, note that for each process , for each state , , and
     ; therefore,
     .
In effect, .
So we apply the finite stutter property from  Section
     \ref{sec:problemDefinition} and see that since
      is true,
      is also true.

We thus have shown that for any interpreted run ,
     the predicate  holds.
In other words,    solves   using
     failure detector .
\end{proof}

\begin{corollary}
 and .
\end{corollary}


\section{Conclusion}

In this paper, we introduced a new proof technique that compares
     failure detectors and does not depend on the ability of one
     failure detector to implement another.
Instead, we propose a novel approach which is based on algorithm
     transformation  so that for every algorithm  that solves
     some problem using failure detector  we derive a new
     algorithm~ which solves the same problem using 
     instead, and thus we show , where  is
     the solvability relation introduced
     in~\cite{charron-bost:10:isolt}.

We demonstrated the utility of the new proof technique by presenting
     two new results.
First, we showed that the  and  failure detectors, which
     are incomparable with respect to the~ and  relations,
     are strictly ordered with respect to the  relation;
      is strictly stronger than .
Second, we showed that the  series of failure detectors
     (denoted by  in \cite{bhatt:oteow}), which were shown to be
     strictly ordered as  for all , are
     equivalent to each other with respect to the  relation.

\paragraph{Significance.}

The primary motivation for the introduction of the  failure
     detector in \cite{guer:01:hfap} was to show that  is not
     the weakest failure detector for certain problems such as
     non-blocking atomic commitment or terminating reliable broadcast.
This was done by showing that  and~, despite being
     incomparable with respect to , can be used to solve the
     aforementioned problems under consideration.
However, we have shown that  and  can be strictly ordered
     with respect to .
This shows that the reasoning used in \cite{guer:01:hfap} is limited
     only to the   relation.\footnote{It was later shown in
     \cite{larrea:otwfd} that failure detectors that are weaker with
     respect to  than both  and  are sufficient to
     solve non-blocking atomic commitment and terminating reliable
     broadcast.
However, our motivation was not to find a weakest failure detector for
     a given problem, but rather to make explicit that certain proofs
     are limited to the  relation.} 


Similarly, the  sequence of failure detectors was introduced
     in \cite{bhatt:oteow} in order to demonstrate that FCFS mutual
     exclusion does not have a weakest failure detector.
The proof relies on the fact that for any ,  is strictly
     stronger than  with  respect to~
     while every such  is sufficient to solve FCFS mutual
     exclusion.
However, we have shown that all the  failure detectors are
     equivalent with respect to~ and, therefore, these
     failure detectors solve the same set of time-free problems.


The above two examples show that some results on weakest failure
     detectors based on the  and  relation do not
     carry over to the  relation.
This, in conjunction with the seemingly contradictory results
     regarding the (non)existence of weakest failure detectors in
     \cite{jayanti:ephawfd} and \cite{bhatt:oteow}, leaves open the
     possibility that the use of failure detectors as ``computability
     benchmark'' \cite{FGK11} may not be appropriate until we have
     resolved the question of the ``right'' comparison relation to
     order failure detectors.


\paragraph{Comparison to standard proofs.}

From a technical viewpoint, our new proof technique is  quite similar
     to proofs that establish the  relation.
In both, one argues about an algorithm using some failure detector.
In  proofs, one usually gives an algorithm more or less
     explicitly, while we give an algorithm~ as function of
     another algorithm~.
In  proofs, one shows that the states the algorithm goes
     through are related to histories of the implemented failure
     detector.
In our  proofs, we show that the states the algorithm goes
     through are related to problem configuration sequences.

The differences in the comparison relations discussed above then come
     from the fact that we relate to a schedule of algorithm  which
     is within the world of asynchronous runs, while  proofs
     relate to a failure detector history, which is defined with
     respect to time, and is hence outside the world of asynchronous
     runs.


\paragraph{Future Work.} 

Our results are preliminary and provide multiple avenues for future
     work.
We present two such open questions.
First, note that the proof technique introduced here does not
     necessarily characterize the  relation completely.
That is, there might be other proof techniques which establish the
      relation between two failure detectors in the cases
     where our proposed technique does not lead to the required
     result.
Thus, there is scope for complete characterization of the 
     relation.
Second, since different comparison relations establish different
     relationships among various failure detectors, an obvious
     question presents itself: is there a ``right'' comparison
     relation for failure detectors? If yes, which one is it?

\paragraph{Acknowledgement.} We would like to thank Jennifer Welch and Martin Hutle for their comments, suggestions, and criticisms that greatly helped improve this article.

\bibliography{master}
\end{document}
