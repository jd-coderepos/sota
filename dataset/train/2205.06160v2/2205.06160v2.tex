

\subsection{Training Details}
\subsubsection{Datasets.}
The \textbf{Common Objects in Context (COCO) dataset} \cite{lin2014microsoft} is a large-scale object detection benchmark widely used in the community. We use the 2017 train and val split for training and evaluation respectively. We use the known and novel object class splits proposed by Bansal~\etal~\cite{bansal2018zero}. The known set consists of 48 classes while the novel set has 17 classes selected from the total of 80 classes of the original COCO dataset. We remove the images which do not contain the known class instances from the training set. 
For the localized semantic matching phase, we use the captions from \textbf{COCO captions \cite{chen2015microsoft}} dataset which has the same train/test splits as the COCO object detection task. COCO captions dataset contains 118,287 images with 5 captions each. 
Additionally in the supplementary material, we test \modelname using \textbf{Visual Attributes in the Wild (VAW) dataset \cite{Pham_2021_CVPR}} a more challenging dataset containing fine-grained classes with a long-tailed distribution.
\begin{table*}[t]\scriptsize
\centering 
\caption{Comparing mAP and AP\textsubscript{50} state-of-the-art methods. \modelname outperforms all other methods for Novel objects in the generalized setup while using only 0.6M of image-caption pairs. Training dataset: ImageNet1k, COCO captions, CLIP400M, Conceptual Captions, Open Images, and COCO
}
    \label{tab:coco_sota_gen}
\begin{tabular}{ |c|c|c c|c c|c c|c c|c c|} 
 \hline 
 \multirow{3}{*}{Method} & Img-Cap &\multicolumn{4}{|c|}{Constrained} & \multicolumn{6}{|c|}{Generalized} \\
 & Data &\multicolumn{2}{|c|}{Novel (17)} & \multicolumn{2}{|c|}{Known (48)} & \multicolumn{2}{|c|}{Novel (17)} & \multicolumn{2}{|c|}{Known (48)} & \multicolumn{2}{|c|}{All (65)}\\ 
 & Size & AP & AP\textsubscript{50} & AP & AP\textsubscript{50} & AP & AP\textsubscript{50} & AP & AP\textsubscript{50} & AP & AP\textsubscript{50}\\ \hline
 Faster R-CNN & \multirow{6}{*}{-} & & - & - & 54.5 & - & - & - & - & - & - \\ \hline
 SB \cite{bansal2018zero} & & - & 0.70 & - & 29.7 & - & 0.31 & - & 29.2 & - & 24.9 \\ LAB \cite{bansal2018zero} & & - & 0.27 & - & 21.1 & - & 0.22 & - & 20.8 & - & 18.0 \\ DSES \cite{bansal2018zero} & & - & 0.54 & - & 27.2 & - & 0.27 & - & 26.7 & - & 22.1 \\ DELO \cite{Zhu_2020_CVPR} & & - & 7.6 & - & 14.0 & - & 3.41 & - & 13.8 & - & 13.0 \\ PL \cite{Rahman_Khan_Barnes_2020} & & - & 10.0 & - & 36.8 & - & 4.12 & - & 35.9 & - & 27.9 \\
STT-ZSD \tiny{(Ours)} & & 0.21 & 0.31 & 33.2 & 53.4 & 0.03 & 0.05 & \textbf{33.0} & 53.1 & 24.4 & 39.2 \\
 \hline
 OVR \cite{zareian2021open} & \multirow{2}{*}{0.6M} & 14.6 & 27.5 & 26.9 & 46.8 & - & 22.8 & - & 46.0 & 22.8 & 39.9\\
\modelname \tiny{(Ours)}& &\textbf{17.2} & 30.1 & \textbf{33.5} & 53.4 & \textbf{16.6} & \textbf{28.6} & 31.9 & 51.3 & \textbf{28.1} & 45.7\\
 \hline
 XP-Mask \cite{huynh2021open}& 5.7M & - & 29.9 & - & 46.8 & - & 27.0 & - & 46.3 & - & 41.2 \\ CLIP (cropped reg) \cite{gu2022openvocabulary}& 400M & - & - & - & - & - & 26.3 & - & 28.3 & - & 27.8 \\ RegionCLIP \cite{zhong2021regionclip}& 400.6M & - & \textbf{30.8} & - & \textbf{55.2} & - & 26.8 & - & 54.8 & - & 47.5 \\ ViLD \cite{gu2022openvocabulary}& 400M & - & - & - & - & - & 27.6 & - & \textbf{59.5} & - & \textbf{51.3} \\ \hline
    \end{tabular}
\end{table*}
 \subsubsection{Evaluation metric.} 
We evaluate our method using mean Average Precision (AP) over IoU scores from 0.5 to 0.95 with a step size of 0.05, and using two fixed thresholds at 0.5 (AP\textsubscript{50}) and 0.75 (AP\textsubscript{75}). We compute these metrics separately for novel and known classes, calculating the softmax within the subsets exclusively; and in a generalized version both sets are evaluated in a combined manner, calculating the probability across all classes.

\subsubsection{Implementation details.}
We base our model on Faster R-CNN C4 \cite{NIPS2015_14bfa6bb} configuration, using ResNet50 \cite{7780459} backbone pre-trained on ImageNet \cite{5206848}, together with a linear layer (projection layer) to obtain the object feature representations. We use Detectron2 framework \cite{wu2019detectron2} for our implementation. For the part-of-word feature representations, we use the embedding module of the pre-trained BERT \cite{DBLP:journals/corr/abs-1810-04805} ``base-uncased" model from the HuggingFace implementation \cite{wolf-etal-2020-transformers}. To get the object proposals for the LSM stage, we train a generic object proposal network, OLN \cite{kim2021oln}. OLN is trained using only the known classes on COCO training set. We use all the proposals generated for the training images which have an objectness score higher than 0.7. For our cross-attention model, we use a transformer-based architecture with 6 hidden layers and 8 attention heads trained from scratch. We train our LSM stage with a base learning rate of 0.001, where the learning rate is divided by 10 at 45k and 60k iterations. We use a batch size of 32 and train on 8 GeForce-RTX-2080-Ti GPUs for 90k iterations.
For the STT stage, we initialize the weights of the Faster R-CNN and projection layer from the LSM stage, freezing the first two blocks of ResNet50 and the projection layer. For object classes that contain more than one part-of-word representation given BERT embedding module, we consider the average of their vector representation. We use a base learning rate of 0.005 with a 10 times drop at 60k iterations and do early stopping to avoid over-fitting. 


\begin{figure}[t!]
\centering
\begin{tabular}{c c c c}
(a) Ground Truth & (b) STT-ZSD & (c) OVR \cite{zareian2021open} & (d) \modelname  \6pt]


 \includegraphics[trim={0cm 0cm 0 0cm}, clip, width=30mm]{./images_qualitative_gt_000000129756.jpg} &
 \includegraphics[trim={0 0cm 0 0cm}, clip, width=30mm]{./images_qualitative_sup_pd_000000129756.jpg} &
 \includegraphics[trim={0 0cm 0 0cm}, clip, width=30mm]{./images_qualitative_ovr_pd_000000129756.jpg} &
 \includegraphics[trim={0 0cm 0 0cm}, clip, width=30mm]{./images_qualitative_ours_pd_000000129756.jpg}  \\

 \includegraphics[trim={0 0cm 0 0cm}, clip, width=30mm]{./images_qualitative_gt_000000532761.jpg} &
 \includegraphics[trim={0 0cm 0 0cm}, clip, width=30mm]{./images_qualitative_sup_pd_000000532761.jpg} &
 \includegraphics[trim={0 0cm 0 0cm}, clip, width=30mm]{./images_qualitative_ovr_pd_000000532761.jpg} &
 \includegraphics[trim={0 0cm 0 0cm}, clip, width=30mm]{./images_qualitative_ours_pd_000000532761.jpg}  \\
 


 \includegraphics[trim={0 0cm 0 0cm}, clip, width=30mm]{./images_qualitative_gt_000000233033.jpg} &
 \includegraphics[trim={0 0cm 0 0cm}, clip, width=30mm]{./images_qualitative_sup_pd_000000233033.jpg} &
 \includegraphics[trim={0 0cm 0 0cm}, clip, width=30mm]{./images_qualitative_ovr_pd_000000233033.jpg} &
 \includegraphics[trim={0 0cm 0 0cm}, clip, width=30mm]{./images_qualitative_ours_pd_000000233033.jpg}  \\
\end{tabular}
\caption{Qualitative results for open-vocabulary object detection on MSCOCO dataset. Novel classes are shown in \textcolor{magenta}{magenta} while known are in \textcolor{green}{green}. Methods compared are described in Section \ref{sec:baselines}. (Best viewed in color)}
\label{fig:coco_qual_results}
\end{figure} \subsection{Baselines} \label{sec:baselines}
\textbf{OVR}. The main baseline approach is proposed by Zareian \etal \cite{zareian2021open}. We utilize some components proposed in the work including the two-stage design, grounding loss and usage of a cross-attention model. In this work, we propose new components, which simplify and improve the model performance over OVR. \\
\textbf{STT-ZSD}. Our second baseline uses only the Specialized Task Tuning stage. This resembles a zero-shot object detection setting. 
The model is initialized with ImageNet~\cite{5206848} weights with a trainable projection layer.\\
\textbf{Zero-shot methods}. We compare to some zero-shot object detection approaches which do not include the weak supervision provided by the captions. We compare to three background-aware zero-shot detection methods, introduced by Bansal \etal \cite{bansal2018zero}, which project features of an object bounding box proposal method to word embeddings. The \textbf{SB} method includes a fixed vector for the background class in order to select which bounding boxes to exclude during the object classification, \textbf{LAB} uses multiple latent vectors to represent the different variations of the background class, and \textbf{DSES} includes more classes than the known set as word embedding to train in a more dense semantic space. \textbf{DELO}~\cite{Zhu_2020_CVPR} method uses a generative model and unknown classes to synthesize visual features and uses them while training to increase background confidence. \textbf{PL}~\cite{Rahman_Khan_Barnes_2020} work deals with the imbalance between positive vs. negative instance ratio by proposing a method that maximizes the margin between foreground and background boxes. \\
\textbf{Faster R-CNN}. We also compare with training the classical Faster R-CNN model only using the known classes.\\
\textbf{Open-vocabulary with large data}. We compare our method with recent open-vocabulary models. RegionClip \cite{zhong2021regionclip} uses the CLIP \cite{radford2021learning} pre-trained model to produce image-region pseudo labels and train an object detector. CLIP (cropped reg)~\cite{gu2022openvocabulary} uses the CLIP pre-trained model on 400M image-caption pairs on object proposals obtained by an object detector trained on known classes. XP-Mask \cite{huynh2021open} learns a class-agnostic region proposal and segmentation model from the known classes and then uses this model as a teacher to generate pseudo masks for self-training a student model. Finally, we also compare with VILD~\cite{gu2022openvocabulary} which uses CLIP soft predictions to distil semantic information and train an object detector.


\subsection{Results}
\begin{table}[t]
\footnotesize
\centering
\caption{Different image regions for the LSM stage. - grid-regions, - proposed box-regions and - ground truth box-regions of (k) known or (n) novel objects use during the LSM stage}
\label{tab:reg_ablation}
\begin{tabular}{ |c@{\hspace{1mm}}c@{\hspace{1mm}}c|c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}|c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}|c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}| } 
 \hline 
 \multicolumn{3}{|c|}{Regions} & \multicolumn{3}{|c|}{Novel (17)} & \multicolumn{3}{|c|}{Known (48)} & \multicolumn{3}{|c|}{Generalized}\\ 
 &  &  & AP & AP\textsubscript{50} & AP\textsubscript{75} & AP & AP\textsubscript{50} & AP\textsubscript{75} & AP & AP\textsubscript{50} & AP\textsubscript{75}\\
 \hline
 100 & & kn & 18.2 & 31.6 & 18.2 & 32.5 & 52.7 & 34.0 & 27.9 & 46.0 & 28.8\\ 
  & & kn & 16.3 & 28.4 & 15.9 & 32.9 & 53.1 & 34.9 & 27.6 & 45.3 & 28.8\\ 
\hline
100 & 100 & & \textbf{17.2} & \textbf{30.1} & \textbf{17.5} & 33.5 & 53.4 & 35.5 & \textbf{28.1} & \textbf{45.7} & \textbf{29.6}\\
 200 & & &15.5 & 27.1 & 15.4 & 32.2 & 52.1 & 33.9 & 27.1 & 44.5 & 28.2\\ 
& 200 & & 13.7 & 25.7 & 12.9 & \textbf{34.2} & \textbf{53.8} & \textbf{36.5} & 27.5 & 43.8 & 29.1\\  
\hline
    \end{tabular}
\end{table}

%
 
\textbf{COCO dataset.} Table \ref{tab:coco_sota_gen} shows the comparison of our method with several zero-shot and open-vocabulary object detection approaches. \modelname outperforms previous zero-shot detection methods, which show weak performance on detecting novel objects. In comparison to OVR, we improve by 2.53 AP, 3.4 AP\textsubscript{50} for the novel classes and 3.91 AP, 3.92 AP\textsubscript{50} for the known categories. We observe open-vocabulary methods including OVR and our methods have a trade-off between known and novel class performance. Our method finds a better trade-off as compared to the previous work. It reduces the performance gap on known classes as compared to the Faster R-CNN and improves over the novel classes as compared to all previous works. Our method is competitive with recent state-of-the-art methods which use more than 700 times more image-captions pairs to train, which makes our method data efficient.





Figure \ref{fig:coco_qual_results} shows some qualitative results of our method compared with the STT-ZSD baseline and OVR. Known categories are drawn in green while novel are highlighted in magenta. The columns correspond to the ground truth, STT-ZSD, OVR and our method from left to right.
\modelname is able to find novel objects with a high confidence, such as the dogs in the first example, the couch in the second and the umbrella in the third one. We observe that our method sometimes misclassifies objects with plausible ones, such as the case of the chair in the second example which shares a similar appearance to a couch. 
These examples show a clear improvement of our approach, over the other methods.
In the supplementary material we include some examples of our method showing the limitations and main cause of errors of \modelname.




\subsection{Ablation Experiments} \label{sec:ablations}





\begin{table}[t]\footnotesize
    \centering
    \caption{Ablation study showing the contribution of our proposed consistency-regularization term () and usage of BERT text embeddings on COCO validation set. We compared using frozen pretrained weights (fz) of the language model and embedding, fine-tuning (ft) or training from scratch}
    \label{tab:cons_bert_ablation_ext}
\begin{tabular}{ |@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}|c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}|c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}|c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}| } 
 \hline
 \multirow{2}{*}{} & BERT & BERT & \multicolumn{3}{|c|}{Novel (17)} & \multicolumn{3}{|c|}{Known (48)} & \multicolumn{3}{|c|}{Generalized} \\
  & Model & Emb. & AP & AP\textsubscript{50} & AP\textsubscript{75} & AP & AP\textsubscript{50} & AP\textsubscript{75} & AP & AP\textsubscript{50} & AP\textsubscript{75} \\
 \hline
 \checkmark &  & fz &  \textbf{17.2} & \textbf{30.1} & \textbf{17.5} & \textbf{33.5} & 53.4 & \textbf{35.5} & 28.1 & 45.7 & \textbf{29.6}\\
\checkmark & fz & fz & 16.7 & 29.7 & 16.7 & 33.4 & \textbf{53.5} & \textbf{35.5} & \textbf{28.2} & \textbf{45.9} & 29.5\\ 
 \checkmark &  & ft & 16.9 & 29.5 & 16.9 & 33.4 & 53.0 & 35.4 & 28.1 & 45.7 & 29.4 \\ 
 \checkmark &  & scratch & 16.0 & 28.3 & 16.2 & 30.4 & 49.6 & 31.8 & 25.8 & 42.9 & 26.6 \\ 
  & & fz & 15.4 & 27.9 & 15.2 & 32.2 & 52.1 & 34.1 & 26.3 & 43.6 & 27.3\\ 
 \hline
    \end{tabular}
\end{table} \textbf{Localized objects matter.} Table \ref{tab:reg_ablation} presents the impact of using box- vs grid-region features in the LSM stage. 
We compare our method using grid-region features , proposed box-region features , and using box-region features from the known () or novel () class annotations .
We find that the combination of grid- and box-regions proves to be best, showing a complementary behavior. 
We also considered two oracle experiments (row 1 and 2) using ground-truth box-region features from both known and novel class annotations instead of proposed box-region features.
The best performance is achieved when combined with additional grid regions (row 1). The additional grid-regions help in capturing the background objects beyond the annotated classes while box-regions focus on foreground objects, which improves the image-caption matching.









\textbf{Consistency loss and text embedding selection.} Table \ref{tab:cons_bert_ablation_ext}, shows the contribution of our consistency-regularization term. We get an improvement of 1.76 AP by introducing our consistency loss.
We compare the performance of using a pre-trained text embedding module vs learning it from scratch, fine-tuning it or considering the complete contextualized language model during the LSM stage in Table \ref{tab:cons_bert_ablation_ext}. Using the pre-trained text embedding, results in a better model.
We find out that using only the embeddings module is sufficient and better than using the complete contextualized BERT language model for the task of object detection. We argue that this is because objects are mostly represented by single word vectors, using simple disentangled text embeddings is better suited for generating object class features. In the supplementary material we include an ablation study showing that both stages of training are necessary and complementary for the success of \modelname. 



Table \ref{tab:ablation_contribution} shows the the improvement in performance for each of our contributions. Our baseline method is our implementation of OVR~\cite{zareian2021open}. Both the consistency-regularization together with the inclusion of the box-regions gives the most increment in performance for both novel and known classes. Using only the BERT Embeddings improves the novel class performance although it affects the known classes. Overall we can see that the three contributions are complementary and improve the method for open-vocabulary detection.



\begin{table}[t]\footnotesize
    \centering
    \caption{Ablation study showing the contributions \modelname.  = consistency-regularization,  = inclusion of box-regions together with grid-regions, BERT Emb. only.}
    \label{tab:ablation_contribution}
\begin{tabular}{ |@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}|c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}|c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}|c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}| } 
 \hline
 \multirow{2}{*}{} & \multirow{2}{*}{} & BERT & \multicolumn{3}{|c|}{Novel (17)} & \multicolumn{3}{|c|}{Known (48)} & \multicolumn{3}{|c|}{Generalized} \\
  &  & Emb. & AP & AP\textsubscript{50} & AP\textsubscript{75} & AP & AP\textsubscript{50} & AP\textsubscript{75} & AP & AP\textsubscript{50} & AP\textsubscript{75} \\
 \hline
 \checkmark & \checkmark & \checkmark &  \textbf{17.2} & \textbf{30.1} & \textbf{17.5} & \textbf{33.5} & 53.4 & \textbf{35.5} & 28.1 & 45.7 & \textbf{29.6}\\
 &  &  & 16.7 & 29.7 & 16.7 & 33.4 & \textbf{53.5} & \textbf{35.5} & \textbf{28.2} & \textbf{45.9} & 29.5\\ 
  &  &   & 15.5 & 27.1 & 15.4 & 32.2 & 52.1 & 33.9 & 27.1 & 44.5 & 28.2\\ 
  &  &  & 15.4 & 27.9 & 15.2 & 32.2 & 52.1 & 34.1 & 26.3 & 43.6 & 27.3\\ 
  &  &  & 14.3 & 25.6 & 14.4 & 28.1 & 47.8 & 29.3 & 23.7 & 40.9 & 24.5\\ 
 \hline
    \end{tabular}
\end{table} 

%
