\pdfoutput=1


\documentclass[11pt]{article}

\usepackage{emnlp2021}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}



\usepackage{array}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{soul}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{subfigure}
\usepackage{algorithmic}
\usepackage{enumitem}
\usepackage{amsthm,amsmath,amssymb}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage{hyperref}

\makeatletter
\newcommand{\thickhline}{\noalign {\ifnum 0=`}\fi \hrule height 1pt
	\futurelet \reserved@a \@xhline
}


\title{Self- and Pseudo-self-supervised Prediction of Speaker and Key-utterance\\ for Multi-party Dialogue Reading Comprehension}

\author{Yiyang Li \and Hai Zhao \\
	 Department of Computer Science and Engineering, Shanghai Jiao Tong University\\
	 Key Laboratory of Shanghai Education Commission for Intelligent Interaction\\and Cognitive Engineering, Shanghai Jiao Tong University\\
	 MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University\\
	\texttt{eric-lee@sjtu.edu.cn,zhaohai@cs.sjtu.edu.cn}
}


\begin{document}
	\maketitle
	\begin{abstract}
		Multi-party dialogue machine reading comprehension (MRC) brings tremendous challenge since it involves multiple speakers at one dialogue, resulting in intricate speaker information flows and noisy dialogue contexts. To alleviate such difficulties, previous models focus on how to incorporate these information using complex graph-based modules and additional manually labeled data, which is usually rare in real scenarios. In this paper, we design two labour-free self- and pseudo-self-supervised prediction tasks on speaker and key-utterance to implicitly model the speaker information flows, and capture salient clues in a long dialogue. Experimental results on two benchmark datasets have justified the effectiveness of our method over competitive baselines and current state-of-the-art models.
	\end{abstract}
	
	\let\thefootnote\relax\footnotetext{*Corresponding author. This paper was partially supported by Key Projects of National Natural Science Foundation of China (U1836222 and 61733011).}
	
	\section{Introduction}
	Dialogue machine reading comprehension (MRC, \citealp{hermann2015teaching}) aims to teach machines to understand dialogue contexts so that solves multiple downstream tasks (\citealp{yang2019friendsqa}; \citealp{li2020molweni}; \citealp{lowe2015ubuntu}; \citealp{wu2017sequential}; \citealp{zhang2018modeling}). In this paper, we focus on question answering (QA) over dialogue, which tests the capability of a model to understand a dialogue by asking it questions with respect to the dialogue context. QA over dialogue is of more challenge than QA over plain text (\citealp{rajpurkar2016squad}; \citealp{reddy2019coqa}; \citealp{yang2019friendsqa}) owing to the fact that conversations are full of informal, colloquial expressions and discontinuous semantics. Among this, multi-party dialogue brings even more tremendous challenge compared to two-party dialogue (\citealp{sun2019dream}; \citealp{cui2020mutual}) since it involves multiple speakers at one dialogue, resulting in complicated discourse structure \citep{li2020molweni} and intricate speaker information flows. Besides this, \citet{zhang2021multi} also pointed that for long dialogue contexts, not all utterances contribute to the final answer prediction since a lot of them are noisy and carry no useful information.
	
	To illustrate the challenge of multi-party dialogue MRC, we extract a dialogue example from FriendsQA dataset \citep{yang2019friendsqa} which is shown in Figure \ref{speaker_info_flow}.
	\begin{figure}[tbp]
		\includegraphics[width=0.45\textwidth]{figures/speaker_info_flow.pdf}
		\centering
		\caption{Right part: A dialogue and its corresponding questions from FriendsQA, whose answers are marked with wavy lines. Left part: The speaker information flows of this dialogue.} 
		\label{speaker_info_flow}
	\end{figure}
	This single dialogue involves four different speakers with intricate speaker information flows. The arrows here represent the direction of information flows, from senders to receivers. Let us consider the reasoning process of : a model should first notice that it is \emph{Rachel} who had a dream and locate , then solve the coreference resolution problem that \emph{I} refers to \emph{Rachel} and \emph{you} refers to \emph{Chandler}. This coreference knowledge must be obtained by considering the information flow from  to , which means \emph{Rachel} speaks to \emph{Chandler}.  follows a similar process, a model should be aware of that  is a continuation of  and solves the above coreference resolution problem as well.
	
	To tackle the aforementioned obstacles, we design a self-supervised speaker prediction task to implicitly model the speaker information flows, and a pseudo-self-supervised key-utterance prediction task to capture salient utterances in a long and noisy dialogue. In detail, the self-supervised speaker prediction task guides a carefully designed Speaker Information Decoupling Block (SIDB, introduced in Section \ref{SIDB}) to decouple speaker-aware information, and the key-utterance prediction task guides a Key-utterance Information Decoupling Block (KIDB, introduced in Section \ref{KIDB}) to decouple key-utterance-aware information. We finally fuse these two kinds of information and make final span prediction to get the answer of a question.
	
	To sum up, the main contributions of our method are three folds:
	\begin{itemize}[leftmargin=*, topsep=1pt]
		\setlength{\itemsep}{0pt}
		\setlength{\parsep}{0pt}
		\setlength{\parskip}{0pt}
		\item We design a novel self-supervised speaker prediction task to better capture the indispensable speaker information flows in multi-party dialogue. Compared to previous models, our method requires no additional manually labeled data which is usually rare in real scenarios.
		\item We design a novel key-utterance prediction task to capture key-utterance information in a long dialogue context and filter noisy utterances.
		\item Experimental results on two benchmark datasets show that our model outperforms strong baselines by a large margin, and reaches comparable results to the current state-of-the-art models even under the condition that they utilized additional labeled data.
	\end{itemize}
	
	\section{Related work}
	\subsection{Pre-trained Language Models}
	Recently, pre-trained language models (PrLMs), like BERT \citep{devlin2019bert}, RoBERTa \citep{liu2019roberta}, ALBERT \citep{lan2019albert}, XLNet \citep{yang2019xlnet} and ELECTRA \citep{clark2020electra}, have reached remarkable achievements in learning universal natural language representations by pre-training large language models on massive general corpus and fine-tuning them on downstream tasks (\citealp{socher2013recursive}; \citealp{wang2018glue}; \citealp{wang2019superglue}; \citealp{lai2017race}). We argue that the self-attention mechanism \citep{vaswani2017attention} in PrLMs is in essence a variant of Graph Attention Network (GAT, \citealp{velivckovic2017graph}), which has an intrinsic capability of exchanging information. Compared to vanilla GAT, a Transformer block consisting of residual connection \citep{He_2016_CVPR} and layer normalization \citep{ba2016layer} is more stable in training. Hence, it is chosen as the basic architecture of our SIDB (Section \ref{SIDB}) and KIDB (Section \ref{KIDB}) instead of vanilla GAT.
	
	\subsection{Multi-party Dialogue Modeling}
	There are several previous works that study multi-party dialogue modeling on different downstream tasks such as response selection and dialogue emotion recognition. \citet{ijcai2019-696} utilize the \emph{response to (@)} labels and a Graph Neural Network (GNN) to explicitly model the speaker information flows. \citet{wang2020response} design a pre-training task named Topic Prediction to equip PrLMs with the ability of tracking parallel topics in a multi-party dialogue. \citet{jia2020multi} make use of an additional labeled dataset to train a dependency parser, then utilize the dependency parser to disentangle parallel threads in multi-party dialogues. \citet{ghosal2019dialoguegcn} propose a window-based heterogeneous Graph Convolutional Network (GCN) to model the emotion flow in multi-party dialogues.
	
	\subsection{Speaker Information Incorporation}
	In dialogue MRC, speaker information plays a significant role in comprehending the dialogue context. In the latest studies, \citet{liu2021filling} propose a Mask-based Decoupling-Fusing Network (MDFN) to decouple speaker information from dialogue contexts, by adding inter-speaker and intra-speaker masks to the self-attention blocks of Transformer layers. However, their approach is restricted to two-party dialogue since they have to specify the sender and receiver roles of each utterance. \citet{gu2020speaker} propose Speaker-Aware BERT (SA-BERT) to capture speaker information by adding speaker embedding at token representation stage of the Transformer architecture, then pre-train the model using next sentence prediction (NSP) and masked language model (MLM) losses. Nonetheless, their speaker embedding lacks of well-designed pre-training task to refine, resulting in inadequate speaker-specific information. Different from previous models, our model is suitable for the more challenging multi-party dialogue and is equipped with carefully-designed task to better capture the speaker information.
	
	\section{Methodology}
	In this part, we will formulate our task and present our proposed model as shown in Figure \ref{overview}. There are four main parts in our model, a shared Transformer encoder, a key-utterance information decoupling block, a speaker information decoupling block and a final fusion-prediction layer. In the following sections, we will introduce these modules in detail.
	
	\begin{figure*}[htbp]
		\includegraphics[width=1.0\textwidth]{figures/model_overview.pdf}
		\centering
		\caption{The overview of our model, which contains a shared Transformer encoder, a key-utterance information decoupling block, a speaker information decoupling block and a fusion-prediction layer. In speaker information decoupling block, the bi-directional arrow means that the information flows from and to both sides, the unidirectional arrow means that the information only flows from start nodes to end nodes.} 
		\label{overview}
	\end{figure*}
	
	\subsection{Task Formulation}
	\label{Task_Formulation}
	Let  be a dialogue context with  utterances. Each utterance  consists of a speaker  specified by a name and a sequence of words  speaker  utters.  can be denoted as a -length sequence . Let a question corresponds to the dialogue context be , where  is the length of the question and each  is a token of the question. Given  and , a dialogue MRC model is required to find an answer  for the question, which is restricted to be a continuous span of the dialogue context. In some datasets,  can be an empty string indicating that there is no answer to the question according to the dialogue context.
	
	\subsection{Shared Transformer Encoder}
	\label{STE}
	To fully utilize the powerful representational ability of PrLMs, we employ a \emph{pack} and \emph{separate} method as \citet{zhang2021multi}, which is supposed to take advantage of the deep Transformer blocks to make the context and question better interacted with each other. We first pack the context and question as a joint input to feed into the Transformer blocks and separate them according to the position for further interaction.
	
	Given the dialogue context  and a corresponding question , we pack them to form a sequence:\\
	 = \{[CLS][SEP]:[SEP]
	: [SEP]\},
	where [CLS] and [SEP] are two special tokens and each : pair is the name and utterance of a speaker separated by a colon. This sequence  is then fed into  layers of Transformer blocks to gain its contextualized representation  where  is the length of the sequence after tokenized by Byte-Pair Encoding (BPE) tokenizer \citep{sennrich2016neural} and  is the hidden dimension of the Transformer block. Here  is the total number of Transformer layers specified by the type of the PrLM,  is a hyper-parameter which means the number of decoupling layers.
	
	\subsection{Key-utterance Information Decoupling Block}
	\label{KIDB}
	Given the contextualized representation  from Section \ref{STE}, follow \citet{zhang2021multi}, we gather the representation of [SEP] tokens from  as the representation of each utterance in the dialogue context. These representations are used to initialize  utterance nodes  and a question node  as illustrated in the middle-upper part of Figure \ref{overview}. The representations of normal tokens are gathered as token nodes  where  is the number of normal tokens in the dialogue context. Then, another  layers of multi-head self-attention Transformer blocks are used to exchange information inter- and intra- the three types of nodes:
	
	Here , , ,  are matrices with trainable weights,  is the number of attention heads and  denotes the concatenation operation.
	
	After stacking  layers of multi-head self-attention:  to fully exchange information between these nodes, we get a question representation , the utterance representations , and the token representations .
	
	 is then paired with each  to conduct the key-utterance prediction task. In detail, we use a heuristic matching mechanism proposed by \citep{mou2016natural} to calculate the matching score of the question representation and utterance representation. Here we define a matching function , where , as follows:
	
	Here  denotes element-wise multiplication and  is a vector with trainable weights. The  is an activation function to get a probability distribution according to the downstream loss function, which can be chosen from  and . In span-based dialogue MRC datasets, we set the pseudo-self-supervised key-utterance target based on the position of the answer span. We name it pseudo-self-supervised since it is generated from the original span labels, but requires no additional labeled data. Specifically, we set  where  is the index of the utterance that contains the answer span. Then we calculate the key-utterance distribution by:
	
	 is later expanded to the length of token nodes to get  which will be put forward to filter noisy utterances in the fusion-prediction layer (introduce in Section \ref{FPL}). We adopt cross-entropy loss to compute the loss of this task:
	
	The gradient of  will flow backwards to refine the representations of the utterance nodes so that they can decouple key-utterance-aware information from the original representations. After the interaction between token nodes and utterance nodes, the token nodes will gather key-utterance-aware information from the utterance nodes. Therefore, we denote the token representations as key-utterance-aware: , which will be forwarded to the fusion-prediction layer described in Section \ref{FPL}.
	
	\subsection{Speaker Information Decoupling Block}
	\label{SIDB}
	This part is the core of our model, which contributes to modeling the complex speaker information flows. In this section, we first introduce the self-supervised speaker prediction task we proposed, then depict the decoupling process of speaker information.
	\subsubsection{Self-supervised Speaker Prediction}
	As defined in Section \ref{Task_Formulation}, we have a dialogue context  where each utterance  consists of a speaker  specified by a name. We randomly choose an  utterance and mask its speaker name. Then for every  pair where , the model should determine whether they are uttered by the same speaker, that is to say, whether .
	
	We figure this task a relatively difficult one since it requires the model to have a thorough understanding of the speaker information flows and solve problems such as coreference resolution. Figure \ref{speaker_prediction_exp} is an example of the self-supervised speaker prediction task, where the speaker of the utterance in gray is masked. We human can determine that the masked speaker should be \emph{Emily Waltham} by considering that \emph{Ross} and \emph{Monica} is persuading \emph{Emily} to attend the wedding by showing her the wedding place, and when \emph{Monica} and \emph{Emily} reaches there, it should be \emph{Emily} who is surprised to say \emph{"Oh My God"}. However, it is not that easy for machines to capture these information flows.
	
	\begin{figure}[htbp]
		\includegraphics[width=0.45\textwidth]{figures/speaker_prediction_example_2.pdf}
		\centering
		\caption{An example of the speaker prediction task, which involves three speakers. \emph{Scene} here is a narrative description which introduces some additional information about the scene.}
\label{speaker_prediction_exp}
	\end{figure}
	
	\subsubsection{Speaker Information Decoupling}
	To fully utilize the interactive feature of self-attention mechanism \citep{vaswani2017attention} and the powerful representational ability of PrLMs, we also use Transformer blocks to capture the interactive speaker information flows and fulfill this difficult task.
	
	We first detach  from the computational graph to get , then as what we do in Section \ref{KIDB}, the representation of [SEP] tokens are gathered from  to initialize  unmasked speaker nodes  and a masked speaker node . The representation of normal tokens are gathered as token nodes . Then, we add attention mask to the token nodes corresponding to the selected speaker name before they are forwarded into the speaker information decoupling block, as illustrated in the middle-lower part of Figure \ref{overview}. The reasons why we use this detach-mask strategy are as follows. First, we mask the selected speaker before the speaker information decoupling block instead of at the very beginning before the encoder since it is better to let the utterance decoupling block see all the speaker names. Based on this point, we detach  from the computational graph and add attention mask to avoid target leakage. If we use a normal forward instead, the encoder would simply attend to the speaker names, which would hurt performance (discuss in detail in Section \ref{detaching}). Besides, this strategy also helps the model better decouple the key-utterance-aware and speaker-aware information from the original representations.
	
	In detail, the mask strategy is similar as \citet{liu2021filling}. We modify Eq. (\ref{eq1}) to:
	
	Let the start index and end index of the masked speaker tokens be  and , to make the selected speaker name unseen to other nodes, the attention mask is obtained as follows:
	
	By adding this mask, other nodes will not attend to the masked token nodes, thus preventing target leakage. On the mean time, the speaker nodes will have to collect clues from other nodes through deep interaction to make prediction, which implicitly models the complex speaker information flows.
	
	After stacking  layers of masked multi-head self-attention:  MultiHead([]), we get a masked speaker representation , the normal speaker representation , and the token representation .
	
	 is then paired with each  to conduct the self-supervised speaker prediction task. We also adopt the matching function defined in Eq. (\ref{match_func}):
	
	For convenience and without loss of generality, we make  which means we mask the speaker of the  utterance, in the following description. We construct the self-supervised target by:
	
	Then binary cross entropy loss is applied here to compute the loss of this task:
	
	The gradient of  will flow backwards to refine the representations of speaker nodes so that they can decouple speaker-aware information from the original representations. After the interaction between token nodes and speaker nodes, the token nodes will gather speaker-aware information from the speaker nodes. Therefore, we denote the token representations as speaker-aware: , which will be forwarded to the fusion-prediction layer described in next section.
	
	
	\subsection{Fusion-Prediction Layer}
	\label{FPL}
	Given the key-utterance-aware token representation  and the speaker-aware token representations , we first fuse these two kinds of decoupled representation using the following transformation:
	
	where  is a linear transformation matrix with trainable weights and  is a non-linear activation function.\\
	Then we compute the start and end distributions over the tokens by:
	
	where  and  are vectors of size  with trainable weights,  is defined on Section \ref{KIDB} and  is element-wise multiplication.\\
	Given the ground truth label of answer span , cross entropy loss is adopted to train our model:
	
	
	If the dataset contains unanswerable question, the representation of  at  position  is used to predict whether a question is answerable or not:
	
	where  and  are vectors of size  with trainable weights.\\
	Given the ground truth of answerability , binary cross entropy is applied to compute the answerable loss:
	
	
	The final loss is the summation of the above losses:
	
	
	
	\section{Experiments}


	\subsection{Benchmark Datasets}
	We adopt FriendsQA \citep{yang2019friendsqa} and Molweni \cite{li2020molweni}, two span-based extractive dialogue MRC datasets, as the benchmarks.
	Molweni is derived from the large-scale multi-party dialogue dataset --- Ubuntu Chat Corpus \citep{lowe2015ubuntu}, whose main theme is technical discussions about problems on Ubuntu system. This dataset features in its informal speaking style and domain-specific technical terms. In total, it contains 10,000 dialogues whose average and maximum number of speakers is 3.51 and 9 respectively. Each dialogue is short in length with the average and maximum number of tokens 104.4 and 208 respectively. Unanswerable questions are asked in this dataset, hence the answerable loss in Eq. (\ref{AL}) is applied. Additionally, this dataset is equipped with discourse parsing annotations which is not used by our model however.\\
	To evaluate our model more comprehensively, another open-domain dialogue MRC dataset FriendsQA is also used to conduct our experiments. FriendsQA excerpts 1,222 scenes and 10,610 open-domain questions from the first four seasons of a well-known American TV show \emph{Friends} to tackle dialogue MRC on everyday conversations. 
	Each dialogue is longer in length and involves more speakers, resulting in more complicated speaker information flows compared to Molweni. 
	For each dialogue context, at least 4 out of 6 types (5W1H) of questions, are generated. This dataset features in its colloquial language style filled with sarcasms, metaphors, humors, etc.
	
	\subsection{Implementation Details}
	We implement our model based on \emph{Transformers} Library \citep{wolf2020transformers}. The number of information decoupling layers  is chosen from 3 - 5 according to the type of the PrLM in our experiments. For Molweni, we set batch size to 8, learning rate to 1.2e-5 and maximum input sequence length of the Transformer blocks to 384. For FriendsQA, they are 4, 4e-6 and 512 respectively. Note that in FriendsQA, there are dialogue contexts whose length (in tokens) are larger than 512. We split those contexts to pieces and choose the answer with highest span probability  as the final prediction.
	
	\let\thefootnote\relax\footnotetext{Codes and data are available at \url{https://github.com/EricLee8/Multi-party-Dialogue-MRC}}
	
	\subsection{Baseline Models}
	For FriendsQA, we adopt BERT as the baseline model follow \citet{li2020transformers} and \citet{liu2020graph}. For Molweni, we follow \citet{li2021dadgraph} who also employ BERT as the baseline model. In addition, we also adpot ELECTRA \citep{clark2020electra} as a strong baseline in both datasets to see if our model still holds on top of stronger PrLMs.
	
	\subsection{Results}
	Table \ref{FriendsResult} shows our experimental results on FriendsQA. BERT \citep{li2020transformers} is a method using pretrain-fine-tune form. They first pre-train BERT on FriendsQA and additional transcripts from Seasons 5-10 of \emph{Friends} using well designed pre-training tasks Utterance-level-Masked-LM (ULM) and Utterance-Order-Prediction (UOP), then fine-tune it on dialogue MRC task. BERT \citep{liu2020graph} is a graph-based model that integrates relation knowledge and coreference knowledge using Relational Graph Convolution Networks (R-GCNs) \citep{schlichtkrull2018modeling}. Note that this model utilizes additional labeled data on coreference resolution \citep{chen2017robust} and character relation \citep{yu2020dialogue}. We adopt the same evaluation metrics as \citet{li2020molweni}: exactly match (EM) and F1 score. Our model reaches new state-of-the-art (SOTA) result on EM metric and comparable result on F1 metric, even without any additional labeled data. Besides, our model still gains great performance improvement under ELECTRA-based condition, which demonstrates the effectiveness of our model over strong PrLMs.
	\begin{table}[htbp]
		\centering
		\begin{tabular}{l c c}
			\thickhline
			Model &  & \\
			\hline \hline
			BERT &  & \\
			BERT (\citeauthor{li2020transformers}) &  & \\
			BERT (\citeauthor{liu2020graph}) &  & \\
			BERT &  & \\
			\hline
			ELECTRA &  & \\
			ELECTRA &  & \\
			\thickhline
		\end{tabular}
		\caption{Results on FriendsQA}
		\label{FriendsResult}
	\end{table}
	
	Table \ref{MolweniResult} presents our experimental results on Molweni. Public Baseline is directly taken from the original paper of Molweni \citep{li2020molweni}. DADGraph \citep{li2021dadgraph} is the current SOTA model that utilizes Graph Convolution Network (GCN) and the additional discourse annotations in Molweni to explicitly model the discourse structure. We see from the the table that our model outperforms strong baselines and the current SOTA model by a large margin, even under the condition that we do not make use of additional discourse annotations.
	\begin{table}[htbp]
		\centering
		\begin{tabular}{l c c}
			\thickhline
			Model &  & \\
			\hline \hline
			BERT (\citeauthor{li2020molweni}) &  & \\
			BERT &  & \\
			BERT (\citeauthor{li2021dadgraph}) &  & \\
			BERT &  & \\
			\hline
			ELECTRA &  & \\
			ELECTRA &  & \\
			\thickhline
		\end{tabular}
		\caption{Results on Molweni}
		\label{MolweniResult}
	\end{table}
	
	\section{Analysis}
	\subsection{Performance Gain Analysis}
	To get more detailed insights on our proposed method, we analyze the results on different question types of FriendsQA over ELECTRA-based model. Also, we compare our model with the baseline model on these types to see where the performance gains come from. Table \ref{PerformanceGains} shows the results of our model on different question types. Dist. means the distribution of each question type, from which we see that the question type of FriendsQA is nearly uniformly distributed. 
	
	Performance gains mainly come from question type \emph{Who}, \emph{When} and \emph{What}. We argue that the speaker information decoupling block is the predominant contributor to \emph{Who} question type since answering this type of question requires the model to have a deep understanding of speaker information flows and solve problems like coreference resolution, which is the same as our self-supervised speaker prediction task. For question type \emph{When}, the key-utterance information decoupling block contributes the most. The answer of question type \emph{When} usually comes from a scene description utterance, hence grabbing key-utterance information helps answer this kind of question. Among these improvements, question type \emph{Who} benefits the most from our model, demonstrating the strong capability of the self-supervised speaker prediction task.
	
	\begin{table}[tbp]
		\centering
		\begin{tabular}{c|c||c|c}
			\thickhline
			Type & Dist. &  & \\
			\hline \hline
			Who &  &  & \\
			When &  &  & \\
			What &  &  & \\
			Where &  &  & \\
			Why &  &  & \\
			How &  &  & \\
			\thickhline
		\end{tabular}
		\caption{Results on different question types, where up arrows represent performance gain and down arrows represent performance drop compared to the baseline model. Significant gains (greater than 3\%) are marked as \textbf{bold}.}
		\label{PerformanceGains}
	\end{table}
	
	\subsection{Ablation Study}
	We conduct ablation study to see the contribution of each module. Table \ref{AblationResult} shows the results of our ablation study. Here KIDB and SIDB are the abbreviation of Key-utterance Information Decoupling Block and Speaker Information Decoupling Block respectively. We see from the results that both of the two modules contributes to the performance gains of our final model. For FirendsQA, SIDB contributes more and otherwise for Molweni. This is because dialogue contexts in FriendsQA tend to be long, involve more speakers and carry more complex speaker information flows. On the contrary, dialogue contexts in Molweni are short with less turns and most of the questions can be answered by considering only one key-utterance.
	
	To further investigate the effectiveness of our self-supervised speaker prediction task, we design a SpeakerEmb model in which we replace the speaker-aware token representation  by speaker representations. The speaker representations are obtained by simply gathering embeddings from a trainable embedding look-up table according to the name of the speaker. Experimental results show that it only makes a slight performance gain compared to SIDB, demonstrating that simply adding speaker information is sub-optimal compared to implicitly modeling speaker information flows using our self-supervised speaker prediction task.
	\begin{table}[tbp]
		\centering
		\begin{tabular}{lcccc}
			\thickhline \multirow{2}{*}{Model} & \multicolumn{2}{c} {FriendsQA} & \multicolumn{2}{c} {Molweni}\\
			& EM & F1 & EM & F1\\
			\hline \hline 
			Our Model &  &  &  & \\
			\quad w/o KIDB &  &  &  & \\
			\quad w/o SIDB &  &  &  & \\
			SpeakerEmb &  &  &  & \\
			\thickhline
		\end{tabular}
		\caption{Results of Ablation Study}
		\label{AblationResult}
	\end{table}
	
	\subsection{Influence of Detaching Operation}
	\label{detaching}
	We conduct experiments to investigate the influence of detaching operation mentioned in Section \ref{SIDB}. As shown in Table \ref{Detach}, if we do not detach  from the original computation graph when performing the speaker prediction task, the prediction accuracy reaches 96.8\% in the test set of FriendsQA, indicating obvious label leakage. In the meantime, the EM and F1 scores drop to 54.5\% and 70.8\%, respectively. On the contrary, our model reaches a speaker prediction accuracy of 80.8\%, which demonstrates that the detaching operation can effectively prevent label leakage.
	
	\subsection{Influence of Speaker and Utterance Numbers}
	Figure \ref{number_study} illustrates the model performance with regard to the number of speakers and utterances on FriendsQA. At the beginning, the baseline model has similar performance to our model. However, with the number of speakers and utterances increasing, there is a growing performance gap between the baseline model and our model. This observation demonstrates that our SIDB and KIDB have strong abilities to deal with more complex dialogue contexts with a larger number of speakers and utterances.
	
	\begin{figure}[tbp]
		\centering
		\subfigure[Scores vs. Number of Speakers]{
			\begin{minipage}{0.47\textwidth} 
				\includegraphics[width=0.95\textwidth]{figures/speaker.pdf} \\
			\end{minipage}
		}
		
		\subfigure[Scores vs. Number of Utterances]{
			\begin{minipage}{0.47\textwidth} 
				\includegraphics[width=0.95\textwidth]{figures/utter.pdf} \\
			\end{minipage}
		}
		\caption{Influence of Speaker and Utterance Numbers}
		\label{number_study}
	\end{figure}
	
	
	\begin{table}[tbp]
		\centering
		\begin{tabular}{l c c c}
			\thickhline
			Model &  &  & \\
			\hline \hline
			Our Model &  &  & \\
			\quad w/o Detaching & 54.5 & 70.8 & 96.8\\
			\thickhline
		\end{tabular}
		\caption{Influence of Detaching Operation}
		\label{Detach}
	\end{table}
	
	\subsection{Case Study}
	To get more intuitive explanations of our model, we select two cases from FriendsQA in which the baseline model fails to answer (F1 = 0, or "exactly not match") but our model is able to answer (exactly match).
	\begin{figure}[tbp]
		\includegraphics[width=0.45\textwidth]{figures/case_study.pdf}
		\centering
		\caption{Two cases from FriendsQA} 
		\label{case_study_pic}
	\end{figure}
	Figure \ref{case_study_pic} illustrates two cases where the context of the first one is shown in Figure \ref{speaker_info_flow}.
	
	In the first case, the baseline model simply predicts that \emph{"you and I"} were in \emph{Rachel's dream} while fails to notice that \emph{"you"} here refers to \emph{Chandler}. On the contrary, our model is able to capture this information since it helps the speaker prediction task. In fact, if we mask \emph{Rachel} in , our model could tell the masked speaker is \emph{Rachel}, indicating that it knows it should be \emph{Rachel} who had a dream and  is in response to .
	
	Similar observations can be seen in the second case. The baseline model simply matches the semantic meaning of the question and the context then makes a wrong prediction. Compared with the baseline model, our model has the ability to catch the information flow from \emph{Rachel} to \emph{Monica} thus predicts the answer correctly.
	
	
	\section{Conclusion}
	In this paper, for multi-party dialogue MRC, we propose two novel self- and pseudo-self-supervised prediction tasks on speaker and key-utterance to capture salient clues in a long and noisy dialogue. Experimental results on two multi-party dialogue MRC benchmarks, FriendsQA and Molweni, have justified the effectiveness of our model.
	
	
\bibliography{main}
	\bibliographystyle{acl_natbib}
	
	\appendix
	




\end{document}
