\pdfoutput=1

\documentclass[11pt]{article}

\usepackage{acl}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}

\usepackage{dirtytalk}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{svg}
\usepackage{booktabs}
\usepackage{diagbox}
\usepackage{soul}  \usepackage[table,xcdraw]{xcolor}
\usepackage{textcomp}
\usepackage{highlight}
\usepackage{xspace}
\usepackage{enumitem}
\usepackage{float}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}

\newcommand\mstoo{{MS\^{}2}\xspace}

\definecolor{MyRed}{HTML}{D63F3F}
\definecolor{MyBlue}{HTML}{265ED4}
\definecolor{MyGreen}{HTML}{0A8F6B}
\definecolor{MyPurple}{HTML}{9467bd}
\definecolor{MyOrange}{HTML}{ff7f0e}
\definecolor{MyGrey}{HTML}{303845}

\makeatletter
    \newcommand\footnoteref[1]{\protected@xdef\@thefnmark{\ref{#1}}\@footnotemark}
\makeatother

\definecolor{brightmaroon}{rgb}{0.76, 0.13, 0.28}
\newcommand{\ac}[1]{\textcolor{brightmaroon}{[ #1 --AC]}}
\newcommand{\ls}[1]{\textcolor{violet}{[ #1 --LS]}}
\newcommand{\jg}[1]{\textcolor{cyan}{[ #1 --JG]}}
\newcommand{\llw}[1]{\textcolor{red}{[ #1 --lucy]}}
\newcommand{\kl}[1]{\textcolor{blue}{[ #1 --kyle]}}

\hyphenation{Multi-X-Science X-Science}

\setlength\titlebox{5cm}

\title{Open Domain Multi-document Summarization: A Comprehensive Study of Model Brittleness under Retrieval}


\author{
John Giorgi$^{1,2,3}$\thanks{\enspace Work performed during internship at AI2}$^{\;\;\dag}$ \quad Luca Soldaini$^{4}$\thanks{\enspace Core contributors. See \hyperref[sec:contrib]{author contributions}} \quad Bo Wang$^{1,3}$ \quad Gary Bader$^{1,2}$ \\ \textbf{Kyle Lo}$^{4\dag}$ \quad \textbf{Lucy Lu Wang}$^{4,5\dag}$ \quad \textbf{Arman Cohan}$^{4,6\dag}$\vspace{6pt}\\
  $^{1}$University of Toronto \quad
  $^{2}$Terrence Donnelly Centre \quad
  $^{3}$Vector Institute for AI \\
  $^{4}$Allen Institute for AI \quad
  $^{5}$University of Washington \quad $^{6}$Yale University\\
  {\footnotesize \bf\texttt{john.giorgi@utoronto.ca, \{lucas, kylel\}@allenai.org, lucylw@uw.edu, arman.cohan@yale.edu}}
}


\begin{document}
\maketitle
\begin{abstract}
Multi-document summarization (MDS) assumes a set of topic-related documents are provided as input. In practice, this document set is not always available; it would need to be retrieved given an information need, i.e. a question or topic statement, a setting we dub ``open-domain'' MDS. We study this more challenging setting by formalizing the task and bootstrapping it using existing datasets, retrievers and summarizers. Via extensive automatic and human evaluation, we determine: (1) state-of-the-art summarizers suffer large reductions in performance when applied to open-domain MDS, (2) additional training in the open-domain setting can reduce this sensitivity to imperfect retrieval, and (3) summarizers are insensitive to the retrieval of duplicate documents and the order of retrieved documents, but highly sensitive to other errors, like the retrieval of irrelevant documents. Based on our results, we provide practical guidelines to enable future work on open-domain MDS, e.g. how to choose the number of retrieved documents to summarize. Our results suggest that new retrieval and summarization methods and annotated resources for training and evaluation are necessary for further progress in the open-domain setting.\footnote{\label{footurl}\url{https://github.com/allenai/open-mds}}
\end{abstract}

\section{Introduction}

Summarization is an NLP task that aims to generate accurate and coherent summaries for some given text automatically. \textit{Multi-document} summarization (MDS) extends this task to provide multiple topic-related documents as input, with the goal of summarizing salient information while avoiding redundancy. MDS is a popular research objective with many proposed approaches \citep{yasunaga-etal-2017-graph, liao-etal-2018-abstract, liu-lapata-2019-hierarchical, li-etal-2020-leveraging-graph, jin-etal-2020-multi, mao-etal-2020-multi, pegasus, pasunuru-etal-2021-efficiently, xiao-etal-2022-primera} and important applications, e.g. the summarization of news articles \citep{fabbri-etal-2019-multi, gholipour-ghalandari-etal-2020-large}, scientific literature \citep{lu-etal-2020-multi-xscience, cochrane, deyoung-etal-2021-ms}, and legal documents \citep{multilexsum}.

\begin{figure}[t]
\includegraphics[width=\columnwidth]{figures/overview}
\caption{``Traditional'' MDS assumes a topic-related set of documents is given at train and test time. Here, we investigate the more challenging ``open-domain'' setting, where the document set must be retrieved given a query.
}
\label{fig:overview}
\vspace{-5mm}
\end{figure}



Existing MDS task definitions, including query-focused MDS (see \textsection \ref{related-work} for detailed comparison), assume a ground-truth, topic-related document set is provided at train and test time. This document set is often an artifact of the dataset curation process; in many practical settings, it is not available a priori and would need to be defined by an \textit{information need}, expressed as a query.\footnote{E.g. a question: \textit{``Does vitamin D improve physical capabilities of elderly patients?''} or topic statement: \textit{``Report on vulnerabilities of US power grid \& efforts to improve it.''}} Documents relevant to the query would need to be retrieved from a large collection of (mostly irrelevant) documents and summarized (\autoref{fig:overview}); a setting we refer to as \textit{open-domain} MDS.\footnote{Inspired by the QA literature, where ``open-domain'' also denotes the setting where only a query is provided as input} Because even state-of-the-art information retrieval (IR) methods are imperfect, errors, like the retrieval of irrelevant documents, will occur. It is an open question how existing summarizers behave under this more challenging but realistic setting. Our major contributions are:

\begin{itemize}[itemsep=0.2pt, topsep=3pt, leftmargin=10pt]
    \item We formalize the task definition of open-domain MDS (\textsection \ref{open-mds}), and bootstrap its study using existing datasets, retrievers and summarizers (\textsection \ref{bootstraping});
    \item We evaluate summarizers in the open-domain setting and find, through automated and human evaluations, that performance degrades significantly (\textsection \ref{experimental-results}). Promisingly, we show that additional training in the open-domain setting can reduce this sensitivity to imperfect retrieval (\textsection \ref{training});
    \item Finally, we subject summarizers to an extensive suite of carefully designed input perturbations to determine which retrieval errors are driving the degradation in summarization performance (\textsection \ref{simulations})
\end{itemize}

\noindent Based on our results, we provide detailed, practical guidelines for future work in open-domain MDS. We release all code, model \& data artifacts, and experimental results created during our investigation.

\vspace{-1.0mm}
\section{Task Definition: Open-domain MDS}
\label{open-mds}
\vspace{-1.0mm}

In traditional MDS, a model is given a set of topic-related documents \(D = \{d_1, ..., d_k\}\) and must generate a summary \(S\) that accurately and coherently summarizes the information in \(D\). Such models are typically trained in a supervised fashion to minimize the difference between \(S\) and a (usually human-written) reference summary \(R\). The goals (and evaluation) remain the same in open-domain MDS, but instead of \(D\), the inputs are a query \(q\) and a large \textit{index} of documents \(\mathcal{D_\text{index}}\), where \(\lvert \mathcal{D_\text{index}} \rvert \gg \lvert D \rvert\). The specifics of \(q\), \(\mathcal{D_\text{index}}\), and \(S\) will depend on the application or use-case. For example, in automatic literature review \citep{cochrane, deyoung-etal-2021-ms}, \(q\) would be a research question or statement, e.g. \textit{``Is massage therapy effective for people with musculoskeletal disorders?''}, \(\mathcal{D_\text{index}}\) would be a large corpus of scientific literature (e.g. PubMed), and \(S\) would be a literature review-style discussion, e.g. \textit{``Massage therapy, as a stand-alone treatment, reduces pain and improves function compared to no treatment...''}

There are multiple ways to approach open-domain MDS. We could think of \(q\) as a \textit{prompt} to a large language model (LLM) capable of in-context learning (ICL, \citealt{GPT3}); in which case we can view \(\mathcal{D_\text{index}}\) as the LLMs training data and retrieval as happening implicitly during inference. However, because all information is stored in the model's weights, this approach requires extremely large models, cannot produce summaries for events outside the training data, and does not provide provenance for the generated summaries. 

Another approach is to introduce an explicit retrieval step over an external knowledge source, which we will refer to as ``retrieve-then-summarize.'' It works as follows: a \textit{retriever} ranks all documents in \(\mathcal{D_\text{index}}\) from most-to-least relevant given \(q\). The top-\(k\) documents are input to a summarizer; \(k\) is a parameter that may be tuned for a particular use case. This approach has some desirable properties: (1) \(\mathcal{D_\text{index}}\) can be updated with new documents without re-training the retriever or summarizer, and (2) it provides provenance for model-generated summaries: the top-\(k\) documents. In the remainder of the paper, we focus our investigation on the retrieve-then-summarize approach.


\vspace{-1.0mm}
\section{Related Work}
\label{related-work}
\vspace{-1.0mm}

\paragraph{Query-focused MDS} In query-focused MDS (QMDS, \citealp{wang-etal-2013-sentence, Feigenblat2017UnsupervisedQM, xu-lapata-2020-coarse, Pasunuru2021DataAF}), a query is provided alongside a set of topic-related input documents and used to guide summarization. For example, \textit{extractive} QMDS methods use query relevance to select the sentences that form the summary. No retrieval from a document index is performed. Here, we propose and investigate the more challenging scenario where, given only a query, the input documents must be retrieved from a large index containing mostly irrelevant documents.

\paragraph{Previous attempts at open-domain MDS} \citet{Liu2018GeneratingWB} proposed the WikiSum dataset. Given the title of a Wikipedia article and a collection of non-Wikipedia reference documents, the goal is to generate the first section of the article. The proposed extractive-abstractive approach resembles open-domain MDS. However, the document index is small (10s to 100s of documents) and composed of relevant documents (references of the article plus ten pages of search results using the title as query). We study the more challenging and more general setting where the index is large (>>10,000 documents, see \autoref{tab:dataset-statistics}) and contains many more irrelevant documents than relevant documents.

In \citet{DBLP:journals/corr/abs-2112-07536}, a method similar to retrieve-then-summarize is proposed, using a pretrained dense passage retriever \citep{karpukhin-etal-2020-dense} and T5 \citep{JMLR:v21:20-074} as summarizer.\footnote{At the time of writing, this work is unpublished} The model is trained \& evaluated on a dataset constructed from existing QMDS datasets. This dataset is small (\(\sim\)90 training examples) and does not appear to be publicly available. Here, we conduct a large-scale analysis on multiple datasets from different domains (each consisting of thousands of examples) and evaluate several of the top-performing multi-document summarizers currently available (\textsection \ref{bootstraping}). We also extensively simulate document retrieval errors to probe their relative impact on summarization (\textsection \ref{simulations}). Together, this allows us to draw broader conclusions about open-domain MDS and provide detailed practical advice for future work.

\paragraph{Open-domain QA} Our open-domain MDS proposal mirrors a similar trend in question answering (QA). While earlier research focused on answering questions provided a text passage \citep{rajpurkar-etal-2016-squad, rajpurkar-etal-2018-know}, the now predominant approach, open-domain QA (ODQA), is to answer questions \textit{without} this passage, usually by referencing an external knowledge source (e.g. Wikipedia). Even broader are knowledge-intensive (KI) language tasks \citep{petroni-etal-2021-kilt}, which include ODQA but also, for example, fact-checking. KI tasks are commonly approached with a retrieve-then-\textit{generate} framework \citep{REALM, RAG, RETRO}. Retrieve-then-\textit{summarize} is similar, except that the outputs are, on average, much longer and tend to be less extractive than the outputs of KI language tasks like ODQA.

\vspace{-1.0mm}
\section{Bootstrapping Open-domain MDS}
\label{bootstraping}
\vspace{-1.0mm}

Since no large-scale annotated datasets\footnote{Existing query-focused MDS datasets (e.g. DUC 2005, 2006 \& 2007) are extremely small (10s of examples) and are therefore not suitable for the large-scale analysis we conducted} or trained models exist for open-domain MDS, we bootstrap this task using existing datasets (\textsection \ref{experimental-setup:datasets}) and models (\textsection \ref{experimental-setup:retrievers}, \textsection \ref{experimental-setup:summarizers}). We describe operationalization considerations in \textsection \ref{experimental-setup:operationalizing} and evaluation in \textsection \ref{experimental-setup:evaluation}.

\subsection{Datasets} \label{experimental-setup:datasets}

We investigate a representative selection of 5 MDS datasets comprised of news articles, medical studies, and scientific literature, deliberately choosing datasets for which high-performing summarizers exist (see \autoref{appendix:dataset-details} for more details). The inputs of these datasets generally consist only of the documents to summarize. However, Multi-XScience and \mstoo each provide additional text as input: the target article's abstract and the target review's background section. In our experiments, we always provide this additional text and do not retrieve it.

\subsection{Retrieval Models} \label{experimental-setup:retrievers}

Broadly speaking, retrievers are divided into two categories, \textit{sparse} \& \textit{dense}. Sparse retrievers determine relevance of a document to a query using weighted counts of overlapping terms. Dense retrievers embed documents \& queries into a shared vector space and use proximity to determine relevance. Retrievers from these families exhibit different characteristics and limitations~\citep{macavaney-etal-2022-abnirml}; therefore, we investigate a representative retriever from each: BM25 (sparse, \citealp{BM25}) and Contriever (dense, \citealp{contriever}). Both achieve strong zero-shot performance,\footnote{See the BEIR~\citep{BEIR} \href{https://docs.google.com/spreadsheets/d/1L8aACyPaXrL8iEelJLGqlMqXKPX2oSP_R10pZoy77Ns/edit?usp=sharing}{zero-shot benchmark}} making them particularly suitable for our purposes. See \autoref{appendix:retrieval} for details.


\subsection{Multi-document Summarization Models} \label{experimental-setup:summarizers}


All MDS models we experiment with are transformer-based encoder-decoders~\citep{aiayn} trained for abstractive summarization, representing the state-of-the-art approach. The input contains one or more documents concatenated with special tokens (e.g. \texttt{<doc-sep>}). As is typical, we truncate each document based on the maximum input length of the model divided by the total number of documents. See \autoref{appendix:model-details} for details.

\subsection{Operationalize Retrieve-then-Summarize} \label{experimental-setup:operationalizing}

To extend these datasets and models to the open-domain setting and operationalize the retrieve-then-summarize approach, we address the following:

\paragraph{How to choose a query?} In open-domain MDS, a query is anything that defines the documents to summarize, e.g. a question or topic statement. Ideally, a human-written query would be available for each example in our dataset; however, existing MDS datasets do not provide these queries. Therefore, we use \(R\),\footnote{Except for \mstoo, where we found the provided \say{background} section to perform better as a query; see \autoref{appendix:dataset-details}} the human-written reference summaries, as \textit{pseudo}-queries, as they naturally describe the input documents of each example.\footnote{We also experimented with query generation using LLMs (e.g. GPT-3), but found that they significantly under-performed the reference summary as query, e.g. by at least 8 points P/R@K on a sample of the Multi-News validation set}

\paragraph{How to assemble the document index?} For our purposes, we take the set of all documents in the train, validation, and test splits of each dataset to form \(\mathcal{D_\text{index}}\). This guarantees that the ground-truth documents for each example are present in the index while providing plenty of negative examples.

\begin{table*}[t]
\centering
\caption{Results of the open-domain MDS experiments. We observe the following: (1) retrieval performance ranges from high (Multi-News, WCEP-10, \textcolor{MyBlue}{\textbf{dark blue}}) to low (Multi-XScience, \mstoo, Cochrane), (2) when summarizers trained on these datasets are provided retrieved documents, they suffer from significant drops in performance (\textcolor{MyRed}{\textbf{dark red}}); more severe performance drops were observed in cases where baseline summarization performance was relatively high (\textcolor{MyGreen}{\textbf{dark green}}). Experiments here used a sparse retriever (BM25) and \textit{max} top-\textit{k} strategy (see Table~\ref{tab:retrieval-sparse-complete} for complete results with all top-\textit{k} strategies). Similar results were observed using a dense retriever (Contriever, see Table~\ref{tab:retrieval-dense}). Statistically significant results are \underline{underlined} (paired t-test, p = 0.01).}
\label{tab:retrieval-sparse}
\small
\resizebox{\textwidth}{!}{\begin{tabular}{@{}llccccccc@{}}
\toprule
 &
   &
  \multicolumn{2}{c}{Retrieval} &
   &
  \multicolumn{4}{c}{Summarization} \\ \cmidrule(lr){3-4} \cmidrule(l){6-9} 
Dataset &
  Model &
  P@K &
  R@K &
   &
  ROUGE-Avg F1 &
  \(\Delta\) ROUGE-Avg F1 &
  BERTScore F1 &
  \(\Delta\) BERTScore F1 \\ \midrule
Multi-News &
  PRIMERA &
  \gradientretrieval{0.22} &
  \gradientretrieval{0.82} &
   &
  \gradientbaseline{31.66} &
  \gradientdiff[1]{-7.39} &
  \gradientbaseline{31.78} &
  \gradientdiff[1]{-10.33} \\
 &
  PEGASUS &
  -- &
  -- &
   &
  \gradientbaseline{31.23} &
  \gradientdiff[1]{-8.49} &
  \gradientbaseline{29.88} &
  \gradientdiff[1]{-10.87} \\
 &
  LSG-BART-base &
  -- &
  -- &
  \multicolumn{1}{c}{} &
  \gradientbaseline{30.05} &
  \gradientdiff[1]{-6.44} &
  \gradientbaseline{26.57} &
  \gradientdiff[1]{-8.17} \\
 &
  GPT-3.5-turbo &
  -- &
  -- &
   \multicolumn{1}{c}{} &
  \gradientbaseline{23.86} &
  \gradientdiff[1]{-2.46} &
  \gradientbaseline{21.68} &
  \gradientdiff[1]{-3.92} \\
WCEP-10 &
  PRIMERA &
  \gradientretrieval{0.63} &
  \gradientretrieval{0.67} &
   &
  \gradientbaseline{35.50} &
  \gradientdiff[1]{-1.02} &
  \gradientbaseline{48.26} &
  \gradientdiff{-0.76} \\
 &
  LSG-BART-base &
  -- &
  -- &
   &
  \gradientbaseline{35.76} &
  \gradientdiff[1]{-1.15} &
  \gradientbaseline{48.17} &
  \gradientdiff{-0.85} \\
 &
  GPT-3.5-turbo &
  -- &
  -- &
   &
  \gradientbaseline{26.36} &
  \gradientdiff{-0.22} &
  \gradientbaseline{32.72} &
  \gradientdiff{-0.25} \\
Multi-XScience &
  PRIMERA &
  \gradientretrieval{0.06} &
  \gradientretrieval{0.40} &
   &
  \gradientbaseline{18.31} &
  \gradientdiff[1]{-0.57} &
  \gradientbaseline{10.57} &
  \gradientdiff[1]{-1.82} \\
\mstoo &
  LED-base &
  \gradientretrieval{0.16} &
  \gradientretrieval{0.22} &
   &
  \gradientbaseline{19.66} &
  \gradientdiff{-0.14} &
  \gradientbaseline{22.74} &
  \gradientdiff{-0.47} \\
Cochrane &
  LED-base &
  \gradientretrieval{0.17} &
  \gradientretrieval{0.57} &
   &
  \gradientbaseline{17.39} &
  \gradientdiff{-0.28} &
  \gradientbaseline{23.12} &
  \gradientdiff[1]{-2.11} \\
  \bottomrule
\end{tabular}}
\vspace{-3.5mm}
\end{table*}

\paragraph{How many documents to summarize?} The number of retrieved documents to summarize, \(k\), is a parameter that can be tuned for different use cases. To determine its impact on summarization performance, we investigate three strategies:

\begin{itemize}[itemsep=0.2pt, topsep=3pt, leftmargin=10pt]
    \item \textbf{Max}: Choose \(k\) as the \textit{maximum} number of input documents for any example in a given dataset. Tends to select for \textit{recall} at the cost of \textit{precision}.
    \item \textbf{Mean}: Choose \(k\) as the \textit{mean} number of input documents for all examples in a given dataset. Tends to select for \textit{precision} at the cost of \textit{recall}.
    \item \textbf{Oracle}: Choose \(k\) as the number of \textit{ground-truth} input documents for each example. This mimics the scenario where all documents with a relevance score (assigned by the retriever) above a certain optimal threshold are retained.
\end{itemize}

\noindent We note that this is a highly idealized setting. Using \(R\) as query leaks information about the reference summary into the retrieval step, likely inflating retrieval and summarization performance. In practice, \(\mathcal{D}_{\text{index}}\) will be much larger (e.g. PubMed-, Wikipedia-, or even Web-scale), making retrieval more difficult. Our intention is to determine a \textit{lower-bound} for the expected performance degradation of state-of-the-art summarizers in the open-domain setting; as we will show in \textsection \ref{experimental-results}, even this idealized setting often leads to large reductions in performance. In \textsection \ref{simulations}, we extensively \textit{simulate} document retrieval errors to determine how summarizers behave in both low- and high-performing retrieval settings across a variety of retrieval error types.

\subsection{Evaluation} \label{experimental-setup:evaluation}


We follow previous work by evaluating summarization with ROUGE-1/2/L scores \citep{lin-2004-rouge}. To provide a single metric for comparison, we report ROUGE-Avg F1, the average F1-score of ROUGE-1/2/L. We also report BERTScore \citep{bertscore}, which has been shown to better correlate with human judgment \citep{bartscore, Fischer2022MeasuringFO}. For document retrieval, we report the precision and recall at \(k\) (P@K and R@K); which are suitable metrics when the input documents do not have an inherent order, as is usually the case in MDS. We evaluate on the test splits of each dataset, except for \mstoo and Cochrane, where we evaluate on the validation set because the test split is blind.

\begin{table*}[t]
\centering
\caption{Results of the open-domain MDS experiments with zero-shot summarizers. Controls for differences in datasets and models, isolating the relationship between summarization performance in the traditional and open-domain settings. Top-\(k\) strategy \textit{mean} is used. Statistically significant results are \underline{underlined} (paired t-test, p = 0.01).}
\label{tab:retrieval-zs}
\small
\resizebox{\textwidth}{!}{\begin{tabular}{@{}llccclcccc@{}}
\toprule
\textbf{} &
   &
  \multicolumn{3}{c}{Retrieval} &
   &
  \multicolumn{4}{c}{Summarization} \\ \cmidrule(lr){3-5} \cmidrule(l){7-10} 
Dataset &
  Model &
  Retriever &
  P@K &
  R@K &
   &
  ROUGE-Avg F1 &
  \(\Delta\) ROUGE-Avg F1 &
  BERTScore F1 &
  \(\Delta\) BERTScore F1 \\ \midrule
Multi-News &
  PRIMERA &
  sparse (BM25) &
  \gradientretrieval{0.64} &
  \gradientretrieval{0.74} &
  \multicolumn{1}{c}{} &
  \gradientbaseline{31.66} &
  \gradientdiff[1]{-2.82} &
  \gradientbaseline{31.78} &
  \gradientdiff[1]{-4.08} \\
 &
   &
  dense (Contriever) &
  \gradientretrieval{0.59} &
  \gradientretrieval{0.70} &
   &
  -- &
  \gradientdiff[1]{-3.31} &
  -- &
  \gradientdiff[1]{-4.60} \\
 &
  \(\hookrightarrow\) \textit{zero-shot} &
  sparse &
  -- &
  -- &
  \multicolumn{1}{c}{} &
  \gradientbaseline{23.58} &
  \gradientdiff{-0.09} &
  \gradientbaseline{18.66} &
  \gradientdiff[1]{-0.39} \\
 &
   &
  dense &
  -- &
  -- &
   &
  -- &
  \gradientdiff{-0.27} &
  -- &
  \gradientdiff[1]{-0.44} \\
WCEP-10 &
  PRIMERA &
  sparse &
  \gradientretrieval{0.66} &
  \gradientretrieval{0.64} &
  \multicolumn{1}{c}{} &
  \gradientbaseline{35.50} &
  \gradientdiff{-0.90} &
  \gradientbaseline{48.26} &
  \gradientdiff{-0.68} \\
 &
   &
  dense &
  \gradientretrieval{0.66} &
  \gradientretrieval{0.63} &
   &
  -- &
  \gradientdiff{-0.14} &
  -- &
  \gradientdiff{+0.68} \\
 &
  \(\hookrightarrow\) \textit{zero-shot} &
  sparse &
  -- &
  -- &
   &
  \gradientbaseline{21.43} &
  \gradientdiff{+0.35} &
  \gradientbaseline{25.48} &
  \gradientdiff{+0.72} \\
 &
   &
  dense &
  -- &
  -- &
   &
  -- &
  \gradientdiff[1]{+1.00} &
  -- &
  \gradientdiff[1]{+2.19} \\
Multi-XScience &
  PRIMERA &
  sparse &
  \gradientretrieval{0.16} &
  \gradientretrieval{0.27} &
  \multicolumn{1}{c}{} &
  \gradientbaseline{18.31} &
  \gradientdiff[1]{-0.25} &
  \gradientbaseline{10.57} &
  \gradientdiff[1]{-1.27} \\
 &
   &
  dense &
  \gradientretrieval{0.16} &
  \gradientretrieval{0.24} &
   &
  -- &
  \gradientdiff[1]{-0.81} &
  -- &
  \gradientdiff[1]{-0.96} \\
 &
  \(\hookrightarrow\) \textit{zero-shot} &
  sparse &
  -- &
  -- &
   &
  \gradientbaseline{15.18} &
  \gradientdiff[1]{+0.69} &
  \gradientbaseline{6.02} &
  \gradientdiff[1]{-0.47} \\
 &
   &
  dense &
  -- &
  -- &
   &
  -- &
  \gradientdiff[1]{+0.46} &
  -- &
  \gradientdiff{+0.00} \\ \bottomrule
\end{tabular}}
\end{table*}

\begin{table*}[t]
\centering
\caption{Comparing ROUGE-Avg F1 scores of model-generated summaries to heuristic baselines. In some cases, heuristics perform surprisingly close to trained summarizers. All Lead is the concatenation of the first sentence from each input document. Oracle document is the document with the highest token overlap with the reference summary; oracle lead is the first sentence from this document. Background/Abstract is the additional input from \mstoo and Multi-XScience. The best baseline for each dataset is \textbf{bolded}.}
\label{tab:baselines}
\resizebox{\textwidth}{!}{\begin{tabular}{@{}lcccccc@{}}
\toprule
               & \multicolumn{1}{l}{}    & \multicolumn{5}{c}{Baselines}                                 \\ \cmidrule(l){3-7} 
Dataset & Best Summarizer & \(\Delta\) Random Summary & \(\Delta\) All Lead & \(\Delta\) Oracle Document & \(\Delta\) Oracle Lead & \(\Delta\) Background/Abstract \\ \midrule
Multi-News     & \gradientbaseline{31.7} & -18.3 & -15.3 & \textbf{-4.1} & -21.8         & --            \\
WCEP-10        & \gradientbaseline{35.8} & -27.8 & -24.4 & -15.3         & \textbf{-9.9} & --            \\
Multi-XScience & \gradientbaseline{18.3} & -6.2  & -5.0  & \textbf{-0.8} & -9.3          & -2.3          \\
\mstoo         & \gradientbaseline{19.7} & -10.4 & -11.0 & -7.6          & -4.0          & \textbf{-0.2} \\
Cochrane       & \gradientbaseline{17.4} & -5.0  & -4.2  & -3.9          & \textbf{-3.5} & --            \\ \bottomrule
\end{tabular}}
\vspace{-3.5mm}
\end{table*}

\vspace{-1.0mm}
\section{Evaluating Open-domain MDS}  \label{experimental-results}
\vspace{-1.0mm}


Here we present the results of our open-domain MDS experiments. In general, we find existing summarizers suffer large reductions in performance when applied to open-domain MDS, even when retrieval performance is high (Table~\ref{tab:retrieval-sparse}).\footnote{Results for sparse and dense retrievers were comparable and exhibited similar trends. We elect to show results for the sparse retriever; see \autoref{appendix:experimental-results} for dense retriever results} Below, we provide key observations on how the individual components (retriever and summarizer) behave within a retrieve-then-summarize framework.

\paragraph{Strong summarizers more sensitive to imperfect retrieval than weak ones}

We observe a relationship between a summarizer's (baseline) performance on a dataset and its sensitivity to imperfect document retrieval (\autoref{tab:retrieval-sparse}). The largest reductions in summarization performance were observed for the most performant summarizers, despite retrieval performance being the highest in these cases. However, this relationship is confounded by differences between datasets. To control for this, we conduct experiments comparing fine-tuned PRIMERA to PRIMERA evaluated \textit{zero-shot} (\autoref{tab:retrieval-zs}).\footnote{The only model we evaluate with zero-shot capabilities} This allows us to hold the dataset, model architecture, and retriever constant, isolating the relationship between summarization performance in the traditional and open-domain settings. Here, the trend is clear: ``strong'' summarizers are more sensitive to imperfect retrieval than ``weak'' summarizers.\footnote{We use ``strong'' and ``weak'' as shorthand to refer to cases where summarization performance is high (e.g. PRIMERA on Multi-News) and low (e.g. LED on Cochrane)}


One explanation is that weak summarizers have less to lose from imperfect retrieval, perhaps because they are not performing adequately even when trained and evaluated on ground-truth inputs. They may, to a greater degree than strong summarizers: hallucinate, exploit heuristics, or use only a fraction of the input documents \citep{kryscinski-etal-2019-neural, https://doi.org/10.48550/arxiv.2210.12688}. To probe this, we construct several baselines that mimic these behaviours. We find that, for example, copying the background section of \mstoo performs comparably to a trained model, suggesting that the observed insensitivity to retrieval errors could be due to summarizers exploiting this heuristic (\autoref{tab:baselines}, see Appendix~\ref{appendix:summarization-baselines} for details). We observe a similar result for Multi-XScience by copying the document with the highest token overlap to the reference summary. Future work should carefully establish that summarizers are performing adequately before attempting the more difficult open-domain setting.

\paragraph{Better retrieval performance \(\neq\) better summarization performance}

Performance of the sparse and dense retrievers was generally comparable (\autoref{tab:retrieval-performance}), with the sparse retriever performing better on some datasets (Multi-News, WCEP-10, Multi-XScience, see \autoref{tab:retrieval-sparse-complete}) and the dense retriever performing better on others (\mstoo, Cochrane, see \autoref{tab:retrieval-dense}). Interestingly, however, better retrieval performance did not always correspond with smaller reductions in summarization performance. E.g. on WCEP-10, the sparse retriever performed slightly better, but the reduction in summarization performance was considerably larger. On \mstoo and Cochrane, the better-performing dense retriever led to a larger reduction in summarization performance. This suggests that the two types of retrievers are making characteristically different errors\footnote{Previously noted by \citet{macavaney-etal-2022-abnirml}} that P/R@K do not completely capture. Future work should consider summarization performance itself, \textit{alongside} IR metrics like P/R@K, when tuning retrieval pipelines for open-domain MDS.

\paragraph{The number of documents to retrieve matters} \label{choosing-k}

We observe clear differences in the strategy for choosing \(k\), the number of retrieved documents to summarize. Unsurprisingly, the oracle strategy almost always leads to the smallest reduction in summarization performance. This strategy closely mimics the setting of retaining all documents with a relevance score (assigned by the retriever) over a certain threshold but assumes a strong retriever and a well-calibrated threshold, both of which may be difficult to achieve in practice. Our results suggest that setting \(k\) as the \textit{mean} number of relevant documents (if an accurate estimate can be made) is a second-best strategy. We note that, relative to max \(k\), mean \(k\) tends to select for precision over recall (see P@K vs. R@K scores in \autoref{tab:retrieval-sparse} \& \autoref{tab:retrieval-dense}); future work should consider tuning \(k\) for precision.

\paragraph{Human evaluation confirms degradation of summarization performance}

Automatic evaluation metrics like ROUGE are imperfect and may not correlate with aspects of human judgment.\footnote{See \textsection \ref{limitations} for an extended discussion} Therefore, we conducted a human evaluation to validate our results. In short, human annotators have a statistically significant preference for summaries produced by the ``baseline'' model (no retrieval) along two facets, \textit{coverage} and \textit{informativeness} (\autoref{tab:human-eval}), corroborating the degradation of summarization performance in the open-domain setting as quantified by the automatic metrics. See \autoref{appendix:human-eval} for full details, including a manual analysis of example summaries produced in the open-domain setting.

\begin{table}[t]
\centering
\caption{Human evaluation on Multi-News. A binomial test on three human annotators for \(n=50\) random test examples was conducted for each facet (excluding ties). All results \underline{statistically significant} (p~\(<\)~0.01). Inter-annotator agreement reported as Fleiss' Kappa (\(\kappa\)).}
\label{tab:human-eval}
\resizebox{\columnwidth}{!}{\begin{tabular}{@{}lcccc@{}}
\toprule
Facet &
  \begin{tabular}[c]{@{}c@{}}baseline\\ preferred\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}open-domain\\ preferred\end{tabular} &
  p &
  \(\kappa\) \\ \midrule
Coverage &
  60 &
  23 &
  \underline{5.97e-05} &
  0.32 \\
Informativeness &
  69 &
  27 &
  \underline{2.15e-05} &
  0.47 \\ \bottomrule
\end{tabular}}
\vspace{-3.5mm}
\end{table}

\begin{figure}[t]
\includegraphics[width=\columnwidth]{figures/prompt}
\caption{Prompt template for our in-context learning (ICL) based approach. Each prompt includes natural language instructions, example reference summaries as in-context examples, and unseen source documents as input. \texttt{max\_words} is set per dataset, 384 for Multi-News and 32 for WCEP-10. \texttt{examples} (2 for Multi-News and 5 for WCEP-10) are randomly selected from the train set, and the same examples are used for every input.}
\label{fig:prompt}
\vspace{-3.5mm}
\end{figure}

\paragraph{In-Context Learning with LLMs}

In-context learning (ICL) with large language models (LLMs) has emerged as a viable approach to zero-shot summarization \cite{Goyal2022NewsSA}. We conducted preliminary experiments using this approach to determine how its behaviour in the open-domain setting compares to the fine-tuned models that were the focus of our evaluation. We chose GPT-3.5-turbo as the LLM and designed a suitable prompt (tuned based on validation set performance) which contains some natural language instructions and a few example summaries (\autoref{fig:prompt}). We omit experiments on the \mstoo and Cochrane datasets, whose source documents, even when truncated to the first 25, exceed GPT-3.5's maximum input token length of 4096 (see \autoref{tab:dataset-statistics}). We also omit experiments on Multi-XScience, as a snapshot of arXiv is presumably included in the model's training set; therefore we cannot control for possible train-test contamination as the reference summaries are the related work sections of arXiv papers. To fit within the maximum token limit of the model, we use only example summaries as the in-context examples (omitting the source documents) and randomly choose 2 example summaries from the train-set for Multi-News and 5 for WCEP-10. To make results as reproducible as possible, we set the \texttt{temperature=0} and used the 03/01/2023 GPT-3.5-turbo snapshot. All other hyperparameters of the OpenAI API are left at their defaults.\footnote{\url{https://platform.openai.com/docs/api-reference/completions}} Lastly, due to associated costs in using the model, we restrict our experiments to the sparse retriever.  Results are presented in \autoref{tab:retrieval-sparse}. The ICL-based approach significantly underperforms the fine-tuned models (by at least \(\sim\)8 points) but outperforms PRIMERA \textit{zero-shot}. The general trends (and relative magnitude) of the degradation in performance in the open domain are comparable. We note that previous work has found that human annotators often prefer GPT-generated summaries over those generated by smaller, fine-tuned models, even when automatic metrics like ROUGE disagree. However, this observation is restricted to the single-document setting \cite{Goyal2022NewsSA}. Future work is needed to determine if LLMs exhibit different behaviour under the open-domain MDS setting than their smaller, fine-tuned pre-trained-language model (PLM) counterparts.

\begin{figure}[t]
\includegraphics[width=\columnwidth]{figures/training}
\caption{Fine-tuning summarizers in the open-domain. Additional training on \textit{retrieved} documents can reduce sensitivity to imperfect retrieval; often at the cost of performance on \textit{ground-truth} documents. Dashed \textcolor{MyGrey}{\textbf{grey}} line represents no change in performance.
}
\label{fig:training}
\vspace{-3.5mm}
\end{figure}

\begin{figure}[t]
\includegraphics[width=0.95\columnwidth]{figures/perturbations}
\caption{Graphical depiction of perturbations. Dashed line indicates input documents. Uncoloured documents are ground-truth, \textcolor{MyGreen}{\textbf{green}} have been added, \textcolor{MyRed}{\textbf{red}} removed and \textcolor{MyBlue}{\textbf{blue}} modified. Unique documents are lettered.}
\label{fig:perturbations}
\vspace{-3.5mm}
\end{figure}

\vspace{-1.0mm}
\section{Training in the Open-domain Setting} \label{training}
\vspace{-1.0mm}

A natural question is whether a summarizer's robustness to document retrieval errors at \textit{test} time can be improved by exposing the model to similar errors at \textit{train} time. To explore this, we retrieve the documents for all examples in the train set of each dataset and fine-tune summarizers on these examples. We then evaluate them on both the retrieved document set and the original (ground-truth) document set (\autoref{fig:training}, see \autoref{appendix:training} for details). We find clear cases where summarization performance in the open-domain benefits from the additional training (e.g. Multi-News, Multi-XScience), but that this benefit can come at the cost of performance on the ground-truth documents (e.g. Multi-News, WCEP-10 and Cochrane), and is sometimes unstable (e.g. WCEP, Cochrane). We note again that our retrieval setting is highly idealized (\textsection \ref{experimental-setup:operationalizing}); nonetheless, our results suggest that existing summarizers could be adapted to the open-domain setting if query-annotated training examples and appropriate document indices are constructed.

\begin{figure*}[t]
\scriptsize{\qquad\quad\textcolor{darkgray}{\textsf{PRIMERA, Multi-News} \hspace{5cm} \textsf{LED-Base, Cochrane}}}
\centering
\centerline{
\includegraphics[width=\columnwidth]{figures/allenai_PRIMERA_multinews_shaded_rescaled}
\includegraphics[width=\columnwidth]{figures/allenai_led_base_16384_cochrane_shaded_rescaled}
}
\caption{Results of the perturbation experiments on Multi-News (left) and Cochrane (right). Mean change in summarization performance plotted against the percent of perturbed input documents. Values above -0.49 ROUGE are shaded in \textcolor{MyGreen}{\textbf{green}}, and values below in \textcolor{MyRed}{\textbf{red}}, the average difference in summarization performance reported in *CL conferences. Y-axis is displayed in symlog scale. 68\% confidence intervals (CI) are plotted as error bands.}
\label{fig:perturbation-results}
\vspace{-3.5mm}
\end{figure*}

\vspace{-1.0mm}
\section{Simulating Document Retrieval Errors} 
\label{simulations}
\vspace{-1.0mm}

In this section, we investigate what is driving the reduction in summarization performance in the open-domain setting (\textsection \ref{experimental-results}). We begin by carefully categorizing the various retrieval errors that can occur. E.g., we could erroneously retrieve documents \textit{irrelevant} to the query. For each error type, we design a corresponding \say{perturbation} that can be applied to the inputs of existing MDS datasets before they are fed to a summarizer. The perturbations are described below and depicted graphically in \autoref{fig:perturbations}:

\begin{itemize}[itemsep=0.2pt, topsep=3pt, leftmargin=10pt]
    \item \textbf{Addition}: \textit{Add one or more irrelevant documents}. This could occur if we retrieve all relevant documents but also retrieve irrelevant ones.
    \item \textbf{Deletion}: \textit{Remove one or more documents}. This could occur if we retrieve only a fraction of all relevant documents.
    \item \textbf{Replacement}: \textit{Replace one or more relevant documents with irrelevant ones}. This could occur if we retrieve the correct number of documents but substitute relevant ones for irrelevant ones.
    \item \textbf{Duplication}: \textit{Duplicate one or more documents}. This could occur if duplicate (or, more likely, near-duplicate) documents exist in the index.\footnote{Deduplication is non-trivial \citep{lee-etal-2022-deduplicating} and near-duplicates are not uncommon in large document collections like C4 \citep{dodge-etal-2021-documenting} or S2ORC \citep{lo-etal-2020-s2orc}}
    \item \textbf{Sorting}: \textit{Modify order of documents}. Input documents for MDS are typically unordered. However, many methods concatenate documents before providing them as input, and it is unknown if models are sensitive to this order. Different orderings could occur, e.g., if documents are sorted by order of relevance before concatenating.
\end{itemize}
\vspace{-1.0mm}

\paragraph{Token-level perturbation}

It is well known that NLP models are sensitive to minor \textit{token}-level changes in their inputs \citep{prabhakaran-etal-2019-perturbation, niu-etal-2020-evaluating, ribeiro-etal-2020-beyond, moradi-samwald-2021-evaluating}. To compare and contrast the \textit{document}-level sensitivity we investigate with this known sensitivity, we include a token-level perturbation, \textbf{backtranslation}, as control. Here, we translate one or more input documents to another high-resource language and back again. This causes small changes, e.g. paraphrasing and synonym substitution, allowing us to create semantics-preserving, token-level changes to a document.\footnote{See Appendix~\ref{appendix:backtranslation} for details}

\subsection{Selecting Documents to Perturb}
\label{experimental-setup:document-selection}

Each perturbation requires selecting one or more documents to perturb, e.g., in addition and deletion, we need to choose which documents to add and which to remove. We investigate two strategies:

\begin{itemize}[itemsep=0.2pt, topsep=3pt, leftmargin=10pt]
    \item \textbf{Random}: Select documents to perturb randomly, mimicking a (very) \textit{weak}
    retriever.
    \item{\textbf{Oracle}}: Select documents in a way that mimics a \textit{strong} retriever. E.g. in deletion, we remove ground-truth documents in order of \textit{least-to-most} similar to the reference summary \(R\),\footnote{Similar to \textsection \ref{experimental-results}, this leverages \(R\) as a pseudo-query} in addition, we add irrelevant documents in order of \textit{most-to-least} similar.\footnote{Determined using Sentence Transformers \citep{sbert}; specifically \href{https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2}{all-MiniLM-L6-v2}}
\end{itemize}


\noindent For perturbations that require selecting irrelevant documents (addition \& replacement), we select from the set of all documents in the train, validation, and test splits (excluding documents from the example we are perturbing). We evaluate summarizers under increasing amounts of perturbation: from 0\% of documents perturbed up to 100\%.

\subsection{Results of Simulation Experiments} \label{experimental-results:simulations}

In \autoref{fig:perturbation-results}, we display the results of our experiments simulating document retrieval errors for two model-dataset pairs.\footnote{These are exemplary of main trends observed across all model-dataset pairs; see \autoref{appendix:perturbations} for complete results} To better contextualize results, we shade differences in ROUGE~$\geq$~0.5 (average difference in summarization performance reported in *CL conferences, \citealp{deutsch-etal-2022-examining}) in \textcolor{MyRed}{\textbf{red}} and the rest in \textcolor{MyGreen}{\textbf{green}}. This serves as a rough yardstick to help identify large drops in performance. We symlog \citep{symlog} the y-axis to make small changes in performance more apparent. In general, results are congruent with our open-domain MDS experiments (\textsection \ref{experimental-results}): (1) large reductions in summarization performance are observed even in cases of few simulated errors (<~20\%) and (2) strong summarizers (\autoref{fig:perturbation-results}, left) are more sensitive to retrieval errors than weak summarizers (\autoref{fig:perturbation-results}, right). Below, we discuss major trends.

\paragraph{Summarizers insensitive to duplicates and small token-level changes}
A consistent trend across models \& datasets was an insensitivity to duplicate documents, even in the extreme case of >80\% duplication, suggesting that deduplication efforts on the document index are unlikely to translate to improvements in summarization performance. However, this assumes duplicate documents are included without replacing relevant documents, which is possible if \(k\) is based on a relevance threshold cutoff. Another trend is that models are not overly sensitive to minor \textit{token}-level changes (exemplified by backtranslation) relative to other perturbations, further motivating our focus on \textit{document}-level errors.

\paragraph{Erroneous additions vs. erroneous deletions}
In the \textit{random} setting, small amounts of deletion led to large drops in summarization performance. Conversely, deletion has surprisingly little impact in the \textit{oracle} setting until most documents (>60\%) are removed.
These results have two non-mutually exclusive explanations: (1) summarizers only consider a select few input documents, (2) reference summaries have low coverage of the input documents, both corroborated by recent work \citep{https://doi.org/10.48550/arxiv.2210.12688}. Based on the addition perturbation results in the oracle setting, summarizers appear to be more sensitive to erroneous \textit{additions} than \textit{deletions}, which, alongside our top-\(k\) strategy results in \textsection \ref{experimental-results}, suggests that retrieval pipelines should be tuned for \textit{precision} in open-domain MDS.
\vspace{-5.0mm}

\paragraph{Summarizers are insensitive to document order} \label{sorting} As far as we know, prior work has yet to investigate whether multi-document summarizers are sensitive to input document order. Although the documents are generally considered unordered, they are usually concatenated before providing them as input. To determine if summarizers are sensitive to this order, we sorted the input documents of each dataset \textit{before} concatenation and re-evaluated the summarizers. We investigate two sorting strategies:

\begin{itemize}[itemsep=0.2pt, topsep=3pt, leftmargin=10pt]
    \item \textbf{Random}: Shuffle documents randomly.
    \item \textbf{Oracle}: Sort documents by similarity to the reference summary, \(R\). This is motivated from two perspectives: (1) prior work has found that transformers are biased toward earlier tokens in their input \citep{Hofsttter2021MitigatingTP}, so we might expect improved performance by placing the most similar content to \(R\) first, (2) a strong retriever would assign a higher rank to the most relevant documents, and we might choose to input documents to our summarizer in this order. 
\end{itemize}

\noindent We find no significant difference (paired t-test, p = 0.01) in summarization performance for any model-dataset pair, \textit{except} for WCEP-10 (see Appendix~\ref{appendix:sorting}). Here we find that both models we evaluate (PRIMERA \& LSG-BART) are negatively affected by random sorting. One possible explanation is that, due to how WCEP-10 was constructed, the documents are (partially) sorted in order of relevance (see \autoref{appendix:dataset-details}). Models trained on this dataset may have learned to exploit this, e.g., by assigning more weight to earlier documents in the input. After randomly shuffling input documents, this learned heuristic would no longer hold, and summarization performance might drop accordingly.

\vspace{-1.0mm}
\section{Conclusion}
\vspace{-1.0mm}

We present a new, open-domain task definition for MDS. This reformulation is more challenging and potentially more useful, enabling users to specify their intent with only a query. Via extensive automatic and manual evaluation, we find that: (1) summarization performance significantly degrades in the open-domain setting, even when retrieval performance is high, (2) additional training can reduce this sensitivity to imperfect retrieval, and (3) summarizers are insensitive to the retrieval of duplicate documents and the order of retrieved documents but highly sensitive to other errors, like the retrieval of irrelevant documents. Based on our results, we provide practical guidelines, e.g. that retrieval pipelines for open-domain MDS should be tuned for \textit{precision}. Curating high-quality MDS datasets annotated with queries will be necessary to enable further progress in the open-domain setting.

\section*{Limitations} \label{limitations}

\paragraph{Automated evaluation metrics may not correlate with human judgment}
Though established metrics such as ROUGE and BERTScore are imperfect \citep{deutsch-etal-2022-examining}, they are frequently used in the summarization literature, do correlate with aspects of summary quality, and are useful for comparing system-level performance, especially in scenarios such as ours where performance differences can be several points below the baseline. To validate our findings, we also conduct a human evaluation to better understand qualitative differences in summaries generated in the open-domain setting (see \autoref{appendix:human-eval}). The investigation of better automated metrics for natural language generation is an active field of research, and we hope to integrate novel and performant metrics in future work.

\paragraph{Results conflate dataset features and model performance}
Our evaluation conflates several issues beyond the relative performance of retrievers and summarizers. Dataset quality, the ``multi-document-ness'' of each dataset, and the shortcomings of automatic metrics all contribute to noise in our results. For example, a dataset whose reference summaries have low coverage of the input documents (as characterized by \citealp{https://doi.org/10.48550/arxiv.2210.12688}) would not be expected to respond to retrieval errors in the same way as a dataset where this coverage is high. By experimenting with multiple datasets, retrievers, and summarizers, as well as in the synthetic perturbation setting (\textsection \ref{simulations}), we expect our results to be more resilient to these confounders.

\paragraph{Specialized retrievers may lead to better performance}
We experiment with standard sparse and dense retrievers in the zero-shot setting. More effort tuning retrieval pipelines, e.g. by introducing re-rankers \citep{Pradeep2021TheED} or by fine-tuning retrievers directly on MDS datasets, may improve retrieval performance and lead to smaller summarization performance reductions. Additionally, better summarization performance might be achieved by retrieving content at the \textit{span}-level, (as opposed to full documents). We leave the development of retrieval pipelines purpose-built and tuned for open-domain MDS to future work.

\section*{Acknowledgements}

This research was enabled in part by support provided by the Digital Research Alliance of Canada (\href{https://alliancecan.ca/}{alliancecan.ca}) and Compute Ontario (\href{https://www.computeontario.ca/}{www.computeontario.ca}).

\section*{Author Contributions}
\label{sec:contrib}

John Giorgi made most of the technical contributions, including dataset collection and processing, model implementation, and running experiments. John also contributed to project scoping and ideation, wrote the paper with feedback from everyone, and led the project in general. Luca, Kyle, Lucy, and Arman were project mentors, contributing equally to project scoping and experimental design and providing the core ideas and direction throughout the course of the project and paper writing. Additionally, Luca made technical contributions to model implementation. Bo and Gary provided high-level feedback and advice in later stages of the project.

\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix

\clearpage

\section{Dataset Details}
\label{appendix:dataset-details}

All datasets were managed in the HuggingFace Datasets library \citep{hf-datasets}. The examples of each dataset consist of an input document set, \(D\) and a human-written reference summary, \(S\). Multi-XScience and \mstoo each have an additional input that is always provided (and never retrieved or perturbed), the target articles abstract and the target reviews background section. See \autoref{tab:dataset-statistics} for dataset statistics. Below, we provide detailed descriptions of each dataset:

\begin{itemize}
    \item \textbf{Multi-News} \citep{fabbri-etal-2019-multi}: Consists of news articles and summaries collected from \url{www.newser.com}. There are 44,972 examples in the train set and 5622 examples in the test set. Each example contains between 1 and 10 documents, with a mean of \(\sim\)2.7.
    \item \textbf{WCEP-10}: Consists of news articles and summaries collected from the Wikipedia Current Events Portal (WCEP\footnote{\url{https://en.wikipedia.org/wiki/Portal:Current_events}}). WCEP-10\footnote{\label{wcep-10}\url{https://huggingface.co/datasets/ccdv/WCEP-10}} sub-samples the top 10 most relevant documents from the original WCEP dataset \citep{gholipour-ghalandari-etal-2020-large}. There are 8158 examples in the train set and 1022 examples in the test set. Each example contains between 1 and 10 documents, with a mean of \(\sim\)9.1.
    \item \textbf{Multi-XScience} \citep{lu-etal-2020-multi-xscience}: The target summary of each example is the related works section of a scientific article, and the input documents are the abstracts of the articles this section cites. Also included is the target article's abstract. There are 30,369 examples in the train set and 5093 examples in the test set. Each example contains between 1 and 20 documents, with a mean of \(\sim\)4.1.
    \item \textbf{\mstoo} \citep{deyoung-etal-2021-ms}: The target summary is a few sentences from a biomedical systemic review which summarize the main findings. The input documents are the included studies for that review. Also included is the target reviews background section. There are 14,188 examples in the train set and 2021 examples in the validation set. Each example contains between 1 and 401 documents, with a mean of \(\sim\)23.2.
    \item \textbf{Cochrane} \citep{cochrane}: Similar to \mstoo, except a background statement is not included as input. There are 3752 examples in the train set and 470 examples in the validation set. Each example contains between 1 and 537 documents, with a mean of \(\sim\)10.9.
\end{itemize}

\section{Retrieval Details}
\label{appendix:retrieval}

\begin{table}[t]
\centering
\caption{Evaluated multi-document summarizers and the datasets for which a fine-tuned model is publicly available (or was trained by us).}
\label{tab:models}
\resizebox{\columnwidth}{!}{\begin{tabular}{@{}lccc@{}}
\toprule
Model    & Fine-tuned on       & Max Input Len. & Zero-shot?         \\ \midrule
LED      & \mstoo, Cochrane       & 16384          & \color{red} \xmark \\
PEGASUS  & Multi-News          & 1024           & \color{red} \xmark \\
PRIMERA & \begin{tabular}[c]{@{}c@{}}Multi-News, WCEP-10,\\ Multi-XScience\end{tabular} & 4096 & \color{MyGreen} \cmark \\
LSG-BART & Multi-News, WCEP-10 & 4096           & \color{red} \xmark \\ \bottomrule
\end{tabular}}
\end{table}

\begin{table*}[t]
\small
\centering
\caption{Dataset statistics, counting whitespace tokens and punctuation. \text{*}Following \citep{deyoung-etal-2021-ms}, we take the first 25 documents as input (full statistics in parentheses). \textsuperscript{\textdagger}Multi-XScience and \mstoo each have inputs that are always provided (and never retrieved), the target articles abstract and the target reviews background section.}
\label{tab:dataset-statistics}
\begin{tabular}{@{}llccclcc@{}}
\toprule
                                              &                       & \multicolumn{3}{c}{Number of Documents} &  & \multicolumn{2}{c}{Average Number of Tokens} \\ \cmidrule(lr){3-5} \cmidrule(l){7-8} 
Dataset          & Domain          & Max      & Mean   & Total   &  & Per document & Per summary \\ \midrule
Multi-News       & News Articles   & 10       & 2.7    & 154,544 &  & 788          & 267         \\
WCEP-10          & News Articles   & 10       & 9.1    & 92,560  &  & 494          & 33          \\
Multi-XScience\textsuperscript{\textdagger}   & Scientific Literature & 20           & 4.1         & 165,546    &  & 153                   & 125                  \\
\mstoo\!\text{*}\textsuperscript{\textdagger} & Medical Studies       & 25 (401)     & 17 (23)     & 415,333    &  & 332                   & 58                   \\
Cochrane\text{*} & Medical Studies & 25 (537) & 9 (11) & 51,208  &  & 266          & 69          \\ \bottomrule
\end{tabular}
\end{table*}

\begin{table*}[!ht]
\small
\centering
\caption{Retrieval performance. The precision and recall at \(k\) for each retriever and top-\(k\) strategy is reported. The index for each dataset is the set of all documents in the train, validation and test sets; the reference summaries are used as queries, except for \mstoo, where we use the provided background section. The Cochrane test set is blind, so we do not have access to the reference summaries to use as queries and therefore do not evaluate on the test set.}
\label{tab:retrieval-performance}
\begin{tabular}{@{}llcccccccc@{}}
\toprule
 &
   &
   &
   &
  \multicolumn{2}{c}{Train} &
  \multicolumn{2}{c}{Validation} &
  \multicolumn{2}{c}{Test} \\ \cmidrule(l){5-10} 
Dataset &
  Retriever &
  Retriever Type &
  Top-\(k\) Strategy &
  P@K &
  R@K &
  P@K &
  R@K &
  P@K &
  R@K \\ \midrule
Multi-News &
  BM25 &
  sparse &
  max(10) &
  \gradientretrieval{0.22} &
  \gradientretrieval{0.83} &
  \gradientretrieval{0.22} &
  \gradientretrieval{0.82} &
  \gradientretrieval{0.22} &
  \gradientretrieval{0.82} \\
 &            &       & mean (3)  & \gradientretrieval{0.64} & \gradientretrieval{0.74} & \gradientretrieval{0.64} & \gradientretrieval{0.74} & \gradientretrieval{0.64} & \gradientretrieval{0.74} \\
 &            &       & oracle    & \gradientretrieval{0.75} & \gradientretrieval{0.75} & \gradientretrieval{0.75} & \gradientretrieval{0.75} & \gradientretrieval{0.75} & \gradientretrieval{0.75} \\
 & Contriever & dense & max       & \gradientretrieval{0.21} & \gradientretrieval{0.80} & \gradientretrieval{0.21} & \gradientretrieval{0.79} & \gradientretrieval{0.21} & \gradientretrieval{0.80} \\
 &            &       & mean      & \gradientretrieval{0.59} & \gradientretrieval{0.69} & \gradientretrieval{0.59} & \gradientretrieval{0.69} & \gradientretrieval{0.59} & \gradientretrieval{0.70} \\
 &            &       & oracle    & \gradientretrieval{0.69} & \gradientretrieval{0.69} & \gradientretrieval{0.69} & \gradientretrieval{0.69} & \gradientretrieval{0.69} & \gradientretrieval{0.69} \\
WCEP-10 &
  BM25 &
  sparse &
  max (10) &
  \gradientretrieval{0.59} &
  \gradientretrieval{0.66} &
  \gradientretrieval{0.60} &
  \gradientretrieval{0.63} &
  \gradientretrieval{0.63} &
  \gradientretrieval{0.67} \\
 &            &       & mean (9)  & \gradientretrieval{0.62} & \gradientretrieval{0.62} & \gradientretrieval{0.63} & \gradientretrieval{0.60} & \gradientretrieval{0.66} & \gradientretrieval{0.64} \\
 &            &       & oracle    & \gradientretrieval{0.64} & \gradientretrieval{0.64} & \gradientretrieval{0.63} & \gradientretrieval{0.63} & \gradientretrieval{0.67} & \gradientretrieval{0.67} \\
 & Contriever & dense & max       & \gradientretrieval{0.60} & \gradientretrieval{0.66} & \gradientretrieval{0.60} & \gradientretrieval{0.64} & \gradientretrieval{0.63} & \gradientretrieval{0.67} \\
 &            &       & mean      & \gradientretrieval{0.62} & \gradientretrieval{0.63} & \gradientretrieval{0.63} & \gradientretrieval{0.60} & \gradientretrieval{0.66} & \gradientretrieval{0.63} \\
 &            &       & oracle    & \gradientretrieval{0.65} & \gradientretrieval{0.65} & \gradientretrieval{0.63} & \gradientretrieval{0.63} & \gradientretrieval{0.66} & \gradientretrieval{0.66} \\
Multi-XScience &
  BM25 &
  sparse &
  max (20) &
  \gradientretrieval{0.05} &
  \gradientretrieval{0.41} &
  \gradientretrieval{0.06} &
  \gradientretrieval{0.40} &
  \gradientretrieval{0.06} &
  \gradientretrieval{0.40} \\
 &            &       & mean (4)  & \gradientretrieval{0.16} & \gradientretrieval{0.27} & \gradientretrieval{0.16} & \gradientretrieval{0.26} & \gradientretrieval{0.16} & \gradientretrieval{0.27} \\
 &            &       & oracle    & \gradientretrieval{0.22} & \gradientretrieval{0.22} & \gradientretrieval{0.22} & \gradientretrieval{0.22} & \gradientretrieval{0.23} & \gradientretrieval{0.23} \\
 & Contriever & dense & max       & \gradientretrieval{0.06} & \gradientretrieval{0.38} & \gradientretrieval{0.06} & \gradientretrieval{0.38} & \gradientretrieval{0.06} & \gradientretrieval{0.38} \\
 &            &       & mean      & \gradientretrieval{0.16} & \gradientretrieval{0.24} & \gradientretrieval{0.16} & \gradientretrieval{0.24} & \gradientretrieval{0.16} & \gradientretrieval{0.24} \\
 &            &       & oracle    & \gradientretrieval{0.20} & \gradientretrieval{0.20} & \gradientretrieval{0.20} & \gradientretrieval{0.20} & \gradientretrieval{0.21} & \gradientretrieval{0.21} \\
\mstoo &
  BM25 &
  sparse &
  max (25) &
  \gradientretrieval{0.17} &
  \gradientretrieval{0.26} &
  \gradientretrieval{0.16} &
  \gradientretrieval{0.22} &
  \gradientretrieval{0.17} &
  \gradientretrieval{0.22} \\
 &            &       & mean (17) & \gradientretrieval{0.21} & \gradientretrieval{0.22} & \gradientretrieval{0.18} & \gradientretrieval{0.18} & \gradientretrieval{0.20} & \gradientretrieval{0.18} \\
 &            &       & oracle    & \gradientretrieval{0.22} & \gradientretrieval{0.22} & \gradientretrieval{0.18} & \gradientretrieval{0.18} & \gradientretrieval{0.19} & \gradientretrieval{0.19} \\
 & Contriever & dense & max       & \gradientretrieval{0.19} & \gradientretrieval{0.29} & \gradientretrieval{0.18} & \gradientretrieval{0.25} & \gradientretrieval{0.19} & \gradientretrieval{0.26} \\
 &            &       & mean      & \gradientretrieval{0.23} & \gradientretrieval{0.24} & \gradientretrieval{0.21} & \gradientretrieval{0.21} & \gradientretrieval{0.23} & \gradientretrieval{0.21} \\
 &            &       & oracle    & \gradientretrieval{0.24} & \gradientretrieval{0.24} & \gradientretrieval{0.21} & \gradientretrieval{0.21} & \gradientretrieval{0.22} & \gradientretrieval{0.22} \\
Cochrane &
  BM25 &
  sparse &
  max (25) &
  \gradientretrieval{0.17} &
  \gradientretrieval{0.55} &
  \gradientretrieval{0.17} &
  \gradientretrieval{0.57} &
  -- &
  -- \\
 &            &       & mean (9)  & \gradientretrieval{0.30} & \gradientretrieval{0.42} & \gradientretrieval{0.31} & \gradientretrieval{0.44} & --   & --   \\
 &            &       & oracle    & \gradientretrieval{0.38} & \gradientretrieval{0.38} & \gradientretrieval{0.40} & \gradientretrieval{0.40} & --   & --   \\
 & Contriever & dense & max       & \gradientretrieval{0.20} & \gradientretrieval{0.63} & \gradientretrieval{0.20} & \gradientretrieval{0.64} & --   & --   \\
 &            &       & mean      & \gradientretrieval{0.34} & \gradientretrieval{0.48} & \gradientretrieval{0.35} & \gradientretrieval{0.49} & --   & --   \\
 &            &       & oracle    & \gradientretrieval{0.45} & \gradientretrieval{0.45} & \gradientretrieval{0.44} & \gradientretrieval{0.44} & --   & --   \\ \bottomrule
\end{tabular}
\end{table*}

Document retrieval and evaluation are conducted in the PyTerrier library \citep{pyterrier}. In \autoref{tab:retrieval-performance}, we present the retrieval performance on the train, validation and test split for each dataset, retriever, and top-\(k\) strategy. Below, we provided detailed descriptions of all retrievers:

\begin{itemize}
    \item \textbf{BM25} \citep{BM25}: Like other sparse retrievers, BM25 represents queries and documents as sparse vectors, where each element of a vector corresponds to a term in the vocabulary. BM25 is a widely used weighting scheme that extends TF-IDF \citep{tf-idf} to account for document length and term-frequency saturation. We use BM25 via PyTerrier with the default settings.
    \item \textbf{Contriever} \citep{contriever}: Contriever is an unsupervised dense retriever that uses a bi-encoder architecture. Documents and queries are encoded independently using the same BERT model \citep{bert}, and the final embedding is obtained by mean-pooling over the hidden representations of the model's last layer. The relevance score between a query and a document is the dot product of their embeddings. Specifically, we use contriever-msmarco,\footnote{\url{https://huggingface.co/facebook/contriever-msmarco}} which has been fine-tuned on the MS MARCO dataset \citep{MSMARCO}. We use Contriever via the PyTerrier Sentence Transformers plugin \citep{Soldaini_PyTerrier_Sentence_Transformers_2022} with the default settings.
\end{itemize}

\section{Model Details}
\label{appendix:model-details}

\begin{table*}[t]
\centering
\caption{Reported versus reproduced ROUGE-1/2/L scores for each model-dataset pair evaluated in the main paper. We also report zero-shot performance on select datasets for PRIMERA. \text{*}Fine-tuned by us.}
\label{tab:reproduction}
\resizebox{\textwidth}{!}{\begin{tabular}{@{}lcccclcccc@{}}
\toprule
                                       & \multicolumn{4}{c}{Reported}                         &  & \multicolumn{4}{c}{Reproduced}                       \\ \cmidrule(lr){2-5} \cmidrule(l){7-10} 
\diagbox{Dataset}{Model} & PRIMERA        & PEGASUS        & LED-base & LSG-BART-base  &  & PRIMERA        & PEGASUS        & LED-base & LSG-BART-base  \\ \midrule
Multi-News               & 49.9/21.1/25.9 & 47.5/18.7/24.9 & --       & 47.1/18.9/25.2 &  & 49.3/20.3/25.4 & 48.2/20.1/25.4 & --       & 46.3/18.8/25.1 \\
\(\hookrightarrow\) \textit{zero-shot} & 42.0/13.6/20.8 & -- & --            & --             &  & 39.7/11.9/19.2 & -- & --            & --             \\
WCEP                                   & 46.1/25.2/37.9 & -- & --            & 46.0/24.2/37.4 &  & 45.1/24.7/36.7 & -- & --            & 45.9/24.1/37.2 \\
\(\hookrightarrow\) \textit{zero-shot} & 28.0/10.3/20.9 & -- & --            & --             &  & 31.3/10.7/22.2 & -- & --            & --             \\
Multi-XScience                         & 31.9/7.4/18.0  & -- & --            & --             &  & 31.7/6.1/17.1  & -- & --            & --             \\
\(\hookrightarrow\) \textit{zero-shot} & 29.1/4.6/15.7  & -- & --            & --             &  & 27.0/3.9/14.6  & -- & --            & --             \\
\mstoo\!\text{*}                       & --             & -- & 26.4/8.0/19.6 & --             &  & --             & -- & 28.5/9.5/20.9 & --             \\
Cochrane\text{*}                       & --             & -- & 23.9/6.6/17.6 & --             &  & --             & -- & 26.9/6.9/18.4 & --             \\ \bottomrule
\end{tabular}}
\end{table*}

\begin{table*}[!ht]
\centering
\caption{Results of the open-domain MDS experiments. We observe the following: (1) retrieval performance ranges from high (Multi-News, WCEP-10, \textcolor{MyBlue}{\textbf{dark blue}}) to low (Multi-XScience, \mstoo, Cochrane), (2) when summarizers trained on these datasets are provided retrieved documents, they suffer from significant drops in performance (\textcolor{MyRed}{\textbf{dark red}}); more severe performance drops were observed in cases where baseline summarization performance was relatively high (\textcolor{MyGreen}{\textbf{dark green}}). Experiments here used a sparse retriever (BM25); similar results were observed using a dense retriever (Contriever, see Table~\ref{tab:retrieval-dense}). Statistically significant results are \underline{underlined} (paired t-test, p = 0.01).}
\label{tab:retrieval-sparse-complete}
\small
\resizebox{\textwidth}{!}{\begin{tabular}{@{}llccclcccc@{}}
\toprule
 &
   &
  \multicolumn{3}{c}{Retrieval} &
   &
  \multicolumn{4}{c}{Summarization} \\ \cmidrule(lr){3-5} \cmidrule(l){7-10} 
Dataset &
  Model &
  Top-\(k\) Strategy &
  P@K &
  R@K &
   &
  ROUGE-Avg F1 &
  \(\Delta\) ROUGE-Avg F1 &
  BERTScore F1 &
  \(\Delta\) BERTScore F1 \\ \midrule
Multi-News &
  PRIMERA &
  max (10) &
  \gradientretrieval{0.22} &
  \gradientretrieval{0.82} &
   &
  \gradientbaseline{31.66} &
  \gradientdiff[1]{-7.39} &
  \gradientbaseline{31.78} &
  \gradientdiff[1]{-10.33} \\
 &
   &
  mean (3) &
  \gradientretrieval{0.64} &
  \gradientretrieval{0.74} &
   &
  -- &
  \gradientdiff[1]{-2.82} &
  -- &
  \gradientdiff[1]{-4.08} \\
 &
   &
  oracle &
  \gradientretrieval{0.75} &
  \gradientretrieval{0.75} &
   &
  -- &
  \gradientdiff[1]{-1.61} &
  -- &
  \gradientdiff[1]{-2.36} \\
 &
  PEGASUS &
  max &
  -- &
  -- &
   &
  \gradientbaseline{31.23} &
  \gradientdiff[1]{-8.49} &
  \gradientbaseline{29.88} &
  \gradientdiff[1]{-10.87} \\
 &
   &
  mean &
  -- &
  -- &
   &
  -- &
  \gradientdiff[1]{-2.08} &
  -- &
  \gradientdiff[1]{-2.93} \\
 &
   &
  oracle &
  -- &
  -- &
   &
  -- &
  \gradientdiff[1]{-1.15} &
  -- &
  \gradientdiff[1]{-1.50} \\
 &
  LSG-BART-base &
  max &
  -- &
  -- &
  \multicolumn{1}{c}{} &
  \gradientbaseline{30.05} &
  \gradientdiff[1]{-6.44} &
  \gradientbaseline{26.57} &
  \gradientdiff[1]{-8.17} \\
 &
   &
  mean &
  -- &
  -- &
   &
  -- &
  \gradientdiff[1]{-1.77} &
  -- &
  \gradientdiff[1]{-2.35} \\
 &
   &
  oracle &
  -- &
  -- &
   &
  -- &
  \gradientdiff[1]{-0.80} &
  -- &
  \gradientdiff[1]{-0.99} \\
 &
  GPT-3.5-turbo &
  max &
  -- &
  -- &
   \multicolumn{1}{c}{} &
  \gradientbaseline{23.86} &
  \gradientdiff[1]{-2.46} &
  \gradientbaseline{21.68} &
  \gradientdiff[1]{-3.92} \\
 &
   &
  mean &
  -- &
  -- &
   &
  -- &
  \gradientdiff[1]{-1.59} &
  -- &
  \gradientdiff[1]{-2.76} \\
 &
   &
  oracle &
  -- &
  -- &
   &
  -- &
  \gradientdiff[1]{-0.47} &
  -- &
  \gradientdiff[1]{-1.03} \\
WCEP-10 &
  PRIMERA &
  max (10) &
  \gradientretrieval{0.63} &
  \gradientretrieval{0.67} &
   &
  \gradientbaseline{35.50} &
  \gradientdiff[1]{-1.02} &
  \gradientbaseline{48.26} &
  \gradientdiff{-0.76} \\
 &
   &
  mean (9) &
  \gradientretrieval{0.66} &
  \gradientretrieval{0.64} &
   &
  -- &
  \gradientdiff{-0.90} &
  -- &
  \gradientdiff{-0.68} \\
 &
   &
  oracle &
  \gradientretrieval{0.67} &
  \gradientretrieval{0.67} &
   &
  -- &
  \gradientdiff{-0.53} &
  -- &
  \gradientdiff{-0.32} \\
 &
  LSG-BART-base &
  max &
  -- &
  -- &
   &
  \gradientbaseline{35.76} &
  \gradientdiff[1]{-1.15} &
  \gradientbaseline{48.17} &
  \gradientdiff{-0.85} \\
 &
   &
  mean &
  -- &
  -- &
   &
  -- &
  \gradientdiff[1]{-1.19} &
  -- &
  \gradientdiff{-0.84} \\
 &
   &
  oracle &
  -- &
  -- &
   &
  -- &
  \gradientdiff{-0.88} &
  -- &
  \gradientdiff{-0.54} \\
 &
  GPT-3.5-turbo &
  max &
  -- &
  -- &
   &
  \gradientbaseline{26.36} &
  \gradientdiff{-0.22} &
  \gradientbaseline{32.72} &
  \gradientdiff{-0.25} \\
 &
   &
  mean &
  -- &
  -- &
   &
  -- &
  \gradientdiff{-0.06} &
  -- &
  \gradientdiff{-0.33} \\
 &
   &
  oracle &
  -- &
  -- &
   &
  -- &
  \gradientdiff{+0.10} &
  -- &
  \gradientdiff{+0.11} \\
Multi-XScience &
  PRIMERA &
  max (20) &
  \gradientretrieval{0.06} &
  \gradientretrieval{0.40} &
   &
  \gradientbaseline{18.31} &
  \gradientdiff[1]{-0.57} &
  \gradientbaseline{10.57} &
  \gradientdiff[1]{-1.82} \\
 &
   &
  mean (4) &
  \gradientretrieval{0.16} &
  \gradientretrieval{0.27} &
   &
  -- &
  \gradientdiff[1]{-0.25} &
  -- &
  \gradientdiff[1]{-1.27} \\
 &
   &
  oracle &
  \gradientretrieval{0.23} &
  \gradientretrieval{0.23} &
   &
  -- &
  \gradientdiff{-0.06} &
  -- &
  \gradientdiff[1]{-0.97} \\
\mstoo &
  LED-base &
  max (25) &
  \gradientretrieval{0.16} &
  \gradientretrieval{0.22} &
   &
  \gradientbaseline{19.66} &
  \gradientdiff{-0.14} &
  \gradientbaseline{22.74} &
  \gradientdiff{-0.47} \\
 &
   &
  mean (17) &
  \gradientretrieval{0.18} &
  \gradientretrieval{0.18} &
   &
  -- &
  \gradientdiff{-0.10} &
  -- &
  \gradientdiff{-0.13} \\
 &
   &
  oracle &
  \gradientretrieval{0.18} &
  \gradientretrieval{0.18} &
   &
  -- &
  \gradientdiff{-0.01} &
  -- &
  \gradientdiff{-0.21} \\
Cochrane &
  LED-base &
  max (25) &
  \gradientretrieval{0.17} &
  \gradientretrieval{0.57} &
   &
  \gradientbaseline{17.39} &
  \gradientdiff{-0.28} &
  \gradientbaseline{23.12} &
  \gradientdiff[1]{-2.11} \\
 &
   &
  mean (9) &
  \gradientretrieval{0.31} &
  \gradientretrieval{0.44} &
   &
  -- &
  \gradientdiff{+0.34} &
  -- &
  \gradientdiff{-0.32} \\
 &
   &
  oracle &
  \gradientretrieval{0.40} &
  \gradientretrieval{0.40} &
   &
  -- &
  \gradientdiff{+0.10} &
  -- &
  \gradientdiff{+0.00} \\ \bottomrule
\end{tabular}}
\end{table*}

\begin{table*}[t]
\centering
\caption{Results of the open-domain MDS experiments using a dense retriever (Contriever). Difference between a summarizers performance on the ground-truth input documents and performance when the documents were retrieved is shown. Statistically significant results are \underline{underlined} (paired t-test, p = 0.01).}
\label{tab:retrieval-dense}
\resizebox{\textwidth}{!}{\begin{tabular}{@{}llccclcccc@{}}
\toprule
 &
   &
  \multicolumn{3}{c}{Retrieval} &
   &
  \multicolumn{4}{c}{Summarization} \\ \cmidrule(lr){3-5} \cmidrule(l){7-10} 
Dataset &
  Model &
  Top-\(k\) Strategy &
  P@K &
  R@K &
   &
  ROUGE-Avg F1 &
  \(\Delta\) ROUGE-Avg F1 &
  BERTScore F1 &
  \(\Delta\) BERTScore F1 \\ \midrule
Multi-News &
  PRIMERA &
  max (10) &
  \gradientretrieval{0.21} &
  \gradientretrieval{0.80} &
   &
  \gradientbaseline{31.66} &
  \gradientdiff[1]{-7.77} &
  \gradientbaseline{31.78} &
  \gradientdiff[1]{-10.47} \\
 &
   &
  mean (3) &
  \gradientretrieval{0.59} &
  \gradientretrieval{0.70} &
   &
  -- &
  \gradientdiff[1]{-3.31} &
  -- &
  \gradientdiff[1]{-4.60} \\
 &
   &
  oracle &
  \gradientretrieval{0.69} &
  \gradientretrieval{0.69} &
   &
  -- &
  \gradientdiff[1]{-2.20} &
  -- &
  \gradientdiff[1]{-3.07} \\
 &
  PEGASUS &
  max &
  -- &
  -- &
   &
  \gradientbaseline{31.23} &
  \gradientdiff[1]{-8.69} &
  \gradientbaseline{29.88} &
  \gradientdiff[1]{-10.88} \\
 &
   &
  mean &
  -- &
  -- &
   &
  -- &
  \gradientdiff[1]{-2.65} &
  -- &
  \gradientdiff[1]{-3.45} \\
 &
   &
  oracle &
  -- &
  -- &
   &
  -- &
  \gradientdiff[1]{-1.76} &
  -- &
  \gradientdiff[1]{-2.28} \\
 &
  LSG-BART-base &
  max &
  -- &
  -- &
   &
  \gradientbaseline{30.05} &
  \gradientdiff[1]{-6.70} &
  \gradientbaseline{26.57} &
  \gradientdiff[1]{-8.15} \\
 &
   &
  mean &
  -- &
  -- &
   &
  -- &
  \gradientdiff[1]{-2.26} &
  -- &
  \gradientdiff[1]{-2.69} \\
 &
   &
  oracle &
  -- &
  -- &
   &
  -- &
  \gradientdiff[1]{-1.41} &
  -- &
  \gradientdiff[1]{-1.54} \\
WCEP-10 &
  PRIMERA &
  max (10) &
  \gradientretrieval{0.63} &
  \gradientretrieval{0.67} &
   &
  \gradientbaseline{35.50} &
  \gradientdiff{+0.10} &
  \gradientbaseline{48.26} &
  \gradientdiff{+0.90} \\
 &
   &
  mean (9) &
  \gradientretrieval{0.66} &
  \gradientretrieval{0.63} &
   &
  -- &
  \gradientdiff{-0.14} &
  -- &
  \gradientdiff{+0.68} \\
 &
   &
  oracle &
  \gradientretrieval{0.66} &
  \gradientretrieval{0.66} &
   &
  -- &
  \gradientdiff{+0.29} &
  -- &
  \gradientdiff{+0.86} \\
 &
  LSG-BART-base &
  max &
  -- &
  -- &
   &
  \gradientbaseline{35.76} &
  \gradientdiff{-0.56} &
  \gradientbaseline{48.17} &
  \gradientdiff{+0.26} \\
 &
   &
  mean &
  -- &
  -- &
   &
  -- &
  \gradientdiff[1]{-0.96} &
  -- &
  \gradientdiff{+0.10} \\
 &
   &
  oracle &
  -- &
  -- &
   &
  -- &
  \gradientdiff{-0.15} &
  -- &
  \gradientdiff{+0.66} \\
Multi-XScience\textsuperscript{\textdagger} &
  PRIMERA &
  max (20) &
  \gradientretrieval{0.06} &
  \gradientretrieval{0.38} &
   &
  \gradientbaseline{18.31} &
  \gradientdiff[1]{-0.45} &
  \gradientbaseline{10.57} &
  \gradientdiff[1]{-0.96} \\
 &
   &
  mean (4) &
  \gradientretrieval{0.16} &
  \gradientretrieval{0.24} &
   &
  -- &
  \gradientdiff[1]{-0.81} &
  -- &
  \gradientdiff[1]{-0.96} \\
 &
   &
  oracle &
  \gradientretrieval{0.21} &
  \gradientretrieval{0.21} &
   &
  -- &
  \gradientdiff[1]{-0.28} &
  -- &
  \gradientdiff[1]{-0.37} \\
\mstoo &
  LED-base &
  max (25) &
  \gradientretrieval{0.18} &
  \gradientretrieval{0.25} &
   &
  \gradientbaseline{19.66} &
  \gradientdiff[1]{-0.43} &
  \gradientbaseline{22.74} &
  \gradientdiff[1]{-0.70} \\
 &
   &
  mean (17) &
  \gradientretrieval{0.21} &
  \gradientretrieval{0.21} &
   &
  -- &
  \gradientdiff{-0.37} &
  -- &
  \gradientdiff[1]{-0.64} \\
 &
   &
  oracle &
  \gradientretrieval{0.21} &
  \gradientretrieval{0.21} &
   &
  -- &
  \gradientdiff{-0.32} &
  -- &
  \gradientdiff{-0.38} \\
Cochrane &
  LED-base &
  max (25) &
  \gradientretrieval{0.20} &
  \gradientretrieval{0.64} &
   &
  \gradientbaseline{17.39} &
  \gradientdiff[1]{-0.94} &
  \gradientbaseline{23.12} &
  \gradientdiff[1]{-2.77} \\
 &
   &
  mean (9) &
  \gradientretrieval{0.35} &
  \gradientretrieval{0.49} &
   &
  -- &
  \gradientdiff{-0.37} &
  -- &
  \gradientdiff{-0.93} \\
 &
   &
  oracle &
  \gradientretrieval{0.44} &
  \gradientretrieval{0.44} &
   &
  -- &
  \gradientdiff{+0.25} &
  -- &
  \gradientdiff{+0.71} \\ \bottomrule
\end{tabular}}
\end{table*}

All models are implemented in PyTorch \citep{pytorch}, and pretrained weights are obtained from the HuggingFace Transformers library \citep{hf-transformers}. Models were trained and evaluated on 1-4 NVIDIA A100 40GB GPUs. We list details about the models in \autoref{tab:models}. Below, we provide detailed descriptions of all models:

\begin{itemize}
    \item \textbf{LED} \citep{Beltagy2020LongformerTL}: LED replaces full self-attention with local windowed attention and global attention mechanisms that scale linearly with input sequence length, allowing for efficient processing of inputs up to 16K tokens. Its parameters are initialized with the pretrained parameters of BART \citep{lewis-etal-2020-bart}, its positional embeddings with 16 copies of BART's 1K position embeddings. The model is fine-tuned on MDS datasets in a supervised fashion.
    \item \textbf{PEGASUS} \citep{pegasus}: PEGASUS is pretrained using a novel Gap Sentences Generation (GSG) objective, where whole sentences from each document are masked, and concatenated to form a pseudo-summary. The model is then fine-tuned on MDS datasets in a supervised fashion. 
    \item \textbf{PRIMERA} \citep{xiao-etal-2022-primera}: Extends the GSG objective with a novel masking strategy explicitly designed for multi-document inputs and pre-trains on a corpus of multi-document examples. The model is then fine-tuned on MDS datasets in a supervised fashion or used in a zero-shot setting.
    \item \textbf{LSG-BART} \citep{condevaux2022lsg}: Like LED, LSG-BART replaces full self-attention with a sparsified version, dubbed Local-Sparse-Global (LSG) attention, to allow for efficient processing of long inputs. It is initialized with the pretrained parameters of BART and fine-tuned on MDS datasets in a supervised fashion.
\end{itemize}

\subsection{Reproducing Reported Scores} Before experimentation, we attempt to reproduce the reported scores of each MDS model. The results are provided in \autoref{tab:reproduction}. In general, we can reproduce the reported ROUGE scores (and sometimes even improve upon them); however, in a few cases, there are differences as large as \(\sim\)3 ROUGE, with the largest differences being observed for PRIMERA, particularly in the zero-shot setting.

\section{Evaluation Details}
\label{appendix:evaluation-details}

The evaluation metrics, ROUGE and BERTScore, were called from the HuggingFace Evaluate library\footnote{\url{https://github.com/huggingface/evaluate}}. Before metrics are calculated, all text is lightly pre-processed by removing leading and trailing whitespace, newline characters and tabs. For ROUGE, we use the default settings besides \texttt{use\_stemmer=True}.\footnote{\url{https://huggingface.co/spaces/evaluate-metric/rouge}} BERTScore has many parameters which affect the final score; for reproducibility, a hashcode is produced. Our hashcode is: \path{microsoft/deberta-xlarge-mnli_L40_no-idf_version=0.3.11(hug_trans=4.22.0.dev0)-rescaled_fast-tokenize}


\section{Extended Results from: \autoref{experimental-results}}
\label{appendix:experimental-results}

In \textsection \ref{experimental-results}, we presented the results from our open-domain MDS experiments for the sparse retriever (BM25) and max top-\textit{k} strategy only. In \autoref{tab:retrieval-sparse-complete} we present the complete set of results for the sparse retriever. The dense retriever (Contriever) results were comparable and exhibited the same general trends; they are presented in \autoref{tab:retrieval-dense}.

\subsection{Summarization Baselines}
\label{appendix:summarization-baselines}

\begin{figure*}[t]
\centering
\includegraphics[width=0.90\textwidth]{figures/retrieval_errors}
\caption{Absolute error counts for different retrieval systems (sparse and dense) and top-\(k\) selection strategies (max, mean, oracle). For each example in a given dataset, a retrieved document that does not exist in the ground-truth input document set is counted as an \textit{addition} and a ground-truth document that was not retrieved as a \textit{deletion}. Instances of one addition and one deletion are counted as a \textit{replacement}.}
\label{fig:retrieval-errors}
\end{figure*}

In \autoref{tab:baselines}, we present scores of heuristic baselines. Detailed descriptions of each baseline follow:

\begin{itemize}
    \item \textbf{Random (length-matched) summary}: For each example, take the summary to be the reference summary of \textit{another} example from the same dataset that is the same (or similar) length as the examples reference summary. This provides us with coherent (but likely irrelevant) summaries of approximately the correct length from the same domain.
    \item \textbf{All lead}: For each example, take the summary to be the concatenation of the first sentence from each input document. This is motivated by the notion of a \textit{lead bias}, namely that in many summarization datasets (particularly those comprised of news articles), sentences at the beginning of a document are more likely to contain information that appears in the reference summary \citep{nenkova-etal-2011-automatic, hong-nenkova-2014-improving, xing-etal-2021-demoting}.
    \item \textbf{Oracle document}: For each example, take the summary to be the input document with the highest ROUGE-1 F1 score with that example's reference summary. This provides us with relevant (but likely incomplete) summaries with high token overlap. A high score may indicate that a dataset is less ``multi'' \citep{https://doi.org/10.48550/arxiv.2210.12688}.
    \item \textbf{Oracle lead}: The first sentence of the oracle document (see above). For \mstoo \& Cochrane, this is the title of the oracle document.
    \item \textbf{Background/abstract}: Applies only to \mstoo and Multi-XScience. For each example, take the summary to be the additional input from \mstoo (target reviews background section) and Multi-XScience (target articles abstract).
\end{itemize}

\subsection{Document Retrieval Error Analysis}
\label{appendix:retrieval-error-analysis}

In \autoref{fig:retrieval-errors}, we tally the total number of errors made by the sparse (BM25) and dense (Contriever) retrievers on each dataset. For each example, we count an \textit{addition} (i.e. erroneous inclusion) each time a document not in the ground-truth input document set is retrieved, a \textit{deletion} (i.e. erroneous exclusion) each time a ground-truth document is not retrieved and a \textit{replacement} (i.e. an erroneous swap of a relevant document for an irrelevant one) each time both one addition and one deletion occur. In general, the sparse and dense retrievers make a highly comparable number of errors of each type. Unsurprisingly, the oracle top-k strategy tends to produce the lowest number of errors in total (primarily replacements), followed by mean (a mix of all error types) and max (primarily additions).

\begin{figure*}[t]
\includegraphics[width=\textwidth]{figures/backtranslation}
\caption{Graphical depiction of the backtranslation perturbation. A truncated document from the Multi-News \citep{fabbri-etal-2019-multi} dataset is shown, and changes after backtranslation are highlighted.}
\label{fig:backtranslation}
\end{figure*}

\begin{table*}[t]
\centering
\caption{Results of the sorting perturbation experiments. Difference between a summarizers performance on the ground-truth input documents and performance when the documents were perturbed is shown. Statistically significant results are \underline{underlined} (paired t-test, p = 0.01).}
\small
\label{tab:sorting}
\begin{tabular}{@{}llcccclcc@{}}
\toprule
 &
   &
   &
   &
  \multicolumn{2}{c}{\(\Delta\) ROUGE-Avg F1} &
   &
  \multicolumn{2}{c}{\(\Delta\) BERTScore F1} \\ \cmidrule(lr){5-6} \cmidrule(l){8-9} 
Dataset &
  Model &
  ROUGE-Avg F1 &
  BERTScore F1 &
  Random &
  Oracle &
   &
  Random &
  Oracle \\ \midrule
Multi-News &
  PRIMERA &
  \gradientbaseline{31.66} &
  \gradientbaseline{31.78} &
  \gradientdiff{+0.06} &
  \gradientdiff{+0.00} &
   &
  \gradientdiff{+0.02} &
  \gradientdiff{+0.02} \\
 &
  PEGASUS &
  \gradientbaseline{31.23} &
  \gradientbaseline{29.88} &
  \gradientdiff{-0.05} &
  \gradientdiff{+0.04} &
   &
  \gradientdiff{-0.05} &
  \gradientdiff{+0.16} \\
WCEP-10 &
  PRIMERA &
  \gradientbaseline{35.50} &
  \gradientbaseline{48.26} &
  \gradientdiff[1]{-0.86} &
  \gradientdiff{+0.11} &
   &
  \gradientdiff{-0.55} &
  \gradientdiff{+0.57} \\
 &
  LSG-BART-base &
  \gradientbaseline{35.76} &
  \gradientbaseline{48.17} &
  \gradientdiff[1]{-0.98} &
  \gradientdiff{-0.18} &
   &
  \gradientdiff{-0.62} &
  \gradientdiff{+0.38} \\
Multi-XScience &
  PRIMERA &
  \gradientbaseline{18.31} &
  \gradientbaseline{10.57} &
  \gradientdiff{+0.07} &
  \gradientdiff{-0.04} &
   &
  \gradientdiff{+0.13} &
  \gradientdiff{-0.03} \\
MS2 &
  LED-base &
  \gradientbaseline{19.66} &
  \gradientbaseline{22.74} &
  \gradientdiff{+0.09} &
  \gradientdiff{+0.24} &
   &
  \gradientdiff{+0.00} &
  \gradientdiff{-0.01} \\
Cochrane &
  LED-base &
  \gradientbaseline{17.39} &
  \gradientbaseline{23.12} &
  \gradientdiff{-0.41} &
  \gradientdiff{-0.32} &
   &
  \gradientdiff{-0.42} &
  \gradientdiff{+0.06} \\ \bottomrule
\end{tabular}
\end{table*}

\section{Training in the Open-domain Setting}
\label{appendix:training}

In \textsection \ref{training}, we presented the results of our experiments fine-tuning summarizers in the open-domain setting. We fine-tuned and evaluated the best-performing model from each dataset: PRIMERA for Multi-News and Multi-XScience, LSG-BART-Base for WCEP-10 and LED-Base for \mstoo and Cochrane. Each model was fine-tuned for an additional three epochs (we found additional epochs made little difference) using the original training hyperparameters. All models were fine-tuned with the AdamW optimizer \citep{Loshchilov2019DecoupledWD} in PyTorch via the HuggingFace Transformers library. The learning rate was linearly increased for the first 10\% of training steps and linearly decayed to zero afterward. Documents were retrieved using the dense retriever (contriever) and the mean top-k strategy.

\section{Extended Results from: \autoref{simulations}}
\label{appendix:perturbations}

In \textsection \ref{simulations}, we presented results from our experiments simulating document retrieval errors for two model-dataset pairs that exemplified the main trends in the rest of the results. We present complete results for all model-dataset pairs in figures \ref{fig:primera-multinews}-\ref{fig:led-cochrane}.

\subsection{Backtranslation}
\label{appendix:backtranslation}

In the main paper, we use backtranslation to create token-level perturbations. The procedure involves selecting one or more documents from the input set and translating them to another language and back again, often creating small, token-level changes like paraphrasing and synonym substitution (this is sometimes called ``round-trip translation'', or RTT). We choose to translate documents to and from Danish, as there exists freely available and high-performing EN\textrightarrow DA and DA\textrightarrow EN machine translation (MT) models. In particular, we use the models provided by the Language Technology Research Group at the University of Helsinki \citep{helsinki}. We implement backtranslation using the \texttt{nlpaug} library \citep{nlpaug}. In \autoref{fig:backtranslation}, we provide an example of a backtranslated document demonstrating synonym substitution (e.g. \say{highly}\textrightarrow \say{very}), paraphrasing (e.g. \say{said the surviving ones}\textrightarrow \say{said that the survivors}) and grammatical errors (e.g. \say{14 critically endangered black rhinoceros \underline{has} died}).


\begin{table*}[t]
	\small
	\centering
	\caption{Examples of degradation of summarization performance in the open-domain setting. Shown is the output of the summarizer in the open-domain setting (truncated) and human annotator comments (paraphrased). The plausible reason for degradation is based on a manual analysis of summarizer inputs and outputs.}
	\label{tab:manual-analysis}
	\small
	\begin{tabular}{p{0.30\textwidth} p{0.30\textwidth} p{0.30\textwidth}}
		\toprule
		Open-domain model summary                    & Annotator comments                                   & Plausible reason for degradation                                                                                                                             \\ \midrule
		\textit{For the second year in a row, the Academy of Motion Picture Arts and Sciences did not nominate any black actors to any of the 20 slots in the four acting categories […] The Hollywood Reporter calls it a ``whiteout,'' and the president of the African American Film Critics Association says […] There needs to be changes across the board. […] The Academy, which is 94\% white and 77\% male, has been trying to diversify its membership […]}      & The \textbf{reference} summary indicates that meaningful progress is being made in improving diversity amongst members of the Academy of Motion Picture Arts and Sciences. The \textbf{baseline model} gets this correct, but the framing of the \textbf{open-domain model} summary is that little to no progress has been made.                                    &  The \textbf{gold} document set contains 2 documents, each about efforts to improve diversity among members of the Academy of Motion Picture Arts and Sciences, both written \textit{after} the 2016 Oscars. The \textbf{retrieved} document set contains 3 documents, 1 from the gold document set, and 2 written \textit{before} the 2016 Oscars, both criticizing the fact that no black actors were nominated for any acting category.            \\ && \\
		\textit{A GoFundMe campaign has raised more than \$400,000 for a man who lost his wife in childbirth […] Christian musician Nathan Johnson gave birth to his first child […] [his wife] started having complications later in the morning […] Johnson is surrounded by friends and family who are helping him deal with the loss of his wife […]}      & The \textbf{reference} summary is about a GoFundMe campaign created on behalf of Dawn Wells (best known for playing Mary Ann Summers on Gilligan's Island) who is experiencing financial hardship. The \textbf{baseline model} gets this correct, but the \textbf{open-domain model}'s summary is about a completely different GoFundMe campaign.                                    &  The \textbf{gold} document set contains 2 documents, both about a GoFundMe campaign for Dawn Wells. The \textbf{retrieved} document set contains 1 additional document, the story of a entirely \textit{different} GoFundMe campaign about a musician who lost his wife due to complications during childbirth.                         \\ && \\
		\textit{A British woman is ``very lucky'' to be alive after falling from a cruise ship […] The cruise line says in a statement that the woman intentionally jumped overboard […] ``the ship and charter company teams are providing support to the family and all impacted guests during this difficult time,'' […]} & The summary produced by the \textbf{open-domain model} contains an insinuation of possible suicide (i.e. that the woman jumped intentionally) that is not present in the \textbf{reference summary} or \textbf{baseline model} summary. & The \textbf{retrieved} document set contains one additional document compared to the \textbf{gold} document set, about a different woman who \textit{did} intentionally jump overboard while on a cruise. \\  \\
        \textit{Prime Minister David Cameron said he will step down in two days in favor of Theresa May […] who will become Britain's second female leader. […] [S]he will have the task of leading a divided country out of the EU […] The winner will be announced Sept. 9 and will replace David Cameron […]} & The \textbf{reference} summary indicates that May and Andrea Leadsom are the final two candidates for Prime Minister of the UK, which is reflected in the \textbf{baseline model} summary. The summary produced by the \textbf{open-domain model} claims both that May is the winner \textit{and} that the winner is yet to be announced. & The \textbf{retrieved} document set contains one additional document compared to the \textbf{gold} set, about the surprise withdrawal of Leadsom from the race. \\ \bottomrule
	\end{tabular}
\end{table*}

\subsection{Sorting Perturbation Results}
\label{appendix:sorting}

In \autoref{tab:sorting}, we present the tabulated results from the sorting perturbation experiments (see \textsection \ref{simulations} for more details on the experimental procedure and \textsection \ref{experimental-results:simulations} for an analysis of the results).

\section{Human Evaluation} \label{appendix:human-eval}

To make a human evaluation feasible, we chose a single model-dataset pair with high summarization and retrieval performance: PRIMERA and Multi-News.\footnote{We take the results from the highest performing retriever (sparse) and non-oracle top-\(k\) strategy (mean)} To conduct the evaluation, we randomly sampled 50 examples from the test set and presented three human annotators\footnote{The three annotators are a subset of the authors who did not interact with model outputs prior to annotation} with the generated summaries for these examples from the ``baseline'' model (no retrieval) and the open-domain model (with retrieval). Annotators were presented the model summaries in randomized order as \say{model summary A} and \say{model summary B} and instructed to select which summary (\say{A}, \say{B} or \say{Neither}) they preferred for each of two facets, \textit{coverage} and \textit{informativeness},\footnote{There are many facets for which a human evaluation of summarization could be conducted; we choose coverage and informativeness as rough proxies for recall and precision} relative to the provided, human-written target summary \(R\):

\begin{itemize}
    \item \textbf{Coverage} \citep{grusky-etal-2018-newsroom}: How many semantic content units from the reference summary are covered by the model summary.
    \item \textbf{Informativeness} \citep{nenkova-passonneau-2004-evaluating}: How well does the model summary capture the key ideas of the reference summary.
\end{itemize}

\noindent The results from a binomial test\footnote{\url{https://en.wikipedia.org/wiki/Binomial_test}} on the human annotations, as well as inter-annotator agreement (IAA), are presented in \autoref{tab:human-eval}. Human annotators have a statistically significant preference for the baseline model along both facets, with fair inter-annotator agreement (\(\kappa >\)~0.21, \citealp{kappa}), providing further evidence for the degradation of summarization performance in the open-domain setting observed throughout this work. In \autoref{tab:manual-analysis}, we provide examples of summaries produced in the open-domain setting alongside (paraphrased) human annotator comments noting issues with the summary. Based on a manual analysis of the inputs and outputs of the summarizer, we also provide plausible reasons for this observed degradation in summarization quality as it relates to the retrieved versus gold document sets.

\clearpage

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/allenai_PRIMERA_multinews}
\caption{Results of the perturbation experiments on the Multi-News test set using PRIMERA. Mean change in summarization performance plotted against percent of perturbed input documents. 68\% confidence intervals (CI) are plotted as error bands.}
\label{fig:primera-multinews}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/google_pegasus_multi_news}
\caption{Results of the perturbation experiments on the Multi-News test set using PEGASUS. Mean change in summarization performance plotted against percent of perturbed input documents. 68\% confidence intervals (CI) are plotted as error bands.}
\label{fig:pegasus-multinews}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/ccdv_lsg_bart_base_4096_multinews}
\caption{Results of the perturbation experiments on the Multi-News test set using LSG-BART-base. Mean change in summarization performance plotted against percent of perturbed input documents. 68\% confidence intervals (CI) are plotted as error bands.}
\label{fig:lsg-bart-multinews}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/allenai_PRIMERA_wcep}
\caption{Results of the perturbation experiments on the WCEP-10 test set using PRIMERA. Mean change in summarization performance plotted against percent of perturbed input documents. 68\% confidence intervals (CI) are plotted as error bands.}
\label{fig:primera-wcep}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/ccdv_lsg_bart_base_4096_wcep}
\caption{Results of the perturbation experiments on the WCEP-10 test set using LSG-BART-base. Mean change in summarization performance plotted against percent of perturbed input documents. 68\% confidence intervals (CI) are plotted as error bands.}
\label{fig:lsg-bart-wcep}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/allenai_PRIMERA_multixscience}
\caption{Results of the perturbation experiments on the Multi-XScience test set using PRIMERA. Mean change in summarization performance plotted against percent of perturbed input documents. 68\% confidence intervals (CI) are plotted as error bands.}
\label{fig:primera-multixscience}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/allenai_led_base_16384_ms2}
\caption{Results of the perturbation experiments on the \mstoo validation set using LED-base. Mean change in summarization performance plotted against percent of perturbed input documents. 68\% confidence intervals (CI) are plotted as error bands.}
\label{fig:led-ms2}
\end{figure}

\begin{figure}[t!]
\centering
\includegraphics[width=\columnwidth]{figures/allenai_led_base_16384_cochrane}
\caption{Results of the perturbation experiments on the Cochrane validation set using LED-base. Mean change in summarization performance plotted against percent of perturbed input documents. 68\% confidence intervals (CI) are plotted as error bands.}
\label{fig:led-cochrane}
\end{figure}

\end{document}
