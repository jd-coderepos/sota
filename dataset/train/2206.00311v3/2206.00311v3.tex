\subsection{Datasets}
\noindent\textbf{Chinese text line images.} 
The pre-training set consists of $100$ million unlabeled text line images collected from practical scenarios for visual pre-training, and $100$ million synthetic  text line images for language pre-training. The real images are collected from documents and street view, and the text in them is almost in Chinese. We collect text corpus from Chinese corpus \footnote{\url{https://github.com/crownpku/awesome-chinese-nlp}}, and generate $100$ million images with $64$ commonly used fonts using Text Render. Specifically, for each synthetic sample, the text transcription as well as the character bounding boxes are given.

We first pre-train the encoder and decoder serially on the collected real images and the synthetic images, and then finetune our model on a large-scale Chinese text image benchmark BCTR \cite{chenBCTR}. BCTR consists of four subsets (scene, web, document, and handwriting) and provides $1.4$ million fully labeled images in total. The scene subset (Sen) is derived from some scene text datasets, including RCTW \cite{RCTW}, ReCTS \cite{rects}, LSVT \cite{lsvt}, ArT \cite{art}, and CTW \cite{CTW}, resulting in 636,455 images. The web subset (Web) is constructed based on the MTWI \cite{MTWI} dataset and contains 140589 text images. The document subset (Doc) is composed of 500000 synthetic text images generated by Text Render in document style. The handwriting subset (Hand) is collected from a handwriting dataset SCUT-HCCDoc \cite{scuthccdoc}, and 116643 text images are included. 

\vspace{2mm}
\noindent\textbf{English text word images.} 
We follow~\cite{dig} and  collect about $15.8$ million unlabeled English word images from CC-OCR~\cite{tap} for visual pre-training. In addition, we also synthesize $100$ million English word images for language pre-training. Similarly, we collect corpus from WikiText103 \cite{merity2016pointer} and generate synthetic images with Text Render and $10$ commonly used English fonts. 

Following \cite{shi2018aster,yu2020towards,ShanchengFang2021ReadLH,YuxinWang2021FromTT,XinyunZhang2022ContextbasedCL}, two synthetic datasets MJSynth \cite{mj} and SynthText \cite{st} are used for the training of downstream recognition tasks. Besides, we also collect 2.78 million real labeled images from TextOCR~\cite{TextOCR} and Open Images Dataset v5~\footnote{\url{https://storage.openvinotoolkit.org/repositories/openvino_training_extensions/datasets/open_images_v5_text}} as ~\cite{dig} to verify the effectiveness of pre-training when finetuned on real images.  We evaluate our model on six public scene text datasets: ICDAR 2013 (IC13) \cite{ic13}, Street View Text (SVT) \cite{wang2011end},  IIIT5K-Words (IIIT5K) \cite{iiit5k}), ICDAR 2015 (IC15) \cite{ic15}, Street View Text-Perspective (SVTP) \cite{svtp}, and CUTE80 (CUTE) \cite{cute}). The samples in the first three datasets are all regular text images and the remaining datasets may contain perspective or curved text images. 


\subsection{Implementation Details}
\noindent\textbf{Encoder-decoder transformer.} The image patches are fed into a linear projection layer and then sent to the ViT. Two ViT structures are studied: ViT-S ($12$ transformer blocks with dimension $384$), and ViT-B ($12$ transformer blocks with dimension $768$). The decoder consists of four decoder layers, each of which includes a self-attention unit, a cross-attention unit, and an FFN unit. Each attention module is a $12$-head attention with dimension $384$.

We train the encoder-decoder transformer with AdamW optimizer \cite{adamw}, cosine learning rate decay \cite{cosinedecay}, a weight decay of 0.05, a drop path ratio of $0.1$, and a batch size of $512$. When the model is trained from scratch, the learning rate is set to $1e-3$. Otherwise, the model is optimized with an initial learning rate of $1e-4$. We set the training epochs as 120 and 20 for the Chinese text line recognition model and the English word recognition model with a warm-up of 5 epochs and 0.5 epochs respectively.  

\vspace{2mm}
\noindent\textbf{Visual pre-training on encoder.}
The latent contextual regressor consists of four regressor layers. Each layer includes a cross-attention unit and an FFN unit. The image decoder consists of four layers, and each layer includes a self-attention unit and an FFN unit. Each attention module is also a 12-head attention with dimension 384. Following \cite{mae}, we use the normalized pixel values of each masked patch as task.

We optimize the model with AdamW optimizer and set the learning rate with the linear learning rate scaling rule \cite{linear-rule}: $lr=base\_lr \times batch size / 256$. By default, the $base\_lr$ is set to $1.5e-4$ with cosine learning rate decay and a 0.5 epoch warm-up.  We train the encoder for $10$ epochs and $30$ epochs for Chinese data and English data pre-training, with the batch size being 4096. 

\vspace{2mm}
\noindent\textbf{Language pre-training on decoder.}
We mask some characters with a ratio of 0.15 and accordingly mask the patches that contain the characters. This might lead to that a different number of patches are masked for different text images as one character may correspond to a different number of patches. We adopt masked attention to replace the original attention in the encoder with the parameters unchanged.

We pre-train the decoder for $5$ epochs with a batch size of $512$, an initial learning rate of $1e-4$, a 0.5 epochs warmup, and a cosine learning rate decay. 

\vspace{2mm}
\noindent\textbf{Data preprocessing.}
Since the Chinese text line images vary greatly in width, we resize the height of the input image to $32$ with the aspect ratio kept and pad the width of the input images to $400$. For the English word samples, we directly resize all input images to $32\times128$.  We set the width of the split vertical patch to $4$ for all datasets by default. During the training of downstream recognition, some data augmentations like rotation, distortion, and color jitter are also used.

\vspace{2mm}
\noindent\textbf{Evaluation} We evaluate BCTR by first processing the predictions and ground truth with the following rules as \cite{chenBCTR}: (i) convert the full-width characters to half-width characters; (ii) convert all traditional Chinese characters to simplified characters; (iii) convert all English characters to lowercase; (iv) remove all spaces. After that, we compute the accuracy in sentence level over each subset and the whole dataset (Avg).


To evaluate the six scene English text datasets, we follow ~\cite{shi2018aster,yu2020towards,ShanchengFang2021ReadLH,YuxinWang2021FromTT,XinyunZhang2022ContextbasedCL} and evaluate the recognition performance of our model with case-insensitive word accuracy. We also report the average accuracy (Avg) over all samples.



\subsection{Ablation Studies}
In this section, we conduct ablation studies on the BCTR dataset to verify the effectiveness of our proposed pre-training method. All experiments are conducted on 8 A100 GPUs with the ViT-B as the encoder.

\vspace{2mm}
\noindent\textbf{The effectiveness of vision-language pre-training.}
To verify the effectiveness of vision-language pre-training, \lyu{four} models are trained. (i) The model is randomly initialized. (ii) The encoder is initialized with visual pre-training, and the rest parts are randomly initialized. \lyu{(iii) The model is initialized with language pre-training which pre-trained on synthetic data.} (iv) The model is initialized with vision-language pre-training.


\lyu{The results are presented in Table \ref{tab:encoderpretraining}, which demonstrate the effectiveness of vision-language pre-training. Specifically, the superiority of the "V" model over the "Scratch" model indicates that visual pre-training of the encoder can lead to an improvement of up to $4\%$. In addition, language pre-training also yields better performance, achieving a $1.9\%$ improvement over the "Scratch" model. Moreover, the visual pre-training and language pre-training are complementary, as evidenced by the $1\%$ improvement achieved by the "V + L" model over the "V" model. These results provide strong evidence that language pre-training is a valuable approach.}

\begin{table}[!t]
  \caption{Ablation about vision-language pre-training. ``Scratch" means the model is trained from scratch. ``V" and ``L" mean visual pre-training and language pre-training respectively.
}
\label{tab:encoderpretraining}
\centering
  \setlength{\tabcolsep}{10pt}
  \begin{tabular}{lccccc}
    \toprule
    &Sce &Web &Doc &Hand &Avg\\
    \midrule
Scratch &68.8 &70.7 &98.6 &49.4 &75.8 \\
V  &72.3 &73.7 &99.2 &62.5 &79.8 \\
    \lyu{L}  &\lyu{71.0} &\lyu{72.4} &\lyu{98.8} &\lyu{54.5} &\lyu{77.7} \\
    V + L  &\textbf{73.9} &\textbf{74.8} &\textbf{99.3} &\textbf{63.7} &\textbf{80.8} \\
  \bottomrule
\end{tabular}
\end{table}


\vspace{2mm}
\noindent\textbf{Evaluation of representation quality.} 
To evaluate the quality of representations learned by our pre-trained model, we conducted linear probing experiments. We fixed the pre-trained encoder and decoder, which was pre-trained using vision-language pre-training, and only updated the parameters of the remaining linear layer. Surprisingly, the model achieved an average accuracy of $47.8\%$, indicating that with vision-language pre-training, the encoder and decoder are already capable of learning meaningful representations.

We also examined the quality of representations learned through visual pre-training, which fixes the pre-trained encoder and fine-tunes the decoder and linear layer of the recognition model. The model achieved an average accuracy of $73.6\%$, demonstrating that visual pre-training also enhances the representation quality of the encoder.



\vspace{2mm}
\noindent\textbf{The effectiveness of our  serially pre-training mechanism.} To address the large domain gap between real and synthesized images, we fixed the pre-trained encoder when conducting language pre-training on the decoder. A comparison of our serially pre-trained mechanism with the encoder retrained from pre-trained weights is shown in Table \ref{tab:retraining_vs_fixing}, which demonstrates the effectiveness of our approach. Specifically, when retraining the encoder from pre-trained weights during decoder pre-training, an average accuracy of $76.7\%$ was achieved, which outperformed the scratch model ($75.8\%$). However, this approach yielded a $4.1\%$ drop in accuracy compared to our serially pre-trained model ($80.8\%$), suggesting that the pre-trained encoder would be impacted by the synthesized text images if it were retrained during decoder pre-training. This emphasizes the large domain gap between synthetic and real data.



\begin{table}[!t]
  \caption{Studying if the pre-trained encoder is simultaneously retrained during decoder pre-training. (i) The model is trained from scratch. (ii) the pre-trained encoder is retrained during the decoder pre-training. (iii) The pre-trained encoder is fixed when conducting decoder pre-training.}
  \label{tab:retraining_vs_fixing}
  \centering
  \setlength{\tabcolsep}{7pt}
  \begin{tabular}{lccccc}
    \toprule
    &Sce &Web &Doc &Hand &Avg\\
    \midrule
    Scratch &68.8 &70.7 &98.6 &49.4 &75.8 \\
    Retrain encoder &69.0 &71.4 &99.0 &53.3 &76.7 \\
    Fix encoder &\textbf{73.9} &\textbf{74.8} &\textbf{99.3} &\textbf{63.7} &\textbf{80.8} \\
  \bottomrule
\end{tabular}
\end{table}

\vspace{2mm}
\noindent\textbf{Masking ratio of visual pre-training} We conducted an exploration into the effect of different masking ratios, namely $0.30$, $0.45$, and $0.60$, for visual pre-training. The results have been presented in Table~\ref{tab:encodermask_ratio}, and we found that the optimal masking ratio for downstream recognition task was $0.45$. It is worth noting that our optimal masking ratio is lower than that reported in MAE \cite{mae} ($0.75$). This discrepancy could be attributed to the higher information density of the text images used in our experiments.


\begin{table}[h]
  \caption{Ablation about the masking ratio of visual pre-training. In our experiments, the optimal
masking ratio for downstream recognition task was $0.45$.}
  \label{tab:encodermask_ratio}
  \centering
  \setlength{\tabcolsep}{8pt}
  \begin{tabular}{lccccc}
    \toprule
    {Masking ratio}
    &Sce &Web &Doc &Hand &Avg\\
    \midrule
    0.30 &71.5 &73.1 &99.1 &61.8 &79.3 \\
    0.45 &\textbf{72.3} &\textbf{73.7} &\textbf{99.2} &\textbf{62.5} &\textbf{79.8} \\
    0.60  &72.0 &73.6 &99.1 &60.7 &79.4 \\
  \bottomrule
\end{tabular}
\end{table}


\vspace{2mm}
\noindent\textbf{Masking strategy of language pre-training} \lyu{We have examined the impact of masking strategy on language pre-training in Table~\ref{tab:maskingdecoderpretraining}. The results show that the language models trained with the masking strategy outperformed those without on both the scenarios of only conducting language pre-training and integrating visual pre-training. The results highlight the efficacy of masking strategy in capturing the underlying linguistic rules during pre-training, particularly for challenging scenarios such as scenes and handwritten text recognition.}



\begin{table}
  \caption{Ablation about the masking strategy in language pre-training. It can be seen that masking helps the performance, particularly for challenging scenarios such as scenes and handwritten text recognition, where accurate recognition is more difficult to achieve. ``M" means trained with the masking strategy and ``V" means trained with the visual pre-training.}
  \label{tab:maskingdecoderpretraining}
\centering
  \setlength{\tabcolsep}{9pt}
  \begin{tabular}{lcccccc}
    \toprule
   M & V &Sce &Web &Doc &Hand &Avg\\
    \midrule
    \xmarkg &\xmarkg    &69.3 &71.4 &98.6 &49.3 &76.1 \\
    \cmark &\xmarkg &\textbf{71.0} &\textbf{72.4} &\textbf{98.8} &\textbf{54.5} &\textbf{77.7} \\
    \midrule
    \xmarkg &\cmark    &73.1 &74.6 &99.3 &63.6 &80.5 \\
    \cmark &\cmark &\textbf{73.9} &\textbf{74.8} &99.3 &\textbf{63.7} &\textbf{80.8} \\
  \bottomrule
\end{tabular}
\end{table}

\vspace{2mm}
\noindent\textbf{Comparison with supervised pre-training.} To demonstrate the superiority of our vision-language pre-training, we conducted a comparison with supervised pre-training, the results of which are presented in Table~\ref{tab:sup-pretraining}. Specifically, we pre-trained the recognition model on 100 million synthetic images and fine-tuned it on the BCTR dataset. Our findings show that, compared to the model trained from scratch, the model with supervised pre-training only achieved marginal improvement, indicating that supervised pre-training with synthetic images is unnecessary when there is an abundance of real labeled data. However, our proposed vision-language pre-training resulted in a $5\%$ improvement, highlighting its effectiveness.


\begin{table}
    \caption{Comparison with supervised pre-training. (i) The model is trained without pre-training model. (ii) The pre-trained model is trained on synthetic data with supervised pre-training. (iii) The model is pre-trained with our vision-language pre-training. }
    \label{tab:sup-pretraining}
    \setlength{\tabcolsep}{4.5pt}
    \centering
    \begin{tabular}{lccccc}
    \toprule
    &Sce &Web &Doc &Hand &Avg\\
    \midrule
    Scratch  &68.8 &70.7 &98.6 &49.4 &75.8 \\
    Supervised pre-training  &69.3 &71.4 &98.6 &49.3 &76.1 \\
    Ours   &\textbf{73.9} &\textbf{74.8} &\textbf{99.3} &\textbf{63.7} &\textbf{80.8} \\
    \bottomrule
    \end{tabular}
  \label{tab:tabe}
\end{table}

\vspace{2mm}
\noindent\textbf{Vertical patch size.} In our study, we evaluated the performance of two different patch sizes without pre-training, the results of which are presented in Table~\ref{tab:patch-size}. We found that the larger patch size led to worse performance, dropping the accuracy by $3.6\%$. We hypothesize that this may be due to the higher information density of the embedded token in the larger patch size, making it more difficult to learn.


\begin{table}[h]
    \caption{Ablation 
    about the vertical patch size.}
    \label{tab:patch-size}
    \setlength{\tabcolsep}{9pt}
    \centering
    \begin{tabular}{lccccc}
    \toprule
    {Patch size}
    &Sce &Web &Doc &Hand &Avg\\
    \midrule
    $32 \times 4$  &\textbf{68.8} &\textbf{70.7} &\textbf{98.6} &\textbf{49.4} &\textbf{75.8} \\
    $32 \times 8$  &64.0 &67.3 &97.5 &43.3 &72.2 \\
    \bottomrule
    \end{tabular}
  \label{tab:tabe}
\end{table}


\begin{figure*}[t]
\includegraphics[width=0.98\textwidth]{imgs/vis.pdf} 
    \caption{Example results of masked image modeling. The first row are the input images, the remaining two rows are masked images and reconstructed images.
    }
    \label{fig:vis}
\end{figure*}


\begin{figure*}[t]
  \includegraphics[width=0.98\textwidth]{imgs/compare.pdf} 
    \caption{Visualization of text recognition results. The images in the first column and the fourth column are the input images.  The results in the second column and the fifth column are predicted by the models trained from scratch. The results in column 3 and column 6 are output by the models with vision-language pre-training.
    }
    \label{fig:compare}
\end{figure*}

\vspace{2mm}
\noindent\textbf{Generalizability.} \lyu{To demonstrate the effectiveness and generalizability of our proposed vision-language pre-training, we conducted experiments on a widely used text recognition decoder, the Connectionist Temporal Classification (CTC) decoder. We utilized multiple transformer encoder layers as the sequence decoder and trained the model using CTC loss. During language pre-training, we randomly masked some characters and predicted the entire text sequence using CTC loss.}

\lyu{To validate the effectiveness of our approach, we trained two models: one trained from scratch and the other initialized with vision-language pre-training. Our results demonstrate that the model initialized with vision-language pre-training achieved higher accuracy, with a $3.4\%$ performance gain over the scratch model. Specifically, we achieved accuracies of $76.7\%$ and $80.1\%$, respectively. These findings support the robustness and generalizability of our proposed model in the encoder-decoder recognition framework.}


\vspace{2mm}
\noindent\textbf{Qualitative analysis.}~\lyu{
We visualize some examples result of masked image modeling in Figure~\ref{fig:vis}. The input images are randomly masked with a masking ratio of 0.45. As a result,  a part of a character or a whole character may be not visible for the encoder. The semantically plausible reconstructed results in the third row show that meaningful visual representation are learned by the encoder.}

\lyu{We also show some results of the models without and with vision-language pre-training in Figure~\ref{fig:compare}. As shown, when trained from scratch, the model is not robust to WordArt, occluded characters, and characters in similar forms. Benefiting from the visual and linguistic prior knowledge of real images and text corpus, the model with vision-language pre-training can handle the above-mentioned scenarios well.}




\vspace{2mm}
\subsection{Comparison with State-of-the-art Methods}
\vspace{2mm}
\noindent\textbf{Comparison with state-of-the-art pre-training method.} \lyu{We compare our method with previous state-of-the-art visual pre-training method DiG~\cite{dig} fairly with the same data setting. Specifically, we first pre-train our model on CC-OCR, MJSynth, and SynthText, following~\cite{dig} and finetune the pre-trained model on varying percentages of real labeled samples ($1\%$, $10\%$, and $100\%$) sourced from TextOCR and Open Images Dataset v5. Our model's performance was thoroughly evaluated and compared against DiG, as presented in Table~\ref{tab:eng_real}.}

\lyu{We discovered that pre-training DiG led to a significant improvement in performance compared to the DiG baseline results obtained without pre-training. However, it is unclear whether this improvement is solely due to DiG pre-training or differences in the training settings. To reduce uncertainty, we trained a commonly used recognition model, CRNN~\cite{shi2016end}\footnote{ https://github.com/PaddlePaddle/PaddleOCR}, using the same data settings as our model, and achieved results of $63.5\%$, $79.6\%$, and $89.4\%$. Notably, CRNN was published in 2015 and its performance is generally not competitive with most recent recognition methods. The results of CRNN in Table~\ref{tab:eng_real} are significantly higher than the DiG baseline, suggesting that the observed performance gains may be primarily due to differences in the training settings used for the DiG baseline rather than the DiG pre-training.}



\lyu{Due to the aforementioned reasons, we directly compared the performance of pre-training. Remarkably, our proposed approach significantly outperformed DiG in all the cases. Most notably, our method achieved an exceptional accuracy of $90.9\%$ with only 27.8K ($1\%$) real labeled images during finetuning, providing substantial evidence of the significant reduction in the demand for labeled data offered by our approach. Furthermore, the performance gains observed in our model, equipped with pre-trained encoder and decoder, further underscore the effectiveness of our pre-training mechanism.}






\begin{table}[!t]
    \caption{Comparison with state-of-the-art pre-training method DiG. “Scratch"  denotes the model is trained from scratch. “V" means finetuned with visual pre-training, “V + L" means finetuned with vision-language pre-training.}
    \label{tab:eng_real}
    \setlength{\tabcolsep}{6pt}
    \centering
    \begin{tabular}{lccccc}
    \toprule
    &$1\%$ &$10\%$ &$100\%$ &\#Params \\
    \midrule
    CRNN Scratch &63.5 &79.6 &89.4 &25M \\ 
    \midrule
    DiG (ViT-S) Scratch  &9.2 &73.4 &91.4 &36M \\
    DiG (ViT-S) V &84.6 &92.0 &94.6 &36M \\
    \midrule
    Ours (ViT-S) Scratch  &72.3 &90.6 &95.2 &31M  \\
    Ours (ViT-S) V  &87.7 &92.6 &95.3 &31M  \\
    Ours (ViT-S) V + L  &\textbf{90.9} &\textbf{93.8} &\textbf{95.6} &31M  \\
    \bottomrule
    \end{tabular}
  \label{tab:tabe}
\end{table}


\vspace{2mm}
\noindent\textbf{Chinese Text Line Recognition.} We evaluate the ability of our model to recognize Chinese text lines on the BCTR dataset. We set the number of character queries $N$ to $40$ since most of the samples in BCTR have less than 40 characters. We show the results of our method and some representative methods on the BCTR dataset in Table~\ref{tab:result_bctr}. When training from scratch, our method with ViT-S as encoder outperforms all the previous methods while with the smallest model size. Specifically, our method is better than the previous best method TransOCR \cite{ChenLX21} by $2.8\%$ ($72.8\% v.s. 75.6\%$). When training with the pre-trained encoder and decoder, our models surpass the previous best results by large margins. In detail, our method shows steady improvement with the increase of the model size and improves over the state-of-the-art by $5.3\%$ and $8.0\%$ respectively.

\vspace{2mm}
\noindent\textbf{English scene text recognition.} Following \cite{shi2016end, shi2018aster}, we set the number of character queries N to 25 which exceeds the lengths of most English words. Since scene text appeared in natural scenes always with distortions or irregular layouts, we employ a spatial transformer network \cite{STN} which is adopted in \cite{shi2018aster} to rectify the input image and train it with our recognizer jointly. 


\begin{table*}
    \caption{Text recognition results on
    the BCTR dataset. VP and LP mean Visual Pre-training and Language Pre-training respectively.}
    \label{tab:result_bctr}
    \setlength{\tabcolsep}{5pt}
    \centering
    \begin{tabular}{lcccccccc}
    \toprule
     Methods & VP & LP &Sce &Web &Doc &Hand &Avg & \#Params \\
    \midrule
    
    CRNN \cite{shi2016end} &\xmarkg &\xmarkg  &53.4 &54.5 &97.5 &46.4 &67.0 &- \\
    ASTER \cite{shi2018aster} &\xmarkg &\xmarkg &54.5 &52.3 & 93.1 &38.9 &64.7 &- \\
SAR \cite{sar} &\xmarkg &\xmarkg &62.5 &54.3 &93.8 &31.4 &67.3 &- \\
    SRN \cite{yu2020towards} &\xmarkg &\xmarkg &60.1 &52.3 &96.7 &18.0 &65.0 &- \\
TransOCR \cite{ChenLX21} &\xmarkg &\xmarkg &63.3 &62.3 &96.9 &53.4 &72.8 &84M \\

    \textbf{Ours (ViT-S)}&\xmarkg &\xmarkg  &68.6 &70.3 &98.5 &49.1 &75.6 &36M \\
    \textbf{Ours (ViT-B)} &\xmarkg &\xmarkg &68.8 &70.7 &98.6 &49.4 &75.8 &100M \\
    
    
    \midrule
    \textbf{Ours (ViT-S)} &\cmark &\cmark &71.4 &72.5 &98.8 &55.6 &78.1 &36M\\
    \textbf{Ours (ViT-B)} &\cmark &\cmark &\textbf{73.9} &\textbf{74.8} &\textbf{99.3} &\textbf{63.7} &\textbf{80.8} &100M \\
\bottomrule
    \end{tabular}
\end{table*}



\begin{table*}[h]
    \caption{Text recognition results on six English scene text datasets. VP, LP and AD mean Visual Pre-training, Language Pre-training and Annotated Data respectively.}
    \label{tab:result_engscenetext}
    \centering
\begin{tabular}{lccccccccccc}
    \toprule
    Methods & VP & LP & AD &IC13 &SVT &IIIT5K &IC15 &SVTP &CUTE &Avg & \#Params\\
    \midrule
    ASTER \cite{shi2018aster} &\xmarkg &\xmarkg &synth &91.8 &89.5 &93.4 &76.1 &78.5 &79.5 &86.7 &- \\  
    TextScanner \cite{TextScanner} &\xmarkg &\xmarkg &synth &92.9 &90.1 &93.9 &79.4 &84.3 &83.3 &84.4 &- \\  
PIMNet \cite{QiaoZWWZJWW21} &\xmarkg &\xmarkg &synth &95.2 &91.2 &95.2 &83.5 &84.3 &84.4 &90.5 &- \\
    SRN \cite{yu2020towards} &\xmarkg &\xmarkg &synth  &95.5 &91.5 &94.8 &82.7 &85.1 &87.8 &90.4 &55M \\
    VisionLan \cite{YuxinWang2021FromTT} &\xmarkg &\xmarkg &synth &95.7 &91.7 &95.8 &83.7 &86.0 &88.5 &91.2 &33M \\
    ABINet \cite{ShanchengFang2021ReadLH} &\xmarkg &\xmarkg &synth &94.9 &90.4 &94.6 &81.7 &84.2 &86.5 &89.8 &24M \\
    I2C2W \cite{I2C2W} &\xmarkg &\xmarkg &synth &95.0 &91.7 &94.3 &82.8 &83.1 &93.1 &90.2 &- \\
    \textbf{Ours (ViT-S)} &\xmarkg &\xmarkg &synth &97.7 &93.7 &95.4 &86.6 &89.0 &87.5 &92.5 &31M \\
    \textbf{Ours (ViT-B)} &\xmarkg &\xmarkg &synth &96.8 &94.7 &95.3 &87.1 &89.3 &90.6 &92.7 &97M \\
    \midrule
    SEED \cite{QiaoZYZ020} &\xmarkg &\cmark &synth &92.8 &89.6 &93.8 &80.0 &81.4 &83.6 &88.3 &- \\
    ABINet \cite{ShanchengFang2021ReadLH} &\xmarkg &\cmark &synth &97.4 &93.5 &96.2 &86.0 &89.3 &89.2 &92.7 &37M \\
    ConCLR \cite{XinyunZhang2022ContextbasedCL} &\xmarkg &\cmark &synth &97.7 &94.3 &96.5 &85.4 &89.3 &91.3 &92.8 &37M \\
    PerSec \cite{HaoLiu2022PerceivingSC} &\cmark &\xmarkg  &synth &97.2 &94.6 &96.3 &84.4 &89.5 &90.2 &92.4 &- \\
    DiG (ViT-S) \cite{dig} &\cmark &\xmarkg &synth &97.1 &93.4 &96.7 &87.1 &90.1 &88.5 &93.1 &36M \\
    DiG (ViT-B) \cite{dig} &\cmark &\xmarkg &synth &96.9 &94.6 &96.7 &87.1 &91.0 &91.3 &93.4 &52M \\
    TrOCR \cite{TrOCR} &\cmark &\cmark   &synth &\textbf{98.3} &93.2 &91.0 &84.0 &91.0 &89.6 &90.3 &558M \\
    \textbf{Ours (ViT-S)} &\cmark &\cmark  &synth &97.7 &94.0 &95.8 &87.5 &90.2 &89.2 &93.0 &31M \\
    \textbf{Ours (ViT-B)} &\cmark &\cmark  &synth &98.1 &94.9 &95.8 &87.5 &89.8 &90.3 &93.1 &97M \\
\midrule
    DiG (ViT-S) \cite{dig} &\cmark &\xmarkg &real &97.3 &96.1 &97.7 &88.6 &91.6 &96.2 &94.6 &36M \\
    DiG (ViT-B) \cite{dig} &\cmark &\xmarkg  &real &97.6 &96.5 &97.6 &88.9 &92.9 &\textbf{96.5} &94.9 &52M \\
    MAERec-S \cite{Jiang_2023_ICCV} &\cmark &\xmarkg  &real &- &- &- &- &- &- &94.6 &21M \\
    \textbf{Ours (ViT-S)} &\cmark &\cmark  &real &97.8 &\textbf{96.9} &\textbf{98.0} &\textbf{90.2} &\textbf{94.9} &96.2 &\textbf{95.6} &31M \\
    \textbf{Ours (ViT-B)} &\cmark &\cmark  &real &98.2 &\textbf{96.9} &\textbf{98.0} &90.1 &94.6 &95.8 &\textbf{95.6} &97M \\
    
    \bottomrule
    \end{tabular}
\end{table*}

\lyu{Text recognition in the early days faced the challenge of limited annotated data, which led to the common practice of training on synthetic data and testing on real data. However, as data has accumulated over the past decade and unsupervised techniques have advanced, pre-training on synthetic data and testing on real data has become increasingly impractical. Firstly, there is now an abundance of millions of real data samples available, providing sufficient resources for training recognition models. Secondly, domain differences between synthetic and real data can cause differences in the effectiveness of training. Therefore, the performance demonstrated in synthetic data training may not be representative of real data training. Thirdly, current unsupervised methods usually pre-train on real data to learn good feature representations, and fine-tuning on synthetic data may impair the effectiveness of pre-training. Consequently, we advocate pre-training on unlabeled real data and fine-tuning on labeled real data for evaluating model performance. As most methods are trained only on synthetic data, we also provide a comparison with this approach for reference. We report the results of our method and existing methods in Table \ref{tab:result_engscenetext} and compare them in detail. }

When trained on ``synth"  (MJSynth + SynthText) without pre-training, our
method significantly surpasses the previous methods and
achieves the best performance.  When trained on synthetic data with pre-training, our method achieves comparable results with previous best model DiG~\cite{dig}. Besides, When finetuned on real datasets (TextOCR + Open Images Dataset v5) as ~\cite{dig, Jiang_2023_ICCV}, our method achieves the best performance ($95.6\%$ vs. $94.9\%$ vs. $94.6\%$). The extensive comparisons demonstrate the excellence of our proposed method. 


Note that, some methods may also achieve good performance but not relevant to pre-training are not listed. These methods, perform semi-supervised learning on real data~\cite{ShanchengFang2021ReadLH,pushing}, refine the results with iterative mechanism~\cite{ShanchengFang2021ReadLH,Autoregressive}, or predict text in multi-granularity~\cite{multi-granularity} are complementary to our approach and may benefit our approach.

\lyu{We  also observe that the improvement brought by pre-training is more pronounced for Chinese benchmarks than for English benchmarks. This could be attributed to several reasons. Firstly, the current English recognition datasets contain fewer characters and shorter text length, making it relatively easier than Chinese recognition. Secondly, the accuracy of current models on English benchmarks is already high and near saturation, with only marginal gains from pre-training as shown by the small improvement margin ($0.4\%$) between DiG (ACM MM2022) and ABINet (CVPR 2021). Lastly, pre-training is more beneficial for challenging scenarios where the original model performs poorly. In many real-world scenarios that are more challenging and lack sufficient labeled data, the performance gain from pre-training can be substantial, such as the handwriting set of BCTR.}

\vspace{2mm}
\noindent\textbf{Runtime Comparison.} \lyu{We compare the runtime of our method with two competitive other recognition methods that with
similar parameters as ours. The runtime is evaluated on an
A100 GPU with a batch size of 64, the results are shown in Table~\ref{tab:run_time}. Compared to ABINet and DiG, our method has the smallest number of parameters, the highest accuracy, and the fastest running speed.}

\begin{table}
    \caption{Comparison of run time.}
    \label{tab:run_time}
\setlength{\tabcolsep}{14pt}
    \centering
    \begin{tabular}{lccc}
    \toprule
    Methods &Avg &\#Params &FPS \\
    \midrule
ABINet~\cite{ShanchengFang2021ReadLH}  &92.7 &37M &545 \\
    DiG~\cite{dig}  &94.6 &36M &188 \\
    Ours(ViT-S)   &\textbf{95.6} &\textbf{31M} &\textbf{1160} \\
    \bottomrule
    \end{tabular}
  \label{tab:tabe}
\end{table}
