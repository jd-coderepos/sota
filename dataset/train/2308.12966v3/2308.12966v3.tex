\documentclass{article}
\usepackage{hello}

\usepackage[margin=1in]{geometry}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{verbatim}
\usepackage{caption}
\usepackage{longtable}
\usepackage{supertabular}


\usepackage{enumitem}
\usepackage{tablefootnote}
\usepackage[round,semicolon]{natbib}
\usepackage{colortbl}
\usepackage{xspace}
\usepackage{textcomp}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{lscape} 
\usepackage{siunitx}

\setlength{\columnsep}{2em}
\setlength{\parindent}{0em}
\setlength{\parskip}{0.7em}




\usepackage{enumitem}
\usepackage{tablefootnote}

\usepackage{colortbl}
\usepackage{xspace}
\usepackage{textcomp}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{lscape} 
\usepackage{siunitx}
\definecolor{dt}{gray}{0.7}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\usepackage{pifont}       \usepackage{bbding}       \usepackage{fontawesome}


\usepackage{amssymb}\usepackage{pifont}\usepackage{scrextend}

\usepackage{array}
\usepackage{tgpagella}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\usepackage[colorlinks,citecolor=mydarkblue,urlcolor=mydarkblue,linkcolor=mydarkblue]{hyperref}
\usepackage{url}            \usepackage{nicefrac}       \usepackage{changepage}
\usepackage{xargs}          \usepackage{wrapfig,lipsum,booktabs}
\usepackage{longtable}
\usepackage{subcaption}
\usepackage{endnotes}


\usepackage{pgfplots}
\usetikzlibrary{pgfplots.groupplots}
\pgfplotsset{compat=1.3}
\usepackage{tikz}
\usetikzlibrary{patterns}

\usepackage[most]{tcolorbox}

\usepackage[capitalize,noabbrev]{cleveref}
\crefname{section}{Section}{\S\S}
\Crefname{section}{Section}{\S\S}
\crefname{table}{Table}{Tables}
\crefname{figure}{Figure}{Figures}
\crefname{algorithm}{Algorithm}{}
\crefname{equation}{eq.}{}
\crefname{appendix}{Appendix}{}
\crefformat{section}{Section #2#1#3}
\usepackage{multicol}
\usepackage{multirow}

\usepackage{titlesec}
\titleformat*{\section}{\large\bfseries}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

\DeclareMathOperator{\softmax}{softmax}
\DeclareMathOperator{\concat}{concat}
\DeclareMathOperator{\layernorm}{LayerNorm}

\definecolor{battleshipgrey}{rgb}{0.3, 0.3, 0.3}
\definecolor{brilliantrose}{rgb}{1.0, 0.33, 0.64}
\definecolor{americanrose}{rgb}{1.0, 0.01, 0.24}
\definecolor{jweigreen}{rgb}{0,0.45,0.24}
\definecolor{bluegray}{rgb}{0.1, 0.1, 0.4}
\definecolor{ao(english)}{rgb}{0.0, 0.5, 0.0}
\definecolor{blanchedalmond}{rgb}{1.0, 0.92, 0.8}
\definecolor{atomictangerine}{rgb}{1.0, 0.6, 0.4}
\definecolor{chocolate(web)}{rgb}{0.82, 0.41, 0.12}
\definecolor{bananayellow}{rgb}{1.0, 0.88, 0.21}
\definecolor{goldenbrown}{rgb}{0.6, 0.4, 0.08}
\definecolor{aliceblue}{rgb}{0.94, 0.97, 1.0}
\definecolor{beige}{rgb}{0.96, 0.96, 0.86}
\definecolor{babyblue}{rgb}{0.54, 0.81, 0.94}
\definecolor{camel}{rgb}{0.76, 0.6, 0.42}
\definecolor{cinnamon}{rgb}{0.82, 0.41, 0.12}
\definecolor{deepskyblue}{rgb}{0.0, 0.75, 1.0}
\definecolor{frenchblue}{rgb}{0.0, 0.45, 0.73}
\definecolor{classicrose}{rgb}{0.98, 0.8, 0.91}
\definecolor{frenchrose}{rgb}{0.96, 0.29, 0.54}
\definecolor{frenchlilac}{rgb}{0.53, 0.38, 0.56}
\definecolor{frenchbeige}{rgb}{0.65, 0.48, 0.36}
\definecolor{verylightgreen}{RGB}{240, 255, 235}
\definecolor{verylightred}{RGB}{255, 235, 235}
\definecolor{verylightyellow}{RGB}{255, 254, 235}
\definecolor{dt}{gray}{0.7}
\newcommand{\battleshipgrey}[1]{{\color{battleshipgrey}{#1}}}
\newcommand{\americanrose}[1]{{\color{americanrose}{#1}}}
\newcommand{\jweigreen}[1]{{\color{jweigreen}{#1}}}
\newcommand{\darkgreen}[1]{{\color{ao(english)}{#1}}}
\newcommand{\aliceblue}[1]{{\color{aliceblue}{#1}}}
\newcommand{\beige}[1]{{\color{beige}{#1}}}
\newcommand{\babyblue}[1]{{\color{babyblue}{#1}}}
\newcommand{\camel}[1]{{\color{camel}{#1}}}
\newcommand{\cinnamon}[1]{{\color{cinnamon}{#1}}}
\newcommand{\deepskyblue}[1]{{\color{deepskyblue}{#1}}}
\newcommand{\frenchblue}[1]{{\color{frenchblue}{#1}}}
\newcommand{\classicrose}[1]{{\color{classicrose}{#1}}}
\newcommand{\frenchrose}[1]{{\color{frenchrose}{#1}}}
\newcommand{\frenchlilac}[1]{{\color{frenchlilac}{#1}}}
\newcommand{\atomictangerine}[1]{{\color{atomictangerine}{#1}}}
\newcommand{\gptcolor}[0]{frenchlilac!80}
\newcommand{\anthrocolor}[0]{black!40}
\newcommand{\optcolor}[0]{atomictangerine}

\definecolor{forestgreen}{HTML}{2e7d43}
\definecolor{color1}{HTML}{FF9999}
\definecolor{color2}{HTML}{FF6666}
\definecolor{color3}{HTML}{FF3333}
\definecolor{color4}{HTML}{E60000}
\definecolor{color5}{HTML}{B30000}
\definecolor{color6}{HTML}{8CD98C}
\definecolor{color7}{HTML}{53c653}
\definecolor{color8}{HTML}{39ac39}
\definecolor{color9}{HTML}{2d862d}
\definecolor{color10}{HTML}{206020}
\definecolor{color11}{HTML}{cca300}

\newcommand{\lehou}[1]{\textcolor{red}{[lehou: #1]}}
\newcommand{\leunsure}[1]{[\textcolor{green}{#1}]}
\newcommand{\slongpre}[1]{\textcolor{magenta}{[slongpre: #1]}}
\newcommand{\hwchung}[1]{\textcolor{magenta}{[hwchung: #1]}}
\newcommand{\jasonwei}[1]{\textcolor{orange}{[jasonwei: #1]}}
\newcommand{\jeff}[1]{\textcolor{blue}{[jeff: #1]}}
\newcommand{\dehghani}[1]{\textcolor{green!10!orange}{[dehghani: #1]}}
\newcommand{\checkme}[1]{\textcolor{blue!90!black}{#1}}


\newcommand{\sbt}{\,\begin{picture}(-1,1)(-1,-3)\circle*{3}\end{picture}\ }
\newcommand{\smallbullet}[0]{\sbt\ \ }

\newcommand{\flan}[0]{flan}
\newcommand{\flanmixture}[0]{Muffin}
\newcommand{\palm}[0]{PaLM}
\newcommand{\stmoe}[0]{ST-MoE}
\newcommand{\flanpalm}[0]{Flan-PaLM}
\newcommand{\flanstmoe}[0]{Flan-ST-MoE}
\newcommand{\flanultwo}[0]{Flan-UL2}
\newcommand{\flantfive}[0]{Flan-T5}
\newcommand{\tzeromixture}[0]{T0-SF}
\newcommand{\upalm}[0]{U-PaLM}
\newcommand{\flanupalm}[0]{Flan-U-PaLM}
\newcommand{\contpalm}[0]{cont-PaLM}
\newcommand{\flancontpalm}[0]{Flan-cont-PaLM}
\newcommand{\palmxaxis}[0]{Model scale (\# params)}

\newcommand{\textdavinci}[0]{InstructGPT}
\newcommand{\codedavinci}[0]{Codex}
\newcommand{\greenbold}[1]{\underline{\textbf{\jweigreen{\normalsize{#1}}}}}
\newcommand{\bluegain}[1]{\textbf{\frenchblue{(+#1)}}}
\usepackage{minitoc}
\usepackage{float}
\renewcommand \thepart{}
\renewcommand \partname{}
\newlength\savewidth\newcommand\shline{\noalign{\global\savewidth\arrayrulewidth
  \global\arrayrulewidth 1pt}\hline\noalign{\global\arrayrulewidth\savewidth}}
\newcommand{\tablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\footnotesize}
\newcommand{\normaltablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\normalsize}
\newcommand{\scripttablestyle}[2]{\setlength{\tabcolsep}{#1}\renewcommand{\arraystretch}{#2}\centering\scriptsize}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \title{
\textbf{
Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond}
}

\author{
\large{}
Jinze Bai \hspace{6mm} Shuai Bai \hspace{6mm} Shusheng Yang \hspace{6mm} Shijie Wang \hspace{6mm} Sinan Tan\\
Peng Wang \hspace{6mm} Junyang Lin \hspace{6mm} Chang Zhou \hspace{6mm} Jingren Zhou
\\
\large{}
Alibaba Group
\\
\small{}
Code \& Demo \& Models: \ \ \url{https://github.com/QwenLM/Qwen-VL}
}

\date{}

\begin{document}

\doparttoc \faketableofcontents 

\maketitle

\begin{abstract}
\noindent
In this work, we introduce the Qwen-VL series, a set of large-scale vision-language models (LVLMs) designed to perceive and understand both texts and images.
Starting from the Qwen-LM as a foundation, we endow it with visual capacity by the meticulously designed \text{(i) visual receptor}, \text{(ii) input-output interface}, \text{(iii) 3-stage training pipeline}, and \text{(iv) multilingual multimodal cleaned corpus}.
Beyond the conventional image description and question-answering, we implement the grounding and text-reading ability of Qwen-VLs by aligning image-caption-box tuples.
The resulting models, including Qwen-VL and Qwen-VL-Chat, set new records for generalist models under similar model scales on a broad range of visual-centric benchmarks (\emph{e.g.}, image captioning, question answering, visual grounding) and different settings (\emph{e.g.}, zero-shot, few-shot).
Moreover, on real-world dialog benchmarks, our instruction-tuned Qwen-VL-Chat also demonstrates superiority compared to existing vision-language chatbots.
All models are public to facilitate future research.
\end{abstract}

{\let\thefootnote\relax\footnotetext{Equal contribution, Corresponding author}}




\begin{figure*}[h]
\centering
\includegraphics[width= 8cm]{images/radar1.pdf}
   \caption{Qwen-VL achieves state-of-the-art performance on a broad range of tasks compared with other generalist models.}
\label{radar}
\end{figure*}

\begin{figure*}[t]
\centering
\includegraphics[width= 1\textwidth]{images/demo2.jpg}
\caption{Some qualitative examples generated by our Qwen-VL-Chat. Qwen-VL-Chat supports multiple image inputs, multi-round dialogue, multilingual conversation, text-reading, localization, fine-grained recognition and understanding ability.}
\label{example}
\end{figure*}

\section{Introduction}

Recently, Large Language Models (LLMs)~\citep{gpt3,gpt4,anil2023palm,gao2023llama,qwen7b} have attracted wide attention due to their powerful capabilities in text generation and comprehension. These models can be further aligned with user intent through fine-tuning instructions, showcasing strong interactive capabilities and the potential to enhance productivity as intelligent assistants.
However, native large language models only live in the pure-text world, lacking the ability to handle other common modalities (such as images, speech, and videos), resulting in great restrictions on their application scope.
Motivated by this, a group of Large Vision Language Models (LVLMs)~\citep{alayrac2022flamingo,chen2022pali, blip2,dai2023instructblip,kosmos,kosmos2,zhu2023minigpt,liu2023visual,ye2023mplug,mPLUG-DocOwl,shikra,li2023otter,videollama,emu,gpt4} have been developed to enhance large language models with the ability to perceive and understand visual signals. These large-scale vision-language models demonstrate promising potential in solving real-world vision-central problems.




Nevertheless, despite that lots of works have been conducted to explore the limitation and potency of LVLMs, current open-source LVLMs always suffer from inadequate training and optimization, thus lag far behind the proprietary models \citep{chen2022pali, chen2023pali, gpt4}, which hinders further exploration and application of LVLMs in open-source community.
What's more, as real-world visual scenarios are quite complicated, fine-grained visual understanding plays a crucial role for LVLMs to assist people effectively and precisely.
But only a few attempts had been made toward this direction \citep{kosmos2, shikra}, the majority of open-source LVLMs remain perceiving the image in a coarse-grained approach and lacking the ability to execute fine-grained perception such as object grounding or text reading.

In this paper, we explore a way out and present the newest members of the open-sourced Qwen families: Qwen-VL series.
Qwen-VLs are a series of highly performant and versatile vision-language foundation models based on Qwen-7B \citep{qwen7b} language model.
We empower the LLM basement with visual capacity by introducing a new visual receptor including a language-aligned visual encoder and a position-aware adapter.
The overall model architecture as well as the input-output interface are quite concise and we elaboratedly design a 3-stage training pipeline to optimize the whole model upon a vast collection of image-text corpus.

Our pre-trained checkpoint, termed Qwen-VL, is capable of perceiving and understanding visual inputs, generating desired responses according to given prompts, and accomplishing various vision-language tasks such as image captioning, question answering, text-oriented question answering, and visual grounding.
Qwen-VL-Chat is the instruction-tuned vision-language chatbot based on Qwen-VL.
As shown in Fig.~\ref{example}, Qwen-VL-Chat is able to interact with users and perceive the input images following the intention of users.

Specifically, the features of the Qwen-VL series models include:
\begin{itemize}

\item Leading performance: Qwen-VLs achieve top-tier accuracy on a vast of vision-centric understanding benchmarks compared to counterparts with similar scales. Besides, Qwen-VL's stuning performance covers not only the conventional benchmarks \emph{e.g.}, captioning, question-answering, grounding), but also some recently introduced dialogue benchmarks.

\item Multi-lingual: Similar to Qwen-LM, Qwen-VLs are trained upon multilingual image-text data with a considerable amount of corpus being in English and Chinese. In this way, Qwen-VLs naturally support English, Chinese, and multilingual instructions.

\item Multi-image: In the training phase, we allow arbitrary interleaved image-text data as Qwen-VL's inputs. This feature allows our Qwen-Chat-VL to compare, understand, and analyze the context when multiple images are given.

\item Fine-grained visual understanding: Thanks to the higher-resolution input size and fine-grained corpus we used in training, Qwen-VLs exhibit highly competitive fine-grained visual understanding ability. Compared to existing vision-language generalists, our Qwen-VLs possess much better grounding, text-reading, text-oriented question answering, and fine-grained dialog performance.
\end{itemize}

\section{Methodology}

\subsection{Model Architecture}
The overall network architecture of Qwen-VL consists of three components and the details of model parameters are shown in Table~\ref{tab:model_parameter}:

\textbf{Large Language Model}: Qwen-VL adopts a large language model as its foundation component. The model is initialized with pre-trained weights from Qwen-7B~\citep{qwen7b}.

 
\textbf{Visual Encoder}: The visual encoder of Qwen-VL uses the Vision Transformer (ViT)~\citep{dosovitskiy2020vit} architecture, initialized with pre-trained weights from Openclip's ViT-bigG~\citep{openclip}. During both training and inference, input images are resized to a specific resolution. The visual encoder processes images by splitting them into patches with a stride of 14, generating a set of image features.

\textbf{Position-aware Vision-Language Adapter}: To alleviate the efficiency issues arising from long image feature sequences, Qwen-VL introduces a vision-language adapter that compresses the image features. This adapter comprises a single-layer cross-attention module initialized randomly. The module uses a group of trainable vectors (Embeddings) as query vectors and the image features from the visual encoder as keys for cross-attention operations. This mechanism compresses the visual feature sequence to a fixed length of 256. The ablation about the number of queries is shown in Appendix \ref{app:n_queries}. Additionally, considering the significance of positional information for fine-grained image comprehension, 2D absolute positional encodings are incorporated into the cross-attention mechanism's query-key pairs to mitigate the potential loss of positional details during compression. The compressed image feature sequence of length 256 is subsequently fed into the large language model.


\begin{table}[h]
    \centering
    \caption{Details of Qwen-VL model parameters.}
    \begin{tabular}{cccc}
         \toprule
         Vision Encoder & VL Adapter & LLM & Total  \\
         \midrule
         1.9B & 0.08B & 7.7B & 9.6B \\
         \bottomrule
    \end{tabular}
    \label{tab:model_parameter}
\end{table}



\begin{figure*}[ht]
\centering
\includegraphics[width= 1\textwidth]{images/qwenvl.pdf}
    \caption{The training pipeline of the Qwen-VL series.}
\label{vl_train}
\end{figure*}

\subsection{Inputs and Outputs}
\textbf{Image Input}: Images are processed through the visual encoder and adapter, yielding fixed-length sequences of image features. To differentiate between image feature input and text feature input, two special tokens (img and /img) are appended to the beginning and end of the image feature sequence respectively, signifying the start and end of image content. 

\textbf{Bounding Box Input and Output}: To enhance the model's capacity for fine-grained visual understanding and grounding, Qwen-VL's training involves data in the form of region descriptions, questions, and detections. Differing from conventional tasks involving image-text descriptions or questions, this task necessitates the model's accurate understanding and generation of region descriptions in a designated format. For any given bounding box, a normalization process is applied (within the range [0, 1000)) and transformed into a specified string format: "". The string is tokenized as text and does not require an additional positional vocabulary. To distinguish between detection strings and regular text strings, two special tokens (box and /box are added at the beginning and end of the bounding box string. Additionally, to appropriately associate bounding boxes with their corresponding descriptive words or sentences, another set of special tokens (ref and /ref) is introduced, marking the content referred to by the bounding box.






\section{Training}
As illustrated in Fig.~\ref{vl_train}, the training process of the Qwen-VL model consists of three stages: two stages of pre-training and a final stage of instruction fine-tuning training.

\subsection{Pre-training}



In the first stage of pre-training, we mainly utilize a large-scale, weakly labeled, web-crawled set of image-text pairs. Our pre-training dataset is composed of several publicly accessible sources and some in-house data. We made an effort to clean the dataset of certain patterns. As summarized in Table~\ref{tab:pretraining_data}, the original dataset contains a total of 5 billion image-text pairs, and after cleaning, 1.4 billion data remain, with 77.3\% English (text) data and 22.7\% Chinese (text) data.

\begin{table}[ht]
    \centering
    \caption{Details of Qwen-VL pre-training data. LAION-en and LAION-zh are the English 
 and Chinese language subset of LAION-5B~\citep{laion5b}. LAION-COCO~\citep{laioncoco} is a synthetic dataset generated from LAION-en. DataComp~\citep{datacomp} and Coyo~\citep{coyo} are collections of image-text pairs. CC12M~\citep{cc12m}, CC3M~\citep{cc3m}, SBU~\citep{sbu} and COCO Caption~\citep{cococaption} are academic caption datasets.} 
    \tablestyle{6pt}{1.1}
    \begin{tabular}{ll ccc}
         \toprule
         \textbf{Language} & \textbf{Dataset} & \textbf{Original} & \textbf{Cleaned} & \textbf{Remaining\%} \\
         \midrule
         \multirow{8}{*}{English} & LAION-en     & 2B       & 280M & 14\% \\
         & LAION-COCO   & 600M     & 300M & 50\% \\
         & DataComp     & 1.4B     & 300M & 21\% \\
         & Coyo         & 700M     & 200M & 28\% \\
         & CC12M        & 12M      & 8M   & 66\% \\
         & CC3M         & 3M       & 3M   & 100\% \\
         & SBU          & 1M       & 0.8M & 80\% \\
         & COCO Caption & 0.6M     & 0.6M & 100\% \\
         \midrule
         \multirow{2}{*}{Chinese} & LAION-zh     & 108M     & 105M & 97\% \\
         & \color{dt}In-house Data & \color{dt}220M & \color{dt}220M & \color{dt}100\% \\
         \midrule
          & Total        & 5B     & 1.4B & 28\% \\
         \bottomrule
    \end{tabular}
    \label{tab:pretraining_data}
\end{table}

We freeze the large language model and only optimize the vision encoder and VL adapter in this stage. The input images are resized to . The training objective is to minimize the cross-entropy of the text tokens. The maximum learning rate is  and the training process uses a batch size of 30720 for the image-text pairs, and the entire first stage of pre-training lasts for 50,000 steps, consuming approximately 1.5 billion image-text samples. More hyperparameters are detailed in Appendix \ref{app:hyperparam} and the convergence curve of this stage is shown in Figure \ref{fig:stage1}.


\subsection{Multi-task Pre-training}



In the second stage of multi-task pre-training, we introduce high-quality and fine-grained VL annotation data with a larger input resolution and interleaved image-text data. As summarized in Table~\ref{tab:multitask_data}, we trained Qwen-VL on 7 tasks simultaneously. For text generation, we use the in-house collected corpus to maintain the LLM's ability. Captioning data is the same with Table~\ref{tab:pretraining_data} except for far fewer samples and excluding LAION-COCO. We use a mixture of publicly available data for the VQA task which includes GQA~\citep{gqa}, VGQA~\citep{vg}, VQAv2~\citep{VQAv2}, DVQA~\citep{dvqa}, OCR-VQA~\citep{ocrvqa} and DocVQA~\citep{docvqa}. We follow Kosmos-2 to use the GRIT~\citep{kosmos2} dataset for the grounding task with minor modifications. For the reference grounding and grounded captioning duality tasks, we construct training samples from GRIT~\citep{kosmos2}, Visual Genome~\citep{vg}, RefCOCO~\citep{refcoco}, RefCOCO+, and RefCOCOg~\citep{refcocog}. In order to improve the text-oriented tasks, we collect pdf and HTML format data from Common Crawl\footnote{\scriptsize\url{https://digitalcorpora.org/corpora/file-corpora/cc-main-2021-31-pdf-untruncated}} and generate synthetic OCR data in English and Chinese language with natural scenery background, following~\citep{synthdog}. Finally, we simply construct interleaved image-text data by packing the same task data into sequences of length 2048.

\begin{table}[ht]
    \centering
    \caption{Details of Qwen-VL multi-task pre-training data. 
    }
    \tablestyle{6pt}{1.1}
    \begin{tabular}{l c l}
         \toprule
         \textbf{Task} & \textbf{\# Samples} & \textbf{Dataset} \\
         \midrule
         Captioning     & 19.7M  & \makecell[l]{LAION-en \& zh, DataComp, Coyo, CC12M \& 3M, SBU, \\ COCO, \color{dt}In-house Data} \\
         VQA            & 3.6M  & \makecell[l]{GQA, VGQA, VQAv2, DVQA, OCR-VQA, DocVQA, \\ TextVQA, ChartQA, AI2D} \\
         Grounding\tablefootnote{This task is to generate noun/phrase grounded captions~\citep{kosmos2}.} & 3.5M  & GRIT \\
         Ref Grounding  & 8.7M  & GRIT, Visual Genome, RefCOCO, RefCOCO+, RefCOCOg \\
         Grounded Cap. & 8.7M  & GRIT, Visual Genome, RefCOCO, RefCOCO+, RefCOCOg \\
         OCR            & 24.8M & SynthDoG-en \& zh, Common Crawl pdf \& HTML \\
         Pure-text Autoregression & 7.8M & \color{dt}In-house Data \\
         \bottomrule
    \end{tabular}
    \label{tab:multitask_data}
\end{table}

We increase the input resolution of the visual encoder from  to , reducing the information loss caused by image down-sampling. Besides, we ablate the window attention and global attention for higher resolutions of the vision transformer in Appendix \ref{app:window_attention}. We unlocked the large language model and trained the whole model. The training objective is the same as the pre-training stage. 


\subsection{Supervised Fine-tuning}
During this stage, we finetuned the Qwen-VL pre-trained model through instruction fine-tuning to enhance its instruction following and dialogue capabilities, resulting in the interactive Qwen-VL-Chat model. The multi-modal instruction tuning data primarily comes from caption data or dialogue data generated through LLM self-instruction, which often only addresses single-image dialogue and reasoning and is limited to image content comprehension. We construct an additional set of dialogue data through manual annotation, model generation, and strategy concatenation to incorporate localization and multi-image comprehension abilities into the Qwen-VL model. We confirm that the model effectively transfers these capabilities to a wider range of languages and question types. Additionally, we mix multi-modal and pure text dialogue data during training to ensure the model's universality in dialogue capabilities. The instruction tuning data amounts to 350k.
In this stage, we freeze the visual encoder and optimize the language model and adapter module. We demonstrate the data format of this stage in Appendix \ref{app:data_format_stage3}.




\section{Evaluation}

In this section, we conduct an overall evaluation on various multi-modal tasks to comprehensively assess our models' visual understanding ability.
In the following, Qwen-VL denotes the model after the multi-task training, and Qwen-VL-Chat denotes the model after supervised fine-tuning (SFT) stage.

Table~\ref{tab:benchmark} provides a detailed summary of the used evaluation benchmarks and corresponding metrics.


\subsection{Image Caption and General Visual Question Answering}

Image caption and general visual question answering (VQA) are two conventional tasks for vision-language models.
Specifically, image caption requires the model to generate a description for a given image and general VQA requires the model to generate an answer for a given image-question pair.


\begin{table}[]
\centering
\caption{Results on Image Captioning and General VQA.}
\scripttablestyle{5pt}{1.05}
\begin{tabular}{@{}l|l|cc|ccccc@{}}
\toprule
\multirow{2}{*}{Model Type} & \multirow{2}{*}{Model} & \multicolumn{2}{c|}{Image Caption} & \multicolumn{5}{c}{General VQA} \\
 &  & \begin{tabular}[c]{@{}c@{}}Nocaps\\ (0-shot)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Flickr30K\\ (0-shot)\end{tabular} & VQAv2 & OKVQA & GQA & \begin{tabular}[c]{@{}c@{}}SciQA-Img\\ (0-shot)\end{tabular} & \begin{tabular}[c]{@{}c@{}}VizWiz\\ (0-shot)\end{tabular} \\ \midrule
\multirow{10}{*}{\begin{tabular}[c]{@{}l@{}}Generalist \\ Models\end{tabular}} & Flamingo-9B & - & 61.5 & 51.8 & 44.7 & - & - & 28.8 \\
 & Flamingo-80B & - & 67.2 & 56.3 & 50.6 & - & - & 31.6 \\
 & Unified-IO-XL & 100.0 & - & 77.9 & 54.0 & - & - & - \\
 & Kosmos-1 & - & 67.1 & 51.0 & - & - & - & 29.2 \\
 & Kosmos-2 & - & 80.5 & 51.1 & - & - & - & - \\
 & BLIP-2 (Vicuna-13B) & 103.9 & 71.6 & 65.0 & 45.9 & 32.3 & 61.0 & 19.6 \\
 & InstructBLIP (Vicuna-13B) & \textbf{121.9} & 82.8 & - & - & 49.5 & 63.1 & 33.4 \\
 & Shikra (Vicuna-13B) & - & 73.9 & 77.36 & 47.16 & - & - & - \\
 & \textbf{Qwen-VL (Qwen-7B)} & 121.4 & \textbf{85.8} & \textbf{79.5} & \textbf{58.6} & \textbf{59.3} & 67.1 & 35.2 \\
 & \textbf{Qwen-VL-Chat} & 120.2 & 81.0 & 78.2 & 56.6 & 57.5 & \textbf{68.2} & \textbf{38.9} \\ \midrule
\begin{tabular}[c]{@{}l@{}}\color{dt}Specialist\\ \color{dt}SOTAs\end{tabular} & \multicolumn{1}{c|}{-} & \begin{tabular}[c]{@{}c@{}}\color{dt}127.0\\ \color{dt}(PALI-17B)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\color{dt}84.5\\ \color{dt}(InstructBLIP\\ \color{dt}-FlanT5-XL)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\color{dt}86.1\\ \color{dt}(PALI-X\\ \color{dt}-55B)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\color{dt}66.1\\ \color{dt}(PALI-X\\ \color{dt}-55B)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\color{dt}72.1\\ \color{dt}(CFR)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\color{dt}92.53\\ \color{dt}(LLaVa+\\ \color{dt}GPT-4)\end{tabular} & \begin{tabular}[c]{@{}c@{}}\color{dt}70.9\\ \color{dt}(PALI-X\\ \color{dt}-55B)\end{tabular} \\ \bottomrule
\end{tabular}
\label{tab:caption_vqa}
\end{table}



For the image caption task, we choose Nocaps \citep{agrawal2019nocaps} and Flickr30K \citep{young2014image_flickr30k} as benchmarks and report CIDEr score~\citep{vedantam2015cider} as metric.
We utilize greedy search for caption generation with a prompt of \textit{"Descripe the image in English:"}.

For general VQA, we utilize five benchmarks including VQAv2~\citep{VQAv2}, OKVQA~\citep{marino2019ok_okvqa}, GQA~\citep{gqa}, ScienceQA (Image Set)~\citep{lu2022learn_scienceqa} and VizWiz VQA~\citep{gurari2018vizwiz}.
For VQAv2, OKVQA, GQA and VizWiz VQA, we employ open-ended answer generation with greedy decoding strategy and a prompt of \textit{"\{question\} Answer:"}, without any constrain on model's output space.
However, for ScienceQA, we constrain the model's output to possible options (instead of open-ended), choose the option with highest confidence as model's prediction, and report the Top- accuracy.

The overall performance on image caption and general VQA tasks are reported in Table \ref{tab:caption_vqa}.  As the results shown, our Qwen-VL and Qwen-VL-Chat both achieve obviously better results compared to previous generalist models in terms of both two tasks.
Specifically, on zero-shot image caption task, Qwen-VL achieves state-of-the-art performance (\emph{i.e}., 85.8 CIDEr score) on the Flickr30K karpathy-test split, even outperforms previous generalist models with much more parameters (\emph{e.g}., Flamingo-80B with 80B parameters).

On general VQA benchmarks, our models also exhibit distinct advantages compared to others. On VQAv2, OKVQA and GQA benchmarks, Qwen-VL achieves 79.5, 58.6 and 59.3 accuracy respectively, which surpasses recent proposed LVLMs by a large margin.
It's worth noting that Qwen-VL also shows strong zero-shot performance on ScienceQA and VizWiz datasets.

\subsection{Text-oriented Visual Question Answering}

Text-oriented visual understanding has a broad application prospect in real-world scenarios. We assess our models' ability toward text-oriented visual question answering on several benchmarks including TextVQA~\citep{sidorov2020textcaps}, DocVQA~\citep{docvqa}, ChartQA~\citep{masry2022chartqa}, AI2Diagram~\citep{kembhavi2016diagram}, and OCR-VQA~\citep{ocrvqa}.
Similarly, the results are shown in Table~\ref{tab:text_vqa}. Compared to previous generalist models and recent LVLMs, our models show better performance on most benchmarks, frequently by a large margin.

\begin{table}[]
\centering
\tablestyle{4pt}{1.05}
\caption{Results on Text-oriented VQA.}
\begin{tabular}{@{}l|l|ccccc@{}}
\toprule
Model type & Model & TextVQA & DocVQA & ChartQA & AI2D & OCR-VQA \\ \midrule
\multirow{5}{*}{Generalist Models} & BLIP-2 (Vicuna-13B) & 42.4 & - & - & - & - \\
 & InstructBLIP (Vicuna-13B) & 50.7 & - & - & - & - \\
 & mPLUG-DocOwl (LLaMA-7B) & 52.6 & 62.2 & 57.4 & - & - \\
 & Pix2Struct-Large (1.3B) & - & \textbf{76.6} & 58.6 & 42.1 & 71.3 \\
 & \textbf{Qwen-VL (Qwen-7B)} & \textbf{63.8} & 65.1 & 65.7 & \textbf{62.3} & \textbf{75.7} \\
 & \textbf{Qwen-VL-Chat} & 61.5 & 62.6 & \textbf{66.3} & 57.7 & 70.5 \\
 \midrule
\color{dt}Specialist SOTAs & \begin{tabular}[c]{@{}l@{}}\color{dt}PALI-X-55B (Single-task fine-\\\color{dt}tuning, without OCR Pipeline)\end{tabular} & \color{dt}71.44 & \color{dt}80.0 & \color{dt}70.0 & \color{dt}81.2 & \color{dt}75.0 \\ \bottomrule
\end{tabular}
\label{tab:text_vqa}
\end{table}

\subsection{Refer Expression Comprehension}

We show our models' fine-grained image understanding and localization ability by evaluating on a sort of refer expression comprehension benchmarks such as RefCOCO~\citep{refcoco}, RefCOCOg~\citep{refcocog}, RefCOCO+~\citep{refcocog} and GRIT~\citep{gupta2022grit}.
Specifically, the refer expression comprehension task requires the model to localize the target object under the guidance of a description.
The results are shown in Table~\ref{tab:grounding}.
Compared to previous generalist models or recent LVLMs, our models obtain top-tier results on all benchmarks.
 

\begin{table}[]
\centering
\caption{Results on Referring Expression Comprehension task.}
\tablestyle{3pt}{1.05}
\begin{tabular}{@{}l|l|ccccccccc@{}}
\toprule
\multirow{2}{*}{Model type} & \multirow{2}{*}{Model} & \multicolumn{3}{c}{RefCOCO} & \multicolumn{3}{c}{RefCOCO+} & \multicolumn{2}{c}{RefCOCOg} & GRIT \\
 &  & val & test-A & test-B & val & test-A & test-B & val & test & refexp \\ \midrule
\multirow{8}{*}{Generalist Models} & GPV-2 & - & - & - & - & - & - & - & - & 51.50 \\
 & OFA-L* & 79.96 & 83.67 & 76.39 & 68.29 & 76.00 & 61.75 & 67.57 & 67.58 & 61.70 \\
 & Unified-IO & - & - & - & - & - & - & - & - & \textbf{78.61} \\
 & VisionLLM-H &  & 86.70 & - & - & - & - & - & - & - \\
 & Shikra-7B & 87.01 & 90.61 & 80.24 & 81.60 & 87.36 & 72.12 & 82.27 & 82.19 & 69.34 \\
 & Shikra-13B & 87.83 & 91.11 & 81.81 & 82.89 & 87.79 & 74.41 & 82.64 & 83.16 & 69.03 \\
 & \textbf{Qwen-VL-7B} & \textbf{89.36} & 92.26 & \textbf{85.34} & \textbf{83.12} & 88.25 & \textbf{77.21} & 85.58 & 85.48 & 78.22 \\
 & \textbf{Qwen-VL-7B-Chat} & 88.55 & \textbf{92.27} & 84.51 & 82.82 & \textbf{88.59} & 76.79 & \textbf{85.96} & \textbf{86.32} & - \\ \midrule
\multirow{3}{*}{\color{dt}Specialist SOTAs} & \color{dt}G-DINO-L & \color{dt}90.56 & \color{dt}93.19 & \color{dt}88.24 & \color{dt}82.75 & \color{dt}88.95 & \color{dt}75.92 & \color{dt}86.13 & \color{dt}87.02 & \color{dt}- \\
 & \color{dt}UNINEXT-H & \color{dt}92.64 & \color{dt}94.33 & \color{dt}91.46 & \color{dt}85.24 & \color{dt}89.63 & \color{dt}79.79 & \color{dt}88.73 & \color{dt}89.37 & \color{dt}- \\
 & \color{dt}ONE-PEACE & \color{dt}92.58 & \color{dt}94.18 & \color{dt}89.26 & \color{dt}88.77 & \color{dt}92.21 & \color{dt}83.23 & \color{dt}89.22 & \color{dt}89.27 & \color{dt}- \\
 \bottomrule
\end{tabular}
\label{tab:grounding}
\end{table}


\subsection{Few-shot Learning on Vision-Language Tasks}
Our model also exhibits satisfactory in-context learning (\emph{a.k.a.}, few-shot learning) ability. 
As shown in Figure~\ref{fig:fewshot}, Qwen-VL achieves better performance through in-context few-shot learning on OKVQA~\citep{marino2019ok_okvqa}, Vizwiz~\citep{gurari2018vizwiz}, TextVQA~\citep{sidorov2020textcaps}, and Flickr30k~\citep{young2014image_flickr30k} when compared with models with similar number of parameters (Flamingo-9B\citep{alayrac2022flamingo}, OpenFlamingo-9B\citep{awadalla2023openflamingo} and IDEFICS-9B\cite{lauren√ßon2023obelics}). Qwen-VL's performance is even comparable with much larger models (Flamingo-80B and IDEFICS-80B).
Note that we adopt na\"ive random sample to construct the few-shot exemplars, sophisticated few-shot exemplar construction methods such as RICES~\citep{rices} are not used despite better results would be achieved.


\begin{figure*}[ht]
\centering
    \includegraphics[width= 1\textwidth]{images/figure_fewshot_compare.png}
   \caption{Few-shot learning results of Qwen-VL in comparison with other models.}
\label{fig:fewshot}
\end{figure*}



\subsection{Instruction Following in Real-world User Behavior}

In addition to previous conventional vision-language evaluations, to evaluate our Qwen-VL-Chat model's capacity under real-world user behavior, we further conduct the evaluations on the TouchStone~\citep{touchstone}, SEED-Bench~\citep{li2023seedbench}, and MME~\citep{fu2023mme}.
TouchStone is an open-ended vision-language instruction-following benchmark.
We compare the instruction-following ability of Qwen-VL-Chat with other instruction-tuned LVLMs in both English and Chinese on the TouchStone benchmark. SEED-Bench consists of 19K multiple-choice questions with accurate human annotations for evaluating Multimodal LLMs, covering 12 evaluation dimensions including both the spatial and temporal understanding. MME measures both perception and cognition abilities on a total of 14 subtasks.

The results on three benchmarks are shown in Table~\ref{tab:intruction_following}.  Qwen-VL-Chat has achieved obvious advantages over other LVLMs on all three datasets, indicating that our model performs better in understanding and answering diverse user instructions. In SEED-Bench, we have found that our model's visual capabilities can be effectively transferred to video tasks by simply sampling four frames.
 In terms of the overall scores presented in TouchStone, our model demonstrates a clear advantage compared to other LVLMs, especially in terms of its Chinese capabilities. In terms of the broad categories of abilities, our model exhibits a more pronounced advantage in understanding and recognition, particularly in areas such as text recognition and chart analysis. For more detailed information, please refer to the TouchStone dataset.

 

\begin{table}[]
\centering
\tablestyle{5pt}{1.05}
\caption{Results on Instruction-following benchmarks.}
\begin{tabular}{@{}l|ccccccc@{}}
\toprule
\multirow{2}{*}{Model} & \multicolumn{2}{c}{TouchStone} & \multicolumn{3}{c}{SEED-Bench} & \multicolumn{2}{c}{MME}   \\
 &  En&Cn & All & Img &Video & Perception & Cognition \\ \midrule
 VisualGLM & - & 247.1 & - & - & - & 705.31  & 181.79 \\
 PandaGPT & 488.5 & - & - & -&-&642.59 &228.57  \\
 MiniGPT4 & 531.7 &- & 42.8 & 47.4 & 29.9 & 581.67&144.29  \\
 InstructBLIP&552.4 & - & 53.4 & 58.8 & 38.1 & 1212.82&291.79\\
 LLaMA-AdapterV2 &590.1 & - &32.7 & 35.2& 25.8 & 972.67 &248.93\\
 LLaVA  & 602.7 &- & 33.5 & 37.0 & 23.8 & 502.82&214.64 \\
 mPLUG-Owl & 605.4 & - & 34.0 & 37.9 & 23.0 & 967.34 & 276.07 \\ \midrule
 \textbf{Qwen-VL} & - & - & 56.3 & 62.3 &  \textbf{39.1} &  - & -   \\
 \textbf{Qwen-VL-Chat} & \textbf{645.2} & \textbf{401.2} & \textbf{58.2} & \textbf{65.4} & 37.8 &  \textbf{1487.58} & \textbf{360.71}  \\
 \bottomrule
\end{tabular}
\vspace{-0.2cm}
\label{tab:intruction_following}
\end{table}


\section{Related Work}
In recent years, researchers have shown considerable interest in vision-language learning~\citep{vlbert,uniter,oscar,vinvl,unimo,m6,vilt,fiber,xvlm,albef,blip}, especially in the development of multi-task generalist models~\citep{unit,flava,uni-perceiver,coca,wang2022ofa,unified_io,bai2022ofasys}.
CoCa~\citep{coca} proposes an encoder-decoder structure to address image-text retrieval and vision-language generation tasks simultaneously. 
OFA~\citep{wang2022ofa} transforms specific vision-language tasks into sequence-to-sequence tasks using customized task instructions. 
Unified I/O~\citep{unified_io} further introduces more tasks like segmentation and depth estimation into a unified framework.
Another category of research focuses on building vision-language representation models~\citep{clip,align,lit,florence,chinese_clip}.
CLIP~\citep{clip} leverages contrastive learning and large amounts of data to align images and language in a semantic space, resulting in strong generalization capabilities across a wide range of downstream tasks. 
BEIT-3~\citep{beit3} employs a mixture-of-experts (MOE) structure and unified masked token prediction objective, achieving state-of-the-art results on various visual-language tasks. 
In addition to vision-language learning, ImageBind~\citep{imagebind} and ONE-PEACE~\citep{one-peace} align more modalities such as speech into a unified semantic space, thus creating more general representation models.

Despite achieving significant progress, previous vision-language models still have several limitations such as poor robustness in instruction following, limited generalization capabilities in unseen tasks, and a lack of in-context abilities. With the rapid development of large language models (LLMs)~\citep{gpt3,gpt4,anil2023palm,gao2023llama,qwen7b}, researchers have started building more powerful large vision-language models (LVLMs) based on LLMs~\citep{alayrac2022flamingo,chen2022pali, blip2,dai2023instructblip,kosmos,kosmos2,zhu2023minigpt,liu2023visual,ye2023mplug,mPLUG-DocOwl,shikra,li2023otter,videollama,emu}. 
BLIP-2~\citep{blip2} proposes Q-Former to align the frozen vision foundation models and LLMs.
Meanwhile, LLAVA~\citep{liu2023visual} and Mini-GPT4~\citep{zhu2023minigpt} introduce visual instruction tuning to enhance instruction following capabilities in LVLMs. 
Additionally, mPLUG-DocOwl~\citep{mPLUG-DocOwl} incorporates document understanding capabilities into LVLMs by introducing digital documents data. 
Kosmos2~\citep{kosmos2}, Shikra~\citep{shikra}, and BuboGPT~\citep{bubogpt} further enhance LVLMs with visual grounding abilities, enabling region description and localization.
In this work, we integrate image captioning, visual question answering, OCR, document understanding, and visual grounding capabilities into Qwen-VL. The resulting model achieves outstanding performance on these diverse style tasks.

\vspace{-0.2cm}
\section{Conclusion and Future Work}
We release the Qwen-VL series, a set of large-scale multilingual vision-language models that aims to facilitate multimodal research. Qwen-VL outperforms similar models across various benchmarks, supporting multilingual conversations, multi-image interleaved conversations, grounding in Chinese, and fine-grained recognition. 
Moving forward, we are dedicated to further enhancing Qwen-VL's capabilities in several key dimensions: 

\begin{itemize}
\item Integrating Qwen-VL with more modalities, such as speech and video. 
\item Augmenting Qwen-VL by scaling up the model size, training data and higher resolution, enabling it to handle more complex and intricate relationships within multimodal data.
\item Expanding Qwen-VL's prowess in multi-modal generation, specifically in generating high-fidelity images and fluent speech.
\end{itemize}


\bibliographystyle{plainnat} 
\bibliography{references}





\clearpage
\newpage

\appendix
\section{Dataset details}
\label{app:dataset}



\subsection{Image-text pairs}
We use web-crawled image-text pairs dataset for pre-training, which includes LAION-en~\citep{laion5b}, LAION-zh~\citep{laion5b}, LAION-COCO~\citep{laioncoco}, DataComp~\citep{datacomp} and Coyo~\citep{coyo}. We clean these noisy data by several steps:
\begin{enumerate}
    \item Removing pairs with too large aspect ratio of the image
    \item Removing pairs with too small image
    \item Removing pairs with a harsh CLIP score (dataset-specific)
    \item Removing pairs with text containing non-English or non-Chinese characters
    \item Removing pairs with text containing emoji characters
    \item Removing pairs with text length too short or too long
    \item Cleaning the text's HTML-tagged part
    \item Cleaning the text with certain unregular patterns
\end{enumerate}

For academic caption datasets, we remove pairs whose text contains the special tags in CC12M~\citep{cc12m} and SBU~\citep{sbu}. If there is more than one text matching the same image, we select the longest one.

\subsection{VQA}
For the VQAv2~\citep{VQAv2} dataset, we select the answer annotation based on the maximum confidence. For other VQA datasets, we didn't do anything special.

\subsection{Grounding}
For the GRIT~\citep{kosmos2} dataset, we found that there are many recursive grounding box labels in one caption. We use the greedy algorithm to clean the caption to make sure each image contains the most box labels with no recursive box labels. For other grounding datasets, we simply concatenate the noun/phrase with respective bounding box coordinates.

\subsection{OCR}

We generated the synthetic OCR dataset using Synthdog~\citep{synthdog}. Specifically, we use the COCO~\citep{lin2014microsoft} train2017 and unlabeld2017 dataset split as the natural scenery background. Then we selected 41 English fonts and 11 Chinese fonts to generate text. We use the default hyperparameters as in Synthdog. We track the generated text locations in the image and convert them to quadrilateral coordinates and we also use these coordinates as training labels. The visualization example is illustrated in the second row of Fig~\ref{ocr_vis}.

\begin{figure*}[htp]
\centering
\includegraphics[width= 1\textwidth]{images/dataset/Dataset_Visualization_1.png}
\includegraphics[width= 0.8\textwidth]{images/dataset/Dataset_Visualization_2.png}
\includegraphics[width= 1\textwidth]{images/dataset/Dataset_Visualization_3_New.png}
   \caption{Visualization of the Grounding and OCR data used for training Qwen-VL}
\label{ocr_vis}
\end{figure*}

For all the PDF data we collected, we follow the steps below to pre-process the data using PyMuPDF~\citep{pymupdf} to get the rendering results of each page in a PDF file as well as all the text annotations with their bounding boxes.
\begin{enumerate}
    \item Extracting all texts and their bounding boxes for each page.
    \item Rendering each page and save them as an image file.
    \item Removing too small image.
    \item Removing images with too many or too few characters.
    \item Removing images containing Unicode characters in the ``Latin Extended-A'' and ``Latin Extended-B'' blocks.
    \item Removing images containing Unicode characters in the ``Private Use Area (PUA)'' block.
\end{enumerate}

For all HTML web pages we collected, we pre-process them in a similar approach to all the PDF data we collected, but we use Puppeteer~\citep{puppeteer} instead of PyMuPDF to render these HTML pages and get the ground truth annotation. We follow the steps below to pre-process the data.
\begin{enumerate}
    \item Extracting all texts for each webpage.
    \item Rendering each page and save them as an image file.
    \item Removing too small image.
    \item Removing images with too many or too few characters.
    \item Removing images containing Unicode characters in the ``Private Use Area (PUA)'' block.
\end{enumerate}

\section{Data Format Details of Training}

\subsection{Data Format of Multi-Task Pre-training}
\label{app:data_format_stage2}

We visualize the Multi-Task Pre-training data format in Box~\ref{mt_format}. The Box contains all 7 tasks with the black-colored text as the prefix sequence without loss and blue-colored text as the ground truth labels with loss.

\begin{tcolorbox}[colback=black!5!white,colframe=black!75!black,title=Image Captioning]
imgcc3m/01581435.jpg/imgGenerate the caption in English: \textcolor{blue}{the beautiful flowers for design.eos}
\tcbsubtitle{Vision Question Answering}
imgVG\_100K\_2/1.jpg/img Does the bandage have a different color than the wrist band? Answer: \textcolor{blue}{No, both the bandage and the wrist band are white.eos}
\tcbsubtitle{OCR VQA}
imgocr\_vqa/1.jpg/img What is the title of this book? Answer: \textcolor{blue}{Asi Se Dice!, Volume 2: Workbook And Audio Activities (Glencoe Spanish) (Spanish Edition)}eos
\tcbsubtitle{Caption with Grounding}
imgcoyo700m/1.jpg/imgGenerate the caption in English with grounding: \textcolor{blue}{Beautiful shot of refbees/refbox(661,612),(833,812)/boxbox(120,555),(265,770) /box gathering nectars from refan apricot flower/refbox(224,13),(399,313) /boxeos}
\tcbsubtitle{Referring Grounding}
imgVG\_100K\_2/3.jpg/imgrefthe ear on a giraffe/ref\textcolor{blue}{box(176,106),(232,160) /boxeos}
\tcbsubtitle{Grounded Captioning}
imgVG\_100K\_2/4.jpg/imgrefThis/refbox(360,542),(476,705)/box is \textcolor{blue}{Yellow cross country ski racing gloveseos}
\tcbsubtitle{OCR}
imgsynthdog/1.jpg/imgOCR with grounding: \textcolor{blue}{refIt is managed/ref quad (568,121), (625,131), (624,182), (567,172)/quad...eos}
\label{mt_format}
\end{tcolorbox}

\subsection{Data Format of Supervised Fine-tuning}
\label{app:data_format_stage3}

To better accommodate multi-image dialogue and multiple image inputs, we add the string "Picture :" before different images, where the  corresponds to the order of image input dialogue. In terms of dialogue format, we construct our instruction tuning dataset using the ChatML~\citep{chatml} format, where each interaction's statement is marked with two special tokens (im\_start and im\_end) to facilitate dialogue termination.

\begin{tcolorbox}[colback=black!5!white,colframe=black!75!black,title=The Dataset Format Example of ChatML]
\textcolor{blue}{im\_start}user

Picture 1: imgvg/VG\_100K\_2/649.jpg/imgWhat is the sign in the picture?\textcolor{blue}{im\_end}

\textcolor{blue}{im\_start}assistant

\textcolor{blue}{The sign is a road closure with an orange rhombus.im\_end}

\textcolor{blue}{im\_start}user

How is the weather in the picture?\textcolor{blue}{im\_end}

\textcolor{blue}{im\_start}assistant

\textcolor{blue}{The shape of the road closure sign is an orange rhombus.im\_end}
\end{tcolorbox}

During training, we ensure the consistency between prediction and training distributions by only supervising answers and special tokens (blue in the example), and not supervising role names or question prompts.  

\section{Hyperparameters}
\label{app:hyperparam}
We report the detailed training hyperparameter settings of Qwen-VL in Table~\ref{tab:hyperparam}.

\begin{table}[htbp]
    \centering
    \tablestyle{7pt}{1.3}
    \caption{Training hyperparameters of Qwen-VL}
    \begin{tabular}{l ccc}
         \toprule
         Configuration            & Pre-training & Multi-task Pre-training & Supervised Fine-tuning \\
         \midrule
         ViT init.                & Open-CLIP-bigG & Qwen-VL 1st-stage & Qwen-VL 2nd-stage \\
         LLM init.                & Qwen-7B & Qwen-7B & Qwen-VL 2nd-stage \\
         VL Adapter init.         & random & Qwen-VL 1st-stage & Qwen-VL 2nd-stage \\
         Image resolution         &  &  &  \\
         ViT sequence length      & 256 & 1024 & 1024 \\
         LLM sequence length      & 512 & 2048 & 2048\\
         Learnable query numbers  & 256 & 256 & 256\\
         Optimizer                & \multicolumn{3}{c}{AdamW} \\
         Optimizer hyperparameter & \multicolumn{3}{c}{} \\
         Peak learning rate       &  &  &  \\
         Minimum learning rate    &  &  &  \\
         ViT learning rate decay  & 0.95 & 0.95 & 0 \\
         ViT Drop path rate       & \multicolumn{3}{c}{0} \\
         Learning rate schedule   & \multicolumn{3}{c}{cosine decay} \\
         Weight decay             & \multicolumn{3}{c}{0.05} \\
         Gradient clip            & \multicolumn{3}{c}{1.0} \\
         Training steps           & 50k & 19k & 8k \\
         Warm-up steps            & 500 & 400 & 3k \\
         Global batch size        & 30720 & 4096 & 128 \\
         Gradient Acc.            & 6 & 8 & 8 \\
         Numerical precision      & \multicolumn{3}{c}{} \\
         Optimizer sharding       & \multicolumn{3}{c}{\ding{51}} \\
         Activation checkpointing & \multicolumn{3}{c}{\ding{55}} \\
         Model parallelism        & \ding{55} & 2 & 2 \\
         Pipeline parallelism     & \multicolumn{3}{c}{\ding{55}} \\
         \bottomrule
    \end{tabular}
    \label{tab:hyperparam}
\end{table}


In the first pre-training stage, the model is trained using AdamW optimizer with . We use the cosine learning rate schedule and set the maximum learning rate of  and minimum of  with a linear warm-up of 500 steps. We use a weight decay of  and a gradient clipping of . For the ViT image encoder, we apply a layer-wise learning rate decay strategy with a decay factor of . 
The training process uses a batch size of 30720 for the image-text pairs, and the entire first stage of pre-training lasts for 50,000 steps, consuming approximately 1.5 billion image-text samples and 500 billion image-text tokens.

In the second multi-task training stage, we increase the input resolution of the visual encoder from  to , reducing the information loss caused by image down-sampling. We unlocked the large language model and trained the whole model. The training objective is the same as the pre-training stage. We use AdamW optimizer with . We trained for 19000 steps with 400 warm-up steps and a cosine learning rate schedule. Specifically, we use the model parallelism techniques for ViT and LLM.

\section{Summary of the evaluation benchmarks}
\label{app:benchmark}

We provide a detailed summary of the used evaluation benchmarks and corresponding metrics in Table~\ref{tab:benchmark}.

\begin{table}[ht]
    \centering
    \caption{Summary of the evaluation benchmarks.}
    \scripttablestyle{3pt}{1.05}
    \begin{tabular}{l|l|l|l|l}
        \toprule
         Task & Dataset & Description & Split & Metric  \\
         \midrule
         \multirow{2}{*}{Image Caption} & Nocaps & Captioning of natural images & val & CIDEr() \\
         & Flickr30K & Captioning of natural images & karpathy-test & CIDEr() \\
         \midrule
         \multirow{5}{*}{General VQA} & VQAv2 & VQA on natural images & test-dev & VQA Score() \\
         & OKVQA & VQA on natural images requiring outside knowledge & val & VQA Score() \\
         & GQA & VQA on scene understanding and reasoning & test-balanced & EM() \\
         & ScienceQA-Img & Multi-choice VQA on a diverse set of science topics & test & Accuracy() \\
         & VizWiz & VQA on photos taken by people who are blind & test-dev & VQA Score()\\
         \midrule
         \multirow{5}{*}{Text-oriented VQA} & TextVQA & VQA on natural images containing text & val & VQA Score() \\
         & DocVQA & VQA on images of scanned documents & test & ANLS() \\
         & ChartQA & VQA on images of charts & test & Relaxed EM()\\
         & OCRVQA & VQA on images of book covers & test & EM() \\
         & AI2Diagram & VQA on images of scientific diagrams & test & EM() \\
         \midrule
         & RefCOCO & Refer grounding on natural images & val \& testA \& testB & Accuracy() \\
         Refer Expression & RefCOCO+ & Refer grounding on natural images & val \& testA \& testB & Accuracy() \\
         Comprehension & RefCOCOg & Refer grounding on natural images & val \& test & Accuracy() \\
         & GRiT & Refer grounding on natural images & test & Accuracy() \\
         \midrule
         \multirow{3}{*}{Instruction Following} & TouchStone & Open-ended VL instruction following benchmark & English \& Chinese & GPT- Score () \\
          & MME & Open-ended VL Benchmark by yes/no questions & Perception \& Cognition & Accuracy () \\
          & Seed-Bench & Open-ended VL Benchmark by Multi-choice VQA & Image \& Video & Accuracy () \\
         \bottomrule
    \end{tabular}
    \label{tab:benchmark}
\end{table}


\section{Additional experimental details}

\subsection{Convergence of the Pre-training Stage}
\label{app:first_stage}

In Figure \ref{fig:stage1}, we show the convergence of the Pre-training Stage (stage one). The whole models are trained using BFloat16
mixed precision, the batch size is 30720, and the learning rate is . All images are only trained once (one epoch). The training loss decreases steadily with the increase of the number of training pictures. Note that, the pre-training stage (Stage one) has no VQA data being added, but the Zero-shot VQA score increases amidst fluctuations.

\begin{figure*}[ht]
\centering
\includegraphics[width= 1\textwidth]{images/figure_t.pdf}
   \caption{Visualization of the Convergence of the Pre-training Stage}
\label{fig:stage1}
\end{figure*}

\subsection{Number of Learnable Queries in the Vision-Language Adapter}
\label{app:n_queries}

The vision-language adapter uses cross-attention to compress the visual feature sequence by a set of learning queries of length. Too few queries can lead to the loss of some visual information, while too many queries may reduce in greater convergence difficulty and computational cost.

\begin{figure*}[ht]
\centering
\includegraphics[width= 0.85\textwidth]{images/figure_q.pdf}
   \caption{Visualization of the training loss when using different compressed feature lengths of the vision-language adapter. The left depicts the initial training loss (within 50 steps), and the right depicts the loss in convergence (1k-5k steps). In the legend, L64 denotes that the adapter uses 64 queries to compress the visual feature sequence to a fixed length of 64, and so on. The loss curves have been smoothed to avoid shading owing to fluctuations.}
\label{fig:ablation_adapter}
\end{figure*}

An ablation experiment is conducted on the number of learnable queries in the vision-language adapter. We used ViT-L/14 as the visual encoder and the  resolution picture as input, so the sequence length of ViT's output is . As shown in the left part of Figure \ref{fig:ablation_adapter}, the fewer queries used at the beginning of training, the lower the initial loss. However, with convergence, too many or too few queries will cause convergence to slow down, as shown in the right part of Figure \ref{fig:ablation_adapter}. Considering that the second training stage (Multi-task Pre-train) applies 448*448 resolution, where the sequence length of ViT's output is . Too few queries can result in more information being lost. We finally chose to use 256 queries for the vision-language adapter in Qwen-VL.

\subsection{Window Attention vs Global Attention for Vision Transformer}
\label{app:window_attention}

Using a high-resolution Vision Transformer in the model will significantly increase the computational cost. One possible solution to reduce the computational cost of the model is to use Window Attention in the Vision Transformer, i.e., to perform Attention only in a window of  in most layers of the ViT part of the model, and to perform Attention for the full  or  image in a small number of layers (e.g. 1 out of every 4 layers) of the ViT part of the model.

To this end, we conducted ablation experiments to compare the performance of the model when using Global Attention and Window Attention for ViT. We compare the experimental results for analysing the trade-off between computational efficiency and convergence of the model.

\begin{figure*}[ht]
\centering
\includegraphics[width= 1\textwidth]{images/figure_loss_attention.png}
   \caption{Visualization of the Loss when using Window Attention vs Global Attention}
\label{fig:ablation_attention}
\end{figure*}

\begin{table}[htbp]
    \centering
    \tablestyle{7pt}{1.3}
    \caption{Training speed of Window Attention vs Global Attention for different input image resolutions}
    \begin{tabular}{l c}
         \toprule
         Model input resolution \& Attention type            & Training speed  \\
         \midrule
         , Global Attention        & ~10s / iter \\
         , Window Attention            & ~9s / iter \\
         , Global Attention      & ~60s / iter \\
         , Window Attention       & ~25s / iter \\
         \bottomrule
    \end{tabular}
    \label{tab:ablation_attention}
\end{table}

As shown in Figure \ref{fig:ablation_attention} and Table \ref{tab:ablation_attention}, the loss of the model is significantly higher when Window Attention instead of Vanilla Attention is used. And the training speeds for both of them are similar. Therefore, we decided to use Vanilla Attention instead of Window Attention for the Vision Transformer when training Qwen-VL.

The reason we don't use Window Attention with  resolution is that its training speed is too slow for us. Although it reaches a loss value similar to model with  resolution input at 5000 steps. It takes almost 2.5 times longer to train than the model with  resolution input.


\subsection{Performance on Pure-text Tasks}

In order to study the effect of multi-modal training on pure-text ability, we show the performance of pure-text tasks of Qwen-VL compared to open-source LLM in Table \ref{tab:pure_text}.

Qwen-VL uses an intermediate checkpoint of Qwen-7B as the LLM initialization. The reason why we did not use the final released checkpoint of Qwen-7B is that Qwen-VL and Qwen-7B were developed at a very similar period. Because Qwen-VL has a good initialization on LLM by Qwen-7B, it is comparable to many text-only LLMs on pure-text tasks.

\begin{table}[ht]
\centering
\caption{Performance on Pure-text Benchmarks of Qwen-VL compared to open-source LLM. Due to the introduction of pure-text data in the multi-task training and SFT stage, Qwen-VL do not compromise any pure-text ability.}
\tablestyle{7pt}{1.3}
\begin{tabular}{@{}l|ccc@{}}
\toprule
Model & MMLU & CMMLU & C-Eval \\ \midrule
LLaMA-7B & 35.1 & 26.8 & - \\
LLaMA2-7B & 46.8 & 31.8 & 32.5 \\
Baichuan-7B & 42.3 & 44.4 & 42.8 \\
Baichuan2-7B & 54.2 & 57.1 & 54.0 \\
ChatGLM2-6B & 47.9 & 48.8 & 51.7 \\
InternLM-7B & 51.0 & 51.8 & 52.8 \\ 
Qwen-7B (final released) & 58.2 & 62.2 & 63.5 \\ \midrule
Qwen-7B (intermediate, use as Qwen-VL's LLM initialization) & 49.9 & - & 48.5 \\
Qwen-VL & 50.7 & 49.5 & 51.1 \\ \bottomrule
\end{tabular}
\label{tab:pure_text}
\end{table}

Furthermore, in the multi-task training and SFT stages, Qwen-VL not only utilizes visual and language-related data but also incorporates pure-text data for training. The purpose of this is to prevent the catastrophic forgetting of text comprehension by leveraging the information from pure-text data. The results in Table \ref{tab:pure_text} indicate that the Qwen-VL model does not exhibit any degradation in terms of its pure text capability and even demonstrates improvement after multi-task training.

\end{document}
