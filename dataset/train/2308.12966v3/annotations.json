[{'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'InfiMM-Eval', 'Metric': 'Overall score', 'Score': '37.39'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'InfiMM-Eval', 'Metric': 'Deductive', 'Score': '37.55'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'InfiMM-Eval', 'Metric': 'Abductive', 'Score': '44.39'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'InfiMM-Eval', 'Metric': 'Analogical', 'Score': '30.42'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'InfiMM-Eval', 'Metric': 'Params', 'Score': '16B'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'DocVQA test', 'Metric': 'ANLS', 'Score': '0.9024'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'DocVQA test', 'Metric': 'ANLS', 'Score': '0.651'}}, {'LEADERBOARD': {'Task': 'Visual Question Answering (VQA)', 'Dataset': 'DocVQA test', 'Metric': 'ANLS', 'Score': '0.626'}}, {'LEADERBOARD': {'Task': 'Chart Question Answering', 'Dataset': 'ChartQA', 'Metric': '1:1 Accuracy', 'Score': '66.3'}}, {'LEADERBOARD': {'Task': 'Chart Question Answering', 'Dataset': 'ChartQA', 'Metric': '1:1 Accuracy', 'Score': '65.7'}}]
