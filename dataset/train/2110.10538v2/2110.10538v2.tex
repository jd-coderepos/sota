\documentclass{article}







\usepackage[nonatbib, preprint]{neurips_2021}

\usepackage{wrapfig}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \hypersetup{
    colorlinks=true,
    linkcolor=magenta,
    urlcolor=magenta
    }
\usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{comment}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{wrapfig}    \usepackage{tikz, siunitx}  \usepackage{amsmath}    \usepackage{amssymb}    \usepackage{xspace}
\usepackage{multirow}

\usepackage{subcaption}



\newcommand{\algLabel}{Algorithm\xspace}
\newcommand{\figLabel}{Figure\xspace}
\newcommand{\eqnLabel}{Equation\xspace}
\newcommand{\eqnref}[1]{(\ref{#1})}
\newcommand{\secLabel}{Section\xspace}
\newcommand{\tblLabel}{Table\xspace}
\newcommand{\mysection}[1]{\vspace{3pt}\noindent\textbf{#1.}}
\newcommand{\supp}{\textbf{Appendix}\xspace}

\newcommand{\R}[1]{\textcolor[rgb]{1.00,0.00,0.00}{#1}}
\newcommand{\B}[1]{\textcolor[rgb]{0.00,0.00,1.00}{#1}}

\newcommand{\savespace}{\vspace{0pt}}


\newcommand{\todo}[1]{{\color{red}[TODO: #1]}}
\newcommand{\BG}[1]{{\bf \color{red}[BG: #1]}}
\newcommand{\AT}[1]{{\bf \color{blue}[AT: #1]}}
\newcommand{\GQ}[1]{{\bf \color{cyan}[GQ: #1]}}
\newcommand{\HH}[1]{{\bf \color{purple}[HH: #1]}}
\newcommand{\GL}[1]{{\bf \color{gray}[GL: #1]}}


\usepackage{algorithm}
\usepackage{listings}
\usepackage{etoolbox}
\makeatletter
\AfterEndEnvironment{algorithm}{\let\@algcomment\relax}
\AtEndEnvironment{algorithm}{\kern2pt\hrule\relax\vskip3pt\@algcomment}
\let\@algcomment\relax
\newcommand\algcomment[1]{\def\@algcomment{\footnotesize#1}}
\renewcommand\fs@ruled{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@ruled
  \def\@fs@pre{\hrule height.8pt depth0pt \kern2pt}\def\@fs@post{}\def\@fs@mid{\kern2pt\hrule\kern2pt}\let\@fs@iftopcapt\iftrue}
\makeatother
 

\begin{document}
\title{ASSANet: An Anisotropic Separable Set Abstraction \\ for Efficient Point Cloud Representation Learning
}

\author{Guocheng Qian 
\And
Hasan Abed Al Kader Hammoud 
\And Guohao Li 
\And Ali Thabet 
\AND Bernard Ghanem \\ 
King Abdullah University of Science and Technology (KAUST)\\
\texttt{\{guocheng.qian, hasanabedalkader.hammoud, bernard.ghanem\}@kaust.edu.sa} \\
\url{https://github.com/guochengqian/ASSANet}
}

\maketitle
\begin{abstract}
Access to 3D point cloud representations has been widely facilitated by LiDAR sensors embedded in various mobile devices. This has led to an emerging need for fast and accurate point cloud processing techniques. In this paper, we revisit and dive deeper into PointNet++, one of the most influential yet under-explored networks, and develop faster and more accurate variants of the model. We first present a novel Separable Set Abstraction (SA) module that disentangles the vanilla SA module used in PointNet++ into two separate learning stages: (1) learning channel correlation and (2) learning spatial correlation. The Separable SA module is significantly faster than the vanilla version, yet it achieves comparable performance.  We then introduce a new Anisotropic Reduction function into our Separable SA module and propose an Anisotropic Separable SA (ASSA) module that substantially increases the network's accuracy. We later replace the vanilla SA modules in PointNet++ with the proposed ASSA module, and denote the modified network as ASSANet. Extensive experiments on point cloud classification, semantic segmentation, and part segmentation show that ASSANet outperforms PointNet++ and other methods, achieving much higher accuracy and faster speeds. In particular, ASSANet outperforms PointNet++ by $7.4$ mIoU on S3DIS Area 5, while maintaining $1.6 \times $ faster inference speed on a single NVIDIA 2080Ti GPU. Our scaled ASSANet variant achieves $66.8$ mIoU and outperforms KPConv, while being more than $54 \times$ faster.

\end{abstract}

 \section{Introduction}
Among the various 3D object representations, point clouds have been surging in popularity, becoming one of the most fundamental 3D representations. This popularity stems from the increased availability of 3D sensors, like LiDAR, which produce point clouds as their raw output. The growing presence of point cloud data has been accompanied by the development of many 3D deep learning methods \cite{Qi2017PointNetDH, dgcnn, Li2019DeepGCNs, Thomas2019KPConvFA, Liu2020ACL}. Even though these methods achieve impressive performance, they are generally computationally expensive (\figLabel \ref{fig:acc-latency}). With the integration of LiDAR sensors into hardware-limited devices, such as mobile devices and AR headsets, interest in efficient models for point cloud processing has grown significantly. Given the limited computational power of mobile devices and embedded systems, the design of mobile-friendly point cloud-based algorithms should not only focus on providing good accuracy, but also on maintaining high computational efficiency.

When processing point cloud data, one can always opt to convert the data into representations that are more accessible to deep learning frameworks. Popular options are multi-view methods \cite{Su2015MultiviewCN, Feng2018GVCNNGC, Wei2020ViewGCNVG} and voxel-based methods \cite{Graham20183DSS, Yan2018SECONDSE}. Converting to these representations generally requires additional computation and memory, and can lead to geometric information loss \cite{Liu2019PointVoxelCF}. It is therefore more desirable to operate directly on point clouds. To that extent, we are currently witnessing a surge in point-based methods \cite{Qi2017PointNetDL, Qi2017PointNetDH, dgcnn, Li2019DeepGCNs, Thomas2019KPConvFA, Liu2020ACL}.  
The first of such methods was introduced by Qi \etal through the seminal PointNet \cite{Qi2017PointNetDL} architecture. PointNet operates directly on point clouds, without the need for an intermediate representation. 
Despite its efficiency, PointNet merely learns per-point features individually and discards local information, which restrains its performance. 
As a variant of PointNet, PointNet++ \cite{Qi2017PointNetDH} presents a novel \textit{Set Abstraction} module that sub-samples the point cloud, groups the neighborhood, extracts local information via a set of multi-layer perceptrons (MLPs), and then aggregates the local information by a reduction layer (\ie pooling).  \figLabel \ref{fig:acc-latency} shows how PointNet++ outperforms the pioneering PointNet \cite{Qi2017PointNetDL} by a large margin. 
PointNet++ also obtains better accuracy than the graph-based method DeepGCN \cite{Li2019DeepGCNs}, and does so with a $100\times$ speed gain.
PointNet++ provided a good balance between accuracy and efficiency, and was therefore widely utilized in various tasks like normal estimation \cite{Guerrero2018PCPNetLL}, segmentation \cite{Qi2019DeepHV,Landrieu2018LargeScalePC}, and object detection \cite{Shi2019PointRCNN3O}.
After PointNet++, graph-based \cite{simonovsky2017dynamic, wang2019graph, dgcnn, Li2019DeepGCNs}, pseudo-grid based \cite{Tatarchenko2018TangentCF, PointCNN,mao2019interpolated,Thomas2019KPConvFA} and adaptive weight-based \cite{wang2018deep,Liu2019RelationShapeCN,accv2018/Groh, PointConv}, became the state-of-the-art in point cloud tasks. As shown in \figLabel \ref{fig:acc-latency}, nearly all of these methods improve performance at the cost of speed.
In this work, we focus on designing point cloud networks that are both fast and accurate. 
Inspired by its success, both in terms of the accuracy-speed balance and its wide adoption, we take a deep dive into PointNet++.
We conduct extensive analysis of its architectural design (\secLabel \ref{sec:pointnet++}) and latency decomposition (\figLabel \ref{fig:latency_pointnet++}). Interestingly, we demonstrate that both its efficiency and accuracy can be improved sharply by minimal modifications to the architecture. These modifications lead to a new architecture design that is faster and more accurate than currently available point methods (shown in \protect\tikz[baseline]{\protect\draw[blue, line width=0.5mm,dash dot] (0,.5ex)--++(0.45,0);}  in \figLabel \ref{fig:acc-latency}).

\begin{figure}
  \begin{minipage}[c]{0.40\textwidth}
    \includegraphics[page=1,trim = 8mm 0mm 15mm 0mm, clip, width=\textwidth]{figures/teaser_acc_speed.pdf}
  \end{minipage}\hfill
  \begin{minipage}[c]{0.50\textwidth}
    \caption{
        \textbf{Tradeoffs between accuracy (mIoU on S3DIS Area-5) and inference speed (instances/second)}. Speed is reported as the mean value of $200$ runs on a single GTX 2080Ti GPU. The proposed ASSANet scaled with different widths and depths shown in \protect\tikz[baseline]{\protect\draw[blue, line width=0.5mm,dash dot] (0,.5ex)--++(0.45,0);} outperform the state-of-the-art methods in \protect\tikz[baseline]{\protect\draw[red,fill=red] (0,.5ex) circle (.45ex);} with better accuracies and faster speeds. Refer to \secLabel \ref{sec:ablation_scaling} for details. 
    }
    \label{fig:acc-latency}
  \end{minipage}
  \vspace{-5mm}
\end{figure}


\mysection{Contributions}
\textbf{(1)} We demonstrate that the MLPs performed on the neighborhood features in the Set Abstraction (SA) module of PointNet++ reduce the inference speed. We introduce a new \textbf{separable SA} module that processes on point features directly allowing for a significant improvement in inference speed. 
\textbf{(2)} We discover that all operations for processing neighbors in the SA module are isotropic which limits the performance (accuracy wise). We present a novel \textbf{Anisotropic Reduction} layer that treats each neighbor differently. We then insert Anisotropic Reduction into our Separable SA and propose the Anisotropic Separable Set Abstraction (ASSA) module that greatly increases accuracy.
\textbf{(3)} We present ASSANet by replace the vanilla SA in PointNet++ with the proposed ASSA. ASSANet shows a much higher accuracy and a faster speed compared to PointNet++ and previous methods on various tasks (point cloud classification, semantic segmentation, and part segmentation). We further study two regimes for up-scaling ASSANet. As shown in \figLabel \ref{fig:acc-latency}, \textit {our scaled ASSANet outperforms the previous state-of-the-art with a much faster inference speed}. In particular, scaled ASSANet achieves better accuracy than the graph-based method DeepGCN~\cite{Li2019DeepGCNs} with an increase in speed of $294\times$, the pseudo grid-based method KPConv~\cite{Thomas2019KPConvFA} ($54\times $ faster), the adaptive weight-based method PosPool*(S) ~\cite{Liu2020ACL} ($9\times $ faster), and the efficient 3D method PVCNN \cite{Liu2019PointVoxelCF} ($2 \times$ faster).  
 \section{Related Work}
\label{sec:related}

\mysection{Projection-based methods}
Due to the unstructured nature of point clouds, convolutional neural networks (CNNs) that tend to work impressively well on grid stuctured data (\eg images, texts and videos) fail to apply directly on point clouds. One common solution for processing point clouds is to project them into collections of images (views) \cite{Su2015MultiviewCN, Feng2018GVCNNGC, Wei2020ViewGCNVG} or 3D voxels \cite{Graham20183DSS, Yan2018SECONDSE, Tang2020SearchingE3}. 
Common CNN backbones (using 2D or 3D convolutions) can be subsequently utilized to perform these intermediate representations. Although projection-based methods allow for utilizing the well studied convolutional neural networks to point-cloud applications, they are computationally expensive as they are associated with additional cost of constructing intermediate representations. Moreover, the projection of point clouds causes loss of important geometric information \cite{Liu2019PointVoxelCF}. 


\mysection{Point-based methods}
Pioneering work explored the possibility of processing point clouds directly. Qi \etal proposed PointNet \cite{Qi2017PointNetDL} that leverages point-wise MLPs to extract per point features individually. To better encode locality, Qi \etal further presented Set Abstraction (SA) to aggregate features from the points' neighborhood, and a hierarchical architecture named PointNet++ \cite{Qi2017PointNetDH} that learns multilevel representations and reduces the required computations. After PointNet++, numerous point-based methods considering neighborhood information were proposed. Graph-based methods \cite{simonovsky2017dynamic,landrieu2018large, dgcnn,wang2019graph, Li2019DeepGCNs, Li2021DeepGCNsMG,Qian_2021_CVPR} represent point clouds as graphs and process point clouds with graph neural networks. Pseudo grid-based methods project neighborhood features onto different forms of pseudo grids such as tangent planes \cite{Tatarchenko2018TangentCF}, grid cells \cite{Hua_2018pointwisecnn,xu2018spidercnn, PointCNN,mao2019interpolated,Thomas2019KPConvFA} and spherical grid points \cite{Zhang2019ShellNetEP} which allow convolving with regular kernel weights like CNNs. Adaptive weight-based methods perform weighted neighborhood aggregation by considering the relative positions of the points \cite{wang2018deep,Liu2019RelationShapeCN,accv2018/Groh,Liu2020ACL} or point density \cite{PointConv}. These methods rely either on designing sophisticated and customized modules, which usually require expensive parameter tuning for different applications \cite{Thomas2019KPConvFA, PointCNN, Ma2018ShuffleNetVP}, or on performing expensive graph kernels \cite{dgcnn, Li2019DeepGCNs} that achieve better performance than PointNet and PointNet++ at the expense of computational complexity. 


\mysection{Efficient Neural Networks}
Efficient neural networks is a class of architectures that target mobile and embedded systems applications. These networks are usually designed to provide a balance between accuracy and efficiency (\eg latency, FLOPs, memory, and power). MobileNet \cite{Howard2017MobileNetsEC} utilizes depth-wise separable convolutions to reduce the required FLOPs and latency of a regular CNN for image processing. Depth-wise separable convolutions disentangle convolutions into learning channel correlations using point-wise convolutions and learning spatial correlations using depth-wise convolutions. Other efficient neural networks usually leverage either depth-wise separable convolutions with better designed architectures to improve performance \cite{Sandler2018MobileNetV2IR, Chollet2017XceptionDL, zhang2018efficient} or study new efficient operations to replace the regular convolutions \cite{Ma2018ShuffleNetVP, Wu2018ShiftAZ}. 
In 3D, efficient neural networks include ShellNet \cite{Zhang2019ShellNetEP}, PVCNN \cite{Liu2019PointVoxelCF}, Grid-GCN \cite{Xu2020GridGCNFF}, RandLA-Net \cite{Hu2020RandLANetES}, SegGCN \cite{Lei2020SegGCNE3} and LPNs \cite{Le2020GoingDW}. ShellNet \cite{Zhang2019ShellNetEP} and SegGCN \cite{Lei2020SegGCNE3} speed up the pseudo grid-based methods by aggregating neighborhood features through efficient 1D convolutions or fuzzy spherical convolutions on the predefined pseudo grids like shells. PVCNN \cite{Liu2019PointVoxelCF} and Grid-GCN \cite{Xu2020GridGCNFF} reduce the time spent in querying a neighborhood by combining voxelization in point-based methods.  RandLA-Net \cite{Hu2020RandLANetES} reduces the subsampling complexity by leveraging random sampling and further improves the speed by operating on a large-scale point cloud directly without chunking. 
LPN \cite{Le2020GoingDW} improves the speed of convolving neighborhood features by a simple group-wise matrix multiplication. Nevertheless, all efficient methods mentioned above require performing convolutions on neighborhood features, which we deem through extensive experiments as unnecessary. Therefore, our algorithm achieves much faster speeds compared to these methods (ref to \secLabel \ref{sec:experiments}). It is also worthwhile to mention that our method can be made even faster with the voxelization trick in PVCNN and Grid-GCN to further reduce the latency of neighborhood querying. We leave that as future work. 
 \section{Methodology}

\begin{figure*}[t]
\begin{center}
\includegraphics[width=\textwidth]{figures/latency_analysis.pdf}
\end{center}
\vspace{-3mm}
\setlength{\tabcolsep}{7mm}
\begin{tabular}{cccc}
(a) 1024 points & 
(b) 4096 points & 
(c) 10,000 points &
(d) 15,000 points 
\end{tabular}
\caption{\textbf{Latency Decomposition of PointNet++.} We show the inference run time decomposition of PointNet++ under different numbers of points as input on one NVIDIA GTX2080Ti GPU. 
}
\label{fig:latency_pointnet++}
\end{figure*}

\subsection{Preliminary: PointNet++} \label{sec:pointnet++}
PointNet++ \cite{Qi2017PointNetDH} improves PointNet \cite{Qi2017PointNetDL} by providing two main contributions: 
(1) developing a U-Net \cite{Ronneberger15unet} like architecture to process a set of points, which are sampled in a metric space in a hierarchical fashion. This mechanism captures multi-scale features and reduces the required computation.
(2) Developing a Set Abstraction (SA) module to process and abstract the locality from the local neighbors to a new set of points with fewer elements. The SA module is used as the basic building block to be stacked to form the backbone of PointNet++. 

\mysection{Analysis of the Set Abstraction Module}
As illustrated in \figLabel \ref{fig:vanilla_sa}, the vanilla SA module proposed in PointNet++ consists of two parts: point subsampling and feature aggregation, .  
The subsampling layer takes a point cloud $X = \{P, F\}$ as an input and leverages iterative farthest-point sampling to acquire $X'$, a subset of $X$. $P$ and $F$ denote the coordinates and features, respectively. 
The feature aggregation block is built for learning locality from local neighbors and is composed of a grouping layer, an MLP block, and a reduction layer. The grouping layer obtains the neighborhood composed of $K$ neighbors for each point in $X'$ using the ball query, with $X$ as the support set. The resulting point neighborhood is denoted as $\mathcal{N}(X')$, which contains $K$ repeated features. 
The MLP block consists of $L$ layers of MLPs, and each MLP is followed by a Batch Normalization (BN \cite{DBLP:conf/icml/IoffeS15}) layer and a ReLU activation. By default, PointNet++ sets $L= 3$. The number of feature aggregation blocks inside one SA module, referred to as depth $D$ in this paper, is set to $D=2$. The reduction layer (\textit{a.k.a}, pooling) aggregates the neighborhood information by a reduction function, \eg mean, max, or sum. The feature aggregation is formulated as shown in \eqnLabel \eqnref{eqn:vanilla_sa}:
\begin{equation}\label{eqn:vanilla_sa}
\mathbf{f}_i^{l+1} =\mathcal{R}\left(\left\{\operatorname{MLPs}((\mathbf{p}_j - \mathbf{p}_i) || \mathbf{f}_j^l)|j\in \mathcal{N}(i)\right\}\right),
\end{equation}
where $\mathcal{R}$ is the reduction function across the neighborhood dimension, which is used for aggregating the neighborhood information. $\mathbf{p}_i$, $\mathbf{f}_i^l$,  $\mathcal{N}(i)$ and $||$ denote the coordinates, the features in the $l^{th}$ layer of the network, the neighborhood of the $i^{th}$ point, and the concatenation operator across the channel dimension, respectively. 
The main issues with the vanilla SA module are: \textbf{(1)} the computational cost is unnecessarily high. \textit{MLPs are unnecessarily performed on the neighborhood features}, which causes a considerable amount of latency in PointNet++. One straightforward remedy is to use MLPs to learn a feature embedding on the point features directly instead of doing so on the neighborhood features. This reduces the FLOPs of each MLP by a factor of $K$. \textbf{(2)} \textit{All operations on neighbors are unnecessarily isotropic.} In other words, the MLPs and the reduction layer treat all local neighbors equally. This severely limits the representation capability of the network. 

\mysection{Latency Decomposition} \figLabel\ref{fig:latency_pointnet++} shows the latency decomposition of PointNet++  \cite{Qi2017PointNetDH} with different numbers of points as input. Here, the latency, which is the overall run time for the inference stage, was measured using a single Nvidia GeForce RTX 2080Ti GPU and one Intel(R) Xeon(R) CPU E5-2687W v4 @ 3.00GHz. We note here that latency is measured on the same hardware setting throughout this work. 
The latency of PointNet++ can be decomposed into three main contributing factors: (1) point subsampling, (2) grouping, (3) actual computations. The actual computations of PointNet++ mainly come from processing neighborhood features by MLPs shown in \eqnLabel \ref{eqn:vanilla_sa}. Note that we consider the time spent on data access implicitly in each part. Point clouds with four different input sizes were studied: $1024$, $4096$, $10,000$, and $15,000$. The first two input sizes are commonly encountered in classification tasks \cite{ben20183dmfv}, and the last two are usually input sizes for patch-based segmentation \cite{Qi2017PointNetDH, Thomas2019KPConvFA} and LiDAR-based object detection \cite{Shi2019PointRCNN3O}. 
Clearly, computations contribute to the majority of latency (over 70 \%). This suggests that \textit{the computational complexity could be the major speed bottleneck for networks involving PointNet++. }


\begin{figure}[!ht]
\begin{minipage}[t][67mm][t]{0.51\textwidth}
    \begin{subfigure}{1in}
        \includegraphics[height=60mm]{figures/vanilla_sa.pdf}
        \caption{Vanilla SA}
        \label{fig:vanilla_sa}
    \end{subfigure}\begin{subfigure}{1in}
    \includegraphics[height=60mm]{figures/assa.pdf}
    \caption{ASSA}
    \label{fig:ASSA}
\end{subfigure}
\end{minipage}\begin{minipage}[t][0mm][t]{.49\textwidth}
\vspace{-3cm}
\begin{minipage}[t][10mm][t]{1.0\textwidth}
\begin{subfigure}{2.5in}
\centering
        \includegraphics[width=0.23\linewidth]{figures/VisualizeActivations/feature_map_conv1_0.png}
        \includegraphics[width=0.23\linewidth]{figures/VisualizeActivations/feature_map_conv1_1.png}
        \includegraphics[width=0.23\linewidth]{figures/VisualizeActivations/feature_map_conv1_2.png}
        \includegraphics[width=0.23\linewidth]{figures/VisualizeActivations/feature_map_conv1_3.png}
\caption{Feature patterns before the first SA}
\label{fig:feature_before_assa}
\end{subfigure}
\end{minipage}

\begin{minipage}[t][35mm][b]{1.0\textwidth}
\begin{subfigure}{2.5in}
        \includegraphics[width=0.23\linewidth]{figures/VisualizeActivations/feature_map_sa_0.png}
        \includegraphics[width=0.23\linewidth]{figures/VisualizeActivations/feature_map_sa_1.png}
        \includegraphics[width=0.23\linewidth]{figures/VisualizeActivations/feature_map_sa_2.png}
        \includegraphics[width=0.23\linewidth]{figures/VisualizeActivations/feature_map_sa_3.png}
\label{fig:feature_after_assa}
\end{subfigure}
\begin{subfigure}{2.5in}
        \includegraphics[width=0.23\linewidth]{figures/VisualizeActivations/feature_map_sa_4.png}
        \includegraphics[width=0.23\linewidth]{figures/VisualizeActivations/feature_map_sa_5.png}
        \includegraphics[width=0.23\linewidth]{figures/VisualizeActivations/feature_map_sa_6.png}
        \includegraphics[width=0.23\linewidth]{figures/VisualizeActivations/feature_map_sa_7.png}
\caption{Feature patterns after the first SA}
\label{fig:feature_after_assa}
\end{subfigure}
\end{minipage}

\end{minipage}
\caption{
\textbf{Comparison of proposed Anisotropical Separable Set Abstraction (ASSA) module and the Vanilla SA module.} (a) Vanilla SA \cite{Qi2017PointNetDH} applies MLPs on neighbor features. (b) The proposed ASSA module separates the MLPs before the grouping layer and after the reduction layer. Therefore the MLPs are applied directly on the point features not on the neighbor features. ASSA also replaces the reduction layer in vanilla SA with a new Anisotropic Reduction layer. 
$X, N, C, K$ are the input point cloud, the number of points, the number of input features, and the number of neighbors. The shortcut layer in blue line is the residual connection with a linear mapping. 
(c) and (d) Show the point cloud feature patterns (activations) before and after the first ASSA module. The proposed ASSA module helps capture better geometric relationships. Refer to \secLabel \ref{sec:ablation_anisotropic} for details. 
}
\label{fig:sa_module}
\end{figure}



\subsection{Anisotropic Separable Set Abstraction (ASSA)}\label{sec:assa}
In this section, we gradually introduce the modified vanilla SA modules. Initially, we focus on speeding up vanilla SA. This is achieved through proposing two modules, namely, PreConv SA module and Separable SA module. Later, we focus our attention on improving the accuracy by proposing an Anisotropic SA module. 

\mysection{PreConv Set Abstraction module}
Vanilla SA repeatedly performs shared MLPs on point neighborhood features. To solve this issue, we modify the feature aggregation layer in vanilla SA, and propose PreConv SA to performs all MLPs on point features directly (not on the $k$ local neighbors) before the grouping layer. The PreConv SA is shown in \supp Figure  S1, and its  feature aggregation is formulated as follows:
\begin{equation}
\mathbf{f}_{i}^{\prime}=\operatorname{MLPs}\left(\mathbf{f}_{i}^{l}\right), \mathbf{f}_{i}^{l+1}=\mathcal{R}\left(\left\{\mathbf{f}_{j}^{\prime} \mid j \in \mathcal{N}(i)\right\}\right)
\end{equation}
PreConv SA reduces the required FLOPs by $K$ times. PreConv SA speeds up PointNet++ by $\sim 55\%$ (15,000 points), as shown in \secLabel \ref{sec:ablation_anisotropic}. Additionally, PreConv SA is equivalent to vanilla SA in the case where the $(\mathbf{p}_j - \mathbf{p}_i)$ term is not included in \eqnLabel \eqnref{eqn:vanilla_sa}. Proof is available in the \supp.

\mysection{Separable Set Abstraction module}
Next, we present Separable Set Abstraction (refer to \supp Figure  S1), which is more accurate than PreConv SA, yet requires the same latency. The aggregation layer in Separable Set Abstraction is formulated by: 
\begin{equation}
    \mathbf{f}_{i}^{res}=\operatorname{MLPs}\left(\mathbf{f}_{i}^{l}\right), \mathbf{f}_{i}^{l+1}=\mathbf{f}_{i}^{res} + \operatorname{MLPs}\left(\mathcal{R}\left(\left\{\mathbf{f}_{j}^{res} \mid j \in \mathcal{N}(i)\right\}\right)\right)
\end{equation}
The main idea of Separable SA is borrowed from depth-wise separable convolutions \cite{Howard2017MobileNetsEC}, where the regular convolution is split into one point-wise convolution (MLPs), one depth-wise convolution (channel shared convolution), and then another point-wise convolution. Separable SA evenly separates the MLPs before the grouping layer and after the reduction layer and further adds a residual connection between the outputs of the two parts of the MLPs. The main reasons why the Separable Set Abstraction module is better than PreConv are: \textbf{(1)} after reduction MLPs further process the aggregated neighborhood information; \textbf{(2)} The residual connection not only stabilizes training, but also provides better feature embedding by fusing the aggregated local information with the point information. Another minor change from PreConv SA to Separable SA is that we query the neighborhood using the subsampled point cloud $X'$ as the support set to further reduce the computational complexity of the second aggregation block. 

\mysection{Anisotropic Separable Set Abstraction module}
PreConv SA and Separable SA cut down computational complexity at the expense of accuracy, \eg Separable SA leads to a reduction of $3$ mIoU on S3DIS Area 5 compared to PointNet++ (\secLabel \ref{sec:ablation_anisotropic}).
There are two reasons for the  drop in accuracy. First, the geometric information is not well encoded in the current variants of the SA module. The geometric information can be represented by any relative information (edge information) between the neighbor and the center, \eg the relative position $(\mathbf{p}_j - \mathbf{p}_i)$ in PointNet++. The experiments in \secLabel \ref{sec:ablation_anisotropic} show that geometric information is essential for point feature embedding. 
Second, the reduction layer is an isotropic operation that treats each neighbor the same and thus leads to a sub-optimal representation. Recall in a depth-wise separable convolution, the depth-wise convolution uses different weights to summarize features from a $3\times3$ receptive field. However, simply introducing the depth-wise convolution kernel to point neighborhood aggregation does not work, as: (1) the neighbors are not necessarily ordered for the sake of efficiency; (2) the convolution kernel is shared by all points and neighbors and leads to poor neighborhood aggregation where the local geometric varies.
We propose an efficient geometric-aware \textit{Anisotropic Reduction} layer to effectively aggregate the point neighborhood information. The term "Anisotropic" indicates that our reduction layer considers each neighbor differently. We insert Anisotropic Reduction into the separable SA module and present our final variant of the SA module, the Anisotropic Separable Set Abstraction (ASSA) module, and show it in \figLabel \ref{fig:ASSA}. The feature aggregation of ASSA is formulated as:
\begin{equation}\label{eqn:assa}
\begin{split}
& \mathbf{f}_i^{res} =\operatorname{MLPs}(\mathbf{f}_i^l)\\
& \mathbf{f}_i^{l+1} =\operatorname{LN}\left(\mathbf{f}_i^{res}\right) + 
\operatorname{MLPs}\left(\mathcal{R}\left(\left\{\left(\frac{\Delta x_{i j}\mathbf{f}_j^{res} || \Delta y_{i j}\mathbf{f}_j^{res} || \Delta z_{i j}\mathbf{f}_j^{res}}{r} \right)|j\in \mathcal{N}(i)\right\}\right)\right)
\end{split}
\end{equation}
$\Delta x_{i j} = x_j - x_i$, $\Delta y_{i j}$ and $\Delta z_{i j}$ are the relative positions between the neighbor $j$ and the center $i$ in the $x, y, z$ dimension, respectively. The relative positions are used as scaling weights for aggregating the features across the neighborhood dimension, and they are normalized by the radius of the ball query $r$. The neighborhood features are scaled by the three corresponding relative positions individually. The three scaled neighborhood features are then concatenated together and passed into the reduction layer.
To reduce the computational complexity caused by the concatenation of the three scaled features, we set the last MLP before reduction as a bottleneck layer. This layer reduces the number of channels by a factor of $3$. The output of the reduction layer is then processed by another MLP block and is added to the output before reduction. Due to channel mismatch, the output of before grouping MLPs is mapped by a linear layer $\operatorname{LN}$ (\textit{a.k.a} the shortcut layer) before the addition. We highlight that our Anisotropic Reduction does not rely on any heuristic grouping (as done in PosPool \cite{Liu2020ACL}), and we make full use of the information from the neighborhood features. The pseudo code for ASSA in PyTorch-like style is available in the \supp. 

It is worth noting that all MLPs in ASSA are processed on the point features directly, not on the neighborhood, which greatly reduces the computations compared to \eqnLabel \eqnref{eqn:vanilla_sa}. 
In particular, for one aggregation block with $L=3$ MLPs, ASSA roughly reduces the FLOPs consumed in vanilla SA by: $\frac{C\times C \times N \times K \times L}{C\times C \times N \times L + C \times N \times K } \approx K$ times. Typically, $K$ is around 32. All of our SA variants are permutation invariant, which favors 3D deep learning on point clouds. More details of the ASSA module and its comparison with previous modules are provided in the \supp. 


\subsection{ASSANet}
We now replace the vanilla SA module in PointNet++ \cite{Qi2017PointNetDH} with our proposed ASSA module. All other parts are kept the same as PointNet++, including the number of SA modules ($4$), the number of aggregation blocks in SA ($D=2$), the layers of MLPs in an aggregation block ($L=3$), the channel sizes, the neighborhood querying configurations (ball query algorithm with maximum neighborhood size $K$ and radius $r$) and the subsampling configurations (farthest point sampling). The modified architecture of PointNet++ is referred to as ASSANet. \secLabel \ref{sec:experiments} shows that ASSANet can achieve much higher accuracies compared to PointNet and PointNet++ and is faster on various vision tasks.

\subsection{Scaling ASSANet}
Since the ASSANet is much faster than both PointNet++ \cite{Qi2017PointNetDH} and the state-the-of-art networks, we now present two ways to up-scale ASSANet to improve its accuracy: width scaling and depth scaling. We show the performance of each scaling regime in the ablation study presented in \secLabel \ref{sec:ablation_scaling}. 

\mysection{Width Scaling Regime} 
In width scaling regime, we modified the channel size of ASSANet. ASSANet is built upon PointNet++ \cite{Qi2017PointNetDH}, which uses hand-crafted channel sizes for each convolution layer. To make the scaling more programmable and user-friendly for the scaled ASSANet, the output of each feature aggregation block inside one ASSA module is set to have the same channel size, and is then concatenated as the output of the module. After this modification, we can easily study the effect of width scaling on the accuracy and the speed, by simply changing the initial channel size $C$.

\mysection{Depth Scaling Regime} 
The second way to scale is to increase the depth of the network, which can be achieved by changing the number of aggregation blocks $D$ stacked in each ASSA module. $D$ is set to $2$ by default in ASSANet. We can decrease $D$ to $1$ to make ASSANet faster or increase $D$ to improve its accuracy. Among all width or depth scaled versions of ASSANet, we emphasize \textbf{ASSANet (L)}, a large ASSANet network with $C=128$ and $D=3$. In most of the experiments, we compare ASSANet and ASSANet (L) with the state-of-the-art. 
 \section{Experiments}\label{sec:experiments}
\begin{wraptable}{r}{8cm}
\vspace{-20mm}
\centering
\begin{tabular}{lcc}
\midrule
\textbf{Methods} & \textbf{mIOU} & \textbf{Inference Speed} \\
& \% & instances/second \\
\midrule
PointNet \cite{Qi2017PointNetDL}	& 41.1 & 185.0 \\
DeepGCN \cite{Li2019DeepGCNs}    & 52.5 & 0.8 \\
PointCNN \cite{PointCNN}	   & 57.3 & 124.1 \\
Grid-GCN \cite{Xu2020GridGCNFF} & 57.8 & 123.5 \\ 
PVCNN \cite{Liu2019PointVoxelCF} & 59.0 & 89.8 \\
PosPool$^{*}$(S) \cite{Liu2020ACL}	& 61.3 & 21.0\\
SegGCN \cite{Lei2020SegGCNE3} & 63.6 & 29.3\\ 
KPConv \cite{Thomas2019KPConvFA} & 65.4 & 1.2 (24.2) \\
PosPool$^{*}$ \cite{Liu2020ACL}	& 66.7& 8.3\\
\midrule
PointNet++ \cite{Qi2017PointNetDH}	&  55.6 & 116.6 \\
ASSANet & 63.0 \R{(+7.4)} & 188.6 \R{($1.6 \times$)}\\
ASSANet (L) & 66.8 \R{(+11.2)} & 65.6\\
\bottomrule
\end{tabular}
\caption{\textbf{S3DIS scores (mIoU) on Area-5.} ASSANet outperforms PointNet++ and other methods with much higher accuracy and faster speed. ASSANet (L) performs better than the state-of-the-art KPConv \cite{Thomas2019KPConvFA} and PosPool$^{*}$ \cite{Liu2020ACL} while being over $7.9 \times$ faster. 
}
\label{tab:s3dis_sota}
\vspace{-10mm}
\end{wraptable}
We studied the accuracy and speed of ~ ASSANet and ASSANet (L) on S3DIS semantic segmentation \cite{s3dis}, ShapeNet part segmentation \cite{shapenet2015}, and ModelNet40 point cloud classification \cite{ben20183dmfv}. To enable a fair comparison, the same data processing and evaluation protocols adopted by the state-of-the-art method PosPool  \cite{Liu2020ACL} were used in our experiments.

\subsection{3D Scene Segmentation}
\mysection{Setups} 
We conducted extensive experiments on the Stanford large-scale 3D Indoor Spaces (S3DIS) dataset \cite{s3dis}. Following \cite{PointCNN, Liu2019PointVoxelCF, Liu2020ACL}, we trained all our models on Area 1, 2, 3, 4, and 6 and tested them on Area 5. We optimized all of our networks using SGD with weight decay $0.001$, momentum $0.98$ and initial learning rate (LR) $0.02$. We trained the models for $600$ epochs and used an exponential LR decay. At each inference time, a single RTX 2080Ti GPU was used to measure the speed for each method using a batch size of 16; each item in the batch has $15,000$ points ($16 \times 15,000$). If the batch size was too large to feed into the GPU, we lowered the batch size. \textit{Note that we focus on the speed since FLOPs and the model parameter size are not indicative of the actual latency \cite{Ma2018ShuffleNetVP, Liu2019PointVoxelCF}.}  The inference speed is calculated as the number of instances evaluated in one second (ins./sec.). The average speed over $200$ runs is reported. Other methods were measured in a similar manner. Note that KPConv \cite{Thomas2019KPConvFA} has to compute the pseudo kernels for each point cloud during data preprocessing. For a fair comparison, we show the speed of calculating the pseudo kernels on the fly.  We also include the speed of KPConv with preprocessed pseudo kernels in $()$ in the table.


\mysection{Comparison with state-of-the-art}
\tblLabel \ref{tab:s3dis_sota} compares the proposed ASSANet and ASSANet (L) with PointNet++ \cite{Qi2017PointNetDH} and the state-of-the-art on S3DIS. \textit{ASSANet outperforms PointNet++ by $7$ mIoU and is $1.6\times$ faster.} ASSANet also achieves much better accuracy than the two efficient point cloud processing algorithms PVCNN \cite{Liu2019PointVoxelCF} and Grid-GCN \cite{Xu2020GridGCNFF}, while also being over $1.5\times$ faster. 
\textit{ASSANet (L) achieves state-of-the-art performance with a mIoU of $66.8 \%$ on S3DIS, with very high speed.} ASSANet (L) is $294 \times$ faster than the graph-based method DeepGCN \cite{Li2019DeepGCNs}, $54.6 \times$ faster than the state-of-the-art pesudo grid-based method KPConv \cite{Thomas2019KPConvFA}, $7.9 \times$ faster than the state-of-the-art adaptive weight-based method PosPool* \cite{Liu2020ACL}, and $2.2 \times$ faster than the best-performing efficient method SegGCN \cite{Lei2020SegGCNE3}. Note that PosPool* refers to PosPool with sinusoidal position weight, and that PosPool* (S) denotes the small model.

\subsection{3D Object Classification}
\mysection{Setup}
As a common practice, we benchmark ASSANet on the ModelNet40 \cite{ben20183dmfv} object classification dataset. We adopted a similar training setting as that of S3DIS except that we used LR $0.001$ and a cosine LR decay. At the inference time, a single RTX 2080Ti GPU was used to measure the speed for the classification task using $16 \times 10,000$ points as input.


\begin{table}[!t]
\begin{minipage}[t]{.45\textwidth}
\begin{center}
\scalebox{0.825}{
\begin{tabular}{lcc}
\midrule
\textbf{Methods} & \textbf{OA} & \textbf{Inference Speed} \\
& \% & instances/second \\
\midrule
PointNet \cite{Qi2017PointNetDL}	& 89.2 & 483.8 \\
SpiderCNN \cite{xu2018spidercnn}	& 90.5 & < 275.7\\
PointCNN \cite{PointCNN}    & 92.5 & 183.4 \\
PosPool$^{*}$(S) \cite{Liu2020ACL}	& 92.6 & 48.8\\
DGCNN \cite{dgcnn} &92.9& 11.6 \\
KPConv \cite{Thomas2019KPConvFA} & 92.9 &  (30.1) \\
Grid-GCN \cite{Xu2020GridGCNFF} & 93.1 & 172.0 \\
PosPool$^{*}$ \cite{Liu2020ACL}	& 93.2& 27.6\\
\midrule
PointNet++ \cite{Qi2017PointNetDH}	&  90.7 &  275.7\\
ASSANet & 92.4 \R{(+1.7)} & 586.4 \R{($2.1\times$)}\\
ASSANet (L) & 92.9 \R{(+2.2)} & 153.2\\
\bottomrule
\end{tabular}}
\end{center}
\caption{\textbf{Comparison of our ASSANet and ASSANet (L) with other methods on ModelNet40 point cloud classification.} 
ASSANet outperforms PointNet++ with $1.7$  higher overall accuracy (OA) than PointNet++ and is $2.1$ times faster. ASSANet (L) achieves on par accuracy with the state-of-the-art while maintaining a high speed. 
}
\label{tab:modelnet40}
\end{minipage}\hfill
\begin{minipage}[t]{.45\textwidth}
\begin{center}
\scalebox{0.825}{
\begin{tabular}{lccc}
\midrule
\textbf{Methods} & \textbf{mIoU} & \textbf{Inference Speed} \\
& \% & instances/second \\
\midrule
PointNet \cite{Qi2017PointNetDL} & 83.7 & 1883.5 \\
PosPool$^{*}$ (S) \cite{Liu2020ACL}	 & 85.1 & 107.7 \\
DGCNN \cite{dgcnn} & 85.2 &  151.4 \\
LPN \cite{Le2020GoingDW} & 85.7 & 190.6 \\
PosPool$^{*}$ \cite{Liu2020ACL}	 & 85.8 & 58.0 \\
PointCNN \cite{PointCNN} & 86.1 & 626.4 \\ 
RS-CNN \cite{Liu2019RelationShapeCN} & 86.2 & <350.4 \\ 
KPConv \cite{Thomas2019KPConvFA} & 86.2 & (56.3)\\
\midrule
PointNet++ \cite{Qi2017PointNetDH}	& 85.1 & 350.4\\
ASSANet & 85.4 \R{(+0.3)} & 782.5 \R{($2.2\times$)}\\
ASSANet (L) & 86.1 \R{(+1.0)} & 438.5 \R{($1.3\times$)}\\
\bottomrule
\end{tabular}}
\end{center}
\caption{\textbf{Comparison of the part-averaged IoU (mIoU) of our ASSANet and ASSANet (L) with other methods on ShapeNetPart part segmentation.} 
Both of ASSANet and ASSANet (L) outperform PointNet++ with a higher speed. ASSANet (L) achieves a comparable accuracy as the state-of-the-art while being much faster.
}
\label{tab:shapenetpart}
\end{minipage}
\vspace{-5mm}
\end{table}



\mysection{Comparison with state-of-the-art}
\tblLabel \ref{tab:modelnet40} compares ASSANet and ASSANet (L) with the state-of-the-art. ASSANet outperforms PointNet++ by $1.7$ units in overall accuracy and is $2.1 \times$ faster than PointNet++. ASSANet (L) achieves on par accuracy as the state-of-the-art methods KPConv \cite{Thomas2019KPConvFA} and PosPool* \cite{Liu2020ACL} while being $5.0\times$ and $4.4\times$ faster, respectively. 

\subsection{3D Part Segmentation}
\mysection{Data}
 ShapeNetPart is a commonly used benchmark for 3D part segmentation. The networks were optimized using Adam \cite{Kingma14adam} with momentum $0.9$. The other training parameters were the same as ModelNet40 experiments. The speed of each method was measured with an input of $16 \times 2048$ points. We report the part-averaged IoU (mIoU) as the evaluation metric for accuracy. 

\mysection{Comparison with state-of-the-art}
\tblLabel \ref{tab:shapenetpart} shows that ASSANet again outperforms PointNet++ with a sharp increase ($2.2\times$) in speed on the ShapeNetPart part segmentation dataset. ASSANet (L) also achieves 1 unit higher mIoU than PoinetNet++ with a $1.3\times$ faster speed. Additionally, ASSANet (L) attains on-par accuracy, $86.1\%$ mIoU, with the state-of-the-art and is much faster. For example, ASSANet (L) is nearly $7.8\times$ faster than KPConv \cite{Thomas2019KPConvFA}. 
 \section{Ablation Study}
\begin{wraptable}{r}{7.5cm}
\vspace{-15mm}
\begin{center}
\begin{tabular}{lcc}
\midrule
\textbf{Aggregation} & \textbf{mIoU} & \textbf{Speed} \\
& \% & ins./sec.\\
\midrule
Vanilla SA & 55.6 &  116.6\\
PreConv SA & 48.7 &  180.9\\
Separable SA (SSA) & 52.4 & 180.0 \\ 
SSA + Relative Position & 58.5 & 184.0 \\ 
SSA + Attentive Pooling\cite{Hu2020RandLANetES} & 59.0 & 142.0 \\ 
SSA + PosPool\cite{Liu2020ACL} &62.0 & 168.4 \\ 
\textbf{Anisotropic Separable SA
} & \textbf{63.0} & \textbf{188.6} \\ 
\bottomrule
\end{tabular}
\end{center}
\caption{\textbf{Ablation study of the proposed SA variants.} All proposed SA variants achieve a faster speed than the vanilla SA. Our ASSA further improves the accuracy of the Separable SA module and outperforms other methods, while also being faster.  
}
\label{tab:abalation_sa}
\vspace{-10mm}
\end{wraptable}

An ablation study was conducted on S3DIS \cite{s3dis} Area-5. We show the effectiveness of the proposed SA variants and the effect of the two scaling regimes on ASSANet. 

\subsection{Ablation on Proposed SA variants}\label{sec:ablation_anisotropic} 
\tblLabel \ref{tab:abalation_sa} shows the speed and the accuracy of the proposed PreConv Set Abstraction (SA) module, the Separable SA module, and the Anisotropic Separable SA (ASSA) module compared to the vanilla SA. All of our proposed SA modules lead to a sharp increase (over $1.6\times$) in inference speed. The proposed Separable SA module can boost the accuracy of PreConv by $3.7$ mIoU, which verifies the effectiveness of separable MLPs and residual connections. Comparing the ASSA module with the Separable SA module, one can clearly see the importance of encoding the geometric information and the effect of the anisotropic operation to achieving higher accuracy. Additionally, we provide a comparison of our proposed Anisotropic Reduction with the Attentive Pooling used in RandLA-Net \cite{Hu2020RandLANetES} and the PosPool proposed in \cite{Liu2020ACL}. Our method clearly outperforms both of these methods in terms of accuracy and inference speed. We also test simple addition of the relative positions $\Delta_x + \Delta_y + \Delta_z $ as the weights of the reduction layer, denoted as Relative Position, the obtained performance is worse than the proposed Anisotropic Reduction.
To further show the benefits of the proposed ASSA module, we visualize the \textbf{feature patterns} before and after the ASSA module in \figLabel \ref{fig:sa_module}. ASSA module helps capture better geometric relationships among points constituting the point cloud (for example, in the second and fourth examples in the first row of \figLabel \ref{fig:feature_after_assa}, one can see that the ASSA module allows the network to learn relationships between the tail of the plane and its wings). \supp provides a more detailed overview and a further set of examples of feature patterns visualization. 

\subsection{Ablation on Scaling  Regimes}\label{sec:ablation_scaling}
We now study the effects of ablating the width and depth of a network on its accuracy and inference speed. The initial channel size of the network is referred to as width (denoted by $C$), whereas the number of aggregation layers inside a single SA module is referred to as depth (denoted by $D$). 

\mysection{Width scaling} \figLabel \ref{fig:scaling} (left) shows the effect of the width scaling regime. When the width of the network is small, increasing the width leads to a significant improvement in accuracy. For example, simply increasing the width $C$ from $3$ to $8$ sharply improves the accuracy from $41.21$ mIoU to $53.95$ with a negligible drop in speed. However, when the network is wide enough ($C \geq 128$), increasing the width further only leads to a marginal improvement in accuracy, yet reduces the speed noticeably.

\mysection{Depth scaling} \figLabel \ref{fig:scaling} (right) shows the effect of the depth scaling regime. We study the depth scaling with $C=128$, which is the sweet point of width scaling. When the network is shallow, with a depth of $D \leq 3$, increasing the depth leads to an obvious increase in accuracy. However, depth scaling rapidly saturates as the depth increases. Depth scaling leads to a linear reduction in speed. 

\begin{figure}[!htb]
    \centering
\begin{minipage}{.48\textwidth}
    \includegraphics[width=\textwidth]{figures/WidthScaling.pdf}
\end{minipage}
\begin{minipage}{.48\textwidth}
      \includegraphics[width=\textwidth]{figures/DepthScaling.pdf}
\end{minipage}
    \caption{\textbf{Effect of Width (left) and Depth Scaling (right).} Increasing either the width or the depth leads to an improvement in accuracy and drop in inference speed. }
    \label{fig:scaling}
\end{figure}
 \section{Conclusion}\label{sec:conclusion}
In this paper, we dove deeper into the architecture of PointNet++. We noticed that PointNet++ suffers from a computational burden attributed to the MLPs that process the neighborhood features in the set abstraction (SA) module. We also found out that the accuracy of PointNet++ is limited by the isotropic nature of its operations. To solve these issues, we proposed a PreConv SA module, a Separable SA module, and finally an Anisotropic Separable SA (ASSA) module that aim to reduce the computational cost and improve the accuracy. We then replaced the vanilla SA module in PointNet++ with our ASSA module and proposed a fast and accurate architecture, namely, ASSANet. Extensive experiments were conducted to verify the presented claims and showed that ASSANet achieves largely improved accuracy and much faster speed on various point cloud tasks, such as classification, semantic segmentation, and part segmentation. We also studied up-scaling ASSANet. The scaled ASSANet set new state-of-the-art on various tasks with faster speeds. For future work, one could leverage both random sampling \cite{Hu2020RandLANetES} and voxelization tricks \cite{Liu2019PointVoxelCF, Xu2020GridGCNFF} to further improve the inference speed. Alternatively, one could consider studying compound scaling, like that in EfficientNet \cite{Tan19EfficientNet}. 
 

\mysection{Acknowledgement}
The authors appreciate the anonymous NeurIPS reviewers for their constructive feedback (including the revised title, the feature pattern visualization, and the additional experiments). This work was supported by the KAUST Office of Sponsored Research (OSR)  through the Visual Computing Center (VCC) funding. {\small
\bibliographystyle{plain}
\bibliography{main}
}

\end{document}
