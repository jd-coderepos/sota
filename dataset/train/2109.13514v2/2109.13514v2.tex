\section{Experiments}
\label{sec:Experiments}
Our focus in this section is to study the influence of the four parameters of our method on classification accuracy, as well as comparing its performance to recent state-of-the-art approaches. All the experiments were run on a DELL PowerEdge R730 on Debian 9 with 2 XEON E5-2630 Corei7 (92 cores) and 64GB of RAM. We provide a python package\footnote{https://github.com/baraline/convst} using community standards to run the method and the interpretability tool on any dataset, along with all result tables, and reproducibility instructions for our experiments.

In the following, we use the 112 univariate datasets from the UCR archive \cite{Dataset} and when comparing to state-of-the-art results, we use the same resamples scheme as the one used in their experiments. We use critical difference diagrams to display the mean ranks of objects, with cliques (formed by horizontal bars) computed using the Wilcoxon-Holm post-hoc analysis \cite{CriticalDiagram}, with a p-value of $0.05$. A clique indicates that the accuracy difference between objects is not statistically significant. 

\subsection{Sensitivity Analysis}
We conduct a sensitivity analysis on the four input parameters of our algorithm and their effect on classification accuracy on 40 datasets selected randomly, with raw results and selected datasets in a specific file in the online repository. 
For each parameter analysis, all other parameters remain fixed at the following default values : $n\_shapelets = 10000$, $p\_norm=0.9$, $L=[7,9,11]$, $P1=5$ and $P2=15$. Figure \ref{fig:sensi_len_nshp} and Figure \ref{fig:sensi_bound_pnorm} give the mean accuracy ranks of each method over the 40 datasets, with the accuracy of each method and each dataset computed as the mean of the same 10 resamples.
Given the tested set of values, the most impactful parameter is the number of shapelets, with a noticeable increase in performance above 10000 shapelets. All other parameters only display minor gains and thus seem to be stable. Based on those results, for all further experiments we set as default parameters $n\_shapelets = 10000$, $p\_norm=0.8$, $L=[11]$ and $P1=5$, $ P2=10$, and report results for datasets used in sensitivity analysis and the others.

\begin{figure}[h]
  \includegraphics[width=1.\textwidth]{len.png}
  \centering
  \caption{Accuracy ranks for (a) different number of shapelets, and (b) different shapelet lengths}
  \label{fig:sensi_len_nshp}
\end{figure}

\begin{figure}[h]
  \includegraphics[width=1.\textwidth]{ppnorm.png}
  \centering
  \caption{Accuracy ranks for (a) different percentiles bounds, and (b) proportion of z-normalized shapelets}
  \label{fig:sensi_bound_pnorm}
\end{figure}

\subsection{Scalability}
We perform a comparison of the scalability of our approach against Hive-Cote 1.0 (HC1), Hive-Cote 2.0 (HC2), DrCIF, ROCKET, and the Shapelet Transform Classifier (STC). Note that when used as a component in HC1 and HC2, STC is by default subject to a time contract of two hours. Except from this default configuration in HC1 and HC2, we are not setting any time contract in other algorithms. Both STC and RDST are by default sampling 10000 shapelets, and ROCKET use 10000 kernels.

We are aware that the runtime of HC1, HC2 and STC could be reduced with time contracts. But, as our goal in this section is to contextualize the gain in classification accuracy against the time complexity of each method, we present the results with the time contracts used to generate the accuracy results of the next section.

We use the Crop Dataset and the Rock Dataset of the UCR archive for evaluating the scalability respectively on the number of time series and their length. As all competing algorithms implemented in the sktime package of \cite{sktime} can use parallel processing, we set each algorithm to use 90 cores. Figure \ref{fig:scal} reports the mean training time over 10 resamples, showing the very competitive scalability of RDST. Note that due to job time limitation on our machine and the computational cost of HC2, we could not consider all samples for the Crop dataset. We report the reader interested in the implementation details of our algorithm to the web page of the project.

\begin{figure}[h]
  \includegraphics[width=1.\textwidth]{scalability.png}
  \centering
  \caption{Result of the scalability study of the competing algorithms for current state-of-the-art, for (a) number of time series and (b) time series length. Y-axis use log-scale.}
  \label{fig:scal}
\end{figure}

\subsection{Comparative study}
We present the results of our comparative study using the mean accuracy over the same 30 resamples for each of the 112 datasets as HC2 \cite{HC2} used in their study, and compare our approach against their experimental result. Figure \ref{fig:ranksdiv} gives the mean accuracy rank of each method over the 40 datasets used for setting the defaults parameters in sensitivity analysis, and for the 72 others. The full result tables including standard deviation per dataset and more visualizations of the results are available online as supplementary materials. 

\begin{figure}[h]
  \includegraphics[width=1.0\textwidth]{ranks2.png}
  \centering
  \caption{Mean accuracy ranks of each method for the 40 dataset used in sensitivity analysis and the 72 others.}
  \label{fig:ranksdiv}
\end{figure}

Given the scalability and simplicity of our method, having an accuracy comparable to the prior developments of HC2 and to deep learning approaches is a very promising result. Notably for future developments where focus would shift to accuracy rather than scalability. For reference, using RDST without any distance normalization is equivalent to STC in terms of mean accuracy rank, with the same protocol as above.

\subsection{Interpretability}
Given a set of $M$ shapelets, RDST generates $3M$ features. Each feature is linked to a weight for each class in the Ridge classifier, as it is trained in a one-vs-all fashion. 
Given a class, we can then visualize either global or local information. Locally, we can inspect a shapelet to show how it discriminates the current class, and where the shapelet is positioned with either training or testing data, as shown in Figure \ref{fig:interp}. Globally, we can display the distribution of weights for each feature type ($min$, $\argmin$ and $SO$) or by shapelet characteristics such as length, dilation, or use of normalization as shown in Figure \ref{fig:interp2}. 
While this only provides a basic interpretation of the results, we believe a more formal framework could be developed to extract explanations from this data.
\begin{figure}[h!]
  \includegraphics[width=1.0\textwidth]{interp1.png}
  \centering
  \caption{The most important shapelet for class 0 of the Coffee dataset, according to weights of the Ridge classifier, with distribution displayed on the testing data, and two testing samples for visualization.}
  \label{fig:interp}
\end{figure}
\begin{figure}[h!]
  \includegraphics[width=1.\textwidth]{inter2.png}
  \centering
  \caption{A global interpretation of RDST, with (a) distribution of weights for each type of feature, and (b) distribution of weights per dilation.}
  \label{fig:interp2}
\end{figure}
