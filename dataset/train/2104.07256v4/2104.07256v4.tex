

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{boldline}
\usepackage[ruled]{algorithm2e}
\usepackage{subfigure}
\newcommand{\myparagraph}[1]{{ \noindent \bf #1}}

\renewcommand{\rmdefault}{ptm}
\renewcommand{\sfdefault}{phv}
\def\SOTA{state-of-the-art\xspace}

\def\SAUG{SDA\xspace} 


\usepackage{cite,nicefrac}
\usepackage{listings}

\usepackage{enumitem}
\setitemize{noitemsep,topsep=3pt,parsep=0pt,partopsep=0pt}

\usepackage[margin=4pt,font=small,labelfont=bf,labelsep=endash,tableposition=top]{caption}


\usepackage{xcolor}
\definecolor{codegreen}{rgb}{0.5,0.5,1}
\definecolor{codeblue}{rgb}{0.5,0.5,1}
\definecolor{codegray}{rgb}{0.6,0.6,0.6}



\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{6658} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}



\begin{document}




\title{A Simple Baseline for Semi-supervised Semantic Segmentation 
\\ 
with  Strong Data Augmentation\thanks{JY and YL contributed equally to this work.
\it Accepted to Proc.\ Int.\ Conf.\ Computer Vision (ICCV) 2021.
}
}


\author{
Jianlong Yuan,
~
~
~
~
Yifan Liu, 
~
~
~
~
Chunhua Shen, 
~
~
~
~
Zhibin Wang,
~
~
~
~
Hao Li
\
    \mu_B = \tfrac{1}{m}\sum_{i=1}^{m}x_i ; \;\;   \sigma_B = 
\Bigl
    [ {\tfrac{1}{m}\sum_{ i =1 }^{m}(x_i-\mu_B)^2 + \epsilon} \Bigr ] ^{0.5},
    \label{equ: batch mean var}

    y_i = \gamma \frac{(x_i - \mu_B)}{\sigma_B} + \beta,
    \label{equ: batch norma}

	\begin{split}
    \mu_{t+1} = \alpha \mu_{t} + (1 - \alpha) \mu_{t+1}, \\
    \sigma_{t+1} = \alpha \sigma_{t} + (1 - \alpha) \sigma_{t+1}.
    \end{split}
    \label{equ: batch norm decay}

	\ell_i = w_i \cdot  {\y_i} \log( {\y_i^*})+(1-w_i) \cdot 
	{\y_i^*} \log( {\y_i}),
	\label{equ: ASCL equation}

\label{equ: ASCL equation weights}
	w_{i} = \max(\frac{ \exp(y_{i0}^*)}{\sum_{j=0}^{c} \exp(y_{ij}^*)} , ... ,\frac{ \exp(y_{ic}^*)}{\sum_{j=0}^{c} \exp(y_{ij}^*)}).




\section{Experiments}
In this section, we first report the implementation details. Then we perform a series of ablation experiments, and analyze the results in detail. Finally, we report our results compared with other state-of-the-art methods.
\subsection{Implementation Details}

\noindent\textbf{Training.} Our implementation is built on Pytorch \cite{paszke2019pytorch}.
Following previous work \cite{chen2018encoder}, we use the ``poly" learning rate policy, where the base learning rate is multiplied by . The initial learning rate is set to , and power is set to . We train the network using mini-bath stochastic gradient descent (SGD).  
We employ the crop size of  on Cityscapes with DeepLabv3Plus. More details is same as \cite{chen2018encoder}.
In the addition study, following \cite{zhao2017pyramid, yuan2020multi, chen2018encoder, lin2017refinenet, wu2019wider}, we adopt the mean intersection of union (mIoU) as the evaluation metrics.

\noindent\textbf{Data augmentation.} Following \cite{chen2018encoder}, we use mean subtraction, and apply data augmentation by random resize between 0.5 and 2 and random left-right mirror during initialization training. In the semi-supervised learning phrase, we use strong augmentation for improving the performance of self-training.

\noindent\textbf{Datasets.} Following \cite{chen2005naive}, We conduct the main experiments and ablation studies on the Cityscapes dataset \cite{cityscpaes}. This large-scale data set contains different stereo video sequences recorded in  different urban street scenes. In addition to k weak annotation frames, there are  high-quality pixel level annotation frames, in which, ; 
 and  were used for training, validation and testing. In addition, there are k images with rough annotations, namely train-coarse. We randomly subsample , 
, and 

of images in the standard training set to construct the pixel-level labeled data. The remaining images in the training set are used as unlabeled data. 

At last, we report our results on the Pascal VOC 2012 dataset \cite{everingham2012pascal}, which contains 21 classes including background. The standard Pascal VOC 2012 dataset has  images as the training set and  images as the validation set. We construct  images as the pixel-level labeled data. The images in the augmented set \cite{SBD} (around 9k images), are used as unlabeled data. 


\begin{table*}[tb]
\footnotesize 
\centering 
\begin{tabular}{l c c c c c c c c c}
\hlineB{2}
Model &s.t. &\SAUG & DSBN &SCL &iter.\  & mIoU- (\%) &mIoU- (\%) & mIoU- (\%) & mIoU-Full (\%) \\
\hline\hline
Deeplabv3plus     & && & & & 68.9&73.4&76.9&78.7 \\
Deeplabv3plus     & && & & &71.2&76.2&78.0&79.5 \\
Deeplabv3plus     & && & & &71.3 &76.3 &78.2 &79.7 \\
Deeplabv3plus     & && & & &72.3&76.9&78.7&80.0 \\
Deeplabv3plus     & & && & & 72.8&77.4&78.7&80.4\\
Deeplabv3plus     & & && & & 74.1&77.8&78.7&80.5\\
\hlineB{2}
\end{tabular}
\caption{Ablation study on the proposed semi-supervised learning framework. The model here 
is 
Deeplabv3plus with Xception65 backbone. `s.t.' denotes self-training with pseudo-labels without strong augmentation. `\SAUG ' means the strong data augmentation. `DSBN' means the distribution specify batch normalization. `SCL' is the proposed self correction loss. `iter.' represents iterative training. The results are evaluated on the validation set with the single-scale input.  `mIoU-' means that we use   labeled data and the remaining images in the training set are used as unlabeled data. `mIoU-Full' means that we use all the labeled data in the training set, and the train-coarse are employed as the unlabeled dataset.}
\label{tab: ablation for ist}
\end{table*}


\subsection{Ablation Study}
In this subsection, we conduct the experiments to explore the effectiveness of each proposed module under different semi-supervised settings. First, we establish the baseline for our experiments. We evaluate DeepLabv3Plus based on Xception65 with cross entropy on the validation set. Following \cite{chen2005naive, xie2020self}, self training adopts the baseline model as the teacher model and the teacher model generates pseudo labels on the unlabeled dataset, then the student model which is not smaller than the teacher model is trained on pseudo labels and original labeled images. We use the scales including , , , ,   and flip for the remaining images to generate pseudo labels. For fair comparison and quick training, the student model is the same model initialized by the teacher model. The main results are shown in Table~\ref{tab: ablation for ist}, and we will give detail discussions on each module in the following sections. All the ablation study are conducted on Cityscapes.

\subsubsection{Different semi-supervised settings}

We follow previous works~\cite{french2019semi,souly2017semi,hung2018adversarial} to divide the train set into labeled data and unlabeled data according to different proportions. We 
use
,
,
and 
of the training dataset as labeled data and other images as the unlabeled data. The evaluation results are shown in Table~\ref{tab: ablation for ist} as 
mIoU-, mIoU-, mIoU-, respectively. From the Table \ref{tab: ablation for ist}, we can see that our algorithm can effectively improve the performance by , , , , respectively. 
When the total number of labeled data and unlabeled data does not change, the semi-supervised learning framework will have a larger improvement, if we have less labeled images. In the meantime, the final results will be better if we have more high-quality labeled images. Besides, we can see that only half the amount of labeled data with the help of proposed semi-supervised learning framework can achieve the same results (mIoU of ) as we use all labeled training data with supervised learning (row 1 and column 10).

When the total number of available data increases, semi-supervised learning will play a more valuable role. We  use all high-quality labeled data in the training set as the labeled data and employ the train-coarse as unlabeled data to conduct the experiment, and the evaluation results are referred to as mIoU-Full in Table~\ref{tab: ablation for ist}. We can see that under a strong baseline with large amount of labeled data, if extra unlabeled data can be obtained, the performance will be further improved with the proposed semi-supervised learning framework.

In practical applications, labeled data is often limited, but unlabeled data is easy to obtain.  
To explore the impact of increasing the ratio of unlabeled data in the training set.
We fix the labeled data as 
of the training set, and increase the number of the unlabeled data. The results are shown in Figure~\ref{fig:paper_Unlabel_images_multiples}.
From Figure~\ref{fig:paper_Unlabel_images_multiples}, we can see that increasing ratios of the unlabeled data to the labeled data would  improve the results, but the growth trend of the performance is gradually flattening limited by the capacity of the initial teacher model trained on the fixed labeled data. The performance even slightly drops if too many unlabeled images are introduced during training.



\begin{figure}[t]
  \centering
    \includegraphics[scale=0.8]{images/paper_Unlabel_images_multiples}
  \caption{Different ratios of unlabeled data to labeled data. 
  }
	\label{fig:paper_Unlabel_images_multiples}
	\vspace{-1em}
\end{figure}




\subsubsection{Impact of the strong augmentation}

In this section, we will show the impact of the proposed strong augmentation and the DSBN. First, as shown in Table~\ref{tab: ablation for ist}, we compare the results of the naive self-training and the self-training combined with strong augmentation and the proposed DSBN. Although self-training can improve the performance over the baseline, 
self training directly uses pseudo labels that makes the network difficult to learn and introduces noise, as discussed in Section \ref{sec:methods}. Meanwhile, in the case of only using strong augmentation, there is no significant improvement in performance.
On the contrary, our method performs favorably against naive self training with strong augmentation and DSBN. In particular, compared with naive self training, our method improves the mIoU by , ,  and  based on DeepLabv3Plus. In the following sections, we  analyze  the distribution of the BN statistics, and then we compare the strong augmentation with different BN settings. Finally, we provided some discussions on how does the strong augmentation work with ground truth labels and the pseudo labels.

\noindent\textbf{Visualization of the BN statistics.}
To verify our statement that the distribution of the BN will be affect by the strong augmentation, we show the statistics of BN training with strong augmentation input images and the weakly augmentation input images. The visualization results can be found in Figure~\ref{fig:bn dif}.

The distribution of the mean and variance obtained from the whole training statistics deviates from the test set after adding strong augmentation. As shown in Figure \ref{fig:bn dif}, we calculate the distribution of the mean and the variance of the BN with the weakly augmentation and the strong augmentation. The blue line denotes the distribution of the BN with weakly augmentation, while the red line represents the BN with strong augmentation. We can see that strong augmentation has changed the distribution of BN statistics, which may lead to a domain gap. We think this is the reason why strong augmentation has not brought much performance improvement, as shown in Table \ref{tab: ablation for ist}.




\noindent\textbf{Impact of the DSBN.}
To show the effectiveness of proposed DSBN, we conduct experiments to compare three different BN settings as follows:
\begin{itemize}
    \item Trainable BN: the mean and the variance are updated during training with both weak augmentation data and strong augmentation data.
    \item Fixed BN: the mean and the variance are fixed during training, and initialized with the pre-trained weights from the ImageNet classification.
    \item DSBN: different mean and the variance are updated during training for weak augmentation data and strong augmentation data, separately.
\end{itemize}
The baseline model is DeepLabV3Plus trained with  labeled data on the training set. The other  images are used as the unlabeled dataset in the semi-supervised learning. The results are shown in Table~\ref{tab: ablation for bn}. We can see that with the proposed DSBN, the performance can be improved by , which indicates the DSBN can help to solve the negative effect caused by the strong augmentation.


\begin{table}[b]
\begin{center}
\resizebox{0.6\linewidth}{!}{
\begin{tabular}{l c }
\hlineB{2}
Baseline  & mIoU (\%)  \\
\hline\hline
ST     & 71.2 \\
ST + \SAUG  + Trainable BN &71.3 \\
ST + \SAUG  + Fixed BN   &71.6 \\
ST + \SAUG  + DSBN & 72.3 \\
\hlineB{2}
\end{tabular}
}
\end{center}
\caption{Ablation study for different batch normalization methods.}
\label{tab: ablation for bn}
\end{table}


\noindent\textbf{Discussions.}
To further understand the impact of the strong augmentation and DSBN on different data groups, we apply strong augmentation to only ground truth labels and pseudo labels.
\begin{itemize}
    \item \textit{ ground truth.} We train the model with only 1/8 images of the training dataset with corresponding ground truth labels.
    \item \textit{Full-ground truth.} We train the model with all images of the training dataset with corresponding ground truth labels.
    \item \textit{ pseudo labels.} We train the model with 7/8 images of the training dataset with corresponding pseudo labels.
\end{itemize}

For each settings, we apply weekly augmentation, strong augmentation and DSBN during training. The experiment results are shown in Table~\ref{tab: ablation for aug}. We can see that under the fully supervised settings, the strong augmentation can not bring extra improvements. Meanwhile, when applying to the unlabeled data with pseudo labels, the mIOU improved by . These observations indicate that improvements under semi-supervised settings are not come from extra image transformation types on labeled data, but benefit from make further use of unlabeled data with strong augmentations.
From Table~\ref{tab: ablation for aug}, we also find that the DSBN can contribute to the training.
\textit{In addition, to our knowledge,  
this is the first time to apply the strong augmentation to semantic segmentation under fully-supervised setting, and improvement is observed.}

\begin{table}[t]
\begin{center}
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{l |c }
\hlineB{2}

\hline
~~~ Methods  & mIoU (\%)\\
\hline
\multicolumn{2}{c}{Training with ground-truth labels}\\
\hline
-ground truth     & 68.9 \\
-ground truth + \SAUG  &68.6 \\
-ground truth + \SAUG  + DSBN & 69.5\\
\hline
Full-ground truth     & 78.7 \\
Full-ground truth + \SAUG  &78.7 \\
Full-ground truth + \SAUG  +  DSBN & 79.2\\
\hline
\multicolumn{2}{c}{Training with pseudo-labels}\\
\hline
-pseudo labels    & 70.6 \\
-pseudo labels + \SAUG  &71.4 \\
-pseudo labels + \SAUG  + DSBN &72.5 \\
\hlineB{2}
\end{tabular}
}
\end{center}
\caption{Effect of the strong augmentation on different data.
We apply the strong augmentation to the baselines trained with only labeled data, and only unlabeled data with pseudo-labels, separately.}
\label{tab: ablation for aug}
\vspace{-1em}
\end{table}


\subsubsection{Impact of the self-correction loss}
To further improve the performance and reduce the impact of noise, we employ a self correction loss (SCL). From Table~\ref{tab: ablation for ist}, we can see that the proposed SCL will play a more important role when the pseudo labels contain more noise. The teacher model trained with a limited labeled dataset may get less useful knowledge, and can not generalize well on unlabeled dataset. Thus, the level of the noise is higher than that training on more labeled data. When we train with 

or 

labeled images, the SCL can improve the mIoU by 
. 

Furthermore, we show the mIoU during the training phase with and without the proposed SCL in Figure \ref{fig:scl iterations}. When training without the SCL, the result will be divergent quickly. On the contrary, the result is relatively stable. 
\begin{figure}[th]
  \centering
    \includegraphics[scale=0.705]{images/paper_scl_weights.eps}
  \caption{This curve shows
  the 
validation mIoU w.r.t.\ the number of iterations during training.
  The green line represents the SCE loss \cite{wang2019symmetric}. In addition, we set the same ratio between CE and RCE. The red line represents our proposed loss with the same initial weights as the SCE loss. }
	\label{fig:scl iterations}
	\vspace{0.2in}
\end{figure}
\subsubsection{Impact of 
iterative 
training
}
Following Naive Students~\cite{chen2005naive}, we also test the impact of the 
iterative 
training in our semi-supervised learning framework. Table~\ref{tab: ablation for ist} has shown that the iteration training will become less useful if more labeled data can be obtained. Because the initial teacher model trained with more labeled data can already produce satisfactory pseudo labels. We show the change of the performance for the number of iterations in Figure~\ref{fig: iter_dp3}. The baseline model is DeepLabV3Plus trained with  labeled images. We can see that more iterations 
indeed 
help improve the performance, but the growth trend of the performance is gradually flattening.




\begin{figure}
  \centering
    \includegraphics[scale=0.649]{images/paper_iters_deeplabv3+}
 
    
\caption{The performance of the DeepLabV3Plus under different number of iterations.} 
\label{fig: iter_dp3}
 	\vspace{-0.2cm}
\end{figure}



\begin{figure*}
    \centering
    \includegraphics[width=0.97\linewidth]{images/results}
    \caption{Qualitative results on the Cityscapes dataset.
The baseline method is trained with 
  of all 
labeled images. 
     `ST' represents the self-training trained with 

pseudo-labels without our methods. 
 `Ours'  is 
 our framework with  labelled data.
    The proposed semi-supervised approach produces improved results compared to the baseline and the naive self training. We highlight the details in yellow boxes.}
    \label{fig: results}
    \vspace{-1em}
\end{figure*}


{\bf Visualization results}
Several visualization results are shown in Figure \ref{fig: results}. 
\iffalse
The baseline method is trained with 
  of all 
labeled images. 
The ST represents the self-training trained with 

pseudo labels without our methods. 
 `ours'  donates our framework.
\fi  
We can see  that the baseline and self-training can not well separate the objects (especially large-size 
objects such as  bus, truck, train, sidewalk) completely while ours corrects these errors. Our method also performs well on these small-size object classes, such as pole and bicycle, compared to the baseline model and self-training. 



\subsection{Comparison with State-of-the-art Methods}
\noindent\textbf{Cityscapes.}
We conduct the comparison experiments with other state-of-the-art algorithms on the Table \ref{tab: SOTA  on different amount cityscapes}. To make a fair comparison, we also apply our proposed framework to DeepLabV2 following~\cite{hung2018adversarial}. In particular, DeepLabV2 is based on ResNet-101 initialized with the pre-trained weights on ImageNet classification. The proposed framework achieves 67.6\%, 69.3\%, 70.7\% with DeepLabV2 and 74.1\%, 77.8\%, 78.7\% with DeepLabV3Plus, respectively, which performs favorably against previous state-of-the-art methods. More details of ablation studies of with DeepLabV2 can be found in the supplementary materials.

\begin{table}
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{r  c c c c c}
\hlineB{2}
Method &Model &  &  &  &Full\\
\hline\hline
AdvSemiSeg\cite{hung2018adversarial}&DeepLabV2 &58.8 & 62.3 &65.7 &66.0 \\
S4GAN + MLMT\cite{mittal2019semi} &DeepLabV2 &59.3 &61.9 &- &65.8 \\
CutMix\cite{french2019semi}&DeepLabV2 &60.3 &63.87 &- &67.7 \\
DST-CBC\cite{feng2020semi} &DeepLabV2 &60.5 &64.4 &- &66.9\\
ClassMix\cite{olsson2020classmix} &DeepLabV2 &61.4 &63.6 &66.3 &66.2 \\
ECS\cite{mendel2020semi} &DeepLabv3Plus &67.4 &70.7 &72.9 &74.8 \\
\hline
Baseline &DeepLabV2  &60.6 &66.7 &69.3 &70.1\\
Ours&DeepLabV2 &67.6 &69.3 &70.7 &70.1\\
Baseline& DeepLabV3Plus&68.9&73.4&76.9&78.7\\
Ours&DeepLabV3Plus &\textbf{74.1} &\textbf{77.8} &\textbf{78.7} &\textbf{78.7}\\
\hlineB{2}
\end{tabular}
}
\end{center}
\caption{Compared with  \SOTA methods on the Cityscapes val set. Here `'  means 
that 
we use   labeled data and the remaining images in the training set are used as unlabeled data.}
\label{tab: SOTA  on different amount cityscapes}
\vspace{-1em}
\end{table}


\noindent\textbf{Pascal VOC.}
We follow DeepLabV3Plus \cite{chen2018encoder} training details with the 513513 crop size on Pascal VOC \cite{everingham2012pascal}. Table \ref{tab: SOTA in voc} lists the performance results of other state-of-the-art methods and ours. We adopt the single-scale testing on our experiments. In order to make a fair comparison, we did experiments under different backbones. Our framework achieves 75.0\% mIoU based on Resnet-101\cite{He2016Deep} and 79.3\% mIoU based on Xception-65\cite{chen2018encoder}, which outperforms the PseudoSeg \cite{zou2020pseudoseg} by 1.8\%. At the same time, we find that our results based on Xception-65 can achieve the same performance 
as that of the fully supervised setting.
In addition, we find that the improvement of iterative training on Pascal VOC is limited, which might be due to the small number of samples.





\begin{table}
\begin{center}
\resizebox{0.7\linewidth}{!}{
\begin{tabular}{ r  c c c c c c c}
\hlineB{2}
Method     & Model &mIoU \\
\hline\hline
GANSeg\cite{souly2017semi} &VGG16 &64.1 \\
AdvSemSeg\cite{hung2018adversarial} &ResNet-101 &68.4 \\
CCT \cite{ouali2020semi} &ResNet-50 &69.4 \\
PseudoSeg \cite{zou2020pseudoseg} & ResNet-101 &73.2 \\
\hline
Ours &ResNet-101 &\textbf{75.0} \\
Ours &Xception-65 &\textbf{79.3} \\
Fully supervised &ResNet-101 &78.3 \\
Fully supervised &Xception-65 & 79.2 \\
\hlineB{2}
\end{tabular}
}
\end{center}
\caption{Comparison with  \SOTA on the Pascal VOC val set (w/ unlabeled data). We use the official training set (1.4k) as labeled data, and the augmented set (9k) as unlabeled data. 
}
\label{tab: SOTA in voc}
\vspace{-1em}
\end{table}
 




\section{Additional Ablation Study with DeepLabV2}

We conduct experiments to explore the effectiveness of each proposed module under \nicefrac{1}{8} labeled image settings. First, we establish the baseline for our experiments. We evaluate DeepLabV2 based on ResNet-101 on the validation set. Same as the experiments with the DeepLabV3Plus model,
self-training adopts the baseline model as the teacher model and the teacher model generates pseudo-labels on the unlabeled dataset, then the student model which is not smaller than the teacher model is trained on pseudo labels and original labeled images. We use the scales including  and mirror for the remaining images to generate pseudo labels. For fair comparison,
the student model is the same model initialized by the teacher model. The main results are shown in Table~\ref{tab: ablation for ist of deeplabv2}.




\begin{table}[tb]
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tabular}{l c c c c c c}
\hlineB{3}
Model &s.t.\  & \SAUG& DSBN &SCL &iter.\  & mIoU (\%) \\
\hline\hline
DeepLabV2     & & & & && 60.6\\
DeepLabV2     & & && & & 62.1 \\
DeepLabV2     & & && & &62.2 \\
DeepLabV2     & & & && & 63.8 \\
DeepLabV2     & & & && & 64.5 \\
DeepLabV2     & & & & && 67.6 \\
DeepLabV2-Full    & & & & && 70.1 \\
\hlineB{3}
\end{tabular}
 }
\end{center}
\caption{Ablation study on the proposed semi-supervised learning framework. Baselines are ResNet101-based Deeplabv2. 's.t.\' denotes self-training with pseudo labels without strong augmentation. \SAUG\  means the strong data augmentation. DSBN means the distribution specify batch normalization. SCL is the proposed self correction loss. `iter.' represents iterative training. The results are evaluated on the validation set with the single-scale input. Except the last row, 
all other experiments use  labeled data and the remaining images in the training set are used as unlabeled data. ` Full' means the fully supervised setting.}
\label{tab: ablation for ist of deeplabv2}
\end{table}




We also report detailed performance under different iterations in Figure \ref{fig:iterations for deeplabv2}. We can see that more iterations in general 
help improve the performance, but the growth trend of the performance is gradually flattening.






\begin{figure}
  \centering
    \includegraphics[scale=0.8]{images/paper_iters_deeplabv2.eps}
  \caption{Performance vs.\ number of iterations, using the DeepLabV2 model.}
	\label{fig:iterations for deeplabv2}
	\vspace{0.2in}
\end{figure}















\section{Conclusion}

In this work, we construct a simple semi-supervised learning framework for semantic segmentation. It employs strong augmentation with distribution-specific batch normalization, together with 
pseudo-label learning.
In particular, DSBN can effectively avoid the BN statistics shift caused by strong augmentation. Meanwhile, we have also designed a self-correction loss, which 
effectively alleviates the label noises introduced by pseudo labeling.
Quantitative and qualitative comparisons show that the proposed method performs favorably against recent state-of-the-art semi-supervised approaches.





{\small
\bibliographystyle{ieee_fullname}
\bibliography{FINAL}
}

\end{document}
