\def\tabCifar#1{
\begin{table}[#1]
\caption{
RIDE achieves the state-of-the-art results on \textbf{CIFAR100-LT} \textit{without} sacrificing the performance of many-shot classes like all previous methods.
Compared with BBN \citep{zhou2020bbn} and LFME \citep{xiang2020learning}, which also contain multiple experts (or branches), RIDE (2 experts) outperforms them by a large margin with fewer GFlops. The relative computation cost (averaged on testing set) with respect to the baseline model and absolute improvements against SOTA (colored in green) are reported.
 denotes our reproduced results with released code.  denotes results copied from \citep{cao2019learning} and the imbalance ratio is 100.
}\vspace{-2mm}
\label{tab:cifar100}
\begin{center}
\setlength{\tabcolsep}{2.6mm}
\begin{tabular}{l||l|l|l|l|l}
\shline
Methods  & \multicolumn{1}{c|}{MFlops} & \multicolumn{1}{c|}{All} & Many & Med & Few \\
\shline
Cross Entropy (CE)  & 69.5 (1.0x) & 38.3 & - & - & - \\ Cross Entropy (CE)  & 69.5 (1.0x) & 39.1 & \cellcolor{gray!25}66.1 & 37.3 & 10.6 \\ \hline
Focal Loss  \citep{lin2017focal} & 69.5 (1.0x) & 38.4 & - & - & - \\ OLTR  \citep{liu2019large} & - & 41.2 & 61.8 & 41.4 & 17.6 \\ LDAM + DRW \citep{cao2019learning} & 69.5 (1.0x) & 42.0 & - & - & - \\ LDAM + DRW  \citep{cao2019learning} & 69.5 (1.0x) & 42.0 & 61.5 & 41.7 & \cellcolor{gray!25}20.2 \\ BBN \citep{zhou2020bbn} & 74.3 (1.1x) & 42.6 & - & - & - \\ 

-norm  \citep{kang2019decoupling} & 69.5 (1.0x) & 43.2 & 65.7 & 43.6 & 17.3 \\ 

cRT  \citep{kang2019decoupling} & 69.5 (1.0x) & 43.3 & 64.0 & \cellcolor{gray!25}44.8 & 18.1 \\ 

M2m \citep{kim2020m2m} & - & 43.5 & - & - & - \\ LFME \citep{xiang2020learning} & - & \cellcolor{gray!25}43.8 & - & - & - \\ \hline
RIDE (2 experts) & \textbf{64.8 (0.9x)} & 47.0 {\small \textcolor{Green}{(+3.2)}} & 67.9 & 48.4 & 21.8 \\
RIDE (3 experts) & 77.8 (1.1x) & 48.0 {\small\textcolor{Green}{(+4.2)}} & 68.1 & 49.2 & 23.9\\
RIDE (4 experts) & 91.9 (1.3x) & \bf{49.1} {\small\bf\textcolor{Green}{(+5.3)}} & \bf{69.3} & \bf{49.3} & \bf{26.0} \\
\shline
\end{tabular}
\end{center}
\end{table}
}

\def\tabImageNetResAll#1{
\begin{table}[#1]
\caption{RIDE achieves state-of-the-art results on \textbf{ImageNet-LT} \citep{liu2019large} and obtains consistent performance improvements on various backbones.
The top-1 accuracy and computational cost are compared with the state-of-the-art methods on ImageNet-LT, with ResNet-50 and ResNeXt-50 as the backbone networks.
Results marked with  are copied from \citep{kang2019decoupling}. Detailed results on each split are listed in appendix materials.}\vspace{-2mm}
\label{table:imagenet-lt}
\begin{center}
\setlength{\tabcolsep}{3.3mm}
\begin{tabular}{l||l|l|l|l}
\shline
\multirow{2}{*}{Methods}& \multicolumn{2}{c|}{ResNet-50} & \multicolumn{2}{c}{ResNeXt-50} \\ 
& \multicolumn{1}{c}{GFlops} & \multicolumn{1}{c|}{Acc. (\%)} & \multicolumn{1}{c}{GFlops} & \multicolumn{1}{c}{Acc. (\%)} \\ [.1em]
\shline
Cross Entropy (CE)  & 4.11 (1.0x) & 41.6 & 4.26 (1.0x) & 44.4 \\ OLTR  \citep{liu2019large} & - & - & - & 46.3 \\ NCM \citep{kang2019decoupling} & 4.11 (1.0x) & 44.3 & 4.26 (1.0x) & 47.3 \\ -norm \citep{kang2019decoupling} & 4.11 (1.0x) & 46.7 & 4.26 (1.0x) & 49.4 \\ cRT \citep{kang2019decoupling} & 4.11 (1.0x) & 47.3 & 4.26 (1.0x) & 49.6 \\ LWS \citep{kang2019decoupling} & 4.11 (1.0x) & 47.7 & 4.26 (1.0x) & 49.9 \\ \hline
RIDE (2 experts) & \bf{3.71 (0.9x)} & 54.4 {\small\bf\textcolor{Green}{(+6.7)}} & \bf{3.92 (0.9x)} & 55.9 {\small\bf\textcolor{Green}{(+6.0)}} \\
RIDE (3 experts) & 4.36 (1.1x) & 54.9 {\small\bf\textcolor{Green}{(+7.2)}} & 4.69 (1.1x) & 56.4 {\small\bf\textcolor{Green}{(+6.5)}}\\
RIDE (4 experts) & 5.15 (1.3x) & \bf{55.4} {\small\bf\textcolor{Green}{(+7.7)}} & 5.19 (1.2x) & \bf{56.8} {\small\bf\textcolor{Green}{(+6.9)}} \\
\shline
\end{tabular}
\end{center}
\vspace{-16pt}
\end{table}
}


\def\tabiNaturalist#1{
\begin{table}[#1]
\caption{RIDE outperforms previous state-of-the-art methods on challenging \textbf{iNaturalist 2018} \citep{van2018inaturalist} dataset, which contains 8,142 classes, by a large margin.
Relative improvements to SOTA result of each split (colored with gray) are also listed, with the largest boost from few-shot classes. Compared with previous SOTA method BBN, which also contains multiple ``experts'', RIDE achieves more than 20\% higher top-1 accuracy on many-shot classes.
Results marked with  are from BBN \citep{zhou2020bbn} and Decouple \citep{kang2019decoupling}. 
BBN's results are from the released checkpoint. 
: Longer training for 200 epochs. : Longer training for 300 epochs without expert assignment module.
}\vspace{-2mm}
\label{table:inaturalist}
\begin{center}
\setlength{\tabcolsep}{2.8mm}
\begin{tabular}{l||l|llll}
\shline
Methods  & GFlops & All & Many & Medium & Few \\ 
\shline
CE  & 4.14 (1.0x) & 61.7 & 72.2 & 63.0 & 57.2 \\ \hline
CB-Focal  & 4.14 (1.0x) & 61.1 & - & - & - \\ OLTR & 4.14 (1.0x) & 63.9 & 59.0 & 64.1 & 64.9 \\ LDAM + DRW  & 4.14 (1.0x) & 64.6 & - & - & - \\ cRT & 4.14 (1.0x) & 65.2 & \cellcolor{gray!25}69.0 & 66.0 & 63.2 \\
-norm & 4.14 (1.0x) & 65.6 & 65.6 & 65.3 & \cellcolor{gray!25}65.9 \\
LWS & 4.14 (1.0x) & 65.9 & 65.0 & 66.3 & 65.5 \\
BBN & 4.36 (1.1x) & \cellcolor{gray!25}66.3 & 49.4 & \cellcolor{gray!25}70.8 & 65.3 \\
\hline RIDE (2 experts) & \bf{3.67 (0.9x)} & 71.4 {\small\bf\textcolor{Green}{(+5.1)}} & 70.2 {\small\bf\textcolor{Green}{(+1.2)}} & 71.3 {\small\bf\textcolor{Green}{(+0.5)}} & 71.7 {\small\bf\textcolor{Green}{(+5.8)}}\\
RIDE (3 experts) & 4.17 (1.0x) & 72.2 {\small\bf\textcolor{Green}{(+5.9)}} & 70.2 {\small\bf\textcolor{Green}{(+1.2)}} & 72.2 {\small\bf\textcolor{Green}{(+1.4)}} & 72.7 {\small\bf\textcolor{Green}{(+6.8)}}\\
RIDE (4 experts) & 4.51 (1.1x) & \bf{72.6} {\small\bf\textcolor{Green}{(+6.3)}} & \bf{70.9} {\small\bf\textcolor{Green}{(+1.9)}} & \bf{72.4} {\small\bf\textcolor{Green}{(+1.6)}} & \bf{73.1} {\small\bf\textcolor{Green}{(+7.2)}}\\
\hline
RIDE (4 experts)  & 4.73 (1.2x) & 73.2 {\small\bf\textcolor{Green}{(+6.9)}} & 70.5 {\small\bf\textcolor{Green}{(+1.5)}} & 73.7 {\small\bf\textcolor{Green}{(+2.9)}} & 73.3 {\small\bf\textcolor{Green}{(+7.4)}}\\
\hline
RIDE (6 experts)  & 9.83 (2.4x) & 74.6 {\small\bf\textcolor{Green}{(+8.3)}} & 71.0 {\small\bf\textcolor{Green}{(+2.0)}} & \bf{75.7} {\small\bf\textcolor{Green}{(+4.9)}} & 74.3 {\small\bf\textcolor{Green}{(+8.4)}}\\
\shline
\end{tabular}
\end{center}
\end{table}
}


\def\tabAblation#1{
\begin{table}[#1]
\vspace{-6pt}
\caption{\textbf{Ablation studies} on the effectiveness of each component on CIFAR100-LT. LDAM is used as our classification loss. The first 3 RIDE models only have architectural change without changes in training method. The performance without  checked indicates directly applying classification loss onto the final model output, which is the mean expert logits. This is referred to as collaborative loss above. In contrast, if  if checked, we apply individual loss to each individual expert. The difference between collaborative loss and individual loss is described above. 
By adding the router module, the computational cost of RIDE can be significantly reduced, while the accuracy degradation is negligible.
Knowledge distillation step is optional if further improvements are desired.
Various knowledge distillation techniques are compared in the appendix.
}\vspace{-3mm}
\label{table:ablation}
\begin{center}
\begin{tabular}{l|ccccc|ll}
\shline
Methods & \#expert &  &   & Router & distill & GFlops & Acc. (\%)\\ 
\shline
LDAM + DRW & 1 &&&&&& 42.0 \\
\hline
\multirow{6}{*}{RIDE} & 2 & &&&& 1.1x & 44.7 {\small\bf\textcolor{Green}{(+2.7)}}\\
& 3 & & & & & 1.5x & 46.1 {\small\bf\textcolor{Green}{(+4.1)}}\\
& 4 & & & & & 1.8x & 46.3 {\small\bf\textcolor{Green}{(+4.3)}}\\
& 4 & \checkmark & &&& 1.8x & 47.8 {\small\bf\textcolor{Green}{(+5.8)}}\\
& 4 & \checkmark & \checkmark &&& 1.8x & 48.7 {\small\bf\textcolor{Green}{(+6.7)}}\\
& 4 & \checkmark & \checkmark & & \checkmark & 1.8x & \bf{49.3} {\small\bf\textcolor{Green}{(+7.3)}}\\
& 4 & \checkmark & \checkmark & \checkmark & \checkmark & 1.3x & 49.1 {\small\bf\textcolor{Green}{(+7.1)}}\\
\shline
\end{tabular}
\end{center}
\end{table}
}

\def\tabImageNetSwinTransformer#1{
\begin{table*}[#1]
\tablestyle{11pt}{1.1}
\caption{Top-1 accuracy comparison on \textbf{ImageNet-LT} \cite{liu2019large} with \textbf{Swin-Transformer} \cite{liu2021swin} (Tiny and Small). Performance on Many-shot (100), Medum-shot (100 \& 20) and Few-shot (20) are also provided. Swin-T: Swin-Transformer Tiny. Swin-S: Swin-Transformer Small.}
\vspace{-6pt}
\label{table:imagenet_lt_swin_transformer}
\begin{center}
\begin{tabular}{l||l|ccc|l}
\shline
Methods & \multicolumn{1}{c|}{GFlops} & Many & Medium & Few & \multicolumn{1}{c}{Overall} \\
\shline
Swin-T & 4.49 (1.0x) & 63.7 & 34.7 & 10.3 & 42.6 \\
Swin-T + LDAM-DRW & 4.49 (1.0x) & 62.3 & 47.5 & 29.3 & 50.6 \\
Swin-T + RIDE (2 experts) &\bf{3.48 (0.8x)} & 65.0 & 50.0 & 34.1 & 53.6 {\small\bf\textcolor{Green}{(+3.0)}} \\
Swin-T + RIDE (3 experts) & 4.95 (1.1x) & \textbf{65.5} & \textbf{50.5} & \textbf{35.1} & \textbf{54.2} {\small\bf\textcolor{Green}{(+3.5)}} \\

\hline
Swin-S & 8.75 (1.0x) & 64.1 & 35.4 & 11.1 & 42.9 \\
Swin-S + LDAM-DRW & 8.75 (1.0x) & 61.8 & 45.3 & 29.6 & 49.5 \\

Swin-S + RIDE (2 experts) &\bf{8.32 (1.0x)} & \textbf{67.4} & \textbf{52.9} & 37.0 & \textbf{56.3} {\small\bf\textcolor{Green}{(+6.8)}} \\

Swin-S + RIDE (3 experts) & 9.68 (1.1x) & 66.9 & 52.8 & \textbf{37.4} & 56.0 {\small\bf\textcolor{Green}{(+6.5)}} \\

\shline
\end{tabular}
\end{center}
\end{table*}
}

\def\figBarPlot#1{
\begin{figure}[#1]
\begin{tabular}{ll}
  \includegraphics[width=0.48\textwidth]{figures/iNaturalist_new_f.pdf}&
  \includegraphics[width=0.48\textwidth]{figures/ImageNet-LT_New_f.pdf}\\
\end{tabular}\vspace{-5pt}
\caption{Compared to SOTAs, RIDE improves top-1 accuracy on all three splits (many-/med-/few-shot).
The absolute accuracy differences of RIDE (blue) over \textit{iNaturalist}'s current state-of-the-art method BBN \citep{zhou2020bbn} ({\bf{left}}) and \textit{ImageNet-LT}'s current state-of-the-art method cRT \citep{kang2019decoupling} ({\bf{right}}) are shown. RIDE improves the performance of few- and medium-shots categories without sacrificing the accuracy on many-shots, and outperforms BBN on many-shots by a large margin.
}
\label{fig:relative-improve}
\end{figure}
}

\def\figPieExpertsExperts#1{
\begin{figure}[#1]
    \centering
    \begin{minipage}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figures/cifar100_lt_experts_f.pdf}\vspace{-2mm} \caption{\textbf{\# experts vs. top-1 accuracy for each split} (All, Many/Medium/Few) of CIFAR100-LT. Compared with the many-shot split, which is 3.8\% relatively improved by adding more experts, the few-shot split can get more benefits, that is, a relative improvement of 16.1\%. }
        \label{figure:expertsNum}
    \end{minipage}\hfill
    \begin{minipage}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{figures/perc_experts_f.pdf}\vspace{-2mm} \caption{\textbf{The proportion of the number of experts} allocated to each split of CIFAR100-LT. For RIDE with 3 or 4 experts, more than half of many-shot instances only require one expert. On the contrary, more than 76\% samples of few-shot classes require opinions from additional experts.}
        \label{fig:pieChart}
    \end{minipage}
\end{figure}
}

\def\figMultiMethods#1{
\begin{figure}[#1]
\centering
\begin{minipage}[t]{0.5\textwidth}
    \vspace{0pt}
    \includegraphics[width=0.95\textwidth]{figures/bar_fat_f.pdf}
\end{minipage}\hfill
\begin{minipage}[t]{0.49\textwidth}
    \vspace{5pt}
    \caption{
    \textbf{RIDE is a universal framework} that can be extended to various long-tail recognition methods and obtain a consistent top-1 accuracy increase.
    RIDE is experimented on CIFAR100-LT and applied to various training mechanisms. By using RIDE, cross-entropy loss (\textit{without any re-balancing strategies}) can even outperforms previous SOTA method on CIFAR100-LT. Although higher accuracy can be obtained using distillation, we did not apply it here.}
    \label{fig:MultiMethods}
\end{minipage}
\vspace{-5pt}
\end{figure}
}

\def\figTsne#1{
\begin{figure}[#1]
    \centering
        \includegraphics[width=1\textwidth]{figures/tsne_all_f.pdf}\vspace{-4pt}
\caption{t-SNE visualization of LDAM's and our model's embedding space of CIFAR100-LT. The feature embedding of RIDE is more compact for both head and tail classes and better separated. This behavior greatly reduces the difficulty for the classifier to distinguish the tail category.}
    \label{fig:tsne_cifar100}
\end{figure}
}

\section{Experiments}
\label{experiments}

We experiment on major long-tailed recognition benchmarks and various backbone networks.
\begin{enumerate}[leftmargin=*,itemsep=0pt]
\item{\bf{CIFAR100-LT}} \citep{cao2019learning}: CIFAR100 is sampled by class per an exponential decay across classes.  We choose imbalance factor 100 and ResNet-32 \citep{he2016deep} backbone. 

\item{\bf{ImageNet-LT}} \citep{liu2019large}: Multiple backbone networks are experimented on ImageNet-LT, including ResNet-10, ResNet-50 and ResNeXt-50 \citep{xie2017aggregated}. All backbone networks are trained with a batch size of 256 on 8 RTX 2080Ti GPUs for 100 epochs using SGD with an initial learning rate of 0.1 decayed by 0.1 at 60 epochs and 80 epochs.
See more details and results on other backbones in Appendix.

\item{\bf{iNaturalist 2018}} \citep{van2018inaturalist}: It is a naturally imbalanced fine-grained dataset with 8,142 categories.
We use ResNet-50 as the backbone  and apply the same training recipe as for ImageNet-LT except batch size 512, as in \citep{kang2019decoupling}.
\end{enumerate}

\tabCifar{!h}

{\bf{CIFAR100-LT Results}}. Table \ref{tab:cifar100} shows that RIDE outperforms SOTA by a large margin on CIFAR100-LT. The average computational cost is even about 10\% less than baseline models with two experts as in BBN.  RIDE surpasses multi-expert methods, LFME \citep{xiang2020learning} and BBN \citep{zhou2020bbn},
by more than 5.3\% and 6.5\% respectively. 

\figMultiMethods{!h}

{\bf{RIDE as a universal framework}}. \fig{MultiMethods} shows that RIDE consistently benefits from better loss functions and training processes.  Whether the model is trained end-to-end ({\it focal loss, CE, LDAM}) or in two stages ({\it cRT, -norm, cosine}), RIDE delivers consistent accuracy gains.
In particular, RIDE with a simple cosine classifier, which we constructed by normalizing the classifier weights and retraining them with a long-tail re-sampling strategy (similar to cRT), achieves on-par performance with the current SOTA methods.  
\fig{MultiMethods} also shows that two-stage methods are generally better than single-stage ones.  Nevertheless, since they require an additional training stage, for simplicity, we use  the single-stage LDAM as the default  in RIDE throughout our remaining experiments.

\tabImageNetResAll{!t}

{\bf{ImageNet-LT Results}}.  Table \ref{table:imagenet-lt} shows that  RIDE outperforms SOTA, LWS and cRT, by more than 7.7\% with ResNet-50.  ResNeXt-50 is based on group convolution \citep{xie2017aggregated}, which divides all filters into several groups and aggregates information from multiple groups.  ResNeXt-50 generally performs better than ResNet-50 on multiple tasks.  It provides 6.9\% gain on ImageNet-LT.

\tabiNaturalist{!h}
\figBarPlot{!h}

{\bf{iNaturalist 2018 Results}}.  Table \ref{table:inaturalist} shows that RIDE outperforms current SOTA by 6.3\%. Surprisingly, RIDE obtains very similar results on many-shots, medium-shots and few-shots, ideal for long tailed recognition.  Current SOTA method BBN also uses multiple experts; however, it significantly decreases the performance on many-shots by about 23\%.   RIDE is remarkable at increasing the few-shot accuracy without reducing the many-shot accuracy. With longer training, RIDE obtains larger improvements.

{\bf{Comparing with SOTAs on iNaturalist and ImageNet-LT}}. As illustrated in \fig{relative-improve}, our approach provides a comprehensive treatment to all the many-shot, medium-shot and few-shot classes, achieving substantial improvements to current state-of-the-art on all aspects. Compared with cRT which reduces the performance on the many-shot classes, RIDE can achieves significantly better performance on the few-shot classes without impairing the many-shot classes. Similar observations can be obtained in the comparison with the state-of-the-art method BBN \citep{zhou2020bbn} on iNaturalist.

\tabAblation{!t}
{\bf{Contribution of each component of RIDE.}} 
RIDE is jointly trained with  and , we use LDAM for  by default. Table \ref{table:ablation} shows that the architectural change from the original ResNet-32 to the RIDE variant with 2  4 experts contributes 2.7\%  4.3\%  gain.  Applying the individual classification loss instead of the collaborative loss brings 1.5\% gain.  Adding the diversity loss further improves about 0.9\%. The computational cost is greatly reduced by adding the dynamic expert router.  Knowledge distillation from RIDE with 6 experts obtains another 0.6\% gain.  All these components deliver 7.1\% gain over baseline LDAM.

{\bf{Impact of the number of experts.}} Fig. \ref{figure:expertsNum} shows that whether in terms of relative or absolute gains, few-shots benefit more with more experts.  For example, the relative gain is 16\% vs. 3.8\% for few-shots and many-shots respectively. No distillation is applied in this comparison.

{\bf{The number of experts allocated to each split.}} \fig{pieChart} shows that instances in few-shots need more experts whereas most instances in many-shots just need the first expert.  That is, low confidence in tail instances often requires the model to seek a second (or a third, ...) opinion.

\figPieExpertsExperts{!t}

{\bf{ImageNet-LT with vision transformer results}}. Since the original vision transformer (ViT) \cite{dosovitskiy2021an} does not work well with medium or small size datasets, we use Swin-Transformer \cite{liu2021swin}, an efficient improvement on ViT, to investigate the effectiveness of our method on vision transformers. We choose ImageNet-LT since CIFAR 100-LT is too small for even Swin-T to perform well. We keep the first two stages intact and use multi-expert structure starting at the third stage. Similar to ResNet 50, we use 3/4 of the original dimensions for the later two stages. Since we found that Swin-S is already over-fitting the dataset, we do not report results with larger Swin-Transformer. For simplicity, we do not use distillation. We follow the original training recipe except that we use a higher weight decay for our models due to our larger model capacity to reduce overfitting. We train the model for 300 epochs, and we begin reweighting for LDAM-DRW at 240 epochs.

\tabImageNetSwinTransformer{ht}

In Table \ref{table:imagenet_lt_swin_transformer}, we found that Swin-T, without any modifications, performs a little better than ResNet-50 on ImageNet-LT. However, this is likely due to the increase in model size in terms of GFLops (4.49 vs 4.11 GFlops). Since vision transformers have relatively little inductive bias, it is not surprising to see large improvements with re-weighting and regularization with margins (i.e., LDAM-DRW), which offers knowledge of dataset distribution to the model. Even after this modeling of the training dataset, adding our method on top of it still gets around  improvement. What we observe on CNNs still hold: although we improve a little more in few-shot, our method improves the model in terms of three subsets. In contrast, re-weighting the training dataset in training improves overall accuracy through improving medium and few shots, it lowers the accuracy on ``Many" subset.  However, the trend that typically holds true on the original ImageNet does not always follows in long-tailed settings. In Swin-S, the performance stays the same when compared to Swin-T without modifications, even though Swin-S is about twice as large as Swin-T. Our method, with better modeling of the dataset, improves on the baseline algorithm by improving in a large amount in all splits, especially the ``Few" subset, reaching an improvement of 6.8\% with 2 experts. Our method does not improve further with 3 experts, which we believe is due to the size of ImageNet-LT dataset, as the size of ImageNet-LT is only a fraction of the size of the original ImageNet. Without enough data, the large Swin-Transformer model can easily overfit. This is also supported by the limited gains from Swin-S to Swin-B on the balanced ImageNet-1k in Swin-Transformer paper \cite{liu2021swin}.
