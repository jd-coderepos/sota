\documentclass{sigchi}

\toappear{\scriptsize Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \\
{\emph{L@S '20, August 12--14, 2020, Virtual Event, USA.} } \\
\copyright~2020 Association for Computing Machinery. \\
ACM ISBN 978-1-4503-7951-9/20/08\ ...\1.8\%1.8\%1.8\%I_1, \cdots, I_nI_i = (E_i, R_i)E_iiR_ir_iE_ir_i \in{\{0,1\}}1i0iE_1, \cdots, E_kR_1, \cdots, R_{k-1}kr_kE_iR_iE_1^e, \cdots, E_k^eR_1^e, \cdots, R_{k-1}^eE_iR_ir_iE_i^eR_i^eE_iE_i^eI_iE^e = [E_1^e, \cdots, E_k^e]O = [O_1, \cdots, O_k]OR^e = [S, R_1^e, \cdots, R_{k-1}^e]S\hat{r} = [\hat{r}_1, \cdots, \hat{r}_k]\hat{r}_kE_1, \cdots, E_kR_1, \cdots, R_{k-1}Q_\textrm{in}, K_\textrm{in}V_\textrm{in}hQ_\textrm{in}, K_\textrm{in}V_\textrm{in}W_i^QW_i^KW_i^VqkvQ_i K_i^T-\inftyhead_iV_idqkhW^OM = [M_1, \cdots, M_k] = \textrm{Multihead}(Q_\textrm{in}, K_\textrm{in}, V_\textrm{in})W_1^{FF}W_2^{FF}b_1^{FF}b_2^{FF}M_iNQ_\textrm{in}K_\textrm{in}V_\textrm{in}E^eNOQ_\textrm{in}K_\textrm{in}V_\textrm{in}R^er_kE_kk-1I = [I_1, \cdots, I_{k-1}]NIO = [O_1, \cdots, O_k]OE_k, \cdots, E_kkN-1Oi\hat{r}_{k, i}E_k(i-1)r_k(E_k)(I_{k-1}, E_k)\cdots(I_1, \cdots, I_{k-1}, E_k)\hat{r}_k\hat{r}_{k, i}I_1, \cdots, I_{k-1}E_1, \cdots, E_klr=0.001, \beta_1=0.9, \beta_2=0.999epsilon=1e-8warmup\_stepsNd\_modelNd\_model1-\text{specificity}$.
Sensitivity (resp. specificity) is the proportion of true positives (resp. negatives) that are correctly predicted to be positive (resp. negative). 
ACC is the proportion of predictions that were correct.
Table \ref{table:comparison} presents the overall results of our evaluation. It shows that our model outperforms the other models in both metrics.
Compared to the current state-of-the-art models, the ACC of SAINT is higher by 1.1\% and AUC is higher by 1.8\%. 
Other Transformer-based variants of deep knowledge tracing models described in Section \ref{section:variant} also perform better than existing approaches.

We visualize the attention weights of the fully trained, best-performing SAINT model to analyze the attention mechanism of SAINT.
The self-attention weights of the last encoder and decoder block show different tendencies (see Figure \ref{fig:sa}).
This shows that SAINT is capable of learning the different attention mechanisms appropriate for exercises and responses, and applying them separately.
Figure \ref{fig:en_de_mu} shows that each head of the first decoder block's encoder-decoder attention layer attends the exercise sequence in varying patterns. 
As seen in Figure \ref{fig:att_lay}, the span of attention in later decoder blocks are more diverse.
This can be interpreted as the attention mechanism capturing the increase in the complexity of the values that incorporate more complex relationships between the exercises and the responses as the values go through successive decoder blocks.

\subsection{Ablation Study}
\label{subsec:ablstudy}
In this section, we present ablation studies for the suggested models.
Firstly, we run ablation studies on each architecture with different hyper-parameters. 
Figure \ref{fig:ablation} shows that SAINT, which applies deep attention layers separately to exercises and responses, gives the best result.

The best performing model of SAINT has 4 layers and a latent space dimension of 512.
It shows an ACC of 0.7368 and AUC of 0.7811.
Next to SAINT, LTMTI models show high ACC and AUC overall.
This shows that the data augmentation effects from lower-triangular masks in LTMTI boosts ACC and AUC.

Secondly, we evaluate the best-performing SAINT model when trained with inputs of different levels of detail.
All models share the same embedding method for exercises. 
For response embeddings, Embedding A only uses positional information and the user response value to build the response embedding while Embedding B uses the exercise category, timestamp, and elapsed time in addition to the features used by Embedding A.
Table \ref{table:embedding} shows that using more information to construct embeddings did not improve results.

\section{Conclusion}
In this paper, we proposed SAINT, a state-of-the-art knowledge tracing model with deep attention networks.
We empirically demonstrated a Transformer-based architecture that shows superior performance on knowledge tracing tasks.
In addition, we showed through extensive experiments on the queries, keys, and values of attention networks that separately feeding exercises and responses to the encoder and decoder, respectively, is ideal for knowledge tracing tasks.
Furthermore, by investigating the attention weights of SAINT, we found that the results of self-attention of encoder and decoder exhibit different patterns. 
We suggested that this supports our idea that the separation of exercises and responses in input allows the model to find attention mechanisms that are especially suited to the respective input values.
Finally, Evaluation of SAINT on a large-scale knowledge tracing dataset showed that the model outperforms existing state-of-the-art knowledge tracing models.

\bibliographystyle{SIGCHI-Reference-Format}
\bibliography{ref}

\end{document}
