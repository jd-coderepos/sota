\def\year{2020}\relax
\documentclass[letterpaper]{article} \usepackage{aaai20}  \usepackage{times}  \usepackage{helvet}  \usepackage{courier}  \usepackage{url}  \usepackage{graphicx}  \usepackage{amsmath}
\usepackage{amssymb}\usepackage{pifont}\usepackage{soul}

\nocopyright

\usepackage{enumitem}
\usepackage{tabularx,tabulary}
\usepackage{algorithm2e}
\usepackage{algpseudocode}\usepackage{amsmath,amssymb} \usepackage[table]{xcolor} 

\definecolor{coolblack}{rgb}{0.0, 0.23, 0.64}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\hypersetup{
     citecolor = coolblack,
}


\newcommand{\etal}{\textit{et al}.}

\usepackage{multirow}
\usepackage{makecell}
\usepackage{amsmath}
\usepackage{bm}
\newcommand{\uveci}{{\bm{\hat{\textnormal{\bfseries\i}}}}}
\newcommand{\uvecj}{{\bm{\hat{\textnormal{\bfseries\j}}}}}
\DeclareRobustCommand{\uvec}[1]{{\ifcsname uvec#1\endcsname
     \csname uvec#1\endcsname
   \else
    \bm{\hat{\mathbf{#1}}}\fi
}}

\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\usepackage{xspace}
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}
\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother

\newcommand{\citet}[1]{\citeauthor{#1} \shortcite{#1}}
\newcommand{\citep}{\cite}
\newcommand{\citealp}[1]{\citeauthor{#1} \citeyear{#1}}

\definecolor{dgreen}{rgb}{0.04,0.7,0.13}
\newcommand{\cmark}{{\color{red}\ding{51}}}\newcommand{\xmark}{{\color{dgreen}\ding{55}}}\newcommand{\ncmark}{{\ding{51}}}\newcommand{\nxmark}{{\ding{55}}}

\frenchspacing  \setlength{\pdfpagewidth}{8.5in}  \setlength{\pdfpageheight}{11in}  \pdfinfo{
/Title (AAAI Paper-1 short term pose prediction)
/Author (AAAI Press Staff)}
\setcounter{secnumdepth}{2}  
 \begin{document}
\title{Kinematic-Structure-Preserved Representation for \\Unsupervised 3D Human Pose Estimation}
\author{Jogendra Nath Kundu\thanks{equal contribution}\qquad Siddharth Seth\footnotemark[1]\qquad Rahul M V\footnotemark[1]\qquad Mugalodi Rakesh\qquad \\
\bf \Large R. Venkatesh Babu\qquad Anirban Chakraborty
\\
Indian Institute of Science, Bangalore, India\\
{\small \{jogendrak, siddharthseth\}@iisc.ac.in, rmvenkat@andrew.cmu.edu, rakeshramesha@gmail.com, \{venky, anirban\}@iisc.ac.in}}


\maketitle
\begin{abstract}
Estimation of 3D human pose from monocular image has gained considerable attention, as a key step to several human-centric applications. However, generalizability of human pose estimation models developed using supervision on large-scale in-studio datasets remains questionable, as these models often perform unsatisfactorily on unseen in-the-wild environments. Though weakly-supervised models have been proposed to address this shortcoming, performance of such models relies on availability of paired supervision on some related tasks, such as 2D pose or multi-view image pairs. In contrast, we propose a novel kinematic-structure-preserved unsupervised 3D pose estimation framework\footnote{{\url{https://sites.google.com/view/ksp-human/}}}, which is not restrained by any paired or unpaired weak supervisions. Our pose estimation framework relies on a minimal set of prior knowledge that defines the underlying kinematic 3D structure, such as skeletal joint connectivity information with bone-length ratios in a fixed canonical scale. The proposed model employs three consecutive differentiable transformations named as forward-kinematics, camera-projection and spatial-map transformation. This design not only acts as a suitable bottleneck stimulating effective pose disentanglement, but also yields interpretable latent pose representations avoiding training of an explicit latent embedding to pose mapper.  Furthermore, devoid of unstable adversarial setup, we re-utilize the decoder to formalize an energy-based loss, which enables us to learn from in-the-wild videos, beyond laboratory settings. Comprehensive experiments demonstrate our state-of-the-art unsupervised and weakly-supervised pose estimation performance on both Human3.6M and MPI-INF-3DHP datasets. Qualitative results on unseen environments further establish our superior generalization ability.


\end{abstract}



\begin{table}[t]
	\footnotesize
	\caption{  
	Characteristic comparison of our approach against prior unsupervised and weakly-supervised human 3D pose estimation works, in terms of access to direct (paired) or indirect (unpaired) supervision levels (MV: Multi-View). Note that, in the proposed framework \textbf{the latent pose representation itself, is the 3D pose coordinates}, thereby avoiding training of a separate latent to 3D pose mapper (last column).}
	\centering
	\setlength\tabcolsep{2.4pt}
	\resizebox{0.47\textwidth}{!}{
	\begin{tabular}{l|ccc|cc|c}
	\hline
 		\multirow{2}{*}{Methods} & \multicolumn{3}{c|}{Paired sup.} &
 		\multicolumn{2}{c|}{\makecell{Unpaired sup.\
f_{am}^{(l)}(u) = \exp(-0.5||u_x^\prime/\sigma_x^{(l)}||^2-0.5||u_y^\prime/\sigma_y^{(l)}||^2) 



,  and  (collectively denoted as ) are designed using perfectly differentiable operations, thus allowing back-propagation of gradients from the loss functions defined at the decoder output. As shown in Fig.~\ref{fig:main1}A, the decoder takes in a tuple of spatial-pose-map representation and appearance ( and  respectively, concatenated along the channel dimension) to reconstruct an RGB image. To effectively disentangle BG information in , we fuse the background image  towards the end of decoder architecture, inline with~({\color{coolblack}Rhodin et al.~\citeyear{rhodin2018unsupervised}}).



\subsection{Access to minimal prior knowledge}
One of the key objectives of this work is to solve the unsupervised pose estimation problem with minimal access to prior knowledge whose acquisition often requires manual annotation or a data collection setup, such as CMU-MoCap
. Adhering to this, we restrict the proposed framework from accessing any paired or unpaired data samples as shown in Table~\ref{tab:char}. Here, we list the specific prior information that has been considered in the proposed framework,
\begin{itemize}[leftmargin=0.35cm]
\item Kinematic skeletal structure (\ie the joint connectivity information) with bone-length ratios in a fixed canonical scale. Note that, {we do not consider access to the kinematic angle limits} for the limb joints, as such angles are highly pose dependent particularly for diverse human skeleton structures~\cite{akhter2015pose}.

\item A set of 20 synthetically rendered SMPL models with diverse 3D poses and FG appearance~\cite{varol2017learning}. We have direct paired supervision loss (denoted by ) on these samples to standardize the model towards the intended 2D or 3D pose conventions (see Supplementary).
\end{itemize}


\begin{figure*}\begin{center}
	\includegraphics[width=1.0\linewidth]{image_final/aaai1_fig2_sid_final.pdf}
	\caption{\textbf{A.} Illustration of the steps to obtain the spatial heat-map and affinity-map from the projected 2D coordinates. \textbf{B.} An overview of the proposed data-flow pipeline enabling energy-based loss formalization targeting unpaired samples from .
	}
 	\label{fig:main2}    
    \vspace{-4mm}
\end{center}
\end{figure*}




\subsection{Unsupervised training procedure}
In contrast to~\cite{jakab2018unsupervised}, we aim to disentangle foreground (FG) and background (BG) appearances, along with the disentanglement of pose. In a generalized setup, we also aim to learn from in-the-wild YouTube videos in contrast to in-studio datasets, avoiding dataset-bias. 

\subsubsection{Separating paired and unpaired samples.}


For an efficient disentanglement, we aim to form image tuples of the form . Here,  and  are video frames, which have identical FG-appearance with a nonidentical \textit{kinematic-pose} (pairs formed between frames beyond a certain time-difference). As each video-clip captures action of an individual in a certain apparel, \textit{FG-appearance} remains identical among frames from the same video. Here,  denotes an estimate of BG image without the human subject corresponding to the image , which is obtained as the median of pixel intensities across a time-window including the frame . However, such an estimate of  is possible only for scenarios with no camera movement beyond a certain time window to capture enough background evidence (\ie static background with a moving human subject).

Given an in-the-wild dataset of videos, we classify temporal clips of a certain duration (5 seconds) into two groups based on the amount of BG motion in that clip. This is obtained by measuring the pixel-wise L2 loss among the frames in a clip, considering human action covers only 10-20\% of pixels in the full video frame (see Supplementary). Following this, we realize two disjoint datasets denoted by  and  as sets of tuples with extractable BG pair (paired) and un-extractable BG pair (unpaired), respectively.


\subsubsection{a) Training objective for paired samples, } As shown in Fig.~\ref{fig:main1}A, given a source and target image (\ie  and ), we aim to transfer the pose of  (\ie ) to the FG-appearance extracted from  (\ie ) and background from  to reconstruct .  Here, the FG and BG appearance information can not leak through pose representation because of the low dimensional bottleneck \ie . Moreover, consecutive predefined matrix  and  spatial-transformation  operations  further restrict the framework from leaking appearance information through the pose branch even as low-magnitude signals. Note that, the BG of  may not register with the BG of , when the person moves in the 3D world (even in a fixed camera scenario) as these images are outputs of an off-the shelf person-detector. As a result of this BG disparity and explicit presence of the clean spatially-registered background ,  catches the BG information directly from , thereby forcing  to solely model FG-appearance from the apparel-consistent source, . Besides this, we also expect to maintain perceptual consistency between  and  through the encoder networks, keeping in mind the later energy-based formalization (next section). Thus, all the network parameters are optimized for the paired samples using the following loss function, . Here,  and .

\subsubsection{b) Training objective for unpaired samples, } Although, we find a good amount of YouTube videos where human motion (e.g. dance videos) is captured on a tripod mounted static camera, such videos are mostly limited to indoor environments. However, a diverse set of human actions are captured in outdoor settings (e.g. sports related activities), which usually involves camera motion or dynamic BG. Aiming to learn a general pose representation, instead of ignoring the frames from video-clips with dynamic BG, we plan to formalize a novel direction to adapt the parameters of  and  even for such diverse scenarios.

We hypothesize that the decoder  expects the pose and FG-appearance representation in a particular form, satisfying the corresponding input distributions,  and . Here, a reliable estimate of  and  can be achieved solely on samples from  in presence of paired supervision, avoiding \textit{mode-collapse}. More concretely, the parameters of  should not be optimized on samples from  (as shown in Fig.~\ref{fig:main2}B with a lock sign). Following this, one can treat  analogous to a \textit{critic}, which outputs a reliable prediction (an image of human with pose from , FG-appearance from  and BG from ) only when its inputs  and  satisfy the expected distributions-  and  respectively. We plan to leverage this analogy to effectively use the frozen  network as an energy function to realize simultaneous adaptation of  and  for the unpaired samples from .

We denote  to represent a random background image. As shown in Fig.~\ref{fig:main2}B, here , in absence of access to a paired image to enforce a direct pixel-wise loss. Thus, the parameters of  and  are optimized for the unpaired samples using the following loss function, 
, where  and . Here,  and  represents a differentiable spatial transformation (such as image flip or in-plane rotation) and its inverse, respectively. We employ this to maintain a consistent representation across spatial-transformations. Note that, for the flip-operation of , we also exchange the indices of the joints associated with the left side to right and vice-versa.

We train on three different loss functions, viz. , and  at separate iterations, each with different optimizer. Here,  denotes the supervised loss directly on  and  for the synthetically rendered images on randomly selected backgrounds, as discussed before.


\begin{table*}[htp]
\caption{ 
	Results on Human3.6M following the standard protocol-II setup. Here, \textit{Sup.} (2nd column) denotes the amount of supervision accessed by the respective approaches. Accordingly, the table is divided into 4 row-groups, \textbf{a)} row 1-5 use full 3D pose sup., \textbf{b)} row 6-10 use full 2D pose as weak sup. \textbf{c)} row 11-12: unsupervised approaches, and \textbf{d)} row 13: \textit{Ours(semi-sup.)}. We outperform prior approaches in both weakly supervised and unsupervised setting (highlighted as boldface).
	}
	\centering
	\setlength\tabcolsep{2.6pt}
	\resizebox{1.0\textwidth}{!}{
	\begin{tabular}{l|c|ccccccccccccccc|c}
	\hline
 		Protocol-II & Sup. & Direct. & Disc. & Eat & Greet & Phone & Photo & Pose & Purch. & Sit & SitD & Smoke & Wait & Walk & WalkD & WalkT & Avg.() \\
		\hline\hline
({\color{coolblack}Akhter et al.} \citeyear{akhter2015pose}) & Full-3D & 199.2 & 177.6 & 161.8 & 197.8 & 176.2 & 186.5 & 195.4 & 167.3 & 160.7 & 173.7 & 177.8 & 181.9 & 198.6 & 176.2 & 192.7 & 181.1\\
\cite{zhou2016sparse} & Full-3D & 99.7 & 95.8 & 87.9 & 116.8 & 108.3 & 107.3 & 93.5 & 95.3 & 109.1 & 137.5 & 106.0 & 102.2 & 110.4 & 106.5 & 115.2 & 106.7 \\
\cite{bogo2016keep} & Full-3D &  62.0 & 60.2 & 67.8 & 76.5 & 92.1 & 77.0 & 73.0 & 75.3 & 100.3 & 137.3 & 83.4 & 77.3 & 79.7 & 86.8 & 87.7 & 82.3\\
({\color{coolblack}Moreno et al. }\citeyear{moreno20173d}) & Full-3D & 66.1 & 61.7 & 84.5 & 73.7 & 65.2 & 67.2 & 60.9 & 67.3 & 103.5 & 74.6 & 92.6 & 69.6 & 78.0 & 71.5 & 73.2 & 74.0 \\
\cite{martinez2017simple} &Full-3D & 44.8 & 52.0 & 44.4 & 50.5 & 61.7 & 59.4 & 45.1 & 41.9 & 66.3 & 77.6 & 54.0 & 58.8 & 35.9 & 49.0 & 40.7 & 52.1 \\
\hline

\cite{wu2016single} & Full-2D & 78.6 & {90.8} & 92.5 & 89.4 & 108.9 & 112.4 & 77.1 & {106.7} & 127.4 & 139.0 & 103.4 & 91.4 & 79.1 & - & - & 98.4 \\
\cite{tung2017adversarial} & Full-2D & {77.6} & 91.4 & {89.9} & {88.0} & {107.3} & {110.1} & {75.9} & 107.5 & {124.2} & {137.8} & {102.2} & {90.3} & {78.6} & - & - & {97.2} \\
		
		\cite{chen2019unsupervised} & Full-2D & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & 68.0 \\
		
({\color{coolblack}Wandt et al.} \citeyear{wandt2019repnet}) & Full-2D &  53.0 & 58.3 & 59.6 & 66.5 & 72.8 & 71.0 & 56.7 &  69.6 &  78.3 &  95.2 &  66.6 &  58.5 &  63.2 &  57.5 &  49.9 &  65.1 \\
		
		Ours (weakly-sup.) & Full-2D & 56.0 & 53.2 & 56.3 & 63.6 & 74.1 & 77.5 & 53.4 & 67.9 & 75.8 & 90.8 & 64.2 & 56.9 & 61.4 & 56.3 & 49.7 & \textbf{63.8} \\
		
        \hline
        \rowcolor{gray!10}
		({\color{coolblack}Rhodin et al.} \citeyear{rhodin2018unsupervised}) & {Multi-view} & - & - & - & - & - & - & - & - & - & - & - & - & - & - & - & 98.2 \\ 
		
		\rowcolor{gray!10}
		Ours (unsup.) & \textbf{No sup.} & 80.2 & 81.3 & 86.0 & 86.7 & 94.1 & 83.4 & 87.5 & 84.2 & 101.2 & 110.9 & 86.0 & 87.8 & 86.9 & 94.3 & 90.9 & \textbf{89.4} \\ \hline
        
        \rowcolor{gray!25}
		Ours (semi-sup.) & 5\%-3D & 46.6 & 54.5 & 50.1 & 46.4 & 81.3 & 42.4 & 41.1 & 56.4 & 86.7 & 82.9 & 49.0 & 47.7 & 64.1 & 48.2 & 44.3 & \textbf{56.1} \\
\hline
	\end{tabular}}
	\vspace{-4mm}
	\label{tab:protocol2results}
\end{table*} 



\begin{table}[htp]
	\footnotesize
	\caption{ 
	Results for the MPI-INF-3DHP dataset. Here, \textit{Trainset} (2nd column) denotes access to 3DHP trainset images before evaluation. \textit{Sup.} (3rd column) denotes supervision level on 3DHP image-pose pairs. 4 row-groups, \textbf{a)} row 1-2: Fully supervised, \textbf{b)} row 3-7: Weakly supervised, \textbf{c)} row 8-10: Unsupervised, \textbf{d)} row 11: Semi-supervised.
	}
	\centering
	\setlength\tabcolsep{2.0pt}
	\resizebox{0.48\textwidth}{!}{
	\begin{tabular}{ll|cc|ccc}
	\hline
 		No.&Method & Trainset & Sup. & PCK () & AUC () & MPJPE () \\
		\hline\hline
1.& \cite{mehta2017vnect} & +3DHP & Full-3D & 76.6 & 40.4 & 124.7 \\
2.&({\color{coolblack}Rogez et al.} \citeyear{rogez2017lcr}) & +3DHP & Full-3D & 59.6 & 27.6 & 158.4 \\		


		\hline
3.&\cite{zhou2017towards} & +3DHP & Full-2D & 69.2 & 32.5 & 137.1 \\
		
		4.&\cite{kanazawa2018end} & +3DHP & Full-2D & 77.1 &	40.7 & 113.2 \\
5.&\cite{yang20183d} & +3DHP & Full-2D & 69.0 & 32.0 & - \\
		6.&\cite{chen2019unsupervised} & +3DHP & Full-2D &   {71.7}   & {36.3}  & - \\




		7.&{Ours (weakly-sup.)} & +3DHP & Full-2D & \textbf{80.2} & \textbf{44.8} & \textbf{97.1} \\
		\hline
		
		\rowcolor{gray!10}
		8.&\cite{chen2019unsupervised} & -3DHP & - &   {64.3}   & {31.6}  & - \\
		\rowcolor{gray!10}
		9.&Ours (unsup.) & -3DHP & - & 76.5 & 39.8 & 115.3 \\ 
		\rowcolor{gray!10}
		10.&Ours (unsup.) & +3DHP & \textbf{No sup.} & \textbf{79.2} & \textbf{43.4} & \textbf{99.2} \\ \hline
		\rowcolor{gray!25}
		11.&Ours (semi-sup.) & +3DHP & 5\%-3D & \underline{81.9} & \underline{52.6} & \underline{89.8} \\
		\hline
	\end{tabular}}
	\vspace{-2mm}
	\label{tab:mpiinf3dhp}
\end{table} 




\section{Experiments}
In this section, we describe experimental details followed by a thorough analysis of the framework for bench-marking on two widely used datasets, Human3.6M and MPI-INF-3DHP.  




We use Resnet-50 (till \textit{res4f}) with ImageNet-pretrained parameters as the base pose encoder , whereas the appearance encoder is designed separately using 10 Convolutions.  later divides into two parallel branches of fully-connected layers dedicated for  and  respectively. We use  for all our experiments as shown in Fig.~\ref{fig:main1}. The channel-wise aggregation of  (16-channels) and  (17-channels) is passed through two convolutional layers to obtain  (128-maps), which is then concatenated with  (512-maps) to form the input for  (each with 1414 spatial dimension). Our experiments use different AdaGrad optimizers (learning rate: 0.001) for each individual loss components in alternate training iterations, thereby avoiding any hyper-parameter tuning. We perform several augmentations (color jittering, mirroring, and in-plane rotation) of the 20 synthetic samples, which are used to provide a direct supervised loss at the intermediate pose representations.

\textbf{Datasets.} The \textit{base-model} is trained on a mixture of two datasets, \ie Human3.6M and an in-house collection of YouTube videos (also refereed as YTube). In contrast to the in-studio H3.6M dataset, YTube contains human subjects in diverse apparel and BG scenes performing varied forms of motion (usually dance forms such as western, modern, contemporary etc.). Note that all samples from H3.6M contribute to the paired dataset , whereas 40\% samples in YTube contributed to  and rest to  based on the associated BG motion criteria. However, as we do not have ground-truth 3D pose for the samples from YTube (in-the-wild dataset), we use MPI-INF-3DHP (also refereed as 3DHP) to quantitatively benchmark generalization of the proposed pose estimation framework.

\subsubsection{a) Evaluation on Human3.6M.}
We evaluate our framework on protocol-II, after performing scaling and rigid alignment of the poses inline with the prior arts~({\color{coolblack}Chen et al.~\citeyear{chen2019unsupervised}}; {\color{coolblack}Rhodin et al.~\citeyear{rhodin2018unsupervised}}). We train three different variants of the proposed framework \ie a) \textit{Ours(unsup.)}, b) \textit{Ours(semi-sup.)}, and c) \textit{Ours(weakly-sup.)} as reported in Table~\ref{tab:protocol2results}. After training the \textit{base-model} on the mixed YTube+H3.6M dataset, we finetune it on the static H3.6M dataset by employing  and  (without using any multi-view or pose supervision) and denote this model as \textit{Ours(unsup.)}. This model is further trained with full supervision on the 2D pose landmarks simultaneously with  and  to obtain \textit{Ours(weakly-sup.)}. Finally, we also train \textit{Ours(unsup.)} with supervision on 5\% 3D of the entire trainset simultaneously with  and  (to avoid over-fitting) and denote it as \textit{Ours(semi-sup.)}. As shown in Table~\ref{tab:protocol2results}, \textit{Ours(unsup.)} clearly outperforms the prior-art~({\color{coolblack}Rhodin et al.~\citeyear{rhodin2018unsupervised}}) with a significant margin (89.4 vs. 98.2) even without leveraging multi-view supervision. Moreover, \textit{Ours(weakly-sup.)} demonstrates state-of-the-art performance against prior weakly supervised approaches. 

\begin{figure*}[!tbhp]\begin{center}
	\includegraphics[width=1.00\linewidth]{image_final/aaai1_fig4_sid_final1.pdf}
	\vspace{-5.2mm}
	\caption{\small 
	Qualitative results, showing disentanglement of Pose (ID'd as P1 and P2), FG (ID'd as A1 and A2) and BG (ID'd as B1, B2, and B3). Images in first column (of each panel) define the IDs which are later used for novel image synthesis. Devoid of a direct pixel-wise loss, energy-based losses for samples from , help to clearly separate the FG person even in absence of a BG estimate (right panel).
	}
    \vspace{-2mm}
    \label{fig:viewsyn}  
\end{center}
\end{figure*}



\begin{figure*}[!tbhp]\begin{center}
	\includegraphics[width=0.98\linewidth]{image_final/aaai1_fig3_sid_final.pdf}
	\vspace{-4mm}
	\caption{\small 
	Qualitative results on 4 different datasets. Note that, results on LSP is obtained in an unseen setting (\ie not even unpaired unsup. training). The pink box highlights some failure cases, specifically in presence of self-occlusion as a result of joint-position ambiguity.
	}
    \vspace{-2mm}
    \label{fig:qualitative}  
\end{center}
\end{figure*}


\subsubsection{b) Evaluation on MPI-INF-3DHP.}
We aim to realize a higher level of generalization in consequence of leveraging rich kinematic prior information. The proposed framework outputs 3D pose, which is bounded by the kinematic plausibility constraints even for unseen apparel, BG and action categories. This characteristic is clearly observed while evaluating performance of our framework on unseen 3DHP dataset. We take \textit{Ours(weakly-sup.)} model trained on YTube+H3.6M dataset to obtain 3D pose predictions on unseen 3DHP testset (9th row in Table~\ref{tab:mpiinf3dhp}). We clearly outperform the prior work~\cite{chen2019unsupervised} by a significant margin in a fully-unseen setting (8th and 9th row with -3DHP in Table~\ref{tab:mpiinf3dhp}). Furthermore, our weakly supervised model (with 100\% 2D pose supervision) achieves state-of-the-art performance against prior approaches at equal supervision level.



\begin{table}[t]
	\footnotesize
	\caption{ 
	Results on ablations of the proposed framework. It clearly highlights importance of , , and use of  in the unsupervised training pipeline. Notice the improvement in 3DPCK on the unseen 3DHP testset as a result of incorporating  in the unsupervised training pipeline.
	}
	\centering
	\setlength\tabcolsep{6.0pt}
	\resizebox{0.46\textwidth}{!}{
	\begin{tabular}{l|c|cc}
	\hline
 		\multirow{2}{*}{\makecell{Method\\ (unsup.)}} & 
 		{Training set} & \multirow{2}{*}{\makecell{MPJPE on\\ H36M}} & \multirow{2}{*}{\makecell{3DPCK on\\ MPI-3DHP}} \\ & YTube+H3.6M \\
		\hline\hline
		Ours w/o  &  & 134.8 & 47.9 \\
		Ours w/o  &  & 101.8 & 61.7 \\
		\hline
		Ours(unsup.) &  & 91.1 & 66.3 \\
		\rowcolor{gray!25}
		Ours(unsup.) &  & \textbf{89.4} & \textbf{71.2} \\
		\hline
	\end{tabular}}
	\vspace{-2mm}
	\label{tab:ablations}
\end{table} 





\subsubsection{c) Ablation study.}
In the proposed framework, our major contribution is attributed to the design of differentiable transformations and an innovative way to facilitate the usage of unpaired samples even in presence of BG motion. Though effectiveness of camera-projection has been studied in certain prior works~\cite{chen2019unsupervised}, use of forward-kinematic transformation  and affinity map in the spatial-map transformation  is employed for the first time in such a learning framework. Therefore, we evaluate importance of both  and  by separately bypassing these modules through neural network transformations. Results in Table~\ref{tab:ablations} clearly highlight effectiveness of these carefully designed transformations for the unsupervised 3D pose estimation task.

\subsubsection{d) Qualitative results.} Fig.~\ref{fig:viewsyn} depicts qualitative results derived from \textit{Ours(unsup.)} on in-studio H3.6M and in-the-wild YTube dataset. It highlights effectiveness of unsupervised disentanglement through separation or cross-transfer of apparel, pose, camera-view and BG, for novel image synthesis. Though, our focus is to disentangle 3D pose information, separation of apparel and pose transfer is achieved as a byproduct of the proposed learning framework. In Fig.~\ref{fig:qualitative} we show results on the 3D pose estimation task obtained from \textit{Ours(weakly-sup.)} model. Though we train our model on H3.6M, 3DHP and YTube datasets, results on LSP dataset~\cite{johnson2010clustered} is obtained without training on the corresponding train-set, \ie in a fully-unseen setting. Reliable pose estimation on such diverse unseen images highlights generalization of the learned representations thereby overcoming the problem of dataset-bias.


\section{Conclusion}




We present an unsupervised 3D human pose estimation framework, which relies on a minimal set of prior knowledge regarding the underlying kinematic 3D structure.
The proposed local-kinematic model indirectly endorses a kinematic plausibility bound on the predicted poses, thereby limiting the model from delivering implausible pose outcomes. Furthermore, our framework is capable of leveraging knowledge from video frames even in presence of background motion, thus yielding superior generalization to unseen environments. 
In future, we would like to extend such frameworks for predicting 3D mesh, by characterizing the prior knowledge on human shape, alongside pose and appearance. 






{
\noindent
\textbf{Acknowledgements.} This work was supported by a Wipro PhD Fellowship (Jogendra) and in part by DST, Govt. of India (DST/INT/UK/P-179/2017).}

{\small
  \bibliographystyle{aaai}
  \bibliography{ms}
}



\end{document}
