\pdfoutput=1


\documentclass[11pt]{article}

\usepackage[]{EMNLP2023}

\usepackage{times}
\usepackage{latexsym}

\usepackage{graphicx}
\usepackage{scalefnt}
\usepackage{svg}
\usepackage{comment}
\usepackage{booktabs}
\usepackage{array}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tablefootnote}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{enumitem}
\usepackage{xcolor, color, soul}
\usepackage[multiple]{footmisc}
\usepackage{amsmath}
\usepackage{csquotes}
\usepackage{fixltx2e}
\usepackage{hyperref}
\usepackage{pifont}
\usepackage{makecell}
\usepackage{longtable}

\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}
\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}}
\newcolumntype{R}[1]{>{\PreserveBackslash\raggedleft}p{#1}}
\newcolumntype{L}[1]{>{\PreserveBackslash\raggedright}p{#1}}

\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}

\newcommand\pwc{\textsc{PwC }}
\newcommand\tdms{\textsc{NLP-TDMS}}
\newcommand\tabext{\textsc{TabExtract}}
\newcommand\method{\textsc{InstrucTE}}
\newcommand\task{\textsc{Schema-to-Json}}
\newcommand\ours{\textsc{Schema-to-Json}}
\newcommand\data{\textsc{MlTables}}
\newcommand\codex{\textsc{Codex}}
\newcommand\axcell{\textsc{AxCell}}
\newcommand{\tkinstruct}{\textsc{T-Instruct}}
\newcommand{\chemtables}{\textsc{ChemTables}}
\newcommand{\discomat}{\textsc{DisCoMat}}

\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}}

\newcommand{\fan}[1]{\textcolor{blue}{\bf\small [#1 --Fan]}}
\newcommand{\ar}[1]{\textcolor{red}{\bf\small [#1 --Alan]}}
\newcommand{\jm}[1]{\textcolor{violet}{\bf\small [#1 --Junmo]}}

\usepackage[normalem]{ulem}
\newcommand{\gabis}[1]{\textcolor{orange}{\bf\small [#1 --Gabi]}}
\newcommand{\gabisrep}[2]{\gabis{\sout{#1} #2}}
\newcommand{\gabist}[1]{\gabis{\sout{#1}}}




\title{Schema-Driven Information Extraction from Heterogeneous Tables}





\author{Fan Bai Junmo Kang  Gabriel Stanovsky  Dayne Freitag Alan Ritter \\
 {College of Computing, Georgia Institute of Technology} \\
   {School of Computer Science and Engineering, The Hebrew University of Jerusalem} \\
   \text{Artificial Intelligence Center, SRI International} \\
  \small
 \texttt{\{fan.bai, alan.ritter\}@cc.gatech.edu}, \texttt{junmo.kang@gatech.edu}, \\
\small
\texttt{gabriel.stanovsky@mail.huji.ac.il}, \texttt{daynefreitag@sri.com} \\
 \\
 }

\begin{document}
\maketitle
\begin{abstract}

In this paper, we explore the question of whether large language models can support cost-efficient information extraction from tables. We introduce {\em schema-driven information extraction}, a new task that transforms tabular data into structured records following a human-authored schema. To assess various LLM's capabilities on this task, we develop a benchmark composed of tables from four diverse domains: machine learning papers, chemistry literature, material science journals, and webpages.  Alongside the benchmark, we present an extraction method based on instruction-tuned LLMs. 
Our approach shows competitive performance without task-specific labels, achieving F\textsubscript{1} scores ranging from 74.2 to 96.1, while maintaining great cost efficiency. Moreover, we validate the possibility of distilling compact table-extraction models to reduce API reliance, as well as extraction from image tables using multi-modal models. 
By developing a benchmark and demonstrating the feasibility of this task using proprietary models, we aim to support future work on open-source schema-driven IE models.\footnote{Our code and datasets are available at \url{https://github.com/bflashcp3f/schema-to-json}.}
\end{abstract} 

\section{Introduction}
\label{sec:intro}

\begin{figure}[!t]
\begin{center}
  \includegraphics[width=0.45\textwidth]
  {figures/task-crop.pdf}
\caption{Overview of Schema-Driven Information Extraction. The input includes two elements: the source code of a table and a human-authored extraction schema, outlining the target attributes and their data types.
The output consists of a sequence of JSON records that conform to the extraction schema. 
}
  \label{fig:task}
\end{center}
\end{figure}

Vast quantities of data are locked away in tables found in scientific literature, webpages, and more.
These tables are primarily designed for visual presentation, and the underlying data is typically not available in any structured format, such as a relational or graph database. Some table collections have simple or uniform structures \cite{cafarella2008webtables}, making them easy to convert to relational data, for example Wikipedia tables \cite{lebret2016neural,iyyer2017search}, however a lot of information is stored in tables with complex and varied layouts, such as tables of experimental data presented in scientific literature.

Prior work on extracting structured data from tables has focused on developing custom pipelines for each new table format or domain, for example extracting machine learning leaderboards from \LaTeX~result tables \cite{kardas-etal-2020-axcell}. 
Importantly, the development of these specialized pipelines necessitates domain-specific labeled data, which not only incurs a significant cost in collection for every new extraction task but also constrains their applicability outside the originating domain.





In this paper, we investigate whether recent advances in large language models can reduce the costs of extracting data from tables.  To do this, we present a new formulation of the table extraction problem, which we refer to as Schema-Driven Information Extraction. 
In Schema-Driven IE, the only human supervision provided is a schema that describes the data model, including the target attributes and their corresponding data types, and resembles a JSON schema,\footnote{\url{https://json-schema.org/}} with a few customizations. 
The output is a sequence of JSON objects describing each cell in the table, adhering to the provided schema.
For example, as demonstrated in Figure \ref{fig:task}, a domain expert outlines the attributes of interest related to experimental result cells in a machine learning table, and the model extracts JSON objects following this schema. 








To evaluate the ability of LLMs to perform Schema-Driven IE from tables, we introduce a new benchmark consisting of table extraction datasets in four diverse domains: machine learning papers, chemistry literature, material science journals, and webpages, each of which has a different data format (\LaTeX, XML, CSV, and HTML, respectively). We curate and annotate data for the first two domains, while adapting existing datasets for the latter two. 

We then use this benchmark to analyze the performance of both open-source and proprietary LLMs. 
We find that proprietary instruction-tuned LLMs perform surprisingly well across diverse domains and data formats, compared to open-source ones. For example, GPT-4 \cite{gpt4} and {\tt code-davinci} \cite{codex}, are capable of surprisingly accurate table extraction (ranging from 74.2 to 96.1 F\textsubscript{1}), given only a relevant data schema as supervision. This performance is comparable to fully supervised models, which operate at an F\textsubscript{1} range of about 64.1 to 96.1. We also present a number of analyses on various factors that are key to achieving good performance while minimizing inference costs, including retrieving text from outside the table, and an iterative error recovery strategy.
Moreover, we demonstrate the utility of Schema-Driven IE from tables by evaluating performance on the downstream task of leaderboard extraction from machine learning papers \citep{kardas-etal-2020-axcell}. We achieve competitive performance, rivaling state-of-the-art supervised methods that use purposely-built and trained pipelines.









While the performance of open-source models, such as StarCoder \cite{starcoder}, and Alpaca currently trails behind proprietary models, we show it is possible to distill compact table extractors, without sacrificing performance. 
Furthermore, we highlight the potential of multi-modal models, such as GPT-4V, to extract data from table images, significantly expanding the set of documents this approach could be applied to.
Through the introduction of a new benchmark for Schema-Driven IE, we aim to foster the development of future open-source instruction-following models that can perform this task without relying on proprietary models.


\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figures/main-crop.pdf}
    \caption{Left: Prompt formulation of our proposed method \method{}. Right: Illustration of our error-recovery strategy, which ensures the model compliance of the instructed cell traversal order and reduces inference costs.
}
    \label{fig:method}
\end{figure*}

\begin{comment}
In conclusion, we contribute as follows: 
\begin{itemize}
\item We leverage recent advances in large language models to simplify the process of information extraction from tables, introducing a new task, Schema-Driven IE, supported by a benchmark covering tables from four diverse domains.
\item We propose \method{}, a novel method that applies instruction-tuned LLMs to extract structured records from tabular data, using only a human-curated extraction schema, complemented with a cost-effective error-recovery strategy.
\item We not only show the impressive performance of the {\tt code-davinci-002} model on the \task{} benchmark but also demonstrate the potential of distilling compact table extraction models from its predictions, thereby reducing the reliance on black-box API-based models.
\end{itemize}

\end{comment}









\begin{comment}
\begin{itemize}
    \item There is a huge amount of valuable information locked in semi-structured tables (scientific literature, webpages, etc.)
    \item  These tables are generally designed for visual presentation, not to support structured queries or data analysis.  
    \item While there do exist uniform table formats (E.g. Wikipedia tables), a lot of data is stored in complex formats that vary across tables (e.g. tables found in scientific papers).
    \item Prior work has extracted structured data from complex table layouts, but this requires manual engineering for each new data source or domain.
    \item Recent progress on large language models trained on code and fine-tuned to follow human instructions raises the question: how well can LLMs understand semi-structured data?
    \item In this paper, we develop a benchmark of three datasets consisting of LaTeX tables from ML papers, XML tables from biomedical journals, and semi-structured data in HMTL webpages.
    \item We demonstrate that large instruction-tuned language models can work surprisingly well at the task of extracting structured data from semi-structured tables.  One can simply prompt LLMs with a table, optionally including relevant context from the caption, paper abstract, etc. along with instructions on how the data should be formatted. 
    \item To extract information from a new data source or domain, only a schema for the new data is required.  There is no need for labeled data or domain-specific engineering.
    \item Furthermore we present a detailed analysis of factors needed for good performance.  We show that XXX all improve performance, and YYY is crucial.
    \item Because good performance on instructing language models to extract data from semi-structured tables currently relies on closed models that are only accessible through APIs, we also experiment with knowledge distillation.  We show how it is possible use a LLM to generate targets for fine-tuning a more compact student model, without sacraficing too much performance.
\end{itemize}
\end{comment}




\section{Schema-Driven Information Extraction}
\label{sec:task_def}

We present a new task for extracting structured records from tables, where the only supervision is a schema describing the desired output.
As shown in Figure \ref{fig:task}, the task input contains two elements: 1) a table , comprised of  cells , optionally supplemented with contextual text;  and 2) an extraction schema  that specifies extracted attributes for  types of records, each containing  attributes  along with their respective data types (formatted as JSON templates in implementation). 
Given the input, the model generates a sequence of  JSON objects, represented as . Each object  corresponds to one record type and comprises  key-value pairs , where  is the value of attribute  extracted for cell . 


Consider a table in an ML paper that displays various models' results.
Our proposed task enables the extraction of result records from each cell in the table.  These records include relevant attributes such as the evaluation metric, task, etc, which are structured in corresponding JSON objects and could facilitate meta-analysis of experiments or support research on reproducibility.







\subsection{Instruction-Based Table Extractor}
\label{sec:task}








To perform Schema-Driven IE on tables, we introduce \textbf{\method{}}, a method to extract structured records from a broad range of semi-structured data, using only task-specific instructions.
\method{} models information extraction as template filling, where the extraction schema is represented as a series of JSON templates. The underlying LLM is instructed to select the appropriate template and populate it with extracted values. As illustrated in Figure \ref{fig:method} (left), the prompt used by \method{} consists of four key components: an input table supplemented with text from the same document, an extraction schema, task-specific instructions, and an initial record for starting the process. For more details on prompt formulation, please refer to Appendix \ref{sec:promp_appendix}. 












Despite our explicit instructions, we found that models often fail to generate all cell descriptions from a single prompt, e.g., deviating from the instructed cell traversal order, leading to partial extraction.
To mitigate this, we employ an iterative error recovery method to refine the model's output when it violates the instructions. 
As shown on the right side of Figure \ref{fig:method}, we detect deviations from the instructed \textit{left-right, top-down} order by comparing predicted cell values with those from a rule-based detector (see Appendix \ref{sec:promp_appendix}) that follows the set order. Then, we truncate the LLMâ€™s output and re-prompt the model with the truncated sequence, adding the value of the next target cell.
This strategy guides the model to adhere to the instructed order, and it continues until all records are generated.
In Section \ref{sec:ablation}, we show that it can achieve comparable results to cell-by-cell prompting while significantly reducing API costs.









\section{A Table Extraction Benchmark}
\label{sec:eval}

We now present the details of our benchmark, \task{}, which is designed to assess the capabilities of LLMs to extract data from tables following a schema. This benchmark comprises table extraction datasets across four diverse domains: machine learning papers, chemistry literature, material science journals, and webpages. Each domain adheres to a unique textual format, namely, \LaTeX, XML, CSV, and HTML, requiring models to generalize across domains and formats. We manually annotate datasets for the first two domains, while the datasets for the latter two are adapted from pre-existing datasets, tailored to fit our task. Statistics of the four datasets are summarized in Table \ref{tab:dataset_stat}. 














\paragraph{\data{}} We create \data{}, a manually annotated dataset 
focused on tables in arXiv machine learning papers, particularly emphasizing numeric cells that are classified into four categories: Result, Hyper-parameter, Data Statistics, or Other. Extraction attributes are pre-defined for the first three categories; for instance, Result records incorporate textual 
attributes such as \textit{evaluation metric} (e.g., F\textsubscript{1}) and \textit{dataset} (e.g., SQuAD), along with dictionary-based attributes like \textit{experimental settings}. 
We collect papers from three pertinent arXiv fields: Machine Learning, Computer Vision, and Computation and Language, and a random sample of 10 papers is selected from each field.\footnote{We limit the paper selection to those published between Oct. and Nov. 2022 to avoid contamination with GPT-4 (\texttt{0613}) and GPT-3.5, which were trained on data until Sep. 2021.} After removing papers without \LaTeX{} source code or any tables, a total of 25 papers is covered in our dataset. 





To optimize the annotation budget and the dataset diversity, we cap the number of annotated tables to five per paper. Recognizing the domain-specific expertise needed, we employ expert annotators with backgrounds in ML research, who are provided with tables in both \LaTeX~and PDF formats and encouraged to thoroughly read the paper before annotation. The annotation process comprises two steps: 1) identifying the numeric cells and their record types, and 2) filling in the slots of pre-determined attributes, forming a JSON record with keys as attribute names and values as extracted content, in a text editor. Consequently, \data{} contains 122 tables, with 3,792 cells and 21K attributes annotated. For more details of \data{}, such as pre-defined attributes and inter-annotator agreement (IAA) score, see Appendix \ref{sec:mltab_appendix}.




\newcommand{\subfifty}[1]{}
\newcommand{\icfifty}{\subfifty{IC}}



\paragraph{\chemtables{}} In addition to \data{}, we annotate a new corpus of tables describing the physical properties of chemical compounds. The automated extraction of physical properties from such tables could provide substantial real-world benefits, for example collecting much-needed data for training ML models that can support inverse molecular design \citep{kim2018deep} and thus accelerating the drug design process \citep{Fields19,STOKES2020688}. 
Here, we focus on cells concerning five important physical properties: \icfifty{}, \subfifty{EC}, \subfifty{GI}, \subfifty{CC}, and MIC.\footnote{\url{https://www.sciencedirect.com/topics/pharmacology-toxicology-and-pharmaceutical-science/ic50}}
Three common attributes are manually extracted from tables for all properties: \emph{unit}, \emph{treatment} (experimental compound), and \emph{target} (measured biological entity, usually a gene expression or disease organism). 
To collect tables, we search through the Open Access subset of PubMed Central,\footnote{\url{https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/}} identifying tables that explicitly mention any of the five target physical properties within the XML paper sources. 
Similar to \data{}, domain experts annotate JSON records for relevant cells (IAA score in Appendix \ref{sec:chemtab_appendix}).
















\paragraph{\discomat{} \citep{gupta2022discomat}} Besides the two newly introduced datasets, we incorporate \discomat{}, an existing dataset focusing on glass composition tables from Elsevier material science journals. The task in \discomat{} is to extract tuples comprising (\textit{material}, \textit{constituent}, \textit{percentage}, \textit{unit}) from given tables. We adapt \discomat{} to fit our Schema-Driven IE framework by grounding the \textit{percentage} element to numeric cells in the table and considering the other elements as attributes. The model is tasked to identify numeric cells representing constituent percentages and predict the associated three attributes. The dataset consists of 5,883 tables from 2,536 papers and includes a distantly supervised training set (using MatSci DB) along with a manually annotated evaluation set.\footnote{In the released corpus, tables are represented as matrices; we, therefore, transform them into CSV tables (using the pipe symbol "|" as the delimiter) prior to feeding them into LLMs.} We refer readers to \citet{gupta2022discomat} for more details of \discomat{}.

\paragraph{\textsc{SWDE} \citep{swde}} In addition to the three scientific literature-based corpora, we add SWDE (Structured Web Data Extraction) as a fourth dataset, aimed at extracting pre-defined attributes from HTML webpages. This dataset comprises roughly 124K pages gathered from eight distinct verticals, such as \textit{Autos}, \textit{Books}, and \textit{Movies}. Each vertical includes ten unique websites (200 to 2000 pages for each website) and is associated with a set of 3 to 5 target attributes for extraction (refer to Table \ref{tab:swde_stats} in the appendix for detailed statistics and vertical-specific attributes).
Due to API budget limitations, 
we estimate InstrucTE performance on a sample of 1,600 pages (20 per website),
and compute a boostrap confidence interval to ensure this provides a reasonable performance estimate.\footnote{To validate that this subset is representative of the larger dataset, we estimate a 95\% confidence interval for \method{}'s performance using 1,000 bootstrap samples, and the resultant margin of error is 0.009, suggesting the sampling strategy is unlikely to substantially impact our performance estimate. \label{footnote:bootstrap}}










\begin{table}[tb!]
\small
\begin{center}
\scalebox{0.77}{
\begin{tabular}{lrrrr}
\toprule


  & \textbf{\textsc{MlTab.}}  & \textbf{\textsc{ChemTab.}} & \textbf{\discomat{}} & \textbf{\textsc{SWDE}}  \\
& {(ours)}  & {(ours)} & {(\citeyear{gupta2022discomat})} & (\citeyear{swde}) \\
  \midrule
\normalsize{\# cell types} & \normalsize{4} & \normalsize{6} & \normalsize{2} & \normalsize{8} \\
\normalsize{\# attr. types} & \normalsize{11} & \normalsize{4} & \normalsize{4} & \normalsize{32} \\
\normalsize{\# papers (web.)} & \normalsize{25} & \normalsize{16} & \normalsize{2,536} & \normalsize{80} \\ 
\normalsize{\# tables (pages)} & \normalsize{122} & \normalsize{26} & \normalsize{5,883} & \normalsize{1,600}  \\
\normalsize{\# anno. records} & \normalsize{3,792} & \normalsize{1,498} & \normalsize{58,481} & \normalsize{1,600} \\
\normalsize{\# records / table} & \normalsize{31.1} & \normalsize{57.6} & \normalsize{9.9} & \normalsize{1} \\
\bottomrule
\end{tabular}
}
\end{center}
\caption{\label{tab:dataset_stat} Dataset statistics of four datasets in our \task{} benchmark. 
}
\end{table}


\section{Experiments}
\label{sec:exp}











\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figures/main_results.pdf}
    \caption{Capability of various LLMs to perform Schema-Driven IE, measured using the \task{} benchmark. 
As noted in Section \ref{sec:eval_metric}, Table-F\textsubscript{1} is employed for \data{} and \chemtables{}, while Tuple-F\textsubscript{1} is used for \discomat{}. For SWDE, we report Page-F\textsubscript{1}, and  represents the number of websites used in training from each vertical. The *GPT-4 result on SWDE is computed on a 1,600 webpage sample due to API budget limitations - see Section \ref{sec:eval} and Footnote \ref{footnote:bootstrap}.
}
    \label{fig:main_results}
\end{figure*}


We evaluate the capability of various LLMs to perform schema-driven information extraction, in addition to full fine-tuning using our benchmark.
For machine learning and chemistry tables, we use a subset of 10 and 7 randomly sampled papers separately for model development, which facilitates the training of supervised models, thereby enabling a comprehensive comparison with a schema-driven approach. For the two pre-existing datasets, we follow data splits used in the original experiments.

\subsection{Evaluation Metrics}
\label{sec:eval_metric}
We introduce Table-F\textsubscript{1}, a new metric gauging overall attribute prediction performance within a table, for our two proposed datasets. Table-F\textsubscript{1} represents the harmonic mean of precision and recall, with precision being the ratio of
correctly predicted attributes to total predicted attributes.
At the attribute level, we consider two metrics: token-level F\textsubscript{1} and exact match (EM). For token-level F\textsubscript{1}, a prediction is deemed correct if the score exceeds a specific threshold, which is determined by maximizing the alignment between the model predictions and human judgments on the dev set of the dataset (see Appendix \ref{sec:eval_metric_app}). 
We report macro-averaged Table-F1 given the wide variance in table sizes.

For \discomat{} and SWDE, we use the metrics specified in the original papers. In the case of \discomat{}, we report Tuple-F\textsubscript{1} (\citealp{gupta2022discomat}), where a predicted 4-element tuple is considered correct only if it exactly matches with the gold tuple. For SWDE,  we report Page-F\textsubscript{1} \citep{swde}, which measures the number of pages where the attributes are accurately predicted.\footnote{Notably, SWDE primarily focuses on identifying textual HTML nodes containing attribute values rather than exact text spans, so we use token-level F\textsubscript{1} to identify the most relevant HTML node for each extracted attribute.} 











\subsection{Baselines \& Implementation Details}
\label{sec:baseline}

For \data{} and \chemtables{}, we experiment with various LLMs as the backbone for \method{}, including API-based GPT-4 and GPT-3.5 models and open-source models, such as LLaMA-7B \citep{touvron2023llama}, Alpaca-7B \citep{alpaca}, and StarCoder-15.5B \citep{starcoder}.
Additionally, we frame Schema-Driven IE as a TableQA problem, where we predict each attribute via a separate question over the input table, 
and use Flan-T5-11B \citep{flant5} as a competitive prompting baseline. 
Furthermore, we incorporate T5-11B \citep{t5} and TaPas \citep{herzig-etal-2020-tapas}, a table-specialized LM, for fine-tuning experiments to facilitate comprehensive comparison with our proposed \method{}. For implementation details of \method{} and baselines, see Appendix \ref{sec:implement_details}.







For \discomat{} and SWDE, we compare \method{} with existing methods, which either design task-specific architectures, such as FreeDom \citep{freedom} and LANTERN \citep{lantern}, or use LMs pretrained on tables or web pages, like TaPas \citep{herzig-etal-2020-tapas}, TaBERT \cite{yin2020tabert}, and MarkupLM \citep{li-etal-2022-markuplm}. 

























\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/ablation.pdf}
    \caption{Ablation studies on various components of our \textsc{InstrucTE} (w/ \texttt{code-davinci-002}) on the \data{} dataset. 
Interestingly, excluding the table caption improves performance. Our detailed analysis in Appendix \ref{sec:error_analysis_of_caption} reveals that low-quality captions (e.g., lack of specificity) may confuse the model, leading to inaccurate predictions.}
    \label{fig:ablation}
\end{figure*}


\subsection{Main Results}
\label{sec:main_results}
Figure \ref{fig:main_results} presents the main results from the comparison between \method{} and various baselines on our \task{} benchmark.
We observe that \method{}, in conjunction with GPT-4, achieves surprisingly strong performance across domains and input formats, supervised solely by human-authored schemas. 


For \data{} and \chemtables{}, \method{} outperforms all baseline models, including the fine-tuned T5-11B. However, a noticeable gap remains compared to human performance, e.g., the Table-F\textsubscript{1} on double-annotated examples for \data{} stands at 96.6 when applying thresholded token-level F\textsubscript{1} for attribute matching, which is 22.4 F\textsubscript{1} points higher than \method{}. 
For \discomat{} and SWDE, prompting-based \method{} performs on par or slightly trails behind the fully supervised state-of-the-art methods, signifying the potential of LLMs to act as flexible, powerful tools for extracting information from tables across diverse data formats and domains.






\subsection{Analysis \& Ablation Studies}
\label{sec:ablation}


We analyze the impact of different modules in \textsc{InstrucTE}, including prompt formulations, retrieved information and error recovery, using \data{}.





\paragraph{LLMs \& Prompt Formulations} In Table \ref{tab:test}, we compare different LLMs for \textsc{InstrucTE}, and three key observations emerge: 1) open-source models of similar scales (for instance, those in the 6-7B range) tend to achieve comparable fine-tuning performance, though they might exhibit variations in prompting performance; 2) notably, GPT-4-based \method{} demonstrates strong performance, outperforming open-source instruction-based models as well as fine-tuning models with limited training data; and 3) among the three GPT-3.5 backbones, \texttt{code-davinci-002} performs the best, followed by \texttt{text-davinci-003} and \texttt{gpt-3.5-turbo}, implying that optimizing for code generation could benefit Schema-Driven IE more compared to reinforcing chat capabilities. Subsequently, we compare two prompt formulations: \task{} and TableQA. From the T5-11B fine-tuning experiments, we observe that \task{} attains better performance, suggesting that incorporating task-specific instructions along with an extraction schema within the input could enhance the extraction process.





\paragraph{Prompt Components \& Error Recoverery}
Figure \ref{fig:ablation} shows \method{}'s performance subject to the exclusion of varying prompt components. Here we use \texttt{code-davinci-002} as the backbone LM due to API budget limitations as well as its resemblance of GPT-4 on performance and context length.\footnote{\texttt{code-davinci-002} is freely accessible through the OpenAI Researcher Access Program.}
We can see that removing supplementary text decreases the performance, highlighting the importance of supplementary textual information and the model's ability to encode longer sequences.  As for table elements, table headers contribute positively as expected, while captions surprisingly do not. Further analysis on table captions is provided in Appendix \ref{sec:error_analysis_of_caption}, which suggests that unclear captions can sometimes mislead the model, resulting in inaccurate predictions.
Notably, discarding the extraction schema, specifically JSON templates, causes a substantial performance decline, primarily due to attribute name mismatches in the evaluation.
Lastly, we show that \method{}'s performance drops significantly without error recovery. 
Compared to a cell-by-cell prompting strategy, error recovery offers similar performance at a fraction of the API cost (\670 based on the pricing on Azure), underscoring its importance to \method{}.\footnote{The pricing for \texttt{code-davinci-002} on \href{https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/}{Azure} is \\times0 \pm 0\geq$ 0.5), with the decision for each sub-attribute being based on the same as in the text-span attributes. For the set of pairs to annotate and use as a test set, we sample a total of 100 cell pairs (i.e., 677 attribute pairs) according to the following process: 1) we first uniformly sample a table from the development set (containing 10 papers); and 2) we then sample a random cell from the table, ensuring there were no duplicate cells. For each pair of predicted-gold attributes, each metric's decision (1 or 0) is made using a specific threshold. For example, if the token-level F\textsubscript{1}'s score for paired attributes is 0.4 and the threshold is 0.5, then the decision would be 0, indicating no match. The decisions over the test set containing 677 attribute pairs are then compared to human evaluation. In this binary classification problem, F\textsubscript{1} is used to evaluate the performance of the metrics.


In Table \ref{tab:metrics}, we present the performances of each metric with the optimal threshold for each. Surprisingly, we find that the token-level F\textsubscript{1} (with a threshold of 0.25) decision aligns nearly perfectly with human judgment, and performs the best among all metrics for our task. This might suggest that discerning subtle differences is more crucial than identifying different phrases with the same meaning for this task. Based on these empirical findings, we opt for the token-level F\textsubscript{1} for automatic evaluation at the attribute level. This choice is highly desirable not only because of its high accuracy but also due to its simplicity.




\begin{table}[t]
\centering
\scalebox{0.75}{
\begin{tabular}{lccc}
\toprule 
 & \textbf{token-level F\textsubscript{1}} & \textbf{SBERT} & \textbf{BERTScore} \\ 
\midrule
Meta Eval. F\textsubscript{1} & 97.0 & 95.6 & 96.7 \\
Threshold & 0.25 & 0.55 & 0.85 \\
\bottomrule
\end{tabular}
}
\caption{
Results of comparing various metrics, including token-level F\textsubscript{1}, SBERT, and BERTScore, to human judgment. 
Numbers are computed over 677 sampled attributes that are paired with gold references. The highest achieved F\textsubscript{1} scores are displayed alongside the thresholds. A complete illustration of results, sorted by thresholds, can be found in Figure \ref{fig:metrics} in Appendix.
}
\label{tab:metrics}
\end{table}


\section{Implementation Details}
\label{sec:implement_details}

Considering the lengthy source code for tables, we employ different strategies to encode the input table and perform Schema-Driven IE, based on the context length of the chosen LLM. For LLMs with a larger context length, such as \texttt{code-davinci-002} and StarCoder (8K tokens), we input the full table and conduct the proposed error recovery process up to 25 iterations. 
For LLMs with a more limited context length, such as LLaMA, Alpaca, and T5-11B, we query each target cell individually. The input table is condensed by rows, retaining only the first two rows, typically containing headers, and the row with the query cell, with a special token \texttt{<select>} pinpointing the position of the query cell. We use greedy decoding to maximize the reproducibility of our results.

For the TableQA setting, we divide the problem into two steps: selecting the record type and predicting the relevant attributes. For T5 and Flan-T5, the first step is modeled as a multi-choice QA problem, where the model chooses the type of the query cell from a list of provided options. The second step is modeled as an extractive QA task, asking the model to pinpoint the answer spans for the attributes associated with the selected type. For TaPas, the initial step is treated as a classification problem, whereas the latter one is handled as a cell selection problem. The hyper-parameters used for fine-tuning T5 and TaPas are presented in Table \ref{tab:hyperparam_ft}.

\begin{table}[t]
\centering
\scalebox{0.75}{
\begin{tabular}{lcc}
\toprule 
 & \textbf{T5 (11B)} & \textbf{TaPas}  \\ 
\midrule
learning rate & 1e-4 & 5e-5  \\
batch size & 8 & 32  \\
\# epoches  & 5 & 10 \\
\bottomrule
\end{tabular}
}
\caption{Hyper-parameters used for fine-tuning T5 and TaPas.
}
\label{tab:hyperparam_ft}
\end{table}


\section{Error Analysis of Caption}
\label{sec:error_analysis_of_caption}

In Section \ref{sec:ablation}, we observe an unexpected finding that table captions do not enhance performance, but rather seem to detract from it, which is counterintuitive. To delve deeper into this observation, we conduct an error analysis. This involves comparing the performances of our \textsc{InstrucTE} system with and without captions at the table level. This analysis uncovers a few outliers (3 out of 68) where including a caption leads to a 0 F\textsubscript{1} score, whereas the score is near perfect when the caption is excluded.
For instance, as depicted in Figure \ref{fig:analysis_of_caption}, the predictions all fall into the ``Other'' category when a caption is included, leading to a 0 F\textsubscript{1} score in these outlier instances. Conversely, removing the caption results in an F\textsubscript{1} score of 89.3. This high score is due to the fact that retrieved paragraphs provide ample contextual information (e.g., ``hate speech detection'') even without the presence of a caption.


We hypothesize that the model's inclination to predict ``Other'' in the presence of a caption may be a consequence of the captions' lack of specificity with respect to the attributes relevant to the table cells (for example, ``hate speech detection''). This lack of explicit, relevant details could create confusion in associating the caption with the retrieved paragraphs, thereby misleading the model.
To test our hypothesis, we manually adjust the captions to include more specific attributes, such as ``hate speech detection'' and ``T5-Base.'' As a result, we observe an improvement in the model's performance with the revised caption, with the total F\textsubscript{1} score even exceeding that achieved without a caption. This outcome partially supports our hypothesis and suggests that carefully crafted captions could indeed be beneficial, aligning with our initial expectations. However, this investigation also points to the fact that the model currently lacks robustness in handling these outlier scenarios.


\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figures/gpt-4v-crop.pdf}
    \caption{Generate \LaTeX~code for image tables using GPT-4V. 
}
    \label{fig:gpt-4v}
\end{figure*}


\section{Leaderboard Extraction from ML Papers}
\label{sec:axcell_details}

\subsection{Task Definition and State-of-the-Art}
The task of leaderboard extraction entails extracting leaderboard tuples (\texttt{task}, \texttt{dataset}, \texttt{metric}, \texttt{score}) from tables in ML papers. Unlike our proposed Schema-Driven extraction, which requires open-domain span identification, leaderboard extraction presumes prior knowledge of all leaderboards, represented as pre-defined (\texttt{task}, \texttt{dataset}, \texttt{metric}) tuples, and centers on linking numeric cells to these leaderboards. 

The state-of-the-art leaderboard extraction method, \axcell{} \cite{kardas-etal-2020-axcell}, is a comprehensive pipeline system comprising four components: Table Type Classification, Table Segmentation, Cell Linking, and Filtering. For each component, except the last one, \axcell{} employs a supervised model. It starts with table type classification to identify result-related tables, which are then passed to the table segmenter responsible for annotating the header cells of the table. Following this step, a retrieval model links numeric cells in the table to pre-defined leaderboards using human-engineered features. Lastly, \axcell{} filters and selects the best record based on the leaderboard taxonomy criteria, such as retaining higher values for "Accuracy" and lower ones for "error rate".

\subsection{Leaderboard Extraction using \method{}}
To extract leaderboards from an ML paper, we consider all tables that contain numeric cells, instead of selecting tables via a trained classifier as in \axcell{}. For each table, we run \method{} using a customized leaderboard extraction JSON template. This template resembles the \data{} template with two additional fixed attributes: \texttt{eval split} and \texttt{eval class} in the ``Result'' cell template. We add the \texttt{eval split} attribute because the evaluated split is essential information for this task; for instance, \textit{``dev F\textsubscript{1}''} and \textit{``test F\textsubscript{1}''} are treated as different metrics in the leaderboard taxonomy. 
The \texttt{eval class} attribute is used to exclude sub-set or sub-class results that are typically present in analysis tables. After generating all predicted cell descriptions, we filter them based on three criteria: 1) the \texttt{type} attribute must be \textit{``Result''}; 2) the \texttt{eval class} attribute must be \textit{``all''} or \textit{``Null''} as observed on the development set; and 3) the cell must be bolded in the table, as this usually indicates its superior performance and possible relevance to the leaderboard. 
For papers without any bolded cells, we experiment with two strategies: 1) include all the remaining cells in the table that meet the first two criteria; 2) use cells selected by \axcell{}, as its engineered features for cell selection may be useful. This hybrid system is referred to as \method{}+.
We then use the predicted \texttt{task}, \texttt{dataset}, and \texttt{metric} attributes in each JSON record to match with the pre-defined leaderboards using token-level F\textsubscript{1}, and we select the leaderboard with the highest average score over three attributes. Finally, following \axcell{}, we choose the best record based on the leaderboard taxonomy criteria, e.g., retaining higher values for "Accuracy" and lower ones for "error rate".


\end{document}
