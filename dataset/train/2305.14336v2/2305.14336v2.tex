\pdfoutput=1


\documentclass[11pt]{article}

\usepackage[]{EMNLP2023}

\usepackage{times}
\usepackage{latexsym}

\usepackage{graphicx}
\usepackage{scalefnt}
\usepackage{svg}
\usepackage{comment}
\usepackage{booktabs}
\usepackage{array}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tablefootnote}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{enumitem}
\usepackage{xcolor, color, soul}
\usepackage[multiple]{footmisc}
\usepackage{amsmath}
\usepackage{csquotes}
\usepackage{fixltx2e}
\usepackage{hyperref}
\usepackage{pifont}
\usepackage{makecell}
\usepackage{longtable}

\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}
\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}}
\newcolumntype{R}[1]{>{\PreserveBackslash\raggedleft}p{#1}}
\newcolumntype{L}[1]{>{\PreserveBackslash\raggedright}p{#1}}

\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}

\newcommand\pwc{\textsc{PwC }}
\newcommand\tdms{\textsc{NLP-TDMS}}
\newcommand\tabext{\textsc{TabExtract}}
\newcommand\method{\textsc{InstrucTE}}
\newcommand\task{\textsc{Schema-to-Json}}
\newcommand\ours{\textsc{Schema-to-Json}}
\newcommand\data{\textsc{MlTables}}
\newcommand\codex{\textsc{Codex}}
\newcommand\axcell{\textsc{AxCell}}
\newcommand{\tkinstruct}{\textsc{T$k$-Instruct}}
\newcommand{\chemtables}{\textsc{ChemTables}}
\newcommand{\discomat}{\textsc{DisCoMat}}

\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}}

\newcommand{\fan}[1]{\textcolor{blue}{\bf\small [#1 --Fan]}}
\newcommand{\ar}[1]{\textcolor{red}{\bf\small [#1 --Alan]}}
\newcommand{\jm}[1]{\textcolor{violet}{\bf\small [#1 --Junmo]}}

\usepackage[normalem]{ulem}
\newcommand{\gabis}[1]{\textcolor{orange}{\bf\small [#1 --Gabi]}}
\newcommand{\gabisrep}[2]{\gabis{\sout{#1} #2}}
\newcommand{\gabist}[1]{\gabis{\sout{#1}}}




\title{Schema-Driven Information Extraction from Heterogeneous Tables}





\author{Fan Bai$^{\clubsuit}$ Junmo Kang$^{\clubsuit}$  Gabriel Stanovsky$^{\diamondsuit}$  Dayne Freitag$^{\spadesuit}$ Alan Ritter$^{\clubsuit}$ \\
$\clubsuit$ {College of Computing, Georgia Institute of Technology} \\
  $\diamondsuit$ {School of Computer Science and Engineering, The Hebrew University of Jerusalem} \\
  $\spadesuit$ \text{Artificial Intelligence Center, SRI International} \\
  \small
 \texttt{\{fan.bai, alan.ritter\}@cc.gatech.edu}, \texttt{junmo.kang@gatech.edu}, \\
\small
\texttt{gabriel.stanovsky@mail.huji.ac.il}, \texttt{daynefreitag@sri.com} \\
 \\
 }

\begin{document}
\maketitle
\begin{abstract}

In this paper, we explore the question of whether large language models can support cost-efficient information extraction from tables. We introduce {\em schema-driven information extraction}, a new task that transforms tabular data into structured records following a human-authored schema. To assess various LLM's capabilities on this task, we develop a benchmark composed of tables from four diverse domains: machine learning papers, chemistry literature, material science journals, and webpages.  Alongside the benchmark, we present an extraction method based on instruction-tuned LLMs. 
Our approach shows competitive performance without task-specific labels, achieving F\textsubscript{1} scores ranging from 74.2 to 96.1, while maintaining great cost efficiency. Moreover, we validate the possibility of distilling compact table-extraction models to reduce API reliance, as well as extraction from image tables using multi-modal models. 
By developing a benchmark and demonstrating the feasibility of this task using proprietary models, we aim to support future work on open-source schema-driven IE models.\footnote{Our code and datasets are available at \url{https://github.com/bflashcp3f/schema-to-json}.}
\end{abstract} 

\section{Introduction}
\label{sec:intro}

\begin{figure}[!t]
\begin{center}
  \includegraphics[width=0.45\textwidth]
  {figures/task-crop.pdf}
\caption{Overview of Schema-Driven Information Extraction. The input includes two elements: the source code of a table and a human-authored extraction schema, outlining the target attributes and their data types.
The output consists of a sequence of JSON records that conform to the extraction schema. 
}
  \label{fig:task}
\end{center}
\end{figure}

Vast quantities of data are locked away in tables found in scientific literature, webpages, and more.
These tables are primarily designed for visual presentation, and the underlying data is typically not available in any structured format, such as a relational or graph database. Some table collections have simple or uniform structures \cite{cafarella2008webtables}, making them easy to convert to relational data, for example Wikipedia tables \cite{lebret2016neural,iyyer2017search}, however a lot of information is stored in tables with complex and varied layouts, such as tables of experimental data presented in scientific literature.

Prior work on extracting structured data from tables has focused on developing custom pipelines for each new table format or domain, for example extracting machine learning leaderboards from \LaTeX~result tables \cite{kardas-etal-2020-axcell}. 
Importantly, the development of these specialized pipelines necessitates domain-specific labeled data, which not only incurs a significant cost in collection for every new extraction task but also constrains their applicability outside the originating domain.





In this paper, we investigate whether recent advances in large language models can reduce the costs of extracting data from tables.  To do this, we present a new formulation of the table extraction problem, which we refer to as Schema-Driven Information Extraction. 
In Schema-Driven IE, the only human supervision provided is a schema that describes the data model, including the target attributes and their corresponding data types, and resembles a JSON schema,\footnote{\url{https://json-schema.org/}} with a few customizations. 
The output is a sequence of JSON objects describing each cell in the table, adhering to the provided schema.
For example, as demonstrated in Figure \ref{fig:task}, a domain expert outlines the attributes of interest related to experimental result cells in a machine learning table, and the model extracts JSON objects following this schema. 








To evaluate the ability of LLMs to perform Schema-Driven IE from tables, we introduce a new benchmark consisting of table extraction datasets in four diverse domains: machine learning papers, chemistry literature, material science journals, and webpages, each of which has a different data format (\LaTeX, XML, CSV, and HTML, respectively). We curate and annotate data for the first two domains, while adapting existing datasets for the latter two. 

We then use this benchmark to analyze the performance of both open-source and proprietary LLMs. 
We find that proprietary instruction-tuned LLMs perform surprisingly well across diverse domains and data formats, compared to open-source ones. For example, GPT-4 \cite{gpt4} and {\tt code-davinci} \cite{codex}, are capable of surprisingly accurate table extraction (ranging from 74.2 to 96.1 F\textsubscript{1}), given only a relevant data schema as supervision. This performance is comparable to fully supervised models, which operate at an F\textsubscript{1} range of about 64.1 to 96.1. We also present a number of analyses on various factors that are key to achieving good performance while minimizing inference costs, including retrieving text from outside the table, and an iterative error recovery strategy.
Moreover, we demonstrate the utility of Schema-Driven IE from tables by evaluating performance on the downstream task of leaderboard extraction from machine learning papers \citep{kardas-etal-2020-axcell}. We achieve competitive performance, rivaling state-of-the-art supervised methods that use purposely-built and trained pipelines.









While the performance of open-source models, such as StarCoder \cite{starcoder}, and Alpaca currently trails behind proprietary models, we show it is possible to distill compact table extractors, without sacrificing performance. 
Furthermore, we highlight the potential of multi-modal models, such as GPT-4V, to extract data from table images, significantly expanding the set of documents this approach could be applied to.
Through the introduction of a new benchmark for Schema-Driven IE, we aim to foster the development of future open-source instruction-following models that can perform this task without relying on proprietary models.


\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figures/main-crop.pdf}
    \caption{Left: Prompt formulation of our proposed method \method{}. Right: Illustration of our error-recovery strategy, which ensures the model compliance of the instructed cell traversal order and reduces inference costs.
}
    \label{fig:method}
\end{figure*}

\begin{comment}
In conclusion, we contribute as follows: 
\begin{itemize}
\item We leverage recent advances in large language models to simplify the process of information extraction from tables, introducing a new task, Schema-Driven IE, supported by a benchmark covering tables from four diverse domains.
\item We propose \method{}, a novel method that applies instruction-tuned LLMs to extract structured records from tabular data, using only a human-curated extraction schema, complemented with a cost-effective error-recovery strategy.
\item We not only show the impressive performance of the {\tt code-davinci-002} model on the \task{} benchmark but also demonstrate the potential of distilling compact table extraction models from its predictions, thereby reducing the reliance on black-box API-based models.
\end{itemize}

\end{comment}









\begin{comment}
\begin{itemize}
    \item There is a huge amount of valuable information locked in semi-structured tables (scientific literature, webpages, etc.)
    \item  These tables are generally designed for visual presentation, not to support structured queries or data analysis.  
    \item While there do exist uniform table formats (E.g. Wikipedia tables), a lot of data is stored in complex formats that vary across tables (e.g. tables found in scientific papers).
    \item Prior work has extracted structured data from complex table layouts, but this requires manual engineering for each new data source or domain.
    \item Recent progress on large language models trained on code and fine-tuned to follow human instructions raises the question: how well can LLMs understand semi-structured data?
    \item In this paper, we develop a benchmark of three datasets consisting of LaTeX tables from ML papers, XML tables from biomedical journals, and semi-structured data in HMTL webpages.
    \item We demonstrate that large instruction-tuned language models can work surprisingly well at the task of extracting structured data from semi-structured tables.  One can simply prompt LLMs with a table, optionally including relevant context from the caption, paper abstract, etc. along with instructions on how the data should be formatted. 
    \item To extract information from a new data source or domain, only a schema for the new data is required.  There is no need for labeled data or domain-specific engineering.
    \item Furthermore we present a detailed analysis of factors needed for good performance.  We show that XXX all improve performance, and YYY is crucial.
    \item Because good performance on instructing language models to extract data from semi-structured tables currently relies on closed models that are only accessible through APIs, we also experiment with knowledge distillation.  We show how it is possible use a LLM to generate targets for fine-tuning a more compact student model, without sacraficing too much performance.
\end{itemize}
\end{comment}




\section{Schema-Driven Information Extraction}
\label{sec:task_def}

We present a new task for extracting structured records from tables, where the only supervision is a schema describing the desired output.
As shown in Figure \ref{fig:task}, the task input contains two elements: 1) a table $T$, comprised of $n$ cells $\{c_1, c_2, \dots, c_n\}$, optionally supplemented with contextual text;  and 2) an extraction schema $S$ that specifies extracted attributes for $k$ types of records, each containing $m$ attributes ${a_1, a_2, \dots, a_m}$ along with their respective data types (formatted as JSON templates in implementation). 
Given the input, the model generates a sequence of $n$ JSON objects, represented as $\{o_1, o_2, \dots, o_n\}$. Each object $o_i$ corresponds to one record type and comprises $m$ key-value pairs $\{(a_{i,1}, v_{i,1}), \dots, (a_{i,m}, v_{i,m})\}$, where $v_{i,j}$ is the value of attribute $a_{i,j}$ extracted for cell $c_i$. 


Consider a table in an ML paper that displays various models' results.
Our proposed task enables the extraction of result records from each cell in the table.  These records include relevant attributes such as the evaluation metric, task, etc, which are structured in corresponding JSON objects and could facilitate meta-analysis of experiments or support research on reproducibility.







\subsection{Instruction-Based Table Extractor}
\label{sec:task}








To perform Schema-Driven IE on tables, we introduce \textbf{\method{}}, a method to extract structured records from a broad range of semi-structured data, using only task-specific instructions.
\method{} models information extraction as template filling, where the extraction schema is represented as a series of JSON templates. The underlying LLM is instructed to select the appropriate template and populate it with extracted values. As illustrated in Figure \ref{fig:method} (left), the prompt used by \method{} consists of four key components: an input table supplemented with text from the same document, an extraction schema, task-specific instructions, and an initial record for starting the process. For more details on prompt formulation, please refer to Appendix \ref{sec:promp_appendix}. 












Despite our explicit instructions, we found that models often fail to generate all cell descriptions from a single prompt, e.g., deviating from the instructed cell traversal order, leading to partial extraction.
To mitigate this, we employ an iterative error recovery method to refine the model's output when it violates the instructions. 
As shown on the right side of Figure \ref{fig:method}, we detect deviations from the instructed \textit{left-right, top-down} order by comparing predicted cell values with those from a rule-based detector (see Appendix \ref{sec:promp_appendix}) that follows the set order. Then, we truncate the LLM’s output and re-prompt the model with the truncated sequence, adding the value of the next target cell.
This strategy guides the model to adhere to the instructed order, and it continues until all records are generated.
In Section \ref{sec:ablation}, we show that it can achieve comparable results to cell-by-cell prompting while significantly reducing API costs.









\section{A Table Extraction Benchmark}
\label{sec:eval}

We now present the details of our benchmark, \task{}, which is designed to assess the capabilities of LLMs to extract data from tables following a schema. This benchmark comprises table extraction datasets across four diverse domains: machine learning papers, chemistry literature, material science journals, and webpages. Each domain adheres to a unique textual format, namely, \LaTeX, XML, CSV, and HTML, requiring models to generalize across domains and formats. We manually annotate datasets for the first two domains, while the datasets for the latter two are adapted from pre-existing datasets, tailored to fit our task. Statistics of the four datasets are summarized in Table \ref{tab:dataset_stat}. 














\paragraph{\data{}} We create \data{}, a manually annotated dataset 
focused on tables in arXiv machine learning papers, particularly emphasizing numeric cells that are classified into four categories: Result, Hyper-parameter, Data Statistics, or Other. Extraction attributes are pre-defined for the first three categories; for instance, Result records incorporate textual 
attributes such as \textit{evaluation metric} (e.g., F\textsubscript{1}) and \textit{dataset} (e.g., SQuAD), along with dictionary-based attributes like \textit{experimental settings}. 
We collect papers from three pertinent arXiv fields: Machine Learning, Computer Vision, and Computation and Language, and a random sample of 10 papers is selected from each field.\footnote{We limit the paper selection to those published between Oct. and Nov. 2022 to avoid contamination with GPT-4 (\texttt{0613}) and GPT-3.5, which were trained on data until Sep. 2021.} After removing papers without \LaTeX{} source code or any tables, a total of 25 papers is covered in our dataset. 





To optimize the annotation budget and the dataset diversity, we cap the number of annotated tables to five per paper. Recognizing the domain-specific expertise needed, we employ expert annotators with backgrounds in ML research, who are provided with tables in both \LaTeX~and PDF formats and encouraged to thoroughly read the paper before annotation. The annotation process comprises two steps: 1) identifying the numeric cells and their record types, and 2) filling in the slots of pre-determined attributes, forming a JSON record with keys as attribute names and values as extracted content, in a text editor. Consequently, \data{} contains 122 tables, with 3,792 cells and 21K attributes annotated. For more details of \data{}, such as pre-defined attributes and inter-annotator agreement (IAA) score, see Appendix \ref{sec:mltab_appendix}.




\newcommand{\subfifty}[1]{$\text{#1}_{50}$}
\newcommand{\icfifty}{\subfifty{IC}}



\paragraph{\chemtables{}} In addition to \data{}, we annotate a new corpus of tables describing the physical properties of chemical compounds. The automated extraction of physical properties from such tables could provide substantial real-world benefits, for example collecting much-needed data for training ML models that can support inverse molecular design \citep{kim2018deep} and thus accelerating the drug design process \citep{Fields19,STOKES2020688}. 
Here, we focus on cells concerning five important physical properties: \icfifty{}, \subfifty{EC}, \subfifty{GI}, \subfifty{CC}, and MIC.\footnote{\url{https://www.sciencedirect.com/topics/pharmacology-toxicology-and-pharmaceutical-science/ic50}}
Three common attributes are manually extracted from tables for all properties: \emph{unit}, \emph{treatment} (experimental compound), and \emph{target} (measured biological entity, usually a gene expression or disease organism). 
To collect tables, we search through the Open Access subset of PubMed Central,\footnote{\url{https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/}} identifying tables that explicitly mention any of the five target physical properties within the XML paper sources. 
Similar to \data{}, domain experts annotate JSON records for relevant cells (IAA score in Appendix \ref{sec:chemtab_appendix}).
















\paragraph{\discomat{} \citep{gupta2022discomat}} Besides the two newly introduced datasets, we incorporate \discomat{}, an existing dataset focusing on glass composition tables from Elsevier material science journals. The task in \discomat{} is to extract tuples comprising (\textit{material}, \textit{constituent}, \textit{percentage}, \textit{unit}) from given tables. We adapt \discomat{} to fit our Schema-Driven IE framework by grounding the \textit{percentage} element to numeric cells in the table and considering the other elements as attributes. The model is tasked to identify numeric cells representing constituent percentages and predict the associated three attributes. The dataset consists of 5,883 tables from 2,536 papers and includes a distantly supervised training set (using MatSci DB) along with a manually annotated evaluation set.\footnote{In the released corpus, tables are represented as matrices; we, therefore, transform them into CSV tables (using the pipe symbol "|" as the delimiter) prior to feeding them into LLMs.} We refer readers to \citet{gupta2022discomat} for more details of \discomat{}.

\paragraph{\textsc{SWDE} \citep{swde}} In addition to the three scientific literature-based corpora, we add SWDE (Structured Web Data Extraction) as a fourth dataset, aimed at extracting pre-defined attributes from HTML webpages. This dataset comprises roughly 124K pages gathered from eight distinct verticals, such as \textit{Autos}, \textit{Books}, and \textit{Movies}. Each vertical includes ten unique websites (200 to 2000 pages for each website) and is associated with a set of 3 to 5 target attributes for extraction (refer to Table \ref{tab:swde_stats} in the appendix for detailed statistics and vertical-specific attributes).
Due to API budget limitations, 
we estimate InstrucTE performance on a sample of 1,600 pages (20 per website),
and compute a boostrap confidence interval to ensure this provides a reasonable performance estimate.\footnote{To validate that this subset is representative of the larger dataset, we estimate a 95\% confidence interval for \method{}'s performance using 1,000 bootstrap samples, and the resultant margin of error is 0.009, suggesting the sampling strategy is unlikely to substantially impact our performance estimate. \label{footnote:bootstrap}}










\begin{table}[tb!]
\small
\begin{center}
\scalebox{0.77}{
\begin{tabular}{lrrrr}
\toprule


  & \textbf{\textsc{MlTab.}}  & \textbf{\textsc{ChemTab.}} & \textbf{\discomat{}} & \textbf{\textsc{SWDE}}  \\
& {(ours)}  & {(ours)} & {(\citeyear{gupta2022discomat})} & (\citeyear{swde}) \\
  \midrule
\normalsize{\# cell types} & \normalsize{4} & \normalsize{6} & \normalsize{2} & \normalsize{8} \\
\normalsize{\# attr. types} & \normalsize{11} & \normalsize{4} & \normalsize{4} & \normalsize{32} \\
\normalsize{\# papers (web.)} & \normalsize{25} & \normalsize{16} & \normalsize{2,536} & \normalsize{80} \\ 
\normalsize{\# tables (pages)} & \normalsize{122} & \normalsize{26} & \normalsize{5,883} & \normalsize{1,600}  \\
\normalsize{\# anno. records} & \normalsize{3,792} & \normalsize{1,498} & \normalsize{58,481} & \normalsize{1,600} \\
\normalsize{\# records / table} & \normalsize{31.1} & \normalsize{57.6} & \normalsize{9.9} & \normalsize{1} \\
\bottomrule
\end{tabular}
}
\end{center}
\caption{\label{tab:dataset_stat} Dataset statistics of four datasets in our \task{} benchmark. 
}
\end{table}


\section{Experiments}
\label{sec:exp}











\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figures/main_results.pdf}
    \caption{Capability of various LLMs to perform Schema-Driven IE, measured using the \task{} benchmark. 
As noted in Section \ref{sec:eval_metric}, Table-F\textsubscript{1} is employed for \data{} and \chemtables{}, while Tuple-F\textsubscript{1} is used for \discomat{}. For SWDE, we report Page-F\textsubscript{1}, and $\textit{k}$ represents the number of websites used in training from each vertical. The *GPT-4 result on SWDE is computed on a 1,600 webpage sample due to API budget limitations - see Section \ref{sec:eval} and Footnote \ref{footnote:bootstrap}.
}
    \label{fig:main_results}
\end{figure*}


We evaluate the capability of various LLMs to perform schema-driven information extraction, in addition to full fine-tuning using our benchmark.
For machine learning and chemistry tables, we use a subset of 10 and 7 randomly sampled papers separately for model development, which facilitates the training of supervised models, thereby enabling a comprehensive comparison with a schema-driven approach. For the two pre-existing datasets, we follow data splits used in the original experiments.

\subsection{Evaluation Metrics}
\label{sec:eval_metric}
We introduce Table-F\textsubscript{1}, a new metric gauging overall attribute prediction performance within a table, for our two proposed datasets. Table-F\textsubscript{1} represents the harmonic mean of precision and recall, with precision being the ratio of
correctly predicted attributes to total predicted attributes.
At the attribute level, we consider two metrics: token-level F\textsubscript{1} and exact match (EM). For token-level F\textsubscript{1}, a prediction is deemed correct if the score exceeds a specific threshold, which is determined by maximizing the alignment between the model predictions and human judgments on the dev set of the dataset (see Appendix \ref{sec:eval_metric_app}). 
We report macro-averaged Table-F1 given the wide variance in table sizes.

For \discomat{} and SWDE, we use the metrics specified in the original papers. In the case of \discomat{}, we report Tuple-F\textsubscript{1} (\citealp{gupta2022discomat}), where a predicted 4-element tuple is considered correct only if it exactly matches with the gold tuple. For SWDE,  we report Page-F\textsubscript{1} \citep{swde}, which measures the number of pages where the attributes are accurately predicted.\footnote{Notably, SWDE primarily focuses on identifying textual HTML nodes containing attribute values rather than exact text spans, so we use token-level F\textsubscript{1} to identify the most relevant HTML node for each extracted attribute.} 











\subsection{Baselines \& Implementation Details}
\label{sec:baseline}

For \data{} and \chemtables{}, we experiment with various LLMs as the backbone for \method{}, including API-based GPT-4 and GPT-3.5 models and open-source models, such as LLaMA-7B \citep{touvron2023llama}, Alpaca-7B \citep{alpaca}, and StarCoder-15.5B \citep{starcoder}.
Additionally, we frame Schema-Driven IE as a TableQA problem, where we predict each attribute via a separate question over the input table, 
and use Flan-T5-11B \citep{flant5} as a competitive prompting baseline. 
Furthermore, we incorporate T5-11B \citep{t5} and TaPas \citep{herzig-etal-2020-tapas}, a table-specialized LM, for fine-tuning experiments to facilitate comprehensive comparison with our proposed \method{}. For implementation details of \method{} and baselines, see Appendix \ref{sec:implement_details}.







For \discomat{} and SWDE, we compare \method{} with existing methods, which either design task-specific architectures, such as FreeDom \citep{freedom} and LANTERN \citep{lantern}, or use LMs pretrained on tables or web pages, like TaPas \citep{herzig-etal-2020-tapas}, TaBERT \cite{yin2020tabert}, and MarkupLM \citep{li-etal-2022-markuplm}. 

























\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/ablation.pdf}
    \caption{Ablation studies on various components of our \textsc{InstrucTE} (w/ \texttt{code-davinci-002}) on the \data{} dataset. 
Interestingly, excluding the table caption improves performance. Our detailed analysis in Appendix \ref{sec:error_analysis_of_caption} reveals that low-quality captions (e.g., lack of specificity) may confuse the model, leading to inaccurate predictions.}
    \label{fig:ablation}
\end{figure*}


\subsection{Main Results}
\label{sec:main_results}
Figure \ref{fig:main_results} presents the main results from the comparison between \method{} and various baselines on our \task{} benchmark.
We observe that \method{}, in conjunction with GPT-4, achieves surprisingly strong performance across domains and input formats, supervised solely by human-authored schemas. 


For \data{} and \chemtables{}, \method{} outperforms all baseline models, including the fine-tuned T5-11B. However, a noticeable gap remains compared to human performance, e.g., the Table-F\textsubscript{1} on double-annotated examples for \data{} stands at 96.6 when applying thresholded token-level F\textsubscript{1} for attribute matching, which is 22.4 F\textsubscript{1} points higher than \method{}. 
For \discomat{} and SWDE, prompting-based \method{} performs on par or slightly trails behind the fully supervised state-of-the-art methods, signifying the potential of LLMs to act as flexible, powerful tools for extracting information from tables across diverse data formats and domains.






\subsection{Analysis \& Ablation Studies}
\label{sec:ablation}


We analyze the impact of different modules in \textsc{InstrucTE}, including prompt formulations, retrieved information and error recovery, using \data{}.





\paragraph{LLMs \& Prompt Formulations} In Table \ref{tab:test}, we compare different LLMs for \textsc{InstrucTE}, and three key observations emerge: 1) open-source models of similar scales (for instance, those in the 6-7B range) tend to achieve comparable fine-tuning performance, though they might exhibit variations in prompting performance; 2) notably, GPT-4-based \method{} demonstrates strong performance, outperforming open-source instruction-based models as well as fine-tuning models with limited training data; and 3) among the three GPT-3.5 backbones, \texttt{code-davinci-002} performs the best, followed by \texttt{text-davinci-003} and \texttt{gpt-3.5-turbo}, implying that optimizing for code generation could benefit Schema-Driven IE more compared to reinforcing chat capabilities. Subsequently, we compare two prompt formulations: \task{} and TableQA. From the T5-11B fine-tuning experiments, we observe that \task{} attains better performance, suggesting that incorporating task-specific instructions along with an extraction schema within the input could enhance the extraction process.





\paragraph{Prompt Components \& Error Recoverery}
Figure \ref{fig:ablation} shows \method{}'s performance subject to the exclusion of varying prompt components. Here we use \texttt{code-davinci-002} as the backbone LM due to API budget limitations as well as its resemblance of GPT-4 on performance and context length.\footnote{\texttt{code-davinci-002} is freely accessible through the OpenAI Researcher Access Program.}
We can see that removing supplementary text decreases the performance, highlighting the importance of supplementary textual information and the model's ability to encode longer sequences.  As for table elements, table headers contribute positively as expected, while captions surprisingly do not. Further analysis on table captions is provided in Appendix \ref{sec:error_analysis_of_caption}, which suggests that unclear captions can sometimes mislead the model, resulting in inaccurate predictions.
Notably, discarding the extraction schema, specifically JSON templates, causes a substantial performance decline, primarily due to attribute name mismatches in the evaluation.
Lastly, we show that \method{}'s performance drops significantly without error recovery. 
Compared to a cell-by-cell prompting strategy, error recovery offers similar performance at a fraction of the API cost (\$100 v.s. \$670 based on the pricing on Azure), underscoring its importance to \method{}.\footnote{The pricing for \texttt{code-davinci-002} on \href{https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/}{Azure} is \$0.1 per 1,000 tokens as of June 23rd, 2023.} 











\begin{table}[!t]
\centering
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{cclcc}
\toprule
\textbf{Exp. Setup} & \textbf{Formulation} & \textbf{Model} & \textbf{Token-F\textsubscript{1}} & \textbf{EM} \\
\midrule

\multirow{6}{*}{\makecell{Fine-tuning \\ (\# Train=1169)}}
& \multirow{2}{*}{TableQA} & TaPas (large) & 27.7 & 21.6 \\ 
&          & T5 (11B) & 61.2 & 46.2 \\ 
  \cmidrule(l){2-5}
& \multirow{4}{*}{\small{\task{}}} 
& GPT-J (6B) & 49.6 & 38.4 \\
& & LLaMA (7B) & 51.3 & 38.0 \\
& & Alpaca (7B) & 50.2 & 39.4 \\
& & T5 (11B) & 64.1 & 50.2 \\ 

\midrule

\multirow{9.5}{*}{\makecell{Prompting \\ (Schema only)}} & TableQA & Flan-T5 (11B) & 36.9 & 27.7 \\ 
 \cmidrule(l){2-5}
& \multirow{8}{*}{\small{\task{}}} 
& GPT-J (6B) & 18.6 & 16.2 \\
& & LLaMA (7B) & 13.5 & 11.5 \\
& & StarCoder (15.5B) & 41.2 & 32.3 \\
& & \texttt{gpt-3.5-turbo} & 64.1 & 47.9 \\
& & \texttt{text-davinci-003} & 67.4 & 50.4 \\
& & \texttt{code-davinci-002} & 72.3 & 57.6 \\
& & \texttt{gpt-4 (0613)} & \textbf{74.2} & \textbf{58.1} \\

\bottomrule
\end{tabular}
}
\caption{
    \textsc{Test} set performance on \data{} with different prompt formulations and backbone LMs.
   }
\label{tab:test}
\end{table}



\subsection{Knowledge Distillation}
\label{sec:distill}
Considering the strong performance of \textsc{InstrucTE} (with API-based LMs), we show it is possible to use knowledge distillation \citep{le-etal-2022-shot, Kang2023DistillOA} to build a cost-efficient compact model, using \data{} as a demonstration.
Specifically, this process first generates synthetic data by performing inference on unlabeled tables using \texttt{code-davinci-002}, followed by fine-tuning a smaller model (e.g., 7B parameters) using the synthetic data.  We compile a collection of 979 arXiv ML papers, submitted between 2008 and 2019, yielding 3,434 tables (containing a total of 100K cells).
In Table \ref{tab:distillation}, we can see that LLaMA-7B and Alpaca-7B demonstrate similar performance as seen in the fine-tuning results (Table \ref{tab:test}). Interestingly, while fine-tuning LLaMA with LoRA presents noticeable computational and parameter efficiency, using the synthetic data to fine-tune the full parameters of T5-11B results in performance that matches that of the teacher model.\footnote{As we observe that the T5-11B student model slight outperforms the teacher model,  we conduct a statistical significance test following \citet{berg-kirkpatrick-etal-2012-empirical}. With 1000 bootstrap samples, the p-value is 42.3\%, suggesting that the performance gap between the T5-11B student model and the teacher model is not statistically significant.}





\begin{table}[!t]
\centering
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{clcccccc}
\toprule
\multirow{2}{*}{} & \multirow{2}{*}{\textbf{Model (\texttt{GPU hours})}} & \multicolumn{3}{c}{\textbf{Token-Level F\textsubscript{1}}} & \multicolumn{3}{c}{\textbf{EM}} \\
  \cmidrule(l){3-5} \cmidrule(l){6-8} 
& & \textbf{P} & \textbf{R} & \textbf{F\textsubscript{1}} & \textbf{P} & \textbf{R} & \textbf{F\textsubscript{1}} \\ 
\midrule

\multirow{1}{*}{\makecell{Teacher}}
& \texttt{code-davinci-002} & 74.1 & 71.8 & 72.3 & 59.4 & 56.9 & 57.6 \\

\midrule

\multirow{3}{*}{\makecell{Student}}
& LLaMA-7B (\texttt{50h}) & 74.1 & 67.6 & 69.1 & 56.8 & 53.4 & 54.3 \\
& Alpaca-7B (\texttt{50h}) & 72.7 & 64.8 & 67.5 & 56.1 & 50.0 & 52.0 \\
& T5-11B (\texttt{380h}) & 75.8 & 71.4 & 73.2 & 60.3 & 56.7 & 58.1 \\ 

\bottomrule
\end{tabular}
}
\caption{
    Experimental results for knowledge distillation. Student models are trained on the synthetic data generated by the teacher, and evaluated on \textsc{Test} set of \data{}. \texttt{GPU hours} refers to the training time ($\times$ number of GPUs) of student models for one epoch. 
}
\label{tab:distillation}
\end{table}

\subsection{Extraction from Image Tables}
One practical challenge is the need for table source code in a textual format. This issue is particularly pronounced in scientific domains where many articles are accessible only as PDFs or images. To address this, we propose a pipeline that first employs multi-modal models to transform image-based tables into their textual counterparts, and then run \textsc{InstrucTE} on the resulting textual tables. 
In a preliminary study with \data{}, we use GPT-4V to produce the \LaTeX~code (see Figure \ref{fig:gpt-4v} in Appendix), which is subsequently fed into \textsc{InstrucTE}. 
We find that the generated \LaTeX~code for all 68 test set tables can be rendered into valid PDFs, but only 51 tables accurately preserve headers and cells after the image-to-text conversion.
Performance-wise, with GPT-4, our \method{} extraction pipeline achieves a Table-F\textsubscript{1} score of 70.2 from image inputs. This result is closely competitive to the 74.2 Table-F\textsubscript{1} achieved with direct textual inputs, demonstrating the effectiveness of our proposed pipeline.  



\section{Extrinsic Evaluation: Extracting Leaderboards from ML Papers}
\label{sec:axcell}
In this section, we showcase the applicability of \method{} by extracting leaderboards from ML publications, a task that has been widely studied \citep{hou-etal-2019-identification, kardas-etal-2020-axcell}.
This task differs from our proposed task as it involves linking results in ML tables to pre-defined leaderboard tuples (\texttt{task}, \texttt{dataset}, \texttt{metric}, \texttt{score}), whereas our task requires exhaustive extraction of table cells, in a domain-agnostic way.
Comprehensive details on task definition, the leading supervised method \axcell{}, and \method{}'s application to the task are available in Appendix \ref{sec:axcell_details}.












\begin{table}[t]
\centering
\scalebox{0.7}{
\begin{tabular}{lcccccc}
\toprule 
\multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c}{\textbf{Micro-Average}} & \multicolumn{3}{c}{\textbf{Macro-Average}} \\
\cmidrule(l){2-4} \cmidrule(l){5-7} 
& \textbf{P} & \textbf{R} & \textbf{F\textsubscript{1}} & \textbf{P} & \textbf{R} & \textbf{F\textsubscript{1}} \\ 
\midrule
\axcell{} & \textbf{25.4} & 18.4 & 21.3 & \textbf{21.5} & 21.5 & 20.0 \\
\method{} & 20.1 & 20.8 & 20.5 & 20.3 & 23.1 & 19.6 \\
\method{}+ & 23.9 & \textbf{21.2} & \textbf{22.4} & 21.2 & \textbf{23.7} & \textbf{20.5} \\
\bottomrule
\end{tabular}
}
\caption{Leaderboard extraction results on the \textsc{PWC Leaderboards} dataset (100 sampled papers). 
}
\label{tab:pwd_results}
\end{table}

We compare \method{} with \axcell{} on \textsc{PWC Leaderboards} \cite{kardas-etal-2020-axcell}, the largest dataset for leaderboard extraction.
We randomly sample 100 papers from the dataset for evaluation due to API budget constraints.
For \method{}, we use \texttt{code-davinci-002} as the backbone given its excellent performance on \task{}. 
Table \ref{tab:pwd_results} presents the results of both methods, averaged over the selected papers. We can see that \method{} achieves competitive performance compared to the supervised \axcell{}, highlighting the efficacy of our proposed approach. 
When we enhance \method{} with \axcell{}'s cell selection capabilities to create \method{}+ (more details in Appendix \ref{sec:axcell_details}), it outperforms \axcell{}, demonstrating the promising potential of combining these two approaches. 







\section{Related Work}
\label{sec:related}





Recently there have been many research efforts involving tables, particularly, table-to-text generation \citep{parikh2020totto, wang-etal-2022-robust, hu-etal-2023-improving}. For example ToTTo \cite{parikh2020totto} introduced the task of open-domain table-to-text generation. 
Another burgeoning area is question answering and fact verification on tables \citep{jauhar-etal-2016-tables, zhong2017seq2sql, yu-etal-2018-spider, schlichtkrull-etal-2021-joint}. 
Various approaches have emerged, such as semantic parsing for compositional question answering \citep{pasupat2015compositional}, and symbolic reasoning for fact verification \citep{Chen2020TabFact:}.
In contrast, our work transforms tables into structured JSON records, where a data schema is the only supervision provided.




Furthermore, the rise of pre-trained language models \cite{devlin-etal-2019-bert}, has stimulated interest in pre-training on semi-structured data.  TaPas \cite{herzig-etal-2020-tapas} and TaBERT \cite{yin2020tabert} pre-train on linearized tables with a specialized cell index embedding. TABBIE \cite{iida2021tabbie} employs dual transformers for separate row and column encoding. 
HTLM \cite{aghajanyanhtlm} uses an HTML-specialized pre-training objective, facilitating a novel structured prompting scheme. Similarly, TabLLM \citep{pmlr-v206-hegselmann23a} utilizes linearized tables for general-purpose LLMs.  In contrast to prior work on table pre-training, our work focuses on schema-driven IE rather than table classification or question answering.




Apart from traditional text-based information extraction \citep{wadden-etal-2019-entity, bai-etal-2021-pre, tamari-etal-2021-process}, there is growing interest in semi-structured data \citep{33333, lockard2019openceres, dong-etal-2020-multi-modal, kardas-etal-2020-axcell, gupta2022discomat, lou2023s2abel}. 
OpenCeres \cite{lockard2019openceres} and ZeroShotCeres \cite{lockard-etal-2020-zeroshotceres}, showcase open-domain information extraction from semi-structured websites, but their approach relies on seed examples from a knowledge base, or labeled webpages making these methods unsuitable for extracting data from scientific literature, etc.
Our approach distinguishes itself by leveraging LLMs, to enable accurate extraction of a broad range of data formats and domains without any labels or custom extraction pipelines.


\section{Conclusion}
\label{sec:conclu}




This paper explores the capabilities of LLMs for extracting structured data from heterogeneous tables. 
We introduce a new task, Schema-Driven Information Extraction, which converts tables into structured records guided by a human-authored data schema.
To facilitate this task, we present a comprehensive benchmark and develop \method{}, an innovative prompting-based extraction method. This method leverages instruction-tuned LLMs and integrates a unique error-recovery strategy for cost-efficient extraction. 
\method{} demonstrates impressive performance on four different datasets in our benchmark.
Our work also elucidates the potential of building more compact models through distillation to reduce dependency on APIs, 
as well as extraction from image tables by using multi-modal models for image-to-text conversion.
These contributions lay a solid foundation for the continued advancement of efficient table data extraction.


\section*{Limitations}
The availability of certain API-based models, such as GPT-4 and \texttt{code-davinci-002} may vary over time.
To mitigate dependence on such APIs, we explore knowledge distillation, which yields promising outcomes. We encourage future research to focus on improving smaller, openly accessible models, as this direction holds significant potential for practical application and accessibility.


\section*{Ethical Considerations}
Our use of OpenAI's API-based models to distill open-source table extractors complies with OpenAI's terms of service, as we do not {\em ``use output from the Services to develop models that compete with OpenAI''}.  










\section*{Acknowledgements}
We would like to thank OpenAI's Researcher Access Program and Azure's Accelerate Foundation Models Research Program for graciously providing access to the API-based models, such as \texttt{code-davinci-002} and GPT-4.
This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR001119C0108, and by the NSF under award number 2052498.
The views, opinions, and/or findings expressed are those of the author(s) and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government. 
This work is approved for Public Release, Distribution Unlimited. 




\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix
\clearpage



\section{\method{}}
\label{sec:promp_appendix}

\paragraph{Prompt Formulation}
Our proposed prompt consists of four components: 1) ``Input Table (w/ supp. text)'' includes the source code of the input table paired with supplementary text from the document;
2) ``Extraction Schema'' defines the JSON formats for extracted records, encompassing the record type, attribute names, and associated data types;
3) ``Task-specific Instructions'' outline the task execution process, addressing both the extraction process from individual cells and the model's traversal strategy across cells, such as \textit{``left-right, top-down''}; 
4) ``Initial Record'' is used to jump-start the prompting process, including the partial JSON record of the first cell..

For ``Input Table (w/ supp. text)'', we employ the BM25 algorithm to retrieve the most relevant paragraphs for each table. For ``Extraction Schema'', we propose two guidelines for schema design: 1) Attribute names should be specific, which decreases the probability of the model generating incorrect attributes, or hallucinations. For instance, when extracting relevant attributes about a movie from a movie webpage, it's advisable to use specific terms such as \texttt{``movie name''} or \texttt{``director name''}, rather than the generic \texttt{``name''}; 2) Attributes should be strategically ordered, placing simpler attributes ahead of more complex ones as errors in preceding attributes can adversely affect the prediction of subsequent ones due to the autoaggressive nature of LMs.
The exact \method{} prompts used in our experiments are shown in Table \ref{tab:full_prompt_ml_chem} and Table \ref{tab:full_prompt_mat_swde}.

\paragraph{Cell Detector} We develop a rule-based method to identify numeric cells for both the \data{} and \chemtables{} datasets. Specifically, for the \data{} dataset, we use the row separator ``\textbackslash\textbackslash'' and the column separator ``\&'' to divide the table into cells. We then loop over each cell, checking for numeric values after stripping away any stylized text. In cases, where a cell contains multiple numeric values, such as ``$0 \pm 0$'', we consistently choose the first numeric value. For the \chemtables{} dataset, the parsing process is more straightforward, owing to the structured XML format of the table. Here, we iterate over each cell, verifying if it contains a numeric value once stylized text has been removed. The performance of our rule-based cell detector on two datasets is presented in Table \ref{tab:cell_detect}. In the case of \discomat{}, we use the cell detector provided by the original paper \citet{gupta2022discomat}.


\begin{table}[h!]
\small
\begin{center}
\begin{tabular}{lcccc}
\toprule
\textbf{Dataset} & \textbf{Split} & \textbf{P} & \textbf{R} & \textbf{F\textsubscript{1}} \\
\midrule

\multirow{2}{*}{\data{}} & Dev & 100.0 & 97.0 & 98.0 \\
 & Test & 99.9 & 99.6 & 99.7 \\

\midrule

\multirow{2}{*}{\chemtables{}} & Dev & 100.0 & 100.0 & 100.0 \\
 & Test & 100.0 & 98.3 & 99.2 \\

\bottomrule
\end{tabular}
\end{center}
\caption{\label{tab:cell_detect} Results of (numeric) cell detection on \data{} and \chemtables{}.
}
\end{table}



\begin{table*}[th!]
\small
\centering
\scalebox{1}{
\begin{tabular}{lL{11cm}}
\toprule 

 \textbf{Dataset} & \textbf{Full Prompt}  \\
 
\midrule

\data{} & 
[Retrieve paragraphs] \newline

[Input table] \newline

Here are JSON templates for four types of numeric cells: ``Other'', ``Result'', ``Data Stat.'', and ``Hyper-parameter/Architecture'':\newline
\{``value'': ``xx'', ``type'': ``Other''\}\newline
\{``value'': ``xx'', ``type'': ``Result'', ``task'': ``xx'', ``metric'': ``xx'', ``training data/set'': ``xx'', ``test data/set'': ``xx'', ``model/method'': ``xx'', ``model/method settings'': \{``xx'': ``yy''\}, ``experimental settings'': \{``xx'': ``yy''\}\}\newline
\{``value'': ``xx'', ``type'': ``Data Stat.'', ``dataset'': ``xx'', ``attribute name'': ``xx'', ``sub-set/group name'': ``xx'', ``dataset features'': \{``xx'': ``yy''\}\}\newline
\{``value'': ``xx'', ``type'': ``Hyper-parameter/Architecture'', ``model'': ``xx'', ``parameter/architecture name'': ``xx'', ``dataset'': ``xx''\}\newline

Please describe all numeric cells in the above latex table following the JSON templates (proceeding by row in a left-right, top-down direction). For each cell, output one JSON description per line. For any unanswerable attributes in the templates, set their value to the placeholder ``xx'' if it is of string type and \{``xx'': ``yy''\} if it is of dictionary type.\newline

Cell Description:\newline
\{``value'': ``[Query cell]'', ``type'':
\\ 


\midrule

\chemtables{} & 

[Input table] \newline

Here are JSON templates for six types of numeric cells: ``Other'', ``IC50'', ``EC50'', ``CC50'', ``MIC'', and ``GI50'':\newline
\{``value'': ``xx'', ``type'': ``Other''\}\newline
\{``value'': ``xx'', ``type'': ``IC50'', ``unit'': ``xx'', ``treatment compound'': ``xx'', ``target compound'': ``xx''\}\newline
\{``value'': ``xx'', ``type'': ``EC50'', ``unit'': ``xx'', ``treatment compound'': ``xx'', ``target compound'': ``xx''\}\newline
\{``value'': ``xx'', ``type'': ``CC50'', ``unit'': ``xx'', ``treatment compound'': ``xx'', ``target compound'': ``xx''\}\newline
\{``value'': ``xx'', ``type'': ``MIC'', ``unit'': ``xx'', ``treatment compound'': ``xx'', ``target compound'': ``xx''\}\newline
\{``value'': ``xx'', ``type'': ``GI50'', ``unit'': ``xx'', ``treatment compound'': ``xx'', ``target compound'': ``xx''\}\newline

Please describe all numeric cells in the above XML table following the JSON templates (proceeding by row in a left-right, top-down direction). For each cell, output one JSON description per line. For any unanswerable attributes in the templates, set their value to the placeholder ``xx''.\newline

Cell Description:\newline
\{``value'': ``[Query cell]'', ``type'':
\\ 

 \bottomrule
\end{tabular}
}

\caption{\label{tab:full_prompt_ml_chem} \method{} prompts used for \data{} and \chemtables{}.
}
\end{table*}


\begin{table*}[th!]
\small
\centering
\scalebox{1}{
\begin{tabular}{lL{11cm}}
\toprule 

 \textbf{Dataset} & \textbf{Full Prompt}  \\
 
\midrule

\discomat{} & 

[Input table] \newline

Here are JSON templates for two types of numeric cells: ``Other'' and ``Glass\_Compound\_Amount'':\newline
\{``value'': ``xx'', ``type'': ``Other''\}\newline
\{``value'': ``xx'', ``type'': ``Glass\_Compound\_Amount'', ``constituent compound name'': ``xx'', ``unit'': ``xx'', ``glass material/sample name/id/code'': ``xx''\}\newline

Please describe all numeric cells in the above table following the JSON templates (proceeding by row in a left-right, top-down direction). For each cell, output one JSON description per line. For any unanswerable attributes in the templates, set their value to the placeholder ``xx''.\newline

Cell Description:\newline
\{``value'': ``[Query cell]'', ``type'':
\\ 


\midrule

SWDE-auto & 

[Input webpage] \newline

Here is the JSON template for automobile attribute extraction:\newline
\{``webpage title'': ``xx'', ``automobile model (year)'': ``xx'', ``price'': ``xx'', ``engine type'': ``xx'', ``fuel economy'': ``xx''\}\newline

Please extract the automobile' attributes from the HTML code above following the JSON template. For any unanswerable attributes in the template, set their value to the placeholder ``<NULL>''.\newline
\{``webpage title'': ``[webpage title]'', ``automobile model (year)'':
\\ 

 \bottomrule
\end{tabular}
}

\caption{\label{tab:full_prompt_mat_swde} \method{} prompts used for \discomat{} and SWDE. For SWDE, we use the ``Auto'' vertical as an illustrative example, and the prompts for other verticals differ only in attribute names (refer to Table \ref{tab:swde_stats} for the attributes of each vertical).
}
\end{table*}



\begin{table}[ht]
\centering
\scalebox{0.6}{
\begin{tabular}{cccc}
\toprule 
\textbf{Vertical} & \textbf{\# Sites} & \textbf{\# Pages} & \textbf{Attributes} \\ 
\midrule
Auto & 10 & 17,923 & \texttt{model, price, engine, fuel-economy} \\ \hline
Book & 10 & 20,000 & \texttt{\makecell{title, author, ISBN-13,\\ publisher, publish-date}} \\ \hline
Camera & 10 & 5,258 & \texttt{model, price, manufacturer} \\ \hline
Job & 10 & 20,000 & \texttt{title, company, location, date} \\ \hline
Movie & 10 & 20,000 & \texttt{title, director, genre, rating} \\ \hline
NBA Player & 10 & 4,405 & \texttt{name, team, height, weight} \\ \hline
Restaurant & 10 & 20,000 & \texttt{name, address, phone, cuisine} \\ \hline
University & 10 & 16,705 & \texttt{name, phone, website, type} \\
\bottomrule
\end{tabular}
}
\caption{SWDE statistics. } 
\label{tab:swde_stats}
\end{table}







\begin{table}[!t]
\centering
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{clcccccc}
\toprule
\multirow{2}{*}{\textbf{Setting}} & \multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c}{\textbf{Token-level F\textsubscript{1}}} & \multicolumn{3}{c}{\textbf{EM}} \\
  \cmidrule(l){3-5} \cmidrule(l){6-8} 
 & & \textbf{P} & \textbf{R} & \textbf{F\textsubscript{1}} & \textbf{P} & \textbf{R} & \textbf{F\textsubscript{1}} \\ 
\midrule

\multirow{2}{*}{Fine-tune} & TaPas (large) & 24.6 & 17.8 & 19.4 & 21.6 & 15.6 & 16.9 \\ 
& T5 (11B) & 45.7 & 45.7 & 41.8 & 43.3 & 42.7 & 39.2 \\ 

\midrule

\multirow{3}{*}{Prompt} & Flan-T5 (11B) & 32.4 & 46.1 & 36.4 & 27.7 & 40.4 & 31.3 \\
& StarCoder (15.5B) & 37.1 & 38.4 & 36.1 & 33.5 & 34.2 & 32.3 \\
& \texttt{code-davinci-002} & 82.4 & 84.4 & 83.0 & 77.2 & 79.2 & 77.8 \\

\bottomrule
\end{tabular}
}
\caption{
    ChemTables results.
}
\label{tab:chemtables}
\end{table}


\section{\task{} Benchmark}
\label{sec:benchmark}


\subsection{\data{}}
\label{sec:mltab_appendix}









\paragraph{Extraction Attributes} We design a set of extraction attributes for each of the three primary types of numeric cells in \data{}: ``Result'', ``Hyper-parameter'', and ``Data Statistics''. These attributes are outlined in detail below.

\begin{itemize}[parsep=3pt,leftmargin=0.5cm]
    \item ``Result'' includes seven attributes: \texttt{training data}, \texttt{test data}, \texttt{task}, \texttt{metric}, \texttt{model}, \texttt{model settings} and \texttt{experimental settings}. The first five attributes are fixed, with answers being text spans in the paper. The last two attributes, \texttt{model settings} and \texttt{experimental settings}, are free-form attributes, with answers being JSON objects. 
    For example, the \texttt{experimental settings} attribute may be \textit{\{``number of training examples'': ``0''}\} for a zero-shot setting.
This scheme is more detailed than previous approaches \citep{hou-etal-2019-identification, kardas-etal-2020-axcell} and can accommodate a broader range of ML paradigms and provide more granular information.

\item ``Hyper-parameter'' includes optimization parameters like learning rate and batch size, as well as numeric descriptions of model architectures such as layer count. The three fixed attributes for this category are: \texttt{model}, \texttt{parameter/architecture}, and \texttt{dataset}.
    


\item ``Data Stat.'' covers four attributes: \texttt{dataset}, \texttt{dataset attribute}, \texttt{sub-set/group}, and \texttt{dataset features}. The \texttt{sub-set/group} specifies a dataset subset (e.g., ``train'' or ``test''), while \texttt{dataset features}, a free-form attribute, captures various dataset characteristics like language or domain.

\end{itemize}


\paragraph{Inter-annotator Agreement Score} For the inter-annotator agreement (IAA) score, we treat annotations from two annotators as the gold and predicted labels separately, and calculate the Table-F\textsubscript{1} (detailed in Section \ref{sec:eval_metric}) as the final IAA score. By applying thresholded token-level F\textsubscript{1} for attribute matching, we reach a Table-F\textsubscript{1} of 96.6. When we require attributes to be perfectly matched, the Table-F\textsubscript{1} drops to 76.6.



\subsection{\chemtables{}}
\label{sec:chemtab_appendix}

\paragraph{Inter-annotator Agreement Score} Similar to \data{}, we calculate the Table-F\textsubscript{1} using the double-annotated examples for the IAA score. For \chemtables{}, we achieve a Table-F\textsubscript{1} of 91.0 and 83.9 when applying thresholded token-level F\textsubscript{1} and exact match for attribute-level evaluation respectively.











\begin{figure}[t]
    \centering
    \includegraphics[scale=0.53]{figures/metrics.pdf}
    \caption{
Results of comparing various metrics, including token-level F\textsubscript{1}, SBERT, and BERTScore, to human judgment over different thresholds. Numbers are computed over 677 sampled attributes that are paired with respective gold references.
}
    \label{fig:metrics}
\end{figure}







































\begin{figure*}[!ht]
    \centering
    \includegraphics[width=\textwidth]{figures/analysis_of_caption.pdf}
    \caption{
An error analysis of edge cases in which the predictions made by \textsc{InstrucTE} with captions default to ``Other'' (resulting in an 0 F\textsubscript{1}). Our hypothesis that this issue may stem from the caption's lack of specificity is tested by manually expanding the caption (displayed on the right). This amendment significantly improves the performance on these edge cases, increasing the F\textsubscript{1} score to 92.3.
    }
    \label{fig:analysis_of_caption}
\end{figure*}


\section{Evaluation Metrics}
\label{sec:eval_metric_app}



Comparing an LLM-predicted JSON object with a gold JSON object is a non-trivial task, as those generative LLMs may produce text spans that do not exactly exist in the input table. Consequently, we devote substantial effort to examining various metrics to determine the one best suited for our task. Here, we consider three metrics: the standard token-level F\textsubscript{1} to capture the level of lexical overlap between the predicted and gold attributes, and two semantic similarity metrics, SBERT \citep{reimers-2019-sentence-bert} and BERTScore \citep{bert-score}, to identify semantically similar expressions (e.g., \# params vs. the number of parameters). 

\paragraph{Meta Evaluation}
To assess how accurate each metric is compared to human evaluation, we manually annotated predicted-gold attribute pairs as to whether or not each pair matches. We consider a given pair to ``match'' if they are semantically equivalent, meaning they can be used interchangeably. For attributes that encapsulated multiple sub-attributes, we consider a pair to match if at least half of the sub-attributes are matched (i.e., F\textsubscript{1} score $\geq$ 0.5), with the decision for each sub-attribute being based on the same as in the text-span attributes. For the set of pairs to annotate and use as a test set, we sample a total of 100 cell pairs (i.e., 677 attribute pairs) according to the following process: 1) we first uniformly sample a table from the development set (containing 10 papers); and 2) we then sample a random cell from the table, ensuring there were no duplicate cells. For each pair of predicted-gold attributes, each metric's decision (1 or 0) is made using a specific threshold. For example, if the token-level F\textsubscript{1}'s score for paired attributes is 0.4 and the threshold is 0.5, then the decision would be 0, indicating no match. The decisions over the test set containing 677 attribute pairs are then compared to human evaluation. In this binary classification problem, F\textsubscript{1} is used to evaluate the performance of the metrics.


In Table \ref{tab:metrics}, we present the performances of each metric with the optimal threshold for each. Surprisingly, we find that the token-level F\textsubscript{1} (with a threshold of 0.25) decision aligns nearly perfectly with human judgment, and performs the best among all metrics for our task. This might suggest that discerning subtle differences is more crucial than identifying different phrases with the same meaning for this task. Based on these empirical findings, we opt for the token-level F\textsubscript{1} for automatic evaluation at the attribute level. This choice is highly desirable not only because of its high accuracy but also due to its simplicity.




\begin{table}[t]
\centering
\scalebox{0.75}{
\begin{tabular}{lccc}
\toprule 
 & \textbf{token-level F\textsubscript{1}} & \textbf{SBERT} & \textbf{BERTScore} \\ 
\midrule
Meta Eval. F\textsubscript{1} & 97.0 & 95.6 & 96.7 \\
Threshold & 0.25 & 0.55 & 0.85 \\
\bottomrule
\end{tabular}
}
\caption{
Results of comparing various metrics, including token-level F\textsubscript{1}, SBERT, and BERTScore, to human judgment. 
Numbers are computed over 677 sampled attributes that are paired with gold references. The highest achieved F\textsubscript{1} scores are displayed alongside the thresholds. A complete illustration of results, sorted by thresholds, can be found in Figure \ref{fig:metrics} in Appendix.
}
\label{tab:metrics}
\end{table}


\section{Implementation Details}
\label{sec:implement_details}

Considering the lengthy source code for tables, we employ different strategies to encode the input table and perform Schema-Driven IE, based on the context length of the chosen LLM. For LLMs with a larger context length, such as \texttt{code-davinci-002} and StarCoder (8K tokens), we input the full table and conduct the proposed error recovery process up to 25 iterations. 
For LLMs with a more limited context length, such as LLaMA, Alpaca, and T5-11B, we query each target cell individually. The input table is condensed by rows, retaining only the first two rows, typically containing headers, and the row with the query cell, with a special token \texttt{<select>} pinpointing the position of the query cell. We use greedy decoding to maximize the reproducibility of our results.

For the TableQA setting, we divide the problem into two steps: selecting the record type and predicting the relevant attributes. For T5 and Flan-T5, the first step is modeled as a multi-choice QA problem, where the model chooses the type of the query cell from a list of provided options. The second step is modeled as an extractive QA task, asking the model to pinpoint the answer spans for the attributes associated with the selected type. For TaPas, the initial step is treated as a classification problem, whereas the latter one is handled as a cell selection problem. The hyper-parameters used for fine-tuning T5 and TaPas are presented in Table \ref{tab:hyperparam_ft}.

\begin{table}[t]
\centering
\scalebox{0.75}{
\begin{tabular}{lcc}
\toprule 
 & \textbf{T5 (11B)} & \textbf{TaPas}  \\ 
\midrule
learning rate & 1e-4 & 5e-5  \\
batch size & 8 & 32  \\
\# epoches  & 5 & 10 \\
\bottomrule
\end{tabular}
}
\caption{Hyper-parameters used for fine-tuning T5 and TaPas.
}
\label{tab:hyperparam_ft}
\end{table}


\section{Error Analysis of Caption}
\label{sec:error_analysis_of_caption}

In Section \ref{sec:ablation}, we observe an unexpected finding that table captions do not enhance performance, but rather seem to detract from it, which is counterintuitive. To delve deeper into this observation, we conduct an error analysis. This involves comparing the performances of our \textsc{InstrucTE} system with and without captions at the table level. This analysis uncovers a few outliers (3 out of 68) where including a caption leads to a 0 F\textsubscript{1} score, whereas the score is near perfect when the caption is excluded.
For instance, as depicted in Figure \ref{fig:analysis_of_caption}, the predictions all fall into the ``Other'' category when a caption is included, leading to a 0 F\textsubscript{1} score in these outlier instances. Conversely, removing the caption results in an F\textsubscript{1} score of 89.3. This high score is due to the fact that retrieved paragraphs provide ample contextual information (e.g., ``hate speech detection'') even without the presence of a caption.


We hypothesize that the model's inclination to predict ``Other'' in the presence of a caption may be a consequence of the captions' lack of specificity with respect to the attributes relevant to the table cells (for example, ``hate speech detection''). This lack of explicit, relevant details could create confusion in associating the caption with the retrieved paragraphs, thereby misleading the model.
To test our hypothesis, we manually adjust the captions to include more specific attributes, such as ``hate speech detection'' and ``T5-Base.'' As a result, we observe an improvement in the model's performance with the revised caption, with the total F\textsubscript{1} score even exceeding that achieved without a caption. This outcome partially supports our hypothesis and suggests that carefully crafted captions could indeed be beneficial, aligning with our initial expectations. However, this investigation also points to the fact that the model currently lacks robustness in handling these outlier scenarios.


\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\textwidth]{figures/gpt-4v-crop.pdf}
    \caption{Generate \LaTeX~code for image tables using GPT-4V. 
}
    \label{fig:gpt-4v}
\end{figure*}


\section{Leaderboard Extraction from ML Papers}
\label{sec:axcell_details}

\subsection{Task Definition and State-of-the-Art}
The task of leaderboard extraction entails extracting leaderboard tuples (\texttt{task}, \texttt{dataset}, \texttt{metric}, \texttt{score}) from tables in ML papers. Unlike our proposed Schema-Driven extraction, which requires open-domain span identification, leaderboard extraction presumes prior knowledge of all leaderboards, represented as pre-defined (\texttt{task}, \texttt{dataset}, \texttt{metric}) tuples, and centers on linking numeric cells to these leaderboards. 

The state-of-the-art leaderboard extraction method, \axcell{} \cite{kardas-etal-2020-axcell}, is a comprehensive pipeline system comprising four components: Table Type Classification, Table Segmentation, Cell Linking, and Filtering. For each component, except the last one, \axcell{} employs a supervised model. It starts with table type classification to identify result-related tables, which are then passed to the table segmenter responsible for annotating the header cells of the table. Following this step, a retrieval model links numeric cells in the table to pre-defined leaderboards using human-engineered features. Lastly, \axcell{} filters and selects the best record based on the leaderboard taxonomy criteria, such as retaining higher values for "Accuracy" and lower ones for "error rate".

\subsection{Leaderboard Extraction using \method{}}
To extract leaderboards from an ML paper, we consider all tables that contain numeric cells, instead of selecting tables via a trained classifier as in \axcell{}. For each table, we run \method{} using a customized leaderboard extraction JSON template. This template resembles the \data{} template with two additional fixed attributes: \texttt{eval split} and \texttt{eval class} in the ``Result'' cell template. We add the \texttt{eval split} attribute because the evaluated split is essential information for this task; for instance, \textit{``dev F\textsubscript{1}''} and \textit{``test F\textsubscript{1}''} are treated as different metrics in the leaderboard taxonomy. 
The \texttt{eval class} attribute is used to exclude sub-set or sub-class results that are typically present in analysis tables. After generating all predicted cell descriptions, we filter them based on three criteria: 1) the \texttt{type} attribute must be \textit{``Result''}; 2) the \texttt{eval class} attribute must be \textit{``all''} or \textit{``Null''} as observed on the development set; and 3) the cell must be bolded in the table, as this usually indicates its superior performance and possible relevance to the leaderboard. 
For papers without any bolded cells, we experiment with two strategies: 1) include all the remaining cells in the table that meet the first two criteria; 2) use cells selected by \axcell{}, as its engineered features for cell selection may be useful. This hybrid system is referred to as \method{}+.
We then use the predicted \texttt{task}, \texttt{dataset}, and \texttt{metric} attributes in each JSON record to match with the pre-defined leaderboards using token-level F\textsubscript{1}, and we select the leaderboard with the highest average score over three attributes. Finally, following \axcell{}, we choose the best record based on the leaderboard taxonomy criteria, e.g., retaining higher values for "Accuracy" and lower ones for "error rate".


\end{document}
