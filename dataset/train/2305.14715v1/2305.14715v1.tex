
\documentclass{article} \usepackage{iclr2023_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}

\usepackage{verbatim}
\usepackage{wrapfig,lipsum}

\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \newcommand\Mark[1]{\textsuperscript#1}

\usepackage{booktabs, tabularx}       \usepackage{makecell}
\usepackage{slashbox}
\usepackage{float}
\usepackage{multirow}

\usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{amsmath}        \usepackage{amssymb}        \usepackage{graphicx}       

\usepackage{subcaption, caption}

\title{Leveraging Future Relationship Reasoning for Vehicle Trajectory Prediction}
\author{Daehee Park\Mark{1}, Hobin Ryu\Mark{2}, Yunseo Yang\Mark{1}, Jegyeong Cho\Mark{1}, Jiwon Kim\Mark{2}, Kuk-Jin Yoon\Mark{1}\\
\Mark{1}Korea Advanced Institute of Science and Technology \hspace{5mm} \Mark{2}NAVER LABS\\
\texttt{\{bag2824,acorn,j2k0618,kjyoon\}@kaist.ac.kr} \\
\texttt{\{hobin.ryu,g1.kim\}@naverlabs.com} \\
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
\vspace{-5pt}
Understanding the interaction between multiple agents is crucial for realistic vehicle trajectory prediction. 
Existing methods have attempted to infer the interaction from the observed past trajectories of agents using pooling, attention, or graph-based methods, which rely on a deterministic approach. 
However, these methods can fail under complex road structures, as they cannot predict various interactions that may occur in the future.
In this paper, we propose a novel approach that uses lane information to predict a stochastic future relationship among agents. 
To obtain a coarse future motion of agents, our method first predicts the probability of lane-level waypoint occupancy of vehicles. 
We then utilize the temporal probability of passing adjacent lanes for each agent pair, assuming that agents passing adjacent lanes will highly interact.
We also model the interaction using a probabilistic distribution, which allows for multiple possible future interactions. 
The distribution is learned from the posterior distribution of interaction obtained from ground truth future trajectories.
We validate our method on popular trajectory prediction datasets: nuScenes and Argoverse. 
The results show that the proposed method brings remarkable performance gain in prediction accuracy, and achieves state-of-the-art performance in long-term prediction benchmark dataset.
\vspace{-8pt}
\end{abstract} 

\section{Introduction}
\vspace{-5pt}
For safe autonomous driving, predicting a vehicle's future trajectory is crucial.
Early heuristic prediction models utilized only the past trajectory of the target vehicle (~\cite{lin2000vehicle, barth2008will}).
However, with the advent of deep learning, more accurate predictions can be made by also considering the vehicle's relationship with the High-Definition (HD) map (~\cite{liang2020learning, zeng2021lanercnn}) or surrounding agents (~\cite{lee2017desire, chandra2020forecasting}).
Since surrounding vehicles are not stationary, predicting relationships with them is much more complicated and has become essential for realistic trajectory prediction.
Furthermore, since individual drivers control each vehicle, their interaction has a stochastic nature.

Previous works modeled interaction from past trajectories of the surrounding vehicles by employing pooling, multi-head attention, or spatio-temporal graph methods.
However, we observed that these methods easily fail under complex road structures. 
For example, Fig.~\ref{fig:attention_map_sa} shows the past trajectories of agents (left) and the attention weights among agents (right) obtained by a previous method (\cite{mercat2020multi}) that learned the interaction among agents using multi-head attention (MHA).
Since agents 0 and 4 are expected to join in the future, the attention weight between them should be high. 
However, the model predicts a low attention weight between them, highlighting the difficulty of reasoning future relationships between agents based solely on past trajectories. 
Incorporating the road structure should make the reasoning process much easier.

The decision-making process of human drivers can provide insights on how to model interaction.
They first set their goal where they are trying to reach on the map.
Next, to infer the interaction with surrounding agents, they roughly infer how the others will behave \textit{in the future}.
After that, they infer the interaction with others by inferring how likely the future path of other vehicles will overlap the path set by themselves.
The drivers consider interaction more significant the more the future paths of other vehicles overlap with their own.
We define the interaction from this process as a "\textit{Future Relationship}".
We use the following approaches to model Future Relationship, as shown in Fig.~\ref{fig:intro}.

\begin{figure}
\centering
\vspace{-10pt}
\begin{minipage}{.55\textwidth}
  \centering
  \includegraphics[width=0.75\linewidth]{fig/self_attention_1.pdf}
  \vspace{-10pt}
  \caption{Past trajectories and corresponding attention map between agents from previous work~(\cite{mercat2020multi}). A weak relationship is inferred between agents that will highly interact in the future: agents 0 and 4.}
  \label{fig:attention_map_sa}
\end{minipage}
\hspace{1cm}
\begin{minipage}{0.25\textwidth}
  \centering
  \vspace{8pt}
  \includegraphics[width=0.72\linewidth]{fig/lane_segments.pdf}
  \caption{Lane segments represented in different colors.}
  \label{fig:lane_segment}
\end{minipage}\vspace{-5pt}
\end{figure}

\begin{figure}
  \centering
  \hspace*{-0.0\linewidth}\includegraphics[width=0.9\linewidth]{fig/intro_.png}
  \vspace{-5pt}
  \caption{Key concept of the proposed method: From past observed trajectories, we predict the lane that a vehicle will pass in the future.
  The Interaction between agents is represented by an edge connecting their nodes, and is determined by the probability that two agents will pass adjacent lanes. 
  The greater the probability, the higher the expected interaction.}
  \label{fig:intro}
  \vspace{-10pt}
\end{figure}


First, we obtain the rough future motion of all vehicles in the scene.
Since vehicles mainly move along lanes, we utilize lane information as strong prior for representing the rough future motion of vehicles.
Because lane centerlines contain both positional and directional information, rough future motion can be represented as waypoint occupancy.
The waypoint occupancy is defined as the probability of a vehicle passing a specific lane segment at every intermediate timestep.
In the middle of Fig.~\ref{fig:intro}, each agent's waypoint occupancy is shown.
Here, we aggregated the temporal axis for simplification.
The probability that the vehicle passes that lane during the prediction horizon is drawn using the tone of red color.

Second, based on the waypoint occupancy, we infer the Future Relationship in probabilistic distribution.
In most vehicle trajectory prediction methods, the interaction between agents is still made in a deterministic manner.
However, we take note that interaction between vehicles is highly stochastic, and there can be multiple possible interactions.
The deterministic relation inference averages out diverse interactions, interrupting socially-aware trajectory prediction.
Therefore, we define Future Relationship in Gaussian Mixture (GM) distribution.
Motivated by Neural Relational Inference (NRI) (~\cite{kipf2018neural}), we propose a method to train the diverse interaction distribution explicitly.

In summary, our contributions are:

1) We propose a new approach for modeling the interaction between vehicles by incorporating the road structure and defining it as \textit{Future Relationship}.

2) We propose to infer the Future Relationship in probabilistic distribution using Gaussian Mixture (GM) distribution to capture diverse interaction.

3) The proposed method is validated on popular real-world vehicle trajectory datasets: nuScenes and Argoverse. 
In both datasets, there is a remarkable improvement in prediction performance and state-of-the-art performance is achieved in the long-range prediction dataset, nuScenes.

\section{Related Work}
\subsection{Goal-conditioned trajectory prediction}
Predicting future trajectories at once is a challenging task.
Instead, goal-conditional prediction, which samples goal candidates and then predicts trajectory conditioned on them, is helpful and has shown state-of-the-art performance, especially in long-range prediction tasks (~\cite{zhao2021tnt,gu2021densetnt,phan2020covernet,chai2020multipath,zhang2021map}).
CoverNet (~\cite{phan2020covernet}) and MultiPath (~\cite{chai2020multipath}), which quantize the trajectory space to a set of anchors, often generate map-agnostic trajectories that cross non-drivable areas because the surrounding map is not considered.
Recently, several studies have exploited the map information to obtain more performant goal candidates based on the assumption that vehicles follow lanes.
TNT (~\cite{zhao2021tnt}) uses goal points sampled from a lane centerline, and GoalNet (~\cite{zhang2021map}) uses lane segments as trajectory anchors.
However, while previous methods assume that the likelihood of arriving at a final destination is random, they assume that trajectories are unimodal in order to reach a specific goal area. 
In this paper, we assume inherent uncertainty in which trajectories can vary due to the interactions with surrounding vehicles in order to reach a specific goal area.

\subsection{Interaction modeling}
\label{sec:related_work_interaction_modeling}
Considering the interaction between agents helps to predict a socially aware trajectory.
In the very early stage, interaction is obtained by pooling interaction features in the local region (~\cite{deo2018convolutional, gupta2018social}).
In other works, researchers attempted to obtain interaction through attention-based (~\cite{ngiam2022scene, mercat2020multi, Vemula2018SocialAM}) or GNN-based method (~\cite{carrasco2021scout,  cao2021spectral, zeng2021lanercnn,casas2020implicit, liang2020learning,gao2020vectornet}).
However, in most previous methods, interactions between agents are learned only with regression loss, which is insufficient to represent dynamic and rapidly changing situations.
There exists a line of works that employs Neural Relational Inference (NRI) (~\cite{kipf2018neural}) that explicitly predicts and learns interaction using a latent interaction graph.
EvolveGraph (~\cite{li2020evolvegraph}) utilizes two interaction graphs, static and dynamic, and NRI-MPM (~\cite{chen2021neural}) uses a relation interaction mechanism and spatio-temporal message passing mechanism.
Similarly, we apply the NRI-based method to predict and train the interaction explicitly.

\subsection{Multi-modal trajectory prediction}
Trajectory prediction is a stochastic problem, which means that there are multiple possible futures instead of a unique answer.
Recently, deep generative models like GAN (\cite{goodfellow2014generative}) or VAE (\cite{kingma2013auto}) have been employed to address this issue.
GAN-based (\cite{gupta2018social,kosaraju2019social,li2021vehicle}) and VAE-based models (\cite{ivanovic2019trajectron,salzmann2020trajectron++,tang2019multiple}) predict multiple futures by sampling multiple latent vectors.
A well-organized latent space is necessary to sample meaningful latent vectors for predicting diverse, yet plausible future trajectories. 
This has become a natural choice in recent works (~\cite{ma2021likelihood, bae2022non}).
The work most closely related to ours is GRIN (\cite{li2021grin}), which argues that multi-modality in trajectory prediction comes from two sources: personal intention and social relations with other agents. 
However, GRIN only considers past interaction, while we propose to consider future interaction by taking into account the characteristics of vehicle motion. 
Since vehicle motion mainly follows lanes, we utilize lane information to infer future interactions.

\section{Formulation}

In each scene, the past and future trajectories of  vehicles are observed. 
The past trajectory  consists of positions for  timesteps before the current timestep, and the future trajectory  consists of positions for  timesteps after the current timestep (). 
Lane information is obtained from the HD map, which consists of  segmented lane polylines. 
The lane information is represented as a graph: , where the nodes () correspond to the different lane segments, and the edges () represent the relationships between the segments. 
There are five relationship between segments: \textit{predecessor}, \textit{successor}, \textit{left/right neighbor} and \textit{in-same-intersection}.
The input to the model is denoted as , which consists of the past and future trajectories of the vehicles and the lane information. 
Here, the future trajectories is only used in training.
The output of the model is denoted as , which consists of  predicted future trajectories for each agent.
The model also predicts the future lane occupancy (i.e., which vehicles are occupying which lanes) as a medium using a probability distribution  for each vehicle and lane segment at each future timestep: . 
The predicted future lane occupancy is denoted as , and the ground truth future lane occupancy is denoted as .

\section{Method}
Our focus is on modeling the "Future Relationship" between agents.
A naive method to infer the Future Relationship is to predict all future vehicle trajectories and then calculate similarity among them.
However, this method is inefficient and redundant, as it requires performing prediction twice.
Moreover, the criteria for calculating similarity between trajectories may not be clear.
In this paper, we utilize lane information for modeling the Future Relationship, inspired by the idea that the vehicles mainly follow lanes.
Our key idea is that \textit{if two vehicles are expected to pass on adjacent lanes, they will have a high chance of interacting in the future}.

We present the overall structure of our method in Fig.~\ref{fig:overall}.
First, we predict the waypoint occupancy, which represents the probability of a vehicle passing a specific lane segment during future time steps  (Sec.~\ref{sec:waypoint_prediction}).
Using this information, our Future Relationship Module (FRM) infers interaction as an edge feature connecting agent node pairs (Sec.~\ref{sec:fri_module}).
These interaction edges are used to transfer information between agent nodes to form the interaction feature through message passing.
Finally, in the decoding stage (Sec.~\ref{sec:decoder}), the decoder predicts future trajectories from the aggregation of the interaction feature and intention feature, which is derived from the concatenation of past motion and goal features.
Following AgentFormer (\cite{yuan2021agentformer}), our method is based on CVAEs 
where the condition corresponds to the intention, and the latent code corresponds to the interaction feature.
Then, we compute prior and posterior distribution of the interaction feature, as described in Sec.~\ref{sec:prior}, \ref{sec:posterior}.

\begin{figure}
  \centering
  \hspace*{-0.0\linewidth}\includegraphics[width=0.99\linewidth]{fig/overall.pdf}
  \vspace{-12pt}
  \caption{Overall structure of the proposed method. Given past/future motion inputs, the waypoint occupancy () is obtained. The goal features are then sampled following . Intention feature is derived from the goal features and the past motion (). The Future Relationship Module (FRM) utilizes the intermediate waypoint occupancy () to sample the interaction edges among agents. Message passing is then performed to obtain the interaction feature. Finally, the decoder predicts  future trajectories from concatenation of intention and interaction features.
  }  
  \label{fig:overall}
  \vspace{-7pt}
\end{figure}

\subsection{Waypoint Occupancy}
\label{sec:waypoint_prediction}
In this section, we describe how to obtain the waypoint occupancy. 
We need two waypoint occupancies: one predicted from past trajectory () and the ground truth () for obtaining prior and posterior distributions of interaction, respectively. 

To predict the waypoint occupancy from past trajectory, we first encode the past trajectories  and the lane graph  into past motion and lane features: , . 
Then, following TNT (\cite{zhao2021tnt}), we predict the waypoint occupancy as Eq.~(\ref{eq:waypoint}). Here,  denotes concatenation, and we apply softmax to ensure that the waypoint occupancy sum up to one: . 

For GT waypoint occupancy, we can directly obtain it from GT future trajectory since we know the position and heading of vehicles.
More details can be found in the supplementary material. 

\subsection{Future Relationship Module (FRM)}
\label{sec:fri_module}
\begin{figure}
  \centering
  \hspace*{-0.0\linewidth}\includegraphics[width=0.8\linewidth]{fig/frm.pdf}
  \caption{Future Relationship Module. During inference, predicted waypoint occupancy () is fed to GCN, dot-producted by itself to obtain inter-agent proximity (). Prior of interaction (, ) is then obtained as Gaussign Mixture. During training, GT waypoint occupancy () is fed to obtain posterior of interaction () as Gaussian distribution.}  
  \label{fig:fpi_module}
  \vspace{-3pt}
\end{figure}

Fig.~\ref{fig:fpi_module} shows the FRM, which consists of three parts: computing inter-agent proximity and obtaining posterior and prior distribution.
From intermediate waypoint occupancy of vehicle (), we compute how each pair of vehicle pass adjacent lanes adjacent to each other at each timestep (inter-agent proximity). 
Based on that information and agents' past motion features, we obtain two distribution of interaction.
In the following sections, we describe the details of each part.

\subsubsection{Inter-agent Proximity}

To compute the inter-agent proximity (), we first smooth the waypoint occupancy using a Graph Convolutional Network (GCN) (~\cite{welling2016semi}).
The reason for doing so is that when a vehicle passes a specific lane, it affects other vehicles that pass the adjacent lane, not necessarily the same lane.
Therefore, we apply different smoothing for each lane connectivity (predecessor, successor, neighbor, in-the-same-intersection) by employing 2-hop GCN layers.
Specifically, each layer aggregated information from neighboring lanes and applies a non-linear transformation.
This allows the model to capture spatial dependencies among agents and improve the accuracy of the inter-agent proximity computation.
Each layer is expressed as Eq.(\ref{eq:graph_convolution}) where , ,  and  are softmax followed by ReLU, degree, adjacency and weight matrix for each edge type, respectively.

With this smoothed waypoint occupancy, we can compute the inter-agent proximity using the dot product of  across the lane axis.


\subsubsection{Prior of the Interaction}
\label{sec:prior}
To obtain the prior distribution, we use the past motion features () and inter-agent proximity.
In this subsection, we omit superscript - for simplification.
There are two design factors for our interaction modeling: (i) interaction should reflect diverse and stochastic properties, and (ii) it occurs in every pair of vehicles.
Consequently, the prior distribution is defined as Gaussian Mixture (GM) per agent pair.
Then, we define interaction edge  between agent i and j as a d-dimensional feature () following GMVAE (~\cite{dilokthanakul2016deep}). 
The distribution parameters () are obtained from the neural network :

 is composed of MLP layers and 1-d conv layer~(\cite{deo2018convolutional}).
We then perform two sampling steps, one for the interaction mode  (from ) and one for  (from Gaussian noise).
This allows for K distinct interactions modes:


Next, we compute the interaction feature () via message passing from sampled interaction edge, as follows:
\vspace{-2pt}


\subsubsection{CVAE posterior}
\label{sec:posterior}
To obtain the posterior distribution, we use GT waypoint occupancy () and the future motion feature (), which is obtained from GT future trajectory and same motion encoder with past trajectory.
Similarly, we omit superscript + in this subsection.
Inter-agent proximity is obtained with same procedure in Eqs.~(\ref{eq:graph_convolution})-(\ref{eq:spatial_proximity}).
The difference from the prior is that the posterior is modeled in a single Gaussian ().
Thus,  is replaced with :

Then we sample , and interaction edge is obtained: .
Finally, following Eq.~(\ref{eq:short_message_passing}), interaction features () is obtained.

\subsection{Decoder}
\label{sec:decoder}
The decoder predicts future trajectories from the aggregation of the interaction feature () and intention feature ().
Here, the intention feature is obtained from past motion feature and goal feature following TNT.
During training, the unique GT intention feature is repeated  times, and we sample the interaction feature ()  times from the posterior distribution.
During inference, the intention feature is obtained from the past motion feature () and goal feature (), which is sampled  times from predicted waypoint occupancy at the final timestep ().
The interaction feature () is sampled  times from the prior distribution.
The decoder is composed of 2-layer MLP and predicts sequence of x,y coordinates. 
More details can be found in the supplementary material.

\subsection{Training}
\label{method_training}
Because the GT waypoint occupancy () is available, we can train the model to predict waypoint occupancy () using negative log-likelihood (NLL): .

However, since the interaction edge  is unobservable, we optimize the evidence lower bound (ELBO) to train the interaction distribution using the CVAE scheme.

Here,  is the approximate posterior, and  is the prior.
Since our model only allows the posterior to be Gaussian distribution, we can simplify the Kullback–Leibler (KL) divergence term as follow:

The detailed derivation with the reparameterization trick can be found in the supplementary material.
However, a common drawback with the NRI-based method is the "degenerate" issue, where the decoder tends to ignore the relation edge during training. 
To address this issue, we train the network to give different roles to the intention and interaction features. 
Since the GT trajectory is conditioned on the GT goal feature, we use the GT goal feature to compute the reconstruction term.
This training strategy restricts the role of interaction edge to momentary motion, resulting in the following reconstruction loss: .

Finally, the overall loss is the sum of the three losses, which are trained jointly:
.

\section{Experiments}
We train and evaluate our method on two popular real-world trajectory datasets: nuScenes (~\cite{caesar2020nuscenes}) and Argoverse (~\cite{chang2019argoverse}). nuScenes/Argoverse datasets provide the 2/2 seconds of past and require 6/3 seconds of future trajectory at 0.5/0.1 second intervals, respectively.
Training/validation/test sets consist of real-world driving scenes of 32,186/8,560/9,041 in nuScenes and 205,942/39,472/78,143 in Argoverse.
For the baseline model in ablation, we follow TNT for goal conditioned model, and MHA encodes interaction from past trajectories.
For implementation and computation details, please refer to the supplementary material.

\subsection{Quantitative result}
\begin{table}[]
\begin{minipage}{.45\linewidth}
\renewcommand{\tabcolsep}{1mm}
    \renewcommand{\arraystretch}{0.3}
    \caption{Comparison on nuScenes test set. Best in \textbf{bold}, second best in \underline{underline}.}
    \label{tab:quantitative}
    \centering
    {\scriptsize
    \begin{tabular}{l|ccccccc}
        \toprule
            \multicolumn{1}{c|}{Paper}                           & mADE      & mADE   & MR        & MR     & mFDE            \\ \hline
            \rule{0pt}{9pt} \makecell[l]{Trajectron++ \\ ~\cite{salzmann2020trajectron++}}         & 1.88          & 1.51          & 0.70          & 0.57          & 9.52         \\
            \rule{0pt}{9pt} \makecell[l]{P2T \\ ~\cite{deo2020trajectory}}         & 1.45          & 1.16          & 0.64          & 0.46          & 10.5         \\
            \rule{0pt}{9pt} \makecell[l]{AgentFormer \\ ~\cite{yuan2021agentformer}}          & 1.86          & 1.45          & -          & -          & -        \\
            \rule{0pt}{9pt} \makecell[l]{LaPred \\ ~\cite{kim2021lapred}}          & 1.47          & 1.12          & 0.53          & 0.46          & 8.37         \\
            \rule{0pt}{9pt} \makecell[l]{MultiPath \\ ~\cite{chai2020multipath}}       & 1.44          & 1.14          & -          & -          & 7.69        \\
            \rule{0pt}{9pt} \makecell[l]{GOHOME \\ ~\cite{gilles2022gohome}}       & 1.42          & 1.15          & 0.57          & 0.47          & 6.99        \\
            \rule{0pt}{9pt} \makecell[l]{Autobot \\ ~\cite{girgis2021latent}}      & 1.37          & 1.03          & 0.62          & 0.44          & 8.19 \\
            \rule{0pt}{9pt} \makecell[l]{THOMAS \\ ~\cite{gilles2022thomas}}       & 1.33          & 1.04          & 0.55          & 0.42          & \underline{6.71}          \\  
\rule{0pt}{9pt} \makecell[l]{PGP \\ ~\cite{deo2022multimodal}}         & \underline{1.27}          & \underline{0.94}          & \underline{0.52}          & \underline{0.34}          & 7.17                   \\ \hline
\rule{0pt}{9pt} Ours         & \textbf{1.18} & \textbf{0.88} & \textbf{0.48} & \textbf{0.30} & \textbf{6.59} \\ 
        \bottomrule
    \end{tabular}
    }
\end{minipage}
\hfill
\begin{minipage}{.45\linewidth}
\renewcommand{\tabcolsep}{1mm}
\renewcommand{\arraystretch}{1}
\caption{Comparison on Argoverse val/test set. Best in \textbf{bold}, second best in \underline{underline}.}
\label{tab:quantitative_argo}
      \centering
      {\scriptsize
        \begin{tabular}{l|cc|cc}
        \toprule
        \multicolumn{1}{c|}{\multirow{2}{*}{Paper}} & \multicolumn{2}{c|}{Val set}        & \multicolumn{2}{c}{Test set}        \\ \cline{2-5} 
        \multicolumn{1}{c|}{}                       & mADE         & mFDE         & mADE         & mFDE         \\ \hline
        \rule{0pt}{9pt}\makecell[l]{TNT \\ ~\cite{zhao2021tnt}}                                         & 0.73             & 1.29             & 0.94             & 1.54             \\
\rule{0pt}{9pt}\makecell[l]{LaneRCNN \\ ~\cite{zeng2021lanercnn}}                                    & 0.77             & 1.19             & 0.90             & 1.45             \\
        \rule{0pt}{9pt}\makecell[l]{TPCN \\ ~\cite{ye2021tpcn}}                                        & 0.73             & 1.15             & 0.87             & 1.38             \\
        \rule{0pt}{9pt}\makecell[l]{Autobot \\ ~\cite{girgis2021latent}}                                & 0.73             & 1.10             & 0.89             & 1.41             \\
        \rule{0pt}{9pt}\makecell[l]{mmTransformer \\ ~\cite{liu2021multimodal}}                                & 0.72             & 1.21             & 0.84             & 1.34             \\
        \rule{0pt}{9pt}\makecell[l]{SceneTransformer \\ ~\cite{varadarajan2022multipath++}}                            & -                & -                & 0.80             & 1.23             \\
        \rule{0pt}{9pt}\makecell[l]{Multipath++ \\ ~\cite{varadarajan2022multipath++}}                                 & -                & -                & \underline{0.79} & \underline{1.21} \\
        \rule{0pt}{9pt}\makecell[l]{HiVT \\ ~\cite{zhou2022hivt}}                                        & 0.66             & 0.96             & \textbf{0.77}    & \textbf{1.17}    \\ \hline
        Baseline                                    & 0.71             & 1.03             &  0.86                & 1.30                 \\
        Ours                                        & \underline{0.68} & \underline{0.99} &  0.82                & 1.27             \\ 
        \bottomrule
        \end{tabular}}
\end{minipage}
\vspace{-5pt}
\end{table}

Our method outperforms SoTA models in all nuScenes benchmark metrics, as shown in Tab.~\ref{tab:quantitative}.
Specifically, our model outperforms the runner-up method, PGP (~\cite{deo2022multimodal}), by a substantial margin.
This result indicates that our explicit interaction modeling via inferring waypoint occupancy helps scene understanding compared to the implicit interaction modeling of PGP.
When predicting 10 samples, our model shows improvements of 5.3\% and 8.8\% in terms of  and .
Previously, THOMAS (~\cite{gilles2022thomas}) was ranked first in  by proposing a recombination module that post-processes marginal predictions into the joint predictions that are aware of other agents.
However, our model performs better than THOMAS in , indicating better interaction modeling ability without post-processing.
This is possible because inferring future relationships helps to better understand the future interaction with other agents; details are provided in the ablation study.

We also evaluated our method on the Argoverse dataset.
While our model does not achieve SoTA performance, it still shows remarkable performance improvement in both validation and test sets.
Moreover, except for the HiVT, our method make competitive performance in .
Please note that our model (0.82) is still comparable to SceneTransformer (0.80) and Multipath++ (0.79) in the test set results.
However, HiVT uses the surrounding vehicles' trajectories for training, resulting in increased training data.
Therefore, a direct comparison to HiVT would be rather unfair.

\begin{table}[]
\begin{minipage}{.47\linewidth}
\caption{Impact of prediction time to the proposed modeling in terms of mADE/mADE.}
\label{tab:ablation_prediction_time}
      \centering
      {\scriptsize
         \begin{tabular}{l|cc|c} \hline
                         & Baseline  & Ours      & Improvement \\ \hline
        nuScenes (6sec)  & 3.23/1.17 & 2.89/1.10 & \textbf{10.5}\%/6.0\%  \\
        nuScenes (3sec)  & 1.26/0.50 & 1.19/0.48 & 5.6\%/4.0\%   \\
        Argoverse (3sec) & 1.41/0.71 & 1.33/0.68 & 5.7\%/4.2\% \\
        \hline
        \end{tabular}}
\end{minipage}
\hfill
\begin{minipage}{.45\linewidth}
    \renewcommand{\arraystretch}{1.3}
\caption{Ablation studies on nuScenes.}
\label{tab:ablation_main}
          \centering
          {\scriptsize
          \begin{tabular}{lcc}
                \hline
                \multirow{2}{*}{}                                                                                             & F=1                         & F=5                         \\
                                                                                                                              & mADE/mFDE                   & mADE/mFDE                   \\ \hline \hline
                \multicolumn{3}{c}{Impact of model design}                                                                            \\ \hline
                Baseline                                                                                                      & 3.23/7.60                   & 1.26/2.49                   \\
                Ours w/o FR                                                                                                   & 3.21/7.59                   & 1.26/2.50                   \\
                Ours w/o GCN                                                                                                  & 3.04/6.94                   & 1.22/2.41                   \\
                Ours w/ Sym                                                                                                    & 2.99/6.78                   & 1.22/2.35                   \\ \hline \hline
                \multicolumn{3}{c}{\makecell{Importance of multimodal stochastic interaction}}          \\ \hline
                Ours w/ GP                                                                                                    & 2.98/6.78                   & 1.20/2.33                   \\
                \makecell{Ours w/  Deterministic}                                                                                         & 2.96/6.80                   & 1.28/2.52                   \\ \hline \hline
                Ours (Full)                                                                                                   & \textbf{2.89}/\textbf{6.61} & \textbf{1.19}/\textbf{2.30} \\ \hline
                \end{tabular}}
    \end{minipage}
\vspace{-5pt}
\end{table}
We do not achieve SoTA in Argoverse because the proposed method is less effective than in nuScenes.
We attribute this disparity to the differences in dataset configurations, where nuScenes requires predicting a longer future trajectory than Argoverse. As intuition suggests, interaction modeling has a more significant impact on longer-range prediction tasks. To validate this assumption, we conducted an ablation study by measuring the performance gain on nuScenes when predicting the same length of future as Argoverse. The results, presented in Tab.~\ref{tab:ablation_prediction_time}, shows that our interaction modeling method improves  by over 10\% in a 6-second prediction task, but its effect was halved in a 3-second prediction task, which is similar to the results obtained in Argoverse. This finding suggests that our interaction modeling method is more effective in longer-range prediction tasks.

\subsection{Qualitative result}
\label{sec:qualitative_result}
\begin{figure}
  \centering
  \hspace*{-0.0\linewidth}\includegraphics[width=0.8\linewidth]{fig/qualatative_ver.pdf}
\caption{Qualitative results of the proposed method. The green solid line is past trajectories, the red lines are 6 predicted samples by baseline (left) and our method (right). The blue line is GT future trajectory of the surrounding vehicles. Lane centerlines are in gray dashed lines. In complex road scenes, baseline generates spatially uniform samples regardless of interaction with surrounding vehicles. On the other hand, our method generates diverse yet interaction-aware samples: wait or surpass other vehicles that would join in the future.}  
  \label{fig:qualatative}
  \vspace{-5pt}
\end{figure}
In Fig.~\ref{fig:qualatative}, we present prediction samples (F=6) from the baseline (left) and our method (right).
To assess the efficacy of our method, we brought the samples with two agents and plotted the prediction of a single agent per scene.
The green, blue, and red solid lines indicate the past trajectories of the both agents, future trajectories of the surrounding agents, and prediction samples of the target agents, respectively.
In two scenes, each target agent sets its intention to where the other agent is likely to pass in the future.
Our method generates prediction samples that incorporate and leverage the Future Relationship with other agents.
Which means, unlike the baseline method that ignores other agents and generates spatially uniform trajectories, our model surpasses or waits for the other agents accounting for interaction.
Moreover, not only considering two modes of interaction; surpass or wait, we also allow stochasticity within a single mode of interaction.
Consequently, our model generates diverse yet interaction-aware samples.

Furthermore, our method can incorporate stochastic interaction when multiple agents are present.
In the experiment shown in Fig.~\ref{fig:qualatative_samples}, we predict the trajectories of the target agents (denoted as 0) with multiple interacting vehicles.
In each scene, the intention of the target vehicle is fixed (denoted in green) and two interaction edges are sampled. 
The corresponding predicted trajectory samples and the degree of interaction () are plotted on the right.
In the first row of the figure, the target agent (0) infers significant interaction with agent 2 in sample 1.
As agent 2 is moving in the same direction and is predicted to move ahead, our model generates an accelerating trajectory to follow agent 2.
In contrast, in sample 2, the interaction with agent 1 is sampled as significant because they are expected to be in the same intersection.
In this case, our model generates decelerating trajectory considering the future motion of agent 1.
Importantly, all predicted trajectories in these samples are appropriately constrained within the goal lane segments as the intention is set to the green colored lane.
This indicates that our training strategy effectively restricts the role of interaction features to momentary motion.


\subsection{Ablation study}
\label{sec:ablation_study}
Ablation on the impact of model design is shown in the upper part in Tab.~\ref{tab:ablation_main}.
The \textbf{Ours w/o FR} variant does not consider Future Relationship in interaction modeling, and only uses past trajectories to infer the relation, similar to the NRI.
This variant performs almost identically to the baseline, which uses MHA of past trajectories to model interaction. 
This result shows the importance of leveraging Future Relationship for plausible interaction inference.
The \textbf{Ours w/o GCN} variant omits the smoothing waypoint occupancy leading to inaccurate inter-agent proximity (PR) estimation, especially in the posterior distribution.
Since GT waypoint occupancy is a binary value, computing PR from it can result in inaccurate proximity and lower prediction performance.
In the proposed model, we allow asymmetric interaction between two agents.
The \textbf{Ours w/ Sym} applies hard symmetric interaction modeling (~\cite{li2019structure}), and it shows that our asymmetric design is more suitable for modeling the driver relation.


The importance of multi-modal stochastic interaction modeling is shown in the lower part of Tab.~\ref{tab:ablation_main}.
The \textbf{Ours w/ GP} variant models the prior distribution as Gaussian distribution instead of GM, considering only a single modality of interaction, which leads to a performance decline compared to the full model with multi-modal interaction.
The \textbf{Ours w/ Deterministic} variant predicts only the mean of interaction edges in Eq.~\ref{eq:interaction_edge}.
Although it can model multi-modal interaction, the diversity is prone to be limited compared to the stochastic counterpart especially when the sample size F is large.
The result shows that stochastic modeling is critical for prediction performance, and deterministic modeling significantly degrades the prediction performance when predicting more samples.
In contrast, the \textbf{Ours w/ GP} variant shows relatively less performance drop as it maintains stochasticity even after removing the GM prior.



\begin{figure}
  \centering
  \hspace*{-0.0\linewidth}\includegraphics[width=0.83\linewidth]{fig/qualitative_multi_agent.png}
  \vspace{-5pt}
  \caption{Qualitative results of the proposed method in multi-agent scene.}  
  \label{fig:qualatative_samples}
  \vspace{-10pt}
\end{figure}

\vspace{-5pt}
\section{Conclusion}
\label{sec:conclusion}
\vspace{-5pt}

In this paper, we propose Future Relationship to effectively learn the interaction between vehicles for trajectory prediction.
By explicitly utilizing lane information in addition to past trajectories, our FRM can infer proper interactions even in complex road structures.
The proposed model generates diverse yet socially plausible trajectory samples by obtaining interaction probabilistically, which provides explainable medium such as waypoint occupancy or inter-agent proximity.
We trained our model using CVAE scheme and validated it on popular real-world trajectory prediction datasets.
Our approach achieved SoTA performance in a long-range prediction task, nuScnes, and brings remarkable performance improvement in a short-range prediction task, Argoverse.
Modeling Future Relationship is a novel approach, and we anticipate that using more sophisticated training methods~(\cite{ye2022dcms, zhou2022hivt}) or a better baseline model (such as GANet~(\cite{wang2022ganet}) may further improve prediction performance.

\subsubsection*{Acknowledgments}
This work was supported by Institute of Information \& Communications Technology Planning \& Evaluation(IITP) grant funded by the Korea government(MSIT) (No.2014-3-00123, Development of High Performance Visual BigData Discovery Platform for Large-Scale Realtime Data Analysis.


\bibliography{mybib}
\bibliographystyle{iclr2023_conference}


\end{document}
