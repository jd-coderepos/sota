

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{wacv} 

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{ulem}
\usepackage{adjustbox}
\usepackage[dvipsnames]{xcolor}
\usepackage{siunitx}
\usepackage{nicefrac}
\usepackage{multirow}
\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\todo}[1]{{\color{red}#1}}
\newcommand{\TODO}[1]{\textbf{\color{red}[TODO: #1]}}
\newcommand{\nparagraph}[1]{\noindent\textbf{#1.  }}


\usepackage[acronym]{glossaries}
\newacronym{mtmc}{MTMC}{Multi-Target Multi-Camera}
\newacronym{bev}{BEV}{Bird’s Eye View}
\newacronym{moda}{MODA}{Multiple Object Detection Accuracy}
\newacronym{modp}{MODP}{Multiple Object Detection Precision}
\newacronym{iou}{IoU}{Intersection over Union}
\newacronym{fov}{FOV}{Field of View}
\newacronym{mot}{MOT}{Multiple Object Tracking}
\newacronym{mota}{MOTA}{Multiple Object Tracking Accuracy}
\newacronym{motp}{MOTP}{Multiple Object Tracking Precision}
\newacronym{reid}{re-ID}{Re-Identification}
\newacronym{pom}{POM}{probabilistic occupancy map}
\newacronym{nms}{NMS}{non-maximum suppression}

\usepackage{xspace}
\def\ap{\ensuremath{\text{AP}}\xspace}
\def\map{\ensuremath{\text{mAP}}\xspace}
\def\mapfifty{\ensuremath{\text{mAP}_{\text{50}}}\xspace}

 
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

\usepackage[shortlabels]{enumitem}

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\wacvPaperID{2} \def\confName{WACV}
\def\confYear{2024}

\def\sname{EarlyBird\xspace}

\begin{document}

\title{EarlyBird: Early-Fusion for Multi-View Tracking in the Bird’s Eye View}


\author{
Torben Teepe\thanks{Correspondence to \texttt{t.teepe@tum.de}} \qquad Philipp Wolters \qquad Johannes Gilg \qquad Fabian Herzog \qquad Gerhard Rigoll\
\label{eq:perspective_4x3}
\resizebox{0.9\linewidth}{!}{
}
\label{eq:perspective_3x3}
\resizebox{0.78\linewidth}{!}{}
where  denotes the  perspective transformation matrix without the third column from . We apply \cref{eq:perspective_3x3} to project the features from all  cameras, with their projection , to the ground plane grid of a predefined size . The size of the ground plane grid depends on the size of the observed and annotated area. Each grid position represents an area of , downsampling the annotation grid further by  due to memory concerns. All stacked feature maps with -channels from  cameras give us \gls{bev} feature of size . 

\subsection{Aggregation \& Decoder}
The goal of the aggregation stage is to combine the features from all  cameras into a single feature, i.e., reduce the -dimension of the \gls{bev} feature map. We concatenate all feature maps along the channel dimension, as in , yielding a high-
dimensional \gls{bev} feature map. With two 2D convolutions, we reduce this high-dimensional \gls{bev} feature to our desired channel size of .

After the aggregation, we feed the \gls{bev} feature into a ResNet-18 decoder. The goal of the decoder is to introduce a large receptive field of the ground plane. The distortion introduced by the perspective projections causes pedestrian features to spread out from their actual location on the ground plane. Other approaches \cite{hou2021multiview, song2021stacked, qiu20223d, lee2023multi} identified this distortion as harmful to the detection accuracy and all proposed complex solutions, like deformable transformers \cite{hou2021multiview} or ROI projection \cite{lee2023multi}. Our decoder offers a simple solution to aggregate location and identification features on the ground plane.

In each layer of the ResNet, the \gls{bev} feature is downsampled by . We then use a pyramid network architecture to upsample the output of each layer to the size of the previous larger output. Then, both features are concatenated in the channel dimension, and a 2D convolution is applied. The feature pyramid yields a decoded output with the same shape as the input of  but a much higher receptive field for each grid location. 

\begin{table*}[th]
\centering
\setlength{\tabcolsep}{4pt}
\begin{tabular}{r|rcccccccc}
\toprule
\multicolumn{2}{c}{} & \multicolumn{4}{c}{Wildtrack} & \multicolumn{4}{c}{MultiviewX} \\\cmidrule(lr){3-6}\cmidrule(lr){7-10}
\multicolumn{2}{c}{}& MODA & MODP  & Precision  & Recall & MODA  & MODP  & Precision  & Recall \\\midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{Two-Stage}}&RCNN \& Cluster \cite{xu2016multi} & 11.3  & 18.4  & 68    & 43     & 18.7  & 46.4  & 63.5  & 43.9   \\ 
&DeepMCD \cite{chavdarova2017deep}     & 67.8  & \underline{64.2}  & 85    & \underline{82}     & 70.0  & \underline{73.0}  & 85.7  & \underline{83.3}   \\ 
&Deep-Occlusion \cite{baque2017deep}   & \underline{74.1}  & 53.8  & \underline{95}    & 80     & \underline{75.2}  & 54.7  & \underline{97.8}  & 80.2   \\ 
&MVTT \cite{lee2023multi}              & \textbf{94.1}  & \textbf{81.3}  & \textbf{97.6} & \textbf{96.5}   & \textbf{95.0} & \textbf{92.8} & \textbf{99.4}  & \textbf{95.6} \\
\midrule
\multirow{5}{*}{\rotatebox[origin=c]{90}{One-Stage}}&MVDet \cite{hou2020multiview}          & 88.2  & 75.7  & 94.7  & 93.6   & 83.9  & 79.6  & 96.8  & 86.7   \\
&SHOT \cite{song2021stacked}           & 90.2  & 76.5  & \underline{96.1}  & 94.0   & 88.3  & 82.0  & 96.6  & 91.5 \\
&3DROM \cite{qiu20223d}      & \underline{91.2}  & 76.9  & 95.9  & \underline{95.3}   & 90.0  & 83.7  & 97.5  & 92.4 \\
&MVDeTr  \cite{hou2021multiview}       & \textbf{91.5}  & \textbf{82.1} & \textbf{97.4} & 94.0   & \underline{93.7}  & \textbf{91.3}  & \textbf{99.5}  & \underline{94.2}   \\
&\textbf{\sname}                       & \underline{91.2} & \underline{81.8} & 94.9 & \textbf{96.3} & \textbf{94.2} & \underline{90.1} & \underline{98.6} & \textbf{95.7} \\
\bottomrule
\end{tabular}
\caption{Evaluation of the detection performance with the state-of-the-art methods on the Wildtrack and MultiviewX datasets.   3DROM results are without additional data augmentations.}
\label{tab:detection-sota}
\end{table*}
 
\subsection{Heads \& Losses}
To get the final prediction of the \gls{pom}, we use prediction heads on our \gls{bev} feature map. The detection architecture follows CenterNet \cite{zhou2019objects}, and we add a head for center detection that reduces the feature to  to yield a heatmap or \gls{pom} on the ground plane. We add another head for offset prediction that helps predict the location more accurately as it mitigates the quantization error from the ground grid. The offset has an  component and has the shape  . Each head is implemented by applying a  convolution (with  channels), followed by and activation layer and a  convolution to the final target size. The center head is trained with Focal Loss, and the offset head is trained with L1 Loss.

We also add detection heads for image features that predict the center of the 2D bounding boxes and estimated foot location at the bottom-center of the bounding box, helping the image features to have higher activations at the location of each pedestrian.
Following FairMOT \cite{zhang2021fairmot}, we add an uncertainty term to automatically balance the single-task losses before summing them up.

\nparagraph{Re-Identification}
The \gls{reid} head aims to generate features that can distinguish individual pedestrians. Ideally, affinity among different pedestrians should be smaller than between the same pedestrian. To archive this, we learn \gls{reid} features through a classification task and as a metric learning task. First, we apply a head that yields the \gls{reid} feature on the ground plane  with  and also of the image features . Afterwards, we extract the feature at the location of the center detection in both planes. We create a class identity distribution with a linear layer that we train with Cross Entropy Loss to the ground truth class identity. As discussed earlier, the perspective transformation introduces strong distortion on the ground plane. Thus, we supervise the \gls{reid} features from the image view. In addition to the Cross-Entropy loss, we apply SupCon Loss \cite{khosla2020supervised}, which pulls features belonging to the same class identity together while simultaneously pushing apart features of samples from different classes.

\begin{table}[t]
\setlength{\tabcolsep}{2.75pt}
\centering
\resizebox{\linewidth}{!}{\begin{tabular}{rcccccc}
\toprule
& \multicolumn{5}{c}{Wildtrack}\\\cmidrule(lr){2-6}
& IDF1 & MOTA & MOTP & MT & ML \\
\midrule
\small{KSP-DO~\cite{chavdarova2018wildtrack}}        & 73.2 & 69.6 & 61.5 & 28.7 & 25.1\\ 
\small{KSP-DO-ptrack~\cite{chavdarova2018wildtrack}} & 78.4 & 72.2 & 60.3 & 42.1 & 14.6\\ 
\small{GLMB-YOLOv3~\cite{ong2020bayesian}}           & 74.3 & 69.7 & 73.2 & 79.5 & 21.6\\ 
\small{GLMB-DO     \cite{ong2020bayesian}}           & 72.5 & 70.1 & 63.1 & \textbf{93.6} & 22.8\\ 
\small{DMCT  \cite{you2020real}}                     & 77.8 & 72.8 & 79.1 & 61.0 & \textbf{4.9}\\ 
\small{DMCT Stack  \cite{you2020real}}               & 81.9 & 74.6 & 78.9 & 65.9 & \textbf{4.9}\\
\small{ReST  \cite{cheng2023rest}}         & \underline{86.7} & \underline{84.9} & \underline{84.1} & \underline{87.8} & \textbf{4.9}\\\midrule
\small{\textbf{\sname}}                              & \textbf{92.3} & \textbf{89.5} & \textbf{86.6} & 78.0 & \textbf{4.9}\\
\midrule
& \multicolumn{5}{c}{MultiviewX}\\\cmidrule(lr){2-6}
& IDF1 & MOTA & MOTP & MT   & ML \\
\midrule
\small{\textbf{\sname}} & \textbf{82.4} & \textbf{88.4} & \textbf{86.2} & \textbf{82.9} & \textbf{1.3} \\
\bottomrule
\end{tabular}}
\caption{Evaluation of tracking results on the Wildtrack and MultiviewX.
 ReST originally reported the tracking metrics on view-based tracking instead of tracking in the projected view. The results shown are re-computed by us.}
\label{tab:track-sota}
\end{table}
 
\subsection{Inference}
At inference time, we take the \gls{pom} predicted by the \gls{bev} center head and perform \gls{nms} by a simple  max pooling operation as in \cite{zhou2020tracking}. We then only extract the detections over a certain threshold of . We also extract the identity embeddings at the estimated pedestrian centers. In the next section, we discuss how we associate the detected boxes over time using the re-ID features.

\nparagraph{Online Association}
We adopt the hierarchical online data association approach described by MOTDT \cite{chen2018real}, but instead of boxes, we only track the pedestrians centers seen from the \textit{\glsentrylong{bev}}. Our first step involves initializing a set of tracklets based on the centers detected in the initial timestep. As each subsequent timestep is processed, we connect the centers detected to the existing tracklets using a two-stage matching strategy.

In the first stage, we use a combination of the Kalman Filter \cite{kalman1960new}, and re-ID features to achieve initial tracking results. Specifically, we use the Kalman Filter to anticipate tracklet locations in the next frame and calculate the Mahalanobis distance () between the anticipated and detected center, similar to the DeepSORT method \cite{Wojke2017simple}. We then combine the Mahalanobis distance with the cosine distance computed on \gls{reid} features into a singular distance measure () using the formula , where  is a pre-determined weighting parameter set to  in our experiments. The Mahalanobis distance is manually set to infinity if it exceeds a certain threshold, which aligns with the JDE protocol \cite{wang2020towards} and prevents the tracking of trajectories exhibiting implausible motion. We then use the Hungarian algorithm with a matching threshold  to conclude the first matching stage.

The second stage involves attempting to match undetected boxes and tracklets based on the center distance of their respective boxes, with an increased matching threshold . We continually update the appearance features of the tracklets at each timestep to account for potential variations in appearance. Any unmatched centers are classified as new tracks, and unmatched tracklets are retained for  timesteps to facilitate recognition if they reemerge later.
 \section{Experiments}\label{sec:experiments}

\subsection{Dataset \& Metrics}

\nparagraph{Wildtrack Dataset} Wildtrack~\cite{chavdarova2018wildtrack} is a real-world dataset captured using seven synchronized and calibrated cameras with an overlapping field-of-view of an area of \qty{12}{\metre}  \qty{36}{\metre}. The movement of the pedestrians is in a public environment and unscripted. Annotations are provided on the ground plane quantized into a  grid, resulting in grid cells of .
The average number of pedestrians per frame is 20, and 3.74 cameras cover each location. Each camera image is recorded at a resolution of  pixels with a frame rate of \qty{2}{fps}, covering a total of \qty{35}{\min}.

\nparagraph{MultiviewX Dataset} MultiviewX~\cite{hou2020multiview} is a synthetic dataset generated in a game engine and is built to be a synthetic copy of the Wildtrack dataset.
MultiviewX contains views generated by 6 virtual cameras with overlapping field-of-view. The captured area is with \qty{16}{\metre}  \qty{25}{\metre} slightly smaller than the area of the Wildtrack dataset. For annotation, the ground plane is quantized into a grid of size , where each grid represents the same  squares. The average number of pedestrians per frame is 40, while 4.41 cameras cover each location. The camera resolution (), frame rate (\qty{2}{fps}), and the length (\qty{400}{frames}) are equal to Wildtrack.

\nparagraph{Detection Metrics}
Unlike monocular-view detection systems, which evaluate the predicted bounding boxes, multi-view detection systems assess the projected ground plane occupancy map. Thus, the comparison to the ground truth is not calculated with the \gls{iou} but with the Euclidean distance as proposed in \cite{chavdarova2018wildtrack}. Detection is classified as true positive if it is within a distance , which roughly corresponds to the radius of a human body. Following previous works \cite{chavdarova2018wildtrack, hou2020multiview}, we use \gls{moda} as the primary performance indicator, as it accounts for the normalized missed detections and false positives. Additionally, we report the \gls{modp}, Precision, and Recall.

\nparagraph{Tracking Metrics}
For tracking the metrics are also calculated in the ground plane. We report the common \gls{mot} metrics \cite{bernardin2008evaluating} and identity-aware metrics \cite{ristani2016performance}, the threshold for a positive assignment is set to  to normalize the \gls{motp}. The primary metrics under consideration are \gls{mota} and IDF1.
\gls{mota} takes missed detections, false detections, and identity switches into account. IDF1 measures missed detections, false positives, and identity switches.
Additionally, we also report Mostly Tracked (MT) and Mostly Lost (ML). These are reported as a percentage of the total count of unique pedestrians present in the test set.

\begin{table}[t]
\setlength{\tabcolsep}{2.75pt}
\centering
\begin{tabular}{lcccccc}
\toprule
& \multicolumn{2}{c}{Detection} & \multicolumn{3}{c}{Tracking} \\\cmidrule(lr){2-3}\cmidrule(lr){4-6}
& MODA & MODP & IDF1 & MOTA & MOTP  \\
\midrule
\small{Baseline} &  77.8 & 78.9 & 71.3 & 72.6 & 80.9\\  \midrule
 \small{Augmentation}         & 89.5 & 81.7 & 84.5 & 87.4 & 83.0\\
 \small{Decoder}              & \textbf{91.3} & \textbf{82.2} & \underline{91.1} & \underline{89.1} & \textbf{86.9}\\
 \small{View Center Loss}     & 91.0 & \underline{82.1} & 90.0 & \underline{89.1} & 84.0\\
 \small{View \gls{reid} Loss} &\underline{91.2} & 81.8 & \textbf{92.3} & \textbf{89.5} & \underline{86.6}\\
\bottomrule
\end{tabular}
\caption{Ablation of the components introduced by our approach compared to the baseline method.}
\label{tab:model-ablation}
\end{table}
 \begin{table}[t!]
\setlength{\tabcolsep}{2.75pt}
\centering
\begin{tabular}{rcccccc}
\toprule
& \multicolumn{2}{c}{Detection} & \multicolumn{3}{c}{Tracking} \\\cmidrule(lr){2-3}\cmidrule(lr){4-6}
& MODA & MODP & IDF1 & MOTA & MOTP  \\
\midrule
ResNet-18 & \textbf{91.2} & \underline{81.8} & \underline{92.3} & \textbf{89.5} & \underline{86.6} \\
ResNet-50 & \underline{89.6} & \textbf{82.3} & \textbf{92.6} & \underline{88.8} & 86.5 \\
Swin-T   & 89.5 & 81.3 & 92.0 & 87.3 & \textbf{87.9} \\
\bottomrule
\end{tabular}
\caption{Ablation of different encoders on detection and tracking results of the Wildtrack dataset.}
\label{tab:backbone-ablation}
\end{table}
 
\begin{figure*}[t]
\vskip0.5em
  \centering
  \begin{subfigure}[b]{\textwidth}
    \centering
    \begin{minipage}{0.49\textwidth}
      \includegraphics[width=\textwidth]{fig/pred.png}
    \end{minipage}\hfill
    \begin{minipage}{0.49\textwidth}
      \includegraphics[width=\textwidth]{fig/gt.png}
    \end{minipage}\vspace{0.5em}
    \caption{Comparison of the detection result on Wildtrack as a heat map. Each point in the ground truth represents a pedestrian in the \gls{bev}.}
    \label{fig:quality-det}
  \end{subfigure}\vspace{1em}
  \begin{subfigure}[b]{\textwidth}
    \centering
    \begin{minipage}{0.49\textwidth}
      \includegraphics[width=\textwidth]{fig/plot_mota_pred.pdf}
    \end{minipage}\hfill
    \begin{minipage}{0.49\textwidth}
      \includegraphics[width=\textwidth]{fig/plot_mota_gt.pdf}
    \end{minipage}\vspace{0.5em}
    \caption{Comparisons of the tracking results on Wildtrack. Each colored line represents a path taken by one tracked pedestrian as seen from the \gls{bev}.}
    \label{fig:quality-track}
  \end{subfigure}
  \caption{Qualitative detection (a) and tracking (b) results of our approach (left) compared to the ground truth (right).}
  \label{fig:quality}
\end{figure*}

\subsection{Implementation Details}
The input size of the images is  pixels. For augmentation at train time, we follow \cite{harley2022simple, hou2021multiview}: we apply random resizing and cropping on the RGB input, in a scale range of  and adapt the camera intrinsic  accordingly. Additionally, we add some noise to the translation vector  of the camera extrinsic to avoid overfitting the decoder. 
We train the detector using an Adam optimizer with a one-cycle learning rate scheduler with a maximum learning rate of . We train for  epochs and depending on the size of the encoder, with a batch size of  but accumulate gradients over multiple batches before updating the weights to have a effective batch size of . The encoder and decoder network are initialized with pre-trained weights on \textit{ImageNet-1K}.
All experiments are conducted using one RTX 3090 GPU.


\subsection{Main Results}

\nparagraph{Detection} In the tracking-by-detection paradigm, good detections are the basis for good tracking results. While our method does not focus on improving the detection, we still need to be close to the state-of-the-art to achieve competitive results. \cref{tab:detection-sota} compares our methods detection performance to previous methods. We first compare our results to the baseline: MVDet \cite{hou2020multiview}, as we base our approach on it. The results show that our decoder architecture and augmentation changes improved MVDet. Other detection-focused methods \cite{hou2021multiview, lee2023multi, song2021stacked} also extend MVDet and achieve comparative results on Wildtrack, but our approach has competitive results on MultiviewX. The current state-of-the-art MVTT \cite{lee2023multi} is a two-stage detection approach and could still be added to our single-state approach to further improve the results.

\nparagraph{Tracking} In \cref{tab:track-sota} we compare our method to state-of-the-art approaches. Our approach outperforms all current approaches by a big margin. Compared to the current best-performing method ReST \cite{cheng2023rest}, we improve the IDF1 by , and the MOTA by  percent points. All other methods \cite{chavdarova2018wildtrack, nguyen2022lmgp, ong2020bayesian, you2020real, cheng2023rest} start from 2D detections, and to compare tracking methods, you also need to take the detection quality into account. For most of these approaches, we cannot directly compare the detection quality, but ReST \cite{cheng2023rest} uses detection from MVDeTr \cite{hou2021multiview}, which performs very close to our detector (\cf \cref{tab:detection-sota}). ReST and \sname follow a similar approach: associate spatially on the ground plane, then associate temporally. However, ReST only projects detections in 2D to the ground plane and associates them with a graph solver. In contrast, we project the complete input image feature space to the ground plane and associate it with the decoder. Our results show that with similar detection quality, our approach outperforms ReST, which shows the advantage of our early-fusion approach compared to graph-based late-fusion.


\subsection{Ablations Studies}
\nparagraph{Influence of Method Components}
Next, we ablate each component introduced in our method compared to the baseline as shown in \cref{tab:model-ablation}. The baseline consists of MVDet~\cite{hou2020multiview} with minimal additions to perform tracking, namely a ReID head added to the \gls{bev}-space. The baseline results suffer from strong overfitting and the added augmentation in the next step mostly aleviates this. We augment the input images with scaling and cropping to avoid overfitting in the encoder and add transitive noise to the projection to the ground plane to avoid overfitting in the prediction heads. With better augmentation, we introduced our larger decoder based on a ResNet-18 with a feature pyramid. These additions gave us one of the most robust detection results, but as tracking is our primary focus, we added the view center and \gls{reid} loss. These two losses are applied to the image features and should help guide these features. While the 2D center detection alone decreased our detection performance, using it with the \gls{reid} loss gave us the final SOTA results.

\nparagraph{Influence of the Encoder Network} The encoder extracts the features of the RGB image that are projected to the ground plane. While all of the similar detection-based approaches \cite{hou2020multiview, hou2021multiview, lee2023multi, qiu20223d} use a ResNet-18 encoder, these approaches do not need to encode the identity feature. Thus, we try our approach with larger encoders and transformer-based encoders, see \cref{tab:backbone-ablation}. The ablation shows that ResNet-18 has the best performance. ResNet-50 may be slightly better in the detection and tracking performance, but the smaller ResNet-18 outperforms it in the main metrics MODA and MOTA and has competitive scores for IDF1. We thus use ResNet-18 to report all other results.




\subsection{Qualitative Results}
In \cref{fig:quality}, we plot the output of our model on the test set of Wildtrack. In \cref{fig:quality-det}, we compare the prediction of the \gls{pom} to the ground truth as a heatmap at a single timestep. Each point in the ground truth represents a pedestrian in the \gls{bev}. The scene's center is crowded with pedestrians, and the prediction in this high-overlap area is almost perfect. The further the pedestrians are on a side, the less accurate the prediction is. This inaccuracy could be due to the increased distortion further from the cameras and less overlap of the views in the border regions. 

\cref{fig:quality-track} shows similar results for tracking. We show all tracks on the ground plane of the full test set of Wildtrack, where each color and line represents the path one identity takes. The tracks in the center of the scene are predicted almost perfectly. However, the top-left and top-right tracks are segmented or switched tracks. This inaccuracy could be due to less accurate or missing detections and identity features outside the ground plane.

 \section{Discussion}

\nparagraph{Limitations} The first limitation of our approach is the requirement for high-quality 3D annotations and camera calibrations. While this is easy to archive with synthetic data, it is costly for real-world data. Therefore, we could not evaluate some older datasets (CAMPUS~\cite{xu2016multi}, PETS09~\cite{ferryman2009pets2009}), where most of the late-fusion models can work with only 2D annotations and ground plane homography.
Furthermore, our approach requires synchronized cameras. Since we lift all cameras to the same 3D space, the time differences should be minimal so that moving objects do not project to different locations in 3D. Late-fusion \gls{mtmc} tracking methods can account for more drift in the temporal domain. Our approach also has higher hardware requirements. While late-fusion approaches may process each camera decentralized and fuse the information centrally, our approach processes all camera images simultaneously. It thus requires more memory and computational resources on a single machine.

\nparagraph{Ethical Impact}Tracking methods always have the risk of being used for illegal surveillance. Methods that focus on pedestrian tracking must face this criticism, especially. The dataset Wildtrack~\cite{chavdarova2018wildtrack} has been criticized~\cite{Exposing.ai} for the missing consent of the recorded persons. Unfortunately, a good comparison to the state-of-the-art is only possible with this dataset, even though a synthetic replication with MultiviewX~\cite{hou2020multiview} is now available. We are the first to evaluate tracking on this synthetic dataset to lower the ethical implications of tracking.

\nparagraph{Future Work}
For the detection part, the biggest challenge of our and other current approaches \cite{hou2021multiview, lee2023multi, song2021stacked} is the distortion caused by the projective transformation. Other methods that lift from 2D to 3D space could be explored for multi-view detection, i.e., Simple-BEV \cite{harley2022simple}, Lift-Splat-Shot \cite{philion2020lift}, or BEVFormer \cite{li2022bevformer}.
Most current approaches only use the current frame for detection. Using more temporal frames could improve the detection performance \cite{li2022bevformer}. Using more temporal context could also improve the tracking quality, and approaches like CenterTrack~\cite{zhou2020tracking} could be used to track via motion cues.
The pedestrian datasets used in this work have about  timestamps, which is relatively small by modern computer vision standards, and the detection and tracking accuracy is saturating. The need for larger datasets is apparent, and datasets with similar \gls{mtmc} problems for traffic surveillance \cite{tang2019cityflow, synthehicle2023herzog} could bridge this gap. 

\section{Conclusion}\label{sec:conclusion}
This paper shows that the \sname catches the worm through multi-view highly accurate tracking. Early-fusion of all views and tracking in the bird's eye view considerably improves \gls{mtmc} tracking. We adapt one-shot tracking to multi-view tracking to propose an online, anchor-free tracker. We propose ways to efficiently train \gls{reid} features in \gls{bev} and ablate each of our tracking improvements.

We expect \sname to inspire feature work in early-fusion multi-view tracking and believe that \sname, together with our suggested future work, makes significant progress towards tackling multi-view tracking problems.
 

{\small
\bibliographystyle{ieee_fullname}
\bibliography{main}
}

\end{document}
