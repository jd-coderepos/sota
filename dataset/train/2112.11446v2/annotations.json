[{'LEADERBOARD': {'Task': 'Implicit Relations', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '36.4'}}, {'LEADERBOARD': {'Task': 'Intent Recognition', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '88.7'}}, {'LEADERBOARD': {'Task': 'LAMBADA', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '74.5'}}, {'LEADERBOARD': {'Task': 'Question Selection', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '41.4'}}, {'LEADERBOARD': {'Task': 'Implicatures', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '62.0'}}, {'LEADERBOARD': {'Task': 'English Proverbs', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '57.6'}}, {'LEADERBOARD': {'Task': 'Fantasy Reasoning', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '64.1'}}, {'LEADERBOARD': {'Task': 'Figure Of Speech Detection', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '52.7'}}, {'LEADERBOARD': {'Task': 'GRE Reading Comprehension', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '27.3'}}, {'LEADERBOARD': {'Task': 'Movie Dialog Same Or Different', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '50.7'}}, {'LEADERBOARD': {'Task': 'Nonsense Words Grammar', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '61.4'}}, {'LEADERBOARD': {'Task': 'Phrase Relatedness', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '81.8'}}, {'LEADERBOARD': {'Task': 'RACE-h', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '71.6'}}, {'LEADERBOARD': {'Task': 'RACE-m', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '75.1'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Humanities', 'Score': '65.8'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Average (%)', 'Score': '60.0'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Parameters (Billions)', 'Score': '280'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'STEM', 'Score': '48.0'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Social Sciences', 'Score': '71.2'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Other', 'Score': '64.0'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Tokens (Billions)', 'Score': '300'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Humanities', 'Score': '28.0'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Average (%)', 'Score': '29.5'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Parameters (Billions)', 'Score': '7.1'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'STEM', 'Score': '30.1'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Social Sciences', 'Score': '31.0'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Other', 'Score': '31.0'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Humanities', 'Score': '27.5'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Average (%)', 'Score': '27.3'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Parameters (Billions)', 'Score': '1.4'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'STEM', 'Score': '26.6'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Social Sciences', 'Score': '30.0'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Other', 'Score': '24.7'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Humanities', 'Score': '26.6'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Average (%)', 'Score': '25.7'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Parameters (Billions)', 'Score': '0.4'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'STEM', 'Score': '26.0'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Social Sciences', 'Score': '23.4'}}, {'LEADERBOARD': {'Task': 'Multi-task Language Understanding', 'Dataset': 'MMLU', 'Metric': 'Other', 'Score': '24.1'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'PIQA', 'Metric': 'Accuracy', 'Score': '81.8'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'BoolQ', 'Metric': 'Accuracy', 'Score': '79.3'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'TruthfulQA', 'Metric': 'MC1', 'Score': '0.295'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'TruthfulQA', 'Metric': 'MC1', 'Score': '0.25'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'TruthfulQA', 'Metric': 'MC1', 'Score': '0.23'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'TruthfulQA', 'Metric': 'MC1', 'Score': '0.217'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'TruthfulQA', 'Metric': 'MC1', 'Score': '0. 27'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'SIQA', 'Metric': 'Accuracy', 'Score': '50.6'}}, {'LEADERBOARD': {'Task': 'Question Answering', 'Dataset': 'Natural Questions', 'Metric': 'EM', 'Score': '28.2'}}, {'LEADERBOARD': {'Task': 'Multiple Choice Question Answering (MCQA)', 'Dataset': 'BIG-bench (Novel Concepts)', 'Metric': 'Accuracy', 'Score': '59.1'}}, {'LEADERBOARD': {'Task': 'Multiple Choice Question Answering (MCQA)', 'Dataset': 'BIG-bench (Movie Recommendation)', 'Metric': 'Accuracy', 'Score': '50.5'}}, {'LEADERBOARD': {'Task': 'Multiple Choice Question Answering (MCQA)', 'Dataset': 'BIG-bench (Hyperbaton)', 'Metric': 'Accuracy', 'Score': '51.7'}}, {'LEADERBOARD': {'Task': 'Multiple Choice Question Answering (MCQA)', 'Dataset': 'BIG-bench (Ruin Names)', 'Metric': 'Accuracy', 'Score': '38.6'}}, {'LEADERBOARD': {'Task': 'Multiple Choice Question Answering (MCQA)', 'Dataset': 'BIG-bench (Navigate)', 'Metric': 'Accuracy', 'Score': '51.1'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'BIG-bench (Known Unknowns)', 'Metric': 'Accuracy', 'Score': '63.6'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'BIG-bench (Winowhy)', 'Metric': 'Accuracy', 'Score': '56.7'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'BIG-bench (Date Understanding)', 'Metric': 'Accuracy', 'Score': '44.1'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'BIG-bench (Causal Judgment)', 'Metric': 'Accuracy', 'Score': '50.8'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'WinoGrande', 'Metric': 'Accuracy', 'Score': '70.1'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'BIG-bench (Logical Sequence)', 'Metric': 'Accuracy', 'Score': '36.4'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'BIG-bench (Sports Understanding)', 'Metric': 'Accuracy', 'Score': '54.9'}}, {'LEADERBOARD': {'Task': 'Common Sense Reasoning', 'Dataset': 'BIG-bench (Disambiguation QA)', 'Metric': 'Accuracy', 'Score': '45.5'}}, {'LEADERBOARD': {'Task': 'Riddle Sense', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '68.2'}}, {'LEADERBOARD': {'Task': 'Crass AI', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '56.8'}}, {'LEADERBOARD': {'Task': 'Discourse Marker Prediction', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '11.7'}}, {'LEADERBOARD': {'Task': 'Timedial', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '50.9'}}, {'LEADERBOARD': {'Task': 'Crash Blossom', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '63.6'}}, {'LEADERBOARD': {'Task': 'Empirical Judgments', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '52.5'}}, {'LEADERBOARD': {'Task': 'Irony Identification', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '69.7'}}, {'LEADERBOARD': {'Task': 'Understanding Fables', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '39.6'}}, {'LEADERBOARD': {'Task': 'Word Sense Disambiguation', 'Dataset': 'BIG-bench (Anachronisms)', 'Metric': 'Accuracy', 'Score': '56.4'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'StackExchange', 'Metric': 'BPB', 'Score': '0.641'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'Gutenberg PG-19', 'Metric': 'BPB', 'Score': '0.656'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'OpenWebtext2', 'Metric': 'BPB', 'Score': '0.677'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'PubMed Cognitive Control Abstracts', 'Metric': 'BPB', 'Score': '0.577'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'arXiv', 'Metric': 'BPB', 'Score': '0.662'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'Books3', 'Metric': 'BPB', 'Score': '0.712'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'Ubuntu IRC', 'Metric': 'BPB', 'Score': '1.09'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'USPTO Backgrounds', 'Metric': 'BPB', 'Score': '0.546'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'Bookcorpus2', 'Metric': 'BPB', 'Score': '0.741'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'OpenSubtitles', 'Metric': 'BPB', 'Score': '0.899'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'PubMed Central', 'Metric': 'BPB', 'Score': '0.525'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'GitHub', 'Metric': 'BPB', 'Score': '0.377'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'Pile CC', 'Metric': 'BPB', 'Score': '0.691'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'PhilPapers', 'Metric': 'BPB', 'Score': '0.695'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'DM Mathematics', 'Metric': 'BPB', 'Score': '1.14'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'NIH ExPorter', 'Metric': 'BPB', 'Score': '0.590'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'FreeLaw', 'Metric': 'BPB', 'Score': '0.513'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'HackerNews', 'Metric': 'BPB', 'Score': '0.890'}}, {'LEADERBOARD': {'Task': 'Language Modelling', 'Dataset': 'Curation Corpus', 'Metric': 'BPB', 'Score': '0.475'}}, {'LEADERBOARD': {'Task': 'Sarcasm Detection', 'Dataset': 'BIG-bench (SNARKS)', 'Metric': 'Accuracy', 'Score': '48.3'}}, {'LEADERBOARD': {'Task': 'Formal Logic', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '35.7'}}, {'LEADERBOARD': {'Task': 'Abstract Algebra', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '25.0'}}, {'LEADERBOARD': {'Task': 'Mathematical Induction', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '57.6'}}, {'LEADERBOARD': {'Task': 'High School Mathematics', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '23.7'}}, {'LEADERBOARD': {'Task': 'Professional Accounting', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '44.3'}}, {'LEADERBOARD': {'Task': 'Analogical Similarity', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '17.2'}}, {'LEADERBOARD': {'Task': 'Identify Odd Metapor', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '38.6'}}, {'LEADERBOARD': {'Task': 'Odd One Out', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '32.5'}}, {'LEADERBOARD': {'Task': 'Sentence Completion', 'Dataset': 'HellaSwag', 'Metric': 'Accuracy', 'Score': '79.2'}}, {'LEADERBOARD': {'Task': 'Dark Humor Detection', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '83.1'}}, {'LEADERBOARD': {'Task': 'Moral Scenarios', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '40.2'}}, {'LEADERBOARD': {'Task': 'Moral Permissibility', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '55.1'}}, {'LEADERBOARD': {'Task': 'Business Ethics', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '70.0'}}, {'LEADERBOARD': {'Task': 'Moral Disputes', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '66.8'}}, {'LEADERBOARD': {'Task': 'Misconceptions', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '61.7'}}, {'LEADERBOARD': {'Task': 'Sentence Ambiguity', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '69.1'}}, {'LEADERBOARD': {'Task': 'FEVER (2-way)', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '77.5'}}, {'LEADERBOARD': {'Task': 'FEVER (3-way)', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '77.5'}}, {'LEADERBOARD': {'Task': 'General Knowledge', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '93.9'}}, {'LEADERBOARD': {'Task': 'Natural Questions', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '28.2'}}, {'LEADERBOARD': {'Task': 'TriviaQA', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '57.1'}}, {'LEADERBOARD': {'Task': 'Miscellaneous', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '75.7'}}, {'LEADERBOARD': {'Task': 'Similarities Abstraction', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '81.8'}}, {'LEADERBOARD': {'Task': 'Global Facts', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '38.0'}}, {'LEADERBOARD': {'Task': 'High School European History', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '72.1'}}, {'LEADERBOARD': {'Task': 'High School US History', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '78.9'}}, {'LEADERBOARD': {'Task': 'High School World History', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '75.1'}}, {'LEADERBOARD': {'Task': 'International Law', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '77.7'}}, {'LEADERBOARD': {'Task': 'Jurisprudence', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '71.3'}}, {'LEADERBOARD': {'Task': 'Logical Fallacies', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '72.4'}}, {'LEADERBOARD': {'Task': 'Management', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '77.7'}}, {'LEADERBOARD': {'Task': 'Marketing', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '83.3'}}, {'LEADERBOARD': {'Task': 'Philosophy', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '68.8'}}, {'LEADERBOARD': {'Task': 'Prehistory', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '67.6'}}, {'LEADERBOARD': {'Task': 'Professional Law', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '44.5'}}, {'LEADERBOARD': {'Task': 'World Religions', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '84.2'}}, {'LEADERBOARD': {'Task': 'Logical Reasoning', 'Dataset': 'BIG-bench (StrategyQA)', 'Metric': 'Accuracy', 'Score': '61.0'}}, {'LEADERBOARD': {'Task': 'Logical Reasoning', 'Dataset': 'BIG-bench (Formal Fallacies Syllogisms Negation)', 'Metric': 'Accuracy', 'Score': '50.7'}}, {'LEADERBOARD': {'Task': 'Logical Reasoning', 'Dataset': 'BIG-bench (Logical Fallacy Detection)', 'Metric': 'Accuracy', 'Score': '58.9'}}, {'LEADERBOARD': {'Task': 'Logical Reasoning', 'Dataset': 'BIG-bench (Temporal Sequences)', 'Metric': 'Accuracy', 'Score': '19.0'}}, {'LEADERBOARD': {'Task': 'Logical Reasoning', 'Dataset': 'BIG-bench (Reasoning About Colored Objects)', 'Metric': 'Accuracy', 'Score': '49.2'}}, {'LEADERBOARD': {'Task': 'Logical Reasoning', 'Dataset': 'BIG-bench (Logic Grid Puzzle)', 'Metric': 'Accuracy', 'Score': '35.1'}}, {'LEADERBOARD': {'Task': 'Logical Reasoning', 'Dataset': 'BIG-bench (Penguins In A Table)', 'Metric': 'Accuracy', 'Score': '40.6'}}, {'LEADERBOARD': {'Task': 'Physical Intuition', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '59.7'}}, {'LEADERBOARD': {'Task': 'Elementary Mathematics', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '33.6'}}, {'LEADERBOARD': {'Task': 'Epistemic Reasoning', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '56.4'}}, {'LEADERBOARD': {'Task': 'Analytic Entailment', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '53.0'}}, {'LEADERBOARD': {'Task': 'Entailed Polarity', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '89.5'}}, {'LEADERBOARD': {'Task': 'Evaluating Information Essentiality', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '16.7'}}, {'LEADERBOARD': {'Task': 'Logical Args', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '59.1'}}, {'LEADERBOARD': {'Task': 'Metaphor Boolean', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '59.3'}}, {'LEADERBOARD': {'Task': 'Presuppositions As NLI', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '34.0'}}, {'LEADERBOARD': {'Task': 'College Mathematics', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '37.0'}}, {'LEADERBOARD': {'Task': 'Anatomy', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '56.3'}}, {'LEADERBOARD': {'Task': 'Clinical Knowledge', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '67.2'}}, {'LEADERBOARD': {'Task': 'College Medicine', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '60.1'}}, {'LEADERBOARD': {'Task': 'Human Aging', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '66.4'}}, {'LEADERBOARD': {'Task': 'Human Organs Senses Multiple Choice', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '84.8'}}, {'LEADERBOARD': {'Task': 'Medical Genetics', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '69.0'}}, {'LEADERBOARD': {'Task': 'Nutrition', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '69.9'}}, {'LEADERBOARD': {'Task': 'Professional Medicine', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '64.0'}}, {'LEADERBOARD': {'Task': 'Virology', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '47.0'}}, {'LEADERBOARD': {'Task': 'Econometrics', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '43'}}, {'LEADERBOARD': {'Task': 'High School Geography', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '76.8'}}, {'LEADERBOARD': {'Task': 'High School Government and Politics', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '83.9'}}, {'LEADERBOARD': {'Task': 'High School Macroeconomics', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '65.1'}}, {'LEADERBOARD': {'Task': 'High School Microeconomics', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '66.4'}}, {'LEADERBOARD': {'Task': 'High School Psychology', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '81.8'}}, {'LEADERBOARD': {'Task': 'Human Sexuality', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '67.2'}}, {'LEADERBOARD': {'Task': 'Professional Psychology', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '68.1'}}, {'LEADERBOARD': {'Task': 'Public Relations', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '71.8'}}, {'LEADERBOARD': {'Task': 'Security Studies', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '64.9'}}, {'LEADERBOARD': {'Task': 'Sociology', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '84.1'}}, {'LEADERBOARD': {'Task': 'US Foreign Policy', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '81.0'}}, {'LEADERBOARD': {'Task': 'Memorization', 'Dataset': 'BIG-bench (Hindu Knowledge)', 'Metric': 'Accuracy', 'Score': '80'}}, {'LEADERBOARD': {'Task': 'Computer Security', 'Dataset': 'BIG-bench', 'Metric': 'Accuracy', 'Score': '65.0'}}]
