\pdfoutput=1


\documentclass[11pt]{article}

\usepackage{EMNLP2023}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}


\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}

\usepackage{multirow}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{graphicx}

\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{array}
\usepackage{hyperref}
\usepackage{url}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{makecell}
\usepackage{threeparttable}
\usepackage{CJKutf8}
\usepackage{lscape}
\usepackage{fancyhdr}
\usepackage[ruled,vlined]{algorithm2e}
\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage{threeparttable}


\usepackage{caption}
\usepackage{graphicx}
\usepackage{float} 
\usepackage{subfigure}
\usepackage{soul}

\usepackage{multicol}
\usepackage{adjustbox}
\usepackage{booktabs}


\hyphenpenalty=500
\tolerance=2000



\title{InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction}

\author{
    {\normalsize
     \textbf{Xiao Wang}$^{\bigstar*}$, 
     \ \ Weikang Zhou$^{\bigstar}$\thanks{$^*$  Equal contribution.} ,
     \ \ Can Zu$^{\bigstar}$, 
     \ \ Han Xia$^{\bigstar}$, 
     \ \ Tianze Chen$^{\bigstar}$,}\\
    {\normalsize
     \textbf{Yuansen Zhang}$^{\bigstar}$, 
    \ \ \textbf{Rui Zheng}$^{\bigstar}$, 
    \ \ \textbf{Junjie Ye}$^{\bigstar}$, 
    \ \ \textbf{Qi Zhang}$^{\bigstar}$$^{\dagger}$,
    \ \ \textbf{Tao Gui}$^{\blacklozenge}$
    \thanks{{} {} Corresponding Author} 
    \textbf{,}
    }\\
    {\normalsize
    \textbf{Jihua Kang}$^{\clubsuit}$\textbf{,} 
    \ \ \textbf{Jingsheng Yang}$^{\clubsuit}$\textbf{,} 
    \ \ \textbf{Siyuan Li}$^{\clubsuit}$\textbf{,} 
    \ \ \textbf{Chunsai Du}$^{\clubsuit}$\textbf{,}
    }\\
  {$^\bigstar$ \normalsize School of Computer Science, Fudan University, Shanghai, China} \\
  {$^\blacklozenge$ \normalsize Institute of Modern Languages and Linguistics, Fudan University, Shanghai, China} \\
  {$^\clubsuit$ \normalsize ByteDance Inc.} \\
  \texttt{\normalsize \{xiao\_wang20,qz,tgui\}@fudan.edu.cn}
}

\begin{document}
\maketitle
\begin{abstract}


Large language models have unlocked strong multi-task capabilities from reading instructive prompts.
However, recent studies have shown that existing large models still have difficulty with information extraction tasks. 
For example, gpt-3.5-turbo achieved an F1 score of 18.22 on the Ontonotes dataset, which is significantly lower than the state-of-the-art performance.
In this paper, we propose InstructUIE, a unified information extraction framework based on instruction tuning, which can uniformly model various information extraction tasks and capture the inter-task dependency.
To validate the proposed method, we introduce IE INSTRUCTIONS, a benchmark of 32 diverse information extraction datasets in a unified text-to-text format with expert-written instructions.
Experimental results demonstrate that our method achieves comparable performance to Bert in supervised settings and significantly outperforms the state-of-the-art and gpt3.5 in zero-shot settings.

\end{abstract}

\section{Introduction}

Large language models (LLMs) \cite{Brown2020LanguageMA,InstructGPT,GPT4} show tremendous promise in generalization within the set of observed tasks through multi-task training and unified encoding \cite{mishra-etal-2022-cross,wang-etal-2022-super,Longpre2023TheFC}. 
Recent research has revealed a significant performance gap in LLMs when it comes to information extraction (IE) tasks \cite{ye2023comprehensive,chen2023robust}. 
For instance, gpt-3.5-turbo achieves an 18.22 F1 score on the Ontonotes dataset, which is far from satisfactory. 
Therefore, it is necessary to explore how to build a unified information extraction (UIE) model with LLMs.

\begin{figure}[t]
\small
\centering
  \includegraphics[width=3.0in]{images/comparison.pdf}
  \caption{Illustration of 3 different paradigms for solving unified information extraction task.}
 \label{comparison}
\end{figure}


Recently, \citet{UIE} proposed UIE, which uniformly encodes different extraction structures via a structured extraction language, and captures the common IE abilities via a large-scale pretrained text-to-structure model (shown in Figure \ref{comparison}a). However, UIE requires separate finetune for different downstream tasks. This lead to the poor performance of UIE in low resource settings or facing new label schema, which greatly restricts the application of UIE in real scenarios. 
\citet{USM} proposed USM, which decouple IE into two basic tasks, token-token linking to extract label-agnostic substructures, and label-token linking to attach substructures to corresponding semantic concepts (shown in Figure \ref{comparison}b). However, USM presents two major limitations. Firstly, it converts IE into a semantic matching task, which makes it difficult to integrate with generative language model. Secondly, the method requires semantic matching for each word, which leads to a significant increase in training and inference time.


\begin{figure*}[t]
\centering
  \includegraphics[width=6.5in]{images/framework.pdf}
  \caption{The overview framework of InstructUIE. The input consists of task instructions, options, and text. The output is a more understandable sentence converted from the original label structures.}
 \label{fig:framework}
\end{figure*}


In this work, we introduce a unified information extraction framework based on multi-task instruction tuning, named InstructUIE (shown in Figure \ref{comparison}c). Specifically, we reformulate IE tasks as a natural language generation problem. 
For the source sentence, we design descriptive instructions to enable the model to understand different tasks and employ an option mechanism including all candidate categories as constraints of output space. 
Then, a pre-trained language model is required to generate the target structure and the corresponding type in the form of natural language. 
We believe that unrestricted decoding would stimulate the latent knowledge of LLMs to complete IE tasks to a larger extent. 
We further propose auxiliary tasks, which enable the model to capture common structure information and deepen the understanding of diverse semantics. 
Specifically, we introduce entity span extraction task and entity typing task for named entity recognition (NER) task, entity pair extraction task and entity pair relationship identification task for relation extraction (RE) task, and trigger extraction task and argument extraction task for event extraction (EE) task. 


To evaluate the effectiveness of the proposed model, we have developed a new benchmark called IE INSTRUCTIONS. The benchmark consists of 32 diverse information extraction datasets that have been unified into a text-to-text format, allowing for a consistent and standardized evaluation of various IE tasks \footnote{The dataset, code, and models can be found at 
https://github.com/BeyonderXX/InstructUIE}. 
Based on the benchmark, we conduct experiments on three main IE tasks under the supervised and zero-shot settings. 



Our main contributions are summarized as follows:

\begin{itemize}[leftmargin=*, align=left]
    \item We propose an end-to-end framework for universal information extraction – InstructUIE, which leverages natural language instructions to guide large language models for IE tasks.
    \item We introduce IE INSTRUCTIONS, a benchmark of 32 diverse information extraction datasets in a unified text-to-text format with expert-written instructions. 
    \item Experimental results demonstrate that InstructUIE achieves comparable performance to Bert in a supervised setup. Notably, our method significantly outperforms the current state-of-the-art and GPT-3.5 in a zero-shot setup.
\end{itemize}




\section{Methodology}
In this section, we first briefly introduce the setup of instruction tuning. 
Then, we discuss the task meta-information schema and how IE tasks are mapped into our schema. 
Next, we discuss the framework of InstructUIE, which consists of two major parts: task schema and auxiliary tasks.
Finally, we explain how IE INSTRUCTION is constructed.


\subsection{Instruction Tuning Background}
Instruction tuning is a multi-task learning framework that enables the use of human-readable instructions to guide the output of LLMs. Given a source text and task-specific instructions, the model is trained to generate a sequence of tokens representing the desired output structure and its corresponding labels. 

In a supervised setup, the instructions are provided during training for all tasks, and the model is fine-tuned on a set of labeled data for each task. This allows the model to learn task-specific features and optimize for each task. In a zero-shot setup, the instructions are only provided for a subset of tasks during training, and the model is evaluated on unseen tasks without additional fine-tuning. This requires the model to generalize across tasks and use the shared features learned from the instruction tuning framework to infer the output structures for new tasks.

\subsection{Framework}
In this section, we discuss the task meta-information schema and how IE tasks are mapped into our schema. 
Next, we propose auxiliary tasks, which enable the model to capture common structure information and deepen the understanding of diverse semantics.

\subsubsection{Task Schema}\label{task schema}
To better transfer and utilize the knowledge learned in pre-trained language models, we reformulate the IE tasks to the seq2seq form and solve it through fine-tuning LLMs, as shown in Figure \ref{fig:framework}. Every task instance is formatted with four properties: task instruction, options, text, and output.


\textbf{Task Instruction} provides a detailed guide on how to extract the relevant information from the input text and produce the desired output structure. It includes information such as the type of information to be extracted, the format of the output structure, and any additional constraints or rules that need to be followed during the extraction process. The task instruction acts as a bridge between the raw input text and the structured output representation, enabling the model to understand the extraction task and generate accurate and meaningful output. In Table \ref{prompts_details} in the Appendix we present the list of instructions for each task.

\textbf{Options} are the output label constraints for a task, which represent the set of possible outputs that can be generated by the model for a given input. These label constraints are specific to each task and provide information on how to map the predicted outputs to the corresponding semantic concepts. For instance, in NER, options could be entity tags such as person, organization, location, or miscellaneous. Similarly, in RE, options could represent the types of relations that can be extracted, such as "works for", "born in", "married to", and so on. In EE, options could represent the event tags that correspond to different types of events, such as "beginning", "end", "occurring", "ceasing", and so on. The options provide a structured output space for the model, allowing it to generate outputs that are consistent with the underlying semantic structure of the task.

\textbf{Text} is the input sentence of a task instance. This sequence is then fed into the pre-trained language model along with the task instruction and options, enabling the model to generate the desired output sequence for the given task.

\textbf{Output} is the sentence converted from the original tags of the sample. Specifically, for NER, the output format is "\textit{entity tag: entity span}". For RE, the output format is "\textit{relationship: head entity, tail entity}". For EE, the output format is "\textit{event tag: trigger word, argument tag: argument span}". In cases where the input does not contain structural information that matches any of the provided options, we assign a value of "\textit{None}" to the corresponding output sentence.


\subsubsection{Auxiliary Tasks}
To boost the performance in a more fine-grained level, we further design auxiliary tasks to be trained in conjunction with the main task. The auxiliary tasks provide additional information that complements the main task, enabling the model to capture common structures better and deepen the understanding of diverse semantics.

For the named entity recognition task, we introduce a span extraction task and an entity typing task. The span extraction task is designed to extract the entity span from the input sentence, while the entity typing task is aimed at identifying the type of entity.

For the relation extraction task, we have introduced an entity pair extraction task and a relation classification task. The entity pair extraction task aims to extract the entity pairs involved in the relationship, while the relation classification task is designed to classify the type of relationship between the entity pairs.

For the event extraction task, we have introduced a trigger extraction task and an argument extraction task. The trigger extraction task is designed to extract the trigger word that triggers the event, while the argument extraction task aims to extract the associated arguments.


\subsection{IE INSTRUCTIONS}
IE INSTRUCTIONS collects 32 publicly available datasets covering three types of IE tasks: NER, RE, and EE. To ensure the diversity of the datasets, we include corpora from various domains, such as science, healthcare, social media, and transportation, in addition to general-domain sources, such as news and Wikidata. Figure \ref{benchmark_sun} shows the breakdown of the benchmark by task, domain, and size. For detailed dataset statistics and train/test split methods, please refer to Appendix Table \ref{dataset-details}.

We carry out the following data processing steps: (1) To address the issue of inconsistent label schemas across different tasks, we unify the names of labels with identical semantics but different names in various datasets. (2) To better test the semantic understanding capabilities of the LLM, we convert labels with underscores, abbreviations, or special formats into natural language formats. For example, we renamed the label "people person place\_of\_birth" to "place of birth." (3) Following the guidelines outlined in the section \ref{task schema}, we transform all datasets into a text-to-text format, which ensures a consistent representation of the input-output pairs across all tasks.

Our benchmark provides a standardized evaluation platform for LLMs' performance on IE tasks. This will facilitate a more accurate comparison of various models and contribute to the development of more effective and robust models for IE tasks.

\begin{figure}[t]
\small
\centering
  \includegraphics[width=3.0in]{images/sun.png}
  \caption{Overview of IE INSTRUCTIONS.}
 \label{benchmark_sun}
\end{figure}


\section{Experiments}
This section conducted extensive experiments under supervised and zero-shot settings to validate the effectiveness of InstructUIE. 
We select 11B FlanT5 \cite{chung2022scaling} as our backbone model because prior research \cite{Longpre2023TheFC} has demonstrated that models fine-tuned on instruction-based tasks offer a computationally efficient starting point for new tasks.
The details of the experimental setup, datasets, and comparison methods are described in the following parts.




\begin{table}[t]
    \centering
    \adjustbox{max width=\columnwidth}{
    \begin{tabular}{c|ccc|c}
    \toprule  
    Dataset & UIE & USM & Bert-base & Ours \\
    \midrule
    ACE2005 & 85.78 & 87.14 & \textbf{87.30} & 86.66 \\
    AnatEM & - & - & 85.82 & \textbf{90.89} \\
    bc2gm & - & - & 80.90 & \textbf{85.16} \\
    bc4chemd & - & - & 86.72 & \textbf{90.30} \\
    bc5cdr & - & - & 85.28 & \textbf{89.59} \\
    broad twitter & - & - & 58.61 & \textbf{83.14} \\
    CoNLL2003 & 92.99 & \textbf{93.16} & 92.40 & 92.94 \\
    FabNER & - & - & 64.20 & \textbf{76.20} \\
    FindVehicle & - & - & 87.13 & \textbf{89.47} \\
    GENIA-Ent & - & - & 73.3 & \textbf{74.71} \\
    HarveyNER & - & - & 82.26 & \textbf{88.79} \\
    MIT Movie & - & -& 88.78 & \textbf{89.01} \\
    MIT Restaurant & - & - & 81.02 & \textbf{82.55} \\
    multiNERD & - & - & 91.25 & \textbf{92.32} \\
    ncbi-disease & - & - & 80.20 & \textbf{90.23} \\
    Ontonotes & - & - & \textbf{91.11} & 90.19 \\
    polyglot-NER & - & - & \textbf{75.65} & 70.15 \\
    tweetNER7 & - & - & 56.49 & \textbf{64.97} \\
    wikiann & - & - & 70.60 & \textbf{85.13} \\
    wikineural & - & - & 82.78 & \textbf{91.36} \\
    Avg & - & - & 80.09 & \textbf{85.19} \\
    \bottomrule
    \end{tabular}
    }
    \caption{
Overall results of InstructUIE on NER task. The evaluation metric is Entity F1. For 20 NER datasets, InstructUIE outperforms the Bert model on 17 of them.
}
\label{supervised-result-NER}
\end{table}

\subsection{Experiments on Supervised Settings}
\subsubsection{Dataset}
We conduct supervised experiments on IE INSTRUCTIONS, including three tasks (named entity extraction, relation extraction, and event extraction). 
Details of the dataset splitting methods and statistics can be found in Appendix \ref{data details}.

To balance the dataset, we apply a sampling strategy \cite{poolsawad2014balancing}. Specifically, we sample 10,000 examples for each dataset and include all examples for datasets with fewer than 10,000 samples.

\subsubsection{Baselines}
\label{supervise-baseline}
We compare the proposed InstructUIE with the following strong baseline models:
\begin{itemize}
\item \textbf{UIE} \cite{UIE} is a unified text-to-structure generation framework that can universally model different IE tasks and adaptively generate targeted structures;
\item \textbf{USM} \cite{USM} is a unified IE tasks framework, which converts IE tasks to a semantic matching problem;
\item \textbf{Bert} \cite{devlin-etal-2019-bert}, which are widely used as text encoders for various tasks.
\end{itemize}

\subsubsection{Evaluation Metrics}
We use span-based offset Micro-F1 as the primary metric to evaluate the model. For NER task, we follow a span-level evaluation setting, where the entity boundary and entity type must be correctly predicted. For RE task, a relation triple is correct if the model correctly predicts the boundaries of the subject entity, the object entity, and the entity relation. For EE task, we report two evaluation metrics: (1) Event Trigger: an event trigger is correct if the event type and the trigger word are correctly predicted. (2) Event Argument: an event argument is correct if its role type and event type match a reference argument mention.

\subsubsection{Results}


\begin{table}[t]
    \centering
    \adjustbox{max width=\columnwidth}{
    \begin{tabular}{c|ccc}
    \toprule  
    Dataset & UIE & USM & Ours \\
    \midrule
    ADE corpus & - & -  & \textbf{82.31} \\
    CoNLL2004 & 75.00 & \textbf{78.84}  & 78.48 \\
    GIDS & - & -  & \textbf{81.98} \\
    kbp37 & - & - & \textbf{36.14} \\
    NYT & - & - & \textbf{90.47} \\
    NYT11 HRL & - & - & \textbf{56.06} \\
    SciERC & 36.53 & 37.36 & \textbf{45.15} \\
    semeval RE & - & - & \textbf{73.23} \\
    Avg & - & - & \textbf{67.98} \\
    \bottomrule
    \end{tabular}
    }
    \caption{Overall results of InstructUIE on RE task. The evaluation metric is Relation Strict F1. Our model reaches an average F1 of 67.98\% on the eight datasets of the RE task and is comparable to the baseline.}
\label{supervised-result-RE}
\end{table}

\begin{table}[t]
\centering
    \begin{subtable}
    \centering
    \begin{tabular}{c|ccc|c}
    \toprule  
    Dataset & UIE & USM & Bert-base & Ours \\
    \midrule
    ACE2005 & 73.36 & 72.41 & 72.5 & \textbf{77.13} \\
    CASIE & 69.33 & \textbf{71.73} & 68.98 & 67.80 \\
    PHEE & - & - & - & \textbf{70.14} \\
    Avg & - & - & - & \textbf{71.69} \\
    \bottomrule
    \end{tabular}
    \caption*{a. Event Trigger F1}
    \label{subtable1}
    \end{subtable}
\hspace{0.1cm}
\begin{subtable}
    \centering
    \begin{tabular}{c|ccc|c}
    \toprule  
    Dataset & UIE & USM & Bert-base & Ours \\
    \midrule
    ACE2005 & 54.79 & 55.83 & 59.9 & \textbf{72.94} \\
    CASIE & 61.30 & 63.26 & 60.37 & \textbf{63.53} \\
    PHEE & - & - & - & \textbf{62.91} \\
    Avg & - & - & - & \textbf{66.46} \\
    \bottomrule
    \end{tabular}
    \caption*{b. Event Argument F1}
    \label{subtable2}
    \end{subtable}
\caption{Overall results of InstructUIE on EE task. The evaluation metric is Event Trigger F1 and Event Argument F1. Our model outperformed USM and UIE on some datasets.}
    \label{supervised-result-EE}
\end{table}



Tabel \ref{supervised-result-NER}, Tabel \ref{supervised-result-RE} and \ref{supervised-result-EE} show the performance of different models for the NER, RE, and EE tasks. 
\paragraph{Named Entity Recognition} 
Our model achieves an average F1 score of 85.19\% on 20 NER datasets, surpassing Bert's 80.09\%. The best performance is on the CoNLL2003 dataset, where InstructUIE achieved an F1 score of 92.94\%. For 20 NER data sets, InstructUIE outperforms the Bert model on 17 of them. Among them, our model outperforms Bert by more than 5 points on eight datasets. The dataset with the biggest gap is the broad twitter dataset, where InstructUIE outperforms Bert by about 25 points.

In the ACE2005, Ontonotes, and Polyglot-NER datasets, our model performs slightly worse than Bert. We speculate that this is due to our strategy of sampling only 10,000 training examples for each dataset. The original corpora for these three datasets contain a larger number of training examples, such as 420,000 for Polyglot-NER, of which we only used around 20\%. The detailed number of training sets for all datasets can be seen in the appendix.

Compared with UIE and USM, our model also achieves comparable results on ACE2005 and CoNLL2003, which are two commonly used datasets. Due to the UIE and USM only test their modelson a small number of commonly used datasets, we are unable to compare our model with these two models on other datasets.





\begin{table*}[htbp]
    \centering
    \begin{tabular}{c|ccccccc}
    \toprule
        Model & Movie & Restaurant & AI & Literature & Music & Politics & Science \\
        \midrule
        USM & 37.73 & 14.73 & 28.18 & \textbf{56.00} & 44.93 & 36.10 & 44.09 \\
        InstructUIE & \textbf{63.00} & \textbf{20.99} & \textbf{49.00} & 47.21 & \textbf{53.16} & \textbf{48.15} & \textbf{49.30} \\
    \bottomrule
    \end{tabular}
    \caption{
    Micro-F1 scores of zero-shot NER on 7 datasets. The best results are in bold. InstructUIE outperforms SOTA by a wide margin on most datasets ranging from 5.21\% to 25.27\%.}
    \label{zero-shot-NER}
\end{table*}

\begin{table*}[t]
    \centering
    \begin{tabular}{c|ccc}
    \toprule
         & Model & FewRel & Wiki-ZSL \\
        \midrule
        \multirow{2}*{Baselines} & $ZETT_{T5-small}$ & 30.53 & 31.74 \\
        ~ & $ZETT_{T5-base}$ & 33.71 & 31.17 \\
        \midrule
        Ours & InstructUIE & \textbf{39.55} & \textbf{ 35.20} \\
    \bottomrule
    \end{tabular}
    \caption{
    Micro-F1 scores of zero-shot RE on FewRel and Wiki-ZSL. The best results are in bold. InstructUIE outperforms SOTA on both datasets.}
    \label{zero-shot-RE}
\end{table*}


\paragraph{Relational Extraction}
Our model reaches an average F1 of 67.98\% on the eight datasets of the RE task, among which the NYT data set reaches 90.47\% F1 score. Among the eight datasets, CoNLL2004 and SciERC datasets are also tested by UIE and USM models. We focus on the analysis of the results of these two datasets. For the SciERC dataset, InstructUIE significantly outperforms UIE and USM by 8.62\% and 7.79\% respectively. For the CoNLL2004 dataset, InstructUIE outperforms UIE by more than three points, and lag USM by less than 0.5\%.
Moreover, noted that as BERT is usually used for relation classification tasks rather than relation extraction. Therefore, we did not use this baseline in the RE task.





\paragraph{Event Extraction}
Our model achieve sota on all datasets except for the Event Trigger F1 metric of the CASIE dataset. On the Event Trigger F1 metric, InstructUIE reaches an average of 71.69\% on these three datasets, with ACE2005 reaching 77.13\%, significantly surpassing UIE's 73.36\%,  USM's 72.41\% and Bert's 72.5\%. On the Event Argument F1 metric, InstructUIE beats three baseline models to reach sota on all three datasets. In particular, ACE2005 dataset reaches 72.94\%, 18 points higher than the UIE and 17 points higher than the USM.



\subsection{Experiments on Zero-shot Settings}

\subsubsection{Dataset}
To evaluate InstructUIE's zero-shot performance, we train the model on 18 NER datasets and 6 RE datasets and test it on 7 NER datasets and 2 RE datasets. Specifically, we eliminate the datasets for zero-shot experimental testing during the training phase. 
For the NER task, We use five CrossNER subsets(AI, literature, music, politics, science) \cite{CrossNERDATASET}, MIT Movie Review, and MIT Restaurant Review \cite{MITReviewDataset} to test the zero-shot capability of the model. 
For RE task, we test the zero-shot capability on FewRel \cite{FewRelDATASET} and Wiki-ZSL \cite{Wiki-ZSLDATASET}. For FewRel and Wiki-ZSL data sets, we follow the previous work \cite{relationprompt} and randomly select 5 unseen labels which do not appear in the training set as the test set. In order to reduce the effect of experimental noise, the unseen label selection process is repeated for five different random seeds to produce the test set.

Since the training and testing tasks do not overlap at all and across various domains as well, this setting is challenging.

\subsubsection{Baselines}
For zero-shot Named Entity Recognition and Relational Extraction, we compare InstructUIE with the following strong baselines:
\begin{itemize}
\item \textbf{ZETT}\cite{zett} is a novel framework based on end-to-end generative transformers and outperform previous state-of-the-art models;
\item \textbf{ChatGPT} \cite{InstructGPT} is also called GPT-3.5-turbo, which is the most capable GPT-3.5 model and optimized for chat;
\item \textbf{UIE} and \textbf{USM} have been introduced in \ref{supervise-baseline}.
\end{itemize}

\subsubsection{Results}


\begin{table*}[htbp]
    \centering
    \begin{tabular}{c|ccccccc|cc}
    \toprule
        Model & Movie & Restaurant & AI & Literature & Music & Politics & Science & FewRel & Wiki-ZSL \\
        \midrule
        davinci & 0.84 & 2.94 & 2.97 & 9.87 & 13.83 & 18.42 & 10.04 & 0.00 & 0.00 \\
        chatgpt & \textbf{41.00} & \textbf{37.76} & \textbf{54.40} & \textbf{54.07} & \textbf{61.24} & \textbf{59.12} & \textbf{63.00} & \textbf{9.96} & \textbf{13.14} \\
    \bottomrule
    \end{tabular}
    \caption{\label{zero-shot-GPT}
    Micro-F1 scores of davinci and chatgpt under zero-shot setting.}
\end{table*}

Table \ref{zero-shot-NER} and Table \ref{zero-shot-RE} show the performance of NER and RE tasks under the zero-shot setting. For the NER task, we can observe that InstructUIE outperforms the current sota model USM in Micro-F1 score on all the datasets except CrossNER\_Literature, ranging from 5.21\% to 25.27\%. For example, compared with the USM model, InstructUIE performs over 20 points better on the MIT Movie Review dataset and the CrossNER\_AI dataset. Noted that USM is trained on the same task corpus and tested on the label held out, while our model has never seen the task corpus. For the RE task, under the setting of 5 unseen labels, InstructUIE outperforms the current sota model ZETT on both the FewRel and Wiki-ZSL datasets by 5.84\% and 3.46\% respectively.

When compared to the GPT series model, InstructUIE significantly outperforms Davinci for the NER task but still falls some way short of Chatgpt's results for the NER task. However, for the RE task, our model performs much better than these two GPT series models. Both Davinci and Chatgpt perform poorly, especially with Davinci completely unable to output correct results.

It is worth mentioning that since Chatgpt is not open source, we have no way of knowing whether the model has seen the two data sets used by the zero-shot setting during training, and we think the huge difference in results for NER and RE tasks may be due to this reason.




















\section{Related Work}

\subsection{Instruction Tuning}
Instruction tuning \cite{mishra-etal-2022-cross, wang-etal-2022-super, Longpre2023TheFC}, a novel paradigm that leverages natural language instructions to guide large language models for downstream tasks, shows tremendous promise in generalization within the set of observed tasks.
Most recent work \cite{wang-etal-2022-super,Longpre2023TheFC} on instruction tuning has focused on general NLP tasks such as question answering and text classification, but not specifically on IE tasks.
While some work such as \cite{Wang2022InstructionNERAM, parmar-etal-2022-boxbart} includes a few IE tasks, those tasks do not provide good coverage of IE tasks and domains. 
No prior work has examined how training a model on a wide range of IE tasks with various instructions.
In this paper, we propose a unified framework for information extraction that involves auxiliary task design as well as specific tuning methods.

\subsection{Information Extraction}
Information extraction is fundamental in natural language processing systems, aiming to extract structured information from unstructured or semi-structured data sources automatically. 
Traditional methods \cite{wang-etal-2022-miner,yan-etal-2021-unified,CoNLL2004SOTA,NYT11HRLSOTA} for IE typically require the design of specific architectures for different IE tasks, and the models are trained separately. However, training dedicated models for different IE tasks requires a significant amount of labeled data, which can be costly and time-consuming to obtain. Secondly, knowledge learned from one IE task cannot be easily applied to another task, even if the tasks have similar characteristics. 
Recently, \citet{UIE} proposed UIE, which uniformly encodes different extraction structures via a structured extraction language and captures the common IE abilities via a large-scale pre-trained text-to-structure model. However, UIE requires separate finetune for different downstream tasks. This lead to the poor performance of UIE in low resource settings or facing new label schema. 
\citet{USM} proposed USM, which decouples IE into two basic tasks, token-token linking and label-token linking.  Unfortunately, USM requires semantic matching for each word, which leads to a significant increase in training and inference time.
InstructUIE addresses these challenges by utilizing instructive guidance to direct pre-trained large models toward the task, facilitating the efficient and adaptive generation of target structures.


\section{Conclusion}
In this paper, we propose an end-to-end framework for universal information extraction – InstructUIE, which leverages natural language instructions to guide large language models for IE tasks. 
We further introduce a new benchmark dataset. The benchmark consists of 32 diverse information extraction datasets that have been unified into a text-to-text format, allowing for a consistent and standardized evaluation of various IE tasks.
Experimental results demonstrate that InstructUIE achieves state-of-the-art results under supervised and zero settings and solves massive tasks using a single multi-task model. 





\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}


\section{Appendix}
\label{sec:appendix}
\subsection{Data Details}
\label{data details}
IE INSTRUCTIONS collects 32 publicly available datasets covering three IE tasks: NER, RE, and EE.
For NER(named entity extraction) task, the 21 used datasets includes ACE2004, ACE2005\cite{ACE2005DATASET}, broad\_twitter\_corpus\cite{broad_twitter_corpusDATASET}, CoNLL2003\cite{CoNLL03Dataset}, multiNERD\cite{multiNERDDATASET}, Ontonotes\cite{OntoNotesDataset}, polyglot-NER\cite{polyglot-NERDATASET}, tweetNER7\cite{tweetNER7DATASET}, wikiann\cite{wikiannDataset}, wikineural\cite{wikineuralDATASET}, AnatEM\cite{AnatEM}, bc2gm\cite{Kocaman2020BiomedicalNE}, bc4chemd\cite{bc4chemdDATASET}, bc5cdr\cite{Li2016BioCreativeVC}, FabNER\cite{Kumar2021FabNERIE}, FindVehicle\cite{FindVehicle}, GENIA\cite{GENIANERDATASET}, HarveyNER\cite{HarveyNERDATASET}, MIT Movie Review\cite{MITReviewDataset}, MIT Restaurant Review\cite{MITReviewDataset} and ncbi-disease\cite{ncbi-diseaseDATASET}. For RE(relation extraction) task, we use 10 datasets including ADE corpus\cite{ADEcorpusDATASET}, CoNLL2004\cite{Roth2004ALP}, GIDS\cite{Jat2018ImprovingDS}, kbp37\cite{kbp37DATASET}, NYT\cite{Riedel2010ModelingRA}, NYT11 HRL\cite{Takanobu2018AHF}, SciERC\cite{SciERCDATASET}, semeval RE\cite{Hendrickx2010SemEval2010T8}, FewRel\cite{FewRelDATASET} and Wiki-ZSL\cite{Wiki-ZSLDATASET}. For task EE(event extraction), ACE2005\cite{ACE2005DATASET}, CASIE\cite{Lu2021Text2EventCS}, GENIA\cite{Kim2003GENIAC} and PHEE\cite{Sun2022PHEEAD} are used. 

For the data set with only training set as the original data, we divided it into training set, validation set and test set according to the ratio of 8:1:1. For the data set with only training set and validation set as the original data, we randomly select half of the data in the validation set as the test set and the other half as the new validation set. For other datasets, we adopt the official split.










Tabel \ref{dataset-details} shows detailed datasets statistics. NER refers to Named Entity Recognition task, RE refers to Relation Extraction task, and EE refers to Event Extraction task. |Labels| indicates the number of labels, and $\#$ is the number of sentences in the specific subset.
 For the |Labels| of event extraction, the number outside the parenthesis indicates the number of event types and the number inside the parenthesis indicates the number of argument types.
\begin{table*}[htbp]
    \centering
    \begin{tabular}{c|c|c|ccc}
    \toprule  
    Task & Dataset & |labels| & \#Train & \#Val & \#Test \\
    \midrule
    \multirow{26}*{NER} & ACE2004 & 7 & 6202 & 745 & 812 \\
    ~ & ACE2005 & 7 & 7299 & 971 & 1060 \\
    ~ & broad\_twitter\_corpus & 3 & 5334 & 2000 & 2001 \\
    ~ & CoNLL2003 & 4 & 14041 & 3250 & 3453 \\
    ~ & multiNERD & 16 & 134144 & 10000 & 10000 \\
    ~ & Ontonotes & 18 & 59924 & 8528 & 8262\\
    ~ & polyglot-NER & 3 & 393982 & 10000 & 10000 \\
    ~ & tweetNER7 & 7 & 7111 & 886 & 576 \\
    ~ & wikiann & 3 & 20000 & 10000 & 10000 \\
    ~ & wikineural & 3 & 92720 & 11590 & 11597 \\
    ~ & AnatEM & 1 & 5861 & 2118 & 3830 \\
    ~ & bc2gm & 1 & 12500 & 2500 & 5000 \\
    ~ & bc4chemd & 1 & 30682 & 30639 & 26364 \\
    ~ & bc5cd & 2 & 4560 & 4581 & 4797 \\
    ~ & CrossNER\_AI & 14 & 100 & 350 & 431 \\
    ~ & CrossNER\_literature & 12 & 100 & 400 & 416 \\
    ~ & CrossNER\_music & 13 & 100 & 380 & 465 \\
    ~ & CrossNER\_politics & 9 & 199 & 540 & 650 \\
    ~ & CrossNER\_science & 17 & 200 & 450 & 543 \\
    ~ & FabNER & 12 & 9435 & 2182 & 2064 \\
    ~ & FindVehicle & 21 & 21565 & 20777 & 20777 \\~ & GENIA & 5 & 15023 & 1669 & 1854 \\
    ~ & HarveyNER & 4 & 3967 & 1301 & 1303 \\
    ~ & MIT Movie Review & 12 & 9774 & 2442 & 2442 \\
    ~ & MIT Restaurant Review & 8 & 7659 & 1520 & 1520 \\
    ~ & ncbi-disease & 1 & 5432 & 923 & 940 \\
    \midrule
    \multirow{10}*{RE} & ADE corpus & 1 & 3417 & 427 & 428 \\
    ~ & CoNLL2004 & 5 & 922 & 231 & 288 \\
    ~ & GIDS & 4 & 8526 & 1417 & 4307 \\
    ~ & kbp37 & 18 & 15917 & 1724 & 3405 \\
    ~ & NYT & 24 & 56196 & 5000 & 5000 \\
    ~ & NYT11 HRL & 12 & 62648 & 149 & 369 \\
    ~ & SciERC & 7 & 1366 & 187 & 397 \\
    ~ & semeval RE & 10 & 6507 & 1493 & 2717 \\
\midrule
    \multirow{4}*{EE} & ACE2005 & 33(22) & 3342 & 327 & 293 \\
    ~ & CASIE & 5(26) & 3751 & 788 & 1500 \\
    ~ & GENIA & 5(0) & 15023 & 1669 & 1854 \\
    ~ & PHEE & 2(16) & 2898 & 961 & 968 \\
    \bottomrule
    \end{tabular}
    \caption{\label{dataset-details}
Detailed datasets statistics.}
\end{table*}


\subsection{Instruction Details}
Table \ref{prompts_details} shows prompts for different tasks. NER refers to the named entity recognition task, the object of which is the entity in the output sentence and its corresponding entity type. RE refers to the relation extraction task, the object of which is to extract the relation triplet in the sentence, including the relation name, the head entity and the tail entity. EE refers to the event extraction task. The task objective is to extract the event types, trigger word and arguments in the sentence. ES refers to entity span, the task target is given sentence and entity category options, and output entities that conform to the entity category, but there is no need to output the entity type of each entity; ET refers to entity type identification. The task target is a given sentence, which contains entity and entity category options, and outputs the entity category corresponding to each entity. EP refers to entity pair identification (entity pair). The task target is given sentence and relation category options, and output entity pairs that conform to relation category, but do not need to output its relation category; EPR refers to entity pair relationship identification. The task target is a given sentence, which contains entity pair and relationship category options, and outputs the corresponding relationship category for each entity pair. ES and ET are auxiliary tasks of NER, EP and EPR are auxiliary tasks of RE, and EEA and EET are auxiliary tasks of EE.

\begin{table*}[htbp]
    \centering
    \begin{tabular}{m{0.1\linewidth}m{0.85\linewidth}}
    \toprule    
        \textbf{Task} & \textbf{{Prompts}} \\ \midrule
        \multirow{7}*{NER} & Please list all entity words in the text that fit the category. Output format is "type1: word1; type2: word2". \\ \\
        ~ & Please find all the entity words associated with the category in the given text. Output format is "type1: word1; type2: word2". \\ \\
        ~ & Please tell me all the entity words in the text that belong to a given category. Output format is "type1: word1; type2: word2". \\ 
        
        \midrule
        \multirow{8}*{RE} & Given a phrase that describes the relationship between two words, extract the words and the lexical relationship between them. The output format should be "relation1: word1, word2; relation2: word3, word4". \\ \\
        ~ & Find the phrases in the following sentences that have a given relationship. The output format is "relation1: word1, word2; relation2: word3, word4". \\ \\
        ~ & Given a sentence, please extract the subject and object containing a certain relation in the sentence according to the following relation types, in the format of "relation1: word1, word2; relation2: word3, word4". \\ 
        
        \midrule
        \multirow{3}*{EE} & Locate the role in the text that participated in the event based on the event type and return it in the event list. \\ \\
        ~ & Extract the event information in the text and return them in the event list. \\ 
        
        \midrule
        \multirow{1}*{ES} & Please list all entity words in the text that fit the category. Output format is word1, word2. \\ 
        
        \midrule
        \multirow{1}*{ET} & Given options, please tell me the categories of all the listed entity words.Output format is "type1: word1; type2: word2". \\ 
        
        \midrule
        \multirow{1}*{EP} & Please list all entity pairs containing a certain relationship in the given options.Output format is "word1, word2; word3, word4". \\ 
        
        \midrule
        \multirow{1}*{EPR} & Given options, please tell me the relationships of all the listed entity pairs.Output format is "relation1: word1, word2; relation2: word3, word4". \\ 
        
        \midrule
        \multirow{1}*{EEA} & Given event type and trigger, please tell me the arguments of all the listed option. Output format is "name: role". \\ 
        
        \midrule
        \multirow{1}*{EET} & Please tell me event type and its trigger word from given type options. Output format is "event type: trigger". \\ \midrule
    \end{tabular}
    \caption{\label{prompts_details}
    Instructions for different tasks.
    }
\end{table*}



\begin{table*}[htbp]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c|c|cc}
    \toprule  
    Dataset & Metric & \multicolumn{2}{c}{Task-specific SOTA Methods} \\
    \midrule
    ACE2004 & Entity F1 & \citet{ACE0405SOTA} & 90.3 \\
    ACE2005-Ent & Entity F1 & \citet{ACE0405SOTA} & 90.9 \\
    AnatEM & Entity F1 & \citet{AnatEMSOTA} & 91.65 \\
    bc2gm & Entity F1 & \citet{bc2gmSOTA} & 88.75 \\
    bc4chemd & Entity F1 & \citet{bc4chemdSOTA} & 94.39 \\
    bc5cdr & Entity F1 & \citet{bc5cdrSOTA} & 91.9 \\
    broad\_twitter\_corpus & Entity F1 & \citet{broad_twitter_corpusSOTA} & 74.70 \\
    CoNLL2003 & Entity F1 & \citet{CoNLL2003SOTA} & 94.60\\
    FabNER & Entity F1 & \citet{FabNERSOTA} & 88 \\
    FindVehicle & Entity F1 & \citet{FindVehicleSOTA} & 80.9 \\
    GENIA-Ent & Entity F1 & \citet{GENIANERSOTA} & 80.80 \\
    HarveyNER & Entity F1 & \citet{HarveyNERSOTA} & 68.97 \\
    MIT Movie Review & Entity F1 & \citet{MITMovieSOTA} & 87.31 \\
    MIT Restaurant Review & Entity F1 & \citet{MITRestaurantSOTA} & 79.6 \\
    multiNERD & Entity F1 & \citet{multiNERDSOTA} & 85.0 \\
    ncbi-disease & Entity F1 & \citet{ncbiSOTA} & 90.48 \\
    Ontonotes & Entity F1 & \citet{OntonotesSOTA} & 92.07 \\
    polyglot-NER & Entity F1 & - \\
    tweetNER7 & Entity F1 & \citet{tweetNER7SOTA} & 66 \\
    wikiann & Entity F1 & \citet{wikiannSOTA} & 91.8 \\
    wikineural & Entity F1 & -  \\
    ADE corpus & Relation Strict F1 & \citet{ADEcorpusSOTA} & 83.9 \\
    CoNLL2004 & Relation Strict F1 & \citet{CoNLL2004SOTA} & 76.65 \\
    GIDS & Relation Strict F1 & - \\
    kbp37 & Relation Strict F1 & - \\
    NYT & Relation Strict F1 & - \\
    NYT11 HRL & Relation Strict F1 & \citet{NYT11HRLSOTA} & 55.47 \\
    SciERC & Relation Strict F1 & \citet{SciERCSOTA} & 38.40 \\ 
    semeval RE & Relation Strict F1 & \
    \citet{semevalRESOTA} & 76.00 \\
    ACE2005 & Event Trigger F1 & - & -  \\
    ACE2005 & Event Argument F1 & - & - \\
    CASIE & Event Trigger F1 & - & - \\
    CASIE & Event Argument F1 & - & - \\
    GENIA-Evt & Event Trigger F1 & - & 63.96 \\
    GENIA-Evt & Event Argument F1 & - & - \\
    PHEE & Event Trigger F1 & - & - \\
    PHEE & Event Argument F1 & - & - \\
    \bottomrule
    \end{tabular}
}
    \caption{
Overall results of InstructUIE on different datasets. InstructUIE perform better or comparable than Bert on popular NER datasets like ACE2005, CoNLL2003, Ontonotes, and tweetNER7. In the RE task, InstructUIE achieved results comparable to the baseline on most datasets. In the EE task, our model outperformed USM, UIE or SOTA on some datasets.
}
\label{supervised-result-withSOTA}
\end{table*}


\end{document}
