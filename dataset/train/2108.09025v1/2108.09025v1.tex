\section{Experiment}

\subsection{Datasets and Settings}

\paragraph{VOC 2012.} The PASCAL VOC 2012 segmentation dataset~\cite{everingham2010pascal} comes with a finely labeled \textsc{train} split of 1,464 images, a \textsc{val} split of 1,449 images, and a coarsely labeled \textsc{aug} split of 9,118 images. We use the public 1/2, 1/4, 1/8, and 1/16 subsets of \textsc{train} in \cite{zou2020pseudoseg} as the labeled data. The \textsc{train} and \textsc{aug} sets are combined as the unlabeled dataset. 
We also compare with existing semi-supervised semantic segmentation methods under the 1.5k/9k split setting \cite{souly2017semi,hung2019adversarial,ouali2020semi,zou2020pseudoseg}, which treats \textsc{train} as the labeled set and \textsc{train}+\textsc{aug} as the unlabeled set. VOC contains 20 object categories (excluding the background class). The performance is evaluated by the \emph{mean intersection-over-union} (mIoU) metric on the \textsc{val} set.

\paragraph{Cityscapes.} Cityscapes~\cite{cordts2016cityscapes} contains images of urban driving scenes. The \textsc{train\_fine} split has 2,975 training images, and \textsc{val\_fine} has 5,000 images. We employ the public 1/4, 1/8, and 1/30 subsets of the \textsc{train\_fine} images in recent literature \cite{hung2019adversarial,mittal2019semi,fenga2020dmt,french2020semi,zou2020pseudoseg,olsson2021classmix} as the labeled data, while all images of the original \textsc{train\_fine} are used as the unlabeled data. We report the mIoU over the 18 foreground categories in the Cityscapes \textsc{val\_fine} set.

\paragraph{COCO.} COCO~\cite{lin2014microsoft} is a challenging detection and segmentation dataset of 80 categories. It is significantly larger (118,287 images in the official \textsc{train} set) and more diverse than VOC or Cityscapes. We reuse the same 1/32, 1/64, 1/128, 1/256, and 1/512 splits of the \textsc{train} set from \cite{zou2020pseudoseg} as the labeled data, and further enrich the benchmark by constructing the uniformly downsampled 1/8 and 1/16 labeled variants. We again evaluate the performance by the mIoU metric on the official \textsc{val} set (5,000 images).


\begin{table}[tb]
    \small
    \centering
    \caption{VOC 2012 1/2-1/16 labeled \textsc{train} set results. The settings follow \cite{zou2020pseudoseg}. The numbers of labeled images are shown inside the parentheses. R50 and R101 refer to ResNet 50 and 101. We measure the mIoU on the VOC 2012 \textsc{val} set. The mIoUs of compared methods are the reproduced results in \cite{zou2020pseudoseg} under these data splits. The results of \cite{zou2020pseudoseg,french2020semi} and our method are obtained by DeepLab-v3+; \cite{ouali2020semi} uses PSP-Net whose base performance is comparable to DeepLab-v3+, while other methods use DeepLab-v2. \ours{} obtains favorable results over existing methods. Due to the high variance, we also include the standard deviation of 3 runs on differently sampled 1/16 splits.} 
    \label{tab:voc1}
    \setlength{\tabcolsep}{0.9pt}
    \begin{tabular}{llcccc}
    \toprule
        Method    & Net & 1/2 (732) & 1/4 (366) & 1/8 (183) & 1/16 (92) \\
    \midrule
        AdvSemSeg \cite{hung2019adversarial}  & R101  & 65.27  & 59.97  & 47.58  & 39.69 \\
        CCT \cite{ouali2020semi}  & R50  & 62.10  & 58.80  & 47.60  & 33.10 \\
        MT \cite{tarvainen2017mean}  & R101  & 69.16  & 63.01  & 55.81  & 48.70 \\
        GCT \cite{ke2020guided}  & R101  & 70.67  & 64.71  & 54.98  & 46.04  \\
        VAT \cite{miyato2018virtual}  & R101  & 63.34  & 56.88  & 49.35  & 36.92  \\
        CutMix \cite{french2020semi}  & R101  & 69.84  & 68.36  & 63.20  & 55.58  \\
        PseudoSeg \cite{zou2020pseudoseg} & R50  & 70.42  & 64.85  & 61.88  & 54.89  \\
        PseudoSeg \cite{zou2020pseudoseg} & R101 & 72.41  & 69.14  & 65.50  & \textbf{57.60}  \\
    \midrule
        Sup. baseline  & R50  & 65.73  & 57.76  & 49.57  & 43.97 \\
        Sup. baseline  & R101 & 66.28  & 60.02  & 49.83  & 40.02 \\
        \ours{} (Ours) & R50  & 70.90  & 67.62  & 64.63  & 56.90 $\pm$1.30 \\
        \ours{} (Ours) & R101 & \textbf{73.05}  & \textbf{69.78}  & \textbf{66.28}  & \textbf{57.00 $\pm$1.32} \\
    \bottomrule
    \end{tabular}
\end{table}

\begin{table}[tb]
    \small
    \centering
    \caption{VOC 2012 1.5k/9k split results (\textsc{train} 1.5k images as labeled data and \textsc{train} 1.5k + \textsc{aug} 9k as unlabeled data). The numbers of other methods are directly taken from the corresponding publications.}
    \label{tab:voc2}
    \setlength{\tabcolsep}{10pt}
    \begin{tabular}{llcccc}
    \toprule
        Method    & Network & mIoU (\%)  \\
    \midrule
        GANSeg \cite{souly2017semi} & VGG-16 & 64.10  \\
        AdvSemSeg \cite{hung2019adversarial} & ResNet-101  & 68.40  \\
        CCT \cite{ouali2020semi} & ResNet-50  & 69.40 \\
        PseudoSeg \cite{zou2020pseudoseg} & ResNet-50  & 71.00 \\
        PseudoSeg \cite{zou2020pseudoseg} & ResNet-101 & 73.23 \\
    \midrule
        Sup. baseline  & ResNet-50  & 68.81 \\
        Sup. baseline  & ResNet-101 & 72.00 \\
        \ours{} (Ours) & ResNet-50  & \textbf{72.26}  \\
        \ours{} (Ours) & ResNet-101 & \textbf{74.15}  \\
    \bottomrule
    \end{tabular}
    \vspace{-5pt}
\end{table}


\paragraph{Implementation Details.}
We adopt the DeepLab-v3+ architecture \cite{chen2018encoder} as the test bed, which consists of a fully convolutional backbone, an atrous spatial pyramid pooling (ASPP) based encoder, and a shallow decoder.
We study different backbones including ResNet-50 (the DeepLab modified beta variant \cite{chen2018encoder}), ResNet-101 \cite{he2016deep}, and Xception-65 \cite{chollet2017xception}, to test the robustness of our approach to backbone variations.
The backbones are initialized from their ImageNet \cite{deng2009imagenet} pretrained weights.

The hyper-parameters of our approach mainly include the loss coefficients, the negative sampling strategies, and other design choices of the pixel contrastive loss. They are tuned with the VOC 1/8 split and ResNet-50. Please refer to the ablation study in Sec.~\ref{sec:exp/analysis} for details. These hyper-parameters are used for other benchmarks without further tuning. The performance could be further improved by dataset-specific hyper-parameter selection. In the main results, we set the loss coefficients as $\lambda_1=0.3$ and $\lambda_2=1$. The contrastive loss is computed for each anchor pixel in the last backbone feature maps right before the encoder network (\ie, Conv5 of ResNets and ExitFlow2 of Xception) of both the weak and strong views. The number of negative pixels is 200. They are sampled from both views with the ``Different Image + Pseudo Label'' strategy. Additional training details and DeepLab-related hyper-parameters are presented in the supplementary material.


\begin{table}[tb]
    \small
    \centering
    \caption{Cityscapes experiment results. The labeled data is varied from 1/30 to full of the original \textsc{train\_fine} set. We evaluate the mIoU on the \textsc{val\_fine} set. The result of \cite{french2020semi} is reproduced by \cite{zou2020pseudoseg} based on DeepLab-v3+, while the results of \cite{hung2019adversarial,mittal2019semi,fenga2020dmt,olsson2021classmix} are their reported numbers based on DeepLab-v2. R50 and R101 refer to ResNet 50 and 101. Our approach {\em achieves higher scores than all previous methods}. In the 1/4 and 1/8 settings, even the weaker ResNet-50 backbone performs quite favorably.}
    \label{tab:cityscapes}
    \setlength{\tabcolsep}{1.5pt}
    \begin{tabular}{@{}llcccc}
    \toprule
        Method    & Net & 1 (2,975) & 1/4 (744) & 1/8 (377) & 1/30 (100)  \\
    \midrule
        AdvSemSeg \cite{hung2019adversarial} & R101  & -  & 62.3\phantom{0}  & 58.8\phantom{0}  & -  \\
        s4GAN \cite{mittal2019semi}  & R101  & 65.8\phantom{0}  & 61.9\phantom{0}  & 59.3\phantom{0}  & -  \\
        DMT \cite{fenga2020dmt} & R101  & 68.16  & -  & 63.03  & 54.80  \\
        ClassMix \cite{olsson2021classmix} & R101 & - & 63.63  & 61.35  & -  \\
        CutMix \cite{french2020semi} & R101  & - &  68.33  & 65.82  & 55.71  \\
        PseudoSeg \cite{zou2020pseudoseg} & R101  & - & 72.36  & 69.81  & 60.96  \\
    \midrule
        Sup. baseline  & R50  & 73.64  & 72.86  & 68.06  & 55.25  \\
        Sup. baseline  & R101 & 74.88  & 73.31  & 68.72  & 56.09  \\
        \ours{} (Ours) & R50  & 75.39  & 73.80  & 72.11  & 60.37  \\
        \ours{} (Ours) & R101 & \textbf{75.99}  & \textbf{75.15}  & \textbf{72.29}  & \textbf{62.89}  \\
    \bottomrule
    \end{tabular}
    \vspace{-5pt}
\end{table}


\begin{table*}[tb]
    \small
    \centering
    \caption{COCO results. The original \textsc{train} set is split into 1/512-1/8 subsets to be the labeled data. PseudoSeg \cite{zou2020pseudoseg} results are taken from their paper. \ours{} performs {\em consistently} better than the supervised baseline and \cite{zou2020pseudoseg}. The gain over the supervised baseline is the largest in the 1/256 split, while our 1/8 split result {\em almost matches the supervised full data result}.}
    \label{tab:coco}
    \setlength{\tabcolsep}{2.9pt}
    \begin{tabular}{llcccccccc}
    \toprule
        Method    & Network  & Full (118k) & 1/8 (14786) & 1/16 (7393) & 1/32 (3697) & 1/64 (1849) & 1/128 (925) & 1/256 (463) & 1/512 (232) \\
    \midrule
        Sup. baseline & Xception-65 & 50.10  & 47.91  & 45.12  & 42.24  & 37.80  & 33.60  & 27.96  & 22.94 \\
        PseudoSeg \cite{zou2020pseudoseg}  & Xception-65 & -  & -  & -  & 43.64  & 41.75  & 39.11  & 37.11  & 29.78  \\
        \ours{} & Xception-65 & \textbf{50.67}  & \textbf{49.91}  & \textbf{48.07}  & \textbf{46.05}  & \textbf{43.67}  & \textbf{40.12}  & \textbf{37.53}  & \textbf{29.94}  \\
    \bottomrule
    \end{tabular}
    \vspace{-10pt}
\end{table*}


\subsection{Main Results}

\paragraph{VOC 2012.}
We report the results on the 1/2-1/16 labeled splits in Tab.~\ref{tab:voc1}. Our method (\ours{}) outperforms the prior state-of-the-art (PseudoSeg \cite{zou2020pseudoseg}) and other consistency-based approaches in almost all data splits with the ResNet-50 and ResNet-101 backbones. Note that the ratio of labeled data is small in this setting, since we use the entire \textsc{train}+\textsc{aug} 10,575 images as the unlabeled data. We observe that the gain of our approach is {\em particularly noteworthy for moderately low-data regimes} such as the 1/2 and 1/4 splits. The more extreme case of 1/16 labeled split (92 labeled images) is quite challenging, as there is very little supervision signal for the semi-supervised model to learn in the first place. We thus augment the weak branch with the momentum averaging technique as in MoCo (momentum 0.99) and report the mean and standard deviation (std) of 3 runs of the 3 randomly generated 1/16 splits and observe that our method is {\em comparable} to \cite{zou2020pseudoseg}.

We also compare the results of the 1.4k/9k split in Tab.~\ref{tab:voc2}.
We find that our method outperforms the prior arts under this setting, achieving $74.15\%$ mIoU with ResNet-101.


\paragraph{Cityscapes.}
The Cityscapes results are shown in Tab.~\ref{tab:cityscapes}. Across all semi-superivsed settings, \ours{} outperforms the supervised baseline by a large margin. Notably, with ResNet-101, the mIoU gap between the full set result ($75.99\%$) and the 1/4 labeled setting result ($75.15\%$) is only $0.84\%$. The performance of our method with ResNet-50 is surprisingly good in the 1/8 and 1/30 settings, suggesting that the deeper ResNet-101 backbone might be over-fitting in such settings due to the limited amount of labeled data.


\paragraph{COCO.}
For the COCO experiments, we mainly compare with the recent state-of-the-art PseudoSeg method \cite{zou2020pseudoseg} following their experiment protocols. Xception-65 \cite{chollet2017xception}, a stronger backbone than ResNet-101, is used. The results are shown in Tab.~\ref{tab:coco}. We observe that our method {\em performs better in all data splits}. We even see gains in the full data setting, possibly because of the stronger data augmentation and regularization in our method. The \textsc{val} mIoU of the 1/8 labeled setting is quite promising, {\em almost catching the supervised learning result of the fully labeled data}.



\subsection{Analyses}
\label{sec:exp/analysis}

We report the ablation studies and diagnosis experiments in this section. Without special mentioning, all experiments are conducted with the ResNet-50 backbone and VOC 1/8 labeled split. The mIoUs are evaluated on the VOC \textsc{val} set.


\paragraph{Negative Sampling Strategy.}
One key technical contribution of our method is the negative sampling strategy.
We measure the average False Negative Rates (FNR) during training with different negative sampling strategies in Tab.~\ref{tab:abl/sample}. 
We notice that FNR reduces with more complex sampling distributions. As expected, the Uniform strategy delivers the worst performance, suggesting that the noise introduced by false negative pixels may indeed hurt performance. The ideal case (marked as Oracle) pretends that the algorithm knows the actual ground-truth masks during contrastive learning and can be seen as an upper bound. Our best sampling strategy, Different Image + Pseudo Label, achieves an mIoU {\em almost as good as the Oracle version}.

\begin{table}[tb]
    \small
    \centering
    \caption{Quantitative comparison of negative sampling strategies in VOC 1/8 setting. We report the \textsc{val} mIoU and the False Negative Rates (FNR) of the sampled negative pixels. Oracle refers to the ideal case of using the ground-truth masks to guide the (uniform) negative pixel sampling.}
    \label{tab:abl/sample}
    \vspace{-5pt}
    \setlength{\tabcolsep}{5pt}
    \begin{tabular}{l|llll|l}
    \toprule
        Strategy  & Unif & Diff & Unif+Pseu & Diff+Pseu & Oracle  \\
    \midrule
        mIoU (\%)  & 62.70  & 63.33  & 64.38  & 64.63  & 64.78  \\
        FNR (\%)   & 14.77  & \phantom{0}3.42  & \phantom{0}2.83   & \phantom{0}2.80   & \phantom{0}0 \\
    \bottomrule
    \end{tabular}
    \vspace{-5pt}
\end{table}

\begin{table}[tb]
    \small
    \centering
    \caption{Study on the number of sampled negative pixels. None refers to not using the feature-space pixel contrastive loss. As the number of negatives increases, the \textsc{val} mIoU rises, but the computation and the memory costs also rise. In our experiment, $N=1,600$ actually causes the out-of-memory error. Therefore, we choose $N=200$ in the main results as a trade-off between accuracy and efficiency.}
    \label{tab:abl/numneg}
    \vspace{-5pt}
    \setlength{\tabcolsep}{3.5pt}
    \begin{tabular}{lccccccc}
    \toprule
        $N$ & None  & 50  & 100  & 200  & 400  & 800 & 1600 \\
    \midrule
    mIoU (\%)      & 63.75  & 64.08  & 64.24  & 64.63  & 64.84  &  65.03 & OOM \\
    \bottomrule
    \end{tabular}
    \vspace{-10pt}
\end{table}


\paragraph{Number of Negative Pixels.}
Another importance factor in our pixel contrastive loss is the number of sampled negative pixels $N$. Previous work suggests that a large number of negatives may be beneficial to image-level contrastive learning \cite{chen2020simple,he2020momentum}. While we generally agree with this argument, using a large number of negative pixels can be costly in semantic segmentation. We believe that there should be a trade-off between efficiency and accuracy. In Tab.~\ref{tab:abl/numneg}, we study the effect of the number of negative pixels. With the help of the negative sampling strategy to reduce false negative rates, our approach can already obtain competitive performance with only $N=200$ negative pixels per anchor.


\paragraph{Loss Coefficients.}
We show the impact of the coefficients on the label consistency loss (Eq.~\ref{eq:consist}) and the feature contrastive loss (Eq.~\ref{eq:contrast}) in Tab.~\ref{tab:abl/coef}. We found that $\lambda_1=0.3$ and $\lambda_2=1$ achieve the best result, therefore adopting these values in \emph{all} other data splits and architectures. {\em The coefficients transfer well to other settings}.
Another important observation with this hyper-parameter study is the {\em complementary nature of the label consistency and the feature contrastive properties}. We notice that the joint contrastive-consistent learning ($64.63\%$ when $\lambda_1=0.3, \lambda_2=1$) outperforms either the purely consistent version ($63.75\%$ when $\lambda_1=0, \lambda_2=1$) or the best purely contrastive version ($53.53\%$ when $\lambda_1=0.9, \lambda_2=0$), supporting our insight.

\begin{table}[tb]
    \footnotesize
    \centering
    \caption{Effect of different combinations of unlabeled loss weights. As in Eq.~\ref{eq:unlabeled}, $\lambda_1$ (column) is the coefficient on the pixel contrastive loss in the feature space, $\lambda_2$ (row) is the coefficient on the consistency loss in the label space. Performance is measured by the VOC \textsc{val} mIoU when trained in the 1/8-labeled setting. $\lambda_1=0$ is the special case of applying only label consistency training, while $\lambda_2=0$ is the special case of applying only feature contrastive learning.}
    \label{tab:abl/coef}
    \setlength{\tabcolsep}{6.5pt}
    \begin{tabular}{lcccccc}
    \toprule
      & $\lambda_1=0$ & 0.1 & 0.3 & 0.5 & 0.7 & 0.9  \\
    \midrule
    $\lambda_2=0$ & 49.57 & 51.92 & 52.29 & 52.28 & 52.74 & 53.53 \\
    $\lambda_2=0.5$ & 62.31 & 61.62 & 62.47 & 63.83 & 63.07 & 63.64 \\
    $\lambda_2=0.7$ & 63.46 & 63.26 & 63.71 & 63.79 & 63.35 & 63.43 \\
    $\lambda_2=1.0$ & 63.75 & 63.60 & \textbf{64.63} & \textbf{64.48} & 62.55 & 63.15 \\
    $\lambda_2=1.2$ & 61.33 & 64.48 & 64.30 & 64.16 & 63.08 & 63.03 \\
    \bottomrule
    \end{tabular}
    \vspace{-5pt}
\end{table}


\paragraph{Computational Cost.}
We report the training time comparison in Tab.~\ref{tab:abl/time}. While achieving higher performance, the training time of PC$^2$Seg is comparable to the previous state-of-the-art semi-supervised method PseudoSeg \cite{zou2020pseudoseg}. If we remove the contrastive component of PC$^2$Seg, the time is reduced by less than 5min, suggesting that the extra cost of our pixel contrastive learning is low.
We further show the cost reduced by our negative sampling strategy through a simple calculation in the supplementary material.

\begin{table}[tb]
\centering
\footnotesize
\caption{Training time comparison in the VOC 1/8 setting.}
\label{tab:abl/time}
\vspace{-5pt}
\setlength{\tabcolsep}{4.5pt}
\begin{tabular}{lcccc}
\toprule
    & {\scriptsize Supervised} & {\scriptsize PseudoSeg} & {\scriptsize PC$^2$Seg consist-only} & {\scriptsize PC$^2$Seg}
       \\
\midrule
    Time (min)
    & 38  & 80  & 75$\sim$80  &  80  \\
    \textsc{val} mIoU (\%)
    & 49.57  & 61.88  & 63.21  &  64.63  \\
\bottomrule
\end{tabular}
\vspace{-10pt}
\end{table}


\paragraph{Comparison to Other Label-Space and Feature-Space Losses.}
In the label space, we compare two variants: the normalized \ltwo loss (Sec.~\ref{sec:method}) and the cross-entropy (CE) loss.
In the feature space, we compare four variants: (1) no feature-space loss (None), (2) image contrastive loss \cite{chen2020simple}, (3) pixel consistency loss, which is the normalized \ltwo distance between pixel features, and (4) pixel contrastive loss. Tab.~\ref{tab:abl/losses} shows that the combination of the label \ltwo loss and the feature pixel contrastive loss achieves the best result.

\begin{table}[tb]
    \small
    \centering
    \caption{Investigation of the variants that use different loss functions on the label space (row) and the feature space (column) with other hyper-parameters fixed. Overall, we observe that the \ltwo loss is more robust as a label-space consistency loss than the cross-entropy (CE) loss. In the 2nd row, the pixel contrastive loss outperforms the pixel consistency loss and the image-level contrastive loss.}
    \label{tab:abl/losses}
    \vspace{-5pt}
    \setlength{\tabcolsep}{4.5pt}
    \begin{tabular}{lcccc}
    \toprule
    mIoU (\%) &  None &  Img Contrast  & Pix Consist  &  Pix Contrast  \\
    \midrule
    Output CE  & 63.54  & 60.41  & 59.33  & 58.47  \\
    Output \ltwo   & 63.75  & 62.49  & 63.21  & \textbf{64.63}  \\
    \bottomrule
    \end{tabular}
    \vspace{-5pt}
\end{table}


\paragraph{Choice of Contrastive Learning Layer.}
In the main results, we choose to apply pixel contrastive learning on the last feature map of the backbone networks (Conv5 block of ResNet and ExitFlow2 block of Xception). We show results of applying it on other ResNet layers in Tab.~\ref{tab:abl/layer}. Earlier stage feature maps are larger in size and contain more low-level details, while later stage feature maps lose resolutions but contain more semantics. A mid-level intermediate layer yields the best performance.

\begin{table}[tb]
    \small
    \centering
    \caption{Varying the feature layer to apply the pixel contrastive loss. We report the VOC \textsc{val} mIoU in the 1/8 setting. Conv3-5 are the corresponding conv blocks of ResNet-50. Encoder refers to the feature map produced by the DeepLab ASPP net. Decoder refers to the final feature map right before classification. Conv5+Enc imposes a contrastive loss on two best-performing layers (Conv5 and Encoder), which does not bring extra gains over a single layer.}
    \label{tab:abl/layer}
    \vspace{-5pt}
    \setlength{\tabcolsep}{1.9pt}
    \begin{tabular}{lcccccc}
    \toprule
        Layer  & Conv3 & Conv4 & Conv5 & Encoder & Decoder & Conv5+Enc \\
    \midrule
    mIoU (\%)  &  60.70 & 63.90 & \textbf{64.63} & 64.25  & 63.74  & 64.59  \\
    \bottomrule
    \end{tabular}
    \vspace{-5pt}
\end{table}


\paragraph{Projection Layer.}
The projection layer performs dimensionality reduction on the feature vectors before contrasive learning. We find that using separate projections for the weak and strong branches yields higher mIoU than using a shared projection head ($64.63\%$ \emph{vs.} $63.97\%$).


\paragraph{Delayed-Start of Semi-Supervised Learning.}
We mimic the two-stage variant that delays the start of semi-supervised learning until certain steps in Tab.~\ref{tab:abl/delay}. We notice that breaking the training into two stages in fact decreases the performance. Applying all losses jointly throughout the training ($\text{Delay}=0$) achieves a better result in our experiments.

\begin{table}[tb]
    \small
    \centering
    \caption{Delaying the unlabeled loss until certain steps. The total number of steps is 30,000. Applying all losses jointly without delay achieves the best result.}
    \label{tab:abl/delay}
\setlength{\tabcolsep}{6.5pt}
    \begin{tabular}{lccccc}
    \toprule
    Delayed Steps  & 0  & 2,000  & 4,000 & 8,000 & 12,000 \\
    \midrule
    mIoU (\%)   & \textbf{64.63}  & 63.30 & 62.67 & 62.56 & 62.50 \\
    \bottomrule
    \end{tabular}
    \vspace{-10pt}
\end{table}
