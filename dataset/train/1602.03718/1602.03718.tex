\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{graphicx,comment}
\usepackage[linesnumbered,vlined]{algorithm2e}
\newcommand{\todo}[1]{\typeout{TODO: #1}\textbf{[[[ #1 ]]]}}

\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage{cite}
\usepackage{fullpage}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{proposition}[theorem]{Proposition}

\newcommand{\concept}[1]{\textbf{#1}}
\newcommand{\etal}{\textit{et al.}\xspace}

\SetKwFor{Procedure}{procedure}{}{end procedure}

\providecommand{\abs}[1]{\left\lvert#1\right\rvert}
\providecommand{\norm}[1]{\left\lVert#1\right\rVert}

\providecommand{\card}[1]{\abs{#1}}

\providecommand{\ceil}[1]{\left\lceil#1\right\rceil}
\providecommand{\floor}[1]{\left\lfloor#1\right\rfloor}
\DeclareMathOperator{\id}{id}





\newenvironment{lemma-repeat}[1]{\begin{trivlist}
		\item[\hspace{\labelsep}{\bf\noindent Lemma \ref{#1} }]\em }{\end{trivlist}}
\newenvironment{theorem-repeat}[1]{\begin{trivlist}
		\item[\hspace{\labelsep}{\bf\noindent Theorem \ref{#1} }]\em }{\end{trivlist}}

\renewcommand\topfraction{0.85}
\renewcommand\bottomfraction{0.85}
\renewcommand\textfraction{0.1}
\renewcommand\floatpagefraction{0.85}
\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}


\newcommand{\accept}{\texttt{accept}}
\newcommand{\reject}{\texttt{reject}}
\newcommand{\poly}{\textrm{poly}}

\SetKwComment{tcpy}{\#~}{} \SetKwFor{Perform}{perform}{times}{endp} \SetKwFor{Simul}{for each}{simultaneously}{ends} \SetKwFor{Use}{use}{to}{endu} \SetKwInput{KwVars}{Variables} \SetKwInput{KwSet}{Variables set} 

\RestyleAlgo{boxruled}
\LinesNumbered

\begin{document}
\begin{titlepage}
\title{Fast Distributed Algorithms for Testing Graph Properties}
\author{Keren Censor-Hillel\thanks{Technion -- Israel Institute of Technology, Department of Computer
  Science. \texttt{ckeren@cs.technion.ac.il},
  \texttt{eldar@cs.technion.ac.il}, \texttt{gregorys@cs.technion.ac.il},
  \texttt{yaduvasudev@gmail.com}. Supported in part by the Israel Science Foundation (grant 1696/14).}
	\and Eldar Fischer\samethanks \and Gregory Schwartzman\samethanks \and Yadu Vasudev\samethanks }

\maketitle
	
\begin{abstract}
We initiate a thorough study of \emph{distributed property testing} -- producing algorithms for the approximation problems of property testing in the CONGEST model. In particular, for the so-called \emph{dense} graph testing model we emulate sequential tests for nearly all graph properties having -sided tests, while in the \emph{general} and \emph{sparse} models we obtain faster tests for triangle-freeness, cycle-freeness and bipartiteness, respectively. In addition, we show a logarithmic lower bound for testing bipartiteness and cycle-freeness, which holds even in the stronger LOCAL model.

In most cases, aided by parallelism, the distributed algorithms have a much shorter running time as compared to their counterparts from the sequential querying model of traditional property testing. The simplest property testing algorithms allow a relatively smooth transitioning to the distributed model. For the more complex tasks we develop new machinery that may be of independent interest.
\end{abstract}


~
\thispagestyle{empty}
\end{titlepage}

\newcommand{\ThmSim}
{
Any -test in the dense graph model for a non-disjointed property that makes  queries can be converted to a distributed -test that takes  communication rounds.
}
\newcommand{\ThmTri}{
Algorithm~\ref{alg:triangle-freeness} is a distributed -test in the general graph model for the property of containing no triangles, that requires  rounds.
}
\newcommand{\ThmBi}{
Algorithm~\ref{alg:dist-bip-test-det} is a distributed -test in the bounded degree graph model for the property of being bipartite, that requires  rounds.
}
\newcommand{\ThmCycle}{
Algorithm~\ref{alg:test-cycle-free} is a distributed -test in the general graph model
for the property of being cycle-free, that requires  rounds.
}
\newcommand{\ThmLBBi}{
Any distributed -test for the property of being bipartite requires
 rounds of communication.
}
\newcommand{\ThmLBCycle}{
Any distributed -test for the property of being cycle-free requires
 rounds of communication.
}



\section{Introduction}
\label{sec:intro}

The performance of many distributed algorithms naturally depends on
properties of the underlying network graph. Therefore, an inherent goal is
to check whether the graph, or some given subgraph, has certain properties.
However, in some cases this is known to be hard, such as in the CONGEST
model~\cite{Peleg00}. In this model, computation proceeds in synchronous
rounds, in each of which every vertex can send an -bit message
to each of its neighbors. Lower bounds for the number of rounds of type
 are known for \emph{verifying} many global graph
properties, where  is the number of vertices in the network and  is
its diameter (see, e.g. Das-Sarma et
al.~\cite{SarmaHKKNPPW12})\footnote{Here  hides factors that are
polylogarithmic in .}.

To overcome such difficulties, we adopt the relaxation used in graph property testing, as first defined in \cite{GGR98,GoldreichR02}, to the distributed setting. That is, rather than aiming for an exact answer to the question of whether the graph  satisfies a certain property , we settle for distinguishing the case of satisfying  from the case of being \emph{-far} from it, for an appropriate measure of being far.

Apart from its theoretical interest, this relaxation is motivated by the common scenario of having distributed algorithms for some tasks that perform better given a certain property of the network topology, or given that the graph \emph{almost} satisfies that property. For example, Hirvonen et al.~\cite{HirvonenRSS14} show an algorithm for finding a large cut in triangle-free graphs (with additional constraints), and for finding an -approximation if at most an  fraction of all edges are part of a triangle. Similarly, Pettie and Su~\cite{PettieS15} provide fast algorithms for coloring triangle-free graphs.

We construct fast distributed algorithms for testing various graph properties. An important byproduct of this study is a toolbox that we believe will be useful in other settings as well.

\subsection{Our contributions}

We provide a rigorous study of property testing methods in the realm of distributed computing under the CONGEST model.
We construct \emph{-sided error distributed -tests}, in which if the graph satisfies the property then all vertices output \accept, and if it is -far from satisfying the property then at least one vertex outputs \reject ~with probability at least .
Using the standard amplification method of invoking such a test  times and having a vertex output \reject~if there is at least one invocation in which it should output \reject, gives rejection with higher probability at the price of a multiplicative  factor for the number of rounds.

The definition of a graph being -far from satisfying a property is roughly one of the following (see Section~\ref{sec:prelim} for precise definitions): (1) Changing any  entries in the adjacency matrix does not give a graph that satisfies the property (dense model), or (2) changing any  entries in the adjacency matrix does not give a graph that satisfies the property, where  is the number of edges (general model). A particular case here is when the degrees are bounded by some constant , and any resulting graph must comply with this restriction as well (sparse model).



In a \emph{sequential -test}, access to the input is provided by queries, whose type depends on the model. In the dense model these are asking whether two vertices  are neighbors, and in the general and sparse models these can be either asking what the degree of a vertex  is, or asking what the -th neighbor of  is (the ordering of neighbors is arbitrary). While a sequential -test can touch only a small handful of vertices with its queries, in a distributed test the lack of ability to communicate over large distances is offset by having all  vertices operating in parallel.



Our first contribution is a general scheme for a near-complete emulation in the distributed context of -tests originating from the dense graph model (Section~\ref{sec:emulation}). This makes use of the fact that in the dense model all (sequential) testing algorithms can be made \emph{non-adaptive}, which roughly means that queries do not depend on responses to previous queries (see Section~\ref{sec:prelim} for definition).
In fact, such tests can be made to have a very simple structure, allowing the vertices in the distributed model to ``band together'' for an emulation of the test. There is only one additional technical condition (which we define below), since in the distributed model we cannot handle properties whose counter-examples can be ``split'' to disjoint graphs. For example, the distributed model cannot hope to handle the property of the graph having no disjoint union of two triangles, a property for which there exists a test in the dense model.

\begin{theorem-repeat}{thm:sim}
\ThmSim
\end{theorem-repeat}

We next move away from the dense graph model to the sparse and general models, that are sometimes considered to be more realistic. In the general model, there exists no test for the property of containing no triangle that makes a number of queries independent of the number of graph vertices~\cite{AKKR}. Here the distributed model can do better, because the reason for this deficiency is addressed by having all vertices operate concurrently. In Section~\ref{sec:triangle-freeness} we adapt the interim lemmas used in the best testing algorithm constructed in~\cite{AKKR}, and construct a distributed algorithm whose number of rounds is independent of .

\begin{theorem-repeat}{thm:tri}
\ThmTri
\end{theorem-repeat}

The sparse and general models inherently require \emph{adaptive} property testing algorithms, since there is no other way to trace a path from a given vertex forward, or follow its neighborhood. Testing triangle freeness sequentially uses adaptivity only to a small degree.
However, other problems in the sparse and general models, such as the one we explore next, have a high degree of adaptivity built into their sequential algorithms, and we need to take special care for emulating it in the distributed setting.

In the sparse model (degrees bounded by a constant ), we adapt ideas from the bipartiteness testing algorithm of~\cite{GR99}, in which we search for odd-length cycles. Here again the performance of a distributed algorithm surpasses that of the test (a number of rounds polylogarithmic in  vs. a number of queries which is  -- a lower bound that is given in~\cite{GoldreichR02}). The following is proved in Section~\ref{sec:bi}.

\begin{theorem-repeat}{thm:bi}
\ThmBi
\end{theorem-repeat}

In the course of proving Theorem~\ref{thm:bi} we develop a method that we consider to be of independent interest\footnote{This technique was recently independently and concurrently devised in~\cite{GhaffariKS16} for a different use.}. The algorithm works by performing  random walks concurrently (two starting from each vertex). The parallel execution of random walks despite the congestion restriction is achieved by making sure that the walks have a uniform stationary distribution, and then showing that congestion is ``close to average'', which for the uniform stationary distribution is constant.

In Section~\ref{sec:cycle} we show a fast test for cycle-freeness. This makes use of a combinatorial lemma that we prove, about cycles that remain in the graph after removing edges independently with probability . 
The following summarizes our result for testing cycle-freeness.
\begin{theorem-repeat}{thm:cycle-free-correctness}
\ThmCycle
\end{theorem-repeat}

We also prove lower bounds for testing bipartiteness and cycle-freeness (matching the upper bound for the latter). Roughly speaking, these are obtained by using the probabilistic method with alterations to construct graphs which are far from being bipartite or cycle-free, but all of their cycles are of length that is at least logarithmic. This technique bears some similarity to the classic result by Erd{\"o}s \cite{erdos1959graph}, which showed the existence of graphs with large girth and large chromatic number. The following are given in Section~\ref{sec:lowerbounds}.
\begin{theorem-repeat}{thm:bipart-lower-bound}
\ThmLBBi
\end{theorem-repeat}

\begin{theorem-repeat}{cor:cycle-lower-bound}
\ThmLBCycle
\end{theorem-repeat}
\paragraph{Roadmap:}
The paper is organized as follows. The remainder of this section consists of related work and historical background on property testing.
Section~\ref{sec:prelim} contains formal definitions and some mathematical tools.
The emulation of sequential tests for the dense model is given in Section~\ref{sec:emulation}. In Section~\ref{sec:triangle-freeness} we give our distributed test for triangle-freeness.
In Section~\ref{sec:bi} we provide a distributed test for bipartiteness, along with our new method of executing many random walks, and in Section~\ref{sec:cycle} we give our test for cycle-freeness. Section~\ref{sec:lowerbounds} gives our logarithmic lower bounds for testing bipartiteness and cycle-freeness.
We conclude with a short discussion in Section~\ref{sec:discussion}.

\subsection{Related work}




The only previous work that directly relates to our distributed setting is due to Brakerski and Patt-Shamir~\cite{brakerski2011distributed}. They show a \emph{tolerant} property testing algorithm for finding large (linear in size) \emph{near-cliques} in the graph. An -near clique is a set of vertices for which all but an -fraction of the pairs of vertices have an edge between them. The algorithm is tolerant, in the sense that it finds a linear near-clique if there exists a linear -near clique. That is, the testing algorithm considers two thresholds of being close to having the property (in this case -- containing a linear size clique). We are unaware of any other work on property testing in this distributed setting.

Testing in a different distributed setting was considered in Arfaoui et al.~\cite{ArfaouiFIM14}. They study testing for cycle-freeness, in a setting where each vertex may collect information of its entire neighborhood up to some distance, and send a short string of bits to a central authority who then has to decide whether the graph is cycle-free or not.


Related to having information being sent to, or received by, a central authority, is the concept of proof-labelling schemes, introduced by Korman et al.~\cite{KormanKP10} (for extensions see, e.g., Baruch et al.~\cite{BaruchFP15}). In this setting, each vertex is given some external label, and by exchanging labels the vertices need to decide whether a given property of the graph holds. This is different from our setting in which no information other than vertex IDs is available. Another setting that is related to proof-labelling schemes, but differs from our model, is the prover-verifier model of Foerster et al.~\cite{FoersterLSW16}.

Sequential property testing has the goal of computing without processing the entire input. The wider family of \emph{local computation algorithms} (LCA) is known to have connections with distributed computing, as shown by Parnas and Ron~\cite{ParnasR07} and later used by others. A recent study by G\"{o}\"{o}s et al.~\cite{goos2015non} proves that under some conditions, the fact that a centralized algorithm can query distant vertices does not help with speeding up computation. However, they consider the LOCAL model, and their results apply to certain properties that are not influenced by distances.

Finding induced subgraphs is a crucial task and has been studied in several different distributed models (see, e.g.,~\cite{KariMRS15,DruckerKO12,Censor-HillelKK15,DolevLP12}). Notice that for \emph{finding} subgraphs, having \emph{many} instances of the desired subgraph can help speedup the computation, as in~\cite{DolevLP12}. This is in contrast to algorithms that perform faster if there are \emph{no} or only \emph{few} instances, as explained above, which is why we test for, e.g., the property of being \emph{triangle-free}, rather for the property of \emph{containing} triangles. (Notice that these are not the same, and in fact every graph with  or more vertices is -close to having a triangle.)


Parallelizing many random walks was addressed in~\cite{AlonAKKLT11}, where the question of graph covering via random walks is discussed. It is shown there that for certain families of graphs there is a substantial speedup in the time it takes for  walks starting from the same vertex to cover the graph, as compared to a single walk. No edge congestion constraints are taken into account.
In~\cite{SarmaNPT13}, it is shown how to perform, under congestion, a single random walk of length  in  rounds, and  random walks in  rounds, where  is the diameter of the graph. Our method has no dependence on the diameter, allowing us to perform a multitude of \emph{short walks} much faster.

\subsection{Historical overview}
The first papers to consider the question of property testing were~\cite{BLR93} and~\cite{RS96}.
The original motivations for defining property testing were its connection to some Computerized
Learning models, and the ability to leverage some properties to construct Probabilistically
Checkable Proofs (PCPs -- this is related to property testing through the areas of Locally Testable
Codes and Locally Decodable Codes, LTCs and LDCs). Other motivations since then have entered the
fray, and foremost among them are sublinear-time algorithms, and other big-data considerations.
Since virtually no property can be decidable without reading the entire input, property testing
introduces a notion of the allowable approximation to the original problem. In general, the
algorithm has to distinguish inputs satisfying the property, from inputs that are {\em
-far} from it.  For more information on the general scheme of ``classical'' property
testing, consult the surveys \cite{Ron08,Fischer,GR10}.

The older of the graph testing models discussed here is the dense model, as defined in the seminal work of Goldreich, Goldwasser and Ron~\cite{GGR98}.
The dense graph model has historically kick-started combinatorial property testing in earnest, but it has some shortcomings. Its main one is the distance function, which makes sense only if we consider graphs having many edges (hence the name ``dense model'') -- any graph with  edges is indistinguishable in this model from an empty graph.

The stricter and at times more plausible distance function is one which is relative to the actual number of edges, rather than the maximum . The general model was defined in~\cite{AKKR}, while the sparse model was defined already in~\cite{GoldreichR02}.
The main difference between the sparse and the general graph models is that in the former there is also a guaranteed upper bound  on the degrees of the vertices, which is given to the algorithm in advance (the query complexity may then depend on , either explicitly, or more commonly implicitly by considering  to be a constant).


\section{Preliminaries}
\label{sec:prelim}
\subsection{Additional background on property testing}
While the introduction provided rough descriptions of the different property testing models, here we provide more formal definitions. The dense model for property testing is defined as follows.

\begin{definition}[dense graph model \cite{GGR98}]
	The dense graph model considers as objects graphs that are given by their adjacency matrix. Hence it is defined by the following features.
	\begin{itemize}
		\item {\bf Distance:} Two graphs with  vertices each are considered to be {\em -close} if one can be obtained from the other by deleting and inserting at most  edges (this is, up to a constant factor, the same as the normalized Hamming distance).
		\item {\bf Querying scheme:} A single query of the algorithm consists of asking whether two vertices  form a graph edge in  or not.
		\item {\bf Allowable properties:} All properties have to be invariant under permutations of the input that pertain to graph isomorphisms (a prerequisite for them being graph properties).
	\end{itemize}
	The number of vertices  is given to the algorithm in advance.
\end{definition}






As discussed earlier, the sparse and general models for property testing relate the distance function to the actual number of edges in the graph. They are formally defined as follows.

\begin{definition}[sparse \cite{GoldreichR02} and general \cite{AKKR} graph models]
	These two models consider as objects graphs given by their adjacency lists. They are defined by the following features.
	\begin{itemize}
		\item {\bf Distance:} Two graphs with  vertices and  edges (e.g.\ as defined by the denser of the two) are considered to be {\em -close} if one can be obtained from the other by deleting and inserting at most  edges\footnote{Sometimes in the sparse graph model the allowed number of changes is , as relates to the maximum possible number of edges; when  is held constant the difference is not essential.}.
		\item {\bf Querying scheme:} A single query consists of either asking what is the degree of a vertex , or asking what is the 'th neighbor of  (the ordering of neighbors is arbitrary).
		\item {\bf Allowable properties:} All properties have to be invariant under graph isomorphisms (which here translate to a relabeling that affects both the vertex order and the neighbor ids obtained in neighbor queries), and reordering of the individual neighbor lists (as these orderings are considered arbitrary).
	\end{itemize}
\end{definition}

In this paper, we mainly refer to the distance functions of these models, and less so to the querying scheme, since the latter will be replaced by the processing scheme provided by the distributed computation model. Note that most property testing models get one bit in response to a query, e.g.,
``yes/no'' in response to ``is uv an edge'' in the dense graph model. However, the sparse and general models may receive  bits of information for one query, e.g., an id of a neighbor of a vertex. Also, the degree of a vertex, which can be given as an answer to a query in the general model, takes  bits.
Since the distributed CONGEST model allows passing a vertex id or a vertex degree along an edge in  rounds, we can equally relate to all three graph models.



Another important point is the difference between -sided and -sided testing algorithms, and the difference between non-adaptive and adaptive algorithms.

\begin{definition}[types of algorithms]
	A property testing algorithm is said to have {\em -sided error} if there is no
	possibility of error on accepting satisfying inputs. That is, an input that satisfies the
	property will be accepted with probability , while an input -far from the
	property will be rejected with a probability that is high enough (traditionally this means a
	probability of at least ). A {\em -sided  error} algorithm is also allowed to reject
	satisfying inputs, as long as the probability for a correct answer is high enough
	(traditionally at least ).
	
	A property testing algorithm is said to be {\em non-adaptive} if it decides all its queries in advance (i.e.\ based only on its internal coin tosses and before receiving the results of any query), while only its accept/reject output may depend on the actual input. An {\em adaptive} algorithm may make each query in turn based on the results of its previous queries (and, as before, possible internal coin tosses).
\end{definition}

In the following we address both adaptive and non-adaptive algorithms. However, we restrict ourselves to -sided error algorithms, since the notion of -sided error is not a good match for our distributed computation model.

\subsection{Mathematical background}

An important role in our analyses is played by the Multiplicative Chernoff Bound (see, e.g.,~\cite{Mitzenmacher}), hence we state it here for completeness.
\begin{fact}
\label{fact:chernoff}
Suppose that  are independent random variables taking values in . Let  denote their sum and let  denote its expected value. Then, for any ,

Some convenient variations of the bounds above are:

\end{fact}



\section{Distributed emulation of sequential tests in the dense model}
\label{sec:emulation}
We begin by showing that under a certain assumption of being \emph{non-disjointed}, which we define below, a property  that has a sequential test in the dense model that requires  queries can be tested in the distributed setting within  rounds.
We prove this by constructing an emulation that translates sequential tests to distributed ones. For this we first introduce a definition of a \emph{witness} graph and then adapt~\cite[Theorem 2.2]{Goldreich}, restricted to -sided error tests, to our terminology.

\begin{definition}
\label{def:rejected-graph}
Let  be a property of graphs with  vertices. Let  be a graph with  vertices. We say that  is a \emph{witness against }, if it is not an induced subgraph of any graph that satisfies .		
\end{definition}
Notice that if  has an induced subgraph  that is a witness against , then by the above
definition  is also a witness against .\par
The work of \cite{Goldreich} transforms tests of graphs in the dense model to a canonical form where the query scheme is based on vertex selection. This is useful in particular for the distributed model, where the computational work is essentially based in the vertices. We require the following special case for 1-sided error tests.

\begin{lemma}[\textbf{\cite[Theorem 2.2]{Goldreich}}]
\label{claim:canonical-tester}
Let  be a property of graphs with  vertices.
If there exists a -sided error -test for  with query complexity , then there exists a -sided error -test for  that uniformly selects a set of  vertices, and accepts if and only if the induced subgraph is not a witness against .
\end{lemma}


Our emulation leverages Lemma~\ref{claim:canonical-tester} under an assumption on the property , which we define as follows.
\begin{definition}
We say that  is a \emph{non-disjointed} property if for every graph  that does not satisfy  and an induced subgraph  of  such that  is a witness against ,  has some connected component which is also a witness against . We call such components \emph{witness components}.
\end{definition}

We are now ready to formally state our main theorem for this section.

\begin{theorem}
\label{thm:sim}
\ThmSim
\end{theorem}


The following lemma essentially says that not satisfying a non-disjointed property cannot rely on subgraphs that are not connected, which is exactly what we need to forbid in a distributed setting.
\begin{lemma}
\label{lem:dist-prop-min}
The property  is a non-disjointed property if and only if all minimal
witnesses that are induced subgraphs of  are connected.
\end{lemma}
Here \emph{minimal} refers to the standard terminology, which means that no proper induced subgraph is a witness against .

\begin{proof}
First, if  is non-disjointed and  does not satisfy , then for
every subgraph  of  that is a witness against ,  has a
witness component. If  is minimal then it must be
connected, since otherwise it contains a connected component which is a
witness against , which contradicts the minimality of .

For the other direction, if all the minimal witnesses that are induced
subgraphs of  are connected, then every induced subgraph  that is a
witness against  is either minimal, in which case it is connected, or is
not minimal, in which case there is a subgraph  of  which is
connected and a minimal witness against . The connected component  of
 which contains  is a witness against  (otherwise  is not a
witness against ), and hence it follows that  is non-disjointed.
\end{proof}


Next, we give the distributed test (Algorithm~\ref{alg:simulate-test}). The test has an outer loop
in which each vertex picks itself with probability , collects its neighborhood of a certain
size of edges between \emph{picked} vertices in an inner loop, and rejects if it identifies a
witness against . The outer loop repeats two times because not only does the sequential test have
an error probability, but also with some small probability we may randomly pick too many or not
enough vertices in order to emulate it. Repeating the main loop twice reduces the error probability
back to below . In the inner loop, each vertex collects its neighborhood of picked vertices and
checks if its connected component is a witness against . To limit communications this is done
only for components of picked vertices that are sufficiently small: if a vertex detects that it is
part of a component with too many edges then it accepts and does not participate until the next
iteration of the outer loop.

\begin{algorithm}[htbp]
\caption{Emulation algorithm with input  for property 
\label{alg:simulate-test}}
\KwVars{ edges known to ,  edges to update and send (temporary variables)}
\Perform{}
{
	reset the state for all vertices\\
			
	\Simul{ vertex   }
	{
Vertex  \textit{picks} itself with probability \\
		\If{ is picked}
		{
			Notify all neighbors that  is picked\\
			Set  and \\
					
			\Perform{}
			{
				\tcpy{At each iteration  is a subgraph of 's connected component}
				 \tcpy{only need recently discovered edges}
				 \tcpy{add them to }
						
				\If(\tcpy*[h]{don't operate if there are too many edges}){}
				{
					Send  to all picked neighbours of  \tcpy{propagate known edges}
				}
				\textit{Wait} until the time bound for all other vertices to finish this iteration\\
				Set  to the union of edge sets received from neighbors
			}
			\If{  is a witness against }
			{
				Vertex  outputs \reject ~(ending all operations)
			}
		}
		\Else
		{
			\textit{Wait} until the time bound for all other vertices to finish this iteration of the outermost loop
		}
	}
}
Every vertex  that did not \reject~outputs \accept

\end{algorithm}
	
	
	
To analyze the algorithm, we begin by proving that there is a constant probability for the number of picked vertices to be sufficient and not too large.
\begin{lemma}
\label{lem:q}
The probability that the number of vertices picked by the algorithm is between  and  is more than  .
\end{lemma}
\begin{proof}
For every , we denote by  the indicator variable for the event that vertex  is picked.
Note that these are all independent random variables. Using the notation  gives that , because each vertex is picked with probability .
Using the Chernoff Bound from Fact~\ref{fact:chernoff} with  and , we can bound the probability of having too few picked vertices:

For bounding the probability that there are too many picked vertices, we use the other direction of the Chernoff Bound with  and , giving:

Thus, with probability at least  it holds that .
\end{proof}


Now, we can use the guarantees of the sequential test to obtain the guarantees of our algorithm.
\begin{lemma}
\label{lemma:correctness}
Let  be a non-disjointed graph property. If  satisfies  then all vertices output \accept ~in Algorithm~\ref{alg:simulate-test}. If  is -far from satisfying , then with probability at least  there exists a vertex that outputs \reject.
\end{lemma}
	
\begin{proof}
First, assume that  satisfies . Vertex  outputs \reject~only if it is part of a witness against , which is, by definition, a component that cannot be extended to some  that satisfies . However, every component is an induced subgraph of  itself, which does satisfy , and thus every component can be extended to . This implies that no vertex  outputs \reject.

Now, assume that  is -far from satisfying . Since the sequential test rejects with probability at least , the probability that a sample of at least  vertices induces a graph that cannot be extended to a graph that satisfies  is at least  . Because  is non-disjointed, the induced subgraph must have a connected witness against .
We note that a sample of more than  vertices does not reduce the rejection probability. Hence, if we denote by  the event that the subgraph induced by the picked vertices has a connected witness against , then , conditioned on that at least  vertices were picked.

However, a sample that is too large may cause a vertex to output \accept ~because it cannot collect its neighborhood. We denote by  the event that the number of vertices sampled is between  and , and by Lemma~\ref{lem:q} its probability is at least . We bound  using Bayes' Theorem, obtaining . Since the outer loop consists of  independent iterations, this gives a  probability of at least  for having a vertex that outputs \reject.
\end{proof}

We now address the round complexity. Each vertex only sends and receives information from its -neighborhood about edges between the chosen vertices. If too many vertices are chosen we detect this and accept. Otherwise we only communicate the chosen vertices and their edges, which requires  communication rounds using standard \emph{pipelining}\footnote{Pipelining means that each vertex has a buffer for each edge, which holds the information (edges between chosen vertices, in our case) it needs to send over that edge. The vertex sends the pieces of information one after the other.}.
Together with Lemma~\ref{lemma:correctness}, this proves Theorem~\ref{thm:sim}.
	
\subsection{Applications: -colorability and perfect graphs}
\label{subsec:sim}
Next, we provide some examples of usage of Theorem~\ref{thm:sim}. A result by Alon and Shapira~\cite{AlonShapira} states that all graph properties closed under induced subgraphs are testable in a number of queries that depends only on . We note that, except for certain specific properties for which there are ad-hoc proofs, the dependence is usually a tower function in  or worse (asymptotically larger).







 From this, together with Lemma~\ref{claim:canonical-tester} and Theorem~\ref{thm:sim}, we deduce that if  is a non-disjointed property closed under induced subgraphs, then it is testable, for every fixed , in a constant number of communication rounds.

	
\paragraph{Example -- -colorability:} The property of being -colorable is testable in a distributed manner by our algorithm. All minimal graphs that are witnesses against  (not -colorable) are connected, and therefore according to Lemma~\ref{lem:dist-prop-min} it is a non-disjointed property. It is closed under induced subgraphs, and by \cite{AlonK02} there exists a -sided error -test for -colorability that uniformly picks  vertices, and its number of queries is the square of this expression (note that the polynomial dependency was already known by~\cite{GGR98}). Our emulation implies a distributed -sided error -test for -colorability that requires  rounds.

	
\paragraph{Example -- perfect graphs:} A graph  is said to be \emph{perfect} if for every induced subgraph  of , the chromatic number of  equals the size of the largest clique in .
Another characterization of a perfect graph is via \emph{forbidden subgraphs}: a graph is perfect if and only if it does not have odd holes (induced cycles of odd length at least ) or odd anti-holes (the complement graph of an odd hole)~\cite{strong}. Both odd holes and odd anti-holes are connected graphs. Since these are all minimal witnesses against the property, according to Lemma~\ref{lem:dist-prop-min} it is a non-disjointed property.
Using the result of Alon-Shapira~\cite{AlonShapira} we know that the property of a graph being perfect is testable. Our emulation implies a distributed -sided error -test for being a perfect graph that requires a number of rounds that depends only on .

\section{Distributed test for triangle-freeness}
\label{sec:triangle-freeness}
In this section we show a distributed -test for triangle-freeness. Notice that since triangle-freeness is a non-disjointed property, Theorem~\ref{thm:sim} gives a distributed -test for triangle-freeness under the dense model with a number of rounds that is , where  is the number of queries required for a sequential -test for triangle-freeness. However, for triangle-freeness, the known number of queries is a tower function in ~\cite{Fox2010}.

Here we leverage the inherent parallelism that we can obtain when checking the neighbors of a vertex, and show a test for triangle-freeness that requires only  rounds (Algorithm~\ref{alg:triangle-freeness}). Importantly, our algorithm works not only for the dense graph model, but for the general graph model (where distances are relative to the actual number of edges), which subsumes it. In the sequential setting, a test for triangle-freeness in the general model requires a number of queries that is some constant power of  by \cite{AKKR}. Our proof actually follows the groundwork laid in \cite{AKKR} for the general graph model -- their algorithm picks a vertex and checks two of its neighbors for being connected, while we perform the check for all vertices in parallel.

\begin{algorithm}[htbp]
\caption{Triangle freeness test\label{alg:triangle-freeness}}
\Simul{vertex  }
{
	\Perform{}
	{	
		Pick  uniformly at random \\
		Send  to  \tcpy{Ask  if it is a neighbor of }
		\ForEach(\tcpy*[h]{Asked by  if  is a neighbor of }){ sent by }
		{
			\If{}
			{
				Send ``yes'' to \\
			}
			\Else
			{
				Send ``no'' to \\
			}
		}
		\If{received ``yes'' from }
		{
					\reject ~(ending all operations)
		}
	}
}
\accept ~(for vertices that did not reject)
\end{algorithm}


\begin{theorem}
\label{thm:tri}
\ThmTri
\end{theorem}

Our line of proof follows that of~\cite{AKKR}, by distinguishing edges that connect two high-degree vertices from those that do not. Formally, let , where  is the number of edges in the graph, and denote . We say that an edge  is \emph{light} if  or , and otherwise, we say that it is \emph{heavy}. That is, the set of heavy edges is . We begin with the following simple claim about the number of heavy edges.

\begin{claim}
\label{claim:tri-heavy}
The number of heavy edges, , is at most .
\end{claim}

\begin{proof}
The number of heavy edges is . Since , we get that .
This gives that .
\end{proof}

Next, we fix an iteration  of the algorithm. Every vertex  chooses two neighbors . Let , where  is the first of the two vertices chosen by the low-degree vertex . Let e, and let .
We say that an edge  is \emph{matched} if  is in the same triangle as .
If  is matched then  is a triangle that is detected by .

We begin with the following lemma that states that if  is -far from being triangle-free, then in any iteration  we can bound the expected number of matched edges from below by . Let  be the number of matched edges.

\begin{lemma}
\label{lem:matched}
The expected number of matched edges by a single iteration of the algorithm, , is greater than .
\end{lemma}
\begin{proof}
For every , let  be a random variable indicating whether  is matched. Then , giving the following bound:

where the last inequality follows because a light edge in  is chosen by a vertex with degree at most , hence the third triangle vertex gets picked with probability at least .

Next, we argue that . To see why, for every edge , let  be a random variable indicating whether .
Let . Then,

where the last inequality follows because a light edge has at least one endpoint with degree at most . Hence, this edge gets picked by it with probability at least .

It remains to bound  from below, for which we claim that  . To prove this, first notice that, since  is -far from being triangle free, it has at least  triangle edges, since otherwise we can just remove all of them and make the graph triangle free with less than  edge changes.
By Claim~\ref{claim:tri-heavy}, the number of heavy edges satisfies . Subtracting this from the number of triangle edges gives that at least  edges are light triangle edges, i.e.,



Finally, by Inequalities (\ref{eq:Y}),  (\ref{eq:A_T}) and (\ref{eq:T}), using iterated expectation we get:

\end{proof}
	
We can now prove the correctness of our algorithm, as follows.	

\begin{lemma}
\label{lem:tri-test}
If  is triangle-free then all vertices output \accept ~in Algorithm~\ref{alg:triangle-freeness}. If  is -far from being triangle-free, then with probability at least 2/3 there exists a vertex that outputs \reject.
\end{lemma}

\begin{proof}
If  is triangle free then in each iteration  receives ``no'' from  and after all iterations it returns \accept.

Assume that  is -far from being triangle-free. Let  be an indicator variable for the event that vertex  detects a triangle at iteration .
First, we note that the indicators are independent, since a vertex detecting a triangle does not affect the chance of another vertex detecting a triangle (note that the graph is fixed), and the iterations are done independently.
Now, let , and notice that  is the total number of detections over all iterations. Lemma~\ref{lem:matched} implies that for a fixed , it holds that , which sums to:

Using the Chernoff Bound from Fact~\ref{fact:chernoff} with  and  gives

and hence with probability at least  at least one triangle is detected and the associated vertex outputs \reject, which completes the proof.
\end{proof}	


In every iteration, each vertex initiates only two messages of size  bits, one sent to
 and one sent back by . Since there are  iterations, this implies that the
number of rounds is  as well. This, together with Lemma~\ref{lem:tri-test},
completes the proof of Theorem~\ref{thm:tri}.


\section{Distributed bipartiteness test for bounded degree graphs}
\label{sec:bi}
In this section we show a distributed -test for being bipartite for graphs with degrees bounded by .
Our test builds upon the sequential test of~\cite{GR99} and, as in the case of triangle freeness, takes advantage of the ability to parallelize queries. While the number of queries of the sequential test is ~\cite{GoldreichR02}, the number of rounds in the distributed test is only \emph{polylogarithmic} in  and polynomial in . As in~\cite{GR99}, we assume that  is a constant, and omit it from our expressions (it is implicit in the  notation for  below).

Let us first outline the algorithm of~\cite{GR99}, since our distributed test borrows from its framework and our analysis is in part derived from it. The sequential test basically tries to detect odd cycles. It consists of  iterations, in each of which a vertex  is selected uniformly at random and  random walks of length  are performed starting from the source . If, in any iteration with a chosen source , there is a vertex  which is reached by an even prefix of a random walk and an odd prefix of a random walk (possibly the same walk), then the algorithm rejects, as this indicates the existence of an odd cycle. Otherwise, the algorithm accepts.
To obtain an -test the parameters are chosen to be , , and .

The main approach of our distributed test is similar, except that a key ingredient is that we can afford to perform much fewer random walks from every vertex, namely . This is because we can run random walks in parallel originating from all vertices at once. However, a crucial challenge that we need to address is that several random walks may collide on an edge, violating its congestion bound. To address this issue, our central observation is that \emph{lazy} random walks (chosen to have a uniform stationary distribution) provide for a very low probability of having too many of these collisions at once. The main part of the analysis is in showing that with high probability there will never be too many walks concurrently in the same vertex, so we can comply with the congestion bound. We begin by formally defining the lazy random walks that we use.

\begin{definition}
\label{def:lazy-rw}
A {\em lazy} random walk over a graph  with degree bound  is a random walk, that is, a (memory-less) sequence of random variables  taking values from the vertex set , where the transition probability  is  if  is an edge of ,  if , and  in all other cases.
\end{definition}
The stationary distribution for the lazy random walk of Definition~\ref{def:lazy-rw} is uniform~\cite[Section 8]{ron2010algorithmic}.
Next, we describe a procedure to handle one iteration of moving the random walks (Algorithm~\ref{alg:dist-move-walks}), followed by our distributed test for bipartiteness using lazy random walks from every vertex concurrently (Algorithm~\ref{alg:dist-bip-test-det}).
		
\begin{algorithm}[htbp]
\caption{Move random walks once with input \label{alg:dist-move-walks}}
\KwVars{ walks residing in  (multiset),  history of walks through }
\KwIn{, the maximum congestion per vertex allowed}
\tcpy{each walk is characterized by  where  is the number of actual moves and  is the origin vertex}
\Simul{vertex }
{
    \If(\tcpy*[h]{give up if exceeded the maximum allowed}){}
    {
        \For{every  in }
        {
            draw next destination  (according to the lazy walk scheme)\\
			\If(\tcpy*[h]{walk exits }){}
            {
				send  to \\
				remove  from 
			}
		}
	}
    {\em wait} until the maximum time for all other vertices to process up to  walks\\
	add the walks received by  to  and  \tcpy{walks entering }
}
\end{algorithm}

It is quite immediate that Algorithm \ref{alg:dist-move-walks} takes  communication rounds.
		
\begin{algorithm}[htbp]
\caption{Distributed bipartiteness test\label{alg:dist-bip-test-det}}
\KwVars{ walks residing in  (multiset),  history of walks through }
\Perform{}
{
    \Simul{vertex }
    {
        initialize  and  with two copies of the walk 
    }
    \Perform{}
    {
        move walks using Algorithm \ref{alg:dist-move-walks} with input 
    }
    \Simul{vertex }
    {
        \If{ contains  and  for some , even  and odd }
        {
            \reject ~(ending all operations) \tcpy{odd cycle found}
        }
    }
}
\accept ~(for vertices that did not reject)
\end{algorithm}

Our main result here is that Algorithm~\ref{alg:dist-bip-test-det} is indeed a distributed -test for bipartiteness.

\begin{theorem}
\label{thm:bi}
\ThmBi
\end{theorem}

The number of communication rounds is immediate from the algorithm -- it is dominated by the  calls to Algorithm \ref{alg:dist-move-walks}, making a total of  rounds, which is indeed . To prove the rest of Theorem~\ref{thm:bi} we need some notation, and a lemma from~\cite{GR99} that bounds from below the probabilities for detecting odd cycles if  is -far from being bipartite.

Given a source , if there is a vertex  which is reached by an even prefix of a random walk  from  and an odd prefix of a random walk  from , we say that walks  and  \emph{detect a violation}. Let  be the probability that, out of  random walks of length  starting from , there are two that detect a violation.
Using this notation,  is the probability that the sequential algorithm outlined in the beginning rejects in an iteration in which  is chosen. Since we are only interested in walks of length , we denote . A good vertex is a vertex for which this probability is bounded as follows.
\begin{definition}
\label{def:good}
A vertex  is called \emph{good} if .
\end{definition}

In \cite{GR99} it was proved that being far from bipartite implies having many good vertices.

\begin{lemma}[\cite{GR99}]
\label{lemma:GR}
If  is -far from being bipartite then at least an -fraction of the vertices are good.
\end{lemma}

In contrast to~\cite{GR99}, we do not perform  random walks from every vertex in each iteration, but rather only . Hence, what we need for our analysis is a bound on . To this end, we use  as a parameter, and express  in terms of  and .

\begin{lemma}
\label{lemma:ps2}
For every vertex , .
\end{lemma}
\begin{proof}
Fix a source vertex . For every , let  be the probability of walks  from  detecting a violation. Because different walks are independent, we conclude that for every  it holds that . Let  be the event of walks  detecting a violation. We have

which implies that .
\end{proof}

Using this relationship between  and  and , we prove that our algorithm is an
-test. First we prove this for the random walks themselves, ignoring the possibility that
Algorithm \ref{alg:dist-move-walks} will skip moving random walks due to its condition in Line~.

\begin{lemma}\label{lem:walks-detect}
If  is -far from being bipartite, and we perform  iterations of starting  random walks of length  from every vertex, then the probability that no violation is detected is bounded by .
\end{lemma}


\begin{proof}
Assume that  is -far from being bipartite. By Lemma~\ref{lemma:GR}, at least  vertices are good, which means that for each of these vertices , . This implies that .
Now, let  be a random variable indicating whether there are two random walks starting at  that detect a violation. Let . We prove that . First, we bound  for some fixed :

For  it holds that .
Using the Chernoff Bound of Fact~\ref{fact:chernoff} with  and  gives:

which completes the proof.
\end{proof}


As explained earlier, the main hurdle on the road to prove Theorem~\ref{thm:bi} is in proving that the allowed congestion will not be exceeded. We prove the following general claim about the probability for  lazy random walks of length  from each vertex to exceed a maximum \emph{congestion factor} of  walks allowed in each vertex at the beginning of each iteration. Here, an iteration is a sequence of rounds in which all walks are advanced by one step (whether or not they actually switch vertices).
\begin{lemma}
\label{lemma:cong}
With probability at least , running  lazy random walks of length  originating from
every vertex will not exceed the maximum \emph{congestion factor} of  walks allowed in each vertex at the beginning of each iteration, if .
\end{lemma}
We show below that plugging ,  and  in Lemma~\ref{lemma:cong}, together with Lemma~\ref{lem:walks-detect}, gives the correctness of Algorithm \ref{alg:dist-bip-test-det}.

To prove Lemma~\ref{lemma:cong}, we argue that it is unlikely for any vertex to have more than  walks in any iteration. Given that this is indeed the case in every iteration, the lemma follows by a union bound. We denote by  the random variable whose value is the number of random walks at vertex  at the beginning of the -th iteration. That is, it is equal to the size of the set  in the description of the algorithm.

\begin{lemma}
\label{lem:exp-walks-in-vertex}
For every vertex  and every iteration  it holds that .
\end{lemma}
\begin{proof}
Let us first define random variables for our walks.
Enumerating our  walks ( from each of the  vertices) arbitrarily, let  denote the sequence corresponding to the 'th walk, that is,  is the vertex where the 'th walk is stationed at the beginning of the 'th iteration. In particular, .

Now let us define new random variables  in the following manner: First, we choose uniformly at random a permutation . Then we set  for all  and . The main thing to note is that for any fixed ,  is a random walk (as it is equal to one of the random walks ). But also, for every ,  is uniformly distributed over the vertex set of , because we started with exactly  random walks from every vertex. Additionally, since the uniform distribution is stationary for our lazy walks, this means that the unconditional distribution of each  is also uniform.

Now, since  is a permutation, it holds that . The expectation (by linearity of expectation) is thus .
\end{proof}


We can now prove Lemma~\ref{lemma:cong}.
\begin{proof}[Proof of Lemma~\ref{lemma:cong}]
We first claim that for every iteration  and every vertex , with probability at least  it holds that .
To show this, first fix some .
Let  be the indicator variable for the event of walk  residing at vertex  at the beginning of iteration , where . Then , and the variables , where , are all independent. We use the Chernoff Bound of Fact~\ref{fact:chernoff} with  and  as proven in Lemma~\ref{lem:exp-walks-in-vertex}, obtaining:

Applying the union bound over all vertices  and all iterations , we obtain that with probability at least  it holds that  for all  and .
\end{proof}

\begin{lemma}
\label{lemma:bi-test}
If  is bipartite then all vertices output \accept ~in Algorithm~\ref{alg:dist-bip-test-det}. If  is -far from being bipartite, then with probability at least  there exists a vertex that outputs \reject.
\end{lemma}
\begin{proof}
If  is bipartite then all vertices output \accept ~in Algorithm~\ref{alg:dist-bip-test-det}, because there are no odd cycles and thus no violation detecting walks.

If  is -far from bipartite, we use Lemma \ref{lem:walks-detect}, in conjunction with Lemma~\ref{lemma:cong} with parameters ,  and  as used by Algorithm~\ref{alg:dist-bip-test-det}. By a union bound the probability to accept  will be bounded by  (assuming ), providing for the required bound on the rejection probability.
\end{proof}

Lemma~\ref{lemma:bi-test}, with the communication complexity analysis of Algorithm~\ref{alg:dist-bip-test-det}, gives Theorem~\ref{thm:bi}.


\section{Distributed test for cycle-freeness}
\label{sec:cycle}
In this section, we give a distributed algorithm to test if a graph  with  edges is cycle-free
or if at least  edges have to be removed to make it so. Intuitively, in order to search for cycles, one can run a breadth-first search (BFS) and have a vertex output \reject~if two different paths reach it. The downside of this exact solution is that its running time depends on the diameter of the graph. To overcome this, a basic approach would be to run a BFS from each vertex of the graph, but for shorter distances. However, running multiple BFSs simultaneously is expensive, due to the congestion on the edges. Instead, we use a simple prioritization rule that drops BFS constructions with lower priority, which makes sure that one BFS remains alive.\footnote{A more involved analysis of multiple prioritized BFS executions was used in \cite{Holzer2012}, allowing all BFS executions to fully finish in a short time without too much delay due to congestion. Since we require a much weaker guarantee, we can avoid the strong full-fledged prioritization algorithm of \cite{Holzer2012} and settle for a simple rule that keeps one BFS tree alive. Also, the multiple BFS construction of \cite{LenzenP13} does not fit our demands as it may not reach all desired vertices within the required distance, in case there are many vertices that are closer.}

Instead, our technique consists of three parts. First, we make the graph  sparser, by removing each of its edges independently with probability . We denote the sampled graph by  and prove that if  is far from being cycle-free then so is , and in particular,  contains a cycle.

Then, we run a partial BFS over  from each vertex, while prioritizing by ids: each vertex keeps only the BFS that originates in the vertex with the largest id and drops the rest of the BFSs. The length of this procedure is according to a threshold . This gives detection of a cycle that is contained in a component of  with a low diameter of up to , if such a cycle exists, since a surviving BFS covers the component. Such a cycle is also a cycle in . If no such cycle exists in , then  has a some component with diameter larger than . For large components, we take each surviving BFS that reached some vertex  at a certain distance , and from  we run a new partial BFS in the \emph{original} graph . These BFSs are again prioritized, this time according to the distance . Our main tool here is proving a claim that says that with high probability, if there is a shortest path in  of length  between two vertices, then there is a cycle in  between them of length at most . This allows our BFSs on  to find such a cycle.

We start with the following combinatorial lemma that shows the above claim.


\begin{lemma}
  Given a graph , let  be obtained by deleting each edge in  with probability
  , independently of other edges. Then, with probability at least , every vertex  that has a vertex  at a distance , has a closed path passing
  through it in , that contains a simple cycle, of length at most .
  \label{lem:main}
\end{lemma}

\begin{proof}
  First, we show that for every pair  of vertices in  that are at a distance of , one of the shortest paths between  and  is removed in the graph  with high
  probability. For a pair of vertices  and  at a distance  in , the probability that a
  shortest path is not removed is , which is at most . Therefore, by a union
  bound over all pairs of vertices, with probability at least , at least one edge is
  removed from at least one shortest path between every pair of vertices that are at a distance of
  . Conditioned on this, we prove the lemma.

  Now, suppose that  and  are two vertices in  at a distance of . Let
   be this shortest path in . Suppose  is the shortest path between  and  in .
  If , then this path is no longer present in  (and thus distinct from
  ) and  is a closed path in , passing through  that has a simple cycle of
  length at most . If , then there are at least two
  shortest paths between  and  in  of length , the one in  and one
  that was removed, which we choose for . Therefore,  is a closed path passing through
   of length at most , and hence contains a simple cycle of length at most
   in it.
\end{proof}

Next, we prove that indeed there is a high probability that  contains a cycle if  is far from being cycle-free.
\begin{claim}
  If  is -far from being cycle-free, then with probability at least ,  is -far from being cycle-free.
  \label{obs:distance}
\end{claim}
\begin{proof}
  The graph  is obtained from  by deleting each edge with probability 
  independently of other edges. The expected number of edges that are deleted is .
  Therefore, by the Chernoff Bound from Fact~\ref{fact:chernoff}, the probability that at least  edges are
  deleted is at most , and the claim follows.
\end{proof}


We now describe a multiple-BFS algorithm that takes as input a length  and a priority condition  over vertices, and starts performing a BFS from each vertex of the graph. This is done for  steps, in each of which a vertex keeps only the BFS with the highest priority while dropping the rest. Each vertex also
maintains a list  of BFSs that have passed through it. The list  is a
list of -tuples , where  is the id of the root of the BFS,  is the
depth of  in this BFS tree and  is the id of the parent of  in the BFS
tree. Initially, each vertex  sets  to include a BFS starting from itself, and then continues this BFS by sending
the tuple  to all its neighbors, where  is the identifier of the vertex .
In an intermediate step, each vertex  may receive a BFS tuple from each of its neighbors. The vertex 
then adds these BFS tuples to the list  and chooses one among  according to the
priority condition , proceeding with the respective BFS and discontinuing the rest. Even when
a BFS is discontinued, the information that the BFS reached  is stored in the list .

Algorithm~\ref{alg:priority-bfs} gives a formal description of the breadth-first search that we
use in the testing algorithm for cycle-freeness.

\begin{algorithm}[htbp]
  \caption{BFS with a priority condition\label{alg:priority-bfs}}
  \KwIn{Length , Priority condition }
    \KwVars{ list of BFS tuples passing through }
    \Simul{vertex }
    {
Initialize  to .\\
      Send  to all neighbors of .
    }
    \Perform{ times}
    {
      \Simul{vertex }
      {
	\If{ receives  from its neighbors}
	{
	  Add  to .

	  Select  from  according to  over 

	  Send  to all neighbors of  except .
        }
      }
    }
\end{algorithm}


We now give more informal details of the test for cycle-freeness. By Lemma~\ref{lem:main}, we
know that if there is a vertex  in  that has a vertex  at a distance of , then there is a closed path in  starting from  that contains a cycle of length
. In the first part, each vertex gets its name as its vertex id, and performs a
BFS on the graph  in the hope of finding a cycle. The BFS is performed using
Algorithm~\ref{alg:priority-bfs}, where the priority condition in the intermediate steps is
selecting the BFS with the lowest origin id. If the cycle is present in a component of diameter at
most  in , then it is discovered during this BFS. To check if there is a
cycle, one needs to find if there are appropriate tuples  and
 in , for some vertex .

If no cycle is discovered in this step, then we change the ids of the vertices in the following way:
The id of each vertex  is now a tuple  where  is the largest depth at which  occurs
in a BFS tree among all the breadth-first searches that reached .  We perform a BFS in  using
Algorithm~\ref{alg:priority-bfs}, where the priority condition is to pick the BFS whose root has the
lexicographically highest id. If there is some vertex with , then the
highest priority vertex is such a vertex, and by Lemma~\ref{lem:main}, the BFS starting from that
vertex will detect a cycle in .

Algorithm~\ref{alg:test-cycle-free} gives a formal description of the tester for cycle-freeness.

\begin{algorithm}[htbp]
  \caption{Cycle-freeness test\label{alg:test-cycle-free}}
\KwVars{ list of BFS tuples passing through , vertex identifier }
  \tcpy{Construct  by deleting edges with probability .}
  \Simul{vertex }
  {
    For each neighbor , mark the edge  with probability  for deletion.

    Send each marked edge  to its corresponding .

    Set .
  }
  \Simul{vertex }
  {
    Delete all edges incident on  that have been marked for deletion.
  }
  \tcpy{Search for cycles in small diameter components.}
  \Use{Algorithm~\ref{alg:priority-bfs}}
  {
    perform BFS on  for  steps, with the priority condition being choosing the
    BFS with the lowest root id.
  }
  \Simul{vertex }
  {
    If  contains two tuples  and , output \reject.

    Set  where  is the highest among all tuples  in
    .
    \label{step:set-id}
  }
  \Use{Algorithm~\ref{alg:priority-bfs}}
  {
    perform BFS on  for  steps, with the priority condition being choosing the BFS
    with the lexicographically highest root id.
  }
  \Simul{vertex }
  {
    If  contains two tuples  and , output \reject.
  }
  \Simul{vertex }
  {
    output \accept, if  did not output \reject~ yet.
  }
\end{algorithm}

We now prove the correctness of the algorithm.


\begin{theorem}
Algorithm~\ref{alg:test-cycle-free} is a distributed -test in the general graph model
for the property of being cycle-free, that requires  rounds.
\label{thm:cycle-free-correctness}
\end{theorem}

\begin{proof}
    Notice that a vertex in Algorithm~\ref{alg:test-cycle-free} outputs \reject~only when it detects a cycle.
      Therefore, if  is cycle-free, then every vertex outputs \accept~with probability .

      Suppose that  is -far from being cycle-free. Notice that, with probability at
      least , the assertion of Lemma~\ref{lem:main} holds.  Furthermore, from
      Claim~\ref{obs:distance}, we know that  is -far from being cycle-free,
      with probability , and hence contains at least one cycle.  This cycle
      could be in a component of diameter less than  or it could be in a
      component of diameter at least  in . We analyse the two cases
      separately.

      Suppose there is a cycle in a component  of  of diameter at most . Let 
      be the vertex with the smallest id in . In Algorithm~\ref{alg:test-cycle-free}, the BFS
      starting at  is always propagated at any intermediate vertex due to the priority
      condition. Furthermore, since the diameter of  is at most , this BFS
      reaches all vertices of .  Hence, this BFS detects the cycle and at least one vertex
      in  outputs \reject.

      On the other hand, if the cycle is present in a component in  of diameter at least , then after Step~ of the algorithm, each vertex  gets the
      length of the longest path from the origin, among all the BFSs that reached , as the
      first component of its id. The vertex  that gets the lexicographically highest id in the
      component has a vertex  that is at least  away in , since the radius
      of the component is at least half the diameter. Therefore, by Lemma~\ref{lem:main}, it is part
      of a cycle of length at most  in . Hence, the vertex with the highest
      priority in the BFS on  is a vertex  that has a vertex at a distance of at least  in , and there is a walk through  that contain a simple cycle of length at
      most . At least one vertex on this simple cycle will output \reject~when
      Algorithm~\ref{alg:test-cycle-free} is run on .


      The number of rounds is  since Algorithm~\ref{alg:test-cycle-free}
      performs two breadth-first searches in the graph with this number of rounds.\end{proof}


\section{Lower bounds for testing bipartiteness and cycle-freeness}
\label{sec:lowerbounds}
In this section, we prove that any distributed algorithm for -testing bipartiteness or cycle-freeness in
bounded-degree graphs requires  rounds of communication\footnote{Our lower bound applies even to the less restricted LOCAL model of communication, which does not limit the size of the messages.}. We
construct bounded-degree graphs that are -far from being bipartite, such that all cycles
are of length .  We argue that any distributed algorithm that runs in  rounds does not detect a witness for non-bipartiteness. We also show that the same
construction proves that every distributed algorithm for -testing cycle-freeness
requires  rounds of communication.
Formally, we prove the following theorem.
\begin{theorem}
\ThmLBBi
  \label{thm:bipart-lower-bound}
\end{theorem}

To prove Theorem~\ref{thm:bipart-lower-bound}, we show the existence of a graph  that is far from being bipartite, but all of its cycles are at least of logarithmic length. Since in  rounds of a distributed algorithm, the output of every vertex cannot depend on vertices that are at distance greater than  from it, no vertex can detect a cycle in  in less than  rounds, which proves Theorem~\ref{thm:bipart-lower-bound}. To prove the existence of  we use the probabilistic method with alterations, and prove the following.
\begin{lemma}
  Let  be a random graph on  vertices where each edge is present with probability .
  Let  be obtained by removing all edges incident with vertices of degree greater than , and
  one edge from each cycle of length at most . Then with probability at least
  ,  is -far from being bipartite.
  \label{lem:graph-far}
\end{lemma}

Since a graph that is -far from being bipartite is also -far from being cycle-free, we immediately obtain the same lower bound for testing cycle-freeness, as follows.

\begin{theorem}
\ThmLBCycle
  \label{cor:cycle-lower-bound}
\end{theorem}


The rest of this section is devoted to proving Lemma~\ref{lem:graph-far}. We need to show three properties of : (a) that it is far from being bipartite, (b) that it does not have small cycles, and (c) that its maximum degree is bounded. We begin with the following definition, which is similar in spirit to being far from satisfying a property and which will assist us in our proof.
\begin{definition}
 A graph  is -removed from being bipartite if at least  edges have to be removed from  to
 make it bipartite.
 \label{def:k-removed}
\end{definition}

Note that a graph  with maximum degree , is -far from being bipartite if it is
-removed from being bipartite.

Let  be a random graph on  vertices where for each pair of vertices, an edge is present with
probability . The expected number of edges in the graph is . Since the edges are
sampled independently with probability , by the Chernoff Bound from
Fact~\ref{fact:chernoff}, with probability at least  the graph has at least 
edges.  We now show that  is far from being bipartite, with high probability.
\begin{lemma}[\textbf{far from being bipartite}]
  With probability at least ,  is -far from being bipartite.
  \label{lem:farness}
\end{lemma}
\begin{proof}
  Fix a bipartition  of the vertex set of  such that .  For each pair of
  vertices , let  be a random variable which is  if the edge  is
  present in  and  otherwise. Its expected value is . The random variable
   counts the number of edges within . By the linearity of expectation, . Since the random variables  are
  independent, by the Chernoff Bound from Fact~\ref{fact:chernoff}, we have that . Therefore,
  with probability at least , there are at least  edges within . The total
  number of such bipartitions of  is at most .
  Taking a union bound over all such bipartitions, the probability that at least one of the
  bipartitions contains less than  edges within its  side is at most , and the lemma
  follows.
\end{proof}

The expected degree of a vertex  in  is . Therefore, by the Chernoff Bound from Fact~\ref{fact:chernoff}, the
probability that the degree of  is greater than  is at most .  We now
show that, with sufficiently high probability, the number of edges that are incident with high
degree vertices is small. We can remove all such edges to obtain a bounded-degree graph that is
still far from being bipartite.

\begin{lemma}[\textbf{mostly bounded degrees}]
  With probability at least , there are at most  edges that are incident with
  vertices of degree greater than  in .
  \label{lem:edges-high-degre}
\end{lemma}
\begin{proof}
  For a pair  of vertices, the probability that there is an edge between them and that one of
   or  is of degree greater than  is . This is at most .
  Therefore, the expected number of edges that are incident with a vertex of degree greater
  than  is at most .  By Markov's inequality, the probability
  that there are at least  edges that are incident with vertices of degree greater than  is at
  most . This completes the proof of the lemma.
\end{proof}

We now bound the number of cycles of length at most  in the graph .
\begin{lemma}[\textbf{few small cycles}]
  With probability at least , there are at most  cycles of length at most  in .
  \label{lem:cycles}
\end{lemma}
\begin{proof}
  For any  fixed vertices, the probability that there is a cycle among the  vertices is at
  most .  Therefore the expected number of cycles in  of length at most  is at
  most . For , this means that the expected number of cycles in  of
  length at most  is . Therefore, with probability at least  there are at
  most  cycles of length at most  in .
\end{proof}



We are now ready to prove Lemma~\ref{lem:graph-far}, completing our lower bounds. Intuitively, since  does not contain many high degree vertices and many small cycles, removing them to obtain  only changes the distance from being bipartite by a small term.
\begin{proof}
  With probability , there are at least  edges in  and by Lemma~\ref{lem:farness}  is -removed from
  being bipartite. By Lemma~\ref{lem:edges-high-degre}, with probability at least ,
  there are at most  edges incident with vertices of degree greater than  and by Lemma~\ref{lem:cycles} with
  probability at least  there are at most  cycles of length at most .
  Hence, with probability at least ,  is a graph with degree at most
   that is -removed from being bipartite.  Therefore,  is -far from being
  bipartite.
\end{proof}




\section{Discussion}
\label{sec:discussion}
This paper initiates a thorough study of distributed property testing. It provides an emulation technique for the dense graph model and constructs fast
distributed algorithms for testing triangle-freeness, cycle-freeness and bipartiteness. We also present lower bounds for both bipartiteness and triangle freeness.   

This work raises many important open questions, the immediate of which is to devise fast distributed testing algorithms for additional problems. One example is testing freeness of other small subgraphs.
More ambitious goals are to handle dynamic graphs, and to find more general connections between testability in the sequential model and the distributed model.
Finally, there is fertile ground for obtaining additional lower bounds in this setting, in order to fully
understand the complexity of distributed property testing.	

\bibliographystyle{plain}
\bibliography{citations}
\end{document}
