\documentclass[11pt]{article}

\usepackage{float}
\usepackage{multicol} \usepackage{graphicx}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{verbatim} \usepackage{geometry} \usepackage{ifpdf}
\usepackage{calc}
\usepackage[labelfont=bf]{caption}

\geometry{tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}

\allowdisplaybreaks[3]      

\setlength{\arraycolsep}{2pt}

\newcommand{\thmabove}{8pt}
\newcommand{\thmbelow}{6pt}
\newcommand{\proofbelow}{8pt}

\newtheoremstyle{mythmstyle}
  {\thmabove}   {\thmbelow}   {\itshape}    {}            {\bfseries}   {. }          {2.5pt}       {\thmname{#1}\thmnumber{ #2}\thmnote{ \normalfont (#3)}}   \theoremstyle{mythmstyle}
\numberwithin{equation}{section}

\newenvironment{alg}{
    \begin{list}{}{
        \setlength{\itemsep}{2pt}
        \setlength{\parsep}{0pt}
        \setlength{\parskip}{0pt}
        \setlength{\topsep}{1pt}
    }
}
{
    \end{list}
}

\floatstyle{ruled}
\newfloat{algorithm}{t}{loa}
\floatname{algorithm}{Algorithm}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem*{definition}{Definition}
\newtheorem*{remark}{Remark}
\newtheorem{claim}[theorem]{Claim}
\renewenvironment{proof}{\noindent\textbf{Proof.}\,}{\afterproof}
\newenvironment{proofof}[1]{\noindent\textbf{Proof} \,(of #1).\,}{\afterproof}
\newenvironment{subproof}{\noindent\textit{Proof.}\,}{\aftersubproof}
\newcommand{\afterproof}{\hfill  \par \vspace{\proofbelow}}
\newcommand{\aftersubproof}{\hfill  \par \vspace{\proofbelow}}

\newcommand{\repeatclaim}[2]{\vspace{6pt}\noindent\textbf{#1. }{\it #2} \vspace{6pt}}

\newcommand{\newterm}[1]{\textit{#1}}
\renewcommand{\th}{\ifmmode{^{\textrm{th}}}\else{\textsuperscript{th}\ }\fi}
\newcommand{\smallfrac}[2]{{\textstyle \frac{#1}{#2}}}

\renewcommand{\check}{\hat}
\renewcommand{\bar}[1]{\overline{#1}}
\newcommand{\TODO}[1]{\textbf{\large TODO: #1}}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}

\newcommand{\bR}{\mathbb{R}}
\newcommand{\bZ}{\mathbb{Z}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cQ}{\mathcal{Q}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\tO}{\tilde{O}}
\newcommand{\eps}{\epsilon}
\newcommand{\intersect}{\cap}
\newcommand{\union}{\cup}
\newcommand{\cond}{\operatorname{cond}}
\newcommand{\polylog}{\operatorname{polylog}}
\newcommand{\poly}{\operatorname{poly}}
\newcommand{\sumstack}[1]{\sum_{\substack{#1}}}
\newcommand{\abs}[1]{\lvert #1 \rvert}
\newcommand{\Abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\card}[1]{\abs{#1}}
\newcommand{\set}[1]{\left \{ #1 \right \}}                     \newcommand{\setst}[2]{\left\{\; #1 \,:\, #2 \;\right\}}        

\newcommand{\prob}[1]{\operatorname{Pr}\left[\,#1\,\right]}               \newcommand{\probg}[2]{\operatorname{Pr}\left[\,#1 \:\mid\: #2\,\right]}  

\newcommand{\expect}[1]{\operatorname{E}\left[\,#1\,\right]}              \newcommand{\expectg}[2]{\operatorname{E}\left[\,#1 \,\mid\, #2\,\right]} 

\newcommand{\AlgorithmName}[1]{\label{alg:#1}}
\newcommand{\Algorithm}[1]{Algorithm~\ref{alg:#1}}
\newcommand{\AppendixName}[1]{\label{app:#1}}
\newcommand{\Appendix}[1]{Appendix~\ref{app:#1}}
\newcommand{\ClaimName}[1]{\label{clm:#1}}
\newcommand{\Claim}[1]{Claim~\ref{clm:#1}}
\newcommand{\CorollaryName}[1]{\label{cor:#1}}
\newcommand{\Corollary}[1]{Corollary~\ref{cor:#1}}
\newcommand{\EquationName}[1]{\label{eq:#1}}
\newcommand{\Equation}[1]{Eq.~\eqref{eq:#1}}
\newcommand{\FigureName}[1]{\label{fig:#1}}
\newcommand{\Figure}[1]{Figure~\ref{fig:#1}}
\newcommand{\LemmaName}[1]{\label{lem:#1}}
\newcommand{\Lemma}[1]{Lemma~\ref{lem:#1}}
\newcommand{\ProblemName}[1]{\label{prob:#1}}
\newcommand{\Problem}[1]{Problem~\ref{prob:#1}}
\newcommand{\PropositionName}[1]{\label{prop:#1}}
\newcommand{\Proposition}[1]{Proposition~\ref{prop:#1}}
\newcommand{\SectionName}[1]{\label{sec:#1}}
\newcommand{\Section}[1]{Section~\ref{sec:#1}}
\newcommand{\TheoremName}[1]{\label{thm:#1}}
\newcommand{\Theorem}[1]{Theorem~\ref{thm:#1}}


\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\title{Graph Sparsification by Edge-Connectivity \\ and Random Spanning Trees}
\author{Wai Shing Fung\footnotemark[1] \qquad Nicholas J. A. Harvey\footnotemark[1]
}
\date{}
\begin{document}

\maketitle
    \renewcommand{\thefootnote}{\fnsymbol{footnote}}
    \footnotetext[1]{
    University of Waterloo, Department of Combinatorics and Optimization. 
    Supported by an NSERC Discovery Grant.
    Email: \texttt{\{wsfung,harvey\}@uwaterloo.ca}.
    }
    \renewcommand{\thefootnote}{\arabic{footnote}}

\begin{abstract}
We present new approaches to constructing graph sparsifiers --- weighted subgraphs
for which every cut has the same value as the original graph, up to a factor of .
Our first approach independently samples each edge  with probability
inversely proportional to the edge-connectivity between  and .
The fact that this approach produces a sparsifier resolves a question
posed by Bencz\'ur and Karger (2002).
Concurrent work of Hariharan and Panigrahi also resolves this question.
Our second approach constructs a sparsifier by forming the union 
of several uniformly random spanning trees.
Both of our approaches produce sparsifiers with  edges.
Our proofs are based on extensions of Karger's contraction algorithm,
which may be of independent interest.
\end{abstract}



\section{Introduction}

Graph sparsification is an important technique in designing efficient graph algorithms.
Different notions of graph sparsifiers have been considered in the literature.
Roughly speaking, given a graph , a sparsifier  of 
is a sparse subgraph of  that approximates  in some measures, 
e.g., pairwise distance, cut values,
or the quadratic form defined by the graph Laplacian.
 may be weighted or not.
Throughout this paper, we let  and .
Since many graph algorithms have running times that depend on ,
if  is dense, then the running time can be improved by replacing  with ,
possibly with some loss in the quality of the solution.

Let us define a \textit{cut sparsifier} to be a weighted subgraph that
approximately preserves the value of every cut to within a multiplicative error of .
The main motivation for cut sparsifiers was to improve the runtime of
approximation algorithms for finding various sorts of cuts;
indeed, they have been used extensively for this purpose \cite{KargerSkel,BKConf,AK,KRV}.
The first cut sparsifier was Karger's \textit{graph skeleton}~\cite{KargerSkelConf,KargerSkel}.
He showed that sampling each edge independently with probability ,
where  is the size of the min cut, gives a sparsifier of size .
Unfortunately, this is of little use when  is small.
The celebrated work of Bencz\'ur and Karger~\cite{BKConf,BK} 
improved on this by using non-uniform sampling,
obtaining a cut sparsifier with only  edges.
Their sparsifier is constructed by randomly sampling every edge with probability
inversely proportional to its \emph{edge strength},
and weighting the sampled edges accordingly.

Spielman and Teng \cite{STConf,ST} define \textit{spectral sparsifiers} --- subgraphs
that approximately preserve the quadratic form of the graph Laplacian.
Such sparsifiers are stronger than the previously mentioned sparsifiers that only preserve cuts.
Spielman and Teng's motivation for studying spectral sparsifiers was to use them as a building
block for algorithms that solve linear systems in near-linear time \cite{STConf,SpielmanSurvey}.
They construct spectral sparsifiers with  edges, for some large constant .
This was improved to  edges by Spielman and Srivastava~\cite{SS},
by independently sampling edges according to their effective resistances.

Spielman and Srivastava conjectured that there exist spectral sparsifiers with
 edges.
Towards that conjecture, Goyal, Rademacher and Vempala \cite{GoyalRV09}
showed that sampling just two random spanning trees gives a cut sparsifier
in bounded-degree graphs and random graphs.
Finally, in a remarkable paper, Batson, Spielman and Srivastava \cite{BSS}
construct spectral sparsifiers with only  edges.

In this paper, we study several questions provoked by this previous work.
\begin{itemize}
\item Bencz\'ur and Karger ask: Does sampling according to
edge connectivity instead of edge strength give a sparsifier?
\item The subgraph produced by Goyal, Rademacher and Vempala is an \textit{unweighted}
subgraph. If we sample random spanning trees and apply weights to the resulting edges,
does this give a better sparsifier?
\item Are there other approaches to achieving sparsifiers with  edges?
\end{itemize}


In this paper, we give a positive answer to the first two questions.
We also give a negative result on using random spanning trees to answer the third question.
In concurrent, independent work, Hariharan and Panigrahi~\cite{HP} also resolve
the first question.


\subsection{Notation}

Before stating our results, we introduce some notation.
For a multigraph  with edge weights 
and a set  of multiedges,
the notation  denotes .
The notation  denotes the total number of copies of all multiedges in .
For any set , we define  to be
the set of all copies of edges in  with exactly one end in .
So the notation  denotes the total weight of the cut .


For an edge , the (local) \newterm{edge connectivity} between  and ,
denoted , is defined to be the minimum weight of a cut that separates  and .
The \newterm{effective conductance} of edge , denoted ,
is the amount of current that flows when 
each edge  is viewed as a resistor of value 
and a unit voltage difference is imposed between  and .
A \newterm{-strong component} of  is a maximal -edge-connected,
vertex-induced subgraph of .
The \newterm{strength} of edge , denoted by , is
the maximum value of  such that a -strong component of  contains both  and .

Informally, all three of ,  and 
measure the connectivity between  and .
The values of  and  are incomparable:
 can be  times larger than  or vice versa.
However  always holds.
For more details, see \Appendix{gapexample}.



\subsection{Our results}


\begin{theorem}
\TheoremName{mainthm}
Let  be a simple, weighted graph with edge weights .
Let  and let  where .
For each edge , let  be a parameter such that .
With high probability,
the graph  produced by \Algorithm{sample} satisfies

Furthermore, with high probability,

\end{theorem}

\begin{algorithm}
\begin{alg}
\item	\textbf{procedure} Sparsify(,\, ,\, )
\item	\textbf{input:} A graph 
        with edge weights ,
        and connectivity estimates 
\item	\textbf{output:} A graph  with edge weights 
\item   For 
    \begin{alg}
    \item    \textit{We refer to this as the  round of sampling}
    \item   For each 
        \begin{alg}
        \item   For 
            \begin{alg}
            \item   With probability , add edge  to 
                    (if it does not already exist) and increase  by 
            \end{alg}
        \end{alg}
    \end{alg}
\item   Return  and 
\end{alg}
\caption{A general algorithm for producing a sparsifier of  by sampling edges.}
\AlgorithmName{sample}
\end{algorithm}

This theorem is proven in Sections \ref{sec:sample_with_connectivity} and \ref{sec:splitting_off_alg}.
The condition that  is not really restrictive because
if  then the theorem is trivial:  is itself a sparsifier with  edges.
The condition that the edge weights are integral is not restrictive either.
If the edge weights are any positive real numbers then they can be approximated
arbitrarily well by rational numbers, and these rationals can be scaled up to integers.
This does not affect the conclusion of \Theorem{mainthm}, as it
does not depend on the magnitude of .

The sampled weight of each copy of edge 
is a binary random variable that takes value 
 with probability  and zero otherwise.
When , this random variable has the highest variance,
and therefore the cuts of  are least concentrated.
So, at least intuitively, the theorem is hardest to prove when ,
and the result for smallest  values will follow as a corollary. 
This intuition is indeed correct, and we obtain several interesting corollaries of \Theorem{mainthm}
by invoking \Algorithm{sample} with different  values.
Proofs are in \Appendix{corollaries}.

\begin{corollary}
\CorollaryName{sample_with_connectivity}
Let . Then \eqref{eq:cutspreserved} holds
and  with high probability.
\end{corollary}

\begin{corollary}
\CorollaryName{sample_with_conductance}
Let .
Then \eqref{eq:cutspreserved} holds
and  with high probability.
\end{corollary}

Spielman and Srivastava \cite{SS} prove a related result:
taking , only  rounds of sampling suffice
for \eqref{eq:cutspreserved} to hold with \emph{constant} probability.
A simple modification of their proof implies \Corollary{sample_with_conductance}.
(See, e.g., Koutis et al.~\cite{KMP}.)
It is unclear whether  rounds suffice
for \eqref{eq:cutspreserved} to hold with high probability.

\begin{corollary}
\CorollaryName{sample_with_strength}
Let .
Then \eqref{eq:cutspreserved} holds
and  with high probability.
\end{corollary}

Bencz\'ur and Karger \cite{BK} prove a stronger result:
taking , only  rounds of sampling suffice
for \eqref{eq:cutspreserved} to hold with high probability.

\vspace{6pt}

An important aspect of our analysis is that we rely only on Chernoff bounds.
In contrast, Spielman and Srivastava \cite{SS} use sophisticated concentration bounds for
matrix-valued random variables.
An advantage of Chernoff bounds is that they are very flexible
and have been generalized in many ways.
This flexibility enables us to prove the following result in \Section{spanning_tree}.


\begin{algorithm}
\begin{alg}
\item	\textbf{procedure} SparsifyByTrees(,\, )
\item	\textbf{input:} A graph 
        with edge weights 
\item	\textbf{output:} A graph  with edge weights 
\item   For each , compute the conductance 
\item   For 
    \begin{alg}
    \item    \textit{We refer to this as the  round of sampling}
    \item   Let  be a uniformly random spanning tree
    \item   For each 
        \begin{alg}
        \item   Add edge  to 
                (if it does not already exist) and increase  by 
        \end{alg}
    \end{alg}
\item   Return  and 
\end{alg}
\caption{An algorithm for producing a sparsifier of  by sampling random spanning trees.}
\AlgorithmName{sampletrees}
\end{algorithm}


\begin{theorem}
\TheoremName{sample_with_trees}
Let  be a graph with edge weights ,
let , and let  where .
With high probability, the graph  produced by \Algorithm{sampletrees} satisfies

Clearly .
\end{theorem}


\paragraph{Counting Small Cuts.}
An important ingredient in the proof of \Theorem{mainthm}
is an extention of Karger's random contraction algorithm
for computing global minimum cuts \cite{KargerContract,KargerStein}.
We describe two variants of this algorithm which introduce the additional ideas 
of splitting off vertices and performing random walks.
The main purpose of these variants is to prove generalizations of Karger's cut-counting
theorem \cite{KargerContract,KargerStein},
which states that the number of cuts of size at most  times the
minimum is less than .
Our generalizations give ``Steiner variants'' of this theorem.
Roughly speaking, we show that, amongst all cuts that separate a certain set of terminals,
the number of size at most  times the minimum is less than .

Since our cut-counting result may be of independent interest,
we state it formally now.

\newcommand{\thmcountcutclass}{
    Let  be a graph and let  be arbitrary.
    Suppose that  for every .
    Then, for every real ,
    }
\begin{theorem}
\TheoremName{count_cut_class}
\thmcountcutclass
\end{theorem}

We discuss this theorem in further detail in \Section{splitting_off_alg}.
For now, let us only mention that this theorem reduces to
Karger's cut-counting theorem
by setting  and setting  to the global minimum cut value.
In this special case, it states that
the number of -minimum cuts is at most .
In concurrent, independent work, Hariharan and Panigrahi \cite{HP}
have also proven \Theorem{count_cut_class}.


\subsection{Algorithms}

In this section we describe several algorithms to efficiently construct sparsifiers.
To make \Algorithm{sample} into a complete algorithm,
the most challenging step is to efficiently compute the  values.
This can be done by computing estimates for either , , or .

\paragraph{Edge Connectivity.}
The simplest approach is to estimate .
Several methods for computing such estimates can be found
in the work of Bencz\'ur and Karger \cite{BK}.
In fact, these methods can be significantly simplified
because they were originally designed for estimating ,
which is more challenging to estimate than .
The following theorems describe how we can combine these methods with \Algorithm{sample}
to efficiently construct sparsifiers.
The resulting algorithms are simple enough for a real-world implementation,
and they have theoretical value too:
they can be used to improve the  running time of 
Bencz\'ur and Karger's sampling algorithm to nearly linear time (cf.~\Theorem{alg3}).

These algorithmic results were also described in the earlier work Hariharan and Panigrahi~\cite{HP}.
In fact, their runtime bounds are slightly better, due to a different method of analysis.

\begin{theorem}
\label{thm:alg1}
Given a graph  and edge weights 
where , 
a sparsifier  of size 
that satisfies \eqref{eq:cutspreserved} can be computed in  time.
If  is simple, the time complexity can be reduced to .
\end{theorem}

\begin{theorem}
\label{thm:alg2}
Given a graph  and edge weights 
where , 
a sparsifier  of size 
that satisfies \eqref{eq:cutspreserved} can be computed in  time.
\end{theorem}

Combining Theorems \ref{thm:alg1} and \ref{thm:alg2}
with Bencz\'ur and Karger's algorithm, we can obtain the following result.

\begin{theorem}
\label{thm:alg3}
Given a graph  and edge weights 
where ,
a sparsifier  of size 
that satisfies \eqref{eq:cutspreserved} can be computed in  time.
\end{theorem}

Proofs of these theorems can be found in \Appendix{SketchAlgorithms}.


\paragraph{Effective Conductance.}
As described above, Spielman and Srivastava~\cite{SS} also
construct sparsifiers by sampling according to the effective conductances.
Moreover, they describe an algorithm to approximate the effective conductances
in  time.
This algorithm can be implemented more efficiently using the recent simplified method of
Koutis, Miller and Peng~\cite{KMP}.
Combining this with \Algorithm{sample}, we can construct a sparsifier
with  edges in  time.


\paragraph{Random Spanning Trees.}
\Algorithm{sampletrees} can also be implemented efficiently,
although we do not know how to do this in nearly linear time.
The best known algorithms for sampling (approximately) uniform spanning trees
run in  time~\cite{CMN,KM}.
Combining this with the method described above for approximating the effective conductances
gives an algorithm to compute sparsifiers with  edges.
The running time of this algorithm is dominated by the time needed to sample
 random spanning trees.
Although this algorithm is not as efficient as those listed above,
the numerous special properties of random spanning trees 
might make it useful in other ways.




\subsection{Limits of sparsification}

In Corollaries \ref{cor:sample_with_connectivity}, \ref{cor:sample_with_conductance}
and \ref{cor:sample_with_strength},
the number of rounds of sampling  cannot be decreased to .
To see this, consider a path of length  --- with probability tending to 
the sampled graph would be disconnected and hence not approximate the original graph.

Sampling random spanning trees overcomes this obstacle since the graph is
connected with probability .
Indeed, Goyal, Rademacher and Vempala \cite{GoyalRV09} show that, for any constant-degree graph,
the unweighted union of just  spanning trees
approximates every cut to within a factor .
In \Section{sp_tree_lb} we prove the following negative result for sampling random spanning trees.

\begin{lemma}
\LemmaName{sp_tree_lb}
For any constant ,
there is a graph such that \Algorithm{sampletrees}
requires  to approximate all cuts within a factor 
with constant probability.
\end{lemma}







\section{Sparsifiers by independent sampling}
\SectionName{sample_with_connectivity}

In this section we prove our main result, \Theorem{mainthm}.
Perhaps the most natural approach would be to analyze the probability of poorly
sampling each cut, then union bound over all cuts.
In \Appendix{partitioning} we explain why this simple approach fails,
why Bencz\'ur and Karger \cite{BK} proposed to decompose the graph and separately analyze the
pieces,
and why their approach does not suffice to prove \Theorem{mainthm}.

Our analysis also involves partitioning the graph, but using a different approach.
In fact, a very similar partitioning was used in an earlier proof of Bencz\'ur and Karger 
\cite[\S 3.2]{BKConf} \cite[\S 9.3.2]{BenczurThesis}.
We will partition the graph into subgraphs,
each consisting of edges with roughly equal values of .
Formally, we partition  into subgraphs with edge sets ,
where

We emphasize that  is defined using , not .

To prove that the weights of all cuts are nearly preserved
(i.e., that \Equation{cutspreserved} holds),
we will use a Chernoff bound to analyze the error contributed to each cut by each subgraph .
A union bound allows us to analyze the probability of large deviation
for all cuts simultaneously.
As in previous work \cite{KargerSkel,BK}, 
the key to making this union bound succeed is to show that most cuts are very large,
so their probability of deviation is very small.
This is achieved by our cut-counting theorem, \Theorem{count_cut_class}.

From this point onwards, to simplify our notation, we will no longer think of  as a weighted
graph, but rather think of it as an unweighted multigraph which has
 parallel copies of each edge .
The main benefit of this change is that the total weight of a cut
can now be written  instead of 
since, for multigraphs, the notation  gives
the total number of copies of all multiedges in .
We hope that this choice of notation makes the following proofs easier to read.


The crucial definition for this paper is as follows.
We say that a non-empty set of edges  is 
\newterm{induced by a cut}  if .
Any such set  is called a \newterm{cut-induced} subset of .
Note that  could be induced by different cuts  and ,
i.e., .
For a cut-induced set , define

So  is the minimum size\footnote{We remind the reader that the notation
 implicitly involves the edge multiplicities.
So, thinking of  as a weighted graph,
 is really the minimum weight of a cut that induces .}
of a cut that induces .
This is an important definition since the amount of error we can allow
when sampling  is naturally constrained by the smallest cut which induces .

We also define a ``normalized'' form of , which is .
Note that  is a lower bound on the size of any cut that intersects ,
because every edge  has .
So we can think of  as a quantity that measures how close 
is to the minimum size of any cut that intersects .
Clearly .

For any set  of edges, the random variable  denotes 
the total weight of all sampled copies of the edges in , over all rounds of sampling.
The main challenge in proving \Theorem{mainthm} is to prove concentration for
all  where  is a cut-induced subset of some .

\subsection{The bad events}
\SectionName{badevents}

Let  be a cut-induced set.
We now define three bad events which indicate that the edges in  were not sampled well.
The first two events are:

The third event is not needed to analyze unweighted graphs
(i.e., if every edge multiplicity is );
it is only needed to deal with arbitrary weights.
The third event is

where  is the function defined by

Note that its derivative is ,
so  is strictly monotonically increasing on , and hence invertible.
Furthermore,  is also strictly monotonically increasing on ,
by the inverse function theorem of calculus.

We will show that,
assuming that these events do not hold (for certain cut-induced sets),
then the weights of all cuts are approximately preserved.
To this end, we bound the probability of the bad events
by the following three claims.
These claims are proven by straightforward applications of Chernoff bounds
in \Appendix{boundproofs}.
Recall that the parameter  in the statement of the claims
satisfies , as stated in \Theorem{mainthm}.


\newcommand{\clmlargelambda}{
    Let  be a cut-induced set with .
    Then
    
}
\begin{claim}
\ClaimName{large_lambda}
\clmlargelambda
\end{claim}

\newcommand{\clmsmalllambda}{
    Let  be a cut-induced set with .
    Then
    
}
\begin{claim}
\ClaimName{small_lambda}
\clmsmalllambda
\end{claim}

\newcommand{\clmerrorub}{
    For every cut-induced set ,
    
}
\begin{claim}
\ClaimName{errorub}
\clmerrorub
\end{claim}

\newcommand{\clmunionbound}{
    By choosing  sufficiently large,
    then with high probability, every cut-induced set  satisfies
    \begin{itemize}
    \item if  then  does not hold;
    \item if  then  does not hold; and
    \item  does not hold.
    \end{itemize}
}
\begin{claim}
\ClaimName{union_bound}
\clmunionbound
\end{claim}

The proof of \Claim{union_bound}, given in \Appendix{boundproofs},
is a straightforward modification of an argument of Karger \cite{KargerSkel}.
The only difference with our proof is that we require a result which
bounds the number of small cut-induced sets.
Such a statement is given by \Corollary{count_canonical_cut},
which follows directly from \Theorem{count_cut_class}.

\begin{corollary}
\CorollaryName{count_canonical_cut}
For each  and any real number , 
the number of cut-induced sets  with 
is less than .
\end{corollary}
\begin{proof}
Since every  satisfies ,
we may apply \Theorem{count_cut_class} with  and .
This yields 

Now, by the definition of  and ,
for every cut-induced set  with ,
there exists a cut  such that 
and .
This proves the desired statement.
\end{proof}

\subsection{All cuts are preserved}
\SectionName{allcuts}

In this section we prove that \Equation{cutspreserved} holds.
Recall that the random variable  denotes total weight of all sampled edges in .
Our main lemma is

\begin{lemma}
\LemmaName{wtderror}
With high probability, every cut  satisfies .
\end{lemma}



For unweighted graphs, the proof of this lemma is quite simple.
For weighted graphs, we require the following three technical claims,
which are proven in \Appendix{calcproof}.


\newcommand{\clmfsmall}{
    Let  be a cut-induced set. Then .
}
\begin{claim}
\ClaimName{Fsmall}
\clmfsmall
\end{claim}


\newcommand{\clmconc}{
    For any integer ,
    
}
\begin{claim}
\ClaimName{concentrate_in_large_Ci}
\clmconc
\end{claim}


\newcommand{\clmginv}{
    Define  by
    
    Then  for all .
}
\begin{claim}
\ClaimName{ginv}
\clmginv
\end{claim}





\vspace{6pt}
\begin{proofof}{\Lemma{wtderror}}
We wish to prove that .
We may assume that the conclusions of \Claim{union_bound} hold,
since they hold with high probability.
We use those facts to bound the error contribution from each cut-induced set
.

To perform the analysis, we partition the 's into three classes,
according to which bad event (,  or )
will be used to analyze the error.
This partitioning depends on the threshold

The sets of indices are

We remark that

since we assume that .

To analyze the error , we expand it as a sum over cut-induced sets.


\paragraph{Unweighted graphs.}
For unweighted graphs, the analysis is simple.
Since any cut satisfies , we have  and so .
We have assumed that the conclusions of \Claim{union_bound} hold,
so the events  and  do not occur
(under the stated conditions on ).
Therefore

This completes the proof for the unweighted case.


\paragraph{Weighted graphs.}
For weighted graphs the analysis is slightly more complicated because
the number of cut-induced sets  that contribute error may be much larger than ,
because  may be non-empty.
To show that the total error is still small, we will need to use the events .

Consider \Equation{errorub} again.
The first two sums were analyzed in \Equation{unwtd},
so it suffices to analyze the third sum.
First we prove a lower bound on this sum:

by \Claim{concentrate_in_large_Ci} and the definition of .

Now we prove an upper bound.
By \Claim{union_bound}, we may assume that the events  do not hold.

The last inequality holds since

The sum in \Equation{harmonic} is a subseries of a harmonic series with at most  terms
(since there are at most  distinct  values)
so the value of this sum is .
Thus we have shown that the third sum in \Equation{errorub} is at most .
\end{proofof}


\subsection{The size of the sparsifier}

To complete the proof of \Theorem{mainthm},
it remains to show that  does not have too many edges,
i.e., \Equation{sizebound} holds.
We have

so the right-hand side of \Equation{sizebound} is at least .
By a simple Chernoff bound, it follows that \Equation{sizebound} holds with high probability.




\section{The cut-counting theorem}
\SectionName{splitting_off_alg}

In this section, we prove \Theorem{count_cut_class}, which is our
generalization of Karger's cut-counting theorem~\cite{KargerContract,KargerStein}.
The proof of Karger's theorem is based on his randomized contraction algorithm for
finding a global minimum cut of a graph.
Roughly speaking he shows that, for any small cut-induced set, it has non-negligible probability
of being output by the algorithm, and hence the number of small cut-induced sets must be small.
We will prove our generalized cut-counting theorem by analyzing a variant of the contraction
algorithm which incorporates the additional idea of splitting off vertices.

The formal statement of our theorem is:

\medskip
\noindent\textbf{\Theorem{count_cut_class}.~}{\it \thmcountcutclass}

This theorem becomes easier to understand by restating it using the terminology of 
\Section{sample_with_connectivity} (cf.~\Corollary{count_canonical_cut}).
A cut-induced subset of  is precisely a set of the form
, so the theorem is counting cut-induced sets
satisfying some condition.
This condition is: for a cut-induced set ,
there must exist  with 
and .
This condition is equivalent to ,
where  is the function defined in \Equation{qdef}.
So the conclusion of \Theorem{count_cut_class} can be restated as





\paragraph{Comparison to Karger's theorem.}
For the sake of comparison, Karger's theorem is:

\begin{theorem}[Karger \protect\cite{KargerContract,KargerStein}]
\TheoremName{karger}
Let  be a connected graph and let  be arbitrary.
Suppose that the value of the global minimum cut is at least .
Then, for every real ,

\end{theorem}

Our theorem improves on Karger's theorem in two ways.
First of all, we count cut-induced sets instead of cuts.
This is clearly more general and, as we mentioned before,
it is useful because it avoids overcounting cut-induced sets that are shared by many cuts.
Secondly, we want to bound the number of ``small'' cut-induced sets in .
The bounds given by both theorem are ,
where  measures how small a cut or a cut-induced set is.
However in our cut-counting lemma,  is 
relative to , the size of a smallest cut that intersects with ,
not relative to the size of a global minimum cut as in Karger's theorem.
This is an improvement since the global minimum cuts may not intersect  at all,
so the global minimum cut value could be much smaller than . 

For concreteness, consider the example in \Figure{example1},
which appears in \Appendix{gapexample}.
Suppose we want to bound the number of cuts that intersect with .
(Here  consists of the single edge .)
Note that all such cuts have size .
However the global minimum cuts all have size , and they do not intersect with .
From \Theorem{karger} we see that there are at most  cuts of size at most 
that intersect .
In contrast, \Theorem{count_cut_class} states that
there are at most  cut-induced subsets of  
that are induced by cuts of size at most .



\paragraph{A weaker theorem based on effective conductance.}
We have also proven a weaker version of \Theorem{count_cut_class} which does not
suffice to prove \Theorem{mainthm} but does suffice to prove
\Corollary{sample_with_conductance}.
This weaker version is stated as \Theorem{count_cut_2};
it is weaker because the hypothesis  is stronger than the hypothesis ,
by \Claim{ck}.

\begin{theorem}
\TheoremName{count_cut_2}
Let  be a graph and let  be arbitrary.
Suppose that  for every .
Then, for every real ,

\end{theorem}

Although this theorem is weaker than \Theorem{count_cut_class},
we feel that it is worth including in this paper because its proof is based on analysis of
random walks that may be of independent interest.
The proof is in \Appendix{random_walk_alg}.




\subsection{The generalized contraction algorithm}

\Theorem{count_cut_class} follows immediately from \Theorem{contract_with_split},
which is the analysis of our generalized contraction algorithm
(\Algorithm{contract}).
Henceforth, we will use the following terminology.
The edges in  are called \newterm{black}.
Also, a cut is \newterm{black} if it contains a black edge,
and a vertex is black if it is incident to a black edge.
An edge, vertex or cut is \newterm{white} if it is not black.



\begin{theorem}
\TheoremName{contract_with_split}
For any cut-induced set  with ,
\Algorithm{contract} outputs  with probability at least .
\end{theorem}

\begin{algorithm}
\begin{alg}
\item	\textbf{procedure} Contract(,\, ,\, )
\item	\textbf{input:} A graph , a set , and an approximation factor 
\item	\textbf{output:} A cut-induced subset of 
\item	While there are more than  vertices remaining
    \begin{alg}
    \item	While there exists a white vertex 
        \begin{alg}
        \item   Perform admissible splitting-off at  until  becomes an isolated vertex
        \item   Remove 
        \end{alg}
    \item	Pick an edge  uniformly at random
    \item	Contract  and remove any self loops
    \end{alg}
\item	Pick a non-empty proper subset  of  uniformly at random
        and output the black edges with exactly one endpoint in 
\end{alg}
\caption{An algorithm for finding a small cut-induced set by splitting off white vertices.}
\AlgorithmName{contract}
\end{algorithm}

\Algorithm{contract} is essentially the same as Karger's contraction algorithm,
except that it maintains the invariant that  has no white vertex by \newterm{splitting-off}.
For a pair of edges  and , splitting-off  and 
is the operation that removes  and  then adds a new edge .
This splitting-off operation is \newterm{admissible} 
if it does not decrease the (local) edge connectivity
between any pair of vertices  and , except of course when one of those vertices is .
Splitting-off has many applications in solving connectivity problems
because of the following theorem.

\begin{theorem}[Mader \protect\cite{Mader}]
\TheoremName{mader}
Let  be a connected graph and  be a vertex.
If  has degree  and  is not incident to any cut edge,
then there is a pair of edges  and  such that
the splitting-off of  and  is admissible.
\end{theorem}


Since \Algorithm{contract} needs to perform admissible splitting-off,
we must ensure that the hypotheses of Mader's theorem are satisfied.
This can be accomplished by the simple trick of duplicating every edge,
which ensures that  is Eulerian and its components are -edge-connected.
Note that these conditions are preserved under
all modifications to the graph performed by \Algorithm{contract},
namely contraction, splitting-off and removal of self loops.


\newcounter{itemctr}
\newenvironment{invariants}{
    \begin{list}{(I\textrm{\arabic{itemctr}}): }{
        \usecounter{itemctr}
        \setlength{\itemindent}{0pt}
        \setlength{\labelwidth}{24pt}
        \setlength{\labelsep}{9pt}
        \setlength{\leftmargin}{\parindent+\labelwidth+\labelsep}
        \setlength{\itemsep}{3pt}
        \setlength{\topsep}{6pt}
        \setlength{\listparindent}{0pt}
    }
}
{
    \end{list}
}

To prove \Theorem{contract_with_split},
we fix a cut-induced set  with .
We will show that, with good probability,
the algorithm maintains the following invariants.
\begin{invariants}
\item  is a cut-induced set in the remaining graph,
\item  (where  now minimizes over cuts in the remaining graph), and
\item every remaining black edge  satisfies .
\end{invariants}

The only modifications to the graph made by \Algorithm{contract}
are splitting-off operations, contraction of edges, and removal of self-loops.
Clearly removing self-loops does not affect the invariants.

\begin{claim}
\ClaimName{splitoff}
The admissible splitting-off operations at  do not affect the invariants.
\end{claim}
\begin{proof}
For (I1), note that splitting-off affects only white edges, and all edges in  are black.
For (I2), note that splitting-off only decreases the size of any cut.
For (I3), the edge connectivity between any two black vertices is unaffected
since the splitting-off is admissible and  is white.
\end{proof}


\begin{claim}
\ClaimName{contract}
Let the number of remaining vertices be .
Assuming that the invariants hold,
they will continue to hold after the contraction operation
with probability at least .
\end{claim}
\begin{proof}
For (I3), note that
any black cut which exists after the contraction
also existed before the contraction,
so the edge connectivity between any two black vertices cannot decrease.

Now, with respect the graph before the contraction,
let  be a minimum cardinality cut that induces ,
i.e., .
We claim that ,
where the probability is over the random choice of  to be contracted.
To see this, note that every remaining vertex  is black, 
so the cut  is a black cut.
By invariant (I3) we have ,
so the number of remaining edges is at least .
Since  is picked uniformly at random,

by (I2).
Let us assume that .
Then  is still induced by  after contracting , so (I1) is preserved. 
Furthermore, (I2) is preserved since .
\end{proof}

The following claim completes the proof of \Theorem{contract_with_split}.
We relegate its proof to \Appendix{contract_alg} as it is the same argument used
to prove Karger's theorem \cite{KargerContract,KargerStein}.
(See also Karger~\cite[App.~A]{KargerSkel}, where a slightly more general result is proven.)

\newcommand{\clmoutput}{
    The probability that \Algorithm{contract} outputs  is at least .
}
\begin{claim}
\ClaimName{output}
\clmoutput
\end{claim}




\subsection{Remarks on cactus representations}

A special case of \Theorem{karger} is that any connected graph  has at most
 (non-trivial) minimum cuts.
(In fact, the theorem actually proves a bound of , which is tight.)
The same fact is implied by a much earlier result of Dinic, Karzanov and Lomonosov~\cite{DKL},
which states that the minimum cuts have a \newterm{cactus representation}.
Fleiner and Frank \cite{FleinerFrank} give a recent exposition of this result.

Dinitz\footnote{E. Dinic and Y. Dinitz are two different transliterations of the same person's name.}
and Vainshtein~\cite{DVConf,DV} generalized this result as follows.
(See also Fleiner and Jord\'an~\cite{FleinerJordan}.)
Let  be a subset of vertices with .
A cut  is called a \newterm{-cut} if 
the partition  of  that it induces
has both parts non-empty.
A -cut  is called minimal if  is minimal amongst all -cuts.
Two minimal -cuts are called equivalent if they induce the same partition of .
Dinitz and Vainshtein showed that the equivalence classes of minimal -cuts have
a cactus representation.
In particular, there are at most  equivalence classes of minimal -cuts.

We now explain how the latter result also follows from \Theorem{count_cut_class}.
Let  be the minimum cardinality of a -cut.
We add dummy edges of weight  between all pairs of -vertices
and let  be the set of dummy edges.
Then every dummy edge  has  and every minimal -cut has weight at most
.
By \Theorem{count_cut_class}, the number of cut-induced sets induced by
cuts of size at most  is at most .
Any two equivalent minimal -cuts induce the same cut-induced subset of ,
so the number of equivalence classes is at most .
Taking  proves that there are at most
 equivalence classes of minimal -cuts.



\section{Sparsifiers by uniform random spanning trees}
\label{sec:spanning_tree}

In this section we describe an alternative approach to constructing a graph sparsifier.
Instead of sampling edges independently at random, as was done in
\Section{sample_with_connectivity},
we will sample edges by picking random spanning trees.
The analysis of this sampling proves \Theorem{sample_with_trees}.
The proof is a small modification of the proof in \Section{sample_with_connectivity},
with some differences to handle the dependence in the sampled edges.
The following two lemmas explain why sampling random spanning trees
is similar to sampling according to effective conductances.

Let us introduce some notation.
For an edge , we denote by  the effective resistance between  and .
This is the inverse of the effective conductance .

\begin{lemma}
\LemmaName{negcor}
Let  be an unweighted simple graph, and let  be a spanning tree in 
chosen uniformly at random.
Let  be distinct edges.
Then

\end{lemma}
\begin{proof}
In the case , Equation~\ref{eq:negcor} was known to Brooks et al.~\cite[Equation (2.34)]{BSST}.
See also Lyons and Peres~\cite[Exercise 4.3]{LyonsPeres}.
For general , this is a consequence of Theorem 4.5 in Lyons and Peres~\cite{LyonsPeres},
which is a result of Feder and Mihail \cite{FederMihail}.
See also Goyal, Rademacher and Vempala~\cite[Section 3]{GoyalRV09}.
\end{proof}

One useful consequence of \Lemma{negcor} is that concentration inequalities
can be proven for the number of edges in  that lie in any given subset.
The concentration is due to the following theorem:

\begin{theorem}
\TheoremName{concentration}
Let  be reals in ,
and let  be -valued random variables.
Suppose that

Suppose .
Then

\end{theorem}
\begin{proof}
See Gandhi et al.~\cite[Theorem 3.1]{Gandhi}.
\end{proof}

We will also use the following corollary.

\begin{corollary}
\CorollaryName{concentration}
Assume the same hypotheses as \Theorem{concentration}.
Let .
Then

\end{corollary}
\begin{proof}
Follows from \Theorem{concentration} and basic calculus.
See also McDiarmid~\cite[Theorem 2.3]{McDiarmid}.
\end{proof}

Now consider the approach of \Algorithm{sampletrees} for constructing a sparsifier.
In each round of sampling, instead of picking edges independently,
we pick a uniformly random spanning tree.
Every edge  in the tree is assigned weight .
This sampling is repeated for  rounds,
and the sparsifier is the sum of these weighted trees.

By \Lemma{kirchoff}, the probability of sampling any particular edge is the same as
when sampling by effective conductances, as was done in \Corollary{sample_with_conductance}.
Furthermore, the same analysis as \Section{sample_with_connectivity} shows that this
sampling method also produces a sparsifier --- the only change to the analysis is that
all uses of Chernoff bounds 
(namely, in Claims \ref{clm:large_lambda}, \ref{clm:small_lambda}
and \ref{clm:errorub})
can be replaced with the concentration bounds in \Theorem{concentration}
and \Corollary{concentration}.
This completes the proof of \Theorem{sample_with_trees}.



\subsection{Lower bound on number of trees}
\SectionName{sp_tree_lb}

In this section, we consider the tradeoff between the number of trees (i.e., the value )
and the quality of sparsification in Theorem~\ref{thm:sample_with_trees}.
We prove a lower bound on the number of trees necessary to produce a sparsifier
with a given approximation factor.

\vspace{6pt}
\begin{proofof}{\Lemma{sp_tree_lb}}
Let  be a graph defined as follows.
Its vertices are .
For every , add  parallel edges
,
and a single length-two path --.
The edges  are called \textit{heavy},
and the edges  and  are called \textit{light}.
Note that the heavy edges each have effective conductance exactly .
The light edges each have effective conductance exactly .

A uniform random spanning tree in this graph can be constructed by repeating
the following experiment independently for each .
With probability , add a uniformly selected heavy edge 
to the tree, and add a uniformly selected light edge  or  to the tree.
In this case we say that the tree is ``heavy in position ''.
Otherwise, with probability , add both light edges  and  to the tree
but no heavy edges.
In this case we say that the tree is ``light in position ''.

Our sampling procedure produces a sparsifier that is the union of  trees,
where every edge  in the sparsifier is assigned weight .
Suppose there is an  such that all sampled trees are light in position .
Then the cut defined by vertices 
has weight exactly  in the sparsifier,
whereas the true value of the cut is .

The probability that at least one tree is heavy in position  is .
The probability that there exists an  such that every tree is light in position  is

Suppose .
Then .
So with constant probability, there is an  such that every tree is light in position ,
and so the sparsifier does not approximate the original graph better than a factor
.
\end{proofof}








\bibliography{Sparsifiers}
\bibliographystyle{plain}
\appendix



\section{Discussion of ,  and }
\AppendixName{gapexample}

As mentioned in the introduction,
the three quantities of an edge  that we consider
(edge connectivity, effective conductance and edge strength)
all roughly measure the connectivity between  and .
However their values can differ significantly.
In this section, we illustrate this with some examples.

Consider a graph which consists of exactly one edge .
To increase  by ,
we can simply add  edge disjoint paths between  and .
In the following examples, we can see that no matter how large  is,
it is possible that  or  increases only by one
while the other increases by .

\begin{figure}
	\centering
    \includegraphics[width=2.5in,clip]{Fig2.eps}
	\caption{Example showing that conductance can be  times larger than strength}
	\FigureName{example1}
\end{figure}

\begin{itemize}
\item
In \Figure{example1},
 and  are connected by an edge 
and  paths of length .
Clearly , 
.
But  as every induced subgraph with at least two vertices is at most  edge connected.

\begin{figure}
	\centering
    \includegraphics[width=2.5in,clip]{Fig1.eps}
	\caption{Example showing that strength can be  times larger than conductance}
	\label{example2}
\end{figure}

\item
In Figure \ref{example2},
 and  are connected by an edge 
and a path of length 
which consists of edges of weight .
The graph is -edge-connected so
 but .
\end{itemize}

Although  and  are incomparable, they are upper bounded by .

\begin{claim}
\ClaimName{ck}
For any edge , .
\end{claim}
\begin{proof}
It is immediate from the definition of edge strength that ,
so we focus on the effective conductance.
Since the connectivity between  and  is ,
there is a cut of size  separating  and .
Contracting both sides of the cut, 
we get two new vertices  and .
By Rayleigh monotonicity~\cite{DoyleSnell},  is at least .
Clearly , so the proof is complete.
\end{proof}



\section{Corollaries of \Theorem{mainthm}}
\AppendixName{corollaries}

First we show that our corollaries satisfy the hypotheses of \Theorem{mainthm}.
By \Claim{ck}, Corollaries \ref{cor:sample_with_connectivity}, \ref{cor:sample_with_conductance}
and \ref{cor:sample_with_strength} all have ,
so \Theorem{mainthm} is applicable.

It remains to analyze , the number of sampled edges.
For Corollaries \ref{cor:sample_with_connectivity} and \ref{cor:sample_with_strength}
we use a property of edge strength proved by Bencz\'ur and Karger \cite[Lemma 2.7]{BK}. 

\begin{lemma}
\LemmaName{sample_size}
In a multigraph with edge strengths , we have

Here the sum is over all copies of the multiedges.
\end{lemma}

Thus, for Corollaries \ref{cor:sample_with_connectivity} and \ref{cor:sample_with_strength}, we have


Finally, we must bound the size of  in \Corollary{sample_with_conductance}.
We require the following lemma.

\begin{lemma}
\LemmaName{kirchoff}
Let  be a multigraph, and let  be a spanning tree in 
chosen uniformly at random.
Then, for any copy of an edge , .
\end{lemma}
\begin{proof}
See Kirchhoff~\cite{Kirchhoff},
Brooks et al.~\cite[pp.~318]{BSST},
Lov\'asz~\cite[Theorem 4.1(i) and Corollary 4.2]{LovaszSurvey} and
Lyons and Peres~\cite[Corollary 4.4]{LyonsPeres}.
\end{proof}

This immediately implies that

This fact is known as Foster's theorem,
and it is independently due to Foster~\cite{Foster} and Tetali~\cite{Tetali}.
Thus,




\section{Motivation for Partitioning Edges}
\AppendixName{partitioning}

The natural first approach to proving \Theorem{mainthm}
would be to bound the probability of large deviation for each cut
and then union bound over all cuts.
This approach is not feasible, as can be illustrated using the example in \Figure{example3}.

\begin{figure}
	\centering
    \includegraphics[width=2.5in,clip]{Fig3.eps}
	\caption{The cut-induced set consisting of  is overcounted  times if we simply union
    bound over all cuts.}
	\FigureName{example3}
\end{figure}

In this graph,  and  are connected by  parallel edges
and  paths of length .
Recall that in our sampling scheme 
each copy of  is sampled with probability 
for  rounds and is assigned a weight of  if sampled.
Each edge other than  is sampled with probability 
for  rounds and is assigned a weight of  if sampled.
Consider a set  that contains .
Then  is at most .
Suppose we want to bound the probability that
the sampled weight  exceeds .
For this to happen, at most  copies of 
can be included in the sparsifier.
By a Chernoff bound, this many edges are included 
with probability at most .
However, there are  such 's,
which is too many for such a union bound to work.

The reason this union bound fails
is that the event ``more than  copies of  are included''
is overcounted  times, once for each .
However since all 's share the same  copies of ,
we actually only need to analyze this event once.

Bencz\'ur and Karger \cite{BK} accomplished this by decomposing the graph.
Assume that the edges  are sorted by increasing edge strengths.
Each  contains all edges  with .
Then  can be viewed as the sum of 's,
with each  scaled by .
An important property of this decomposition is that
if  is in  
then the strength of  in  is the same as
its in the original graph . 
This is because the -strong component  in  that contains 
must also be present in , as all edges in  have strengths at least .

Therefore, even though edges in 
have small sampling probabilities (at most ), 
the expected number of sampled edges in every cut is 
at least ,
since the min cut of  is large (at least ).
Thus Karger's graph skeleton analysis is applicable to sampling in .
Roughly speaking, in order to use the Chernoff bound 
to obtain a constant factor approximation
in the number of sampled edges in a cut
with a failure probability of ,
the expected number of sampled edges in the cut needs to be .

To prove \Theorem{mainthm}, we could attempt to use the same decomposition to 
analyze our sampling scheme where edge connectivity is used instead of strength.
The problem is that in general
edge connectivity is not preserved under such decomposition.
To see this, consider the example in \Figure{example1}.
Observe that the subgraph induced by those edges 
with connectivities at least  
consists of only one edge , so this subgraph has min cut value .
The expected number of copies of  in the sparsifier is ,
so we cannot expect to say that sampling preserves every cut of this subgraph
to within .



\section{Proofs for \Section{badevents}}
\AppendixName{boundproofs}

In this section we prove \Claim{large_lambda}, \Claim{small_lambda} and \Claim{errorub}.
We require the following three versions of the Chernoff bound.
For the case , these can be found in the survey of McDiarmid~\cite{McDiarmid};
the case of larger  reduces to that case by scaling.

\begin{theorem}
\TheoremName{chernoff_3}
Let  be independent random variables with values in .
Let  be scalars in .
Let  be a weighted sum of Bernoulli trials defined by
, and let .
Then for any , we have

\end{theorem}


\begin{corollary}
\CorollaryName{chernoff_1}
Let  and  be as in \Theorem{chernoff_3}.
Then for any , we have

\end{corollary}

\begin{theorem}
\TheoremName{chernoff_4}
Let  and  be as in \Theorem{chernoff_3}.
Then for any , we have

where  is the function defined in \Equation{gdef}.
\end{theorem}


\repeatclaim{\Claim{large_lambda}}{\clmlargelambda}

\begin{proof}
Let  and .
By the definition of our sampling process,
 is a weighted sum of Bernoulli trials where each weight is less than .
Thus

This concludes the proof.
\end{proof}

\repeatclaim{\Claim{small_lambda}}{\clmsmalllambda}

\begin{proof}
Let , 
and .
Then  is a weighted sum of Bernoulli trials, and  is an upper bound on the weights.
Note that  and .
Thus

Also, 

Thus

This concludes the proof.
\end{proof}


\repeatclaim{\Claim{errorub}}{\clmerrorub}

\begin{proof}
Let  and .
Then

This completes the proof.
\end{proof}


\repeatclaim{\Claim{union_bound}}{\clmunionbound}

\begin{proof}
Fix an  and let  be all the cut-induced subsets of ,
ordered such that .
Let

By Claims \ref{clm:large_lambda}, \ref{clm:small_lambda} and \ref{clm:errorub},
there exists a value  such that 

We consider the first  cut-induced sets.
Note that for all , .
Therefore, a union bound shows that
the probability that any bad event happens for some  with 
is at most .

Now we consider the remaining cut-induced sets  for .
\Corollary{count_canonical_cut} states that, for any ,

Letting  be such that ,
we see that .
Thus, from \Equation{p_j} we have

Thus 

This completes the proof.
\end{proof}


\section{Proofs for \Section{allcuts}}
\AppendixName{calcproof}

\repeatclaim{\Claim{Fsmall}}{\clmfsmall}

\begin{proof}
Since , every  satisfies .
Since  for every , we obtain .
Thus .
This proves the claim.
\end{proof}


\repeatclaim{\Claim{concentrate_in_large_Ci}}{\clmconc}

\begin{proof}
By Claim~\ref{clm:Fsmall},

This completes the proof.
\end{proof}


\repeatclaim{\Claim{ginv}}{\clmginv}

The purpose of this claim is to give a simple, asymptotically tight upper bound on .
We thank ``mathphysicist'' from the web site MathOverflow
for pointing out that a precise expression for  can be given using the Lambert  function.
Specifically, one can show that

Unfortunately this exact expression is not terribly useful,
since we do not know of any simple, asymptotically tight bounds on .

\vspace{6pt}
\begin{proofof}{\Claim{ginv}}
For all , we have

Next, for ,

Substituting  into this and applying \Equation{ginv1} yields

As observed above,  is strictly monotonically increasing.
Thus  implies that .
\end{proofof}



\section{Probability of success in \Algorithm{contract}}
\AppendixName{contract_alg}

In this appendix we complete the proof of \Theorem{contract_with_split}
by proving the following claim.

\repeatclaim{\Claim{output}}{\clmoutput}

\begin{proof}
Define .
In the last iteration of the algorithm, the number of remaining vertices is at least .
The probability that the invariants hold at the end of the algorithm
is at least the product of the probabilities that the 
invariants are not violated at any step of the algorithm.
By Claims \ref{clm:splitoff} and \ref{clm:contract}, this probability is at least

where the factorial function is extended to arbitrary real numbers via the Gamma function.

Since there are at most  remaining vertices at the end of the algorithm
there are less than  remaining non-trivial cuts,
at least one of which induces , by (I1).
Therefore, the probability that the last step of the algorithm
selects a set  that induces  is at least

where we have used the inequalities ,
,
and  for .
\end{proof}





\section{Random contraction algorithm by random walks}
\AppendixName{random_walk_alg}

In this appendix we present \Algorithm{contractRW},
which is a variant of the contraction algorithm that contracts
random walks instead of random edges.
We use the similar terminology and notation to \Section{splitting_off_alg},
e.g., black vertices.
The following analysis of the algorithm immediately implies \Theorem{count_cut_2}.

\begin{theorem}
\TheoremName{contract_with_RW}
For any cut-induced set  with ,
\Algorithm{contract} outputs  with probability at least .
\end{theorem}


\begin{algorithm}
\begin{alg}
    \item	\textbf{procedure} ContractRW(,\, ,\, )
    \item	\textbf{input:} A graph , a set ,
            and an approximation factor 
    \item	\textbf{output:} A cut-induced subset of 
	\item	While there are more than  black vertices remaining
	\begin{alg}
        \item	Randomly pick a black vertex  with probability proportional to its degree
\item	Perform a random walk starting from  and
                stopping when it hits a black vertex  (possibly )
        \item   If 
        \begin{alg}
            \item	Contract all edges traversed by the random walk and remove any self loops
        \end{alg}
    \end{alg}
	\item	Pick a non-empty proper subset  of  uniformly at random
	        and output the black edges with exactly one endpoint in 
\end{alg}
\caption{An algorithm for finding a small cut-induced set by contracting random walks.}
\AlgorithmName{contractRW}
\end{algorithm}

The approach for proving \Theorem{contract_with_RW}
is again similar to Karger's analysis of the contraction algorithm
--- for any cut , we can bound the probability that an edge in  is contracted. 
Formally, our analysis is:

\begin{lemma}
\LemmaName{rw}
Consider an iteration of the while loop that begins with  remaining black vertices.
Suppose that no edge in  has been contracted so far.
Suppose that the random walk in this iteration has .
Then

\end{lemma}

To prove \Theorem{contract_with_RW}, one applies \Lemma{rw}
where  is a cut which induces  and satisfies .
The remainder of the proof follows by the same argument as \Claim{output}.

The key method in proving \Lemma{rw} is to understand the probability
that a random walk hits a certain set of vertices before hitting some other set of vertices.
To that end, let us introduce some notation.
For any two sets of vertices  and , 
let  denote the effective conductance between  and .
Equivalently, identify  into a single vertex , identify  into a single vertex ,
and let  be the effective conductance between  and .

Next, suppose that  and that  and  are disjoint subsets of .
We use  to denote the event that 
a random walk starting at  hits  before it hits .
If , we use the shorthand ,
and similarly if .
In the case that , the term ``hits'' means
``hits after performing at least one step of the random walk''.

\begin{remark}
The event  can also be understood in another way.
Let  be the graph obtained by identifying all nodes in  into a single node ,
and identifying all nodes in  into a single node .
Then  equals the probability that a random walk in 
starting at  hits  before .
\end{remark}

The main tool in the proof of \Lemma{rw} is the following reciprocity law.
It will allow us to consider random walks originating at the cut  rather
than random walks that cross .

\begin{lemma}
\LemmaName{switch}
Let  and .
Assume that  and .
Then

\end{lemma}

To prove this lemma, we need to understand the relationship between the following events:

In English,  is the event
that the random walk hits  before hitting  or returning to ,
 is the event that the random walk hits  or  before returning to ,
and  is the same as  except that
the random walk is permitted to return to  before hitting  or .

\begin{claim}
\ClaimName{split_xi}
.
\end{claim}

\begin{claim}
\ClaimName{indep_xi}
 and  are independent.
\end{claim}
\begin{proof}
The claim essentially follows from the ``craps principle''.
In more detail, consider any random walk starting  and ending at .
It can be viewed as a sequence  of random walks where
\begin{itemize}
\item for each ,  starts and ends at  but otherwise does not traverse ,
\item  starts at  and ends at  but otherwise does not traverse .
\end{itemize}
So  is the probability that  ends at ,
and by the Markov property, this equals

Thus we have argued that , as required.
\end{proof}


\begin{claim}
\ClaimName{split_xi_2}
.
\end{claim}
\begin{proof}
Doyle and Snell~\cite[\S 1.3.4]{DoyleSnell}.
\end{proof}

\vspace{12pt}

\begin{proofof}{\Lemma{switch}}
It is known \cite[Theorem IX.22]{bollobas} that

By \Claim{split_xi} and \Claim{indep_xi}, this is equivalent to

By \Claim{split_xi_2}, this is equivalent to \eqref{eq:recip}.
\end{proofof}

\vspace{12pt}

\begin{proofof}{\Lemma{rw}}
It is more convenient to consider hitting a vertex than a cut,
so we subdivide every edge in  and merge the subdividing vertices into a new vertex .
We consider the random walk in the modified graph induced by the random walk in the
original graph.
The former walk hits  iff the latter walk intersects . 

For the remainder of this proof,  denotes the set of currently remaining black vertices.

\begin{claim}
\ClaimName{uw}
For any ,

\end{claim}
\begin{subproof}
Let .
Then

This proves the claim.
\end{subproof}

\begin{claim}
\ClaimName{given}
Let  and  be disjoint events. Let  be another event.

\end{claim}

\begin{claim}
\ClaimName{mess}
For any ,

\end{claim}
\begin{subproof}
Define

Since , the hypotheses of Claim~\ref{clm:given} are satisfied,
and therefore 

By \Claim{indep_xi}, the latter quantity equals
.
\end{subproof}


\vspace{12pt}

Now, we analyze the probability that the random walk hits the cut.
We condition on the event , since that is the only case when
the algorithm contracts edges.

The last inequality is because the node  has degree ,
and every node  is connected to some node 
by a black edge, and .
\end{proofof}




\section{Algorithms for constructing sparsifiers}
\AppendixName{SketchAlgorithms}

In this section,
we sketch the algorithms as stated in Theorems \ref{thm:alg1}, \ref{thm:alg2} and \ref{thm:alg3}.
They are simple modifications of the algorithms of Bencz\'ur and Karger \cite{BK}.
The main difference is that Bencz\'ur and Karger's algorithms compute approximate edge strengths
whereas our modifications compute approximate edge connectivities.
The sparsifiers we obtain have slightly larger size but the algorithms are simpler and more efficient
because approximating the edge connectivities is quite a bit easier
than approximate the edge strengths.
Our algorithms can be easily implemented, and furthermore they can be used as a preprocessing step
for computing smaller sparsifiers.
The proofs of correctness of these algorithms are almost exactly the
same as the proofs in \cite{BK}.


\subsection{Finding -size sparsifiers for graphs with polynomial weights}

We now present an algorithm that computes a sparsifier of size .
It runs in  time for unweighted graphs and 
 time for graphs with polynomial weights.
Recall that, in \Theorem{mainthm},
it is sufficient to find  
that is a lower bound of the edge connectivity .

The main tool that we use is the \emph{-certificate}
introduced by Nagamochi and Ibaraki \cite{NIForest1}.
Given a multigraph , they partition  into a set of forests
 in the follwing way.
Let  be a maximal forest of 
and for , let  be a maximal forest of the subgraph .
Each  is called a \emph{NI-forest}.
Nagamochi and Ibaraki showed that for any integer ,
,
the union of the first  NI-forests, preserves all cuts of 
that have size at most .
Thus  contains all -light edges of 
(an edge  is -heavy if  and -light otherwise).
 is called a -certificate.
Clearly, it contains at most  edges.

Nagamochi and Ibaraki \cite{NIForest1} presented an algorithm for labeling 
every edge  with a label ,
such that if an edge  has multiplicity , the  copies of 
are contained in the  NI-forests .
For simple graphs, it runs in  time.
For multigraphs, a slightly modified version \cite{NIForest2} of this algorithm runs in
 time.

Let  be an edge.
Note that if  appears in , 
then  and  must be connected in  for every ,
for otherwise  can be added to ,
which contradicts the maximality of .
Therefore,  is a lower bound of 
and we can set .
Suppose we sample with probability  as described in \Theorem{mainthm}.
Then with high probability, this will produce a sparsifier that
preserves every cut to within a  factor.
Since we assume the weights are polynomially bounded,
the expected number of edges per round is 

Therefore with high probability,
the sparsifier contains  edges.

Using the Nagamochi-Ibaraki Certificate algorithm,
all  can be found in  time.
For each edge , we can decide in expected constant time 
whether at least one copy of  is included in the sparsifier.
For an edge that has at least one copy in the sparsifier,
we can find the sum of weights of all copies of it 
by sampling from the distribution Binomial(, )
instead of sampling  Bernoulli random variables per round.
This sampling is easy to do in
 time,
for each edge included in the sparsifier.
We suspect that this can be improved using the technique of Knuth and Yao
\cite{KnuthYao} \cite[Ch.~15]{Devroye},
but leave the details to future work.
Therefore, the total running time is  for graphs with polynomial weights.

For unweighted  graphs, we can reduce this to  time.
Note that for unweighted graphs,  for all .
If an edge  has ,
instead of performing  rounds of sampling,
we can include  with probability  and assign it a weight of .
Thus we can sample the weight of an edge in expected constant time.
This change can only decrease the expected size of the sparsifier
and the sampled weight of every cut can only be more concentrated around its mean.

We remark that recent work of Hariharan and Panigrahi \cite{HP}
analyzes the same algorithm and shows that 
actually setting  is sufficient,
whereas we set .
Therefore the size of their sparsfier is only .


\subsection{Finding -size sparsifiers for graphs with polynomial weights}

In this section, we describe another algorithm for computing 
which has the advantage that the computed 's
satisfy . 
By the last statement of \Theorem{mainthm},
the size of the sparsifier would be .
The algorithm, given in \Algorithm{connest},
is a slight variation of the Estimation algorithm in \cite{BK}.

\begin{algorithm}
\begin{alg}
	\item	\textbf{procedure} ConnectivityEstimation(,)
	\item	\textbf{input:} subgraph  of 
	\item	 Partition(,)
	\item	for each 
        \begin{alg}
        \item	
        \end{alg}
	\item	for each nontrivial connected component 
        \begin{alg}
    	\item	ConnectivityEstimation(,)
        \end{alg}
\end{alg}
\caption{Algorithm for estimating edge connectivities.}
\AlgorithmName{connest}
\end{algorithm}

The algorithm is based on finding -partitions.
A \emph{-partition} of a graph 
is a set  of edges that includes all -light edges
such that  if  has  components.
A -partition is a ``sparser'' version of -certificate
as a -certificate can have  edges.

\begin{lemma}[Bencz\'ur and Karger \cite{BK}]
\label{alg:partition}
There is an algorithm Partition that outputs 
a -partition in  time for unweighted graphs
and  time for graphs with arbitrary weights.
\end{lemma}

We compute the 's by using the ConnectivityEstimation procedure below.
It is almost the same as the Estimation procedure in \cite{BK},
which is for finding lower bounds of edge strengths.
The only difference is that they call a WeakEdges procedure
to find the -weak edges
instead of calling Partition to find -light edges.

\begin{lemma}
After a call to ConnectivityEstimation(,),
all the labels  satisfy .
\end{lemma}
\begin{proof}
The proof is the same as the proof of Corollary 4.8 in \cite{BK}.
\end{proof}

\begin{lemma}
\label{lem:sumKappa}
The values  output by ConnectivityEstimation satisfy .
\end{lemma}
\begin{proof}
The proof is the same as the proof of Lemma 4.9 in \cite{BK}.
\end{proof} 

\begin{lemma}
ConnectivityEstimation runs in  time on a graph with polynomial weights.
\end{lemma}
\begin{proof}
For a graph with polynomial weights, 
the maximum connectivity is bounded by some fixed polynomial,
so there are at most  levels of recursion.
The total number of edges in all the input graphs to Partition at each level of recursion
is at most .
By Lemma \ref{alg:partition}, each level of computation takes  time. 
\end{proof}

Suppose we sample as described in \Theorem{mainthm}.
By \Theorem{mainthm} and Lemma \ref{lem:sumKappa},
the size of the sparsifier is .
The time needed for finding 's is 
and the time needed for finding the weight of every edge is .
Therefore, the total running time is .


\subsection{Finding -size sparsifiers for graphs with polynomial weights}

In \cite{BK}, Bencz\'ur and Karger presented an algorithm that finds a sparsifier of size 
 for graphs with arbitrary weights in  time. 
This can be combined with the algorithms in the last two sections
to prove Theorem \ref{thm:alg3}.

Suppose we are given a graph .
First we apply Theorem \ref{thm:alg1} to find a sparsifier 
which approximately preserves all cuts of  to within a multiplicative error of .
 has size  and
this takes  time. 
Then we apply Theorem \ref{thm:alg2} to find a sparsifier 
which is a -approximation of .
 has size 
and this takes  time.
Finally, we use Bencz\'ur and Karger's algorithm to 
obtain a sparsifier  that is a -approximation of .
 has size  and
this takes  time. 

Note that  approximately preserves all cuts of  
to within a multiplicative error of .
The total running time is .



\end{document}
