
\documentclass{article} \usepackage{iclr2020_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\usepackage{hyperref}
\usepackage{url}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{xparse}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{xspace}
\usepackage{relsize}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{comment}
\usepackage{tabu}
\usepackage{appendix}
\usepackage{threeparttable}
\usepackage{adjustbox}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{subcaption}
\usepackage{wrapfig}


\title{LAMOL: LAnguage MOdeling for \\
Lifelong Language Learning}



\author{Fan-Keng Sun\thanks{Equal contribution.} \, \thanks{Work done while at National Taiwan University.} \\
MIT \\
Cambridge, MA, USA \\
\texttt{fankeng@mit.edu} \\
\And
Cheng-Hao Ho\footnotemark[1] \\
National Taiwan University \\
Taipei, Taiwan \\
\texttt{jojotenya@gmail.com}
\And
Hung-Yi Lee \\
National Taiwan University \\
Taipei, Taiwan \\
\texttt{hungyilee@ntu.edu.tw}
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
Most research on lifelong learning applies to images or games, but not language.
We present LAMOL, a simple yet effective method for lifelong language learning (LLL) based on language modeling.
LAMOL replays pseudo-samples of previous tasks while requiring no extra memory or model capacity.
Specifically, LAMOL is a language model that simultaneously learns to solve the tasks and generate training samples.
When the model is trained for a new task, it generates pseudo-samples of previous tasks for training alongside data for the new task.
The results show that LAMOL prevents catastrophic forgetting without any sign of intransigence and can perform five very different language tasks sequentially with only one model. 
Overall, LAMOL outperforms previous methods by a considerable margin and is only 2--3\% worse than multitasking, which is usually considered the LLL upper bound.
The source code is available at \url{https://github.com/jojotenya/LAMOL}.
\end{abstract}

\section{Introduction}\label{sec:introduction}

The current dominant paradigm for machine learning is to run an algorithm on a given dataset to produce a trained model specifically for a particular purpose; this is \emph{isolated learning}~\citep[p.~150]{lll_book}. 
In isolated learning, the model is unable to retain and accumulate the knowledge it has learned before. 
When a stream of tasks are joined to be trained sequentially, isolated learning faces \emph{catastrophic forgetting}~\citep{mccloskey1989catastrophic} due to a non-stationary data distribution that biases the model (left figure of Figure~\ref{fig:framework}).
In contrast, lifelong learning is designed to address a stream of tasks by accumulating interconnected knowledge between learned tasks and retaining the performance of those tasks.
A human easily achieves lifelong learning, but this is nontrivial for a machine; thus lifelong learning is a vital step toward artificial general intelligence.



In this paper, we focus on lifelong language learning, where a machine achieves lifelong learning on a stream of natural language processing (NLP) tasks.
To the best of our knowledge, lifelong language learning has been studied in only a few instances; 
for sentiment analysis~\citep{chen-etal-2015-lifelong,8101496}, conversational agents~\citep{LLL_chatbot}, word representation learning~\citep{LLL_word}, sentence representation learning~\citep{Liu2019ContinualLF}, text classification, and question answering~\citep{d2019episodic}.
However, in all previous work, the tasks in the stream are essentially the same task but in different domains.
To achieve lifelong language learning on fundamentally different tasks, we propose LAMOL~--- LAnguage MOdeling for Lifelong language learning.

\begin{figure*}[!htp]
    \centering
    \includegraphics[width=0.95\textwidth]{imgs/framework.png}
    \caption{\textbf{Left}: After learning Task 2, the learner has already forgetten how to solve Task 1. This is ``catastrophic forgetting''. 
    \textbf{Middle}: The basic idea of the data-based LLL approach. A generator is learned to generate examples it has seen before. Using the generator, the learner also learns from examples from the previous task to prevent it from forgetting.
    \textbf{Right}: A language model that simultaneously takes on the roles of learner and generator.}
  \label{fig:framework}
\end{figure*}     
It has been shown that many NLP tasks can be considered question answering (QA)~\citep{McCann2018decaNLP}.
Therefore, we address multiple NLP tasks with a single model by training a language model (LM) that generates an answer based on the context and the question.  
Treating QA as language modeling is beneficial because the LM can be pre-trained on a large number of sentences without any labeling~\citep{radford2019language}; however, this does not directly solve the problem of LLL.
If we train an LM on a stream of tasks, catastrophic forgetting still occurs. 
However, as an LM is intrinsically a text generator, we can use it to answer questions while generating pseudo-samples of the previous task to be replayed later.
LAMOL is inspired by the data-based approach for LLL in which a generator learns to generate samples in previous tasks (middle of Figure~\ref{fig:framework})~\citep{shin2017continual,kemker2017fearnet}.
In contrast to previous approaches, LAMOL needs no extra generator (right of Figure~\ref{fig:framework}).
LAMOL is also similar to multitask training, but the model itself generates data from previous tasks instead of using real data.


Our main contributions in this paper are:
\begin{itemize}[leftmargin=*]
    \item We present LAMOL, a simple yet effective method for LLL. Our method has the advantages of no requirements in terms of extra memory or model capacity. We also do not need to know how many tasks to train in advance and can always train on additional tasks when needed.
    \item Experimental results show that our methods outperform baselines and other state-of-the-art methods by a considerable margin and approaches the multitasking upper bound within 2--3\%.
    \item Furthermore, we propose adding task-specific tokens during pseudo-sample generation to evenly split the generated samples among all previous tasks. This extension stabilizes LLL and is particularly useful when training on a large number of tasks.
    \item We analyze how different amounts of pseudo-samples affect the final performance of LAMOL, considering results both with and without the task-specific tokens.
    \item We open-source our code to facilitate further LLL research.
\end{itemize}




\section{Related Work}
Lifelong learning research is based on regularization, architecture, or data.
Here is a brief survey of works in these three categories.

\subsection{Regularization-based methods}

In this approach, a constraint, i.e., a regularization term, is added to minimize deviation from trained weights while updating the weights in a new task.
Most regularization based methods estimate the importance of each parameter and add the importance as a constraint to the loss function.
Elastic weight consolidation (EWC)~\citep{kirkpatrick2017overcoming} calculates a Fisher information matrix to estimate the sensitivity of parameters as importance.
Online EWC~\citep{schwarz2018progress} is a transformed version of EWC.
Instead of tracking the importance of parameters for each task, online EWC simply accumulates the importance of the stream of tasks.
Synaptic intelligence (SI)~\citep{zenke2017continual} assigns importance to each parameter according to its contribution to the change in the total loss.
Memory aware synapses (MAS)~\citep{aljundi2018memory} estimate importance via the gradients of the model outputs. 
In contrast to estimating the importance of weights, incremental moment matching (IMM)~\citep{lee2017overcoming} matches the moment of weights between different tasks.



\subsection{Architecture-based methods}

For this category, the main idea is to assign a dedicated capacity inside a model for each task.
After completing a task, the weights are frozen and may not be changed thereafter.
Some methods allow models to expand, whereas some fix the size but must allocate capacity for tasks at the beginning.
Progressive neural networks~\citep{rusu2016progressive} utilize one column of the neural network per task.
Once a new task is trained, progressive neural networks augment a new column of the neural network for the task while freezing the past trained columns.
Columns that have been frozen are not allowed to change but are connected to the new column to transfer knowledge from old tasks.
Towards Training Recurrent Neural Networks for Lifelong Learning~\citep{sodhani2018training} unifies Gradient episodic memory~\citep{lopez2017gradient} and Net2Net ~\citep{chen2015net2net}.
Using the curriculum-based setting, the model learns the tasks in easy-to-hard order.
The model alleviates the forgetting problem by GEM method, and if it fails to learn the current task and has not been expanded yet, the model will expand to a larger model by the Net2Net approach.

PathNet~\citep{fernando2017pathnet} reuses subsets of a neural network to transfer knowledge between tasks. Unlike progressive neural networks, PathNet does not allow the model to expand. Instead, it builds a huge fixed-size model composed of a neural network and paths between different layers of the neural networks. While training a task, it selects the best combination of neural networks and paths for that particular task.
Similar to progressive neural networks, selected parts are fixed to allow only inference and not training. 
Inspired by network pruning, PackNet~\citep{mallya2018packnet}  prunes and re-trains the network iteratively to pack numerous tasks into a single huge model.



This category has some drawbacks. 
When resources are limited, model expansion is prohibited.
Also, some architecture-based methods require the number of tasks in advance to allocate the capacity for the tasks, which greatly reduces their practicality.



\subsection{Data-based methods}

This method restricts weights through the data distribution of old tasks. One data-based approach keeps a small amount of real samples from old tasks, and the other distills the knowledge from old data and imagines pseudo-data of old tasks later on. While training a new task, the data or pseudo-data is used to prevent weights from greatly deviating from the previous status.

Gradient episodic memory (GEM)~\citep{lopez2017gradient} preserves a subset of real samples from previous tasks.
Utilizing these real samples during optimization helps somewhat to constrain parameter gradients.
Averaged-GEM (A-GEM)~\citep{chaudhry2018efficient} is a more efficient version of GEM which achieves the same or even better performance than the original GEM. Learning without forgetting~\citep{li2017learning} minimizes the alteration of shared parameters by recording the outputs from old task modules on data from the new task before updating. \citet{shin2017continual} and \citet{kemker2017fearnet} encode data from old tasks into a generative model system. The latter imitates the dual-memory system of the human brain, in that the model automatically decides which memory should be consolidated. 
Both methods replay pseudo-data of previous tasks using the generative model during training. 

\citet{d2019episodic} investigates the performance of the episodic memory system on NLP problems. 
It distills the knowledge of previous tasks into episodic memory and replays it afterward.
This work evaluates the method on two streams of tasks: question answering and text classification.\\





\section{LAMOL}\label{sec:our}
A pre-trained LM can generate a coherent sequence of text given a context.  Thus, we propose LAMOL, a method of training a single LM that learns not only to answer the question given the context but also to generate the context, the question, and the answer given a generation token.
That is, in LAMOL, a model plays the role of both LM and QA model.
Hence, answering questions and generating pseudo-old samples can both be done by a single model.
During LLL, these pseudo-old samples are trained with new samples from new tasks to help mitigate catastrophic forgetting.
\begin{figure}[!tp]
    \centering
    \includegraphics[width=0.45\linewidth]{imgs/training_v2.png}
    \caption{\textbf{Upper}: LM learns to answer question given context.
    \textbf{Lower}: LM learns to generate training samples given generation token GEN.}
  \label{fig:training}
\end{figure}
%
 



\subsection{Data formatting} \label{subsec:data_format}




Inspired by the protocol used by decaNLP~\citep{McCann2018decaNLP}, samples from the datasets we used are framed into a SQuAD-like scheme, which consists of context, question, and answer.
Although the LM is simultaneously a QA model, 
the data format depends on the training objective.  When training as a QA model, the LM learns to decode the answer after reading the context and question.
On the other hand, when training as an LM, the LM learns to decode all three parts given a generation token.

In addition to context, question, and answer, we add three special tokens:
\begin{description}
    \item[ANS] Inserted between question and answer. As the context and question are known during inference, decoding starts after inputting ANS.
    \item[EOS] The last token of every example. Decoding stops when EOS is encountered.
    \item[GEN] The first token during pseudo-sample generation. Decoding starts after inputting GEN.
\end{description}
The data formats for QA and LM training are shown in Figure~\ref{fig:training}.





\subsection{Training}
Assume a stream of tasks , where the number of tasks may be unknown.
Directly training the LM on these tasks sequentially results in catastrophic forgetting.
Thus, before beginning training on a new task , the model first generates pseudo samples  
by top- sampling that represent the data distribution 
of previous tasks .
Then, the LM trains on the mixture of  and .
To balance the ratio between  and , the LM generates  pseudo samples, where  denotes the number of samples in task  and  is the sampling ratio.
If the generated sample does not have exactly one ANS in it, then the sample is discarded.
This happens in only 0.5\%-1\% of generated samples.

During training, each sample is formatted into both the QA format and the LM format.
Then, in the same optimization step, both formats are fed into the LM to minimize the QA loss  and LM loss  together. Overall, the loss is , where  is the weight of the LM loss.

















\subsection{Task-specific tokens}
Using the same GEN token for all tasks is problematic when training for many tasks because the portion of old tasks decreases exponentially in theory.
For instance, if , then the portion of the first task when training the second task is about 1\%, but is only about 0.01\% when training the third task.
This issue is definitely harmful to LLL.
To mitigate this, we can choose to replace the GEN token with a task-specific token for each task to inform the model to generate pseudo-samples belonging to the specific task.
Under this setup, all previous tasks have the same share of the  generated pseudo samples.
That is, when beginning training for the -th task , we generate  for the previous  tasks.
Note that as each task uses a specific token, the vocabulary size and the embedding weight of the LM increase slightly as more tasks are trained.








\begin{comment}
\subsubsection{Definition}
We annotate our method as \textbf{}.
 means the \% of current task size we generate.
 corresponds to what kind of generation token we use.  is equivalent to task-specific token while  means the constant generation token .
For all tasks, We simply fit the model on training dataset without any supervision by development set.
\end{comment}

\section{Experiment Setup} \label{sec:expsetup}

\subsection{Tasks, datasets, and metrics} 

We collect five disparate tasks mentioned in decaNLP~\citep{McCann2018decaNLP}: question answering, semantic parsing, sentiment analysis, semantic role labeling, and goal-oriented dialogue, with a dataset for each task.

Furthermore, to compare our method with~\citet{d2019episodic}, we conducted experiments on four text classification tasks: news classification, sentiment analysis, Wikipedia article classification, and question-and-answer categorization with five datasets. We use the procedure from~\citet{d2019episodic} to produce equal-sized datasets.

We do not train on all datasets from both papers due to a lack of computational resources.
For each task, there is a corresponding evaluation metric.
Table~\ref{tab:data_attrs} contains a summary of tasks, datasets, and metrics.
Additional details are provided in Appendix~\ref{appendix:dataset}.
Note that the score of any metric lies between 0 and 100\%.

\begin{table}[!tp]
    \centering
    \scalebox{0.85}{
    \begin{tabular}{l@{\hspace{0.05in}}l@{\hspace{0.03in}}r@{\hspace{0.05in}}r@{\hspace{0.03in}}r}
    \toprule
    Task & Dataset & \# Train & \# Test & Metric \\
    \midrule
    Question answering & SQuAD & 87599 & 10570 & nF1\\
    Semantic parsing & WikiSQL & 56355 & 15878 & lfEM\\
    Sentiment analysis & SST & 6920 & 1821 & EM\\
    Semantic role labeling & QA-SRL & 6414 & 2201 & nF1\\
    Goal-oriented dialogue & WOZ & 2536 & 1646 & dsEM\\
    \midrule
    \multirow{5}{*}{Text classification} & AGNews & \multirow{5}{*}{115000} & \multirow{5}{*}{7600} & \multirow{5}{*}{EM} \\
    & Amazon & & & \\
    & DBPedia& & & \\
    & Yahoo & & \\
    & Yelp & & \\
    \bottomrule
    \end{tabular}
    }
    \caption{Summary of tasks, datasets, dataset sizes, and their corresponding metrics. As this work uses no development set, only the training and test datasets are shown. nF1 is the normalized version of the F1 score; EM represents an exact match between texts: for text classification, this amounts to accuracy; for WOZ, it is equivalent to dfEM (turn-based dialogue state exact match); for WikiSQL, it is equivalent to lfEM (exact match of logical forms).}
    \label{tab:data_attrs}
\end{table}
 \begin{table*}[t]
\centering
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{l*{10}{c}}
    \toprule
    & SQuAD & WikiSQL & SST & SRL & WOZ & AGNews & Amazon & DBPedia & Yahoo & Yelp \\
    \midrule
    GPT-2 score & 72.3 & 70.7 & \textbf{90.9} & 70.4 & \textbf{84.9} & \textbf{94.6} & \textbf{62.3} & \textbf{99.1} & \textbf{73.9} & \textbf{67.7} \\
    Other scores & \textbf{75.5} & \textbf{72.6} & 88.1 &  \textbf{75.2} & 84.4 & 93.8 & 60.1 & 30.5 & 68.6 & 50.7 \\
\bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{Comparison of GPT-2 and other methods on single task scores. Other scores are retrieved from~\cite{McCann2018decaNLP} or~\cite{d2019episodic}. Better performance in boldface.}
\label{tab:single_task}
\end{table*} 
\subsection{Methods to be compared}

All methods use the smallest pre-trained GPT-2 model~\citep{radford2019language}\footnote{https://github.com/huggingface/pytorch-transformers} as the LM.
Each task is trained for nine epochs; greedy decoding is applied during inference.

\begin{itemize}[leftmargin=*]

    \item \textbf{LAMOL} In all experiments,  in top- sampling and  for weight of the LM loss are set.
 denotes LAMOL with a sampling ratio of , and the same GEN token is used for all tasks.
If the task-specific tokens are used, GEN is replaced by TASK.

    \item \textbf{Keep real data} Pseudo-samples are replaced by real samples from previous tasks. 
The quantity of real samples is equally split between previous tasks.
This approach can be considered the upper bound of LAMOL.
We denote it as .

    \item \textbf{Fine-tune}
The model is directly fine-tuned on the stream of tasks, one after another.


    \item \textbf{Multitask learning} All tasks are trained simultaneously.
Multitask learning is often seen as an upper bound of lifelong learning.
In addition, it is also used to determine whether forgetting is caused by a lack of model capacity.

    \item \textbf{Regularization-based methods}
Online EWC~\citep{schwarz2018progress} and MAS~\citep{aljundi2018memory} are compared.
They are chosen because they are more computationally efficient than SI~\citep{zenke2017continual} and more memory efficient than 
IMM~\citep{lee2017overcoming}.
Additionally, experiments such as~\cite{elhoseiny2018exploring} show that MAS has better performance overall.

    \item \textbf{Gradient Episodic Memory (GEM)} When training each task, we randomly sample data from previous task with the amount equivalent to 5\% of the current task size into the memory. In each optimization step, the GEM~\citep{lopez2017gradient} approach retrieves all the data in the memory to calculate the gradients for the previous tasks.

    \item \textbf{Improved memory-based parameter adaptation (MBPA++)} Sparse experience replay and local adaptation for LLL as proposed in~\citet{d2019episodic}. We also re-implement the paper and report better scores using different hyperparameters.

\end{itemize}











\section{Experimental Results}

\subsection{Single Task}
To establish a reference on the capability of the GPT-2 model on every dataset, we trained the model on each dataset independently.
The results are shown in Table~\ref{tab:single_task}.
We observe that the performance of the GPT-2 model is actually quite good, even beating the BERT-based model~\citep{d2019episodic} on text classification datasets by a large margin.
Thus, the GPT-2 model has the potential for superior LLL performance, as long as we can prevent catastrophic forgetting.

\begin{table*}[t]
\centering
\begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{l*{6}c|c|c}
    \toprule
    Methods & {\scriptsize SST SRL WOZ} & {\scriptsize SST WOZ SRL} & {\scriptsize SRL SST WOZ} & {\scriptsize SRL WOZ SST} & {\scriptsize WOZ SST SRL} & {\scriptsize WOZ SRL SST} & Average & Std \\
    \midrule
    Fine-tuned & 50.2 & 24.7 & 62.9 & 31.3 & 32.8 & 33.9 & 39.3 & 12\\
    EWC & 50.6 & 48.4 & 64.7 & 35.5 & 43.9 & 39.0 & 47.0 & 8.7\\
    MAS & 36.5 & 45.3 & 56.6 & 31.0 & 49.7 & 30.8 & 41.6 & 8.9\\
    GEM & 50.4 & 29.8 & 63.3 & 32.6 & 44.1 & 36.3 & 42.8 & 11 \\
     & 46.5 & 36.6 & 56.6 & 38.6 & 44.9 & 45.2 & 44.8 & 6.0\\
     & 79.6 & 78.9 & 73.1 & 73.7 & 68.6 & 75.7 & 74.9 & 3.4\\
     & 80.0 & 80.7 & 79.6 & 78.7 & 78.4 & 80.5 & \textbf{79.7} & 0.8 \\
     & 41.0 & 33.5 & 50.1 & 41.9 & 49.3 & 41.5 & 42.9 & 5.2\\
     & 77.3 & 76.9 & 78.1 & 74.7 & 73.4 & 75.8 & 76.0 & 1.5\\
     & 79.4 & 79.9 & 80.1 & 78.7 & 79.8 & 79.0 & 79.5 & \textbf{0.5}\\
    \midrule
     & 81.0 & 78.9 & 80.1 & 80.9 & 77.7 & 78.0 & 79.4 & 1.2\\
     & 81.8 & 80.6 & 81.6 & 81.2 & 80.4 & 80.5 & 81.0 & 0.5\\
    \midrule
    Multitasked & \multicolumn{7}{c}{81.5} \\
    \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{Summary of averaged metric scores for different methods under permuted task orders using models at last epoch of last task. The Average and Std columns respectively are the average and standard deviation of the averaged scores for each row of the methods. Multitasked learning as an upper bound is shown at the bottom.}
\label{tab:small_perm}
\end{table*}
%
 \begin{table*}[!tp]
\centering
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{*{6}{c}||ccc}
    \toprule
    \small Fine-tuned & \small MAS & \small  & \small  & \small  & \small  & \small  & \small  & \small Multitasked \\
    \midrule
    51.5 & 49.5 & 69.6 & 73.1 & 71.5 & \textbf{74.3} & 74.5 & 76.0 & 76.6 \\
    \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{Summary of averaged score on five tasks. The scores are reported as the averaged score over all tasks of the models after training on every task. The rightmost three columns~-- LAMOL with  and  of real samples from previous tasks and Multitasked~-- are upper bounds for comparison. Best performance in boldface.}
\label{tab:five_tasks}
\end{table*}
 
\subsection{SST, QA-SRL, and WOZ Tasks}
For an initial understanding of the performance on all of the methods and the effect of task order, we first conducted a small-scale experiment on three small datasets: SST, QA-SRL, and WOZ.
We trained all but the the multitasked method on all six permutations of the task order.
The final score for each order was obtained by evaluating the model at the conclusion of the training process.
The results are shown in Table~\ref{tab:small_perm}; we make several observations.
Note that LAMOL with  is not the same as Fine-tuned, as the LM loss is still optimized.
\begin{itemize}[leftmargin=*]
    \item Fine-tuned, EWC, MAS, and LAMOL with  show similar performance and are much worse than LAMOL with .
    \item , our best performing method, is only 1.8 percent away from Multitasked, which implies almost no forgetting during LLL.
    \item The order of the tasks is crucial to the performance.
    For instance, the WOZ score drops significantly after training other tasks.
    Thus, if WOZ is not the last task, the performance is usually noticeably worse.
    \item When using LAMOL, the performance of old tasks maintains almost the same level throughout the training process.
    When the sampling ratio  is increased, the performance also increases, especially when increased from 0 to 0.05.
    \item When , adding task-specific tokens harms performance, because the model must fit additional special tokens that are useless.
    Adding task-specific tokens is also not helpful if . We believe that 0.2 is enough for three tasks; thus task-specific tokens are redundant.
    However, when , task-specific tokens are beneficial because the tokens are needed to help retain a substantial presence of the first task when training the third task.
    \item  We see that a better LLL method usually has a smaller standard deviation, which implies that it is effected less by task order. Adding task-specific tokens also has a stabilizing effect.
\end{itemize}









The complete forgetting progress is illustrated in Appendix~\ref{appendix:small_perm}.
Clearly, Fine-tuned, EWC, MAS, , and  reveal similar patterns.
However, the proposed LAMOL with  displays the ability to retain its learned knowledge.
In the case of 
WOZ  SRL  SST,  the WOZ score even increases after training the third task using LAMOL with .



\begin{comment}
\begin{figure}
   \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\linewidth,height=1.4\linewidth]{imgs/five_tasks.png}
    \caption{Training progress of five tasks. The graph records the performance of the model at each epoch of each task.
The task order is    SQuAD, WikiSQL, SST, QA-SRL and then WOZ.}
    \label{fig:five_tasks}
   \end{subfigure}
   \hfill
   \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\linewidth,height=1.4\linewidth]{imgs/cmp_gen_size.png}
    \caption{Performance after each epoch under five different sampling ratios, with or without task specific-specific tokens. In total, ten combinations are tested on WikiSQL, SST, QA-SRL, and WOZ.}
    \label{fig:cmp_gen_size}
   \end{subfigure}
\end{figure}
\end{comment}

\begin{figure}[!t]
    \begin{minipage}{.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/five_tasks.png}
        \caption{Training progress of five tasks. The graph records the performance of the model at each epoch of each task.}
        \label{fig:five_tasks}
    \end{minipage}\hfill
    \begin{minipage}{.52\textwidth}
        \centering
        \includegraphics[width=\linewidth]{imgs/cmp_gen_size.png}
        \caption{Performance after each epoch under five different sampling ratios, with or without task specific-specific tokens.}
        \label{fig:cmp_gen_size}
    \end{minipage}
\end{figure} 
\subsection{Five decaNLP Tasks}
Here, we train the following five tasks sequentially: SQuAD, WikiSQL, SST, QA-SRL, and WOZ.
Given the limited computing resources, we explore only one task order: from large to small tasks, according to the number of training samples.

As shown in Table~\ref{tab:five_tasks}, LAMOL outperforms all baselines by a large margin and on average approaches within 2--3\% of the multitasked upper bound.
Also, as expected, the performance of LAMOL improves as the sampling ratio  increases and task-specific tokens are used.

There is also a gap between our method and the method of keeping real samples.
As shown in the table, using real samples is much more sample-efficient, as 5\% of real samples beats 20\% of pseudo-samples.
This may be due to the less-than-ideal quality of the pseudo-data.
The longer the paragraphs are, the harder it is for the model to create high-quality samples.
After observing the samples generated when using task-specific tokens, we discover some ``chaos''.
That is, some examples generated by the model do not exactly correspond to the task-specific token.
This implies that the task-specific tokens are sometimes too weak to constrain the model; thus their influence is overshadowed by other tokens.
We believe that solving this problem will bring the performance when using task-specific tokens closer to using real samples; however, we leave this as future work. 

Figure~\ref{fig:five_tasks} illustrates the test scores of each method on each task throughout the training.
We clearly see that when using LAMOL, the model remembers nearly perfectly.

We make several observations:
\begin{itemize}
    \item When training SQuAD, QA-SRL has not been trained yet, but the score of QA-SRL is already around 40.
    Also, when training QA-SRL, the SQuAD score revives if the model has forgotten SQuAD.
    These two facts imply that SQuAD and SRL are similar tasks, such that the model is capable of transferring knowledge from one to the other.
    \item If forward transfer exists, replaying pseudo-data also retains the forward transfer. That is, the QA-SRL score does not drop after training on WikiSQL and SST when LAMOL is used but drops significantly for other methods.
    \item The transferability between SQuAD and QA-SRL is expected. 
On the other hand, the transferability between WikiSQL and QA-SRL is quite surprising; the WikiSQL score improves considerably when training on QA-SRL for Fine-tuned and MAS after WikiSQL is forgotten during SST training. 
\end{itemize}









\subsection{Text classification tasks}
We compared the proposed method against the state-of-the-art MBPA++ proposed in~\citet{d2019episodic}, both by citing their original numbers and also by reproducing their methods.
We chose text classification as opposed to QA because we believe that LM has more of a disadvantage in text classification than in QA.
We compared with  due to its good performance and stability.
Following their paper and testing our model on the same four kinds of task orders, the results are shown in Table~\ref{tab:text_cls}.

Our implementation results in much higher scores than the original ones.
However, the proposed  still outperforms our implementation of MBPA++.







\subsection{Influence of sampling ratio }
As the value of  determines the performance of LLL,
we conducted a medium-scale experiment to understand the influence of  with and without task-specific tokens.
In this experiment we used WikiSQL (blue color), SST (orange), QA-SRL (green), and WOZ (red), in that training order.
The results are shown in Figure~\ref{fig:cmp_gen_size}.

Unsurprisingly, the less generation done by the model, the more likely the vanishing distribution in Section~\ref{sec:our} occurs: the model forgets how to generate previous tasks, as the ratio of previous tasks in the total dataset decreases exponentially over time. 
Models using task-specific tokens mitigate this somewhat, as demonstrated in the first subgraph where the performance of  is much better than that of .

In addition, the more samples the model generates, the better the overall performance of the model.
However, this performance gain disappears when the sampling ratio  is around 0.1 to 0.3.


\begin{table}[!tp]
    \centering
    \scalebox{0.9}{
    \begin{tabular}{l*{3}{c}}
    \toprule
    \small Order & \small MBPA++ & \small MBPA++ (our impl.) & \small  \\
    \midrule
    i & 70.8 & 74.1 & \textbf{76.7} \\
    ii & 70.9 & 74.9 & \textbf{77.2} \\
    iii & 70.2 & 73.1 & \textbf{76.1} \\
    iv & 70.7 & 74.9 & \textbf{76.1} \\
    \midrule
    Average & 70.7 & 74.2 & \textbf{76.5} \\
    \bottomrule
    \end{tabular}
    }
    \caption{Summary of results on text classification tasks using averaged EM score (equivalent to averaged accuracy in \cite{d2019episodic}) of models at last epoch of last task. The four orders mirror those in \cite{d2019episodic}. For MBPA++ (our impl.) and , the results are averaged over two runs. The -value of pairted -test between eight numbers of MBPA++ (our impl.) and  is smaller than 1\%, which shows that there is significant difference. Our implementation of MBPA++ is available at \url{https://github.com/Daikon-Sun/EM-in-LLL}.}
    \label{tab:text_cls}
\end{table}

 
\section{Conclusion}

We propose LAMOL, a simple yet effective method for LLL based on language modeling.
A single LM achieves LLL without additional model components and without keeping old examples.
Moreover, any pre-trained LM can be used to leverage a large amount of unlabeled text to improve LLL.
Finally, more tasks can be added whenever needed. 


\section*{Acknowledgement}
This work was supported by the Ministry of Science and Technology of
Taiwan.

\bibliography{iclr2020_conference}
\bibliographystyle{iclr2020_conference}

\clearpage
\appendix

\section{Tasks, Dataset, and Metrics} \label{appendix:dataset}
Five tasks and their corresponding datasets from decaNLP~\citep{McCann2018decaNLP}:
\begin{itemize}
\item \textbf{Question Answering -- Stanford Question Answering Dataset (SQuAD)}~\citep{rajpurkar2016squad}: This dataset consists of context, questions, and answers. The context is paragraphs from English Wikipedia, and the answers are spans from its corresponding question paragraphs. For evaluation, we use the normalized F1 score (nF1), which strips out articles and punctuation as in \citet{McCann2018decaNLP}. 
Test datasets in this task are hidden from the host so that users must upload models to their platform to generate the test results; due to this inconvenience and our many models, we elected to use the development set to test the metric. 
Note that we do not use the development set in the training process. The size of the training set is 87,599 while that of the development set is 10,570.

\item \textbf{Semantic Parsing -- WikiSQL}~\citep{zhong2017seq2sql}: In this task, normal sentences are translated into SQL-structured SQL queries. WikiSQL provides logical forms along with natural language utterances. The exact match of the logical forms (lfEM) is used to evaluate the performance. The model outputs are required to be matched the SQL format. Otherwise, its won't get any score. The size of the training set is 56,355; that of the test set is 15,878.

\item \textbf{Sentiment Analysis -- Stanford Sentiment Treebank (SST, binary version)}~\citep{radford2017learning}: This dataset consists of movie reviews with its answers, including positive and negative binary options. The exact match score is used as the metric. The size of the training set is 6,920; that of the test set is 1,821.

\item \textbf{Semantic Role Labeling -- QA-SRL}~\citep{he2017deep}: QA-SRL is a question answering form of the SRL task. The normalized F1 (nF1) score is used. The size of the training set is 6,414; that of the test set is 2,201.

\item \textbf{Goal-Oriented Dialogue -- English Wizard of Oz (WOZ)}~\citep{wen2016network}: WOZ is a restaurant reservation task that provides a predefined ontology of a series of information for helping an agent to make reservations for customers. To keep track of the dialogue state, turn-based dialogue state EM (dsEM), which requires the model outputs exactly follow the characters' conversation order, is used for judgment. The size of the training set is 2,536; that of the test set is 1,646.
\end{itemize}

\noindent Four text classification tasks and five datasets from MBPA++ (d’Autume et al. 2019):

\begin{itemize}
\item \textbf{News Classification -- AGNews:} News articles to be classified into 4 classes.
\item \textbf{Sentiment Analysis -- Yelp and Amazon:} Customer reviews and ratings on Yelp and Amazon. Both datasets include 5 classes.
\item \textbf{Wikipedia Article Classification -- DBPedia:} Articles and their corresponding categories on Wikipedia, including 14 classes.
\item \textbf{Questions and Answers Categorization -- Yahoo:} Questions and answers on the Yahoo! platform, including 10 classes.
\end{itemize}
The dataset collected by \citet{cls} is available at http://goo.gl/JyCnZq. Given the unbalanced dataset sizes, we randomly sample 115,000 training examples and 7,600 test examples from all the datasets per~\citet{d2019episodic}. 
All the tasks use exact match accuracy as the evaluation metric.

\begin{comment}
\section{MAS Implementation}
The original paper of MAS indicates it is possible to utilize unlabeled data to estimate the importance of the weights by measuring the sensitivity to the outputs; However, it is not clear what is the "outputs" indicated in the paper. We traced the code of their implementation on github \footnote{https://github.com/rahafaljundi/MAS-Memory-Aware-Synapses} and found they use the outputs right before the softmax layer. Yet, we have misgivings about the L2-Norm implementing on the outputs. First, not like common classification tasks mentioned in the paper, the outputs of a language model involving a huge dimensions due to its vocabulary size. We concern about if it results in a low variety of outputs (after L2-Norm) because we also squares a huge number of negative values that makes certain dimensions not so outstanding. Second, a negative value, let's say -1,000 is not so different from a tremendous negative value, let's say -100,000,000 after the softmax function. Obviously the magnitude for negative values before and after softmax function is not consistent. Also, considering an extreme situation that all outputs before softmax function are negative, which means the model is not so sure about the result; If we square all those negative values and calculate gradients as parameter importance, it might lead to a misleading. For the reasons mentioned above, we also compare the result before and after softmax function.
\end{comment}

\clearpage
\section{Overview of the forgetting progress for three tasks}
\label{appendix:small_perm}
\begin{figure*}[!h]
  \includegraphics[width=\linewidth]{imgs/small_perm.png}
  \caption{Overview of the forgetting progress for different methods and permuted orders. The blue line indicate the scores of the first task after training each task. The orange line corresponds to that of the second task.}
  \label{fig:small_perm}
\end{figure*} 
\clearpage
\section{Reverse order of five DECANLP tasks}
\label{appendix:r_five_tasks}
\begin{table*}[h]
\centering
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{*{6}{c}||ccc}
    \toprule
    \small Fine-tuned & \small MAS & \small  & \small  & \small  & \small  & \small  & \small  & \small Multitasked \\
    \midrule
    45.4 & 44.7 & 63.2 & 73.0 & 75.3 & \textbf{76.9} & 75.9 & 78.2 & 76.6 \\
    \bottomrule
    \end{tabular}
    \end{adjustbox}
    \caption{Summary of averaged score on reversed five tasks. The scores are reported as the averaged score over all tasks of the models after training on every task. The rightmost three columns~-- LAMOL with  and  of real samples from previous tasks. Best performance in boldface.}
\label{tab:r_five_tasks}
\end{table*} 
\begin{figure*}[h]
  \centering
\includegraphics[width=0.6\linewidth]{imgs/r_five_tasks.png}
  
  \caption{Training progress of the five tasks with reverse order, i.e. small to large. The graph records the performance of the model at each epoch of each tasks. The order of tasks in the progress follows: WOZ, QA-SRL, SST, WikiSQL, and then SQuAD. From the figure, we can clearly see that Fine-tune and MAS completely forget WOZ and WikiSQL.}
  \label{fig:r_five_tasks}
\end{figure*}
 
\clearpage
\section{Generated Examples}
\begin{table}[h]
\begin{threeparttable}
\centering
    \begin{adjustbox}{max width=\textwidth}
    \begin{tabular}{|m{50em}|}
    \hline
    \textbf{Normal Examples}
    \\
    \hline
    \\
    \textit{\textunderscore \textunderscore squad1\textunderscore \textunderscore The United States has been accused of a wide ranging war in Afghanistan since 9 / 11. During the campaign, US forces in Afghanistan were involved in an extensive air campaign. At least 1,600 American servicemen and women were killed, while more than 1,600 civilians were injured. After the US-led invasion of Afghanistan on 12 / 11 / 2001, an estimated 10,000 American soldiers were killed in combat.What were the targets included in the conflict? \textunderscore \textunderscore ans\textunderscore \textunderscore Afghanistan}
    \\
    \\
    \textit{\textunderscore \textunderscore squad1\textunderscore \textunderscore In 1849, the French army was forced to withdraw, and the French were finally expelled, although it was not until late November that the French recaptured most of their territories. French troops then reached Egypt. On 21 January 1852 (the year after he left), in Cairo, they captured Tripoli, Benghazi, Benghazi, and the eastern part of Libya. After Gaddafi's return to office, he established the Gaddafi regime. On 13 February 1856, the Gaddafi family relocated to Egypt. On 13 May 1857, the army was forced to withdraw from Libya, and the army returned to Benghazi.On whom did Gaddafi's army return to Benghazi?\textunderscore \textunderscore ans\textunderscore \textunderscore Gaddafi's family} 
    \\
    \\
    \textit{\textunderscore \textunderscore wikisql\textunderscore \textunderscore the table has columns rank, athlete, sport, nation, gold, silver and key words max, min, count, sum, avg, =, , , op, select, where, and, col, table, caption, page, section, op, cond, question, agg, aggops, condops - - what is the nation when they have the rank 10 and the gold is 6?what is the translation from english to sql?\textunderscore \textunderscore ans\textunderscore \textunderscore select nation from table where rank = 10 and gold = 6}
    \\
    \\
    \textit{\textunderscore \textunderscore wikisql\textunderscore \textunderscore the table has columns home team, home team score, away team, away team score, venue, crowd, date and key words max, min, count, sum, avg, =, , , op, select, where, and, col, table, caption, page, section, op, cond, question, agg, aggops, condops - - what is the home team's score when they played at richfield?what is the translation from english to sql?\textunderscore \textunderscore ans\textunderscore \textunderscore select home team score from table where venue = richfield}
    \\
    \\
    \textit{\textunderscore \textunderscore sst\textunderscore \textunderscore it's a movie that's not worth a look.is this review negative or positive?\textunderscore \textunderscore ans\textunderscore \textunderscore negative}
    \\
    \\
    \textit{\textunderscore \textunderscore sst\textunderscore \textunderscore it's an ugly movie about an old man who's been given power by a new god to bring about a new era.is this review negative or positive?\textunderscore \textunderscore ans\textunderscore \textunderscore negative}
    \\
    \\
    \textit{\textunderscore \textunderscore srl\textunderscore \textunderscore in a statement on his official page, juan valderrama said that the president's nominees would be appointed by the president and confirmed by the national senate.who might be appointed?\textunderscore \textunderscore ans\textunderscore \textunderscore president}
    \\
    \\
    \textit{\textunderscore \textunderscore srl\textunderscore \textunderscore the first two films, released on august 21, 1999, are the first in a line of films that were made by two different people, each of whom worked in tandem on the film.what was released?\textunderscore \textunderscore ans\textunderscore \textunderscore the first two films}
    \\
    \\
    \hline
    \textbf{Examples of Chaos}
    \\
    \hline
    \\
    \textit{\textunderscore \textunderscore squad1\textunderscore \textunderscore the table has columns rank ( nation ), gold, silver, bronze, total and key words max, min, count, sum, avg, =, , , op, select, where, and, col, table, caption, page, section, op, cond, question, agg, aggops, condops - - which total has a gold smaller than 2, and has the lowest rank of bronze of 3?what is the translation from english to sql?\textunderscore \textunderscore ans\textunderscore \textunderscore select min total from table where gold  2 and bronze = 3}
    \\
    \\
    \textit{\textunderscore \textunderscore squad1\textunderscore \textunderscore In addition to the role played by the main characters, the main cast members are played by the same cast members who were in season one of the original series.is this review negative or positive?\textunderscore \textunderscore ans\textunderscore \textunderscore positive}
    \\
    \\
    \textit{\textunderscore \textunderscore wikisql\textunderscore \textunderscore it's a shame that the story doesn't end with a good ending, but it certainly means the world to me.is this review negative or positive?\textunderscore \textunderscore ans\textunderscore \textunderscore positive}
    \\
    \\
    \textit{\textunderscore \textunderscore wikisql\textunderscore \textunderscore on the same day, june 19, 2012, former heavyweight champion arthur lang defended his title against alexander green in the final of the world heavyweight championship.what did someone defend?\textunderscore \textunderscore ans\textunderscore \textunderscore his title}
    \\
    \\
    \textit{\textunderscore \textunderscore sst\textunderscore \textunderscore at this point, he was only twenty - five years old at the time of his first appearance on the mcf tv series.who was something twenty - five years old?\textunderscore \textunderscore ans\textunderscore \textunderscore he}
    \\
    \\
    \textit{\textunderscore \textunderscore sst\textunderscore \textunderscore on paper, the movie's two most fascinating characters are two peas in the same class, both of whom have made an appearance in this film.who did something feature?\textunderscore \textunderscore ans\textunderscore \textunderscore two most fascinating characters}
    \\
    \\
    \textit{\textunderscore \textunderscore srl\textunderscore \textunderscore a popular seasonal film in austrian films and the most influential in the history of cinema.is this review negative or positive?\textunderscore \textunderscore ans\textunderscore \textunderscore positive}
    \\
    \\
    \textit{\textunderscore \textunderscore srl\textunderscore \textunderscore it's not a bad film, it's just not as good as you've seen it before.is this review negative or positive?\textunderscore \textunderscore ans\textunderscore \textunderscore negative}
    \\
    \\
    \hline
    \end{tabular}
    \end{adjustbox}
    \caption{Examples generated by LAMOL with task-specific tokens. Annotations \textit{\textunderscore \textunderscore squad1\textunderscore \textunderscore, \textunderscore \textunderscore wikisql\textunderscore \textunderscore, \textunderscore \textunderscore sst\textunderscore \textunderscore, \textunderscore \textunderscore srl\textunderscore \textunderscore} correspond to each task-specific token of SQuAD, WikiSQL, SST, and QA-SRL, respectively. \textit{\textunderscore \textunderscore ans\textunderscore \textunderscore} is the ANS token that separates the question from the answer. The upper frame shows the normal situation whereas the lower frame shows generated contents that are inconsistent with their task-specific token.}
\label{tab:gen_ex}
\end{threeparttable}
\end{table}
 
\end{document}
