\documentclass[sigconf]{acmart}


\AtBeginDocument{\providecommand\BibTeX{{\normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\setcopyright{iw3c2w3}
\copyrightyear{2021}
\acmYear{2021}
\acmDOI{10.1145/3442381.3449897}

\acmConference[WWW '21]{Proceedings of the Web Conference 2021}{April 19--23, 2021}{Ljubljana, Slovenia}
\acmBooktitle{Proceedings of the Web Conference 2021 (WWW '21), April 19--23, 2021,
Ljubljana, Slovenia}
\acmPrice{}
\acmISBN{978-1-4503-8312-7/21/04}
\settopmatter{printacmref=true}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{pifont}
\usepackage{bm}

\begin{document}

\title{Boosting the Speed of Entity Alignment \textbf{}: Dual Attention Matching Network with Normalized Hard Sample Mining}

\author{Xin Mao, Wenting Wang, Yuanbin Wu, Man Lan}
\email{xmao@stu.ecnu.edu.cn, {wenting.wang}@lazada.com, {ybwu,mlan}@cs.ecnu.edu.cn}
\affiliation{East China Normal University, Alibaba Group}

\begin{abstract}
Seeking the equivalent entities among multi-source Knowledge Graphs (KGs) is the pivotal step to KGs integration, also known as \emph{entity alignment} (EA).
However, most existing EA methods are inefficient and poor in scalability.
A recent summary points out that some of them even require several days to deal with a dataset containing  nodes (DWYK).
We believe over-complex graph encoder and inefficient negative sampling strategy are the two main reasons.
In this paper, we propose a novel KG encoder --- \emph{Dual Attention Matching Network} (Dual-AMN), which not only models both intra-graph and cross-graph information smartly, but also greatly reduces computational complexity.
Furthermore, we propose the \emph{Normalized Hard Sample Mining Loss} to smoothly select hard negative samples with reduced loss shift.
The experimental results on widely used public datasets indicate that our method achieves both high accuracy and high efficiency.
On DWYK, the whole running process of our method could be finished in  seconds, at least  faster than previous work.
The performances of our method also outperform previous works across all datasets, where  and  have been improved from  to .
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010178.10010187</concept_id>
<concept_desc>Computing methodologies~Knowledge representation and reasoning</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010178.10010179</concept_id>
<concept_desc>Computing methodologies~Natural language processing</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010257.10010258.10010259</concept_id>
<concept_desc>Computing methodologies~Supervised learning</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Knowledge representation and reasoning}
\ccsdesc[300]{Computing methodologies~Natural language processing}
\ccsdesc[300]{Computing methodologies~Supervised learning}


\keywords{Graph Neural Networks; Knowledge Graph; Entity Alignment}


\maketitle

\section{Introduction}
\label{sec:intro}
Typically, knowledge graphs (KGs) store real-world knowledge in the form of triples (i.e., entity, relation, entity), where entities are connected through relations.
In recent years, many general KGs (e.g., DBpedia \cite{DBLP:conf/semweb/AuerBKLCI07}, YAGO \cite{DBLP:conf/www/SuchanekKW07}) and domain-specific KGs (e.g., Scientific \cite{DBLP:conf/kdd/TangZYLZS08}) have proliferated
and been widely used in downstream applications, such as search engines and recommendation systems.

In practice, a KG is usually constructed from one single data source.
Therefore, it is unlikely to cover the full domain.
As shown in Figure \ref{Figure:Intro}(a), integrating KGs in the same domain but built from different languages can transfer the information from high resource language to low resource language.
This in turn will facilitate downstream cross-lingual applications, especially for minority language users.
Moreover, consolidating multi-domain KGs (Figure \ref{Figure:Intro}(b)) can supplement cross-domain information and improve the coverage, thus making KGs more complete.

\begin{figure}
  \centering
  \includegraphics[width=1\linewidth]{Intro.pdf}
  \caption{Two examples of KGs integration. Figure (a) represents cross-lingual and Figure (b) represents cross-domain.}\label{Figure:Intro}
\end{figure}

Seeking the equivalent entities among multi-source KGs is the pivotal step to KGs integration, also known as \emph{entity alignment} (EA).
Recently, EA attracts enormous attention and progresses rapidly.
Dozens of related papers have been published in recent years.
In general, these methods all share one core framework:
assume that equivalent entities possess similar neighboring structure, apply \emph{KG embedding} methods (e.g., TransE \cite{DBLP:conf/nips/BordesUGWY13} or GCN \cite{DBLP:journals/corr/KipfW16}) to obtain dense embeddings for each entity, then map these embeddings into a unified vector space by \emph{alignment module} (e.g., Triplet loss and Contrastive Loss\cite{DBLP:conf/cvpr/SchroffKP15,DBLP:conf/cvpr/HadsellCL06}),
and finally the pair-wise distance between entities determines whether they are aligned or not.

However, previous EA methods are inefficient and poor in scalability, as summarized by \citet{9174835} that most of them require several hours \cite{DBLP:conf/emnlp/LiCHSLC19,DBLP:conf/ijcai/SunHZQ18,DBLP:conf/acl/CaoLLLLC19} or even days \cite{DBLP:conf/acl/XuWYFSWY19} on a dataset containing  nodes (i.e., DWYK).
In reality, KGs usually consist of millions of entities and relations (e.g., the full DBpedia contains  billion entities,  trillion triples).
Such large-scale datasets impose huge challenges in the efficiency and scalability of EA methods.
Obviously, the high cost in time hinders the feasibility of applying these EA methods to large-scale KGs.

We believe there are two main reasons that cause the high time complexity of these advanced methods:

(1) \textbf{Over-complex graph encoder}:
Since vanilla GCN is unable to model the heterogeneous relation information in KGs, many relation-aware GNN variants are proposed in EA task.
However, some GNN variants are over-complex and inefficient.
The running time of the vanilla GCN \cite{DBLP:conf/emnlp/WangLLZ18} is only  of those from complex encoders.
Every time a complex technique is introduced, e.g., Graph Attention mechanism \cite{DBLP:conf/iclr/VelickovicCCRLB18}, Graph Matching Networks \cite{DBLP:conf/icml/LiGDVK19} (GMN), Joint Learning \cite{DBLP:conf/emnlp/LiCHSLC19}, the time complexity is dramatically increased.
For instance, GM-Align \cite{DBLP:conf/acl/XuWYFSWY19} incorporates GMN and achieves decent performances on a small dataset (DBPK), but the performance improvement most likely is contributed from the literal information.
When moving to a larger dataset (DWYK), GM-Align needs five days to obtain the results.
We believe the graph encoder still has some redundancy in design and its architecture can be further simplified to reduce time consumption.

(2) \textbf{Inefficient negative sampling strategy}:
Almost all existing EA methods rely on the pair-wise loss functions (e.g., TransE, Triplet loss, and Contrastive Loss).
In pair-wise loss, the negative samples are constructed via uniform random sampling.
In this way, the samples are usually highly redundant and have limited information.
The learning process could be hampered by the low-quality negative samples, resulting in slow convergence and model degradation.
To alleviate this problem, BootEA \cite{DBLP:conf/ijcai/SunHZQ18} proposes a \emph{Truncated Uniform Negative Sampling} strategy to choose K-nearest neighbors as negative samples (i.e., hard samples).
Such an intuitive and effective strategy has been widely adopted in subsequent studies \cite{DBLP:conf/emnlp/LiCHSLC19,DBLP:journals/corr/abs-2004-13579,DBLP:conf/acl/CaoLLLLC19}.
However, ranking all neighbors to find the K-nearest is highly time-consuming and difficult to be fully parallelized on GPU.
For example, \emph{Truncated Uniform Negative Sampling Strategy} takes more than  of the whole time cost of BootEA.

Instead of trading efficiency for better performance, in this paper, we propose \emph{Dual Attention Matching Network} (Dual-AMN) to capture dual relational information within a single graph and across two graphs:
The \emph{Simplified Relational Attention Layer} captures relational information within each KG by generating relation-specific embeddings through \emph{Relational Anisotropy Attention} and \emph{Relational Projection}.
The \emph{Proxy Matching Attention Layer} treats alignment as a special relation type and explicitly models it via proxy vectors.
In addition, to tackle the inefficient sampling issue, we further propose a \emph{Normalized Hard Sample Mining Loss}.
First, \emph{LogSumExp} operation is used to approximate \emph{Max} operation to generate hard samples smoothly but efficiently.
Then, to resolve the dilemma of hyper-parameter selection in \emph{LogSumExp}, we introduce a loss normalization strategy adjusting the distribution of loss dynamically.


Experiment with the same hardware environment, our method could finish the whole running process in  seconds on DWY-K, including data loading, training, and evaluating, which is  faster compared to the fastest existing model (i.e., GCN-Align \cite{DBLP:conf/emnlp/WangLLZ18}) and only takes up  of advanced methods.
On DBPK with a smaller scale, our method even could obtain results in less than  seconds.
More surprisingly, the alignment results obtained by our method have very high accuracy.
The experiments show that our method beats all state-of-the-art competitors across all datasets, and the performance improvement on  and  ranges from  to .
The main contributions are summarized as follows:
\begin{itemize}
  \item \textbf{Model.}
  We propose a novel graph encoder  \emph{Dual Attention Matching Network} (Dual-AMN) composing of \emph{Simplified Relational Attention Layer} and \emph{Proxy Matching Attention Layer}.
  The proposed encoder not only models both intra-graph and cross-graph relations smartly, but also greatly reduces computational complexity.
  \item \textbf{Training.}
  Instead of the inefficient sampling strategy, we propose a \emph{Normalized Hard Sample Mining Loss}, where the \emph{LogSumExp} operation generates hard samples efficiently and the loss normalization alleviates the dilemma of hyper-parameter selection.
  The new loss dramatically cuts down the sampling consumption and accelerates the convergence speed of the model.
  \item \textbf{Experiments.}
  The experimental results on widely used public datasets indicate that our method has high efficiency and accuracy.
  Furthermore, we design many auxiliary experiments to demonstrate the effectiveness of each component and the interpretability of the model.
\end{itemize}

\section{Task Definition}
\textbf{\emph{Definition of Knowledge Graph:}}
The formal definition of a KG is a directed graph  comprising three sets --- entities , relations , and triples .
KG stores the real-world information in the form of triples entity, relation, entity, which describe the inherent relation between two entities.
In addition, we define  to represent the neighbor set of entity  and  represent the set of relations between  and .

\noindent
\textbf{\emph{Definition of Entity Alignment:}}
Given two KGs , , and a pre-aligned entity pair set , where  denotes equivalence.
EA aims to obtain more potential equivalent entity pairs based on the information of , , and .

\section{Related Work}
\label{sec:rw}
As mentioned in Section \ref{sec:intro}, existing EA methods can be abstracted into one framework containing three major components:
\begin{itemize}
  \item \textbf{Graph embedding module} is responsible for encoding entities and relations of KGs into dense embeddings.
  \item \textbf{Entity alignment module} aims to map the embeddings of multi-source KGs into a unified vector space via pre-aligned entity pairs.
  \item \textbf{Information enhancement module} is able to generate semi-supervised data or introduce additional literal information for enhancement.
\end{itemize}
In this section, we categorize existing EA approaches based on their designs of these three components, as shown in Table \ref{tabel:rw}.

\begin{table}
\begin{center}

\resizebox{1\linewidth}{!}{
\renewcommand\arraystretch{1.6}
\large
\begin{tabular}{cccc}
  \toprule
\textbf{Method}&\textbf{Embedding}&\textbf{Alignment}&\textbf{Enhancement}\\
  \toprule
  MTransE \cite{DBLP:conf/ijcai/ChenTYZ17} &TransE&Mapping&None\\
  GCN-Align \cite{DBLP:conf/emnlp/WangLLZ18}&GNN&Margin-based&None\\
  RSNs \cite{DBLP:conf/icml/GuoSH19}&RSNs&Corpus fusion&None\\
  MuGNN \cite{DBLP:conf/acl/CaoLLLLC19}&Hybrid&Margin-based&None\\
KECG \cite{DBLP:conf/emnlp/LiCHSLC19}&Hybrid&Margin-based&None\\
  \midrule
  BootEA \cite{DBLP:conf/ijcai/SunHZQ18}&TransE&Corpus fusion&Semi-supervised\\
  NAEA \cite{DBLP:conf/ijcai/ZhuZ0TG19}&Hybrid&Corpus fusion&Semi-supervised\\
  TransEdge\cite{DBLP:journals/corr/abs-2004-13579}&TransE&Corpus fusion&Semi-supervised\\
  MRAEA \cite{DBLP:conf/wsdm/MaoWXLW20}&GNN&Margin-based&Semi-supervised\\
  \midrule
  GM-Align \cite{DBLP:conf/acl/XuWYFSWY19}&GNN&Margin-based&Entity Name\\
  RDGCN \cite{DBLP:conf/ijcai/WuLF0Y019}&GNN&Margin-based&Entity Name\\
  HMAN \cite{DBLP:conf/emnlp/YangZSLLS19}&GNN& Margin-based&Attribute\\
  HGCN \cite{DBLP:conf/emnlp/WuLFWZ19}&GNN&Margin-based&Entity Name\\
\bottomrule
\end{tabular}
}
\end{center}
\caption{Categorization of some popular EA approaches.}\label{tabel:rw}
\end{table}

\subsection{Embedding Module}
TransE \cite{DBLP:conf/nips/BordesUGWY13}, GNN, and Hybrid are the three mainstream embedding approaches.
TransE interprets relations as the translation from head entities to tail entities and assumes that the embeddings of entities and relations follow the assumption  if a triple  holds.
Based on this hypothesis, many variants (e.g., TransH \cite{DBLP:conf/aaai/WangZFC14} and TransR \cite{DBLP:conf/aaai/LinLSLZ15}) are proposed and proven to be effective in subsequent studies.
Graph Neural Network (GNN) is famous for its strong modeling capability on the non-Euclidean structure.
Different from TransE optimizing triples, GNN generates node-aware embeddings by aggregating the neighboring information of entities.
However, vanilla GNN \cite{DBLP:journals/corr/KipfW16} is unable to encode heterogeneous relational graphs such as KGs.
Thus, many subsequent studies focus on modifying GNN to fit into KG.
The main direction is to use the anisotropic attention mechanism \cite{DBLP:conf/iclr/VelickovicCCRLB18} to assign different weight coefficients to entities.
A GNN model whose node update equation treats every edge direction equally, is considered isotropic (e.g., vanilla GCN); and a GNN model whose node update equation treats every edge direction differently, is considered anisotropic (e.g., GAT \cite{DBLP:conf/iclr/VelickovicCCRLB18}).
Hybrid embedding approaches combine TransE and GNN together, which aim to enhance the expression ability of the model.
However, for now, the best-performing methods TransEdge \cite{DBLP:journals/corr/abs-2004-13579} and MRAEA \cite{DBLP:conf/wsdm/MaoWXLW20} are not hybrid.
The hybrid-based methods do not show necessity while introducing additional complexity.

In addition to these three mainstream approaches, RSNs \cite{DBLP:conf/icml/GuoSH19} integrates Recurrent Neural Networks (RNNs) with a skipping mechanism to efficiently capture the long-term relational dependencies within and between KGs.
RSNs performs well on sparse KGs, but it is still weaker than SOTA mainstream methods.

\subsection{Alignment Module}
The most common alignment methods are as follows:
(1) \textbf{Mapping} \cite{DBLP:conf/ijcai/ChenTYZ17} uses one or two linear transformation matrices to map the embeddings of entities in different KGs into a unified vector space.
This idea is inspired by the cross-lingual word embedding task \cite{DBLP:conf/iclr/LampleCRDJ18}, and the first proposed EA method \cite{DBLP:conf/ijcai/ChenTYZ17} adopts this alignment module.
(2) \textbf{Corpus fusion} \cite{DBLP:conf/ijcai/SunHZQ18} swaps the entities in the pre-aligned set and generates new triples to calibrate the embeddings into a unified space.
For example, there are two triples  and .
If  holds, \emph{Corpus fusion} adds two extra triples  and .
This approach not only integrates two KGs into one KG but also plays the role of data augmentation.
(3) \textbf{Margin-based} represents a series of pair-wise margin-based loss functions, such as Triplet loss \cite{DBLP:conf/cvpr/SchroffKP15}, Contrastive loss \cite{DBLP:conf/cvpr/HadsellCL06}, and so on.
Margin-based loss functions are often combined with Siamese Neural Network in ranking tasks (e.g., face recognition and text similarity).
Actually, GNN-based EA methods are inspired by the Siamese Neural Network and have similar architecture, so most of them use Margin-based loss to be their alignment module.

\subsection{Enhancement Module}
Because manually aligning entities is expensive in practice, pre-aligned pairs are usually a small part of all entities.
Therefore, existing methods usually reserve  or even less of the aligned pairs as training data to simulate this situation.
Due to the lack of labeled data, some EA methods \cite{DBLP:conf/ijcai/SunHZQ18, DBLP:journals/corr/abs-2004-13579} adopt bootstrapping to generate semi-supervised data iteratively.
Based on the asymmetric nature of cross-KG alignment, MRAEA \cite{DBLP:conf/wsdm/MaoWXLW20} further proposes a bi-directional iterative strategy.
These data augmentation techniques have been proved effective in improving alignment performance.

In addition to structure, some methods \cite{DBLP:conf/emnlp/WuLFWZ19,DBLP:conf/emnlp/YangZSLLS19} propose that introducing literal information could provide a multi-aspect view for alignment models and improve accuracy.
However, it should be noted that not all datasets contain literal information, especially in practical applications.
For example, there are privacy risks when using User Generated Content (UGC).
Compared with the literal methods, the structure-only methods are more general.
Therefore, these literal methods should be compared among themselves.

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.85\textwidth]{model.pdf}
  \caption{The architecture illustration of \emph{Dual Attention Matching Network} (Dual-AMN), composing of \emph{Simplified Relational Attention Layer} and \emph{Proxy Matching Attention Layer}.}\label{Figure:model}
\end{figure*}

\section{Dual Attention Matching Network}
\label{sec:model}
As mentioned in Section \ref{sec:intro}, existing graph encoders, which are over-complex in certain designs and poor in scalability, are not suitable to be applied to large-scale KG.
To address these defects, we propose \emph{Dual Attention Matching Network} (Dual-AMN).
Figure \ref{Figure:model} depicts that Dual-AMN is composed of two major components: \emph{Simplified Relational Attention Layer} and \emph{Proxy Matching Attention Layer}.
The \emph{Simplified Relational Attention Layer} captures relational information within each KG by generating relation-specific embeddings through \emph{Relational Anisotropy Attention} and \emph{Relational Projection}.
By treating alignment as a special relation, our \emph{Proxy Matching Attention Layer} leverages a list of proxies to explicitly capture the cross-graph information.
By combining the outcomes of these two proposed components, our Dual-AMN not only embeds both intra-graph and cross-graph relations smartly, but also greatly reduces computational complexity.
The experimental results show that the proposed method achieves the SOTA in both performance and efficiency.
In this section, we describe the architecture of Dual-AMN in details.


\subsection{Simplified Relational Attention Layer}
Since vanilla GCN is unable to model the heterogeneous relation information in KGs, many relation-aware GNN variants are proposed in EA task.
Most of them could be described by the following equation:

where  represents the embedding vector of  obtained by the -th GNN layer,  represents the weight coefficient between  and ,  represents the transformation matrix.
Table \ref{tabel:ra} lists some popular GNN encoders.
We summarize three findings:
(1) Except GCN-Align which first utilizes GCN in EA, all the other methods adopt anisotropic attention mechanism.
This indicates that it is necessary to distinguish the importance of entities.
(2) There is a tendency that more recent methods are not joint learning based, probably because joint methods are not superior in performance.
For example, MRAEA and TransEdge outperform MuGNN, KECG, and NAEA.
So joint learning which introduces extra computation complexity is not necessary.
(3) We also notice that many methods constrain the transformation matrix  of GNN layer to be diagonal or even remove  in order to avoid performance degradation.
We believe the main reason is that the entity embeddings are all trainable and the standard linear transformation may introduce too many parameters, causing over-fitting when updating these embeddings.
Inspired by these findings, we design a simplified relation-aware GNN layer.

\begin{table}
\begin{center}
\resizebox{0.9\linewidth}{!}{
\renewcommand\arraystretch{1.1}
\begin{tabular}{cccc}
  \toprule
\textbf{Method}&&Joint&\\
  \toprule
  GCN-Align \cite{DBLP:conf/emnlp/WangLLZ18}&Isotropic&&None\\
  MuGNN \cite{DBLP:conf/acl/CaoLLLLC19}&Anisotropy &&Diagonal\\
  KECG \cite{DBLP:conf/emnlp/LiCHSLC19}&Anisotropy &&Diagonal\\
  NAEA \cite{DBLP:conf/ijcai/ZhuZ0TG19}&Anisotropy &&Normal\\
  HMAN \cite{DBLP:conf/emnlp/YangZSLLS19}&Anisotropy &&Diagonal\\
  MRAEA \cite{DBLP:conf/wsdm/MaoWXLW20}&Anisotropy &&None\\
  \bottomrule
\end{tabular}
}
\end{center}
\caption{Categorization of GNN encoders in some popular EA approaches. }\label{tabel:ra}
\end{table}


The inputs of our model are two metrics,  represents the initial entity features
and  represents the initial relation features.
Both of them are randomly initialized by \emph{He\_initializer} \cite{DBLP:conf/iccv/HeZRS15}.
Similar to existing EA methods, we use anisotropic relational attention mechanism to aggregate the neighborhood information around entities.
The output embedding of entity  at the -th layer is obtained by the following equation:

here we employ  as the activation function.
Instead of standard linear transformation matrix , we utilize \emph{Relational Projection} operation \cite{DBLP:conf/cikm/MaoWXWL20}.
Such operation generates relation-specific embedding for each entity without extra parameters.
As for the calculation of , we adopt the meta-path \cite{DBLP:conf/nips/YunJKKK19} mechanism to assign weights:

where  is an attention vector.
\emph{Softmax} operation selects the most critical path from all types of edges connected to the entities (i.e., meta-path), which embeds the relational anisotropy but simplifies the calculation to the greatest extent.

In previous studies \cite{DBLP:conf/aaai/SunW0CDZQ20,DBLP:conf/wsdm/MaoWXLW20}, GNN is able to expand to multi-hop neighboring level information by stacking more layers, thus to create a more global-aware representation of the graph.
Following this idea, we concatenate the embeddings from different layers together to obtain the \emph{Multi-Hop Embeddings} for entity :

where  represents the concatenate operation.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{GMN.pdf}
  \caption{The illustration of two approaches. (a) represents \emph{Graph Matching Network} (GMN) and (b) represents \emph{Proxy Matching Attention Layer}.}\label{Figure:GMN}
\end{figure}

\subsection{Proxy Matching Attention Layer}
So far, the GNN encoders we have discussed only focus on modeling a single KG while leaving the cross-graph information to be learned by the alignment module alone.
Graph Matching Network \cite{DBLP:conf/icml/LiGDVK19} (GMN) builds a cross-graph attention mechanism to learn similarities, although they view the alignment purely as a node-to-node interaction (as illustrated in Figure \ref{Figure:GMN}(a)).
Formally, GMN measures the difference between  and its closest neighbor in the other graph as follows:




 is a vector space similarity metric,  represents the difference of  against all entities from .
Such node-to-node interaction enforces the embeddings to be learned jointly on a pair, at the cost of massive extra computation efficiency.
Since attention weights are required for every pair of nodes across two graphs, this operation has a computation cost of .
As mentioned in Section \ref{sec:intro}, GM-Align which incorporates GMN needs several days to obtain the results on the large-scale dataset (DWYK).
Driven by similar motivation, but in our interpretation, alignment itself is nothing but a special relation type whose representation can be explicitly learned in early stage.

Inspired by the above, we propose the \emph{Proxy Matching Attention Layer}.
As shown in Figure \ref{Figure:GMN} (b), we employ a limited set of proxy vectors to represent the cross-graph alignment relation, similar to use anchor points to present a space.
If two entities are equivalent, their similarity distributions associated with these proxy vectors should also be consistent.
In this way, the proposed layer is able to capture the cross-graph alignment information without computing node-to-node interaction.
The interaction of the \emph{Proxy Matching Attention Layer} is to calculate the similarity between all entities and limited anchors, which is similar to clustering.
On large-scale KGs or dense graphs, this interaction approach can greatly reduce the computational complexity from O() to O().

The inputs of the \emph{Proxy Matching Attention Layer} are two matrices:  represents the entities embeddings obtained by the \emph{Simplified Relational Attention Layer} and  represents proxy vectors with random initialization, where  represents the number of proxy vectors.
Just like GMN, the first step is to compute the similarity between each entity and all proxy vectors:

 represents the set of proxy vectors.
Here we use the cosine metric to measure the similarity between embeddings.
Then, the cross-graph embedding for entity  can be computed as:

 intuitively describes the difference between  and all proxy vectors.
Finally, we employ a gate mechanism \cite{DBLP:journals/corr/SrivastavaGS15} to combine  and , controlling the information flow between single graph and multiple graphs:




 and  are the gate weight matrix and gate bias vector.


\section{Normalized Hard Sample Mining}
Typically, in KGs, only a small portion of cross-graph entity pairs are aligned.
So negative sampling is crucial to EA methods.
However, the most common approach which selects the K-nearest neighbors, spends a lot of time on candidate ranking in each epoch.
In this section, we propose a \emph{Normalized Hard Sampling Mining} strategy, which is efficient and reduces loss shift.

\subsection{Smooth Hard Sample Mining}
Both TransE-based and GNN-based EA methods rely on the pair-wise loss functions to optimize the similarities between samples.
TransE-based methods use TransE loss to encode KGs:

GNN-based methods use Triplet loss to map the embeddings from two KGs into a unified space:

where  represents a fixed margin,  represents the operation Max,  represents the negative sample of .

Initially, the negative samples in pair-wise loss are generated through uniform random sampling, but this kind of samples is highly redundant and comprises too many easy even uninformative samples.
Training with such low-quality negative samples may significantly degrade the model's learning capability and slow down the convergence.
A simple but effective strategy is to select the K-nearest neighbors around the positive sample to be negative samples.
This is also known as \textbf{Hard Sample Mining}.
BootEA proposes the \emph{Truncated Uniform Negative Sampling} (TUNS) based on this strategy and reports that it could significantly reduce the number of training epochs and improve performance.
Most of the subsequent works follow this approach, such as KECG \cite{DBLP:conf/emnlp/LiCHSLC19}, MuGNN \cite{DBLP:conf/acl/CaoLLLLC19}, TransEdge \cite{DBLP:conf/semweb/SunHL17}, and etc.
However, faster convergence cannot shorten the overall training time.
Because it has to spent massive time in candidate ranking for the next epoch, and this process is difficult to be fully parallelized on GPU.


In the field of deep metric learning, some studies \cite{DBLP:conf/cvpr/SunCZZZWW20,DBLP:conf/cvpr/SongXJS16} propose to use the \emph{LogSumExp} operation to smoothly generate hard negative samples:

where  represents the positive sample set of the anchor and  represents the negative sample set.
 is a scale factor.
If :



\emph{LogSumExp} is approximate to TUNS with K = .
When  is set to an appropriate value, \emph{LogSumExp} could replace the K-nearest sampling strategy to generate high-quality negative samples, but with better computational efficiency (because this process could be fully parallelized on GPU).
More interestingly, when , the loss function is equivalent to \emph{Softmax} with \emph{Cross-Entropy} loss.
This also indicates that the classification losses and the pair-wise losses are essentially two sides of the same coin.

\subsection{Loss Normalization}
Both TUNS and \emph{LogSumExp} face the same dilemma of how to select the proper value for their hyper-parameters.
In TUNS, the hyper-parameter is the number of nearest neighbors K.
A small K will lead to slow convergence in the initial training process, while an overlarge K makes the negative samples too "easy."
In \emph{LogSumExp} operation, the hyper-parameter is the scalar factor .
As illustrated in Table \ref{dilemma}, if  is set too large, the weights of samples are greatly affected by the random disturbance at the beginning of training.
For example, when , these five pair losses are closer to each other while their corresponding weights vary a lot.
In such a case, the model would tend to only focus on a few samples, slowing down the convergence.
On the other hand, if  is too small, it would be difficult for the model to pick up hard samples in the later stage, which causes model degradation.
For example, when , though  is seven times larger than , the weights difference is small.

\begin{table}
\begin{center}
\renewcommand\arraystretch{1.1}
\begin{tabular}{cccccc}
\toprule
&&&&&\\
\toprule
pair loss&1.01&0.99&0.98&1.02&1.00\\
&0.111&0.015&0.005&0.826&0.041\\
\toprule
pair loss&0.07&0.01&0.02&0.02&0.02\\
&0.211&0.156&0.164&0.164&0.156\\
\bottomrule
\end{tabular}
\caption{Examples of the dilemma of selecting hyper-parameter , where  represents the loss of a pair .} \label{dilemma}
\end{center}
\end{table}

Inspired by batch normalization \cite{DBLP:conf/icml/IoffeS15} which reduces the internal covariate shift, we propose to use a normalization step that fixes the mean and variance of sample losses and reduces the dependence on the scale of the hyper-parameter.
Our overall loss function is defined as follow:



 represents the normalized loss of the triple .
 and  represent the new mean and the new variance of normalized loss respectively.
 is defined as follow:


where  represents the original loss of the triple ,  and  represent the mean and the variance of original loss, which are computed by:


The calculation process of  is similar to .

During training, we choose L distance as the metric to measure the similarity between entities:

During testing, in order to address the hubness problem in high-dimensional space, CSLS \cite{DBLP:conf/iclr/LampleCRDJ18} is set to be the distance metric.
Note that in training,  and  won't participate in gradient calculation and backpropagation.
This is because our loss normalization is designed to change the weights of the samples, not the gradient direction.
If  and  are updated in the backpropagation step, our loss will fail to converge.


\section{Experiments}
We use the Keras framework for developing our approach.
Our experiments are conducted on a workstation with a GeForce GTX TITAN X GPU and GB memory, which is consistent with the summary \cite{9174835}.
The code is now available on GitHub \footnote{https://github.com/MaoXinn/Dual-AMN}.

\begin{table}[t]
\begin{center}
\resizebox{0.8\linewidth}{!}{
\renewcommand\arraystretch{1.1}
\begin{tabular}{p{1.55cm}c|cccccc}
\toprule
\multicolumn{2}{c|}{Datasets} &  &   & \\
\toprule
\multirow{2}{1.3cm}{} & DBpedia & 100,000 &  302  & 428,952\\
& YAGO3 & 100,000 & 31  &  502,563 \\
\multirow{2}{1.3cm}{} & DBpedia & 100,000 &  330 & 463,294\\
& Wikipedia & 100,000 &  220  &  448,774  \\
\hline
\multirow{2}{1.3cm}{} & Chinese & 19,388 & 1,701& 70,414\\
& English & 19,572 & 1,323 & 95,142 \\
\multirow{2}{1.3cm}{} & Japanese & 19,814 & 1,299 & 77,214\\
& English & 19,780 & 1,153  & 93,484 \\
\multirow{2}{1.3cm}{} & French & 19,661 & 903 & 105,998\\
& English & 19,993 & 1,208 & 115,722  \\
\hline
\multirow{2}{1.3cm}{} & French & 15,000 & 177& 33,532\\
& English & 15,000 & 221& 36,508 \\
\multirow{2}{1.3cm}{} & German & 15,000 & 120 & 37,377\\
& English & 15,000 & 222 & 38,363  \\
\multirow{2}{1.3cm}{} & DBpedia & 15,000 &  223  & 33,748\\
& YAGO3 & 15,000 & 30  &  36,569 \\
\multirow{2}{1.3cm}{} & DBpedia & 15,000 &  253 & 38,421\\
& Wikipedia & 15,000 & 144 &  40,159  \\
\bottomrule
\end{tabular}
}
\end{center}
\caption{Statistical data of DBP15K, DWY100K and SRPRS.}\label{table:data}
\end{table}

\subsection{Datasets}
To fairly and comprehensively verify the effectiveness, robustness and scalability of our model, we construct experiments on three widely used public datasets:

(1) \textbf{DBP15K} \cite{DBLP:conf/semweb/SunHL17}:
This dataset consists of three cross-lingual subsets constructed from DBpedia: English-French (), English-Chinese (), English-Japanese ().
Each subset contains  pre-aligned entity pairs for training and testing.
As an early dataset, DBPK is widely used but has some defects: small scale and dense links.
These defects prompt more datasets to be proposed.

(2) \textbf{DWY100K} \cite{DBLP:conf/ijcai/SunHZQ18}:  This dataset comprises two mono-lingual subsets, each containing  pre-aligned entities pairs and nearly one million triples.
 represents the subset extracted from DBpedia and Wikidata, and  represents DBpedia and YAGO.
, as the largest dataset of the three, brings challenges to space and time complexity.

(3) \textbf{SRPRS} \cite{DBLP:conf/icml/GuoSH19}:
Compared with the real-world KGs, the above two datasets are too dense, and the degree distribution is quite different from the real.
Thus, \citet{DBLP:conf/icml/GuoSH19} propose a sparse dataset, including two cross-lingual subsets ( and ) and two mono-lingual subsets ( and ).
Same with DBPK, each subset of SRPRS contains  pre-aligned entity pairs for training and testing.
This dataset challenges the modeling ability of EA approaches when facing limited information.

The statistics of these datasets are listed in Table \ref{table:data}.
Consistent with previous studies, we randomly split  of the pre-aligned entity pairs for training and developing, while the remaining  for testing.

\begin{table*}[!t]
\begin{center}
\resizebox{\textwidth}{!}{
\renewcommand\arraystretch{1.1}
\begin{tabular}{c|cccccccccccccccc}
  \toprule
  \multicolumn{2}{c}{\multirow{2}{*}{Method}} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{} & \multicolumn{3}{c}{}& \multicolumn{3}{c}{}& \multicolumn{3}{c}{}  \\
  \multicolumn{2}{c}{} & H@1 & H@10 & MRR & H@1 & H@10 & MRR & H@1 & H@10 & MRR & H@1 & H@10 & MRR & H@1 & H@10 & MRR\\
  \hline
  \multirow{6}*{Basic}
  & MTransE & 0.209 & 0.512 & 0.310 & 0.250 & 0.572 & 0.360 & 0.247 & 0.577 & 0.360 &0.213& 0.447&0.290&0.107&0.248&0.160\\
  & GCN-Align & 0.434 & 0.762 & 0.550 & 0.427 & 0.762 & 0.540 & 0.411 & 0.772 & 0.530 & 0.243& 0.522&0.340&0.385&0.600& 0.460\\
  & MuGNN & 0.494 & 0.844 & 0.611 & 0.501 & 0.857 & 0.621 & 0.495 & 0.870  &0.621 &0.131&0.342& 0.208& 0.245&0.431&0.310\\
  & KECG&0.477&0.835&0.598& 0.489&0.844& 0.610&0.486&0.851& 0.610& 0.298& 0.616& 0.403&0.444 &0.707& 0.540\\
& RSNs&0.508&0.745&0.591&0.507&0.737&0.590&0.516&0.768&0.605&0.350&0.636& 0.440& 0.484& 0.729&0.570\\
  & \textbf{Dual-AMN} &\textbf{0.731}&\textbf{0.923}&\textbf{0.799}&\textbf{0.726}&\textbf{0.927}&\textbf{0.799}&\textbf{0.756}&\textbf{0.948}&\textbf{0.827}&\textbf{0.452}&\textbf{0.748}&\textbf{0.552}&\textbf{0.591}&\textbf{0.820}&\textbf{0.670}\\
  \hline
  \multirow{5}*{Semi}
  & BootEA & 0.629 & 0.847 & 0.703 & 0.622 & 0.853 & 0.701 & 0.653 & 0.874 & 0.731 & 0.365&0.649&0.460&0.503&0.732&0.580\\
  & NAEA & 0.650 & 0.867 & 0.720 & 0.641 & 0.872 & 0.718 & 0.673 & 0.894 & 0.752  &0.177&0.416& 0.260&0.307&0.535&0.390\\
  & TransEdge&0.735&0.919&0.801&0.719&0.932&0.795&0.710&0.941&0.796&0.400&0.675&0.490&0.556&0.753&0.630\\
  & MRAEA &0.757&0.930&0.827&0.758&0.934&0.826&0.781&0.948&0.849&0.460&0.768&0.559&0.594&0.818&0.666\\
  & \textbf{Dual-AMN} &\textbf{0.808}&\textbf{0.940}&\textbf{0.857}&\textbf{0.801}&\textbf{0.949}&\textbf{0.855}&\textbf{0.840}&\textbf{0.965}&\textbf{0.888}&\textbf{0.481}&\textbf{0.778}&\textbf{0.568}&\textbf{0.614}&\textbf{0.823}&\textbf{0.687}\\
  \hline
  \multirow{5}*{Literal}
  & GM-Align & 0.679 & 0.785 & - & 0.739 & 0.872 & - & 0.894 & 0.952 & - & 0.574&0.646&0.602&0.681&0.748&0.710\\
  & RDGCN & 0.697 & 0.842 & 0.750 & 0.763 & 0.897 & 0.810 & 0.873 & 0.950 & 0.901  &0.672&0.767& 0.710&0.779&0.886&0.820 \\
  & HMAN&0.561&0.859&0.670&0.557&0.860&0.670&0.550&0.876&0.660&0.401&0.705&0.500&0.528&0.778&0.620\\
  & HGCN &0.720&0.857&0.760&0.766&0.897&0.810&0.892&0.961&0.910&0.670&0.770&0.710&0.763&0.863&0.801\\
& \textbf{Dual-AMN} &\textbf{0.861}&\textbf{0.964}&\textbf{0.901}&\textbf{0.892}&\textbf{0.978}&\textbf{0.925}&\textbf{0.954}&\textbf{0.994}&\textbf{0.970}&\textbf{0.802}&\textbf{0.932}&\textbf{0.851}&\textbf{0.891}&\textbf{0.972}&\textbf{0.923}\\
    \bottomrule
\end{tabular}
}
\caption{Experimental results on cross-lingual datasets.}\label{table:res1}
\resizebox{\textwidth}{!}{
\renewcommand\arraystretch{1}
\begin{tabular}{c|ccccccccccccc}
  \toprule
  \multicolumn{2}{c}{\multirow{2}{*}{Method}} & \multicolumn{3}{c}{}& \multicolumn{3}{c}{}& \multicolumn{3}{c}{}& \multicolumn{3}{c}{}\\
  \multicolumn{2}{c}{} & H@1 & H@10 & MRR & H@1 & H@10 & MRR & H@1 & H@10 & MRR & H@1 & H@10 & MRR \\
  \hline
  \multirow{6}*{Basic}
  & MTransE & 0.238 & 0.507 & 0.330 & 0.227 & 0.414 & 0.290 & 0.188 & 0.382 & 0.260 &0.196& 0.401&0.270\\
  & GCN-Align & 0.494 & 0.756 & 0.590 & 0.598 & 0.829 & 0.680 & 0.291 & 0.556 & 0.380 & 0.319& 0.586&0.410\\
  & MuGNN & 0.604 & 0.894 & 0.701 & 0.739 & 0.937 & 0.810 & 0.151 & 0.366  &0.220 &0.175&0.381& 0.240\\
  & KECG&0.631&0.888&0.720& 0.719&0.904& 0.790&0.323&0.646& 0.430& 0.350& 0.651& 0.450\\
  & RSNs&0.607&0.793&0.673&0.689&0.878&0.756&0.391&0.663&0.480&0.393&0.665& 0.490\\
& \textbf{Dual-AMN} &\textbf{0.786}&\textbf{0.952}&\textbf{0.848}&\textbf{0.866}&\textbf{0.977}&\textbf{0.907}&\textbf{0.513}&\textbf{0.801}&\textbf{0.609}&\textbf{0.495}&\textbf{0.790}&\textbf{0.596}\\
  \hline
  \multirow{5}*{Semi}
  & BootEA & 0.748 & 0.898 & 0.801 & 0.761 & 0.894 & 0.808 & 0.384 & 0.667 & 0.480 & 0.381&0.651&0.470\\
  & NAEA & - & - & - & - & - & - & 0.182 & 0.429 & 0.260  &0.195&0.451& 0.280\\
  & TransEdge&0.788&0.938&0.824&0.792&0.936&0.832&0.461&0.738&0.560&0.443&0.699&0.530\\
  & MRAEA &0.794&0.930&0.856&0.819&0.951&0.875&0.509&0.795&0.597&0.485&0.768&0.574\\
  & \textbf{Dual-AMN} &\textbf{0.869}&\textbf{0.969}&\textbf{0.908}&\textbf{0.907}&\textbf{0.981}&\textbf{0.935}&\textbf{0.546}&\textbf{0.813}&\textbf{0.635}&\textbf{0.518}&\textbf{0.795}&\textbf{0.613}\\
    \bottomrule
\end{tabular}
}
\caption{Experimental results on mono-lingual datasets.
Because of the memory limitation, NAEA cannot work on DWYK.
}
\label{table:res2}
\end{center}
\end{table*}

\subsection{Baselines}
As mentioned in Section \ref{sec:rw}, many studies adopt enhancement modules.
For instance, GM-Align and RDGCN propose to introduce literal information to provide a multi-aspect view.
The introduction of additional information leads to unfair comparisons between methods.
Thus, existing EA methods will be compared separately according to the enhancement category:

(1) \textbf{Basic}:
This kind of method only uses the original structure information (i.e., triples) in the dataset and does not introduce any extra enhancement module:
MTransE \cite{DBLP:conf/ijcai/ChenTYZ17}, GCN-Align \cite{DBLP:conf/emnlp/WangLLZ18}, RSNs \cite{DBLP:conf/icml/GuoSH19}, MuGNN \cite{DBLP:conf/acl/CaoLLLLC19}, KECG \cite{DBLP:conf/emnlp/LiCHSLC19}.

(2) \textbf{Semi-supervised}:
These methods adopt bootstrapping to generate semi-supervised structure data:
BootEA \cite{DBLP:conf/ijcai/SunHZQ18}, NAEA \cite{DBLP:conf/ijcai/ZhuZ0TG19}, TransEdge \cite{DBLP:journals/corr/abs-2004-13579}, and MRAEA \cite{DBLP:conf/wsdm/MaoWXLW20}.

(3) \textbf{Literal}:
To obtain a multi-aspect view, literal methods use literal information (e.g., entity name) of entities as input features:
GM-Align \cite{DBLP:conf/acl/XuWYFSWY19}, RDGCN \cite{DBLP:conf/ijcai/WuLF0Y019}, HMAN \cite{DBLP:conf/emnlp/YangZSLLS19}, HGCN \cite{DBLP:conf/emnlp/WuLFWZ19}.


To make a fair comparison against above three types of methods, our model also has three corresponding versions:
(1) Dual-AMN is the basic version without any enhancement module, as described in Section \ref{sec:model}.
(2) Dual-AMN (Semi) introduces the bi-directional iterative strategy proposed by MRAEA to generate semi-supervised data.
(3) Dual-AMN (Lit) adopts a simple strategy to utilize literal information.
For  and , we use Dual-AMN (Semi) to obtain the structural similarity .
Then, using the cross-lingual word embedding \footnote{Same with GM-Align \cite{DBLP:conf/iclr/LampleCRDJ18}.} to calculate the literal similarity .
Finally, the entities are ranked according to .

\subsection{Experimental Settings}
\textbf{Metrics}.
Following convention, we use  and \emph{Mean Reciprocal Rank} () as our evaluation metrics.
The  score is calculated by measuring the proportion of correctly aligned pairs in the top-k.
In particular,  represents accuracy.
In order to be convincing, the reported performance is the average of five independent training runs.


\noindent
\textbf{Hyper-parameters}.
For all dataset, we use a same config:
The dimensionality for embeddings ;
depth of GNN ;
number of proxy vectors ;
margin ;
new mean and new variance of normalized loss are  and ;
batch size is ; dropout rate is set to .
RMSprop is adopted to optimize the model with learning rate set to .


\subsection{Main Experiments}
In Table \ref{table:res1} and Table \ref{table:res2}, we report the performances of all methods on cross-lingual datasets and mono-lingual datasets, respectively.
We compare the performances within each category.

\noindent
\textbf{Dual-AMN vs. Basic Methods}.
Our method consistently achieves the best performance across all datasets.
On the small-scale dense dataset (DBPK), Dual-AMN outperforms other methods by at least  in terms of both  and .
On the large-scale dense dataset (DWYK), the performances are increased by more than  compared to previous SOTA.
Experimental results show that the designs of Dual-AMN effectively captures the rich structural information of these two datasets.
By cutting down the number of triples, SRPRS challenges EA methods' ability to model sparse KGs.
It is not surprising to see that the performances of all methods drop significantly compared to the results on dense datasets.
RSNs outperforms the previous SOTA on this dataset, which could be credited to the long-term relational paths it captures.
But our Dual-AMN still achieves the best performance, exceeding RSNs by at least  on  and .
All these experimental results demonstrate the effectiveness of Dual-AMN in capturing the structural information.

\noindent
\textbf{Dual-AMN vs. Semi-supervised Methods}.
Benefiting from the semi-supervised strategy to generate more labeled data for the next training round, the overall performances of the semi-supervised methods surpass the basic methods.
Compared with previous SOTA methods, our method outperforms at least  on .
Compared to its own basic version, the semi-supervised strategy greatly improves the performances on DBPK and DWYK.
On SRPRS, although the semi-supervised strategy still has some benefit, the improvement is reduced to .
We believe the reason for the smaller improvement is because the sparse nature of SRPRS makes its structure information insufficient to generate high-quality semi-supervised data.
Overall, the semi-supervised strategy performs well on dense datasets, while its improvement is marginal in sparse datasets.

\noindent
\textbf{Dual-AMN vs. Literal Methods}.
According to \citet{9174835}, because the entity names between mono-lingual KGs are almost identical, the edit distance algorithm could achieve the ground-truth performance.
Therefore, the literal methods only experiment on cross-lingual datasets.

By combining with cross-lingual embeddings, the performances of Dual-AMN are further improved and surpass the previous SOTA methods across all datasets.
From observing Table \ref{table:res1}, we found that the performances of the literal methods vary significantly according to language pairs, which is completely different from the structure-only methods.
On DBPK, the introduction of literal information increases  by , , and , which indicates that French is the most similar language to English, while Chinese is the most different.
Besides, due to the lack of structural information, the literal information is more critical on SRPRS.
Literal information improves the performances by  on .

It must be admitted that our way of utilizing literal information is too simple and crude.
Compared with other methods, performance improvement mainly comes from better structural embeddings.
How to better integrate literal information is our future work.

\begin{table}
\begin{center}
\resizebox{0.9\linewidth}{!}{
\renewcommand\arraystretch{0.9}
\begin{tabular}{cccc}
  \toprule
  \textbf{Method}&\textbf{DBP15K}&\textbf{SRPRS}&\textbf{DWY100K}\\
  \toprule
  MTransE \cite{DBLP:conf/ijcai/ChenTYZ17} &6,467&3,355&70,085\\
  GCN-Align \cite{DBLP:conf/emnlp/WangLLZ18}&103&87&3,212\\
  RSNs \cite{DBLP:conf/icml/GuoSH19}&7,539&2,602&28,516\\
  MuGNN \cite{DBLP:conf/acl/CaoLLLLC19}&3,156&2,215&47,735\\
  KECG \cite{DBLP:conf/emnlp/LiCHSLC19}&3,724&1,800&125,386\\
  \textbf{Dual-AMN} \cite{DBLP:conf/aaai/SunW0CDZQ20}&\textbf{35}&\textbf{27}&\textbf{1,094}\\
  \midrule
  BootEA \cite{DBLP:conf/ijcai/SunHZQ18}&4,661&2,659&64,471\\
  NAEA \cite{DBLP:conf/ijcai/ZhuZ0TG19}&19,115&11,746&-\\
  TransEdge\cite{DBLP:journals/corr/abs-2004-13579}&3,629&1,210&20,839\\
  MRAEA \cite{DBLP:conf/wsdm/MaoWXLW20}&3,894&1,248&23,275\\
  \textbf{Dual-AMN(Semi)} \cite{DBLP:conf/aaai/SunW0CDZQ20}&\textbf{85}&\textbf{79}&\textbf{3,169}\\
  \midrule
  GM-Align \cite{DBLP:conf/acl/XuWYFSWY19}&26,328&13,032&459,715\\
  RDGCN \cite{DBLP:conf/ijcai/WuLF0Y019}&6,711&886&-\\
  HMAN \cite{DBLP:conf/emnlp/YangZSLLS19}&5,455&4,424&31,895\\
  HGCN \cite{DBLP:conf/emnlp/WuLFWZ19}&11,275&2,504&60,005\\
\textbf{Dual-AMN(Lit)} \cite{DBLP:conf/aaai/SunW0CDZQ20}&\textbf{101}&\textbf{96}&\textbf{3,257}\\
  \bottomrule
\end{tabular}
}
\end{center}
\caption{Time costs of EA methods (seconds).}\label{tabel:time}
\end{table}

\noindent
\textbf{Efficiency Analysis}.
Better performance is just the cherry on the cake.
Dual-AMN's trump card is superior efficiency.
Table \ref{tabel:time} reports the overall time costs of existing EA methods on each dataset, including data loading, pre-processing, training, and evaluating.
All results are obtained by directly running the source code provided by the authors.
And hyper-parameters are set to be the same as reported in their original papers.
Certainly, implement details such as learning rate, batch size, and pre-processing might influence the time costs.
However, we believe that these experimental results still reflect the overall efficiency of EA methods.

Obviously, the efficiency of Dual-AMN far exceeds competitors.
The time costs of complex EA methods are tens or even hundreds of times more than that of Dual-AMN.
Even compared with the fastest baseline (i.e., GCN-Align), the speed of Dual-AMN is  faster, while the  outperforms more than .
Comparing Dual-AMN and Dual-AMN (Semi), semi-supervised strategy increases the time consumption about three times.
Due to the simple combining strategy, Dual-AMN (Lit) hardly increases the time consumption.

In particular, the large-scale dense dataset (DWYK) poses a severe challenge to the space and time complexity of all EA methods.
Due to the limitation of GPU memory, MuGNN, KECG, and HMAN have to be run on CPU, resulting in massive time costs.
GM-Align is the least efficient method, because it uses GMN and requires a complicated pre-processing.
We fail to obtain results for NAEA and RDGCN in our experiment environment because they require extremely high memory space.
Benefit from the simplification of the encoder architecture and the \emph{Normalized Hard Sample Mining Loss}, our model could fully utilize the GPU to obtain high-accuracy results efficiently.
Even using the semi-supervised strategy for data augmentation, the proposed method still could obtain results within an hour.

In summary, the high efficiency of Dual-AMN makes the entity alignment application on large-scale KGs possible.

\begin{table}[t]
\renewcommand\arraystretch{1.5}
\centering
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{l|cccccc}
\toprule
\multirow{2}{* }{Method} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{}\\
& Hits@1 & MRR & Hits@1 & MRR & Hits@1 & MRR\\
\toprule
 Dual-AMN &&&&&&\\
 \;\;-RA.&&&&&&\\
 \;\;-RP.&&&&&&\\
 \;\;-MHE. &&&&&&\\
 \;\;-PAM. &&&&&&\\
\bottomrule
\end{tabular}
}
\caption{Ablation experiment of architecture on DBP15K.}
\label{table:model_ablation}
\end{table}

\subsection{Ablation Experiment}
To demonstrate the effectiveness of each design in architecture and loss function, we construct two ablation experiments on DBPK.

\noindent
\textbf{Ablation Experiment of architecture}.
Dual-AMN adopts the following four components to capture multi-aspect information existing in KGs:
(1) \emph{Relational Attention} mechanism (RA) finds the critical path around entities.
(2) \emph{Relational Projection} operation (RP) generates the relation-specific embedding for entities.
(3) \emph{Multi-Hop Embeddings} (MHE) creates a more global-aware representation of the KGs.
(4) \emph{Proxy Attention Matching Layer} (PAM) captures the cross-graph information.
Table \ref{table:model_ablation} reports the performances with  after removing these components from Dual-AMN.
Among all these components, MHE has the greatest impact on performance.
Without MHE, the performance is degraded by at least  on .
Only stacking GNN layers cannot fully capture the global information, it is necessary to concatenate the output embeddings of each layer explicitly.
Besides, the remaining three components also show the necessity as our expectation.
On average, adopting these technologies improves performance by  to .
By adopting these new designs, Dual-AMN further breaks the ceiling of EA accuracy.

\begin{figure}
  \centering
  \includegraphics[width=1\linewidth]{loss.pdf}
  \caption{Ablation experiment of loss on .
  Tri. represents Triplet Loss;
  TUNS represents Triplet loss with \emph{Truncated Uniform Negative Sampling Strategy.}
  For TUNS, top- nearest neighbors are selected as negative samples, following the same setting in most EA methods'.
  NHSM represents our \emph{Normalized Hard Sample Mining Loss.}}\label{Figure:loss}
\end{figure}
\noindent
\textbf{Ablation Experiment of Loss}.
Besides architecture, the \emph{Normalized Hard Sample Mining Loss} is also one of our main contributions.
To verify its effectiveness, we compare it with several common loss functions.
The results are visualized in Fig \ref{Figure:loss}.
Compared with the other three, the proposed loss could make the model converge faster and achieve the best performance.
\emph{Truncated Uniform Negative Sampling Strategy} also has a similar decent performance.
However, as we have mentioned, this sampling strategy requires massive time consumption.
Since most of the negative samples are redundant, the Triplet loss has the worst efficiency of all loss functions.
In our experiments, the Triplet loss function usually needs thousands of epochs to converge, and the performance is lower than the proposed loss about .
The performance of \emph{Softmax} with \emph{Cross-Entropy} is stronger than Triplet loss, but there is obviously a performance gap with \emph{Normalized Hard Sample Mining Loss}.
These experimental results show that the proposed loss function significantly increases the convergence speed without losing any accuracy.
\subsection{Relation Interpretability}
\begin{table}[t]
\center
\resizebox{0.9\linewidth}{!}{
\renewcommand\arraystretch{1}
\begin{tabular}{c|c|c}
  \toprule
  Importance &  & Examples \\
  \toprule
\multirow{2}{*}{High} &\multirow{2}{*}{}& , , , , \\
        &&,  \\
  \hline
  \multirow{2}{*}{Medium} &\multirow{2}{*}{}& , , , , \\
        && \\
  \hline
  \multirow{2}{*}{Low} &\multirow{2}{*}{}& , , , \\
        &&,  \\
  \bottomrule
\end{tabular}
}
\caption{Relation examples of different importance.}
\label{sample}
\end{table}

In addition to the performance and speed advantages, our model also has a certain degree of interpretability.
Because the weights of adjacent entities are determined by the relations between them, thus these weights can reflect the importance of different relations to some extent.
The importance of each relation is obtained by the following equation:

We train the model on the  and output the importance  of relations.
After clustering the relations according to , we obtain the Table \ref{sample}.
From the observation, we summarize an interesting phenomenon.
The relations with high importance (i.e., meta-path) are usually able to identify the entity from another.
For example, if holding an  entity and  relation, we can reduce the potential options down to a small space.
However,  relation does not have this ability. A country can have many presidents, so its importance becomes extremely low.
Of course, this is inseparable from the characteristics of the  dataset, which contains a large number of celebrities, especially the president, prime minister, and so on.
Therefore, such kinds of relations become unimportant in this dataset.

\begin{figure}
  \centering
  \includegraphics[width=1\linewidth]{error.pdf}
  \caption{ of the entities with different degrees.}\label{Figure:error}
\end{figure}

\subsection{Degree Analysis}
The main experiments show that the performances of all EA methods on sparse datasets are much lower than that of standard datasets.
In order to further explore the correlation between model performance and dataset density, we design an experiment on .
Figure \ref{Figure:error} shows the  of the three variants on different levels of entity degrees.
We observe a strong correlation between performance and degree.
As the degree increasing, the model performance improves significantly.
For Dual-AMN, the  of the entities with one neighbor is only .
The introduction of semi-supervised strategy improves the overall performance of the model, but it has a limited effect on those entities with extremely sparse local structures.
In sparse graphs, it is difficult to make correct inferences only based on limited structural information.
On the other hand, Dual-AMN (Lit) has much higher performance when the degree value is small, which proves that the incorporation of literal information effectively improves the accuracy of these sparse entities.
However, this strategy cannot work on the datasets without literal information.
Therefore, how to better represent these sparse entities without extra information is a key point of future work.

\section{Conclusion}
Over complex graph encoders and inefficient negative sampling strategies lead to the general inefficiency of existing EA methods, resulting in difficulty for applying on large-scale KGs.
In this paper, we propose a novel KG encoder \emph{Dual Attention Matching Network} (Dual-AMN), which not only models both intra-graph and cross-graph relations smartly but also greatly reduces computational complexity.
To replace the inefficient sampling strategy, we propose \emph{Normalized Hard Sample Mining Loss} to cut down the sampling consumption and accelerate the convergence speed.
These two modifications enable the proposed model to achieve the SOTA performance while the speed is several times than other EA methods.
The main experiments indicate that our method outperforms competitors across all datasets and metrics.
Furthermore, we design auxiliary experiments to demonstrate the effectiveness of each component and the interpretability of the model.

\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

\end{document}
