\documentclass[runningheads]{llncs}
\usepackage[vlined]{algorithm2e}

\newcommand{\Oh}[1]
    {\ensuremath{\mathcal{O}\!\left( {#1} \right)}}
\newcommand{\sfX}{\ensuremath{\mathsf{X}}}

\begin{document}

\title{Grammar-Based Compression\\in a Streaming Model}
\toctitle{Grammar-Based Compression in a Streaming Model}
\author{Travis Gagie\inst{1}\fnmsep\thanks
    {Funded by the Millennium Institute for Cell Dynamics and Biotechnology (ICDB),
    Grant ICM P05-001-F, Mideplan, Chile.}
    \and Pawe\l\ Gawrychowski\inst{2}}
\authorrunning{T. Gagie and P. Gawrychowski}
\tocauthor{Travis Gagie and Pawe\l\ Gawrychowski}
\institute{Department of Computer Science\\
    University of Chile\\
    \email{travis.gagie@gmail.com}
    \\\mbox{}\\
    \and Institute of Computer Science\\
    University of Wroc{\l}aw, Poland\\
    \email{gawry1@gmail.com}}
\maketitle

\begin{abstract}
We show that, given a string  of length , with constant memory and logarithmic passes over a constant number of streams we can build a context-free grammar that generates  and only  and whose size is within an -factor of the minimum .  This stands in contrast to our previous result that, with polylogarithmic memory and polylogarithmic passes over a single stream, we cannot build such a grammar whose size is within any polynomial of .
\end{abstract}

\section{Introduction} \label{sec:intro}

In the past decade, the ever-increasing amount of data to be stored and manipulated has inspired intense interest in both grammar-based compression and streaming algorithms, resulting in many practical algorithms and upper and lower bounds for both problems.  Nevertheless, there has been relatively little study of grammar-based compression in a streaming model.  In a previous paper~\cite{Gag09} we proved limits on the quality of the compression we can achieve with polylogarithmic memory and polylogarithmic passes over a single stream.  In this paper we show how to achieve better compression with constant memory and logarithmic passes over a constant number of streams.

For grammar-based compression of a string  of length , we try to build a small context-free grammar (CFG) that generates  and only .  This is useful not only for compression but also for, e.g., indexing~\cite{CN09,Lif07} and speeding up dynamic programs~\cite{LMWZ09}.  (It is sometimes desirable for the CFG to be in Chomsky normal form (CNF), in which case it is also known as a straight-line program.)  We can measure our success in terms of universality~\cite{KY00}, empirical entropy~\cite{NR08} or the ratio between the size of our CFG and the size  of the smallest such grammar.  In this paper we consider the third and last measure.  Storer and Szymanski~\cite{SS82} showed that determining the size of the smallest grammar is NP-complete; Charikar et al.~\cite{CLLP+05} showed it cannot be approximated to within a small constant factor in polynomial time unless P\,\,NP, and that even approximating it to within a factor of  in polynomial time would require progress on a well-studied algebraic problem.  Charikar et al. and Rytter~\cite{Ryt03} independently gave -approximation algorithms, both based on turning the LZ77~\cite{ZL77} parse of  into a CFG and both initially presented at conference in 2002; Sakamoto~\cite{Sak05} then proposed another -approximation algorithm, based on Re-Pair~\cite{LM00}.  Sakamoto, Kida and Shimozono~\cite{SKS04} gave a linear-time -approximation algorithm that uses  workspace, again based on LZ77; together with Maruyama, they~\cite{SMKS09} recently modified their algorithm to run in  time but achieve an  approximation ratio.

A few years before Charikar et al.'s and Rytter's papers sparked a surge of interest in grammar-based compression, a paper by Alon, Matias and Szegedy~\cite{AMS99} did the same for streaming algorithms.  We refer the reader to Babcock et al.~\cite{BBDMW02} and Muthukrishnan~\cite{Mut05} for a thorough introduction to streaming algorithms.  In this paper, however, we are most concerned with more powerful streaming models than these authors consider, ones that allow the use of multiple streams.  A number of recent papers have considered such models, beginning with Grohe and Schweikardt's~\cite{GS05} definition of -bounded Turing Machines, which use at most  reversals over  ``external-memory'' tapes and a total of at most  space on ``internal-memory'' tapes to which they have unrestricted access.  While Munro and Paterson~\cite{MP80} proved tight bounds for sorting with one tape three decades ago, Grohe and Schweikardt proved the first tight bounds for sorting with multiple tapes.  Grohe, Hernich and Schweikardt~\cite{GHS09} proved lower bounds in this model for randomized algorithms with one-sided error, and Beame, Jayram and Rudra~\cite{BJR07} proved lower bounds for algorithms with two-sided error (renaming the model ``read/write streams'').  Beame and Huynh~\cite{BH08} revisited the problem considered by Alon, Matias and Szegedy, i.e., approximating frequency moments, and proved lower bounds for read/write stream algorithms.  Hernich and Schweikardt~\cite{HS08} related results for read/write stream algorithms to results in classical complexity theory, including results by Chen and Yap~\cite{CY91} on reversal complexity.  Hernich and Schweikardt's paper drew our attention to a theorem by Chen and Yap implying that, if a problem can be solved deterministically with read-only access to the input and logarithmic workspace then, in theory, it can be solved with constant memory and logarithmic passes (in either direction) over a constant number of read/write streams.  This theorem is the key to our main result in this paper.  Unfortunately, the constants involved in Chen and Yap's construction are enormous; we leave as future work finding a more practical proof of our results.

The study of compression in something like a streaming model goes back at least a decade, to work by Sheinwald, Lempel and Ziv~\cite{SLZ95} and De Agostino and Storer~\cite{DS96}.  As far as we know, however, our joint paper with Manzini~\cite{GM07} was the first to give nearly tight bounds in a standard streaming model.  In that paper we proved nearly matching bounds on the compression achievable with a constant amount of internal memory and one pass over the input, as well as upper and lower bounds for LZ77 with a sliding window whose size grows as a function of the number of characters encoded.  (Our upper bound for LZ77 used a theorem due to Kosaraju and Manzini~\cite{KM99} about quasi-distinct parsings, a subject recently revisited by Amir, Aumann, Levy and Roshko~\cite{AALR09}.)  Shortly thereafter, Albert, Mayordomo, Moser and Perifel~\cite{AMMP08} showed the compression achieved by LZ78~\cite{ZL78} is incomparable to that achievable by pushdown transducers; Mayordomo and Moser~\cite{MM09} then extended their result to show both kinds of compression are incomparable with that achievable by online algorithms with polylogarithmic memory.  (A somewhat similar subject, recognition of the context-sensitive Dyck languages in a streaming model, was recently broached by Magniez, Mathieu and Nayak~\cite{MMN09}, who gave a one-pass algorithm with one-sided error that uses polylogarithmic time per character and  space.)  In a recent paper with Ferragina and Manzini~\cite{FGM10} we demonstrated the practicality of streaming algorithms for compression in external memory.

In a recent paper~\cite{Gag09} we proved several lower bounds for compression algorithms that use a single stream, all based on an automata-theoretic lemma: suppose a machine implements a lossless compression algorithm using sequential accesses to a single tape that initially holds the input; then we can reconstruct any substring given, for every pass, the machine's configurations when it reaches and leaves the part of the tape that initially holds that substring, together with all the output it generates while over that part.  (We note similar arguments appear in computational complexity, where they are referred to as ``crossing sequences'', and in communication complexity.)  It follows that, if a streaming compression algorithm is restricted to using polylogarithmic memory and polylogarithmic passes over one stream, then there are periodic strings with polylogarithmic periods such that, even though the strings are very compressible as, e.g., CFGs, the algorithm must encode them using a linear number of bits; therefore, no such algorithm can approximate the smallest-grammar problem to within any polynomial of the minimum size.  Such arguments cannot prove lower bounds for algorithms with multiple streams, however, and we left open the question of whether extra streams allow us to achieve a polynomial approximation.  In this paper we use Chen and Yap's result to confirm they do: we show how, with logarithmic workspace, we can compute the LZ77 parse and turn that into a CFG in CNF while increasing the size by a factor of  --- i.e., at most polynomially in .  It follows that we can achieve that approximation ratio while using constant memory and logarithmic passes over a constant number of streams.

\section{LZ77 in a Streaming Model} \label{sec:LZ77}

Our starting point is the same as that of Charikar et al., Rytter and Sakamoto, Kida and Shimozono, but we pay even more attention to workspace than the last set of authors.  Specifically, we begin by considering the variant of LZ77 considered by Charikar et al. and Rytter, which does not allow self-referencing phrases but still produces a parse whose size is at most as large as that of the smallest grammar.  Each phrase in this parse is either a single character or a substring of the prefix of  already parsed.  For example, the parse of ``{\sf how-much-wood-would-a-woodchuck-chuck-if-a-woodchuck-could-chuck-wood?}'' is ``{\sf how-much-wo
\linebreak
od-would-a-woodchuck-chuck-if-a-woodchuck-could-chuck-wood?}''.

\begin{lemma}[Charikar et al., 2002; Rytter, 2002] \label{lem:C+R02}
The number of phrases in the LZ77 parse is a lower bound on the size of the smallest grammar.
\end{lemma}

\noindent As an aside, we note that Lemma~\ref{lem:C+R02} and results from our previous paper~\cite{Gag09} together imply we cannot compute the LZ77 parse with one stream when the product of the memory and passes is sublinear in .

It is not difficult to show that this LZ77 parse --- like the original --- can be computed with read-only access to the input and logarithmic workspace.  Pseudocode for doing this appears as Algorithm~\ref{alg:parse}.  On the example above, this pseudocode produces ``{\sf how-much-wood(9,3)ul(13,2)a(9,5)(7,2)(6,2)k-(27,6)if(20,14)
\linebreak
(16,5)(27,6)(10,4)?}''.

\begin{lemma} \label{lem:lz77}
We can compute the LZ77 parse with logarithmic workspace.
\end{lemma}

\begin{proof}
The first phrase in the parse is the first letter in ; after outputting this, we always keep a pointer  to the division between the prefix already parsed and the suffix yet to be parsed.  To compute each later phrase in turn, we check the length of the longest common prefix of  and , for ; if the longest match has length 0 or 1, we output ; otherwise, we output the value of the minimal  that maximizes the length of the longest common prefix, together with that length.  This takes a constant number of pointers into  and a constant number of -bit counters. \qed
\end{proof}

\begin{algorithm}[t]
\;
\While{}
    {\;
    \;
    \For{}
        {\;
        \While{}
            {\;}
        \If{}
            {\;
            \;}}
    \eIf{}
        {print \;
        \;}
        {print \;
        \;}}
\BlankLine
\BlankLine

\caption{pseudocode for computing the LZ77 parse in logarithmic workspace}
\label{alg:parse}
\end{algorithm}

Combined with Chen and Yap's theorem below, Lemma~\ref{lem:lz77} implies that we can compute the LZ77 parse with constant workspace and logarithmic passes over a constant number of streams.

\begin{theorem}[Chen and Yap, 1991] \label{thm:CY91}
If a function can be computed with logarithmic workspace, then it can be computed with constant workspace and logarithmic passes over a constant number of streams.
\end{theorem}

As an aside, we note that Chen and Yap's theorem is actually much stronger than what we state here: they proved that, if  is reversal-computable (see~\cite{CY91} or~\cite{HS08} for an explanation) and a problem can be solved deterministically in  time, then it can be solved with constant workspace and  passes over a constant number of tapes.  Chen and Yap showed how a reversal-bounded Turing machine can simulate a space-bounded Turing machine by building a table of the possible configurations of the space-bounded machine.  Schweikardt~\cite{Sch07} pointed out that ``this is of no practical use, since the resulting algorithm produces huge intermediate results, but it is of major theoretical interest'' because it implies that a number of lower bounds are tight.  We leave as future work finding a more practical proof our our results.

In the next section we prove the following lemma, which is the most technical part of this paper.  By the size of a CFG, we mean the number of symbols on the righthand sides of the productions; notice this is at most a logarithmic factor less than the number of bits needed to express the CFG.

\begin{lemma} \label{lem:cfg}
With logarithmic workspace we can turn the LZ77 parse into a CFG whose size is within a -factor of minimum.
\end{lemma}

\noindent Together with Lemma~\ref{lem:lz77} and Theorem~\ref{thm:CY91}, Lemma~\ref{lem:cfg} immediately implies our main result.

\begin{theorem}
With constant workspace and logarithmic passes over a constant number of streams, we can build a CFG generating  and only  whose size is within a -factor of minimum.
\end{theorem}

\section{Logspace CFG Construction} \label{sec:grammar}

Unlike the LZ78 parse, the LZ77 parse cannot normally be viewed as a CFG, because the substring to which a phrase matches may begin or end in the middle of a preceding phrase.  We note this obstacle has been considered by other authors in other circumstances, e.g., by Navarro and Raffinot~\cite{NR04} for pattern matching.  Fortunately, we can remove this obstacle in logarithmic workspace, without increasing the number of phrases more than quadratically.  To do this, for each phrase for which we output , we ensure  is the first character in a phrase and  is the last character in a phrase (by breaking phrases in two, if necessary).  For example, the parse
\begin{center}
``{\sf how-much-wood-would-a-woodchuck-chuck
\linebreak
-if-a-woodchuck-could-chuck-wood?}''
\end{center}
becomes
\begin{center}
``{\sf how-much-wood-w\ \raisebox{-.6ex}{\rule{.3ex}{2.5ex}}ould\ \raisebox{-.6ex}{\rule{.3ex}{2.5ex}}-a-woodc\ \raisebox{-.6ex}{\rule{.3ex}{2.5ex}}huck-c\ \raisebox{-.6ex}{\rule{.3ex}{2.5ex}}huck
\linebreak
-if-a-woodchuck-could-chuck-wood?}'',
\end{center}
where the thick lines indicate new breaks and superscripts indicate which breaks cause the new ones (which are subscripted).  Notice the break ``{\sf a-woodchuck-could}'' causes both ``{\sf w\ \raisebox{-.6ex}{\rule{.3ex}{2.5ex}}ould}'' (matching ``{\sf ould}'') and ``{\sf a-woodchuck-c\ \raisebox{-.6ex}{\rule{.3ex}{2.5ex}}huck}'' (matching ``{\sf a-woodchuck-c}''); in turn, the latter new break causes ``{\sf woodc\ \raisebox{-.6ex}{\rule{.3ex}{2.5ex}}huck}'' (matching ``{\sf huck}''), which is why it has a superscript 3.

\begin{lemma} \label{lem:breaks}
Breaking the phrases takes at most logarithmic workspace and at most squares the number of phrases.  Afterwards, every phrase is either a single character or the concatenation of complete, consecutive, preceding phrases.
\end{lemma}

\begin{proof}
Since the phrases' start points are the partial sums of their lengths, we can compute them with logarithmic workspace; therefore, we can assume without loss of generality that the start points are stored with the phrases.  We start with the rightmost phrase and work left.  For each phrase's endpoints, we compute the corresponding position in the matching, preceding substring (notice that the position corresponding to one phrase's finish may not be the one corresponding to the start of the next phrase to the right) and insert a new break there, if there is not one already.  If we have inserted a new break, then we iterate, computing the position corresponding to the new break; eventually, we will reach a point where there is already a break, so the iteration will stop.  This process requires only a constant number of pointers, so we can perform it with logarithmic workspace.  Also, since each phrase is broken at most twice for each of the phrases that initially follow it in the parse, the final number of phrases is at most the square of the initial number.  By inspection, after the process is complete every phrase is the concatenation of complete, consecutive, preceding phrases. \qed
\end{proof}

Notice that, after we break the phrases as described above, we can view the parse as a CFG.  For example, the parse for our running example corresponds to

where  is the starting nonterminal.  Unfortunately, while the number of productions is polynomial in the number of phrases in the LZ77 parse, it is not clear the size is and, moreover, the grammar is not in CNF.  Since all the righthand sides of the productions are either terminals or sequences of consecutive nonterminals, we could put the grammar into CNF by squaring the number of nonterminals --- giving us an approximation ratio cubic in .  This would still be enough for us to prove our main result but, fortunately, such a large increase is not necessary.

\begin{lemma} \label{lem:cnf}
Putting the CFG into CNF takes logarithmic workspace and increases the number of productions by at most a logarithmic factor.  Afterwards, the size of the grammar is proportional to the number of productions.
\end{lemma}

\begin{proof}
We build a forest of complete binary trees whose leaves are the nonterminals: if we consider the trees in order by size, the nonterminals appear in order from the leftmost leaf of the first tree to the rightmost leaf of the last tree; each tree is as large as possible, given the number of nonterminals remaining after we build the trees to its left.  Notice there are  such trees, of total size at most .  We then assign a new nonterminal to each internal node and output a production which takes that nonterminal to its children.  This takes logarithmic workspace and increases the number of productions by a constant factor.

Notice any sequence of consecutive nonterminals that spans at least two trees, can be written as the concatenation of two consecutive sequences, one of which ends with the rightmost leaf in one tree and the other of which starts with the leftmost leaf in the next tree.  Consider a sequence ending with the rightmost leaf in a tree; dealing with one that starts with a leftmost leaf is symmetric.  If the sequence completely contains that tree, we can write a binary production that splits the sequence into the prefix in the preceding trees, which is the expansion of a new nonterminal, and the leaves in that tree, which are the expansion of its root.  We need do this  times before the remaining subsequence is contained within a single tree.  After that, we repeatedly produce new binary productions that split the subsequence into prefixes, again the expansions of new nonterminals, and suffixes, the expansions of roots of the largest possible complete subtree.  Since the size of the largest possible complete subtree shrinks by a factor of two at each step (or, equivalently, the height of its root decreases by 1), we need repeat  times.  Again, this takes logarithmic workspace (we will give more details in the full version of this paper).

In summary, we may replace each production with  new, binary productions.  Since the productions are binary, the number of symbols on the righthand sides is linear in the number of productions themselves. \qed
\end{proof}

Lemma~\ref{lem:cnf} is our most detailed result, and the diagram below showing the construction with our running example is also somewhat detailed.  On the left are modifications of the original productions, now made binary; in the middle are productions for the internal nodes of the binary trees; and on the right are productions breaking down the consecutive subsequences that appear on the righthand sides of the productions in the left column, until the subsequences are single, original nonterminals or nonterminals for nodes in the binary trees (i.e., those on the lefthand sides of the productions in the middle column).



Combined with Lemma~\ref{lem:C+R02}, Lemmas~\ref{lem:breaks} and~\ref{lem:cnf} imply that with logarithmic workspace we can build a CFG in CNF whose size is .  We can use a similar approach with binary trees to build a CFG in CNF of size  that generates  and only , still using logarithmic workspace.  If we combine all non-terminals that have the same expansion, which also takes logarithmic workspace, then this becomes Kieffer, Yang, Nelson and Cosman's~\cite{KYNC00} {\sc Bisection} algorithm, which gives an -approximation~\cite{CLLP+05}.  By taking the smaller of these two CFGs we achieve an -approximation.  Therefore, as we claimed in Lemma~\ref{lem:cfg}, with logarithmic workspace we can turn the LZ77 parse into a CFG whose size is within a -factor of minimum.

\section{Recent Work} \label{sec:recent}

We recently improved the bound on the approximation ratio in Lemma~\ref{lem:cfg} from  to .  The key observation is that, by the definition of the LZ77 parse, the first occurrence of any substring must touch or cross a break between phrases.  Consider any phrase in the parse obtained by applying Lemma~\ref{lem:breaks} to the LZ77 parse.  By the observation above, that phrase can be written as the concatenation of some consecutive new phrases (all contained within one old phrase and ending at that old phrase's right end), some consecutive old phrases, and some more consecutive new phrases (all contained within one old phrase and starting at the old phrase's left end).  Since there are  old phrases, there are  sequences of consecutive old phrases; since there are  new phrases, there are  sequences of consecutive new phrases that are contained in one old phrase and either start at that old phrase's right end or end at that old phrase's left end.

While working on the improvement above, we realized how to improve the bound further, to .  To do this, we choose a value  between 2 and  and, for , we associate a nonterminal to each of the  blocks of  characters to the left and right of each break; we thus start building the grammar with  nonterminals.  We then add  binary productions such that any sequence of nonterminals associated with a consecutive sequence of blocks, can be derived from  nonterminals.  Notice any substring is the concatenation of 0 or 1 partial blocks, some number of full blocks to the left of a break, some number of blocks to the right of a break, and 0 or 1 more partial blocks.  We now add more binary productions as follows: we start with  (the only block of length ; find the first break it touches or crosses (in this case it is the start of ); consider  as the concatenation of blocks of size  (in this case only the rightmost block can be partial); associate nonterminals to the partial blocks (if they exist); add  productions to take the symbol associated to  (in this case, the start symbol) to the sequence of nonterminals associated with the smaller blocks in order from left to right; and recurse on each of the smaller blocks.  To guarantee each smaller block touches or crosses a break, we work on the first occurrence in  of the substring contained in that block.  We stop recursing when the block size is 1, and add  productions taking those blocks' nonterminals to the appropriate characters.

Analysis shows that the number of productions we add during the recursion is proportional to the number of blocks involved, either full or partial.  Since the number of distinct full blocks in any level of recursion is  and the number of partial blocks is at most twice the number of blocks (full or partial) in the previous level of recursion, the number of productions we add during the recursion is .  Therefore, the grammar has size ; when , this is .  The first of the two key observations that let us build the grammar in logarithmic workspace, is that we can store the index of a block (full or partial) with respect to the associated break, in  bits; therefore, we can store  indices for each of the  levels of recursion, in a total of  bits.  The second key observation is that, given the indices of the block we are working on in each level of recursion, with respect to the appropriate break, we can compute the start point and end point of the block we are currently working on in the deepest level of recursion.  We will give details of these two improvements in the full version of this paper.

While working on this second improvement, we realized that we can use the same ideas to build a compressed representation that allows efficient random access.  We refer the reader to the recent papers by Kreft and Navarro~\cite{KN10} and Bille, Landau and Weimann~\cite{BLW10} for background on this problem.  Suppose that, for each of the  full blocks described above, we store a pointer to the first occurrence in  of the substring in that block, as well as a pointer to the first break that first occurrence touches or crosses.  Notice this takes a total of  bits.  Then, given a block's index and an offset in that block, in  time we can compute a smaller block's index and offset in that smaller block, such that the characters in those two positions are equal; if the larger block has size 1, in  time we can return the character.  Since  itself is a block, an offset in it is just a character's position, and there are  levels of recursion, it follows that we can access any character in  time.  Further analysis shows that it takes  time to access a substring of length .  Of course, for any positive constant , if we are willing to use  bits of space, then we can access any character in constant time.  If we make the data structure slightly larger and more complicated --- e.g., storing searchable partial sums at the block boundaries --- then, at least for strings over fairly small alphabets, we can also support fast rank and select queries.

This implementation makes it easy to see the data structure's relation to LZ77 and grammar-based compression.  We can use a simpler implementation, however, and use LZ77 only in the analysis.  Suppose that, for , we break  into consecutive blocks of length  (the last block may be shorter), always starting from the first character of .  For each block, we store a pointer to the first occurrence in  of that block's substring.  Given a block's index and an offset in that block, in  time we can again compute a smaller block's index and offset in that smaller block, such that the characters in those two positions are equal: the new block's index is the sum of pointer and the old offset, divided by the new block length and rounded down; the new offset is the sum of the pointer and the old offset, modulo the new block length.  We can discard any block that cannot be visited during a query, so this data structure takes at most a constant factor more space than the one described above.  Indeed, this data structure seems likely to be smaller in practice, because blocks of the same size can overlap in the previous data structure but cannot in this one.  We plan to implement this data structure and report the results in a future paper.

\bibliographystyle{splncs}
\bibliography{lata10}

\end{document} 