\vspace{-5pt}
\section{Experiments}

\begin{table*}[t]
    \centering
    \small
    
    \begin{tabular}{c|clccccccc}
        \toprule
      General Arch.  &  Token Mixer    & Outcome Model    & Image Size & Params (M)  & MACs (G) & Top-1 (\%) \\
        \whline
    {\multirow{5}{*}{\makecell[c]{Convolutional \\ Neural Netowrks}}}  & \multirow{5}{*}{---}   & \resnetdot{} RSB-ResNet-18 \cite{resnet, resnet_improved} & 224 & 12 & 1.8 & 70.6 \\
         &   & \resnetdot{} RSB-ResNet-34 \cite{resnet, resnet_improved} & 224 & 22 & 3.7 & 75.5 \\
         &   & \resnetdot{} RSB-ResNet-50 \cite{resnet, resnet_improved} & 224 & 26 & 4.1 & 79.8 \\
         &   & \resnetdot{} RSB-ResNet-101 \cite{resnet, resnet_improved} & 224 &  45 & 7.9 & 81.3 \\
         &   & \resnetdot{} RSB-ResNet-152 \cite{resnet, resnet_improved} & 224 & 60 & 11.6 & 81.8 \\
        \hline
         
   \multirow{22}{*}{MetaFormer}  & \multirow{8}{*}{Attention} & \vitdot{} ViT-B/16 \cite{vit} & 224 &  86 & 17.6 & 79.7 \\
             &          & \vitdot{} ViT-L/16 \cite{vit} & 224 & 307 & 63.6 & 76.1 \\
             &          & \deitdot{} DeiT-S \cite{deit} & 224 & 22 & 4.6 & 79.8 \\
             &          & \deitdot{} DeiT-B \cite{deit} & 224 &  86 & 17.5 & 81.8 \\
             &          & \pvtdot{} PVT-Tiny \cite{pvt} & 224 & 13 & 1.9 & 75.1 \\
             &          & \pvtdot{} PVT-Small \cite{pvt} & 224 & 25 & 3.8 & 79.8 \\
             &          & \pvtdot{} PVT-Medium \cite{pvt} & 224 & 44 &  6.7 & 81.2 \\
             &          & \pvtdot{} PVT-Large \cite{pvt} & 224 & 61 &  9.8 & 81.7 \\
    \cline{2-7}
             & \multirow{9}{*}{Spatial MLP} & \mlpmixerdot{} MLP-Mixer-B/16 \cite{mlp-mixer} & 224 & 59 & 12.7 & 76.4 \\
             &             & \resmlp{} ResMLP-S12 \cite{resmlp} & 224 & 15 & 3.0 & 76.6 \\
             &             & \resmlp{} ResMLP-S24 \cite{resmlp} & 224 & 30 & 6.0 & 79.4 \\
             &             & \resmlp{} ResMLP-B24 \cite{resmlp} & 224 & 116 & 23.0 & 81.0 \\
             &             & \swinmixer{} Swin-Mixer-T/D24 \cite{swin} & 256 & 20 & 4.0 & 79.4 \\
             &             & \swinmixer{} Swin-Mixer-T/D6 \cite{swin} & 256 & 23 & 4.0 & 79.7 \\
             &             & \swinmixer{} Swin-Mixer-B/D24 \cite{swin} & 224 & 61 & 10.4 & 81.3 \\
             &             & \gmlp{} gMLP-S \cite{gmlp} & 224 & 20 & 4.5 & 79.6 \\
             &             & \gmlp{} gMLP-B \cite{gmlp} & 224 & 73 & 15.8 & 81.6 \\
    \cline{2-7}
             & \multirow{5}{*}{Pooling}  & \poolformer{} PoolFormer-S12 & 224 & 12 & 1.8 & 77.2 \\ &         & \poolformer{} PoolFormer-S24 & 224 & 21 & 3.4 & 80.3 \\
             &         & \poolformer{} PoolFormer-S36 & 224 & 31 & 5.0 & 81.4 \\ &         & \poolformer{} PoolFormer-M36 & 224 & 56 & 8.8 & 82.1 \\
             &         & \poolformer{} PoolFormer-M48 & 224 & 73 & 11.6 & 82.5 \\
    \bottomrule
             
    \end{tabular}     \vspace{-3mm}
    \caption{
    \textbf{Performance of different types of models on ImageNet-1K  classification.} 
    All these models are only trained on the ImageNet-1K training set and the accuracy on the validation set is reported. RSB-ResNet means the results are from ``ResNet Strikes Back" \cite{resnet_improved} where ResNet \cite{resnet} is trained with improved training procedure for 300 epochs.  denotes results of ViT trained with extra regularization from \cite{mlp-mixer}. The numbers of MACs of PoolFormer are counted by \texttt{fvcore} \cite{fvcore} library.
    \label{tab:imagenet}}
    \vspace{-2mm}
\end{table*}

\begin{figure*}[t]
  \centering
   \includegraphics[width=0.95\linewidth]{figures/MetaFormer_full_plot.pdf}
   \vspace{-4mm}
   \caption{\textbf{ImageNet-1K validation accuracy \vs MACs/Model Size.} RSB-ResNet means the results are from ``ResNet Strikes Back" \cite{resnet_improved} where ResNet \cite{resnet} is trained with improved training procedure for 300 epochs.}
   \label{fig:overall_comparision}
\vspace{-1mm}
\end{figure*}


\subsection{Image classification}
\myPara{Setup} 
ImageNet-1K \cite{imagenet} is one of the most widely used datasets in computer vision. It contains about 1.3M training images and 50K validation images, covering common 1K classes. Our training scheme mainly follows \cite{deit} and \cite{cait}. Specifically, MixUp \cite{mixup}, CutMix \cite{cutmix}, CutOut \cite{cutout} and RandAugment \cite{randaugment} are used for data augmentation. The models are trained for 300 epochs using AdamW optimizer \cite{adam, adamw} with weight decay  and peak learning rate  (batch size 4096 and learning rate  are used in this paper). The number of warmup epochs is 5 and cosine schedule is used to decay the learning rate. Label Smoothing \cite{label_smoothing} is set as 0.1. Dropout is disabled but stochastic depth \cite{stochastic_depth} and LayerScale \cite{cait} are used to help train deep models. 
We modified Layer Normalization \cite{layer_norm} to compute the mean and variance along token and channel dimensions compared to only channel dimension in vanilla Layer Normalization. Modified Layer Normalization (MLN) can be implemented for channel-first data format with GroupNorm API in PyTorch by specifying the group number as 1. MLN is preferred by PoolFormer as shown in Section \ref{sec:ablation}. See the appendix for more details on hyper-parameters. Our implementation is based on the \texttt{Timm} codebase \cite{timm} and the experiments are run on TPUs. 


\myPara{Results} 
Table \ref{tab:imagenet} shows the performance of PoolFormers on ImageNet classification. Qualitative results are shown in the appendix. Surprisingly, despite the simple pooling token mixer, PoolFormers can still achieve highly competitive performance compared with CNNs and other MetaFormer-like models. For example, PoolFormer-S24 reaches the top-1 accuracy of more than 80 while only requiring 21M parameters and 3.4G MACs. Comparatively, the well-established ViT baseline DeiT-S \cite{deit}, attains slightly worse accuracy of 79.8 and requires 35\% more MACs (4.6G). To obtain similar accuracy, MLP-like model ResMLP-S24 \cite{resmlp} needs 43\% more parameters (30M) as well as 76\% more computation (6.0G) while only 79.4 accuracy is attained. Even compared with more improved ViT and MLP-like variants \cite{pvt, gmlp}, PoolFormer still shows better performance. Specifically, the pyramid Transformer PVT-Medium obtains 81.2 top-1 accuracy with 44M parameters and 6.7G MACs while PoolFormer-S36 reaches 81.4 with 30\% fewer parameters (31M) and 25\% fewer MACs (5.0G) than those of PVT-Medium. 


Besides, compared with RSB-ResNet (``ResNet Strikes Back") \cite{resnet_improved} where ResNet \cite{resnet} is trained with improved training procedure for the same 300 epochs, PoolFormer still performs better. With  22M parameters/3.7G MACs, RSB-ResNet-34 \cite{resnet_improved} gets 75.5 accuracy while PoolFormer-S24 can obtain 80.3. Since the local spatial modeling ability of the pooling layer is much worse than the neural convolution layer, the competitive performance of PoolFormer can only be attributed to its general architecture MetaFormer.


With the pooling operator,  each token evenly aggregates the features from its nearby tokens. Thus it is an extremely basic token mixing operation. However, the experiment results show that even with this embarrassingly simple token mixer, MetaFormer still obtains highly competitive performance. 
Figure \ref{fig:overall_comparision} clearly shows that PoolFormer surpasses other models with fewer MACs and parameters. 
This finding conveys that the general architecture MetaFormer is actually what we need when designing vision models. By adopting MetaFormer, it is guaranteed that the derived models would have the potential to achieve reasonable performance. 

\begin{table*}[t]
\small
\centering
\setlength{\tabcolsep}{2pt}
\begin{tabular}{l|c|ccc|ccc|c|ccc|ccc}
\toprule
\multirow{2}{*}{Backbone} & \multicolumn{7}{c|}{RetinaNet 1} & \multicolumn{7}{c}{Mask R-CNN 1} \\
\cline{2-15}
& Params (M) & AP & AP &AP & AP & AP & AP & Params (M) & AP &AP &AP  &AP &AP &AP \\
\whline
\resnetdot{} ResNet-18~\cite{resnet}  & 21.3 & 31.8 & 49.6 & 33.6 & 16.3 & 34.3 & 43.2 & 31.2 &  34.0 & 54.0 & 36.7 & 31.2 & 51.0 & 32.7 \\
\poolformer{} PoolFormer-S12          & 21.7 & 36.2 & 56.2 & 38.2 & 20.8 & 39.1 & 48.0 & 31.6 &  37.3 & 59.0 & 40.1 & 34.6 & 55.8 & 36.9 \\
\hline
\resnetdot{} ResNet-50~\cite{resnet}  & 37.7 & 36.3 & 55.3 & 38.6 & 19.3 & 40.0 & 48.8 & 44.2 &  38.0 & 58.6 & 41.4 & 34.4 & 55.1 & 36.7 \\
\poolformer{} PoolFormer-S24          & 31.1 & 38.9 & 59.7 & 41.3 & 23.3 & 42.1 & 51.8 & 41.0 &  40.1 & 62.2 & 43.4 & 37.0 & 59.1 & 39.6 \\
\hline
\resnetdot{} ResNet-101~\cite{resnet}  & 56.7 & 38.5 & 57.8 & 41.2 & 21.4 & 42.6 & 51.1 & 63.2 &  40.4 & 61.1 & 44.2 & 36.4 & 57.7 & 38.8 \\
\poolformer{} PoolFormer-S36           & 40.6 & 39.5 & 60.5 & 41.8 & 22.5 & 42.9 & 52.4 & 50.5 &  41.0 & 63.1 & 44.8 & 37.7 & 60.1 & 40.0 \\ 
\bottomrule
\end{tabular} \vspace{-3mm}
\caption{\textbf{Performance of object detection using RetinaNet, and object detection and instance segmentation using Mask R-CNN on COCO \texttt{val2017}~\cite{coco}.}  training schedule (\ie 12 epochs) is used for training detection models.  and  represent bounding box AP and mask AP, respectively.}
\label{tab:coco_det} 
\vspace{-3mm}
\end{table*}


\begin{comment}
\begin{table*}[t]
\centering
\setlength{\tabcolsep}{10pt}
\input{tables/coco_det_seg}
\vspace{-2mm}
\caption{\textbf{Performance of object detection and instance segmentation on COCO \texttt{val2017} \cite{coco}}.  and  represent bounding box AP and mask AP, respectively. All models are based on Mask R-CNN and trained by  training schedule (\ie 12 epochs).}
\label{tab:coco_det_seg} 
\end{table*}
\end{comment}


\begin{table}[t]
\small
\centering
\setlength{\tabcolsep}{3.5pt}
\begin{tabular}{l|c|c}
\toprule
\multirow{2}{*}{Backbone} & \multicolumn{2}{c}{Semantic FPN}\\
\cline{2-3}
& Params (M) & mIoU (\%) \\
    \whline
	\resnetdot{} ResNet-18~\cite{resnet}                & 15.5 &  32.9 \\
	\pvtdot{} PVT-Tiny~\cite{pvt}           & 17.0 &  35.7 \\
	\poolformer{} PoolFormer-S12                    & 15.7 &  37.2 \\
	\hline
    \resnetdot{} ResNet-50~\cite{resnet}                & 28.5 &  36.7 \\
    \pvtdot{} PVT-Small~\cite{pvt}          & 28.2 &  39.8 \\
	\poolformer{} PoolFormer-S24                    & 23.2 &  40.3 \\
    \hline
    \resnetdot{} ResNet-101~\cite{resnet}               & 47.5 &  38.8\\
    \resnetdot{} ResNeXt-101-32x4d~\cite{xie2017aggregated} & 47.1 &  39.7 \\
    \pvtdot{} PVT-Medium~\cite{pvt}         & 48.0 & 41.6 \\
    \poolformer{} PoolFormer-S36                    & 34.6 & 42.0 \\
    \hline
    \pvtdot{} PVT-Large~\cite{pvt}          & 65.1 &  42.1 \\
    \poolformer{} PoolFormer-M36                    & 59.8 & 42.4 \\
    \hline
    \resnetdot{} ResNeXt-101-64x4d~\cite{xie2017aggregated} & 86.4 &  40.2 \\
    \poolformer{} PoolFormer-M48                   & 77.1 &  42.7 \\
\bottomrule
\end{tabular} \vspace{-3mm}
\caption{\textbf{Performance of Semantic segmentation on ADE20K~\cite{ade20k} validation set.} All models are equipped with Semantic FPN~\cite{fpn}.}
\label{tab:ade20k}
\normalsize
\vspace{-5mm}
\end{table}


\subsection{Object detection and instance segmentation}
\vspace{-1pt}
\myPara{Setup} We evaluate PoolFormer on the challenging COCO benchmark \cite{coco} that includes 118K training images (\texttt{train2017}) and 5K validation images (\texttt{val2017}). The models are trained on training set and the performance on validation set is reported. PoolFormer is employed as the backbone for two standard detectors, \ie, RetinaNet~\cite{retinanet} and Mask R-CNN~\cite{mask_rcnn}. ImageNet pre-trained weights are utilized to initialize the backbones and Xavier \cite{glorot2010understanding} to initialize the added layers. AdamW~\cite{adam, adamw}  is adopted for training with an initial learning rate of  and batch size of 16. Following \cite{retinanet, mask_rcnn}, we employ 1 training schedule, \ie, training the detection models for 12 epochs. The training images are resized into shorter side of 800 pixels and longer side of no more than 1,333 pixels. For testing, the shorter side of the images is also resized to 800 pixels. The implementation is based on the \texttt{mmdetection} \cite{mmdetection} codebase and the experiments are run on 8 NVIDIA A100 GPUs. 

\myPara{Results}
Equipped with RetinaNet for object detection, PoolFormer-based models consistently outperform their comparable ResNet counterparts as shown in Table \ref{tab:coco_det}. For instance, PoolFormer-S12 achieves 36.2 AP, largely surpassing that of ResNet-18 (31.8 AP). 
Similar results are observed for those models based on Mask R-CNN on object detection and instance segmentation. For example, PoolFormer-S12 largely surpasses ResNet-18 (bounding box AP 37.3 \vs 34.0, and mask AP 34.6 \vs 31.2). 
Overall, for COCO object detection and instance segmentation, PoolForemrs achieve competitive performance, consistently outperforming those counterparts of ResNet. 


\begin{table*}[t]
\small
\centering
\setlength{\tabcolsep}{2pt}
\begin{tabular}{l|l|c c c}
\toprule
	Ablation & Variant & Params (M) & MACs (G) & Top-1 (\%) \\
\whline
Baseline & None (PoolFormer-S12) & 11.9 & 1.8 & 77.2 \\
\hline
\multirow{6}{*}{Token mixers}  & Pooling  Identity mapping & 11.9 & 1.8 & 74.3 \\
             & Pooling  Global random matrix (extra 21M frozen
parameters) & 11.9 & 3.3 & 75.8 \\
             & Pooling  Depthwise Convolution \cite{chollet2017xception, mamalet2012simplifying} & 11.9 & 1.8 & 78.1 \\
             & Pooling size 3  5 & 11.9 & 1.8 & 77.2 \\
             & Pooling size 3  7 & 11.9 & 1.8 & 77.1 \\
             & Pooling size 3  9 & 11.9 & 1.8 & 76.8 \\
\hline
\multirow{3}{*}{Normalization}  & Modified Layer Normalization   Layer Normalization \cite{layer_norm} & 11.9 & 1.8 & 76.5 \\
             & Modified Layer Normalization  Batch Normalization \cite{batch_norm} & 11.9 & 1.8 & 76.4 \\
             & Modified Layer Normalization  None & 11.9 & 1.8 & 46.1 \\
\hline
\multirow{2}{*}{Activation}  & GELU \cite{gelu}   ReLU \cite{relu}  & 11.9 & 1.8 & 76.4 \\
             & GELU  SiLU \cite{silu} & 11.9 & 1.8 & 77.2 \\
\hline
\multirow{2}{*}{Other components}  & Residual connection \cite{gelu}   None  & 11.9 & 1.8 & 0.1 \\
             & Channel MLP  None & 2.5 & 0.2 & 5.7 \\
\hline
\multirow{4}{*}{Hybrid Stages}  & [Pool, Pool, Pool, Pool]   [Pool, Pool,                                 Pool, Attention] & 14.0 & 1.9 & 78.3 \\
                                & [Pool, Pool, Pool, Pool]   [Pool, Pool, Attention, Attention]  & 16.5 & 2.5 & 81.0 \\
                                & [Pool, Pool, Pool, Pool]   [Pool, Pool, Pool, SpatialFC] & 11.9 & 1.8 & 77.5  \\
                                & [Pool, Pool, Pool, Pool]   [Pool, Pool, SpatialFC, SpatialFC] & 12.2 & 1.9 & 77.9 \\
             
\bottomrule
\end{tabular} \vspace{-3mm}
\caption{\textbf{Ablation for PoolFormer on ImageNet-1K classification benchmark.} PoolFormer-S12 is utilized as the baseline to conduct ablation study. The top-1 accuracy on the validation set is reported. This token mixer utilizes global random matrix  (parameters are frozen after random initialization) to conduct token mixing by  where  are input tokens with the token length of  and channel dimension of . Modified Layer Normalization (MLN) computes the mean and variance along token and channel dimensions compared with vanilla Layer Normalization only along channel dimension. MLN can be implemented with GroupNorm API in PyTorch by specifying the group number equal to 1. The numbers of MACs are counted by \texttt{fvcore} \cite{fvcore} library.
}
\label{tab:ablation}
\normalsize
\vspace{-2mm}
\end{table*}


\vspace{-1pt}
\subsection{Semantic segmentation}
\vspace{-3pt}
\myPara{Setup} 
ADE20K~\cite{ade20k}, a challenging scene parsing benchmark, is selected to evaluate the models for semantic segmentation. The dataset includes 20K and 2K images in the training and validation set, respectively, covering 150 fine-grained semantic categories. PoolFormers are evaluated as backbones equipped with Semantic FPN~\cite{fpn}. ImageNet-1K trained checkpoints are used to initialize the backbones while Xavier~\cite{glorot2010understanding} is utilized to initialize other newly added layers. Common practices~\cite{fpn,chen2017deeplab} train models for 80K iterations with a batch size of 16. To speed up training, we double the batch size to 32 and decrease the iteration number to 40K. The AdamW \cite{adam, adamw} is employed with an initial learning rate of  that will decay in the polynomial decay schedule with a power of 0.9. Images are resized and cropped into  for training and are resized to shorter side of 512 pixels for testing. Our implementation is based on the \texttt{mmsegmentation} \cite{mmseg2020} codebase and the experiments are conducted on 8 NVIDIA A100 GPUs. 


\myPara{Results} Table~\ref{tab:ade20k} shows the ADE20K semantic segmentation performance of different backbones using FPN~\cite{fpn}. PoolFormer-based models consistently outperform the models with backbones of CNN-based ResNet~\cite{resnet} and ResNeXt \cite{xie2017aggregated} as well as Transformer-based PVT. For instance, PoolFormer-12 achieves mIoU of 37.1, 4.3 and 1.5 better than ResNet-18 and PVT-Tiny, respectively. 


These results demonstrate that our PoorFormer which serves as backbone can attain competitive performance on semantic segmentation although it only utilizes pooling for basically communicating information among tokens. This further indicates the great potential of MetaFormer and supports our claim that MetaFormer is actually what we need.


\subsection{Ablation studies}
\label{sec:ablation}
The experiments of ablation studies are conducted on ImageNet-1K \cite{imagenet}.
Table \ref{tab:ablation} reports the ablation study of PoolFormer. 
We discuss the ablation below according to the following aspects. 


\myPara{Token mixers} Compared with Transformers, the main change made by PoolFormer is using simple pooling as a token mixer. We first conduct ablation for this operator by directly replacing pooling with identity mapping. Surprisingly, MetaFormer with identity mapping can still achieve 74.3\% top-1 accuracy, supporting the claim that MetaFormer is actually what we need to guarantee reasonable performance.


Then the pooling is replaced with global random matrix  for each block. The matrix is initialized with random values from a uniform distribution on the interval [0, 1), and then Softmax is utilized to normalize each row. After random initialization, the matrix parameters are frozen and it conducts token mixing by  where  are the input token features with the token length of  and channel dimension of . The token mixer of random matrix introduces extra 21M frozen parameters for the S12 model since the token lengths are extremely large at the first stage. Even with such random token mixing method, the model can still achieve reasonable performance of 75.8\% accuracy, 1.5\% higher than that of identity mapping. It shows that MetaFormer can still work well even with random token mixing, not to say with other well-designed token mixers. 


Further, pooling is replaced with Depthwise Convolution \cite{chollet2017xception, mamalet2012simplifying} that has learnable parameters for spatial modeling. Not surprisingly, the derived model still achieve highly competitive performance with top-1 accuracy of 78.1\%, 0.9\% higher than PoolFormer-S12 due to its better local spatial modeling ability. Until now, we have specified multiple token mixers in Metaformer, and all resulted models keep promising results, well supporting the claim that MetaFormer is the key to guaranteeing models' competitiveness. Due to the simplicity of pooling, it is mainly utilized as a tool to demonstrate MetaFormer.


We test the effects of pooling size on PoolFormer. We observe similar performance when pooling sizes are 3,  5, and 7. However, when the pooling size increases to 9, there is an obvious performance drop of 0.5\%.  Thus, we adopt the default pooing size of 3 for PoolFormer. 


\myPara{Normalization} We modify Layer Normalization \cite{layer_norm} into Modified Layer Normalization (MLN) that computes the mean and variance along token and channel dimensions compared with only channel dimension in vanilla Layer Normalization. The shape of learnable affine parameters of MLN keeps the same as that of Layer Normalization, \ie, . MLN can be implemented with GroupNorm API in PyTorch by setting the group number as 1. See the appendix for details. We find PoolFormer prefers MLN with 0.7\% or 0.8\% higher than Layer Normalization or Batch Normalization. Thus, MLN is set as default for PoolFormer. When removing normalization, the model can not be trained to converge well, and its performance dramatically drops to only 46.1\%.


\myPara{Activation} We change GELU \cite{gelu} to ReLU \cite{relu} or SiLU \cite{silu}. When ReLU is adopted for activation, an obvious performance drop of 0.8\% is observed. For SiLU, its performance is almost the same as that of GELU. Thus, we still adopt GELU as default activation. 


\myPara{Other components} Besides token mixer and normalization discussed above, residual connection \cite{resnet} and channel MLP \cite{rosenblatt1961principles, rumelhart1985learning} are two other important components in MetaFormer. Without residual connection or channel MLP, the model cannot converge and only achieves the accuracy of 0.1\%/5.7\%, proving the indispensability of these parts.


\myPara{Hybrid stages} Among token mixers based on pooling, attention, and spatial MLP, the pooling-based one can handle much longer input sequences while attention and spatial MLP are good at capturing global information. 
Therefore, it is intuitive to stack MetaFormers with pooling in the bottom stages to handle long sequences and use attention or spatial MLP-based mixer in the top stages, considering the sequences have been largely shortened. Thus, we replace the token mixer pooling with attention or spatial FC \footnote{Following \cite{resmlp}, we use only one spatial fully connected layer as a token mixer, so we call it FC.} in the top one or two stages in PoolFormer. From Table \ref{tab:ablation}, the hybrid models perform quite well. The variant with pooling in the bottom two stages and attention in the top two stages delivers highly competitive performance. 
It achieves 81.0\% accuracy with only 16.5M parameters and 2.5G MACs. 
As a comparison, ResMLP-B24 needs  parameters (116M) and  MACs (23.0G) to achieve the same accuracy. These results indicate that combining pooling with other token mixers for MetaFormer may be a promising direction to further improve the performance. 




