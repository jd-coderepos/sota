
\section{Method}
We introduce our approach for collaborative motion prediction, aiming to set the first performance baseline to help future developments.

\subsection{Pipeline}
\label{sec:pipeline}
The idea of our method is to learn two person-specific motion prediction mappings, and to propose a strategy to share information between these two mappings. The possibility to include information from the other person involved in the interaction, should push the network to learn a better representation for motion prediction.
The overall pipeline is described in Figure~\ref{fig:pipline}-left.

For the two single person motion prediction mappings, we draw inspiration from~\cite{mao2020history}, using an attention model for leaning temporal attention w.r.t. the past motions, and a predictor based on Graph Convolutional Network (GCN)~\cite{kipf2016semi} to model the spatial attention among joints using an adjacency matrix. The temporal attention model aims to find the most relative sub-sequence in the past by measuring the similarity between the last observed sub-sequence and a set of past sub-sequences.
In this attention model, the query  is learnt by MLP from the last observation   (blue dashed rectangle in Figure~\ref{fig:pipline}-left, length ). The keys  are learnt by MLP from the starting chunk of sub-sequences (red dashed rectangles in Figure~\ref{fig:pipline}-left, length ).  And the values  consist of DCT representations built from the sub-sequences  (black dashed rectangles in Figure~\ref{fig:pipline}-left, length ), where  with  indicates the start frame of each past sub-sequence. 

Training such strategy separately for each actor does not account for any interaction between the two dancing partners. To deal with this, we design a cross-interaction attention (XIA) module based on multi-head attention, to introduce guidance form the interacted person. In the next section we introduce this XIA module.

\begin{table*}[t!]
\caption{Results on common action split with the two evaluation metrics (in ). Lower value means better performance. Obviously, our proposal outperforms all the other methods both on JME and AME.}\vspace{-2mm}
\label{tab:proto1}
\centering
\rowcolors{3}{gray!15}{white}
\resizebox{1\textwidth}{!}{\setlength{\tabcolsep}{0.8mm}{
\begin{tabular}{ll|cccc|cccc|cccc|cccc|cccc|cccc|cccc|cccc}
\toprule
\multicolumn{2}{c|}{Action} & \multicolumn{4}{c|}{A1 A-frame} & \multicolumn{4}{c|}{A2 Around the back} & \multicolumn{4}{c|}{A3 Coochie} & \multicolumn{4}{c|}{A4 Frog classic} & \multicolumn{4}{c|}{A5 Noser} & \multicolumn{4}{c|}{A6 Toss Out} & \multicolumn{4}{c|}{A7 Cartwheel} & \multicolumn{4}{c}{AVG} \\
\toprule
\multicolumn{2}{c|}{Time (sec)} & 0.2 & 0.4 & 0.6 & 1.0 & 0.2 & 0.4 & 0.6 & 1.0 & 0.2 & 0.4 & 0.6 & 1.0 & 0.2 & 0.4 & 0.6 & 1.0 & 0.2 & 0.4 & 0.6 & 1.0 & 0.2 & 0.4 & 0.6 & 1.0 & 0.2 & 0.4 & 0.6 & 1.0 & 0.2 & 0.4 & 0.6 & 1.0 \\
\midrule
{\cellcolor{white}} & {\cellcolor{white}Res-RNN~\cite{martinez2017human}}& 83 & 141 & 182 & 236 & 127 & 224 & 305 & 433 & 99 & 177 & 239 & 350 & 74 & 135 & 182 & 250 & 87 & 152 & 201 & 271 & 93 & 166 & 225 & 321 & 104 & 189 & 269 & 414 & 95 & 169 & 229 & 325\\
{\cellcolor{white}} & {\cellcolor{white}LTD~\cite{mao2019learning}} & 70 & 125 & 157 & 189 & 131 & 242 & 321 & 426 & 102 & 194 & 260 & 357 & 62 & 117 & 155 & 197 & 72 & 131 & 173 & 231 & 81 & 151 & 200 & 280 & 112 & 223 & 315 & 442 & 90 & 169 & 226 & 303 \\
{\cellcolor{white}} & {\cellcolor{white}Hisrep~\cite{mao2020history}} & 52 & 103 & {139} & {188} & 96 & 186 & 256 & 349 & 57 & 118 & 167 & 240 & 45 & 93 & 131 & 180 & 51 & 105 & 149 & 214 & 61 & 125 & 176 & 252 & 71 & 150 & 222 & 333 & 62 & 126 & 177 & 251\\
{\cellcolor{white}} &{\cellcolor{white} MSR~\cite{Dang_2021_ICCV}} & 56 & 100 & \textbf{132} & \textbf{175} & 102 & 187 & 256 & 365 & 65 & 120 & 166 & 244 & 50 & 95 & 127 & 172 & 54 & 100 & 138 & 202 & 70 & 132 & 182 & 258 & 82 & 154 & 218 & 321 & 69 & 127 & 174 & 248 \\
\multirow{-5}{*}{\rotatebox[origin=c]{90}{\cellcolor{white}\textbf{JME}}}
& {\cellcolor{white}Ours}  & \textbf{49} & \textbf{98} & 140 & 192 & \textbf{84} & \textbf{166} & \textbf{234} & \textbf{346} & \textbf{51} & \textbf{105} & \textbf{154} & \textbf{234} & \textbf{41} & \textbf{84} & \textbf{120} & \textbf{161} & \textbf{43} & \textbf{90} & \textbf{132} & \textbf{197} & \textbf{55} & \textbf{113} & \textbf{163} & \textbf{242} & \textbf{62} & \textbf{130} & \textbf{192} & \textbf{291} & \textbf{55} & \textbf{112} & \textbf{162} & \textbf{238} \\


\midrule
{\cellcolor{white}} &{\cellcolor{white}Res-RNN~\cite{martinez2017human}} & 59 & 102 & 132 & 167 & 62 & 112 & 152 & 229 & 57 & 102 & 139 & 215 & 48 & 85 & 113 & 157 & 51 & 90 & 120 & 167 & 53 & 94 & 126 & 183 & 74 & 131 & 178 & 265 & 58 & 102 & 137 & 197\\
{\cellcolor{white}} & {\cellcolor{white}LTD ~\cite{mao2019learning}} & 51 & 92 & 116 & 132 & 51 & 91 & 116 & \textbf{148} & 43 & 80 & 103 & 130 & 38 & 70 & 89 & 111 & 39 & 70 & 90 & 116 & 42 & 75 & 94 & 123 & 52 & 101 & 139 & 198 & 45 & 83 & 107 & 137 \\
{\cellcolor{white}} & {\cellcolor{white}Hisrep~\cite{mao2020history}} & 34 & 69 & 97 & 130 & 44 & 84 & \textbf{115} & 150 & 32 & 65 & 91 & 121 & 27 & 56 & 82 & 112 & 28 & 58 & 85 & 121 & 34 & 66 & 88 & 115 & 42 & 83 & 120 & 171 & 34 & 69 & 97 & 131 \\
{\cellcolor{white}} & {\cellcolor{white}MSR~\cite{Dang_2021_ICCV}}  & 41 & 75 & 99 & \textbf{126} & 54 & 96 & 129 & 180 & 41 & 74 & 98 & 135 & 34 & 61 & 82 & 106 & 33 & 59 & 79 & \textbf{109} & 42 & 71 & 93 & 124 & 57 & 103 & 146 & 210 & 43 & 77 & 104 & 141 \\
\multirow{-5}{*}{\rotatebox[origin=c]{90}{\cellcolor{white}\textbf{AME}}} & {\cellcolor{white}Ours}   & \textbf{32} & \textbf{68} & \textbf{99} & {128} & \textbf{41} & \textbf{82} & 116 & 163 & \textbf{29} & \textbf{58} & \textbf{84} & \textbf{116} & \textbf{24} & \textbf{50} & \textbf{73} & \textbf{96} & \textbf{24} & \textbf{51} & \textbf{75} & \textbf{109} & \textbf{31} & \textbf{62} & \textbf{86} & \textbf{114} & \textbf{41} & \textbf{81} & \textbf{115} & \textbf{160} & \textbf{32} & \textbf{65} & \textbf{93} & \textbf{127}\\


\bottomrule

\end{tabular}}}
\end{table*}
%
 

\subsection{Cross-Interaction Attention (XIA)}
XIA aims to share motion information between the two predictors. 
In particular, we denote the query and the key-value pairs for one person by  and   respectively, and use the superscript  and  to indicate the two person, follower and leader. We naturally cast the collaborative human motion prediction task into learning how to jointly exploit the information in  when querying with  to predict motion of each person.

Our intuition is that the pose information (key-value pairs) of one person can be used to transform the pose information of the other person for better motion prediction. We implement this intuition with the help of the proposed \textit{cross-interaction attention module}. Such a module takes as input  and the corresponding vector from the interacted pose , and uses multi-head self attention to get the refined vector  (see Figure~\ref{fig:pipline}-right):
 
where  stands for multi-head attention with query , key  and value , and  indicates fully connected layers.
We use different XIA modules to update keys and values mentioned in Section~\ref{sec:pipeline}: in our implementation, XIA modules for keys have 8 attention heads, and XIA for values have a single attention head. Moreover, we add a skip-connection for the MHA module followed by 2 FC layers. XIA modules for leader/follower do not share weights.

The proposed XIA module is integrated at several stages of the computing flow as shown in Figure~\ref{fig:pipline}. More precisely, we refine all keys:

and analogously for the values. 

XIA could be \guo{potentially} generalised to any number of participants by considering either several XIA modules and fusing their outcome, or performing the fusion at the input of XIA module.

\begin{figure*}[t]
\centering
\includegraphics[width=0.98\textwidth]{Model_compare_v3.pdf}
\vspace{-0.3cm}
  \caption{\textbf{Left:} Percentages of improvement of  our method comparing with different state-of-the-art methods, measured by average JME error on the common action split, at different forecast time. Lower value means closer performance with our model. Our method surpasses these methods up to  on short term,  and  on long term. \textbf{Right:} Joint-wise JME improvement(mm) of our method over Hisrep~\cite{mao2020history} and MSR~\cite{Li_2021_ICCV}. Darker color means larger improvement.}\vspace{-5mm}
\label{fig:res_ca}
\end{figure*}


\vspace{7mm}
\subsection{Pose Normalization}
\label{sec:norm}
Raw poses of ExPI are represented in world coordinate. Similar with single person motion prediction, we normalize the data by removing the global displacement of the poses based on a selected root joint.
While our task aims at predicting not only the the distinct poses but also the relative position of the two person, so 
we have to normalize by the same person to keep the information of their related position. We could either normalize by leader/follower and we choose to normalize by the leader for better visualisation.
Specifically, for each frame, we take the the root joint (middle of the two hips) of the leader as coordinate origin, use the root point and left hip of the leader to define -axis, and use the neck of leader to determine XOZ plane. We normalize all the joints of both persons to this coordinate, then the pose errors can be computed directly in this coordinate. More precisely,
we represent the raw poses in world coordinate as , and  is the rigid transformation aligning the two actors to the leader's coordinate system. The normalized coordinates are thus  ,  and . In the following  shall always represent the normalized pose unless specified otherwise.

\vspace{-2mm}
\section{Experimental Evaluation}
This section describes the experimental protocol on ExPI, and discuss the results of our proposed method.

\begin{table*}[t!]
\caption{Results on single action split with the two evaluation metrics (in ). Lower value means better performance. Seven action-wise models are trained independently. Our method performs the best in 5 actions, and close to the best for the other 2 actions.}\vspace{-2mm}
\label{tab:proto2}
\centering
\rowcolors{3}{gray!15}{white}
\resizebox{0.9\textwidth}{!}{\setlength{\tabcolsep}{0.8mm}{
\begin{tabular}{ll|cccc|cccc|cccc|cccc|cccc|cccc|cccc}
\toprule
\multicolumn{2}{c|}{Action} & \multicolumn{4}{c|}{A1 A-frame} & \multicolumn{4}{c|}{A2 Around the back} & \multicolumn{4}{c|}{A3 Coochie} & \multicolumn{4}{c|}{A4 Frog classic} & \multicolumn{4}{c|}{A5 Noser} & \multicolumn{4}{c|}{A6 Toss Out} & \multicolumn{4}{c}{A7 Cartwheel} \\
\midrule
\multicolumn{2}{c|}{Time (sec)} & 0.2 & 0.4 & 0.6 & 1.0 & 0.2 & 0.4 & 0.6 & 1.0 & 0.2 & 0.4 & 0.6 & 1.0 & 0.2 & 0.4 & 0.6 & 1.0 & 0.2 & 0.4 & 0.6 & 1.0 & 0.2 & 0.4 & 0.6 & 1.0 & 0.2 & 0.4 & 0.6 & 1.0 \\
\midrule
{\cellcolor{white}} & {\cellcolor{white}Res-RNN~\cite{martinez2017human}} & 75 & 131 & 171 & 226 & 122 & 215 & 287 & 403 & 97 & 174 & 235 & 329 & 73 & 131 & 177 & 246 & 76 & 136 & 184 & 255 & 100 & 184 & 252 & 357 & 88 & 162 & 219 & 293\\
&LTD~\cite{mao2019learning}  & 70 & 126 & 155 & {183} & 131 & 243 & 312 & 415 & 102 & 194 & 252 & 338 & 62 & 117 & 153 & 203 & 71 & 131 & 171 & 231 & 81 & 151 & 199 & 299 & 112 & 223 & 306 & 411 \\
{\cellcolor{white}} & {\cellcolor{white}Hisrep~\cite{mao2020history}} & 66 & {118} & {153} & 190 & 128 & 231 & 308 & 417 & 74 & 143 & 205 & 295 & 64 & 120 & 159 & {191} & 63 & 121 & 166 & 227 & 90 & 168 & 232 & 312 & 88 & 166 & 232 & 332  \\
&MSR\cite{Dang_2021_ICCV}   & \textbf{64} & \textbf{108} & \textbf{136} & \textbf{171} & 119 & 210 & 282 & 385 & 79 & 144 & 189 & 265 & \textbf{59} & \textbf{103} & \textbf{134} & \textbf{173} & 65 & 118 & 162 & 225 & 86 & 151 & 201 & 283 & 96 & 178 & 255 & 362\\
\multirow{-5}{*}{\rotatebox[origin=c]{90}{\cellcolor{white}\textbf{JME}}} 
& {\cellcolor{white}Ours} & \textbf{64} & 120 & 160 & 199 & \textbf{109} & \textbf{200} & \textbf{275} & \textbf{381} & \textbf{59} & \textbf{117} & \textbf{174} & \textbf{277} & {60} & {116} & 162 & 209 & \textbf{53} & \textbf{106} & \textbf{152} & \textbf{221} & \textbf{65} & \textbf{122} & \textbf{166} & \textbf{223} & \textbf{74} & \textbf{144} & \textbf{203} & \textbf{301} \\



\midrule
&Res-RNN~\cite{martinez2017human}  & 56 & 99 & 129 & 163 & 61 & 110 & 150 & 229 & 53 & 96 & 131 & 188 & 46 & 81 & 106 & 142 & 44 & 79 & 106 & 147 & 53 & 100 & 162 & 176 & 70 & 133 & 163 & 198\\
{\cellcolor{white}} & {\cellcolor{white}LTD~\cite{mao2019learning}} & 51 & 93 & {114} & {127} & \textbf{51} & \textbf{91} & \textbf{116} & \textbf{162} & 43 & 80 & 100 & \textbf{126} & 38 & {70} & \textbf{88} & {118} & 39 & 70 & 90 & 125 & 42 & 75 & \textbf{93} & 123 & 52 & 101 & 137 & 188 \\
&Hisrep~\cite{mao2020history}  & 45 & 83 & 106 & 118 & 57 & 102 & 135 & 178 & 39 & 72 & 100 & 132 & 41 & 77 & 103 & 119 & 35 & 70 & 97 & 125 & 46 & 82 & 107 & 137 & 48 & 90 & 121 & 169 \\
{\cellcolor{white}} & {\cellcolor{white}MSR\cite{Dang_2021_ICCV}} & 46 & 79 & \textbf{98} & \textbf{118} & 60 & 107 & 141 & 192 & 48 & 86 & 111 & 150 & 39 & \textbf{68} & \textbf{88} & \textbf{111} & 39 & 69 & 91 & \textbf{121} & 55 & 93 & 117 & 156 & 66 & 118 & 163 & 222\\
\multirow{-5}{*}{\rotatebox[origin=c]{90}{\textbf{AME}}}
& Ours & \textbf{43} & \textbf{84} & 115 & 131 & 53 & 99 & 136 & 185 & \textbf{35} & \textbf{68} & \textbf{98} & 140 & \textbf{37} & 74 & 106 & 128 & \textbf{29} & \textbf{59} & \textbf{86} & {125} & \textbf{39} & \textbf{72} & 94 & \textbf{119} & \textbf{43} & \textbf{82} & \textbf{112} & \textbf{152} \\



\bottomrule
\end{tabular}}}\vspace{-3mm}
\end{table*}
%
 
\subsection{Splitting the ExPI Dataset}
As described in Sect.~\ref{sec:data_structure}, we record 16 actions in ExPI dataset. Seven of them are common actions ( to ) which are performed by both of the 2 couples: we denote them as  performed by couple 1 and  by couple 2. The other actions are couple-specific, which are performed only by one couple: we denote the actions performed by couple 1 ( to ) as , and actions by couple 2 ( to ) as . With these notations, we propose three data splits.


\vspace{0mm}
\noindent\textbf{Common action split.}
Similar to \cite{ionescu2013human3}, we consider the common actions performed by different couples of actors as train and test data. More precisely,  is the train dataset and  is the test dataset. Thus, train and test data contain the same actions but performed by different people. 

\vspace{0mm}
\noindent\textbf{Single action split.}
Similar to~\cite{fragkiadaki2015recurrent,jain2016structural}, we train 7 action specific models separately for each common action, by taking one action from couple 2 as train set and the related one from couple 1 as test set.

\vspace{0mm}
\noindent\textbf{Unseen action split.}
The train set is the entire set of common actions . We regard the extra couple-specific actions  as unseen actions and use them as our test set. Thus the train and test data contain both couples of actors, but the test actions are not used in training.

To sum up, common action split is designed for a single model on different actions, single action split is designed for action-wise models, and unseen action split focuses on testing unseen actions to measure methods generalization.



\subsection{Evaluation Metrics}
The most common metric for evaluating 3D joint position in pose estimation and motion prediction tasks is the mean per joint position error , where  is the number of joints,  and  are the estimated and ground truth position of joint . Based on MPJPE, we propose two different metrics to evaluate the multi-person motion task. 

\vspace{1mm}
\noindent\textbf{Joint mean error (JME):} 
We Propose \textit{Joint Mean per joint position Error} to measure poses of different persons in a same coordinate, and denote it as JME for simplicity:

where  and  are normalized (see  Section~\ref{sec:norm}) prediction and ground truth. JME provides an overall idea for the performance of collaborative motion prediction by considering the two interacted persons jointly as a whole, measuring both the error of poses and the error of their relative position.


\noindent\textbf{Aligned mean error (AME):}
We propose \textit{Aligned Mean per joint position Error} to measure pure pose error without the position bias. 
We first erase the errors on the relative position between the two persons by normalizing the poses independently to obtain . However the precision of  is importantly influenced by the joints that are used to determine the coordinate (hips and back). To mitigate this effect, we compute the best rigid alignment  between the estimated pose and the ground-truth {using Procrustes analysis}~\cite{gower1975generalized}:

where  are independently normalized predictions  and , and   is the normalisation transformation computed from the pose  as defined in Section~\ref{sec:norm}. The same calculation is done for the ground truth . This normalization is only used for evaluation purpose.



\begin{table*}[t!]
\caption{Action-wise results on unseen action split with the two evaluation metrics (in ). Lower value means better performance. Our method still performs the best on most of the unseen actions and on the average result.}\vspace{-2mm}
\label{tab:proto3}
\centering
\rowcolors{3}{gray!15}{white}
\resizebox{1\textwidth}{!}{\setlength{\tabcolsep}{0.8mm}{
\begin{tabular}{ll|ccc|ccc|ccc|ccc|ccc|ccc|ccc|ccc|ccc|ccc}
\toprule
\multicolumn{2}{c|}{Action} & \multicolumn{3}{c|}{A8} & \multicolumn{3}{c|}{A9} & \multicolumn{3}{c|}{A10} & \multicolumn{3}{c|}{A11} & \multicolumn{3}{c|}{A12} & \multicolumn{3}{c|}{A13} & \multicolumn{3}{c|}{A14} & \multicolumn{3}{c|}{A15} & \multicolumn{3}{c|}{A16} & \multicolumn{3}{c}{AVG} \\
\midrule
\multicolumn{2}{c|}{Time (sec)} &0.4&0.6&0.8&0.4&0.6&0.8&0.4&0.6&0.8&0.4&0.6&0.8& 0.4&0.6&0.8&0.4&0.6&0.8&0.4&0.6&0.8&0.4&0.6&0.8&0.4&0.6&0.8&0.4&0.6&0.8\\
\midrule
{\cellcolor{white}} & {\cellcolor{white}Res-RNN~\cite{martinez2017human}} & 239 & 312 & 371 & 193 & 256 & 303 & 189 & 257 & 310 & 305 & 425 & 520 & 215 & 289 & 348 & 165 & 214 & 252 & 214 & 293 & 357 & 149 & 187 & 210 & 167 & 226 & 277 & 204 & 273 & 327\\
&LTD ~\cite{mao2019learning} & 239 & 324 & 394 & 175 & 226 & 259 & 148 & 191 & 220 & 176 & 240 & 286 & 143 & 178 & 192 & 146 & 193 & 226 & 252 & 333 & 387 & 174 & 228 & 264 & 139 & 184 & 217 & 177 & 233 & 272\\
{\cellcolor{white}} & {\cellcolor{white}Hisrep~\cite{mao2020history}} & 195 & \textbf{283} & \textbf{358} & 121 & 169 & 206 & 92 & 129 & \textbf{160} & 129 & 193 & 245 & \textbf{80} & \textbf{104} & 121 & 112 & 154 & 187 & 157 & 219 & 257 & 134 & 190 & 233 & \textbf{96} & \textbf{146} & \textbf{187} & 124 & 176 & \textbf{218}\\
&MSR~\cite{Dang_2021_ICCV} & 230 & 289 & 335 & 188 & 245 & 290 & 148 & 198 & 248 & 234 & 319 & 384 & 176 & 232 & 278 & 162 & 218 & 266 & 177 & 239 & 295 & 143 & 179 & 213 & 157 & 222 & 281 & 179 & 238 & 288 \\
\multirow{-5}{*}{\rotatebox[origin=c]{90}{\cellcolor{white}\textbf{JME}}}
& {\cellcolor{white}Ours}  & \textbf{191} & 287 & 377 & \textbf{118} & \textbf{165} & \textbf{203} & \textbf{91} & \textbf{129} & 162 & \textbf{122} & \textbf{183} & \textbf{232} & 81 & 107 & \textbf{128} & \textbf{106} & \textbf{150} & \textbf{185} & \textbf{156} & \textbf{216} & \textbf{256} & \textbf{126} & \textbf{175} & \textbf{213} & \textbf{96} & 152 & 205 & \textbf{121} & \textbf{174} & \textbf{218}\\



\midrule
&Res-RNN.~\cite{martinez2017human}& 124 & 165 & 195 & 125 & 157 & 181 & 131 & 166 & 189 & 148 & 198 & 240 & 149 & 169 & 192 & 102 & 128 & 147 & 181 & 237 & 279 & 100 & 129 & 144 & 93 & 124 & 147 & 128 & 164 & 190\\
{\cellcolor{white}} & {\cellcolor{white}LTD ~\cite{mao2019learning}} & \textbf{95} & \textbf{123} & \textbf{146} & 85 & 106 & 116 & 74 & 91 & 101 & 86 & 115 & 137 & 98 & 125 & 134 & 85 & 110 & 124 & 106 & 136 & 155 & 91 & 119 & 135 & 72 & 96 & 116 & 88 & 113 & 129 \\
&Hisrep~\cite{mao2020history} & 101 & 144 & 176 & 61 & 82 & 94 & \textbf{49} & \textbf{67} &\textbf{ 80 }& 73 & \textbf{105} & \textbf{129} & \textbf{53 }& \textbf{73 }& \textbf{86} & {64} & {89} & \textbf{104 }& 86 & 120 & \textbf{142} & 73 & 104 & 128 & 54 & 82 & \textbf{104 }& 68 & 96 & \textbf{116}\\
{\cellcolor{white}} & {\cellcolor{white}MSR~\cite{Dang_2021_ICCV}} & 103 & 134 & 155 & 101 & 135 & 160 & 74 & 98 & 121 & 103 & 143 & 173 & 87 & 111 & 132 & 84 & 106 & 122 & 88 & 118 & \textbf{142} & 90 & 113 & 136 & 90 & 122 & 148 & 91 & 120 & 143\\
\multirow{-5}{*}{\rotatebox[origin=c]{90}{\cellcolor{white}\textbf{AME}}}
& Ours & \textbf{95} & 137 & 171 & \textbf{58} & \textbf{80} & \textbf{93} & 51 & 70 & 84 & \textbf{70} & \textbf{105} & 134 & \textbf{53} & \textbf{73} & 88 & \textbf{63} & \textbf{88} & \textbf{104} & \textbf{82} & \textbf{116} & \textbf{142} & \textbf{69} & \textbf{97} & \textbf{120} & \textbf{52} & \textbf{79} & \textbf{104} & \textbf{66} & \textbf{94} & \textbf{116}\\



\bottomrule
\end{tabular}}}\vspace{-4mm}
\end{table*} 
\subsection{Implementation Details}
Since this is the first time the collaborative motion prediction task is presented in the literature, there are no available methods to compare with. Thus we choose 4 code-released state-of-the-art methods of single person motion prediction~\cite{martinez2017human,mao2019learning,mao2020history,Dang_2021_ICCV}, and implement their released codes\footnote{All the codes we use are under MIT license.} on ExPI dataset. For fair comparison, all these models are trained with 50 frames of input, train/test for the leader and the follower separately.

We train our model for 25 epochs and calculate the average MPJPE loss of 10 predicted frames. As the data is normalized by the leader, the corresponding branch converges faster, so we compensate by exponentially down-weighting the loss of the leader with the number of epochs , using the loss function: .

When predicting longer horizons, we use the predicted motion as input to predict future motion. {Inspired by~\cite{mao2020history}, we take 64 sub-sequences for each sequence to reduce the variance of the test results.} Overall, we have  and  sub-sequences for training and testing respectively in the {common action split} and {the single action split}, and  /   training/testing samples in the {unseen action split}.



\vspace{-2mm}
\subsection{Results and Discussion}
\label{sec:res_proto1}

\vspace{-1mm}
\noindent\textbf{Common action split.} 
Table~\ref{tab:proto1} reports the results  on the \wen{common action split}. We observe that our proposed method outperforms other methods systematically almost for all actions, in all metrics and for different testing time. 
In Figure~\ref{fig:res_ca}-left we calculate the percentage of improvement of our method compared with the state-of-the-art methods, and find that we significantly surpass these methods up to  on short term,  and  on long term. 
We further compare our per-joint results with Hisrep~\cite{mao2020history} and MSR~\cite{Dang_2021_ICCV} in Figure~\ref{fig:res_ca}-right, and observe that our proposed method gets better results on almost on all the joints. More importantly, the keypoints of the limbs (joints of arms and legs) are improved largely. This is reasonable as interaction between persons comes mostly through the limbs, while joints on the torso have little influence on it. So our cross-interaction attention is able to improve the accuracy on the limbs more than on the torso. We could also notice the large improvement on the feet of the follower which usually fly in the air, indicating that our method works even better for these extreme high dynamic joints.  

\noindent\textbf{Single action split and unseen action split.} 
We also reported our proposed method by reporting the results on \wen{single action split and unseen action split}. For \wen{single action split},  XIA outperforms the state-of-the-art methods also on action-specific models, as shown in Table~\ref{tab:proto2}. Interestingly, we observe that the performance on \wen{single action split} is worse than the corresponding results on \wen{common action split}, meaning that training on different actions helps regularising the network for this very challenging collaborative extreme motion prediction task.
Regarding \wen{unseen action split} shown in Table~\ref{tab:proto3}, we can see that XIA still outperforms the state-of-the-art methods on most of the actions, demonstrating the robustness of our method.


\begin{table}[t!]
\caption{Ablations. 
\textit{'mix /cat /sep'} use the single person motion prediction model (Hisrep~\cite{mao2020history}) for multi-person by: mixing two poses together / concatenate two poses as a single vector / train two person-specific models.
\textit{'w.o. XIA'} indicates training leader and follower in parallel using our defined loss without XIA module;
\textit{'XIA kqv / kq / kv / v'} use XIA module to update key, value and query of the temporal attention, or just some of them.
}\vspace{-2mm}
\label{tab:abla}
\centering
\rowcolors{3}{gray!15}{white}
\resizebox{0.48\textwidth}{!}{\setlength{\tabcolsep}{2.5mm}{
\begin{tabular}{cc|ccccc|ccccc}\toprule
\multicolumn{2}{c|}{} & \multicolumn{5}{c|}{\bf JME} & \multicolumn{5}{c}{\bf AME} \\
\midrule
\multicolumn{2}{c|}{Time (sec)} & 0.2 & 0.4 & 0.6 & 0.8 & 1.0 & 0.2 & 0.4 & 0.6 & 0.8 & 1.0 \\
\midrule
{\cellcolor{white}} & {\cellcolor{white}mix} & 69 & 132 & 185 & 233 & 271  & 41 & 77 & 104 & 126 & 142\\
& cat & 61 & 123 & 176 & 223 & 262 & 37 & 71 & 99 & 121 & 138\\
{\cellcolor{white}}& {\cellcolor{white}sep} & 62 & 126 & 177 & 218 & 251 & 34 & 69 & 97 & 116 & 131 \\
\midrule
& w.o. XIA  & 58 & 120 & 174 & 217 & 249 & 33 & 68 & 98 & 118 & 131\\
{\cellcolor{white}} & {\cellcolor{white}XIA kq} & 58 & 118 & 169 & 211 & 245 & 33 & 67 & 95 & 114 & 128\\
& XIA kqv   & 57 & 117 & 170 & 215 & 251 & 32 & 65 & 95 & 116 & 131\\
{\cellcolor{white}} & {\cellcolor{white}XIA v} & 56 & 116 & 168 & 210 & 244 & 32 & 66 & 94 & 113 & \textbf{127}\\
& XIA kv  & \textbf{55} & \textbf{112} & \textbf{162} & \textbf{204} & \textbf{238} & \textbf{32} & \textbf{65} & \textbf{93} & \textbf{112} & \textbf{127}\\
\bottomrule
\end{tabular}}}\vspace{-6mm}
\end{table}





%
 

\noindent\textbf{Qualitative results.} 
Figure~\ref{fig:teaser} shows some example of our visualisation results compared to  Hisrep~\etal~\cite{mao2020history}, MSR~\cite{Dang_2021_ICCV} and the ground truth, on the \wen{common action split}. We can see that the poses estimated by our method are much closer to the ground truth than the other methods, and it works well even on some extreme actions where other methods totally fail (Figure~\ref{fig:teaser}-right). More qualitative examples could be found in the supplementary material.



\noindent\textbf{Ablation study.} 
Taking Hisrep~\cite{mao2020history} as example, we first tried 3 different ways of training the single-person motion prediction models on our multi-person dataset: (i) 'mix': train a single model using data of the two poses ; (ii) 'cat': concatenate the two poses as a single input vector ; (iii) 'sep': train two person-specific models for  and . Since 'sep' gives best performance, all the state-of-the-art methods reported above in this paper is using this setting.
As for our collaborative motion prediction model, we report performances of several different design choices of our model. We found that updating the key and values of the temporal attention using our XIA module provide the best results.
We demonstrate the interest of the design of our method as the proposed one is the best in performance and our method significantly improves all the single-person motion prediction results.