\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tabularray}
\usepackage{rotating}
\usepackage{multirow}
\usepackage[accsupp]{axessibility}  \usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false,linkcolor = blue,
            urlcolor  = blue,
            citecolor = blue,
            anchorcolor = blue]{hyperref}


\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{****} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{NAPA-VQ: Neighborhood Aware Prototype Augmentation with Vector Quantization for Continual Learning}

\author{Tamasha Malepathirana \and Damith Senanayake \and Saman Halgamuge \\
Dept. of Mechanical Engineering\\
The University of Melbourne\\
{\tt\small \{tamasha.malepathirana, damith.senanayake, saman.halgamuge\}@unimelb.edu.au}
}
\maketitle
\ificcvfinal\thispagestyle{empty}\fi

\begin{abstract}
Catastrophic forgetting; the loss of old knowledge upon acquiring new knowledge, is a pitfall faced by deep neural networks in real-world applications. Many prevailing solutions to this problem rely on storing exemplars (previously encountered data), which may not be feasible in applications with memory limitations or privacy constraints. Therefore, the recent focus has been on Non-Exemplar based Class Incremental Learning (NECIL) where a model incrementally learns about new classes without using any past exemplars. However, due to the lack of old data, NECIL methods struggle to discriminate between old and new classes causing their feature representations to overlap. We propose NAPA-VQ: \textbf{N}eighborhood \textbf{A}ware \textbf{P}rototype \textbf{A}ugmentation with \textbf{V}ector \textbf{Q}uantization, a framework that reduces this class overlap in NECIL. We draw inspiration from Neural Gas to learn the topological relationships in the feature space, identifying the neighboring classes that are most likely to get confused with each other. This neighborhood information is utilized to enforce strong separation between the neighboring classes as well as to generate old class representative prototypes that can better aid in obtaining a  discriminative decision boundary between old and new classes. Our comprehensive experiments on CIFAR-100, TinyImageNet, and ImageNet-Subset demonstrate that NAPA-VQ outperforms the State-of-the-art NECIL methods by an average improvement of 5\%, 2\%, and 4\% in accuracy and 10\%, 3\%, and 9\% in forgetting respectively. Our code can be found in \url{https://github.com/TamashaM/NAPA-VQ.git}.
\end{abstract}



\section{Introduction}

The achievements of deep neural networks over the years have grown significantly, and their efficiency and applicability have been demonstrated by numerous state-of-the-art works \cite{Krizhevsky2012ImageNetNetworks, He2016DeepRecognition, Long2015FullySegmentation, Girshick2015FastR-CNN}. However, a major requirement for the optimal operation of gradient-based optimization -- the ubiquitous learning paradigm -- is the data samples being independently and identically distributed (IID) \cite{Hadsell2020EmbracingNetworks}. This assumption may not hold in real data owing to various factors such as the addition of new classes of data, the removal of old data due to memory or availability constraints, and the changes in the data-generating phenomena (concept drift). As a result, neural networks may experience catastrophic forgetting where the network forgets the previously learned knowledge upon acquiring new knowledge \cite{Kirkpatrick2017OvercomingNetworks}. 

``Continual Learning'' is a field of research pursuing mechanisms to mitigate this forgetting \cite{DeLange2021ATasks}. In this manuscript, we focus on one paradigm of continual learning, named Class Incremental Learning (CIL) \cite{Lampert2017}. In CIL, a neural network is trained over a series of tasks and at each task, the network learns a new set of classes. At any given time, the network should classify between all learned classes thus far. Among the techniques proposed for CIL, rehearsal-based methods have demonstrated promising results in mitigating forgetting by storing exemplars (old samples) and reusing them while learning new tasks \cite{Lampert2017, Castro2018End-to-endLearning,NIPS2017_f8752278}. However, such storage is not always possible due to memory limitations and privacy constraints \cite{Smith2022ALearning}. Therefore, we focus on Non-Exemplar based CIL (NECIL), a more pragmatic yet challenging scenario, which attempts to preserve the old knowledge without storing any exemplars \cite{Zhu2021PrototypeLearning, Zhu2022Self-SustainingLearning}. 

NECIL methods often struggle with overlapping old and new class representations due to the unavailability of exemplars, resulting in catastrophic forgetting. \cite{Zhu2021PrototypeLearning}.
While prototypes of old classes in the deep feature space are a viable alternative to reusing exemplars \cite{Zhu2021PrototypeLearning, Zhu2021Class-IncrementalAugmentation}, if not properly generated, the class boundaries refined using such prototypes tend to be muddled, causing confusion between the old and new classes, and in turn, leading to catastrophic forgetting. To overcome this limitation, we propose NAPA-VQ: Neighborhood-Aware Prototype Augmentation with Vector Quantization framework. NAPA-VQ not only proposes a novel way to create prototypes of old classes by considering class neighborhoods but also incorporates a novel quantization mechanism to create clearer class boundaries by enforcing a strong separation between the neighboring classes. This reduction in representation overlap effectively mitigates catastrophic forgetting. NAPA-VQ builds on the principles of unsupervised Neural Gas (NG) \cite{ThomasMartinetzandKlausSchulten1991ATopologies} and supervised Learning Vector Quantization (LVQ) \cite{Kohonen1990ImprovedQuantization} to facilitate this neighborhood awareness and enforce more discriminative boundaries. 

NAPA-VQ contains two  components: (I) a Neighborhood-aware Vector Quantizer (NA-VQ) and (II) a Neighborhood-aware Prototype Augmenter  (NA-PA).  NA-VQ learns the topology of the feature space manifold  which is the output of a deep feature extractor (\eg ResNet), identifying the neighboring classes that share similar features and are hence prone to get confused with each other. This knowledge of the neighboring classes and their class distributions is utilized by NA-VQ to increase their separability and by NA-PA for old class prototype augmentation, i.e., to generate surrogate exemplars to facilitate the retention of old information when new classes are being learned.

To summarize,
\begin{itemize}
  \item We propose an improved supervised vector quantization method to discretize the latent space and improve class separation.
  \item We propose a prototype augmentation method that uses the topological information of classes in the latent space to avoid confusion between classes and catastrophic forgetting.
    \item We demonstrate the utility of the above two contributions combined in NECIL, obtaining superior performance compared to the existing NECIL methods on CIFAR-100, TinyImageNet, ImageNet-Subset, and ImageNet-1k datasets.
\end{itemize}

\section{Related work}
\subsection{Incremental learning}

The techniques proposed to combat catastrophic forgetting can be broadly categorized into three \cite{DeLange2021ATasks}. 
(1) The regularisation-based methods that add an extra regularisation loss term either to penalize changes to the network parameters that are important for previous tasks \cite{Kirkpatrick2017OvercomingNetworks, Liu2018RotateForgetting, Zenke2017ContinualIntelligence, Aljundi2018MemoryForget} or to distill knowledge from previous tasks to the current task \cite{Li2018LearningForgetting, Dhar2019LearningMemorizing,Zhang2020Class-incrementalConsolidation}. (2) The parameter isolation-based methods that assign each task with an isolated set of  parameters to prevent task interference either by dynamically increasing the network capacity \cite{Rusu2016ProgressiveNetworks,Yoon2017LifelongNetworks, Aljundi2017ExpertExperts} or by masking previous task parameters in a fixed size network \cite{Fernando2017Pathnet:Networks, Serra2018OvercomingTask,Mallya2018Packnet:Pruning}. Although parameter isolation methods are effective in overcoming catastrophic forgetting, they experience either a linear increase in network parameters or a decrease in capacity per task as the number of tasks grows \cite{Smith2021AlwaysLearning}. (3) The rehearsal-based methods that store a small subset of previous task data to either retrain \cite{Lampert2017,Castro2018End-to-endLearning,Chaudhry2019OnLearning} or constrain the optimisation \cite{NIPS2017_f8752278,Chaudhry2019EfficientA-gem,Aljundi2019GradientLearning} during the learning of new tasks in order to retain the discriminability between old and new classes. However, these methods also encounter pitfalls due to memory limitations, and other pragmatic concerns such as privacy or consent issues when storing samples.
An alternative to rehearsal-based methods is ``pseudo-rehearsal", which involves training a generative model to mimic past task distributions \cite{Shin2017,Seff2017ContinualNets}. Despite the encouraging results, generative models are computationally expensive to train \cite{deVen2018GenerativeLearning} and are also prone to catastrophic forgetting \cite{Thanh-Tung2020CatastrophicGans}. This motivated the development of NECIL strategies that neither depends on real nor fake past samples \cite{Zhu2021PrototypeLearning,Zhu2022Self-SustainingLearning,Liu2020MoreLearning,Yu2020}.

NECIL methods benefit from powerful feature extractors learning transferable features across tasks, as demonstrated by SDC \cite{Yu2020}, which showed that embedding networks suffer significantly less from catastrophic forgetting. PASS \cite{Zhu2021PrototypeLearning} also showed that self-supervised learning alleviates task-level overfitting. Furthermore, to maintain the decision boundaries of previously learned classes, PASS introduced a class-mean prototype augmentation technique based on Gaussian noise. While this technique aids in the retention of old information, it can be further improved by leveraging the knowledge of the distribution of classes in the feature space. Accordingly, IL2A \cite{Zhu2021Class-IncrementalAugmentation} proposed storing covariance matrices to retain class variations, but this approach can be memory intensive. SSRE \cite{Zhu2022Self-SustainingLearning} proposed a dynamic structure reorganization strategy to retain and transfer knowledge between tasks along with a prototype selection mechanism that utilizes an up-sampling technique of non-augmented class-mean prototypes. Similar to these approaches, we also store the mean prototype, while proposing a new method to augment them. To this end, we use the topological connections derived from an NG-like vector quantization to generate prototypes that lie within the shared feature regions of the confusing classes which aid in establishing better class discrimination.











\begin{figure*}[t]
\centering
\includegraphics[width=0.75\textwidth]{
framework.png}
\caption{Illustration of NAPA-VQ. Data from the current task  are augmented using a rotation-based technique \cite{Zhu2021PrototypeLearning} and are fed to the feature extractor.
The obtained feature representations () and the NA-PA generated old class representative prototypes () are sent to the vector quantizer (NA-VQ) to identify and repel confusing classes, establishing better discrimination in the feature space. 
Knowledge Distillation () is used to minimize the feature drift across tasks.}\label{fig:framework}
\end{figure*} 

\subsection{Vector quantization}

Vector Quantization (VQ), a technique used to discretize a continuous data space into a finite set of ``coding vectors'' (CVs) was popularised with the advents of Self-organizing Maps (SOMs) \cite{Kohonen1990TheMap}. In addition to quantizing the data manifold, a SOM captures a topological mapping from data to the CVs. Neural Gas (NG) networks \cite{ThomasMartinetzandKlausSchulten1991ATopologies,Fritzke1994ATopologies}, on the other hand, were introduced to address a shortcoming of the original SOM by allowing a generic graph structure rather than a fixed lattice structure. In NG, the CVs are adjusted to capture the data-dense regions, and the edges between these CVs are formed based on their proximity. These edges and the CVs form a graph that approximates the topology of the data manifold. 



Coding vector-based learning can be traced back to the K-nearest neighbor (K-NN) algorithm \cite{Fix1989DiscriminatoryProperties}. 
For instance, Learning Vector Quantization (LVQ) was proposed to derive the CVs used in a 1-NN classifier \cite{Kohonen1990ImprovedQuantization, Sato1996GeneralizedQuantization}.
Despite common roots, LVQ algorithms and unsupervised VQ algorithms such as SOM differ in their primary usages of CVs; the unsupervised algorithms attempt to obtain a set of CVs to best represent the data while LVQ algorithms attempt to reduce the misclassification rate by focusing on the decision boundaries between classes. These complementary properties allow us to combine unsupervised and supervised VQ methods \cite{Hammer2005SupervisedMeasure,DeVries2016DeepQuantization} to obtain CVs to both reduce the misclassification rate and represent the data distribution \cite{Kohonen1990TheMap}.




Multiple studies explored the integration of the hierarchical feature-extracting capability of deep feature extractors with VQ \cite{DeVries2016DeepQuantization, Villmann2017FusionLearning,Blaes2017Few-shotPrototyping} which were also later adapted to Continual Learning. TPCIL \cite{Tao2020Topology-PreservingClassificatio} proposed to retain the topology of the feature space to preserve old knowledge over the increments. IDLVQ \cite{Chen2021IncrementalSpace} proposed to adapt a margin-based loss for the task of few-shot class incremental learning (FSCIL) -- a special case of CIL, therefore not directly transferrable to CIL/NECIL -- to create a large margin between classes to mitigate overlap. TOPIC \cite{Tao2020Few-ShotLearning} was also proposed for the FSCIL setting with the aim of preserving old knowledge by stabilizing a NG network. 
We highlight that changes to the topology are possible due to the inevitable feature drift occurring over incremental steps thus a method that uses both augmented prototypes and new data to update the topological graph between CVs is warrented.


\section{Methodology}


\subsection{Preliminaries and notations}
In CIL, a model is continually trained over a series of tasks, where at each task the model learns a set of new classes that are distinct from the previously learned classes. At any given time, the model should classify between samples from all classes seen thus far. The training data at task  is denoted as  
where  is the set of input images and   are their target labels.  and  correspond to the number of samples and the set of classes at task . We further define  as the total number of classes seen at the end of task .

\subsection{Overview of the framework}

An illustration of our framework is shown in Fig. \ref{fig:framework} which consists of a feature extractor (), a Vector Quantizer (NA-VQ), and a prototype augmenter (NA-PA).  is used to obtain the feature space of the input data. NA-VQ quantizes this feature space by learning a set of CVs named  such that  is associated with class . These CVs are trained to effectively improve the inter-class variance and reduce intra-class variance in the feature space reducing the representational overlap in classes.
The module parameters  and  are shared across all tasks but are updated continually with the data at the current task, thus  denotes the states of the parameters at each task.
Consequently,  and  refer to the states of the feature extractor and the set of CVs at task .
The goal of task  is to jointly update  and  using  to obtain  and . At incremental tasks (when ), a set of old class prototypes () generated using our NA-PA technique is used alongside  to retain the discrimination between old and new classes.

\begin{figure}[t]
\centering
\includegraphics[width=0.4\textwidth]{
conceptual.png}
\caption{Conceptual illustration of NA-VQ. 
Each color represents a single class. 
(a) Given , edges are created between the closest CV (solid circle) and the next  closest CVs (dotted circles) to approximate the local topology around . (b) Class A is overlapped with classes B, C, and D thus a sample from class A () could be misclassified into B, C, or D. NA-VQ pulls  and  together and pushes  and direct neighbors of  (, and ) away from each other, reducing the class overlap. Closer the CV to , the higher these forces. 
The edge between  and  is pruned due to edge weakening over time. (c) When a new class G is introduced,  is inserted randomly and refined through attractive and repulsive forces over the iterations, identifying a less-overlapping space in the feature space.}
 \label{fig:concept}
\end{figure}


\subsection{Neighborhood-Aware Vector Quantizer (NA-VQ)}\label{sec:navq}



At task , we extract the features of  by computing  using , where  is the dimensionality of the feature space (Eq. \ref{eq:ft}). 


We assume  lies in a feature space manifold  which captures the topological properties between the classes. NA-VQ partitions  into disjoint regions such that  contains the feature distribution of class . We encode  using , i.e.,  contains one CV per class.
 The algorithm can be summarised into two iterative steps (1) the topology approximation and (2) the CV adaptation. 
These steps are performed concurrently, i.e., the topology learned depends on the CV adaptation and vice versa. A conceptual illustration of NA-VQ is shown in Fig. \ref{fig:concept}.

\textbf{(1) Topology approximation.}
Inspired by NG \cite{ThomasMartinetzandKlausSchulten1991ATopologies}, the topology of  is approximated by learning an undirected graph , where CVs are the nodes and the topological connections between the CVs are the edges represented by the adjacency matrix . 
At the beginning of task ,  and  are extended to  and  to accomodate task . Specifically,  new randomly initialized CVs are inserted without any edges that link to, from, or between them.
 During the learning of task ,  is modified by adding, decaying, and removing edges between the CVs. 



Concretely, given  and its feature representation , we calculate the distance from  to each CV in :
 where  is the Euclidean distance. 
 is then sorted to assign a rank to each CV:
 such that . Next, the edges between the CVs are formed based on a connectivity factor denoted as . A higher value of  leads to a denser graph, whereas a lower value of  results in a sparser graph. Specifically, edges are created between the closest CV and the next  closest CVs, i.e.,  for . 
Consequently, CVs that lie within the high-density regions of   develop edges, allowing to identify classes that share similar features in . Since the CVs get updated over time, the edges created in a previous iteration may become obsolete as their endpoints may have moved. To remove such edges we employ an edge decaying mechanism. All edges from the closest CV are decayed by a constant multiplier  so that the edges created at a previous iteration that no longer fall within a high-density region are weakened, i.e.,  for .
If the edge strength goes below a predefined , such edges would be pruned, i.e., if , .




\textbf{ (2) CV adaptation.} During the learning of task , the class label  in  is employed to adapt CVs in  {} to improve the discriminability in the feature space. Concretely, we create attractive forces between  and  and repulsive forces between  and any other ``confusing CVs" of . The confusing CVs of  (referred to as ) are the direct neighbors of  as determined by graph  in Step (1). 

Any  is considered to be close to , thus, there is a high likelihood that  will be mistaken for  as the winner CV when presented with sample . To counteract this, we propose a Neighborhood-Adaptation loss () (Eq. \ref{eq:sung}) to bring  and  closer together while pushing  and  farther apart. 

 computes the difference between  and  when , the scenario where  is most likely to be misclassified.
 is the distance from  to its correct CV  and  is a linear combination of the distances from  to . The weight given to  () is calculated using a monotonic decaying function (Eq. \ref{eq: wm}) to reduce the impact of  on  as the distance between  and  increases. 

 

To avoid the repelling forces in  from diverging any  from their respective class distributions, we take three steps. First, the repelling forces are distributed over multiple confusing CVs. Second,  prevents any adjustments to the CVs when , since it is unlikely that this scenario would lead to misclassification. Finally, Distance-based cross-entropy loss () \cite{Yang2018RobustLearning}, as described next, is used to encourage  to more accurately represent the distribution of class  which aids in minimizing the distance between the feature representations of class  and . 

The probability of sample , belonging to class  can be measured by the distance between  and  \cite{Yang2018RobustLearning}, i.e., . 
Considering the non-negative and sum-to-one properties of the probability, we can define  using a softmax function as shown in Eq. \ref{eq:dce_sub} where  is a temperature parameter.

Since the true probability distribution is a one hot encoded vector,  we can define  for  as Eq. \ref{eq:dce}.


These two steps are conducted concurrently at each mini-batch gradient update during the optimization of  and . By backpropagating the gradients calculated for  and  through  and , we effectively reduce the class overlapping in the feature space and establish a more discriminative decision boundary between classes. During the incremental steps , we freeze the old class representative CVs but update their topological connections using both the augmented prototypes (Sec. \ref{sec:pa}) and .

\subsection{Neighborhood-Aware Prototype Augmenter (NA-PA)}\label{sec:pa}

In NECIL, we cannot directly 
compute  or  for the previous task samples. 
While the class mean in the feature space serves as a central representative point for each old class, it does not capture the class variance. Thus, if the model relies solely on class-means without any augmentation, it may overfit to the class mean and forget to distinguish samples of the same class that are close to but not exactly equal to the class mean. Augmentation mitigates this issue by generating multiple representative points for each old class, considering the underlying class variance. Therefore, we store representative mean prototypes
and augment them on the fly with NA-PA during the learning of each new task. Specifically, we generate prototypes in the regions where the model is uncertain about the class labels. We consider pairs of classes that the model may confuse between using the topological graph  established in Sec. \ref{sec:navq}, and generate prototypes to distinguish between these pairs of classes, increasing the quality of the prototypes used for retaining old knowledge.

Inspired by the work \cite{Chu2020FeatureData}, we identify that the features of each class can be decomposed into a class-specific component (features placed closer to the class mean) and a class-shared component (features that lie between itself and another class). For a given old class, the samples near the boundaries shared with its confusing classes have the biggest impact on recovering good decision boundaries because they are closer to the regions where the model is uncertain about the class labels. Thus, NA-PA generates augmented representations of the old classes by fusing the class-specific features from the old classes with the class-generic/shared features from their confusing classes to create such high-impact prototypes (Fig. \ref{fig:framework}). 
At task , an augmented prototype  of old class  is generated as shown in Eq. \ref{eq:ai} by fusing the mean prototype of class  () with the mean prototype of class   (), a randomly picked neighbor of class . By varying , we ensure the augmented prototypes are composed with varying degrees of uncertainty, i.e., the lower the , the higher the uncertainty. Here, we are using the mean prototypes instead of CVs as these CVs are adjusted continually to reduce misclassification and may potentially be positioned near class boundaries (away from class means) for improved discrimination. We determined that class-means serve as better representatives of the class-specific features than the CVs. 


 is the collection of augmented prototypes and their class labels representing all old classes up to task . 
 is used alongside  in NA-VQ for topology approximation as well CV adaptation. Specifically, we calculate  and  using . 


\subsection{Knowledge distillation}
As  gets updated continually, the actual feature distributions of old classes drift away from their original distributions. To mitigate this drift we incorporate a feature-level knowledge distillation () \cite{Zhu2021PrototypeLearning, Zhu2021Class-IncrementalAugmentation} that attempts to align the feature spaces of the current and  the previous models.
 
The total loss used in our framework is shown below and  and  are loss weights (See Supp. Materials for explanations on loss weights)



\subsection{Rotation-based data augmentation}
In order to learn richer features, we transform the training data using the same rotation-based approach used in \cite{Zhu2021PrototypeLearning}. Concretely, the training samples are rotated by 90, 180 and 270 degrees to generate 3 new pseudo-classes, learning  classes instead of  classes at the training stage. However, the classification occurs only between the original classes during the evaluation stage.

\subsection{Classification}
We perform the nearest CV-based classification. At the end of task , given a test sample , we obtain , calculate the distance from normalized  to each normalized CV and assign the class label of the closest CV to . 


\section{Experiments}

\subsection{Datasets}
We perform comprehensive experiments using four datasets; CIFAR-100 \cite{Krizhevsky2009LearningImages}, TinyImageNet \cite{Le2015TinyChallenge}, ImageNet-Subset \cite{Deng2009Imagenet:Database}, ImageNet-1K \cite{Deng2009Imagenet:Database} in three incremental scenarios;  where  is the number incremental tasks. For comparability, the classes are arranged into tasks using the same fixed random order and division settings as \cite{Zhu2021PrototypeLearning, Zhu2022Self-SustainingLearning} for the first three datasets. For the ImageNet-1K dataset, we train the model on 400 classes for the first task, and equal classes in the rest of the tasks. 


\subsection{Implementation details} \label{sec:impl}
For a fair comparison, we adapted the same backbone architecture, ResNet-18 \cite{He2016DeepRecognition} from \cite{Zhu2021PrototypeLearning}. The concrete details of our implementation can be found in the Supp. materials and the \href{https://github.com/TamashaM/NAPA-VQ.git}{publicly available codebase}.
All the experiments were conducted on the University of Melbourneâ€™s high-performance computing system, Spartan \cite{Lafayette2016SpartanChimera}.


\begin{table*}
\centering
\caption{Average Accuracy (\%) of NAPA-VQ compared to the top three SOTA using four datasets. The higher the values, the better.  is the number of incremental tasks. Values for the methods with * were extracted from \cite{Zhu2022Self-SustainingLearning}. Our improvement is shown in red.}.\label{Tab:accuracy}
\begin{tblr}{
  width = \linewidth,
  colspec = {Q[98]Q[70]Q[70]Q[70]Q[70]Q[70]Q[70]Q[70]Q[70]Q[70]Q[70]Q[70]Q[70]},
  cell{1}{1} = {r=2}{},
  cell{1}{2} = {c=3}{0.200\linewidth},
  cell{1}{5} = {c=3}{0.200\linewidth},
  cell{1}{8} = {c=3}{0.200\linewidth},
  cell{1}{11} = {c=3}{0.200\linewidth},
  vlines,
  hline{1,3-7} = {-}{},
  hline{2} = {2-13}{},
}
Method           & CIFAR-100                                           &                                                  &                                                     & TinyImageNet                                        &                                                     &                                                     & ImageNet-Subset                                     &                                                     &                                                     & ImageNet-1K &      &      \\
                 & T=5                                                 & T=10                                             & T=20                                                & T=5                                                 & T=10                                                & T=20                                                & T=5                                                 & T=10                                                & T=20                                                & T=5         & T=10 & T=20 \\
PASS *           & 63.47                                               & 61.84                                            & 58.09                                               & 49.55                                               & 47.29                                               & 42.07                                               & 66.84                                               & 61.80                                               & 54.46                                               & -           & -    & -    \\
IL2A             & 65.61                                               & 59.09                                            & 58.82                                               & 47.02                                               & 44.48                                               & 39.68                                               & -                                                   & -                                                   & -                                                   & -           & -    & -    \\
SSRE *           & 65.88                                               & 65.04                                            & 61.70                                               & 50.39                                               & 48.93                                               & 48.17                                               & -                                                   & 67.69                                               & -                                                   & -           & -    & -    \\
\textbf{NAPA-VQ} & {\textbf{70.44}\\\textbf{\textcolor{red}{\small{(+4.56)}}}} & {\textbf{69.04}\\\textbf{\textcolor{red}{\small{(+4)}}}} & {\textbf{67.42}\\\textbf{\textcolor{red}{\small{(+5.72)}}}} & {\textbf{52.77}\\\textbf{\textcolor{red}{\small{(+2.38)}}}} & {\textbf{51.78}\\\textbf{\textcolor{red}{\small{(+2.85)}}}} & {\textbf{49.51}\\\textbf{\textcolor{red}{\small{(+1.34)}}}} & {\textbf{69.15}\\\textbf{\textcolor{red}{\small{(+2.31)}}}} & {\textbf{68.83}\\\textbf{\textcolor{red}{\small{(+1.14)}}}} & {\textbf{63.09}\\\textbf{\textcolor{red}{\small{(+8.63)}}}} & \textbf{55.11}        & \textbf{53.04} & \textbf{45.46} 
\end{tblr}
\end{table*}
\begin{table*}
\centering
\caption{Average Forgetting (\%) of NAPA-VQ compared to the top three SOTA using four datasets. The lower the values, the better.  is the number of incremental tasks. Values for the methods with * were extracted from \cite{Zhu2022Self-SustainingLearning}. Our improvement is shown in red.}
\label{Tab:forgetting}
\begin{tblr}{
  width = \linewidth,
  colspec = {Q[90]Q[85]Q[72]Q[72]Q[72]Q[75]Q[75]Q[85]Q[75]Q[85]Q[60]Q[60]Q[60]},
  cell{1}{1} = {r=2}{},
  cell{1}{2} = {c=3}{},
  cell{1}{5} = {c=3}{},
  cell{1}{8} = {c=3}{},
  cell{1}{11} = {c=3}{},
  vlines,
  hline{1,3-7} = {-}{},
  hline{2} = {2-13}{},
}
Method           & CIFAR-100                                           &                                                    &                                                    & TinyImageNet                                       &                                                     &                                                    & ImageNet-Subset                                     &                 &                                                      & ImageNet-1K &      &      \\
                 & T=5                                                 & T=10                                               & T=20                                               & T=5                                                & T=10                                                & T=20                                               & T=5                                                 & T=10            & T=20                                                 & T=5         & T=10 & T=20 \\
PASS  *          & 25.20                                               & 30.25                                              & 30.61                                              & 18.04                                              & 23.11                                               & 30.55                                              & 19.66                                               & 25.85           & 30.98                                                & -           & -    & -    \\
IL2A             & 28.72                                               & 39.86                                              & 40.70                                              & 19.74                                              & 29.90                                               & 39.99                                              & -                                                   & -               & -                                                    & -           & -    & -    \\
SSRE *           & 18.37                                               & 19.48                                              & 19.00                                              & 9.17                                               & 14.06                                               & 14.20                                              & -                                                   & \textbf{8.30}   & -                                                    & -           & -    & -    \\
\textbf{NAPA-VQ} &{\textbf{6.90}\\\textbf{\textcolor{red}{\small{(-11.47)}}}}& {\textbf{9.65}\\\textbf{\textcolor{red}{\small{(-9.83)}}}}& {\textbf{9.08}\\\textbf{\textcolor{red}{\small{(-9.92)}}}}& {\textbf{9.08}\\\textbf{\textcolor{red}{\small{(-0.09)}}}} & {\textbf{10.81}\\\textbf{\textcolor{red}{\small{(-3.25)}}}} & {\textbf{9.31}\\\textbf{\textcolor{red}{\small{(-4.89)}}}} & {\textbf{7.17}\\\textbf{\textcolor{red}{\small{(-12.49)}}}} & {9.67\\\small{(+1.37)}} & {\textbf{14.49}\\\textbf{\textcolor{red}{\small{(-16.49)}}}} & \textbf{10.45}        & \textbf{10.94} & \textbf{18.23} 
\end{tblr}
\end{table*}

\begin{table*}[t]
\centering
\caption{Average Accuracy (\%) and Average Forgetting (\%) obtained for the ablation study conducted using CIFAR-100 to evaluate the effectiveness of NA-VQ and NA-PA. We refer to the combination of DCE and NA as NA-VQ.}
\label{Tab:ablation}
\begin{tblr}{
  width = \linewidth,
  colspec = {Q[392]Q[88]Q[88]Q[88]Q[90]Q[90]Q[90]},
  cell{1}{1} = {r=2}{},
  cell{1}{2} = {c=3}{0.264\linewidth},
  cell{1}{5} = {c=3}{0.27\linewidth},
  vlines,
  hline{1,3-8} = {-}{},
  hline{2} = {2-7}{},
}
Ablation~                                       & Average Accuracy  &                &                & Average Forgetting  &               &                \\
                                                & T=5              & T=10           & T=20           & T=5                & T=10          & T=20           \\
1) KD + CCE (Baseline)                          & 26.21            & 16.70          & 10.93          & 85.29              & 89.77         & 93.49          \\
2) KD + \textbf{DCE} & 49.83            & 32.90          & 17.01          & 48.84              & 75.37         & 81.71          \\
3) KD + DCE + \textbf{NA}  KD + \textbf{NA-VQ}           & 49.31            & 42.16          & 42.13          & 45.03              & 41.76         & 30.09          \\
4) KD + NA-VQ + \textbf{Gaussian-PA}                & 68.84            & 65.44          & 62.39          & 10.78              & 16.20         & 18.30          \\
5) KD + NA-VQ + \textbf{NA-PA}  \textbf{NAPA-VQ}              & \textbf{70.44}   & \textbf{69.04} & \textbf{67.42} & \textbf{ 6.90}     & \textbf{ 9.65} & \textbf{ 9.08} 
\end{tblr}

\end{table*}

\begin{figure}[t]
\centering
\includegraphics[width=0.42\textwidth]{
acc.png}
\caption{Detailed Accuracy curves showing the Top-1 Accuracy at each incremental step.}
\label{fig:acc}
\end{figure} 

\subsection{Evaluation metrics}
In line with previous works \cite{Zhu2021PrototypeLearning,Zhu2022Self-SustainingLearning}, we report the standard metrics used to evaluate CIL strategies: Average Accuracy and Average Forgetting. Accuracy at task  is the average accuracy of all the classes that have been learned up to and during task . 
\textbf{Average accuracy} \cite{Chaudhry2018RiemannianIntransigence} is the mean accuracy across all the tasks, including the initial task. \textbf{Forgetting} at any given time for a task previously encountered, is measured by the difference between the maximum accuracy for the task during the learning process and the current accuracy for the same task. \textbf{Average Forgetting} at the end of task  is therefore defined as the average of forgetting values for all the tasks learned up to task  \cite{Chaudhry2018RiemannianIntransigence}. 
We report the average forgetting at the end of the final task. Additional explanations related to evaluation metrics can be found in Supp. materials.

\begin{figure}[t]
\centering
\includegraphics[width=0.47\textwidth]{cfm.png}
\caption{Confusion matrices for Fine-tuning, SSRE and NAPA-VQ for CIFAR-100. Both SSRE and NAPA-VQ reduce the task recency bias observed in fine-tuning. Along the diagonal, NAPA-VQ has more red patches than SSRE.}
\label{fig:cfm}
\end{figure}

\begin{figure*}[t]
\centering
\includegraphics[width=0.98\textwidth]{overlap_new.png}
\caption{Visualization of the impact of NA-VQ and NA-PA on the feature representations. Each colour represents a single class. The areas highlighted depict the observable differences between experiments. After learning the initial task (first row), NA-VQ integrated models (b and c) reduces much of the class overlap seen in the baseline model (a). The feature space for (b) and (c) are identical at this stage since NA-PA is only applied to the incremental tasks. After learning the final task (second row), NA-VQ integrated model (b) shows better discrimination between the old classes as well as between old and new classes compared to the baseline (a). When NA-PA is integrated, the discrimination between old and new classes improves further. 
}
\label{fig:overlap}
\end{figure*} 
\subsection{Comparison with SOTA}


We compare NAPA-VQ with the existing state-of-the-art (SOTA) methods in NECIL, including EWC \cite{Kirkpatrick2017OvercomingNetworks}, LwF\_MC \cite{Lampert2017}, MUC \cite{Liu2020MoreLearning},
PASS \cite{Zhu2021PrototypeLearning}, IL2A \cite{Zhu2021Class-IncrementalAugmentation}, and SSRE \cite{Zhu2022Self-SustainingLearning}. We report the average accuracy and average forgetting of NAPA-VQ against the top three performing methods in Tables \ref{Tab:accuracy} and \ref{Tab:forgetting}. Reported values are the average of three separate runs. As illustrated in Table \ref{Tab:accuracy}, NAPA-VQ demonstrates an average improvement of 5\%, 2\%, and 4\% in accuracy for CIFAR-100, TinyImageNet, and ImageNet-Subset, respectively, over the best existing NECIL-SOTA technique. The detailed accuracy curves for the compared methods for CIFAR-100, TinyImageNet (in Fig. \ref{fig:acc}) and ImageNet-Subset (in Supp. Fig. 1), show that NAPA-VQ maintains higher accuracies over the incremental tasks. Moreover, NAPA-VQ exhibits a significant reduction in forgetting by an average of 10\%, 3\% and 9\% for CIFAR-100, TinyImageNet, and ImageNet-Subset, respectively. This reduction in forgetting is prominent when dealing with a larger number of tasks (Table \ref{Tab:forgetting}). 
In addition, we provide results on ImageNet-1K, demonstrating the effectiveness of NAPA-VQ on large-scale datasets. Furthermore, we compare NAPA-VQ to traditional exemplar-based methods, iCARL \cite{Lampert2017}, EEIL \cite{Castro2018End-to-endLearning}, and UCIR \cite{Hou2019LearningRebalancing} trained using a limited number of exemplars (20) (Fig. \ref{fig:acc}) and show that NAPA-VQ obtains competitive performance.  

\subsection{Ablation study}
     The ablation study is conducted using the CIFAR-100 dataset to demonstrate the impact of NA-VQ and NA-PA (Table \ref{Tab:ablation}). 
We first train a baseline model using the Categorical Cross Entropy loss () and  (Ablation-1) \cite{Zhu2022Self-SustainingLearning}. We then incorporate NA-VQ (Ablation-2 and 3) and NA-PA (Ablation-5) into the baseline model in sequence. For comparability, all the models use the same ResNet-18 architecture and the rotation-based data transformation.

  NA-VQ combines two losses;  and  thus these losses are separately incorporated into the baseline model to comprehend their individual impact. First we substitute  for  (Ablation-2). Although this replacement improves accuracy across all three incremental scenarios compared to the baseline, the forgetting is still significant when  and . In Ablation-3, we add  to  and  and observe comparable or better accuracy in all three scenarios while significantly reducing forgetting compared to  Ablation-2. This showcases the effectiveness of  in mitigating the interference of feature representations over the incremental steps. Finally, we integrate NA-PA in Ablation-5 and compare results to Ablation-3 which does not employ any prototypes and Ablation-4 which employs Gaussian-augmented prototypes  \cite{Zhu2021PrototypeLearning}. Our findings show that NA-PA enhances accuracy and reduces forgetting in all incremental scenarios. This improvement can be attributed to the prototypes generated closer to the boundaries which aid in identifying optimal decision boundaries, subsequently reducing the misclassification rate. Moreover, the advantages of NA-VQ and NA-PA are prominent when a larger number of tasks are involved, highlighting the method's importance.
\subsection{Reduced overlapping in the feature space}

We visualize the feature space of  scenario in Ablation-1 (Baseline), Ablation-3 (KD + NA-VQ) and Ablation-5  (NAPA-VQ) in Fig. \ref{fig:overlap} using t-SNE \cite{derMaaten2008VisualizingT-SNE.} to show the impact of NA-VQ and NA-PA. Specifically, we visualize the feature representations of a randomly selected subset of classes learned during the initial task at two-time points: (1) after the initial task training, and (2) after the final task training with a subset of new classes from the final task. Once the first task is learned, the class-representative features of the baseline model overlap, whereas those of the NA-VQ integrated models are more compact and distinct, reducing the misclassification rate. Once the final task is learned, the overlapping in the baseline model increases creating further confusion between classes. NA-VQ integrated model reduces this overlap due to the more discretized feature space of the old classes and the repulsive forces between old and new classes. When NA-PA is integrated on top of NA-VQ the discrimination between the old classes as well as the discrimination between the old and new classes improve further showing the positive impact of prototypes.

\subsection{Comparison of confusion matrices}

Fig. \ref{fig:cfm} shows a comparison between the confusion matrices generated for (1) simple Fine-tuning where a model is trained using CCE loss incrementally without using any strategies to mitigate forgetting, (2) SSRE and (3) NAPA-VQ. The diagonal entries in the matrices represent correct predictions, while off-diagonal entries denote misclassifications. The predictions of Fine-tuning are heavily biased towards the most recent tasks due to the forgetting of old classes. SSRE and NAPA-VQ eliminate much of this bias by correctly classifying both old and new classes. Although quite similar, more red patches are visible along the diagonal in NAPA-VQ compared to SSRE, which explains the higher average accuracies in NAPA-VQ compared to SSRE.


\subsection{Impact of the connectivity factor }

 To determine the impact of the connectivity factor  on performance, we conducted an experiment on the CIFAR-100 dataset by varying the value of  between 2 and 50, with  being the commonly used heuristic \cite{ThomasMartinetzandKlausSchulten1991ATopologies, Fritzke1994ATopologies}. The results show that as  increases, the performance improves but so does the running time (Supp. Fig. 3). The improved performance 
 can be attributed to a wider neighborhood being considered to
improve both decision boundary learning and prototype augmentation. A value of  was found to provide desirable performance without compromising algorithm efficiency.

\section{Conclusion}
In this manuscript, we proposed NAPA-VQ, a novel method for CIL that does not rely on previous task exemplars to retain old knowledge. Instead, we increase the discriminability of the feature space by using class neighborhood information captured by a topological approximation of the feature space. Furthermore, we show that generating representative prototypes for old classes by borrowing the shared features of their neighboring classes helps to establish good decision boundaries between the areas where the classes tend to overlap. Comprehensive experiments on four benchmarking datasets demonstrate the superiority of our method over existing NECIL methods. While the proposed method exerts no limit on how many CVs to be used per class, we used one CV per class in our experiments. A future study may explore the effect of using a larger number of CVs per class on both the running time and the incremental learning performance. 

\textbf{Acknowledgements} T.M. acknowledges Melbourne Graduate Research Scholarship and
GCI Women in STEM Student Award support scheme. D.S. and S.H.
acknowledge Australian Research Council grant DP210101135. The authors thank Sachith Seneviratne, Maneesha Perera, Rashindrie Perera, and Nisal Ranasinghe for proofreading.


{\small
\bibliographystyle{ieee_fullname}
\bibliography{references}
}

\end{document}
