
\documentclass{llncs}
\usepackage{version}
\usepackage{ma}

\pagestyle{plain}
\raggedbottom

\title{Instruction Sequences Expressing \\ Multiplication Algorithms}
\author{J.A. Bergstra \and C.A. Middelburg}
\institute{Informatics Institute, Faculty of Science, University of
           Amsterdam, \\
           Science Park~904, 1098~XH Amsterdam, the Netherlands \\
           \email{J.A.Bergstra@uva.nl,C.A.Middelburg@uva.nl}}

\begin{document}
\maketitle

\begin{abstract}
For each function on bit strings, its restriction to bit strings of any 
given length can be computed by a finite instruction sequence that 
contains only instructions to set and get the content of Boolean 
registers, forward jump instructions, and a termination instruction.
We describe instruction sequences of this kind that compute the function 
on bit strings that models multiplication on natural numbers less than 
 with respect to their binary representation by bit strings of 
length , for a fixed but arbitrary , according to the long 
multiplication algorithm and the Karatsuba multiplication algorithm.
We find among other things that the instruction sequence expressing the 
former algorithm is longer than the one expressing the latter algorithm 
only if the length of the bit strings involved is greater than .
We also go into the use of an instruction sequence with backward jump 
instructions for expressing the long multiplication algorithm.
This leads to an instruction sequence that it is shorter than the other 
two if the length of the bit strings involved is greater than .
\begin{keywords} 
bit string function, 
single-pass instruction sequence, backward jump instruction, 
long multiplication algorithm, Karatsuba multiplication algorithm,
halting problem.
\end{keywords}\begin{classcode}
F.1.1, F.2.1.
\end{classcode}
\end{abstract}

\section{Introduction}
\label{sect-intro}

This paper belongs to a line of research in which issues relating to 
various subjects from computer science, including programming language
expressiveness, computability, computational complexity, algorithm 
efficiency, algorithmic equivalence of programs, program verification, 
program performance, program compactness, and program parallelization, 
are rigorously investigated thinking in terms of instruction sequences.
An enumeration of most papers belonging to this line of research is 
available at~\cite{SiteIS}.
The work on computational complexity presented in~\cite{BM13a,BM14e} and 
the work on algorithmic equivalence of programs presented 
in~\cite{BM14a} were prompted by the fact that, for each function on bit 
strings, its restriction to bit strings of any given length can be 
computed by a finite instruction sequence that contains only 
instructions to set and get the content of Boolean registers, forward 
jump instructions, and a termination instruction.

This fact also incited us to look for finite instruction sequences 
containing only the above-mentioned instructions that compute a 
well-known function on bit strings of a given length.
In~\cite{BM13b}, we did so taking the hash function SHA-256 from the 
Secure Hash Standard~\cite{NIST12a} as the well-known function on bit 
strings.
In the current paper, we do so taking the function that models 
multiplication on natural numbers less than  with respect to their 
binary representation by bit strings of length , for a fixed but 
arbitrary , as the well-known function on bit strings.

We describe finite instruction sequences containing only the 
above-mentioned instructions that compute this function according to the 
standard multiplication algorithm, which is known as the long 
multiplication algorithm, and according to the Karatsuba multiplication 
algorithm~\cite{Kar95a,KO62a}.
We calculate the exact size of the instruction sequence expressing the 
long multiplication algorithm and lower and upper estimates for the size
of the instruction sequence expressing the Karatsuba multiplication 
algorithm.
We find among other things that the instruction sequence expressing the 
former algorithm is longer than the instruction sequence expressing the 
latter algorithm  only if the length of the bit strings involved is 
greater than .

We also go into the use of an instruction sequence with backward jump 
instructions for expressing the long multiplication algorithm.
We describe a finite instruction sequence containing a backward jump 
instruction, in addition to the above-mentioned instructions, that 
expresses a minor variant of the long multiplication algorithm.
We calculate the exact size of this instruction sequence and find among 
other things that it is shorter than the other two if the length of the 
bit strings involved is greater than .
In addition, we argue that the instruction sequences expressing the
long multiplication algorithm form a hard witness of the inevitable 
existence of a halting problem in the practice of impera\-tive
programming.

The Karatsuba multiplication algorithm was devised by Karatsuba in 1962 
to disprove the conjecture made by Kolmogorov that any algorithm to 
compute the function that models multiplication on natural numbers with
respect to their representations in the binary number system has time 
complexity .
Shortly afterwards, this divide-and-conquer algorithm was generalized by 
Toom and Cook~\cite{Coo66a,Too63a}.
Later, asymptotically faster multiplication algorithms, based on fast 
Fourier transforms, were devised by Sch\"{o}nhage and 
Strassen~\cite{SS71a} and F\"{u}rer~\cite{Fur09a}.
To our knowledge, except for the Sch\"{o}nhage-Strassen algorithm, only 
informal (natural language or pseudo code) descriptions of these 
multiplication algorithms are available.
In this paper, we provide a mathematically precise alternative to the
informal descriptions of the Karatsuba multiplication algorithm, using
terms from an algebraic theory of single-pass instruction sequences
introduced in~\cite{BL02a}.

It is customary that computing practitioners phrase their explanations 
of issues concerning programs from an empirical perspective such as 
the perspective that a program is in essence an instruction sequence.
An attempt to approach the semantics of programming languages from this 
perspective is made in~\cite{BL02a}.
The groundwork for the approach is an algebraic theory of single-pass
instruction sequences, called program algebra, and an algebraic theory
of mathematical objects that represent the behaviours produced by
instruction sequences under execution, called basic thread algebra.\footnote
{In~\cite{BL02a}, basic thread algebra is introduced under the name
 basic polarized process algebra.
}
The line of research referred to at the beginning of this introduction 
originates from the above-mentioned work on an approach to programming 
language semantics.

The general aim of this line of research is to bring instruction 
sequences as a theme in computer science better into the picture.
This is the general aim of the work presented in the current paper as 
well.
However, different from usual in the work referred to above, the accent 
is this time mainly on a practical problem, namely the problem to devise 
instruction sequences that express the long multiplication algorithm and 
the Karatsuba multiplication algorithm.
As in the work referred to above, the work presented in the current 
paper is carried out in the setting of program algebra.

This paper is organized as follows.
First, we survey program algebra and the particular fragment and 
instantiation of it that is used in this paper (Section~\ref{sect-PGA})
and sketch the Karatsuba multiplication algorithm 
(Section~\ref{sect-sketch-KMA}).
Next, we describe how we deal with -bit words by means of Boolean 
registers (Section~\ref{sect-words}) and how we compute the operations 
on -bit words that are used in the long multiplication algorithm 
and/or the Karatsuba multiplication algorithm 
(Section~\ref{sect-opns-words}).\linebreak[2]
Then, we describe and analyze instruction sequences that express these 
algorithms (Section~\ref{sect-KMA}).
After this, we go into the use of an instruction sequence with backward 
jump instructions for expressing the long multiplication algorithm
(Sections~\ref{sect-BJMP}) and relate the findings to the halting 
problem (Section~\ref{sect-HP}). 
Finally, we make some concluding remarks (Section~\ref{sect-concl}).

We rely in this paper on an intuitive understanding of what is an 
algorithm and when an instruction sequence expresses an algorithm.
A rigorous study of these issues and related ones, carried out in the 
same setting as the work presented in this paper, is presented 
in~\cite{BM14a}.

The preliminaries to the work presented in this paper are the same as 
the preliminaries to the work presented in~\cite{BM13b}, which are in
turn a selection from the preliminaries to the work presented 
in~\cite{BM13a}.
For this reason, there is some text overlap with those papers.
The preliminaries concern program algebra.
We only give a brief summary of program algebra.
A comprehensive introduction, including examples, can among other things
be found in~\cite{BM12b}.

This paper consolidates material from~\cite{BM13c,BM13d}.

\section{Program Algebra}
\label{sect-PGA}

In this section, we present a brief outline of \PGA\ (ProGram Algebra) 
and the particular fragment and instantiation of it that is used in 
the remainder of this paper.
A mathematically precise treatment can be found in~\cite{BM13a}.

The starting-point of \PGA\ is the simple and appealing perception
of a sequential program as a single-pass instruction sequence, i.e.\ a
finite or infinite sequence of instructions of which each instruction is
executed at most once and can be dropped after it has been executed or
jumped over.

It is assumed that a fixed but arbitrary set  of
\emph{basic instructions} has been given.
The intuition is that the execution of a basic instruction may modify a 
state and produces a reply at its completion.
The possible replies are  and .
The actual reply is generally state-dependent.
Therefore, successive executions of the same basic instruction may
produce different replies.
The set  is the basis for the set of instructions that may 
occur in the instruction sequences considered in \PGA.
The elements of the latter set are called \emph{primitive instructions}.
There are five kinds of primitive instructions, which are listed below:
\begin{itemize}
\item
for each , a \emph{plain basic instruction} ;
\item
for each , a \emph{positive test instruction} ;
\item
for each , a \emph{negative test instruction} ;
\item
for each , a \emph{forward jump instruction} ;
\item
a \emph{termination instruction} .
\end{itemize}
We write  for the set of all primitive instructions.

On execution of an instruction sequence, these primitive instructions
have the following effects:
\begin{itemize}
\item
the effect of a positive test instruction  is that basic
instruction  is executed and execution proceeds with the next
primitive instruction if  is produced and otherwise the next
primitive instruction is skipped and execution proceeds with the
primitive instruction following the skipped one --- if there is no
primitive instruction to proceed with,
inaction occurs;
\item
the effect of a negative test instruction  is the same as
the effect of , but with the role of the value produced
reversed;
\item
the effect of a plain basic instruction  is the same as the effect
of , but execution always proceeds as if  is produced;
\item
the effect of a forward jump instruction  is that execution
proceeds with the th next primitive instruction of the instruction
sequence concerned --- if  equals  or there is no primitive
instruction to proceed with, inaction occurs;
\item
the effect of the termination instruction  is that execution
terminates.
\end{itemize}

To build terms, \PGA\ has a constant for each primitive instruction and 
two operators. 
These operators are: the binary concatenation operator  and 
the unary repetition operator .
We use the notation , where  are 
\PGA\ terms, for the PGA term .
We also use the notation . 
For each \PGA\ term  and ,  is the \PGA\ term defined by 
induction on  as follows:  and .

The instruction sequences that concern us in the remainder of this paper 
are the finite ones, i.e.\ the ones that can be denoted by closed \PGA\ 
terms in which the repetition operator does not occur. 
Moreover, the basic instructions that concern us are instructions to set 
and get the content of Boolean registers.
More precisely, we take the set
\begin{ldispl}
\set{\inbr{i}.\getbr \where i \in \Natpos} \union
\set{\outbr{i}.\setbr{b} \where i \in \Natpos \Land b \in \Bool}
\\ \;\; {} \union
\set{\auxbr{i}.\getbr \where i \in \Natpos} \union
\set{\auxbr{i}.\setbr{b} \where i \in \Natpos \Land b \in \Bool} 
\end{ldispl}as the set  of basic instructions.

Each basic instruction consists of two parts separated by a dot.
The part on the left-hand side of the dot plays the role of the name of 
a Boolean register and the part on the right-hand side of the dot plays 
the role of a command to be carried out on the named Boolean register.
For each :
\begin{itemize}
\item
 serves as the name of the Boolean register that is used as 
th input register in instruction sequences;
\item
 serves as the name of the Boolean register that is used as
th output register in instruction sequences;
\item
 serves as the name of the Boolean register that is used as 
th auxiliary register in instruction sequences.
\end{itemize}
On execution of a basic instruction, the commands have the following 
effects:
\begin{itemize}
\item
the effect of  is that nothing changes and the reply is the 
content of the named Boolean register;
\item
the effect of  is that the content of the named Boolean 
register becomes  and the reply is ;
\item
the effect of  is that the content of the named Boolean 
register becomes  and the reply is .
\end{itemize}

Let , let , and let
 be a finite instruction sequence that can be denoted by a closed 
\PGA\ term in the case that  is taken as specified above.
Then  \emph{computes}  if there exists a  such that 
for all : if  is executed in an environment 
with  input registers,  output registers, and  auxiliary 
registers, the content of the input registers with names 
 are  when execution starts, 
and the content of the output registers with names 
 are  when execution 
terminates, then .

\section{Sketch of the Karatsuba Multiplication Algorithm}
\label{sect-sketch-KMA}

Suppose that  and  are two natural numbers with a binary 
representation of  bits. 
As a first step toward multiplying  and ,
split each of these representations into a left part of length 
 and a right part of length .
Let us say that the left and right part of the representation of  
represent natural numbers  and  and the left and right part of 
the representation of  represent natural numbers  and .
It is obvious that  and 
.
From this it follows immediately that
\begin{ldispl} 
x\mul y = 
2^{2 \mul \ceil{n/2}} \mul (x_L \mul y_L) + 
2^\ceil{n/2} \mul (x_L \mul y_R + x_R \mul y_L) + x_R \mul y_R\;.
\end{ldispl}In addition to this, it is known that
\begin{ldispl} 
x_L \mul y_R + x_R \mul y_L = 
(x_L + x_R) \mul (y_L + y_R) - x_L \mul y_L - x_R \mul y_R\;.
\end{ldispl}Moreover, it is easy to see that multiplications by powers of  are 
merely bit shifts on the binary representation of the natural numbers 
involved.
All this means that, on the binary representations of  and , the 
multiplication  can be replaced by three multiplications: 
, , and .
These three multiplications concern natural numbers with binary 
representations of length , , and 
, respectively.
For each of these multiplications it holds that, if the binary 
representation length concerned is greater than , the multiplication 
can be replaced by three multiplications of natural numbers with binary 
representations of even shorter length. 

The \emph{Karatsuba multiplication algorithm} is the algorithm that 
computes the binary representation of the product of two natural numbers 
with binary representations of the same length by dividing the 
computation into the computation of the binary representations of three 
products as indicated above and doing so recursively until it not any 
more leads to further length  reduction. 
The remaining products are usually computed according to the standard 
multiplication algorithm, which is known as the long multiplication 
algorithm.

Both the Karatsuba multiplication algorithm and the long multiplication 
algorithm can actually be applied to natural numbers represented in the 
binary number system as well as natural numbers represented in the 
decimal number system.
The long multiplication algorithm is the multiplication algorithm that 
is taught in schools for computing the product of natural numbers 
represented in the decimal number system.
It is known that the long multiplication algorithm has uniform time 
complexity  and the the Karatsuba multiplication algorithm 
has uniform time complexity 
, 
so the Karatsuba multiplication algorithm is asymptotically faster than 
the long multiplication algorithm.

\section{Dealing with -Bit Words}
\label{sect-words}

This section is concerned with dealing with bit strings of length  
by means of Boolean registers.
It contains definitions which facilitate the description of instruction 
sequences that express the long multiplication algorithm and the 
Karatsuba multiplication algorithm.

Henceforth, it is assumed that a fixed but arbitrary positive natural 
number  has been given.
The above-mentioned algorithms compute the binary representation of the 
product of two natural numbers represented by bit strings of the same 
length.
In Section~\ref{sect-KMA}, the instruction sequences expressing these 
algorithms will be described for the case where this length is .

In the sequel, bit strings of length  will mostly be called 
\emph{-bit words}.
The prefix ``-bit'' is left out if  is irrelevant or clear from
the context.

Let  
(, 
 ) be the name of a Boolean register.
Then  and  are called the \emph{kind} and \emph{number} of 
the Boolean register.
Successive Boolean registers are Boolean registers of the same kind with
successive numbers.
Words are stored by means of Boolean registers such that the successive 
bits of a stored word are the contents of successive Boolean registers.

Henceforth, the name of a Boolean register will mostly be used to refer 
to the Boolean register in which the least significant bit of a word is 
stored.
Let  and  be the names of Boolean registers 
and let .
Then we say that  and  \emph{lead to partially 
coinciding -bit words} if  and .

The -bit words representing the two natural numbers for which the 
binary representation of their product is to be computed are stored in 
advance of the computation in input registers, starting with the input 
register with number .
It is convenient to have available the names  and  for the 
input registers in which the least significant bit of these words are 
stored.
The -bit word representing the product is stored just before the end 
of the computation in output registers, starting with the output 
register with number . 
It is convenient to have available the name  for the output register 
in which the least significant bit of this word is stored.
The words representing intermediate values that arise during the 
computation are temporarily stored in auxiliary registers, starting with 
the auxiliary register with number .

In the case of the Karatsuba algorithm, the binary representation of the 
product of two natural numbers with binary representations of the same 
length is computed by dividing the computation into the computation of 
the binary representations of three products and doing so recursively 
until it not any more leads to further length reduction. 
Therefore, it is convenient to have available, for sufficiently many 
natural numbers , the names ,  and  for the 
auxiliary registers in which the least significant bit of the binary
representations of smaller natural numbers and their product are stored. 
Because at each level of recursion, except the last level, the 
computation of the binary representation of a product involves the 
computation of the binary representations of three products at the next 
level, it is convenient to have available, for sufficiently many natural 
numbers , the names ,  and  for the auxiliary 
registers in which the least significant bit of these binary 
representations of products are stored. 

It is also convenient to have available the names  for 
the auxiliary registers in which the least significant bit of the words 
that represent the intermediate values that arise, other than the ones 
mentioned in the previous paragraph, are stored.
Moreover, it is convenient to have available the name  for the 
auxiliary register that contains the carry/borrow bit that is repeatedly 
stored when computing the operations that model addition and subtraction 
on natural numbers with respect to their binary representation.

Therefore, we define: 
\begin{ldispl}
\begin{asceqns}
I_1           & \deq & \inbr{1}, \\
I_2 \hsp{.25} & \deq & \inbr{k}
              & \mathrm{where}\; k = N + 1, \\ 
O             & \deq & \outbr{1}, \\
c             & \deq & \auxbr{1}, & \hsp{24.6} \\ S_1           & \deq & \auxbr{2}, 
\end{asceqns}
\end{ldispl}\begin{ldispl}
\begin{asceqns} 
S_2        & \deq & \auxbr{k}
           & \mathrm{where}\; k = 2 \mul N + 2, \\   
T_1        & \deq & \auxbr{k}
           & \mathrm{where}\; k = 4 \mul N + 2, \\
T_2        & \deq & \auxbr{k}
           & \mathrm{where}\; k = 6 \mul N + 2, \\ 
I_1^i      & \deq & \auxbr{k}
           & \mathrm{where}\; k = 10 \mul N \mul i +  8 \mul N + 2
 & (0 \leq i \leq \ceil{\log_2(N-2)}), \\
I_2^i      & \deq & \auxbr{k}
           & \mathrm{where}\; k = 10 \mul N \mul i +  9 \mul N + 2
 & (0 \leq i \leq \ceil{\log_2(N-2)}), \\
O^i        & \deq & \auxbr{k}
           & \mathrm{where}\; k = 10 \mul N \mul i + 10 \mul N + 2
 & (0 \leq i \leq \ceil{\log_2(N-2)}), \\
P_1^i      & \deq & \auxbr{k}
           & \mathrm{where}\; k = 10 \mul N \mul i + 12 \mul N + 2
 & (0 \leq i \leq \ceil{\log_2(N-2)}), \\
P_2^i      & \deq & \auxbr{k}
           & \mathrm{where}\; k = 10 \mul N \mul i + 14 \mul N + 2
 & (0 \leq i \leq \ceil{\log_2(N-2)}), \\
P_3^i      & \deq & \auxbr{k}
           & \mathrm{where}\; k = 10 \mul N \mul i + 16 \mul N + 2
 & (0 \leq i \leq \ceil{\log_2(N-2)}).
\end{asceqns}
\end{ldispl}Here  ranges over natural numbers in the interval with lower endpoint 
 and upper endpoint .
This needs some explanation.
\begin{proposition}
\label{prop-recursion-depth}
The recursion depth of the Karatsuba multiplication algorithm applied to 
bit strings of length  is .
\end{proposition}
\begin{proof}
Let .
In the Karatsuba multiplication algorithm, the binary representation of 
the product of two natural numbers with binary representations of length 
 is computed by dividing the computation into the computation of the 
binary representation of a product of two natural numbers with binary 
representations of length , the binary representation of a 
product of two natural numbers with binary representations of length 
, and the binary representation of a product of two natural 
numbers with binary representations of length .
The function  defined by  has the following
properties: (a)~ iff ; and (b)~for , the least 
 such that  is .
This implies that the recursion depth is .
\qed
\end{proof}
Proposition~\ref{prop-recursion-depth} tells us that the maximum level 
of recursion that can be reached is .
So there are  possible levels of recursion, 
\linebreak[2] viz.\ , \ldots, .
This means that there are sufficiently many natural numbers  for 
which the names , , , , , and  
have been introduced above.
In Section~\ref{sect-KMA}, we will use the names , , 
, , , and  at the level of recursion 
.

\section{Computing Operations on -Bit Words}
\label{sect-opns-words}

This section is concerned with computing operations on bit strings of 
length .
It contains definitions which facilitate the description of instruction 
sequences that express the long multiplication algorithm and the 
Karatsuba multiplication algorithm.

In this section, we will write , where  and 
 are bit strings, for the concatenation of  and .
In other words, we will use juxtaposition for concatenation.
Moreover, we will use the bit string notation .
For , the bit string , where , is defined 
by induction on  as follows:  and .

The basic operations on words that are relevant to the long 
multiplication algorithm and/or the Karatsuba multiplication algorithm 
are the operations that model addition, subtraction, and multiplication 
by , modulo , on natural numbers less than , with respect 
to their binary representation by -bit words (, 
).
The operation modeling multiplication by  is commonly known as 
``shift left by  positions''.
For these operations, we define parameterized instruction sequences 
computing them in case the parameters are properly instantiated (see 
below):
\begin{ldispl}
\ADD{n}(\srcbri{1}{k_1},\srcbri{2}{k_2},\dstbr{l}) \deq {} 
\\ \quad
c.\setbr{\False} \conc {}
\\ \quad
\Conc{i = 0}{n-1} 
 (\ptst{\srcbri{1}{k_1{+}i}.\getbr} \conc \fjmp{8}   \conc 
  \ptst{\srcbri{2}{k_2{+}i}.\getbr} \conc \fjmp{8}   \conc 
  \ntst{c.\getbr}                   \conc \fjmp{14}  \conc {} 
\\ \quad \phantom{\Conc{i = 0}{n-1} (}
  \dstbr{l{+}i}.\setbr{\True} \conc c.\setbr{\False} \conc
   \fjmp{13} \conc 
  \ptst{\srcbri{2}{k_2{+}i}.\getbr} \conc \fjmp{4}   \conc 
  \ptst{c.\getbr}    \conc \fjmp{7} \conc \fjmp{7}   \conc {} 
\\ \quad \phantom{\Conc{i = 0}{n-1} (} 
  \ptst{c.\getbr}                   \conc \fjmp{5}   \conc 
  \dstbr{l{+}i}.\setbr{\False} \conc c.\setbr{\True} \conc
   \fjmp{3} \conc
  \ptst{\dstbr{l{+}i}.\setbr{\False}} \conc
  \dstbr{l{+}i}.\setbr{\True})\;,
\eqnsep
\SUB{n}(\srcbri{1}{k_1},\srcbri{2}{k_2},\dstbr{l}) \deq {} 
\\ \quad
c.\setbr{\False} \conc {}
\\ \quad
\Conc{i = 0}{n-1} 
 (\ntst{\srcbri{1}{k_1{+}i}.\getbr} \conc \fjmp{8}    \conc 
  \ptst{\srcbri{2}{k_2{+}i}.\getbr} \conc \fjmp{8}    \conc 
  \ntst{c.\getbr}                   \conc \fjmp{14}   \conc {} 
\\ \quad \phantom{\Conc{i = 0}{n-1} (}
  \dstbr{l{+}i}.\setbr{\False} \conc c.\setbr{\False} \conc
   \fjmp{13} \conc 
  \ptst{\srcbri{2}{k_2{+}i}.\getbr} \conc \fjmp{4}    \conc 
  \ptst{c.\getbr}    \conc \fjmp{7} \conc \fjmp{7}    \conc {} 
\\ \quad \phantom{\Conc{i = 0}{n-1} (} 
  \ptst{c.\getbr}                   \conc \fjmp{5}    \conc 
  \dstbr{l{+}i}.\setbr{\True} \conc c.\setbr{\True}   \conc
   \fjmp{3} \conc
  \ntst{\dstbr{l{+}i}.\setbr{\True}} \conc
  \dstbr{l{+}i}.\setbr{\False})\;,
\eqnsep
\SHL{n}{m}(\srcbr{k},\dstbr{l}) \deq {}
\\ \quad
\Conc{i = 0}{n-1-m} 
 (\ptst{\srcbr{k{+}n{-}1{-}m{-}i}.\getbr} \conc 
  \ntst{\dstbr{l{+}n{-}1{-}i}.\setbr{\True}} \conc
  \dstbr{l{+}n{-}1{-}i}.\setbr{\False}) \conc {}
\.5ex] \quad
\Conc{i = 0}{m-1} (\dstbr{l{+}n{-}m{+}i}.\setbr{\False})\;,
\hsp{17.75} \end{ldispl}\begin{ldispl}
\DEC{n}(\srcbr{k},\dstbr{l}) \deq {}
\\ \quad
\Conc{i = 0}{n-1} 
 (\ntst{\srcbr{k{+}i}.\getbr} \conc \fjmp{3} \conc
  \dstbr{l{+}i}.\setbr{\False} \conc \fjmp{5} \conc
  \dstbr{l{+}i}.\setbr{\True}) \conc
\fjmp{1} \conc \fjmp{1} \conc \fjmp{1}\;,
\eqnsep
\TSTNZ{n}(\srcbr{k}) \deq {}
\\ \quad
\Conc{i = 0}{n-1} 
 (\ptst{\srcbr{k{+}i}.\getbr} \conc \fjmp{2}) \conc \fjmp{2}\;, 
\end{ldispl}where 
 ranges over , 
 ranges over , and
 range over .
For each of the first two parameterized instruction sequences, the first 
parameter correspond to the operand of the operation concerned and the 
second parameter corresponds to the result of the operation concerned.
The intended operations are computed provided that the instantiation of 
the first parameter and the instantiation of the second parameters do 
not lead to partially coinciding -bit words.
In this section, this condition will always be satisfied.
No result is stored on execution of .
Instead, the first primitive instruction following  is 
skipped if the nonzero test fails.

\begin{proposition}
\label{prop-add-basic-operations-correct}
Let  be such that  and .
Then the function on bit strings of length  computed by 
\begin{enumerate}
\item
 models Euclidean division by  
modulo  on natural numbers less than  with respect to their 
binary representation by -bit words;
\item
 models subtraction by  modulo  on 
natural numbers less than  with respect to their binary 
representation by -bit words;
\item
 
models the function  from natural num\-bers less than  
to natural numbers less than  defined by  and 
 with respect to their binary representation by 
-bit words and -bit words, respectively.
\end{enumerate}
\end{proposition}
\begin{proof}
Each of these properties is easy to prove by induction on  with case 
distinction on the content of the input register containing the most 
significant bit of the operand of the operation concerned in both the 
basis step and the inductive step.
\qed
\end{proof}

The lengths of the parameterized instruction sequences defined above are 
as follows:
\begin{ldispl}
\len(\SHR{n}{m}(\srcbr{k},\dstbr{l})) = 3 \mul n - 2 \mul m\;, \\
\len(\DEC{n}(\srcbr{k},\dstbr{l})) = 5 \mul n + 3\;, \\
\len(\TSTNZ{n}(\srcbr{k})) = 2 \mul n + 1\;.
\end{ldispl}
For each bit of the representation of the multiplier,  
contains a different instruction sequence.
This seems to exclude the use of backward jump instructions to obtain 
an instruction sequence of significantly shorter length, unless 
provision is made for some form of indirect addressing for Boolean 
registers.
However, there exists a minor variant of the long multiplication 
algorithm that makes it possible to have the same instruction sequence 
for each bit of the representation of the multiplier.
From the least significant bit of the representation of the multiplier
onwards, the algorithm concerned shifts the representation of the 
multiplier by one position to the right after it has dealt with a bit.
In this way, the next bit remains the least significant one throughout.

We proceed with describing an instruction sequence without backward jump 
instructions that expresses this minor variant of the long multiplication 
algorithm.

 is the instruction sequence described by 
\begin{ldispl}
\MOV{N}(I_1,S_1) \conc \ZPAD{2N}{N}(S_1) \conc
\SET{2N}(0^{2N},S_2) \conc \MOV{N}(I_2,T_1) \conc {}
\\ 
 \bigl(
  \ntst{T_1.\getbr} \conc \fjmp{l} \conc \ADD{2N}(S_1,S_2,S_2) \conc
  \SHL{2N}{1}(S_1,S_1) \conc \SHR{N}{1}(T_1,T_1)
 \bigr)^N \conc {}
\\ 
\MOV{2N}(S_2,O) \conc \halt\;, 
\end{ldispl}\begin{ldispl}
\mbox{where} \hsp{28.05} \1.1ex]
\mbox{where} 
\1.1ex]
l_1 = \len(\ADD{2N}(S_1,S_2,S_2)) + 1 = 42 \mul N + 2\;,
\\
l_2 = \len(\ntst{T_1.\getbr} \conc \ldots \conc 
           \TSTNZ{\floor{\log_2(N)}+1}(T_2)) = 
       51 \mul N + 7 \mul \floor{\log_2(N)} + 10\;.
\end{ldispl}
\begin{proposition}
\label{prop-LMULiii-correct}
The function on bit strings of length  computed by  
models multiplication on natural numbers less than  with respect to 
their binary representation by -bit words.
\end{proposition}
\begin{proof}
We prove a stronger property that also covers the final contents of the 
 successive auxiliary registers starting with the one named , 
the  successive auxiliary registers starting with the one named 
, the  successive auxiliary registers starting with the one 
named , and the  successive auxiliary 
registers starting with the one named .
This stronger property is straightforward to prove, using 
Propositions~\ref{prop-basic-operations-correct}, 
\ref{prop-transfer-operations-correct}, 
and~\ref{prop-add-basic-operations-correct},
by induction on  with case distinction on the content of the input 
register containing the most significant bit of the second operand of 
the operation concerned in both the basis step and the inductive step.
\qed
\end{proof}

\begin{proposition}
\label{prop-LMULiii-length}
.
\end{proposition}
\begin{proof}
This is a matter of simple additions, subtractions, and multiplications.
\linebreak[2]
\qed
\end{proof}

The following is a corollary of Propositions~\ref{prop-LMULi-length}, 
\ref{prop-LMULii-length}, and~\ref{prop-LMULiii-length}.
\begin{corollary}
 while both
, and .
\end{corollary}
Hence,  is asymptotically shorter than both  and
.
By Corollary~\ref{corollary-LMULi-KMUL-length-1}, we know that 
 is asymptotically shorter than  too.

The following is a corollary of Propositions~\ref{prop-LMULi-length}, 
\ref{prop-KMUL-length}, \ref{prop-LMULii-length}, 
and~\ref{prop-LMULiii-length}.
\begin{corollary}
 and
 if , and what is more, 
 if .
\end{corollary}
Hence,  is already shorter than , , 
and  if  is still very small.
In fact, long multiplication is non-trivial only if  and 
Karatsuba multiplication is applicable only if .

\section{Long Multiplication and the Halting Problem}
\label{sect-HP}

In this section, we argue that the instruction sequences  
and  from Section~\ref{sect-BJMP} form a hard witness of 
the inevitable existence of a halting problem in the practice of 
imperative programming.

Turing's result regarding the undecidability of the halting problem 
(see e.g.~\cite{Tur37a}) is a result about Turing machines.
In~\cite{BM09k}, we consider it as a result about programs rather than 
machines, taking instruction sequences as programs.
The instruction sequences concerned are essentially the finite 
instruction sequences that can be denoted by closed \PGAbj\ terms.
Unlike in the current paper, the basic instructions are not fixed, but 
their effects are restricted to the manipulation of something that can 
be understood as the content of the tape of a Turing machine with a 
specific tape alphabet, together with the position of the tape head.
Different choices of basic instructions give rise to different halting 
problem instances and one of these instances is essentially the same as 
the halting problem for Turing machines.
Because of their orientation to Turing machines, we consider all 
instances treated in~\cite{BM09k} theoretical halting problem instances.

All halting problem instances would evaporate if the instruction 
sequences concerned would be restricted to the ones without backward 
jump instructions.
This is irrespective of whether the effects of the basic instructions 
have anything to do with the manipulation of a Turing machine tape.
In the case that we have basic instructions to set and get the content 
of Boolean registers, instruction sequences without backward jump 
instructions are sufficient to compute all functions 
 ().
This raises the question whether there exists a good reason for not 
abandoning backward jump instructions altogether in such cases.
The function that models multiplication on natural numbers less than 
 with respect to their binary representation by -bit words 
offers a good reason: the length of the instruction sequence that 
computes it according to the long multiplication algorithm can be 
reduced significantly by the use of backward jump instructions.
The length of the instruction sequence that computes this function can 
be reduced even more by the use of backward jump instructions than by 
going over to one of the multiplication algorithms that are known to 
yield shorter instruction sequences without backward jump instructions 
than the long multiplication algorithm such as for example the Karatsuba 
multiplication algorithm.

Thus, the instruction sequences  and  form a 
hard witness of the inevitable existence of a halting problem in the 
practice of imperative programming, where programs must have manageable 
size.
Because of its orientation to actual programming, we consider the 
halting problem for the instruction sequences with forward and backward 
jump instructions, and with only basic instructions to set and get the 
content of Boolean registers, a practical halting problem.
It is unknown to us whether there is a connection between the 
solvability or unsolvability of the halting problem for these 
instruction sequences and some form of diagonal argument.
It is easy to prove that this halting problem is both NP-hard and 
coNP-hard.
We do not know whether stronger lower bounds for its complexity can be 
found in the literature.
An extensive search for such lower bounds and other results concerning 
this halting problem or a similar halting problem has been unsuccessful.

\section{Concluding Remarks}
\label{sect-concl}

We have described finite instruction sequences, containing only 
instructions to set and get the content of Boolean registers, forward 
jump instructions, and a termination instruction, that compute the 
function that models multiplication on natural numbers less than 
 with respect to their binary representation by -bit words
according to the long multiplication algorithm and the Karatsuba 
multiplication algorithm.
We have described those instruction sequences by means of terms of \PGA,
an algebraic theory of single-pass instruction sequences.

Thus, we have provided mathematically precise alternatives to the
natural language and pseudo code descriptions of these multiplication 
algorithms found in mathematics and computer science literature on 
multiplication algorithms.
Moreover, we have calculated the exact size of the instruction sequence 
 expressing the long multiplication algorithm and lower and 
upper estimates for the size of the instruction sequence  
expressing the Karatsuba multiplication algorithm.
We have among other things found that: 
(a)~ and
    ; 
(b)~ if , and
     if .
It is suggested by~(a) that instruction sequence size and computation 
time are polynomially related measures.
It is still an open question whether this is the case.

As a bonus, we have found that the number of auxiliary registers used by 
 is  and the number of auxiliary registers 
used by  is 
.
It is also an open question whether the number of auxiliary registers 
that are used by an instruction sequence and computation space are 
related measures.

We have also gone into the use of an instruction sequence with backward 
jump instructions for expressing the long multiplication algorithm.
We have described a finite instruction sequence  containing 
a backward jump instruction, in addition to the instructions to set and 
get the content of Boolean registers, forward jump instructions, and a 
termination instruction, that expresses a minor variant of the long 
multiplication algorithm.
We have calculated the exact size of this instruction sequence and have 
among other things found that:
(a)~;
(b)~ if , and 
     if .
Furthermore, we have related these findings to the halting problem.

\subsection*{Acknowledgements}
We thank Dimitri Hendriks from the VU University Amsterdam for carefully
reading a draft of this paper and for pointing out an error in it.

\bibliographystyle{splncs03}
\bibliography{IS}

\end{document}
