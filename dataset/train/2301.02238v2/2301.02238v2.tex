
\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{cvpr} \makeatletter
\@namedef{ver@everyshi.sty}{}
\makeatother

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{mathtools}
\usepackage{lipsum}
\usepackage{multirow}
\usepackage{textcomp}
\usepackage[numbers,sort]{natbib}
\usepackage{xcolor}
\usepackage{microtype}
\usepackage{overpic}
\usepackage{comment}
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}
\usepackage{pgfplotstable}
\setlength{\bibsep}{1pt plus 1pt minus 1pt}
\graphicspath{{figures/}}


\usepackage[export]{adjustbox}
\usepackage{tikz}
\usetikzlibrary{calc}

\usepackage[ruled]{algorithm2e} \DontPrintSemicolon
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}

\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}{0.75ex \@plus.5ex \@minus.2ex}{-1em}{\normalfont\normalsize\bfseries\maybe@addperiod}}
\newcommand{\maybe@addperiod}[1]{#1\@addpunct{.}}
\makeatother

\usepackage[breaklinks,colorlinks,bookmarks=false,linkcolor=blue,citecolor=blue]{hyperref}
\usepackage[capitalise,noabbrev,nameinlink]{cleveref}
\creflabelformat{equation}{#2#1#3}
\crefmultiformat{equation}{Equations~#2#1#3}{ and~#2#1#3}{, #2#1#3}{ and~#2#1#3}
\frenchspacing
\urlstyle{tt}

\AtBeginDocument{\setlength\abovedisplayskip{5pt plus 1pt minus 2.5pt} \setlength\belowdisplayskip{5pt plus 1pt minus 2.5pt} \setlength\abovedisplayshortskip{0pt plus 3pt} \setlength\belowdisplayshortskip{3pt plus 1.5pt minus 1.5pt} }

\newcommand{\tb}[1]{\textbf{#1}}

\newcommand{\ben}[1]{\textcolor{red}{#1}}
\newcommand{\changil}[1]{\textcolor{red}{\textbf{Changil:} #1}}
\newcommand{\jiabin}[1]{\textcolor{green}{\textbf{Jia-Bin:} #1}}
\newcommand{\CR}[1]{\textcolor{cyan}{[\textbf{Christian:} #1]}}
\newcommand{\matt}[1]{\textcolor{purple}{\textbf{Matt:} #1}}
\newcommand{\TODO}[1]{\textcolor{red}{[\textbf{TODO:} #1]}}


\def\cvprPaperID{0000} \def\confName{CVPR}
\def\confYear{2023}


\begin{document}


\newcommand{\mo}[1]{\textcolor{red}{[\texttt{Matt}: #1]}}
\newcommand{\MZ}[1]{\textcolor{red}{[\texttt{MZ}: #1]}}

\newcommand{\isdraft}{false}


\newcommand{\flux}{\Phi}
\newcommand{\normal}{\mathbf{n}}
\newcommand{\position}{\mathbf{x}}
\newcommand{\xpos}{\mathbf{x}}
\newcommand{\ypos}{\mathbf{y}}
\newcommand{\direction}{\boldsymbol{\omega}}
\newcommand{\dirin}{\direction_\text{i}}
\newcommand{\dirout}{\vec{\direction}}
\newcommand{\hemisphere}{\altmathcal{H}^2}
\newcommand{\withnorm}{[\normal]}
\newcommand{\withgeom}{[\normal_g]}
\newcommand{\withshading}{[\normal_s]}
\newcommand{\withboth}{[\normal_g, \normal_s]}
\newcommand{\inout}{\dirin \rightarrow \dirout}
\newcommand{\outin}{\dirout \rightarrow \dirin}
\newcommand{\emitted}{L_\text{e}}
\newcommand{\importance}{W_\text{e}}
\newcommand{\freeflight}{p_\text{ff}}
\newcommand{\im}{\mathrm{i}}
\newcommand{\tnear}{{t_\text{n}}}
\newcommand{\tfar}{{t_\text{f}}}
\newcommand{\network}{F_{\boldsymbol\theta}}
\newcommand{\rgbL}{L_{\text{RGB}}}
\newcommand{\irL}{L_{\text{IR}}}
\newcommand{\tofL}{L_{\text{ToF}}}
\newcommand{\rgbhatL}{\hat{L}_{\text{RGB}}}
\newcommand{\irhatL}{\hat{L}_{\text{IR}}}
\newcommand{\tofhatL}{\hat{L}_{\text{ToF}}}
\newcommand{\Iref}{I_{\text{s}}}
\newcommand{\timet}{\tau}
\newcommand{\latent}{\mathbf{z}}
\newcommand{\static}{\text{stat}}
\newcommand{\dynamic}{\text{dyn}}

\newcommand{\firstplane}{\pi^{xy}}
\newcommand{\secondplane}{\pi^{uv}}
\newcommand{\posenc}{\boldsymbol\gamma}
\newcommand{\ray}{\mathbf{r}}
\newcommand{\rayparam}{x, y, u, v}
\newcommand{\voxset}{\altmathcal{V}}
\newcommand{\vox}{\upsilon}
\newcommand{\col}{\mathbf{c}}

\newcommand{\mpage}[2]
{
\begin{minipage}{#1\linewidth}\centering
#2
\end{minipage}
} 
\title{
HyperReel: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling
}








\author{\parbox{.2\linewidth}{\centering Benjamin Attal
}
\hspace{-3mm}\and\hspace{-3mm}
\parbox{.2\linewidth}{\centering Jia-Bin Huang}
\hspace{-3mm}\and\hspace{-3mm}
\parbox{.2\linewidth}{\centering Christian Richardt}
\hspace{-3mm}\and\hspace{-3mm}
\parbox{.2\linewidth}{\centering Michael Zollh\"ofer}
\hspace{-3mm}\and\hspace{-3mm}
\parbox{.2\linewidth}{\centering Johannes Kopf}
\hspace{-3mm}\and\hspace{-3mm}
\parbox{.2\linewidth}{\centering Matthew O'Toole}
\hspace{-3mm}\and\hspace{-3mm}
\parbox{.2\linewidth}{\centering Changil Kim}
}

\twocolumn[{
\renewcommand\twocolumn[1][]{#1}
\maketitle
\vspace{-10mm}
\begin{center}
Carnegie Mellon University\quad
University of Maryland\quad
Reality Labs Research\quad
Meta\quad
\end{center}
\begin{center}
\textbf{\url{https://hyperreel.github.io}}
\end{center}
\vspace{7mm}
\vspace{-5mm}
\centering
\footnotesize
\captionsetup{type=figure}
  \mpage{0.02}{\rotatebox{90}{Dynamic 6-DoF rendering}}\hspace{-1.5mm}\hfill \mpage{0.975}{\adjincludegraphics[width=0.14\linewidth,trim={{0.25\width} {0.25\height} {0.35\width} {0.25\height}},clip]{figures/fig_teaser/flames/flames_0000.jpg}\hfill \adjincludegraphics[width=0.14\linewidth,trim={{0.25\width} {0.25\height} {0.35\width} {0.25\height}},clip]{figures/fig_teaser/flames/flames_0008.jpg}\hfill \adjincludegraphics[width=0.14\linewidth,trim={{0.25\width} {0.25\height} {0.35\width} {0.25\height}},clip]{figures/fig_teaser/flames/flames_0016.jpg}\hfill \adjincludegraphics[width=0.14\linewidth,trim={{0.25\width} {0.25\height} {0.35\width} {0.25\height}},clip]{figures/fig_teaser/flames/flames_0024.jpg}\hfill \adjincludegraphics[width=0.14\linewidth,trim={{0.25\width} {0.25\height} {0.35\width} {0.25\height}},clip]{figures/fig_teaser/flames/flames_0032.jpg}\hfill \adjincludegraphics[width=0.14\linewidth,trim={{0.25\width} {0.25\height} {0.35\width} {0.25\height}},clip]{figures/fig_teaser/flames/flames_0040.jpg}\hfill \adjincludegraphics[width=0.14\linewidth,trim={{0.25\width} {0.25\height} {0.35\width} {0.25\height}},clip]{figures/fig_teaser/flames/flames_0048.jpg}\-1mm]\parbox{0.14\linewidth}{\centering \,s}\hfill \parbox{0.14\linewidth}{\centering \,s}\hfill \parbox{0.14\linewidth}{\centering \,s}\hfill \parbox{0.14\linewidth}{\centering \,s}\hfill \parbox{0.14\linewidth}{\centering \,s}\hfill \parbox{0.14\linewidth}{\centering \,s}\hfill \parbox{0.14\linewidth}{\centering \,s}\0.5mm]\rule{\linewidth}{1pt}\0.5mm]\adjincludegraphics[width=0.14\linewidth,trim={{0.2\width} {0.15\height} {0.2\width} {0.25\height}},clip]{figures/fig_teaser/tarot/tarot_0000.jpg}\hfill \adjincludegraphics[width=0.14\linewidth,trim={{0.2\width} {0.15\height} {0.2\width} {0.25\height}},clip]{figures/fig_teaser/tarot/tarot_0008.jpg}\hfill \adjincludegraphics[width=0.14\linewidth,trim={{0.2\width} {0.15\height} {0.2\width} {0.25\height}},clip]{figures/fig_teaser/tarot/tarot_0016.jpg}\hfill \adjincludegraphics[width=0.14\linewidth,trim={{0.2\width} {0.15\height} {0.2\width} {0.25\height}},clip]{figures/fig_teaser/tarot/tarot_0024.jpg}\hfill \adjincludegraphics[width=0.14\linewidth,trim={{0.2\width} {0.15\height} {0.2\width} {0.25\height}},clip]{figures/fig_teaser/tarot/tarot_0032.jpg}\hfill \adjincludegraphics[width=0.14\linewidth,trim={{0.2\width} {0.15\height} {0.2\width} {0.25\height}},clip]{figures/fig_teaser/tarot/tarot_0040.jpg}\hfill \adjincludegraphics[width=0.14\linewidth,trim={{0.2\width} {0.15\height} {0.2\width} {0.25\height}},clip]{figures/fig_teaser/tarot/tarot_0048.jpg}\-1mm]\captionof{figure}{\label{fig:teaser}\tb{HyperReel: A novel 6-DoF video representation.}
HyperReel converts synchronized multi-view video streams into a high-fidelity, memory efficient scene representation that can be rendered from novel views and time steps at interactive rates.
HyperReel's combination of high rendering quality, speed, and compactness sets it apart from existing 6-DoF video representations.
The upper two rows show 6-DoF (i.e., varying viewpoint and viewing orientation) rendering of dynamic scenes \cite{BroxtFOEHDDBWD2020,sabater2017dataset}; the lower two of static scenes \cite{wizadwongsa2021nex,wilburn2005high}.
}\vspace{1em} }]


\maketitle


\begin{abstract}Volumetric scene representations enable photorealistic view synthesis for static scenes and form the basis of several existing 6-DoF video techniques.
However, the volume rendering procedures that drive these representations necessitate careful trade-offs in terms of quality, rendering speed, and memory efficiency. 
In particular, existing methods fail to simultaneously achieve real-time performance, small memory footprint, and high-quality rendering for challenging real-world scenes. 
To address these issues, we present HyperReel --- a novel 6-DoF video representation.
The two core components of HyperReel are: 
(1) a ray-conditioned sample prediction network that enables high-fidelity, high frame rate rendering at high resolutions and 
(2) a compact and memory-efficient dynamic volume representation. 
Our 6-DoF video pipeline achieves the best performance compared to prior and contemporary approaches in terms of visual quality with small memory requirements, while also rendering at up to 18 frames-per-second at megapixel resolution without any custom CUDA code. 

\end{abstract} 

\section{Introduction}
\label{sec:introduction}


Six--Degrees-of-Freedom (6-DoF) videos allow for free exploration of an environment by giving the users the ability to change their head position (3 degrees of freedom) and orientation (3 degrees of freedom). 
As such, 6-DoF videos offer immersive experiences with many exciting applications in AR/VR.
The underlying methodology that drives 6-DoF video is \emph{view synthesis}: the process of rendering new, unobserved views of an environment---static or dynamic---from a set of posed images or videos.
Volumetric scene representations such as neural radiance fields \cite{MildeSTBRN2020} and instant neural graphics primitives \cite{MuelleESK2022} have recently made great strides toward photorealistic view synthesis for static scenes.

While several recent works build dynamic view synthesis pipelines on top of these volumetric representations \cite{XianHKK2021, LiNSW2021, GaoSKH2021, ParkSBBGSB2021, LiSZGLKSLGL2022}, it remains a challenging task to create a 6-DoF video format that can achieve high quality, fast rendering, and a small memory footprint (even given many synchronized video streams from multi-view camera rigs \cite{sabater2017dataset, ParraTFHMSSC2019, BroxtFOEHDDBWD2020}).
Existing approaches that attempt to create \textit{memory-efficient} 6-DoF video can take nearly a minute to render a single megapixel image \cite{LiSZGLKSLGL2022}.
Works that target \textit{rendering speed} and represent dynamic volumes directly with 3D textures require gigabytes of storage even for short video clips \cite{WangZLZZZWXY2022}.
While other volumetric methods achieve memory efficiency \textit{and} speed by leveraging sparse or compressed volume storage for static scenes \cite{MuelleESK2022, ChenXGYS2022}, only contemporary work \cite{LiSWST2022, SongCLCCYXG2023} addresses the extension of these approaches to dynamic scenes.
Moreover, all of the above representations struggle to capture highly view-dependent appearance, such as reflections and refractions caused by non-planar surfaces.





In this paper, we present \emph{HyperReel}, a novel 6-DoF video representation that achieves state-of-the-art quality while being memory efficient and real-time renderable at high resolution.
The first ingredient of our approach is a novel \emph{ray-conditioned sample prediction network} that predicts sparse point samples for volume rendering.
In contrast to existing static view synthesis methods that use sample networks \cite{NeffSPKCKS2021, KurzNLZS2022},
our design is unique in that it both 
(1) \emph{accelerates} volume rendering and at the same time 
(2) \emph{improves} rendering quality for challenging view-dependent scenes.

Second, we introduce a memory-efficient dynamic volume representation that achieves a high compression rate by exploiting the spatio-temporal redundancy of a dynamic scene.
Specifically, we extend Tensorial Radiance Fields~\cite{ChenXGYS2022} to compactly represent a set of volumetric keyframes, and capture intermediate frames with trainable scene flow. 

The combination of these two techniques comprises our high-fidelity 6-DoF video representation, \emph{HyperReel}.
We validate the individual components of our approach and our representation as a whole with comparisons to state-of-the-art sampling network-based approaches for static scenes as well as 6-DoF video representations for dynamic scenes.
Not only does HyperReel outperform these existing works, but it also provides high-quality renderings for scenes with challenging non-Lambertian appearances.
Our system renders at up to 18 frames-per-second at megapixel resolution \emph{without} using any custom CUDA code.

The contributions of our work include the following:
\begin{enumerate}\itemsep0em
    \item
    A novel sample prediction network for volumetric view synthesis that accelerates volume rendering and accurately represents complex view-dependent effects.

    \item
    A memory-efficient dynamic volume representation that compactly represents a dynamic scene.
    
    \item
    HyperReel, a 6-DoF video representation that achieves a desirable trade-off between speed, quality, and memory, while rendering in real time at high resolutions.
\end{enumerate}
 \section{Related Work}
\label{sec:relatedwork}

\paragraph{Novel View Synthesis} 
Novel-view synthesis is the process of rendering new views of a scene given a set of input posed images.
Classical image-based rendering techniques use approximate scene geometry to reproject and blend source image content onto novel views \cite{BuehlBMGC2001,ShumCK2007,PenneZ2017}.
Recent works leverage the power of deep learning and neural fields \cite{XieTSLYKTTSS2022} to improve image-based rendering from both structured (e.g., light fields \cite{GortlGSC1996, LevoyH1996}) and unstructured data \cite{BemanMSR2020, SuhaiESM2022}.
Rather than performing image-based rendering, which requires storing the input images, another approach is to optimize some 3D scene representation augmented with appearance information \cite{RichaTW2020}.
Examples of such representations include point clouds \cite{AlievSKUL2020, RakhiALB2022}, voxel grids \cite{LombaSSSLS2019, NguyeLTRY2019, SitzmTHNWZ2019}, meshes \cite{RieglK2020, RieglK2021}, or layered representations like multi-plane \cite{ZhouTFFS2018, MildeSOKRNK2019, FlynnBDDFOST2019} or multi-sphere images \cite{AttalLGRT2020, BroxtFOEHDDBWD2020}.


\paragraph{Neural Radiance Fields}
NeRFs are one such 3D scene representation for view synthesis \cite{MildeSTBRN2020} that parameterize the appearance and density of every point in 3D space with a multilayer perceptron (MLP).
While NeRFs enable high-quality view synthesis at a small memory cost, they do not lend themselves to real-time rendering. 
To render the color of a ray from a NeRF, one must evaluate and integrate the color and opacity of many points along a ray---necessitating, in the case of NeRF, hundreds of MLP evaluations per pixel.
Still, due to its impressive performance for static view synthesis, recent methods build on NeRFs in the quest for higher visual quality, more efficient training, and faster rendering speed \cite{TewarTMSTWLSMLSTNBWZG2022, GaoGHLXL2022}.
Several works improve the quality of NeRFs by
accounting for finite pixels and apertures \cite{BarroMTHMS2021, WuLPLCZ2022},
by enabling application to unbounded scenes \cite{ZhangRSK2020, BarroMVSH2022, YuFTCRK2022}, large scenes~\cite{TanciCYPMSBK2022,meuleman2023localrf} or by modifying the representation to 
allow for better reproduction of challenging view-dependent appearances like  reflections and refractions \cite{GuoKBHZ2022, VerbiHMZBS2022, KopanLRJD2022, BemanMFSR2022}.
One can achieve significant training and inference speed improvements by replacing the deep multilayer perceptron with a feature voxel grid in combination with a small neural network \cite{SunSC2022, MuelleESK2022, ChenXGYS2022} or no network at all \cite{YuFTCRK2022, KarneRWM2022}.
Several other works achieve both fast rendering and memory-efficient storage with tensor factorizations \cite{ChenXGYS2022}, learned appearance codebooks, or quantized volumetric features \cite{TakikETMMJF2022}.

\paragraph{Adaptive Sampling for Neural Volume Rendering}
Other works aim to improve the speed of volumetric representations by reducing the number of volume queries required to render a single ray.
Approaches like DoNeRF \cite{NeffSPKCKS2021}, TermiNeRF \cite{PialaC2021}, and AdaNeRF \cite{KurzNLZS2022} learn weights for each segment along a ray 
as a function of the ray itself, and use these weights for adaptive evaluation of the underlying NeRF. 
In doing so, they can achieve near-real-time rendering. 
NeuSample \cite{FangXWZLT2021} replaces the NeRF coarse network with a module that directly predicts the distance to each sample point along a ray.
Methods like AutoInt \cite{LindeMW2021}, DIVeR \cite{WuLBWF2022}, and neural light fields \cite{AttalHZKK2022, SitzmRFTD2021, LiSLYX2022} learn integrated opacity and color along a small set of ray segments (or just one segment),
requiring only a single network evaluation per segment.
A key component of our framework is a flexible sampling network, which is among one of the few schemes that both \textit{accelerates} volume rendering, and also \textit{improves} volume rendering quality for challenging scenes.


\begin{figure*}
\centering
\includegraphics[width=1.0\linewidth]{figures/01_overview/pipeline_2.pdf}
\mpage{0.25}{(a) Ray parameterization} \hfill
\mpage{0.24}{(b) Sample prediction network} \hfill
\mpage{0.23}{(c) Sample generation} \hfill
\mpage{0.235}{(d) Volume rendering}
\vspace{-5mm}
\caption{\label{fig:overview}\tb{Overview of HyperReel for static scenes.}
Given a set of images and camera poses, the training objective is to reconstruct the measured color associated with every ray.
(a) For a ray originating at the camera origin  and traveling in direction , we first reparameterize the ray using Plücker coordinates.
(b) A network  takes this ray as input and outputs the parameters for a set of geometric primitives  (such as axis-aligned planes and spheres) and displacement vectors .
(c) To generate sample points  for volume rendering, we compute the intersections between the ray and the geometric primitives, and add the displacement vectors to the results. 
(d) Finally, we perform volume rendering via \cref{eqn:quadrature} to produce a pixel color and supervise training based on the corresponding observation.
}
\vspace{-5mm}
\end{figure*}



 

\paragraph{6--Degrees-of-Freedom Video}
6-DoF video is an emergent technology that allows users to explore new views within videos \cite{RichaTW2020}. 
Systems for 6-DoF video \cite{ParraTFHMSSC2019} use multi-view camera rigs that capture a full 360-degree field of view and use variants of depth-based reprojection \cite{SerraKCDGHM2019} for view synthesis at each frame of the video. 
Other methods optimize time-varying multi-sphere images (MSIs) \cite{BroxtFOEHDDBWD2020, AttalLGRT2020}, which can provide better visual quality but at a higher training cost.

\paragraph{6-DoF from Monocular Captures}
Due to the success of neural radiance fields for static view synthesis, many recent approaches attempt to extend volumetric scene representations to dynamic scenes.
Several such works reconstruct 6-DoF video from single-view (i.e. monocular) RGB sequences \cite{LiNSW2021,GaoSKH2021,ParkSBBGSB2021,liu2023robust}. 
This is a highly under-constrained setting, which requires decoupling camera and object motion. 
The natural signal priors provided by neural radiance fields help during reconstruction. However, most methods typically rely on additional priors, such as off-the-shelf networks for predicting scene flow and geometry or depth from ToF cameras \cite{XianHKK2021, AttalLGKRTO2021}. 
Still, other approaches model the scene at different time steps as smoothly ``warped'' copies of some canonical frame \cite{ParkSBBGSB2021, PumarCPM2021}, which works best for small temporal windows and smooth object motion. 

\paragraph{6-DoF from Multi-View Captures}
Other methods, like ours, aim to produce 6-DoF video from multi-view camera rigs \cite{LombaSSSLS2019, BroxtFOEHDDBWD2020, LiSZGLKSLGL2022}. 
Despite the additional constraints provided by multiple cameras, this remains a challenging task; an ideal 6-DoF video format must simultaneously achieve high visual quality, rendering speed, and memory efficiency.
Directly extending recent volumetric methods to dynamic scenes can achieve high quality and rendering speed \cite{WangZLZZZWXY2022}, but at the cost of substantial memory requirements, potentially gigabytes of memory \cite{YuFTCRK2022} for each video frame.
Contemporary works such as StreamRF \cite{LiSWST2022} and NeRFPlayer \cite{SongCLCCYXG2023} design volumetric 6-DoF video representations that mitigate storage requirements but sacrifice either rendering speed or visual quality.
On the other hand, our approach achieves both fast and high-quality 6-DoF video rendering while maintaining a small memory footprint.


 
\section{Method}
\label{sec:technical}


We start by considering the problem of optimizing a volumetric representation for static view synthesis.
Volume representations like NeRF \cite{MildeSTBRN2020} model the density and appearance of a static scene at every point in the 3D space.
More specifically, a function 
maps position  and direction  along a ray
to a color  and density .
Here, the trainable parameters  may be neural network weights, -dimensional array entries, or a combination of both.


We can then render new views of a static scene with

where  denotes the transmittance from  to .

In practice, we can evaluate \cref{eqn:volume_rendering} using numerical quadrature by taking many sample points along a given ray:

where the weights  specify the contribution of each sample point's color to the output.


\subsection{Sample Networks for Volume Rendering}
\label{sec:sampling}


Most scenes consist of solid objects whose surfaces lie on a 2D manifold within the 3D scene volume.
In this case, only a small set of sample points contributes to the rendered color for each ray.
To accelerate volume rendering, we would like to query color and opacity only for points with non-zero .
While most volume representations use importance sampling and pruning schemes that help reduce sample counts, they often require hundreds or even thousands of queries per ray to produce accurate renderings \cite{ChenXGYS2022, MuelleESK2022}.

As shown in \cref{fig:overview}, we use a feed-forward network to predict a set of sample locations .
Specifically, we use a \emph{sample prediction network}  that maps a ray  to the sample points  for volume rendering in \cref{eqn:quadrature}. 
We use either the two-plane parameterization~\cite{LevoyH1996} (for forward facing scenes) or the Plücker parameterization (for all other scenes) to represent the ray:

While many designs for the sample prediction network  are possible, giving the network too much flexibility may negatively affect view synthesis quality.
For example, if  are completely arbitrary points, then renderings may not appear to be multi-view-consistent.

To address this problem, we choose to predict the parameters of a set of geometric primitives  defined in the world coordinate frame, where the primitive parameters themselves are a function of the input ray.
To get our sample points, we then intersect the ray with each primitive:

Above,  is a differentiable operation that intersects the ray with the primitive . 
In all of our experiments, we use axis-aligned -planes (for forward-facing scenes) or concentric spherical shells centered at the origin (for all other scenes) as our geometric primitives.

This approach is constrained in that it produces sample points that initially lie along the ray. Further, predicting primitives defined in world space makes the sample signal easier to interpolate. 
For example, if two distinct rays observe the same point in the scene, then the sample network needs only predict one primitive for both rays (i.e., defining a primitive that passes through the point). 
In contrast, existing works such as NeuSample \cite{FangXWZLT2021}, AdaNeRF \cite{KurzNLZS2022}, and TermiNeRF \cite{PialaC2021} predict distances or per-segment weights that do not have this property.


\paragraph{Flexible Sampling for Challenging Appearance.} To grant our samples additional flexibility to better represent challenging view-dependent appearance, we also predict a set of Tanh-activated per-sample-point offsets , as well as a set of scalar values . 
We convert these scalar values to weights with a sigmoid activation, \ie,  where  is the sigmoid operator. 
Specifically, we have:

where we use  to denote the final displacement, or ``point-offset'' added to each point.

\begin{figure}
\centering
\includegraphics[trim={0 8cm 12cm 5cm},width=\linewidth]{figures/01_overview/dynamic.pdf}
\caption{\textbf{Extracting sample point appearance and opacity in the dynamic setting from our keyframe-based representation.} (1) We first advect the sample points  at time  into the nearest keyframe , using velocities  from the sample prediction network. 
(2) We then query the outer products of space-time textures in order to produce per-sample-point appearance and opacity features, which are converted to colors/densities via \cref{eqn:tensorappearance}.}\label{fig:dynamic}
\vspace{-5mm}
\end{figure}

While the sample network outputs may appear to be over-parameterized and under-constrained, this is essential to achieve good-quality view synthesis.
In particular, initializing the scalars  to negative values, where the sigmoid is close to 0, and its gradient is small, implicitly discourages the network from unmasking the point offsets, while still allowing the network to use them as necessary.




In addition to enabling real-time rendering with low sample counts, one added benefit of our sample network architecture is the improved modeling of complex view-dependent appearance.
For example, distorted refractions break epipolar geometry and appear to change the depth of the refracted content depending on the viewpoint.
As illustrated in \cref{fig:overview}, our sample network, on the other hand, has the flexibility to model sample points that warp depending on viewpoint, similar to flow-based models of scene appearance in IBR~\cite{nieto2017linearizing}

Existing works like Eikonal fields \cite{BemanMFSR2022} can be considered a special case of this sample warping approach; they use physically derived Eikonal constraints to learn ray-conditional warp fields for refractive objects.
Although our sample network is not guaranteed to be physically interpretable, it can handle \textit{both} reflections and refractions. Further, it is far more efficient at inference time and does not require evaluating costly multi-step ODE solvers during rendering.
See \cref{fig:teaser} and our supplemental materials for additional results and comparisons on challenging view-dependent scenes.

\subsection{Keyframe-Based Dynamic Volumes}

So far, we have covered how to efficiently \textit{sample} a 3D scene volume, but have not yet discussed how we \textit{represent} the volume itself.
In the static case, we use memory-efficient Tensorial Radiance Fields (TensoRF) approach (\cref{sec:tensorialradiancefields}), and in the dynamic case we extend TensoRF to a keyframe-based dynamic volume representation (\cref{sec:keyframe}).


\subsubsection{Representing 3D Volumes with TensoRF~\cite{ChenXGYS2022}}
\label{sec:tensorialradiancefields}

Recall that TensoRF factorizes a 3D volume as a set of outer products between functions of one or more spatial dimensions.
Specifically, we can write the set of spherical harmonic coefficients  capturing the appearance of a point  as:

Above,  and  are vector-valued functions with output dimension , and `' is an element-wise product.
In the original TensoRF work \cite{ChenXGYS2022}, the functions  and  are discretized into  different 2D and and 1D arrays, respectively.

Further,  denote linear transforms that map the products of  and  to spherical harmonic coefficients.
The color  for point  and direction  is then given by the dot product of the coefficients  and the spherical harmonic basis functions evaluated at ray direction .

Similar to appearance, for density, we have:

where  is a vector of ones, and  and  are vector-valued functions with output dimension .
Given the color  and density  for all sample points  along a ray, we can then make use of \cref{eqn:quadrature} to render the final color for that ray.


\subsubsection{Representing Keyframe-Based Volumes}
\label{sec:keyframe}

To handle dynamics, we adapt TensoRF to parameterize volumetric ``keyframes'', or snapshots of a dynamic volume at a set of discrete time steps. 
If we denote  as the time step corresponding to the  keyframe, we can write:
4pt]
    \sigma(\xpos_k, \tau_i)
    &= \mathbf{1}^\top \left( \mathbf{h}_1(x_k, y_k) \odot \mathbf{k}_1(z_k, \tau_i) \right) \nonumber \\
    &+ \mathbf{1}^\top \left( \mathbf{h}_2(x_k, z_k) \odot \mathbf{k}_2(y_k, \tau_i) \right) \label{eqn:tensoropacity} \\
    &+ \mathbf{1}^\top \left( \mathbf{h}_3(y_k, z_k) \odot \mathbf{k}_3(x_k, \tau_i) \right) \text{,} \nonumber

    \xpos_k \leftarrow \xpos_k + \mathbf{v}_k(\tau_i - \tau) \text{.}
    \label{eq:advect}

    C(\mathbf{o}, \dirout, \tau) = \sum_{k = 1}^{N} w_k \,
    L_\text{e}\left(\xpos_k, \dirout, \tau_i\right) \text{,}
\label{eqn:quadrature2}

    \mathcal{L} &= \mathcal{L}_\text{L2} + w_\text{L1} \mathcal{L}_\text{L1} + w_\text{TV} \mathcal{L}_\text{TV} \quad \text{where} \label{eq:loss} \\
    \mathcal{L}_\text{L2} &= \sum_{\mathbf{o}, \dirout, \tau} \lVert C(\mathbf{o}, \dirout, \tau) - C_\text{GT}(\mathbf{o}, \dirout, \tau) \rVert \text{.}
0.2mm]}
  }\hfill \mpage{0.32}{\mpage{0.49}{\begin{tikzpicture}
        \node[anchor=south west,inner sep=0] (image) at (0,0) {\adjincludegraphics[height=\linewidth,trim={{0.14\width} 0 {0.3\width} 0},clip,angle=90]{figures/fig_dynamic/img/technicolor1/0002_ours.jpg}};
        \begin{scope}[x={()},y={()}]
          \draw[red,thick] (0.32,0.13) rectangle (0.64,0.48);
        \end{scope}
      \end{tikzpicture}}\hspace{-1mm}\hfill \mpage{0.49}{\adjincludegraphics[height=\linewidth,trim={{0.22\width} {0.36\height} {0.6\width} {0.32\height}},clip,angle=90]{figures/fig_dynamic/img/technicolor1/0002_ours.jpg}\0.2mm]}
  }\0.2mm]}
  }\hfill \mpage{0.32}{\mpage{0.49}{\begin{tikzpicture}
        \node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=\linewidth]{figures/fig_dynamic/img/technicolor3/0002_ours.jpg}};
        \begin{scope}[x={()},y={()}]
          \draw[red,thick] (0.43,0.27) rectangle (0.56,0.4);
        \end{scope}
      \end{tikzpicture}}\hspace{-1mm}\hfill \mpage{0.49}{\adjincludegraphics[width=\linewidth,trim={{0.43\width} {0.27\height} {0.44\width} {0.6\height}},clip]{figures/fig_dynamic/img/technicolor3/0002_ours.jpg}\0.2mm]}
  }\1mm]\mpage{0.32}{\mpage{0.49}{\begin{tikzpicture}
        \node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=\linewidth]{figures/fig_dynamic/img/n3dv/0010_gt.jpg}};
        \begin{scope}[x={()},y={()}]
          \draw[red,thick] (0.4,0.26) rectangle (0.72,0.58);
        \end{scope}
      \end{tikzpicture}}\hspace{-1mm}\hfill \mpage{0.49}{\adjincludegraphics[width=\linewidth,trim={{0.4\width} {0.26\height} {0.28\width} {0.42\height}},clip]{figures/fig_dynamic/img/n3dv/0010_gt.jpg}\0.2mm]}
  }\hfill \mpage{0.32}{\mpage{0.49}{\begin{tikzpicture}
        \node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=\linewidth]{figures/fig_dynamic/img/n3dv/0010_nerfplayer.jpg}};
        \begin{scope}[x={()},y={()}]
          \draw[red,thick] (0.4,0.26) rectangle (0.72,0.58);
        \end{scope}
      \end{tikzpicture}}\hspace{-1mm}\hfill \mpage{0.49}{\adjincludegraphics[width=\linewidth,trim={{0.4\width} {0.26\height} {0.28\width} {0.42\height}},clip]{figures/fig_dynamic/img/n3dv/0010_nerfplayer.jpg}\0.1mm]\mpage{0.32}{Ground truth (Neural 3D Video~\cite{LiSZGLKSLGL2022})}\hfill \mpage{0.32}{Ours}\hfill \mpage{0.32}{NeRFPlayer~\cite{SongCLCCYXG2023}
  }\0.2mm]}
  }\hfill \mpage{0.32}{\mpage{0.49}{\begin{tikzpicture}
        \node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=\linewidth]{figures/fig_dynamic/img/immersive/ours.jpg}};
        \begin{scope}[x={()},y={()}]
          \draw[red,thick] (0,0.47) rectangle (0.5,0.97);
        \end{scope}
      \end{tikzpicture}}\hspace{-1mm}\hfill \mpage{0.49}{\adjincludegraphics[width=\linewidth,trim={{0\width} {0.47\height} {0.5\width} {0.03\height}},clip]{figures/fig_dynamic/img/immersive/ours.jpg}\0.2mm]}
  }\-2mm]\caption{\textbf{Qualitative comparisons of dynamic reconstruction.} 
We show visual comparisons of our method on three datasets against two baselines on heldout views. 
We pick non-keyframe time-steps for evaluation, except for the Google Immersive light field video (last row), for which we pick the matching image to the NeRFPlayer \cite{SongCLCCYXG2023} result.
See our project webpage for more results and comparisons.
}\label{fig:qual}
\vspace{-5mm}
\end{figure*}
 
\paragraph{DoNeRF Dataset}
The DoNeRF dataset \cite{NeffSPKCKS2021} contains six synthetic sequences with images of 800800 pixel resolution.
Here, we validate the efficacy of our sample prediction network approach by comparing it to existing methods for static view synthesis, including NeRF, InstantNGP, and three sampling-network--based approaches \cite{NeffSPKCKS2021, KurzNLZS2022, PialaC2021}.

As demonstrated in \cref{tab:quant_static_donerf}, our approach outperforms all baselines in terms of quality and improves the performance of other sampling network schemes by a large margin.
Additionally, our model is implemented in vanilla PyTorch and renders 800800 pixel images at 6.5 FPS on a single RTX 3090 GPU (or 29 FPS with our \textit{Tiny} model).

We also compare our sampling network-based approach to the single-sample R2L light field representation \cite{WangRHOCFT2022} on the downsampled 400400 resolution DoNeRF dataset (with their provided metrics). 
We outperform their approach quantitatively without using pretrained teacher networks.
Further, inference with our six-layer, 256-hidden-unit network, and TensoRF volume backbone is faster than R2L's deep 88-layer, 256-hidden-unit MLP.

\paragraph{LLFF Dataset}
See supplementary material for additional quantitative comparisons on the LLFF dataset \cite{MildeSTBRN2020}, showing our network achieving high quality on real-world scenes.





\subsection{Comparisons on Dynamic Scenes}

\paragraph{Technicolor Dataset}
The Technicolor light field dataset \cite{sabater2017dataset} contains videos of varied indoor environments captured by a time-synchronized 44 camera rig.
Each image in each video stream is 20481088 pixels, and we hold out the view in the second row and second column for evaluation.
We compare HyperReel to Neural 3D Video \cite{LiSZGLKSLGL2022} at full image resolution on five sequences (\textit{Birthday, Fabien, Painter, Theater, Trains}) from this dataset, each 50 frames long.
We train Neural 3D Video on each sequence for approximately one week on a machine with 8 NVIDIA V100 GPUs.

We show in \cref{tab:quant_dynamic} that the quality of HyperReel exceeds that of Neural 3D Video \cite{LiSZGLKSLGL2022} while also training in just 1.5 GPU hours per sequence (rather than 1000+ GPU hours for Neural 3D), and rendering far more quickly.








\paragraph{Neural 3D Video Dataset}
The Neural 3D Video dataset \cite{LiSZGLKSLGL2022} contains six indoor multi-view video sequences captured by 20 cameras at 27042028 pixel resolution.
We downsample all sequences by a factor of 2 for training and evaluation and hold out the central view for evaluation.
Metrics are averaged over all scenes.
Additionally, due to the challenging nature of this dataset (time synchronization errors, inconsistent white balance, imperfect poses), we output 64 -planes per ray with our sample network rather than 32.

We show in \cref{tab:quant_dynamic} that we quantitatively outperform NeRFPlayer \cite{SongCLCCYXG2023} while rendering approximately 40 times faster.
While StreamRF~\cite{LiSWST2022} makes use of a custom CUDA implementation that renders faster than our model, our approach consumes less memory on average per frame than both StreamRF and NeRFPlayer.


\paragraph{Google Immersive Dataset}
The Google Immersive dataset \cite{BroxtFOEHDDBWD2020} contains light field videos of various indoor and outdoor environments captured by a time-synchronized 46-fisheye camera rig.
Here, we compare our approach to NeRFPlayer and select the same seven scenes as NeRFPlayer for evaluation on this dataset (\textit{Welder, Flames, Truck, Exhibit, Face Paint 1, Face Paint 2, Cave}), holding out the central view for validation.
Our results in \cref{tab:quant_dynamic} outperform NeRFPlayer's by a 3\,dB margin and renders more quickly.

\paragraph{DeepView Dataset}
As Google's Immersive Light Field Video \cite{BroxtFOEHDDBWD2020} does not provide quantitative benchmarks for the performance of their approach in terms of image quality, we provide an additional comparison of our approach to DeepView \cite{FlynnBDDFOST2019} in the supplementary material.



\subsection{Ablation Studies}

\paragraph{Number of Keyframes.}
In \cref{tab:quant_ablation}, we ablate our method on the Technicolor light field dataset with different numbers of keyframes.
Increasing the number of keyframes allows our model to capture more complex motions, but also distributes the volume's capacity over a larger number of time steps.
Our choice of one keyframe for every four frames strikes a good balance between temporal resolution and spatial rank, and achieves the best overall performance (\cref{tab:quant_ablation}).

\paragraph{Network Size and Number of Primitives.}
We also show the performance of our method with different network designs in \cref{tab:quant_ablation},
including the performance for a \textit{Tiny} model (4-layers, 128-hidden-unit MLP with 8 predicted primitives), and \textit{Small} model (4-layers, 256-hidden-unit MLP with 16 predicted primitives).
Our \textit{Tiny} model runs at 18\,FPS, and our \textit{Small} model runs at 9\,FPS at megapixel resolution, again without any custom CUDA code.
Our \textit{Tiny} model performs reasonably well but achieves worse quality than Neural 3D Video on the Technicolor dataset. 
In contrast, our \textit{Small} model achieves comparable overall performance to Neural3D---showing that we can still achieve good quality renderings at even higher frame rates.
We show accompanying qualitative results for these models in \cref{fig:abl_samples}.

\paragraph{With and Without Sample Prediction Network.}
We show results on the Technicolor dataset \textit{without} our sample prediction network, using every frame as a keyframe, and with 4 the number of samples (128 vs. 32).
Our full method outperforms this approach by a sizeable margin. 

\paragraph{With and Without Point Offset.}
In \cref{tab:quant_point}, we show results on two static scenes \textit{with} and \textit{without} point offsets (\cref{eqn:point-offsets}): one diffuse and one highly refractive scene.
Point offsets improve quality in both cases, suggesting that they may help with better model capacity allocation in addition to view-dependence---similar to ``canonical frame'' deformations used in Nerfies \cite{ParkSHBBGMS2021} and Neural Volumes \cite{LombaSSSLS2019}.



\begin{figure}[t]
\footnotesize
\centering \mpage{0.19}{\adjincludegraphics[height=\linewidth,trim={{0.22\width} {0.36\height} {0.6\width} {0.32\height}},clip,angle=90]{figures/fig_ablations/samples/fabien_0002_gt.jpg}\0.2mm]}\hspace{-1mm}\hfill \mpage{0.19}{\adjincludegraphics[height=\linewidth,trim={{0.22\width} {0.36\height} {0.6\width} {0.32\height}},clip,angle=90]{figures/fig_ablations/samples/fabien_0002_small.jpg}\0.2mm]}\hspace{-1mm}\hfill \mpage{0.19}{\adjincludegraphics[height=\linewidth,trim={{0.22\width} {0.36\height} {0.6\width} {0.32\height}},clip,angle=90]{figures/fig_ablations/samples/fabien_0002_nosample.jpg}\1mm]\mpage{0.19}{\adjincludegraphics[width=\linewidth,trim={{0.43\width} {0.27\height} {0.44\width} {0.6\height}},clip]{figures/fig_ablations/samples/train_0002_gt.jpg}\0.2mm]}\hspace{-1mm}\hfill \mpage{0.19}{\adjincludegraphics[width=\linewidth,trim={{0.43\width} {0.27\height} {0.44\width} {0.6\height}},clip]{figures/fig_ablations/samples/train_0002_small.jpg}\0.2mm]}\hspace{-1mm}\hfill \mpage{0.19}{\adjincludegraphics[width=\linewidth,trim={{0.43\width} {0.27\height} {0.44\width} {0.6\height}},clip]{figures/fig_ablations/samples/train_0002_nosample.jpg}\1mm]\mpage{0.19}{GT}\hspace{-1mm}\hfill \mpage{0.19}{Full model}\hspace{-1mm}\hfill \mpage{0.19}{Small}\hspace{-1mm}\hfill \mpage{0.19}{Tiny}\hspace{-1mm}\hfill \mpage{0.19}{No sampling}\0.2mm]}\hspace{-1mm}\hfill \mpage{0.56}{\hfill \begin{tikzpicture}
      \node[anchor=south west,inner sep=0] (image) at (0,0) {\adjincludegraphics[height=16mm]{figures/fig_failure/img/train_0002_ours.jpg}};
      \begin{scope}[x={()},y={()}]
        \draw[red,thick] (0.89,0.11) rectangle (1,0.61);
      \end{scope}
    \end{tikzpicture}\hfill \adjincludegraphics[height=16mm,trim={{0.89\width} {0.11\height} 0 {0.39\height}},clip]{figures/fig_failure/img/train_0002_ours.jpg}\hfill \adjincludegraphics[height=16mm,trim={{0.89\width} {0.11\height} 0 {0.39\height}},clip]{figures/fig_failure/img/train_0002_gt.jpg}\0.5mm]\mpage{0.34}{}\hspace{-2mm}\hfill \mpage{0.08}{GT}\hspace{-1mm}\hfill \mpage{0.48}{}\hspace{-2mm}\hfill \mpage{0.08}{GT}\
    \mathbf{r} = \textit{Pl\"{u}cker}(\mathbf{o}, \dirout) = \left( \dirout, \dirout \times \mathbf{o} \right) \text{.}

    L_e(\xpos_k, \dirout, \tau_i) \leftarrow L_e(\xpos_k, \dirout, \tau_i) \cdot 
 \mathbf{c}^{\textit{scale}}_k + \mathbf{c}^{\textit{shift}}_k \text{.}
    \label{eq:scaleshift}
0.2mm]}
  }\hfill \mpage{0.32}{\mpage{0.49}{\begin{tikzpicture}
        \node[anchor=south west,inner sep=0] (image) at (0,0) {\adjincludegraphics[height=\linewidth,trim={{0.14\width} 0 {0.3\width} 0},clip,angle=0]{figures/fig_dynamic/img/technicolor4/0000_ours.png}};
        \begin{scope}[x={()},y={()}]
          \draw[red,thick] (0.22,0.13) rectangle (0.64,0.68);
        \end{scope}
      \end{tikzpicture}}\hspace{-1mm}\hfill \mpage{0.49}{\adjincludegraphics[height=\linewidth,trim={{0.27\width} {0.22\height} {0.54\width} {0.37\height}},clip,angle=0]{figures/fig_dynamic/img/technicolor4/0000_ours.png}\0.2mm]}
  }\0.2mm]}
  }\hfill \mpage{0.32}{\mpage{0.49}{\begin{tikzpicture}
        \node[anchor=south west,inner sep=0] (image) at (0,0) {\includegraphics[width=\linewidth]{figures/fig_dynamic/img/technicolor2/0002_ours.jpg}};
        \begin{scope}[x={()},y={()}]
          \draw[red,thick] (0.39,0.46) rectangle (0.51,0.58);
        \end{scope}
      \end{tikzpicture}}\hspace{-1mm}\hfill \mpage{0.49}{\adjincludegraphics[width=\linewidth,trim={{0.39\width} {0.46\height} {0.49\width} {0.42\height}},clip]{figures/fig_dynamic/img/technicolor2/0002_ours.jpg}\0.2mm]}
  }\1mm]\caption{\textbf{Additional qualitative comparisons to Neural 3D Video Synthesis.} 
We show two additional qualitative comparisons against Neural 3D Video Synthesis~\cite{LiSZGLKSLGL2022} on the Technicolor dataset~\cite{sabater2017dataset}, demonstrating that our approach recovers more accurate/detailed appearance.
}\label{fig:dynamic_supp}
\vspace{1mm}
\end{figure*}
 
\begin{figure*}[htp]
\footnotesize
\centering \mpage{1.0}{\begin{overpic}[width=\linewidth]{figures/rebuttal/panorama.png}\end{overpic}
	}\caption{Example panoramic rendering from our approach applied to a synthetic scene with captures spanning a full 360 degree FoV. In this case, our sample network predicts spherical geometric primitives. The scene is one of the shots from the Blender Foundations Agent 327 open movie~\cite{blenderfoundation}.
}\label{fig:360}
\vspace{5mm}

\footnotesize
\centering \mpage{0.2}{\begin{overpic}[width=\linewidth]{figures/rebuttal/point.png}\end{overpic}\-0.5mm]}\hfill \mpage{0.3}{\includegraphics[trim={10cm 8cm 10cm 12cm},clip,width=\linewidth]{figures/rebuttal/point.png}\-0.5mm]}
	}\1mm]\mpage{0.2}{\begin{overpic}[trim={10cm 0cm 0cm 0cm},clip,width=\linewidth]{figures/rebuttal/lab_point.png}\end{overpic}\-0.5mm]}\hfill \mpage{0.3}{\includegraphics[trim={16cm 2cm 12cm 10cm},clip,width=\linewidth]{figures/rebuttal/lab_point.png}\-0.5mm]}
	}\-2mm]\caption{Comparison of our method with and without point offset on the \textit{Tarot} sequence from the Stanford Light Field dataset~\cite{WilbuJVTABAHL2005} and \textit{Lab} sequence from the Shiny dataset~\cite{WizadPYS2021}.
}\label{fig:refractive}
\vspace{-5mm}
\end{figure*}
 

\clearpage{}\begin{table*}[!htb]
\centering
\caption{Per-scene results from the Technicolor dataset~\cite{sabater2017dataset}. See Section~\cref{sec:ssim} for a discussion of the reliability of SSIM metrics.}
\vspace{-1mm}
\small
\resizebox{\linewidth}{!}{\begin{tabular}{@{}l@{\hspace{3\tabcolsep}}c@{\hspace{0.2\tabcolsep}}c@{\hspace{0.9\tabcolsep}}c@{\hspace{0.8\tabcolsep}}cc@{\hspace{0.2\tabcolsep}}c@{\hspace{0.9\tabcolsep}}c@{\hspace{0.8\tabcolsep}}cc@{\hspace{0.2\tabcolsep}}c@{\hspace{0.9\tabcolsep}}c@{\hspace{0.8\tabcolsep}}c@{}}
\toprule
\multirow{2}{*}[-0.5ex]{Scene}
& \multicolumn{4}{c}{PSNR} & \multicolumn{4}{c}{SSIM} & \multicolumn{4}{c}{LPIPS} \\
\cmidrule(r){2-5} \cmidrule(lr){6-9} \cmidrule(l){10-13}
& Neural 3D Video~\cite{LiSZGLKSLGL2022} & Ours & Small & Tiny \hspace{-1mm}
& Neural 3D Video~\cite{LiSZGLKSLGL2022} & Ours & Small & Tiny \hspace{-1mm}
& Neural 3D Video~\cite{LiSZGLKSLGL2022} & Ours & Small & Tiny \hspace{-1mm} \\
\midrule
Birthday     & 29.20 & \bf 29.99 & 29.32 & 27.80 & \bf 0.952 & 0.922 & 0.907 & 0.876 & 0.0668 & \bf 0.0531 & 0.0622 & 0.0898 \\
Fabien     & 32.76 & \bf 34.70 & 33.67 & 32.25 & \bf 0.965 & 0.895 & 0.882 & 0.860 & 0.2417 & \bf 0.1864 & 0.1942 & 0.2233 \\
Painter     & 35.95 & 35.91 & \bf 36.09 & 34.61 & \bf 0.972 & 0.923 & 0.920 & 0.905 & 0.1464 & \bf 0.1173 & 0.1182 & 0.1311 \\
Theater     & 29.53 & \bf 33.32 & 32.19 & 30.74 & \bf 0.939 & 0.895 & 0.880 & 0.845 & 0.1881 & \bf 0.1154 & 0.1306 & 0.1739 \\
Trains     & \bf 31.58 & 29.74 & 27.51 & 25.02 & \bf 0.962 & 0.895 & 0.835 & 0.773 & \bf 0.0670 & 0.0723 & 0.1196 & 0.1660 \\
\bottomrule
\end{tabular}
}
\label{tab:technicolor}

\vspace{10mm}
\caption{Per-scene results from the Neural 3D Video dataset~\cite{LiSZGLKSLGL2022}, available only for our method and NeRFPlayer~\cite{SongCLCCYXG2023}.}
\vspace{-1mm}
\small
\resizebox{0.75\linewidth}{!}{\begin{tabular}{@{}l@{\hspace{3\tabcolsep}}c@{\hspace{0.2\tabcolsep}}c@{\hspace{0.9\tabcolsep}}c@{\hspace{0.8\tabcolsep}}cc@{\hspace{0.2\tabcolsep}}c@{\hspace{0.9\tabcolsep}}c@{\hspace{0.8\tabcolsep}}cc@{\hspace{0.2\tabcolsep}}c@{\hspace{0.9\tabcolsep}}c@{\hspace{0.8\tabcolsep}}c@{}}
\toprule
\multirow{2}{*}[-0.5ex]{Scene}
& \multicolumn{2}{c}{PSNR} & \multicolumn{2}{c}{SSIM} & \multicolumn{2}{c}{LPIPS} \\
\cmidrule(r){2-3} \cmidrule(lr){4-5} \cmidrule(l){6-7}
& NeRFPlayer~\cite{SongCLCCYXG2023} & Ours \hspace{-1mm}
& NeRFPlayer~\cite{SongCLCCYXG2023} & Ours \hspace{-1mm}
& NeRFPlayer~\cite{SongCLCCYXG2023} & Ours \hspace{-1mm} \\
\midrule
Coffee Martini     & \bf 31.534 & 28.369 & \bf 0.951 & 0.892 & \bf 0.085 & 0.127 \\
Cook Spinach       & 30.577 & \bf 32.295 & 0.929 & \bf 0.941 & 0.113 & \bf 0.089 \\
Cut Roasted Beef   & 29.353 & \bf 32.922 & 0.908 & \bf 0.945 & 0.144 & \bf 0.084 \\
Flame Salmon       & \bf 31.646 & 28.260 & \bf 0.940 & 0.882 & \bf 0.098 & 0.136 \\
Flame Steak        & 31.932 & \bf 32.203 & \bf 0.950 & 0.949 & 0.088 & \bf 0.078 \\
Sear Steak         & 29.129 & \bf 32.572 & 0.908 & \bf 0.952 & 0.138 & \bf 0.077 \\
\bottomrule
\end{tabular}
}
\label{tab:n3d}

\vspace{10mm}
\caption{Per-scene results from the  Google Immersive Light Field Video dataaset~\cite{BroxtFOEHDDBWD2020}, available only for our method and NeRFPlayer~\cite{SongCLCCYXG2023}.}
\vspace{-1mm}
\small
\resizebox{0.75\linewidth}{!}{\begin{tabular}{@{}l@{\hspace{3\tabcolsep}}c@{\hspace{0.2\tabcolsep}}c@{\hspace{0.9\tabcolsep}}c@{\hspace{0.8\tabcolsep}}cc@{\hspace{0.2\tabcolsep}}c@{\hspace{0.9\tabcolsep}}c@{\hspace{0.8\tabcolsep}}cc@{\hspace{0.2\tabcolsep}}c@{\hspace{0.9\tabcolsep}}c@{\hspace{0.8\tabcolsep}}c@{}}
\toprule
\multirow{2}{*}[-0.5ex]{Scene}
& \multicolumn{2}{c}{PSNR} & \multicolumn{2}{c}{SSIM} & \multicolumn{2}{c}{LPIPS} \\
\cmidrule(r){2-3} \cmidrule(lr){4-5} \cmidrule(l){6-7}
& NeRFPlayer~\cite{SongCLCCYXG2023} & Ours \hspace{-1mm}
& NeRFPlayer~\cite{SongCLCCYXG2023} & Ours \hspace{-1mm}
& NeRFPlayer~\cite{SongCLCCYXG2023} & Ours \hspace{-1mm} \\
\midrule
01\_Welder         & \bf 25.568 & 25.554 & \bf 0.818 & 0.790 & 0.289 & \bf 0.281 \\
02\_Flames         & 26.554 & \bf 30.631 & 0.842 & \bf 0.905 & \bf 0.154 & 0.159 \\
04\_Truck          & 27.021 & \bf 27.175 & \bf 0.877 & 0.848 & \bf 0.164 & 0.223 \\
09\_Exhibit        & 24.549 & \bf 31.259 & 0.869 & \bf 0.903 & 0.151 & \bf 0.140 \\
10\_Face\_Paint\_1 & 27.772 & \bf 29.305 & \bf 0.916 & 0.913 & 0.147 & \bf 0.139 \\
11\_Face\_Paint\_2 & \bf 27.352 & 27.336 & \bf 0.902 & 0.879 & \bf 0.152 & 0.195 \\
12\_Cave           & 21.825 & \bf 30.063 & 0.715 & \bf 0.881 & 0.314 & \bf 0.214 \\
\bottomrule
\end{tabular}
}
\label{tab:immersive}
\end{table*}
\clearpage{}

 
\end{document}
