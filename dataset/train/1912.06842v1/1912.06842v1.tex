\def\year{2020}\relax
\documentclass[letterpaper]{article} \usepackage{aaai20}  \usepackage{times}  \usepackage{helvet} \usepackage{courier}  \usepackage[hyphens]{url}  \usepackage{graphicx} \usepackage{multirow}

\usepackage{epsfig}
\usepackage{amssymb}
\usepackage{amsmath,bm}
\usepackage{sidecap}
\sidecaptionvpos{figure}{t}

\newcommand*{\pd}[3][]{\ensuremath{\frac{\partial^{#1} #2}{\partial #3}}}
\usepackage{pifont}\newcommand{\etal}{\emph{et al.}}

\urlstyle{rm} \def\UrlFont{\rm}  \frenchspacing  \setlength{\pdfpagewidth}{8.5in}  \setlength{\pdfpageheight}{11in}  \pdfinfo{
/Title (Fine-grained Recognition: Accounting for Subtle Differences between Similar Classes)
/Author (Guolei Sun, Hisham Cholakkal, Salman Khan, Fahad Shahbaz Khan, Ling Shao)
} 



\setcounter{secnumdepth}{2} 

\setlength\titlebox{2.5in} \title{Fine-grained Recognition: Accounting for Subtle\\ Differences between Similar Classes}
\author{Guolei Sun,\textsuperscript{\rm 1} Hisham Cholakkal,\textsuperscript{\rm 2} Salman Khan,\textsuperscript{\rm 2} Fahad Shahbaz Khan,\textsuperscript{\rm 2} Ling Shao\textsuperscript{\rm 2}   \\ \textsuperscript{\rm 1}ETH Zurich, \textsuperscript{\rm 2}Inception Institute of Artificial Intelligence\\ guolei.sun@vision.ee.ethz.ch, \{hisham.cholakkal, salman.khan, fahad.khan, ling.shao\}@inceptioniai.org }
\begin{document}

\maketitle

\begin{abstract}
The main requisite for fine-grained recognition task is to focus on subtle discriminative details that make the subordinate classes different from each other. We note that existing methods implicitly address this requirement and leave it to a data-driven pipeline to figure out what makes a subordinate class different from the others. This results in two major limitations: \emph{First,} the network focuses on the most obvious distinctions between classes and overlooks more subtle inter-class variations. \emph{Second,} the chance of misclassifying a given sample in any of the negative classes is considered equal, while in fact, confusions generally occur among only the most similar classes. Here, we propose to explicitly force the network to find the subtle differences among closely related classes. In this pursuit, we introduce two key novelties that can be easily plugged into existing end-to-end deep learning pipelines. On one hand, we introduce ``diversification block" which masks the most salient features for an input to force the network to use more subtle cues for its correct classification. Concurrently, we introduce a ``gradient-boosting" loss function that focuses only on the confusing classes for each sample and therefore moves swiftly along the direction on the loss surface that seeks to resolve these ambiguities. The synergy between these two blocks helps the network to learn more effective feature representations. Comprehensive experiments are performed on five
challenging datasets. Our approach outperforms existing methods using similar experimental setting on all five datasets.
\end{abstract}

\begin{figure}[ht]
\centering
\includegraphics[width=0.86\linewidth]{intro2.png}\\
\caption{Illustration of two novel components of our approach. \emph{Left:} comparison between class activation maps obtained from the model with our diversification block (DB) and the one without DB. Our DB forces the network to capture more discriminative regions. With DB (\emph{below}), the network finds beak, tail and feet of the bird as informative regions, while without DB (\emph{middle}), the network only focuses on beak. \emph{Right:} visual comparison in terms of - tSNE \cite{van2014accelerating} plot for features of 24 kinds of \emph{Walbler} (confusing and difficult classes)  in CUB-200-2011 between network trained with cross entropy (CE) (\emph{top}) and our gradient-boosting loss (\emph{below}). By focusing on difficult classes, our gradient-boosting loss can distinguish between hard classes which are not well separated by CE. 
}
\label{Fig:intro_image}
\end{figure}


\section{Introduction}
Fine-grained recognition focuses on discriminating between children classes of a main parent category (e.g., cars \cite{dataset_cars}, dogs \cite{dataset_dogs}, birds  \cite{dataset_cub}, and aircrafts \cite{dataset_aircraft}). Deep CNNs have excelled immensely on traditional visual recognition tasks where categories greater differ from each other.  However, fine-grained visual categorization (FGVC) poses a significant challenge mainly due to the close resemblance between subcategories e.g., different species of the same bird. The challenge is compounded by the fact that the classifier has to be invariant to intra-class variations, e.g., pose, appearance and lighting changes.


Common deep learning based approaches for FGVC learn a mapping between input images and output labels. While doing so, a natural tendency during learning is to focus on only few distinguishing parts in an object to deal with confusing inter-class similarities and large intra-class variations (see Fig. \ref{Fig:intro_image}). The analysis of attention based models provides the evidence that attention maps are often densely concentrated on a few parts, thus considering only a limited set of cues. In contrast, here we propose to spread the attention to  consolidate a diverse set of relevant cues spread across the activation map. While we diversify attention at the feature level, we do the opposite at the prediction level, i.e., focus on only the most confusing cases to achieve better discriminability. Popular loss functions such as cross entropy, consider all classes to compute the error signal for parameter update. When closely related classes are present in the data, this leads to a weak supervision signal resulting in slower convergence and low recall rates. We show that selectively attending to the hard negative 
classes helps in achieving much faster convergence and higher accuracy.

Our approach can also be understood as a mechanism to enhance network generalization and avoid overfitting. This consideration is of particular relevance to FGVC, since the datasets are generally smaller due to the high cost of obtaining fine-grained annotations from experts. In effort to minimize loss on training data, a high-capacity network can end up associating unrelated concepts (such as those of background) to the fine-grained object itself. By concentrating on only the closely related classes and diversifying the model's attention, we are in fact regularizing the model to avoid overfitting the training samples. Our approach reduces classifierâ€™s confidence on training samples and therefore makes it more generalizable. We note that regularization schemes such as label-smoothing \cite{szegedy2016rethinking} and maximum prediction entropy \cite{max_entropy} are related to ours, but significantly different as we impose regularization on both features and output predictions.

Our main contributions are as follows:
\begin{itemize}\setlength{\itemsep}{0em}
\item We introduce a \emph{gradient-boosting} loss  that seeks to resolve ambiguities among closely related classes by appropriately magnifying the gradient updates.
\item Our \emph{diversification block} masks out the salient features in order to force the network to look for subtle differences between similar-looking categories.
\item The proposed method makes the convergence faster while outperforming existing methods on five datasets.
\end{itemize}

\section{Related Works}
Fine-grained classification has attracted much research attention in the recent years. 
Despite several attempts \cite{nts_2018,mamc_2018}, 
FGVC is still an active research problem. 
To deal with the problem of subtle intra-class distance, many approaches focused on obtaining more relevant features~\cite{fine_grain_annotation1,lin2015bilinear,cbp,nts_2018,mamc_2018}. One of the earliest but naive strategy was to exploit part annotations \cite{fine_grain_annotation1} to locate the objects so that more informative features were used.  Such an approach requires more labeling effort and has therefore limited scalability. Another stream of works ~\cite{lin2015bilinear,cbp,Li_2018_CVPR} developed complex pooling methods, so that complex local features can be used for classification. However, one obvious drawback of those methods is the high computation complexity. To deal with the problem of small fine-grained datasets, Cui {\etal} ~\cite{cui2018large} proposed a transfer learning scheme from selected subset of the source domain to target domain. However, it requires to re-train models on a subset of large datasets like ImageNet \cite{ILSVRC15} and iNaturalist \cite{van2018inaturalist}.

Recent efforts~\cite{nts_2018,mamc_2018,Chen_2019_CVPR,zheng2019looking,ge2019weakly} used only class labels to automatically locate informative regions. 
Specifically, Yang \etal ~\cite{nts_2018} adapted a Navigator-Teacher-Scrutinizer system under a multi-stage scheme. Sun \etal ~\cite{mamc_2018} leveraged multiple channel attentions to learn several relevant regions. Wang \etal ~\cite{DFL_cnn_2018} used a bank of convolutional filters to capture discriminative regions in the feature maps.
Chen ~\cite{Chen_2019_CVPR} deconstructed and reconstructed input images to find discriminative regions and features. Zheng \etal ~\cite{zheng2019looking} proposed trilinear attention sampling network to learn features from different details. 
Despite the fact that the above methods perform well, they generally need to be trained in multiple stages or learn high-dimension features, resulting in increased training times. Another recent work \cite{ge2019weakly} developed a computationally complex, three-stage pipeline for fine-grained classification. Their framework requires a weakly supervised object detector, a mask-rcnn \cite{maskrcnn} based instance segmentation and an LSTM for capturing the context. Moreover, the mask-rcnn needs to be pretrained on an additional dataset: MS-COCO \cite{coco_eccv2014}. 
Our proposed diversification block adopts a novel way to find more relevant features by suppressing the most prominent discriminative regions in class activation maps  \cite{CAMs} and thus forcing the network to find other informative regions. We note that hide-and-seek \cite{has_2017} is related to ours, but largely different since our module works on feature maps and selectively suppresses discriminative regions. Our module is trained end-to-end with a computational cost nearly equal to the backbone.

Lately, FGVC strategies aimed to learn optimal classifiers on top of deep features have been proposed \cite{pc_2018,max_entropy}. Qian \etal~\cite{qian2015fine} employed a multi-stage framework which accepted pre-computed feature maps and learned the distance metric for classification.   Dubey \etal~ \cite{pc_2018} adapted the idea from pairwise learning and used Siamese-like neural network. 
A triplet loss was used in \cite{wang2016mining} to achieve better inter-class separation. The contrastive and triplet losses, however, increase the computational cost of training. \cite{max_entropy} proposed a maximum entropy loss for fine-grained classification by using the principle of maximum-entropy. All above methods do not specifically focus on differentiating confusing classes. Further, all the negative categories for a given sample are considered as equal. Our proposed gradient-boosting loss solves the problem by explicitly focusing on hard classes, incurs no additional cost and provides faster convergence rates.

\begin{figure*}[t]
\centering
\includegraphics[width=0.98\linewidth]{introduction_image2.png}\\
\caption{Overview of our overall architecture. Our method contains two novel components: diversification block and gradient-boosting loss. The diversification block suppresses the discriminative regions of the class activation maps, and hence the network is forced to find alternative informative features. The gradient-booting loss focuses on difficult (confusing) classes for each image and boosts their gradient. As a result, the network moves swiftly (faster convergence) to discriminate the hard classes.}
			\label{Fig:architectue}
\end{figure*}

\section{Method}
In this section, we introduce our method which can be easily plugged into any classification network. As shown in Fig. \ref{Fig:architectue}, to deploy our approach, we need to replace global pooling layer and the last fully connected layer of the backbone network with a 11 convolution having output channels equal to the number of classes. Our method includes two novel components: (a) A diversification module which forces the network to capture more subtle features, rather than only the most obvious ones; (b) A gradient boosting loss  which trains the network to focus on highly confusing classes. These two components will be addressed in this section.


\subsection{Diversification Block}\label{part_enhan}
Consider the multi-class image classification task with  classes as shown in Fig. \ref{Fig:architectue}. Let  be a training image with ground-truth label , where  is the label set containing all labels. The input to our diversification block is the category-specific activation maps , which is the output of the modified network. We denote , where  is the individual activation map corresponding to  class.  Here,  and  refer to the height and width of the output activation maps.  

The basic idea of our diversification block is to suppress the discriminative regions of the activation map , so that the network is forced to look for other informative regions which is expected to enhance classification performance. In the following, we will target two relevant questions: (1) Where to suppress information? and (2) How to suppress?

\subsubsection{Mask Generation} \label{sup_loca}
Here, we explain the procedure to generate the mask that indicates the locations in  that are suppressed. Let , where  denotes the binary suppressing mask for its corresponding activation map . Each element in mask  is in the domain , where  indicates the corresponding location will be suppressed while  means that no suppression will take place.

\textbf{\textit{Peak suppression:}} First, we randomly suppress the peak locations of the activation maps because they are the most discriminative regions for the classifier. By suppressing the peaks, the network is forced to find alternative relevant regions in the image. Let  be the peak map derived from  object category map () such that:

Here,  denotes the maximum of matrix . We suppress the peaks of different object categories with probability . The masks  to randomly hide the peaks are generated as follows:

where '' denotes element-wise multiplication and  is a Bernoulli random variable that has  probability of being 1.

\textbf{\textit{Patch suppression:}}
Peaks are the most discriminative regions, but there are other discriminative regions as well that encompass more subtle inter-class differences. In the following, we explain how to suppress locations other than peaks in the activation maps. We divide each  into a grid of patches, where each fixed sized patch  is indexed by row  and column . Lets assume the set of all such patches on the grid is given by:

After this operation, the activation map  will be divided into  patches. Let  be the mask for randomly hiding patches for  activation map .  For each patch inside , we randomly hide it with probability  and set the elements of corresponding locations of  as 1. Otherwise, the elements of  are set to :

where,  and  are in the same range as Eq.~\ref{eq:patches}. To consider only the non-peak locations, we then set the element of  in the peak location of  as 0,

The final suppressing mask for  category is obtained as:






\subsubsection{Activation Suppression Factor}\label{sup_values}
Setting values that replace the suppressed features is of much importance to achieve good performance. Let  represents the category activation maps obtained after our diversification module, which is generated as follows.

where,  denotes the suppressing factor. Basically, we replace the values in the suppressing locations as  times of their initial values. In general, setting  to a low number will lead to good performance. Throughout our experiments, we set  as 0.1.

After feature masking, we perform global average pooling to get the confidence scores   as follows:

where,  denotes global average pooling.

\subsection{Gradient-boosting Cross Entropy Loss}\label{selected_loss}
While diversification module aims at finding more subtle variations in the input images, our second contribution is  a gradient-boosting loss function that specifically focuses on confusing classes to avoid misclassifications between them. We elaborate the proposed loss function below. 

\subsubsection{Loss Function}
The most widely used loss for image classification is cross entropy (CE) loss. For an image , CE loss can be written as follows:

where  is the ground-truth label for image . Here, the loss considers all negative classes equally. However, in fine-grained classification, the ground-truth class is generally much closer to a related subset of classes than others. For example, in CUB-200-2011 \cite{dataset_cub}, bird class of \emph{Acadian Flycatcher} is more closer to categories such as \emph{Great Crested Flycatcher}, \emph{Least Flycatcher}, \emph{Olive sided Flycatcher} and other kinds of \emph{Flycatcher}, since they all belong to the same species. As a result, the network is prone to making mistakes among these similar (thus confusing) classes and predicting relatively higher confidence scores for them. Based on this observation, we argue that the loss should focus more on the confusing classes, rather than simply considering all negative classes equally for the normalization in Eq.~\ref{euq1}. Hence, we propose a novel and simple gradient-boosting cross entropy (GCE) loss which focuses only on  negative classes with top- highest confidence scores among all negative classes. Here,  simply means the number of negative classes to focus on. We will show in the next section, that the proposed loss basically boosts gradients to more swiftly resolve ambiguities between closely related confusing classes.  

We define  as the set of all negative classes, where . Let  be the set containing confidence scores of all negative classes. We get the  highest values of  by heap-max algorithm \cite{max_heap} and denote it as . Next, we split  into  and  by thresholding  using , defined as follows:


where,  contains the negative classes whose confidence scores are within the top- of all negative classes, and  is the set of negative classes whose confidence scores rank below the top- classification scores.  

Instead of considering all negative classes in Eq.~ \ref{euq1}, our gradient-boosting cross entropy loss only focuses on confusing classes (). The negative classes in  do not contribute to the loss since the network can easily distinguish them from the ground-truth class. Our proposed loss is given by:


As shown in Eq.~\ref{equ_set1} and \ref{gce_loss}, GCE loss focuses only on , containing  negative classes with top- highest confidence scores. Here,  is a hyper-parameter (we found  works best in our experiments). When , GCE is equivalent to CE. 

In the following analysis, we will show how our loss can boost the gradient for both the ground-truth class and confusing negative classes.




\subsubsection{Gradient Boosting}
We analyze the loss from the perspective of gradient. For the original cross entropy (CE) loss, the gradient for  is computed as:

For our gradient-boosting cross entropy loss, the gradient for  is computed as:

From our definition in Eq. \ref{equ_set1} and Eq. \ref{equ_set2}, the following relation exists between  and ,

As such, we obtain,


We can see that for both the ground-truth class and confusing negative classes, the gradient of our proposed loss is larger than the gradient of the original cross entropy loss. With our novel loss, the network can focus on differentiating difficult classes from the ground-truth class and converge faster, which is validated by our experiments.



\subsection{Training and Inference}
Our method is trained end-to-end in a single stage. The diversification block is only used during the training phase. As shown in Fig.~\ref{Fig:architectue}, during the training phase, class activation maps are passed through our novel diversification block and then to the global average pooling. As a result, discriminative regions are randomly masked and the network is forced to find other relevant areas. During test phase, the whole class activation maps are passed to global average pooling directly, without being suppressed at any region so that all informative regions found during training phase contribute to the final confidence score.

\section{Experiments}
\subsection{Datasets}
We comprehensively evaluate our algorithm on CUB-200-2011 \cite{dataset_cub}, Stanford Cars \cite{dataset_cars}, FGVC Aircraft \cite{dataset_aircraft}, and Stanford Dogs \cite{dataset_dogs}, all of which are widely used for fine-grained recognition. Statistics of all datasets are shown in Table~\ref{tab:dataset}. We follow the same train/test splits as in the table. For evaluation metric, we use top-1 accuracy following \cite{mamc_2018,pc_2018,max_entropy}.

Furthermore, we also evaluate on the recent terrain dataset for terrain recognition: GTOS-mobile \cite{dataset_gtosmobile} dataset and GTOS (Ground Terrain in Outdoor Scenes) \cite{dataset_gtos} dataset, which have potential use for autonomous agents (automatic car). The datasets are large-scale, containing classes of outdoor ground terrain, i.e. glass, sand, soil, stone-cement, and so on. Since those terrain classes are closely related, visually similar and thus difficult to classify, we use this challenging dataset to evaluate our method. Following \cite{dataset_gtosmobile}, we use GTOS as training and GTOS-mobile as test.
\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Dataset        & \#Class & \#Train & \#Test \\ \hline\hline
CUB-200-2011  & 200     & 5,994    & 5,794   \\ \hline
Stanford Cars  & 196     & 8,144    & 8,041   \\ \hline
FGVC Aircraft  & 100     & 6,667    & 3,333   \\ \hline
Stanford Dogs  & 120     & 12,000    & 8,580   \\ \hline
GTOS-mobile & 31     & 31,315    & 6,066   \\ \hline
\end{tabular}
\caption{Five commonly used benchmarks.}
\label{tab:dataset}
\end{table}

\subsection{Implementation Details}
For fair comparisons with other methods \cite{nts_2018,DFL_cnn_2018}, we use an input image resolution of 448448 in all experiments. 
We fine-tune pretrained network (ResNet-50 \cite{resnet_2016}) using our proposed diversification block and gradient-boosting loss due to its popularity in existing works. Momentum SGD optimizer is used with an initial learning rate of 0.001, which decays by 0.1 for every 50 epochs. We set weight decay as . Our algorithm is implemented using Pytorch \cite{paszke2017automatic} using two Tesla V100 GPU.

\subsection{Quantitative Results}
Our method does not require any part-annotation and can be trained using only class labels. Moreover, it is parameter-free and does not  increase the number of parameters compared to the ResNet-50 backbone. Our results are compared with the most recent and top-performing approaches evaluated under similar experimental setting. Several  approaches such as RACNN \cite{racnn_2017}, RAM \cite{li2017dynamic}, and NTS-net \cite{nts_2018} extract multiple crops at different scale from an input image. The classification score obtained from these crops are averaged to predict the final class during inference. For fair comparison, we report our  `multi-scale'  (five crops)  results, in addition to the  `single-scale' using one crop from an image.

The comparisons with various methods on four challenging fine-grained datasets, namely CUB-200-2011 \cite{dataset_cub}, FGVC Aircraft \cite{dataset_aircraft}, Stanford Cars \cite{dataset_cars}, and Stanford Dogs \cite{dataset_dogs},  are shown in Table \ref{tab:cub}. Additionally, results for GTOS-mobile \cite{dataset_gtos} are shown in Table \ref{tab:gtos}.  Overall, our proposed method outperforms previous methods on \emph{all} five datasets.

\begin{table*}[h]
\centering
\resizebox{2.1\columnwidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Methods} & \multirow{2}{*}{Backbone} & \multirow{2}{*}{Resolution} & \multirow{2}{*}{\#Parameters} & \multicolumn{4}{c|}{Accuracy}          \\ \cline{5-8} 
&      &     &      & \textbf{CUB-200-2011} & \textbf{Aircrafts} & \textbf{~~Cars~~} & \textbf{~~Dogs~~}
 \\ \hline \hline
RACNN \cite{racnn_2017} & VGG-19 & 448 & 429M & 85.3 &88.2&92.5&87.3\\\hline
RAM  \cite{li2017dynamic} & ResNet-50  & 448 & 23.9M & 86.0&-& 93.1 &-\\\hline
MACNN \cite{zheng2017learning} & VGG-19 & 448 &144M & 86.5 &89.9&92.8&-\\\hline
MAMC \cite{mamc_2018} & ResNet-50 & 448   & 434M  & 86.3 &-& 93.0&85.2\\ \hline
MaxEnt \cite{max_entropy} & ResNet-50 & -  & 23.9M   & 86.5 &89.8&93.9&83.6\\ \hline
PC \cite{pc_2018} & ResNet-50 & -   & 23.9M  & 86.9  &89.2& 93.4&83.8\\ \hline
DFL-CNN \cite{DFL_cnn_2018} & ResNet-50& 448    & 26.3M     & 87.4  &91.7& 93.1&-\\ \hline
NTS-net \cite{nts_2018} & ResNet-50 &   448  &  25.5M    &  87.5  &91.4&93.9&-\\ \hline
TASN \cite{zheng2019looking} & ResNet-50 &   448  &  35.2M    &  87.6 &-& 93.8&-\\ \hline
\hline
Ours (single scale) & ResNet-50& 448& 23.9M & 87.7 & 92.1 & 94.3 & 87.1\\ \hline
Ours (multi scale) & ResNet-50& 448& 23.9M & \textbf{88.6}&\textbf{93.5}&\textbf{94.9}&\textbf{87.7}\\ \hline
\end{tabular}}
\caption{Experimental results on four standard datasets. ``-'' means the information is not mentioned in the relevant paper. 
Our method outperforms existing approaches on four commonly used fine-grained datasets, and  requires no additional parameters compared to the ResNet-50 backbone.
Here, the parameters are computed on CUB-200-2011, having 200 output classes.}
\label{tab:cub}
\end{table*}

We observe that our method achieves the best accuracy on birds classification task (Table \ref{tab:cub}). Specifically, our method obtains an accuracy of 88.6\% which outperforms TASN (87.6\%) \cite{zheng2019looking}. TASN \cite{zheng2019looking} performs well because it first uses a small network to find the attentive regions and then distills knowledge from various informational regions to the model. With a low parametric complexity, our method can capture more relevant regions by focusing on hard classes and diversifying informative areas in the class activation maps.


For other four datasets, our method also outperforms the compared methods. In Aircraft, we achieve 93.5\% top-1 accuracy, surpassing NTS-net \cite{nts_2018} (91.4\%). In Cars, we obtain 94.9\%, outperforming the best performances: 93.8\% of TASN \cite{zheng2019looking}. In Dogs, we obtain 87.7\% top-1 accuracy compared to 87.3\% obtained by RACNN approach \cite{racnn_2017}. Note that RACNN has much more parameters (429M) than our methods (23.9M). In GTOS-mobile, we show our result using ResNet-50 with "single scale", for fair comparison with  Deep-TEN \cite{zhang2017deep} and DEP \cite{dataset_gtosmobile}. We get 85.0\%, which is 2.8\% better than the current state-of-the-art 82.2\%.

\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{|c|c|}
\hline
Methods        & Accuracy\\ \hline
B-CNN \cite{lin2015bilinear}&75.4  \\\hline
Deep-TEN \cite{zhang2017deep} &  76.1 \\\hline
DEP \cite{dataset_gtosmobile}  & 82.2\\\hline \hline
Ours (single scale) & \textbf{85.0}\\ \hline
\end{tabular}}
\caption{Experimental results on GTOS-mobile.}
\label{tab:gtos}
\end{table}

\subsection{Ablation Study}
To fully analyze our method, Table \ref{tab:ablation_study} provides a detailed ablation analysis on the key components of our method. It basically highlights the importance of diversification block and gradient-boosting loss. We conduct all ablation studies on CUB-200-2011 using the ResNet-50 \cite{resnet_2016}.

\textbf{Diversification block (DB)}. DB is important because it diversifies the informative regions by forcing the network to find relevant parts other than the most obvious ones. Integrating DB block in the ResNet-50 backbone results in a performance improvement of 0.8\%  (from 85.5\% to 86.3\%). 

\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{|c|c|}
\hline
Methods  & Accuracy \\ \hline \hline
ResNet-50  &   85.5   \\ \hline 
ResNet-50+DB  &  86.3    \\ \hline
ResNet-50+DB+Center loss \cite{center_loss}  &   86.4   \\ \hline
ResNet-50+DB+LGM loss \cite{lgm_loss} &   86.5  \\ \hline
ResNet-50+DB+MaxEnt \cite{max_entropy} &  86.2    \\ \hline
Ours (single scale)    &  \textbf{87.7} \\ \hline
Ours (multi scale)    &  \textbf{88.6} \\ \hline

\end{tabular}}
\caption{Ablation analysis on the CUB-200-2011. Our diversification block (DB) and gradient-boosting loss provide progressive improvements over the baseline.}
\label{tab:ablation_study}
\end{table}

\textbf{Gradient-boosting loss}. Our gradient-boosting loss is another important component that shows significant improvement. Using this loss, we improve the results from 86.3\% to 87.7\% (an absolute gain of 1.4\%).We also compare our loss with other recent losses that aim at achieving better discriminability: Center loss \cite{center_loss}, LGM loss \cite{lgm_loss}, and max entropy loss \cite{max_entropy}.  The results show that gradient-boosting loss outperforms all these loss functions. Our loss targets on difficult/confusing classes and selectively boosts the gradients for them, while other losses consider all negative classes as equal.
\begin{table}[t]
\centering
\begin{tabular}{| *{5}{c|}}
\hline
\centering
      & 0 & 0.1 &0.2 & 1.0 \\ \hline
Accuracy & 86.2  &  \textbf{86.3}   &  85.8   & 85.5 \\ \hline
\end{tabular}
\caption{Ablation study on suppressing factor . Keeping suppressing factor as small leads to good performance.}
\label{tab:ablation_alpha}
\end{table}



\begin{figure}[t]
		\centering
		\includegraphics[width=0.6\columnwidth]{loss_k3.png}
	\caption{Ablation study on  for our loss in CUB-200-2011.}
	\label{fig:loss_k}
\end{figure}

\textbf{Suppressing Factor}.
Here, we show a parameter sensitivity analysis on the suppressing factor . Top-1 accuracy with respect to different  settings is shown in Table~\ref{tab:ablation_alpha}. It shows that keeping  as a small value consistently leads to better performance than without using diversification block (). Specifically,  gives the best performance on CUB-200-2011 dataset.
\begin{figure*}[t]
\centering
\includegraphics[width=0.95\linewidth]{qual_result.png}\\
\caption{Class activation map (CAM) comparison between our method and baseline in different datasets. \emph{Top} to \emph{below}: original image, CAM of the ground-truth class of baseline, CAM of the ground-truth class of our method. While baseline only focuses on the most discriminative region, our method accurately diversifies attentions to other informative regions of the objects. }
\label{Fig:qual_result}
\end{figure*}
\begin{figure}[htp]
\centering
\includegraphics[width=0.86\columnwidth]{converge.png}\\
\caption{Training curves of our methods and the baseline (CE loss) on CUB-200-2011. Using our loss, our method converges faster and performs better than the baseline.}
	\label{fig:converge}
\end{figure}
\begin{figure}[t]
\centering
\includegraphics[width=0.89\columnwidth]{imagenet.png}\\
	\caption{Training curves of using our loss and Cross Entropy (CE) on ImageNet. Our loss converges considerably faster than CE and also leads to a better performance. 
	}
	\label{fig:imagenet}
\end{figure}

\textbf{Choices of }.
Here, we show ablation study on , the number of negative classes to focus on for gradient-boosting loss, in Fig. \ref{fig:loss_k}. It shows that by reducing , our loss focuses on more confusing classes and achieves consistent improvement in top-1 accuracy. 

\textbf{Convergence Analysis}.
We compare the training curves of our methods and baseline (ResNet-50) in Fig.~\ref{fig:converge}. It shows that our method converges much faster than the baseline, and also attains a lower error rate on test set. Remarkably, the baseline achieves a lower error rate on training set after 50 epochs, but fails to generalize well to the test set. This shows that the baseline is prone to overfitting on the train set, which our method successfully avoids. Our method uses diversification block which prevents the network to only focus on the most discriminative regions, like the beak or head of a bird. In contrast, using diversification block, the network finds various informative areas, thus reducing overfitting.


\subsection{Qualitative Results}
We qualitatively illustrate the comparison between our approach and the baseline in Fig.~\ref{Fig:qual_result}. We note that the diversification block indeed helps the network to find more discriminative regions in the image. In contrast, the baseline model generally focuses on the most obvious distinguishing patterns and its attention is limited to only a limited set of spatial locations. This explains why our approach generalizes better to test images, attaining a higher accuracy (Fig.~\ref{fig:converge}). 


\subsection{ImageNet Results}
To validate the generality of gradient-boosting loss in visual recognition, we apply it to a ResNet-50 backbone on ImageNet \cite{ILSVRC15}. Here, we use the input size of 224224 and follow the same training strategy as used in \cite{resnet_2016}. Since our loss focuses on difficult classes, we apply it only half way (50 epochs) during training when easy categories are already well-classified. The comparison of training curve between using our loss and using cross entropy (CE) is shown in Fig. \ref{fig:imagenet}. It shows that the proposed loss converges much faster than the CE loss and achieves a lower error rate on the challenging ImageNet benchmark. 

\section{Conclusion}
We proposed a novel approach to better discriminate closely related categories in fine-grained classification task. Our method has two novel components: (a) diversification block that forces the network to find subtle distinguishing features between each pair of classes and (b) gradient-boosting loss that specifically focuses on maximally separating the highly similar and confusing classes. Our approach not only outperforms existing methods on all studied fine-grained datasets, but also demonstrates much faster convergence rates. In comparison to previous methods, our solution is both simple and elegant, leads to higher accuracy and demonstrates better computational efficiency. 

\bibliographystyle{aaai}
\bibliography{egbib}

\end{document}
