

\documentclass[sigconf]{acmart}


\settopmatter{printacmref=true}


\fancyhead{}


\usepackage{balance}


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08emT\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}






\copyrightyear{2019}
\acmYear{2019}
\setcopyright{acmcopyright}
\acmConference[CIKM '19]{The 28th ACM International Conference on Information and Knowledge Management}{November 3--7, 2019}{Beijing, China}
\acmBooktitle{The 28th ACM International Conference on Information and Knowledge Management (CIKM '19), November 3--7, 2019, Beijing, China}
\acmPrice{15.00}
\acmDOI{10.1145/3357384.3358140}
\acmISBN{978-1-4503-6976-3/19/11}







\usepackage{multirow}
\begin{document}

\fancyhead{}



\title{Interactive Matching Network for Multi-Turn Response Selection in Retrieval-Based Chatbots}







\author{Jia-Chen Gu, Zhen-Hua Ling, Quan Liu}
\affiliation{\institution{University of Science and Technology of China, Hefei, China}}
\affiliation{\institution{State Key Laboratory of Cognitive Intelligence, iFLYTEK Research, Hefei, China}}
\email{gujc@mail.ustc.edu.cn, zhling@ustc.edu.cn, quanliu@ustc.edu.cn}



\begin{abstract}
  In this paper, we propose an interactive matching network (IMN) for the multi-turn response selection task. First, IMN constructs word representations from three aspects to address the challenge of out-of-vocabulary (OOV) words. Second, an attentive hierarchical recurrent encoder (AHRE), which is capable of encoding sentences hierarchically and generating more descriptive representations by aggregating with an attention mechanism, is designed. Finally, the bidirectional interactions between whole multi-turn contexts and response candidates are calculated to derive the matching information between them. Experiments on four public datasets show that IMN outperforms the baseline models on all metrics, achieving a new state-of-the-art performance and demonstrating compatibility across domains for multi-turn response selection.
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002951.10003317.10003338</concept_id>
<concept_desc>Information systems~Retrieval models and ranking</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}
\ccsdesc[500]{Information systems~Retrieval models and ranking}

\keywords{Interactive matching network, multi-turn response selection, retrieval-based chatbot}



\maketitle

\section{Introduction}
  Building a chatbot that can converse naturally with humans on open domain topics is a challenging yet intriguing problem in artificial intelligence \cite{DBLP:journals/sigkdd/ChenLYT17}. Response selection, which aims to select the best-matched response from a set of candidates given the context of a conversation, is an important retrieval-based approach for chatbots \cite{DBLP:conf/sigdial/LowePSP15,DBLP:conf/acl/WuWXZL17,DBLP:conf/acl/WuLCZDYZL18}.

  The techniques of word embeddings and sentence embeddings are important to response selection as well as many other natural language processing (NLP) tasks. The context and the response must be projected to a vector space appropriately to capture their relationships, which are essential for the subsequent procedures. Typically, word embeddings established on the task-specific training set and a single-layer recurrent neural network are employed for the response selection task. Another key technique to the response selection task lies in context-response matching. \citet{DBLP:conf/acl/ChenZLWJI17} showed that interactions between pairs of sentences can provide useful information to help matching.

  \citet{DBLP:conf/acl/WuWXZL17} proposed the sequential matching network (SMN) to match the response with each utterance and then to accumulate matching information by an RNN. \citet{DBLP:conf/coling/ZhangLZZL18} refined utterance and employed self-matching attention to route the vital information in each utterance based on the SMN. \citet{DBLP:conf/acl/WuLCZDYZL18} proposed the deep attention matching network (DAM) to construct representations at different granularities with stacked self-attention.

  In this paper, we propose a novel neural network architecture, called the interactive matching network (IMN), for multi-turn response selection in retrieval-based chatbots. Our proposed IMN is similar to SMN but has three main differences: (1) constructing word representations from three aspects to enhance the representations at the word-level, (2) enhancing sentence representations through  an attentive hierarchical recurrent encoder to enhance the representations at the sentence-level and (3) capturing interactions between contexts and responses by collecting matching information bidirectionally to enrich the representations.

  We test our model on Ubuntu Dialogue Corpus V1 \cite{DBLP:conf/sigdial/LowePSP15}, Ubuntu Dialogue Corpus V2 \cite{DBLP:journals/dad/LowePSCLP17}, Douban Conversation Corpus \cite{DBLP:conf/acl/WuWXZL17} and E-commerce Dialogue Corpus \cite{DBLP:conf/coling/ZhangLZZL18}. The results show that our model can outperform the baseline models on all metrics, achieving new state-of-the-art performance and showing compatibility across domains for multi-turn response selection.

  In summary, our contributions in this paper are threefold. (1) This paper proposes a new model, named IMN, for multi-turn response selection in retrieval-based chatbots. (2) The empirical results show that our proposed model outperforms the baseline models in terms of all metrics on four datasets, achieving new state-of-the-art performance for multi-turn response selection. (3) This paper presents detailed experiments and discussions on contributions of each part to context-response pair matching.

  \begin{figure*}
    \centering
    \includegraphics[width=15cm]{model_overview_V7.pdf}
    \caption{An overview of our proposed IMN model.}
    \label{fig1}
  \end{figure*}


\section{Interactive Matching Network}

  We present here our proposed IMN model, which is composed of five layers.Figure~\ref{fig1} shows an overview of the architecture. 

  \subsection{Problem Formalization}
    Given a dialogue dataset , an example of the dataset can be represented as . Specifically,  represents a conversation context with  as the utterances.  is a response candidate, and  denotes a label.  indicates that  is a proper response for ; otherwise, . Our goal is to learn a matching model , which provides the matching degree between  and  by minimizing the sigmoid cross entropy from .


  \subsection{Word Representation Layer}

    One challenge of large dialogue corpora is the large number of OOV words. To address this issue, we propose to construct word representations with a combination of general pretrained word embedding, those estimated on the task-specific training set and character-level embeddings. 

    Formally, the embeddings of the \emph{k}-th utterance in a conversation and a response candidate at this layer are denoted as  and .  and  are embeddings of a \emph{d}-dimensional vector.  and  are the numbers of words in  and  respectively.

  \subsection{Sentence Encoding Layer}

    Typically, the outputs of the top layer in a multi-layer RNNs are regarded as the final sentence representations, and the other layers are neglected. However, the lower layers can also provide useful sentence descriptions, such as part-of-speech tagging and syntax-related information. Motivated by the method of ELMo \cite{DBLP:conf/naacl/PetersNIGCLZ18}, we propose a new sentence encoder, called the attentive hierarchical recurrent encoder (AHRE) to make full use of the representations at all hidden layers.

    BiLSTMs \cite{DBLP:journals/neco/HochreiterS97} are employed as our basic building blocks. In an \emph{M}-layer RNN, each  layer takes the output of the  layer as its input.

    Finally, we obtain a set of \emph{M} representations \{\} and \{\} for the \emph{k}-th utterance in a conversation and a response candidate through the \emph{M}-layer RNNs, where  and , .
Here, we propose to combine the set of representations to obtain the enhanced representations  and  by learning the attention weights of all the layers. Mathematically, we have

    where ,  and  are the softmax-normalized weights shared between utterances and responses, which need to be estimated during the training process. As a result, representations given by AHRE are expected to fuse multi-level characteristics of sentences.

  \subsection{Matching Layer}

    \begin{table*}[!hbt]
     \small
     \caption{Evaluation results of IMN and previous methods on Ubuntu Dialogue Corpus V1 and  V2.}
     \centering
     \begin{tabular}{l|c|c|c|c|c|c|c|c}
      \toprule
                             & \multicolumn{4}{c|}{Ubuntu Corpus V1} & \multicolumn{4}{c}{Ubuntu Corpus V2} \\
      \hline
                             &  &  &  &  &  &  &  & \\
      \hline


CompAgg \cite{DBLP:journals/corr/WangJ16b}         & 0.884 & 0.631 & 0.753 & 0.927 & 0.895 & 0.641 & 0.776 & 0.937 \\
       BiMPM \cite{DBLP:conf/ijcai/WangHF17}              & 0.897 & 0.665 & 0.786 & 0.938 & 0.877 & 0.611 & 0.747 & 0.921 \\
       HRDE-LTC \cite{DBLP:conf/naacl/YoonSJ18}           & 0.916 & 0.684 & 0.822 & 0.960 & 0.915 & 0.652 & 0.815 & 0.966 \\
       SMN \cite{DBLP:conf/acl/WuWXZL17}                  & 0.926 & 0.726 & 0.847 & 0.961 & -     & -     & -     & -     \\
       DUA \cite{DBLP:conf/coling/ZhangLZZL18}            & -     & 0.752 & 0.868 & 0.962 & -     & -     & -     & -     \\
       DAM \cite{DBLP:conf/acl/WuLCZDYZL18}               & 0.938 & 0.767 & 0.874 & 0.969 & -     & -     & -     & -     \\
      \hline
       IMN                  & \textbf{0.946} & \textbf{0.794} & \textbf{0.889} & \textbf{0.974} & \textbf{0.945} & \textbf{0.771} & \textbf{0.886} & \textbf{0.979} \\
       IMN(Ensemble)        & \textbf{0.951} & \textbf{0.807} & \textbf{0.900} & \textbf{0.978} & \textbf{0.950} & \textbf{0.791} & \textbf{0.899} & \textbf{0.982}  \\
      \bottomrule
      \end{tabular}
      \label{tab2}
    \end{table*}

    \begin{table*}[!hbt]
     \small
     \caption{Evaluation results of IMN and previous methods on the Douban Conversation Corpus and E-commerce Corpus.}
\centering
     \begin{tabular}{l|c|c|c|c|c|c|c|c|c}
      \toprule
                             & \multicolumn{6}{c|}{Douban Conversation Corpus} & \multicolumn{3}{c}{E-commerce Corpus} \\
      \hline
                             & \textbf{MAP} & \textbf{MRR} &  &  &  &  &  &  &  \\
      \hline
SMN \cite{DBLP:conf/acl/WuWXZL17}                  & 0.529 & 0.569 & 0.397 & 0.233 & 0.396 & 0.724 & 0.453 & 0.654 & 0.886  \\
       DUA \cite{DBLP:conf/coling/ZhangLZZL18}            & 0.551 & 0.599 & 0.421 & 0.243 & 0.421 & 0.780 & 0.501 & 0.700 & 0.921  \\
       DAM \cite{DBLP:conf/acl/WuLCZDYZL18}               & 0.550 & 0.601 & 0.427 & 0.254 & 0.410 & 0.757 & -     & -     & -      \\
      \hline
       IMN                  & \textbf{0.570} & \textbf{0.615} & \textbf{0.433} & \textbf{0.262} & \textbf{0.452} & \textbf{0.789} & \textbf{0.621} & \textbf{0.797} & \textbf{0.964}\\
       IMN(Ensemble)        & \textbf{0.576} & \textbf{0.618} & \textbf{0.441} & \textbf{0.268} & \textbf{0.458} & \textbf{0.796} & \textbf{0.672} & \textbf{0.845} & \textbf{0.970} \\
      \bottomrule
      \end{tabular}
      \label{tab3}
    \end{table*}

Unlike previous work, which matches responses with each utterance in a context separately in an utterance-response manner \cite{DBLP:conf/acl/WuWXZL17,DBLP:conf/acl/WuLCZDYZL18,DBLP:conf/coling/ZhangLZZL18}, IMN matches the response with the whole context in a global context-response way, i.e., considering the whole context as a single sequence.
The global context-response matching can help select the most relevant parts of the whole context and neglect the irrelevant parts.

    First, the context  with  is formed by concatenating the set of utterance representations .

    Then, an attention-based alignment is employed to collect information between two sequences by computing the attention weight between each tuple as .

For a word in the response, its response-to-context relevant representation is composed as
    
    where ,  is a weighted summation of .
The same calculation is performed for each word in a context to form context-to-response representation .


To further enhance the collected information, the matching matrices are formed as
    

    Finally, the concatenated context  need to be converted to separate utterances .

  \subsection{Aggregation Layer}
    The aggregation layer converts the matching matrices of separated utterances and responses into a final matching vector.


    First, the set of utterance embeddings  and the response embeddings  are obtained by composing the enhanced local matching information  and  with a BiLSTM, and a combination of max pooling and last hidden state pooling.

    Furthermore, the set of utterance inference vectors  is fed into another BiLSTM in chronological order of the utterances in the context, followed by another pooling operation to obtain the aggregated context embeddings .

    The final matching feature vector is the concatenation of the context embeddings and the response embeddings as .

  \subsection{Prediction Layer}

    We then input the matching feature vector  into a multi-layer perceptron (MLP) classifier. The MLP returns a score to denote the matching degree of a context-response pair.


\section{Experiments}

  \subsection{Datasets}

    We tested IMN on Ubuntu Dialogue Corpus V1 \cite{DBLP:conf/sigdial/LowePSP15}, Ubuntu Dialogue Corpus V2 \cite{DBLP:journals/dad/LowePSCLP17}, Douban Conversation Corpus \cite{DBLP:conf/acl/WuWXZL17} and E-commerce Dialogue Corpus \cite{DBLP:conf/coling/ZhangLZZL18}.


  \subsection{Evaluation Metrics}
    We used the same evaluation metrics as those used in previous work \cite{DBLP:conf/sigdial/LowePSP15,DBLP:conf/acl/WuWXZL17,DBLP:conf/coling/ZhangLZZL18}. We calculated the recall of the true positive replies among the  selected responses from  available candidates, denoted as . In addition, mean average precision (\textbf{MAP}), mean reciprocal rank (\textbf{MRR}) and precision-at-one (), are especially considered for the Douban corpus, following the settings of previous work.


  \subsection{Experimental Results}

    Table~\ref{tab2} and Table~\ref{tab3} present the evaluation results of IMN and previous methods. All the results except ours are from the existing literature. IMN  outperforms other models on all metrics and datasets, which demonstrates its ability to select the best-matched response and its compatibility across domains (system troubleshooting, social network and e-commerce). The Douban Corpus includes multiple correct candidates for a context in its test set. Hence,  and  are recommended for reference.

Our proposed model outperforms the present state-of-the-art methods on the respective datasets by a margin of 2.6\% in terms of  on Ubuntu V1; 11.9\% in terms of  on Ubuntu V2; 2.0\% in terms of \textbf{MAP} and 1.4\% in terms of \textbf{MRR} on Douban Corpus; and 12.0\% in terms of  on E-commerce Corpus, achieving a new state-of-the-art performance on all datasets. Furthermore, we provide ensemble models built by averaging the outputs of four single models with identical architectures and different random initializations.
    Our code has been published at \emph{https://github.com/JasonForJoy/IMN} to help replicate our results. 


\section{Ablations and Analysis}

    To demonstrate the importance of each component in our proposed model, various parts of the architecture were ablated, as shown in Table~\ref{tab4}.

    \begin{table}
     \small
     \caption{Ablation tests on Ubuntu V2 test set.}
     \centering
     \begin{tabular}{lcccc}
      \toprule
&  &  &  &  \\
      \midrule
       IMN                   & 0.945 & 0.771 & 0.886 & 0.979  \\
       - AHRE                & 0.940 & 0.758 & 0.874 & 0.974  \\
       - Char emb            & 0.941 & 0.762 & 0.878 & 0.976  \\
       - Match               & 0.904 & 0.613 & 0.792 & 0.958  \\
      \bottomrule
      \end{tabular}
      \label{tab4}
    \end{table}

    \paragraph{AHRE} The number of layers in the AHRE was set to 3. The AHRE can be considered as a generalized recurrent encoder that degenerates into a single-layer RNN when the number of layers in the AHRE is set to 1. The softmax-normalized weights of layers in the AHRE are listed in Table~\ref{tab5}, which indicates that each layer of the multi-layer RNNs contributes to the embeddings. 

    \begin{table}
\caption{Layer-wise weights of a three-layer AHRE.}
     \centering
     \begin{tabular}{lccc}
      \toprule
                  & Layer 1 & Layer 2 & Layer 3 \\
      \midrule
       Weights    & 0.4938  & 0.2181  & 0.2881  \\
      \bottomrule
      \end{tabular}
      \label{tab5}
    \end{table}

    \paragraph{Char emb} The character embeddings in the word representation layer were ablated, which resulted in a performance decrease. Additionally, we found that the lowest layer of the RNN in the AHRE constituted the highest weight, as shown in Table~\ref{tab5}. These two results may be explained by the importance of morphology information to the response selection.


    \paragraph{Match} The decreased performance indicates that interactions between contexts and responses are beneficial for matching.
    We conduct a case study and visualize the response-to-context weights used in Eq.~\ref{equ3} to demonstrate their ability to select relevant parts as shown in Figure~\ref{fig3}. Some important words such as ``\emph{connect}", ``\emph{router}" and ``\emph{ethernet}" in the response can select their relevant words in the context, and some unimportant words such as ``\emph{tried}", ``\emph{channels}" and ``\emph{the}" in the context occupy small weights when forming representations.

    \begin{figure}
    \centering
    \includegraphics[width=5cm]{match_case.pdf}
    \caption{Response-to-context attention weights for a sample. The darker units mean larger values.}
    \label{fig3}
    \end{figure}


\section{Conclusion}
  In this paper, we propose an interactive matching network for the response selection task. An empirical study on four public datasets shows that our proposed model outperforms the baseline models on all metrics, achieving new state-of-the-art performance and showing compatibility across domains for multi-turn response selection.

\section*{Acknowledgements}
  We thank anonymous reviewers for their valuable comments.

\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


\appendix

\section{Supplemental Material}

  \subsection{Detailed Dataset Descriptions}

    We tested IMN on two English public multi-turn response selection datasets, Ubuntu Dialogue Corpus V1 \cite{DBLP:conf/sigdial/LowePSP15} and Ubuntu Dialogue Corpus V2 \cite{DBLP:journals/dad/LowePSCLP17}, and two Chinese datasets, Douban Conversation Corpus \cite{DBLP:conf/acl/WuWXZL17} and E-commerce Dialogue Corpus \cite{DBLP:conf/coling/ZhangLZZL18}.
    Ubuntu Dialogue Corpus V1 and V2 contain multi-turn dialogues about Ubuntu system troubleshooting in English.
    Here, we adopted the version of Ubuntu Dialogue Corpus V1 shared in \citet{DBLP:journals/corr/XuLWSW16}, in which numbers, paths and URLs were replaced by placeholders.
    Compared with Ubuntu Dialogue Corpus V1, the training, validation and test dialogues in the V2 dataset were generated in different periods without overlap. Besides, the V2 dataset discriminates between the end of an utterance (\_eou\_) and the end of a turn (\_eot\_). In both of the Ubuntu corpora, the positive responses are true responses from humans, and the negative responses are randomly sampled.
    The Douban Conversation Corpus was crawled from a Chinese social network on open-domain topics. It was constructed in a similar way to the Ubuntu corpus. The Douban Conversation Corpus collected responses via a small inverted-index system, and labels were manually annotated. The E-commerce Dialogue Corpus collected real-world conversations between customers and customer service staff from the largest e-commerce platform in China.
    Some statistics of these datasets are provided in Table~\ref{tab1}.

    \begin{table}[!hbt]
\caption{Statistics of the datasets that our model is tested on.}
   \centering
   \setlength{\tabcolsep}{1.6mm}{
   \begin{tabular}{c|c|ccc}
   \toprule
    \multicolumn{2}{c|}{Dataset}                      & Train & Valid & Test \\
    \hline
    \multirow{3}*{Ubuntu V1}    & pairs               & 1M    & 0.5M  & 0.5M \\
                                & positive:negative   & 1: 1  & 1: 9  & 1: 9 \\
                                & positive/context    & 1     & 1     & 1    \\
    \hline
    \multirow{3}*{Ubuntu V2}    & pairs               & 1M    & 195k  & 189k \\
                                & positive:negative   & 1: 1  & 1: 9  & 1: 9 \\
                                & positive/context    & 1     & 1     & 1    \\
    \hline
    \multirow{3}*{Douban}       & pairs               & 1M    & 50k   & 10k  \\
                                & positive:negative   & 1: 1  & 1: 1  & 1: 9 \\
                                & positive/context    & 1     & 1     & 1.18 \\
    \hline
    \multirow{3}*{E-commerce}   & pairs               & 1M    & 10k   & 10k  \\
                                & positive:negative   & 1: 1  & 1: 1  & 1: 9 \\
                                & positive/context    & 1     & 1     & 1    \\
   \bottomrule
   \end{tabular}}
   \label{tab1}
   \end{table}

  \subsection{Training Details}

    The Adam method was employed for optimization, with a batch size of 96 for the two English datasets and 128 for the two Chinese datasets.
    The initial learning rate was 0.001 and was exponentially decayed by 0.96 every 5000 steps.
    Dropout with a rate of 0.2 was applied to the word embeddings and all hidden layers.

    The word embeddings for the English datasets were concatenations of the 300-dimensional GloVe embeddings, 100-dimensional embeddings estimated on the training set using the Word2Vec algorithm and 150-dimensional character-level embeddings with window sizes of \{3, 4, and 5\}, each consisting of 50 filters.
    The word embeddings for the Chinese datasets were concatenations of the 200-dimensional embeddings from and the 200-dimensional embeddings estimated on the training set using the Word2Vec algorithm. Character-level embeddings were not employed for the two Chinese datasets due to the large number of Chinese characters. The word embeddings were not updated during training.

    All hidden states of the LSTM had 200 dimensions. The number of BiLSTM layers in the AHRE was 3.
    The MLP at the prediction layer had a hidden unit size of 256 with ReLU activation.
    The maximum word length was set to 18, the maximum utterance length was set to 50, and the maximum context length was set to 10. We padded with zeros if the number of utterances in a context was less than 10; otherwise, we kept the last 10 utterances.
    We used the development dataset to set the stop condition to select the best model for testing.



  \subsection{Performance of Different Number of Layers in AHRE}
    For the Ubuntu V2 dataset, the number of layers in the AHRE was tuned on its validation set. Using three layers achieved the best performance as shown in Figure~\ref{fig2}.
    \begin{figure}
    \centering
    \includegraphics[width=6cm]{number_layers_AHRE.pdf}
    \caption{Performance of IMN for different numbers of layers in the AHRE on Ubuntu V2 validation set.}
    \label{fig2}
    \end{figure}


\end{document}
