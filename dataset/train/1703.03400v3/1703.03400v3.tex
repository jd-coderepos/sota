\documentclass{article}



\usepackage{nips_2016}
\setcitestyle{numbers,square,citesep={,},aysep={,},yysep={,}}



\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{arrows}

\usepackage{times}
\usepackage{graphicx} \usepackage{caption}
\usepackage{subcaption}
\usepackage{wrapfig}


\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{hyperref}







\usepackage{amsmath}
\usepackage{amssymb}

\usepackage[font=footnotesize]{caption,subcaption}
\usepackage{sidecap}

\usepackage{xspace}

\usepackage{color}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]

\newcommand{\pushright}[1]{\ifmeasuring@#1\else\omit\hfill\fi\ignorespaces}

\newcommand{\xb}{\mathbf{x}}
\newcommand{\Xb}{\mathbf{X}}
\newcommand{\Yb}{\mathbf{Y}}
\newcommand{\Kb}{\mathbf{K}}

\newcommand{\specialcell}[2][c]{\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

\clearpage{}\newcommand{\cmt}[1]{{\footnotesize\textcolor{red}{#1}}}
\newcommand{\note}[1]{\cmt{Note: #1}}
\newcommand{\todo}[1]{\cmt{TO-DO: #1}}
\newcommand{\sergey}[1]{\cmt{Sergey: #1}}
\newcommand{\chelsea}[1]{\cmt{Chelsea: #1}}
\newcommand{\pieter}[1]{\cmt{Pieter: #1}}
\newcommand{\review}[1]{\noindent\textcolor{red}{ #1}}
\newcommand{\response}[1]{\noindent{#1}}

\long\def\ignorethis#1{}

\newcommand{\etal}{{et~al.}\ }
\newcommand{\eg}{e.g.\ }
\newcommand{\ie}{i.e.\ }
\newcommand{\nth}{\text{th}}
\newcommand{\pr}{^\prime}
\newcommand{\tr}{^\mathrm{T}}
\newcommand{\inv}{^{-1}}
\newcommand{\pinv}{^{\dagger}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\gauss}{\mathcal{N}}
\newcommand{\norm}[1]{\left|#1\right|}
\newcommand{\trace}{\text{tr}}

\newcommand{\figtodo}[1]{\framebox[0.8\columnwidth]{\rule{0pt}{1in}#1}}
\newcommand{\figref}[1]{Figure~\ref{fig:#1}}
\newcommand{\secref}[1]{Section~\ref{sec:#1}}

\newcommand{\secpaperlin}{5.1}
\newcommand{\eqnpapergradmul}{3}
\newcommand{\eqnpapergrads}{4}
\newcommand{\applinhess}{A}
\newcommand{\applqr}{B}
\newcommand{\appfastgrad}{C}

\newcommand{\vnorm}[1]{\|#1\|}
\newcommand{\lscnorm}[1]{\ell_{12}(#1)}
\newcommand{\costnorm}{r_\ell}

\newcommand{\badmmtraj}{\phi^{\trajdist}}
\newcommand{\badmmpol}{\phi^{\params}}
\newcommand{\lagtraj}{\mathcal{L}_{\trajdist}}
\newcommand{\lagpol}{\mathcal{L}_{\params}}

\newcommand{\energy}{\mathcal{E}}

\newcommand{\admmrate}{\alpha}
\newcommand{\cluster}{c}
\newcommand{\dualstep}{\eta}
\newcommand{\polwt}{\lambda}
\newcommand{\lgmult}{\lambda}
\newcommand{\lgmultt}{\lambda_t}
\newcommand{\lgmu}{\lambda_\mu}
\newcommand{\lgmut}{\lambda_{\mu t}}
\newcommand{\admmrho}{\nu}
\newcommand{\lagrangian}{\mathcal{L}_{\text{traj}}}
\newcommand{\lagrangiangps}{\mathcal{L}_{\text{GPS}}}
\newcommand{\trajdist}{p}
\newcommand{\policy}{\pi}
\newcommand{\policytraj}{\pi_{\traj}}
\newcommand{\return}{J}
\newcommand{\params}{\theta}
\newcommand{\reward}{R}
\newcommand{\cost}{\ell}
\newcommand{\state}{\mathbf{x}}
\newcommand{\action}{\mathbf{a}}
\newcommand{\obs}{\mathbf{o}}
\newcommand{\opt}{\mathcal{R}}
\newcommand{\hstate}{\hat{\mathbf{x}}}
\newcommand{\haction}{\hat{\mathbf{u}}}
\newcommand{\states}{\mathcal{X}}
\newcommand{\actions}{\mathcal{A}}
\newcommand{\traj}{\tau}
\newcommand{\covar}{\Sigma}
\newcommand{\trajmu}{\mu^\trajdist}
\newcommand{\trajmut}{\mu^\trajdist_t}
\newcommand{\polmu}{\mu^\policy}
\newcommand{\polsig}{\Sigma^\policy}
\newcommand{\polmut}{\mu^\policy_t}
\newcommand{\polmudiffavgt}{\hat{\mu}^\policy_t}
\newcommand{\polsigt}{\Sigma^\policy_t}
\newcommand{\polgrad}{\polmu_\state}
\newcommand{\polgradt}{\polmu_{\state t}}
\newcommand{\polgradavgt}{\hat{\mu}^\policy_{\state t}}
\newcommand{\ucovar}{\mathbf{C}}
\newcommand{\xcovar}{\mathbf{S}}
\newcommand{\ucovart}{\mathbf{C}_t}
\newcommand{\xcovart}{\mathbf{S}_t}
\newcommand{\hxcovart}{\hat{\mathbf{S}}_t}
\newcommand{\xcovartp}{\mathbf{S}_{t+1}}
\newcommand{\xucovar}{\Sigma}
\newcommand{\xucovart}{\Sigma_t}
\newcommand{\xcovarchol}{\mathbf{L}}
\newcommand{\xcovarcholt}{\xcovarchol_t}
\newcommand{\samp}{\mathbf{s}}
\newcommand{\sampti}{\samp_{ti}}
\newcommand{\sampdist}{q}
\newcommand{\obj}{\mathcal{L}}
\newcommand{\objut}{\obj_{\action t}}
\newcommand{\objuut}{\obj_{\action,\action t}}
\newcommand{\objxt}{\obj_{\state t}}
\newcommand{\objxxt}{\obj_{\state,\state t}}
\newcommand{\objucovart}{\obj_{\ucovar t}}
\newcommand{\objxcovart}{\obj_{\xcovar t}}
\newcommand{\objxcovartp}{\obj_{\xcovar t+1}}
\newcommand{\feedback}{\mathbf{K}}
\newcommand{\detpolicy}{g}
\newcommand{\wtreg}{w_r}
\newcommand{\velx}{v_x}
\newcommand{\posy}{p_y}
\newcommand{\pos}{\mathbf{p}}
\newcommand{\torquepen}{w_\action}
\newcommand{\pospen}{w_\pos}
\newcommand{\velpen}{w_v}
\newcommand{\heightpen}{w_h}
\newcommand{\all}{{1..T}}

\newcommand{\channel}{c}
\newcommand{\softmax}{\mathbf{s}}
\newcommand{\softmaxpix}{s_{cij}}
\newcommand{\responsemap}{\mathbf{a}}
\newcommand{\responsepix}{a_{cij}}
\newcommand{\responsepixprime}{a_{ci'j'}}

\newcommand{\innerterm}{\xi}

\newcommand{\second}{\text{s}}
\newcommand{\ms}{\text{m/s}}
\newcommand{\meter}{\text{m}}

\newcommand{\kl}{D_\text{KL}}
\newcommand{\tv}{D_\text{TV}}
\newcommand{\ent}{\mathcal{H}}
\newcommand{\rdist}{\rho}

\newcommand{\fc}{f_c}
\newcommand{\fx}{f_\state}
\newcommand{\fu}{f_\action}
\newcommand{\fct}{f_{c t}}
\newcommand{\fxt}{f_{\state t}}
\newcommand{\fut}{f_{\action t}}
\newcommand{\fy}{f_{\state\action}}
\newcommand{\fyt}{f_{\state\action t}}
\newcommand{\rx}{\reward_\state}
\newcommand{\ru}{\reward_\action}
\newcommand{\rxx}{\reward_{\state,\state}}
\newcommand{\ruu}{\reward_{\action,\action}}
\newcommand{\rux}{\reward_{\action,\state}}
\newcommand{\rxt}{\reward_{\state t}}
\newcommand{\rut}{\reward_{\action t}}
\newcommand{\rxxt}{\reward_{\state,\state t}}
\newcommand{\ruut}{\reward_{\action,\action t}}
\newcommand{\ruxt}{\reward_{\action,\state t}}
\newcommand{\Qx}{Q_\state}
\newcommand{\Qu}{Q_\action}
\newcommand{\Qy}{Q_{\state\action}}
\newcommand{\Qxx}{Q_{\state,\state}}
\newcommand{\Quu}{Q_{\action,\action}}
\newcommand{\Qux}{Q_{\action,\state}}
\newcommand{\Qxu}{Q_{\state,\action}}
\newcommand{\Qyy}{Q_{\state\action,\state\action}}
\newcommand{\Qxt}{Q_{\state t}}
\newcommand{\Qut}{Q_{\action t}}
\newcommand{\Qyt}{Q_{\state\action t}}
\newcommand{\Qxxt}{Q_{\state,\state t}}
\newcommand{\Quut}{Q_{\action,\action t}}
\newcommand{\Quxt}{Q_{\action,\state t}}
\newcommand{\Qxut}{Q_{\state,\action t}}
\newcommand{\Qyyt}{Q_{\state\action,\state\action t}}
\newcommand{\Vx}{V_\state}
\newcommand{\Vxx}{V_{\state,\state}}
\newcommand{\Vxt}{V_{\state t}}
\newcommand{\Vxxt}{V_{\state,\state t}}
\newcommand{\Vxtp}{V_{\state t+1}}
\newcommand{\Vxxtp}{V_{\state,\state t+1}}
\newcommand{\kpol}{\mathbf{k}}
\newcommand{\Kpol}{\mathbf{K}}
\newcommand{\passivedyn}{p}
\newcommand{\lmdp}{\mathcal{G}}
\newcommand{\samples}{\mathcal{S}}
\newcommand{\ft}{f_t}
\newcommand{\noise}{\mathbf{F}}
\newcommand{\siglinmatt}{\mathbf{M}}


\newcommand{\objKt}{\obj_{\Kpol t}}
\newcommand{\objKht}{\obj_{\hat{\Kpol} t}}
\newcommand{\polsampcovar}{\mathbf{C}_t}
\newcommand{\cholin}{\mathbf{D}_t}

\newcommand{\costgrad}{\cost_{\state\action}}
\newcommand{\costhess}{\cost_{\state\action,\state\action}}
\newcommand{\tcostgrad}{\tilde{\cost}_{\state\action}}
\newcommand{\tcosthess}{\tilde{\cost}_{\state\action,\state\action}}
\newcommand{\costxx}{\cost_{\state,\state}}
\newcommand{\costuu}{\cost_{\action,\action}}
\newcommand{\costxu}{\cost_{\state,\action}}
\newcommand{\costux}{\cost_{\action,\state}}
\newcommand{\costxxt}{\cost_{\state,\state t}}
\newcommand{\costuut}{\cost_{\action,\action t}}
\newcommand{\costxut}{\cost_{\state,\action t}}
\newcommand{\costuxt}{\cost_{\action,\state t}}
\newcommand{\costxt}{\cost_{\state t}}
\newcommand{\costut}{\cost_{\action t}}
\newcommand{\costgradu}{\cost_\action}
\newcommand{\costhessu}{\cost_{\action,\action}}

\newcommand{\costgradt}{\cost_{\state\action t}}
\newcommand{\costhesst}{\cost_{\state\action,\state\action t}}
\newcommand{\tcostgradt}{\tilde{\cost}_{\state\action t}}
\newcommand{\tcosthesst}{\tilde{\cost}_{\state\action,\state\action t}}
\newcommand{\costgradut}{\cost_{\action t}}
\newcommand{\costhessut}{\cost_{\action,\action t}}


\newcommand{\tcgradt}{\tilde{c}_{\state\action t}}
\newcommand{\tchesst}{\tilde{c}_{\state\action,\state\action t}}

\newcommand{\choldiff}{\text{choldiff}}

\newcommand{\st}{\state_t}
\newcommand{\at}{\action_t}
\newcommand{\ot}{\obs_t}
\newcommand{\sti}{\state_{ti}}
\newcommand{\ati}{\action_{ti}}
\newcommand{\sth}{\hstate_t}
\newcommand{\ath}{\haction_t}

\newcommand{\points}{\mathbf{p}}
\newcommand{\Cost}{\langle\ell\rangle}
\newcommand{\Penalty}{a}
\newcommand{\Rate}{b}

\newcommand{\empsig}{\hat{\Sigma}}
\newcommand{\empmu}{\hat{\mu}}
\newcommand{\empn}{N}
\newcommand{\priorphi}{\mathbf{\Phi}}
\newcommand{\priormu}{\mu_0}
\newcommand{\priormut}{\mu_{0t}}
\newcommand{\priorm}{m}
\newcommand{\priorn}{n_0}
\newcommand{\datapt}{\mathbf{p}}
\newcommand{\ddpdiscount}{\gamma}
\newcommand{\onlinediscount}{\beta}
\newcommand{\xxt}{\Delta}
\newcommand{\net}{\bar{f}}
\newcommand{\sigt}{\bar{\Sigma}_{\state\action,\state\action}}
\newcommand{\sigp}{\bar{\Sigma}_{\state\pr,\state\pr}}
\clearpage{}

\title{Notes: Training Sensitive Policies}

\author{
  David S.~Hippocampus\thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
}

\begin{document}


\maketitle



\begin{abstract} 

These notes discuss how we can train policies that are able to adapt quickly.

\end{abstract} 



\section{Sensitive Policies}
\label{sec:sensitivity}

Fast, sample-efficient learning is a huge challenge for deep RL. But does it have to be? The conventional wisdom is that ``shallow'' RL is sample-efficient because the hand-designed features provide a good basis for fast learning. But don't deep networks learn good features? Shouldn't deep RL therefore be \emph{faster} once good features have been learned? Indeed, one of the huge benefits of deep architectures should be the ability to learn high-sensitivity neurons that can change the policy behavior in large but structured ways. For example, if a bipedal walker policy can learn a ``direction'' neuron it should be able to toggle walking right versus walking left just by changing the sign on that neuron, which requires changing a relatively small number of connections in relatively local ways.

So the goal is to learn policy parameters  that can change the policy in large but structured ways in response to small local changes in . That is, we should have a \emph{sensitive} policy. We can define sensitivity in terms of the change in the reward function. For example, if our goal is to maximize , and we take gradient steps of the form

we can define sensitivity as

Equivalently, this corresponds to the derivative of the expected reward with respect to the step size. The goal is to have a large (positive) sensitivity with respect to a reward of interest. If we have that, we can learn quickly. If we have high sensitivity to \emph{many} different rewards, we can likely learn quickly even with respect to new rewards.

\section{Optimizing for Sensitivity}

Let us define \emph{locally adapted} parameters in terms of an adaptation operator :

where  is a reward function from the set of rewards . We can define a sensitivity optimization problem as

where  is a \emph{base} reward function (which may be omitted in general) that defines the main problem we are solving, while  are auxiliary reward functions that encourage the discovery of a policy that is sensitive to the types of rewards we care about. For example,  for a humanoid might be to stand upright, while each  might correspond to running in a different direction. If we solve this type of problem, we get a policy that can very quickly learn to run in different directions, and perhaps can generalize to other structured exploration behaviors.

As Chelsea pointed out, this procedure assumes that the reward expectations are smooth. This is not the same as assuming that the rewards \emph{themselves} are smooth, since the expectation of a discontinuous function might still be continuous. This also assumes that the auxiliary rewards are sufficiently diverse.

The optimization requires second derivatives, but these can likely be computed efficiently with most automatic differentiation packages.

\section{The \emph{Other} Fast Weights}

An interesting extension on the above optimization problem is to define a \emph{fast weights} mask , so that only a subset of the policy parameters are adapted, and redefine  as

where  represents an elementwise product. Under the ``feature learning'' view of deep networks, this amounts to saying that some weights should be trained for sensitivity, while others are frozen during adaptation because they represent broadly useful features.  might be either fixed or learned.

\section{Open Problems}

While the procedure described above may be effective for optimizing sensitive policies that adapt quickly to new situations, it leaves a number of openings for improvement. First, the adaptation procedure considered involves only one gradient step. This is fine in the ``differential'' view of sensitivity defined in Section~\ref{sec:sensitivity}, but may be limiting in general. It's also not immediately obvious how to extend this framework from policy gradient to other algorithms such as actor-critic, though the extension may be relatively straightforward.

\end{document}
