\section{Experiment}

To verify the performance of the proposed DCT network, we conduct a set of extensive experiments and compare the proposed model against multiple baselines in various scenarios.

\subsection{Settings}
\textbf{Datasets.} We employ two datasets for evaluation: the KITTI \cite{geiger2012we} and Argoverse \cite{chang2019argoverse} datasets. First, the KITTI dataset provides a number of splits. We evaluate the performance of each model on three KITTI splits: the KITTI 3D object detection split (KITTI 3D Object) \cite{chen2016monocular} with 3,712 training and 3,769 validation images for 3D vehicle detection, the KITTI Odometry split for road layout estimation with 15,806 training and 6,636 validation images, and the KITTI Raw \cite{schulter2018learning} with 10,156 training and 5,074 validation images for road layout estimation. Next, the Argoverse dataset consists of 6,723 training and 2,418 validation images for road layout estimation.

\textbf{Metrics.} For quantitative analysis, we adopt the standard object detection and segmentation metrics \cite{lin2014microsoft}: the mean of intersection-over-union (mIoU) and the mean average precision (mAP) metrics.


\textbf{Baselines.} We compare the performance of the proposed method against a set of state-of-the-art methods: MonoOccupancy \cite{lu2019monocular}, MonoLayout \cite{mani2020monolayout}, VPN \cite{pan2020cross}, Mono3D \cite{chen2016monocular}, OFTNet \cite{roddick2019orthographic}, and CVT \cite{yang2021projecting}. One thing to note is that we use Mono3D \cite{chen2016monocular} and  OFTNet \cite{roddick2019orthographic} for detecting vehicles in the top-view and adapted versions of VPN \cite{pan2020cross} and MonoOccupancy \cite{lu2019monocular} for fair comparison \cite{yang2021projecting}. Further, we utilize the latest performance of baselines reported online which is slightly distinctive than the reported results in their original papers. We exclude baselines that employ additional information such as multiple input images \cite{zhao2022jperceiver, saha2022translating} rather than a monocular image input.


\subsection{Implementation Details}
We implement the proposed model using the Pytorch framework and train the model using a single NVIDIA A30 GPU. We normalize the input images to 10241024. The size of the ground truth label of KITTI Odometry, KITTI 3D Object, KITTI Raw and Argoverse are originally 128128, 256256, 256256, and 512512, respectively; they all get interpolated as 10241024. Next, we render the ground truths of single-class instances as three-channel images by repeating the image content in the channel axis; we prioritize vehicles over roads to make each pixel contain one class label when ground truth pixels include multiple class labels. In addition, we render the ground truths of multi-class instances as one-hot encoded three-channel image. For feature extraction, we utilize the ResNet-18 architecture \cite{he2016deep} without bottleneck layers; for domain transfer between the front-view and the top-view, we employ MLPs with two fully-connected layers interspersed with the ReLU activation. For training, we employ the Adam optimizer \cite{kingma2015adam}. Moreover, the batch size, learning rate and the number of training epochs are 6, 1e-4 and 120, respectively. In the single class learning setting, we decay the learning rate once by 0.1 at the 50-th epoch; in the multi-class learning setting, we decay the learning rate once by 0.1 at the 100-th epoch. 
\subsection{Ablation Study}
To verify the effectiveness of each component of the proposed learning scheme, we conduct ablation studies in two learning settings: single-class learning and multi-class learning.

\textbf{Single-Class Learning.} Table \ref{tb:result_ablation_singleclass} describes the ablation study result in the single-class learning scenario. The baseline model learned with the naive cross-entropy loss. As we applied the focal loss to the baseline model, the performance enhanced since the proposed learning scheme with the focal loss effectively dealt with the class imbalance. Moreover, the dual cycle loss further improved the performance as the loss incorporated the complete dual cycle consistency. In short, sequential application of the focal loss and the proposed dual cycle loss to the baseline model boosts the performance sequentially---which demonstrates the effectiveness of both the proposed application of the focal loss and the designed dual cycle loss.

\textbf{Multi-Class Learning.}
Table \ref{tb:result_ablation_multiclass} describes the ablation study result in the multi-class learning scenario. Similar to the single-class learning scenario, the application of the focal loss and the dual-cycle loss in progression to the baseline model enhanced the performance with substantial performance margins consecutively---corroborating the validity of each component of the proposed learning scheme in the multi-class learning setting. However, note that the multi-class learning deteriorates performance and the performance drop is more severe for vehicles. We surmise that vehicles occupying less areas than roads in a road layout require more deliberate learning scheme design.



\begin{table}[!t]
\caption{Ablation study results using different losses in the single-class learning scenario on the Argoverse Dataset.}
\label{tb:result_ablation_singleclass}
\begin{center}
\begin{footnotesize}
\begin{tabular}{ccccc}
\toprule
\multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{Loss items}}} & \multicolumn{2}{c}{\textbf{Argoverse Road}} & \multicolumn{2}{c}{\textbf{Argoverse Vehicle}} \\ 
\cmidrule(lr){2-5}
 & mIoU (\%) & mAP (\%) & mIoU (\%) & mAP (\%)  \\

\midrule
Baseline & 76.56& 87.30 & 47.86 & 62.69 \\ 
\midrule
Focal &\textbf{76.74} &88.47  & 47.41 & 67.48\\ 
Focal +Dual Cycle &76.71&\textbf{88.86}&\textbf{47.94}&\textbf{68.95}\\
\bottomrule
\end{tabular}
\end{footnotesize}
\end{center}
\end{table}


 \begin{table}[!t]
\caption{Ablation study results using different losses in the multi-class learning sceneario on the Argoverse Dataset.}
\label{tb:result_ablation_multiclass}
\begin{center}
\begin{footnotesize}
\begin{tabular}{ccccc}
\toprule
\multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{Loss items}}} & \multicolumn{2}{c}{\textbf{Argoverse Road}} & \multicolumn{2}{c}{\textbf{Argoverse Vehicle}} \\ 
\cmidrule(lr){2-5}
 & mIoU (\%) & mAP (\%) & mIoU (\%) & mAP (\%)  \\

\midrule
Baseline& 74.45 & 86.51 & 29.53 & 43.84 \\  \midrule
Focal &73.93 & 86.00 & \textbf{31.89} & 46.19\\
Focal +Dual Cycle  &\textbf{74.73}& \textbf{86.76} &31.75&\textbf{46.20}\\
\bottomrule
\end{tabular}
\end{footnotesize}
\end{center}
\end{table}

 
\subsection{Comparative Study}
\begin{figure*}[t]
\begin{center}
   \includegraphics[width=1\linewidth]{Figures/Files/Qual.png}
\end{center}
   \caption{The qualitative comparison results of road layout estimation and vehicle occupancy segmentation in BEV on the KITTI Odometry dataset and the KITTI 3D Object dataset, respectively. In short, the proposed DCT network displays superior performance compared to the latest baseline models in both tasks; the DCT network can transform the road layout from the front-view to the top-view more accurately and even distinguish adjacent vehicles in a row.}
\label{fig:qual}
\end{figure*} \begin{figure}[t]
\begin{center}
   \includegraphics[width=0.95\linewidth]{Figures/Files/Qual-Multi.png}
\end{center}
   \caption{The qualitative comparison results of simultaneous road layout estimation and vehicle occupancy segmentation in BEV on the Argoverse dataset. The proposed DCT network displays less noise and produces more accurate segmentation results compared to the baseline method.}
\label{fig:multiclass_qual}
\end{figure} 
We perform comparative studies to establish the performance contribution against the baseline models. Note that, in Tables \ref{tb:result_comparative} and \ref{tb:result_comparative_3d}, * means the latest performance published online, which are slightly different from the reported results in the original papers.

\textbf{Single-Class Learning.}
Tables \ref{tb:result_comparative} and \ref{tb:result_comparative_3d} exhibit the comparative study result in the single-class learning scenario. The proposed DCT architecture shows state-of-the-art performance on both the KITTI and Argoverse datasets with substantial performance gains. The performance gain is more prominent in mAP than mIoU (7.85\% and 6.26\% mAP enhancement in the KITTI 3D and Argoverse vehicle datasets, respectively) since we proposed to handle the class imbalance utilizing the focal loss.



Moreover, Fig. \ref{fig:qual} presents the qualitative comparison of our approach against the baseline methods. The comparison result demonstrates that our method overcomes existing class imbalance problem effectively. The predicted road layout from the DCT network contains less noise than the baselines (in fact, almost zero noise). Plus, the DCT network generates road layouts with more accurate and clearer boundaries. Next, our DCT method shows superior performance in the 3D object detection in BEV task. The DCT network successfully distinguishes densely arranged cars in a row (marked as red boxes in the figure) whereas the baseline models likely failed to differentiate them.


\textbf{Multi-Class Learning.}
Table \ref{tb:result_comparative_multiclass} quantifies the comparative study result in the multi-class learning scenario; the proposed DCT network accomplished state-of-the-art performance with considerable margins. The performance margin is more conspicuous for predicting vehicles where the class imbalance problem is more severe. In addition, all methods considered in the experiment displayed degraded performance in the multi-class learning scenario compared to the single-class learning scenario. We assume that vehicles taking up much fewer pixels than roads in a road layout cause the performance degradation---demanding an advanced learning scheme for multi-class learning.

Besides, Fig. \ref{fig:multiclass_qual} illustrates the qualitative comparison result. The qualitative comparison result establishes the superiority of the proposed DCT architecture likewise. Our DCT architecture transforms the road layout from the front-view to the top-view with more accurate and clearer boundaries compared to the baseline model; furthermore, the road layouts from the DCT network contains less noise. In addition, the DCT network differentiates adjacent vehicles, which the baseline model can hardly distinguish.



\begin{table*}[!t]
\caption{Comparative study results of top-view semantic segmentation on the KITTI and Argoverse datasets.}
\label{tb:result_comparative}
\begin{center}
\begin{small}
\begin{tabular}{ccccccccc}
\toprule
\multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{Method}}} & \multicolumn{2}{c}{\textbf{KITTI Raw}} & \multicolumn{2}{c}{\textbf{KITTI Odometry}} & \multicolumn{2}{c}{\textbf{Argoverse Road}} & \multicolumn{2}{c}{\textbf{Argoverse Vehicle}} \\ 
\cmidrule(lr){2-5}
\cmidrule(lr){6-9}
 & mIoU (\%) & mAP (\%) & mIoU (\%) & mAP (\%) & mIoU (\%) & mAP (\%) & mIoU (\%) & mAP (\%) \\

\midrule

MonoOccupancy \cite{lu2019monocular} & 58.41 & 66.01 & 65.74 & 67.84 & 72.84 & 78.11 & 24.16 & 36.83\\
VPN \cite{pan2020cross} & 59.58 & 79.07 & 66.81 & 81.79 & 71.07 & 86.83 & 16.58 & 39.73 \\
MonoLayout \cite{mani2020monolayout} & \textbf{66.02} & 75.73 & 76.15 & 85.25 & 73.25 & 84.56 & 32.58 & 51.06\\
CVT \cite{yang2021projecting} & 64.13 & 83.37 & \textbf{77.47} & 86.39 & 76.56 & 87.30 & 47.86 & 62.69\\
\midrule

\textbf{Ours}& 65.86&  \textbf{86.56}&  77.15& \textbf{88.28} & \textbf{76.71}&  \textbf{88.86}& \textbf{47.94} & \textbf{68.95}\\

\bottomrule
\end{tabular}
\end{small}
\end{center}
\end{table*}
 \begin{table}[!t]
\caption{Comparative study results on the KITTI 3D Object dataset.}
\label{tb:result_comparative_3d}
\begin{center}
\begin{small}
\begin{tabular}{ccc}
\toprule
\multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{Method}}} & \multicolumn{2}{c}{\textbf{KITTI 3D Object}}\\ 
\cmidrule(lr){2-3}
 & mIoU (\%) & mAP (\%) \\

\midrule

MonoOccupancy \cite{lu2019monocular} & 20.45 & 22.59\\
Mono3D \cite{chen2016monocular}& 17.11 & 26.62\\
OFTNet \cite{roddick2019orthographic} & 25.24 & 34.69\\
VPN \cite{pan2020cross}& 16.80 & 35.54\\
MonoLayout \cite{mani2020monolayout}& 30.18 & 45.91\\
CVT \cite{yang2021projecting} & 38.85 & 51.04\\ 
\midrule

\textbf{Ours}& \textbf{39.44} & \textbf{58.89} \\

\bottomrule
\end{tabular}
\end{small}
\end{center}
\end{table}
 \begin{table}[!t]
\caption{Comparative study results of Multi-Class learning on the Argoverse dataset.}
\label{tb:result_comparative_multiclass}
\begin{center}
\begin{footnotesize}
\begin{tabular}{ccccc}
\toprule
\multicolumn{1}{c}{\multirow{2}[2]{*}{\textbf{Method}}} & \multicolumn{2}{c}{\textbf{Argoverse Road}} & \multicolumn{2}{c}{\textbf{Argoverse Vehicle}} \\ 
\cmidrule(lr){2-5}
 & mIoU (\%) & mAP (\%) & mIoU (\%) & mAP (\%)  \\

\midrule
MonoLayout \cite{mani2020monolayout} & 67.29& 79.57 & 16.39 & 26.42 \\
CVT \cite{yang2021projecting} & 74.40 & \textbf{87.07} & 30.02& 44.07 \\
\midrule
\textbf{Ours} &\textbf{74.73}&86.76&\textbf{31.75}&\textbf{46.20}\\

\bottomrule
\end{tabular}
\end{footnotesize}
\end{center}
\end{table}

 