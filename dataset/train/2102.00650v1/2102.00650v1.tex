\documentclass{article} \usepackage{iclr2021_conference,times}



\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \def\lkl{L_{\mathrm{kd}}}
\def\lce{L_{\mathrm{ce}}}
\def\pykd{\hat{\rvy}_{\mathrm{kd}}}
\def\pyce{\hat{\rvy}_{\mathrm{ce}}}
\def\aykd{\bar{\rvy}_{\mathrm{kd}}}
\def\ayce{\bar{\rvy}_{\mathrm{ce}}}
\def\zkd{{Z}_{\mathrm{kd}}}
\def\zce{{Z}_{\mathrm{ce}}}
\def\fkd{{f}_{\mathrm{kd}}}
\def\fce{{f}_{\mathrm{ce}}}
\def\hy{\hat{y}}
\newtheorem{assumption}{Assumption}
\usepackage{subfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\usepackage{booktabs}
\newcommand\revise[1]{#1}
\usepackage{multirow}
\usepackage{wrapfig}

\usepackage{hyperref}
\usepackage{url}

\title{Rethinking Soft Labels for Knowledge Distillation: A Bias-Variance Tradeoff Perspective}



\author{
Helong Zhou\textsuperscript{\rm 1}\thanks{These authors contributed equally to this work.} ~, 
Liangchen Song\textsuperscript{\rm 2}\footnotemark[1]~~\thanks{Work done while the author was a research intern at Horizon Robotics.} ~,
Jiajie Chen\textsuperscript{\rm 1}\footnotemark[1] ~, 
Ye Zhou\textsuperscript{\rm 1}, 
Guoli Wang\textsuperscript{\rm 1}\textsuperscript{\rm 3}, \\
\textbf{
Junsong Yuan\textsuperscript{\rm 2},
Qian Zhang\textsuperscript{\rm 1}} \\
\textsuperscript{\rm 1}Horizon Robotics \quad
\textsuperscript{\rm 2}University at Buffalo \quad
\textsuperscript{\rm 3}Tsinghua University\\
\texttt{\{helong.zhou,jiajie.chen,ye.zhou,guoli.wang\}@horizon.ai}\\
\texttt{\{lsong8,jsyuan\}@buffalo.edu}, \texttt{qian01.zhang@horizon.ai}
}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy \begin{document}


\maketitle

\begin{abstract}
Knowledge distillation is an effective approach to leverage a well-trained network or an ensemble of them, named as the teacher, to guide the training of a student network. The outputs from the teacher network are used as soft labels for supervising the training of a new network. Recent studies \citep{muller2019does,yuan2020revisiting} revealed an intriguing property of the soft labels that making labels soft serves as a good regularization to the student network. 
From the perspective of statistical learning, regularization aims to reduce the variance, however how bias and variance change is not clear for training with soft labels. 
In this paper, we investigate the bias-variance tradeoff brought by distillation with soft labels. 
Specifically, we observe that during training the bias-variance tradeoff varies sample-wisely. Further, under the same distillation temperature setting, we observe that the distillation performance is negatively associated with the number of some specific samples, which are named as regularization samples since these samples lead to bias increasing and variance decreasing. Nevertheless, we empirically find that completely filtering out regularization samples also deteriorates distillation performance. Our discoveries inspired us to propose the novel weighted soft labels to help the network adaptively handle the sample-wise bias-variance tradeoff. Experiments on standard evaluation benchmarks validate the effectiveness of our method. Our code is available at \url{https://github.com/bellymonster/Weighted-Soft-Label-Distillation}.
\end{abstract}


\section{Introduction}
For deep neural networks \citep{goodfellow2016deep}, knowledge distillation (KD) \citep{ba2014deep,Hinton2015DistillingTK} refers to the technique that uses well-trained networks to guide the training of another network. Typically, the well-trained network is named as the teacher network while the network to be trained is named as the student network. For distillation, the predictions from the teacher network are leveraged and referred to as the soft labels \citep{balan2015bayesian,muller2019does}. Soft labels generated by the teacher network have been proven effective in large-scale empirical studies \citep{liang2019knowledge,Tian2020Contrastive,ZagoruykoK17,RomeroBKCGB14} as well as recent theoretical studies \citep{phuong2019towards}.

However, the reason why soft labels are beneficial to the student network is still not well explained. Giving a clear theoretical explanation is challenging: The optimization details of a deep network with the common one-hot labels are still not well-studied \citep{NIPS2019_9336}, not to mention training with the soft labels. Nevertheless, two recent studies \citep{muller2019does,yuan2020revisiting} shed light on the intuitions about how the soft labels work. Specifically, label smoothing, which is a special case of soft labels based training, is shown to regularize the activations of the penultimate layer to the network \citep{muller2019does}. The regularization property of soft labels is further explored in \citep{yuan2020revisiting}. They hypothesize that in KD, one main reason why the soft labels work is the regularization introduced by soft labels. Based on the assumption, the authors design a teacher-free distillation method by turning the predictions of the student network into soft labels. 

Considering that soft labels are targets for distillation, the evidence of the regularization brought by soft labels drives us to rethink soft labels for KD: Soft labels are both supervisory signals and regularizers.
Meanwhile, it is known that there is a tradeoff between fitting the data and imposing regularizations, i.e., the bias-variance dilemma \citep{kohavi1996bias,bishop2006pattern}, but it is unclear how bias and variance change for distillation with soft labels.
Since the bias-variance tradeoff is an important issue in statistical learning, we investigate whether the bias-variance tradeoff exists for soft labels and how the tradeoff affects distillation performance.

We first compare the bias and variance decomposition of direct training with that of distillation with soft labels, noticing that distillation results in a larger bias error and a smaller variance. 
Then, we rewrite distillation loss into the form of a regularization loss adding the direct training loss. 
Through inspecting the gradients of the two terms during training, we notice that for soft labels, the bias-variance tradeoff varies sample-wisely. Moreover, by looking into a conclusion from \citep{muller2019does}, we observe that under the same temperature setting, the distillation performance is negatively associated with the number of some certain samples. These samples lead to bias increase and variance decrease and we name them as regularization samples.  
To investigate how regularization samples affect distillation, we first examine if we can design ad hoc filters for soft labels to avoid training with regularization samples. But completely filtering out regularization samples also deteriorates distillation performance, leading us to speculate that regularization samples are not well handled by standard KD.
In the light of these findings, we propose weighted soft labels for distillation to handle the sample-wise bias-variance tradeoff, by adaptively assigning a lower weight to regularization samples and a larger weight to the others. 
To sum up, our contributions are:
\vspace{-5pt}
\begin{itemize}
\item For knowledge distillation, we analyze how the soft labels work from a perspective of bias-variance tradeoff.
\vspace{-2pt}
\item We discover that the bias-variance tradeoff varies sample-wisely. Also, we discover that if we fix the distillation temperature, the number of regularization samples is negatively associated with the distillation performance.
\vspace{-2pt}
\item We design straightforward schemes to alleviate negative impacts from regularization samples and then propose the novel weighted soft labels for distillation. Experiments on large scale datasets validate the effectiveness of the proposed weighted soft labels.
\vspace{-5pt}
\end{itemize}

\vspace{-5pt}
\section{Related works}
\vspace{-5pt}
\paragraph{Knowledge distillation.} 
\citet{Hinton2015DistillingTK} proposed to distill outputs from large and cumbersome models into smaller and faster models, which is named as knowledge distillation. The outputs for large networks are averaged and formulated as soft labels. 
Also, other kinds of soft labels have been widely used for training deep neural networks \citep{szegedy2016rethinking,pereyra2017regularizing}.
Treating soft labels as regularizers were pointed out in \citep{Hinton2015DistillingTK} since a lot of helpful information can be carried in soft labels. 
More recently, \citet{muller2019does} showed the adverse effect of label smoothing upon distillation. It is a thought-provoking discovery for the reason that both label smoothing and distillation are exploiting the regularization property behind soft labels. 
\citet{yuan2020revisiting} further investigated the regularization property of soft labels and then proposed a teacher free distillation scheme. 

\paragraph{Distillation loss.} 
One of our main contributions is that we improve the distillation loss.
For adaptively adjusting the distillation loss, \citet{tang2019learning} pays attention to hard-to-learn and hard-to-mimic samples, and the latter is weighted based on the prediction gap between teacher and student. However, it does not consider that the teacher may give an incorrect guide to the student, under which the prediction gap is still large and such a method may lead to the performance being hurt. \citet{saputra2019distilling} transfers teacher's guidance only on the samples where the performance of the teacher surpasses the student, while \citet{wen2019preparing} deals with the incorrect guidance by probability shifting strategy. Our approach is different from the above methods, in terms of motivations as well as the proposed solutions.

\paragraph{Bias-variance tradeoff.} 
Bias-variance tradeoff is a well-studied topic in machine learning \citep{kohavi1996bias,domingos2000unified,valentini2004bias,bishop2006pattern} and for neural networks \citep{geman1992neural,neal2018modern,belkin2019reconciling,yang2020rethinking}. Existing methods are mainly concerned with the variance brought by the choice of network models. Our perspective is different from the previous methods since we focus on the behavior of samples during training. 
\revise{In our work, based on the results from \citet{heskes1998bias}, we present the decomposition of distillation loss, which is defined by Kullback-Leibler divergence.}
Besides, our main contribution is not to study how to theoretically analyze the tradeoff, but how to adaptively tune the sample-wise tradeoff during training.

\section{Bias-variance tradeoff for soft labels}
\begin{wrapfigure}{r}{0.3\textwidth}
    \vspace{-40pt}
\begin{center}
    \includegraphics[width=0.29\textwidth]{fig/bias-variance.pdf}
    \end{center}
    \vspace{-5pt}
    \caption{Bias and variance.}
    \label{fig:bias-variance}
    \vspace{-8pt}
\end{wrapfigure}
Soft labels play the role of supervisory signals and regularizations at the same time, which inspires us to rethink soft labels from the perspective of the bias-variance tradeoff.
We begin our analysis with some mathematical descriptions. For a sample  labeled as -th class, let the ground-truth label be a one-hot vector  where  and other entries are 0. Then for  and softmax output temperature , the soft label predicted by the teacher network is denoted as  and the output from the student is denoted as . 
The soft label  is then used for training the student by the distillation loss, i.e. , where  means the -th element of the student's output  and the teacher's output , respectively. With the above notations, the cross-entropy loss for training with one-hot labels is .


We now present the bias-variance decomposition for  and , based on the definition and notations from \citet{heskes1998bias}. 
\revise{
First, we denote the train dataset as  and the output distribution on a sample  of the network trained without distillation as . For the network trained with distillation, the model also depends on the teacher network, so we define the output on  as , where  is the selected teacher network. 
}
Then, let the averaged output of  and  be  and , that is,
\revise{

}
where  are two normalization constant.
\revise{
Then according to \citet{heskes1998bias}, we have the following decomposition for the expected error on the sample  and  is the ground truth label:

}
where  is the Kullback-Leibler divergence. The  derivation of the variance term is based on the facts that  is a constant and . Detailed derivations can be found from Eq. (4) in \citet{heskes1998bias}.
\revise{
Next, we analyze the bias-variance decomposition of . As mentioned above, when training with soft labels, extra randomness is introduced for the selection of a teacher network.
In Fig. \ref{fig:bias-variance}, we illustrate the corresponding bias and variance for the selection process of a set of soft labels, which are generated by a teacher network.
In this case, a high variance model indicates the model (grey point) is closer to the one-hot trained model (black point), while a low variance model indicates that the model is closer to other possible models trained with soft labels (red points). 
Although for KD there are more sources introducing randomness, the overall variance brought by  is not necessarily higher than .
}
In fact, existing empirical results strongly suggest that the overall variance is smaller with KD. For example, students trained with soft labels are better calibrated than one-hot baselines \citep{muller2019does} and KD makes the predictions of students more consistent when facing adversarial noise \citep{papernot2016distillation}. Here, we present these empirical evidence as an assumption:
\begin{assumption}
The variance brought by KD is smaller than direct training, that is, 
\revise{
.
}
\end{assumption}
Similar to Eq. (\ref{eq:ce-decompose}), we write the decomposition for  as
\revise{

An observation here is that  converges to one-hot labels while  converges to soft labels, so  is closer to the one-hot ground-truth distribution  than , i.e., . 
If we rewrite  as , then  causes that the bias increases by  and the variance decreases by .
}



From the above analysis, we separate  into two terms, and  leads to variance reduction, and  leads to bias reduction.
In the following sections, we first analyze how  links to the bias-variance tradeoff during training. 
Then we analyze the changes in the relative importance between bias reduction and variance reduction during training with soft labels. 


\subsection{The bias-variance tradeoff during training}
\label{sec:intuitive}
It is known that bias reduction and variance reduction are often in conflict and we cannot minimize bias and variance together. 
However, if we consider the change of bias and variance during the training process, the importance of tuning the tradeoff also changes during training.
Specifically, shortly after the training of the network starts, the bias error dominates the total error and the variance is less important.
As training goes on, gradients of reducing the bias error (induced by ) and reducing the variance (induced by ) can be of the same scale for some samples, then we need to balance the tradeoff because reducing one term is likely to increase another one.
Therefore for soft labels, we need to handle the bias-variance tradeoff in a sample-wise manner and take the training process into consideration.

To study the bias-variance tradeoff during training, we consider the gradients of bias and variance reduction.
Let  be the logits output of the student on input  and  is -th element of it, then we are interested in . 
For simplifying analysis, we are concerned with the gradients on the ground-truth related logit, that is, the sample  is labeled as -th class.
Mathematically, for the gradients of variance reduction, we have 

where  denotes the -th element of the teacher's prediction, i.e., . The term  is easy to understand when  since the gradient now becomes . Meanwhile, for the bias reduction, we have , so  and  always have different signs, leading to a tradeoff.

If  is much higher than , the bias reduction dominates the overall optimization direction. Instead, if  becomes higher, the sample is used for variance reduction.
Interestingly, we discover that under a fixed distillation temperature, the final performance is worse when more training samples are used for variance reduction, which will be introduced in the next section. 

\subsection{Regularization samples}
\label{sec:samplewise}







\begin{table}
    \caption{We count the number of regularization samples with different distillation settings on CIFAR-100.
The teacher-student network pair is WRN-40-2 \citep{ZagoruykoK17} and WRN-16-2. Results are averaged over 5 repeated runs.
    The temperature column means the temperature for distillation and the label smoothing column means whether the teacher network is trained with label smoothing trick.
    }
    \label{tab:label-smoothing}
    \begin{center}
    \vspace{-5pt}
    \begin{tabular}{cccc}
    \toprule
    \multicolumn{4}{c}{
        \begin{tabular}{rl}
        \multirow{2}{*}{Baseline Top-1 Acc} & 
        Teacher: 76.55 w/ label smoothing, 75.61 w/o label smoothing \\
        & Student: 73.26 \\
        \end{tabular}
    } \\ \midrule
    Temperature & Label smoothing? & Student Top-1 Acc & Number of regularization samples \\ \midrule
    \multirow{2}{*}{}& \xmark & \textbf{74.79} & 15379 \\
    & \cmark & 74.62 & 25235 \\ \cmidrule[0.3pt](lr){1-4}
    \multirow{2}{*}{}& \xmark & \textbf{74.92} & 17709 \\
    & \cmark & 74.59 & 24775 \\ \cmidrule[0.3pt](lr){1-4}
    \multirow{2}{*}{}& \xmark & \textbf{75.10} & 17408 \\
    & \cmark & 74.46 & 24538 \\ 
    \bottomrule
    \end{tabular}
    \end{center}
    \vspace{-10pt}
\end{table}



Our analysis starts with a conclusion from \citet{muller2019does}: \emph{if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective.} 
Inspired by the phenomenon, we gather the impact of bias and variance during training with different distillation settings. 
Let  and , then as introduced before, we use  and  to represent the impact of bias and variance, respectively. 
If we have  for a sample, we name the sample as a \textbf{regularization sample} since the variance dominates the optimization direction.  
From the collected data, we find that the number of regularization samples is closely related to distillation performance. 


In Tab. \ref{tab:label-smoothing}, we present the count of regularization samples for a student network trained by knowledge distillation. 
For distillation with a temperature higher than 1, which is the common setting, we observe that if the teacher network is trained with label smoothing, more samples will be involved in variance reduction. 
Also, distillation from a teacher trained with label smoothing performs worse, which is consistent with \citet{muller2019does}.
Therefore, we conclude that for distillation with soft labels, the regularization samples during training affect the final distillation performance. 

\begin{figure}[t]
    \vspace{-20pt}
    \centering
    \subfloat[][Temperature ]{\includegraphics[width=0.3\textwidth]{fig/num2.pdf}} \quad
    \subfloat[][Temperature ]{\includegraphics[width=0.3\textwidth]{fig/num4.pdf}} \quad
    \subfloat[][Temperature ]{\includegraphics[width=0.3\textwidth]{fig/num5.pdf}}
    \vspace{-3pt}
    \caption{The number of regularization samples with respect to training epochs. The distillation settings are the same as the settings in Tab. \ref{tab:label-smoothing}.}
    \label{fig:num-stage}
\end{figure}

Moreover, we plot the number of regularization samples with respect to different training epochs in Fig. \ref{fig:num-stage}. As demonstrated in the plots, the number of such samples increases much faster when using the teacher trained with label smoothing for distillation. For regularization samples, the gap of their number between with and without label smoothing becomes larger for more training epochs. These observations verify our motivation that the bias-variance tradeoff varies sample-wisely and evolves during the training process.


From the above results, we conclude that bias-variance tradeoff for soft labels varies sample-wisely, therefore the strategy for tuning the tradeoff should also be sample-wise.
In the next section, we set up ad hoc filters for soft labels and further investigate how regularization samples affect distillation.



\vspace{-5pt}
\subsection{How regularization samples affect distillation}
The results presented in the last section suggest that we should avoid training with regularization samples. 
Hence, we design two straightforward solutions and then find that totally filtering out regularization samples deteriorates the distillation performance. 

The first experiment we conduct is to manually resolve the conflicting gradient on the label related logit, as defined in section \ref{sec:samplewise}. Specifically, we apply a mask to the distillation loss  such that  where  is the label. Consequently, the loss for this sample now becomes . 
The motivation behind the masked distillation loss is that we only transfer the knowledge of resemblances among the labels.
Another experiment is to figure out what role in distillation those regularization samples will play. To investigate this, we carry out knowledge distillation on two subsets of samples: 1)  is not valid on regularization samples, and 2)  is valid only on regularization samples. 


\begin{figure}[t]
    \centering
    \vspace{-8pt}
    \includegraphics[width=\textwidth]{fig/label-illu.pdf}
\caption{Computational graph of knowledge distillation with our proposed weighted soft labels.}
    \label{fig:label-illu}
\end{figure}

\begin{table}
    \caption{Study of the impact on distillation for regularization samples. Loss function presented here is for the loss on a specific sample. Results are classification Top-1 accuracy. We follow the settings used in Tab. \ref{tab:label-smoothing} and set . Results are averaged over 5 runs.}
    \label{tab:useful}
\resizebox{\textwidth}{!}{
    \begin{tabular}{ccccc}
        \toprule
        \multicolumn{5}{c}{
Teacher: 75.61; \quad Student with direct training: 73.26; \quad Student with standard KD: 74.92 
        } \\ \midrule
\multirow{2}{*}{Setting} & \multirow{2}{*}{Loss function} & \multirow{2}{*}{Student performance} & \multicolumn{2}{c}{Performance gap} \\ \cmidrule(lr){4-5}
        & & & to direct training & to KD \\ \midrule
        \begin{tabular}{cc}
            Mask KD loss on \\
            the label related logit
        \end{tabular} &  & 73.51 & +0.25 & -1.41 \\ 
        \cmidrule(lr){1-5}
        Excluding regularization samples & 
           
        & 74.59 & +1.33 & -0.33 \\ 
        \cmidrule(lr){1-5}
        Only on regularization samples & 
            
         & 73.86 & +0.60 & -1.06 \\ 
         \bottomrule
    \end{tabular}
    }
    \vspace{-10pt}
\end{table}

The results of the two experiments are presented in Tab. \ref{tab:useful}. We can observe that all of the three approaches are not as good as the baseline knowledge distillation performance, but are better than the direct training baseline. 
First, since masking  loss on the label related logit results in worse performance compared to standard KD, we cannot resolve the tradeoff by applying a mask on the ground truth related logit.
Then, from the second experiment, we can see that filtering out regularization samples deteriorates the distillation performance. Moreover, the result of the third experiment is higher than the direct training baseline, indicating that regularization samples are still valuable for distillation. The above results motivate us to think that regularization samples are not fully exploited by standard KD and we can tune the tradeoff to fulfill the potential of regularization samples.





\vspace{-5pt}
\section{Weighted soft labels}
\vspace{-5pt}
From the last section, we realize that the bias-variance tradeoff varies sample-wisely during training and under fixed distillation settings, the number of regularization samples is negatively associated with the final distillation performance. 
Yet, discarding regularization samples deteriorates distillation performance and distilling knowledge from these samples is better than the direct training baseline. 
The above evidence inspires us to lower the weight of regularization samples. 


Recall that regularization samples are defined by the relative value of  and , we propose to assign importance weight to a sample according to  and . 
However, since  is computed with the hyperparameter temperature,  and  are correlated with the temperature and thus bring difficulty to tuning the hyperparameter. 
To make the weighting scheme independent of the temperature hyperparameter, we compare  and  with temperature . Note that when ,  and , so we compare  and  instead. 
Finally, in the light of previous works that assign sample-wise weights \citep{lin2017focal,tang2019learning}, we propose weighted soft labels for knowledge distillation, which is formally defined as

where  is the ground truth class of the sample.
The above equation means that a weighting factor is assigned to each sample's  according to the predictions of the teacher and the student. 
In this way, if compared to the teacher, a student network is relatively better trained on a sample, we have , then a smaller weight is assigned to this sample.
In Fig. \ref{fig:label-illu}, the whole computational graph of knowledge distillation with the proposed weighted soft labels is demonstrated. 
Finally, we add  and  together to supervise the network, i.e., , where  is a balancing hyperparameter.



\begin{table}
    \caption{Top-1 classification accuracy results on CIFAR-100. Comparison results are quoted from \citet{Tian2020Contrastive}. We report our results over 5 repeated runs.}
    \label{tab:cifar}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{ccccccccc}
    \toprule
    & \multicolumn{5}{c}{Same architecture style} & \multicolumn{3}{c}{Different architecture style} \\ \cmidrule(lr){2-6} \cmidrule(lr){7-9} 
    Teacher & {\scriptsize WRN-40-2} & {\scriptsize resnet56} & {\scriptsize resnet110} & {\scriptsize resnet110} & {\scriptsize resnet32x4} & {\scriptsize resnet32x4} & {\scriptsize resnet32x4} & {\scriptsize WRN-40-2} \\
    Student & {\scriptsize WRN-40-1} & {\scriptsize resnet20} & {\scriptsize resnet20} & {\scriptsize resnet32} & {\scriptsize resnet8x4} & {\scriptsize ShuffleNetV1} & {\scriptsize ShuffleNetV2} & {\scriptsize ShuffleNetV1} \\ \midrule
    Teacher & 75.61 & 72.34 & 74.31 & 74.31 & 79.42 & 79.42 & 79.42 & 75.61\\
    Student & 71.98 & 69.06 & 69.06 & 71.14 & 72.50 & 70.5 & 71.82 & 70.5\\ \midrule
    FitNet & 72.24 & 69.21 & 68.99 & 71.06 & 73.50 & 73.59 & 73.54 & 73.73\\
    AT & 72.77 & 70.55 & 70.22 & 72.31 & 73.44 & 71.73 & 72.73 & 73.32\\
    SP & 72.43 & 69.67 & 70.04 & 72.69 & 72.94 & 73.48 & 74.56 & 74.52\\
    CC & 72.21 & 69.63 & 69.48 & 71.48 & 72.97 & 71.14 & 71.29 & 71.38\\
    VID & 73.30 & 70.38 & 70.16 & 72.61 & 73.09 & 73.38 & 73.40 & 73.61\\
    RKD & 72.22 & 69.61 & 69.25 & 71.82 & 71.90 & 72.28 & 73.21 & 72.21\\
    PKT & 73.45 & 70.34 & 70.25 & 72.61 & 73.64 & 74.10 & 74.69 & 73.89\\
    AB & 72.38 & 69.47 & 69.53 & 70.98 & 73.17 & 73.55 & 74.31 & 73.34\\
    FT & 71.59 & 69.84 & 70.22 & 72.37 & 72.86 & 71.75 & 72.50 & 72.03\\
    FSP & n/a & 69.95 & 70.11 & 71.89 & 72.62 & n/a & n/a & n/a\\
    NST & 72.24 & 69.60 & 69.53 & 71.96 & 73.30 & 74.12 & 74.68 & 74.89\\
    KD & 73.54 & 70.66 & 70.67 & 73.08 & 73.33 & 74.07 & 74.45 & 74.83\\
    CRD & 74.14 & 71.16 & 71.46 & 73.48 & 75.51 & 75.11 & 75.65 & 76.05\\
\midrule
    \textit{Ours} & \textbf{74.48} & \textbf{72.15} & \textbf{72.19} & \textbf{74.12} & \textbf{76.05} & \textbf{75.46} & \textbf{75.93} & \textbf{76.21}\\
    \bottomrule 
    \end{tabular}
    }
    \vspace{-10pt}
\end{table}


\vspace{-5pt}
\section{Experiments}
\vspace{-5pt}
To evaluate our weighted soft labels comprehensively, we first conduct experiments with various teacher-student pair settings on CIFAR-100 \citep{krizhevsky2009learning}. Next, we compare our method with current state-of-the-art distillation methods on ImageNet \citep{deng2009imagenet}. 
To validate the effectiveness of our method in terms of handling the bias-variance tradeoff, we conduct ablation experiments by applying weighted soft labels on different subsets. 


\subsection{Dataset and hyperparameter settings}
The datasets used in our experiments are CIFAR-100 \citep{krizhevsky2009learning} and ImageNet \citep{deng2009imagenet}. CIFAR-100 contains 50K training and 10K test images of size . ImageNet contains 1.2 million training and 50K validation images. Except the loss function, training settings like learning rate or training epochs are the same with \citet{Tian2020Contrastive} for CIFAR-100 and \citet{heo2019comprehensive} for ImageNet. For distillation, we set the temperature  for CIFAR and  for ImageNet. For loss function, we set  for distillation on CIFAR and  for ImageNet via grid search. The teacher network is well-trained previously and fixed during training.

For comparison, the following recent state-of-the-art methods are chosen: FitNet \citep{RomeroBKCGB14}, AT \citep{ZagoruykoK17}, SP \citep{tung2019similarity}, CC \citep{peng2019correlation}, VID \citep{ahn2019variational}, RKD \citep{park2019relational}, PKT \citep{passalis2018learning}, AB \citep{heo2019comprehensive}, FT \citep{kim2018paraphrasing}, FSP \citep{yim2017gift}, NST \citep{huang2017like}, Overhaul \citep{heo2019comprehensive} and CRD \citep{Tian2020Contrastive}.

\subsection{Model compression}
\paragraph{Results on CIFAR-100}
In Tab. \ref{tab:cifar}, we present the Top-1 classification accuracy of our method and comparison methods. The results of comparison methods are quoted from \citet{Tian2020Contrastive}. 
Teacher-student pairs of the same and different architecture styles are considered.
For pairs of same architecture style, we use wide residual networks \citep{ZagoruykoK17} and residual networks \citep{he2016deep}. For pairs of different architecture style, residual networks and ShuffleNet \citep{zhang2018shufflenet} pairs are chosen for experiments.
As shown in the table, for distillation with both same and different architecture style, our method reached new state-of-the-art results. 
Specifically, our method outperforms standard KD by a large margin, which verifies the effectiveness of our method. 


\paragraph{Results on ImageNet}
In Tab. \ref{tab:imagenet}, we compare our method with current SOTA methods on ImageNet. Note that for the ResNet34  ResNet-18 distillation setting, the result of CRD is trained 10 more extra epochs while ours is the same as other methods. 
For ResNet-50  MobileNet-v1 distillation setting, NST and FSP are not chosen for comparison as the two methods require too large GPU memories, so we include the accuracy of FT and AB reported in \citet{heo2019comprehensive} for comparison. 
Our results outperform all the existing methods, verifying the practical value of our method.

\begin{table}
    \caption{Top-1 and Top-5 classification accuracy results on ImageNet validation set. 
All training hyperparameter like learning rate and training epochs are in accordance with \citep{heo2019comprehensive}.}
    \label{tab:imagenet}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{ccc}
        \toprule
        \multicolumn{3}{c}{Teacher: ResNet-34  Student: ResNet-18}\\ \midrule
        Method & Top-1 Acc & Top-5 Acc \\ \midrule
        Teacher & 73.31 & 91.42 \\
        Student & 69.75 & 89.07 \\ \midrule
        KD & 70.67 & 90.04\\
        AT & 71.03 & 90.04\\
        NST & 70.29 & 89.53\\
        FSP & 70.58 & 89.61\\
        RKD & 70.40 & 89.78\\
        Overhaul & 71.03 & 90.15\\
        CRD & 71.17 & 90.13 \\ \midrule
\textit{Ours} & \textbf{72.04} & \textbf{90.70} \\
        \bottomrule
    \end{tabular}
    \quad
    \begin{tabular}{ccc}
        \toprule
        \multicolumn{3}{c}{Teacher: ResNet-50  Student: MobileNet-v1}\\ \midrule
        Method & Top-1 Acc & Top-5 Acc \\ \midrule
        Teacher & 76.16 & 92.87\\
        Student & 68.87 & 88.76\\ \midrule
        KD & 70.49 & 89.92\\
        AT & 70.18 & 89.68\\
        FT & 69.88 & 89.5\\
        AB & 68.89 & 88.71\\
        RKD & 68.50 & 88.32\\
        Overhaul & 71.33 & 90.33\\
        CRD & 69.07 & 88.94 \\ \midrule
\textit{Ours} & \textbf{71.52} & \textbf{90.34} \\
        \bottomrule
    \end{tabular}
    }
    \vspace{-10pt}
\end{table}

\vspace{-5pt}
\subsection{Ablation studies}
\vspace{-5pt}

\begin{wraptable}{r}{0.42\textwidth}
\caption{Performance on different subsets with soft labels and our weighted soft labels. RS means regularization samples. Results are averaged over 5 runs.}
\label{tab:subsets}
\vspace{-10pt}
\begin{center}
\resizebox{0.42\textwidth}{!}{
\begin{tabular}{ccc}
\toprule
Subsets & Standard KD & Weighted \\ \midrule
Only on RS & 73.86 & 74.46 \\ 
Excluding RS & 74.59 & 75.35 \\ 
\bottomrule
\end{tabular}}
\end{center}
\vspace{-10pt}
\end{wraptable}
\paragraph{Weighted soft labels on different subsets.} 
Recall that we propose weighted soft labels for tuning sample-wise bias-variance tradeoff, it is still unclear whether the improvements come from a well-handled sample-wise bias-variance tradeoff. To investigate this issue, we compare the performance gain of weighted soft labels on different training subsets. Similar to the settings used in Tab. \ref{tab:useful}, we apply weighted soft labels on two different subsets: only the regularization samples and excluding regularization samples. 
In Tab. \ref{tab:subsets}, we show the results on subsets of only regularization samples and excluding regularization samples. From the significant improvements, we can see that our method can not only improve performance on the RS subset, the improvements on excluding RS subset is also significant. 
We conclude that weighted soft labels can tune sample-wise bias-variance tradeoff globally and lead to an improved distillation performance.

\begin{wraptable}{r}{0.4\textwidth}
\centering
    \caption{Distillation using weighted soft labels and teacher trained with label smoothing (denoted as \textit{LS?}). Results are averaged over 5 runs.}
    \vspace{-5pt}
    \label{tab:abla-smooth}
    \begin{tabular}{cccc}
        \toprule
         & \textit{LS?} & Standard KD & Weighted \\ \midrule
        4 & \xmark & 74.92 & 75.78 \\
        4 & \cmark & 74.59 & 75.60 \\
        6 & \xmark & 75.10 & 75.74 \\
        6 & \cmark & 74.46 & 75.42 \\
        \bottomrule
    \end{tabular}
    \vspace{-8pt}
\end{wraptable}
\paragraph{Distillation with label smoothing trained teacher.}
Our exploration of bias-variance tradeoff starts with the conclusion made in \citet{muller2019does}: a teacher network trained with the label smoothing trick is less effective for distillation. 
It is worthwhile to study whether the conclusion remains true for distillation with our weighted soft labels. As discussed before, we hold the opinion that too many regularization samples make the distillation less effective. Since our weighted soft label is proposed to mitigate the negative effects of the regularization samples, with the same settings from Tab. \ref{tab:label-smoothing}, we conduct comparison experiments in Tab. \ref{tab:abla-smooth} to see if the negative effects still exist. It is evident that weighted soft labels significantly improve the distillation performance, especially for distillation from the teacher trained with label smoothing. 
Besides, using the teacher trained with label smoothing still performs worse than that without label smoothing, which again verifies the conclusion drawn by \citet{muller2019does}. 



\vspace{-5pt}
\section{Conclusion}
\vspace{-5pt}
Recent studies \citep{muller2019does,yuan2020revisiting} point out that one important reason behind the effectiveness of distillation is the regularization effect brought by being soft. In this paper, we rethink the soft labels for distillation from a bias-variance tradeoff perspective. The tradeoff varies sample-wisely and we propose weighted soft labels to handle the tradeoff, of which the effectiveness is verified with experiments on standard evaluation benchmarks.


\section*{Acknowledgements}
This work is supported in part by a gift grant from Horizon Robotics and National Science Foundation Grant CNS-1951952. We thank Yichen Gong, Chuan Tian, Jiemin Fang and Yuzhu Sun for the discussion and assistance.

\bibliography{distillation}
\bibliographystyle{iclr2021_conference}

\appendix
\section{Appendix}
\subsection{Visualization of the resemblances introduced by soft label regularizer}
In Sec \ref{sec:intuitive}, we propose to study  during the training process. 
When , we show that  equals .
As  is the output from the teacher network and computed by a linear mapping of the activations in the teacher's penultimate layer, the regularization indicates that the student should follow the learned the resemblances between classes \citep{Hinton2015DistillingTK,muller2019does}. 
Still, two questions are unclear: 1) what the resemblances are and 2) whether the regularization still indicates resemblances if  is set to 4, a widely adopted hyperparameter \citep{Tian2020Contrastive}.


\begin{figure}[t]
    \centering
\subfloat[][]{\includegraphics[width=0.3\textwidth]{fig/vgg.png}} \quad
    \subfloat[][]{\includegraphics[width=0.3\textwidth]{fig/res18.png}} \\
    \subfloat[][]{\includegraphics[width=0.3\textwidth]{fig/imagenet-lch-similarity.png}} \quad
    \subfloat[][]{\includegraphics[width=0.3\textwidth]{fig/imagenet-wup-similarity.png}}
    \caption{Visualization of the resemblances introduced by soft label regularizers: (a) VGG-19 (Teacher)  VGG-16 (Student), (b) ResNet-50 (Teacher)  ResNet-18 (Student). And semantic similarity between label names: (c) LCH similarity \citep{pedersen2004wordnet},  (d) WUP similarity \citep{pedersen2004wordnet}. Darker areas denote larger values.}
    \label{fig:resemblances}
\end{figure}

Towards answering the questions, we visualize the value of gradient vector  concerning each class. Specifically, on ImageNet \citep{deng2009imagenet} training set and , we calculate the average value of  for each class. 
Let  be the matrix of values with the -th entry  means averaged  for class . Since , diagonal elements are ignored for visualization. The results are visualized in Fig. \ref{fig:resemblances}. We find that plotting the common correlation matrix heatmap is ambiguous, because the matrix to be visualized is of large size () with a large variance. By treating each entry  as a vertex and then constructing a mesh for the matrix, we apply subdivision \citep{loop1987smooth} to the mesh for smoothing the extreme points and finally rendering the mesh by ray-tracing package \citet{plotopti19}. We can observe the several facts from the figures: 1) Comparing the sub-figure (a) and (b), we can see that for distillation resemblances implied by regularizers are similar across different teacher-student pairs. 2)  Comparing (ab) with (cd), we can see that the resemblances are consistent with the semantic similarity of image class names. 

In a word, for , the variance reduction brought by soft labels still implies resemblances among labels, which are consistent with the semantic distance of class names.
In the next section, we will analyze how bias-variance tradeoff changes when training with soft labels.

\revise{
\subsection{Intermediate states between excluding and only on regularization samples}
To further investigate the phenomenon about regularization samples, we conduct experiments to show the intermediate states between excluding and only on regularization samples. Two settings are considered here: First, we gradually exclude regularization samples during training, from excluding all regularization samples to excluding 25\% regularization samples; Second, we keep all regularization samples and then gradually add non-regularization samples. Since we judge a sample is regularization or not according to the training loss, we cannot pre-define a sample set such that a certain percentage samples are kept or dropped. 
Therefore, we propose to conduct these experiments by assigning a probability to whether backward the loss computed with regularization samples. For example, if during training, a sample is marked as regularization sample according to the value of  and , we backward the loss of this sample by a probability . In this way, we can get the performance of excluding 75\% regularization samples. In Tab. \ref{tab:intermediate}, we first present result with KD in (a) and then present result with weighted soft labels applied in (b). We can observe that weighted soft labels are indeed balancing the sample-wise, not on dataset scale, bias and variance.
}

\begin{table}
\caption{Intermediate states between excluding and only on regularization samples}\label{tab:intermediate}
\begin{center}
(a) CIFAR100 (WRN-40-2WRN-40-1 with KD)\\
\begin{tabular}{cccccc}
\toprule
\multicolumn{6}{c}{Percentage of excluded regularization samples.}
\\
&100\% & 75\% & 50\% & 25\% &\\ \midrule
\qquad\qquad&74.59 & 74.63 & 74.72 & 74.87 &\\
\toprule
\multicolumn{6}{c}{Percentage of adding non-regularization samples}
\\
&0\% & 25\% & 50\% & 75\% &\\ \midrule
\qquad\qquad&73.86 & 74.12 & 74.47 & 74.71 &\\
\bottomrule
\end{tabular} \\ ~\\~\\
(b) CIFAR100 (WRN-40-2WRN-40-1 with weighted soft labels)\\
\begin{tabular}{cccccc}
\toprule
\multicolumn{6}{c}{Percentage of excluded regularization samples}
\\
&100\% & 75\% & 50\% & 25\% &\\ \midrule
\qquad\qquad&75.35 & 75.48 & 75.61 & 75.72 &\\
\toprule
\multicolumn{6}{c}{Percentage of adding non-regularization samples}
\\
&0\% & 25\% & 50\% & 75\% &\\ \midrule
\qquad\qquad&74.46 & 74.79 & 75.18 & 75.53 &\\
\bottomrule
\end{tabular} 
\end{center}
\end{table}

\revise{
\subsection{Combining with RKD \citep{park2019relational}.} 
To investigate how the weighted soft labels can be applied to the variants of KD, we conduct an experiment of combining RKD \citep{park2019relational} with our weighted soft labels.
Relational knowledge distillation measures the L2 distance of features between two samples or the angle formed by three samples as knowledge to transfer. In other words, the knowledge in RKD is measured by the relations between sample pairs. It is no longer sample-independent, which is different from the weighted soft labels applied to KD which can assign the weights sample-wisely. We currently take the averaged weighting factors of the involved sample pairs when calculating the distance/angle matrix. The results on CIFAR-100 are presented in Tab. \ref{tab:rkd} (averaged over 5 runs). As can be observed from the table, the weighted soft label applied to RKD still brings improvements, though not that big compared with WSL applied to KD. 
Also, we believe that it is an important future direction to explore the applications to more variants of KD.

\begin{table}
\caption{Combining weighted soft labels with RKD \citep{park2019relational}.}
\label{tab:rkd}
\resizebox{\textwidth}{!}{
\begin{tabular}{cccc}
\toprule
Distillation settings & WRN-40-2  WRN-16-2 & WRN-40-2  WRN-40-1 & resnet56  resnet20 \\
\midrule
Teacher  & 75.61 & 75.61 & 72.34 \\
Student  & 73.26 & 71.98 & 69.06 \\
RDK  & 74.12 & 73.34 & 70.25 \\
WSL + RKD  & 74.65 & 73.89 & 70.73 \\
\bottomrule
\end{tabular}
}
\end{table}
}

\revise{
\subsection{Other variants of weighting.} 
In the work, the weighting scheme is defined as , which is inspired by \citet{lin2017focal,tang2019learning}. The basic idea is to convert  into a value in , so that the weights of regularization samples are lower than those non-regularization samples. A straightforward baseline is that we can use the Sigmoid function to convert  into a value in . Note that   is always bigger than 1, so the weight needs scaling and can be defined as . In Tab. \ref{tab:sigmoid}, we present the comparison between adopted weighting form and the Sigmoid baseline. 
We can see that as long as we can adaptively tune the sample-wise bias-variance tradeoff, the performance is better than KD, i.e., without weighted soft labels.Therefore, although the proposed weighting form is not mathematically optimal, the not-too-big or not-too-small weights for these regularization examples are not hard to tune. These results verify our main contribution that there is sample-wise bias-variance tradeoff and we need to assign weights to the regularization examples. 
}

\begin{table}
\caption{Comparison to other weighting forms. (Setting: CIFAR100, WRN-40-2WRN-40-1)}\label{tab:sigmoid}
\begin{center}
\begin{tabular}{cccc}
\toprule
&2.0&3.0&4.0\\ \midrule
Sigmoid baseline&74.13&73.97&73.29\\
Ours&74.38&74.12&73.46\\\bottomrule
\end{tabular}
\end{center}
\end{table}

\revise{
\subsection{Ablation on .} In Tab. \ref{tab:alpha} We first tune the value of  on CIFAR100, with four values  tested. Then we test with three values in  in (b). Finally, we tune  on ImageNet in (c). As a conclusion, the results are not very sensitive to  and the cost of searching  in our work is not expensive.
}

\begin{table}
\caption{Ablation on .}\label{tab:alpha}
\begin{center}
(a) CIFAR100 (WRN-40-2WRN-40-1)\\
\begin{tabular}{ccccc}
\toprule
&1&2&3&4\\ \midrule
Top1&73.67&74.38&74.12&73.46\\\bottomrule
\end{tabular}\\ ~\\ ~\\
(b) CIFAR100 (WRN-40-2WRN-40-1)\\
\begin{tabular}{cccc}
\toprule
&2.25&2.5&2.75\\ \midrule
Top1&74.48&74.34&74.21\\\bottomrule
\end{tabular}\\ ~\\ ~\\
(c) ImageNet (ResNet-34ResNet-18)\\
\begin{tabular}{cccc}
\toprule
&2&2.25&2.5\\ \midrule
Top1&71.91&71.96&72.04\\\bottomrule
\end{tabular}
\end{center}
\end{table}

\begin{table}
\caption{Results on MultiNLI.}\label{tab:nlp}
\resizebox{\textwidth}{!}{
\begin{tabular}{ccccc}
\toprule
& Teacher (BERT-12) & Student (BERT-3) & KD (BERT-3) & Ours (BERT-3) \\ \midrule
Results reported by \citet{sun2019patient} & 83.7 & 74.8 & 75.4 & -\\
Our replications & 83.57 & 75.06 & 75.50 & 76.28 \\\bottomrule
\end{tabular}
}
\end{table}


\revise{
\subsection{Results on MultiNLI}
To further validate our method, we conduct experiments on an NLP dataset MultiNLI \citep{N18}. In this setting, the teacher is BERT-base-cased with 12 layers, 768 Hidden and 108M params. The student is T3 with 3 layers, 768 Hidden and 44M params. Besides, we follow the training setting in \citet{sun2019patient}. In Tab. \ref{tab:nlp}, we present the result comparisons of standard KD and our weighted soft labels. 
}



\end{document}
