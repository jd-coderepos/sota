

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{microtype}
\usepackage{subfigure}
\usepackage{textcomp}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage[normalem]{ulem}
\usepackage{times}
\usepackage{ulem}
\usepackage{url}
\usepackage[hang,flushmargin]{footmisc}  \setenumerate[1]{leftmargin=*}         \setitemize[1]{leftmargin=*}           






\aclfinalcopy 




\newcommand\BibTeX{B\textsc{ib}\TeX}
\newcommand{\textsec}[1]{\textsection\ref{#1}}
\newcommand{\LN}{\linebreak\noindent}


\title{Transformers to Learn Hierarchical Contexts in Multiparty Dialogue\\for Span-based Question Answering}

\author{Changmao Li \\
  Department of Computer Science \\
  Emory University \\
  Atlanta, GA, USA \\
  \texttt{changmao.li@emory.edu} \\\And
  Jinho D. Choi\\
  Department of Computer Science \\
  Emory University \\
  Atlanta, GA, USA \\
  \texttt{jinho.choi@emory.edu} \\}
\date{}
\begin{document}
\maketitle

\begin{abstract}
We introduce a novel approach to transformers that learns hierarchical representations in multiparty dialogue.
First, three language modeling tasks are used to pre-train the transformers, token- and utterance-level language modeling and utterance order prediction, that learn both token and utterance embeddings for better understanding in dialogue contexts.
Then, multi-task learning between the utterance prediction and the token span prediction is applied to fine-tune for span-based question answering (QA).
Our approach is evaluated on the \textsc{FriendsQA} dataset and shows improvements of 3.8\% and 1.4\% over the two state-of-the-art transformer models, \texttt{BERT} and \texttt{RoBERTa}, respectively.

\end{abstract} \section{Introduction}
\label{sec:introduction}

Transformer-based contextualized embedding approaches such as \texttt{BERT} \cite{devlin_2019}, \texttt{XLM} \cite{lample_2019}, \texttt{XLNet} \cite{yang_2019a}, \texttt{RoBERTa} \cite{liu_2019}, and \texttt{AlBERT} \cite{lan_2019} have re-established the state-of-the-art for practically all question answering (QA) tasks on not only general domain datasets such as \textsc{SQuAD} \cite{Rajpurkar_2016,Rajpurkar_2018}, \textsc{MS Marco} \cite{bajaj_2016}, \textsc{TriviaQA} \cite{Joshi_2017}, \textsc{NewsQA} \cite{Trischler_2017}, or \textsc{NarrativeQA} \cite{Ko_isk__2018}, but also multi-turn question datasets such as \textsc{SQA} \cite{iyyer_2017}, \textsc{QuAC} \cite{Choi_2018}, \textsc{CoQA} \cite{Reddy_2019}, or CQA \cite{Talmor_2018}.
However, for span-based QA where the evidence documents are in the form of multiparty dialogue, the performance is still poor even with the latest transformer models \cite{Sun_2019,yang_2019}  due to the challenges in representing utterances composed by heterogeneous speakers.


Several limitations can be expected for language models trained on general domains to process dialogue.
First, most of these models are pre-trained on formal writing, which is notably different from colloquial writing in dialogue; thus, fine-tuning for the end tasks is often not sufficient enough to build robust dialogue models.
Second, unlike sentences in a wiki or news article written by one author with a coherent topic, utterances in a dialogue are from multiple speakers who may talk about different topics in distinct manners such that they should not be represented by simply concatenating, but rather as sub-documents interconnected to one another.




This paper presents a novel approach to the latest transformers that learns hierarchical embeddings for tokens and utterances for a better understanding in dialogue contexts.
While fine-tuning for span-based QA, every utterance as well as the question are separated encoded and multi-head attentions and additional transformers are built on the token and utterance embeddings respectively to provide a more comprehensive view of the dialogue to the QA model.
As a result, our model achieves a new state-of-the-art result on a span-based QA task where the evidence documents are multiparty dialogue.
The contributions of this paper are:\footnote{All our resources including the source codes and the dataset with the experiment split are available at\\\url{https://github.com/emorynlp/friendsqa}}



\begin{itemize}[itemsep=0.1em]
\item New pre-training tasks are introduced to improve the quality of both token-level and utterance-level embeddings generated by the transformers, that better suit to handle dialogue contexts (\textsec{ssec:pretraining}). 
\item A new multi-task learning approach is proposed to fine-tune the language model for span-based QA that takes full advantage of the hierarchical embeddings created from the pre-training (\textsec{ssec:finetuning}).
\item Our approach significantly outperforms the previous state-of-the-art models using \texttt{BERT} and \texttt{RoBERTa} on a span-based QA task using dialogues as evidence documents  (\textsec{sec:experiments}).

\end{itemize}
 \begin{figure*}[htbp!]
\centering

\subfigure[Token-level MLM (\textsec{sssec:pretraining-1})]
{\label{fig:pretraining-1}\includegraphics[scale=0.35]{img/pretraining-1.pdf}}
~~~~~~~~~~~~~~~~~
\subfigure[Utterance-level MLM (\textsec{sssec:pretraining-2})]
{\label{fig:pretraining-2}\includegraphics[scale=0.35]{img/pretraining-2.pdf}}
\\
\subfigure[Utterance order prediction (\textsec{sssec:pretraining-3})]
{\label{fig:pretraining-3}\includegraphics[scale=0.35]{img/pretraining-3.pdf}}

\caption{The overview of our models for the three pre-training tasks (Section~\ref{ssec:pretraining}).}
\label{fig:pretraining}
\end{figure*}



\section{Transformers for Learning Dialogue}
\label{sec:approach}

This section introduces a novel approach for pre-training (Section~\ref{ssec:pretraining}) and fine-tuning (Section~\ref{ssec:finetuning}) transformers to effectively learn dialogue contexts.
Our approach has been evaluated with two kinds of transformers, BERT \cite{devlin_2019} and RoBERTa \cite{liu_2019}, and shown significant improvement to a question answering task (QA) on multiparty dialogue (Section~\ref{sec:experiments}).



\subsection{Pre-training Language Models}
\label{ssec:pretraining}

Pre-training involves 3 tasks in sequence, the token-level masked language modeling (MLM; \textsec{sssec:pretraining-1}), the utterance-level MLM (\textsec{sssec:pretraining-2}), and the utterance order prediction (\textsec{sssec:pretraining-3}), where the trained weights from each task are transferred to the next task.
Note that the weights of publicly available transformer encoders are adapted to train the token-level MLM, which allows our QA model to handle languages in both dialogues, used as evidence documents, and questions written in formal writing. 
Transformers from BERT and RoBERTa are trained with static and dynamic MLM respectively, as described by \citet{devlin_2019,liu_2019}.



\subsubsection{Token-level Masked LM}
\label{sssec:pretraining-1}

Figure~\ref{fig:pretraining-1} illustrates the token-level MLM model.
Let  be a dialogue where  is the 'th utterance in ,  is the speaker of , and  is the 'th token in .
All speakers and tokens in  are appended in order with the special token \texttt{CLS}, representing the entire dialogue, which creates the input string sequence .
For every , let , where  is the masked token substituted in place of .
 is then fed into the transformer encoder (\texttt{TE}), which generates a sequence of embeddings  where  is the embedding list for , and  are the embeddings of  respectively.
Finally,  is fed into a softmax layer that generates the output vector  to predict , where  is the set of all vocabularies in the dataset.\footnote{: the maximum number of words in every utterance,\\: the maximum number of utterances in every dialogue.}





\begin{figure*}[htbp!]
\centering
\includegraphics[scale=0.35]{img/finetuning.pdf}
\caption{The overview of our fine-tuning model exploiting multi-task learning (Section~\ref{ssec:finetuning}).}
\label{fig:finetuing}
\end{figure*}

\subsubsection{Utterance-level Masked LM}
\label{sssec:pretraining-2}

The token-level MLM (t-MLM) learns attentions among all tokens in  regardless of the utterance boundaries, allowing the model to compare every token to a broad context; however, it fails to catch unique aspects about individual utterances that can be important in dialogue.
To learn an embedding for each utterance, the utterance-level MLM model is trained (Figure~\ref{fig:pretraining-2}).
Utterance embeddings can be used independently and/or in sequence to match contexts in the question and the dialogue beyond the token-level, showing an advantage in finding utterances with the correct answer spans (\textsec{sssec:finetuning-1}).

\noindent For every utterance , the masked input sequence  is generated.
Note that \texttt{CLS} now represents  instead of  and  is much shorter than the one used for t-MLM.
 is fed into \texttt{TE}, already trained by t-MLM, and the embedding sequence  is generated.
Finally, , instead of , is fed into a softmax layer that generates  to predict .
The intuition behind the utterance-level MLM is that once  learns enough contents to accurately predict any token in , it consists of most essential features about the utterance; thus,  can be used as the embedding of .





\subsubsection{Utterance Order Prediction}
\label{sssec:pretraining-3}

The embedding  from the utterance-level MLM (u-MLM) learns contents within , but not across other utterances.
In dialogue, it is often the case that a context is completed by multiple utterances; thus, learning attentions among the utterances is necessary.
To create embeddings that contain cross-utterance features, the utterance order prediction model is trained (Figure~\ref{fig:pretraining-3}).
Let  where  and  comprise the first and the second halves of the utterances in , respectively.
Also, let  where  contains the same set of utterances as  although the ordering may be different.
The task is whether or not  preserves the same order of utterances as .

For each , the input  is created and fed into \texttt{TE}, already trained by u-MLM, to create the embeddings .
The sequence  is fed into two transformer layers, \texttt{TL1} and \texttt{TL2}, that generate the new utterance embedding list .
Finally,  is fed into a softmax layer that generates  to predict whether or not  is in order.





\subsection{Fine-tuning for QA on Dialogue}
\label{ssec:finetuning}

Fine-tuning exploits multi-task learning between the utterance ID prediction (\textsec{sssec:finetuning-1}) and the token span prediction (\textsec{sssec:finetuning-2}), which allows the model to train both the utterance- and token-level attentions.
The transformer encoder (\texttt{TE}) trained by the utterance order prediction (UOP) is used for both tasks.
Given the question   ( is the 'th token in ) and the dialogue ,  and all  are fed into \texttt{TE} that generates  and  for  and every , respectively.










\subsubsection{Utterance ID Prediction}
\label{sssec:finetuning-1}

The utterance embedding list  is fed into \texttt{TL1} and \texttt{TL2} from UOP  that generate .
 is then fed into a softmax layer that generates  to predict the ID of the utterance containing the answer span if exists; otherwise, the 'th label is predicted, implying that the answer span for  does not exist in .





\subsubsection{Token Span Prediction}
\label{sssec:finetuning-2}

For every , the pair  is fed into the multi-head attention layer, \texttt{MHA}, where  and .
\texttt{MHA} \cite{vaswani_2017} then generates the attended embedding sequences, , where .
Finally, each  is fed into two softmax layers, \texttt{SL} and \texttt{SR}, that generate  and  to predict\LN the leftmost and the rightmost tokens in  respectively, that yield the answer span for .
It is possible that the answer spans are predicted in multiple utterances, in which case, the span from the utterance that has the highest score for the utterance ID prediction is selected, which is more efficient than the typical dynamic programming approach.


 \section{Experiments}
\label{sec:experiments}

\subsection{Corpus}
\label{ssec:corpus}

Despite of all great work in QA, only two datasets are publicly available for machine comprehension that take dialogues as evidence documents.
One is \textsc{Dream} comprising dialogues for language exams with multiple-choice questions \cite{Sun_2019}.\LN
The other is \textsc{FriendsQA} containing transcripts from the TV show \textit{Friends} with\ annotation for span-based question answering \cite{yang_2019}.\LN
Since \textsc{Dream} is for a reading comprehension task that does not need to find the answer contents from the evidence documents, it is not suitable for our approach; thus, \textsc{FriendsQA} is chosen.

Each scene is treated as an independent dialogue in \textsc{FriendsQA}.
\citet{yang_2019} randomly split the corpus to generate training, development, and evaluation sets such that scenes from the same episode can be distributed across those three sets, causing inflated accuracy scores.
Thus, we re-split them by episodes to prevent such inflation.
For fine-tuning (\textsec{ssec:finetuning}), episodes from the first four seasons are used as described in Table~\ref{tbl:dataset-stats}.
For pre-training (\textsec{ssec:pretraining}), all transcripts from Seasons 5-10 are used as an additional training set.

\begin{table}[htbp!]
\centering\small
\begin{tabular}{c||r|r|r||l}
\bf Set & \multicolumn{1}{c|}{\bf D} & \multicolumn{1}{c|}{\bf Q} & \multicolumn{1}{c||}{\bf A} & \multicolumn{1}{c}{\bf E} \\
\hline\hline
Training    & 973 & 9,791 & 16,352 & 1 - 20 \\
Development & 113 & 1,189 &  2,065 & 21 - 22 \\
Evaluation  & 136 & 1,172 &  1,920 & 23 - *  \\
\end{tabular}
\caption{New data split for \texttt{FriendsQA}. D/Q/A: \# of dialogues/questions/answers, E: episode IDs.}
\label{tbl:dataset-stats}
\vspace{-2ex}
\end{table}

\subsection{Models}
\label{ssec:models}

The weights from the \texttt{BERT}\textsubscript{base} and \texttt{RoBERTa}\textsubscript{base} models \cite{devlin_2019,liu_2019} are transferred to all models in our experiments.
Four baseline models, \texttt{BERT}, \texttt{BERT}\textsubscript{pre}, \texttt{RoBERTa}, and \texttt{RoBERTa}\textsubscript{pre}, are built, where all models are fine-tuned on the datasets in Table~\ref{tbl:dataset-stats} and the \texttt{*}\textsubscript{pre} models are pre-trained on the same datasets with the additional training set from Seasons 5-10 (\textsec{ssec:corpus}).
The baseline models are compared to \texttt{BERT}\textsubscript{our} and \texttt{RoBERTA}\textsubscript{our} that are trained by our approach.\footnote{Detailed experimental setup are provided in Appendices.}


\subsection{Results}

Table~\ref{tab:result} shows results achieved by all the models.
Following \citet{yang_2019}, exact matching (EM), span matching (SM), and utterance matching (UM) are used as the evaluation metrics.
Each model is developed three times and their average score as well as the standard deviation are reported. 
The performance of \texttt{RoBERTa*} is generally higher than \texttt{BERT*} although \texttt{RoBERTa}\textsubscript{base} is pre-trained with larger datasets including \textsc{CC-News} \cite{nagel_2016}, \textsc{OpenWebText} \cite{gokaslan_2019}, and \textsc{Stories} \cite{trinh_2018} than \texttt{BERT}\textsubscript{base} such that results from those two types of transformers cannot be directly compared.



\begin{table}[htp!]
\centering\resizebox{\columnwidth}{!}{
\begin{tabular}{l||c|c|c}
\multicolumn{1}{c|}{\bf Model} & \textbf{EM} & \textbf{SM} & \textbf{UM} \\
\hline \hline
\texttt{BERT}                    &         43.3(0.8)  &         59.3(0.6)  &         70.2(0.4) \\ 
\texttt{BERT}\textsubscript{pre} &         45.6(0.9)  &         61.2(0.7)  &         71.3(0.6) \\ 
\texttt{BERT}\textsubscript{our} & \textbf{46.8}(1.3) & \textbf{63.1}(1.1) & \textbf{73.3}(0.7) \\
\hline\hline
\texttt{RoBERTa}                    &         52.6(0.7)  &         68.2(0.3)  &         80.9(0.8) \\ 
\texttt{RoBERTa}\textsubscript{pre} &         52.6(0.7)  &         68.6(0.6)  &         81.7(0.7) \\ 
\texttt{RoBERTa}\textsubscript{our} & \textbf{53.5}(0.7) & \textbf{69.6}(0.8) & \textbf{82.7}(0.5) \\ 
\end{tabular}}
\caption{Accuracies ( standard deviations) achieved by the \texttt{BERT} and \texttt{RoBERTa} models.}
\label{tab:result}
\end{table}





\noindent The \texttt{*}\textsubscript{pre} models show marginal improvement over their base models, implying that pre-training the language models on \textsc{FriendsQA} with the original transformers does not make much impact on this QA task. 
The models using our approach perform noticeably better than the baseline models, showing 3.8\% and 1.4\% improvements on SM from \texttt{BERT} and \texttt{RoBERTa}, respectively.

\begin{table}[htp!]
\centering\resizebox{\columnwidth}{!}{
\begin{tabular}{c|c||c|c|c}
\textbf{Type} &\textbf{Dist.} & \textbf{EM} & \textbf{SM} & \textbf{UM} \\
\hline \hline
\tt Where & 18.16 & 66.1(0.5) & 79.9(0.7) & 89.8(0.7) \\
\tt When  & 13.57 & 63.3(1.3) & 76.4(0.6) & 88.9(1.2) \\
\tt What  & 18.48 & 56.4(1.7) & 74.0(0.5) & 87.7(2.1) \\
\tt Who   & 18.82 & 55.9(0.8) & 66.0(1.7) & 79.9(1.1) \\
\tt How   & 15.32 & 43.2(2.3) & 63.2(2.5) & 79.4(0.7) \\
\tt Why   & 15.65 & 33.3(2.0) & 57.3(0.8) & 69.8(1.8) \\
\end{tabular}}
\caption{Results from the \texttt{RoBERTa}\textsubscript{our} model by different question types.}
\label{tab:question-type}
\end{table}

\noindent Table~\ref{tab:question-type} shows the results achieved by \texttt{RoBERTa}\textsubscript{our} w.r.t.\ question types.
UM drops significantly for \texttt{Why} that often spans out to longer sequences and also requires deeper inferences to answer correctly than the others.
Compared to the baseline models, our models show more well-around performance regardless the question types.\footnote{Question type results for all models are in Appendices.}


\subsection{Ablation Studies}

Table~\ref{tab:ablation} shows the results from ablation studies to analyze the impacts of the individual approaches.
\texttt{BERT}\textsubscript{pre} and \texttt{RoBERTa}\textsubscript{pre} are the same as in Table~\ref{tab:result}, that are the transformer models pre-trained by the token-level masked LM (\textsec{sssec:pretraining-1}) and fine-tuned by the token span prediction (\textsec{sssec:finetuning-2}).
\texttt{BERT}\textsubscript{uid} and \texttt{RoBERTa}\textsubscript{uid} are the models that are pre-trained by the token-level masked LM and jointly fine-tuned by the token span prediction as well as the utterance ID prediction (UID: \textsec{sssec:finetuning-1}).
Given these two types of transformer models, the utterance-level masked LM (ULM: \textsec{sssec:pretraining-2}) and the utterance order prediction (UOP: \textsec{sssec:pretraining-3}) are separately evaluated.

\begin{table}[htp!]
\centering\resizebox{\columnwidth}{!}{
\begin{tabular}{l||c|c|c}
\multicolumn{1}{c||}{\bf Model} & \bf EM & \bf SM & \bf UM  \\
\hline \hline
\texttt{BERT}\textsubscript{pre}  & 45.6(0.9) & 61.2(0.7) & 71.3(0.6) \\
\textsc{ulm}              & 45.7(0.9) & 61.8(0.9) & 71.8(0.5) \\
\textsc{ulmuop}   & 45.6(0.9) & 61.7(0.7) & 71.7(0.6) \\
\hline
\texttt{BERT}\textsubscript{uid}  & 45.7(0.8) & 61.1(0.8) & 71.5(0.5) \\
\textsc{ulm}              & 46.2(1.1) & 62.4(1.2) & 72.5(0.8) \\
\textsc{ulmuop}   & \textbf{46.8}(1.3) & \textbf{63.1}(1.1) & \textbf{73.3}(0.7) \\
\hline \hline
\texttt{RoBERTa}\textsubscript{pre} & 52.6(0.7) & 68.6(0.6) & 81.7(0.7) \\
\textsc{ulm}                & 52.9(0.8) & 68.7(1.1) & 81.7(0.6) \\
\textsc{ulmuop}     & 52.5(0.8) & 68.8(0.5) & 81.9(0.7) \\
\hline
\texttt{RoBERTa}\textsubscript{uid} & 52.8(0.9) & 68.7(0.8) & 81.9(0.5) \\
\textsc{ulm}                & 53.2(0.6) & 69.2(0.7) & 82.4(0.5) \\
\textsc{ulmuop}     & \textbf{53.5}(0.7) & \textbf{69.6}(0.8) & \textbf{82.7}(0.5) \\
\end{tabular}}
\caption{Results for the ablation studies. Note that the \texttt{*}\textsubscript{uid}\textsc{ulm}\textsc{uop} models are equivalent to the \texttt{*}\textsubscript{our} models in Table~\ref{tab:result}, respectively.}
\label{tab:ablation}
\end{table}

\noindent These two dialogue-specific LM approaches, ULM and UOP, give very marginal improvement over the baseline models, that is rather surprising.
However, they show good improvement when combined with UID, implying that pre-training language models may not be enough to enhance the performance by itself but can be effective when it is coupled with an appropriate fine-tuning approach.
Since both ULM and UOP are designed to improve the quality of utterance embeddings, it is expected to improve the accuracy for UID as well. The improvement on UM is indeed encouraging, giving 2\% and 1\% boosts to \texttt{BERT}\textsubscript{pre} and \texttt{RoBERTa}\textsubscript{pre}, respectively and consequently improving the other two metrics.




\subsection{Error Analysis}

As shown in Table~\ref{tab:question-type}, the major errors are from the three types of questions, \texttt{who}, \texttt{how}, and \texttt{why}; thus, we select 100 dialogues associated with those question types that our best model, \texttt{RoBERTa}\textsubscript{our}, incorrectly predicts the answer spans for.
Specific examples are provided in Tables \ref{tab:error_why}, \ref{tab:error_who} and \ref{tab:error_how} (\textsec{sup:error-examples}).\LN
Following \citet{yang_2019a}, errors are grouped into 6 categories, entity resolution, paraphrase and partial match, cross-utterance reasoning, question bias, noise in annotation, and miscellaneous.

\noindent Table~\ref{tab:error_types} shows the errors types and their ratios with respect to the question types.
Two main error types are entity resolution and cross-utterance reasoning.
The entity resolution error happens when many of the same entities are mentioned in multiple utterances. 
This error also occurs when the QA system is asked about a specific person, but predicts wrong people where there are so many people appearing in multiple utterances. 
The cross-utterance reasoning error often happens with the \texttt{why} and \texttt{how} questions where the model relies on pattern matching mostly and predicts the next utterance span of the matched pattern.


\begin{table}[htp!]
\centering\resizebox{\columnwidth}{!}{\begin{tabular}{c||c|c|c}
\bf Error Types & \bf\texttt{Who} & \bf\texttt{How} & \bf\texttt{Why}  \\
\hline \hline
Entity Resolution            & \textbf{34\%} & 23\% & 20\% \\
Paraphrase and Partial Match & 14\% & 14\% & 13\% \\
Cross-Utterance Reasoning    & 25\% & \textbf{28\%} & \textbf{27\%} \\
Question Bias                & 11\% & 13\% & 17\% \\
Noise in Annotation          & 4\%  & 7\%  & 9\%  \\
Miscellaneous                & 12\% & 15\% & 14\%
\end{tabular}}
\caption{Error types and their ratio with respect to the three most challenging question types.}
\label{tab:error_types}
\end{table} \section{Conclusion}
This paper introduces a novel transformer approach that effectively interprets hierarchical contexts in multiparty dialogue by learning utterance embeddings.
Two language modeling approaches are proposed, utterance-level masked LM and utterance order prediction.
Coupled with the joint inference between token span prediction and utterance ID prediction, these two language models significantly outperform two of the state-of-the-art transformer approaches, \texttt{BERT} and \texttt{RoBERTa}, on a span-based QA task called \textit{FriendsQA}   .
We will evaluate our approach on other machine comprehension tasks using dialogues as evidence documents to further verify the generalizability of this work. 






\label{sec:conclusion} \section*{Acknowledgments}

We gratefully acknowledge the support of the AWS Machine Learning Research Awards (MLRA).
Any contents in this material are those of the authors and do not necessarily reflect the views of them. 
\bibliography{acl2020}
\bibliographystyle{acl_natbib}

\cleardoublepage\appendix
\section{Appendices}
\label{sec:supplemental-materials}

\subsection{Experimental Setup}
\label{sup:experimental-setup}

The \texttt{BERT} model and the \texttt{RoBERTa} model use the same configuration. The two models both have 12 hidden transformer layers and 12 attention heads. The hidden size of the model is 768 and the intermediate size in the transformer layers is 3,072. The activation function in the transformer layers is \texttt{gelu}.

\paragraph{Pre-training} 

The batch size of 32 sequences is used for pre-training.
\texttt{Adam} with the learning rate of , , , the \texttt{L2} weight decay of , the learning rate warm up over the first 10\% steps, and the linear decay of the learning rate are used.
A dropout probability of  is applied to all layers.
The cross-entropy is used for the training loss of each task.
For the masked language modeling tasks, the model is trained until the perplexity stops decreasing on the development set. For the other pre-training tasks, the model is trained until both the loss and the accuracy stop decreasing on the development set.

\paragraph{Fine-tuning}

For fine-tuning, the batch size and the optimization approach are the same as the pre-training.
The dropout probability is always kept at . The training loss is the sum of the cross-entropy of two fine-tuning tasks as in \textsec{ssec:finetuning}.

\subsection{Question Types Analysis}
\label{sup:question-type-analysis}

Tables in this section show the results with respect to the question types using all models (Section~\ref{ssec:models}) in the order of performance. 


\begin{table}[htp!]
\centering\resizebox{\columnwidth}{!}{
\begin{tabular}{c|c||c|c|c}
\textbf{Type} &\textbf{Dist.} & \textbf{EM} & \textbf{SM} & \textbf{UM} \\
\hline \hline
\tt Where & 18.16 & 68.3(1.3) & 78.8(1.2) & 89.2(1.5) \\
\tt When  & 13.57 & 63.8(1.6) & 75.2(0.9) & 86.0(1.6) \\
\tt What  & 18.48 & 54.1(0.8) & 72.5(1.5) & 84.0(0.9) \\
\tt Who   & 18.82 & 56.0(1.3) & 66.1(1.3) & 79.4(1.2) \\
\tt How   & 15.32 & 38.1(0.7) & 59.2(1.6) & 77.5(0.7) \\
\tt Why   & 15.65 & 32.0(1.1) & 56.0(1.7) & 68.5(0.8) \\
\end{tabular}}
\caption{Results from \texttt{RoBERTa} by question types.}
\label{tab:types_result_for_RoBERTa}
\end{table}

\begin{table}[htp!]
\centering\resizebox{\columnwidth}{!}{
\begin{tabular}{c|c||c|c|c}
\textbf{Type} &\textbf{Dist.} & \textbf{EM} & \textbf{SM} & \textbf{UM} \\
\hline \hline
\tt Where & 18.16 & 67.1(1.2) & 78.9(0.6) & 89.0(1.1) \\
\tt When  & 13.57 & 62.3(0.7) & 76.3(1.3) & 88.7(0.9) \\
\tt What  & 18.48 & 55.1(0.8) & 73.1(0.8) & 86.7(0.8) \\
\tt Who   & 18.82 & 56.2(1.4) & 64.0(1.7) & 77.1(1.3) \\
\tt How   & 15.32 & 41.2(1.1) & 61.2(1.5) & 79.8(0.7) \\
\tt Why   & 15.65 & 32.4(0.7) & 57.4(0.8) & 69.1(1.4) \\
\end{tabular}}
\caption{Results from \texttt{RoBERTa}\textsubscript{pre} by question types.}
\label{tab:types_result_for_RoBERTa_pre}
\end{table}

\begin{table}[htp!]
\centering\resizebox{\columnwidth}{!}{
\begin{tabular}{c|c||c|c|c}
\textbf{Type} &\textbf{Dist.} & \textbf{EM} & \textbf{SM} & \textbf{UM} \\
\hline \hline
\tt Where & 18.16 & 66.1(0.5) & 79.9(0.7) & 89.8(0.7) \\
\tt When  & 13.57 & 63.3(1.3) & 76.4(0.6) & 88.9(1.2) \\
\tt What  & 18.48 & 56.4(1.7) & 74.0(0.5) & 87.7(2.1) \\
\tt Who   & 18.82 & 55.9(0.8) & 66.0(1.7) & 79.9(1.1) \\
\tt How   & 15.32 & 43.2(2.3) & 63.2(2.5) & 79.4(0.7) \\
\tt Why   & 15.65 & 33.3(2.0) & 57.3(0.8) & 69.8(1.8) \\
\end{tabular}}
\caption{Results from \texttt{RoBERTa}\textsubscript{our} by question types.}
\label{tab:types_result_for_RoBERTa_Our}
\end{table}

\begin{table}[htp!]
\centering\resizebox{\columnwidth}{!}{
\begin{tabular}{c|c||c|c|c}
\textbf{Type} &\textbf{Dist.} & \textbf{EM} & \textbf{SM} & \textbf{UM} \\
\hline \hline
\tt Where & 18.16 & 57.3(0.5) & 70.2(1.3) & 79.4(0.9) \\
\tt When  & 13.57 & 56.1(1.1) & 69.7(1.6) & 78.6(1.7) \\
\tt What  & 18.48 & 45.0(1.4) & 64.4(0.7) & 77.0(1.0) \\
\tt Who   & 18.82 & 46.9(1.1) & 56.2(1.4) & 67.6(1.4) \\
\tt How   & 15.32 & 29.3(0.8) & 48.4(1.2) & 60.9(0.7) \\
\tt Why   & 15.65 & 23.4(1.6) & 46.1(0.9) & 56.4(1.3) \\
\end{tabular}}
\caption{Results from \texttt{BERT} by question types.}
\label{tab:types_result_for_BERT}
\end{table}

\begin{table}[htp!]
\centering\resizebox{\columnwidth}{!}{
\begin{tabular}{c|c||c|c|c}
\textbf{Type} &\textbf{Dist.} & \textbf{EM} & \textbf{SM} & \textbf{UM} \\
\hline \hline
\tt Where & 18.16 & 62.8(1.8) & 72.3(0.8) & 82.1(0.7) \\
\tt When  & 13.57 & 60.7(1.5) & 70.7(1.8) & 80.4(1.1) \\
\tt What  & 18.48 & 43.2(1.3) & 64.3(1.7) & 75.6(1.8) \\
\tt Who   & 18.82 & 47.8(1.1) & 56.9(1.9) & 69.7(0.7) \\
\tt How   & 15.32 & 33.2(1.3) & 48.3(0.6) & 59.8(1.1) \\
\tt Why   & 15.65 & 22.9(1.6) & 46.6(0.7) & 54.9(0.9) \\
\end{tabular}}
\caption{Results from \texttt{BERT}\textsubscript{pre} by question types.}
\label{tab:types_result_for_BERT_pre}
\vspace{-2ex}
\end{table}

\begin{table}[htp!]
\centering\resizebox{\columnwidth}{!}{
\begin{tabular}{c|c||c|c|c}
\textbf{Type} &\textbf{Dist.} & \textbf{EM} & \textbf{SM} & \textbf{UM} \\
\hline \hline
\tt Where & 18.16 & 63.3(1.2) & 72.9(1.7) & 77.0(1.2) \\
\tt When  & 13.57 & 48.4(1.9) & 66.5(0.8) & 79.5(1.5) \\
\tt What  & 18.48 & 52.1(0.7) & 69.2(1.1) & 81.3(0.7) \\
\tt Who   & 18.82 & 51.3(1.1) & 61.9(0.9) & 67.5(0.9) \\
\tt How   & 15.32 & 30.9(0.9) & 52.1(0.7) & 65.4(1.1) \\
\tt Why   & 15.65 & 29.2(1.6) & 53.2(1.3) & 65.7(0.8) \\
\end{tabular}}
\caption{Results from \texttt{BERT}\textsubscript{our} by question types.}
\label{tab:types_result_for_BERT_our}
\end{table}

\subsection{Error Examples}
\label{sup:error-examples}

Each table in this section gives an error example from the excerpt.
The gold answers are indicated by the \uline{solid} underlines whereas the predicted answers are indicated by the \uwave{wavy} underlines. 

\begin{table}[htp!]
\centering\resizebox{\columnwidth}{!}{\begin{tabular}{c|l}
\textbf{Q} & \multicolumn{1}{c}{\textbf{Why is Joey planning a big party?}} \\
\hline\hline
J & Oh, \uwave{we're having a big party tomorrow night.} Later! \\
R & Whoa! Hey-hey, you planning on inviting us? \\
J & Nooo, later. \\
P & Hey!! Get your ass back here, Tribbiani!! \\
R & Hormones! \\
M & What Phoebe meant to say was umm, how come \\
  & you're having a party and we're not invited? \\
J & Oh, \uline{it's Ross' bachelor party.} \\
M & Sooo? \\
\end{tabular}}
\caption{An error example for the \texttt{why} question (Q).\\J: Joey, R: Rachel, P: Pheobe, M: Monica.}
\label{tab:error_why}
\end{table}


\begin{table}[htp!]
\centering\resizebox{\columnwidth}{!}{\begin{tabular}{c|l}
\textbf{Q} & \multicolumn{1}{c}{\textbf{Who opened the vent?}} \\
\hline\hline
\uline{R} & Ok, got the vent open. \\
P & Hi, I'm Ben. I'm hospital worker Ben. \\
  & It's Ben... to the rescue! \\
R & Ben, you ready? All right, gimme your foot. \\
  & Ok, on three, Ben. One, two, three. Ok, That's it, Ben. \\
- & (\textit{Ross and Susan lift Phoebe up into the vent}.) \\
S & What do you see? \\
P & Well, Susan, I see what appears to be a dark vent. \\
  & Wait. Yes, it is in fact a dark vent. \\
- & (\textit{\uwave{A janitor} opens the closet door from the outside.}) \\
\end{tabular}}
\caption{An error example for the \texttt{who} question (Q).\\P: Pheobe, R: Ross, S: Susan.}
\label{tab:error_who}
\end{table}


\begin{table}[htp!]
\centering\resizebox{\columnwidth}{!}{\begin{tabular}{c|l}
\multirow{2}{*}{\bf Q} & \multicolumn{1}{c}{\textbf{How does Joey try to convince the girl }} \\
                       & \multicolumn{1}{c}{\textbf{to hang out with him?}} \\
\hline\hline
J & Oh yeah-yeah. And I got the duck totally trained. \\
  & Watch this. Stare at the wall. Hardly move. Be white. \\
G & You are really good at that. \\
  & So uh, I had fun tonight, you throw one hell of a party. \\
J & Oh thanks. Thanks. It was great meetin' ya. And listen \\
  & if any of my friends gets married, or have a birthday, ... \\
G & Yeah, that would be great. So I guess umm, good night. \\
J & \uwave{Oh unless you uh, you wanna hang around.} \\
G & Yeah? \\
J & Yeah. \uline{I'll let you play with my duck.} \\
\end{tabular}}
\caption{An error example for the \texttt{how} question (Q).\\J: Joey, G: The Girl.}
\label{tab:error_how}
\end{table} 
\end{document}
