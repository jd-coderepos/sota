\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[pagenumbers]{cvpr}      

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{lipsum}
\usepackage{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{multirow}

\makeatletter
\newcommand\fs@norules{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@ruled
  \def\@fs@pre{}\def\@fs@post{}\def\@fs@mid{\kern3pt}\let\@fs@iftopcapt\iftrue}
\makeatother
\restylefloat{algorithm}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\begin{document}

\title{R-Pred: Two-Stage Motion Prediction Via Tube-Query Attention-Based Trajectory Refinement}


\author{{Sehwan Choi
\qquad
Jungho Kim
\qquad
Junyong Yun
\qquad
Jun Won Choi}\\
Hanyang University and Qualcomm 
\and
{\tt\small \{sehwanchoi, jhkim, jyyun\}@spa.hanyang.ac.kr} \and {\tt\small junwchoi@hanyang.ac.kr}
}

\maketitle


\begin{abstract}
 Predicting the future motion of dynamic agents is of paramount importance to ensuring safety and assessing risks in motion planning for autonomous robots. In this study, we propose a two-stage motion prediction method, called R-Pred, designed to effectively utilize both scene and interaction context using a cascade  of the initial trajectory proposal and trajectory refinement networks. The  initial trajectory proposal network produces   trajectory proposals corresponding to the  modes of the future trajectory distribution. The trajectory refinement network enhances each of the  proposals  using 1) tube-query scene attention (TQSA) and 2) proposal-level interaction attention (PIA) mechanisms. TQSA uses tube-queries to aggregate local scene context features pooled from proximity around trajectory proposals of interest. PIA further enhances the trajectory proposals by modeling inter-agent interactions using a group of trajectory proposals selected by their distances from neighboring agents. Our experiments conducted on Argoverse and nuScenes datasets demonstrate that the proposed refinement network provides significant performance improvements compared to the single-stage baseline and that R-Pred achieves state-of-the-art performance in some categories of the benchmarks. 

 
\end{abstract}

\section{Introduction}
\label{sec:intro}
In autonomous vehicles and robotics applications, dynamic objects move in complex environments while avoiding collisions with other agents. Each dynamic agent plans its motion by predicting the future motion and behavior of other agents around it.  Motion prediction refers to the task of predicting the future trajectory of  dynamic agents based on their past trajectory history and information on the surrounding environment.
The task of predicting motion is challenging because the trajectory of an agent is affected by a variety of contextual factors, which must be taken into account when modeling motion.
In the context of autonomous vehicles, examples of such factors include permissible roads, lanes, traffic signals, blinker states, interactions with other agents, and so forth. The difficulty of predicting future motion also arises from the fact that the distribution of future trajectories tends to be multi-modal.   In a given scene, a target agent can choose one of several distinct maneuvers such as changing lanes, turning left, turning right, or continuing straight ahead. Accordingly, prediction models should be able to generate one or more plausible future trajectories with probabilities.

\begin{figure}[t]
\centering  
\includegraphics[width=8cm]{intro_11.pdf}
\caption{\textbf{Key concept of R-Pred.} R-Pred performs two stages of trajectory prediction. ITPNet produces  trajectory proposals with the corresponding proposal features and TRNet refines each trajectory proposal using separate networks. ITPNet uses a scene context acquired from a relatively large area, whereas TRNet uses a local scene context that exists in a tube-shaped area.  TRNet also refines the trajectory proposal  using an inter-agent interaction context represented at a proposal level.}
\label{intro}  
\end{figure}

Recently, deep neural networks have been developed as a new paradigm in trajectory prediction, and have achieved considerable improvements in performance compared to traditional prediction models through data-driven modeling of trajectory data.  Sequence modeling networks such as {\it long short term memory} (LSTM) \cite{LSTM} or {\it gated recurrent unit} (GRU) \cite{GRU} architectures have been shown to be effective in representing sequential trajectory data \cite{LSTM_bdkim, LaPred}. Trajectory prediction task has been successfully performed using encoder-decoder architectures  \cite{seq2seq, TNT, LaneGCN, mmTransformer, THOMAS, DenseTNT, LTP, HiVT, multipath++, MHA-JAM, LaPred, gohome, autobot, PGP, convolutional-social-pooling, social-GAN, trajectron, trajectron++, desire, diverse-addmissible}.
Recently, the accuracy of trajectory prediction has been rapidly improved by utilizing various sources of contextual information available for prediction.  Numerous methods have jointly modeled the trajectories of multiple neighboring agents  to account for their interactions, including  social-LSTM \cite{social-lstm}, soft hardwired attention \cite{soft+hardwired-attention}, social GAN \cite{social-GAN}, MATF \cite{matp}, Trajectron \cite{trajectron, trajectron++}, DESIRE \cite{desire}, DATF \cite{diverse-addmissible}, and SoPhie \cite{sophie}. Static scene information around the target agent was also used to generate more physically plausible trajectories.
A scene was represented by a two-dimensional raster image that describes the scene  \cite{uncertainty, multipath, MTP, MHA-JAM, CoverNet, map-adaptive}. 
A vector representation of the scene was also proposed for scene encoding \cite{VectorNet, LaneGCN, TNT, DenseTNT, HiVT, LaPred, THOMAS, Scenetransformer, mmTransformer, PGP, LTP, GroupNet, adap_GNN}. Recently, Transformer models \cite{Transformer} have been used to model the scene and interaction context using the attention mechanism \cite{mmTransformer, Scenetransformer, HiVT, Wayformer, LTP, Agentformer}. 

In this paper, we propose a new two-stage motion prediction framework, referred to as {\it R-Pred}. As shown in Fig. \ref{intro}, the proposed R-Pred architecture consists of two-stage networks: an initial trajectory proposal network (ITPNet) and a trajectory refinement network (TRNet).  The ITPNet produces  {\it initial  trajectory proposals } corresponding to the  modes of the trajectory distribution for a target agent.  TRNet then refines each trajectory proposal using the contexts customized for each proposal.  Using the initial trajectory proposals as a {\it priori} information, TRNet can utilize the scene and interaction contexts in more selective and effective  ways.

TRNet employs the following two refinement sub-modules.  First, we present {\it tube-query scene attention} (TQSA) to utilize the local scene context effectively. Unlike ITPNet, which uses a global scene context, TQSA extracts local scene context features within a tube-shaped region around each trajectory proposal. (See Fig. \ref{intro} for illustration). 
Then, the extracted scene context features are used to enhance the corresponding trajectory proposal through cross-attention mechanism. TQSA allows the important scene context to be used to improve each  trajectory proposal.  
Second, we propose the {\it proposal-level interaction attention} (PIA) mechanism.  PIA  models inter-agent interactions using the trajectory proposals produced for multiple neighboring agents by ITPNet. PIA selects a group of trajectory proposals that have the highest influence on the trajectory proposal of interest using the {\it distance-wise proposal grouping strategy}. The proposal group is used to refine the trajectory of interest through cross-attention. 




In fact, our {\it per-proposal refinement strategy} is motivated by  two-stage object detectors (e.g., Faster RCNN \cite{Faster}), where the initial object proposals are first obtained from the entire convolutional neural network (CNN) features and then local features in the region of interest (RoI) are pooled to refine each object proposal \cite{Fast, Faster, Mask}. Similarly, R-Pred generates the context features based on the initial trajectory proposals produced by ITPNet and uses them to refine each proposal.







By combining these two sub-modules, R-Pred can  generate  refined trajectory outputs. We conducted an experimental evaluation of the proposed approach on the generally used Argoverse \cite{Argoverse} and nuScenes \cite{nuScenes} datasets, and the results demonstrate that the proposed refinement network significantly improved the accuracy of ITPNet baseline.
The results also show that R-Pred achieves the state-of-the-art performance in some categories of official Argoverse and nuScenes leaderboards.

The main contributions are summarized below;

\begin{itemize}
\setlength\itemsep{0.1em}
    \item We propose a novel trajectory refinement network that refines each of   trajectory proposals using the local  scene context and  the proposal-level inter-agent interactions. The per-proposal refinement strategy effectively improves the trajectory predictions obtained by the first-stage network.  
    \item We introduce the concept of global-to-local hierarchical attention to effectively utilize the scene context. Our refinement network  uses a tube-query to gather the scene context from the local region around the proposal trajectory. 
The proposed global-local hierarchical attention mechanism is contrasted with {\it factorized attention} \cite{Scenetransformer, Wayformer}, which iterates attention over different sources of context.
    \item We use trajectory proposals of neighboring agents to model interactions between agents. 
Using the trajectory proposal features reflecting particular intentions of other agents, PIA can better model inter-agent interactions  than the conventional interaction modeling that uses the past trajectory features only.
    \item R-Pred can use any {\it off-the-shelf} single-stage  trajectory prediction network as the initial trajectory proposal network.  The proposed refinement framework can also be readily applied to any trajectory prediction network available. 
    \item The source code used in this work will be released publicly. 
\end{itemize}


\section{Related Works}
\label{sec:related work}

\subsection{Context-Aware Trajectory Prediction}
Numerous methods have improved the performance of trajectory prediction by leveraging the contextual information available for prediction. For example, information on the static scene around a target agent has been utilized as scene context.
Raster-based scene encoding uses 2D raster images to summarize scene information and extract semantic features using convolutional neural networks \cite{MHA-JAM, multipath, CoverNet, home, trajectron++, P2T, AIR2}.
 The advantage of this approach is that different types of scene components can be easily accommodated in a raster image. However, the performance of these methods is limited owing to quantization errors and a lack of receptive fields in the encoding architectures.
Vector-based scene encoding represents scene components using vectors, and encodes their relationships using a graph structure or attention models  \cite{LaneGCN, PGP, trajectron++, social-GAN, trajectron, interaction-hybrid-traffic-graph, VectorNet}.  VectorNet \cite{VectorNet} introduced a hierarchical graph neural network that exploited the spatial locality of road components by vectorized representation. Similarly, LaneGCN \cite{LaneGCN} was proposed as an effective graph convolutional network to represent complicated topology and long-range dependency. Trajectron++ \cite{trajectron++} employed a directed spatio-temporal graph to represent scene context vectors while incorporating agent dynamics and heterogeneous data.

The performance of trajectory prediction has also been improved by taking into account interactions with surrounding  dynamic agents and static environment information. Social pooling methods  pooled the trajectory features of other agents for interaction modeling \cite{social-lstm, desire, social-GAN, sophie, convolutional-social-pooling}. 
Recently, attention mechanisms  have been proposed to exploit the meaningful relationships between dynamic agents and lane segments \cite{LaPred, PRIME, HiVT, PGP, DenseTNT, LaneRCNN, LaneGCN, VectorNet, THOMAS, MHA-JAM, LTP, LaneGCN}. 



\subsection{Transformer-based Trajectory Prediction}

Transformer architectures provide an efficient approach to training an attention mechanism that can capture long-distance dependencies of sequence data. Recently, Transformers have been adopted for trajectory prediction to model context in spatial and temporal domains as well as interactions between agents \cite{Agentformer, LTP, HiVT, mmTransformer, Wayformer, Scenetransformer}. 
mmTransformer \cite{mmTransformer} employed an efficient stacked Transformer that applied cross-attention on  multi-agent and scene contexts one by one.   HiVT \cite{HiVT} summarized the spatio-temporal features of agents using translation-invariant  agent-centric local scene structure. In SceneTransformer \cite{Scenetransformer}, a simple varying form of self-attention was exploited to integrate various features, generating scene-level consistent predictions for all agents jointly. Along these lines, Wayformer \cite{Wayformer} was proposed as a general multi-dimensional attention architecture designed to jointly encode multiple agent trajectories and map data in time, space, and agent dimensions. 

\begin{figure*}[t]
\centering  
\includegraphics[width=15cm]{fig2_rv_v2.pdf}
\caption{\textbf{Overall structure of R-Pred.} Given the past trajectories of the dynamic agents and the scene information,  ITPNet generates  trajectory proposals for each agent. TRNet refines each of the  trajectory proposals of the target agent through TQSA and PIA.  TQSA utilizes local scene context features obtained by {\it tubular region pooling}. PIA captures interaction context using the proposal group found by the {\it distance-wise proposal grouping}. Finally, the  prediction header produces the refined future trajectories based on the joint trajectory features obtained by concatenation of the attention values from TQSA and PIA. }
\label{overall framework}  
\end{figure*}

\section{Problem Setup}

Suppose that the historical trajectory states of  dynamic agents are obtained from the multi-object tracker, where  can vary depending on the scenario. The agent whose future trajectory is to be predicted is referred to as the target agent and the remaining agents as neighbor agents. The past trajectory states over  time steps for the -th agent are given by , where  denotes the current time step and    is the state of the -th agent at the time step . The trajectory state is of the form  consisting of the  position and the agent's semantic property . The  coordinates are represented in the agent-centric reference frame, where the current position of a target agent is the origin and its heading angle is aligned with the positive x-axis.    Similarly, the future trajectory states over  time steps for the -th agent are given by 
.    
We assume that the vector representation of a scene is available along with the trajectory data.  For instance, a lane is represented by a set of points on its centerline, where the   coordinates of each point are expressed in an agent-centric frame. We accommodate different types of scene components by appending an  attribute index  to the  coordinate   as . The set of scene components around the target agent at the time  is expressed as the vector , where the dimension  varies depending on the scene complexity and the region of interest on the map.  


Without loss of generality, let  be the trajectory of the target agent and  be the trajectories of neighboring agents. 
Finally, the goal of the motion prediction task is to estimate the future trajectory of the target agent 
 given the past trajectories of  agents  and the scene information . 

\section{Proposed Trajectory Prediction Network}
\label{sec: methods}

In this section, we present the details of the proposed motion prediction method, R-Pred.



\subsection{Overall Framework}

An overview of the proposed R-Pred framework is presented in Fig. \ref{overall framework}. Our proposed architecture produces  multi-modal trajectory predictions for a target agent over two stages. First, ITPNet produces  trajectory proposals for all  agents present in the scene. Along with the trajectory proposals, ITPNet returns  intermediate features used to produce the trajectory proposals. These features are called {\it proposal features}.
ITPNet follows traditional trajectory prediction methods for scene and interaction encoding; that is, it utilizes the global scene context features extracted from a fixed region around the current position of the agent. In addition, the ITPNet exploits the interaction context derived from the past trajectories of  agents. Second, TRNet refines each of the  trajectory proposals of the target agent using the prior trajectory information generated by the ITPNet.  TRNet employs TQSA and PIA to generate the  context features customized for each proposal.    TQSA extracts the local scene context features cropped from a tube-shaped region around the target trajectory proposal.  PIA generates the inter-agent interaction context features using a group of selected proposal trajectories of  agents. 
The context features generated by TQSA and PIA enhance the proposal features through cross-attention. The final joint trajectory features are obtained by concatenating two attention values from TQSA and PIA. Finally, the {\it prediction head} produces  future trajectory predictions and their confidence scores for each target agent.





\subsection{Initial Trajectory Proposal Network}

Given the past trajectories  of the  agents present in the scene and the scene information , ITPNet produces  trajectory proposals and their confidence scores for each agent.  proposal features are also produced for the refinement step. 
The trajectory proposals,  their confidence scores, and  the proposal features for the -th agent are denoted as , , and , respectively. 







\subsection{Trajectory Refinement Network }
TRNet refines the trajectory proposals  of the target agent using the trajectory proposals , the proposal confidence scores , the proposal features ,  and the scene information   obtained from ITPNet. 




\begin{algorithm}[t]
\caption{Tubular Region Pooling} \label{eq:loc algo}
\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\REQUIRE Trajectory proposal , scene context vectors , scene embedding features , and thresholding radius .
\ENSURE  Local scene context features .
\\
\STATE 
\FOR { to }
\FOR {each } 
\STATE 
\IF {}
\item .add()
\ENDIF
\ENDFOR
\ENDFOR
\STATE
\RETURN 
\end{algorithmic}
\end{algorithm}

\noindent\textbf{Tube-Query Scene Attention.} We construct the set of scene embedding features  by encoding each element of  via a linear projection, i.e., . 
TQSA pools the scene embedding features in a tubular region around the trajectory proposal on the map from . We denote the set of  scene embedding features prepared for the target trajectory proposal  as  .  {\it Tubular region pooling} is efficiently performed to aggregate  scene embedding features   within the search radius  around each waypoint of the trajectory proposal. Note that a collection of search disks centered at all waypoints on the trajectory forms an approximately tubular polygon. See Algorithm \ref{eq:loc algo} for further details.  




TQSA decodes the proposal features  of the target agent by performing cross-attention on the local scene context . Specifically, we use the proposal features  as the query and the local scene context features  as the key  and value, i.e., 

where , , and  are learnable weight matrices and  is the dimension of the embedding vectors. We combine the attention value  and the  proposal features  using the gating function introduced in \cite{HiVT}

where , , and  are learnable matrices,  indicates element-wise product,  indicates the sigmoid function and  denotes a {\it multi-layer perceptron} (MLP). We add {\it layer normalization} \cite{LayerNormalization}, {\it Dropout} \cite{Dropout}, and {\it residual connection} \cite{ResNet} in the middle of the attention process.  

Note that the aforementioned local scene attention process to produce the output of TQSA  is performed for each trajectory proposal (for each  value)  in parallel.

\begin{table*} [ht!] 
    \centering
    \begin{tabular}{c||cccccc}
       \bottomrule[1.5pt]
         &  &  &  &  &  & \textit{\%}\\\hline\hline
LaneRCNN\cite{LaneRCNN}         & 1.69 & 3.69 & 0.90 & 1.45 & 2.15 & 12.3\\   
        LaPred\cite{LaPred}             & 1.93 & 4.33 & 0.91 & 1.50 & 2.13 & 18.0\\
        TNT\cite{TNT}                   & 2.17 & 4.96 & 0.91 & 1.45 & 2.14 & 16.6\\ PRIME\cite{PRIME}               & 1.91 & 3.82 & 1.22 & 1.56 & 2.10 & 11.5\\
HOME\cite{home}                 & 1.72 & 3.73 & 0.92 & 1.36 & -    & 11.3 \\
        LaneGCN\cite{LaneGCN}           & 1.70 & 3.76 & 0.87 & 1.36 & 2.05 & 16.2\\ mmTransformer\cite{mmTransformer}& 1.77 & 4.00 & 0.84 & 1.34 & 2.03 & 15.4\\
        DenseTNT\cite{DenseTNT}         & 1.68 & 3.63 & 0.88 & 1.28 & 1.98 & 12.6\\
        THOMAS\cite{THOMAS}             & 1.67 & 3.59 & 0.94 & 1.44 & 1.97 & 10.3\\
        SceneTransformer\cite{Scenetransformer}& 1.81 & 3.62 & 0.80 & 1.23 & 1.89 & 12.5\\
        LTP\cite{LTP}                   & 1.62 & 3.55 & 0.83 & 1.30 & 1.86 & 14.7\\
HOME+GOHOME\cite{gohome}        & 1.70 & 3.68 & 0.89 & 1.29 & 1.86 & \bf 8.5 \\
        TPCN\cite{TPCN}                 & 1.66 & 3.69 & 0.87 & 1.38 & -    & 15.8\\   HiVT\cite{HiVT}                 & 1.60 & 3.52 & 0.77 & 1.17 & 1.84 & 12.7\\
        MultiPath++\cite{multipath++}   & 1.62 & 3.61 & 0.79 & 1.21 & 1.79 & 13.2\\
        Wayformer\cite{Wayformer}       & 1.64 & 3.67 & 0.77 & 1.16 & \bf 1.74 & 11.9\\        
        \Xhline{0.9pt}
ITPNet baseline                 & 1.62 & 3.57 & 0.79 & 1.21 & 1.86 & 13.1\\
        R-Pred       & \bf 1.58 & \bf 3.47 & \bf 0.76 & \bf 1.12 &  1.77 & 11.6 \\\bottomrule[1.5pt]
\end{tabular}
    \vspace{2pt}
    \caption{Performance comparison on {\it Argoverse test set} in the official leaderboard. The best performed metrics are shown in bold. The "-" symbol means the corresponding metric is unknown, either because the authors have not disclosed it or it was not specified in the leaderboard. Our model achieves the state-of-the-art performance in terms of the , ,  and  metrics.}
        \label{argoverse result}
\end{table*}

\begin{algorithm}[t]
\caption{Distance-Wise Proposal Grouping} \label{alg:dis_algo}
\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\REQUIRE Trajectory proposals , proposal confidence scores ,  proposal features , distance threshold  and confidence threshold . 

\ENSURE  Proposal group set .
\FOR { to }
\FOR { to }
\IF{}
\STATE 
\vspace{0.4em}
\IF{}
\STATE  .append()
\ENDIF
\ENDIF
\ENDFOR
\ENDFOR
\STATE
\RETURN 
\end{algorithmic}
\end{algorithm}

\noindent\textbf{Proposal-level Interaction Attention.} PIA uses a {\it distance-wise proposal grouping} algorithm to find a group of the trajectory proposals for the nearby agents to model their inter-agent interactions. First, among  trajectory proposals from  agents, those with a confidence score below the threshold  are discarded because they are unlikely to occur. Subsequently, for a given -th trajectory proposal  of the target agent, the algorithm selects the trajectory proposals of the nearby agents, whose distance from  is closer than the distance threshold . These are considered the most influential trajectory proposals to use for interaction modeling. A distance between two trajectories  and  is defined by 

For the selected trajectory proposals, the algorithm groups the corresponding proposal features into the proposal feature group .
 The {\it distance-wise proposal grouping} algorithm is summarized in Algorithm \ref{alg:dis_algo}. 
 
 The proposal feature group is used to decode the proposal features  through cross-attention.
 Using  as query and  as key and value, the cross-attention module produces the attention value  similarly to (\ref{eq:CrossAttn}) - (\ref{eq:FFN}).

\noindent\textbf{Multi-modal Prediction Head.} For each trajectory proposal , TRNet generates the final joint trajectory feature   by concatenating the attention value  from TQSA and the attention value  from PIA. The prediction head is then applied to produce the refined trajectories and the confidence scores. The prediction head consists of the regression branch and a classification branch. First, by modeling the trajectory points as multi-variate random vectors with independent Laplace distribution, the regression branch applies an MLP to  to predict the mean and covariance of .
 The predicted mean is denoted as
 and the predicted variance is denoted as 
. Note that the regression branch is applied separately for each mode .
  Second, the classification branch applies another MLP to the concatenation of  to produce the confidence scores  for all modes.


\subsection{Training Details}

The total  loss function  used to train the entire network is given by  

where  and  are the regression loss functions for ITPNet, and TRNet and  and  are the classification loss functions for ITPNet, and TRNet.   We used  in our setup. The negative log-likelihood function for the Laplace distribution is used for  as follows,

 where   is the probability density function of Laplace distribution.  is defined similarly. When evaluating the loss function during training, we adopt a {\it winner-takes-all} strategy \cite{MTP} in which the mode  of the trajectory output that yields the smallest average displacement error is used, i.e.,  . We used the cross entropy loss for the classification losses  and . 
 We trained the entire network end-to-end with random initialization.

\begin{table}[ht]
    \centering
     \resizebox{\columnwidth}{!}{
    \begin{tabular}{c||ccccc}
        \bottomrule[1.5pt]
         &   &  &  &  \\\hline\hline
        MTP\cite{MTP}                       & 2.22 & 4.83 & 1.74 &3.54 \\
        CoverNet\cite{CoverNet}                            & 1.96 & - & 1.48 & - \\
        Trajectron++\cite{trajectron++}               & 1.88 & - & 1.51 & - \\
        MHA-JAM\cite{MHA-JAM}               & 1.81 & 3.72 & 1.24 & 2.21 \\
        MultiPath\cite{multipath}           & 1.78 & 3.62 & 1.55 & 2.93 \\
        CXX\cite{cxx}           & 1.63 & - & 1.29 & - \\
        LaPred\cite{LaPred}                 & 1.53& 3.37 & 1.12 & 2.39 \\P2T\cite{P2T}           & 1.45 & - & 1.16 & - \\THOMAS\cite{THOMAS}                 & 1.33 & - & 1.04 & - \\
        PGP\cite{PGP}                       & 1.27 & 2.47 & 0.94 & 1.55 \\\Xhline{0.9pt}
        ITPNet baseline                    & 1.29 &2.58 & 0.97 & 1.60
        &     \\
        R-Pred                                 & \textbf{1.19} & \textbf{2.28} & \textbf{0.94} & \textbf{1.50} \\\bottomrule[1.5pt]
    \end{tabular}}
    \vspace{0pt}
    \caption{Performance comparison on {\it nuScenes validation set} in the official leaderboard. The "-" symbol means the corresponding metric unknown. Our model achieves state-of-the-art performance on all metrics.}
        \label{nuscene result}
\end{table}

\begin{figure*} [ht]
\centering  
\includegraphics[width=14.5cm,height=7.5cm]{fig3_rv_v4.pdf}
\caption{Qualitative results of R-Pred on {\it Argoverse validation set}. Yellow, green, blue, and red lines represent the history, ground truth, the initial trajectory proposal with the highest score, and the final (refined) trajectory, respectively. The sky blue region denotes tubular regions used for TQSA. These figures present different vehicle motion scenarios such as continuing straight ahead, turning left, slowing down, changing lanes, etc.}
\label{qualitative}  
\end{figure*}

\section{Experiments}
In this section, we describe the experimental setup used to evaluate the performance of the proposed R-Pred, and present both quantitative and qualitative analyses of the behavior of the model.

\noindent{\bf Datasets.} Both Argoverse \cite{Argoverse} and nuScenes \cite{nuScenes} datasets provide dynamic agent trajectories and HD-map in real-world driving scenarios. The Argoverse dataset was collected from two US cities, including Miami and Pittsburgh. Each sample contains  seconds trajectories of tracked vehicles sampled at  Hz. Argoverse dataset  provides HD-map with detailed lane information.  The prediction task is to predict future trajectories for  seconds given past trajectories of  seconds.  The dataset contains  training,  validation, and  test samples. 

The nuScenes dataset was collected in Boston and Singapore. The collected trajectories are  seconds long and are sampled at Hz.  The prediction task is defined as that of predicting future trajectories for  seconds given past trajectories of  seconds. HD-map is also provided along with the trajectory data. The dataset is split into  training,  validation, and  test samples.

\noindent{\bf Metrics.} For the performance evaluation, we adopt widely used performance metrics including {\it average displacement error} (), {\it final displacement error} (), and {\it miss rate} ().  refers to the mean square error compared to the ground truth over the entire time steps, and  is defined as the average displacement error at the endpoint. We evaluate the prediction accuracy with  trajectory predictions using   and , which are the minimum  and  over  predicted trajectories, respectively.  measures the ratio within 2 meters of the endpoint of the best-predicted trajectory and ground truth. We also use {\it brier minimum FDE} (), which measures the value of , where  is corresponding trajectory probability. This imposes a penalty when the probability of the best trajectory is low. 

\noindent{\bf Implementation Details.} We set the threshold  and  of PIA to 0.1 and  meters and  of TQSA to  meters. The embedding size of all features is set to 128 for comparative performance analysis.  
We used existing prediction architectures for the ITPNet. We chose the structure of \cite{HiVT} on Argoverse and that of \cite{PGP} on nuScenes. These models achieved excellent performance on both benchmarks. They are considered ITPNet baselines and are used to measure the performance gain achieved by our refinement network. 
 We trained the proposed model on TiTAN RTX GPU for 64 epochs with 32 batch sizes. The data are batched randomly. For all experiments, we trained the model using AdamW optimizer with an initial learning rate of 5e-4. We used cosine annealing to decay the learning rate and applied dropout with a 0.1 ratio. Data augmentation was not used. 

\begin{table} [ht]
    \centering \resizebox{\columnwidth}{!}{
    \begin{tabular}{ccc||cccc}
   \bottomrule[1.5pt]
        ITPNet &  PIA   & TQSA  &  &  & \textit{\%} \\
\hline\hline
        \checkmark & & & 0.694 & 1.057 & 10.63 \\
        \checkmark & \checkmark & & 0.677 & 0.992 & 9.64  \\\checkmark & \checkmark & \checkmark & \textbf{0.657} & \textbf{0.945} & \textbf{8.69}  \\\bottomrule[1.5pt]
    \end{tabular}}
    \vspace{2.0pt}
    \caption{Ablation study. Contributions of the main components evaluated on the \textit{Argoverse validation set}.}
        \label{ablation} 
\end{table}
\vspace{-5.0pt}

\subsection{Quantitative Results}

Table \ref{argoverse result} presents the performance of R-Pred evaluated on {\it Argoverse test set}. We compare the performance of R-Pred with that of several top ranked models \cite{LaneRCNN,TNT,PRIME,LaneGCN,home,mmTransformer,DenseTNT,THOMAS,Scenetransformer,LTP,gohome,TPCN,multipath++,Wayformer, HiVT}. R-Pred achieves the best prediction accuracy in terms of the , ,  and  metrics and competitive performance in terms of  and .  R-Pred sets a new state-of-the-art performance surpassing the current best methods, Wayformer \cite{Wayformer} and MultiPath++ \cite{multipath++}.  Compared to the ITPNet baseline, the proposed method offers performance improvements of 3.80\% and 7.43\% in terms of , and , respectively. This indicates that our trajectory refinement strategy can effectively improve the reliability and robustness of the initial prediction using ITPNet.


Table \ref{nuscene result} presents the performance of several motion prediction methods on {\it nuScenes validation set}. We compare R-Pred with the top ranked methods \cite{CoverNet, trajectron++, MHA-JAM, multipath, cxx, LaPred, P2T, THOMAS, PGP} on the nuScenes leaderboard.
R-Pred achieves significant performance gains compared with  other prediction methods. R-Pred exhibits the performance gain of 7.75\% and 11.62\% in  and   compared to the ITPNet baseline. These results demonstrate that the proposed two refinement modules have high-quality learning capabilities.

\begin{table} [ht]
 \centering 
    \begin{tabular}{c||ccc}
    \bottomrule[1.5pt]
        & &   & \textit{\%} \\
         \hline\hline
5 & 0.659 & 0.957  & 8.927   \\
        10 & 0.658 & 0.952  & 8.840   \\
        20 & \textbf{0.657} & \textbf{0.945}  & \textbf{8.698}   \\
        30 & 0.657 & 0.946  & 8.714 \\
50 & 0.659 &  0.954  & 8.878    \\
           \bottomrule[1.5pt]
    \end{tabular}
\vspace{-10.0pt}
    \caption{Ablation study. Performance versus   parameter of TQSA evaluated on the \textit{Argoverse validation set}.}
        \label{th_tqsa} 
\end{table}
\vspace{-15.0pt}
\begin{table} [ht]
 \centering 
    \begin{tabular}{c||ccc}
    \bottomrule[1.5pt]
         & &   & \textit{\%} \\
         \hline\hline
        10 & \textbf{0.657} & \textbf{0.945}  & \textbf{8.698}   \\
        20 & 0.658 & 0.947  & 8.730   \\
        30 & 0.659 & 0.949  & 8.811 \\
        40 &0.659 & 0.950  & 8.821    \\
           \bottomrule[1.5pt]
    \end{tabular}
\vspace{-10.0pt}
    \caption{Ablation study. Performance versus  parameter of PIA evaluated on the \textit{Argoverse validation set}.}
        \label{th_pia} 
\end{table}

\subsection{Ablation Study}
We conducted several ablation studies on the {\it Argoverse validation set}. We reduced the training time required to conduct the ablation study by decreasing the feature size from 128 to 64 and increasing the batch size from 32 to 64. The trained model was evaluated on the entire validation set.
 
\noindent{\bf Contribution of Each Module.} Table \ref{ablation} shows the contributions of each module to overall performance achieved by R-Pred. We evaluated performance as we added each component one by one. We consider the following three modules of R-Pred: 1) ITPNet baseline, 2) PIA, and 3) TQSA. When the PIA is added to ITPNet baseline, the  improves by 6.15\% and  improves by 9.31\%. This indicates the effectiveness of  inter-agent interaction modeling by PIA.
When TQSA is added, it achieves the performance gains of 4.74\% and  9.85\% in  and . We observe that proposal refinement using local scene context improves the performance significantly.  Finally, the combination of PIA and TQSA has improved the performance of ITPNet baseline by 10.6\% and 18.25\%, in  and , respectively.

\noindent{\bf Performance Versus  and .} We investigated the impact of the parameters    and  on the performance. Recall that  and  are the distance thresholds used in TQSA and PIA, respectively.  Table \ref{th_tqsa} presents the performance of R-Pred evaluated for several values of .   R-Pred performs best at , and degrades as  becomes larger or smaller than these values. This is likely due to the fact that when the threshold gets too large, a lot of irrelevant scene context can be used for cross-attribution.
Table \ref{th_pia} presents the performance of R-Pred for several values of .  R-Pred achieves the best performance with . Note that increasing the threshold  above 10 does not improve performance. \\
\noindent{\bf Effect of Per-proposal Scene Context Strategy.} One of our key innovations is to use the tube-query for pooling the customized scene context for each proposal. We investigated the benefit of our per-proposal feature pooling method. We compare R-Pred with the baseline that shares the global scene context features for all proposals in the refinement step. 
Table \ref{permodal} shows that our strategy offers 1.76\% and 3.24\% performance gains in  and  over the baseline, which confirms the advantage of the proposed per-proposal feature pooling method.

\begin{table} [ht]
    \centering \resizebox{\columnwidth}{!}{
    \begin{tabular}{cc||ccc}
    \bottomrule[1.5pt]
          \textit{Per-proposal} & \textit{Shared} & \multirow{2}{*}{} & \multirow{2}{*}{} & \multirow{2}{*}{} \\
          \textit{scene context} & \textit{scene context} &  & & \\
         \hline\hline
           & \checkmark & 0.662 & 0.962  & 8.981\\
         \checkmark & & \textbf{0.657} & \textbf{0.945} & \textbf{8.69} \\\bottomrule[1.5pt]
    \end{tabular} }
    \vspace{-5.0pt}
    \caption{Ablation study. Comparison of per-proposal scene context versus shared scene context evaluated on the \textit{Argoverse validation set}.}
        \label{permodal} 
\end{table}

\vspace{-10.0pt}
\subsection{Qualitative Results}

 Fig. \ref{qualitative} shows a visualization of the actual predicted trajectory samples generated by R-Pred using {\it Argoverse validation set}. We visualize the final trajectory with the best  score produced by R-Pred (red line) and the corresponding trajectory proposal from ITPNet (blue line). We also include the ground truth trajectory (green line). We added the tubular area used for TQSA to the figure (sky blue region). We observe that our refinement network reduced the prediction error in the trajectories generated by ITPNet. Note that in the third column, our refinement stage modifies the initial trajectory proposal off-road to the trajectory within the road. More qualitative results are provided in Supplementary Material.




\section{Conclusions}

In this paper, we have proposed a two-stage trajectory prediction method, referred to as R-Pred. We have introduced a novel {\it per proposal trajectory refinement} strategy in which each trajectory proposal generated in the first-stage network is refined using contextual information tailored to the proposal.  TRNet utilized the local scene context captured by pooling scene component features in a tubular region around the trajectory proposal. 
TRNet also uses the inter-agent interaction context inferred from a group of influential trajectory proposals of neighboring agents. 
The results of an experimental evaluation conducted on Argoverse and nuScenes benchmark datasets confirmed that R-Pred significantly outperforms existing methods and achieves state-of-the-art performance in terms of some evaluation metrics. 

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage
\setcounter{section}{0}
\renewcommand\thesection{\Alph{section}}
\section*{\LARGE\selectfont{Supplementary Material}}
\linespread{2.5}

\section{The Detailed Network Architecture}
Fig. \ref{refine_detail} presents the detailed network architecture of our R-Pred. The entire network structure consists of ITPNet, TQSA module, PIA module and {\it prediction head}. TQSA and PIA take the output of ITPNet as an input and perform scene encoding and interaction encoding for trajectory refinement. 

\linespread{1.0}

\section{Additional Ablation Study}
\noindent{\bf Input Formats of TRNet.}  TRNet is applied to  the proposal features for refinement. The trajectory proposals are used only to extract the local scene features and conduct the distance-wise proposal grouping. To verify the advantage of our strategy, we compare our method with the baseline that re-encodes the trajectory proposals through MLP and applies the TRNet to the resulting features. Table \ref{proposal} shows that by using the proposal features for refinement, our strategy outperforms the baseline by 1.87\% and 4.4\% in  and . 
This confirms the benefit of using the proposal features for the refinement network. 


\begin{table} [ht!]
    \centering \resizebox{\columnwidth}{!}{
    \begin{tabular}{cc||ccc}        
    \bottomrule[1.5pt]
          \textit{Proposal} & \textit{Trajectory} & \multirow{2}{*}{} & \multirow{2}{*}{}  & \multirow{2}{*}{} \\         
          \textit{feature} & \textit{re-encoding} &&&\\
         \hline\hline
         & \checkmark & 0.666 & 0.963 & 9.09  \\
         \checkmark &  & \textbf{0.657} & \textbf{0.945} & \textbf{8.69} \\\bottomrule[1.5pt]
    \end{tabular} }
    \vspace{2pt}
    \caption{Ablation study. Comparison between our strategy and the baseline evaluated on the \textit{Argoverse validation set}.}
        \label{proposal} 
\end{table}

\linespread{1.0}

\section{Additional Qualitative Examples}
We visualize additional qualitative examples obtained in diverse interaction scenes. The examples were selected from \textit{Argoverse validation set}. We compare the trajectory samples produced by ITPNet and TRNet to demonstrate the effectiveness of our refinement framework. We present the figures in two columns, where the left figures provide  initial trajectory proposals, shown in blue, and the right figures provide the corresponding refined trajectories from TRNet, shown in red. The ground truth is shown as a green line and the trajectories of other agents are shown as black.

\noindent{\bf Speed Control Scenarios.} In Fig. \ref{joint1},  we consider the scenarios where the target agents slow down or accelerate while interacting with other neighboring agent. 
  In these examples, TRNet produces improved predictions by maintaining an appropriate distance from other agents.
  
\noindent{\bf Overtaking Scenarios.} Fig. \ref{joint2} shows three cases in which the target agents change lanes to overtake another agents. In all cases, TRNet produces trajectory predictions that are closer to the ground truth than ITPNet. 

\noindent{\bf Intersection Scenarios.} Fig. \ref{joint3} shows the scenarios where the target agents interact with the nearby agents at intersections. 
Even if the initial trajectory proposals for two neighboring agents conflict, the trajectories in TRNet will not conflict after refinement.

\noindent{\bf Multi-modal Trajectory Behavior.} Fig. \ref{compare_hivt} shows the multi-modal trajectory samples generated by ITPNet and TRNet.  Some of ITPNet's trajectories do not seem plausible because they fall outside of road boundaries. In contrast, TRNet predicts the trajectories that better fit the scene structures and do not compromise the diversity of trajectory modes.



\begin{figure*} [ht]
\centering  
\includegraphics[width=17cm]{sup_diagram3.pdf}
\caption{\textbf{Detailed architecture of R-Pred model.} }
\label{refine_detail}  
\end{figure*}

\begin{figure*} [ht]
\centering  
\includegraphics[width=12cm]{sup1.pdf}
\caption{\textbf{Visualization of trajectories for several speed control scenarios.} In these scenarios, the target agents slow down or accelerate while interacting with other agents. The proposed refinement framework generates the predictions improved over the initial proposals from ITPNet.}
\label{joint1}  
\end{figure*}


\begin{figure*} [ht]
\centering  
\includegraphics[width=12cm]{sup2.pdf}
\caption{\textbf{Visualization of trajectories for several overtaking scenarios.} In these scenarios, the target agents change lanes to overtake other agents. Considering  proposal-level interactions between the agents, TRNet produces trajectory predictions that are closer to the ground truth than ITPNet. Note that  conflicts between the initial proposals from two neighboring agents are resolved by the proposed refinement framework.
}
\label{joint2}  
\end{figure*}

\begin{figure*} [ht]
\centering  
\includegraphics[width=12cm]{sup3.pdf}
\caption{\textbf{Visualization of trajectories for several intersection scenarios.} The target agents interact with the other agents at interactions. For all cases considered, TRNet produces trajectories that do not collide with other trajectories.}
\label{joint3}  
\end{figure*}

\begin{figure*} [ht]
\centering  
\includegraphics[width=12cm]{sup4.pdf}
\caption{\textbf{Visualization of multi-modal trajectories obtained by ITPNet and TRNet.} The multi-modal trajectories predicted by TRNet mostly conform to the scene structures, whereas some trajectories generated by ITPNet are not physically plausible. 
 }
\label{compare_hivt}  
\end{figure*}



\end{document}