
\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} 

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2023}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}


\usepackage{mathtools}
\usepackage{pifont}         \usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{url}            \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{lipsum}
\usepackage{fancyhdr}       \usepackage{caption}
\usepackage{flushend}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{xcolor}         \usepackage[frozencache]{minted}         \usepackage{tikz}           \usepackage{pifont}         \usepackage{makecell}       \usepackage[export]{adjustbox} \usepackage{dblfloatfix}    \usepackage{placeins}       \usepackage{siunitx}        


\newcommand{\todo}[1]{\textcolor{red}{(Todo: #1)}}
\definecolor{skyblue}{rgb}{0.37,0.87,0.96}

\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\DeclareMathSymbol{\shortminus}{\mathbin}{AMSa}{"39}

\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 




\graphicspath{assets}


\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\eqref#1{equation~\ref{#1}}
\def\Eqref#1{Equation~\ref{#1}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 
\newcommand*{\rom}[1]{\uppercase\expandafter{\romannumeral #1\relax}}

\newcommand{\PE}{\operatorname{PE}}
\newcommand{\eEigval}{\Lambda}
\newcommand{\Eigval}{\boldsymbol{\Lambda}}
\newcommand{\eigval}{\lambda}
\newcommand{\eEigvec}{\Gamma}
\newcommand{\Eigvec}{\boldsymbol{\Gamma}}
\newcommand{\eigvec}{\boldsymbol{\gamma}}
\newcommand{\C}{\mathbb{C}}

\newcommand{\params}{\theta}
\newcommand{\loss}{\ell}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}

\providecommand*\theoremautorefname{Theorem}
\providecommand*\definitionautorefname{Definition}
\providecommand*\propositionautorefname{Proposition}
\providecommand*\conjectureautorefname{Conjecture}
\providecommand*\corollaryautorefname{Corollary}
\providecommand*\lemmaautorefname{Lemma}

\renewcommand{\equationautorefname}{Eq.}
\renewcommand{\figureautorefname}{Fig.}
\newcommand{\algorithmautorefname}{Algorithm}
\renewcommand{\sectionautorefname}{\S}
\renewcommand{\subsectionautorefname}{\S}
\renewcommand{\subsubsectionautorefname}{\S}
\renewcommand{\appendixautorefname}{\S}


\usepackage[capitalize,noabbrev]{cleveref}



\frenchspacing

\icmltitlerunning{Transformers Meet Directed Graphs}


\begin{document}

\twocolumn[
\icmltitle{Transformers Meet Directed Graphs}



\icmlsetsymbol{equal}{*}


\begin{icmlauthorlist}
\icmlauthor{Simon Geisler}{equal,tum}
\icmlauthor{Yujia Li}{dm}
\icmlauthor{Daniel Mankowitz}{dm}
\icmlauthor{Ali Taylan Cemgil}{dm}
\icmlauthor{Stephan G\"unnemann}{tum}
\icmlauthor{Cosmin Paduraru}{dm}
\end{icmlauthorlist}

\icmlaffiliation{tum}{Dept. of Computer Science \& Munich Data Science Institute, Technical University of Munich}
\icmlaffiliation{dm}{Google DeepMind}

\icmlcorrespondingauthor{Simon Geisler}{s.geisler@tum.de}

\icmlkeywords{Transformer, Graphs, Positional Encoding}

\vskip 0.3in
]



\printAffiliationsAndNotice{}  





\begin{abstract}
Transformers were originally proposed as a sequence-to-sequence model for text but have become vital for a wide range of modalities, including images, audio, video, and \emph{undirected} graphs. 
However, transformers for \emph{directed} graphs are a surprisingly underexplored topic, despite their applicability to ubiquitous domains, including source code and logic circuits. 
In this work, we propose two direction- and structure-aware positional encodings for \emph{directed} graphs: (1) the eigenvectors of the Magnetic Laplacian -- a direction-aware generalization of the combinatorial Laplacian; (2) directional random walk encodings. Empirically, we show that the extra directionality information is useful in various downstream tasks, including correctness testing of sorting networks and source code understanding. 
Together with a data-flow-centric graph construction, our model outperforms the prior state of the art on the Open Graph Benchmark Code2 relatively by 14.7\%.\hyperlink{code}{\textsuperscript{3}}
\end{abstract}

\section{Introduction}\label{sec:intro}


Transformers have become a central component in many state-of-the-art machine learning models spanning a wide range of modalities. For example, transformers are used to generate solutions for competitive programming tasks from textual descriptions~\citep{li_competition-level_2022}, for conversational question answering with the popular ChatGPT~\citep{openai_chatgpt_2022}, or to find approximate solutions to combinatorial optimizations problems like the Traveling Salesman Problem~\citep{kool_attention_2019}. 
Transformers have also had success in graph learning tasks, e.g., for predicting the properties of molecules~\citep{min_transformer_2022}. While virtually all prior works focus on undirected graphs, we advocate the use of directed graphs as they are omnipresent, and directedness can rule semantics. Transformers that handle both undirected and directed graphs could become an important building block for many applications. For this, the attention mechanism needs to become aware of the graph structure. For example, prior work modified the attention mechanism to incorporate structural information \citep{ying_transformers_2021} or proposed hybrid architectures that also contain Graph Neural Networks (GNNs) \citep{mialon_graphit_2021, chen_structure-aware_2022}. Another (complementary) option are positional encodings that are used by many, if not most, structure-aware transformers~\citep{min_transformer_2022, muller_attending_2023}.

\textbf{Directional positional encodings.}
Most of the literature for structure-aware positional encodings either uses basic measures like pair-wise shortest path distances~\citep{guo_graphcodebert_2021, ying_transformers_2021} or symmetrizes the graph for principled positional encodings, e.g., based on the combinatorial Laplacian~\citep{dwivedi_generalization_2021,kreuzer_rethinking_2021}. Importantly, symmetrization might discard essential information that determines the semantics of the input. 
For these reasons, we propose two direction-aware positional encodings: (1) the eigenvectors of the Magnetic Laplacian (\autoref{sec:spectral}), which naturally generalizes the well-known combinatorial Laplacian to directed graphs (see \autoref{fig:example_graphs}); and (2) directional random walk encodings  (\autoref{sec:random_walks}) that generalize basic measures like the shortest path distances. We show that our positional encodings are predictive for various distances on graphs (\autoref{sec:playground}) and are useful in downstream tasks. Moreover, our positional encodings can also improve GNNs (see \autoref{fig:sn_gnn}).

\begin{figure}[b]
  \centering
  \makebox[\linewidth][c]{
    
  }
  \caption{First eigenvector of Magnetic Laplacian. Node size encodes the real value and colors the imaginary value.}
  \label{fig:example_graphs}
\end{figure}

\textbf{Motivation for directed graphs.} We make the impact of appropriately modeling inputs via directed graphs explicit for our applications. One example is the correctness prediction of sorting networks (\autoref{sec:sorting_networks}). Sorting networks~\citep{knuth_art_1973} are a certain sorting procedures that can be represented by a fixed sequence of operations. The goal is then to predict whether the sequence is a correct sorting network. Based on the sequence of operations, we can construct a (directed acyclic) data-flow graph that models the dependencies between the operations. Conversely, the topological sorts of this graph correspond to different but semantically equivalent sequences of operations. Considering the potentially large number of topological sorts, directed graphs can drastically reduce the effective input dimensionality (e.g., see \autoref{fig:sorting_batcher_n_topsorts}). Moreover, we show that ignoring the edge directions maps both correct and incorrect sorting networks to the same \emph{undirected} graph, losing critical information.

Interestingly, representing source code as a sequence is the de facto standard~\citep{li_competition-level_2022, feng_codebert_2020, chen_evaluating_2021, openai_chatgpt_2022}. Even graph-based representations of code~\citep{allamanis_learning_2018, hu_open_2020,  cummins_programl_2020, guo_graphcodebert_2021, bieber_library_2022} only enrich sequential source code, e.g., with an Abstract Syntax Tree (AST). However, similar to sorting networks, we can often reorder certain statements without affecting the code's functionality. This motivates us to rethink the graph construction for source code, which not only boosts performance but makes the model invariant w.r.t.\ certain meaningless reorderings of statements (see \autoref{sec:source_code} for details).

\textbf{Contributions:} 
\textbf{[\rom{1}]} We make the connection between sinusoidal positional encodings and the eigenvectors of the Laplacian explicit (\autoref{sec:sinlap}).\;
\textbf{[\rom{2}]} We propose \emph{spectral positional encodings} that also generalize to directed graphs  (\autoref{sec:spectral}).\;
\textbf{[\rom{3}]} We extend random walk positional encodings to directed graphs (\autoref{sec:random_walks}).\;
\textbf{[\rom{4}]} As a plausibility check, we assess the predictiveness of structure-aware positional encodings for a set of graph distances (\autoref{sec:playground}).\;
\textbf{[\rom{5}]} We introduce the task of predicting the correctness of \emph{sorting networks}, a canonical ambiguity-free application where directionality is essential (\autoref{sec:sorting_networks}).\;
\textbf{[\rom{6}]} We quantify the benefits of modeling a sequence of program statements as a directed graph and rethink the graph construction for source code to boost predictive performance and robustness (\autoref{sec:sorting_networks} \& \ref{sec:source_code}).\;
\textbf{[\rom{7}]} We set a new \emph{state of the art} on the OGB Code2 dataset (2.85\% higher F1 score, 14.7\% relatively) for function name prediction (\autoref{sec:source_code}).

\section{Sinusoidal and Laplacian Encodings}
\label{sec:sinlap}

Due to the permutation equivariant attention, one typically introduces domain-specific inductive biases with Positional Encodings (PEs). For example, \citet{vaswani_attention_2017} proposed sinusoidal positional encodings for sequences along with the transformer architecture. It is commonly argued~\citep{bronstein_geometric_2021, dwivedi_generalization_2021} that the eigenvectors of the (combinatorial) Laplacian can be understood as a generalization of the sinusoidal positional encodings (see \autoref{fig:sin_lap_bitmap}) to graphs, due to their relationship via the Graph Fourier Transformation (GFT) and Discrete Fourier Transformation (DFT)~\citep{smith_scientist_1999}. Even though sinusoidal positional encodings capture the direction, eigenvectors of the Laplacian do not. But why is this the case? To understand differences and commonalities, we next contrast sinusoidal encodings, DFT, and Laplacian eigenvectors for a sequence (\autoref{fig:example_graphs:a},\ref{fig:example_graphs:b}).

\textbf{Sequence encodings.} Sinusoidal encodings~\citep{vaswani_attention_2017} form a -dimensional embedding of token 's integer position in the sequence using cosine  and sinus  waves of varying frequencies with . Analogously, the \textbf{DFT} could be used to define positional encodings:

      X_j \coloneqq
      \sum\limits_{v = 0}^{n - 1} x_v \Big[ \underbrace{\cos\Big(v \cdot \frac{2 \pi}{n} j \Big)}_{\PE^{(\text{DFT})}_{v, 2j}} - i \cdot \underbrace{\sin\Big(v \cdot \frac{2 \pi}{n} j \Big)}_{\PE^{(\text{DFT})}_{v, 2j + 1}} \Big]
      
\\
Here  corresponds to signal  in the frequency domain. In contrast to the DFT, sinusoidal encodings (a) sweep the frequencies using a geometric series instead of linear; (b) also contain frequencies below  (longest wavelength is ; and (c) have  components instead of  (i.e.,  in \autoref{eq:dft}).

\begin{figure}[t]
  \centering
  \subfloat[Sinusoidal]{
  \includegraphics[width=0.44\linewidth]{assets/sinusoidal_bitmap_stack.pdf}}
  \hfill{}
  \subfloat[Eigenvec.\  of Laplacian]{
  \includegraphics[width=0.44\linewidth]{assets/laplacian_bitmap_sequence_n100_q0.0_image_real.pdf}}
  \caption{(a) Sinusoidal encodings ( components top and  below) with denominator  and . (b) Lap.\ eigenvec.\ of sequence \autoref{fig:example_graphs:b} of len.\ .
  \label{fig:sin_lap_bitmap}}
  \vspace{-0.1in}
\end{figure}

\textbf{Graphs} generalize sequences to sets of tokens/nodes with arbitrary connections. In a graph , the  edges  represent connections between the  nodes . We use  for node features and  for edge features. We denote the in-degree of node  with  and out-degree with . Alternatively to , we denote the adjacency matrix  and refer with  to the diagonalized degree matrix. Analogously, we describe the symmetrized adjacency matrix  with set of edges  and degree matrix  . In the main part of the paper, we only discuss unweighted graphs; however, our methods naturally generalize to weighted graphs (see \autoref{sec:appendix_weighted}).

\textbf{Eigenvectors of Laplacian.} The Graph Fourier Transformation (GFT) for undirected graphs  can be defined based on the eigendecomposition of the combinatorial Laplacian , with diagonal matrix  of eigenvalues and orthogonal matrix 
of eigenvectors (see \autoref{sec:appendix_graph_fourier_transformation} for details). Similarly to the DFT,  are suitable positional encodings. The unnormalized Laplacian  as well as degree-normalized Laplacian  are defined as:\\
\begin{minipage}{0.37\linewidth}

\end{minipage}
\begin{minipage}{0.62\linewidth}

\end{minipage}
\vspace{3pt}\\
The symmetrization  discards directionality but is required s.t.\  is guaranteed to be symmetric and positive semi-definite. This ensures that  form an orthogonal basis, which entails important properties of the GFT and for PEs (see \autoref{sec:appendix_laplacians_directed_graphs} for a discussion). In the following, we index eigenvalues and eigenvectors by order: . We call  or  the first eigenvalue and  or  the first eigenvector that reflect the lowest frequency.

\textbf{Laplacian vs. DFT.} Two notable differences to the DFT in \autoref{eq:dft} are (1) that the eigenvectors of the Laplacian are real-valued; (2) the eigenvectors are not unique, e.g., due to the \emph{sign ambiguity}. That is, if  is an eigenvector, so is .

\textbf{Cosine Transformation.} A possible set of eigenvectors of the combinatorial Laplacian for a sequence (\autoref{fig:example_graphs:b}) is given by the Cosine Transformation Type \rom{2} \citep{strang_discrete_1999}: 
, where we must choose the same sign per . The  is due to the \emph{sign ambiguity} (2) of the eigenvector and, thus, we cannot distinguish the embedding of, e.g., the first and last node. Note that in traditional applications of the Cosine Transformation, it is possible to identify the first token and fix the sign. 
However, for general graphs, it is not that easy to resolve the sign ambiguity (e.g., multiple sink and source nodes). Thus, in positional encodings, we typically use an arbitrary sign for each ~\citep{dwivedi_generalization_2021}, and are not able to distinguish direction.

\section{Directional Spectral Encodings}
\label{sec:spectral}

The Magnetic Laplacian is a generalization of the combinatorial Laplacian that encodes the direction with complex numbers. We then use its eigenvectors for a structure-aware positional encoding that acknowledges the directedness.

We define the \textbf{Magnetic Laplacian} ~\citep{forman_determinants_1993, shubin_discrete_1994, de_verdiere_magnetic_2013, furutani_graph_2020}

with Hadamard product , element-wise , ,
, and potential . 
Recall,  is the symmetrized degree matrix and  the symmetrized adjacency matrix. The Magnetic Laplacian is a Hermitian matrix since it is equal to its conjugate transpose  and, thus, comes with complex eigenvectors .  \autoref{eq:unsymmetric_magnetic_laplacian} is equivalent to the combinatorial Laplacian for . 
Moreover, if the graph is undirected, we recover the combinatorial Laplacian for any finite .

The  term in \autoref{eq:unsymmetric_magnetic_laplacian} encodes the edge direction. It resolves to  if  and, otherwise, to , with the sign encoding the edge direction. The \emph{potential}  determines the ratio of real and imaginary parts. Recall that 
. Conversely,  with real / imag.\ value  / . For illustration, we next give the Magnetic Laplacian for a sequence with  and  (\autoref{eq:lap_chain_graph} \& \ref{eq:maglap_chain_graph}), as well as their first eigenvector in \autoref{fig:example_graphs:b} and \ref{fig:example_graphs:a}.\newline
\begin{minipage}{0.495\linewidth}

    \mL_{U}^{(0)} = \begin{bsmallmatrix}
        1 & \shortminus 1 & \cdots & 0 & 0 \\
        \shortminus 1 & 2 & \cdots & 0 & 0 \\
        \vdots & \vdots & \ddots & \vdots & \vdots \\
        0 & 0 & \cdots & 2 & \shortminus 1 \\
        0 & 0 & \cdots & \shortminus 1 & 1 \\
        \end{bsmallmatrix}

\end{minipage}
\begin{minipage}{0.495\linewidth}

\mL_U^{(\nicefrac{1}{4})} =
    \begin{bsmallmatrix}
        1 & \shortminus i & \cdots & 0 & 0 \\
        i & 2 & \cdots & 0 & 0 \\
        \vdots & \vdots & \ddots & \vdots & \vdots \\
        0 & 0 & \cdots & 2 & \shortminus i \\
        0 & 0 & \cdots & i & 1 \\
        \end{bsmallmatrix}

\end{minipage}

In our experiments, we use the degree-normalized counterpart  that we find to result in slightly superior performance.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.925\linewidth]{assets/SmallDirectedMagLap.pdf}
  \caption{First eigenvec.\  of Magnetic Laplacian (\autoref{eq:unsymmetric_magnetic_laplacian}).}
  \label{fig:exemplary_eigenvecotr_maglap}
  \vspace{-0.1in}
\end{figure}

\textbf{Directedness.} We next illustrate how the \emph{eigenvectors} of the Magnetic Laplacian  encode direction. For the special case of a sequence (\autoref{fig:example_graphs:a}), the eigenvectors are given by  with . This corresponds to the Cosine Transformation Type II (see \autoref{sec:sinlap}) with additional factor  that encodes the node position . Importantly, the eigenvectors of the Magnetic Laplacian also encode the directionality in arbitrary (directed) graph topologies, where each \emph{directed} edge  encourages a phase difference in the (otherwise constant) first eigenvector , i.e., between  and . 
For simple cases with , as in \autoref{fig:exemplary_eigenvecotr_maglap}, each directed edge induces a rotation of  while each undirected edge synchronizes the rotation of the adjacent nodes. Note that self-loops are assumed to be undirected and only affect the symmetrically degree-normalized . In this example, the one-hop neighbors of node 3, namely nodes 1 and 4, have a relative rotation of , while the two-hop neighbor 2 has a relative rotation of . In general, the first normalized eigenvector  minimizes the Rayleigh quotient

Thus, the eigenvectors trade off conflicting edges, e.g., if multiple (directed) routes of different lengths exist between nodes  and . For more details, see \autoref{sec:appendix_magnetic_laplacian}.

\textbf{The potential } determines the strength of the induced phase shift by each edge. Thus,  plays a similar role as the lowest frequency in sinusoidal positional encodings (typically . Following the convention of sinusoidal encodings, one could fix  to an appropriate value for the largest expected graphs. However, scaling potential  with the number of nodes  and the amount of directed edges leads to slightly superior performance in our experiments. 
Specifically, we choose  with \emph{relative potential}  
and graph-specific normalizer . This normalizer is an upper bound on the number of directed edges in a simple path  with the number of \emph{purely directed edges}  
and is motivated by \autoref{eq:maglap_raleigh} (see \autoref{sec:appendix_maglap_influence_q}). We typically choose  and empirically verify this in \autoref{fig:sn_qsweep} where it is among the best. For high values of  the performance drops severely (corresponds to absolute .

\textbf{Scale and rotation.} Eigenvectors are not unique and we normalize them by a convention. We visualize the first eigenvector \emph{after applying our normalization} for different graphs in \autoref{fig:example_graphs} \& \ref{fig:appendix_example_graphs}. One source of ambiguity is its scale and rotation. If  is an eigenvector of  then so is , even if  (proof: . For real symmetric matrices, there is the convention to choose  s.t.\  is orthonormal (. 
Similarly, (1) we choose  s.t.\  is unitary (. Moreover, if not using a sign-invariant architecture, as described below, (2) we determine the sign of each eigenvector such that the maximum real magnitude is positive. This resolves the sign-ambiguity up to ties in the maximum real magnitude and numerical errors. (3) we fix the rotation. If possible for the task at hand, we use the graph's distinct ``root'' node . For example, in function name prediction, the root node marks the start of the function definition. Alternatively, we use the foremost (source) node  as root, i.e., the node with maximum phase shift in the first eigenvector . In both cases, we then rotate all eigenvectors, such that the phase shift  is 0 for all . Due to the rotation in (3), our normalization is best suited for graphs with root/source node(s). For details, see \autoref{sec:appendix_maglap_scale_rotate}.


\begin{figure}[t]
  \centering
  \hfill{}
  \subfloat[Transformer Encoder\label{fig:maglapnet_with_trans:a}]{\includegraphics[height=5.6cm]{assets/VanillaTransformerArchitectureColored.pdf}}
  \hfill{}
  \subfloat[MagLapNet\label{fig:maglapnet_with_trans:b}]{\includegraphics[height=5.6cm]{assets/MagLapNet.pdf}}
  \hfill{}
  \caption{(a) shows a transformer encoder operating on a graph with omitted residual connection. (b) is one specific instantiation of the (optional) ``PosEncNet'' using the eigenvectors of the Magnetic Lap (see ``MagLapNet'' paragraph) with batch size . See \autoref{sec:random_walks} for random walk encodings.}
  \label{fig:maglapnet_with_trans}
  \vspace{-0.12in}
\end{figure}

\textbf{MagLapNet.} Inspired by prior approaches~\citep{lim_sign_2022, kreuzer_rethinking_2021}, we also preprocess eigenvectors before using them as positional encodings (\autoref{fig:maglapnet_with_trans:b}) to obtain a structure-aware transformer (\autoref{fig:maglapnet_with_trans:a}). We consider the eigenvectors associated with the  lowest eigenvalues  and treat  as hyperparameter. 
We study two architecture variants for processing the eigenvectors after stacking real and imaginary components: (a) a model that ignores the sign-invariance  and (b) the sign-invariant SignNet~\citep{lim_sign_2022} that processes each eigenvector as , where  is permutation equivariant over the nodes, like a point-wise Multi-Layer Perceptron (MLP) or GNN. 
However, when utilizing an MLP, we observe that choice (a) yields superior performance. This outcome can be attributed to several factors, including our aforementioned selection of the sign (see above) and the characteristic of a point-wise MLP to disregard relative differences in . Note that we always process the first eigenvector as  since we fully resolve its sign ambiguity.

Thereafter, we apply LayerNorm, Self-Attention, and Dropout. Similar to \citet{kreuzer_rethinking_2021}, we apply self-attention independently for each node  over its  eigenvector embeddings. In other words, for each node, we apply self-attention over a set of  tokens. This models the node-wise interactions between the eigenvectors, i.e., ( for node . The last reshape stacks each node's encoding, and the MLP  matches the transformer dimensions.

\section{Directional Random Walks}\label{sec:random_walks}

An alternative principled approach for encoding node positions in a graph is through random walks. \citet{li_distance_2020} have shown that such positional encodings can provably improve the expressiveness of GNNs, and such random walk encodings have been applied to transformers as well~\citep{mialon_graphit_2021}. Interestingly, random walks generalize, e.g., shortest path distances via the number of steps required for a non-zero landing probability. However, na\"ively applying random walks to directed graphs comes with caveats. 

\textbf{Random walks on graphs.} A -step random walk on a graph is naturally expressed via the powers  of the transition matrix . In each step, the random walk proceeds along one of the outgoing edges with equal probability or probability proportional to the edge weight. We then obtain the landing probability  at node  if starting from node . Note that even in connected graphs, we might have node pairs  that have zero transition probability regardless of . Thus, the na\"ive application of random walks for positional encodings on directed graphs is not ideal.

\textbf{Directedness.} To overcome the issue of only walking in \emph{forward} direction and in contrast to \citet{li_distance_2020}, we additionally consider the \emph{reverse} direction . Additionally, we add self-loops to sink nodes (nodes with zero out or in degree for  or , respectively). This avoids that  might be nilpotent and ensures that the landing probabilities sum up to one. We then define the positional encoding for node  as , where  f^{(2)}_{\text{rw}}[(\mR^k)_{v,u}, \dots, (\mR^2)_{v,u}, \emR_{v,u}, \emT_{v,u}, (\mT^2)_{v,u}, \dots, (\mT^k)_{v,u}] and  performs summation.  and  is an MLP.

\textbf{Large distances.} A large amount of random walk steps  is expensive and for a sufficiently large  the probability mass concentrates in sink nodes. Thus, the random walk positional encodings are best suited for capturing short distances. For the global relations, we extend  with a forward and reverse infinite step random walk, namely Personalized Page Rank (PPR)~\citep{page_pagerank_1999}. Importantly, PPR includes the restart probability  to jump back to the starting node  and has closed form solution .

We provide an overview of our positional encodings in \autoref{sec:appendix_overview} and discuss computational cost/complexity in \autoref{sec:appendix_scalablity}.

\section{Positional Encodings Playground}\label{sec:playground}

We next assess the efficacy of our two directional structure-aware positional encodings. As there is no (established) way of assessing positional encodings standalone, we rely on downstream tasks. In our first task, we verify if the encodings are predictive for distances on graphs.

\textbf{Tasks.} We hypothesize that a good positional encoding should be able to distinguish between ancestors/successors and should have a notion of distance on the graph. To cope with general graphs, instead of ancestor/successor nodes, we predict if a node is reachable, acknowledging the edge directions. As distance measures, we study the prediction of adjacent nodes as well as the \emph{directed} and \emph{undirected} shortest path distance. With \emph{undirected} shortest path distance, we refer to the path length on the symmetrized graph, and in both cases we ignore node pairs for which no path exists. In summary, we study pair-wise binary classification of \emph{(1) reachability} and \emph{(2) adjacency} as well as pair-wise regression of \emph{(3) undirected distance} and \emph{(4) directed distance}.

\textbf{Models.} We use a vanilla transformer encoder~\citep{vaswani_attention_2017} with positional encodings (see \autoref{fig:maglapnet_with_trans:a}). We compare our Magnetic Laplacian (ML) positional encodings w/o SignNet (\autoref{sec:spectral}) with our direction-aware random walk (RW) of \autoref{sec:random_walks} and eigenvectors of the combinatorial Laplacian (Lap.) from \autoref{sec:sinlap}. The eigenvectors of the combinatorial Laplacian are preprocessed like the ones of the Magnetic Laplacian (\autoref{fig:maglapnet_with_trans:b}), except that the ``Stack'' step is superfluous due to the real eigenvectors. Additionally, we compare to the SVD encodings of \citet{hussain_global_2022} that perform a low-rank decomposition of the (directed) adjacency matrix. Moreover, with the goal of obtaining general positional encodings, we do not study any heuristics that can be considered ``directional''. For example, if solely considering trees, it might be sufficient to add features for source and sink nodes next to undirected positional encodings.

All studied tasks are instances of link prediction or link regression where the predictions are of shape  (ignoring the disconnected pairs of nodes in distance regression), modeling the relative interactions between nodes. For this, we broadcast the resulting node encodings  (see \autoref{fig:maglapnet_with_trans:a}) of the sender and receiver nodes and stack a global readout. Thereafter, we use a shallow MLP with 3-layers in total and task-dependent output activation (softmax or softplus).

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{assets/playground.pdf}\\normltwo\{1, 1.5, 2\}\{1, 1.5, 2, 2.5, 3\}nk=1\zeta(v | u)n \times n \times dp=5pq = \nicefrac{q'}{\max(\min(\vec{m}, n), 1)}k=25pnpi, j \in \{0, 1, \dots, p-1\}ijijijpn![(0, 2), (0, 1), (1, 2)]7 \le p_{\text{train}} \le 11p_{\text{val}} = 1213 \le p_{\text{test}} \le 16\nicefrac{1}{3}\nicefrac{2}{3}\approx0.1\%\lfloor \nicefrac{n}{2} \rfloor\lfloor \nicefrac{3n}{10} \rfloor\lfloor \nicefrac{7n}{10} \rfloor\lfloor \nicefrac{n}{4} \rfloor\lfloor \nicefrac{3n}{4} \rfloor
    \begin{array}{ccccc}
      \subfloat[Sequence\label{fig:appenidx_example_graphs:a}]{
      \includegraphics[width=0.18\linewidth]{assets/spectrum/small_sequence_graph_0.pdf}} &
      \subfloat[Undirected sequence\label{fig:appenidx_example_graphs:b}]{
      \includegraphics[width=0.18\linewidth]{assets/spectrum/small_undirected_sequence_graph_0.pdf}} &
      \subfloat[Balanced binary tree\label{fig:appenidx_example_graphs:c}]{
      \includegraphics[width=0.18\linewidth]{assets/spectrum/small_reversed_balanced_binary_tree_graph_0.pdf}} &
      \subfloat[Trumpet\label{fig:appenidx_example_graphs:d}]{
      \includegraphics[width=0.18\linewidth]{assets/spectrum/small_trumpet_loop_graph_0.pdf}} \\
      
      \subfloat[Reversed sequence\label{fig:appenidx_example_graphs:e}]{
      \includegraphics[width=0.18\linewidth]{assets/spectrum/small_reversed_sequence_graph_0.pdf}} &
      \subfloat[Cirlce\label{fig:appenidx_example_graphs:f}]{
      \includegraphics[width=0.18\linewidth]{assets/spectrum/small_circle_graph_0.pdf}} &
      \subfloat[Reversed bal.\ bin.\ tree\label{fig:appenidx_example_graphs:g}]{
      \includegraphics[width=0.18\linewidth]{assets/spectrum/small_balanced_binary_tree_graph_0.pdf}} &
      \subfloat[Trumpet (DAG)\label{fig:appenidx_example_graphs:h}]{
      \includegraphics[width=0.18\linewidth]{assets/spectrum/small_trumpet_forward_graph_0.pdf}} \\
      
      \subfloat[Disconnected seq.\label{fig:appenidx_example_graphs:i}]{
      \includegraphics[width=0.18\linewidth]{assets/spectrum/small_two_sequences_graph_0.pdf}} &
      \subfloat[Fully connected DAG]{
      \includegraphics[width=0.18\linewidth]{assets/spectrum/small_fully_connected_dag_graph_0.pdf}} &
      \subfloat[Mix DAG \& fully con.\label{fig:appenidx_example_graphs:k}]{
      \includegraphics[width=0.18\linewidth]{assets/spectrum/small_fully_connected_graph_dag_mix_graph_0.pdf}} &
      \subfloat[Trumpet (fully con.)\label{fig:appenidx_example_graphs:l}]{
      \includegraphics[width=0.18\linewidth]{assets/spectrum/small_belly_snake_graph_0.pdf}} \\
    \end{array}
    \Eigvec\vx \in \R^n
    \mX \coloneqq \Eigvec^\top \vx

    \emX(\eigvec_k) = \emX_{k} = \sum_{i=1}^n \evx_i \eEigvec_{i, k}
\vx = \Eigvec \mX\Eigvec\Eigvec \Eigvec^\top = \mI\eEigvec_{u, 0} = \pm \nicefrac{1}{\sqrt{n}}\label{eq:appendix_quadratic_form_lap}
    \vx^\top \mL_U \vx = \frac{1}{2} \sum_{(u,v) \in E} (\evx_u - \evx_v)^2 = \frac{1}{2} \sum_{u=1}^n \sum_{v=1}^n \emA_{u,v} (\evx_u - \evx_v)^2
\eigvec_0^\top \mL_U \eigvec_0 = \eigval_0 = 0\label{eq:appendix_graph_convolution}
    g_\theta \circledast \vx = \Eigvec g_\theta(\Eigval) \underbrace{\Eigvec^\top \vx}_{\text{GFT}} = \Eigvec \underbrace{g_\theta(\Eigval) \mX}_{\text{filter }\times\text{ signal}} = \underbrace{\Eigvec \hat{\mX}}_{\text{inverse GFT}}
g_\theta(\Eigval)\theta\Eigvalg_\theta(\Eigval)k\mL = \mD - \mA\mL\bar{\Eigvec}^\top \Eigvec = \mI\bar{\Eigvec}^\top\Eigvec\mathbf{0}\label{eq:appendix_unsymmetric_magnetic_laplacian}
\begin{aligned}
    \mL^{(q)}_{U} 
    &\coloneqq \mD_{s} - \mA_{s} \odot \exp \left( i \boldsymbol{\Theta}^{(q)} \right)
\end{aligned}
\odot\exp\smash{i = \sqrt{\shortminus 1}}
    \Theta^{(q)}_{u,v} \coloneqq 2\pi q (\emA_{u,v} - \emA_{v,u})
q \ge 0
    \mL^{(q)}_{N} 
    \coloneqq \mI - \left( \mD_{s}^{- \nicefrac{1}{2}} \mA_{s} \mD_{s}^{- \nicefrac{1}{2}}  \right) \odot \exp \left( i \boldsymbol{\Theta}^{(q)} \right)
\label{eq:appendix_quadratic_form_maglap}
\begin{aligned}
    & \frac{1}{2} \sum_{(u,v) \in E_s} | \evx_u - \evx_v \exp(i\Theta^{(q)}_{u, v}) | ^2 \\
    &= \frac{1}{2} \sum_{(u,v) \in E_s} \overline{(\evx_u - \evx_v \exp(i\Theta^{(q)}_{u, v}))} (\evx_u - \evx_v \exp(i\Theta^{(q)}_{u, v})) \\
    &= \frac{1}{2} \sum_{(u,v) \in E_s} \bar{\evx}_u\evx_u - \exp(i \Theta_{u,v}^{(q)}) \bar{\evx}_u\evx_v - \exp(i \Theta_{v,u}^{(q)}) \bar{\evx}_v\evx_u + \underbrace{\exp(i \Theta_{u,v}^{(q)})\exp(i \Theta_{v,u}^{(q)})}_{=1} \bar{\evx}_v\evx_v \\
    &= \frac{1}{2} \sum_{(u,v) \in E_s} \bar{\evx}_u\evx_u - \exp(i \Theta_{u,v}^{(q)}) \bar{\evx}_u\evx_v - \exp(i \Theta_{v,u}^{(q)}) \bar{\evx}_v\evx_u + \bar{\evx}_v\evx_v \\
    &= \sum_{(u,v) \in E_s} \bar{\evx}_u\evx_u - \exp(i \Theta_{u,v}^{(q)}) \bar{\evx}_u\evx_v \\
    &= \underbrace{\sum_{(u,v) \in E_s} \bar{\evx}_u\evx_u}_{=\bar{\vx}^\top \mD_{s} \vx} \,-\, \underbrace{\sum_{(u,v) \in E_s} \exp(i \Theta_{u,v}^{(q)}) \bar{\evx}_u\evx_v}_{=\bar{\vx}^\top (\mA_{s} \odot \exp ( i \boldsymbol{\Theta}^{(q)})) \vx} \\
    &= \bar{\vx}^\top \left( \mD_{s} - \mA_{s} \odot \exp ( i \boldsymbol{\Theta}^{(q)}) \right) \vx \\
    &= \bar{\vx}^\top \mL_U^{(q)} \vx \\
\end{aligned}
E_s\Theta_{v,u}^{(q)} = - \Theta_{u, v}^{(q)}\Theta_{v,u}^{(q)} = 0\label{eq:appendix_maglap_rayleigh}
    \min_{\vx \in \C^n} \frac{\bar{\vx}^\top \mL_U^{(q)} \vx}{\bar{\vx}^\top \vx} 
    = \frac{\bar{\eigvec}_0^\top \mL_U^{(q)} \eigvec_0}{\bar{\eigvec}_0^\top \eigvec_0} 
    = \eigval_0
\eigvec_0\normltwon\boldsymbol{\alpha}m\alpha_u - \alpha_v \mod 2\piu, v \in \{0, 1, \dots, n-1\}\label{eq:appendix_angular_synchron}
    \angle(\eigvec_0) \in {\arg\min}_{\boldsymbol{\alpha} \in [0, 2\pi)^n} \eta(\boldsymbol{\alpha}) \quad \text{with }
    \eta(\boldsymbol{\alpha}) = \sum\nolimits_{u, v \in E} |\exp(i \alpha_v)- \exp(i \alpha_u + i \Theta_{u,v})|^2
\begin{array}{ccc}
      \subfloat[Sequence]{\includegraphics[width=0.165\linewidth]{assets/spectrum/small_sequence_graph_0.pdf}} & \subfloat[Eigenvectors of Magnetic Laplacian]{\includegraphics[width=0.35\linewidth]{assets/spectrum/spec_sequence_n100_q0.25_rel_u_norm_eigenvecs.pdf}} & \subfloat[Eigenvec. of comb. Lap. w/o symmetrization]{\includegraphics[width=0.35\linewidth]{assets/spectrum/lap_sequence_eigenvecs.pdf}} \\
      
      \subfloat[Reversed sequence]{\includegraphics[width=0.165\linewidth]{assets/spectrum/small_reversed_sequence_graph_0.pdf}} & \subfloat[Eigenvectors of Magnetic Laplacian]{\includegraphics[width=0.35\linewidth]{assets/spectrum/spec_reversed_sequence_n100_q0.25_rel_u_norm_eigenvecs.pdf}} & \subfloat[Eigenvec. of comb. Lap. w/o symmetrization]{\includegraphics[width=0.35\linewidth]{assets/spectrum/lap_reversed_sequence_eigenvecs.pdf}} \\
      
      \subfloat[Undirected sequence]{\includegraphics[width=0.165\linewidth]{assets/spectrum/small_undirected_sequence_graph_0.pdf}} & \subfloat[Eigenvectors of Magnetic Laplacian]{\includegraphics[width=0.35\linewidth]{assets/spectrum/spec_undirected_sequence_n100_q0.25_rel_u_norm_eigenvecs.pdf}} & \subfloat[Eigenvec. of comb. Lap. w/o symmetrization]{\includegraphics[width=0.35\linewidth]{assets/spectrum/lap_undirected_sequence_eigenvecs.pdf}} \\
      
      \subfloat[Circle]{\includegraphics[width=0.165\linewidth]{assets/spectrum/small_circle_graph_0.pdf}} & \subfloat[Eigenvectors of Magnetic Laplacian]{\includegraphics[width=0.35\linewidth]{assets/spectrum/spec_circle_n100_q0.25_rel_u_norm_eigenvecs.pdf}} & \subfloat[Eigenvec. of comb. Lap. w/o symmetrization]{\includegraphics[width=0.35\linewidth]{assets/spectrum/lap_circle_eigenvecs.pdf}} \\
      
      \subfloat[Disconnected seq. sequence]{\includegraphics[width=0.165\linewidth]{assets/spectrum/small_two_sequences_graph_0.pdf}} & \subfloat[Eigenvectors of Magnetic Laplacian]{\includegraphics[width=0.35\linewidth]{assets/spectrum/spec_two_sequences_n100_q0.25_rel_u_norm_eigenvecs.pdf}} & \subfloat[Eigenvec. of comb. Lap. w/o symmetrization]{\includegraphics[width=0.35\linewidth]{assets/spectrum/lap_two_sequences_eigenvecs.pdf}} \\
      
      \subfloat[Binary tree]{\includegraphics[width=0.165\linewidth]{assets/spectrum/small_balanced_binary_tree_graph_0.pdf}} & \subfloat[Eigenvectors of Magnetic Laplacian]{\includegraphics[width=0.35\linewidth]{assets/spectrum/spec_balanced_binary_tree_n100_q0.25_rel_u_norm_eigenvecs.pdf}} & \subfloat[Eigenvec. of comb. Lap. w/o symmetrization]{\includegraphics[width=0.35\linewidth]{assets/spectrum/lap_balanced_binary_tree_eigenvecs.pdf}} \\
    \end{array}q=0.25n=100\begin{array}{ccc}
      \subfloat[Reversed bin. tree]{\includegraphics[width=0.165\linewidth]{assets/spectrum/small_reversed_balanced_binary_tree_graph_0.pdf}} & \subfloat[Eigenvectors of Magnetic Laplacian]{\includegraphics[width=0.35\linewidth]{assets/spectrum/spec_reversed_balanced_binary_tree_n100_q0.25_rel_u_norm_eigenvecs.pdf}} & \subfloat[Eigenvec. of comb. Lap. w/o symmetrization]{\includegraphics[width=0.35\linewidth]{assets/spectrum/lap_reversed_balanced_binary_tree_eigenvecs.pdf}} \\
      
      \subfloat[Trumpet]{\includegraphics[width=0.165\linewidth]{assets/spectrum/small_trumpet_loop_graph_0.pdf}} & \subfloat[Eigenvectors of Magnetic Laplacian]{\includegraphics[width=0.35\linewidth]{assets/spectrum/spec_trumpet_loop_n100_q0.25_rel_u_norm_eigenvecs.pdf}} & \subfloat[Eigenvec. of comb. Lap. w/o symmetrization]{\includegraphics[width=0.35\linewidth]{assets/spectrum/lap_trumpet_loop_eigenvecs.pdf}} \\
      
      \subfloat[Trumpet (forward)]{\includegraphics[width=0.165\linewidth]{assets/spectrum/small_trumpet_forward_graph_0.pdf}} & \subfloat[Eigenvectors of Magnetic Laplacian]{\includegraphics[width=0.35\linewidth]{assets/spectrum/spec_trumpet_forward_n100_q0.25_rel_u_norm_eigenvecs.pdf}} & \subfloat[Eigenvec. of comb. Lap. w/o symmetrization]{\includegraphics[width=0.35\linewidth]{assets/spectrum/lap_trumpet_forward_eigenvecs.pdf}} \\
      
      \subfloat[Trumpet (DAG)]{\includegraphics[width=0.165\linewidth]{assets/spectrum/small_belly_snake_graph_0.pdf}} & \subfloat[Eigenvectors of Magnetic Laplacian]{\includegraphics[width=0.35\linewidth]{assets/spectrum/spec_belly_snake_n100_q0.25_rel_u_norm_eigenvecs.pdf}} & \subfloat[Eigenvec. of comb. Lap. w/o symmetrization]{\includegraphics[width=0.35\linewidth]{assets/spectrum/lap_belly_snake_eigenvecs.pdf}} \\
      
      \subfloat[Fully con.\ DAG]{\includegraphics[width=0.165\linewidth]{assets/spectrum/small_fully_connected_dag_graph_0.pdf}} & \subfloat[Eigenvectors of Magnetic Laplacian]{\includegraphics[width=0.35\linewidth]{assets/spectrum/spec_fully_connected_dag_n100_q0.25_rel_u_norm_eigenvecs.pdf}} & \subfloat[Eigenvec. of comb. Lap. w/o symmetrization]{\includegraphics[width=0.35\linewidth]{assets/spectrum/lap_fully_connected_dag_eigenvecs.pdf}} \\
      
      \subfloat[Mix DAG \& f.\ con.]{\includegraphics[width=0.165\linewidth]{assets/spectrum/small_fully_connected_graph_dag_mix_graph_0.pdf}} & \subfloat[Eigenvectors of Magnetic Laplacian]{\includegraphics[width=0.35\linewidth]{assets/spectrum/spec_fully_connected_graph_dag_mix_n100_q0.25_rel_u_norm_eigenvecs.pdf}} & \subfloat[Eigenvec. of comb. Lap. w/o symmetrization]{\includegraphics[width=0.35\linewidth]{assets/spectrum/lap_fully_connected_graph_dag_mix_eigenvecs.pdf}} \\
    \end{array}q=0.25n=100\emA_{u,v} \ne \emA_{v,u}\eEigvec_{u, 0}\eEigvec_{v, 0}\emA_{u,v} = \emA_{v,u} = 1\eigvec_02\pi q h \mod 2\pi\eigvec_0| \eEigvec_{u, 0} - \eEigvec_{v, 0} \exp(\Theta^{(q)}_{u, v}) | ^2 = 0u,v) \in \E\max_u |\eigvec_u| = 1\eEigvec^{(q)}_{v, j} = c \evs_v \eEigvec^{(0)}_{v, j}vjc \in \C \setminus \{0\}\vs \in \C^{n}\mS\vs\mS\mL^{(q)} \mS = \mS \mL^{(0)}\mL^{(q)} \mS \eigvec_j^{(0)} = \mS \mL^{(0)} \eigvec_j^{(0)} = \mS (\mL^{(0)} \eigvec_j^{(0)})\mL^{(0)} \eigvec_j^{(0)} = \eigval_j^{(0)} \eigvec_j^{(0)}\mL^{(q)} \mS \eigvec_j^{(0)} =  \mS (\eigval_j^{(0)} \eigvec_j^{(0)}) = \eigval_j^{(0)} \mS \eigvec_j^{(0)}\eigvec_j^{(q)} = \mS \eigvec_j^{(0)}\mS\mS\emS_{v,v} = \exp(-i 2 \pi \evd_v)\evd_vv\Eigvecllc \in \C \setminus \{0\}c \mL \eigvec = c \eigval \eigvec \implies \mL (c \eigvec) = \eigval (c \eigvec)c\eigval\Eigvecqqqqqqqqqqqq = \nicefrac{1}{3}\nicefrac{2}{3} \piqqnn\bar{\eigvec}_0^\top \mL^{(q)} \eigvec_0 = \eigval_0 = 00 < q \le \nicefrac{1}{4}2 \pi q ll\{(u, v) \in E \,|\, (v, u) \notin E\}llllq=0q'=0q=\num{2.5e-3}q'=2.5e-1q=\num{2.5e-2}q'=2.5q=\num{2.5e-1}q'=25n=101quv\angle(\eEigvec_{u, 0}) = \angle(\eEigvec_{v, 0})\eigval_0 > 02 \pi q luvluvlo < l\bar{\eigvec}_0^\top \mL^{(q)} \eigvec_02 \pi q o, 2 \pi q l)q = \nicefrac{q'}{d_\gG}q'd_\gG = \max(\min(\vec{m}, n), 1)\vec{m}\nicefrac{2 \pi}{q'}q' \le \nicefrac{1}{4}q'=\nicefrac{1}{4}qqnq\eigvec\mLc \eigvecc \in \C|c| > 0c \mL \eigvec = c \eigval \eigvec \implies \mL (c \eigvec) = \eigval (c \eigvec)ccq[-\nicefrac{\pi}{2}, \nicefrac{\pi}{2}]\Eigvec \in \C^{n \times k}\vj \leftarrow \operatorname{argmax}(|\Re(\Eigvec)|,\, \operatorname{axis}=0)k\vs \leftarrow \operatorname{sign}(\Re(\Eigvec)[0:n-1, \vj])k\Eigvec \leftarrow \vs \odot \Eigvecj \leftarrow \operatorname{max}(\Im(\Eigvec[:, 0]))\alpha \leftarrow \angle(\Eigvec[j, :])\Eigvec \leftarrow \exp(-i \alpha)^\top \odot \Eigvec\Eigvecc\Eigvec\eigvecj = 0\argmax_v \angle(\eEigvec_{v, 0}) = u\angle(\eEigvec_{u, j}) = 0\,,\,\,\forall\,j \in \{0, 1, \dots, n-1\}\operatorname{idx} = \operatorname{argsort}(\Im(\eigvec_0))\begin{array}{cccccc}
      \subfloat[Sequence]{
      \includegraphics[height=2.5cm]{assets/permuted_png/raw_permuted_sequence_n9_q0.027777777777777776_rel_u_graph_0.pdf}} &
      \subfloat[Rand. permuted]{
      \includegraphics[height=2.5cm]{assets/permuted_png/permuted_sequence_n9_q0.027777777777777776_rel_u_graph_0_0.png}} &
      \subfloat[Rand. permuted]{
      \includegraphics[height=2.5cm]{assets/permuted_png/permuted_sequence_n9_q0.027777777777777776_rel_u_graph_1_0.png}} &
      \subfloat[Rand. permuted]{
      \includegraphics[height=2.5cm]{assets/permuted_png/permuted_sequence_n9_q0.027777777777777776_rel_u_graph_2_0.png}} &
      \subfloat[Rand. permuted]{
      \includegraphics[height=2.5cm]{assets/permuted_png/permuted_sequence_n9_q0.027777777777777776_rel_u_graph_3_0.png}} &
      \subfloat[Rand. permuted]{
      \includegraphics[height=2.5cm]{assets/permuted_png/permuted_sequence_n9_q0.027777777777777776_rel_u_graph_4_0.png}} \\
      
      \subfloat[Sequence]{
      \includegraphics[height=2.5cm]{assets/permuted_png/raw_permuted_sequence_n9_q0.027777777777777776_rel_u_graph_0.pdf}} &
      \subfloat[Reordered]{
      \includegraphics[height=2.5cm]{assets/permuted_png/reordered_sequence_n9_q0.027777777777777776_rel_u_graph_0_0.png}} &
      \subfloat[Reordered]{
      \includegraphics[height=2.5cm]{assets/permuted_png/reordered_sequence_n9_q0.027777777777777776_rel_u_graph_1_0.png}} &
      \subfloat[Reordered]{
      \includegraphics[height=2.5cm]{assets/permuted_png/reordered_sequence_n9_q0.027777777777777776_rel_u_graph_2_0.png}} &
      \subfloat[Reordered]{
      \includegraphics[height=2.5cm]{assets/permuted_png/reordered_sequence_n9_q0.027777777777777776_rel_u_graph_3_0.png}} &
      \subfloat[Reordered]{
      \includegraphics[height=2.5cm]{assets/permuted_png/reordered_sequence_n9_q0.027777777777777776_rel_u_graph_4_0.png}} \\
      \end{array}\operatorname{idx} = \operatorname{argsort}(\Im(\eigvec_0))\begin{array}{cccccc}
      \subfloat[Binary tree]{
      \includegraphics[height=2.5cm]{assets/permuted_png/raw_permuted_balanced_binary_tree_n9_q0.027777777777777776_rel_u_graph_0.pdf}} &
      \subfloat[Rand. permuted]{
      \includegraphics[height=2.5cm]{assets/permuted_png/permuted_balanced_binary_tree_n9_q0.027777777777777776_rel_u_graph_0_0.png}} &
      \subfloat[Rand. permuted]{
      \includegraphics[height=2.5cm]{assets/permuted_png/permuted_balanced_binary_tree_n9_q0.027777777777777776_rel_u_graph_1_0.png}} &
      \subfloat[Rand. permuted]{
      \includegraphics[height=2.5cm]{assets/permuted_png/permuted_balanced_binary_tree_n9_q0.027777777777777776_rel_u_graph_2_0.png}} &
      \subfloat[Rand. permuted]{
      \includegraphics[height=2.5cm]{assets/permuted_png/permuted_balanced_binary_tree_n9_q0.027777777777777776_rel_u_graph_3_0.png}} &
      \subfloat[Rand. permuted]{
      \includegraphics[height=2.5cm]{assets/permuted_png/permuted_balanced_binary_tree_n9_q0.027777777777777776_rel_u_graph_4_0.png}} \\
      
      \subfloat[Binary tree]{
      \includegraphics[height=2.5cm]{assets/permuted_png/raw_permuted_balanced_binary_tree_n9_q0.027777777777777776_rel_u_graph_0.pdf}} &
      \subfloat[Reordered]{
      \includegraphics[height=2.5cm]{assets/permuted_png/reordered_balanced_binary_tree_n9_q0.027777777777777776_rel_u_graph_0_0.png}} &
      \subfloat[Reordered]{
      \includegraphics[height=2.5cm]{assets/permuted_png/reordered_balanced_binary_tree_n9_q0.027777777777777776_rel_u_graph_1_0.png}} &
      \subfloat[Reordered]{
      \includegraphics[height=2.5cm]{assets/permuted_png/reordered_balanced_binary_tree_n9_q0.027777777777777776_rel_u_graph_2_0.png}} &
      \subfloat[Reordered]{
      \includegraphics[height=2.5cm]{assets/permuted_png/reordered_balanced_binary_tree_n9_q0.027777777777777776_rel_u_graph_3_0.png}} &
      \subfloat[Reordered]{
      \includegraphics[height=2.5cm]{assets/permuted_png/reordered_balanced_binary_tree_n9_q0.027777777777777776_rel_u_graph_4_0.png}} \\
      \end{array}\operatorname{idx} = \operatorname{argsort}(\Im(\eigvec_0))\begin{array}{cccccc}
      \subfloat[Trumpet (fully c.)]{
      \includegraphics[height=2.5cm]{assets/permuted_png/raw_permuted_belly_snake_n9_q0.027777777777777776_rel_u_graph_0.pdf}} &
      \subfloat[Rand. permuted]{
      \includegraphics[height=2.5cm]{assets/permuted_png/permuted_belly_snake_n9_q0.027777777777777776_rel_u_graph_0_0.png}} &
      \subfloat[Rand. permuted]{
      \includegraphics[height=2.5cm]{assets/permuted_png/permuted_belly_snake_n9_q0.027777777777777776_rel_u_graph_1_0.png}} &
      \subfloat[Rand. permuted]{
      \includegraphics[height=2.5cm]{assets/permuted_png/permuted_belly_snake_n9_q0.027777777777777776_rel_u_graph_2_0.png}} &
      \subfloat[Rand. permuted]{
      \includegraphics[height=2.5cm]{assets/permuted_png/permuted_belly_snake_n9_q0.027777777777777776_rel_u_graph_3_0.png}} &
      \subfloat[Rand. permuted]{
      \includegraphics[height=2.5cm]{assets/permuted_png/permuted_belly_snake_n9_q0.027777777777777776_rel_u_graph_4_0.png}} \\
      
      \subfloat[Trumpet (ful. c.)]{
      \includegraphics[height=2.5cm]{assets/permuted_png/raw_permuted_belly_snake_n9_q0.027777777777777776_rel_u_graph_0.pdf}} &
      \subfloat[Reordered]{
      \includegraphics[height=2.5cm]{assets/permuted_png/reordered_belly_snake_n9_q0.027777777777777776_rel_u_graph_0_0.png}} &
      \subfloat[Reordered]{
      \includegraphics[height=2.5cm]{assets/permuted_png/reordered_belly_snake_n9_q0.027777777777777776_rel_u_graph_1_0.png}} &
      \subfloat[Reordered]{
      \includegraphics[height=2.5cm]{assets/permuted_png/reordered_belly_snake_n9_q0.027777777777777776_rel_u_graph_2_0.png}} &
      \subfloat[Reordered]{
      \includegraphics[height=2.5cm]{assets/permuted_png/reordered_belly_snake_n9_q0.027777777777777776_rel_u_graph_3_0.png}} &
      \subfloat[Reordered]{
      \includegraphics[height=2.5cm]{assets/permuted_png/reordered_belly_snake_n9_q0.027777777777777776_rel_u_graph_4_0.png}} \\
      \end{array}\operatorname{idx} = \operatorname{argsort}(\Im(\eigvec_0))\mU \boldsymbol{\Sigma} \mVn=5
    \mA = \resizebox{!}{0.7\height}{\mathbf{1}\mathbf{1}} \approx \resizebox{!}{0.7\height}{} = \resizebox{!}{0.7\height}{\mathbf{0}\mathbf{0}}
\mU \in \C^{n \times n}\mV \in \C^{n \times n}\mU \boldsymbol{\Sigma} \mV = (\shortminus \mU) \boldsymbol{\Sigma} (\shortminus \mV)\mA \in \R_{\ge 0}^{n \times n}q\label{eq:appendix_maglap_raleigh}
    \min_{\vx \in \C} \frac{\bar{\vx}^\top \mL^{(q)} \vx}{\bar{\vx}^\top \vx}
    = \frac{1}{2} \sum_{(u,v) \in E_s} w_{u, v} | \eEigvec_{u, 0} - \eEigvec_{v, 0} \exp(\Theta^{(q)}_{u, v}) | ^2
w_{u, v}u, v)\mA_s\mA \in \{0, 1\}^{n \times n}qk\mL^{(q)} \leftarrow \operatorname{Laplacian}(\mA, q)\Eigval, \Eigvec \leftarrow \operatorname{eigh}(\mL^{(q)}, k)\Eigvec, \Eigval \leftarrow \operatorname{norm}(\Eigvec, \Eigval)\hat{\Eigvec} \leftarrow \operatorname{MagLapNet}(\Eigvec, \Eigval)\hat{\Eigvec}
    \zeta(v | u) = f^{(2)}_{\text{rw}}[\Pi(\mR)_{v,u}, (\mR^k)_{v,u}, \dots, (\mR^2)_{v,u}, \emR_{v,u}, \emT_{v,u}, (\mT^2)_{v,u}, \dots, (\mT^k)_{v,u}, \Pi(\mT)_{v,u}]
\Pi(\mT) = p_r (\mI - (1 - p_r) \mT)^{-1}v
\zeta(v | \gG) = f_{\text{rw}}^{(1)}(\operatorname{AGG}(\{\zeta(v | u)\,|\,u \in V\}))
\eta \approx \num{8.3e-6} \times \text{batch size}\alpha = \num{6e-5}\beta_1=0.7\beta_2=0.9k=25k=16p_{\text{pos}} = 0.15k=25k=16p' = \nicefrac{1}{4}p_{\text{pos}} = 0.15k=3p_r = 0.05\eta /approx \num{5.4e-5} \times \text{batch size}\alpha = \num{7.5e-5}\beta_1=0.75\beta_2=0.935\eta = \num{5.4e-5} \times \text{batch size}\alpha = \num{6e-5}\beta_1=0.9\beta_2=0.95k=25p' = \nicefrac{1}{4}p_{\text{pos}} = 0.15k=3p_r = 0.05l\mH^{(j)}1 \le j \le l\mH^{(l)}4 \times2 \timesn < 256n < 512k\mathcal{O}(n^2)\mathcal{O}(n^2)q=0.25q=0k=25qq = 0.1q \in \{0, 0.05, 0.1, 0.15, 0.2, 0.25\}n \times n\zeta(v | u)f^{(2)}_{\text{rw}}\operatorname{softmax}(\nicefrac{\mQ\mK^\top}{\sqrt{d}} + \boldsymbol{\zeta}) \mV\boldsymbol{\zeta} \in \R^{n \times n}\zeta(v | u)f_{\text{elem}}(-\eigvec_j) + f_{\text{elem}}(\eigvec_j)f_{\text{elem}}kk\mT = \mA \mD_{\text{out}}^{-1}\mR = \mA^\top \mD_{\text{in}}^{-1}k > 3-4ex]
  \subfloat[W/o rev., w/o PPR \citep{li_distance_2020}\label{fig:appendix_playground_rw:a}]{\hspace{0.3\linewidth}}
  \subfloat[W/ reversal, w/o PPR\label{fig:appendix_playground_rw:b}]{\hspace{0.25\linewidth}}
  \subfloat[W/ reversal, w/ PPR\label{fig:appendix_playground_rw:c}]{\hspace{0.3\linewidth}}
  \caption{Hyperparemter study for random walk encodings on the playground tasks: \emph{(1) reachability}, \emph{(2) adjacency}, \emph{(3) undirected distance}, and \emph{(4) directed distance} Dark green encodes the best scores and bright green bad ones. For F1 score high values are better and for RMSE low values.}
  \label{fig:appendix_playground_rw}
  \vspace{-0.12in}
\end{figure}


\section{Sorting Networks Dataset Construction}\label{sec:appendix_sorting_networks}

We give the data generation process for a single sorting network in \hyperref[algo:appendixsortingnetwork]{Algorithm~\ref{algo:appendixsortingnetwork}}. Additionally, we only consider sorting networks with less than 512 comparators and abort early if this bound is exceeded. Since adding a comparator cannot diminish any progress of the sorting network, we only need to generate all possible test sequences once in the beginning and sort ti successively. Moreover, we make use of the so-called ``0-1-principle''~\citep{knuth_art_1973}. That is, if a sorting network sorts all possible sequences in , then it sorts all possible sequences consisting of comparable elements. Once a valid sorting network was constructed, we drop the last comparator  for an instance of an incorrect sorting network. Moreover, for test and validation, we include another (typically incorrect) sorting network via swapping the order of comparators .

\begin{figure}[t]
\centering
\hfill
\begin{minipage}{.6\linewidth}
\centering
\begin{algorithm}[H]
  \caption{Generate Sorting Network}
  \label{algo:appendixsortingnetwork}
  \begin{algorithmic}[1]
    \STATE {\bfseries Input:} Number of nodes set \STATE Sample uniformly 
    \STATE Empty list of comparators 
    \STATE Unsorted sequences 
    \STATE Unsorted locations 
    \WHILE{}\STATE Sample uniformly  without replacement
        \IF{}
            \STATE 
            \STATE 
            \STATE 
        \ENDIF
    \ENDWHILE

    \STATE Return 
  \end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}{.3\linewidth}
    \centering
    \includegraphics[width=0.9\linewidth]{assets/sorting_adjacency_l12_0.pdf}
    \caption{Sorting networks are \emph{near-sequential} since the edges align well with main diagonal.}
    \label{fig:appenidx_sorting_adj}
\end{minipage}
\hfill
\end{figure}

\section{Sorting Networks Are Near-Sequential}\label{sec:appendix_sorting_networks_near_seq}


All the comparators perform a data-dependent in-place swap operation. In other words, there are no buffers to store, e.g., a value at the beginning of the program and retrieve it at the end. Consequently, edges in the resulting directed graph (e.g.\ \autoref{fig:sorting_graphs}) are typically connected to close ancestor nodes and align well with the main diagonal of the adjacency matrix. We give an example in \autoref{fig:appenidx_sorting_adj}. For this reason, we call the graphs in the sorting network task \emph{near-sequential}. 


Due to the near-sequentiality, sinusoidal positional encodings are intuitively an appropriate choice (see \autoref{sec:sinlap}). However, even these near-sequential graphs can have a huge amount of topological sorts. Enumerating all topological orders for the training instances rarely terminates within a few minutes (using python). We estimate, that the number of topological sorts typically exceeds \num{1e6}. This, surprisingly high number of topological sorts could be the reason why the positional encodings for directed graphs are superior to the sinusoidal encodings (see \autoref{sec:sorting_networks}) and shows the significance of this relationship.


\section{Application: Function Name Prediction}\label{sec:appendix_source_code}

We next describe how we construct the directed graph in the function name prediction task. Recall, that we construct a data-flow-centric directed graph that is also able to handle the sharp bits like if-else, loops, and exceptions. In our graph, we avoid sequential connections that originate from the sequence of statements in the source. We aim for a similar reduction of the input space size to the sorting network task \autoref{sec:sorting_networks}. To explain how we construct this graph, we will first give a high-level description of the design narratives. Then, we include the Abstract Syntax Tree (AST) in the discussion.

\textbf{Design principles.} In \autoref{fig:appendix_data_centric_construction}, we give an example function and give a data-flow-centric dependency graph for the actual computations. The function can be clustered into three blocks (excluding function definition and return statement): (1) variable  is calculated, (2) if-else-statement for refining the value of variable , and (3) variable  is changed if negative. These three blocks are each represented by a grey box. Further, we highlight (sub-) blocks white that correspond to the bodies of the if-else statement.

\begin{figure}[H]
    \hspace{-15pt}
    \begin{minipage}{0.325\textwidth}
        \begin{minted}{python}
        def func(a, b, c):
          d = min(a, b)
          if a > 0:
            e = a ** 2
            f = b ** 2
            a = sqrt(e + f)
          else:
            a = -a
          if b < 0:
            b = b + d
          return a + b
        \end{minted}
    \end{minipage}
    \hspace{25pt}
    \begin{minipage}{0.6\textwidth}
        \includegraphics[width=\linewidth]{assets/data_flow_blocks.pdf}
    \end{minipage}
    \hfill{}
    \caption{Exemplary data-flow-centric graph construction\label{fig:appendix_data_centric_construction}}
\end{figure}

We connect nodes based on the required inputs for the computation (aka data flow). Moreover, we add edges to describe the control flow of the if-statements (dashed blue lines). Last, we add edges if values are being overwritten (dash-dotted green line). Conceptually, we generate a Directed Acyclic Graph (DAG) for each block and then recursively traverse the hierarchy and connect the contained instructions. Hence, the resulting graph is not necessarily acyclic.

\textbf{Order of computation.} In this example, each computation can only take place after all its immediate ancestors have been calculated (and if these values are kept in memory). Vice versa, all computations could take in arbitrary order as long as the parent nodes have been executed. For general functions, the picture is a bit more complicated (but similar) due to, for example, loops, exceptions, or memory constraints.

\textbf{Hierarchy.} To generate the connections we rely on the fact that (Python) code can be decomposed into a hierarchy of blocks. Except for ancestor and successor nodes, these blocks build a mutually exclusive separation of instructions. This decomposition is what an AST provides. While also prior work uses ASTs for their graph construction, they retain the sequential structure of the source code in the graph construction. For example in \autoref{fig:appendix_ogb_vs_ours:a}, we show the graph constructed by the OGB Code2 dataset~\citep{hu_open_2020} for the code in \autoref{fig:appendix_ogb_vs_ours_code}. From this, it should be clear without additional explanation why we argue that the AST solely enriches the sequence of code statements with a hierarchy. In stark contrast, our graph construction (\autoref{fig:appendix_ogb_vs_ours:b}) is by far not as sequential.

\begin{figure}[H]
    \hfill
    \subfloat[OGB Code 2 graph \label{fig:appendix_ogb_vs_ours:a}]{\includegraphics[width=0.25\linewidth]{assets/sample_graphs/ogb_transform_add.pdf}}
    \hfill
    \subfloat[Our data-centric graph\label{fig:appendix_ogb_vs_ours:b}]{\includegraphics[width=0.74\linewidth]{assets/sample_graphs/parsed_transform_add.pdf}}
    \hfill
    \caption{Comparison of OGB Code2's graph construction to ours. \label{fig:appendix_ogb_vs_ours}}
\end{figure}


\textbf{Sequentialization.} Generating semantically equivalent sequences of program statements from such a directed graph is more challenging than determining feasible orders of computation or in the sorting network task \autoref{sec:sorting_networks}. For example, in DAG of \autoref{fig:appendix_data_centric_construction}, not every possible topological sort corresponds to a possible sequentialization of the program. To determine sequentializations one needs to consider the hierarchical block structure. For example, it is possible to reorder the blocks highlighted in grey, depending on their dependencies. However, our data and control flow does not capture all dependencies required to generate the program. For example, as already hinted above, one caveat resides in the availability of intermediate values. Although the first block (to determine \verb|d|) and second block (if else construct) are independent in the shown graph, \emph{overwriting} the value of \verb|a| has not been modeled. In other words, it would make a difference to swap these blocks since \verb|a| changes its value in the second block. Thus, for constructing a possible sequence of program instructions, we would also need to address changing variable values. For example, we could assign a new and unique name after changing a variable's value (as in functional programming or like a compiler does). Alternatively, adding further edges could be sufficient. Nevertheless, these dependencies are not important for the semantic meaning of a program.


\begin{wrapfigure}[9]{r}{0.45\textwidth}
    \vspace{-15pt}
    \hspace{-25pt}
    \begin{minted}{python}
    def transform_add(
        a, b: float = 3.14):
      a = a**2
      c = math.sqrt(b)
      return c + a
    \end{minted}
    \caption{Code used for \autoref{fig:appendix_ogb_vs_ours}.\label{fig:appendix_ogb_vs_ours_code}}
\end{wrapfigure}
\textbf{OGB's graph construction} first converts the source to AST and adds additional edges to simulate the sequential order of instructions. In \autoref{fig:appendix_ogb_vs_ours:a}, the black edges are the edges from the AST and the red edges for the sequential order.

\textbf{Our graph construction.} We also construct the AST from source (\verb|FIELD| edges) and build on top of the graph construction / static code analysis of \citet{bieber_library_2022}. In the example in \autoref{fig:appendix_ogb_vs_ours:b}, we have the same amount of nodes as \autoref{fig:appendix_ogb_vs_ours:a} except for two ``Param'' nodes belonging to the argument nodes. Similarly to the example in \autoref{fig:appendix_data_centric_construction}, we augment the AST with additional edges mimicking the data flow and program flow. Here, we have Control Flow Graph edges \verb|CFG_NEXT| that model the possible order of statement execution. Moreover, the variable nodes (close to leaf nodes) are connected by \verb|LAST_WRITE| and \verb|CALCULATED_FROM| edges. These edges model where the same variable has been written the last time and from which variables is was constructed. Additionally, we use a \verb|CALLS| edge that model function calls / recursion (not present in this example). Thereafter, since the task is function name prediction, we need to prevent data leakage. For this, we mask the occurrences of the function name in its definition as well as in recursive calls.

\textbf{Comparison to \citet{bieber_library_2022}.} We largely follow them and build upon their code base. The most significant difference is the avoidance of unnecessary sequentialism. Specifically, (1) their \verb|CFG_NEXT| edges connect instructions sequentially while ours form a dependency graph, and (2) we omit their \verb|LAST_READ| edges. Moreover, we address commutative properties of the basic python operations (\verb|And|, \verb|Or|, \verb|Add|, \verb|Mult|, \verb|BitOr|, \verb|BitXor|, and \verb|BitAnd|). This can also be observed in \autoref{fig:appendix_ogb_vs_ours:b}, where we name the edges for the inputs to these operations \verb|input| and concatenate \verb|:<order>| if the operation is non-commutative and \verb|<order>| > 1. Last, we do not reference the tokenized source code and choose a less verbose graph similar to OGB. 

\textbf{Sequentialization.} Reconstructing the code from AST is a straightforward procedure. Following our discussion above, we need to acknowledge the hierarchical structure to retrieve valid programs. Fortunately, this hierarchical structure is provided by the AST. However, similar to the example above, we do not model all dependencies for an immediate sequentialization. However, as stated before, these dependencies are not important for semantics. Thus, we most certainly map more semantically equivalent programs to the same directed graph, as if we would compare to a graph construction that models all dependencies.


\end{document}
