\documentclass[journal, one column]{IEEEtran}
\usepackage{amsmath, amsthm, amsfonts}
\usepackage[english]{babel}
\usepackage[dvips]{graphicx}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{enumerate}
\usepackage{mathtools}

\setcounter{page}{1}
\usepackage{setspace}
\doublespacing
\begin{document}
\title{Convergence Analysis of Proportionate-type Least Mean Square Algorithms}


\author{{Vinay Chakravarthi Gogineni, Subrahmanyam Mula}\\
Department of Electronics and Electrical Communication Engineering\\
Indian Institute of Technology, Kharagpur, INDIA\\
E.Mail : vinaychakravarthi@ece.iitkgp.ernet.in, svmula@iitkgp.ac.in}

\maketitle
\thispagestyle{empty}

\begin{abstract}
In this paper, we present the convergence analysis of proportionate-type least mean square (Pt-LMS) algorithm that identifies the sparse system effectively and more suitable for real time VLSI applications. Both first and second order convergence analysis of Pt-LMS algorithm is studied. Optimum convergence behavior of Pt-LMS algorithm is studied from the second order convergence analysis provided in this paper. Simulation results were conducted to verify the analytical results.
\end{abstract}
\textbf{Index terms}-Sparse Systems,  Norm, Compressive Sensing, Excess Mean Square Error.
\section{Introduction}
Usually, many real-life systems exhibit sparse representation
i.e., their system impulse response is characterized by small
number of non zero taps in the presence of large number of
inactive taps. Sparse systems are encountered in many important
practical applications such as network and acoustic echo
cancelers -, HDTV channels ,
wireless multipath channels , underwater acoustic
communications . The conventional
system identification algorithms such as LMS and NLMS
are sparsity agnostic i.e., they are unaware of
underlying sparsity of the system impulse response.  Recent
studies have shown that the \emph{a priori} knowledge about the
system sparsity, if utilized properly by the identification
algorithm, can result in substantial improvement in its estimation
performance. This resulted in a flurry of research activities in
the last decade or so towards developing sparsity aware adaptive
filter algorithms, notable amongst them being the Proportionate
Normalized LMS (PNLMS) algorithm  and its variants
-. Unlike the NLMS, the weighted Euclidean norm of the input vector presented in Proportionate-type NLMS (Pt-NLMS) can not be computed recursively due to the presence of gain matrix , which varies at each time instance . Computation of this weighted Euclidean norm of the input vector requires requires  multiplications and  additions in each iteration that limits the throughput for real-time applications. In this paper, we present the performance analysis of Proportionate-type LMS (Pt-LMS) algorithm that is more suitable for real time VLSI applications.
\section{Proportionate-type LMS Algorithm}
We consider the problem of identifying an unknown system (supposed to
be sparse), modeled by the  tap coefficient vector 
which takes a signal  with variance 
as the input and produces the observable output
, where
 is the
input data vector at time , and  is the observation
noise with zero mean and variance  which is assumed to be white and independent of
 for all . The Pt-LMS algorithm iteratively updates
the filter coefficient vector  as,

where   is the step size,  is a diagonal gain
matrix that distributes the adaptation energy unevenly over the
filter taps by modifying the step size of each tap, and
 is the filter output
error.
\par

The gain matrix  is evaluated as,

where,

with,


where  is a very small, positive constant which, together
with , ensures that  and thus
 do not turn out to be zero for the inactive taps and
thus the corresponding updation does not stall. The parameter
 is again a small positive constant employed to avoid
stalling of the weight updation at the start of the iterations
when the tap weight iterates are initialized to zero. The function
 is chosen differently for different
Pt-LMS algorithms, as described in the table below. From (1), it
is easily seen that  provides the effective step size
for the -th tap which, through the function
, is monotonically related to .
\begin{table}[h!]
\centering
\caption {The function  for a few popular
Pt-LMS algorithms}
    \begin{tabular}{ | l | p{4.5cm} |}
    \hline \-1.0em]
    1. Standard LMS &  \\ [ 2ex]\hline \-1.0em]
    3. IPLMS & \hspace{1em}  \\ [ 2ex] \hline \\label{eq2.1}
\begin{split}
\widetilde{\textbf{w}}(n+1)&=\Big[\textbf{I}_{L}-\mu \hspace{0.2em} \textbf{G}(n) \hspace{0.2em} \textbf{u}(n) \hspace{0.2em} \textbf{u}^{T}(n) \Big] \widetilde{\textbf{w}}(n)
 -\mu \hspace{0.2em} \textbf{G}(n) \hspace{0.2em} \textbf{u}(n) \hspace{0.2em} v(n)
\end{split}
\label{eq2.2}
\begin{split}
E[\widetilde{\textbf{w}}(n+1)]&=\Big[\textbf{I}_{L}-\mu \hspace{0.2em} E\big[\textbf{G}(n) \hspace{0.2em} \textbf{u}(n) \hspace{0.2em} \textbf{u}^{T}(n) \big]\Big] E[\widetilde{\textbf{w}}(n)]
\end{split}
\label{eq2.3}
\begin{split}
E[\widetilde{\textbf{w}}(n+1)]&=\Big[\textbf{I}_{L}-\mu \hspace{0.2em} \overline{\textbf{G}} \hspace{0.2em} \textbf{R}\Big] E[\widetilde{\textbf{w}}(n)]
\end{split}
\label{eq2.4}
\begin{split}
| \lambda_{max}\big( \textbf{I}_{L}-\mu \hspace{0.2em} \overline{\textbf{G}} \hspace{0.2em} \textbf{R} \big) |< 1
\end{split}
\label{eq2.5}
\begin{split}
0 < \mu < \frac{2}{\lambda_{max}\big(\overline{\textbf{G}} \hspace{0.2em} \textbf{R} \big)}
\end{split}
\label{eq2.6}
\begin{split}
0 < \mu < \frac{2}{\overline{g}_{max} \hspace{0.2em} \lambda_{max}\big( \hspace{0.2em} \textbf{R} \big)}
\end{split}
\label{eq2.7}
\begin{split}
E\|\widetilde{\textbf{w}}(n+1)\|_{\boldsymbol{\Sigma}}^{2}&= E\|\widetilde{\textbf{w}}(n)\|_{E \boldsymbol{\Sigma}^{'}}^{2}
+ \mu^{2} \hspace{0.2em} E[v^{2}] \hspace{0.2em} E\big[\textbf{u}^{T}(n) \textbf{G}(n) \Sigma \textbf{G}(n) \textbf{u}(n) \big]
\end{split}
\label{eq2.8}
\begin{split}
\boldsymbol{\Sigma}^{'}&= \boldsymbol{\Sigma} - \mu \hspace{0.2em} \textbf{u}(n) \textbf{u}^{T}(n) \hspace{0.2em} \textbf{G}(n) \hspace{0.2em} \boldsymbol{\Sigma} - \mu \hspace{0.2em} \boldsymbol{\Sigma} \hspace{0.2em} \textbf{G}(n) \hspace{0.2em} \textbf{u}(n) \textbf{u}^{T}(n) + \mu^{2} \hspace{0.2em} \textbf{u}(n) \textbf{u}^{T}(n) \hspace{0.2em} \textbf{G}(n) \hspace{0.2em} \boldsymbol{\Sigma}  \hspace{0.3em} \textbf{G}(n) \hspace{0.2em} \textbf{u}(n) \textbf{u}^{T}(n)\\
\end{split}
\label{eq2.9}
\begin{split}
\boldsymbol{\sigma}= \text{vec}\{\boldsymbol{\Sigma}\} \hspace{2em} \text{and} \hspace{2em} \boldsymbol{\sigma^{'}}= \text{vec}\{E\boldsymbol{\Sigma}^{'}\}
\end{split}
\label{eq2.10}
\begin{split}
\text{vec}\{\textbf{Q} \boldsymbol{\Sigma} \textbf{P}\}= ( \textbf{P}^{T} \otimes \textbf{Q} ) \hspace{0.2em} \boldsymbol{\sigma}
\end{split}
\label{eq2.11}
\begin{split}
\boldsymbol{\sigma}^{'}= \textbf{F} \boldsymbol{\sigma}
\end{split}
\label{eq2.12}
\begin{split}
\textbf{F} = \textbf{I} - \mu \hspace{0.2em} \big(\textbf{I} \otimes \textbf{R} \big) \hspace{0.2em} \big(\textbf{I} \otimes \overline{\textbf{G}} \big) -\mu \hspace{0.2em} \big( \textbf{R} \otimes \textbf{I} \big) \hspace{0.2em} \big( \overline{\textbf{G}} \otimes \textbf{I} \big) + \mu^{2} \hspace{0.2em} \boldsymbol{\Pi} \hspace{0.2em} E\big(\textbf{G} \otimes \textbf{G} \big)
\end{split}
\label{eq2.13}
\begin{split}
E[v^{2}] \hspace{0.2em} E\big[\textbf{u}^{T}(n) \textbf{G}(n) \boldsymbol{\Sigma} \textbf{G}(n) \textbf{u}(n) \big]&=\sigma^{2}_{v} \hspace{0.2em} Tr\bigg(E\Big[ \textbf{G}(n) \hspace{0.2em} \textbf{u}(n)\textbf{u}^{T}(n) \hspace{0.2em} \textbf{G}(n) \Big] \boldsymbol{\Sigma}  \bigg)\\
&= \sigma^{2}_{v} \hspace{0.2em} \boldsymbol{\gamma}^{T} \hspace{0.2em} \boldsymbol{\sigma}
\end{split}
\label{eq2.14}
\begin{split}
\boldsymbol{\gamma}&= \text{vec}\Big\{E\big[ \textbf{G}(n) \hspace{0.2em} \textbf{u}(n)\textbf{u}^{T}(n) \hspace{0.2em} \textbf{G}(n) \big]\Big\}\\
&=E \big(\textbf{G} \otimes \textbf{G} \big) \hspace{0.2em}\boldsymbol{\gamma}_{R}
\end{split}
\label{eq2.15}
\begin{split}
E\|\widetilde{\textbf{w}}(n+1)\|_{\boldsymbol{\sigma}}^{2}&= E\|\widetilde{\textbf{w}}(n)\|_{\textbf{F} \hspace{0.2em} \boldsymbol{\sigma}}^{2}
+ \mu^{2} \hspace{0.2em} \sigma^{2}_{v} \hspace{0.2em} \boldsymbol{\gamma}^{T} \hspace{0.2em} \boldsymbol{\sigma}
\end{split}
\label{eq2.16}
\begin{split}
E\|\widetilde{\textbf{w}}(n+1)\|_{\boldsymbol{\sigma}}^{2}&= E\|\widetilde{\textbf{w}}(0)\|^{2}_{\textbf{F}^{n+1} \hspace{0.2em} \boldsymbol{\sigma}}
+ \mu^{2} \hspace{0.2em} \sigma^{2}_{v} \hspace{0.2em} \boldsymbol{\gamma}^{T} \hspace{0.1em} \sum\limits_{i=0}^{n} \textbf{F}^{i}\boldsymbol{\sigma}
\end{split}
\label{eq2.16}
\begin{split}
E\|\widetilde{\textbf{w}}(n+1)\|_{\boldsymbol{\sigma}}^{2}&= E\|\widetilde{\textbf{w}}(n)\|_{\boldsymbol{\sigma}}^{2}- E\|\widetilde{\textbf{w}}(0)\|^{2}_{[\textbf{I}-\textbf{F}]\textbf{F}^{n} \hspace{0.2em} \boldsymbol{\sigma}}
+ \mu^{2} \hspace{0.2em} \sigma^{2}_{v} \hspace{0.2em} \boldsymbol{\gamma}^{T} \hspace{0.1em}  \textbf{F}^{n}\boldsymbol{\sigma}
\end{split}
\label{eq2.18}
\begin{split}
\lim\limits_{n \to \infty}E\|\widetilde{\textbf{w}}(n)\|_{\big(I_{L^{2}}- \textbf{F} \big) \boldsymbol{\sigma}}^{2}&= \mu^{2} \hspace{0.2em} \sigma^{2}_{v} \hspace{0.2em} \boldsymbol{\gamma}^{T} \boldsymbol{\sigma}
\end{split}
\label{eq2.19}
\begin{split}
\lim\limits_{n \to \infty}E\|\widetilde{\textbf{w}}(n)\|^{2}&= \mu^{2} \hspace{0.2em} \sigma^{2}_{v} \hspace{0.2em} \boldsymbol{\gamma}^{T} \big(\textbf{I}_{L^{2}}- \textbf{F} \big)^{-1} \hspace{0.2em} \text{vec}\{ \textbf{I} \}
\end{split}
\label{eq2.17}
\begin{split}
0 < \mu < \text{min}\bigg\{ \frac{1}{\lambda_{max}(\textbf{C}^{-1} \hspace{0.2em}\textbf{D})}, \frac{1}{\text{max}(\lambda(\textbf{H}))} \bigg\}
\end{split}

where .

\section{Simulation Studies and Discussion}
Here the Simulation results are presented for system identification example. First, the proposed algorithm has been simulated for identifying the system () of length  having  active taps with the remaining coefficients being inactive as shown in Fig. 1. 
\begin{figure}[h!]
\centering
\includegraphics [height=30mm,width=85mm]{Sparse_System.eps}
\caption{Sparse system impuse response}
\label{the-label-for-cross-referencing}
\end{figure}
\par
Simulations were performed using zero mean, Gaussian white noise with unit variance ( ). The observation noise  was taken to be zero-mean Gaussian white noise with variance . The performance of the proposed Pt-LMS algorithm was compared with the existing PNLMS and LMS algorithms by plotting the respective learning curves (i.e., normalized MSD in dB vs no. of iterations) which are shown in Fig. 2. The simulation results shown in Fig.  are obtained by plotting the normalized MSD against the iteration index , by averaging over  experiments.
\par
\begin{figure}[h!]
\centering
\includegraphics [height=80mm,width=85mm]{Pt_LMS.eps}
\caption{Leaning curves of Pt-LMS}
\label{the-label-for-cross-referencing}
\end{figure}
\par
Secondly, for theoretical performance comparison purpose, we considered a sparse system of length  having  active taps with the remaining coefficients being inactive. The input was taken to be zero mean, Gaussian white noise with unit variance ( ). The observation noise  was taken to be zero-mean Gaussian white noise with variance . Theoretical and simulation results were compared by plotting the steady-state normalized MSD in dB vs step size value () which are shown in Fig. 3. From Fig. 3, we can see the analytical results are coinciding with simulation results.
\begin{figure}[h!]
\centering
\includegraphics [height=50mm,width=90mm]{Steady_state_MSD_P_LMS.eps}
\caption{Steady-state MSD comparison}
\label{the-label-for-cross-referencing}
\end{figure}
\section{Conclusions}
We presented the performance analysis of Proportionate-type LMS (Pt-LMS) algorithm that is more suitable for real time VLSI applications. The convergence analysis of Pt-LMS algorithm is studied in mean and mean-square sense.
\begin{thebibliography}{1}
\bibitem{1}
J. Radecki, Z. Zilic, and K. Radecka, ``Echo cancellation in IP networks'',
in \emph{Proc. of the 45th Midwest Symposium on Circuits and Systems,} vol. 2, pp. 219-222, August, 2002.

\bibitem{2}
V. V. Krishna, J. Rayala and B. Slade, ``Algorithmic and Implementation
Apsects of Echo Cancellation in Packet Voice Networks'',
in \emph{Proc. 36th Asilomar Conf. Signals, Syst. Comput.,} vol. 2, pp. 1252-1257, 2002.

\bibitem{3}
W. Schreiber, ``Advanced Television Systems for Terrestrial Broadcasting'',
in \emph{Proc. IEEE,} vol. 83, no. 6, pp. 958-981, 1995.

\bibitem{4}
W. Bajwa, J. Haupt, G. Raz and R. Nowak, ``Compressed Channel
Sensing'', in \emph{Prof. IEEE CISS}, pp. 5-10, 2008.

\bibitem{5}
M. Kocic, D. Brady and M. Stojanovic, ``Sparse Equalization for
Real-Time Digital Underwater Acoustic Communications'',
in \emph{Proc. IEEE OCEANS,} vol. 3, pp. 1417-1422, 1995.

\bibitem{6}
D. L . Duttweiler, ``Proportionate normalized least-mean-squares adaptation in echo cancelers'',
in \emph{IEEE Trans. Speech Audio Process.,} vol. 8, no. 5, pp. 508-518, 2000.

\bibitem{7}
S. L. Gay, ``An efficient, fast converging adaptive filter for network echo cancellation,''
in \emph{Proc. 32nd Asilomar Conf. Signals, Syst. Comput.,} vol. 1, pp. 394-398, 1998.

\bibitem{8}
H. Deng and M. Doroslovacki, ``Improving convergence of the PNLMS algorithm for sparse impulse response identification,''
in \emph{IEEE Signal Process. Letters,} vol. 12, no. 3, pp. 181-184, 2005.

\bibitem{9}
J. Benesty and S. L. Gay, ``An improved PNLMS algorithm,''
in \emph{IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP),} vol. 2, pp.1881-1884, 2002.

\bibitem{10}
R. L. Das and M. Chakraborty, ``On convergence of proportionate-type normalized least mean square algorithms,''
in \emph{IEEE Trans. on Circuits and Systems II: Express Briefs,} vol. 62, no. 5, pp. 491-495, May, 2015.

\bibitem{11}
H-C. Shin and A. H. Sayed, ``Mean-square performance of a family of affine projection algorithms,''
in \emph{IEEE Trans. Signal Process.,} vol. 52, no. 1, pp. 90-102, January, 2004.

\bibitem{12}
G. Alexander, \emph{Kronecker products and matrix calculus with applications}, New York:Halsted, 1981.

\bibitem{13}
A. H. Sayed, \emph{Fundamentals of adaptive filtering}, John Wiley and Sons, 2003.

\end{thebibliography}

\end{document} 