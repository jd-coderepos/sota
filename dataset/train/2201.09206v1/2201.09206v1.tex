\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage{booktabs}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{multirow}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{bm}
\usepackage[colorlinks,linkcolor=blue]{hyperref}

\usepackage{cite}
\hyphenation{optical networks semi-conductor IEEE-Xplore}


\begin{document}

\title{A Transformer-Based Feature Segmentation and Region Alignment Method For UAV-View Geo-Localization}

\author{
Ming Dai, Jianhong Hu, Jiedong Zhuang, Enhui Zheng
\thanks{Ming Dai, Jianhong Hu, Jiedong Zhuang, Enhui Zheng are with the Unmanned System Application Technology Research Institute, China Jiliang University, Hangzhou 310018, China (email: s20010802003@cjlu.edu.cn; zjuhjh@126.com; p1901085206@cjlu.edu.cn; ehzheng@cjlu.edu.cn). Enhui Zheng is the Corresponding Author.}
}

\IEEEpubid{\begin{minipage}{\textwidth}\ \
\label{eq1}
\mathcal{Z}_0=[{\ x}_{cls};\mathcal{F}(x_p^1);\mathcal{F}(x_p^2);\cdots{};\mathcal{F}(x_p^N)]+\mathcal{P}

\label{eq2}
\mathcal{L}=[\mathcal{F}(x_p^1);\mathcal{F}(x_p^2);\cdots{};\mathcal{F}(x_p^N)]

\label{eq3}
P^c=\frac{1}{S}\textstyle\sum_{i=1}^S{\mathcal{M}^i} \ \ \ \ c=\{1,2,\cdots{},N\}

\label{eq4}
\mathcal{N}^i=\{
\begin{array}{l}\ \ \ \ \ \ \ \ 
\lfloor{}\frac{N}{n}\rfloor{}\ \ \ \ \ \ \ \ \ \ \ \ \ 
i=\{1,2,\cdots{},n-1\} \\
N-(n-1)\times{}\lfloor{}\frac{N}{n}\rfloor{}\ \ 
\ i=n
\end{array}

\label{eq5}
\mathcal{V}_i=\frac{1}{\mathcal{N}^i}\textstyle\sum_{j=1}^{\mathcal{N}^i}f_i^j\ \ \ \ \ \ i=\{1,2,\cdots{},n\}

\label{eq6}
KLDiv(O_1\vert{}\vert{}O_2)=\sum_{i=1}^Np(O_1^i)\cdot{}log\frac{p(O_1^i)}{q(O_2^i)}

\label{eq7}
 p(x_i)=log(\frac{e^{x_i}}{\sum_je^{x_j}}\ )

\label{eq8}
q(x_i)=\frac{e^{x_i}}{\sum_je^{x_j}}\

\label{eq9}
KLLoss=KLDiv(O_d\Vert{}O_s)+KLDiv(O_s\Vert{}O_d)

\label{eq10}
TL={\lVert{}d(a,p)-d(a,n)+M\rVert{}}_+

\label{eq11}
d(a,x)={\lVert{}a-x\lVert{}}_2\
M=0.3)}} & \multirow{2}{*}{\shortstack{Sampling\\Rate}} & \multicolumn{2}{c}{AP (\%)} 
\\
& & &{DS}&{SD}
\\
\specialrule{0.5pt}{0pt}{0pt}   
 {} & {} & {1} & {83.30} & {79.87} \\
{\checkmark} & {} & {1} & {84.14} & {80.93} \\
{\checkmark} & {\checkmark} & {1} & {84.82} & {81.53} \\
{} & {\checkmark} & {1} & {84.85} & {81.52} \\
{} & {\checkmark} & {2} & {86.36} & {82.69} \\
{} & {\checkmark} & {3} & {\textbf{86.71}} & {\textbf{83.37}} \\
\specialrule{0.75pt}{0pt}{0pt}  
\end{tabular}
}
\end{table}

\textbf{Effect of some other tricks.} For the task of matching the drone-view and the satellite-view images, we adopt three tricks of KLLoss, TripletLoss with margin=0.3, and multiple sampling for the FSRA to improve the performance. As shown in Table VII. Only using KLLoss increases by 0.84\%/1.06\% AP on the task of DroneSatellite / SatelliteDrone. Only using TripletLoss increases by 1.52\%/1.66\% AP. When we use KLLoss and TripletLoss at the same time, the accuracy of AP is not improved much. Thus, we did not use KLLoss but TripletLoss in our model. We guess that TripletLoss and KLLoss are consistent in the same direction of network fitting. In addition, we deploy the sampling strategy as a trick. Based on TripletLoss with margin=0.3. When the number of sampling reaches 2, the AP of FSRA increases by 1.51\%/1.17\% on the task of DroneSatellite / SatelliteDrone. When the number of sampling reaches 3, the AP increases by 1.86\%/1.85\%. The performance improvement obtained by multiple sampling is due to the expansion of the data which can strengthen the fitting of the network and balance the resources from different domains. 


\begin{figure}[!t]
\centering
\includegraphics[width=0.5\textwidth]{fig12}
\caption{Qualitative image retrieval results. (I) Top-5 retrieval results of drone view target localization on University-1652. (II) Top-5 retrieval results of drone navigation on University-1652. The yellow box indicates the true-matched image, and the blue box indicates the false-matched image.}
\label{fig_12}
\end{figure}


\subsection{Visualization Of Qualitative Result}
For the two basic tasks of the University-1652 dataset: drone view target localization and Drone Navigation, we visualize some retrieved results in Fig. \ref{fig_12}. We observe that FSRA can adapt to retrieving the available images from the gallery set in both drone view target localization and drone navigation tasks. In the task of drone view target localization, we randomly take out three drone-view images from the test dataset. For each drone-view image, we take out the top five similar images from the gallery set, and the FSRA obtains completely correct results as in Fig. \ref{fig_12} (I). In the Drone Navigation task, we randomly take out three Satellite-view images from the test dataset. For each Satellite-view image, we also take out the top 5 similar images in the gallery, because there is only one satellite image for each category. The proposed FSRA still achieved completely correct results as in Fig. \ref{fig_12} (II). 



\section{Conclsion}
In this paper, we apply the structure of the Transformer to the field of cross-view geo-localization. The context information contained in the attention mechanism can distinguish more fine-grained features, and explore some associated information. Our experiments prove that the transformer-based FSRA can obtain state-of-the-art performance in the benchmark of the University-1652. In addition, some modules are proposed to improve model performance. HSM is proposed to implement patch-level semantic segmentation, and HAB is proposed to achieve region-level feature alignment. Although experiments shows that the proposed FSRA has strong robustness to feature misalignment and position shifts, there are still many parts that can be further improved. e.g., the structure of Vit can be modified to achieve more amazing performance. the backbone based on Vit-S has an increase in inference time compared to Resnet-50, which will be considered a shortcoming of this method. Besides, we also adopted a multiple sampling strategy to fit the model to a better state. This strategy can achieve a stunning rise, but the disadvantage is that it increases the training time. Finally, some other tricks such as mutual learning and TripletLoss are applied to make the FSRA stronger.
In the field of current geo-localization based on the perspective of drones. It is very necessary to construct a dense geographic dataset that the model can learn more distinctive and fine-grained features to achieve precise positioning. In the future, we will propose a new intensive UAV cross-view geo-localization dataset to meet the requirements of practical applications.


\section*{Acknowledgments}
The research is supported by the National Natural Science Foundation of China (Project 51605462).

{\appendices

\begin{table}[h]
\renewcommand\arraystretch{1.5}
\caption{The models were trained on ground, drone and satellite views, and the accuracy of matching between ground and drone views was tested. Where G refers to ground-view and D refers to drone-view.}
\label{table8}
\resizebox{1.0\hsize}{!}{
\begin{tabular}{c|c|ccc}
\specialrule{0.75pt}{0pt}{0pt}   

Model & Direction & R@1 & R@Top1\% &AP\\
\specialrule{0.5pt}{0pt}{0pt}   
\multirow{2}{*}{University-1652} & {GD} & {0.85} & {20.36} & {0.71} \\
& {DG} & {0.99}& {15.07} & {1.11}\\
\multirow{2}{*}{LPN} & {GD} & {0.85} & {20.47} & {0.94} \\
& {DG} & {1.70}& {17.41} & {1.70}\\
\multirow{2}{*}{FSRA(ours)} & {GD} & {1.94} & {31.91} & {1.67} \\
& {DG} & {2.75}& {24.92} & {2.63}\\
\specialrule{0.75pt}{0pt}{0pt}
\end{tabular}
}
\end{table}

\section{Does it work for ground view?}
The drone-view can be used as an intermediate view between the satellite and ground view because there is a 90 degree deviation between the ground-view image and the satellite-view image, and the occlusion between objects, the drastic differences in viewpoints and even the temporal gap between scenes make it very challenging. We try to use the drone-view to match the ground-view and thus indirectly reduce the difficulty of the matching between ground and satellite. For fairness, we apply the same learning strategy to the different models (all using only classification loss). The experimental results are shown in Table VIII. The proposed FSRA has improved somewhat compared with University-1652 and LPN, but still remains in single digits. Matching single-view ground images with UAV images is a huge challenge, mainly because there is a mismatch of shooting angles between ground-view images and UAV-view images, which in turn leads to large differences in the included content.

\begin{figure}[!t]
\centering
\includegraphics[width=0.5\textwidth]{fig13}
\caption{The effect of the variation of numbers of sampling k on the final training results of the model when batchsize=16 is demonstrated, where the results of the R@1 evaluation metric are shown on the left and the results of the AP evaluation metric are shown on the right.}
\label{fig_13}
\end{figure}

\section{Does batchsize have effects on the choice of k?}
In order to verify the effect of batchsize on the choice of the optimal  value, we increased the batchsize from 8 to 16 and conducted experiments for  varying from 1 to 8 (to avoid the effect of TripletLoss positive and negative sample ratios on the experiments, only classification loss was used in the experiments). as shown in Fig. \ref{fig_13}, when the batchsize increases to 16, the optimal hyperparameter  should be chosen to be around 5 ( reaches the optimum for batchsize=8). This also verifies our statement in \emph{The impact of sampling on accuracy} that the proportion of samples of the same class in a batch affects the effectiveness of model training. Our proposed multiple sampling strategy can be used not only to expand the severely underrepresented satellite images in University-1652, but also to change the distribution of samples in the batch, so our proposed multiple sampling strategy needs to select the best  value according to the actual batchsize. We conclude that  is optimal when batchsize=8 and  is optimal when batchsize=16. It should also be noted that increasing the value of  increases the model training time exponentially, but does not have any effect on the inference process.
}



\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,FSRA}


\end{document}
