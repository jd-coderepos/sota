
\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  \usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{soul}
\usepackage{multirow, booktabs}
\usepackage{siunitx}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{array,multirow,graphicx}
\usepackage{mathtools}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{xcolor}
\usepackage{bbm}
\usepackage{arydshln}
\usepackage{tabularx}
\newcommand{\STAB}[1]{\begin{tabular}{@{}c@{}}#1\end{tabular}}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{hyperref}

\hypersetup{colorlinks,linkcolor={blue},citecolor={green}} 
\IEEEoverridecommandlockouts                              

\overrideIEEEmargins                                      \newcommand{\myparagraph}[1]{\vspace{1pt}\noindent{\bf #1}}
\setlength\parindent{0pt}
\newcommand{\sota}{state-of-the-art }
\newcommand{\wildtrack}{WildTrack }
\newcommand{\multiviewx}{MultiViewX }
\newcommand{\mvdet}{MVDet}
\providecommand{\eg}[0]{e.g\xperiod}
\newcommand{\etal}{\textit{et al.}}



\title{\LARGE \bf
Bringing Generalization to Deep Multi-View Pedestrian Detection
}


\author{Jeet Vora, Swetanjal Dutta, Kanishk Jain, Shyamgopal Karthik, Vineet Gandhi \thanks{ CVIT, KCIS, International Institute for Information Technology, Hyderabad, India
        {\tt\footnotesize jeet.vora@research.iiit.ac.in}}\thanks{University of T{\.u}bingen, Germany}}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


\begin{abstract}



Multi-view Detection (MVD) is highly effective for occlusion reasoning in a crowded environment. While recent works using deep learning have made significant advances in the field, they have overlooked the generalization aspect, which makes them \emph{impractical for real-world deployment}. The key novelty of our work is to \emph{formalize} three critical forms of generalization and \emph{propose experiments to evaluate them}:  generalization with i) a varying number of cameras, ii) varying camera positions, and finally, iii) to new scenes. We find that existing state-of-the-art models show poor generalization by overfitting to a single scene and camera configuration. To address the concerns: (a) we propose a novel Generalized MVD (GMVD) dataset, assimilating diverse scenes with changing daytime, camera configurations, varying number of cameras, and (b) we discuss the properties essential to bring generalization to MVD and propose a barebones model to incorporate them. We perform a comprehensive set of experiments on the WildTrack, MultiViewX and the GMVD datasets to motivate the necessity to evaluate generalization abilities of MVD methods and to demonstrate the efficacy of the proposed approach. The code and the proposed dataset can be found at \url{https://github.com/jeetv/GMVD}



\end{abstract}


\section{Introduction}

\begin{minipage}{0.45\textwidth}
``Essentially all models are wrong, but some are useful.''
\end{minipage}
\
I = s \begin{pmatrix} x\\ y\\ 1\end{pmatrix} &= K[R|t]\begin{pmatrix} X\\ Y\\ Z\\ 1\end{pmatrix} = P \begin{pmatrix} X\\ Y\\ Z\\1\end{pmatrix}

F = \frac{\sum_{i=1}^{N}fm_{i}}{N}.

L(p,g) = \frac{\sigma(p,g)}{\sigma(p) \times \sigma(g)} - \sum_{i}g_{i} \log \left( \frac{g_{i}}{p_{i}} \right), 
 where  is the covariance of  and ,  is the standard deviation of  and  is the standard deviation of . The loss function was selected empirically using the scene generalization experiment, i.e. training on \multiviewx and testing on \wildtrack, where using KLDiv+CC gave best results (compared with MSE, CC or KLDiv alone). 


\begin{table*}[t]
\centering
\caption{Comparison against the \sota methods. Our method refers to the proposed model in Section~\ref{sec:method}. We made five runs for some of the experiments and the variances are presented in the bracket.}
\resizebox{\textwidth}{!}{\begin{tabular}{@{}lc|cccc|cccc@{}}
\toprule
\multirow{2}{*}{Method} &
  \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}ImageNet\\ (pre-train)\end{tabular}} &
  \multicolumn{4}{c|}{\wildtrack} &
  \multicolumn{4}{c}{\multiviewx} \\ \cmidrule(l){3-10} 
                 &     & MODA  & MODP  & Prec  & Recall & MODA  & MODP  & Prec  & Recall \\ \midrule
RCNN  Clustering~\cite{xu2016multi} &     & 11.3           & 18.4           & 68.0             & 43.0              & 18.7           & 46.4           & 63.5           & 43.9            \\
POM-CNN~\cite{Fleuret2008MulticameraPT}          &     & 23.2           & 30.5           & 75.0             & 55.0              & -              & -              & -              & -               \\
Lopez-Cifuentes \etal~\cite{LpezCifuentes2018SemanticDM}    &     & 39.0           & 55.0           & -          & -          & -              & -              & -              & -               \\
Lima \etal~\cite{Lima2021GeneralizableM3}      &     & 56.9           & 67.3           & 80.8           & 74.6            & -              & -              & -              & -               \\
DeepMCD~\cite{Chavdarova2017DeepMP}          &     & 67.8           & 64.2           & 85.0             & 82.0              & 70.0           & 73.0           & 85.7           & 83.3            \\
Deep-Occlusion \cite{Baqu2017DeepOR}  &     & 74.1           & 53.8           & 95.0             & 80.0              & 75.2           & 54.7           & 97.8           & 80.2            \\
MVDet~\cite{hou2020multiview}            &     & 88.2           & 75.7           & 94.7           & 93.6            & 83.9           & 79.6           & 96.8           & 86.7            \\
MVDeTr~\cite{hou2021multiview}    &   \checkmark  & 91.5           & 82.1           & 97.4           & 94.0            & 93.7           & 91.3           & 99.5           & 94.2            \\
SHOT~\cite{song2021stacked}    &     & 90.2           & 76.5           & 96.1           & 94.0            & 88.3           & 82.0           & 96.6           & 91.5            \\ \bottomrule
Ours             &     & 87.2(0.6) & 74.5(0.4) & 93.8(1.6) & 93.4(1.8)  & 78.6(0.9) & 78.1(0.4) & 96.8(0.5) & 81.3(0.9)  \\

Ours             & \checkmark & 85.4(0.4) & {76.7}(0.2) & {95.2}(0.4) & 89.9(0.8)  & 86.9(0.2) & 79.8(0.1) & {97.2}(0.2) & 89.6(0.2)  \\
Ours (DropView) & \checkmark & 86.7(0.4) & 76.2(0.2) & 95.1(0.3) & 91.4(0.6) & 88.2(0.1) & 79.9(0.0) & 96.8(0.2) & 91.2(0.1) \\
\bottomrule
\end{tabular}}

\label{tab:sota_table}
\end{table*}


\section{Experiments}
\subsection{Experimental setup}
\textbf{Datasets:} In addition to our proposed GMVD dataset, we use the WildTrack and MultiViewX datasets. The \emph{WildTrack} dataset consists of 7 static calibrated cameras with overlapping fields of view,  covering an area of  . The dataset comprises a single 200 second sequence annotated at 2 fps. The image resolution is 1080  1920 pixels. The ground plane grid is discretized into a  grid, where each grid cell is 2.5  square. On average, the dataset captures 23.8 persons per frame. The \emph{\multiviewx} dataset is a synthetic dataset which has similar configurations as the \emph{\wildtrack} dataset. However, it consists of 6 static calibrated cameras with overlapping fields of view and 400 synchronized frames of resolution 1080  1920 annotated at 2 fps for ground-truth covering an area of  . The ground plane grid is discretized into a  grid, where each grid cell is 2.5  square. On average, the dataset captures 40 persons per frame. For both datasets, we use the first 90\% frames in training and the last 10\% frames for testing, as done in previous work~\cite{hou2020multiview, Chavdarova2018WILDTRACKAM}. 


\textbf{Evaluation metrics:} We use the standard evaluation metrics proposed in \cite{Chavdarova2018WILDTRACKAM}. \emph{Multiple Object Detection Accuracy} (MODA) is the primary performance indicator that accounts for normalized missed detections and false positives, i.e., it considers both false negatives and false positives. \emph{Multiple Object Detection Precision} (MODP) assesses the localization precision \cite{Kasturi2009FrameworkFP}. \emph{Precision} and \emph{Recall} is calculated by Precision = TP/(TP+FP) and Recall = TP/(TP+FN) respectively; where TP, FP and FN are True Positives, False Positives, False Negatives. A threshold of 0.5 meters is used to determine the true positives.

\textbf{State of the Art comparisons:}
We compare against nine different methods. The set includes one monocular object detection baseline (referred to as RCNN clustering~\cite{xu2016multi}); a classical probabilistic occupancy map method~\cite{Fleuret2008MulticameraPT}; four anchor based methods~\cite{Lima2021GeneralizableM3,Baqu2017DeepOR,Chavdarova2017DeepMP,LpezCifuentes2018SemanticDM} and three recent end-to-end trainable deep MVD approaches ~\cite{hou2020multiview,hou2021multiview,song2021stacked}. For generalization experiments, we only compare against the recent \sota methods MVDet~\cite{hou2020multiview}, MVDetr~\cite{hou2021multiview} and SHOT~\cite{song2021stacked}.



\subsection{Implementation Details}

Down sampled images of  pixels serve as an input to the model. The feature extracted from ResNet-18 has  channel features, which is bilinearly interpolated to get the shape of . These  extracted features are projected onto top view to obtain  sized features for {\it N} viewpoints, which are average pooled to obtain the ground plane grid shape of .  and  vary from scene-to-scene, depending on the area of ground plane. 

The spatial aggregation has three layers of dilated convolution with a  kernel size and dilation factor of 1, 2, and 4. Training is done for ten epochs with early stopping; we set batch size as 1, SGD optimizer with momentum = 0.9 has been used with one-cycle learning rate scheduler. A probability of  or more on the occupancy grid is considered a detection. For GMVD experiments,  is determined using MultiViewX as a validation set, and for other experiments, we use  in alignment with the previous works. Non-Maximal Suppression (NMS) is applied with a spatial resolution of 0.5m. All training and testing have been performed on a single Nvidia GTX 1080 Ti GPU. Unless specifically mentioned, we always use pre-trained ImageNet~\cite{Deng2009ImageNetAL} weights while training our proposed model.

\begin{table*}[t]
\centering
\caption{Results for evaluating with a varying number of cameras. The model is trained on all 7 cameras on WildTrack, and is tested on 2 different sets of 4 cameras each.}
\resizebox{0.7\linewidth}{!}{\begin{tabular}{@{}lcccccccc@{}}
\toprule
      & \multicolumn{4}{c}{Inference on \{1,3,5,7\}} & \multicolumn{4}{c}{Inference on \{2,4,5,6\}} \\ \midrule
        Method  & MODA      & MODP     & Prec     & Recall     & MODA     & MODP     & Prec      & Recall     \\ \midrule
MVDet         & 38.9      & 71.5     & \textbf{93.8}     & 41.6       & 16.2     & 47.6     & 80.3      & 21.4       \\
MVDeTr        & 55.8      & \textbf{76.7}     & 80.8     & 73.2       & 34.6     & 69.2     & 68.6     & 63.8       \\
SHOT          & 66.6      & 75.1     & 91.0     & 73.9       & 46.3     & 67.8     & 88.2      & 53.5       \\
Ours   & 76.5      & 74.0     & 91.7     & 84.0       & \textbf{79.3}     & 71.4     & \textbf{91.1}      & 87.9             \\
Ours (DropView) & \textbf{77.0}      & 74.5     & 90.3     & \textbf{86.2}       & 79.2     & 72.5     & 88.6      & \textbf{90.9}            \\ \bottomrule
\end{tabular}
}

\label{tab:per_inv}
\end{table*}


\subsection{Results}
Like prior works, we evaluate our approach on the WildTrack and MultiViewX datasets in Table~\ref{tab:sota_table}. We find that our proposed models attains satisfactory performance on the test sets of both WildTrack (best MODA score of 87.2)  and MultiViewX (best MODA score of 88.2). This is slightly worse than the recently proposed methods~\cite{hou2021multiview,song2021stacked}, but is far superior to the performance of the classical and the anchor-based MVD methods. However, we would like to highlight that the traditional evaluation protocol is highly misleading since the train and test sets have significant overlap, thereby encouraging overfitting. Therefore, we emphasize the evaluation across a varying number of cameras, changing camera configurations, and on new scenes. 


\textbf{Generalization to Varying Number of Cameras:} An interesting scenario that can potentially occur in practical scenarios is the loss of some camera feeds due to various issues. In this case, a model trained with 7 cameras, may need to be able to perform inference with just 4 cameras. To simulate this setting, we train all the models (MVDet, MVDeTr, SHOT and Ours) on all 7 cameras and test them on 2 different sets of 4 cameras (\texttt{\{1,3,5,7\}},\texttt{\{2,4,5,6\}}) in Table~\ref{tab:per_inv}. Our proposed model is able to naturally work in this setting without any issues. For MVDet, MVDeTr, and SHOT, we randomly duplicate 3 of these views to ensure that 7 views are available. We observe that the performance of MVDet, MVDeTr, and SHOT degrades drastically when evaluated in this setting. When trained with the DropView regularization, our proposed model outperforms these methods by a huge margin (MODA of 77.0 vs 66.6 and 79.2 vs 46.3). This experiment clearly illustrates the need for the architectures to automatically work with an arbitrary number of views. Furthermore, since MVDet, MVDeTr, and SHOT learn a separate spatial aggregation module for each view, the spatial aggregation module overfits to the order of input cameras (indicated by the significant performance variations across the two sets). Future works should ensure that the model has permutation invariance to the order of input views in addition to working with an arbitrary number of views.

\begin{table}[t]
\centering
\caption{Scene Generalization : Evaluation of our method while training on synthetic dataset (MultiViewX) and testing on real dataset (WildTrack). Camera 7 of the \wildtrack  dataset was discarded for the experiments in the first five rows.}
\resizebox{\linewidth}{!}{\begin{tabular}{@{}lcccccc@{}}
\toprule
Method & \begin{tabular}[c]{@{}c@{}}Inference on\\ total cameras\end{tabular} & \begin{tabular}[c]{@{}c@{}}ImageNet\\ (pre-train)\end{tabular} & MODA  & MODP   & Prec   & Recall \\ \midrule
MVDet & 6 &    & 17.0   & 65.8 & 60.5 & 48.8 \\
MVDeTr & 6 & \checkmark & 50.2 & 69.1 & 74.0 & 77.3 \\
SHOT   & 6 &  & 53.6 & 72.0 & 75.2 & 79.8 \\ \midrule
Ours  & 6 & \checkmark & 60.1 & 72.1 & 75.6 & \textbf{88.7} \\ 
Ours (DropView) & 6 & \checkmark & 66.1 & 72.2 & 82.0 & 84.7 \\ 
Ours & 7 & \checkmark & 69.4 & 72.96 & \textbf{83.7} & 86.14 \\
Ours (DropView) & 7 & \checkmark & \textbf{70.7} & \textbf{73.8} & \textbf{89.1} & 80.6 \\\bottomrule
\end{tabular}}
\label{tab:scene_general}
\end{table}
\begin{table*}[t]
\centering
\caption{Experiments on the \wildtrack dataset with changing camera configurations}
\resizebox{0.7\linewidth}{!}{\begin{tabular}{@{}lllccccccccc@{}}
\toprule
 &  &  & \multicolumn{4}{c}{Inference on \{2,4,5,6\}} & & \multicolumn{4}{c}{Inference on \{1,3,5,7\}} \\ \midrule
 &  & Method  & MODA  & MODP  & Prec & Recall && MODA  & MODP  & Prec & Recall \\ \midrule
\multirow{10}{*}{{\rotatebox[origin=c]{90}{Trained on camera set}}} & 
\multirow{5}{*}{{\rotatebox[origin=c]{90}{\{2,4,5,6\}}}}
& MVDet  & \textbf{85.2} & 72.2 & 92.6 & \textbf{92.5} && 43.2 & 68.2 & \textbf{94.6} & 45.8\\ &
& MVDeTr & 75.4 & \textbf{79.5} & \textbf{96.9} & 77.9 && 41.7 & \textbf{73.7} & 92 & 45.7 \\  & 
& SHOT   & 81.9 & 74.1 & 94.1 & 87.4 && 51.4 & 72.5 & 94.4 & 54.6\\  &              
& Ours   & 81.8 & 73.5 & 93.5 & 87.9 && 66.5 & 71.4 & 94.3 & 70.8\\ &
& Ours (DropView) & 84 & 72.9 & 92.4 & 91.6  && \textbf{75.1} & 71.1 & 94.3 & \textbf{79.9}\\\cmidrule{2-12}
& \multirow{5}{*}{{\rotatebox[origin=c]{90}{\{1,3,5,7\}}}} 
& MVDet     & 27.8 & \textbf{68.7} & \textbf{90.8} & 31.0 && 78.2 & 73.6 & 89.5 & \textbf{88.6} \\  & 
& MVDeTr    & 5.6 & 65.5 & 62.4 & 14.0 && 72.5 & \textbf{78.9} & 95 & 76.5  \\  &            
& SHOT      & 15.3 & 62.9 & 89.2 & 17.4 && 79.7 & 76.4 & \textbf{95.7} & 83.5   \\ & 
& Ours     & 52.4 & 67.4 & 81 & 68.5 && 76.4 & 74.6 & 91.5 & 84.1  \\ &
& Ours (DropView) & \textbf{62.6} & 67.4 & 86.7 & \textbf{73.9}  && \textbf{80.8} & 74.0 & 94.2 & 86 \\ \midrule
\end{tabular}
}

\label{tab:camera_config}
\end{table*}


\begin{table}[t]
\centering
\caption{Changing configuration and scene generalization experiment on the setting introduced in~\cite{song2021stacked}}
\footnotesize
\begin{tabular}{@{}lcccc@{}}
\toprule
Method   & MODA & MODP & Prec & Recall \\ \midrule
MVDet           & 33.0 & 76.5 & 64.5      & 73.4   \\
MVDeTr          & 56.5 & 70.8 & 85.0      & 68.6       \\ 
SHOT            & 49.1 & \textbf{77.0} & 73.3      & \textbf{77.1}   \\
Ours            & 57.8 & 76.5 & 88.7 & 66.3   \\
Ours (DropView) & \textbf{66.1} & 75.8  & \textbf{89.3}  &  75.2       \\ \bottomrule
\end{tabular}

\label{tab:shot_exp}
\end{table}

\begin{table}[t]
\centering
\caption{Evaluation when trained on GMVD training set: first row shows the result on GMVD test set and second row is when tested on \wildtrack dataset. }
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
Inference on & MODA & MODP & Prec & Recall \\ \midrule
GMVD & 68.2 & 76.3 & 91.5  & 75.5   \\
WildTrack & 80.1 & 75.6 & 90.9  & 89.1      \\ \bottomrule
\end{tabular}

\label{tab:gmvd_dataset_exp}
\end{table}
\textbf{Generalization to New Camera Configurations:} Another practical scenario that we explore is when the camera positions are varied between the train and test sets. We train all the models on two sets of camera views and then test the trained models on both sets. The results are provided in Table~\ref{tab:camera_config}. When the models are evaluated on the same camera configuration, all the models have satisfactory performance. However, when evaluated on the different camera configuration, MVDet, MVDeTr, and SHOT see a huge degradation in performance. Our model is fairly robust to the changing camera configuration. Especially when trained with DropView regularization, the resulting model outperforms all other models by over 20 percentage points.



\textbf{Scene Generalization:} Finally, an important concern with the practical utility of MVD methods is that since real-world data is scarce, a trained model should be able to generalize to new scenes. We first evaluate the scene generalization abilities of the MVD methods by training them on MultiViewX and evaluating them on WildTrack in Table~\ref{tab:scene_general}. Our proposed model is able to utilize the extra camera present in the WildTrack dataset and achieves a MODA score of 70.7. This further highlights the benefits of an architecture that works with arbitrary number of views, since the performance during inference can be enhanced by adding more view. However, even without the additional view, our model achieves a MODA score of 66.1, which is much higher than SHOT which only achieves a MODA score of 53.6. 

In addition to this, we perform the scene generalization experiment proposed in \cite{song2021stacked} where the MultiViewX scene is split into two halves, and each half is covered using 3 cameras each. In this setting as well (Table~\ref{tab:shot_exp}), our proposed approach with DropView regularization has a MODA score of 66.1, which is significantly higher than both SHOT (49.1) and MVDeTr (56.5).



\textbf{GMVD Benchmark:} Having shown that our proposed model is capable of comprehensive generalization abilities, we benchmark our proposed approach on the GMVD dataset (Table~\ref{tab:gmvd_dataset_exp}). We train our model on the training set of the GMVD dataset and use MultiViewX dataset for validation. Since each sequence in the training set has a different number of cameras, \emph{none} of the existing methods can be adapted to this setting, since they can be trained only on a \emph{fixed} set of cameras. 
When evaluated on WildTrack, our model is able to achieve a MODA score of 80.1, which is a significant improvement over the results from training on MultiViewX. Notably, this shows that training on our synthetic dataset, we can nearly attain the same performance as training on WildTrack itself. When evaluated on GMVD test set, our model achieves a MODA score of 68.2. The results empirically suggest the difficulty of the GMVD test set, compared to WildTrack and MultiViewX, resulting from a distinct train-test split and the presence of extensive variations. We believe that our dataset can serve two important purposes. The first is as a diverse, synthetic dataset from which a model can be adapted to real-world data. The second is that the GMVD dataset itself can be a challenging benchmark to evaluate the generalization capabilities of MVD methods. In this setting, MultiViewX being used for validation is ideal, since this ensures that no information from the test set is leaked during training. 

\section{Discussion and Future work}
The biggest limitation in the field of Multi-View Detection is that real-world capture of data is extremely challenging due to the difficulty in collecting a dataset with people in addition to the challenges involved in the hardware setup and annotations. The absence of a large, diverse benchmark significantly hampers the progress of this topic. Therefore, the existing WildTrack dataset is extremely valuable for the community. However, due to its limited size and variety, it is not suitable for training and should only be used to evaluate the generalization abilities of the models. In this regard, we hope that our proposed dataset and our barebone model serves as a useful tool in bridging the gap between the theory and real-world application of MVD methods. In our work, we have not explored the use of unsupervised domain adaptation techniques to bridge the gap between the feature distributions of the synthetic and real datasets and the direction is left for exploration in the future work.

\section{Conclusion}
We find the current Multi-View Detection setup severely limited and encouraging models to overfit the training configuration. Therefore, we conceptualize and propose novel experimental setups to evaluate the generalization capabilities of MVD models in a more practical setting. We find the state-of-the-art models to have poor generalization capabilities on our proposed setups. To alleviate this issue, we introduce changes to the feature aggregation strategy, loss function, as well as a novel regularization strategy. With the help of comprehensive experiments, we demonstrate the benefits of our proposed architecture. In addition to this, we propose a diverse, synthetic, but realistic dataset which can be used both as an evaluation benchmark, as well as a training dataset for various MVD methods. Overall, we hope our work plays a crucial role in steering the community towards more practical Multi-View Detection solutions. 

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,egbib}


\clearpage
\appendix




\begin{figure}[t]
    \centering
    \includegraphics[scale=0.25]{images/wild_cam_set.pdf}
    \caption{Camera splits of \wildtrack dataset for changing camera configuration experiment.}
    \label{fig:wildtrack_cam_split}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{images/trad_result.pdf}
    \caption{ Sample frames from \wildtrack and \multiviewx dataset with corresponding occupancy maps of ground truth, our result MVDet, MVDeTr and SHOT for comparison. We can see the clusters forming in the MVDet predictions, in contrast our method gives much sharper and distinct predictions.}
    \label{fig:wildtrack_results}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{images/wildtrack_varying_cam.pdf}
    \caption{Occupancy maps for varying number of cameras on \wildtrack dataset when trained on seven cameras and tested on varying subsets of the cameras.}
    \label{fig:wildtrack_varying_cam}
\end{figure*}

\section{Choice of Loss Function}
\label{sec:choice_loss}

\begin{table}[htbp!]
\begin{center}
\resizebox{\linewidth}{!}{\begin{tabular}{@{}cccccc@{}}
\toprule
\textbf{Method} & \textbf{\begin{tabular}[c]{@{}c@{}}ImageNet\\ (pre-train)\end{tabular}} & \textbf{MODA}  & \textbf{MODP}   & \textbf{Prec}   & \textbf{Recall} \\ \midrule
 MSE & \checkmark & 57.3(0.2) & 72.6(0.0) & 75.6(0.1) & 84.5(0.05) \\
 CC & \checkmark & 55.5(5.5) & \textbf{74.2}(0.4) & 72.1(4.4) & \textbf{89.5}(2.6) \\
 KL & \checkmark & 62.5(0.1) & 73.4(0.04) & \textbf{89.1}(0.0) & 71.3(0.0) \\
 KLCC & \checkmark & \textbf{69.4}(0.6) & \ 72.96(0.2) & 83.74(0.5) & 86.14(0.3) \\ \bottomrule
\end{tabular}}
\end{center}
\caption{Choice of Loss Function: we present an ablation study for our proposed method on the scene generalization experiment. Overall, the model trained with both KL-Divergence and Cross-Correlation achieves the best performance.}
\label{tab:scene_loss_compare}
\end{table}

We ablate the choice of the loss function in Table~\ref{tab:scene_loss_compare} for the scene generalization experiment. We consider the Mean Squared Error (MSE), KL-Divergence(KL), Pearson Cross-Correlation (CC), as well as our chosen loss function (KL+CC). We find that the combination of KL-Divergence and Pearson Cross-Correlation achieves significantly better results than any other loss function. 

\section{Qualitative results}
\label{sec:qualitative_results}

First we show the predicted occupancy maps of MVDet, MVDeTr, SHOT and our method and compare them with the ground truth, in the traditional setting. Subsequently, qualitative results are shown w.r.t to three generalization abilities obtained from both the \wildtrack and \multiviewx datasets.
\subsection{\wildtrack Dataset}
The traditionally evaluated results which contains occupancy maps of ground truth, our method, MVDet, MVDeTr and SHOT are shown in Fig. \ref{fig:wildtrack_results}. The occupancy map from our method which uses average pooling, KLCC loss function and ImageNet pretraining gives us more accurate localization as compared to the base MVDet architecture. The results (maps) are competitive when compared to SHOT and MVDeTr. The maps obtained using MVDeTr are sharper and focused, however, it also has more false positives. 


\textbf{Varying number of cameras:} The output occupancy map for varying number of cameras are shown in Fig. \ref{fig:wildtrack_varying_cam}. \wildtrack consists of seven cameras, we show the results inferred with three cameras upto six cameras. As the number of views are increasing, we get an accurately localized occupancy map. 


\textbf{Changing camera configurations:} The output occupancy map for cross subset evaluation are shown in Fig. \ref{fig:wildtrack_cam_subset}. Here, we have the occupancy maps for a model trained on one set and tested on other set. For example, trained on camera views one, three, five and seven and tested on cameras two, four, five and six or vice-versa like the camera splits shown in Figure \ref{fig:wildtrack_cam_split}. Clearly the pre-training is improving localization in both the methods. Furthermore, our method with average pooling is better at disambiguating the occlusions and also giving brighter outputs (resulting in sharp maxima's).


\subsection{\multiviewx Dataset}
In this subsection the qualitative results for \multiviewx dataset are been shown. We consider similar configurations as in the Wildtrack dataset. The obtained results clearly indicates the improvements our method brings over the MVDet, MVDeTr and SHOT model and observations are similar to that of the Wildtrack dataset.  Fig. \ref{fig:wildtrack_results} shows the traditionally evaluated results.


\textbf{Varying number of cameras:} The output occupancy map for varying number of cameras are shown in Fig. \ref{fig:mx_varying_cam}. \multiviewx consists of six cameras, we show the results inferred with three cameras upto five cameras. As the number of views are increasing, we get an accurately localized occupancy map. 

\textbf{Changing camera configurations:} The output occupancy map for cross subset evaluation are shown in Fig. \ref{fig:mx_cam_subset}. Here, we have the occupancy maps for a model trained on one set and tested on other set. For example, trained on camera views one, three, and four and tested on cameras two, five and six or vice-versa, the camera splits are shown in Figure \ref{fig:mx_cam_split} and their results are shown in Table \ref{tab:camera_config_supp}.


\begin{figure*}[t!]
    \centering
    \includegraphics[scale=0.2]{images/mx_cam_set.pdf}
    \caption{Camera splits of \multiviewx dataset for changing camera configuration experiment shown in Table \ref{tab:camera_config_supp}.}
    \label{fig:mx_cam_split}
\end{figure*}

\begin{table}[h!]
\centering
\resizebox{\linewidth}{!}{\begin{tabular}{@{}lllccccccccc@{}}
\toprule
 &  &  & \multicolumn{4}{c}{Inference on \{1,3,4\}} & & \multicolumn{4}{c}{Inference on \{2,5,6\}} \\ \midrule
 &  & Method  & MODA  & MODP  & Prec & Recall && MODA  & MODP  & Prec & Recall \\ \midrule
\multirow{10}{*}{{\rotatebox[origin=c]{90}{Trained on camera set}}} & 
\multirow{5}{*}{{\rotatebox[origin=c]{90}{\{1,3,4\}}}}
& MVDet  & 72 & 76.1 & 93.5 & 77.4 && 46.3 &66.4 &94.5 &49.1\\ &
& MVDeTr & \textbf{77.4} & \textbf{85.1} & 97.9 & 79 && 60.4 & 71.3 & 95.4 & 63.5\\  & 
& SHOT   & 74.3 & 76.3 & 94.1 & \textbf{79.3} && 37.3 & 67 & 67.5 & \textbf{72.1}\\  &
& Ours   & 67.7 & 76.4 & 96.2 & 70.5 && 59.6 & 73.4 & 94.7 & 63.2 \\ &
& Ours (DropView) & 67.3 & 75.3 & \textbf{98.4} & 68.5 && \textbf{62.9} & \textbf{73.6} & \textbf{96.3} & 65.4\\\cmidrule{2-12}
& \multirow{5}{*}{{\rotatebox[origin=c]{90}{\{2,5,6\}}}} 
& MVDet     & 34.3 & 66.2 & 93.8 & 36.7 && 77.6 & 77.4 & 93.8 & 83.1 \\  & 
& MVDeTr    & 51.1 & 72.1 & \textbf{94.9} & 54 && \textbf{83.1} & \textbf{87.1} & \textbf{97.8} & \textbf{85}\\  &            
& SHOT      & 47.3 & \textbf{73} & 94.2 & 50.3 && 80.7 & 78.7 & 96.1 & 84.1\\ & 
& Ours     & 45.8 & 71.8 & 94.5 & 48.6 && 76.1 & 78.7 & 95.9 & 79.5 \\ &
& Ours (DropView) & \textbf{53.4} & 71.6 & 88.2 & \textbf{61.6} && 75.2 & 77.4 & 92.8 & 81.5\\ \midrule
\end{tabular}
}
\caption{Experiments on the \multiviewx dataset with changing camera configurations}
\label{tab:camera_config_supp}
\end{table}


\subsection{Scene Generalization}
The qualitative results of output occupancy map for cross-dataset evaluation are shown in Fig. \ref{fig:scene_general}, when we train on synthetic dataset (\multiviewx) and test on real dataset (\wildtrack).
First four occupancy maps are the outputs of MVDet, MVDeTr, SHOT and our method when tested on only 6 views of \wildtrack dataset for having a fair comparison with other methods. We also show the output occupancy map when tested on all the views of \wildtrack dataset. Our method provides accurately localized occupancy maps and disambiguate the occlusions as compared to other methods.


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{images/wildtrack_cam_subset.pdf}
    \caption{Result occupancy maps for cross subset evaluation from \wildtrack dataset.}
    \label{fig:wildtrack_cam_subset}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{images/mx_varying_cam.pdf}
    \caption{Occupancy maps for varying number of cameras on \multiviewx dataset when trained on seven cameras and tested on varying subsets of the cameras.}
    \label{fig:mx_varying_cam}
\end{figure*}


\begin{figure*}[t!]
    \centering
    \includegraphics[scale=0.5]{images/mx_cam_subset.pdf}
    \caption{Result occupancy maps for cross subset evaluation from \wildtrack dataset.}
    \label{fig:mx_cam_subset}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\linewidth]{images/scene_general.pdf}
    \caption{Occupancy maps obtained on inference from \wildtrack dataset where the models where trained on the synthetic dataset (\multiviewx).}
    \label{fig:scene_general}
\end{figure*}


\end{document}
