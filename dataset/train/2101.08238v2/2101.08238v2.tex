\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\usepackage{multirow}
\setlength{\arrayrulewidth}{0.25mm}
\renewcommand{\arraystretch}{1.1}
\usepackage{comment}
\usepackage{xcolor}


\usepackage{enumitem}
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy 

\def\iccvPaperID{10559} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi

\begin{document}

\title{AXM-Net: Cross-Modal Alignment and Contextual Attention for Person Re-ID}


\author{Ammarah Farooq
\and
Muhammad Awais
\and
Josef Kittler
\and
Syed Safwan Khalid\\
CVSSP, University of Surrey\\
{\tt\small \{ammarah.farooq, m.a.rana, j.kittler, s.khalid\}@surrey.ac.uk}

}

\maketitle


\begin{abstract}
    Cross-modal person re-identification (Re-ID) is critical for modern video surveillance systems. The key challenge is to align inter-modality representations conforming to semantic information present for a person and ignore background information. In this work, we present a novel convolutional neural network (CNN) based architecture designed to learn semantically \textbf{A}ligned cross-\textbf{M}odal (AXM-Net) visual and textual representations. The underlying building block, \textbf{AXM-Block}, is a unified multi-layer network that dynamically exploits the multi-scale knowledge from both modalities and re-calibrates each modality according to shared semantics. To complement the convolutional design, a contextual attention is applied in text branch to manipulate long-term dependencies. Moreover, we propose contextual attention on local image parts to capture fine-grained details of the person. Our design is unique in its ability to implicitly learn aligned semantics between modalities during feature learning stage. The unified feature learning effectively utilises textual data as a super-annotation signal for visual representation learning and automatically rejects irrelevant information. The entire AXM-Net is trained end-to-end on CUHK-PEDES data. We report results on two tasks, person search and cross-modal Re-ID. The AXM-Net outperforms the current state-of-the-art (SOTA) method by 5.22\% in Rank@1 on the CUHK-PEDES and by 10\% for cross-viewpoint text-to-image Re-ID scenario on CrossRe-ID and CUHK-SYSU datasets.
\end{abstract}

\section{Introduction}
Person re-identification (Re-ID) has become a principal component of intelligent video surveillance systems that aims to retrieve a queried person from a large database of pedestrian images. The database typically contains non-overlapping camera viewpoints with respect to the query images. Depending on the type of information provided as a query, the task is referred to as person Re-ID or person search in the literature. Person search~\cite{li2017person} aims to find a person based on the natural language description of the person while images are provided as a query in person Re-ID.In person search, there is no constraint on the camera viewpoints of the person, i.e., in the visual gallery person may have the same pose for which the description is given. Nevertheless, in both tasks, it is critical to learn discriminative feature representations which are unique to an individual and well-aligned within the class for finer matching.  
 
 \begin{figure}
\centering
        \includegraphics[width=\linewidth]{figures/axm_intro_4.png}
        \footnotesize{\caption{Illustration of semantic alignment for visual and textual features. The semantic information present in the features should be aligned to learn the cross-modality associations.}}
    \label{axm_intro}
\end{figure}
 
 The literature is packed with numerous deep learning based person re-identification approaches. These methods aim to learn robust person representations~\cite{zhou2019omni,dai2019batch}, apply various attention mechanisms~\cite{Xia_2019_ICCV,Chen_2019_ICCV}, look for cross-domain knowledge transfer~\cite{Jing_2020_CVPR,Chen_2019_ICCV_cross} and so on. Cross-modal person Re-ID is another important aspect of Re-ID task~\cite{yan2018person,farooq2020convolutional,farooq2020IJCB,Lu_2020_CVPR,Jing_2020_CVPR}. The dependency on available image queries limits the practical application of a vision-based system. For example, in the case of criminal search, often the CCTV footage (or image) of a criminal is not available. Therefore, police rely on the unique cues of the criminal from the witnesses’ descriptions often given in terms of natural language description. In such cases, with no images available, this descriptive information serves as a query for person Re-ID. Hence, employing a multi-modal Re-ID system can overcome the limitations of image-based systems. 
 
 This work focuses on cross-modal person Re-ID using visual and textual information of the person. The aim of the work is to design a system that semantically aligns cross-modal and cross-pose representations implicitly by focusing on the information present in the two modalities. By implicit alignment we mean alignment of two modalities without using external cues, like segmentation, human body landmarks, attributes prediction from image etc. Note, that existing methods~\cite{aggarwal2020text,wang2020vitaa,jing2018pose} rely on complex external cues and explicit alignment of the feature embeddings. 
 There are several challenges while dealing with two distinct modalities. First, the structure of information in both modalities is quite different for persons. Presumably, images have persons always standing upright while the person description can have any sequence. Second, it is critical to learn a network that can extract the semantics in data instead of memorising corresponding image-text pairs for the identities seen during training. Third, the attributes information present in the features, for example, colour and type of clothes person is wearing, the activity of the person, accessories, etc, should be aligned across the modalities to learn the associations among image parts and textual phrases and disregard background noise (Figure~\ref{axm_intro}). Henceforth, the terms `semantic concepts’ and  `attributes’ of the person are used interchangeably.
 
The main idea of this work is to align the visual and textual features of a person to enable cross-modal or multi-modal search seamlessly. To achieve this goal, we present AXM-Net, a novel convolutional neural network (CNN) based architecture designed to deal with the challenges mentioned above and capable of learning semantically aligned cross-modal feature representations. The underlying building block consists of multiple streams of feature maps capturing a variable amount of intra-modal information and an alignment network that is learnt based on the semantics present in both modalities. The output representations are, hence, attended by the fused cross-modal details. The alignment network leverages multi-context intra-modality information and cross-modal attributes to boost informative concepts and suppress the noisy or background information.
 
 Apart from exploiting inter-modal semantic knowledge, we also propose modality-specific contextual attention mechanisms to effectively extract complementary intra-modality representations. To be specific, we introduce an image part-based contextual attention block to locally attend different spatial parts for finer details. We also note that contextual information from other parts can provide useful information while attending a given spatial part, therefore, we propose to use part-level context sharing. Since the unified feature learning is performed using a CNN based design, the sequential nature of the language modality demands learning long-term associations among the person attributes. For example, a description may contain information about the head (hairs, style) at the beginning follows by a description for the lower body, and again carries information about the head (wearing hat). Hence, we also propose to employ contextual attention to learn these useful associations among the textual attributes.
 Our contributions can be summarised as follows:
 \begin{itemize}[noitemsep,nolistsep,itemindent=0pt,leftmargin=7pt]
     \item Unified cross-modal semantic alignment block (AXM-Block) is proposed to mutually learn representations based on person attributes. To our knowledge, this is the first work to employ implicit semantic alignment across modalities at feature learning stage in the Re-ID setting.
     \item We put forward effective intra-modality contextual attention mechanisms to extract local spatial region based details in vision while exploiting part-level context sharing and to manage inter-dependencies among textual phrases.
     \item Extensive experiments demonstrate the superiority 
     of the proposed AXM-Net over the current SOTA by 5.22\% on the CUHK-PEDES~\cite{li2017person} benchmark and by a wide margin on CrossRe-ID and CUHK-SYSU~\cite{farooq2020convolutional,farooq2020IJCB} in all retrieval scenarios. We also propose a cross-modal protocol for the famous Market-1501 data~\cite{zheng2015scalable}. Note that performance is evaluated based on the stricter but more realistic Re-ID constraints of non-overlapping camera viewpoints. 
 \end{itemize}
 





\section{Related Work}

\noindent\textbf{Person Re-identification: }
Recent vision based Re-ID approaches are mainly based on deep learning techniques~\cite{quan2019auto,li2019global,wang2019spatial,dai2019batch,zhou2019omni,fu2019horizontal,sun2018beyond}. These approaches can be divided into the following categories. ~\cite{xu2018attention,Xia_2019_ICCV,Chen_2019_ICCV,li2018harmonious,song2018mask,cai2019multi} devise attention based mechanisms on the basis of higher order information present in the features, person's body mask or pixel/region level features. ~\cite{Chen_2019_ABD} advocates a diverse attention mechanism along with orthogonality constraint to preserve diversity between the layer activations and weights. Pose based methods ~\cite{jing2018pose,Miao_2019_pose} use pose detectors to learn aligned features across poses. ~\cite{liu2018pose,ge2018fd} use GANs for learning pose-invariant features. Another group of works~\cite{sun2018beyond,wei2018glad, li2017learning} focus on local part based feature learning. The work ~\cite{zhou2019omni} proposes an omni-scale network to learn fused multi-scale features. Our approach also uses multi-scale features, however, the design aims to attend the most useful features across modalities based on the context present in these features.

\begin{figure*}
\centering
        \includegraphics[width=\textwidth]{figures/axm_blockv5.png}
        \footnotesize{\caption{Illustrative diagram of our cross-modal AXM-Net, which generates global visual feature , part based visual feature  and textual feature . Softmax loss  is shared for all the features. Matching losses are trained pairwise with the textual feature for each visual feature.}
    \label{block-diagram}}
\end{figure*}

\noindent\textbf{Person Search: }The task of person search by natural language descriptions was introduced by ~\cite{li2017person}. The proposed method was a CNN-RNN network to learn global level cross-modal features. The following works~\cite{li2017identity,chen2018Pwm} also focused on similar network architectures and a little improvement was observed in performance. Major improvements were shown by ~\cite{chen2018improving,zhang2018deep,zheng2020dual} where researchers start exploiting global-local associations and improving the feature embedding space. More recent works  ~\cite{jing2018pose,wang2020img,wang2020vitaa,aggarwal2020text} started employing auxiliary learning branches to explicitly make use of pose key-points, person attributes, segmentation masks, body parts and textual phrases. These approaches brought improvements as compared to using only global features. Our method also takes advantage of local level features from both modalities in a unique and effective way. Moreover, it is the first design to learn visual and textual features using an integrated convolutional block. 

\noindent\textbf{Cross-modal Person Re-identification:} As mentioned earlier, cross-modal searches help to overcome limitations of vision only systems. A number of approaches have been proposed based on using infra-red images~\cite{Wang_2019_ICCV,Lu_2020_CVPR,9107428} along with RGB images. For text based person Re-ID,~\cite{yan2018person} published the pioneering work and reported results on the CUHK03 and Viper datasets under multiple retrieval scenarios. Recent works~\cite{farooq2020convolutional,farooq2020IJCB} proposed to jointly optimise the two modalities and applied canonical correlation analysis to enhance similarity between the corresponding features. These works also reported results for larger cross-modal test splits including CrossRe-ID and CUHK-SYSU.



\section{Cross-modal AXM-Net Framework}
Figure~\ref{block-diagram} shows the block diagram of the proposed AXM-Net, which includes a unified feature learning network, Visual Contextual Attention (VCA) branch and a Textual Contextual Attention (TCA) branch. The details of each part are presented in the following subsections.


\subsection{Unified Feature Learning using AXM-Block}
Intrinsically, the semantic information present in both modalities is the same as both are describing the same person. However, the corresponding semantic concepts and their relationships may be available at different scales and locations within each modality. We propose a novel idea to align these representations across modalities based on the semantic information during feature extraction phase. For this purpose, we introduce a cross-modal semantic alignment block called as AXM-block (Figure~\ref{axm_block}) which is the basic building block of unified feature learning across modalities. The unified feature learning module consists of stacked AXM-blocks. The design is based on interaction of multi-scale features within and across modalities. Intra-modal multi-scale learning captures the locally critical context with respect to a spatial location. While inter-modal multi-scale alignment dynamically conform each modality with respect to the other. Each stream of channels contains a coarser-to-finer level of semantics present in the input image or text. Likewise, cross-modal multi-scale learning captures and semantically align an unaligned coarser-to-finer level of semantics present in multi-modal input. It is important to understand the cross-modal unified feature learning for semantic alignment to differentiate from typical multi-scale feature learning methods~\cite{zhou2019omni,chen2017person,cai2019multi}.To show this difference, we show baseline results in the experiments section for a model that leverages intra-modal multi-scale information but fails to outperform AXM-Net. The proposed cross-modal cross-scale alignment is the key element of the AXM-Net giving it an advantage over baseline and SOTA methods. The alignment network analyses the semantic concepts in each stream of channels and reinforces the mutually agreed concepts across the  different modalities. The details of the layer-wise architecture of the feature learning backbone in the AXM-Net are provided in supplementary materials.

\begin{figure}
\centering
        \includegraphics[width=\linewidth,height=0.6\linewidth]{figures/axm_block_v5.png}
        \footnotesize{\caption{AXM-Block design. Unified cross-modal feature learning block.  indicates features at receptive field size 33 and 13 for text.}
        \label{axm_block}}
\end{figure}
\noindent\textbf{Architecture of the AXM-Block:}\\ 
\noindent The design of the proposed AXM-block is shown in Figure~\ref{axm_block}. To model the shared semantics across modalities, we first define  to be the visual and textual features maps at receptive fields . We apply a global average pooling operation to collect the channel-wise global concepts present in each feature map generating  vectors of length spanning the entire channel dimension. These vectors are then passed to the semantic alignment network together with the feature maps to get re-calibrated by the mutual semantic information. 


The alignment function  indicates the non-linear mapping to generate a set of scale vectors  corresponding to each input feature map. These scaling vectors carry the importance of each channel with respect to the shared context between modalities. Finally, feature alignment is performed by taking element-wise product between scaling vectors and the corresponding input feature map.


To our knowledge, the proposed system is the first approach to align cross-modal features during feature learning stage in Re-ID. The implicit learning approach of AXM-Net utilises the textual input as a super-annotation signal and does not require auxiliary learning tasks as in the current SOTA~\cite{aggarwal2020text,wang2020vitaa}. This property is beneficial as the textual input does not have background clutter information. Therefore, the effect of background can be reduced by the unified feature learning as shown in Subsection~\ref{sec:Visualisation}. 

\subsection{Visual Contextual Attention (VCA)}
The person Re-ID task depends on the discriminative local cues characterising each individual. The global visual branch focuses on the person as a whole while preserving shared semantics across modalities. However, it is important to pay attention to local cues within the modality. Recent method~\cite{wang2020img} used sum of global channel-wise and spatial attention to attend intra-modal information. However, this way is sub-optimal for attending finer person details related to each body part. To overcome this limitation of typical channel and spatial attention methods, we propose part based visual contextual attention. The key idea is to enhance information in each local image part while taking into account the context from all other parts. To do this, we divide the feature maps into multiple horizontal strips. Each strip is locally attended channel-wise by a learnable multi-layer network. The network looks into each local channel strip and preserves the most informative channels for a given spatial region. The attended region strips are concatenated back into the input-sized features. We use single fused part level feature  for ID classification instead of applying multi-label classification setting, which is computationally efficient.

\subsection{Textual Contextual Attention (TCA)}
The design of unified feature learning is based on simultaneous alignment of modalities from the beginning of the network. This leads to the choice of convolution based feature extraction in both modalities. The output of the convolution operation is characterised by the local receptive field. However, the structure of the free-form natural language descriptions demands to incorporate long-term relationships among the attributes present in the textual representations. To model these long-term dependencies we take inspiration from~\cite{wang2018non,Huang_2019_ICCV} and propose a non-local textual contextual attention by directly computing interactions between pairs of textual features. 
This step is important to complement our convolutional design for unified feature learning. We take feature maps from the last convolution block of the feature learning network as input to TCA. Intuitively, these features represent the important semantic attributes present in the person's description. Each spatial index represents a response from a local spatial region (phrases). The TCA block takes the feature from each spatial position, computes its affinity with all other features(context) and attends input feature maps accordingly.

\subsection{Objective Function}
The loss function for training is the sum of cross-entropy (CE) loss of the three feature branches, triplet-loss~\cite{chechik2009large} and a simple affinity loss defined below. Specifically, the weights of the classifier layer are shared among all the branches to enforce intra-identity feature alignment and the applied CE loss is denoted as . The other losses are optimised in a pairwise manner for each visual feature with textual feature.
\setlength{\abovedisplayskip}{0pt} \setlength{\abovedisplayshortskip}{0pt}

where, , i=1,2,3 determines the proportion of each loss in the total.
Given tuples ,  is a margin () based ranking loss, defined over similarity() of cross-modal positive and negative feature pairs as follows:
\setlength{\abovedisplayskip}{0pt} \setlength{\abovedisplayshortskip}{0pt}


\noindent\textbf{Cross-modal Affinity Loss:}
In order to enhance retrieval performance, we propose to use a simple yet effective affinity loss. We experimented with more complex losses but they bring little advantage. It is based on the affinity between image-text feature pairs. The image features and the corresponding textual features should be aligned and have high similarity, ideally +1 (in case of cosine similarity) and non corresponding features should be unaligned (uncorrelated) and have as low similarity as possible, ideally 0. Given the representations for an image-text pair, affinity is measured in terms of cosine similarity score between image feature  and text feature . The affinity loss is implemented as a binary cross entropy criterion, defined over   where  for matching pairs and  for non-matching ones.
\setlength{\belowdisplayskip}{0pt} \setlength{\belowdisplayshortskip}{0pt}
\setlength{\abovedisplayskip}{0pt} \setlength{\abovedisplayshortskip}{0pt}

where  is sigmoid function applied to features similarity.
\begin{comment}
\begin{table}[]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{c||c|c|c}
\hline
Stage & Module              & Vision Output & Textual Output \\ \hline \hline
input & - & 224  224, \ 3 & 1  56, \ 300 \\ \hline
conv1 & \begin{tabular}[c]{@{}c@{}}7  7 conv, stride=2, 3   3 max pool \\ (1  3 conv, stride=1) on text\end{tabular} & 56  56, \ 64 & 1  56, \ 64  \\ \hline
conv2 & AXM block  2 & 28  28, \ 256     &      1  28, \ 256\\ \hline
conv3 & AXM block  2       &  14  14, \ 384 &   1  14, \ 384 \\ \hline
conv4 & AXM block  2       & 14  14, \ 512 &   1  14, \ 512 \\ \hline
conv5 & 1 x 1 conv          & 14  14, \ 512 &   1  14, \ 512 \\ \hline
GAP   & global average pool & 1   1, \  512   & 1   1, \ 512    \\ \hline
\end{tabular}}
\caption{\footnotesize{Architecture of unified feature learning network.}}
\label{architecture}
\end{table}
\end{comment}

\section{Experiments and Results}
\subsection{Implementation Details}
We follow a two stage training strategy to learn the AXM-Net. In the first stage, we focus on learning the textual branch, the VCA branch and all fully connected layers from scratch, while keeping the vision backbone fixed to pretrained ImageNet weights. For the first stage the training follows the standard classification paradigm considering each person as an individual class and only using . We also apply label smoothing to our cross entropy loss. We use batch size 64, weight decay 5e-4 and initial learning rate 0.01 with stochastic gradient descent optimisation. Images are resized to 224  224. Each textual description is mapped to its 300 dimensional word2vec~\cite{mikolov2013distributed} embedding and resized as 1  56  300 where 56 is the maximum sentence length. We adopted random flipping, random erasing for images and random circular shift of sentences as data augmentation. To achieve computational efficiency, we employ depth-wise separable convolutions at each layer. The retrieval performance is measured based on the cosine similarity between the features and reported in terms of Rank@1 and mean average precision ({m}AP).

\begin{table*}[h!]
\centering
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{l|c||cccc}
\hline
\multicolumn{1}{c|} {Model} & Feature Type  & Rank@1 & Rank@5 & Rank@10 & mAP \\ \hline \hline
GNA-RNN \cite{li2017person} & \multirow{7}{*}{global} &  19.05 & - & 53.64      & -    \\
IATV \cite{li2017identity}  &   &   25.94   & - & 60.48  &   -  \\
PWM \cite{chen2018Pwm} & & 27.14 & 49.45 & 61.02 & -\\
DPCE \cite{zheng2020dual}   && 44.40 & 66.26 & 75.07& - \\
GLA \cite{chen2018improving} & & 43.58 & 66.93 & 76.26 & - \\
CMPC + CMPM \cite{zhang2018deep}  & & 49.37 &-& 79.27& - \\
\textbf{Baseline: Multi-scale features + joint ID} & & 52.78 & 72.33 & 80.29 & 49.04 \\
\hline
PMA \cite{jing2018pose} & global + keypoints & 53.81 & 73.54 & 81.23& - \\ \hline
IMG-Net \cite{wang2020img} & global + parts  & 56.48 & 76.89 & 85.01 & - \\ \hline
ViTAA \cite{wang2020vitaa} & \multirow{2}{*}{global + attribute} & 55.97 &75.84 &83.52 & - \\
CMAAM \cite{aggarwal2020text}  &  & 56.68 & 77.18 & 84.86 & - \\
\hline
\textbf{AXM-Net + joint ID} & \multirow{5}{*}{global+ part} & 59.11 &77.46 & 83.80 & 54.24 \\
\textbf{AXM-Net + joint ID + affinity }& & 59.81& 77.43 & 84.27& 54.89   \\
\textbf{AXM-Net + joint ID + triplet} && 60.12 & 77.84 & 84.95& 55.35 \\
\textbf{AXM-Net + joint ID + triplet + affinity} && \textbf{61.9} & \textbf{79.41} & \textbf{85.75} & \textbf{57.38} \\
\hline
\end{tabular}}
\caption{Comparison with SOTA models on the CUHK-PEDES dataset}
\label{SOTA}
\end{table*}


\subsection{Datasets}
\noindent\textbf{CUHK-PEDES:}
The CUHK person description data \cite{li2017person} is the only large-scale benchmark available for cross-modal person search. It has 13003 person IDs with 40,206 images and 80,440 descriptions. There are 11003,1000,1000 pre-defined IDs for training, validation and test sets. The training and test set include 34054/68126 and 3074/6156 images/descriptions respectively. 

\noindent\textbf{CrossRe-ID Dataset:}
For cross-modal Re-ID, we evaluate the models on the protocol introduced by \cite{farooq2020IJCB} on the test split of CUHK-PEDES data. The gallery and query splits have been carefully separated across viewpoints. The descriptions are also varying across viewpoints. The dataset includes 824 unique IDs. There are 1511/3022 and 1096/2200 images/descriptions in gallery and query sets respectively. 

\noindent\textbf{CUHK-SYSU:}
We evaluate our model on the test protocol provided by \cite{farooq2020convolutional}. 
There are 5532 IDs for training and 2099 IDs for testing. The corresponding descriptions have been extracted from CUHK-PEDES data. The final gallery and query splits contain 5070/10140 and 3271/6550 images/descriptions respectively.

\noindent\textbf{Market-1501:}
We also propose a test split for the Market Re-ID dataset. It is a part of the CUHK-PEDES data. We separate the train and test IDs of Market-1501 according to the original protocol. Then we merge the train IDs with the rest of the CUHK-PEDES data to form the training set and evaluate on the test set. The test set is again split into non-overlapping gallery and query sets to define Re-ID specific protocol. Hence, the training set includes 11253 IDs with 34191/68396 images/descriptions. The test set has 750 IDs. The gallery and query sets have 1816/3632 and 1173/2346 images/descriptions respectively. The results are evaluated under more strict single-shot and single-query scenarios for this split.
Please note that this split respect Re-ID protocol and hence, it is different from test split commonly used for Market-1501. 
\subsection{Comparison with State-of-the-Art Methods}
\subsubsection{Results on Person Search}
We summarise the performance of the proposed AXM-Net with the state-of-the-art methods on person search in Table ~\ref{SOTA}. The methods have been grouped according to the type of representations used for learning. We implement the baseline network with multi-scale features for both modalities and a joint classifier layer. The baseline method performs best among all global level techniques which signifies the benefit of having multi-scale features but still it falls behind the other complex methods~\cite{wang2020vitaa,aggarwal2020text,wang2020img} due to lack of cross-modal alignment.
\begin{comment}
Although ~\cite{wang2020img} uses horizontal image feature parts and intra-modal attention, this attention is performed on global feature maps followed by extracting multiple part features. However, our attentive local feature learning block attends local information of the person and its design implicitly induces both channel and spatial attentions. We incorporate context from all spatial parts to attend a given spatial part.  Other recent techniques~\cite{aggarwal2020text,wang2020vitaa}, employ multi-label classification loss over the extracted attribute features for both modalities. The features are obtained by an attribute prediction network and pixel-wise segmentation network respectively. 
\end{comment}
It is worth noting that our method is simple but powerful and enhances the semantic alignment between modalities without any explicit supervision from segmented body parts~\cite{wang2020vitaa} or attributes~\cite{aggarwal2020text}. The proposed AXM-Net with simple  loss outperforms current SOTA by a large margin, achieving over 59\% Rank@1 performance. By using affinity and triplet loss together, we set the new SOTA Rank@1 of 61.9\%. Note that parameter free affinity loss enhances the retrieval and is competitive to the triplet loss.
\begin{comment}
\begin{table}[]
\centering
\resizebox{\linewidth}{!}{\begin{tabular}{c||cl|cl|cl}
\hline
\multirow{2}{*}{Model} & \multicolumn{2}{c|}{V  V}                      & \multicolumn{2}{c|}{T  V} & \multicolumn{2}{c}{VT  V}   \\\cline{2-7}
                       & Rank@1  & \multicolumn{1}{c|}{mAP} & Rank@1 &  \multicolumn{1}{c|}{mAP} & Rank@1               & \multicolumn{1}{c}{mAP}               \\ \hline \hline
                       
\multicolumn{1}{l||}{JT + CCA \cite{farooq2020convolutional}}  & \multicolumn{1}{l}{86.77} & 88.90 & \multicolumn{1}{l}{33.61} & 39.40 & \multicolumn{1}{l}{88.59} & 87.95 \\
\multicolumn{1}{l||}{AXM-Net + joint ID + affinity}  & \multicolumn{1}{l}{95.14} & 96.04 & \multicolumn{1}{l}{44.66} & 50.49 & \multicolumn{1}{l}{95.26} & 95.22 \\
\multicolumn{1}{l||}{AXM-Net + joint ID + triplet}  & \multicolumn{1}{l}{95.02} & 96.00 & \multicolumn{1}{l}{47.33} & 52.58 & \multicolumn{1}{l}{95.75} & 95.41 \\
\multicolumn{1}{l||}{AXM-Net + joint ID + affinity + triplet}  & \multicolumn{1}{l}{94.29} & 98.9 & \multicolumn{1}{l}{46.48} & 52.21 & \multicolumn{1}{l}{94.05} & 93.93 \\
\hline
\end{tabular}}
\footnotesize\caption{Performance comparison on crossRe-ID data. Query  Gallery}
\label{cmReID}
\end{table}
\end{comment}



\begin{table*}[]
\resizebox{\linewidth}{!}{
\begin{tabular}{l||cc|cc|cc||cc|cc|cc}
\hline
\multicolumn{1}{c||}{\multirow{3}{*}{Model}} & \multicolumn{6}{c||}{\textbf{CrossRe-ID}}                  & \multicolumn{6}{c}{\textbf{CUHK-SYSU}}  \\ \cline{2-13} 
\multicolumn{1}{c||}{} &
  \multicolumn{2}{c|}{V  V} &
  \multicolumn{2}{c|}{T  V} &
  \multicolumn{2}{c||}{VT  V} &
  \multicolumn{2}{c|}{V  V} &
  \multicolumn{2}{c|}{T  V} &
  \multicolumn{2}{c}{VT  V} \\ \cline{2-13} 
\multicolumn{1}{c||}{}                       & Rank@1 & mAP   & Rank@1 & mAP   & Rank@1 & mAP   & Rank@1 & mAP   & Rank@1 & mAP   & Rank@1 & mAP   \\ \hline \hline
JT + CCA~\cite{farooq2020IJCB} & 86.77  & 88.90 & 33.61  & 39.40 & 88.59  & 87.95 &  74.13 & 77.16 & 11.37 & 15.78 & 77.68 & 75.8\\
AXM-Net + joint ID + affinity               & 95.14  & 96.04 & 44.66  & 50.49 & 95.26  & 95.22 & 86.00 & 87.75 & 19.93 & 24.82 & 88.72 & 87.02\\
AXM-Net + joint ID + triplet & 95.02  & 96.00 & 47.33  & 52.58 & 95.75  & 95.41 & 86.24 & 88.02 & 20.93 & 26.04 & 87.86 & 86.40 \\
AXM-Net + joint ID + affinity + triplet     & 94.29  & 98.9  & 46.48  & 52.21 & 94.05  & 93.93 & 85.86 & 87.70  & 21.44 & 26.77  & 88.62 & 86.73 \\ \hline
\end{tabular}}
\caption{Performance comparison on cross-modal Re-ID. Query  Gallery}
\label{crossModal}
\end{table*}

\begin{comment}
\begin{table}[]
\centering
\resizebox{\linewidth}{!}{\begin{tabular}{c||cl|cl|cl}
\hline
\multirow{2}{*}{Model} & \multicolumn{2}{c|}{V  V}                      & \multicolumn{2}{c|}{T  V}                      & \multicolumn{2}{c}{VT  V}   \\ \cline{2-7} 
                       & Rank@1               & \multicolumn{1}{c|}{mAP} & Rank@1               & \multicolumn{1}{c|}{mAP} & Rank@1               & \multicolumn{1}{c}{mAP}  \\ \hline \hline
                       
\multicolumn{1}{l||}{JT + CCA \cite{farooq2020convolutional}}  & \multicolumn{1}{l}{74.13} & 77.16 & \multicolumn{1}{l}{11.37} & 15.78 & \multicolumn{1}{l}{77.68} & 75.8 \\

\multicolumn{1}{l||}{AXM-Net + joint ID + affinity}  & \multicolumn{1}{l}{86.00} & 87.75 & \multicolumn{1}{l}{19.93} & 24.82 & \multicolumn{1}{l}{88.72} & 87.02\\
\multicolumn{1}{l||}{AXM-Net + joint ID + triplet}  & \multicolumn{1}{l}{86.24} & 88.02 & \multicolumn{1}{l}{20.93} & 26.04 & \multicolumn{1}{l}{87.86} & 86.40 \\
\multicolumn{1}{l||}{AXM-Net + joint ID + affinity + triplet}  & \multicolumn{1}{l}{85.86} & 87.70  & \multicolumn{1}{l}{21.44} & 26.77  & \multicolumn{1}{l}{88.62} & 86.73  \\
\hline
\end{tabular}}
\footnotesize\caption{Performance comparison on CUHK-SYSU data. Query  Gallery}
\label{SYSU}
\end{table}

\end{comment}


\begin{table*}[]
\centering
\resizebox{0.7\linewidth}{!}{\begin{tabular}{c||cl|cl|cl}
\hline
\multirow{2}{*}{Model} & \multicolumn{2}{c|}{V  V}                      & \multicolumn{2}{c|}{T  V}                      & \multicolumn{2}{c}{VT  V}     \\ \cline{2-7} 
                       & Rank@1               & \multicolumn{1}{c|}{mAP} & Rank@1               & \multicolumn{1}{c|}{mAP} & Rank@1               & \multicolumn{1}{c}{mAP} \\ \hline \hline
                       
\multicolumn{1}{l||}{OSNet~\cite{zhou2019omni} (trained/tested on new training/test splits)} & \multicolumn{1}{l}{48.66} & 54.39  & \multicolumn{1}{l}{-} & - & \multicolumn{1}{l}{-} & - \\

\multicolumn{1}{l||}{AXM-Net + joint ID + affinity}  & \multicolumn{1}{l}{85.73} & 88.01 & \multicolumn{1}{l}{41.86} & 48.09 & \multicolumn{1}{l}{90.53} & 86.56 \\

\multicolumn{1}{l||}{AXM-Net + joint ID + triplet}  & \multicolumn{1}{l}{85.33} & 87.72& \multicolumn{1}{l}{43.73} & 49.44  & \multicolumn{1}{l}{89.86} & 86.53 \\

\multicolumn{1}{l||}{AXM-Net + joint ID + affinity + triplet}  & \multicolumn{1}{l}{84.80} & 87.35 & \multicolumn{1}{l}{43.20} & 49.24 & \multicolumn{1}{l}{90.40} & 86.64 \\
\hline
\end{tabular}}
\caption{\footnotesize{Performance comparison on the proposed cross-modal split for Market-1501 . Query  Gallery}}
\label{Market}
\end{table*}

 

\begin{table}[!htbp]
\center 
\resizebox{\linewidth}{0.25\linewidth}{
\begin{tabular}{c|ccc||c}
\hline
\textbf{Model} & \textbf{SA} & \textbf{VCA} & \textbf{TCA} & \textbf{Rank@1} \\ \hline
Baseline & - & - & -   & 52.78  \\
Model: 1 & \checkmark& - &  -  & 55.90 \\
Model: 2 & - & \checkmark&  -   & 56.99 \\
Model: 3 & - & - &  \checkmark  & 53.25 \\ \hline
Model: 4 & \checkmark& - & \checkmark  & 56.27 \\
Model: 5 & \checkmark& \checkmark&  -   & 58.59 \\ \hline
Unified Visual   & \checkmark& \checkmark&  \checkmark & 54.53   \\
Single Stage & \checkmark& \checkmark&  \checkmark & 55.05 \\
Proposed AXM-Net & \checkmark& \checkmark&  \checkmark & 59.11\\
\hline
\end{tabular}}
\caption{\footnotesize{Ablation study on the AXM-Net on CUHK-PEDES test set}}
\label{ablation}
\end{table}

\begin{table}[!htbp]
\resizebox{\linewidth}{!}{
\begin{tabular}{c|c||c|c}
\hline
Attention Weights & Rank@1 & Pooling Type     & Rank@1 \\ \hline
Separate          & 57.62  & Average (GAP)         & 57.36  \\
Shared            & 57.93  & Max (GMP)             & 58.82  \\ \hline\hline

Feature Drop      & Rank@1 & No. of FC Layers & Rank@1 \\ \hline
Random            & 57.62  & 1                & 58.45  \\
Part              & 58.33  & 2                & 58.06  \\
No drop           & 59.11  & Proposed         & 59.11  \\ \hline
\end{tabular}}
\caption{\footnotesize{Design parameters for the visual contextual attention part branch}}
\label{part-design}
\end{table}



\subsubsection{Results on Cross-modal Re-ID}
We follow the evaluation protocol of ~\cite{farooq2020IJCB} for cross-modal re-identification. In Tables \ref{crossModal}, \ref{Market}, V  V indicates image based search, T  V indicates description to image search and VT  V indicates using both modalities for query and vision as gallery.We report the detailed results including Rank@5 and Rank@10 for all the datasets in the supplementary materials.
For CrossRe-ID and CUHK-SYSU, we compare the results with the recently reported joint training technique followed by applying canonical correlation analysis to embed cross-modal features~\cite{farooq2020IJCB}. It is mentioned as JT+CCA in Tables \ref{crossModal}. For both datasets, the proposed AXM-Net outperformed the previous method by a significant margin in all retrieval scenarios. Note that, now the  T  V indicates a description of a person from a different pose, and the gallery images have different poses. We can witness the potential of semantic alignment across-modalities in this challenging case. The improvement in Rank@1 performance shows the capability of the AXM-Net in matching viewpoint-invariant semantic details. 

Retrieval results on the proposed test split of Market-1501 are reported in Table~\ref{Market}. We trained the OSNet~\cite{zhou2019omni} on the proposed training set and report the results for different scenarios. Focusing on , we see large performance gap between single and multi-modality learning. Note that the OSNet results are different from the ones report in~\cite{zhou2019omni} due to change in split and stricter protocol. Here, the OSNet is trained with much larger data as compared to training set of only Market data which was the case in~\cite{zhou2019omni}. Nevertheless, due to stricter Re-ID protocol the results of OSNet are worse here. As can be seen from the rest of the rows in  under proposed AXM-Net base cross-modal learning the results improved significantly. 
This shows that the matching capability of the single modality is limited by number of samples per class seen during training. It gives an important insight that having textual descriptions also decrease the dependency on number of samples of training set. More importantly, these results established that the textual description of images can act as a super-annotation signal. Experimental results for the AXM-Net proves that the proposed system is able to focus on important cues of the person instead of holistically memorising the image-description pairs. 
\subsection{Component Analysis}
In order to assess the contribution of each component in the complete AXM-Net, we perform ablation study on the proposed framework by adding the components step-by-step. The study is performed on the CUHK-PEDES test set with joint cross-entropy loss  and all hyper-parameters are kept the same for training in all settings. Table ~\ref{ablation} presents the corresponding results. The \textbf{baseline} model has the same architecture as the global visual and textual branch, including multi-scale features for both modalities but without alignment. We abbreviate unified semantic alignment using AXM-blocks as SA in the table. We list our observations as follows:

\begin{figure*}[!htbp]
\centering
        \includegraphics[width=\linewidth,height=0.55 \linewidth]{figures/axm_attn_visual_v4.png}
        \caption{Visualisation of attention maps generated by baseline network and our proposed AXM-Net (warmer colours show higher attention). High-lighted text phrases correspond to the semantic concepts of the person which are precisely attended by the proposed framework.}
    \label{attn_visual}
\end{figure*}

\begin{itemize}[noitemsep,nolistsep,itemindent=0pt,leftmargin=7pt]
    \item \textbf{Effect of Individual Components.} \textbf{Models:\ 1,\ 2,\ 3} correspond to individual component's contribution. We note that each component has boosted the retrieval capability compared to the baseline. Each component is essential to the design of cross-modal Re-ID. The semantic alignment induced by the unified feature learning enhances the useful information across modalities. The auxiliary VCA branch brings the locally informative cues, while the TCA keeps track of textual dependencies. \textbf{Models:\ 4,\ 5} also emphasise the complementary effect of various components together.
    \item \textbf{Effect of Unified Visual Feature Branch.} 
    We experiment with unified visual branch by removing the global branch and keeping only the VCA based part branch. It is indicated as \textbf{Unified Visual} in Table \ref{ablation}. We observe a performance drop of 4.58\% as compared to the proposed design. Separate learning branches for global and part level features complement each other in our design.
    \item \textbf{Effect of Single Stage versus Two Stages of Learning.}
    Working with cross-modal networks is also challenging in terms of the learning policy being adopted. As mentioned earlier, we used two stage training for network learning. We also test a single stage policy in which we tune all parameters together with the same initialisation setting. \textbf{Single stage} model clearly shows the difference between the two policies. We notice that aligning the textual branch with vision in the first stage and then training them jointly is the best strategy. 
    \item \textbf{Design Parameters for VCA Part Branch.}
    We consider several parameters for designing the part-based visual feature branch as presented in Table \ref{part-design}. First of all, we test with separate attention networks for each strip of feature maps. We find that having a shared attention network helps in learning as well as reducing the number of parameters. It supports our idea of context sharing between image parts and signifies the connectedness of various semantic concepts across the strips, for example, a long coat covers both the upper and lower parts of the body. Next, we note that the global max pool helps in capturing local cues by focusing on the highest responses in each region. For each branch of AXM-Net, we also optimise the number of FC layers as it is critical to avoid any performance degradation and network overfitting. We observe from the table that having identical linear layer structure not always implies the best performing solution. Being a richer modality and focusing on information from the whole image, two FC layers help in the global branch to learn better representations. We also consider feature dropping technique~\cite{dai2019batch} to obtain robust features. However, we observed a decrease in Rank@1 using both batchwise random location and horizontal strip (part) drops. 
    \end{itemize}

\subsection{Visualisation} \label{sec:Visualisation}
We analyse the attention maps for the proposed AXM-Net to deduce its capability to learn person specific semantic information from the input data. We show the visual attention maps for the baseline network and our proposed AXM-Net in Figure~\ref{attn_visual}. The visualisations are arranged in three columns (a,b,c), highlighting different aspects of a retrieval system. In all three columns, we observe that AXM-Net ignores person's background with high confidence. Specifically, in the column (b), top example, we note that the attention is focused on the lady whose description is provided, while the baseline model generates a spanned attention. In the column (c), we  observe that with the help of the textual description, the visual attention is refined to minute details in the image like `backpack with a white stripe' and `flip flops'. Overall, the semantic information present in the textual description is highly emphasised across vision, which verifies the feature alignment induced by the inter-modal semantic alignment. Note that the baseline network also has access to multi-scale information, but the proposed method has intelligently aligned this information. We provide query and output retrieval images in the supplementary materials.

\section{Conclusion}

In this work, we present a novel AXM-Net model to address the challenges of cross-modal person re-identification and search. Our innovation involves a unified feature learning block, called AXM-Block, which implicitly aligns the features coming from the visual and textual modalities and effective intra-modal contextual attentions. In contrast to existing methods, the proposed AXM-Net is the first framework that is based on an integrated cross-modal feature alignment and learning stage in Re-ID context. The experimental results show that our network defines new SOTA performance on the CUHK-PEDES benchmark and also demonstrate the potential of the proposed network for more challenging cross-modal person Re-ID applications.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
