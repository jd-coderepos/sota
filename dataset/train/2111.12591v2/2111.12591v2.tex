\begin{figure*}
    \centering
    \includegraphics[width=.9\linewidth]{fig/pipeline.jpg}
    \caption{
\textbf{Overview of the proposed method.}
(The symbols are 
\includegraphics[height=0.7em]{fig/pe.jpg} 
: Positional encoding function ;
\includegraphics[height=0.7em]{fig/warp.jpg}: 
Rigid 3D transformation;
\includegraphics[height=0.7em]{fig/mul.jpg}: 
Matrix-vector multiplication;
\includegraphics[height=0.7em]{fig/pcl.jpg}: 
Source and target).
Given the input point cloud  and , the KPFCN backbone grid-subsample them to  and , and extract geometry features  and  (Sec.~\ref{sec:backbone}). 
The positions informations are encoded as  and  using the 3D relative positional encoding function (Sec.~\ref{sec:3d_pe}).
The position codes and geometry features are then processed by the first \textbf{TMP} layer which includes a \textbf{T}ransformer block with self and cross attentions (Sec.~\ref{sec:transformer}), a differentiable \textbf{M}atching layer (Sec.~\ref{sec:matching}), and a soft \textbf{P}rocurestes layer to estimate the rigid fitting  (Sec.~\ref{sec:prucrustes}).  Based on the rigid fitting estimation, the Repositioning layer adjusts source's position code  (Sec.~\ref{sec:Repositioning}). Given the updated positions and transformed features, the second TMP layer predicts the final matches. }
    \label{fig:pipeline}
\end{figure*}

\section{Problem Definition}
Given a source point clouds  and a target point cloud , where  are the number of points, 
our goal is to find a set of matches , which can be used to recover the warp function  that aligns  to . 
In this paper, we focus on both rigid and deformable point clouds.
In rigid cases, the warp function  is parameterized by a  transformation.
In deformable cases,  is generalized to the dense per-point warp field.
Given the ground truth warp function ,
an inlier match  should satisfies 
,
where  is the Euclidean norm, and  is the tolerance radius for a match. 

\smallskip
\noindent
\textbf{Partial Overlap.}
In real-world range sensor data, due to object motion or viewpoint change, a point in  does not necessarily have a corresponding point in . This is referred to as a non-overlapping point. 
We define the set of overlapping points  for the source point cloud by:

where  is the nearest neighbor search operator.
Then the overlap ratio can be calculated by . Fig.~\ref{fig:4dmatch_eg} shows examples of non-rigid point clouds pairs with different overlap ratio. 


\section{Method}

Fig.~\ref{fig:pipeline} shows the overview of the proposed method.

\subsection{Local geometry feature extraction}\label{sec:backbone}
Given the input source and target point clouds  and ,
We use the function  to extract multi-level geometry features.  does the following two mappings:

Where  and   are the output point clouds,  and  are the extracted features with dimension .

We build  based on the  KPFCN backbone~\cite{thomas2019kpconv}.
KPFCN possesses the inductive bias of translation equivariance and locality, which are well suited for local geometry feature extraction.
The default KPFCN has an UNet-like structure with the same number of pooling/un-pooling layers in the encoder/decoder. We remove the decoder units after the 2nd to the last un-pooling layer from the KPFCN backbone. Thus the output  and  are the down-sampled point clouds. See supplementary for details.
This down-sampling is crucial for efficient computation in the following transformer (Sec.~\ref{sec:transformer}) and matching ((Sec.~\ref{sec:matching}) layers, where the time complexity are both  of the number of points.


\begin{figure*}[!t]
    \centering
\includegraphics[width=1\linewidth]{fig/attention4d.jpg}
\caption{
Visualization of self/cross attention heat maps and the rigid fitting based repositioning.  In the 2nd TMP layer, self-attention expands to cover larger context, and cross attention converges to the corresponding region. 
}
\label{fig:attention4d}
\end{figure*}



\subsection{Relative 3D Positional Encoding}
\label{sec:3d_pe}
The KPFCN backbone learns strict translation invariant features.
Translation invariant could cause ambiguity in scenes that have symmetric structures or globally repetitive geometry patches.
To resolve such ambiguity, we enhance features with the transformation sensitive 3D positional information.

We use the Rotary positional encoding proposed in~\cite{su2021roformer} and extend it to the 3D case.
Given a 3D point , 
and the it's feature .
The position encoding function  is defined by

\begin{center}

\scalebox{0.65}{

}    

\end{center}
where  is a block diagonal matrix. Each diagonal block with size  is defined by 
\begin{center}
\scalebox{0.7}{

}    
\end{center}
where , 
\scalebox{0.7}{} encodes the index in the feature channel.

Compared to the Sinusoidal encoding~\cite{attention_is_all_u_need,carion2020deter}, it has two advantages: 1)  is an orthogonal function, the encoding only changes the feature's direction but not the feature's length, which could potentially stabilize the learning process. 2) The dot product of two encoded features  can be derived to:

which means the relative 3D distance information can be explicitly revealed by the dot product.
We adopt this positional encoding in both the transformer (Sec.~\ref{sec:transformer}) and matching (Sec.~\ref{sec:matching}) layers.
The comparison with Sinusoidal encoding can be found in Sec.~\ref{sec:ablation}.





\subsection{Transformer}\label{sec:transformer}

After the local geometry extraction,  and  are passed through the transformer block with a self attention layer to aggregate the global context, followed by a cross attention layer to exchange information between two point clouds.  Following~\cite{attention_is_all_u_need}, the attention operation selects the relevant information by measuring the similarity between the query vector , and the key vector . 
The output vector is the sum of the value vector  weighted by the similarity scores. 

\medskip
\noindent
\textbf{Self Attention Layer.}
In self attention layer,  and () are obtained from the same point clouds (either from source or the target).  Below shows the example of self attention for the source .
The vectors  are first computed by

where  are learnable projection matrices. The feature  is finally updated by

where  is the attention weight,  denotes a 3-layer fully connected network, and  is the concatenation operator.

\medskip
\noindent
\textbf{Cross-Attention Layer.}
In cross-attention layer, the input vectors  and () are obtained from different point clouds depending on the direction of cross-attention ( or  ). After replacing the contents for , , and , the formations are the same with self attention.





\subsection{Position Aware Feature Matching}\label{sec:matching}

After the transformer layer, we compute the scoring matrix  between two point clouds as 

where  are learnable projection matrices.
The features are position-encoded such that the matching algorithm could take the spatial distance into account.
We apply softmax on both dimensions (kown as the dual-softmax operation~\cite{sun2021loftr,rocco2018neighbourhood}) to convert the scroing matrix to confidence matrix .

Another option for matching is the sinkhorn optimal transport algorthm as in~\cite{sarlin2020superglue}. The  comparison can be seen in Sec.~\ref{sec:ablation}.

\medskip
\noindent
\textbf{Match Prediction.}
Based on the confidence matrix , we select matches with confidence higher than a threshold of , and further enforce with mutual nearest neighbor (MNN) criteria. The influence of  can be found in the supplemental ablation study.


\subsection{Disentanglement between Position and Feature.}
As shown in Fig.~\ref{fig:pipeline}, we hold the position code and geometry features in separated data streams. They are combined only when a similarity matrix needs to be computed.  In the transformer layers  (c.f. Eqn.~\ref{eqn:attention} and ~\ref{eqn:transormer_output}), position code  is only multiplied to query  and key  but not to the value , i.e.  can influence the attention weights but can not be a part of the feature.  
This technique leads to the \textit{Disentanglement} between position and feature. 
Opposed to this is regarded as \textit{Entangled}, which does not hold separate data streams for position and feature, i.e. they are mixed at the very beginning of the transformer.
The comparison is seen in Sec.~\ref{sec:ablation}.


\subsection{Rigid Fitting With Soft Prucrustes}\label{sec:prucrustes}
Given the the confidence matrix , we select the matches set  with top  matching scores, and then fit them with a rigid rotation  and translation  .  Here  is the number of points in the source cloud .
Following~\cite{arun1987procrustes}, the rotation is computed from the SVD decomposition of the matrix .   is obtained with:  

where  is the normalized confidence score. Then the rotation and is computed by
 
Then the translation is obtained with 



\subsection{Repositioning}\label{sec:Repositioning}
The position encoding in Eqn.~\ref{eqn:relative_position} can reveal 3D relative distance information between a pair of points.
However, this distance knowledge is often incorrect for point pairs from two unaligned point clouds.
In other words, this position encoding benefits self-attention but may become irrelevant or noisy signals for cross-attention and matching.
To this end, we adjust the position code  of  with the rigid fitting  from the soft Procrustes layer by
We call it as repositioning. An example is shown in Fig.~\ref{fig:attention4d}.
Intuitively, repositioning pushs corresponding points closer in the position space such that it better informs cross attention and also facilitates position aware matching.

\subsection{Supervision}\label{sec:Supervision}


\medskip
\noindent
\textbf{Matching Loss.}
We minimize the focal loss over the confidence matrix  returned by the matching layer. 
It is defined as:

where  and  are empirically decided focal loss parameters as in~\cite{focal_loss}, and  is the set of ground-truth matches.
During training, we warp  to  using the ground-truth wrap function , and then collect the set of mutual nearest neighbors of the two-point clouds that are below a distance threshold as . 


\medskip
\noindent
\textbf{Warping Loss.}
We minimize the  loss for the point clouds that is warped by the , from the Procrustes layer. It is defined as

where  is the set of overlapping points in , and  is the ground truth warp function.
Intuitively, in rigid cases,  regularizes the optimization by suppressing false-positives in  and also encourages sub-point accuracy for soft correspondences; 
in deformable cases,  tries to approximate a ``root'' pose that aligns the principal part of the overlapping region, e.g. the deer in Fig.~\ref{fig:attention4d}.


 

 
\medskip
\noindent
\textbf{Total Loss.}
As shown in Fig.~\ref{fig:pipeline}, the \textbf{T}ransform-\textbf{M}atching-\textbf{P}rocrustes (TMP) block repeats two times. The total loss combines the matching loss and warpping loss from the 1st and 2nd TMP blocks:

where  is the weighting factor of warpping loss.  We show ablation study for  and the number of TMP blocks (2, 3, and 4) in supplementary.
