\begin{figure*}
    \centering
    \includegraphics[width=.9\linewidth]{fig/pipeline.jpg}
    \caption{
\textbf{Overview of the proposed method.}
(The symbols are 
\includegraphics[height=0.7em]{fig/pe.jpg} 
: Positional encoding function $\Theta(\cdot)$;
\includegraphics[height=0.7em]{fig/warp.jpg}: 
Rigid 3D transformation;
\includegraphics[height=0.7em]{fig/mul.jpg}: 
Matrix-vector multiplication;
\includegraphics[height=0.7em]{fig/pcl.jpg}: 
Source and target).
Given the input point cloud $\mathbf{S}$ and $\mathbf{T}$, the KPFCN backbone grid-subsample them to $\mathbf{\hat{S}}$ and $\hat{\mathbf{T}}$, and extract geometry features $\mathbf{x}^\mathbf{\hat{S}}$ and $\mathbf{x}^\mathbf{\hat{T}}$ (Sec.~\ref{sec:backbone}). 
The positions informations are encoded as $\Theta (\mathbf{\hat{S}})$ and $\Theta (\mathbf{\hat{T}})$ using the 3D relative positional encoding function (Sec.~\ref{sec:3d_pe}).
The position codes and geometry features are then processed by the first \textbf{TMP} layer which includes a \textbf{T}ransformer block with self and cross attentions (Sec.~\ref{sec:transformer}), a differentiable \textbf{M}atching layer (Sec.~\ref{sec:matching}), and a soft \textbf{P}rocurestes layer to estimate the rigid fitting $\mathbf{R,t}$ (Sec.~\ref{sec:prucrustes}).  Based on the rigid fitting estimation, the Repositioning layer adjusts source's position code $\Theta (\mathbf{\hat{S}})$ (Sec.~\ref{sec:Repositioning}). Given the updated positions and transformed features, the second TMP layer predicts the final matches. }
    \label{fig:pipeline}
\end{figure*}

\section{Problem Definition}
Given a source point clouds $\mathbf{S} \in \mathbb{R}^{n\times3}$ and a target point cloud $\mathbf{T} \in \mathbb{R}^{m\times3}$, where $n,m$ are the number of points, 
our goal is to find a set of matches $\mathcal{K}$, which can be used to recover the warp function $\mathcal{W}: \mathbb{R}^3 \mapsto \mathbb{R}^3$ that aligns $\mathbf{S}$ to $\mathbf{T}$. 
In this paper, we focus on both rigid and deformable point clouds.
In rigid cases, the warp function $\mathcal{W}$ is parameterized by a $ SE(3)$ transformation.
In deformable cases, $\mathcal{W}$ is generalized to the dense per-point warp field.
Given the ground truth warp function $\mathcal{W}_{gt}$,
an inlier match $(\mathbf{S}_i\in \mathbb{R}^3, \mathbf{T}_j \in \mathbb{R}^3)\in \mathcal{K}$ should satisfies 
$
||\mathcal{W}_{gt}(\mathbf{S}_i) - \mathbf{T}_j ||_2  < \sigma
$,
where $||\cdot||_2$ is the Euclidean norm, and $\sigma$ is the tolerance radius for a match. 

\smallskip
\noindent
\textbf{Partial Overlap.}
In real-world range sensor data, due to object motion or viewpoint change, a point in $\mathbf{S}$ does not necessarily have a corresponding point in $\mathbf{T}$. This is referred to as a non-overlapping point. 
We define the set of overlapping points $\mathcal{O}^{\mathbf{T}}_{\mathbf{S}}$ for the source point cloud by:
$$
\mathcal{O}^{\mathbf{T}}_{\mathbf{S}}=\{ \mathbf{S}_i| \mathbf{S}_i \in \mathbf{S}\; \wedge \; ||\mathcal{W}_{gt}(\mathbf{S}_i) - \text{NN}(\mathcal{W}(\mathbf{S}_i), \mathbf{T}) ||_2  < \sigma \}
$$
where $\text{NN}(\cdot,\cdot)$ is the nearest neighbor search operator.
Then the overlap ratio can be calculated by $|\mathcal{O}^{\mathbf{T}}_{\mathbf{S}}|/|\mathbf{S}|$. Fig.~\ref{fig:4dmatch_eg} shows examples of non-rigid point clouds pairs with different overlap ratio. 


\section{Method}

Fig.~\ref{fig:pipeline} shows the overview of the proposed method.

\subsection{Local geometry feature extraction}\label{sec:backbone}
Given the input source and target point clouds $\mathbf{S}$ and $\mathbf{T}$,
We use the function $\Phi$ to extract multi-level geometry features. $\Phi$ does the following two mappings:
$$   
(\mathbf{\hat{S}}, \mathbf{x}^\mathbf{\hat{S}} ) = \Phi (\mathbf{S}) ,   \;\;\;\;   (\mathbf{\hat{T}}, \mathbf{x}^\mathbf{\hat{T}} ) = \Phi (\mathbf{T})
$$
Where $\mathbf{\hat{S}} \in \mathbb{R}^{\hat{n}\times3}$ and  $\mathbf{\hat{T}} \in \mathbb{R}^{\hat{m}\times3}$ are the output point clouds, $\mathbf{x}^\mathbf{\hat{S}} \in \mathbb{R}^{\hat{n}\times d}$ and $\mathbf{x}^\mathbf{\hat{T}} \in \mathbb{R}^{\hat{m}\times d}$ are the extracted features with dimension $d=528$.

We build $\Phi$ based on the  KPFCN backbone~\cite{thomas2019kpconv}.
KPFCN possesses the inductive bias of translation equivariance and locality, which are well suited for local geometry feature extraction.
The default KPFCN has an UNet-like structure with the same number of pooling/un-pooling layers in the encoder/decoder. We remove the decoder units after the 2nd to the last un-pooling layer from the KPFCN backbone. Thus the output $\mathbf{\hat{S}}$ and $\mathbf{\hat{T}}$ are the down-sampled point clouds. See supplementary for details.
This down-sampling is crucial for efficient computation in the following transformer (Sec.~\ref{sec:transformer}) and matching ((Sec.~\ref{sec:matching}) layers, where the time complexity are both $O(n^2)$ of the number of points.


\begin{figure*}[!t]
    \centering
\includegraphics[width=1\linewidth]{fig/attention4d.jpg}
\caption{
Visualization of self/cross attention heat maps and the rigid fitting based repositioning.  In the 2nd TMP layer, self-attention expands to cover larger context, and cross attention converges to the corresponding region. 
}
\label{fig:attention4d}
\end{figure*}



\subsection{Relative 3D Positional Encoding}
\label{sec:3d_pe}
The KPFCN backbone learns strict translation invariant features.
Translation invariant could cause ambiguity in scenes that have symmetric structures or globally repetitive geometry patches.
To resolve such ambiguity, we enhance features with the transformation sensitive 3D positional information.

We use the Rotary positional encoding proposed in~\cite{su2021roformer} and extend it to the 3D case.
Given a 3D point $\mathbf{S}_i=(x,y,z)\in \mathbb{R}^3$, 
and the it's feature $\mathbf{x}^{\mathbf{S}}_i\in \mathbb{R}^d$.
The position encoding function $\mathcal{PE}: \mathbb{R}^3 \times \mathbb{R}^d \mapsto  \mathbb{R}^d$ is defined by

\begin{center}
$
\mathcal{PE}(\mathbf{S}_i, \mathbf{x}^\mathbf{ S}_i)=
\Theta(\mathbf{S}_i)  \mathbf{x}^\mathbf{ S}_i=
$
\scalebox{0.65}{
$
\begin{pmatrix}
M_1 & & & \\
& M_2 & & \\
& & \ddots & \\
& & & M_{d/6}
\end{pmatrix}
$
}    
$ \mathbf{x}^\mathbf{ S}_i$
\end{center}
where $\Theta(\mathbf{S}_i)$ is a block diagonal matrix. Each diagonal block with size $6\times 6$ is defined by 
\begin{center}
\scalebox{0.7}{
$
M_k=
\begin{pmatrix}
\cos x\theta_k  & -\sin x\theta_k  &0&0&0&0 \\
\sin x\theta_k  & \cos x\theta_k  &0&0&0&0 \\
0&0&\cos y\theta_k  & -\sin y\theta_k  &0&0\\
0&0&\sin y\theta_k  & \cos y\theta_k  &0&0 \\
0&0&0&0&\cos z\theta_k  & -\sin z\theta_k  \\
0&0&0&0&\sin z\theta_k  & \cos z\theta_k \\
\end{pmatrix} $
}    
\end{center}
where $\theta_k = \frac{1}{10000^{6(k-1)/d}}$, 
\scalebox{0.7}{$k\in [1,2,..,d/6]$} encodes the index in the feature channel.

Compared to the Sinusoidal encoding~\cite{attention_is_all_u_need,carion2020deter}, it has two advantages: 1) $\Theta(\cdot)$ is an orthogonal function, the encoding only changes the feature's direction but not the feature's length, which could potentially stabilize the learning process. 2) The dot product of two encoded features $ \langle \mathcal{PE}(\mathbf{S}_i,  \mathbf{x}^\mathbf{ S}_i), \mathcal{PE}(\mathbf{S}_j,  \mathbf{x}^\mathbf{ S}_j)  \rangle$ can be derived to:
\begin{equation}\label{eqn:relative_position}
[ 
\Theta(\mathbf{S}_i)
\mathbf{x}^\mathbf{ S}_i  
] ^T 
\Theta(\mathbf{S}_j) 
\mathbf{x}^\mathbf{ S}_j 
= (\mathbf{x}^\mathbf{ S}_i)^T \Theta(\mathbf{S}_j-\mathbf{S}_i)  \mathbf{x}^\mathbf{ S}_j
\end{equation}
which means the relative 3D distance information can be explicitly revealed by the dot product.
We adopt this positional encoding in both the transformer (Sec.~\ref{sec:transformer}) and matching (Sec.~\ref{sec:matching}) layers.
The comparison with Sinusoidal encoding can be found in Sec.~\ref{sec:ablation}.





\subsection{Transformer}\label{sec:transformer}

After the local geometry extraction, $\mathbf{x}^\mathbf{\hat{S}}$ and $\mathbf{x}^\mathbf{\hat{T}}$ are passed through the transformer block with a self attention layer to aggregate the global context, followed by a cross attention layer to exchange information between two point clouds.  Following~\cite{attention_is_all_u_need}, the attention operation selects the relevant information by measuring the similarity between the query vector $\mathbf{q}$, and the key vector $\mathbf{k}$. 
The output vector is the sum of the value vector $\mathbf{v}$ weighted by the similarity scores. 

\medskip
\noindent
\textbf{Self Attention Layer.}
In self attention layer, $\mathbf{q}$ and ($\mathbf{k},\mathbf{ v}$) are obtained from the same point clouds (either from source or the target).  Below shows the example of self attention for the source $\mathbf{\hat{S}}$.
The vectors $\mathbf{q},\mathbf{k},\mathbf{ v}$ are first computed by
\begin{equation}\label{eqn:attention}
    \resizebox{0.9\hsize}{!}{$
\mathbf{q}_i =\Theta(\mathbf{\hat{S}}_i)  W_\mathbf{q} \mathbf{x}_i^\mathbf{\hat{S}} \;\;\;\;
\mathbf{k}_j =\Theta(\mathbf{\hat{S}}_j)  W_\mathbf{k} \mathbf{x}_j^\mathbf{\hat{S}} \;\;\;\;
\mathbf{v}_j = W_\mathbf{v} \mathbf{x}_j^\mathbf{\hat{S}} 
$
        }
\end{equation}
where $W_\mathbf{q},W_\mathbf{k},W_\mathbf{v} \in \mathbb{R}^{d\times d}$ are learnable projection matrices. The feature $ \mathbf{x}_i^\mathbf{\hat{S}}$ is finally updated by
\begin{equation}\label{eqn:transormer_output}
\mathbf{x}_i^\mathbf{\hat{S}} \gets  \mathbf{x}_i^\mathbf{\hat{S}} + \text{MLP}( \text{cat}[\mathbf{q}_i,  \sum_j a_{ij}\mathbf{v}_j ] )    
\end{equation}
where $a_{ij}=\text{softmax}(\mathbf{q}_i\mathbf{k}_j^T /\sqrt{d}) $ is the attention weight, $\text{MLP}(\cdot)$ denotes a 3-layer fully connected network, and $\text{cat}[\cdot,\cdot]$ is the concatenation operator.

\medskip
\noindent
\textbf{Cross-Attention Layer.}
In cross-attention layer, the input vectors $\mathbf{q}$ and ($\mathbf{k},\mathbf{ v}$) are obtained from different point clouds depending on the direction of cross-attention ($\mathbf{\hat{S}}\to\mathbf{\hat{T}}$ or $\mathbf{\hat{T}}\to \mathbf{\hat{S}}$ ). After replacing the contents for $\mathbf{q}$, $\mathbf{k}$, and $\mathbf{ v}$, the formations are the same with self attention.





\subsection{Position Aware Feature Matching}\label{sec:matching}

After the transformer layer, we compute the scoring matrix $\mathcal{S}$ between two point clouds as 
\begin{equation}\label{eqn:matching}
\mathcal{S}(i,j)=\frac{1}{\sqrt{d}} \langle \Theta(\mathbf{\hat{S}}_i) W_{\mathbf{\hat{S}}}\mathbf{x}_i^\mathbf{\hat{S}}, \Theta(\mathbf{\hat{T}}_j) W_{\mathbf{\hat{T}}} \mathbf{x}_j^\mathbf{\hat{T}} \rangle 
\end{equation}
where $W_{\mathbf{\hat{S}}},W_{\mathbf{ \hat{ T}} }\in\mathbb{R}^{d\times d}$ are learnable projection matrices.
The features are position-encoded such that the matching algorithm could take the spatial distance into account.
We apply softmax on both dimensions (kown as the dual-softmax operation~\cite{sun2021loftr,rocco2018neighbourhood}) to convert the scroing matrix to confidence matrix $\mathcal{C}$.
$$
\mathcal{C}(i,j) = \text{Softmax}(\mathcal{S}(i,\cdot))\cdot \text{Softmax}(\mathcal{S}(\cdot,j))    
$$
Another option for matching is the sinkhorn optimal transport algorthm as in~\cite{sarlin2020superglue}. The  comparison can be seen in Sec.~\ref{sec:ablation}.

\medskip
\noindent
\textbf{Match Prediction.}
Based on the confidence matrix $\mathcal{C}$, we select matches with confidence higher than a threshold of $\theta_c$, and further enforce with mutual nearest neighbor (MNN) criteria. The influence of $\theta_c$ can be found in the supplemental ablation study.


\subsection{Disentanglement between Position and Feature.}
As shown in Fig.~\ref{fig:pipeline}, we hold the position code and geometry features in separated data streams. They are combined only when a similarity matrix needs to be computed.  In the transformer layers  (c.f. Eqn.~\ref{eqn:attention} and ~\ref{eqn:transormer_output}), position code $\Theta(\cdot)$ is only multiplied to query $\mathbf{q}$ and key $\mathbf{k}$ but not to the value $\mathbf{v}$, i.e. $\Theta(\cdot) $ can influence the attention weights but can not be a part of the feature.  
This technique leads to the \textit{Disentanglement} between position and feature. 
Opposed to this is regarded as \textit{Entangled}, which does not hold separate data streams for position and feature, i.e. they are mixed at the very beginning of the transformer.
The comparison is seen in Sec.~\ref{sec:ablation}.


\subsection{Rigid Fitting With Soft Prucrustes}\label{sec:prucrustes}
Given the the confidence matrix $\mathcal{C}$, we select the matches set $\mathcal{K}_{soft}$ with top $\hat{n}$ matching scores, and then fit them with a rigid rotation $\mathbf{R} \in SO3$ and translation $\mathbf{t}\in \mathbb{R}^3$ .  Here $\hat{n}$ is the number of points in the source cloud $\mathbf{\hat{S}}$.
Following~\cite{arun1987procrustes}, the rotation is computed from the SVD decomposition of the matrix $H = U\Sigma V^T$.  $H\in\mathbb{R}^{3\times3}$ is obtained with:  
$$H=\sum_{(i,j)\in \mathcal{K}_{soft} } \mathcal{\tilde{C}}(i,j) \mathbf{\hat{S}}_i \mathbf{\hat{T}}_j^T $$
where $\mathcal{\tilde{C}}(i,j)$ is the normalized confidence score. Then the rotation and is computed by
$$\mathbf{R}=U\text{diag}(1,1,\text{det}(UV^T))V $$ 
Then the translation is obtained with 
$$
\mathbf{t} = 
\frac{1}{|\mathcal{K}_{soft}|} 
(\sum_{(i,\cdot)\in \mathcal{K}_{soft}}\mathbf{\hat{S}}_i 
- \mathbf{R}
\sum_{(\cdot,j)\in \mathcal{K}_{soft}}
\mathbf{\hat{T}}_j) 
$$


\subsection{Repositioning}\label{sec:Repositioning}
The position encoding in Eqn.~\ref{eqn:relative_position} can reveal 3D relative distance information between a pair of points.
However, this distance knowledge is often incorrect for point pairs from two unaligned point clouds.
In other words, this position encoding benefits self-attention but may become irrelevant or noisy signals for cross-attention and matching.
To this end, we adjust the position code $\Theta(\mathbf{\hat{S}}_i)$ of $\mathbf{\hat{S}}$ with the rigid fitting $\mathbf{R},\mathbf{t}$ from the soft Procrustes layer by
$$
\Theta(\mathbf{\hat{S}}_i) \gets  \Theta( \mathbf{R}\mathbf{\hat{S}}_i + \mathbf{t})
$$We call it as repositioning. An example is shown in Fig.~\ref{fig:attention4d}.
Intuitively, repositioning pushs corresponding points closer in the position space such that it better informs cross attention and also facilitates position aware matching.

\subsection{Supervision}\label{sec:Supervision}


\medskip
\noindent
\textbf{Matching Loss.}
We minimize the focal loss over the confidence matrix $\mathcal{C}$ returned by the matching layer. 
It is defined as:
$$
\mathcal{L}_m = -  \frac{1}{|\mathcal{K}_{gt}|} \sum_{(i,j)\in \mathcal{K}_{gt} } \alpha (1-\mathcal{C}(i,j))^{\gamma} \log \mathcal{C}(i,j)
$$
where $\alpha=0.25$ and $\gamma=2$ are empirically decided focal loss parameters as in~\cite{focal_loss}, and $\mathcal{K}_{gt}$ is the set of ground-truth matches.
During training, we warp $\mathbf{\hat{S}} $ to $\mathbf{\hat{T}} $ using the ground-truth wrap function $\mathcal{W}_{gt}$, and then collect the set of mutual nearest neighbors of the two-point clouds that are below a distance threshold as $\mathcal{K}_{gt}$. 


\medskip
\noindent
\textbf{Warping Loss.}
We minimize the $L_1$ loss for the point clouds that is warped by the $\mathbf{R}$,$\mathbf{t}$ from the Procrustes layer. It is defined as
$$\mathcal{L}_w = \frac{1}{|\mathcal{O}^{ \mathbf{\hat{T}}}_{ \mathbf{\hat{S}} }|} 
\sum_{i \in \mathcal{O}^{ \mathbf{\hat{T}} }_{ \mathbf{\hat{S}}}} 
| \mathcal{W}_{gt}(\mathbf{\hat{S}}_i) - \mathbf{R}\mathbf{\hat{S}}_i - \mathbf{t} | $$
where $\mathcal{O}^{\mathbf{\hat{T}}}_{ \mathbf{\hat{S}} }$ is the set of overlapping points in $\mathbf{\hat{S}}$, and $\mathcal{W}_{gt}(\cdot)$ is the ground truth warp function.
Intuitively, in rigid cases, $\mathcal{L}_w$ regularizes the optimization by suppressing false-positives in $\mathcal{K}_{soft}$ and also encourages sub-point accuracy for soft correspondences; 
in deformable cases, $\mathcal{L}_w$ tries to approximate a ``root'' pose that aligns the principal part of the overlapping region, e.g. the deer in Fig.~\ref{fig:attention4d}.


 

 
\medskip
\noindent
\textbf{Total Loss.}
As shown in Fig.~\ref{fig:pipeline}, the \textbf{T}ransform-\textbf{M}atching-\textbf{P}rocrustes (TMP) block repeats two times. The total loss combines the matching loss and warpping loss from the 1st and 2nd TMP blocks:
$
\mathcal{L} = (\mathcal{L}_m^1 + \mathcal{L}_m^2)  + \lambda_w (\mathcal{L}_w^1 + \mathcal{L}_w^2 )
$
where $\lambda_w$ is the weighting factor of warpping loss.  We show ablation study for $\lambda_w$ and the number of TMP blocks (2, 3, and 4) in supplementary.
