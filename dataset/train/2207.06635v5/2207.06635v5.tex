\documentclass{article}


\PassOptionsToPackage{numbers, compress}{natbib}





\usepackage[final]{neurips_2022}









\usepackage{amsmath,amsfonts,bm}

\newcommand{\figleft}{{\em (Left)}}
\newcommand{\figcenter}{{\em (Center)}}
\newcommand{\figright}{{\em (Right)}}
\newcommand{\figtop}{{\em (Top)}}
\newcommand{\figbottom}{{\em (Bottom)}}
\newcommand{\captiona}{{\em (a)}}
\newcommand{\captionb}{{\em (b)}}
\newcommand{\captionc}{{\em (c)}}
\newcommand{\captiond}{{\em (d)}}

\newcommand{\newterm}[1]{{\bf #1}}


\def\figref#1{figure~\ref{#1}}
\def\Figref#1{Figure~\ref{#1}}
\def\twofigref#1#2{figures \ref{#1} and \ref{#2}}
\def\quadfigref#1#2#3#4{figures \ref{#1}, \ref{#2}, \ref{#3} and \ref{#4}}
\def\secref#1{section~\ref{#1}}
\def\Secref#1{Section~\ref{#1}}
\def\twosecrefs#1#2{sections \ref{#1} and \ref{#2}}
\def\secrefs#1#2#3{sections \ref{#1}, \ref{#2} and \ref{#3}}
\def\plaineqref#1{\ref{#1}}
\def\chapref#1{chapter~\ref{#1}}
\def\Chapref#1{Chapter~\ref{#1}}
\def\rangechapref#1#2{chapters\ref{#1}--\ref{#2}}
\def\algref#1{algorithm~\ref{#1}}
\def\Algref#1{Algorithm~\ref{#1}}
\def\twoalgref#1#2{algorithms \ref{#1} and \ref{#2}}
\def\Twoalgref#1#2{Algorithms \ref{#1} and \ref{#2}}
\def\partref#1{part~\ref{#1}}
\def\Partref#1{Part~\ref{#1}}
\def\twopartref#1#2{parts \ref{#1} and \ref{#2}}

\def\ceil#1{\lceil #1 \rceil}
\def\floor#1{\lfloor #1 \rfloor}
\def\1{\bm{1}}
\newcommand{\train}{\mathcal{D}}
\newcommand{\valid}{\mathcal{D_{\mathrm{valid}}}}
\newcommand{\test}{\mathcal{D_{\mathrm{test}}}}

\def\eps{{\epsilon}}


\def\reta{{\textnormal{}}}
\def\ra{{\textnormal{a}}}
\def\rb{{\textnormal{b}}}
\def\rc{{\textnormal{c}}}
\def\rd{{\textnormal{d}}}
\def\re{{\textnormal{e}}}
\def\rf{{\textnormal{f}}}
\def\rg{{\textnormal{g}}}
\def\rh{{\textnormal{h}}}
\def\ri{{\textnormal{i}}}
\def\rj{{\textnormal{j}}}
\def\rk{{\textnormal{k}}}
\def\rl{{\textnormal{l}}}
\def\rn{{\textnormal{n}}}
\def\ro{{\textnormal{o}}}
\def\rp{{\textnormal{p}}}
\def\rq{{\textnormal{q}}}
\def\rr{{\textnormal{r}}}
\def\rs{{\textnormal{s}}}
\def\rt{{\textnormal{t}}}
\def\ru{{\textnormal{u}}}
\def\rv{{\textnormal{v}}}
\def\rw{{\textnormal{w}}}
\def\rx{{\textnormal{x}}}
\def\ry{{\textnormal{y}}}
\def\rz{{\textnormal{z}}}

\def\rvepsilon{{\mathbf{\epsilon}}}
\def\rvtheta{{\mathbf{\theta}}}
\def\rva{{\mathbf{a}}}
\def\rvb{{\mathbf{b}}}
\def\rvc{{\mathbf{c}}}
\def\rvd{{\mathbf{d}}}
\def\rve{{\mathbf{e}}}
\def\rvf{{\mathbf{f}}}
\def\rvg{{\mathbf{g}}}
\def\rvh{{\mathbf{h}}}
\def\rvu{{\mathbf{i}}}
\def\rvj{{\mathbf{j}}}
\def\rvk{{\mathbf{k}}}
\def\rvl{{\mathbf{l}}}
\def\rvm{{\mathbf{m}}}
\def\rvn{{\mathbf{n}}}
\def\rvo{{\mathbf{o}}}
\def\rvp{{\mathbf{p}}}
\def\rvq{{\mathbf{q}}}
\def\rvr{{\mathbf{r}}}
\def\rvs{{\mathbf{s}}}
\def\rvt{{\mathbf{t}}}
\def\rvu{{\mathbf{u}}}
\def\rvv{{\mathbf{v}}}
\def\rvw{{\mathbf{w}}}
\def\rvx{{\mathbf{x}}}
\def\rvy{{\mathbf{y}}}
\def\rvz{{\mathbf{z}}}

\def\erva{{\textnormal{a}}}
\def\ervb{{\textnormal{b}}}
\def\ervc{{\textnormal{c}}}
\def\ervd{{\textnormal{d}}}
\def\erve{{\textnormal{e}}}
\def\ervf{{\textnormal{f}}}
\def\ervg{{\textnormal{g}}}
\def\ervh{{\textnormal{h}}}
\def\ervi{{\textnormal{i}}}
\def\ervj{{\textnormal{j}}}
\def\ervk{{\textnormal{k}}}
\def\ervl{{\textnormal{l}}}
\def\ervm{{\textnormal{m}}}
\def\ervn{{\textnormal{n}}}
\def\ervo{{\textnormal{o}}}
\def\ervp{{\textnormal{p}}}
\def\ervq{{\textnormal{q}}}
\def\ervr{{\textnormal{r}}}
\def\ervs{{\textnormal{s}}}
\def\ervt{{\textnormal{t}}}
\def\ervu{{\textnormal{u}}}
\def\ervv{{\textnormal{v}}}
\def\ervw{{\textnormal{w}}}
\def\ervx{{\textnormal{x}}}
\def\ervy{{\textnormal{y}}}
\def\ervz{{\textnormal{z}}}

\def\rmA{{\mathbf{A}}}
\def\rmB{{\mathbf{B}}}
\def\rmC{{\mathbf{C}}}
\def\rmD{{\mathbf{D}}}
\def\rmE{{\mathbf{E}}}
\def\rmF{{\mathbf{F}}}
\def\rmG{{\mathbf{G}}}
\def\rmH{{\mathbf{H}}}
\def\rmI{{\mathbf{I}}}
\def\rmJ{{\mathbf{J}}}
\def\rmK{{\mathbf{K}}}
\def\rmL{{\mathbf{L}}}
\def\rmM{{\mathbf{M}}}
\def\rmN{{\mathbf{N}}}
\def\rmO{{\mathbf{O}}}
\def\rmP{{\mathbf{P}}}
\def\rmQ{{\mathbf{Q}}}
\def\rmR{{\mathbf{R}}}
\def\rmS{{\mathbf{S}}}
\def\rmT{{\mathbf{T}}}
\def\rmU{{\mathbf{U}}}
\def\rmV{{\mathbf{V}}}
\def\rmW{{\mathbf{W}}}
\def\rmX{{\mathbf{X}}}
\def\rmY{{\mathbf{Y}}}
\def\rmZ{{\mathbf{Z}}}

\def\ermA{{\textnormal{A}}}
\def\ermB{{\textnormal{B}}}
\def\ermC{{\textnormal{C}}}
\def\ermD{{\textnormal{D}}}
\def\ermE{{\textnormal{E}}}
\def\ermF{{\textnormal{F}}}
\def\ermG{{\textnormal{G}}}
\def\ermH{{\textnormal{H}}}
\def\ermI{{\textnormal{I}}}
\def\ermJ{{\textnormal{J}}}
\def\ermK{{\textnormal{K}}}
\def\ermL{{\textnormal{L}}}
\def\ermM{{\textnormal{M}}}
\def\ermN{{\textnormal{N}}}
\def\ermO{{\textnormal{O}}}
\def\ermP{{\textnormal{P}}}
\def\ermQ{{\textnormal{Q}}}
\def\ermR{{\textnormal{R}}}
\def\ermS{{\textnormal{S}}}
\def\ermT{{\textnormal{T}}}
\def\ermU{{\textnormal{U}}}
\def\ermV{{\textnormal{V}}}
\def\ermW{{\textnormal{W}}}
\def\ermX{{\textnormal{X}}}
\def\ermY{{\textnormal{Y}}}
\def\ermZ{{\textnormal{Z}}}

\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vsigma{{\bm{\sigma}}}
\def\vtheta{{\bm{\theta}}}
\def\vphi{{\bm{\phi}}}
\def\vepsilon{{\bm{\epsilon}}}
\def\veta{{\bm{\eta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\DeclareMathAlphabet{\mathsfit}{\encodingdefault}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsfit}{bold}{\encodingdefault}{\sfdefault}{bx}{n}
\newcommand{\tens}[1]{\bm{\mathsfit{#1}}}
\def\tA{{\tens{A}}}
\def\tB{{\tens{B}}}
\def\tC{{\tens{C}}}
\def\tD{{\tens{D}}}
\def\tE{{\tens{E}}}
\def\tF{{\tens{F}}}
\def\tG{{\tens{G}}}
\def\tH{{\tens{H}}}
\def\tI{{\tens{I}}}
\def\tJ{{\tens{J}}}
\def\tK{{\tens{K}}}
\def\tL{{\tens{L}}}
\def\tM{{\tens{M}}}
\def\tN{{\tens{N}}}
\def\tO{{\tens{O}}}
\def\tP{{\tens{P}}}
\def\tQ{{\tens{Q}}}
\def\tR{{\tens{R}}}
\def\tS{{\tens{S}}}
\def\tT{{\tens{T}}}
\def\tU{{\tens{U}}}
\def\tV{{\tens{V}}}
\def\tW{{\tens{W}}}
\def\tX{{\tens{X}}}
\def\tY{{\tens{Y}}}
\def\tZ{{\tens{Z}}}


\def\gA{{\mathcal{A}}}
\def\gB{{\mathcal{B}}}
\def\gC{{\mathcal{C}}}
\def\gD{{\mathcal{D}}}
\def\gE{{\mathcal{E}}}
\def\gF{{\mathcal{F}}}
\def\gG{{\mathcal{G}}}
\def\gH{{\mathcal{H}}}
\def\gI{{\mathcal{I}}}
\def\gJ{{\mathcal{J}}}
\def\gK{{\mathcal{K}}}
\def\gL{{\mathcal{L}}}
\def\gM{{\mathcal{M}}}
\def\gN{{\mathcal{N}}}
\def\gO{{\mathcal{O}}}
\def\gP{{\mathcal{P}}}
\def\gQ{{\mathcal{Q}}}
\def\gR{{\mathcal{R}}}
\def\gS{{\mathcal{S}}}
\def\gT{{\mathcal{T}}}
\def\gU{{\mathcal{U}}}
\def\gV{{\mathcal{V}}}
\def\gW{{\mathcal{W}}}
\def\gX{{\mathcal{X}}}
\def\gY{{\mathcal{Y}}}
\def\gZ{{\mathcal{Z}}}

\def\sA{{\mathbb{A}}}
\def\sB{{\mathbb{B}}}
\def\sC{{\mathbb{C}}}
\def\sD{{\mathbb{D}}}
\def\sF{{\mathbb{F}}}
\def\sG{{\mathbb{G}}}
\def\sH{{\mathbb{H}}}
\def\sI{{\mathbb{I}}}
\def\sJ{{\mathbb{J}}}
\def\sK{{\mathbb{K}}}
\def\sL{{\mathbb{L}}}
\def\sM{{\mathbb{M}}}
\def\sN{{\mathbb{N}}}
\def\sO{{\mathbb{O}}}
\def\sP{{\mathbb{P}}}
\def\sQ{{\mathbb{Q}}}
\def\sR{{\mathbb{R}}}
\def\sS{{\mathbb{S}}}
\def\sT{{\mathbb{T}}}
\def\sU{{\mathbb{U}}}
\def\sV{{\mathbb{V}}}
\def\sW{{\mathbb{W}}}
\def\sX{{\mathbb{X}}}
\def\sY{{\mathbb{Y}}}
\def\sZ{{\mathbb{Z}}}

\def\emLambda{{\Lambda}}
\def\emA{{A}}
\def\emB{{B}}
\def\emC{{C}}
\def\emD{{D}}
\def\emE{{E}}
\def\emF{{F}}
\def\emG{{G}}
\def\emH{{H}}
\def\emI{{I}}
\def\emJ{{J}}
\def\emK{{K}}
\def\emL{{L}}
\def\emM{{M}}
\def\emN{{N}}
\def\emO{{O}}
\def\emP{{P}}
\def\emQ{{Q}}
\def\emR{{R}}
\def\emS{{S}}
\def\emT{{T}}
\def\emU{{U}}
\def\emV{{V}}
\def\emW{{W}}
\def\emX{{X}}
\def\emY{{Y}}
\def\emZ{{Z}}
\def\emSigma{{\Sigma}}

\newcommand{\etens}[1]{\mathsfit{#1}}
\def\etLambda{{\etens{\Lambda}}}
\def\etA{{\etens{A}}}
\def\etB{{\etens{B}}}
\def\etC{{\etens{C}}}
\def\etD{{\etens{D}}}
\def\etE{{\etens{E}}}
\def\etF{{\etens{F}}}
\def\etG{{\etens{G}}}
\def\etH{{\etens{H}}}
\def\etI{{\etens{I}}}
\def\etJ{{\etens{J}}}
\def\etK{{\etens{K}}}
\def\etL{{\etens{L}}}
\def\etM{{\etens{M}}}
\def\etN{{\etens{N}}}
\def\etO{{\etens{O}}}
\def\etP{{\etens{P}}}
\def\etQ{{\etens{Q}}}
\def\etR{{\etens{R}}}
\def\etS{{\etens{S}}}
\def\etT{{\etens{T}}}
\def\etU{{\etens{U}}}
\def\etV{{\etens{V}}}
\def\etW{{\etens{W}}}
\def\etX{{\etens{X}}}
\def\etY{{\etens{Y}}}
\def\etZ{{\etens{Z}}}

\newcommand{\pdata}{p_{\rm{data}}}
\newcommand{\ptrain}{\hat{p}_{\rm{data}}}
\newcommand{\Ptrain}{\hat{P}_{\rm{data}}}
\newcommand{\pmodel}{p_{\rm{model}}}
\newcommand{\Pmodel}{P_{\rm{model}}}
\newcommand{\ptildemodel}{\tilde{p}_{\rm{model}}}
\newcommand{\pencode}{p_{\rm{encoder}}}
\newcommand{\pdecode}{p_{\rm{decoder}}}
\newcommand{\precons}{p_{\rm{reconstruct}}}

\newcommand{\laplace}{\mathrm{Laplace}} 

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\newcommand{\parents}{Pa} 

\newcommand{\reff}[1]{Fig.~\ref{fig:#1}}
\newcommand{\reft}[1]{Tab.~\ref{table:#1}}
\newcommand{\refe}[1]{Eqn.~(\ref{eqn:#1})}
\newcommand{\refs}[1]{Sec.~\ref{sec:#1}}



\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\Tr}{Tr}
\let\ab\allowbreak
 \usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{xcolor}         \usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsthm}
\usepackage{booktabs}  \usepackage{arydshln}
\usepackage{subfig}
\usepackage{multirow}

\newcommand{\ep}{\mathbb{E}}


\newtheorem{lem}{Lemma}
\newtheorem{pro}{Proposition}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem*{remark}{Remark}

\newcommand{\fan}[1]{{\color{purple}{[[\textbf{fan: }#1]]}}}
\newcommand{\cx}[1]{{\color{green}{[[\textbf{cx: }#1]]}}}
\newcommand{\zm}[1]{{\color{red}{#1}}}

\title{EGSDE: Unpaired Image-to-Image Translation via Energy-Guided Stochastic Differential Equations}




    
\author{Min Zhao, Fan Bao, Chongxuan Li\thanks{Correspondence to Chongxuan Li and Jun Zhu.}~~, Jun Zhu\\
  Dept. of Comp. Sci. \& Tech., BNRist Center, THU-Bosch ML Center,
  Tsinghua University, China \\
   Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China\\
   Beijing Key Laboratory of Big Data Management and Analysis Methods , Beijing, China\\
   Pazhou Laboratory (Huangpu), Guangzhou, China \\
  \texttt{gracezhao1997@gmail.com;} \texttt{bf19@mails.tsinghua.edu.cn;} \\
  \texttt{chongxuanli@ruc.edu.cn;} \texttt{dcszj@tsinghua.edu.cn} 
}


\begin{document}


\maketitle







\begin{abstract}
Score-based diffusion models (SBDMs) have achieved the SOTA FID results in unpaired image-to-image translation (I2I). However, we notice that existing methods totally ignore the training data in the source domain, leading to sub-optimal solutions for unpaired I2I. 
To this end, we propose energy-guided stochastic differential equations (EGSDE)
that employs an energy function pretrained on both the source and target domains to guide the inference process of a pretrained SDE for realistic and faithful unpaired I2I. Building upon two feature extractors, we carefully design the energy function such that it encourages the transferred image to preserve the domain-independent features and discard domain-specific ones. Further, we provide an alternative explanation of the EGSDE as a product of experts, where each of the three experts (corresponding to the SDE and two feature extractors) solely contributes to faithfulness or realism. Empirically, we compare EGSDE to a large family of baselines on three widely-adopted unpaired I2I tasks under four metrics. EGSDE not only consistently outperforms existing SBDMs-based methods in almost all settings but also achieves the SOTA realism results without harming the faithful performance. Furthermore, EGSDE allows for flexible trade-offs between realism and faithfulness and we improve the realism results further (e.g., FID of 51.04 in Cat  Dog and FID of 50.43 in Wild  Dog on AFHQ) by tuning hyper-parameters. The code is available at \href{https://github.com/ML-GSAI/EGSDE}{https://github.com/ML-GSAI/EGSDE}.   
\end{abstract}
\begin{figure}
  \centering
  \includegraphics[width=.9\columnwidth]{images/intro.pdf}
  \caption{(a) Apart from the SDE, the EGSDE incorporates a realism expert and a faithful expert to preserve the domain-independent features and discard domain-specific ones. (b) Representative translation results on three unpaired I2I tasks.}
  \label{fig:intro}
  \vspace{-0.5cm}
\end{figure}

 
 
\section{Introduction}
Unpaired image-to-image translation (I2I) aims to transfer an image from a source domain to a related target domain, which involves a wide range of computer vision tasks such as style transfer, super-resolution and pose estimation~\cite{pang2021image}.  In I2I, the translated image should be \emph{realistic} to fit the style of the target domain by changing the domain-specific features accordingly, and \emph{faithful} to preserve the domain-independent features of the source image. Over the past few years, generative adversarial networks~\cite{goodfellow2014generative} (GANs)-based methods~\cite{fu2019geometry,zhu2017unpaired,yi2017dualgan,park2020contrastive,benaim2017one,zheng2021spatially,shen2019towards,jiang2020tsit,huang2018multimodal,lee2018diverse,fu2019geometry} dominated this field due to their ability to generate high-quality samples. 

In contrast to GANs, score-based diffusion models (SBDMs)~\cite{song2019generative,ho2020denoising,nichol2021improved,song2020score,bao2021analytic,lu2022dpm} perturb data to a Gaussian noise by a diffusion process and learn the reverse process to transform the noise back to the data distribution. Recently, SBDMs achieved competitive or even superior image generation performance to GANs~\cite{dhariwal2021diffusion} and thus were naturally applied to unpaired I2I~\cite{choi2021ilvr,meng2021sdedit},
which have achieved the state-of-the-art FID~\cite{heusel2017gans} and  KID~\cite{binkowski2018demystifying} results empirically. However, we notice that these methods \emph{did not leverage the training data in the source domain at all}. Indeed, they trained a diffusion model solely on the target domain and exploited the test source image during inference (see details in Sec.~\ref{sec:back_i2i}). Therefore, we argue that if the training data in the source domain can be exploited together with those in the target domain, one can learn domain-specific and domain-independent features to improve both the realism and faithfulness of the SBDMs in unpaired I2I.
 

To this end, we propose \emph{energy-guided stochastic differential equations} (EGSDE)
that employs an energy function pretrained across the two domains to guide the inference process of a pretrained SDE for realistic and faithful unpaired I2I. 
Formally, EGSDE defines a valid conditional distribution via a reverse time SDE that composites the energy function and the pretrained SDE. Ideally, the energy function should encourage the transferred image to preserve the domain-independent features and discard domain-specific ones.
To achieve this, we introduce two feature extractors that learn domain-independent features and domain-specific ones respectively, and define the energy function upon the similarities between the features extracted from the transferred image and the test source image. Further, we provide an alternative explanation of the discretization of EGSDE in the formulation of \emph{product of experts}~\cite{hinton2002training}. In particular, the pretrained SDE and the two feature extractors in the energy function correspond to three experts and each solely contributes to faithfulness or realism. 

Empirically, we validate our method on the  widely-adopted AFHQ~\cite{choi2020stargan} and CelebA-HQ~\cite{karras2018progressive}  datasets including Cat  Dog, Wild  Dog and Male  Female tasks. We compare to a large family of baselines, including the GANs-based ones~\cite{park2020contrastive,zhu2017unpaired,huang2018multimodal,lee2018diverse,benaim2017one,fu2019geometry,zheng2021spatially,zheng2022ittr} and SBDMs-based ones~\cite{choi2021ilvr,meng2021sdedit} under four metrics (e.g., FID). EGSDE not only consistently outperforms SBDMs-based methods in almost all settings but also achieves the SOTA realism results without harming the faithful performance. Furthermore, EGSDE allows for flexible trade-offs between realism and faithfulness and we improve the FID further (e.g., 51.04 in Cat  Dog and 50.43 in Wild  Dog ) by tuning hyper-parameters. EGSDE can also be extended to multi-domain translation easily. 

\section{Background}

\subsection{Score-based Diffusion Models}

Score-based diffusion models (SBDMs) gradually perturb data by a forward diffusion process, and then reverse it to recover the data~\cite{song2020score,bao2021analytic,song2021maximum,ho2020denoising,dhariwal2021diffusion}. Let  be the unknown data distribution on . The forward diffusion process , indexed by time , can be represented by the following forward SDE:

where  is a standard Wiener process,  is the drift coefficient and  is the diffusion coefficient. The  and  is related into the noise size and determines the perturbation kernel  from time  to . In practice, the  is usually affine so that the the perturbation kernel is a linear Gaussian distribution and can be sampled in one step.

Let  be the marginal distribution of the SDE at time  in Eq.~\eqref{eq:forward-sde}. Its time reversal can be described by another SDE~\cite{song2020score}:

where  is a reverse-time standard
Wiener process, and  is an infinitesimal negative timestep. \cite{song2020score} adopts a score-based model  to approximate the unknown  by score matching, thus inducing a score-based diffusion model (SBDM), which is defined by a SDE:

There are numerous SDE solver to solve the Eq. \eqref{eq:reverse-sde-score} to generate images. \cite{song2020score} discretizes it using the Euler-Maruyama solver. Formally, adopting a step size of , the iteration rule from  to  is:

\subsection{SBDMs in Unpaired Image to Image Translation}
\label{sec:back_i2i}
Given unpaired images from the source domain  and the target domain  as the training data, the goal of unpaired I2I is to transfer an image from the source domain to the target domain. Such a process can be formulated as designing a distribution  on the target domain  conditioned on an image  to transfer. 
The translated image should be \emph{realistic} for the target domain by changing the domain-specific features and \emph{faithful} for the source image by preserving the domain-independent features.

ILVR~\cite{choi2021ilvr} uses a diffusion model on the target domain for realism. Formally, ILVR starts from  and samples from the diffusion model according to Eq.~\eqref{eq:sampling} to obtain . For faithfulness, it further refines  by adding the residual between the sample  and the perturbed source image  through a non-trainable low-pass filter

where  is a low-pass filter and  is the perturbation kernel determined by the forward SDE in Eq.~\eqref{eq:forward-sde}.

Similarly, SDEdit~\cite{meng2021sdedit} also adopts a SBDM on the target domain for realism, i.e., sampling from the SBDM according to Eq.~\eqref{eq:sampling}. For faithfulness, SDEdit starts the generation process from the noisy source image , where  is a middle time between  and , and is chosen to preserve the original overall structure and discard local details. We use  to denote the marginal distribution defined by such SDE conditioned on .

Notably, these methods did not leverage the training data in the source domain at all and thus can be sub-optimal in terms of both the realism and faithfulness in unpaired I2I.

\begin{figure}
  \centering
  \includegraphics[width=1.0\columnwidth]{images/method.pdf}
  \caption{
  The overview of our EGSDE. Starting from the noisy source image, we can run the EGSDE for unpaired I2I, which employs an energy function  pretrained on both the source and target domains to guide the inference process of a pretrained SDE (, realism expert 1). The energy function is decomposed into two terms further, where the realistic expert 2  encourages the transferred image to discard domain-specific features and the faithful expert  aims to preserve the domain-independent ones.}
  \label{fig:method}
  \vspace{-0.5cm}
\end{figure}

\section{Method}
\label{method}
To overcome the limitations of existing methods~\cite{choi2021ilvr,meng2021sdedit} as highlighted in Sec.~\ref{sec:back_i2i}, we propose \emph{energy-guided stochastic differential equations} (EGSDE)
that employs an energy function pre-trained across the two domains to guide the inference process of a pretrained SDE for realistic and faithful unpaired I2I (see Fig.~\ref{fig:method}).
EGSDE defines a valid conditional distribution  by compositing a pretrained SDE  and a pretrained energy function under mild regularity conditions\footnote{The assumptions are very similar to those in prior work~\cite{song2020score}. We list them for completeness in Appendix \ref{assumptions}} as follows:

where  is a reverse-time standard Wiener process,  is an infinitesimal negative timestep,  is the score-based model in the pretrained SDE and  is the energy function. The start point  is sampled from the perturbation distribution ~\cite{meng2021sdedit}, where  typically. We obtain the transferred images by taking the samples at endpoint  following the SDE in Eq.~\eqref{eq:ours}.


Similar to the prior work~\cite{choi2021ilvr,meng2021sdedit}, EGSDE employs an SDE trained solely in the target domain as in Eq.~\eqref{eq:reverse-sde}, which defines a marginal distribution of the target images and mainly contributes to the realism of the transferred samples. In contrast, the energy function involves the training data across both the source and target domain, making EGSDE distinct from the prior work~\cite{choi2021ilvr,meng2021sdedit}. Notably, although many other possibilities exist, we carefully design the energy function
such that it (approximately) encourages the sample to retain the domain-independent features and discard the domain-specific ones to improve both the faithfulness and realism of the transferred sample. 
Below, we formally formulate the energy function.




\subsection{Choice of Energy}
\label{choice of energy}





In this section, we show how to design the energy function. Intuitively, during the translation, the domain-independent features (pose, color, \emph{etc}. on Cat  Dog) should be preserved while the domain-specific features (beard, nose, \emph{etc}. on Cat  Dog) should be changed  accordingly. Motivated by this, we decompose the energy function  as the sum of two log potential functions~\cite{bishop:2006:PRML}: 

where  and  are the log potential functions,  is the perturbed source image in the forward SDE, 
 is the perturbation kernel from time  to time  in the forward SDE,    and
 are two functions measuring the similarity between the sample and perturbed source image,
and  are two weighting hyper-parameters. Note that
the expectation w.r.t.   in Eq.~\eqref{eq:decompose energy} guarantees that the energy function changes slowly over the trajectory to satisfy the regularity conditions in Appendix \ref{assumptions}. 


To specify 
, we introduce a time-dependent domain-specific feature extractor , where  is the channel-wise dimension,  and  are the dimension of height and width.
In particular,  is the all but the last layer of a classifier that is trained on both domains to predict whether an image is from the source domain or the target domain. Intuitively,   will preserve the domain-specific features and discard the domain-independent features for accurate predictions. Building upon it,  is defined as the cosine similarity between the features extracted from the generated sample and the source image as follows:

where  denote the channel-wise feature at spatial position . Here we employ the cosine similarity since it preserves the spatial information and helps to improve the FID score empirically (see Appendix \ref{sec:metrics} for the ablation study). 
Intuitively, reducing the energy value in Eq.~\eqref{eq:decompose energy} encourages the transferred sample to discard the domain-specific features to improve realism. 


 
To specify , we introduce a  domain-independent feature extractor , which is a low-pass filter.  Intuitively,  will preserve the overall structures (i.e., domain-independent features)
and discard local information like textures (i.e., domain-specific features).    Building upon it,  is defined as the negative squared  distance between the features extracted from the generated sample and source image as follows:

Here, we choose negative squared  distance as the similarity metric because it helps to preserve more domain-independent features empirically (see Appendix \ref{sec:metrics} for the ablation study). Intuitively, reducing the energy value in Eq.~\eqref{eq:decompose energy} encourages the transferred sample to preserve the domain-independent features to improve faithfulness. 
In this paper we employ a low-pass filter for its simpleness and effectiveness while we can train more sophisticated , e.g., based on disentangled representation learning methods~\cite{sanchez2020learning,chen2016infogan,higgins2016beta,kim2018disentangling,liu2018unified}, on the data in the two domains.
 
In our preliminary experiment, alternative to Eq.~\eqref{eq:decompose energy}, we consider a simpler energy function that only involves the original source image  as follows:

which does not require to take the expectation w.r.t. . We found that it did not perform well because it is not reasonable to measure the similarity between the noise-free source image and the transferred sample in a gradual denoising process. See Appendix \ref{sec:simple energy} for empirical results.





































\subsection{Solving the Energy-guided Reverse-time SDE}



Based on the pretrained score-based model  and energy function , we can solve the proposed energy-guided SDE to generate samples from conditional distribution . There are numerical solvers to approximate trajectories from SDEs. In this paper, we take the Euler-Maruyama solver following~\cite{meng2021sdedit} for a fair comparison. Given the EGSDE as in Eq.~(\ref{eq:ours}) and adopting a step size , the iteration rule from  to  is:




The expectation in  is estimated by the Monte Carlo method of a single sample for efficiency. 
For brevity, we present the general sampling procedure of our method in Algorithm~{\ref{alg:general}}. In experiments, we use the variance preserve energy-guided SDE (VP-EGSDE)~\cite{song2020score,ho2020denoising} and the details are explained in Appendix \ref{sec:VP-EGSDE}, where we can modify the noise prediction network to  and take it into the sampling procedure in DDPM~\cite{ho2020denoising}. Following SDEdit~\cite{meng2021sdedit}, we further extend this by repeating the Algorithm~{\ref{alg:general}}  times (see details in Appendix \ref{sec:K}). Further, we explain the connection with classifier guidance\cite{dhariwal2021diffusion} in Appendix \ref{sec:classifier guidance}.


\begin{algorithm}
    \caption{EGSDE for unpaired image-to-image translation}
    \label{alg:general}
    \begin{algorithmic}
        \REQUIRE the source image , the initial time , denoising steps , weighting hyper-parameters , the similarity function , the score function  
        \STATE  \# the start point
        \STATE 
    \FOR{ to }
        \STATE 
        \STATE  \# sample perturbed source image from the perturbation kernel
        \STATE  \# compute energy with one Monte Carlo
        \STATE  \# the update rule in Eq. \eqref{sampling-egsde}
        \STATE  if , else 
        \STATE 
    \ENDFOR
    \STATE 
    \RETURN 
    \end{algorithmic}
\end{algorithm}






\subsection{EGSDE as Product of Experts}
\label{sec:products as experts}



















Inspired by the posterior inference process in diffusion models~\cite{sohl2015deep}, 
we present a  \emph{product of experts}~\cite{hinton2002training} explanation for the discretized sampling process of EGSDE, which formalizes our motivation in an alternative perspective and provides insights on the role of each component in EGSDE. 

We first define a conditional distribution  at time  as a product of experts:
 
where  is the partition function,  and  is the marginal distribution at time  defined by SDEdit based on a pretrained SDE on the target domain.  

To sample from , we need to construct a transition kernel , where  and  is small. Following \cite{sohl2015deep}, using the desirable equilibrium , we construct the  as follows:

where  is the partition function and  is the transition kernel of the pretrained SDE in Eq. \eqref{eq:sampling}, i.e.,  and .
Assuming that  has low curvature relative to , it can be approximated using Taylor expansion around  and further we can obtain

More details about derivation are available in Appendix \ref{sec:products}. We can observe the transition kernel  in \eqref{eq:sampling for product expert} is equal to the discretization of our EGSDE in Eq. \eqref{sampling-egsde}. Therefore, solving the energy-guided SDE in a discretization manner is approximately equivalent to drawing samples from a product of experts in Eq. \eqref{eq:product marginal}. Note that , the  can be rewritten as:

where , .

In Eq.~\eqref{eq:product of expert}, by setting , we can explain that the transferred samples approximately follow the distribution defined by the product of three experts, where  and  are the \emph{realism experts} and  is the \emph{faithful expert}, corresponding to the score function  and the log potential functions  and  respectively.
Such a formulation clearly explains the role of each expert in EGSDE and supports our empirical results.


















\section{Related work}




Apart from the prior work mentioned before, we discuss other related work including GANs-based methods for unpaired I2I and SBDMs-based methods for image translation.

\textbf{GANs-based methods for Unpaired I2I.}
Although previous paired image translation methods have also achieved remarkable performances~\cite{isola2017image,wang2018high,tang2019multi,park2019semantic,zhang2020cross,zhou2021cocosnet}, we mainly focus on unpaired image translation in this work. The methods for two-domain unpaired I2I are mainly divided into two classes: two-side and one-side mapping~\cite{pang2021image,zheng2021spatially}. In the two-side framework~\cite{zhu2017unpaired,yi2017dualgan,kim2017learning,liu2017unsupervised,li2018unsupervised,kim2019u,gokaslan2018improving,amodio2019travelgan,wu2019transgaga,zhao2020unpaired,katzir2020cross}, the cycle-consistency constraint is the most widely-used strategy such as in CycleGAN~\cite{zhu2017unpaired}, DualGAN~\cite{yi2017dualgan} and DiscoGAN~\cite{kim2017learning}. 
The key idea is that the translated image should be able to be reconstructed by an inverse mapping. More recently, there are numerical studies to improve this such as SCAN~\cite{li2018unsupervised} and U-GAT-IT~\cite{kim2019u}. Specifically, U-GAT-IT~\cite{kim2019u} applies an attention module to let the generator and discriminator focus on more important regions instead of the whole regions through the auxiliary classifier. Since such bijective projection is too restrictive, several studies are devoted to one-side mapping~\cite{park2020contrastive,benaim2017one,fu2019geometry,zhu2017unpaired,zheng2021spatially,park2020swapping,jiang2020tsit}. One representative approach is to design some kind of geometry distance to preserve content~\cite{pang2021image}. For example, DistanceGAN~\cite{benaim2017one} keeps the distances between images within domains. GCGAN~\cite{fu2019geometry} maintains geometry-consistency between input and output. CUT~\cite{park2020contrastive} maximizes the mutual information between the input and output using contrastive learning. LSeSim~\cite{zheng2021spatially} learns spatially-correlative representation to preserve scene structure consistency via self-similarities.

\textbf{SBDMs-based methods for Image Translation.}
Several studies leveraged SBDMs for image translation due to their powerful generative ability and achieved good results. For example, DiffusionCLIP~\cite{kim2022diffusionclip} fine-tune the score network with CLIP~\cite{radford2021learning} loss, which is applied on text-driven image manipulation, zero-shot image manipulation and multi-attribute transfer successfully. GLIDE~\cite{nichol2021glide} and SDG~\cite{liu2021more} has achieved great performance on text-to-image translation. As for I2I, SR3~\cite{saharia2022image} and Palette~\cite{saharia2022palette} learn a conditional SBDM and outperform state-of-art GANs-based methods on super-resolution, colorization and so on, which needs paired data. For unpaired I2I, UNIT-DDPM~\cite{sasaki2021unit} learns two SBDMs and two domain translation models using cycle-consistency loss. Compared with it, our method only needs one SBDM on the target domain, which is a kind of one-side mapping. ILVR~\cite{choi2021ilvr} and SDEdit~\cite{meng2021sdedit} utilize a SBDM on the target domain and exploited the test source image to refine inference, which ignored the training data in the source domain. Compared with these methods, our method employs an energy function pretrained across both the source and target domains to improve the realism and faithfulness of translated images.
\begin{figure}
  \centering
  \includegraphics[width=1.0\columnwidth]{images/visualize.pdf}
  \caption{The qualitative comparison on Cat  Dog, Wild  Dog and Male  Female. Our method achieved better visual quality for both \emph{realism} and \emph{faithfulness}. For example, in the forth column, we successfully preserve the domain-independent features (i.e. green ground, pose and yellow color of body) and discard the domain-specific ones (i.e. leopard print).}
  \vspace{-0.5cm}
  \label{fig:qualitative results}
\end{figure}

\section{Experiment} 
\label{sec:Implementation}
\textbf{Datasets.} We validated the EGSDE on following datasets, where all images are resized to : (1) CelebA-HQ~\cite{karras2018progressive} contains high quality face images and is separated into two domains: male and
female. Each category
has 1000 testing images. 
We perform MaleFemale on this dataset. \\
(2) AFHQ~\cite{choi2020stargan} consists of high-resolution animal face images including three domains: cat, dog and wild, which has relatively large variations. Each domain has 500 testing images. 
We perform CatDog and WildDog on this dataset. We also perform \emph{multi-domain translation} on AFHQ dataset and the experimental results are reported in Appendix \ref{sec:multi-domain}. 

\textbf{Implementation.} The time-dependent domain-specific extractor  is trained based on the backbone in \cite{dhariwal2021diffusion}. The resize function including downsampling and upsampling operation is used as low-pass filter and is implemented by~\cite{ResizeRight}. For generation process, by default, the weight parameter ,  is set   and  respectively. The initial time  and denoising steps  is set  and  by default. More details about implementation are available in Appendix \ref{sec:implementation}.

\textbf{Evaluation Metrics.} We evaluate translated images from two aspects: realism and faithfulness. For realism, we report the widely-used Frechet Inception Score (FID)~\cite{heusel2017gans} between translated images and the target dataset. To quantify faithfulness, we report the  distance, PSNR and SSIM~\cite{wang2004image} between each input-output pair. To quantify both faithfulness and realism, we leverage Amazon Mechanical Turk(AMT) human evaluation to perform pairwise comparisons between the baselines and EGSDE. More details is available in Appendix \ref{sec:evaluation}.  
\begin{table}
\caption{Quantitative comparison. ILVR~\cite{choi2021ilvr}, SDEdit~\cite{meng2021sdedit} and CUT~\cite{park2020contrastive} are reproduced using public code. StarGAN v2~\cite{choi2020stargan} is evaluated by the provided public checkpoint and the other methods marked by * are public results from CUT\cite{park2020contrastive} and ITTR~\cite{zheng2022ittr}. All SBDMs-based methods and StarGAN v2 are repeated 5 times to eliminate randomness. CUT is conducted once since it learns a deterministic mapping. AMT show the preference rate of EGSDE against baselines via human evaluation. The EGSDE use the default-parameters () and EGSDE use the parameters with .}
\vspace{.2cm}
\label{tb:two doamin}
\centering
\renewcommand\arraystretch{1}
\begin{tabular}{lccccc} 
\toprule
Model         & FID                   & L2                    & PSNR                  & SSIM    & AMT                 \\
\midrule
\multicolumn{5}{c}{Cat  Dog}                                                                          \\ 
\midrule
CycleGAN~\cite{zhu2017unpaired}      & 85.9 &- & -&-&-\\
MUNIT~\cite{huang2018multimodal}         & 104.4 &-&-&-&-\\
DRIT~\cite{lee2018diverse}         & 123.4 &-&-&-&-\\
Distance~\cite{benaim2017one}     &155.3 &-&-&-&-\\
SelfDistance~\cite{benaim2017one} &144.4 &-&-&-&-\\
GCGAN~\cite{fu2019geometry}        & 96.6 &-&-&-&-\\
LSeSim~\cite{zheng2021spatially}       & 72.8 &-&-&-&-\\
ITTR (CUT)~\cite{zheng2022ittr}   & 68.6 &-&-&-&-\\
StarGAN v2 ~\cite{choi2020stargan}          & 54.88 ± 1.01              & 133.65 ± 1.54               & 10.63 ± 0.10               & 0.27 ± 0.003 &-\\ 
CUT~\cite{park2020contrastive}           & 76.21               & 59.78               & 17.48               & \textbf{0.601} & 79.6\\       
\midrule
ILVR~\cite{choi2021ilvr}          & 74.37 ± 1.55          & 56.95 ± 0.14          & 17.77 ± 0.02          & 0.363 ± 0.001  & 75.4        \\
SDEdit~\cite{meng2021sdedit}        & 74.17 ± 1.01          & 47.88 ± 0.06          & 19.19 ± 0.01          & \textbf{0.423 ± 0.001} & 65.2  \\
EGSDE & \textbf{65.82 ± 0.77} & \textbf{47.22 ± 0.08} & \textbf{19.31 ± 0.02} & 0.415 ± 0.001  &-         \\
EGSDE  & \textbf{51.04 ± 0.37} & 62.06 ± 0.10 & 17.17 ± 0.02  & 0.361 ± 0.001   & -        \\
\midrule
\multicolumn{5}{c}{Wild  Dog}                                                                         \\
\midrule
CUT~\cite{park2020contrastive}            & 92.94               & 62.21               & 17.2                & \textbf{0.592}  & 82.4       \\
\midrule
ILVR~\cite{choi2021ilvr}           & 75.33 ± 1.22          & 63.40 ± 0.15          & 16.85 ± 0.02          & 0.287 ± 0.001   & 73.4         \\
SDEdit~\cite{meng2021sdedit}         & 68.51 ± 0.65          & 55.36 ± 0.05          & 17.98 ± 0.01          & \textbf{0.343 ± 0.001} & 57.2  \\
EGSDE & \textbf{59.75 ± 0.62} & \textbf{54.34 ± 0.08} & \textbf{18.14 ± 0.01} & \textbf{0.343 ± 0.001} &- \\
EGSDE   & \textbf{50.43± 0.52} & 66.52± 0.09 & 16.40± 0.01 & 0.300± 0.001    &-      \\
\midrule
\multicolumn{5}{c}{Male  Female}                                                         \\
\midrule
CUT~\cite{park2020contrastive}  & \textbf{31.94}               & 46.61               & 19.87             & \textbf{0.74}  & 58.6        \\
\midrule
ILVR~\cite{choi2021ilvr}           & 46.12 ± 0.33            & 52.17 ± 0.10                    &   18.59 ± 0.02                  &   0.510 ± 0.001       & 88.2               \\
SDEdit~\cite{meng2021sdedit}         & 49.43 ± 0.47              & 43.70 ± 0.03              & 20.03 ± 0.01             & 0.572 ± 0.000    & 74.4              \\

EGSDE & \textbf{41.93 ± 0.11}     & \textbf{42.04 ± 0.03}      & \textbf{20.35 ± 0.01}      & \textbf{0.574 ± 0.000}& -        \\
EGSDE & \textbf{30.61 ± 0.19}     & 53.44 ± 0.09      & 18.32 ± 0.02     & 0.510 ± 0.001& -    \\    
\bottomrule
\end{tabular}
\end{table}
\subsection{Two-Domain Unpaired Image Translation}
In this section, we compare EGSDE with the following state-of-the-art I2I methods in three tasks: SBDMs-based methods including ILVR~\cite{choi2021ilvr} and SDEdit~\cite{meng2021sdedit}, and GANs-based methods including CUT~\cite{park2020contrastive}, which are reproduced using public code. On the most popular benchmark Cat  Dog, we also report the performance of other state-of-the-art GANs-methods , where StarGAN v2~\cite{choi2020stargan} is evaluated by the provided public checkpoint and the others are public results from CUT\cite{park2020contrastive} and ITTR~\cite{zheng2022ittr}. We provide more details about reproductions in Appendix \ref{sec:reproductions}. 

The quantitative comparisons and qualitative results are shown in Table \ref{tb:two doamin} and Figure \ref{fig:qualitative results}. We can derive several observations. \emph{First}, our method outperforms the SBDMs-based methods significantly in almost all realism and faithfulness metrics, suggesting the effectiveness of employing energy function pretrained on both domains to guide the generation process. Especially, compared with the most direct competitor, i.e., SDEdit, with a lower  distance at the same time, EGSDE improves the FID score by 8.35, 8.76 and 7.5 on Cat  Dog, Wild  Dog and Male  Female respectively. \emph{Second}, EGSDE outperforms the current state-of-art GANs-based methods by a large margin on the challenging AFHQ dataset. For example, compared with CUT~\cite{park2020contrastive}, we achieve an improvement of FID score with 25.17 and 42.51 on the Cat  Dog and Wild  Dog tasks respectively. In addition, the human evaluation shows that EGSDE are preferred compared to all baselines (> 50). The qualitative results in Figure \ref{fig:qualitative results} agree with quantitative comparisons in Table \ref{tb:two doamin}, where our method achieved the results with the best visual quality for both realism and faithfulness.  We show more qualitative results and select some failure cases in Appendix \ref{sec:more resutls}.        
    






\begin{figure}
  \centering
  \includegraphics[width=1.0\columnwidth]{images/diffferent_M_K.pdf}
  \caption{(a) The results of different initial time . The larger  results in more realistic and less faithful images. (b) The results of repeating the Algorithm 1  times. With the increase of K, SDEdit~\cite{meng2021sdedit} tend to discard the domain-independent information of the source image (e.g., color and background) while our method still preserve them without harming realism.}
  \label{fig:K}
  \vspace{-0.5cm}
\end{figure}


\subsection{Ablation Studies}
\label{ablation studies}
\textbf{The function of each expert.} We validate the function of \emph{realistic expert}  and \emph{faithful expert}  by changing the weighting hyper-parameter  and . As shown in Table \ref{tb:lambda analysis} and Figure ~\ref{fig:intro}, larger  results in more realistic images and larger  results in more faithful images. More results is available in Appendix \ref{sec:expert function}.

\textbf{The choice of initial time .} We explore the effect of the initial time  of EGSDE. As shown in Figure~\ref{fig:K}, the larger  results in more realistic and less faithful image. More results is available in Appendix \ref{sec:initial time}.

\textbf{Repeating  Times.} Following SDEdit~\cite{meng2021sdedit}, we show the results of repeating the Algorithm 1  times. The quantitative and qualitative results are depicted in Table \ref{tb : K time} and Figure~\ref{fig:K}. The experimental results show the EGSDE outperforms SDEdit in each  step in all metrics. With the increase of , the SDEdit generates more realism images but the faithful metrics decrease sharply, because it only utilizes the source image at the initial time . As shown in Figure~\ref{fig:K}, when =3, SDEdit discard the domain-independent information of the source image (i.e., color and background) while our method still preserves them without harming realism.



\begin{figure}[htbp]
    \centering
	\begin{minipage}{0.48\linewidth}
		\centering
		\captionof{table}{Comparison with SDEdit~\cite{meng2021sdedit} under different  times on  Male  Female. The results on other tasks are reported in Appendix \ref{sec:repeating K}.}
		\renewcommand\arraystretch{1.3} {
		\resizebox{\linewidth}{!}{
		\begin{tabular}{cccccc}
\toprule
Methods & K &   
                                   FID    & L2     & PSNR   & SSIM   \\
\midrule
SDEdit~\cite{meng2021sdedit}       & \multirow{2}{*}{1}           & 49.95       &  43.71     & 20.03  &0.572    \\
EGSDE                   &                   & \textbf{42.17}  & \textbf{42.07}  & \textbf{20.35} & \textbf{0.573} \\
\hdashline
SDEdit~\cite{meng2021sdedit}                    & \multirow{2}{*}{2}                   & 46.26       &  50.70      &  18.77     & 0.542      \\
EGSDE          &          &  \textbf{38.68 } & \textbf{47.10 }  & \textbf{19.40 } & \textbf{0.548} \\
\hdashline
SDEdit~\cite{meng2021sdedit} & \multirow{2}{*}{3}                      &  45.19      &    55.03    &  18.08     &  0.527     \\
EGSDE                   &                   & \textbf{37.55}  & \textbf{49.63}  & \textbf{18.96} & \textbf{0.536}\\
\bottomrule
\end{tabular}}}
		
		\label{tb : K time}
	\end{minipage}
\hfill
	\begin{minipage}{0.5\linewidth}
		\centering
        \captionof{table}{The results of different  and  on Wild  Dog.  corresponds to SDEdit~\cite{meng2021sdedit}.}
		\renewcommand\arraystretch{1.25} {
		\resizebox{\linewidth}{!}{
        \begin{tabular}{ccccc}
\toprule
 & FID     & L2      & PSNR  & SSIM  \\
\midrule
   & 67.87    & 55.39 & 17.97 & 0.344 \\
\hdashline
                     & 60.80     & 56.19 & 17.85 & 0.341 \\
                   & 53.72    & 58.65 & 17.47 & 0.335\\
                  & 53.01    & 60.02 & 17.27 & 0.331  \\
\hdashline
              & 68.31    & 53.23 & 18.32 & 0.347 \\
                      & 71.10 & 51.99 & 18.52 & 0.349  \\
 & 72.70     & 51.44 & 18.61 & 0.351\\
\bottomrule
\end{tabular}}}
        \label{tb:lambda analysis}
	    \end{minipage}
\end{figure}




\section{Conclusions and Discussions}
\label{sec:conclusion}
In this paper, we propose energy-guided stochastic differential equations (EGSDE) for realistic and faithful unpaired I2I, which employs an energy function pretrained on both domains to guide the generation process of a pretrained SDE. Building upon two feature extractors, we carefully design the energy function to preserve the domain-independent features and discard domain-specific ones of the source image. We demonstrate the EGSDE by outperforming state-of-art I2I methods on three widely-adopted unpaired I2I tasks.

One limitation of this paper is we employ a low-pass filter as the domain-independent feature extractor for its simpleness and effectiveness while we can train more sophisticated extractor, e.g. based on disentangled representation learning methods~\cite{sanchez2020learning,chen2016infogan,higgins2016beta,kim2018disentangling,liu2018unified}, on the data in the two domains. We leave this issue in future work. In addition, we must take care to exploit the method to avoid the potential negative social impact (i.e., generating fake images to mislead people). 


\section*{Acknowledgement}

We thank Cheng Lu, Yuhao Zhou, Haoyu Liang and Shuyu Cheng for helpful discussions about the method and its limitations.
This work was supported by the National Key Research and Development Program of China (2020AAA0106302); NSF of China Projects (Nos. 62061136001, 61620106010, 62076145, U19B2034, U1811461, U19A2081, 6197222); Beijing NSF Project (No. JQ19016); Beijing Outstanding Young Scientist Program NO. BJJWZYJH012019100020098; a grant from Tsinghua Institute for Guo Qiang; the High Performance Computing Center, Tsinghua University; the Fundamental Research Funds for the Central Universities, and the Research Funds of Renmin University of China (22XNKJ13).
Part of the computing resources supporting this work, totaled 500 A100 GPU hours, were provided by High-Flyer AI. (Hangzhou High-Flyer AI Fundamental Research Co., Ltd.). J.Z was also supported by the XPlorer Prize.




\bibliographystyle{plain}
\bibliography{example.bib}



























\clearpage
\appendix


\section{Details about EGSDE}
\subsection{Assumptions about EGSDE}
\label{assumptions}
\textbf{Notations.}  is the drift coefficient.  is the diffusion coefficient.   is the score-based model.  is the energy function.  is the given source image. 

\textbf{Assumptions.} EGSDE defines a valid conditional distribution  under following assumptions: 
\begin{itemize}
  \item [(1)] 
  \item [(2)]
  \item [(3)] 
  \item [(4)] 
  \item [(5)] 
   \item [(6)] 
   \item [(7)]
\end{itemize}
\subsection{An Extention of EGSDE}
\label{sec:K}
Following SDEdit~\cite{meng2021sdedit}, we further extend the original EGSDE by repeating it  times. The general sampling procedure is summarized in Algorithm~{\ref{alg:general K}}.
\begin{algorithm}
    \caption{An extention of EGSDE for unpaired image-to-image translation}
    \label{alg:general K}
    \begin{algorithmic}
        \REQUIRE the source image , the initial time , denoising steps , weighting hyper-parameters , the similarity function , the score function , repeating times  
        \STATE 
        \STATE 
    \FOR{ to }
        \STATE  \# the start point
    \FOR{ to }
        \STATE 
        \STATE  \# sample perturbed source image from the perturbation kernel
        \STATE  \# compute energy with one Monte Carlo
        \STATE  
        \STATE  if , else 
        \STATE 
    \ENDFOR
    \STATE 
    \ENDFOR
    \STATE 
    \RETURN 
    \end{algorithmic}
\end{algorithm}
\subsection{Variance Preserve Energy-guided SDE (VP-EGSDE)}
\label{sec:VP-EGSDE}
In this section, we show a specific EGSDE: variance preserve energy-guided SDE (VP-EGSDE)~\cite{song2020score,ho2020denoising}, which is conducted in our experiments. The VP-EGSDE is defined as follows:

where  is the given source image,  is a positive function,  is a reverse-time standard Wiener process,  is an infinitesimal negative timestep,  is the score-based model in the pretrained SDE and  is the energy function. The perturbation kernel  is  and  in practice. Following~\cite{meng2021sdedit,ho2020denoising}, we use . The iteration rule from  to  of VP-EGSDE in Eq.~(\ref{eq:VP-EGSDE}) is:

where  is a small step size. \cite{song2020score} showed the iteration rule in Eq.~\eqref{eq:sampling for VP-EGSDE}  is equivalent to that using Euler-Maruyama solver when  is small in Appendix E. In other words, the score network is modified to  in EGSDE . Accordingly, we can modify the noise prediction network to  and take it into the sampling procedure in DDPM~\cite{ho2020denoising}. 

\begin{algorithm}
    \caption{VP-EGSDE for unpaired image-to-image translation}
    \label{alg:VP-EGSDE}
    \begin{algorithmic}
        \REQUIRE the source image , the initial time , denoising steps , weighting hyper-parameters , the similarity function , the score function  
        \STATE  \# the start point
        \STATE 
    \FOR{ to }
        \STATE 
        \STATE  \# sample perturbed source image from the perturbation kernel
        \STATE  \# compute energy with one Monte Carlo
        \STATE  \# the update rule in Eq. \eqref{eq:sampling for VP-EGSDE}
        \STATE  if , else 
        \STATE 
    \ENDFOR
    \STATE 
    \RETURN 
    \end{algorithmic}
\end{algorithm}



\subsection{EGSDE as Product of Experts}
\label{sec:products}
In this section, we provide more details about the \emph{product of experts}~\cite{hinton2002training} explanation for the discretized sampling process of EGSDE. Recall that we construct the  as follows:

where  is the partition function,  is the transition kernel of the pretrained SDE, i.e.,  and . For brevity, we denote .  Assuming that  has low curvature relative to , then we can use Taylor expansion around  to approximate it: 

where . Taking it into Eq. \eqref{eq:new kernel norm}, we can get:

Therefore,

Therefore, solving the EGSDE in a discretization manner is approximately equivalent to drawing samples from a product of experts. 
\begin{figure}
  \centering
  \includegraphics[width=1.0\columnwidth]{images/appendix_qualitative.pdf}
  \caption{More qualitative results on three unpaired I2I tasks.}
  \vspace{-0.5cm}
  \label{fig:more qualitative results}
\end{figure}
\subsection{The Connection with Classifier Guidance}
\label{sec:classifier guidance}
In this section, we show the classifier guidance\cite{dhariwal2021diffusion} can be regarded as a special design of energy function and provide an alternative explanation of the classifier guidance as a product of experts.

Recall that the EGSDE, which leverages an energy function to guide the inference process of a pretrained SDE, is defined as follows:

which defines a distribution  conditioned on c. Let , where  is a time-dependent classifier and  is the class label, the EGSDE can be rewritten as:

Sovling variance preserve energy-guided SDE (VP-EGSDE) in Eq. \ref{eq:classifier guidance} with Euler-Maruyama solver is equal to the classifier guidance in \cite{dhariwal2021diffusion,song2020score}.


Assuming , we can define a conditional distribution  at time  as follows: 


According to the analysis in section \ref{sec:products as experts}, solving the EGSDE in Eq. \ref{eq:classifier guidance} is approximately equivalent to drawing samples from a product of experts as follows:

where  is the marginal distribution at time  defined by a pretrained SDE. Therefore, the generated samples in classifier guidance approximately follow the distribution:

Similarly, combining a conditional socre-based model and a classifier in \cite{dhariwal2021diffusion} is approximately equivalent to drawing samples from a product of experts as follows:

where  is the marginal distribution at time  defined by a pretrained SDE.
\section{Implementation Details}
\label{sec:implementation}
\subsection{Datasets}
To validate our method, we conduct experiments on the following datasets: 

(1) CelebA-HQ~\cite{karras2018progressive} contains high quality face images and is separated into two domains: male and female. For training data, it contains 10057 male images and 17943 female images. Each category has 1000 testing images. MaleFemale task was conducted on this dataset. 

(2) AFHQ~\cite{choi2020stargan} consists of high-resolution animal face images including three domains: cat, dog and wild, which has relatively large variations. For training data, it contains 5153, 4739 and 4738 images for cat, dog and wild respectively. Each domain has 500 testing images. We performed CatDog, WildDog and multi-domain translation on this dataset. 

During training, all images are resized 256256, randomHorizontalFliped with p = 0.5, and scaled to . During sampling, all images are resized 256256 and scaled to .  \begin{table}[]
\caption{The used codes and license.}
\vspace{.2cm}
\label{tb:code}
\centering
\renewcommand\arraystretch{1.2}
\begin{tabular}{ccc}
\toprule
URL     & citations & License  \\
\midrule
\href{https://github.com/openai/guided-diffusion}{https://github.com/openai/guided-diffusion} & \cite{dhariwal2021diffusion}&MIT License\\
\href{https://github.com/taesungp/contrastive-unpaired-translation}{https://github.com/taesungp/contrastive-unpaired-translation} & \cite{park2020contrastive}       & BSD License    \\
\href{https://github.com/jychoi118/ilvr_adm}{https://github.com/jychoi118/ilvradm} & \cite{choi2021ilvr}      & MIT License \\
\href{https://github.com/ermongroup/SDEdit}{https://github.com/ermongroup/SDEdit} & \cite{meng2021sdedit}      & MIT License\\
\href{https://github.com/mseitzer/pytorch-fid}{https://github.com/mseitzer/pytorch-fid} &\cite{heusel2017gans} &Apache V2.0 License\\
\bottomrule
\end{tabular}
\end{table}
\subsection{Code Used and License}
All used codes in this paper and its license are listed in Table \ref{tb:code}.
\subsection{Details of the Score-based Diffusion Model}
On CatDog and WildDog, we use the public pre-trained score-based diffusion model (SBDM) provided in the official code
\href{https://github.com/jychoi118/ilvr_adm}{https://github.com/jychoi118/ilvradm} of ILVR~\cite{choi2021ilvr}. The pretrained model includes the variance and mean networks and we only use the mean network. 

On Male  Female, we trained a SBDM for  iterations on the training set of female category using the recommended training code by SDEdit \href{https://github.com/ermongroup/ddim}{https://github.com/ermongroup/ddim}. We use the same setting as SDEdit~\cite{meng2021sdedit} and DDIM~\cite{song2020denoising} for a fair comparison, where the models is trained with a batch size of , a learning rate
of , the Adam optimizer with ,  and grad clip = 1.0, an exponential moving average (EMA) with a rate of 0.9999. The U-Net architecture is the same as \cite{ho2020denoising}. The timesteps  is 1000
 and the noise schedule is linear as described in \ref{sec:VP-EGSDE}. 
 \subsection{Details of the Domain-specific Feature Extractor}
 The domain-specific feature extractor  is the all but the last layer of a classifier that is trained on both the source and target domains. The time-dependent classifier is trained using the official code \href{https://github.com/openai/guided-diffusion}{https://github.com/openai/guided-diffusion} of \cite{dhariwal2021diffusion}. We use the ImageNet (256256) pretrained classifier provided in \href{https://github.com/openai/guided-diffusion}{https://github.com/openai/guided-diffusion} as the initial weight and train  iterations for two-domain I2I and  iterations for multi-domain I2I. We train the classifier with a batch size of , a learning rate of  with the AdamW optimizer (weight decay = 0.05). For the architecture, the depth is set to 2, the channels is set to 128, the attention resolutions is set to 32,16,8 and the other hyperparameters are the default setting. The timesteps  is 1000 and the noise schedule is linear.
 

\begin{table}[]
\caption{Computation cost comparison.}
\label{tb : computation cost}
\centering
\vspace{.2cm}
\renewcommand\arraystretch{1.2}
\begin{tabular}{lcccc}
\toprule
 Methods      & sec/iter     & Mem(GB)    \\
\midrule
CUT & 0.24 & 2.91  \\
ILVR  & 60 & 1.84  \\ 
SDEdit & 33 & 2.20  \\ 
EGSDE & 85 & 3.64  \\ 
\bottomrule
\end{tabular}
\end{table}

 \subsection{Training and Inference Time}
 On Cat  Dog, training the domain-specific feature extractor for  iterations takes 7 hours based on 5 2080Ti GPUs. The computation cost comparison for sampling is shown in Table \ref{tb : computation cost}, where the batch size is set 1. Compared with the ILVR, ours takes 1.42 times as long as ILVR. The speed of inference can be improved further by the latest progress on faster sampling~\cite{song2020denoising,bao2021analytic}.
 \begin{figure}
  \centering
  \includegraphics[width=1.0\columnwidth]{images/failure.pdf}
  \caption{Selected failure cases. On Cat  Dog, the EGSDE sometimes fails to generate eyes and noses. On Wild  Dog, the EGSDE sometimes preserves some undesired features of the source image like tiger stripes. On Male  Female, the EGSDE fails to change the hairstyle. }
  \vspace{-0.5cm}
  \label{fig:failure cases}
\end{figure}
\subsection{Evaluation}
\label{sec:evaluation}
\textbf{FID}. We evaluate the FID metric using the code \href{https://github.com/mseitzer/pytorch-fid}{https://github.com/mseitzer/pytorch-fid}. On AFHQ dataset, following CUT~\cite{park2020contrastive}, we use the test data as reference without any data preprocessing. On CelebA-HQ dataset, following StarGANv2~\cite{park2020contrastive}, we use the training data as reference and conduct the following data preprocessing: resize images to 256, 299 and then normalize data with . Note that the FID evaluation in StarGANv2 is still different with ours because it generates 10 images for each source image.


\textbf{Human evaluation.} We evaluate the human preference from both faithfulness and realism aspects via the Amazon Mechanical Turk (AMT). Given a source image, the AMT workers are instructed to select which translated image is more satisfactory in the pairwise comparisons between the baselines and EGSDE. The reward for each pair of picture comparison is kept as 0.02\
     \label{eq:noisefree}
     \gE(\vy, \vx, t)  
     &= \lambda_s \mathcal{S}_{s}(\vy, \vx , t) - \lambda_i  \mathcal{S}_{i}(\vy, \vx , t) ,

which does not require to take the expectation w.r.t. . As shown in Table \ref{tb:variant}, it did not perform well because it is not reasonable to measure the similarity between the noise-free source image and the transferred sample in a gradual denoising process. 


\subsection{Choice of Initial Time }
\label{sec:initial time}
\begin{table}
\caption{The results of different initial time . The larger  results in more realistic and less faithful images.}
\vspace{.2cm}
\label{tb:initial time}
\centering
\renewcommand\arraystretch{1.2}
\begin{tabular}{ccccc} 
\toprule
Initial Time         & FID                   & L2                    & PSNR                  & SSIM                    \\
\midrule
\multicolumn{5}{c}{Cat  Dog}                                                                          \\ 
\midrule
0.3T & 97.02 & 33.39 & 22.17 & 0.516 \\
0.4T & 78.64 & 39.95 & 20.70 & 0.461 \\
0.5T & 65.23 & 47.15 & 19.32 & 0.415 \\
0.6T & 57.31 & 55.98 & 17.88 & 0.374 \\
0.7T & 53.01 & 65.61 & 16.55 & 0.333 \\
\midrule
\multicolumn{5}{c}{Wild  Dog}                                                                         \\
\midrule
0.3T & 96.80 & 38.76 & 20.93 & 0.472 \\
0.4T & 73.86 & 46.50 & 19.43 & 0.395 \\
0.5T & 58.82 & 54.34 & 18.14 & 0.344 \\
0.6T & 55.53 & 62.52 & 16.94 & 0.307 \\
0.7T & 54.56 & 72.02 & 15.72 & 0.274 \\
\midrule
\multicolumn{5}{c}{Male  Female}                                                         \\
\midrule
0.3T & 51.66 & 31.66 & 22.71 & 0.639 \\
0.4T & 47.13 & 36.74 & 21.48 & 0.605 \\
0.5T & 42.09 & 42.03 & 20.35 & 0.574 \\
0.6T & 36.07 & 48.94 & 19.09 & 0.534 \\
0.7T & 30.59 & 59.18 & 17.48 & 0.472      \\
\bottomrule
\end{tabular}
\end{table}
In this section, we explore the effect of the initial time . The quantitative results are shown in Table \ref{tb:initial time}. We found that the larger  results in more realistic and less faithful images, because it preserve less information of the source image at start time with the increase of .
\subsection{Repeating  Times}
\label{sec:repeating K}
\begin{table}[]
\caption{Comparison with SDEdit~\cite{meng2021sdedit} under different  times.}
\label{tb : K time appendix}
\centering
\vspace{.2cm}
\renewcommand\arraystretch{1.2}
\begin{tabular}{cccccccccc}
\toprule
\multirow{2}{*}{Methods} & \multirow{2}{*}{K} & \multicolumn{4}{c}{Wild  Dog}  & \multicolumn{4}{c}{Cat  Dog} \\
 \cmidrule(r){3-6}
 \cmidrule(r){7-10}
                         &                    & FID   & L2    & PSNR  & SSIM   & FID    & L2     & PSNR   & SSIM   \\
\midrule
SDEdit~\cite{meng2021sdedit}                  &  \multirow{2}{*}{1}                  & 68.22 & 55.38 & 17.97 & \textbf{0.342 }       & 73.70  & 47.74   & 19.22 & \textbf{0.424 }   \\
EGSDE                   &                   & \textbf{58.85} & \textbf{54.38} & \textbf{18.13} & \textbf{0.342} & \textbf{66.34} & \textbf{47.20}  & \textbf{19.30}  & 0.415 \\
\hdashline
SDEdit~\cite{meng2021sdedit}                    & \multirow{2}{*}{2}                   & 60.91 & 62.32 & 16.97 & 0.312 & 65.59          & 55.10           & 18.01          & \textbf{0.395}      \\
EGSDE                    &                   & \textbf{55.47} & \textbf{60.25} & \textbf{17.28} & \textbf{0.314} & \textbf{62.23} & \textbf{53.45} & \textbf{18.26} & 0.385\\
\hdashline
SDEdit~\cite{meng2021sdedit}                    & \multirow{2}{*}{3}                  & 60.52 & 66.16 & 16.46 & 0.303 & 61.10           & 59.69          & 17.33          & \textbf{0.382}     \\
EGSDE                   &                   & \textbf{55.07} & \textbf{63.15} & \textbf{16.86} & \textbf{0.304} & \textbf{59.78} & \textbf{56.41} & \textbf{17.81} & 0.376 \\
\bottomrule
\end{tabular}
\end{table}
In this section, we provide the comparison with SDEdit~\cite{meng2021sdedit} under different  times on Cat  Dog and Wild  Dog. The experimental results are reported in Table \ref{tb : K time appendix} and it is consistent with the results in the main text on Male  Female. With the increase of , the faithful metrics of SDEdit decrease sharply, because it only utilizes the source image at the initial time . 
\subsection{Choice of  and }
\label{sec:expert function}
\begin{table}[]
\caption{The results of different  and .  corresponds to SDEdit~\cite{meng2021sdedit}.}
\vspace{.2cm}
\label{tb:lambda analysis appendix}
\centering
\renewcommand\arraystretch{1.2}
\begin{tabular}{ccccccccc}
\toprule
\multirow{2}{*}{} & \multicolumn{4}{c}{Cat  Dog}     & \multicolumn{4}{c}{Male  Female} \\
 \cmidrule(r){2-5}
 \cmidrule(r){6-9}
 & FID       & L2     & PSNR   & SSIM  & FID     & L2      & PSNR  & SSIM  \\
\midrule
              & 73.85 & 47.87   & 19.19 & 0.423  & 49.68  & 43.68  & 20.03 & 0.572 \\
\hdashline
                     & 66.17 & 48.56   & 19.07 & 0.419 & 44.97  & 44.26  & 19.92 & 0.569 \\
                    & 62.44 & 51.02   & 18.64 & 0.405 & 38.44  & 45.92  & 19.6  & 0.559 \\
                    & 60.14 & 52.92   & 18.33 & 0.397 & 36.14  & 47.05  & 19.39 & 0.551 \\
\hdashline
                      & 74.09 & 45.58   & 19.61 & 0.428 & 50.77  & 41.67  & 20.43 & 0.58  \\
                      & 77.05 & 44.23   & 19.86 & 0.431 & 51.42  & 40.29  & 20.71 & 0.585 \\
                     & 79.12 & 43.63   & 19.98 & 0.433 & 52.13  & 39.57  & 20.87 & 0.588\\
\bottomrule
\end{tabular}
\end{table}


In this section, we provide the effect of weighting hyper-parameter  and  on Cat  Dog and Male  Female. The results are shown in Table \ref{tb:lambda analysis appendix} and Figure \ref{fig: weight}. It is consistent with the results in the full text on Wild  Dog.
Larger  results in more realistic images and larger  results in more faithful images. 

\begin{figure}
  \centering
  \includegraphics[width=1.0\columnwidth]{images/weight.pdf}
  \caption{The qualitative results about the ablation of experts.}
  \vspace{-0.5cm}
  \label{fig: weight}
\end{figure}

\subsection{More Qualitative Results}
\label{sec:more resutls}
In this section, we show more qualitative results on three unpaired I2I tasks using the default hyper-parameters in Figure ~\ref{fig:more qualitative results}. We also select some failure cases in Figure \ref{fig:failure cases} and randomly selected qualitative results in Figure \ref{fig: random select}.
\begin{figure}
  \centering
  \includegraphics[width=1.0\columnwidth]{images/appendix_random_select.pdf}
  \caption{The randomly selected qualitative results with EGSDE.}
  \vspace{-0.5cm}
  \label{fig: random select}
\end{figure}

\subsection{Comparison with StarGAN v2}
In this section, we compare the EGSDE with StarGAN v2\cite{choi2020stargan} on the most popular benchmark . Since the FID measurement in CUT we mainly follow and StarGAN v2 is different, for fairness, we perform experiments under both FID metrics. The results are shown in Table \ref{tb:two doamin}  and Table \ref{tb : starganv2}. The qualitative comparisons are shown in Figure \ref{fig: StarGAN v2}. 

As shown in Table  \ref{tb:two doamin}  and Table \ref{tb : starganv2}, the EGSDE outperforms StarGAN v2 in all metrics under the two different measurements. It also should be noted that the three faithful metrics for StarGAN v2 is much worse than ours. This is because the goal of StarGAN v2 is to generate diverse images over multi-domains, which pays little attention into the faithfulness.  The qualitative comparisons in Figure \ref{fig: StarGAN v2} shows that StarGAN v2 loses much domain-independent features (e.g. background and color) while EGSDE can still retain them.



\begin{table}[]
\caption{ The comparison with StarGAN v2\cite{choi2020stargan} on Cat  Dog following the FID measurement of StarGAN v2. The EGSDE use the default-parameters () and EGSDE use the parameters (). }
\label{tb : starganv2}
\centering
\vspace{.2cm}
\renewcommand\arraystretch{1.2}
\begin{tabular}{lcccc}
\toprule
 Methods      & FID    \\
\midrule
StarGAN v2~\cite{choi2020stargan}   & 36.37  \\
EGSDE  & 48.20\\
EGSDE  & \textbf{31.14}\\
\bottomrule
\end{tabular}
\end{table}
\begin{figure}
  \centering
  \includegraphics[width=1.0\columnwidth]{images/starganv2.pdf}
  \caption{The qualitative comparisons with StarGAN v2. Each method generates five images by random seed for each source image. StarGAN v2 loses much domain-independent features (e.g. background and color) while EGSDE can still retain them.}
  \vspace{-0.5cm}
  \label{fig: StarGAN v2}
\end{figure}
\begin{table}[]
\caption{The results of replacing  with classifier guidance on Cat  Dog.}
\label{tb : classifier-guidance}
\centering
\vspace{.2cm}
\renewcommand\arraystretch{1.2}
\begin{tabular}{lcccc}
\toprule
 Methods      & FID    & L2     & PSNR   & SSIM    \\
\midrule
EGSDE () & 65.82  & 47.22  & 19.31  & 0.415 \\
EGSDE-Classifier( ) & 73.36 & 46.4 & 19.75 & 0.430  \\
EGSDE-Classifier ()  & 71.90 & 46.8 & 19.67 & 0.428  \\ 
EGSDE-Classifier ()  & 68.80 & 47.89 & 19.46 & 0.423  \\ 
\bottomrule
\end{tabular}
\end{table}

\begin{table}[]
\caption{The comparison with EGSDE-DDIM.}
\label{tb : ddim}
\centering
\vspace{.2cm}
\renewcommand\arraystretch{1.2}
\begin{tabular}{lcccc}
\toprule
 Methods      & FID    & L2     & PSNR   & SSIM    \\
\midrule
EGSDE-DDPM()            & 74.17          & 47.88         & 19.19          & 0.423 \\
EGSDE-DDPM() & 65.82  & 47.22  & 19.31  & 0.415\\ EGSDE-DDPM() &62.44 & 51.02   & 18.64 & 0.405\\ EGSDE-DDPM() & 77.05 & 44.23   & 19.86 & 0.431\\ 
\hline
EGSDE-DDIM()    & 88.29 & 41.93 & 20.62  & 0.472 \\

EGSDE-DDIM()    & 78.11 & 41.95 & 20.61  & 0.468 \\
EGSDE-DDIM()    & 74.32 & 44.32 & 20.12  & 0.460 \\
EGSDE-DDIM()    & 91.81 & 39.80 & 21.10  & 0.481 \\
\bottomrule
\end{tabular}
\end{table}



\begin{table}[]
\caption{ Quantitative results in multi-domain translation, where the source domain includes \emph{Cat} and \emph{Wild} and the target domain is \emph{Dog}. All experiments are repeated 5 times to eliminate randomness.}
\label{tb : multi-domain}
\centering
\vspace{.2cm}
\renewcommand\arraystretch{1.2}
\begin{tabular}{lcccc}
\toprule
 Methods      & FID    & L2     & PSNR   & SSIM    \\
\midrule
ILVR~\cite{choi2021ilvr}   & 74.85 ± 1.24 & 60.16 ± 0.14 & 17.31 ± 0.02  & 0.325 ± 0.001 \\
SDEdit~\cite{meng2021sdedit} & 71.34 ± 0.64  & 51.62 ± 0.05 & 18.58 ± 0.01 & \textbf{0.383 ± 0.001 } \\
EGSDE  & \textbf{64.02 ± 0.43} & \textbf{50.74 ± 0.04} & \textbf{18.73 ± 0.01 } & 0.373 ± 0.000 \\
\bottomrule
\end{tabular}
\end{table}

\section{Multi-Domain Image Translation}
\label{sec:multi-domain}
    Following~\cite{choi2021ilvr}, we extend our method into multi-domain translation on AFHQ dataset, where the source domain includes \emph{Cat} and \emph{Wild} and the target domain is \emph{Dog}. In this setting, similar to two-domain unpaired I2I, the EGSDE also employs an energy function pretrained on both the source and target domains to guide the inference process of a pretrained SDE. The only difference is the domain-specific feature extractor  involved in the energy function is the all but the last layer of a three-class classifier rather than two-class. All experiments are repeated 5 times to eliminate randomness. The quantitative results are reported in Table~\ref{tb : multi-domain}. We can observe that the EGSDE outperforms the baselines in almost all realism and faithfulness metrics, showing the great generalization of our method.




\end{document}
