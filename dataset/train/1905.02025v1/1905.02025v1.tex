\pdfoutput=1

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}



\usepackage[ruled,norelsize]{algorithm2e}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{tabularx}

\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy 

\def\cvprPaperID{23} \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\begin{document}

\title{DisplaceNet: Recognising Displaced People from Images by Exploiting Dominance Level}

\author{
	Grigorios Kalliatakis, \,\,
	Shoaib Ehsan, \,\,
	Maria Fasli, \,\,
	Klaus McDonald-Maier\vspace{3pt}\\
	School of Computer Science and Electronic Engineering, University of Essex\\
	{\tt\small \{gkallia, sehsan, mfasli, kdm\}@essex.ac.uk} 
}

\maketitle


\begin{abstract}
	Every year millions of men, women and children are forced to leave their homes and seek refuge from wars, human rights violations, persecution, and natural disasters. The number of forcibly displaced people came at a record rate of 44,400 every day throughout 2017, raising the cumulative total to 68.5 million at the yearâ€™s end, overtaken the total population of the United Kingdom. Up to 85\% of the forcibly displaced find refuge in low- and middle-income countries, calling for increased humanitarian assistance worldwide. To reduce the amount of manual labour required for human-rights-related image analysis, we introduce DisplaceNet, a novel model
	which infers potential displaced people from images by integrating the control level of the situation and conventional convolutional neural network (CNN) classifier into one framework for image classification. Experimental results show that DisplaceNet achieves up to 4\% coverage\textendash the proportion of a data set for which a classifier is able to produce a prediction\textendash gain over the sole use of a CNN classifier. Our dataset, codes and trained models will be available online at \url{https://github.com/GKalliatakis/DisplaceNet}
\end{abstract}

\section{Introduction}

The displacement of people refers to the forced movement of people from their locality or environment and occupational activities \footnote{A distinction is often made between conflict-induced and disaster-induced displacement, yet the lines between them may be blurred in practice.}. It is a form of social change caused by a number of factors such as armed conflict, violence, persecution and human rights violations. Globally, there are now almost 68.5 million forcibly displaced people\textendash and most are hosted in developing regions, while today 1 out of every 110 people in the world is displaced \cite{global_trends_report}.

In the era of social media and big data, the use of visual evidence to document conflict and human rights abuse has become an important element for human rights organizations and advocates. However, the omnipresence of visual evidence
may deluge those accountable for analysing it. Currently, information extraction from human-rights-related imagery requires manual labour by human rights analysts and advocates. Such analysis is time consuming, expensive, and remains emotionally traumatic for analysts to focus on images of horrific events. In this work, we strive to reconcile this gap by automating parts of this process; given a single image we label the image as either \textit{displaced people} or \textit{non displaced people}. Figure \ref{Fig. 1} illustrates that naive schemes based solely on object detection or scene recognition are doomed to
fail in this binary classification problem. If we can exploit existing smartphone cameras, which are ubiquitous, it may be possible to turn recognition of displaced populations into a powerful and cost-effective computer vision application that could improve humanitarian responses. 


\begin{figure}[t!]
	\centering
	\begin{tabular}{cc}
		\includegraphics[width=0.2\textwidth,height=0.38\textheight,keepaspectratio]{figures/Fig1_a.jpg} &   \includegraphics[width=0.22\textwidth,height=0.43\textheight,keepaspectratio]{figures/Fig1_b.jpg} \\
		(a) Child playing & (b) Displaced people \ \label{eq:1}
s_{img,d}^{DP} = s_h  \cdot s_{h,img}^{d}  \cdot s_{img}^{DP}
 \label{eq:2}
s_{img}^{d} = \frac{1}{n} \sum_{i=1}^{n} s_{h,img}^d  
7pt]
		\includegraphics[width=0.23\textwidth,height=0.4\textheight,keepaspectratio]{figures/VAD_pos.jpg} &   \includegraphics[width=0.23\textwidth,height=0.4\textheight,keepaspectratio]{figures/DOMINANCE_pos.jpg} \\
		(c)  & (d)  \ \label{eq:6}
s_{img,d}^{DP} = s_{img}^{DP} 
-1pt]
		\includegraphics[width=0.29\textwidth,height=0.3\textheight,keepaspectratio]{figures/results_4.jpg} &   \includegraphics[width=0.35\textwidth,height=0.4\textheight,keepaspectratio]{figures/results_5.jpg} &
		\includegraphics[width=0.26\textwidth,height=0.3\textheight,keepaspectratio]{figures/results_6.jpg} \\
		(e) & (f)  & (g)\1pt]
	\end{tabular}
	\caption{Displaced people detected by our method. Each image shows two predictions alongside their probabilities. Top prediction is given by DisplaceNet, while the bottom prediction is given by the respective vanilla CNN. Green colour implies that no displace people were detected, while red colour signifies that displaced people were detected. In some instances such as (a), (b), and (c), DisplaceNet overturns the initial-false prediction of the vanilla CNN, where in other instances such as (e), (f) and (g), DisplaceNet strengthens the initial-true prediction, resulting in higher coverage.}
	\label{Fig. 5}
\end{figure*}

\subsection{Training}

Due to different datasets, convergence times and loss imbalance, all three branches have been trained separately. For object detection we adopted an existing implementation of the RetinaNet object detector, pre-trained on the COCO dataset \cite{lin2014microsoft}, with a ResNet-50 backbone. 

For emotion recognition in continuous dimensions, we formulate this task as a regression problem using the Euclidean loss. The two feature extraction modules are designed as truncated versions of various well-known CNNs and initialised using pretrained models on two large-scale image classification datasets, ImageNet \cite{krizhevsky2012imagenet} and Places \cite{zhou2018places}. The truncated version of those CNNs removes the fully connected layer and outputs features from the last convolutional layer in order to maintain the localisation of different parts of the images which is significant for the task at hand. Features extracted from these two modules (red and blue boxes in Fig. \ref{Fig. 2}B) are then combined by a fusion module. This module first uses a global average pooling layer to reduce the number of features from each network and then a fully connected layer, with an output of a 256-D vector,  functions as a dimensionality reduction layer for the concatenated pooled features. Finally, we include a second fully connected layer with 3 neurons representing valence, arousal and dominance. The parameters of the three modules are learned jointly using stochastic gradient descent with momentum of 0.9. The batch size is set to 54 and we use dropout with a ratio of 0.5.

We formulate displaced people recognition as a binary classification problem. We train an end-to-end model for classifying everyday images as displaced people positive or displaced people negative, based on the context of the images. We fine-tune various CNN models for the two-class classification task. First, we conduct feature extraction utilising only the convolutional base of the original networks in order to end up with more generic representations as well as retaining spatial information similar to emotion recognition pipeline. The second phase consists of unfreezing some of the top layers of the convolutional base and jointly training a newly added fully connected layer and these top layers.

All the CNNs presented here were trained using the Keras Python deep learning framework \cite{chollet2015keras} over TensorFlow \cite{abadi2016tensorflow} on Nvidia GPU P100.


\section{Dataset and Metrics}

There are a limited number of image datasets for human rights violations recognition \cite{kalliatakis2019exploring, visapp17}. In order to find the main test platform on which we could demonstrate the effectiveness of DisplaceNet and analyse its various components, we construct a new image dataset by maintaining the verified samples intact for the category \textit{displaced populations}. The constructed dataset contains 609 images of displaced people and the same number of non displaced people counterparts for training, as well as 100 images collected from the web for testing and validation. The dataset is made publicly available for future research.
We evaluate DisplaceNet with two metrics \textit{accuracy} and \textit{coverage} and compare its performance against the sole use of a CNN classifier.


\section{Experiments}

\noindent
\textbf{Implementation details.} Our emotion recognition implementation is based on the emotion recognition in context (EMOTIC) model \cite{kosti2017emotion}, with the difference that our model estimates only continuous dimensions in VAD space. We train the three main modules on the EMOTIC database, which contains a total number of 18,316 images with 23,788 annotated people, using pre-trained CNN feature extraction modules. We treat this multiclass-multilabel problem as a regression problem by using a weighted Euclidean loss to compensate for the class imbalance of EMOTIC. 

For the classification part, we fine-tune our models for 50 iterations on the HRA subset with a learning rate of 0.0001 using the stochastic gradient descent (SGD) \cite{lecun1989backpropagation} optimizer for cross-entropy minimization. These \textit{vanilla} models will be examined against DisplaceNet. Here, vanilla means pure image classification using solely fine-tuning without any alteration. 



\noindent
\textbf{Baseline.} To enable a fair comparison between vanilla CNNs and DisplaceNet, we use the same backbone combinations for all modules described in Fig. \ref{Fig. 2}. We report comparisons in both \textit{accuracy} and \textit{coverage} metrics for fine-tuning up to two convolutional layers in order to be consistent with the implementation of \cite{kalliatakis2019exploring}. The per-network results are shown in Table \ref{tab1}. The implementation of vanilla CNNs is solid with up to 61.5\% accuracy. 
Regarding coverage, vanilla CNNs achieve up to 16.83\%. This shows that it is possible to trade coverage with accuracy in the context of human rights image analysis. One can always obtain high accuracy by refusing to process a number of examples, but this reduces the coverage of the system. Nevertheless, vanilla CNNs provide a strong baseline to which we will compare our method.

Our method, has a mean coverage of 20.83\%. This is an absolute gain of 4 points over the baseline of 16.83\%. This is a relative improvement of 23.76\%. In relation to accuracy, DisplaceNet has a mean accuracy of 57.33\% which is an absolute drop of 4.17 points over the strong baselines of 61.5\%. This indicates a relative loss of only 6.7\%. We believe that this negligible drop in accuracy is mainly due to the fact that the  set is not solely made up of images with people in their context, it also contains images of generic objects and scenes, where only the sole classifier's prediction is taken into account.


\begin{table}[t!]
	\centering
	\resizebox{\columnwidth}{!}{\begin{tabular}{c|c|cc|cc}
			\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}backbone \\ network\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}layers \\ fine-tuned\end{tabular}} & \multicolumn{2}{c|}{vanilla CNN} & \multicolumn{2}{c|}{\textbf{DisplaceNet}} \\ \cline{3-6} 
			&                                                                               & Top-1 acc.       & Coverage      & Top-1 acc.     & \textbf{Coverage}     \\ \hline
			VGG16                                                                        & \multirow{4}{*}{1}                                                            & 58\%             & 0\%           & 54\%           & \textbf{3\%}       \\
			VGG19                                                                        &                                                                               & 69\%             & 3\%           & 60\%           & \textbf{6\%}         \\
			ResNet50                                                                     &                                                                               & 60\%             & 0\%           & 55\%           & \textbf{4\%}         \\ \hline
			VGG16                                                                        & \multirow{4}{*}{2}                                                            & 63\%             & 43\%          & 63\%           & \textbf{49\%}         \\
			VGG19                                                                        &                                                                               & 77\%             & 54\%          & 74\%           & \textbf{58\%}         \\
			ResNet50                                                                     &                                                                               & 42\%             & 1\%           & 38\%           & \textbf{5\%}         \\ \hline
			\textbf{mean}                                                                        & -                                                            & 61.5\%             & 16.83\%            & 57.33\%           & \textbf{20.83\%}           \\ \hline
		\end{tabular}
	}
	\caption{\label{tab1}Detailed results on displaced people recognition using DisplaceNet. We show the main baseline and DisplaceNet for various network backbones. We bold the leading entries on coverage.}
\end{table}



\noindent
\textbf{Qualitative results.} We show our human rights abuse detection results in Fig. \ref{Fig. 5}. Each subplot illustrates two predictions alongside their probability scores. The top of the two predictions is given by DisplaceNet, while the bottom one is given by the respective vanilla CNN sole classifier. Our method can successfully classify displaced people by overturning the initial-false prediction of the vanilla CNN. Moreover, DisplaceNet can strengthen the initial-true prediction of the sole classifier. Finally, our method can be incorrect, because of false dominance score inferences. Some of them are caused by a failure of continuous dimensions emotion recognition, which is an interesting open problem for future research.






\section{Conclusion}

We have presented a human-centric approach for recognising displaced people from still images. This two-class labelling problem is not trivial, given the high-level image interpretation required. Understanding a person's control level of the situation from his frame of reference is closely related with situations where people have been forcibly displaced. Thus, the key to our computational framework is people's dominance level, which resonates well with our own common sense in judging potential displacement cases. We introduce the overall dominance level of the image which is responsible for weighting the classifiers prediction during inference. We benchmark performance of our DisplaceNet model against sole CNN classifiers. Our experimental results showed that this is an effective strategy, which we believe has good potential beyond human rights related classification. We hope this paper will spark interest and subsequent work along this line of research. All our code and data are publicly available.


























{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
