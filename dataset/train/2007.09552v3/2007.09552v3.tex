\documentclass[journal]{IEEEtran}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{calligra}
\usepackage{color}
\usepackage{array}
\usepackage{hhline}
\usepackage{subfig}
\usepackage{multirow}
\usepackage{caption}
\usepackage{boxhandler}
\usepackage{tabularx}
\usepackage{adjustbox}
\usepackage{verbatim}
\usepackage{bm}
\usepackage{float}
\usepackage{amssymb}\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}

\hyphenation{op-tical net-works semi-conduc-tor}
\begin{document}
	\title{Progressive Multi-Scale Residual Network for Single Image Super-Resolution}
	\author{
		Yuqing~Liu,
		Xinfeng~Zhang,~\IEEEmembership{Member,~IEEE,}
		Shanshe~Wang,
		Siwei~Ma,~\IEEEmembership{Member,~IEEE,}
		and~Wen~Gao,~\IEEEmembership{Fellow,~IEEE}\thanks{Y. Liu is with the School of Software, Dalian University of Technology, 
			Dalian 116620, China (e-mail:liuyuqing@mail.dlut.edu.cn).}
		\thanks{X. Zhang is with the School of Computer Science and Technology, University of the Chinese Academy of Sciences, Beijing 100049, China (e-mail:
			xfzhang@ucas.ac.cn).}
		\thanks{S. Wang, S. Ma, and W. Gao are with the School of Electronics Engineering and Computer Science, Institute of Digital Media, Peking University, Beijing 100871, China (e-mail: sswang@pku.edu.cn; swma@pku.edu.cn; wgao@pku.edu.cn).}
	}

	\markboth{manuscript}{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
	
	

	\maketitle
	
	
\begin{abstract}
Multi-scale convolutional neural networks (CNNs) achieve significant success in single image super-resolution (SISR), which considers the comprehensive information from different receptive fields.
However, recent multi-scale networks usually aim to build the hierarchical exploration with different sizes of filters, which lead to high computation complexity costs, and seldom focus on the inherent correlations among different scales.
This paper converts the multi-scale exploration into a sequential manner, and proposes a progressive multi-scale residual network (PMRN) for SISR problem. 
Specifically, we devise a progressive multi-scale residual block (PMRB) to substitute the larger filters with small filter combinations, and gradually explore the hierarchical information.
Furthermore, channel- and pixel-wise attention mechanism~(CPA) is designed for finding the inherent correlations among image features with weighting and bias factors, which concentrates more on high-frequency information.
Experimental results show that the proposed PMRN recovers structural textures more effectively with superior PSNR/SSIM results than other small networks. The extension model PMRN with self-ensemble achieves competitive or better results than large networks with much fewer parameters and lower computation complexity.
 


\end{abstract}

\begin{IEEEkeywords}
	Image super-resolution, multi-scale network, attention mechanism, progressive design.
\end{IEEEkeywords}


\IEEEpeerreviewmaketitle
	
\section{Introduction}
\IEEEPARstart{S}{ingle} image super-resolution (SISR) aims to recover the high resolution (HR) image from corresponding low resolution (LR) instance. SISR is a highly ill posed issue, which may comprehensively contain down-sampling, blurry, noise, and other degradation situations~\cite{review2_3}. As a traditional image restoration issue, SISR is widely applied in different computer vision tasks, such as video codec~\cite{tcsvt_1}, view synthesis~\cite{tcsvt_2}, facial analysis~\cite{tcsvt_3}, pansharpening~\cite{review2_5}, and safety driving~\cite{review2_4}. 

Convolutional neural networks (CNNs) have demonstrated impressive restoration performances on SISR problem with powerful feature representation and exploration capacities. SRCNN proposed by Dong~\textit{et al.} is the first CNN-based work for image SR. VDSR~\cite{vdsr}, EDSR~\cite{edsr_lim2017}, CARN~\cite{carn} and other works aims to build a deeper network with well-designed blocks for better restoration performance. Among different elaborate architectures, multi-scale design proves to be one of the effective network patterns for image restoration, which considers the comprehensive information from different scales. Inspired by Laplacian pyramid, Lai~\textit{et al.} proposed LapSRN~\cite{lapsrn} for graduate image super-resolution with different scaling factors. MSRN~\cite{msrn} devised by Li~\textit{et al.} extracts the multi-scale features with different sizes of filters. However, the larger filters lead to more parameters and high computation costs. 
Existing works seldom concentrate on the inherent correlations among features from different scales. In practice, small scale information contains richer texture details, and may be helpful for larger scale structural information exploration. 

Multi-scale exploration comprehensively considers the hierarchical information, which treat different feature maps equally. Attention mechanism aims to focus more on important information and textures. SENet~\cite{senet_hu2018} proposed by Hu~\textit{et al.} introduced a channel-wise attention with global average pooling (GAP), which is widely considered in recent SISR works~\cite{rcan, imdn_hui2019, san_dai2019}. Non-local attention~\cite{nonlocal2018wang} proposed by Wang~\textit{et al.} and its derivatives also address amazing performance in image restoration~\cite{san_dai2019}. However, matrix multiplication in non-local attentions makes them hard for flexible applications. GAP-based methods only considers the channel-wise correlations without spatial attentions. Furthermore, existing attention methods mainly focus on the weighting factors for importance, but almost neglect the bias of different feature values.

This paper proposes a progressive multi-scale residual network for SISR problem, which is termed as PMRN. Motivated by the calculation of convolution, we substitute the larger filters with combinations of several layers with small kernel size for efficiency, and devise a progressive multi-scale block (PMRB) for hierarchical feature exploration. PMRB processes the multi-scale features in a sequential manner, which aims to make full use of the relations among information from different scales. Furthermore, we design a channel- and pixel-wise attention mechanism (CPA) to focus more on high-frequency and important information, which considers the weighting and bias factors jointly for different channels and pixels from the feature maps. With the elaborate designs, PMRN achieves competitive or better restoration performance than other works with much fewer parameters and lower computation complexity. Fig.~\ref{fig:slogan} demonstrates an example of qualitative comparison among different works. From the results, PMRN can restore more accurate textures than others with higher PSNR/SSIM values.

Our contributions can be concluded as follows:
\begin{itemize}
	\item
	Motivated by the calculation of convolution, we design a progressive multi-scale residual block (PMRB) to sequentially explore the hierarchical information with small filter combinations, which considers the relations among multi-scale features.
	\item
	We devise a channel- and pixel-wise attention (CPA) mechanism to focus more on high-frequency information, which considers the spatial features with weighting and bias factors.
	\item
	We build the progressive multi-scale network (PMRN) for SISR with elaborate components. Experimental results show PMRN achieves competitive or better restoration performances with much fewer parameters and lower computation complexity.
\end{itemize}



\begin{figure}
	\captionsetup[subfloat]{labelformat=empty, justification=centering}
	\begin{center}
		\newcommand{\rowArg}{1.68cm}
		\newcommand{\fullSize}{4.55cm}
		\newcommand{\fullWidth}{5.7cm}
		\newcommand{\patchSize}{1.85cm}
		\scriptsize
		\setlength\tabcolsep{0.05cm}
		\begin{tabular}[b]{c c c c}
			\multicolumn{3}{c}{\multirow{2}{*}[\rowArg]{
					\subfloat[image\_024 from Urban100~\cite{urban100}]
					{\includegraphics[height=\fullSize, width=\fullWidth]
						{Figs/img_024/full_HR.png}}}} &
			\subfloat[HR~\protect\linebreak(PSNR/SSIM)]
			{\includegraphics[width = \patchSize, height = \patchSize]
				{Figs/img_024/part_HR.png}} \\
			& & & 
			\subfloat[Bicubic~\protect\linebreak(16.94/0.5539)]
			{\includegraphics[width = \patchSize, height = \patchSize]
				{Figs/img_024/part_bicubic.png}} \\ [-0.3cm]
			\subfloat[LapSRN~\cite{lapsrn} \protect\linebreak(18.10/0.6714)]
			{\includegraphics[width = \patchSize, height = \patchSize]
				{Figs/img_024/part_LapSRN.png}} &
			\subfloat[CARN~\cite{carn} \protect\linebreak(18.84/0.7132)]
			{\includegraphics[width = \patchSize, height = \patchSize]
				{Figs/img_024/part_CARN.png}} &
			\subfloat[MSRN~\cite{msrn} \protect\linebreak(18.81/0.7224)]
			{\includegraphics[width = \patchSize, height = \patchSize]
				{Figs/img_024/part_MSRN.png}} &
			\subfloat[PMRN\protect\linebreak(\textbf{19.09}/\textbf{0.7340})]
			{\includegraphics[width = \patchSize, height = \patchSize]
				{Figs/img_024/part_PMRN.png}}
		\end{tabular}
	\end{center}
	\setlength{\abovecaptionskip}{0pt plus 2pt minus 2pt}
	\setlength{\belowcaptionskip}{0pt plus 2pt minus 2pt}
	\vspace{-0.2cm}
	\caption{Visual quality comparisons for various image SR methods with scaling factor .}
	\label{fig:slogan}
\end{figure}


\begin{figure*}
	\centering
	\includegraphics[width=0.9\linewidth]{Figs/pmrn.pdf}
	\caption{Illustration of proposed PMRN. There are three modules in PMRN sequentially restore the resolution from corresponding LR images. In PMRB, there are layer combinations for feature exploration with different scales. CPA block is utilized for joint channel-wise and pixel-wise attentions.}
	\label{fig:network-design}
\end{figure*}
	
\section{Related Works}
\subsection{Deep Learning for SISR} 
SISR has proved to be a challenging issue in image restoration area. Besides the complex degradation situations, different image acquisition circumstances also influence the restoration quality, such as night vision~\cite{review1_1}, inpainting~\cite{review1_2}, and moving blur~\cite{review1_3}. From this point of view, SISR is highly ill-posed with a large information loss.

CNN has shown its amazing performance on SISR with superior restoration capacity. SRCNN~\cite{srcnn} is the first CNN-based method for image SR with a three-layer network, which denotes a sparse coding like structure. After SRCNN, FSRCNN~\cite{fsrcnn}, VDSR~\cite{vdsr}, and DRCN~\cite{drcn_ghifary2016deep} increased the network depth for better restoration performance. Besides building a deeper network, there are well-designed architectures with good restoration performances.  CNF~\cite{cnf_ren2017image} proposed by Ren~\textit{et al.} introduced context-wise fusion for image ensemble. Inspired by Laplacian pyramid, Lai~\textit{et al.} investigated LapSRN~\cite{lapsrn} and MS-LapSRN~\cite{ms-lapsrn} for progressive restoration. Fan~\textit{et al.} utilized BTSRN~\cite{btsrn_fan2017balanced} to balance the LR and HR stages for SISR. DRRN~\cite{drrn_tai2017image} proposed by Tai~\textit{et al.} considered a recursive network for restoration. MemNet~\cite{memnet}, proposed by Tai~\textit{et al.}, also achieved good performances on several low-level tasks. Recently, elaborate components are proposed for better feature exploration. EDSR~\cite{edsr_lim2017} removed the batch normalization and introduced residual blocks for SISR problem. SRDenseNet~\cite{srdensenet} proposed by Tong~\textit{et al.} utilized dense connection for better gradient transmission. Zhang~\textit{et al.} embedded residual and dense connection in RDN~\cite{rdn_zhang2018}. CARN~\cite{carn} proposed by Ahn~\textit{et al.} considered a cascading block for restoration. 
Wavelet~\cite{review2_1}, UNet~\cite{review2_7}, and optimization methods\cite{review2_2} are also considered for better network architecture designs.
In general, a deeper or wider network with higher computation complexity and more parameters can recover the textures more effectively.

Among these CNN-based networks, multi-scale architecture has proved to be an effective design for image restoration. LapSRN~\cite{lapsrn, ms-lapsrn} performs the image SR from different scales jointly. Li~\textit{et al.} investigated a multi-scale block in MSRN~\cite{msrn} for feature exploration. Furthermore, MDCN~\cite{mdcn} proposed by Li~\textit{et al.} jointly considered residual learning, dense connection, and multi-scale features for SISR. MGHCNet~\cite{mghcnet}, proposed by Esmaeilzehi~\textit{et al.} also addressed amazing restoration capacity with a multi-scale granular and holistic channel feature generation network. These works concentrate on effective feature exploration, which almost neglect the diversity of spatial information.

\subsection{Attention Mechanism}
High-frequency information and details act as a critical role for image restoration~\cite{review2_6}. Attention mechanism has proved to be a success component for computer vision issues, which concentrates more on the important information from features. SENet~\cite{senet_hu2018} proposed by Hu~\textit{et al.} is one of the most famous attentions with GAP. IMDN~\cite{imdn_hui2019}, RCAN~\cite{rcan}, SAN~\cite{san_dai2019} and other recent works utilize SENet or its derivations and achieve state-of-the-art performances. Non-local attention~\cite{nonlocal2018wang} is another impressive design for global correlation consideration, which has been applied in recent SR works, such as SAN~\cite{san_dai2019}, CS-NL~\cite{csnl}, and PFNL~\cite{pfnl}. However, non-local attention requires matrix multiplication to consider the spatial correlations, which require large memory cost and more parameters.


\section{Methodology}
\subsection{Network Structure}
As shown in Fig.~\ref{fig:network-design}, there are three modules in PMRN: feature extraction, non-linear feature exploration and restoration. 
Let's denote ,  as the input LR instances and restored HR outputs separately. Features from LR images will be extracted as,

where  denotes the feature extraction module, and  denotes the features.

After feature extraction, non-linear feature exploration builds the mapping from LR to HR space, which is composed of several PMRBs and a padding structure. Suppose there are  PMRBs, for the -th block, there is,

where  denotes the PMRB, and  denotes the output feature. After PMRBs, feature will pass the padding structure with residual learning, as,

where  denotes the padding structure.

Finally, HR images will be restored from features, which can be demonstrated as,

where  denotes the restoration module.
	
\subsection{Progressive Multi-scale Residual Block}
Fig.~\ref{fig:network-design} demonstrates the design of PMRB, which can be separated into three steps. First, the progressive multi-scale processing (PMP) step exploits the hierarchical features in a sequential manner. After PMP, the multi-scale features are aggregated in the multi-scale feature fusion (MFF) step. Finally, the local residual learning (LRL) step introduces the shortcut to preserve the information and accelerate the gradient transmission.

\textbf{Progressive multi-scale processing} step aims to sequentially exploit the hierarchical features.
Let's denote ,  as the combination and features for scale  separately, then there is,


\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{Figs/pmrb2.pdf}
	\caption{Illustration of different layer combination design. The combinations are defined in a recursive way for larger scales.}
	\label{fig:pmrb}
\end{figure}

Fig.~\ref{fig:pmrb} shows the designs of different combinations, which are defined in a recursive fashion. For scaling factor , there is one convolutional layer for feature extraction. For other scales, the combinations are composed of an identical structure of previous scale combination and a convolutional layer with ReLU activation. With the accumulation of small convolutional layers, the combinations hold different larger receptive fields. The formulation of  can be described as,

where  and  denote the convolution and ReLU activation respectively.

From Eqn.~\ref{eqn:PMP}, there is no explicit residual when . On one hand, the identical information will be delivered by the local residual learning step in PMRB. On the other hand, there is no activation in , and the identical addition will be implied by the convolution operation.



\textbf{Multi-scale feature fusion} step concatenates and fuses the multi-scale features, which contains one point-wise convolution and a CPA block.
The operation can be demonstrated as,

where  denotes the MFF step, and  is the output feature.

\textbf{Local residual learning} is devised to preserve the information and improve the gradient flow. 
Finally, the output of PMRB is,



\subsection{Channel-wise and Pixel-wise Attention}
\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{Figs/fcnorm.pdf}
	\caption{Illustration of proposed CPA. Scale factor  and bias factor  are adaptively learned from the attention mechanism. In CPA, point-wise~(P-Conv) and depth-wise~(D-Conv) convolutional layers exploit the channel-wise and pixel-wise relations separately.}
	\label{fig:FCNorm}
\end{figure}

As shown in Fig.~\ref{fig:FCNorm}, there are three parts in CPA. Firstly, space transformation~(ST) step converts the input features into a specific space for attention exploration. After ST, factor extraction~(FE) step exploits the weighting and bias factors jointly from two parallel paths, which considers channel-wise and pixel-wise features separately. Finally, attention allocation~(AA) step distributes the learned adaptive attentions onto the features.

\textbf{Space transformation} step transforms the input feature into a specific space with one convolution. The operation of ST can be demonstrated as,

where  denotes the features after transformation, and  is the input features.

\textbf{Factor extraction} step exploits the scale and bias factors after ST. In FE step, channel-wise and pixel-wise attentions are jointly considered.
Channel-wise attentions are firstly explored by one point-wise convolutional layer~(P-Conv), then the pixel-wise attentions are considered by one depth-wise convolutional layer~(D-Conv). The two layers process attentions from different perspectives orthogonally. One ReLU activation is utilized between the two convolutional layers for non-linearity. The operations of FE module can be demonstrated as,


where  denotes the extraction layers, and  denotes the sigmoid activation. ,  are the bias and scale factors separately. Sigmoid activation after  introduces the non-negativity of learned scales.

\textbf{Attention allocation} step allocates the attentions to features via learned scale and bias factors. The output of AA step is,

From Eq.(\ref{eqn:fcnorm}), there is a residual structure in CPA.  contains the self-adaptive scale factors and an identical addition of input features, which is utilized to preserve the information and improve the gradient transmission.

	
\subsection{Discussion}
\paragraph{Difference to MSRN~\cite{msrn}}
MSRN introduced a multi-scale block termed as MSRB with  and  convolutional layers. In MSRB, features from two kinds of convoluitonal layers are crossly concatenated and explored, and an  convolutional layer is utilized to fuse the multi-scale features. 
Different from MSRB, there are features from four different scales extracted by PMRB, and concatenated with one convolutional layer for fusion. Features from different scales are explored sequentially, and residual connections are utilized for information preservation and better gradient flow. 
Multi-scale information is extracted by layers with different kernel sizes in MSRB, while PMRB designs the multi-scale structure in a recursive way, which decreases the parameters and computation complexity. 
Besides multi-scale design, a novel attention mechanism CPA is designed in PMRB. Features from different MSRBs are collected and concatenated with an convolutional layer for global feature fusion. Different from the global feature fusion, blocks in PMRN are stacked with global residual learning. With the elaborated design, PMRN achieves better PSNR/SSIM results on all testing benchmarks than MSRN with fewer parameters and lower computation complexity.

\paragraph{Difference to Channel-wise Attention~\cite{senet_hu2018}}
There is an effective channel-wise attention design in SENet, which has been widely utilized for different image restoration problems. In channel-wise attentions, information from different channels is evaluated by global average pooling. Two full connection layers with a ReLU activation are designed to explore the attentions, and a Sigmoid activation is introduced for non-negativity.
In PMRN, CPA is devised for joint channel-wise and pixel-wise attentions. Different from channel-wise attentions, features are extracted and explored by convolutional layers, which concentrates more on complex textures and information. Squeezing step in SENet shrinks the channel number, which may cause information loss. In CPA, the numbers of filters are invariable for all convolutional layers. Besides weighting factors, bias factors are also explored in CPA to shift the features and find a better attention representation. Finally, a shortcut is designed in CPA to maintain the origin information.

\paragraph{Difference to LapSRN~\cite{lapsrn}}
LapSRN is a progressive network for image super-resolution. In LapSRN, the progressive structure is designed for images restorations with multiple resolutions by using one network. Residual maps are learned from the network sequentially with the increase of resolutions. In PMRN, an end-to-end network is proposed for image super-resolution with a specific scaling factor. The progressive structure is mainly designed in PMRB to extract the multi-scale features. Information from multi-scale features is sequentially extracted with different layer combinations and fused with one convolutional layer.
 
 
 
 
 
 
 
 
 
 
 
 
 
\section{Experiments}
In PMRN, all convolutional layers are with kernel size as  expect for MFF step in PMRB, which is designed with . The filter number of convolutional layers is set as . There are  PMRBs stacked in non-linear feature exploration module, and the padding structure is composed of two convolutional layers with a ReLU activation.

The proposed PMRN is trained with DIV2K~\cite{timofte2017ntire} dataset. DIV2K is a high-quality dataset with 2K resolution images from real world. There are 800 training images, 100 validation images and 100 test images in DIV2K dataset. In this paper, 800 images are chosen for training and 5 images for validation. For testing, five benchmarks widely used in image super-resolution works: Set5~\cite{set5}, Set14~\cite{set14}, B100~\cite{b100}, Urban100~\cite{urban100}, and Manga109~\cite{manga109} are chosen. The training images are randomly flipped and rotated for data augmentation. Patch size of LR image for training is set as . PMRN are trained for 1000 iterations with  loss, and the parameters are updated with an Adam~\cite{adam} optimizer. The learning rate of optimizer is chosen as , and halved for every 200 iterations. The degradation model is chosen as \textit{bicubic down}~(\textbf{BI}) with scaling factor , , and . PSNR and SSIM are chosen as the indicators for quantitive comparison with other works. Self-ensemble strategy is used to improve the performance, and the extension model is termed as as PMRN.




\subsection{Model Analysis}
\textbf{Analysis on Network Settings.}
In PMRN, the largest scale of PMRB is chosen as  and the number of PMRB is chosen as . To show the effect of different  and , models are trained with different scales and block numbers for 200 epochs. Quantitative comparisons are made on B100 with scaling factor . The visualization results are shown in Fig.~\ref{Fig:vis-network-depth}. From Fig.~\ref{Fig:vis-network-depth}, both  and  will affect the network performance. In general, with the increase of  and , the networks will achieve better results. Compared with ,  counts more for the performance. On one hand, when  is larger, the network will be deeper. On the other hand, with the increase of , features from more scales will be considered.



\textbf{Analysis on PMRB.}
There is multi-scale structure in PMRB, extracting information from different scales. To show the performance of multi-scale design, comparisons are conducted without different combinations of convolutional layers. All combinations are replaced by only one  convolutional layer. In other words, all the scales in PMRB are identical to . The results are shown in Table.~\ref{tab:abl_ms} on four benchmarks with scaling factor .
From Table~\ref{tab:abl_ms}, model with multi-scale design achieves better PSNR/SSIM results than the other one. There are two reasons for the performance improvement. On one hand, the features of different scales will contain more information, which helps to recover the complex structural textures. On the other hand, the multi-scale structures are built in a recursive way. With the combination of convolutional layers, the depth of PMRN will be increased, which may be helpful to improve the network representation.

Furthermore, we analyze the exploited features from different scales, which are shown in Fig.~\ref{fig:feat}. The multi-scale features are exploited from different layer combinations. With the increasing of scale factors, the structural information will be sharper and more clear, and the tiny textures will be flat. This accords with the notion that multi-scale features contain different information.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{Figs/vis-network-depth.pdf}
	\caption{Investigation on different  and  with scaling factor .}
	\label{Fig:vis-network-depth}
\end{figure}

\begin{figure*}
	\captionsetup[subfloat]{labelformat=empty, justification=centering}
	\begin{center}
		\scriptsize
		\setlength\tabcolsep{0.1cm}
		\begin{tabular}[b]{cccccc}
			\subfloat[(a)]{\includegraphics[width=0.15\linewidth]{Figs/feat/input.png}}&
			\subfloat[(b)]{\includegraphics[width=0.15\linewidth]{Figs/feat/feat3.png}}&
			\subfloat[(c)]{\includegraphics[width=0.15\linewidth]{Figs/feat/feat5.png}}&
			\subfloat[(d)]{\includegraphics[width=0.15\linewidth]{Figs/feat/feat7.png}}&
			\subfloat[(e)]{\includegraphics[width=0.15\linewidth]{Figs/feat/feat9.png}}&
			\subfloat[(f)]{\includegraphics[width=0.15\linewidth]{Figs/feat/output.png}}
		\end{tabular}
	\end{center}
	\setlength{\abovecaptionskip}{0pt plus 2pt minus 2pt}
	\setlength{\belowcaptionskip}{0pt plus 2pt minus 2pt}
	\caption{Illustrations of multi-scale features. (a) and (f) denote the input and output features. (b)-(e) denote the features with scale factor 3, 5, 7, and 9.}
	\label{fig:feat}
\end{figure*}

\begin{table}
	\centering
	\caption{Investigation on multi-scale mechanism in PMRB with scaling factor  for different benchmarks.}
	\label{tab:abl_ms}
	\begin{tabular}{|c|c|c|c|c|}
		\hline  
		\textbf{Multi}& \textbf{Set5}& \textbf{Set14}& \textbf{B100} &\textbf{Urban100}\\
		\hline
		\hline
		\textbf{w}& 32.34/0.8971& 28.71/0.7850& 27.66/0.7392& 26.37/0.7953\\ 
		\textbf{w/o}& 32.03/0.8932& 28.51/0.7799& 27.53/0.7348& 25.90/0.7803\\
		\hline
	\end{tabular}
\end{table}

In PMRB, residual connections are introduced to preserve the information from small scales. Feature fusion with  convolution is also used to concatenate information from different scales. To show the performance of information preservation and feature fusion, we perform the comparisons without residual and  convolution. The results are shown in Table.~\ref{tab:abl_pmrb}, where \textbf{Res} and \textbf{Fuse} denote the residual connection and concatenation separately. Three benchmarks covering different kinds of textures are used for testing with scaling factor .
From the Table~\ref{tab:abl_pmrb}, residual and feature fusion are both efficient for different benchmarks. For Set5, residual structure performs better than fusion, achieving around 0.1db improvement. For B100 and Urban100, feature fusion can recover the texture more effectively. Set5 contains less high-frequency information than the other benchmarks, while B100 and Urban100 are composed of abundant images from real world. From this perspective, residual connection is suitable for simple images, while feature fusion performs better on complex structural textures.

\begin{table}
	\centering
	\caption{Investigation on different structures in PMRB with scaling factor  for different benchmarks.}
	\label{tab:abl_pmrb}
\begin{tabular}{|c|c|c|c|c|}
		\hline  
		\textbf{Res}& \textbf{Fuse}& \textbf{Set5}& \textbf{B100} &\textbf{Urban100} \\
		\hline
		\hline
		\cmark&\cmark& 32.34/0.8971& 27.66/0.7392& 26.37/0.7953\\ 
		\cmark&\xmark& 32.35/0.8971& 27.64/0.7384& 26.34/0.7942\\
		\xmark&\cmark& 32.24/0.8963& 27.65/0.7388& 26.36/0.7955\\
		\hline
	\end{tabular}
\end{table}

\textbf{Analysis on Combination Substitution.}
In PMRN, recursive layer combinations are proposed to substitute convolutional layers with different kernel sizes. To show the performance of substitution, PSNR/SSIM comparisons are made on five benchmarks with scaling factor . For ensuring the same receptive field, network without combinations is built with layers holding the kernel sizes as ,  and  separately. The results are shown in Table~\ref{tab:abl_3x3}.
From Table~\ref{tab:abl_3x3}, model built with layer combinations achieves better PSNR/SSIM results on all five testing benchmarks, showing the performance of recursive design. Meanwhile, there are around 40.2\% off on parameters and MACs when utilizing recursive combinations. 

\begin{table*}
	\centering
	\caption{Investigation on recursive combination in PMRB with scaling factor  on different benchmarks.}
	\label{tab:abl_3x3}
	\begin{tabular}{|c|c|c|c|c|c|c|c|}
		\hline  
		\textbf{Comb}&\textbf{Param}& \textbf{MACs}& \textbf{Set5}& \textbf{Set14}& \textbf{B100} &\textbf{Urban100}& \textbf{Manga109}\\
		\hline
		\hline
		\textbf{w}& 3,598K& 207.2G&32.34/0.8971& 28.71/0.7850& 27.66/0.7392& 26.37/0.7953& 30.71/0.9107\\ 
		\textbf{w/o}& 6,020K& 346.7G&32.07/0.8932& 28.53/0.7804& 27.53/0.7350& 25.93/0.7819& 30.16/0.9043\\
		\hline
	\end{tabular}
\end{table*}




\textbf{Analysis on Attentions Mechanism.}
In PMRN, CPA is investigated for joint attention mechanism. To show the performance of proposed CPA, comparisons are designed on three testing benchmarks. We compare the models with CPA, channel-wise attention~(CA)~\cite{senet_hu2018}, and no attentions. The results are shown in Table~\ref{tab:abl_norm}.
From the table, the model with CPA achieves the best performance on all testing benchmarks. The model with channel-wise attentions achieves better PSNR/SSIM results than that without attentions. The results demonstrate that attention mechanism is efficient for image super-resolution.

\begin{table}
	\centering
	\caption{Investigation on different normalization methods with scaling factor  for different benchmarks.}
	\label{tab:abl_norm}
	\begin{tabular}{|c|c|c|c|}
		\hline  
		\textbf{Method}& \textbf{Set5}& \textbf{Set14}& \textbf{Urban100}\\
		\hline
		\hline
		\textbf{CPA}	& 32.34/0.8971& 28.71/0.7850& 26.37/0.7953\\ 
		\textbf{CA~\cite{senet_hu2018}}	& 32.31/0.8968& 28.69/0.7844& 26.34/0.7940\\
		\textbf{w/o}		& 32.29/0.8965& 28.68/0.7851& 26.29/0.7940\\
		\hline
	\end{tabular}
\end{table}

\begin{table*}[!ht]
	\centering
	\caption{Average PSNR/SSIM, parameters and MACs results with degradation model \textbf{BI} , , and  on five benchmarks. The best and second performances are shown in \textbf{bold} and \underline{underline}.}
	\label{tab:BI-result}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
		\hline
		\multirow{2}{*}{Scale}& \multirow{2}{*}{Model}&  \multirow{2}{*}{Params}& \multirow{2}{*}{MACs}& Set5~\cite{set5}& Set14~\cite{set14}& B100~\cite{b100}& Urban100~\cite{urban100}& Manga109~\cite{manga109} \\
		& & & & PSNR/SSIM & PSNR/SSIM & PSNR/SSIM & PSNR/SSIM & PSNR/SSIM\\
\hline
		\hline
		\multirow{17}{*}{} &SRCNN~\cite{srcnn}&57K &52.7G & 
		36.66/0.9542& 32.42/0.9063& 31.36/0.8879& 29.50/0.8946& 35.74/0.9661\\
		
		& FSRCNN~\cite{fsrcnn}&12K&6.0G& 
		37.00/0.9558& 32.63/0.9088& 31.53/0.8920& 29.88/0.9020& 36.67/0.9694\\
		
		& VDSR~\cite{vdsr}&665K&612.6G& 
		37.53/0.9587& 33.03/0.9124& 31.90/0.8960& 30.76/0.9140& 37.22/0.9729\\
		
		& DRCN~\cite{drcn_ghifary2016deep}&1,774K&17,974.3G& 
		37.63/0.9588& 33.04/0.9118& 31.85/0.8942& 30.75/0.9133& 37.63/0.9723\\
		
		& CNF~\cite{cnf_ren2017image}&337K&311.0G& 
		37.66/0.9590& 33.38/0.9136& 31.91/0.8962& - & - \\
		
		&LapSRN~\cite{lapsrn}&813K&29.9G& 
		37.52/0.9590& 33.08/0.9130& 31.80/0.8950& 30.41/0.9100& 37.27/0.9740\\
		
		&DRRN~\cite{drrn_tai2017image}&297K&6,796.9G& 
		37.74/0.9591& 33.23/0.9136& 32.05/0.8973& 31.23/0.9188& 37.92/0.9760\\
		
		&BTSRN~\cite{btsrn_fan2017balanced}&410K&207.7G& 
		37.75/-& 33.20/-& 32.05/-& 31.63/-& -\\
		
		&MemNet~\cite{memnet}&677K&2,662.4G& 
		37.78/0.9597& 33.28/0.9142& 32.08/0.8978& 31.31/0.9195& 37.72/0.9740 \\
		
		&SelNet~\cite{selnet_choi2017deep}&974K&225.7G& 
		37.89/0.9598& 33.61/0.9160& 32.08/0.8984& - &  - \\
		
		&CARN~\cite{carn}&1,592K&222.8G& 
		37.76/0.9590& 33.52/0.9166& 32.09/0.8978& 31.92/0.9256& 38.36/0.9765\\
		
		&MSRN~\cite{msrn}& 5,930K& 1367.5G& 
		38.08/0.9607& 33.70/0.9186& 32.23/0.9002& 32.29/0.9303& \underline{38.69/0.9772} \\
		
		&OISR-RK2~\cite{oisr}& 4,970K& 1145.7G&
		\underline{38.12/0.9609}& \underline{33.80/0.9193}& \underline{32.26/0.9006}& \underline{32.48/0.9317}& - \\
		
		&PMRN& 3,577K& 824.2G& 
		\textbf{38.13/0.9609}& \textbf{33.85/0.9204}& \textbf{32.28/0.9010}& \textbf{32.59/0.9328}& \textbf{38.91/0.9775} \\
		
		\cline{2-9}
		
		&EDSR~\cite{edsr_lim2017}&40,729K& 9,388.8G& 
		38.11/0.9602& \textbf{33.92}/0.9195& \underline{32.32/0.9013}& \textbf{32.93/0.9351}& \underline{39.10/0.9773}\\
		
		&D-DBPN~\cite{dbpn_haris2018deep}& 5,953K& 3,746.2G& 
		38.09/0.9600& 33.85/0.9190& 32.27/0.9000& 32.55/0.9324& 38.89/0.9775\\
		
		&SRFBN~\cite{srfbn}& 2,140K& 5,043.5G& 
		\underline{38.11/0.9609}& 33.82/\underline{0.9196}& 32.29/0.9010& 32.62/0.9328& 39.08/0.9779\\
		


		&PMRN& 3,577K& 6,593.6G&\textbf{38.22/0.9612}& \underline{33.90}/\textbf{0.9205}& \textbf{32.34/0.9015}& \underline{32.78/0.9342}& \textbf{39.15/0.9781} \\
		\hline
\hline
		\multirow{16}{*}{}& SRCNN~\cite{srcnn} &57K &52.7G & 
		32.75/0.9090& 29.28/0.8209& 28.41/0.7863& 26.24/0.7989& 30.59/0.9107\\
		
		&FSRCNN~\cite{fsrcnn}&12K&5.0G& 
		33.16/0.9140& 29.43/0.8242& 28.53/0.7910& 26.43/0.8080& 30.98/0.9212\\
		
		&VDSR~\cite{vdsr}&665K&612.6G& 
		33.66/0.9213& 29.77/0.8314& 28.82/0.7976& 27.14/0.8279& 32.01/0.9310\\
		
		&DRCN~\cite{drcn_ghifary2016deep}&1,774K&17,974.3G& 
		33.82/0.9226& 29.76/0.8311& 28.80/0.7963& 27.15/0.8276& 32.31/0.9328\\
		
		&CNF~\cite{cnf_ren2017image}&337K&311.0G& 
		33.74/0.9226& 29.90/0.8322& 28.82/0.7980& - & - \\
		
		&DRRN~\cite{drrn_tai2017image}&297K&6,796.9G& 
		34.03/0.9244& 29.96/0.8349& 28.95/0.8004& 27.53/0.8378& 32.74/0.9390\\
		
		&BTSRN~\cite{btsrn_fan2017balanced}&410K&176.2G& 
		34.03/-& 29.90/-& 28.97/-& 27.75/-& -\\
		
		&MemNet~\cite{memnet}&677K&2,662.4G& 
		34.09/0.9248& 30.00/0.8350& 28.96/0.8001& 27.56/0.8376& 32.51/0.9369 \\
		
		&SelNet~\cite{selnet_choi2017deep}&1,159K&120.0G& 
		34.27/0.9257& 30.30/0.8399& 28.97/0.8025& - &  - \\
		
		&CARN~\cite{carn}&1,592K&118.8G& 
		34.29/0.9255& 30.29/0.8407& 29.06/0.8034& 28.06/0.8493& 33.49/0.9440\\
		
		&MSRN~\cite{msrn}& 6,114K& 626.6G& 
		34.46/0.9278& 30.41/0.8437& 29.15/0.8064& 28.33/0.8561& \underline{33.67/0.9456} \\
		
		&OISR-RK2~\cite{oisr}& 5,640K& 578.6G&
		\underline{34.55/0.9282}& \textbf{30.46}/0.8443& \underline{29.18/0.8075}& \underline{28.50/0.8597}& -\\
		
		&PMRN&3,586K& 366.6G& 
		\textbf{34.57/0.9284}& \underline{30.43}/\textbf{0.8444}& \textbf{29.19/0.8075}& \textbf{28.51/0.8601}& \textbf{33.85/0.9465}  \\
		
		\cline{2-9}
		
		&EDSR~\cite{edsr_lim2017}&43,680K& 4,471.5G& 34.65/0.9280& \underline{30.52}/\textbf{0.8462}& \textbf{29.25/0.8093}& \textbf{28.80/0.8653}& \underline{34.17/0.9476}\\
		
		&SRFBN~\cite{srfbn}&2,832K& 6,023.8G& \textbf{34.70/0.9292}& 30.51/0.8461& 29.24/0.8084& \underline{28.73/0.8641}& \textbf{34.18/0.9481}\\
		


		&PMRN&3,586K& 2,932.8G&\underline{34.65/0.9289}& \textbf{30.54}/\underline{0.8461}& \underline{29.24/0.8087}& 28.71/0.8630& 34.10/0.9480 \\
\hline
		\hline
		\multirow{18}{*}{}&SRCNN~\cite{srcnn}&57K &52.7G & 
		30.48/0.8628& 27.49/0.7503& 26.90/0.7101& 24.52/0.7221& 27.66/0.8505\\
		
		&FSRCNN~\cite{fsrcnn}&12K&4.6G& 
		30.71/0.8657& 27.59/0.7535& 26.98/0.7150& 24.62/0.7280& 27.90/0.8517\\
		
		&VDSR~\cite{vdsr}&665K&612.6G& 
		31.35/0.8838& 28.01/0.7674& 27.29/0.7251& 25.18/0.7524& 28.83/0.8809\\
		
		&DRCN~\cite{drcn_ghifary2016deep}&1,774K&17,974.3G& 
		31.53/0.8854& 28.02/0.7670& 27.23/0.7233& 25.14/0.7510& 28.98/0.8816\\
		
		&CNF~\cite{cnf_ren2017image}&337K&311.0G& 
		31.55/0.8856& 28.15/0.7680& 27.32/0.7253& - & - \\
		
		&LapSRN~\cite{lapsrn}&813K&149.4G& 
		31.54/0.8850& 28.19/0.7720& 27.32/0.7280& 25.21/0.7560& 29.09/0.8845\\
		
		&DRRN~\cite{drrn_tai2017image}&297K&6,796.9G& 
		31.68/0.8888& 28.21/0.7720& 27.38/0.7284& 25.44/0.7638& 29.46/0.8960\\
		
		&BTSRN~\cite{btsrn_fan2017balanced}&410K&207.7G& 
		31.85/-& 28.20/-& 27.47/-& 25.74/-& -\\
		
		&MemNet~\cite{memnet}&677K&2,662.4G& 
		31.74/0.8893& 28.26/0.7723& 27.40/0.7281& 25.50/0.7630& 29.42/0.8942 \\
		
		&SelNet~\cite{selnet_choi2017deep}&1,417K&83.1G& 
		32.00/0.8931& 28.49/0.7783& 27.44/0.7325& - &  - \\
		
		&SRDenseNet~\cite{srdensenet}&2,015K&389.9G&
		32.02/0.8934& 28.50/0.7782& 27.53/0.7337& 26.05/0.7819& - \\
		
		&CARN~\cite{carn}&1,592K&90.9G& 
		32.13/0.8937& 28.60/0.7806& 27.58/0.7349& 26.07/0.7837& 30.40/0.9082\\
		
		&MSRN~\cite{msrn}&6,373K& 368.6G& 
		32.26/0.8960& 28.63/0.7836& 27.61/0.7380& 26.22/0.7911& \underline{30.57/0.9103} \\
		
		&OISR-RK2~\cite{oisr}&5,500K&412.2G&
		\underline{32.32/0.8965}& \textbf{28.72}/\underline{0.7843}& \underline{27.66/0.7390}& \underline{26.37/0.7953}& - \\
		
		&PMRN&3,598K& 207.2G& 
		\textbf{32.34/0.8971}& \underline{28.71}/\textbf{0.7850}& \textbf{27.66/0.7392}& \textbf{26.37/0.7953}& \textbf{30.71/0.9107}  \\
		
		\cline{2-9}
		
		&EDSR~\cite{edsr_lim2017}&43,089K& 2,895.8G& 32.46/0.8968& 28.80/\textbf{0.7876}& 27.71/\textbf{0.7420}& \textbf{26.64/0.8033}& 31.02/\underline{0.9148}\\
		
		&D-DBPN~\cite{dbpn_haris2018deep}&10,426K& 5,213.0G& 32.47/0.8980& \textbf{28.82}/0.7860& 27.72/0.7400& 26.38/0.7946&30.91/0.9137\\
		
		&SRFBN~\cite{srfbn}&3,631K& 7,466.1G& \underline{32.47/0.8983}& 28.81/0.7868& \textbf{27.72}/\underline{0.7409}& \underline{26.60/0.8015}& \textbf{31.15/0.9160}\\
		


		&PMRN&3,598K& 1,657.6G&\textbf{32.47/0.8984}& \underline{28.81/0.7870}& \underline{27.72}/0.7405& 26.55/0.7995& \underline{31.07}/0.9144 \\
		\hline
		
	\end{tabular}
\end{table*}

To analyze the operation of CPA, attention factors ,  and the feature maps before and after attention are visualized in Fig.~\ref{fig:vis-attention}. From the illustrations, learned attentions are more concentrated on structural textures.  and  vary sharply on the area of edges and complex textures. After attentions, the features are more discriminative on structural textures, which is a convincing evidence of that the attention mechanism concentrates more on the important high-frequency information

\begin{figure}
	\captionsetup[subfloat]{labelformat=empty, justification=centering}
	\begin{center}
\begin{tabular}[b]{cc}
			\subfloat[(a)~Feature before attention]{\includegraphics[width=0.45\linewidth]{Figs/attention_x.pdf}}&
			\subfloat[(b)~Feature after attention]{\includegraphics[width=0.45\linewidth]{Figs/attention_y.pdf}}\\
			\subfloat[(c)~ from CPA]{\includegraphics[width=0.45\linewidth]{Figs/attention_gamma.pdf}}&
			\subfloat[(d)~ from CPA]{\includegraphics[width=0.45\linewidth]{Figs/attention_beta.pdf}}\\
		\end{tabular}
	\end{center}
\caption{Visualization attention factors and feature maps about CPA.}
	\label{fig:vis-attention}
\end{figure}





\subsection{Comparison with State-of-the-Arts}
To make quantitive comparison, we compare the PSNR/SSIM results with several small works: bicubic, SRCNN~\cite{srcnn} FSRCNN~\cite{fsrcnn}, VDSR~\cite{vdsr}, DRCN~\cite{drcn_ghifary2016deep}, CNF~\cite{cnf_ren2017image}, LapSRN~\cite{lapsrn, ms-lapsrn}, DRRN~\cite{drrn_tai2017image}, BTSRN~\cite{btsrn_fan2017balanced}, MemNet~\cite{memnet}, SelNet~\cite{selnet_choi2017deep}, CARN~\cite{carn}, MSRN~\cite{msrn}, and OISR~\cite{oisr}. For a fair comparison, extension model PMRN is compared with large networks: EDSR~\cite{edsr_lim2017}, D-DBPN~\cite{dbpn_haris2018deep}, and SRFBN~\cite{srfbn}. 

Table~\ref{tab:BI-result} shows the PSNR/SSIM comparisons among several methods. From the results, PMRN achieves competitive or better performance than other small works on all five benchmarks. Compared with MSRN, PMRN gains 0.3dB increase on Urban100 with  degradation. Notice that PMRN achieves the best performances on B100, Urban100, and Manga109 with all degradation models. The three benchmarks contain plentiful structural information and edges, which consist of comic covers and real world photos. From this point of view, PMRN can recover the high-frequency information more effectively than others.

Meanwhile, we compare the computation complexity and parameters with other works. The total number of parameters is calculated as,

where ,  denote the input and output number of filters in -th convolutional layer,  and  denote the width and height of the kernel size,  denotes the number of groups, and  represents as the bias. 

Computation complexity is modeled as the number of multiply-accumulate operations~(MACs). Since it is a implementation independent factor, MACs can purely describe the computation complexity from the mathematical perspective. Comparisons of MACs are conducted by producing a 720P~() resolution image from corresponding LR image with different scaling factors. 

From the results, PMRN achieves competitive or better PSNR/SSIM results than others with fewer parameters and MACs, which proves to be the efficient design. Compared with OISR, PMRN holds near half MACs and parameters with competitive or better PSNR/SSIM performances with  degradation. Compared with larger networks, PMRN achieves competitive performances with much fewer MACs and parameters. Specially, PMRN holds near two thirds of the MACs and one tenth of the parameters than EDSR with  degradation, and achieves superior PSNR results on Set5, Set14, B100, and Manga109 datasets.

Visualization comparisons on parameters and MACs are shown in Fig.~\ref{fig:vis_param} and Fig.~\ref{fig:vis_flops}. A running time comparison is investigated in Fig.~\ref{fig:time}. The time cost and performance are evaluated on Manga109 with \textbf{BI} degradation.

\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{Figs/vis_param.pdf}
	\caption{An illustration comparison of performance and parameters.}
	\label{fig:vis_param}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{Figs/vis_gflop.pdf}
	\caption{An illustration comparison of performance and MACs.}
	\label{fig:vis_flops}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{Figs/pmrn_time.png}
    \caption{An illustration comparison of performance and running time.}
    \label{fig:time}
\end{figure}



	
	
	
	
	
Besides quantitative comparisons, we also analyze the qualitative restoration performance via visualization comparisons. Three images from Urban100 benchmark are chosen for comparison with \textbf{BI}~ degradation, which is shown in Fig.~\ref{fig:vis-result-bi}. These images are from real world with abundant high-frequency textures and competitive for restoration with large scaling factors. From the result, PMRN can recover the structural information effectively, and find more accurate textures than other works.

Besides Urban100, we also conduct the experiments on Manga109, which is composed of comic book covers with plentiful line structures. The result is shown in Fig.~\ref{fig:manga109}. From the visualization comparison, PMRN recovers more lines and structural textures.

Since Manga109 is a normal textured benchmark, we also compare the methods in the very textured situation, which is shown in Fig.~\ref{fig:set14}. The feather contains plentiful small lines and textures which are hard for recovery. From the comparison, PMRN can restore the textured image more accurately than MSRN.

\begin{figure*}
	\captionsetup[subfloat]{labelformat=empty, justification=centering}
	\begin{center}
		\newcommand{\rowArg}{1.5cm}
		\newcommand{\fullSizeWid}{7cm}
		\newcommand{\fullSizeHei}{4.2cm}
		\newcommand{\patchSize}{1.7cm}
		\scriptsize
		\setlength\tabcolsep{0.1cm}
		\begin{tabular}[b]{ccccc}
			\multirow{2}{*}[\rowArg]{
				\subfloat[image\_059 from Urban100]
				{\includegraphics[width = \fullSizeWid, height = \fullSizeHei]
					{Figs/img_059/full_HR.png}}} &
			\subfloat[HR~\protect\linebreak(PSNR/SSIM)]
			{\includegraphics[width = \patchSize, height = \patchSize]
				{Figs/img_059/part_HR.png}} &
			\subfloat[LR~\protect\linebreak(18.96/0.7246)]
			{\includegraphics[width = \patchSize, height = \patchSize]
				{Figs/img_059/part_nearest.png}} &
			\subfloat[Bicubic~\protect\linebreak(19.21/0.7331)]
			{\includegraphics[width = \patchSize, height = \patchSize]
				{Figs/img_059/part_bicubic.png}} &
			\subfloat[VDSR~\cite{vdsr}~\protect\linebreak(19.94/0.7910)]
			{\includegraphics[width = \patchSize, height = \patchSize]
				{Figs/img_059/part_VDSR.png}} \\ [-0.3cm]&
			\subfloat[LapSRN~\cite{lapsrn}~\protect\linebreak(19.92/0.7894)]
			{\includegraphics[width = \patchSize, height = \patchSize]
				{Figs/img_059/part_LapSRN.png}} &
			\subfloat[CARN~\cite{carn}~\protect\linebreak(20.82/0.8234)]
			{\includegraphics[width = \patchSize, height = \patchSize]
				{Figs/img_059/part_CARN.png}} &                   
			\subfloat[MSRN~\cite{msrn}~\protect\linebreak(21.11/0.8369)]
			{\includegraphics[width = \patchSize, height = \patchSize]
				{Figs/img_059/part_MSRN.png}} &
			\subfloat[Ours~\protect\linebreak(\textbf{21.44/0.8447})]
			{\includegraphics[width = \patchSize, height = \patchSize]
				{Figs/img_059/part_PMRN.png}}
		\end{tabular}
		
		\begin{tabular}[b]{ccccc}
			\multirow{2}{*}[\rowArg]{
				\subfloat[image\_067 from Urban100]
				{\includegraphics[width = \fullSizeWid, height = \fullSizeHei]
					{Figs/img_067/full_HR.png}}} &
			\subfloat[HR~\protect\linebreak(PSNR/SSIM)]
			{\includegraphics[width = \patchSize, height = \patchSize]
				{Figs/img_067/part_HR.png}} &
			\subfloat[LR~\protect\linebreak(14.95/0.7116)]
			{\includegraphics[width = \patchSize, height = \patchSize]
				{Figs/img_067/part_nearest.png}} &
			\subfloat[Bicubic~\protect\linebreak(15.80/0.7490)]
			{\includegraphics[width = \patchSize, height = \patchSize]
				{Figs/img_067/part_bicubic.png}} &
			\subfloat[VDSR~\cite{vdsr}~\protect\linebreak(17.30/0.8474)]
			{\includegraphics[width = \patchSize, height = \patchSize]
				{Figs/img_067/part_VDSR.png}} \\ [-0.3cm]&
			\subfloat[LapSRN~\cite{lapsrn}~\protect\linebreak(17.34/0.8577)]
			{\includegraphics[width = \patchSize, height = \patchSize]
				{Figs/img_067/part_LapSRN.png}} &
			\subfloat[CARN~\cite{carn}~\protect\linebreak(18.12/0.8882)]
			{\includegraphics[width = \patchSize, height = \patchSize]
				{Figs/img_067/part_CARN.png}} &                   
			\subfloat[MSRN~\cite{msrn}~\protect\linebreak(18.58/0.8950)]
			{\includegraphics[width = \patchSize, height = \patchSize]
				{Figs/img_067/part_MSRN.png}} &
			\subfloat[Ours~\protect\linebreak(\textbf{18.84/0.9035})]
			{\includegraphics[width = \patchSize, height = \patchSize]
				{Figs/img_067/part_PMRN.png}}
		\end{tabular}
		
		\begin{tabular}[b]{ccccc}
			\multirow{2}{*}[\rowArg]{
				\subfloat[image\_078 from Urban100]
				{\includegraphics[width = \fullSizeWid, height = \fullSizeHei]
					{Figs/img_078/full_HR.png}}} &
			\subfloat[HR~\protect\linebreak(PSNR/SSIM)]
			{\includegraphics[width = \patchSize, height = \patchSize]
				{Figs/img_078/part_HR.png}} &
			\subfloat[LR~\protect\linebreak(23.74/0.7624)]
			{\includegraphics[width = \patchSize, height = \patchSize]
				{Figs/img_078/part_nearest.png}} &
			\subfloat[Bicubic~\protect\linebreak(24.49/0.7866)]
			{\includegraphics[width = \patchSize, height = \patchSize]
				{Figs/img_078/part_bicubic.png}} &
			\subfloat[VDSR~\cite{vdsr}~\protect\linebreak(25.49/0.8401)]
			{\includegraphics[width = \patchSize, height = \patchSize]
				{Figs/img_078/part_VDSR.png}} \\ [-0.3cm]&
			\subfloat[LapSRN~\cite{lapsrn}~\protect\linebreak(25.41/0.8395)]
			{\includegraphics[width = \patchSize, height = \patchSize]
				{Figs/img_078/part_LapSRN.png}} &
			\subfloat[CARN~\cite{carn}~\protect\linebreak(25.88/0.8536)]
			{\includegraphics[width = \patchSize, height = \patchSize]
				{Figs/img_078/part_CARN.png}} &                   
			\subfloat[MSRN~\cite{msrn}~\protect\linebreak(26.12/0.8598)]
			{\includegraphics[width = \patchSize, height = \patchSize]
				{Figs/img_078/part_MSRN.png}} &
			\subfloat[Ours~\protect\linebreak(\textbf{26.45/0.8658})]
			{\includegraphics[width = \patchSize, height = \patchSize]
				{Figs/img_078/part_PMRN.png}}
		\end{tabular}
	\end{center}
	\setlength{\abovecaptionskip}{0pt plus 2pt minus 2pt}
	\setlength{\belowcaptionskip}{0pt plus 2pt minus 2pt}
\caption{Visualization comparisons on Urban100 with \textbf{BI}~ degradation.}
	\label{fig:vis-result-bi}
\end{figure*}

\begin{figure*}
	\captionsetup[subfloat]{labelformat=empty, justification=centering}
	\begin{center}
		\scriptsize
		\setlength\tabcolsep{0.1cm}
		\begin{tabular}[b]{cccc}
			\subfloat[HR~(PSNR/SSIM)]{\includegraphics[width=0.2\linewidth]{Figs/Manga109/HR1.png}}&
			\subfloat[LR~(20.16/0.8521)]{\includegraphics[width=0.2\linewidth]{Figs/Manga109/LR1.png}}&
			\subfloat[MSRN~\cite{msrn}~(26.31/0.9296)]{\includegraphics[width=0.2\linewidth]{Figs/Manga109/MSRN1.png}}&
			\subfloat[PMRN~(\textbf{26.46/0.9637})]{\includegraphics[width=0.2\linewidth]{Figs/Manga109/PMRN1.png}} \\
			
			\subfloat[HR~(PSNR/SSIM)]{\includegraphics[width=0.2\linewidth]{Figs/Manga109/HR2.png}}&
			\subfloat[LR~(21.34/0.8693)]{\includegraphics[width=0.2\linewidth]{Figs/Manga109/LR2.png}}&
			\subfloat[MSRN~\cite{msrn}~(27.49/0.9690)]{\includegraphics[width=0.2\linewidth]{Figs/Manga109/MSRN2.png}}&
			\subfloat[PMRN~(\textbf{27.76/0.9637})]{\includegraphics[width=0.2\linewidth]{Figs/Manga109/PMRN2.png}} \\
		\end{tabular}
	\end{center}
	\setlength{\abovecaptionskip}{0pt plus 2pt minus 2pt}
	\setlength{\belowcaptionskip}{0pt plus 2pt minus 2pt}
	\caption{Visualization comparisons on Manga109 with \textbf{BI}~ degradation.}
	\label{fig:manga109}
\end{figure*}

\begin{figure*}
	\captionsetup[subfloat]{labelformat=empty, justification=centering}
	\begin{center}
		\scriptsize
		\setlength\tabcolsep{0.1cm}
		\begin{tabular}[b]{cccc}
			\subfloat[HR~(PSNR/SSIM)]{\includegraphics[width=0.2\linewidth]{Figs/set14/part_hr.png}}&
			\subfloat[LR~(23.92/0.6380)]{\includegraphics[width=0.2\linewidth]{Figs/set14/part_lr.png}}&
			\subfloat[MSRN~\cite{msrn}~(26.11/0.7529)]{\includegraphics[width=0.2\linewidth]{Figs/set14/part_msrn.png}}&
			\subfloat[PMRN~(\textbf{26.18/0.7547})]{\includegraphics[width=0.2\linewidth]{Figs/set14/part_pmrn.png}} \\
		\end{tabular}
	\end{center}
	\setlength{\abovecaptionskip}{0pt plus 2pt minus 2pt}
	\setlength{\belowcaptionskip}{0pt plus 2pt minus 2pt}
	\caption{Visualization comparisons on Set14 with \textbf{BI}~ degradation.}
	\label{fig:set14}
\end{figure*}



Furthermore, we investigate the restoration capacity on blurred images. From the formulation of SISR problem, these works can naturally handle the blurry issue. We compare the proposed PMRN with recent works with blur and  downsampling~(\textbf{BD}) degradation, which is shown in Tab.~\ref{tab:bd}. From the comparison, PMRN achieves competitive PSNR/SSIM performance with RDN, and the extension model PMRN achieves superior performance than all other works. It should be noted that RDN holds 22.308K parameters and 2282.2G MACs, which are much more than PMRN. From this perspective, PMRN is an efficient design which can effectively restore the blurred images.

\begin{table*}
	\centering
	\caption{PSNR/SSIM comparisons on \textbf{BD} degradation.}
	\label{tab:bd}
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
		\hline
		\textbf{Method}& Bicubic& SRCNN~\cite{srcnn}& VDSR~\cite{vdsr}& IRCNN\_G~\cite{ircnn_zhang2017learning}& IRCNN\_C~\cite{ircnn_zhang2017learning}& RDN~\cite{rdn2020}& PMRN& PMRN\\
		\hline
		\hline
		\textbf{Set5}& 			28.78/0.8308& 32.05/0.8944& 33.25/0.9150& 33.38/0.9182& 33.17/0.9157& 34.58/0.9280& 34.53/0.9274& \textbf{34.66/0.9284}\\
		\textbf{Set14}& 		26.38/0.7271& 28.80/0.8074& 29.46/0.8244& 29.63/0.8281& 29.55/0.8271& 30.53/0.8447& 30.51/0.8442& \textbf{30.60/0.8453}\\
		\textbf{B100}& 			26.33/0.6918& 28.13/0.7736& 28.57/0.7893& 28.65/0.7922& 28.49/0.7886& 29.23/0.8079& 29.22/0.8073& \textbf{29.28/0.8083}\\
		\textbf{Urban100}& 		23.52/0.6862& 25.70/0.7770& 26.61/0.8136& 26.77/0.8154& 26.47/0.8081& 28.46/0.8582& 28.48/0.8580& \textbf{28.63/0.8603}\\
		\textbf{Manga109}& 		25.46/0.8149& 29.47/0.8924& 31.06/0.9234& 31.15/0.9245& 31.13/0.9236& 33.97/0.9465& 34.05/0.9464& \textbf{34.36/0.9480}\\
		\hline
	\end{tabular}
\end{table*}
	

To further investigate the performance of the proposed model, we perform the PSNR significant tests among PMRN, MSRN~\cite{msrn}, MS-LapSRN~\cite{ms-lapsrn}, VDSR~\cite{vdsr}, and bicubic on Urban100 dataset with \textbf{BI} degradation, which is shown in Fig.~\ref{fig:significant}. From the comparison, PMRN achieves higher average PSNR than other works. Compared with MSRN and MS-LapSRN, PMRN achieves higher median  and  values, which proves the effectiveness of the proposed method. We also perform the ANOVA (analysis of variance) between PMRN and MSRN, and find the P-value . Although there is no statistical significant difference, PMRN requires near half parameters and MACs than MSRN, which proves to be an efficient network for restoration.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Figs/significant.png}
    \caption{PSNR significant tests on Urban100 dataset with \textbf{BI} degradation.}
    \label{fig:significant}
\end{figure}



\section{Conclusion}
In this paper, we proposed a progressive multi-scale residual network~(PMRN) with limited parameters and computation complexity for single image super-resolution~(SISR) problem. Specifically, a novel progressive multi-scale residual block~(PMRB) was introduced in PMRN for information exploration from various scales. Different layer combinations for multi-scale features extraction were designed in a recursive way to decrease the parameters and computation complexity, which progressively exploited the features. Besides PMRB, we also proposed a joint channel-wise and pixel-wise attention mechanism named CPA for inherent correlation consideration of features. Different from previous works, weighting and bias factors were explored in parallel for better representations. Experimental results shows PMRN could not only achieve competitive or better PSNR/SSIM results than other small works on five testing benchmarks, but also recover more complex structural textures. Meanwhile, the extension model PMRN with much fewer parameters and lower computation complexity could achieve competitive or better PSNR/SSIM results than other large networks.

\bibliographystyle{ieeetr}
\bibliography{main}	

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{liuyuqing.png}}]{Yuqing Liu}
received the B.S. degree in software engineering from the Dalian University of Technology, China, in 2017. He is currently pursuing the Ph.D. degree. His current research interests include video compression, processing, and analysis.
\end{IEEEbiography}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{zhangxinfeng.png}}]{Xinfeng Zhang}
(M’16) received the B.S. degree in computer science from the Hebei University of Technology, Tianjin, China, in 2007, and the Ph.D. degree in computer science from the Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China, in 2014. From 2014 to 2017, he was a Research Fellow with the Rapid-Rich Object Search Lab, Nanyang Technological University, Singapore. From 2017 to 2018, he was a Postdoctoral Fellow with the Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, CA, USA. From December 2018 to August 2019, he was a Research Fellow with the Department of Computer Science, City University of Hong Kong. He is currently an Assistant Professor with the Department of Computer Science and Technology, University of Chinese Academy of Sciences. He has authored over 100 technical articles in important conferences and journals. His research interests include image and video processing, and image and video compression.
\end{IEEEbiography}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{wangshanshe.png}}]{Shanshe Wang}
received the B.S. degree from the Department of Mathematics, Heilongjiang University, Harbin, China, in 2004, the M.S. degree in computer software and theory from Northeast Petroleum University, Daqing, China, in 2010, and the Ph.D. degree in computer science from the Harbin Institute of Technology. He held a postdoctoral position at Peking University, Beijing, from 2016 to 2018. He joined the School of Electronics Engineering and Computer Science, Institute of Digital Media, Peking University, where he is currently a Research Assistant Professor. His current research interests include video compression and image and video quality assessment.
\end{IEEEbiography}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{masiwei.png}}]{Siwei Ma}
(M’03–SM’12) received the B.S. degree from Shandong Normal University, Jinan, China, in 1999, and the Ph.D. degree in computer science from the Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China, in 2005. He held a postdoctoral position with the University of Southern California, Los Angeles, CA, USA, from 2005 to 2007. He joined the School of Electronics Engineering and Computer Science, Institute of Digital Media, Peking University, Beijing, where he is currently a Professor. He has authored over 200 technical articles in refereed journals and proceedings in image and video coding, video processing, video streaming, and transmission. He is an Associate Editor of the IEEE Transaction on Circuits and Systems for Video Technology and Journal of Visual Communication and Image Representation.
\end{IEEEbiography}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{gaowen.png}}]{Wen Gao}
(M’92–SM’05–F’09) received the Ph.D. degree in electronics engineering from The University of Tokyo, Japan, in 1991. He was a Professor of computer science with the Harbin Institute of Technology, from 1991 to 1995, and a Professor with the Institute of Computing Technology, Chinese Academy of Sciences. He is currently a Professor of computer science with Peking University, China.. He has published extensively including five books and over 600 technical articles in refereed journals and conference proceedings in the areas of image processing, video coding and communication, pattern recognition, multimedia information retrieval, multimodal interface, and bioinformatics. He chaired a number of prestigious international conferences on multimedia and video signal processing, such as the IEEE ICME and the ACM Multimedia, and also served on the advisory and technical committees of numerous professional organizations. He served or serves on the Editorial Board for several journals, such as the IEEE Transactions on Circuits and Systems for Video Technology, the IEEE Transactions on Multimedia, the IEEE Transactions on Image Processing, the IEEE Transactions on Autonomous Mental Development, the EURASIP Journal of Image Communications, and the Journal of Visual Communication and Image Representation.
\end{IEEEbiography}
\end{document}
