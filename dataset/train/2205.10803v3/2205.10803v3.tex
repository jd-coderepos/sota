\section{Experiments}\label{sec:experiments}

In this section, we demonstrate that \model  is a general self-supervised framework for various graph learning tasks, including:
\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.5em,topsep=0.3em,partopsep=0.3em]
    \item Unsupervised representation learning for \textit{node classification};
    \item Unsupervised representation learning for \textit{graph classification};
    \item \textit{Transfer learning} on molecular property prediction.
\end{itemize}
Extensive experiments on various datasets are conducted to evaluate 
the performance of \model against state-of-the-art (SOTA) contrastive and generative methods on these three tasks. 
In each task, we follow exactly the same experimental procedure, e.g., data splits, evaluation protocol, as the  standard settings~\cite{velivckovic2018deep,zhang2021canonical,sun2019infograph,hu2020strategies}. 




\subsection{Node Classification}




\vpara{Setup.}
The node classification task is to predict the unknown node labels in networks. 
We test the performance of \model on 6 standard benchmarks: Cora, Citeseer, PubMed~\cite{yang2016revisiting}, Ogbn-arxiv~\cite{hu2020open}, PPI, and Reddit. 
Following the inductive setup in GraphSage~\cite{hamilton2017inductive}, the testing for Reddit and PPI is carried out on unseen nodes and graphs, while the other networks are used for transductive learning.

For the evaluation protocol, we follow the experimental setting in ~\cite{velivckovic2018deep,hassani2020contrastive,thakoor2021bootstrapped,zhang2021canonical}. 
First, we train a GNN encoder by the proposed \model without supervision. 
Then we freeze the parameters of the encoder and generate all the nodes' embeddings. 
For evaluation, we train a linear classifier and report the mean accuracy on the test nodes through 20 random initializations. 
We follow the public data splits ~\cite{velivckovic2018deep,hassani2020contrastive,zhang2021canonical}  of Cora, Citeseer, and PubMed. 
The graph encoder  and decoder  are both specified as standard GAT. 
Detailed hyper-parameters can be found in  Appendix \ref{app:appendix}.


\vpara{Results.} 
We compare  \model with SOTA  contrastive self-supervised models, DGI~\cite{velivckovic2018deep}, MVGRL~\cite{hassani2020contrastive}, GRACE~\cite{zhu2020deep}, BGRL~\cite{thakoor2021bootstrapped}, InfoGCL~\cite{Xu2021InfoGCLIG}, and CCA-SSG~\cite{zhang2021canonical}, as well as supervised baselines GCN and GAT. 
We also report the results of previous generative self-supervised models, GAE~\cite{kipf2016variational}, GPT-GNN~\cite{hu2020gpt}, and GATE~\cite{amin2020gate}.
We report results from previous works with the same experimental setup if available.
If results are not previously reported and codes are provided, we implement them based on the official codes and conduct a hyper-parameter search. 
Table~\ref{tab:node_clf} lists the results. 
\model achieves the best or competitive results compared to the SOTA self-supervised approaches in all benchmarks. 
Notably, \model outperforms existing generative methods by a large margin. 
The results in the inductive setting of PPI and Reddit suggest the self-supervised \model technique provides strong generalization to unseen nodes. 









\subsection{Graph Classification}


\vpara{Setup.}
For graph classification,  we conduct experiments on 7 benchmarks: MUTAG, IMDB-B, IMDB-M, PROTEINS, COLLAB, REDDIT-B, and NCI1~\cite{yanardag2015deep}.
Each dataset is a collection of graphs where each graph is associated with a label.
Node labels are used as input features in MUTAG, PROTEINS, and NCI1, whereas node degrees are used in IMDB-B, IMDB-M, REDDIT-B, and COLLAB.

For the evaluation protocol, after generating graph embeddings with \model's encoder and readout function, we feed encoded graph-level representations into a downstream LIBSVM~\cite{chang2011libsvm} classifier to predict the label, and report the mean 10-fold cross-validation accuracy with standard deviation after 5 runs.  We adopt GIN~\cite{xu2019powerful}, which is commonly used in previous graph classification works, as the backbone of encoder and decoder.


\vpara{Results.}
In addition to classical graph kernel methods---Weisfeiler-Lehman sub-tree kernel (WL)~\cite{shervashidze2011weisfeiler} and deep graph kernel (DGK)~\cite{yanardag2015deep}, we also compare \model with SOTA unsupervised and contrastive methods, 
GCC~\cite{qiu2020gcc}, graph2vec~\cite{narayanan2017graph2vec}, Infograph~\cite{sun2019infograph}, GraphCL~\cite{you2020graph}, JOAO~\cite{you2021graph},  MVGRL~\cite{hassani2020contrastive}, and InfoGCL~\cite{Xu2021InfoGCLIG}. 
The supervised baselines, GIN~\cite{xu2019powerful} and DiffPool~\cite{ying2018hierarchical}, are also included. 
Per graph classification research tradition, we report results from previous papers if available.
The results are shown in Table~\ref{tab:graph_clf}.
We find that \model outperforms all self-supervised baselines on five out of seven datasets and has competitive results on the other two.
The node features of these seven datasets are all one-hot vectors representing node-labels or degrees, which are considered to be less informative than node features in node classification. The results manifest that generative auto-encoders could learn meaningful information and offer potentials in graph-level tasks.









\begin{table*}[htbp]
    \centering
    \caption{Experiment results in unsupervised representation learning for \underline{node classification}. \textmd{We report the Micro-F1 (\%) score for PPI and accuracy (\%) for the other datasets. }
    }
    \begin{threeparttable}
    \renewcommand\tabcolsep{10pt}
    \renewcommand\arraystretch{1.05}
    \begin{tabular}{c|c|cccccc}
        \toprule[1.2pt]
            & Dataset &   Cora      & CiteSeer      & PubMed                & Ogbn-arxiv        & PPI               & Reddit        \\


         \midrule
        \multirow{2}{*}{Supervised} 
        & GCN     &  81.5          & 70.3          & 79.0                   & 71.740.29    & 75.70.1    & 95.30.1           \\
        & GAT     &  83.00.7  & 72.50.7  & 79.00.3           & 72.100.13     & 97.300.20    & 96.00.1           \\
        \midrule
        \multirow{10}{*}{Self-supervised} 
        & GAE     &  71.50.4  & 65.80.4  & 72.10.5           & -               & -           & - \\
        & GPT-GNN &  80.11.0  & 68.41.6  & 76.30.8 & - & - & -\\
        & GATE    &  83.20.6  & 71.80.8  & 80.90.3           & -                 & -             & -   \\ 
        & DGI     &  82.30.6  & 71.80.7  & 76.80.6           & 70.340.16 & 63.800.20     & 94.00.10 \\
        & MVGRL   & 83.50.4   & 73.30.5  & 80.10.7           & -               & - & - \\
        & GRACE   & 81.90.4   & 71.20.5  & 80.60.4           & 71.510.11  & 69.710.17  &    94.720.04\\  
        & BGRL    & 82.70.6   & 71.10.8  & 79.60.5           & \underline{71.640.12}   & \underline{73.630.16}  & 94.220.03         \\
        & InfoGCL  & 83.50.3   & \bf 73.50.4  & 79.10.2  & - & - & - \\
        & CCA-SSG & \underline{84.00.4}   & 73.10.3  & \underline{81.00.4}  & 71.240.20  & 73.340.17  & \underline{95.070.02}   \\
        \cmidrule{2-8}
& \model  & \bf 84.2±0.4  & \underline{73.4±0.4}  & \bf 81.1±0.4  & \bf 71.750.17 & \bf 74.500.29    & \bf 96.010.08    \\

        \bottomrule[1.2pt]
    \end{tabular}
     \begin{tablenotes}
        \footnotesize
        \item[] The results not reported are due to unavailable code or out-of-memory.
        \item[1] Results are from reproducing using authors' official code, as they did not report the results in part of datasets. The result of PPI is a bit different from what the authors' reported. This is because we train the linear classifier until convergence, rather than for a small fixed number of epochs during evaluation, using the official code.
    \end{tablenotes}

    \end{threeparttable}
    \label{tab:node_clf}
\end{table*}

\begin{table*}[htbp]
    \centering
    \caption{Experiment results in unsupervised representation learning for \underline{graph classification}. \textmd{We report accuracy (\%) for all datasets.}}
    \begin{threeparttable}
    \renewcommand\arraystretch{1.05}
    \begin{tabular}{c|c|ccccccc}
        \toprule[1.2pt]
              & Dataset  & IMDB-B     & IMDB-M     & PROTEINS   & COLLAB     & MUTAG      & REDDIT-B   & NCI1     \\ 

        \midrule
        \multirow{2}{*}{Supervised}
        & GIN         & 75.15.1   & 52.32.8   & 76.22.8   & 80.21.9   & 89.45.6   & 92.42.5   & 82.71.7 \\ & DiffPool    & 72.63.9 &  -           &  75.13.5   & 78.92.3 & 85.010.3 & 92.12.6 & - \\
        \midrule
        \multirow{2}{*}{Graph Kernels}
        & WL          & 72.303.44 & 46.950.46 & 72.920.56 & - & 80.723.00 & 68.820.41 & 80.310.46 \\ & DGK         & 66.960.56 & 44.550.52 & 73.300.82 & - & 87.442.72 & 78.040.39 & 80.310.46 \\ \midrule
        \multirow{8}{*}{Self-supervised}
        & graph2vec   & 71.100.54 & 50.440.87 & 73.302.05 & -              & 83.159.25 & 75.781.03 & 73.221.81 \\ & Infograph   & 73.030.87 & 49.690.53 & 74.440.31 & 70.651.13 & 89.011.13 & 82.501.42 & 76.201.06 \\& GraphCL     & 71.140.44 & 48.580.67 & 74.390.45 & 71.361.15 & 86.801.34 & \underline{89.530.84} & 77.870.41 \\& JOAO        & 70.213.08 & 49.200.77     & \underline{74.550.41} & 69.500.36 & 87.351.02 & 85.291.35 & 78.070.47 \\& GCC         & 72.0           & 49.4           & -    & 78.9    &  - & \bf 89.8 & - \\& MVGRL       & 74.200.70   & 51.200.50   & -              & -              & \underline{89.701.10}   & 84.500.60   & -               \\& InfoGCL     & \underline{75.100.90}   & \underline{51.400.80}   &  -             & \underline{80.001.30}   & \bf 91.201.30   & -              &  \underline{80.200.60}   \\\cmidrule{2-9}
        & \model      & \bf 75.520.66 & \bf 51.630.52 & \bf 75.300.39 & \bf 80.320.46 & 88.191.26 & 88.010.19 & \bf 80.400.30  \\\bottomrule[1.2pt]
    \end{tabular}
        \begin{tablenotes}
            \footnotesize
            \item[]   The reported results of baselines are from previous papers if available.
        \end{tablenotes}
    \end{threeparttable}
    \label{tab:graph_clf}
\end{table*}


\begin{table*}[htp]
\caption{Experiment results in \underline{transfer learning} on molecular property prediction benchmarks. \textmd{The model is first pre-trained on ZINC15 and then finetuned on the following datasets. We report ROC-AUC scores (\%).}}
\label{tab:mol_clf}
\renewcommand\tabcolsep{8pt}
\renewcommand\arraystretch{1.05}
\begin{tabular}{c|cccccccc|c}
\toprule[1.2pt]
                        & BBBP       & Tox21      & ToxCast    & SIDER      & ClinTox    & MUV        & HIV        & BACE & Avg.      \\
\midrule
    No-pretrain         & 65.5±1.8   & 74.3±0.5   & 63.3±1.5   & 57.2±0.7   & 58.2±2.8   & 71.7±2.3   & 75.4±1.5   & 70.0±2.5 & 67.0  \\
\midrule    
    ContextPred         & 64.3±2.8   & \underline{75.7±0.7}   & 63.9±0.6   & 60.9±0.6   & 65.9±3.8   & 75.8±1.7   & 77.3±1.0   & 79.6±1.2 & 70.4 \\
    AttrMasking         & 64.3±2.8   & \bf 76.7±0.4   & \bf 64.2±0.5   &  \underline{61.0±0.7}   & 71.8±4.1   & 74.7±1.4   & 77.2±1.1   & 79.3±1.6 & 71.1 \\
    Infomax             & 68.8 ±0.8  & 75.3 ±0.5  & 62.7 ±0.4  & 58.4 ±0.8  & 69.9±3.0  & 75.3 ±2.5  & 76.0 ±0.7  & 75.9 ±1.6 & 70.3 \\
GraphCL             & 69.7±0.7 & 73.9±0.7 & 62.4±0.6 & 60.5±0.9 & 76.0±2.7 & 69.8±2.7 & \bf 78.5±1.2 & 75.4±1.4 & 70.8 \\
    JOAO                & 70.2±1.0 & 75.0±0.3 & 62.9±0.5 & 60.0±0.8 & \underline{81.3±2.5} & 71.7±1.4 & 76.7±1.2 & 77.3±0.5 & 71.9 \\
    GraphLoG            & \bf 72.5±0.8 &  \underline{75.7±0.5}  &  63.5±0.7     & \bf 61.2±1.1  & 76.7±3.3   & \underline{76.0±1.1}    & \underline{77.8±0.8}  & \bf 83.5±1.2 & \underline{73.4} \\ 
\midrule
GraphMAE            & \underline{72.0±0.6} & 75.5±0.6 & \underline{64.1±0.3} & 60.3±1.1 & \bf 82.3±1.2 & \bf 76.3±2.4 & 77.2±1.0 & \underline{83.1±0.9} & \bf 73.8 \\
\bottomrule[1.2pt]
\end{tabular}
\end{table*}





\subsection{Transfer Learning}



\vpara{Setup.} To evaluate the transferability of the proposed method, we test the performance on transfer learning on molecular property prediction, following the setting of ~\cite{hu2020strategies,you2020graph,you2021graph}. The model is first pre-trained in  2 million unlabeled molecules sampled from the ZINC15~\cite{sterling2015zinc}, and then finetuned in 8 classification benchmark datasets contained in MoleculeNet~\cite{wu2018moleculenet}. The downstream datasets are split by scaffold-split to mimic real-world use cases. Input node features are the atom number and chirality tag, and edge features are the bond type and direction. 


For the evaluation protocol,  we run experiments for 10 times and report the mean and standard deviation of ROC-AUC scores (\%). Following the default setting in ~\cite{hu2020strategies},  we adopt a 5-layer GIN as the encoder and a single-layer GIN as the decoder.


\vpara{Results.}
We evaluate \model against methods including Infomax, AttrMasking and ContextPred~\cite{hu2020strategies} , and SOTA contrastive learning methods, GraphCL~\cite{you2020graph}, JOAO~\cite{you2021graph}, and GraphLoG~\cite{xu2021self}. Table \ref{tab:graph_clf} shows that the performance on downstream tasks is comparable to SOTA methods, in which \model achieves the best average scores and has a small edge over previous best results in two tasks. 
This demonstrates the robust transferability of \model.


To summarize,  the self-supervised \model  method achieves  competitive performance on node classification, graph classification, and transfer learning across 21 benchmarks. Note that we do not customize a dedicated \model for each task. 
The consistent results on the three tasks demonstrate that \model is an effective and universal self-supervised graph pre-training framework for various applications. 













\hide{
\begin{figure}
    \centering
    \includegraphics[width=0.43\textwidth]{imgs/ppi_hidden.pdf}
    \vspace{-4mm}
    \caption{Performance on PPI using GAT with 4 attention heads, compared to other baselines. Self-supervised methods benefit much from larger model size, and \model could outperform supervised model. }
    \label{fig:ppi_hidden}
\end{figure}
}


\subsection{Ablation Studies}
To verify the effects of the main components in \model, we further conduct several ablation studies. Without loss of generalization, we choose three datasets from node classification and two datasets from graph classification for experiments.


\vpara{Effect of reconstruction criterion.} 
We study the influence of reconstruction criterion, and Table \ref{tab:ablation} shows the results of MSE and the SCE loss function.
Generally, the input features in node classification lie in continuous or high-dimensional discrete space, containing more discriminative information.
The results manifest that SCE has a significant advantage over MSE,
with an absolute gain of 1.5\%  8.0\%.
In graph classification, pre-training with either MSE or SCE improves accuracy. 
The input features are discrete one-hot encoding in these benchmarks, 
representing either node degrees or node labels. 
Reconstructing one-hot encoding with MSE is
similar to classification tasks, thus MSE also works. Nevertheless, SCE offers a performance edge (though limited) over MSE. 


Figure \ref{fig:ablation} shows the influence of scaling factor .  We observe that  offers benefits in most cases, especially in node classification. However, in MUTAG, a larger  value harms the performance. In our experiments, we notice that the training loss in node classification is much higher than that in graph classification. 
This further demonstrates that aligning continuous vectors in a unit sphere is more challenging. Therefore, scaling  brings improvements. 





\begin{table}[t]
\caption{Ablation studies of the decoder type, re-mask and reconstruction criterion on node- and graph-level datasets.}
\label{tab:ablation}
\renewcommand\tabcolsep{4pt}






\begin{tabular}{cccccccc}
\toprule[1.2pt]
\multicolumn{2}{c}{\multirow{2}{*}{Dataset}}
                     & \multicolumn{3}{c}{Node-Level} & \phantom{} & \multicolumn{2}{c}{Graph-Level} \\
\cmidrule{3-5} \cmidrule{7-8}
                    & & Cora        & PubMed      & Arxiv  && MUTAG              & IMDB-B              \\

\midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{\small{COMP.}}}
& \model        & 84.2  & 81.1  & 71.75  && 88.19         & 75.52         \\
& \small{w/o mask} &  79.7 & 77.9 & 70.97 & & 82.58 & 74.42 \\
& \small{w/o re-mask}   & 82.7  & 80.0  & 71.61  && 86.29         & 74.42          \\
& \small{w/ MSE}                & 79.1  & 73.1  & 67.44  && 86.30         & 74.04          \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{\small{Decoder}}}
& MLP                   & 82.2  & 80.4  & 71.54  && 87.16         & 73.94          \\
& GCN                   & 81.3  & 79.1  & 71.59  && 87.78         & 74.54             \\
& GIN                   & 81.8  & 80.2  & 71.41  && 88.19         & 75.52          \\
& GAT                   & 84.2  & 81.1  & 71.75  && 86.27         & 74.04           \\
\bottomrule[1.2pt]
\end{tabular}


 \vspace{-2mm}
\end{table}





\vpara{Effect of mask and re-mask.} Masking plays an important role in the proposed \model method. \model employs two masking strategies --- masking input feature before encoder, and re-masking encoded code before decoder. Table \ref{tab:ablation} studies the designs. We observe a significant drop in performance if not masking input features, indicating that masking inputs is vital to avoid  the trivial solution. For the re-mask strategy, the accuracy drops by 0.1\%1.9\% without it. 
Re-mask is designed for the GNN decoder and can be regarded as regularization, which makes the self-supervised task more challenging.






\hide{
\begin{table}
\centering
\caption{Experiment results using different encoder backbones in node classification.}
\renewcommand\tabcolsep{3pt}
\begin{tabular}{c|cccc}
\toprule[1.2pt]
          & Cora & Citeseer & Pubmed & Ogbn-arxiv \\
\midrule
BGRL (GCN)    &   82.70.6    &  71.10.7        & 79.40.6       &  71.640.12      \\
BGRL (GAT)    &   82.80.5    &   71.10.8       &  79.60.5      &   70.070.02 \\
CCA-SSG (GCN) &   84.00.4      &  73.10.3         &  81.00.4        &  70.810.13  \\
CCA-SSG (GAT) &   83.80.5    &  72.60.7        &   79.91.1     &  71.240.20  \\
\model (GCN)  &   82.90.6             &  72.50.5                 &  81.00.5               &  \bf 71.870.21\\   
\model (GAT)  &   \bf 84.20.4        &  \bf 73.40.4       & \bf 81.10.4      &  71.750.17 \\
\bottomrule[1.2pt]
\end{tabular}
\vspace{-2mm}
\label{tab:backbone}
\end{table}
}



\begin{figure}
    \centering
    \begin{minipage}[t]{0.235\textwidth}
        \includegraphics[width=\textwidth]{imgs/gmae_mask_ratio.pdf}
\end{minipage}
    \begin{minipage}[t]{0.235\textwidth}
        \includegraphics[width=\textwidth]{imgs/gamma_value.pdf}
\end{minipage}
    \vspace{-6mm}
    \caption{Ablation studies of mask ratio and scaling factor.}
    \label{fig:ablation}
    \vspace{-4mm}
\end{figure}








\vpara{Effect of mask ratio.} 
Figure \ref{fig:ablation} shows the influence of mask ratio. 
In most cases, the reconstruction task with a low mask ratio (0.1) is not challenging enough to learn useful features.
The optimal ratio varies across graphs. 
Results on Ogbn-arxiv and IMDB-B can be found in Appendix \ref{app:appendix}. 
In Cora, increasing the mask ratio by more than 0.5 degrades the performance, while \model still works with a surprisingly high ratio (0.70.9) in PubMed and MUTAG. 
This can be connected with the information redundancy in graphs. 
Large node degrees or high homogeneity may lead to heavy information redundancy, in which missing node features may be recovered from very few neighboring nodes with little high-level understanding of
features and local context. 
In contrast, lower redundancy means that an excessively high mask ratio would make it impossible to recover features, thus degrading the performance. 









\vpara{Effect of decoder type.} 
In Table \ref{tab:ablation}, we compare different decoder types, including MLP, GCN, GIN, and GAT. The re-mask strategy is only used for GNN decoders. As the results show, using GNN decoder typically boosts the performance. Compared to MLP, which reconstructs original features from latent representations, GNN enforces masked nodes to extract information relevant to their original features from the neighborhood. One reasonable assumption is that GNN avoids the representation tending to be the same as original features.
MLP also works in \model, which might be partly attribute to the usage of the SCE metric.


Among different GNNs, GIN performs better in graph-level tasks and GAT is a more reasonable option for node classification. It is observed that replacing GAT with GCN causes a significant drop, especially in Cora (2.9\%) and PubMed (2.0\%). 
We speculate that the attention mechanism matters in reconstructing continuous features with the re-mask strategy.












\hide{
\vpara{Effect of encoder architecture.}  In node classification, \model uses GAT as the encoder. To have a fair comparison and investigate the influence of different GNN backbones, we compare the best baselines, BGRL and CCA-SSG, in node classification datasets using GCN and GAT as the encoder. The results are shown in Table \ref{tab:backbone}. We observe that \model still outperforms the baselines with the same GAT backbone. 
In addition, the results manifest that attention mechanism would not always benefit graph self-supervised learning, as GCN is inferior to GAT, for instance, in (Cora, CCA-SSG) and (Ogbn-arxiv, \model). Under the training setting of \model, GAT could be a better option in most cases.
}
























































































\hide{

\section{Experiments}\label{sec:experiments}

In this section, we show our proposed \model is a general self-supervised framework for a wide range of graph tasks, including:
\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.5em,topsep=0.3em,partopsep=0.3em]
    \item Unsupervised representation learning for \textit{node classification}
    \item Unsupervised representation learning for \textit{graph classification}
    \item \textit{Transfer learning} on molecular property prediction.
\end{itemize}
\noindent We comprehensively evaluate the performance of \model against state-of-the-art methods on these three tasks. 

\subsection{Task Evalution}

\subsubsection{Node classification}\hfill


\vpara{Setup.}
The node classification task is to predict unknown node labels in  network. We analyze the performance of \model on a set of 6 standard transductive and inductive benchmarks. Cora, Citeseer, PubMed, and Ogbn-arxiv are citation networks where nodes correspond to documents and edges
represent citations. 
PPI is a protein-protein interaction dataset and  Reddit contains posts belonging to different communities with user comments. 
Following the inductive setup in ~\cite{hamilton2017inductive}, the testing for Reddit and PPI is carried out on unseen (untrained) nodes and graphs, while the citation networks are used for transductive learning.

For evaluation protocol, we follow experiment settings in ~\cite{velivckovic2018deep,hassani2020contrastive,thakoor2021bootstrapped,zhang2021canonical}. We first train the model using the method proposed in this paper, without supervision. Then we freeze the parameters of the encoder and obtain all the nodes' embeddings. For the evaluation, we train a linear classifier and report the mean classification accuracy with standard deviation on the test nodes through 20 random initialization. We follow the public data split ~\cite{velivckovic2018deep,hassani2020contrastive,zhang2021canonical}  of Cora/Citeseer/PubMed. The graph encoder  is specified as a standard GAT model and the decoder  is a single-layer GAT. To have a fair comparison, we also implement previous SOTA contrastive methods based on GAT and conduct hyper-parameter search, as shown in Table \ref{tab:node_clf}. GAT doesn't always outperform GCN, indicating that GAT is not a absolute winner in self-supervised learning. Under the training setting of \model, GAT could be a better option.
Detailed hyper-parameters can be found in the Appendix.


\vpara{Experiment results.} 
We compare our \model with state-of-the-art (SOTA) contrastive self-supervised models, DGI~\cite{velivckovic2018deep}, MVGRL~\cite{hassani2020contrastive}, GRACE~\cite{zhu2020deep}, BGRL~\cite{thakoor2021bootstrapped}, and CCA-SSG~\cite{zhang2021canonical}, and supervised baselines GCN and GAT. We also report the results of previous generative self-supervised models, GAE~\cite{kipf2016variational}, GPT-GNN~\cite{hu2020gpt}, and GATE~\cite{amin2020gate}.
We report results from previous papers with the same experimental setup if available. If results are not previously reported and codes are provided, we implement them based on the official codes and conduct a hyper-parameter search. Otherwise we only report the results on Cora/Citeseer/PubMed using our re-implemented codes according to the paper. Table~\ref{tab:node_clf} represents the results, and it shows that our approach achieves the best or competitive results compared to the state-of-the-art self-supervised approaches. To test the influence of different GNN backbones, we also compare the best baselines, BGRL and CCA-SSG, in citation networks using GCN and GAT as the backbone. The results is shown in Table \ref{tab:backbone}. On the one hand, we observe that \model still outperforms the baselines with the same GAT backbone. 
On the other hand, the results manifest that, attention mechanism would not always bring benefits in graph self-supervised learning, as GCN is inferior to GAT, for instance, in (Cora, CCA-SSG) and (Ogbn-arxiv, \model).
Besides, the results in the inductive setting of PPI and Reddit substantiates the ability of generalization to unseen nodes. \model successfully outperformed all the competing self-supervised methods.  
Previous studies often reports a large gap between supervised and self-supervised results on PPI. We found that increasing the number of model parameters helps little in supervised setting, but could highly boost the performance in self-supervised setting. As show in Table~\ref{tab:ppi_hd}, \model even outperforms supervised counter-part, although the model would be too much larger. This might indicate the potential of large-scale GNNs in pre-training.





\begin{table}
\centering
\small
\caption{Experiment results using different encoder backbones in node classification.}
\renewcommand\tabcolsep{3pt}
\begin{tabular}{c|cccc}
\toprule
          & Cora & Citeseer & Pubmed & Ogbn-arxiv \\
\midrule
BGRL (GCN)    &   82.660.60    &  71.060.66        & 79.440.58       &  71.640.12      \\
BGRL (GAT)    &   82.750.52    &   71.080.79       &  79.550.50      &   70.070.02 \\
CCA-SSG (GCN) &   84.00.4      &  73.10.3          &  80.90.4        &  70.810.13  \\
CCA-SSG (GAT) &   83.780.48    &  72.590.66        &   79.911.09     &  71.240.20  \\
\model (GCN)  &   82.910.56             &  72.500.45                 &  81.050.54               &  \bf 71.870.21\\   
\model (GAT)  &   \bf 84.16±0.44        &  \bf 73.350.42       & \bf 81.100.41      &  71.750.17 \\
\bottomrule
\end{tabular}
\label{tab:backbone}
\end{table}




\subsubsection{Graph classification}\hfill


\vpara{Setup.}
For unsupervised learning for graph classification,  we conduct experiments on 8 well-known benchmarks: MUTAG, IMDB-B, IMDB-M, PROTEINS, COLLAB, REDDIT-B, DD, and NCI1~\cite{yanardag2015deep}, which are widely used in recent graph classification models. Each dataset is a set of graphs where each graph is associated with a label. MUTAG, PROTEINS, DD, and NCI1 have node labels as input features, while IMDB-B, IMDB-M, REDDIT-B, and COLLAB use node degrees as input features. 

For evaluation protocol, after generating graph embeddings with \model's encoder, we feed encoded graph-level representation into a down-stream LIBSVM~\cite{chang2011libsvm} classifier to predict the label, and report the mean 10-fold cross validation accuracy with standard deviation after 5 runs.  We adopt GIN~\cite{xu2019powerful}, which is commonly used in previous graph classification works, as the backbone of encoder and decoder.


\vpara{Experiment results.}
Aside from classical graph kernel methods, Weisfeiler-Lehman sub-tree kernel (WL)~\cite{shervashidze2011weisfeiler} and deep graph kernel (DGK)~\cite{yanardag2015deep}, we also compare \model with SOTA unsupervised and contrastive learning methods, graph2vec~\cite{narayanan2017graph2vec}, Infograph~\cite{sun2019infograph}, GraphCL~\cite{you2020graph}, JOAO~\cite{you2021graph}, and InfoGCL~\cite{Xu2021InfoGCLIG}. The performance of supervised baselines, GIN~\cite{xu2019powerful} and DGCNN~\cite{Zhang2018AnED} is also included. We report results from previous papers if available.
The results are shown in Table~\ref{tab:graph_clf}.
We find that \model outperforms all unsupervised baselines on 4 out of 7 of the datasets, and has competitive results in two of the other. In these benchmarks, node features are all one-hot vectors, representing node-labels or degrees, which are often considered to be less informative compared with node features in node classification.  The results manifest that generative auto-encoding could also learn meaningful information and be potentional in graph-level tasks.









\begin{table*}[htbp]
    \centering
    \caption{\textmd{Experiment results in unsupervised representation learning for \textbf{\textit{node classification}}. We report Micro-F1(\%) score for PPI and accuracy(\%) for the other datasets. }}
    \begin{threeparttable}
    \renewcommand\tabcolsep{8pt}
    \begin{tabular}{c|c|cccccc}
        \toprule[1.2pt]
            & Dataset &   Cora      & CiteSeer      & PubMed                & Ogbn-arxiv        & PPI               & Reddit        \\


         \midrule
        \multirow{2}{*}{Supervised} 
        & GCN     &  81.5          & 70.3          & 79.0                   & 71.740.29    & 75.70.1    & 95.4           \\
        & GAT     &  83.00.7  & 72.50.7  & 79.00.3           & 72.10.13     & 97.300.20    & 96.5           \\
        \midrule
        \multirow{10}{*}{Self-supervised} 
        & GAE     &  71.50.4  & 65.80.4  & 72.10.5           & OOM               & OOM           & OOM \\
        & GPT-GNN &  80.11.0  & 68.41.6  & 76.30.8 & - & - & -\\
        & GATE    &  83.20.6  & 71.80.8  & \underline{80.90.3}           & -                 & -             & -   \\ 
        & DGI     &  82.30.6  & 71.80.7  & 76.80.6           & 70.340.16 & 63.80.20     & 94.00.10 \\
        & MVGRL   & 83.50.4   & 73.30.5  & 80.10.7           & OOM               & OOM & OOM \\
        & GRACE   & 81.90.4   & 71.20.5  & 80.60.4           & 71.510.11  & 69.710.17  &    94.720.04\\  
        & BGRL    & 82.70.6   & 71.10.8  & 79.60.5           & \underline{71.640.12}   & \underline{73.630.16}  & 94.220.03         \\
        & InfoGCL  & 83.50.3   & \bf 73.50.4  & 79.10.2  & - & - & - \\
        & CCA-SSG & \underline{84.00.4}   & 73.10.3  & \underline{80.90.4}  & 71.240.20  & 73.340.17  & \underline{95.070.02}   \\
        \cmidrule{2-8}
         & \model  & \bf 84.160.44  & \underline{73.350.42}  & \bf 81.100.41  & \bf 71.750.17 & \bf 74.500.29    & \bf 96.010.08    \\
        \bottomrule[1.2pt]
    \end{tabular}
     \begin{tablenotes}
        \footnotesize
        \item[1] Results are from our reproducing with authors’ public code, as they didn't report the results in part of these datasets. The result of PPI is a bit different from that the authers' reported. This is because we use the authors' public code, and train the linear classifier until convergence, instead for a fixed number of epochs, in downstream evaluation.
        \item[2] The code is not publicly available.
    \end{tablenotes}

    \end{threeparttable}
    \label{tab:node_clf}
\end{table*}

\begin{table*}[htbp]
    \centering
    \caption{\textmd{Experiment results in unsupervised representation learning for \textbf{\textit{graph classification}}. We report accuracy(\%) for all datasets.}}
    \begin{threeparttable}
    \begin{tabular}{c|c|ccccccc}
        \toprule[1.2pt]
              & Dataset  & IMDB-B     & IMDB-M     & PROTEINS   & COLLAB     & MUTAG      & REDDIT-B   & NCI1     \\ 

        \midrule
        \multirow{2}{*}{Supervised}
        & GIN         & 75.15.1   & 52.32.8   & 76.22.8   & 80.21.9   & 89.45.6   & 92.42.5   & 82.71.7 \\ & DiffPool \\
        \midrule
        \multirow{2}{*}{Graph Kernels}
        & WL          & 72.303.44 & 46.950.46 & 72.920.56 & - & 80.723.00 & 68.820.41 & 80.310.46 \\ & DGK         & 66.960.56 & 44.550.52 & 73.300.82 & - & 87.442.72 & 78.040.39 & 80.310.46 \\ \midrule
        \multirow{8}{*}{Self-supervised}
        & graph2vec   & 71.100.54 & 50.440.87 & 73.302.05 & -              & 83.159.25 & 75.781.03 & 73.221.81 \\ & Infograph   & 73.030.87 & 49.690.53 & 74.440.31 & 70.651.13 & 89.011.13 & 82.501.42 & 76.201.06 \\& GraphCL     & 71.140.44 & 48.580.67 & 74.390.45 & 71.361.15 & 86.801.34 & \underline{89.530.84} & 77.870.41 \\& JOAO        & 70.213.08 & 49.200.77     & \underline{74.550.41} & 69.500.36 & 87.351.02 & 85.291.35 & 78.070.47 \\& GCC         & 72.0           & 49.4           & -    & 78.9    &  - & \bf 89.8 & - \\& MVGRL       & 74.20.7   & 51.20.5   & -              & -              & \underline{89.71.1}   & 84.50.6   & -               \\& InfoGCL     & \underline{75.10.9}   & \underline{51.40.8}   &  -             & \underline{80.01.3}   & \bf 91.21.3   & -              &  \underline{80.20.6}   \\\cmidrule{2-9}
        & \model      & \bf 75.520.66 & \bf 51.630.52 & \bf 75.300.39 & \bf 80.320.46 & 88.191.26 & 88.010.19 & \bf 80.400.30  \\\bottomrule[1.2pt]
    \end{tabular}
        \begin{tablenotes}
            \footnotesize
            \item[]   The reported results of baselines are from previous papers if available.
        \end{tablenotes}
    \end{threeparttable}
    \label{tab:graph_clf}
\end{table*}


\begin{table*}[htp]
\caption{\textmd{Experiment results in \textbf{\textit{transfer learning}} on molecular property prediction benchmarks. The model is first pre-trained on ZINC15 and then finetuned on the following datasets. We report ROC-AUC(\%) scores.}}
\label{tab:mol_clf}
\renewcommand\tabcolsep{6pt}
\begin{tabular}{c|cccccccc}
\toprule[1.2pt]
                        & BBBP       & Tox21      & ToxCast    & SIDER      & ClinTox    & MUV        & HIV        & BACE       \\
\midrule
    No-pretrain         & 65.51.8   & 74.30.5   & 63.31.5   & 57.20.7   & 58.22.8   & 71.72.3   & 75.41.5   & 70.02.5   \\
    ContextPred         & 64.32.8   & 75.70.7   & 63.90.6   & 60.90.6   & 65.93.8   & 75.81.7   & 77.31.0   & 79.61.2   \\
    AttrMasking         & 64.32.8   & 76.70.4   & 64.20.5   & 61.00.7   & 71.84.1   & 74.71.4   & 77.21.1   & 79.31.6   \\
    Infomax             & 68.80.8  & 75.30.5  & 62.70.4  & 58.4 0.8  & 69.93.0  & 75.32.5  & 76.00.7  & 75.91.6  \\
GraphCL             & 69.680.67 & 73.870.66 & 62.400.57 & 60.530.88 & 75.992.65 & 69.802.66 & 78.471.22 & 75.381.44 \\
    JOAO                & 70.220.98 & 74.980.29 & 62.940.48 & 59.970.79 & 81.322.49 & 71.661.43 & 76.731.23 & 77.340.48 \\
GraphLog \\ 
\midrule
GraphMAE        & 72.040.66 & 75.510.61 & 64.060.29 & 60.251.13 & 82.321.15 & 76.262.39 & 77.190.95 & 83.130.86 \\
\bottomrule[1.2pt]
\end{tabular}
\end{table*}





\subsubsection{Transfer Learning}\hfill



\vpara{Setup.} To evaluate the transferability of our proposed method, we test the performance on transfer learning on molecular property prediction in chemistry, following the setting of ~\cite{hu2020strategies,you2020graph,you2021graph}. The model is first pre-trained in  2 million unlabeled molecules sampled from the ZINC15 database~\cite{sterling2015zinc}, and then finetuned in 8 binary classification benchmark datasets contained in MoleculeNet~\cite{wu2018moleculenet}. The downstream datasets are splitted by scaffold-split to mimic real-world use case.  Atom number and chirality tag are input as node features and bond type and direction are regarded as edge features. In our experiments, we only consider reconstructing node features.

For evaluation protocal,  we run experiments for 10 times and report the mean and standard deviation of ROC-AUC scores(\%). Following the default setting in ~\cite{hu2020strategies},  we adopt a 5-layer GIN as backbone of the encoder, and a single-layer GIN as the decoder.


\vpara{Experiment results.}
We evaluate the performance of \model against SOTA self-supervised pre-training methods for GNNs, AttrMasking~\cite{hu2020strategies} and ContextPred~\cite{hu2020strategies} , and SOTA contrastive learning methods, GraphCL~\cite{you2020graph}, JOAO~\cite{you2021graph}, and Grover~\cite{rong2020self}. We also report the result of another generating pre-training method, GPT-GNN~\cite{hu2020gpt}. Table \ref{tab:graph_clf} shows that the performance on downstream molecular property prediction tasks is comparable to SOTA self-supervised methods, in which our model has a small edge over previous best results in 4 datasets. 


To summarize, our \model achieves the competitive performance on node, graph classification, and transfer learning benchmarks using a unified learning approach. and we do not devise a specialized self-supervised learning method for each task. The results in three tasks demonstrate that \model is effective and universal approach for various applications. 













\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{imgs/ppi_hidden.pdf}
    \vspace{-4mm}
    \caption{Performance on PPI using GAT with 4 attention heads, compared to other baselines. Self-supervised methods benefit much from larger model size, and \model could outperform supervised model. }
    \label{fig:ppi_hidden}
\end{figure}



\subsection{Ablation Studies}
To verify the effects of main components of our model, we further conduct a series of ablation studies. Without loss of generalization, we choose 3 datasets from node classification and 2 datasets from graph classification as the study objects.



\vpara{Effect of reconstruction criterion.} We study the influence of reconstruction criterion, and Table \ref{tab:ablation} shows the results of MSE and our proposed SCE loss function. Node features are normalized in preprocessing. This helps reconstruction and improves performance in downstream evaluation. The conclusion from Table \ref{tab:ablation} varies between node classification and graph classification.
Generally, input features in node classification lie in continuous space, containing more discriminative information.
The results manifest that SCE has significant advantage over MSE as replacing MSE with SCE brings 4.1\%  15.6\% absolute gain.
In graph classification, pre-training with either MSE or SCE as criterion improves accuracy. The input features typically lie in discrete value space, and are one-hot encoding in these benchmarks, representing degree or node-label. Reconstructing one-hot feature is, to some extent, similar to a classification task, and MSE works in this setting of discrete node feature.  Nevertheless, SCE has a small edge over MSE, as indicated by the results.
The same may also apply to transfer learning of molecule graphs. 
This suggests that a proper criterion is the cornerstone of the success of generative graph self-supervised learning.


Figure \ref{fig:ablation} shows the influence of scaling factor .  We observe that  benefits in most cases, especially in node classification. However, in IMDB-B, it seems that a larger  value harms the performance. In our experiments, we noticed that the training error in node classification is much higher than that in graph classification. This further demonstrates that aligning continuous vectors in unit sphere is more challenging thus scaling  brings improvement. 


\vpara{Effect of decoder type.} Previous GAEs typically follow BERT and use an MLP as the decoder. In Table \ref{tab:ablation}, we compare different decoder types, including MLP/GCN/GAT for node classification, and MLP/GIN/GAT for graph classification. The re-mask strategy is not used for MLP decoder.  The type of decoder matters in the training, and using GNN decoder typically achieves better performance in downstream tasks. Compared to MLP, which reconstructs original node feature from latent representation, GNN aims at forcing the masked nodes to extract information about its original feature from neighbors' representations. One reasonable assumption is that GNN avoids the output of encoder being aligned with original features. The learned representations is expected to be discriminative in downstream classification tasks. Therefore, the encoder could learn meaningful latent representations. MLP also works in our framework, and this may attribute to the design of our criterion.

Among different GNNs, GIN performs better in graph-level tasks, and GAT is a more reasonable option for node classification. It is observed that replacing GAT with GCN causes a significant drop, especially in Cora (2.9\%) and PubMed (2.0\%). We speculate that attention mechanism plays an important role when the latent representations vary in neighbors with re-masked ones.



\begin{table}[htbp]
\caption{Ablation study of decoder type, re-mask and reconstruction criterion on node- and graph-level benchmarks.}
\label{tab:ablation}
\renewcommand\tabcolsep{4pt}








\begin{tabular}{cccccccc}
\toprule[1.2pt]
\multicolumn{2}{c}{\multirow{2}{*}{Dataset}}
                     & \multicolumn{3}{c}{Node-Level} & \phantom{} & \multicolumn{2}{c}{Graph-Level} \\
\cmidrule{3-5} \cmidrule{7-8}
                    & & Cora        & PubMed      & Arxiv  && MUTAG              & IMDB-B              \\

\midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{\small{COMP.}}}
& \model        & 84.2  & 81.1  & 71.75  && 88.19         & 75.52         \\
& \small{w/o mask} &  79.7 & 77.9 & 70.97 & & 82.58 & 74.42 \\
& \small{w/o re-mask}   & 82.7  & 80.0  & 71.61  && 86.29         & 74.42          \\
& \small{w/ MSE}                & 79.1  & 73.1  & 67.44  && 86.30         & 74.04          \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{90}{\small{Decoder}}}
& MLP                   & 82.2  & 80.4  & 71.54  && 87.16         & 73.94          \\
& GCN                   & 81.3  & 79.1  & 71.59  && 87.78         & 74.54             \\
& GIN                   & 81.8  & 80.2  & 71.41  && 88.19         & 75.52          \\
& GAT                   & 84.2  & 81.1  & 71.75  && 86.27         & 74.04           \\
\bottomrule[1.2pt]
\end{tabular}


 \end{table}



\vpara{Effect of mask and re-mask.} \textit{Mask} plays an important role in our method. \model employ two mask strategies special --- masking input feature before encoder, and \textit{re-mask} latent representations before decoder. Table \ref{tab:ablation} studies the designs. We observe a significant drop in performance if without masking input features, indicating that input masking is vital to avoid trivial solution. For the re-mask strategy, the accuracy drops by 0.1\%2.9\% if removing the operation. In such case, it behaves like training a  layer GNN and adopting the output of the -th layer as the encoding representations. Re-mask is designed for GNN decoder and could be regarded as regularization, making the self-supervisory task more challenging.



\vpara{Effect of mask ratio.} Figure \ref{fig:ablation} shows the influence of mask ratio. Similar to the observation in ~\cite{he2021masked}, in most cases, the reconstruction task is not challenging enough to learn useful features with low mask ratio (0.1), and would be too difficult if the mask ratio is too high (0.9). Different from the behavior in BERT and MAE, the optimal ratio varies in different graphs. In Cora and IMDB-B, increasing the mask ratio degrades the performance after the optimal value (0.5), while \model still works with a surprisingly high ratio (0.70.9) in PubMed, Ogbn-arxiv and MUTAG. This motivates us to think about information redundancy in graphs. Large node degree and high homogeneity may lead to heavy information redundancy, in which a missing node feature can be recovered from very few neighboring nodes with little high-level understanding of local structure and features. Masking a very high portion of node features is necessary to learn useful representations. In contrast, lower redundancy means excessively high mask ratio would make it impossible to recover features and degrade the performance. As for how to measure information redundancy, we leave it for future work. 


























\begin{figure}
    \centering
    \begin{minipage}[t]{0.235\textwidth}
        \includegraphics[width=\textwidth]{imgs/gmae_mask_ratio.pdf}
\end{minipage}
    \begin{minipage}[t]{0.235\textwidth}
        \includegraphics[width=\textwidth]{imgs/gamma_value.pdf}
\end{minipage}
    \caption{Ablation study of mask ratio and scaling factor .}
    \label{fig:ablation}
\end{figure}






}%
