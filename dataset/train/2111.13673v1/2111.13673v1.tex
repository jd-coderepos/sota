

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage[review]{cvpr}      

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{multirow}
\usepackage{booktabs}
\usepackage{amsmath,amssymb}
\usepackage{wrapfig}
\usepackage{color}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\def\cvprPaperID{1831} \def\confName{CVPR}
\def\confYear{2022}

\newcommand{\parsection}[1]{\vspace{1mm}\noindent\textbf{#1}}


\begin{document}



\title{Mask Transfiner for High-Quality Instance Segmentation}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}
\maketitle

\begin{abstract}
   Two-stage and query-based instance segmentation methods have achieved remarkable results. However, their segmented masks are still very coarse.
   In this paper, we present a new Mask Transfiner for high-quality and efficient instance segmentation. Instead of computing on regular dense tensors, our {\em Mask Transfiner} decomposes and represents the image regions as a quadtree, operating only in detected error-prone regions and self-correcting errors in parallel. While these sparse pixels consist of only a small proportion of the total pixels, they are critical  to the overall performance, both in mask quality and computational efficiency.  In contrast to traditional convolution and transformers, 
   Mask Transfiner is linear in running time and memory in a small number of detected pixels, thanks to the sparse and hierarchical design while achieving higher mask quality. 
   Extensive experiments demonstrate that Mask Transfiner outperforms current instance segmentation methods on COCO, LVIS and Cityscapes datasets, significantly improving 
   both one/two-stage and query-based segmentation frameworks.
\end{abstract}

\section{Introduction}
\label{sec:intro}


State-of-the-art instance segmentation methods, including the Mask R-CNN family~\cite{he2017mask,liu2018path,huang2019mask,ChengWHL20,ke2021bcnet} and the recently proposed query-based instance segmentation methods~\cite{dong2021solq,QueryInst,hu2021ISTR}, mainly follow a multi-stage paradigm, where a high-accuracy object detector is employed to extract the RoI feature or localize the objects in the given image. Although these methods have demonstrated leading detection performance on the COCO challenge~\cite{lin2014microsoft}, the problem of efficiently predicting highly accurate segmentation masks has remained elusive.

As shown in Table~\ref{tab:gap} there is still a significant gap between their object detection and segmentation performance. This indicates a large room of improvement in the segmentation mask quality. In Figure~\ref{fig:fig1}, the predicted masks by xxx methods are very coarse and in low resolution exhibiting many errors along the object boundaries. [challenges of accurate mask prediction. requires high resolution, but this is computationally costly] 



We propose Mask Transfiner, an efficient transformer-based method for high-quality instance segmentation. Our approach first identifies error-prone regions, where refinement is necessary. This allows us to only process small parts of the hight resolution feature maps .....
We design a method, employing a quadtree ....
















\begin{figure*}[!t]
	\centering
\includegraphics[width=0.8\linewidth]{figures/fig1_teaser.pdf}
	\vspace{-0.15in}
	\caption{Instance Segmentation on \textbf{COCO}~\cite{lin2014microsoft} validation set by a) Mask R-CNN~\cite{he2017mask}, b) BMask R-CNN~\cite{liu2018path}, c) SOLQ~\cite{huang2019mask}, d) PointRend~\cite{kirillov2020pointrend}, g) Mask Transfiner (Ours) using R50-FPN as backbone. }
\label{fig:fig1}
    \vspace{-0.1in}
\end{figure*}





\begin{table}[!t]
	\caption{The performance gap between object detection and segmentation for instance segmentation models on COCO \textit{test-dev} set using ResNet-50-FPN as backbone.}
	\vspace{-0.1in}
	\centering
	\resizebox{0.95\linewidth}{!}{
		\begin{tabular}{c | c | c | c | c }
			\toprule
			Method & Type & AP & AP & Performance Gap \\
			\midrule
			Mask R-CNN~\cite{he2017mask} & two-stage & 37.5 & 41.3 & 3.8 \\
			HTC~\cite{chen2019hybrid} & two-stage & 38.4 & 43.6 & 5.2 \\
			Mask Transfiner (Ours)  & two-stage  & - & - & - \\
			\midrule
			SOLQ~\cite{dong2021solq}  & query-based & 39.7 & 47.8 & 8.1 \\
			ISTR~\cite{hu2021ISTR}  & query-based & 38.6 & 46.8 & 8.2 \\
			Mask Transfiner (Ours) & query-based  & - & - & - \\
			\bottomrule
		\end{tabular}
	}
	\vspace{-0.15in}
	\label{tab:gap}
\end{table}





\section{Related Work}
\label{sec:formatting}

\parsection{Instance Segmentation}
Two-stage instance segmentation methods~\cite{he2017mask,cai2018cascade,chen2019hybrid,chen2019tensormask,liang2020polytransform,ChengWHL20,ke2021bcnet} achieve state-of-the-art performance by first detecting bounding boxes and then performing segmentation in each RoI region.
FCIS~\cite{li2017fully} introduces the position-sensitive score maps within instance proposals for mask segmentation. 
Mask R-CNN~\cite{he2017mask} extends Faster R-CNN~\cite{ren2015faster} with a FCN branch to segment objects in the detected boxes.
The follow-up works~\cite{chen2018masklab,ChengWHL20} also contribute to the family of Mask R-CNN models, such as integrating multi-level feature of FPN~\cite{liu2018path}, mitigating the misalignment between mask quality and score~\cite{huang2019mask}, and making it occlusion aware using bi-layer structure modeling~\cite{ke2021bcnet}. One-stage methods~\cite{chen2020blendmask, chen2019tensormask, kuo2019shapemask}, such as 
PolarMask~\cite{xie2019polarmask}, CenterMask~\cite{lee2019centermask}, YOLOACT~\cite{bolya2019yolact}, and SOLO~\cite{wang2019solo,wang2020solov2} remove the proposal generation and feature re-pooling steps, and achieve comparable results with simpler procedures and higher  efficiency.

Query-based instance segmentation methods~\cite{QueryInst,dong2021solq,wang2020end,hu2021ISTR},
which was inspired by DETR~\cite{carion2020end} and have emerged very recently, treat segmentation as a set prediction problem.
These methods use queries to represent the interested objects and train the queries to jointly perform classification, detection and mask regression with recurrent multi-stage refinement. In~\cite{dong2021solq,hu2021ISTR}, the raw spatial object masks are compressed as encoding vectors using DCT or PCA algorithms, while QueryInst~\cite{QueryInst} adopts dynamic mask heads with mask information flow. However, the large gaps between the detection and segmentation performance in Table~\ref{tab:gap} reveals that the mask quality produced by these query-based methods are still unsatisfactory. In contrast to the above methods, Mask Transfiner is targeted for high-quality instance segmentation with the efficient Mask Transfiner, which is applicable to  and effective in both the two-stage and query-based frameworks. 


\parsection{Refinement for Instance Segmentation} Most existing works on instance segmentation refinement rely on specially designed convolutional networks~\cite{tang2021look,refinemask} or MLPs~\cite{kirillov2020pointrend}. PointRend~\cite{kirillov2020pointrend} samples feature points with low-confidence scores and refines their labels with a shared MLP, where the selected points are determined by the coarse predictions of the Mask R-CNN. RefineMask~\cite{refinemask} incorporates fine-grained features with an additional semantic head and fuses them with instance features as the segmentation guidance. 
The post-processing method BPR~\cite{tang2021look} extracts small boundary patches of images and initial masks as input and use~\cite{wang2020deep} for segmentation. Notably some methods~\cite{takikawa2019gated,wang2020deep,yuan2020segfix,cheng2020cascadepsp} focus on refining semantic segmentation details. However, it is much more challenging for instance segmentation due to the more complex segmentation setting, with varying number of objects per image and the requirement of delineating similar and overlapping objects.   

Compared to these refinement methods, Mask Transfiner is an end-to-end high-quality instance segmentation method, using the Mask Transfiner for self-correcting errors, where the regions in need of refinement are predicted using a lightweight FCN trained by computed ground truth, instead of sampling based on initial mask scores~\cite{kirillov2020pointrend}. Different from the MLP in~\cite{kirillov2020pointrend}, the sequential and hierarchical input representation and multi-head attention enable Mask Transfiner to efficiently take non-local sparse feature points as input queries. Such representation and attention mechanism are more powerful.



\parsection{Sparse Quadtree Structure}
Utilizing sparsity can increase efficiency in processing high-resolution inputs without compromising quality~\cite{liu2015sparse}. Sparse convolutions were used~\cite{graham2014spatially} on 2D images for handwriting recognition, and on videos for recognizing the actions of space-time objects~\cite{graham2015sparse}. To handle the dense volumetric tensors of 3D data, Octnet~\cite{riegler2017octnet} uses octree and grid structures, which constrain CNN operations to the interior volume of 3D shapes or only the surface voxels~\cite{wang2017cnn}. QCNN~\cite{qcnn} uses quadtree CNNs to process sparse data such as images of handwriting and strokes. No relevant research however has been conducted on detail-preserving instance segmentation and building transformers based on a quadtree, where typically error-prone pixels regions are sparse following a hierarchical distribution while requiring high-resolution final predictions.



\section{Our Method: Mask Transfiner}


\subsection{Overview}



We propose an approach to efficiently tackle high-quality instance segmentation. The overall architecture of Mask Transfiner is depicted in Figure~\ref{fig:model}. From the base object detection network, \eg Mask R-CNN~\cite{he2017mask}, we employ a multi-scale deep feature pyramid. The object detection head then predicts bounding boxes as instance proposals. This component also generates a coarse initial mask prediction at low resolution. Given this input data, our aim is to design a module capable of predicting highly accurate instance segmentation masks.



Since much of the segmentation errors are attributed to the loss of spatial resolution, we first define such \emph{incoherent regions} and analyze their properties in Section~\ref{sec:incoherent}. To identify and refine incoherent regions in multiple scales, we employ a quadtree, discussed in Section~\ref{quadtree_refinement}. The lightweight incoherent region detector takes as input the coarse initial mask alongside the multi-scale features, and predicts the incoherent regions for each scale in a cascaded manner. This allows ours Mask Transfiner to save huge computational and memory burdens, since only a small part of the high-resolution image features are processed by the refinement network itself.
Our refinement transformer, detailed in Section~\ref{sec:architecture}, operates in the detected incoherent regions. Since it operates on feature points on the constructed quadtree, and not in a uniform grid, we design a transformer architecture which jointly processes all incoherent nodes in all levels of the quadtree. 
Finally, in Section~\ref{refinement} we present the training strategy of Mask Transfiner along with the implementation details.





\begin{figure}[!h]
	\centering
\includegraphics[width=1.0\linewidth]{figures/incoherent4.pdf}
	\caption{\textbf{Illustration on incoherent regions.}}
\label{fig:incoherent}
	\vspace{-0.2in}
\end{figure}

\subsection{Incoherent Regions: Definition and Analysis}
\label{sec:incoherent}
Much of the segmentation errors produced by existing instance segmentation methods~\cite{he2017mask,dong2021solq} are due to the loss of spatial resolution since the mask prediction itself is performed at a coarse feature scale.
Despite its efficiency, low spatial resolution makes it challenging to predict accurate object boundaries, due to the loss of high-frequency details. In this section, we first define \emph{incoherent regions}, where mask information is lost due to reduced spatial resolution. Then, by analyzing their properties, we observe that a large portion of the segmentation errors are indeed located in these regions. 





\parsection{Definition of Incoherent Regions}
To identify incoherent regions, we simulate the loss of information due to downsampling in the network by also downsampling the mask itself. Specifically, information is lost in regions where the mask cannot be correctly reconstructed by a subsequent upsampling step,  as illustrated in Figure~\ref{fig:incoherent}. Formally, let  be a binary ground-truth instance mask of an object at scale level . The resolution at each scale level differs by a factor of 2, where  is the finest and  is the coarsest scale. We denote  nearest neighbor down and upsampling by  and  respectively. The incoherent region at scale  is then the binary mask achieved as, 

Here,  denotes the logical `exclusive or' operation and  is  downsampling by performing the logical `or' operation in each  neighborhood. A pixel  is thus incoherent  if the original mask value  differs from its reconstruction in at least one pixel in the finer scale level.
Intuitively, incoherent regions are mostly strewn along object instance boundaries or high-frequency regions, consisting of points with missing or extra predicted wrong labels by coarse masks.  We provide the visualizations of them in the Supp. file, which are sparsely and non-contiguously distributed on a typical image.

\vspace{-0.06in}
\begin{table}[!h]
	\caption{Experimental analysis of the incoherent regions on COCO \emph{val} set. Percent denotes the area ratio of incoherent regions in the object bounding boxes. Recall is the ratio for all wrongly predicted pixels per object. Acc is the accuracy rate for coarse mask predictions inside incoherent regions. AP is measured by using coarse mask predictions for whole object regions while AP only fills the incoherent regions with the ground truth labels.}
	\vspace{-0.1in}
	\centering
	\resizebox{1.0\linewidth}{!}{
		\begin{tabular}{c | c | c | c | c | c}
			\toprule
			Percent & Recall & Acc & AP & AP & AP \\
			\midrule
			14\% & 43\% & 56\% & 51.0 & 35.5 & 37.8 \\
			\bottomrule
		\end{tabular}
	}
	\vspace{-0.2in}
	\label{tab:property}
\end{table}

\parsection{Properties of Incoherent Regions}
In Table~\ref{tab:property}, we provide an analysis of the incoherent regions defined above. It shows that a large portion of prediction errors are concentrated in these incoherent regions, occupying 43 of all wrongly predicted pixels, while only taking 14 to the corresponding bounding box areas. The accuracy of the coarse mask prediction in incoherent regions is 56. By fixing the bounding boxes detector, we conduct an oracle study to fill all these incoherent regions for each object with ground truth labels, while leaving the remaining parts as initial mask predictions. Compared to using initial mask predictions in the incoherent regions, the performance surges from 35.5 AP to 51.0 AP, indeed justifying they are critical for improving final performance.


\begin{figure}[!t]
	\centering
\includegraphics[width=1.0\linewidth]{figures/incoherence_detect5.pdf}
	\caption{Detection on multi-level incoherent regions (Yellow). The higher-resolution detection is under the guidance of the predicted incoherent mask up-sampled from lower level.}
\label{fig:incoherent_detect}
	\vspace{-0.2in}
\end{figure}

\subsection{Quadtree for Mask Refinement}
\label{quadtree_refinement}


In this section, we describe our approach for detecting and refining incoherent regions in the image.
Our approach is based on the idea of iteratively detecting and dividing the incoherent regions in each feature scale. By only splitting the identified incoherent pixels for further refinement, our approach efficiently processes high-resolution features by only focusing on the important regions. To formalize our approach, we employ a quadtree structure to first identify incoherent regions across scales. We then predict the refined segmentation labels for all incoherent nodes in the quadtree, using our network detailed in Section~\ref{sec:architecture}. Finally, our quadtree is employed to fuse the new predictions from multiple scales by propagating the refined mask probabilities from coarse to finer scales. 






\begin{figure*}[!t]
    \vspace{-0.2in}
	\centering
\includegraphics[width=0.9\linewidth]{figures/framework14.pdf}
\caption{The framework of Mask Transfiner. On the point quadtree, yellow point grids denote detected incoherent regions requiring further subdivision while grey ones are leaf (coherent) nodes. The incoherent query sequence is composed of points across three levels of the quadtree for parallel refinement. The encoder of Transfiner consists of point encoder and sequence encoder, while the pixel decoder is on top of each self-attended query pixel and output their final labels. The incoherence detector is detailed in Figure~\ref{fig:incoherent_detect}.}
    \label{fig:model}
    \vspace{-0.2in}
\end{figure*}

\parsection{Detection of Incoherent Regions}
Figure~\ref{fig:incoherent_detect} depicts the design of our lightweight module to efficiently detect incoherent regions on a multi-scale feature pyramid. Following a cascaded design, we first concatenate the smallest features and coarse object mask predictions as input, and use a simple fully convolutional network followed by a binary classifier to predict the coarsest incoherence masks. Then, the detected lower-resolution masks are upsampled and fused with the larger-resolution feature in neighboring level to guide the finer incoherence predictions, where only single 11 convolution layer is employed. During training, we enforce both the groundtruth incoherent points in lower-level generated by Eq.~\ref{eq1} within the coverage of their parent points in higher-level feature map. 


\parsection{Quadtree Definition and Construction} We define a \emph{point quadtree} for decomposing the detected incoherent regions. Our structure is illustrated in Figure~\ref{fig:incoherent_detect}, where one yellow point in higher-level of FPN feature (such as feature resolution 28 28) has four quadrant points in its neighboring lower-level FPN feature map (such as resolution 5656). These are all feature points but with different granularities because they are on different pyramid levels. In contrast to the conventional quadtree `cells' used in computer graphics, where a quadtree `cell' can have multiple points, the subdivision unit for our point quadtree is always on a point, with the division of each point decided by the detected incoherent values and the threshold for the binary classifier.

Based on the detected incoherent points, we construct a multi-level hierarchical quadtree, beginning from using the detected points in the highest-level feature map as root nodes. These root nodes are selected for subdividing to their four quadrants on the lower-level feature map, with larger resolution and more local details. Note that at the fine level, only the quadrant points detected as incoherent could make a further break down and the expansion of incoherent tree nodes is restricted in regions corresponding to the incoherent predictions at the previous coarse level. 

\parsection{QuadTree Refinement}
We refine the mask predictions of the incoherent nodes of the quadtree using a transformer-based architecture. Our design is described in Sec.~\ref{sec:architecture}. It directly operates on the nodes of the quadtree, jointly providing refined mask probabilities at each incoherent node.

\parsection{QuadTree Propagation}
Given the refined mask predictions, we design a hierarchical mask propagation scheme that exploits our quadtree structure. Given the initial coarse masks predictions in low-resolution, Mask Transfiner first corrects the points labels belong to the root level of the quadtree, and then propagates these corrected point labels to their corresponding four quadrants in neighboring finer level by nearest neighbor interpolation. The process of labels correction is recursively conducted on the incoherent nodes in a level-wise manner until reaching the finest quadtree level. Comparing to only correcting the labels of finest leaf nodes on the quadtree, it enlarges the refinement areas by also propagating refinement labeled to leaf nodes in intermediate tree levels and we validate its effectiveness in Table~\ref{tab:prop}.

\subsection{Mask Transfiner Architecture}
\label{sec:architecture}




In this section, we describe the architecure for refinement network xxx.
After constructing the quadtree consisting of detected incoherent points, we design the refinement network, Mask Transfiner, that can refine the predictions of the incoherent quadtree nodes. The principles for our design of Mask Transfiner are: \textbf{1)} The incoherent feature points are sparsely distributed along the high-frequency image regions but not spatially contiguous, thus not suitable for standard convolutional network operating on uniform grids. \textbf{2)} The accurate prediction of these ambiguous feature points requires both fine-grained deep features and coarse semantic information, which needs a powerful model to sufficiently relate points and their surrounding context, including both 2D spatial and cross-level neighboring points. Thus, visual transformers~\cite{vaswani2017attention} with sequential input and powerful multi-head attention, is a natural choice for our network design for refining object masks within detected incoherent regions.
Compared to the MLP in~\cite{kirillov2020pointrend}, the self-attention of transformer is a natural fit for the quadtree structure, because of the effective fusion of the multi-level feature information and the explicit modeling of pairwise point relations. 

Figure~\ref{fig:model} shows the overall architecture of Mask Transfiner. Based on the hierarchical FPN~\cite{lin2017feature}, instance segmentation is tackled in a multi-stage and coarse-to-fine manner. Instead of using single-level FPN feature for each object~\cite{he2017mask}, Mask Transfiner takes as input sequence the sparsely detected feature points in incoherent image regions across the object feature pyramid levels, and outputs the corresponding segmentation labels.

\parsection{RoI Feature Pyramid} Given an input image, the CNN backbone network equipped with FPN first extracts hierarchical feature maps for downstream processing, where we utilize feature levels from  to  by further extending , the conventional highest resolution feature map of FPN. Following~\cite{he2017mask, dong2021solq}, the object detector is in charge of predicting bounding boxes as instance proposals. Then the RoI feature pyramid is built by extracting RoI features across 3 different levels  of FPN with increasing square sizes , where the beginning level  is computed as , where ,  and  are the RoI width and height. The highest-level RoI feature has smallest RoI size but containing more contextual and semantic information, while the lower-level feature maps resolving more local details with larger sizes.



\subsubsection{Transfiner Encoder and Decoder} 
Now that the quadtree has been constructed, to form a feature sequence that can be fed into the transformer encoder, we fetch and concatenate all incoherent features points from all three levels of the quadtree, resulting in a 2D feature map size , where  is the total number of selected points for each refined object, and  is the channel dimension. Notably,  due to  sparsity, where the order of points in the formed sequence does not matter because the transformer architecture is permutation-invariant. In contrast to standard transformer encoder, the encoder of Transfiner consists of two parts: point encoder and sequence encoder.

\parsection{Point Encoder} 
To enrich the incoherent points feature, the point encoder of Mask Transfiner encodes each point by the following four types of features: \textbf{1)}~The fined-grained features extracted from corresponding locations and levels of the FPN feature. \textbf{2)}~The coarse segmentation features from initial segmentation predictions to provide region-specific and semantic information, because one point could be both the foreground of one instance and the background for the others. Specifically, we use the coarse output of a lightweight 1414 resolution mask head. \textbf{3)}~The relative positional encoding features in each RoI to encode the 2D spatial distances (horizontal and vertical) between points since segmentation is position-sensitive. \textbf{4)}~The surrounding context for each point with local details to enrich the point information, where an incoherent point is the concatenation of the 33 neighboring points and compressed by a fully connected layer to original dimension. Intuitively, this is to let the each point augmented by neighboring points of the local patches, so that the colour consistency and contrast inside the small patch could also be aggregated. In Figure~\ref{fig:model}, the fine-grained features, coarse segmentation features and context feature are first concatenated and fused by a fully connected layer to original feature dimension. Position embeddings are then added to the fused feature to retain positional information. We validate the effectiveness for each type of point feature in experiment sections. 

\parsection{Sequence Encoder and Pixel Decoder} 
Then, the sequence encoder of Mask Transfiner models the similarities among all encoded points in the input query sequence, even these points are on different quadtree levels or in a long-range spatial distance. Each sequence encoder layer has a standard structure formed by a multi-head self-attention module and a fully connected feed forward network (FFN). To equip the incoherent points sequence with adequate positive and negative references, we also use all feature points from the highest FPN level with small size 1414. Different from the standard transformer decoder~\cite{carion2020end} with deep attention layers, the pixel decoder in Mask Transfiner is a small two-layer MLP, which decodes on top of each self-attended query pixel and shares weights across them to predict the final class labels.

\subsubsection{Training of Mask Transfiner}
\label{refinement} 

Based on the constructed quadtree, we develop flexible and adaptive schemes for training of Mask Transfiner, where all incoherent points on the quadtree are predicted in parallel. Since the query sequence consists of multi-scale features points across tree levels with different granularities, the incoherent points from higher levels with richer semantic context can guide the prediction of the points in lower levels with more local details. 

\parsection{Auxiliary Supervision}
We also design an auxiliary instance boundaries loss during training, which is to directly enhance the lower-level FPN feature representation with more fine-grained details. Taking  FPN feature (stride 4) as input, a light-weight FCN network with four 33 convolutional layers is employed to predict the instance boundaries from all foreground objects. This direct boundary supervision also supplements the lost details in the mask compression for detail-preserving instance segmentation.

\parsection{Multi-task Losses} The whole Mask Transfiner framework can be trained in an end-to-end manner defined by a multi-task loss as,


\vspace{-0.2in}

where~ denotes the Mask Transfiner refinement with L loss,  and  are respectively the BCE loss for auxiliary instance boundary and incoherent regions predictions.~ supervises both the object positional prediction and the category classification, which is  borrowed from the Faster R-CNN~\cite{tian2019fcos} or DETR detector.~ is the initial coarse segmentation loss used by~\cite{he2017mask}.  are hyper-parameter weights to balance the loss functions, which are tuned to be  respectively on the validation set. 

\parsection{Implementation Details}
Mask Transfiner is implemented on both the two-stage detector Faster R-CNN~\cite{ren2015faster} and query-based detector~\cite{carion2020end}. 
We design a 3-level quadtree and follow the hyper-parameters and training schedules of Mask R-CNN implemented in Detectron2~\cite{wu2019detectron2} for the backbone and coarse mask head, except the newly designed refinement modules.
The Mask Transfiner encoder consists of three standard transformer layers. Each layer has four attention heads with input feature dimension at 256.
During training, we randomly permute the order of the incoherent points and select 300 of them, so as to maintain the  same sequence length for each object for batch efficiency. 
To make the detection more robust, we adopt jittering operations along the boundaries of the ground truth incoherent regions because in our case, the recall rate of the detection to cover all the incoherent regions play a more critical role in influencing final performance.
In our ablation study, R-50-FPN~\cite{he2016deep} and Faster R-CNN are adopted, which are trained with 1 learning schedule.
For COCO leaderboard comparison, we adopt the scale-jitter with shorter image side randomly sampled from [640, 800], following 3 training schedule in~\cite{lee2019centermask,chen2019tensormask,ke2021bcnet}.
For Cityscapes evaluation, we train the models on the fine annotations of the train set with 64 epochs, following PointRend~\cite{kirillov2020pointrend}.
The incoherent regions detector is a simple FCN network consists of four 33 convolutional layers.
More details are provided in the supplemental file.


\section{Experiments}

\subsection{Experimental Setup}

\parsection{COCO}
We perform experiments on COCO dataset~\cite{lin2014microsoft}, where we train our networks on 2017{\it train} (115k images) and evaluate our results on both 2017{\it val} and 2017{\it test-dev}, using the standard AP metrics and the recently proposed boundary IoU metrics~\cite{cheng2021boundary}. Note that the result AP for boundary IoU is a measure focusing on boundary quality. Following~\cite{kirillov2020pointrend}, we also report AP, which evaluates the \textit{val} set of COCO categories using the same models trained on COCO, but with significantly higher-quality LVIS annotations~\cite{gupta2019lvis}, which can better reveal improvements in mask quality.

\parsection{Cityscapes} We also report the results of Mask Transfiner on Cityscapes~\cite{cordts2016cityscapes}, a real-world high-quality instance segmentation dataset that contains 2975, 500, 1525 images with resolution of 2048×1024 for training, validation and test respectively. Cityscapes focus on self-driving scenes with eight classes categories for instance segmentation task, such as car, person, bicycle, etc.

\parsection{LVIS} We further train and evaluate Mask Transfiner on the long-tail instance segmentation dataset LVIS~\cite{gupta2019lvis}, which consists of 1203 categories with more than 2 million high-quality instance mask annotations. We follow the standard practice, using 100k, 20k, 20k images for training, validation and testing respectively.


\subsection{Ablation Experiments}

We conduct detailed ablation studies on COCO validation set, where we investigate the effect of the proposed incoherent regions and individual components of Mask Transfiner.

\parsection{Effect of the Incoherent Regions} 
Table~\ref{tab:property} presents
an analysis on the properties of incoherent regions described in Section~\ref{sec:incoherent},  which reveals they are critical to the final segmentation performance.  Table~\ref{tab:effect_incoherent} presents the results of ablation experiments to validate the effectiveness of the detected incoherent regions, by replacing the refinement regions with full RoIs and detected object boundary regions. Here, the boundary regions are pixels within two-pixel Euclidean distance to the detected object mask contours on all three levels of the object feature pyramid. The object boundary detector~\cite{ke2021bcnet} is used. Due to  memory limitation, the full RoIs only uses output size 2828. The comparison shows the advantage of incoherent regions, with 1.8 AP and 0.7 AP improvements over the use of full RoIs and detected boundary regions respectively.

To study the influence of incoherent regions on different pyramid levels, in Table~\ref{tab:effect_incoherent}, we also perform ablation experiments by removing the refinement regions of the Mask Transfiner in a level-wise order. We find that all three levels are beneficial to the final performance, while L contributes most with 0.8 AP increase, where L denotes the root level of Mask Transfiner with the smallest feature size.

\begin{table}[!h]
	\caption{Effect of the incoherent regions on COCO \textit{val} set. AP is evaluated Boundary IoU~\cite{cheng2021boundary} while AP uses LVIS annotations.}
	\vspace{-0.1in}
	\centering
	\resizebox{0.9\linewidth}{!}{
		\begin{tabular}{c | c | c | c | c}
			\toprule
			Region Type & AP & AP & AP & AP  \\
			\midrule
			Full RoIs (28  28) & 35.5 & 21.4 & 38.3 & 59.5 \\
			Boundary regions & 36.6 & 23.8 & 40.1 & 60.2 \\
			Incoherent regions & \textbf{37.3} & \textbf{24.2} & \textbf{40.5} & \textbf{60.7} \\
			\midrule
			Incoherent regions (\textbf{w/o} L) & 36.5 & 23.5 & 39.8 & 59.7 \\
			Incoherent regions (\textbf{w/o} L) & 36.8 & 23.8 & 40.2 & 60.1 \\
			Incoherent regions (\textbf{w/o} L) & 36.7 & 23.6 & 40.0 & 59.9 \\
			\bottomrule
		\end{tabular}
	}
	\vspace{-0.1in}
	\label{tab:effect_incoherent}
\end{table}

\begin{table}[!h]
	\caption{Effect of the lower-level masks guidance in detecting incoherent regions on COCO \textit{val}. AP and AP are final segmentation performance.}
	\vspace{-0.1in}
	\centering
	\resizebox{0.8\linewidth}{!}{
		\begin{tabular}{c | c | c | c | c }
			\toprule
		    Lower-level Guidance & Acc & Recall & AP & AP   \\
			\midrule
			 & 79 & 73 & 36.6 & 23.7 \\
			\checkmark & \textbf{84} & \textbf{86} & \textbf{37.3} & \textbf{24.2} \\
			\bottomrule
		\end{tabular}
	}
	\vspace{-0.1in}
	\label{tab:guidance}
\end{table}

\begin{table}[!h]
	\caption{Effectiveness of the point encoding on COCO \textit{val} set.}
	\vspace{-0.1in}
	\centering
	\resizebox{0.9\linewidth}{!}{
		\begin{tabular}{c | c | c | c | c | c | c | c }
			\toprule
			Fine & Coarse & Pos. & Surr. & AP & AP & AP & AP\\
			\midrule
			\checkmark & & &  & 33.8 & 20.1 & 37.0 & 53.8 \\
			\checkmark & \checkmark & & & 34.2 & 20.4 & 37.3 & 54.3 \\
			\checkmark & \checkmark & \checkmark & & 36.8 & 23.9 & 40.1 & 60.1 \\
			\checkmark & \checkmark & \checkmark  & \checkmark  & \textbf{37.3} & \textbf{24.2} & \textbf{40.5} & \textbf{60.7} \\
			\bottomrule
		\end{tabular}
	}
	\vspace{-0.2in}
	\label{tab:component}
\end{table}

\begin{table*}[!t]
	\begin{minipage}[t]{0.32\linewidth}
		\caption{Ablation on different number of refinement stages on COCO \textit{val} set using ResNet-50-FPN as backbone.}
		\centering
		\vspace{-0.1in}
		\resizebox{1.0\linewidth}{!}{
			\begin{tabular}{c | c | c | c | c | c | c }
				\toprule
				Stage & Output size & AP & AP & AP & AP & AP  \\
				\midrule
				0 & 2828 & 35.2 & 37.6 & 50.3 & 37.7  & 17.2 \\
\midrule
				1 & 2828 & 35.5 & 38.4 & 50.9 & 38.1  & 17.2\\
				2 & 5656 & 36.2 & 39.1 & 51.9 & 38.7  & 17.3 \\
			    3 & 112112 & \textbf{37.3} & 40.5 & 52.9 & \textbf{39.5}  & \textbf{17.5} \\
			    4 & 224224 & 37.1 & \textbf{40.7} & \textbf{53.1} & 39.3  & 17.4 \\
				\bottomrule
			\end{tabular}
		}
		\label{tab:stage}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.32\linewidth}
		\caption{Mask Transfiner vs. MLP vs. CNN on COCO \textit{val} set using ResNet-50-FPN.}
		\centering
		\vspace{-0.1in}
		\resizebox{1.0\linewidth}{!}{
			\begin{tabular}{l | c | c | c| c}
				\toprule
				Model & AP & AP & AP & AP \\
				\midrule
				CNN (full regions, 56  56) & 35.7 & 21.8 & 38.7 & 58.8 \\
				\midrule
				MLP (full regions, 56  56) & 36.1 & 23.4 & 39.2 & 59.2 \\
				MLP (PointRend~\cite{kirillov2020pointrend}, 112  112) & 36.2 & 23.1 & 39.1 & 59.0 \\
				MLP (incoherent regions) & 36.4 & 23.7 & 40.0 & 59.8 \\
				\midrule
				Mask Transfiner (D = 3, H = 4) & 37.3 & 24.2 & 40.5 & 60.7 \\
				Mask Transfiner (D = 3, H = 8) & 37.1 & 24.1 & 40.2 & 60.8 \\
				Mask Transfiner (D = 6, H = 4) & 37.4 & 24.4 & 40.6 & 60.9 \\
				\bottomrule
			\end{tabular}
		}
		\label{tab:struct_compare}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.32\linewidth}
		\caption{Efficacy of Mask Transfiner compared to standard attention models on COCO \textit{val} set using ResNet-50-FPN.}
		\centering
		\vspace{-0.1in}
		\resizebox{1.0\linewidth}{!}{
			\begin{tabular}{l | c | c | c}
				\toprule
				Model & AP & FLOPs (G) & Memory (M) \\
				\midrule
				Non-local Attention~\cite{wang2018non} (112112) & 36.3 & 24.6 & 8347 \\
				Non-local Attention~\cite{wang2018non} (224224) & 36.6 & 80.2 & 18091 \\
		        \midrule
				Transformer~\cite{carion2020end} (2828) & 36.1 & 37.2 & 4368 \\
				Transformer~\cite{carion2020end} (5656) & 36.5 & 68.3 & 17359 \\
				\midrule
				Mask Transfiner (112112) & 37.3 & 16.8 & 2316 \\
				Mask Transfiner (224224) & 37.1 & 38.1 & 4871 \\
				\bottomrule
			\end{tabular}
		}
		\label{tab:trans_compare}
	\end{minipage}
	\vspace{-0.1in}
\end{table*}


\parsection{Ablation on the Incoherent Regions Detector} 
We evaluate the performance of the light-weight incoherent region detector by computing its recall and accuracy rates. In Table~\ref{tab:guidance}, with the guidance of the predicted incoherent mask up-sampled from lower level (Figure~\ref{fig:incoherent_detect}), the mask recall rate of detected incoherent regions has an obvious improvement from 74 to 86, and the accuracy rate also increases from 79 to 84. Note that recall rate is more important here to cover all the error-prone regions for further refinements.

\parsection{Effect of Incoherent Points Encoding} 
We validate the effect of the four feature components in incoherent points encoding. In Table~\ref{tab:component}, comparing to only using fine-grained feature, the coarse segmentation features with semantic information brings a gain of 0.4 point AP. The positional encoding feature has a large influence on model performance by significantly improving 2.6 points on AP and 3.5 points on AP respectively. The positional encoding for incoherent points are crucial, because transformer architecture is permutation-invariant and the segmentation task is position-sensitive. The surrounding context feature further promotes the segmentation results from 36.8 AP to 37.3 AP by aggregating local neighboring details.



\parsection{Effect of Multi-stage Hierarchical Refinement} 
In Table~\ref{tab:stage}, we study the influence on hierarchical refinement stages by constructing the quadtree structure in our Mask Transfiner with different depths. Stage 0 denotes the baseline using coarse head mask prediction \textbf{w/o} refinement steps. The output size grows twice larger than its preceding stage. By varying the output sizes from 2828 to 224224, the mask AP increases from 38.4 to 40.7, which reveals that models with more stages and larger output sizes for an object indeed brings more gain to segmentation performance. The large objects benefits most from the increasing sizes by improving 2.8 point in AP. We further find that the performance saturates when the output size is larger than 112112 but the 3-stage one obviously had lower computational cost and better efficiency. 

\parsection{Mask Transfiner vs MLP vs CNN}
We compare different popular choices of the refinement networks, including the MLP and CNN structures. MLP is implemented with three hidden layers with 256 channels following~\cite{kirillov2020pointrend}, while CNN is a fully convolutional network with four convolution layers with 33 kernel~\cite{he2017mask}.
Note that for full refinement regions, CNN and MLP are limited to the ROI size 56  56 due to memory limitation, and CNN is not suitable for incoherent regions because uniform grids are required.
In Table~\ref{tab:struct_compare}, our Mask Transfiner outperforms the MLP by 0.9 AP, benefiting from the non-local pixel-wise relation modeling and information propagation, where we use the same incoherent regions on all three quadtree levels for fair comparison.
Besides, we also investigate the influence of layer depth  and width  of Mask Transfiner and find that deeper and wider attention layers only lead to minor performance change.


\parsection{Efficacy of Quadtree Structure} 
Table~\ref{tab:trans_compare} compares Mask Transfiner with different attention mechanisms. Compared to standard pixels relation modeling using non-local attention~\cite{wang2018non} or standard transformer~\cite{carion2020end}, Mask Transfiner not only obtains higher accuracy but also is very efficient in computation and memory consumption. For example, Mask Transfiner with multi-head attention uses 3 times less memory than the non-local attention given  same output size, due to the small number of incoherent pixels. Compared to standard transformer operating on full RoI regions of much smaller size 5656, the quadtree subdivision and inference allows Mask Transfiner to produce a high-resolution 224224 prediction using only half of the FLOPs computation. Note that the standard transformer with output size 112112 runs out of memory in our experiments on Titan RTX.

\begin{table}[!h]
	\caption{Effect of the auxiliary semantic boundary supervision.}
	\vspace{-0.1in}
	\centering
	\resizebox{0.75\linewidth}{!}{
		\begin{tabular}{c | c | c | c | c}
			\toprule
		    Aux. Bound Loss & AP & AP & AP & AP  \\
			\midrule
			 & 37.0 & 23.8 & 39.9 & 60.3 \\
			\checkmark & \textbf{37.3} & \textbf{24.2} & \textbf{40.5} & \textbf{60.7} \\
			\bottomrule
		\end{tabular}
	}
	\vspace{-0.1in}
	\label{tab:aux}
\end{table}

\parsection{Effect of Parallel Refinement and Mask Propagation}
xxxxxx
with Multi-scale Points
xxxx

\begin{table}[!h]
	\caption{Effect of the parallel refinement and mask propagation.}
	\vspace{-0.1in}
	\centering
	\resizebox{0.5\linewidth}{!}{
		\begin{tabular}{c | c | c | c | c}
			\toprule
		    - & AP & AP & AP & AP  \\
			\midrule
			 & - & - & - & - \\
			\checkmark & \textbf{-} & \textbf{-} & \textbf{-} & \textbf{-} \\
			\bottomrule
		\end{tabular}
	}
	\vspace{-0.1in}
	\label{tab:prop}
\end{table}

\parsection{Effect of the Auxiliary Loss Supervision} 
We also evaluate the effect of auxiliary semantic boundary loss. In Table~\ref{tab:aux}, with additional semantic boundary supervision, Transfiner improves from 23.8 to 24.2 in boundary AP, which indicates the usefulness of object boundary details in lower-level high-resolution FPN feature maps in high-quality instance segmentation.


\begin{table*}[!t]
	\caption{Comparison with SOTA methods on COCO {\it test-dev} and \textit{val} set. All methods are trained on COCO \emph{train2017}. AP denotes evaluation using LVIS~\cite{gupta2019lvis} annotation and AP denotes evaluation using Boundary IoU~\cite{cheng2021boundary}.} 
	\vspace{-0.2in}
	\begin{center}{\small
			\resizebox{0.9\linewidth}{!}{
				\begin{tabular}{cc|c|c|c|cc|c|ccc}
					\hline
					& Method & Backbone & Type & AP & AP & AP & AP & AP & AP & AP \\ \hline
					\multirow{4}{*}{} 
					& HRNet~\cite{wang2020deep} & HRNetV2p-W18 & Two-stage & 35.4 & - & - & -& 18.4 & 38.0 & 46.4 \\
					& Mask R-CNN~\cite{he2017mask} & R50-FPN & Two-stage & 37.5 & 38.2 & 21.2 & 41.3 & 21.1 & 39.6 & 48.3 \\ & PointRend~\cite{kirillov2020pointrend} & R50-FPN & Two-stage & 38.1 & 39.7 & 23.5 & 41.5 & 18.8 & 40.2 & 55.4 \\
					& BMask R-CNN~\cite{ChengWHL20} & R50-FPN & Two-stage & 37.8 & 39.8 & 23.5 & 41.6 & 19.7 & 41.3 & 55.6 \\
					& RefineMask~\cite{refinemask} & R50-FPN & Two-stage & 40.2 & 41.5 & - & - & - & - & - \\
					& \textbf{Mask Transfiner} & R50-FPN & Two-stage &  - & - & - & - & - & - & -\\
					\hline
					& Mask R-CNN~\cite{he2017mask} & R101-FPN & Two-stage & 38.8 & 39.3 &  23.1 & 43.1 & 21.8 & 41.4 & 50.5 \\ & PointRend~\cite{kirillov2020pointrend} & R101-FPN & Two-stage & 39.6 & 41.4 & 25.3 & 43.3 & 19.8 & 42.6 & 57.7 \\
					& BMask R-CNN~\cite{ChengWHL20} & R101-FPN & Two-stage & 39.0 & 41.3 & 25.2  & - & 19.6 & 43.1 & 57.2 \\
					& BCNet~\cite{ke2021bcnet} & R101-FPN & Two-stage & 39.8 & - & - & 43.5 & 22.7 & 42.4 & 51.1 \\
					& HTC~\cite{chen2019hybrid} & R101-FPN & Two-stage & 39.7 & 42.5 & 25.4 & 45.9 & 21.0 & 42.2 & 53.5 \\
					& RefineMask~\cite{refinemask} & R101-FPN & Two-stage & 41.2 & 42.3 & - & - & - & - & - \\
					& \textbf{Mask Transfiner} & R101-FPN & Two-stage & - & - & - & - & - & - & -\\
					\hline
					\hline
& ISTR~\cite{hu2021ISTR}  & R50-FPN & Query-based & 38.6 & 39.5 & 23.0 & 46.8 & 22.1 & 40.4 & 50.6 \\
					& SOLQ~\cite{QueryInst} & R50-FPN & Query-based & 39.7 & 39.8 & 23.3 & 47.8 & 21.5 & 42.5 & 53.1 \\ & QueryInst~\cite{QueryInst} & R50-FPN & Query-based & 39.9 & - & 23.7 & 44.5 & 22.9 & 41.7 & 51.9 \\
					& \textbf{Mask Transfiner} & R50-FPN & Query-based & - & - & - & - & - & - & - \\
					\hline
		\end{tabular}}}
	\end{center}
	\label{table:fully}
	\vspace{-0.2in}
\end{table*}

\begin{figure*}[!t]
	\centering
\includegraphics[width=1.0\linewidth]{figures/cityscape_comp.pdf}
	\vspace{-0.15in}
	\caption{Qualitative comparison with instance segmentation methods Mask R-CNN~\cite{he2017mask}, Boundary-preserving Mask R-CNN~\cite{ChengWHL20}, PointRend~\cite{kirillov2020pointrend} on Cityscapes \textit{val} set. xxxx predicts masks with substantially higher quality than xxx. Zoom in for better view. To draw the refinement results under different stages. Using enlarge to show details. }
\label{fig:quali_comparison}
    \vspace{-0.2in}
\end{figure*}

\begin{table}[!h]
	\caption{Performance comparison on Cityscapes \textit{val} set using ResNet-50-FPN.}
	\vspace{-0.1in}
	\centering
	\resizebox{0.85\linewidth}{!}{
		\begin{tabular}{c | c | c | c | c }
			\toprule
			Method & AP & AP & AP & AP\\
			\midrule
			Mask R-CNN~\cite{he2017mask} & 33.8 & 61.5 & 11.4 & 37.4 \\
			PointRend~\cite{kirillov2020pointrend} & 35.9 & 61.8 & 16.7 & 47.2 \\
			BMask R-CNN~\cite{ChengWHL20} & 36.2 & 62.6 & 15.7 & 46.2 \\
			Panoptic-DeepLab~\cite{cheng2020panoptic} & 35.3 & 57.9 & 16.5 & 47.7 \\
			\midrule
			Mask Transfiner (Ours) & \textbf{37.2} & \textbf{64.1} & \textbf{18.0} & \textbf{49.8} \\
			\bottomrule
		\end{tabular}
	}
	\vspace{-0.1in}
	\label{tab:cityscape_comp}
\end{table}

\begin{figure}[!t]
	\centering
\includegraphics[width=1.0\linewidth]{figures/att_weight.pdf}
	\caption{\textbf{Attention Weight Visualization.}}
	\label{fig:att_vis}
	\vspace{-0.1in}
\end{figure}

\subsection{Main Results}

We compare our approach with the state-of-the-art methods on the aforementioned benchmarks COCO and Cityscapes, where Mask Transfiner outperforms all existing methods without bells and whistles, and shows efficacy to both two-stage and query-based segmentation frameworks. 
Code and models will be released upon publication.

\subsubsection{Comparison with SOTA Methods}

\parsection{COCO} Table~\ref{table:fully} compares Mask Transfiner with state-of-the-art instance segmentation methods on COCO dataset.
Transfiner achieves consistent improvement on different backbones and object detectors, demonstrating its effectiveness by outperforming both HTC~\cite{chen2019hybrid} and BCNet~\cite{ke2021bcnet} by xxx AP using Faster R-CNN, and exceeding QueryInst~\cite{QueryInst} by xx AP using query-based detector~\cite{carion2020end}. Note QueryInst consists of six-stage refinement in parallel with far more parameters to optimize.
Besides, we find that Transfiner using Faster R-CNN detector and R50-FPN with much lower object detection performance still outperforms all existing query-based instance segmentation methods in both mask and boundary AP, further validating the higher AP achieved by Transfiner is indeed contributed by the finer-grained mask quality.


\parsection{Cityscapes}
The results of Cityscapes benchmark is tabulated in Table~\ref{tab:cityscape_comp}, where Mask Transfiner achieves the best mask AP 36.1 and boundary AP 18.0. Our approach significantly surpasses existing SOTA methods, including PointRend~\cite{kirillov2020pointrend}
and BMask R-CNN~\cite{ChengWHL20} by a margin of 1.3 AP and 2.3 AP using the same Faster R-CNN detector. Note that PointRend treats each point separately using MLP without sequential relation modeling. Compared to our baseline Mask R-CNN~\cite{he2017mask}, Transfiner greatly improves the mask AP from 33.8\% to 37.2\%, which shows the effectiveness of the quadtree refinement in high quality instance segmentation.



\parsection{Attention Weight Visualization} 
We visualize the attended pixels distribution for the single incoherent pixel during refinement in Figure~\ref{fig:att_vis}, where xxxxx. analyze which part helps most, near or far, or very similar regions.

\parsection{Limitations}
xxxx

\subsubsection{Qualitative Comparisons.}
xxxxxx; more in Supp. file




\section{Conclusion}
We present Mask Transfiner, a new high-quality and efficient instance segmentation method. Transfiner first detects and decomposes the image regions for segmentation refinement to build a hierarchical quadtree. Then, all points on the quadtree are transformed into to a query sequence, which is input to our transformer encoder and decoder for predicting their final labels. In contrast to previous instance segmentation methods using convolutions limited by uniform image grids, Mask Transfiner produces high-quality masks with low computation and memory cost. We validate the efficacy of Transfiner on both the existing two-stage and query-based instance segmentation frameworks, and show that Transfiner achieves large performance gain on COCO, LVIS and Cityscapes.
In the future, we believe Transfiner may significantly benefit more dense prediction tasks requiring high-resolution output, such as optical flow estimation and semantic segmentation.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
