\section{Experiments}
\subsection{Learning Order as Noise Level Hypothesis}\label{sec:learning_order_hypothesis}
We first seek to confirm our hypothesis that the learning order automatically orders distantly supervised data from clean, high-quality instances to noisy, low-quality instances. However, given the large amount of pre-training data, statistically significant confirmation via manual annotation is prohibitively expensive. So, we devise the following experiment to test our hypothesis in lieu of a significant manual annotation effort.

We begin with the assumption that a model trained on a dataset without noise will perform better than a model trained on a dataset with noise. Suppose learning order denoising successfully orders instances relative to their noise; then, we should observe a boost in performance by training on a subset of early-learned instances compared to a model trained on the complete, noisy dataset.

As reported by~\citet{Gao2021ManualEM}, up to 53\% of relation instances labeled via distant supervision are incorrect. Using this estimation, we attempt to use learning order denoising to remove the roughly 50\% of instances that are noisy instances from the DocRED's distantly supervised training set. To do this, we first obtain the learning order of relation instances using the methodology described in Section~\ref{sec:learning_order}. Without loss of generalization, we choose RoBERTa~\cite{Liu2019RoBERTaAR}, specifically the \textit{roberta-base} checkpoint\footnote{\url{https://huggingface.co/roberta-base}}, as the base model to develop the order of learned instances. 


\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{l c c c } 
\toprule
    \textbf{Learning order}  & \textbf{Training set} & \textbf{Training set size} & \textbf{F1} \\ \midrule
    \rule{-3pt}{3ex}
        None                    & $\mathcal{T}$                             & 100\%      & 45.8 \\
    \rule{-3pt}{3ex}
        Batch-based             & $\mathcal{T}_{\mathcal{A}_0^B}$  & 45.0\%     & \textbf{46.6}\\
    \rule{-3pt}{3ex}
        Epoch-based             & $\mathcal{T}_{\mathcal{A}_0^E}$  & 64.9\%     & 46.0 \\
\bottomrule
\end{tabular}} \caption{Results comparing performance on the DocRED test set using trimmed sets of distantly supervised training data. The \textit{batch-based} and \textit{epoch-based} training sets consist of training instances determined by the instances learned within the first epoch using the respective learning order collection methods.}
\label{tab:batch_vs_epoch_based}
\end{table}

We observe that the set of training instances learned via \textit{batch-based} learning order in the first epoch, $\mathcal{A}_0^B$, consists of 45\% of the total training instances. We use $\mathcal{A}_0^B$ to construct a trimmed training set $\mathcal{T}_{\mathcal{A}_0^B}$. We then compare performance in two settings: (1) RoBERTa trained with the complete distantly supervised training dataset $\mathcal{T}$ and (2) RoBERTa trained on the trimmed, denoised training data $\mathcal{T}_{\mathcal{A}_0^B}$. Table~\ref{tab:batch_vs_epoch_based} reports the results of this experiment. Significantly, the denoised training set consisting of only 45\% training data outperforms the baseline model. 

We also conduct an informal manual analysis of the learning order. We randomly selected 120 instances from the first six training epochs---60 correctly, and 60 incorrectly predicted instances. We find that 93\% of the correct predictions have accurate labels within the first three epochs. However, in epochs 4 through 6, label accuracy drops to 53\% among correct predictions. Furthermore, we find a relatively low label accuracy of 50\% from the first three epochs of incorrect predictions, illustrating that the model struggles to learn noisy instances compared to clean instances early in training. We use these results and the results presented in Table~\ref{tab:batch_vs_epoch_based} to argue that learning order successfully orders instances from clean, high-quality to noisy, low-quality instances. 



\subsection{Learning Order: Batch- vs. Epoch-based}
We experiment with two methods of collecting learning order data: \textit{batch-based} and \textit{epoch-based} (see Appendix~\ref{app:bb_vs_eb} for pseudo-code describing these methods). \\
\textbf{Batch-based:} As previously mentioned, for \textit{batch-based} learning order we collect learned instances per batch across each epoch during training. However, we recognize that this may bias the set of learned instances by the random batch for which they are selected. For example, accurately labeled relation instances selected for the first few batches during training may not be predicted correctly because the model has not learned much. \\
\textbf{Epoch-based:} To reduce potential selection order bias from \textit{batch-based} learning order, we experiment with \textit{epoch-based} learning order 
by evaluating the model on the entire training set at the end of each epoch. We rerun the experiment detailed in Section~\ref{sec:learning_order_hypothesis} using \textit{epoch-based} learning order to construct the trimmed dataset $\mathcal{T}_{\mathcal{A}_0^E}$ and present the results in Table~\ref{tab:batch_vs_epoch_based}.

Using \textit{epoch-based} learning order, we observe that the model learns 64.9\% of the training instances within the first epoch, an increase compared to the 45.0\% of learned instances from \textit{batch-based} learning order. However, training RoBERTa on the \textit{epoch-based} training subset, we obtain an F1 score of 46.0, which under-performs relative to the 46.6 F1 score from the \textit{batch-based} learning order experiment. We hypothesize that, while \textit{epoch-based} learning order may capture more learned instances, it leads to noisier instances leaking into the sets of learned data because the model is more prone to simply memorizing noisy labels encountered previously in the epoch. 

Note that we do not use DocRED's human-annotated training data in these learning order experiments. Instead, we train on the distantly supervised training data and test on human-annotated data. This is done to assess the quality of the various subsets of distantly labeled data. It is why the performance of these tests is considerably lower than the results from the experiments in Section~\ref{subsec:experiments_relation_extraction} that leverage human-annotated training data.


\subsection{Pre-training Details}
To ensure a fair comparison and highlight the effectiveness of \our, we align our pre-training data and settings to those used by ERICA. The ERICA pre-training dataset is constructed using distant supervision for RE by pairing documents from Wikipedia (English) with the Wikidata knowledge graph. This distantly labeled dataset creation method mirrors the method used to create the distantly labeled training set in DocRED but differs in that it is much larger and more diverse. It contains 1M documents, 7.2M relation instances, and 1040 relation types compared to DocRED's 100k documents, 1.5M relation instances, and 96 relation types (not including \textit{no relation}). Additional checks are performed to ensure no fact triples overlap between the training data and the test sets of the various downstream RE tasks. Detailed pre-training settings can found in Appendix~\ref{app:pretrain}.



\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{l | c c | c c | c c } 
    \hline
    \rule{0pt}{3ex}
    \textbf{Size}               & \multicolumn{2}{c}{1\%}   & \multicolumn{2}{c}{10\%}  & \multicolumn{2}{c}{100\%} \\  \hline
    \rule{0pt}{3ex}
    \textbf{Metrics}            & F1     & IgF1      & F1    & IgF1      & F1    & IgF1  \\ \hline 
    \rule{-2pt}{3ex}
    CNN*                         & -      & -         & -     & -         & 42.3  & 40.3  \\ 
    BiLSTM*                      & -      & -         & -     & -         & 51.1  & 50.3  \\  \hline   
    \rule{-3pt}{3ex}
    HINBERT*                     & - & - & - & - & 55.6 & 53.7 \\
    CorefBERT*                   & \underline{32.8} & \textbf{31.2} & 46.0 & 43.7 & 57.0 & 54.5 \\
    SpanBERT*                    & 32.2 & 30.4 & 46.4 & 44.5 & 57.3 & 55.0 \\
    ERNIE*                       & 26.7 & 25.5 & 46.7 & 44.2 & 56.6 & 54.2 \\
    MTB*                         & 29.0 & 27.6 & 46.1 & 44.1 & 56.9 & 54.3 \\
    CP*                          & 30.3 & 28.7 & 44.8 & 42.6 & 55.2 & 52.7 \\
    BERT                        & 19.9 & 18.8 & 45.2 & 43.1 & 56.6 & 54.4 \\
    RoBERTa                     & 29.6 & 27.9 & 47.6 & 45.7 & 58.2 & 55.9 \\
    ERICA\textsubscript{BERT}   & 22.9 & 21.7 & 48.5 & 46.4 & 57.4 & 55.2 \\
    ERICA\textsubscript{RoBERTa}& 30.0 & 28.2 & \underline{50.1} & \underline{48.1} & \underline{59.1} & \underline{56.9} \\
    WCL\textsubscript{RoBERTa}  & 22.3 & 20.8 & 49.4 & 47.5 & 58.5 & 56.2 \\ \hline 
    \rule{-3pt}{3ex}
    \our                        & \textbf{33.2} & \textbf{31.2} & \textbf{50.3} & \textbf{48.3} & \textbf{59.5} & \textbf{57.1} \\ \hline
\end{tabular}} \caption{F1-micro scores reported on the DocRED test set. IgF1 ignores performance on fact triples in the test set overlapping with triples in the train/dev sets. (* denotes performance as reported in~\cite{Qin2021ERICAIE}; all other numbers are from our implementations).}
\label{tab:docred}
\end{table}

\subsection{Relation Extraction}\label{subsec:experiments_relation_extraction}

\textbf{Document-level RE}: To assess our framework's ability to extract document-level relations, we report performance on DocRED~\cite{docred}. We compare our model to the following baselines: (1) CNN~\cite{zeng-etal-2014-relation}, (2) BiLSTM \cite{lstm}, (3) BERT~\cite{Devlin2019BERTPO}, (4) RoBERTa~\cite{Liu2019RoBERTaAR}, (5) MTB~\cite{BaldiniSoares2019MatchingTB}, (6) CP~\cite{Peng2020LearningFC}, (7 \& 8) ERICA\textsubscript{BERT} \& ERICA\textsubscript{RoBERTa}~\cite{Qin2021ERICAIE}, (9) WCL~\cite{wcl}.  We fine-tune the pre-trained models on DocRED's human-annotated train/dev/test splits (see Appendix~\ref{app:docred} for detailed experimental settings). We implement WCL with identical settings from our other pre-training experiments and, for fair comparison, we use RoBERTa instead of BERT as the base model for WCL, given the superior performance we observe from RoBERTa in all other experiments. Table~\ref{tab:docred} reports performance across multiple data reduction settings (1\%, 10\%, and 100\%), using an overall F1-micro score and an F1-micro score computed by ignoring fact triples in the test set that overlap with fact triples in the training and development splits. We observe that \our outperforms all baselines in all experimental settings, offering evidence that \our produces better relationship representations from noisy data.

Given that learning-order denoising weighs earlier learned instances over later learned instances, \our may be biased towards easier, or common relation classes. The increase in F1-micro performance may result from improved predictions on common relation classes at the expense of predictions on rare classes. To better understand the performance gains, we also report F1-macro and F1-macro weighted in Table~\ref{tab:macro_docred}. The results show that \our outperforms the top baselines in both F1-macro metrics indicating that, on average, our method improves performance across all relation classes. However, the low F1-macro scores from all the models highlight an area for improvement---future pre-trained RE models should focus on improving performance on long-tail relation classes. 


\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{l | c | c } 
    \hline
    \rule{0pt}{3ex}
    \textbf{Metric}                 & F1-macro     & F1-macro-weighted \\ \hline 
    \rule{-3pt}{3ex}
    BERT                            & 37.3              & 54.9 \\
    RoBERTa                         & 39.6              & 56.9 \\
    ERICA\textsubscript{BERT}       & 37.9              & 55.8 \\
    ERICA\textsubscript{RoBERTa}    & \underline{40.1 } & \underline{57.8} \\ 
    WCL\textsubscript{RoBERTA}      & 39.9              & 57.2 \\ \hline
    \rule{-3pt}{3ex}
    \our                            & \textbf{40.7 }     & \textbf{58.2} \\ \hline
\end{tabular}} \caption{F1-macro and F1-macro-weighted scores reported from the DocRED test set.}
\label{tab:macro_docred}
\end{table}


\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{l | c c c | c c c } 
    \hline
    \rule{0pt}{3ex}
    \textbf{Dataset}               & \multicolumn{3}{c}{TACRED}   & \multicolumn{3}{c}{SemEval} \\  \hline
    \rule{-3pt}{3ex}
    \textbf{Size}               & 1\%     & 10\%      & 100\%    & 1\%      & 10\%    & 100\%  \\ \hline 
    \rule{-3pt}{3ex}
    MTB*                            & 35.7 & 58.8 & 68.2 & 44.2 & 79.2 & 88.2 \\
    CP*                             & 37.1 & 60.6 & 68.1 & 40.3 & 80.0 & \underline{88.5} \\ 
    BERT                            & 22.2 & 53.5 & 63.7 & 41.0 & 76.5 & 87.8 \\
    RoBERTa                         & 27.3 & 61.1 & 69.3 & 43.6 & 77.7 & 87.5 \\
    ERICA\textsubscript{BERT}       & 34.9 & 56.0 & 64.9 & 46.4 & 79.8 & 88.1 \\
    ERICA\textsubscript{RoBERTa}    & \underline{41.1} & \underline{61.7} & 69.5 & \underline{50.3} & \underline{80.9} & 88.4 \\ 
    WCL\textsubscript{RoBERTA}      & 37.6 & 61.3 & \underline{69.7} & 47.0 & 80.0 & 88.3 \\ \hline
    \rule{-3pt}{3ex}
    \our                & \textbf{43.7} & \textbf{62.7} & \textbf{70.3} & \textbf{51.2} & \textbf{81.0} & \textbf{88.7}\\ \hline
\end{tabular}} \caption{F1-micro scores reported from the TACRED and SemEval test sets (* denotes performance as reported in \cite{Qin2021ERICAIE}; all other numbers are from our implementations).}
\label{tab:tacred_semeval}
\end{table}


\textbf{Sentence-level RE}: To assess our framework's ability to extract sentence-level relations, we report performance on TACRED \cite{zhang2017tacred} and SemEval-2010 Task 8 \cite{hendrickx-etal-2010-semeval}. We compare our model to MTB, CP, BERT, RoBERTa, ERICA\textsubscript{BERT}, ERICA\textsubscript{RoBERTa}, and WCL (see Appendix \ref{app:semeval_tacred} for detailed experimental settings). Table \ref{tab:tacred_semeval} reports F1 scores across multiple data reduction settings (1\%, 10\%, 100\%). Again, we observe that \our outperforms all baselines in all settings.