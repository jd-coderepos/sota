\documentclass[sigconf]{acmart}




\usepackage{booktabs} \usepackage{multirow}
\usepackage{amsmath}
\usepackage[noend]{algpseudocode}
\usepackage{graphicx}
\usepackage{hhline}
\usepackage{url}
\usepackage{amsmath,amssymb}
\usepackage{subcaption}

\usepackage{bm}
\usepackage{mathtools}
\newcommand{\bs}[1]{\ensuremath{\bm{\mathit{#1}}}}


\newcommand{\spara}[1]{\smallskip\noindent{\bf #1}}
\newcommand{\mpara}[1]{\medskip\noindent{\bf #1}}
\newcommand{\para}[1]{\noindent{\bf #1}}



\newcommand{\squishlist}{
 \begin{list}{}
  {  \setlength{\itemsep}{0pt}
     \setlength{\parsep}{3pt}
     \setlength{\topsep}{3pt}
     \setlength{\partopsep}{0pt}
     \setlength{\leftmargin}{2em}
     \setlength{\labelwidth}{1.5em}
     \setlength{\labelsep}{0.5em}
} }
\newcommand{\squishlisttight}{
 \begin{list}{}
  { \setlength{\itemsep}{0pt}
    \setlength{\parsep}{0pt}
    \setlength{\topsep}{0pt}
    \setlength{\partopsep}{0pt}
    \setlength{\leftmargin}{2em}
    \setlength{\labelwidth}{1.5em}
    \setlength{\labelsep}{0.5em}
} }

\newcommand{\squishdesc}{
 \begin{list}{}
  {  \setlength{\itemsep}{0pt}
     \setlength{\parsep}{3pt}
     \setlength{\topsep}{3pt}
     \setlength{\partopsep}{0pt}
     \setlength{\leftmargin}{1em}
     \setlength{\labelwidth}{1.5em}
     \setlength{\labelsep}{0.5em}
} }

\newcommand{\squishend}{
  \end{list}
}





\makeatletter
\providecommand*{\diff}{\@ifnextchar^{\DIfF}{\DIfF^{}}}
\def\DIfF^#1{\mathop{\mathrm{\mathstrut d}}\nolimits^{#1}\gobblespace
}
\def\gobblespace{\futurelet\diffarg\opspace}
\def\opspace{\let\DiffSpace\!\ifx\diffarg(\let\DiffSpace\relax
        \else
                \ifx\diffarg
P(\bs{X}) = & \prod_i P(\bs{x}_i) = \prod_i\int P_\phi\left(\bs{x}_i|\bs{z}\right) P_\theta(\bs{z}) \diff \bs{z} \, .

  \log P(\mathbf{x}) & = \int Q(\bs{z}|\bs{x}) \log P(\bs{x}) \diff \bs{z}\\
& =  \int Q(\bs{z}|\bs{x}) \log \frac{P(\bs{x}|\bs{z})P(\bs{z})}{P(\bs{z}|\bs{x})} \diff \bs{z}\\
& =  \int Q(\bs{z}|\bs{x}) \log P(\bs{X}|\bs{z}) \diff \bs{z} 
   +\int Q(\bs{z}|\bs{x}) \log \frac{Q(\bs{z}|\bs{x})}{P(\bs{z}|\bs{x})} \diff \bs{z} \\
& - \int Q(\bs{z}|\bs{x}) \log \frac{Q(\bs{z}|\bs{x})}{P(\bs{z})} \diff \bs{z} \\
   & =  E_{z\sim Q}[\log P(\bs{x}|\bs{z})]
     +\mathit{KL}\left(Q(\bs{z}|\bs{x}) \|P(\bs{z}|\bs{x})\right) \\
& - \mathit{KL}\left(Q(\bs{z}|\bs{x})  \| P(\bs{z})\right) \, .

\begin{split} \label{eq:elbo}
\log P(\mathbf{x}) &- \mathit{KL}\left[Q(\bs{z}|\bs{x})
\|P(\bs{z}|\bs{x})\right] \\
& = E_{z\sim Q}[\log P(\bs{x}|\bs{z})] - \mathit{KL}\left[Q(\bs{z}|\bs{x})  \| P(\bs{z})\right] \, .
\end{split}

\begin{split}
\mathit{KL}&\left[\mathcal{N}(\bs{\mu}, \bs{\Sigma}) \|
  \mathcal{N}(\bs{m}, \bs{S})\right]=\\
  & \frac{1}{2}\left(log\frac{|\bs{S}|}{|\bs{\Sigma}|} -K 
+ \mathit{tr}\left(\bs{S}^{-1}\bs{\Sigma}\right) 
+ \left(\bs{m} - \bs{\mu}\right)^T\bs{S}^{-1}\left(\bs{m} -
  \bs{\mu}\right)
\right) \, ,
\end{split}
\label{eq:vaeloss}
\begin{split}
\mathcal{L}(\phi, \lambda; \bs{X})  =  \sum_i &\left\{  
\frac{1}{2}
    \sum_k \left(\sigma_{\lambda,k} (\bs{x}_i)  - 1 - 
      \log\sigma_{\lambda,k}(\bs{x}_i) 
    + \mu_{\lambda,k}(\bs{x}_i)^2
\right)\right.\\
& - \left.
\vphantom{\sum_{k} k} E_{\bs{\epsilon} \sim \mathcal{N}(\bs{0}, \bs{I})} \left[
    \log P_\phi\left(\bs{x}_i | \bs{z}_\lambda(\bs{\epsilon}, \bs{x}_i)\right) \right]
\right\} \, ,
\end{split}

\begin{gathered}
\bs{z}_u \sim \mathcal{N}(\bs{0}, \bs{I}_K) \, , \qquad  \pi(\bs{z}_u)
\sim \exp\left\{f_\phi(\bs{z}_u)\right\} \, , \\
 \bs{x}_u \sim \mathit{Multi}\left(N_u,\pi(\bs{z}_u)\right) \, .
\end{gathered}

\log P_\phi(\bs{x}_u |\bs{z}_u) = \sum_i x_{u,i} \log \pi_i(\bs{z}_u) \, , 

\begin{gathered}
\bs{z}_{u,i} \sim \mathcal{N}(\bs{0}, \bs{I}_K) \, ,\\
i \prec_u j \sim \mathit{Bernoulli}\left(\pi(\bs{z}_{u,i},
   \bs{z}_{u,j})\right) \, ,
\end{gathered}
\label{eq:rvaep}
\pi(\bs{z}_{u,i}, \bs{z}_{u,j})
\sim \sigma\left(f_\phi(\bs{z}_{u,i}, \bs{z}_{u,j})\right) \, ,
\label{eq:rvaed}
\pi(\bs{z}_{u,i}, \bs{z}_{u,j})
\sim \sigma\left(f_\phi(\bs{z}_{u,j}) -  f_\phi(\bs{z}_{u,i})\right) \, .

P\left(\bs{x}_{(1:T)}\right) = \prod_{t=0}^{T-1}
P\left(\bs{x}_{(t+1)}| \bs{x}_{(1:t)}\right) \, .
\label{eq:svae}
\begin{gathered}
\bs{z}_{u(t)} \sim \mathcal{N}\left(\bs{0},\bs{I}_K\right) \, , 
\qquad  \pi\left(\bs{z}_{u(t)}\right)
\sim \exp\left\{f_\phi\left(\bs{z}_{u(t)}\right)\right\} \, , \
which results in the joint likelihood

Here, we can approximate the posterior  with the factorized proposal distribution 

where  is a gaussian
distribution whose parameters  and
 depend upon the 
current history , by means of a recurrent layer : 


The resulting loss function follows directly from eq.~\ref{eq:vaeloss}: 




The proposal distribution introduces a
dependency of the latent variable from a recurrent layer, which allows
to recover the information from the previous history. We call this
model SVAE. Figure \ref{fig:architecture} shows the main
architectural difference with respect to the models proposed so
far. In SVAE, we can observe the recurrent relationship occurring in the layer
upon which  depends. 

Notably, the prediction step can be easily accomplished in a similar
way as for MVAE: given a user history , we can resort
to eq.~\ref{eq:recurrent} and set , upon which we can devise the probability for the
 by means of . 


\subsubsection{A taxonomy of sequential variational autoencoders}
\label{sec:svaetaxonomy}
We already discussed that within a sequence modeling framework, the
core of the approach is on modeling  through a conditional variational autoencoder. SVAE describes
just one of several possible modeling choices. 
\begin{figure*}[ht!]
 \centering
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \begin{tabular}{c}
          \includegraphics[height=1.2in]{rvae1}
          \\
          
        \end{tabular}
        \caption{Single latent dependency}
        \label{fig:SLD}
    \end{subfigure}~ 
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \begin{tabular}{c}
          \includegraphics[height=1.2in]{rvae2}
          \\
          
        \end{tabular}
        \caption{Multiple latent dependencies}
        \label{fig:MLD}
    \end{subfigure}
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \begin{tabular}{c}
          \includegraphics[height=1.2in]{rvae3}
          \\
          
        \end{tabular}
        \caption{Recurrent latent dependency}
        \label{fig:RLD}
    \end{subfigure}
    ~ 
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \begin{tabular}{c}
          \includegraphics[height=1.2in]{rvae4}
          \\
          
        \end{tabular}
        \caption{Global recurrent dependency}
        \label{fig:GLD}
    \end{subfigure}
    \caption{Recurrent Variational Autoencoders. Diamond boxes
      represent deterministic variables.}
    \label{fig:recvae}
\end{figure*}
In fact, four alternate formalizations can take place, as illustrated in
fig.~\ref{fig:recvae}:  
\begin{itemize}
\item The simplest modeling, considers a single gaussian variable
  \bs{z} and a parameterization of the conditional distribution 
 with a function
, where  represents the hidden
state of a recurrent neural function 

Figure \ref{fig:SLD} illustrates the graphical model and the sequence
likelihood.
\item Following \cite{BayerO14a}, we can alternatively introduce 
  independent gaussian variables and parameterize
  the conditional likelihood

by means of a function 
, where again  represents the hidden
state of a recurrent neural function 

Figure \ref{fig:MLD} illustrates the graphical model and the sequence
likelihood. 
\item So far, the modeling combines history and latent variables to
  define the conditional distribution. An alternative consists in
  assuming that history affects the latent variable \bs{z} instead. In
  practice, the conditional 
  likelihood  would
  only depend on the latent variable, which exhibits a prior distribution
  , modeled as a gaussian
  with parameters depending on the current state  of the
  network, devised as in eq.~\ref{rnn-x}. The graphical model and
  sequence likelihood are shown in fig. \ref{fig:RLD} and it
  resembles the SVAE model discussed above. 
\item Finally, \cite{Chung:2015:RLV:2969442.2969572} propose a comprehensive
  model, where the gaussian latent variables  also depend on each
  other through the Markovian dependency . Both this dependency and 
   can be specified by
  the hidden state  of a recurrent network, devised as in
  eq. \ref{rnn-x-z}.  The graphical model and
  sequence likelihood are shown in fig. \ref{fig:GLD}. 
\end{itemize}



\subsubsection{Extending SVAE}\label{sec:svae-ext}
The generative model of eq. \ref{eq:svae} only focuses on the
next item in the sequence. The base model however is flexible enough
to extend its focus on the next  items, regardless of the time: 

Again, the resulting joint likelihood can be modeled in different
ways. 
The simplest way consists in considering  as a time-ordered multi-set, 

Alternatively, we can consider the probability of an item as a mixture
relative to all the time-steps where it is considered:

where  is the probability of
observing  according to . 
In both cases, the variational approximation is modeled exactly as
shown above, and the only difference lies in the second component of
the loss function, which has to be adapted according to the above
equations.  



There is an interesting analogy between eq. \ref{eq:attention} and the
attention mechanism \cite{Vaswani:2017}. In fact, it can be noticed in
the equation that the prediction of  depends on the
latent status of the  previous steps in the sequence. In practice,
this enables to capture short-term dependencies and to encapsulate
them in the same probabilistic framework by weighting the likelihood
based on . 






\section{Evaluation}\label{sec:exp}


















We evaluate SVAE on some benchmark datasets, by comparing with various
baselines and the current state-of-the-art competitors, in order to
assess its capabilities in modeling preference data. Additionally, we
provide a sensitivity analysis relative to the configurations/contour
conditions upon which SVAE is better suited. The main highlight from
our experiments is that SVAE provides a huge edge over the current
state-of-the-art for the task of top-N recommendation across various
metrics. 



\vspace*{-5mm}
\subsection{Datasets}
We evaluate our model along with the competitors on two popular
publicly available datasets, namely \textit{Movielens-1M} and
\textit{Netflix}. 
\textrm{Movielens-1M} is a time series dataset containing
user-item ratings pairs along with the corresponding
timestamp. Since we work on implicit feedback, we binarize the
data, by considering only the user-item pairs where the rating provided
by the user was strictly greater than 3 on a range 1:5. 
\textrm{Netflix} has the same data format as \textrm{Movielens-1M} and
the same technique is used to binarize the ratings. We use a subset
of the full dataset that matches the user-distribution with the
full dataset. The subset is built by stratifying users according to
their history length, and then sampling a subset of size inversely
proportional to the size of the
strata. Figure~\ref{fig:netflix_dist} compares the distributions of
the full and the sampled dataset: We can notice that the
distributions share the same shape, but in the sample users with
small history length are undersampled whereas users with large
histories are kept.



\begin{table}
\begin{center}
\includegraphics[width=0.4\textwidth]{table-datasets}
\end{center}
\caption{Basic statistics of the datasets used in the experiments.}
\label{dataStats}
\end{table}

Table~\ref{dataStats} shows the basic statistics of the data. For
illustration purposes, we also show the basic statistics of the full
Netflix dataset. We can see that the average length of sequences in
the Netflix subset is significantly increased, as a result of
downsampling users with small history length. Also, notice that the
sampling procedure does not affect the number of items. 
To preprocess the data, we
first group the interacted items for each user, and ignore the users
who have interacted with less than five items. After preprocessing, we
split the remaining users into train, validation and test sets. 

\begin{figure}[h!]
\begin{minipage}[t]{0.45\linewidth}
    \includegraphics[height=1.3in]{Netflix-Full_count_dist_user}
\end{minipage}\hfill \begin{minipage}[t]{0.45\linewidth}
    \includegraphics[height=1.3in]{Netflix-Subset_count_dist_user}
\end{minipage}
  \vspace{-5mm}
  \caption{Comparison of the user distributions of the full Netflix
    dataset and the subsample produced by progressive stratified
    sampling.}
    \label{fig:netflix_dist}
\end{figure}


\vspace*{-5mm}
\subsection{Evaluation Metrics and protocol}
Since we are considering implicit preferences, the evaluation is
done on top-n recommendation, and it relies on the following
metrics.
\begin{description}
\item[Normalized Discounted Cumulative Gain.] Also abbreviated as
  \textit{NDCG@n}, the metric gives more weight to the relevance of
  items on top of the recommender list and is defined as

where

Here,  is the relevance (either 1 or 0 within the implicit
feedback scenario) of the -th recommended items in the
recommendation list, and  is the set of relevant items. 
\item[Precision.] By defining  as the number of items
  occurring in the recommendation list that were actually
  preferred by the user, we have
  
\item[Recall,] defined as the percentage of items actually preferred by the user
  that were present in the recommendation list: 

\end{description}
In our experiments, we use the above metrics with two values of ,
respectively  and . 

The evaluation protocol works as follows.
We partition users into training, validation and test set. The
model is trained using the full histories of the users in the training
set. During evaluation, for each user in the validation/test
set we split the \textbf{time-sorted} user history into two parts,
\textit{fold-in} and \textit{fold-out} splits. The \textit{fold-in}
split is used as a basis to learn the necessary representations and provide a
recommendation list which is then evaluated with the \textit{fold-out} split
of the user history using the metrics defined above. 
We believe that this strategy is more robust when compared to
other methodologies wherein the same user can be in both the training
as well as testing sets. Table~\ref{dataStats} shows the number of
heldout users for each datasets. 

It is worth noticing that Liang et. al
\cite{Liang:2018:VAC:3178876.3186150} follow a similar strategy but
with a major difference: they do not consider any sorting for user
histories. That is, for the validation/test users, the \textit{fold-in}
set doesn't precede the \textit{fold-out} with respect to time. By
contrast, we keep the
\textit{fold-in} set to be the first \textit{80\%} of the time-sorted
user history, and the last  represents the \textit{fold-out} set. We
shall see in the following sections that this difference is substantial in the
evaluation. 

\subsection{Competitors}
We compare our model with various baselines and current
state-of-the-art models including recently published neural 
architectures and we now present a brief summary about our
competitors to provide a better understanding of these models. 

\begin{itemize}
\item \textbf{POP} is a simple baseline where users are recommended
  the most popular items in the training set. 

\item \textbf{BPR}, already mentioned in section \ref{sec:bpr}, is a
  state of the art model based on Matrix
  Factorization, which ranks items
  differently for each user \cite{Rendle:2009}.
There is a subtle
  issue concerning BPR: by separating users on
  training/validation/test as discussed above, the latent
  representation of users in the validation/test is not
  meaningful. That, is, BPR is only capable of providing meaningful
  predictions for users that were already exploited in the training
  phase. To solve this, we extended the training set to include
  the partial history in \textit{fold-in} for each user in the
  validation/test. The evaluation still takes place on their
  corresponding \textit{fold-out} sets. 

\item \textbf{FPMC} \cite{Rendle:2010} is a model which
  clubs both Matrix Factorization and Markov Chains together using
  personalized transition graphs over underlying Markov
  chains.


\item \textbf{CASER} \cite{Tang:2018}, already discussed in section
  \ref{sec:related}, is a convolutional model that uses vertical and
  horizontal convolutional layers to embed a sequence of recent items
  thereby learning sequential patterns for next-item
  recommendation. The authors have shown that this model outperforms
  other approaches based on recurrent neural network modeling, such as
  GRU4Rec. We use the implementation provided by the authors and tune
  the network by keeping the number of horizontal filters to be 16,
  and vertical filters to be 4. 

\item \textbf{MVAE}, discussed in section \ref{sec:mvae}, from which
  the SVAE model draws heavily. We use the implementation provided by
  the authors, with the default hyperparameter settings.

\end{itemize}

We also include the \textbf{RVAE} model proposed in section
\ref{sec:bpr}, that we consider a baseline here. Notably, despite
being considered a simple extension of the BPR
model, RVAE relies on a neural layer for embedding users: as a
consequence,  it does a better job in ranking items for the users which the
model has never seen before, contrary to BPR. In practice, RVAE
upgrades BPR to session-based recommendations. 

\subsection{Training Details}
The experiments only consider the SVAE model illustrated in subsection
\ref{sec:svae-simple} and the extensions of subsection~\ref{sec:svae-ext}. We
reserve a more detailed analysis of the extensions discussed in
subsection~\ref{sec:svaetaxonomy} to future work. 
The model is trained end-to-end on the full histories of the training
users.
Model hyperparameters are set using
the evaluation metrics obtained on validation users.


The SVAE architecture includes an embedding layer of size 256, a recurrent layer 
realized as a GRU with 200 cells, and two encoding layers (of size 150
and 64) and finally two decoding layers (again, of size 64 and 150).
We set the number  of latent factors for the
variational autoencoder to be 64. Adam \cite{KingmaB14} was used to
optimize the loss function coupled with a weight decay of .
As for RVAE, the architecture includes user/item embedding layers (of
size 128), two encoding layers (size 100 and 64), and a final layer
that produces the score . 
Both SVAE and RVAE were implemented in PyTorch \cite{paszke2017automatic}
and trained on a single GTX 1080Ti GPU.
The source code is available
on GitHub\footnote{\url{https://github.com/noveens/svae_cf}.}. 

\subsection{Results}

\begin{table*}
\begin{center}
\includegraphics[width=0.8\linewidth]{table-results.pdf}
\caption{Results of the evaluation (in percentage). MVAE considers random
  splits that disregards the temporal order of user history.
  BPR relies on including the \textit{fold-in} subsequences in
  the training phase.}
\label{table:resultsTable}
\end{center}
\end{table*}

In a first set of experiments, we compare SVAE with all competitors
described above. Table~\ref{table:resultsTable} shows the results of
the comparison. SVAE consistently outperforms the competitors on both
datasets with a significant gain on all the metrics. It is important
here to highlight how the temporal \textit{fold-in}/\textit{fold-out}
split is crucial for a fair evaluation of the predictive capabilities:
MVAE was evaluated both on temporal and random split, exhibiting
totally different performances. Our interpretation is that, with
random splits, the prediction for an item is easier if the encoding
phase is aware of forthcoming items in the same user history. This
severely affects the performance and overrates the predictive
capabilities of a model: In fact, the accuracy of MVAE drops
substantially when a temporal split is considered.

By contrast, SVAE is trained to capture the actual temporal
dependencies and ultimately results in better predictive accuracy.
This is also shown in fig.~\ref{fig:seq_len_vs_ndcg}, where we show
that SVAE consistently outperforms the competitors irrespective of the
size of \textit{fold-in}. The only exception is with very short
sequences (less than 10 items), where MVAE gets better results with
respect to the sequential models. It is also worth noticing how the
performance of both sequential models tend to degrade with increasing
sequences, but SVAE maintains its advantage over CASER. 

\begin{figure}[th!]
  \vspace*{-2mm}
  \centering
  \includegraphics[width=\columnwidth]{bars_dist.pdf}
  \vspace*{-8mm}
  \caption{Average NDCG@100 for MVAE, CASER \& SVAE across
    various history lengths.}
  \label{fig:seq_len_vs_ndcg}
\end{figure}




We discussed in section \ref{sec:svae-ext} how the basic SVAE framework can
be extended to focus on predicting the next  items, rather then
just the next item. We analyse this capability in
fig.~\ref{fig:next_k_vs_ndcg}, where the accuracy for different values
of  is considered according to the modeling in
\ref{eq:svae-ext-std}. On Movielens, the best value is achieved for
, and acceptable values range within the interval .


\begin{figure}[th!]
  \centering
  \includegraphics[height=1.2in]{next_k_vs_ndcg.pdf}
        \vspace*{-4mm}
    \caption{NDCG@100 values for SVAE on Movielens, across different sizes for the
      number of items forward in time to predict on. 
}
    \label{fig:next_k_vs_ndcg}
\end{figure}

Finally, in fig.~\ref{fig:learningcurve} we analyse the convergence
rate of SVAE, in terms of NDCG (left y-axis and blue line) and loss
function values (right y-axis and red line). The learning phase
converges quickly and does not exhibit 
overfitting: on Movielens, a stable model is reached within 8
epochs, whereas Netflix requires 13 epochs. The average runtime per
epoch is 197 seconds on Movielens and 2 hours on Netflix: in practice,
the learning scales linearly with the number of
interactions in the dataset. 


\begin{figure}[th!]
  \centering
  \includegraphics[height=1.4in,width=1.6in]{learning_curve_ml.pdf}
  ~
  \includegraphics[height=1.4in,width=1.6in]{learning_curve_netflix.pdf}
  \vspace*{-4mm}
  \caption{Learning curves on validation data.}
  \label{fig:learningcurve}
\end{figure}





\section{Conclusions and future work}\label{sec:conc}







Combining the representation power of latent spaces, provided by
variational autoencoders, with the sequence modeling capabilities of
recurrent neural networks is an effective strategy to sequence
recommendation. To prove this, we devised SVAE, a simple yet robust
mathematical framework capable of modeling temporal dynamics 
upon different perspectives, within the fundamentals of variational
autoencoders. The experimental evaluation highlights the
capability of SVAE to consistently outperform state-of-the-art models.






The framework proposed here is worth further extensions
that we plan to accomplish in a future work. From a conceptual point
of view, we need to perform a thorough analysis of the taxonomy defined in section
\ref{sec:svaetaxonomy}. From an architectural point of view,
the attention mechanism, outlined in
section \ref{eq:attention}, requires a better understanding and a more
detailed analysis of its possible impact in view of the recent
developments \cite{Bahdanau14} in the literature. Also, the SVAE
framework relies on recurrent networks. However, different
architectures (e.g. based on convolution \cite{Tang:2018} or translation
invariance \cite{He:2017}) are worth being investigated within a
probabilistic variational setting.  














\begin{thebibliography}{53}



\ifx \showCODEN    \undefined \def \showCODEN     #1{\unskip}     \fi
\ifx \showDOI      \undefined \def \showDOI       #1{#1}\fi
\ifx \showISBNx    \undefined \def \showISBNx     #1{\unskip}     \fi
\ifx \showISBNxiii \undefined \def \showISBNxiii  #1{\unskip}     \fi
\ifx \showISSN     \undefined \def \showISSN      #1{\unskip}     \fi
\ifx \showLCCN     \undefined \def \showLCCN      #1{\unskip}     \fi
\ifx \shownote     \undefined \def \shownote      #1{#1}          \fi
\ifx \showarticletitle \undefined \def \showarticletitle #1{#1}   \fi
\ifx \showURL      \undefined \def \showURL       {\relax}        \fi
\providecommand\bibfield[2]{#2}
\providecommand\bibinfo[2]{#2}
\providecommand\natexlab[1]{#1}
\providecommand\showeprint[2][]{arXiv:#2}

\bibitem[\protect\citeauthoryear{Agarwal and Chen}{Agarwal and Chen}{2010}]{flda}
\bibfield{author}{\bibinfo{person}{D. Agarwal} {and} \bibinfo{person}{B.~C.
  Chen}.} \bibinfo{year}{2010}\natexlab{}.
\newblock \showarticletitle{fLDA: Matrix Factorization Through Latent Dirichlet
  Allocation}. In \bibinfo{booktitle}{\emph{Proceedings of the Third ACM
  International Conference on Web Search and Data Mining}}
  \emph{(\bibinfo{series}{WSDM '10})}. \bibinfo{publisher}{ACM},
  \bibinfo{address}{New York, NY, USA}, \bibinfo{pages}{91--100}.
\newblock


\bibitem[\protect\citeauthoryear{Aggarwal}{Aggarwal}{2016}]{Aggarwal:2016:RST:2931100}
\bibfield{author}{\bibinfo{person}{C. Aggarwal}.}
  \bibinfo{year}{2016}\natexlab{}.
\newblock \bibinfo{booktitle}{\emph{Recommender Systems: The Textbook}}.
\newblock \bibinfo{publisher}{Springer Publishing Company, Incorporated}.
\newblock


\bibitem[\protect\citeauthoryear{Bahdanau, Cho, and Bengio}{Bahdanau
  et~al\mbox{.}}{2014}]{Bahdanau14}
\bibfield{author}{\bibinfo{person}{D. Bahdanau}, \bibinfo{person}{K. Cho},
  {and} \bibinfo{person}{Y. Bengio}.} \bibinfo{year}{2014}\natexlab{}.
\newblock \showarticletitle{Neural Machine Translation by Jointly Learning to
  Align and Translate.}
\newblock \bibinfo{journal}{\emph{CoRR}}  \bibinfo{volume}{abs/1409.0473}
  (\bibinfo{year}{2014}).
\newblock


\bibitem[\protect\citeauthoryear{Barbieri and Manco}{Barbieri and
  Manco}{2011}]{BM2011}
\bibfield{author}{\bibinfo{person}{N. Barbieri} {and} \bibinfo{person}{G.
  Manco}.} \bibinfo{year}{2011}\natexlab{}.
\newblock \showarticletitle{An Analysis of Probabilistic Methods for Top-N
  Recommendation in Collaborative Filtering}. In
  \bibinfo{booktitle}{\emph{Proceedings of the Joint European Conference on
  Machine Learning and Knowledge Discovery in Databases}}
  \emph{(\bibinfo{series}{ECML/PKDD '11})}. \bibinfo{pages}{172--187}.
\newblock


\bibitem[\protect\citeauthoryear{Barbieri, Manco, and Ritacco}{Barbieri
  et~al\mbox{.}}{2014}]{2014Barbieri}
\bibfield{author}{\bibinfo{person}{N. Barbieri}, \bibinfo{person}{G. Manco},
  {and} \bibinfo{person}{E. Ritacco}.} \bibinfo{year}{2014}\natexlab{}.
\newblock \bibinfo{booktitle}{\emph{Probabilistic Approaches to
  Recommendations}}.
\newblock \bibinfo{publisher}{Morgan {\&} Claypool Publishers}.
\newblock


\bibitem[\protect\citeauthoryear{Barbieri, Manco, Ritacco, Carnuccio, and
  Bevacqua}{Barbieri et~al\mbox{.}}{2013}]{Barbieri:2013}
\bibfield{author}{\bibinfo{person}{N. Barbieri}, \bibinfo{person}{G. Manco},
  \bibinfo{person}{E. Ritacco}, \bibinfo{person}{M. Carnuccio}, {and}
  \bibinfo{person}{A. Bevacqua}.} \bibinfo{year}{2013}\natexlab{}.
\newblock \showarticletitle{Probabilistic topic models for sequence data}.
\newblock \bibinfo{journal}{\emph{Machine Learning}} (\bibinfo{year}{2013}),
  \bibinfo{pages}{1--25}.
\newblock


\bibitem[\protect\citeauthoryear{Bayer and Osendorfer}{Bayer and
  Osendorfer}{2014}]{BayerO14a}
\bibfield{author}{\bibinfo{person}{J. Bayer} {and} \bibinfo{person}{C.
  Osendorfer}.} \bibinfo{year}{2014}\natexlab{}.
\newblock \showarticletitle{Learning Stochastic Recurrent Networks}.
\newblock \bibinfo{journal}{\emph{CoRR}}  \bibinfo{volume}{abs/1411.7610}
  (\bibinfo{year}{2014}).
\newblock
\urldef\tempurl \url{http://arxiv.org/abs/1411.7610}
\showURL{\tempurl}


\bibitem[\protect\citeauthoryear{Blei, Kucukelbir, and McAuliffe}{Blei
  et~al\mbox{.}}{2017}]{Blei2017}
\bibfield{author}{\bibinfo{person}{D.~M. Blei}, \bibinfo{person}{A.
  Kucukelbir}, {and} \bibinfo{person}{J.~D. McAuliffe}.}
  \bibinfo{year}{2017}\natexlab{}.
\newblock \showarticletitle{Variational Inference: A Review for Statisticians}.
\newblock \bibinfo{journal}{\emph{J. Amer. Statist. Assoc.}}
  \bibinfo{volume}{112}, \bibinfo{number}{518} (\bibinfo{year}{2017}),
  \bibinfo{pages}{859--877}.
\newblock


\bibitem[\protect\citeauthoryear{Cho, van Merrienboer, G\"ul\c{c}ehre,
  Bahdanau, Bougares, Schwenk, and Bengio}{Cho et~al\mbox{.}}{2014}]{Cho14}
\bibfield{author}{\bibinfo{person}{K. Cho}, \bibinfo{person}{B. van
  Merrienboer}, \bibinfo{person}{\c{C}. G\"ul\c{c}ehre}, \bibinfo{person}{D.
  Bahdanau}, \bibinfo{person}{F. Bougares}, \bibinfo{person}{H. Schwenk}, {and}
  \bibinfo{person}{Y. Bengio}.} \bibinfo{year}{2014}\natexlab{}.
\newblock \showarticletitle{Learning Phrase Representations using RNN
  Encoder-Decoder for Statistical Machine Translation}. In
  \bibinfo{booktitle}{\emph{Proceedings of the Conference on Empirical Methods
  in Natural Language Processing}} \emph{(\bibinfo{series}{EMNLP '14})}.
  \bibinfo{pages}{1724--1734}.
\newblock


\bibitem[\protect\citeauthoryear{Chung, G{\"{u}}l{\c{c}}ehre, Cho, and
  Bengio}{Chung et~al\mbox{.}}{2014}]{ChungGCB14}
\bibfield{author}{\bibinfo{person}{J. Chung}, \bibinfo{person}{{\c{C}}.
  G{\"{u}}l{\c{c}}ehre}, \bibinfo{person}{K. Cho}, {and} \bibinfo{person}{Y.
  Bengio}.} \bibinfo{year}{2014}\natexlab{}.
\newblock \showarticletitle{Empirical Evaluation of Gated Recurrent Neural
  Networks on Sequence Modeling}.
\newblock \bibinfo{journal}{\emph{CoRR}}  \bibinfo{volume}{abs/1412.3555}
  (\bibinfo{year}{2014}).
\newblock
\newblock
\shownote{Presented at the Deep Learning workshop at NIPS2014.}


\bibitem[\protect\citeauthoryear{Chung, Kastner, Dinh, Goel, Courville, and
  Bengio}{Chung et~al\mbox{.}}{2015}]{Chung:2015:RLV:2969442.2969572}
\bibfield{author}{\bibinfo{person}{J. Chung}, \bibinfo{person}{K. Kastner},
  \bibinfo{person}{L. Dinh}, \bibinfo{person}{K. Goel}, \bibinfo{person}{A.
  Courville}, {and} \bibinfo{person}{Y. Bengio}.}
  \bibinfo{year}{2015}\natexlab{}.
\newblock \showarticletitle{A Recurrent Latent Variable Model for Sequential
  Data}. In \bibinfo{booktitle}{\emph{Proceedings of the 28th International
  Conference on Neural Information Processing Systems}}
  \emph{(\bibinfo{series}{NIPS'15})}. \bibinfo{pages}{2980--2988}.
\newblock


\bibitem[\protect\citeauthoryear{Devooght and Bersini}{Devooght and
  Bersini}{2017}]{Devooght:2017}
\bibfield{author}{\bibinfo{person}{R. Devooght} {and} \bibinfo{person}{H.
  Bersini}.} \bibinfo{year}{2017}\natexlab{}.
\newblock \showarticletitle{Long and Short-Term Recommendations with Recurrent
  Neural Networks}. In \bibinfo{booktitle}{\emph{Proceedings of the 25th
  Conference on User Modeling, Adaptation and Personalization}}
  \emph{(\bibinfo{series}{UMAP '17})}. \bibinfo{pages}{13--21}.
\newblock


\bibitem[\protect\citeauthoryear{Greff, Srivastava, Koutnik, Steunebrink, and
  Schmidhuber}{Greff et~al\mbox{.}}{2017}]{Greff:2017}
\bibfield{author}{\bibinfo{person}{K. Greff}, \bibinfo{person}{R.~K.
  Srivastava}, \bibinfo{person}{J. Koutnik}, \bibinfo{person}{B.~R.
  Steunebrink}, {and} \bibinfo{person}{J. Schmidhuber}.}
  \bibinfo{year}{2017}\natexlab{}.
\newblock \showarticletitle{LSTM: A Search Space Odyssey}.
\newblock \bibinfo{journal}{\emph{IEEE Transactions on Neural Networks and
  Learning Systems}} \bibinfo{volume}{28}, \bibinfo{number}{10}
  (\bibinfo{year}{2017}), \bibinfo{pages}{2222--2232}.
\newblock


\bibitem[\protect\citeauthoryear{{Gupta}, {Yelahanka Raghuprasad}, and
  {Kumar}}{{Gupta} et~al\mbox{.}}{2018}]{2018arXiv180801006G}
\bibfield{author}{\bibinfo{person}{K. {Gupta}}, \bibinfo{person}{M. {Yelahanka
  Raghuprasad}}, {and} \bibinfo{person}{P. {Kumar}}.}
  \bibinfo{year}{2018}\natexlab{}.
\newblock \showarticletitle{{A Hybrid Variational Autoencoder for Collaborative
  Filtering}}.
\newblock \bibinfo{journal}{\emph{ArXiv e-prints}} (\bibinfo{year}{2018}).
\newblock
\showeprint[arxiv]{1808.01006}


\bibitem[\protect\citeauthoryear{He, Kang, and McAuley}{He
  et~al\mbox{.}}{2017a}]{He:2017}
\bibfield{author}{\bibinfo{person}{R. He}, \bibinfo{person}{W. Kang}, {and}
  \bibinfo{person}{J. McAuley}.} \bibinfo{year}{2017}\natexlab{a}.
\newblock \showarticletitle{Translation-based Recommendation}. In
  \bibinfo{booktitle}{\emph{Proceedings of the ACM Conference on Recommender
  Systems}} \emph{(\bibinfo{series}{RecSys '17})}. \bibinfo{pages}{161--169}.
\newblock


\bibitem[\protect\citeauthoryear{He and McAuley}{He and McAuley}{2016}]{HeM16}
\bibfield{author}{\bibinfo{person}{R. He} {and} \bibinfo{person}{J. McAuley}.}
  \bibinfo{year}{2016}\natexlab{}.
\newblock \showarticletitle{Fusing Similarity Models with Markov Chains for
  Sparse Sequential Recommendation}. In \bibinfo{booktitle}{\emph{Proceedings
  of the {IEEE} 16th International Conference on Data Mining}}
  \emph{(\bibinfo{series}{ICDM '16})}. \bibinfo{pages}{191--200}.
\newblock


\bibitem[\protect\citeauthoryear{He, Liao, Zhang, Nie, Hu, and Chua}{He
  et~al\mbox{.}}{2017b}]{He17}
\bibfield{author}{\bibinfo{person}{X. He}, \bibinfo{person}{L. Liao},
  \bibinfo{person}{H. Zhang}, \bibinfo{person}{L. Nie}, \bibinfo{person}{X.
  Hu}, {and} \bibinfo{person}{T. Chua}.} \bibinfo{year}{2017}\natexlab{b}.
\newblock \showarticletitle{Neural Collaborative Filtering}. In
  \bibinfo{booktitle}{\emph{Proceedings of the 26th International Conference on
  World Wide Web}} \emph{(\bibinfo{series}{WWW '17})}.
  \bibinfo{pages}{173--182}.
\newblock


\bibitem[\protect\citeauthoryear{Hidasi, Karatzoglou, Baltrunas, and
  Tikk}{Hidasi et~al\mbox{.}}{2016}]{Hidasi:2016}
\bibfield{author}{\bibinfo{person}{B. Hidasi}, \bibinfo{person}{A.
  Karatzoglou}, \bibinfo{person}{L. Baltrunas}, {and} \bibinfo{person}{D.
  Tikk}.} \bibinfo{year}{2016}\natexlab{}.
\newblock \showarticletitle{Session-based Recommendations with Recurrent Neural
  Networks}. In \bibinfo{booktitle}{\emph{International Conference on Learning
  Representations}} \emph{(\bibinfo{series}{ICLR '16})}.
\newblock


\bibitem[\protect\citeauthoryear{Hofmann}{Hofmann}{2004}]{Hofmann:2004}
\bibfield{author}{\bibinfo{person}{T. Hofmann}.}
  \bibinfo{year}{2004}\natexlab{}.
\newblock \showarticletitle{Latent semantic models for collaborative
  filtering}.
\newblock \bibinfo{journal}{\emph{ACM Transactions on Information Systems}}
  \bibinfo{volume}{22}, \bibinfo{number}{1} (\bibinfo{year}{2004}),
  \bibinfo{pages}{89--115}.
\newblock


\bibitem[\protect\citeauthoryear{Jannach and Ludewig}{Jannach and
  Ludewig}{2017}]{Jannach:2017}
\bibfield{author}{\bibinfo{person}{D. Jannach} {and} \bibinfo{person}{M.
  Ludewig}.} \bibinfo{year}{2017}\natexlab{}.
\newblock \showarticletitle{When Recurrent Neural Networks Meet the
  Neighborhood for Session-Based Recommendation}. In
  \bibinfo{booktitle}{\emph{Proceedings of the ACM Conference on Recommender
  Systems}} \emph{(\bibinfo{series}{RecSys '17})}. \bibinfo{pages}{306--310}.
\newblock


\bibitem[\protect\citeauthoryear{Kabbur, Ning, and Karypis}{Kabbur
  et~al\mbox{.}}{2013}]{Kabbur:2013}
\bibfield{author}{\bibinfo{person}{S. Kabbur}, \bibinfo{person}{X. Ning}, {and}
  \bibinfo{person}{G. Karypis}.} \bibinfo{year}{2013}\natexlab{}.
\newblock \showarticletitle{FISM: Factored Item Similarity Models for top-N
  Recommender Systems}. In \bibinfo{booktitle}{\emph{Proceedings of the 19th
  ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}}
  \emph{(\bibinfo{series}{KDD '13})}. \bibinfo{pages}{659--667}.
\newblock


\bibitem[\protect\citeauthoryear{Kingma and Welling}{Kingma and
  Welling}{2014}]{kingma14}
\bibfield{author}{\bibinfo{person}{D.P. Kingma} {and} \bibinfo{person}{M.
  Welling}.} \bibinfo{year}{2014}\natexlab{}.
\newblock \showarticletitle{Auto-Encoding Variational Bayes}. In
  \bibinfo{booktitle}{\emph{Proceedings of the 2nd International Conference on
  Learning Representations}} \emph{(\bibinfo{series}{ICLR '14})}.
\newblock


\bibitem[\protect\citeauthoryear{Kingma and Ba}{Kingma and Ba}{2014}]{KingmaB14}
\bibfield{author}{\bibinfo{person}{D.~P. Kingma} {and} \bibinfo{person}{J.
  Ba}.} \bibinfo{year}{2014}\natexlab{}.
\newblock \showarticletitle{Adam: {A} Method for Stochastic Optimization}.
\newblock \bibinfo{journal}{\emph{CoRR}}  \bibinfo{volume}{abs/1412.6980}
  (\bibinfo{year}{2014}).
\newblock
\urldef\tempurl \url{http://arxiv.org/abs/1412.6980}
\showURL{\tempurl}


\bibitem[\protect\citeauthoryear{Koren}{Koren}{2008}]{Koren08}
\bibfield{author}{\bibinfo{person}{Y. Koren}.} \bibinfo{year}{2008}\natexlab{}.
\newblock \showarticletitle{Factorization Meets the Neighborhood: A
  Multifaceted Collaborative Filtering Model}. In
  \bibinfo{booktitle}{\emph{Proceedings of the ACM SIGKDD International
  Conference on Knowledge Discovery and Data Mining}}
  \emph{(\bibinfo{series}{KDD '08})}. \bibinfo{pages}{426--434}.
\newblock


\bibitem[\protect\citeauthoryear{Li and She}{Li and She}{2017}]{Li:2017}
\bibfield{author}{\bibinfo{person}{X. Li} {and} \bibinfo{person}{J. She}.}
  \bibinfo{year}{2017}\natexlab{}.
\newblock \showarticletitle{Collaborative Variational Autoencoder for
  Recommender Systems}. In \bibinfo{booktitle}{\emph{Proceedings of the ACM
  SIGKDD International Conference on Knowledge Discovery and Data Mining}}
  \emph{(\bibinfo{series}{KDD '17})}. \bibinfo{pages}{305--314}.
\newblock


\bibitem[\protect\citeauthoryear{Liang, Krishnan, Hoffman, and Jebara}{Liang
  et~al\mbox{.}}{2018}]{Liang:2018:VAC:3178876.3186150}
\bibfield{author}{\bibinfo{person}{D. Liang}, \bibinfo{person}{R.~G. Krishnan},
  \bibinfo{person}{M.D. Hoffman}, {and} \bibinfo{person}{T. Jebara}.}
  \bibinfo{year}{2018}\natexlab{}.
\newblock \showarticletitle{Variational Autoencoders for Collaborative
  Filtering}. In \bibinfo{booktitle}{\emph{Proceedings of the 2018 World Wide
  Web Conference}} \emph{(\bibinfo{series}{WWW '18})}.
  \bibinfo{pages}{689--698}.
\newblock


\bibitem[\protect\citeauthoryear{Liu, Wu, Wang, Li, and Wang}{Liu
  et~al\mbox{.}}{2016}]{Liu:2016}
\bibfield{author}{\bibinfo{person}{Q. Liu}, \bibinfo{person}{S. Wu},
  \bibinfo{person}{D. Wang}, \bibinfo{person}{Z. Li}, {and} \bibinfo{person}{L.
  Wang}.} \bibinfo{year}{2016}\natexlab{}.
\newblock \showarticletitle{Context-Aware Sequential Recommendation}. In
  \bibinfo{booktitle}{\emph{Proceedings of the IEEE International Conference on
  Data Mining}} \emph{(\bibinfo{series}{ICDM '16})}.
  \bibinfo{pages}{1053--1058}.
\newblock


\bibitem[\protect\citeauthoryear{Murphy}{Murphy}{2012}]{Murphy:2012:MLP:2380985}
\bibfield{author}{\bibinfo{person}{K.~P. Murphy}.}
  \bibinfo{year}{2012}\natexlab{}.
\newblock \bibinfo{booktitle}{\emph{Machine Learning: A Probabilistic
  Perspective}}.
\newblock \bibinfo{publisher}{The MIT Press}.
\newblock
\showISBNx{0262018020, 9780262018029}


\bibitem[\protect\citeauthoryear{Ning and Karypis}{Ning and Karypis}{2011}]{Ning:2011}
\bibfield{author}{\bibinfo{person}{X. Ning} {and} \bibinfo{person}{G.
  Karypis}.} \bibinfo{year}{2011}\natexlab{}.
\newblock \showarticletitle{SLIM: Sparse Linear Methods for Top-N Recommender
  Systems}. In \bibinfo{booktitle}{\emph{Proceedings of the IEEE 11th
  International Conference on Data Mining}} \emph{(\bibinfo{series}{ICDM
  '11})}. \bibinfo{pages}{497--506}.
\newblock


\bibitem[\protect\citeauthoryear{Paszke, Gross, Chintala, Chanan, Yang, DeVito,
  Lin, Desmaison, Antiga, and Lerer}{Paszke et~al\mbox{.}}{2017}]{paszke2017automatic}
\bibfield{author}{\bibinfo{person}{A. Paszke}, \bibinfo{person}{S. Gross},
  \bibinfo{person}{S. Chintala}, \bibinfo{person}{G. Chanan},
  \bibinfo{person}{E. Yang}, \bibinfo{person}{Z. DeVito}, \bibinfo{person}{Z.
  Lin}, \bibinfo{person}{A. Desmaison}, \bibinfo{person}{L. Antiga}, {and}
  \bibinfo{person}{A. Lerer}.} \bibinfo{year}{2017}\natexlab{}.
\newblock \showarticletitle{Automatic differentiation in PyTorch}. In
  \bibinfo{booktitle}{\emph{NIPS Autodiff Workshop}}.
\newblock


\bibitem[\protect\citeauthoryear{Quadrana, Cremonesi, and Jannach}{Quadrana
  et~al\mbox{.}}{2018}]{Quadrana:2018}
\bibfield{author}{\bibinfo{person}{M. Quadrana}, \bibinfo{person}{P.
  Cremonesi}, {and} \bibinfo{person}{D. Jannach}.}
  \bibinfo{year}{2018}\natexlab{}.
\newblock \showarticletitle{Sequence-Aware Recommender Systems}.
\newblock \bibinfo{journal}{\emph{ACM Comput. Surv.}} \bibinfo{volume}{51},
  \bibinfo{number}{4} (\bibinfo{year}{2018}), \bibinfo{pages}{66:1--66:36}.
\newblock


\bibitem[\protect\citeauthoryear{Quadrana, Karatzoglou, Hidasi, and
  Cremonesi}{Quadrana et~al\mbox{.}}{2017}]{Quadrana:2017}
\bibfield{author}{\bibinfo{person}{M. Quadrana}, \bibinfo{person}{A.
  Karatzoglou}, \bibinfo{person}{B. Hidasi}, {and} \bibinfo{person}{P.
  Cremonesi}.} \bibinfo{year}{2017}\natexlab{}.
\newblock \showarticletitle{Personalizing Session-based Recommendations with
  Hierarchical Recurrent Neural Networks}. In
  \bibinfo{booktitle}{\emph{Proceedings of the ACM Conference on Recommender
  Systems}} \emph{(\bibinfo{series}{RecSys '17})}. \bibinfo{pages}{130--137}.
\newblock


\bibitem[\protect\citeauthoryear{Rendle}{Rendle}{2012}]{Rendle:2012}
\bibfield{author}{\bibinfo{person}{S. Rendle}.}
  \bibinfo{year}{2012}\natexlab{}.
\newblock \showarticletitle{Factorization Machines with libFM}.
\newblock \bibinfo{journal}{\emph{ACM Trans. Intell. Syst. Technol.}}
  \bibinfo{volume}{3}, \bibinfo{number}{3} (\bibinfo{year}{2012}),
  \bibinfo{pages}{57:1--57:22}.
\newblock


\bibitem[\protect\citeauthoryear{Rendle, Freudenthaler, Gantner, and
  Schmidt-Thieme}{Rendle et~al\mbox{.}}{2009}]{Rendle:2009}
\bibfield{author}{\bibinfo{person}{S. Rendle}, \bibinfo{person}{C.
  Freudenthaler}, \bibinfo{person}{Z. Gantner}, {and} \bibinfo{person}{L.
  Schmidt-Thieme}.} \bibinfo{year}{2009}\natexlab{}.
\newblock \showarticletitle{BPR: Bayesian Personalized Ranking from Implicit
  Feedback}. In \bibinfo{booktitle}{\emph{Proceedings of the Twenty-Fifth
  Conference on Uncertainty in Artificial Intelligence}}
  \emph{(\bibinfo{series}{UAI '09})}. \bibinfo{pages}{452--461}.
\newblock


\bibitem[\protect\citeauthoryear{Rendle, Freudenthaler, and
  Schmidt-Thieme}{Rendle et~al\mbox{.}}{2010}]{Rendle:2010}
\bibfield{author}{\bibinfo{person}{S. Rendle}, \bibinfo{person}{C.
  Freudenthaler}, {and} \bibinfo{person}{L. Schmidt-Thieme}.}
  \bibinfo{year}{2010}\natexlab{}.
\newblock \showarticletitle{Factorizing Personalized Markov Chains for
  Next-basket Recommendation}. In \bibinfo{booktitle}{\emph{Proceedings of the
  International Conference on World Wide Web}} \emph{(\bibinfo{series}{WWW
  '10})}. \bibinfo{pages}{811--820}.
\newblock


\bibitem[\protect\citeauthoryear{Rezende, Mohamed, and Wierstra}{Rezende
  et~al\mbox{.}}{2014}]{RezendeMW14}
\bibfield{author}{\bibinfo{person}{D.~J. Rezende}, \bibinfo{person}{S.
  Mohamed}, {and} \bibinfo{person}{D. Wierstra}.}
  \bibinfo{year}{2014}\natexlab{}.
\newblock \showarticletitle{Stochastic Backpropagation and Approximate
  Inference in Deep Generative Models}. In
  \bibinfo{booktitle}{\emph{Proceedings of the 31th International Conference on
  Machine Learning}} \emph{(\bibinfo{series}{ICML '14})}.
  \bibinfo{pages}{1278--1286}.
\newblock


\bibitem[\protect\citeauthoryear{Rifai, Vincent, Muller, Glorot, and
  Bengio}{Rifai et~al\mbox{.}}{2011}]{Rifai:2011}
\bibfield{author}{\bibinfo{person}{S. Rifai}, \bibinfo{person}{P Vincent},
  \bibinfo{person}{X. Muller}, \bibinfo{person}{X. Glorot}, {and}
  \bibinfo{person}{Y. Bengio}.} \bibinfo{year}{2011}\natexlab{}.
\newblock \showarticletitle{Contractive Auto-encoders: Explicit Invariance
  During Feature Extraction}. In \bibinfo{booktitle}{\emph{Proceedings of the
  International Conference on International Conference on Machine Learning}}
  \emph{(\bibinfo{series}{ICML'11})}. \bibinfo{pages}{833--840}.
\newblock


\bibitem[\protect\citeauthoryear{Salakhutdinov and Mnih}{Salakhutdinov and
  Mnih}{2008a}]{Salakhutdinov:2008}
\bibfield{author}{\bibinfo{person}{R. Salakhutdinov} {and} \bibinfo{person}{A.
  Mnih}.} \bibinfo{year}{2008}\natexlab{a}.
\newblock \showarticletitle{Bayesian Probabilistic Matrix Factorization Using
  Markov Chain Monte Carlo}. In \bibinfo{booktitle}{\emph{Proceedings of the
  International Conference on Machine Learning}} \emph{(\bibinfo{series}{ICML
  '08})}. \bibinfo{pages}{880--887}.
\newblock


\bibitem[\protect\citeauthoryear{Salakhutdinov and Mnih}{Salakhutdinov and
  Mnih}{2008b}]{Ruslan:2008}
\bibfield{author}{\bibinfo{person}{R. Salakhutdinov} {and} \bibinfo{person}{A.
  Mnih}.} \bibinfo{year}{2008}\natexlab{b}.
\newblock \showarticletitle{Probabilistic Matrix Factorization}. In
  \bibinfo{booktitle}{\emph{Proceedings of the International Conference on
  Neural Information Processing Systems}} \emph{(\bibinfo{series}{NIPS '08})}.
  \bibinfo{pages}{1257--1264}.
\newblock


\bibitem[\protect\citeauthoryear{Sedhain, Menon, Sanner, and Xie}{Sedhain
  et~al\mbox{.}}{2015}]{Sedhain15}
\bibfield{author}{\bibinfo{person}{S. Sedhain}, \bibinfo{person}{A.~K. Menon},
  \bibinfo{person}{S. Sanner}, {and} \bibinfo{person}{L. Xie}.}
  \bibinfo{year}{2015}\natexlab{}.
\newblock \showarticletitle{AutoRec: Autoencoders Meet Collaborative
  Filtering}. In \bibinfo{booktitle}{\emph{Proceedings of the International
  Conference on World Wide Web}} \emph{(\bibinfo{series}{WWW '15})}.
  \bibinfo{pages}{111--112}.
\newblock


\bibitem[\protect\citeauthoryear{Strub and Mary}{Strub and Mary}{2015}]{strub15}
\bibfield{author}{\bibinfo{person}{F. Strub} {and} \bibinfo{person}{J. Mary}.}
  \bibinfo{year}{2015}\natexlab{}.
\newblock \showarticletitle{{Collaborative Filtering with Stacked Denoising
  AutoEncoders and Sparse Inputs}}. In \bibinfo{booktitle}{\emph{{NIPS Workshop
  on Machine Learning for eCommerce}}}.
\newblock
\urldef\tempurl \url{https://hal.inria.fr/hal-01256422}
\showURL{\tempurl}


\bibitem[\protect\citeauthoryear{Tan, Xu, and Liu}{Tan et~al\mbox{.}}{2016}]{Tan16}
\bibfield{author}{\bibinfo{person}{Y.~K. Tan}, \bibinfo{person}{X. Xu}, {and}
  \bibinfo{person}{Y. Liu}.} \bibinfo{year}{2016}\natexlab{}.
\newblock \showarticletitle{Improved Recurrent Neural Networks for
  Session-based Recommendations}. In \bibinfo{booktitle}{\emph{Proceedings of
  the 1st Workshop on Deep Learning for Recommender Systems}}
  \emph{(\bibinfo{series}{DLRS '16})}. \bibinfo{pages}{17--22}.
\newblock


\bibitem[\protect\citeauthoryear{Tang and Wang}{Tang and Wang}{2018}]{Tang:2018}
\bibfield{author}{\bibinfo{person}{J. Tang} {and} \bibinfo{person}{K. Wang}.}
  \bibinfo{year}{2018}\natexlab{}.
\newblock \showarticletitle{Personalized Top-N Sequential Recommendation via
  Convolutional Sequence Embedding}. In \bibinfo{booktitle}{\emph{Proceedings
  of the ACM International Conference on Web Search and Data Mining}}
  \emph{(\bibinfo{series}{WSDM '18})}. \bibinfo{pages}{565--573}.
\newblock


\bibitem[\protect\citeauthoryear{Tavakol and Brefeld}{Tavakol and
  Brefeld}{2014}]{Tavakol:2014}
\bibfield{author}{\bibinfo{person}{M. Tavakol} {and} \bibinfo{person}{U.
  Brefeld}.} \bibinfo{year}{2014}\natexlab{}.
\newblock \showarticletitle{Factored MDPs for Detecting Topics of User
  Sessions}. In \bibinfo{booktitle}{\emph{Proceedings of the 8th ACM Conference
  on Recommender Systems}} \emph{(\bibinfo{series}{RecSys '14})}.
  \bibinfo{pages}{33--40}.
\newblock


\bibitem[\protect\citeauthoryear{Twardowski}{Twardowski}{2016}]{Twardowski:2016}
\bibfield{author}{\bibinfo{person}{B. Twardowski}.}
  \bibinfo{year}{2016}\natexlab{}.
\newblock \showarticletitle{Modelling Contextual Information in Session-Aware
  Recommender Systems with Neural Networks}. In
  \bibinfo{booktitle}{\emph{Proceedings of the ACM Conference on Recommender
  Systems}} \emph{(\bibinfo{series}{RecSys '16})}. \bibinfo{pages}{273--276}.
\newblock


\bibitem[\protect\citeauthoryear{Vaswani, Shazeer, Parmar, Uszkoreit, Jones,
  Gomez, Kaiser, and Polosukhin}{Vaswani et~al\mbox{.}}{2017}]{Vaswani:2017}
\bibfield{author}{\bibinfo{person}{A. Vaswani}, \bibinfo{person}{N. Shazeer},
  \bibinfo{person}{N. Parmar}, \bibinfo{person}{J. Uszkoreit},
  \bibinfo{person}{L. Jones}, \bibinfo{person}{A.~N Gomez}, \bibinfo{person}{L.
  Kaiser}, {and} \bibinfo{person}{I Polosukhin}.}
  \bibinfo{year}{2017}\natexlab{}.
\newblock \showarticletitle{Attention is All you Need}.
\newblock In \bibinfo{booktitle}{\emph{Advances in Neural Information
  Processing Systems 30}}. \bibinfo{pages}{5998--6008}.
\newblock


\bibitem[\protect\citeauthoryear{Vincent, Larochelle, Lajoie, Bengio, and
  Manzagol}{Vincent et~al\mbox{.}}{2010}]{Vincent10}
\bibfield{author}{\bibinfo{person}{P. Vincent}, \bibinfo{person}{H.
  Larochelle}, \bibinfo{person}{I. Lajoie}, \bibinfo{person}{Y. Bengio}, {and}
  \bibinfo{person}{P.~A. Manzagol}.} \bibinfo{year}{2010}\natexlab{}.
\newblock \showarticletitle{Stacked Denoising Autoencoders: Learning Useful
  Representations in a Deep Network with a Local Denoising Criterion}.
\newblock \bibinfo{journal}{\emph{J. Mach. Learn. Res.}}  \bibinfo{volume}{11}
  (\bibinfo{year}{2010}).
\newblock


\bibitem[\protect\citeauthoryear{Wang and Blei}{Wang and Blei}{2011}]{Wang:2011}
\bibfield{author}{\bibinfo{person}{C. Wang} {and} \bibinfo{person}{D. Blei}.}
  \bibinfo{year}{2011}\natexlab{}.
\newblock \showarticletitle{Collaborative Topic Modeling for Recommending
  Scientific Articles}. In \bibinfo{booktitle}{\emph{Proceedings of the 17th
  ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}}
  \emph{(\bibinfo{series}{KDD '11})}. \bibinfo{pages}{448--456}.
\newblock


\bibitem[\protect\citeauthoryear{Wang, Wang, and Yeung}{Wang
  et~al\mbox{.}}{2015}]{Wang15}
\bibfield{author}{\bibinfo{person}{H. Wang}, \bibinfo{person}{N. Wang}, {and}
  \bibinfo{person}{D.~Y. Yeung}.} \bibinfo{year}{2015}\natexlab{}.
\newblock \showarticletitle{Collaborative Deep Learning for Recommender
  Systems}. In \bibinfo{booktitle}{\emph{Proceedings of the ACM SIGKDD
  International Conference on Knowledge Discovery and Data Mining}}
  \emph{(\bibinfo{series}{KDD '15})}. \bibinfo{pages}{1235--1244}.
\newblock


\bibitem[\protect\citeauthoryear{Wu, Ahmed, Beutel, and Smola}{Wu
  et~al\mbox{.}}{2017}]{Wu:2017}
\bibfield{author}{\bibinfo{person}{C. Wu}, \bibinfo{person}{A. Ahmed},
  \bibinfo{person}{A. Beutel}, {and} \bibinfo{person}{H. Smola, A.and~Jing}.}
  \bibinfo{year}{2017}\natexlab{}.
\newblock \showarticletitle{Recurrent Recommender Networks}. In
  \bibinfo{booktitle}{\emph{Proceedings of the ACM International Conference on
  Web Search and Data Mining}} \emph{(\bibinfo{series}{WSDM '17})}.
  \bibinfo{pages}{495--503}.
\newblock


\bibitem[\protect\citeauthoryear{Wu, Ren, Yu, Chen, Zhang, and Zhu}{Wu
  et~al\mbox{.}}{2016}]{Wu16}
\bibfield{author}{\bibinfo{person}{S. Wu}, \bibinfo{person}{W. Ren},
  \bibinfo{person}{C. Yu}, \bibinfo{person}{G. Chen}, \bibinfo{person}{D.
  Zhang}, {and} \bibinfo{person}{J. Zhu}.} \bibinfo{year}{2016}\natexlab{}.
\newblock \showarticletitle{Personal recommendation using deep recurrent neural
  networks in NetEase}. In \bibinfo{booktitle}{\emph{Proceedings of the IEEE
  International Conference on Data Engineering}} \emph{(\bibinfo{series}{ICDE
  '16})}. \bibinfo{pages}{1218--1229}.
\newblock


\bibitem[\protect\citeauthoryear{Zhang, Yao, and Sun}{Zhang
  et~al\mbox{.}}{2017a}]{ZhangYS17aa}
\bibfield{author}{\bibinfo{person}{S. Zhang}, \bibinfo{person}{L. Yao}, {and}
  \bibinfo{person}{A. Sun}.} \bibinfo{year}{2017}\natexlab{a}.
\newblock \showarticletitle{Deep Learning based Recommender System: {A} Survey
  and New Perspectives}.
\newblock \bibinfo{journal}{\emph{CoRR}}  \bibinfo{volume}{abs/1707.07435}
  (\bibinfo{year}{2017}).
\newblock
\urldef\tempurl \url{http://arxiv.org/abs/1707.07435}
\showURL{\tempurl}


\bibitem[\protect\citeauthoryear{Zhang, Yao, and Xu}{Zhang
  et~al\mbox{.}}{2017b}]{Zhang17}
\bibfield{author}{\bibinfo{person}{S. Zhang}, \bibinfo{person}{L. Yao}, {and}
  \bibinfo{person}{X. Xu}.} \bibinfo{year}{2017}\natexlab{b}.
\newblock \showarticletitle{AutoSVD++: An Efficient Hybrid Collaborative
  Filtering Model via Contractive Auto-encoders}. In
  \bibinfo{booktitle}{\emph{Proceedings of the International ACM SIGIR
  Conference on Research and Development in Information Retrieval}}
  \emph{(\bibinfo{series}{SIGIR '17})}. \bibinfo{pages}{957--960}.
\newblock


\end{thebibliography}



\end{document}
