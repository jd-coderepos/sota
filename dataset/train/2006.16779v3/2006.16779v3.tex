

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{graphicx}
\graphicspath{{Figure/}}
\usepackage{microtype}

\aclfinalcopy 



\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{PLATO-2: Towards Building an Open-Domain Chatbot via\\ Curriculum Learning}

\author{Siqi Bao\thanks{~~Equal contribution.}~~~~ Huang He\footnotemark[1]~~~~ Fan Wang\footnotemark[1]~~~~ Hua Wu\footnotemark[1]~~~~ Haifeng Wang\\
	\bf{Wenquan Wu~~~~ Zhen Guo~~~~ Zhibin Liu~~~~ Xinchao Xu}\\
	Baidu Inc., China \\
	\texttt{\{baosiqi, hehuang, wang.fan, wu\_hua\}@baidu.com}}

\date{}

\begin{document}
	\maketitle
	\begin{abstract}
		To build a high-quality open-domain chatbot, we introduce the effective training process of PLATO-2 via curriculum learning. There are two stages involved in the learning process. In the first stage, a coarse-grained generation model is trained to learn response generation under the simplified framework of one-to-one mapping. In the second stage, a fine-grained generation model and an evaluation model are further trained to learn diverse response generation and response coherence estimation, respectively. PLATO-2 was trained on both Chinese and English data, whose effectiveness and superiority are verified through comprehensive evaluations, achieving new state-of-the-art results.
	\end{abstract}
	
	\section{Introduction}
	Recently, task agnostic pre-training with large-scale transformer models \cite{vaswani2017attention} and general text corpora has achieved great success in natural language understanding \cite{devlin2019bert} as well as natural language generation, especially open-domain dialogue generation. For instance, based on the general language model GPT-2 \cite{radford2019language}, DialoGPT \cite{zhang2019dialogpt} is further trained for response generation using Reddit comments. To obtain a human-like open-domain chatbot, Meena \cite{adiwardana2020towards} scales up the network parameters to 2.6B and employs more social media conversations in the training process, leading to significant improvement on response quality. To mitigate undesirable toxic or bias traits of large corpora, Blender \cite{roller2020recipes} further fine-tunes the pre-trained model with human annotated datasets and emphasizes desirable conversational skills of engagingness, knowledge, empathy and personality. 
	
	Besides the above attempts from model scale and data selection, PLATO \cite{bao2019plato} aims to tackle the inherent one-to-many mapping problem to improve response quality. The one-to-many mapping refers to that one dialogue context might correspond to multiple appropriate responses. It is widely recognized that the capability of modeling one-to-many relationship is crucial for response generation \cite{zhao2017learning, chen2019generating}. PLATO explicitly models this one-to-many relationship via discrete latent variables, aiming to boost the quality of dialogue generation.
	
	In this work, we will try to scale up PLATO to PLATO-2 and discuss its effective training schema via curriculum learning \cite{bengio2009curriculum}. There are two stages involved in the whole learning process, as sketched in Figure \ref{fig:overview}. In the first stage, under the simplified one-to-one mapping modeling, a \textbf{coarse-grained} generation model is trained for appropriate response generation 
	under different conversation contexts. The second stage continues to refine the generation with a \textbf{fine-grained} generation model and an evaluation model. The \textbf{fine-grained} generation model explicitly models the one-to-many mapping relationship for diverse response generation. To select the most appropriate responses generated by the fine-grained generation model, the evaluation model is trained to estimate the coherence of the responses.
	\begin{figure}
		\centering
		\includegraphics[width=0.4\textwidth]{./fig3}
		\caption{Curriculum learning process in PLATO-2.}
		\label{fig:overview}
	\end{figure} 
	
	As for response selection, previous studies have employed variant scoring functions, including forward response generation probability \cite{adiwardana2020towards}, backward context recover probability \cite{zhang2019dialogpt} and bi-directional coherence probability \cite{bao2019plato}. However, the forward score favors safe and generic responses due to the property of maximum likelihood, while the backward score tends to select the response with a high overlap with the context, resulting in repetitive conversations. In order to ameliorate the above problems, we adopt the bi-directional coherence estimation in the evaluation model of PLATO-2, whose effectiveness is also verified in the experiments.
	
	We trained PLATO-2 models with different sizes: 1.6 Billion parameters and 310 Million parameters. In addition to the English models, we also trained Chinese models with massive social media conversations. Comprehensive experiments on both English and Chinese datasets demonstrate that PLATO-2 outperforms the state-of-the-art models. We have released our English models and source codes at GitHub, hoping to facilitate the research in open-domain dialogue generation.\footnotemark[1]
	\footnotetext[1]{\url{https://github.com/PaddlePaddle/Knover/tree/master/plato-2}}
	
	\section{Methodology}
	\subsection{Model Architecture}
	The infrastructure of PLATO-2 is shown in Figure \ref{fig:network}(a), consisted of transformer blocks. The key components of the transform block include layer normalization, multi-head attention and feed forward layers. In the arrangement of these components, there are two options with regarding to the location of layer normalization. One way is the original post-normalization used in BERT \cite{devlin2019bert}, where layer normalization is placed between residual connections. The other way is the recent pre-normalization used in GPT-2 \cite{radford2019language}, where layer normalization is placed within residual connections. As reported in Megatron-LM \cite{shoeybi2019megatron}, post-normalization leads to performance degradation as the model size increases, and pre-normalization enables stable training in large-scale models. As such, pre-normalization is adopted in our model.
	\begin{figure*}
		\centering
		\includegraphics[width=\textwidth]{./fig1}
		\caption{PLATO-2 illustration. (a) Network overview with the details of transformer blocks. (b) Curriculum learning process with self-attention visualization and training objectives.}
		\label{fig:network}
	\end{figure*} 
	
	Besides, unlike conventional Seq2Seq, there are no separate encoder and decoder networks in our infrastructure. For the sake of training efficiency, PLATO-2 keeps the unified network for bi-directional context encoding and uni-directional response generation through flexible attention mechanism \cite{dong2019unified, bao2019plato}. 


	\subsection{Curriculum Learning}
	In this work, we carry out effective training of PLATO-2 via curriculum learning. As shown in Figure \ref{fig:network}(b), there are two stages involved in the learning process: during stage 1, a coarse-grained baseline model is trained for general response generation under the simplified one-to-one mapping relationship; during stage 2, two models of fine-grained generation and evaluation are further trained for diverse response generation and response coherence estimation respectively. Although the backbones of these models are all transformer blocks, the self-attention masks are designed accordingly in order to fit the training objectives. 
	
	\subsubsection{General Response Generation}
	It is well known that there exists a one-to-many relationship in conversations, where a piece of context may have multiple appropriate responses. Since the classical generation network is designed to fit one-to-one mapping, they tend to generate generic and dull responses. Whereas, it is still an efficient way to capture the general characteristics of response generation. As such, we first train a coarse-grained baseline model to learn general response generation under the simplified relationship of one-to-one mapping. Given one training sample of context and response , we need to minimize the following negative log-likelihood (NLL) loss:
	
	where  is the length of the target response  and  denotes previously generated words. Since the response generation is a uni-directional decoding process, each token in the response only attends to those before it, shown as dashed orange lines in Figure \ref{fig:network}. As for the context, bi-directional attention is enabled for better natural language understanding, shown as blue lines in Figure \ref{fig:network}.
	
	\subsubsection{Diverse Response Generation}
	Based upon the coarse-grained baseline model, diverse response generation is further trained under the relationship of one-to-many mapping. Following our previous work PLATO \cite{bao2019plato}, the discrete latent variable is introduced for the one-to-many relationship modeling. It will first estimate the latent act distribution of the training sample  and then generate the response with the sampled latent variable . The NLL loss of diverse response generation is defined as follows:
	
	where  is the latent act sampled from . The posterior distribution over latent values is estimated through the task of latent act recognition: 
	
	where  is the final hidden state of the special mask [M],  and  denote the weight matrices of one fully-connected layer.
	
	Besides the classical NLL loss, the bag-of-words (BOW) loss \cite{zhao2017learning} is also employed to facilitate the training process of discrete latent variables:
	
	where  refers to the whole vocabulary. The function  tries to predict the words within the target response in a non-autoregressive way:
	
	where  is the final hidden state of the latent variable and  is the vocabulary size.  denotes the estimated probability of word . As compared with NLL loss, the BOW loss discards the order of words and forces the latent variable to capture the global information of the target response.
	
	To sum up, the objective of the fine-grained generation model is to minimize the following integrated loss:
	
	
	\subsubsection{Response Coherence Estimation}
	Recently one strategy has been shown effective in boosting response quality, which first generates multiple candidate responses and then performs ranking according to a score function. While there are different definitions about this score function, which can be divided into three categories. Firstly, the length-average log-likelihood is employed in Meena \cite{adiwardana2020towards} as the score function, which considers the forward generation probability of the response given the context . Secondly, the maximum mutual information is utilized in DialoGPT \cite{zhang2019dialogpt}, which considers the backward probability to recover the context given the candidate response . Thirdly, a discriminative function is used in PLATO \cite{bao2019plato} for coherence estimation between the context and response , where  is the coherence label. Given that the forward score favors safe responses and the backward score produces repetitive conversations, PLATO-2 adopts the bi-directional coherence estimation in the evaluation model.
	
	The loss of response coherence estimation (RCE) is defined as follows:
	
	The positive training samples come from the dialogue context and corresponding target response , with coherence label . And the negative samples are created by randomly selecting responses from the corpus , with coherence label .
	
	To maintain the capacity of distributed representation, the task of masked language model (MLM) \cite{devlin2019bert} is also included in the evaluation network. Within this task, 15\% of the input tokens will be masked at random and the network needs to recover the masked ones. The MLM loss is defined as:
	
	where  refers to the input tokens of context and response.  stands for masked tokens and  denotes the rest unmasked ones.
	
	To sum up, the objective of the evaluation model is to minimize the following integrated loss:
	
	
	\section{Experiments}
	\subsection{Training Data}
	PLATO-2 has English and Chinese models, with training data extracted from open-domain social media conversations. The details are elaborated as follows.
	
	\subsubsection{English Data}
	The English training data is extracted from Reddit comments, which are collected by a third party and made publicly available on pushshift.io \cite{baumgartner2020pushshift}. As the comments are formatted in message trees, any conversation path from the root to a tree node can be treated as one training sample, with the node as response and its former turns as context. To improve the generation quality, we carry out elaborate data cleaning. A message node and its sub-trees will be removed if any of the following conditions is met.
	\begin{enumerate}[label=\arabic*),leftmargin=*,noitemsep,topsep=0pt]
		\item The number of BPE tokens is more than 128 or less than 2.
		\item Any word has more than 30 characters or the message has more than 1024 characters.
		\item The percentage of alphabetic characters is less than 70\%.
		\item The message contains URL.
		\item The message contains special strings, such as r/, u/, \&amp.
		\item The message has a high overlap with the parent's text.
		\item The message is repeated more than 100 times.
		\item The message contains offensive words.
		\item The subreddit is quarantined.
		\item The author is a known bot.
	\end{enumerate}
	
	After filtering, the data is split into training and validation sets in chronological order. The training set contains 684M (context, response) samples, ranging from December 2005 to July 2019. For the validation set, 0.2M samples are selected from the rest data after July 2019. The English vocabulary contains 8K BPE tokens \cite{sennrich2016neural}, constructed with SentencePiece library.
	
	\subsubsection{Chinese Data}
	The Chinese training data is collected from public domain social medias, followed by a similar cleaning process. After filtering, there are 1.2B (context, response) samples in the training set and 0.1M samples in the validation set. As for the Chinese vocabulary, it contains 30K BPE tokens.
	
	\subsection{Training Details}
	PLATO-2 has two model sizes: a small version of 310M parameters and a standard version of 1.6B parameters. The 310M parameter model has 24 transformer blocks and 16 attention heads, with the embedding dimension of 1024. The 1.6B parameter model has 32 transformer blocks and 32 attention heads, with the embedding dimension of 2048. 
	
	The hyper-parameters used in the training process are listed as follows. The maximum sequence lengths of context and response are all set to 128. We use Adam \cite{kingma2015adam} as the optimizer, with a learning rate scheduler including a linear warmup and an invsqrt decay \cite{vaswani2017attention}. To train the large-scale model with a relatively large batch size, we employ gradient checkpointing \cite{chen2016training} to trade computation for memory. Detailed configurations are summarized in Table \ref{tab:training}. The training was carried out on 64 Nvidia Tesla V100 32G GPU cards. It takes about 3 weeks for the 1.6B parameter model to accomplish the curriculum learning process.
	
	\subsection{Evaluation Settings}
	\subsubsection{Compared Methods}
	The following methods have been compared in the experiments.
	\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
		\item DialoGPT \cite{zhang2019dialogpt} is trained on the basis of GPT-2 \cite{radford2019language} using Reddit comments. There are three model sizes: 117M, 345M and 762M. Since the 345M parameter model obtains the best performance in their evaluations, we compare with it in the experiments.
		
		\item Blender \cite{roller2020recipes} is firstly trained using Reddit comments and then fine-tuned with human annotated conversations, to help emphasize desirable conversational skills of engagingness, knowledge, empathy and personality. During fine-tuning, there are four datasets included: ConvAI2 \cite{zhang2018personalizing,dinan2020second}, Empathetic Dialogues \cite{rashkin2019towards}, Wizard of Wikipedia \cite{dinan2019wizard} and Blended Skill Talk \cite{smith2020can}. These annotated conversations are referred as BST in short. Blender has three model sizes: 90M, 2.7B and 9.4B. Since the 2.7B parameter model obtains the best performance in their evaluations, we compare with it in the experiments.
		
		\item Meena \cite{adiwardana2020towards} is an open-domain chatbot trained with social media conversations. Meena has 2.6B model parameters, similar to Blender. Given that Meena has not released the model or provided a service interface, it is difficult to perform comprehensive comparison. In the experiments, we include the provided samples in their paper for static evaluation.
		
		\item Microsoft XiaoIce \cite{zhou2020design} is a popular social chatbot in Chinese. In the experiments, we use the official Weibo platform to chat with XiaoIce.
	\end{itemize}
	
	For the sake of comprehensive and fair comparisons, three versions of PLATO-2 are included in the experiments.
	\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
		\item PLATO 1.6B parameter model is the standard version in English, which is first trained using Reddit comments and then fine-tuned with BST conversations. This model will be compared to the state-of-the-art open-domain chatbot Blender, to measure the effectiveness of PLATO-2.
		
		\item PLATO 310M parameter model is a small version in English, which is trained with Reddit comments. This model will be compared to DialoGPT, as they have similar model scales and training data.
		
		\item PLATO 333M parameter Chinese model\footnotemark[2] will be compared to XiaoIce in the experiments.
	\end{itemize}
	\footnotetext[2]{This model has 24 transformer blocks and 16 attention heads, with the embedding dimension of 1024. As the Chinese vocabulary contains 30K BPE tokens, this model has 23M more parameters than the English small model.}
	
	\begin{table*}[ht]
		\centering
		\includegraphics[width=\textwidth]{./tab1}
		\caption{Training configurations of PLATO-2.}
		\label{tab:training}
	\end{table*} 
	
	\begin{table*}[ht]
		\centering
		\includegraphics[width=\textwidth]{./tab2}
		\caption{Self-chat evaluation results, with best value written in bold.}
		\label{tab:self-chat}
	\end{table*} 
	
	\begin{table*}[ht]
		\centering
		\includegraphics[width=\textwidth]{./tab3}
		\caption{Chinese interactive evaluation results, with best value written in bold.}
		\label{tab:interactive}
	\end{table*} 
	
	\begin{figure*}[ht]
		\centering
		\includegraphics[width=\textwidth]{./fig2}
		\caption{Self-chat examples by Blender and PLATO-2.}
		\label{fig:case}
	\end{figure*} 
	
	\subsubsection{Evaluation Metrics}
	We carry out both automatic and human evaluations in the experiments. In automatic evaluation, to assess the model's capacity on lexical diversity, we use the corpus-level metric of distinct-1/2 \cite{li2016diversity}, which is defined as the number of distinct uni- or bi-grams divided by the total number of generated words.
	
	In human evaluation, we employ four utterance-level and dialogue-level metrics, including coherence, informativeness, engagingness and humanness. Three crowd-sourcing workers are asked to score the response/dialogue quality on a scale of [0, 1, 2], with the final score determined through majority voting. The higher score, the better. These criteria are discussed as follows, with scoring details provided in the Appendix.
	\begin{itemize}[leftmargin=*,noitemsep,topsep=0pt]
		\item Coherence is an utterance-level metric, measuring whether the response is relevant and consistent with the context.
		
		\item Informativeness is also an utterance-level metric, evaluating whether the response is informative or not given the context.
		
		\item Engagingness is a dialogue-level metric, assessing whether the annotator would like to talk with the speaker for a long conversation.
		
		\item Humanness is also a dialogue-level metric, judging whether the speaker is a human being or not.
	\end{itemize}
	
	\subsection{Experimental Results}
	In the experiments, we include both static and interactive evaluations. 
	
	\subsubsection{Self-Chat Evaluation}
	Self-chats have been widely used in the evaluation of dialogue systems \cite{li2016deep,bao2019know,roller2020recipes}, where a model plays the role of both partners in the conversation. As compared with human-bot conversations, self-chat logs can be collected efficiently at a cheaper price. As reported in \newcite{li2019acute}, self-chat evaluations exhibit high agreement with the human-bot chat evaluations. In the experiments, we ask the bot to perform self-chats and then invite crowd-sourcing workers to evaluate the dialogue quality.
	
	The way to start the interactive conversation needs special attention. As pointed out by \newcite{roller2020recipes}, if starting with 'Hi!', partners tend to greet with each other and only cover some shallow topics in the short conversation. Therefore, to expose the model's weaknesses and explore the model's limits, we choose to start the interactive conversation with pre-selected topics. We use the classical 200 questions as the start topic \cite{vinyals2015neural} and ask the bot to performance self-chats given the context. There are 10 utterances in each dialogue, including the input start utterance. We carry out automatic evaluation on the 200 self-chat logs and randomly select 50 conversations from 200 self-chat logs for human evaluation. 
	
	The compared models are divided into two groups. The first group includes DialoGPT 345M model and PLATO 310M model. Both of them are trained using Reddit comments and have similar model scales. The second group includes Blender 2.7B model and PLATO 1.6B model. Both of them are trained using Reddit comments and further fine-tuned with BST conversations. In human evaluation, two self-chat logs, which are from the same group and have the same start topic, will be displayed to three crowd-sourcing workers. One example is given in Figure \ref{fig:case}. As suggested in ACUTE-Eval \cite{li2019acute}, we ask crowd-sourcing workers to pay attention to only one speaker within a dialogue. In the evaluation, they need to give scores on coherence and informativeness for each P1's utterance, and assess P1's overall quality on engagingness and humanness. The self-chat evaluation results are summarized in Table \ref{tab:self-chat}. These results demonstrate that PLATO 1.6B model obtains the best performance across human and automatic evaluations. The gap of Blender and PLATO-2 on the corpus-level metric distinct-1/2 suggests that PLATO-2 has a better capacity on lexical diversity. In addition, the difference between these two groups indicates that enlarging model scales and 
	exploiting human annotated conversations help improve the dialogue quality.
	
	\subsubsection{Human-Bot Chat Evaluation}
	In the Chinese evaluation, it is difficult to carry out self-chats for Microsoft XiaoIce, as there is no public available API. Therefore, we collect human-bot conversations through their official Weibo platform. The interactive conversation also starts with a pre-selected topic and continues for 7-14 rounds, where 50 diverse topics are extracted from the high-frequency topics of a commercial chatbot, including travel, movie, hobby and so on. The collected human-bot conversations are distributed to crowd-sourcing workers for evaluation. The human and automatic evaluation results are summarized in Table \ref{tab:interactive}. XiaoIce obtains higher distinct values, 
	which may use a retrieval-based strategy in response generation. The human evaluations demonstrate that our PLATO-2 model achieves significant improvements over XiaoIce across all the 
	human evaluation metrics. 
	
	\begin{table}
		\centering
		\includegraphics[width=0.48\textwidth]{./tab4}
		\caption{Static evaluation results, with best value written in bold.}
		\label{tab:static_1}
	\end{table}
	\begin{figure*}[!htb]
		\centering
		\includegraphics[width=\textwidth]{./fig4}
		\caption{Human-bot chat examples by Microsoft XiaoIce and PLATO-2.}
		\label{fig:case_zh}
	\end{figure*} 
	\subsubsection{Static Evaluation}
	Besides the interactive evaluation, we also include static evaluation to analyze the model's performance. In static evaluation, each model will produce a response towards the given multi-turn context. To compare with Meena, we include their provided 60 static samples in the paper's Appendix and generate corresponding responses with other models. We also include 60 test samples about daily life from Daily Dialog \cite{li2017dailydialog} and 60 test samples about in-depth discussion from Reddit. Given that the measurement of humanness usually needs multi-turn interaction, this metric is excluded from static evaluation. The evaluation results are summarized in Table \ref{tab:static_1}. It can be observed that PLATO-2 is able to produce coherent, informative and engaging responses under different chat scenarios.
	
	\subsection{Discussions}
	\subsubsection{Case Analysis}
	To further analyze the models' features, two self-chat examples of Blender and PLATO-2 are provided in Figure \ref{fig:case}. Although both models are able to produce high-quality engaging conversations, they exhibit distinct discourse styles. Blender tends to switch topics quickly in the short conversation, including alcohol, hobbies, movies and work. The emergence of this style might be related with BST fine-tuning data. For instance, ConvAI2 is about the exchange of personal information between two partners, where topics need to switch quickly to know more about each other. Due to the task settings of data collection, some human annotated conversations might be a little unnatural. Nevertheless, fine-tuning with BST conversations is essential to mitigate undesirable toxic traits of large corpora and emphasize desirable skills of human conversations. 
	
	Distinct with Blender, PLATO-2 can stick to the start topic and conduct in-depth discussions. The reasons might be two-fold. On the one hand, our model is able to generate diverse and informative responses with the accurate modeling of one-to-many relationship. On the other hand, the evaluation model helps select the coherent response and stick to current topic. We also asked crowd-sourcing workers to compare the responses generated by these two models via pairwise ranking. The comparison result is shown in Table \ref{tab:thorough}, which also verifies our above analysis on discourse styles. 
	
	Besides, we also provide two human-bot chat examples of XiaoIce and PLATO-2 in Figure \ref{fig:case_zh}, with original interactive logs shown on the left and translated logs on the right. It can be observed that some responses produced by XiaoIce are not coherent with the contexts and there are some abrupt changes of topics. By contrast, the interaction with PLATO-2 is more coherent and engaging. 
	\begin{table}
		\centering
		\includegraphics[width=0.48\textwidth]{./tab5}
		\caption{Thoroughness with regard to the start topic.}
		\label{tab:thorough}
	\end{table} 
	\begin{table}
		\centering
		\includegraphics[width=0.48\textwidth]{./tab6}
		\caption{Comparison of different score functions in response selection, with best value written in bold.}
		\label{tab:selection}
	\end{table} 
	
	\subsubsection{Response Selection Comparison}
	We carry out more experiments to compare the performance of distinct score functions in response selection. Firstly, one Chinese response selection dataset is constructed: 100 dialogue contexts are selected from the test set and 10 candidate responses are retrieved for each context with a commercial chatbot. Secondly, we ask crowd-sourcing workers to annotate the label whether the candidate response is coherent with the context. Thirdly, we train three 333M parameter models as the score function, including the forward response generation probability , the backward context recover probability  and the bi-directional coherence probability . Their results on the annotated response selection dataset are summarized in Table \ref{tab:selection}. The metrics of mean average precision (MAP) \cite{baeza1999modern}, mean reciprocal rank (MRR) \cite{voorhees1999trec} and precision at position 1 (P@1) are employed. These results indicate that PLATO-2's evaluation model is better at selecting appropriate responses. 
	


	\section{Related Work}
	Related works include large-scale language models and open-domain dialogue generation.
	
	\noindent \textbf{Large-scale Language Models.} Pre-trained large-scale language models have brought many breakthroughs on various NLP tasks. GPT \cite{radford2018improving} and BERT \cite{devlin2019bert} are representative uni-directional and bi-directional language models, trained on general text corpora. By introducing pre-normalization and modifying weight initialization, GPT-2 \cite{radford2019language} successfully extends the model scale from 117M to 1.5B parameters. To cope with memory constraints, Megatron-LM \cite{shoeybi2019megatron} exploits model parallelism to train an 8.3B parameter model on 512 GPUs. GPT-3 \cite{brown2020language} further trains an 175B parameter autoregressive language model, demonstrating strong performance on many NLP tasks. The development of large-scale language models is also beneficial to the task of dialogue generation. 
	
	\noindent \textbf{Open-domain Dialogue Generation.} On the basis of GPT-2, DialoGPT \cite{zhang2019dialogpt} is trained for response generation using Reddit comments. To obtain a human-like open-domain chatbot, Meena \cite{adiwardana2020towards} scales up the network parameters to 2.6B and utilizes more social media conversations in the training process. To emphasize desirable conversational skills of engagingness, knowledge, empathy and personality, Blender \cite{roller2020recipes} further fine-tunes the pre-trained model with human annotated conversations. Besides the above attempts on model scale and data selection, PLATO \cite{bao2019know} introduces discrete latent variable to tackle the inherent one-to-many mapping problem to improve response quality. In this work, we further scale up PLATO to PLATO-2 and discuss its effective training via curriculum learning.
	
	\section{Conclusion}
	In this work, we discuss the effective training of open-domain chatbot PLATO-2 via curriculum learning, where two stages are involved. In the first stage, one coarse-grained model is trained for general response generation. In the second stage, two models of fine-grained generation and evaluation are trained for diverse response generation and response coherence estimation. Experimental results demonstrate that PLATO-2 achieves substantial improvements over the state-of-the-art methods in both Chinese and English evaluations.  
	
	\section*{Acknowledgments}
	We would like to thank Jingzhou He, and Tingting Li for the help on resource coordination; Daxiang Dong, and Pingshuo Ma for the support on PaddlePaddle; Yu Sun, Yukun Li, and Han Zhang for the assistance with infrastructure and implementation. This work was supported by the Natural Key Research and Development Project of China (No. 2018AAA0101900).
	
	\bibliography{bibtex}
	\bibliographystyle{acl_natbib}
	
	\clearpage
	\appendix
	\section{Scoring Criteria}
	\begin{table}[!hbt]
		\centering
		\includegraphics[width=0.48\textwidth]{./tab7}
		\caption{Score details of four metrics in human evaluation.}
		\label{tab:criteria}
	\end{table}
	
\end{document}
