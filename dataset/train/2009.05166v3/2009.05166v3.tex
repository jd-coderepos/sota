\begin{table}[t!]
\resizebox{1.0\columnwidth}{!}{

\begin{tabular}{lcccccc}
\toprule
Benchmark & Task & Dataset  & \#train & \#dev & \#test & \#languages \\
\midrule
\multirow{9}{*}{XTREME} &
\multirow{2}{*}{Classification} & XNLI & 392K & 2.5K & 5K & 15 \\
& & PAWS-X & 49.4K & 2K & 2K & 7 \\
\cmidrule{2-7}
& \multirow{2}{*}{Struct. pred. } & POS & 21K & 4K & 47-20K & 33 \\
& & NER & 20K & 10K & 1K-10K & 40 \\
\cmidrule{2-7}
& \multirow{3}{*}{QA} & XQuAD & \multirow{2}{*}{87K} &  \multirow{2}{*}{34K} & 1190 & 11 \\
& & MLQA & & & 4.5K-11K & 7 \\
& & TyDiQA-GoldP &  3.7K & 0.6K & 0.3K–2.7K & 9 \\
\cmidrule{2-7}
& \multirow{2}{*}{Retrieval} & BUCC & - & - & 1.9K–14K & 5 \\
& & Tatoeba & - & - & 1K & 33 \\
\midrule
\multirow{9}{*}{XGLUE} & 
\multirow{6}{*}{Classification} & XNLI & 392K & 2.5K & 5K & 15 \\
& & PAWS-X & 49.4K & 2K & 2K & 4 \\
& & NC & 100K & 10K & 10K & 5 \\
& & QADSM & 100K & 10K & 10K & 3 \\
& & WPR & 100K & 10K & 10K & 7 \\
& & QAM & 100K & 10K & 10K & 3 \\
\cmidrule{2-7}
& \multirow{2}{*}{Struct. pred. } & POS & 25.4K & 1.0K & 0.9K & 18 \\
& & NER & 15.0K & 2.8K & 3.4K & 4 \\
\cmidrule{2-7}
& QA & MLQA & 87.6K & 0.6K & 5.7K & 7 \\
\bottomrule
\end{tabular}}
\caption{Statistics of the datasets in XTREME and XGLUE. \#train, \#dev and \#test are the numbers of examples in the training, dev and test sets, respectively. For dev and test set, the number is for each target language. \#languages is the number of target languages in the test set. Note that language generation tasks in XGLUE are not included.}
\label{tbl:stat}
\end{table} \begin{table*}[!htb]
\centering
\begin{adjustbox}{scale=0.95,center}
\begin{tabular}{lccccc}
\toprule
Model & Avg & Pair sentence & Structured prediction & Question answering & Sentence retrieval \\
\midrule
XLM             & 55.8 & 75.0 & 65.6  & 43.9 & 44.7 \\
MMTE            & 59.3 & 74.3 & 65.3  & 52.3 & 48.9 \\
mBERT           & 59.6 & 73.7 & 66.3  & 53.8 & 47.7 \\
XLM-R           & 68.2 & 82.8  & 69.0 & 62.3 & 61.6 \\
X-STILTs         & 73.5 & 83.9  & 69.4 & 67.2 & 76.5 \\
VECO            & 74.8	& 84.7 & 70.4 & 67.2 & 80.5 \\
\midrule
\textsc{Filter}            & \textbf{77.0} & \textbf{87.5} & \textbf{71.9} & \textbf{68.5} & \textbf{84.5} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Results on the test set of XTREME. \textsc{Filter} achieves new state of the art at the time of submission (Sep. 8, 2020). For TydiQA-GoldP dataset, we use additional SQuAD v1.1 English training data. The score on question answering is calculated by the average of EM and F1 scores on three datasets. () indicates unpublished work.
}
\label{tbl:leaderboard}
\end{table*} \begin{table*}[t!]
\centering
\begin{adjustbox}{scale=0.95,center}
\begin{tabular}{lcccccccccc}
\toprule

Model & Avg & NER & POS	& NC & MLQA & XNLI & PAWS-X & QADSM & WPR & QAM \\
\midrule
Unicoder & 76.1 & 79.7 & 79.6	& \textbf{83.5} & 66.0 & 75.3 & 90.1 & 68.4 & 73.9 & 68.9 \\
\midrule
\textsc{Filter} & \textbf{80.1} & \textbf{82.6} & \textbf{81.6} & \textbf{83.5} & \textbf{76.2} & \textbf{83.9} & \textbf{93.8} & \textbf{71.4} & \textbf{74.7} & \textbf{73.4} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Results on the test set of XGLUE. \textsc{Filter} achieves new state of the art at the time of submission (Sep. 14, 2020). Note that cross-lingual language generation tasks are not included.
Leaderboard: \href{https://microsoft.github.io/XGLUE}{https://microsoft.github.io/XGLUE}.}
\label{tbl:xglue_leaderboard}
\end{table*} \section{Experiments}
In this section, we present experimental results on the XTREME and XGLUE benchmarks and provide detailed analysis on the effectiveness of \textsc{Filter}.


\subsection{Datasets}
There are nine datasets in both XTREME~\cite{hu2020xtreme} and XGLUE~\cite{liang2020xglue} benchmarks for cross-lingual language understanding, which can be grouped into four categories (Classification, Structured Prediction, QA, and Retrieval). The statistics of each dataset is summarized in Table~\ref{tbl:stat}. Note that cross-lingual language generation tasks in XGLUE are not included. 

\noindent\textbf{Cross-lingual Sentence Classification} includes two common tasks: () Cross-lingual Natural Language Inference (XNLI)~\cite{conneau2018xnli}, and () Cross-lingual Paraphrase Adversaries from Word Scrambling (PAWS-X)~\cite{yang2019paws}. 
In XGLUE, they further include another four practical tasks selected from Search, Ads and News scenarios: News Classification, Query-Ad Matching, Web Page Ranking and QA Matching.


\noindent\textbf{Cross-lingual Structured Prediction} includes two tasks: POS tagging and NER. 
In XTREME, the Wikiann dataset~\cite{pan2017cross} is used for experiments, and in XGLUE, they use a subset of two tasks from CoNLL-2002~\cite{tjong-kim-sang-2002-introduction} and CoNLL-2003 NER~\cite{tjong-kim-sang-de-meulder-2003-introduction}.

\noindent\textbf{Cross-lingual Question Answering} includes three tasks: () Cross-lingual Question Answering (XQuAD)~\cite{Artetxe:etal:2019}, () Multilingual Question Answering (MLQA)~\cite{lewis2019mlqa}, and () the gold passage version of the Typologically Diverse Question Answering dataset (TyDiQA-GoldP)~\cite{clark2020tydi}.


\noindent\textbf{Cross-lingual Sentence Retrieval} includes two tasks: BUCC~\cite{zweigenbaum2018overview} and Tatoeba~\cite{artetxe2019massively}. For leaderboard submission, we apply models trained on XNLI directly on these two datasets for inference.

\begin{table*}[t!]
\centering
\begin{adjustbox}{scale=0.95,center}
\begin{tabular}{lccccccc}
\toprule
\multirow{2}{*}{Model} & \multicolumn{2}{c}{Pair sentence} & \multicolumn{2}{c}{Structured prediction} & \multicolumn{3}{c}{Question answering}  \\
& XNLI & PAWS-X & POS & NER & XQuAD & MLQA & TyDiQA-GoldP \\
\midrule
Metrics & Acc. & Acc. & F1 & F1 & F1 / EM & F1 / EM & F1 / EM  \\
\midrule
\multicolumn{8}{l}{\emph{Cross-lingual zero-shot transfer (models are trained on English data)}} \\
\midrule
mBERT          & 65.4 & 81.9 & 70.3 & 62.2 & 64.5 / 49.4 & 61.4 / 44.2 & 59.7 / 43.9 \\
XLM            & 69.1 & 80.9 & 70.1 & 61.2 & 59.8 / 44.3 & 48.5 / 32.6 & 43.6 / 29.1 \\
XLM-R          & 79.2 & 86.4 & 72.6 & 65.4 & 76.6 / 60.8 & 71.6 / 53.2 & 65.1 / 45.0 \\
InfoXLM        & 81.4 & - & - & - & - / - & 73.6 / 55.2 & - / - \\
X-STILTs        & 80.0 & 87.9 & 74.4 & 64.0 & 78.7 / 63.3 & 72.4 / 53.7 & \textbf{76.0} / \textbf{59.5} \\
\midrule
\multicolumn{8}{l}{\emph{Translate-train (models are trained on English training data and its translated data on the target language)}} \\
\midrule
mBERT                    & 74.0 & 86.3 & -    & -    & 70.0 / 56.0 & 65.6 / 48.0 & 55.1 / 42.1 \\
mBERT, multi-task        & 75.1 & 88.9 & -    & -    & 72.4 / 58.3 & 67.6 / 49.8 & 64.2 / 49.3 \\
XLM-R, multi-task (Ours)  & 82.6 & 90.4 &  -   & -    & 80.2 / 65.9 & 72.8 / 54.3 & 66.5 / 47.7 \\
\midrule
\textsc{Filter} (Ours)                & 83.6   & 91.2 & 75.5 & 66.7 & 82.3 / 67.8  & 75.8 / 57.2 & 68.1 / 49.7 \\
\textsc{Filter} + Self-Teaching (Ours)  & \textbf{83.9}   & \textbf{91.4} & \textbf{76.2} & \textbf{67.7} & \textbf{82.4} / \textbf{68.0}  & \textbf{76.2} / \textbf{57.7} & 68.3 / 50.9 \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Overall test results on three different categories of cross-lingual language understanding tasks. Results of mBERT~\cite{devlin-etal-2019-bert}, XLM~\cite{lample2019cross} and XLM-R~\cite{conneau2019unsupervised} are from XTREME~\cite{hu2020xtreme}. InfoXLM~\cite{chi2020infoxlm} only provides results on XNLI and MLQA. We also experimented on translate-train with XLM-R as an additional baseline for fair comparison with \textsc{Filter}. 
}
\label{tbl:detailed_results_full}
\end{table*} 
\subsection{Implementation Details}

Our implementation is based on HuggingFace's Transformers~\cite{Wolf2019HuggingFacesTS}.
We leverage the pre-trained XLM-R model~\cite{conneau2019unsupervised} to initialize our \textsc{Filter}, which contains 24 layers, each layer with 1,024 hidden states. For fair comparison to XLM-R, each transformer layer in \textsc{Filter} is shared for encoding both source and target languages, so that the total number of parameters are exactly the same as XLM-R.

We conduct experiments on 8 Nvidia V100-32GB GPU cards for model finetuning, and set batch size to 64 for all tasks. 
For self-teaching loss, we set the weight of the KL loss to 1.0 for structured prediction tasks where no labels are available in the target language.
We set the weight of KL loss for classification and QA tasks to 0.5 and 0.1 respectively, by searching over [0.1, 0.3, 0.5]. 
As the official XTREME repo\footnote{https://github.com/google-research/xtreme} does not provide translated target language data for POS and NER,
we use Microsoft Machine Translator\footnote{https://azure.microsoft.com/en-us/services/cognitive-services/translator/} for translation.
More details on translation data and model hyper-parameters are provided in Appendix.




\begin{table*}[t!]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{l|ccccccccccccccc|c}
\toprule
Model & en & ar & bg & de & el & es & fr & hi & ru & sw & th & tr & ur & vi & zh & avg \\
\midrule
mBERT                     & 80.8 & 64.3 & 68.0 & 70.0 & 65.3 & 73.5 & 73.4 & 58.9 & 67.8 & 49.7 & 54.1 & 60.9 & 57.2 & 69.3 & 67.8 & 65.4 \\
MMTE                      & 79.6 & 64.9 & 70.4 & 68.2 & 67.3 & 71.6 & 69.5 & 63.5 & 66.2 & 61.9 & 66.2 & 63.6 & 60.0 & 69.7 & 69.2 & 67.5 \\
XLM                       & 82.8 & 66.0 & 71.9 & 72.7 & 70.4 & 75.5 & 74.3 & 62.5 & 69.9 & 58.1 & 65.5 & 66.4 & 59.8 & 70.7 & 70.2 & 69.1 \\
XLM-R                     & 88.7 & 77.2 & 83.0 & 82.5 & 80.8 & 83.7 & 82.2 & 75.6 & 79.1 & 71.2 & 77.4 & 78.0 & 71.7 & 79.3 & 78.2 & 79.2 \\
\midrule
\shortstack{XLM-R (\emph{translate-train})}  & 88.6 & 82.2 & 85.2 & 84.5 & 84.5 & 85.7 & 84.2 & 80.8 & 81.8 & 77.0 & 80.2 & 82.1 & 77.7 & 82.6 & 82.7 & 82.6 \\
\textsc{Filter}          & \textbf{89.7} & 83.2 & 86.2 & 85.5 & 85.1 & \textbf{86.6} & 85.6 & 80.9 & 83.4 & 78.2 & \textbf{82.2} & 83.1 & 77.4 & 83.7 & 83.7 & 83.6 \\
\textsc{Filter} + Self-Teaching & 89.5 & \textbf{83.6} & \textbf{86.4} & \textbf{85.6} & \textbf{85.4} & \textbf{86.6} & \textbf{85.7} & \textbf{81.1} & \textbf{83.7} & \textbf{78.7} & 81.7 & \textbf{83.2} & \textbf{79.1} & \textbf{83.9} & \textbf{83.8} & \textbf{83.9} \\

\bottomrule
\end{tabular}
}
\caption{XNLI accuracy scores for each language. Results of mBERT, MMTE, XLM and XLM-R are from XTREME~\cite{hu2020xtreme}. \emph{mtl} denotes translate-train in multi-task version.}
\label{tbl:xnli_detailed_results}
\end{table*} 



\subsection{Baselines}
We compare \textsc{Filter} with previous state-of-the-art multilingual models: \begin{itemize}[itemsep=1pt,topsep=2pt,leftmargin=12pt]
\item \textbf{Pre-trained models}: \textit{mBERT}~\cite{devlin-etal-2019-bert}, \textit{XLM}~\cite{lample2019cross}, \textit{XLM-R}~\cite{conneau2019unsupervised}, \textit{MMTE}~\cite{siddhant2020evaluating}, \textit{InfoXLM}~\cite{chi2020infoxlm},
\textit{Unicoder}~\cite{huang2019unicoder} pre-train Transformer models on large-scale multi-lingual dataset including machine translation data.
\item \textbf{Data augmentation}: 	X-STILTs~\cite{phang2020english} first finetunes XLM-R on an additional intermediate auxiliary task, then further finetunes on the target task.
\item \textbf{Translate-train}~\cite{hu2020xtreme} finetunes cross-lingual pre-trained language model XLM-R on  English training data and all translated data by using Google's in-house Machine Translation system.


\end{itemize}

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{effect_km.png}
\caption{Results on the dev set of PAWS-X, POS and MLQA  with different  and  values.}
\label{fig:abl_km}
\end{figure*}


\begin{table*}[t!]
\centering
\resizebox{1.55\columnwidth}{!}{
\begin{tabular}{lccccc|c|cc}
\toprule
Model         & XNLI & PAWS-X & XQuAD & MLQA & TyDiQA-GoldP & Avg  & POS  & NER \\
\midrule
mBERT\cite{hu2020xtreme}           & 16.5 & 14.1   & 25.0  & 27.5 & 22.2 & 21.1 & 25.5 & 23.6 \\

XLM-R\cite{hu2020xtreme}            & 10.2 & 12.4   & 16.3 & 19.1 & 13.3 & 14.3 & 24.3 & 19.8 \\
Translate-train\cite{hu2020xtreme}  & 7.3  & 9.0    & 17.6 & 22.2 & 24.2 & 16.1 & - & - \\
\midrule
\textsc{Filter}         & \textbf{6.0}    & \textbf{5.2}    & \textbf{7.3}  & \textbf{15.7} & \textbf{9.2} & \textbf{8.7} & \textbf{19.7} & \textbf{16.3} \\
\bottomrule
\end{tabular}
}
\caption{Analysis on cross-lingual transfer gap of different models on XTREME benchmark (except for retrieval  task). A lower gap indicates a better cross-lingual transfer model. 
The average score (Avg) is calculated on all classification and QA tasks. 
}
\label{tbl:trans_gap}
\end{table*} 
\subsection{Experimental Results}
Table~\ref{tbl:leaderboard} and \ref{tbl:xglue_leaderboard} summarizes our results on XTREME and XGLUE, outperforming all the leaderboard submissions.
On XTREME, compared to the unpublished state-of-the-art VECO approach, \textsc{Filter} outperforms by 2.8/1.5/1.3/4.0 points on the four categories respectively, achieving an average score of 77.0, an absolute 2.2-point improvement.
Compared to the XLM-R baseline, we achieve an absolute 8.8-point improvement (77.0 vs. 68.2), which is a significant margin. On XGLUE, compared to the Unicoder baseline, \textsc{Filter} achieves an absolute 4.0-point improvement.

Table~\ref{tbl:detailed_results_full} provides more detailed results on different tasks in XTREME.
First, we build a strong translate-train baseline using XLM-R as the backbone, which already outperforms previous state-of-the-art models by a significant margin on every dataset.
Second, compared to the translate-train XLM-R baseline, \textsc{Filter} further provides 0.9 and 2.28 points improvement on average on classification and question answering tasks.
Lastly, the self-teaching loss further boosts the performance of \textsc{Filter} on every dataset, especially on POS and NER tasks.


To provide a deeper look into the model performance across languages, Table~\ref{tbl:xnli_detailed_results} provides results on each language, taking the XNLI dataset as an example. Results show that \textsc{Filter} outperforms all baselines on each language.
Complete results on other datasets are provided in Appendix.

\subsection{Ablation Analysis}
Below, we provide a detailed analysis to better understand the effectiveness of \textsc{Filter} and the self-teaching loss on different tasks. 
In general, we observe that different tasks need different numbers of ``local'' transformer layers () and
intermediate fusion layers ()
Furthermore, the self-teaching loss is helpful on all tasks, especially on tasks lacking labels in the target languages.

\noindent\textbf{Effect of Fusing Languages} 
As shown in Table~\ref{tbl:detailed_results_full} and discussed above, \textsc{Filter} outperforms the translate-train baseline by a significant margin on classification and QA datasets, demonstrating the effectiveness of fusing languages. 
For POS and NER, there is no translate-train baseline as labels are unavailable in translated target language. 
Nonetheless, \textsc{Filter} improves XLM-R by 2.9 and 1.3 points, thanks to the use of intermediate cross-attention between language pair.
For the simple concatenation baseline, its performance can be analyzed by setting  in Figure~\ref{fig:abl_km}. Compared to \textsc{Filter}, the performance drops 2.5/15.2 points on PAWS-X and POS datasets.
For MLQA, there is only a minor drop.
We hypothesize that for simple classification tasks, single-language input already provides rich information, while concatenating the paired language input directly at the very beginning introduces more noise, therefore making the model more difficult to train.
Overall, performing cross-attention between the language pair in intermediate layers performs the best. 


\noindent\textbf{Effect of Intermediate Fusion Layers}
Figure~\ref{fig:abl_km} shows the results on the dev sets with different  and  combinations (see Figure~\ref{fig:model} for its definition). 
We perform experiments on PAWS-X, POS and MLQA, and consider them as representative datasets for classification, structured prediction and question answering tasks.
For MLQA, performance is consistently improved with the number of intermediate fusion layers increasing, resulting in 2.6 points improvement from  to  when  is set to 1.
By contrast, the performance on PAWS-X and POS drops significantly when the number of intermediate fusion layers increases.
For example, when  is set to 1, accuracy decreases by 2.5/16.5 points from  to  on PAWS-X and POS datasets.

\noindent\textbf{Effect of Local Transformer Layers}
As shown in Figure~\ref{fig:abl_km}, for POS and MLQA, \textsc{Filter} performs better when using more local transformer layers.
For example, when  is set to 10, we observe performance  improvement by setting  to  sequentially.
On the contrary, for PAWS-X, when , the performance of setting  is better than setting .
This suggests that we should use more local layers for complex tasks such as QA and structured prediction, and fewer local layers for classification tasks. 


\noindent\textbf{Effect of Self-teaching Loss}
As can be seen from Table~\ref{tbl:detailed_results_full}, for POS and NER, the use of self-teaching loss improves \textsc{Filter} by 0.7 and 1.0 points. This confirms that self-teaching loss is very helpful in addressing the no-label issue for target languages.
For classification and question answering tasks, we observe minor improvement, which is expected, as ground-truth labels are available for target languages, and adding the self-teaching loss only provides some label smoothing effect. 


\noindent\textbf{Cross-lingual Transfer Gap}
Table~\ref{tbl:trans_gap} shows analysis results of cross-lingual gap of different models, by calculating the difference between the performance on English test set and the average performance of other target languages.
We observe that \textsc{Filter} reduces the cross-lingual gap significantly among all tasks compared to mBERT, XLM-R and translate-train baselines.
The transfer learning gap of \textsc{Filter} is reduced by additional 2.5 and 10.6 points on average for classification and QA tasks, respectively, compared to the translate-train baseline respectively. 
For structured prediction tasks, the gap reduces even further, but a large gap still exists, indicating that this task demands stronger cross-lingual transfer. 


\section{Conclusion}
We present \textsc{Filter}, a new approach for cross-lingual language understanding that  
first encodes paired language input independently, then fuses them in the intermediate layers of XLM, and finally performs further language-specific encoding.
An additional self-teaching loss is proposed for enhanced model training. By combining \textsc{Filter} and self-teaching loss, we achieve new state of the art on the challenging XTREME and XGLUE benchmarks.
Future work points to more effective ways of automatically discovering the best configuration of \textsc{Filter} for different cross-lingual tasks.