\documentclass[11pt]{article}
\usepackage[left=1.1in,right=1.1in,top=1.3in,bottom=1.3in]{geometry}
\usepackage{xcolor}
\usepackage{natbib}
\usepackage{fancyhdr}
\usepackage{enumitem}
\setlist[itemize]{leftmargin=*}
\setlist[enumerate]{leftmargin=*}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{svg}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{bm}
\usepackage{comment}
\usepackage{hyperref}
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{booktabs}       \usepackage{amssymb}\usepackage{pifont}\newcommand{\cmark}{\ding{51}}\newcommand{\xmark}{\ding{55}}\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{dsfont}
\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{xspace}
\usepackage{amsthm}
\newcommand{\C}{\mathbb{C}} \newcommand{\F}{\mathbb{F}} \newcommand{\N}{\mathbb{N}} \newcommand{\Q}{\mathbb{Q}} \newcommand{\R}{\mathbb{R}} \newcommand{\Z}{\mathbb{Z}} 

\newcommand{\tr}{\mathop{\mathrm{tr}}}
\newcommand{\diag}{\mathop{\mathrm{diag}}}
\newcommand{\rank}{\mathop{\mathsf{rank}}}
\newcommand{\vect}{\mathop{\mathrm{vec}}}
\newcommand{\nnz}{\mathop{\mathrm{nnz}}}

\newcommand*{\mini}{\mathop{\mathrm{minimize}}}
\newcommand*{\maxi}{\mathop{\mathrm{maximize}}}
\newcommand*{\argmin}{\mathop{\mathrm{argmin}}}
\newcommand*{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand*{\arginf}{\mathop{\mathrm{arginf}}}
\newcommand*{\argsup}{\mathop{\mathrm{argsup}}}
\newcommand{\st}{\text{subject to: }}
\newcommand{\dom}{\mathop{\mathrm{dom}}}
\newcommand{\grad}{{\nabla}}
\newcommand{\hess}{\grad^{2}}

\newcommand{\cov}{\mathrm{Cov}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\tv}{\mathsf{TV}}
\newcommand{\EE}{\mathsf{E}} \newcommand{\HH}{\mathsf{H}} \newcommand{\id}{\mathsf{Id}} \newcommand{\KK}{\mathds{K}} \newcommand{\MM}{\mathsf{M}} 

\newcommand{\RRbar}{\overline{\RR}} \newcommand{\RRi}{\RR_{\infty}} \newcommand{\pp}{\mathsf{p}} \newcommand{\II}{\mathsf{I}} 



\newcommand{\sgn}{\mathop{\mathrm{sign}}}
\newcommand{\traj}{\mathop{\mathrm{Traj}}}
\newcommand{\conv}{\mathrm{conv}}
\newcommand{\cone}{\mathrm{cone}}
\newcommand{\const}{\mathrm{constant}}
\newcommand{\ri}{\mathop{\mathrm{ri}}}
\newcommand{\sri}{\mathop{\mathrm{sri}}}
\newcommand{\cl}{\mathop{\mathrm{cl}}}
\newcommand{\intr}{\mathop{\mathrm{int}}}
\newcommand{\bd}{\mathop{\mathrm{bd}}}
\newcommand{\emp}{\mathop{\mathrm{emp}}}
\newcommand{\core}{\mathop{\mathrm{core}}}
\newcommand{\epi}{\mathop{\mathrm{epi}}}
\newcommand{\co}{\mathop{\mathrm{co}}}
\newcommand{\med}{\mathop{\mathrm{med}}}
\newcommand{\eproof}{}





\newcommand{\atil}{\tilde{a}}
\newcommand{\btil}{\tilde{b}}
\newcommand{\ctil}{\tilde{c}}
\newcommand{\dtil}{\tilde{d}}
\newcommand{\etil}{\tilde{e}}
\newcommand{\ftil}{\tilde{f}}
\newcommand{\gtil}{\tilde{g}}
\newcommand{\htil}{\tilde{h}}
\newcommand{\itil}{\tilde{i}}
\newcommand{\jtil}{\tilde{j}}
\newcommand{\ktil}{\tilde{k}}
\newcommand{\ltil}{\tilde{l}}
\newcommand{\mtil}{\tilde{m}}
\newcommand{\ntil}{\tilde{n}}
\newcommand{\otil}{\tilde{o}}
\newcommand{\ptil}{\tilde{p}}
\newcommand{\qtil}{\tilde{q}}
\newcommand{\rtil}{\tilde{r}}
\newcommand{\stil}{\tilde{s}}
\newcommand{\ttil}{\tilde{t}}
\newcommand{\util}{\tilde{u}}
\newcommand{\vtil}{\tilde{v}}
\newcommand{\wtil}{\tilde{w}}
\newcommand{\xtil}{\tilde{x}}
\newcommand{\ytil}{\tilde{y}}
\newcommand{\ztil}{\tilde{z}}

\newcommand{\ahat}{\hat{a}}
\newcommand{\bhat}{\hat{b}}
\newcommand{\chat}{\hat{c}}
\newcommand{\dhat}{\hat{d}}
\newcommand{\ehat}{\hat{e}}
\newcommand{\fhat}{\hat{f}}
\newcommand{\ghat}{\hat{g}}
\newcommand{\hhat}{\hat{h}}
\newcommand{\ihat}{\hat{i}}
\newcommand{\jhat}{\hat{j}}
\newcommand{\khat}{\hat{k}}
\newcommand{\lhat}{\hat{l}}
\newcommand{\mhat}{\hat{m}}
\newcommand{\nhat}{\hat{n}}
\newcommand{\ohat}{\hat{o}}
\newcommand{\phat}{\hat{p}}
\newcommand{\qhat}{\hat{q}}
\newcommand{\rhat}{\hat{r}}
\newcommand{\shat}{\hat{s}}
\newcommand{\that}{\hat{t}}
\newcommand{\uhat}{\hat{u}}
\newcommand{\vhat}{\hat{v}}
\newcommand{\what}{\hat{w}}
\newcommand{\xhat}{\hat{x}}
\newcommand{\yhat}{\hat{y}}
\newcommand{\zhat}{\hat{z}}

\newcommand{\abar}{\bar{a}}
\newcommand{\bbar}{\bar{b}}
\newcommand{\cbar}{\bar{c}}
\newcommand{\dbar}{\bar{d}}
\newcommand{\ebar}{\bar{e}}
\newcommand{\fbar}{\bar{f}}
\newcommand{\gbar}{\bar{g}}
\newcommand{\hbr}{\bar{h}}
\newcommand{\ibar}{\bar{i}}
\newcommand{\jbar}{\bar{j}}
\newcommand{\kbar}{\bar{k}}
\newcommand{\lbar}{\bar{l}}
\newcommand{\mbar}{\bar{m}}
\newcommand{\nbar}{\bar{n}}
\newcommand{\obar}{\bar{o}}
\newcommand{\pbar}{\bar{p}}
\newcommand{\qbar}{\bar{q}}
\newcommand{\rbar}{\bar{r}}
\newcommand{\sbar}{\bar{s}}
\newcommand{\tbar}{\bar{t}}
\newcommand{\ubar}{\bar{u}}
\newcommand{\vbar}{\bar{v}}
\newcommand{\wbar}{\bar{w}}
\newcommand{\xbar}{\bar{x}}
\newcommand{\ybar}{\bar{y}}
\newcommand{\zbar}{\bar{z}}

\newcommand{\abf}{\mathbf{a}}
\newcommand{\bbf}{\mathbf{b}}
\newcommand{\cbf}{\mathbf{c}}
\newcommand{\dbf}{\mathbf{d}}
\newcommand{\ebf}{\mathbf{e}}
\newcommand{\fbf}{\mathbf{f}}
\newcommand{\gbf}{\mathbf{g}}
\newcommand{\hbf}{\mathbf{h}}
\newcommand{\ibf}{\mathbf{i}}
\newcommand{\jbf}{\mathbf{j}}
\newcommand{\kbf}{\mathbf{k}}
\newcommand{\lbf}{\mathbf{l}}
\newcommand{\mbf}{\mathbf{m}}
\newcommand{\nbf}{\mathbf{n}}
\newcommand{\obf}{\mathbf{o}}
\newcommand{\pbf}{\mathbf{p}}
\newcommand{\qbf}{\mathbf{q}}
\newcommand{\rbf}{\mathbf{r}}
\newcommand{\sbf}{\mathbf{s}}
\newcommand{\tbf}{\mathbf{t}}
\newcommand{\ubf}{\mathbf{u}}
\newcommand{\vbf}{\mathbf{v}}
\newcommand{\wbf}{\mathbf{w}}
\newcommand{\xbf}{\mathbf{x}}
\newcommand{\ybf}{\mathbf{y}}
\newcommand{\zbf}{\mathbf{z}}

\newcommand{\abftil}{\tilde{\abf}}
\newcommand{\bbftil}{\tilde{\bbf}}
\newcommand{\cbftil}{\tilde{\cbf}}
\newcommand{\dbftil}{\tilde{\dbf}}
\newcommand{\ebftil}{\tilde{\ebf}}
\newcommand{\fbftil}{\tilde{\fbf}}
\newcommand{\gbftil}{\tilde{\gbf}}
\newcommand{\hbftil}{\tilde{\hbf}}
\newcommand{\ibftil}{\tilde{\ibf}}
\newcommand{\jbftil}{\tilde{\jbf}}
\newcommand{\kbftil}{\tilde{\kbf}}
\newcommand{\lbftil}{\tilde{\lbf}}
\newcommand{\mbftil}{\tilde{\mbf}}
\newcommand{\nbftil}{\tilde{\nbf}}
\newcommand{\obftil}{\tilde{\obf}}
\newcommand{\pbftil}{\tilde{\pbf}}
\newcommand{\qbftil}{\tilde{\qbf}}
\newcommand{\rbftil}{\tilde{\rbf}}
\newcommand{\sbftil}{\tilde{\sbf}}
\newcommand{\tbftil}{\tilde{\tbf}}
\newcommand{\ubftil}{\tilde{\ubf}}
\newcommand{\vbftil}{\tilde{\vbf}}
\newcommand{\wbftil}{\tilde{\wbf}}
\newcommand{\xbftil}{\tilde{\xbf}}
\newcommand{\ybftil}{\tilde{\ybf}}
\newcommand{\zbftil}{\tilde{\zbf}}

\newcommand{\abfhat}{\hat{\abf}}
\newcommand{\bbfhat}{\hat{\bbf}}
\newcommand{\cbfhat}{\hat{\cbf}}
\newcommand{\dbfhat}{\hat{\dbf}}
\newcommand{\ebfhat}{\hat{\ebf}}
\newcommand{\fbfhat}{\hat{\fbf}}
\newcommand{\gbfhat}{\hat{\gbf}}
\newcommand{\hbfhat}{\hat{\hbf}}
\newcommand{\ibfhat}{\hat{\ibf}}
\newcommand{\jbfhat}{\hat{\jbf}}
\newcommand{\kbfhat}{\hat{\kbf}}
\newcommand{\lbfhat}{\hat{\lbf}}
\newcommand{\mbfhat}{\hat{\mbf}}
\newcommand{\nbfhat}{\hat{\nbf}}
\newcommand{\obfhat}{\hat{\obf}}
\newcommand{\pbfhat}{\hat{\pbf}}
\newcommand{\qbfhat}{\hat{\qbf}}
\newcommand{\rbfhat}{\hat{\rbf}}
\newcommand{\sbfhat}{\hat{\sbf}}
\newcommand{\tbfhat}{\hat{\tbf}}
\newcommand{\ubfhat}{\hat{\ubf}}
\newcommand{\vbfhat}{\hat{\vbf}}
\newcommand{\wbfhat}{\hat{\wbf}}
\newcommand{\xbfhat}{\hat{\xbf}}
\newcommand{\ybfhat}{\hat{\ybf}}
\newcommand{\zbfhat}{\hat{\zbf}}

\newcommand{\abfbar}{\bar{\abf}}
\newcommand{\bbfbar}{\bar{\bbf}}
\newcommand{\cbfbar}{\bar{\cbf}}
\newcommand{\dbfbar}{\bar{\dbf}}
\newcommand{\ebfbar}{\bar{\ebf}}
\newcommand{\fbfbar}{\bar{\fbf}}
\newcommand{\gbfbar}{\bar{\gbf}}
\newcommand{\hbfbar}{\bar{\hbf}}
\newcommand{\ibfbar}{\bar{\ibf}}
\newcommand{\jbfbar}{\bar{\jbf}}
\newcommand{\kbfbar}{\bar{\kbf}}
\newcommand{\lbfbar}{\bar{\lbf}}
\newcommand{\mbfbar}{\bar{\mbf}}
\newcommand{\nbfbar}{\bar{\nbf}}
\newcommand{\obfbar}{\bar{\obf}}
\newcommand{\pbfbar}{\bar{\pbf}}
\newcommand{\qbfbar}{\bar{\qbf}}
\newcommand{\rbfbar}{\bar{\rbf}}
\newcommand{\sbfbar}{\bar{\sbf}}
\newcommand{\tbfbar}{\bar{\tbf}}
\newcommand{\ubfbar}{\bar{\ubf}}
\newcommand{\vbfbar}{\bar{\vbf}}
\newcommand{\wbfbar}{\bar{\wbf}}
\newcommand{\xbfbar}{\bar{\xbf}}
\newcommand{\ybfbar}{\bar{\ybf}}
\newcommand{\zbfbar}{\bar{\zbf}}

\newcommand{\asf}{\mathsf{a}}
\newcommand{\bsf}{\mathsf{b}}
\newcommand{\csf}{\mathsf{c}}
\newcommand{\dsf}{\mathsf{d}}
\newcommand{\esf}{\mathsf{e}}
\newcommand{\fsf}{\mathsf{f}}
\newcommand{\gsf}{\mathsf{g}}
\newcommand{\hsf}{\mathsf{h}}
\newcommand{\isf}{\mathsf{i}}
\newcommand{\jsf}{\mathsf{j}}
\newcommand{\ksf}{\mathsf{k}}
\newcommand{\lsf}{\mathsf{l}}
\newcommand{\msf}{\mathsf{m}}
\newcommand{\nsf}{\mathsf{n}}
\newcommand{\osf}{\mathsf{o}}
\newcommand{\psf}{\mathsf{p}}
\newcommand{\qsf}{\mathsf{q}}
\newcommand{\rsf}{\mathsf{r}}
\newcommand{\ssf}{\mathsf{s}}
\newcommand{\tsf}{\mathsf{t}}
\newcommand{\usf}{\mathsf{u}}
\newcommand{\vsf}{\mathsf{v}}
\newcommand{\wsf}{\mathsf{w}}
\newcommand{\xsf}{\mathsf{x}}
\newcommand{\ysf}{\mathsf{y}}
\newcommand{\zsf}{\mathsf{z}}

\newcommand{\att}{\mathtt{a}}
\newcommand{\btt}{\mathtt{b}}
\newcommand{\ctt}{\mathtt{c}}
\newcommand{\dtt}{\mathtt{d}}
\newcommand{\ett}{\mathtt{e}}
\newcommand{\ftt}{\mathtt{f}}
\newcommand{\gtt}{\mathtt{g}}
\newcommand{\htt}{\mathtt{h}}
\newcommand{\itt}{\mathtt{i}}
\newcommand{\jtt}{\mathtt{j}}
\newcommand{\ktt}{\mathtt{k}}
\newcommand{\ltt}{\mathtt{l}}
\newcommand{\mtt}{\mathtt{m}}
\newcommand{\ntt}{\mathtt{n}}
\newcommand{\ott}{\mathtt{o}}
\newcommand{\ptt}{\mathtt{p}}
\newcommand{\qtt}{\mathtt{q}}
\newcommand{\rtt}{\mathtt{r}}
\newcommand{\stt}{\mathtt{s}}
\newcommand{\ttt}{\mathtt{t}}
\newcommand{\utt}{\mathtt{u}}
\newcommand{\vtt}{\mathtt{v}}
\newcommand{\wtt}{\mathtt{w}}
\newcommand{\xtt}{\mathtt{x}}
\newcommand{\ytt}{\mathtt{y}}
\newcommand{\ztt}{\mathtt{z}}

\newcommand{\abld}{\boldsymbol{a}}
\newcommand{\bbld}{\boldsymbol{b}}
\newcommand{\cbld}{\boldsymbol{c}}
\newcommand{\dbld}{\boldsymbol{d}}
\newcommand{\ebld}{\boldsymbol{e}}
\newcommand{\fbld}{\boldsymbol{f}}
\newcommand{\gbld}{\boldsymbol{g}}
\newcommand{\hbld}{\boldsymbol{h}}
\newcommand{\ibld}{\boldsymbol{i}}
\newcommand{\jbld}{\boldsymbol{j}}
\newcommand{\kbld}{\boldsymbol{k}}
\newcommand{\lbld}{\boldsymbol{l}}
\newcommand{\mbld}{\boldsymbol{m}}
\newcommand{\nbld}{\boldsymbol{n}}
\newcommand{\obld}{\boldsymbol{o}}
\newcommand{\pbld}{\boldsymbol{p}}
\newcommand{\qbld}{\boldsymbol{q}}
\newcommand{\rbld}{\boldsymbol{r}}
\newcommand{\sbld}{\boldsymbol{s}}
\newcommand{\tbld}{\boldsymbol{t}}
\newcommand{\ubld}{\boldsymbol{u}}
\newcommand{\vbld}{\boldsymbol{v}}
\newcommand{\wbld}{\boldsymbol{w}}
\newcommand{\xbld}{\boldsymbol{x}}
\newcommand{\ybld}{\boldsymbol{y}}
\newcommand{\zbld}{\boldsymbol{z}}

\newcommand{\Atil}{\tilde{A}}
\newcommand{\Btil}{\tilde{B}}
\newcommand{\Ctil}{\tilde{C}}
\newcommand{\Dtil}{\tilde{D}}
\newcommand{\Etil}{\tilde{E}}
\newcommand{\Ftil}{\tilde{F}}
\newcommand{\Gtil}{\tilde{G}}
\newcommand{\Htil}{\tilde{H}}
\newcommand{\Itil}{\tilde{I}}
\newcommand{\Jtil}{\tilde{J}}
\newcommand{\Ktil}{\tilde{K}}
\newcommand{\Ltil}{\tilde{L}}
\newcommand{\Mtil}{\tilde{M}}
\newcommand{\Ntil}{\tilde{N}}
\newcommand{\Otil}{\tilde{O}}
\newcommand{\Ptil}{\tilde{P}}
\newcommand{\Qtil}{\tilde{Q}}
\newcommand{\Rtil}{\tilde{R}}
\newcommand{\Stil}{\tilde{S}}
\newcommand{\Ttil}{\tilde{T}}
\newcommand{\Util}{\tilde{U}}
\newcommand{\Vtil}{\tilde{V}}
\newcommand{\Wtil}{\tilde{W}}
\newcommand{\Xtil}{\tilde{X}}
\newcommand{\Ytil}{\tilde{Y}}
\newcommand{\Ztil}{\tilde{Z}}

\newcommand{\Ahat}{\hat{A}}
\newcommand{\Bhat}{\hat{B}}
\newcommand{\Chat}{\hat{C}}
\newcommand{\Dhat}{\hat{D}}
\newcommand{\Ehat}{\hat{E}}
\newcommand{\Fhat}{\hat{F}}
\newcommand{\Ghat}{\hat{G}}
\newcommand{\Hhat}{\hat{H}}
\newcommand{\Ihat}{\hat{I}}
\newcommand{\Jhat}{\hat{J}}
\newcommand{\Khat}{\hat{K}}
\newcommand{\Lhat}{\hat{L}}
\newcommand{\Mhat}{\hat{M}}
\newcommand{\Nhat}{\hat{N}}
\newcommand{\Ohat}{\hat{O}}
\newcommand{\Phat}{\hat{P}}
\newcommand{\Qhat}{\hat{Q}}
\newcommand{\Rhat}{\hat{R}}
\newcommand{\Shat}{\hat{S}}
\newcommand{\That}{\hat{T}}
\newcommand{\Uhat}{\hat{U}}
\newcommand{\Vhat}{\hat{V}}
\newcommand{\What}{\hat{W}}
\newcommand{\Xhat}{\hat{X}}
\newcommand{\Yhat}{\hat{Y}}
\newcommand{\Zhat}{\hat{Z}}

\newcommand{\Abar}{\bar{A}}
\newcommand{\Bbar}{\bar{B}}
\newcommand{\Cbar}{\bar{C}}
\newcommand{\Dbar}{\bar{D}}
\newcommand{\Ebar}{\bar{E}}
\newcommand{\Fbar}{\bar{F}}
\newcommand{\Gbar}{\bar{G}}
\newcommand{\Hbar}{\bar{H}}
\newcommand{\Ibar}{\bar{I}}
\newcommand{\Jbar}{\bar{J}}
\newcommand{\Kbar}{\bar{K}}
\newcommand{\Lbar}{\bar{L}}
\newcommand{\Mbar}{\bar{M}}
\newcommand{\Nbar}{\bar{N}}
\newcommand{\Obar}{\bar{O}}
\newcommand{\Pbar}{\bar{P}}
\newcommand{\Qbar}{\bar{Q}}
\newcommand{\Rbar}{\bar{R}}
\newcommand{\Sbar}{\bar{S}}
\newcommand{\Tbar}{\bar{T}}
\newcommand{\Ubar}{\bar{U}}
\newcommand{\Vbar}{\bar{V}}
\newcommand{\Wbar}{\bar{W}}
\newcommand{\Xbar}{\bar{X}}
\newcommand{\Ybar}{\bar{Y}}
\newcommand{\Zbar}{\bar{Z}}

\newcommand{\Abf}{\mathbf{A}}
\newcommand{\Bbf}{\mathbf{B}}
\newcommand{\Cbf}{\mathbf{C}}
\newcommand{\Dbf}{\mathbf{D}}
\newcommand{\Ebf}{\mathbf{E}}
\newcommand{\Fbf}{\mathbf{F}}
\newcommand{\Gbf}{\mathbf{G}}
\newcommand{\Hbf}{\mathbf{H}}
\newcommand{\Ibf}{\mathbf{I}}
\newcommand{\Jbf}{\mathbf{J}}
\newcommand{\Kbf}{\mathbf{K}}
\newcommand{\Lbf}{\mathbf{L}}
\newcommand{\Mbf}{\mathbf{M}}
\newcommand{\Nbf}{\mathbf{N}}
\newcommand{\Obf}{\mathbf{O}}
\newcommand{\Pbf}{\mathbf{P}}
\newcommand{\Qbf}{\mathbf{Q}}
\newcommand{\Rbf}{\mathbf{R}}
\newcommand{\Sbf}{\mathbf{S}}
\newcommand{\Tbf}{\mathbf{T}}
\newcommand{\Ubf}{\mathbf{U}}
\newcommand{\Vbf}{\mathbf{V}}
\newcommand{\Wbf}{\mathbf{W}}
\newcommand{\Xbf}{\mathbf{X}}
\newcommand{\Ybf}{\mathbf{Y}}
\newcommand{\Zbf}{\mathbf{Z}}

\newcommand{\Abftil}{\tilde{\Abf}}
\newcommand{\Bbftil}{\tilde{\Bbf}}
\newcommand{\Cbftil}{\tilde{\Cbf}}
\newcommand{\Dbftil}{\tilde{\Dbf}}
\newcommand{\Ebftil}{\tilde{\Ebf}}
\newcommand{\Fbftil}{\tilde{\Fbf}}
\newcommand{\Gbftil}{\tilde{\Gbf}}
\newcommand{\Hbftil}{\tilde{\Hbf}}
\newcommand{\Ibftil}{\tilde{\Ibf}}
\newcommand{\Jbftil}{\tilde{\Jbf}}
\newcommand{\Kbftil}{\tilde{\Kbf}}
\newcommand{\Lbftil}{\tilde{\Lbf}}
\newcommand{\Mbftil}{\tilde{\Mbf}}
\newcommand{\Nbftil}{\tilde{\Nbf}}
\newcommand{\Obftil}{\tilde{\Obf}}
\newcommand{\Pbftil}{\tilde{\Pbf}}
\newcommand{\Qbftil}{\tilde{\Qbf}}
\newcommand{\Rbftil}{\tilde{\Rbf}}
\newcommand{\Sbftil}{\tilde{\Sbf}}
\newcommand{\Tbftil}{\tilde{\Tbf}}
\newcommand{\Ubftil}{\tilde{\Ubf}}
\newcommand{\Vbftil}{\tilde{\Vbf}}
\newcommand{\Wbftil}{\tilde{\Wbf}}
\newcommand{\Xbftil}{\tilde{\Xbf}}
\newcommand{\Ybftil}{\tilde{\Ybf}}
\newcommand{\Zbftil}{\tilde{\Zbf}}

\newcommand{\Abfhat}{\hat{\Abf}}
\newcommand{\Bbfhat}{\hat{\Bbf}}
\newcommand{\Cbfhat}{\hat{\Cbf}}
\newcommand{\Dbfhat}{\hat{\Dbf}}
\newcommand{\Ebfhat}{\hat{\Ebf}}
\newcommand{\Fbfhat}{\hat{\Fbf}}
\newcommand{\Gbfhat}{\hat{\Gbf}}
\newcommand{\Hbfhat}{\hat{\Hbf}}
\newcommand{\Ibfhat}{\hat{\Ibf}}
\newcommand{\Jbfhat}{\hat{\Jbf}}
\newcommand{\Kbfhat}{\hat{\Kbf}}
\newcommand{\Lbfhat}{\hat{\Lbf}}
\newcommand{\Mbfhat}{\hat{\Mbf}}
\newcommand{\Nbfhat}{\hat{\Nbf}}
\newcommand{\Obfhat}{\hat{\Obf}}
\newcommand{\Pbfhat}{\hat{\Pbf}}
\newcommand{\Qbfhat}{\hat{\Qbf}}
\newcommand{\Rbfhat}{\hat{\Rbf}}
\newcommand{\Sbfhat}{\hat{\Sbf}}
\newcommand{\Tbfhat}{\hat{\Tbf}}
\newcommand{\Ubfhat}{\hat{\Ubf}}
\newcommand{\Vbfhat}{\hat{\Vbf}}
\newcommand{\Wbfhat}{\hat{\Wbf}}
\newcommand{\Xbfhat}{\hat{\Xbf}}
\newcommand{\Ybfhat}{\hat{\Ybf}}
\newcommand{\Zbfhat}{\hat{\Zbf}}

\newcommand{\Abfbar}{\bar{\Abf}}
\newcommand{\Bbfbar}{\bar{\Bbf}}
\newcommand{\Cbfbar}{\bar{\Cbf}}
\newcommand{\Dbfbar}{\bar{\Dbf}}
\newcommand{\Ebfbar}{\bar{\Ebf}}
\newcommand{\Fbfbar}{\bar{\Fbf}}
\newcommand{\Gbfbar}{\bar{\Gbf}}
\newcommand{\Hbfbar}{\bar{\Hbf}}
\newcommand{\Ibfbar}{\bar{\Ibf}}
\newcommand{\Jbfbar}{\bar{\Jbf}}
\newcommand{\Kbfbar}{\bar{\Kbf}}
\newcommand{\Lbfbar}{\bar{\Lbf}}
\newcommand{\Mbfbar}{\bar{\Mbf}}
\newcommand{\Nbfbar}{\bar{\Nbf}}
\newcommand{\Obfbar}{\bar{\Obf}}
\newcommand{\Pbfbar}{\bar{\Pbf}}
\newcommand{\Qbfbar}{\bar{\Qbf}}
\newcommand{\Rbfbar}{\bar{\Rbf}}
\newcommand{\Sbfbar}{\bar{\Sbf}}
\newcommand{\Tbfbar}{\bar{\Tbf}}
\newcommand{\Ubfbar}{\bar{\Ubf}}
\newcommand{\Vbfbar}{\bar{\Vbf}}
\newcommand{\Wbfbar}{\bar{\Wbf}}
\newcommand{\Xbfbar}{\bar{\Xbf}}
\newcommand{\Ybfbar}{\bar{\Ybf}}
\newcommand{\Zbfbar}{\bar{\Zbf}}

\newcommand{\Asf}{\mathsf{A}}
\newcommand{\Bsf}{\mathsf{B}}
\newcommand{\Csf}{\mathsf{C}}
\newcommand{\Dsf}{\mathsf{D}}
\newcommand{\Esf}{\mathsf{E}}
\newcommand{\Fsf}{\mathsf{F}}
\newcommand{\Gsf}{\mathsf{G}}
\newcommand{\Hsf}{\mathsf{H}}
\newcommand{\Isf}{\mathsf{I}}
\newcommand{\Jsf}{\mathsf{J}}
\newcommand{\Ksf}{\mathsf{K}}
\newcommand{\Lsf}{\mathsf{L}}
\newcommand{\Msf}{\mathsf{M}}
\newcommand{\Nsf}{\mathsf{N}}
\newcommand{\Osf}{\mathsf{O}}
\newcommand{\Psf}{\mathsf{P}}
\newcommand{\Qsf}{\mathsf{Q}}
\newcommand{\Rsf}{\mathsf{R}}
\newcommand{\Ssf}{\mathsf{S}}
\newcommand{\Tsf}{\mathsf{T}}
\newcommand{\Usf}{\mathsf{U}}
\newcommand{\Vsf}{\mathsf{V}}
\newcommand{\Wsf}{\mathsf{W}}
\newcommand{\Xsf}{\mathsf{X}}
\newcommand{\Ysf}{\mathsf{Y}}
\newcommand{\Zsf}{\mathsf{Z}}

\newcommand{\Att}{\mathtt{A}}
\newcommand{\Btt}{\mathtt{B}}
\newcommand{\Ctt}{\mathtt{C}}
\newcommand{\Dtt}{\mathtt{D}}
\newcommand{\Ett}{\mathtt{E}}
\newcommand{\Ftt}{\mathtt{F}}
\newcommand{\Gtt}{\mathtt{G}}
\newcommand{\Htt}{\mathtt{H}}
\newcommand{\Itt}{\mathtt{I}}
\newcommand{\Jtt}{\mathtt{J}}
\newcommand{\Ktt}{\mathtt{K}}
\newcommand{\Ltt}{\mathtt{L}}
\newcommand{\Mtt}{\mathtt{M}}
\newcommand{\Ntt}{\mathtt{N}}
\newcommand{\Ott}{\mathtt{O}}
\newcommand{\Ptt}{\mathtt{P}}
\newcommand{\Qtt}{\mathtt{Q}}
\newcommand{\Rtt}{\mathtt{R}}
\newcommand{\Stt}{\mathtt{S}}
\newcommand{\Ttt}{\mathtt{T}}
\newcommand{\Utt}{\mathtt{U}}
\newcommand{\Vtt}{\mathtt{V}}
\newcommand{\Wtt}{\mathtt{W}}
\newcommand{\Xtt}{\mathtt{X}}
\newcommand{\Ytt}{\mathtt{Y}}
\newcommand{\Ztt}{\mathtt{Z}}

\newcommand{\Abld}{\boldsymbol{A}}
\newcommand{\Bbld}{\boldsymbol{B}}
\newcommand{\Cbld}{\boldsymbol{C}}
\newcommand{\Dbld}{\boldsymbol{D}}
\newcommand{\Ebld}{\boldsymbol{E}}
\newcommand{\Fbld}{\boldsymbol{F}}
\newcommand{\Gbld}{\boldsymbol{G}}
\newcommand{\Hbld}{\boldsymbol{H}}
\newcommand{\Ibld}{\boldsymbol{I}}
\newcommand{\Jbld}{\boldsymbol{J}}
\newcommand{\Kbld}{\boldsymbol{K}}
\newcommand{\Lbld}{\boldsymbol{L}}
\newcommand{\Mbld}{\boldsymbol{M}}
\newcommand{\Nbld}{\boldsymbol{N}}
\newcommand{\Obld}{\boldsymbol{O}}
\newcommand{\Pbld}{\boldsymbol{P}}
\newcommand{\Qbld}{\boldsymbol{Q}}
\newcommand{\Rbld}{\boldsymbol{R}}
\newcommand{\Sbld}{\boldsymbol{S}}
\newcommand{\Tbld}{\boldsymbol{T}}
\newcommand{\Ubld}{\boldsymbol{U}}
\newcommand{\Vbld}{\boldsymbol{V}}
\newcommand{\Wbld}{\boldsymbol{W}}
\newcommand{\Xbld}{\boldsymbol{X}}
\newcommand{\Ybld}{\boldsymbol{Y}}
\newcommand{\Zbld}{\boldsymbol{Z}}

\newcommand{\Acal}{\mathcal{A}}
\newcommand{\Bcal}{\mathcal{B}}
\newcommand{\Ccal}{\mathcal{C}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Ecal}{\mathcal{E}}
\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Gcal}{\mathcal{G}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Ical}{\mathcal{I}}
\newcommand{\Jcal}{\mathcal{J}}
\newcommand{\Kcal}{\mathcal{K}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Mcal}{\mathcal{M}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Ocal}{\mathcal{O}}
\newcommand{\Pcal}{\mathcal{P}}
\newcommand{\Qcal}{\mathcal{Q}}
\newcommand{\Rcal}{\mathcal{R}}
\newcommand{\Scal}{\mathcal{S}}
\newcommand{\Tcal}{\mathcal{T}}
\newcommand{\Ucal}{\mathcal{U}}
\newcommand{\Vcal}{\mathcal{V}}
\newcommand{\Wcal}{\mathcal{W}}
\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Ycal}{\mathcal{Y}}
\newcommand{\Zcal}{\mathcal{Z}}

\newcommand{\Abb}{\mathbb{A}}
\renewcommand{\Bbb}{\mathbb{B}}
\newcommand{\Cbb}{\mathbb{C}}
\newcommand{\Dbb}{\mathbb{D}}
\newcommand{\Ebb}{\mathbb{E}}
\newcommand{\Fbb}{\mathbb{F}}
\newcommand{\Gbb}{\mathbb{G}}
\newcommand{\Hbb}{\mathbb{H}}
\newcommand{\Ibb}{\mathbb{I}}
\newcommand{\Jbb}{\mathbb{J}}
\newcommand{\Kbb}{\mathbb{K}}
\newcommand{\Lbb}{\mathbb{L}}
\newcommand{\Mbb}{\mathbb{M}}
\newcommand{\Nbb}{\mathbb{N}}
\newcommand{\Obb}{\mathbb{O}}
\newcommand{\Pbb}{\mathbb{P}}
\newcommand{\Qbb}{\mathbb{Q}}
\newcommand{\Rbb}{\mathbb{R}}
\newcommand{\Sbb}{\mathbb{S}}
\newcommand{\Tbb}{\mathbb{T}}
\newcommand{\Ubb}{\mathbb{U}}
\newcommand{\Vbb}{\mathbb{V}}
\newcommand{\Wbb}{\mathbb{W}}
\newcommand{\Xbb}{\mathbb{X}}
\newcommand{\Ybb}{\mathbb{Y}}
\newcommand{\Zbb}{\mathbb{Z}}

\renewcommand{\vec}[1]{\mathbf{\boldsymbol{#1}}}

\newcommand{\avec}{\vec{a}}
\newcommand{\bvec}{\vec{b}}
\newcommand{\cvec}{\vec{c}}
\newcommand{\dvec}{\vec{d}}
\newcommand{\evec}{\vec{e}}
\newcommand{\fvec}{\vec{f}}
\newcommand{\gvec}{\vec{g}}
\newcommand{\hvec}{\vec{h}}
\newcommand{\ivec}{\vec{i}}
\newcommand{\jvec}{\vec{j}}
\newcommand{\kvec}{\vec{k}}
\newcommand{\lvec}{\vec{l}}
\newcommand{\mvec}{\vec{m}}
\newcommand{\nvec}{\vec{n}}
\newcommand{\ovec}{\vec{o}}
\newcommand{\pvec}{\vec{p}}
\newcommand{\qvec}{\vec{q}}
\newcommand{\rvec}{\vec{r}}
\newcommand{\svec}{\vec{s}}
\newcommand{\tvec}{\vec{t}}
\newcommand{\uvec}{\vec{u}}
\newcommand{\vvec}{\vec{v}}
\newcommand{\wvec}{\vec{w}}
\newcommand{\xvec}{\vec{x}}
\newcommand{\yvec}{\vec{y}}
\newcommand{\zvec}{\vec{z}}

\newcommand{\Avec}{\vec{A}}
\newcommand{\Bvec}{\vec{B}}
\newcommand{\Cvec}{\vec{C}}
\newcommand{\Dvec}{\vec{D}}
\newcommand{\Evec}{\vec{E}}
\newcommand{\Fvec}{\vec{F}}
\newcommand{\Gvec}{\vec{G}}
\newcommand{\Hvec}{\vec{H}}
\newcommand{\Ivec}{\vec{I}}
\newcommand{\Jvec}{\vec{J}}
\newcommand{\Kvec}{\vec{K}}
\newcommand{\Lvec}{\vec{L}}
\newcommand{\Mvec}{\vec{M}}
\newcommand{\Nvec}{\vec{N}}
\newcommand{\Ovec}{\vec{O}}
\newcommand{\Pvec}{\vec{P}}
\newcommand{\Qvec}{\vec{Q}}
\newcommand{\Rvec}{\vec{R}}
\newcommand{\Svec}{\vec{S}}
\newcommand{\Tvec}{\vec{T}}
\newcommand{\Uvec}{\vec{U}}
\newcommand{\Vvec}{\vec{V}}
\newcommand{\Wvec}{\vec{W}}
\newcommand{\Xvec}{\vec{X}}
\newcommand{\Yvec}{\vec{Y}}
\newcommand{\Zvec}{\vec{Z}}

\newcommand{\yvecbar}{\bar{\vec{y}}}
\newcommand{\wvecbar}{\bar{\vec{w}}}
\newcommand{\xvecbar}{\bar{\vec{x}}}
\newcommand{\yvectil}{\tilde{\vec{y}}}
\newcommand{\yvechat}{\hat{\vec{y}}}




\ifx\BlackBox\undefined
\newcommand{\BlackBox}{\rule{1.5ex}{1.5ex}}  \fi

\ifx\QED\undefined
\def\QED{~\rule[-1pt]{5pt}{5pt}\par\medskip}
\fi

\ifx\proof\undefined
\newenvironment{proof}{\par\noindent{\em Proof:\ }}{\hfill\BlackBox\\label{eq:concat}
    \textnormal{Cat}(S) = \frac{1}{Z}\sbr{k_0, k_1, \cdots, k_{N-1}}, \text{ where }k_i = \alpha^i\textnormal{Upsample}_{2^{\max\sbr{i-1, 0}}d}\rbr{\wbf_i}.
\label{eq:concat_ablation}
    \textnormal{Cat*}(S) = \frac{1}{Z}\sbr{k_0, k_1, \cdots, k_{N-1}}\odot\sbr{\frac{1}{1^t}, \frac{1}{2^t}, \cdots, \frac{1}{L^t}}, 
\text{ where }k_i = \textnormal{Upsample}_{2^{\max\sbr{i-1, 0}}d}\rbr{\wbf_i}.

 here then controls the decay speed, which is independent of each scale's dimension. We conduct two sets of experiments: 1) Fix , vary  from  (which means no decay) to , and 2) Fix , vary  from  to . Figure~\ref{fig:ablation_decay_dim} reports the accuracies in different settings. We can observe that 1) The decay structure is crucial for getting good performance, and 2) In a reasonable range,  has less impact on the performance than . Nevertheless, we observe a trend of performance drop when increasing  from  to . Experiments on larger  show worse performance, which can be attributed to overfitting.
\subsubsection{Speed Comparison}\label{sec:speed}
In Table~\ref{tab:speed}, we compare the speed of the S4 kernel and \texttt{SGConv} kernel in different settings. Due to its simplicity, \texttt{SGConv} is faster than S4 for any sentence length. \texttt{SGConv} is about  faster than the vanilla implementation of the S4 kernel and is  faster than the optimized CUDA kernel implementation without resorting to optimized CUDA kernels. 

\subsection{Speech Commands}
The Speech Command (SC) dataset~\citep{warden2018speech} is a 35-class dataset of 1 second (16000 HZ sampling rate) spoken words in English. However, followup works~\citep{kidger2020neural,gu2021combining,romero2021ckconv,romero2021flexconv} adopted a smaller 10-class subset of SC. And works~\citep{romero2021flexconv,gu2021combining} on the SC dataset specifically use pre-processing such as MFCC features. Our baselines are obtained from~\citep{gu2021efficiently,gu2022parameterization}. Note that besides SSM-based models, there is no strong baseline for raw waveform classification using either the 10-class or the full dataset. And SSM-based methods also show the ability to perform 0-shot testing at lower sampling rate such as 8000 Hz. Table ~\ref{table:scresults} shows that the \texttt{SGConv} yields better results compared to the SSM-based method among 4 out of 5 tasks. Notably, for the original SC (35-class), \texttt{SGConv} achieves marginally higher accuracy for raw-sequence classification and significantly better results () compared to the existing SoTA method.

\begin{table}[h]
  \setlength{\tabcolsep}{3pt}
  \renewcommand{\arraystretch}{0.8}
  \centering
\begin{tabular}{lcccccccc}
\toprule
  10-cls   & Transformer & Performer & NRDE & CKConv & WaveGAN-D & S4 & S4 & SGConv \\ \midrule
     MFCC  &90.75 & 80.85 &89.8&\textbf{95.3} &\xmark&93.96 & {92.05} &   {94.91} \\ \midrule
16000HZ  &\xmark & 30.77 &16.49 &11.6&71.66&98.32&  \textbf{97.98} & 97.52    \\ \midrule
8000HZ (0-shot)  & \xmark & 30.68 & 15.12 & 65.96 & \xmark & 96.30 &  {91.83} & \textbf{96.03}    \\ \midrule

\end{tabular}

\begin{tabular}{lccccccccc}
\toprule
  35-cls   & InceptionNet & ResNet-18 & XResNet-50 & ConvNet & S4D & S4 & S4 & SGConv \\ \midrule
16000HZ  &61.24&77.86&83.01&95.51& 96.25& 96.08 & 96.27  & \textbf{96.42}        \\ \midrule
8000HZ (0-shot) &5.18&8.74&7.72&7.26& 91.58 & 91.32 & 91.89 & \textbf{94.29}        \\ \midrule
\end{tabular}
\caption[Caption for LOF]{Speech Command (SC) classification results compared to existing methods. () We carefully reproduce the S4 method based on the official released code\protect\footnotemark.Since the latest version removed 10-class experiments settings, we utilized earlier released version. \protect\footnotemark.The results suggest that for the SC 35-classification, \texttt{SGConv} achieves SoTA on both full length task and 2X sampling rate, zero-shot task.}\label{table:scresults}


\end{table}

\addtocounter{footnote}{-1}
\footnotetext{\url{https://github.com/HazyResearch/state-spaces}\label{state-spaces}}

\addtocounter{footnote}{1}
\footnotetext{\url{https://github.com/HazyResearch/state-spaces/tree/307f11bba801d5734235a1791df1859f6ae0e367}\label{state-spaces-earlier}}



\subsection{Further Applications of SGConv}
We further study \texttt{SGConv} as a generic network architecture \emph{drop-in} component targeting tasks in language modeling and computer vision. In Section~\ref{sec: lm} we present an efficient mixture of attention and \texttt{SGConv} layers architecture that replaces half of the attention blocks in the Transformer with the \texttt{SGConv} blocks. We demonstrate the potential of utilizing such a model for long text processing. In Section~\ref{sec: imagenet}, we incorporate \texttt{SGConv} (1D) into ConvNeXt~\citep{liu2022convnet}. Surprisingly, \texttt{SGConv} achieves comparable or even better results compared to several SoTA CNN and Vision Transformer models by treating the 2D features as a 1D sequence.

\subsubsection{Language Tasks}\label{sec: lm}

\paragraph{Language modeling.}
We propose the \texttt{SGConv} block (shown in Figure~\ref{fig:gconvlmblock}) which is similar to the Attention block in Transformer~\citep{vaswani2017attention}. \texttt{SGConv} block enjoys both  time complexity and space complexity. We benchmark the inference time and GPU memory usage of both \texttt{SGConv} and Attention in Table~\ref{tab:attntimetrial}. When the sequence length is 1024, \texttt{SGConv} block is 2.1X faster than the Attention block. For language modeling, we utilize the feature of \texttt{SGConv} to directly process the long sequences. The Attention block only targets the short range data termed SAttention. We illustrate the structure in Figure~\ref{fig:lm conv attn}. Furthermore, we investigate the strategy to replace the Attention blocks with \texttt{SGConv} blocks. We generate 50 architectures with 8 \texttt{SGConv} blocks and 8 Attention blocks where the order is shuffled. We denote the average depth to replace the Attention blocks as:  where the  denotes the th \texttt{SGConv} depth position.  and  in this case. 
The results in Figure~\ref{fig:lm nas} suggest that when fixing the number of \texttt{SGConv} layer, models achieve better performance by placing \texttt{SGConv} blocks in \emph{deeper} layers.
Guided by the strategy, we handcraft two Transformer-XL~\citep{dai2019transformer} style models. (1) 16-layer: \{A, A, A, C\}2 + \{A, C, C, C\}2. (2) 18-layer: \{A, A, C\}3 + \{A, C, C\}3. A denotes SAttention and C denotes \texttt{SGConv}.  denotes repeating the order of layers for  times. We test the model on WikiText-103~\citep{merity2016pointer} which is a wide-used language modeling benchmark with an average length of 3.6K tokens per article.
We set both the attention and memory length to 384 for 18L model and 192 for 16L model. The length of input sequence is 3092 which can be processed by \texttt{SGConv} directly. We show the results in Table~\ref{table:wikitext}. Our results suggest that when the attention range is short, the 16L model outperform the baseline with -1.17 perplexity. For the 18L model, our model achieves 18.70 perplexity. Note that we use a smaller and affordable batch size (16) for training. Under the same setting, our model gains slightly better perplexity than Transformer-XL (-0.05). Our results show the potential of adopting \texttt{SGConv} as part of the language model for long range language sequence processing. 


\begin{table}[]
\centering
\begin{tabular}{lcccccc}\toprule
\multicolumn{2}{l}{}                                                                                                          & 256 & 512 & 1024 & 2048 & 3072   \\ \midrule
\multicolumn{1}{l|}{\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Attn.\\ Block\end{tabular}}}  & \multicolumn{1}{l|}{Inf. (ms/batch)} & \textbf{2.6} & 7.3 & 23.2 & 91.7 & \xmark \\ \cmidrule{2-7}
\multicolumn{1}{l|}{}                                                                        & \multicolumn{1}{l|}{Mem. (GB)} & \textbf{2.6} & 3.9 & 7.9  & 23.9 & OOM    \\ \midrule
\multicolumn{1}{l|}{\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}SGConv\\ Block\end{tabular}}} & \multicolumn{1}{l|}{Inf. (ms/batch)} & 2.7 & \textbf{5.4} & \textbf{10.9} & \textbf{21.8} & \textbf{43.6}   \\ \cmidrule{2-7}
\multicolumn{1}{l|}{}                                                                        & \multicolumn{1}{l|}{Mem. (GB)} & \textbf{2.6} & \textbf{3.4} & \textbf{5.2}  & \textbf{8.7}  & \textbf{15.7}  \\ \bottomrule 
\end{tabular}
\caption{Comparison of inference time and GPU memory utilization with Attention blocks. \texttt{SGConv} has significantly less memory usage and faster inference speed when the sequence increases exponentially. }\label{tab:attntimetrial}
\end{table}
\paragraph{Sentence classification.}  We combine the \texttt{SGConv} block with the BERT model~\citep{devlin2018bert}. Concretely, we utilize the 12-layer \{A, A, C\}+\{A, C, C\}2 model. The pretraining is conducted on BooksCorpus~\citep{zhu2015aligning} and  English Wikipedia~\citep{wikidump}. We then fine-tune the model on the GLUE benchmark~\citep{wang2019glue}. To avoid the instability of fine-tuning on small datasets, we only test on tasks with more than  training samples. We follow the training and fine-tuning pipeline of \citet{ke2020rethinking} (BERT-A in Table 1 of \citet{ke2020rethinking}) and report the average accuracy of  different random seeds. \texttt{SGConvBERT} achieves comparable performance to the original BERT model, while the \texttt{SGConv} layer is more efficient than the attention layer.

 \begin{figure}[t]
     \centering
     
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/lm.pdf}
         \caption{Illustration of \texttt{SGConv} and Transformer-XL style Short Attention used in language modeling task. \texttt{SGConv} directly processes the full length sequence.}
         \label{fig:lm conv attn}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/nas.pdf}
         \caption{The depth to replace SAttention with \texttt{SGConv} vs. validation PPL on WikiText-103}
         \label{fig:lm nas}
     \end{subfigure}

        \caption{Incorporating \texttt{SGConv} to Transformer models in language tasks. }
        \label{fig:lm structure}
\end{figure}

\begin{table}[]
\centering
\begin{tabular}{lcc}
\hline
Model           &Valid. & Test \\ \hline
LSTM+Hebb. & 29.0& 29.2 \\
16L Transformer-XL   &-      &  24.0   \\
16L SGConv+SAttn        & 21.90    &   \textbf{22.83}  \\ \hline
Adaptive Input & - & 18.7\\
S4 & - & 20.95 \\
18L Transformer-XL &  - & \textbf{18.3}  \\
18L Transformer-XL & 18.16  &  18.75\\
18L SGConv+SAttn     &  18.10   & 18.70\\ \hline
\end{tabular}
\caption{Performance comparison on WikiText-103.}
\label{table:wikitext}
\end{table}

\begin{table}[t]
\label{table:bertresult}
  \setlength{\tabcolsep}{3pt}
  \renewcommand{\arraystretch}{0.8}
  \centering
\begin{tabular}{lccccccc}
\toprule
     & MNLI-m/mm & QNLI & QQP & SST & CoLA & STS & Avg. \\ \midrule
    BERT & \textbf{84.93/84.91} & \textbf{91.34} & 91.04 & \textbf{92.88} & 55.19 & 88.29 & 84.08     \\ \midrule
SGConvBERT & 84.78/84.70 & 91.25 & \textbf{91.18} & 92.55 & \textbf{57.92} & \textbf{88.42} & \textbf{84.40}    \\ \midrule
\end{tabular}
\caption{Performance comparison of BERT and \texttt{SGConvBERT} on GLUE dataset. \texttt{SGConvBERT} is comparable with BERT while being more efficient. We exclude MRPC and RTE datasets in GLUE because their sizes are too small ( training samples).}\label{tab:bert}
\end{table}


 \begin{figure}[]
     \centering
     
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/accvsflops.pdf}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/accvsthroughputs.pdf}
     \end{subfigure}
        \caption{Comparison of ImageNet-1k Top-1 accuracy with SoTA works. Left: Top-1 Accuracy vs. FLOPs. Right: Top-1 Accuracy vs. throughputs. }
        \label{fig:imagenet}
\end{figure}

\subsubsection{Image Classification}\label{sec: imagenet}
We also evaluate the adaptability of \texttt{SGConv} by applying it on large-scale image classification. We conduct experiments on ImageNet-1k~\citep{deng2009imagenet} which consists of more than 1.28 million high-resolution training and 50,000 validation images. We replace the  2D convolutional kernels with \texttt{SGConvs} in ConvNeXt~\citep{liu2022convnet} denoted as \texttt{SGConvNeXt}. The block designs of \texttt{SGConvNeXt} are shown in Figure~\ref{fig:gconvnext}. Note we train the SGConveNeXt-Tiny/Small/Base/Large using hyperparameter settings from ConvNeXt\footref{foot: convnext} without any changes. By treating the 2D features as sequences, our \texttt{SGConvNeXt} achieves better results compared to existing SoTA methods such as EfficientNets~\citep{tan2019efficientnet}, Swin Transformers~\citep{liu2021swin} (shown in Figure~\ref{fig:imagenet}). Note that Vision Transformer~\citep{dosovitskiy2020image} and its variants~\citep{touvron2021training, touvron2021going,yu2022metaformer} adopt patching techniques that can lead to a quadratic increase in complexity with image size. Also, patching is incompatible with dynamic input resolutions while \texttt{SGConvNeXt} processes the data globally. 
We list several interesting directions that can be explored for future work: 1) Optimization for the long-range convolution:
we noticed that though FFT theoretically requires less FLOPs than plain convolution, the throughput drops empirically. One reason is that there is no optimized CUDA implementation for 1D long-range convolution and can be a good direction for future work. 2) Optimized hyperparameters and data augmentation methods: ConvNeXts' hyperparameters are tuned for maximum performance, which may not be ideal for \texttt{SGConvNeXt}. 3) \texttt{SGConv} for vision reasoning tasks: we show that \texttt{SGConv} is powerful for long-range synthetic reasoning tasks and large-scale classification tasks. It could be effective in visual reasoning applications such as Vision-Language Reasoning~\citep{johnson2017clevr,zhu2020vision} with great potential. 


          





\section{Discussion}
In this paper, we attempt to answer the question of what makes convolutional models great again on long sequence modeling and summarize two principles contributing to the success. Based on the principles, we propose a simple and intuitive global convolutional model \texttt{SGConv} that has both direct implications and solid performance. Concurrent to our work there are also attempts to simplify the S4 model by restricting the state transition matrix to be diagonal~\citep{gu2022parameterization}. Compared to our paper, the proposal \citet{gu2022parameterization} again involves a nuanced design of parameterization and initialization schemes, which give intuition from state-space-model perspective to explain the S4.
Instead, we hope our simpler principles and non-SSM-based model can open up a direction for general audiences to understand and try global convolution as a general-purpose module for tackling long-range dependency. This potential has been shown in a very recent paper~\citep{ma2022mega} concurrent to our work, where the authors incorporate an exponential moving average layer to a Transformer-like model and achieve promising performance over several long sequence modeling tasks. The exponential moving average layer is a particular type of global convolution layer that naturally satisfies our two principles. We believe that similar global convolutional modules will emerge in the future as long-range dependency becomes increasingly critical for sequence modeling.

\section*{Acknowledgements}
We thank SÃ©bastien Bubeck for helpful comments and discussions on the paper and Albert Gu and his coauthors for introducing an interesting line of works and sharing their well-organized codebase.
    



\bibliographystyle{plainnat}
\bibliography{gconv}
\newpage
\appendix
\section{Detailed Experimental Results}
\subsection{Long Range Arena}\label{app:lra}
Here we report the detailed implementation of the LRA experiments. We use the concatenation style combination of sub-kernels in all experiments and mildly tune the dimension of each scale. Since the \texttt{SGConv} exhibits a strong ability to fit data, we slightly increase the dropout for some tasks to prevent overfitting. Table~\ref{table:lrahparam} lists the detailed hyperparameters used in LRA. In most experiments, we set  to , which approximately decays in speed . Experiments on flattened 2D images require some special modification of the kernel. We hypothesize that it is because images require more subtle inductive bias. For the experiment on the Image dataset, we use the disentangled version of parameterization and combination weights as described in Section~\ref{sec:ablation} and set the decay speed to be . For the experiment on the Pathfinder-X task, we initialize convolution kernels in different channels with cosine waves with different frequencies and randomly assign  ranging from  to  to different channels. Both these modifications bring about  improvement compared to standard fixed  and random initialization. The remaining hyperparameters and experimental settings are same to \citet{gu2022parameterization} which can be found in the Github repo\footref{state-spaces}.
\begin{table}[h]
  \setlength{\tabcolsep}{3pt}   
  \renewcommand{\arraystretch}{0.8}
  \centering
\begin{tabular}{lccccccc}
\toprule
 & ListOps & Text & Retrieval & Image & Pathfinder & Path-X \\ \midrule
Acc. & 61.45 & 89.20 & 91.11 & 87.97 & 95.46 & 97.83 \\
Scale dim. & 1 & 2 & 1 & 32 & 32 & 64 \\
Dropout & 0 & 0 & 0 & 0.2 & 0.2 & 0 \\
\bottomrule
\end{tabular}
  \caption{Hyperparameters used in LRA experiments.}\label{table:lrahparam}
\end{table}
\subsection{Speech Command}
For Speech Command 10-class task, we use the same training setting from \citet{gu2021efficiently} earlier version Github repo\footref{state-spaces-earlier}. For Speech Command 35-class task, we use the training setting from the Github repo\footref{state-spaces}. The scale dimension of \texttt{SGConv} is 32.

\subsection{Language Task}
Our implementation for Language Task is based on the project~\footnote{\url{https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/Transformer-XL}\label{foot: nvidia}}. For the 16-L model, we utilize 3072 as the sequence length for \texttt{SGCONV} and 192 as both the attention and memory length for SAttention. For the 18-L model, we utilize 3072 as the sequence length for \texttt{SGCONV} and 384 as both the attention and memory length for SAttention. The \texttt{SGConv} has 96 as the scale dimension. We adopt the training settings from the above mentioned project~\ref{foot: nvidia} except the batch size which is reduced to 64. The \texttt{SGConv} block is shown in Figure~\ref{fig:lm structure}.

\label{app:language_block}
\begin{figure}[h]
  \begin{center}
    \includegraphics[width=0.3\textwidth]{img/gconvblock.pdf}
  \end{center}
  \caption{SGConv block}
  \label{fig:gconvlmblock}
\end{figure}

\begin{figure}[h]
  \begin{center}
    \includegraphics[width=0.7\textwidth]{img/gconvnext.pdf}
  \end{center}
  \caption{SGConvnext}
  \label{fig:gconvnext}
\end{figure}
\subsection{Image Classification}\label{app:image_block}


We use the training settings in the work \citet{liu2022convnet}\footnote{\url{https://github.com/facebookresearch/ConvNeXt}\label{foot: convnext}}. Since the \texttt{SGConvNeXt} has several downsampling layers, we fixed the scale to  and the scale dimensions are calculated based on the flattened features length of the corresponding layers. The structure is shown in Figure~\ref{fig:gconvnext}. The results are shown in Table~\ref{table:imagenet}. The visualization of the \texttt{SGConvNeXt}-Base outputs are shown in Figure~\ref{fig:visualization}. The visualization of the \texttt{SGConv} kernels at different stages are shown in Figure~\ref{fig:stages}.

\begin{table}[!htb]
    \begin{minipage}{.5\linewidth}
      \centering
        \scriptsize
       \begin{tabular}{lcccc}
       \toprule
\multirow{2}{*}{model} & \multirow{2}{*}{FLOPs} & throughput & \multirow{2}{*}{params} & \multirow{2}{*}{Acc.} \\
                       &                        & (image/s)  &                         &                       \\\midrule
Swin-T&              4.5G          &944.5&                 29M        &                81.3       \\
Swin-S&             8.7G           &     576.8       &         50M                &        83.0               \\
Swin-B&             15.4G           &     433.4       &          88M               &         83.5              \\
Swin-B&        47.0G                &       134.6     &          88M               &           84.5           \\\midrule
ConvNeXt-T&            4.5G            &   1252.6         &           29M              &        82.1               \\
ConvNeXt-S&              8.7G          &       801.4     &              50M           &         83.1              \\
ConvNeXt-B&             15.4G           &        588.3    &               89M          &        83.8               \\
ConvNeXt-L&                 34.4G       &      349.8      &                 198M        &       84.3               \\\bottomrule

\end{tabular}
    \end{minipage}\begin{minipage}{.5\linewidth}
      \centering
          \scriptsize
        \begin{tabular}{lcccc}
        \toprule
\multirow{2}{*}{model} & \multirow{2}{*}{FLOPs} & throughput & \multirow{2}{*}{params} & \multirow{2}{*}{Acc.} \\
                       &                        & (image/s)  &                         &                       \\\midrule
EffNet-B3&             1.8G           &      693.9      &                 12M        &      81.6                 \\
EffNet-B4&                4.2G        &     341.5       &             19M            &             82.9          \\
EffNet-B5&               9.9G         &      223.5      &            30M             &              83.6         \\
EffNet-B6&                 19.0G       &        91.5    &             43M            &                84.0       \\
EffNet-B7&                37.0G        &        52.9    &              66M           &                84.3       \\\midrule
SGConvNeXt-T&           4.3G             &      872.6      &             29M            &    82.0                   \\
SGConvNeXt-S&             8.3G           &      565.3      &                    51M     &       83.4                \\
SGConvNeXt-B&               14.6G         &       417.9    &                   90M      &         83.9               \\
SGConvNeXt-L&                 32.5G       &     256.7       &        200M                 &    84.4                   \\\bottomrule
\end{tabular}
    \end{minipage} 
    \caption{Comparison of ImageNet-1k Top-1 accuracy with SoTA works. }\label{table:imagenet}
\end{table}


\begin{figure}[h]
  \begin{center}
    \includegraphics[width=1.0\textwidth]{img/visualization.pdf}
  \end{center}
  \caption{Visualization of the intermediate features of \texttt{SGConvNeXt} on ImageNet-1k dataset.}
  \label{fig:visualization}
\end{figure}


 \begin{figure}[t]
     \centering
     
     \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[width=0.6\textwidth]{img/stage0.pdf}
         \caption{Visulization of kernels at Stage 0.}
         \label{fig:stage0}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[width=0.6\textwidth]{img/stage1.pdf}
         \caption{Kernels at Stage 1.}
         \label{fig:stage1}
         \includegraphics[width=0.6\textwidth]{img/stage2.pdf}
         \caption{Kernels at Stage 2.}
         \label{fig:stage0}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.6\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/stage3.pdf}
         \caption{Kernels at Stage 3.}
         \label{fig:stage1}
     \end{subfigure}

        \caption{Kernels in \texttt{SGConvNeXt} at different stages. }
        \label{fig:stages}
\end{figure}

\end{document}