\documentclass[dvipsnames]{article}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{showlabels}
\usepackage{enumitem}
\usepackage[ruled,vlined]{algorithm2e}


\PassOptionsToPackage{numbers, compress}{natbib}


\usepackage[final]{neurips_2020}








\usepackage[utf8]{inputenc} \usepackage[T1]{fontenc}    \usepackage{hyperref}       \usepackage{url}            \usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{hyperref}
\usepackage{multirow, makecell}

\definecolor{darkgreen}{rgb}{0,0.6,0}
\newcommand{\dsp}[1]{{\color{red}[DSP: #1]}}




\newcommand{\N}{{\mathcal N}}
\newcommand{\bb}{\mathbb}
\newcommand{\vop}{\operatornamewithlimits{vec}}

\SetKwFunction{Range}{range}
\SetKwInput{KwInput}{Input}
\SetKwInput{KwParams}{Parameters}

\def\train{{\text{train}}}
\def\val{{\text{val}}}
\def\all{{\text{all}}}
\def\ensemble{{\text{ensemble}}}
\def\eq#1{(\ref{#1})}

\title{Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition}




\author{Yu Zhang\thanks{Equal contribution.} \And
  James Qin \And
  Daniel S. Park \And
  Wei Han \And
  Chung-Cheng Chiu \And
  Ruoming Pang \And
  Quoc V. Le \And
  Yonghui Wu \AND
\\
  Google Research, Brain Team \-7pt]Unlabeled data\hrs)}
    & \multicolumn{4}{c}{\bfseries No LM} & \multicolumn{4}{c}{\bfseries With LM} \\
    \cmidrule(r){3-6} \cmidrule(r){7-10}
    & & \bfseries dev & \bfseries dev-other & \bfseries test & \bfseries test-other
     & \bfseries dev & \bfseries dev-other & \bfseries test & \bfseries test-other \\
    \midrule
    \bfseries Baseline \\
    \quad  Conformer L \cite{conformer}
    & -
    & 1.9 & 4.4 & 2.1 & 4.3 
    &  &  & 1.9 & 3.9 \\
\midrule
    \bfseries NST Only \\
    \quad Gen4 ContextNet \cite{nstasr}
    & 60k
    & 1.6 & 3.7 & 1.7 & 3.7
    & 1.6 & 3.4 & 1.7 & 3.4 \\
    \quad Gen4 Conformer L
    & 60k
    & 1.6 & 3.3 & 1.7 & 3.5
    & 1.6 & 3.1 & 1.7 & 3.3 \\
    \midrule
    \bfseries Pre-training Only \\
    \quad Pre-trained CTC \cite{wav2vec2}
    & 60k
    & 2.1 & 4.5 & 2.2 & 4.5 & 1.6 & 3.0 & 1.8 & 3.3 \\ 
    \quad Pre-trained Conformer XL
    & 60k
    & 1.7 & 3.5 & 1.7 & 3.5 
    & 1.6 & 3.2 & 1.5 & 3.2 \\
    \quad Pre-trained Conformer XXL
    & 60k
    & 1.6 & 3.2 & 1.6 & 3.3 
    & 1.5 & 3.0 & 1.5 & 3.1 \\
    \midrule
    \bfseries Combined SSL \\
    \quad  Gen3 Conformer XXL
    & 60k
    &  \bfseries 1.3 & 2.7  & 1.5  & 2.8 
    &  \bfseries 1.3 & \bfseries 2.6  & \bfseries 1.4  & 2.7  \\
    \quad  Gen3 Conformer XXL+
    & 60k
    &  \bfseries 1.3 & 2.7  & 1.5  & 2.7 
    &  \bfseries 1.3 & \bfseries 2.6 & \bfseries 1.4 & \bfseries 2.6 \\
    \bottomrule
  \end{tabular}
  }
\end{table}


The result of training Conformers with our pipeline is presented in table \ref{t:librispeech}, against some baselines. As we present in the next section, we have been unsuccessful in achieving gains from training Conformer XL and XXL from scratch, and only present the performance we obtained by fine tuning pre-trained models on LibriSpeech for these models.

As one of the baselines, we train a Conformer L model with NST without pre-training, using ContextNets \cite{contextnet} as intermediate models. We follow the exact procedure of \cite{nstasr}, in which five generations of ContextNets are trained, up to the fourth generation, then replace the fifth generation model with the Conformer L.

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\columnwidth]{figures/gens.png}
\vskip 0.05in
\caption{Performance of models on dev/test sets of LibriSpeech at each generation with and without LM fusion. The dev and test-other performances of LM-fused models coincide at the level of precision considered. The final generation model is taken to be Conformer XXL+. The y-axis is log-scaled.}
\label{f:gens}
\end{figure}

We find that the generation-3 Conformer XXL model shows a 7-15\% relative improvement in WER across the dev and test sets compared to the pre-trained baseline. The performance of models at each NST generation, with Conformer XXL+ as the last generation model, are plotted in figure \ref{f:gens}.



\section{Discussion}

\subsection{Model Size and Pre-training} \label{ss:model size}

Here we present a set of experiments where we train models of various sizes and observe the effect of pre-training. To make fair comparisons, we experiment with a modified version of Conformer L, where relative positional embedding has been removed.

As shown in Table \ref{t:scale_models}, we find that merely scaling up the model size from 100M to 1B parameters alone does not improve performance, as we found it difficult to get gains from training the larger models on the supervised dataset. Upon pre-training, however, we observe consistent improvement by increasing the model size up to 1 billion parameters. We see that pre-training enables the model size growth to transfer to model performance.
\begin{table}[h!]
  \vskip -0.05in
  \caption{WERs(\%) from LibriSpeech experiments. LM fusion has not been used.}
  \vskip 0.1in
  \label{t:scale_models}
  \centering
  \small
  \resizebox{0.9\columnwidth}{!}{\begin{tabular}{lccccc}
    \toprule
    \bfseries Method & \# Params (B) & \bfseries dev & \bfseries dev-other & \bfseries test & \bfseries test-other \\
    \midrule
    \bfseries Trained from scratch \\
    \quad Conformer L (no rel. attn.)  & 0.1 & 2.0 & 4.7 & 2.2 & 4.8 \\
    \quad Conformer XL & 0.6 & 2.1 & 4.9 & 2.3 & 4.9 \\
    \quad Conformer XLL & 1.0 & 2.3 & 5.5 & 2.6 & 5.6 \\
    \midrule
    \bfseries With pre-training \\
    \quad Pre-trained Conformer L (no rel. attn.) & 0.1 & 2.0 & 4.6 & 2.0 & 4.5 \\
    \quad Pre-trained Conformer XL & 0.6 & 1.7 & 3.5 & 1.7 & 3.5 \\
    \quad Pre-trained Conformer XXL & 1.0 & 1.6 & 3.2 & 1.6 & 3.3 \\
    \bottomrule
  \end{tabular}
  }
\end{table}

\subsection{Ablations for Pre-training}

As explained previously, our pre-training procedure differs from that of \cite{wav2vec2} in a number of ways. First, we use the log-mel spectrogram instead of the waveform as the input for the ASR network. Secondly, we do not use quantization or a diversity loss for simplicity. It would be interesting to further investigate quantization and diversity loss settings that improve the performance of Conformers.

To reduce the computation and memory cost for the larger models, the length of the time series of the input features are reduced by the convolutional subsampling block. In table \ref{t:pretraining}, we present multiple experiments with different audio segment lengths for training with multiple time reduction factors by varying the strides of the convolutional layers in the convolutional subsampling block. We also experiment with using voice activation detection (VAD) with tools provided with \cite{librilight} to segment the inputs. For quick experimentation, we fine-tune the pre-trained models on the 100-hour clean subset of LibriSpeech.

We find that the convolutional subsampling layer is required to reduce the input length more aggressively when using longer input segments to pre-train the model. We also find that using VAD tends to degrade the performance, as we cannot guarantee that all the segments have enough context to benefit pre-training.

\begin{table}[h!]
  \vskip -0.05in
  \caption{WERs(\%) from fine-tuning pre-trained Conformer XL on LibriSpeech 100h. LM fusion has not been used.}
  \vskip 0.1in
  \label{t:pretraining}
  \centering
  \resizebox{0.8\columnwidth}{!}
  {\begin{tabular}{ccccccc}
    \toprule
    VAD & Segment length (s) & Reduction & \bfseries dev & \bfseries dev-other & \bfseries test & \bfseries test-other \\
    \midrule
    No & 16 & 4x &  3.4 & 7.2 & 3.4 & 7.0 \\
    No & 16 & 2x & \bfseries 2.5  & 5.5 & 2.6 & 5.6 \\
    No & 32 & 4x & \bfseries 2.5  & \bfseries 4.7 & 2.6 & 4.9 \\
    Yes & 0 to 36 & 4x & 2.8 & 5.6 & 2.8 & 5.7 \\
    \bottomrule
  \end{tabular}
  }
\end{table}



\subsection{Ablations for Fine-tuning}

We present the results of ablation studies on the NST parameters in table \ref{t:finetuning}. These studies are conducted at generation 1 of the noisy student training pipeline, where a Conformer XL model is trained with transcripts generated by the fused generation-0 Conformer XL teacher-model.

\begin{table}[h!]
  \vskip -0.05in
  \caption{WERs(\%) from the generation-1 Conformer XL with different mix ratios and data processing policies. LM fusion has not been used.}
  \vskip 0.1in
  \label{t:finetuning}
  \centering
  \small
  \resizebox{0.9\columnwidth}{!}{\begin{tabular}{lcccccc}
    \toprule
    & B.W. Mix Ratio & Data Processing & \bfseries dev & \bfseries dev-other & \bfseries test & \bfseries test-other \\
    \midrule
    \bfseries Selected & 1:9 & None & \bfseries 1.4 & 3.2 & 1.6 & 3.1 \\
    \midrule
    \bfseries Mix Ratio & 2:8 & None & 1.5 & 3.2 & 1.6 & 3.1 \\
    & Not Used & None & 1.5 & \bfseries 3.1 & 1.6 & 3.0 \\
    \midrule
    \bfseries Data Processing & 1:9 & Balanced & 1.5 & 3.2 & 1.5 & 3.1 \\
    & 1:9 & LM-filtered & 1.5 & 3.2 & 1.6 & 3.2 \\
    \bottomrule
  \end{tabular}
  }
\end{table}

We experiment with three different batch-wise mixing settings, where the ratio of the supervised versus teacher-labeled utterances were mixed with ratio 2:8, 1:9 or not mixed batch-wise, but randomly distributed. We find the 1:9 mix ratio and non-batch-wise mixing to be comparable, while the 2:8 mix ratio gives the worst performance.

In \cite{nstasr}, filtering teacher-generated transcripts using the confidence derived from the fused ASR model was shown not to be beneficial for the NST pipeline for the LibriSpeech + Libri-Light task. On the other hand, the authors found balancing the teacher-generated transcripts using sub-modular sampling useful. We have thus experimented with balancing, and filtering based on the log-perplexity score of generated transcripts computed by the LM trained on the LibriSpeech LM corpus, rather than the confidence of the ASR model.

The balancing procedure of \cite{nstasr} uses batch-wise optimization to assign sampling weights to the teacher-generated transcripts so that the KL divergence between the token distribution of the transcripts of the training set and the weighted teacher-generated transcripts is minimized. We use the exact same procedure as in \cite{nstasr} with equivalent parameters to balance the teacher-generated transcripts and train the generation-1 model for comparison.

Meanwhile, we also experiment with LM-filtering, motivated by the fact that the LM can capture more subtle features of the distribution of the LM corpus beyond token frequency. We compute the token length and LM score of all the transcripts generated by the fused generation-0 Conformer XL model, and compute the normalized filtering score of the transcripts using equation (1) of \cite{nstasr}. We use this score to filter out 40\% of the transcripts and use the remaining transcripts to train the model.

For this particular task and series of models, it turns out that neither balancing nor LM-filtering helps with generation-1 model dev performance. We hypothesize that this might be due to the fact that the generation-0 model is already extremely good, and that maximizing the amount of data produced by the model should be prioritized over improving the quality by a marginal amount.

\section{Conclusion}

We have combined recent developments in architecture, augmentation and especially semi-supervised learning to push the state-of-the-art on the speech recognition task LibriSpeech. 

\begin{ack}
We thank Françoise Beaufays, Yuan Cao, Shefali Garg, Parisa Haghani, Dongseong Hwang, Ye Jia, Bo Li, Manasa Prasad, Khe Chai Sim and Trevor Strohman for useful discussions. We thank Zhifeng Chen for helping us with scaling up the size of the models used in this work.
\end{ack}

\small
\bibliography{references}
\bibliographystyle{unsrtnat}

\end{document}
