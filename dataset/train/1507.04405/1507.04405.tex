\documentclass[reprint,twocolumn,showpacs,preprintnumbers,amsmath, aps,pre,amssymb]{revtex4-1}

\usepackage[algoruled]{algorithm2e}

\usepackage{tikz}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{times}
\usepackage{subcaption}

\captionsetup{compatibility=false}

\usetikzlibrary{arrows,shapes,snakes,automata,backgrounds,petri}


\usepackage{expl3}
\ExplSyntaxOn
\newcommand\latinabbrev[1]{
  \peek_meaning:NTF . {#1\@}{ \peek_catcode:NTF a {#1.\@ }{#1.\@}}}
\ExplSyntaxOff
\newcommand\etal{\latinabbrev{et al}}
\newcommand\vs{\latinabbrev{vs}}


\newcommand{\avg}[1]{\left\langle #1 \right\rangle}
\newcommand\kpr{k^{\prime}}
\newcommand{\xpr}{x^{\prime}}
\newcommand\eg{\emph{e.g.}}
\newcommand\ie{\emph{i.e.}}
\newcommand\etc{\emph{etc.}}
\newcommand\bcmdtab{\noindent\bgroup\tabcolsep=0pt\begin{tabular}{@{}p{10pc}@{}p{20pc}@{}}}
\newcommand\ecmdtab{\end{tabular}\egroup}
\newcommand\rch[1]{#1\hspace{1em}}
\newcommand\lra{\ensuremath{\quad\longrightarrow\quad}}
	
\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\tabref}[1]{Table~\ref{#1}}
 \newcommand{\remove}[1]{}
\usepackage{color}
\newcommand{\note}[2]{{\color{red} {\bf [#1]:}~#2}}
\newcommand{\noteTW}[1]{\note{TW}{#1}}
\newcommand{\sect}[1]{Section~\ref{#1}}
\newcommand{\sectA}[1]{Appendix Sec.~\ref{#1}}


\newcommand{\from}[2]{{\bf [{\sc from #1:} #2]}}
\newcommand{\nop}[1]{}


\textwidth 6in \textheight 9in \topmargin -0.5in \oddsidemargin
0.25in \evensidemargin 0.25in
\newcommand{\figwidth}{1.9in}
\newcommand{\figwidthW}{3.6in}


\usepackage{color}
\newcommand{\changed}[1]{{\color{red} #1}}

\usepackage[nolist]{acronym}

\begin{document}


\label{firstpage}


\title{Thinking Like a Vertex: a Survey of Vertex-Centric Frameworks for Large-Scale Distributed Graph Processing}
\author{Robert Ryan McCune}
 \email{rmccune@nd.edu}
\author{Tim Weninger}
 \email{tweninge@nd.edu}
\author{Greg Madey}
 \email{gmadey@nd.edu}
\affiliation{Department of Computer Science and Engineering
Notre Dame, IN 46556
}\date{\today}


\begin{abstract}
The vertex-centric programming model is an established computational paradigm recently incorporated into distributed processing frameworks to address challenges in large-scale graph processing.  Billion-node graphs that exceed the memory capacity of standard machines are not well-supported by popular Big Data tools like MapReduce, which are notoriously poor-performing for iterative graph algorithms such as PageRank.  In response, a new type of framework challenges one to ``think like a vertex" (TLAV) and implements user-defined programs from the perspective of a vertex rather than a graph.  Such an approach improves locality, demonstrates linear scalability, and provides a natural way to express and compute many iterative graph algorithms.  These frameworks are simple to program and widely applicable, but, like an operating system, are composed of several intricate, interdependent components, of which a thorough understanding is necessary in order to elicit top performance at scale.  To this end, the first comprehensive survey of TLAV frameworks is presented.  In this survey, the vertex-centric approach to graph processing is overviewed, TLAV frameworks are deconstructed into four main components and respectively analyzed, and TLAV implementations are reviewed and categorized.
\end{abstract}

\maketitle

\section{Introduction}
\label{sec:intro}
The proliferation of mobile devices,  ubiquity of the web, and plethora of sensors has led to an exponential increase in the amount data created, stored, managed, and processed.  In March 2014, an IBM report claimed that 90\% of the world's data had been generated in the last two years \cite{ibmbigdata}.  Big Data characterizes the problems faced by conventional analytics systems with this dramatic expansion of data volume, velocity, and variety.

To address the challenges posed by Big Data, analytical systems are shifting from shared, centralized architectures to distributed, decentralized architectures.  The MapReduce framework, and its open-source variant, Hadoop, exemplifies this effort by introducing a programming model to facilitate efficient, distributed algorithm execution while abstracting away lower-level details \cite{Dean2008}.  Since inception, the Hadoop/MapReduce ecosystem has grown considerably in support of related Big Data tasks.  

However, these distributed frameworks are not suited for all purposes, in many cases can even result in poor performance~\cite{Munagala1999,Cohen2009,Kang2009}. Algorithms that make use of multiple iterations, especially those using graph or matrix data representations, are particularly poorly suited for popular Big Data processing systems. 

Graph computation is notoriously difficult to scale and parallelize, often due to inherent interdependencies within graph data \cite{Lumsdaine2007}.  As Big Data drives graph sizes beyond the memory capacity of a single machine, data must be partitioned to out-of-memory storage or distributed memory.  However, for sequential graph algorithms, which require random access to all graph data, poor locality and the indivisibility of the graph structure cause time- and resource-intensive pointer-chasing between storage mediums in order to access each datum.

In response to these shortcomings, new frameworks based on the {\em vertex-centric programming model} have been developed with the potential to transform the ways in which researchers and practitioners approach and solve certain problems~\cite{Malewicz2010}.  Vertex-centric computing frameworks are platforms that iteratively execute a user-defined program over vertices of a graph.  The user-defined vertex function typically includes data from adjacent vertices or incoming edges as input, and the resultant output is communicated along outgoing edges.  Vertex program kernels are executed iteratively for a certain number of rounds, or until a convergence property is met.  As opposed to the randomly-accessible, ``global" perspective of the data employed by conventional shared-memory sequential graph algorithms, vertex-centric frameworks employ a local, vertex-oriented perspective of computation, encouraging practitioners to ``think like a vertex" (TLAV).

The first published TLAV framework was Google's Pregel system \cite{Malewicz2010}, which, based off of Valiant's Bulk Synchronous Parallel (BSP) model \cite{Valiant1990}, employs synchronous execution.  While not all TLAV frameworks are synchronous, these frameworks are first introduced here within the context of BSP in order to provide foundational understanding of TLAV concepts.

\subsection{Bulk Synchronous Parallel}

After spending a year with Bill McColl at Oxford in 1988, Les Valiant published the seminal paper on the Bulk Synchronous Parallel (BSP) computing model~\cite{Valiant1990} for guiding the design and implementation of parallel algorithms. Initially touted as ``A Bridging Model for Parallel Computation,'' the BSP model was created to simplify the design of software for parallel hardware, thereby ``bridging'' the gap between high-level programming languages and multi-processor systems. 

As opposed to distributed shared memory or other distributed systems abstractions, BSP makes heavy use of a message passing interface (MPI) which avoids high latency reads, deadlocks and race conditions. BSP is, at the most basic level, a two step process performed iteratively and synchronously: 1) perform task computation on local data, and 2) communicate the results, and then repeat the two steps. In BSP each compute/communicate iteration is called a {\em superstep}, with synchronization of the parallel tasks occurring at the superstep barriers, depicted in Figure~\ref{fig:bsp}.

\begin{figure*}[!ht]
\centering
\includegraphics[width=\textwidth]{./bsp_example}
\caption{Example of Bulk Synchronous Parallel execution with 3 tasks/workers over 4 supersteps. Each task may have varying durations after which messages are passed. The barriers control synchronization across the entire system.}
\label{fig:bsp}
\end{figure*}

\subsection{Graph Parallel Systems}

Introduced in 2010, the Pregel system~\cite{Malewicz2010} is a BSP implementation that provides an API specifically tailored for graph algorithms, challenging the programmer to ``think like a vertex.'' Graph algorithms are developed in terms of what each vertex has to compute based on local vertex data, as well as data from incident edges and adjacent vertices.  The Pregel framework, as well other synchronous TLAV implementations, split computation into BSP-style supersteps. Analogous to ``components" in BSP \cite{Valiant1990}, at each superstep a vertex can execute the user-defined vertex function and then send results to neighbors along graph edges. Supersteps always end with a synchronization barrier, shown in Figure~\ref{fig:bsp}, which guarantees that messages sent in a given superstep are received at the beginning of the next superstep. Unlike the original BSP model, vertices may change status between active and inactive, depending on the overall state of execution. Pregel terminates when all vertices halt and no more messages are exchanged.

A comparison of TLAV frameworks and BSP is presented in Figure~\ref{fig:bspvstlav}.  BSP employs a general model of broad applicability, including graph algorithms at varying levels of granularity.  Underlying BSP execution is the global synchronization barrier among distributed processors.  TLAV frameworks utilize a vertex-centric programming model, and while Pregel and its derivatives employ BSP-founded synchronous execution, other frameworks implement asynchronous execution, which has been demonstrated to improve performance in some instances \cite{Xie2013}.  

In contrast to TLAV and BSP, MapReduce does not natively support iterative algorithms.  Several recent frameworks have extended the MapReduce model to support iterative execution \cite{Kajdanowicz2014}, but for iterative graph algorithms, the graph topological data, which remains static, must be transferred from mappers to reducers, resulting in significant network overhead that renders iterative MapReduce frameworks uncompetitive with TLAV frameworks \cite{Kajdanowicz2014}.  A theoretical comparison between MapReduce and BSP is presented in \cite{bspvsmr}.

\begin{figure}
\centering
\def\firstcircle{(0,0) circle (1.4cm)}
\def\secondcircle{(0:5cm) circle (1.4cm)}
\def\thirdcircle{(0:1.6cm) circle (1.4cm)}

\begin{tikzpicture}
    \draw \firstcircle node[xshift=0,yshift=17mm] {TLAV};
    \draw node[text width=.5cm,align=center,xshift=-6mm] {Asyn-\-1em] nous};
    \draw node[text width=.5cm,align=center,xshift=7mm] {Syn-\-1em] nous};
    \draw \thirdcircle node [yshift=17mm] {BSP};
    \draw node[text width=.5cm,xshift=19mm] {Not Vertex-Centric};

\end{tikzpicture}

\caption{Comparison of the Think Like a Vertex (TLAV) and Bulk Synchronous Parallel (BSP) models of computation.  Both models are commonly employed for iterative computation.}

\label{fig:bspvstlav}

\end{figure}

\subsection{TLAV Frameworks}

Since Pregel, several TLAV frameworks have been proposed that either employ conceptually alternative framework components (such as asynchronous execution), or improve upon the Pregel model with various optimizations.  This survey provides the first comprehensive examination into TLAV framework concepts, and makes these other contributions:

\begin{enumerate}
\item Analyzes 4 principle components in the design of vertex programs execution in TLAV frameworks, identifying the trade-offs in component implementations and providing data-driven discussion
\item Overviews approaches related to TLAV system architecture, including fault tolerance on distributed systems and novel techniques for large-scale processing on single-machines
\item Discusses how the scalability of a graph algorithm varies inversely with the algorithm's scope, illustrated by vertex-centric and related subgraph-centric, or hybrid, frameworks
\end{enumerate}

This article is organized as follows: First, Section~\ref{sec:overview} overviews the vertex-centric programming model, including an example program and execution.  Section~\ref{sec:sys_theory} presents the four major design decisions, or pillars, of the vertex-centric model. Section~\ref{sec:implementation} presents details for distributed implementation, as well as novel techniques utilized by TLAV frameworks that enable large-scale graph processing on a single machine.  Section~\ref{sec:alt} presents subgraph-centric, or hybrid, frameworks, that adopt a computational scope of the graph that is greater than a vertex (TLAV) but less than the entire graph.  Section~\ref{sec:related} discusses related work.  Finally, Section~\ref{sec:conc} presents a summary, conclusions, and directions for future work.

First, a brief note on terminology:  The TLAV paradigm is described interchangeably as \textit{vertex-centric}, \textit{vertex-oriented}, or \textit{think-like-a-vertex}.  A \textit{vertex program kernel} refers to an instance of the user-defined vertex \textit{program}, \textit{function}, or \textit{process} that is executed on a particular vertex.  A graph is a data structure made up of vertices and edges, both with (potentially empty) data properties.  As in the literature, \textit{graph} and \textit{network} may be used interchangeably, as may \textit{node} and \textit{vertex}, and \textit{edge} and \textit{link}.  \textit{Network} may also refer to hardware connecting two or more machines, depending on context.  A \textit{worker} refers to a slave machine in the conventional master-worker architectural pattern, and a \textit{worker process} is the program that governs worker behavior, including, but not limited to, execution of vertex programs, inter-machine communication, termination, check-pointing, etc.  Graphs are assumed to be directed without loss of generality.

\section{Overview}
\label{sec:overview}

Graph processing is transitioning from centralized to decentralized design patterns.  Sequential, shared-memory graph algorithms are inherently centralized.  Conventional graph algorithms, such as Dijkstra's shortest path \cite{Dijkstra1971} or betweenness centrality \cite{Freeman1977}, receive the entire graph as input, presume all data is randomly accessible in memory ({\em i.e.}, graph-omniscient algorithms), and a centralized computational agent processes the graph in a sequential, top-down manner.  However, the unprecedented size of Big Data-produced graphs, which may contain hundreds of billions of nodes and occupy terabytes of data or more, exceed the memory capacity of standard machines.  Moreover, attempting to centrally compute graph algorithms across distributed memory results in unmanageable pointer-chasing \cite{Lumsdaine2007}.  A more local, decentralized approach is required for processing graphs of scale.

Think like a vertex frameworks are platforms that iteratively execute a user-defined program over vertices of a graph. The vertex program is designed from the perspective of a vertex, receiving as input the vertex's data as well as data from adjacent vertices and incident edges.  The vertex program is executed across vertices of the graph synchronously, or may also be executed asynchronously.  Execution halts after either a specified number of iterations, or all vertices have converged.  The vertex-centric programming model is less expressive than conventional graph-omniscient algorithms, but is easily scalable with more opportunity for parallelism. 

The frameworks are founded in the field of distributed algorithms. Although vertex-centric algorithms are local and bottom-up, they have a provable, global result.  TLAV frameworks are heavily influenced by distributed algorithms theory, including synchronicity and communication mechanisms \cite{Lynch1996}.  Several distributed algorithm implementations, such as distributed Bellman-Ford single-source shortest path \cite{Lynch1996}, are used as benchmarks throughout the TLAV literature.  The recent introduction of TLAV frameworks has also spurred the adaptation of many popular Machine Learning and Data Mining (MLDM) algorithms into graph representations for high-performance TLAV processing of large-scale data sets \cite{Low2010}.

Many graph problems can be solved by both a sequential, shared-memory algorithm as well as a distributed, vertex-centric algorithm.  For example, the PageRank algorithm for calculating web-page importance has a centralized matrix form \cite{Page1999} as well as a distributed, vertex-centric form \cite{Malewicz2010}.  The existence of both forms illustrates that many problems can be solved in more than one way, by more than one approach or computational perspective, and deciding which approach to use depends on the task at hand.  While the sequential, shared-memory approach is often more intuitive and easier to implement on a single machine or centralized architecture, the limits of such an approach are being reached. 

Vertex programs, in contrast, only depend on data local to a vertex, and reduce computational complexity by increasing communication between program kernels.   As a result, TLAV frameworks are highly scalable and inherently parallel, with manageable inter-machine communication.  For example, runtime on the Pregel framework has been shown to scale linearly with the number of vertices on 300 machines \cite{Malewicz2010}.  Furthermore, TLAV frameworks provide a common interface for vertex-program execution, abstracting away low-level details of distributed computation, like MPI, allowing for a fast, re-usable development environment.  A paradigm shift from centralized to decentralized approaches to problem solving is represented by TLAV frameworks.

\subsection{Example: Single Source Shortest Path in TLAV paradigm}
\label{subsec:algo}

The following describes a simple vertex program that calculates the shortest paths from a given vertex to all other vertices in a graph. In contrast to this distributed implementation example, consider a centralized, sequential, shared-memory, or ``graph-omniscient," solution to the single-source shortest path algorithm known as Djikstra's algorithm~\cite{Dijkstra1959} or the more general Bellman–Ford algorithm~\cite{Bellman1958}.

Both Dijkstra's and the Bellman-Ford algorithms are based on repeated relaxations, which iteratively replace distance estimates with more accurate values until eventually reaching the solution. Both variants are have a superlinear time complexity: Djisktra's runs in  and Bellman-Ford's runs in , where  is the number of edges and  is the number of vertices in the graph and typically . Perhaps more importantly, both procedural, shared-memory algorithms keep a large state matrix resulting in a space complexity of .

\begin{algorithm*}[t]
\small{
\SetKwInOut{Input}{input}
\SetKwFunction{send}{send}\SetKwFunction{receive}{receive}
\SetKwFunction{halt}{halt}\SetKwFunction{min}{min}
\SetAlgoNoLine
    \Input{A graph  with vertices  and edges from  s.t. , \\ and starting point vertex }
    \BlankLine
    \lForEach(\tcc*[f]{initialize each vertex data to }){}{shrtest\_path\_len}
    \send(0, )\tcc*[r]{to activate, send msg of 0 to starting point}
    \Repeat(\tcc*[f]{The outer loop is synchronized with BSP-styled barriers}){no more messages are sent}{
        \Indp\ForPar(\tcc*[f]{vertices execute in parallel}){}{
            \LinesNumbered
            \Indp\tcc{vertices inactive by default; activated when msg received}
            \tcc{compute minimum value received from incoming neighbors}
            minIncomingData(\receive(path\_length))\;
            \tcc{set current vertex-data to minimum value}
            \If{\emph{minIncomingData}  \emph{shrtest\_path\_len}}{
             \Indp   shrtest\_path\_len minIncomingData\;
                \ForEach(){} {
                \Indp\tcc{send shortest path + edge weight to outgoing edges}
                path\_length  shrtest\_path\_lenweight\;
                \send(path\_length, )\;}
            }
            \halt()\;
        }
    }
 \caption{Single Source Shortest Path for a Synchronized TLAV Framework}
 \label{alg:sssp}
 }
\end{algorithm*}

In contrast, to solve the same single-source shortest path problem in the TLAV programming model, a vertex program need only pass the minimum value of its incoming edges to its outgoing edges during each superstep. This algorithm, considered a distributed version of Bellman-Ford \cite{Lynch1996}, is shown in Alg.~\ref{alg:sssp}.  The computational complexity of each vertex program kernel is less than that of the sequential solution, however a new dimension is introduced in terms of the communication complexity, or the messaging between vertices \cite{Lynch1996}.  For TLAV implementation, a user need only to write the inner-portion of Alg.~\ref{alg:sssp} denoted by line numbers; the outermost loop and the parallel execution is handled by the framework. Because lines 1-10 are executed on the each vertex these lines are known as the \emph{vertex program}.

The TLAV-solution to the single source shortest path problem has surprisingly few lines of code, and understating its execution requires a different way of thinking.  

Figure~\ref{fig:minval} depicts the execution of Alg.~\ref{alg:sssp} for a graph with 4 vertices and 6 weighted directed edges. Only the source vertex begins in an active state.  In each superstep, a vertex processes its incoming messages, determines the smallest value among all messages received, and if the smallest received value is less than the vertex's current shortest path, then the vertex adopts the new value as its shortest path, and sends the new path length plus respective edge weights to outgoing neighbors.  If a vertex does not receive any new messages, then the vertex becomes inactive, represented as a shaded vertex in Figure~\ref{fig:minval}.  Overall execution halts once no more messages are sent and all vertices are inactive.

\begin{figure*}
\centering

\begin{tikzpicture}[node distance=1.3cm,bend angle=45,auto]
  \tikzstyle{active}=[circle,thick,draw=black!75,fill=white!20,minimum size=6mm]
  \tikzstyle{inactive}=[circle,thick,draw=black!75,fill=gray!20,minimum size=6mm]
  \tikzstyle{every label}=[red]

  \begin{scope}
    \node [inactive] (A1) {};
   \node [inactive] (B1) [right of=A1] {}
        edge [<->,bend right] node[above] {2} (A1);
	\node [active] (C1) [right of=B1] {0}
	    edge [->,bend left] node[above] {2} (B1);
	\node [inactive] (D1) [right of=C1] {}
	    edge [<-, bend right] node[above] {1} (B1)
        edge [<->] node[above] {4} (C1);
    \node (lab1) [right of=D1, xshift=15mm,text width=4cm] {\textit{Superstep 0} \\ \hspace{1mm} message values = 2 and 4};
	
	\node [inactive] (A2) [below of=A1] {};
   \node [active] (B2) [right of=A2] {2}
        edge [<->,bend right] (A2)
        edge [<-, dashed] (C1);
	\node [inactive] (C2) [right of=B2] {0}
	    edge [->,bend left] (B2);
	\node [active] (D2) [right of=C2] {4}
	    edge [<-, bend right] (B2)
        edge [<->] (C2)
        edge [<-, dashed] (C1);
    \node (lab2) [right of=D2, xshift=20mm,text width=5cm] {\textit{Superstep 1} \\ \hspace{1mm} message values = 4, 3, and 8};
	
	\node [active] (A3) [below of=A2] {4}
        edge [<-, dashed] (B2);
   \node [inactive] (B3) [right of=A3] {2}
        edge [<->,bend right] (A3);
	\node [inactive] (C3) [right of=B3] {0}
	    edge [<-, dashed] (D2)
	    edge [->,bend left] (B3);
	\node [active] (D3) [right of=C3] {3}
	    edge [<-, bend right] (B3)
        edge [<->] (C3)
        edge [<-, dashed]  (B2);
    \node (lab3) [right of=D3, xshift=15mm,text width=4cm] {\textit{Superstep 2} \\ \hspace{1mm} message values = 6 and 7};
	
	\node [inactive] (A4) [below of=A3] {4} ;
   \node [inactive] (B4) [right of=A4] {2}
        edge [<->,bend right] (A4)
        edge [<-, dashed] (A3);
	\node [inactive] (C4) [right of=B4] {0}
	    edge [<-, dashed] (D3)
	    edge [->,bend left] (B4);
	\node [inactive] (D4) [right of=C4] {3}
	    edge [<-, bend right] (B4)
        edge [<->] (C4); 
    \node (lab4) [right of=D4, xshift=20mm,text width=5cm] {\textit{Superstep 3} \\ \hspace{1mm} Complete, no new messages};
	
  \end{scope}

\end{tikzpicture}

\caption{Computing the Single Source Shortest Path in a graph.  Dashed lines between supersteps represent messages (with values listed to the right), and shaded vertices are inactive.  Edge weights pictorially included in first layer for Superstep 0, then subsequently omitted.} \label{fig:minval}
\end{figure*}

With this example providing insight into TLAV operation, particularly the synchronous message-passing model of Pregel, the survey continues by more completely detailing TLAV properties and categorizing different TLAV frameworks.

\section{Four Pillars of TLAV Frameworks}
\label{sec:sys_theory}

A TLAV framework is software that supports the iterative execution of a user-defined vertex programs over vertices of a graph.  Frameworks are composed of several interdependent components that drive program execution and ultimate system performance.  These frameworks are not unlike an analytic operating system, where component design decisions dictate how computations for a particular topology utilize the underlying hardware.

This section introduces the four principle pillars of TLAV frameworks.  They are:

\begin{enumerate}
\item Timing - How user-defined vertex programs are scheduled for execution
\item Communication - How vertex program data is made accessible to other vertex programs
\item Execution Model - Implementation of vertex program execution and flow of data
\item Partitioning - How vertices of the graph, originally in storage, are divided up to be stored across memory of the system's multiple\footnote{TLAV systems generally distribute a graph across multiple machines because of the graph's prohibitive size.  However, regarding the categorization of ``single machine frameworks" in Section~\ref{sec:arch}, while some TLAV frameworks are implemented for a single machine without the specific intention of developing for a non-distributed environment ( {\em e.g.} the framework is first developed for a single machine before developing the framework for a distributed environment, like the original GraphLab [which published a distributed version 2 years later, see Section~\ref{subsubsec:async}] or GRACE [see Section~\ref{subsubsec:hybrid}]), the single-machine frameworks presented in Section~\ref{sec:arch} are frameworks that implement particularly \textit{novel} methods with the \textit{stated objective} of processing, on a single machine, graphs of size that exceed the single machine's memory capacity.  These single machine frameworks still partition the graph, using framework-specific methods detailed in the respective section.} worker machines
\end{enumerate}

The discussion proceeds as follows: the timing policy of vertex programs is presented in Subsection~\ref{subsec:execution_policy}, where system execution can be synchronous, asynchronous, or hybrid.  Communication between vertex programs is presented in Subsection~\ref{subsec:communication}, where intermediate data is shared primarily through message-passing or shared-memory.  The implementation of vertex program execution is presented in Subsection~\ref{subsec:comp_models}, which overviews popular models of program execution and demonstrates how a particular model implementation impacts execution and performance.  Finally, partitioning of the graph from storage into distributed memory is presented in Subsection~\ref{subsec:partitioning}.

Each pillar is heavily interdependent with other pillars, as each design decision is tightly integrated and strongly influenced by other design decisions.  While each pillar may be understood through a sequential reading of the information provided, a more efficient, yet thorough understanding may be achieved by freely forward- and cross-referencing other pillars, especially when related sections are cited.  The inter-relation of the four pillars is unavoidable and indivisible, \textit{not unlike a graph data structure itself.}  The difficulty of independently describing each pillar certainly reflects the challenge of processing a vertex in which a given result depends on the concurrent processing of neighboring vertices.  This survey is restricted to a sequential presentation of information in the form of a paper.  However, each pillar, though unique, depends on, and may only be described in relation to, other pillars, so a sufficient understanding of any given pillar may only be achieved by understanding all pillars of a TLAV framework, collectively.  Thus one may begin to understand the challenges of processing graphs (especially large graphs, when not all ``pillars" are in the same ``paper") as in Section~\ref{sec:intro}, Section~\ref{sec:overview}, and \cite{Lumsdaine2007}.

\subsection{Timing}
\label{subsec:execution_policy}
In TLAV frameworks, the scheduling and timing of the execution is separate from the logic of the vertex program.  The \textit{timing} of a framework characterizes how active vertices are ordered by the scheduler for computation.  Timing can be synchronous, asynchronous, or a hybrid of the two models.  Frameworks that represent the different fundamental timing models are presented in Table~\ref{table:timing}.

\subsubsection{Synchronous}
\label{subsubsec:sync}
The \textit{synchronous} timing model is based on the original bulk synchronous parallel (BSP) processing model discussed above. In this model, active vertices are executed conceptually in parallel over one or more iterations, called \textit{supersteps}. Synchronization is achieved through a global synchronization \textit{barrier} situated between each superstep that blocks vertices from computing the next superstep until all workers complete the current superstep. Each worker coordinates with the master to progress to the next superstep. Synchronization is achieved because the barrier ensures that each vertex within a superstep has access to only the data from the previous superstep. Within a single processing unit, vertices can be scheduled in a fixed or random order because the execution order does not affect the state of the program.  The global synchronization barrier introduces several performance trade-offs.

Synchronous systems are conceptually simple, demonstrate scalability, and perform exceptionally well for certain classes of algorithms.  While not all TLAV programs consistently converge to the same values depending on system implementation, synchronous systems are almost always deterministic, making synchronous applications easy to design, program, test, debug, and deploy.  Although coordinating synchronization imposes consistent overhead, the overhead becomes largely amortized for large graphs.  Synchronous systems demonstrate good scalability, with runtime often linearly increasing with the number of vertices \cite{Malewicz2010}.  As will be discussed in Section~\ref{subsubsec:msg}, synchronous systems are often implemented along with message-passing communication, which enables a more efficient ``batch messaging'' method. Batch messaging can especially benefit systems with lots of network traffic induced by algorithms with a low computation-to-communication ratio \cite{Xie2013}.

Although synchronous systems are conceptually straight-forward and scale well, the model is not without drawbacks.  One study found that synchronization, for an instance of finding the shortest path in a highly-partitioned graph, accounted for over 80\% of the total running time \cite{Chen}, so system throughput must remain high to justify the cost of synchronization, since such coordination can be relatively costly.  However, when the number of active vertices drops or the workload amongst workers becomes imbalanced, system resources can become under-utilized.   Iterative algorithms often suffer from ``the curse of the last reducer'' otherwise known as the ``straggler'' problem where many computations finish quickly, but a small fraction of computations take a disproportionately longer amount of time \cite{Suri2011}. {\em For synchronous systems, each superstep takes as long as the slowest vertex}, so synchronous systems generally favor lightweight computations with small variability in runtime.

Finally, synchronous algorithms may not converge in some instances.  In graph coloring algorithms, for example, vertices attempt to choose colors different than adjacent neighbors \cite{Gonzalez2011} and require coordination between neighboring vertices. However, during synchronous execution, the circumstance may arise where two neighboring vertices continually flip between each others' color.  In general, algorithms that require some type of neighbor coordination may not always converge with the synchronous timing model without the use of some extra logic in the vertex program \cite{Xie2013}.

\begin{table}[t]
\centering
\small{
\begin{tabular} {l | c l}
Framework & Timing & \\ \hline
Pregel      &   Synchronous & \cite{Malewicz2010} \\
Giraph      & Synchronous   & \cite{Avery2011} \\
Hama        & Synchronous   & \cite{Seo2010} \\
GraphLab    & Asynchronous  & \cite{Low2012,Low2010} \\
PowerGraph  & Both          & \cite{Gonzalez2012} \\ 
PowerSwitch & Hybrid        & \cite{Xie2013} \\
GRACE       & Hybrid        & \cite{Wang2013} \\
GraphHP     & Hybrid        & \cite{Chen} \\
P++         & Hybrid        & \cite{Zhou2014}
\end{tabular}
}
\caption{Execution timing model of selected frameworks.}
\label{table:timing}
\end{table}

\subsubsection{Asynchronous}
\label{subsubsec:async}
In the asynchronous iteration model, no explicit synchronization points, {\em i.e.}, barriers, are provided, so any active vertex is eligible for computation whenever processor and network resources are available.  Vertex execution order can be dynamically generated and reorganized by the scheduler, and the ``straggler'' problem is eliminated. As a result, many asynchronous  models outperform corresponding synchronous models, but at the expense of added complexity.

Theoretical and empirical research has demonstrated that asynchronous execution can generally outperform synchronous execution \cite{Bertsekas1989,Low2012}, albeit precise comparisons for TLAV frameworks depend on a number of properties \cite{Xie2013}.  Asynchronous systems especially outperform synchronous systems when the workload is imbalanced. For example, when computation per vertex varies widely, synchronous systems must wait for the slowest computation to complete, while asynchronous systems can continue execution maintaining high throughput. One disadvantage, however, is that asynchronous execution cannot take advantage of batch messaging optimizations (see Section~\ref{subsubsec:opts}). Thus, synchronous execution generally accommodates I/O-bound algorithms, while asynchronous execution well-serves CPU-bound algorithms by adapting to large and variable workloads.

Many iterative algorithms exhibit asymmetric convergence.  Low {\em et al}. demonstrated that, for PageRank, the majority of vertices converged within one superstep, while only 3\% of vertices required more than 10 supersteps \cite{Low2012}.  Asynchronous systems can utilize prioritized computation via a dynamic schedule to focus on more challenging computations early in execution to achieve better performance \cite{Zhang2011,Low2012}.  Generally, asynchronous systems perform well by providing more execution flexibility, and by adapting to dynamic or variant workloads.

Although intelligent scheduling can improve performance, schedules resulting in sub-optimal performance are also possible.  In some instances, a vertex may perform more updates than necessary to reach convergence, resulting in excessive computation \cite{Zhang2013}.  Moreover, if implementing the pull model of execution, which is commonly implemented in asynchronous systems \cite{Low2012} and described in Section~\ref{subsubsec:pushpull}, communication becomes redundant when neighboring vertex values don't change \cite{Zhang2013,Hant2014}.

The flexibility provided by asynchronous execution comes at the expense of added complexity, not only from scheduling logic, but also from maintaining data consistency.  Asynchronous systems typically implement shared memory, discussed in Section~\ref{subsubsec:shared}, where data race conditions can occur when parallel computations simultaneously attempt to modify the same data.  Additional mechanisms are necessary to ensure mutual exclusion, which can challenge algorithm development because framework users may have to consider low-level concurrency issues \cite{Wang2013}, like, for example, in GraphLab where users must select a consistency model \cite{Low2012}.

\subsubsection{Hybrid}
\label{subsubsec:hybrid}
Rather than adhering to the inherent strengths and weaknesses of a strict execution model, several frameworks work around a particular shortcoming through design improvements.  One such implementation, GraphHP, reduces the high fixed cost of the global synchronization barrier using {\em pseudo-supersteps} \cite{Chen}. Another implementation, GRACE, explores dynamic scheduling within a single superstep \cite{Wang2013}. The PowerSwitch system removes the need to choose between synchronous and asynchronous execution and instead adaptively switches between the two modes to improve performance \cite{Xie2013}.  Together, these three frameworks illustrate how weaknesses with a particular execution model can be overcome through engineering and problem solving, rather than strict adoption of an execution model.

As previously discussed, synchronous systems suffer from the high, fixed cost of the global synchronization barrier.  The hybrid execution model introduced by GraphHP, and also used by P++ framework \cite{Zhou2014}, reduces the number of supersteps by decoupling intra-processor computation from the inter-processor communication and synchronization \cite{Chen}.  To do this GraphHP distinguishes between two types of nodes: \textit{boundary nodes} that share an edge across partitions, and \textit{local nodes} that only have neighboring nodes within the local partition.  During synchronization, messages are only exchanged between boundary nodes. As a result, in GraphHP, a given superstep is composed of two phases: global and local. The global phase, which is executed first, runs the user program across all boundary vertices using data transmitted from other boundary vertices as well as its own local vertices. Once the global phase is complete, the local phase executes the vertex program on local vertices within a pseudo-superstep; the pseudo-superstep is different from a regular superstep in that: 1) pseudo-supersteps have local barriers resulting in local iterations independent of any global synchronization or communication; and 2) local message passing is done through direct, in-memory message passing, which is much faster than standard MPI-style messages.

A similar approach to segmented execution, as in GraphHP and P++, is the KLA paradigm \cite{Harshvardhan2014}, which creates a hybrid of synchronous and asynchronous execution.  For graphs, the depth of asynchronous execution is parameterized, and asynchronous execution is allowed for a certain number of levels before a synchronous round.  Similar to how GraphHP implements a round of boundary vertex execution before several rounds of local execution, KLA has multiple traversals of asynchronous execution before coordinating a round of synchronous execution.  The trade-off is between expensive global synchronizations with cheap but possibly redundant asynchronous computations.  KLA is also similar to delta-stepping used for single source shortest path \cite{Meyer2003}.

The single-machine framework GRACE explores dynamic scheduling of vertices from within a single synchronous round\cite{Wang2013}. To do this GRACE exposes a programming interface that, from within a given superstep, allows for prioritized execution of vertices and selective receiving of messages outside of the previous superstep.  Results demonstrate comparable runtime to asynchronous models, with better scaling across multiple worker threads on a single machine.

Knowing \textit{a priori} which execution mode will perform better for a given problem, algorithm, system, or circumstance is challenging.  Furthermore, the underlying properties that give one execution model an advantage over another may change over the course of processing.  For example, in the distributed Single Source Shortest Path algorithm \cite{Bertsekas1996}, the process begins with few active vertices, where asynchronous execution is advantageous, then propagates to a high number of active vertices performing lightweight computations, which is ideal for synchronous execution, before finally converging amongst few active vertices \cite{Xie2013}.  For some algorithms, one execution mode may outperform another only for certain stages of processing, and the best mode at each stage can be difficult to predict.

Motivated by the necessity for execution mode dynamism, PowerSwitch was developed to adaptively switch between synchronous and asynchronous execution modes \cite{Xie2013}.  Developed on top of the PowerGraph platform, PowerSwitch can quickly and efficiently switch between synchronous and asynchronous execution.  PowerSwitch incorporates throughput heuristics with online sampling to predict which execution mode will perform better for the current period of computation.  Results demonstrate that the PowerSwitch's heuristics can accurately predict throughput, the switching between the two execution modes is well-timed, and overall runtime is improved for a variety of algorithms and system configurations \cite{Xie2013}.

\subsection{Communication}
\label{subsec:communication}

Communication in TLAV frameworks entails how data is shared between vertex programs.  The two conventional models for communication in distributed systems, as well as distributed algorithms, are message passing and shared memory \cite{Yan2013,Lu1995,Lynch1996}.  In message passing systems, data is exchanged between processes through messages, whereas in shared memory systems data for one process is directly and immediately accessible by another process.  This section compares and contrasts message passing and shared memory for TLAV frameworks. A third method of communication, active messages, is also presented. Finally, techniques to optimize distributed message passing are discussed.

Diagrams in Figure~\ref{fig:comm_imp} are referenced throughout this section to illustrate the different communication implementations.  A sample graph is presented in Figure~\ref{fig:samp_graph}, and Figures~\ref{fig:pregel}-\ref{fig:gre} depict 4 TLAV communication implementations of the sample graph.  For each implementation, vertices are partitioned across 2 machines, namely, vertices A, B, and C are partitioned to machine p1, and vertices D, E, and F are put on machine p2 (except Figure~\ref{fig:powergraph} and \ref{fig:gre}, where the graph is cut along vertex C).  Solid arrows represent local communication\footnote{Local communication means communication between vertices residing on the same machine} and dashed arrows represent network traffic.  


\begin{figure*}
    \centering
    \begin{subfigure}[b]{.28\textwidth}
        \centering
        \raisebox{12mm}{\includegraphics[width=\textwidth]{sample_graph.png}}
        \caption{Sample Graph}
        \label{fig:samp_graph}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{.7\textwidth}
        \centering    
    
        \begin{subfigure}[b]{.43\textwidth}
            \centering
            \includegraphics[width=.75\textwidth]{pregel.png}
            \caption{Message Passing}
            \label{fig:pregel}
        \end{subfigure}
        ~
        \begin{subfigure}[b]{.5\textwidth}
            \centering
            \includegraphics[width=\textwidth]{graphlab.png}
            \caption{Shared Memory}
            \label{fig:graphlab}
        \end{subfigure}
    
        \begin{subfigure}[b]{.45\textwidth}
            \centering
            \includegraphics[width=.77\textwidth]{powergraph.png}
            \caption{Shared Mem w/Vertex-Cuts}
            \label{fig:powergraph}
        \end{subfigure}
        ~
        \begin{subfigure}[b]{.4\textwidth}
            \centering
            \includegraphics[width=.9\textwidth]{GREscatter.png}
            \caption{Active Msgs w/Agent-Graph Scatter Vertex}
            \label{fig:gre}
        \end{subfigure}
    
    \end{subfigure}
    
    \caption{Distributed communication patterns for common communication implementations.  The sample graph is partitioned across two machines (see Section~\ref{subsec:partitioning}), with vertices A, B, and C residing on machine p1, and vertices D, E, and F on machine p2.  Pregel is represented in (b), GraphLab in (c), PowerGraph in (d), and GRE in (e).}
    \label{fig:comm_imp}
\end{figure*}

\subsubsection{Message Passing}
\label{subsubsec:msg}

In the message passing method of communication, also known as the LOCAL model of distributed computation \cite{Peleg}, information is sent from one vertex program kernel to another via a message. A message contains local vertex data and is addressed to the ID of the recipient vertex.  In the archetypal message-passing framework Pregel \cite{Malewicz2010}, a message can be addressed anywhere, but because vertices do not have ID information of all of other vertices, destination vertex IDs are typically obtained by iterating over outgoing edges.

After computation is complete and a destination ID for each message is determined, the vertex dispatches messages to the local worker process.  The worker process determines whether the recipient resides on the local machine or a remote machine.  In the case of the former, the worker process can place the message directly into the vertex's incoming message queue.  Else, the worker process looks up the worker-id of the destination vertex\footnote{If the graph is partitioned using random hash partitioning, then the destination worker can be determined by hashing the destination vertex ID.  Otherwise, for more advanced partitioning methods (see Section~\ref{subsec:partitioning}), the worker process typically has access to a local routing table, provided by the master during initialization.} and places the message in an outgoing message buffer.  The outgoing message buffer in Pregel, a synchronously-timed system, is flushed when it reaches a certain capacity, sending messages over the network in batches. Waiting until the end of a superstep to send all outgoing remote messages can exceed memory limits \cite{Satish}.

Message passing is commonly implemented with synchronized execution, which guarantees data consistency without low-level implementation details.  All messages sent during superstep  are received in superstep , at which point a vertex program can access the incoming message queue at the beginning of 's program execution.  Synchronous execution also facilitates batch messaging, which improves network throughput.  For I/O bound algorithms with lightweight computation, such as PageRank \cite{Brin1998}, where vertices are ``always active" so messaging is high \cite{Shang2013}, synchronous execution has been shown to significantly outperform asynchronous execution \cite{Xie2013}.

Message passing is depicted in Figure~\ref{fig:pregel}, where vertex  sends (an) inter-machine message(s) to vertices , , and .  Technically, messages are first sent from  to the worker process of , which routes the messages to worker process , which places the message in a vertex's incoming message queue, but the worker process-related routing is omitted from the figure without loss of generality.  Figure~\ref{fig:pregel} represents a general message passing framework, such as Pregel or Giraph.  The three messages sent by  across the network can be potentially reduced using optimization techniques in Section~\ref{subsubsec:opts}, namely, Receiver-side Scatter, depicted in Figure~\ref{fig:rec_scat}.  

\subsubsection{Shared Memory}
\label{subsubsec:shared}

Shared memory exposes vertex data as shared variables that can be directly read or be modified by other vertex programs.  Shared memory avoids the additional memory overhead constituted by messages, and doesn't require intermediate processing by workers.  Shared memory is often implemented by TLAV frameworks developed for a single machine (see Section~\ref{sec:arch}), since challenges to a shared memory implementation arise in the distributed setting \cite{Protic1998,Nitzberg1991}, where consistency must be guaranteed for remotely-accessed vertices.  Inter-machine communication for distributed shared memory still occurs through network messages.  The Trinity framework \cite{Shao2013} implements a shared global address space that abstracts away distributed memory.

For shared memory TLAV frameworks, race conditions may arise when an adjacent vertex resides on a remote machine.  Shared memory TLAV frameworks often ensure memory consistency through mutual exclusion by requiring serializable schedules. Serializability, in this case, means that every parallel execution has a corresponding sequential execution that maintains consistency, {\em cf.}, the dining philosophers problem \cite{Low2012,Gonzalez2012}.  

In GraphLab \cite{Low2012} border vertices are provided locally-cached \textit{ghost} copies of remote neighbors, where consistency between ghosts and the original vertex is maintained using pipelined distributed locking \cite{Dijkstra1971}.  In PowerGraph \cite{Gonzalez2012}, the second generation of GraphLab, graphs are partitioned by edges and cut along vertices (see vertex-cuts in Section~\ref{subsec:partitioning}), where consistency across cached \textit{mirrors} of the cut vertex is maintained using parallel Chandy-Misra locking \cite{Chandy1984}.  GiraphX is a Giraph derivative with a synchronous shared memory implementation \cite{Tasci2013}, which again provides serialization through Chandy-Misra locking of border vertices, although without local cached copies.  The reduced overhead of shared memory compared to message passing is demonstrated by GiraphX, which converges 35\% faster than Giraph when computing PageRank on a large Web Graph \cite{Tasci2013}.  Moreover, some iterative algorithms perform better under serialized conditions, such as Dynamic ALS \cite{Zhou2008,Low2012}, and popular Gibbs sampling algorithms that actually require serializability for correctness \cite{Gonzalez2011}.  

Shared memory implementations are depicted in Figure~\ref{fig:graphlab} and Figure~\ref{fig:powergraph}.  In Figure~\ref{fig:graphlab}, ghost vertices, represented by dashed circles, are created for every neighboring vertex residing on a remote machine, as implemented by GraphLab \cite{Low2012}.  One disadvantage of shared-memory frameworks is seen when computing on scale-free graphs which have a certain percentage of high degree vertices, such as vertex . In these cases the graph can be difficult to partition \cite{Leskovec2009} resulting in many ghost vertices.

Figure~\ref{fig:powergraph} depicts shared memory with vertex cuts as implemented by PowerGraph \cite{Gonzalez2012}.  PowerGraph combines vertex-cuts (discussed in Section~\ref{subsec:partitioning}) with the three-phase Gather-Apply-Scatter computational model (see Section~\ref{subsubsec:gas}) to improve processing of scale-free graphs.  In Figure~\ref{fig:powergraph}, the graph is cut along vertex , where  is arbitrarily chosen as the master and  as the mirror.  For each iteration, a distributed vertex preforms computation where: (i) both  and  compute a partial result based on local neighbors, (ii) the partial result is sent over the network from the mirror  to the master , (iii) the master computes the final result for the iteration, (iv) the master transmits the result back to the mirror over the network, then (v) the result is sent to local neighbors as necessary.  PowerGraph demonstrates how the combination of advanced components, {\em i.e.}, vertex-cuts and three-phase computation, can overcome processing challenges like imbalances arising from high-degree vertices in scale-free graphs.  

Shared memory systems are often implemented with asynchronous execution.  Although consistency is fundamentally maintained in synchronous message passing frameworks like Pregel, asynchronous, shared memory frameworks like GraphLab may execute faster because of prioritized execution and low communication overhead, but at the expense of added complexity for scheduling and maintaining consistency.  The added complexity challenges scalability, for as the number of machines and partitions increase, more time and resources become devoted to locking protocols.  

Dynamic computation addresses asymmetric convergence by only updating necessary vertices.  Shared memory with asynchronous execution is an effective platform for dynamic computation, because the movement of data is separated from computation, allowing vertices to access neighboring values even if the values haven't changed between iterations.  This implies the {\em pull} mode of information flow \ref{subsubsec:pushpull}.  In contrast, a vertex in a message-passing framework would need all neighboring values delivered in order to perform an update, even if some values had not changed.  Dynamic computation is possible with message passing in the Cyclops framework, which implements a {\em distributed immutable view}.  Cyclops is a synchronous shared memory framework \cite{Chen2014}, where one of the replicated vertices is designated the master, which computes updates and messages the updated state to replicas at the end of an iteration.  Cyclops outperforms synchronous message passing frameworks by reducing the amount of processing performed by each worker parsing messages, and is comparable to PowerGraph by delivering significantly fewer messages.

Significant deterioration in performance was noted in \cite{Han2014,Lu2014} for larger graphs, although admittedly performance largely depends on algorithm behavior \cite{Xie2013,Shang2013}.  In short, asynchronous shared memory systems can potentially outperform synchronous message passing systems, though the latter often demonstrate better scalability and generalization.

\subsubsection{Active Messages}
\label{subsubsec:acivemsg}

While message passing and shared memory are the two most commonly implemented forms of communication in distributed systems, a third method called {\em active messages} is implemented in the GRE framework \cite{Yan2013}\footnote{Active messages as described in this section are different from message-passing messages that activate a vertex, like in Pregel}.  Active messaging is a way of bringing computation to data, where a message contains both data as well as the operator to be applied to the data \cite{VonEicken1992}.  Active messages are sent asynchronously, and executed upon receipt by the destination vertex.  Within the GRE architecture, active messages combine the process of sending and receiving messages, removing the need to store intermediate state, like message queues or edge data.  When combined with the framework's novel Agent-Graph model, described below, GRE demonstrates 20\%--55\% reduction in runtime compared to PowerGraph across three benchmark algorithms in real and synthetic datasets, including 39\% reduction in the execution time per iteration for PageRank on the Twitter graph when scaled across 192 cores over 16 machines when compared to a PowerGraph implementation on 512 cores across 64 machines \cite{Yan2013}.

The GRE framework modifies the data graph into an Agent-Graph.  The Agent-Graph is a model used internally by the framework, but is not accessible to the user.  The Agent-Graph adds \textit{combiner} and \textit{scatter} vertices to the original graph in order to reduce inter-machine messaging.  Figure~\ref{fig:gre} shows that an extra \textit{scatter} vertex, , is added to create the internal Agent-Graph model. The  vertex acts as a Receiver-side Scatter depicted in Figure~\ref{fig:rec_scat}.  This is useful because the new  vertex allows  to only send one message across the network, which  then disperses to vertices , , and .  Combiner vertices are also added to the Agent-Graph in the same way as Server-side Aggregation depicted in Figure~\ref{fig:send_agg}.  The Agent-Graph employed by GRE is similar to vertex-cuts in PowerGraph except that GRE messaging is unidirectional, and active messages are also utilized for parallel graph computation in the Active Pebbles framework \cite{Willcock2011,Edmonds2013}.

\subsubsection{Message Passing Optimizations}
\label{subsubsec:opts}

Message passing can be costly, especially over a network. Thus several message-reducing strategies have been developed in order to improve performance.  Some strategies are topology-driven and, as such, exploit the graph layout across machines, while other techniques are applied to specific algorithmic behavior.  Three topology-driven optimizations are depicted in Figure~\ref{fig:opt} for messaging between machines  and  (or messaging from , , and  to , for Figure~\ref{fig:rec_agg}). 

\begin{figure*}
    \centering
    
    \begin{subfigure}[b]{.3\textwidth}
        \centering
        \raisebox{5mm}{\includegraphics[width=\textwidth]{serv_comb.png}}
        \caption{Sender-side Combiner}
        \label{fig:send_agg}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{.3\textwidth}
        \centering
        \includegraphics[width=.9\textwidth]{combiner.png}
        \caption{Receiver-side Combiner}
        \label{fig:rec_agg}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{.3\textwidth}
        \centering
        \raisebox{4.5mm}{\includegraphics[width=.9\textwidth]{rec_scat.png}}
        \caption{Receiver-side Scatter}
        \label{fig:rec_scat}
    \end{subfigure}
    
    \caption{Partition-driven optimization strategies for distributed message passing.  The Combiner technique employs both Sender-side and Receiver-side Combiners.}
    \label{fig:opt}
\end{figure*}

The Combiner, inspired by the MapReduce function of the same name \cite{Dean2008}, is a message passing optimization originally used by Pregel \cite{Malewicz2010}.  Presuming the commutative and associative properties of a vertex function, a Combiner executes on a worker process and combines many messages destined for the same vertex into a single message.  For example, if a vertex function computes the sum of all incoming messages, then a Combiner would detect all messages destined for a vertex , compute the sum of the messages, then send the new sum to .  A Combiner can especially reduce network traffic when  is remote, shown as  {\em sender-side aggregation} (Figure~\ref{fig:send_agg}).  When  is local, a combiner can still reduce memory overhead by aggregating messages before placement into the incoming message queue, shown as {\em receiver-side aggregation} (Figure~\ref{fig:rec_agg}). For the single-source shortest path algorithm, a combiner implementation resulted in a four-fold reduction in network traffic \cite{Malewicz2010}.

A related technique is the {\em receiver-side scatter}.  For instances where the same message is sent to multiple vertices on the same remote machine, network traffic can be reduced by sending only one message and then having the destination worker distribute multiple copies, depicted in Figure~\ref{fig:rec_scat}.  The strategy has been employed in multiple frameworks, including the \textit{Large Adjacency List Partitioning} in GPS \cite{Salihoglu2013}, IBM's X-Pregel \cite{Bao2013}, as the \textit{fetch-once} behavior in LFGraph \cite{Hoque2013}, and through \textit{scatter} nodes of the Agent-Graph in GRE \cite{Yan2013}.  The technique reduces network traffic by increasing memory and processing overhead, as worker-nodes must store the out-going adjacency lists of other workers.  With this in mind, GPS maintains a threshold where receiver-side scatter would only be applied for vertices above a certain degree.  Experiments showed that as the threshold is lowered, network traffic at first decreases then plateaus, while runtime decreases but then increases, demonstrating the existence of an optimal vertex-degree threshold.  In X-Pregel, a ten-fold reduction in network traffic from Receiver-side Scatter resulted in a 1.5 times speedup \cite{Bao2013}. Clearly, the receiver-side scatter strategy can be effective, but unlike the combiner is not guaranteed to improve performance.

The three partition-driven optimizations in Figure~\ref{fig:opt} are related to the messaging structure of a framework, and not specific to algorithm behavior, albeit some assumptions are made regarding message computation.  Computation for the combiner must be commutative and associative because order cannot be guaranteed, while messages for the receiver-side scatter must be identical, and independent of the adjacency list.  Still, the techniques are oriented around partition-level messaging and apply to the worker process, only requiring certain operational properties in order to work.  The Message-Online-Computing model proposed in \cite{Zhou2014}, which improves memory usage by processing messages in the queue as they are delivered, also requires operations be commutative.

Conversely, algorithm-specific message optimizations have also been developed that restructure vertex messaging patterns for certain algorithmic behaviors \cite{Salihoglu2014,Quick2012}.  For algorithms that combine vertices into a supervertex, like Boruvka's Minimum Spanning Tree \cite{Chung1996}, the Storing Edges at Subvertices (SEAS) optimization implements a subroutine where each vertex tracks its parent supervertex instead of sending adjacency lists \cite{Salihoglu2014}.  For algorithms where vertices remove edges, like in the 1/2-approximation for maximum weight matching \cite{Preis1999}, the Edge Cleaning on Demand (ECOD) optimization only deletes stale edges when, counter-intuitively, activity is requested for the stale edge \cite{Salihoglu2014}.  To avoid slow convergence, ECOD is only employed above a certain threshold, {\em e.g.}, when more than 1\% of all vertices are active.  Both SEAS and ECOD exploit a trade-off between sending messages proportional to the number of vertices or proportional to the number of edges.  Other strategies for reducing communication, based on {\em aggregate computation}, are discussed in Section~\ref{subsec:scope_opt}. 

\subsection{Execution Model}
\label{subsec:comp_models}

The model of execution for vertex-centric programs describes the implementation of the vertex function, and how data moves during computation.  

\subsubsection{Vertex Program Implementation}

Vertex functions have been implemented as 1, 2, or 3 phase-models.  Vertex functions have also been implemented as edge-centric functions.  While the model choice does not typically impact the accuracy of the final result, combining certain implementations with other TLAV components can yield improved system performance for certain graph characteristics.

\paragraph{One Phase}  

The vertex programming abstraction implemented as a single function is well-characterized by the Pregel framework \cite{Malewicz2010}.  The single compute function of a vertex object follows the general sequence of accessing input data, computing a new vertex value, and distributing the update.  In a typical Pregel program, the input data is accessed by iterating through the input message queue (messages that may have utilized a combiner), applying an update function based on received data, and then sending the new value through messages addressed by iterating over outgoing edges.  Details based on other design decisions may vary, {\em e.g.}, input and output data may be distributed through incident edges, or neighboring vertex data may be directly accessible, but in one-phase models the general sequence of vertex execution is performed within a single, programed function.  The \texttt{Vertex.Compute()} function is implemented in several TLAV frameworks in addition to Pregel, including its open-source implementations \cite{Avery2011,Seo2010} and several related variants \cite{Salihoglu2013,Bao2013,Redekopp2013}.  The One-phase function implementation is conceptually straight-forward, but other frameworks provide opportunities for improvement by dividing up the computation.

\paragraph{Two Phase} 

A two-phase vertex-oriented programming model breaks up vertex programming into two functions, most commonly referred to as the Scatter-Gather model.  In Scatter-Gather, the \textit{scatter} phase distributes a vertex value to neighbors, and the \textit{gather} phase collects the inputs and applies the vertex update.  While most single-phase frameworks {\em e.g.}, Pregel, can be converted into two phases, the Scatter-Gather model was first explicitly put forward in the Signal/Collect framework \cite{Stutz2010}. The two phase model is also presented as Scatter-Gather in \cite{Roy2013}, and is presented as the Iterative Vertex-Centric (IVEC) programming model in \cite{Yoneki2013}.  The Scatter-Gather programming model commonly occurs in TLAV systems where data is read/written to/from edges.  

Ligra and Polymer are frameworks implemented for single-machines (see Section~\ref{sec:arch}) that both implement a two-phase model.  The user provides two functions, one function that executes across each vertex in the active subset and another function that executes all outgoing edges in the subset.  The frameworks adopt a vertex-subset-centric programming model, which is similar to vertex-centric, but the framework retains a centralized view of the graph, where the whole graph is within the scope of computation, which is possible because the entire graph resides on a single machine in this case.  The two phase model is executed within a program processing the whole graph.

A related two-phase programming model for message passing called Scatter-Combine is implemented in the GRE framework \cite{Yan2013}.  This model utilizes active messages, which are messages that include both data as well as the operator to be executed on the data \cite{VonEicken1992}.  In the first phase of the model, messages are both sent (Scattered) and the operators in the messages are executed (Combined) at the destination vertex.  In the second phase, the combined result is used to update the vertex value.  The Scatter-Combine model incorporates two phases differently than Scatter-Gather.  Instead of the two phase Scatter-Gather model of (i) Gather-Apply, and (ii) Scatter, the Scatter-Combine model uses active messages to institute (i) Scatter-Gather, and then (ii) Apply.  The GRE framework combines  Scatter-Combine with a novel representation of the underlying data graph, called the Agent-Graph, described above, to reduce communication and improve scalability for processing graphs with scale-free degree distributions.

\paragraph{Three Phase}  
\label{subsubsec:gas}
A three-phase programming model is introduced in PowerGraph as the Gather-Apply-Scatter (GAS) model \cite{Gonzalez2012}.  The \textit{Gather} phase performs a generic summation over all input vertices and/or edges, like a commutative associative combiner.  The result is used in the \textit{Apply} phase, which updates the central vertex value.  The \textit{Scatter} phase distributes the update by writing the value to the output edges.  PowerGraph incorporates the GAS model with vertex-cut partitioning (see Section~\ref{subsubsec:vert_cuts}) to improve processing of power-law graphs.

\paragraph{Edge-Centric} 
The X-Stream framework provides an \textit{edge-centric} two phase Scatter-Gather programming model \cite{Roy2013}, as opposed to a vertex-centric programming model.  The model is edge-centric because the framework iterates over edges of the graph instead of vertices.  However, the framework may still be considered TLAV because the two phase program operates on source and target vertices, adopting a similar local scope.  X-Stream leverages streaming edge data instead of random access for efficient large scale graph processing on a single machine, and is discussed in Section~\ref{sec:arch} in further detail.

\subsubsection{Push vs. Pull}
\label{subsubsec:pushpull}

The flow of information for vertex-programs can be characterized as data being \textit{pushed} or \textit{pulled} \cite{Nguyen2013,Hant2014,Cheng2012}.  In \textit{push} mode, information flows from the active vertex performing the update outward to neighboring vertices, as in Pregel-like message-passing.  In \textit{pull} mode, information flows from neighboring vertices inward to the active vertex, as in GraphLab-like shared memory, when an active vertex reads neighbor's data.  Few TLAV frameworks explicitly adopt a push or pull mode.  Instead, the information flow arises from other design decisions.  Still, analyzing a system as push or pull allows one to reason about other system properties.  For example, asynchronous execution is supported by both modes, but sender-side combining is only possible in push mode \cite{Cheng2012}.

Push and pull modes are more commonly associated with databases and transactional processing, though have been more explicitly incorporated in broader graph engines and temporal frameworks  (see Section~\ref{sec:related} for related work).  The Galois framework, with a flexible computation model enabling the implementation of a vertex-centric interface, allows users to choose push or pull mode \cite{Kulkarni2007,Nguyen2013}, as does Kineograph \cite{Cheng2012}.  Chronos experiments with how push and pull modes impact caching \cite{Hant2014}.

Ligra is a single-machine graph processing framework that dynamically switches between push and pull-based operators based on a threshold.  The framework is in part inspired by a recently developed shared-memory breadth-first search algorithm that achieves remarkable performance by switching between push and pull modes of exploration \cite{Beamer2013}.  This algorithm, Ligra, and PowerSwitch from Section~\ref{subsubsec:hybrid} exemplify how performance can be improved by dynamically adapting the processing technique to properties of the graph.

The delta-caching optimization, which is introduced in PowerGraph \cite{Gonzalez2012}, which reduces the pulling of redundant data by tracking value changes.  In a three phase model, an accumulator value is the result of gather step.  With delta-caching, a cached copy of the accumulator for each vertex is stored by the worker, requiring additional storage.  If, for a given update, the change in the accumulator is minimal, then neighboring vertices aren't activated, and any change can be applied to the cached copy stored by worker.  A neighboring vertex can then use the cached copy during an update.  For delta-caching to be available, the apply function must be commutative, associative, and have an inverse function.  Delta-caching reduces redundant pulling by not activating neighboring vertices for small changes, and resulted in a 45\% decrease in runtime for computing PageRank on the Twitter graph \cite{Gonzalez2012}.

\subsection{Partitioning}
\label{subsec:partitioning}

Large-scale graphs must be divided into parts to be placed in distributed memory.  Good partitions often lead to improved performance \cite{Salihoglu2013}, but expensive strategies can end up dominating processing time, leading many implementations to incorporate simple strategies, such as random placement \cite{Jain2013}.  Effective partitioning evenly distributed the vertices for balanced workload, while minimizing inter-partition edges to avoid costly network traffic, a problem formally known as {\em k-way graph partitioning} that is NP-complete with no fixed-factor approximation \cite{Andreev2006,Meyerhenke2014}.  

Leading work in graph partitioning can be broadly characterized as (1) rigorous but impractical mathematical strategies, or (2) pragmatic heuristics used in practice \cite{Tsourakakis2014}. Practical strategies, such as those employed in the suite of algorithms known as METIS \cite{Karypis1995a}, often employ a three-phase multi-level partitioning approach \cite{Abou-Rjeili2006}.  Partition size is often allowed to deviate in the form of a ``slackness" parameter in exchange for better cuts \cite{Karypis1996}.  

Graph partitioning with METIS partitioning software is often considered the \textit{de facto} standard for near-optimal partitioning in TLAV frameworks \cite{Stanton2012}.  Despite a lengthy preprocessing time, METIS-algorithms significantly reduce total communication and improve overall runtime for TLAV processing on smaller graphs \cite{Salihoglu2013}.  However, for graphs of even medium-size, the high computational cost and necessary random access the entire graph renders METIS and related heuristics impractical.  Alternatives for large-scale graph partitioning include distributed heuristics presented in Section~\ref{subsubsec:dist_heur}, streaming algorithms in Section~\ref{subsubsec:streaming}, vertex cuts in Section~\ref{subsubsec:vert_cuts}, and dynamic repartitioning in Section~\ref{subsubsec:dyn_repart}.

\subsubsection{Distributed Heuristics}
\label{subsubsec:dist_heur}
Distributed heuristics are decentralized methods, requiring little or no centralized coordination.  Distributed partitioning is related to distributed community detection in networks \cite{Gehweiler2010,Ramaswamy2005}, the two main differences being: 1) communities can overlap whereas partitions cannot, and 2) partitioning requires \textit{a priori} specification of the number of partitions, whereas community detection typically does not.  Much distributed partitioning work has been inspired by distributed community detection, namely label propagation \cite{Raghavan2007}.

Label propagation occurs at the vertex level, where each vertex adopts the label of the plurality of its neighbors.  Though the process is decentralized, label propagation for partitioning necessitates a varying amount of centralized coordination in order to maintain balanced partitions and prevent ``densification": a cascading phenomenon where one label becomes the overwhelming preference \cite{Raghavan2007}. The densification problem is addressed in \cite{Vaquero2013} wherein a simple capacity constraint is enforced that is equal to the available capacity of the local worker divided by the number of non-local workers.  In \cite{Ugander2013}, balanced vertex distribution is maintained by constraining label propagation and solving a linear programming optimization problem that maximizes a relocation utility function. In \cite{Rahimian2013}, vertices swap labels, either with a neighbor or possibly a random node, and simulated annealing is employed to escape local optima.  The cost of centralized coordination incurred by these methods is much less than the cost of random vertex access on a distributed architecture, as with ParMETIS.  

More advanced label propagation schemes for partitioning are presented in \cite{Wang2014} and \cite{Madduri}.  In \cite{Wang2014}, label propagation is used as the coarsening phase of a multi-level partitioning scheme, which processes the partitioning in blocks to accommodate multi-level partitioning for large-scale graphs.  In \cite{Madduri}, several stages of label propagation are utilized to satisfy multiple partitioning objectives under multiple constraints.  \cite{Zeng2012} use a parallel multi-level partitioning algorithm for k-way balanced graphs that operates in two phases: an aggregate phase that uses weighted label propagation, and then a partition phase that performs the stepwise minimizing RatioCut method.

\subsubsection{Streaming}
\label{subsubsec:streaming}

Streaming partitioning is a form of online processing that partitions a graph in a single-pass.  For TLAV frameworks, streaming partitioning is especially efficient since the partitioning can be performed by the graph loader, which loads the graph from disk onto the cluster.  The accepted streaming model assumes a single, centralized graph loader that reads data serially from disk and chooses where to place the data amongst available workers \cite{Stanton2012,Tsourakakis2014}.  Centralized streaming heuristics can be adapted to run in parallel \cite{Stanton2012}, however, depending on the heuristic, concurrency between the parallel partitioners would likely be required \cite{Nishimura2013}.  One of the first online heuristics was presented by Kernighan and Lin and is used as a subroutine in METIS \cite{Kernighan1970}.  GraphBuilder \cite{Jain2013} is a a similar library that, in addition to partitioning, supports an extensive variety of graph loading-related processing tasks.  
A streaming partitioner on a graph loader reads data serially from disk, receiving one vertex at a time along with its neighboring vertices.  In a single look at the vertex the streaming partitioner must decide the final placement for the vertex on a worker partition, but the streaming partitioner has access to the entire subgraph of already placed vertices.  In a variant of the streaming model, the partitioner has an available storage buffer with a capacity equal to that of a worker partition, so the partitioner may temporarily store a vertex and decide the partitioning later \cite{Stanton2012}, however this buffer is not utilized by the top performing streaming partitioners.  For most heuristics, the placement of later vertices is dependent on placement of earlier vertices, so the presentation order of vertices can impact the partitioning. Thus, an adverse ordering can drastically subvert partitioning efforts, however, experiments demonstrate that performance remains relatively consistent for breadth-first, depth-first, and random orderings of a graph \cite{Stanton2012,Tsourakakis2014}.

Two top-performing streaming partitioning algorithms are greedy heuristics.  The first is linear deterministic greedy (LDG), a heuristic that assigns a vertex to the partition with which it shares the most edges while weighted by a penalty function linearly associated with a partition's remaining capacity.  The LDG heuristic is presented in \cite{Stanton2012}, where 16 streaming partitioning heuristics are evaluated across 21 different data sets.  The use of a buffer in addition to the LDF heuristic has been adapted for streaming partitioning of massive Resource Description Framework (RDF) data \cite{Wang2013a}. Another variant uses {\em unweighted} deterministic greedy instead of linear deterministic greed (LDG), to perform greedy selection based on neighbors without any penalty function; this unweighted variant has been employed for distributed matrix factorization \cite{Ahmed2013}.  Further analysis of LDG-related heuristics on random graphs, as well as lower bound proofs for random and adversarial stream ordering, is presented in \cite{Stanton2014}.  

Another top-performing streaming partitioner is FENNEL \cite{Tsourakakis2014}, which is inspired by a generalization of optimal quasi-cliques \cite{Tsourakakis2013}.  FENNEL achieves high quality partitions that are in some instances comparable with near-optimal METIS partitions.  Both FENNEL and LDG have been adapted to the restreaming graph partitioning model, where a streaming partitioner is provided access to previous stream results \cite{Nishimura2013}.  Restreaming graph partitioning is motivated by environments such as online services where the same, or slightly modified, graph is repeatedly streamed with regularity.  Despite adhering to the same linear memory bounds as a single-pass partitioning, the presented restreaming algorithms not only provide results comparable to METIS, but are also capable of partitioning in the presence of multiple constraints and in parallel without inter-stream communication.

\subsubsection{Vertex Cuts}
\label{subsubsec:vert_cuts}
A vertex-cut, depicted in Figure~\ref{fig:powergraph}, is equivalent to partitioning a graph by edges instead of vertices.  Partitioning by edges results in each edge being assigned to one machine, while vertices are capable of spanning multiple machines.  Only changes to values of cut vertices are passed over the network, not changes to edges. Vertex-cuts are implemented by TLAV frameworks in response to the challenges of finding well-balanced edge cuts in power-law graphs \cite{Abou-Rjeili2006,Leskovec2009}.  Complex network theory suggests power-law graphs have good vertex cuts in the form of nodes with high degree \cite{Albert2000}. A rigorous review of vertex separators is presented in \cite{Feige2008}.

PowerGraph combines vertex-cuts with the three-phase GAS model (Section~\ref{subsubsec:gas}) for efficient communication and balanced computation \cite{Gonzalez2012}.  For vertices that are cut and span multiple machines, one copy is randomly designated the master, and remaining copies are mirrors.  During an update all vertices first execute a gather, where all incoming edge values are combined with a commutative associative sum operation.  Then the mirrors transmit the sum value over the network to the master, which executes the apply function to produce the updated vertex value. The master then sends the result back over the network to the mirrors.  Finally, each vertex completes the update by scattering the result along its outgoing edges.  For each update, network traffic is proportional to the number of mirrors, therefore, breaking up high-degree vertices reduces network communication and helps to balance computation. 

Since its initial implementation in PowerGraph, the vertex-cut approach has been adopted by several other TLAV frameworks.  GraphX is a vertex programming abstraction for the Spark processing framework \cite{Gonzalez2014,Zaharia2010} where the adoption of vertex-cuts demonstrated an 8-fold decrease in the platform's communication cost. GraphBuilder \cite{Jain2013}, an open-source graph loader, supports vertex-cuts and implements grid and torus-based vertex-cut strategies that were later included in PowerGraph.  PowerLyra \cite{Chen2013b} is a modification to PowerGraph that hybridizes partitioning where vertices with a degree above a user-defined threshold are cut, while vertices below the threshold are partitioned using an adaptation of the FENNEL streaming algorithm \cite{Tsourakakis2014}. PowerLyra also incorporates unidirectional locality similar to GRE framework (see Section~\ref{subsubsec:acivemsg}).  BiGraph is a framework developed on PowerGraph that implements partitioning algorithms for large-scale bipartite graphs \cite{Chen2014bi}.  LightGraph \cite{Zhao2014} is a framework that optimizes vertex-cut partitions by using edge-direction-aware partitioning, and by not sending updates to mirrors with only in-edges.

Several edge partitioning analyses and algorithms have recently been developed.  A thorough analysis comparing expected costs of vertex partitioning and edge partitioning is presented in \cite{Bourse2014}. In this study, edge partitioning is empirically demonstrated to outperform vertex partitioning, and a streaming least marginal cost greedy heuristic is introduced that outperforms the greedy heuristic from PowerGraph.  

Centralized hypergraph partitioning, including edge partitioning, is NP-hard, and several exact algorithms have been developed \cite{Biha2011,Kim2012,Hager2014,Sevim}. However, because of their complexity, such algorithms are too computationally expensive and not practical for large-scale graphs.  Centralized heuristics have been shown to be equally impractical \cite{Benlic2013}.  A large-scale vertex-cut approach for bipartite graphs based on hypergraph partitioning is presented in \cite{Miao2013} as part of a vertex-centric program for computing the alternating direction of multipliers optimization technique.  A distributed edge partitioner was developed in \cite{Rahimian2014} that creates balanced partitions while reducing the vertex cut, based on the vertex partitioner in \cite{Rahimian2013}.  Good workload balance for skewed degree distributions can also be achieved with degree-based hashing \cite{Xie2014}.  Finally, as part of a non-vertex-centric BSP graph processing framework, a distributed vertex-cut partitioner is presented in \cite{Guerrieri2014} that uses a market-based model where partitions use allocated funds to buy an edge.

\subsubsection{Dynamic Repartitioning}
\label{subsubsec:dyn_repart}

While an effective partitioning equally distributes vertices among the partitions, for TLAV frameworks, the number of active vertices performing updates on a given superstep can vary drastically over the course of computation, which creates processing imbalances and increases run time.  Dynamic repartitioning was developed to maintain balance during processing by migrating vertices between workers as necessary.

Reasons for changing active vertex sets include topological mutations to the graph and algorithmic execution properties.  Topological mutations may occur if the framework supports dynamic or temporal graphs (see Related Work in Section~\ref{sec:related}).  Topology may also change due to the algorithm, such as graph coarsening \cite{Wang2014}.  

With a static topology, the execution pattern of the algorithm can also change the active vertex set.  While vertex algorithms such as synchronous PageRank execute on every vertex for every superstep, other algorithms introduce dynamism.  \cite{Shang2013} classifies 9 vertex algorithms as either (i) always active, (ii) traversal, or (iii) multi-phase, where the active vertex set of the latter two classifications can vary widely and unpredictably, depending on the graph.  For dynamic repartitioning to prove beneficial, the associated overhead must be less than the additional costs stemming from processing imbalance.

According to \cite{Salihoglu2013}, a dynamic repartitioning strategy must directly address (i) how to select vertices to reassign, (ii) how and when to move the assigned vertices, and (iii) how to locate the reassigned vertices.  Other properties of a strategy include whether coordination is centralized or decentralized, and how the strategy combats ``densification" and enforces vertex balance.  Densification is akin to the rich-get-richer phenomenon, and can occur in greedy or decentralized protocols for partitioning/clustering, where one partition becomes over-populated as the repeated destination for migrated vertices \cite{Vaquero2013a}.  In response, protocols often implement constraints that prevent a partition from exceeding a certain capacity.  The XPregel framework, for example, only permits the worker with the most vertices and edges to migrate vertices \cite{Bao2013}.  

Table~\ref{table:dyn_repart} presents 6 TLAV frameworks that support dynamic repartitioning: GPS \cite{Salihoglu2013}, Mizan \cite{Khayyat2013}, XPregel \cite{Bao2013}, xDGP \cite{Vaquero2013a}, LogGP \cite{Xu2013}, and the Catch the Wind prototype \cite{Shang2013}.  The table includes what active vertex set imbalances are targeted by the frameworks, what metrics are used to identify vertices for reassignment, how reassigned vertices are located after migration, how densification is avoided, and whether the protocol is centralized or decentralized.  

Among the 6 frameworks that implement dynamic repartitioning, all are synchronous, and repartitioning occurs at the end of a superstep, separate from the updates.  When a vertex is selected for migration, the worker must send all associated data to the new worker, including the vertex ID, the adjacency list, and the incoming messages to be processed in the next superstep.  To avoid sending all incoming messages over the network, many dynamic repartitioning frameworks implement a form of delayed migration, where the new worker is recognized as the owner of the migrated vertex, but the vertex value remains on the old worker for an extra iteration in order to compute an update.  With delayed migration, the incoming message queue doesn't need to be migrated, but the new worker still receives new incoming messages \cite{Khayyat2013,Salihoglu2013}.

\begin{table*}[t]
\centering
\small{
\begin{tabular} {l | l l l l l}
Framework & \begin{tabular}{@{}l@{}}Cause of \\ Imbalance\end{tabular} & \begin{tabular}{@{}l@{}}Reassignment \\ Metric\end{tabular} & \begin{tabular}{@{}l@{}}How to Locate \\ Migrated Verts\end{tabular} & \begin{tabular}{@{}l@{}}Densification \\ Avoidance\end{tabular} & Coordination  \\ \hline
GPS  & Algorithm & Sent Msgs & \begin{tabular}{@{}l@{}}Broadcast \\ Vert ID\end{tabular}  & Swap Min-Set  & Decentralized \\
Mizan & Algorithm & \begin{tabular}{@{}l@{}}Sent/Recv Msgs \\ and Run Time \end{tabular}& \begin{tabular}{@{}l@{}}Distributed \\ Hash Table\end{tabular}  & Metric-based Swap & Decentralized\\
XPregel & Algorithm & Sent/Recv Msgs & \begin{tabular}{@{}l@{}}Broadcast \\ Worker ID\end{tabular}  & \begin{tabular}{@{}l@{}}Repartition \\ Largest Worker\end{tabular}& Centralized  \\
xDGP & Topology & \begin{tabular}{@{}l@{}}Labels of \\ Neighbors\end{tabular} & \begin{tabular}{@{}l@{}}Broadcast \\ Worker ID\end{tabular} & \begin{tabular}{@{}l@{}}Fraction of \\ Capacity\end{tabular} & Decentralized  \\
LogGP & Both & Runtime & Lookup Table & \begin{tabular}{@{}l@{}}Repartition Longest- \\ Running Workers\end{tabular} & Centralized \\
\begin{tabular}{@{}l@{}}Catch \\ the Wind\end{tabular} & Algorithm & Sent/Recv Msgs & Lookup Table & Quota & Decentralized\\
\end{tabular}
}
\caption{Feature summary for TLAV frameworks that implement dynamic repartitioning.  Active vertex set imbalance may arise from topology changes, algorithm execution, or both.  A repartitioning strategy includes how to select vertices for reassignment, and how reassigned vertices are later located.  Strategies should also avoid densification, and can be centrally or decentrally implemented.  All implemented frameworks are synchronous.}
\label{table:dyn_repart}
\end{table*}

Though fundamentally sound, many experiments demonstrate that dynamic repartitioning is often not worth the high overhead.  Results in \cite{Bao2013} show that while network I/O is significantly reduced over time, overall runtime shows minor improvements.  Independent tests of GPS show dynamic repartitioning to be detrimental for all cases in \cite{Lu2014}, and similar results are observed for GPS and Mizan in \cite{Han2014}.  However, one major shortcoming in these evaluations is the use of the PageRank algorithm for experimentation.  Dynamic repartitioning is most effective for dynamic active vertex sets, but with PageRank vertices are always active, so dynamic repartitioning performs predictably poorly.  Asynchronous dynamic repartitioning protocols have yet to be explored for TLAV frameworks, but the added complexity and overhead for asynchrony demonstrated in Section~\ref{subsec:execution_policy} suggest that such an implementation is not practical.

\section{Implementation}
\label{sec:implementation}

This section overviews implementation details of TLAV frameworks relating to the distributed environment. These details include system architecture and fault tolerance.  Additionally, TLAV frameworks that employ novel techniques to process large-scale graphs on single machines are surveyed.

\subsection{System Architecture}

TLAV frameworks generally always employ the master-slave architecture.  A master node initializes the slave workers, monitors execution, and manages coordination (and synchronization if invoked) amongst the workers.  Generally, the master is responsible for graph loading and partitioning, but with a network filesystem available, the loading and partitioning can be performed in parallel \cite{Salihoglu2013}.  The master also stores global values, such as aggregators \cite{Malewicz2010}.  The workers each execute a copy of the program on the local partitions and inform the master of runtime status.

One notable exception to the general master-slave architecture is XPregel \cite{Bao2013}, implemented in X10 \cite{Charles2005}.  X10 implements an Asynchronous Partitioned Global Address Space (APGAS), which is a shared address space but with a local structure that enables highly productive distributed and parallel programming.  With APGAS, the number of local ``places" is provided at runtime, which the programmer may utilize as necessary.  XPregel does implement master-slave, but in X10, the master is actually just place 0, sans hierarchy, and opens the door for alternative architectures, like recursive structures.

\subsection{Multi-Core Support}

For multi-core machines, many BSP-based frameworks including Pregel \cite{Malewicz2010} simply assign a partition to a given core, but frameworks can better utilize computational resources through multi-threading.  XPregel \cite{Bao2013} supports multi-threading by dividing a partition into a user-defined number of subpartitions, assigning one thread to each subpartition.  GraphLab \cite{Low2012} implements multi-threading and avoids deadlocks through scheduler restrictions.  GPS \cite{Salihoglu2013} implements 3 types of threads: a thread for vertex computation, a thread for communication, and a thread for parsing.  Cyclops \cite{Chen2014} implements a hierarchical BSP model \cite{Cha2001hbsp} with a split design to parallelize computation and messaging while exploiting locality and avoiding synchronization contention.  Cyclops demonstrates that multi-threading can improve runtime relative to single-threaded execution for the same framework, at the expense of added complexity.


\subsection{Fault Tolerance}
\label{subsec:fault-tol}

Distributed systems must often account for the potential failure of one or more nodes over the course of computation.  When a node fails, a replacement node may become available, but all data and computation performed on the failed node is lost.

Checkpointing is a common fault tolerance implementation, where an immutable copy of the data is written to persistent storage, such as a network filesystem.  Pregel implements synchronous checkpointing, where the graph is copied in between supersteps \cite{Malewicz2010}.  When a failure occurs, the system rolls back to the most recently saved point, all partitions are reloaded, and the entire system resumes processing from the checkpoint.  The partition of the failed node is reloaded to a new replacement node.  If messaging information is also logged, then resources can be saved by only reloading and recomputing data  on the replacement node.  GraphLab \cite{Low2012} implements asynchronous vertex checkpointing, based on Chandy-Lamport \cite{Chandy1985} snapshots, which need not halt the entire program and can result in slightly faster overall execution than synchronous checkpointing, minding certain program constraints.

GraphX is a graph processing library for Apache Spark, which is developed based on the Resilient Distributed Dataset (RDD) abstraction \cite{Gonzalez2014}.  RDDs are immutable, partitioned collections created through data-parallel operators, like map or reduce.  RDDs are either stored externally, or generated in-memory from operations on other RDDs.  Spark maintains the lineage of operations on an RDD, so upon any node failure the RDD can be automatically recovered. GraphX leverages the RDDs of Spark to create a graph abstraction and Pregel interface.

The Imitator \cite{wang-replication} framework implements fault-tolerance based on vertex replicas, or ghosts/mirrors used in shared memory (see Section!\ref{subsubsec:shared}).  The use of replicas for fault tolerance is founded in the observation that the hash partitioning of many real-world directed graphs results in the replication of over 99\% of vertices \cite{wang-replication}.  By replicating \textit{every} vertex, a full copy of the graph can reside in distributed memory, enabling faster recovery times at the expense of relatively little additional memory consumption and network messaging  \cite{wang-replication}.  The efficiency of Imitator is tied to the effectiveness of the partitioning (see Section~\ref{subsec:partitioning}).  Imitator outperforms checkpointing for large graphs distributed over several nodes, when only one replica per vertex is required.  State-of-the-art partitioning methods like METIS, or a smaller number of partitions (Imitator experiments were run on 50 nodes), would likely lead to increased overhead for Imitator.  Also, the number of replicas is tied to the degree of fault tolerance.  To support the failure of {\em k} machines, then {\em k} replicas are required, increasing overhead for each additional failure supported.

A partition-based checkpoint method for fault tolerance is presented in \cite{shen-partitionbased}.  During execution, a recovery executor node collects run-time statistics, and upon failure, uses heuristics to redistribute the partitions.  Checkpointed partitions of the failed nodes can be reassigned amongst both new and old nodes, parallelizing recovery.  Partitions on healthy nodes can also be reassigned for load balancing. 

\subsection{Single Machine Architectures}
\label{sec:arch}

Like MapReduce, TLAV frameworks are advantageous because they are highly scalable while providing a simple programming interface, abstracting away the lower level details of distributed computing.  However, such environments also stipulate the availability of elaborate infrastructure, cluster management, and performance tuning, which may not be available to all users.

Single machine systems are easier to manage and program, but commodity machines do not have the memory capacity to process large-scale graphs in-memory.  This section overviews single machine TLAV frameworks that employ {\em novel} methods to process large-scale graphs.  The main features of the 4 single machine frameworks in this section are presented in Table~\ref{table:singlemachine_frameworks}.

Processing large-scale graphs on a single machine requires either substantial amounts of memory, or storing part of the graph out-of-memory, in which case performance is dictated by how efficiently the graph can be fetched from storage.  In \cite{Shun2013}, it's argued that high-end servers, offering 100GB to 1TB of memory or more, is enough capacity for many real and synthetic graphs reported in the literature.  Such machines would be capable of storing large graphs and executing relatively simple graph algorithms, though more complex algorithms would likely exhaust resources.  

The recommendation service at Twitter \cite{Gupta2013}, which implements a single machine graph processing system with 144 GB of RAM, finds that in practice one edge occupies roughly five bytes of RAM on average.  Compression techniques are further explored for large memory servers in \cite{shun2015}.  Yet, graphs of scale are not practical on lower-end machines containing around 8 to 16 GB of memory \cite{Kyrola2012}.  Accordingly, single machine frameworks have been developed that implement the vertex-centric programming model and process a graph in parts.  Central to many single machine TLAV frameworks are novel data layouts that efficiently read and write graph data to/from external storage.  One common representation is the compressed sparse row format, which organizes graph data as out-going edge adjacency sets, allowing for the fast look-up of outgoing edge, and has been implemented in many state-of-the-art shared memory graph processors \cite{Pearce2010,Hong2011}, including Galois \cite{Nguyen2013}.

\begin{table*}
\centering
\small{
\begin{tabular} {l | c c l}
Framework & Storage Medium & Data Layout & \\ \hline
GraphChi & Disk/SSD & Parallel Sliding Window & \cite{Kyrola2012}\\
X-Stream & Disk/SSD & Streaming Partitions & \cite{Roy2013}\\
FlashGraph & SSD Array & Semi-External Memory with Page Cache & \cite{Zheng2015} \\
PathGraph  & Disk/SSD & Compressed DFS Traversal Trees & \cite{Yuan} \\
\end{tabular}
}
\caption{Single Machine Frameworks}
\label{table:singlemachine_frameworks}
\end{table*}

\paragraph*{GraphChi} The seminal single machine TLAV framework is GraphChi \cite{Kyrola2012}, which was explicitly developed for large-scale graph processing on a commodity desktop.  GraphChi enables large-scale graph processing by implementing the Parallel Sliding Window (PSW) method, a graph data layout previously utilized for efficient PageRank and sparse-matrix dense-vector multiplication \cite{Chen2002,Bender2010}.  PSW partitions vertices into disjoint sets, associating with each interval a shard containing all of the interval's incoming edges, sorted by source vertex.  Intervals are selected to form balanced shards, and the number of intervals is chosen so any interval can fit completely in memory.  A sliding window is maintained over every interval, so when vertices from one shard are updated from in-edges,  the results can be sequentially written to out-edges found in sorted order in the window on other shards.  GraphChi may not be faster than most distributed frameworks, but often reaches convergence within an order of magnitude of the performance of distributed frameworks \cite{Kyrola2012}, which is reasonable for a desktop with an order of magnitude less RAM.  The GraphChi framework was later extended to a general graph management system for a single machine called GraphChi-DB \cite{Kyrola2014}.

Storage concepts for single machine graph processing are further explored in \cite{Yoneki2013} through two directions.  The first project investigates reducing random accesses in SSDs through prefetching, in a project called RASP that later evolved into PrefEdge \cite{Nilakant2014}.  The second project is X-Stream \cite{Roy2013}, an edge-centric single machine graph processing framework that exploits the trade-off between random memory access and sequential access from streaming data.

\paragraph*{X-Stream} Streaming data from any storage medium provides much greater bandwidth than random access.  Experiments on the X-Stream testbed, for example, demonstrate that streaming data from disk is 500 times faster than random access \cite{Roy2013}. X-Stream combines a novel data layout, where an index is built over a storage-based edge list with an edge-centric Scatter-Gather programming model that includes a shuffle phase. Data is read from, and updates are written to, streaming edge data.  Though the framework is edge-centric, a user-defined update function is executed on the destination vertex of an edge.  X-Stream reports that it can process a 64-billion edge graph on a single machine with a pair of 3TB magnetic disks attached \cite{xstreamweb}.

\paragraph*{FlashGraph} While GraphChi and X-Stream are designed for general external storage, the FlashGraph framework is developed for graphs stored on any fast I/O device, such as an array of SSDs.  FlashGraph is deployed on top of the set-associative file system (SAFS) \cite{Zheng2015}, which includes a scalable lightweight page cache, and implements a custom asynchronous user-task I/O interface that reduces overhead for asynchronous I/O.  FlashGraph employs asynchronous message-passing and vertex-centric programming with the semi-external memory (SEM) model \cite{Pearce2010}, where vertices and algorithmic state reside in RAM, but edges are stored externally. In experiments comparing GraphChi and XStream, FlashGraph outperformed both by orders of magnitude even when the data for GraphChi and XStream was placed into RAM-disk \cite{Zheng2015}.  

\paragraph*{PathGraph} In addition to the path-centric programming model, further discussed in Section~\ref{sec:alt}, PathGraph also implements a path-centric compact storage system that improves compactness and locality \cite{Yuan}.  Because most iterative graph algorithms involve path traversal, PathGraph stores edge traversal trees in depth-first search order.  Both the forward and reverse edge trees are each stored in a chunk storage structure that compresses data structure information including the adjacency set, vertex IDs, and the indexing of the chunk.  The efficient computational model and storage structure of PathGraph resulted in improved graph loading time, lower memory footprint, and faster runtime for certain algorithms when compared to GraphChi and X-Stream.

\section{Alternative Graph Granularity}
\label{sec:alt}

The strengths of the vertex-centric programming model are also its weaknesses.  Whereas vertex programs may be relatively simpler to reason about since only local data is available, the algorithms are less expressive than conventional centralized algorithms.  While TLAV frameworks exhibit better scalability, execution can be slow because of high overhead from synchronization and message traffic that takes magnitudes longer compared to computation.  Several frameworks strive for the best of both worlds by adopting a scope that is greater than a vertex but less than the graph, summarized in Table~\ref{table:alt_frameworks}.

\subsection{Subgraph-centric Frameworks}

Considering the challenges addressed by TLAV frameworks, taking a subgraph-centric approach is sensible.  Conventional graph algorithms require the entire graph in memory, which is not possible with graphs of scale.  A subgraph, though, can be partitioned into a size small enough to fit into memory (considering computation) while the connections between subgraphs would be no more, and likely much less, than the total number of edges.  The system would better utilize processing while retaining scalability.

The subgraph-centric programming model is implemented in varying degrees by several frameworks.  The Giraph++ \cite{Tian2013}, Blogel \cite{Yan2014}, and GoFFish \cite{Simmhan2013} frameworks provide a subgraph-centric interface for progrmaming sequential algorithms.  Both Giraph++ and Blogel provide a subgraph-centric interface in addition to a vertex-centric interface.  The results of the sequential programs can then be shared either through vertex programs on boundary nodes, or in the case of Blogel, results can be shared directly between subgraphs.  GoFFish exclusively offers a subgraph-centric interface, and implements messaging between subgraphs and also from subgraphs to specific vertices, the latter being used for traversal algorithms.  By allowing subgraphs to directly message vertices, any vertex-centric algorithm can be implemented by a subgraph-centric framework, maintaining scalability while enabling significant performance improvement.  Collectively, subgraph-centric frameworks dramatically outperform TLAV frameworks, often by {\em orders of magnitude} in terms of computing time, number of messages, and total supersteps \cite{Tian2013,Yan2014}.  

The GraphHP \cite{Chen} and  P++ \cite{Zhou2014} frameworks do not implement an interface for sequential programs, but do differentiate between inter-partition nodes to improve performance.  In these two frameworks, supersteps are split into two phases: in the first phase messages are exchanged between vertices on partition boundaries, and in the second phase, vertices within a partition repeatedly execute the vertex program to completion, exchanging messages in memory.  This method reduces communication and improves performance, however, iteratively executing intra-worker vertex programs is less efficient than executing a sequential algorithm. Message-passing algorithms are typically more scalable than sequential graph algorithms, but P++ is not distributed, nor is Block-based GRACE \cite{Xie2013a}, an extension of \cite{Wang2013}, although the later demonstrates that executing vertex updates on a subgraph block basis improves locality and cache hits while reducing memory access time, which is a bottleneck for computationally light algorithms like PageRank.

TLAV frameworks illustrate the principal ideas for scalable graph processing, but for the best performance, users may consider subgraph-centric frameworks. Subgraph frameworks leverage principles of TLAV frameworks to execute sequential graph algorithms in a distributed environment.  The Giraph++, Blogel, and GoFFish frameworks reduce the scope of sequential graph algorithms for the subgraph to fit in memory while utilizing vertex or subgraph messaging to maintain scalability.  Together, the vertex-centric and subgraph-centric programming model, compared to sequential graph algorithms, demonstrate how scalability varies inversely with scope.  

\begin{table*}
\centering
\small{
\begin{tabular} {l | c c c c l}
Framework & \begin{tabular}{@{}l@{}}Programming \\ Model\end{tabular} & \begin{tabular}{@{}l@{}}Sequential \\ Algorithms\end{tabular} & \begin{tabular}{@{}l@{}}Vertex \\ Messaging\end{tabular} & Distributed & \\ \hline
Giraph++ & Subgraph & Y & Y & Y & \cite{Tian2013} \\
Blogel & Subgraph & Y & Y & Y & \cite{Yan2014} \\
GoFFish & Subgraph & Y & N & Y & \cite{Simmhan2013} \\
GraphHP & Subgraph & N & Y & Y & \cite{Chen} \\ 
P++ & Subgraph & N & Y & N & \cite{Zhou2014} \\
GRACE (block) & Subgraph & N & Y & N & \cite{Xie2013a} \\ 
PathGraph & Path & N & Y & Y & \cite{Yuan} \\
Ligra & Vertex Subset & Y & Y & N & \cite{Shun2013} \\
Polymer & Vertex Subset & Y & Y & N & \cite{Zhang2015} \\
Galois & User-Defined Set & Y & Y & N & \cite{Nguyen2013} \\
\end{tabular}
}
\caption{Frameworks of Alternative Scope}
\label{table:alt_frameworks}
\end{table*}

\subsection{Other Scopes: Paths and Sets}

While subgraph-centric frameworks illustrate the scope/scalability trade-off, several other frameworks adopt alternative computational scopes that demonstrate additional benefits.

A more specific type of subgraph, a traversal tree, is used for the programming model in PathGraph \cite{Yuan}.  Traversals are a fundamental component of many graph algorithms, including PageRank and Bellman-Ford shortest path.  PathGraph first partitions the graph into paths, with each partition represented as two trees, a forward and reverse edge traversal.  Then, for the path-centric computational model, path-centric scatter and path-centric gather functions are available to the user to define an algorithm that traverse each tree.  The user also defines a vertex update function, which is executed by the path-centric functions during the traversal.  Like block-based GRACE, the path-centric model utilizes locality to improve performance through reduced memory usage and efficient caching.  PathGraph also implements a path-centric storage model that enables the framework to process billion node graphs on a single machine (see Section~\ref{sec:arch}) \cite{Yuan}.

Graph processing frameworks designed for single machines can implement interfaces of unique granularity.  A vertex subset interface is implemented in Ligra \cite{Shun2013}.  Ligra argues that high-end servers provide enough memory for large-scale graphs, and thus implements a vertex-centric programming interface while retaining a global view of the graph.  Inspired by a hybrid breadth-first search (BFS) algorithm \cite{Beamer2013}, Ligra dynamically switches between sparse and dense representations of edge sets depending on the size of the vertex subset, which impacts whether push or pull operations are performed with the vertex subset.  Polymer \cite{Zhang2015} adopts a similar interface as Ligra, but with several NUMA-aware optimizations.  Galois \cite{Kulkarni2007} is a shared memory framework that executes user-defined set operators while exploiting amorphous data parallelism \cite{pingali11}.  Galois can be implement a variety of programming interfaces, including the vertex-centric paradigm \cite{Nguyen2013}.

\subsection{Optimizations}
\label{subsec:scope_opt}

Two optimizations have been introduced in \cite{Salihoglu2014} for TLAV frameworks that improve performance by adopting a scope of the graph other than vertex-centric.  The Finishing Computation Serially (FCS) method is applicable when an algorithm with a shrinking set of active vertices converges slowly near the end of execution \cite{Salihoglu2014}.  The FCS method is triggered when the remaining active graph can fit in the memory of a single machine; in these instances the active portions are sent to the master and completed serially from a global, shared memory perspective of the graph. 

Similarly, the Single Pivot (SP) optimization \cite{Salihoglu2014}, first presented in \cite{Quick2012}, also temporarily adopts a global view.  For algorithms that execute breadth-first search (BFS) across all vertices, {\em e.g.}, the connected components algorithm, instead of executing BFS from every node, which incurs a high messaging cost, SP randomly selects one vertex from the graph and performs BFS just from that vertex.  Since most graphs have one big component, in addition to many small ones, the BFS from a random node can be executed until the big component is found, then BFS from every vertex that's not in the big component can execute BFS to complete the algorithm, resulting in significantly fewer total messages.  This optimization adjusts scope by randomly selecting a single vertex by utilizing a global aggregator \cite{Malewicz2010}, which also adopt a scope beyond vertex.

\section{Related Work}
\label{sec:related}

In this paper, vertex-centric graph processing systems for large-scale graphs are surveyed.  In previous related work, Pregel and GraphLab have been compared \cite{Sakr2013}, and general graph processing systems have been surveyed \cite{Khan2014,Nisar2013}, and 4 TLAV frameworks have been empirically evaluated on 4 algorithms \cite{Han2014}.  A tutorial on TLAV frameworks was recently delivered at an international conference \cite{Ajwani2015}.

TLAV frameworks intersect several subjects, including graph processing, distributed computing, Big Data, and distributed algorithms.  Several graph processing frameworks have been recently developed outside of the vertex-centric programming model.  PEGASUS combines the BSP model with generalized matrix-vector multiplication (GIM-V) \cite{Kang2011}, while TurboGraph introduces the pin-and-slide model to perform GIM-V on a single machine \cite{Han2013}. Combinatorial BLAS \cite{Bulucc2011} and the Parallel Boost Graph Library \cite{Gregor2005} are software libraries for high-performing parallel computation of sequential programs.  Piccolo performs distributed graph computation using distributed tables \cite{power2010}.  

Graph databases, such as Neo4j \cite{Webber2012}, HyperGraphDB \cite{Iordanov2010}, and GBASE \cite{Kang2011}, are decidedly different from TLAV frameworks.  Both treat vertices as first class citizens, and both face related problems like partitioning, but the key distinction is that databases focus on transactional processing while TLAV frameworks focus on batch processing \cite{Chen2012}. Databases offer local or online queries, such as 1-hop neighbors, whereas TLAV systems iteratively process the entire graph offline in batch.  Some more general graph management systems, like  Trinity \cite{Shao2013} and Grace \cite{Prabhakaran2012}, offer suites of features that include both vertex-centric processing and queries.  Sensibly, a graph processing engine may be developed on top of a graph database.  However the two should not be confused, and performance is incomparable.

A closely related Big Data framework is MapReduce \cite{Dean2008,Polato2014}.  MapReduce is a different programming model from TLAV frameworks, but similarly enables large-scale computation and, when implemented, abstracts away the details of distributed programming.  The programming model is effective for many types of computation, but addresses neither iterative processing nor graph processing \cite{Polato2014,Malewicz2010}.  Iterative computation is not natively supported, as the programming model performs only a single pass over the data with no loop awareness.  Moreover, I/O is read/written to/from a distributed filesystem, {\em e.g.,} HDFS, rendering iterative computation inefficient \cite{Polato2014}.  Nonetheless, several frameworks have extended MapReduce to support iterative computation \cite{Ekanayake2010,Bu2012,Zhang2012} but such frameworks are still agnostic to the challenges of graph processing.  Graph computation with MapReduce has been explored \cite{Lin2010}, but is generally acknowledged to be lacking \cite{Cohen2009,Malewicz2010}.  A comparison of MapReduce and BSP is provided in \cite{Kajdanowicz2014}.  Still, some argue that MapReduce should remain the sole ``hammer" for Big Data analytics because of the widespread adoption throughout industry \cite{Lin2013}.

Similarly, in response to TLAV shortcomings, such as poor out-of-core support and lengthy loading times, some frameworks rework pre-existing graph database technologies to provide a vertex-centric interface \cite{Fan2015}.  However, many of these projects lose sight of the main problems addressed by the vertex-centric processing.  TLAV frameworks are ultimately Big Data solutions, designed large graphs to be leveraged against the memory and processing power of several machines, not single machines.  Moreover, TLAV frameworks iteratively process the entire graph, and do not provide graph queries like 1-hop or 2-hop neighbors.  TLAV frameworks are not a universal solution for graph analytics, but rather provide an approach for scalable, iterative graph processing.

Temporal graph processing is beyond the scope of this survey, though a small number of TLAV frameworks have been developed for temporal analysis \cite{Cheng2012,Hant2014}.  These frameworks compute temporal properties offline in batch through graph snapshots, necessitating multiple framework components, including a front-end ingress component, an analytics engine, and a storage component such as a graph database.  Temporal  graph layout optimizations were introduced in Chronos \cite{Hant2014}.  These frameworks illustrate how advanced graph analytics systems utilize the strengths of different graph technologies for different components, {\em e.g.,} graph databases for storage and online queries, and vertex-centric computation for batch analytics.  Dynamic graph algorithms and general analytics systems have also been surveyed \cite{Aggarwal2014,Vaquero2014}.  Dynamic graphs are supported by many frameworks including Pregel, but the topic was omitted from this survey due to widely varying support by the frameworks and broad scope of the topic.

While coined "vertex-centric" relative to conventional graph processing approaches, the algorithms executed by TLAV frameworks are more formally known as distributed algorithms.  Distributed algorithms is a mature field of study \cite{Lynch1996}, and further examples beyond Figure~\ref{fig:minval} may be found within the referenced frameworks.  Some works have explored distributed algorithms within the context of TLAV frameworks \cite{Yan2013}, but researchers and practitioners should be aware that TLAV frameworks execute distributed algorithms \cite{Lynch1996}, which come from a field with a considerable body of work, including theory and analysis.  The theoretical limits of what can be computed with vertex-centric frameworks, specifically with the synchronous, message-passing LOCAL model, has been studied \cite{kuhn2010}.  

This paper surveys and compares the various components of TLAV frameworks, which are a platform for executing vertex-centric algorithms.  Like MapReduce, these frameworks provide an interface for a user-defined function, while abstracting away the lower-level details of cluster computing.  Changing the components of the framework will impact system performance and run-time characteristics, but will generally not impact the design or result of the algorithm \footnote{An exception to this rule is synchronous versus asynchronous execution some algorithms, such as graph coloring Section~\ref{subsec:execution_policy}.}.

\section{Conclusions}
\label{sec:conc}

TLAV frameworks have been designed in response to the challenges of processing large graphs.  Primary challenges include the unstructured nature of graphs, where an edge may span any two vertices, so the entire graph must be randomly accessible for conventional processing.  TLAV frameworks are also developed for ease of use, providing a simple vertex-centric interface while abstracting away the lower level details of cluster computing.  MapReduce similarly enables highly scalable computing, but is ill-suited for iterative graph processing.

By adopting a vertex-centric programming model, the scope of computation is dramatically reduced.  To perform an update, each vertex only needs data from immediate neighbors.  Data residing on a separate machine can be acquired directly between workers, avoiding the bottleneck of central coordination, enabling excellent scalability.  The four pillars of the vertex-centric programming model, (i) timing, (ii) communication, (iii) the execution model, and (iv) partitioning, were presented and surveyed in the context of distributed graph processing frameworks.  However, vertex-centric algorithms, colloquially known as distributed algorithms, have an established history and are still actively researched \cite{Lynch1996,kuhn2010}.

Several related frameworks were explored that similarly adopt a computational scope of the graph at varying granularity.  These frameworks of alternative scope are like a Goldilocks solution to graph processing.  Centralized algorithms with the entire graph in scope require too much memory, vertex-centric algorithms can scale but are less expressive and require many relatively slow messages, whereas subgraph-centric algorithms can utilize the two resources just right.  A significant contribution of TLAV frameworks is exposing how, for graphs, reducing the scope of a program increases scalability.  

Of course, expressing a particular algorithm as subgraph-centric is not trivial.  The future of practical large-scale distributed graph processing may be related to finding algorithms that process a graph as independent subgraphs, such as divide-and-conquer, or algorithms that can process graphs at multiple, or even dynamic, scopes \cite{Wang2014}.  The performance of the subgraph-centric processing is also closely tied to the effectiveness of large-scale graph partitioning, including streaming and distributed partitioning techniques.

TLAV frameworks are a tool for graph processing at scale.  Not all graphs are large enough to necessitate distributed processing, and not all graph problems need the whole graph to be computed iteratively.  Moreover, there is often more than one way to solve a problem, but these frameworks are simple to program, easy to distribute, and are not a bad choice for the right type of problem.  Subgraph-centric frameworks take vertex-centric frameworks a step further for performance.  Datasets will continue to grow dramatically into the new age of Big Data, and the design of processing systems should begin asking if they can scale out infinitely.  TLAV frameworks illustrate how conventional centralized systems will fail in the Big Data ecosystem, and how decentralized platforms must be embraced.

\section{Acknowledgements}
This work was supported in part by the AFOSR Grant \#FA9550-15-1-0003, as well as a Department of Education GAANN Fellowship awarded by the University of Notre Dame Department of Computer Science and Engineering.


\begin{thebibliography}{00}



\ifx \showCODEN    \undefined \def \showCODEN     #1{\unskip}     \fi
\ifx \showDOI      \undefined \def \showDOI       #1{{\tt DOI:}\penalty0{#1}\ }
  \fi
\ifx \showISBNx    \undefined \def \showISBNx     #1{\unskip}     \fi
\ifx \showISBNxiii \undefined \def \showISBNxiii  #1{\unskip}     \fi
\ifx \showISSN     \undefined \def \showISSN      #1{\unskip}     \fi
\ifx \showLCCN     \undefined \def \showLCCN      #1{\unskip}     \fi
\ifx \shownote     \undefined \def \shownote      #1{#1}          \fi
\ifx \showarticletitle \undefined \def \showarticletitle #1{#1}   \fi
\ifx \showURL      \undefined \def \showURL       #1{#1}          \fi

\bibitem[\protect\citeauthoryear{Abou-Rjeili and Karypis}{Abou-Rjeili and
  Karypis}{2006}]{Abou-Rjeili2006}
{Amine Abou-Rjeili} {and} {George Karypis}. 2006.
\newblock \showarticletitle{Multilevel Algorithms for Partitioning Power-law
  Graphs}. In {\em Proceedings of the 20th International Conference on Parallel
  and Distributed Processing} {\em (IPDPS'06)}. IEEE Computer Society,
  Washington, DC, USA, 124--124.
\newblock
\showISBNx{1-4244-0054-6}
\showURL{\url{http://dl.acm.org/citation.cfm?id=1898953.1899055}}


\bibitem[\protect\citeauthoryear{Aggarwal and Subbian}{Aggarwal and
  Subbian}{2014}]{Aggarwal2014}
{Charu Aggarwal} {and} {Karthik Subbian}. 2014.
\newblock \showarticletitle{Evolutionary Network Analysis: A Survey}.
\newblock {\em ACM Comput. Surv.\/} {47}, 1, Article 10 (May 2014), 36 pages.
\newblock
\showISSN{0360-0300}
\showDOI{\url{http://dx.doi.org/10.1145/2601412}}


\bibitem[\protect\citeauthoryear{Ahmed, Shervashidze, Narayanamurthy,
  Josifovski, and Smola}{Ahmed et~al\mbox{.}}{2013}]{Ahmed2013}
{Amr Ahmed}, {Nino Shervashidze}, {Shravan Narayanamurthy}, {Vanja Josifovski},
  {and} {Alexander~J. Smola}. 2013.
\newblock \showarticletitle{Distributed Large-scale Natural Graph
  Factorization}. In {\em Proceedings of the 22Nd International Conference on
  World Wide Web} {\em (WWW '13)}. International World Wide Web Conferences
  Steering Committee, Geneva, Switzerland, 37--48.
\newblock
\showISBNx{978-1-4503-2035-1}
\showURL{\url{http://dl.acm.org/citation.cfm?id=2488388.2488393}}


\bibitem[\protect\citeauthoryear{Ajwani, Karnstedt, and Sala}{Ajwani
  et~al\mbox{.}}{2015}]{Ajwani2015}
{Deepak Ajwani}, {Marcel Karnstedt}, {and} {Alessandra Sala}. 2015.
\newblock \showarticletitle{Processing Large Graphs: Representations, Storage,
  Systems and Algorithms}. In {\em Proceedings of the 24th International
  Conference on World Wide Web Companion} {\em (WWW '15 Companion)}.
  International World Wide Web Conferences Steering Committee, Republic and
  Canton of Geneva, Switzerland, 1545--1545.
\newblock
\showISBNx{978-1-4503-3473-0}
\showDOI{\url{http://dx.doi.org/10.1145/2740908.2741990}}


\bibitem[\protect\citeauthoryear{Albert, Jeong, and Barab{\'a}si}{Albert
  et~al\mbox{.}}{2000}]{Albert2000}
{R{\'e}ka Albert}, {Hawoong Jeong}, {and} {Albert-L{\'a}szl{\'o} Barab{\'a}si}.
  2000.
\newblock \showarticletitle{Error and attack tolerance of complex networks}.
\newblock {\em Nature\/} {406}, 6794 (2000), 378--382.
\newblock
\showDOI{\url{http://dx.doi.org/10.1038/35019019}}


\bibitem[\protect\citeauthoryear{Andreev and Racke}{Andreev and Racke}{2006}]{Andreev2006}
{Konstantin Andreev} {and} {Harald Racke}. 2006.
\newblock \showarticletitle{Balanced graph partitioning}.
\newblock {\em Theory of Computing Systems\/} {39}, 6 (2006), 929--939.
\newblock


\bibitem[\protect\citeauthoryear{Avery}{Avery}{2011}]{Avery2011}
{Ching Avery}. 2011.
\newblock \showarticletitle{Giraph: Large-scale graph processing infrastructure
  on Hadoop}. In {\em Proceedings of Hadoop Summit}. Santa Clara, USA.
\newblock


\bibitem[\protect\citeauthoryear{Bao and Suzumura}{Bao and Suzumura}{2013}]{Bao2013}
{Nguyen~Thien Bao} {and} {Toyotaro Suzumura}. 2013.
\newblock \showarticletitle{Towards Highly Scalable Pregel-based Graph
  Processing Platform with x10}. In {\em Proceedings of the 22Nd International
  Conference on World Wide Web Companion} {\em (WWW '13 Companion)}.
  International World Wide Web Conferences Steering Committee, Geneva,
  Switzerland, 501--508.
\newblock
\showISBNx{978-1-4503-2038-2}
\showURL{\url{http://dl.acm.org/citation.cfm?id=2487788.2487984}}


\bibitem[\protect\citeauthoryear{Beamer, Asanovi\'{c}, and Patterson}{Beamer
  et~al\mbox{.}}{2013}]{Beamer2013}
{Scott Beamer}, {Krste Asanovi\'{c}}, {and} {David Patterson}. 2013.
\newblock \showarticletitle{Direction-optimizing Breadth-first Search}.
\newblock {\em Sci. Program.\/} {21}, 3-4 (July 2013), 137--148.
\newblock
\showISSN{1058-9244}
\showURL{\url{http://dl.acm.org/citation.cfm?id=2590251.2590258}}


\bibitem[\protect\citeauthoryear{Bellman}{Bellman}{1958}]{Bellman1958}
{Richard Bellman}. 1958.
\newblock \showarticletitle{{On a Routing Problem}}.
\newblock {\it Quart. Appl. Math.}  {16} (1958), 87--90.
\newblock


\bibitem[\protect\citeauthoryear{Bender, Brodal, Fagerberg, Jacob, and
  Vicari}{Bender et~al\mbox{.}}{2007}]{Bender2010}
{Michael~A. Bender}, {Gerth~St{\o}lting Brodal}, {Rolf Fagerberg}, {Riko
  Jacob}, {and} {Elias Vicari}. 2007.
\newblock \showarticletitle{Optimal Sparse Matrix Dense Vector Multiplication
  in the I/O-model}. In {\em Proceedings of the Nineteenth Annual ACM Symposium
  on Parallel Algorithms and Architectures} {\em (SPAA '07)}. ACM, New York,
  NY, USA, 61--70.
\newblock
\showISBNx{978-1-59593-667-7}
\showDOI{\url{http://dx.doi.org/10.1145/1248377.1248391}}


\bibitem[\protect\citeauthoryear{Benlic and Hao}{Benlic and Hao}{2013}]{Benlic2013}
{Una Benlic} {and} {Jin-Kao Hao}. 2013.
\newblock \showarticletitle{Breakout Local Search for the Vertex Separator
  Problem}. In {\em Proceedings of the Twenty-Third International Joint
  Conference on Artificial Intelligence} {\em (IJCAI '13)}. AAAI Press,
  461--467.
\newblock
\showISBNx{978-1-57735-633-2}
\showURL{\url{http://dl.acm.org/citation.cfm?id=2540128.2540196}}


\bibitem[\protect\citeauthoryear{Bertsekas, Guerriero, and Musmanno}{Bertsekas
  et~al\mbox{.}}{1996}]{Bertsekas1996}
{D.~P. Bertsekas}, {F. Guerriero}, {and} {R. Musmanno}. 1996.
\newblock \showarticletitle{Parallel Asynchronous Label-correcting Methods for
  Shortest Paths}.
\newblock {\em J. Optim. Theory Appl.\/} {88}, 2 (Feb. 1996), 297--320.
\newblock
\showISSN{0022-3239}
\showDOI{\url{http://dx.doi.org/10.1007/BF02192173}}


\bibitem[\protect\citeauthoryear{Bertsekas and Tsitsiklis}{Bertsekas and
  Tsitsiklis}{1989}]{Bertsekas1989}
{Dimitri~P. Bertsekas} {and} {John~N. Tsitsiklis}. 1989.
\newblock {\em Parallel and Distributed Computation: Numerical Methods}.
\newblock Prentice-Hall, Inc., Upper Saddle River, NJ, USA.
\newblock
\showISBNx{0-13-648700-9}


\bibitem[\protect\citeauthoryear{Bourse, Lelarge, and Vojnovic}{Bourse
  et~al\mbox{.}}{2014}]{Bourse2014}
{Florian Bourse}, {Marc Lelarge}, {and} {Milan Vojnovic}. 2014.
\newblock \showarticletitle{Balanced Graph Edge Partition}. In {\em Proceedings
  of the 20th ACM SIGKDD International Conference on Knowledge Discovery and
  Data Mining} {\em (KDD '14)}. ACM, New York, NY, USA, 1456--1465.
\newblock
\showISBNx{978-1-4503-2956-9}
\showDOI{\url{http://dx.doi.org/10.1145/2623330.2623660}}


\bibitem[\protect\citeauthoryear{Brin and Page}{Brin and Page}{1998}]{Brin1998}
{Sergey Brin} {and} {Lawrence Page}. 1998.
\newblock \showarticletitle{The Anatomy of a Large-scale Hypertextual Web
  Search Engine}.
\newblock {\em Comput. Netw. ISDN Syst.\/} {30}, 1-7 (April 1998), 107--117.
\newblock
\showISSN{0169-7552}
\showDOI{\url{http://dx.doi.org/10.1016/S0169-7552(98)00110-X}}


\bibitem[\protect\citeauthoryear{Bu, Howe, Balazinska, and Ernst}{Bu
  et~al\mbox{.}}{2012}]{Bu2012}
{Yingyi Bu}, {Bill Howe}, {Magdalena Balazinska}, {and} {Michael~D. Ernst}.
  2012.
\newblock \showarticletitle{The HaLoop Approach to Large-scale Iterative Data
  Analysis}.
\newblock {\em The VLDB Journal\/} {21}, 2 (April 2012), 169--190.
\newblock
\showISSN{1066-8888}
\showDOI{\url{http://dx.doi.org/10.1007/s00778-012-0269-7}}


\bibitem[\protect\citeauthoryear{Bulu\~c{c} and Gilbert}{Bulu\~c{c} and
  Gilbert}{2011}]{Bulucc2011}
{Ayd\'in Bulu\~c{c}} {and} {John~R Gilbert}. 2011.
\newblock \showarticletitle{The Combinatorial BLAS: Design, Implementation, and
  Applications}.
\newblock {\em Int. J. High Perform. Comput. Appl.\/} {25}, 4 (Nov. 2011),
  496--509.
\newblock
\showISSN{1094-3420}
\showDOI{\url{http://dx.doi.org/10.1177/1094342011403516}}


\bibitem[\protect\citeauthoryear{Cha and Lee}{Cha and Lee}{2001}]{Cha2001hbsp}
{Hojung Cha} {and} {Dongho Lee}. 2001.
\newblock \showarticletitle{H-BSP: A Hierarchical BSP Computation Model}.
\newblock {\em J. Supercomput.\/} {18}, 2 (Feb. 2001), 179--200.
\newblock
\showISSN{0920-8542}
\showDOI{\url{http://dx.doi.org/10.1023/A:1008113017444}}


\bibitem[\protect\citeauthoryear{Chandy and Lamport}{Chandy and
  Lamport}{1985}]{Chandy1985}
{K.~Mani Chandy} {and} {Leslie Lamport}. 1985.
\newblock \showarticletitle{Distributed Snapshots: Determining Global States of
  Distributed Systems}.
\newblock {\em ACM Trans. Comput. Syst.\/} {3}, 1 (Feb. 1985), 63--75.
\newblock
\showISSN{0734-2071}
\showDOI{\url{http://dx.doi.org/10.1145/214451.214456}}


\bibitem[\protect\citeauthoryear{Chandy and Misra}{Chandy and Misra}{1984}]{Chandy1984}
{K.~M. Chandy} {and} {J. Misra}. 1984.
\newblock \showarticletitle{The Drinking Philosophers Problem}.
\newblock {\em ACM Trans. Program. Lang. Syst.\/} {6}, 4 (Oct. 1984), 632--646.
\newblock
\showISSN{0164-0925}
\showDOI{\url{http://dx.doi.org/10.1145/1780.1804}}


\bibitem[\protect\citeauthoryear{Charles, Grothoff, Saraswat, Donawa, Kielstra,
  Ebcioglu, von Praun, and Sarkar}{Charles et~al\mbox{.}}{2005}]{Charles2005}
{Philippe Charles}, {Christian Grothoff}, {Vijay Saraswat}, {Christopher
  Donawa}, {Allan Kielstra}, {Kemal Ebcioglu}, {Christoph von Praun}, {and}
  {Vivek Sarkar}. 2005.
\newblock \showarticletitle{X10: An Object-oriented Approach to Non-uniform
  Cluster Computing}.
\newblock {\em SIGPLAN Not.\/} {40}, 10 (Oct. 2005), 519--538.
\newblock
\showISSN{0362-1340}
\showDOI{\url{http://dx.doi.org/10.1145/1103845.1094852}}


\bibitem[\protect\citeauthoryear{Chen, Bai, Li, Gou, Suo, and Pan}{Chen
  et~al\mbox{.}}{2014a}]{Chen}
{Qun Chen}, {Song Bai}, {Zhanhuai Li}, {Zhiying Gou}, {Bo Suo}, {and} {Wei
  Pan}. 2014a.
\newblock GraphHP: A Hybrid Platform for Iterative Graph Processing.
\newblock Retrieved July 17, 2014 from
  \url{http://wowbigdata.net.cn/paper/GraphHP(2014).
\newblock


\bibitem[\protect\citeauthoryear{Chen, Ding, Wang, Chen, Zang, and Guan}{Chen
  et~al\mbox{.}}{2014b}]{Chen2014}
{Rong Chen}, {Xin Ding}, {Peng Wang}, {Haibo Chen}, {Binyu Zang}, {and}
  {Haibing Guan}. 2014b.
\newblock \showarticletitle{Computation and Communication Efficient Graph
  Processing with Distributed Immutable View}. In {\em Proceedings of the 23rd
  International Symposium on High-performance Parallel and Distributed
  Computing} {\em (HPDC '14)}. ACM, New York, NY, USA, 215--226.
\newblock
\showISBNx{978-1-4503-2749-7}
\showDOI{\url{http://dx.doi.org/10.1145/2600212.2600233}}


\bibitem[\protect\citeauthoryear{Chen, Shi, Chen, and Chen}{Chen
  et~al\mbox{.}}{2015}]{Chen2013b}
{Rong Chen}, {Jiaxin Shi}, {Yanzhe Chen}, {and} {Haibo Chen}. 2015.
\newblock \showarticletitle{PowerLyra: Differentiated Graph Computation and
  Partitioning on Skewed Graphs}. In {\em Proceedings of the Tenth European
  Conference on Computer Systems} {\em (EuroSys '15)}. ACM, New York, NY, USA,
  Article 1, 15 pages.
\newblock
\showISBNx{978-1-4503-3238-5}
\showDOI{\url{http://dx.doi.org/10.1145/2741948.2741970}}


\bibitem[\protect\citeauthoryear{Chen, Shi, Zang, and Guan}{Chen
  et~al\mbox{.}}{2014}]{Chen2014bi}
{Rong Chen}, {Jiaxin Shi}, {Binyu Zang}, {and} {Haibing Guan}. 2014.
\newblock \showarticletitle{Bipartite-oriented Distributed Graph Partitioning
  for Big Learning}. In {\em Proceedings of 5th Asia-Pacific Workshop on
  Systems} {\em (APSys '14)}. ACM, Article 14, 7 pages.
\newblock
\showISBNx{978-1-4503-3024-4}
\showDOI{\url{http://dx.doi.org/10.1145/2637166.2637236}}


\bibitem[\protect\citeauthoryear{Chen, Yang, Weng, Choi, He, and Li}{Chen
  et~al\mbox{.}}{2012}]{Chen2012}
{Rishan Chen}, {Mao Yang}, {Xuetian Weng}, {Byron Choi}, {Bingsheng He}, {and}
  {Xiaoming Li}. 2012.
\newblock \showarticletitle{Improving Large Graph Processing on Partitioned
  Graphs in the Cloud}. In {\em Proceedings of the Third ACM Symposium on Cloud
  Computing} {\em (SoCC '12)}. ACM, New York, NY, USA, Article 3, 13 pages.
\newblock
\showISBNx{978-1-4503-1761-0}
\showDOI{\url{http://dx.doi.org/10.1145/2391229.2391232}}


\bibitem[\protect\citeauthoryear{Chen, Gan, and Suel}{Chen
  et~al\mbox{.}}{2002}]{Chen2002}
{Yen-Yu Chen}, {Qingqing Gan}, {and} {Torsten Suel}. 2002.
\newblock \showarticletitle{I/O-efficient Techniques for Computing Pagerank}.
  In {\em Proceedings of the Eleventh International Conference on Information
  and Knowledge Management} {\em (CIKM '02)}. ACM, New York, NY, USA, 549--557.
\newblock
\showISBNx{1-58113-492-4}
\showDOI{\url{http://dx.doi.org/10.1145/584792.584882}}


\bibitem[\protect\citeauthoryear{Cheng, Hong, Kyrola, Miao, Weng, Wu, Yang,
  Zhou, Zhao, and Chen}{Cheng et~al\mbox{.}}{2012}]{Cheng2012}
{Raymond Cheng}, {Ji Hong}, {Aapo Kyrola}, {Youshan Miao}, {Xuetian Weng},
  {Ming Wu}, {Fan Yang}, {Lidong Zhou}, {Feng Zhao}, {and} {Enhong Chen}. 2012.
\newblock \showarticletitle{Kineograph: Taking the Pulse of a Fast-changing and
  Connected World}. In {\em Proceedings of the 7th ACM European Conference on
  Computer Systems} {\em (EuroSys '12)}. ACM, New York, NY, USA, 85--98.
\newblock
\showISBNx{978-1-4503-1223-3}
\showDOI{\url{http://dx.doi.org/10.1145/2168836.2168846}}


\bibitem[\protect\citeauthoryear{Chung and Condon}{Chung and Condon}{1996}]{Chung1996}
{Sun Chung} {and} {Anne Condon}. 1996.
\newblock \showarticletitle{Parallel implementation of Bouvka's minimum
  spanning tree algorithm}. In {\em Proceedings of the 10th International
  Parallel Processing Symposium} {\em (IPPS '06)}. IEEE Computer Society,
  Washington, DC, USA, 302 -- 308.
\newblock
\showISBNx{0-8186-7255-2}
\showDOI{\url{http://dx.doi.org/10.1109/IPPS.1996.508073}}


\bibitem[\protect\citeauthoryear{Cohen}{Cohen}{2009}]{Cohen2009}
{Jonathan Cohen}. 2009.
\newblock \showarticletitle{Graph Twiddling in a MapReduce World}.
\newblock {\em Computing in Science and Engg.\/} {11}, 4 (July 2009), 29--41.
\newblock
\showISSN{1521-9615}
\showDOI{\url{http://dx.doi.org/10.1109/MCSE.2009.120}}


\bibitem[\protect\citeauthoryear{Dean and Ghemawat}{Dean and Ghemawat}{2008}]{Dean2008}
{Jeffrey Dean} {and} {Sanjay Ghemawat}. 2008.
\newblock \showarticletitle{MapReduce: Simplified Data Processing on Large
  Clusters}.
\newblock {\em Commun. ACM\/} {51}, 1 (Jan. 2008), 107--113.
\newblock
\showISSN{0001-0782}
\showDOI{\url{http://dx.doi.org/10.1145/1327452.1327492}}


\bibitem[\protect\citeauthoryear{Didi~Biha and Meurs}{Didi~Biha and
  Meurs}{2011}]{Biha2011}
{Mohamed Didi~Biha} {and} {Marie-Jean Meurs}. 2011.
\newblock \showarticletitle{An Exact Algorithm for Solving the Vertex Separator
  Problem}.
\newblock {\em J. of Global Optimization\/} {49}, 3 (March 2011), 425--434.
\newblock
\showISSN{0925-5001}
\showDOI{\url{http://dx.doi.org/10.1007/s10898-010-9568-y}}


\bibitem[\protect\citeauthoryear{Dijkstra}{Dijkstra}{1959}]{Dijkstra1959}
{E.W. Dijkstra}. 1959.
\newblock \showarticletitle{A note on two problems in connection with graphs}.
\newblock {\it Numer. Math.} {1}, 1 (1959), 269--271.
\newblock


\bibitem[\protect\citeauthoryear{Dijkstra}{Dijkstra}{1971}]{Dijkstra1971}
{E.~W. Dijkstra}. 1971.
\newblock \showarticletitle{Hierarchical Ordering of Sequential Processes}.
\newblock {\em Acta Inf.\/} {1}, 2 (June 1971), 115--138.
\newblock
\showISSN{0001-5903}
\showDOI{\url{http://dx.doi.org/10.1007/BF00289519}}


\bibitem[\protect\citeauthoryear{Edmonds}{Edmonds}{2013}]{Edmonds2013}
{Nicholas Edmonds}. 2013.
\newblock {\em Active messages as a spanning model for parallel graph
  computation}.
\newblock Ph.D. Dissertation. Indiana University.
\newblock


\bibitem[\protect\citeauthoryear{Ekanayake, Li, Zhang, Gunarathne, Bae, Qiu,
  and Fox}{Ekanayake et~al\mbox{.}}{2010}]{Ekanayake2010}
{Jaliya Ekanayake}, {Hui Li}, {Bingjing Zhang}, {Thilina Gunarathne},
  {Seung-Hee Bae}, {Judy Qiu}, {and} {Geoffrey Fox}. 2010.
\newblock \showarticletitle{Twister: A Runtime for Iterative MapReduce}. In
  {\em Proceedings of the 19th ACM International Symposium on High Performance
  Distributed Computing} {\em (HPDC '10)}. ACM, New York, NY, USA, 810--818.
\newblock
\showISBNx{978-1-60558-942-8}
\showDOI{\url{http://dx.doi.org/10.1145/1851476.1851593}}


\bibitem[\protect\citeauthoryear{Fan, Raj, and Patel}{Fan
  et~al\mbox{.}}{2015}]{Fan2015}
{Jing Fan}, {Adalbert Gerald~Soosai Raj}, {and} {Jignesh~M. Patel}. 2015.
\newblock \showarticletitle{The Case Against Specialized Graph Analytics
  Engines}. In {\em {CIDR} 2015, Seventh Biennial Conference on Innovative Data
  Systems Research, Asilomar, CA, USA, January 4-7, 2015, Online Proceedings}.
\newblock
\showURL{\url{http://www.cidrdb.org/cidr2015/Papers/CIDR15_Paper20.pdf}}


\bibitem[\protect\citeauthoryear{Feige, Hajiaghayi, and Lee}{Feige
  et~al\mbox{.}}{2008}]{Feige2008}
{Uriel Feige}, {MohammadTaghi Hajiaghayi}, {and} {James~R. Lee}. 2008.
\newblock \showarticletitle{Improved Approximation Algorithms for Minimum
  Weight Vertex Separators}.
\newblock {\em SIAM J. Comput.\/} {38}, 2 (May 2008), 629--657.
\newblock
\showISSN{0097-5397}
\showDOI{\url{http://dx.doi.org/10.1137/05064299X}}


\bibitem[\protect\citeauthoryear{Freeman}{Freeman}{1977}]{Freeman1977}
{Linton~C Freeman}. 1977.
\newblock \showarticletitle{A set of measures of centrality based on
  betweenness}.
\newblock {\em Sociometry\/} (1977), 35--41.
\newblock


\bibitem[\protect\citeauthoryear{Gehweiler and Meyerhenke}{Gehweiler and
  Meyerhenke}{2010}]{Gehweiler2010}
{J. Gehweiler} {and} {H. Meyerhenke}. 2010.
\newblock \showarticletitle{A distributed diffusive heuristic for clustering a
  virtual P2P supercomputer}. In {\em Parallel Distributed Processing,
  Workshops and Phd Forum (IPDPSW), 2010 IEEE International Symposium on}.
  1--8.
\newblock
\showDOI{\url{http://dx.doi.org/10.1109/IPDPSW.2010.5470922}}


\bibitem[\protect\citeauthoryear{Gonzalez, Low, Gretton, and Guestrin}{Gonzalez
  et~al\mbox{.}}{2011}]{Gonzalez2011}
{Joseph Gonzalez}, {Yucheng Low}, {Arthur Gretton}, {and} {Carlos Guestrin}.
  2011.
\newblock \showarticletitle{Parallel gibbs sampling: From colored fields to
  thin junction trees}. In {\em International Conference on Artificial
  Intelligence and Statistics}. 324--332.
\newblock


\bibitem[\protect\citeauthoryear{Gonzalez, Low, Gu, Bickson, and
  Guestrin}{Gonzalez et~al\mbox{.}}{2012}]{Gonzalez2012}
{Joseph~E. Gonzalez}, {Yucheng Low}, {Haijie Gu}, {Danny Bickson}, {and}
  {Carlos Guestrin}. 2012.
\newblock \showarticletitle{PowerGraph: Distributed Graph-parallel Computation
  on Natural Graphs}. In {\em Proceedings of the 10th USENIX Conference on
  Operating Systems Design and Implementation} {\em (OSDI'12)}. USENIX
  Association, Berkeley, CA, USA, 17--30.
\newblock
\showISBNx{978-1-931971-96-6}
\showURL{\url{http://dl.acm.org/citation.cfm?id=2387880.2387883}}


\bibitem[\protect\citeauthoryear{Gonzalez, Xin, Dave, Crankshaw, Franklin, and
  Stoica}{Gonzalez et~al\mbox{.}}{2014}]{Gonzalez2014}
{Joseph~E. Gonzalez}, {Reynold~S. Xin}, {Ankur Dave}, {Daniel Crankshaw},
  {Michael~J. Franklin}, {and} {Ion Stoica}. 2014.
\newblock \showarticletitle{GraphX: Graph Processing in a Distributed Dataflow
  Framework}. In {\em Proceedings of the 11th USENIX Conference on Operating
  Systems Design and Implementation} {\em (OSDI'14)}. USENIX Association,
  Berkeley, CA, USA, 599--613.
\newblock
\showISBNx{978-1-931971-16-4}
\showURL{\url{http://dl.acm.org/citation.cfm?id=2685048.2685096}}


\bibitem[\protect\citeauthoryear{Gregor and Lumsdaine}{Gregor and
  Lumsdaine}{2005}]{Gregor2005}
{Douglas Gregor} {and} {Andrew Lumsdaine}. 2005.
\newblock \showarticletitle{The parallel BGL: A generic library for distributed
  graph computations}. In {\em In Proceedings of the Parallel Object-Oriented
  Scientific Computing} {\em (POOSC'14)}.
\newblock


\bibitem[\protect\citeauthoryear{Guerrieri and Montresor}{Guerrieri and
  Montresor}{2014}]{Guerrieri2014}
{Alessio Guerrieri} {and} {Alberto Montresor}. 2014.
\newblock \showarticletitle{Distributed Edge Partitioning for Graph
  Processing}.
\newblock {\em arXiv preprint arXiv:1403.6270\/} (2014).
\newblock
\showURL{\url{http://arxiv.org/abs/1403.6270}}


\bibitem[\protect\citeauthoryear{Gupta, Goel, Lin, Sharma, Wang, and
  Zadeh}{Gupta et~al\mbox{.}}{2013}]{Gupta2013}
{Pankaj Gupta}, {Ashish Goel}, {Jimmy Lin}, {Aneesh Sharma}, {Dong Wang}, {and}
  {Reza Zadeh}. 2013.
\newblock \showarticletitle{WTF: The Who to Follow Service at Twitter}. In {\em
  Proceedings of the 22Nd International Conference on World Wide Web} {\em (WWW
  '13)}. International World Wide Web Conferences Steering Committee, Geneva,
  Switzerland, 505--514.
\newblock
\showISBNx{978-1-4503-2035-1}
\showURL{\url{http://dl.acm.org/citation.cfm?id=2488388.2488433}}


\bibitem[\protect\citeauthoryear{Hager, Hungerford, and Safro}{Hager
  et~al\mbox{.}}{2014}]{Hager2014}
{William~W Hager}, {James~T Hungerford}, {and} {Ilya Safro}. 2014.
\newblock \showarticletitle{A Multilevel Bilinear Programming Algorithm For the
  Vertex Separator Problem}.
\newblock {\em arXiv preprint arXiv:1410.4885\/} (2014).
\newblock
\showURL{\url{http://arxiv.org/abs/1410.4885}}


\bibitem[\protect\citeauthoryear{Han, Daudjee, Ammar, Ozsu, Wang, and Jin}{Han
  et~al\mbox{.}}{2014}]{Han2014}
{Minyang Han}, {Khuzaima Daudjee}, {Khaled Ammar}, {M~Tamer Ozsu}, {Xingfang
  Wang}, {and} {Tianqi Jin}. 2014.
\newblock \showarticletitle{An Experimental Comparison of Pregel-like Graph
  Processing Systems}.
\newblock {\em Proceedings of the VLDB Endowment\/} {7}, 12 (2014), 1047--1058.
\newblock


\bibitem[\protect\citeauthoryear{Han, Lee, Park, Lee, Kim, Kim, and Yu}{Han
  et~al\mbox{.}}{2013}]{Han2013}
{Wook-Shin Han}, {Sangyeon Lee}, {Kyungyeol Park}, {Jeong-Hoon Lee}, {Min-Soo
  Kim}, {Jinha Kim}, {and} {Hwanjo Yu}. 2013.
\newblock \showarticletitle{TurboGraph: A Fast Parallel Graph Engine Handling
  Billion-scale Graphs in a Single PC}. In {\em Proceedings of the 19th ACM
  SIGKDD International Conference on Knowledge Discovery and Data Mining} {\em
  (KDD '13)}. ACM, New York, NY, USA, 77--85.
\newblock
\showISBNx{978-1-4503-2174-7}
\showDOI{\url{http://dx.doi.org/10.1145/2487575.2487581}}


\bibitem[\protect\citeauthoryear{Hant, Miao, Li, Wu, Yang, Zhou, Prabhakaran,
  Chen, and Chen}{Hant et~al\mbox{.}}{2014}]{Hant2014}
{Wentao Hant}, {Youshan Miao}, {Kaiwei Li}, {Ming Wu}, {Fan Yang}, {Lidong
  Zhou}, {Vijayan Prabhakaran}, {Wenguang Chen}, {and} {Enhong Chen}. 2014.
\newblock \showarticletitle{Chronos: A Graph Engine for Temporal Graph
  Analysis}. In {\em Proceedings of the Ninth European Conference on Computer
  Systems} {\em (EuroSys '14)}. ACM, New York, NY, USA, Article 1, 14 pages.
\newblock
\showISBNx{978-1-4503-2704-6}
\showDOI{\url{http://dx.doi.org/10.1145/2592798.2592799}}


\bibitem[\protect\citeauthoryear{Harshvardhan, Fidel, Amato, and
  Rauchwerger}{Harshvardhan et~al\mbox{.}}{2014}]{Harshvardhan2014}
{Harshvardhan}, {Adam Fidel}, {Nancy~M. Amato}, {and} {Lawrence Rauchwerger}.
  2014.
\newblock \showarticletitle{KLA: A New Algorithmic Paradigm for Parallel Graph
  Computations}. In {\em Proceedings of the 23rd International Conference on
  Parallel Architectures and Compilation} {\em (PACT '14)}. ACM, New York, NY,
  USA, 27--38.
\newblock
\showISBNx{978-1-4503-2809-8}
\showDOI{\url{http://dx.doi.org/10.1145/2628071.2628091}}


\bibitem[\protect\citeauthoryear{Hong, Oguntebi, and Olukotun}{Hong
  et~al\mbox{.}}{2011}]{Hong2011}
{Sungpack Hong}, {Tayo Oguntebi}, {and} {Kunle Olukotun}. 2011.
\newblock \showarticletitle{Efficient Parallel Graph Exploration on Multi-Core
  CPU and GPU}. In {\em Proceedings of the 2011 International Conference on
  Parallel Architectures and Compilation Techniques} {\em (PACT '11)}. IEEE
  Computer Society, Washington, DC, USA, 78--88.
\newblock
\showISBNx{978-0-7695-4566-0}
\showDOI{\url{http://dx.doi.org/10.1109/PACT.2011.14}}


\bibitem[\protect\citeauthoryear{Hoque and Gupta}{Hoque and Gupta}{}]{Hoque2013}
{Imranul Hoque} {and} {Indranil Gupta}.
\newblock \showarticletitle{LFGraph: Simple and Fast Distributed Graph
  Analytics}. In {\em Proceedings of the ACM Symposium on Timely Results in
  Operating Systems}.
\newblock


\bibitem[\protect\citeauthoryear{Iordanov}{Iordanov}{2010}]{Iordanov2010}
{Borislav Iordanov}. 2010.
\newblock \showarticletitle{HyperGraphDB: A Generalized Graph Database}.
\newblock In {\em Proceedings of the 2010 International Conference on Web-age
  Information Management}. Springer-Verlag, Berlin, Heidelberg, 25--36.
\newblock
\showISBNx{3-642-16719-5, 978-3-642-16719-5}
\showURL{\url{http://dl.acm.org/citation.cfm?id=1927585.1927589}}


\bibitem[\protect\citeauthoryear{Jain, Liao, and Willke}{Jain
  et~al\mbox{.}}{2013}]{Jain2013}
{Nilesh Jain}, {Guangdeng Liao}, {and} {Theodore~L. Willke}. 2013.
\newblock \showarticletitle{GraphBuilder: Scalable Graph ETL Framework}. In
  {\em First International Workshop on Graph Data Management Experiences and
  Systems} {\em (GRADES '13)}. ACM, New York, NY, USA, Article 4, 6 pages.
\newblock
\showISBNx{978-1-4503-2188-4}
\showDOI{\url{http://dx.doi.org/10.1145/2484425.2484429}}


\bibitem[\protect\citeauthoryear{Kajdanowicz, Kazienko, and Indyk}{Kajdanowicz
  et~al\mbox{.}}{2014}]{Kajdanowicz2014}
{Tomasz Kajdanowicz}, {Przemyslaw Kazienko}, {and} {Wojciech Indyk}. 2014.
\newblock \showarticletitle{Parallel Processing of Large Graphs}.
\newblock {\em Future Gener. Comput. Syst.\/}  {32} (March 2014), 324--337.
\newblock
\showISSN{0167-739X}
\showDOI{\url{http://dx.doi.org/10.1016/j.future.2013.08.007}}


\bibitem[\protect\citeauthoryear{Kang, Tong, Sun, Lin, and Faloutsos}{Kang
  et~al\mbox{.}}{2011}]{Kang2011}
{U. Kang}, {Hanghang Tong}, {Jimeng Sun}, {Ching-Yung Lin}, {and} {Christos
  Faloutsos}. 2011.
\newblock \showarticletitle{GBASE: A Scalable and General Graph Management
  System}. In {\em Proceedings of the 17th ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining} {\em (KDD '11)}. ACM, New York, NY,
  USA, 1091--1099.
\newblock
\showISBNx{978-1-4503-0813-7}
\showDOI{\url{http://dx.doi.org/10.1145/2020408.2020580}}


\bibitem[\protect\citeauthoryear{Kang, Tsourakakis, and Faloutsos}{Kang
  et~al\mbox{.}}{2009}]{Kang2009}
{U. Kang}, {Charalampos~E. Tsourakakis}, {and} {Christos Faloutsos}. 2009.
\newblock \showarticletitle{PEGASUS: A Peta-Scale Graph Mining System
  Implementation and Observations}. In {\em Proceedings of the 2009 Ninth IEEE
  International Conference on Data Mining} {\em (ICDM '09)}. IEEE Computer
  Society, Washington, DC, USA, 229--238.
\newblock
\showISBNx{978-0-7695-3895-2}
\showDOI{\url{http://dx.doi.org/10.1109/ICDM.2009.14}}


\bibitem[\protect\citeauthoryear{Karypis and Kumar}{Karypis and Kumar}{1995}]{Karypis1995a}
{George Karypis} {and} {Vipin Kumar}. 1995.
\newblock \showarticletitle{Multilevel graph partitioning schemes}. In {\em
  Proceedings of the International Conference on Parallel Processing} {\em
  (ICPP'95)}. 113--122.
\newblock


\bibitem[\protect\citeauthoryear{Karypis and Kumar}{Karypis and Kumar}{1996}]{Karypis1996}
{George Karypis} {and} {Vipin Kumar}. 1996.
\newblock \showarticletitle{Parallel Multilevel K-way Partitioning Scheme for
  Irregular Graphs}. In {\em Proceedings of the 1996 ACM/IEEE Conference on
  Supercomputing} {\em (Supercomputing '96)}. IEEE Computer Society,
  Washington, DC, USA, Article 35.
\newblock
\showISBNx{0-89791-854-1}
\showDOI{\url{http://dx.doi.org/10.1145/369028.369103}}


\bibitem[\protect\citeauthoryear{Kernighan and Lin}{Kernighan and Lin}{1970}]{Kernighan1970}
{Brian~W Kernighan} {and} {Shen Lin}. 1970.
\newblock \showarticletitle{An Efficient Heuristic Procedure for Partitioning
  Graphs}.
\newblock {\em Bell System Technical Journal\/} {49}, 2 (1970), 291--307.
\newblock
\showISSN{1538-7305}
\showDOI{\url{http://dx.doi.org/10.1002/j.1538-7305.1970.tb01770.x}}


\bibitem[\protect\citeauthoryear{Khan and Elnikety}{Khan and Elnikety}{2014}]{Khan2014}
{Arijit Khan} {and} {Sameh Elnikety}. 2014.
\newblock \showarticletitle{Systems for Big-graphs}.
\newblock {\em Proc. VLDB Endow.\/} {7}, 13 (Aug. 2014), 1709--1710.
\newblock
\showISSN{2150-8097}
\showURL{\url{http://dl.acm.org/citation.cfm?id=2733004.2733067}}


\bibitem[\protect\citeauthoryear{Khayyat, Awara, Alonazi, Jamjoom, Williams,
  and Kalnis}{Khayyat et~al\mbox{.}}{2013}]{Khayyat2013}
{Zuhair Khayyat}, {Karim Awara}, {Amani Alonazi}, {Hani Jamjoom}, {Dan
  Williams}, {and} {Panos Kalnis}. 2013.
\newblock \showarticletitle{Mizan: A System for Dynamic Load Balancing in
  Large-scale Graph Processing}. In {\em Proceedings of the 8th ACM European
  Conference on Computer Systems} {\em (EuroSys '13)}. ACM, New York, NY, USA,
  169--182.
\newblock
\showISBNx{978-1-4503-1994-2}
\showDOI{\url{http://dx.doi.org/10.1145/2465351.2465369}}


\bibitem[\protect\citeauthoryear{Kim, Trimi, and Chung}{Kim
  et~al\mbox{.}}{2014}]{ibmbigdata}
{Gang-Hoon Kim}, {Silvana Trimi}, {and} {Ji-Hyong Chung}. 2014.
\newblock \showarticletitle{Big-data Applications in the Government Sector}.
\newblock {\em Commun. ACM\/} {57}, 3 (March 2014), 78--85.
\newblock
\showISSN{0001-0782}
\showDOI{\url{http://dx.doi.org/10.1145/2500873}}


\bibitem[\protect\citeauthoryear{Kim and Candan}{Kim and Candan}{2012}]{Kim2012}
{Mijung Kim} {and} {K.~Sel\c{c}uk Candan}. 2012.
\newblock \showarticletitle{SBV-Cut: Vertex-cut Based Graph Partitioning Using
  Structural Balance Vertices}.
\newblock {\em Data Knowl. Eng.\/}  {72} (Feb. 2012), 285--303.
\newblock
\showISSN{0169-023X}
\showDOI{\url{http://dx.doi.org/10.1016/j.datak.2011.11.004}}


\bibitem[\protect\citeauthoryear{Kuhn, Moscibroda, and Wattenhofer}{Kuhn
  et~al\mbox{.}}{2010}]{kuhn2010}
{Fabian Kuhn}, {Thomas Moscibroda}, {and} {Roger Wattenhofer}. 2010.
\newblock \showarticletitle{Local computation: Lower and upper bounds}.
\newblock {\em arXiv preprint arXiv:1011.5470\/} (2010).
\newblock


\bibitem[\protect\citeauthoryear{Kulkarni, Pingali, Walter, Ramanarayanan,
  Bala, and Chew}{Kulkarni et~al\mbox{.}}{2009}]{Kulkarni2007}
{Milind Kulkarni}, {Keshav Pingali}, {Bruce Walter}, {Ganesh Ramanarayanan},
  {Kavita Bala}, {and} {L.~Paul Chew}. 2009.
\newblock \showarticletitle{Optimistic Parallelism Requires Abstractions}.
\newblock {\em Commun. ACM\/} {52}, 9 (Sept. 2009), 89--97.
\newblock
\showISSN{0001-0782}
\showDOI{\url{http://dx.doi.org/10.1145/1562164.1562188}}


\bibitem[\protect\citeauthoryear{Kyrola, Blelloch, and Guestrin}{Kyrola
  et~al\mbox{.}}{2012}]{Kyrola2012}
{Aapo Kyrola}, {Guy Blelloch}, {and} {Carlos Guestrin}. 2012.
\newblock \showarticletitle{GraphChi: Large-scale Graph Computation on Just a
  PC}. In {\em Proceedings of the 10th USENIX Conference on Operating Systems
  Design and Implementation} {\em (OSDI'12)}. USENIX Association, Berkeley, CA,
  USA, 31--46.
\newblock
\showISBNx{978-1-931971-96-6}
\showURL{\url{http://dl.acm.org/citation.cfm?id=2387880.2387884}}


\bibitem[\protect\citeauthoryear{Kyrola and Guestrin}{Kyrola and
  Guestrin}{2014}]{Kyrola2014}
{Aapo Kyrola} {and} {Carlos Guestrin}. 2014.
\newblock \showarticletitle{GraphChi-DB: Simple Design for a Scalable Graph
  Database System--on Just a PC}.
\newblock {\em arXiv preprint arXiv:1403.0701\/} (2014).
\newblock
\showURL{\url{http://arxiv.org/abs/1403.0701}}


\bibitem[\protect\citeauthoryear{Leskovec, Lang, Dasgupta, and
  Mahoney}{Leskovec et~al\mbox{.}}{2009}]{Leskovec2009}
{Jure Leskovec}, {Kevin~J Lang}, {Anirban Dasgupta}, {and} {Michael~W Mahoney}.
  2009.
\newblock \showarticletitle{Community structure in large networks: Natural
  cluster sizes and the absence of large well-defined clusters}.
\newblock {\em Internet Mathematics\/} {6}, 1 (2009), 29--123.
\newblock


\bibitem[\protect\citeauthoryear{Lin}{Lin}{2013}]{Lin2013}
{Jimmy Lin}. 2013.
\newblock \showarticletitle{Mapreduce is good enough? if all you have is a
  hammer, throw away everything that's not a nail!}
\newblock {\em Big Data\/} {1}, 1 (2013), 28--37.
\newblock


\bibitem[\protect\citeauthoryear{Lin and Schatz}{Lin and Schatz}{2010}]{Lin2010}
{Jimmy Lin} {and} {Michael Schatz}. 2010.
\newblock \showarticletitle{Design Patterns for Efficient Graph Algorithms in
  MapReduce}. In {\em Proceedings of the Eighth Workshop on Mining and Learning
  with Graphs} {\em (MLG '10)}. ACM, New York, NY, USA, 78--85.
\newblock
\showISBNx{978-1-4503-0214-2}
\showDOI{\url{http://dx.doi.org/10.1145/1830252.1830263}}


\bibitem[\protect\citeauthoryear{Low, Bickson, Gonzalez, Guestrin, Kyrola, and
  Hellerstein}{Low et~al\mbox{.}}{2012}]{Low2012}
{Yucheng Low}, {Danny Bickson}, {Joseph Gonzalez}, {Carlos Guestrin}, {Aapo
  Kyrola}, {and} {Joseph~M. Hellerstein}. 2012.
\newblock \showarticletitle{Distributed GraphLab: A Framework for Machine
  Learning and Data Mining in the Cloud}.
\newblock {\em Proc. VLDB Endow.\/} {5}, 8 (April 2012), 716--727.
\newblock
\showISSN{2150-8097}
\showDOI{\url{http://dx.doi.org/10.14778/2212351.2212354}}


\bibitem[\protect\citeauthoryear{Low, Gonzalez, Kyrola, Bickson, Guestrin, and
  Hellerstein}{Low et~al\mbox{.}}{2010}]{Low2010}
{Yucheng Low}, {Joseph Gonzalez}, {Aapo Kyrola}, {Danny Bickson}, {Carlos
  Guestrin}, {and} {Joseph~M Hellerstein}. 2010.
\newblock \showarticletitle{Graphlab: A new framework for parallel machine
  learning}.
\newblock {\em arXiv preprint arXiv:1006.4990\/} (2010).
\newblock


\bibitem[\protect\citeauthoryear{Lu, Dwarkadas, Cox, and Zwaenepoel}{Lu
  et~al\mbox{.}}{1995}]{Lu1995}
{Honghui Lu}, {Sandhya Dwarkadas}, {Alan~L. Cox}, {and} {Willy Zwaenepoel}.
  1995.
\newblock \showarticletitle{Message Passing Versus Distributed Shared Memory on
  Networks of Workstations}. In {\em Proceedings of the 1995 ACM/IEEE
  Conference on Supercomputing} {\em (Supercomputing '95)}. ACM, New York, NY,
  USA, Article 37.
\newblock
\showISBNx{0-89791-816-9}
\showDOI{\url{http://dx.doi.org/10.1145/224170.224285}}


\bibitem[\protect\citeauthoryear{Lu, Cheng, Yan, and Wu}{Lu
  et~al\mbox{.}}{2014}]{Lu2014}
{Yi Lu}, {James Cheng}, {Da Yan}, {and} {Huanhuan Wu}. 2014.
\newblock \showarticletitle{Large-Scale Distributed Graph Computing Systems: An
  Experimental Evaluation}.
\newblock {\em Proceedings of the VLDB Endowment\/} {8}, 3 (2014).
\newblock


\bibitem[\protect\citeauthoryear{Lumsdaine, Gregor, Hendrickson, and
  Berry}{Lumsdaine et~al\mbox{.}}{2007}]{Lumsdaine2007}
{Andrew Lumsdaine}, {Douglas Gregor}, {Bruce Hendrickson}, {and} {Jonathan
  Berry}. 2007.
\newblock \showarticletitle{Challenges in parallel graph processing}.
\newblock {\em Parallel Processing Letters\/} {17}, 01 (2007), 5--20.
\newblock
\showDOI{\url{http://dx.doi.org/10.1142/S0129626407002843}}


\bibitem[\protect\citeauthoryear{Lynch}{Lynch}{1996}]{Lynch1996}
{Nancy~A Lynch}. 1996.
\newblock {\em Distributed algorithms}.
\newblock Morgan Kaufmann.
\newblock


\bibitem[\protect\citeauthoryear{Malewicz, Austern, Bik, Dehnert, Horn, Leiser,
  and Czajkowski}{Malewicz et~al\mbox{.}}{2010}]{Malewicz2010}
{Grzegorz Malewicz}, {Matthew~H. Austern}, {Aart~J.C Bik}, {James~C. Dehnert},
  {Ilan Horn}, {Naty Leiser}, {and} {Grzegorz Czajkowski}. 2010.
\newblock \showarticletitle{Pregel: A System for Large-scale Graph Processing}.
  In {\em Proceedings of the 2010 ACM SIGMOD International Conference on
  Management of Data} {\em (SIGMOD '10)}. ACM, New York, NY, USA, 135--146.
\newblock
\showISBNx{978-1-4503-0032-2}
\showDOI{\url{http://dx.doi.org/10.1145/1807167.1807184}}


\bibitem[\protect\citeauthoryear{Malicevic, Bindschaedler, Roy, and
  Zwaenepoel}{Malicevic et~al\mbox{.}}{2014}]{xstreamweb}
{Jasmina Malicevic}, {Laurent Bindschaedler}, {Amitabha Roy}, {and} {Willy
  Zwaenepoel}. 2014.
\newblock X-Stream.
\newblock   (2014).
\newblock
\showURL{\url{http://labos.epfl.ch/x-stream}}


\bibitem[\protect\citeauthoryear{Meyer and Sanders}{Meyer and Sanders}{2003}]{Meyer2003}
{U. Meyer} {and} {P. Sanders}. 2003.
\newblock \showarticletitle{Δ-stepping: a parallelizable shortest path
  algorithm}.
\newblock {\em Journal of Algorithms\/} {49}, 1 (2003), 114 -- 152.
\newblock
\showISSN{0196-6774}
\showDOI{\url{http://dx.doi.org/10.1016/S0196-6774(03)00076-2}}
\newblock
\shownote{1998 European Symposium on Algorithms.}


\bibitem[\protect\citeauthoryear{Meyerhenke, Sanders, and Schulz}{Meyerhenke
  et~al\mbox{.}}{2014}]{Meyerhenke2014}
{Henning Meyerhenke}, {Peter Sanders}, {and} {Christian Schulz}. 2014.
\newblock \showarticletitle{Parallel Graph Partitioning for Complex Networks}.
\newblock {\em arXiv preprint arXiv:1404.4797\/} (2014).
\newblock


\bibitem[\protect\citeauthoryear{Miao, Liu, Huang, and Getoor}{Miao
  et~al\mbox{.}}{2013}]{Miao2013}
{Hui Miao}, {Xiangyang Liu}, {Bert Huang}, {and} {Lise Getoor}. 2013.
\newblock \showarticletitle{A hypergraph-partitioned vertex programming
  approach for large-scale consensus optimization}. In {\em Proceedings of the
  2013 IEEE International Conference on Big Data}. 193--198.
\newblock


\bibitem[\protect\citeauthoryear{Munagala and Ranade}{Munagala and
  Ranade}{1999}]{Munagala1999}
{Kameshwar Munagala} {and} {Abhiram Ranade}. 1999.
\newblock \showarticletitle{I/O-complexity of Graph Algorithms}. In {\em
  Proceedings of the Tenth Annual ACM-SIAM Symposium on Discrete Algorithms}
  {\em (SODA '99)}. Society for Industrial and Applied Mathematics,
  Philadelphia, PA, USA, 687--694.
\newblock
\showISBNx{0-89871-434-6}
\showURL{\url{http://dl.acm.org/citation.cfm?id=314500.314891}}


\bibitem[\protect\citeauthoryear{Nguyen, Lenharth, and Pingali}{Nguyen
  et~al\mbox{.}}{2013}]{Nguyen2013}
{Donald Nguyen}, {Andrew Lenharth}, {and} {Keshav Pingali}. 2013.
\newblock \showarticletitle{A Lightweight Infrastructure for Graph Analytics}.
  In {\em Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems
  Principles} {\em (SOSP '13)}. ACM, New York, NY, USA, 456--471.
\newblock
\showISBNx{978-1-4503-2388-8}
\showDOI{\url{http://dx.doi.org/10.1145/2517349.2522739}}


\bibitem[\protect\citeauthoryear{Nilakant, Dalibard, Roy, and Yoneki}{Nilakant
  et~al\mbox{.}}{2014}]{Nilakant2014}
{Karthik Nilakant}, {Valentin Dalibard}, {Amitabha Roy}, {and} {Eiko Yoneki}.
  2014.
\newblock \showarticletitle{PrefEdge: SSD Prefetcher for Large-Scale Graph
  Traversal}. In {\em Proceedings of International Conference on Systems and
  Storage} {\em (SYSTOR 2014)}. ACM, New York, NY, USA, Article 4, 12 pages.
\newblock
\showISBNx{978-1-4503-2920-0}
\showDOI{\url{http://dx.doi.org/10.1145/2611354.2611365}}


\bibitem[\protect\citeauthoryear{Nisar, Fard, and Miller}{Nisar
  et~al\mbox{.}}{2013}]{Nisar2013}
{M.~Usman Nisar}, {Arash Fard}, {and} {John~A. Miller}. 2013.
\newblock \showarticletitle{Techniques for Graph Analytics on Big Data}. In
  {\em Proceedings of the 2013 IEEE International Congress on Big Data} {\em
  (BIGDATACONGRESS '13)}. IEEE Computer Society, Washington, DC, USA, 255--262.
\newblock
\showISBNx{978-0-7695-5006-0}
\showDOI{\url{http://dx.doi.org/10.1109/BigData.Congress.2013.78}}


\bibitem[\protect\citeauthoryear{Nishimura and Ugander}{Nishimura and
  Ugander}{2013}]{Nishimura2013}
{Joel Nishimura} {and} {Johan Ugander}. 2013.
\newblock \showarticletitle{Restreaming Graph Partitioning: Simple Versatile
  Algorithms for Advanced Balancing}. In {\em Proceedings of the 19th ACM
  SIGKDD International Conference on Knowledge Discovery and Data Mining} {\em
  (KDD '13)}. ACM, New York, NY, USA, 1106--1114.
\newblock
\showISBNx{978-1-4503-2174-7}
\showDOI{\url{http://dx.doi.org/10.1145/2487575.2487696}}


\bibitem[\protect\citeauthoryear{Nitzberg and Lo}{Nitzberg and Lo}{1991}]{Nitzberg1991}
{Bill Nitzberg} {and} {Virginia Lo}. 1991.
\newblock \showarticletitle{Distributed Shared Memory: A Survey of Issues and
  Algorithms}.
\newblock {\em Computer\/} {24}, 8 (Aug. 1991), 52--60.
\newblock
\showISSN{0018-9162}
\showDOI{\url{http://dx.doi.org/10.1109/2.84877}}


\bibitem[\protect\citeauthoryear{Pace}{Pace}{2012}]{bspvsmr}
{Matthew~Felice Pace}. 2012.
\newblock \showarticletitle{\{BSP\} vs MapReduce}.
\newblock {\em Procedia Computer Science\/} {9}, 0 (2012), 246 -- 255.
\newblock
\showISSN{1877-0509}
\showDOI{\url{http://dx.doi.org/10.1016/j.procs.2012.04.026}}
\newblock
\shownote{Proceedings of the International Conference on Computational Science,
  \{ICCS\} 2012.}


\bibitem[\protect\citeauthoryear{Page, Brin, Motwani, and Winograd}{Page
  et~al\mbox{.}}{1999}]{Page1999}
{Lawrence Page}, {Sergey Brin}, {Rajeev Motwani}, {and} {Terry Winograd}. 1999.
\newblock {\em The PageRank citation ranking: Bringing order to the web.}
\newblock {T}echnical {R}eport. Stanford InfoLab.
\newblock


\bibitem[\protect\citeauthoryear{Pearce, Gokhale, and Amato}{Pearce
  et~al\mbox{.}}{2010}]{Pearce2010}
{Roger Pearce}, {Maya Gokhale}, {and} {Nancy~M. Amato}. 2010.
\newblock \showarticletitle{Multithreaded Asynchronous Graph Traversal for
  In-Memory and Semi-External Memory}. In {\em Proceedings of the 2010 ACM/IEEE
  International Conference for High Performance Computing, Networking, Storage
  and Analysis} {\em (SC '10)}. IEEE Computer Society, Washington, DC, USA,
  1--11.
\newblock
\showISBNx{978-1-4244-7559-9}
\showDOI{\url{http://dx.doi.org/10.1109/SC.2010.34}}


\bibitem[\protect\citeauthoryear{Peleg}{Peleg}{2000}]{Peleg}
{David Peleg}. 2000.
\newblock {\em Distributed Computing: A Locality-sensitive Approach}.
\newblock Society for Industrial and Applied Mathematics, Philadelphia, PA,
  USA.
\newblock
\showISBNx{0-89871-464-8}


\bibitem[\protect\citeauthoryear{Pingali, Nguyen, Kulkarni, Burtscher, Hassaan,
  Kaleem, Lee, Lenharth, Manevich, M\'{e}ndez-Lojo, Prountzos, and Sui}{Pingali
  et~al\mbox{.}}{2011}]{pingali11}
{Keshav Pingali}, {Donald Nguyen}, {Milind Kulkarni}, {Martin Burtscher},
  {M.~Amber Hassaan}, {Rashid Kaleem}, {Tsung-Hsien Lee}, {Andrew Lenharth},
  {Roman Manevich}, {Mario M\'{e}ndez-Lojo}, {Dimitrios Prountzos}, {and} {Xin
  Sui}. 2011.
\newblock \showarticletitle{The tao of parallelism in algorithms}. In {\em
  Proceedings of the ACM SIGPLAN Conference on Programming Language Design and
  Implementation} {\em (PLDI '11)}. 12--25.
\newblock
\showISBNx{978-1-4503-0663-8}
\showDOI{\url{http://dx.doi.org/10.1145/1993498.1993501}}


\bibitem[\protect\citeauthoryear{Polato, Ré, Goldman, and Kon}{Polato
  et~al\mbox{.}}{2014}]{Polato2014}
{Ivanilton Polato}, {Reginaldo Ré}, {Alfredo Goldman}, {and} {Fabio Kon}.
  2014.
\newblock \showarticletitle{A comprehensive view of Hadoop research -- A
  systematic literature review}.
\newblock {\em Journal of Network and Computer Applications\/} {46}, 0 (2014),
  1 -- 25.
\newblock
\showISSN{1084-8045}
\showDOI{\url{http://dx.doi.org/10.1016/j.jnca.2014.07.022}}


\bibitem[\protect\citeauthoryear{Power and Li}{Power and Li}{2010}]{power2010}
{Russell Power} {and} {Jinyang Li}. 2010.
\newblock \showarticletitle{Piccolo: Building Fast, Distributed Programs with
  Partitioned Tables.}. In {\em OSDI}, Vol.~10. 1--14.
\newblock


\bibitem[\protect\citeauthoryear{Prabhakaran, Wu, Weng, McSherry, Zhou, and
  Haridasan}{Prabhakaran et~al\mbox{.}}{2012}]{Prabhakaran2012}
{Vijayan Prabhakaran}, {Ming Wu}, {Xuetian Weng}, {Frank McSherry}, {Lidong
  Zhou}, {and} {Maya Haridasan}. 2012.
\newblock \showarticletitle{Managing Large Graphs on Multi-cores with Graph
  Awareness}. In {\em Proceedings of the 2012 USENIX Conference on Annual
  Technical Conference} {\em (USENIX ATC'12)}. USENIX Association, Berkeley,
  CA, USA, 4--4.
\newblock
\showURL{\url{http://dl.acm.org/citation.cfm?id=2342821.2342825}}


\bibitem[\protect\citeauthoryear{Preis}{Preis}{1999}]{Preis1999}
{Robert Preis}. 1999.
\newblock \showarticletitle{Linear Time 1/2 -approximation Algorithm for
  Maximum Weighted Matching in General Graphs}. In {\em Proceedings of the 16th
  Annual Conference on Theoretical Aspects of Computer Science} {\em
  (STACS'99)}. Springer-Verlag, Berlin, Heidelberg, 259--269.
\newblock
\showISBNx{3-540-65691-X}
\showURL{\url{http://dl.acm.org/citation.cfm?id=1764891.1764924}}


\bibitem[\protect\citeauthoryear{Protic, Tomasevic, and Milutinovic}{Protic
  et~al\mbox{.}}{1997}]{Protic1998}
{Jelica Protic}, {Milo Tomasevic}, {and} {Veljko Milutinovic} (Eds.). 1997.
\newblock {\em Distributed Shared Memory: Concepts and Systems\/} (1st ed.).
\newblock IEEE Computer Society Press, Los Alamitos, CA, USA.
\newblock
\showISBNx{0818677376}


\bibitem[\protect\citeauthoryear{Quick, Wilkinson, and Hardcastle}{Quick
  et~al\mbox{.}}{2012}]{Quick2012}
{Louise Quick}, {Paul Wilkinson}, {and} {David Hardcastle}. 2012.
\newblock \showarticletitle{Using Pregel-like Large Scale Graph Processing
  Frameworks for Social Network Analysis}. In {\em Proceedings of the 2012
  International Conference on Advances in Social Networks Analysis and Mining
  (ASONAM 2012)} {\em (ASONAM '12)}. IEEE Computer Society, Washington, DC,
  USA, 457--463.
\newblock
\showISBNx{978-0-7695-4799-2}
\showDOI{\url{http://dx.doi.org/10.1109/ASONAM.2012.254}}


\bibitem[\protect\citeauthoryear{Raghavan, Albert, and Kumara}{Raghavan
  et~al\mbox{.}}{2007}]{Raghavan2007}
{Usha~Nandini Raghavan}, {R{\'e}ka Albert}, {and} {Soundar Kumara}. 2007.
\newblock \showarticletitle{Near linear time algorithm to detect community
  structures in large-scale networks}.
\newblock {\em Physical Review E\/} {76}, 3 (2007), 036106.
\newblock
\showURL{\url{http://dx.doi.org/10.1103/PhysRevE.76.036106}}


\bibitem[\protect\citeauthoryear{Rahimian, Payberah, Girdzijauskas, and
  Haridi}{Rahimian et~al\mbox{.}}{2014}]{Rahimian2014}
{Fatemeh Rahimian}, {AmirH. Payberah}, {Sarunas Girdzijauskas}, {and} {Seif
  Haridi}. 2014.
\newblock \showarticletitle{Distributed Vertex-Cut Partitioning}.
\newblock In {\em Distributed Applications and Interoperable Systems}, {Kostas
  Magoutis} {and} {Peter Pietzuch} (Eds.). Springer Berlin Heidelberg,
  186--200.
\newblock
\showISBNx{978-3-662-43351-5}
\showDOI{\url{http://dx.doi.org/10.1007/978-3-662-43352-2_15}}


\bibitem[\protect\citeauthoryear{Rahimian, Payberah, Girdzijauskas, Jelasity,
  and Haridi}{Rahimian et~al\mbox{.}}{2013}]{Rahimian2013}
{Fatemeh Rahimian}, {Amir~H. Payberah}, {Sarunas Girdzijauskas}, {Mark
  Jelasity}, {and} {Seif Haridi}. 2013.
\newblock \showarticletitle{JA-BE-JA: A Distributed Algorithm for Balanced
  Graph Partitioning}. In {\em Proceedings of the 2013 IEEE 7th International
  Conference on Self-Adaptive and Self-Organizing Systems} {\em (SASO '13)}.
  IEEE Computer Society, Washington, DC, USA, 51--60.
\newblock
\showISBNx{978-0-7695-5129-6}
\showDOI{\url{http://dx.doi.org/10.1109/SASO.2013.13}}


\bibitem[\protect\citeauthoryear{Ramaswamy, Gedik, and Liu}{Ramaswamy
  et~al\mbox{.}}{2005}]{Ramaswamy2005}
{Lakshmish Ramaswamy}, {Bugra Gedik}, {and} {Ling Liu}. 2005.
\newblock \showarticletitle{A Distributed Approach to Node Clustering in
  Decentralized Peer-to-Peer Networks}.
\newblock {\em IEEE Trans. Parallel Distrib. Syst.\/} {16}, 9 (Sept. 2005),
  814--829.
\newblock
\showISSN{1045-9219}
\showDOI{\url{http://dx.doi.org/10.1109/TPDS.2005.101}}


\bibitem[\protect\citeauthoryear{Redekopp, Simmhan, and Prasanna}{Redekopp
  et~al\mbox{.}}{2013}]{Redekopp2013}
{Mark Redekopp}, {Yogesh Simmhan}, {and} {Viktor~K. Prasanna}. 2013.
\newblock \showarticletitle{Optimizations and Analysis of BSP Graph Processing
  Models on Public Clouds}. In {\em Proceedings of the 2013 IEEE 27th
  International Symposium on Parallel and Distributed Processing} {\em (IPDPS
  '13)}. IEEE Computer Society, Washington, DC, USA, 203--214.
\newblock
\showISBNx{978-0-7695-4971-2}
\showDOI{\url{http://dx.doi.org/10.1109/IPDPS.2013.76}}


\bibitem[\protect\citeauthoryear{Roy, Mihailovic, and Zwaenepoel}{Roy
  et~al\mbox{.}}{2013}]{Roy2013}
{Amitabha Roy}, {Ivo Mihailovic}, {and} {Willy Zwaenepoel}. 2013.
\newblock \showarticletitle{X-Stream: Edge-centric Graph Processing Using
  Streaming Partitions}. In {\em Proceedings of the Twenty-Fourth ACM Symposium
  on Operating Systems Principles} {\em (SOSP '13)}. ACM, New York, NY, USA,
  472--488.
\newblock
\showISBNx{978-1-4503-2388-8}
\showDOI{\url{http://dx.doi.org/10.1145/2517349.2522740}}


\bibitem[\protect\citeauthoryear{Sakr}{Sakr}{2013}]{Sakr2013}
{Sherif Sakr}. 2013.
\newblock \showarticletitle{Processing large-scale graph data: A guide to
  current technology}.
\newblock {\em IBM Developerworks\/} (jun 2013), 15.
\newblock


\bibitem[\protect\citeauthoryear{Salihoglu and Widom}{Salihoglu and
  Widom}{2013}]{Salihoglu2013}
{Semih Salihoglu} {and} {Jennifer Widom}. 2013.
\newblock \showarticletitle{GPS: A Graph Processing System}. In {\em
  Proceedings of the 25th International Conference on Scientific and
  Statistical Database Management} {\em (SSDBM)}. ACM, New York, NY, USA,
  Article 22, 12 pages.
\newblock
\showISBNx{978-1-4503-1921-8}
\showDOI{\url{http://dx.doi.org/10.1145/2484838.2484843}}


\bibitem[\protect\citeauthoryear{Salihoglu and Widom}{Salihoglu and
  Widom}{2014}]{Salihoglu2014}
{Semih Salihoglu} {and} {Jennifer Widom}. 2014.
\newblock {\em Optimizing graph algorithms on pregel-like systems}.
\newblock {T}echnical {R}eport. Stanford InfoLab.
\newblock


\bibitem[\protect\citeauthoryear{Satish, Sundaram, Patwary, Seo, Park, Hassaan,
  Sengupta, Yin, and Dubey}{Satish et~al\mbox{.}}{2014}]{Satish}
{Nadathur Satish}, {Narayanan Sundaram}, {Md. Mostofa~Ali Patwary}, {Jiwon
  Seo}, {Jongsoo Park}, {M.~Amber Hassaan}, {Shubho Sengupta}, {Zhaoming Yin},
  {and} {Pradeep Dubey}. 2014.
\newblock \showarticletitle{Navigating the Maze of Graph Analytics Frameworks
  Using Massive Graph Datasets}.
\newblock  (2014), 979--990.
\newblock
\showISBNx{978-1-4503-2376-5}
\showDOI{\url{http://dx.doi.org/10.1145/2588555.2610518}}


\bibitem[\protect\citeauthoryear{Seo, Yoon, Kim, Jin, Kim, and Maeng}{Seo
  et~al\mbox{.}}{2010}]{Seo2010}
{Sangwon Seo}, {Edward~J. Yoon}, {Jaehong Kim}, {Seongwook Jin}, {Jin-Soo Kim},
  {and} {Seungryoul Maeng}. 2010.
\newblock \showarticletitle{HAMA: An Efficient Matrix Computation with the
  MapReduce Framework}. In {\em Proceedings of the 2010 IEEE Second
  International Conference on Cloud Computing Technology and Science} {\em
  (CLOUDCOM '10)}. IEEE Computer Society, Washington, DC, USA, 721--726.
\newblock
\showISBNx{978-0-7695-4302-4}
\showDOI{\url{http://dx.doi.org/10.1109/CloudCom.2010.17}}


\bibitem[\protect\citeauthoryear{Sevim, Kutucu, and Berberler}{Sevim
  et~al\mbox{.}}{2012}]{Sevim}
{Tina~Beseri Sevim}, {Hakan Kutucu}, {and} {Murat~Ersen Berberler}. 2012.
\newblock \showarticletitle{New mathematical model for finding minimum vertex
  cut set}. In {\em Problems of Cybernetics and Informatics (PCI), 2012 IV
  International Conference}. 1--2.
\newblock
\showDOI{\url{http://dx.doi.org/10.1109/ICPCI.2012.6486469}}


\bibitem[\protect\citeauthoryear{Shang and Yu}{Shang and Yu}{2013}]{Shang2013}
{Zechao Shang} {and} {Jeffrey~Xu Yu}. 2013.
\newblock \showarticletitle{Catch the Wind: Graph Workload Balancing on Cloud}.
  In {\em Proceedings of the 2013 IEEE International Conference on Data
  Engineering (ICDE 2013)} {\em (ICDE '13)}. IEEE Computer Society, Washington,
  DC, USA, 553--564.
\newblock
\showISBNx{978-1-4673-4909-3}
\showDOI{\url{http://dx.doi.org/10.1109/ICDE.2013.6544855}}


\bibitem[\protect\citeauthoryear{Shao, Wang, and Li}{Shao
  et~al\mbox{.}}{2013}]{Shao2013}
{Bin Shao}, {Haixun Wang}, {and} {Yatao Li}. 2013.
\newblock \showarticletitle{Trinity: A Distributed Graph Engine on a Memory
  Cloud}. In {\em Proceedings of the 2013 ACM SIGMOD International Conference
  on Management of Data} {\em (SIGMOD '13)}. ACM, New York, NY, USA, 505--516.
\newblock
\showISBNx{978-1-4503-2037-5}
\showDOI{\url{http://dx.doi.org/10.1145/2463676.2467799}}


\bibitem[\protect\citeauthoryear{Shen, Chen, Jagadish, Lu, Ooi, and Tudor}{Shen
  et~al\mbox{.}}{2014}]{shen-partitionbased}
{Yanyan Shen}, {Gang Chen}, {H.~V. Jagadish}, {Wei Lu}, {Beng~Chin Ooi}, {and}
  {Bogdan~Marius Tudor}. 2014.
\newblock \showarticletitle{Fast Failure Recovery in Distributed Graph
  Processing Systems}.
\newblock {\em Proc. VLDB Endow.\/} {8}, 4 (Dec. 2014), 437--448.
\newblock
\showISSN{2150-8097}
\showURL{\url{http://dl.acm.org/citation.cfm?id=2735496.2735506}}


\bibitem[\protect\citeauthoryear{Shun and Blelloch}{Shun and Blelloch}{2013}]{Shun2013}
{Julian Shun} {and} {Guy~E. Blelloch}. 2013.
\newblock \showarticletitle{Ligra: A Lightweight Graph Processing Framework for
  Shared Memory}. In {\em Proceedings of the 18th ACM SIGPLAN Symposium on
  Principles and Practice of Parallel Programming} {\em (PPoPP '13)}. ACM, New
  York, NY, USA, 135--146.
\newblock
\showISBNx{978-1-4503-1922-5}
\showDOI{\url{http://dx.doi.org/10.1145/2442516.2442530}}


\bibitem[\protect\citeauthoryear{Shun, Dhulipala, and Blelloch}{Shun
  et~al\mbox{.}}{2015}]{shun2015}
{Julian Shun}, {Laxman Dhulipala}, {and} {Guy Blelloch}. 2015.
\newblock \showarticletitle{Smaller and Faster: Parallel Processing of
  Compressed Graphs with Ligra+}. In {\em Proceedings of the IEEE Data
  Compression Conference (DCC)}.
\newblock


\bibitem[\protect\citeauthoryear{Simmhan, Kumbhare, Wickramaarachchi, Nagarkar,
  Ravi, Raghavendra, and Prasanna}{Simmhan et~al\mbox{.}}{2014}]{Simmhan2013}
{Yogesh Simmhan}, {Alok Kumbhare}, {Charith Wickramaarachchi}, {Soonil
  Nagarkar}, {Santosh Ravi}, {Cauligi Raghavendra}, {and} {Viktor Prasanna}.
  2014.
\newblock \showarticletitle{GoFFish: A Sub-graph Centric Framework for
  Large-Scale Graph Analytics}.
\newblock In {\em Euro-Par 2014 Parallel Processing}, {Fernando Silva}, {Inês
  Dutra}, {and} {Vítor Santos~Costa} (Eds.). Lecture Notes in Computer
  Science, Vol. 8632. Springer International Publishing, 451--462.
\newblock
\showISBNx{978-3-319-09872-2}
\showDOI{\url{http://dx.doi.org/10.1007/978-3-319-09873-9_38}}


\bibitem[\protect\citeauthoryear{Slota, Madduri, and Rajamanickam}{Slota
  et~al\mbox{.}}{2014}]{Madduri}
{George~M. Slota}, {Kamesh Madduri}, {and} {Sivasankaran Rajamanickam}. 2014.
\newblock \showarticletitle{PULP: Scalable Multi-Objective Multi-Constraint
  Partitioning for Small-World Networks}.
\newblock  (2014).
\newblock


\bibitem[\protect\citeauthoryear{Stanton}{Stanton}{2014}]{Stanton2014}
{Isabelle Stanton}. 2014.
\newblock \showarticletitle{Streaming Balanced Graph Partitioning Algorithms
  for Random Graphs}. In {\em Proceedings of the Twenty-Fifth Annual ACM-SIAM
  Symposium on Discrete Algorithms} {\em (SODA '14)}. SIAM, 1287--1301.
\newblock
\showISBNx{978-1-611973-38-9}
\showURL{\url{http://dl.acm.org/citation.cfm?id=2634074.2634169}}


\bibitem[\protect\citeauthoryear{Stanton and Kliot}{Stanton and Kliot}{2012}]{Stanton2012}
{Isabelle Stanton} {and} {Gabriel Kliot}. 2012.
\newblock \showarticletitle{Streaming Graph Partitioning for Large Distributed
  Graphs}. In {\em Proceedings of the 18th ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining} {\em (KDD '12)}. ACM, New York, NY,
  USA, 1222--1230.
\newblock
\showISBNx{978-1-4503-1462-6}
\showDOI{\url{http://dx.doi.org/10.1145/2339530.2339722}}


\bibitem[\protect\citeauthoryear{Stutz, Bernstein, and Cohen}{Stutz
  et~al\mbox{.}}{2010}]{Stutz2010}
{Philip Stutz}, {Abraham Bernstein}, {and} {William Cohen}. 2010.
\newblock \showarticletitle{Signal/Collect: Graph Algorithms for the (Semantic)
  Web}.
\newblock In {\em Proceedings of the 9th International Semantic Web Conference
  on The Semantic Web - Volume Part I}. Springer-Verlag, Berlin, Heidelberg,
  764--780.
\newblock
\showISBNx{3-642-17745-X, 978-3-642-17745-3}
\showURL{\url{http://dl.acm.org/citation.cfm?id=1940281.1940330}}


\bibitem[\protect\citeauthoryear{Suri and Vassilvitskii}{Suri and
  Vassilvitskii}{2011}]{Suri2011}
{Siddharth Suri} {and} {Sergei Vassilvitskii}. 2011.
\newblock \showarticletitle{Counting Triangles and the Curse of the Last
  Reducer}. In {\em Proceedings of the 20th International Conference on World
  Wide Web} {\em (WWW '11)}. ACM, New York, NY, USA, 607--614.
\newblock
\showISBNx{978-1-4503-0632-4}
\showDOI{\url{http://dx.doi.org/10.1145/1963405.1963491}}


\bibitem[\protect\citeauthoryear{Tasci and Demirbas}{Tasci and
  Demirbas}{2013}]{Tasci2013}
{Serafettin Tasci} {and} {Murat Demirbas}. 2013.
\newblock \showarticletitle{Giraphx: Parallel Yet Serializable Large-scale
  Graph Processing}.
\newblock In {\em Proceedings of the 19th International Conference on Parallel
  Processing}. Springer-Verlag, Berlin, Heidelberg, 458--469.
\newblock
\showISBNx{978-3-642-40046-9}
\showDOI{\url{http://dx.doi.org/10.1007/978-3-642-40047-6_47}}


\bibitem[\protect\citeauthoryear{Tian, Balmin, Corsten, Tatikonda, and
  McPherson}{Tian et~al\mbox{.}}{2013}]{Tian2013}
{Yuanyuan Tian}, {Andrey Balmin}, {Severin~Andreas Corsten}, {Shirish
  Tatikonda}, {and} {John McPherson}. 2013.
\newblock \showarticletitle{From ``think like a vertex'' to ``think like a
  graph''}.
\newblock {\em Proceedings of the VLDB Endowment\/} {7}, 3 (2013).
\newblock


\bibitem[\protect\citeauthoryear{Tsourakakis, Bonchi, Gionis, Gullo, and
  Tsiarli}{Tsourakakis et~al\mbox{.}}{2013}]{Tsourakakis2013}
{Charalampos Tsourakakis}, {Francesco Bonchi}, {Aristides Gionis}, {Francesco
  Gullo}, {and} {Maria Tsiarli}. 2013.
\newblock \showarticletitle{Denser Than the Densest Subgraph: Extracting
  Optimal Quasi-cliques with Quality Guarantees}. In {\em Proceedings of the
  19th ACM SIGKDD International Conference on Knowledge Discovery and Data
  Mining} {\em (KDD '13)}. ACM, New York, NY, USA, 104--112.
\newblock
\showISBNx{978-1-4503-2174-7}
\showDOI{\url{http://dx.doi.org/10.1145/2487575.2487645}}


\bibitem[\protect\citeauthoryear{Tsourakakis, Gkantsidis, Radunovic, and
  Vojnovic}{Tsourakakis et~al\mbox{.}}{2014}]{Tsourakakis2014}
{Charalampos Tsourakakis}, {Christos Gkantsidis}, {Bozidar Radunovic}, {and}
  {Milan Vojnovic}. 2014.
\newblock \showarticletitle{FENNEL: Streaming Graph Partitioning for Massive
  Scale Graphs}. In {\em Proceedings of the 7th ACM International Conference on
  Web Search and Data Mining} {\em (WSDM '14)}. ACM, New York, NY, USA,
  333--342.
\newblock
\showISBNx{978-1-4503-2351-2}
\showDOI{\url{http://dx.doi.org/10.1145/2556195.2556213}}


\bibitem[\protect\citeauthoryear{Ugander and Backstrom}{Ugander and
  Backstrom}{2013}]{Ugander2013}
{Johan Ugander} {and} {Lars Backstrom}. 2013.
\newblock \showarticletitle{Balanced Label Propagation for Partitioning Massive
  Graphs}. In {\em Proceedings of the Sixth ACM International Conference on Web
  Search and Data Mining} {\em (WSDM '13)}. ACM, New York, NY, USA, 507--516.
\newblock
\showISBNx{978-1-4503-1869-3}
\showDOI{\url{http://dx.doi.org/10.1145/2433396.2433461}}


\bibitem[\protect\citeauthoryear{Valiant}{Valiant}{1990}]{Valiant1990}
{Leslie~G. Valiant}. 1990.
\newblock \showarticletitle{A Bridging Model for Parallel Computation}.
\newblock {\em Commun. ACM\/} {33}, 8 (Aug. 1990), 103--111.
\newblock
\showISSN{0001-0782}
\showDOI{\url{http://dx.doi.org/10.1145/79173.79181}}


\bibitem[\protect\citeauthoryear{Vaquero, Cuadrado, Logothetis, and
  Martella}{Vaquero et~al\mbox{.}}{2013}]{Vaquero2013a}
{Luis Vaquero}, {F{\'e}lix Cuadrado}, {Dionysios Logothetis}, {and} {Claudio
  Martella}. 2013.
\newblock \showarticletitle{xdgp: A dynamic graph processing system with
  adaptive partitioning}.
\newblock {\em arXiv preprint arXiv:1309.1049\/} (2013).
\newblock
\showURL{\url{http://arxiv.org/abs/1309.1049}}


\bibitem[\protect\citeauthoryear{Vaquero, Cuadrado, and Ripeanu}{Vaquero
  et~al\mbox{.}}{2014}]{Vaquero2014}
{Luis Vaquero}, {Felix Cuadrado}, {and} {Matei Ripeanu}. 2014.
\newblock \showarticletitle{Systems for near real-time analysis of large-scale
  dynamic graphs}.
\newblock {\em arXiv preprint arXiv:1410.1903\/} (2014).
\newblock
\showURL{\url{http://arxiv.org/abs/1410.1903}}


\bibitem[\protect\citeauthoryear{Vaquero, Cuadrado, Logothetis, and
  Martella}{Vaquero et~al\mbox{.}}{2014}]{Vaquero2013}
{Luis~M. Vaquero}, {Felix Cuadrado}, {Dionysios Logothetis}, {and} {Claudio
  Martella}. 2014.
\newblock \showarticletitle{Adaptive Partitioning for Large-Scale Dynamic
  Graphs}. In {\em Proceedings of the 2014 IEEE 34th International Conference
  on Distributed Computing Systems} {\em (ICDCS '14)}. IEEE Computer Society,
  Washington, DC, USA, 144--153.
\newblock
\showISBNx{978-1-4799-5169-7}
\showDOI{\url{http://dx.doi.org/10.1109/ICDCS.2014.23}}


\bibitem[\protect\citeauthoryear{von Eicken, Culler, Goldstein, and
  Schauser}{von Eicken et~al\mbox{.}}{1992}]{VonEicken1992}
{Thorsten von Eicken}, {David~E. Culler}, {Seth~Copen Goldstein}, {and}
  {Klaus~Erik Schauser}. 1992.
\newblock \showarticletitle{Active Messages: A Mechanism for Integrated
  Communication and Computation}. In {\em Proceedings of the 19th Annual
  International Symposium on Computer Architecture} {\em (ISCA '92)}. ACM, New
  York, NY, USA, 256--266.
\newblock
\showISBNx{0-89791-509-7}
\showDOI{\url{http://dx.doi.org/10.1145/139669.140382}}


\bibitem[\protect\citeauthoryear{Wang, Xie, Demers, and Gehrke}{Wang
  et~al\mbox{.}}{2013}]{Wang2013}
{Guozhang Wang}, {Wenlei Xie}, {Alan~J Demers}, {and} {Johannes Gehrke}. 2013.
\newblock \showarticletitle{Asynchronous Large-Scale Graph Processing Made
  Easy.}. In {\em Proceedings of the 6th Biennial Conference on Innovative Data
  Systems Research} {\em (CIDR'13)}.
\newblock


\bibitem[\protect\citeauthoryear{Wang, Xiao, Shao, and Wang}{Wang
  et~al\mbox{.}}{2014}]{Wang2014}
{Lu Wang}, {Yanghua Xiao}, {Bin Shao}, {and} {Haixun Wang}. 2014.
\newblock \showarticletitle{How to partition a billion-node graph}. In {\em
  Data Engineering (ICDE), 2014 IEEE 30th International Conference on}.
  568--579.
\newblock
\showDOI{\url{http://dx.doi.org/10.1109/ICDE.2014.6816682}}


\bibitem[\protect\citeauthoryear{Wang, Zhang, Chen, Chen, and Guan}{Wang
  et~al\mbox{.}}{2014}]{wang-replication}
{Peng Wang}, {Kaiyuan Zhang}, {Rong Chen}, {Haibo Chen}, {and} {Haibing Guan}.
  2014.
\newblock \showarticletitle{Replication-Based Fault-Tolerance for Large-Scale
  Graph Processing}. In {\em Dependable Systems and Networks (DSN), 2014 44th
  Annual IEEE/IFIP International Conference on}. 562--573.
\newblock
\showDOI{\url{http://dx.doi.org/10.1109/DSN.2014.58}}


\bibitem[\protect\citeauthoryear{Wang and Chiu}{Wang and Chiu}{2013}]{Wang2013a}
{Rui Wang} {and} {K. Chiu}. 2013.
\newblock \showarticletitle{A stream partitioning approach to processing large
  scale distributed graph datasets}. In {\em Big Data, 2013 IEEE International
  Conference on}. 537--542.
\newblock
\showDOI{\url{http://dx.doi.org/10.1109/BigData.2013.6691619}}


\bibitem[\protect\citeauthoryear{Webber}{Webber}{2012}]{Webber2012}
{Jim Webber}. 2012.
\newblock \showarticletitle{A Programmatic Introduction to Neo4J}. In {\em
  Proceedings of the 3rd Annual Conference on Systems, Programming, and
  Applications: Software for Humanity} {\em (SPLASH '12)}. ACM, New York, NY,
  USA, 217--218.
\newblock
\showISBNx{978-1-4503-1563-0}
\showDOI{\url{http://dx.doi.org/10.1145/2384716.2384777}}


\bibitem[\protect\citeauthoryear{Willcock, Hoefler, Edmonds, and
  Lumsdaine}{Willcock et~al\mbox{.}}{2011}]{Willcock2011}
{Jeremiah~James Willcock}, {Torsten Hoefler}, {Nicholas~Gerard Edmonds}, {and}
  {Andrew Lumsdaine}. 2011.
\newblock \showarticletitle{Active Pebbles: Parallel Programming for
  Data-driven Applications}. In {\em Proceedings of the International
  Conference on Supercomputing} {\em (ICS '11)}. ACM, New York, NY, USA,
  235--244.
\newblock
\showISBNx{978-1-4503-0102-2}
\showDOI{\url{http://dx.doi.org/10.1145/1995896.1995934}}


\bibitem[\protect\citeauthoryear{Xie, Chen, Guan, Zang, and Chen}{Xie
  et~al\mbox{.}}{2015}]{Xie2013}
{C Xie}, {R Chen}, {H Guan}, {B Zang}, {and} {H Chen}. 2015.
\newblock \showarticletitle{Sync or async: Time to fuse for distributed
  graph-parallel computation}. In {\em Proceedings of 20th ACM SIGPLAN
  Symposium on Principles and Practice of Parallel Programming}.
\newblock


\bibitem[\protect\citeauthoryear{Xie, Yan, Li, and Zhang}{Xie
  et~al\mbox{.}}{2014}]{Xie2014}
{Cong Xie}, {Ling Yan}, {Wu-Jun Li}, {and} {Zhihua Zhang}. 2014.
\newblock \showarticletitle{Distributed Power-law Graph Computing: Theoretical
  and Empirical Analysis}. In {\em Advances in Neural Information Processing
  Systems 27}, {Z.~Ghahramani}, {M.~Welling}, {C.~Cortes}, {N.D. Lawrence},
  {and} {K.Q. Weinberger} (Eds.). Curran Associates, Inc., 1673--1681.
\newblock
\showURL{\url{http://papers.nips.cc/paper/5396-distributed-power-law-graph-computing-theoretical-and-empirical-analysis.pdf}}


\bibitem[\protect\citeauthoryear{Xie, Wang, Bindel, Demers, and Gehrke}{Xie
  et~al\mbox{.}}{2013}]{Xie2013a}
{Wenlei Xie}, {Guozhang Wang}, {David Bindel}, {Alan Demers}, {and} {Johannes
  Gehrke}. 2013.
\newblock \showarticletitle{Fast Iterative Graph Computation with Block
  Updates}.
\newblock {\em Proc. VLDB Endow.\/} {6}, 14 (Sept. 2013), 2014--2025.
\newblock
\showISSN{2150-8097}
\showDOI{\url{http://dx.doi.org/10.14778/2556549.2556581}}


\bibitem[\protect\citeauthoryear{Xu, Chen, and Cui}{Xu et~al\mbox{.}}{2014}]{Xu2013}
{Ning Xu}, {Lei Chen}, {and} {Bin Cui}. 2014.
\newblock \showarticletitle{LogGP: A Log-based Dynamic Graph Partitioning
  Method}.
\newblock {\em Proceedings of the VLDB Endowment\/} {7}, 14 (2014).
\newblock


\bibitem[\protect\citeauthoryear{Yan, Cheng, Lu, and Ng}{Yan
  et~al\mbox{.}}{2014a}]{Yan2014}
{Da Yan}, {James Cheng}, {Yi Lu}, {and} {Wilfred Ng}. 2014a.
\newblock \showarticletitle{Blogel: A block-centric framework for distributed
  computation on real-world graphs}.
\newblock {\em Proceedings of the VLDB Endowment\/} {7}, 14 (2014).
\newblock


\bibitem[\protect\citeauthoryear{Yan, Cheng, Xing, Lu, Ng, and Bu}{Yan
  et~al\mbox{.}}{2014b}]{Yan2013}
{Da Yan}, {James Cheng}, {Kai Xing}, {Li Lu}, {Wilfred Ng}, {and} {Yingyi Bu}.
  2014b.
\newblock \showarticletitle{Pregel Algorithms for Graph Connectivity Problems
  with Performance Guarantees}. In {\em Proceedings of the VLDB Endowment},
  Vol.~7.
\newblock


\bibitem[\protect\citeauthoryear{Yoneki and Roy}{Yoneki and Roy}{2013}]{Yoneki2013}
{Eiko Yoneki} {and} {Amitabha Roy}. 2013.
\newblock \showarticletitle{Scale-up Graph Processing: A Storage-centric View}.
  In {\em First International Workshop on Graph Data Management Experiences and
  Systems} {\em (GRADES '13)}. ACM, New York, NY, USA, Article 8, 6 pages.
\newblock
\showISBNx{978-1-4503-2188-4}
\showDOI{\url{http://dx.doi.org/10.1145/2484425.2484433}}


\bibitem[\protect\citeauthoryear{Yuan, Zhang, Xie, Jin, Liu, and Lee}{Yuan
  et~al\mbox{.}}{2014}]{Yuan}
{Pingpeng Yuan}, {Wenya Zhang}, {Changfeng Xie}, {Hai Jin}, {Ling Liu}, {and}
  {Kisung Lee}. 2014.
\newblock \showarticletitle{Fast Iterative Graph Computation: A Path Centric
  Approach}.
\newblock  (2014), 401--412.
\newblock
\showISBNx{978-1-4799-5500-8}
\showDOI{\url{http://dx.doi.org/10.1109/SC.2014.38}}


\bibitem[\protect\citeauthoryear{Zaharia, Chowdhury, Franklin, Shenker, and
  Stoica}{Zaharia et~al\mbox{.}}{2010}]{Zaharia2010}
{Matei Zaharia}, {Mosharaf Chowdhury}, {Michael~J. Franklin}, {Scott Shenker},
  {and} {Ion Stoica}. 2010.
\newblock \showarticletitle{Spark: Cluster Computing with Working Sets}. In
  {\em Proceedings of the 2Nd USENIX Conference on Hot Topics in Cloud
  Computing} {\em (HotCloud'10)}. USENIX Association, Berkeley, CA, USA,
  10--10.
\newblock
\showURL{\url{http://dl.acm.org/citation.cfm?id=1863103.1863113}}


\bibitem[\protect\citeauthoryear{Zeng, Wu, and Wang}{Zeng
  et~al\mbox{.}}{2012}]{Zeng2012}
{ZengFeng Zeng}, {Bin Wu}, {and} {Haoyu Wang}. 2012.
\newblock \showarticletitle{A Parallel Graph Partitioning Algorithm to Speed Up
  the Large-scale Distributed Graph Mining}. In {\em Proceedings of the 1st
  International Workshop on Big Data, Streams and Heterogeneous Source Mining:
  Algorithms, Systems, Programming Models and Applications} {\em (BigMine
  '12)}. ACM, New York, NY, USA, 61--68.
\newblock
\showISBNx{978-1-4503-1547-0}
\showDOI{\url{http://dx.doi.org/10.1145/2351316.2351325}}


\bibitem[\protect\citeauthoryear{Zhang, Chen, and Chen}{Zhang
  et~al\mbox{.}}{2015}]{Zhang2015}
{Kaiyuan Zhang}, {Rong Chen}, {and} {Haibo Chen}. 2015.
\newblock \showarticletitle{NUMA-aware Graph-structured Analytics}. In {\em
  Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of
  Parallel Programming} {\em (PPoPP 2015)}. ACM, New York, NY, USA, 183--193.
\newblock
\showISBNx{978-1-4503-3205-7}
\showDOI{\url{http://dx.doi.org/10.1145/2688500.2688507}}


\bibitem[\protect\citeauthoryear{Zhang, Gao, Gao, and Wang}{Zhang
  et~al\mbox{.}}{2012a}]{Zhang2013}
{Yanfeng Zhang}, {Qixin Gao}, {Lixin Gao}, {and} {Cuirong Wang}. 2012a.
\newblock \showarticletitle{Accelerate Large-scale Iterative Computation
  Through Asynchronous Accumulative Updates}.
\newblock  (2012), 13--22.
\newblock
\showISBNx{978-1-4503-1340-7}
\showDOI{\url{http://dx.doi.org/10.1145/2287036.2287041}}


\bibitem[\protect\citeauthoryear{Zhang, Gao, Gao, and Wang}{Zhang
  et~al\mbox{.}}{2012b}]{Zhang2012}
{Yanfeng Zhang}, {Qixin Gao}, {Lixin Gao}, {and} {Cuirong Wang}. 2012b.
\newblock \showarticletitle{iMapReduce: A Distributed Computing Framework for
  Iterative Computation}.
\newblock {\em J. Grid Comput.\/} {10}, 1 (March 2012), 47--68.
\newblock
\showISSN{1570-7873}
\showDOI{\url{http://dx.doi.org/10.1007/s10723-012-9204-9}}


\bibitem[\protect\citeauthoryear{Zhang, Gao, Gao, and Wang}{Zhang
  et~al\mbox{.}}{2013}]{Zhang2011}
{Yanfeng Zhang}, {Qixin Gao}, {Lixin Gao}, {and} {Cuirong Wang}. 2013.
\newblock \showarticletitle{PrIter: A Distributed Framework for Prioritizing
  Iterative Computations}. {\em IEEE Trans. Parallel Distrib. Syst.\/} {24}, 9
  (Sept. 2013), 1884--1893.
\newblock
\showISSN{1045-9219}
\showDOI{\url{http://dx.doi.org/10.1109/TPDS.2012.272}}


\bibitem[\protect\citeauthoryear{Zhao, Yoshigoe, Xie, Zhou, Seker, and
  Bian}{Zhao et~al\mbox{.}}{2014}]{Zhao2014}
{Yue Zhao}, {Kenji Yoshigoe}, {Mengjun Xie}, {Suijian Zhou}, {Remzi Seker},
  {and} {Jiang Bian}. 2014.
\newblock \showarticletitle{LightGraph: Lighten Communication in Distributed
  Graph-Parallel Processing}. In {\em Proceedings of the 2014 IEEE
  International Congress on Big Data} {\em (BIGDATACONGRESS '14)}. IEEE
  Computer Society, Washington, DC, USA, 717--724.
\newblock
\showISBNx{978-1-4799-5057-7}
\showDOI{\url{http://dx.doi.org/10.1109/BigData.Congress.2014.106}}


\bibitem[\protect\citeauthoryear{Zheng, Mhembere, Burns, Vogelstein, Priebe,
  and Szalay}{Zheng et~al\mbox{.}}{2015}]{Zheng2015}
{Da Zheng}, {Disa Mhembere}, {Randal Burns}, {Joshua Vogelstein}, {Carey~E.
  Priebe}, {and} {Alexander~S. Szalay}. 2015.
\newblock \showarticletitle{FlashGraph: Processing Billion-node Graphs on an
  Array of Commodity SSDs}. In {\em Proceedings of the 13th USENIX Conference
  on File and Storage Technologies} {\em (FAST'15)}. USENIX Association,
  Berkeley, CA, USA, 45--58.
\newblock
\showISBNx{978-1-931971-201}
\showURL{\url{http://dl.acm.org/citation.cfm?id=2750482.2750486}}


\bibitem[\protect\citeauthoryear{Zhou, Chang, and Chen}{Zhou
  et~al\mbox{.}}{2014}]{Zhou2014}
{Xianke Zhou}, {Pengfei Chang}, {and} {Gang Chen}. 2014.
\newblock \showarticletitle{An Efficient Graph Processing System}.
\newblock In {\em Web Technologies and Applications}, {Lei Chen}, {Yan Jia},
  {Timos Sellis}, {and} {Guanfeng Liu} (Eds.). Lecture Notes in Computer
  Science, Vol. 8709. Springer International Publishing, 401--412.
\newblock
\showISBNx{978-3-319-11115-5}
\showDOI{\url{http://dx.doi.org/10.1007/978-3-319-11116-2_35}}


\bibitem[\protect\citeauthoryear{Zhou, Wilkinson, Schreiber, and Pan}{Zhou
  et~al\mbox{.}}{2008}]{Zhou2008}
{Yunhong Zhou}, {Dennis Wilkinson}, {Robert Schreiber}, {and} {Rong Pan}. 2008.
\newblock \showarticletitle{Large-Scale Parallel Collaborative Filtering for
  the Netflix Prize}.
\newblock In {\em Proceedings of the 4th International Conference on
  Algorithmic Aspects in Information and Management}. Springer-Verlag, Berlin,
  Heidelberg, 337--348.
\newblock
\showISBNx{978-3-540-68865-5}
\showDOI{\url{http://dx.doi.org/10.1007/978-3-540-68880-8_32}}


\end{thebibliography}

\end{document}