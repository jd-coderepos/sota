

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{booktabs} 



\usepackage{comment}
\usepackage{amsmath}
\def \submission {}
\usepackage{color}
\usepackage{ulem}
\usepackage{multirow}
\usepackage{float}
\usepackage{caption}
\usepackage{array}
\usepackage{arydshln}
\usepackage{adjustbox}
\renewcommand{\arraystretch}{1.3}
\normalem


\definecolor{Crimson}{rgb}{0.86, 0.08, 0.24}
\definecolor{DarkGreen}{rgb}{0.00, 0.60, 0.00}
\definecolor{RoyalBlue}{rgb}{0.15, 0.25, 0.54}
\definecolor{DarkCyan}{rgb}{0.0, 0.54, 0.54}
\ifx \submission \undefined
\newcommand{\djc}[1]{\textcolor{DarkGreen}{#1}}
\newcommand{\djr}[2]{{\sout{#1}} \color{DarkGreen}{#2}}
\newcommand{\ycc}[1]{\color{blue}{#1}}
\newcommand{\ycr}[2]{{\sout{#1}} \color{RoyalBlue}{#2}}
\newcommand{\cyc}[1]{\color{DarkCyan}{#1}}
\newcommand{\cyr}[2]{{\sout{#1}} \color{DarkCyan}{#2}}
\else
\newcommand{\djc}[1]{{#1}}
\newcommand{\djr}[2]{{#2}}
\newcommand{\ycc}[1]{{#1}}
\newcommand{\ycr}[2]{{#2}}
\newcommand{\cyc}[1]{{#1}}
\newcommand{\cyr}[2]{{#2}}
\fi



\usepackage{bbm}
\usepackage{bm}

\usepackage{booktabs}       \usepackage{amsfonts}       \usepackage{nicefrac}       \usepackage{microtype}      \usepackage{color}
\usepackage{subfig}
\let\proof\relax
\let\endproof\relax
\usepackage{amsthm}
\usepackage[para,online,flushleft]{threeparttable}
\usepackage{lineno}


\newcommand{\bZero}{{\mathbf{0}}}
\newcommand{\bOne}{{\mathbf{1}}}

\newcommand{\ba}{{\mathbf{a}}}
\newcommand{\bb}{{\mathbf{b}}}
\newcommand{\bc}{{\mathbf{c}}}
\newcommand{\bd}{{\mathbf{d}}}
\newcommand{\be}{{\mathbf{e}}}
\newcommand{\bff}{{\mathbf{f}}}
\newcommand{\bg}{{\mathbf{g}}}
\newcommand{\bh}{{\mathbf{h}}}
\newcommand{\bi}{{\mathbf{i}}}
\newcommand{\bj}{{\mathbf{j}}}
\newcommand{\bk}{{\mathbf{k}}}
\newcommand{\bl}{{\mathbf{l}}}
\newcommand{\bfm}{{\mathbf{m}}}
\newcommand{\bo}{{\mathbf{o}}}
\newcommand{\bp}{{\mathbf{p}}}
\newcommand{\bq}{{\mathbf{q}}}
\newcommand{\br}{{\mathbf{r}}}
\newcommand{\bs}{{\mathbf{s}}}
\newcommand{\bu}{{\mathbf{u}}}
\newcommand{\bv}{{\mathbf{v}}}
\newcommand{\bw}{{\mathbf{w}}}
\newcommand{\bx}{{\mathbf{x}}}
\newcommand{\by}{{\mathbf{y}}}
\newcommand{\bz}{{\mathbf{z}}}

\newcommand{\bA}{{\mathbf{A}}}
\newcommand{\bB}{{\mathbf{B}}}
\newcommand{\bC}{{\mathbf{C}}}
\newcommand{\bD}{{\mathbf{D}}}
\newcommand{\bE}{{\mathbf{E}}}
\newcommand{\bF}{{\mathbf{F}}}
\newcommand{\bG}{{\mathbf{G}}}
\newcommand{\bH}{{\mathbf{H}}}
\newcommand{\bI}{{\mathbf{I}}}
\newcommand{\bJ}{{\mathbf{J}}}
\newcommand{\bK}{{\mathbf{K}}}
\newcommand{\bM}{{\mathbf{M}}}
\newcommand{\bP}{{\mathbf{P}}}
\newcommand{\bS}{{\mathbf{S}}}
\newcommand{\bV}{{\mathbf{V}}}
\newcommand{\bW}{{\mathbf{W}}}
\newcommand{\bX}{{\mathbf{X}}}
\newcommand{\bY}{{\mathbf{Y}}}
\newcommand{\bZ}{{\mathbf{Z}}}

\newcommand{\bbE}{{\mathbb{E}}}
\newcommand{\bbI}{{\mathbb{I}}}
\newcommand{\bbR}{{\mathbb{R}}}
\newcommand{\bbK}{{\mathbb{K}}}

\newcommand{\cA}{{\mathcal{A}}}
\newcommand{\cB}{{\mathcal{B}}}
\newcommand{\cD}{{\mathcal{D}}}
\newcommand{\cE}{{\mathcal{E}}}
\newcommand{\cL}{{\mathcal{L}}}
\newcommand{\cJ}{{\mathcal{J}}}
\newcommand{\cN}{{\mathcal{N}}}
\newcommand{\cO}{{\mathcal{O}}}
\newcommand{\cS}{{\mathcal{S}}}
\newcommand{\cU}{{\mathcal{U}}}
\newcommand{\cV}{{\mathcal{V}}}
\newcommand{\cW}{{\mathcal{W}}}

\newcommand{\balpha}{\bm{\alpha}}
\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\btheta}{\bm{\theta}}
\newcommand{\blambda}{\bm{\lambda}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\bepsilon}{\bm{\epsilon}}
\newcommand{\boldeta}{\bm{\eta}}
\newcommand{\btau}{\bm{\tau}}
\newcommand{\bsigma}{\bm{\sigma}}
\newcommand{\bphi}{\bm{\phi}}
\newcommand{\bSigma}{\bm{\Sigma}}
\newcommand{\bPsi}{\bm{\Psi}}

\newcommand{\etal}{\textit{et al}.}
\newcommand{\ie}{\textit{i}.\textit{e}.}
\newcommand{\eg}{\textit{e}.\textit{g}.}

\def\argmin{\mathop{\mathrm{arg}\, \mathrm{min}}\limits}
\def\argmax{\mathop{\mathrm{arg}\, \mathrm{max}}\limits}

\newcommand{\xsubseq}[2]{\ensuremath{\mathbf{x}_{#1\mathbf{:} #2}}}
\newcommand{\ysubseq}[2]{\ensuremath{\mathbf{y}_{#1\mathbf{:} #2}}}
\newcommand{\usubseq}[2]{\ensuremath{\mathbf{u}_{#1\mathbf{:} #2}}}
\newcommand{\xseq}[2]{\ensuremath{\mathbf{x}_{#1},\ldots,\mathbf{x}_{#2}}}
\newcommand{\yseq}[2]{\ensuremath{\mathbf{y}_{#1},\ldots,\mathbf{y}_{#2}}}
\newcommand{\useq}[2]{\ensuremath{\mathbf{u}_{#1},\ldots,\mathbf{u}_{#2}}}

\def\argmin{\mathop{\mathrm{arg}\, \mathrm{min}}\limits}
\def\argmax{\mathop{\mathrm{arg}\, \mathrm{max}}\limits}




\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{icml2021}

\icmltitlerunning{Submission and Formatting Instructions for ICML 2021}

\begin{document}

\twocolumn[
\icmltitle{ACE: Adaptive Confusion Energy for Natural World Data Distribution}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Yen-Chi Hsu}{iis, ntu}
\icmlauthor{Cheng-Yao Hong}{iis}
\icmlauthor{Wan-Cyuan Fan}{ntu}
\icmlauthor{Ming-Sui Lee}{ntu}
\icmlauthor{Davi Geiger}{nyu}
\icmlauthor{Tyng-Luh Liu}{iis}
\end{icmlauthorlist}

\icmlaffiliation{iis}{Academia Sinica}
\icmlaffiliation{ntu}{National Taiwan University}
\icmlaffiliation{nyu}{New York University}

\icmlcorrespondingauthor{Yen-Chi Hsu}{d06922021@csie.ntu.edu.tw}
\icmlcorrespondingauthor{Cheng-Yao Hong}{sensible@iis.sinica.edu.tw}

\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]







\begin{abstract}
With the development of deep learning, standard classification problems have achieved good results. However, conventional classification problems are often too idealistic. Most data in the natural world usually have imbalanced distribution and fine-grained characteristics. Recently, many state-of-the-art approaches tend to focus on one or another separately, but rarely on both. In this paper, we introduce a novel and adaptive batch-wise regularization based on the proposed Adaptive Confusion Energy (ACE) to flexibly address the nature world distribution, which usually involves fine-grained and long-tailed properties at the same time. ACE increases the difficulty of the training process and further alleviates the overfitting problem. Through the datasets with the technical issue in fine-grained (CUB, CAR, AIR) and long-tailed (ImageNet-LT), or comprehensive issues (CUB-LT, iNaturalist), the result shows that the ACE is not only competitive to some state-of-the-art on performance but also demonstrates the effectiveness of training.
\end{abstract}

\section{Introduction}
\label{sec:intro}


With the development of deep learning, the fundamental classification problem has been solved. Subsequent classification studies focus on two more challenging issues, fine-grained characteristics, and imbalanced data distribution. Fine-grained visual classification (FGVC) is an active and challenging problem in computer vision. Such a recognition task differs from the classical problem of large-scale visual classification (LSVC) by focusing on differentiating {\em similar} sub-categories of the same meta-category. In FGVC, the inter-class similarity among the object categories is often pervasive. The intra-class variations further impose ambiguities in learning a unified and discriminative representation for each category. Long-tailed distribution brings another aspect of the challenge that the head categories tend to dominate the training procedure. Thus, the learned classification model performs better on these categories while yielding significantly poor performance for the tail categories. The performance distribution somewhat resembles the data distribution. As the natural world distribution often assumes both fine-grained and long-tailed properties, how to satisfactorily address the recognition problem under such a general setting raises a practical and challenging issue.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.48\textwidth]{fig/F1_n.pdf}
    \caption{Data distribution is not ideal in the real world and is usually accompanied by more than one complicated issue to be solved. For example, iNaturalist 2018~\cite{van2018inaturalist} hard to learn the tailed classes due to an extreme imbalanced ratio in the \textit{long-tailed} distribution. Meanwhile, it is hard to disentangle the inter-class similarity and intra-class variation that results from \textit{fine-grained} characteristics.  Eventually, this leads to overfitting.}
    \label{fig:intro}
    \vspace{-5pt}
\end{figure}


\begin{figure*}[ht!]
    \centering
    \includegraphics[width=\textwidth]{fig/F2_n.pdf}
    \caption{Left: Different datasets have varying degrees of long-tail and fine-grained characteristics. Right: The existing approaches can only solve one aspect of the problem.}
    \label{fig:confusionLoss}
    \vspace{-10pt}
\end{figure*}

Most of the current visual classification tasks usually only have the challenge in one aspect, such as FGVC or long-tailed issues, as mentioned above. However, the data distribution is not always idealistic in the natural world. Comprehensive issues usually accompany it. For instance, Figure~\ref{fig:intro} illustrates two concurrent challenges in the iNaturalist 2018~\citep{van2018inaturalist}. First, the task is a long-tailed distribution with an extremely imbalanced ratio. Since there is a thousand-fold difference in the number of categories, it is hard to learn the tail classes' representation. Meanwhile, it is also an FGVC task where the inter-class similarity and the intra-class variations are subtly intertwined, yielding a daunting classification task, no matter what orders of magnitude of categories (frequent, common, and rare). That makes the model easy to overfit \cite{dubey2018pairwise}.

From the existing literature, there are only a few attempts to solve these two problems simultaneously. Relevant efforts mostly focus on tackling either task. In FGVC, most of the recent research efforts have converged to learn pivotal local/part details relevant to distinguishing fine-grained categories \eg, \cite{fu2017look,yang2018learning,zheng2019looking}, and typically require the fusion of several sophisticated computer vision techniques to accomplish the task such as in \cite{ge2019weakly}. In resolving the long-tailed issue, previous approaches have looked into data balanced sampling \cite{huang2016learning,wang2017learning} and the recent development such as \citet{Kang2020Decoupling} learns the representation at the first stage and refines the classifier by balanced data sampling. Different existing tasks have varying degrees of fine-grained and long-tail factor. As shown in Figure~\ref{fig:confusionLoss} (left), we leverage the maximum imbalanced ratio and the normalized feature cosine similarity between each category as the fine-grained and long-tailed factor to evaluate the characteristic of each task.

Motivated by these developments, we propose a flexible and effective regularization design that aims at guiding the resulting DNN learning to improve model efficiency on tackling the FGVC and long-tailed issues at the same time. Our method is relevant to the {\it pairwise confusion} (PC) \cite{dubey2018pairwise}.  PC only makes an image confuse with another image, but it does not use the rest sufficiently. Furthermore, while PC encounters imbalanced problems, it offers few improvements. The Figure~\ref{fig:confusionLoss} (right) shows that those approaches can alleviate the overfitting issue in FGVC fail in overcoming the imbalanced issue and vice versa. In this paper, the proposed formulation goes beyond the restriction of working on pairs of data and develops a batch norm-based framework with sufficient model capacity to deal with  FGVC and long-tailed issues simultaneously. We first assume all samples/images within a batch are of different classes. A batch-wise matrix norm then models the targeted confusion energy, termed Adaptive Confusion Energy (ACE). The matrix is constructed by including prediction results from all images within a batch and an adaptive matrix to adjust class-specific weights. The former is used to handle the FGVC task or overfitting issue, and the latter is for resolving the long-tailed distribution. To achieve efficient DNN learning, we provide an approximation scheme to ACE so that gradient backpropagation can be readily carried out. The promising experimental results support that ACE has good potential to function as a generic regularizer for solving a wide range of classification tasks, no matter the fine-grained property or imbalanced distribution. 

\section{Related Work}
\label{sec:related}


Researches in fine-grained and long-tailed visual classification are going on in two different branches. Most articles focus on just one of these issues. We will introduce recent studies on both sides and then briefly explain our approach.
\vspace{-10pt}
\paragraph{FGVC.}
In the early works, the training data are annotated with additional information such as part labels. Along this line, \cite{berg2014birdsnap} explore the labeled part locations to eliminate highly similar object categories for improving the learned classifiers. The approach in \cite{huang2016part} is established based on a two-stream classification network to capture both object-level and part-level information explicitly. However, due to the rapid research advances in visual classification, the most recent FGVC approaches are designed to complete the model learning solely based on category labels' information.
\cite{sun2019fine,dubey2018pairwise,wang2018learning,li2018towards,yang2018learning,zheng2019looking,chen2019destruction, du2020fine}.
\vspace{-10pt}
\paragraph{Long-tailed visual recognition}
To alleviate the impact of the imbalanced data, the two common basic methods are re-sampling and re-weighting. Re-sampling in the early studies includes under-sampling \cite{drummond2003c4} for head categories and over-sampling \cite{ChawlaBHK02,HanWM05,mahajan2018exploring} for tail categories. In recently, the most common strategy is called class-balanced sampling \cite{ShenLH16}. Unlike instance-balanced sampling, every image has the same probability of being selected; class-balanced is to weight the sampling frequency of each image according to the number of samples of different categories. Furthermore, \cite{gupta2019lvis} proposed repeat
factor sampling (RFS), a dynamic-sampling mechanism, to balance the instances. Unlike sampling, because of the flexibility and convenience of loss calculation, many more complex tasks, such as object detection and instance segmentation, are more likely to leverage the re-weighted loss to solve the problem of long-tail distribution. From the reverse weighting based on category distribution to the Hard Example Mining \cite{ShrivastavaGG16} which is carried out directly according to the credibility of classification without knowing the category, such as Focal loss \cite{LinGGHD17} and LDAM \cite{cao2019learning}. Also, due to implementation is easy, some works \cite{CuiJLSB19,JamalB0WG20,TanWLLOYY20} show competitive results in complex tasks.  On the other way, the two-stage training strategies that learn the classifier with re-balancing data and to learn representation with original data is regarded as an effective solution to the constant tail distribution.
\cite{Kang2020Decoupling, zhou2020, LiWKTWLF20, HuJTCMZ20, abs-2007-11978, tang2020long, DBLP:conf/nips/YangX20}.
\vspace{-10pt}
\paragraph{Confusion energy.} 
In FGVC, the confusion-related formulation for dealing with intra-class variations and inter-class similarity has two main implications. First, it can be applied to alleviate the overfitting problem in training an FGVC model. \cite{dubey2018pairwise} construct a Siamese neural network, trained with a loss function including {\em pairwise confusion} (PC). The design reasons that bringing the class probability distributions closer to each other could prevent the learned FGVC model from overfitting sample-specific artifacts. Second, the confusion tactic can be used to boost the FGVC performance by focusing on local evidence. \cite{chen2019destruction} partition each training image into several local regions and then shuffle them by a {\em region confusion mechanism} (RCM). It implicitly excludes the global object structure information and forces the model to predict the category label based on local information. In other words, the ability to identify the object category from local details is expected to be enhanced through shape confusion.

Our approach to FGVC and long-tail is most relevant to the above confusion-based approaches. We retain the advantages of confusion energy and exploit the potential in the long-tailed distribution. And then propose a novel confusion energy term called {\em Adaptive Confusion Energy} (ACE), which can flexibly adjust the confusion strength corresponding to the data distribution.

\section{Approach}
\label{sec:approach}


We propose the adaptive confusion energy (ACE) to address the image classification on data with natural world distribution. Our ACE module combines two novel components: 1) Batch confusion norm (BCN) and 2) adaptive matrix . We elaborate our method as follows. 


\subsection{Overfitting Elimination by Batch Confusion Norm}
\label{sec:BCN}

Given a training set  over total  fine-grained categories, an arbitrary sample  from  is denoted as  where  represents an image and  denotes the corresponding class label. We define a batch  as a set of sample  randomly sampled from . Note that  is the batch size. For each training sample  in a batch , we forward propagate it through a classification model  and then obtain the predicted probability (\ie, softmax) . After that we define batch-wise class prediction matrix  by 
\begin{linenomath}

\end{linenomath}
where  is the predicted probability over the C fine-grained categories. Notice that, in our BCN module, we assume that  and all images within a batch  are randomly sampled from the . On the contrary, the confusion regularization of PC \cite{dubey2018pairwise} only affects the paired images with distinct labels. In a nutshell, BCN considers global optimization in substitution a pair.  

The explicit purpose of BCN is to increase the difficulty for a model to learn classification problems by infusing slight classification confusions into the training procedure. To this end, it is reasonable to minimize the rank of the batch-wise class prediction matrix  so that the predictions for all samples in a batch are {\em similar}:
\begin{linenomath}

\end{linenomath}
However, the rank-related minimization problems are often NP-hard. To address this problem, in this paper, we utilize convex relaxation methods to approximate the solutions. With the help of convex relaxation methods, minimizing the rank of  can be reduced as the minimization of its {\em nuclear norm}. That is the {\em batch confusion norm} of  can be formulated as
\begin{linenomath}

\end{linenomath}
where  is the nuclear norm which computes the sum of the singular values of the underlying tensor/matrix.

\paragraph{Stability.} In order to make the matrix decomposition of  stable and prevent the negative singular values from heavily affecting the training loss, we replace the right-hand side of (\ref{eqn:norm_BCN}) with  since it is known that
\begin{linenomath}
 
\end{linenomath}

Finally, by combining all the technique above, our batch confusion norm can be formulated as 
 

\subsection{Data imbalance control via adaptive matrix }
\label{sec:MA}
In this subsection, we introduce the adaptive matrix , which equips the BCN in Sec. \ref{sec:BCN} with the ability to handle imbalanced data and finally evolve into our Adaptive Confusion Energy (ACE). 

Empirically, the classification accuracy of different categories (with various data distribution) depends on different levels of confusion energy. Take the tailed classes with few samples; for example, applying high confusion energy on tailed classes may damage the classification performance. To fix this issue, we adopt an adaptive matrix  to generalize the BCN. The adaptive matrix  enables the BCN to adjust the strength of confusion energy for each category. Here are a couple of criteria for initializing a proper :
\begin{itemize}
    \item When it comes to a dataset with long-tailed distribution,  should alleviate the confusion energy on the tailed categories to prevent the model from getting excessive confusion over these classes. 
    \item When the data distribution is balanced,  should be approximately the same as an identity matrix.
\end{itemize}

Following these guidelines, we design the adaptive matrix  as
\begin{linenomath}

\end{linenomath}
where  represents the number of data for each category. Also,  and  denote the mean and standard deviation of , respectively. Finally,  stands for a tunable hyper-parameter. Note that when , we have . Also, if  then . This means that  will downgrade to the identity matrix when the data distribution is balanced.

Finally, by incorporating the batch confusion norm with the adaptive matrix , we can now formulate our novel adaptive confusion energy (ACE) as follows. 
\begin{linenomath}

\end{linenomath}
where the adaptive confusion energy loss  is computed based on the eigenvalues of . 
It is worth noting that our ACE has sufficient capability to handle data with natural world distribution by alleviating the overfitting problem in a fine-grained model and considering the imbalance problem in the long-tailed data distribution.

\paragraph{Learnability.} In practice, there is no feasible way to ensure that the parameters of  given in (\ref{eqn:loss_ACE}) is optimal by pre-defined parameters. Therefore, We alternately use it as a {\em good} initialization and set  as a learnable model, denoted as . Consequently,  we revise the  into
\begin{linenomath}

\end{linenomath}
where  is a tunable weight for the regularization term which regulates the learnable adaptive matrix  should not be too far away from . In practice, we initialize  with  and set  to simply improve the original adaptive matrix using the hand-crafted .

\subsection{Loss function}


Combine our ACE in Eq. (\ref{eqn:learnable}) with the original classification loss, the overall objective function can now be easily expressed by
\begin{linenomath}

\end{linenomath}
\noindent where  is the cross-entropy loss which is usually applied in classification task and  is a hyper-parameter to adjust the influence of the ACE loss to learning the model.

\section{Experimental Results}
\label{sec:result}


\begin{table*}[ht!]
\caption{Head-to-head comparisons of the confusion energy scenarios on the standard FGVC datasets CUB-200-2011 (CUB), Stanford Cars (Cars), and FGVC-Aircraft (Aircraft).}
\label{tab:fgvc_ab}
\centering
\begin{tabular}{lccccccccccccccc}
\toprule
\multirow{2}{*}{Model} & \multicolumn{3}{c}{ResNet-50} & \multicolumn{3}{c}{ResNeXt-50} & \multicolumn{3}{c}{ResNeXt-101} & \multicolumn{3}{c}{DenseNet-161}\\
        \cmidrule(lr){2-4}
        \cmidrule(lr){5-7}
        \cmidrule(lr){8-10}
        \cmidrule(lr){11-13}
        \cmidrule(lr){14-16}
                    & CUB  & CAR  & AIR  & CUB  & CAR  & AIR  & CUB  & CAR  & AIR & CUB  & CAR  & AIR \\
        \midrule
        \midrule
        Baseline    & 85.5 & 92.7 & 90.3 & 86.3 & 93.1 & 90.9 & 87.3 & 93.5 & 91.6 & 87.5 & 93.4 & 92.7 \\
        PC          & 87.0 & 92.4 & 90.1 & 87.5 & 93.2 & 91.2 & 88.2 & 93.7 & 92.4 & 88.2 & 93.6 & 92.9 \\
        \midrule
        \midrule
        Ours        & \bf 87.8 & \bf 94.3 & \bf 93.2 & \bf 88.1 & \bf 94.4 & \bf 93.3 & \bf 88.6 & \bf 94.5 & \bf 93.5 & \bf 89.2 & \bf 94.8 & \bf 93.5 \\
        \bottomrule
\end{tabular}
\vspace{-5pt}
\end{table*}

\begin{table}[ht!]
\centering
\caption{Compare the results with the typical state-of-the-art. The CNN backbone is ResNet-50.}
\label{tab:fgvc_all}
\resizebox{0.47\textwidth}{!}{
\begin{threeparttable}
\begin{tabular}{lcccc}
    \toprule
    Method & Param. (M) & CUB & CAR & AIR \\
    \midrule
    Baseline & 24 & 85.5 & 92.7 & 90.3 \\
    PC & 24 & 87.0 & 92.4 & 90.1 \\
    \midrule
    DB      & 24 & 87.7 & \bf 94.3 & 92.1 \\
    DFL-CNN & 24 & 87.4 & 93.1 & 91.7 \\
    NTS-Net & 24 & 87.5 & 93.9 & 91.4 \\
    DCL     & 24 & 87.8 & 94.5 & 93.0 \\
    iSQRT-COV & 24 & \bf 88.1 & 92.8 & 90.0 \\
    \midrule
    Ours    & 24 & 87.8 & \bf 94.3 & \bf 93.2 \\
    \midrule
    \midrule
    PC (DenseNet-161) & 28 & 88.2 & 93.6 & 92.9 \\
    S3N     & 101 & 88.5 & 94.7 & 92.8 \\
    PMG     & 45 & 88.9 & 95.0 & 92.8 \\
    \midrule
    Ours (DenseNet-161) & 28 & \bf 89.2 & \bf 94.8 & \bf 93.5 \\
    \bottomrule
\end{tabular}
    \begin{tablenotes}
            \item[] Re-implemented by the same training setting as ours.
            \item[] Modified ResNet-50 with additional modules.
    \end{tablenotes}
\end{threeparttable}
}
\vspace{-10pt}
\end{table}

We conduct extensive experiments to evaluate our approach on three balanced benchmark FGVC datasets, imbalanced datasets, and the natural world distribution dataset. We then describe comparisons to prior work as well as the implementation details. We also provide an insightful ablation study for assessing the performance gains of using adaptive confusion energy (ACE). Finally, several visualization examples are demonstrated for further discussions.

\subsection{Datasets}


We first evaluate the effectiveness of the proposed approach on three standard fine-grained visual classification datasets, namely, CUB-200-2011 \cite{WahCUB_200_2011}, Stanford Cars \cite{KrauseStarkDengFei-Fei_3DRR2013}, and FGVC-Aircraft \cite{maji13fine-grained}. 
The data ratio between training and testing sets is about  for CUB-200-2011, and Stanford Cars is about  in FGVC-Aircraft. The class distribution of the three datasets is nearly balanced, which can be used to measure the proposed method's performance only in the fine-grained scenario with the adaptive matrix  approximating identity matrix. Compared with other datasets for the large-scale visual classification task, these three FGVC datasets have fewer training data for each category.

Next, we go through the experiments on the imbalanced datasets, ImageNet-LT \cite{DBLP:journals/corr/abs-1904-05160}. The former is a long-tailed distribution with a low fine-grained factor, confirming whether the proposed approach will adjust on the purely imbalanced dataset. The latter is a fine-grained dataset that also has a long-tailed property. It can more clearly measure the impact of different approaches, \eg, \cite{Kang2020Decoupling,dubey2018pairwise,du2020fine}.

Finally, we then focus on the natural world distribution datasets and CUB-LT \cite{samuel2021generalized} and iNaturalist2018 \cite{van2018inaturalist} 
which has the properties of both fine-grained and long-tailed distribution. Besides, it is also a large-scale dataset. Judging from the recent literature \cite{cao2019learning,Kang2020Decoupling}, this is a reasonably challenging dataset that the performance can serve as an objective measure about our method's usefulness. Finally, we remark that the proposed model does not require any additional annotations in the training process but merely the image-level class annotations.


\subsection{Implementation details}
\label{subsec:details}


We describe the implementation details with FGVC, long-tailed, and the comprehensive task. All our inference results are obtained from end-to-end training except the results on ImageNet-LT. The experimental results are the mean of three run. We implement our method using the Pytorch framework \cite{paszke2017automatic}, and the platform with eight Nvidia V100. The source code will be made available. 
\vspace{-10pt}
\paragraph{FGVC.} Following relevant work \cite{yang2018learning,chen2019destruction,zheng2019looking}, we evaluate our method on the widely-used classification backbone ResNet series \cite{he2016deep} and DenseNet-161 \cite{huang2017densely} which is pre-trained on the ImageNet dataset. For the sake of fair comparison in FGVC training, we use the data augmentation setting as in \citet{chen2019destruction} that the input size is set as  , and horizontal flipping is randomly performed. The initial learning rate, the hyper-parameter , and  are , , and , respectively. The training batch size usually is  if the GPU memory is enough and the training optimizer is Momentum SGD, which accompanies with cosine annealing \cite{loshchilov2016sgdr} as the learning rate decay.
\vspace{-10pt}
\paragraph{Long-tailed visual recognition.} We further evaluate the proposed ACE on the imbalanced datasets, ImageNet-LT. For the sake of fair comparison, we follow the implementation details as in \cite{Kang2020Decoupling} on ImageNet-LT. We present the ResNeXt-50 performance in the following section and the ResNet-10 and ResNeXt-152 at the supplementary. The phenomena between shallow and deep models are almost consistent. Since the ImageNet-LT has a low fine-grained factor but a substantial imbalanced issue, we set the hyper-parameter  and  as  and , respectively.
\vspace{-10pt}
\paragraph{Comprehensive tasks.} Finally, we have experimental results on the CUB-LT and iNaturalist2018. In addition to using similar augmentation schemes, the setting is following \cite{Kang2020Decoupling,cao2019learning}. The backbones are ResNet-50 with 224  224 input size by 90 training epochs in SGD optimization. The batch size is 16, and the initial learning rate is 0.025 with a cosine annealing decreasing schedule. The confusion weight  is  and class-wise confusion weight  is . Moreover, the experiment about CUB-LT is the same as the previous FGVC setting. 
\vspace{-10pt}
\paragraph{Evaluation.} After training on the FGVC, imbalanced, and natural world datasets, we evaluate the models on the corresponding balanced test/validation datasets and report the top-1 accuracy, which is used commonly. The value of accuracy is reported in the format of percentage.

\subsection{Fine-grained}


To investigate the performance of different confusion energies between the different backbones, we conduct an ablation study from shallow to deep on the ResNet-50, ResNeXt-50, ResNeXt101, and DenseNet-161. Table~\ref{tab:fgvc_ab} shows the head-to-head comparison between PC and ACE.  We re-implement the PC at the same training condition, and the experimental results show that ACE has comprehensively improved against PC. Table~\ref{tab:fgvc_all} shows the comparison to the other state-of-the-art approaches with ResNet-50 backbone. Baseline combines with our approach provides a competitive performance. Note that the state-of-the-art PMG \cite{du2020fine} contains four classifiers with ResNet-50 backbone, which leads the size of the parameters becomes 45 million floating points. The size of PMG is larger than the DenseNet-161 backbone, about 29 million. The Table~\ref{tab:fgvc_ab} presents the competitiveness of our approach on the DenseNet-161 against PMG. Moreover, while the recent state-of-the-art PMG meets our ACE, it also improves. However, while the datasets are not large-scale, although ACE gains additional improvement, the confusion energies only provide little help. Hence, look at the FGVC research recently; it seems to have reached the limitation so far. Hence, it is reasonable to go through the more challenging tasks, which are large-scale, fine-grained, and long-tailed.

\subsection{Long-tailed}


\begin{table}[ht!]
    \centering
    \caption{Following the approach \cite{Kang2020Decoupling} on ImageNet-LT, the proposed ACE gains a significant improvement.}
    \label{tab:imagenet_lt}
    \begin{tabular}{lcccc}
        \toprule
        Method   & Many & Median & Few & Total \\
        \midrule
        \midrule
        ResNeXt-50 & 65.9	& 37.5	 & 7.7 & 44.4 \\
        \midrule
        NCM     & 56.6 & 45.3 & 28.1 & 47.3 \\
        cRT     & 61.8 & 46.2 & 27.4 & 49.6 \\
        -norm & 59.1 & 46.9 & 30.7 & 49.4 \\
        LWS     & 60.2 & 47.2 & 30.3 & 49.9 \\
        \midrule
        \midrule
        ResNeXt-50 +PC & 63.9 & 35.5 & 8.8 & 42.8 \\
        \midrule
        NCM     & 52.3 & 42.9 & 28.7 & 44.6 \\
        cRT     & 59.3 & 46.1 & 29.5 & 48.9 \\
        LWS     & 57.3 & 46.4 & 29.8 & 48.4 \\
        \midrule
        \midrule
        ResNeXt-50 + ACE & \bf 67.5 & 42.1 & 10.2 & 47.5 \\
        \midrule
        NCM     & 57.9 & 46.7 & 31.0 & 48.9 \\
        cRT     & 63.2 & 48.1 & 29.7 & 51.4 \\
        LWS     & 60.7 & \bf 49.7 & \bf 33.1 & \bf 51.7 \\
        \bottomrule
    \end{tabular}
    \vspace{-10pt}
\end{table}

ImageNet-LT has a low fine-grained factor but contains a strong imbalance issue. It is a reasonable dataset to measure the performance of ACE on the purely long-tailed distribution. Table~\ref{tab:imagenet_lt} shows the experimental results with the strategy same as \cite{Kang2020Decoupling}. At stage 1 with end-to-end training, while the baseline trained with ACE gains a significant improvement, but drop the performance if it is trained with PC. The reason is that PC does not consider the number of each category on the training set, which will destroy the representation learning. ACE has handled the weight of confusion strength to each category, which will carefully alleviate the overfitting issue. Furthermore, through stage 2, no matter cRT or LWS, ACE also gains an additional improvement against baseline or PC. Hence, tackling the imbalanced data distribution with ACE can learn a better representation.

\subsection{Natural World}


\begin{table}[t]
\centering
\caption{The comparison with some recent stat-of-the-art works \cite{Kang2020Decoupling,DBLP:conf/nips/YangX20} on the iNaturalist 2018.}
\label{tab:inat}
\resizebox{0.48\textwidth}{!}{
\begin{threeparttable}
\begin{tabular}{lccccc}
    \toprule
    Method & Backbone & Many & Median & Few & Total \\
    \midrule
    \midrule
    Baseline    & ResNet-50 & \bf 72.2 & 63.0 & 57.2 & 61.7 \\
    PC   & ResNet-50 & 70.9 & 64.6 & 59.6 & 62.1 \\
    \midrule
    LDAM-DRW        & ResNet-50 & -    & -    & -    & 64.6 \\
    NCM         & ResNet-50 & 55.5 & 57.9 & 59.3 & 58.2 \\
    cRT         & ResNet-50 & 69.0 & 66.0 & 63.2 & 65.2 \\
    LWS         & ResNet-50 & 65.0 & 66.3 & 65.5 & 65.9 \\
    BBN         & ResNet-50 & -    & -    & -    & 66.3 \\
    SSP         & ResNet-50 & -    & -    & -    & 68.1 \\
    \midrule
    Ours        & ResNet-50 & 66.6	& \bf 68.0 & \bf 68.2 & \bf 68.3  \\
    \midrule
    \midrule
    Baseline    & ResNet-152 & \bf 75.2 & 66.3 & 60.7 & 65.0 \\
    PC   & ResNet-152 & 72.1 & 67.2 & 61.3 & 65.9 \\
    \midrule
    NCM         & ResNet-152 & 59.3 & 61.9 & 62.6 & 61.9 \\
    cRT         & ResNet-152 & 73.6 & 69.3 & 66.3 & 68.5  \\
    LWS         & ResNet-152 & 69.4 & 69.5 & 68.6 & 69.1 \\
    \midrule
    Ours        & ResNet-152 & 69.2 & \bf 70.8 & \bf 72.7 & \bf 71.7 \\
    \bottomrule
\end{tabular}
    \begin{tablenotes}
            \item[] Re-implemented by the same training setting as ours.
            \item[] The results reproduced with author's code.
    \end{tablenotes}
\end{threeparttable}
}
\vspace{-10pt}
\end{table}


\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.47\textwidth]{fig/ablation_cublt.pdf}
    \caption{The accuracy with different confusion weight .}
    \label{fig:cub_lt}
    \vspace{-10pt}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.47\textwidth]{fig/F3.pdf}
    \caption{The accuracy of fine-grained and long-tailed approaches in CUB-LT. The proposed work alleviates the concurrent issues.}
    \label{tab:cub_lt}
    \vspace{-10pt}
\end{figure}

Before we look at the nature world dataset, let us quickly look at a small-scale one, CUB-LT, which contains both high fine-grained factors and imbalance issues. It is an ideal dataset for investigating the effect the approaches whether fine-grained \cite{dubey2018pairwise,DBLP:conf/eccv/DuCBXMSG20} or long-tailed \cite{Kang2020Decoupling,samuel2021generalized}. Figure~\ref{fig:cub_lt} shows that PC really can tackle the fine-grained property, but the performance drops when it faces the long-tailed issue. Figure~\ref{tab:cub_lt} presents that the long-tailed approaches only focus on the imbalanced issue but lack the fine-grained property. Similarly, the fine-grained approaches tackle the fine-grained property but not enough to address the imbalanced problem. Hence, the proposed ACE is a comprehensive approach that can easily and efficiently solve fine-grained and imbalanced problems simultaneously.

The proposed ACE preserves the benefits of confusion energy in the FGVC task and addresses the downside of the confusion energy in the long-tailed challenge. Table~\ref{tab:inat} shows the results on the natural world distribution dataset.  enables the BCN to focus on the head categories but alleviate the confusion energy effect on the tailed categories. Note that our models are trained not only with the most common way of data sampling {\em instance-balanced sampling} but also end-to-end. In contrast, LWS \cite{Kang2020Decoupling} trains the model in two stages and requires the use of {\em class-balanced sampling}. Besides, SSP \cite{DBLP:conf/nips/YangX20} starts with the self-supervised learning step and then follows the work of \cite{Kang2020Decoupling}, which contains three stages.

\citet{dubey2018pairwise} has shown that confusion energy alleviates the overfitting problem and improves the FGVC performance. However, we observe that if the baseline model is coupled with the confusion energy directly, the overall performance only improves slightly on the natural world dataset. It suggests that the long-tailed issue needs further investigations beyond the model of confusion energy. 

\subsection{Analysis}


\begin{figure*}[t]
    \centering
    \subfloat[Baseline]{\includegraphics[width=0.33\textwidth]{fig/base_train_val_acc.pdf}}
    \subfloat[PC]{\includegraphics[width=0.33\textwidth]{fig/pc_train_val_acc.pdf}}
    \subfloat[ACE]{\includegraphics[width=0.33\textwidth]{fig/ace_train_val_acc.pdf}}
    \vspace{-10pt}
    \caption{Observation of the overfitting issue. (a) and (b) shows that there is a large gap between training accuracy and validation performance. (c) presents ACE alleviates the overfitting issue and improves the validation performance.}
    \label{fig:overfit}
    \vspace{-10pt}
\end{figure*}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.47\textwidth]{fig/plota.pdf}
    \vspace{-15pt}
    \caption{The -norm of each category corresponds to the weight  in the classifier.}
    \label{fig:w_norm}
    \vspace{-15pt}
\end{figure}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.47\textwidth]{fig/F7CAM.pdf}
    \vspace{-15pt}
    \caption{Heatmap-visualization of testing images by Grad-CAM. We show each model's corresponding heatmap. 
    }
    \label{fig:cam}
    \vspace{-15pt}
\end{figure}

In this section, we give some analysis about the influence of ACE. Classification frameworks usually use the cross-entropy loss as the objective function. The training loss always converges to a shallow level, regardless of the characteristics of the dataset. However, this is entirely unreasonable, and to some extent, the model overfits the training data. Hence, we show the performance during training on Figure~\ref{fig:overfit} to present that ACE can prevent the model from overfitting the training data.

Next, consider each category's magnitude corresponding to the classifier weight  in Figure~\ref{fig:w_norm}. The scale of   distribution on the baseline method is very similar to the data distribution. Although PC has alleviated the scale of the head categories, the distribution does not change significantly.  Nevertheless, the adaptive confusion energy ACE makes the scale of the head to become smoother. This means that the weights of head categories will not dominate the prediction of the classification. 

In summary, ACE provides several benefits. First, it alleviates the overfitting problem of the cross-entropy loss. While training with the cross-entropy loss concerning the ground truth label in the manner of the one-hot vector, the inter-class similarity information is usually significantly suppressed. Consequently, it cannot capture the fine-grained essence by one single cross-entropy loss while handling the overfitting issue. The proposed ACE successfully alleviates this issue. Second, ACE forces the model to learn the inter-class similarity so that the classifier is more focused on the discriminative parts. This phenomenon can be found by using the class activation mapping (Grad-CAM) \cite{selvaraju2017grad} presented in Figure~\ref{fig:cam}. Third, ACE does not require additional processing of inputs and outputs during training. There is no extra cost at inference time, which makes it flexible and applicable to real applications. Finally, ACE solves the confusion energy problem while meets the long-tailed distribution. ACE coupled with the adaptive matrix  preserves the benefits of confusion energy in the FGVC task and addresses its downside in the long-tailed scenario.

\section{Conclusions}


We have developed a general regularization technique specifically designed for addressing the fine-grained visual classification and the long-tailed data distribution problems simultaneously. The proposed adaptive confusion energy (ACE), together with the standard cross-entropy loss, can be used to account for the inherent classification difficulties due to inter-class similarity and intra-class variations. Moreover, it can also solve the long-tailed problem by an adaptive matrix term. The proposed ACE considers the confusion regularization within each training batch and thus is more general than the suitable formulation of pairwise confusion energy. The resulting model can learn discriminative features within regions of interest and alleviate the overfitting problem in training. The provided experimental results nearly achieve state-of-the-art over the three mainstream FGVC datasets and are competitive to leading long-tailed approaches on the imbalanced or natural world distribution datasets. Our future work will focus on generalizing the ACE concept to tensors and extending its applications to other challenging computer vision problems.


\bibliography{reference}
\bibliographystyle{icml2021}

\end{document}
