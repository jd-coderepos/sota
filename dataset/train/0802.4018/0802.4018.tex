\documentclass{LMCS}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{stmaryrd}
\usepackage{epsfig}
\usepackage{color}
\usepackage{listings}
\lstset{language=Join,aboveskip=0.6\abovedisplayskip,belowskip=0.1\belowdisplayskip,mathescape=true,indent={1.5em}}
\let \lst \lstinline

\usepackage{xspace}
\usepackage{mathpartir,enumerate,hyperref}

\newcommand{\forget}[1]{}
\newcommand{\etal}{\emph{et al.}\@\xspace}
\newcommand{\pair}[2]{#1\mathord{\texttt{,}}#2}
\renewcommand{\exp}{e}
\newcommand{\resetequationcounter}{\setcounter{equation}{0}}
\newcommand{\const}{\kappa}
\newcommand{\pt}{\pi}
\newcommand{\ptbis}{\omega}
\newcommand{\ptsbis}{\Omega}
\newcommand{\pts}{\Pi}
\newcommand{\lessprecise}{\preceq}
\newcommand{\ins}[1]{{\sf Ins}(#1)}


\newcommand{\rln}[1]{\text{\small\sc #1}}
\newcommand{\ie}{\emph{i.e.}\@\xspace}
\newcommand{\eg}{\emph{e.g.}\@\xspace}
\newcommand{\jocaml}{\textrm{JoCaml}\xspace}
\newcommand{\ocaml}{\textrm{OCaml}\xspace}
\newcommand{\erlang}{\textrm{Erlang}\xspace}            
\newcommand{\csharp}{C\xspace}
\newcommand{\haskell}{\textrm{Haskell}\xspace}
\newcommand{\comega}{C\xspace}
\newcommand{\funnel}{\textrm{Funnel}\xspace}
\newcommand{\scala}{\textrm{Scala}\xspace}
\newcommand{\java}{\textrm{Java}\xspace}
\newcommand{\joinjava}{\textrm{JoinJava}\xspace}
\newcommand{\rcham}{\textrm{RCHAM}\xspace}
\newcommand{\cham}{\textrm{CHAM}\xspace}
\newcommand{\Id}{\emph{Id}\@\xspace}
\newcommand{\kwd}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\prefix}[1]{\mathopen{}\mathrel{\kwd {#1}}}
\newcommand{\infix}[1]{\mathrel{\kwd {#1}}}

\newcommand{\id}[1]{\textit{#1}}
\newcommand{\none}{\kwd{None}}
\newcommand{\set}[1]{\{#1\}}
\newcommand{\lubop}{\mathop{\uparrow}}
\newcommand{\lub}[2]{#1 \lubop #2}
\newcommand{\cons}[2]{#1\mathord{\texttt{::}}#2}
\newcommand{\nil}{\texttt{[]}}
\newcommand{\single}[1]{\texttt{[}#1\texttt{]}}
\newcommand{\repr}[2]{#1 \mathop{\updownarrow} #2}
\renewcommand{\_}{\mathord{\rule[-.25ex]{1ex}{.15ex}}}
\newcommand{\appnv}[4]{{#1}(#2, #3, \ldots{} , #4)}
\newcommand{\V}{\mathcal{V}}
\newcommand{\E}{\mathcal{E}}

\newcommand{\nullp}{0}
\newcommand{\defineas}{\;\stackrel{\mathrm{def}}=\;}
\newcommand{\C}[1]{\llbracket#1\rrbracket}
\newcommand{\CPLambda}[1]{\llbracket#1\rrbracket_{\lambda}}
\newcommand{\para}[2]{#1\mathop{\&}#2}
\newcommand{\dis}[2]{#1\infix{or}#2}
\newcommand{\define}[2]{\prefix{def} #1 \infix{in} #2}
\newcommand{\matchfour}[4]{\prefix{match}  #1 \infix{with} #2_1
  \rightarrow #3_1 \mid \ldots \mid #2_{#4} \rightarrow #3_{#4}}
\newcommand{\matchthree}[3]{\prefix{match} #1 \infix{with} \mid^{i
    \in I} #2_i \rightarrow #3_i}
\newcommand{\matchone}[3]{\prefix{match} #1 \infix{with} #2
  \rightarrow #3} 
\newcommand{\matchC}[3]{\prefix{match} #1 \infix{with} \mid^{i
    \in I} #2_i \rightarrow \C{#3_i}}
\newcommand{\reaction}[2]{#1 \triangleright #2}
\newcommand{\soup}[2]{#1\vdash#2}
\newcommand{\heat}{\rightharpoonup}
\newcommand{\cool}{\rightharpoondown}
\newcommand{\reduces}{\longrightarrow}
\newcommand{\hc}{\rightleftharpoons}
\newcommand{\dd}{\mathcal{D}}
\newcommand{\pp}{\mathcal{P}}
\newcommand{\wide}[1]{\mathrel {{}#1{}}}
\newcommand{\hcred}{\overset{\heat}{\underset{\cool}{\reduces}}}
\newcommand{\sub}[2]{#1#2}
\newcommand{\restrict}{\mathbin{\upharpoonright}}
\newcommand{\composite}[2]{#1 \mathop{\circ} #2} 
\newcommand{\wbc}{\mathrel{\approx}} 
\newcommand{\R}{\mathrel{\mathcal{R}}}
\newcommand{\pseq}{\;\mathop{\Bumpeq}\;}
\newcommand{\weakbarb}[2]{#1\!\Downarrow_{#2}}
\newcommand{\barb}[2]{#1\!\downarrow_{#2}}
\newcommand{\econtext}[1]{E[#1]}
\newcommand{\mctxt}[1]{\prefix{match} \exp \infix{with} \ldots \mid \pi_k
  \rightarrow #1 \mid \ldots}
\newcommand{\jctxt}[2][R]{\define{\dis{\reaction{J}{#2}}{D}}{#1}}


\newcommand{\preds}[1]{I(#1)}
\newcommand{\fv}[1]{\mbox{\sf fv}[#1]}
\newcommand{\dv}[1]{\mbox{\sf dv}[#1]}
\newcommand{\rv}[1]{\mbox{\sf rv}[#1]}
\def \dom #1{\mbox{\sf dom}(#1)}
\newcommand{\bisi}[1]{\widehat{#1}}

\def\doi{4 (1:7) 2008}
\lmcsheading {\doi}
{1--41}
{}
{}
{Jan.~25, 2007}
{Mar.~21, 2008}
{}   


\begin{document}

\title{Algebraic Pattern Matching in Join Calculus\rsuper *}

\author[Q.~Ma]{Qin Ma\rsuper a}
\address{{\lsuper a}OFFIS, Escherweg 2, 26121 Oldenburg, Germany}
\email{Qin.Ma@offis.de}

\author[L.~Maranget]{Luc Maranget\rsuper b}
\address{{\lsuper b}INRIA-Rocquencourt, BP 105, 78153 Le Chesnay Cedex, France}
\email{Luc.Maranget@inria.fr}

\keywords{join-calculus, pattern-matching, process calculus, concurrency}
\subjclass{D.1.3, D.3.3, F.3.2}
\titlecomment{{\lsuper *}Extended version of~\cite{MaMaranget2004pattern}}


\begin{abstract}
We propose an extension of the join calculus with pattern matching on
  algebraic data types. Our initial motivation is twofold: to provide an
  intuitive semantics of the interaction between concurrency and pattern
  matching; to define a practical compilation scheme from extended join
  definitions into ordinary ones plus ML pattern matching. To assess the
  correctness of our compilation scheme, we develop a theory of the applied
  join calculus, a calculus with value passing and value matching. We
  implement this calculus as an extension of the current \jocaml system.
\end{abstract}

\maketitle



\section{Introduction}


\label{sec.intro}

The join calculus~\cite{Fournet98:PhD,FournetGonthier96} is a process
calculus in the tradition of the -calculus of Milner
\etal~\cite{MPW92}. One distinctive feature of join calculus is the
simultaneous definition of all receptors on several channels through
\emph{join definitions}. A~join definition is structured as a list of
\emph{reaction rules}, with each reaction rule being a pair of one
\emph{join pattern} and one \emph{guarded process}. A join pattern is
in turn a list of channel names (with formal arguments), specifying
the synchronization among those channels: namely, a join pattern is
matched only if there are messages present on all its channels.
Finally, the reaction rules of one join definition define competing
behaviors with a non-deterministic choice of which guarded process to
trigger when several join patterns are satisfied.

In this paper, we extend the matching mechanism of join patterns, such
that \emph{message} contents are also taken into account. As an
example, let us consider the following list-based implementation of a
concurrent stack:\footnote{We use the \ocaml syntax for lists,
  with \emph{Nil} being  and \emph{Cons} being the infix
  .}
\begin{lstlisting}{Join}
def pop(r) & State(x::xs) |> r(x) & State(xs)
 or push(v) & State(ls) |> State (v::ls) 
 in State([]) & 
\end{lstlisting}
The second join pattern \lst"push(v) & State(ls)" is an
\emph{ordinary} one: it is matched whenever there are messages on both
\lst"State" and \lst"push". By contrast, the first join pattern is an
\emph{extended} one, where the formal argument of channel \lst"State"
is an \emph{algebraic pattern}, matched only by messages that are
cons cells.  Thus, when the stack is empty (\ie, when message
 is pending on channel \lst"State"), pop requests are delayed.
Note that we follow the convention that capitalized channels are
private: only \lst"push" and \lst"pop" will be visible outside.

A similar stack can be implemented without using extended join
patterns, but instead, using an extra private channel and ML pattern
matching in guarded processes:
\begin{lstlisting}{Join}
def pop(r) & Some(ls)  |> match ls with 
                           | [x] -> r(x) & Empty() 
                           | y::x::xs -> r(y) & Some(x::xs)
 or push(v) & Empty() |> Some ([v])
 or push(v) & Some(ls) |> Some (v::ls)
 in Empty() & 
\end{lstlisting}
This second definition encodes the empty/non-empty status of
the stack as a message on channels \lst|Empty| and \lst|Some| respectively.
Pop requests on an empty stack are still delayed, since there is
no rule for the join pattern \lst|pop(r) & Empty()|.
The second definition obviously requires more programming
effort. Moreover, it is not immediately apparent that messages on
\lst"Some" are non-empty lists, and that the partial ML pattern
matching thus never fails.

Join definitions with (constant) pattern arguments appear informally
in functional nets~\cite{odersky:esop2000}. Here we generalize this
idea to full algebraic patterns. A similar attempt has also been
scheduled by Benton \etal as an interesting future work for
\comega~\cite{Cw}.

The new semantics is a smooth extension, since both join pattern
matching and pattern matching rest upon classical substitution (or
semi-unification).  However, an efficient implementation is more
involved. Our idea is to address this issue by transforming programs
whose definitions contain extended join patterns into equivalent
programs whose definitions use ordinary join patterns and whose
guarded processes use ML pattern matching. Doing so, we leave most of
the burden of pattern matching compilation to an ordinary ML~pattern
matching compiler. However, such a transformation is far from obvious.
More specifically, there is a gap between (extended) join pattern
matching, which is non-deterministic, and ML pattern matching, which
is deterministic (following the ``first match policy''). For example,
in our definition of a concurrent stack with extended join patterns,
\lst"State(ls)" is still matched by any message on \lst"State",
regardless of the presence of the more precise \lst"State(x::xs)" in
the competing reaction rule that precedes it.  Our solution to this
problem relies on partitioning matching values into non-intersecting
sets.  In the case of our concurrent stack, those sets simply are the
singleton  and the set of non-empty lists.  Then,
pattern \lst"State(ls)" is matched by values from both sets, while
pattern \lst"State(x::xs)" is matched only by values of the second
set.

The rest of the paper is organized as follows: Section~\ref{sec.pt}
first gives a brief review of algebraic patterns and ML pattern
matching. Section~\ref{sec.joinpi} presents the applied join calculus
--- an extension of join with algebraic pattern matching.
We introduce the semantics and the appropriate equivalence relations.
Section~\ref{sec.trans-idea} informally explains the key ideas to
transform the extension to the ordinary join calculus, and especially how
we deal with the nondeterminism problem.
Section~\ref{sec.trans} formalizes the transformation as a
compilation scheme and presents the algorithm which essentially works
by building a meet semi-lattice of patterns.  We go through a complete
example in Section~\ref{sec.example}, and finally, we deal with the
correctness of the compilation scheme in Section~\ref{sec.correct}.
Implementation has been carried out as an extension of the \jocaml system.
We discuss the issues that have arisen during the
implementation work in Section~\ref{sec.imp}.

An earlier version of this paper (lacking the detailed proofs and the
discussion of the implementation) appeared
as~\cite{MaMaranget2004pattern}.

\section{Algebraic data types and ML pattern matching}
\label{sec.pt}

This section serves as a brief introduction to algebraic data types
and ML pattern matching. Interested readers are referred to
\cite{warning,LefessantMarangetPattern} for further details.

\subsection{Algebraic data types}
\label{subsec.pt}

In functional languages, new types can be introduced by using
\emph{data type definitions} and such types are algebraic
data types.
For example, using \ocaml syntax, binary trees can be defined as follows:
\begin{lstlisting}{Join}
type tree = Empty | Leaf of int | Node of tree * tree
\end{lstlisting}
The \emph{complete signature} of type \lst"tree" has three
\emph{constructors}: \lst"Empty", \lst"Leaf", and \lst"Node", which
are used to build the values of this type. Every constructor has an
arity, \ie the number of arguments it requires and meanwhile specifies
the corresponding types of each argument. In this definition,
\lst"Empty" is of arity zero, \lst"Leaf" is of arity one (and accepts
integer arguments),
and \lst"Node" is of arity two (both its arguments being themselves of
type \lst"tree"). A constructor of zero arity is sometimes called a
\emph{constant constructor}.

Most native ML data types can be seen as particular instances of
algebraic data types. For example, lists are defined by two
constructors: constant \lst"Nil" (written ) for empty
lists and \lst"Cons" (written as the infix ) for
nonempty ones; pairs are defined by one constructor with arity two,
(written as the infix ``''); and integers
are defined by infinitely many (or ) constant constructors.


Formally, the algebraic values (for short values) of type~ are well-typed terms built from the constructors of .
``Well-typed'' here means correct with respect to constructor arity
and argument types.
Assuming a countable set of identifiers for constructors,
ranged over by , we give the formal definition of values as
follows:

Type correctness is left implicit: we shall consider well typed terms
only.


Algebraic patterns (for short patterns) of type  are also well-typed
terms built from the constructors of , but with
variables.\footnote{We freely replace variables
whose names are of no importance by wildcards~``''.} The formal
definition of patterns is given as follows.
We further require all variables in
a pattern to be pairwise distinct, that is, we only consider
\emph{linear} patterns.


Again, we assume a typed context. More precisely, we rely on the ML type
system to guarantee that values and patterns are well-typed.
Moreover, we rely on a ML type inferer to enrich syntax with
explicit types (which we leave implicit), and consider that the
type of any syntactic structure is available whenever needed.
Doing so, we focus on our main issue and avoid complications that would
be of little explanatory value.

Patterns are used to discriminate values according to their
structures. More specifically, a pattern denotes a set of values that
have a common prefix specified by the pattern. We say a value~ (of
type~) is an \emph{instance} of pattern~ (of type~), or
that  matches , when  describes the prefix of , in other words,
when there exists a substitution , such that .  For linear patterns, the instance relation can be defined
inductively as follows:
\begin{defi}[Instance]
  Let  be a pattern and  be a value, such that  and 
  have the same type, the instance relation  is
  defined as:
  
\end{defi}
\noindent We write  for the set of the instances of
pattern~. The instance relation induces the following relations
among patterns. These relations apply to patterns  and 
that have the same type.
\begin{defi}[Pattern relations] \hfill
\begin{enumerate}[]
\item Patterns  and  are {\bf compatible} when they
  share at least one instance. Otherwise  and  are {\bf
    incompatible} written . Two compatible patterns
  admit a least upper bound written , whose
  instance set is .
\item Pattern  is {\bf less precise} than pattern 
  written  when .
\item Patterns  and  are {\bf equivalent} written  when .  If so, their least
  upper bound is their representative, written~.
\end{enumerate}
\end{defi}
\noindent Note that we use the same notation  for both
relations: ``being an instance of'' (which is between a
pattern and a value) and ``being less precise'' (which is between two
patterns).
Indeed, values are in fact a special case of
patterns (with no variables), and in that case, both relations collapse.

The least upper bound of two patterns can be computed at the same time
when compatibility is checked by the following rules: 

Deciding the relation ``being less precise'' is more involved. Because
of typing, there exists nontrivial such relations, for instance
.  The \jocaml compiler relies on an
efficient algorithm for this task, called the  algorithm,
with  standing for ``Usefulness''~\cite{warning}.
Algorithm~ takes two parameters: a list of patterns
 and a pattern , and returns a boolean. Roughly speaking, it
checks the usefulness of  with respect to . More
specifically, algorithm~ tests the existence of at least
one value  such that  admits~ as an instance, and none of
the patterns in  does.

From the point of view of algorithm~, deciding
the relation  amounts
to compute the \emph{negation} of .
Namely,  is less precise then , if and only if
all the instances of~ are instances of~.

We now give a simplified definition of algorithm~.
The simplified definition suffices for our needs and also conveys the
basic idea behind the algorithm.

Consider , where
 and~ are patterns of a common type~.
The following two cases are distinguished.\medskip

\noindent{\bf Case}   \hfill
  \begin{enumerate}[]
  \item If , then check if
    , s.t.
    .
  \item If  and , then  (\ie  for
    ).
  \item If , then  (\ie  for
    ).
  \end{enumerate}\medskip

\noindent{\bf Case}   \hfill
  \begin{enumerate}[]
  \item If , then  (\ie  for
    ).
  \item If ,
    \begin{enumerate}[]
    \item if  is the unique constructor of type , then
      check if , s.t.
      .
    \item otherwise  (\ie  for
      ).
    \end{enumerate}
  \end{enumerate}\medskip


\noindent Once we can decide relation ``'', we can easily
decide pattern equivalence, since, by definition, 
means  and .

\subsection{ML pattern matching}
\label{subsec.MLpt}

In ML, operating on algebraic data types is performed by the use of
the following \emph{\kwd{match} construct}
that we extend to processes (,  etc. below are processes of
the join calculus).
\begin{lstlisting}{Join}
  match  with  ->  |  ->  |  |  -> 
\end{lstlisting}
Above, we attempt a matching of value~ against a sequence of patterns
 of the same type. 

ML pattern matching is deterministic. It follows the ``first match
policy''. That is, when value~ is an instance of more than one of
the patterns~, the \kwd{match} construct chooses the one with
the smallest index~.  This can be seen as checking patterns
, , \ldots ,  for admitting~ as an instance
sequentially, stopping as soon as a match is found.  As a consequence,
pattern  is matched only by the values in set
.
Moreover, patterns in ML pattern matching also act as binding
constructs.  Once a successful match is found, say ,
the variables in  are all bound to the corresponding subterms
of  in the guarded process~.

Additionally, we say a \kwd{match} construct is \emph{exhaustive}
when  is the whole set of values of the considered type.
We accept non-exhaustive \kwd{match} constructs.


\section{The applied join calculus}
\label{sec.joinpi}

We define the applied join calculus by analogy with the applied
-calculus~\cite{AppliedPi}. The applied join calculus inherits
its capabilities of communication and concurrency from pure join.
Moreover it supports algebraic value passing and algebraic pattern
matching in both join patterns and processes.

\subsection{Syntax and scopes}
\label{subsec.syntax}

The syntax of the applied join calculus is given in
Figure~\ref{fig.syntax}. As it is customary in process calculi
definitions, we assume an infinite set of
identifiers for variables, ranged over by .

\begin{figure}[ht]
\centering

\caption{Syntax of the applied join calculus}\label{fig.syntax}
\end{figure}

With respect to pure join calculus, two new syntactic categories are
introduced: expressions and patterns.  At first glance, both
expressions~ and patterns~ are terms constructed from
variables and constructors, where  stands for the arity of
constructor .  We make them different syntactic categories
for clarity, and also because we require patterns to be linear. We
also formalize the ML pattern matching in processes, as the new
\kwd{match} construct.  Moreover, in contrast to ordinary name passing
join calculus, there are two other, more radical, extensions: first,
in message sending, message contents become expressions as ,
that is, we have value passing; second, when a channel name is defined
in a join pattern, in addition to the synchronization requirement, we
also specify what pattern the message content should satisfy by
.

There are two kinds of bindings: the definition process
 binds all the channel names defined in  (written
) with scope ; and the reaction rule  or
the ML pattern matching  bind all the local
variables (written  or ) with scope  or ,
. The definition of the sets of defined
channel names  is the same as in pure join.  By contrast,
the definition of sets  has to be extended, so as to take
pattern arguments into account. Meanwhile, the definition of sets
 should also be extended, to cater for the new \kwd{match}
process and expressions. We present the formal definitions of
, , and  in
Figure~\ref{fig.scope}. In these rules,  is the disjoint
union, which expresses linearity constraints on both algebraic and
join patterns.

\begin{figure}[!p]
\centering

\caption{Bindings and scopes in the applied join
  calculus}\label{fig.scope}
\end{figure}

In applied join, values become of two kinds: channel names or
algebraic values.  We assume a type discipline in the style of the
type system of the
join-calculus~\cite{Fournet-Laneve-Maranget-Remy:typing-join},
extended with algebraic data types and the rule for ML pattern
matching.  Without making the type discipline more explicit, we
consider only well-typed terms (whose type we know), and assume that
substitutions preserve types. It should be observed that tuples are
now represented as a kind of constructed expressions and the arity
checking of polyadic join calculus is now replaced by a well-typing
assumption in applied join, which is thus monadic. One important
consequence of typing is that any (free) variable in a term possesses
a type and that we know this type.  Hence, we can discriminate between
those variables that are of a type of constructed values and those
that are of channel type.  Following the semantics of name passing
calculi such as join, we treat the latter kind of variables as channel
names, that is, values.  While, in any reasonable semantics, the
former kind of variables cannot be treated so.  We call a term
\emph{variable-closed} (\emph{closed} for short) when its free
variables are all of channel type, and otherwise \emph{open}.

\subsection{Chemical semantics}
\label{subsec.cham}

We establish the semantics following the reflexive chemical abstract
machine (\rcham) style --- the reflexive variant of
\cham~\cite{CHAM92}, whose states are chemical solutions. A
\emph{chemical solution} is a pair ,
where  is a multiset of (active) join definitions, and
 is a multiset of (running) processes.  Extending the
notion of closeness to solutions in the member-wise manner, we say a
solution is closed when all its active join definitions and running
processes are closed, namely, free variables are all of channel type.
We define semantics only on closed solutions. The chemical rewriting
rules are given in Figure~\ref{fig.cham},
consisting of two kinds as
in join: structural rules  or  represent the syntactical
rearrangement of the terms, and reduction rules  represent
the computation steps. We follow the convention to omit the part of
the solution that remains unchanged during rewrite.
This can also be expressed by the following context rule: 
where
 stands for either  or , and
 and  are the independent context of the considered
subsolution. Rule~\rln{Str-Def} is a bit of exception because its side
condition actually requires the following relationship hold between
the rewriting part and its context: .

Finally, it is perhaps to be noticed that, amongst the various,
slightly different, semantics of join-machines, we extend the one
of~\cite{Fournet-Laneve-Maranget-Remy:typing-join}, which is adapted
to static typing.  This means that we need to state explicitly that
 is an associative-commutative operator.  As a consequence,
the notation  in rule~\rln{React} stands for
a definition that possesses a reaction rule whose pattern is~.
\begin{figure}
\centering

\caption{RCHAM of the applied join calculus}\label{fig.cham}
\end{figure}

Matching of message contents against formal pattern arguments
is integrated in the substitution~ in rule \rln{React}.
As a consequence  this rule does not formally change
with respect to ordinary join calculus. However its semantical power has
much increased.
The \rln{Match} rule is new and expresses ML pattern matching.  
Its side condition enforces the first match policy.

According to the convention of processes as solutions, namely  as
, the semantics is also defined on closed processes
in the following sense.
\begin{defi}
  Let  denote the transitive closure of
  , 
  \begin{enumerate}[(1)]
  \item 
  \item 
  \end{enumerate}
\end{defi}
Subsequently, we have the following structural rule:
\begin{lem}\label{struct-rule}
  If , , and , then .
\end{lem}
\begin{proof}
  Trivially follow the definitions of  and , and the
  transitivity of . \forget{\qed}
\end{proof}
\subsection{Equivalence relation}
\label{subsec.eq}

In this section, we equip the applied join calculus with equivalence
relations to allow reasoning over processes. The classical
notion of \emph{barbed congruence} is a sensible behavioral
equivalence based on a reduction semantics and barb predicates. It was
initially proposed by Milner and Sangiorgi for
CCS~\cite{Milner92barbed}, and adapted to many other process
calculi~\cite{Honda95reductionbased,Amadio96bisimulations}, including
the join calculus. We take \emph{weak
  barbed congruence}~\cite{Milner92barbed} as our basic notion of
behavioral equivalence for closed processes.

\subsubsection{Observational equivalence for closed processes}
\begin{defi}[Barb predicates]
  Let  be a closed process, and  be a free channel name in ,
  \begin{enumerate}[(1)]
  \item  has a strong barb on channel : , iff , for some ,  and
    , where .
  \item  has a weak barb on channel : , iff , such that .
  \end{enumerate}
  where  denotes the reflexive and
  transitive closure of .
\end{defi}
Following the definition, it is easy to check that two structurally
congruent processes maintain the same barbs, \ie the lemma below.
\begin{lem}\label{lemma.barb}
  For two closed processes  and , whenever , we have
   iff , and  iff
  .
\end{lem}
\begin{proof}
  The part for strong barb holds following the transitivity of
  , and the part for weak barb holds following
  Lemma~\ref{struct-rule}.  \forget{\qed}
\end{proof}

\begin{defi}[Weak barbed bisimulation]
\label{def.joinpi.bisi}
  A binary relation  on closed processes is a weak barbed
  bisimulation, iff whenever , we have:
  \begin{enumerate}[(1)]
  \item If , then , such that  and , and vice versa. ( is a reduction
    bisimulation.)
  \item  implies  for any channel
    , and vice versa. ( preserves barbs.)
  \end{enumerate}
\end{defi}
To make the definition easier to work with, we prove the following
lemma where  is replaced by  in the
first clause, and  is replaced by  in
the second clause.
\begin{lem}\label{lemma.joinpi.bisi}
  Let  be a binary relation on closed processes that satisfies the
  following two conditions for any processes  and  such that
  :
  \begin{enumerate}[\em(1)]
  \item If , then , such that  and , and vice versa.
  \item  implies  for any channel ,
    and vice versa.
  \end{enumerate}
  Then  is a weak barbed bisimulation.
\end{lem}
\begin{proof} We check against the two clauses of
  Definition~\ref{def.joinpi.bisi} for one direction. The proof of the
  other direction is symmetric.
  \begin{enumerate}[(1)]
  \item  is a reduction bisimulation, that is
     We reason on the length of the derivation
    , written .

\paragraph{\bf Base case}\ , trivial.

\paragraph{\bf Induction case}\  As illustrated in the following
      diagram chase,
    \begin{center}
      \begin{picture}(0,0)\includegraphics{wbbisi-alt.eps}\end{picture}\setlength{\unitlength}{3947sp}\begingroup\makeatletter\ifx\SetFigFont\undefined \gdef\SetFigFont#1#2#3#4#5{\reset@font\fontsize{#1}{#2pt}\fontfamily{#3}\fontseries{#4}\fontshape{#5}\selectfont}\fi\endgroup \begin{picture}(1364,1812)(2065,-5287)
\put(2701,-3586){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2701,-4411){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2701,-5236){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2101,-3586){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2101,-4411){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2101,-5236){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(3301,-3586){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(3301,-4411){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(3301,-5236){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\end{picture}     \end{center}
    we have .  By induction
    hypothesis, we have . By applying hypothesis~ to
    to  and , we also have . And we conclude.
\item  preserves barbs, that is 
    .
    We thus assume . That is,
    
    By ~above,
    
    Then by applying hypothesis~ to  and~,
    we get . Hence we have .
  \end{enumerate}
\forget{\qed} 
\end{proof}
In later discussion, we sometimes directly check against the two
conditions of Lemma~\ref{lemma.joinpi.bisi} instead of the ones
of Definition~\ref{def.joinpi.bisi} for weak barbed bisimulation.

We define \emph{a context} as a term built by the grammar of process
with a single process placeholder .  \emph{An evaluation
  context}  is a context in which the placeholder is
not guarded. Namely: 
In addition to evaluation contexts,
there are two kinds of \emph{guarded contexts},
referred to as \emph{definition contexts} (\ie
) and \emph{pattern matching contexts} (\ie
).
We say that a context is closed if all the free variables in it are of
channel types.
\begin{defi}[Weak barbed congruence]
  A binary relation on closed processes is a weak barbed congruence, iff
  it is a weak barbed bisimulation and closed by application of any
  closed evaluation context. We denote the largest weak barbed
  congruence as~.
\end{defi}
The weak barbed congruence  is defined on the closed subset of
the applied join calculus. Although the definition itself only
requires the closure of evaluation contexts, it can be proved that the
full congruence does not provide more discriminative power. Similarly
to what Fournet has established for the pure join calculus in his
thesis~\cite{Fournet98:PhD}, we first have the property that  is
closed by substitution because, roughly, name substitutions may be
mimicked by evaluation contexts with ``forwarders''.
\begin{lem}\label{lemma.wbcsub}
  Given two closed processes  and , if , then for any
  substitution , .
 (Note that ``closed'' stands for ``variable-closed''.)
\end{lem}
\begin{proof}
  The main idea is to build an evaluation context 
  whose task is to forward messages from names to names
  according to the substitution , and to prove the
  equivalences  and . Because  is closed by evaluation contexts, we
  also have . Then we
  conclude by the transitivity of . 
  Refer to the proof of Fournet in~\cite[Lemma 4.14 of Chapter
  4]{Fournet98:PhD} for details. \forget{\qed}
\end{proof}
Then based on this property, the full congruence is also
guaranteed considering the fact that the essence of a guarded
  context is substitution.
\begin{thm}\label{th.wbcfull}
  Weak barbed congruence  is closed by application of any
  closed context.
\end{thm}
\begin{proof}
  Corollary of
  Theorem~\ref{th.pseq-cong} that we prove later on. \forget{\qed}
\end{proof}

Up to now, we have defined the weak barbed congruence to express the
equivalence of two closed processes. However, our purpose is to study
the correctness of a static transformation. Since static
transformations apply perfectly well to processes with free
variables of non-channel type, restricting ourselves to the world of
closed processes is not an option. In the next section, we will derive
an equivalence relation for open processes. But before getting into
the definition, let us first establish some up-to techniques on the
closed sub-set of the calculus. Such up-to techniques will be used
during the courses of proving upcoming lemmas and theorems.

\begin{defi}[Weak barbed congruence up to ]
\label{def.wbc-upto-equiv}
A binary relation  on closed processes is a weak barbed congruence
up to , iff  implies:
  \begin{enumerate}[(1)]
  \item for any closed evaluation context ,
     ( is closed
    under evaluation contexts up to );
  \item whenever , , such that  and , and vice versa ( is
    a reduction bisimulation up to );
  \item  implies  for any channel
    , and vice versa. ( preserves barbs.)
  \end{enumerate}
\end{defi}
As we did for plain weak barbed bisimulation
(Definition~\ref{def.joinpi.bisi})
in Lemma~\ref{lemma.joinpi.bisi}, we introduce the following
weakened conditions for checking weak barbed congruence up to~.
\begin{lem}\label{lemma.wbc-upto-equiv-alt}
  Let  be a binary relation on closed processes and  that satisfies the
  following three conditions for any processes  and  such that
  :
  \begin{enumerate}[\em(1)]
  \item for any closed evaluation context ,
    ;
  \item If , then , such that  and , and vice versa.
  \item  implies  for any channel ,
    and vice versa.
  \end{enumerate}
  Then  is a weak barbed congruence up to .
\end{lem}
\begin{proof}
  We check against the three clauses of
  Definition~\ref{def.wbc-upto-equiv}.
  \begin{enumerate}[(1)]
  \item The first clause is the same as clause  of
    Definition~\ref{def.wbc-upto-equiv}.
  \item We show:  We reason on the
    length of the derivation , written .

\paragraph{\bf Base case}\ , trivial.

\paragraph{\bf Induction case}\ As illustrated in the following
      diagram chase,
    \begin{center}
      \begin{picture}(0,0)\includegraphics{wbcuptoequiv-alt.eps}\end{picture}\setlength{\unitlength}{3947sp}\begingroup\makeatletter\ifx\SetFigFont\undefined \gdef\SetFigFont#1#2#3#4#5{\reset@font\fontsize{#1}{#2pt}\fontfamily{#3}\fontseries{#4}\fontshape{#5}\selectfont}\fi\endgroup \begin{picture}(2564,1812)(1465,-5287)
\put(2701,-3586){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(1501,-3586){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(1501,-4411){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(1501,-5236){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(3901,-3586){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(3901,-4411){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2701,-4411){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2251,-4411){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(1876,-4411){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(3451,-4411){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(3076,-4411){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2251,-5236){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(1876,-5236){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2701,-5236){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(3076,-5236){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\end{picture}     \end{center}
    we have . By induction
    hypothesis, we have , s.t. .  Following
    Lemma~\ref{struct-rule}, we have , too.  By
    applying hypothesis~ to  and , we also have
    . Then by Lemma~\ref{struct-rule} again, we
    have , too. To conclude, we have .


    The proof of the other direction is symmetric.
  \item We show: 
    We thus assume :
    
    By ~above, we get:
    
    By Lemma~\ref{lemma.barb}, we have . Applying
    hypothesis~ to  and~, we get
    . Then by Lemma~\ref{lemma.barb} again, we have
    . To conclude, we have  and
    , \ie .
    The proof of the other direction is symmetric.
  \end{enumerate}
\end{proof}

\begin{lem}\label{lemma.wbc-upto-equiv}
  If  is a weak barbed congruence up to , then
  .
\end{lem}
\begin{proof}
  We first show   , \ie  is
  a weak barbed congruence.
  \begin{enumerate}[(1)]
  \item  is closed under evaluation contexts. Given
    , there exist  and  such that
    .
    Let us name two properties:
    \begin{enumerate}
    \item  is closed under evaluation contexts;
    \item clause  of Definition~\ref{def.wbc-upto-equiv}.
    \end{enumerate}
    Then, for any closed evaluation context , we have:
    
    By transitivity of~, we conclude:
    
  \item  is a reduction bisimulation. We use
     clause~ of Definition~\ref{def.wbc-upto-equiv} and then
     Lemma~\ref{struct-rule} to reason by
     diagram chase as follows:
    \begin{center}
      \begin{picture}(0,0)\includegraphics{wbcuptoequiv.eps}\end{picture}\setlength{\unitlength}{3947sp}\begingroup\makeatletter\ifx\SetFigFont\undefined \gdef\SetFigFont#1#2#3#4#5{\reset@font\fontsize{#1}{#2pt}\fontfamily{#3}\fontseries{#4}\fontshape{#5}\selectfont}\fi\endgroup \begin{picture}(4346,987)(1390,-5287)
\put(2701,-4411){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2401,-4411){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2101,-5236){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2401,-5236){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2701,-5236){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2101,-4411){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(1801,-4411){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(1426,-4411){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(1801,-5236){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(1426,-5236){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(3001,-4411){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(3301,-4411){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(4426,-4411){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(4726,-4411){\rotatebox{360.0}{\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}}
\put(5026,-5236){\rotatebox{360.0}{\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}}
\put(4726,-5236){\rotatebox{360.0}{\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}}
\put(4426,-5236){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(5026,-4411){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(5701,-4411){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(5701,-5236){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(4126,-4411){\rotatebox{360.0}{\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}}
\put(3826,-4411){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(5326,-4411){\rotatebox{360.0}{\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}}
\put(5326,-5236){\rotatebox{360.0}{\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}}
\end{picture}     \end{center}
  \item  preserves barbs. Given ,
    we have , and the following statement,
    
  \end{enumerate}
  Then because     ,
  we conclude that   . \forget{\qed}
\end{proof}

A standard proof technique is then to consider weak barbed congruence up
to~.
However, as demonstrated in~\cite{SaMi92}, such a technique does not work
in general in weak settings.
Thus, we instead define another relation, where up to 
is performed on one side only.
This new relation is sound, as shown by the forthcoming
Lemma~\ref{lemma.wbc-upto-id}.

\begin{defi}[Weak barbed congruence up to \Id~\footnote{\Id stands for
    the identity relation on closed processes. Note that this relation
    is derived from  ``bisimulation up to almost-weak
    bisimulation'' in~\cite{SaMi92}, because \Id is included in
    almost-weak bisimulation, with some adjustments to the
    barbed setting.}]
\label{def.wbc-upto-id}
  A binary relation~ on closed processes is a weak barbed
  congruence up to \Id, iff  implies:
  \begin{enumerate}[(1)]
  \item for any closed evaluation context ,
     ( is closed under
    evaluation contexts up to );
  \item whenever , , such that  and ;
  \item whenever , , such that  and ;
    
    (The two clause above say that  is a reduction bisimulation up to
    \Id.)
  \item  implies  for any channel
    , and vice versa. ( preserves barbs.)
  \end{enumerate}
\end{defi}
Again, we first derive the following alternative conditions for
checking weak barbed congruence up to \Id.
\begin{lem}\label{lemma.wbc-upto-id-alt}
   is a binary relation on closed processes and  satisfies the
  following conditions for any processes  and  such that :
  \begin{enumerate}[(1)]
  \item for any closed evaluation context ,
    ;
  \item whenever , , such that  and ;
  \item whenever , , such that  and ;
  \item  implies  for any channel
    , and vice versa.
  \end{enumerate}
  Then  is a weak barbed congruence up to \Id.
\end{lem}
\begin{proof}
  We check against the clauses of Definition~\ref{def.wbc-upto-id}.
  \begin{enumerate}
  \item The first clause is the same.
  \item We show:  We reason on the
    length of the derivation , written .

\paragraph{\bf Base case}\ , trivial.

\paragraph{\bf Induction case}\ As illustrated in the following
      diagram chase, 
      \begin{center}
        \begin{picture}(0,0)\includegraphics{redbisiuptoid.eps}\end{picture}\setlength{\unitlength}{3947sp}\begingroup\makeatletter\ifx\SetFigFont\undefined \gdef\SetFigFont#1#2#3#4#5{\reset@font\fontsize{#1}{#2pt}\fontfamily{#3}\fontseries{#4}\fontshape{#5}\selectfont}\fi\endgroup \begin{picture}(1964,1812)(1765,-5287)
\put(2701,-3586){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(1801,-3586){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(1801,-4411){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(1801,-5236){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2701,-5236){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(3601,-3586){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2251,-4411){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2701,-4411){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(3601,-4411){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(3151,-4411){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(3601,-5236){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2326,-5236){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2101,-5236){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(3151,-5236){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\end{picture}       \end{center}
      we have . By induction
      hypothesis, we get , such that 
      and . That is, , such that .  By applying hypothesis~ to  and~,
      we have  such that  and
      .  Because , we also have  such that  and  --- remember
      that  is the largest weak barbed congruence and thus a
      reduction bisimulation.  We conclude by transitivity of~.
\item Symmetric of (2) above.
  \item We show: 
    We thus assume . That is, we have:
    
    By ~above, we get:
    
    Applying hypothesis~ to  and~, we get
    . Applying clause  of
    Definition~\ref{def.joinpi.bisi} to  and , we then get
    . To conclude, we have  and
    , \ie .  The proof of the
    other direction is symmetric.
  \end{enumerate}
\end{proof}

\begin{lem}\label{lemma.wbc-upto-id}
  If  is a weak barbed congruence up to \Id, then
  .
\end{lem}
\begin{proof}
  We first show   , \ie  is
  a weak barbed congruence.
  \begin{enumerate}[(1)]
  \item  is closed under evaluation contexts. Given
    , there exist  and  such that . Let us name two properties:
    \begin{enumerate}[(a)]
    \item  is closed under evaluation contexts;
    \item clause  of Definition~\ref{def.wbc-upto-id};
    \end{enumerate}
    Then, for any closed evaluation context , we have:
    
    Because   , we have 
     . Hence we have:
    
    And we conclude, by transitivity of~.
  \item  is a reduction bisimulation. We use clause~
    of Definition~\ref{def.joinpi.bisi}, clause  of
    Definition~\ref{def.wbc-upto-id}, clause  of
    Definition~\ref{def.joinpi.bisi}, and the transitivity of ,
    in the proof sketched by the following diagram:
    \begin{center}
      \begin{picture}(0,0)\includegraphics{wbcuptoid.eps}\end{picture}\setlength{\unitlength}{3947sp}\begingroup\makeatletter\ifx\SetFigFont\undefined \gdef\SetFigFont#1#2#3#4#5{\reset@font\fontsize{#1}{#2pt}\fontfamily{#3}\fontseries{#4}\fontshape{#5}\selectfont}\fi\endgroup \begin{picture}(4496,1212)(1390,-5512)
\put(2701,-5236){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2401,-5236){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2101,-5236){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(1801,-5236){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(1426,-5236){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(3001,-5236){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(3301,-5236){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(5776,-5236){\rotatebox{360.0}{\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}}
\put(4501,-5236){\rotatebox{360.0}{\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}}
\put(4801,-5236){\rotatebox{360.0}{\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}}
\put(5101,-5236){\rotatebox{360.0}{\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}}
\put(5401,-5236){\rotatebox{360.0}{\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}}
\put(4201,-5236){\rotatebox{360.0}{\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}}
\put(3901,-5236){\rotatebox{360.0}{\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}}
\put(4501,-4411){\rotatebox{360.0}{\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}}
\put(4801,-4411){\rotatebox{360.0}{\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}}
\put(5101,-4411){\rotatebox{360.0}{\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}}
\put(5401,-4411){\rotatebox{360.0}{\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}}
\put(5776,-4411){\rotatebox{360.0}{\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}}
\put(4201,-4411){\rotatebox{360.0}{\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}}
\put(3901,-4411){\rotatebox{360.0}{\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}}
\put(2701,-4411){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2401,-4411){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2101,-4411){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(1801,-4411){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(1426,-4411){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(3001,-4411){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(3301,-4411){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(1801,-5461){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(1426,-5461){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(3001,-5461){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(3301,-5461){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2401,-5461){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(5776,-5461){\rotatebox{360.0}{\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}}
\put(5401,-5461){\rotatebox{360.0}{\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}}
\put(4201,-5461){\rotatebox{360.0}{\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}}
\put(3901,-5461){\rotatebox{360.0}{\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}}
\put(4801,-5461){\makebox(0,0)[lb]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\end{picture}     \end{center}
  \item  preserves barbs. Given , we have
    , and the following statement,
    
  \end{enumerate}
  Then because     ,
  we conclude that   .  \forget{\qed}
\end{proof}

\subsubsection{Observational equivalence for open processes}
The approach we follow here is to lift the equivalence relation of
closed processes to open processes by closing up by all substitutions,
and we call the resulting relation \emph{open equivalence}.

Although both are ``open'', our open equivalence is
unrelated to the open bisimilarity of Sangiorgi
in~\cite{Sangiorgi96OpenBisi}. We use ``open'' to name our equivalence
relation because it relates
open terms. By contrast, ``open'' in open
bisimilarity emphasizes a characteristic of the bisimulation
definition, namely free names are open to equality throughout the
bisimulation game. From the perspective of where and when to apply
name substitutions, for open equivalence, we instantiate free names
(and variables) only at the beginning before we test the resulting (closed)
processes for weak barbed congruence.
On the contrary, in the case of open bisimilarity, such instantiation
happens at every co-inductive step.
\iffalse It is to be noticed
that our approach is similar to one that is often followed
for defining congruence relations from non-congruent
bisimilarity definitions: preliminary closure under all contexts.\fi


Another way to define equivalence relations on open terms could be to
adapt the semantics to symbolic transition system and to define a
symbolic barbed congruence like in~\cite{SymbolicBisi}.  Although the
symbolic method is claimed to be easier for analysis and verification,
we found open equivalence to be lighter and more intuitive.  As a
matter of fact, it is not uncommon to define functions extensionally,
\ie by considering application to all possible arguments.  Moreover,
as can be seen in Section~\ref{sec.correct}, our proofs remain
tractable.
\begin{defi}[Open equivalence ]
  \label{def.joinpi.pseq}
  Two processes  and  are open equivalent, written ,
  iff for any substitution  such that  and
   are closed, we have .
\end{defi}
As a corollary,  is closed by any substitution.
\begin{lem}\label{lemma.pseq-sub}
  
\end{lem}
\resetequationcounter
\begin{proof}
  We assume  and let  be a substitution. We need to
  prove that . That is,
  we need to prove that, for all closing substitution~, we
  have:

Thus, we need to prove that, for all closing substitution~, we
have:

where  stands for substitution composition, \ie
.
It remains to observe that
 closes both processes  and~,
and to apply the definition of~, before concluding
that statement~(\ref{lb.joinpi.pseq-subb}) above holds. \forget{\qed}
\end{proof}

We aim at proving that  is closed by any contexts
(Theorem~\ref{th.pseq-cong} below). To prove the theorem, we need the
following rather unusual lemma, to state the fact that although we
have introduced ``deterministic'' reduction into the process calculus
by extending it with the \kwd{match} construct, this kind of
determinism does not impact process equivalence.
\begin{lem}\label{lemma.joinpi.reduces}
  We say a closed process  \emph{deterministically reduces} to ,
  iff for all  such that , we have .
  For any such pair of closed processes  and , we have  .
\end{lem}
\begin{proof}
  Let  be the relation  for all closed definitions
  , closed processes  and , and all  pairs such
  that  deterministically reduces to~. We prove that 
  is a weak barbed congruence up to~.
  \begin{enumerate}[]
  \item By definition,  is closed by evaluation contexts
    up to  (\ie Lemma~\ref{lemma.wbc-upto-equiv-alt}.(1)).
  \item We show that  preserves barbs (\ie
    Lemma~\ref{lemma.wbc-upto-equiv-alt}.(3)). We omit the (trivial) discussion
    of pairs of identical processes  in .
    We show that . We distinguish the
    cases that make  hold.
    \begin{enumerate}[]
    \item .
      Obviously reduction cannot erase a barb  (), \ie
      we have .
      Hence, we have .
    \item . Trivial.
    \end{enumerate}
    As to the opposite direction \ie
    , it holds trivially
    because .
  \item We show  to be a reduction bisimulation up to 
    (\ie Lemma~\ref{lemma.wbc-upto-equiv-alt}.(2)). We omit the
    trivial case of pairs of identical processes in , that is,
    we only consider process pairs of form: .
    \begin{enumerate}[]
    \item If the reduction of the left part is caused by a reduction
      on  alone or by the interaction between  and , yielding
      , then the right part can perform
      the same reduction step, yielding .
      The resulting two processes are still in relation  with 
      being . Vice versa.
    \item If the reduction of the left part is caused by a reduction
      on  alone, then, because  deterministically reduces to ,
      the resulting process
      is  (up to ).
      Thus, the right part simulates
      with no reduction and  satisfies
      relation  with itself.
    \item If the reduction of the left part is caused by the
      interaction between  and , then we must have  where  is a reaction
      rule in  and the resulting process is
      . Because 
      does not reduce by itself and  deterministically reduces to
      , we have  and 
      deterministically reduces to . Therefore, the right part
      simulates by an identical reduction and gives
      . The resulting two
      processes are still in relation  with  being
      ,  being , and  being .
    \item If the reduction of the right part is caused by a reduction
      on  itself or by the interaction between  and , then
      the left part can always simulate the reduction by first
      reducing  to
      .
    \end{enumerate}
  \end{enumerate}
  Following the analysis above,  is a weak barbed congruence up to
  . Besides we have 
  Moreover, by the proof of Lemma~\ref{lemma.wbc-upto-equiv},
  relation  is a weak barbed congruence. Hence we conclude
  . \forget{\qed}
\end{proof}

\begin{thm}\label{th.pseq-cong}
  The open equivalence  is a full congruence.
\end{thm}
\resetequationcounter
\begin{proof}
  We demonstrate  is closed by 1. evaluation contexts, 2.
  definition contexts, and 3. pattern matching contexts.
  In the proof, we locally use , , , , , , , ,
  ,  to denote various processes.

  \paragraph{\bf 1. Closed by evaluation contexts:
    .}  We show:  For any substitution  such
  that  and  are
  closed, we need to prove .  We write  as
   and  as
  , where ,
  ,  are closed and  is
   minus the (possible) bindings for the channel names bound
  by  in . By hypothesis , we have
  .  Then, 
  being a closed evaluation context, we conclude, by definition
  of~.
  
  \paragraph{\bf 2. Closed by definition contexts: .}
  We show:  For
  any substitution  such that  and
   are closed, we need to prove:  namely,
  
  where  is  minus the (possible) bindings for the
  channel names defined in  (\ie
  ), and  is  minus
  the (possible) bindings for the variables of~.  Notice that,
  by contrast with the subcomponents  and~ that
  are closed, the processes  and  may \emph{not}
  be closed, since some of the variables in~ may be of an
  algebraic type.  Nevertheless, by hypothesis  and
  Lemma~\ref{lemma.pseq-sub}, we have .

  Then, we build the following relation  on closed processes:
  
  We analyze the following three aspects of : closure
  by closed evaluation contexts; preserving barbs; and reduction
  bisimulation.

  \begin{enumerate}[]
  \item  is closed by closed evaluation contexts up to 
    (\ie Lemma~\ref{lemma.wbc-upto-id-alt}.(1)). For any closed
    , with necessary -conversions left
    implicit, we have:
    
    where , because  is preserved by
    the closed evaluation context .
   
    \def\C#1{\dd[#1]}
  \item  preserves barbs (\ie
    Lemma~\ref{lemma.wbc-upto-id-alt}.(4)).  We write  for
    the closed process .  Since
     is a symmetric relation, we only need to prove:
    
    Because  implies  and , we also have .
    Moreover, because  is a closed evaluation context,
    and by hypothesis , we have
    
    By clause  of Definition~\ref{def.joinpi.bisi}, we finally get
    .

  \item  is a reduction bisimulation up to \Id (\ie
    Lemma~\ref{lemma.wbc-upto-id-alt}.(2) and (3)).  We first prove
    the following statement.  For any two  and~, we
    have:
        
    There are three subcases, depending on the nature of the reduction
    to~. 
\begin{enumerate}[(1)]
    \item  and . Then
    , with obviously ,
    since .

    \item  and
    . Then .
    Notice that  and  are
    closed. Then, from , we get ,
    and thus  .  That is, we get .

    \item ,  has
    form , and . Then . And we conclude,
     as we did in case 1 above.
\end{enumerate}\medskip

Moreover, from equivalence \eqref{st.barb1}
and since  is a bisimulation, we have:
    
    Combining both statements \eqref{st.red2} and~\eqref{st.red1},
    we get:
    
    The proof of the other direction is by symmetry.
  \end{enumerate}

  Following the analysis above,  is a weak barbed congruence up to
  \Id, hence by Lemma~\ref{lemma.wbc-upto-id}, .
  Obviously, the two processes of statement
  \eqref{lb.joinpi.pseq-cong1} are related by~. Therefore,
  \eqref{lb.joinpi.pseq-cong1} holds.  In other words,  is
  closed by any definition context.

  \paragraph{\bf 3. Closed by pattern matching contexts: }
  We show: 
  To establish the right part, we need to show: 
  for all , s.t.  and 
  are closed. Namely,
  
  where  is  minus the (possible) bindings for the
  variables of . Notice that  is closed, while
   and  may not be.

  By the semantics of ML pattern matching,  deterministically reduces to either
   or , depending
  on the value of .  Process  is the th
  guarded process () in this pattern matching,  and
   stand for the substitutions that originate from algebraic
  matching.  Notice that  and
   now are closed processes.  We have the similar
  statement for .  Therefore, by
  Lemma~\ref{lemma.joinpi.reduces}, we have either:
  
  Obviously we have . Moreover, since , we get . Then, by the transitivity
  of~ and, either by \eqref{st.matchl1}--\eqref{st.matchr1}, or
  by \eqref{st.matchl2}--\eqref{st.matchr2}, we conclude that the
  statement~\eqref{lb.joinpi.pseq-cong2} holds.

  Additionally, in the case where  matches none of the patterns
  in~\eqref{lb.joinpi.pseq-cong2}, both processes are blocked
  and are  to the null process . \forget{\qed}
\end{proof}

There is still a good property worth noticing: for the closed
subset of the applied join-calculus,  the equivalences  and 
coincide. This is straightforward by the definition of~
and by Lemma~\ref{lemma.wbcsub}.
Then, Theorem~\ref{th.wbcfull} follows as a corollary.

\section{Transforming pattern arguments into ML pattern matching}
\label{sec.trans-idea}

The extension of the join calculus that we have presented up to now
remains quite  simple, in particular as regards chemical
semantics. However, an efficient implementation is more involved. Our
approach is to first transform the extended join definitions into
ordinary ones plus ML pattern matching, then reuse the existing
implementation of join. In this section, we explain informally the
key ideas of the transformation.

The extended join-pattern matching in applied join requires to test
message contents against pattern arguments, while the ordinary
join-pattern matching in join is only capable of testing message
presence.  Our idea is to separate algebraic pattern testing from
join-pattern synchronization, and to perform the former operation by
using ML pattern matching.  To avoid inappropriate message
consumption, message contents are tested first.  Let us consider the
following join definition where channel~\lst"x" has two pattern
arguments:
\begin{lstlisting}{Join}
def x() & y() |> 
 or x() & y() |> 
\end{lstlisting}
We refine channel~\lst"x" into more precise ones, each
of which carries the instances of patterns~ or :
\begin{lstlisting}{Join}
def x() & y() |> 
 or x() & y() |> 
\end{lstlisting}
Then, we add a new reaction rule to dispatch the messages on
channel~\lst"x" to either \lst"x" or~\lst"x":
\begin{lstlisting}{Join}
 or x(z) |> match z with
            |  -> x(...)
            |  -> x(...)
            | _ -> 
\end{lstlisting}
Note that the null process is used in the last matching rule to discard
messages that match neither  nor~.

The simple compilation above works perfectly, as long as  and
 are incompatible. Unfortunately, it falls short when 
and  have common instances.
Consider the situation where there is a message pending
on channel~, none on~, and also a message~
on~ where  is a common instance of patterns  and .
Then, following the first match policy, the
deterministic ML pattern matching can only dispatch  to the
refined channel .
As a result, the guarded process~ is not
triggered, whereas it could have been.\footnote{Given
our implementation ``limited fairness guarantee'', it
can be argued that~ should be~triggered.}
To tackle this problem, further
refinements are called for according to the following cases.
\begin{enumerate}[]
\item If , (but ), that
  is if all instances of  are instances of , then, to
  get a chance of meeting its instances, pattern~ must come
  first:
\begin{lstlisting}{Join}
 or x(z) |> match z with
           |  -> x(...)
           |  -> x(...)
           | _ -> 
\end{lstlisting}
But now, channel  does not carry all the possible
instances of pattern~ any more, instances shared by
pattern~ are dispatched to~. As a consequence,
the actual transformation of the initial reaction rules is as follows:
\begin{lstlisting}{Join}
def x() & y() |> 
 or x() & y() |> 
 or x() & y() |> 
\end{lstlisting}
Observe that nondeterminism is now more explicit: an instance of
 sent on channel~\lst"x" can be consumed by either the second
or the third reaction rule to trigger either  or .  We can
shorten the new definition a little by using \lst"or" in
join~patterns:
\begin{lstlisting}{Join}
def (x() or x()) & y() |> 
 or x() & y() |> 
\end{lstlisting}
Here the disjunctive composition () in join patterns
works as syntactic sugar, in the following sense: 


\item If , then matching by their representative
is enough:
\begin{lstlisting}{Join}
def x() & y() |> 
 or x() & y() |> 
 or x(z) |> match z with
            |  ->  x()
            | _ -> 
\end{lstlisting}

\item Finally, if neither  nor  holds, with  and  being nevertheless compatible,
  then an extra matching by pattern  is needed:
\begin{lstlisting}{Join}
def (x() or x()) & y() |> 
 or (x() or x()) & y() |> 
 or x(z) |> match z with
           |  -> x()
           |  -> x() |  -> x()
           | _ -> 
\end{lstlisting}
Note that the relative order of  and  is irrelevant here.
\end{enumerate}

In the transformation rules above, we paid little attention to
variables in patterns, by writing
\lst"x"\lst"("\lst")".  We now show variable
management by means of the concurrent stack example.  Here, the
relevant patterns are  and  and we are in the case where  (and  because of instance empty
list~).  Our idea is to let dispatching focus on instance
checking, and to perform variable binding after synchronization:
\begin{lstlisting}{Join}
def pop(r) & State(z) |> match z with x::xs -> r(x) & State(xs)
 or push(v) & (State(z) or State(z)) |> match z with ls -> State(v::ls)
 or State(z) |> match z with
                | _::_ -> State(z)
                | _ -> State(z)
\end{lstlisting}
One may believe that the matching of the pattern \lst"x::xs" needs to
be performed twice (once in the dispatcher, once in the first reaction
rule), but it is not necessary.  The compiler should know that the
matching of \lst against \lst in the first reaction rule
cannot~fail, and as a consequence, no test needs to be performed here,
only the binding of the pattern variables.
See Section~\ref{subsec.optimization} for details.

 \section{The compilation \texorpdfstring{}{scheme}}
\label{sec.trans}

We formalize the intuitive idea described in Section~\ref{sec.trans-idea} as a
transformer , which transforms a join definition  with respect to
channel . The algorithm essentially works by constructing the meet
semi-lattice of the formal pattern arguments of channel~ in , modulo
pattern equivalence , with the less precise relation  being
the partial order.  Moreover, we visualize the lattice as a Directed Acyclic
Graph (DAG), namely, vertices as patterns, and edges representing the partial
order.  If we reason more on instance sets than on patterns, this structure is
quite close to the ``subset graph'' of~\cite{pritchard99computing}.
 
\medskip\noindent \textbf{Algorithm :}
Given , a join definition, where  is a channel defined by~.
\begin{enumerate}[\ ]
\item {\bf Step 0: Preprocess.} \hspace*{1cm}
  \begin{enumerate}[(1)]
  \item Collect all the pattern arguments of channel  into the
    sequence: 
  \item Let  be formed from  by replacing all
    variables by wildcards~``'' and taking the  of all
    equivalent patterns; thus  is a sequence of pairwise
    nonequivalent patterns.
  \item Perform exhaustiveness check on , if not exhaustive,
    issue a warning.
  \item
\begin{enumerate}[\ ]
    \item {\bf IF:}\ There is only one pattern in , and that
       is exhaustive
    \item {\bf THEN:}\ goto Step 5. (In that case, no dispatching is needed.)
    \end{enumerate}
  \end{enumerate}
\item {\bf Step 1: Closure by least upper bound.} \hspace*{1cm} \\
  For any pattern~ and pattern sequence , we define  as the sequence
  , where the \,s are the patterns from
   that are compatible with .

  We also define function~, which takes a pattern sequence~
  as argument and returns a pattern sequence.
  \begin{enumerate}[\ ]
  \item {\bf IF:}\  is empty
  \item {\bf THEN:}\ 
  \item {\bf ELSE:}\ Decompose  as  and state 
  \end{enumerate}
  Compute the sequence .  It is worth noticing
  that  is the sequence of all valid patterns
  , with , and , where we
  decompose  as .
\item {\bf Step 2: Up to equivalence.}\ \hspace*{1cm} \\
  As in Step , build  by taking the  of all
  equivalent patterns in .
\item {\bf Step 3: Build DAG:} \hspace*{1cm} \\
  Corresponding to the semi-lattice , build a
  directed acyclic graph .
  \begin{enumerate}[(1)]
  \item .
  \item For each pattern~ in , add a new vertex 
    into  and annotate the vertex with~.
  \item , with annotations
     and  respectively, if ,
    then add an edge from  to~ into~.
  \end{enumerate}
\item {\bf Step 4: Add dispatcher.} \hspace*{1cm} \\
  Following one topological order, the vertices of  are indexed as
  , correspondingly with annotations . We extend the join definition  with a
  dispatcher on channel  of the form:  \lst"|> match" 
  \lst"with" , where  is a fresh variable and  is
  built as follows:
  \begin{enumerate}[(1)]
  \item Let  ranges over . Following the
    topological order above, for all vertices~ in  append a
    rule ``'' to~,
    where  is a fresh channel name assigned to
    vertex~ whose annotation is . Such fresh channels are here
    for the purpose of carrying messages originally sent to  then
    forwarded by the dispatcher, hence are also referred to as
    \emph{forwarding channels}.
  \item If  is not exhaustive, then add a rule ``'' at the end.
  \end{enumerate}
\item {\bf Step 5: Rewrite reaction rules.} \hspace*{1cm} \\
  For each reaction rule defining channel  in :
  , we rewrite it according to
  the following policy. Let , where
   is a fresh variable.
  \begin{enumerate}[\ ]
  \item {\bf IF:} coming from Step 0
  \item {\bf THEN:} rewrite to
    
  \item {\bf ELSE:}\hspace*{1cm}
    \begin{enumerate}[(1)]
    \item Let  be the unique vertex in , s.t.  its
      annotation .
    \item We collect all the predecessors of  in , and
      we record the indices of them, together with , into a set
      that we note .
    \item Rewrite to ,
      where  is the generalized  construct of
      join patterns.
    \end{enumerate}
    \end{enumerate}
    \end{enumerate}


Given a join definition~, we note  , that is we order the channel names arbitrarily.
To transform , we apply . And the compilation of
processes  is inductively defined as follows: 
Observe that the compilation preserves the interface of join
definitions. Namely, it only affects the join definitions, never
suppressing a channel, while message sending remains the same.

\section{Example of compilation}
\label{sec.example}

Given the following join definition for an enriched integer stack:
\begin{lstlisting}{Join}
def push(v) & State(ls) |> State (v::ls) 
 or pop(r) & State(x::xs) |> r(x) & State(xs)
 or insert(n) & State(0::xs) |> State(0::n::xs)
 or last(r) & State([x]) |> r(x) & State([x])
 or swap() & State(x::x::xs) |> State(x::x::xs)
 or pause(r) & State([]) |> r()
 or resume(r) |> State([]) & r()
\end{lstlisting}
The \lst"insert" channel inserts an integer as the second topmost
element, but only when the topmost element is . The \lst"last"
channel gives back the last element in the stack, keeping the stack
unchanged.  The \lst"swap" channel exchange the topmost two elements
in the stack.  The \lst"pause" channel temporarily freezes the stack
when it is empty, while the \lst"resume" channel brings the stack back
into work.  We now demonstrate our transformation with respect to channel
\lst"State".
\def\one{1}
\def\two{2}
\def\three{3}
\def\four{4}
\def\five{5}
\def\six{6}
\def\seven{7}
\def\eight{8}
\begin{enumerate}[\ ]
\item {\bf Step 0:}\ We collect the pattern arguments of channel \lst"State"
  into :  We drop
  the last equivalent  pattern during the up to equivalence
  substep~0.2, and we get:  Additionally,
   is exhaustive (pattern  alone covers
  all possibilities). Note that in the demonstration of this example,
  we sometimes keep variable names in patterns for readers'
  convenience. They are not necessary and are actually all replaced
  by~``'' in the implementation.
\item {\bf Step 1,2:}  extends  with all
  possible least upper bounds. Then we form  from 
  by taking the  of all equivalent patterns.  
  Note that the
  last two patterns are new, where: 
  
\item {\bf Step 3:}\ We build the semi-lattice , see
Figure~\ref{fig.ptjoin.dag}.
\begin{figure}[ht]
\centering
\begin{picture}(0,0)\includegraphics{dag.eps}\end{picture}\setlength{\unitlength}{3947sp}\begingroup\makeatletter\ifx\SetFigFont\undefined \gdef\SetFigFont#1#2#3#4#5{\reset@font\fontsize{#1}{#2pt}\fontfamily{#3}\fontseries{#4}\fontshape{#5}\selectfont}\fi\endgroup \begin{picture}(3772,1737)(1087,-8893)
\put(4844,-7368){\makebox(0,0)[lb]{\smash{{\SetFigFont{9}{10.8}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}    }}}}}
\put(4844,-7562){\makebox(0,0)[lb]{\smash{{\SetFigFont{9}{10.8}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}    }}}}}
\put(4844,-7754){\makebox(0,0)[lb]{\smash{{\SetFigFont{9}{10.8}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}    }}}}}
\put(4844,-7947){\makebox(0,0)[lb]{\smash{{\SetFigFont{9}{10.8}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}    }}}}}
\put(4844,-8140){\makebox(0,0)[lb]{\smash{{\SetFigFont{9}{10.8}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}    }}}}}
\put(4844,-8333){\makebox(0,0)[lb]{\smash{{\SetFigFont{9}{10.8}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}    }}}}}
\put(4844,-8525){\makebox(0,0)[lb]{\smash{{\SetFigFont{9}{10.8}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}    }}}}}
\put(4844,-8718){\makebox(0,0)[lb]{\smash{{\SetFigFont{9}{10.8}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}    }}}}}
\put(4137,-8333){\makebox(0,0)[lb]{\smash{{\SetFigFont{9}{10.8}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2337,-8333){\makebox(0,0)[lb]{\smash{{\SetFigFont{9}{10.8}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2337,-7818){\makebox(0,0)[lb]{\smash{{\SetFigFont{9}{10.8}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(3237,-8847){\makebox(0,0)[lb]{\smash{{\SetFigFont{9}{10.8}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2827,-7255){\makebox(0,0)[lb]{\smash{{\SetFigFont{9}{10.8}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{1,0,0}}}}}}
\put(1773,-7255){\makebox(0,0)[lb]{\smash{{\SetFigFont{9}{10.8}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{1,0,0}}}}}}
\put(1308,-7818){\makebox(0,0)[lb]{\smash{{\SetFigFont{9}{10.8}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(3055,-7833){\makebox(0,0)[lb]{\smash{{\SetFigFont{9}{10.8}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\end{picture} \caption{The semi-lattice of patterns and the topological order}\label{fig.ptjoin.dag}
\end{figure}
\item {\bf Step 4:}\ One possible topological order of the vertices is also
  given at the right of Figure~\ref{fig.ptjoin.dag}. Following that
  order, we build the dispatcher on channel~\lst"State".
\begin{lstlisting}{Join}
  or State(z) |> match z with
                 | 0:::: -> State(z)
                 | [0] -> State(z)
                 | :::: -> State(z)
                 | 0:: -> State(z)
                 | [] -> State(z)
                 | :: -> State(z)
                 | [] -> State(z)
                 |  -> State(z)
\end{lstlisting}
  where , ,  are the
  fresh forwarding channels.
\item {\bf Step 5:}\ We rewrite the original reaction rules.  As an example,
  consider the third reaction rule for the \lst"insert" behavior: the
  pattern in \lst"State(0::xs)" corresponds to vertex  with
  annotation  in the graph, which has two
  predecessors: vertex  with annotation
   and vertex  with annotation
  .  Therefore, the reaction rule is rewritten to:
\begin{lstlisting}{Join}
insert(n) & (State(z) or State(z) or State(z)) 
            |> match z with 0::xs -> State(0::n::xs)
\end{lstlisting}
where  is a fresh variable.
\end{enumerate}
As a final result of our transformation, we get the disjunction of the
following rules and of the dispatcher built in Step 4.

\begin{lstlisting}{Join}
def push(v) & (State(z) or  or State(z)) 
              |> match z with ls -> State (v::ls) 
 or pop(r) & (State(z) or  or State(z))
              |> match z with x::xs -> r(x) & State(xs)
 or insert(n) & (State(z) or State(z) or State(z))
              |> match z with 0::xs -> State(0::n::xs)
 or last(r) & (State(z) or State(z))
              |> match z with [x] -> r(x) & State([x])
 or swap() & (State(z) or State(z))
              |> match z with x::x::xs -> State(x::x::xs)
 or pause(r) & State(z) |> match z with [] -> r()
 or resume(r) |> State([]) & r()
\end{lstlisting}

\section{Correctness}
\label{sec.correct}

A program written in the applied join calculus of
Section~\ref{sec.joinpi} is a process . The compilation 
replaces all the join definitions  in  by , where . To guarantee the
correctness, we require the programs before and after the compilation
be open equivalent. Namely, the following theorem should hold.
\begin{thm}\label{th.joinpi.correct}
  For any process , .
\end{thm}
\begin{proof}
  By structural induction on processes. Because  is a full
  congruence and a transitive relation, it suffices to prove one step
  of the compilation, that is,  is correct (see
  Lemma~\ref{lemma.Y} below). \forget{\qed}
\end{proof}
\begin{lem}\label{lemma.Y}
  For any join definition , channel name , and
  process , we have:
  
\end{lem}
This lemma is crucial to the correctness of the compilation. \iffalse
The proof relies on the properties of the meet semi-lattice
constructed from the pattern arguments. In particular the proof
exploits the deterministic semantics of the ML pattern matching in the
dispatcher, which is built following the topological order of the
lattice. \fi We elaborate the proof in the coming sections.
First, we recall the notations of algorithm  in
Section~\ref{subsec.Yx}. Then, we discuss the properties of the
dispatcher built by  in Section~\ref{subsec.dispatcher}. Finally,
we prove Lemma~\ref{lemma.Y} in Section~\ref{subsec.prooflemmaY}.

\subsection{Summary of notations}
\label{subsec.Yx}

We summarize the connection between the input and the output of .
For simplicity, we omit the  superscripts everywhere.
According to the algorithm given in Section~\ref{sec.trans},
there are two cases during the procedure of , chosen at the end
of Step 0:
\paragraph{\bf Case ``jump''}\ The case where Steps 1 to 4 are skipped. Then,
  for any reaction rule of the form 
  of , , the pattern  is irrefutable, namely,
  . And in , we have the corresponding
  reaction rule
  , where 
  is fresh.
\paragraph{\bf Case ``go through''}\ The general case. We recall the notations
  of the DAG  built by the algorithm.  has  vertices,
  and following the topological order, the vertices are indexed as
   with pattern annotations .  Each vertex is
  also assigned a fresh forwarding channel, written .
  
  For any reaction rule of the form
   of , , there
  exists a unique vertex in  called , such that its
  annotation . We use 
  to record the indices of the predecessors of  as well as
  . Note that we have  iff
  . In , we have a corresponding reaction
  rule as , where the variable  is fresh.
  Moreover, we add a dispatcher on channel  into  as:
\begin{lstlisting}{Join}
x(z) |> match z with
        |  -> 
        | 
        |  -> 
        | _ ->               (* if non-exhaustive *) 
\end{lstlisting}
where  is a fresh variable.


\subsection{Property of the dispatcher}
\label{subsec.dispatcher}

We go on to discuss the property of the dispatcher built during the
transformation on channel~. Let  range over closed expressions,
that is over values. Modulo pattern equivalence , the patterns
of the dispatcher () are all the least upper
bounds of the pattern arguments of channel  in the original~
(). Thus, the \,s and the \,s
admit the same instances: .  As an immediate
consequence, on one hand, for the set of values that do not match any
of the original~\,s, written , the values of  do not match
any  either, and those values are silently eaten by the
dispatcher. On the other
hand, given any value~ such that there exists at least one 
with , then the dispatcher must forward  onto
one of the forwarding channels. More precisely, the following lemma holds.
\begin{lem}\label{lemma.joinpi.dispatcher}
  For any value~ that is an instance of some original pattern
  argument~, the dispatcher forwards~ to the forwarding
  channel assigned to a vertex in , whose index belongs to
  .
\end{lem}
\begin{proof}
  We thus assume .  Let  be the set of indices  and . Let
   be the least upper bound of the patterns in
  , written  ( exists, since
   is non-empty).  By steps 1--3 of
  the compilation algorithm~, there must exist some vertex denoted by
   in  with annotation .    
  The dispatcher forwards message~ onto the forwarding
  channel~, for the following two
  reasons.
  \begin{enumerate}[(1)]
  \item Value~ is an instance of .
  \item No pattern of the dispatcher that appears before
   admits~ as an instance.
  Namely, any pattern of the dispatcher ,
  such that  must be the least upper
  bound of a subset of~. Then, since the
  patterns of the dispatchers are ordered topologically (with
  precision order ),  must be the foremost
  pattern in the dispatcher which has  as an instance.
  Namely, precision order  applied to least upper bounds is
  reverse set inclusion applied to instance sets.
  \end{enumerate}
  Moreover, because  and , we have .
  Thus, by definition of~, we have . \forget{\qed}
\end{proof}

In the following, given some value~, we write  for the forwarding
channel to which  is sent by the dispatcher.  Using the new notation,
Lemma~\ref{lemma.joinpi.dispatcher} is reformulated as follows: if , then  exists and we have .

\subsection{Proof of Lemma~\ref{lemma.Y}}
\label{subsec.prooflemmaY}
\forget{Now we are ready to prove Lemma~\ref{lemma.Y}.}
\resetequationcounter
\begin{proof}
  Following the
  definition of , we should prove , for any closing substitution~.
  In other words, since ,
  we should prove:
  
  where  is  minus the (possible) bindings of the
  variables of .
  Notice that all subcomponents ,  and
   are closed. 
  Hence, to prove
  \eqref{dagger}, it suffices to prove that  is correct for
  closed terms (Lemma~\ref{lemma.joinpi.Y.close} below). \forget{\qed}
\end{proof}

\begin{lem}\label{lemma.joinpi.Y.close}
  For any closed join definition , channel name , and
  closed process , we have:
  
\end{lem}
\resetequationcounter
\begin{proof}
  There are two subcases.
  \paragraph{\bf Case ``go through''}
  We construct the following relation :  Above, process~ and
  definition~ range respectively over closed processes and closed
  definitions; while  and  are particular.  Dissect the
  structure of  as: 
  We define  and  to be:
  
  We note  the generalized parallel composition.
  Note that processes  and   are (implicitly) parameterized by
  the multisets of substitutions
   and  , and by the multiset of values~.
  In the definition of~, ,  and  range over all
  appropriate multisets.
  More precisely, given any reaction rule
   from ,
  we note  any (closed) substitution on domain .
  Then,  stands for any multiset of such substitutions~.
  Similarly, let  be a (closed) substitution on domain
  .
  Moreover, for any such~, let
   be  (the restriction of
   on domain ), and  be
  . Because ,
  the substitution  is the sum of  and ,
  written , and we further
  require  .
  Then,  is any multiset of such substitutions~.
  Finally,  is a multiset of elements from~.
    \begin{figure}
\centering
        \begin{picture}(0,0)\includegraphics{bisi.eps}\end{picture}\setlength{\unitlength}{3947sp}\begingroup\makeatletter\ifx\SetFigFont\undefined \gdef\SetFigFont#1#2#3#4#5{\reset@font\fontsize{#1}{#2pt}\fontfamily{#3}\fontseries{#4}\fontshape{#5}\selectfont}\fi\endgroup \begin{picture}(2446,3184)(1841,-6481)
\put(4248,-3444){\makebox(0,0)[b]{\smash{{\SetFigFont{9}{10.8}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(1881,-3444){\makebox(0,0)[b]{\smash{{\SetFigFont{9}{10.8}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(4248,-5879){\makebox(0,0)[b]{\smash{{\SetFigFont{9}{10.8}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(4248,-6421){\makebox(0,0)[b]{\smash{{\SetFigFont{9}{10.8}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(1881,-5879){\makebox(0,0)[b]{\smash{{\SetFigFont{9}{10.8}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(4248,-3849){\makebox(0,0)[b]{\smash{{\SetFigFont{9}{10.8}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(4248,-5473){\makebox(0,0)[b]{\smash{{\SetFigFont{9}{10.8}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(4248,-4932){\makebox(0,0)[b]{\smash{{\SetFigFont{9}{10.8}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(1881,-5473){\makebox(0,0)[b]{\smash{{\SetFigFont{9}{10.8}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(4248,-4391){\makebox(0,0)[b]{\smash{{\SetFigFont{9}{10.8}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(1881,-3849){\makebox(0,0)[b]{\smash{{\SetFigFont{9}{10.8}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\end{picture} \caption{Reduction chasing in case ``go through''}\label{fig.joinpi.bisi}
    \end{figure}

  Intuitively, we use  and  to bridge the differences
  caused by  and . More specifically: a message
   may be forwarded to
   by the dispatcher in
  ; furthermore, if a guarded process  is
  triggered from , then from , we have the corresponding
  guarded process
  
  triggered; finally, a message on channel  with a non-matching
  content, that is from , will be eaten by .

  We analyze the following three aspects of : closure by (closed)
  evaluation contexts; reduction bisimulation; and preservation of barbs.

  \begin{enumerate}[]
  \item  is closed by closed evaluation contexts up to 
    (\ie Lemma~\ref{lemma.wbc-upto-equiv-alt}.(1)). For any closed
    evaluation context , we have:
    
    where , so that . Therefore, we have
    .
  \item  is a reduction bisimulation (\ie a special case of
    Lemma~\ref{lemma.wbc-upto-equiv-alt}.(2) because the identity in
    included in~). We only detail the nontrivial cases.
    \begin{enumerate}[(1)]
    \item If there is a message  in , the
      right part can forward it to a message
       by the dispatcher
      in . This reduction is simulated in the left part by no
      reduction, and we add the new substitution  into
      .
    \item\label{2} Similarly, if there is a message  in ,
      for some , the right part can eat the message by
      the dispatcher in . This reduction is simulated by no
      reduction in the left part and we add  into .
    \item If a reduction according to the reaction rule
       consumes a molecule
       in the left part, for
      some  (\ie 
      occurs in ) and  from , with
      ; it can be simulated by consuming
       in
      the right part, using the corresponding reaction rule
      , because 
      (Lemma~\ref{lemma.joinpi.dispatcher}). The derivatives are still
      in , with  shrinking to ,
      and  expanding to . We
      assume -conversion when necessary to guarantee
      .  Vice versa.
    \item Similar to the previous case but this time the left part
      consumes a molecule ,
      where  is not from . Then, the right part
      simulates this reduction by first forwarding the message
       to the message
       as in case~\ref{2},
      then consuming the molecule
      .
       expands to .
    \item The
      
      in  of the right part can be reduced to
       by the semantic rule
      \rln{Match}.  Because we have
      ,
      the result of the reduction equals to
      , that is
      . This reduction is simulated by no reduction in
      the left part.  However, the process  becomes
      , and  shrinks to
      .
    \item If a reduction involves  from  of the left
      part, for some , it can be simulated by first
      reducing the correspondent
      
      from  into  as in the previous case.
    \end{enumerate}
      Figure~\ref{fig.joinpi.bisi} summarizes the various cases we
      just examined, where thick lines
      express the ~relation.
    \item  preserves barbs (\ie
      Lemma~\ref{lemma.wbc-upto-equiv-alt}.(3)). We demonstrate
       and vice
      versa. We distinguish the cases that make
       hold.
    \begin{enumerate}[(1)]
    \item . We have . Because all
      variables in  are fresh, we also
      have . According to the structure of ,
      we must have  for some .
      Then in , we have 
      reduces to  and . That is,
      ,
      \ie , \ie
      .
    \item . Obvious.
    \end{enumerate}
    The proof of the other direction, \ie
    , is obvious since the
    only case for
     is when
    .

    Following the analysis above,  is a weak barbed congruence up
    to . By Lemma~\ref{lemma.wbc-upto-equiv}, we have  is a
    weak barbed congruence.

    Let ,  and  be empty sets. We have the two
    processes of Lemma~\ref{lemma.joinpi.Y.close} satisfy relation
    , hence . That is, we proved that
    Lemma~\ref{lemma.joinpi.Y.close} holds for case ``go through''.
  \end{enumerate}
    \begin{figure}
\centering
        \begin{picture}(0,0)\includegraphics{bisi2.eps}\end{picture}\setlength{\unitlength}{3947sp}\begingroup\makeatletter\ifx\SetFigFont\undefined \gdef\SetFigFont#1#2#3#4#5{\reset@font\fontsize{#1}{#2pt}\fontfamily{#3}\fontseries{#4}\fontshape{#5}\selectfont}\fi\endgroup \begin{picture}(2408,1674)(2053,-5150)
\put(4379,-3956){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(4379,-3611){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2068,-3611){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2068,-3956){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2101,-5086){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(4426,-5086){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(4426,-4561){\makebox(0,0)[b]{\smash{{\SetFigFont{8}{9.6}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\end{picture} \caption{Reduction chasing in case
          ``jump''}\label{fig.joinpi.bisi2}
    \end{figure}
  \paragraph{\bf Case ``jump''} 
  We build another relation , with  and  defined as
  follows:
  
  and we summarize the property of reduction bisimulation by the diagram of
  Figure~\ref{fig.joinpi.bisi2}. \forget{\qed}
\end{proof}

\section{Implementing applied join}
\label{sec.imp}

We carried out the practical implementation work of the applied join
calculus as an extension of the \jocaml system.
The extended system is publicly released~\cite{JoCaml}.
The release includes a tutorial that makes extensive use of
algebraic patterns in join patterns.
In this section, we
first sketch out the structure of the extended \jocaml compiler,
pointing out where the transformation should take place. Then some
optimizations of our algorithm  are reported.

\subsection{Front end of the (extended) \jocaml compiler}
\label{subsec.frontend}

The \jocaml compiler is an extension of the \ocaml compiler,
as the \jocaml language is an extension of the \ocaml language.
Extensions are confined to the first four phases of the compiler.

More precisely, there are additional tokens in the lexer (such as the
keyword \lst|def|).  Then, all the constructs of
Figure~\ref{fig.syntax} are parsed and rendered as specific constructs
in the abstract syntax tree. Typed syntax undergoes a
similar extension. Amongst those first three compiler phases, only the
typer significantly differs from the original \ocaml compiler, since
the \jocaml compiler has to deal with the specific rules for typing
the join calculus
polymorphically~\cite{Fournet-Laneve-Maranget-Remy:typing-join}.
Finally, the typed syntax is translated to \emph{lambda-code}, which
basically is -calculus enriched with primitive types and calls
to the runtime library.  All constructs specific to \jocaml disappear,
being replaced by calls to specific primitives in a ``\textsf{Join}''
library, built on top of one of the \ocaml thread libraries.  In the
following, we denote as ``the \jocaml runtime'', the ordinary (thread
aware) \ocaml runtime, plus the thread library, plus the \textsf{Join}
library. To summarize, extending the \ocaml system to the \jocaml
system amounts to modifying the front end of the compiler, and to
writing the \textsf{Join} library.

\begin{figure}[ht]
\centering
\begin{picture}(0,0)\includegraphics{ejocaml_flow.eps}\end{picture}\setlength{\unitlength}{4144sp}\begingroup\makeatletter\ifx\SetFigFont\undefined \gdef\SetFigFont#1#2#3#4#5{\reset@font\fontsize{#1}{#2pt}\fontfamily{#3}\fontseries{#4}\fontshape{#5}\selectfont}\fi\endgroup \begin{picture}(4009,3047)(1111,-4457)
\put(3745,-3924){\makebox(0,0)[b]{\smash{{\SetFigFont{10}{12.0}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2685,-1774){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(2725,-3052){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(4669,-1771){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\put(4633,-3028){\makebox(0,0)[b]{\smash{{\SetFigFont{12}{14.4}{\rmdefault}{\mddefault}{\updefault}{\color[rgb]{0,0,0}}}}}}
\end{picture} \caption{The extended \jocaml compiler front end}\label{fig.ejocaml.flow}
\end{figure}
Extending \jocaml to handle pattern arguments in join definitions
requires further modifications.  Figure~\ref{fig.ejocaml.flow} shows
the structure of the extended \jocaml compiler. With respect to plain
\jocaml (without algebraic pattern matching in join definitions), the
parser and the typer have to be modified to take pattern arguments in
channel definitions into account. However these extensions are
mechanical. The critical modification manifests itself as an extra
sub-phase (enclosed in the dashed polygon) between the typing phase and
the translation phase. Not surprisingly, the additional phase carries
out the transformation from extended join definitions to plain ones,
by implementing the compilation scheme  of
Section~\ref{sec.trans}.  Once this new transformation is performed,
all join definitions in the typed trees are plain ones (without
pattern arguments).  Then, the translator to lambda-code and, more
importantly, the \jocaml runtime system need not be changed, with
respect to the ones of the original \jocaml system.

We in fact also slightly extended the translator, for the sake of
performing a few optimizations (see Section~\ref{subsec.optimization})
and of avoiding excessive duplications of guarded processes (see
Section~\ref{subsec.share}). The optimizations we perform make use of
the sophisticated pattern matching compiler and analyzer that are
already present in the standard \ocaml compiler.

\subsection{Matching optimizations}
\label{subsec.optimization}

\subsubsection{Avoiding redundant matchings}
As discussed at the end of Section~\ref{sec.trans-idea},
the compilation introduces redundant matchings.
For instance, in the stack example, we get:
\begin{lstlisting}{Join}
def pop(r) & State(z) |> match z with x::xs -> r(x) & State(xs)
  
or State(y) |> match y with
               | _::_ -> State(y)
                         
\end{lstlisting}
A  operation apparently involves matching the 
argument twice: once in the dispatcher, to select the appropriate
forwarding channel~\lst|State|, and again
in the reaction rule, to perform the bindings of variables \lst|x| and
\lst|xs| to the head and tail of the cons-cell~\lst|z|.

However, by construction, the value of argument~\lst|z| is guaranteed
to be an instance of the pattern \lst|x::xs|. This remark is general
(see Lemma~\ref{lemma.joinpi.dispatcher}): for any matching
 introduced in reaction rules by Step~5 of
algorithm~, the value of  always matches the
pattern~.  In other words, the matching
 never fails, hence no test need to be
performed at all. As a consequence, in the case of the pattern
\lst|x::xs|, we aim at getting the the following
lambda-code:\footnote{In examples, we show lambda-code as \ocaml code,
enriched with a few primitives.}
\begin{lstlisting}{Join}
let x = field 0 z in 
let xs = field 1 z in

\end{lstlisting}
Primitives ``'' and ``'' extract the head and tail from the cons-cell~.

The requirement is then to write a specific matching compiler that
does not issue tests when test outputs can be predicted at compile
time. In fact, such a matching compiler is already present in the
\ocaml compiler: as it stands, the optimizing pattern matching
compiler of~\cite{LefessantMarangetPattern} can output such code,
provided it is informed that the compiled matching has only one clause
and never fails, which is exactly the case for all the matchings
 introduced in reaction rules by Step~5 of
algorithm~.
Incidentally, the condition ``the matching can never fail'' is
expressed simply as ``the matching is exhaustive''.
We also rely on a later phase of the \ocaml compiler
to inline \lst|let|-bindings when appropriate. 

As a final remark, it is worth observing that, when the original
pattern does not contain variables, the compilation of
 yields no code: neither test, nor
binding.

\subsubsection{Avoiding useless forwarding channels}
\label{subsubsec.remove}
Simple analysis of the dispatcher matching enables use to spare some
of the forwarding channels.  Let us first re-consider the example of
the complete stack. Our transformer  applied to channel~\lst|State|
yields the following dispatcher:
\begin{lstlisting}{Join}
or State(z) |> match z with
                 | 0::_::_ -> State(z)
                 | [0] -> State(z)
                 | _::_::_ -> State(z)
                 | 0::_ -> State(z)
                 | [_] -> State(z)
                 | _::_ -> State(z)
                 | [] -> State(z)
                 | _ -> State(z)
\end{lstlisting}
In the matching above, some clauses are never matched at runtime.  For
instance, the last clause ``\lst|_ -> State(z)|'' is
useless, because of the two immediately preceding clauses
``\lst"_::_->"~'' and ``\lst"[] ->"~'' that obviously
match all the lists.  As a consequence, the forwarding channel
 never carries any message hence it is also
useless.  Similarly, channels  and channel
 are useless. We can optimize by removing both the
useless clauses from the dispatcher and all occurrences of useless
channels from the rewritten join patterns.

To summarize, by applying the optimizations discussed so far, the
stack example after compilation looks as follows:
\begin{lstlisting}{Join}
def push(v) & (State(z) or State(z) or State(z) or State(z) or State(z))
      |> State (v::z) 
 or pop(r) & (State(z) or State(z) or State(z) or State(z))
      |> r(field 0 z) & State(field 1 z)
 or insert(n) & (State(z) or State(z))
      |> State(0::n::field 1 z)
 or last(r) & (State(z) or State(z))
      |> let x = field 0 z in r(x) & State([x])
 or swap() & (State(z) or State(z))
      |> let m = field 1 z in State(field 0 m::field 0 z::field 1 m)
 or pause(r) & State(z) |> r()
 or resume(r) |> State([]) & r()
 or State(z) |> match z with
                 | 0:::: -> State(z)
                 | [0] -> State(z)
                 | :::: -> State(z)
                 | [] -> State(z)
                 | [] -> State(z)
\end{lstlisting}
Thanks to the optimization, three cases are spared from the
dispatcher, three channels are not allocated, and the size of the
~join-patterns decrease significantly.

To integrate this optimization into the implementation, we modify the
algorithm , as regards dispatcher construction (Step 4) and
rewriting of reaction rules (Step 5). In Step 4, after the topological
sort, we check the usefulness of each vertex.  More specifically, to
check whether vertex  is useful or not, with respect to the
preceding vertices , \ldots{},  in the topological
order, we check the usefulness of pattern  with respect to
patterns , \ldots{}, , where  is the
annotation pattern of vertex~. For that purpose, we use the
standard usefulness checker of \ocaml~\cite{warning}, of which we
present a simplified version in Section~\ref{subsec.pt}.  Then, in
Step~5 of the algorithm we retain only the 's that are useful.

\subsection{Compiling \textbf{or} in join patterns}
\label{subsec.share}

The compilation scheme  introduces disjunctive composition
into join patterns, a construct that \jocaml did not support before
the introduction of pattern argument in join definitions. In this
section, we describe our extensions to the \jocaml compiler so as to
integrate this new feature.

When we introduced \kwd{or} in join patterns, we claimed that it is
syntactic sugar. That is, we define this new construct by distributing 
 over , until \kwd{or} reaches the reaction rule
level, where we finally duplicate the reaction rules themselves.

The whole process of distributing  over  and of
duplicating the rules can be summarized as ``expansion of \kwd{or} in
join patterns''.

It is not difficult to see that the above mentioned expansion easily
produces an exponential number of reaction rules.
For instance, consider the definition:
\begin{lstlisting}{Join}
def a(true) |> P or  a(true) |> P  or a(true)  |> P
or a(_) & a(_) &  & a(_) |> P
\end{lstlisting}
For each channel  there are two forwarding channels
 and .
As a consequence, after rewriting, the last reaction rule from the definition
above becomes:
\begin{lstlisting}{Join}
or (a(z) or a(z)) & (a(z) or a(z)) &  & (a(z) or a(z)) |> P
\end{lstlisting}
And the expansion of \kwd{or} in join patterns finally yields 
reaction rules.

The extended \jocaml compiler indeed performs the expansion
of \kwd{or} in join patterns as sketched above, except for one point:
the guarded processes (\lst|P| in example) is not duplicated.
Instead, guarded processes are compiled into (lambda-code) closures
and duplication of guarded processes is performed by duplication of
pointers to those closures.

We will illustrate two successive refinements of the idea of sharing
guarded processes.  But before that, let us first examine how guarded
processes are compiled and triggered in the general case.
\begin{lstlisting}{Join}
def a(x) & b(y) |> 
or  a(x) & c(y) |> 
\end{lstlisting}
The above join definition defines three channels organized in two
reaction rules. Target lambda-code can be sketched as follows:
\begin{lstlisting}[indent=1.8em,labelstep=1]{imppt}
let jdef =
    
  let g = fun jdef ->                   
                let x = Join.get_queue jdef  in 
                let y = Join.get_queue jdef  in 
                Join.unlock jdef;                        
                Join.spawn (fun () -> ) in  
  let g = fun jdef ->                   
                let x = Join.get_queue jdef  in 
                let y = Join.get_queue jdef  in 
                Join.unlock jdef;                         
                Join.spawn (fun () -> ) in 
   
\end{lstlisting}
The presented lambda-code only describes the compilation of guarded
processes to closures  and~.  Those
\emph{guarded closures} are subparts of the complete compilation of
the join definition. They appear as local bindings in the more
complete definition \lst|jdef|, which is not shown.  We refer
to~\cite{LeFessantMarangetCompileJoin} for a full explanation about
how the \jocaml compiler deals with join definitions and guarded
processes.  Nevertheless, we give a brief description, based upon the
example.  Join definitions are compiled into vector-like structures,
and channels are pairs of a pointer to such a structure and of a
\emph{channel slot} (written  etc. above).  Channel slots
are small integers.  Here, we assume  to be~,
 to be~, and  to~be~.  Based upon
channel slots, join patterns are compiled into bitsets. In this
example, we have  for pattern ``\lst!a(x) & b(y)!''  and 
for ``\lst!a(x) & c(y)!''.  The join definition runtime structure
holds a list of pairs made of such a bitset and of a pointer to a
guarded closure (\lst|[(,g ; (,g)]|
in our example).  This \emph{join matching list} can be seen as the
result of reaction rules compilation.  The definition structure also
holds a mutex, an array of queues (indexed by channel slots), and an
internal bitset that describes the current status of queues.  In
response to message sending over a channel, specific code from the
\textsf{Join} library first locks the mutex, alters the internal
bitset, stores the message in the appropriate queue, and then attempt a
match.  In case a match is found, the corresponding closure
(\lst|g| or \lst|g| above) is called, with the
definition itself as an argument.

Notice that the closures \lst|g| or \lst|g|
have the responsibility to bind formal arguments \lst|x| and \lst|y|
to the
appropriate actual arguments, which are extracted from the appropriate
queues (lines~\ref{lstGbx}--\ref{lstGby}) and
\ref{lstGcx}--\ref{lstGcy}), and to release the mutex
(lines~\ref{lstGbunlock} and \ref{lstGcunlock}).
The guarded process is finally triggered by the means of
the primitive \lst|Join.spawn| that takes a closure as argument
(lines~\ref{lstGbspawn} and~\ref{lstGcspawn}) and creates a new thread
to run that closure.
Here,   and  represent the
compilation to lambda-code of  and~ respectively.
It is to be noticed that formal parameters may occur free in
 and~.

Now let us consider the compilation of join definitions with \kwd{or}
in their join patterns, such as this one:
\lst|def a(x) & (b(y) or c(y))|~.
Target lambda-code  can be sketched as follows:
\begin{lstlisting}[indent=1.8em,labelstep=1]{bonga}
let jdef = 
   
  let p = fun jdef x y ->
                Join.unlock jdef;
                Join.spawn (fun () -> ) in 
  let g = fun jdef ->
                let x = Join.get_queue jdef  in
                let y = Join.get_queue jdef  in
                p jdef x y in 
  let g = fun jdef ->
                let x = Join.get_queue jdef  in
                let y = Join.get_queue jdef  in
                p jdef x y in 
   
\end{lstlisting}
As a consequence of the expansion of the disjunctive pattern
``\lst"b(y) or c(y)"'', the join matching list is
\lst|[(,g ; (,g)]|, like in the
previous example. The two guarded closures 
and~ are different, because the value bound to the
formal argument~ has to be extracted either from the queue of
channel  or from the queue of channel , depending upon
the matched join pattern being ``\lst!a(x) & b(y)!'' or ``\lst!a(x) & c(y)!''.
However, the task of unlocking the mutex and of triggering
the process  is common to both and is performed by a third
closure~\lst|p| (lines~\ref{sharebegin}--\ref{shareend}), which is
called by the two guarded closures  and~
 at lines \ref{call1} and~\ref{call2} respectively.
As a result, duplication of most of the guarded process code is
avoided and a reasonable amount of sharing is achieved.  One should
observe that the interface between the library code that performs join
matching and the guarded closures is preserved: guarded closures are
still functions that take a join structure as argument.

It is in fact possible for the compiler to completely share guarded
closures between reactions rules that originate from \kwd{or} pattern
expansion.  But then, guarded closure code must be abstracted further
with respect to the exact join pattern that is matched.  The idea of
dictionary can be used for this purpose. A \emph{dictionary} is
an array built by the compiler.  Dictionaries represent mappings from
formal parameters to channel slots and the compilation of a join
pattern now yields a pair of a bitset and of a dictionary.  More
significantly, disjunctive patterns are now compiled into a series of
such pairs.  For instance, the pattern ``\lst|a(x) & (b(y) or c(y))|''
is now compiled into the two pairs ``\lst!(,[| ; |])!''
and ``\lst!(,[| ; |])!'', where for instance the dictionary
component ``\lst![| ; |]!'' expresses that the formal parameters
\lst|x| and~\lst|y| are to be bound to messages sent on channels
 (at slot 0) and~ (at slot 2) respectively.  The
compiler then generates guarded closures abstracted with respect to
dictionaries.
\begin{lstlisting}{Join}
let g = fun jdef dict -> 
            let x = Join.get_queue jdef (field 0 dict) in
            let y = Join.get_queue jdef (field 1 dict) in
            Join.unlock jdef;
            Join.spawn (fun () -> )
\end{lstlisting}
where ``\lst|field i dict|'' returns the th element of the
dictionary ``\id{dict}''. The join matching list now becomes the
following list of triples:
\begin{lstlisting}{foo}
  [ (,[| ; |],g) ; (,[| ; |],g) ]
\end{lstlisting}
In case a join-pattern bitset is matched, the corresponding closure in
the triple is called, with the join definition structure and
additionally the dictionary in the triple as arguments.

Adding one dictionary component is the price we should pay to achieve
complete sharing of guarded closures. However, such a dictionary is
not necessary for reaction rules whose pattern is not disjunctive.  In
that case, the compiler can avoid the extra ``\lst|field i dict|''
calls and replace them by the appropriate channel slots, which are
known at compile time.  However, for the sake of keeping an uniform
structure of the join matching list, guarded closures should always
accept the extra ``\lst|dict|'' argument, even when not needed.  A
simple solution is to consider a dummy dictionary, to be passed to
such guarded closures that do not need a dictionary.

The current implementation of \jocaml does not use dictionaries.  We
are still lacking experience to be able to assert whether they are
worth the price or not.

\section{Related work}
\label{sec.relatedwork}

Applied join is ``impure'' in the sense of Abadi and Fournet's
applied -calculus~\cite{AppliedPi}. We too extend an archetypal
name passing calculus with pragmatic constructs, in order to provide a
full semantics that handles realistic language features without
cumbersome encodings.  It is worth noticing that like
in~\cite{AppliedPi}, we distinguish between variables and names (only
variables of channel type are treated as names), a distinction that is
seldom made in pure calculi. Since we aim to prove a program
transformation correct, we define the equivalence on open terms, those
that contain free variables. Abadi and Fournet are able to require
their terms to have no free variables, since their goal is to prove
properties of program execution, namely the correctness of security
protocols.

Our compilation scheme presented in Section~\ref{sec.trans} can be
seen as the combination of two basic steps: refining channels and
forwarding by dispatcher. The desired property of the forwarding
behavior (Lemma~\ref{lemma.joinpi.dispatcher}) constitutes the core of
the correctness proof of the compilation scheme, which essentially
stems from pattern matching theory. There are other work that perform
the formal treatments of forwarders, for
instance~\cite{Merro98asynchrony,Gardner2003lf}, but in different
contexts. Our forwarder demultiplexes messages into separate channels
according to the pattern of the messages,
while~\cite{Merro98asynchrony,Gardner2003lf} use plain
channel-to-channel linear forwarders to achieve the locality property,
\ie reception on a given channel takes place on an unique site.
It is to be noticed that the equivalence proof of~\cite{Gardner2003lf}
is with respect to ordinary barbed congruence
and by the means of a labelled transition system.
Yet another example is the correctness proof of the compilation of
join patterns to smooth orchestrators in~\cite{Orchestrators}.
The compilation of~\cite{Orchestrators} is less involved than ours
since it basically amounts to inserting forwarders.



We established the correctness of our compilation scheme by showing
the programs before and after compilation to be behavioral congruent. It is
usual practice in the literature to prove correctness of program
transformations by showing semantics preservation.
(\cite{DaveCompVeriSurvey} is a survey).
Here, variations are numerous:
they consist in different connections between source and
target formalism (two independent languages, or with the target being
a sub-set of the source), different semantics (denotational vs.
operational), different equivalence relations (observational
equivalence, refinement relation, simulation, etc.), and different
settings (sequential, concurrent, parallel, object-oriented, etc.),
Consequently, proof techniques also differ.
For example, recent work of Blazy~\etal~\cite{Blazy-Dargaye-Leroy-06}
reports the formal verification of a C compiler front-end in the Coq
proof assistant. It handles two independent source and target
languages, both with big-step operational semantics. The major
difficulty of the correctness proof resides in relating the different
memory states and evaluation environments of the two languages. A
simulation relation is demonstrated from target code to source code by
induction on evaluation derivation and case study over the last
applied evaluation rule.
Closer to our work, \cite{EmirMaOdersky2007} shows the correctness
of an optimizing translation that compiles away pattern matching in
\scala. Proof techniques analogous with ours are applied, \ie they
also tackle contexts explicitly by proving congruence and define
observational equivalence on open terms based on the one between
closed terms and closing up by substitutions. Moreover, specific to
its extractor-base pattern matching, extractors are required to always
terminate without exception in order to achieve the correctness.

We now review some programming languages that support concurrency and
examine how our work can be related to those. Languages whose model
for concurrency directly stems from the join calculus should benefit
from our work. More precisely, if a language already offers \`{a} la
ML pattern matching and join definitions, then its authors can
implement our ideas in their framework, and their implementation
effort would be small. An early example of a language based upon the
join calculus is \funnel~\cite{funnel}. \funnel later evolved into
\scala~\cite{Odersky:scala}, where \`{a} la ML pattern matching is
supported and join style concurrency is provided in terms of a
library~\cite{JoinScala}. Another similar work is~\cite{Singh06},
which introduces join style concurrency in \haskell. We believe that
extending the two settings above
with algebraic patterns as formal arguments
can be made by direct application of our techniques.
Smooth orchestrators~\cite{Orchestrators} differ from join definitions
in rather subtle ways: an orchestrator is syntactically similar to a
join definition and can be seen as defining competing reaction rules;
however, (1)~once a reaction rule of an orchestrator is selected and
continuation fired, the whole orchestrator (together with other
non-selected competing rules) gets expired and discarded; and~(2) the
definitions of channels and of orchestrators that synchronize them are
separated.
Point~(2) above is quite subtle: one can orchestrate receptions
on channels whose definitions are unknown, provided
all the orchestrated channels are defined on the same site.
Nevertheless,
orchestrators are controlled by finite automata that extends the ones
of~\cite{LeFessantMarangetCompileJoin} for join~definitions.  Thus,
the adaptation of our techniques to orchestrators looks feasible.

In addition, there is a sustained interest in integrating join
calculus into object-oriented languages~: polyphonic~\csharp and its
successor \comega~\cite{Cw} for \csharp; and \joinjava~\cite{JoinJava}
for \java. Unfortunately, the issue here is the lack of pattern
matching, which neither \csharp nor \java offers.  A detailed
discussion on the introduction of \`{a} la ML pattern matching in
object-oriented languages would be out of scope.  Briefly, proposed
solutions are either by the means of preprocessing~\cite{TOM}, or by
tighter language integration~\cite{Odersky:scala,OOMatch}. As our
compilation scheme requires precise information on pattern semantics
(\eg to decide the precision relation~), we think
that solutions of the second kind would facilitate the extension of
the introduced pattern matching to join patterns.

\erlang~\cite{Erlang} features both pattern matching and concurrency.
However, concurrency in Erlang is based upon the actor
model~\cite{actor73,actor86}. In this model, messages are sent to
\emph{actors} and actors manage a queue of messages.
Moreover, the
reception behavior of an actor can be specified by
the  \texttt{receive}~\textit{m} construct.
This construct is similar to ML pattern matching
\texttt{match}~\textit{v}~\texttt{with}~\textit{m}, except for the
value matched~, which is left implicit.
The semantics of \texttt{receive}~\textit{m}
can be described as follows: attempt a match in the actor's queue,
scanning it from the oldest to the most
recent message, stopping when a match is found.
This simple combination of message passing and pattern
matching proves convenient, as witnessed by the success of Erlang.
However, \erlang in general misses a simple and efficient handling of
synchronization between actors as join patterns offer. Lacking
necessary knowledge of \erlang internals, it is difficult for us to
assess whether the selection of messages from actors queues can
benefit from our techniques or not. In any case, difference in
semantics is outstanding and we conjecture that an adaption of our
technique would not be immediate. In particular, the existence of one
message queue per receiving agent is central to \erlang model, while a
join definition naturally handles several message queues.

Finally, we discuss the transplantation of our compilation scheme to a
language whose semantics for concurrency is based upon the original
-calculus of~\cite{MPW92}, like for instance
\textrm{Pict}~\cite{Pict}, or \textrm{PiDuce}~\cite{PiDuce:2005}
without orchestrators.  Such a task is apparently impossible.  Namely,
on the one hand, we propose a \emph{compilation} scheme, and we thus
need to isolate all the instances of reception on a given channel from
program source~; while, on the other hand, the -calculus features
\emph{unrestricted input capability}.
More precisely, in the  -calculus,
any process that knows of some
channel~ can input on it.  As a channel name~ can be passed via
messages, reception on~ may occur anywhere.  The join~calculus
originates from a radical solution to the distributed implementation
issue: channels and reception behaviors are defined by a synthetic
construct, and input on channels cannot occur anywhere else.  However,
there are other solutions that retain the -calculus as a basis
while restricting input capability, such as the localized
-calculus~\cite{Merro98asynchrony}.
Moreover, the located channels of Nomadic Pict~\cite{nomadic}
allows to lift such solutions to a distributed setting.
Given such frameworks, we
shall assume that all receptors on a given channel are known
statically.  Then, we can extend the input construct  as
, where  is pattern, and expect to be able to translate
this extended language into ordinary -calculus.  In that process,
we see at least one additional complication.  Let  and~
be two patterns that are compatible (\ie that have instances in
common), and let us consider the following program, an analog of the
simple examples of Section~\ref{sec.trans-idea}.

The above process significantly differs from a join definition, since
a successful input does not discard the other input.
A tentative translation in the spirit of ours would
be the parallel composition of a dispatcher:

and of the following process:

Where  is ,
and ``'' is internal choice that we use here to express input-guarded choice.
Thus, we need input-guarded choice.
This is a noticeable complication, even though
input-guarded choice can be expressed
in the -calculus without choice~\cite{NestmannPierce00}.
Another concern is the usage of the replication operator~``''
in the dispatcher.
Clearly, the adaptation of our technique to a -calculus setting
is not immediate.


\section{Conclusion and future work}
\label{sec.conclusion}

This paper is part of our effort to develop a practical concurrent
programming language with firm semantical foundations.
In our opinion, a programming language is more
than an accumulation of features. That is, features interact sometimes
in unexpected ways, especially when intimately entwined.  Here, we
have studied the interaction between pattern matching and concurrency.
The framework we have used was the applied join calculus --- an
extension of the join calculus with algebraic data types. Applied join
inherits its capabilities of communication and concurrency from join
and supports value passing.  More significantly, it allows algebraic
pattern matching in both formal arguments of channel definitions and
guarded processes.  Compared with join, applied join provides a more
convenient (or ``pragmatic''), precise and realistic language model to
programmers.  From that perspective, pattern matching and join
calculus appear to live well together, with mutual benefits. The
result of this work reinforces our interest in using \`{a} la ML
pattern matching as a general purpose programming paradigm, and join
calculus as the basic paradigm for concurrency.

Exploiting the fact that \jocaml already had an efficient
implementation for both ML pattern matching and join primitives, we
have designed the implementation of applied join as defining a
practical compilation scheme that transforms extended join definitions
into ordinary ones plus ML pattern matching. We have solved the
non-determinism problem during the design of this compilation scheme.
Moreover, we have actually integrated it into the \jocaml system with
several optimizations.
It is worth observing that a direct
implementation of extended join-pattern matching at the runtime level
would significantly complicate the management of message queues, which
would then need to be scanned in search of matching messages before
consuming them.
As we remarked, our compilation technique may yield
code of exponential size.
However, we expect such blowup not to occur
in practice, an expectation which is apparently confirmed by
our preliminary experiments in the \jocaml system.
Should this prove wrong in the future, we could
face the issue in two manners~: either complicate the runtime system
as sketched above, or design a direct implementation of \kwd{or} in
join patterns.

A theory of process equivalence has also been developed in applied
join in order to assess the correctness of our compilation scheme. In
archetypal name passing calculi, where every free variable is of
channel type, it is sufficient to only consider terms closed in our
sense, \ie terms without free variables of non-channel type, when
defining equivalence relations. By contrast, applied join supports
real values and its static transformations should apply to open
processes with free variables of non-channel type.  To tackle this
problem, we have first defined a weak barbed congruence to express the
equivalence of two closed processes, then we have lifted the
equivalence relation to open processes by closing up by all
substitutions. The resulting relation is called ``open equivalence''.
We have demonstrated it is also a full congruence and have proved our
compilation scheme correct by showing that the processes before and
after transformation are open equivalent.  The proof technique we have
used, which can be summarized as ``full abstraction'', stems from
pattern matching theory and the fact that inserting an internal
forwarding step in communications does not change process behavior.

In previous work, we have designed an object-oriented extension of the
join
calculus~\cite{FournetLaneve03,MaMaranget2003type,MaMaranget2005hideTR},
which appeared to be more difficult. The difficulties reside in the
refinement of the synchronization behavior of objects by using the
inheritance paradigm. We solved the problem by designing a delicate
way of rewriting join patterns at the class level.  However, the
introduction of algebraic patterns in join patterns impacts this
class-rewriting mechanism. The interaction is not immediately clear.
Up to now, we are aware of no object-oriented language where the
formal arguments of methods can be patterns. We thus plan to
investigate such a combination of pattern matching and inheritance,
both at the calculus and language level.

Another interesting future work would be to extend our framework with
more sophisticated patterns for XML data. As a matter of fact, the
authors of \scala have already extended the notion of pattern matching
to the processing of XML data with the help of regular expression
patterns (a similar system is \textrm{PiDuce}~\cite{PiDuce:2005}).  Their
extension makes \scala suitable for developing web service
applications. Our model of pattern matching in join calculus works
with general algebraic data types. At the moment, we do not see any
particular barrier that prevent our model from also working with XML
trees.


\section*{Acknowledgement}
The authors wish to thank James Leifer and Jean-Jacques L\'{e}vy for
fruitful discussions and comments.
We also thank the anonymous referees for their suggestions.

\bibliographystyle{plain}
\bibliography{ptjoin}
\end{document}
