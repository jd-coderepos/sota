\documentclass{article}

\usepackage[preprint]{neurips_2023}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{epsfig}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{float}
\usepackage{subcaption}
\usepackage{pifont}
\usepackage{enumitem}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}

\setcitestyle{square,sort,comma,numbers}


\def\eg{\emph{e.g}.} \def\Eg{\emph{E.g}.}
\def\ie{\emph{i.e}.} \def\Ie{\emph{I.e}.}
\def\cf{\emph{cf}.} \def\Cf{\emph{Cf}.}
\def\etc{\emph{etc}.} \def\vs{\emph{vs}.}
\def\wrt{w.r.t.} \def\dof{d.o.f.}
\def\iid{i.i.d.} \def\wolog{w.l.o.g.}
\def\etal{\emph{et al}.}


\title{VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation}

\author{
Wenjing Wang \quad Huan Yang \quad Zixi Tuo \quad Huiguo He \\ \textbf{Junchen Zhu} \quad \textbf{Jianlong Fu} \quad \textbf{Jiaying Liu} \\
Wangxuan Institute of Computer Technology, Peking University, Microsoft Research\\
\texttt{\{daooshee, liujiaying\}@pku.edu.cn}\\
\texttt{\{huayan, v-zixituo, v-huiguohe, v-junchenzhu, jianf\}@microsoft.com}\\
}

\begin{document}

\maketitle

\begin{abstract}
We present \textit{VideoFactory}, an innovative framework for generating high-quality open-domain videos.
\textit{VideoFactory} excels in producing high-definition (1376768), widescreen (16:9) videos without watermarks, creating an engaging user experience.
Generating videos guided by text instructions poses significant challenges, such as modeling the complex relationship between space and time, and the lack of large-scale text-video paired data. Previous approaches extend pretrained text-to-image generation models by adding temporal 1D convolution/attention modules for video generation.
However, these approaches overlook the importance of jointly modeling space and time, inevitably leading to temporal distortions and misalignment between texts and videos. In this paper, we propose a novel approach that strengthens the interaction between spatial and temporal perceptions. In particular, we utilize a swapped cross-attention mechanism in 3D windows that alternates the ``query'' role between spatial and temporal blocks, enabling mutual reinforcement for each other. 
To fully unlock model capabilities for high-quality video generation, we curate a large-scale video dataset called HD-VG-130M. This dataset comprises 130 million text-video pairs from the open-domain, ensuring high-definition, widescreen and watermark-free characters.
Objective metrics and user studies demonstrate the superiority of our approach in terms of per-frame quality, temporal correlation, and text-video alignment, with clear margins. 
\end{abstract}

\section{Introduction}
\label{sec:introduction}

Automated video production is experiencing a surge in demand across various industries, including media, gaming, film, and television~\cite{2017_DigiPro,2021_Playable}. This increased demand has propelled video generation research to the forefront of deep generative modeling, leading to rapid advancements in the field~\cite{2022_VDM, 2016_VG_ICLR, 2017TGAN,2018MoCoGAN, 2016_VG_NIPS}. In recent years, diffusion models~\cite{2020DDPM} have demonstrated remarkable success in generating visually appealing images in open-domains~\cite{2022LDM,imagen}. Notably, some commercial applications have leveraged these advanced techniques to create engaging and imaginative pictures, such as using the text of ``A Chinese girl's wedding, 1980s, China,'' or ``Albert Einstein eating vegetables, cow in the background.'' 
Building upon such success, in this paper, we take one step further and aim to extend their capabilities to high-quality text-to-video generation.

As is widely known, the development of open-domain text-to-video models poses grand challenges, due to the limited availability of large-scale text-video paired data and the complexity of constructing space-time models from scratch. To solve the challenges, current approaches are primarily built on pretrained image generation models. These approaches typically adopt space-time separable architectures, where spatial operations are inherited from the image generation model~\cite{2022_VDM,2023CogVideo}. To further incorporate temporal modeling, various strategies have been employed, including pseudo-3D modules~\cite{MakeAVideo,zhou2022magicvideo}, serial 2D and 1D blocks~\cite{imagen}, and parameter-free techniques like temporal shift~\cite{2023LatentShift} or tailored spatiotemporal attention~\cite{wu2022tuneavideo}. 
However, these approaches overlook the crucial 
interplay between time and space for visually engaging text-to-video generation. On one hand, parameter-free approaches~\cite{2023LatentShift,wu2022tuneavideo} rely on manually designed rules that fail to capture the intrinsic nature of videos and often lead to the generation of unnatural motions. On the other hand,  learnable 2D+1D modules and blocks~\cite{VideoLDM, MakeAVideo,zhou2022magicvideo} primarily focus on temporal modeling, either directly feeding temporal features to spatial features, or combining them through simplistic element-wise additions. This limited interactivity usually results in temporal distortions and discrepancies between the input texts and the generated videos, which hinders the overall quality and coherence of the generated content.


To address the above issues, we take one step further in this paper which highlights the complementary nature of both spatial and temporal features in videos. Specifically, we propose a novel Swapped Spatiotemporal Cross-Attention  (Swap-CA) for text-to-video generation. 
Instead of solely relying on separable 2D+1D self-attention~\cite{2021_space_time_attn} that replaces computationally expensive 3D self-attention as shown in Fig.~\ref{fig:teaser_attn} (a) and (c), we aim to further enhance the interaction between spatial and temporal features. While 3D window self-attention~\cite{2022VideoSwin} reduces the computational cost and incorporates both modalities, such work treats space and time dimensions indiscriminately, which largely limits its ability to capture complex spatiotemporal patterns, especially in generation tasks.  Compared with existing works, our swap attention mechanism facilitates bidirectional guidance between spatial and temporal features by considering one feature as the query and the other as the key/value. To ensure the reciprocity of information flow, we swap the role of the "query" in adjacent layers.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{attention.pdf}
    \caption{The paradigm of Swapped Spatiotemporal Cross-Attention (Swap-CA) in comparison with existing video attention schemes. Instead of only conducting self-attention in (a)-(c), we perform cross-attention between spatial and temporal modules in a U-Net, which encourages more spatiotemporal mutual reinforcement.}
    \label{fig:teaser_attn}
\end{figure}

By deeply interplaying spatial and temporal features through the proposed swap attention, we present a holistic \textbf{VideoFactory} framework for text-to-video generation. In particular, we adopt the latent diffusion framework and design a spatiotemporal U-Net for 3D noise prediction. To unlock the full potential of the proposed model and fulfill high-quality video generation, we propose to construct a large-scale video generation dataset, named \textbf{HD-VG-130M}. This dataset consists of 130 million text-video pairs from open-domains, encompassing high-definition, widescreen, and watermark-free characters. 
Additionally, our spatial super-resolution model can effectively upsample videos to a resolution of , thus ensuring engaging visual experience. We conduct comprehensive experiments and show that our approach outperforms existing methods in terms of both quantitative and qualitative comparisons. In summary, our paper makes the following significant \textbf{contributions}: 
\begin{itemize}[nosep]
    \item We reveal the significance of learning joint spatial and temporal features for video generation, and introduce a novel swapped spatiotemporal cross-attention mechanism to reinforce both space and time interactions. 
    \item To facilitate training, we curate a comprehensive video dataset comprising the largest 130 million text-video pairs to-date, which can support high-quality video generation with high-definition, widescreen, and watermark-free characters.
\end{itemize}
By effectively enforcing the mutual learning of spatial and temporal representations, our approach achieves outstanding visual quality in text-to-video generation tasks, while ensuring precisely semantic alignment between the input text and the generated videos.




\section{Related Works}
\label{sec:related_works}

\noindent\textbf{Text-to-Image Generation.}
Generating realistic images from corresponding descriptions combines the challenging components of language modeling and image generation.
Traditional text-to-image generation methods~\cite{2016T2I,ICML_16,AttnGAN,StackGAN,AI_Illustrator} are mainly based on GANs~\cite{GAN} and are only able to model simple scenes such as birds~\cite{WahCUB_200_2011}.
Later work extends the scope of text-to-image generation to open domains with better modeling techniques and training data on much larger scales.
DALL·E~\cite{dalle} and CogView~\cite{cogview} leverage auto-regressive vision transformers with variational auto-encoders and jointly train on text and image tokens.
In recent years, diffusion models have shown great ability in visual generation~\cite{2021ADM}.
For text-to-image multi-modality generation, GLIDE~\cite{2021GLIDE}, DALL·E~2~\cite{2022DALLE2}, and Imagen~\cite{imagen} leverage diffusion models to achieve impressive results. 
Based on these successes, some work further extends customization~\cite{2022DreamBooth,UnifiedMMLD2023}, image guidance~\cite{2023PaintbyExample}, and precise control~\cite{2022eDiff-I}.
Despite advances in generation ability, diffusion models are computationally expensive for training and inference, especially on high resolutions.
To reduce the cost, latent diffusion~\cite{2022LDM} conducts the diffusion process on a compressed latent space rather than the original pixel space.
This paper further explores how to extend the high-efficient latent diffusion for video generation.


\noindent\textbf{Text-to-Video Generation.}
Additional controls are often added to make the generated videos more responsive to demand~\cite{2016_VG_ICLR,2017Create_What_You_Tell,Vid2Vid,FaceAnimation2022}, and this paper focuses on the controlling mode of texts.
Early text-to-video generation models~\cite{2018Video_Text,2017Create_What_You_Tell} mainly use convolutional GAN models with Recurrent Neural Networks (RNNs) to model temporal motions. 
Although complex architectures and auxiliary losses are introduced, GAN-based models cannot generate videos beyond simple scenes like moving digits and close-up actions.
Recent works extend text-to-video to open domains with large-scale transformers~\cite{2022MAGVIT} or diffusion models~\cite{2022ImagenVideo}.
Considering the difficulty of high-dimensional video modeling and the scarcity of text-video datasets, training text-to-video generation from scratch is unaffordable.
As a result, most works acquire knowledge from pretrained text-to-image models.
CogVideo~\cite{2023CogVideo} inherits from a pretrained text-to-image model CogView2~\cite{cogview2}.
Imagen Video~\cite{2022ImagenVideo} and Phenaki~\cite{Phenaki} adopt joint image-video training.
Make-A-Video~\cite{MakeAVideo} learns motion on video data alone, eliminating the dependency on text-video data.
To reduce the high cost of video generation, latent diffusion has been widely utilized for video generation~\cite{2023LatentShift, VideoLDM, esser2023structure,he2022lvdm, he2022latent, 2023Text2VideoZero, 2023FollowYourPose,wu2022nuwa, wu2022tuneavideo, PVDM,  zhou2022magicvideo}.
MagicVideo~\cite{zhou2022magicvideo} inserts a simple adaptor after the 2D convolution layer.
Latent-Shift~\cite{2023LatentShift} adopts a parameter-free temporal shift module to exchange information across different frames.
PDVM~\cite{PVDM} projects the 3D video latent into three 2D image-like latent spaces.
Although the research on text-to-video generation is very active, existing research ignores the inter and inner correlation between spatial and temporal modules.
In this paper, we revisit the design of text-driven video generation. 



\section{High-Definition Video Generation Dataset}
\label{sec:hd_vg}
Datasets of diverse text-video pairs are the prerequisite for training open-domain text-to-video generation models.
However, existing text-video datasets are always limited in either scale or quality, thus hindering the upper bound of high-quality video generation. 
Referring to Tab.~\ref{tab:hqvg_100m}, MSR-VTT~\cite{xu2016msr} and UCF101~\cite{soomro2012ucf101} only have 10K and 13K video clips respectively.
Although large in scale, HowTo100M~\cite{miech2019howto100m} is specified for instructional videos, which has limited diversity for open-domain generation tasks.
Despite being appropriate in both scale and domain, the formats of textual annotations in HD-VILA-100M~\cite{xue2022advancing} are subtitle transcripts, which lack visual contents related descriptions for high-quality video generation.
Additionally, the videos in HD-VILA-100M have complex scene transitions, which are disadvantageous for models to learn temporal correlations.
WebVid-10M~\cite{bain2021frozen} has been used in some previous video generation works~\cite{2022ImagenVideo, MakeAVideo}, considering its relatively large-scale (10M) and descriptive captions.
Nevertheless, videos in WebVid-10M are of low resolution and have poor visual qualities with watermarks in the center. 

To tackle the problems above and achieve high-quality video generation, we propose a large-scale text-video dataset, namely \textbf{HD-VG-130M}, including 130M text-video pairs from open-domain in high-definition (720p), widescreen and watermark-free formats. We first sample according to the video labels of HD-VILA-100M~\cite{xue2022advancing} to collect original high-definition videos from YouTube.
As the original videos have complex scene transitions which are adverse for models to learn temporal correlations, we then detect and split scenes in these original videos using PySceneDetect\footnote{We use the open source video analysis tool: \url{https://github.com/Breakthrough/PySceneDetect}}, resulting in 130M single scene video clips.
Finally, we caption video clips with BLIP-2~\cite{li2023blip}, in view of its large vision-language pre-training knowledge.
To be specific, we extract the central frame in each clip as the keyframe, and get the annotation for each clip by captioning the keyframe with BLIP-2~\cite{li2023blip}. Note that the video clips in HD-VG-130M are in single scenes, which ensures that the keyframe captions are representative enough to describe the content of the whole clips in most circumstances. The statistics of HD-VG-130M are shown in Fig.~\ref{fig:hdvg_statis}. The videos in HD-VG-130M cover 15 categories. The wide range of domains is beneficial for training the models to generate diverse content. After scene detection, the video clips are mostly in single scenes with duration less than 20 seconds. The textual annotations are visual contents related to descriptive captions, which are mostly around 10 words. Text-video examples of our HD-VG-130M can be found in the supplementary.

\begin{table}[t]
    \centering
    \footnotesize
    \caption{Comparison of different video datasets. Existing text-video datasets are always limited in either scale or quality, while our HD-VG-130M includes 130M text-video pairs from open-domain in high-definition, widescreen and watermark-free formats.}
    \vspace{1mm}
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{cccccc}
        \Xhline{1.2pt}
         Dataset                            & Video clips & Resolution & Domain & Text & Watermark-free \\ 
        \Xhline{0.4pt}
        MSR-VTT~\cite{xu2016msr}            &  10K        &   240p  & open & caption &  \cmark\\
        UCF101~\cite{soomro2012ucf101}      & 13K         &   240p  & human action & class label & \cmark\\
        HowTo100M~\cite{miech2019howto100m} & 136M        & 240p & instructional & subtitle &  \cmark\\
        HD-VILA-100M~\cite{xue2022advancing}     & 103M        & 720p & open & subtitle &  \cmark\\
        WebVid-10M~\cite{bain2021frozen}    & 10M         & 360p & open & caption & \xmark \\ 
        \Xhline{0.4pt}
        HD-VG-130M (Ours)                    & 130M        & 720p & open & caption & \cmark\\ 
        \Xhline{1.2pt}
    \end{tabular}
    \label{tab:hqvg_100m}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{hdvg_statis_d.pdf}
    \caption{Statistics of video categories, clip durations, and caption word lengths in HD-VG-130M. HD-VG-130M covers a wide range of video categories. }
    \label{fig:hdvg_statis}
\end{figure}




\section{High-Quality Text-to-Video Generation}

To enable spatiotemporal interaction, we design a diffusion model for high-quality video generation.

\label{sec:method}

\noindent\textbf{Spatiotemporal Inter-Connection.} To reduce computational costs and leverage pretrained image generation models, space-time separable architectures have gained popularity in text-to-video generation~\cite{2022_VDM,2023CogVideo}.
These architectures handle spatial operations independently on each frame, while temporal operations consider multiple frames for each spatial position. 
In the following, we refer to the features predicted by 2D/spatial modules in space-time separable networks as "spatial features", and ``temporal features'' vice versa.
As discussed in Sec.~\ref{sec:introduction}, prior works have neglected the crucial interaction between spatial and temporal features.
To tackle this limitation, we promote the mutual reinforcement of these features through a series of cross-attention operations.










\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{model.pdf}
    \caption{An illustration of our video diffusion model incorporating Swapped Spatiotemporal Cross-Attention (Swap-CA). At the end of each U-Net block, we employ a swapped cross-attention scheme on 3D windows to facilitate a comprehensive integration of spatial and temporal features. In the case of two consecutive blocks, the first block employs temporal features to guide spatial features, while in the second block, their roles are reversed. This reciprocal arrangement ensures a balanced and mutually beneficial interaction between the spatial and temporal modalities throughout the model.}
    \label{fig:win_attn}
\end{figure}

Denote a basic operation , with

where , , and  are learnable projection matrices in the -th layer.
The direction of cross-attention, specifically whether  originates from spatial or temporal features, plays a decisive role in determining the impact of cross-attention.
In general, spatial features tend to encompass a greater amount of contextual information, which can improve the alignment of temporal features with the input text. On the other hand, temporal features have a complete receptive field of the time series, which may enable spatial features to generate visual content more effectively.
To leverage both aspects effectively, we propose a strategy of swapping the roles of  and  in adjacent two blocks. This approach ensures that both temporal and spatial features receive sufficient information from the other modality, enabling a comprehensive and mutually beneficial interaction.


Global attention greatly increases the computational costs in terms of memory and running time. 
To improve efficiency, we conduct 3D window attention.
Given a video feature in the shape of  and a 3D window size of , we organize the windows to process the feature in a non-overlapping manner, leading to  distinct 3D windows.
Within each window, we perform spatiotemporal cross-attention. By adopting the 3D window scheme, we effectively reduce computational costs without compromising performance.



Following prior text-to-image arts~\cite{VideoLDM, 2022LDM}, we incorporate 2 down/upsampling along the spatial dimension to establish a hierarchical structure.
Furthermore, research~\cite{2019VideoCompression,2020VideoCompression} has pointed out that the temporal dimension is sensitive to compression.
In light of these considerations, we do compress the temporal dimension and conduct shift windows~\cite{2022VideoSwin}, which advocates an inductive bias of locality.
On the spatial dimension, we do not shift since the down/upsampling already introduces connections between neighboring non-overlapping 3D windows.



To this end, we propose a Swapped Spatiotemporal Cross-Attention (Swap-CA) in 3D windows.
Let  and  represent the predictions of 2D and 1D modules. We utilize Multi-head Cross Attention (MCA) to compute their interactions by Swap-CA as

where GN, Proj, LN, 3D Window-based Multi-head Cross-Attention (3DW-MCA) are learnable modules.
By initializing the output projection Proj by zero, we have , \ie, Swap-CA is skipped so that it is reduced to a basic addition operation.
This allows us to initially train the diffusion model using addition operations, significantly speeding up the training process.
Subsequently, we can switch to Swap-CA to enhance the model's performance.

Then for the next spatial-temporal separable block, we apply shifted 3D window multi-head cross-attention (3DSW-MCA) and interchange the roles of  and , as

In all 3DSW-MCA, we shift the window along the temporal dimension by  elements.

\noindent\textbf{Overall Architecture.}
We adopt LDM~\cite{2022LDM} as the text-to-image backbone.
We employ an auto-encoder to compress the video into a down-sampled 3D latent space.
Within this latent space, we perform diffusion optimization using an hourglass spatial-temporal separable U-Net model.
Text features are extracted with a pretrained CLIP~\cite{CLIP} model and inserted into the U-Net model through cross-attention on the spatial dimension.

Our framework is illustrated in Fig.~\ref{fig:win_attn}.
To strike a balance between performance and efficiency, we exclusively apply Swap-CA at the end of each U-Net encoder and decoder block. In other positions, we employ a straightforward fusion technique using a  convolution to combine spatial and temporal features.
To enhance the connectivity among temporal modules, we introduce skip connections that connect temporal modules separated by spatial down/upsampling modules.
This strategy promotes stronger integration and information flow within the temporal dimension of the network architecture.

 

\noindent\textbf{Super-Resolution Towards Higher Quality.}
To obtain visually satisfying results, we further perform Super-Resolution (SR) on the generated video. One key to improving SR performance is designing a degradation model that closely resembles the actual degradation process~\cite{SR1,SR2,SR3,SR4,SR5,SR6}. In our scenario, the generated video quality suffers from both the diffusion and auto-encoder processes. Therefore, we adopt the hybrid degradation model in Real-ESRGAN~\cite{Wang_2021_RealESRGAN_ICCV} to simulate possible quality degradation caused by the generated process. 
During training, an original video frame is downsampled and degraded using our model, and the SR network attempts to perform SR on the resulting low-resolution image. 
We adopt RCAN~\cite{zhang2018_RCAN} with 8 residual blocks as our SR network. It is trained with a vanilla GAN~\cite{GAN} to improve visual satisfaction. With a suitable degradation design, our SR network can further reduce possible artifacts and distortion in the frames, increase their resolution, and improve their visual quality.



\begin{table}[t]
    \centering
    \footnotesize
    \caption{Ablation study on spatiotemporal interaction strategies. We report the FVD~\cite{FVD} and CLIPSIM~\cite{CLIP} on 1K samples from the validation set of WebVid-10M~\cite{bain2021frozen}. The computational cost is evaluated on inputs of shape . Details can be found in the supplementary material.  and  represent spatial and temporal features, respectively.
    }
    \vspace{1mm}
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{c|c|c|ccccc}
        \Xhline{1.2pt}
         Attention   &  &  & Param. (G) & Mem. (GB) & Time (ms) &  FVD  & CLIPSIM  \\
        \Xhline{0.4pt}
         -                  & -    &   -  & 1.480 & 9.37  & 135.35 & 566.16  & 0.3070 \\ \Xhline{0.4pt}
                            &  &   & 1.601 & 22.96 & 202.12 & 555.35 & 0.3091 \\ Global              &  &   & 1.601 & 22.96 & 205.00 & 496.25 & 0.3073 \\
        \cline{2-8}
                            & \multicolumn{2}{c|}{Swapped}  & 1.601 & 22.96 & 201.51 & 485.86 & 0.3092 \\
           
        \Xhline{0.4pt}
                            &  &   & 1.601 & 9.83 & 150.49 & 563.12 & 0.3086 \\ 3D Window           &  &    & 1.601 & 9.83 & 149.93 & 490.60 & 0.3076 \\
        \cline{2-8}
                            & \multicolumn{2}{c|}{Swapped}  & 1.601 & 9.83 & 148.24 & \textbf{475.09} & \textbf{0.3107} \\
        \Xhline{1.2pt}
    \end{tabular}
    \label{tab:aba_interaction}
\end{table}

\section{Experiments}

\subsection{Implementation Details}


Our model predicts images at a resolution of 344192 (with a latent space resolution of 4324). Then a 4upscaling is produced in our SR model, resulting in a final output resolution of .
Our model is trained with 32 NVIDIA V100 GPUs.
We utilize our HD-VG-130M as training data to promote the generation visual qualities. Furthermore, considering that the textual captions in HD-VG-130M are annotated by BLIP-2~\cite{li2023blip}, which may have some discrepancies with human expressions, we adopt a joint training strategy with WebVid-10M~\cite{bain2021frozen} to ensure the model could generalize well to diverse humanity textual inputs. 
This approach allows us to benefit from the large-scale text-video pairs and the superior visual qualities of HD-VG-130M while maintaining the generalization ability to diverse textual inputs in real scenarios, enhancing the overall training process.
More details can be found in the supplementary.



\subsection{Ablation Studies}


\noindent\textbf{Spatiotemporal Inter-Connection.}
We first evaluate the design of our swapped cross-attention mechanism. 
As shown in Tab.~\ref{tab:aba_interaction}, using temporal as  generally leads to better CLIP similarity (CLIPSIM)~\cite{CLIP}, revealing a better text-video alignment.
The reason might be that language cross-attention only exists in spatial modules. Thus, using spatial features to guide temporal ones implicitly enhance semantic guidance.
Reversely, using spatial as  leads to significantly better FVD, revealing better video quality. The reason might be that the spatial features can better perceive the overall video by using temporal features as guidance.
This experiment demonstrates the benefits of introducing cross-attention, as well as the different acts of spatial and temporal features.
Combining these two aspects, we propose to swap the roles of  and  every two blocks.
In this way, both the temporal and spatial features can get sufficient information from the other modality, leading to improved FVD and CLIPSIM scores.
3D window attention not only does not decrease the performance but also greatly reduces the computational cost.

\noindent\textbf{High-Definition Video Generation Dataset.}
As shown in Tab.~\ref{tab:aba_dataset}, we evaluate the effect of our HD-VG-130M. After adding HD-VG-130M in training, the result on the validation set of WebVid-10M~\cite{bain2021frozen} has been improved by 45.74 in FVD, which verifies the superior quality of our HD-VG-130M for training text conditioned video generation model. The visual comparison can also be found in Fig.~\ref{fig:com_w_wo_HD}. The visual qualities are greatly improved with the help of our high-quality text-video dataset, especially the watermark on the generated video is eliminated.

\begin{table}
    \begin{minipage}{.5\linewidth}
    \centering
      \caption{Text-to-video generation on UCF101.}
        \label{tab:result_ucf101}
        \vspace{0.5mm}
        \footnotesize
        \renewcommand{\arraystretch}{1.2}
        \begin{tabular}{lcc}
        \Xhline{1.2pt}
        Method       & Zero-shot & FVD    \\ 
        \Xhline{0.4pt}
        VideoGPT~\cite{yan2021videogpt}     & No        & 2880.6 \\
        MoCoGAN~\cite{2018MoCoGAN}      & No        & 2886.8 \\
        +StyleGAN2~\cite{viazovetskyi2020stylegan2}   & No        & 1821.4 \\
        MoCoGAN-HD~\cite{tian2021good_mocohd}   & No        & 1729.6 \\
        DIGAN~\cite{yu2022generating_digan}        & No        & 1630.2 \\
        StyleGAN-V~\cite{skorokhodov2022stylegan}   & No        & 1431.0 \\
        PVDM~\cite{PVDM}         & No        & 343.6  \\ 
        \Xhline{0.4pt}
        CogVideo~\cite{2023CogVideo}     & Yes       & 701.6  \\
        MagicVideo~\cite{zhou2022magicvideo}   & Yes       & 699.0  \\
        LVDM~\cite{he2022lvdm}   & Yes       & 641.8  \\
        ModelScope~\cite{VideoFusion}   & Yes       & 639.9  \\
        Video LDM~\cite{VideoLDM}    & Yes       & 550.6  \\ 
        \Xhline{0.4pt}
        Ours         & Yes       & \textbf{410.0}      \\ 
        \Xhline{1.2pt}
        \end{tabular}
    \end{minipage}
    \begin{minipage}{.5\linewidth}
        \centering
        \begin{minipage}{\textwidth}
        \vspace{0pt}
        \footnotesize
        \centering
            \caption{Text-to-video generation on MSR-VTT.}
            \label{tab:result_msrvtt}
            \vspace{0.5mm}
            \renewcommand{\arraystretch}{1.2}
            \centering
            \begin{tabular}{>{\raggedright\arraybackslash}p{.35\linewidth} >{\centering\arraybackslash}p{.21\linewidth} >{\centering\arraybackslash}p{.21\linewidth}}
            \Xhline{1.2pt}
            Method       & Zero-shot & CLIPSIM \\ 
            \Xhline{0.4pt}
            GODIVA~\cite{wu2021godiva}       & No        & 0.2402  \\
            NUWA~\cite{wu2022nuwa}         & No        & 0.2439  \\
            \Xhline{0.4pt}
            LVDM~\cite{he2022lvdm}   & Yes       & 0.2381 \\
            CogVideo~\cite{2023CogVideo}     & Yes       & 0.2631  \\
            ModelScope~\cite{VideoFusion}    & Yes       & 0.2795  \\
            Video LDM~\cite{VideoLDM}    & Yes       & 0.2929  \\ 
            \Xhline{0.4pt}
            Ours          & Yes       & \textbf{0.3005}       \\ 
            \Xhline{1.2pt}
            \end{tabular}
        \end{minipage}
        
        \begin{minipage}{\textwidth}
        \vspace{0pt}
        \footnotesize
        \centering
            \caption{Text-to-video generation on WebVid.}
            \vspace{0.5mm}
            \renewcommand{\arraystretch}{1.2}
            \centering
            \begin{tabular}{>{\raggedright\arraybackslash}p{.35\linewidth} >{\centering\arraybackslash}p{.21\linewidth} >{\centering\arraybackslash}p{.21\linewidth}}
            \Xhline{1.2pt}
            Method           & FVD & CLIPSIM \\ 
            \Xhline{0.4pt}
            ModelScope~\cite{VideoFusion}        & 414.11 &  0.3000 \\
            LVDM~\cite{he2022lvdm}     & 455.53 &  0.2751 \\ 
            \Xhline{0.4pt}
            Ours            & \textbf{292.35} & \textbf{0.3070} \\ 
            \Xhline{1.2pt}
            \end{tabular}
            \label{tab:results_WebVid-10M}
        \end{minipage} 
    \end{minipage} 
    \vspace{-5mm}
\end{table}

\begin{minipage}[t]{\textwidth}
\centering
\footnotesize
\begin{minipage}[t]{0.29\textwidth}
\makeatletter\def\@captype{table}
\centering
    \caption{Effect of training on different datasets.}
    \renewcommand{\arraystretch}{1.2}
        \begin{tabular}{c c}
        \Xhline{1.2pt}
         Training Data        & FVD  \\
        \Xhline{0.4pt}
         WebVid-10M~\cite{bain2021frozen}  & 475.09 \\
         \Xhline{0.4pt}
         WebVid-10M~\cite{bain2021frozen} & \multirow{2}{*}{429.75} \\
         + HD-VG-130M    &  \\
        \Xhline{1.2pt}
        \end{tabular}\label{tab:aba_dataset}
    \end{minipage}
\hspace{0.01\textwidth}
\begin{minipage}[t]{0.67\textwidth}
\makeatletter\def\@captype{figure}
\centering
    \caption{Text-to-video generation effects w/o and w/ HD-VG-130M for training.}
    \centering
    \includegraphics[width=\linewidth]{HDVILA-com-2.pdf}
    \label{fig:com_w_wo_HD}
\end{minipage}
\end{minipage}


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{Compared_with_Imagen_Makeavideo.pdf}
    \caption{Subjection text-to-video generation results compared with Imagen Video, Make-A-Video, and Video-LDM (Cases above are collected from their public project websites).}
    \label{fig:com_imagen_makeavideo}
    \vspace{-7mm}
\end{figure}


\subsection{Quantitative Results}
To fully evaluate the generation performance of our VideoFactory, we conduct automatic evaluations on three different datasets, 
WebVid-10M~\cite{bain2021frozen} (Val) same as the domain of part of our training data, as well as UCF101~\cite{soomro2012ucf101} and MSR-VTT~\cite{xu2016msr} in zero-shot setting. 


\noindent \textbf{Automatic Evaluation on UCF101.} 
As mentioned in Sec.~\ref{sec:hd_vg}, the textual annotations in UCF101~\cite{soomro2012ucf101} are class labels. We first follow~\cite{2022_VDM, MakeAVideo} and rewrite the labels of 101 classes to descriptive captions, and then generate 100 samples for each class. As shown in Tab.~\ref{tab:result_ucf101}, we report Fréchet Video Distance (FVD) of our VideoFactory compared with other methods. The FVD of our methods reaches 410.0, which achieves the best compared with other methods both in zero-shot setting and beats most of the methods which have tuned on UCF101~\cite{soomro2012ucf101}. The results verify that our proposed VideoFactory could generate more coherent and realistic videos.

\noindent \textbf{Automatic Evaluation on MSR-VTT.} As shown in Tab.~\ref{tab:result_msrvtt}, we also evaluate the CLIPSIM on the widely used video generation benchmark MSR-VTT~\cite{xu2016msr}. We randomly choose one prompt per example from MSR-VTT~\cite{xu2016msr} to generate 2990 videos in total. Although in a zero-shot setting, our method achieves the best compared to other methods with an average CLIPSIM score of 0.3005, which suggests the semantic alignment between the generated videos and the input text.

\noindent \textbf{Automatic Evaluation on WebVid-10M (Val).} 
Referring to Tab.~\ref{tab:results_WebVid-10M}, we randomly extract 5K text-video pairs from WebVid-10M which are exclusive from the training data to form a validation set and conduct evaluations on it.
Our method achieves an FVD of 292.35 and a CLIPSIM of 0.3070, significantly surpassing the existing methods ModelScope and LVDM. The results demonstrate the superiority of our approach.

\noindent \textbf{Human Evaluation.}
To overcome the limitation of existing metrics, and evaluate the performance from the aspect of humans, we conduct a user study to compare our VideoFactory with four state-of-the-arts. Specifically, we choose two models (i.e., ModelScope and LVDM) which have released their codes and pretrained models, and two methods (i.e., Make-A-Video and Imagen Video) which only show some samples on their websites. In each case, each participant will be given two samples of the same text from our method and one competitor, and is asked to compare the two samples in terms of the video quality and text-video correlation and give an overall preference. We demonstrate the results in Tab.~\ref{tab:user_study}, and we also report the number of parameter ratios for fair comparisons.

\begin{table}[t]
\centering
\caption{User Preference. The number indicates the percentage of humans that prefer our method over the compared method. We also show the ratio of the network parameter v.s. Ours.}
\vspace{1mm}
\renewcommand{\arraystretch}{1.2}
\scalebox{0.95}{
    \begin{tabular}{l l  c c c c }
    \Xhline{1.2pt}
    Sample                &     Method     & Param Ratio        & Video Quality & Text-Video & Overall \\ 
    \Xhline{0.4pt}
    \multirow{2}{*}{Pretrained Model} & ModelScope~\cite{VideoFusion}    & 0.90              & 0.8875 & 0.8575 & 0.9300 \\
                                   & LVDM~\cite{he2022lvdm} & 0.57       & 0.9155 & 0.8555 & 0.9370\\ 
    \Xhline{0.4pt}
    \multirow{2}{*}{Open Website} & Make-A-Video~\cite{MakeAVideo}    & 4.76  & 0.5417 & 0.4958 & 0.5417 \\ 
                                & Imagen Video~\cite{2022ImagenVideo} & 7.97  & 0.4291 & 0.2582 & 0.3818 \\
    \Xhline{1.2pt}
    \end{tabular}
}
\label{tab:user_study}
\end{table}




\subsection{Qualitative Results}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{showcase.pdf}
    \caption{Generated samples of our VideoFactory. We can observe high-quality generated results with clear motion, rich detail, and well semantic alignment.}
    \label{fig:showcase_our}
    \vspace{-5mm}
\end{figure}

In Fig.~\ref{fig:com_imagen_makeavideo}, we show the text-to-video comparison results against Make-A-Video, Imagen Video, and Video LDM.
The prompts and generated results are collected from their official project website.
Make-A-Video only generates 1:1 videos, which limits user experience.
Compared with Imagen Video and Video LDM, our model generate the Panda and golden retriever with more vivid details.
Besides, we demonstrate more generated samples of our method in Fig.~\ref{fig:showcase_our}.
\textbf{Video demos can be found in our supplementary.}


\section{Conclusion}
In this paper, we propose a high-quality open-domain video generation framework namely VideoFactory, which produces high-definition (1376768), widescreen (16:9) videos without watermarks.
We revisit the spatial and temporal modeling in video generation, and present a novel swapped cross-attention mechanism which enables spatial and temporal information alternately to attend to each other.
Furthermore, we propose a widescreen, watermark-free, high-definition HD-VG-130M dataset, with 130 million open-domain text-video pairs to unlock the power of our model as much as possible.
Experiments confirm the high spatial quality, temporal consistency, and fitness to the text of synthesized videos from our VideoFactory, proving it the new benchmark of text-to-video generation.

\bibliographystyle{plain}
\bibliography{main}

\end{document}