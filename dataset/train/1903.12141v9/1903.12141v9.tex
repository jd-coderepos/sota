

\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} 

\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}



\usepackage[accepted]{icml2019}


\interfootnotelinepenalty=10000
\usepackage{xcolor,colortbl}

\newcommand{\mc}[2]{\multicolumn{#1}{c}{#2}}
\definecolor{Gray}{gray}{0.85}
\definecolor{LightCyan}{rgb}{0.48,0.78,1}
\definecolor{LightColor1}{rgb}{0.68,0.88,1.2}

\definecolor{LightGray}{gray}{.8}
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}

\newcolumntype{y}{>{\columncolor{yellow}}c}
\newcolumntype{a}{>{\columncolor{lightgray}}c}
\newcolumntype{g}{>{\columncolor{pink}}c}
\newcolumntype{o}{>{\columncolor{LightColor1}}c}
\newcolumntype{u}{>{\columncolor{LightCyan}}c}



\newcommand{\specialcell}[2][c]{\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

\PassOptionsToPackage{hyphens}{url}
\usepackage{hyperref}
\graphicspath{{Figures/}}
\usepackage[T1]{fontenc}

\usepackage{amsmath,amssymb,amsfonts} 
\usepackage{fixltx2e}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{amsmath}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\usepackage{subcaption}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{makecell}
\usepackage{graphicx}
\usepackage[T1]{fontenc}

\usepackage{etoolbox}
\makeatletter
\makeatother




\icmltitlerunning{Mean Absolute Error Does Not Treat Examples Equally and Gradient Magnitude's Variance Matters}

\begin{document}

\twocolumn[
\icmltitle{
	IMAE for Noise-Robust Learning: Mean Absolute Error Does Not Treat Examples Equally and Gradient Magnitude's Variance Matters  
}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Xinshao Wang}{oxford}
\icmlauthor{Yang Hua}{queens}
\icmlauthor{Elyor Kodirov}{anyvision}
\icmlauthor{Neil M. Robertson}{queens}
\end{icmlauthorlist}


\icmlaffiliation{oxford}{The University of Oxford, UK.}
\icmlaffiliation{anyvision}{Anyvision Research Team, UK}
\icmlaffiliation{queens}{Queen's University Belfast, UK.}

\icmlcorrespondingauthor{}{xinshao.wang@eng.ox.ac.uk; xinshaowang@gmail.com}




\icmlkeywords{Cross Entropy, Mean Absolute Error, Underfitting, Overfitting, Label Noise, Robustness}

\vskip 0.3in
]





\printAffiliationsAndNotice{
Work done at Queenâ€™s University Belfast and Anyvision.\\
~
\\
\href{https://arxiv.org/abs/1905.11233}{Derivative Manipulation for General Example Weighting, May, 2019~\cite{wang2019derivative}} is a following work and generalisation of IMAE, March, 2019. 
\\
~
\\}  





\date{}





\begin{abstract}


In this work, we study robust deep learning against abnormal training data from the perspective of example weighting built in empirical loss functions, i.e., gradient magnitude with respect to logits, an angle that is not thoroughly studied so far. 
Consequently, we have two key findings: 
(1) Mean Absolute Error (MAE) Does Not Treat Examples Equally. We present new observations and insightful analysis about MAE, which is theoretically proved to be noise-robust.
	First, we reveal its underfitting problem in practice.   
	Second, we analyse that MAE's noise-robustness is from emphasising on uncertain examples instead of treating training samples equally, as claimed in prior work.   
(2) The Variance of Gradient Magnitude Matters. We propose an effective and simple solution to enhance MAE's fitting ability while preserving its noise-robustness. 
	Without changing MAE's overall weighting scheme, i.e., what examples get higher weights, we simply change its weighting variance non-linearly so that the impact ratio between two examples are adjusted. 
	Our solution is termed Improved MAE (IMAE). 
We prove IMAE's effectiveness using extensive experiments: image classification under clean labels, synthetic label noise, and real-world unknown noise.   
	We conclude IMAE is superior to CCE, the most popular loss for training DNNs. 
\end{abstract}













\section{Introduction}

In this work, we target at robust deep learning, which is indispensable when it comes to large-scale industrial applications. 
It is non-affordable to guarantee the quality of training data as its scale grows dramatically. Consequently, abnormal examples\footnote{A training example is denoted as an observation-label pair, where the observation can be an image or video while the label defines its semantic information. We regard a training example as abnormal unrestrictedly whenever its observation and label are semantically unmatched, e.g., out-of-distribution examples (the observations contain only background or objects that do not belong to any training class), or examples with wrongly annotated labels.} generally exist in large-scale real-world scenarios \cite{berrada2018smooth}, which is caused by many factors, such as incomplete annotation, wrong labelling, subjectiveness, bias and so forth. 
Unfortunately, DNNs trained with categorical cross entropy (CCE) can fit random patterns \cite{zhang2017understanding}.


















Great advances have been made towards training DNNs robustly when abnormal training examples exist \cite{arpit2017closer,chang2017active,ren2018learning,jiang2018mentornet}. The robust loss function is one of them. In this paper, we study a so-claimed robust loss function, mean absolute error (MAE) following \cite{ghosh2017robust, zhang2018generalized}. 
According to the theoretical analysis of CCE and MAE in \cite{ghosh2017robust}, CCE is sensitive to label noise while MAE is noise-tolerant. Thereafter, generalised cross entropy (GCE) \cite{zhang2018generalized} concludes MAE treats training samples equally, thus being noise-robust. 

However, our empirical observation and technical analysis lead us to a contradictory and more reasonable conclusion. 

\textit{Observation: In Table~\ref{table:MAE_observations}, when 40\% noise exists, compared with CCE, MAE underfits to clean training data points, thus fitting much fewer abnormal examples. }

\textit{Conclusion: In Figure~\ref{fig:absolute_weight_CCE_MAE_IMAE_V02}, MAE emphasises more on uncertain examples, whose probabilities of being classified to its labelled class are around 0.5, thus being noise-robust. }  











Specifically, according to Table~\ref{table:MAE_observations}, MAE is much more noise-tolerant than CCE. However, its ability of learning meaningful patterns is much weaker, fitting only 74.3\% of the clean subset. We provide an intuitive interpretation for this according to Figure~\ref{fig:absolute_weight_CCE_MAE_IMAE_V02}:
\textit{The variance of MAE's weight curve along with probability is only 0.09. As a result, the impact ratio between two examples is too small}.\footnote{The terms, examples' weight or impact, and examples' gradient magnitude w.r.t. logits, are used interchangeably because we define the weight by gradient's magnitude. The impact ratio between two examples is changed only when gradients' magnitude is scaled non-linearly.}
The impact ratio reflects the relative impact of one example versus another for updating parameters.
Due to MAE's small weight variance, informative samples cannot contribute enough against non-informative ones. 
Therefore, MAE cannot learn meaningful patterns well and is not widely used.

To adjust MAE's weight variance, we design an effective and simple solution, IMAE, which non-linearly transforms MAE's weighting scheme by an exponential function. 
On the one hand, by preserving MAE's overall weighting scheme, IMAE is noise-robust. On the other hand, by making the gradient magnitude's variance over training examples controllable, it learns meaningful patterns much better.





We demonstrate the effectiveness of IMAE under different scenarios.
Most importantly, these empirical evidences justify that our interpretation of MAE's underfitting problem is reasonable and our proposed solution is superior. Our key findings are summarised as follows: 
\begin{itemize}
	\vspace{-0.3cm}
	\item CCE overfits to noise easily because it emphasises on low-probability examples to which abnormal ones generally belong. Although CCE's weight variance is not large (0.33), its fitting ability benefits from emphasising on low-probability examples. 
	
	\vspace{-0.2cm}
	\item MAE is noise-robust by focusing on uncertain (medium-probability) examples instead of treating all equally.  However, MAE generally underfits due to its small weights variance (0.09), leading to small impact ratio between even far different examples.
	  
	\vspace{-0.2cm}
	\item Our proposed IMAE achieves new state-of-the-art on robust training against synthetic label noise and realistic unknown noise simply by adjusting MAE's weight variance, which is inspiring.
	\vspace{-0.2cm}
\end{itemize}




\begin{table}[!t]
	\caption{
		Classification accuracy (\%) of CCE, MAE, and IMAE on CIFAR-10 \cite{krizhevsky2009learning}. 
		40\% of training examples, i.e., the noisy subset, have wrong labels. 
		We test each model's performance on test set, noisy subset and clean subset of training data. 
		The backbone is ResNet56 owning enough capacity \cite{he2016deep}. 
	}
	\vspace{-0.2cm}
	\centering
	\setlength{\tabcolsep}{0.85pt} \fontsize{9.6pt}{9.6pt}\selectfont
	\begin{tabular}{lccc}
		\toprule
		Loss &  \makecell{Test set\Noise-tolerance)} & \makecell{Clean subset\
	\label{eq:probability}
	p(j|\mathbf{x}_i) = \frac{\exp(\mathbf{z}_{ij})}{\sum_{m=1}^{C} \exp(\mathbf{z}_{im})} ,

	\label{eq:loss_CCE}
	\begin{aligned}
		L_{\mathrm{CCE}} (\mathbf{X};f_\theta,\mathbf{W}) 
&= 	- \frac{1}{N} \sum_{i=1}^N \sum_{j=1}^{C} q(j|\mathbf{x}_i) \log p(j|\mathbf{x}_i) \\
		&= 	
- \frac{1}{N} \sum_{i=1}^N \log p(y_i|\mathbf{x}_i)
		. 
	\end{aligned}

\vspace{-0.2cm}
	\label{eq:loss_MAE}
	\begin{aligned}
		L_{\mathrm{MAE}} (\mathbf{X};f_\theta,\mathbf{W}) 
&= 	 \frac{1}{N} \sum_{i=1}^N \sum_{j=1}^{C}
		|p(j|\mathbf{x}_i)- q(j|\mathbf{x}_i) |
		\\
		&= 	 \frac{2}{N} \sum_{i=1}^N 
		(1-p(y_i|\mathbf{x}_i))
		,
	\end{aligned}

	\label{eq:both_prob_derivation_final_MainPaper}
	\begin{aligned}
		\frac{\partial p(y_i|\mathbf{x}_i)}{\partial \mathbf{z}_{ij}} 
		&=
		\begin{cases} 
			p(y_i|\mathbf{x}_i) 
			(1-p(y_i|\mathbf{x}_i))
			\text{, } &j = y_i  \\
			-p(y_i|\mathbf{x}_i) 
			p(j|\mathbf{x}_i)  
			\text{, } &j \neq y_i
		\end{cases}
	\end{aligned}

	\label{eq:loss_CCE_x_i}
	\begin{aligned}
		L_{\mathrm{CCE}} (\mathbf{x}_i;f_\theta,\mathbf{W}) 
&= 	- \log p(y_i|\mathbf{x}_i)
		.
	\end{aligned}

	\label{eq:derivation_CCE_x_i_MainPaper}
	\begin{aligned}
		\frac{\partial L_{\mathrm{CCE}}(\mathbf{x}_i)}{\partial p(j|\mathbf{x}_i)} 
		&=
		\begin{cases} 
-p(y_i|\mathbf{x}_i)^{-1} 
			\text{, } &j = y_i  \\
			0       
			\text{, } &j \neq y_i
		\end{cases}
		.
	\end{aligned}

	\label{eq:loss_MAE_x_i}
	\begin{aligned}
		L_{\mathrm{MAE}} (\mathbf{x}_i;f_\theta,\mathbf{W}) 
&= 	2(1- (p(y_i|\mathbf{x}_i))
		.
	\end{aligned}

	\label{eq:derivation_MAE_x_i_MainPaper}
	\begin{aligned}
		\frac{\partial L_{\mathrm{MAE}}(\mathbf{x}_i)}{\partial p(j|\mathbf{x}_i)} 
		&=
		\begin{cases} 
			-2  \text{, } &j = y_i  \\
			0        \text{, } &j \neq y_i
		\end{cases}
		.
	\end{aligned}

	\label{eq:summary_CCE_z_MainPaper}
	\begin{aligned}
		\frac{\partial L_{\mathrm{CCE}}(\mathbf{x}_i)}{\partial \mathbf{z}_{ij}} 
		&=
		\begin{cases} 
			p(y_i|\mathbf{x}_i)-1  \text{, } &j = y_i  \\
			p(j|\mathbf{x}_i)        \text{, } &j \neq y_i
		\end{cases}
		.
	\end{aligned}

	\label{eq:summary_MAE_z_MainPaper}
	\begin{aligned}
		\frac{\partial L_{\mathrm{MAE}}(\mathbf{x}_i)}{\partial \mathbf{z}_{ij}} 
		&=
		\begin{cases} 
			2 p(y_i|\mathbf{x}_i)
			(p(y_i|\mathbf{x}_i)-1) \text{,} &j = y_i  \\
			2 p(y_i|\mathbf{x}_i)
			p(j|\mathbf{x}_i) \text{,} &j \neq y_i
		\end{cases}
		.
	\end{aligned}

	\label{eq:implicit_CCE}
	w_\mathrm{CCE}(\mathbf{x}_{i}) = ||\frac{\partial L_{\mathrm{CCE}}(\mathbf{x}_i)}{\partial \mathbf{z}_{i}}||_1 = 2
(1-p(y_i|\mathbf{x}_i)),

	\label{eq:implicit_MAE}
	w_\mathrm{MAE} (\mathbf{x}_{i}) = ||\frac{\partial L_{\mathrm{MAE}}(\mathbf{x}_i)}{\partial \mathbf{z}_{i}}||_1 = 4
p(y_i|\mathbf{x}_i) 
(1-p(y_i|\mathbf{x}_i)).

	w_\mathrm{IMAE} (\mathbf{x}_{i}) =  \exp(T   
p(y_i|\mathbf{x}_i) 
(1-p(y_i|\mathbf{x}_i))),
	\label{eq:weight_IMAE}

	\label{eq:implicit_IMAE}
	\begin{aligned}
		&\frac{\partial L_{\mathrm{IMAE}}(\mathbf{x}_i)}{\partial \mathbf{z}_{i}} = \frac{\partial L_{\mathrm{MAE}}(\mathbf{x}_i)}{\partial \mathbf{z}_{i}}  \frac{w_\mathrm{IMAE} (\mathbf{x}_{i})}{w_\mathrm{MAE} (\mathbf{x}_{i})}\\
		&=> ||\frac{\partial L_{\mathrm{IMAE}}(\mathbf{x}_i)}{\partial \mathbf{z}_{i}}||_1 = w_\mathrm{IMAE} (\mathbf{x}_{i}).
	\end{aligned}

	\begin{aligned}
		\sigma_{\mathrm{MAE}}&=
		\displaystyle\int^{1}_{0} 
		w_{\mathrm{MAE}}^{2} (p)
		\, \mathrm d p
		- (\displaystyle\int^{1}_{0} 
		w_{\mathrm{MAE}} (p)
		\, \mathrm d p)^2
	\end{aligned}

	\label{eq:variance_IMAE}
	\begin{aligned}
		\sigma_{\mathrm{IMAE}}&=
		\displaystyle\int^{1}_{0} 
		w_{\mathrm{\mathrm{IMAE}}}^{2} (p)
		\, \mathrm d p
		- (\displaystyle\int^{1}_{0} 
		w_{\mathrm{IMAE}} (p)
		\, \mathrm d p)^2.
	\end{aligned}

	w_{\mathrm{IMAE}} (p) =  e^{T  \cdot
p 
(1-p)},

	\begin{aligned}
		\sigma_{\mathrm{IMAE}}&=
		\displaystyle\int^{1}_{0} 
		w_{\mathrm{\mathrm{IMAE}}}^{2} (p)
		\, \mathrm d p
		- (\displaystyle\int^{1}_{0} 
		w_{\mathrm{IMAE}} (p)
		\, \mathrm d p)^2 \\
		&=
		\displaystyle\int^{1}_{0} \mathrm{e}^{2Tp{}\left(1-p\right)}\, \mathrm d p
		-
		(\displaystyle\int^{1}_{0} \mathrm{e}^{Tp{}\left(1-p\right)}\, \mathrm d p)^2 \\
		&=
		{{\sqrt{\pi}\,\mathrm{erf}\left({{\sqrt{2T}}\over{{2}}}\right)
				\,e^{{{T}\over{2}}}}\over{\sqrt{2T}}}
		-
		{{{\pi}\,\mathrm{erf^2}\left({{\sqrt{T}}\over{2}}\right)\,e^{{{T
						}\over{2}}}}\over{{T}}}
		.
	\end{aligned}
lr: 0.01)} & \makecell{SGD + Momentum \\ (lr: 0.01)} & \makecell{Nesterov \lr: 0.01, \\delta: 0.1)} & \makecell{Adam \lr: 0.005, \\delta: 1)} \\
		\midrule
		CCE & 64.3 & 60.6 & 56.4 & 42.5 & 44.5 & 50.3\\
		MAE & 39.3 & 64.7 & 64.1 & 68.2 & 59.9 & 41.4\\
		GCE & 68.8 & 80.5 & 79.7 & 73.2 & 70.6 & 69.3\\
		IMAE & \textbf{82.0} & \textbf{83.5} & \textbf{83.7} & \textbf{75.5} & \textbf{76.3} & \textbf{78.6}\\
		\bottomrule
	\end{tabular}
	\label{table:stochastic_optimisers}
	\vspace{-0.2cm}
\end{table*}



\section{Video person re-identification }

\noindent
\textbf{Dataset and evaluation settings.} MARS contains 20,715 videos of 1,261 persons \cite{zheng2016mars}. 
There are 1,067,516 frames in total. 
Because person videos are collected by tracking and detection algorithms, 
abnormal examples exist as shown in Figure~\ref{fig:abnormal_examples}.
The exact noise rate is unknown.
Following standard settings, we use 8,298 videos of 625 persons for training 
and 12,180 videos of other 636 persons for testing. 
We report the cumulated matching characteristics (CMC) and mean average precision (mAP) results.

\noindent
\textbf{Implementation details.\footnote{We explore the performance of different losses in real-world applications instead of pushing the state-of-the-art results.}}  Following \cite{liu2017qan,wang2019deep}, we train GoogleNet V2. We also treat a video as an image set, which means we use only appearance information without exploiting latent temporal information. 
A video's representation is simply the average fusion of its frames' representations.
\textit{We apply the same training settings for each loss}. The learning rate starts from 0.01 and is divided by 2 every 10k iterations. We stop training at 50k iterations. We choose SGD optimiser with a weight decay of 0.0005 and momentum of 0.9. The batch size is set to 180. We use standard data augmentation: a  crop is randomly sampled and flipped after resizing an original image to . 
At testing, following \cite{wang2019deep,movshovitz2017no,law2017deep}, we first  normalise videos' features and then calculate the cosine similarity between every two features.  


\noindent
\textbf{Results.} We compare our method with CCE, MAE and GCE. 
We implement GCE with its best settings. 
The results are shown in Table~\ref{table:MARS_ReID}. 
IMAE outperforms other related methods by a significant margin.




\begin{table}[!h]
\caption{
		The retrieval results of CCE, MAE, GCE and IMAE on MARS with GoogLeNet V2 \cite{ioffe2015batch}.
	}
	\centering
	\vspace{-0.2cm}
	\setlength{\tabcolsep}{6pt} \begin{tabular}{lcccc}
		\hline
		Metric & CCE & MAE & GCE & IMAE\\
		\hline
		mAP (\%) & 58.1 & 12.0 & 31.6 & \textbf{70.9}\\
		CMC-1 (\%) & 73.8 & 26.0 & 51.5 & \textbf{83.5}\\
		\hline
	\end{tabular}
	\label{table:MARS_ReID}
	\vspace{-0.3cm}
\end{table}








\begin{figure*}[t!]
	\centering
	\begin{subfigure}[t!]{0.33\linewidth}
		\centering
		\includegraphics[width=0.780\linewidth]{N08_ResNet20_CCE_MAE_SMAE_Test}
		\caption{ResNet20: Testing set (higher is better).}
	\end{subfigure}\begin{subfigure}[t!]{0.33\linewidth}
		\centering
		\includegraphics[width=0.780\linewidth]{N08_ResNet20_CCE_MAE_SMAE_NoisyPart}
		\caption{ResNet20: Noisy subset  (lower is better).}
	\end{subfigure}
	\begin{subfigure}[t!]{0.33\linewidth}
		\centering
		\includegraphics[width=0.780\linewidth]{N08_ResNet20_CCE_MAE_SMAE_CleanPart}
		\caption{ResNet20: Clean subset (higher is better).}
	\end{subfigure}
\begin{subfigure}[t!]{0.33\linewidth}
		\centering
		\includegraphics[width=0.780\linewidth]{N08_ResNet56_CCE_MAE_SMAE_Test}
		\caption{ResNet56: Testing set (higher is better).}
	\end{subfigure}\begin{subfigure}[t!]{0.33\linewidth}
		\centering
		\includegraphics[width=0.780\linewidth]{N08_ResNet56_CCE_MAE_SMAE_NoisyPart}
		\caption{ResNet56: Noisy subset  (lower is better).}
	\end{subfigure}
	\begin{subfigure}[t!]{0.33\linewidth}
		\centering
		\includegraphics[width=0.780\linewidth]{N08_ResNet56_CCE_MAE_SMAE_CleanPart}
		\caption{ResNet56: Clean subset (higher is better).}
	\end{subfigure}
\caption{CIFAR-10 with noise rate . The accuracies on testing set, noisy subset and clean subset of training set along with training iterations.
		The legend on the top left is shared by all subfigures. 
		\textit{Better viewed in colour.} }
	\label{fig:ResNet2056_Noise80_CCE_MAE_SMAE_dynamics}
\end{figure*}





\section{The Results of IMAE Using Different Stochastic Optimisers}
In this section, we study the performance of IMAE when different stochastic optimisers are used. The results are presented in Table~\ref{table:stochastic_optimisers}. We observe that IMAE's results are the best consistently. 







\section{Derivation of Softmax, CCE and MAE Layers}

\subsection{Derivation of softmax layer}
As the softmax layer is shared by CCE and MAE, we present the derivation of softmax layer first. 
Based on Eq.~(\ref{eq:probability}), we have





\noindent
If , for left and right sides of Eq.~(\ref{eq:predict_probability}), we calculate their derivatives w.r.t.  simultaneously:





\noindent
If , analogously we have:



\noindent
{In summary}, the derivation of softmax layer is:


\noindent
\subsection{Derivation of loss layer: CCE}
According to Eq.~(\ref{eq:loss_CCE}), 
we have

Therefore, we obtain (the parameters are omitted for brevity),   




\noindent
\subsection{Derivation of loss layer: MAE}
According to Eq.~(\ref{eq:loss_MAE}), 
we have

Therefore, we obtain 



\subsection{Derivatives w.r.t. }




\noindent
{}: The calculation is based on Eq.~(\ref{eq:derivation_CCE_x_i}) and Eq.~(\ref{eq:both_prob_derivation_final}).

\noindent
If , we have:


\noindent
If , it becomes:


\noindent
{In summary},  can be represented as:


\noindent
: 
The calculation is analogous with that of  .
According to Eq.~(\ref{eq:derivation_MAE_x_i}) and Eq.~(\ref{eq:both_prob_derivation_final}), if :

otherwise ():


\noindent
{In summary},  is:



\end{document}
