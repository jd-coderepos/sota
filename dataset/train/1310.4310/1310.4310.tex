\documentclass{article}

\usepackage{fullpage}

\usepackage[draft]{hyll-report}

\usepackage{lastpage}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage{lmodern}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{relsize}
\usepackage{xspace}



\def\eqdef{\overset{\mathrm{def}}{=}} 

\newcommand\proofsystem[1]{{\smaller\rmfamily\slshape #1}\xspace}
\newcommand\LF{\proofsystem{LF}}
\newcommand\HLF{\proofsystem{HLF}}
\newcommand\Spi{\proofsystem{S}}
\newcommand\CTMC{\proofsystem{CTMC}}

\title{A Hybrid Linear Logic for Constrained Transition Systems
       \\ with Applications to Molecular Biology}

\author{
  Kaustuv Chaudhuri\\INRIA\\\texttt{kaustuv.chaudhuri@inria.fr}
  \and
  Jo\"{e}lle Despeyroux\\INRIA and CNRS, I3S\\\texttt{joelle.despeyroux@inria.fr}
}

\date{{\em Rapport de recherche INRIA-HAL nb} inria-00402942 --- \today\ --- \pageref{LastPage} pages}


\begin{document}

\maketitle

\begin{abstract}
  Linear implication can represent state transitions, but real transition
  systems operate under temporal, stochastic or probabilistic constraints that
  are not directly representable in ordinary linear logic. We propose a general
  modal extension of intuitionistic linear logic where logical truth is indexed
  by constraints and hybrid connectives combine constraint reasoning with
  logical reasoning. The logic has a focused cut-free sequent calculus that can
  be used to internalize the rules of particular constrained transition systems;
  we illustrate this with an adequate encoding of the synchronous stochastic
  pi-calculus.
  We also present some preliminary experiments of direct encoding of biological 
  systems in the logic.
\end{abstract}

\section{Introduction}
\label{sec:intro}

To reason about state transition systems, we need a logic of state. 
Linear logic~\cite{girard87tcs} is such a logic and has been successfully used
to model such diverse systems as 
process calculi, references and concurrency in programming languages, 
security protocols, multi-set rewriting, and graph algorithms.
Linear logic achieves this versatility by representing propositions as \emph{resources}
that are composed into elements of state using "tens", which can then be
transformed using the linear implication ("-o").  However, linear implication is
timeless: there is no way to correlate two concurrent transitions.
If resources have lifetimes and state changes have temporal, probabilistic or
stochastic \emph{constraints}, then the logic will allow inferences that may not
be realizable in the system being modelized. 
The need for formal reaosning in such constrained systems has led to the
creation of specialized formalisms such as Computation Tree Logic
(\proofsystem{CTL})\cite{Emerson95}, Continuous Stochastic Logic
(\proofsystem{CSL})~\cite{aziz00tcl} or Probabilistic CTL
(\proofsystem{PCTL})~\cite{hansson94fac}.
These approaches pay a considerable encoding overhead for the states and
transitions in exchange for the constraint reasoning not provided by linear logic.
A prominent alternative to the logical approach is to use a suitably enriched
process algebra; a short list of examples includes 
reversible CCS~\cite{danos03bc}, bioambients~\cite{regev04tcs}, 
brane calculi~\cite{cardelli03bc}, stochastic and probabilistic -calculi, 
the PEPA algebra~\cite{hillston96book}, and the -calculus~\cite{danos04tcs}.
Each process algebra comes equipped with an underlying algebraic semantics which
is used to justify mechanistic abstractions of observed reality as processes. 
These abstractions are then animated by means of simulation and then
compared with the observations. 
Process calculi do not however completely fill the need for
{\em formal logical reasoning for constrained transition systems}. 
For example, there is no uniform process calculus to encode 
different stochastic process algebras\footnote{Stochastic process algebras are 
typical examples of the constrained transition systems we aim at formalizing.}. 

Note that logics like CSL or CTL are not such uniform languages either. These formalisms
are not \emph{logical frameworks}\footnote
{
Logical frameworks are uniform languages that allow to formally not only specify and analyse,
but also compare, or translate from one to the other, different systems, through their 
(adequate) encodings.
}: 
Encoding the stochastic  calculus in CSL, for example, would be
inordinately complex because CSL does not provide any direct means of encoding
-calculus dynamics such as the linear production and consumption of
messages in a synchronous interaction.
Actually CSL and CTL mainly aim at specifying properties of behaviors of constrained 
transition systems, not the systems themselves.

We propose a simple yet general method to add constraint reasoning to linear logic.
It is an old idea---\emph{labelled deduction}~\cite{simpson94phd} with 
\emph{hybrid} connectives~\cite{brauener06jal}---applied to a new domain. 
Precisely, we parameterize ordinary logical truth on a \emph{constraint domain}: 
"A @ w" stands for the truth of "A" under constraint "w". 
Only a basic monoidal structure is assumed about the constraints from a
proof-theoretic standpoint.
We then use the hybrid connectives of \emph{satisfaction} ("at") and
\emph{localization} ("dn") to perform generic symbolic reasoning on the
constraints at the propositional level.
We call the result \emph{hybrid linear logic} (\hyll); it has a generic cut-free
(but cut admitting) sequent calculus that can be strengthened with a focusing
restriction~\cite{andreoli92jlc} to obtain a normal form for proofs.
Any instance of \hyll that gives a semantic interpretation to the constraints
enjoys these proof-theoretic properties.

Focusing allows us to treat \hyll as a \emph{logical framework} 
for constrained transition systems.
Logical frameworks with hybrid connectives have been considered before; hybrid
\LF (\HLF), for example, is a generic mechanism to add many different kinds of
resource-awareness, including linearity, to ordinary \LF~\cite{reed06hylo}.
\HLF follows the usual \LF methodology of keeping the logic of the framework
minimal: its proof objects are -normal -long natural deduction
terms, but the equational theory of such terms is sensitive to permutative
equivalences~\cite{watkins03tr}.
With a focused sequent calculus, we have more direct access to a canonical
representation of proofs, so we can enrich the framework with any connectives
that obey the focusing discipline.
The representational adequacy \footnote
{Encodings -of a system or of a property of a system- in a logical framework
are always required to be {\it adequate} in a strong sense 
sometimes called {\it representational adequacy} and illustrated here in \secref{spi.adq}.
}
of an encoding in terms of (partial) focused
sequent derivations tends to be more straightforward than in a natural deduction
formulation.
We illustrate this by encoding the synchronous stochastic -calculus (\Spi)
in \hyll using rate functions as constraints.

In addition to the novel stochastic component, our encoding of \Spi is a
conceptual improvement over other encodings of -calculi in linear logic.
In particular, we perform a full propositional reflection of processes as
in~\cite{miller92welp}, but our encoding is first-order and adequate as
in~\cite{cervesato03tr}.
\hyll does not itself prescribe an operational semantics for the encoding of
processes; thus, bisimilarity in continuous time Markov chains (\CTMC) is not
the same as logical equivalence in stochastic \hyll, unlike in
\proofsystem{CSL}~\cite{desharmais03jlap}.
This is not a deficiency; rather, the \emph{combination} of focused \hyll proofs
and a proof search strategy tailored to a particular encoding is necessary to
produce faithful symbolic executions.
This exactly mirrors \Spi where it is the simulation rather than the transitions
in the process calculus that is shown to be faithful to the \CTMC
semantics~\cite{phillips04cmmb}.

This work has the following main contributions.
First is the logic \hyll itself and its associated proof-theory, which has a
very standard and well understood design in the Martin-LÃ¶f tradition.
Second, we show how to obtain many different instances of \hyll for particular
constraint domains because we only assume a basic monoidal structure for
constraints.
Third, we illustrate the use of focused sequent derivations to obtain adequate
encodings by giving a novel adequate encoding of \Spi.
Our encoding is, in fact, \emph{fully adequate}, \ie, partial focused proofs are
in bijection with traces.
The ability to encode \Spi gives an indication of the versatility of \hyll.
Finally, we show how to encode (a very simple example of) biological systems in \hyll.
This is a preliminary step
towards a logical framework for systems biology, our initial motivation for this work.

The sections of this paper are organized as follows: in \secref{hyll}, we
present the inference system (natural deduction and sequent calculus) for \hyll
and describe the two main semantic instances: temporal and probabilistic constraints.  
In \secref{focusing} we sketch the general focusing restriction on \hyll sequent
proofs. 
In \secref{spi} we give the encoding of \spi in probabilistic \hyll, and show that
the encoding is representationally adequate for focused proofs
(theorems \ref{thm:completeness} and \ref{thm:adeq}).  
In \secref{bio} we present some preliminary experiments of direct encoding of 
biological systems in \hyll.
We end with an overview of related (\secref{related}) and future work 
(\secref{concl}).

\section{Hybrid linear logic}
\label{sec:hyll}

In this section we define \hyll, a conservative extension of intuitionistic
first-order linear logic (\ill)~\cite{girard87tcs} where the truth judgements
are labelled by worlds representing constraints. Like in \ill, propositions are
interpreted as \emph{resources} which may be composed into a \emph{state} using
the usual linear connectives, and the linear implication ("-o") denotes a
transition between states. The world label of a judgement represents a
constraint on states and state transitions; particular choices for the worlds
produce particular instances of \hyll. The common component in all the instances
of \hyll is the proof theory, which we fix once and for all. We impose the
following minimal requirement on the kinds of constraints that \hyll can deal
with.

\begin{defn} \label{defn:constraint-domain}
A \emph{constraint domain} "\cal W" is a monoid structure
  "\langle W, ., rid\rangle". The elements of "W" are called \emph{worlds}, and
  the partial order "\preceq\ : W \times W"---defined as "u \preceq w" if there
  exists "v \in W" such that "u . v = w"---is the \emph{reachability relation}
  in "\cal W".
\end{defn}

\noindent
The identity world "rid" is "\preceq"-initial and is intended to represent the
lack of any constraints. Thus, the ordinary \ill is embeddable into any instance
of \hyll by setting all world labels to the identity. When needed to
disambiguate, the instance of \hyll for the constraint domain "\cal W" will be
written \chyll[W].

The reader may wonder why we require worlds to be monoids, instead of lattices, for example.
The answer is to give a more general definition, suitable for rates constraints. 
One may then ask why we don't ask the worlds to be commutative. The answer is to allow 
lattices.

Atomic propositions are written using minuscule letters ("a, b, ...") applied to
a sequence of \emph{terms} ("s, t, \ldots"), which are drawn from an untyped
term language containing term variables ("x, y, \ldots") and function symbols
("f, g, ...") applied to a list of terms.. Non-atomic propositions are
constructed from the connectives of first-order intuitionistic linear logic and
the two hybrid connectives \emph{satisfaction} ("at"), which states that a
proposition is true at a given world ("w, u, v, \ldots"), and
\emph{localization} ("dn"), which binds a name for the (current) world the proposition is
true at. The following grammar summarizes the syntax of \hyll propositions.

\smallskip
\bgroup
\begin{tabular}{l@{\ }r@{\ }l}
  "A, B, ..." & "::=" & "a~\vec t OR A tens B OR one OR A -o B OR A with B OR top OR A plus B OR zero OR ! A OR all x. A OR ex x. A" \\ 
              & "|\ " & "(A at w) OR now u. A OR all u. A OR ex u. A" \\
\end{tabular}
\egroup

\smallskip
\noindent
Note that in the propositions "now u. A", "all u. A" and "ex u. A", the scope of
the world variable "u" is all the worlds occurring in "A". World variables
cannot be used in terms, and neither can term variables occur in worlds; this
restriction is important for the modular design of \hyll because it keeps purely
logical truth separate from constraint truth.  We let "\alpha" range over
variables of either kind.

The unrestricted connectives "and", "or", "imp", \etc of intuitionistic
(non-linear) logic can also be defined in terms of the linear connectives and
the exponential "!"  using any of the available embeddings of intuitionistic
logic into linear logic, such as Girard's embedding~\cite{girard87tcs}.

\subsection{Natural deduction for \hyll}

We start with the judgements from linear logic~\cite{girard87tcs} and enrich
them with a modal situated truth. We present the syntax of hybrid linear logic
in a natural deduction style, using Martin-L\"{o}f's principle of separating
judgements and logical connectives.  
Instead of the ordinary mathematical judgement ``"A" is true'', for a proposition ,
judgements of \hyll are of the form ``"A" is true at world "w"'', abbreviated as "A @ w". 
We use dyadic hypothetical derivations of
the form "\G ; \D \ "|-"\ C @ w" where "\G" and "\D" are sets of judgements of
the form "A @ w", with "\D" being moreover a \emph{multiset}.  "\G" is called
the \emph{unrestricted context}: its hypotheses can be consumed any number of
times.  "\D" is a \emph{linear context}: every hypothesis in it must be consumed
singly in the proof.
Note that in a judgement "A @ w" (as in a proposition "A at w"),  can be 
any expression in , not only a variable.

The rules for the linear connectives are borrowed from \cite{chaudhuri03tr}
where they are discussed at length, so we omit a more thorough discussion here.
The rules for the first-order quantifiers are completely standard.  The
unrestricted context "\G" enjoys weakening and contraction; as usual, this is a
theorem that is attested by the inference rules of the logic, and we omit its
straightforward inductive proof. 
The notation  stands for the replacement of all free occurrences
of the variable  in  with the expression , avoiding capture. 
Note that the expressions in the rules are to be readen up to alpha-conversion.

\begin{thm}[structural properties] \label{thm:struct} \mbox{}
\begin{ecom}
  \item If "\G ; \D |-nd C @ w", then "\G, \G' ; \D |-nd C @ w". (weakening)
  \item If "\G, A @ u, A @ u ; \D |-nd C @ w", then "\G, A @ u ; \D |-nd C @ w". (contraction)
  \end{ecom}
\end{thm}

\begin{figure*}[tp]
\framebox{ \begin{minipage}{0.9\textwidth}

\setlength{\parindent}{0pt}

\paragraph{Judgemental rules}


\paragraph{Multiplicatives}
1ex]
  \I["one I"]{"\G ; . |-nd one @ w"}
  \SP
  \I["one E"]{"\G ; \D, \D' |-nd C @ {w'}"}
    {"\G ; \D |-nd one @ w" & "\G ; \D' |-nd C @ {w'}"}
  \

\paragraph{Additives}
1ex]
  \I["plus I_i"]{"\G ; \D |-nd A_1 plus A_2 @ w"}
    {"\G ; \D |-nd A_i @ w"}
  \SP
  \I["plus E"]{"\G ; \D, \D' |-nd C @ {w'}"}
    {"\G ; \D |-nd A plus B @ w"
     &
     \begin{array}[b]{c}
       "\G ; \D', A @ w |-nd C @ {w'}" \\
       "\G ; \D', B @ w |-nd C @ {w'}"
     \end{array}
    }
  \

\paragraph{Quantifiers}
1ex]
  \I["\exists I"]{"\G ; \D |-nd \fex \alpha A @ w"}
    {"\G ; \D |-nd [\tau / x] A @ w"}
  \SP
  \I["\exists E^\alpha"]{"\G ; \D, \D' |-nd C @ {w'}"}
    {"\G ; \D |-nd \fex \alpha A @ w" & "\G ; \D', A @ w |-nd C @ {w'}"}

  \I["! I"]{"\G ; . |-nd {! A} @ w"}
    {"\G ; . |-nd A @ w"}
  \SP
  \I["! E"]{"\G ; \D, \D' |-nd C @ {w'}"}
    {"\G ; \D |-nd {! A} @ w" & "\G, A @ w ; \D' |-nd C @ {w'}"}

  \I["at I"]{"\G ; \D |-nd (A at w) @ {w'}"}
    {"\G ; \D |-nd A @ w"}
  \SP
  \I["at E"]{"\G ; \D |-nd A @ w"}
    {"\G ; \D |-nd (A at w) @ {w'}"}
  \
\end{minipage}}
\caption{Natural deduction for \hyll}.
\label{fig:nd-rules}
\end{figure*}

The full collection of inference rules are in \figref{nd-rules}. A brief
discussion of the hybrid rules follows. To introduce the \emph{satisfaction}
proposition "(A at w)" (at any world "w'"), the proposition "A" must be true in the
world "w". The proposition "(A at w)" itself is then true at any world, not just
in the world "w". In other words, "(A at w)" carries with it the world at which
it is true. Therefore, suppose we know that "(A at w)" is true (at any world "w'");
then, we also know that "A @ w". These two introduction and
elimination rules match up precisely to (de)construct the information in the "A
@ w" judgement.
The other hybrid connective of \emph{localisation}, "dn", is intended to be
able to name the current world. That is, if "now u. A" is true at world "w",
then the variable "u" stands for "w" in the body "A". This interpretation is
reflected in its introduction rule "{dn} I". For elimination, suppose we have a
proof of "now u. A @ w" for some world "w". Then, we also know "[w / u] A @ w".

For the linear and unrestricted hypotheses, substitution is no different 
from that of the usual linear logic.

\begin{thm}[substitution] \label{thm:subst} \mbox{}
\begin{ecom}
  \item \label{thm:subst.1}
    If "\G ; \D |-nd A @ u" and "\G ; \D', A @ u |-nd C @ w", then "\G ; \D, \D' |-nd C @ w".
  \item  \label{thm:subst.2}
    If "\G ; . |-nd A @ u" and "\G, A @ u ; \D |-nd C @ w", then "\G ; \D |-nd C @ w".
  \end{ecom}
\end{thm}

\begin{proof}[Proof sketch]
  By structural induction on the second given derivation in each case.
\end{proof}

Note that the "dn" connective commutes with every propositional connective, including itself. That
is, "now u. (A * B)" is equivalent to "(now u. A) * (now u. B)" for all binary connectives "*", and
"now u. * A" is equivalent to "* (now u. A)" for every unary connective "*", assuming the
commutation will not cause an unsound capture of "u". It is purely a matter of taste where to place
the "dn", and repetitions are harmless.

\begin{thm}[conservativity]
  \label{thm:conserv}
  Call a proposition or multiset of propositions \emph{pure} if it contains no
  instance of the hybrid connectives and no instance of quantification over a world variable, 
  and let "\G", "\D" and "A" be pure. 
  Then, "\G ; \D |-nd A @ w" in \hyll iff "\G ; \D |-nd A" in intuitionistic linear logic.
\end{thm}

\begin{proof}
  By structural induction on the given \hyll derivation.
\end{proof}

\subsection{Sequent calculus for \hyll}

In this section, we give a sequent calculus presentation of \hyll and prove a
cut-admissibility theorem.  The sequent formulation in turn will lead to an
analysis of the polarities of the connectives in order to get a focused sequent
calculus that can be used to compile a logical theory into a system of derived
inference rules with nice properties (\secref{focusing}).  For instance, if a
given theory defines a transition system, then the derived rules of the focused
calculus will exactly exhibit the same transitions. This is key to obtain the
necessary representational adequacy theorems,
as we shall see for the \spi-calculus example chosen in this paper (\secref{spi.adq}). 

In the sequent calculus, we depart from the linear hypothetical judgement "|-"
which has only an ``active'' right-hand side to a sequent arrow "==>" that has
active zones on both sides. A rule that infers a proposition on the right of the
sequent arrow is called a ``right'' rule, and corresponds exactly to the
introduction rules of natural deduction. Dually, introductions on the left of
the sequent arrow correspond to elimination rules of natural deduction; however,
as all rules in the sequent calculus are introduction rules, the information
flow in a sequent derivation is always in the same direction: from the
conclusion to the premises, incidentally making the sequent calculus ideally
suited for proof-search.


\begin{figure}[p]
\framebox{ 
\begin{minipage}{0.9\textwidth}
  \setlength{\parindent}{0pt}
   \bgroup \small
   \paragraph{Judgemental rules}
   

   \paragraph{Multiplicatives}
   1ex]
     \I["{one}R"]{"\G ; . ==> one @ w"}
     \SP
     \I["{one}L"]{"\G ; \D, one @ u ==> C @ w"}{"\G ; \D ==> C @ w"}
     \SP
     \I["{-o}R"]{"\G ; \D ==> A -o B @ w"}{"\G ; \D, A @ w ==> B @ w"}
     \

   \paragraph{Additives}
   1ex]
     \I["{with}L_i"]{"\G ; \D, \D', A_1 with A_2 @ u ==> C @ w"}
     {"\G ; \D, A_i @ u ==> C @ w"}
     \1ex]
     \I["{plus}L"]{"\G ; \D, A plus B @ u ==> C @ w"}
     {"\G ; \D, A @ u ==> C @ w" & "\G ; \D, B @ u ==> C @ w"}
   
     \I["\forall R^\alpha"]{"\G ; \D ==> \fall \alpha A @ w"}{"\G ; \D ==> A @ w"}
     \SP
     \I["\forall L"]{"\G ; \D, \fall \alpha A @ u ==> C @ w"}
     {"\G ; \D, [\tau / \alpha] A @ u ==> C @ w"}
     \

   For "\forall R^\alpha" and "\exists L^\alpha", "\alpha" is assumed to
   be fresh with respect to the conclusion. For "\exists R" and "\forall
   L", "\tau" stands for a term or world, as appropriate.

   \paragraph{Exponentials}
   

   \paragraph{Hybrid connectives}

   1ex]
     \I["{dn}R"]{"\G ; \D ==> now u. A @ w"}{"\G ; \D ==> [w/u] A @ w"}
     \LSP
     \I["{dn}L"]{"\G ; \D, now u. A @ v ==> C @ w"}{"\G ; \D, [v/u] A @ v ==> C @ w"}
   
  \Ic[]{"\G ; A_1 @ {u . w_1} \cdots A_k @ {u . w_k} |- B @ {u . v}"}
         {"\G ; A_1 @ w_1 \cdots  A_k @ w_k |- B @ v"}

    \begin{aligned}
      "box A" &\triangleq "now u. all w. (A at u . w)" & \qquad
      "dia A" &\triangleq "now u. ex w. (A at u . w)" \\
      "rate v A" &\triangleq "now u. (A at u . v)" &
      "!! A" &\triangleq "all u. (A at u)"
    \end{aligned}
  
  \Ic["\rhoup R"]{"\G ; \D |- rate v A @ w"}{"\G ; \D |- A @ {w . v}"}

  \Ic[]{"\G ; \D, \D' \not\vdash (now u. (u \neq w) tens A) @ w"}
       {"\G ; \D |- (w \neq w) @ w" \quad "\G ; \D' |- A @ w"}

     \I["at' R"]{"\G ; \D ==> (A at' u) @ v"}{"\G ; \D ==> A @ u" \quad "(w \neq u)"}
     \LSP
     \I["at' L"]{"\G ; \D, (A at' u) @ v ==> C @ w"}
                {"\G ; \D, A @ u ==> C @ w" \quad "(w \neq u)"}
    "H~A" \triangleq "now u. all w. (A at u - w)" \qquad
   "P~A" \triangleq "now u. ex w. (A at u - w)"

\mu_{X+Y}(A)=\mu_X*\mu_Y(A)=\int_{\{x+y\in A\}}\mu_X(dx)\otimes\mu_Y(dy)

(P(t)f)(x)  
           = \mathbf{E}[f(X_t)\mid X_0=x],

P(t+s) = P(t) * P(s)

(P(t)f)(x) = {\mathbf E}[f(X_t)\mid X_0=x].

(P(t)f)(x) = (P(t){\mathbf 1}_A)(x)
= {\mathbf E}[{\mathbf 1}_A(X_t)\mid X_0=x]
= P\{X_t \in A \mid X_0=x\}

(P(t)~ \text{sq})(x) - (P(t)~ \text{id})^2(x) = {\mathbf E}(X_t^2 \mid X_0=x) -
({\mathbf E}(X_t \mid X_0=x))^2
  = \mathbf{Var} (X_t \mid X_0=x)

    \I[li]{"\G ; foc{n\ \vec t @ w} ==> pos {n\ \vec t @ w}"}
    \quad
    \I["neg L"]{"\G ; \D ; foc{neg P @ u} ==> Q @ w"}{"\G ; \D ; P @ u ==> . ; Q @ w"}
    \qquad
    \I["with L_i"]{"\G ; \D ; foc{N_1 with N_2 @ u} ==> Q @ w"}{"\G ; \D ; foc{N_i @ u} ==> Q @ w"}
    \1ex]
    \I["{dn}LF"]{"\G ; \D ; foc{now u. N @ v} ==> Q @ w"}{"\G ; \D ; foc{[v / u] N @ v} ==> Q @ w"}
    \quad
    \I["{at}LF"]{"\G ; \D ; foc{(N at u) @ v} ==> Q @ w"}{"\G ; \D ; foc{N @ u} ==> Q @ w"}
    \1ex]
    \I["one R"]{"\G ; . ==> foc {one @ w}"}
    \quad
    \I["plus R_i"]{"\G ; \D ==> foc{P_1 plus P_2 @ w}"}{"\G ; \D ==> foc{P_i @ w}"}
    \quad
    \I["!R"]{"\G ; . ==> foc{{!N} @ w}"}{"\G ; . ; . ==> N @ w ; ."}
    \

  \paragraph{Active logical rules}

  ("\RR" of the form ". ; Q @ w" or "N @ w ; .", and "\LL" of the form
  "\G ; \D ; \W")
1ex]
    \I["{dn}LA"]{"\LL, now u. P @ v ==> \RR"}{"\LL, [v/u] P @ v ==> \RR"}
    \quad
    \I["{at}LA"]{"\LL, (P at u) @ v ==> \RR"}{"\LL, P @ u ==> \RR"}
    \quad
    \I["\exists L^\alpha"]{"\LL, \fex \alpha P @ u ==> \RR"}{"\LL, P @ u ==> \RR"}
    \1ex]
    \I["with R"]{"\LL ==> M with N @ w ; ."}{"\LL ==> M @ w ; ." & "\LL ==> N @ w ; ."}
    \quad
    \I["top R"]{"\LL ==> top @ w ; ."}
    \quad
    \I["{-o}R"]{"\LL ==> P -o N @ w ; ."}{"\LL, P @ w ==> N @ w ; ."}
    \1ex]
    \I["neg R"]{"\LL ==> {neg P @ w} ; ."}{"\LL ==> . ; P @ w"}
    \quad
    \I[rp]{"\LL ==> n\ \vec t @ w ; ."}{"\LL ==> . ; pos n\ \vec t @ w"}
  
    \I[lf]{"\G ; \D, N @ u ; . ==> . ; Q @ w"}{"\G ; \D ; foc {N @ u} ==> Q @ w" & N \text{ not } "neg p\ \vec t"}
    \quad
    \I[cplf]{"\G, N @ u ; \D ; . ==> . ; Q @ w"}{"\G, N @ u ; \D ; foc {N @ u} ==> Q @ w"}
    \
  \egroup
  \end{minipage}}
\caption{Focusing rules for \hyll.}
\label{fig:foc-rules}
\end{figure}

\medskip
\noindent
The two syntactic classes refer to each other via the new connectives "neg" and
"pos". Sequents in the focusing calculus are of the following forms.
\begin{center} \small
  \begin{tabular}{r@{\ }l@{\qquad}r@{\ }l}
     & active &
     & focused \\
  \end{tabular}
\end{center}
In each case, "\G" and "\D" contain only negative propositions (\ie, of the form
"N @ u") and "\W" only positive propositions (\ie, of the form "P @ u"). 
The full collection of inference rules are in \figref{foc-rules}.
The sequent form "\G ; \D ; . ==> . ; P @ w" is called a \emph{neutral sequent}; from
such a sequent, a left or right focused sequent is produced with the rules lf,
cplf or rf. Focused logical rules are applied (non-deterministically) and focus
persists unto the subformulas of the focused proposition as long as they are of
the same polarity; when the polarity switches, the result is an active sequent,
where the propositions in the innermost zones are decomposed in an irrelevant
order until once again a neutral sequent results.

Soundness of the focusing calculus with respect to the ordinary sequent calculus
is immediate by simple structural induction. In each case, if we forget the
additional structure in the focused derivations, then we obtain simply an
unfocused proof. We omit the obvious theorem. Completeness, on the other hand,
is a hard result. We omit the proof because focusing is by now well known for
linear logic, with a number of distinct proofs via focused cut-elimination (see
\eg the detailed proof in~\cite{chaudhuri08jar}). The hybrid connectives pose no
problems because they allow all cut-permutations.

\bgroup \begin{thm}[focusing completeness]
Let "\G^-" and "C^- @ w" be negative polarizations of "\G" and "C @ w" (that
  is, adding "neg" and "pos" to make "C" and each proposition in "\G" negative)
  and "\D^+" be a positive polarization of "\D". If "\G ; \D ==> C @ w", then
  ". ; . ; {!  \G^-}, \D^+ ==> C^- @ w ; .".
\end{thm}
\egroup

\section{Encoding the synchronous stochastic -calculus}
\label{sec:spi}

In this section, we shall illustrate the use of \hyllp as a logical framework
for constrained transition systems by encoding the syntax and the operational
semantics of the synchronous stochastic -calculus (\spi), which extends the
ordinary -calculus by assigning to every channel and internal action
an \emph{inherent} rate of synchronization.
In \spi, each rate characterises an exponential distribution\cite{phillips06tcsb}, 
such that the probability of a reaction with rate  
is given by
,
where the {\emph rates}  are functions depending on the time .
We have seen in \secref{hyllp} that, in this case, we can use 
a particular instance of \hyllp,
where the worlds  are , 
representing the probability of  to have its value less or equal than .
Note that the distributions have the same shape for any variables ; 
They only depend on {\emph rates}  and time .
We shall use this fact to encode \spi in \hyllp: 
a \spi reaction with rate  will be encoded by a transition 
of probability 
.
In the rest of this section, worlds  of this shape,
defined by a rate , will simply be written  
(see \defnref{sencoding} and \defnref{iencoding}).

\hyllp can therefore be seen as a formal language for expressing \spi executions (traces). 
For the rest of this
section we shall use "r, s, t, \ldots" instead of "u, v, w, \ldots" to highlight
the fact that the worlds represent (probabilities defined by) rates, 
with the understanding that "." is convolution (fact \ref{fact:convolution}) 
and "rid" is . We don't directly use
rates because the syntax and transitions of \spi are given generically for a
-calculus with labelled actions, and it is only the interpretation of the
labels that involves probabilities.

We first summarize the
syntax of \spi, which is a minor variant of a number of similar presentations
such as~\cite{phillips06tcsb}. For hygienic reasons we divide entities into the
syntactic categories of \emph{processes} () and
\emph{sums} (), defined as follows. We also include
environments of recursive definitions ("pr E") for constants.

\smallskip
\bgroup \begin{tabular}{l@{\quad}l@{\ }r@{\ }l}
\emph{(Processes)} & "pr P, pr Q, ..." & "::=" & "pr{\nu_r\ P} OR pr{P par Q} OR pr 0 OR pr{X_n\,x_1 \cdots x_n} OR pr M" \\
\emph{(Sums)} & "pr M, pr N, ..." & "::=" & "pr {act{oup{x}(y)} P} OR pr {act{inp{x}} P} OR pr{act{\tau_r} P} OR pr{M + N}" \\
\emph{(Environments)} & "pr E" & "::=" & "pr {E, X_n \triangleq P} OR pr ."
\end{tabular}
\egroup

\smallskip

"pr{P par Q}" is the parallel composition of "pr P" and "pr Q", with unit "pr
0". The restriction "pr{\nu_r\ P}" abstracts over a free channel "x" in the
process "pr{P\,x}". We write the process using higher-order abstract
syntax~\cite{pfenning88pldi}, \ie, "pr{P}" in "pr{\nu_r\ P}" is (syntactically)
a function from channels to processes. This style lets us avoid cumbersome
binding rules in the interactions because we reuse the well-understood binding
structure of the -calculus. A similar approach was taken in the
earliest encoding of (ordinary) -calculus in (unfocused) linear
logic~\cite{miller92welp}, and is also present in the encoding in
CLF~\cite{cervesato03tr}.

A sum is a non-empty choice ("+") over terms with \emph{action prefixes}: the
output action "oup{x}(y)" sends "y" along channel "x", the input action "inp{x}"
reads a value from "x" (which is applied to its continuation process), and the
internal action "\tau_r" has no observable I/O behaviour. Replication of
processes happens via guarded recursive definitions~\cite{milner99book};
in~\cite{Regev01psb} it is argued that they are more practical for programming
than the replication operator "!". In a definition "pr{X_n \triangleq P}", "pr
{X_n}" denotes a (higher-order) defined constant of arity "n"; given channels
"x_1, ..., x_n", the process "pr {X_n\,x_1 \cdots x_n}" is synonymous with
"pr{P\,x_1 \cdots x_n}". The constant "pr{X_n}" may occur on the right hand side
of any definition in "pr E", including in its body "pr P", as long as it is
prefixed by an action; this prevents infinite recursion without progress.

Interactions are of the form "pr E |- pr P ->[r] pr Q" denoting a transition
from the process "pr P" to the process "pr Q", in a global environment "pr E",
by performing an action at rate "r". Each channel "x" is associated with an
inherent rate specific to the channel, and internal actions "\tau_r" have rate
"r". The restriction "pr{\nu_r\ P}" defines the rate of the abstracted channel
as "r".

\sdef{crate}{\mathop{\mathrm{rate}}}

\begin{figure*}[tp]
\small
\hspace{-1.8em}
\framebox{ \begin{minipage}{1.05\textwidth}
  \emph{Interactions} \vspace{-1em}

  

  \vspace{-1.5em} \dotfill

  \emph{Congruence} \vspace{-1em}
  

\end{minipage}}
\caption{Interactions and congruence in \spi. The environment  is elided in most rules.}
\label{fig:spi}
\end{figure*}

The full set of interactions and congruences are in fig.~\ref{fig:spi}. We
generally omit the global environment "pr E" in the rules as it never changes.
It is possible to use the congruences to compute a normal form for processes
that are a parallel composition of sums and each reaction selects two suitable
sums to synchronise on a channel until there are no further reactions possible;
this refinement of the operational semantics is used in "spi" simulators such as
SPiM~\cite{phillips04bc}.

\bgroup \begin{defn}[syntax encoding] \mbox{} \label{defn:sencoding}
  \begin{ecom}[1.]
  \item The encoding of the process "pr P" as a positive proposition, written
    "proc P", is as follows ("dt" is a positive atom and \crt a negative atom).
    
  \item The encoding of the sum "pr M" as a negative proposition, written "sum
    M", is as follows (\cn{out}, \cn{in} and \cn{tau} are positive atoms).
    
  \item The encoding of the definitions "pr E" as a context, written "env E", is
    as follows.
    
    where "P o-o Q" is defined as "(P -o neg Q) with (Q -o neg P)".
  \end{ecom}
\end{defn}
\egroup

The encoding of processes is positive, so they will be decomposed in the active
phase when they occur on the left of the sequent arrow, leaving a collection of
sums.  The encoding of restrictions will introduce a fresh unrestricted
assumption about the rate of the restricted channel. Each sum encoded as a
processes undergoes a polarity switch because "-o" is negative; the antecedent
of this implication is a \emph{guard} "dt". This pattern of guarded switching of
polarities prevents unsound congruences such as "pr{act{oup x(m)} act{oup y(n)}
  P} == pr{act{oup y(n)} act{oup x(m)} P}" that do not hold for the synchronous
 calculus.\footnote{Note:  is not provable in linear logic.}  This guard also
\emph{locks} the sums in the context: the \spi interaction rules \set{INT} and
\set{SYN} discard the non-interacting terms of the sum, so the environment will
contain the requisite number of "dt"s only when an interaction is in progress.
The action prefixes themselves are also synchronous, which causes another
polarity switch. Each action releases a token of its respective kind---\cn{out},
\cn{in} or \cn{tau}---into the context. 
These tokens must be consumed by the interaction before 
the next interaction occurs.
For each action, 
the (encoding of the) continuation process is also released into the context.

The proof of the following congruence lemma is omitted. Because the encoding is
(essentially) a "tens / with" structure, there are no distributive laws in
linear logic that would break the process/sum structure.

\begin{thm}[congruence] 
  \label{thm:congr} \hfill\break
  "pr E |- pr P == pr Q" iff both "env E @ rid ; . ; proc P @ rid ==> . ; proc Q @ rid" 
  and "env E @ rid ; . ; proc Q @ rid ==> . ; proc P @ rid".
\end{thm}

Now we encode the interactions. Because processes were lifted into propositions,
we can be parsimonious with our encoding of interactions by limiting ourselves
to the atomic interactions \set{syn} and \set{int} (below); the \set{par},
\set{res} and \set{cong} interactions will be ambiently implemented by the
logic. Because there are no concurrent interactions---only one interaction can
trigger at a time in a trace---the interaction rules must obey a locking
discipline. We represent this lock as the proposition \cact that is consumed at
the start of an interaction and produced again at the end. This lock also
carries the net rate of the prefix of the trace so far: that is, an interaction
"pr P ->[r] pr Q" will update the lock from "\cact @ s" to "\cact @ {s .
  r}". The encoding of individual atomic interactions must also remove the
\cn{in}, \cn{out} and \cn{tau} tokens introduced in context by the interacting
processes.

\begin{defn}[interaction] \mbox{} \newline
\label{defn:iencoding}
Let "\cinter \triangleq !! (\cact -o neg \cint with neg \csyn)" where \cact is
  a positive atom and \cint and \csyn are as follows:
  
\end{defn}

\noindent
The number of interactions that are allowed depend on the number of instances of
\cinter in the linear context: each focus on \cinter implements a single
interaction. If we are interested in all finite traces, we will add \cinter to
the unrestricted context so it may be reused as many times as needed.

\subsection{Representational adequacy.}
\label{sec:spi.adq}

Adequacy consists of two components: completeness and soundness. Completeness is
the property that every \spi execution is obtainable as a \hyll derivation using
this encoding, and is the comparatively simpler direction (see
\thmref{completeness}). Soundness is the reverse property, and is false for
unfocused \hyll as such. However, it \emph{does} hold for focused proofs (see
\thmref{adeq}). In both cases, we reason about the following canonical sequents
of \hyll.

\begin{defn} The \emph{canonical context of} "P", written "can P", is given by:
  
  For "can{\nu_r\ P}", the right hand side uses a \emph{fresh} channel "a" that
  is not free in the rest of the sequent it occurs in.
\end{defn}

\noindent
As an illustration, take "pr P \triangleq pr{act{oup x(a)} Q par act{inp x} R}". We have:

Obviously, the canonical context is what would be emitted to the linear zone at
the end of the active phase if "proc P" were to be present in the left active
zone.

\begin{defn} A neutral sequent is \emph{canonical} iff it has the shape
  
  where \cn{rates} contains elements of the form "rt x @ r" defining the
  rate of the channel "x" as "r", and all free channels in "env{E}, can{P_1 par \cdots
    par P_k par Q}" have a single such entry in \cn{rates}.
\end{defn}

\begin{figure*}[tp]
  \centering
  \small
  1ex]
    \infer={"\LL ; neg \cact @ s, can{act{oup x(a)} Q par act{inp x} R} ; . ==> . ; \RR"}{
    \I[1]{"\LL ;
              neg \cact @ s,
              dt -o neg (\cout x\ a tens proc Q),
              dt -o all y. neg (\cin x~y tens proc {R\,y})
              ; . ==> . ; \RR"}
         {\I[2]{"\LL ; 
                     neg \cact @ s,
                     dt -o neg (\cout x\ a tens proc Q),
                     dt -o all y. neg (\cin x~y tens proc {R\,y})
                     ; foc{\cinter} ==> \RR"
               }
               {\I[3]{\LL\ ;\ 
                      \begin{array}[t]{l}
                        "dt -o neg (\cout x\ a tens proc Q)",
                        "dt -o all y. (\cin x~y tens proc {R\,y})", \\
                        "neg dt, neg dt, all x, r, m. ((\cout x~m tens \cin x~m at rid) -o pos(\crt x at r) -o rate {r} \cact) @ s"
                      \end{array}
                      \begin{array}[t]{l}
                        {} \\ "{} ; . ==> . ; \RR"
                      \end{array}
                     }
                     {\I[4]{\LL\ ; \
                            \begin{array}[t]{l}
                              "neg \cout x\ a", "can Q",
                              "dt -o all y. neg (\cin x~y tens proc {R\,y})", \\
                              "neg dt, all x, r, m. ((\cout x~m tens \cin x~m at rid) -o pos(\crt x at r) -o rate {r} \cact) @ s"
                            \end{array}
                            \begin{array}[t]{l}
                              {} \\ "{} ; . ==> . ; \RR"
                            \end{array}
                           }
                           {\I[5]{\LL\ ;\
                                  \begin{array}[t]{l}
                                    "can Q", "neg \cout x~a", "neg \cin x~a", "can{R\,a}", \\
                                    "all x, r, m. ((\cout x~m tens \cin x~m at rid) -o pos(\crt x at r) -o rate {r} \cact) @ s"
                                  \end{array}
                                  \begin{array}[t]{l}
                                    {} \\ "{} ; . ==> . ; \RR"
                                  \end{array}
                                 }
                                 {"\LL ; can{Q}, can{R\,a}, neg \cact @ {s . r} ; . ==> . ; \RR"}
                           }
                     }
               }
         }
       }\
  \begin{tabular}{l@{:\ }l@{\SP}l@{:\ }l@{\SP}l@{:\ }l}
    \multicolumn{2}{l}{Steps}\\\hline
    1 & focus on "\cinter \in \LL" &
    3 & "dt" for output + full phases &
    5 & cleanup \\
    2 & select \csyn from \cinter, active rules & 
    4 & "dt" for input + full phases
  \end{tabular}
  \caption{Example interaction in the \spi-encoding.}
  \label{fig:example-syn}
\end{figure*}

\figref[Figure]{example-syn} contains an example of a derivation for a canonical
sequent involving "pr P". Focusing on any (encoding of a) sum in "can P @ rid"
will fail because there is no "dt" in the context, so only \cinter can be given
focus; this will consume the \cact and release two copies of "(dt at rid)" and
the continuation into the context. Focusing on the latter will fail now (because
"\cout x~m" and "\cin x~m" (for some "m") are not yet available), so the only
applicable foci are the two sums that can now be ``unlocked'' using the
"dt"s. The output and input can be unlocked in an irrelevant order, producing
two tokens "\cin x~a" and "\cout x~a". Note in particular that the witness "a"
was chosen for the universal quantifier in the encoding of "pr{act{inp{x}}Q}"
because the subsequent consumption of these two tokens requires the messages to
be identical. (Any other choice will not lead to a successful proof.)  After
both tokens are consumed, we get the final form "\cact @ {s . r}", where "r" is
the inherent rate of "x" (found from the \cn{rates} component of the
unrestricted zone). This sequent is canonical and contains "can{Q par R\,a}".

Our encoding therefore represents every \spi action in terms of ``micro''
actions in the following rigid order: one micro action to determine what kind of
action (internal or synchronization), one micro action per sum to select the
term(s) that will interact, and finally one micro action to establish the
contract of the action. Thus we see that focusing is crucial to maintain the
semantic interpretation of (neutral) sequents. In an unfocused calculus, several
of these steps could have partial overlaps, making such a semantic
interpretation inordinately complicated. We do not know of any encoding of the
 calculus that can provide such interpretations in unfocused sequents
without changing the underlying logic. In CLF~\cite{cervesato03tr} the logic is
extended with explicit monadic staging, and this enables a form of
adequacy~\cite{cervesato03tr}; however, the encoding is considerably more
complex because processes and sums cannot be fully lifted and must instead be
specified in terms of a lifting computation. Adequacy is then obtained via a
permutative equivalence over the lifting operation. Other encodings of 
calculi in linear logic, such as~\cite{garg05concur} and~\cite{baelde05stage},
concentrate on the easier asynchronous fragment and lack adequacy proofs anyhow.

\def\sproc{\set{proc}}
\def\ssum{\set{sum}}
\def\sinter{\set{inter}}

\begin{thm}[completeness] \label{thm:completeness} If "pr E |- pr P ->[r] pr Q", then the following canonical sequent is derivable.
  
\end{thm}

\begin{proof}
  By structural induction of the derivation of "pr E |- pr P ->[r] pr Q". Every
  interaction rule of \spi is implementable as an admissible inference rule for
  canonical sequents. For \set{cong}, we appeal to \thmref{congr}.
\end{proof}

Completeness is a testament to the expressivity of the logic -- all executions
of \spi are also expressible in \hyll. However, we also require the opposite
(soundness) direction: that every canonical sequent encodes a possible \spi
trace. The proof hinges on the following canonicity lemma.

\begin{lem}[canonical derivations] \label{lem:canonical} In a derivation for a canonical sequent, the derived inference rules for
  \cinter are of one of the two following forms (conclusions and premises
  canonical).
  1ex]
    \I{"env{E}, \cn{rates}, \cinter @ rid ; neg \cact @ s, can P @ rid ; . ==> . ; (proc R at rid) tens \cact @ t"}
      {"env{E}, \cn{rates}, \cinter @ rid ; neg \cact @ {s . r}, can Q @ rid ; . ==> . ; (proc R at rid) tens \cact @ t"}
  
  "!! cn{system} @ 0"
  \ ; \ 
  \underbrace{"rate{r + t}cn{on}\,{cn a} @ 0, cn{off}\,{cn b} @ 0"}_{\text{initial state}}
  \ "==>" 
  \underbrace{"rate{r + t + d}cn{off}\,{cn a} tens top @ 0"}_{\text{final state}}

  "cn{repress}~a~b" &\DEF "cn{prot}~a tens cn{on}~b -o rate{d} (cn{off}~b tens cn{prot}~a)" \\
  "cn{synt}" &\DEF "all a. cn{on}~a -o rate{t} (cn{on}~a tens cn{prot}~a)". \\
  "cn{react}" &\DEF "all a. cn{off}~a -o rate{r} cn{on}~a". \\
  "cn{diss}" &\DEF "all a. cn{prot}~a -o rate{s} one". 

  \I[\cn{react}]{"cn{on}~cn{a}, cn{off}~cn{b} ==> ex k. rate{k} (cn{off}~cn{a} tens cn{on}~cn{b})"}
    {\I{"cn{off}~cn{b} ==> cn{off}~cn{b}"}{}
     &
     \I[\cn{synt}]{"cn{on}~cn{a}, rate{r} cn{on}~cn{b} ==> ex k. rate{k} (cn{off}~cn{a} tens cn{on}~cn{b})"}
       {\I{"cn{on}~cn{b} ==> cn{on}~cn{b}"}
        &
        \I{"cn{on}~cn{a}, rate{r} rate{t} (cn{on}~cn{b} tens cn{prot}~cn{b}) ==> ex k. rate{k} (cn{off}~cn{a} tens cn{on}~cn{b})"}
          {\I[]{"cn{on}~cn{a}, rate{r} rate{t} cn{prot}~cn{b} ==> rate{r} rate{t} rate{d} cn{off}~cn{a}"}
           &
           \cdots
          }
       }
    }
  \

In this proof we are using the transition rules at many different worlds. This
is allowed because the rules are prefixed with "!!" and therefore available at
all worlds. Importantly, in the first premise of "P" we need to show that
"cn{on}~cn{a} ==> rate r rate t cn{on}~cn{a}". This is only possible if the rate
of a self-transition on "cn{on}~cn{a}" is "r . t". Of course, this is not
derivable from the rest of the theory (and may not actually be true), so it must
be added as a new rule; it is the contract that must be satisfied by the
repressilator in order for it to oscillate in the desired fashion.

All existing methods for modelling biology have algebraic foundations and 
none treats logic as the primary inferential device. 
In this section, we have sketched a mode of use of \hyll that lets one
represent the biological elements directly in the logic.
Note, however,
that unlike formalisms such as the brane or -calculi, we do not
propose \hyll as a new idealisation of biology. Instead, as far as systems
biology is concerned, our proposal should be seen as a uniform language to
encode biological systems; 
providing genuine means to reason about them is left for future work.

\section{Related work}
\label{sec:related}

Logically, the \hyll sequent calculus is a variant of labelled deduction, a very
broad topic not elaborated on here. The combination of linear logic with
labelled deduction isn't new to this work. In the
-logic~\cite{deyoung08csf} the constraint domain is intervals of time, and
the rules of the logic generate constraint inequalities as a side-effect;
however its sole aim is the representation of proof-carrying authentication, and
it does not deal with genericity or focusing. The main feature of  not in
\hyll is a separate constraint context that gives new constrained
propositions. \hyll is also related to the Hybrid Logical Framework
(HLF)~\cite{reed06hylo} which captures linear logic itself as a labelled form of
intuitionistic logic. Encoding constrained  calculi directly in HLF would
be an interesting exercise: we would combine the encoding of linear logic with
the constraints of the process calculus. Because HLF is a very weak logic with a
proof theory based on natural deduction, it is not clear whether (and in what
forms) an adequacy result in \hyll can be transferred to HLF.



Temporal logics such as CSL and PCTL~\cite{hansson94fac} are popular
for logical reasoning on temporal properties of transition systems with probabilities.
In such logics, truth is defined in terms of correctness with respect to a
constrained forcing relation on the constraint algebra.
In CSL and PCTL
states are formal entities (names) labeled with atomic propositions.
Formulae are interpreted on algebraic structures that are discrete (in PCTL)
or continuous (in CSL) time Markov chains.
Transitions between states are viewed as couples of states labeled with a probability
(the probability of the transition), which is defined as a function
from  into , where  is the set of states.
While such logics have been very successful in practice with
efficient tools, the proof theory of these logics is very complex. Indeed, such
modal logics generally cannot be formulated in the sequent calculus, and
therefore lack cut-elimination and focusing. In contrast, \hyll has a very
traditional proof theoretic pedigree, but lacks such a close correspondence
between logical and algebraic equivalence. Probably the most well known and
relevant stochastic formalism not already discussed is that of stochastic
Petri-nets~\cite{marsan95book}, which have a number of sophisticated model
checking tools, including the PRISM framework~\cite{kwiatkowska04sttt}. Recent
advances in proof theory suggest that the benefits of model checking can be
obtained without sacrificing proofs and proof search~\cite{baelde07cade}.

\section{Conclusion and future work}
\label{sec:concl}

We have presented \hyll, a hybrid extension of intuitionistic linear logic with
a simple notion of situated truth, a traditional sequent calculus with
cut-elimination and focusing, and a modular and instantiable constraint system
(set of worlds) that can be directly manipulated using hybrid connectives.
We have proposed three instances of \hyll
(i.e three particular instances of the set of worlds):
one modelling temporal constraints and the others modelling probabilistic
or stochastic (continuous time Markov processes) constraints.
We have shown how to obtain representationally adequate encodings of constrained
transition systems, such as the synchronous stochastic -calculus in a suitable
instance of \hyll.
We have also presented some preliminary experiments of direct encoding of biological 
systems, viewed as transition systems, in \hyll, using either temporal or probabilistic 
constraints.

Several instantiations of \hyll besides the ones in this paper seem
interesting. For example, we can already use disjunction ("plus") to explain
disjunctive states, but it is also possible to obtain a more extensional
branching by treating the worlds as points in an arbitrary partially-ordered set
instead of a monoid. Another possibility is to consider lists of worlds instead
of individual worlds -- this would allow defining periodic availability of a
resource, such as one being produced by an oscillating process. The most
interesting domain is that of discrete probabilities: here the underlying
semantics is given by discrete time Markov chains instead of CTMCs, which are
often better suited for symbolic evaluation~\cite{wu07qest}.

The logic we have provided so far is a logical framework well suited {\it to represent}
constrained transition systems. The design of a logical framework {\it for}
(i.e. to reason about) constrained transition systems is left for future work
-and might be envisioned by using a two-levels logical framework such as the Abella system.

An important open question is whether a general logic such as \hyll can serve as
a framework for specialized logics such as CSL and PCTL. A related question is
what benefit linearity truly provides for such logics -- linearity is obviously
crucial for encoding process calculi that are inherently stateful, but CSL
requires no such notion of single consumption of resources.

In the -calculus, reactions in a biological system are
modeled as reductions on graphs with certain state annotations.
It appears (though this has not been formalized)
that the -calculus can be embedded in \hyll even more naturally than
\spi, because a solution---a multiset of chemical products---is simply a tensor
of all the internal states of the binding sites together with the formed
bonds. One important innovation of  is the ability to extract
semantically meaningful ``stories'' from simulations. We believe that \hyll
provides a natural formal language for such stories.

We became interested in the problem of encoding stochastic reasoning in a
resource aware logic because we were looking for the logical essence of
biochemical reactions. What we envision for the domain of ``biological
computation'' is a resource-aware stochastic or probabilistic
-calculus that has \hyll propositions as (behavioral) types.
First step in this direction consists in exploiting and polishing 
the logic we have provided; This is the focus of our efforts at the CNRS, I3S.


\paragraph{Acknowledgements}

This work was partially supported by INRIA through the Information Society Technologies
programme of the European Commission, Future and Emerging Technologies under the
IST-2005-015905 MOBIUS project, and by the European TYPES project.
We thank Fran\c{c}ois Fages, Sylvain Soliman, Alessandra Carbone, Vincent Danos
and Jean Krivine for fruitful discussions on various preliminary versions of the
work presented here.
Thanks also go to Nicolas Champagnat and Luc Pronzato who helped us understand 
the algebraic nature of stochastic constraints.

\bgroup \small
\bibliographystyle{plain}
\bibliography{master,hyll-report,markov}
\egroup


\clearpage
\appendix

\section{Proofs}
\label{sec:proofs}

\subsection{Identity principle}
\label{sec:proofs.identity}

\begin{thm}[Identity principle] The following rule is derivable.
  
\end{thm}

\begin{proof}
  By induction on the structure of "A". We have the following cases.
  \begin{ecom} [{\itshape {case}}] 
  \item "A" is an atom "p~\vec t". Then, "\G ; p~\vec t @ w ==> p~\vec t @ w" by init.
  \item "A" is "B with C".
    
  \item "A" is "top".
    
  \item "A" is "B plus C".
    
  \item "A" is "zero".
    
  \item "A" is "B -o C".
    
  \item "A" is "B tens C".
    
  \item "A" is "one".
    
  \item "A" is "all x. B".
    
  \item "A" is "ex x. B".
    
  \item "A" is "! B".
    
  \item "A" is "now u. B".
    
  \item "A" is "(B at v)".
    
  \end{ecom}
\end{proof}

\subsection{Cut admissibility}
\label{sec:proofs.cut}

\begin{thm}[Cut admissibility] The following two rules are admissible.
  
\end{thm}

\begin{proof}
  Name the two premise derivations 
  "\DD" and "\EE" respectively. The proof proceeds by induction on
  the structure of the derivations "\DD" and "\EE", and more precisely on
  a lexicographic order that allows the induction hypothesis to be used whenever:
  \begin{ecom}
  \item The cut formula becomes strictly smaller (in the subformula relation), or
  \item The cut formula remains the same, but an instance of cut is used to justify an instance of cut!.
  \item The cut formula remains the same, but the derivation "\DD" is strictly smaller, or
  \item The cut formula remains the same, but the derivation "\EE" is strictly smaller, or
  \end{ecom}
  In each case, we consider derivations to be identical that differ in such a way that one can be
  derived from the other simply by weakening and contracting the unrestricted contexts of their
  respective sequents. The lexicographic order is well-founded because the given derivations "\DD"
  and "\EE" are finite, and cut! is used at most once per subformula of "A" (see ``copy cuts''
  below). All the cuts break down into the following four major categories.

  \paragraph{Atomic cuts} where the formula "A" is an atom "p~(\vec t)". We have the following two
  cases;
  \begin{ecom}  [{\itshape {Case}.}]  
  \item "\DD" is:
    
    Then the result of the cut has the same conclusion as that of "\EE".

  \item "\EE" is
    
    Then the result of the cut has the same conclusion as that of "\DD".
  \end{ecom}

  \paragraph{Principal cuts} where a non-atomic cut formula "A" is introduced by a final right rule
  in "\DD" and a final left-rule in "\EE". We have the following cases.
  \begin{ecom} [{\itshape {Case}.}]  
  \item "A" is "A_1 with A_2", and:
    
    Then:
    \begin{quote}
      "\G ; \D, \D' ==> C @ {w'}" \by cut on "\DD_i" and "\EE'".
    \end{quote}
  \item "A" is "A_1 plus A_2", and:
    
    Then:
    \begin{quote}
      "\G ; \D, \D' ==> C @ {w'}" \by cut on "\DD'" and "\EE_i".
    \end{quote}
  \item "A" is "A_1 -o A_2", and:
    
    Then:
    \begin{quote}
      "\G ; \D, A_1 @ w, \D'_2 ==> C @ {w'}" \by cut on "\DD'" and "\EE_2". \\
      "\G ; \D, \D'_1, \D'_2 ==> C @ {w'}" \by cut on "\EE_1" and above.
    \end{quote}

  \item "A" is "A_1 tens A_2", and:
    
    Then:
    \begin{quote}
      "\G ; \D', \D_2, A_1 @ w ==> C @ {w'}" \by cut on "\DD_2" and "\EE'".\\
      "\G ; \D', \D_1, \D_2 ==> C @ {w'}" \by cut on "\DD_1" and above.
    \end{quote}
  \item "A" is "one", and:
    
    The result of the cut is the conclusion of "\EE'".

  \item "A" is "all x. B", and:
    
    Let "a" be any parameter. Then:
    \begin{quote}
      "\G ; \D, \D' ==> C @ {w'}" \by cut on "\DD'(\tau)" and "\EE'".
    \end{quote}

  \item "A" is "ex x. B", and:
    
    Let "a" be any parameter. Then:
    \begin{quote}
      "\G ; \D, \D' ==> C @ {w'}" \by cut on "\DD'" and "\EE'(\alpha)".
    \end{quote}

  \item "A" is "! B", and:
    
    Then:
    \begin{quote}
      "\G ; \D' ==> C @ {w'}" \by cut! on "\DD'" and "\EE'".
    \end{quote}

  \item "A" is "now u. B", and:
    
    Then:
    \begin{quote}
      "\G ; \D, \D' ==> C @ {w'}" \by cut on "\DD'" and "\EE'".
    \end{quote}

  \item "A" is "(B at v)", and:
    
    Then:
    \begin{quote}
      "\G ; \D, \D' ==> C @ {w'}" \by cut on "\DD'" and "\EE'".
    \end{quote}
  \end{ecom}

  \paragraph{Copy cuts} where the cut formula in "\EE" was transferred using copy, i.e.:
  
  Here,
  \begin{quote}
    "\G, A @ w ; . ==> A @ w" \by weakening on "\DD".\\
    "\G, A @ w ; \D' ==> C @ {w'}" \by cut on "\DD" and "\EE'".\\
    "\G ; \D' ==> C @ {w'}" \by cut! on "\DD" and above.
  \end{quote}
  The first cut is applied on a variant of "\DD" that differs from "\DD" only in terms of a weaker
  unrestricted context. In the last step, a cut was used to justify a cut!, which is allowed by the
  lexicographic order.

  \paragraph{Left-commutative cuts} where the cut formula "A" is a side formula in the derivation
  "\DD". The following is a representative case.
  
  Here,
  \begin{quote}
    "\G ; \D, D @ {w''}, E @ {w''}, \D' ==> C @ {w'}" \by cut on "\DD'" and "\EE". \\
    "\G ; \D, \D', D tens E @ {w''} ==> C @ {w'}" \by "tens L".
  \end{quote}

  \paragraph{Right-commutative cuts} where the cut formula "A" is a side formula in the derivation
  "\EE". The following is a representative case.
  
  Here,
  \begin{quote}
    "\G ; \D, \D' ==> D @ {w'}" \by cut on "\DD" and "\EE_1". \\
    "\G ; \D, \D' ==> E @ {w'}" \by cut on "\DD" and "\EE_2". \\
    "\G ; \D, \D' ==> D with E @ {w'}" \by "with R".
  \end{quote}

  \noindent
  This completes the inventory of all possible cuts.
\end{proof}

\subsection{Invertibility}
\label{sec:proofs.invert}

\begin{thm}[Invertibility] The following rules are invertible:
  \begin{ecom}
  \item On the right: "with R", "top R", "{-o} R", "\forall R", "{dn} R" and "@ R";
  \item On the left: "tens L", "one L", "plus L", "zero L", "\exists L", "!L", "{dn} L" and "at L".
  \end{ecom}
\end{thm}

\begin{proof}
  Each inversion is shown to be admissible using a suitable cut.
  \begin{ecom} [{\itshape {Case of}}]  
  \item "with R":
    
  \item "top R": trivial.
  \item "{-o} R":
    
  \item "\forall R":
    
  \item "{dn} R":
    
  \item "at R":
    

  \item "tens L":
    

  \item "one L":
    

  \item "plus L":
    

  \item "zero L": trivial.

  \item "\exists L":
    

  \item "!L":
    

  \item "{dn}L":
    

  \item "at L":
    
  \end{ecom}
\end{proof}

\subsection{Correctness and consistency}
\label{sec:proofs.correct}

\begin{thm}[Correctness of the sequent calculus] \mbox{}
  \begin{ecom}
  \item If "\G ; \D ==> C @ w", then "\G ; \D |-nd C @ w". (soundness)
  \item If "\G ; \D |-nd C @ w", then "\G ; \D ==> C @ w". (completeness)
  \end{ecom}
\end{thm}

\begin{proof}
  The right rules of the sequent calculus and the introduction rules of natural deduction
  coincide. Therefore, for (1), we need only to show that the judgemental and left rules of the
  sequent calculus are admissible in natural deduction, and for (2), only to show that the
  judgemental and elimination rules of natural deduction are admissible in the sequent calculus. The
  following are the main cases.
  \begin{ecom} ["==>"/"|-nd" {case}.]  
  \item (init) 
    
  \item (copy)
    
  \item ("with L_i")
    
  \item ("plus L")
    
  \item ("zero L")
    
  \item ("tens L")
    
  \item ("one L")
    
  \item ("{-o} L")
    
  \item ("\forall L")
    
  \item ("\exists L")
    
  \item ("!L")
    
  \item ("{dn} L")
    
  \item ("at~ L")
    
  \end{ecom}

  \begin{ecom} ["|-nd"/"==>" {case}.]  
  \item (hyp)
    
  \item (hyp!)
    
  \item ("with E_i")
    
  \item ("plus E")
    
  \item ("zero E")
    
  \item ("tens E")
    
  \item ("one E")
    
  \item ("\forall E")
    
  \item ("\exists E")
    
  \item ("!E")
    
  \item ("{dn} E")
    
  \item ("at~ E")
    
  \end{ecom}
\end{proof}

\begin{cor}[Consistency of \hyll]
  There is no proof of ". ; . |-nd zero @ w".
\end{cor}

\begin{proof}
  Suppose ". ; . |-nd zero @ w" is derivable. Then, by the completeness and cut-admissibility
  theorems on the sequent calculus, ". ; . ==> zero @ w" must have a cut-free proof. 
  But, we can see by simple inspection that there can be no cut-free proof of 
  ". ; . ==> zero @ w", as this sequent cannot be the conclusion of any rule of inference 
  in the sequent calculus. 
  Therefore, ". ; . |-nd zero @ w" is not derivable.
\end{proof}

\subsection{Connection to IS5}
\label{sec:proofs.is5}

\begin{thm}[\hyll is intuitionistic S5]
  The following sequent is derivable: ". ; dia A @ w ==> box dia A @ w".
\end{thm}

\begin{proof}
  
\end{proof}

\end{document}
