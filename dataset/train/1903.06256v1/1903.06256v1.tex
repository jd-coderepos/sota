To show the effectiveness of our proposed method, 
we conduct range of experiments, 
evaluating HEX's resilience against dataset shift.
To form intuition, we first examine the NGLCM and HEX 
separately with two basic testings, 
then we evaluate on two synthetic datasets, 
on in which \textit{dataset shift} is introduced at the semantic level 
and another at the raw feature level, respectively. 
We finally evaluate other two standard domain generalization datasets 
to compare with the state-of-the-art. 
All these models are trained with ADAM \citep{kingma2014adam}. 

We conducted ablation tests on our two synthetic datasets 
with two cases 1) replacing NGLCM with one-layer MLP (denoted as \textbf{M}), 
2) not using HEX/ADV (training the network with $F_A$ (Equation~\ref{eq:all})
instead of $F_L$ (Equation~\ref{eq:hex})) (denoted as \textbf{N}). 
We also experimented with the two alternative forms of HEX: 
1) with $F_G$ in the loss and $\lambda=1$ (referred as HEX-ADV), 
2) predicting with $F_L$ (referred as HEX-ALL).
We also compare with the popular DG methods (DANN \citep{ganin2016domain}) 
and another method called information-dropout \citep{achille2018information}. 

\subsection{Synthetic Experiments for Basic Performance Tests}
\label{sec:synthetic}

\subsubsection{NGLCM Only Extracts Textural Information}
To show that the NGLCM only extracts textural information, 
we trained the network with a mixture of four digit recognition data sets: MNIST \citep{lecun1998gradient}, SVHN \citep{netzer2011reading}, 
MNIST-M \citep{ganin2014unsupervised}, 
and USPS \citep{denker1989neural}. 
We compared NGLCM with a single layer of MLP. 
The parameters are trained to minimize prediction risk of \textit{digits} (instead of \textit{domain}).
We extracted the representations of NGLCM and MLP 
and used these representations as features to test 
the five-fold cross-validated Na\"ive Bayes classifier's accuracy 
of predicting digit and domain. 
With two choices of learning rates, 
we repeated this for every epoch through 100 epochs of training 
and reported the mean and standard deviation 
over 100 epochs in Table~\ref{tab:glcm}: 
while MLP and NGLCM perform comparably well 
in extracting textural information, 
NGLCM is significantly less useful for recognizing the semantic label. 

\begin{table}[]
\centering
\begin{tabular}{cccccc}
\hline
 & Random & MLP (1e-2) & NGLCM (1e-2) & MLP (1e-4) & NGLCM (1e-4) \\ \hline
Domain & 0.25 & 0.686$\pm$0.020 & 0.738$\pm$0.018 & 0.750$\pm$0.054 & 0.687$\pm$0.029 \\
Label & 0.1 & 0.447$\pm$0.039 & 0.161$\pm$0.008 & 0.534$\pm$0.022 & 0.142$\pm$0.023 \\ \hline
\end{tabular}
\caption{Accuracy of domain classification and digit classification}
\label{tab:glcm}
\end{table}

\subsubsection{HEX projection}
\label{sec:exp:basic:hex}

To test the effectiveness of HEX, 
we used the extracted SURF \citep{bay2006surf} features ($800$ dimension) and GLCM \citep{lam1996texture} features ($256$ dimension) 
from office data set \citep{saenko2010adapting} ($31$ classes). 
We built a two-layer MLP ($800 \times 256$, and $256 \times 31$) 
as baseline that only predicts with SURF features. 
This architecture and corresponding learning rate are picked 
to make sure the baseline can converge 
to a relatively high prediction performance. 
Then we plugged in the GLCM part 
with an extra first-layer network $256 \times 32$
and the second layer of the baseline is extended to $288 \times 31$ 
to take in the information from GLCM. 
Then we train the network again with HEX with the same learning rate. 


\begin{table}
\centering
\begin{tabular}{cccccc}
\hline
Train & Test & Baseline & HEX & HEX-ADV & HEX-ALL \\ \hline
$A$, $W$ &$D$ & 0.405$\pm$0.016 & 0.343$\pm$0.030 & 0.343$\pm$0.030 & 0.216$\pm$0.119 \\
$D$, $W$ &$A$ & 0.112$\pm$0.008 & 0.147$\pm$0.004 & 0.147$\pm$0.004 & 0.055$\pm$0.004\\
$A$, $D$ &$W$ & 0.400$\pm$0.016 & 0.378$\pm$0.034 & 0.378$\pm$0.034 & 0.151$\pm$0.008\\ \hline
\end{tabular}
\caption{Accuracy on Office data set with extracted features. 
The Baseline refers to MLP with SURF features. 
The HEX methods refer to adding another MLP with 
features extracted by traditional GLCM methods. 
Because $D$ and $W$ are similar domains (same obejcts even share the same background), 
we believe these results favor the HEX method (see Section~\ref{sec:exp:basic:hex}) for duscussion).}
\label{tab:office}
\end{table}



The Office data set has three different subsets: 
Webcam ($W$), Amazon ($A$), and DSLR ($D$). 
We trained and validated the model on a mixture 
of two and tested on the third one. 
We ran five experiments and reported the averaged accuracy 
with standard deviation in Table~\ref{tab:office}. 
These performances are not comparable to the state-of-the-art 
because they are based on features. 
At first glance, one may frown upon on the performance of HEX 
because out of three configurations, 
HEX only outperforms the baseline in the setting \{$W$, $D$\} $\rightarrow$ $A$. 
However, a closer look into the datasets gives some promising indications for HEX: 
we notice $W$ and $D$ are distributed similarly 
in the sense that objects have similar backgrounds, 
while $A$ is distributed distinctly (Appendix~\ref{append:office}). 
Therefore, if we assume that there are two classifiers $C_1$ and $C_2$: $C_1$ 
can classify objects based on object feature 
and background feature while $C_2$ can only classify objects 
based on object feature ignoring background feature. 
$C_2$ will only perform better than $C_1$ in \{$W$, $D$\} $\rightarrow$ $A$ case, 
and will perform worse than $C_2$ in the other two cases, 
which is exactly what we observe with HEX.

\subsection{Facial Expression Classification with Nuisance Background}
We generated a synthetic data set extending the Facial Expression Research Group Database \citep{aneja2016modeling}, 
which is a dataset of six animated individuals 
expressing seven different sentiments. 
For each pair of individual and sentiment, 
there are over $1000$ images. 
To introduce the data shift, 
we attach seven different backgrounds to these images. 
In the training set (50\% of the data) 
and validation set (30\% of the data), 
the background is correlated with the sentiment label 
with a correlation of $\rho$; 
in testing set (the rest 20\% of the data), 
the background is independent of the sentiment label. 
A simpler toy example of the data set is shown in Figure~\ref{data:sentiment}. 
In the experiment, we format the resulting images to $28\times28$ grayscale images. 



\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{sentiment.pdf}
    \caption{Averaged testing accuracy and standard deviation of five repeated experiments with different correlation level on sentiment with nuisance background data. Notations: baseline CNN (\textbf{B}), Ablation Tests (\textbf{M} (replacing NGLCM with MLP) and \textbf{N} (training without HEX projection)), ADVE (\textbf{E}), ADV (\textbf{A}), HEX (\textbf{H}), HEX-ADV (\textbf{V}), HEX-ALL (\textbf{L}), DANN (\textbf{D}), and InfoDropout (\textbf{I}).}
    \label{fig:sentiment}
\end{figure}

We run the experiments first with the baseline CNN 
(two convolutional layers and two fully connected layers) 
to tune for hyperparameters. 
We chose to run 100 epochs with learning rate 5e-4 
because this is when the CNN can converge for all these 10 synthetic datasets. 
We then tested other methods with the same learning rate. 
The results are shown in Figure~\ref{fig:sentiment} 
with testing accuracy and standard deviation 
from five repeated experiments. 
Testing accuracy is reported by the model 
with the highest validation score. 
In the figure, we compare baseline CNN (\textbf{B}), Ablation Tests (\textbf{M} and \textbf{N}), ADV (\textbf{A}), HEX (\textbf{H}), DANN (\textbf{D}), and InfoDropout (\textbf{I}). 
Most these methods perform well when $\rho$ is small 
(when testing distributions are relatively similar to training distribution). 
As $\rho$ increases, most methods' performances decrease, 
but Adv and HEX behave relatively stable across these ten correlation settings. We also notice that, as the correlation becomes stronger, 
\textbf{M} deteriorates at a faster pace than other methods. 
Intuitively, we believe this is because the MLP learns both from the  
\emph{semantic} signal together with superficial signal, 
leading to inferior performance when HEX projects this signal out. 
We also notice that ADV and HEX improve the speed of convergence (Appendix~\ref{append:sentiment}). 

\subsection{Mitigating the Tendency of Surface Statistical Regularities in MNIST}


\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{MNIST_Pattern.pdf}
    \caption{Averaged testing accuracy and standard deviation of five repeated experiments with different strategies of attaching patterns to MNIST data. Notations: baseline CNN (\textbf{B}), Ablation Tests (\textbf{M} (replacing NGLCM with MLP) and \textbf{N} (training without HEX projection)), ADVE (\textbf{E}), ADV (\textbf{A}), HEX (\textbf{H}), HEX-ADV (\textbf{V}), HEX-ALL (\textbf{L}), DANN (\textbf{D}), and InfoDropout (\textbf{I}).}
    \label{fig:MNIST_P}
\end{figure}

As \citet{jo2017measuring} observed, 
CNNs have a tendency to learn the surface statistical regularities: 
the generalization of CNNs is partially due to the abstraction of high level semantics of an image, 
and partially due to surface statistical regularities. 
Here, we demonstrate the ability of HEX to overcome such tendencies.
We followed the radial and random Fourier filtering 
introduced in \citep{jo2017measuring} 
to attach the surface statistical regularities 
into the images in MNIST. 
There are three different regularities altogether 
(radial kernel, random kernel, and original image). 
We attached two of these into training and validation images 
and the rest one into testing images. 
We also adopted two strategies in attaching surface patterns to training/validation images: 
1) \textit{independently}: the pattern is independent of the digit,
and 2) \textit{dependently}:
images of digit 0-4 have one pattern while images of digit 5-9 
have the other pattern. 
Some examples of this synthetic data are shown in Appendix~\ref{append:mnist}. 


We used the same learning rate scheduling strategy 
as in the previous experiment. 
The results are shown in Figure~\ref{fig:MNIST_P}.
Figure legends are the same as previous. 
Interestingly, NGLCM and HEX contribute differently across these cases. 
When the patterns are attached independently,
\textbf{M} performs the best overall, 
but when the patterns are attached dependently,
\textbf{N} and HEX perform the best overall.
In the most challenging case of these experiments 
(random kerneled as testing, pattern attached dependently),
HEX shows a clear advantage. 
Also, HEX behaves relatively more stable overall. 


\subsection{MNIST with Rotation as Domain}
We continue to compare HEX with other state-of-the-art DG methods 
(that use distribution labels) on popular DG data sets. 
We experimented with the MNIST-rotation data set, 
on which many DG methods have been tested. 
The images are rotated with different degrees to create different domains. We followed the approach introduced by \citet{ghifary2015domain}.
To reiterate: we randomly sampled a set $\mathcal{M}$ of 1000 images 
out of MNIST (100 for each label). 
Then we rotated the images in $\mathcal{M}$ counter-clockwise 
with different degrees to create data in other domains, 
denoted by $\mathcal{M}_{\ang{15}}$, $\mathcal{M}_{\ang{30}}$, $\mathcal{M}_{\ang{45}}$, $\mathcal{M}_{\ang{60}}$, $\mathcal{M}_{\ang{75}}$. 
With the original set, denoted by $\mathcal{M}_{\ang{0}}$,
there are six domains altogether. 

\begin{table}[]
\centering
\begin{tabular}{cccccccccc}
\hline
Test & CAE & MTAE & CCSA & DANN & Fusion & LabelGrad & CrossGrad & HEX & ADV\\ \hline
$\mathcal{M}_{\ang{0}}$ & 72.1 & 82.5 & 84.6 & 86.7& 85.6 & 89.7 & 88.3 & \textbf{90.1} & \textbf{89.9} \\
$\mathcal{M}_{\ang{15}}$ & 95.3 & 96.3 & 95.6 & 98& 95.0 & 97.8 & 98.6 & \textbf{98.9} & 98.6 \\
$\mathcal{M}_{\ang{30}}$ & 92.6 & 93.4 & 94.6 & 97.8& 95.6 & 98.0 & 98.0 & \textbf{98.9} & 98.8 \\
$\mathcal{M}_{\ang{45}}$ & 81.5 & 78.6 & 82.9 & 97.4&95.5 & 97.1 & 97.7 & \textbf{98.8} & 98.7\\
$\mathcal{M}_{\ang{60}}$ & 92.7 & 94.2 & 94.8 & 96.9&95.9 & 96.6 & 97.7 & 98.3 & \textbf{98.6} \\
$\mathcal{M}_{\ang{75}}$ & 79.3 & 80.5 & 82.1 & 89.1& 84.3 & \textbf{92.1} & 91.4 & 90.0 & 90.4\\
\hline
Avg & 85.6 & 87.6 & 89.1 & 94.3 & 92.0 & 95.2 & 95.3 & \textbf{95.8} & 95.2 \\
\hline
\end{tabular}
\caption{Accuracy on MNIST-Rotation data set}
\label{tab:rotated}
\end{table}

We compared the performance of HEX/ADV with several methods tested on this data including CAE \citep{rifai2011contractive}, MTAE \citep{ghifary2015domain}, CCSA \citep{motiian2017unified}, DANN \citep{ganin2016domain}, Fusion \citep{mancini2018best}, LabelGrad, and CrossGrad \citep{shankar2018generalizing}. 
The results are shown in Table~\ref{tab:rotated}: 
HEX is only inferior to previous methods in one case 
and leads the average performance overall. 

\subsection{PACS: Generalization in Photo, Art, Cartoon, and Sketch}
Finally, we tested on the PACS data set \citep{li2017deeper}, which consists of collections of images of seven different objects over four domains, including photo, art painting, cartoon, and sketch. 

Following \citep{li2017deeper}, we used AlexNet as baseline method 
and built HEX upon it. 
We met some optimization difficulties in directly training AlexNet 
on PACS data set with HEX, 
so we used a heuristic training approach: 
we first fine-tuned the AlexNet pretrained on ImageNet 
with PACS data of training domains without plugging in NGLCM and HEX,
then we used HEX and NGLCM to further train the top classifier of AlexNet 
while the weights of the bottom layer are fixed. 
Our heuristic training procedure allows us to tune the AlexNet with only 10 epoches and train the top-layer classifier 100 epochs (roughly only 600 seconds on our server for each testing case). 

We compared HEX/ADV with the following methods that have been tested on PACS: AlexNet (directly fine-tuning pretrained AlexNet on PACS training data \citep{li2017deeper}), DSN \citep{bousmalis2016domain}, L-CNN \citep{li2017deeper}, MLDG \citep{li2017learning}, Fusion \citep{mancini2018best}. Notice that most of the competing methods (DSN, L-CNN, MLDG, and Fusion) have explicit knowledge about the domain identification of the training images. The results are shown in Table~\ref{tab:pacs}. Impressively, HEX is only slightly shy of Fusion in terms of overall performance. Fusion is a method that involves three different AlexNets, one for each training domain, and a fusion layer to combine the representation for prediction. The Fusion model is roughly three times bigger than HEX since the extra NGLCM component used by HEX is negligible in comparison to AlexNet in terms of model complexity. Interestingly, HEX achieves impressively high performance when the testing domain is Art painting and Cartoon, while Fusion is good at prediction for Photo and Sketch. 
\begin{table}[]
\centering
\begin{tabular}{cccccccc}
\hline
Test Domain & AlexNet & DSN & L-CNN & MLDG & Fusion & HEX & ADV\\ \hline
Art & 63.3 & 61.1 & 62.8 & 63.6 & 64.1 & \textbf{66.8} & 64.9 \\
Cartoon & 63.1 & 66.5 & 66.9 & 63.4 & 66.8 & \textbf{69.7} & 69.6 \\
Photo & 87.7 & 83.2 & 89.5 & 87.8 & \textbf{90.2} & 87.9 & 88.2\\
Sketch & 54 & 58.5 & 57.5 & 54.9 & \textbf{60.1} & 56.3 & 55.5\\ \hline
Average & 67.0 & 67.3 & 69.2 & 67.4 & \textbf{70.3} & 70.2 & 69.5\\ \hline
\end{tabular}
\caption{Testing Accuracy on PACS}
\label{tab:pacs}
\end{table}